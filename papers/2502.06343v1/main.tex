\documentclass{article} % For LaTeX2e
%\usepackage{iclr2025, times}
\usepackage{arxiv,times}

%\usepackage{newtxtext} 
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage[colorlinks=true, linkcolor=BrickRed, urlcolor=DarkGreen, citecolor=DarkGreen, anchorcolor=DarkGreen,backref=page]{hyperref}
\graphicspath{ {../figures/} }
\usepackage{titletoc}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{wrapfig}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{natbib}
%\usepackage{macros}
\usepackage{mdframed}
\usepackage{tcolorbox}
\usepackage[capitalize,noabbrev]{cleveref}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{theorem*}{Theorem}
\newtheorem{example}{Example}
\numberwithin{equation}{section}
\numberwithin{theorem}{section}



% Define colors
\definecolor{green}{HTML}{17891a}
\definecolor{DarkGreen}{HTML}{054802}
\definecolor{SmokeBlue}{HTML}{2F5E90}
\definecolor{purple}{HTML}{800080}
\definecolor{blue}{HTML}{0000FF}   
\definecolor{red}{HTML}{FF0000}    
\definecolor{orange}{HTML}{FFA500}
\definecolor{gray}{HTML}{808080}  
\definecolor{pastelcyan}{rgb}{0.5, 0.8, 0.8}
\definecolor{pastelyellow}{rgb}{0.9, 0.8, 0.6}



\newtcolorbox{empheqboxedblue}{colback=pastelcyan, 
 colframe=white,
 width=\linewidth,
 sharpish corners,
 top=1mm, % default value 2mm
 bottom=0pt,
 left=2pt,
 right=2pt
}
\newtcolorbox{empheqboxedyellow}{colback=pastelyellow, 
 colframe=white,
 width=\linewidth,
 sharpish corners,
 top=1mm, % default value 2mm
 bottom=0pt,
 left=2pt,
 right=2pt
}
\newtcolorbox{highlight}{colback=gray!20, 
 colframe=white,
 width=\linewidth,
 sharpish corners,
 top=1mm, % default value 2mm
 bottom=0pt,
 left=2pt,
 right=2pt
}
\newmdenv[
  leftline=true,
  topline=false,
  bottomline=false,
  rightline=false,
  linewidth=2pt,
  linecolor=darkgray,
  skipabove=\baselineskip,
  skipbelow=\baselineskip % Adjust spacing above
]{theorembox}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{Causal Lifting of Neural Representations: Zero-Shot Generalization for Causal Inferences}
\fancyfoot[C]{\thepage}

\title{Causal Lifting of Neural Representations: \\ 
Zero-Shot Generalization for Causal Inferences}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
    Riccardo Cadei$^1$,  
    Ilker Demirel$^2$\thanks{Equal contribution.},  
    Piersilvio De Bartolomeis$^3$\footnotemark[1],  
    Lukas Lindorfer$^1$,  
    \\
    \textbf{Sylvia Cremer}$^1$,  
    \textbf{Cordelia Schmid}$^4$,  
    \textbf{Francesco Locatello}$^1$ \\
    \\
    $^1$Institute of Science and Technology Austria (ISTA) \\
    $^2$Massachusetts Institute of Technology (MIT) \\
    $^3$Department of Computer Science, ETH Zurich \\
    $^4$INRIA, Ecole Normale Supérieure, CNRS, PSL Research University \\
    %\texttt{riccardo.cadei@ist.ac.at}
}


% \author{
%     Riccardo Cadei \\
%     Institute of Science and Technology, Austria (ISTA) \\
%     \texttt{riccardo.cadei@ist.ac.at} \\
%     \And
%     Ilker Demirel\thanks{Equal contribution.} \\
%     Massachusetts Institute of Technology (MIT) \\
%     \And
%     Piersilvio De Bartolomeis\footnotemark[1] \\
%     Department of Computer Science, ETH Zurich \\
%     \And
%     Lukas Lindorfer \\
%     Institute of Science and Technology, Austria (ISTA) \\
%     \And
%     Sylvia Cremer \\
%     Institute of Science and Technology, Austria (ISTA) \\
%     \And
%     Cordelia Schmid \\
%     INRIA, Ecole Normale Supérieure, CNRS, PSL Research University \\
%     \And
%     Francesco Locatello \\
%     Institute of Science and Technology, Austria (ISTA) \\
% }


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
A plethora of real-world scientific investigations is waiting to scale with the support of trustworthy predictive models that can reduce the need for costly data annotations.  We focus on causal inferences on a target experiment with unlabeled factual outcomes, retrieved by a predictive model fine-tuned on a labeled \textit{similar} experiment. 
First, we show that factual outcome estimation via Empirical Risk Minimization (ERM) may fail to yield valid causal inferences on the target population, even in a randomized controlled experiment and infinite training samples. Then, we propose to leverage the observed experimental settings during training to empower generalization to downstream interventional investigations, ``\textit{Causal Lifting}'' the predictive model. We propose \textit{Deconfounded Empirical Risk Minimization} (DERM), a new simple learning procedure minimizing the risk over a fictitious target population, preventing potential confounding effects. We validate our method on both synthetic and real-world scientific data. Notably, for the first time, we zero-shot generalize causal inferences on ISTAnt dataset (without annotation) by causal lifting a predictive model on our experiment variant.
\end{abstract}

\section{Introduction}
Artificial Intelligence (AI) systems hold great promise for accelerating scientific discovery by providing flexible models capable of automating complex tasks. We already depend on deep learning predictions across various applications, including biology \citep{jumper2021highly, tunyasuvunakool2021highly, elmarakeby2021biologically, mullowney2023artificial}, sustainability \citep{castello2021quantification}, and the social sciences \citep{jerzak2022image, daoud2023using}.

While these models offer transformative potential for scientific research, their black-box nature poses new challenges. They can perpetuate hidden biases, which are difficult to detect and quantify, and risk invalidating conclusions drawn from their predictions for downstream experiments. 
Recent efforts have focused on combining capable black-box models with partially annotated data to power valid and efficient statistical inference  \citep{angelopoulos2023prediction,angelopoulos2023ppi++}. Drawing inspiration from there, we focus on enabling \textit{causal inference} on unlabeled experimental data via \textit{factual} predictions, developing methods that can leverage powerful AI models reliably in that endeavor, i.e., Prediction-Powered Causal Inference (PPCI). A key challenge in this setting is that small modeling biases can invalidate the causal conclusions, even in the simplest possible scenario, where the downstream experiment is a randomized controlled trial \citep{cadei2024smoke}. Secondly, we aim to retrieve the annotations even out-of-distribution, allowing for zero-shot generalization.
Yet, manual annotation of scientific experiments is costly, requiring experts to identify subtle signals, e.g., analyzing hours of videos to detect behavioral markers in experimental ecology. Automating the annotation process with machine learning models without any further training can alleviate this burden completely, tremendously accelerating the full pipeline. 

At the same time, in scientific applications, experimentalists often collect data through multiple experiments with similar designs, e.g., investigating the effect on the same outcome of interest under different treatment or environmental settings. While historical experiments may yield too little data to train a performant model from scratch, one can fine-tune a pre-trained foundational model to learn the patterns needed for annotating experiments. Despite being a promising direction, a critical hurdle to generalize across experiments without introducing bias remains. Fine-tuning foundational models is typically done via Empirical Risk Minimization (ERM), which tends to exploit any \textit{statistical association} in the training data to minimize prediction error. Therefore, one risks leveraging spurious associations between experiment-specific factors (e.g., equipment artifacts) and outcomes, leading to systematic prediction errors on the target experiment. 
To address the problem, we propose ``causal lifting'' such foundation models from potential confounding effects, suppressing the application-specific spurious correlations during fine-tuning.

We first discuss the challenges and feasibility of the problem, and in agreement with \citet{yao2024unifying} we show how the supervised objective has to be paired with a conditional independence constraint enforcing the model to not rely on spurious correlations in its class of experiments. We then propose a simple and tailored implementation for such constraint via a resampling approach, reweighting the samples in the empirical risk, i.e., Deconfounded Empirical Risk Minimization (DERM). 
We validate the full pipeline for Causal Lifting on both synthetic and real-world data. Notably, we leverage a new experiment (ours) \textit{similar} to ISTAnt \citep{cadei2024smoke} yet differing in several experimental and technical details, including lower-quality light conditions and diverse treatments.  For the first time, our method enables a foundational model to retrieve valid Causal Inference on ISTAnt dataset without annotation, i.e., 0-shot generalization of causal inferences on a completely unlabelled experiment.

In broader terms, this paper emphasizes the ``representation learning'' aspect of ``causal representation learning'', which has traditionally focused on identification. In~\citet{bengio2013representation}, good representations are defined as ones ``\textit{that make it easier to extract useful information when building classifiers or other predictors}.'' In a similar spirit, we focus on representations that make extracting causal information easier or at all possible with some downstream estimator. As we shall demonstrate, guaranteeing identification of the causal effect is not always possible depending on the distributional differences between the experiments and our modeling choices. Yet, we hope that our viewpoint can also offer benchmarking opportunities that are currently missing in the causal representation learning literature \citep{scholkopf2021toward} and have great potential, especially in the context of scientific discoveries.

Overall, our contributions are: 
\begin{enumerate}%[leftmargin=*]
    \item[i.] a \textbf{new problem} formulation, i.e., PPCI, reshaping the definition of Causal Representation Learning as Representation Learning for Causal Downstream Tasks beyond untestable identifiability results and enabling quantitative benchmarking,
    \item[ii.] a \textbf{new method}, i.e., DERM, for \textit{Causal Lifting} of foundational models unconfounding their representations from spurious correlations between the perceived experiment settings and the outcome of interest,
    \item[iii.] \textbf{first} valid and efficient \textbf{0-shot generalization} for PPCI on ISTAnt, \textit{Causal Lifting} DINOv2 on our lower-quality experiment.%, which \textbf{data}, i.e., recordings and annotations, we plan to \textbf{release} publicly upon acceptance (preview: \href{https://figshare.com/s/9a490b6f6eeebd73350b}{https://figshare.com/s/9a490b6f6eeebd73350b}).
\end{enumerate}



\section{Problem Formulation}
\label{sec:problem}

Let $\mathcal{E}$ a countable index set, and consider a class of Structural Causal Models (SCM) $\mathfrak{S}:=\{\mathcal{M}^e\}_{e \in \mathcal{E}}$, characterized by the following (universal) Structural Equations: 
\begin{equation}
    \left\{ 
\begin{aligned}
    \bm{Z} &:= n_{\bm{Z}} \\
    Y &:= f_Y(\bm{Z},n_Y) \\
    \bm{X} &:= f_{\bm{X}}(\bm{Z},Y,n_{\bm{X}})
\end{aligned}   
\right.
\end{equation}
and varying the exogenous variables distribution\footnote{Note that $\bm{Z}, Y$ and $\bm{X}$ distributions all depend on the environment $e$, but we omit the reference for simplicity of language by always explicit the considered distribution.}:
\begin{equation}
    n_\textbf{Z}, n_Y, n_\textbf{X} \sim  \mathbb{P}^e.
\end{equation}
We further assume that there exists a model $g^*$ retrieving $Y$ from $\bm{X}$ almost surely for the whole class, i.e.,
\begin{equation}
\label{eq:determinism}
    \exists g^*: \mathbb{P}^e(Y=g^*(\bm{X}))=1 \quad \forall e \in \mathcal{E}.
\end{equation}
Many variants of real-world experiments can be modeled via such a class of SCM, where:
\begin{itemize}
    \item $\bm{Z}$ is the universal set of (possible) experimental settings potentially affecting the outcome, i.e., all the ancestors of $\bm{X}$ and $Y$ (excluding noise),
    \item $Y$ is the outcome of interest,
    \item $\bm{X}$ is a fully informative high-dimensional observation of the experiment (e.g., video or text description), which, without machine learning, is analyzed by hand by human experts, also relying on the existence of an invariant model $g^*$.
\end{itemize}
This framework is particularly suitable for Causal Inference applications\footnote{We focus here only on \textit{causality in mean} \citep{pearl2018book}, ignoring counterfactual reasoning.}, where the outcome of interest is commonly not observed directly but extracted from a high-dimensional observation, and some experiment settings are naturally collected and potentially controlled.
In the following, we use $\bm{Z}^{e}$ to refer to the experiment settings actually observed in experiment $\mathcal{M}^e$, and $\bm{U}^e$ for the unobserved. We can further distinguish, within  $\bm{Z}^{e}$, between a treatment variable $T^e$ and observed pre-treatment variables $\bm{W}^e$. 
All together:
\begin{equation}
    \bm{Z} = {\underbrace{T^e \cup \bm{W}^e}_{\bm{Z}^e}} \cup \bm{U}^e \quad \forall e \in \mathcal{E}.
\end{equation}
Note that which variables $\bm{Z}^{e}\subseteq \bm{Z}$ are observed may change across experiments, in particular, the treatment of interest $T^e$ and the observed pre-treatment variables, $\bm{W}^e$, together with their distributions.

When a new experiment is performed, we collect observations $\bm{X}$ (and experimental conditions $T^{e}, \bm{W}^{e}$), from which the outcome $Y$ can be extracted. Instead of annotating $Y$ by hand for every new experiment, we wonder when we could leverage similar experiments, i.e., in the same class, to train or fine-tune a machine-learning system capable of supplying accurate predictions about the outcome of interest and obtain trustworthy confidence interval on a causal downstream task.
We refer to this problem as (\textit{factual}) Prediction-Powered Causal Inference (PPCI). In summary:
\begin{highlight}
    \centerline{
    \textbf{Prediction-Powered Causal Inference}}
    \vspace{0.3cm}
    \textbf{Sources: \ }
    \setlength{\leftmargini}{18pt}
    \begin{itemize}
        \item A random sample  $\mathcal{D}^{e_1}=\{(T^{e_1}_i, \bm{W}^{e_1}_i, Y_i, \bm{X}_i)\}_{i=1}^{n^{e_1}}$ 
        from a reference experiment $\mathcal{M}^{e_1} \in \mathfrak{S}$,
        \item A random sample  $\mathcal{D}^{e_2}=\{(T^{e_2}_i, \bm{W}^{e_2}_i, \_, \bm{X}_i)\}_{i=1}^{n^{e_2}}$ from a target experiment $\mathcal{M}^{e_2} \in \mathfrak{S}$, not observing the factual outcome of interest\footnote{The observed experiment settings are not necessarily shared between experiments.}.
    \end{itemize}
    \textbf{Assumption: \ } Existence of an invariant factual outcome model from the raw observations $\bm{X}$, i.e., 
    \begin{equation}
    \label{eq:determinismppci}
        \exists g^*: \mathbb{P}^e(Y=g^*(\bm{X}))=1 \quad \forall e \in \{e_1,e_2\}. 
    \end{equation}
    \textbf{Task: \ } Learn a factual outcome model estimator $\hat{g}$ conditionally unbiased on the target population, i.e,
    \begin{equation}
    \label{eq:unbiased}
        \mathbb{E}_{\mathbb{P}_{e_2}}[Y-\hat{g}(\bm{X}) |\bm{Z}]\overset{\text{a.s.}}{=} 0,
    \end{equation}
    enabling different downstream causal inferences.
\end{highlight}
Figure \ref{fig:causalmodels} illustrates the reference and target experiment using their causal models.
 \begin{figure}[h!] % Add [t] or [h] for float position control
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/RE.png} 
        \caption{\footnotesize{Reference Experiment ($\mathcal{M}^{e_1}$)}}
        \label{fig:RE}
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/TE.png} 
        \caption{\footnotesize{Target Experiment ($\mathcal{M}^{e_2}$)}}
        \label{fig:TE}
    \end{subfigure}
    \caption{Causal Model visualization of a Reference and Target Experiment from the same SCM class $\mathfrak{S}$. The observed variables are in light gray, and the unobserved in white.}
    \label{fig:causalmodels}
\end{figure}
Condition \ref{eq:unbiased} effectively means that the factual outcome estimator, $\hat{g}$, is unbiased under \textit{any} experimental setting $Z$ that can be observed in the \textit{target} distribution, ${\mathbb{P}}_{e_2}$. Once we have a factual outcome estimator that satisfies Condition~\ref{eq:unbiased}, we can use it to impute the missing outcome on the target sample and then estimate, e.g., Average Treatment Effect (ATE) via AIPW estimator~\citep{robins1994estimation,robins1995semiparametric}. As Theorem \ref{th:feasibility} formalizes, it ensures (asymptotically) valid confidence intervals---a key requirement for scientific research---on the ATE without any factual outcome observations (assuming the causal effect is identifiable). 

\begin{theorembox}
\begin{theorem}[Informal]
\label{th:feasibility}
    Given a PPCI problem and a factual outcome model $g$ conditionally unbiased on the target population, i.e., satisfying Eq. \ref{eq:unbiased}. Assume that the ATE would be identifiable in the target experiment with ground-truth labels of the effect. Then,  the AIPW estimator over the prediction-powered target sample provides an asymptotically valid confidence interval for the ATE. 
\end{theorem}
\end{theorembox}

See the formal proposition and proof in Appendix \ref{sec:proofs}.  Analogous results hold for interventional causal inferences on continuous treatment and heterogenous effect estimation, i.e., CATE estimation.


\subsection{Zero-Shot Generalization}
\label{ssec:challenges}

There are generally no guarantees for Condition \ref{eq:unbiased} to hold while training $\hat{g}$ on the reference experiment. Indeed,  due to interventions to the experimental settings $\bm{Z}$, and being $\bm{Z} \rightarrow \bm{X}$, also the high-dimensional observation $\bm{X}$ may shift out of support on target, leaving the factual outcome model not \textit{identifiable} even in the infinite sample setting. Foundational models pre-trained on extended corpus offer a promising solution to the issue, practically enabling to consistently extract all the useful information hidden in $\bm{X}$ to predict $Y$ (but not only).
This section discusses potential distribution shift issues in infinite and finite sample settings, motivating why standalone Empirical Risk Minimization (ERM) cannot mitigate any of these challenges.
Indeed, even if we assume access to an oracle encoder $\phi^*(\bm{X})=\begin{bmatrix}
\phi_{Y}(\bm{X}) \\
\psi(\bm{X})
\end{bmatrix}$, extracting from $\bm{X}$, and among other features:
\begin{itemize}
    \item all the information of $Y$, i.e., $H_{\mathbb{P}^{e}}[Y|\phi_{Y}(\bm{X})]=0$,
    \item disentangled from all the experiment setting  $\bm{Z}$, i.e., $I_{\mathbb{P}^{e}}(\phi_{Y}(\bm{X}), \bm{Z}|Y)=0$,
\end{itemize}
for all possible experiments $e \in \mathcal{E}$; we may still have trouble in learning a factual outcome classifier $h$ on top by ERM, 
since it could still perfectly minimize the empirical risk, but rely on spurious correlations between some experimental settings, e.g., retrievable from $\psi(\bm{X})$, and the outcome of interest.

\subsubsection{Issues in Infinite-Sample}
If a specific instance of observed experiment settings $\bm{z}$ is fully informative of the outcome on the reference population, e.g., $\text{Var}(Y|\bm{Z}^{e_1}=\bm{z})=0$, while varying on target for distribution shifts of the unobserved ones, standole ERM has no criteria to privilege an invariant solution to one relying on the retrieved experiment setting spurious correlations. More generally, it is enough that the outcome support is not full on the reference experiment, conditioning on some experiment settings, that ERM may privilege a model overfitting on such spurious correlation, \textit{stereotyping}.

\begin{example} \textit{Consider a hypothetical behavior classification task from videos where two different treatments with the same appearance are considered, respectively $T^{e_1}$ on the reference experiment and $T^{e_2}$ on the target experiment, e.g., two observable micro-particle applications on an ant with the same appearance as in the illustration in Figure \ref{fig:t1}\footnote{In ISTAnt dataset such effect is not applicable since the considered treatments are not visually distinguishable, but the discussion still apply to several other experiments also if two experimental settings have the same appearance but different effect on the outcome of interest, e.g., artificial light and sun light.}. Let's assume that in the reference experiment, a certain behavior $y$ is not happening if $T^{e_1}=1$, despite it being observed if $T^{e_1}=0$. A ``confounded'' model may retrieve $T^{e_1}$ appearance and simplify the classification when $T^{e_1}=1$. Such a short-cut may not hold at test time since $T^{e_2}$ has a different relation with the outcome of interest but looks like $T^{e_1}$ to the model. See Figure \ref{fig:e1re}-\ref{fig:e1te} for visualizing the reference and target causal models.
}
\end{example}

\begin{figure}[ht!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=0.9\textwidth]{figures/ant.jpg} 
        \caption{Treatment}
        \label{fig:t1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=0.9\textwidth]{figures/E1RE.png} 
        \caption{Reference}
        \label{fig:e1re}
    \end{subfigure}
    \hfill
    % Third subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=0.9\textwidth]{figures/E1TE.png} 
        \caption{Target}
        \label{fig:e1te}
    \end{subfigure}
    \caption{Illustration of a confounding effect due to different treatments of interest with the same appearance, e.g., a liquid drop, affecting ERM even in the infinite-sample regime.}
    \label{fig:example1}
\end{figure}

\subsubsection{Issues in Finite-Sample} 
In real-world applications, similar issues are due to weak overlap between the conditional outcome on the experimental settings and outcome distribution, still allowing a candidate model to leverage spurious correlations, even if not perfectly solving the task.

\begin{example}
    \textit{Consider a behavior classification task from videos with a few possible backgrounds considered and repeated varying other experimental settings. In ISTAnt dataset, for example, videos were recorded with nine possible backgrounds recognizable by some pen lines. See Figure \ref{fig:position} to visualize a batch example. In a too-small reference sample, a certain behavior $y$ may rarely appear in a certain position $p$, i.e., $\mathbb{P}_{e_1}(Y=y|P=p)\ll1$. In Figure \ref{fig:E2CM}, a visualization of the true causal model, and in Figure \ref{fig:E2PCM}, an intuitive visualization of the misleading causal model perceived in finite-sample.
    Although a human annotator is naturally agnostic to such position information for behavior classification, a model trained to minimize the empirical risk over such a sample may blindly rely on such spurious correlation and fail in generalization, avoiding predicting such behavior for that position. 
    }
\end{example}

\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.32\linewidth}
    \centering
        \includegraphics[width=0.74\textwidth]{figures/batch.png} 
        \caption{Position}
        \label{fig:position}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=0.9\textwidth]{figures/E2CM.png} 
        \caption{Causal Model}
        \label{fig:E2CM}
    \end{subfigure}
    \hfill
    % Third subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=0.9\textwidth]{figures/E2PCM.png} 
        \caption{Perceived Model}
        \label{fig:E2PCM}
    \end{subfigure}
    \caption{Illustration of a confounding effect due to positivity assumption violation in the finite-sample setting.}
    \label{fig:example2}
\end{figure}



\section{Deconfounded Empirical Risk Minimization (DERM)}
\label{sec:method}
As motivated in Section \ref{sec:problem}, directly minimizing the empirical risk of an expressive head on top of a (pre-trained) encoder may not be sufficient in generalization for PPCI, even in the infinite sample setting and relying on an oracle encoder. \textit{What should we enforce then, to enable} (or at least attempt) \textit{such desired causal generalization?}

The natural mitigation to prevent the described overfitting on spurious dependencies is to optimize for the model sufficiency while enforcing unconfoundness in the representation space, i.e., 
\begin{equation}
\label{eq:suff+unconf}
\begin{split}
    \min_{h,\phi} \quad &\mathbb{E}_{\mathbb{P}^{e_1}}[ \mathcal{L}\left(Y, h \circ \phi(\bm{X})\right) ]\\
    \text{s.t.} \quad &\phi(\bm{X})\indep_{\mathbb{P}^{e_1}} \bm{Z} | Y=y \quad \forall y \in \mathcal{Y}.
\end{split}
\end{equation}
The conditional independence constraint enforces that the mutual information between the representation and the experimental settings, conditioning on the outcome of interest, is null, i.e.,
\begin{equation}
\label{eq:mutualinfo}
    \text{I}_{\mathbb{P}^{e_1}}(\phi(\bm{X}), \bm{Z}|Y)=0,
\end{equation}
such that it cannot be used to leverage some conditional dependencies $\mathbb{P}^{e_1}_{Y|\bm{Z}^{e_1}=\bm{z}}$ potentially breaking on the target experiment since only $\mathbb{P}^e_{Y|\bm{Z}=\bm{z}}$ is invariant.
Note that such conditions can be enforced while learning $f=h\circ \phi$ from scratch or just fine-tuning a head $h$ on top of a pre-trained (oracle) encoder $\phi^*$. The former approach is exactly the problem described in Formulation \ref{eq:suff+unconf}, while in the latter case, the unconfoundness constraint, i.e., Eq. \ref{eq:mutualinfo}, has to be enforced on an internal representation of the head $h$, ideally isolating only the disentangled information of the outcome of interest (see $\phi_Y$ in Section \ref{sec:problem} discussion).


Formulation \ref{eq:suff+unconf} is a well-known problem in Representation Learning literature, and different approaches were developed, enforcing the unconfoundness constraint directly or indirectly. In Section \ref{sec:relatedwork}, we discuss a brief overview of the different paradigms. 
Among them we propose a resampling approach \citep{kirichenko2022last,li2019repair}, carefully designing a fictitious auxiliary experiment $\mathcal{M}^{e_1^{\perp\!\!\!\perp}}$ to sample from during training. It is obtained by manipulating the original reference sample so that its spurious correlations between $\bm{Z}^{e_1}$ and $Y$ are not observed, and confounding effects are prevented. In particular, we define such fictitious \textit{unconfounded} population intervening on the joint distribution $\mathbb{P}^{e_1}_{\bm{Z}^{e_1},Y}$ and enforcing independence, i.e., $\bm{Z}^{e_1} \perp\!\!\!\perp Y$, blocking any not-causal path from $\bm{X}$ to $Y$ during learning. Among the possible joint distribution guaranteeing independence, we define, for all $y \in \mathcal{Y}$ and $\bm{z}\in \mathcal{Z}^{e_1}$ in support of $\mathbb{P}^{e_1}_{\bm{Z}^{e_1},Y}$: 
\begin{equation}
    \label{eq:disentangleddist}
    \mathbb{P}^{e_1^{\perp\!\!\!\perp}}(Y=y, \bm{Z}^{e_1}=\bm{z}) := \frac{\text{Var}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z})}{\displaystyle\sum_{\bm{z}'\in \mathcal{Z}^{e_1}} \text{Var}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z}')},
\end{equation}
weighting more the least informative experimental settings for the outcome of interest (high conditional variance) and ignoring the fully informative ones (low or null variance) over the reference population. Indeed, let's observe that the marginal outcome given the observed experimental settings is constant, i.e., uniform distribution. If, for each not fully informative observed experimental setting $\bm{z}$\footnote{i.e.,  $\forall \bm{z}:H_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z})>0$.},
\begin{equation}
\label{eq:support}
    \{y\in \mathcal{Y}:\mathbb{P}^{e_1}(Y=y|\bm{Z}^{e_1}=\bm{z})>0\} = \{y\in \mathcal{Y}:\mathbb{P}^{e_1}(Y=y)>0\},
\end{equation}
then the joint distribution described in Eq. \ref{eq:disentangleddist} trivially implies independence, i.e., $\bm{Z}^{e_1} \perp\!\!\!\perp Y$, and an unconfounded representation is enforced. It is important to observe that in such implementation, spurious solutions may still optimize the risk, but such solutions cannot be selected and preferred since there is no signal in the task to retrieve the experimental settings. 

We then propose to train/fine-tune the factual outcome model on such a fictitious population by ERM sampling from the reference population and reweighting the estimated joint distribution $\mathbb{P}^{e_1}_{\bm{Z}^{e_1},Y}$ to enforce the desired disentangled distribution described in Eq. \ref{eq:disentangleddist}. We refer to this approach as Deconfounded Empirical Risk Minimization (DERM). Such implementation is suitable for applications in Causal Inference where both the experimental settings and the outcome of interest are commonly low dimensional and discrete or anyway discretized for interpretability \citep{pearl2000models, rosenbaum2010design}. In Formula:

\begin{figure}[H]
    \centering
    \begin{minipage}{0.65\textwidth}
        \begin{highlight}
        \begin{center}
            \textbf{Deconfounded Empirical Risk Minimization:}
        \end{center}
        \begin{equation}
            \label{eq:DERM}
            \hat{g}:=\arg \min_{g \in \mathcal{G}} \sum_{i\in \mathcal{D}^{e_1}} \underbrace{w_i}_{\text{unconfoundness}}\cdot\underbrace{\mathcal{L}(g(\bm{x}_i), y_i)}_{\text{sufficiency}}
        \end{equation}
        where:
        \begin{equation*}
            w_i :=\underbrace{\frac{1}{\widehat{\mathbb{P}}^{e_1}(Y=y_i, \bm{Z}^{e_1}=\bm{z}_i)}}_{\text{reference distribution}} \cdot \overbrace{\frac{\widehat{\text{Var}}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z_i})}{\displaystyle\sum_{\bm{z}'\in \mathcal{Z}^{e_1}} \widehat{\text{Var}}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z}')}}^{\text{fictitious distribution s.t. $\bm{Z} \indep Y$}}
        \end{equation*}
        \noindent and the weights $w_i$ are computed una tantum before training, the joint distribution is estimated by frequency, and the conditional variances with the sample variance.
        \end{highlight}
    \end{minipage}
    \hfill
    \begin{minipage}{0.34\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/HS.png} 
        \caption{Illustration of the factual model hypothesis space $\mathcal{G}$, the ERM solution set over the reference $\mathcal{G}^{e_1}$ and target sample $\mathcal{G}^{e_2}$, and an \textit{unconfounded} fictitious sample $\mathcal{G}^{e_1^{\perp\!\!\!\perp}}$.}
        \label{fig:hypspace} 
    \end{minipage}
\end{figure}

Let $\mathcal{G}$ be an expressive factual outcome model hypothesis space (containing an invariant factual outcome model $g^*$). The ERM solution set over the reference sample $\mathcal{G}^{e_1}$ and the target sample $\mathcal{G}^{e_2}$ overlap, while the ERM solution set $\mathcal{G}^{e_1^{\perp\!\!\!\perp}}$ over an \textit{unconfounded} fictitious sample from $\mathcal{M}^{e_1^{\perp\!\!\!\perp}}$ is included in such intersection and it still includes the invariant factual outcome model. In Figure \ref{fig:hypspace}, we illustrate the relations among these hypotheses and solution spaces. 

\looseness=-1\paragraph{Challenge: Beyond Full Support Assumption} When Condition \ref{eq:support} doesn't hold, it is not possible to retrieve a joint distribution enforcing such independence condition, i.e., Eq. \ref{eq:mutualinfo}, without setting $\mathbb{P}^{e_1^{\perp\!\!\!\perp}}(\bm{Z}^{e_1}=\bm{z})=0$ where the conditional outcome's support is strictly contained in the marginal outcome's support on the reference population. 
Our fictitious distribution still considers these samples while reducing their weight with respect to the predictivity of the observed experimental setting (approximately $\propto \text{Var}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z})$. Despite some spurious correlations may still be retrieved, it is a trade-off with ignoring a potentially substantial part of the reference sample. Tailored modification of our joint distribution can be proposed case-by-case. If necessary, alternative approaches enforcing Condition \ref{eq:mutualinfo} directly should be considered, not discarding any sample data, but computationally much more expensive and tricky to estimate. 


\subsection{Causal Lifting}
\looseness=-1An extensive supervised dataset to train a factual outcome model from scratch is rarely available in real-world applications. However, foundational models trained on extensive corpus may still be able to process complex data structures, e.g., images and text,  preserving sufficient information for the task while having never been supervised for it directly. We can then leverage such external sources to preprocess the data and train a deconfounded head on top via DERM on the available reference sample alone. We refer to this procedure as \textit{Causal Lifting} since enabling an expressive foundation model to filter only the invariant features for a task, thanks to a small fine-tuning on a single sample with additional supervision for the unconfoundness, i.e., the experiment settings information. Such procedure ideally
leads to a conditionally unbiased factual outcome estimator, enabling efficient Causal Inference on a prediction-powered target experiment\footnote{Either a Randomized Controlled Trial or Observational Study with observed confounders.} according to Theorem \ref{th:feasibility}. 
To summarize:

\begin{algorithm}[h!]
\caption{0-shot Generalization for PPCI (\textit{Causal Lifting})}
\label{alg:ppci}
\begin{algorithmic}[1] 
    \STATE \textbf{Input:} PPCI problem 
    \STATE \textbf{Output:} ATE inference on the target experiment
    \STATE \textbf{Procedure:} 
    \STATE \quad \textbf{Factual Outcome Model} 
    Extract representations from experiment observations via a foundational model and fine-tune its head factual outcome estimator using DERM.
    %\textit{Causal Lifting} of a foundational model\footnotemark{} via DERM on the reference experiment.
    \STATE \quad \textbf{Causal Inference} Via AIPW estimator on the prediction-powered target experiment.
\end{algorithmic}
\end{algorithm}
%\footnotetext{E.g., a pre-trained Vision Transformer.}
An in-detailed description of the procedure is reported in Appendix \ref{sec:algorithm}.






\section{Related Works} 
\label{sec:relatedwork}

\paragraph{Prediction-Powered Causal Inference}
The factual outcome estimation problem for causal inference from high-dimensional observation was first introduced by \citet{cadei2024smoke}. We extended the problem to generalization to a class of SCMs, also considering observational studies motivated by practitioners desiderata, e.g., experimental ecologists. In our paper, we formalize what they describe as ``encoder bias'' (see our discussion on Sufficiency and Unconfoundness in Section \ref{sec:method}), and our DERM is the first proposal to their call for ``\textit{new methodologies to mitigate this bias during adaptation}''.
\citet{demirel2024prediction} already attempted to discuss some generalization challenges in a PPCI problem but with unrealistic motivating assumptions. They ignored any high dimensional projection of the outcome of interest and assumed the experimental settings alone as sufficient for factual outcome estimation together with support overlapping, i.e., not generalization, making the model too application-specific and ignoring any connection with representation learning. Let's further observe that their framework is a special case of ours when $\bm{X}=\bm{Z}$ and low-dimensional, with the target experiment in-distribution. In contrast to the classic Prediction-Powered Inference~(PPI)~\citep{angelopoulos2023prediction,angelopoulos2023ppi++}, which improves estimation efficiency by imputing unlabeled in-distribution data via a predictive model, and recent causal inference extensions \citep{de2025efficient,poulet2025prediction} relying on counterfactual predictions, PPCI focuses on imputing missing \textit{factual} outcomes to generalize across unlabeled experiments, that have different and potentially non-overlapping ``covariate'' distributions, agnostic of the causal estimator. 

\paragraph{Unconfoundness} Learning representations invariant to certain attributes is a challenging and widely studied problem in different machine learning communities \citep{moyer2018invariant}. We wish to learn useful representations of $X$ and that can predict the outcome $Y$, but are invariant to the enviromental variables $Z$. In agreement with \citet{yao2024unifying}, we achieve Causal Representation Learning in DERM by combining a sufficiency objective with an invariance constraint enforcing unconfoundness in the representation space. Several alternative approaches can be considered to enforce such conditional independence constraints:  (i) Conditional Mutual Information Minimization \citep{song2019learning, cheng2020club, gupta2021controllable} (ii) Adversarial Independence Regularization such as \citet{louizos2015variational} which modifies variational autoencoder (VAE) architecture in \citet{kingma2014adam} to learn \textit{fair} representations that are invariant sensitive attributes, by training against an adversary that tries to predict those variables (iii) Conditional Contrastive Learning such as \citet{ma2021conditional} whereby one learns representations invariant to certain attributes by optimizing a conditional contrastive loss (iv) Variational Information Bottleneck methods where one learns useful and sufficient representations invariant to a specific \textit{domain} \cite{alemi2016deep, li2022invariant}.

\paragraph{Causal Representation Learning} In the broader context of causal representation learning methods \citep{scholkopf2021toward}, our proposal largely focuses on representation learning applications to causal inference: learning representations of data that make it possible to estimate causal estimands. We find this is in contrast with most recent works in causal representation learning, which uniquely focused on complete identifiability of all the variables or blocks, see \citet{yao2024unifying,varici2024general,von2024identifiable} for recent overviews targeting general settings. The main exceptions are \citet{yao2024unifying,yao2024marrying}. The former leverages domain generalization regularizers to debias treatment effect estimation in ISTAnt from selection bias. However, their proposal is not sufficient to prevent confounding when no data from the target experiment is given. The latter uses multi-view causal representation learning models to model confounding for adjustment in an observational climate application. In our paper, we also discuss conditions for identification, but we focus on a specific causal estimand, as opposed to block-identifiability of causal variables. Additionally, our perspective offers clear evaluation and benchmarking potential -- even in theoretically underspecified setting: the accuracy of the causal estimate. As opposed to virtually all existing work in causal representation learning, this can be empirically tested in \textit{real world} scientific experiments.


\section{Experiments}
\looseness=-1We empirically validate our approach for 0-shot generalization of PPCI solving a real-world scientific problem from experimental Behavioural Ecology. In particular, we considered ISTAnt dataset\footnote{So far the only benchmark dataset for scientifically motivated representation learning for a causal downstream task.} \citep{cadei2024smoke}, and we designed and collected a similar experimental dataset with lower filming quality and higher diversity in treatments to enable causal lifting of different pre-trained models for 0-shot generalization. We further validate our analysis on a synthetic causal manipulation of the MNIST dataset \citep{lecun1998mnist}.

\subsection{Zero-Shot Generalization on ISTAnt}
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.62\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/or_teb_ref.png}
        \caption{0-shot ATE Inference on ISTAnt dataset from our experiment, varying method and pre-trained encoder. 95\% confidence intervals estimated via AIPW asymptotic normality and baseline in black using AIPW on the ground truth outcome. Our approach applied to unsupervised backbones yield consistent estimates, unlike ERM or a general invariance regularization.}
        \label{fig:istantgeneralization}
    \end{minipage}
    \hfill
    \begin{minipage}{0.36\linewidth}
        \centering
        \begin{subfigure}[b]{0.48\linewidth}
        \centering
            \includegraphics[height=2.4cm]{figures/replicafs.jpg}
            \includegraphics[height=2.4cm]{figures/ISTantreplica.png} 
            \caption{Our experiment}
            \label{fig:ISTAntreplica}  
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\linewidth}
        \centering
            \includegraphics[height=2.4cm]{figures/istantfs.jpg} 
            \includegraphics[height=2.4cm]{figures/ISTAnt.png} 
            \caption{ISTAnt}
            \label{fig:ISTAnt}
        \end{subfigure}
        \caption{Filming box and example frame from our experiment and ISTAnt. The two datasets mainly differ in lighting quality, treatments considered, experimental nests (wall height) and color marking.}
        \label{fig:examples}
    \end{minipage}
\end{figure}


To empirically validate our methodology, we replicated a real-world scientific pipeline requiring generalization for PPCI and compared our performances with the existing approaches suitable for the task. We performed a similar experiment to the ISTAnt dataset with lower-quality filming conditions (in particular light conditions), slightly different ant coloring and diversified treatments with or without micro-particle application. We considered it the reference experiment and proposed to generalize to ISTAnt, the target experiment in the PPCI problem. Our experiment consists of 44 annotated videos, 30 minutes long, each randomly assigning one of three possible treatments (one of which does not entail usage of micro-particles and serves as a control) and considering the same outcome of interest as in ISTAnt, i.e., directional grooming, blindly annotated by a single expert (versus three different in ISTAnt). The pipeline reflects a common desiderata in experimental research: learning a model from previously annotated experiments able to cheaply and validly annotate new, out-of-distribution experiments. The new experiments commonly consider more experimental settings and rely on better or generally different data acquisition techniques.
In Figure \ref{fig:examples} we compare a random frame from ISTAnt dataset, and one from our experiment. A full description of the design and collection of the data is reported in Section \ref{ssec:replica}.
 
We started from the five best-performing vision transformers in \citet{cadei2024smoke} -- ViT-B \citep{dosovitskiy2020image}, ViT-L \citep{zhai2023sigmoid}, CLIP-ViT-B,-L \citep{radford2021learning}, DINOv2 \citep{oquab2023dinov2}. We trained several simple nonlinear heads, i.e., multi-layer perceptron, on top via (i) vanilla ERM, (ii) Variance Risk Extrapolation (vREx) \citet{krueger2021out} and (iii) DERM (ours) for Causal Lifting and used the model for 0-shot generalization for PPCI on the original ISTAnt dataset, via AIPW. For reference we considered the ATE Inference on ISTAnt by the AIPW estimator on the human-annotated factual outcomes (ground truth). On average, the treatment in ISTAnt increases the grooming time towards the focal ant by $\approx 40$ seconds.  Further details on the modeling choices, hyper-parameter, and fine-tuning are discussed in Appendix \ref{ssec:ISTAexp}

Figure \ref{fig:istantgeneralization} summarizes the results of our generalization experiment comparing the 95\% confidence intervals obtained by AIPW asymptotic normality. As expected, with vanilla ERM, there are no guarantees to Causal Lift any foundational model due to potential confounding effects, and the ATE estimates are consistently offset by underestimating/ignoring it. Similar results, using v-REx, as proposed by \citet{yao2024unifying}. Indeed, experiment setting performance invariance is not sufficient to prevent confounding effects on the target when certain association switches (see Example 2 in Section \ref{sec:problem}). DERM is the only method enabling 0-shot generalization for PPCI  with DINOv2 and (partially) with CLIP-based vision transformers. Interestingly enough, the most supervised encoder, i.e., ViT-based (trained on ImageNet \citep{deng2009imagenet}), struggles in the task, underestimating the effect, as opposed to the ones trained in a fully unsupervised fashion. We hypothesize that encoders pre-trained in a supervised fashion are more inclined to extract more entangled representations, more challenging to causal lift. 

\subsection{CausalMNIST}
\looseness=-1We replicated the analysis on colored manipulations of the MNIST dataset, enabling some fictitious PPCI problems, e.g., estimating the effect of the background color or pen color on the digit value, allowing complete control of the causal effects. While simpler, this experiment supplements the fact that obtaining ground-truth causal effect on real-world data is challenging, and one whole experiment only yields a single measurement of a target causal estimand.  

We test both on RCT and Observational Study experiments with observed confounders in either reference or target. A full description of the data-generating processes and analysis are reported in Appendix \ref{sec:CausalMNIST}. In Table \ref{tab:generalization}, we report the ATE inference as for ISTAnt, (i) on a target experiment $\mathcal{D}^{e_2}$ with a new treatment with the same appearance (see Example 1 in Section \ref{sec:problem}) and (ii) on a target experiment $\mathcal{D}^{e_3}$ strongly out-of-support. 
DERM is the unique method solving the problem on $\mathcal{D}^{e_2}$, and despite no method having guarantees on $\mathcal{D}^{e_3}$, it is still the least biased. 

\begin{table}[h]
\centering
\setlength{\tabcolsep}{8pt} % Adjust column spacing
\renewcommand{\arraystretch}{1.3} % Increase row height
    \begin{tabular}{cc|cc}
    Method & $\mathcal{D}^{e_1}$ & $\mathcal{D}^{e_2}$ & $\mathcal{D}^{e_3}$ \\ 
    & \footnotesize{($\text{ATE}=1.5$)} & \footnotesize{($\text{ATE}=0$)} & \footnotesize{($\text{ATE}=0$)} \\ \hline
    ERM & \textbf{0.00 $\pm$ 0.02} & 0.86 $\pm$ 0.14  & 1.05 $\pm$ 0.15 \\
    v-REx & 0.01 $\pm$ 0.03 & 0.83 $\pm$ 0.15 & 1.05 $\pm$ 0.14 \\
    Ours & 0.10 $\pm$ 0.07 & \textbf{0.14 $\pm$ 0.14} & \textbf{0.75 $\pm$ 0.05}
    \end{tabular}
    \medskip
\caption{ATE bias and standard deviation via AIPW on a reference trial $\mathcal{D}^{e_1}$ and two target samples $\mathcal{D}^{e_2}$-$\mathcal{D}^{e_3}$ of CausalMNIST not annotated and prediction-powered by a Convolutional Neural Network trained with different objectives.
Sample mean and standard deviation are computed over the same PPCI problem repeated 50 times, re-sampling both reference and target samples. ERM and v-REX yield biased estimates on the new population $\mathcal{D}^{e_2}$, unlike our approach.}
\label{tab:generalization}
\end{table}


\section{Conclusion}
We introduced Causal Lifting, a novel paradigm enabling zero-shot generalization of foundational models for prediction-powered causal inferences. Our concrete implementation in the Deconfounded Empirical Risk Minimization (DERM) leverages a sufficiency loss paired with an unconfoundness objective in the representation space to prevent overfitting on experiment-specific spurious correlation. Additionally, we thoroughly described in which settings causal lifting can yield unbiased estimates, unlike empirical risk minimization. Our framework is widely applicable to the analysis of experimental data, which we have empirically evaluated on the ISTAnt data set. Overall, this work offers a paradigm shift from the causal representation learning literature to learning representations that enable downstream causal estimates on real-world data, which we think is a critical component of representation learning to accelerate scientific discovery. 
The main limitation of this work is that via PPCIs we can rarely have guarantees a priori on the Causal Estimates, being Condition \ref{eq:unbiased} untestable without target annotations and Condition \ref{eq:support} potentially violated (on top of unobserved confounders issues). Model convergence is also not discussed, which is particularly interesting in the finite setting.
At the same time, we hope that more systematic (scientifically motivated) benchmarking will lead the progress of the field, e.g., challenging and comparing Causal Representation Learning identifiability results beyond their controlled assumptions. 


% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


\section*{Acknowledgments} We thank the Causal Learning and Artificial Intelligence group at ISTA, and particularly Marco Fumero, for the continuous feedback and inspiring discussions during the last year. We thank the Social Immunity group at ISTA, particularly Jinook Oh, for the annotation program and Michaela Hoenigsberger for supporting our ecological experiment. We thank Irene Guerrieri for the illustration in Figure \ref{fig:t1}.
Riccardo Cadei is supported by a Google Research Scholar Award and a Google Initiated Gift to Francesco Locatello. 

\bibliographystyle{unsrtnat}
\bibliography{refs}
\clearpage
\appendix
\onecolumn
\input{appendix}


\end{document}
