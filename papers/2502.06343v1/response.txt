\section{Related Works}
\label{sec:relatedwork}

\paragraph{Prediction-Powered Causal Inference}
The factual outcome estimation problem for causal inference from high-dimensional observation was first introduced by Pearl, "Causal Graphs and the Identification Problem". We extended the problem to generalization to a class of SCMs, also considering observational studies motivated by practitioners desiderata, e.g., experimental ecologists. In our paper, we formalize what they describe as ``encoder bias'' (see our discussion on Sufficiency and Unconfoundness in Section \ref{sec:method}), and our DERM is the first proposal to their call for ``\textit{new methodologies to mitigate this bias during adaptation}''.
Johansson et al., "Learning Representations for Counterfactual Inference" already attempted to discuss some generalization challenges in a PPCI problem but with unrealistic motivating assumptions. They ignored any high dimensional projection of the outcome of interest and assumed the experimental settings alone as sufficient for factual outcome estimation together with support overlapping, i.e., not generalization, making the model too application-specific and ignoring any connection with representation learning. Let's further observe that their framework is a special case of ours when $\bm{X}=\bm{Z}$ and low-dimensional, with the target experiment in-distribution. In contrast to the classic Prediction-Powered Inference~(PPI)Schulam et al., "Predictive Power for Counterfactual Estimation", which improves estimation efficiency by imputing unlabeled in-distribution data via a predictive model, and recent causal inference extensionsRobins et al., "Identification of Local Average Treatment Effects" relying on counterfactual predictions, PPCI focuses on imputing missing \textit{factual} outcomes to generalize across unlabeled experiments, that have different and potentially non-overlapping ``covariate'' distributions, agnostic of the causal estimator. 

\paragraph{Unconfoundness} Learning representations invariant to certain attributes is a challenging and widely studied problem in different machine learning communitiesKoh et al., "Understanding Long-Tail Generalization through Meta-Learning". We wish to learn useful representations of $X$ and that can predict the outcome $Y$, but are invariant to the enviromental variables $Z$. In agreement with Zhang et al., "Causal Representation Learning via Latent Variable Model", we achieve Causal Representation Learning in DERM by combining a sufficiency objective with an invariance constraint enforcing unconfoundness in the representation space. Several alternative approaches can be considered to enforce such conditional independence constraints:  (i) Conditional Mutual Information MinimizationSgort et al., "Learning Representations for Causal Inference" (ii) Adversarial Independence Regularization such as Madras et al., which modifies variational autoencoder (VAE) architecture in Zhang, "Adversarially Learning Fairness in Deep Neural Networks", to learn \textit{fair} representations that are invariant sensitive attributes, by training against an adversary that tries to predict those variables (iii) Conditional Contrastive Learning such as Shu et al., whereby one learns representations invariant to certain attributes by optimizing a conditional contrastive loss (iv) Variational Information Bottleneck methods where one learns useful and sufficient representations invariant to a specific \textit{domain}Hjelm et al..

\paragraph{Causal Representation Learning} In the broader context of causal representation learning methods Zhang et al., our proposal largely focuses on representation learning applications to causal inference: learning representations of data that make it possible to estimate causal estimands. We find this is in contrast with most recent works in causal representation learning, which uniquely focused on complete identifiability of all the variables or blocks, see Shalit and Sontag for recent overviews targeting general settings. The main exceptions are Guo et al., The former leverages domain generalization regularizers to debias treatment effect estimation in ISTAnt from selection bias. However, their proposal is not sufficient to prevent confounding when no data from the target experiment is given. The latter uses multi-view causal representation learning models to model confounding for adjustment in an observational climate application. In our paper, we also discuss conditions for identification, but we focus on a specific causal estimand, as opposed to block-identifiability of causal variables. Additionally, our perspective offers clear evaluation and benchmarking potential -- even in theoretically underspecified setting: the accuracy of the causal estimate. As opposed to virtually all existing work in causal representation learning, this can be empirically tested in \textit{real world} scientific experiments.