\section{Related Works}
\label{sec:relatedwork}

\paragraph{Prediction-Powered Causal Inference}
The factual outcome estimation problem for causal inference from high-dimensional observation was first introduced by \citet{cadei2024smoke}. We extended the problem to generalization to a class of SCMs, also considering observational studies motivated by practitioners desiderata, e.g., experimental ecologists. In our paper, we formalize what they describe as ``encoder bias'' (see our discussion on Sufficiency and Unconfoundness in Section \ref{sec:method}), and our DERM is the first proposal to their call for ``\textit{new methodologies to mitigate this bias during adaptation}''.
\citet{demirel2024prediction} already attempted to discuss some generalization challenges in a PPCI problem but with unrealistic motivating assumptions. They ignored any high dimensional projection of the outcome of interest and assumed the experimental settings alone as sufficient for factual outcome estimation together with support overlapping, i.e., not generalization, making the model too application-specific and ignoring any connection with representation learning. Let's further observe that their framework is a special case of ours when $\bm{X}=\bm{Z}$ and low-dimensional, with the target experiment in-distribution. In contrast to the classic Prediction-Powered Inference~(PPI)~\citep{angelopoulos2023prediction,angelopoulos2023ppi++}, which improves estimation efficiency by imputing unlabeled in-distribution data via a predictive model, and recent causal inference extensions \citep{de2025efficient,poulet2025prediction} relying on counterfactual predictions, PPCI focuses on imputing missing \textit{factual} outcomes to generalize across unlabeled experiments, that have different and potentially non-overlapping ``covariate'' distributions, agnostic of the causal estimator. 

\paragraph{Unconfoundness} Learning representations invariant to certain attributes is a challenging and widely studied problem in different machine learning communities \citep{moyer2018invariant}. We wish to learn useful representations of $X$ and that can predict the outcome $Y$, but are invariant to the enviromental variables $Z$. In agreement with \citet{yao2024unifying}, we achieve Causal Representation Learning in DERM by combining a sufficiency objective with an invariance constraint enforcing unconfoundness in the representation space. Several alternative approaches can be considered to enforce such conditional independence constraints:  (i) Conditional Mutual Information Minimization \citep{song2019learning, cheng2020club, gupta2021controllable} (ii) Adversarial Independence Regularization such as \citet{louizos2015variational} which modifies variational autoencoder (VAE) architecture in \citet{kingma2014adam} to learn \textit{fair} representations that are invariant sensitive attributes, by training against an adversary that tries to predict those variables (iii) Conditional Contrastive Learning such as \citet{ma2021conditional} whereby one learns representations invariant to certain attributes by optimizing a conditional contrastive loss (iv) Variational Information Bottleneck methods where one learns useful and sufficient representations invariant to a specific \textit{domain} \cite{alemi2016deep, li2022invariant}.

\paragraph{Causal Representation Learning} In the broader context of causal representation learning methods \citep{scholkopf2021toward}, our proposal largely focuses on representation learning applications to causal inference: learning representations of data that make it possible to estimate causal estimands. We find this is in contrast with most recent works in causal representation learning, which uniquely focused on complete identifiability of all the variables or blocks, see \citet{yao2024unifying,varici2024general,von2024identifiable} for recent overviews targeting general settings. The main exceptions are \citet{yao2024unifying,yao2024marrying}. The former leverages domain generalization regularizers to debias treatment effect estimation in ISTAnt from selection bias. However, their proposal is not sufficient to prevent confounding when no data from the target experiment is given. The latter uses multi-view causal representation learning models to model confounding for adjustment in an observational climate application. In our paper, we also discuss conditions for identification, but we focus on a specific causal estimand, as opposed to block-identifiability of causal variables. Additionally, our perspective offers clear evaluation and benchmarking potential -- even in theoretically underspecified setting: the accuracy of the causal estimate. As opposed to virtually all existing work in causal representation learning, this can be empirically tested in \textit{real world} scientific experiments.