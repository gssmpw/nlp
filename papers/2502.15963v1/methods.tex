\section{Methods}\label{sec:methods}

\noindent We use an exploratory sequential two-phased design ($\textbf{interviews} \rightarrow \textbf{focus groups}$) \citep{creswell2017designing}. We used interviews in Phase I, which gave us rich insights into software engineers' intrinsic drivers and how they influence their sense of accountability towards the quality of their code.

In Phase II, we used user enactments with targeted focus groups in both peer-led (\textbf{RQ1}) and LLM-assisted (\textbf{RQ2}) code reviews. This design allowed us to further validate the findings from Phase I and to triangulate our findings~\citep{Storey2020} using two distinct research methods and two sources of data, ensuring empirical rigor in investigating the uncharted relationship between intrinsic drivers and accountability for code quality.

Furthermore, in Phase 1, the inductive analysis of the interview data allowed us to identify the key findings. In Phase 2, we sought to validate the earlier findings using focus groups. The secondary data analysis enhanced the earlier insights by contextualizing them within a setting close to a professional context, which the interviews alone could not fully capture.


\subsection{Phase I: Interview Study}


\begin{table*}[t!]

  \begin{center}
    \footnotesize
    \caption{Interviewees' characteristics.}
    \label{tbl:population}
    \renewcommand\arraystretch{0.80}
        
    \begin{tabular}{l|p{2.5cm}|c|c|p{4cm}|c|p{1.1cm}}
      \hline
      \textbf{\#} & \textbf{Role} & \textbf{Exp.} & \textbf{Method} & \textbf{Industry sector} & \textbf{Gender} & \textbf{Country}\\
      \hline
    
        P1 & Software Engineer & 3-5 years & DevOps & Information Technology services & Male & Germany\\
        P2 & Software Engineer & 3-5 years & Hybrid & Information Technology services & Male & UK\\
        P3 & Software developer & 9-11 years & Scrum & Robotics manufacturing & Male & USA\\
        P4 & Software developer & 9-11 years & Hybrid & Information Technology services & Male & Italy\\
        P5 & Sr. software engineer & 6-8 years & Scrum & Information Technology services & Non-binary & Germany\\
        P6 & Sr. software engineer & $>$12 years & Kanban & Banking services & Male & Canada\\
        P7 & Software developer & $<$3 years & Hybrid & Information Technology services & Male & France\\
        P8 & Sr. software engineer & $>$12 years & Scrum & Information Technology services & Female & India\\
        P9 & Sr. software engineer & 6-8 years & Scrum & Information Technology services & Male & Serbia\\
        P10 & Sr. software engineer & 9-11 years & Hybrid & Global software vendor & Male & Canada\\
        P11 & Software developer & 3-5 years & Scrum & Global software vendor & Male & UK\\
        P12 & Software engineer & $<$3 years & DevOps & Information Technology services & Female & India\\
         \rowcolor{Gray} 
        P13 & Sr. software engineer & 9-11 years & Scrum & Wearables \& IoT Technology & Female & Finland\\
        \rowcolor{Gray} 
        P14 & Software developer & 3-5 years & Hybrid & Design consultancy & Female & Canada\\
        \rowcolor{Gray} 
        P15 & Tech Lead & $>$12 years & Scrum & Technology \& engineering services & Female & USA\\
        \rowcolor{Gray} 
        P16 & Software Engineer & $<$3 years & DevOps & Information Technology services & Female & Portugal\\
    
     \bottomrule
     
    \end{tabular}
   
  \end{center}
  
\end{table*}

\paragraph*{Interviewee recruitment \& selection}\label{Interviewee_recruitment} We used Prolific\footnote{\url{https://www.prolific.co/}}, a research market platform, to recruit participants for the interviews. Recruiting qualified participants on crowd-worker platforms can be challenging \cite{alami2024you}. We followed the best practices we learned in our previous research, as explicated in \citep{alami2024understanding}. We used coding tasks and critical thinking questions grounded in the participant's experiences to evaluate their skills \citep{alami2024you}.

We sent an invitation to 562 pre-screened participants to participate in a further pre-screening survey for the interviews\footnote{All study materials, including the pre-screening survey for interviewees are available in the replication package}. Based on 170 responses we received, 16 accepted to participate. We used a broad range of criteria to select our interviewees, such as country of residence, project role, experience, gender, and accountability practices within the participant team and organization. Table \ref{tbl:population} overviews the demographics. Six respondents identified as female, one as non-binary, and nine as male.


We used purposive sampling for participant selection \citep{baltes2022sampling}. This method allowed us to use our ``judgment'' (i.e., ``the researcher can exercise expert judgment'') \citep{baltes2022sampling} in determining which participants fit our selection criteria. Even though this sampling is inherently subjective \citep{baltes2022sampling}, we mitigated potential biases in our selection by employing a diverse set of criteria \citep{baltes2022sampling}, such as participants' roles, levels of experience, industry sectors, software development methods, and geographical locations. This diversity ensures that our sample reflects a diverse range of perspectives.

For example, while P1 and P16 are both early-career software engineers working in DevOps environments, they identify with different genders and are geographically located in different countries (male from Germany and female from Portugal). Similarly, P6 and P15, both highly experienced software engineers (over 12 years of experience), differ in their roles (senior software engineer vs. Tech Lead) and industry sectors (banking services vs. technology and engineering services). In our selection process, we intentionally varied the attributes of each participant in subsequent selections to promote diversity. Each time a participant was considered for selection, we sought differences in variables such as role, experience level, methods, industry sector, gender, or geographical location from those already included in the sample. This approach allowed us to systematically build a sample that captures a diverse range of perspectives and contexts.

We also sought participants with varying degrees of accountability and practices used to control it, reinforcing the depth and breadth of our sample. For example, among our interviewees (P1, P2, and P3, as listed in Tbl. \ref{tbl:population}), all identified ``code quality'' as their primary individual accountability within their respective teams. However, they reported varying levels of accountability. P1 expressed a high level of accountability, responding with ``strongly agree'' to all accountability-related questions. In contrast, P2 and P3 displayed a moderate level of accountability, responding with ``somewhat agree'' to most of the accountability questions. Below, we highlight the recruitment process:

\begin{itemize}

    \item [-] \textbf{Initial pre-screening:} The purpose of this pre-screening phase is to evaluate potential participants' skills. Although Prolific provides an array of self-reported skills accessible in the participant's profile, they are not vetted \citep{alami2024you}. To ensure high-quality sample, researchers must prescreen their own subjects \cite{chandler2017lie}. In this pre-screening activity, we used an iterative and controlled prescreening, and task-oriented questions \citep{alami2024you}. While the iterative process (we assessed 50 responses a day) allowed us close scrutiny of the answers, we developed questions to avoid merely testing theoretical understanding, but we wanted participants to apply their skills to solve specific problems or discuss complex scenarios \citep{alami2024you}. For example, in one of the pre-screening survey, we asked: ``Please describe a specific instance where you contributed to the success of a software development project. What challenges did you face, and how did your involvement impact the project's outcome?'' This approach allowed us to ensure that selected participants had practical experience and demonstrated critical thinking skills, demonstrating genuine experience and relevant to SE. All text-free questions were scrutinized manually and ChatGPT 4.0 was used for AI-generated content. At the end of this activity, we curated a total of 562 qualified participants. This activity took place in the period of August - September 2023.

    \item [-] \textbf{Additional pre-screening:} To ensure participants with varying degrees of accountability and practices used to control it, we carried out an additional pre-screening survey. In this survey, we collected data on how accountability mechanisms shape the participant's work environment, which we did not have in previous pre-screening data. This additional data reinforced depth and breadth in our sample. For example, interviewees P1, P2, and P3, as listed in Tbl. \ref{tbl:population}, all identified ``code quality'' as their primary individual accountability in their teams. However, they reported varying levels of accountability. P1 expressed a high level of accountability, responding with ``strongly agree'' to all accountability-related questions. In contrast, P2 and P3 displayed a moderate level of accountability, responding with ``somewhat agree'' to most of the accountability questions. We received 170 responses and we selected twenty participants; twelve accepted to participate (i.e., P1-P12) (the data and findings from this sample are reported in \citep{alami2024understanding}. Due to low participation of females in the earlier selection (reported in \citep{alami2024understanding}), we invited more female to take part in the study from the pre-screened sample in the earlier pre-screening activity. We successfully recruited four additional participants (i.e., P13-P16). The first selection took place in September 2023 and the second in January 2024.

    \item [-] \textbf{Interviews:} Upon the successful selection of our participants, we interviewed P1-P12 in October 2023 and P13-P16 in January 2024.

\end{itemize}

The recruitment and data analysis processes were conducted iteratively and in parallel to facilitate the monitoring of saturation (see Sect. \ref{sec:saturation}) and to determine the appropriate sample size \citep{patton2014qualitative}. Following each interview, we carried out a preliminary analysis and performed saturation checks. These checks provided us with confidence in our sample size and the depth of evidence collected thus far.

\paragraph*{Relationship to ongoing research study}\label{sec:chase} Our Phase I interviews are part of an ongoing research project into accountability. We reported on twelve participants in \citep{alami2024understanding} and examined broadly the concept of accountability in SE. In this paper, we add four further participants to the interviews (grey rows in Table \ref{tbl:population}) and re-analyze the entire dataset, from scratch, in the context of code review, using \textbf{RQ1} and \textbf{RQ2} as new analytical lenses. We expand on the way individual and team accountability influence code quality in the context of one team-level control mechanism of accountability, code reviews. In addition, we expand on our findings by including the role that AI assistance might play in influencing accountability in the context of code review. While some intrinsic drivers (only personal standards and professional reputation) emerged as themes in the previous study \citep{alami2024understanding}, they were not a main focus of the research.

In addition, the new dataset and analysis have yielded additional themes in Phase I of the study not previously reported in \citep{alami2024understanding}, namely, professional integrity and pride in code quality. Furthermore, Phase II data has also provided new and complementary insights into the earlier findings.

Methodologically, \textit{secondary analysis}---re-analyzing existing qualitative data sets to address new research questions---is a well-accepted and sound method in qualitative research \citep{heaton2008secondary,ruggiano2019conducting}. This is permitted when the dataset adds value to the new research questions and brings deeper understanding without the need to collect new data \citep{heaton2008secondary}. We recruited four additional interviewees to the original twelve, enhancing female participation in our sample. With our new analytical lens (\textbf{RQ1}), we focused our re-analysis on code review as an accountability mechanism and how intrinsic drivers ignite a sense of accountability for code quality.

\begin{table*}[th!]
\footnotesize
    \caption{Example of Interview Questions}

    \label{tab:phase1_guide}
    \renewcommand\arraystretch{1.8}
    
    \begin{tabular}{p{2.8cm}p{10.5cm}}
    \toprule
      \textbf{Interview component} & \textbf{Examples of questions}\\
      \hline
      
      \multirow{5}{2.8cm}{\textbf{Accountability mechanisms}} &  What rules, policies, and guidelines do you have in place to ensure the outcome set for the team (e.g., quality, efficiency)?\\
      
      & Are there other team or peer expectations that aren't formal like the guidelines you mentioned?\\

      & How are these expectations controlled in your team? \\

      & How about you? How do these mechanisms influence you? Do you have an example of how you felt when you were asked to be answerable?\\ \hline
      
      \multirow{4}{2.8cm}{\textbf{Outcomes sought by accountability mechanisms}} & In your experience, how have the accountability mechanisms you mentioned influenced the desired outcomes you cited?\\

      & Have you noticed any unintended consequences or challenges associated with the implementation of these mechanisms?\\

      &  Do you have an example of yourself or a colleague who hasn't met expectations? What happened?\\ \hline

      \multirow{4}{2.8cm}{\textbf{Moderating factors}} & Are there any factors or conditions that you believe can influence the effectiveness of the accountability mechanisms you cited in your team?\\
      
      &  How do team dynamics and individual characteristics (e.g., experience, role) influence the impact of accountability mechanisms?\\

      &  Can you share an example of how you felt accountable and why?\\
      
     \bottomrule
     
    \end{tabular}
  
\end{table*}

\paragraph*{Data collection} We used semi-structured interviews to collect Phase I data. We remained flexible during the interviews, aligning the predefined interview questions with the flow of the discussion.

Our interviews covered background, a general understanding of accountability in their teams, and then questions to identify how accountability mechanisms impact the desired outcomes. In line with \textbf{RQ1} objectives, we asked questions to understand how personal traits interplay with the feeling of accountability towards a team's and individual's outcomes. In line with best practices in qualitative research, we did not ask directly about intrinsic drivers to avoid desirability bias and self-perception inaccuracies \citep{stone1999science,podsakoff2003common}. Instead, we sought to identify possible moderating factors of accountability, such as experience, role hierarchy, and personal traits. Before concluding the interview, we asked how the interviewee's team implements effective accountability mechanisms. Table \ref{tab:phase1_guide} documents examples from the interview guide. A detailed and complete guide is available in the replication package (Sect. \ref{sec:replication}).

We anchored the design of the interview guide in the existing theoretical foundation drawn from work in social sciences (see Sect. \ref{sec:theory}). For instance, the theory of accountability proposes the use of either formal or informal methods to control it. To align with this perspective, we designed questions to identify formal and informal accountability practices, such as ``What rules, policies, and guidelines do you have in place to ensure the outcome of discussed (e.g., quality, efficiency)?'' and ``Are there any other expectations set by the team or your peers, but they are not necessarily formal like the guidelines you mentioned?''

We structured the guide into five sections: after the introduction to the interview, section one sought to capture data on the background of the interviewee and a general understanding of how accountability manifests in their teams. Section Two sets out to identify accountability mechanisms and the outcomes they seek to control in the interviewee's team. Section Three delves into how the discussed accountability mechanisms impact the desired outcomes. Section Four aimed to reveal any moderating factors or conditions (e.g., roles or interpersonal relations at work) that may influence the effectiveness of accountability mechanisms. Before concluding the interview, Section Five identified best practices used by the interviewee's team to implement effective accountability mechanisms.

Our interviewees were distributed geographically. We conducted the interviews using Zoom to accommodate the distributed nature of our sample. Even though ten of our participants are non-native English speakers, we did not encounter any language challenges. This is likely because our interviewees are highly educated and from countries where English is widely spoken as a second language, e.g., India, Finland, and Germany. The interviews lasted 40-60 minutes with a total of 13h40m of audio. The audio generated a total of 189 pages verbatim after transcribing. The first author conducted the interviews in October 2023 (for P1-P12) and in January 2024 (for P13-P16). We used Otter.ai,\footnote{\url{https://otter.ai/}} an online transcription tool, to transcribe the audios. We paid \textsterling30 to each interviewee for their participation.

\paragraph*{Saturation} \label{sec:saturation} To ensure that the sample size is adequate, we monitored data saturation \citep{morse2004theoretical,aldiabat2018data} throughout our iterative analysis process. During this iterative process, we continuously compared data and emerging themes, switching back and forth between data collection and analysis \citep{bowen2008naturalistic}. This process allowed us to observe when some of our themes reoccur strongly in the data. This iterative approach also allowed us to monitor and assess the sample size, i.e., determine whether more participants are required. We were able to conclude data collection with 16 participants, and at the same time, no new themes emerged from the analysis. Personal standards and reputations reached saturation at 12 interviews, pride and professional integrity at 16. The additional four interviews collected for this study accentuated intrinsic drivers and provided greater depth to the themes.

In this process, we combined iterative analysis and saturation monitoring, which allowed us to determine that 16 participants provided sufficient data to address our RQs comprehensively, as no new themes emerged beyond this point \citep{guest2006many}. Our approach aligns with best practices in qualitative research, we determined our sample size by the depth and richness of the data we collected than a pre-determined numerical thresholds \citep{guest2006many}.

As this study represents a re-analysis with new research questions, we recommenced saturation monitoring from the first interview to align with the updated analytical lens (this study's \textbf{RQ1} and \textbf{RQ2}) \citep{heaton2008secondary}. Saturation monitoring followed an iterative process, documented using a tracking spreadsheet (available in the replication package, see Sect. \ref{sec:replication}).

\paragraph*{Member checking} Upon the completion of the analysis, we conducted member checking~\citep{miles2014qualitative,birt2016member}. We sought feedback from our interviewees on our findings. We documented our themes in a separate document for each interviewee. Then, we shared the link with each interviewee in Prolific and asked them to comment and provide feedback. Twelve interviewees provided feedback, and four opted not to provide feedback. No major objections to our interpretations were reported. Some interviewees requested further clarifications prior to providing their support for the findings. We paid an additional \textsterling5 for this effort.


\subsection{Phase II: Focus groups}\label{sec:phase_2}

\afterpage{

\begin{landscape}

\renewcommand\arraystretch{1.8}

\footnotesize

    \begin{longtable}{lp{3.5cm}p{6.5cm}p{5.5cm}p{2cm}}
    \caption{Configuration of the Four Focus Groups}
     \label{tbl:focus_groups}\\
     \toprule

     \textbf{\#} & \textbf{Scenario} & \textbf{Research Objective} & \textbf{Participants} & \textbf{Main Variable} \\
        \midrule

        \multirow{6}{0.5cm}{\textbf{FG1}} & \multirow{6}{3cm}{Role hierarchy} & To explore how hierarchical dynamics (e.g., junior developers reviewing senior developers' code) influence the persistence of intrinsic drivers like pride in code quality and professional integrity. This setup assesses whether intrinsic motivations are moderated by authority bias and social influence in hierarchical settings, as participants navigate power dynamics during reviews. This scenario helped validate whether intrinsic drivers such a personal standards are upheld in the face of authority. & Two junior and one mid-career developer as reviewers, and three code authors (two senior developers, and a team lead) & Role hierarchy.\\
        
        \hline
        
        \multirow{6}{0.5cm}{\textbf{FG2}} & \multirow{6}{3cm}{Public reviews} & Impact of publicly available review comments on accountability. The setup used public visibility in simulated review platforms to explore whether participants adapted their feedback style and accountability practices. This scenario aligns with prior findings that public feedback increases performance due to visibility but may compromise candidness and individual accountability due to fear of judgment and retaliation. By observing participants' behaviors and reflections on providing feedback in this context, the study captured whether their intrinsic drivers, such as professional reputation, were heightened or compromised in public review settings. & 3 developers (mixed roles seniority) as reviewers, 3 developers (mixed roles seniority) as code authors & Public review. \\
        
        \hline
        
        \multirow{6}{0.5cm}{\textbf{FG3}} & \multirow{6}{3cm}{Cross-team review} & The focus group design introduced complex interdependencies between participants assigned to different ``teams'' contributing to the same codebase. This cross-team reviews was informed by complexity theory, which suggests that accountability becomes diffused in such settings. Observing how participants balanced individual accountability with the need for collective ownership helped validate whether intrinsic drivers persisted or were overridden by external pressures in collaborative settings. & Mixed team roles (3 senior developers, and 2 mid-career developers) as reviewers and authors & Cross-team reviews. \\
        
        \hline
        
        \multirow{6}{0.5cm}{\textbf{FG4}} & \multirow{6}{3cm}{Complex code module review with varying quality levels} & To investigate the persistence of intrinsic drivers like personal standards and professional integrity in reviewing technically complex and variable-quality code. This scenario tests how participants' accountability adapts to cognitively demanding and high-pressure review conditions. For example, when faced with complex code and high cognitive effort, both individual and collective accountability may reduce under high cognitive demand. & Three developers (mixed roles) as reviewers, three developers (mixed roles) as code authors & Code complexity. \\
        
        \bottomrule
    
    \end{longtable}

\end{landscape}

}

In Phase II we conducted four focus groups with five to six participants in each group (see Tbl. \ref{tbl:focus_groups_population}). This method complements and elaborates Phase I findings in a more interactive and collaborative environment. 

Self-reported data is subject to potential biases, such as social desirability bias, recall bias, and self-perception inaccuracies, resulting in over- or under-reporting of behaviors and attitudes \citep{podsakoff2003common}. For example, participants might unintentionally or intentionally report data and events in a more favorable light, either to themselves or in a way that aligns with what they perceive the researcher favors \citep{stone1999science,podsakoff2003common}. Such behavior may impact the reliability of the data. In addition, self-reported data often fail to capture the complexity and dynamics of group interactions and decision-making processes, which we have observed in this study. For example, while in Phase I we learned that individual accountability is the main driver for pursuing higher code quality, Phase II focus groups revealed that the collaborative aspects and peers' influence during code review shift the sense of accountability from an individual-level to a collective-level. This intricate detail brought a more comprehensive understanding of how accountability may take different levels depending on the activity within the social and collaborative contexts of code review.

By incorporating focus groups in Phase II, we managed to capture the social and collaborative aspects of code review processes and observe participants' behaviors and interactions in real-time, providing richer and more nuanced data. This methodological triangulation \citep{perlesz2003methodological} enhances the validity and reliability of our findings. 

However, we acknowledge that focus groups, as a method, also carry inherent biases, such as social pressure and groupthink, which may influence participants' responses and participation \citep{o2018use}. We employed several techniques to mitigate these biases. The moderated focus groups ensured inclusive contributions from all participants. The discussions used open-ended questions to prompt participants to share their viewpoints, reducing the likelihood of convergence toward a single opinion. At the end of each focus group, all participants were encouraged to provide individual reflections to capture perspectives uninfluenced by group dynamics. Additionally, we employed participant-authored code to lessen the artificiality often associated with focus group settings. This approach facilitated the observation of genuine interactions and decision-making processes, as participants engaged more naturally with their own code. Best practices in focus group and simulation studies highlight the importance of using contextually relevant and participant-generated materials to enhance ecological validity and minimize biases inherent in simulated settings \citep{gaba2004future}.

We organized our two-hour focus groups into two parts: the first hour was dedicated to peer-led reviews (\textbf{RQ1}) and the second hour to LLM-led reviews (\textbf{RQ2}).

\subsubsection{Peer-led reviews}

We designed the focus groups to enable the ``enactment'' of our participants. In user enactments, researchers design a physical and social context to simulate a future situation \citep{odom2012fieldwork}. Users are asked to enact loose scenarios of situations familiar to them \citep{odom2012fieldwork}. This design allows the researcher to observe and probe participants, grounding the discussion in a context similar to the participants' professional experiences \citep{odom2012fieldwork} and enhancing the authenticity of qualitative data collected \citep{carroll2003making}.

\paragraph*{Configuration} We designed four different configurations for our focus groups' scenarios, as illustrated in Tbl. \ref{tbl:focus_groups}. Table \ref{tbl:variables_description} documents the variables we used to configure each focus group. By incorporating a diverse range of variables, such as role hierarchy, review visibility, urgency, code complexity, and team collaboration, we aimed to test software engineers' intrinsic drivers influencing their sense of accountability for code quality in settings that resonate with their real-life circumstances. For example, while the first scenario focuses on evaluating the \textit{resilience} of reported intrinsic drivers like pride in code quality and professional integrity when junior developers review their senior counterparts' code, the third scenario does the same in a \textit{cross-team review} setting.

We designed the first focus group, \textbf{FG1}, to test the persistence of intrinsic drivers, such as maintaining personal standards for code quality, in a hierarchical setting. Several studies in SE reported the impact of role seniority on team's dynamics \citep{cunha2021code,sadowski2018modern,bacchelli2013expectations}. In summary, these studies report that senior developers influence the code review process positively by providing mentorship, maintaining high standards, and enriching the learning experience of juniors \citep{cunha2021code,sadowski2018modern,bacchelli2013expectations}. However, authority bias theory suggests that employees defer decisions to those perceived in higher positions or more knowledgeable, reducing their sense of individual accountability \citep{kipnis1972does,milgram1963behavioral}. Similarly, social influence theory posits that individuals are more likely to conform to the opinions and behaviors of those they perceive as more knowledgeable or authoritative \citep{cialdini2004social,asch2016effects}. In a code review context, junior developers might align their actions and decisions with those of senior developers, thereby reducing their sense of individual accountability for the code they are asked to review or have authored.

The aim of \textbf{FG2} is to examine the influence of how public feedback settings impact the sense of accountability for code quality among team members. Impression management theory posits that in a public setting, individuals may attempt to control their behaviors to influence impressions others form of them \citep{bolino2016impression}. Such behavior can lead to appearing in conformance to social norms \citep{bolino2016impression}. In the context of code review, public feedback may incentivize individuals to perform well due to broader visibility, which may heighten accountability. Fear of negative evaluation can also influence behaviors when feedback is shared publicly \citep{watson1969measurement}. Developers may avoid conveying negative comments to peers to mitigate the risk of damaging relationships or future retaliations \citep{kocovski2000social}, which may result in compromised levels of accountability for code quality. These theoretical perspectives have been echoed in SE literature. For example, Rigby et al. report that public peer reviews enhance performance due to the social facilitation effects \citep{rigby2013convergent}. Bosu et al. suggest that developers align their comments with the group's norms and expectations \citep{bosu2015characteristics}, indicative of impression management behaviors in code review.

\begin{table}[t!]

    \caption{Variables Used to Configure Focus Group Scenarios}
    \label{tbl:variables_description}
    \centering
    
    \begin{tabular}{rp{10.5cm}}
    
        \toprule
        \textbf{Variable} & \textbf{Description and Rationale} \\
        
        \midrule
        Role hierarchy & Refers to the structured levels of authority, seniority, and responsibility within the focus group participants.\\

        Public reviews & Code reviews conducted in a open tool with broader visibility, e.g., review comments made available on GitHub or Bitbucket for for other than the group members to consult. \\

        Cross-team reviews & This variable reflects the need to review cross teams when a codebase involves multiple teams collaboration. \\

        Code complexity & Presents the technical challenge presented by the code under review. We used this variable to to present code with varying level of complexity in our scenarios. \\
        
        \bottomrule
        
    \vspace{-0.5cm}
    
    \end{tabular}

\end{table}

\textbf{FG3} reflects the need to review cross teams when a codebase involves multiple teams collaboration. In a cross-team reviews, dealing with complex codebases can often involve critical interdependencies, thereby heightening accountability. Complexity theory suggests that complex system often exhibit nonlinearity, meaning that the effect is not always proportionate to a cause \citep{byrne2002complexity,larsen2013complexity}. In a nonlinear system, a trivial change in one variable can cause significant implications downstream \citep{byrne2002complexity,larsen2013complexity}. This uncertainty may require a clear and higher accountability to ensure that all aspects of the code are thoroughly reviewed before integration. In complex systems, accountability is also difficult to
pinpoint \citep{kacianka2021designing}. For example, a ``pilot error'' is not simply a failure of the pilot, but rather a complex interplay of the humans and the technical systems \citep{kacianka2021designing}. To test the manifestation and the persistence of Phase I findings of accountability for code quality in a complex setting, we assigned \textbf{FG3}' participants to three different groups with defined interdependencies in the code subject to review (see our focus group scenario designs in the replication package, Sect. \ref{sec:replication}, for further details).

While \textbf{FG3} is to test accountability when the system is socially complex, multiple teams managing the same codebase with interdependencie, \textbf{FG4} aims to explore accountability in technically complex scenarios within the same team. Transactive memory systems (TMS) theory suggests that in such environments, the division of cognitive labor and shared knowledge among team members are crucial for effective coordination and accountability \citep{wegner1987transactive}. To evaluate Phase I findings regarding accountability in the face of technical complexity, we presented \textbf{FG4} participants with complex code snippets of varying quality. This allowed us to test how well our participants maintain accountability under more technically challenging conditions.

To mimic real-world conditions and capture discussions in relatable SE settings, we designed each focus group scenario to reflect typical challenges faced by software engineers in professional settings. For instance, in \textbf{FG1}, the hierarchical setup of juniors reviewing seniors' code was structured to replicate dynamics where authority and experience levels intersect, creating opportunities to observe potential authority bias. \textbf{FG2} aims to test whether accountability intensifies when reviews are visible to broader audiences outside the team. Similarly, in \textbf{FG3}, the cross-team review configuration was designed to simulate the interdependencies often seen in large-scale collaborative projects, where multiple teams contribute to a shared codebase. \textbf{FG4} intended to also evaluate whether accountability shifts when complexity is higher. Existing literature also informed these scenarios.

Admittedly, our scenarios do not represent every possible SE condition, nor does their design capture the true complexity of real-life circumstances. However, we selected and designed these scenarios based on potential changes in the social setting introduced by the variables (e.g., hierarchy, and public reviews), which may compromise the individual and collective levels of accountability. Our choices are also grounded in theoretical perspectives. We further discuss this threat to validity in Sect. \ref{sec:trust}.

\paragraph*{Code generation} We opted for Python to write code snippets subject to reviews in the focus groups. The choice of Python is rationalized by its widespread acceptance and use in the software development community. Python's readability and straightforward \citep{van1995python} syntax make it an ideal choice to illustrate potential quality issues in a review setting. This choice also facilitates better focus on quality issues rather than the intricacies of the programming language \citep{dagenais2011recommending}, which may distract the focus of the discussions. Dagenais and Robillard, and Baxter and Sommerville studies suggest that using familiar programming languages and frameworks facilitates better understanding and focus in the scope of software engineering studies \citep{dagenais2011recommending,baxter2011socio}.

For the code snippets, we aimed for realistic scenarios. Scenario-based design literature recommends maintaining realistic scenarios in qualitative research \citep{carroll2003making}. This approach does not only mitigate the risk of drifting away from the actual scope of the study but also ensures that the data collected facilitates a shared understanding with the participants \citep{carroll2003making}. This approach was echoed in nursing education literature \citep{jeffries2005framework,shin2015effectiveness,cant2010simulation}. Realistic simulation scenarios are used to help students bridge the gap between theoretical knowledge and practical skills \citep{jeffries2005framework,shin2015effectiveness,cant2010simulation}. Realistic scenarios also improve the ecological validity of the design and its setting, meaning that the data collected is more relatable \citep{gaba2004future}. Table \ref{tbl:code_snippets} summarizes the code snippets for all our focus groups; further details are available in the replication package (see Sect. \ref{sec:replication}). The ''Ref.'' column lists the snippet file names as they appear in our replication package. All snippets were written by the first author and reviewed by the third author prior to the focus groups taking place.


\afterpage{

\begin{landscape}

\renewcommand\arraystretch{1.0}

\footnotesize

    \begin{longtable}{lp{3cm}p{7.5cm}p{6.2cm}}
    \caption{Python Code Snippets Used in Focus Groups}
    \label{tbl:code_snippets}\\
     \toprule

        \textbf{Focus Group} & \textbf{Ref.} & \textbf{Python Code Snippets} & \textbf{Main Quality Issues} \\
        
        \midrule

         \multirow{4}{2.5cm}{\textbf{FG1}- Role hierarchy} & FG1\_Scenario\_1\_Snippet\_1 & \multirow{2}{7.5cm}{This code snippet processes a list of data, doubling the value of integers.} & 1. Inefficient handling of different data types.\\

         &  &  & 2. Lack of type hinting and docstrings.\\

        &  &  & 3. No exception handling for unexpected data types.\\
        
        \cline{3-4}
        
         & FG1\_Scenario\_1\_Snippet\_2 & \multirow{2}{7.5cm}{This code snippet retrieves user data from an API using the requests library.} & 1. Limited error handling; only catches RequestException.\\

         &  &  & 2. Hard coded URL without parameter validation.\\
        
        \hline
        
        \multirow{8}{2.5cm}{\textbf{FG2}- Public reviews} & \multirow{2}{2.5cm}{FG2\_Scenario\_2\_Snippet\_1} & \multirow{2}{8.5cm}{This code snippet extracts the file extension from a filename.} & 1. Does not handle filenames without extensions.\\

         &  &  & 2. No validation for input types.\\
        
        \cline{3-4}
        
         & \multirow{4}{2.5cm}{FG2\_Scenario\_2\_Snippet\_2} & \multirow{4}{8.5cm}{Authenticates a user against a user database.} & 1. Passwords are stored and checked in plain text, which is insecure.\\

         &  &  & 2. Should use hashed passwords and secure comparison methods.\\

         &  &  & 3. No exception handling for dictionary key errors.\\

         \cline{3-4}

        & \multirow{3}{2.5cm}{FG2\_Scenario\_2\_Snippet\_3} & \multirow{3}{8.5cm}{Aggregates data records by category.} & 1. Potential for key errors if category or data fields are missing in any record.\\

         &  &  & 2. Could validate input data before processing.\\

         &  &  & 3. Inefficient handling of large datasets.\\
        
        \hline
        
        \multirow{9}{2.5cm}{\textbf{FG3}- Cross-team reviews} & \multirow{2}{2.5cm}{FG3\_Scenario\_3\_Snippet\_1} & \multirow{2}{8.5cm}{Executes a database query and returns the results.} & 1. No error handling for connection establishment failures.\\

         &  &  & 2. Could have handled specific database errors more robustly.\\
        
        \cline{3-4}
        
         &  \multirow{2}{2.5cm}{FG3\_Scenario\_3\_Snippet\_2} & \multirow{2}{8.5cm}{Asynchronously processes data from a queue.} & 1. Exception handling is minimal; could be improved to handle specific errors.\\

         &  &  & 2. Should ensure that data queue operations are safe and handle timeouts.\\

         \cline{3-4}

        &  \multirow{2}{2.5cm}{FG3\_Scenario\_3\_Snippet\_3} & \multirow{2}{8.5cm}{Predicts an outcome using a machine learning model.} & 1. Retraining the model within the prediction function is inefficient.\\

         &  &  & 2. Should handle model errors separately and more efficiently.\\
        
        \hline

        \multirow{4}{2.5cm}{\textbf{FG4}- Complex code} & \multirow{2}{3cm}{FG4\_Scenario\_4\_Snippet\_1} & \multirow{2}{8.5cm}{This snippet processes data from a file asynchronously.} & 1. No validation for file content structure.\\

        &  &  & 2. Lack of proper error handling for file operations.\\
        
        \cline{3-4}
        
        &  \multirow{2}{3cm}{FG4\_Scenario\_4\_Snippet\_2} & \multirow{2}{8.5cm}{This snippet extracts links from a webpage using BeautifulSoup.} & 1. No validation for input URL.\\

        &  &  & 2. Lack of error handling for HTTP and parsing errors.\\

        
        \bottomrule
    
    \end{longtable}

\end{landscape}

}

\subsubsection{LLM-assisted reviews} We asked the participants in the focus groups to volunteer to submit Python source code that they had authored. We asked for professionally authored code or accepted and merged contributions to open-source projects. We used ChatGPT 4 for the LLM reviews. We used this prompt to generate a ChatGPT review: ``You are a Python software developer expert. Conduct a code review of the attached code and provide thorough feedback to the author of the code.'' Eighteen participants submitted their Python code, and five opted not to; still, they contributed to the discussion part of the LLM review. The five participants chose not to submit code, citing concerns of making it publicly available, given it was authored in proprietary contexts.

\subsubsection{Participant recruitment and selection} We used UpWork\footnote{\url{https://www.upwork.com/}}, a marketplace for freelancing work. Although we used Prolific successfully in Phase I, the effort of pre-screening and qualifying candidates was lengthy. In UpWork, by comparison, potential participants disclose their GitHub and LinkedIn profiles as part of the recruitment process to qualify for the study, which has allowed for fast and accurate vetting. 

We posted a job description for the study (available in the replication package). We asked prospects to submit their GitHub and LinkedIn profile links or a copy of their resumes to help us qualify their skills. We received 56 submissions. Based on the requirements we set in the design of our focus groups, we selected 24 participants, but only 23 participated. We used similar requirements as per Phase I. We aimed for a diverse sample while aligning with our focus group design. For instance, for FG1, we sought participants with varied seniority levels as per the scenario design. We also examined potential participants' profiles to ensure recent hands-on experience in code review and proficiency in Python as evidenced by GitHub contributions or their resumes.

Table \ref{tbl:focus_groups_population} documents the characteristics of the focus group participants, the corresponding focus group they took part in, and the roles they assumed in the scenario (e.g., junior developer, team lead, etc.). For example, P17 assumed the role of a junior developer in the first focus group (see Tbl. \ref{tbl:focus_groups}). We were only able to recruit one female participant (P26). We paid each participant \$60 for their participation in the 2-hour focus group.

We also aimed to match the participants' career and experience levels with their roles in the focus groups. For example, P17 played the role of a junior developer in FG1 and also worked as a junior software engineer at the time of recruitment. Participants are more likely to act naturally within familiar professional contexts when asked to engage in a setting that closely mirrors their professional collaborative settings \citep{jeffries2005framework,gaba2004future}.

\begin{table*}[th!]
\footnotesize
    \caption{Example of Focus Group Interview Questions}

    \label{tab:phase2_guide}
    \renewcommand\arraystretch{1.8}
    
    \begin{tabular}{p{2.8cm}p{10.5cm}}
    \toprule
      \textbf{Discussion component} & \textbf{Examples of questions}\\
      \hline
      
      \multirow{4}{2.8cm}{\textbf{Peer-led review (FG1)}} &  What thoughts or considerations were in your mind while reviewing the code?\\

      & Can you share your thoughts or feelings during the junior developers' review?\\

      & Were there moments in the review that stood out to you? Why?\\

      & How do you typically react to feedback in a similar hierarchical team setting?\\ \hline
      
      \multirow{4}{2.8cm}{\textbf{LLM-assisted review}} & What was your initial reaction to the LLM's feedback on your code?\\

      & How does the idea of having an LLM review your code impact your approach to maintaining high personal standards?\\

      & Do you believe that LLM-based reviews can accurately reflect and potentially influence your professional reputation among peers? Why or why not?\\ 
      
     \bottomrule
     
    \end{tabular}
  
\end{table*}

\afterpage{

\begin{landscape}

\renewcommand\arraystretch{1.5}

\footnotesize

    \begin{longtable}{clp{2.5cm}lllp{3.1cm}l}
    \caption{Participants Characteristics and Their Distribution Across Focus Groups}
     \label{tbl:focus_groups_population}\\
     \toprule
    

         \textbf{Group} & \textbf{ID} & \textbf{Role} & \textbf{Experience} & \textbf{Industry} & \textbf{Country} & \textbf{Role in the Focus Groups} & \textbf{Review/Author}\\
         \hline

         \multirow{6}{*}{FG1} & P17 & Jr. Software Engineer & 3-5 years & Mobile Apps development & Hungary & Junior Developer \#1 & Reviewer\\ 
                              & P18 & Lead Java Developer & $>$12 years & Information Technology services & USA & Team Lead & Author\\ 
                              & P19 & Sr. Software Engineer & 9-11 years & Financial services & Bulgaria & Senior Developer \#1 & Author\\ 
                              & P20 & Software Engineer & 5-9 years & Technology startup & Germany & Developer & Reviewer\\ 
                              & P21 & Jr. Software Engineer & 3-5 years & Global software vendor & UK & Junior Developer \#2 & Reviewer\\ 
                              & P22 & Tech Lead & $>$12 years & Motor vehicle manufacturing & Poland & Senior Developer \#2 & Author\\  \hline
         \multirow{6}{*}{FG2} & P23 & Software Engineer & 3-5 years & Information Technology services & Dominican Republic & Junior Developer \#1 & Author\\ 
                              & P24 & Software Engineer & 3-5 years & Information Technology services & India & Junior Developer \#2 & Reviewer\\ 
                              & P25 (Feedback)& Sr. Software Engineer & $>$12 years & IT Services and Consulting & Canada & Senior Developer \#1 & Reviewer\\ 
                              & P26 (Feedback)& Sr. Software Engineer & 9-11 years & Information Technology services & India & Senior Developer \#2 & Author\\ 
                              & P27 (Feedback)& Sr. Software Engineer & $>$12 years & Technology startup & US & Senior Developer \#3 & Author\\ 
                              & P28 (Feedback)& Tech Lead & $>$12 years & Broadcast  \& media production & US & Senior Developer \#4 & Reviewer\\  \hline
         \multirow{6}{*}{FG3} & P29 & Software Engineer & 9-11 years & Medical equipment manufacturing & Hungary & Developer from Team A & Reviewer\\ 
                              & P30 (Feedback)& Software Engineer & 9-11 years & eCommerce software development & UK & Developer from Team B & Author\\ 
                              & P31 & Tech Lead & $>$12 years & Higher education & Lithuania & Tech Lead from Team A & Author\\ 
                              & P32 & Sr. Software Engineer & $>$12 years & IT Services and Consulting & Serbia & Sr. Developer from Team A & Author\\ 
                              & P33 & Sr. Software Engineer & $>$12 years & Software development services & India & Sr. Developer from Team C & Reviewer\\ \hline
         \multirow{6}{*}{FG4} & P34 & Sr. Software Engineer & $>$12 years & Public policy advisory & UK & Senior Developer \#1 & Author\\ 
                              & P35 & Software Engineer & 9-11 years & Global software vendor & UK & Developer \#1 & Reviewer\\ 
                              & P36 (Feedback)& Software Engineer & 6-8 years & IT Services and Consulting & France & Developer \#2 & Reviewer\\ 
                              & P37 & Sr. Software Engineer & $>$12 years & Software development service & Brazil & Senior Developer \#2 & Author\\ 
                              & P38 & Sr. Software Engineer & $>$12 years & Information Technology services & Serbia & Senior Developer \#3 & Author\\ 
                              & P39 & Sr. Software Engineer & $>$12 years & Software development service & Canada & Senior Developer \#4 & Reviewer\\ 
                              \bottomrule
       

    \end{longtable}

\end{landscape}

}


\subsubsection{Data collection} We designed our scenarios for groups of six participants. Prior to each focus group taking place, we sent detailed instructions to each participant describing the focus group process and the code they had to review (the instructions sent to the participants are shared in the replication package). 

To ensure that the code under review would be familiar and accessible to all participants, we asked them to answer a pre-screening question related to their competence in programming with Python (see UpWork job ad in the replication package).

All focus groups were conducted online using Zoom. One participant did not show up to \textbf{FG3}, yet we managed to conduct that group with minimal impact on its purpose. All focus groups were audio recorded and transcribed using Otter.ai. The first author facilitated the focus groups in February 2024. The focus group's audios generated between 24-27 pages of verbatim each after transcription, presenting approximately a total of seven hours of audio recordings.

We deemed four groups with varying scenarios sufficient to cover a spectrum of settings. We prepared a discussion guide for each focus group to structure the dialogue; however, we remained flexible during the facilitation and prompted the participants with follow-up questions when necessary. Table \ref{tab:phase2_guide} documents an example of questions used to guide the discussion. A detailed and complete guide is available in the replication package (Sect. \ref{sec:replication}).

We used a pre-focus group questionnaire to collect data on participants' intrinsic drivers, i.e., pride, integrity, reputation, and upholding personal standards, and how they influence their accountability for code quality. This methodological choice was strategically implemented to mitigate the risk of social desirability bias \citep{furnham1986response} and self-censorship \citep{yanos2008false} within the focus group discussions. We used the collected data as a reference point to prompt participants during the focus groups, encouraging them to share their true and authentic thoughts. For example, while most participants in the pre-focus group questionnaire reported pride in code quality as a key driver for their accountability to meet quality expectations, the focus group discussion hinted that their pride becomes less relevant in a group setting. When the researcher prompted the participant to explain the misalignment between what they previously reported, we learned that pride is tuned down to foster group consensus. This method is aligned with best practices in qualitative research, which suggest using pre-collected data and previously expressed views to reduce social desirability bias by encouraging honest and reflective responses \citep{king2000social,tourangeau2007sensitive}.

When the participants' discussions diverged from the pre-reported answers, we asked them to provide an explanation. For instance, when some participants' inputs during the focus group discussion did not align, the moderator prompted them to clarify. This was more prominent for ``pride in code quality'' and ``professional integrity.'' Then, we learned from the participants' explanation that their intrinsic drivers are regulated to accommodate the collective accountability and consensus.

\paragraph*{Peer-led review} We assigned the roles to the participants and asked them to prepare their reviews prior to the focus groups. The first hour of the session was used to share and discuss the feedback. Then, the researcher prompted the participants to elaborate and discuss how they felt accountable for the quality of the code. We used a pre-defined discussion guide across all sessions, which is available in the replication package. However, we allowed for some fluidity in the discussion to facilitate a natural conversation, where participants could express their thoughts freely. This approach is aligned with qualitative research best practices, which recommend a balance between structured guidance and flexible exploration \citep{patton2014qualitative}.

\paragraph*{LLM review} In the second part of the focus groups, we shared the reviews generated by ChatGPT with the participants, then asked them to reflect on the comments. Similarly to the first part, the researcher guided the discussion using a pre-defined guide for this part of the session.

\subsubsection{Feedback session} After the analysis of Phase II data was completed, we conducted a feedback session on our findings~\citep{alami2022scrum} with six participants from Phase II, annotated with asterisks in Tbl. \ref{tbl:focus_groups_population}. The purpose of this session is to collect feedback on the findings and ensure they resonate with our participants \citep{birt2016member}. During the session, we presented the findings in a scenario-like manner to ease comprehension and relatability to the participants. Our findings received support from the participants, and we did not deem revising our analysis necessary. The transcript of the session and the scenarios used are available in the replication package. The session was carried out by the first author on the first week of March 2024.

\subsection{Data Analysis and Integration} 

\begin{table*}[th!]
\footnotesize
    \caption{Example of \textbf{RQ1} Pattern Codes}

    \label{tab:themes}
    \renewcommand\arraystretch{1.8}
    
    \begin{tabular}{p{3.2cm}p{2.8cm}p{6.5cm}}
    \toprule
      \textbf{Pattern codes} & \textbf{First Cycle codes} & \textbf{Examples from the data}\\
      \hline
      
      \multirow{8}{*}{\textbf{Code quality}} & Code quality  & \emph{``So I have to write good quality code, good code, the good code, which is working and my teammates are happy with''} (P11).\\
      
      & Code maintainability & \emph{``So talking about that if I am accountable to the code quality, because suppose some other developer takes over me, or maybe someone has to extend the functionality, it should be very quick. And like it should be in a modular fashion. So, that is why we focus on code quality ...''} (P12). \\
      
      & Code readability  & \emph{``... sometimes if my code is reviewed by my senior engineer, sometimes even if the rule is not followed, but if code is readable. And if there is no very big, silly mistakes, you can say they pass the code reviews.''} (P11). \\
      \hline
      
      \multirow{2}{4.1cm}{\textbf{Individual accountability}} & Accountable for code  & \emph{``... you have to be accountable for your code ...'' P(6).} \\
      
      & Accountability to self  & \emph{``You know, just accountability towards myself''} P(7).  \\ \hline

      \multirow{6}{*}{\textbf{Intrinsic drivers}} &  Professional integrity & \emph{`` ... Integrity as in a general sense ... I think [it's s] core value for me not doing something the wrong way''} (P7). \\
      
      & Professional reputation & \emph{``If I'm working somewhere, I think that I should have a good image that is a good employee is getting good results is writing good quality code'' (P11).}  \\
      
      & Pride in code quality & \emph{``I believe the primary motivation for achieving high-quality code comes from within. It's about the personal satisfaction and pride I feel in my work'' (P14).}  \\
      
     \bottomrule
     
    \end{tabular}
  
\end{table*}

We used the same process to analyze both interview and focus group datasets, first analyzing interviews, then focus groups as per our two-phased design. Our analysis employed \textbf{inductive thematic analysis}, following the guidelines from Miles et al.~\citep{miles2014qualitative} and Salda{\~n}a~\citep{saldana2021coding} to analyze the interview data. The guidelines recommend two phases: (1) \textit{First Cycle} and (2) \textit{Second Cycle} \citep{miles2014qualitative,saldana2021coding}. The iterative approach of these guidelines allowed us to move between the data and emerging codes, which facilitated better monitoring of saturation.

\paragraph*{First Cycle} In this phase of the coding, data ``chunks'' or segments are assigned labels or codes that represent their meanings. We used an inductive coding approach to derive codes directly from the data without imposing any preconceived notions. This approach allowed us to gain data-driven insights contextualized to SE.

The first author led the first phase of coding and induced a preliminary set of codes. The initial list of codes was then reviewed by the second and third authors, who provided feedback, proposed new codes, and suggested modifications to existing labels. Following this review, the first author integrated the feedback and proposed a final list of codes. This collaborative and iterative process allowed us to refine our initial coding efforts and also ensured a robust and consistent coding scheme. This enhances the credibility and analytical process of our conclusions \citep{miles2014qualitative}.

\paragraph*{Second Cycle} In this second phase of coding, we evolved the detailed list of the First Cycle codes into a consolidated thematic structure \citep{saldana2021coding}, known as Pattern Codes~\citep{miles2014qualitative}. This condensing exercise is based on codes that share themes, patterns, or logical characteristics. The first author led this phase, with subsequent reviews and input from the second and third authors to ensure a unified perspective and reach consensus. Table \ref{tab:themes} documents examples of some Pattern Codes and their corresponding \textit{First Cycle} code and quotes from the data.

To mitigate the risk of social desirability bias (responses favored by the researchers and other participants) \citep{furnham1986response}, we explicitly did not use Phase I Pattern Codes in the design of the discussion guides for Phase II (focus groups). During focus group discussions, without being led towards specific responses, participants were encouraged to reflect on how their personal standards, pride, reputation, and professional integrity affect their approach to code review and feedback, based on their responses in a pre-focus group questionnaire (see Sect. \ref{sec:phase_2}).

\begin{figure*}[!t]
    \includegraphics*[trim=1cm 1.5cm 1cm 1cm, clip, width=1.0\textwidth]{research_process}
        \caption{A summary of our research process.}
        \label{fig:research_process}
        \Description[]{Research process flowchart. We begin with the earlier study, then conduct 4 more interviews, re-analyze all the data, design a focus group, analyze the focus groups, and then get feedback from our respondents.}
\end{figure*} 

\paragraph*{Methodological Triangulation}

The use of two methods across sequential phases allowed us to use methodological triangulation \citep{perlesz2003methodological}. By comparing and integrating insights across methods, we refined and contextualized themes, leading to a more informed interpretation. This comparative process enabled us to build a more comprehensive understanding of our themes and the processes we identified. For instance, in Phase I interviews, participants identified pride as an intrinsic driver of accountability, particularly when writing code. However, Phase II focus group discussions revealed a self-regulatory process during collaborative contexts. We learned from Phase II data that individual pride becomes less pronounced to align with collective accountability.

\paragraph*{Integration of Phases I and II} We merged the findings of Phases I and II after Phase II was completed (including data analysis) and during the writing of this manuscript. This part of the investigation is referred to as ``interpretation of the related outcomes'' \citep{creswell2017designing}. Section \ref{sec:findings} presents the results and discusses how far Phase II findings corroborate earlier results in Phase I. We also explore how the outcomes of both stages sync and complement one another.

This integration process entails juxtaposing the findings from Phase I with those from Phase II to identify consistencies, discrepancies, and complementary and new insights. However, in the case of our study, we find mostly complementary and new insights. For example, self-regulation, a new pattern code that has emerged in this exercise, shows that software engineers regulate or tune down some of their intrinsic drivers to accommodate collective consensus and shared accountability. The data collected in the LLM-led reviews also helped us to understand new insights on the impact of AI on accountability.

During this process, we employed a comparative method to analyze how the themes and patterns identified in Phase I were supported, expanded, or challenged by the data from Phase II and whether some of these insights changed the findings. For example, while collective accountability has emerged as findings in Phase I analysis, we only learned that it transpires once the code becomes subject to collective review. This method aligns with Creswell et al. recommendations for mixed-methods research \citep{creswell2017designing}, which emphasize the importance of integrating qualitative data analysis to enhance the depth and breadth of understanding of a particular phenomenon.

To recap this section, figure \ref{fig:research_process} illustrates our research process. We carried out this study in two phases: an interview-based investigation (N = 16) and a focus group study. For Phase I, to enhance the diversity of our sample, we conducted four additional interviews with female participants. The data from these interviews were re-analyzed with a refined focus on \textbf{RQ1}. In Phase II, we designed and conducted focus groups of code reviews. The data from these focus groups were analyzed, emphasizing \textbf{RQ2}. Finally, we conducted a feedback session with six participants from Phase II, which provided further validation of our findings.

\paragraph*{Informed consent} Informed consents from the interviewees and the focus groups' participants were obtained prior to them taking place in accordance with best practices and institutional requirements of the authors' institutions. 

\subsection{Replication package} \label{sec:replication} We share our data and other artifacts at \href{https://doi.org/10.5281/zenodo.14601149}{link.}\footnote{\url{https://doi.org/10.5281/zenodo.14601149}} Interviewees consented to sharing anonymized interview and focus group transcripts.
