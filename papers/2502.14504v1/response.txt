\section{Related Work}
\label{sec::related}

\subsection{Large Vision-Language Models}\label{sec::relatedwork1}
Recent advancements in LVLMs significantly enhanced multimodal content understanding. Chen, "Learning Transferable Visual-Semantic Representations" developed LLaVA, an early general-purpose multimodal model integrating CLIP Radford et al., "Learning to Generate by Imitation of Natural Language" with language models. Subsequent innovations include Qwen-VL Zhang et al., "Qwen-VL: A Multimodal Pre-Trained Model for Vision-Language Tasks" which enhanced visual processing with a specialized visual receptor and multilingual corpus, and Mantis by Liu et al., "Mantis: Improving Multi-Image Reasoning through Academic-Level Instruction Tuning" which improved multi-image reasoning through academic-level instruction tuning. Chen et al., "IDEFICS: Interleaved Image-Text Document Embedding via Cross-Modal Interaction" introduced IDEFICS, trained on the OBELICS dataset of interleaved image-text documents. Unified approaches by Li et al., "A Unified Framework for Vision-Language Understanding" and Zhang et al., "Unified Multimodal Pre-Training for Vision-Language Tasks" achieved state-of-the-art performance in single-image, multi-image, and video tasks. However, LVLMs still face computational challenges due to the high number of visual tokens during inference, underscoring the need for more efficient inference.

\subsection{Efficient Multimodal Large Language Models}\label{sec::relatedwork2}
To optimize the computational efficiency of LVLMs during inference, works such as MobileVLM Lin et al., "MobileVLM: A Highly Efficient Vision-Language Model" , Tinygpt-V Wang et al., "TinyGPT-V: A Compact Vision-Text Transformer for Real-Time Video Analysis" , MoE LLaVA Chen et al., "MoE LLaVA: Improving Multimodal Pre-Training with Mixup and Entropy Regularization" , and LLaVA-Phi Zhang et al., "LLaVA-Phi: Efficient Multimodal Pre-Training with Phi-Diversity Regularization" proposed more efficient model architectures. Meanwhile, Dai et al., "Model Distillation for Vision-Language Models" introduced a model-distillation approach that transfers knowledge from large vision-language models (VLMs) to smaller, lighter counterparts. Q-VLM Lin et al., "Q-VLM: A Post-Training Quantization Framework for Vision-Language Models" provided a post-training quantization framework for LVLMs by mining cross-layer dependencies to improve quantization efficiency. From the perspective of token pruning, TokenPacker Zhang et al., "TokenPacker: Efficient Pruning for Multimodal Pre-Trained Models" , Dynamic-LLaVA Chen et al., "Dynamic-LLaVA: Adaptive Vision-Language Fusion via Soft Attention" , and AVG-LLaVA Li et al., "AVG-LLaVA: Average Vision-Language Model for Efficient Inference" investigated training LVLMs with fewer vision tokens to boost computational efficiency. However, these methods typically require additional model training, which imposes further computational overhead.

Training-free token pruning has also been widely employed in prior research to alleviate token redundancy in vision transformers (ViTs) and large language models (LLMs). For example, PruMerge Chen et al., "PruMerge: Pruning Vision Transformers for Efficient Inference" and VisionZip Zhang et al., "VisionZip: Compressing Vision-Language Models via Joint Quantization" suggested strategies to reduce vision tokens generated by vision encoders, thereby lowering vision token volume. FastV Li et al., "FastV: A Lightweight Framework for Real-Time Video Analysis" and SparseVLM Lin et al., "SparseVLM: Efficient Multimodal Pre-Training with Sparse Attention" observed that visual tokens become less significant in deeper layers, thus proposing to eliminate redundant vision tokens during inference. VTW Dai et al., "VTW: Vision Token Weighting for Efficient Inference" introduced a strategy to remove all vision tokens at a specific layer based on KL Divergence. Although these methods have demonstrated effectiveness, they overlook the distinctions among different layers and attention heads within LVLMs, leading to a significant performance decline on complex tasks. Our research addresses this gap by proposing a fine-grained pruning method including both Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning.