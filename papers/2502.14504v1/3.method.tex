\section{Method}\label{sec::method}

Our method is a plug-and-play module during the inference process of LVLMs. Therefore, we first outline the inference process of LVLMs as preliminary, followed by the design of our proposed PLPHP.

\begin{figure*}
	\centering
		\includegraphics[width=\textwidth]{figs/arch.pdf}
	\caption{\textbf{Overview of PLPHP.} PLPHP has a two-level design including \textbf{Layer-Level Retention Rate Allocation} (as indicated by the \textcolor{red}{red dashed boxes}) and \textbf{Head-Level Vision Token Pruning} (as indicated by the \textcolor{blue}{blue dashed boxes}). Upon the completion of prefilling a certain decoder layer, PLPHP categorizes the layer as vision indifferent, balanced or attentive, and assigns a vision token retention rate to the layer based on its average attention scores to the vision tokens. Subsequently, according to the allocated retention rate, PLPHP performs fine-grained pruning for each head within the layer. It removes the visual tokens with lower attention scores from the KV cache of each attention head, ensuring that the remaining proportion of vision tokens does not exceed the pre-assigned retention rate.}
        \vspace{-0.5cm}
		\label{fig:arch}
\end{figure*}


\subsection{Preliminary}

LVLMs typically employâ€Œ an autoregressive generation paradigm during inference, which comprises two stages: the Prefilling Stage and the Decoding Stage.

\noindent \textbf{Prefilling Stage.}  In the Prefilling Stage, different modalities are mapped into a sequence of embedding vectors (tokens), which serves as the input to the LLM backbone. We denote the interleaved multimodal input token sequence of $m$ text segments and $n$ images $\mathbf{X}^1  \in \mathbb{R}^{S \times D}$ as:
\begin{equation}
\mathbf{X}^1 = \begin{pmatrix}
    \mathbf{X}_1^{(T)}\\ \mathbf{X}_1^{(I)}\\ \vdots, \\ \mathbf{X}_m^{(T)}\\ \mathbf{X}_n^{(I)}
\end{pmatrix},
\end{equation}
where $\mathbf{X}_i^{(T)} \in \mathbb{R}^{S_i^{(T)} \times D}$ represents the token sequence of the $i$-th text segment, and $\mathbf{X}_j^{(I)} \in \mathbb{R}^{S_j^{(I)} \times D}$ represents the token sequence of the $j$-th image. $S_i^{(T)}$ and $S_j^{(I)}$ represent the number of tokens for the $i$-th text segments and the $j$-th image, respectively, while \(S = \sum_{i=1}^m S_i^{(T)} + \sum_{j=1}^n S_j^{(I)}\) represents the total length of the input token sequence. $\mathcal{I}^{\left(T\right)}_i \in \mathbb{N}_0^{S^{(T)}_i}$ and $\mathcal{I}^{\left(I\right)}_j \in \mathbb{N}_0^{S^{(I)}_j}$ denote the corresponding token index sets of $\mathbf{X}_i^{\left(T\right)}$ and $\mathbf{X}_j^{\left(I\right)}$ within $\mathbf{X}^1$.

$\mathbf{X}^1$ is then fed into an LLM composed of $N$ decoder layers. Since the output and input shapes of each decoder layer are the same, we can denote the input of the $l$-th decoder layer as $\mathbf{X}^l \in \mathbb{R}^{S \times D}$. For the $h$-th attention head in the $l$-th layer:
\begin{equation}\label{eq:calcq}
\mathbf{Q}^{l, h} = \mathbf{X}^l \mathbf{W}_Q^{l, h},
\end{equation}
\begin{equation}\label{eq:calck}
\mathbf{K}^{l, h} = \mathbf{X}^l \mathbf{W}_K^{l, h},
\end{equation}
\begin{equation}\label{eq:calcv}
\mathbf{V}^{l, h} = \mathbf{X}^l \mathbf{W}_V^{l, h},
\end{equation}
where $\mathbf{W}_Q^{l, h} \in \mathbb{R}^{D \times D_k}$, $\mathbf{W}_K^{l, h} \in \mathbb{R}^{D \times D_k}$, and $\mathbf{W}_V^{l, h} \in \mathbb{R}^{D \times D_k}$ are referred to as the query projector, key projector, and value projector, respectively. $D_k$ is called the head dimension. $\mathbf{K}^{l, h}$ and $\mathbf{V}^{l, h}$ are then stored as the KV cache for the current attention head.

The attention weights $\mathbf{A}^{l,h} \in \mathbb{R}^{S\times S}$ are given by:

\vspace{-0.3cm}
\begin{equation}
    \mathbf{A}^{l,h} = \text{Softmax}\left(\frac{\mathbf{Q}^{l,h}\left(\mathbf{K}^{l,h}\right)^\top + \mathbf{\Lambda}}{\sqrt{D_k}}\right),
\end{equation}
where $\mathbf{\Lambda} \in \mathbb{R}^{S \times S}$ is an upper triangular matrix whose non-zero values are set to $-\inf$ and diagonal elements are set to $0$.


\noindent \textbf{Decoding Stage.} During the Decoding Stage, the model sequentially generates tokens and updates the KV cache of each attention head. At each timestep $t$, the input to the $l$-th decoder layer is a single token $\mathbf{x}^{l}_t \in \mathbb{R}^{1\times D}$. For the $h$-th attention head of the $l$-th layer, the KV cache is updated by:
\begin{equation}
    \mathbf{K}^{l,h} \leftarrow \begin{pmatrix} \mathbf{K}^{l,h} \\ \mathbf{x}^l_t\mathbf{W}^{l,h}_K\end{pmatrix},
\end{equation}
\begin{equation}
    \mathbf{V}^{l,h} \leftarrow \begin{pmatrix} \mathbf{V}^{l,h} \\ \mathbf{x}^l_t\mathbf{W}^{l,h}_V \end{pmatrix}. 
\end{equation}


\subsection{PLPHP}

\subsubsection{Overview}

PLPHP is a two-level adaptive fine-grained pruning method with \textbf{Layer-Level Retention Rate Allocation} and \textbf{Head-Level Vision Token Pruning}. The architecture is illustrated in Figure \ref{fig:arch}.

\subsubsection{Layer-Level Retention Rate Allocation}

To measure the extent of a decoder layer's attention to visual information, thereby determining the number of vision tokens to retain, we define the \textit{Vision Attention Score} $\gamma^l$ of the $l$-th layer as:
\begin{equation}
    \gamma^l = \sum_{k \in \bigcup_{j=1}^n\mathcal{I}^{\left(I\right)}_j}\frac{1}{H}\sum_{h=1}^H \mathbf{A}^{l,h}_{S,k},
\end{equation}
where $H$ represents the number of attention heads in each decoder layer. Note that the value of $\gamma^l$ is between $0$ and $1$. The larger the value of $\gamma^l$, the higher the $l$-th layer's attention to visual information.

In order to properly allocate the vision token retention rate based on the Vision Attention Score, given two thresholds $\alpha$ and $\beta$ ($0 \le \beta \le \alpha \le 1$), the $l$-th decoder layer is categorized as a \textbf{vision-attentive} layer when $\gamma^l \ge \alpha$, as a \textbf{vision-indifferent} layer if $\gamma^l < \beta$, and as a \textbf{vision-balanced} layer otherwise. The token retention rate $r^l$ for the $l$-th layer is defined as:
\begin{equation}
    r^l = \begin{cases}r + \Delta r, \quad &\gamma^l \ge \alpha \\ r - \Delta r,\quad &\gamma^l < \beta \\ r,\quad &\text{otherwise}\end{cases},
\end{equation}
where $0 \le \Delta r \le r \le 1 - \Delta r$. For example, selecting $\alpha=0.25$, $\beta=0.1$, $r=0.4$, and $\Delta r=0.3$ signifies that we regard decoder layers with $\gamma^l \ge 0.25$ as vision-attentive layers, and decoder layers with $\gamma^l < 0.1$ as vision-indifferent layers. For vision-attentive layers, we retain $0.4+0.3$, that is, $70\%$ of the vision tokens. For vision-indifferent layers, we retain $0.4-0.3$, that is, only $10\%$ of the visual tokens. For vision-balanced layers, we retain $40\%$ of the visual tokens. 

Through this dynamic calculation of token retention rates, we retain a larger number of vision tokens for the vision-attentive layers to leverage their heightened focus on image information, while we keep fewer vision tokens for the vision-indifferent layers to achieve higher efficiency with the least sacrifice of critical visual information. As for the vision-balanced layers, we strike a compromise, seeking an equilibrium between efficiency and performance.

\subsubsection{Head-Level Vision Token Pruning}

Given the retention rate $r^l$ calculated in the first level, we proceed to perform fine-grained pruning. According to FastV and \citet{zhang2024redundancy}, LVLMs typically exhibit a global focus on images in the first two layers and the last layer. Therefore, for a model composed of $N$ decoder layers, we select the third layer and the penultimate layer as the starting and ending layers for pruning.

To extract the most important vision tokens to preserve, for the $h$-th ($1 \le h \le H$) attention head in the $l$-th layer ($3 \le l \le N-1$), we calculate the indices of vision tokens with the highest attention scores within the $j$-th image input, accounting for the proportion $r^l$:
\begin{equation}
    \mathcal{I}^{\left(I_R\right), h}_{j} = \text{argtop}K_j\left(\mathbf{A}^{l,h}_{S}\left[\mathcal{I}_{j}^{\left(I\right)}\right]\right),
\end{equation}
where $K_j= r^l S_{j}^{\left(I\right)}$ and the $\text{argtop}K$ operation identifies the indices of the top $K$ elements with the highest values in the given sequence.

We then prune vision tokens by updating the key cache and value cache of the attention head by:
\begin{equation}\label{eq:update-k}
    \mathbf{K}^{l, h} \leftarrow \mathbf{K}^{l, h}\left[\bigcup_{i=1}^{m}\mathcal{I}^{\left(T\right)}_{i} \cup \bigcup_{j=1}^{n} \mathcal{I}_{j}^{\left(I_R\right), h} \right],
\end{equation}
\begin{equation}\label{eq:update-v}
    \mathbf{V}^{l, h} \leftarrow \mathbf{V}^{l, h}\left[\bigcup_{i=1}^{m}\mathcal{I}^{\left(T\right)}_{i} \cup \bigcup_{j=1}^{n} \mathcal{I}_{j}^{\left(I_R\right), h} \right],
\end{equation}
where $\left[\cdot\right]$ represents the indexing operation, which retrieves elements from a sequence according to the given indices.

To provide an intuitive explanation, for every attention head of the $l$-th decoder layer, we retain only the top $r^l$ proportion of the most attended tokens for each image, and remove the remaining $1-r^l$ proportion from the context. Since the number of text tokens is typically negligible compared to vision tokens, we retain all text tokens.

Our method allows different attention heads within the same decoder layer to selectively drop different contexts, thereby better utilizing the property of multi-head attention mechanisms where distinct heads can focus on various parts of the contextual information.
