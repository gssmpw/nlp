[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "radford2021learningtransferablevisualmodels",
        "author": "Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      },
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jiang2024mantis",
        "author": "Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max and Liu, Qian and Chen, Wenhu",
        "title": "Mantis: Interleaved multi-image instruction tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "laurenccon2024obelics",
        "author": "Lauren{\\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others",
        "title": "Obelics: An open web-scale filtered dataset of interleaved image-text documents"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2024llava-interleave",
        "author": "Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan",
        "title": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2024llava-onevision",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others",
        "title": "Llava-onevision: Easy visual task transfer"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chu2023mobilevlm",
        "author": "Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others",
        "title": "Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yuan2023tinygpt",
        "author": "Yuan, Zhengqing and Li, Zhaoxu and Huang, Weiran and Ye, Yanfang and Sun, Lichao",
        "title": "Tinygpt-v: Efficient multimodal large language model via small backbones"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lin2024moe",
        "author": "Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li",
        "title": "Moe-llava: Mixture of experts for large vision-language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhu2024llava",
        "author": "Zhu, Yichen and Zhu, Minjie and Liu, Ning and Xu, Zhiyuan and Peng, Yaxin",
        "title": "Llava-phi: Efficient multi-modal assistant with small language model"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2023distilling",
        "author": "Li, Xuanlin and Fang, Yunhao and Liu, Minghua and Ling, Zhan and Tu, Zhuowen and Su, Hao",
        "title": "Distilling large vision-language model with out-of-distribution generalizability"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2024q",
        "author": "Wang, Changyuan and Wang, Ziwei and Xu, Xiuwei and Tang, Yansong and Zhou, Jie and Lu, Jiwen",
        "title": "Q-VLM: Post-training Quantization for Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2024tokenpacker",
        "author": "Li, Wentong and Yuan, Yuqian and Liu, Jian and Tang, Dongqi and Wang, Song and Qin, Jie and Zhu, Jianke and Zhang, Lei",
        "title": "Tokenpacker: Efficient visual projector for multimodal llm"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "huang2024dynamic",
        "author": "Huang, Wenxuan and Zhai, Zijie and Shen, Yunhang and Cao, Shaoshen and Zhao, Fei and Xu, Xiangfeng and Ye, Zheyu and Lin, Shaohui",
        "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lan2024avg",
        "author": "Lan, Zhibin and Niu, Liqiang and Meng, Fandong and Li, Wenbo and Zhou, Jie and Su, Jinsong",
        "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "shang2024llava",
        "author": "Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan",
        "title": "Llava-prumerge: Adaptive token reduction for efficient large multimodal models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yang2024visionzip",
        "author": "Yang, Senqiao and Chen, Yukang and Tian, Zhuotao and Wang, Chengyao and Li, Jingyao and Yu, Bei and Jia, Jiaya",
        "title": "Visionzip: Longer is better but not necessary in vision language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "chen2024image",
        "author": "Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao",
        "title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhang2024sparsevlm",
        "author": "Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others",
        "title": "Sparsevlm: Visual token sparsification for efficient vision-language model inference"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "lin2024boosting",
        "author": "Lin, Zhihang and Lin, Mingbao and Lin, Luxi and Ji, Rongrong",
        "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference"
      }
    ]
  }
]