@article{bai2023qwen,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  volume={1},
  number={2},
  pages={3},
  year={2023}
}

@inproceedings{chen2024image,
  title={An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models},
  author={Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao},
  booktitle={European Conference on Computer Vision},
  pages={19--35},
  year={2024},
  organization={Springer}
}

@article{chu2023mobilevlm,
  title={Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices},
  author={Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
  journal={arXiv preprint arXiv:2312.16886},
  year={2023}
}

@article{huang2024dynamic,
  title={Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification},
  author={Huang, Wenxuan and Zhai, Zijie and Shen, Yunhang and Cao, Shaoshen and Zhao, Fei and Xu, Xiangfeng and Ye, Zheyu and Lin, Shaohui},
  journal={arXiv preprint arXiv:2412.00876},
  year={2024}
}

@article{jiang2024mantis,
  title={Mantis: Interleaved multi-image instruction tuning},
  author={Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max and Liu, Qian and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.01483},
  year={2024}
}

@article{lan2024avg,
  title={AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity},
  author={Lan, Zhibin and Niu, Liqiang and Meng, Fandong and Li, Wenbo and Zhou, Jie and Su, Jinsong},
  journal={arXiv preprint arXiv:2410.02745},
  year={2024}
}

@article{laurenccon2024obelics,
  title={Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2023distilling,
  title={Distilling large vision-language model with out-of-distribution generalizability},
  author={Li, Xuanlin and Fang, Yunhao and Liu, Minghua and Ling, Zhan and Tu, Zhuowen and Su, Hao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2492--2503},
  year={2023}
}

@article{li2024llava-interleave,
  title={Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@article{li2024llava-onevision,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{li2024tokenpacker,
  title={Tokenpacker: Efficient visual projector for multimodal llm},
  author={Li, Wentong and Yuan, Yuqian and Liu, Jian and Tang, Dongqi and Wang, Song and Qin, Jie and Zhu, Jianke and Zhang, Lei},
  journal={arXiv preprint arXiv:2407.02392},
  year={2024}
}

@article{lin2024boosting,
  title={Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference},
  author={Lin, Zhihang and Lin, Mingbao and Lin, Luxi and Ji, Rongrong},
  journal={arXiv preprint arXiv:2405.05803},
  year={2024}
}

@article{lin2024moe,
  title={Moe-llava: Mixture of experts for large vision-language models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@article{shang2024llava,
  title={Llava-prumerge: Adaptive token reduction for efficient large multimodal models},
  author={Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan},
  journal={arXiv preprint arXiv:2403.15388},
  year={2024}
}

@article{wang2024q,
  title={Q-VLM: Post-training Quantization for Large Vision-Language Models},
  author={Wang, Changyuan and Wang, Ziwei and Xu, Xiuwei and Tang, Yansong and Zhou, Jie and Lu, Jiwen},
  journal={arXiv preprint arXiv:2410.08119},
  year={2024}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{yang2024visionzip,
  title={Visionzip: Longer is better but not necessary in vision language models},
  author={Yang, Senqiao and Chen, Yukang and Tian, Zhuotao and Wang, Chengyao and Li, Jingyao and Yu, Bei and Jia, Jiaya},
  journal={arXiv preprint arXiv:2412.04467},
  year={2024}
}

@article{yuan2023tinygpt,
  title={Tinygpt-v: Efficient multimodal large language model via small backbones},
  author={Yuan, Zhengqing and Li, Zhaoxu and Huang, Weiran and Ye, Yanfang and Sun, Lichao},
  journal={arXiv preprint arXiv:2312.16862},
  year={2023}
}

@article{zhang2024sparsevlm,
  title={Sparsevlm: Visual token sparsification for efficient vision-language model inference},
  author={Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others},
  journal={arXiv preprint arXiv:2410.04417},
  year={2024}
}

@inproceedings{zhu2024llava,
  title={Llava-phi: Efficient multi-modal assistant with small language model},
  author={Zhu, Yichen and Zhu, Minjie and Liu, Ning and Xu, Zhiyuan and Peng, Yaxin},
  booktitle={Proceedings of the 1st International Workshop on Efficient Multimedia Computing under Limited},
  pages={18--22},
  year={2024}
}

