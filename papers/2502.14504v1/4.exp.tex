\section{Experiments}\label{sec::exp}

\begin{table*}[h!]
    \centering
    \caption{\textbf{Comparison of different methods on Multi-Image and Single-Image benchmarks.} $(\cdot)$ signifies the values by which the performance exceeds that of the uncompressed model after applying the corresponding method.}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
        & \multicolumn{4}{c}{Multi-Image} & \multicolumn{3}{c}{Single-Image} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-8}
        & Spot-the-Diff & Image-Edit & Visual-Story-Telling & Multi-View & Flickr30k & COCO 2017 & DetailCaps4870 \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8}
        Methods & ROUGE-L $\uparrow$ & ROUGE-L $\uparrow$ & ROUGE-L $\uparrow$ & Overall Score $\uparrow$ & CIDEr $\uparrow$ & CIDEr $\uparrow$ & CIDEr $\uparrow$ \\
        \midrule
        \multicolumn{8}{c}{LLaVA-OneVision-7B} \\
        \midrule
        Full Tokens & 39.16 & 22.15 & 31.74 & 57.29 & 79.39 & 137.97 & 11.24 \\
        \midrule
        FastV ($K=3,R=0.5$) & 37.41 & 21.16 & 24.78 & 43.22 & 77.38 & 125.01 & 9.59 \\
        FastV ($K=2,R=0.5$) & 36.19 & 20.77 & 23.99 & 43.04 & 75.37 & 120.8 & 9.31 \\
        \midrule
        VTW ($K=20$) & 30.13 & 19.59 & 29.17 & 52.68 & 39.28 & 76.23 & 7.03  \\
        VTW ($K=14$) & 30.47 & 16.17 & 25.35 & 41.47 & 16.80 & 41.43 & 3.03 \\
        \midrule
        PLPHP ($r=0.5$) & \underline{39.72} (+0.56) & \textbf{22.10} & \textbf{31.88} (+0.14)    & \textbf{57.46} (+0.17)    & \textbf{78.93} & \textbf{137.90} & \textbf{10.43} \\
        PLPHP ($r=0.4$) & \textbf{39.81}(+0.65)    & \underline{22.06} & \underline{31.82} (+0.08) & \underline{57.41} (+0.12) & \underline{78.55} & \underline{137.64} & \underline{9.89} \\
        \midrule
        \multicolumn{8}{c}{LLaVA-OneVision-0.5B} \\
        \midrule
        Full Tokens & 36.37 & 17.12 & 29.76 & 54.01 & 75.39 & 129.87 & 10.45 \\
        \midrule
        FastV ($K=3,R=0.5$) & 23.06 & 12.87 & 24.97 & 39.03 & 64.22 & 97.74 & 8.25 \\
        FastV ($K=2,R=0.5$) & 21.81 & 11.18 & 24.51 & 34.15 & 61.97 & 98.73 & 7.91 \\
        \midrule
        VTW ($K=17$) & 24.43 & \textbf{16.91} & 26.96 & 41.16 & 12.79 & 14.54 & 2.38 \\
        VTW ($K=12$) & 24.74 & 16.51 & 24.35 & 46.60 & 7.35 & 9.80 & 1.25 \\
        \midrule
        PLPHP ($r=0.5$) & \textbf{36.35} & 16.81 & \underline{29.88} (+0.12) & \textbf{54.01} & \textbf{72.34} & \textbf{126.72} & \textbf{9.31} \\
        PLPHP ($r=0.4$) & \underline{36.19} & \underline{16.82} & \textbf{30.03} (+0.27) & \underline{53.91} & \underline{71.04} & \underline{123.75} & \underline{8.35} \\
        \bottomrule
    \end{tabular}}
    \label{tab:main-results}
\end{table*}

\begin{figure*}[h!]
	\centering
	\subfloat[Spot-the-Diff]{
		\includegraphics[width=0.22\textwidth]{figs/main-StD.pdf}}
	\subfloat[Image-Edit]{
		\includegraphics[width=0.22\textwidth]{figs/main-IE.pdf}}
	\subfloat[Visual-Story-Telling]{
		\includegraphics[width=0.22\textwidth]{figs/main-VIST.pdf}}
	\subfloat[Multi-View]{
		\includegraphics[width=0.22\textwidth]{figs/main-multi-view.pdf}}
        \\ \quad \\ \quad \\
	\subfloat[Flickr30k]{
		\includegraphics[width=0.22\textwidth]{figs/main-flickr30k.pdf}}
	\subfloat[COCO 2017 Caption]{
		\includegraphics[width=0.22\textwidth]{figs/main-coco.pdf}}
	\subfloat[DetailCaps4870]{
		\includegraphics[width=0.22\textwidth]{figs/main-detailcaps.pdf}}
	\caption{\textbf{Visualization of vision token retention rates and performance across seven different benchmarks}. A point on each polyline represents a certain hyperparameter setting. We record the vision token retention rate and performance of the method under the corresponding setting. For VTW, we evaluated cases with $K=10, 14$ and $20$. For FastV, we assessed the cases of $(K, R)=(2,0.75), (3,0.5)$ and $(3,0.25)$. As for PLPHP, we examined the situations where $(r, \Delta r)=(0.3,0.3), (0.4,0.3)$ and $(0.5,0.3)$.}
        \label{fig:main}
\end{figure*}

\begin{figure*}[h!]
	\centering
	\subfloat[DetailCaps4870]{
		  \includegraphics[width=0.22\textwidth]{figs/ablation-detailcaps.pdf}\label{fig:ab-dc}
        }
	\subfloat[Spot-the-Diff]{
		  \includegraphics[width=0.22\textwidth]{figs/ablation-Spot-the-Diff.pdf}\label{fig:ab-SD}
        }
	\subfloat[Image-Edit]{
		  \includegraphics[width=0.22\textwidth]{figs/ablation-IE.pdf}\label{fig:ab-IE}
        }
	\subfloat[Visual-Story-Telling]{
		  \includegraphics[width=0.22\textwidth]{figs/ablation-VIST.pdf}\label{fig:ab-VST}
        }
	\caption{\textbf{Ablation studies on $r$ and $\Delta r$.} Each polyline in the figure corresponds to a specific value of $r$, with different points on a single line representing various values of $\Delta r$ and their corresponding performance metrics.}
		\label{fig:ablation-r-dr}
\end{figure*}

\subsection{Experimental Setting}

\noindent \textbf{Benchmarks.} 
In terms of multi-image benchmarks, we select four subsets from LLaVA-NeXT-Interleave-Bench \cite{li2024llava-interleave}: Spot-the-Diff (SD), Image-Edit (IE), Visual-Story-Telling (VST), and Multi-View (MV). We also select three single-image benchmarks: Flickr30k \cite{plummer2015flickr30k}, COCO 2017 Caption\cite{lin2014microsoft}, and DetailCaps4870 \cite{dong2024benchmarking}.


\noindent \textbf{Metrics.} Open-ended VQA tasks are evaluated using the ROUGE-L \cite{lin2004rouge} (R) metric. CIDEr \cite{vedantam2015cider} (C) and METEOR \cite{banerjee2005meteor} (M) are employed to assess image captioning tasks. Overall Score is used to evaluate the performance on Multi-View benchmark. Regarding efficiency analysis, we utilize Vision Token Retention Rate (RR), KV Cache Size (KV), and Decoding Latency as our metrics for evaluation.

\noindent \textbf{Baselines.} 
We choose FastV and VTW as our baselines. FastV discards image tokens with low attention scores in the shallow layers, while VTW retains all image tokens in the shallow layers and discards them in the deeper layers.

\noindent \textbf{Implementation Details.} 
We implement PLPHP and all baselines on an NVIDIA A100 (80GB) GPU. All methods are evaluated using LMMs-Eval \cite{lmms_eval2024,zhang2024lmmsevalrealitycheckevaluation}. More discussions regarding our benchmark selection, baseline configuration, and implementation details can be found in Appendix \ref{asec:eval-setting}.

Unless otherwise specified, the experimental results we report are based on LLaVA-OneVision-7B, and the default hyperparameter setting of PLPHP is $(r, \Delta r, \alpha, \beta) = (0.4, 0.3, 0.25, 0.1)$. The bolded text in the tables indicates the \textbf{best} performance under the corresponding metric, while the underlined text denotes the \underline{second best}.

\begin{table*}[h!]
    \centering
    \caption{\textbf{Ablation studies on $\alpha$ and $\beta$.}}
    \vspace{-0.3cm}
    \renewcommand{\arraystretch}{1}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        & Spot-the-Diff & Image-Edit & Visual-Story-Telling & DetailCaps &  &  \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} 
        Methods & ROUGE-L $\uparrow$ & ROUGE-L $\uparrow$ & ROUGE-L $\uparrow$ & CIDEr $\uparrow$ & Avg. Retention Rate (\%) $\downarrow$ & Avg. KV Cache Size (\%) $\downarrow$ \\
        \midrule
        $\alpha=0.25,\beta=0.05$ & \underline{39.74} & \textbf{22.10} & \underline{31.82} & \textbf{10.66} & 50.6\% & 53.2\% \\
        $\alpha=0.2,\beta=0.1$ & 39.15 & \textbf{22.10} & \textbf{31.87} & \underline{10.16} & 44.0\% & 50.4\% \\
        $\alpha=0.25,\beta=0.1$ & \textbf{39.81} & 22.06 & \underline{31.82} & 9.89 & 41.6\% & 47.7\% \\
        $\alpha=0.3,\beta=0.1$ & 39.35 & 22.02 & 31.81 & 9.63 & \underline{39.6\%}  & \underline{45.1\%} \\
        $\alpha=0.25,\beta=0.15$ & 39.51 & \underline{22.07} & 31.80 & 9.55 & \textbf{35.8}\% & \textbf{42.6\%} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5cm}
    \label{tab:ablation-alpha-beta}
\end{table*}



\subsection{Main Results}

We first conduct experiments with our method based on LLaVA-OneVision across different benchmarks. The main results are shown in Table \ref{tab:main-results}. From the table, we can observe that:
\begin{itemize}[leftmargin=*]
\setlength{\topmargin}{0pt}
\setlength{\itemsep}{0em}
    \item \textbf{PLPHP significantly outperforms both baselines across different benchmarks.} For the LLaVA-OneVision-7B model, the average performance of PLPHP under default hyperparameter settings surpasses FastV by 11.4\% and VTW by 48.4\%. Compared to the uncompressed model, the average performance degradation brought by PLPHP is merely 0.46\%. We attribute this performance enhancement to the granularity and adaptability of PLPHP. In contrast to FastV and VTW, which discard a fixed set of vision tokens from \textit{all} pruned attention heads, the dynamic nature of PLPHP offers a distinct performance advantage.
    \item \textbf{Model with PLPHP outperforms uncompressed model on various multi-image tasks.} Notably, the average performance of PLPHP surpasses that of the uncompressed model by 0.51\% across multiple multi-image task benchmarks on LLaVA-OneVision-7B through appropriate pruning. The improvement on multi-image benchmarks could be attributed to the increased redundancy in visual information inherent in multi-image tasks, which could potentially be detrimental to model inference. This redundancy is effectively eliminated by PLPHP, thereby enhancing both the efficiency and performance. 
    \item \textbf{The performance of PLPHP remains relatively stable under different retention rates.} The carefully designed pruning dynamics in PLPHP allow it to prioritize the removal of the most redundant tokens, thereby ensuring that performance is less affected by the pruning rate. On the other hand, VTW is highly sensitive to the selection of $K$. It discards \textit{all} vision tokens at a specific layer, thus once the model exhibits significant \textit{Vision Token Re-attention} after this layer, it is likely to severely impact the performance, which could be the cause of its high sensitivity to the hyperparameter and substantial performance decline in image captioning tasks.
\end{itemize}

To provide a more intuitive analysis of how each method performs under varying pruning rates, we evaluated their performance across different vision token retention rates and visualized the results in Figure \ref{fig:main}. It can be observed that PLPHP consistently outperforms the baseline at the same pruning rate and maintains nearly no performance degradation within a certain pruning rate range, indicating that we can achieve better performance while discarding more vision tokens, which directly leads to a higher computational efficiency.

These performance boosts highlight the superiority of our method, which dynamically adjusts the pruning rate based on the attention allocated to image tokens in different layers and independently preserve different contextual information for different attention heads.

\begin{table}[h]
    \centering
    \caption{\textbf{Performance of PLPHP on various models.} Bolded text indicates that PLPHP surpasses the uncompressed model.}
    \vspace{-0.3cm}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccccccc}
        \toprule
        & SD
        & IE
        & VST
        & MV 
        & Flickr30k
        & COCO
        & 
        & \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9} 
        Methods
        & R $\uparrow$ 
        & R $\uparrow$ 
        & R $\uparrow$
        & R $\uparrow$
        & C $\uparrow$ 
        & C $\uparrow$
        & RR (\%) $\downarrow$
        & KV (\%) $\downarrow$\\
        \midrule
        Qwen2-VL  & 27.56 & 21.21 & 24.92 & 12.78 & 77.24 & 96.18 & 100\% & 100\% \\
         w/ PLPHP & \textbf{27.78} & \textbf{21.40} & \textbf{25.02} & \textbf{12.96} & \textbf{78.02} & \textbf{98.67} & \textbf{35.8\%} & \textbf{41.9\%} \\
        \midrule
        IDEFICS2 & 18.98 & 14.90 & 23.91 & 13.84 & 51.73 & 72.12 & 100\% & 100\% \\
        w/ PLPHP & 18.55 & 14.89 & \textbf{23.93} & \textbf{13.96} & 51.68 & \textbf{72.60} & \textbf{36.1\%} & \textbf{51.3\%} \\
        \midrule
        Mantis   & 16.30 &  9.56 & 13.27 & 11.02 & 70.41 & 91.41 & 100\% & 100\% \\
        w/ PLPHP & \textbf{16.41} & \textbf{9.81} & \textbf{13.41} & \textbf{11.14} & 69.90 & 90.61 & \textbf{29.1\%} & \textbf{33.7\%} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5cm}
    \label{tab:performance-on-different-models}
\end{table}

\subsection{Generality of PLPHP on Various LVLMs}

To further demonstrate the generality of PLPHP on various model architectures, we implement PLPHP on common LVLMs with different LLM backbones, and directly compared them with uncompressed models to highlight our effectiveness, with results shown in Table \ref{tab:performance-on-different-models}. Since IDEFICS2 and Mantis are unable to follow instructions in DetailCaps4870, we evaluate PLPHP on the other six benchmarks. \textbf{Remarkably, Qwen2-VL equipped with PLPHP surpasses the uncompressed model across all benchmarks}, achieving an average improvement rate of 1.5\%, while saving an average of 58.1\% KV Cache storage space. For the other two models, our method also achieves an average of 57\% KV Cache compression while surpassing the original models across multiple benchmarks.

\begin{figure}[h]
	\centering
        \vspace{-0.2cm}
        \subfloat[Decoding Latency]{
		  \includegraphics[width=0.23\textwidth]{figs/exp-latency.pdf}\label{fig:efficiency-latency}
        }
        \subfloat[KV Cache Size]{
		  \includegraphics[width=0.23\textwidth]{figs/exp-kv-size.pdf}\label{fig:efficiency-kv}
        }
        \vspace{-0.1cm}
	\caption{\textbf{The decoding latency and KV Cache size results.} Both baselines maintain constant KV Cache sizes due to unchanging pruning rates, while PLPHP adaptively assigns retention rates, producing a fluctuating curve with a smaller mean.}
        \vspace{-0.4cm}
		\label{fig:efficiency-latency-kv}
\end{figure}

\begin{table}[h]
    \centering
    \caption{\textbf{Performance and efficiency comparison among different methods.}}
    \vspace{-0.3cm}
    \renewcommand{\arraystretch}{1}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        & \multicolumn{5}{c}{DetailCaps4870} \\
        \cmidrule(lr){2-6}
        Methods & C $\uparrow$ & M $\uparrow$ & R $\uparrow$ & Time (h) $\downarrow$ & RR (\%) $\downarrow$ \\
        \midrule
        Full Tokens & 11.24 & 20.13 & 30.01 & 5.63 & 100\%\\
        \midrule
        FastV ($K=3, R=0.5$) & \underline{9.59} & \underline{18.55} & \underline{28.72} & \underline{5.23} & 55.4\%\\
        VTW ($K=14$) & 3.03 & 15.05 & 24.38 & \textbf{5.22} & \underline{50.0\%} \\
        PLPHP & \textbf{9.89} & \textbf{19.33} & \textbf{29.19} & \underline{5.23} & \textbf{47.5\%}\\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.4cm}
    \label{tab:efficiency-on-detailcaps}
\end{table}

\subsection{Efficiency Analysis}

To analyze the efficiency of PLPHP, we conduct experiments on DetailCaps4870 since it includes long generation contents. We can observe from Figure \ref{fig:efficiency-latency} that PLPHP achieves a comparable total decoding latency to both baselines. The latency introduced by the unpruned Prefilling Stage is minimal (less than 0.5 tokens of delay). Figure \ref{fig:efficiency-kv} shows that PLPHP maintains a lower KV cache size during the evaluation process compared to all baselines, leading to a shorter decoding latency. Table \ref{tab:efficiency-on-detailcaps} shows that PLPHP attains performance closest to the uncompressed model. The nearly consistent evaluation time also indicates that the additional computation during the Prefilling Stage gradually becomes negligible as generation progresses.

\begin{table}[h]
    \centering
    \caption{\textbf{Decoding Latency and KV Cache Size of PLPHP under different retention rates.}}
    \vspace{-0.3cm}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Methods  & Decoding Latency (ms/token) $\downarrow$ & KV Cache Size (\%) $\downarrow$ \\
        \midrule
        Full Tokens &  49.10 &  100\% \\
        \midrule
        PLPHP ($r=0.5$)  & 41.26 & 54.9\% \\
        PLPHP ($r=0.4$) & \underline{40.20} & \underline{46.2\%} \\
        PLPHP ($r=0.3$) & \textbf{39.19} & \textbf{37.6\%} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5cm}
    \label{tab:efficiency-analysis}
\end{table}


\subsection{Ablation Study}

To explore the impact of $r$ and $\Delta r$, we conduct ablation experiments on four benchmarks, with the results illustrated in Figure \ref{fig:ablation-r-dr}. It can be observed that setting $\Delta r > 0$ consistently outperforms the cases where $\Delta r=0$, indicating that adaptive pruning rates are superior to a fixed pruning rate. This finding demonstrates that our proposed \textbf{layer-level pruning rate allocation has a positive impact on model performance}.

Since $r$ is the most direct parameter reflecting the average pruning rate, we test the impact of $r$ on efficiency, with the results presented in Table \ref{tab:efficiency-analysis}. PLPHP achieves an 18.1\% decoding speedup and a 53.8\% KV Cache compression under the default settings where $r=0.4$, and further reaches a 20.2\% acceleration and a 62.4\% compression at a lower retention rate, enhancing the computational efficiency of LVLM decoding remarkably.

$\alpha$ and $\beta$ also indirectly influence pruning rates, thus we also conduct ablation studies with the results shown in Table \ref{tab:ablation-alpha-beta}. Intuitively, increasing $\alpha$ and $\beta$ elevates the criteria for vision-attentive layers and vision-balanced layers more stringent, leading to higher pruning rates at the cost of performance loss. Conversely, decreasing them relaxes the criteria, enhancing the performance but at greater computational expense.
