\section{Related Work}
\label{sec::related}

\subsection{Large Vision-Language Models}\label{sec::relatedwork1}
Recent advancements in LVLMs significantly enhanced multimodal content understanding. ____ developed LLaVA, an early general-purpose multimodal model integrating CLIP ____ with language models. Subsequent innovations include Qwen-VL ____, which enhanced visual processing with a specialized visual receptor and multilingual corpus, and Mantis by ____, which improved multi-image reasoning through academic-level instruction tuning. ____ introduced IDEFICS, trained on the OBELICS dataset of interleaved image-text documents. Unified approaches by ____ and ____ achieved state-of-the-art performance in single-image, multi-image, and video tasks. However, LVLMs still face computational challenges due to the high number of visual tokens during inference, underscoring the need for more efficient inference.

\subsection{Efficient Multimodal Large Language Models}\label{sec::relatedwork2}
To optimize the computational efficiency of LVLMs during inference, works such as MobileVLM ____, Tinygpt-V ____, MoE LLaVA ____, and LLaVA-Phi ____ proposed more efficient model architectures. Meanwhile, ____ introduced a model-distillation approach that transfers knowledge from large vision-language models (VLMs) to smaller, lighter counterparts. Q-VLM ____ provided a post-training quantization framework for LVLMs by mining cross-layer dependencies to improve quantization efficiency. From the perspective of token pruning, TokenPacker ____, Dynamic-LLaVA ____, and AVG-LLaVA ____ investigated training LVLMs with fewer vision tokens to boost computational efficiency. However, these methods typically require additional model training, which imposes further computational overhead.

Training-free token pruning has also been widely employed in prior research to alleviate token redundancy in vision transformers (ViTs) and large language models (LLMs). For example, PruMerge ____ and VisionZip ____ suggested strategies to reduce vision tokens generated by vision encoders, thereby lowering vision token volume. FastV ____ and SparseVLM ____ observed that visual tokens become less significant in deeper layers, thus proposing to eliminate redundant vision tokens during inference. VTW ____ introduced a strategy to remove all vision tokens at a specific layer based on KL Divergence. Although these methods have demonstrated effectiveness, they overlook the distinctions among different layers and attention heads within LVLMs, leading to a significant performance decline on complex tasks. Our research addresses this gap by proposing a fine-grained pruning method including both Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning.