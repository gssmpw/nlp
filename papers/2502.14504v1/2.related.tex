\section{Related Work}
\label{sec::related}

\subsection{Large Vision-Language Models}\label{sec::relatedwork1}
Recent advancements in LVLMs significantly enhanced multimodal content understanding. \citet{liu2023llava} developed LLaVA, an early general-purpose multimodal model integrating CLIP \cite{radford2021learningtransferablevisualmodels} with language models. Subsequent innovations include Qwen-VL \cite{bai2023qwen,wang2024qwen2}, which enhanced visual processing with a specialized visual receptor and multilingual corpus, and Mantis by \citet{jiang2024mantis}, which improved multi-image reasoning through academic-level instruction tuning. \citet{laurenccon2024obelics} introduced IDEFICS, trained on the OBELICS dataset of interleaved image-text documents. Unified approaches by \citet{li2024llava-interleave} and \citet{li2024llava-onevision} achieved state-of-the-art performance in single-image, multi-image, and video tasks. However, LVLMs still face computational challenges due to the high number of visual tokens during inference, underscoring the need for more efficient inference.

\subsection{Efficient Multimodal Large Language Models}\label{sec::relatedwork2}
To optimize the computational efficiency of LVLMs during inference, works such as MobileVLM \cite{chu2023mobilevlm}, Tinygpt-V \cite{yuan2023tinygpt}, MoE LLaVA \cite{lin2024moe}, and LLaVA-Phi \cite{zhu2024llava} proposed more efficient model architectures. Meanwhile, \citet{li2023distilling} introduced a model-distillation approach that transfers knowledge from large vision-language models (VLMs) to smaller, lighter counterparts. Q-VLM \cite{wang2024q} provided a post-training quantization framework for LVLMs by mining cross-layer dependencies to improve quantization efficiency. From the perspective of token pruning, TokenPacker \cite{li2024tokenpacker}, Dynamic-LLaVA \cite{huang2024dynamic}, and AVG-LLaVA \cite{lan2024avg} investigated training LVLMs with fewer vision tokens to boost computational efficiency. However, these methods typically require additional model training, which imposes further computational overhead.

Training-free token pruning has also been widely employed in prior research to alleviate token redundancy in vision transformers (ViTs) and large language models (LLMs). For example, PruMerge \cite{shang2024llava} and VisionZip \cite{yang2024visionzip} suggested strategies to reduce vision tokens generated by vision encoders, thereby lowering vision token volume. FastV \cite{chen2024image} and SparseVLM \cite{zhang2024sparsevlm} observed that visual tokens become less significant in deeper layers, thus proposing to eliminate redundant vision tokens during inference. VTW \cite{lin2024boosting} introduced a strategy to remove all vision tokens at a specific layer based on KL Divergence. Although these methods have demonstrated effectiveness, they overlook the distinctions among different layers and attention heads within LVLMs, leading to a significant performance decline on complex tasks. Our research addresses this gap by proposing a fine-grained pruning method including both Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning.
