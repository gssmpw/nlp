\section{Introduction}
\label{sec::intro}

Recent advancements in Large Vision-Language Models (LVLMs) have established them as a prominent research focus in multimodal learning. Numerous open-source implementations have demonstrated remarkable capabilities across various tasks, including multimodal understanding and reasoning.

\begin{figure}[h!]
	\centering
	\subfloat[LLaVA-OneVision]{
		\includegraphics[width=0.23\textwidth]{figs/refocus-ov.pdf}}
	\subfloat[Qwen2-VL]{
		\includegraphics[width=0.23\textwidth]{figs/refocus-qwen.pdf}}
	\\ \quad \\ \quad
	\subfloat[IDEFICS2]{
		\includegraphics[width=0.23\textwidth]{figs/refocus-idefics.pdf}}
	\subfloat[Mantis]{
		\includegraphics[width=0.23\textwidth]{figs/refocus-mantis.pdf}}
	\caption{\textbf{The phenomenon of Vision Token Re-attention in different LVLMs.} Various LVLMs demonstrate the phenomenon of refocusing on images within deep decoder layers. In these layers, the attention scores corresponding to vision tokens increase, as indicated by the \textcolor{red}{red boxes} highlighted in the figure.}
        \vspace{-0.4cm}
		\label{fig:refocus}
\end{figure}

Nevertheless, LVLMs face computational inefficiency challenges, mainly due to converting visual inputs into lengthy vision token sequences, ranging from thousands to tens of thousands. Previous studies \cite{chen2024image,lin2024boosting} find that LVLMs exhibit lower attentions to vision tokens in deeper layers compared to shallower layers, thus a certain amount of vision tokens are pruned at specific shallow layers, and the \textit{same} tokens are pruned in \textit{all} subsequent layers. However, such coarse-grained pruning strategies often lead to a significant performance decline in complex tasks that require comprehensive visual information, including open-ended VQA and image captioning. To address this challenge, in this work, we propose \textbf{P}er-\textbf{L}ayer \textbf{P}er-\textbf{H}ead Vision Token \textbf{P}runing (\textbf{PLPHP}), a plug-and-play adaptive fine-grained vision token pruning method that includes two levels: \textbf{1) Layer-Level Retention Rate Allocation} and \textbf{2) Head-Level Vision Token Pruning}, significantly reducing the performance loss associated with pruning.

\begin{figure}[h!]
	\centering
        \subfloat[LLaVA-OneVision]{
		  \includegraphics[width=0.23\textwidth]{figs/image-parts-attn-weights.pdf}
        }
        \subfloat[Qwen2-VL]{
		  \includegraphics[width=0.23\textwidth]{figs/qwen2-image-attention-part.pdf}
        }
	\caption{\textbf{The proportion of attention scores received by different parts of the same image varies across different decoder layers.} Each polyline in the figure represents the proportion of attention scores for the corresponding group of tokens across different decoder layers.}
        \vspace{-0.2cm}
		\label{fig:image-part}
\end{figure}

The first level of our proposed method stems from our analysis of the attention to visual information in the deeper layers of LVLMs. As shown in Figure \ref{fig:refocus}, we observe the phenomenon of \textit{Vision Token Re-attention} across LVLMs with different architectures where attention scores of vision tokens are initially high and decrease in intermediate layers, but rise again in certain deeper layers. This indicates that LVLMs \textit{do not} always disregard vision tokens in deep layers, thus we need to dynamically adjust the pruning rate to accommodate the unique attention patterns of different decoder layers.

\begin{figure}[h!]
	\centering
	\subfloat[Head 2]{
		\includegraphics[width=0.16\textwidth]{figs/layer12-head2-attn.png}}
	\subfloat[Head 9]{
		\includegraphics[width=0.16\textwidth]{figs/layer12-head9-attn.png}}
	\subfloat[Head 12]{
		\includegraphics[width=0.16\textwidth]{figs/layer12-head17-attn.png}}
	\caption{\textbf{Visualization of attention maps in various attention heads.} Different heads within the same decoder layer exhibit different attention patterns.}
        \vspace{-0.2cm}
		\label{fig:per-head-attention-pattern}
\end{figure}

The second level of our method is motivated by an in-depth investigation on the variations in vision token attention across different decoder layers. As shown in Figure \ref{fig:image-part}, we divide the vision tokens into five groups based on their spatial relationships and plot the proportions of attention scores for each group across different layers. We observe that different parts of the same input image receive varying proportions of attention across different decoder layers, suggesting that each decoder layer specializes in processing distinct contexts. Furthermore, we conduct a more granular analysis at the level of attention heads. As illustrated in Figure \ref{fig:per-head-attention-pattern}, different attention heads within the same decoder layer exhibit distinct patterns of attention, demonstrating that the focus on different contexts occurs at the attention head level. This observation suggests that the unique contextual information processed by each attention head should be independently preserved during the pruning process to maintain model performance.

Built on these motivations, by dynamically adjusting retention rates according to layer-specific attention patterns layer by layer, PLPHP retains more vision tokens in layers where image attention scores are high, while aggressively pruning layers with low attention scores. Additionally, through head-level independent context pruning, PLPHP preserves the most critical contextual information for each attention head, leading to performance improvements. Comprehensive evaluations across multiple model architectures and various benchmarks demonstrate the effectiveness of PLPHP. Our method achieves over 50\% compression of the KV cache, over 18\% decoding acceleration, and only a 0.46\% average performance degradation with notable improvements on multi-image tasks.

The contributions of our work can be summarized into the following three points:
\begin{itemize}[leftmargin=*]
\setlength{\topmargin}{0pt}
\setlength{\itemsep}{0em}
    \item  We uncover the widespread phenomenon of \textit{Vision Token Re-attention} through investigations on various LVLMs, which could be a significant factor leading to the performance degradation of existing pruning methods.
    \item  We propose PLPHP, a plug-and-play adaptive fine-grained vision token pruning method that improves the performance of pruned models significantly while maintaining high computational efficiency.
    \item We conduct extensive experiments across multiple benchmarks and model architectures, validating the superiority of our proposed method.
\end{itemize}