\cleardoublepage
\section{Appendix}

\subsection{Details of Evaluation Settings}\label{asec:eval-setting}

\subsubsection{Benchmarks}\label{asec:eval-setting-bench}

Since PLPHP maintains the computational integrity of the LVLMs' Prefilling Stage, its efficiency advantage is primarily reflected in the low decoding latency during the subsequent Decoding Stage. Therefore, we mainly choose benchmarks composed of open-ended VQA and image captioning tasks. The benchmarks we select encompasses both multi-image task benchmarks and single-image task benchmarks.

$\bullet$ \textbf{Multi-Image benchmarks}: The LLaVA-Interleave Bench is a comprehensive benchmark dataset designed to evaluate the performance of LVLMs in multi-image scenarios. It consists of 13 challenging tasks with a total of 17,000 instances. We curated four subsets consisting of open-ended VQA tasks from LLaVA-NeXT-Interleave-Bench: Spot-the-Diff, Image-Edit, Visual-Story-Telling, and Multi-View.

$\bullet$ \textbf{Single-Image benchmarks}: The Flickr30k dataset is a widely used benchmark in the field of image captioning and visual understanding. It consists of 31,783 images collected from the Flickr platform, each paired with five human-annotated captions. The COCO2017 Caption subset contains more than 45,000 images, each annotated with five captions written by human annotators, describing the visual content of the images in detail, including objects, their attributes, and the relationships between them. DetailCaps4870 provides more fine-grained and specific image content descriptions than standard captioning datasets, which is more useful for efficiency analysis. 

\subsubsection{Baselines}\label{asec:eval-setting-baseline}

We select FastV and VTW as our baselines in our experiments. Notably, FastV offers two versions of implementation: one that supports KV cache and one that does not. Since the non-KV-cache implementation introduces substantial computational overhead, we use the version that supports KV cache to ensure a fair comparison. For both of the baselines, we refer to the official open source code \footnote{\url{https://github.com/pkunlp-icler/FastV}} \footnote{\url{https://github.com/lzhxmu/VTW}} and implement them on the models we evaluate.

\subsubsection{Models}\label{asec:eval-setting-impl}

For Qwen2-VL, we set \texttt{max\_pixels} to $1280 \times 28 \times 28$ and \texttt{min\_pixels} to $256 \times 28 \times 28$ according to the official recommendation. The Mantis model that we choose is Mantis-8B-SigLIP-LLaMA3. For LLaVA-OneVision and Mantis, we use the official original versions \footnote{\url{https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov}} \footnote{\url{https://huggingface.co/TIGER-Lab/Mantis-8B-siglip-llama3}}, while using the versions provided by the transformers library \cite{wolf-etal-2020-transformers} for all other models.

\subsection{Case Study}

To showcase the effectiveness of our proposed method, we present a series of case studies in the form of multimodal chatbots, as shown in Figure \ref{fig:case-studies}.

\begin{figure*}[ht]
	\centering
	\subfloat[]{
		\includegraphics[width=0.48\textwidth]{figs/appendix-case3.png}}
        \subfloat[]{
		\includegraphics[width=0.48\textwidth]{figs/appendix-case4.png}}
        \\ \quad \\ \quad \\
	\subfloat[]{
		\includegraphics[width=0.48\textwidth]{figs/appendix-case1.png}}
        \subfloat[]{
		\includegraphics[width=0.48\textwidth]{figs/appendix-case2.png}}
 %        \\
	% \subfloat[]{
	% 	\includegraphics[width=0.9\textwidth]{figs/appendix-case5.png}}
	\caption{\textbf{Multimodal Chatbots with different pruning methods.}}
		\label{fig:case-studies}
\end{figure*}