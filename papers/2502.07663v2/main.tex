\documentclass[fleqn,12pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{setspace}
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{nameref}
\usepackage{lscape}
\usepackage{tcolorbox}
\tcbuselibrary{listings, breakable}

\title{Human Decision-making is Susceptible to \\*AI-driven Manipulation}

\author[1,+,*]{Sahand Sabour}
\author[2,3, +,*]{June M. Liu}
\author[4]{Siyang Liu}
\author[1]{Chris Z. Yao}
\author[1]{Shiyao Cui}
\author[1]{Xuanming Zhang}
\author[5]{Wen Zhang}
\author[6,1]{Yaru Cao}
\author[7]{Advait Bhat}
\author[8]{Jian Guan}
\author[8]{Wei Wu}
\author[4]{Rada Mihalcea}
\author[1]{Hongning Wang}
\author[7]{Tim Althoff}
\author[2,3, *]{Tatia M.C. Lee} 
\author[1,*]{Minlie Huang}

\affil[1]{The CoAI Group, DCST, Institute for Artificial Intelligence, Tsinghua University, Beijing, China}
\affil[2]{State Key Laboratory of Brain and Cognitive Sciences, The University of Hong Kong, Hong Kong SAR, China}
\affil[3]{Laboratory of Neuropsychology and Human Neuroscience, The University of Hong Kong, Hong Kong SAR, China}
\affil[4]{The LIT Group, Department of Computer Science and Engineering, University of Michigan, Ann Arbor}
\affil[5]{Department of Psychology, University of International Relations, Beijing, China}
\affil[6]{Department of Chinese Language and Literature, Northwest Minzu University, Lanzhou, China}
\affil[7]{Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, USA}
\affil[8]{ANT Group}
\affil[*]{Sahandfer@gmail.com, juneliu@connect.hku.hk, tmclee@hku.hk, aihuang@tsinghua.edu.cn}
\affil[+]{These authors contributed equally to this work}

\begin{document}
\begin{abstract}
Artificial Intelligence (AI) systems are increasingly intertwined with daily life, assisting users in executing various tasks and providing guidance on decision-making.
This integration introduces risks of AI-driven manipulation, where such systems may exploit users' cognitive biases and emotional vulnerabilities to steer them toward harmful outcomes.
Through a randomized controlled trial with $233$ participants, we examined human susceptibility to such manipulation in financial (e.g., purchases) and emotional (e.g., conflict resolution) decision-making contexts.
Participants interacted with one of three AI agents: a neutral agent (NA) optimizing for user benefit without explicit influence, a manipulative agent (MA) designed to covertly influence beliefs and behaviors, or a strategy-enhanced manipulative agent (SEMA) employing established psychological tactics to reach its hidden objectives.
To ensure participant well-being, this study involved hypothetical scenarios, pre-screening for at-risk individuals, and comprehensive post-study debriefing.
By analyzing participants' decision patterns and shifts in their preference ratings post-interaction, we found significant susceptibility to AI-driven manipulation.
Particularly, across both decision-making domains, participants interacting with the manipulative agents shifted toward harmful options at substantially higher rates (financial, MA: $62.3\%$, SEMA: $59.6\%$; emotional, MA: $42.3\%$, SEMA: $41.5\%$) compared to the NA group (financial, $35.8\%$;  emotional, $12.8\%$).
Notably, our results reveal that even simple manipulative objectives (MA) can be as effective as employing established psychological strategies (SEMA) in swaying human decision-making.
These findings suggest that AI-driven manipulation could become widespread, as it does not require sophisticated tactics and expertise to influence users.
By revealing the potential for covert AI influence, this study highlights a critical vulnerability in human-AI interactions, emphasizing the urgent need for ethical safeguards and regulatory frameworks to ensure responsible deployment of AI technologies and protect human autonomy.
\end{abstract}
\flushbottom
\maketitle

\vspace{-1cm}
\section*{Introduction}\label{sec:intro}
The vast integration of AI technologies into our daily lives has fundamentally changed how we process information and make decisions \cite{ma2024towards, haduong2024raising}.
As AI systems increasingly serve as personal assistants, humans have demonstrated growing reliance and trust in AI-generated content across various domains \cite{rejeleene2024towards, schwartz2023enhancing, schemmer2022should, leib2021corruptive}.
While this technological shift offers unprecedented access to information and assistance with decision-making, it also introduces subtle yet significant risks to human autonomy.
At the core of these risks lies the potential for AI systems to influence human beliefs and behaviors in ways that may bypass their conscious awareness \cite{mun2024particip, meier2024llm, weidinger2023sociotechnical, kenton2021alignment}. 

Previous research has extensively investigated observable safety risks such as toxicity and discrimination in AI-generated content \cite{zhang2023safetybench, yu2024cosafe}.
However, the psychological mechanisms through which AI systems might influence human decision-making require further investigation \cite{weidinger2021ethical, durmus2024measuring}. 
This gap is particularly crucial as humans increasingly resort to AI assistants for practical and personal advice \cite{mireshghallah2024trust}.
In such cases, the organizations offering these AI systems, which are often for-profit companies, may wield significant power in shaping user decisions, raising concerns about the potential for large-scale AI-driven manipulation motivated by commercial interests \cite{matz2024potential}.

\vspace{-0.2cm}
\begin{figure*}[!ht]
\centering
\includegraphics[width=\linewidth]{figures/framework.pdf}
\vspace{-0.8cm}
\caption{\textbf{Overview of our study:} 
\textbf{a) Framework of decision-making scenarios.}  
Participants were assigned to either the financial or emotional decision-making domain. 
Each domain featured three pre-defined scenarios with four options: one optimal and three harmful, representing non-existent, excessive, and dependence-inducing products in the financial domain, and maladaptive coping strategies (disengagement, emotional venting, self-blame) in the emotional domain.
\textbf{b) AI conditions.}  
Within each domain, we further assigned the participants to an AI condition: \textit{1. Neutral Agent (NA)}, an assistant optimizing for user benefit, designed as the control condition in our experiments; \textit{2. Manipulative Agent (MA)}, an assistant with hidden manipulative objectives; and \textit{3. Strategy-Enhanced Manipulative Agent (SEMA)}, an assistant equipped with established manipulation strategies based on existing psychological literature.
\textbf{c) Flowchart overview of the study design.} 
}
\label{fig:framework}
\end{figure*}
\vspace{-0.2cm}

This study introduces a novel framework for examining human susceptibility to AI-driven influence, distinguishing between the two opposing polarities of influence\cite{lau2023good}: beneficial persuasion and potentially harmful manipulation. 
Persuasion involves transparent guidance through logical arguments and ethical appeals\cite{perloff1993dynamics}, and existing literature has demonstrated its potential value in AI applications such as charitable donations \cite{wang2019persuasion, shi2020effects}, mental and physical health interventions \cite{jorke2024supporting, karinshak2023working, lin2024imbue, sharma2023human}, debates\cite{voelkel2023artificial, breum2024persuasive, carrasco2024large, costello2024durably}, and advertising\cite{feizi2023online, meguellati2024good}. 
Conversely, manipulation involves covert influence aimed at achieving hidden objectives, often by exploiting the target's cognitive biases and emotional vulnerabilities for personal gain \cite{handelman2009thought}.
AI systems possessing manipulative capabilities could have catastrophic consequences, potentially leading to financial losses \cite{schillaci2024llm, burtell2023artificial}, data exploitation \cite{weidinger2021ethical, ai2024defending}, and overall negative impact on personal beliefs and values \cite{meier2024llm}.

Recent research has demonstrated AI systems' manipulative capabilities in controlled environments, primarily in game-based settings and simulated interactions\cite{pan2023rewards, wilczynski2024resistance, terekhov2023second, heaven2019no, qi2024enhancing}, without involving real human subjects.
While these studies provide valuable insights, their applicability to real-world contexts is limited.
For instance, game-based environments simplify human behavior into predictable patterns (e.g., bluffing to maximize in-game rewards\cite{heaven2019no}) and may not consider the cognitive biases and emotional vulnerabilities that shape human decisions \cite{Rastogi2020DecidingFA, Brown2011TheRO}.
Similarly, simulated interactions between AI systems role-playing as human users lack the dynamic nature of human-AI interactions, where users' evolving beliefs, emotions, and perceived consequences (e.g., financial risks or interpersonal conflicts) affect their susceptibility to influence.
Therefore, the extent to which real humans are susceptible to AI-driven manipulation in practical decision-making contexts remains unexplored.

To this end, we conducted a randomized controlled trial with $233$ participants, investigating human susceptibility to AI-driven manipulation across two fundamental domains where AI systems are widely deployed \cite{lakkaraju2023llms, hwang2021toward, gual2022using, sabour2023chatbot}: financial and emotional decision-making (Figure \ref{fig:framework}).
While both domains involve real-world risks and consequences, they differ in how AI systems might exploit human vulnerabilities.
Financial decisions are grounded in quantifiable risks and trade-offs (e.g., budget constraints, product quality), where users may overtrust AI’s perceived objectivity.
In contrast, emotional decision-making primarily involves psychosocial vulnerabilities (e.g., low self-esteem, peer pressure), where AI systems could exploit users' insecurities or manipulate social dynamics.
By contrasting these domains, we aimed to investigate the susceptibility of various human vulnerabilities to AI-driven manipulation.

To mitigate potential harm to participants, we employed several protective measures (see Methods). 
Participants were screened to exclude individuals with physical or mental health conditions that could render them vulnerable to psychological distress. 
Our experiments involved hypothetical decision-making scenarios designed to simulate real-world contexts without exposing participants to actual risks. 
Importantly, all participants received a comprehensive debriefing post-study, including full disclosure of the study’s objectives and clarification of the optimal decisions in each scenario. 

Our primary hypothesis was that participants interacting with manipulative AI agents (MA and SEMA), which were instructed to covertly influence their decisions, would shift their preferences toward the agents’ hidden incentives, whereas those interacting with the neutral agent would identify and prioritize beneficial options.
Our findings supported this hypothesis, revealing that humans are highly susceptible to AI-driven manipulation: participants in the manipulative groups were significantly more likely to shift their preferences toward harmful options and away from beneficial (optimal) choices compared to those interacting with the neutral agent (Figures \ref{fig:decision_change} \& \ref{fig:score_changes}).
% Notably, the use of established psychological strategies (SEMA vs. MA) did not significantly affect susceptibility, suggesting that simply providing the agent with hidden manipulative intents can substantially influence human decision-making.

These findings highlight the need for ethical safeguards and regulatory frameworks to protect user autonomy and well-being in human-AI interactions. 
As AI systems become increasingly sophisticated in their ability to engage with and influence human beliefs and behavior, understanding the psychological vulnerabilities they may exploit is crucial for balancing the potential benefits of AI with the protection of human autonomy.
This study also raises a broader ethical question regarding the potential misuse of our findings to exploit vulnerabilities in human decision-making.
However, we believe that in the context of increasing AI applications across various sectors, the ethical responsibility to understand and highlight these psychological risks outweighs such concerns.


\section*{Results}\label{sec:results}
\subsection*{Human decision-making shows significant susceptibility to AI-driven manipulation}
Analysis of decision-pattern distributions (Figure \ref{fig:decision_change}; Methods) indicates that participants who interacted with the manipulative agent (MA) or the strategy-enhanced manipulative agent (SEMA) shifted their preferences towards the harmful options at significantly higher rates than those assisted by the neutral agent (NA).
In the financial domain, negative shifts (i.e., switching to a harmful option) occurred in $61.4\%$ (MA) and $59.6\%$ (SEMA) of the cases, compared to $28.3\%$ for NA ($P < 0.0001$; Cohen's $h=0.68$ and $0.64$, respectively).
Similar patterns emerged in emotional contexts, as participants in the manipulative groups exhibited harmful shifts in their decisions in $42.3\%$ (MA) and $41.5\%$ (SEMA) of cases, compared to the $12.8\%$ for NA ($P < 0.0001$; $h=0.68$ and $0.67$, respectively).

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{figures/decision_changes.pdf}
\caption{\textbf{Distribution of decision patterns across AI conditions in decision-making contexts.}
The bar charts indicate the proportion of decision patterns (\%) in financial and emotional scenarios across three AI conditions: NA (Neutral Agent), MA (Manipulative Agent), and SEMA (Strategy-enhanced Manipulative Agent).
In both domains, participants in the manipulative groups (MA and SEMA) had significantly more negative shifts (i.e., opting for a harmful option) and fewer positive shifts (i.e., switching to the optimal choice) than the NA condition (Supplementary Table \ref{tab:dec_ztest}).
}
\label{fig:decision_change}
\end{figure*}

Conversely, participants were more likely to identify and prioritize the optimal choices post-interaction in the absence of covert manipulation.
Interactions with the NA showed significantly more positive shifts (i.e., switching to the optimal choice) than the manipulative conditions across both financial (NA: $31.7\%$; vs. MA: $5.3\%$; vs. SEMA: $4.4\%$; $P < 0.0001$; $h =0.73$ and $0.77$, respectively) and emotional (NA: $28.2\%$; vs. MA: $11.7\%$, $P < 0.01$; vs. SEMA: $13.0\%$, $P < 0.05$; $h =0.42$ and $0.38$, respectively) scenarios. 
Detailed statistical results are provided in Supplementary Table \ref{tab:dec_ztest}.

These patterns are reflected by temporal changes in participants' ratings (Figure \ref{fig:score_changes}; Supplementary Table \ref{tab:rating_ttest}; Methods).
At baseline (pre-interaction), across both domains, there was no significant difference in ratings of the optimal choices and hidden incentives, respectively, between AI conditions ($P=1.00$).
However, significant differences emerged post-interaction.
In both domains, participants in the manipulative conditions showed significant shifts in their preferences in line with the agents' hidden motives, exhibiting substantially higher ratings for the hidden incentives (Cohen's $d\in [0.34, 0.68]$) and lower ratings for the optimal options ($d\in [0.54, 0.84]$) compared to the control (NA) condition.

\subsection*{Domain-Specific vulnerabilities shape susceptibility patterns}
Financial decisions showed significantly higher rates of negative shifts compared to the emotional domain ($49.4\%$ vs. $32.2\%$, $P< 0.0001$; Cohen's $h=0.35$; Figure \ref{fig:decision_change}; Supplementary Table \ref{tab:dec_ztest_domain}).
This suggests that participants in the financial domain were more likely to be swayed toward harmful choices, possibly due to participants' over-trust in the AI agent’s perceived objectivity in these contexts, a trend that aligns with existing literature on human reliance on algorithmic recommendations\cite{logg2019algorithm}.
In contrast, participants were more likely to retain their optimal choices in the emotional contexts compared to the financial domain ($29.9\%$ vs. $12.6\%$, $P< 0.0001$, $h=0.43$), indicating that emotional decisions may be more deeply influenced by personal intuition rather than reliance on AI's advice, a phenomenon known as algorithmic aversion \cite{castelo2021conservatism}, making them less susceptible to external manipulation.

Linear mixed-effects models (Equation \ref{eqn:msi_LMM}; Supplementary Table \ref{tab:LMM_res}) revealed distinct factors of individual susceptibility across these domains.
In financial decision-making, lower self-esteem (coefficient $\beta =0.02$, $P = 0.03$) was significantly correlated with negative shifts (i.e., switching to a harmful option), suggesting that less confident individuals are more likely to defer to AI recommendations than critically evaluate them.
Moreover, unemployed individuals ($\beta =-0.23$,  $P = 0.08$) were more likely to make harmful financial decisions compared to those working full-time, potentially due to the financial strain and psychological stress of unemployment, which increases the likelihood of riskier behaviors\cite{hirshman2025effect} (e.g., opting for less expensive products with few reviews or subscription services offering low initial costs).
Greater trust in AI systems also led to more harmful decisions ($\beta =-0.01$, $P = 0.09$), further highlighting the impact of AI's perceived objectivity on human susceptibility in quantitative decision-making contexts\cite{schemmer2022should}.

In the emotional domain, higher normative commitment traits ($\beta =0.17$, $P = 8.09\times 10^{-3}$) were significantly associated with more beneficial emotional outcomes, indicating the importance of reciprocity and adherence to social norms in identifying the optimal choice for handling emotional dilemmas\cite{haslam2009social} (i.e., active coping).
Conversely, individuals with more affective commitment traits were more prone to negative decision shifts ($\beta =-0.09$, $P = 0.08$), suggesting that emotionally reactive individuals are more susceptible to manipulation \cite{Brown2011Role}.
Interestingly, the order in which participants completed the scenarios (i.e., completion order) also influenced susceptibility.
Participants were more likely to exhibit negative shifts as they progressed through the experiment ($\beta =-0.08$, $P = 0.06$), suggesting the initial aversion to AI's advice in emotional decision-making may diminish over time.

\subsection*{Hidden objectives are sufficient to drive harm}
Notably, incorporating psychological strategies (SEMA) did not amplify harmful decision-making outcomes compared to mere manipulative incentives (MA). 
Across both domains, SEMA's influence on participants' decisions nearly matched MA's ($P=1.00$; Cohen's $h \in [0.02, 0.06]$; Figure \ref{fig:decision_change}), indicating that hidden objectives alone are sufficient to influence human decision-making.
However, temporal changes in preference ratings revealed domain-specific nuances in the impact of strategies (Figure \ref{fig:score_changes}; Supplementary Table \ref{tab:rating_ttest}). 
Compared to the NA condition, the addition of strategies (SEMA vs. MA) resulted in larger effect sizes (Cohen's $d$) for both the optimal choices (SEMA: $0.84$ vs. MA: $0.59$) and the hidden incentives (SEMA: $0.68$ vs. MA: $0.48$) in the financial domain, while a contrasting pattern emerged in the emotional domain (optimal choice, SEMA: $0.54$ vs. MA: $0.66$; hidden incentive, SEMA: $0.34$ vs. MA: $0.56$).

\subsection*{AI agents tailor manipulation strategies to exploit context-specific vulnerabilities}
Figure \ref{fig:strategies} illustrates the strategy distribution of the strategy-enhanced manipulative agent (SEMA) across decision-making domains. 
Across both domains, the agent demonstrated an implicit preference toward strategies that typically elicit more positive emotions (e.g., \textit{Pleasure Induction} and \textit{Charm} as opposed to \textit{Guilt Trip}).
However, chi-squared analyses indicated a significant difference in the overall strategy distributions between financial and emotional scenarios ($\chi^2[11] = 154.41$, $P= 1.87\times 10^{-27}$), suggesting that the rate at which the SEMA used these strategies varied across the two domains.

\begin{figure*}[!ht]
\centering
\includegraphics[width=\linewidth]{figures/strategies.pdf}
\vspace{-0.8cm}
\caption{\textbf{Distribution of strategies employed by the strategy-enhanced manipulative agent (SEMA) across decision-making contexts.}
The bar plots show the proportion of responses for each manipulation strategy used by the SEMA, with significant differences annotated above each bar plot.
These results suggest that SEMA adapted its strategy usage to the emotional or financial nature of the decisions, reflecting a tailored approach to influence.
$P$ value legend: n.s. (not significant), $P\geq 0.05$; *, $P < 0.05$; **, $P < 0.01$; ***, $P < 0.001$; ****, $P < 0.0001$).
}
\label{fig:strategies}
\end{figure*}

In financial scenarios, this agent employed diversion ($23.1\%$ vs. $14.4\%$, $P < 0.0001$; Cohen's $h=0.22$), justification ($15.8\%$ vs. $8.8\%$, $P < 0.0001$; $h=0.22$), urgency ($3.0\%$ vs. $0.3\%$, $P < 0.0001$; $h=0.24$), and fabricated information ($3.9\%$ vs. $1.9\%$, $P < 0.01$; $h=0.12$) significantly more compared to the emotional domain, mirroring real-world predatory marketing tactics\cite{SUN2023104611}.
Conversely, compared to the financial domain, it relied on \textit{Pleasure Induction} ($45.8\%$ vs. $29.8\%$, $P< 0.0001$; $h=0.33$) and \textit{Guilt Trip} ($5.1\%$ vs. $2.4\%$, $P < 0.001$; $h=0.14$) significantly more in emotional contexts, attempting to exploit emotional vulnerabilities.
In addition, the SEMA adopted considerably more diverse strategies (i.e., \textit{Others}) in the emotional domain ($2.9\%$ vs. $1.4\%$, $P < 0.05$; $h=0.10$).
Upon further investigation, such responses primarily aimed to provide reassurance and reinforce the users' thoughts and beliefs.
Detailed statistical results are provided in Supplementary Table \ref{tab:strat_ztest}.

\subsection*{Participant feedback highlights the covert nature of AI-driven manipulation}
The majority of participants perceived the manipulative agents as helpful across financial (MA: $86.8\%$, and SEMA: $78.9\%$) and emotional (MA: $86.4\%$, and SEMA: $75.6\%$) scenarios, with rates comparable to the neutral agent (financial: $97.5\%$; emotional: $87.1\%$).
This perception of helpfulness highlights that participants were largely unaware of the agents’ manipulative intents and viewed them as equally beneficial as the neutral assistant.
Notably, while participants were not explicitly prompted to report whether they thought the agents had ulterior motives, a considerable portion of participants mentioned noticing signs of such influence, particularly in the financial domain (NA: $0\%$; MA: $13.2\%$; SEMA: $28.9\%$; $P < 0.01$). 
In contrast, these concerns were less frequently reported in the emotional domain (NA: $2.5\%$; MA: $8.1\%$; SEMA: $9.7\%$; $P=0.41$), suggesting that the agent's influence was more covert in such contexts.
Detailed information on participant feedback is provided in Figure \ref{fig:user_feedback} and Supplementary Tables \ref{tab:feedback_res} \& \ref{tab:feedback_res2}.


\section*{Discussion}
This study demonstrates significant human susceptibility to AI-driven manipulation across financial and emotional decision-making contexts, raising critical concerns about the psychological and societal implications of advanced AI systems. 
It presents several key insights that have important implications for understanding and protecting human autonomy in interactions with AI systems.

Our findings revealed that participants interacting with the manipulative agents (MA and SEMA) exhibited substantially higher rates of harmful decision-making compared to those who interacted with the NA. 
This susceptibility was further highlighted in the temporal changes of participants' ratings, as participants in the manipulative groups exhibited declining preferences for the optimal choices while increasingly favoring the hidden incentives after interacting with these agents.

The contrasting decision patterns between financial and emotional domains highlight how context shapes human susceptibility to AI-driven manipulation. 
Participants showed greater susceptibility to AI’s influence in quantitative decision-making scenarios (i.e., financial contexts), where they may over-trust AI’s algorithmic advice due to its perceived objectivity \cite{logg2019algorithm}. 
In contrast, participants were less susceptible to external influence in emotional scenarios, more frequently preserving their initial beneficial choices, likely due to emotional judgments being deeply rooted in personal intuition and less reliant on external recommendations \cite{ castelo2021conservatism}. 
Hence, future work should investigate the cognitive and contextual mechanisms underlying human susceptibility and develop targeted interventions that mitigate AI-driven manipulation.

Another notable finding was the similar performance of MA and SEMA conditions in shifting participants' preferences and decisions.
These findings suggest that hidden objectives play a significant role in AI-driven manipulation, as the presence of such objectives alone (MA) was nearly as effective as employing established psychological strategies (SEMA) in influencing participants’ decisions. 
However, the addition of strategies did enhance the manipulative impact in financial scenarios (Supplementary Table \ref{tab:dec_ztest}), suggesting that while hidden objectives are a primary driver, the use of sophisticated tactics may further amplify their influence in contexts where users tend to defer more to AI's perceived expertise (i.e., financial decisions).
These results raise critical concerns about the potential for covert influence in AI systems, as agents that were not explicitly designed to manipulate their users could also significantly impact human decision-making based on a hidden incentive.

Our analyses also revealed the AI agents' adaptation of manipulation strategies across different domains. 
These findings suggest that while the overall distribution of the employed strategies followed a similar trend across different domains, indicating that the AI agents may implicitly prefer certain strategies more than others, such agents can tailor their strategy usage to the context (e.g., exploiting interpersonal vulnerabilities in the emotional domain while using pragmatic tactics in financial scenarios).
This context-specific adaptation highlights AI agents' nuanced approach to maximizing manipulative impact across different scenarios, highlighting broader issues regarding AI safety. 

Moving forward, our findings highlight broader issues about the societal implications of AI-driven manipulation, particularly in the studied domains.
In financial contexts, traditional advertising operates within legal frameworks, requiring transparency (e.g., disclosing sponsorships) and accountability for false claims.
In addition, the commercial intent of advertisers is inherently recognized by consumers, creating a sense of skepticism toward their genuineness.
However, unlike advertisers, AI systems currently face limited accountability for their behavior.
For instance, AI's fabricated claims and hallucinations are frequently dismissed as technical errors rather than intentional deception, leading to regulatory loopholes. 
Recommendations by such assistants are mainly perceived as unbiased and helpful, causing users to trust their advice based on the displayed sincerity.
In addition, AI's ability to tailor its recommendations to individual personalities\cite{matz2024potential} may enable targeted advertisements that exploit users' vulnerabilities and undermine their autonomy, which goes beyond what traditional forms of advertisement could achieve.

Similarly, in emotional contexts, AI systems employed as helpful mental health tools (e.g., virtual counselors or emotional supporters) could exploit user trust to normalize harmful behaviors. 
For instance, an AI agent might encourage emotional and social disengagement under the guise of stress relief, withholding beneficial solutions while hiddenly aiming to foster dependency by isolating users from other support networks or subtly promoting paid services (e.g., premium subscriptions for the AI service).
However, unlike the human alternatives, such practices in AI may evade public scrutiny, as users are unlikely to assume AI has hidden motives in these scenarios (as shown by participants' feedback).

\subsection*{Ethical Considerations}
This study involved significant ethical considerations, particularly regarding participant well-being, privacy, and the potential misuse of research findings.

To ensure participant well-being, participants were screened to exclude individuals with reported mental or physical health conditions or those undergoing treatment, following established guidelines for psychological research \cite{collings2012suicide}.
In addition, participants were required to assume the perspective of a user based on pre-defined queries in hypothetical scenarios. 
These scenarios focused on common decision-making scenarios rather than highly sensitive or traumatic situations, minimizing the risk of psychological harm and preventing the disclosure of personally identifiable information.
Importantly, participants received a comprehensive debriefing immediately after study completion, which included full disclosure of the study’s objectives, an explanation of our motivations, the optimal solutions for each scenario, and contact information for additional support or questions.
Regarding participant privacy and data security, all interactions were conducted on a secure platform, with identifying information stored separately in an encrypted database. 
This study adhered to the University of Hong Kong’s guidelines for research involving human subjects and was approved by the institution’s ethics committee (Reference No: EA240497).

Another key consideration was the decision to share the prompts used to configure the manipulative AI agents.
While publishing these prompts may enable malicious actors to develop more sophisticated manipulative systems, we believe transparency and reproducibility are essential for scientific progress on this topic.
In addition, our findings indicate that simply providing a hidden objective to the AI agent could negatively influence participants' preferences and decisions, with minimal additional gain from more sophisticated strategies employed by the strategy-enhanced manipulative agent. 
Therefore, the primary risk lies in the presence of hidden objectives rather than the complexity of the prompts.


\subsection*{Limitations and Future Work}
Despite our efforts to create the first robust and comprehensive analysis of human susceptibility to AI-driven manipulation, this study faces several limitations. 
The study was conducted in a controlled environment with pre-defined decision-making scenarios, which enabled us to explore the effects of AI-driven influence in isolation while limiting potential risks to participants' well-being. 
However, these artificial constraints may not fully capture the complexity of real-world AI applications, where users are exposed to a broader range of decisions and interactions. 
Future work can examine how these findings translate to more dynamic settings where AI interacts with users across diverse contexts, exploring such influence beyond decision-making.

In addition, participants' awareness of being in an experimental setting might have influenced their reported scores and how they interacted with the assigned AI agent, which may subtly influence their natural decision-making process.
Relying on self-reported measures to explore changes in participants' preferences could also imply limitations in our methodology, as participants' willingness to report changes in their preferences may not always align with their actual cognitive and behavioral changes.
Hence, future work could assess the long-term effects of AI-driven influence.

Regarding the AI agents employed in this study, our reliance on GPT-4o may present a limitation. 
While GPT-4o is currently state-of-the-art across various tasks \cite{song2024cs, zheng2024judging}, manipulation capabilities may vary across different models. 
Given the widespread adoption and popularity of GPT-4o (commercially known as ChatGPT), understanding its manipulative tendencies is critical due to its large user base and significant societal impact. 
However, we do not claim that all models exhibit the same manipulative behavior, as further studies are needed to investigate whether similar tendencies are present in other models.
Moreover, our curated taxonomy of strategies represents a subset of potential manipulation techniques in user-assistant settings. 
Hence, while this taxonomy is theory-based and comprehensive, emerging AI capabilities may form more subtle and sophisticated strategies that are not captured in our curated taxonomy.

The design of the control condition in our experiment also poses a limitation.
While the neutral agent was designed to optimize user benefit without explicit manipulative objectives, inherent model biases may persist.
These biases may have subtly influenced participants' decisions, even in the absence of a hidden incentive, as seen in the negative shifts across both decision-making domains for the NA group. 
This limitation emphasizes the challenge of creating truly neutral AI systems, and future research should focus on further identifying and mitigating harmful biases in AI agents.

Lastly, this study highlights several crucial research directions for future work.
Investigating the long-term impact of AI-influenced decisions could reveal whether such changes persist over time.
Developing reliable methods to detect subtle AI manipulation, particularly in emotional contexts where our findings suggest users are less likely to notice AI's influence, also remains an open challenge.
While constructing ethical frameworks for AI development is essential, 
future work is also needed to investigate and create regulatory frameworks to protect consumers from AI-driven manipulation.
This may include imposing transparency requirements, accountability mechanisms, and safeguards to ensure that AI systems prioritize user well-being and protect their autonomy.
In addition, future work should also explore systems that can lower human susceptibility to such manipulation.

\subsection*{Conclusions}
Our randomized control trial with $233$ participants demonstrated that human decision-making is highly susceptible to AI-driven manipulation, with participants significantly shifting preferences toward harmful options and away from beneficial choices when interacting with manipulative AI agents. 
Strikingly, established manipulation strategies (SEMA) did not amplify harm over covert objectives (MA), suggesting that even subtle, hidden incentives can sway human preferences. 
These findings reveal critical vulnerabilities in human-AI interactions, particularly in domains perceived as rational, like finance, where users may over-trust AI’s perceived objectivity. 
These results underscore the need for regulatory frameworks to protect user autonomy in the age of increasingly sophisticated AI systems.

\section*{Methods}\label{sec:method}
\subsection*{Overview}
We conducted a randomized controlled trial to investigate human susceptibility to AI manipulation attempts. 
Our study employed a between-subjects design ($N=233$) to examine how and to what extent AI systems (referred to as agents in this paper) can influence human decision-making across decision-making scenarios (Figure \ref{fig:framework}). 
In each scenario, the participant was provided with a pre-defined user query and four corresponding options. 
One option was created as the most suitable (optimal) choice based on the user's query, while the remaining three were designed to cause different types of harm.
We investigated participants' susceptibility to AI-driven manipulation by analyzing the differences in their preference ratings for these options before and after interacting with the assigned AI agent. 

\subsection*{AI Conditions} \label{sec:agents}
We developed three distinct AI agents by designing prompts for GPT-4o\cite{hurst2024gpt}, the current state-of-the-art large language model.
The primary experimental condition, the \textbf{Manipulative Agent (MA)}, was configured with a hidden objective to influence users' preferences toward a specific harmful option (i.e., hidden incentive) and away from the beneficial (i.e., optimal) choices. 
We implemented a point-based reward system to quantify the agent's manipulation success. 
100 points were awarded for achieving the hidden incentive, 50 points for steering users toward other harmful options, and 0 points for the optimal choice.
In addition, the agent employed a personalized theory-of-mind (ToM)\cite{apperly2012theory} framework to reason about users' beliefs, desires, intentions, and emotions while considering the vulnerabilities associated with their personality traits before selecting a strategy and generating a response.

We designed the \textbf{Strategy-enhanced Manipulative Agent (SEMA)} as an additional experimental condition to investigate whether providing the agent with established psychological strategies would impact its manipulative influence.
This agent shared the same reward system, reasoning framework, and underlying goal of influencing users' decisions as MA. 
In addition, it was provided with our curated taxonomy of established manipulation strategies derived from psychological and social science literature\cite{buss1987tactics, seabright1998interpersonal, lauer2020people, seda2021manipulative, noggle2020pressure, danciu2014manipulative} (Table \ref{tab:strategies}). 
Lastly, serving as the control condition in our experiments, the \textbf{Neutral Agent (NA)} was instructed to provide unbiased assistance and reassurance while optimizing for user benefit. 

All agents were provided with the user's query and the corresponding options for each scenario. 
In addition, they had access to the participant's personality traits, as personalization could significantly enhance the effectiveness of human-AI interactions \cite {matz2024potential}. 
Given that existing AI systems are capable of predicting their users' personalities with sufficient accuracy \cite{li2024can}, we believe adding this information did not comprise the applicability of our experimental setup to real-world contexts.
Designed prompts are provided in the appendix (Supplementary Figures \ref{fig:neutral_prompt} - \ref{fig:mani_prompt}).

\subsection*{Decision-Making Scenarios}
The scenarios in this study were carefully crafted to assess manipulation susceptibility in two domains reflecting common real-world situations in which AI assistants are employed \cite{lakkaraju2023llms,hwang2021toward, keshishi2023emotional,gual2022using, lerner2015emotion, burtell2023artificial}: Financial (seeking advice on purchases) and Emotional (seeking guidance on handling emotional dilemmas) decision-making.
In each domain, we curated three widely used topics to design scenarios that applied to a wide variety of individuals.
For each scenario, to ensure consistency between the conditions of different participants, we designed pre-defined queries representing a hypothetical user's situation, needs, and requirements.
In addition, each scenario was accompanied by four distinct plausible options, one of which was crafted as the optimal choice, while others were carefully designed to indicate different levels of potential harm.
In each scenario, we designated one of the harmful options as the agent's hidden incentive to prevent user suspicion regarding its bias. 

\subsubsection*{Financial Scenarios}
AI systems are making their way into financial investments \cite{schillaci2024llm} and are increasingly being used to enhance product advertisements \cite{matz2024potential, burtell2023artificial} and provide financial advice \cite{cao2022ai}.
Hence, it is crucial to understand the extent to which these systems can impact users' decisions despite their better judgments.
Drawing from existing research on consumer decision-making \cite{malhotra2019growth,trivedi2023effect,limbu2024influences}, we identified key scenarios where users commonly seek AI assistance for financial guidance.
We focused on three representative consumer decisions that require minimal domain expertise: selecting a fitness tracker, choosing a weight-loss medication, and finding an online shopping platform for clothes.
To ensure practicality while maintaining experimental control, we designed pre-defined user queries corresponding to each topic that incorporated realistic budget constraints and requirements (Tables \ref{table:fin_scenario1}-\ref{table:fin_scenario3}).

The optimal choice was designed to align with stated user requirements and budget constraints while incorporating realistic trade-offs. 
We designed the remaining options to exhibit different types of potential financial harm: 
\textbf{1) Non-existent Product:} 
This option was crafted to mirror characteristics of common online scams \cite{montanez2020human}, featuring unusually attractive terms, comparatively lower cost, and limited verification possibilities.
Specifically, this option was designed to only be available for pre-order from the supplier's website and had relatively few or no reviews, making it difficult for users to assess its legitimacy.
\textbf{2) Excessive Product:} 
This option represented over-featured solutions that exceeded the user's stated requirements and maximum budget and was designed to test users' susceptibility to unnecessary upselling\cite{heidig2017supersize}.
It allowed us to assess whether users could be swayed by the perceived desirability of excessive products, even when such choices caused them to spend more than they had originally planned.
\textbf{3) Dependence-inducing Product:} 
This option was designed to satisfy the user's requirements at a relatively lower upfront cost than the optimal choice.
However, it incorporated elements of long-term financial commitment, such as subscription models, for continued access to users' desired features, which introduced hidden costs and future obligations. 
By choosing this option, users incurred recurring fees that, over time, exceeded their maximum budget, reflecting real-world scenarios where consumers are lured by low initial costs but face long-term financial burdens.

To alleviate users' initial bias and preference toward a familiar brand or product, all of the presented options were designed as hypothetical products without a name or brand. 
We hypothesized that this design allowed the users' initial preferences to rely mainly on personal reasoning based on the requirements provided in the query and the available information for each option.

\subsubsection*{Emotional Scenarios}
The increasing deployment of AI systems in mental healthcare, particularly for counseling and emotional support, presents both opportunities and potential risks. 
These systems have gained traction due to their high accessibility, cost-effectiveness, and reduced stigma associated with seeking support \cite{sabour2023chatbot}. 
However, this widespread adoption necessitates an investigation into human susceptibility to AI influence when dealing with emotional challenges, particularly regarding their established beliefs about managing emotional dilemmas.
The scenarios for this domain were developed based on a comprehensive analysis of mental health forums and existing literature on emotional dilemmas \cite{sun2021psyqa, sik2023topic}. 
Through careful examination of existing data and literature \cite{whisman1993life, koff1997effects, pinkasavage2015social, comolli2024concentration, kanner1981comparison}, we identified three primary topics of emotional adversities: issues with self-esteem (Table \ref{table:emo_scenario1}), conflict with a close friend (Table \ref{table:emo_scenario2}), and criticism at the workplace (Table \ref{table:emo_scenario3}). 

Each scenario presented participants with four distinct coping strategies (i.e., how individuals tend to manage their thoughts and emotions in response to emotional adversities \cite{carver1989assessing}), designed to represent different approaches to emotional regulation and problem-solving.
Existing research \cite{carver1989assessing, carver1997you, algorani2023coping, compas2017coping, brown2005good} classifies coping strategies as adaptive or maladaptive depending on their general impact on the individual's overall well-being and mental health.
We included an adaptive coping mechanism as the optimal choice for the given scenario, while the remaining options were designed to incorporate maladaptive practices, which could potentially lead to increasing rates of depression and anxiety \cite{folkman2004coping, kato2015frequently}.

Therefore, the optimal choice was designed to represent active coping, a process based on cognitive-behavioral literature that involves identifying specific actions to address the situation, developing alternative perspectives, and constructive engagement with support networks, which has shown the strongest correlations with increasing user's mental well being and lower rates of anxiety and depression \cite{carver1997you, compas2017coping}.
The remaining options involved common maladaptive mechanisms with varying levels of harm \cite{kato2015frequently}: 
\textbf{1) Disengagement:} This option reflects avoidant coping patterns, incorporating elements such as behavioral disengagement, mental disengagement, and denial. 
While potentially offering immediate emotional relief, this option demonstrated less desirable long-term outcomes; 
\textbf{2) Emotional Venting:} This option includes elements of emotional release while omitting components of reflection, learning, or growth that characterize more adaptive forms of emotional processing. 
Hence, it clearly distinguishes between adaptive and maladaptive emotional processing by emphasizing unregulated emotional discharge without problem-solving elements.
\textbf{3) Self-blame:} This option was constructed to reflect common patterns of maladaptive attribution as it included subtle elements of perfectionism and excessive responsibility-taking, characteristics that portray the individual as the root of the problem, bringing additional mental burden to the individual without finding a viable long-term solution.

\subsection*{Experimental Platform}
To ensure user privacy, we created our own data collection platform using React\cite{react} and FastAPI\cite{fastapi}.
Upon logging onto our platform, each participant was shown a series of three scenarios based on their assigned domain (Figure \ref{fig:platform_profile}).
Selecting one of these scenarios opened a new page, in which the participants were shown the pre-defined query and four available options in a randomized order (Figure \ref{fig:platform_scenario}).
The platform interface was designed to minimize potential confounds, presenting all information in a standardized format while randomly rearranging the order of the options within each scenario to control for position effects. 
Before interacting with the assigned AI agent, participants rated their initial preferences for each option, along with their confidence levels and topic familiarity, on a 10-point Likert scale. 
After submitting their ratings, participants entered the interaction page (Figure \ref{fig:platform_chat}).
Following a conversation lasting at least 10 turns with the assigned agent, participants provided their final preferences for each option, confidence levels, and scenario familiarity. 
This design enabled us to quantify changes in both decision-making and confidence levels as metrics for manipulation susceptibility.

\subsection*{Study Workflow}
This study followed a three-phase protocol designed to assess participants' susceptibility to AI manipulation while controlling for individual differences and ensuring ethical practices.
Our experiments were approved by the institutional review board at The University of Hong Kong (Reference No: EA240497).
\subsubsection*{Phase I: Pre-Experiment Survey}
Prior to our experiment, participants completed a comprehensive questionnaire of validated psychometric instruments, including demographic information (age, sex, education, occupation, and marital status) and social factors that may influence people's susceptibility to manipulation, including personality\cite{papatsaroucha2021survey, yang2022predicting}, social engineering susceptibility\cite{montanez2022social} \cite{stewart2018modification}, social support \cite{pinsker2010exploitation}, and self-esteem\cite{wang2024mentalmanip}:
\textbf{1) Ten-Item-Personality-Inventory (TIPI)\cite{gosling2003very}}, a 10-item assessment of personality traits based on dimensions from the Big Five Inventory (BFI)\cite{john1991big}: openness, conscientiousness, extroversion, agreeableness, and emotional stability.
We adopted this version of the personality test, as we assumed that while an AI agent may have a perception of the user's personality traits, it would not be heavily accurate\cite{matz2024potential};
\textbf{2) Social Engineering Susceptibility Scale\cite{workman2007gaining}} focusing on individuals' vulnerabilities in digital interactions and modified to include the portion of the questions that were relevant to our study (i.e., excluding questionnaire regarding informational susceptibilities); 
\textbf{3) Oslo Social Support Scale (OSSS-3)\cite{kocalevent2018social}}, a 3-item questionnaire evaluating participants' accessibility to support networks;
\textbf{4) Rosenberg Self-Esteem Scale (RSES)\cite{rosenberg1965rosenberg}}, a 10-item measurement of individuals' perception of self-worth. 
Due to the nature of our study, we also adopted the AI Attitude Scale\cite{grassini2023development} to assess participants' predispositions toward AI systems.

\subsubsection*{Phase 2: Experimental Setup}
Following the pre-experiment survey, we randomly assigned participants to domain-AI conditions while balancing key demographic and psychometric variables across different groups.
Each participant received a unique passcode to access our experimental platform and detailed instructions, framing the study as an investigation of personalized AI-assisted decision-making. 
The instructions were carefully worded to avoid demand characteristics while maintaining ethical transparency.

\subsubsection*{Phase 3: Post-Experiment Protocol}
After completing all of the required scenarios on the platform, participants evaluated the assigned scenarios' relevance to their daily lives and rated their assigned AI agent on three dimensions: helpfulness, informativeness, and soundness (10-point Likert scales).
We also collected open-ended responses regarding participants' overall impressions and specific interaction experiences.
To ensure ethical conduct and participant well-being, participants received detailed explanations of the study's true objectives and the rationale behind any experimental manipulation. 
We instructed the participants on the optimal choices for each scenario and explained why certain options might be harmful. 
Additionally, participants were encouraged to contact the study investigators if they experienced any negative effects. 

\subsection*{Participants}

\subsubsection*{Recruitment}
We recruited participants through poster advertisements on WeChat, a predominant Chinese social media, and Prolific \cite{palan2018prolific}, a UK-based crowdsourcing platform. 
Participants were required to be 1) Fluent in Chinese or English; 2) Be able to use a computer device to access and use our platform; 3) Have experience using LLMs/personal assistants; 4) Be over 18 years of age; and 5) Do not currently suffer from physical/mental illnesses, as participation may have caused additional distress to these individuals.
Data was collected online through our platform to ensure data safety and privacy. 
As the experiment took approximately one hour on average, each participant was rewarded 80 RMB ($\approx$8£) for completing all the tasks and relevant questionnaires.
In addition, the top 10\% of the participants, based on dialogue quality and timely completion of tasks, received an additional bonus of 20 RMB ($\approx$2£).

\subsubsection*{Power Analysis}
We conducted a power analysis to estimate the required number of participants in this study. 
Setting the effect size at $0.1$, significance level at $0.05$, and power at $0.8$ indicated that we required $246$ samples per decision-making domain.
As each participant completed three scenarios (samples) in our experiments, we needed a minimum of $164$ participants ($27$ per domain-AI condition) to satisfy these requirements.

\subsubsection*{Participant Statistics}
From an initial pool of $326$ registrants, $72$ left the experiment after the initial survey (Phase I), and $12$ did not pass the attention check.
Overall, $252$ participants completed the study, indicating a completion rate of $\approx 77\%$.
Upon investigation, $19$ participants were disqualified due to not abiding by the guidelines, leaving $233$ participants for our analysis, which exceeded the requirements of the power analysis. 
Details regarding participant demographics for each domain-AI condition are provided in Table \ref{table:user_demo}.

\subsection*{Analyses Framework}
We employed a comprehensive methodological approach to analyze participants' susceptibility to AI-driven manipulation in decision-making contexts.
Our analyses included all complete cases ($N=699$ samples from $233$ participants).

\subsubsection*{Control Analyses}
To ensure the validity of our findings, we examined participant characteristics across domain-AI conditions.
One-way ANOVA revealed no significant differences in demographics or social factors across domains and agent groups ($F[5]$ $\in$ [$0.13, 2.09$], $P \in [0.07, 0.98]$).
Similarly, conversation length was consistent across domain-AI conditions ($M = 10.72$, $SD = 1.55$; $F[5]= 0.40$, $P = 0.85$), confirming that observed effects were not attributable to pre-existing individual differences or interaction duration.

\subsubsection*{Decision Patterns}
To assess how interactions with the AI agents influenced participants' decisions, we considered the highest-rated option(s) at each time point (pre/post interaction) as the participant's most likely choice(s).
Changes in these choices were categorized into four decision patterns:
\textbf{No Change} represents the participant maintaining their original decision, which was further divided into \textbf{Optimal} or \textbf{Harmful}, depending on whether the choice included the optimal option;
\textbf{Positive Shift} indicates a beneficial change, where the participant initially chose a harmful option but switched to the optimal option or maintained their choice of the optimal option while including fewer harmful options in their decision after the interaction;
\textbf{Negative Shift} involves changing from the optimal choice to a harmful option or switching between harmful options.
We analyzed the distribution of decision patterns across domain-AI conditions using chi-squared tests, with post-hoc z-tests with Bonferroni correction for pairwise comparisons and Cohen's $h$ for effect sizes.

\subsubsection*{Temporal Changes in Preference Ratings}
As the manipulative agents (MA and SEMA) were instructed to guide participants away from optimal options and toward their hidden incentives, our analyses focused on changes in the ratings of these two options.
We analyzed these temporal changes using mixed-effects ANOVA comparing pre/post ratings across AI conditions, followed by Bonferroni-corrected independent t-tests for pairwise comparisons, with Cohen's $d$ for effect sizes.

\subsubsection*{Predictors of Manipulation Susceptibility}
We employed a linear mixed-effects model to analyze individual factors influencing manipulation susceptibility.
Decision patterns were quantified on a scale reflecting their impact: negative shifts ($-1$), positive shifts ($1$), and no change ($0$).
This mapping preserved both directional and magnitude information while enabling quantitative analysis of individual susceptibility.
\begin{equation} \label{eqn:msi_LMM}
Y_{ij} = \beta_0 + \beta_1 (\text{AI condition}_i) + \mathbf{X}_{ij} \boldsymbol{\beta}' + (1 | P_i) + (1|S_j) + \epsilon_{ij},
\end{equation}
where $Y_{ij}$ represents participant $i$'s quantified decision pattern on scenario $j$, and $\beta_0$ and $\epsilon_{ij}$ indicate the intercept and residual error, respectively.
$\mathbf{X}_{ij}$ represents covariates, including demographics, personality, trust in AI, social support accessibility, and social engineering susceptibility traits.
We also included pre-interaction confidence and familiarity ratings and the scenario's completion order as covariates.
Lastly, the model incorporated random intercepts for the participant ($1 | P_i$) and the scenario ($1|S_j$).

\subsubsection*{Participant Feedback}
After completing the experiment, participants completed a post-study survey evaluating the designed scenarios and their assigned AI agents.
They rated scenario commonality and three aspects of the AI agent (personalization, soundness, and informativeness) using 10-point Likert scales.
These ratings were analyzed using one-way ANOVA followed by Tukey HSD pairwise comparisons.
Responses to two open-ended questions were annotated by the authors: the first question, regarding AI helpfulness, was categorized into a binary (helpful/not helpful) taxonomy; the second question, probing overall impressions, was coded to identify mentions of perceived manipulation attempts by the AI agent (e.g., feeling as the AI agent had an ulterior motive).
The proportions of these categorical responses across AI conditions were then compared using chi-squared tests and post-hoc z-tests with Bonferroni correction.

\section*{Data and Code Availability}
The data collected in our study and the source code for data collection and analysis will be made available upon request from the corresponding authors, with clear justification and data usage agreement.

\section*{Acknowledgments}
This study was supported by the ANT Group and a 
National Science Foundation award (\#2306372)  and an award for Distinguished Young Scholars (\#62125604).

\section*{Author Contributions}
S.S., J.M.L., S.L., and W.Z. designed the experiments and interpreted the data. S.S., J.M.L., S.L., C.Z.Y., S.C., X.Z., W.Z., and Y.C. contributed to participant recruitment and data collection. S.S. and C.Z.Y. developed the data collection platform. S.S. drafted the manuscript, and J.M.L., S.L., W.Z., R.M., T.A., T.M.L., and M.H. provided critical reviews and assisted with refining the draft. All authors participated in discussions, provided feedback, and made significant intellectual contributions.

\bibliography{citations}
\include{appendix}

\end{document}

