\section{Introduction}
\label{sec:intro}

%-------------------------------------------------------------------------

Recent Vision Language Models (VLMs) \citep{gpt4o, Sonnet, gemini} have shown tremendous promise in a wide range of real-world applications, such as autonomous driving \citep{ad,RAG-drive}, robotics \citep{rt2, manipulate, pgvlm2024}, personalized virtual assistants \citep{assistant}, search engines \citep{search} and recommendation \citep{rec}. However, the ever-growing size of these recent VLMs has made at-scale deployment and operation challenging due to high consumption of cloud computing resource, high latency, and expensive API calls. 

In response to this, there has been research focusing on developing smaller VLMs for on-device capabilities or cheap inference, such as MobileVLM-1.7B \citep{mobilevlm,mobilevlm2}, GPT-4o-mini \citep{gpt4omini} and Qwen-VL 7B \citep{Qwen2VL} etc. Unfortunately, one could not simply replace the larger VLM with a smaller one and expect the result to be within some practical tolerance, as the performance gap between large and small VLMs is still too \textit{huge}  (\Cref{fig:experiment_subindex_scatterplot}): compared to the impressive performance of large VLMs, some smaller VLMs offer only marginal improvements over random guessing on challenging benchmarks such as MMMU \citep{mmmu}.

\subfile{figures/intro_figure}

In this paper, we seek a middle ground between smaller and larger models through collaboration between models of different sizes. To enhance on-the-fly performance of smaller models for a cost-effective system, we propose \textit{Cache of Thought (CoT)}, a master-apprentice framework that enables small (\textit{apprentice}) VLMs to generate responses of significant closer quality versus large (\textit{master}) VLMs via a novel design of dynamic \textit{Cache}. The Cache stores historical high-quality answer responses generated by master VLMs, which serves as a guidance for apprentice VLM query answering. Inspired by the principle of well-established case-based reasoning \citep{cbr}, CoT performs this guidance from master to apprentice via a specialized form of Retrieval-Augmented Generation (RAG): when CoT selects apprentice VLMs for question answering, it retrieves the most similar historical queries and responses in the cache to provide to the apprentice VLM as in-context examples. Through in-context learning, apprentice can benefit from the accumulated cached history of master, and become more capable of handling new queries. Notably, since the cache is allowed to grow, it is expected that the quality and relevancy of in-context samples should grow as well, leading to further improved capabilities of the apprentice VLM and the overall system. From the practicality perspective, CoT's in-context learning incurs negligible cost: it prepends the retrieved queries to the inference prompt of apprentice VLMs without introducing any extra training workloads, additional data annotations, nor recomputation expenses. CoT is also complementary to other VLM serving strategies and highly adaptable: as VLM routing and selection strategies mature, CoT can be easily integrated, continuing to offer significant benefits to smaller VLMs.

To the best of our knowledge, CoT is the first framework that demonstrates VLM in-context learning, combined with multimodal retrieval, can be applied effectively to real-world VQA tasks with lengthy question prompts (e.g., MMMU \citep{mmmu}). In comparison, previous works explored VLM in-context learning under more constrained conditions, such as short textual image descriptions or questions of several words (e.g. "what is in the image") \citep{flamingo, manyshot}, purely visual contexts \citep{visualcontext}, or synthetic datasets with short, simple questions \citep{vl-icl}. These works often relied on straightforward top-$k$ image embeddings (or even random selection) for selecting in-context examples and utilized ground-truth (rather than model-generated) responses. Prior to this work, it was unclear which retrieval methods were most appropriate for dual-modality similarity, especially for complex VQA questions and in-context learning scenarios. CoT proposes and systematically evaluates various retrieval techniques (e.g., CLIP-based text+image embeddings, keyword extraction, keyword clustering), and maintains a dynamic cache of master VLM-generated responses, thereby expanding the scope and effectiveness of in-context learning for general, real-world VQA tasks without using ground truths.

In summary, we contribute (1) CoT, a general master-apprentice framework for multi-VLM inference with a cache, (2) an effective multi-modal retrieval and in-context learning approach that boosts apprentice VLM performance on the fly without training, and (3) extensive experiments demonstrating the effectiveness of CoT's multi-modal in-context learning and desirability of CoT's cost-response quality trade-off for general VQA.

\begin{figure*}[]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/main_mmmu.pdf}
    \vspace{-4mm}
	\caption{Apprentice VLM answers new query with help of past similar cases answered by the master VLM:\textit{ Multi-modal Retrieval and In-context learning}. Images and prompt examples cited from MMMU \citep{mmmu}}
	\label{fig:RAG}
    \vspace{-4mm}
\end{figure*}
