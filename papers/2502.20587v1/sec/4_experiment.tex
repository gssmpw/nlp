\section{Experiment}
\label{sec:experiment}
This section describes our experimental evaluation of CoT. We design our experiments as follows: (1) We evaluate the performance gains on general VQA tasks achieved by CoT's applying of in-context learning to apprentice VLMs via multi-modal retrieval.
(2) We evaluate the sensitivity of CoT to different system configurations such as cache size and choice of retrieval methods.
(3) We evaluate CoT in both static and dynamic configurations (explained shortly). 
(4) We study CoT's trade-offs between performance and cost advantage (metric defined by \citep{hybridllm}) by adjusting the ratio of master and apprentice VLM usage.

Specifically, for (3), in the static configuration, We pre-construct $\mathcal{L}_t$ using offline data, which remains unchanged in each round $t$. We also fix the model multiplexer $J$ to only select the apprentice VLM. These settings allow us to evaluate how much the apprentice VLM potentially benefits from CoT under a static cache. In the dynamic configuration, CoT processes a continuous data stream and uses a dynamic cache $\mathcal{L}_t$ (explained shortly) which may start either empty or containing offline data. The model multiplexer randomly selects between the apprentice and master VLM according to a specified probability (e.g., a 10\% apprentice VLM rate means that it is selected for query serving 10\% of the time). After each round, CoT inserts into the cache with the new QA-pair (when the master VLM is used) and may perform eviction (when the cache is at capacity) to maintain cache effectiveness for downstream tasks. This setup allows us to evaluate both CoT's performance-cost trade-offs and capability to adapt over time.

\subsection{Experiment Datasets}

\textbf{MMMU} \citep{mmmu} is a widely recognized and challenging multi-disciplinary VLM benchmark that organizes subjects into a hierarchical structure. It evaluates VLMs on both breadth and depth of subject knowledge, as well as on their expert-level reasoning and understanding. MMMU covers complex VQA tasks across six major fields, ranging from art to science and businesses. The dataset consists of 11.5K questions, divided into development (\textit{dev}), validation (\textit{val}), and test sets (the test set does not include ground truth answers). To ensure fair evaluation of in-context learning, we filter out instances containing multiple images, as most off-the-shelf VLMs are less capable of handling multi-image questions, which could interfere with their in-context learning capabilities. After filtering, the sizes of the dev, val and test set are 146, 857, 9702 respectively.

\textbf{VL-ICL} \citep{vl-icl} is a pioneering synthetic benchmark designed to measure the broader capabilities and limitations of multi-modal in-context learning. We employ this dataset to examine whether CoT retains the benefits of in-context learning even when only responses generated by the master VLM instead of ground-truth annotations are used. We specifically adopt the TextOCR and Clevr subtasks, as the remaining subtasks lack meaningful question prompts in general language settings. The data set is divided into two subsets of 200 and 800 samples, which we refer to as dev and val, respectively, in this paper.

\subsection{Models}
We choose between the open-source Qwen-VL-2 7B \citep{Qwen2VL} model as a representative of recent VLMs, and open-flamingo 3B and 9B \citep{flamingo} (from an earlier generation of VLMs) as CoT's apprentice VLM in various experiments, to evaluate whether CoT can benefit older VLMs and newer, well-instruction tuned VLMs alike. We run all apprentice models with 4 A100 GPUs of 40GB. We use GPT4-o \citep{gpt4o} as CoT's master VLM. 

\input{tables/Qwen-Static}
\input{tables/OpenFlamingo}

\subsection{Evaluation}

We use accuracy as the evaluation metric. We note that the main purpose of our evaluations is to present a holistic picture of how CoT would work for end users, thus the absolute performance of the models are not the priority. For both multiple-choice answers in MMMU and open responses in VL-ICL, we allow the VLMs to produce intermediate steps before generating the final answer. We prompt the VLMs to produce the final answer in a specific format, and use a rule-based parser to determine the VLMs’ choice. More specifically, for multipe-choice answers, we extract the VLMs' choice, and for open responses, we check whether there exists an exact match of the label in the VLMs’ response. If the parser fails to determine the VLMs’ choice, we assign a \texttt{None} value instead, since in reality, an randomly generated choice is misleading if not detrimental.

\input{figures/ablation_study}
\input{figures/e2e_performance2}
\input{figures/qwen_percent}
\input{figures/rag_on_empty_cache}

\section{Analysis}
\subsection{Static Performance}
This section preliminarily validates the effectiveness of CoT's multi-modal in-context learning in the static cache setting. We present our findings in Table~\ref{tab:Qwen}, which reports Qwen-7B's performance on the challenging MMMU dataset when enhanced by CoT's various multi-modal retrieval strategies. Tables~\ref{tab:Flamingo1} and \ref{tab:Flamingo2} report OpenFlamingo's performance on both MMMU and VL-ICL benchmarks when similarly enhanced by CoT. We experiment with two caching strategies on these datasets: storing a larger validation set (Val) while testing on a smaller development set (Dev), and vice versa.

\noindent \textbf{More Benefits with Larger Cache.}
As shown in Table~\ref{tab:Qwen}, caching a larger split while testing on a smaller split leads to greater performance gains when applying in-context learning on smaller models. A similar trend is observed when we manually restrict the cache size to half its original capacity.

\noindent \textbf{Hierarchical Retrieval Outperforms Dense Retrieval, but at a Cost.}
Our experiments aim to determine which retrieval method performs better and is more promising for in-context example selection. As shown in Table~\ref{tab:Qwen}, the hierarchical retrieval method slightly outperforms the dual-modality dense retrieval approach. However, this marginal advantage can only be achieved after fine-tuning a sensitive hyperparameter (cluster number). Figure~\ref{fig:ablation} details more related ablations.

\noindent \textbf{Best Dense Retrieval Strategy.}
We find that caching image embeddings, and querying with the average of image and text embeddings encoded from keywords extracted from VLM responses is the most effective dense retrieval strategy. Interestingly, (1) pure image retrieval, as reported in the OpenFlamingo paper \citep{flamingo}, also performs reasonably well, and (2) when the test set is relatively small (potentially introducing bias), pure image retrieval can even outperform other dense retrieval methods. In Table~\ref{tab:Qwen}, all presented text-based dense retrieval results utilize our LLM keyword extractor (\cref{sec:retrieval_method}), as the CLIP embedding context window is often too small to encompass complete VLM responses.

Based on these findings, we choose to evaluate our hierarchical retrieval method and the best-performing dense retrieval strategy in our following more important dynamic experiments (\cref{sec:experiment_dynamic}).

\noindent \textbf{Openflamingo-3B and 9B Receive Significant Benefits from CoT}. We present results for OpenFlamingo combined with the best dense retriever in Tables~\ref{tab:Flamingo1} and \ref{tab:Flamingo2}. CoT achieves high performance gains across the OpenFlamingo series, on both datasets, varying cache capacities, and the number of in-context examples. In MMMU, OpenFlamingo struggles to accommodate two-shot GPT-4o responses due to shorter context length, which limits the effectiveness of in-context learning.

\subsection{Dynamic Performance}
\label{sec:experiment_dynamic}
This section studies CoT's trade-offs between performance and cost advantage in the dynamic cache setting with various cache sizes. Figure \ref{fig:experiment_e2e} reports the MMMU score versus cache and retrieval configurations (within each sub-plot) and percentage of apprentice VLM usage (between each sub-plot). For \textit{NoStart}, \textit{ColdStart} and \textit{WarmStart}, we initialize CoT's cache as an empty set, the dev set, and the test set, respectively. The \textit{Hierarchical} setting uses the hierarchical retrieval method with CoT's cache startting with the dev set. Figure \ref{fig:experiment_checkout_undo} presents the performance gains of CoT's apprentice VLM versus different cache initialization methods. Figure \ref{fig:experiment_checkout_three_stage} shows the MMMU score increase of CoT's apprentice VLM on the first, second, and third (temporal, equal-sized) splits of the validation set. 

\noindent \textbf{Disparity Between Higher and Lower Use of Apprentice VLMs.} The less frequently apprentice VLMs are used, the better the overall performance. However, CoT mitigates this performance loss: as shown in Figure \ref{fig:experiment_e2e}, with \textit{WarmStart} and CoT enabled, the MMMU score with 90\% apprentice VLM usage matches the performance without CoT with only 70\% apprentice VLM usage.

\noindent \textbf{Performance gains from CoT in the dynamic setting.} CoT consistently provides performance gains in all settings, with higher gains in settings with higher ratio of apprentice VLM usage. Most notably in Figure \ref{fig:experiment_e2e}'s WarmStart setting, the MMMU score gain increased from 0.007 to 0.77 as the apprentice VLM usage increased sfrom 10\% to 90\%. 

\noindent \textbf{Hierarchical vs. Dense Retrieval.} Unlike in static settings, hierarchical cluster-based retrieval underperforms vs. dense retrieval, primarily due to its hyperparameters remaining fixed while the cache content evolves. We defer exploration of dynamic hyperparameter tuning to future work.

\noindent \textbf{Apprentice VLM performance gains.} CoT consistently achieves performance increases for the apprentice VLM no matter how much it has been used. As shown in Figure~\ref{fig:experiment_checkout_undo}, when the usage of apprentice VLM is between 30\% and 70\%, the performance gains are between 10\% and 23\%.

\noindent \textbf{Apprentice VLM performance gain vs. cache usage.} As CoT's cache grows as its multiplexer routes queries to the master VLM, the effectiveness of CoT's in-context learning will improve over time: as seen in Figure~\ref{fig:experiment_checkout_three_stage}), at 10\%-30\% apprentice VLM usage, the MMMU score increased between the first and third dataset splits, showing that the apprentice VLM was able to perform better as more entries were added to the CoT's cache. 

\section{Conclusion}
We proposed CoT, a VLM query serving framework that achieves an effective cost-quality trade-off for general VQA. CoT interleaves large (master) and small (apprentice) VLM usage for query serving, caching high-quality responses generated from master VLMs to significantly boost the performance of apprentice VLMs via a novel multi-modal retrieval and in-context learning technique. CoT features a multiplexer to balance master VLM calls for cached example generation, and apprentice VLM calls, which uses a dual-modality dense retrieval mechanism to fetch the most relevant examples from the cache for in-context learning. We evaluate CoT on various challenging VQA benchmarks and show that CoT's techniques can increase overall VQA performance by up to 7.7\% under the same budget constraint, and specifically boosts the performance of apprentice VLMs by up to 36.6\%.

In the future, we plan to explore generalizing CoT's techniques to models handling other modalities (e.g., code, audio) where similar accuracy-cost trade-offs are present.

\section{Limitations}
First, we conduct extensive experiments in a setup featuring one master model and one apprentice model, and CoT also has potential to extend this approach to a multi-level master-apprentice framework (e.g., incorporating a 7B model, a 72B model, a 405B model, and a GPT4-o). Second, while our current focus is on image and text modalities, the framework can be extended to include additional modalities, such as video data, acoustic data, code data or other sensory inputs, given more multi-modal large models are ready to use. 