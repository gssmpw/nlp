\section{Related Work}
\label{sec:related}

\textbf{RAG and Multi-modal RAG}. RAG was originally proposed for language tasks \citep{RAG}. In RAG, a retriever is designed to extract relevant document chunks by similarity, and then these chunks are prepended to question prompts. RAG can reduce hallucinations, support knowledge-intensive tasks \citep{RAGsurvery}, and improve capability of small models \citep{ragsv}. Recent research has expanded RAG's capabilities beyond text. Multimodal RAG \citep{ragvl, finegrain, lin-etal-2024-preflmr, murag, RACM3} retrieves world knowledge from relevant multimodal documents and improves knowledge-seeking or fact-based VQA tasks \citep{FVQA, okvqa, info, evqa}. CoT differs from multi-modal RAG in that its cache is dynamic, and all stored responses are generated by a large master VLM rather than factual documents. 

\noindent \textbf{In-context Learning and Multi-modal In-context Learning}. In-context learning emerged with large auto-regressive models like GPT-3 \citep{gpt3}, which adapt to new tasks by observing a few in-context demonstrations. This success motivated research into VLMs. Flamingo \citep{flamingo} demonstrated significant gains in VQA by providing random demonstrations in context, while \citet{visualcontext} extended these benefits to tasks like image segmentation and classification. More recently, \citet{vl-icl} validated in-context learning for VLMs ranging from 4B to 70B parameters using synthetic VQA. CoT makes unique contribution in multi-modal in-context learning, with we detail in \cref{sec:incontext}.
