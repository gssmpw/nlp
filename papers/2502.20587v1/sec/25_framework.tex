\section{Cache-of-Thought Framework}
\label{sec:system_components}
\subfile{figures/framework}
CoT (\cref{fig:system_overview}) is a query serving framework for VLM queries that interleaves large (\textit{master}) and small (\textit{apprentice})  VLM calls for significant performance boost and cost savings: query results from master VLM calls are cached, which are then used to enhance apprentice VLM calls via multi-modal retrieval and in-context learning.

\noindent \textbf{Master VLM}. CoT's master VLM (often with several hundred billion parameters and serves with per-token charge), under the assumption that it produces high-quality answers, acts as the generator of the QA-pairs stored in the cache.

\noindent \textbf{Apprentice VLM}. CoT's apprentice VLM (often with less than 7 billion parameters) is used to answer queries via augmentation with various in-context examples fetched from the cache.

\noindent \textbf{Model Multiplexer}. The Model Multiplexer routes incoming VLM queries by choosing one of two serving methods: use the master VLM to directly answer the query, or use the apprentice VLM to answer the query augmented with in-context examples fetched from the Cache (described shortly). The multiplexer balances cost and quality by controlling how often CoT calls the master and apprentice VLMs: frequently calling the master VLM increases the rate of populating examples into the cache (hence more effective in-context learning) but incurs a higher cost, and vice versa.

\noindent \textbf{Cache.} CoT's cache stores high-quality QA pairs from the master VLM. When the multiplexer routes a new query to the master VLM, the question asked, image prompt, and resulting answer (QA-pair) is stored into the cache, which then computes its dual-modality embedding from the question and image and inserts it into a HNSW index \citep{HNSW} to facilitate accurate retrieval as an in-context example. 
CoT's cache can also be pre-loaded with QA-pairs to efficiently warm-start on query serving, and can incorporate common eviction policies (e.g., LRU or LFU) for more resource-constrained settings.\footnote{We defer concrete explorations on CoT's eviction policy to future work when real-world dual modality user query benchmarks are available.}

\noindent \textbf{Retrieval Module}. The Retrieval Module retrieves relevant in-context examples (i.e., QA-pairs) from the Cache to augment queries routed to the apprentice VLM by the Multiplexer. It performs an efficient ANN vector search with the dual-modality embedding of the incoming query in the Cache's HNSW index, retrieving examples similar to the query in terms of both the question asked and image prompt.