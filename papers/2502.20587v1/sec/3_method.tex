\section{Methodology}
This section describes CoT's methodology. \Cref{sec:method_preliminary} overviews CoT's formulation, \cref{sec:incontext} describes CoT's multi-modal in-context learning, and \cref{sec:retrieval} covers how CoT retrieves relevant examples for in-context learning.
\subsection{Formulation}
\label{sec:method_preliminary}

This section formulates CoT's notations. Without loss of generality, CoT deploys two VLM instances with frozen weights, the master VLM $f$ and apprentice VLM $g$, where deployment cost of $g$ is significantly lower than $F$. CoT's workload is a set of (finite) queries $\mathcal{Q} \subset \mathbb{R}^d$: At each round $t$, the system receives a query from the workload $(x_t, q_t)\in \mathcal{Q}$ consisting of the visual input $x_t$ and the textual question $q_t$ (about the content of $x_t$ e.g., 'how many ducks are there in the picture?').

For each query $(x_t, q_t)$ from round $t$, CoT's model multiplexer $J$ determines which VLM to use (the master $f$ or apprentice $g$) with the multiplexer function: $J: \mathcal{Q} \mapsto\{f, g\}$, which routes the query according to $J((x_t, q_t))$. Then, the selected VLM will serve the query by generating an answer from an answer space $\mathcal{A}$ with the respective function $f: \mathcal{Q} \mapsto \mathcal{A}$ or $g: \mathcal{Q} \mapsto \mathcal{A}$, e.g., $g((x_t, q_t)) = a_t \in \mathcal{A}$. $x_t$, $q_t$, and the answer from the master/apprentice VLM $a_t$ will then comprise the \textit{QA-pair} $P_t = \{(x_i, q_i), a_i\}$.

CoT's cache, denoted as $\mathcal{L}_t \subset \mathcal{Q}$, will store $\left|\mathcal{L}_t\right| \leq L$ QA-pairs $P_t$ generated with $g$, i.e., $P_t$ s.t. $J((x_t, q_t)) = g$, where $L$ is the cache capacity. CoT inserts into $\mathcal{L}_t$ whenever $g$ is invoked; however, due to the bounded size $L$, insertions performed when $\left|\mathcal{L}_t\right| = L$ will trigger evictions. Initially, $\mathcal{L}_0$ can either be empty (cold start) or a prepopulated set of QA-pairs (warm start) from the master VLM $g$. Hence, the cache $\mathcal{L}_t$ serves as an external knowledge base to support the apprentice VLM $f$ during inference via multi-modal retrieval and in-context learning (\cref{sec:incontext}). 

For retrieval from the Cache $\mathcal{L}_t$, CoT uses a \textit{retrieval mechanism} $R$ to retrieve the top-k relevant QA-pairs $\mathcal{L}_k$ from the cache: $R(\mathcal{L}_{t_i}, (x_{t_j}, q_{t_j})) \mapsto \mathcal{L}_k \subseteq \mathcal{L}_{t_i}, j > i$, which we describe in \cref{sec:retrieval}.
\subsection{Multi-modal In-context learning}
\label{sec:incontext}
CoT performs multi-modal in-context learning by transferring knowledge from the cache to the apprentice VLM. Given the pre-trained apprentice VLM $f$ with frozen parameters $\theta$ and a query $(x_t, q_t)\in \mathcal{Q}$, CoT retrieves a set of QA-pairs from the cache $M = \{(x_i, q_i), a_i\}$ from $\mathcal{L}_t$ used to aid inference. The apprentice VLM $f$ then generates an answer $a_t$ in one forward pass: $a_t = f_\theta\left( (x_t, q_t), M\right)$.

\noindent \textbf{Flexibility for Dynamic Systems}.
Compared to finetuning apprentice VLM instances with Lora using cached content \citep{lora}, CoT's in-context learning offers several advantages in dynamic systems: (1) CoT pre-computes QA-pairs and stores them in the cache for reuse, incurring no extra data annotation and/or recomputation expenses during retrieval; hence, CoT's performance will steadily improve with minimal costs as it answers more high-quality queries with the master VLM. (2) CoT's self-improvement via in-context learning does not require weight updates for its VLMs, hence there are no extra training overheads or delays (e.g., waiting for model convergence).

\noindent \textbf{Prompt Construction}. For each query routed to the apprentice VLM, CoT performs in-context learning by retrieving one or more demonstration examples to prepend to the inference prompt. CoT utilizes special tokens for prompt formatting, interleaving support QA-pairs in the form of (image,text): "\{support image\}, Question: \{support question\}, Answer: \{\textit{ground truth answer}\}, \{query image\}, Question: \{query question\}, Answer:" In CoT, \{\textit{ground truth answer}\} is replaced by stored VLM responses from the cache $\mathcal{L}_t$. Beyond this core prompt structure, additional components---such as n-shots and system prompts required by the VLM’s input format---are incorporated in the final prompt. For effective prompt construction, CoT explicitly instructs the master VLM to "include reasoning steps" when using it to serve queries to obtain complete responses (to prepend to prompts sent to the apprentice VLM) rather than just short answers or multiple-choice selections. This allows the apprentice VLM to better leverage the master VLM's reasoning capabilities. Full prompts details can be found in the \cref{sec:appendix}.

\subsection{Multi-modal Retrieval}
\label{sec:retrieval}
In this section, we describe CoT's multimodal retrieval of in-context examples from its Cache, on which performance on downstream tasks is highly sensitive to \citep{visualcontext}. Much to our surprise, existing works on VLM in-context learning present ungrounded design choices without further experiments in the following aspects:

\noindent \textbf{Choice of Retrieval}. Recent works \citep{vl-icl, manyshot} randomly select in-context examples for all test instances. Flamingo \citep{flamingo} attempts retrieval-based in-context example selection, which retrieves top-k similar samples using frozen pre-trained vision encoder embeddings from CLIP \citep{CLIP}. While Flamingo verifies that its image-driven retrieval is better than random selection in test cases with short, few-word questions, it remains unclear whether this method can be extend to general VQA systems where questions can be long and complicated, as seen in benchmarks like MMMU \citep{mmmu}.

\noindent \textbf{Choice of Support Set}. All of the aforementioned studies randomly partition the dataset into a support set and test set offline. However, in-context performance can heavily depend on the distribution of support examples. A setting where the support set is dynamic (i.e., continuously growing as in CoT's case) has not been explored.

\noindent \textbf{Assumption of Ground-Truth Availability}. Previous studies often use in-context examples with human-annotated ground truth answers, which can be inpractical for real-world, online data streams.

In contrast, CoT targets a more challenging and general VQA setting, and makes significant contributions to multi-modal retrieval for in-context learning in terms of the aforementioned 3 aspects: (1) CoT proposes a retrieval method that leverages both CLIP image and text embeddings, supplemented by keyword-based techniques for long or complex queries. CoT also introduces a hierarchical keyword clustering approach tailored to the query distribution (\cref{sec:retrieval_method}). (2) CoT's in-context example is performed over a dynamically updating (as opposed to static in previous works) knowledge base $\mathcal{L}_t$, where new items (that align more closely with recent queries) are continuously added to the cache. (3) CoT directly utilizes responses from the master VLM as in-context examples rather than the human-labeled ground truth answers strongly assumed by all prior works. This allows CoT to incorporate more diverse examples as long as they have been seen by the master VLM, rather than relying on human annotators.

\subsubsection{Dual-Modality Dense Retrieval}
\label{sec:retrieval_method}
We begin with a simple dense-retrieval method that uses a unified embedding vector to identify the top-\emph{k} matches. A natural starting point is CLIP \citep{CLIP}, which has proven effective in multimodal retrieval \citep{RACM3, inquire} and is pretrained to align image and text embeddings on internet-scale (image, text) pairs. Recognizing CLIP as a well aligned dual-modality representation, we use its image and text encoders to separately embed each image and text, then average these embeddings to form a unified vector that is stored in our cache index and use for query embeddings. 

However, CLIP embeddings also have limitations. They are pretrained primarily on short textual descriptions of images (e.g., “a photo of a cat”) and support a relatively small text context window—often fewer than 20 tokens effectively \citep{longclip}. This limitation makes CLIP less ideal for lengthy, sentence-like questions. To mitigate this issue, we break down long questions before encoding them with CLIP. Specifically, we employ a small auxiliary LLM, denoted as $Extractor$ to extract keywords from the long sentence-like query. Details of how we prompt the $Extractor$ can be found in the Appendix.

In some cases, the question text alone may not be sufficiently informative for retrieval (e.g. a question like "what is in the image"); Fortunately, CoT's cache also stores responses from the master VLM, allowing us to extract text embeddings from these richer responses instead of relying solely on the original question. For new queries, we can even make a single pass with the apprentice VLM to generate a text answer response, extract keywords from that response, then feed them into CLIP text encoder to enhance retrieval. Because these auxiliary or apprentice LLM calls are much cheaper than invoking the master VLM, we can use them to achieve significantly better retrieval results without noticeably increasing inference cost. This strategy, which combines embeddings from both the text response and the corresponding image, significantly improves the robustness of our dense-retrieval process, particularly in VQA scenarios where the question alone lacks sufficient context.

\subsubsection{Multi-stage Hashtag Retrieval}
In real-world scenarios, the streaming data distribution $\mathcal{Q}$ is highly complex and diverse, varying across different contexts. When the cache is large, directly performing dense retrieval from the entire knowledge base $\mathcal{L}_t$ may fail to locate the relevant contexts due to the limited expressiveness of CLIP embeddings even if they are carefully designed (\cref{sec:retrieval_method}). A straightforward approach to improve retrieval accuracy is to restrict retrieval to a specific knowledge domain. However, without a predefined distribution for $\mathcal{Q}$, categorizing streaming data under fixed domains is often difficult.

To enable more granular unsupervised retrieval, we design a two-stage hashtag tree where Level 1 hashtags $\mathcal{H}_1$ capture high-level, task-oriented representations, and Level 2 hashtags $\mathcal{H}_2$ provide finer, concept-specific details. Each data entry in $\mathcal{L}_t$ is assigned at least one Level 1 hashtag and one Level 2 hashtag. We use $h_{i,j}$ to denote the $j^{th}$ hashtag at Level $i$, where $\mathcal{H}_i = \{ h_{i,j} \}$, $i = 1, 2$.

Initially, both the hashtag tree and the cache $\mathcal{L}_t$ are empty. The master VLM g is tasked with inferring $a_t$ given each new-coming data $(x_t, q_t)$ during the cold start stage, after which the Level 2 hashtag $h_{2,t}$ is obtained as: $h_{2,t} = CLIPTextEncoder(Extractor(a_t)) $.
Once $|\mathcal{H}_2| > \tau$ or the cold start ends, we use K-Means to cluster $\mathcal{H}_2$ into $K$ groups $\{C_k\}_{k=1}^{K}$, where each cluster is assigned a Level 1 hashtag $h_{1,k}$, computed as the mean embedding of its corresponding Level 2 hashtags. 

Given new data $t'$, when the master VLM $g$ is selected for inference, the generated answer is stored in $\mathcal{L}_t$ and assigned to the closest Level 1 hashtag $h_{1,j^*}$ based on Euclidean distance. The Level 1 hashtag is then updated as the weighted average of its current Level 2 hashtag children. Otherwise, if the apprentice VLM $f$ is selected to infer on $t'$, $\mathcal{R}$ first retrieves all examples from $\mathcal{L}_t$ that shares the same Level 1 hashtag as $d_t$. It then selects top-$k$ results as the $k$-shot in-context examples for $f$.