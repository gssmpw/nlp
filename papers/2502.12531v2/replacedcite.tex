\section{Related Work}
\label{RelatedWork}

\subsection{LLM for Robotics}
LLMs have demonstrated remarkable potential in semantic understanding and context generation, making them a valuable asset in robotics. ChatGPT for Robotics ____ and Code as Policies ____ achieve remarkable progress in using LLM-generated code to control robotic arms. For drone applications, TypeFly ____ proposes an end-to-end system for piloting a quadcopter with LLM, while REAL ____ provides a strategy to employ LLM as part of control and planning on drones. Additionally, LLM can act as a cognitive agent in closed-loop vehicle motion planning. ____ These efforts highlight the diverse and impactful applications of LLMs across different robotic platforms. However, robotic performance remains constrained by the reasoning capabilities of LLMs. LLMs often exhibit unreliability in tasks requiring complex reasoning.  Notably, ChatGPT has acknowledged the necessity of human intervention for code generation ____. % providing correction feedback in code generation ____. Furthermore, other studies have also reported the reasoning shortcomings of LLMs in robotics applications ____.%

\subsection{Prompt Engineering}
Recent research has increasingly focused on leveraging prompt engineering ____ strategies to enhance the reasoning capabilities of LLMs for robotics. In ChatGPT for Robotics ____, LLMs are provided with a description of guidelines, constraints, and accessible APIs, and then LLMs generate code to control robots completing the given task description. Similarly, ViLaIn ____ provides detailed descriptions of the robot and its environmental states, helping LLMs develop a general understanding of the robot's condition before generating control code. However, LLMs exhibit sensitivity to input perturbations ____. To address this, PluginSafety ____ applies safety chips that infer specification constraints, ensuring that LLM-generated content adheres to predefined NL constraints. AutoTamp ____ adopts a different approach by prompting the LLM to first generate an intermediate task representation and then translate it into task plans. Additionally, some studies integrate human or automated checker feedback ____ to re-prompt the LLM, refining its outputs for improved reasoning accuracy.

\subsection{LLM In-context Learning}
In-context learning is a powerful capability of LLMs that enables them to perform tasks by leveraging examples provided in the input prompt. Instead of retraining model parameters, LLMs recognize patterns from the given context and generalize them to new outputs, allowing users to shape the characteristics of LLMs with few examples ____. In robotics, in-context learning enables LLMs to generate code following robot policy by incorporating example demonstrations within the prompt  ____. This paradigm approach helps LLMs generalize across tasks by analyzing prior examples. Furthermore, LLMs can learn to ground human instructions using the provided APIs from few-shot examples ____. However, most existing studies primarily focus on constructing examples to illustrate LLM, often overlooking their integration with prompt engineering to further enhance performance.


\subsection{Chain of Thought (COT) Reasoning}
CoT ____ encourages LLMs to articulate their intermediate reasoning step by step when solving a task. Given that robotic tasks often require executing a series of sequential actions, CoT facilitates the generation of code that aligns with each step of the action plan. Prior research incorporates CoT into the comment of code examples ____, while another study utilizes CoT for translating plans into actionable steps ____. %All these studies report performance improvements compared to approaches that do not apply CoT. By integrating CoT reasoning, LLMs for robotics can tackle more complex, real-world tasks with enhanced adaptability, safety, and efficiency.%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%