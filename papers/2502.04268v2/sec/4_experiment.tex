\section{Experiments}
\label{sec:experiment}

Experiments are carried out on NVIDIA RTX4090 GPUs using PyTorch 2.2.0 \cite{paszke2019pytorch} and the rotation detection tool kits: MMRotate 1.0.0 \cite{zhou2022mmrotate}. All the experiments follow the same hyper-parameters (learning rate, batch size, optimizer, etc.).

Average precision (AP) is adopted as the primary metric. All the models are configured upon ResNet50 \cite{he2016deep} and trained with AdamW \cite{loshchilov2018decoupled}.
\textbf{1) Learning rate.} Initialized at 5e-5, warm-up for 500 iterations, and divided by ten at each decay step. 
\textbf{2) Epochs.} 72 for HRSC; 12 for the others.
\textbf{3) Augmentation.} Random rotation/flip for HRSC; random flip for the others.
\textbf{4) Image size.} Split into 1,024 $\times$ 1,024 with an overlap of 200 for DOTA/FAIR1M/STAR; scaled to 800 $\times$ 800 for others.
\textbf{5) Multi-scale.} All experiments evaluated without multi-scale technique \cite{zhou2022mmrotate}. 
\textbf{6) Datasets.} Six remote sensing and one retail scene datasets, covering all datasets used by the main counterparts \cite{yu2024point2rbox, luo2024pointobb, cao2023p2rbox}:

\begin{table*}[!tb]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{0.65mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\centering
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\toprule
{\textbf{Methods}} & {*} & {\textbf{\,DOTA-v1.0\,}} & {\textbf{\,DOTA-v1.5\,}} & {\textbf{\,DOTA-v2.0\,}} & {\textbf{~~DIOR~~}} & {\textbf{~~HRSC~~}} & {\textbf{\,FAIR1M\,}} & {\textbf{~~STAR~~}} & {\textbf{\,SKU110K\,}} & {\textbf{~~RSAR~~}} \\
\hline
\rowcolor{gray!20} \multicolumn{11}{l}{$\blacktriangledown$ \textit{RBox-supervised OOD}} \\ \hline
RetinaNet (2017) \cite{lin2017focal} & \checkmark & 68.69 & 60.57        & 47.00 & 54.96 & 84.49   & 37.67   & 21.80 & 78.50 & 57.67  \\
GWD (2021) \cite{yang2021rethinking} & \checkmark & 71.66 & 63.27        & 48.87 & 57.60 & 86.67   & 39.11   & 25.30 & 79.16 & 57.80 \\
FCOS (2019) \cite{tian2019fcos} & \checkmark & 72.44 & 64.53        & 51.77    &  59.83  & 88.99  & 41.25   & \textbf{28.10} & 80.09 & \textbf{66.66} \\
S$^2$A-Net (2022) \cite{han2022align} & \checkmark & \textbf{75.81} & \textbf{66.53} & \textbf{52.39} & \textbf{61.41} & \textbf{90.10} & \textbf{42.44}   & 27.30 & \textbf{80.36} & 66.47 \\
\hline
\rowcolor{gray!20} \multicolumn{11}{l}{$\blacktriangledown$ \textit{HBox-supervised OOD}} \\ \hline
Sun et al. (2021) \cite{sun2021oriented} & $\times$ & 38.60 & - & - & - & - & - & - & - & - \\
KCR (2023) \cite{zhu2023knowledge} & \checkmark & - & - & - & - &  79.10  & -  & - & - & -  \\
H2RBox (2023) \cite{yang2023h2rbox} & \checkmark & 70.05 & 61.70        & 48.68    & 57.80 &  7.03  & 35.94  & 17.20 & 57.15 & 49.92    \\
H2RBox-v2 (2023) \cite{yu2023h2rboxv2} & \checkmark & 72.31 & 64.76 & 50.33 & 57.64 & \textbf{89.66} & \textbf{42.27} & \textbf{27.30} & \textbf{70.70} & \textbf{65.16} \\
AFWS (2024) \cite{lu2024afws} & \checkmark & \textbf{72.55} & \textbf{65.92} & \textbf{51.73} & \textbf{59.07} & - & 41.80 & - & - & - \\
\hline
\rowcolor{gray!20} \multicolumn{11}{l}{$\blacktriangledown$ \textit{Point-supervised OOD}} \\ \hline
P2RBox (2024) \cite{cao2023p2rbox}$^\dagger$ & $\times$ & \underline{59.04} & -        & - & - & -   & -  & -  & - & -  \\
PointSAM (2024) \cite{liu2024pointsam}$^\dagger$ & $\times$ & - & - & - & \textbf{46.20} & -   & -  & -  & - & - \\
PointOBB (2024) \cite{luo2024pointobb} & $\times$ & 30.08 & 10.66        & 5.53     &  37.31  & -   & 11.19 & 9.19  & - & 13.80    \\
Point2RBox+SK (2024) \cite{yu2024point2rbox}$^\dagger$ & \checkmark & 40.27 & 30.51        & 23.43    & 27.34 & 79.40   & 20.03 & 7.86  & 3.41 & 27.81    \\
PointOBB-v2 (2025) \cite{ren2024pointobbv2} & $\times$ & 41.68 & 30.59        & 20.64    &  39.56  & -   & 13.36 & 9.00  & 56.63 & 18.99   \\
PointOBB-v3 (2025) \cite{zhang2025pointobbv3} & $\checkmark$ & 41.20 & 31.25 & 22.82 & 37.60 & - & 11.42  & 11.31 & - & 15.84 \\
PointOBB-v3 (2025) \cite{zhang2025pointobbv3} & $\times$ & 49.24 & 33.79 & 23.52 & 40.18 & - & 18.35 & \underline{12.85} & - & 22.60 \\
\rowcolor{gray!20} Point2RBox-v2 (ours) & \checkmark & 51.00 & \underline{39.45} & \underline{27.11} & 34.70 & \underline{82.67} & \underline{25.72} & 7.80 & \underline{64.00} & \underline{28.60}
 \\
\rowcolor{gray!20} Point2RBox-v2 (ours) & $\times$ & \textbf{62.61} & \textbf{54.06}        & \textbf{38.79}   & \underline{44.45}  & \textbf{86.15}   & \textbf{34.71}  & \textbf{14.20} & \textbf{65.64} & \textbf{30.90}    \\
\bottomrule
\specialrule{0pt}{2pt}{0pt}
\multicolumn{11}{l}{$^*$Comparison tracks: \checkmark = End-to-end training and testing; $\times$ = Generating pseudo labels to train the FCOS detector (two-stage training).} \\
\multicolumn{11}{l}{$^\dagger$Using additional priors. P2RBox/PointSAM: Pre-trained SAM model; Point2RBox+SK: One-shot sketches for each class.} \\
\bottomrule
\end{tabular}
\caption{Accuracy (AP$_{50}$) comparisons on the DOTA-v1.0/1.5/2.0, DIOR, HRSC, FAIR1M, STAR, SKU110K, and RSAR datasets.}
\label{tab:exp_other}
\vspace{-4pt}
\end{table*}

\begin{itemize}
    \item \textbf{DOTA \cite{xia2018dota}.} DOTA-v1.0 has 2,806 aerial images annotated with 15 categories, while DOTA-v1.5/2.0 are the extended versions with more small objects and categories.
    
    \item \textbf{DIOR \cite{cheng2022anchor}.} It is an aerial image dataset re-annotated with RBoxes based on its original HBox version \cite{li2020object}, with a high variation in object size and high intra‚Äêclass diversity. 

    \item \textbf{HRSC \cite{liu2017hrsc}.} It contains ship instances on the sea and inshore. The train/val/test set includes 436/181/444 images.

    \item \textbf{FAIR1M \cite{sun2022fair1m}.} It has more than 1 million instances and more than 40,000 images for fine-grained object recognition in remote sensing imagery, annotated with 37 categories. The results are evaluated on FAIR1M-1.0.

    \item \textbf{STAR \cite{li2024star}.} It is extensive for scene graph generation, covering more than 210,000 objects with diverse spatial resolutions, classified into 48 fine-grained categories and precisely annotated with oriented bounding boxes. 

    \item \textbf{SKU110K \cite{pan2020dynamic}.} It focuses on the detection of densely packed retail scenes with 110,712 objects in 11,762 images. The density reaches 86 instances per image. 

    \item \textbf{RSAR \cite{zhang2025rsar}.} It is a remote sensing dataset based on Synthetic Aperture Radar (SAR) imagery with 6 categories.

\end{itemize}

\begin{table*}[!tb]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{2.08mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\hspace{1pt}
\begin{minipage}[t]{0.315\linewidth}
\centering
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{$w_\text{O}$} & \multicolumn{2}{c|}{\textbf{DOTA}} & \multicolumn{2}{c}{\textbf{HRSC}} \\
                  & {E2E} & {FCOS} & {E2E} & {FCOS} \\ \midrule
3  & 48.76 & 61.62 & 81.85 & 84.36 \\
5  & 49.81 & 62.44 & 82.46 & 85.76 \\
\rowcolor{gray!20} 10 & \textbf{51.00} & \textbf{62.61} & \textbf{82.67} & \textbf{86.15} \\
30 & 45.88 & 57.83 & 81.56 & 85.61 \\
\bottomrule
\end{tabular}
\caption{Ablation with the weight of $\mathcal{L}_\text{O}$.}
\label{tab:abl_lo}
\end{minipage}
\quad
\begin{minipage}[t]{0.315\linewidth}
\centering
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{$w_\text{W}$} & \multicolumn{2}{c|}{\textbf{DOTA}} & \multicolumn{2}{c}{\textbf{HRSC}} \\
                  & {E2E} & {FCOS} & {E2E} & {FCOS} \\ \midrule
3  & 50.85 & 56.78 & 78.42 & 83.49 \\
\rowcolor{gray!20} 5  & \textbf{51.00} & \textbf{62.61} & \textbf{82.67} & \textbf{86.15} \\
10 & 49.15 & 60.54 & 30.37 & 35.13 \\
30 & 42.84 & 52.53 & 23.89 & 25.91 \\
\bottomrule
\end{tabular}
\caption{Ablation with the weight of $\mathcal{L}_\text{W}$.}
\label{tab:abl_lw}
\end{minipage}
\quad
\begin{minipage}[t]{0.315\linewidth}
\setlength{\tabcolsep}{2.04mm}
\centering
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{$w_\text{E}$} & \multicolumn{2}{c|}{\textbf{DOTA}} & \multicolumn{2}{c}{\textbf{HRSC}} \\
                  & {E2E} & {FCOS} & {E2E} & {FCOS} \\ \midrule
0.1 & 48.75 & 57.62 & 34.71 & 39.45 \\
\rowcolor{gray!20} 0.3 & 51.00 & 62.61 & \textbf{82.67} & \textbf{86.15} \\
0.5 & \textbf{51.36} & \textbf{62.63} & 76.85 & 85.22 \\
1.0 & 49.05 & 60.63 & 56.59 & 59.59 \\
\bottomrule
\end{tabular}
\caption{Ablation with the weight of $\mathcal{L}_\text{E}$.}
\label{tab:abl_le}
\end{minipage}
\vspace{-4pt}
\end{table*}

\begin{table*}[!tb]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{2.04mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\hspace{1pt}
\begin{minipage}[t]{0.315\linewidth}
\centering
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{$w_\text{ss}$} & \multicolumn{2}{c|}{\textbf{DOTA}} & \multicolumn{2}{c}{\textbf{HRSC}} \\
                  & {E2E} & {FCOS} & {E2E} & {FCOS} \\ \midrule
0.1 & 49.28 & 59.66 & 73.66 & 78.92 \\
\rowcolor{gray!20} 1.0 & \textbf{51.00} & \textbf{62.61} & \textbf{82.67} & \textbf{86.15} \\
3.0 & 49.15 & 59.20 & 1.30  & 1.65 \\
\bottomrule
\end{tabular}
\caption{Ablation with the weight of $\mathcal{L}_\text{ss}$.}
\label{tab:abl_lss}
\end{minipage}
\quad
\begin{minipage}[t]{0.647\linewidth}
\setlength{\tabcolsep}{3.05mm}
\centering
\begin{tabular}{c|c|c||c|c|c}
\toprule
{R / F / S} & {\textbf{DOTA}} & {\textbf{HRSC}} & {R / F / S} & {\textbf{DOTA}} & {\textbf{HRSC}} \\
 \midrule
90\% / 10\% / 0\% & 60.42 & 85.46 & 80\% / 20\% / 0\%  & 59.46 & 84.73 \\
75\% / 0\% / 25\% & 60.79 & 86.22 & 60\% / 15\% / 25\% & 62.38 & 84.21 \\
\cellcolor{gray!20}68\% / 7\% / 25\% & \cellcolor{gray!20}\textbf{62.61} & \cellcolor{gray!20}\textbf{86.15} & 38\% / 37\% / 25\% & 45.87 & 8.56  \\
45\% / 5\% / 50\% & 60.55 & 85.34 & 40\% / 10\% / 50\% & 60.49 & 10.74 \\
\bottomrule
\end{tabular}
\caption{Ablation with the proportion of augmented views in self-supervision.}
\label{tab:abl_pro}
\end{minipage}
\vspace{-10pt}
\end{table*}

\subsection{Main Results on DOTA-v1.0}
\label{sec:experiment-main}

Table \ref{tab:exp_dota} compares Point2RBox-v2 with the state-of-the-art methods, which can be categorized into two tracks: 

\textbf{1) End-to-end training.} These methods apply the trained weakly-supervised detector directly to the test set. Without relying on priors, our approach demonstrates an improvement of 16.93\% (51.00\% vs. 34.07\%) compared to Point2RBox. Even when compared to Point2RBox+SK, which incorporates additional data-side priors (i.e. one-shot examples for each class), our method still outperforms it by 10.73\% (51.00\% vs. 40.27\%).

\textbf{2) Two-stage training.} These methods generate RBox labels on train/val sets, with which the FCOS detector is trained. In this two-stage mode, Point2RBox-v2 achieves an accuracy of 62.61\%, considerably surpassing PointOBB series. Remarkably, it even outperforms the SAM-powered method P2RBox by 3.57\% (62.61\% vs. 59.04\%).

\textbf{Class-wise analysis.} The FCOS detector trained with labels generated by Point2RBox-v2 achieves accuracy nearly equivalent to RBox-supervised FCOS across six high-density categories: SH (86.9\% vs. 87.1\%), SV (79.6\% vs. 79.8\%), LV (76.3\% vs. 79.8\%), PL (88.0\% vs. 89.1\%), ST (82.9\% vs. 84.6\%), and TC (89.1\% vs. 90.4\%). Interestingly, these six high-density categories account for 88\% of DOTA instances. By annotating these categories with points and generating RBoxes using Point2RBox-v2 while labeling the other sparse categories with RBoxes, we can significantly reduce annotation labor without sacrificing much accuracy, highlighting the valuable role our method can play.

\begin{figure*}[t!]
\setlength{\abovecaptionskip}{1.2mm}
\centering
\includegraphics[width=0.96\linewidth]{figs/case.pdf}
\caption{Qualitative analysis on failed cases and overlap cases.}
\label{fig:case}
\vspace{-6pt}
\end{figure*}

\subsection{Results on More Datasets}

The results are displayed in Table \ref{tab:exp_other}.
On more challenging DOTA-v1.5/2.0, Point2RBox-v2 presents a similar trend, 23.47\%/18.15\% higher than PointOBB-v2 in the pseudo-generation track. 
On the ship detection dataset HRSC, the gap between Point2RBox-v2 and RBox-supervised FCOS is only 2.84\% (86.15\% vs. 88.99\%).
DIOR is relatively sparse, leading to less improvement with our methods---lower than PointSAM (44.45\% vs. 46.20\%) but still higher than methods that do not use SAM. 
Our method also provides competitive performance on fine-grained datasets FAIR1M and STAR. 
In addition to remote sensing scenarios, we carry out experiments on SKU110K for densely packed retail scenes. Existing point-supervised methods struggle in this case, whereas Point2RBox-v2 achieves performance on par with HBox-supervised H2RBox (65.64\% vs. 57.15\%).

\begin{table}[!tb]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{1.78mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\centering
\begin{tabular}{ccccc|cc|cc}
\toprule
\multicolumn{5}{c|}{\textbf{Modules}} & \multicolumn{2}{c|}{\textbf{DOTA}} & \multicolumn{2}{c}{\textbf{HRSC}} \\
$\mathcal{L}_\text{O}$ & $\mathcal{L}_\text{W}$ & $\mathcal{L}_\text{ss}$ & $\mathcal{L}_\text{E}$ & \textit{CP} & {E2E} & {FCOS} & {E2E} & {FCOS} \\ \midrule
\checkmark & & & & & 0.00 & 0.00 & 0.00 & 0.00 \\
\checkmark & \checkmark & & & & 41.54 & 52.98 & 17.96 & 19.64 \\
\checkmark & \checkmark & \checkmark & & & 46.64 & 54.26 & 18.10 & 22.13 \\
\checkmark & \checkmark & \checkmark & \checkmark & & 49.55 & 61.88 & 78.79 & 83.79 \\
& \checkmark & \checkmark & \checkmark & \checkmark & 48.58 & 59.56 & 20.35 & 24.76 \\
\checkmark & & \checkmark & \checkmark & \checkmark & 38.94 & 48.44 & 11.64 & 14.93 \\
\checkmark & \checkmark & \checkmark & & \checkmark & 47.08 & 55.05 & 19.58 & 21.78 \\
\rowcolor{gray!20} \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{51.00} & \textbf{62.61} & \textbf{82.67} & \textbf{86.15} \\
\bottomrule
\end{tabular}
\caption{Ablation with incremental addition of modules.}
\label{tab:abl_mod}
\vspace{-4pt}
\end{table}

\begin{table}[!tb]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{2.85mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\centering
\begin{tabular}{c|c|c||c|c|c}
\toprule
16 & \cellcolor{gray!20}$K\!=\!24$ & 32 & 1.2 & \cellcolor{gray!20}$\beta\!=\!1.6$ & 2.0 \\ \midrule
50.87 & \cellcolor{gray!20}\textbf{51.00} & 48.08 & 48.14 & \cellcolor{gray!20}51.00 & \textbf{51.33} \\
\bottomrule
\end{tabular}
\caption{Ablation with $K$ and $\beta$ in edge loss on DOTA (E2E).}
\label{tab:abl_edgeparam}
\vspace{-4pt}
\end{table}

\begin{table}[!tb]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{1.75mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\centering
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{$\sigma$} & \multicolumn{2}{c|}{Point2RBox} & \multicolumn{2}{c|}{PointOBB-v2} & \multicolumn{2}{c}{Point2RBox-v2} \\
 & {\textbf{DOTA}} & {\textbf{HRSC}} & {\textbf{DOTA}} & {\textbf{HRSC}} & {\textbf{DOTA}} & {\textbf{HRSC}} \\ \midrule
0\%  & 40.27 & 79.40 & 44.85 & - & 62.61 & 86.15 \\
10\% & 39.60 & 78.81 & 42.30 & - & 61.58 & 85.76 \\
30\% & 38.42 & 78.28 & 38.46 & - & 60.31 & 85.71 \\
\bottomrule
\end{tabular}
\caption{Ablation with the inaccuracy in point annotations.}
\label{tab:abl_noise}
\vspace{-10pt}
\end{table}

\subsection{Ablation Studies}
\label{sec:experiment-ablation}

Tables \ref{tab:abl_lo}-\ref{tab:abl_noise} display the ablation studies on DOTA-v1.0 and HRSC. ``E2E'' denotes end-to-end training; ``FCOS'' denotes two-stage training (i.e. generating pseudo labels to train FCOS). The final values adopted are highlighted in gray.

\textbf{Weight of each loss.} Tables \ref{tab:abl_lo}-\ref{tab:abl_le} determine the weights of the proposed losses. Based on these experiments, the weights $(w_\text{O},w_\text{W},w_\text{E},w_\text{ss})$ are set to $(10, 5, 0.3, 1)$.

\textbf{Proportion of augmented views.} Table \ref{tab:abl_pro} studies the proportion between rotation, flip, and scale. The results are reported with two-stage training (FCOS). Based on the results, the proportion is set to 68\%, 7\%, and 25\%.

\textbf{Incremental addition of modules.} Table \ref{tab:abl_mod} demonstrates the constraints from Gaussian and Voronoi achieve an accuracy of 52.98\% on DOTA. Adding consistency loss and edge loss further boosts it to 54.26\% and 61.88\%, respectively, whereas the improvement from copy-paste is 0.73\%. We also demonstrate the impact of omitting each core loss.

\textbf{Edge loss parameters.} We set $K=24$ and $\beta=1.6$ as they are observed to discern the correct edges during code development. Table \ref{tab:abl_edgeparam} provides a more precise ablation.

\textbf{Annotation inaccuracy.} We offset the annotated points by a noise from the uniform distribution $\left[-\sigma H, +\sigma H \right ]$, where $H$ is the height of objects. Table \ref{tab:abl_noise} shows that the AP$_{50}$ of Point2RBox-v2 decreases by less than 3\% when noise is added to point annotations, demonstrating the robustness of the proposed learning mechanisms.

\subsection{More Discussions}
\label{sec:experiment-discussions}

The qualitative analysis on the failed/overlap cases is shown in Fig. \ref{fig:case}. \textbf{1) Failed cases.} Although our method performs well overall, it struggles with certain categories that are sparse and not constrained by other objects. \textbf{2) Overlap cases.} 
Minimizing overlap as a soft constraint during training does not entirely eliminate overlap. Once trained, the model remains robust to some overlap during inference.
