\section{Method}
\label{sec:method}

\subsection{Overview and Preliminary}
\label{sec:method-arch}

\begin{figure*}[t]
\setlength{\abovecaptionskip}{1.2mm}
\centering
\includegraphics[width=0.96\linewidth]{figs/loss.pdf}
\caption{To illustrate the procedure of the three newly proposed loss functions and their impact on the learning results. (a) Gaussian overlap loss (see Sec. \ref{sec:method-lo}). (b) Voronoi watershed loss (see Sec. \ref{sec:method-lw}). (c) Edge loss (see Sec. \ref{sec:method-le}).}
\label{fig:loss}
\vspace{-6pt}
\end{figure*}

An overview of Point2RBox-v2 is illustrated in Fig.~\ref{fig:arch}. The network is based on ResNet50 \cite{he2016deep} backbone, FPN \cite{Lin2017Feature} head, and PSC \cite{yu2024boundary} angle coder. Objects of varying sizes are typically assigned to different FPN layers based on the scale. However, points lack size, so we assign them all to the P3 layer (stride = 8). Assume the detector $f_\text{nn}\left ( \cdot \right ) $ maps an image $I$ to a set of RBoxes as detection results:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\left(
 x_c, y_c, w, h, \theta
\right) = f_\text{nn}\left ( I \right )
\end{equation}

Equivalently, oriented objects can also be represented by 2D Gaussian distributions $\mathcal{N}(\mu, \Sigma)$ \cite{yang2023detecting}:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\begin{Bmatrix}
\mu=\begin{bmatrix}
x_c & y_c
\end{bmatrix},
&
\Sigma=\mathbf{R}\begin{bmatrix}
 w/2 & 0 \\
 0 & h/2
\end{bmatrix} ^{2} \mathbf{R}^\top 
\end{Bmatrix}
\end{equation}
where 
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathbf{R}=\begin{bmatrix}
 \cos \theta & -\sin \theta \\
 \sin \theta & \cos \theta
\end{bmatrix}
\label{equ:rotation}
\end{equation}

At the core of Point2RBox-v2 is to utilize the constraints from the layout, which is achieved by Gaussian overlap loss (Sec. \ref{sec:method-lo}) and Voronoi watershed loss (Sec. \ref{sec:method-lw}). These two losses effectively limit the size and rotation of objects to a reasonable range. Upon that, edge loss (Sec. \ref{sec:method-le}) aligns the bounding box with the edge of objects to improve the accuracy. Incorporated with symmetry-aware learning (Sec. \ref{sec:method-lss}) and copy-paste augmentation (Sec. \ref{sec:method-cp}), we achieve a stable and high-accuracy solution for point-supervised OOD. In subsequent subsections, these modules are detailed. 

\subsection{Gaussian Overlap Loss}
\label{sec:method-lo}

As mentioned, oriented objects can be represented by 2D Gaussian distributions $\mathcal{N}(\mu, \Sigma)$. The overlap volume between two distributions $\mathcal{N}_1(\mu_1, \Sigma_1)$ and $\mathcal{N}_2(\mu_2, \Sigma_2)$ can be approximated by the Bhattacharyya coefficient \cite{yang2023detecting} as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
B\left(\mathcal{N}_1, \mathcal{N}_2 \right) = \exp\!\left( - \frac{1}{8} \mu^\top \Sigma^{-1} \mu \right) \!\cdot\! \frac{|\Sigma_1|^{1/4} |\Sigma_2|^{1/4}}{| \Sigma |^{1/2}}
\end{equation}
where $\mu = \mu_2 - \mu_1$, $\Sigma = \frac{1}{2}(\Sigma_1 + \Sigma_2)$, $ |\Sigma| $ denotes the determinant of the covariance matrix.

Based on the above equation, we build a Gaussian overlap matrix $\mathbf{M} \in \mathbb{R}^{N\times N}$ for each scene image as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathbf{M}_{i,j} = B\left ( \mathcal{N}_i, \mathcal{N}_j \right )
\end{equation}
where $i,j = 1, 2, \dots, N$; $N$ is the distribution count (or the instance count) within one training image.

The Gaussian overlap loss can then be expressed as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathcal{L}_O = \frac{1}{N} \sum_{i \neq j}^{}  \left ( \mathbf{M}_{i,j} \right )
\end{equation}
where $i \neq j$ omits diagonal elements.
With this loss, the detector learns to arrange instances (see Fig. \ref{fig:loss}a) based on the mutual exclusivity among instances.

\subsection{Voronoi Watershed Loss}
\label{sec:method-lw}

A Voronoi diagram \cite{aurenhammer1991voronoi} is a partitioning of a space based on a set of points. Point annotations can be effectively utilized to calculate a Voronoi diagram, where a distinct polygon region is assigned to each point-annotated instance. The watershed algorithm \cite{vincent1991watersheds}, on the other hand, is a region segmentation technique that treats the intensity of pixels as a topographic surface, identifying regions as ``catchment basins''.

The calculation of Voronoi ridges can be formulated as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
V = \mathit{Voronoi} \left (X \right ) 
\end{equation}
where $X$ are the annotated points within a training image; $V$ are the output Voronoi ridges (pixel coordinates).

Interestingly, we find that Voronoi diagrams can be utilized as initial markers for watershed to obtain a region for each instance (see Fig. \ref{fig:loss}b). In concrete terms, the points $X$ can be employed as foreground markers, while the Voronoi ridges $V$ can act as background boundaries:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
S = \mathit{Watershed} \left (I, X, V \right ) 
\end{equation}
where $S$ are the output basin regions (pixel coordinates) corresponding to each annotated instance.
By rotating $S$ to align with the direction of the current prediction, the regression target of width and height can be expressed as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\begin{bmatrix}
w_t \\
h_t
\end{bmatrix} = 2\max \left |  \mathbf{R}^\top
\left ( S- \begin{bmatrix}
x_c \\
y_c
\end{bmatrix} \right )  \right | 
\end{equation}
where $\left(
 x_c, y_c, w, h, \theta
\right)$ is the current prediction; $\mathbf{R}$ is defined by Eq. (\ref{equ:rotation}); $w_t$ and $h_t$ are detached to stop the gradient.

Afterward, the Voronoi watershed loss to regress the width and height of objects can be calculated as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathcal{L}_W = L_\text{GWD}\!\left (\! \begin{bmatrix}
 w/2 & 0 \\
 0 & h/2
\end{bmatrix} ^{2}\!,\begin{bmatrix}
 w_t/2 & 0 \\
 0 & h_t/2
\end{bmatrix} ^{2} \right ) 
\end{equation}
where $L_\text{GWD}\left (\cdot\right )$ is Gaussian Wasserstein Distance Loss \cite{yang2023detecting}.

\subsection{Edge Loss}
\label{sec:method-le}

The above two losses have limited the size to a reasonable range. To make it more accurate, we propose the edge loss to snap the boundaries toward the edges (see Fig. \ref{fig:loss}c).

First, a region $P \in \mathbb{R}^{(2K+1)\times (2K+1)}$ around each predicted RBox is extracted via Rotated RoI Align \cite{he2017maskrcnn}:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
P = \mathit{RoIAlign}\left ( \mathit{E}\left ( I \right ) ,\left ( x_c, y_c, \beta w, \beta h, \theta \right )  \right ) 
\end{equation}
where $\mathit{E}\left ( \cdot \right )$ is the edge detection function \cite{soria2023teed}. We set $K=24$ and $\beta=1.6$ in our experiments (see Table \ref{tab:abl_edgeparam}).

By calculating the sum of each row of $P$, the edge distribution in $y$ direction can be obtained as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mu_i = \sum_{j=1}^{2K+1} (P_{(K+1-i),j} + P_{(K+1+i),j})
\end{equation}
where $i = 1, 2, \dots, K$, indicating that the upper half of $P$ is reversed and added to the lower half.

Meanwhile, the current prediction of the edge can also be softened into a distribution as: 
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\lambda_i = \exp\left(-\frac{(i - K / \beta)^2}{2{\sigma_E}^2}\right)
\end{equation}
where $i = 1, 2, \dots, K$; $\sigma_E$ is set to 6. Note that we crop $P$ based on the predicted RBox, thus $\lambda$ is always the same.

Multiplying the two distributions yields the target:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
h_t = \frac{\beta h}{K} \arg \max (\mu \times \lambda)
\end{equation}
where $h_t$ is the regression target of height. Likewise, the width target $w_t$ is calculated along $x$ direction.

The edge loss is then obtained as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathcal{L}_E = smooth_{L1}\left ( \begin{bmatrix}
w & h
\end{bmatrix}, 
\begin{bmatrix}
w_t & h_t
\end{bmatrix}
\right )
\end{equation}

Note that $\mathcal{L}_W$ and $\mathcal{L}_E$ merely refines the width and height of boxes, without involving the angle regression.

\begin{table*}[t]
\fontsize{8.5pt}{10pt}\selectfont
\setlength{\tabcolsep}{1.2mm}
\setlength{\aboverulesep}{0.4ex}
\setlength{\belowrulesep}{0.4ex}
\setlength{\abovecaptionskip}{1.5mm}
\centering
\begin{tabular}{l|c|ccccccccccccccc|c}
\toprule
\textbf{Methods}  & * & \textbf{PL}$^1$    & \textbf{BD}    & \textbf{BR}    & \textbf{GTF}   & \textbf{SV}    & \textbf{LV}    & \textbf{SH}    & \textbf{TC}    & \textbf{BC}    & \textbf{ST}    & \textbf{SBF}   & \textbf{RA}    & \textbf{HA}    & \textbf{SP}    & \textbf{HC}    & \textbf{AP}$_\text{50}$  \\ \hline
\rowcolor{gray!20} \multicolumn{18}{l}{$\blacktriangledown$ \textit{RBox-supervised OOD}} \\ \hline
RepPoints (2019) \cite{yang2019reppoints} & \checkmark & 86.7  & 81.1  & 41.6  & 62.0  & 76.2  & 56.3  & 75.7  & 90.7  & 80.8  & 85.3  & 63.3 & 66.6  & 59.1  & 67.6  & 33.7  & 68.45 \\ 
RetinaNet (2017) \cite{lin2017focal} & \checkmark & 88.2  & 77.0  & 45.0  & 69.4  & 71.5  & 59.0  & 74.5  & 90.8  & 84.9  & 79.3  & 57.3 & 64.7  & 62.7  & 66.5  & 39.6  & 68.69 \\
GWD (2021) \cite{yang2021rethinking} & \checkmark & 89.3  & 75.4  & 47.8  & 61.9  & 79.5  & 73.8  & 86.1  & 90.9  & 84.5  & 79.4  & 55.9 & 59.7  & 63.2  & 71.0  & 45.4  & 71.66 \\
FCOS (2019) \cite{tian2019fcos} & \checkmark & 89.1  & 76.9  & 50.1  & 63.2  & 79.8  & 79.8  & 87.1  & 90.4  & 80.8  & 84.6  & 59.7 & 66.3  & 65.8  & 71.3  & 41.7  & 72.44 \\
S$^2$A-Net (2022) \cite{han2022align} & \checkmark & 89.2  & 83.0  & 52.5  & 74.6  & 78.8  & 79.2  & 87.5  & 90.9  & 84.9  & 84.8  & 61.9 & 68.0  & 70.7  & 71.4  & 59.8  & \textbf{75.81} \\ \hline
\rowcolor{gray!20} \multicolumn{18}{l}{$\blacktriangledown$ \textit{HBox-supervised OOD}} \\ \hline
Sun et al. (2021) \cite{sun2021oriented} & $\times$ & 51.5  & 38.7  & 16.1  & 36.8  & 29.8  & 19.2  & 23.4  & 83.9  & 50.6  & 80.0  & 18.9 & 50.2  & 25.6  & 28.7  & 25.5  & 38.60 \\
BoxInst-RBox (2021) \cite{tian2021boxinst}$^2$ & $\times$ & 68.4 & 40.8 & 33.1 & 32.3 & 46.9 & 55.4 & 56.6 & 79.5 & 66.8 & 82.1 & 41.2 & 52.8 & 52.8 & 65.0 & 30.0 & 53.59 \\
H2RBox (2023) \cite{yang2023h2rbox} & \checkmark & 88.5  & 73.5  & 40.8  & 56.9  & 77.5  & 65.4  & 77.9  & 90.9  & 83.2  & 85.3  & 55.3 & 62.9  & 52.4  & 63.6  & 43.3  & 67.82 \\
EIE-Det (2024) \cite{wang2024explicit}  & \checkmark & 87.7 & 70.2 & 41.5 & 60.5 & 80.7 & 76.3 & 86.3 & 90.9 & 82.6 & 84.7 & 53.1 & 64.5 & 58.1 & 70.4 & 43.8 & 70.10 \\
H2RBox-v2 (2023) \cite{yu2023h2rboxv2} &\checkmark & 89.0 & 74.4 & 50.0 & 60.5 & 79.8 & 75.3 & 86.9 & 90.9 & 85.1 & 85.0 & 59.2 & 63.2 & 65.2 & 70.5 & 49.7 & \textbf{72.31} \\ \hline
\rowcolor{gray!20} \multicolumn{18}{l}{$\blacktriangledown$ \textit{Point-supervised OOD}} \\ \hline
\footnotesize Point2Mask-RBox (2023) \cite{li2023point2mask}$^2$  & $\times$ & 4.0 & 23.1 & 3.8 & 1.3 & 15.1 & 1.0 & 3.3 & 19.0 & 1.0 & 29.1 & 0.0 & 9.5 & 7.4 & 21.1 & 7.1 & 9.72 \\
\scriptsize P2BNet+H2RBox (2023) \cite{chen2022pointtobox,yang2023h2rbox} & $\times$ & 24.7 & 35.9 & 7.1 & 27.9 & 3.3 & 12.1 & 17.5 & 17.5 & 0.8 & 34.0 & 6.3 & 49.6 & 11.6 & 27.2 & 18.8 & 19.63 \\
\scriptsize P2BNet+H2RBox-v2 (2023) \cite{chen2022pointtobox,yu2023h2rboxv2} & $\times$ & 11.0 & 44.8 & 14.9 & 15.4 & 36.8 & 16.7 & 27.8 & 12.1 & 1.8 & 31.2 & 3.4 & \underline{50.6} & 12.6 & 36.7 & 12.5 & 21.87\\
P2RBox (2024) \cite{cao2023p2rbox}$^\dagger$ & $\times$ & \underline{87.8} & \underline{65.7} & \underline{15.0} & \textbf{60.7} & \underline{73.0} & \underline{71.7} & \underline{78.9} & 81.5 & 44.5 & \underline{81.2} & \textbf{41.2} & 39.3 & \underline{45.5} & \underline{57.5} & \underline{41.2} & \underline{59.04} \\ 
PointOBB (2024) \cite{luo2024pointobb} & $\times$ & 26.1 & \underline{65.7} & 9.1 & \underline{59.4} & 65.8 & 34.9 & 29.8 & 0.5 & 2.3 & 16.7 & 0.6 & 49.0 & 21.8 & 41.0 & 36.7  & 30.08 \\ 
\footnotesize Point2RBox (2024) \cite{yu2024point2rbox} & \checkmark & 62.9 & 64.3 & 14.4 & 35.0 & 28.2 & 38.9 & 33.3 & 25.2 & 2.2  & 44.5 & 3.4  & 48.1 & 25.9 & 45.0 & 22.6 & 34.07 \\
\footnotesize Point2RBox+SK (2024) \cite{yu2024point2rbox}$^\dagger$ & \checkmark & 53.3 & 63.9 & 3.7  & 50.9 & 40.0 & 39.2 & 45.7 & 76.7 & 10.5 & 56.1 & 5.4  & 49.5 & 24.2 & 51.2 & 33.8 & 40.27 \\
\footnotesize Point2RBox+SK (2024) \cite{yu2024point2rbox}$^\dagger$ & $\times$ & 66.4 & 59.5 & 5.2 & 52.6 & 54.1 & 53.9 & 57.3 & \textbf{90.8} & 3.2 & 57.8 & 6.1  & 47.4 & 22.9 & 55.7 & 40.5 & 44.90 \\
PointOBB-v2 (2025) \cite{ren2024pointobbv2} & $\times$ & 64.5 & 27.8 & 1.9 & 36.2 & 58.8 & 47.2 & 53.4 & \underline{90.5} & 62.2 & 45.3 & 12.1 & 41.7 & 8.1 & 43.7 & 32.0 & 41.68 \\ 
PointOBB-v3 (2025) \cite{zhang2025pointobbv3} & \checkmark & 30.9 & 39.4 & 13.5 & 22.7 & 61.2 & 7.0 & 43.1 & 62.4 & 59.8 & 47.3 & 2.7 & 45.1 & 16.8 & 55.2 & 11.4 & 41.29 \\
PointOBB-v3 (2025) \cite{zhang2025pointobbv3} & $\times$ & 52.9 & 54.4 & \textbf{21.3} & 52.7 & 65.6 & 44.9 & 67.8 & 87.2 & 26.7 & 73.4 & \underline{32.6} & \textbf{53.3} & 39.0 & 56.4 & 10.2 & 49.24 \\ 
\rowcolor{gray!20} Point2RBox-v2 (ours) & \checkmark & 78.4 & 52.7 & 8.3 & 40.9 & 71.0 & 60.5 & 74.7 & 88.7 & \underline{65.5} & 72.1 & 24.4 & 26.1 & 30.1 & 50.7 & 21.0 & 51.00 \\ 
\rowcolor{gray!20} Point2RBox-v2 (ours) & $\times$ & \textbf{88.0} & \textbf{72.6} & 8.0 & 46.2 & \textbf{79.6} & \textbf{76.3} & \textbf{86.9} & 89.1 & \textbf{79.7} & \textbf{82.9} & 26.2 & 45.3 & \textbf{45.8} & \textbf{66.3} & \textbf{46.3} & \textbf{62.61} \\ \bottomrule
\specialrule{0pt}{2pt}{0pt}
\multicolumn{18}{l}{$^*$Comparison tracks: \checkmark = End-to-end training and testing; $\times$ = Generating pseudo labels to train the FCOS detector (two-stage training).} \\
\multicolumn{18}{l}{$^\dagger$Using additional priors. P2RBox: Pre-trained SAM model; Point2RBox+SK: One-shot sketches for each class.} \\
\multicolumn{18}{l}{$^1$PL: Plane, BD: Baseball diamond, BR: Bridge, GTF: Ground track field, SV: Small vehicle, LV: Large vehicle, SH: Ship, TC: Tennis court,} \\
\multicolumn{18}{l}{$\,\;$BC: Basketball court, ST: Storage tank, SBF: Soccer-ball field, RA: Roundabout, HA: Harbor, SP: Swimming pool, HC: Helicopter.} \\
\multicolumn{18}{l}{$^2$-RBox: The minimum rectangle operation is performed on the output Mask to obtain the RBox.} \\
\bottomrule
\end{tabular}
\caption{Detection performance of each category on the DOTA-v1.0 and the mean AP$_\text{50}$ of all categories.}
\label{tab:exp_dota}
\vspace{-10pt}
\end{table*}

\subsection{Symmetry-aware Learning}
\label{sec:method-lss}

Proven effective in learning the rotation of objects \cite{yu2023h2rboxv2}, in this work, symmetry-aware learning is extended to Gaussian-based OOD. In the left part of Fig.~\ref{fig:arch}, the training image $I$ is transformed (randomly selected from rotation, flip, and scale, see Table \ref{tab:abl_pro}) to generate an augmented view as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
I_\text{aug} = \alpha I
\end{equation}
where
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\alpha = \begin{bmatrix}
 \cos \mathcal{R} & -\sin \mathcal{R} \\
 \sin \mathcal{R} & \cos \mathcal{R}
\end{bmatrix}^{p_1}
\begin{bmatrix}
 1 & 0 \\
 0 & -1
\end{bmatrix}^{p_2}
\begin{bmatrix}
 s & 0 \\
 0 & s
\end{bmatrix}^{p_3}
\end{equation}
where $(p_1, p_2, p_3) = (1, 0, 0)$ when rotation is selected, $(0, 1, 0)$ when flip, $(0, 0, 1)$ when scale. $\mathcal{R}$ and $s$ are the random amount of rotation and scale, $s \in (0.5, 0.9)$.

By feeding both $I$ and $I_\text{aug}$ into the network, we obtain two sets of output Gaussian distributions and angles:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\left\{\begin{array}l
\left ( \Sigma, \theta
\right ) = f_\text{nn}\left ( I \right ) \\
\left ( \Sigma_\text{aug}, \theta_\text{aug}
\right ) = f_\text{nn}\left ( I_\text{aug} \right )
\end{array}\right.
\end{equation}

The consistency loss is calculated between the two sets so that the network learns the size and rotation variation:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathcal{L}_\text{ss} = L_\text{GWD}\left ( \alpha\Sigma\alpha^\top, \Sigma_\text{aug} \right ) 
\!+\! L_\text{ANG}\left ( m\theta \!+\! \mathcal{R}, \theta_\text{aug} \right )
\end{equation}
where $\mathcal{R}$ is the rotation angle and $m=1$ when rotation is selected; $(m,\mathcal{R})=(-1,0)$ when flip; $(m,\mathcal{R})=(1,0)$ when scale. $L_\text{GWD}\left (\cdot\right )$ is Gaussian Wasserstein Distance Loss \cite{yang2023detecting} and $L_\text{ANG}\left (\cdot\right )$ is defined as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
L_\text{ANG}\left ( \theta_1, \theta_2 \right ) =\min_{k\in Z} \left ( smooth_{L1}\left ( \theta_1, k\pi+\theta_2  \right ) \right )
\end{equation}
where $\min\left ( \cdot \right )$ regresses the prediction toward the closest target to circumvent the periodicity problem \cite{yu2023h2rboxv2}.

\subsection{Copy-paste Augmentation}
\label{sec:method-cp}

Inspired by \cite{ghiasi2021copypaste}, we propose to crop the detected instances of step $k$ and paste them on the training image of step $k + 1$. The maximum number of paste boxes is limited to 10 in each step. We simply use the bounding boxes of the cropped instances as the regression targets, and use Gaussian Wasserstein Distance Loss \cite{yang2023detecting} to calculate $\mathcal{L}_\text{box}$.

\subsection{Overall Loss}

The overall loss $\mathcal{L}$ for Point2RBox-v2 can be expressed as:
\begin{equation} \setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt}
\mathcal{L}_\text{cls}\! +\! w_\text{box} \mathcal{L}_\text{box}\! +\! w_\text{O} \mathcal{L}_\text{O}\! +\! w_\text{W} \mathcal{L}_\text{W}\! +\! w_\text{E} \mathcal{L}_\text{E}\! +\! w_\text{ss} \mathcal{L}_\text{ss}
\end{equation}
where $\mathcal{L}_\text{cls}$ is the focal loss \cite{lin2017focal} for classification, $\mathcal{L}_\text{box}$ regresses boxes/centers toward copy-paste/ground-truth labels, $w_\text{box}$ is set to one by default, $(w_\text{O},w_\text{W},w_\text{E},w_\text{ss})$ are set to $(10, 5, 0.3, 1)$ based on our ablation studies (see Tables \ref{tab:abl_lo}-\ref{tab:abl_le}).
