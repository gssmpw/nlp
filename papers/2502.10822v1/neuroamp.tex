% \documentclass[lettersize,journal]{IEEEtran}


\documentclass[journal]{IEEEtai}
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021




\begin{document}

\title{NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for Personalized Hearing Aids}

\author{Shafique Ahmed, Ryandhimas E. Zezario, Hui-Guan Yuan, Amir Hussain, Hsin-Min Wang ~\IEEEmembership{IEEE Senior Member}, Wei-Ho Chung, Yu Tsao~\IEEEmembership{IEEE Senior Member}


% \author{Anonymous
        % <-this % stops a space
% \thanks{This paper was produced by the BIO-ASP, CITI, AS, Taiwan.}% <-this % stops a space
\thanks{}}

% The paper headers
%\markboth{abc}
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.
\maketitle

\begin{abstract}

The prevalence of hearing aids is increasing. However, optimizing the amplification processes of hearing aids remains challenging due to the complexity of integrating multiple modular components in traditional methods. To address this challenge, we present NeuroAMP, a novel deep neural network designed for end-to-end, personalized amplification in hearing aids. NeuroAMP leverages both spectral features and the listener's audiogram as inputs, and we investigate four architectures: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer. We also introduce Denoising NeuroAMP, an extension that integrates noise reduction along with amplification capabilities for improved performance in real-world scenarios. To enhance generalization, a comprehensive data augmentation strategy was employed during training on diverse speech (TIMIT and TMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the Hearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index (HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the Transformer architecture within NeuroAMP achieves the best performance, with SRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI) on the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy maintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ). Furthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC approach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a 10\% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results highlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable improvements in personalized hearing aid amplification.

\end{abstract}



\vspace{7mm} 

\begin{IEEEImpStatement}
This research introduces NeuroAMP, a novel deep learning-based hearing aid amplifier that replaces complex multi-stage signal processing with streamlined, end-to-end personalized amplification. We also develop Denoising NeuroAMP, which integrates noise reduction along with the amplification process. Validated on diverse datasets, these models outperform conventional and two-stage methods on standardized metrics such as HASPI and HASQI scores. Furthermore, we believe NeuroAMP's data-driven approach can contribute to advancements in hearing aid technology, particularly by enhancing user-centered hearing assistance and potentially supporting more effective communication for individuals with hearing loss. Future work will focus on real-time implementation and integrating user feedback to further refine personalization.

\end{IEEEImpStatement}


\begin{IEEEkeywords}
hearing aids, deep neural network, end-to-end amplification, NeuroAMP, Denoising
\end{IEEEkeywords}


\section{Introduction}

\IEEEPARstart{H}{earing} loss, a pervasive global health issue, continuously impacts the lives of hundreds of millions, leading to social isolation~\cite{shukla2020hearing}, depression~\cite{lawrence2020hearing}, diminished quality of life~\cite{ciorba2012impact}, and even cognitive decline~\cite{lin2013hearing}. The World Health Organization (WHO) highlights that approximately 430 million people worldwide currently face problems related to hearing loss~\cite{WHO_hearing_loss_2024, Audiologists_org_2024}. Despite the availability of hearing aids (HAs), which can mitigate these adverse effects, only 16\% of hearing-loss individuals in the United States consistently use them~\cite{HA2018use, HA2018use2}. This low adoption rate suggests a notable gap between the potential of hearing aids and their real-world utilization, emphasizing the importance of developing more effective and user-friendly solutions.


Non-use of hearing aids by people with hearing loss can be attributed to a variety of factors, including hearing aids being ineffective in noisy environments, suboptimal sound quality, insufficient benefit, and incompatibility with the individual's specific type of hearing loss \cite{HA2015use3}. To address these challenges, various amplifier strategies for hearing aids have been developed \cite{NAL-R, DSL, NAL-NL1, Compression, WDRCbenefit1}. These strategies primarily involve two key components: the prescription fitting formulas and compression techniques to determine the desired insertion gain, as shown in Fig. ~\ref{fig_1}. The prescription formula aims to enhance the audibility of audio based on an individual's audiogram~\cite{NAL-R, DSL}. Well-known prescription fitting formulas include NAL-NL1~\cite{NAL-NL1}, NAL-NL2~\cite{NAL-NL2}, NAL-R~\cite{NAL-R}, and DSL m[i/o]~\cite{DSLm}.


A common problem for people with hearing loss is a reduction in the auditory dynamic range, known as loudness recruitment. In this case, weak sounds may become inaudible, while loud sounds can be uncomfortably intense. Non-linear prescription formulas like NAL-NL1, NAL-NL2, and DSL m[i/o] play a crucial role in addressing the reduced auditory dynamic range through compression. These advanced formulas, however, are primarily used in commercial hearing aids and require licensing, which may limit broader accessibility. In this study, we utilize the widely adopted NAL-R prescription formula and incorporate it with Wide Dynamic Range Compression (WDRC) \cite{WDRCbenefit1} to introduce non-linearity, mimicking the behavior of more advanced systems like NAL-NL1. This approach aligns with the established practice in the open-source OpenMHA platform~\cite{OpenMHA}. Moreover, while conventional amplifiers demonstrate notable performance, they often face challenges in adapting to dynamic acoustic environments.

\begin{figure}[t]
\centering
\includegraphics[scale=0.50]{Figure_1.pdf}
\caption{Overview of a typical hearing-aid system, illustrating the modular components involved in signal processing, including microphone input, amplification, compression, and speaker output.}
\label{fig_1}
\end{figure}

These limitations in adaptability and optimization have led to the exploration of alternative approaches. One such promising approach is deep learning, which offers the potential to learn complex patterns from large datasets and create more adaptable systems. Recently, with the availability of huge amounts of training data and their corresponding labels, deep learning models have achieved notable performance in several speech processing applications. Additionally, the integration of deep learning models for speech processing in hearing aid devices has demonstrated robust performance, for example, in speech enhancement \cite{wang2017deep,akeroyd20232nd} and speech assessment tasks \cite{tu22_interspeech, edozezario22_interspeech, MAWALIM2023109663}.


Due to the advancement of deep learning, in this paper, we propose a deep learning-based neural amplifier model called NeuroAMP, designed to provide personalized amplification in an end-to-end manner. Our initial goal is to demonstrate that NeuroAMP can achieve comparable performance to established methods, while also providing the potential for future improvements through its inherent adaptability. Unlike conventional amplifiers (as indicated by the red dashed box in Fig.~\ref{fig_1}) that combine prescription fitting formulas and compression techniques to amplify audio inputs, NeuroAMP uses a neural network to perform amplification in an end-to-end manner. We hypothesized that the non-linearity of the neural network would allow NeuroAMP to capture a greater variety of acoustic information and more effectively adjust gain and compression based on the user's specific degree of hearing loss at different frequencies. The development of NeuroAMP involved exploring various neural network architectures, including convolutional neural networks (CNN), long short-term memory (LSTM), convolutional recurrent neural networks (CRNN), and Transformers. The training objective of NeuroAMP is to minimize the loss between the estimated amplified audio and the corresponding ground-truth audio. To ensure robust performance across diverse conditions, we also apply a data-augmentation strategy using datasets containing multiple languages and music. Experimental results confirm the superiority of the Transformer model over other models, achieving the best performance in almost all metrics (with Spearman's Rank Correlation Coefficient (SRCC) scores of 0.9927 for Hearing Aid
Speech Quality Index (HASQI) and 0.9905 for Hearing Aid Speech Perception Index (HASPI) on the TIMIT dataset, and 0.9738 for HAAQI on the Cadenza dataset). Additionally, we also verify the effectiveness of data augmentation in maintaining good performance on unseen datasets (with SRCC scores of 0.9810 for HASQI and 0.9914 for HASPI on the VCTK dataset, and 0.9892 for HAAQI on the MUSDDB18-HQ dataset). Furthermore, we demonstrate that NeuroAMP is able to mimic conventional amplification methods and provide personalized amplification in an end-to-end manner. 



To demonstrate its integration potential, we further introduce Denoising NeuroAMP, which combines NeuroAMP with a speech enhancement module, showcasing direct integration between amplification and enhancement. The development of Denoising NeuroAMP involves combining amplification and noise reduction in one module. Specifically, the model takes noisy audio input and outputs enhanced-amplified audio, and the training objective is to minimize the loss between the predicted enhanced-amplified output from the noisy input and the corresponding ground-truth clean audio. Experimental results confirm that Denoising NeuroAMP can achieve a HASPI score of 0.90 and a HASQI score of 0.59, compared to 0.89 and 0.55 for NAL-R+WDRC, and 0.85 and 0.30 for the two-stage Denoising\textgreater NAL-R+WDRC baseline, which further confirms the advantages of Denoising NeuroAMP as an integrated amplification and denoising approach.


The remainder of this paper is organized as follows. First, we review related work in Section \ref{sec:related}. Second, we introduce the proposed methods in Section \ref{sec:methodology}. In Section \ref{sec:experiments}, we describe the experimental setup, report the experimental results, and discuss our findings. Finally, we conclude our work in Section \ref{sec:conclusion}. 


\section{Related Work}
\label{sec:related}

\subsection{Prescription Fitting Formulas}
Prescription fitting formulas aim to amplify sound based on the user's degree of hearing loss, making speech signals audible across different frequencies while maintaining comfort. These formulas can be broadly categorized into linear and non-linear approaches. Linear amplification, exemplified by methods like NAL-R and NAL-RP \cite{NAL-RP}, provides a constant gain regardless of input volume and is generally preferred for individuals with flatter hearing loss profiles, larger dynamic ranges, and less variability in dynamic range across frequencies. It is also well-suited for those with less varied lifestyles and environments \cite{Compression}. NAL-R, developed by the Australian National Acoustic Laboratories, is commonly used for mild to moderate hearing loss, while NAL-RP is tailored for severe hearing loss. In contrast, non-linear amplification, such as NAL-NL1 \cite{NAL-NL1}, NAL-NL2 \cite{NAL-NL2}, and DSL m[i/o] \cite{DSLm}, adjusts the gain dynamically based on the input volume, frequency, and the user's specific hearing loss characteristics. This makes it more suitable for individuals with sloping hearing loss, reduced dynamic ranges, significant variations in dynamic range across frequencies, and more diverse lifestyles and listening environments. NAL-NL1 aims to equalize and normalize the relative volume levels of individual frequencies while optimizing the speech intelligibility for a given volume \cite{NAL-NL1}. NAL-NL2, an improved version of NAL-NL1, considers additional factors in calculating gains, such as gender, age, and whether the native language is tonal, which may affect output volume. NAL-NL2 is also the most widely used fitting formula at present. In contrast, DSL m[i/o] is designed to prevent uncomfortable loudness while using hearing aids and to maximize the audibility of essential messages in conversations. Furthermore, these non-linear formulas utilize compression techniques to manage the reduced auditory dynamic range commonly experienced by individuals with sensorineural hearing loss.

\subsection{Hearing Aid Compression}

People with sensorineural hearing loss tend to have a smaller auditory dynamic range. For these individuals, soft sounds are inaudible, while intense sounds are perceived as loudly as they are by a normal-hearing ear. To optimize the use of the residual dynamic range and restore normal loudness perception, most modern hearing aids adopt compression. Wide Dynamic Range Compression (WDRC) \cite{WDRCbenefit1} is a widely adopted compression strategy designed to prevent sounds from being either too loud or too soft. The use of WDRC in hearing aids provides several benefits, such as ensuring consistent audibility of speech signals at safe and comfortable hearing levels \cite{WDRCbenefit1}. Moreover, WDRC involves multiple steps of audio processing, which are \cite{WDRCblock}:

\begin{itemize}
\item \textbf{Signal analysis through short-time Fourier transform (STFT):} The input audio signal undergoes signal analysis through STFT.

\item \textbf{Filterbank construction:} Frequency bins are grouped into a predefined number of filterbanks, and their volume levels are estimated.

\item \textbf{Calculation of filterbank-specific gain functions:} Gain functions specific to each filterbank are calculated based on the level estimations. 

\item \textbf{Interpolation and modification:} The gain functions are interpolated to individual frequency bins and then used to modify the STFT representation of the input signal. 

\item \textbf{Signal reconstruction:} The modified STFT representation is converted back to the time domain using the Inverse STFT.
\end{itemize}

These steps ensure that the WDRC system effectively compresses a wide range of volume levels while maintaining listener comfort. In addition, the WDRC considers the listener's hearing thresholds and dynamic range variations across frequencies, aiming to enhance speech intelligibility.



\section{Methodology}
\label{sec:methodology}

This section presents the architectures of the two proposed frameworks. 
We first introduce the \textit{NeuroAMP} architecture, which outlines the core design for audio amplification 
using a neural network-based system. Subsequently, we introduce the \textit{Denoising NeuroAMP} architecture, which is an extension of \textit{NeuroAMP} that includes noise reduction capabilities.


\subsection{NeuroAMP Architecture}
The processing workflow of NeuroAMP involves taking an audio signal and the listener's audiogram as inputs, processing them through a neural network, and producing an amplified output signal. The detailed architecture of the NeuroAMP model is illustrated in Fig.~\ref{fig_N}.
Specifically, given the input signal $y$, the STFT is processed to extract $y$ into its time-frequency spectral features ($\textbf{Y} = [\textbf{y}_1, \dots, \textbf{y}_t, \dots, \textbf{y}_T]$). Simultaneously, the audiogram $z$ is transformed into a vector representation via a dense layer and then replicated into a sequence of length $T$ ($\textbf{Z} = [\textbf{z}_1, \dots, \textbf{z}_t, \dots, \textbf{z}_T]$). The two input streams are concatenated along the y-axis feature dimension and then passed to the subsequent layers of the model. In our setup, we plan to concatenate two sets of features in the latent space, leveraging its ability to capture semantic feature similarity and perform feature fusion. This approach has been shown to be effective for building personalized deep-learning models \cite{speaker_emb}. The feedforward process of NeuroAMP is defined as follows:
\begin{equation}
\begin{array}{c}
\mathbf{Y} = \text{STFT}(y) \\
\mathbf{Z} = \text{Dense}(z) \\
\mathbf{Concat} = [\mathbf{Y} \mid \mathbf{Z}] \\
\mathbf{\hat{Y}} = \text{NeuroAMP}(\mathbf{Concat}). \\
\end{array}
\label{eq}
\end{equation}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.57]{Figures/Neuro_model.pdf}
\caption{Architecture of the NeuroAMP model, illustrating the process flow from the input signal and audiogram to feature extraction and neural network processing to the final output signal.
}
\label{fig_N}
\end{figure}

In this work, for the core NN module in NeuroAMP, we select four neural network architectures, including the Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer.

\subsubsection{Convolutional Neural Network (CNN)}
The Convolutional Neural Network (CNN) is integrated into the NeuroAMP model due to its ability to extract hierarchical features from spatial or spectral data~\cite{cnn_general}. CNNs are particularly adept at recognizing patterns and features in audio spectrograms from complex audio signals~\cite{fcnse, cnn_speech}. In the NeuroAMP model, the CNN is used to process spectral features extracted by the STFT, with the goal of capturing detailed representations of the frequency components of the audio signal. The operation process of the CNN in the NeuroAMP model can be described by the following equations:
\begin{equation}
\begin{aligned}
\mathbf{h}_1 &= \text{ReLU}(\text{Conv}(\mathbf{Concat}, \mathbf{W}_1) + \mathbf{b}_1) \\
\mathbf{h}_2 &= \text{ReLU}(\text{Conv}(\mathbf{h}_1, \mathbf{W}_2) + \mathbf{b}_2) \\
&\vdots \\
\mathbf{h}_l &= \text{ReLU}(\text{Conv}(\mathbf{h}_{l-1}, \mathbf{W}_l) + \mathbf{b}_l) \\
&\vdots \\
\mathbf{\hat{Y}} &= \text{Dense}(\text{Flatten}(\mathbf{h}_{L-1}))
\end{aligned}
\end{equation}
where $\mathbf{W}_l$ and $\mathbf{b}_l$ are the convolutional weights and biases of the \(l\)-th layer, respectively, and \(L\) represents the number of convolutional layers. This architecture ensures a comprehensive feature extraction process, allowing the NeuroAMP model to more effectively distinguish between auditory elements.

\subsubsection{Long Short-Term Memory (LSTM)}
LSTM is chosen for its proficiency in modeling temporal dependencies in sequential data~\cite{lstm, lstm1}. In the context of hearing aids, preserving the temporal structure of the audio signal is essential to maintaining speech intelligibility and naturalness. LSTM is effective at capturing long-range dependencies and temporal patterns~\cite{lstm2}, which is crucial for processing continuous audio signals and adapting to varying acoustic environments. The process of the LSTM in the NeuroAMP model is described as follows:
\begin{equation}
\begin{aligned}
\mathbf{h}_t, \mathbf{c}_t &= \text{LSTM}(\mathbf{Concat}, \mathbf{h}_{t-1}, \mathbf{c}_{t-1}) \\
\mathbf{\hat{Y}} &= \text{Dense}(\mathbf{h}_T),
\end{aligned}
\end{equation}
where $\mathbf{h}_t$ and $\mathbf{c}_t$ are the hidden state and cell state at time step $t$, respectively.

\subsubsection{Convolutional Recurrent Neural Network (CRNN)}
CRNN combines the strengths of CNN and RNN, making it ideal for signal processing tasks~\cite{crnn, crnnse} that involve both spatial and temporal feature extraction. Its convolutional layers capture local spectral features, while the recurrent layers model temporal dynamics. This hybrid approach enhances the network's ability to manage audio signal complexity, thereby improving performance in tasks like speech enhancement~\cite{tan2018convolutional, hsieh2020wavecrn}. The process of the CRNN in the NeuroAMP model is described as follows:
\begin{equation}
\begin{aligned}
\mathbf{h}_c &= \text{CNN}(\mathbf{Concat}) \\
\mathbf{h}_t, \mathbf{c}_t &= \text{LSTM}(\mathbf{h}_c, \mathbf{h}_{t-1}, \mathbf{c}_{t-1}) \\
\mathbf{\hat{Y}} &= \text{Dense}(\mathbf{h}_T),
\end{aligned}
\end{equation}
where $\mathbf{h}_c$ is the output of the convolutional layer.

\subsubsection{Transformer}
The Transformer is included for its state-of-the-art performance in sequence modeling tasks~\cite{vaswani2017attention, transformersurvey, wolf2020transformers, baevski2020wav2vec}. Unlike traditional RNNs, the Transformer uses a self-attention mechanism~\cite{vaswani2017attention} to process entire sequences in parallel, allowing for efficient handling of long-range dependencies~\cite{karitaasru}. This capability is particularly beneficial in audio signal processing~\cite{stransformer}, where capturing the global context and dependencies can enhance performance. The process of the Transformer in the NeuroAMP model is described as follows:

\begin{equation}
\begin{aligned}
\mathbf{Q}, \mathbf{K}, \mathbf{V} &= \mathbf{W}_Q \mathbf{Concat}, \mathbf{W}_K \mathbf{Concat}, \mathbf{W}_V \mathbf{Concat} \\
\mathbf{Attention} &= \text{Softmax} \left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V} \\
\mathbf{h} &= \text{FFN}(\mathbf{Attention}) \\
\mathbf{\hat{Y}} &= \text{Dense}(\mathbf{h}),
\end{aligned}
\end{equation}
where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are the query, key, and value matrices, $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$ are the weight matrices for the query, key, and value, $d_k$ is the dimension of the key, and FFN is the feedforward network.

\subsubsection{Loss function}
To optimize NeuroAMP, we use Mean Squared Error (MSE) as the loss function. This approach penalizes deviations between predicted ($\hat{\textbf{Y}}$) and ground-truth ($\textbf{Y}$) spectral representations, guiding the network to learn spectral mappings. The loss function is calculated as:

\begin{equation}
\label{mse}
L_{NeuroAMP} = \frac{1}{T} \sum_{i=1}^T |\textbf{y}_t - \hat{\textbf{y}}_t|^2,
\end{equation}
where $\textbf{y}_t$ and $\hat{\textbf{y}}_t$ are the ground-truth amplified and estimated-amplified spectral representations of the $t$-th frame of the training audio signal $y$, and $T$ is the number of frames.


\subsection{Denoising NeuroAMP Architecture}


To address the challenges of noisy environments, which severely impact the effectiveness of hearing aids, we propose the Denoising NeuroAMP system. This system is built on the NeuroAMP architecture, allowing simultaneous denoising and amplification by seamlessly integrating speech enhancement within the neural network. The architecture of the Denoising NeuroAMP model is illustrated in Fig.~\ref{fig_DN}.

\begin{figure}[!t]
\centering
\includegraphics[scale=0.57]{Figures/DNeuro_model.pdf}
\caption{Architecture of the Denoising NeuroAMP model, illustrating the process flow from the input signal and audiogram to feature extraction and neural network processing to the final output signal.
}
\label{fig_DN}
\end{figure}


As shown in Fig.~\ref{fig_DN}, Denoising NeuroAMP takes a noisy audio signal (\textbf{X}) as input, represented as a magnitude spectrogram. Simultaneously, the listener's audiogram (\textbf{Z}) is processed through a dense layer to construct a vector representation corresponding to the hearing profile. These two features – the noisy magnitude spectrogram and the audiogram vector – are concatenated and then fed into the core Denoising NeuroAMP neural network. This network, while conceptually similar to the original NeuroAMP model, is trained to perform both speech enhancement and personalized amplification in a single unified process. It learns to adapt its processing based on the characteristics of the noisy input and the specific hearing loss profile, using the audiogram to guide optimization toward the amplified clean audio output.

The training objective of Denoising NeuroAMP, as shown in Eq. \ref{mse_den_neuro}, is to minimize the MSE between the ground-truth magnitude spectrogram (obtained by applying the NAL-R prescription formula and WDRC to the clean input audio) and the magnitude spectrogram of the predicted enhanced-amplified output from the noisy input. This ensures that the system learns to produce amplified outputs that are both enhanced and aligned with established hearing aid fitting practices.


\begin{equation}
\begin{array}{c}
\mathbf{X} = \text{STFT}(x) \\
\mathbf{Z} = \text{Dense}(z) \\
\mathbf{Concat} = [\mathbf{X} \mid \mathbf{Z}] \\
\mathbf{\tilde{X}} = \text{Den-NeuroAMP}(\mathbf{Concat}). \\
\end{array}
\label{eq1}
\end{equation}

\begin{equation}
\label{mse_den_neuro}
L_{Den-NeuroAMP} = \frac{1}{T} \sum_{i=1}^T |\textit{NALR-WDRC}(\textbf{y}_t) - \tilde{\textbf{x}}_t|^2,
\end{equation}

The output of the Denoising NeuroAMP network is a predicted magnitude spectrogram of the amplified-enhanced signal. The phase information of the input noisy signal is combined with the predicted magnitude spectrogram to reconstruct the time-domain, enhanced-amplified audio signal through the Inverse Short-Time Fourier Transform (ISTFT). 


\section{Experiments}
\label{sec:experiments}
\subsection{Dataset}
% \subsection{Data Augmentation}
In this work, the input to NeuroAMP includes two types of speech signals (English and Taiwanese Mandarin speech data) and music data. We aim to evaluated the proposed models under diverse acoustic properties of cross-linguistic speech and music signals. The speech data contains three conditions: clean, noisy, and enhanced speech. To generate noisy speech, we added 100 types of environmental noise from~\cite{noise} to clean speech at different signal-to-noise ratio (SNR) levels. Enhanced speech was then obtained by applying two deep learning-based speech enhancement models to the noisy data. The first model is based on LSTM~\cite{lstmse}, which operates on STFT features to enhance noisy speech. The second model is based on a Fully Convolutional Network (FCN)~\cite{fcnse}, which operates on the raw waveform to enhance noisy speech. For music data, we mainly consider two categories: clean music and noisy music (i.e., music with added noise). Clean music represents the original recording, while noisy music incorporates various environmental disturbances to simulate real-world listening scenarios. 
To train the NeuroAMP model, we used TIMIT, TMHINT, and the MUSIC dataset provided by the Cadenza Challenge~\cite{cedenza1}. To test and evaluate the model's performance and generalization, we also included the unseen test set (the acoustic conditions and audio content are entirely different from the training conditions) from the VoiceBank and MUSDB18-HQ datasets.

\subsubsection{Training Dataset Description}
\begin{itemize}
    \item \textbf{TIMIT Dataset}~\cite{timit}: The TIMIT dataset is a well-known speech corpus used for tasks like speech recognition~\cite{timitasr, timitse} and speech enhancement~\cite{timitse, timitse1}. It includes recordings from 630 speakers representing eight major dialects of American English, with each speaker reading 10 phonetically rich sentences. The TIMIT training set consists of 4,620 audio files, while the test set comprises 1,690 audio files. In addition, to augment the dataset, noise signal was injected at different SNR levels(-5 dB, 0 dB, and 5 dB) into the clean audio to create noisy audio. In addition, enhanced versions of these noisy audio were generated using two SE models, which are LSTM and FCN. Following augmentation, a total of 13,700 training samples were selected from the TIMIT dataset, consisting of 3425 clean audio, 3425 noisy audio, and 6850 enhanced audio.
    
    \item \textbf{TMHINT Dataset}: During model training, we incorporated an additional dataset from the Taiwan Mandarin Hearing in Noise Test (TMHINT) to improve the reliability of our models by including diverse acoustic conditions. The TMHINT dataset has been widely used in various speech-related tasks, such as speech assessment~\cite{mosa} and enhancement~\cite{timitse}. Specifically, the TMHINT dataset consists of recordings from multiple speakers, with 16 phonemically balanced sentence sets, each containing 20 sentences. Each sentence comprises ten characters representing diverse and balanced phonetic elements of Mandarin, including four primary lexical tones and a neutral tone. The total number of utterances in this dataset was 1440, which we divided into training and testing sets using an 80/20 ratio, ensuring that speakers in the test set were not used in training. In addition, we expanded the TMHINT dataset by adding environmental noises at different SNR levels (-5 dB, 0 dB, and 5 dB). We also generated enhanced speech from the corresponding noisy speech using LSTM and FCN SE models. We then generated reference data using NAL-R+WDRC, and split this equally between the training and testing sets.
    % In total, we selected 5,856 training samples from the TIMIT dataset, consisting of 1,464 clean audio samples, 1,464 noisy audio samples, and 1,464 samples for each type of enhanced audio.
    
    \item \textbf{MUSIC Dataset}: The music dataset from the Cadenza Challenge~\cite{cedenza1, cedenza2} was incorporated to train the AMP-Net model, utilizing its diverse collection of musical samples. Additionally, to simulate real-world listening conditions, various types of environmental noise were injected into the music data at multiple SNR levels (-5 dB, 0 dB, and 5 dB). This augmentation process was designed to mimic common scenarios where music is played alongside background noise. In total, we selected 11,856 training samples, consisting of 5,928 clean audio samples and 5,928 noisy audio samples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
\subsubsection{Testing Dataset Description}
To evaluate the models' performance, we first utilized the test sets from the same datasets used during training: TIMIT, TMHINT, and the Cadenza Challenge music data. Our evaluation methodology involved testing the models on these datasets with added unseen noises at varying SNR levels of -6 dB, 0 dB, and 6 dB. This approach allows us to assess how well the models handle different noise conditions. 

In addition, we added separate speech and music test sets that were not added to train the model (VoiceBank \cite{vctk}, and MUSDB18-HQ \cite{musdb}) datasets to evaluate the models' ability to generalize to the unseen environments. The detailed information for the two unseen datasets is as follows:

 
    \item \textbf{VoiceBank Dataset}~\cite{vctk}: This dataset has been widely employed in various speech-related tasks, such as speech enhancement~\cite{matricse, vctkse} and recognition~\cite{vctkasr}. The dataset contains recordings of isolated English digits spoken by multiple speakers, capturing diverse accents and conditions. The test set includes both clean and noisy samples, each containing 824 utterances from 2 speakers. The noisy samples cover four SNR levels: 17.5 dB, 12.5 dB, 7.5 dB, and 2.5 dB, representing different degrees of noise added to the speech signals. 

    
    \item \textbf{MUSDB18-HQ Dataset}~\cite{musdb}: The evaluation set was expanded by including the MUSDB18 dataset, another prominent resource in the field of audio processing~\cite{musdb1, musdb2}. The MUSDB18 dataset comprises multi-track music recordings, facilitating source separation and music transcription tasks. By incorporating the MUSDB18 dataset into our evaluation process, we aim to assess the performance of our models in handling music signals.

\end{itemize}


\subsubsection{Denoising NeuroAMP Dataset}
For training and evaluating the Denoising NeuroAMP framework, we utilized the VoiceBank+DEMAND dataset~\cite{vctk}. This dataset is widely recognized and employed in the field of speech enhancement, making it particularly suitable for our task of combining denoising with personalized amplification.
The VoiceBank+DEMAND dataset comprises the VoiceBank corpus, which contains clean speech recordings from multiple speakers, and the DEMAND corpus, which provides a diverse set of environmental noises. This combination allows the creation of noisy speech samples by mixing clean speech with various types of noise at different SNR levels. The official split of the dataset was used, which includes a training set of 11,572 utterances from 28 speakers and a test set of 824 utterances from 2 speakers. The availability of both clean and noisy speech makes it ideal for training models to perform both speech enhancement and amplification.

\subsubsection{Hearing Loss Patterns}
In our study, we use audiograms from the Clarity Challenge, which provides comprehensive samples for both training and testing. These audiograms are essential for evaluating hearing abilities across a standard set of frequencies. We specifically examine audiograms that cover six fundamental frequencies: 250, 500, 1000, 2000, 4000, and 6000 Hz. These frequencies are important for assessing speech intelligibility and understanding the auditory challenges faced by individuals with hearing loss. We follow the approach described by Alshuaib et al.~\cite{hlp} to generate hearing loss patterns, which offers a systematic method for categorizing audiometric data into distinct patterns. 


\subsection{Experimental Setup}

This section details the experimental setups for both the NeuroAMP and Denoising NeuroAMP models.

\subsubsection{NeuroAMP Setup}

In the NeuroAMP experimental setup, the input signal is preprocessed with a 512-point STFT, a Hamming window of 32 ms, and a hop of 16 ms. The prescription input, which is a 1x6 matrix representing the listener's audiogram with intensity values for specific frequencies, is processed through dense layers in our model to create a refined representation. This processed audiogram information is then concatenated with the spectral features to form the input vector for further modeling.

The various model architectures and their configurations employed in our study are as follows:

\begin{itemize}
\item \textbf{Convolutional Neural Network (CNN):} The CNN configuration in NeuroAMP includes four convolutional layers with filter sizes of 32, 64, 128, and 256. This setup aims to extract detailed spatial features from the audio signals.
\item \textbf{Long Short-Term Memory (LSTM):} The LSTM model configuration includes two layers, each with 256 units, designed to capture temporal dependencies in the audio signal.
\item \textbf{Convolutional Recurrent Neural Network (CRNN):} The CRNN model combines four convolutional layers with 16, 32, 64, and 128 filter sizes, followed by two LSTM layers, each with 256 units. This structure is particularly effective for handling both spectral features and temporal sequences.
\item \textbf{Transformer:} The transformer model includes four encoder blocks, each with 16 heads. These blocks use an attention mechanism that helps the model better understand subtle details in complex audio settings.
\end{itemize}

Each model configuration includes a 257-unit dense layer for the final output, ensuring robust feature extraction and signal processing. This architecture enables NeuroAMP to effectively integrate the spectral features from the STFT and the detailed information from the prescription input.

To optimize our model, we have utilized the Adam optimizer with the following hyperparameters: a learning rate of 0.0001, \(\beta_1\) of 0.9, \(\beta_2\) of 0.999, and \(\epsilon\) of 1e-07. The training process is configured with a batch size of 1 and 100 epochs. Early stopping is enabled to prevent overfitting, ensuring the system achieves optimal performance.


\subsubsection{Denoising NeuroAMP Setup}

For the Denoising NeuroAMP experiments, the setup largely mirrors that of the NeuroAMP, with a few key distinctions. The input signal is similarly preprocessed with a 512-point STFT using a 32 ms Hamming window and a 16 ms hop. The listener's audiogram, represented as a 1x6 matrix, is processed through a dense layer and concatenated with the spectral features of the noisy input signal.
In the Denoising NeuroAMP, we specifically employ an LSTM-based neural network architecture. The LSTM model includes two layers, each with 256 units. This configuration is chosen for its effectiveness in capturing temporal dependencies in the audio signal, which is crucial for both speech enhancement and personalized amplification.
The training target for Denoising NeuroAMP is generated by applying the NAL-R prescription formula combined with WDRC to the clean version of the input audio signal. We use the Mean Squared Error (MSE) as the loss function, calculated between the predicted magnitude spectrogram and the target magnitude spectrogram.

Similar to NeuroAMP, the Denoising NeuroAMP model is optimized using the Adam optimizer. The hyperparameters are identical: a learning rate of 0.0001, \(\beta_1\) of 0.9, \(\beta_2\) of 0.999, and \(\epsilon\) of 1e-07. The model is trained with a batch size of 1 for 100 epochs, and early stopping is used to prevent overfitting.

\subsection{Evaluation Results}

This section outlines the evaluation strategy, focusing on assessing the intelligibility and quality of amplified speech processed by the proposed NeuroAMP model and NAL-R+WDRC (baseline). We employ two established metrics in hearing aid auditory research: HASPI and HASQI for speech, along with the Hearing Aid Audio Quality Index (HAAQI) for music. These metrics are important for quantifying the model's impact on both speech and music quality. For benchmarking purposes, we compare the metrics derived from NeuroAMP with those obtained from the NAL-R+WDRC processed signal, using three statistical measures: Linear Correlation Coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC), and Mean Squared Error (MSE). Higher LCC values, higher SRCC values, and lower MSE values indicate greater similarity between NeuroAMP and NAL-R+WDRC.

Furthermore, we explore the relationship between model parameter scale and performance through two experimental settings: a basic parameter configuration for all neural networks to establish a baseline (denoted as basic (same)) and an optimized configuration to evaluate the full potential of each architecture (denoted as optimized).


\subsubsection{Impact of Model Parameter Scale on Performance}

\textbf{Table I} summarizes the number of parameters for each model under basic (same) and optimized settings. This comparison aims to evaluate the trade-offs between model complexity and performance.

\begin{table}[t]
  \centering
  \caption{Number of Parameters of Basic and Optimized for Each Model}
  \label{tab:combined_parameters}
  \resizebox{\columnwidth}{!}{ % Ensures the table fits within the column width
  \begin{tabular}{lcc}
    \toprule
    Model & Parameters (Basic(Same)) & Parameters (Optimized) \\
    \midrule
    CNN & 498,411 & 672,235 \\
    LSTM & 498,224 & 1,134,193 \\
    CRNN & 496,408 & 1,503,595 \\
    Transformer & 499,884 & 2,456,398 \\
    \bottomrule
  \end{tabular}
  }
\end{table}


\textbf{Table II} presents the results under the basic (same) parameter setting. With these constraints, the Transformer model consistently exhibited superior performance across most datasets and metrics. It achieved the highest LCC and SRCC values and the lowest MSE, indicating its inherent architectural efficiency and ability to generalize well even with limited parameters.

\begin{table*}[t]
  \centering
  \caption{LCC, SRCC, and MSE Results of Different Models on Speech Datasets (basic (same) Parameter Setting)}
  \label{tab:performance1}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccccccccccc}
    \toprule
    \multirow{2}{*}{Metric} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{TIMIT} & \multicolumn{3}{c}{TMHINT} & \multicolumn{3}{c}{VCTK (Unseen)} \\
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}
    & & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ \\
    \midrule
    \multirow{4}{*}{HASPI} 
    & CNN & 0.9707 & 0.9846 & 0.0058 & 0.9863 & 0.9833 & 0.0021 & 0.9314 & 0.9705 & 0.0038 \\
    & LSTM & 0.9816 & 0.9894 & 0.0037 & 0.9882 & 0.9855 & 0.0018 & 0.9447 & 0.9716 & 0.0031 \\   
    & CRNN & 0.9728 & 0.9788 & 0.0057 & 0.9785 & 0.9736 & 0.0041 & 0.9212 & 0.9437 & 0.0059 \\
    & Transformer & \textbf{0.9814} & \textbf{0.9901} & \textbf{0.0036} & \textbf{0.9903} & \textbf{0.9902} & \textbf{0.0015} & \textbf{0.9548} & \textbf{0.9811} & \textbf{0.0023} \\
    \midrule
    \multirow{4}{*}{HASQI} 
    & CNN & 0.9943 & 0.9767 & 0.0013 & 0.9948 & 0.9770 & 0.0014 & 0.9894 & 0.9822 & 0.0020 \\
    & LSTM & \textbf{0.9971} & \textbf{0.9869} & 0.0010 & 0.9954 & 0.9784 & 0.0014 & \textbf{0.9950} & \textbf{0.9924} & 0.0019 \\
    & CRNN & 0.9902 & 0.9757 & 0.0084 & 0.9878 & 0.9671 & 0.0112 & 0.9851 & 0.9789 & 0.0154 \\
    & Transformer & 0.9966 & 0.9858 & \textbf{0.0007} & \textbf{0.9957} & \textbf{0.9805} & \textbf{0.0010} & 0.9918 & 0.9874 & \textbf{0.0011} \\
    \bottomrule
  \end{tabular}
  }
\end{table*}

\textbf{Table III} shows the results after optimizing the parameters. With increased parameter counts, almost all models exhibited consistent performance improvements. Notably, the CNN and LSTM models showed substantial gains in their ability to generalize, particularly on the TMHINT dataset. The CRNN model, which initially struggled under the minimal setting, demonstrated a notable improvement and achieved comparable performance after tuning. Furthermore, the Transformer model achieves the best performance and further improves out-of-domain generalization in the unseen VCTK dataset.

\begin{table*}[t]
  \centering
  \caption{LCC, SRCC, and MSE Results of Different Models on Speech Datasets (Optimized Parameter Setting)}
  \label{tab:performance}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccccccccccc}
    \toprule
    \multirow{2}{*}{Metric} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{TIMIT} & \multicolumn{3}{c}{TMHINT} & \multicolumn{3}{c}{VCTK (Unseen)} \\
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}
    & & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ \\
    \midrule
    \multirow{4}{*}{HASPI} 
    & CNN & 0.9684 & 0.9822 & 0.0063 & 0.9821 & 0.9801 & 0.0028 & 0.9211 & 0.9619 & 0.0046 \\
    & LSTM & 0.9822 & 0.9893 & 0.0036 & 0.9872 & 0.9834 & 0.0021 & 0.9435 & 0.9708 & 0.0031 \\   
    & CRNN & \textbf{0.9864} & 0.9923 & \textbf{0.0026} & 0.9892 & 0.9858 & 0.0016 & \textbf{0.9513} & 0.9779 & \textbf{0.0025} \\
    & Transformer & \textbf{0.9864} & \textbf{0.9927} & 0.0027 & \textbf{0.9928} & \textbf{0.9923} & \textbf{0.0010} & 0.9493 & \textbf{0.9810} & 0.0026 \\
    \midrule
    \multirow{4}{*}{HASQI} 
    & CNN & 0.9941 & 0.9771 & 0.0017 & 0.9929 & 0.9755 & 0.0023 & 0.9870 & 0.9775 & 0.0035 \\
    & LSTM & 0.9964 & 0.9847 & 0.0018 & 0.9958 & 0.9804 & 0.0023 & 0.9942 & 0.9912 & 0.0026 \\
    & CRNN & 0.9971 & 0.9864 & 0.0011 & 0.9951 & 0.9794 & 0.0014 & \textbf{0.9946} & \textbf{0.9920} & 0.0021 \\
    & Transformer & \textbf{0.9979} & \textbf{0.9905} & \textbf{0.0005} & \textbf{0.9965} & \textbf{0.9827} & \textbf{0.0007} & 0.9941 & 0.9914 & \textbf{0.0008} \\
    \bottomrule
  \end{tabular}
  }
\end{table*}


\textbf{Table IV} provides a detailed performance evaluation of the CNN, LSTM, CRNN, and Transformer models on the VoiceBank dataset (unseen) under different speech conditions: clean, noisy, and enhanced (using FCN and LSTM). The CRNN model excels in clean and enhanced conditions, achieving high LCC and SRCC scores along with notably low MSE values. The Transformer model also performs well, particularly in clean and enhanced conditions. 


\begin{table*}[t]
  \centering
  \caption{Detailed evaluation metrics for clean, noisy, and enhanced Speech conditions}
  \label{tab:detail_performance}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lclcccccccccccc}
    \toprule
    \multirow{4}{*}{Metric} & \multirow{4}{*}{Model} & \multicolumn{3}{c}{Clean} & \multicolumn{3}{c}{Enhanced(FCN)} & \multicolumn{3}{c}{Enhanced(LSTM)} & \multicolumn{3}{c}{Noisy} \\
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}
    & & LCC \textuparrow & SRCC \textuparrow & MSE \textdownarrow & LCC \textuparrow & SRCC \textuparrow & MSE \textdownarrow & LCC \textuparrow & SRCC \textuparrow & MSE \textdownarrow & LCC \textuparrow & SRCC \textuparrow & MSE \textdownarrow \\
    \midrule
    \multirow{4}{*}{HASPI} & CNN & 0.9280 & 0.9654 & 0.0012 & 0.8913 & 0.9209 & 0.0073 & 0.8932 & 0.9199 & 0.0071 & 0.9608 & 0.9781 & 0.0027 \\
                          & LSTM  & 0.9457 & 0.9761 & 0.001 & 0.9285 & 0.9503 & 0.0045 & 0.9162 & 0.9381 & 0.0053 & 0.9754 & 0.9831 & 0.0016 \\
                          & CRNN & 0.9530 & 0.9819 & 0.0007 & 0.9315 & 0.9543 & 0.0043 & 0.9332 & 0.9543 & 0.0040 & 0.9811 & 0.9883 & 0.0010 \\
                          & Transformer & 0.9427 & 0.9876 & 0.0008 & 0.9465 & 0.9688 & 0.0033 & 0.9145 & 0.9504 & 0.0052 & 0.9781 & 0.9882 & 0.0012 \\
    \midrule
    \multirow{4}{*}{HASQI} & CNN & 0.7654 & 0.3800 & 0.0088 & 0.9530 & 0.9533 & 0.0026 & 0.9645 & 0.9639 & 0.0006 & 0.9865 & 0.9856 & 0.0020 \\
                          & LSTM  & 0.9164 & 0.7983 & 0.0074 & 0.9747 & 0.9791 & 0.0013 & 0.9774 & 0.9795 & 0.0004 & 0.994 & 0.9937 & 0.0014 \\
                          & CRNN & 0.9255 & 0.8421 & 0.0055 & 0.9715 & 0.9755 & 0.0011 & 0.9773 & 0.9795 & 0.0004 & 0.9941 & 0.9935 & 0.0012 \\
                          & Transformer & 0.9013 & 0.8062 & 0.0016 & 0.9729 & 0.9693 & 0.0006 & 0.9786 & 0.9781 & 0.0003 & 0.9927 & 0.9922 & 0.0007 \\
    \bottomrule
  \end{tabular}
  }
\end{table*}


\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{./Figures/TMHINT_HASPI_plots.pdf}
\caption{TMHINT dataset scatter plots for HASPI scores (a) CNN model architecture (b) LSTM model architecture (c) CRNN model architecture (d) Transformer model architecture}
\label{fig_6}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{./Figures/TMHINT_HASQI_plots.pdf}
\caption{TMHINT dataset scatter plots for HASQI scores (a) CNN model architecture (b) LSTM model architecture (c) CRNN model architecture (d) Transformer model architecture}
\label{fig_7}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{./Figures/VCTK_HASPI_plots.pdf}
\caption{VCTK dataset scatter plots for HASPI scores (a) CNN model architecture (b) LSTM model architecture (c) CRNN model architecture (d) Transformer model architecture}
\label{fig_8}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{./Figures/VCTK_HASQI_plots.pdf}
\caption{VCTK dataset scatter plots for HASQI scores (a) CNN model architecture (b) LSTM model architecture (c) CRNN model architecture (d) Transformer model architecture}
\label{fig_9}
\end{figure} 

We also utilize scatter plots to visually compare and assess the prediction distribution between the NeuroAMP and the ground truth (NAL-R+WDRC) for various speech datasets. Specifically, we selected two datasets: the TMHINT dataset, which is included in our training data, and the VoiceBank dataset, which is unseen during training. For the evaluation, HASPI and HASQI scores are used as assessment metrics.

Fig. \ref{fig_6} displays scatter plots of the HASPI scores for the TMHINT dataset, separated by each model architecture: (a) CNN, (b) LSTM, (c) CRNN, and (d) Transformer. Fig. \ref{fig_7} shows scatter plots of the HASQI scores for the same dataset. Fig.s \ref{fig_8} and \ref{fig_9} illustrate scatter plots for HASPI and HASQI scores on the VoiceBank dataset, respectively, across all model architectures. Each scatter plot is color-coded to distinguish between different types of data: noisy, enhanced by FCN, enhanced by LSTM, and clean, allowing for a clear visualization of score distributions using HASPI and HASQI scores. The density of points along the diagonal in these plots reflects a high degree of agreement between the model outputs (NeuroAMP) and the ground truth (NAL-R+WDRC), suggesting superior model performance. Specifically, the Transformer and CRNN models exhibit distinctive patterns in these scatter plots. The Transformer model (Fig.s 4d and 7d) shows a particularly dense clustering of points along the diagonal across all types of data, suggesting robust performance and effective handling of variations in audio quality. Moreover, the CRNN model (Fig.s 4c and 7c) shows better alignment with the ground truth but exhibits slightly more dispersion, which may be due to the CRNN’s varying ability to handle certain dataset characteristics or noise levels.



\subsubsection{Performance Evaluation on Music Dataset}

\textbf{Table V} presents a comprehensive evaluation of our models using the HAAQI metric to assess music quality against ground-truth scores across two test sets: the Cadenza Challenge and MUSDB18-HQ. We employ LCC, SRCC, and MSE to assess correlation and accuracy. The Cadenza Challenge dataset, which is seen during the training phase, and the unseen MUSDB18-HQ dataset, which is not exposed to the model during training, are used to evaluate the models' abilities to generalize to new data.

\begin{table}[t]
  \centering
  \caption{Detailed evaluation metrics for Music datasets}
  \label{tab:performance_music}
  \small % Reduce font size
  \setlength{\tabcolsep}{3.0pt} % Reduce column padding
  \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{3}{c}{Cadenza Challenge} & \multicolumn{3}{c}{MUSDB18-HQ (Unseen)} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ & LCC $\uparrow$ & SRCC $\uparrow$ & MSE $\downarrow$ \\
    \midrule
    % DNN & 0.9579 & 0.9313 & 0.0042 & 0.9663 & 0.9658 & 0.0038 \\
    CNN & 0.9479 & 0.9305 & 0.0052 & 0.9683 & 0.9674 &  0.0034 \\
    LSTM & 0.9660 & 0.9553 & 0.0073 & 0.9840 & 0.9833 & 0.0031 \\
    CRNN & 0.9682 & 0.9608 & 0.0051 & 0.9820 & 0.9821 & 0.0024 \\
    Transformer & \textbf{0.9762} & \textbf{0.9738} & \textbf{0.0028} & \textbf{0.9888} & \textbf{0.9892} & \textbf{0.0023} \\
    \bottomrule
  \end{tabular}
\end{table}

Notably, the performance metrics across these datasets demonstrate that our models, particularly the Transformer and CRNN architectures, can handle unseen scenarios effectively. This is evident from their notable performances in LCC, SRCC, and particularly low MSE values in the MUSDB18-HQ dataset. For example, the Transformer model achieves an LCC of 0.9888 and an SRCC of 0.9892 on the unseen MUSDB18-HQ dataset, indicating notable prediction performance and correlation with the ground truth, despite having no prior exposure during training. These findings confirm the capability of our models to adapt and perform robustly across different types of audio content, such as speech and music, which inherently possess distinct acoustic characteristics. In addition, the models show good generalization performance on the unseen data. Furthermore, the effectiveness of the models across both seen and unseen datasets highlights their potential for practical applications in various real-world scenarios


Next, we utilize scatter plots to visually analyze how well our models predicted HAAQI scores across two different music datasets. In Fig. 9, we show scatter plots for the Clarity-Cadenza Challenge dataset, showing the results for each model architecture: (a) CNN, (b) LSTM, (c) CRNN, and (d) Transformer. Fig. 10 presents scatter plots of HAAQI scores on the MUSDB18-HQ dataset, providing insights into the performance of the same models on this unseen music dataset. The plots indicate that all models performed well, with the Transformer architecture particularly excelling in handling both noisy and clean data, as evidenced by its densely clustered points along the diagonal line. These visualizations align with the numerical results in Table II and demonstrate the models' ability to maintain high accuracy and consistency across diverse scenarios, including seen and unseen datasets. 


\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{./Figures/MUSIC_HAAQI_plot.pdf}
\caption{ Clarity-Cadenza Challenge dataset scatter plots for HAAQI scores (a) CNN model architecture (b) LSTM model architecture (c) CRNN model architecture (d) Transformer model architecture}
\label{fig_10}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{./Figures/MUSIC_HAAQI_plot_New.pdf}
\caption{MUSDB18-HQ dataset scatter plots for HAAQI scores (a) CNN model architecture (b) LSTM model architecture (c) CRNN model architecture (d) Transformer model architecture}
\label{fig_11}
\end{figure}



\subsubsection{Waveform Analysis}

To further validate our model's performance, we conducted an in-depth analysis encompassing waveform comparison, spectral analysis, and frequency band energy over time. We compared the unprocessed NAL-R, NAL-R+WDRC, and NeuroAMP amplified signals. Fig. \ref{waveform_fig} presents the waveform comparison, revealing that the Transformer model's output closely aligns with the ground truth (NAL-R+WDRC) signal. The similarity in waveform patterns demonstrates the effectiveness of our model in mimicking the ground-truth waveform, as highlighted in the red boxes.


\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{Figures/waveform_comparison1.pdf}
\caption{Waveform comparison: (a) unprocessed, (b) NAL-R, (c) NAL-R+WDRC, (d) NeuroAMP(Transformer)}
\label{waveform_fig}
\end{figure}

\subsubsection{Spectrogram Analysis}

Fig. \ref{specc} presents the spectral comparison for unprocessed NAL-R, NAL-R+WDRC, and NeuroAMP amplified signals. The spectral analysis aligns the findings from the waveform comparison, indicating that the Transformer model's spectral characteristics closely resemble those of the NAL-R+WDRC amplified signal. This consistency across both waveform and spectral domains highlights NeuroAMP's capability to replicate the performance characteristics of the ground-truth signals.

\begin{figure}[!t]
\centering
\includegraphics[scale=0.33]{Figures/stft_magnitude_analysis1.pdf}
\caption{Spectrogram comparison: (a) unprocessed, (b) NAL-R, (c) NAL-R+WDRC, (d) NeuroAMP(Transformer)}
\label{specc}
\end{figure}


\subsubsection{Frequency Band Analysis}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.24]{Figures/frequency_band_energy_improved.pdf}
\caption{Frequency band energy over time comparison for unprocessed, NAL-R, NAL-R+WDRC, and NeuroAMP(Transformer). (Top panel): low frequency; (Middle panel): mid. frequency; (Bottom panel): high frequency.}
\label{band_energy}
\end{figure}

Fig. \ref{band_energy} illustrates the comparative analysis of frequency band energy over time for unprocessed, NAL-R, NAL-R+WDRC, and NeuroAMP-processed signals. This analysis provides insights into how each processing method affects the time-frequency distribution, focusing on specific frequency bands and their energy distribution.
In the low-frequency band (0-500 Hz), the unprocessed signal shows the lowest energy levels with significant fluctuations over time. The NAL-R processed signal exhibits higher energy levels, indicating effective amplification in this region. The NAL-R+WDRC processing maintains these elevated energy levels while adding dynamic range compression, which is visible as slight variations over time. Notably, the NeuroAMP-processed signal closely mirrors the energy pattern of the NAL-R+WDRC, demonstrating its ability to effectively replicate low-frequency enhancements.

In the mid-frequency band (500-2000 Hz), the unprocessed signal shows the lowest energy levels. The NAL-R processing significantly increases energy in this range, enhancing mid-frequency components crucial for speech intelligibility. While similar to NAL-R in energy levels, the NAL-R+WDRC processing introduces slight variations due to dynamic range compression. The NeuroAMP model closely aligns with the NAL-R+WDRC energy distribution, demonstrating a high degree of accuracy in replicating mid-frequency enhancements.

In the high-frequency band (2000-8000 Hz), the unprocessed signal maintains low energy levels. The NAL-R processed signal shows significant amplification, enhancing high-frequency components crucial for speech intelligibility. As with other bands, the NAL-R+WDRC processing introduces minor variations due to compression effects. The NeuroAMP-processed signal closely mirrors the NAL-R+WDRC pattern, effectively replicating both high-frequency amplification and compression.

Overall, this detailed analysis across specific frequency bands confirms that the NeuroAMP model performs comparably to traditional NAL-R and NAL-R+WDRC processing techniques. By effectively preserving the time-frequency energy distribution, the NeuroAMP model closely mirrors the ground truth provided by the NAL-R+WDRC processed signal.


\begin{figure}[!t]
\centering
\includegraphics[scale=0.24]{./Figures/gain_comparison.pdf}
\caption{Frequency-specific gain comparison of NerouAmp with NAL-R and NAL-R+WDRC.}
\label{gain}
\end{figure}

\subsubsection{Gain Function Analysis}

Additionally, in our comparative study, we analyze gain patterns across different listener audiograms to evaluate the performance of various signal processing methods. Fig. \ref{gain} presents an analysis of audiograms and frequency-specific gain comparisons for two subjects with different hearing loss profiles. Panels (a) and (c) show the audiograms, illustrating the hearing thresholds of the subjects relative to the normal hearing threshold. Panels (b) and (d) display the frequency-specific gain comparisons for the NAL-R, NAL-R+WDRC, and NeuroAMP-processed signals.

In Panel (a), the audiogram displays the hearing threshold (red line) for the first subject, indicating significant hearing loss at higher frequencies. The normal hearing threshold (green dashed line) serves as a reference, highlighting the extent of hearing loss. Panel (c) shows the audiogram for the second subject, who has more pronounced hearing loss, particularly at higher frequencies, compared to the first subject. Panels (b) and (d) compare the gains provided by different processing methods. In Panel (b), the NAL-R processed gain (blue line) shows substantial amplification at lower frequencies with a gradual decrease towards higher frequencies. The NAL-R+WDRC processed gain (red line) closely follows the NAL-R curve but exhibits a more stable gain distribution across frequencies due to compression effects. The NeuroAMP-processed gain (green line with triangles) closely mirrors the NAL-R+WDRC gain, demonstrating NeuroAMP's effective replication of traditional processing methods.

In Panel (d), the NAL-R processed gain (blue line) shows a more pronounced increase, particularly at higher frequencies, to address the greater hearing loss shown in Panel (c). The NAL-R+WDRC processed gain (red line) continues to follow the NAL-R curve, with compression effects providing a stable gain distribution. The NeuroAMP-processed gain (green line with triangles) closely aligns with the NAL-R+WDRC gain pattern, demonstrating NeuroAMP’s effective replication of the gain characteristics. This analysis highlights that the NeuroAMP model successfully replicates the gain features of NAL-R+WDRC processing across both hearing loss profiles. Its ability to offer appropriate amplification and compression tailored to specific hearing loss profiles suggests it as a viable alternative for hearing aid signal processing. By matching the performance of established methods, the NeuroAMP model ensures comparable speech intelligibility and listening comfort for users, similar to traditional methods.

\begin{figure}[!t]
\centering
\includegraphics[scale=0.35]{Figures/subjective_listening_all.pdf}
\caption{Aggregate results showing participant preferences for NeuroAMP, NAL-R+WDRC, or if both types were considered similar, across all sample types.}
\label{subjective1}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.29]{Figures/subjective_listening_category.pdf}
\caption{Detailed breakdown showing preferences for NeuroAMP, NAL-R+WDRC, or if both types were considered similar for each audio sample category.}
\label{subjective2}
\end{figure}


\subsection{Subjective Evaluations}

This section presents a preference test comparing our proposed NeuroAMP model with the traditional NAL-R+WDRC processing. The evaluation involved 20 subjects, evenly split between males and females, all confirmed to have normal hearing through audiometric testing. To simulate hearing loss, we used the MSBG hearing loss simulator~\cite{msbg}, enabling a realistic assessment of processed audio as perceived by individuals with hearing loss. Each subject listened to pairs of audio samples processed by the NAL-R+WDRC and NeuroAMP models, followed by the MSBG hearing loss simulator. They were asked to determine whether sample A was better, sample B was better, or if the samples were similar and indistinguishable. The sample order was randomized, and the tests were conducted in a double-blind manner. We selected a diverse range of samples from our test dataset to ensure a comprehensive evaluation, including 10 clean speech samples, 10 noisy speech samples, 10 speech samples enhanced by the FCN model, 10 speech samples enhanced by the LSTM model, 5 clean music samples, and 5 noisy music samples. Participants evaluated each pair based on intelligibility (clarity of speech) and quality (overall audio quality, including naturalness and pleasantness).

Fig. \ref{subjective1} shows the total number of evaluation results, aggregating preferences and ratings across different sample types. Detailed visual representations of the evaluation results for each sample type—clean speech, noisy speech, enhanced speech by FCN, enhanced speech by LSTM, clean music, and noisy music—are shown in Fig. \ref{subjective2}. From Fig. \ref{subjective1}, we observe that "Similar" responses significantly outnumber those for "NAL-R+WDRC" and "NeuroAMP." Fig. \ref{subjective2} shows a consistent trend: "Similar" decisions prevail across all conditions, including speech, noisy speech, enhanced speech, and music scenarios. These results confirm that the proposed NeuroAMP model effectively simulates the performance of NAL-R+WDRC and demonstrates its capability to deploy a personalized deep learning-based amplification approach.


\subsection{Denoising NeuroAMP Performance Evaluation}
This section evaluates the performance of the Denoising NeuroAMP model, which combines speech enhancement and personalized amplification in a single unified process. The evaluation focuses on the VoiceBank+DEMAND dataset, a widely used benchmark for speech enhancement tasks. We compare Denoising NeuroAMP to two baselines:


\begin{itemize}
\item NAL-R+WDRC: This represents the conventional approach of applying the NAL-R prescription formula and WDRC directly to the noisy speech signal.
\item Denoising\textgreater NAL-R+WDRC: This is a two-stage baseline that first uses a separate LSTM-based speech enhancement model to enhance noisy speech, and then uses NAL-R+WDRC for amplification.
\end{itemize}


We use HASPI and HASQI as evaluation metrics. These metrics are commonly used to assess the intelligibility and quality of processed speech for hearing aid users.
Table VI lists the HASPI and HASQI scores of Denoising NeuroAMP and the two baselines on the noisy test set of the VoiceBank+DEMAND dataset.

\begin{table}[t]
    \centering
    \caption{Performance Comparison of Denoising NeuroAMP on the VCTK Noisy Test Set}
    \label{tab:haspi_hasqi_comparison}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Method & HASPI & HASQI \\
        \midrule
        NAL-R+WDRC (Noisy Input) & 0.89 & 0.55 \\
        Denoising\textgreater NAL-R+WDRC (Two-Stage) & 0.85 & 0.30 \\
        Denoising NeuroAMP (End-to-End) & 0.90 & 0.59 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

The results show that Denoising NeuroAMP outperforms both baselines in both HASPI and HASQI scores. Specifically, Denoising NeuroAMP achieves a HASPI score of 0.90 and a HASQI score of 0.59, while NAL-R+WDRC has scores of 0.89 and 0.55, and the two-stage Denoising\textgreater NAL-R+WDRC baseline has scores of 0.85 and 0.30. The improvements of Denoising NeuroAMP over Denoising\textgreater NAL-R+WDRC (a standard two-stage approach commonly used in hearing aids) are confirmed by a t-test to be statistically significant. These findings suggest that Denoising NeuroAMP, an integrated approach that jointly optimizes speech enhancement and personalized amplification, is more effective than applying these processes separately. The lower performance of the two-stage baseline, particularly with respect to HASQI, highlights the potential drawbacks of a disjointed approach, where errors in the speech enhancement stage may negatively impact the subsequent amplification stage.
Denoising NeuroAMP's superior performance confirms its potential to improve speech intelligibility and quality for hearing aid users in noisy environments. By learning a unified denoising and amplification process tailored to an individual's hearing profile, Denoising NeuroAMP provides an alternative solution to more effective hearing aid technology.

\section{Conclusion}
\label{sec:conclusion}

This research introduces NeuroAMP, a novel deep learning-based system for personalized hearing aid amplification, and Denoising NeuroAMP, an extension that integrates noise reduction for better performance in adverse environments. We investigated four neural network architectures: CNN, LSTM, CRNN, and Transformer, using spectral features and audiograms as inputs. Extensive evaluation using HASPI, HASQI, and HAAQI, together with statistical measures like LCC, SRCC, and MSE, demonstrated the superior performance of the Transformer within the NeuroAMP framework. Notably, NeuroAMP achieved SRCC scores of 0.9810 (HASQI) and 0.9914 (HASPI) on the unseen VCTK dataset, and 0.9892 (HAAQI) on the MUSDB18-HQ dataset, validating the effectiveness of our data augmentation strategy. Next, Subjective listening tests confirmed that both NeuroAMP produce outputs that are perceptually similar to those of traditional methods while offering better adaptability. Furthermore, Denoising NeuroAMP outperformed conventional NAL-R+WDRC and two-stage baselines on the VoiceBank+DEMAND dataset, highlighting its potential for enhanced speech intelligibility in noisy environments. In conclusion, this work demonstrates the potential of NeuroAMP and Denoising NeuroAMP to perform a personalized, user-centric approach to amplification and noise reductions. In our future work, we will focus on refining these models, exploring real-time implementation, incorporating user feedback, and integrating other essential modules, such as a hearing loss model for compensation, to achieve a better personalized hearing assistance solution.


\bibliographystyle{IEEEbib}
\bibliography{ref}


\end{document}
