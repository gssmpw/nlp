\section{Human-Like or Liked-by-Human?}
We used the prompting strategy to bridge the gap between human-written and machine-generated text, aiming to make machine outputs more human-like. However, do humans favor human-like text? 
% The common assumption is that humans expect machines to behave more like humans, but is this hypothesis true? 
Below, we examine human preferences among four options: (1) human-written text, (2) machine-generated text using the original prompt, (3) machine-generated text using the improved prompt, or (4) none of the above. 
%, to examine which type of text is preferred? 


\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.37]{section/images/zh-preference.pdf}
    \caption{Human preferences for three Chinese datasets (five annotators): QA-emo is an emotion-rich question subset of Zhihu-QA with 100 examples.}
    \label{fig:pre-zh}
\end{figure}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[scale=0.50]{section/images/ru-preference.pdf}
%     \includegraphics[scale=0.45]{section/images/ar-preference.pdf}
%     \caption{Human preferences for two Russian (three annotators) and two Arabic datasets (two annotators).}
%     \label{fig:pre-ru-ar}
% \end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.45]{section/images/ru-preference.pdf}
    \includegraphics[scale=0.4]{section/images/ar-preference.pdf}
    \caption{Human preferences for two Russian (three annotators) and two Arabic datasets (two annotators).}
    \label{fig:pre-ru-ar}
\end{figure}


\paragraph{Preference Labeling Setup:}
We labeled the preferences for three languages: Chinese, Russian, and Arabic.
For Chinese, we annotated Zhihu-QA and student essays (300 examples for each), along with 100 responses particularly for Zhihu questions, where emotional and empathetic comforts are highlighted. Five unique annotators participated, identified by \textit{nationality-gender-degree}. For example, \textit{Zh-Male-PhD} refers to a Chinese male, who is a PhD student.  
Similarly, we labeled two datasets for Russian (three annotators) and two datasets for Arabic (two annotators). 


\paragraph{Do People Always Prefer Human Text?}
The answer is \emph{No}.
Analyzing the preferences of ten annotators across six datasets in \figref{fig:pre-zh} and \ref{fig:pre-ru-ar}, human text was preferred in about half of the cases. Notably, for Russian and Arabic, annotators tended to favor machine text. 
This is particularly evident for Russian summaries using the improved prompts (green bars) and Arabic summaries using the original prompts (orange bars).
% This echos the finding in detection that machine-produced summary reaches human-expert quality, closely resembling human-written ones and making differentiation challenging.


For Chinese datasets, including Zhihu QA and student essays, human-written text is generally preferred though there are exceptions.
% For instance, 
\textit{Zh-Male-postdoc} exhibits a unique preference distribution that deviates from the rest.
% \footnote{This difference is not due to inattentive annotation but rather reflects a unique philosophy of the annotator.} %  across various judgments
Interestingly, for emotion-rich questions (QA-emo), where human responses are expected to be more empathetic and be preferred, three out of four annotators actually prefered the machine-generated responses. The remaining annotator disliked both the human and the machine text in 22\% of the cases. The annotator feedback suggested that this was influenced by the presence of mean-spirited responses from Zhihu users, where some human answers expressed personal biases and lacked empathy (see \ref{app:humanpreference}).
% \ref{sec:chinese-distinguishable-signal}).


\paragraph{Why are Human-Written Essays Favored?}
% \textbf{Student Essay:}
% Essays written by humans are more coherent and sincere.
% For model essays, the coherence between sections is poor, presenting hard boundary between independent sections, sometimes repeating the title to make it superficially coherent. 
% Model tends to tell concepts, related concepts. It is hard for models to tell a good story, or connect several stories naturally with the same theme, and then return back to the main topic.
% Therefore, articles by models always lack substances, full of empty concepts and rhetoric, coming across as superficial and ungrounded, overly wordy but fails to deliver any real insights. They feel grandiose but devoid of meaningful content. Model tends to play a role of teacher, rather than a peer. Some article are written like answering questions. % (id 26)

Human-written essays exhibit greater coherence and sincerity. In contrast, machine-generated essays often lack cohesion, displaying abrupt transitions and sometimes repeating the title to create superficial continuity. While LLMs can generate related concepts, they struggle to construct a compelling narrative or to seamlessly connect multiple stories under a unified theme. As a result, their outputs often lack depth, relying on abstract concepts and rhetorical flourishes without delivering substantive insights. The text tends to be verbose yet superficial, giving an impression of grandiosity without meaning. Moreover, LLM-generated essays often adopt an instructive tone, resembling answers to questions rather than peer-level discourse.


% Human: 更具有文学气息，委婉的优美感，氛围感
% 好的文章，或生动可爱俏皮之感，或立意新奇，启发人深思，或首尾巧妙呼应，或真诚质朴，真情实感的动情，或优美凄婉，用词清新准确，情趣意境佳
From a literary perspective, a well-written human article may be lively, charming, and playful, or it may present a novel perspective that inspires deep reflection. It might feature a cleverly structured beginning and ending, convey sincerity and heartfelt emotions, or captivate with its elegance and melancholy. With fresh and precise wording, it creates a rich atmosphere, evoking a refined aesthetic and a sense of poetic depth.  
% Model: 没有值得回味的感觉，一眼就可以看清楚要说什么，不需要太多思考和回味，说教式议论文
In contrast, a piece generated by an LLM lacks this literary nuance, leaving little room for contemplation or lingering thoughts. The expression is immediately clear, requiring no further reflection, resembling more of a lecture than an engaging discourse.


\paragraph{Preference Distributions Vary Across Individuals.}
The annotators for Russian and Arabic exhibited similar preference distributions, whereas large differences occured for Chinese QA.
For example, in the Chinese Zhihu QA, the second annotator selected only seven human-written texts, while the fourth one chose 284 (95\%). In the QA-emo, the first and the second annotators prefered human-written responses, whereas the third and the fourth favored machine-generated text.
This variance shows the charm of collecting personal preferences and then optimizing models to align with individual philosophies. 
Our preference annotations can serve as a valuable resource for investigating the relationship between annotator characteristics (e.g., MBTI personality, gender, and age) and preferences. Also the data can guide models to match individual preferences in multilingual contexts.
% \footnote{We will release all annotator metadata to facilitate future research of the community.}



\paragraph{Human-Like or Liked-by-Human?}
Human texts are not always preferred. This inspires us to reflect the ultimate goal of building LLMs that are human-like vs. liked-by-human.
The goal of being human-like has a single target, i.e.,~mimicking human behavior, while to be liked-by-humans involves optimization towards billions of local optima, each shaped by individual preferences.  
Current language models establish a foundation by learning from human data towards being human-like. 
As they get more advanced, they can further adapt by incorporating personal data, thus transitioning from merely imitating human behavior to aligning with individual user preferences, and moving from human-like to liked-by-human.