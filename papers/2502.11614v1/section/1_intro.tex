\section{Introduction}
% Anticipating the rapid advance of modern computing, Alan Turing famously proposed a test to assess a machine’s ``intelligence'' by determining whether its textual outputs could trick a human evaluator into thinking that it was in fact a fellow human~\citep{chein2024human}.
Recent technological developments have advanced the  generative artificial intelligence (AI) models, such as GPT-*, Gemini, Claude, and Llama~\cite{OpenAI2023GPT4TR, team2023gemini, Claude3, llama3}, thus aggressively blurring the lines between human and AI capabilities.
How often can state-of-the-art (SOTA) LLMs fool human evaluators, i.e.,~passing the Turing test? Can human evaluators correctly predict the origins of encountered content (human vs. machine), thus safeguarding against the potential misuse of LLMs? 

 
Several studies have explored whether humans can distinguish between content generated by a human vs. a machine. Unsurprisingly, the findings depend in part on the quality of the machine generator.
Early studies probing the outputs of less advanced AI models found that human evaluators could indeed detect the difference~\cite{van-der-lee-etal-2019-best}. 
However, in studies using LLMs such as GPT-3.5-turbo, human evaluators are frequently close to random chance~\cite{guo-etal-2023-hc3, jimpei2023creative, liam2023reakfake, chein2024human, wang2024m4gtbench}, particularly for laymen who are not frequent users of LLMs. 
%-based Chatbots like ChatGPT. % Many previous work exposed that it is very challenging for human to distinguish human-written and machine-generated text, ranking at the level of random guess. 

While covering different domains, e.g.,~academic paper abstracts, Wikipedia paragraphs, question-answering responses~\cite{wang-etal-2024-m4, wang2024m4gtbench}, news and review comments~\cite{chein2024human}, most studies focused on GPT-3.5-turbo and English (see \tabref{tab:human-mgt-detection-survey}). Generally, fewer than 300 examples were evaluated by experts (LLM researchers or frequent users) in such studies. 
% as a supplementary analysis of the main content of a paper. Also, in many cases, human annotators are not native speakers of the evaluated languages. 

With the advancement of newer LLMs such as GPT-4o, Claude-3.5-Sonnet, and Llama3.1, we ask whether these findings generalize to them, to other languages, and to native expert annotators? What are the upper bounds of human detection performance across languages, domains and new LLMs? % In addition to the language, domain, and AI generator of a given text, how do the cognitive intelligence, empathy and AI outputs familiarity of human annotators influence evaluation accuracy?

To answer these questions, we perform a comprehensive case study over sixteen datasets spanning nine languages, nine domains, and eleven SOTA LLMs.
% explore human evaluators’ ability to discern content produced by humans from that generated by AI systems. 
We specifically focus on the following four research questions: (\emph{i})~\emph{How well can human annotators distinguish human-written text from SOTA LLMs generations?} (\emph{ii})~\emph{What are the notably detectable linguistic qualities in human- and AI-authored texts that shape decisions about text origin?} (\emph{iii})~\emph{Can a prompting strategy fill the gap between human and machine?} and (\emph{iv})~\emph{Do humans always prefer human-authored text?}


Our human preference ranking dataset can serve as a useful alignment source for multilingual LLM improvement. Our comprehensive analysis over both detection and analysis exposes the major gaps between current LLMs and human expectations, thus suggesting possible directions for future LLM development. Moreover, our human detection evaluation challenges previous findings in terms of random-guess detection capability for humans.
% 
Our contributions are as follows:
\begin{itemize}
    \item Our extensive case study shows that expert annotators achieve detection accuracy of \textbf{87.6\%} on average over $\sim$9K examples, demonstrating that humans can identify LLM outputs and safeguard against LLM misuse. We identified a spectrum of language- and domain-specific distinguishable features between a human and a machine, and we summarize five generalized distinctions across languages, which clearly expose the limitations of current LLM outputs and indicate directions for improvements. 
    % evaluating human detection ability to distinguish human-written text and AI outputs over nine languages, nine domains, 11 generators and 16 datasets. 
    \item Prompting by explicitly explaining the gap patterns between human text and LLM generations in instructions can either fully or partially address issues for over 50\% of the cases. However, cultural nuances, diversity in length, structure, and sentiment remain challenging.
    \item Humans do not always like human-written text: preferences are evenly split between human vs. machine, with a strong tendency to favor machine-generated text (MGT) in Russian and Arabic (for domains such as \emph{summary} and \emph{tweets}).
    
    \item We release 17K original and 32K improved MGTs, labels of 13.5K human detection instances, 5K preferences and 1.6K fill-in-the-gap survey, and metadata for 19 annotators to facilitate future work on human vs. MGT detection, and their relationship with individual characteristics and preferences.  
    % 13508 detection labels and 4968 preference, 1.6K fill-gap survey label
    % 11 generators and nine languages using normal prompts and prompts deliberately to mimic human styles based on human-summarized distinguishable features, and also make our collective human detection labels, and preference labeling data publicly available. 
\end{itemize}


\begin{table*}[t!]
%\tabcolsep3pt
  \centering
  \small
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lll |p{4cm}p{4cm}r |ccc| r}
    \toprule
    \textbf{Study/} & \textbf{MGT Detection} & \textbf{Lang-} & \multicolumn{3}{c|}{\textbf{MGT}} & \multicolumn{3}{c|}{\textbf{Human Annotators}} & \textbf{Avg.Acc/}  \\
    \textbf{Paper} & \textbf{Task Formulation} & \textbf{uage} & \textbf{Generative Models} & \textbf{Domain} & \textbf{\#Sample} & \textbf{Where and Size} & \textbf{Native} & \textbf{Layman} & \textbf{Range} \\
    \midrule 
    \multicolumn{10}{c}{\textbf{\textit{Before OpenAI released GPT-3.5-turbo in November 2022}} } \\
    \citet{garbacea-etal-2019-judge} & 2-class & English & 12 sequence models before 2019  & online product reviews & 3,600 & AMT-900 & -- & \cmark & 66.61\% \\
    \citet{ippolito-etal-2020-automatic} & 2-class & English & GPT2-large & web text & 150 & AMT, uni students & \cmark & \xmark & 71.4\% \\
    \citet{clark-etal-2021-thats} & 2-class & English & GPT2, GPT3 & stories, news articles, recipes & 3,900 & AMT-780 & \cmark & \cmark & 50-58\% \\    
    \midrule
    \multicolumn{10}{c}{\textbf{\textit{After OpenAI GPT-3.5-turbo}} } \\
    \citet{guo-etal-2023-hc3} & 2-class & English & GPT-3.5 & Wiki text, QA & 150 & 8 experts + 9 layman & \xmark & \cmark & 48-90\%\\
    \citet{guo-etal-2023-hc3} & 2-class & Chinese & GPT-3.5 & Wiki text, QA & 210 & 8 experts + 9 layman & \cmark & \cmark & 54-93\%\\
    \citet{chein2024human} & 2-class & English & GPT-3.5-turbo & news & 96 & 203 Prolific & \cmark & \cmark & 57\%\\
    \citet{chein2024human} & 2-class & English & GPT-3.5-turbo & social media comments & 96 & 203 Prolific & \cmark & \cmark & 78\%\\
    \citet{wang-etal-2024-m4} & 2-class & English & GPT-3.5-turbo & Reddit and arxiv abstract & 100 & 6 experts & \xmark & \xmark & 41-94\%\\
     \citet{liam2023reakfake} & boundary detection & English & GPT2 (-XL), CTRL & news, stories, recipes, speeches & 7,257 & RoFT game player & -- & \cmark & 23.4\% \\
    \citet{wang2024m4gtbench} & 4-class & English & davinci-text-003, GPT-3.5-turbo, Cohere, Dolly-v2, BLOOMz, GPT-4 & wikipedia, wikihow, Reddit, arxiv abstract, peer review & 140 & 4 experts & \xmark & \xmark & 10.4-27.4\% \\
    \bottomrule
  \end{tabular}
  }
  \caption{\emph{Experts} refers to individuals who are frequent users of LLMs, \emph{laymen} are people who have never heard of or used LLMs or have used them only rarely, \emph{AMT} is Amazon Mechanical Turk.} % Accuracy is averaging across domains, showing the accuracy range of individual annotators. 
  \label{tab:human-mgt-detection-survey}
\end{table*}





\begin{comment}
Progress updates:
we have finished detection of generation using original prompt and improved prompt
we see the apparent gap between human and machine in some domains, while minimal gaps between human and machine on some domains such as tweets, linkedin posts, improving the prompting can somewhat mitigate some gaps, but not all. just partially. Then we will ask what leads to this gap?
some gaps focus on formatting, some gaps lie in the content, do we expect the model to mimic all human answers? or maybe we like the machine answers? This is two ways to go, train models to be liked by humans, presenting as human preferences, or train models to be like humans, mimicking all human behaviors. Let’s perform a case study to see what humans like.
We plan to perform a case study to select preferred answers, among the 3-4 choices: 1 or 2 texts from humans, 1 generation using the previous prompt, and 1 generation using the improved prompt, over Arabic, Chinese, and Russian.


Paper outlines:
I. Can people differentiate between human and AI materials?
1. for each source, accuracy for human, accuracy for AI, average accuracy, (several annotator, calcuate mean and standard deviation, the variation is also important here)
2. compared with different domains, different languages, which language is easy, which domain is easy?

II. How do human discern human vs. AI text? 
To characterize the specific linguistic properties that distinguish human from AI texts,
Whether these same patterns of linguistic difference generalize to other types of gAI texts, and guide judgements about the origin of these texts, is also considered in the present study.
1. Linguistic differences between human and AI materials
2. human-summarized features, are these consistent with automatic analysis?
3. when the features summarized by human were used to improve the machine text, will it be harder for human to detect?


III. How does human vs. AI discrimination affect human preference?
1. Individual persional ability: given the same language and a same text, what matters?
In addition to individual annotators, other factors such as AI-generated text quality given a language and the AI system.
news vs. wikipedia across different languages: Given the same domain, high-resource language vs. low-resource language
individual phychological traits?

2. evaluate the relationship between detection accuracy and preferences of human or AI text, in which there are two variables: human detection accuracy, and their propensity of preferring human text, regardless of language and LLMs used to generate MGT.
\end{comment}