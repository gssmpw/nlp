\section{Related Work}
\label{sec:relatedwork}
% Todo: collect more MGT human detection papers from 19/20 to 25, and summarize details in the Table below
We first review studies investigating human evaluation of detecting MGT, and then summarize distinguishable features between human-written and machine-generated text, and end up with a discussion of relationship between individual factors, human detection capability and preference.

\subsection{Human Detection on MGT}
\label{sec:human-eval-MGT}
Early studies probing the outputs of less advanced generative models found that human evaluators could detect the differences.
For example, \citet{garbacea-etal-2019-judge} reported that individual evaluators obtained 66.61\% accuracy when evaluating online product reviews generated by 12 sequential models before 2019, and majority voting of five annotators' predictions improved to 72.63\%. \citet{ippolito-etal-2020-automatic} found that trained evaluators were able to detect GPT2-large text 71.4\%.

However, since the emergent of GPT3, it has become challenging for human evaluators to distinguish between human-written and machine-generated text. Evaluators could guess GPT3-generated stories, news articles and recipes with 50\% accuracy~\citep{clark-etal-2021-thats}. 203 crowd-soured workers employed from Prolific show 57\% average accuracy when discerning news generated by GPT-3.5-turbo and news collected from trusted news organizations, e.g., New York Times, Wall Street Journal, and they obtained 78\% on social media comments, both with a wide spread in performance across individuals~\citep{chein2024human}.
\citet{guo-etal-2023-hc3} also showed large differences in detection capability between experts and laymen, where the former is much higher than the later across different domains. Average accuracy of experts is above 90\%, but random guess for amateurs, i.e., 48\% and 54\% for English and Chinese respectively. With expert annotators, academic paper abstracts are harder to detect than the Reddit answers.
Detection accuracy ranges from 41 to 94 for Reddit with an average of 77, while 72 for abstracts with individual performance rangining from 60 to 84, revealing that scientific materials are more challenging even for experts who are researchers and frequent users of LLMs~\cite{wang-etal-2024-m4}.

In addition to binary detection human vs. AI, \citet{wang2024m4gtbench} performed 4-class detection to identify which LLM generated a given text, achieving accuracy of 21.2\%, below the random guess of 25\%, though annotators are all NLP PhD students who are exploring machine-generated text in their research. \citet{liam2023reakfake} asked annotators to select boundary sentences between human- and machine-authored snippets given a mixed collaborative text by a game tool. They found that players correctly identified the boundary sentence 23.4\% of the time (chance being 10\%).
% Different from identifying whether a full text from human or LLMs, or which LLMs, 
% were alightly better than random chance at the boundary detection task,

% literature review guides us to introduce the research question we aim to explore in this paper.
As shown in \tabref{tab:human-mgt-detection-survey}, previous studies primarily focus on English and outputs of GPT-3.5-turbo, non-English languages and more advanced models have been under-investigated. The accuracy of human detection is influenced by languages, generative models, and domains of the given text, as well as the annotators. Large variance is observed among individual evaluators, particularly between experts and laymen.
Therefore, we aim to fill this gap by a comprehensive case study exploring whether human can safeguard against AI over 16 dataset spanning nine languages, nine domains and 11 SOTA LLMs in four evaluation setups.
% Though these results are not directly comparable to ours due to differences in the evaluation setup, data, and participants.
% There are rarely studies investigate human detection capability of identifying Non-English MGT.


\subsection{Distinguishable Signals} % : GPT-3.5 vs. Human
% summarize science paper and the hc3, and our own paper findings and write together
To characterize the specific linguistic properties that distinguish human from AI texts, \citet{markowitz2024linguistic} compared real human hotel reviews to a set of LLM-created fake hotel reviews, and observed that machine texts had a more ``analytical'' style and exhibited increased use of affective language (stronger positive ``emotional tone''). Similar work has shown that AI ``smart replies'' likewise demonstrate an emotional positivity bias~\cite{mieczkowski2021ai}. 
\citet{guo-etal-2023-hc3} summarized that GPT-3.5-turbo writes in an organized manner, with clear logic, tending to offer a long and detailed answer. Also, the model responses generally strictly focused on the given question, whereas humans’ are divergent and easily shift to other topics. Model answers are objective, typically formal, while humans’ are more subjective, colloquial. and emotion-rich.
% shows less bias and harmful information
% refuses to answer the question out of its knowledge
% may fabricate facts.
\citet{wang2024m4gtbench} reported that human-written texts have some imperfect formatting patterns compared to machine text, e.g., initial double newlines, initial space, completely missing new lines in the paragraph, typos, inconsistencies within texts, or specific references and URLs.

In addition to probe the upper bound of human detection ability, we also try to answer: do these linguistic patterns mentioned above generalize to other languages and advanced AI systems? Can they guide more accurate judgment of the text origins?

% Individual Capability, 
\subsection{Detection Accuracy and Preference}
\label{sec:whyexperts}
\textbf{What factors may influence individuals' discerning accuracy?}
\citet{chein2024human} investigated whether psychological attributes (e.g., fluid intelligence, executaive functionining, empathy) or experience (e.g., smartphone and social media habits) predict the ability to differentiate between human and AI materials. Results show that only fluid intelligence --- the capacity to reason and solve novel problems without relying on previously acquired knowledge, was strongly associated with the ability to differentiate human from AI materials. Other three factors were not significantly related to the overall detection accuracy.\footnote{Fluid intelligence involves the ability to think abstractly, analyze patterns, make decisions in unfamiliar situations, and adapt to new challenges. Fluid intelligence is typically thought to be influenced by genetic and biological factors, and it tends to peak in early adulthood before gradually declining with age. It's often contrasted with crystallized intelligence, which is based on accumulated knowledge and experience.}
They also find that increased exposure to mixed AI and human materials online does not enhance one’s ability to differentiate human from AI materials, but rather, may make AI-generated content appear more human-like. 
% This inspires us to analyze the relationship between exposure to AI text with detection accuracy and human preference.


Combine together, language proficiency, cognitive intelligence, and familiarity with AI outputs may affect detection accuracy.

Intuitively, a high level of language proficiency will make it easier to spot overly formal, repetitive, or mechanically structured phrasing typical of AI outputs. Native speakers with strong awareness of cultural nuance can better understand cultural and idiomatic expressions, facilitating identify content that lacks human-like subtlety or creativity.
Strong analytical skills can help detecting inconsistencies, logical flaws, style patterns that indicate MGT.
Individuals with frequent interaction with LLMs may recognize their writing patterns like positive tone bias~\cite{guo-etal-2023-hc3, chein2024human}.
Therefore, to probe the upper bound of human capability of detecting current SOTA LLM generations, all annotators for MGT detection and preference labeling are native speakers for each language, Masters', who are PhD and postdoc students studying LLMs, with 5 female and 14 male.

% a. cognitive abilities: strong analytical skills may be better at detecting inconsistencies, logical flaws, or patterns that indicate AI-generated text; and attention to details, where individuals who pay close attention to grammar, syntax, and style nuances might notice subtle signs of AI output.
% b. language proficiency: A high level of language proficiency may make it easier to spot overly formal, repetitive, or mechanically structured phrasing typical of AI outputs; and cultural nuance awareness, understanding cultural or idiomatic expressions can help identify content that lacks human-like subtlety or creativity.
% c. domain knowledge (subject matter experts are more likely to spot factual errors or lack of depth in AI-generated text) and familiarity with AI output. Individuals with experience interacting with AI models may recognize their writing patterns, such as positive tone bias.
% d. personal biases: A person’s predisposition to trust or distrust technology may influence their discernment; Beliefs about what constitutes human creativity or uniqueness may shape how someone distinguishes human from AI text.
% e. other individual traits: gender, age, race and certificate degree.



% \paragraph{Hypothesis}
% Todo: Research question and then provide our hypothesis of these questions.


% We hypothesized that group-level performance would be only slightly above random chance levels, with relatively poorer performance when participants had less relevant expertise (science stories). We also hypothesized that individuals possessing strong analytical reasoning and executive processing skills (e.g., higher fluid intelligence and executive control) and a greater inclination toward empathy might be able to make more apt judgments. We further considered whether online experience is associated with evaluation accuracy, testing the competing hypotheses that online experience promotes, or diminishes, the ability to discriminate human and AI texts. We further test the hypothesis that individuals who possess stronger evaluation skills may be less likely to share AI-generated materials. Finally, we probe the specific linguistic characteristics that differentiate human-generated from AI-generated content, seeking to determine whether these characteristics might guide judgments of origin.

