\section{Conclusion and Future Work}
We conducted a comprehensive study to investigate the capability of humans to detect text generated by SOTA LLMs. 
Based on 16 datasets spanning 9 languages, 9 domains, and 11 LLMs, 19 expert annotators explored the upper bounds of human detection potential. Accuracy ranged in 50-100\%, with an average of 87.6\%, revealing that it is not that challenging for experts to identify MGTs.

We found that the major gaps between human-written and machine-generated text lie in concreteness, cultural nuances, diversity in length, structure, style, and sentiment, formatting and mixture of languages (with English) in the generations. 
Prompting by explicitly indicating the distinctions could partially bridge this gap, but cultural nuances and diversity remained a challenge.
Yet, humans favored machine-generated text in over half of the cases, particularly when they could not recognize which text was written by a human.
% (less inherent bias leaning towards human text). % (Arabic tweets and Russian summary), .
% since potentially inherent bias toward fellow humans will lead us to choose human text

In future work, we plan to further explore the subtle relationship between human detection accuracy, preference, and individual characteristics: (\emph{i})~Are humans more likely to favor human-written text when they can identify it? and (\emph{ii})~How do an individual's philosophy, personality, and experience influence their ability to discern between human-written and machine-generated text?



% explore (i) whether one’s conclusions about the likely origins (human or AI) of encountered content influence the propensity to choose their preferences; and (ii)
% We thus further annotated human preference: human written, naive MGT, generations using prompts to fill gaps. We find that annotators do not necessarily prefer human text, while prefer MGT over some topics and domains.


% To get an assertive answer of the doubt, 
% For each dataset, we applied two latest state–of-the-art LLMs to generate the corresponding AI outputs, including one multilingual LLM and one target-language-centric LLM. Then we sampled 300 example from each dataset, and 2-5 native expert annotators were asked to identify human-written text in settings of given a single text or a pair of texts. 
% At the same time of detection, annotators also summarized the representative distinguishable signals used in their detection. 

% By summarizing these distinguishable signals, we find the gap between human-written text and machine-generated text mainly focuses on xxx. Some of them result from xxx in pre-training, and some of them stem from SFT, and some of them from alignment (towards human preference).
% This further makes us think about what kind of gaps should be filled and what kind of gaps should kept? In other words, what’s the ultimate target of an LLM, totally human-like without gaps or preferred-by-human with gaps?



\section*{Limitations}
\paragraph{Annotator Bias} 
All annotators involved in the detection process are advanced NLP researchers, including Msc and PhD students, as well as postdocs, specializing in LLMs. No laymen participated in the annotation process, which ensures that the findings and the conclusions were relevant within the expert domain. It is possible that different results could be obtained if laypersons were involved in the detection tasks. However, given our aim to explore the upper bound of human capability in detecting machine-generated text, the current setting is appropriate. Future work could consider involving a broader range of participants to assess the average detection capability across different population groups.


\paragraph{Statistical Significance}
In our study, each dataset includes at least 300 examples for annotation. For some datasets, 3-5 annotators labeled the same set of data, and the average values were calculated. However, a larger sample size would be beneficial to ensure more reliable results. In particular, in the survey exploring whether improved prompts bridge the gap between human-written and machine-generated text, substantial variability between individuals was observed. This subjectivity should be carefully considered in future work. Fortunately, three evaluation methods were used to assess whether prompting could mitigate this gap in our work, partially addressing this limitation by comparing human and automatic detection accuracy before and after the application of the improved prompts.


\paragraph{Linguistic Analysis}
Due to page limitations, we were unable to present a detailed linguistic analysis of the text from human-written, original machine-generated, and newly generated outputs. We will perform analysis of vocabulary features, part-of-speech tagging, dependency structures, sentiment analysis, language model perplexity, and other relevant metrics in future work.


\section*{Ethical Statement}
This section outlines potential ethical considerations related to our work.

\paragraph{Data Collection and Licenses}
A primary ethical consideration is the data license. We reused existing dataset for our research, such as HC3 M4GT-Bench, MAGE, RAID, OUTFOX and LLM-DetectAIve, which have been publicly released under clear licensing agreements. We adhere to the intended usage of these dataset licenses.

\paragraph{Security Implications}
The shared task datasets support the development of robust MGT detection systems, essential for addressing security and ethical concerns. These systems help prevent automated misinformation campaigns, protect against financial fraud, and ensure content integrity in critical domains like journalism, academia, and law. Beyond security, effective detection promotes digital literacy by fostering public awareness of LLM limitations and encouraging critical engagement with online content.

In multilingual contexts, MGT detection is particularly challenging due to linguistic and cultural nuances. Advanced systems must address these complexities in order to prevent disinformation, especially in less-resourced languages. Strengthening multilingual detection enhances the global trust in AI technologies and mitigates the security risks associated with their misuse.

\paragraph{Human Subject Considerations}
Our study engaged human evaluators in distinguishing between human-written and machine-generated texts, while expressing their preference. All annotators gave informed consent, were fully aware of the study's objectives, and had the right to withdraw at any time. Since this study was designed to be evaluated by experts, all annotators participating in it were advanced NLP researchers (MSc and PhD studnets, as well as postdocs, specializing in LLMs). While this ensures a rigorous assessment of the detection capabilities within an expert domain, we acknowledge that findings may not generalize to laypeople. Future research should include a more diverse range of participants to evaluate human detection performance across various demographic and professional backgrounds.

\paragraph{Transparency and Reproducibility}
With the aim to promote open research, we release our collected datasets to the public, including all human annotations and preference rankings, enabling other researchers to build upon our work. We further provide comprehensive methodological documentation to ensure full reproducibility and transparency.