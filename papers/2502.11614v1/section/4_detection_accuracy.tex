\section{Human Detection}
\label{sec:human-detection-acc}
% Previous human detection on English machine-generated text show that it is challenging for humans to discern MGT from human text.
We performed an extensive case study on 9K instances across nine languages to verify how difficult it is for native speakers to detect AI outputs in everyday domains.
\tabref{tab:detection-acc} demonstrates that the average human detection accuracy is 87.6\%.
This reveals that this is not particularly difficult for native human experts, contrary to what previous studies have reported. Below, we zoom into the impact of various factors.

\begin{table*}[t!]
    \centering
    \small
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{llr cll c}
    \toprule
    \textbf{Language} & \textbf{Source/Model} & \textbf{\#Example} & \textbf{\#Annotator} & \textbf{Anno Setup} & \textbf{Shot} & \textbf{Avg. Acc} \\
    \midrule
    \multirow{4}{*}{Arabic} 
    & Dialect Tweet & 900 & 1 & III. Single-binary & Zero & 50.1  \\
    & EASC Summary & 100 & 1 & I. Pair-binary & Zero & 82.0  \\
    & Youm7 News & 1,000 & 1 & I. Pair-binary & Zero & 92.7  \\
    & SANAD News & 100 & 1 & I. Pair-binary & Zero & 100.0  \\
    \midrule
    \multirow{6}{*}{Chinese} 
    % & Zhihu-QA (\gptfouro) & 428 & 5 & I. Pair-binary & Zero & 0.99, 0.99, 1.0, 1.0, 1.0 & 0.996 \\
    & Zhihu-QA (\gptfouro) & 428 & 5 & I. Pair-binary & Zero & 99.6 \\
    & Zhihu-QA (\gptfouro) & 160 & 1 & I. Pair-binary & Few  & 100.0  \\
    % & Zhihu-QA (\qwenturbo) & 588 & 2 & I. Pair-binary & Zero & 0.99, 0.97 & 0.98 \\
    & Zhihu-QA (\qwenturbo) & 588 & 2 & I. Pair-binary & Zero & 98.0 \\
    & Student essay & 102 & 1 & I. Pair-binary & Few  & 98.0  \\
    % & Student essay & 600 & 3 & II. Pair-four-class & Zero & 0.96, 0.96, 0.99 & 0.97 \\
    & Student essay & 600 & 3 & II. Pair-four-class & Zero & 97.0 \\
    & Government Report & 500 & 1 & IV. Triplet-three-class & Few & 97.2  \\
    \midrule
    English & Peersum & 400 & 1 & I. Pair-binary & Few & 99.8   \\
    \midrule
    Hindi & News & 600 & 1 & I. Pair-binary & Few & 85.2   \\
    \midrule
    \multirow{3}{*}{Italian} 
    & DICE News (Anita) & 300 & 1 & I. Pair-binary & Few & 88.0  \\
    & DICE News (Llama3-405B) & 300 & 1 & I. Pair-binary & Few & 99.7  \\
    & DICE News (GPT-4o) & 300 & 1 & I. Pair-binary & Few & 100.0  \\
    % & CItA & \\
    \midrule
    Japanese & News & 300 & 2 & I. Pair-binary & Zero & 86.4  \\
    \midrule
    Kazakh & Wikipedia & 300 & 2 & I. Pair-binary & Zero & 79.7   \\
    \midrule
    \multirow{2}{*}{Russian} 
    & News & 300 & 1 & I. Pair-binary & Few & 100.0   \\
    & Academic summary & 300 & 1 & III. Single-binary & Few & 80.0   \\
    \midrule
    \multirow{2}{*}{Vietnamese} 
    & Wikipedia & 600 & 1 & I. Pair-binary & Zero & 50.7  \\
    & News & 600 & 1 & I. Pair-binary & Zero & 80.3  \\
    \midrule
    \bf Total & -- & 8,778 & 30 & & & 87.6\\
    \bottomrule
    \end{tabular}
   % }
    \caption{Human annotator detection accuracy over 16 datasets and 9 languages: we have a total of 30 annotation settings and 19 unique human annotators. The average accuracy of the human expert guesses is 87.6\%.}
    \label{tab:detection-acc}
\end{table*}


\subsection{Language}
% \paragraph{Language, Domain, Generator}
Human detection accuracy exceeds 87.6\% for Chinese, English, Arabic, Italian and Russian, while it falls below this level for Vietnamese, Kazakh, Hindi, and Japanese. This discrepancy is largely due to the challenge of Wikipedia text.  

\subsection{Domain}
Wikipedia is widely used as training data for LLMs, particularly for low-resource languages, due to the scarcity of alternative datasets. Consequently, models often memorize Wikipedia content, leading to generated text that closely resembles human-written Wikipedia. 
Arabic tweets also present challenges for detection due to their short length and limited context, along with summaries, e.g., for Arabic and Russian summaries, the expert detection accuracy is about 80\%. 
This conversely highlights the ability of language models to generate high-quality human-like text in the domains of Wikipedia, tweets, and summaries. In contrast, substantial differences between machine-generated and human-written text persist in news articles, QA, student essays, and peer reviews, making them much easier to recognize for human experts. 

% \begin{table}[t!]
%     \centering
%     \resizebox{\columnwidth}{!}{
%         \begin{tabular}{lcccc}
%             \toprule
%             \textbf{Dialect} & \textbf{Human} & \textbf{\gptfouro} & \textbf{\qwentwo-7.5B} & \textbf{Overall MGT} \\
%             \midrule
%             EGY & 52.00 & 53.33 & 58.67 & 56.00 \\
%             MOR & 54.00 & 53.33 & 48.00 & 50.67 \\
%             LEV & 69.33 & 14.67 & 58.67 & 36.00 \\
%             GULF & 81.33 & 26.67 & 30.67 & 28.67 \\
%             \bottomrule
%         \end{tabular}
%     }
%     \caption{Arabic dialect tweet human detection accuracy over human vs. \gptfouro vs. \qwentwo-7.5B. Machine-generated text is harder than human text to discern. \gptfouro is harder than \qwentwo.}
%     \label{tab:arabic-dialect_tweet-accuracy}
% \end{table}


\subsection{Generator} 

It is hard to detect MGT across generators and languages.
While there are minimal differences for Chinese (accuracy is 100\% vs. 98\% for \gptfouro vs. \qwenturbo), there are sizable differences for Italian and Arabic.
Based on Italian DICE News, the same annotator detected generations by Anita (an Italian fine-tuned Llama3-8B), Llama3-405B, and \gptfouro, achieving accuracy of 88\%, 99\%, and 100\%, respectively.
Similarly, for Arabic tweets, \gptfouro's outputs are more similar to human text and thus more difficult to detect compared to those by \qwentwo, as shown in \tabref{tab:arabic-dialect_tweet-accuracy}.

\subsection{Annotation Setting}
We conducted the majority of our annotations under setting I. \emph{pair-binary}: given a pair ($hwt$, $mgt$), it asks to identify which of the two texts is human-written. 
We assumed that more complex settings would be more challenging. For instance, II. \emph{pair-four-class} should be harder, as each of the texts could be human- or machine-generated, independently of the other. %, introduces additional options.
Yet, for Chinese student essays, the performance for II does not degrade compared to I.
Similarly, for government reports in IV. \emph{triplet-three-class}, where the annotators have to select the human-written text among three candidates, there was no degradation compared to I.  

However, III. \emph{single-binary} proves to be more difficult than I. \emph{pair-binary} for both Arabic and Russian. While domain differences do exist, e.g.,~tweets vs. summary vs. news in Arabic and news vs. summary for Russian, the substantial performance gap (>20\%) can still be partially attributed to the annotation settings.
Overall, comparing the results for I. vs. III., it is easier to distinguish machine-generated content when given a comparison pair, rather than for single answer. Yet, introducing text triplets or increasing the number of machine-generated or human-written texts had minimal impact on detection performance. 
Moreover, using few shots before detection boosted the confidence of the annotators, resulting in higher accuracy compared to zero-shot.
% \footnote{Slight improvement in our results can be largely owing to expert-level annotators, given that most of them are LLM researchers.}
For datasets with high accuracy, before seeing labeled samples, the annotators found the distinction to be obvious. After seeing a few examples, the annotator was extremely confident in distinguishing human vs. machine text based on indicative features of MGT. 




\subsection{Expert Annotators}
% \paragraph{What factors may influence individuals' discerning accuracy?}
% 1. Individual personal ability: given the same language and a same text, what matters?
For the same language and dataset, individual annotator ability influences detection accuracy but not significantly. 
For instance, in Chinese Zhihu-QA (GPT-4o vs. human), five annotators achieved accuracies of 99\%, 99\%, 100\%, 100\%, and 100\%. Similarly, for Zhihu-QA (Qwen-turbo vs. human), two annotators obtained 99\% and 97\%. In student essays (II. pair-four-class), three annotators recorded accuracies of 96\%, 96\%, and 99\%.
This may also result from the bias that all annotators are native speakers and expert-level LLM practitioners or researchers. Differences between individuals are minor in their cognitive abilities, language proficiency and domain knowledge.

% a. cognitive abilities: strong analytical skills may be better at detecting inconsistencies, logical flaws, or patterns that indicate AI-generated text; and attention to details, where individuals who pay close attention to grammar, syntax, and style nuances might notice subtle signs of AI output.
% b. language proficiency: A high level of language proficiency may make it easier to spot overly formal, repetitive, or mechanically structured phrasing typical of AI outputs; and cultural nuance awareness, understanding cultural or idiomatic expressions can help identify content that lacks human-like subtlety or creativity.
% c. domain knowledge (subject matter experts are more likely to spot factual errors or lack of depth in AI-generated text) and familiarity with AI output. Individuals with experience interacting with AI models may recognize their writing patterns, such as positive tone bias.
% d. personal biases: A personâ€™s predisposition to trust or distrust technology may influence their discernment; Beliefs about what constitutes human creativity or uniqueness may shape how someone distinguishes human from AI text.
% e. other individual traits: gender, age, ...


\subsection{Distinguishable Factors}
\label{sec:dis-factor}
We summarize five remarkable distinguishable signals between machine-generated vs. human-written text across the 16 datasets and the 9 languages; see more details in \appref{sec:distinctionfactor}. % regarding detection setting, accuracy and distinction factors 

\begin{compactitem}
% \begin{itemize}
    \item \textbf{Human text is more informative and concrete.}
    Human-written text contains concrete numbers, specific names of people or institutions, exact places or dates, URLs, and other references, while machine-generated text tends to provide generic information, with little detail to support its statements. 
    
    \item \textbf{Machine-generated text lacks regional, cultural, and religious nuances.}
    For languages such as Arabic, Japanese, Hindi, Kazakh, and Chinese, human texts reflect the cultural and the religious nuances of the language, which is not true for machine-generated text.

    \item \textbf{Human-written text varies substantially in terms of length, structure, style, and sentiment.}
    Human texts show diversity and inconsistency with large deviations in length, structure, style and emotions, while machine-generated texts tend to use a formulaic structure and neutral/positive emotion. This can be partially attributed to LLMs rigorously following instructions, and thus losing on flexibility. 
    

    \item \textbf{Machine-generated text has formatting.}
    MGTs are generally well-segmented with bullet points for better readability, while human-written texts are typically just large block of plain text, which may be due to human text collection and conversion. Moreover, machine-generated texts often use Markdown style, e.g., \texttt{**} and \texttt{\#\#\#}, while human-written texts have typos, grammatical errors, hashtags, and other social media elements.

    \item \textbf{Machine-generated text shows a mixture of other languages.} Non-English language responses often contain some English parts, which is very rare in human text.
% \end{itemize}
\end{compactitem}
