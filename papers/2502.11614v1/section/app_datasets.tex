\section{Datasets}
\label{sec:datasets}
% describe 
% 1. original dataset (license, size, domain, topic); 
% 2. how we did sampling and explain why (size and topic); 
% 3. how we did machine-generation (model, prompt, model generation configuration)

\begin{table*}[ht!]
    \centering
    \small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllr|ccclr}
    \toprule
    \textbf{Language} & \textbf{Source/} & \textbf{Data} &  \textbf{Total} & \multicolumn{5}{c}{\textbf{Sampled Parallel Data}}  \\
                      & \textbf{Domain} & \textbf{License} &\textbf{Human} & \textbf{Human} & \textbf{GPT-4o} & \textbf{Claude} &  \textbf{LLM2 Name (\#)} & \textbf{Total}  \\
                      % \textbf{Vikhr-Nemo-12B} & \textbf{Llama3-405B} & \textbf{ChatGLM4} & \textbf{Qwen2} & \textbf{Qwen-turbo} & 
    \midrule
    \multirow{4}{*}{Arabic} & Dialect Tweet &  Apache 2.0 &  1400 & 300 & 300* & -- & Qwen2 (300*) & 900 \\
    & EASC & cc-by-sa-3.0 & 765 & 153 & 153 & -- & -- & 306 \\
    & Youm7 News & --- & 21,000 & 1,000 & 1,000 & -- & AceGPT (1,000) & 3,000 \\
    & SANAD & cc-by-4.0 & 194,797 & 100 & 100 & -- & -- & 200 \\
    \midrule
    \multirow{4}{*}{Chinese} & Zhihu-QA & cc-by-4.0 & 224,761 & 588 & 588 & -- & Qwen-turbo (588) & 1,764 \\
                             & Student essay & cc-by-4.0 & 93,002 & 600 & -- & 300* & ChatGLM4 (300*) & 1,200 \\
                             & Student essay & cc-by-4.0 & 51 & 51 & -- & 51 &  ChatGLM4 (51) & 153 \\
                             & Government Report & MIT & 200,409& 500 & 500 & -- &  Baichuan2-13B (500) & 1,500 \\
    \midrule
    English & Peersum~\citep{peersum_2023} & cc-by-sa-4.0 &  5,158 & 400 & 200 & 200  & -- &  800\\
    \midrule
    Hindi & News & cc-by-4.0 & 3,995 & 600 & 600 & -- & -- & 1,200 \\
    \midrule
    Italian & DICE & cc-by-sa &  10,518 & 300 & 300 & Anita (300) &  Llama3-405B (300) & 1,200\\
    % & CItA & & 1352 & 300 & & & & 300 & & & \\
    \midrule
    Japanese & News & cc-by-nc-sa-4.0 & 7,110 & 300 & 300 & -- & -- & 600 \\
     \midrule
    Kazakh & Wikipedia & cc-by-sa-4.0 &  4,827 & 300 & 300  & -- & -- & 600\\
    \midrule
    \multirow{2}{*}{Russian} 
    & News & MIT & 800,000 & 300 & 300 & -- & Vikhr-Nemo-12B (300) & 900 \\
    & Academic summary & MIT & 31,000 & 300 & 300 & -- & Vikhr-Nemo-12B (300) & 900 \\
    \midrule
    \multirow{2}{*}{Vietnamese} 
    & Wikipedia & cc-by-sa-3.0 & 600 & 600 & 600 & -- & -- & 1,200 \\
    & News & Kaggle data & 290,282 & 600 & 600 & -- & -- & 1,200 \\
    \midrule
    \bf Total & -- & -- & 1,690,803	& 6,992 & 6,141 & 851 & 3,639 & \textbf{17,623} \\
    \bottomrule
    \end{tabular}
   }
    \caption{Statistics of multilingual data for human annotation. Machine data with * means non-parallel data. }
    \label{tab:multilingual-data}
\end{table*}



% \begin{table*}[t!]
%     \centering
%     \small
%     \tabcolsep2pt
% %    \resizebox{\textwidth}{!}{
%             % \setlength{\tabcolsep}{3pt}
%     \adjustbox{max width=\linewidth}{
%     \begin{tabular}{lllr|ccccccccr}
%     \toprule
%     \textbf{Language} & \textbf{Source/} & \textbf{Data} &  \textbf{Total} & \multicolumn{9}{c}{\textbf{Sampled Parallel Data}}  \\
%                       & \textbf{Domain} & \textbf{License} &\textbf{Human} & \textbf{Human} & \textbf{GPT-4o} & \textbf{Claude} & \textbf{Llama3-8B} & \textbf{Llama3-405B} & \textbf{ChatGLM4} & \textbf{Qwen2} & \textbf{Qwen-turbo} & \textbf{Total}  \\
%     \midrule
%      English & Peersum & cc-by-sa-4.0 &   & 400 & 200 & 200 \\
%      \midrule
%     \multirow{4}{*}{Chinese} & Zhihu-QA & cc-by-4.0 & 224761 & 588 & 588 & &&&&& 588 & 1,764 \\
%                              & Student essay & cc-by-4.0 & 93,002 & 600 &  & 600* & && 600* & & & 1,800 \\
%                              & Student essay & cc-by-4.0 & 51 & 51 & & 51 & && 51 & & & 153 \\
%                          & Government Report & \\
%     % \midrule
%     \midrule
%     \multirow{2}{*}{Russian} & News &  &   &  &  & & &  & & & \\
%     & Academic summaries & &  &  & & & &  & & & \\
%     \midrule
%     Italian & DICE & CC-by-sa &  10,518 & 300 & 300 & & & 300 & & & \\
%     % & CItA & & 1352 & 300 & & & & 300 & & & \\
%     \midrule
%     Kazakh & Wikipedia & cc-by-sa-4.0 &  4827 & 300 & 300 & -- \\
%     \midrule
%     Japanese & News & cc-by-nc-sa-4.0 & 7,110 & 300 & 300 & &&&&&& 600 \\
%     \midrule
%     \multirow{2}{*}{Vietnamese} & Wikipedia &  &   &  &  & & &  & & & \\
%     & News & &  &  & & & &  & & & \\
%     \midrule
%     \multirow{3}{*}{Arabic} & Arabic POS Dialect &  Apache 2.0 &  1400 & 300 & 300 & & & & &300 & &900 \\
%     & ESAC & CC BY-SA 3.0 & 765 & 153 & 153 & & & & & & & 306 \\
%     & KALIMAT& NA & & & & & & & & & & \\
%     & SANAD & CC BY 4.0 & 194,797 & 100 & 100 & -- & -- & -- & -- & -- & -- & 200 \\
%     \midrule
%     Hindi & News & CC BY 4.0 & 3,995 & 600 & 600 & &&&&&& 1200 \\
%     \midrule
%         \bf Total & -- & -- &  \\
%     \bottomrule
%     \end{tabular}
%    }
%     \caption{Statistical information of multilingual data for human evaluation. Machine data with * means non-parallel data. }
%     \label{tab:multilingual-data}
% \end{table*}

\subsection{Arabic}
We collected three categories of text for Arabic including tweets, summaries, and news, involving four datasets.

\paragraph{Arabic Dialect Tweets}
% Mervat
We randomly selected 300 tweets from the QCRI Arabic POS Dialect dataset\footnote{\url{https://huggingface.co/datasets/QCRI/arabic_pos_dialect}}, which includes tweets in four dialects: Egyptian, Moroccan, Gulf, and Levantine. The dataset was originally curated for part of speech segmentation and covers a broad range of topics. The selected tweets were cleaned by removing the usernames of the original tweets' authors and parsing the words from each tweet to organize into sentences.

We generated 600 machine-generated tweets: 150 per dialect using \gptfouro and Qwen-2 (7.5B). Instead of calling APIs, we collected generations by the chat interface in order to esaily observe the patterns of the machine-produced content. The prompt employed for tweet generation was: \textit{write a random tweet with \texttt{}{dialect}.} In this structure, \texttt{dialect} denotes one of the four targeted dialects. Additionally, the corresponding Arabic prompt was also used in generation: 
\begin{RLtext} \texttt{ اكتب تغريدة بال\LR{\{dialect\}}}\end{RLtext}
% \begin{quote}
%     \small
%     \begin{RLtext}
%             \texttt{ اكتب تغريدة بال\LR{\{dialect\}}}.
%     \end{RLtext}
% \end{quote}


% The data was then cleaned, with each sentence written on a new line, with the prompt used and the label in a JSON file.
\paragraph{EASC Articles Summaries}
% Kareem
We used the Essex Arabic Summaries Corpus, a dataset featuring 153 Arabic articles sourced from Wikipedia, Alrai Newspaper, and Alwatan Newspaper. Each article is accompanied by 5 human-written summaries from which we sampled one. The corpus has a diverse range of topics, including art, education, politics, health, science, and finance. To generate machine-produced counterparts, we utilized \gptfouro using the prompt below. The prompt emphasizes that the generated summary should be highly informative, reserving all key ideas while remaining concise and to the point.
\begin{quote}
    \small
    \begin{RLtext}
            \texttt{قم بتلخيص هذا المقال محافظا على اهم النقاط ملتزما الايجاز و الدقة\LR{\{article\}:}}.
    \end{RLtext}

\end{quote}


\paragraph{Youm7 News Articles}
% Saad
The Kalimat dataset consists of articles sourced from the Egyptian news journal \textit{Youm7}. This dataset comprises a total of 21,000 articles across various topics, providing a diverse base for text generation and analysis. For our study, we sampled 1,000 news and generated using GPT-4o (a multilingual LLM) and AceGPT (a Arabic-centric LLM). 

The text generation process was guided by a carefully crafted prompt to simulate the typical writing style of \textit{Youm7} articles. This prompt instructed the models to act as a professional journalist, ensuring coherence and alignment with the original content’s structure and tone. Below is the prompt used for this task:
\begin{quote}
    \small
    \begin{RLtext}
        \texttt{تصرف كأنك كاتب أخبار محترف، واكتب مقالًا باللغة العربية يتألف من حوالي \LR{\{word\_count\_rounded\}} كلمة، بعنوان \LR{\{title\}}. اعتمد في أسلوبك على طريقة كتابة مؤلفي \LR{\{source\}}، مع الأخذ في الاعتبار أن الموضوع يندرج تحت فئة \LR{\{topic\_arabic\}}.}
    \end{RLtext}
\end{quote}

The goal of the generated samples was to maintain narrative consistency and topical relevance while adhering to the typical article patterns found in \textit{Youm7}. 
% This dataset provided a valuable foundation for comparing human- and machine-generated text and evaluating model performance in generating coherent Arabic content.


\paragraph{SANAD News}
% Tarek
\label{sanad_before_prompt_eng}
The SANAD news dataset \cite{EINEA2019104076} is an extensive collection of Arabic news articles in Modern Standard Arabic (MSA), designed to support various Arabic NLP tasks. SANAD dataset comprises over $190,000$ articles from three prominent news websites: AlKhaleej, AlArabiya, and Akhbarona, and it is released under the license of CC-BY-4.0.

For this study, we generated a total of $500$ news articles using titles from the SANAD news dataset, with $100$ articles for each of five selected news category topics present in SANAD spanning finance, politics, sports, medicine, and technology. The articles were generated using the OpenAI's GPT-4o-2024-05-13 and the prompt used to generate these articles is shown below:

\begin{quote}
    \small
    \begin{RLtext}
    \texttt{تصرف كأنك كاتب أخبار محترف ، و اكتب مقالًا باللغة العربية يتألف من حوالي \LR{\{word\_count\_rounded\}} كلمة، بعنوان \LR{\{title\}}. اعتمد في أسلوبك على طريقة كتابة مؤلفي \LR{\{source\}}، مع الأخذ في الاعتبار أن الموضوع يندرج بشكل عام تحت فئة \LR{\{topic\_arabic\}}.}
\end{RLtext}
\end{quote}


In this prompt, we ask the LLM to act as a professional news writer, and write an Arabic news article consisting of around  `\texttt{word\_count\_rounded}' words carrying the title `\texttt{title}', using the writing style of authors of `\texttt{source}' while taking into account that the topic falls under the general umbrella of `\texttt{topic\_arabic}'. 

`\texttt{word\_count\_rounded}' represents the number of words rounded to the nearest hundred of the corresponding human-written article for the same title. `\texttt{source}' is the publisher name of the given title obtained from SANAD. `\texttt{topic\_arabic}' is one of the five topics mentioned earlier selected to match the topic of the given title.


\subsection{Chinese}
We collected Zhihu question answering, high school student essays and government reports.

\paragraph{Zhihu QA}
Based on a Chinese question answering dataset collected from Zhihu, where the users can post and answer questions as well as liking or sharing them, we randomly sampled 588 examples.\footnote{\url{https://huggingface.co/datasets/zirui3/zhihu_qa}} 
They span life, technology, art, science, emotion, law, fashion, humor, and China-specific cultures. 
For emotion-rich questions, we specially collected across 10 topics including breaking-up, quarrel, happiness, marriage issue, divorce, depression, inferiority, parents, forgiveness, future concern (10 questions for each), emphasizing emotional and humorous responses.
Then we collect corresponding responses from a multilingual LLM \gptfouro and a Chinese SOTA model \qwenturbo.
% topic distribution of 488 examples + 200 emotion-rich questions
% life               100
% technology         100
% art                 69
% science             59
% emo                 50
% Chinese-culture     45
% law                 34
% fashion             26
% humor                5

\paragraph{Junior and Senior High School Student Essays}
We sampled 600 Chinese student essays from \citet{song-etal-2020-multi}, attempting to ensure the uniform distribution over the genre of an essay and the grade of the student.\footnote{Each instance includes the essay, and the genre (character, narratives, scenery, objects, argumentative and prose) and the rating (bad, moderate, good, excellent) of the essay, and the grade of the student (7-9 is junior and 10-12 is senior high school).}
However, the dataset does not provide the corresponding problem statements or titles of essays, making it challenging to generate the counterpart by LLMs. To produce the high school student essays by LLMs, we approached by collecting:
(i) essay problem statements of Gaokao from 1977 to 2024, along with some junior high school problems, in total of 376 problems, and (ii) 51 (problem, essay) pairs, with the excellent essays published in the website as the human text.\footnote{\url{https://docs.google.com/spreadsheets/d/1iwtnQkamxoqnThWUT9F007WUHm1XmNESAwSVFM8xNdU/edit?usp=sharing}} 
% 
We sampled 300 problem statements from (i), prioritizing the latest years, and the whole set of (2), and then generate machine counterparts by \claude-3.5-Sonnet and \chatglmfour.



\paragraph{Government Report}
The GovReport dataset, sourced from the MNBVC GovReport subset~\cite{mnbvc}, includes titles and main bodies of reports from various entities, such as schools, corporations, and local government units, without additional labels or classifications. We randomly sampled 500 entries to generate a diverse, high-quality LLM dataset. To achieve varied outputs, we used a set of eight distinct prompts, encompassing continuation tasks based on titles or opening sentences and rephrasing tasks based on the full report content. These prompts, presented in Chinese, were alternated to guide LLMs in generating work reports, with responses collected from GPT-4o, Baichuan2-13B-Chat, and ChatGLM3-6B.



\subsection{English Peer Meta Review}
% Jinyan
We randomly sample 600 reviews (NeurIPS 2021-2022) from PeerSum dataset~\citep{peersum_2023}. 
Peersum crawled both meta reviews and reviews from each reviewer and rebuttal from authors from openreview\footnote{\url{https://openreview.net/}}. 
We leveraged this dataset to generate meta reviews based on comments from other reviewers and compared with human-written meta reviews.
We generate meta reviews using \texttt{\gptfouro-2024-08-06} and \texttt{claude-3.5-sonnet-20240620} respectively, with prompt ``Generate a meta review based on the reviews' opinions and authors' rebuttal to make the final decision on whether the paper should be accepted: \texttt{\{Reviews\}}$\backslash $n$\backslash$n Meta review:'', where ``\texttt{Reviews}'' contains both the reviewers' review and authors rebuttals. 
We sampled 400 machine-generated meta reviews, paired with humans, and asked annotators to discern. 






\subsection{Hindi News}
% : Raj
The BBC Hindi news article dataset comprises a diverse selection of around 4,000 Hindi news articles sourced from the BBC Hindi website, covering a wide range of topics and categories. Each article includes three primary components: headline, main content and the thematic category of the article.
% \begin{itemize}
%     \item \textbf{Headline}: The title of the article.
%     \item \textbf{Content}: The main text of the news article.
%     \item \textbf{Category}: The thematic category of the article.
% \end{itemize}

We sampled 600 articles from the original dataset, focusing on articles that span multiple prominent topics in Indian news to ensure a balanced representation. These articles were selected randomly to avoid any thematic bias and provide a broad scope for comparison.
We generated 600 machine-written samples using GPT-4o model, employing a prompt that guided the model to write concise and formal articles: ``Here is a news headline: '{headline}' and the content: '{content}'. Write a machine-generated version of the news based on this headline''.
% Improved Prompt - Here is a news headline: '{headline}' and its content: '{content}'. Generate a machine-written version of the news based on this headline. Return the news in Hindi, formatted as plain text. Do not include any additional text—just return the generated news content in Hindi.


% This prompt encouraged the model to generate content that aligned with the factual and narrative style typical of professional news articles in Hindi. We configured the model generation settings to ensure clarity and coherence, aiming to produce content that mirrors the structure and tone of the human-written articles in the dataset. % This setup provided a solid foundation for analyzing machine-generated Hindi text in terms of authenticity, language quality, and narrative consistency.


\subsection{Italian DICE News}
% Giovanni
For Italian, we used DICE~\citep{bonisoli2023} --- local news from La Gazzetta di Modena ($\approx 10,000$ samples) licensed as CC-by-nc-sa.
We sampled 300 news, where the original text has at least 1000 characters. 
We applied three large language models: Llama-3.1-405b-instruct, \gptfouro, and Anita (an Italian fine-tuned Llama-3.1-8B: \citet{polignano2024advanced}). 
% \begin{itemize}
% \item Local News: (1) DICE \citep{bonisoli2023}, news from La Gazzetta di Modena ($\approx 10,000$ samples) licensed as CC-by-nc-sa; (2) 300 samples where the original text is at least 1000 characters; (3) GPT-4o, Anita, Llama 3.1 405B Instruct \citep{llama3}.

% \item Student Essays: (1) CiTA \citep{barbagli-etal-2016-cita}, essays from lower secondary school students ($1,352$ samples); (2) 300 random samples; (3) Anita and Llama 3.1 405B Instruct.
% \end{itemize}

% \section{Generation Details}
% \subsection{Italian}
When generating in Italian, we use one of 4 possible system prompts as \tabref{tab:Italian-prompt}.
The model is always prompted as a journalist but the mother tongue and the newspaper scope may vary lightly.
% The prompt used for generation is composed of a system prompt and a user prompt, we use a few variant, but 
The general format is the following:
\begin{quote}
\textbf{System:} You are an Italian journalist writing for a national newspaper focusing on criminal events
happening in the area surrounding Modena. \\
\textbf{User:} Write a piece of news in Italian, that will appear in a local Italian newspaper and that has the following title: ...
\end{quote}


\begin{table*}[ht]
\centering
\begin{tabular}{|p{7cm}|p{7cm}|}
% \toprule
\hline
\multicolumn{2}{|c|}{English-first LLMs} \\
\hline
\multicolumn{2}{|c|}{System Prompt} \\
% \midrule
\hline
    You are an Italian journalist writing for a national newspaper focusing on criminal events happening in the area surrounding Modena & You are an Italian journalist writing for a local newspaper focusing on criminal events happening in the area surrounding Modena \\
    \cline{1-2}
    You are an Italian-French journalist writing in Italian about criminal events happening in the area surrounding Modena & You are an Italian-American journalist writing for a local newspaper focusing on criminal events happening in the area surrounding Modena \\
% \midrule
\hline
\multicolumn{2}{|c|}{User Prompt} \\
\hline
% \midrule
    Write a piece of news in Italian, that will appear in a local Italian newspaper and that has the following title: & Write a piece of news in Italian, that will appear in a local Italian newspaper and that has the following title: \\
    \cline{1-2}
    Write a piece of news in Italian, that will appear in a national Italian newspaper and that has the following title: &     Write a piece of news in Italian, that will appear in a local Italian newspaper and that has the following title: \\
    % \midrule
    \hline
\multicolumn{2}{|c|}{Italian-first LLMs} \\
\hline
\multicolumn{2}{|c|}{System Prompts} \\
    \cline{1-2}
    Sei un giornalista italiano che che scrive per un giornale nazionale focalizzandosi su eventi criminali che accadono a Modena & Sei un giornalista italo-francese che scrive in italiano su eventi criminali che accadono a Modena \\
    \cline{1-2}
    Sei un giornalista italiano che scrive per un giornale locale focalizzandosi su eventi criminali che accadono a Modena & Sei un giornalista italo-americano che scrive per un giornale locale focalizzandosi su eventi criminali che accadono a Modena \\
    \cline{1-2}
\multicolumn{2}{|c|}{User Prompts} \\
    \cline{1-2}
    Scrivi un articolo di giornale in italiano. L'articolo sarà pubblicato su un giornale locale e avrà il seguente titolo: & Scrivi un articolo di giornale in italiano. L'articolo sarà pubblicato su un giornale nazionale e avrà il seguente titolo: \\
    \cline{1-2}
    Scrivi un articolo di giornale in italiano. L'articolo sarà pubblicato su un giornale locale e avrà il seguente titolo: & Scrivi un articolo di giornale in italiano. L'articolo sarà pubblicato su un giornale locale e avrà il seguente titolo:\\
% \bottomrule
\hline
\end{tabular}
\caption{Italian machine generation prompts.}
\label{tab:Italian-prompt}
\end{table*}


\subsection{Japanese News}
% Masahiro and Ryuto
We randomly sample 300 news articles of the BBC news from the XLSUM dataset \citep{hasan-etal-2021-xl}.
XLSUM has title, content, and summary of a news article.
We generate the corresponding news content using \texttt{\gptfouro-2024-08-06} based on the article titles.
% The content of the news articles in XLSUM is human-written text, 
For both text generated with \gptfouro and the human-written text, we remove obvious formatting indicators (e.g., line breaks and template messages at the beginning and end of the texts).
% For output text length, it sets the max tokens parameter of LLMs to the number of tokens in each human-written text.
% We calculated the number of tokens using the \texttt{tiktoken} library\footnote{https://pypi.org/project/tiktoken/0.1.1/}.

We generate 75 articles for each of the four settings: simple prompt zero-shot, diverse expression zero-shot, content-rich zero-shot, and few-shot.
Simple prompt zero-shot, diverse expression zero-shot, and content-rich zero-shot use the following instructions, respectively: ``\CJK{UTF8}{min}{次のニュースタイトルに合わせたニュース記事を生成してください。}'' (Please generate a news article that matches the following news title.), ``\CJK{UTF8}{min}{次のニュースタイトルに合わせたニュース記事を多様な表現を使用して生成してください。}'' (Please generate a news article that matches the following news title, using diverse expressions.), and ``\CJK{UTF8}{min}{次のニュースタイトルに合わせたニュース記事を生成してください。このとき生成するニュース記事には、誰かに対するインタビューや実際の出来事を組み込んでください。}'' (Please generate a news article that matches the following news title. When creating the article, include interviews with individuals or actual events.).
The few-shot uses the same instruction as the simple prompt zero-shot.
Add ``\CJK{UTF8}{min}{ニュースタイトル:}'' (news title:) and ``\CJK{UTF8}{min}{ニュース記事:}'' (news article:) at the beginning of the news title and the content, respectively.
We randomly sample three contents as examples for few-shot, and use the same examples for all generations.


\subsection{Kazakh Wikipedia}
KazQad is a closed question-answering dataset focused on the Kazakh language~\citep{kazqad}. It contains 5,000 distinct passages covering 1,700 topics derived from Kazakh Wikipedia. These passages span a variety of domains, including art, science, history, sports, and other general topics.
 
We randomly selected 300 titles along with their corresponding paragraphs, ensuring that each paragraph contained at least 3-5 sentences. Additionally, the sampled texts were cleaned and merged by title to increase the length of the texts. This step was necessary because some samples in the original dataset contained extraneous elements such as references, markdown formatting symbols, and other unnecessary characters.
For the machine-generated data, we used the GPT-4o model to generate passages based on the sampled titles. The generation process was initiated with the following prompt: ``Please, write one paragraph about the following topic in Kazakh: [title].''





\subsection{Russian}
We generated machine text for both news and summaries for Russian.

\paragraph{News}
Based on Lenta.ru news from Corus with around 800,000 news samples, we sampled 50 cases for each topic among the six topics: Russia, World, Economy, Sport, Culture, and Science \& Technology. We prompted GPT-4o and Vikhr-Nemo-12B-Instruct-R-21-09-24 to generate to corresponding machine text using \foreignlanguage{russian}{''Напиши новость в области "{topic}" с сайта lenta.ru используя заголовок {title}. Ты должен генерировать новость без заголовка. Новость: ''}

\paragraph{Academic Article Summaries} 
Based on ai-forever/ru-scibench-grnti-clustering-p2p with 31 000 samples, we sampled 30 summaries for each topic among 'Psychology', 'Mechanical Engineering', 'Agriculture and Forestry', 'Geology', 'Biology', and 'Energy', and generated machine counterparts using GPT-4o and Vikhr-Nemo-12B-Instruct-R-21-09-24.  
We used the following prompt: \foreignlanguage{russian}{''Напиши краткое содержание для статьи в области "{topic}" используя заголовок {title}. Ты должен генерировать краткое содержание без заголовка. Содержание:''}



\subsection{Vietnamese}
\selectlanguage{Vietnamese}
\paragraph{Vietnamese News} 
The dataset is crawled from Lao Động newspaper\footnote{\url{https://www.kaggle.com/datasets/phamtheds/news-dataset-vietnameses}} before May 2022 (before the releases of all LLM that support Vietnamese). The dataset contains 290,282 articles with a headlines and a summary of an article in various topics, such as politics, lifestyles, legal, etc. We cleaned the data by removing some missing values and too short summaries (less than 20 words), then randomly selected 600 examples to generate the corresponding 600 machine-generated summaries with GPT-4o using title of articles. The prompt for generation process was
\begin{quote}
    \textit{Bạn là một nhà báo Việt Nam chuyên viết những mẩu tin tóm gọn cho các bài báo bằng cách sử dụng tiêu đề của chúng. Hãy viết cho tôi một đoạn tóm tắt bài báo tiêu đề dưới đây:}
\end{quote}
which means \textit{You are a Vietnamese journalist who writes summaries for articles using their headlines. Please write me a summary of the article with the following headline:} 


\paragraph{Vietnamese Wikipedia} 
We randomly crawled 600 sites from Vietnamese Wikipedia\footnote{\url{https://vi.wikipedia.org/}} with its ID, title, and the introduction part of each topic. For our studies, we generated 600 introduction given a subject using GPT-4o with the prompt:
\begin{quote}
    \textit{Bạn là một nhà đóng góp cho Wikipedia tiếng Việt. Hãy viết cho tôi một đoạn giới thiệu ngắn gọn bằng tiếng Việt về chủ thể bên dưới để đăng trên trang Wikipedia. Lưu ý bắt buộc viết với khoảng \texttt{word\_count} từ.}
\end{quote}
which means \textit{You are a contributor to Vietnamese Wikipedia. Please write me a brief introduction in Vietnamese about the subject below to post on the Wikipedia page. Note that it must be written within \texttt{word\_count} words:}

In this prompt, to avoid the newly generated text has too long passage, since GPT-4o tends to write much longer than human does for a Wikipedia topic, we set the word limits \texttt{word\_count} as word count of the original one.
\selectlanguage{English}

