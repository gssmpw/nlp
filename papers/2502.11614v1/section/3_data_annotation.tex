\section{Case Study}
\label{sec:data-annotation}

Previous studies presented the difficulty humans face in distinguishing SOTA LLM-generated content from human-written text, often resulting in a random guess (see more in \appref{sec:relatedwork}). However, most evaluations focused on English and generations by GPT-3.5-turbo, leaving the detectability of MGT in other languages and LLMs uncertain.

To verify whether this observation can be generalized to other languages and more advanced LLMs, we collected LLM generations based on 16 datasets across nine domains and nine languages. 19 native speakers who are either LLM researchers or practitioners performed human evaluations, investigating (\emph{i})~whether humans can correctly discern human vs. AI outputs, and (\emph{ii})~whether humans prefer fellow human answers or LLM responses.


% \begin{table*}[t!]
%     \centering
%     \small
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lllr|ccccccccr}
%     \toprule
%     \textbf{Language} & \textbf{Source/} & \textbf{Data} &  \textbf{Total} & \multicolumn{9}{c}{\textbf{Sampled Parallel Data}}  \\
%                       & \textbf{Domain} & \textbf{License} &\textbf{Human} & \textbf{Human} & \textbf{GPT-4o} & \textbf{Claude} & \textbf{Vikhr-Nemo-12B} & \textbf{Llama3-405B} & \textbf{ChatGLM4} & \textbf{Qwen2} & \textbf{Qwen-turbo} & \textbf{Total}  \\
%     \midrule
%     \multirow{4}{*}{Arabic} & Dialect Tweet &  Apache 2.0 &  1400 & 300 & 300* & & & & &300* & &900 \\
%     & ESAC & cc-by-sa-3.0 & 765 & 153 & 153 & & & & & & & 306 \\
%     & Youm7 News & --- & 21,000 & 1,000 & 1,000 & & & & & & & 3,000 \\
%     & SANAD & cc-by-4.0 & 194,797 & 100 & 100 & -- & -- & -- & -- & -- & -- & 200 \\
%     \midrule
%     \multirow{4}{*}{Chinese} & Zhihu-QA & cc-by-4.0 & 224761 & 588 & 588 & &&&&& 588 & 1,764 \\
%                              & Student essay & cc-by-4.0 & 93,002 & 600 &  & 300* & && 300* & & & 1,200 \\
%                              & Student essay & cc-by-4.0 & 51 & 51 & & 51 & && 51 & & & 153 \\
%                              & Government Report & \\
%     \midrule
%     English & Peersum~\citep{peersum_2023} & cc-by-sa-4.0 &  5158 & 400 & 200 & 200  & &&&& & 800\\
%     \midrule
%     Hindi & News & cc-by-4.0 & 3,995 & 600 & 600 & &&&&&& 1200 \\
%     \midrule
%     Italian & DICE & cc-by-sa &  10,518 & 300 & 300 & & & 300 & & & & 900\\
%     % & CItA & & 1352 & 300 & & & & 300 & & & \\
%     \midrule
%     Japanese & News & cc-by-nc-sa-4.0 & 7,110 & 300 & 300 & &&&&&& 600 \\
%      \midrule
%     Kazakh & Wikipedia & cc-by-sa-4.0 &  4,827 & 300 & 300  & &&&&&& 600\\
%     \midrule
%     \multirow{2}{*}{Russian} 
%     & News & MIT & 800,000 & 300 & 300 & & 300  &  & & & & 900 \\
%     & Academic summary & MIT & 31,000 & 300 & 300 & & 300 &  & & & & 900 \\
%     \midrule
%     \multirow{2}{*}{Vietnamese} 
%     & Wikipedia & --- & 600 & 600 & 600 & & &  & & & & 1,200 \\
%     & News & --- & 290,282 & 600 & 600 & & &  & & & & 1,200 \\
%     \midrule
%     \bf Total & -- & -- &  \\
%     \bottomrule
%     \end{tabular}
%    }
%     \caption{Statistics of multilingual data for human annotation. Machine data with * means non-parallel data. }
%     \label{tab:multilingual-data}
% \end{table*}



\subsection{Task and Dataset}
% 1. original dataset (license, size, domain, topic); 
% 2. how we did sampling and explain why (size and topic); 
% 3. how we did machine-generation (model, prompt, model generation configuration)

\paragraph{MGT detection} The goal is to identify whether the text was written by a human or generated by models given a single text, or to recognize which text is written by a human given a pair of texts: one human-written and one machine-generated.

In data collection, we focused on datasets from common domains such as community QA, news, tweets, and government reports, alongside domains requiring high-integrity LLM applications, including educational and academic contexts, such as accurate knowledge verification in Wikipedia-style texts, and identifying the authorship of student essays and peer reviews. 
For a given language and given a dataset, we sampled 300-600 human texts and then generated corresponding machine text using two SOTA LLMs: a multilingual model (e.g., from the GPT or the Claude series) and a language-specific model (e.g., ChatGLM or Qwen for Chinese and AceGPT for Arabic), to analyze the impact of different LLMs on detection performance, particularly for non-English languages.
% 
As shown in \tabref{tab:multilingual-data}, we collected data based on 16 datasets across nine languages. The generation prompts and collection details are shown in \appref{sec:datasets}. % Note that we generated more data and used the subset for human annotation.

% Yuxia Steps:
% 1. check original dataset section, organize into a appendix section
% 2. list what are empty and ask them to fill
% 3. check where is the collected data, and where is the evaluation results


\subsection{Human Detection Setups}
\label{sec:anno-setup}

\paragraph{Annotation Settings}
To mimic real-world machine-generated text detection scenarios, we set up four human evaluation settings.
Given human-written text and machine-generated text, representing by $hwt$ and $mgt$ respectively, human annotators are asked to identify which text was written by a human. Note that $mgt$ can be generated by multiple different LLMs, referring to as $mgt_i$, where $i \in [1,2, \cdots, n]$.

As shown in \tabref{tab:detection-settings}, according to the input and the output options, we categorize detection settings as I. \emph{pair-binary}, II. \emph{pair-four-class}, III. \emph{single-binary}, and IV. \emph{triplet-three-class}. 
For single text input, either $hwt$ or $mgt$, the goal is to recognize whether the text was written by a human, by answering just Yes or No. This is suitable for the scenario where for the human text there is no necessarily a corresponding machine-generated text, and thus they can be collected from different sources and for different topics, such as Arabic tweets.
Given a pair of texts (text1, text2), a binary output is easier than the four-class detection. The \emph{pair-binary} setting asks that either text1 or text2 is $hwt$, and the other one is $mgt$, while the \emph{pair-four-class} setting has no restrictions: each of text1 and text2 can be $hwt$ or $mgt$, regardless of the label of the other text. 
Sometimes, we want to compare human text to generations from different LLMs, in which case, we apply IV, which we limit to a three-class detection: human vs. LLM$_1$ vs. LLM$_2$.

Overall, I and IV are suitable for scenarios where there is a human text and its corresponding machine-generated text. II and III can be used if there are non-corresponding human-written and machine-generated texts.
% Settings I and III are commonly used in other studies, as well as this work.
If the annotators have seen some human-written and some machine-generated text before detection, we refer to this as a few-shot setting; otherwise, we have zero-shot.


\begin{table*}[t!]
\centering
%\tabcolsep3pt
\resizebox{\textwidth}{!}{\small
\begin{tabular}{l p{4cm} p{8cm} p{4cm} p{4cm}}
\toprule
\textbf{Setting ID} & \textbf{Input} & \textbf{Task Description} & \textbf{Output Options} & \textbf{Applicable Scenarios} \\
    \midrule 
    \multirow{3}{3cm}{\textbf{I. Pair-Binary}} & ($hwt$, $mgt$) \textbf{or} ($mgt$, $hwt$) & Given a pair of (text1, text2), identify which one is human-written? Either text1 or text2 must be $hwt$, and another is $mgt$ randomly sampled from $mgt_i$. & \textbf{A.} text1; \textbf{B.} text2 & parallel data is available. \\
    \midrule
    \multirow{2}{3cm}{\textbf{II. Pair-Four-Class}} & ($hwt$, $mgt$) \textbf{or} ($mgt$, $hwt$) \textbf{or} ($hwt$, $hwt$) \textbf{or} ($mgt$, $mgt$) & Given a pair of (text1, text2), identify which one is human-written? Both text1 and text2 can be $hwt$, and can be $mgt$. & \textbf{A.} text1; \textbf{B.} text2; \textbf{C.} none of them; \textbf{D.} both & parallel data is not necessary. \\
    \midrule
    \multirow{1}{3cm}{\textbf{III. Single-Binary}} & $hwt$ or $mgt$ & Given a piece of text, identify whether it is written by human? & \textbf{A.} Yes, human; \textbf{B.} No, machine & parallel data is not necessary. \\
    \midrule
    \multirow{3}{3.5cm}{\textbf{IV. Triplet-Three-Class}} & ($hwt$, $mgt_1$, $mgt_2$)  & Given a set of texts (text1, text2, text3), identify which one is human-written? One of the text1, text2 and text3 must be $hwt$, and others are $mgt$ randomly sampled from $mgt_i$. & \textbf{A.} text1; \textbf{B.} text2; \textbf{C.} text3 & parallel human and multiple LLM generations are collected to make comparisons. \\
    \bottomrule
\end{tabular}
}
\caption{The four human detection settings: the setting name refers to input-output options, pair/binary means the input is a pair of texts and the goal is to predict binary labels, and whether the text is human-written (Yes or No).}
% I and IV are suitable for the scenarios where there is a human text and its corresponding machine text (parallel data is available). II and III can be used if parallel data is not available, there are only separate human text and machine text.
\label{tab:detection-settings}
\end{table*}



\paragraph{Annotation Tool}
% Rui: annotation pipeline we used and why we used these two: quality and efficiency
To mitigate potential labeling biases arising from raw spreadsheet annotation and to enhance efficiency, we implemented two methods with optimized interfaces and workflows for our annotation: (1)~a custom pipeline using the Google Workspace suite, including Apps Script, Google Sheets, and Google Forms. The core idea was to store all data in Google Sheets, use Apps Script to extract data and generate a survey in Google Forms; and (2)~Label Studio, an open-source multi-type data labeling and annotation tool with a standardized output format. We designed a custom template for our annotation task and collected results using this platform. The annotators were given the choice to use their preferred tool. 

% \footnote{https://developers.google.com/apps-script}, 
% \footnote{https://github.com/HumanSignal/label-studio/}  


\paragraph{Annotator Background}
In order to explore the upper bound of human detection capability, instead of using crowd-sourcing annotators, we conducted in-house labeling. 
The annotators were BSc, MSc, and PhD students, as well as postdocs, who were familiar with NLP tasks and LLM generations. All annotators independently completed their individual annotations. For each language, the annotators were all native speakers of that language. See more detail in \appref{sec:whyexperts}.

% Name, Age, Gender, degree, mother language, personality test type, 