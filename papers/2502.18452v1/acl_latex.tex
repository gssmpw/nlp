% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{csquotes}

\usepackage{todonotes}
\newcommand{\ff}[1]{\todo[color=purple!40]{FF:[#1]}}
\newcommand{\ffinline}[1]{\todo[inline,color=purple!40]{FF:[#1]}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response}

\author{%
\textbf{Mollie Shichman$^1$,
Claire Bonial$^2$,
Austin Blodgett$^2$,
Taylor Hudson$^3$,} \\
\textbf{Francis Ferraro$^4$, Rachel Rudinger$^1$}
\\
$^1$ University of Maryland, College Park, 
$^2$ Army Research Lab \\
$^3$ Oak Ridge Associated Universities,
$^4$ University of Maryland, Baltimore County \\
\texttt{mshich@umd.edu},
\texttt{claire.n.bonial.civ@army.mil},\\
\texttt{ferraro@umbc.edu}, \texttt{rudinger@umd.edu}
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
% LLMs hold potential to be the basis of communication between human experts and robots in disaster scenarios. However, practical hardware constraints and the limited availability of relevant data mean we need a method to up-scale a smaller LLM to take on this task. To solve these problems, we introduce a pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models, where disaster experts and linguists combine their knowledge to make high quality seed data that is used to generate synthetic data for fine-tuning. We create a set of 130 seed instructions and 112 evaluation instructions to generate and evaluate a FRIDA model trained on 25,000 synthetic instructions. We find that FRIDA models outperform their base models of LLaMa Instruct 1B, 3B, and 8B parameters when measuring by both exact match and embedding vector distance. We then run an ablation study to understand which kinds of synthetic data most affect performance and find that generic physical common sense knowledge causes the greatest improvement in performance. We conclude that the FRIDA pipeline is a blueprint for affordable and effective domain-specific LLMs that will pave the way for effective human-robot partnerships.
Large Language Models (LLMs) have the potential for substantial common sense reasoning. % about physical objects. 
However, these capabilities %behaviors 
are often emergent in larger models. This means smaller models that can be run locally are less helpful and capable with respect to certain reasoning tasks. 
%To solve these problems, 
To meet our problem space requirements, we fine-tune smaller LLMs to disaster domains, as these domains involve complex and low-frequency physical common sense knowledge. We introduce a pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models, where domain experts and linguists combine their knowledge to make high-quality seed data that is used to generate synthetic data for fine-tuning.  We create a set of 130 seed instructions for synthetic generation, a synthetic dataset of 25000 instructions, and 119 evaluation instructions relating to both general and earthquake-specific object affordances. We fine-tune several LLaMa and Mistral instruction-tuned models and find that \textbf{FRIDA models outperform their base models at a variety of sizes}. We then run an ablation study to understand which kinds of synthetic data most affect performance and find that training physical state and object function common sense knowledge alone improves over FRIDA models trained on all data. We conclude that the FRIDA pipeline is
% ideal\ff{``ideal'' is too strong} 
capable of instilling general common sense, but needs to be augmented with information retrieval for specific domain knowledge.
\end{abstract}

\section{Introduction}
% goal to make LLM reasoning capabilities more adaptable, available, accessible to more people by getting them to run locally on edge devices

%need to explain why disasters are an interesting avenue for examining the above problem. It's an interesting domain to try to improve because there's not a lot of pre-existing data, making it a novel and important problem. 

% It's also a case where both general and specific common sense reasoning is required, particularly when it comes to object affordances.

% We propose FRIDA as a method to create smaller LLMs with stronger levels of physical common sense, as well as a method to examin what kinds of common sense are most easily imbued with synthetic data during fine-tuning. Split this up into 2 clear points

% My model does better than the base model. My data makes it so I can figure out what data best improves performance in my domain. 
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{images/frida_ex.png.png}
    \caption{An example of how a FRIDA-tuned LLM outperforms its base model on questions combining an object's affordances and physical characteristics.}
    \label{fig:frida_ex}
\end{figure}
\textit{Which of the following would be most dangerous if it collapsed?} This question is fairly trivial for humans to answer, but requires several types of semantic knowledge. First, one must know what objects are capable of collapsing. One must also know the general size of these items and the item's other functions to assess the danger. A collapse is also a change of state that fundamentally shifts the use of these objects; a collapsed chair is more likely to cut, scrape, or be carried if the chair folds. All of this knowledge is needed to answer this question, and all of it is embedded in our semantic understanding of things that can cause danger and things that can collapse. 

As LLMs have improved exponentially, their abilities at reasoning about objects have improved as well. LLMs have long proven to encode physical world knowledge \cite{knowledgebase}, and their embeddings can improve physical understanding of an environment and its objects both within and beyond a fine-tuned domain \cite{cohen2024surveyroboticlanguagegrounding}. However, much of this improvement only emerges %is from emergent capabilities of 
in larger models trained on more data \cite{wei2022emergentabilitieslargelanguage}. This makes these essential semantic capabilities less accessible, particularly for running LLMs locally on edge devices. We thus wanted to answer: \textbf{how can we imbue all the physical common sense and semantics needed for smaller models to be more capable at understanding the physical world?}

To answer this question, we turned to improving small LLMs in the disaster relief domain. The first reason for this was because disasters require a lot of both general and domain-specific semantic knowledge: one needs to know both the standard objects in daily life and the specialized tools required for search and rescue, such as hydraulic pumps and concrete saws. Secondly, the specific knowledge (and to a lesser extent, the general knowledge) needed for reasoning varies by disaster, which requires a high level of adaptability in any potential pipeline. Finally, most publicly available data on disasters is social media-based reactions \cite{LLMdisaster}, which does not pertain much to our subdomain of the objects used during these events. This is due to the writer's assumption of latent semantic knowledge (e.g., everyone knows that counters don't collapse, so no one makes a point to tweet that) \cite{reportingbias}. This makes the task more novel for LLMs which likely have not seen explicit reasoning in this domain during pre-training. 

We present a pipeline to create Field Research and Instruction Decoding Agent (FRIDA) models. For FRIDA, we leveraged both disaster and linguistic expertise to create gold-standard instructions that in turn are used as a basis for synthetic data generation, as seen in Figure \ref{fig:pipeline}. This synthetic data is then used to fine-tune smaller models to increase Like its rescue dog eponym,\footnote{\url{https://en.wikipedia.org/wiki/Frida_(dog)}} our FRIDA models were initially developed and tested for earthquake disaster relief, based on expert knowledge pertaining to the February 6th, 2023 earthquakes in Turkiye and Syria \cite{reuters}. 

We found our FRIDA models out-performed their base models overall in both exact match and embedding vector similarity \cite{semscore}, regardless of model size or architecture. Knowing that our pipeline improved performance, we wanted to investigate which synthetic data were most influential in that improvement. Do accomplish this, we ran an ablation study where we fine-tuned a variety of small LLMs on subsets of our synthetic data corresponding to a specific type of object-based common sense. We call these resulting models the ablation FRIDA (aFRIDA) models. We found that aFRIDA models trained on general common sense significantly outperformed models trained on the domain-specific common sense knowledge and the full FRIDA models themselves. We posit that FRIDA succeeds in improving object-related general common sense and is a strong basis for improving domain-specific general common sense as well.
%\ff{this last sentence undercuts everything before it. Rephrase it.}

% \ff{These contributions are probably too fine-grained.}
Our contributions are as follows:
\begin{enumerate}
    \item An expert-in-the-loop pipeline (Figure~\ref{fig:pipeline}) %based on \citet{ppdc} 
    for generating specific and high-quality synthetic data that can be used for fine-tuning when man-made data are not feasible to obtain, as well as the resulting gold-standard datasets
    \item A synthetic dataset of 25,000 instructions relating to common sense and earthquake with accompanying analysis
    \item The FRIDA 1B, 3B, Minstal 8B, and LLaMa 8B models, trained on the above synthetic data.
    \item A series of ablation FRIDA (aFRIDA) models trained on subsets of the synthetic dataset
\end{enumerate}
An anonymous github containing code and a complete example of the FRIDA pipeline is currently available.\footnote{\url{https://anonymous.4open.science/r/FRIDA-0215-D02A/}}

% In perilous situations such as search and rescue missions, humans need to be able to communicate effectively with robots to complete the task at hand. Instructing robots in natural language is essential for this goal, because it lowers the barriers of usage and allows rescue workers and robots to work as true partners on the mission. In order for the relief worker to administer these instructions, the robot must have a firm grasp of both physical common sense and the particulars of the disaster. For instance, in order to execute the instruction \textit{``Find the entrance and open the door if there's nothing dangerous overhead,''} the robot needs to understand the abstract concept of an entrance and the states in which a door can exist; its capabilities to open doors and what that action entails; and what constitutes a overhead danger for itself and its human handlers. 

% Large Language Models (LLMs) are prized for their ability to encode world knowledge \cite{knowledgebase} and are increasingly used as sources of all types of information by the mainstream \cite{gemini}. Particularly enticing is research showing that using LLM embeddings to formulate robot instructions improves general and out of domain performance \cite{cohen2024surveyroboticlanguagegrounding}. However, there are several problems with these models that make their implementation challenging. Practically, they are simply too large to make logistical and financial sense to implement on a robot. Assuming a robot operates with one commercially available GPU with 24 GB of RAM, it is limited to running a 40 Billion parameter model \cite{qLORA}. Since internet service is not a given in the aftermath of a disaster, we also cannot rely on LLMs in the clod. Even with bit-quantization \cite{lora} and parameter-efficient fine tuning \cite{peft}, the limited computing power means that smaller LLMs are easier to deploy. %the ideal size for the job.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/workflow.png}
    \caption{The pipeline to create the FRIDA suite of models. A search and rescue expert fills out a survey on the relevant tasks and objects a robot needs to be aware of, then a semantics expert adds those terms to the ontology and fills in the templates to generate new seed instructions for a variety of different disasters. These seed sentences are then utilized to generate synthetic data for fine-tuning an LLM with the necessary expertise on the specific disaster at hand.}
    \label{fig:pipeline}
\end{figure*}

% There are also issues with implementing the latest LLMs on robots from a theoretical perspective. LLMs are generalists by nature, but we want to fine-tune a model for a specific disaster. Even fine-tuned models for instruction following are often built for all types of instructions, not just instructions for a specific domain. The black-box nature of LLMs is another concern, especially in a field where following instructions is a matter of life and death. Most of all,  it is unclear how much physical common sense these models can attain on language alone and what types of data enhance physical common sense reasoning abilities.

% The straightforward answer to this would be to fine-tune a model on publicly available data about a type of disaster (e.g., tornadoes, famine), but that is easier said than done. Most disaster-related data available tracks general population response to disasters online, not search and rescue work \cite{LLMdisaster}. This is a particular challenge for us, as LLMs require substantial amounts of data for fine-tuning to ground them in real world tasks, especially when compared to the amount of data needed to train purely symbolic methods of grounding\cite{cohen2024surveyroboticlanguagegrounding}. Additionally, the disasters vary widely from case to case. For example, a robot deployed in response to a hurricane in Appalachia would come across different geography, objects, and tools needed than a robot responding to a hurricane in Florida, a place with comparatively more infrastructural protections against hurricanes. Hence, we need a system for quickly developing models to handle specific disasters.

% As a first step to developing AI for specific disasters, we propose a pipeline for disaster relief experts and linguists to create gold-standard data using templates we created to describe a wide variety of knowledge and objects needed to navigate a disaster scenario. These filled-in template instructions is then used for the few-shot generation of synthetic data (see Figure~\ref{fig:pipeline}). The resulting synthetic data can then be used for fine-tuning a smaller LLM to become a Field Ready Instruction Decoding Agent (FRIDA) model. Like its rescue dog eponym,\footnote{\url{https://en.wikipedia.org/wiki/Frida_(dog)}} our FRIDA is initially developed and tested for earthquake disaster relief, based on responses to the February 6th, 2023 earthquakes in Turkiye and Syria \cite{reuters}. 
%  %specializes in earthquakes. 
 
%  In this work, we show that the templates we created and the instructions resulting from completing them with data relevant to a particular disaster are effective at generating large quantities of quality data. Specifically, we synthetically generated 25,000 common-sense and earthquake-related multiple choice queries based on 130 linguist-crafted instructions using Gemini 1.5 Flash \cite{gemini}. We then fine-tuned the 1 Billion (1B), 3 Billion (3B) and 8 Billion (8B) parameter LLaMa 3 Instruction tuned models\cite{llama3} to create FRIDA 1B, FRIDA 3B, and FRIDA 8B. Our models were quick to train, taking between 30 minutes (1B) to 1 hour 15 minutes (8B) on 2 A100 GPUs. They were also quick to automatically evaluate, with all evaluation data being multiple choice with 1 correct answer. 
 
%  Most importantly, we demonstrate the complete FRIDA pipeline is effective at improving relevant common sense and disaster-related knowledge; our models resulting from this pipeline significantly outperformed their corresponding base models in exact match to the gold standard evaluation, as well cosine similarity of the instruction embeddings. We then ran ablation studies for each FRIDA model to better understand how our fine-tuning data affected the results. We found that our generic object-based common-sense data helped performance significantly more than our disaster and domain-specific %more factual 
%  data. We believe this pipeline is a blueprint for affordable and effective domain-specific LLMs that will be a key component in %lifesaving 
%  human-robot partnerships.

% Our contributions are as follows:
% \begin{enumerate}
%     \item An expert-in-the-loop pipeline (Figure~\ref{fig:pipeline}) %based on \citet{ppdc} 
%     for generating specific and high-quality synthetic data that can be used for fine-tuning when man-made data are not feasible to obtain. 
%     \item Two separate gold-standard datasets for few-shot synthetic data generation and evaluation on earthquake search and rescue that serve as a proof of concept for our ability to create the initial high-quality data.
%     \item A synthetic dataset of 25,000 instructions relating to common sense and earthquake that proves the effectiveness of our gold standard data for few shot prompting for synthetic training data.
%     \item The FRIDA 1B, 3B, and 8B models, trained on the above synthetic data.
%     \item A series of ablation FRIDA (aFRIDA) models trained on subsets of the synthetic dataset whose performances on our evaluations will help us determine which synthetically generated data were most effective at improving performance.
% \end{enumerate}
% We will make these contributions available upon publication.
\section{Background}
\subsection{Synthetic Data Generation}
Synthetic data, or data generated by an LLM, has become increasingly popular as an inexpensive and relatively proficient method of data collection. While cyclically fine-tuning LLMs on the synthetic data they generate denigrates the models' performance \cite{MAD}, fine-tuning on synthetic data has nevertheless improved short term performance in instruction following and social common sense \cite{tinystories,self-instruct}. 

This paper is inspired in particular by the pipeline used in \citet{self-instruct}. \citealp{self-instruct} hand crafted 175 instructions that were used for 8-shot learning to prompt GPT's \texttt{text-davinci-001} model to generate more than 50,000 instructions for a generic and ungrounded AI assistant. These instructions were then used to fine-tune \texttt{text-davinci-001}. The authors found that their method and resulting fine-tuned model performed comparably to OpenAI's GPTInstruct \cite{self-instruct}. \citealp{alpaca} innovated on \citealp{self-instruct} by fine-tuning a separate, smaller language model with a different architecture, as opposed to fine-tuning the same model that generated the data. They subsequently found that their resulting model's answers were rated as highly as GPT's \texttt{davinci-text-003}.  We adopt the latter innovation in our approach, wherein we leverage Gemini 1.5 Flash for generating synthetic data, which we use to fine-tune LLaMa3 and Minstal instruction-tuned models. 
\subsection{%Propbank Power 
Data Creation}
%We based our seed data generation process off of the pipeline outlined in \citealp{ppdc}. 
We developed an expert-in-the loop pipeline to generate high-quality seed data that leverages disaster-relief and linguistic expertise. The purpose of this pipeline is to enable quick and efficient fine-tuning of small LLMs capable of addressing critical informational needs in specific disasters.  
 The details of the pipeline are described in a previous work, here we provide a brief overview.  %Briefly, t
We developed a series of templates that can be filled in with vocabulary from an affordance ontology, based on the Rich Event Ontology \cite{kazeminejad2018automatically}. This affordance ontology is extended to serve as %\cite{firstpaper} 
 an ontology of disaster-related objects and their functionalities, as defined by the objects' PropBank semantic roles labels \cite{palmer2005proposition}. 
 
To fill in these templates with proper data, a disaster expert first provides information about the relevant objects and situations a they would encounter in their work. For this paper, the authors simulated this step by gathering existing resources authored by experts on the Turkiye-Syria Earthquake recovery efforts \cite{reuters}. After gathering domain-specific data, linguists go through a template-filling pipeline. Summarily, the linguists select the relevant vocabulary from the expert knowledge to add to the aforementioned affordance ontology. They then use this ontology and template-specific generation instructions to fill in these templates to create gold-standard data, our ``seed instructions''. These templates are formatted as multiple choice questions with semantically distinct answers. Some examples of this process, as well as some synthetic instructions that result from 5-shot prompting, can be seen in Table \ref{tab:data_ex}. 

\begin{table}[t]
    \centering
    \begin{tabular}{p{1.75cm} p{5cm}}
    \toprule
     Template &   What state should \textbf{OBJECT} be in to easily use it: \textbf{X STATE} or \textbf{Y STATE}? \\
     \cmidrule(lr){1-1}\cmidrule(lr){2-2}
     Gold Standard ``Seed'' Instruction & What state should a \textbf{drawbridge} be in for cars to cross a river? \textbf{A) Lowered} or \textbf{B) Raised}  \\ 
     \cmidrule(lr){1-1}\cmidrule(lr){2-2}
     Synthetic Instruction& What state should a \textbf{door} be in to easily enter a room? \textbf{A) Open} \textbf{B) Closed} \\
     \toprule
     Template & What role does \textbf{OBJECT} play in \textbf{DISASTER-RELATED TASK}\\
     \cmidrule(lr){1-1}\cmidrule(lr){2-2}
     Gold Standard ``Seed'' Instruction & What role do \textbf{hydraulic lifts} play in \textbf{rescuing people after an earthquake?} \\
     \cmidrule(lr){1-1}\cmidrule(lr){2-2}
     Synthetic Instruction& How is a \textbf{crowbar} typically used during \textbf{earthquake rescue operations}?\\
     \bottomrule
    \end{tabular}
    \caption{2 Examples of templates and their corresponding gold standard and synthetic instructions. Note that the blanks in the first template can only be filled in by objects with multiple states (linguistic knowledge), while the blanks in the second template can only be filled in with specific tools (disaster expert knowledge).}
    \label{tab:data_ex}
\end{table}

In total, we created 26 templates, which were then grouped by topic or type of common sense. For example, the first example in Table \ref{tab:data_ex} describes a change in state, which is often a change in the shape, size, or weight of an object. It therefore was put in the category \textit{Relative Sizes}, the category concerned with how the object exists in space. The second example in table \ref{tab:data_ex} describes how a specific tool related to the disaster is used. It was placed in the category \textit{Required Equipment}, the category concerned with how to use the specific tools required for the disaster. A complete list of categories, the templates within them, and examples, can be found in the Appendix Table~\ref{tab:templates}.
\section{Methods}
\subsection{Synthetic Dataset Generation and Analysis}
\begin{table}
    \centering
    \begin{tabular}{l l r}
    \toprule
     Metric & Range & Averages\\
    \midrule
    Instruction length & 19.11-36.98 & 39.44 words\\
    Maximum ROUGE & 0.51-0.73 & 0.67\\
    \midrule
    Reasonableness & & 0.80\\
    Informativeness & & 0.49\\
    \bottomrule
    \end{tabular}
    \begin{tabular}{l r}
    \toprule
    Category  &  Training/Dev split\\
    \midrule
     Relative Size & 3620 / 403 \\
     Object Functions & 4460 / 496 \\
     Objects Causing Harm & 2675 / 298 \\
     Earthquakes & 882 / 99\\
     Specialized Equipment & 2679 / 298 \\
     Instruction Understanding & 1792 / 200 \\
     Differences & 4458 / 496 \\
     Non-functional Object Facts & 2662 / 296 \\
     \midrule
     Total Instructions & 23232 / 2582\\
     \bottomrule
    \end{tabular}
    \caption{The top table contains overall statistics for our synthetic dataset. There is a large range in values across categories when compared to the average for both length and ROUGE values. We had high average human ratings of reasonableness to fitting the prompt, but only fair informativeness. The bottom table is the sizes of the training and development datasets used for fine-tuning FRIDA and its ablations.}
    \label{tab: set_size}
\end{table}
% \begin{table}
%     \centering
%     \begin{tabular}{l l r}
%     \toprule
%      Metric & Range & Averages  \\
%     \midrule
%     Instruction length & 19.11-36.98 & 39.44 words \\
%     Maximum ROUGE & 0.51-0.73 & 0.67 \\
%     \midrule
%     Reasonableness & & 0.80  \\
%     Cohen's Kappa & & 0.28  \\
%     \bottomrule
%     \end{tabular}
%     \caption{Overall statistics for our synthetic dataset. There is a large range in values across categories when compared to the average for both length and ROUGE values. We had high average human ratings of reasonableness to fitting the prompt, but only fair agreement.}
%     \label{tab:synth_data_stats}
% \end{table}
The dataset we use in this work is about the Turkiye-Syria Earthquake. Our data is formatted as a user-assistant chat in order to more closely align our data to the general instruction tuning all our base models received. 
% The instruction responses themselves are all multiple choice. While we acknowledge that this is less applicable to real-world applications where we would expect open-ended responses, it made evaluation more timely and less expensive. 
%, which was more important for this first stage of research. The original multiple choice instructions sometimes had multiple correct answers. 
% We also ensured that all questions only had a single correct answer, avoiding ambiguity and the evaluation challenge of multiple correct answers. 
%As we did not find a consensus in the literature as to how to give partial credit for multiple choice questions, we removed the ambiguity and changed the answer choices to all instructions to only have one correct answer. 
%In our original work, the Seed instruction were also used to evaluate the fine-tuned models, as they technically were not used directly for fine-tuning. We made our work more rigorous by developing 
In addition to the seed data, we also developed a separate gold-standard evaluation dataset.
%with our new and improved data creation pipeline. 
The final result is that each template has 5 seed instructions for synthetic data generation (130 total instructions) and at least 4 evaluation instructions (119 examples). %\footnote{The original dataset we drew from sometimes had more than 5 seed sentences per template. If that was the case, we added them to the evaluation in addition to the 4 new instructions we generated}. 

For each template, we used 5-shot prompting with Gemini-1.5-flash to generate 980 examples of said template \cite{gemini}. We chose Gemini for its accessible and affordable API, as well as its high scores on our evaluation (0.725 exact match accuracy, 0.94 average Semscore, see section \ref{sec:eval}). We prompted Gemini to return 40 instructions per API call. To ensure our synthetic data were robust, we used ROUGE scoring \cite{rouge} to ensure Gemini was not giving us duplicates of previously generated instructions. Depending on the template, the cut-off ROUGE score went from 0.8 for templates with more varied language to 0.97 for templates with very structured wording. We also increased model temperature for the more structured templates to increase diversity of responses.

We get a sense of the resulting synthetic dataset from the topmost chart in table \ref{tab: set_size}. We automatically evaluated for instruction length and each instruction's maximum ROUGE score with the other instructions in the dataset. We found we had substantial average instruction length, and reasonable ROUGE scores given our data is template-based. However, there was a large range in both metrics across the different template categories. We attribute this to the overall complexity of the individual templates, which vary substantially.

We then randomly selected 190 synthetic instructions, with 4-5 examples from each template and had 2 authors examine them. The authors rated these instructions on 2 binary metrics: reasonableness and informativeness. An instruction is marked as ``Reasonable'' if the model sufficiently followed the template's general format and intent. An instruction marked as ``Informative'' means the question is non-trivial. For instance, the instruction ``Which would be most likely to cause a serious injury if it fell from a roof? A leaf, a small pebble, a large tile, a piece of paper, or a feather'' follows the injury template ``Which of the following would be most likely to cause an injury in [SCENARIO]?''. However, the four wrong answers are so harmless the instruction becomes too easy to be meaningful. It follows that an instruction can only be informative if it is reasonable. As seen in table \ref{tab: set_size}, 80\% of sampled instructions were reasonable, but only 50\% were informative. The 2 authors had Cohen's $\kappa$ values of 0.28 for reasonableness and 0.35 for informativeness, meaning the authors had fair agreement for both metrics.  
\subsection{FRIDA Model construction}
We used our synthetic dataset to fine-tune the 1 Billion, 3 Billion, and 8 Billion parameter Instruct models from the LLaMa-3 herd \cite{llama3} as well as the Ministral 8B Instruction tuned model \cite{mistral7}. We chose to use the LLaMa suite due to its strong performance on its open source small LLMs, as well as it having multiple small instruction tuned models of different sizes \cite{llama3}. We chose Ministral 8B to serve as a comparison, since it is trained with sliding window attention, unlike the LLaMa models \cite{mistral7}. It also was released after the LLaMa 3 herd and outperformed it in many metrics \cite{mistral7}. We chose to fine-tune the instruct variations of these models because our task is based in answering questions. %, and we hoped instruction-following knowledge base before fine-tuning. 
Fine-tuning specifics can be found in Appendix section \ref{sec:ft}.


% One of our goals for this pipeline was for it to run quickly to simulate the need for fast deployment in a real disaster scenario. It took 2 days for linguistic experts to modify the seed dataset, 3.5 days for them to construct the evaluation dataset, and 5 days to generate all of the synthetic data. Fine-tuning the full FRIDA models on all examples took from 30 minutes (1B Instruct) to a bit over 1 hour (8B Instruct). The most time-consuming tasks for the human-generated data were time spent learning more about earthquake search and rescue and effectively communicating how each template works. The most time-consuming aspect of %biggest time-sink for 
% synthetic data generation was 
% %, ironically, 
% finding the balance of generating novel instructions while remaining faithful to the templates.  %the templates themselves. 
% Gemini tended to follow the templates fairly closely, and as such for some categories it was very difficult for the synthetic data to be below the ROUGE score requirements. 
\subsection{Evaluation}
\label{sec:eval}
We implement 2 different evaluations to compare FRIDA output to the gold-standard evaluation. We first evaluate with an Exact Match (EM) comparison between the gold standard and FRIDA's output. This is a strict metric, %harsh test, 
as formatting mistakes (i.e. not predicting the letter corresponding to the answer, adding punctuation) lead to an incorrect mark. We do provide system instructions to help the FRIDA models respond in the correct format, but all prompts are zero-shot. We feel this is a reasonable metric since the instructions are multiple choice, so the model sees the full answer in proper format before responding.

To get a better idea of how close the answers are from a semantic perspective, we also used SemScore \cite{semscore,semcode}. SemScore is a scoring metric that uses cosine similarity to compare the a tokenizer's embedding vectors of the gold standard and FRIDA responses. \citet{semscore} compared human rankings of 252 LLM instruction responses collected by \citet{self-instruct}, and found that SemScore most closely resembled human rankings when compared to other automatic semantic evaluations. Additionally, SemScore was tested with a variety of general instruction based models \cite{semscore}, making it the best silver-standard metric for understanding how semantically close the FRIDA models were to the gold standard. 

\subsection{Ablation Study}
As we developed the templates used to generate seed instructions, we wanted to explore which types of data improved model performance the most. To better understand the effectiveness of our data, we ran an ablation study where we fine-tuned all base models on subsets of the synthetic fine-tuning data, which can be seen in Table \ref{tab: set_size}. 

We made an ablated model for each category, where each model is fine-tuned only on the synthetic data generated by templates in said category. For example, the Relative Sizes ablation model is trained on data generated from 4 templates testing size, weight, objects fitting in containers, and objects changing state, respectively. We refer to these ablated models as ablated-FRIDA (or aFRIDA) models.

The resulting name for a FRIDA model trained only on data from the Relative Size category would thus be, ``aFRIDA 3B: relative sizes'' where the ``3B'' represents the number of parameters in the base model and ``relative sizes'' refers to the subset of data used for fine-tuning (see Appendix Table~\ref{tab:templates} for data categories). Since we fine-tuned a LLaMa and Mistral model which both had 8 Billion parameters, we the fine-tuned LLaMa model was named aFRIDA 8B, and the fine-tuned Mistral model was named MaFRIDA 8B (Minstral ablated FRIDA, 8B). The ablated models were tuned with the same hyper-parameters and hardware as the FRIDA models trained on the entire synthetic dataset.

A model suite for a given base model contains FRIDA, trained on the full dataset, as well as 8 aFRIDA models trained on the categorical subsets of the data: relative sizes and state, object function, object differences, specialized equipment, objects causing harm, non-function object facts, earthquake knowledge, and instruction understanding. Examples of data for each category can be found in \ref{tab:templates} in the appendix.
% The following are the groups of models (of different sizes) we evaluated.
% \begin{enumerate}
%     \item \textbf{FRIDA} Models fine-tuned on the entire synthetic dataset.
%     \item \textbf{FRIDA: relative sizes} Models fine-tuned on data relating to the relative shapes, weights, and sizes of various objects.
%     \item \textbf{FRIDA: object functions} Models fine-tuned on instructions requiring selection %asking the LLM to choose 
%     of an object based on its function as well as other constraining criteria (shape, specific use case, etc).
%     \item \textbf{aFRIDA: object differences} Models fine-tuned on data concerning the differences and/or hypernyms between objects with the same basic function (e.g., stairs vs. ladder).
%     \item \textbf{aFRIDA: specialized equipment} Models fine-tuned on data about using the specialized equipment needed for rescue and recovery after the disaster.
%     \item \textbf{aFRIDA: objects causing harm} Models fine-tuned on data relating to objects causing injuries, damage, and dangerous situations.
%     \item \textbf{aFRIDA:non-functional object facts} Models fine-tuned on where objects are found and their secondary uses (breaking windows, forming a barricade, etc.).
%     \item \textbf{aFRIDA: earthquake knowledge} Models fine-tuned on facts about earthquakes and ways to prepare for them. 
%     \item \textbf{aFRIDA: instruction understanding} Models fine-tuned on categorizing simple instructions and picking relevant follow-up questions to said instructions.
% \end{enumerate}
\section{Results}
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.50\linewidth]{images/overall_em.png}\hfill
%     \includegraphics[width=0.50\linewidth]{images/overall_sem.png}
%     \caption{Exact Match evaluation scores and SemScores for FRIDA and ablation models}
%     \label{fig:bars}
% \end{figure*

\begin{table*}[h]
    \begin{tabular}{p{2.375cm} p{1cm} p{2.375cm} p{1cm} p{2.375cm} p{1cm}p{2.375cm} p{1cm}}
        \toprule
        \multicolumn{8}{c}{\textbf{Base Models EM Accuracy} }\\
        \midrule
        LLaMa 3.2 1B Instruct &0.23 & LLaMa 3.2 3B Instruct & 0.18& LLaMa 3.1 8B Instruct & 0.53 & Ministral 8B Instruct & 0.65\\
        \midrule
        \multicolumn{8}{c}{\textbf{Fine-tuned FRIDA Models EM Accuracy}} \\
        \midrule
        FRIDA 1B& 0.22 & FRIDA 3B & 0.51& LLaMa FRIDA 8B & 0.56 & Ministral FRIDA 8B & 0.73\\
        \cmidrule(lr){1-2}\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
        aFRIDA 1B: relative sizes& \textbf{0.34}&aFRIDA 3B: relative sizes&\textbf{0.55}& aFRIDA 8B: relative sizes& 0.60&MaFRIDA: relative sizes 8B& \textbf{0.75}\\
        aFRIDA 1B: object functions& 0.28 &aFRIDA 3B: object functions &0.51& aFRIDA 8B: object functions& \textbf{0.65} & MaFRIDA: 8B object functions& 0.71\\
        aFRIDA 1B: object differences&0.19&aFRIDA 3B: object differences&0.48& aFRIDA 8B: object differences&  0.55& MaFRIDA: object differences&0.66 \\
        aFRIDA 1B: objects causing harm&0.27&aFRIDA 3B: objects causing harm&0.45& aFRIDA 8B: objects causing harm&  0.56&MaFRIDA: objects causing harm& 0.66\\
        aFRIDA 1B: specialized equipment&0.1&aFRIDA 3B: specialized equipment&0.41& aFRIDA 8B: specialized equipment& 0.56&MaFRIDA: specialized equipment & 0.68\\
        aFRIDA 1B: non-functional obj facts&0.15&aFRIDA 3B: non-functional obj facts&0.49& aFRIDA 8B: non-functional obj facts& 0.61&MaFRIDA non-functional obj facts 8B&0.65\\
        aFRIDA 1B: earthquake knowledge& 0.25&aFRIDA 3B: earthquake knowledge&0.33& aFRIDA 8B: earthquake knowledge& 0.59&MaFRIDA 8B: earthquake knowledge&0.59\\
        aFRIDA 1B: instruction understanding&0.11 &aFRIDA 3B: instruction understanding&0.45& aFRIDA 8B: instruction understanding& 0.11&MaFRIDA 8B: instruction understanding&0.03\\
        \bottomrule
    \end{tabular}
    \caption{The Exact Match Accuracy Scores for the base, FRIDA, and aFRIDA models. While the larger models trained on all data do improve over the base model, the best performance comes from models trained on far smaller subsets of the synthetic dataset involving comparing objects by their physical state or by their functions.} 
    \label{tab:accs_for_all}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.42]{images/heatmap.png}
    \caption{ SemScores (embedding-vector cosine similarity scores) for the M-FRIDA 8B and FRIDA 8B models, as well as their corresponding ablation and base models. These heatmaps show that across all models, performance is best in evaluation data corresponding to general common sense (object fuctions, differences) and worst in evaluation data corresponding to specialized object knowledge (earthquake, non-functional object facts). Interestingly, there is little correlation between evaluation category and ablation models trained on synthetic data based on that category.}
    \label{fig:heatmap}
\end{figure*}
As seen in Table \ref{tab:accs_for_all}, all  FRIDA models larger than 1 Billion parameters scored better in 0-shot exact match scoring than their base models. Surprisingly, the aFRIDA models for the Relative Size and Object Functions categories outperformed their corresponding FRIDA models and pre-trained baselines. The top scoring ablation models consistently outperformed their full FRIDA models by 4-9 points. Minstral-FRIDA 8B not only outperformed LLaMa-FRIDA 8B; it and MaFRIDA 8B: relative sizes also outperformed Gemini-1.5-flash's Exact Match score of 0.725 in a 0 shot setting. %These results are especially promising 

We also assessed each model's capability on each subset of templates within the evaluation dataset, and we show the Semscore (cosine similarity) results for the two best fine-tuned models in Figure \ref{fig:heatmap}. Overall, %it seems 
models fine-tuned only on objects' basic physical characteristics and functionality data performed more strongly across all categories. This was despite these categories being the ``easiest'' in our evaluation, receiving the highest exact match scores and SemScores across all models. It is also clear from both heatmaps that the most difficult topic is equipment-related questions. 

Another observation from Figure \ref{fig:heatmap} is that the base models get more consistent scores across the board, while the ablation models are more varied across categories. This is especially true for MaFRIDA 8B, where fine-tuning caused it to perform worse than the base model when measuring by SemScore. Additionally, the highest subset accuracy scores for the aFRIDA models were not always for the data on which they were fine tuned. An example of this is that the model ``MaFRIDA 8B: relative size'' scored lower on the relative size evaluation templates than the model ``MaFRIDA 8B: specialized equipment'', as seen in the leftmost heatmap for M-FRIDA 8B in Figure \ref{fig:heatmap}.  

\section{Discussion}
We were surprised by the level of improvement the FRIDA pipeline imbued, especially when our sample for data informativeness was judged to be less than ideal by the authors. We were especially surprised that the aFRIDA relative size and object function models outperformed all other models across the board. It's possible that clarifying the basic properties and affordances of objects provided a better basis for the model to have better reasoning. Another contributing factor is likely that our synthetic data on specific objects and tasks tended to be longer, more diverse, and less informative according to our dataset analysis. The sample size of these more specific synthetic data may have been too small for major improvement in those areas. 

An additional unexpected observation was that the Instruction Understanding ablation models performed so poorly in the exact match metric, but comparably to the other aFRIDA models with SemScore. We believe this is because the templates for instruction understanding included punctuation in the answer choices, but none of the other templates did. Thus the models fine-tuned only of instruction understanding would add punctuation and miss the Exact Match, reinforcing the need for multiple types of evaluation on this task.

Overall, the FRIDA pipeline is effective at improving small LLMs on its data. Another positive is that they are very lightweight, with comparable performance to a much larger Gemini model. The data relating to objects function and physicality seem to improve model performances on both the Exact Match and the SemScore metrics. It is clear, however, that FRIDA models could improve with different data distributions more heavily favoring object size and functions (which helped it improve) and specialized equipment (which all models struggled on). \textbf{The base models clearly have enough access to this disaster to correctly answer the more fact-based templates regarding disaster response, while physical common sense-related questions help to improve real-world knowledge for practical interactions.}

\section{Related Work}
% \subsection{Diversifying Synthetic Data}
% \label{sec:synth_data}
% \citet{bestsynth} provide an extensive survey of synthetic data usage for various NLP tasks. Our use case is synthetic data for aligning a model to a specific task, but we used templates, which limited our data diversity during synthetic generation. A particularly important issue for aligning a model with synthetic data is that of data diversity. \citet{personas} achieve this by using LLMs to generate ``personas'', or short descriptions of various individuals' interests and/or jobs to bias a model to create different types of questions to solve. They generated 1 billion personas, and saw strong performance when using them to generate and solve math problems. \citet{ultrachat} used 2 separate instances of GPT Turbo and gathered user-assistant dialogue data from them after specific prompting both LLMs to more closely emulate human dialogue patterns. \citet{orca} used Chain of Thought Prompting and a wide variety of system prompts to augment the FLAN dataset to create 5 million high quality examples that proved to be more robust on a variety of tasks than models that did not use these techniques.

\subsection{LLMs Reasoning about the World}
% \citet{agentsLLM} make the argument that LLMs that work as agents cannot simply be Question Answering modules, but need profiles, memory access, planning, and execution modules for full grounding. Within this framework, FRIDA serves 
% as a complementary component to a planning module---facilitating information on the canonical affordances of objects and specialized equipment, as well as physical common sense knowledge that supports actions that \emph{can} be taken with an object, even if they are not actions that are normally taken with respect to that object.  %as a basis to be trained as a planning module. 
There a wide variety of methods for leveraging LLMs for reasoning in a physical environment based on Chain of Thought prompting \cite{CoT}. These include variants like re-prompting \cite{reprompt}, which prompts the LLM to regenerate a plan if certain criteria aren't met at each steps, or Tree of Thought \cite{ToT}, which generates a tree of potential steps and evaluates each potential path via either a breadth-first or depth-first search. 

There are also methods that take allow the LLM to take in environmental feedback in response to its output. For Inner-Monologue \cite{innermono}, the LLM is given the option to ask for more scene descriptors from a human handler, which it then incorporates into its prompts, improving task completion and decreasing hallucination. Another example is SayPlan \cite{sayplan}, which uses 3D scene plans to iterate on proposed strategies until an effective path is discovered. \citet{humanplan} get feedback from LLMs themselves by using a wide variety of LLM agents to do various sub-tasks for planning, including generating a general outline, using external tools to gain information, and evaluating which plan is best. 

One resource for improving LLM understanding object affordances specifically is \citet{Text2Afford}, who curate a dataset of naturally occurring sentences and corresponding images, then transform them into inference, probing, and text and visual masking tasks. They further prove that even Visual Language Models (VLMs) do not have straightforward understandings of affordances, but few-shot fine-tuning improved LLM and VLM performance on identifying object affordances.

\subsection{Disaster Work and Natural Language Processing}
\citet{LLMdisaster} completed a systematic search and analysis of over 100 peer-reviewed papers relating to Natural Language Processing (NLP) tools being applied to disasters. 85 of the 107 papers found were for analyzing social media, with 67 of the papers analyzing twitter data specifically. Over half of the total papers had a sentiment analysis component to their work, and the 2nd and 3rd most common tools used were text classification and information extraction. 87.8\% of papers focused on natural disasters, with only 3.7\% being solely about man-made disasters (the rest pertained to both) \cite{LLMdisaster}. \citet{LLMdisaster} and \cite{agentsLLM} together showcase that while there is much research on LLMs as agents and much research on NLP analysis of disasters, there is not much overlap in these spaces.
\section{Future Work}
\label{sec:fw}
Even with strong performance, we still feel there are several ways we can further improve the FRIDA pipeline. Firstly, we want to improve our prompting for synthetic data to make them less trivial. We want to refine our common sense related templates and make sub-templates with different phrasing to make our synthetic data more reflective of real world natural language. We also hope implementing the strategies in other work \cite{personas, ultrachat, orca} for diversifying synthetic data will improve quality and make data generation more time-efficient. Finally, we plan to test the pipeline on a variety of specific disasters with %a real 
disaster experts who can provide feedback on the feasibility of our process. 

\section{Conclusion}
We introduce a pipeline to create expert-knowledge-based synthetic data that is then used for fine-tuning to create  FRIDA models. We found our pipeline substantially improved performance over 3 different instruction-tuned LLaMa models and 1 instruction-tuned Ministral model. We then performed an ablation study and found that data generated from templates based in physical common sense reasoning about objects improved performance most; ablation models trained on those data scored higher than FRIDA models trained on all synthetically generated data. This pipeline is an important step in understanding and improving LLM object reasoning for practical use. 

\section{Limitations, Risks, and Ethics}
One limitation is that we train and evaluate on template-generated data rather than naturally occurring language; there could be linguistic or stylistic differences between template-generated data and naturally occurring instructions. Though our approach still relies on access to expert input and non-trivial computational power for fine-tuning to counter these shortcomings, we outline solutions in Section \ref{sec:fw} which we believe are ripe avenues for future work. 

We note that multiple choice questions can be different and less complicated than an unconstrained turn between a user and an AI assistant . Nevertheless, we believe this work is an important step towards our goal of imbuing smaller language models with physical common sense. This is because we prove the feasibility and capability of small LLMs to complete this more constrained task. We argue that FRIDA should be seen as a proof-of-concept for LLM physical common sense understanding, which sets the stage for increasingly challenging training data and evaluations.

FRIDA is built by biasing an LLM to a specific domain. While this is important for our work, this could be misused to bias models in harmful ways, especially when considering applications involving social common sense. When modifying our seed data and templates, we took care to reduce gender bias as much as possible. This was fairly trivial since all questions pertained to objects and events, not people. We acknowledge that many objects from the ontology we used were annotated with a Western perspective, and that other cultures likely have additional uses for these objects.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}
\appendix
\section{Categories and Descriptions}
See Table \ref{tab:templates}.
\begin{table*}[h]
    \centering
    \begin{tabular}{|p{0.15\linewidth}|p{0.25\linewidth}|p{0.40\linewidth}|p{0.09\linewidth}|}
    \hline
    \textbf{Category} & \textbf{Templates}  &  \textbf{Examples} & \textbf{Instances in Seed Sets}\\
    \hline
    Relative Sizes & Biggest Object, Heaviest Object, Relative Fit &Which of these objects is the lightest? outlet, broom, pail, orange, screen  & 20\\
    &Ease of Interaction Given Object State &Is a raised or lowered drawbridge more effective at getting cars across the river?& \\
     & & Would a shoe fit in a bag? & \\
    \hline
    Object Functions& Basic Affordance, Size Restricted, Shape Restricted, General Property Restricted,  &Which of the following can be used to climb and is bigger than a table? stile, stairway, stepladder, step, ladder &25\\
    & Goal Restricted & What should I use if I want to learn something from the internet? & \\
    \hline
    Object Differences and Hypernyms & Difference within Affordance, Difference within Affordance given Criteria, & What is the difference between a window and a pane? & 25\\ & Basic Is-A, Identical Usage, Sub-Types & Can you use a shed as a barn?&  \\
    & & Choose the truck from the list. coupe,  minivan, 18 wheeler, sedan, ATV& \\
    \hline
    Objects in Risky Situations & Cause Injury, Cause Danger, Cause Object Damage & Which of the following objects would be the most dangerous if it hit something? dvd, screen, wall, drum, mat& 15\\
    \hline
    Required Equipment & How to Use, Equipment for Scenarios, Role of Equipment in Task & Give a step by step explanation of how to use a concrete saw. & 15\\
    & &  What role does a thermal imaging camera play in identifying survivors? & \\
    \hline
    Primary and Secondary Object Facts & Where Object Found, Objects in Location, Secondary Uses & Hey, which of the following can be used as a lever? art, motorcycle, picture, dvd, broom& 15 \\
    \hline
    Disaster Specific Knowledge& Earthquake knowledge &Choose the relevant precautions one should take to prepare for an earthquake.& 5\\
    \hline
    Instruction Following & Instruction Identification, Follow-Up Questions &Choose the navigation instruction: drink from the bottle, sail a boat, enter the doorway & 11\\
    \hline
    \end{tabular}
    \caption{An overview of the types of templates within each category, some examples of resulting seed sentences within each category, and the number of instances of each category within the resulting seed dataset. Note the emphasis on affordances, object knowledge, and instruction knowledge.}
    \label{tab:templates}
\end{table*}
\section{Synthetic Data Histograms by Category}
\section{Fine Tuning Specifics}
\label{sec:ft}
For fine-tuning, we used Huggingface TRL\cite{trl} supervised fine-tuning example script modified to access our custom dataset. We used random sampling to split each dataset 90-10 into training and development subsets. We fine-tuned using PEFT \cite{peft} and LORA \cite{lora} to both decrease the computational load on the robot and the time spent fine-tuning. We mostly used parameters suggested by the fine-tuning software we used \cite{trl}, with a learning rate of 2.0e-4, and lora r and alpha values of 32 and 16, respectively. The main differences between our training and the default parameters were training over 3 epochs instead of 1 and not using data packing. We fine-tuned on 2 A100 GPUs.
\section{Synthetic Data Generation Prompting}
We primed Gemini with a system prompt that read as follows:
\begin{displayquote}
    You will be creating multiple choice questions on a variety of topics related to common sense and/or earthquake knowledge. Be creative in choosing the vocabulary and phrasing of these questions. All responses must be given as json objects with the following format: 
    
    \{``instruction'':``example instruction'', ``input'':``A) this B) is C) an D) example E) question'',``output'':``E) Question''\}
\end{displayquote}
A subsequent template prompt from each template category can be seen in Table \ref{tab:prompts}. The corresponding 5 shot examples followed these prompts.
\begin{table*}[h]
    \centering
    \begin{tabular}{|p{0.1\linewidth}|p{0.6\linewidth}|}
    \hline
    Category & Prompt \\
    \hline
    Heaviest & Create 40 unique multiple choice questions about which objects weigh the most. These questions must be multiple choice and they must have 5 options with 1 correct answer. Choose lots of different objects that people interact with.\\
    \hline
    Affordances and Shape & Create 40 unique multiple choice questions about which objects can complete a given function and are a certain shape. \\
     &  These questions must be multiple choice and they must have 5 options with 1 correct answer. Choose lots of different objects that people interact with.\\
     \hline
    Use As & Create 40 unique multiple choice questions about if an object can be used as a substitute for another object. \\
     & These questions must be multiple choice with the two choices being ``it can'' or ``it cannot''. Choose lots of different objects that people interact with. \\
     \hline
     Damage to Objects& Create 40 unique multiple choice questions about which object would cause the most damage to a larger object or structure. \\
      & These questions must be multiple choice and they must have 5 options with 1 correct answer. Choose lots of different objects that people interact with.\\
      \hline
    Equipment Used in Task & Create 40 unique multiple choice questions about how an object is used in a task. The tasks and objects should be related to earthquakes. The answer choices should be brief descriptions of potential ways to use the object in the task. These questions must be multiple choice and they must have 5 options with 1 correct answer. Make sure each answer option is unique. \\
    \hline
    Secondary Uses & Create 40 unique multiple choice questions about objects that are not created to complete a task, but nevertheless can complete the task. These questions must be multiple choice and they must have 5 options with 1 correct answer. \\
    & Make sure the answer choices do not include objects that are meant to do the task described. Make sure to pick lots of unique tasks and objects. \\
    \hline
    Earthquake & Create 40 unique multiple choice questions about earthquakes, earthquake preparation, and earthquake search and rescue protocols. These questions must be multiple choice and they must have 5 options with 1 correct answer. Be as creative as possible with the types of questions you generate, as long as they have something to do with earthquakes.\\
    \hline
    Instruction ID & Create 40 unique multiple choice questions about the purpose of instructions. These questions must be multiple choice and they must have 5 options with 1 correct answer. The answer choices must all be simple instructions. Make sure the correct answer falls under the given category. Use lots of different simple instructions. \\

    \hline
    \end{tabular}
    \caption{A selection of prompts used to generate the synthetic data using Gemini Flash 1.5. Note all prompts had similar language encouraging creativity and strict multiple choice answer requirements.}
    \label{tab:prompts}
\end{table*}
\section{Licenses}
We used TRL \cite{trl} under the Apache License. SemScore \cite{semcode} implements the MIT license, and the LLaMa models were used after author agreement to the LLaMa 3.1 and 3.2 Community License Agreement \cite{llama3}. Ministral 8B Instruct was used under the Mistral Research License \cite{mistral7battention}. We used the Disagree github to calculate inter-annotator agreement scores under the MIT license \cite{disagree}. We will release our code, datasets, and FRIDA models under a open-source license which will be chosen upon conference acceptance.  \cite{mistral7}.
\end{document}
