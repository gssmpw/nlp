@article{CoT,
   abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
   author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
   month = {1},
   note = {went from (prompt, answer) tuples to (prompt, chain-of-thought explanation, answer) tuples and got much better results from few shot training.},
   title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
   url = {https://arxiv.org/pdf/2201.11903.pdf},
   year = {2022},
}

@phdthesis{LLMdisaster,
author={Godinho,Matilde M. L.},
year={2024},
title={The Impact of Natural Language Processing in Disaster Management: A Systematic Literature Review},
journal={PQDT - Global},
pages={108},
note={Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2024-10-19},
abstract={In an era where undeniably large amounts of data are produced at an unprecedented rate by a broad variety of sources, artificial intelligence and, more specifically, natural language processing present interesting solutions to today’s complex challenges. When properly implemented, natural language processing may play an essential role in all phases of the disaster management cycle by delivering timely and reliable information, optimizing resources and logistics, and assisting in reducing the effect that both natural and man-made disasters have on society. This paper presents a systematic literature review that aims to (1) identify the range of natural language processing methods used in disaster management, (2) explore the main challenges and opportunities in disaster management when using natural language processing, and (3) determine how this field impacts the four phases of disaster as well as the nature of such impact. The data was gathered using Scopus database and, with the help of Zotero citation manager tool, filtered by full-text peer-reviewed quartile 1 journal articles written in English and published between 2014 and 2023. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines, this review is based on 107 articles selected out of 548 search results obtained in February 2024. The findings indicate that social media and sentiment analysis have had an exponential growth in applications and importance within the field of disaster management. We analyze the current academic literature on the link between natural language processing and disaster management, identifying research gaps and potential for future research.},
keywords={Artificial intelligence; Social networks; Information retrieval; Web studies; 0646:Internet and social media studies; 0800:Artificial intelligence},
isbn={9798384262282},
language={English},
url={https://www.proquest.com/dissertations-theses/impact-natural-language-processing-disaster/docview/3110355993/se-2},
}

@article{Text2Afford,
  title={Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text},
  author={Sayantan Adak and Daivik Agrawal and Animesh Mukherjee and Somak Aditya},
  journal={Proceedings of the 28th Conference on Computational Natural Language Learning},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267759895}
}

@inproceedings{ToT,
title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=5Xc1ecxO1h}
}

@article{agentsLLM,
   abstract = {Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
   author = {Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Jirong Wen},
   doi = {10.1007/s11704-024-40231-1},
   issn = {2095-2228},
   issue = {6},
   journal = {Frontiers of Computer Science},
   month = {12},
   pages = {186345},
   title = {A survey on large language model based autonomous agents},
   volume = {18},
   url = {https://link.springer.com/article/10.1007/s11704-024-40231-1#Abs1},
   year = {2024},
}

@inproceedings{bestsynth,
title={Best Practices and Lessons Learned on Synthetic Data},
author={Ruibo Liu and Jerry Wei and Fangyu Liu and Chenglei Si and Yanzhe Zhang and Jinmeng Rao and Steven Zheng and Daiyi Peng and Diyi Yang and Denny Zhou and Andrew M. Dai},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=OJaWBhh61C}
}

@article{humanplan,
  title={A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models},
  author={Chengxing Xie and Difan Zou},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.18208},
  url={https://api.semanticscholar.org/CorpusID:270068266}
}

@InProceedings{innermono,
  title = 	 {Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author =       {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Jackson, Tomas and Brown, Noah and Luu, Linda and Levine, Sergey and Hausman, Karol and ichter, brian},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {1769--1782},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/huang23c/huang23c.pdf},
  url = 	 {https://proceedings.mlr.press/v205/huang23c.html},
  abstract = 	 {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.}
}

@article{orca,
  title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},
  author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Hassan Awadallah},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.02707},
  url={https://api.semanticscholar.org/CorpusID:259075316}
}

@misc{personas,
      title={Scaling Synthetic Data Creation with 1,000,000,000 Personas}, 
      author={Tao Ge and Xin Chan and Xiaoyang Wang and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2406.20094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.20094}, 
}

@inproceedings{reprompt,
title={Planning With Large Language Models Via Corrective Re-Prompting},
author={Shreyas Sundara Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and David Paulius and Stefanie Tellex},
booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
year={2022},
url={https://openreview.net/forum?id=cMDMRBe1TKs}
}

@InProceedings{sayplan,
  title = 	 {SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning},
  author =       {Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko},
  booktitle = 	 {Proceedings of The 7th Conference on Robot Learning},
  pages = 	 {23--72},
  year = 	 {2023},
  editor = 	 {Tan, Jie and Toussaint, Marc and Darvish, Kourosh},
  volume = 	 {229},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v229/rana23a/rana23a.pdf},
  url = 	 {https://proceedings.mlr.press/v229/rana23a.html},
  abstract = 	 {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a "semantic search" for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an "iterative replanning" pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.}
}

@inproceedings{ultrachat,
title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=oEsYs3WRc3}
}

