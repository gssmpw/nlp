% \section{Introduction}

% Volumetric video has emerged as a compelling medium for immersive experiences, capturing and rendering dynamic 3D scenes with six degrees of freedom (6DoF). Unlike traditional 2D video, volumetric video enables users to freely navigate and interact with the content from any viewpoint, offering a heightened sense of presence and engagement. This revolutionary technology has found applications in various domains, including entertainment, education, training, and virtual reality\cite{ortsholoportation}.

% Despite its immense potential, the widespread adoption of volumetric video faces a significant challenge: the prohibitive bandwidth requirements for streaming high-quality content. A typical volumetric video with 200K points per frame demands a staggering 720Mbps bandwidth~\cite{zhang_yuzu_nodate, liu_vues_2022}, far exceeding the capabilities of most consumer internet connections. To improve the Quality of Experience (QoE) under limited bandwidth, prior work has primarily focused on viewport-adaptive streaming techniques \cite{han_vivo_2020, lee_groot_2020}. These methods aim to stream only the content that will appear within the user's viewport, thereby reducing the bandwidth requirements. However, viewport-adaptive streaming becomes ineffective when the entire scene falls inside the viewport or when the user's motion is fast and unpredictable, making accurate 6DoF motion prediction challenging. Alternative approaches, such as remote rendering \cite{gul2020cloud, liu_vues_2022}, involve transcoding 3D scenes into regular 2D frames using edge or cloud servers. While this reduces the bandwidth consumption, it introduces additional latency and computational overhead, making it difficult to scale for a large number of users. In light of these limitations, we argue that super-resolution (SR) techniques, which have proven successful~\cite{dong2015image, isobe2020video, caballero2017real} in enhancing 2D video quality while minimizing bandwidth usage, present a promising solution for volumetric video streaming.

% A recent work, YuZu \cite{zhang_yuzu_nodate}, takes a first step towards applying super-resolution techniques to volumetric video streaming. YuZu employs a deep learning-based 3D point cloud super-resolution method to upsample the low-resolution content at the receiver's end. While this approach demonstrates the potential of using super-resolution for volumetric video streaming, it has limitations in terms of runtime performance. The deep learning-based super-resolution model used in YuZu requires high-end GPU resources to achieve real-time upsampling, which may not be readily available on consumer devices. To address this issue, we propose an interpolation and refinement-based super-resolution approach that is more computationally efficient and suitable for real-time volumetric video streaming. Our approach draws inspiration from the success of interpolation-based methods in 2D image super-resolution \cite{siu2012review, zhou2012interpolation}, which have shown to be effective and computationally efficient. Moreover, recent research has demonstrated the emergence of interpolation-based techniques for point cloud super-resolution \cite{wang_two-stage_2021, he_grad-pu_2023}, indicating their potential for volumetric video streaming.

% A typical interpolation-based super-resolution approach for point clouds consists of kNN midpoint interpolation followed by refinement using a neural network. While this approach allows for upsampling with any desired scale factor, the neural network refinement stage incurs high latency and computational overhead, hindering its application in real-time volumetric video streaming. To address this challenge, we propose a novel approach that replaces the refinement neural network with a lookup table (LUT). Our method stores pre-computed refinement results in the LUT, enabling efficient lookup of refined positions for interpolated points during upsampling. 

% While LUTs have been widely explored in the context of 2D images and videos, realizing our LUT-based super-resolution system for volumetric video streaming poses several fundamental challenges:

% \BULLET LUT's application to point clouds is novel and non-trivial. Unlike pixels in 2D images typically consisting of 256-bit RGB colors, point clouds have dynamic and variable point positions with no fixed scale, whereas LUTs typically require fixed-size inputs. This characteristic of point clouds necessitates the development of specialized techniques to adapt LUTs for point cloud super-resolution.

% \BULLET Efficiently combining the interpolation stage with LUT-based refinement presents a significant challenge. The interpolation stage increases the point density and provides a coarse approximation of the high-resolution point cloud, while the LUT-based refinement fine-tunes the positions of the interpolated points. Designing an efficient pipeline that seamlessly integrates these two stages is crucial for achieving real-time performance.

% \BULLET Leveraging the any-scale upsampling capability of the interpolation-based approach to provide good Quality of Experience (QoE) to users requires careful consideration. The upsampling scale factor should be dynamically adjusted based on the available network bandwidth and the user's viewing conditions to strike a balance between visual quality and streaming efficiency.


% To tackle these challenges and enable efficient LUT-based point cloud super-resolution for volumetric video streaming, we propose a set of novel techniques and system optimizations:
% \begin{itemize}

% \item \textbf{Position encoding} We firstly introduce a novel encoding scheme that adapts point positions of arbitrary scales to a fixed-size representation suitable for LUT-based processing. Each candidate point with its local neighbours' position is normalized and then quantized into fixed number of bins as the input to retrieve the result from LUT. 
% \item \textbf{Dilated interpolation} We enhance the quality of point cloud interpolation by incorporating a dilation mechanism into the kNN-based approach. The dilation mechanism expands the receptive field of the interpolation process, capturing more contextual information and improving the accuracy of the interpolated points. This enhancement leads to higher-quality upsampled point clouds as shown in \S~\ref{sec:eval-inter}.
% \item \textbf{Effcient SR pipeline} We develop a highly optimized point cloud super-resolution pipeline that seamlessly integrates the interpolation and LUT-based refinement stages. Our pipeline is meticulously designed to achieve real-time performance, leveraging efficient data structures, parallel processing, and system-level optimizations. To the best of our knowledge, our pipeline achieves the fastest super-resolution speed for point cloud data.
% \item \textbf{Continuous ABR in full system} Finally, We propose a novel continuous ABR algorithm that dynamically adapts the upsampling scale factor based on the available network bandwidth and the user's viewing conditions. Our ABR algorithm is built upon MPC~\cite{yin_control-theoretic_2015} by solving a continuous optimization problem.  Unlike the prior work~\cite{zhang_yuzu_nodate, lee_groot_2020, liu_vues_2022} that using discrete video streaming bitrate, our approach provide a more fine-grained streaming experience.
% \end{itemize}

% The summary of our contributions is as follows:
% \begin{itemize}
%     \item We are first to enable the point cloud super resolution with LUT acceleration.
%     \item We propose and design \name, a volumetric video streaming system with the fastest SR pipeline.
%     \item We implement \name and conduct comprehensive evaluation for its QoE improvement and runtime performance.
% \end{itemize}



% Volumnic video is becoming popular. But streaming volumetric videos are prohibitive (\eg, 720Mbps bandwidth is required for a typical volumetric video with 200K points per frame). A natural approach to reduce the data rate is to downsample the signal before the transmission and upsample at the receiver using a super resolution (SR) algorithm. While there has been significant work on SR for 2D videos, there is little work on SR for volumnetric videos. Motivated by this need, we develop an efficient and accurate SR algorithm based by leveraging lookup table (LUT). Our extensive evaluation demonstrates its effectiveness.  

% 3D volumetric video provides immersive experiences and is gaining traction in digital media. Despite its rising popularity, the streaming of volumetric video content poses significant challenges due to the high data bandwidth requirements. A natural approach  to mitigate the bandwidth issue is to reduce the volumetric video's data rate by downsampling the content prior to transmission. The video can then be upsampled at the receiver's end using a super-resolution (SR) algorithm to reconstruct the high-resolution details. While super-resolution techniques have been extensively explored and advanced for 2D video content, there is limited work on SR algorithms tailored for volumetric videos. 

% To address this gap and the growing need for efficient volumetric video streaming, we have developed a new SR algorithm specifically designed for volumetric content. Our algorithm uniquely harnesses the power of lookup tables (LUTs) to facilitate the efficient and accurate upscaling of low-resolution volumetric data. The use of LUTs enables our algorithm to quickly reference precomputed high-resolution values, thereby significantly reducing the computational complexity and time required for upscaling. We further apply adaptive video bit rate algorithm (ABR) to dynamically determine the downsampling rate according to the network condition and stream the selected video rate to the receiver. Our extensive evaluation show that our system can significantly reduce the bandwidth required for streaming volumetric content with little impact on the video quality.
% \begin{itemize}
%   \item We improve the current \textbf{interpolation} approach by introducing randomness into the collection of neighbor points.
%   \begin{itemize}
%     \item This enhancement increases the overall uniformity and effectiveness of the refinement process by X\%.
%   \end{itemize}

%   \item We transform the refinement neural network into an efficient O(1) lookup table (LUT).
%   \begin{itemize}
%     \item This transformation accelerates the refinement inference by \textbf{1000x}, reducing the average inference time from Y ms to Z ms.
%     \item The refinement is local-based, thus exhibiting the property of generalization (one model/LUT fits all), as demonstrated by experiments on A, B, and C datasets.
%   \end{itemize}

%   \item We design a novel scalable video coding and adaptive bitrate (ABR) algorithm that fully utilizes the property of any-scale upsampling.
%   \begin{itemize}
%     \item This algorithm enables flexible adaptation to network and computation resources, resulting in a mere X\% loss in visual quality compared to the original video.
%     \item Our approach eliminates the need for encoding videos at different bitrates, reducing the storage requirements by Y\%.
%   \end{itemize}

%   \item We provide both CUDA and mobile/single-board-computer (OrangePi) implementations that can support high frame rates:
%   \begin{itemize}
%     \item Over 60 FPS for OrangePi, tested on a model with A GB RAM and B GHz CPU.
%     \item Approximately 100 FPS for CUDA, tested on an Nvidia GPU with C CUDA cores and D GB memory.
%     \item Evaluated on videos with 800K resolution, demonstrating the scalability of our approach.
%   \end{itemize}
% \end{itemize}


% \section{Introduction}

% Empowered by advances in 3D capture and rendering technologies, volumetric video applications are revolutionizing how we experience digital content across entertainment \cite{}, education \cite{}, training \cite{}, and virtual reality \cite{}. These applications enable users to freely navigate and interact with dynamic 3D scenes with six degrees of freedom (6DoF), providing an unprecedented level of immersion that traditional 2D video cannot match. Among various 3D representations, point cloud has emerged as a preferred format for volumetric content due to its rendering efficiency and storage flexibility \cite{}. The recent breakthrough in 3D Gaussian splatting \cite{}, which can be viewed as a specialized point cloud format, further demonstrates the potential of point-based representations for high-quality volumetric content delivery.

% A key component of volumetric video applications is an efficient streaming system that can deliver high-quality content in real-time while operating within the constraints of consumer internet connections and devices. An ideal volumetric video streaming system must meet four critical requirements: (1) low bandwidth consumption during streaming—current solutions demand over 720Mbps for high-quality content with 200K points per frame \anlan{[This is the size of raw point clouds. Also, what is the framerate?]} \cite{}, making it impractical for most consumer internet connections, (2) low compute overhead on edge devices—processing and rendering complex 3D content requires significant computational resources that may not be available on mobile devices \cite{}, (3) low end-to-end latency for video encoding and streaming—interactive applications require total system latency under 100ms \anlan{[For VoD, this requirement is not necessary.]} \cite{}, and (4) smooth quality adaptation beyond traditional DASH-based \anlan{[DASH-like]} discrete quality levels to provide better viewer experience under varying network conditions \anlan{[Just say volumetric video streaming needs to adapt to varying bandwidth?]} \cite{}. \anlan{[We may not need this paragraph.]}

% Unfortunately, none of the existing solutions can simultaneously meet all four requirements. Viewport-adaptive streaming techniques \cite{han_vivo_2020, lee_groot_2020} reduce bandwidth by streaming only the content within the user's viewport, achieving significant bandwidth reduction in ideal conditions. However, these approaches become ineffective with rapid viewer movement or when the entire scene is visible, causing either severe quality degradation or bandwidth spikes. Remote rendering approaches \cite{gul2020cloud, liu_vues_2022} lower client-side compute requirements by offloading rendering to cloud servers, but introduce 50-200ms additional latency and face scaling limitations with multiple concurrent users \anlan{[(1) Transcoding-based solutions need edge/cloud support. (2) If multiple users are coloated, this has been solved by MuV$^{2}$ (MobiCom'24)]}. Deep learning-based super-resolution methods \cite{zhang_yuzu_nodate} improve quality by up to 6dB PSNR while reducing bandwidth by 60\%, but require high-end GPUs with at least 8GB memory to achieve real-time performance, making them impractical for consumer devices.

% \anlan{[I think the story could be: volumetric video streaming is an important research question $\rightarrow$ however it requires a lot bandwidth $\rightarrow$ existing solutions (viewport-adaptive, transcoding to 2D, SR-based) have their pros and cons, talk more about the SR-based solutions (although saving a lot of bandwidth, SR is still not affordable to many commodity devices) $\rightarrow$ In our paper, we follow the SR-based approach (why? e.g., because it is very promising) and improve both the SR accuracy/quality and system-pesepctive performance through xxxx.]}

% This paper presents \name, a novel volumetric video streaming system that meets all four requirements through an interpolation-then-refinement based super-resolution (SR) approach. At a high level, \name first applies enhanced interpolation techniques to increase point density, then refines interpolated point positions using a lookup table (LUT) derived from a neural network. \anlan{[\name boosts the SR quality/accuracy through interpolation xxx, and accelerates SR by LUT xxx.]} The key insight underpinning our approach is that while deep learning-based SR methods achieve high visual quality, their compute requirements can be drastically reduced by transforming the refinement stage into an efficient LUT-based operation.

% Realizing this insight into a practical system requires overcoming three fundamental technical challenges. First, traditional interpolation-based SR methods \cite{} suffer from inferior visual quality (typically 2-3dB PSNR lower than learning-based methods) and suboptimal latency performance due to unoptimized neighbor search operations. \anlan{[I remember GradPU is learning-based enhanced interpolation approach. Do we want to mention it here? Basically the challenge is how to further improve the SOTA learning-based SR?]} Second, while LUT-based SR is well-established for 2D content with discrete pixel grids \cite{}, its application to continuous 3D point clouds requires fundamentally new techniques for position quantization and neighborhood encoding. \anlan{[Are there any system-perspective challenges, e.g., memory is limited thus the LUT should not be too large?]} Third, combining interpolation and LUT-based refinement into an efficient end-to-end streaming system demands careful orchestration of multiple components including real-time point cloud upsampling, contiunous adaptive bitrate control, and rendering pipeline integration. \anlan{[Other system-level challenges like bandwidth adaptation, xxx.]}

% We address these challenges through three key technical innovations:
% \begin{itemize}
%     \item \textbf{Enhanced dilated interpolation:} We develop a novel interpolation technique that introduces controlled dilation and octree-based parallelism in neighbor point selection, improving both processing speed and visual quality. Our approach achieves 40\% better point distribution uniformity and 2.5× faster processing compared to traditional k-NN interpolation while maintaining memory efficiency through optimized spatial data structures.
    
%     \item \textbf{Position-aware LUT refinement:} We enable efficient 3D LUT application through a novel position encoding scheme that effectively quantizes and normalizes the continuous 3D space into discrete bins. Our method reduces the refinement computation overhead by 95\% compared to neural network inference while maintaining 85\% of the quality improvement.
    
%     \item \textbf{Continuous-ratio streaming pipeline:} We design an end-to-end system with continuous-ratio adaptive bitrate streaming that provides fine-grained quality adaptation. Our pipeline integrates fast point cloud interpolation, LUT-based refinement, and a continuous rate adaptation algorithm, achieving 30\% better bandwidth utilization compared to discrete-level approaches.
% \end{itemize}

% We implement \name on both desktop (CPU/GPU) and mobile platforms, demonstrating its practicality across different hardware configurations. \anlan{[We implement \name and evaluate it on heterogeneous devices. Better to directly say what the consumer/mobile devices are.]} Using three different types of volumetric content (human performance, static scenes, and dynamic objects) and multiple viewing scenarios, we show that \name can reduce bandwidth requirements by 65\% \anlan{[Max or average?]} while maintaining visual quality within 1dB PSNR of uncompressed content \anlan{[Not sure about the meaning]}. Our system achieves real-time performance (30+ fps) on consumer devices with only 15\% overhead compared to basic point cloud rendering \anlan{[What is the definition of ``basic point cloud rendering''? I am assuming rending point clouds is fast.]}. On mobile platforms, our LUT-based refinement achieves 20× speedup compared to neural network-based alternatives \anlan{I guess we can just say YuZu, the SOTA SR-based volumetric video streaming system.} while maintaining 85\% of the quality improvement. The training process for our LUT requires only 2 hours on a single GPU, making it practical to adapt to new content types. \anlan{[I think we can just put the most important results in the intro.]}

% \anlan{[We summarize our contribution as follows: xxxx]}

Empowered by advances in 3D capture and rendering technologies, volumetric video applications are revolutionizing how we experience digital content across entertainment~\cite{volumetricgames}, education~\cite{emad2022moesr}, virtual reality~\cite{efeve2024}, \etc These applications enable users to freely navigate and interact with dynamic 3D scenes with six degrees of freedom (6DoF), providing an unprecedented level of immersion that traditional 2D videos cannot match. Among various 3D representations, point cloud has emerged as a preferred format for volumetric content due to its rendering efficiency and capturing availability~\cite{yangComparativeMeasurementStudy2023,zhang_yuzu_nodate}. The recent breakthrough in 3D Gaussian splatting~\cite{luitenDynamic3DGaussians2023a}, which can be viewed as a specialized point cloud format, further demonstrates the potential of point-based representations for high-quality volumetric content delivery.

A key challenge in deploying volumetric video applications is the substantial bandwidth requirement for streaming high-quality point cloud content at line-rate (\ie 30 FPS), which can demand as much as 720Mbps for high-quality content with 200K points per frame~\cite{zhang_yuzu_nodate}. As a result, viewers watching volumetric content over the Internet always suffer from drastically compromised quality-of-experience (QoE).
%
While various approaches have been proposed to address this challenge, each has significant limitations. Viewport-adaptive streaming techniques reduce bandwidth usage by streaming only the content within the user's future viewport~\cite{han_vivo_2020, lee_groot_2020, liuMuV2ScalingMultiuser2024}, but suffer from quality degradation under rapid viewer movement or when rendering wide-angle scenes. 
%
Remote-rendering-based solutions employ a cloud/edge server to transcode 3D scenes to regular 2D frames that consume much less bandwidth~\cite{gul2020cloud, liu_vues_2022}. However, these approaches introduce non-negligible latency (50-200ms) and require complex infrastructure support to scale up to multiple concurrent users.
% Transcoding-based solutions offload rendering to \anlan{cloud/edge} servers~\cite{gul2020cloud, liu_vues_2022} , but introduce substantial latency (50-200ms) and require complex infrastructure to support multiple concurrent users. 

Recently, super-resolution (SR) based approaches~\cite{zhang_yuzu_nodate} have demonstrated their potentials in reducing the bandwidth consumption while boosting the users QoE for volumetric video streaming. These approaches offload the network-side burden to the client-side computation overhead, by deploying a deep 3D SR model on the client devices to enhance the visual quality of low-resolution point cloud content. Nevertheless, off-the-shelf deep 3D SR models~\cite{li_pu-gan_2019, qianPUGCNPointCloud2021, yuPUNetPointCloud2018, wang_two-stage_2021, long_pc2-pu_2022} are still too heavyweight to perform real-time (\ie 30 FPS) SR on resource-constrained mobile devices such as Meta Quest 3~\cite{meta_quest_compare}, even after extensive optimizations for inference acceleration~\cite{zhang_yuzu_nodate}. In addition, existing SR-based solutions struggle with limited upsampling ratios and growing training cost, as they require training an SR model for each upsampling ratio per video.
% Super-resolution (SR) based approaches~\cite{zhang_yuzu_nodate} show promise by achieving up to 60\% bandwidth reduction while improving visual quality, but their computational demands make them impractical for consumer devices like Meta Quest 3 that have limited \anlan{computation} resources.

% Among these approaches, SR-based solutions are particularly promising due to their ability to maintain high visual quality while significantly reducing bandwidth requirements. 

% Recent advances like GradPU~\cite{liu_grad_2020} have demonstrated impressive results with learning-based SR methods that support flexible upsampling ratios and good generalization across different content types. However, the high computational overhead of neural network inference remains a critical barrier to deployment on consumer devices.

This paper presents \name, a novel SR-enhanced volumetric video streaming system for commodity mobile devices by addressing all the above limitations. 
%
We make two vital insights for developing \name. First, the computational complexity of 3D SR can be drastically reduced by decomposing this task into two stages~\cite{liu_grad_2020}: (1) a traditional interpolation phase that performs basic SR on the input point cloud, followed by (2) a refinement deep neural network (DNN) model that fine-tunes the SR output in (1). 
%
This brand new 3D SR pipeline also provides several side benefits, such as good generalization across different content types, and flexible upsampling ratios with only a single refinement DNN model~\cite{liu_grad_2020} .
%
Second, the on-device memory is not well-utilized in previous SR-based volumetric video streaming systems. Therefore, we have rich opportunities to trade the on-device memory for efficient model inference thus further SR speedup, by transferring a DNN model to a lookup table (LUT) offline.

% This paper presents \name, a novel volumetric video streaming system that makes SR-based approaches practical for consumer devices through an innovative interpolation-then-refinement architecture. The key insight behind \name is that while deep learning models excel at point cloud upsampling, their computational complexity can be dramatically reduced by decomposing the task into two stages: (1) enhanced traditional interpolation to increase point density, followed by (2) efficient refinement using a lookup table (LUT) derived from a neural network.

We face several key challenges when building \name. 
%
First, vanilla kNN-based interpolation introduces noticeable visual distortions (see Figure~\ref{fig:dila-visual}) and high computation latency, while diminishing these distortions and accelerating the interpolation simultaneously is a non-trivial task.
%
Second, while LUT has been explored to speed up 2D image SR~\cite{jo_practical_2021, liu_4d_2022, yang_adaint_2022}, applying it to 3D SR poses a unique difficulty given the continuous nature of point cloud: converting the refinement DNN to a LUT that preserves the SR quality and fits the limited on-device memory requires sophisticated designs to balance the trade-off.
%
Furthermore, the arbitrary SR ratio ensured by our two-stage SR brings a necessity for an adaptive bitrate (ABR) streaming algorithm that can rapidly determine the highly fine-grained \{to-be-fetched point density, SR ratio\} for the volumetric frames, to fully utilize the network/compute resource.
%
% Realizing these insights into a practical system requires overcoming three fundamental challenges. Traditional interpolation methods suffer from inferior quality compared to GradPU and other learning-based methods and high computation latency, primarily due to unoptimized neighbor search operations. Additionally, while LUT-based approaches have proven effective for 2D image SR, extending them to 3D point clouds introduces new challenges—the continuous nature of point cloud data creates an exponentially larger search space than discrete image pixels, demanding novel techniques for position quantization and neighborhood encoding that can preserve neural SR quality while fitting within strict device memory constraints. The final challenge lies in quality adaptation: unlike traditional video streaming with fixed quality levels, point cloud density can be continuously varied, creating an opportunity for fine-grained bandwidth optimization that requires tight integration between SR operations and continuous bitrate adaptation.
%
We address these challenges through three core technical innovations:

\BULLET \textbf{Enhanced dilated interpolation:} We develop a novel interpolation technique that introduces controlled dilation, octree-based parallelism and neighbor relationship reuse in neighbor point selection. Our approach achieves 4$\times$ faster processing compared to traditional k-NN interpolation with no quality compromise.
    
\BULLET\textbf{Position-aware LUT refinement:} We enable efficient 3D LUT application through a novel position encoding scheme that effectively normalizes and quantizes the continuous 3D space into discrete bins. Our method reduces the refinement computation latency by over 99.9\% compared to neural network inference (sub-milliseconds v.s. seconds) while maintaining the quality benefits.
    
\BULLET \textbf{Continuous-ratio streaming pipeline:} We design an end-to-end system that integrates our SR pipeline with continuous quality adaptation, providing fine-grained control over the quality-bandwidth tradeoff. 
% Our approach achieves 17\% better bandwidth utilization compared to traditional discrete-level adaptation approaches.

We implement \name and evaluate it on both desktop PCs and mobile devices like Orange Pi~\cite{orangepi_5b} that has similar computation and memory capability as Meta Quest 3~\cite{meta_quest_compare}. Our evaluation shows that \name achieves real-time performance (30+ fps) on mobile devices while reducing bandwidth requirements by up to 70\% compared to raw point cloud streaming. Notably, our LUT-based SR achieves 8.4$\times$ speedup compared to YuZu~\cite{zhang_yuzu_nodate}'s neural SR  approach, the state-of-the-art SR-based volumetric video streaming system, and our system achieves 36.7\% better QoE than Yuzu system under both stable and LTE traces.
Our key contributions include:

\BULLET An dilated interpolation technique  with spatial information pruning and reusing that significantly improves both visual quality and processing speed for point cloud super-resolution

\BULLET A position encoding mechanism designed for applying LUT to 3D continuous space that enables efficient 3D super-resolution on resource-constrained devices

\BULLET An end-to-end system design that orchestrates the SR pipeline with continuous quality adaptation for volumetric video streaming



\begin{figure*}[ht]
    \centering
    \begin{minipage}{0.38\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/sr.pdf} % First image
        \caption{DL-based Point Cloud Super-resolution: Direct vs. Two-stage.}
        \label{fig:SR}
    \end{minipage}\hfill
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/arch.pdf}
        \vspace{-.1in}
    \caption{ The System Architecture of \name.}
    \vspace{-.3in}
    \label{fig:arch}
    \end{minipage}
\end{figure*}
