% \mysubsection{Evaluation Setup}
% \textbf{Volumetric Videos.} We use four point-cloud-based volumetric videos throughout our evaluations.
% \begin{itemize}
%     \item \textbf{The Long Dress (Dress) and Loot videos.} They have 300 frames (10 sec) each consisting of $\sim$100K points. We loop them (with cold caches) 10 times in our evaluations due to the short lasting time.
%     \item \textbf{The Haggle video.} It has 7,800 frames (4’20”) each consisting of $\sim$100K points.
%     \item \textbf{The Lab video} has 3,622 frames (2 min) each consisting of $\sim$100K points.
% \end{itemize}

% \textbf{PCSR Model and LUT tables.} We choose GradPU~\cite{he_grad-pu_2023} as our reference model for LUT construcution. Two models with different hyperparameters are trained with the Long Dress video. Then we transfer the models to the LUT tables as mentioned in Section ~\ref{sec:lut}. Each LUT table is around 1.5GB.

% \textbf{Metrics and Roadmap.} We thoroughly evaluate \name in terms of SR quality, SR runtime, and QoE. \S~\ref{sec:eval-inter} and \S~\ref{sec:eval-srqual} evaluates the quality improvement brought by our 3D SR optimizations using both subjective (i.e., real-user ratings) and objective (e.g., PSNR) metrics. \S~\ref{sec:eval-runtime} focuses on the runtime performance of our SR approach, from the perspectives of resource usage, and inference time. \S~\ref{sec:eval-e2e}  evaluate the end-to-end performance (e.g., QoE and data usage) of \name. 

% \textbf{Devices.} We use a commodity machine with an Intel Xeon Gold 6230 CPU @ 2.10GHz and 32GB memory as the \name server. We use two client hosts: a desktop with an Intel Core i9-10900X CPU @ 3.70GHz, an NVIDIA GeForce RTX 3080Ti GPU, and 32GB memory (the default client used in our evaluations); an Orange Pi embedded system board with a Rockchip RK3588S 8-core 64-bit processor @ 2.4GHz and 8GB memory, which is comparable to the most accessible VR headsets on the market, Meta Quest 3~\cite{metaQuestComparison2024} with Qualcomm XR2 chip ~\cite{qualcommXR22024}.

% \textbf{User traces} We use a multi-users’ 6DoF motion traces when watching videos, and replay them in some experiments.

\mysubsection{Evaluation Setup}
\label{sec:eval-setup}
\textbf{Volumetric Videos.} We use four point-cloud-based volumetric videos in our evaluations:

\BULLET \textbf{The Long Dress (Dress) and Loot Videos:} Each has 300 frames lasting 10 seconds and containing approximately 100K points. We loop these videos ten times in our evaluations due to their short duration.

\BULLET \textbf{The Haggle Video:} Comprises 7,800 frames (4.3 minutes) each containing approximately 100K points.

\BULLET\textbf{The Lab Video:} Features 3,622 frames (2 minutes) each with approximately 100K points.


\textbf{Model Training and LUT Generation.} We use GradPU~\cite{he_grad-pu_2023} as our reference model, training it exclusively on the Long Dress video. The training process involves downsampling the original frames to different densities and using pairs of low/high-resolution point clouds as training data. The trained model is then transformed into a single LUT table with $RF=4$ and $bin=128$ (approximately 1.5GB) following the process described in \S~\ref{sec:lut}. We apply this LUT for super-resolution across all test videos to evaluate its generalization capability.

\textbf{Evaluation Metrics.} We assess \name's performance using both geometric and perceptual metrics:
Point-to-point (P2P) Chamfer Distance (CD)~\cite{wuDensityawareChamferDistance2021,li_pu-gan_2019} measure geometric accuracy between upsampled and ground truth point clouds. Peak Signal-to-Noise Ratio (PSNR) evaluates the visual quality of upsampled points. These metrics are computed per-frame and averaged over all frames. Runtime performance is measured through CPU/GPU memory utilization, frame processing latency and frame per second (FPS). For streaming evaluation, we assess Quality of Experience (QoE) discussed in \S~\ref{sec:ABR} and data usage during transmission. \S~\ref{sec:eval-inter} and \S~\ref{sec:eval-srqual} present quality results, \S~\ref{sec:eval-runtime} examines computational efficiency, while \S~\ref{sec:eval-qoe} \S~\ref{sec:eval-e2e} analyzes end-to-end streaming performance.

\textbf{Network traces} We consider the following network conditions that are representative of today's wired and wireless networks. (1) Wired network with stable bandwidth (\eg, 50, 75, and 100 Mbps) and a round-trip time (RTT) of approximately 10ms. (2) Fluctuating bandwidth captured from real-world LTE networks, with average bandwidths varying from 32.5 to 176.5 Mbps and standard deviations ranging from 13.5 to 26.8 Mbps. Among these traces, we include a LTE trace with an average throughput of 32.5 Mbps to represent lower-bandwidth wireless network scenarios. 

\textbf{Devices.} Our server setup includes a commodity machine with an Intel Xeon Gold 6230 CPU @ 2.10GHz and 32GB of RAM. We utilize two client hosts: (1) a desktop with an Intel Core i9-10900X CPU @ 3.70GHz, an NVIDIA GeForce RTX 3080Ti GPU, and 32GB of RAM, serving as our standard evaluation client; (2) an Orange Pi embedded system equipped with a Rockchip RK3588S 8-core 64-bit processor @ 2.4GHz and 8GB of RAM, comparable to the Meta Quest 3~\cite{metaQuestComparison2024} with a Qualcomm XR2 chip~\cite{qualcommXR22024}.

\textbf{User Traces.} We employ multi-user 6DoF motion traces during video playback, replicating user movements in some experiments.

\textbf{Baselines for SR Quality Evaluation:}
For evaluating the quality of super-resolution (SR) techniques, we employ two primary baselines: GradPU~\cite{he_grad-pu_2023}, and a naive interpolation method with a dilation factor of 1. GradPU serves not only as a baseline for assessing SR quality but also as a benchmark for runtime performance. Additionally, we implemented Yuzu~\cite{zhang_yuzu_nodate} with its SR pipeline, for runtime comparisons and end-to-end evaluations. To ensure a fair comparison, we disable Yuzu’s cache and delta-coding mechanisms, as these features are orthogonal to our SR approach.




% \mysubsection{Interpolation improvement}
% \label{sec:eval-inter}
% We first examine how dilation scale improves kNN-based interpolation. 


% % \todo{todo}
% % How dilation improve the interpolation results and the effectiveness of the fine-tuning model?


% % Graph for closer demonstration of the point cloud uniformity improvement.

% % Chart comparing the dilation rate vs the interpolation effectiveness(in terms of CD/HD)

% \subsection{SR Quality and effectiveness of MoE structure}
% \label{sec:eval-srqual}


% How the LUT-based SR behaves compared to the traditional SR models and the groundtruth?

% Graph: approach comparison in different dataset in terms of CD. 

% How the hyper-parameters influence the SR quality?


% How Mixture of Expert design boosts the SR quality?
\mysubsection{SR Quality}


\begin{figure*}[t]
    \small
    \centering
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/srquality-psnrx2.png}
    \vspace{-.3in}
    \caption{PSNR for $\times2$ SR}
    \label{fig:srquality-psnrx2}    
    \vspace{-.1in}
    \end{minipage}
    \hfill
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/srquality-cdx2.png}
    \vspace{-.3in}
    \caption{ Chamfer Distance for $\times2$ SR}
    \label{fig:srquality-cdx2}  
    \vspace{-.1in}
    \end{minipage}
\end{figure*}

\begin{figure*}[t]
    \small
    \centering
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/srquality-psnrx4.png}
    \vspace{-.3in}
    \caption{PSNR for $\times4$ SR}
    \label{fig:srquality-psnrx4}    
    \vspace{-.1in}
    \end{minipage}
    \hfill
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/srquality-cdx4.png}
    \vspace{-.3in}
    \caption{Chamfer Distance for $\times4$ SR}
    \label{fig:srquality-cdx4}  
    \vspace{-.1in}
    \end{minipage}
\end{figure*}

% \begin{minipage}{.48\textwidth}
%     \centering
%     \begin{minipage}{.48\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/inter-gpu.png}
%         \vspace{-.2in}
%         \captionof{figure}{Interpolation FPS on commodity GPU}
%         \label{fig:eval-nq}    
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.48\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/inter-pi.png}
%         \vspace{-.2in}
%         \captionof{figure}{Interpolation FPS on Orange Pi}
%         \label{fig:eval-du}  
%     \end{minipage}
%     \vspace{-.1in}
% \end{minipage}
\begin{figure*}[t]
    \begin{minipage}{0.7\textwidth}
        % \begin{minipage}{\textwidth}
        %     \begin{minipage}{0.49\textwidth}
        %         \centering
        %         \includegraphics[width=\textwidth]{figures/inter-gpu.png}
        %         \vspace{-.3in}
        %         \caption{Interpolation FPS on commodity GPU}
        %         \label{fig:eval-gpu}    
        %     \end{minipage}
        %     \hfill
        %     \begin{minipage}{0.49\textwidth}
        %         \centering
        %         \includegraphics[width=\textwidth]{figures/inter-pi.png}
        %         \vspace{-.3in}
        %         \caption{Interpolation FPS on Orange Pi}
        %         \label{fig:eval-pi}  
        %     \end{minipage}
            \centering
            \includegraphics[width=\textwidth]{figures/inter.png}
            \vspace{-.3in}
            \caption{Interpolation FPS on Orange Pi (Left) and NVDIA 3080Ti (Right)}
            \label{fig:eval-inter}
        % \end{minipage}
        % \vspace{.2in}
        
        \begin{minipage}{\textwidth}
            \begin{minipage}{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/e2e-nq.png}
                \vspace{-.3in}
                \caption{Normalized QoE}
                \label{fig:eval-qoe}    
            \end{minipage}
            \hfill
            \begin{minipage}{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/e2e-du.png}
                \vspace{-.3in}
                \caption{Data usage}
                \label{fig:eval-du}  
            \end{minipage}
        \end{minipage}
    \end{minipage}
    \hfill
    \begin{minipage}{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/e2e_scatter.png}
        \vspace{-.15in}
        \caption{QoE vs. Data usage over fluctuating bandwidth (LTE traces)}
        \label{fig:scatter}
        % \vspace{-.2in}
        
        \small
        \begin{tabular}{c|l}
            \textit{H1} & VoLUT with Continuous ABR \\
            \hline
            \textit{H2} & VoLUT with Discrete ABR \\
            \hline
            \textit{H3} & VoLUT with Discrete ABR\\
            & and Yuzu SR
        \end{tabular}
        % \vspace{.05in}
        \captionof{table}{Variants of VoLUT}
        \label{tab:variants}
    \end{minipage}
\end{figure*}
To rigorously evaluate the impact of super-resolution (SR) techniques on image and geometric quality, we conduct a comprehensive set of experiments. These experiments involve multiple users watching videos while their 6 Degrees of Freedom (6DoF) motion traces are recorded. For each SR setting, we render viewports as images, denoted as \{ISR\}, and repeat this process with the original videos to capture the baseline images, denoted as \{Igt\}. We then compute the Peak Signal-to-Noise Ratio (PSNR) by comparing each image in \{ISR\} against its corresponding image in \{Igt\}. Additionally, we measure the Chamfer Distance to evaluate the geometric accuracy by comparing the SR-enhanced point clouds with the corresponding ground truth point clouds. The corresponding videos are downsampled to 100K and upsampled to $\times2$ and $\times4$.

\mysubsubsection{Interpolation with dilation}
\label{sec:eval-inter}

In this analysis, we explore the effect of varying the dilation factor ("d") within the kNN interpolation process used in SR. Specifically, we assess the PSNR outcomes for both $\times2$ and $\times4$ super-resolution settings, presented in Figures~\ref{fig:srquality-psnrx2} and \ref{fig:srquality-psnrx4}. The results indicate a clear improvement in PSNR values when dilation is increased from $K4d1$ to $K4d2$, suggesting better image quality across different upsampling ratios. Concurrently, the Chamfer Distance results, shown in Figures~\ref{fig:srquality-cdx2} and \ref{fig:srquality-cdx4}, reveal a reduction in geometric discrepancies as dilation is incorporated. These findings illustrate that enhanced dilation provides a broader spatial context during interpolation which not only improves visual clarity but also significantly enhances the geometric accuracy of the super-resolved images.

\mysubsubsection{LUT refinement}
\label{sec:eval-srqual}

The LUT refinement process targets the optimization of interpolated point cloud data by looking up the precomputed offsets stored in the Look-Up Table. This step is crucial for enhancing the final SR quality. $K4d2-lut$ represents our approach using network generated LUT.  By analyzing both PSNR and Chamfer Distance metrics post-refinement, as depicted in Figures~\ref{fig:srquality-psnrx2}, \ref{fig:srquality-psnrx4}, \ref{fig:srquality-cdx2}, and \ref{fig:srquality-cdx4}, we observe noticeable improvements in image fidelity and geometric accuracy. By comparing the GradPU and our lut results, we show that the integration of our interpolation with dilation adjustments and subsequent LUT refinement ensures that the accelerated SR process does not compromise on visual or geometric quality. 

Overall, our experimental analysis demonstrates that the applied SR techniques not only preserve but significantly enhance both the visual and geometric qualities of the images. Notably, achieving consistent PSNR values over 30 dB across various settings underscores the excellent visual quality of our SR process~\cite{thomos2005optimized,dasari2020streaming}.



\mysubsection{Runtime Performance}
\label{sec:eval-runtime}


% \begin{figure}[t]
%     \small
%     \includegraphics[width=0.5\textwidth]{figures/mem.png} % Second image
%         \caption{\small GPU memory usage.}
%         \label{fig:lut1}
% \end{figure}

% \begin{figure*}[t]
%     \small
%     \centering
%     \begin{minipage}{.48\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/mem.png} % Second image
%         \caption{GPU memory usage (3080Ti Desktop).}
%         \label{fig:gpu-mem}   
%     \vspace{-.1in}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.48\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/e2e-breakdown.png}
%     \vspace{-.3in}
%     \caption{End to End SR Runtime breakdown}
%     \label{fig:breakdown}  
%     \vspace{-.1in}
%     \end{minipage}
% \end{figure*}

% \begin{figure*}[t]
%     \small
%     \centering
%     \begin{minipage}{.48\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/GPU_runtime.png}
%     \vspace{-.3in}
%     \caption{SR Runtime on commodity GPU}
%     \label{fig:gpu}    
%     \vspace{-.1in}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.48\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/pi-runtime.png}
%     \vspace{-.3in}
%     \caption{SR Runtime on OrangePi under various upsampling ratio}
%     \label{fig:pi-runtime}  
%     \vspace{-.1in}
%     \end{minipage}
% \end{figure*}

\begin{figure*}[t]
    \small
    \centering
    \begin{minipage}{.24\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/mem.png}
    \caption{GPU memory usage (3080Ti Desktop).}
    \label{fig:gpu-mem}   
    \vspace{-.1in}
    \end{minipage}
    \hfill
    \begin{minipage}{.24\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/e2e-breakdown.png}
    \vspace{-.3in}
    \caption{End to End SR Runtime breakdown}
    \label{fig:breakdown}  
    \vspace{-.1in}
    \end{minipage}
    \hfill
    \begin{minipage}{.24\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/GPU_runtime.png}
    \vspace{-.3in}
    \caption{SR Runtime on commodity GPU}
    \label{fig:gpu}    
    \vspace{-.1in}
    \end{minipage}
    \hfill
    \begin{minipage}{.24\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/pi-runtime.png}
    \vspace{-.3in}
    \caption{SR Runtime on OrangePi under various upsampling ratio}
    \label{fig:pi-runtime}  
    \vspace{-.1in}
    \end{minipage}
\end{figure*}


We now focus on analyzing how different parts of our SR system contribute to the overall latency in both desktop and mobile settings. The experiments are conducted using 100Mbps wired netork with our continuous ABR disabled.

\textbf{Interpolation Speedup:} As shown in Figures~\ref{fig:eval-inter}, our optimized interpolation achieves significant speedups across different platforms. On the Orange Pi, we maintain 3.7$\times$-3.9$\times$ speedup over vanilla interpolation, reaching 31.2 FPS at 8$\times$ upsampling (vs vanilla's 8.0 FPS). The improvement is even more substantial on the commordity GPU empowered by the cuda implementation, where we achieve 7.5$\times$-8.1$\times$ speedup, processing at 357.1 FPS for 2$\times$ upsampling and maintaining 138.9 FPS even at 8$\times$ upsampling. This consistent performance across upsampling ratios demonstrates the effectiveness of our spatial-parallel optimization in reducing the kNN search overhead.


\textbf{GPU Memory Usage:} As shown in Figure~\ref{fig:gpu-mem},  Our approach using one LUT can improve the GPU memory usage by 86\% compared to GradPU and is comparable to Yuzu with frozen tensorflow model in c++, which is particularly beneficial for devices with limited GPU resources.

\textbf{Runtime Breakdown:} Figure~\ref{fig:breakdown} provides a detailed breakdown of the time spent in each stage of the SR process on both desktop and Orange Pi platforms. On both GPU (desktop) and mobile scenario, kNN search takes the most significant portion of time, followed by interpolation, with LUT refinement consuming the least time.

\textbf{SR Performance on Different Platforms:} Figure~\ref{fig:gpu} and Figure~\ref{fig:pi-runtime} further illustrate the SR runtime on a commodity GPU and the impact of various upsampling ratios on the Orange Pi, respectively. We show the average upsampling rate(in FPS). On the desktop (Figure~\ref{fig:gpu}), our method outperforming Yuzu by $8.4\times$ and outperforming GradPU by $46400\times$. Our approach is mainly benefited by the efficient LUT look up compared to heavy neural network inferencing even if accelerated in a frozen cpp implementation as Yuzu~\cite{zhang_yuzu_nodate} did.  As shown in Figure~\ref{fig:pi-runtime}, the upsampling speed on the Orange Pi maintains relatively stable even as the upsampling ratio increases. This is due to the fact that the main bottleneck (shown in Figure~\ref{fig:breakdown}) of our SR approach lies in the kNN based interpolation which is mainly related to the number of input points.

\mysubsection{QoE measurement under various network conditions}
\label{sec:eval-qoe}

We evaluate the QoE of our system under different network conditions, using various videos and associated motion traces. We compare our approach with Yuzu-SR, a re-implementation of Yuzu~\cite{zhang_yuzu_nodate} with caching and delta coding disabled for fair comparison. The normalized QoE results are shown in Figure~\ref{fig:eval-qoe} and the data usage (defined by the total downloaded bytes including SR models for yuzu SR and meta data) results are shown in Figure~\ref{fig:eval-du}.

\textbf{Stable Bandwidth.} We first consider a stable bandwidth scenario with a throughput of 50Mbps. Under this condition, our system achieves a normalized QoE of 100, while Yuzu-SR, on the other hand, achieves a normalized QoE of 75.8. \name is mainly benefited from the fast SR speed for any-scale upsampling compared to Yuzu-SR. 

In terms of data usage, \name can reduce it by 23\% compared to Yuzu-SR because our fine-grained ABR algorithms allows any-scale downsampling rate for transmission while Yuzu-SR's discrete SR options (1x2, 2x2, 1x3, 1x4, 4x1, 2x1) provide less optimal decision.

\textbf{Fluctuating Bandwidth.} We also evaluate the performance of our system under fluctuating bandwidth conditions using real-world LTE traces(\S~\ref{sec:eval-setup}). In this scenario, our system achieves a normalized QoE of 83 while consuming only 17\% of the data. In comparison, Yuzu-SR achieves a normalized QoE of 57 but requires 31\% of the data. Notably, QoE performance gain is higer under LTE (26) traces compared to the stable trace (24) is due to the the limited bandwidth, which pushes the systems to fetch content with lower density and introduces more SR workload. Thus \name's fast SR will benefit more under limited network resources.


% \begin{figure*}[t]
%     \small
%     \centering
%     \begin{minipage}{.48\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/e2e-nq.png}
%     \vspace{-.3in}
%     \caption{Normalized QoE}
%     \label{fig:eval-nq}    
%     \vspace{-.1in}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.48\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/e2e-du.png}
%     \vspace{-.3in}
%     \caption{Data usage}
%     \label{fig:eval-du}  
%     \vspace{-.1in}
%     \end{minipage}
% \end{figure*}

\mysubsection{Ablation study}
\label{sec:eval-e2e}
How the system is compared to Yuzu and simple Adaptation in terms of FPS and resource consumption?
% \begin{table}[t]
%     \centering
%     \begin{minipage}{0.4\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/e2e_scatter.png}
%         % \captionsetup{size=small}
%        \vspace{-.33in}
%         \captionof{figure}{QoE vs. Data usage over fluctuating bandwidth (LTE traces)}
%         \label{fig:ablation}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.55\textwidth}
%         \centering
%         \begin{tabular}{c|l}
%             \textit{H1} & \name with Continuous ABR \\
%             \hline
%             \textit{H2} & \name with Discrete ABR \\
%             \hline
%             \textit{H3} & \name with Discrete ABR and Yuzu SR \\
%         \end{tabular}
%         \vspace{-.1in}
%         % \captionsetup{size=small}
%         \caption{Variants of \name }
%         \label{tab:ablation}
%     \end{minipage}
% \end{table}


% \begin{minipage}{.48\textwidth}
%     \centering
%     \begin{minipage}{.48\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/e2e-nq.png}
%         \vspace{-.2in}
%         \captionof{figure}{Normalized QoE}
%         \label{fig:eval-nq}    
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.48\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/e2e-du.png}
%         \vspace{-.2in}
%         \captionof{figure}{Data usage}
%         \label{fig:eval-du}  
%     \end{minipage}
%     \vspace{-.1in}
% \end{minipage}


Our ablation study evaluate three variants of our system (In Table~\ref{tab:variants}). Figure~\ref{fig:scatter} shows the normalized QoE vs. data usage trade-off for these system variants under fluctuating bandwidth conditions.

Our proposed system (H1) achieves the best balance between QoE and data usage. It maintains a high normalized QoE of 98 while consuming only 31\% of the data compared to the baseline.
% This demonstrates the effectiveness of our continuous ABR approach in adapting to network conditions and our faster SR method in reducing data usage without compromising quality.
Using discrete ABR (H2) instead of continuous ABR leads a reduction of normalized QoE by 15.3\% and increases the data usage by 14\% compared to H1. This highlights the advantage of our continuous ABR approach in fine-grained bitrate adaptation, which allows better utilization of available bandwidth and reduces data consumption.

Replacing our faster SR method with Yuzu's SR (H3) results in a notable drop in QoE by 36.7\% compared to H1 while still consuming 48\% of the data. This emphasizes the faster SR speed will also benefit the stall time which is a major components of the QoE(\S~\ref{sec:ABR}).







