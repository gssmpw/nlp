% \begin{figure}[t]
%     \small
%     \centering
%     \includegraphics[width=0.4\textwidth]{VoluLUT/figures/LUT-1.pdf}
%     \vspace{-.0in}
%     \caption{\small DL-based Point Cloud Super-resolution: Direct vs. Two-stage.}
%     \label{fig:SR}
%     \vspace{-.0in}
% \end{figure}

% \subsection{Volumetric video}
% Volumetric video is an emerging technology that captures and represents real-world scenes in three dimensions, allowing users to interact with the content from any direction and location with six degrees of freedom (6DoF). This is achieved by using multiple RGB-D cameras, such as Microsoft Kinect~\cite{Kinect_wikipedia_2024}, Intel RealSense~\cite{Intel_RealSense_2024}, LIDAR scanners, which capture color (RGB) and depth (D) information from different viewpoints. The captured data are then merged to form the entire scene using open-source software systems like LiveScan3D~\cite{LiveScan3D}.

% Volumetric videos can be represented in various formats, including 3D mesh and point cloud (PtCl). PtCl is a simpler and more flexible representation that consists of unstructured 3D points with attributes such as color or intensity. Compression techniques for 3D point clouds have been investigated, with most schemes using octree-based~\cite{meagher1982geometric} or k-d tree-based compression~\cite{bentley1975multidimensional}.

% \subsection{Point cloud super resolution and application }

% Point cloud super resolution (PCSR) techniques aim to upsample low-resolution point clouds to generate high-resolution counterparts with improved quality and density. Existing direct PCSR methods, such as PU-Net~\cite{yu_pu-net_2018}, MPU~\cite{yifan2019patch}, PUGAN~\cite{li_pu-gan_2019}, and others, have made notable progress but often face limitations, including fixed upsampling ratio and inefficiency.

% GradPU~\cite{he_grad-pu_2023} allows any ratio upsampling by proposing a two-stage point cloud upsampling algorithm. Figure~\ref{fig:SR} shows its difference from the previous approach. In the first stage, GradPU directly upsamples the input low-resolution point cloud in Euclidean space using midpoint interpolation, allowing the method to generalize to arbitrary upsampling rates. In the second stage, the interpolated point cloud is refined through an iterative process that minimizes the point-to-point distance between the interpolated and ground truth high-resolution point clouds.

% YuZu~\cite{zhang_yuzu_nodate} is a volumetric video streaming system that applies existing direct PCSR models to enhance the QoE of volumetric video. Upon receipt of the low-density point cloud from a remote server, it performs super-resolution to provide the users with volumetric video with higher fidelity and fewer stalls. However, it incurs significant computational cost and high latency due to the use of deep learning-based models.

% \subsection{LUT-based Efficient Deep Learning}
% Look-up table (LUT) based approaches have recently gained attention in 2D image super-resolution (SR) as a means to improve the efficiency and practicality of deep learning-based methods~\cite{tang_lut-nn_2023,jo_practical_2021}. These approaches train a deep SR network with a small receptive field and transfer the output values to the LUT. During inference, instead of running a DNN model, which involves a large number of floating point operations, it uses a table lookup using the receptive field as the index to retrieve the precomputed high-resolution output values, thereby speeding up the inference. 

% The success of LUT-based approaches in 2D image SR motivates their application to point cloud super-resolution (PCSR) in the context of volumetric video streaming. LUT-based methods are well-suited for two-stage PCSR approaches, such as GradPU, where the second stage involves local refinement of the upsampled point cloud. The local refinement step allows for a limited receptive field of the input, making it possible to effectively transfer the learned model's output values to the LUT. By adopting a LUT-based approach in the second stage of PCSR, we can significantly reduce the computational overhead and latency associated with deep learning-based methods while still benefiting from their improved quality.



\mysubsection{Point Cloud Super-Resolution}
\label{sec:pcsr}
Point cloud super-resolution (PCSR) has emerged as a promising technique for enhancing volumetric video quality while reducing bandwidth requirements. Early PCSR methods like PU-Net~\cite{yu_pu-net_2018}, MPU~\cite{yifan2019patch}, and PUGAN~\cite{li_pu-gan_2019} demonstrated the potential of learning-based approaches but were constrained by fixed upsampling ratios and high computational demands. The recent GradPU~\cite{he_grad-pu_2023} overcame the ratio limitation by introducing a two-stage approach (shown in Figure~\ref{fig:SR}): first performing midpoint interpolation in Euclidean space, then refining point positions through iterative optimization to minimize point-to-point distances with ground truth.

While GradPU's flexible upsampling ratio and improved generalization make it particularly attractive for volumetric video streaming, its computational requirements remain prohibitive for consumer devices. YuZu~\cite{zhang_yuzu_nodate}, the first system to apply PCSR for volumetric video streaming, demonstrates this challengeâ€”despite achieving significant quality improvements, it requires high-end GPUs and introduces substantial latency due to neural network inference. This computational barrier has limited the practical deployment of PCSR-based streaming solutions on resource-constrained devices like mobile VR headsets.

\mysubsection{LUT-based Image Super-resolution}
Look-up table (LUT) based approaches have recently shown promise in accelerating 2D image super-resolution by replacing expensive neural network computations with efficient table lookups~\cite{tang_lut-nn_2023,jo_practical_2021, liu4DLUTLearnable2022, yang_adaint_2022}. These methods work by training a deep SR network with a constrained receptive field, then transferring the learned mappings to a lookup table. During inference, the system uses the local input pattern as an index to retrieve pre-computed high-resolution outputs, dramatically reducing computational overhead compared to full neural network inference.

However, extending LUT-based optimization to 3D point clouds introduces fundamental challenges not present in 2D scenarios. Unlike image pixels that exist on a discrete grid, point clouds occupy continuous 3D space, making it impossible to directly index all potential local point configurations. The indexing space grows exponentially with the number of neighboring points considered, and naive quantization schemes can lead to significant quality degradation. Additionally, while 2D images have regular neighborhoods defined by fixed pixel grids, point cloud neighborhoods vary in size and spatial distribution, further complicating the design of an effective lookup mechanism.

These challenges motivate our development of positional encoding (\S~\ref{sec:lut_construction}) that can effectively bridge the gap between 2D LUT-based approaches and 3D point cloud processing while maintaining the quality benefits of learning-based PCSR methods. By carefully addressing the continuous space quantization and neighborhood encoding problems, we can make PCSR practical for real-time volumetric video streaming on consumer devices.
