[
  {
    "index": 0,
    "papers": [
      {
        "key": "evans2023msvec",
        "author": "Evans, Michael and So{\\'o}s, Dominik and Landers, Ethan and Wu, Jian",
        "title": "MSVEC: A Multidomain Testing Dataset for Scientific Claim Verification"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "tarsi2024sciol",
        "author": "Tarsi, Tim and Adel, Heike and Metzen, Jan Hendrik and Zhang, Dan and Finco, Matteo and Friedrich, Annemarie",
        "title": "SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xiong2023trigo",
        "author": "Xiong, Jing and Shen, Jianhao and Yuan, Ye and Wang, Haiming and Yin, Yichun and Liu, Zhengying and Li, Lin and Guo, Zhijiang and Cao, Qingxing and Huang, Yinya and others",
        "title": "Trigo: Benchmarking formal mathematical proof reduction for generative language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "baral2025drawedumath",
        "author": "Baral, Sami and Lucy, Li and Knight, Ryan and Ng, Alice and Soldaini, Luca and Heffernan, Neil T and Lo, Kyle",
        "title": "DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kim2023ain",
        "author": "Kim, Jiwoo and Kim, Youngbin and Baek, Ilwoong and Bak, JinYeong and Lee, Jongwuk",
        "title": "It ain\u2019t over: A multi-aspect diverse math word problem dataset"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "jassim2023grasp",
        "author": "Jassim, Serwan and Holubar, Mario and Richter, Annika and Wolff, Cornelius and Ohmer, Xenia and Bruni, Elia",
        "title": "GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2023evaluating",
        "author": "Zhang, Xiaotian and Li, Chunyang and Zong, Yi and Ying, Zhengyu and He, Liang and Qiu, Xipeng",
        "title": "Evaluating the performance of large language models on gaokao benchmark"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zong2024gaokao",
        "author": "Zong, Yi and Qiu, Xipeng",
        "title": "GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "he2024olympiadbench",
        "author": "He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others",
        "title": "Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liang2024scemqa",
        "author": "Liang, Zhenwen and Guo, Kehan and Liu, Gang and Guo, Taicheng and Zhou, Yujun and Yang, Tianyu and Jiao, Jiajun and Pi, Renjie and Zhang, Jipeng and Zhang, Xiangliang",
        "title": "SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "parcalabescu2021valse",
        "author": "Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert",
        "title": "VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bitton2023visit",
        "author": "Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schmidt, Ludwig",
        "title": "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "bitton2022winogavil",
        "author": "Bitton, Yonatan and Bitton Guetta, Nitzan and Yosef, Ron and Elovici, Yuval and Bansal, Mohit and Stanovsky, Gabriel and Schwartz, Roy",
        "title": "WinoGAViL: Gamified association benchmark to challenge vision-and-language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "nagar2024zero",
        "author": "Nagar, Aishik and Jaiswal, Shantanu and Tan, Cheston",
        "title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis"
      },
      {
        "key": "xu2024benchmarking",
        "author": "Xu, Zhenlin and Zhu, Yi and Deng, Siqi and Mittal, Abhay and Chen, Yanbei and Wang, Manchen and Favaro, Paolo and Tighe, Joseph and Modolo, Davide",
        "title": "Benchmarking zero-shot recognition with vision-language models: Challenges on granularity and specificity"
      }
    ]
  }
]