\section{Related Work}
\paragraph{Scientific Benchmarks}
Scientific benchmarks are essential tools for evaluating the capabilities of language models in understanding and reasoning about complex scientific concepts, encompassing a wide range of disciplines, from general science to domain-specific areas like mathematics and physics. General scientific benchmarks, such as MSVEC ____ and SciOL ____, have been developed to assess various aspects of language models’ abilities in specific scientific domains, including claim verification, figure retrieval, and multimodal information comprehension. However, the increasing complexity of language models necessitates more specialized benchmarks to evaluate their performance in specific scientific domains.

In mathematics, benchmarks like TRIGO ____, DrawEduMath ____, and DMath ____ have been developed to assess AI models on targeted mathematical tasks. 
TRIGO focuses on formal mathematical proof reduction, evaluating models’ abilities to understand and manipulate complex mathematical expressions. DrawEduMath is designed to assess models’ proficiency in solving visual math problems, where both image and textual inputs are required to extract and process mathematical information. DMath, on the other hand, evaluates models on a diverse set of math word problems, testing their natural language understanding alongside mathematical reasoning.
Similarly, in physics, datasets such as GRASP ____ have been introduced to assess models’ understanding of ``Intuitive Physics'' principles, including object permanence and continuity.

Additionally, benchmarks like GAOKAO-Bench ____, GAOKAO-MM ____, OlympiadBench ____, and SceMQA ____ span multiple scientific domains, including mathematics, physics, chemistry, and biology. These benchmarks focus on high-school, Olympiad, and pre-college levels, offering comprehensive evaluations of AI models’ scientific reasoning capabilities across key disciplines.


\paragraph{Benchmarks for LVLMs} 
Benchmarks for LVLMs have been developed to evaluate their performance across various tasks, including visual question answering, image captioning, and multimodal reasoning. These benchmarks typically consist of datasets with image-text pairs accompanied by corresponding questions or instructions, assessing the ability of LVLMs to generate accurate and relevant responses.
For example, the VALSE benchmark ____ focuses on evaluating the visio-linguistic grounding capabilities of pretrained VLMs on specific linguistic phenomena. Other benchmarks, such as VisIT-Bench ____, WinoGAViL ____, and those designed for zero-shot visual reasoning ____, are aimed at assessing the ability of LVLMs to reason about visual scenes and answer questions that require minimal world knowledge. These benchmarks often analyze the impact of conveying scene information either as visual embeddings or as purely textual scene descriptions to the underlying LLM of the LVLM.




To address the scarcity of scientific benchmarks specifically designed for the high school level—supporting both text-only and multimodal reasoning—we introduce MMSciBench. As detailed in Table \ref{tab:comparison}, this dataset achieves a balanced trade-off between size and comprehensiveness, enabling efficient evaluation while offering a diverse selection of challenging high-school-level scientific problems. Additionally, MMSciBench prioritizes quality, with a significant portion of problems including detailed solution explanations and a three-level taxonomy of key knowledge points, facilitating fine-grained analysis of AI model performance.