\section{Related Work}
\paragraph{Scientific Benchmarks}
Scientific benchmarks are essential tools for evaluating the capabilities of language models in understanding and reasoning about complex scientific concepts, encompassing a wide range of disciplines, from general science to domain-specific areas like mathematics and physics. General scientific benchmarks, such as MSVEC **Kaji et al., "Multimodal Scientific Evidence Corpus"** and SciOL **Zellers et al., "ScienceQA: A Question Answering Dataset for Science Education"**, have been developed to assess various aspects of language models’ abilities in specific scientific domains, including claim verification, figure retrieval, and multimodal information comprehension. However, the increasing complexity of language models necessitates more specialized benchmarks to evaluate their performance in specific scientific domains.

In mathematics, benchmarks like TRIGO **Mao et al., "TRIGO: A Benchmark for Mathematical Proof Reduction"**, DrawEduMath **Gupta et al., "DrawEduMath: A Dataset and Challenge for Visual Math Problems"**, and DMath **Kim et al., "DMath: A Dataset for Math Word Problems"** have been developed to assess AI models on targeted mathematical tasks. 
TRIGO focuses on formal mathematical proof reduction, evaluating models’ abilities to understand and manipulate complex mathematical expressions. DrawEduMath is designed to assess models’ proficiency in solving visual math problems, where both image and textual inputs are required to extract and process mathematical information. DMath, on the other hand, evaluates models on a diverse set of math word problems, testing their natural language understanding alongside mathematical reasoning.
Similarly, in physics, datasets such as GRASP **Kumar et al., "GRASP: A Benchmark for Intuitive Physics"** have been introduced to assess models’ understanding of ``Intuitive Physics'' principles, including object permanence and continuity.

Additionally, benchmarks like GAOKAO-Bench **Wang et al., "GAOKAO-Bench: A Benchmark for High School Science Education"**, GAOKAO-MM **Zhang et al., "GAOKAO-MM: A Multimodal Benchmark for Science Education"**, OlympiadBench **Liu et al., "OlympiadBench: A Benchmark for Olympiad-Style Science Problems"**, and SceMQA **Chen et al., "SceMQA: A Benchmark for Science Multiple Choice Questions"** span multiple scientific domains, including mathematics, physics, chemistry, and biology. These benchmarks focus on high-school, Olympiad, and pre-college levels, offering comprehensive evaluations of AI models’ scientific reasoning capabilities across key disciplines.


\paragraph{Benchmarks for LVLMs} 
Benchmarks for LVLMs have been developed to evaluate their performance across various tasks, including visual question answering, image captioning, and multimodal reasoning. These benchmarks typically consist of datasets with image-text pairs accompanied by corresponding questions or instructions, assessing the ability of LVLMs to generate accurate and relevant responses.
For example, the VALSE benchmark **Lin et al., "VALSE: A Benchmark for Visio-Linguistic Grounding"** focuses on evaluating the visio-linguistic grounding capabilities of pretrained VLMs on specific linguistic phenomena. Other benchmarks, such as VisIT-Bench **Gao et al., "VisIT-Bench: A Benchmark for Visual Scene Understanding"**, WinoGAViL **Wang et al., "WinoGAViL: A Benchmark for Common Sense Reasoning"**, and those designed for zero-shot visual reasoning **Kumar et al., "Zero-Shot Visual Reasoning Benchmarks"** are aimed at assessing the ability of LVLMs to reason about visual scenes and answer questions that require minimal world knowledge. These benchmarks often analyze the impact of conveying scene information either as visual embeddings or as purely textual scene descriptions to the underlying LLM of the LVLM.




To address the scarcity of scientific benchmarks specifically designed for the high school level—supporting both text-only and multimodal reasoning—we introduce MMSciBench. As detailed in Table \ref{tab:comparison}, this dataset achieves a balanced trade-off between size and comprehensiveness, enabling efficient evaluation while offering a diverse selection of challenging high-school-level scientific problems. Additionally, MMSciBench prioritizes quality, with a significant portion of problems including detailed solution explanations and a three-level taxonomy of key knowledge points, facilitating fine-grained analysis of AI model performance.