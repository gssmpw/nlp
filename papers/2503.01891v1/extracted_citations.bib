@article{baral2025drawedumath,
  title={DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images},
  author={Baral, Sami and Lucy, Li and Knight, Ryan and Ng, Alice and Soldaini, Luca and Heffernan, Neil T and Lo, Kyle},
  journal={arXiv preprint arXiv:2501.14877},
  year={2025}
}

@article{bitton2022winogavil,
  title={WinoGAViL: Gamified association benchmark to challenge vision-and-language models},
  author={Bitton, Yonatan and Bitton Guetta, Nitzan and Yosef, Ron and Elovici, Yuval and Bansal, Mohit and Stanovsky, Gabriel and Schwartz, Roy},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26549--26564},
  year={2022}
}

@article{bitton2023visit,
  title={Visit-bench: A benchmark for vision-language instruction following inspired by real-world use},
  author={Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2308.06595},
  year={2023}
}

@inproceedings{evans2023msvec,
  title={MSVEC: A Multidomain Testing Dataset for Scientific Claim Verification},
  author={Evans, Michael and So{\'o}s, Dominik and Landers, Ethan and Wu, Jian},
  booktitle={Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={504--509},
  year={2023}
}

@article{he2024olympiadbench,
  title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

@article{jassim2023grasp,
  title={GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models},
  author={Jassim, Serwan and Holubar, Mario and Richter, Annika and Wolff, Cornelius and Ohmer, Xenia and Bruni, Elia},
  journal={arXiv preprint arXiv:2311.09048},
  year={2023}
}

@inproceedings{kim2023ain,
  title={It ainâ€™t over: A multi-aspect diverse math word problem dataset},
  author={Kim, Jiwoo and Kim, Youngbin and Baek, Ilwoong and Bak, JinYeong and Lee, Jongwuk},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14984--15011},
  year={2023}
}

@article{liang2024scemqa,
  title={SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark},
  author={Liang, Zhenwen and Guo, Kehan and Liu, Gang and Guo, Taicheng and Zhou, Yujun and Yang, Tianyu and Jiao, Jiajun and Pi, Renjie and Zhang, Jipeng and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.05138},
  year={2024}
}

@inproceedings{nagar2024zero,
  title={Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis},
  author={Nagar, Aishik and Jaiswal, Shantanu and Tan, Cheston},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@article{parcalabescu2021valse,
  title={VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena},
  author={Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert},
  journal={arXiv preprint arXiv:2112.07566},
  year={2021}
}

@inproceedings{tarsi2024sciol,
  title={SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain},
  author={Tarsi, Tim and Adel, Heike and Metzen, Jan Hendrik and Zhang, Dan and Finco, Matteo and Friedrich, Annemarie},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4560--4571},
  year={2024}
}

@article{xiong2023trigo,
  title={Trigo: Benchmarking formal mathematical proof reduction for generative language models},
  author={Xiong, Jing and Shen, Jianhao and Yuan, Ye and Wang, Haiming and Yin, Yichun and Liu, Zhengying and Li, Lin and Guo, Zhijiang and Cao, Qingxing and Huang, Yinya and others},
  journal={arXiv preprint arXiv:2310.10180},
  year={2023}
}

@inproceedings{xu2024benchmarking,
  title={Benchmarking zero-shot recognition with vision-language models: Challenges on granularity and specificity},
  author={Xu, Zhenlin and Zhu, Yi and Deng, Siqi and Mittal, Abhay and Chen, Yanbei and Wang, Manchen and Favaro, Paolo and Tighe, Joseph and Modolo, Davide},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1827--1836},
  year={2024}
}

@article{zhang2023evaluating,
  title={Evaluating the performance of large language models on gaokao benchmark},
  author={Zhang, Xiaotian and Li, Chunyang and Zong, Yi and Ying, Zhengyu and He, Liang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2305.12474},
  year={2023}
}

@article{zong2024gaokao,
  title={GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation},
  author={Zong, Yi and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2402.15745},
  year={2024}
}

