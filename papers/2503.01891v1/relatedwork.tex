\section{Related Work}
\paragraph{Scientific Benchmarks}
Scientific benchmarks are essential tools for evaluating the capabilities of language models in understanding and reasoning about complex scientific concepts, encompassing a wide range of disciplines, from general science to domain-specific areas like mathematics and physics. General scientific benchmarks, such as MSVEC \cite{evans2023msvec} and SciOL \cite{tarsi2024sciol}, have been developed to assess various aspects of language models’ abilities in specific scientific domains, including claim verification, figure retrieval, and multimodal information comprehension. However, the increasing complexity of language models necessitates more specialized benchmarks to evaluate their performance in specific scientific domains.

In mathematics, benchmarks like TRIGO \cite{xiong2023trigo}, DrawEduMath \cite{baral2025drawedumath}, and DMath \cite{kim2023ain} have been developed to assess AI models on targeted mathematical tasks. 
TRIGO focuses on formal mathematical proof reduction, evaluating models’ abilities to understand and manipulate complex mathematical expressions. DrawEduMath is designed to assess models’ proficiency in solving visual math problems, where both image and textual inputs are required to extract and process mathematical information. DMath, on the other hand, evaluates models on a diverse set of math word problems, testing their natural language understanding alongside mathematical reasoning.
Similarly, in physics, datasets such as GRASP \cite{jassim2023grasp} have been introduced to assess models’ understanding of ``Intuitive Physics'' principles, including object permanence and continuity.

Additionally, benchmarks like GAOKAO-Bench \cite{zhang2023evaluating}, GAOKAO-MM \cite{zong2024gaokao}, OlympiadBench \cite{he2024olympiadbench}, and SceMQA \cite{liang2024scemqa} span multiple scientific domains, including mathematics, physics, chemistry, and biology. These benchmarks focus on high-school, Olympiad, and pre-college levels, offering comprehensive evaluations of AI models’ scientific reasoning capabilities across key disciplines.


\paragraph{Benchmarks for LVLMs} 
Benchmarks for LVLMs have been developed to evaluate their performance across various tasks, including visual question answering, image captioning, and multimodal reasoning. These benchmarks typically consist of datasets with image-text pairs accompanied by corresponding questions or instructions, assessing the ability of LVLMs to generate accurate and relevant responses.
For example, the VALSE benchmark \cite{parcalabescu2021valse} focuses on evaluating the visio-linguistic grounding capabilities of pretrained VLMs on specific linguistic phenomena. Other benchmarks, such as VisIT-Bench \cite{bitton2023visit}, WinoGAViL \cite{bitton2022winogavil}, and those designed for zero-shot visual reasoning \cite{nagar2024zero, xu2024benchmarking}, are aimed at assessing the ability of LVLMs to reason about visual scenes and answer questions that require minimal world knowledge. These benchmarks often analyze the impact of conveying scene information either as visual embeddings or as purely textual scene descriptions to the underlying LLM of the LVLM.




To address the scarcity of scientific benchmarks specifically designed for the high school level—supporting both text-only and multimodal reasoning—we introduce MMSciBench. As detailed in Table \ref{tab:comparison}, this dataset achieves a balanced trade-off between size and comprehensiveness, enabling efficient evaluation while offering a diverse selection of challenging high-school-level scientific problems. Additionally, MMSciBench prioritizes quality, with a significant portion of problems including detailed solution explanations and a three-level taxonomy of key knowledge points, facilitating fine-grained analysis of AI model performance.