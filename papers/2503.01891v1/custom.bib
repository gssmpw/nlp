@inproceedings{evans2023msvec,
  title={MSVEC: A Multidomain Testing Dataset for Scientific Claim Verification},
  author={Evans, Michael and So{\'o}s, Dominik and Landers, Ethan and Wu, Jian},
  booktitle={Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={504--509},
  year={2023}
}

@inproceedings{tarsi2024sciol,
  title={SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain},
  author={Tarsi, Tim and Adel, Heike and Metzen, Jan Hendrik and Zhang, Dan and Finco, Matteo and Friedrich, Annemarie},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4560--4571},
  year={2024}
}

@article{xiong2023trigo,
  title={Trigo: Benchmarking formal mathematical proof reduction for generative language models},
  author={Xiong, Jing and Shen, Jianhao and Yuan, Ye and Wang, Haiming and Yin, Yichun and Liu, Zhengying and Li, Lin and Guo, Zhijiang and Cao, Qingxing and Huang, Yinya and others},
  journal={arXiv preprint arXiv:2310.10180},
  year={2023}
}

@inproceedings{kim2023ain,
  title={It ainâ€™t over: A multi-aspect diverse math word problem dataset},
  author={Kim, Jiwoo and Kim, Youngbin and Baek, Ilwoong and Bak, JinYeong and Lee, Jongwuk},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14984--15011},
  year={2023}
}

@inproceedings{zhang2024infinitymath,
  title={Infinitymath: A scalable instruction tuning dataset in programmatic mathematical reasoning},
  author={Zhang, Bo-Wen and Yan, Yan and Li, Lin and Liu, Guang},
  booktitle={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
  pages={5405--5409},
  year={2024}
}

@article{jassim2023grasp,
  title={GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models},
  author={Jassim, Serwan and Holubar, Mario and Richter, Annika and Wolff, Cornelius and Ohmer, Xenia and Bruni, Elia},
  journal={arXiv preprint arXiv:2311.09048},
  year={2023}
}

@article{lai2024enzchemred,
  title={EnzChemRED, a rich enzyme chemistry relation extraction dataset},
  author={Lai, Po-Ting and Coudert, Elisabeth and Aimo, Lucila and Axelsen, Kristian and Breuza, Lionel and De Castro, Edouard and Feuermann, Marc and Morgat, Anne and Pourcel, Lucille and Pedruzzi, Ivo and others},
  journal={Scientific Data},
  volume={11},
  number={1},
  pages={982},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{liang2024scemqa,
  title={SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark},
  author={Liang, Zhenwen and Guo, Kehan and Liu, Gang and Guo, Taicheng and Zhou, Yujun and Yang, Tianyu and Jiao, Jiajun and Pi, Renjie and Zhang, Jipeng and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.05138},
  year={2024}
}

@article{he2024olympiadbench,
  title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

@article{parcalabescu2021valse,
  title={VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena},
  author={Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert},
  journal={arXiv preprint arXiv:2112.07566},
  year={2021}
}

@article{bitton2023visit,
  title={Visit-bench: A benchmark for vision-language instruction following inspired by real-world use},
  author={Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2308.06595},
  year={2023}
}

@article{bitton2022winogavil,
  title={WinoGAViL: Gamified association benchmark to challenge vision-and-language models},
  author={Bitton, Yonatan and Bitton Guetta, Nitzan and Yosef, Ron and Elovici, Yuval and Bansal, Mohit and Stanovsky, Gabriel and Schwartz, Roy},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26549--26564},
  year={2022}
}

@inproceedings{nagar2024zero,
  title={Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis},
  author={Nagar, Aishik and Jaiswal, Shantanu and Tan, Cheston},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@inproceedings{xu2024benchmarking,
  title={Benchmarking zero-shot recognition with vision-language models: Challenges on granularity and specificity},
  author={Xu, Zhenlin and Zhu, Yi and Deng, Siqi and Mittal, Abhay and Chen, Yanbei and Wang, Manchen and Favaro, Paolo and Tighe, Joseph and Modolo, Davide},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1827--1836},
  year={2024}
}

@article{zhang2023evaluating,
  title={Evaluating the performance of large language models on gaokao benchmark},
  author={Zhang, Xiaotian and Li, Chunyang and Zong, Yi and Ying, Zhengyu and He, Liang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2305.12474},
  year={2023}
}

@article{zong2024gaokao,
  title={GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation},
  author={Zong, Yi and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2402.15745},
  year={2024}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@article{openai2024gpt4technicalreport,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{grattafiori2024llama3herdmodels,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{anthropic2024claude,
  title={Claude 3.5 sonnet model card addendum},
  author={Anthropic, AI},
  journal={Claude-3.5 Model Card},
  volume={2},
  year={2024}
}

@misc{yang2024qwen25mathtechnicalreportmathematical,
      title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}, 
      author={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},
      year={2024},
      eprint={2409.12122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12122}, 
}

@misc{shao2024deepseekmathpushinglimitsmathematical,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03300}, 
}

@article{truhn2023large,
  title={Large language models should be used as scientific reasoning engines, not knowledge databases},
  author={Truhn, Daniel and Reis-Filho, Jorge S and Kather, Jakob Nikolas},
  journal={Nature medicine},
  volume={29},
  number={12},
  pages={2983--2984},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{ma2024sciagent,
  title={Sciagent: Tool-augmented language models for scientific reasoning},
  author={Ma, Yubo and Gou, Zhibin and Hao, Junheng and Xu, Ruochen and Wang, Shuohang and Pan, Liangming and Yang, Yujiu and Cao, Yixin and Sun, Aixin and Awadalla, Hany and others},
  journal={arXiv preprint arXiv:2402.11451},
  year={2024}
}

@article{sprueill2023monte,
  title={Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design},
  author={Sprueill, Henry W and Edwards, Carl and Olarte, Mariefel V and Sanyal, Udishnu and Ji, Heng and Choudhury, Sutanay},
  journal={arXiv preprint arXiv:2310.14420},
  year={2023}
}

@article{gu2024survey,
  title={A Survey on LLM-as-a-Judge},
  author={Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others},
  journal={arXiv preprint arXiv:2411.15594},
  year={2024}
}

@article{chen2024mllm,
  title={Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark},
  author={Chen, Dongping and Chen, Ruoxi and Zhang, Shilin and Liu, Yinuo and Wang, Yaochen and Zhou, Huichi and Zhang, Qihui and Wan, Yao and Zhou, Pan and Sun, Lichao},
  journal={arXiv preprint arXiv:2402.04788},
  year={2024}
}

@article{raju2024constructing,
  title={Constructing domain-specific evaluation sets for llm-as-a-judge},
  author={Raju, Ravi and Jain, Swayambhoo and Li, Bo and Li, Jonathan and Thakker, Urmish},
  journal={arXiv preprint arXiv:2408.08808},
  year={2024}
}

@article{baral2025drawedumath,
  title={DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images},
  author={Baral, Sami and Lucy, Li and Knight, Ryan and Ng, Alice and Soldaini, Luca and Heffernan, Neil T and Lo, Kyle},
  journal={arXiv preprint arXiv:2501.14877},
  year={2025}
}