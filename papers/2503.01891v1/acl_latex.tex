% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}  % 对号
\newcommand{\xmark}{\ding{55}}  % 叉号

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{fontawesome}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{linguex}
\usepackage{makecell}
\usepackage[breakable]{tcolorbox}
\usepackage{CJKutf8}
\usepackage{amsmath}
% \usepackage{ctex}

% \usepackage{makecell} % Include this in your preamble for multi-line cells
% \usepackage{longtable}
\usepackage{longtable}  % For tables that span multiple pages
\usepackage{booktabs}    % For professional-looking tables
\usepackage{makecell}    % For multi-line cells
\usepackage{array}       % For extended column definitions

\newcommand{\new}[1]{\textcolor{red}{#1}}
% \newcommand{\Xhline}[1]{\noalign{\ifnum0=`}\fi\hrule height #1}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Xinwu Ye\textsuperscript{1}, Chengfan Li\textsuperscript{2}, Siming Chen\textsuperscript{3}, Xiangru Tang\textsuperscript{1}, Wei Wei\textsuperscript{4}\footnotemark[1] \\
\textsuperscript{1}Department of Computer Science, Yale University, \\
\textsuperscript{2}Department of Computer Science, Brown University, \\
\textsuperscript{3}School of Data Science, Fudan University, \\
\textsuperscript{4}Datawiz LLC
}

\begin{document}
\begin{CJK*}{UTF8}{gbsn}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding Author.}
% 恢复脚注标记为数字
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}


Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings.
Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only \textbf{63.77\%} accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. 
The code for MMSciBench is open-sourced at GitHub\footnote{\url{https://anonymous.4open.science/r/MMSciBench-code-812A/}}, and the dataset is available at Hugging Face\footnote{\url{https://huggingface.co/datasets/anonymous-acl-submission/mmscibench-anonymous}}.


\end{abstract}


\section{Introduction}


\begin{figure}[tb!]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        Question (Single Choice): As shown in the figure, two identical right-angled glass prisms $ABC$ are placed with their $AC$ faces parallel to each other, and between them is a uniform unknown transparent medium. A monochromatic thin light beam $O$ is incident perpendicular to the $AB$ face. (\quad) is the possible exit light path in the diagram.

        \includegraphics[width=\textwidth]{images/PhysicsMCQQuestion.png}
        
        Options:
        
        A. Any one of the lines 1, 2, 3 (parallel to each other)
        
        B. Any one of the lines 4, 5, 6 (parallel to each other)
        
        C. Any one of the lines 7, 8, 9 (parallel to each other)
        
        D. Only one of the lines 4 or 6

        \textbf{Standard Solution}: B
    \end{tcolorbox}

    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        This question primarily tests knowledge of prism-related problems.
        
        Option analysis: According to the problem description, the refractive index of the medium between the two right-angled prisms is unknown. It may be greater than, equal to, or smaller than the refractive index of the glass. The possible light path diagrams are as follows:

        \includegraphics[width=\textwidth]{images/PhysicsMCQSol.png}
        
        Therefore, Option B is correct, and Options A, C, and D are incorrect.
        
        In conclusion, the correct answer to this question is B.
    \end{tcolorbox}
    \vspace{-.1cm}
    \caption{The English translation of an example of a physics MCQ, featuring a single-choice question, the correct answer, and a detailed explanation to aid understanding. The original Chinese version is shown in Fig. \ref{fig:physics_mcq} in the appendix.}
    \label{fig:physics_mcq_en}
        \vspace{-.1cm}    \vspace{-.1cm}
    \vspace{-.1cm}
    \vspace{-.1cm}

\end{figure}


Scientific reasoning represents a crucial test of artificial intelligence (AI) systems' ability to understand and apply complex concepts, making it essential for developing truly intelligent models ~\cite{evans2023msvec,liang2024scemqa,zhang2023evaluating,truhn2023large,ma2024sciagent,sprueill2023monte}.
Recent advancements in LLMs like GPTs \cite{brown2020languagemodelsfewshotlearners,openai2024gpt4technicalreport} and Llama \cite{grattafiori2024llama3herdmodels} have significantly transformed the field of natural language processing (NLP). 
Despite these advances, scientific reasoning remains challenging for these models, facing several key limitations:
\textit{(1) Lack of multimodal evaluation}: While LVLMs have emerged as powerful models capable of processing both images and text, existing scientific benchmarks are predominantly text-only, preventing comprehensive assessment of visual-textual reasoning abilities.
\textit{(2) Limited domain coverage}: Current scientific datasets either focus too narrowly on individual subjects or too broadly across scientific areas, failing to systematically evaluate understanding of key concepts within specific disciplines.
\textit{(3) Insufficient assessment granularity}: Existing benchmarks lack human-annotated difficulty levels and structured taxonomies of scientific concepts, making it challenging to evaluate models' performance across different complexity levels and specific knowledge domains.
These limitations create an urgent need for a benchmark that can effectively evaluate both LLMs' and LVLMs' scientific reasoning abilities while addressing these challenges.




To address these challenges, we introduce MMSciBench, a benchmark focused on mathematics and physics that evaluates scientific reasoning capabilities. Our benchmark makes three key contributions:
(1) A comprehensive evaluation framework that combines multiple-choice questions (MCQs) and open-ended Q\&A problems, designed to test diverse reasoning skills across mathematical and physical domains.
(2) A novel multimodal assessment approach incorporating both text-only and text-image formats, enabling direct comparison of models' unimodal versus multimodal reasoning capabilities.
(3) A hierarchical taxonomy of scientific concepts with human-annotated difficulty levels, detailed solutions, and explanations for each problem.
We conducted extensive experiments using four state-of-the-art LVLMs (including both open-source and proprietary models) on the complete dataset, and two mathematics-specialized LLMs on text-only questions. For consistent evaluation across models, we employed GPT-4o as an automated assessor.



Our evaluation reveals significant limitations in current models' multimodal scientific reasoning capabilities. Gemini 1.5 Pro 002 achieved the highest accuracy (\textbf{63.77\%}), followed by Claude 3.5 Sonnet (\textbf{53.95\%}) and GPT-4o (\textbf{50.94\%}), while Llama-3.2-90B-Vision-Instruct performed substantially lower (\textbf{31.19\%}). Analysis across task types exposed three critical challenges:
(1) Performance degradation on open-ended tasks, with accuracy dropping by an average of \textbf{22.32\%} compared to multiple-choice questions
(2) Systematic failures in complex mathematical and physical reasoning, particularly in domains requiring multi-step problem-solving
(3) Limited visual-textual integration, evidenced by a \textbf{36.28\%} performance gap between text-only and text-image questions
Notably, model performance improved when utilizing explicit chain-of-thought prompting and English-language reasoning, even for Chinese-language questions, suggesting potential pathways for enhancing scientific reasoning capabilities.



\section{MMSciBench}


\subsection{Data Collection and Preprocessing} 
% The benchmark data was provided by Datawiz LLC, a prominent organization specializing in AI and education. 
The benchmark data was originally curated by K-12 teachers who annotate questions, detailed step-by-step solutions, final answers, difficulty level, knowledge points, as well as a bunch of other metadata.
The dataset\footnote{The dataset is released under the apache-2.0 license.} includes precise text descriptions, high-resolution images, and high-quality solutions, all compiled and shared as part of a collaborative research effort aimed at advancing AI benchmarking standards.
Each question in the dataset is assigned a human-annotated hardness score ranging from 0 to 1, where 1 represents the most challenging questions, and zero denotes the easiest.

To ensure benchmark quality and rigor, we implemented a systematic data curation process. We filtered out questions with incomplete information or duplicate content, focusing on problems with well-defined, quantifiable answers. Following our emphasis on challenging scientific reasoning, we selected questions with human-annotated difficulty scores $\geq$ 0.7 on a standardized scale. To maintain consistent evaluation conditions, we limited visual content to a maximum of one image per question.
To enable systematic knowledge categorization, we employed GPT-4o to annotate each question according to a three-level subject-specific taxonomy, detailed in Section \ref{sec:dataset_description}. The classification results were thoroughly validated by experienced K-12 curriculum specialists to ensure accuracy and alignment with educational standards. This taxonomic analysis confirmed that our filtered dataset maintains comprehensive coverage of key scientific concepts while focusing on challenging problems. Following preprocessing and validation, the final benchmark contains \textbf{4,482} question-solution pairs that enable rigorous evaluation of models' scientific reasoning capabilities across diverse domains.


\subsection{Dataset Description}
\label{sec:dataset_description}

\paragraph{Data Characteristics} The MMSciBench dataset offers several distinct advantages over previous scientific datasets:


\begin{enumerate}
\item \textbf{Curriculum Coverage:} The benchmark spans essential high school mathematics and physics concepts through carefully curated MCQs and open-ended Q\&A questions. We maintain comprehensiveness while keeping the dataset size tractable ($N=\textbf{4,482}$).
\vspace{-.2cm}
\item \textbf{Quality Assurance:} Questions undergo multi-stage validation by K-12 educators and domain experts, ensuring pedagogical relevance and technical accuracy. Each question includes detailed solutions and explanations.
\vspace{-.2cm}
\item \textbf{Multimodal Design:} The parallel text-only and text-image question formats enable systematic comparison of unimodal and multimodal reasoning capabilities.
\vspace{-.2cm}
\item \textbf{Structured Assessment:} Questions are organized through a three-level taxonomy and annotated with standardized difficulty scores, facilitating fine-grained analysis of model performance.
\end{enumerate}

An example of a physics MCQ in English is shown in Fig. \ref{fig:physics_mcq_en}, with the original Chinese version available in Fig. \ref{fig:physics_mcq} in the appendix. Additionally, a detailed comparison between MMSciBench and other scientific benchmarks is provided from multiple perspectives in Table \ref{tab:comparison}.

\begin{table*}[!htbp]
\setlength{\tabcolsep}{3pt}
\centering
 \resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Benchmark} & \textbf{Subject} & \textbf{Modality} & \textbf{key knowledge point} & \textbf{Explanation} & \textbf{Language} & \textbf{Difficulty} & \textbf{Size} \\
\midrule
\textbf{MSVEC} & P, O & T & \xmark & \cmark & EN & College & 200 \\
\textbf{SciOL} & P, O & T\&I & \xmark & \xmark & EN & College & 18M \\
\textbf{TRIGO} & M & T & \xmark & \cmark & Lean & High School & 11K \\
\textbf{DMath} & M & T & \cmark & \cmark & EN\&KR & Grade School & 10K \\
\textbf{GRASP} & P & T\&V & \cmark & \xmark & EN & Basic & 2K \\
\textbf{SceMQA} & M, P, O & T\&I & \cmark & \cmark & EN & Pre-College & 1K \\
\textbf{OlympiadBench} & M, P & T, T\&I & \cmark & \cmark & EN, ZH & Olympiad & 8K \\
\textbf{GAOKAO-Bench} & M, P, O & T & \xmark & \cmark & ZH & High School & 3K \\
\textbf{GAOKAO-MM} & M, P, O & T, T\&I & \xmark & \cmark & ZH & High School & 650 \\
\midrule
\textbf{MMSciBench (Ours)} & M, P & T, T\&I & \cmark & \cmark & ZH & High School & 4K \\
\bottomrule
\end{tabular}}
\caption{Comparison of MMSciBench with existing benchmarks. T denotes text-only data, T\&I denotes text-image data pairs, and T\&V denotes text-video data pairs. EN, ZH, and KR represent English, simplified Chinese, and Korean, respectively.}
\label{tab:comparison}
\end{table*}

\begin{figure}[t!]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/key_point_pie.pdf}
    \end{adjustbox}
    \caption{The distribution of data in MMSciBench according to the first-level key knowledge points for each subject.}
    \label{fig:key_point_pie}
    \vspace{-.2cm}
\end{figure}

\paragraph{Data Statistics} 

\begin{table}[!htbp]
% \setlength{\tabcolsep}{4pt}
\centering
 \resizebox{0.5\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Question Type} & \multicolumn{2}{c}{\textbf{Math}} & \multicolumn{2}{c}{\textbf{Physics}} & \multicolumn{2}{c}{\textbf{Overall}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
 & \textbf{MCQs} & \textbf{Q\&A} & \textbf{MCQs} & \textbf{Q\&A} & \textbf{MCQs} & \textbf{Q\&A} \\
\midrule
\textbf{Text\&Image} & 260 & 197 & 450 & 260 & 710 & 457 \\
\textbf{Text} & 500 & 319 & 2257 & 239 & 2757 & 558 \\
\midrule
\textbf{Total} & 760 & 516 & 2707 & 499 & 3467 & 1015 \\
\bottomrule
\end{tabular}}
\caption{Distribution of questions in MMSciBench by image presence, subject, and question type.}
\label{tab:data_statistics}
\end{table}


MMSciBench comprises \textbf{4,482} questions, distributed across modalities and question types, as shown in Table~\ref{tab:data_statistics}. The distribution of core knowledge areas for mathematics and physics is illustrated in Figure~\ref{fig:key_point_pie}.

\paragraph{Taxonomy} 
The taxonomy used in MMSciBench has three levels: \textit{Domain}, \textit{Module}, and \textit{Chapter}:
\begin{itemize}
    \item \textbf{Domain}: Core subject areas that define fundamental knowledge boundaries. Mathematics domains include ``Sets'' and ``Functions'', while physics encompasses ``Classical Mechanics'', ``Electrodynamics'', and ``Quantum Mechanics''. \textit{Domains} group related topics under a common framework.
    
    \item \textbf{Module}: Subdivisions within \textit{Domains} that focus on key themes or methods. Examples include ``Probability and Statistics'' in mathematics and ``Mechanical Motion and Physical Models'' in physics. \textit{Modules} scaffold learning by clustering related topics.
    
    \item \textbf{Chapter}: The most detailed level, covering specific topics within a \textit{Module}. For instance, mathematics \textit{Chapters} under ``Functions'' include ``Exponential Functions'' and ``Trigonometric Functions'', while physics \textit{Chapters} under ``Interactions and Laws of Motion'' include ``Hooke's Law'' and ``Equilibrium Conditions of Concurrent Forces''. \textit{Chapters} enable fine-grained content analysis and annotation.
\end{itemize}

\section{Experiment Settings}
\subsection{Evaluated Models}

We evaluated our benchmark using four state-of-the-art LVLMs: GPT-4o, Claude 3.5 Sonnet \cite{anthropic2024claude}, Gemini 1.5 Pro 002 \cite{team2024gemini}, and Llama-3.2-90B-Vision-Instruct.


In addition, as there are models specifically designed for mathematical problem-solving, we extend our evaluation to include two math-focused LLMs: Qwen2.5-Math-72B-Instruct \cite{yang2024qwen25mathtechnicalreportmathematical} and DeepSeekMath-7B-Instruct \cite{shao2024deepseekmathpushinglimitsmathematical}. Additionally, we assessed two specialized mathematical LLMs—Qwen2.5-Math-72B-Instruct \cite{yang2024qwen25mathtechnicalreportmathematical} and DeepSeekMath-7B-Instruct \cite{shao2024deepseekmathpushinglimitsmathematical}—on the text-only mathematics subset. For reproducibility, all evaluations used a fixed sampling temperature of 0.


\subsection{Evaluation Criteria}

To evaluate the models, we use accuracy as the metric, a widely adopted standard in existing research, for all question types in MMSciBench. Our evaluation focuses solely on whether the final answer is correct, without considering intermediate solution steps. This criterion is naturally suited for MCQ evaluation, as grading is based on the selected choice(s) in practice. For Q\&A questions, this approach ensures a fair and objective comparison by emphasizing the correctness of the final answer rather than incorporating subjective human-defined grading that accounts for intermediate steps.

The evaluation workflow involves first generating answers for MMSciBench questions using each model. GPT-4o is then employed to assess answer correctness by comparing the models’ final outputs with the dataset’s standard solutions. In existing studies, MCQs often require models to adhere to a specified output format, imposed through prompts, with regular expression rules used to extract the selected choice(s). However, during our experiments, we observed that some models struggled to consistently follow these formatting instructions, complicating this approach. In fact, none of the models achieved a 100\% compliance rate with the formatting guidelines.
To ensure the evaluation focuses on the models’ scientific knowledge and reasoning abilities, rather than being influenced by format compliance issues, we employ GPT-4o to judge whether the final answers are equivalent.

\subsection{Prompt Design}

\begin{figure}[t!]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/prompt_chn.pdf}
    \end{adjustbox}
    \caption{The prompt template designed for requesting models to answer questions in Chinese, where the <Question> is sourced from MMSciBench.}
    \label{fig:prompt_chn}
\end{figure}

We use prompts customized for different question types to evaluate the models in a zero-shot setting. For each question type, we apply the same specific prompt template across all models, avoiding model-specific prompt engineering that might explicitly guide reasoning or impose tailored requirements. The prompt template is illustrated in Fig. \ref{fig:prompt_chn}. To assess the models’ intrinsic scientific abilities, the prompts used in the evaluation do not include additional key knowledge points or supplementary information from the dataset, although such information could be incorporated in future research for other purposes.
Since the dataset is in Chinese, we instruct the models to provide their answers in Chinese to ensure consistency with the dataset’s language.

For the LLM-as-a-judge evaluation \cite{gu2024survey,chen2024mllm,raju2024constructing}, we sample 180 instances of evaluated data and iteratively refined the judging prompts by manually verifying the accuracy of the judgments. This refinement process resulted in a judgment accuracy of 97.22\%. Detailed prompts are provided in Sec. \ref{app:prompts} in the appendix.


\section{Results}

\subsection{Model Performance}

\begin{table}[t!]
\centering
 \resizebox{0.5\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Models} & \textbf{Math} & \textbf{Physics} & \textbf{Overall} \\
\midrule
\textbf{Llama-3.2-90B-Vision-Instruct} & 16.69\% & 36.96\% & 31.19\% \\
\textbf{Gemini 1.5 Pro 002} & 56.74\% & 66.56\% & 63.77\% \\
\textbf{Claude 3.5 Sonnet} & 37.38\% & 60.54\% & 53.95\% \\
\textbf{GPT-4o} & 35.97\% & 56.89\% & 50.94\% \\
\midrule
\textbf{Qwen2.5-Math-72B-Instruct} & 57.39\%$^{*}$ & -- & -- \\
\textbf{DeepSeekMath-7B-Instruct} & 21.86\%$^{*}$ & -- & -- \\
\bottomrule
\end{tabular}}
\caption{Accuracies of models across different subjects. Values marked with $^{*}$ indicate accuracies reported only on text-only questions, as the corresponding models are not multimodal.}
    \vspace{-.1cm}
    \vspace{-.1cm}
\label{tab:overall_acc}
\end{table}


\paragraph{Overall and Subject-wise Performance}
Table \ref{tab:overall_acc} presents the overall and subject-specific accuracies of the four LVLMs on the full MMSciBench dataset, along with the accuracies of the two math-specific LLMs on the text-only math subset. Gemini 1.5 Pro 002 achieves the highest overall accuracy at \textbf{63.77\%}, significantly outperforming the other LVLMs in the evaluation. It consistently surpasses all competitors across each of the examined subjects, highlighting the substantial challenge posed by the benchmark, even for the most advanced LVLMs.
Among the remaining LVLMs, Claude 3.5 Sonnet ranks second overall with an accuracy of \textbf{53.95\%}, outperforming GPT-4o (\textbf{50.94\%}) specifically in physics. In contrast, Llama-3.2-90B-Vision-Instruct lags far behind, recording the lowest overall accuracy of \textbf{31.19\%}.
For the two math-specific LLMs, Qwen2.5-Math-72B-Instruct demonstrates notable performance with an accuracy of \textbf{57.39\%} on text-only math questions, while DeepSeekMath-7B-Instruct significantly underperforms, achieving only \textbf{21.86\%}. This discrepancy is expected, given the difference in model sizes.
Another noteworthy observation is the variation in performance across subjects, with models consistently performing better in physics. This finding will be analyzed further in Sec. \ref{sec:visual_understanding}.



\begin{table*}[!htbp]
% \setlength{\tabcolsep}{2pt}
\centering
 \resizebox{\textwidth}{!}{
\begin{tabular}{l cc cc cc}
\toprule
\multirow{2}{*}{\textbf{Models}} & 
\multicolumn{2}{c}{\textbf{Math}} &
\multicolumn{2}{c}{\textbf{Physics}} & 
\multicolumn{2}{c}{\textbf{Overall}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
 & \textbf{MCQs} & \textbf{Q\&A} & \textbf{MCQs} & \textbf{Q\&A} & \textbf{MCQs} & \textbf{Q\&A} \\
\midrule
\textbf{Llama-3.2-90B-Vision-Instruct} & 25.39\% & 3.88\% & 41.49\% & 12.42\% & 37.96\% & 8.08\% \\
& \underline{1.52\%} &  & \underline{21.48\%} &  & \underline{17.1\%} &  \\[5pt]
    
\textbf{Gemini 1.5 Pro 002} & 63.16\% & 47.29\% & 70.41\% & 45.69\% & 68.82\% & 46.50\% \\
& \underline{39.29\%} &  & \underline{50.40\%} &  & \underline{47.96\%} &  \\[5pt]
    
\textbf{Claude 3.5 Sonnet} & 48.03\% & 21.71\% & 65.35\% & 34.47\% & 61.55\% & 27.98\% \\
& \underline{24.16\%} &  & \underline{45.34\%} &  & \underline{40.69\%} &  \\[5pt]
    
\textbf{GPT-4o} & 44.47\% & 23.45\% & 61.17\% & 33.67\% & 57.51\% & 28.47\% \\
& \underline{20.60\%} &  & \underline{41.16\%} &  & \underline{36.65\%} &  \\[5pt]
    
\midrule
\textbf{Qwen2.5-Math-72B-Instruct} & 66.80\%$^{*}$ & 42.63\%$^{*}$ & -- & -- & -- & -- \\
& \underline{41.80\%$^{*}$} &  &  &  &  &  \\[5pt]
    
\textbf{DeepSeekMath-7B-Instruct} & 32.40\%$^{*}$ & 5.33\%$^{*}$ & -- & -- & -- & -- \\
& \underline{7.40\%$^{*}$} &  &  &  &  &  \\[5pt]
    
\midrule
% \textbf{Theoretical Random Baseline} & 23.87\% & 0 & 20.01\% & 0 & 20.86\% & 0 \\
\multirow{2}{*}{\textbf{Theoretical Random Baseline}} & 23.87\% & 0 & 20.01\% & 0 & 20.86\% & 0 \\
 & 25.00\%$^{*}$ & 0$^{*}$ & -- & -- & -- & -- \\
\bottomrule
\end{tabular}}
\caption{Accuracies of models across different question types, with \underline{underscored values} indicating the accuracy improvement over the theoretical accuracy of random guess for MCQs. Values marked with $^{*}$ indicate accuracies on text-only math subsets for specialized math models.}
\label{tab:acc_by_question_type}
\end{table*}


\paragraph{Performance on Different Questions Types}
Table \ref{tab:acc_by_question_type} reflects the performance of models on MCQs and Q\&A questions in different subjects and the whole dataset, as well as the theoretical random-guess baselines. 
The random-guess baselines of MCQs are calculated based on the approximation that all MCQs in MMSciBench are 4-choice questions, as over 99\% of MCQs in MMSciBench have 4 choices (see Table \ref{tab:choice_distribution} in the appendix for detailed statistics).
For single-choice questions, the random-guess accuracy is 1/4, as only one option is correct. 
For multiple-choice questions, where valid subsets include combinations of more than one choice, the random-guess accuracy is $1 / (C_4^2 + C_4^3 + C_4^4) = 1/11$. For indeterminate-choice questions, where any non-empty subset of choices is valid, the random-guess accuracy is $1 / 2^4 = 1/16$. These probabilities were weighted to compute random-guess baselines of MCQs.

While the raw accuracies suggest that models generally perform better on MCQs than on Q\&A questions, subtracting the baseline accuracies from their MCQ results reveals smaller yet positive gaps. This indicates that the provided answer choices in MCQs may assist the models by narrowing the possible answer space, making these questions easier to answer correctly compared to Q\&A questions. Interestingly, this pattern does not hold true for math, where the MCQ advantage disappears after accounting for the baseline. In fact, some models seem to struggle more with MCQs than with Q\&A questions in this subject.
This suggests that the provided choices in math MCQs might mislead the models, making these questions more challenging.

\subsection{Key Knowledge Point-Based Analysis}




To better understand where different models excel or struggle within scientific domains—and to identify inherently challenging key knowledge points—all models’ performances were analyzed across the taxonomy of first- and second-level key knowledge points, i.e., \textit{Domain} and \textit{Module} levels (see Fig. \ref{fig:combined_accuracy_by_category}). This analysis reveals that, while models generally maintain consistent relative rankings across entire subjects, their strengths can vary significantly at the subfield level.
For instance, although Gemini 1.5 Pro 002 often leads overall, it falls behind Claude 3.5 Sonnet and GPT-4o in the subfield of ``Electrodynamics - Magnetic Field''. Additionally, certain subfields prove universally challenging, e.g., ``Electrodynamics - Electromagnetic Induction and Its Applications'' in physics, as well as ``Geometry and Algebra – Geometry and Algebra'' and ``Functions – Preliminary Knowledge'' in mathematics.
These findings highlight both the nuanced capabilities and the current limitations of state-of-the-art models in addressing scientific knowledge.


\begin{figure*}[t!]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/combined_accuracy_by_category.pdf}
    \end{adjustbox}
    \caption{Accuracies of models across different key knowledge points.}
    \label{fig:combined_accuracy_by_category}
\end{figure*}
\subsection{Visual Understanding}
\label{sec:visual_understanding}


\begin{table*}[t!]
% \setlength{\tabcolsep}{3pt}
\centering
 \resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Models} & \multicolumn{2}{c}{\textbf{Math}} & \multicolumn{2}{c}{\textbf{Physics}} & \multicolumn{2}{c}{\textbf{Overall}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & \textbf{Text} & \textbf{T\&I} & \textbf{Text} & \textbf{T\&I} & \textbf{Text} & \textbf{T\&I} \\
\midrule
\textbf{Llama-3.2-90B-Vision-Instruct} & 19.54\% & 11.60\% & 42.83\% & 16.34\% & 37.07\% & 14.48\% \\
\textbf{Gemini 1.5 Pro 002} & 69.60\% & 33.70\% & 74.40\% & 39.01\% & 73.21\% & 36.93\% \\
\textbf{Claude 3.5 Sonnet} & 44.57\% & 24.51\% & 67.75\% & 35.21\% & 62.02\% & 31.02\% \\
\textbf{GPT-4o} & 44.69\% & 20.35\% & 64.10\% & 31.55\% & 59.31\% & 27.16\% \\
\midrule
\textbf{Qwen2.5-Math-72B-Instruct} & 57.39\% & -- & -- & -- & -- & -- \\
\textbf{DeepSeekMath-7B-Instruct} & 21.86\% & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}}
\caption{Accuracies of models on text-only (\textbf{Text}) and text-image paired (\textbf{T\&I}) questions across different subjects.}
\label{tab:acc_w_wo_img}
\end{table*}


MMSciBench includes both text-only and text-image paired questions. To evaluate the impact of visual input, we assess models on both types of questions, as shown in Table \ref{tab:acc_w_wo_img}. Notably, all LVLMs perform worse on tasks involving both textual and visual elements compared to those relying solely on text. This highlights that bridging the gap between text comprehension and text-image co-reasoning remains a significant challenge for current LVLMs. Furthermore, the higher proportion of text-only questions in physics partially explains why models perform better on physics questions compared to math questions, as observed in Table \ref{tab:overall_acc}.

\subsection{The Effect of Chain-of-Thought in Reasoning}





To evaluate the full scientific potential of the models, we design a suite of prompts to instruct them to answer step-by-step in Chinese, as detailed in Sec. \ref{app:rompt_templates_effect_cot_reasoning} in the appendix. As shown in Table \ref{tab:acc_step_by_step}, step-by-step prompting improves the accuracies of Llama-3.2-90B-Vision-Instruct and DeepSeekMath-7B-Instruct compared to their results in Table \ref{tab:overall_acc}. However, the accuracy of Qwen2.5-Math-72B-Instruct decreases, while the performance of the other models remains unchanged.

This observation suggests that explicitly prompting certain models to use chain-of-thought reasoning can enhance their performance, and that different models exhibit varying degrees of alignment or readiness in this regard. Notably, Gemini 1.5 Pro 002, Claude 3.5 Sonnet, GPT-4o, and Qwen2.5-Math-72B-Instruct are more capable of generating effective reasoning steps without explicit prompting, whereas other models show more significant improvements when guided explicitly.

Considering that models typically have access to richer English training resources, we conducted additional experiments by prompting them to answer step-by-step in English to further explore their scientific capabilities. The corresponding prompts are detailed in Sec. \ref{app:rompt_templates_effect_cot_reasoning} of the appendix. As shown in Table \ref{tab:acc_step_by_step}, the results indicate that all models, except Gemini 1.5 Pro 002, benefit from this instruction. This underscores the effectiveness of explicit chain-of-thought prompting and its importance in accurately assessing models’ capabilities. The differing behavior of Gemini 1.5 Pro 002 may suggest that its performance relies on the compatibility between the language of the questions and the language of the answers.


\begin{table}[t!]
% \setlength{\tabcolsep}{2pt}
\centering
 \resizebox{0.5\textwidth}{!}{
\begin{tabular}{l lccc}
\toprule
& \textbf{Models} & \textbf{Math} & \textbf{Physics} & \textbf{Overall} \\
\midrule
\multirow{6}{*}{\begin{tabular}{@{}l@{}}\textbf{}\\\textbf{in Chinese}\\\textbf{}\end{tabular}} 
 & \textbf{Llama-3.2-90B-Vision-Instruct} & 19.12\% & 38.86\% & 33.24\% \\
 & \textbf{Gemini 1.5 Pro 002} & 56.90\% & 66.28\% & 63.61\% \\
 & \textbf{Claude 3.5 Sonnet} & 36.83\% & 61.42\% & 54.42\% \\
 & \textbf{GPT-4o} & 35.74\% & 56.86\% & 50.85\% \\
 & \textbf{Qwen2.5-Math-72B-Instruct} & 55.68\%$^{*}$ & -- & -- \\
 & \textbf{DeepSeekMath-7B-Instruct} & 23.32\%$^{*}$ & -- & -- \\[6pt]

\midrule

\multirow{6}{*}{\begin{tabular}{@{}l@{}}\textbf{}\\\textbf{in English}\\\textbf{}\end{tabular}} 
 & \textbf{Llama-3.2-90B-Vision-Instruct} & 22.41\% & 44.20\% & 38.00\% \\
 & \textbf{Gemini 1.5 Pro 002} & 55.17\% & 65.07\% & 62.25\% \\
 & \textbf{Claude 3.5 Sonnet} & 40.67\% & 61.26\% & 55.40\% \\
 & \textbf{GPT-4o} & 37.23\% & 59.08\% & 52.86\% \\
 & \textbf{Qwen2.5-Math-72B-Instruct} & 55.31\%$^{*}$ & -- & -- \\
 & \textbf{DeepSeekMath-7B-Instruct} & 23.69\%$^{*}$ & -- & -- \\

\bottomrule
\end{tabular}}
\caption{Accuracies of models asked to provide step-by-step answers in Chinese and English. Values marked with $^{*}$ indicate accuracies on text-only math questions for the corresponding specialized math models.}
\label{tab:acc_step_by_step}
\end{table}


\section{Related Work}

\paragraph{Scientific Benchmarks}
Scientific benchmarks are essential tools for evaluating the capabilities of language models in understanding and reasoning about complex scientific concepts, encompassing a wide range of disciplines, from general science to domain-specific areas like mathematics and physics. General scientific benchmarks, such as MSVEC \cite{evans2023msvec} and SciOL \cite{tarsi2024sciol}, have been developed to assess various aspects of language models’ abilities in specific scientific domains, including claim verification, figure retrieval, and multimodal information comprehension. However, the increasing complexity of language models necessitates more specialized benchmarks to evaluate their performance in specific scientific domains.

In mathematics, benchmarks like TRIGO \cite{xiong2023trigo}, DrawEduMath \cite{baral2025drawedumath}, and DMath \cite{kim2023ain} have been developed to assess AI models on targeted mathematical tasks. 
TRIGO focuses on formal mathematical proof reduction, evaluating models’ abilities to understand and manipulate complex mathematical expressions. DrawEduMath is designed to assess models’ proficiency in solving visual math problems, where both image and textual inputs are required to extract and process mathematical information. DMath, on the other hand, evaluates models on a diverse set of math word problems, testing their natural language understanding alongside mathematical reasoning.
Similarly, in physics, datasets such as GRASP \cite{jassim2023grasp} have been introduced to assess models’ understanding of ``Intuitive Physics'' principles, including object permanence and continuity.

Additionally, benchmarks like GAOKAO-Bench \cite{zhang2023evaluating}, GAOKAO-MM \cite{zong2024gaokao}, OlympiadBench \cite{he2024olympiadbench}, and SceMQA \cite{liang2024scemqa} span multiple scientific domains, including mathematics, physics, chemistry, and biology. These benchmarks focus on high-school, Olympiad, and pre-college levels, offering comprehensive evaluations of AI models’ scientific reasoning capabilities across key disciplines.


\paragraph{Benchmarks for LVLMs} 
Benchmarks for LVLMs have been developed to evaluate their performance across various tasks, including visual question answering, image captioning, and multimodal reasoning. These benchmarks typically consist of datasets with image-text pairs accompanied by corresponding questions or instructions, assessing the ability of LVLMs to generate accurate and relevant responses.
For example, the VALSE benchmark \cite{parcalabescu2021valse} focuses on evaluating the visio-linguistic grounding capabilities of pretrained VLMs on specific linguistic phenomena. Other benchmarks, such as VisIT-Bench \cite{bitton2023visit}, WinoGAViL \cite{bitton2022winogavil}, and those designed for zero-shot visual reasoning \cite{nagar2024zero, xu2024benchmarking}, are aimed at assessing the ability of LVLMs to reason about visual scenes and answer questions that require minimal world knowledge. These benchmarks often analyze the impact of conveying scene information either as visual embeddings or as purely textual scene descriptions to the underlying LLM of the LVLM.




To address the scarcity of scientific benchmarks specifically designed for the high school level—supporting both text-only and multimodal reasoning—we introduce MMSciBench. As detailed in Table \ref{tab:comparison}, this dataset achieves a balanced trade-off between size and comprehensiveness, enabling efficient evaluation while offering a diverse selection of challenging high-school-level scientific problems. Additionally, MMSciBench prioritizes quality, with a significant portion of problems including detailed solution explanations and a three-level taxonomy of key knowledge points, facilitating fine-grained analysis of AI model performance.

\section{Conclusion}

This paper introduces MMSciBench, a benchmark designed to evaluate the scientific capabilities of both unimodal and multimodal language models. MMSciBench consists of a collection of high school-level MCQs and Q\&A questions in mathematics and physics, with a subset of the questions incorporating images. The benchmark organizes its questions into a three-level taxonomy, ensuring comprehensive coverage of key knowledge points in both subjects.
Our evaluation of four advanced LVLMs and two specialized math LLMs on MMSciBench demonstrates that current models still have significant room for improvement in scientific problem-solving. The analysis highlights that the inclusion of visual elements in questions presents a substantial challenge for model performance, emphasizing the complexity of integrating textual and visual reasoning.
This work contributes to the ongoing development of robust benchmarks aimed at evaluating the evolving capabilities of language models, particularly in the domain of scientific reasoning.

\section*{Limitations}
Despite the advances presented in MMSciBench, several limitations warrant discussion and open avenues for future research.

\begin{enumerate}
\item \textbf{Domain and Content Scope:}
MMSciBench is focused on high-school level mathematics and physics, a scope chosen for its educational relevance and well-defined problem sets. However, this focus also limits the benchmark’s applicability to broader scientific domains. While the curated questions capture essential concepts, they do not encompass other fields such as chemistry, biology, or advanced scientific topics. Additionally, the dataset’s reliance on K–12 educational standards may introduce biases that do not reflect the diverse challenges encountered in higher-level or interdisciplinary scientific reasoning.

\item \textbf{Evaluation Metrics and Reasoning Transparency:}
The evaluation framework is centered on final answer accuracy, a metric that, while objective, does not capture the nuances of intermediate reasoning steps or the quality of explanations generated by models. By discounting partial correctness or the reasoning process, the assessment may obscure important differences in how models arrive at their answers. Future iterations of the benchmark may benefit from incorporating multi-faceted evaluation criteria that assess both the correctness of conclusions and the soundness of the reasoning process.

\item \textbf{Language and Cultural Considerations:}
MMSciBench is primarily composed in Chinese, with some experiments extended to English. Models predominantly trained on English data may therefore be disadvantaged, and cultural or linguistic biases could affect performance. Future work should consider expanding the benchmark to include a more balanced representation of languages and educational contexts.

\item \textbf{Dataset Size and Filtering Practices:}
While MMSciBench comprises \textbf{4,482} question–solution pairs, the dataset size is modest relative to some large-scale benchmarks. The strict filtering criteria (e.g., including only questions with a human-annotated hardness score $\ge$ 0.7) may also limit the diversity of problem difficulties, potentially excluding edge cases that could be valuable for assessing nuanced reasoning. Enlarging the dataset and diversifying the difficulty distribution would further strengthen the benchmark’s comprehensiveness.
\end{enumerate}




\bibliography{custom}

% \begin{CJK*}{UTF8}{gbsn}

\appendix

\clearpage




\section{Prompts}
\label{app:prompts}
In this section, we present the prompts used in our work.

\subsection{The Prompt for Question Categorization}
\label{app:prompt_for_question_categorization}
Fig. \ref{fig:prompt_classify} presents the prompt designed for categorizing MMSciBench questions into specific categories using GPT-4o. The category sets for each subject are derived from a Chinese high school key knowledge point taxonomy.

\begin{figure}[!htbp]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/prompt_classify.pdf}
    \end{adjustbox}
    \caption{The prompt template is designed to use GPT-4 as a classifier, categorizing each question into a three-level hierarchy. <Categories> represents the predefined set of categories for the target subject.}
    \label{fig:prompt_classify}
\end{figure}

\subsection{Prompt Templates for the Effect of Chain-of-Thought in Reasoning}
\label{app:rompt_templates_effect_cot_reasoning}
Fig. \ref{fig:prompt_chn_stp} and Fig. \ref{fig:prompt_eng_stp} are prompts templates that ask models to think step by step in Chinese and English, respectively. 

\begin{figure}[!htbp]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/prompt_chn_stp.pdf}
    \end{adjustbox}
    \caption{The prompt template is designed for requesting models to answer questions in Chinese step by step, where the <Question> is sourced from MMSciBench.}
    \label{fig:prompt_chn_stp}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/prompt_eng_stp.pdf}
    \end{adjustbox}
    \caption{The prompt template is designed for requesting models to answer questions in English step by step, where the <Question> is sourced from MMSciBench.}
    \label{fig:prompt_eng_stp}
\end{figure}

\subsection{The Prompt Template for Using GPT-4o as a Judge}
\label{app:prompt_gpt4o_as_judge}
Fig. \ref{fig:prompt_judge} (with its English translation in Fig. \ref{fig:prompt_judge_en}) illustrates the prompt used to instruct GPT-4o to evaluate whether a ``student solution''—that is, the model’s response being assessed—is correct or incorrect compared to the standard solution in MMSciBench.
For MCQs, only the model’s answer and the standard solution are provided, omitting the actual questions. This approach is sufficient because the evaluation solely involves comparing whether the selected choices match the standard answer, eliminating the need to understand the question’s context.
In contrast, for Q\&A questions, GPT-4o is provided with the question, the standard solution, and the model’s answer. This comprehensive context enables accurate semantic understanding and a thorough comparison between the two responses.
The prompt for Q\&A questions have been iteratively refined and enhanced to improve GPT-4o’s judgment, particularly in cases where misjudgments are likely. This refinement process involves sampling a subset of evaluated responses and manually diagnosing the reasons for any misjudgments, thereby continually improving the evaluation accuracy.

\begin{figure}[!htbp]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/prompt_judge.pdf}
    \end{adjustbox}
    \caption{The prompt template designed for using GPT-4o as a judge, where the <Question> and <Standard Solution> is sourced from MMSciBench, while <Student Solution> is the solution provided by the tested model.}
    \label{fig:prompt_judge}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{adjustbox}{center}
    \includegraphics[width=\linewidth]{images/prompt_judge_en.pdf}
    \end{adjustbox}
    \caption{The English translation of the prompt template shown in Fig. \ref{fig:prompt_judge}.}
    \label{fig:prompt_judge_en}
\end{figure}

\clearpage

\section{Data Examples}
\label{app:data_examples}
% In this section, we present examples from MMSciBench, including a math MCQ (Fig. \ref{fig:math_mcq}), a math Q\&A question (Fig. \ref{fig:math_qa}), and a physics Q\&A question (Fig. \ref{fig:physics_qa}). Each example is accompanied by its standard solution and explanation.
In this section, we present examples from MMSciBench, including a physics MCQ (Fig. \ref{fig:physics_mcq} and the corresponding English translation in Fig. \ref{fig:physics_mcq_en}), a physics Q\&A question (Fig. \ref{fig:physics_qa} and the corresponding English translation in Fig. \ref{fig:physics_qa_en}), a math MCQ (Fig. \ref{fig:math_mcq} and the corresponding English translation in Fig. \ref{fig:math_mcq_en}), and a math Q\&A question (Fig. \ref{fig:math_qa} and the corresponding English translation in Fig. \ref{fig:math_qa_en}). Each example is accompanied by its standard solution and explanation.

\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        问题（单选）：如图所示，两块同样的玻璃直角三棱镜$ABC$，两者的$AC$面是平行放置的，在它们之间是均匀的未知透明介质。一束单色细光$O$垂直于$AB$面入射，在图示的出射光线中（\quad）。 

        \includegraphics[width=\textwidth]{images/PhysicsMCQQuestion.png}
        
        选项：
        
        A. 1、2、3（彼此平行）中的任一条都有可能
        
        B. 4、5、6（彼此平行）中的任一条都有可能
        
        C. 7、8、9（彼此平行）中的任一条都有可能
        
        D. 只能是4、6中的某一条

        \textbf{Standard Solution}: B
    \end{tcolorbox}

    % \vspace{0.1em}
    
    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        本题主要考查三棱镜问题。
        
        选项分析：据题述，两个直角三棱镜之间的介质折射率未知，可能比玻璃大，可能与玻璃相同，也可能比玻璃小，可能的光路图如下：

        \includegraphics[width=\textwidth]{images/PhysicsMCQSol.png}
        
        故B项正确，ACD项错误。
        
        综上所述，本题正确答案为B。
    \end{tcolorbox}
    
    \caption{An example of a physics MCQ.}
    \label{fig:physics_mcq}
\end{figure}

\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        问题（多选）：下图是函数$y=\sin(\omega x+\varphi )$的部分图象，则$\sin(\omega x+\varphi)=$（  ）。

        \includegraphics[width=\textwidth]{images/MathMCQQuestion.png}
        
        选项：
        
        A. $\sin(x+{\pi \over 3})$ 
        
        B. $\sin({\pi \over 3}-2x)$
        
        C. $\cos(2x+{\pi \over 6})$ 
        
        D. $\cos({5\pi \over 6}-2x)$

        \textbf{Standard Solution}: B, C
    \end{tcolorbox}

    % \vspace{0.1em}
    
    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        本题主要考查三角函数。
        
        由题图可知，
        \[
        \displaystyle \frac{T}{2} = \frac{2}{3} \pi - \frac{\pi}{6} = \frac{\pi}{2},
        \]
        所以
        \[
        T = \frac{2\pi}{|\omega|} = \pi,
        \]
        所以$|\omega| = 2$。
        
        当$\omega = 2$时，由函数图象过点$\left(\frac{\pi}{6}, 0\right)$，$\left(\frac{2\pi}{3}, 0\right)$，
        且$f(0) > 0$，得
        \[
        \varphi = \frac{2\pi}{3} + 2k\pi \quad (k \in \mathbb{Z}),
        \]
        所以
        \[
        y = \sin\left(2x + \frac{2\pi}{3}\right) = -\cos\left(\frac{5\pi}{6} - 2x\right),
        \]
        同理，当$\omega = -2$时，
        \[
        \varphi = \frac{\pi}{3} + 2k\pi \quad (k \in \mathbb{Z}),
        \]
        所以
        \[
        y = \sin\left(-2x + \frac{\pi}{3}\right) = \cos\left(2x + \frac{\pi}{6}\right)
        \]
        
        故本题正确答案为 BC。
    \end{tcolorbox}
    
    \caption{An example of a math MCQ.}
    \label{fig:math_mcq}
\end{figure}


\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        Question (Multiple Choice): The figure below shows a part of the graph of the function \( y = \sin(\omega x + \varphi) \). Determine \( \sin(\omega x + \varphi) = \) ( ).

        \includegraphics[width=\textwidth]{images/MathMCQQuestion.png}
        
        Options:
        
        A. \( \sin\left(x + \frac{\pi}{3}\right) \) 
        
        B. \( \sin\left(\frac{\pi}{3} - 2x\right) \)
        
        C. \( \cos\left(2x + \frac{\pi}{6}\right) \) 
        
        D. \( \cos\left(\frac{5\pi}{6} - 2x\right) \)

        \textbf{Standard Solution}: B, C
    \end{tcolorbox}

    % \vspace{0.1em}
    
    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        This question primarily assesses trigonometric functions.
        
        From the figure, we know that
        \[
        \frac{T}{2} = \frac{2}{3}\pi - \frac{\pi}{6} = \frac{\pi}{2},
        \]
        therefore
        \[
        T = \frac{2\pi}{|\omega|} = \pi,
        \]
        so \( |\omega| = 2 \).
        
        When \( \omega = 2 \), since the graph passes through the points \( \left(\frac{\pi}{6}, 0\right) \) and \( \left(\frac{2\pi}{3}, 0\right) \),
        and \( f(0) > 0 \), we have
        \[
        \varphi = \frac{2\pi}{3} + 2k\pi \quad (k \in \mathbb{Z}),
        \]
        thus
        \[
        y = \sin\left(2x + \frac{2\pi}{3}\right) = -\cos\left(\frac{5\pi}{6} - 2x\right),
        \]
        similarly, when \( \omega = -2 \),
        \[
        \varphi = \frac{\pi}{3} + 2k\pi \quad (k \in \mathbb{Z}),
        \]
        so
        \[
        y = \sin\left(-2x + \frac{\pi}{3}\right) = \cos\left(2x + \frac{\pi}{6}\right)
        \]
        
        Therefore, the correct answer is BC.
    \end{tcolorbox}
    
    \caption{The English translation of the math MCQ example in Fig. \ref{fig:math_mcq}.}
    \label{fig:math_mcq_en}
\end{figure}










\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        问题（解答）：如图，建立平面直角坐标系$xOy$，$x$轴在地平面上，$y$轴垂直于地平面，单位长度为$1$千米。某炮位于坐标原点。已知炮弹发射后的轨迹在方程
        $$y=kx-\frac1{20}(1+k^2)x^2(k>0)$$
        表示的曲线上，其中$k$与发射方向有关。炮的射程是指炮弹落地点的横坐标。（1）求炮的最大射程；（2）设在第一象限有一飞行物（忽略其大小），其飞行高度为$3.2$千米，试问它的横坐标$a$不超过多少时，炮弹可以击中它？请说明理由。

        \includegraphics[width=\textwidth]{images/MathQAQuestion.jpg}

        \textbf{Standard Solution}

        （1）令$y=0$，得$kx-\frac1{20}(1+k^2)x^2=0$，由实际意义和题设条件知$x>0$，$k>0$，故
        $$x=\frac{20k}{1+k^2}=\frac{20}{k+\frac1k}\le\frac{20}2=10,$$
        当且仅当$k=1$时取等号。所以炮的最大射程为$10$千米。
        
        （2）因为$a>0$，所以炮弹可击中目标
        
        $\Leftrightarrow$存在$k>0$，使$3.2=ka-\frac1{20}(1+k^2)a^2$成立
        
        $\Leftrightarrow$关于$k$的方程$a^2k^2-20ak+a^2+64=0$有正根
        
        $\Leftrightarrow$判别式
        $$\Delta=(-20a)^2-4a^2(a^2+64)\ge0$$
        
        $\Leftrightarrow a\le6$
        
        此时，
        $$k=\frac{20a+\sqrt{(-20a)^2-4a^2(a^2+64)}}{2a^2}>0$$
        （不考虑另一根）。所以当$a$不超过$6$千米时，可击中目标。
    \end{tcolorbox}

    % \vspace{0.1em}
    
    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        本题主要考查函数与方程和基本不等式的应用等相关知识。（1）求炮的最大射程，即$y=0$时的一个较大的根，因为含有参数$k$，所以需根据$k$的取值范围确定另外一个根的最大值，即为炮的最大射程。（2）炮弹能击中目标的含义为炮弹的飞行高度$y=3.2$时有解。根据二次函数有正根，可得出$a$的取值范围。
    \end{tcolorbox}
    
    \caption{An example of a math Q\&A question.}
    \label{fig:math_qa}
\end{figure}


\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        Question (Q\&A): As shown in the figure, set up a Cartesian coordinate system $xOy$, with the $x$-axis on the ground, the $y$-axis perpendicular to the ground, and the unit length is 1 kilometer. A cannon is located at the origin. It is known that the trajectory of the cannonball after firing is represented by the equation
        $$y = kx - \frac{1}{20}(1 + k^2)x^2 (k > 0)$$
        where $k$ is related to the firing direction. The cannon's range refers to the x-coordinate of the landing point of the cannonball. (1) Find the maximum range of the cannon; (2) Suppose there is a flying object in the first quadrant (ignoring its size) with a flight height of 3.2 kilometers. What is the maximum x-coordinate $a$ such that the cannonball can hit it? Please explain your reasoning.

        \includegraphics[width=0.5\textwidth]{images/MathQAQuestion_en.jpg}

        \textbf{Standard Solution}

        (1) Set $y=0$, obtaining $kx - \frac{1}{20}(1 + k^2)x^2 = 0$. From the actual meaning and problem conditions, we know $x > 0$, $k > 0$, thus
        $$x = \frac{20k}{1 + k^2} = \frac{20}{k + \frac{1}{k}} \leq \frac{20}{2} = 10,$$
        equality holds if and only if $k=1$. Therefore, the maximum range of the cannon is 10 kilometers.
        
        (2) Because $a > 0$, the cannonball can hit the target 
        
        $\Leftrightarrow$ there exists $k > 0$ such that $3.2 = ka - \frac{1}{20}(1 + k^2)a^2$ holds
        
        $\Leftrightarrow$ the equation $a^2k^2 - 20ak + a^2 + 64 = 0$ in terms of $k$ has positive roots
        
        $\Leftrightarrow$ the discriminant
        $$\Delta = (-20a)^2 - 4a^2(a^2 + 64) \geq 0$$
        
        $\Leftrightarrow a \leq 6$
        
        At this time,
        $$k = \frac{20a + \sqrt{(-20a)^2 - 4a^2(a^2 + 64)}}{2a^2} > 0$$
        (Not considering the other root). Therefore, when $a$ does not exceed 6 kilometers, the target can be hit.
    \end{tcolorbox}

    % \vspace{0.1em}

    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        This question primarily tests the application of functions, equations, and basic inequalities. (1) To find the maximum range of the cannon, which is the larger root when $y=0$, because there is a parameter $k$, we need to determine the maximum value of the other root based on the range of $k$, which gives the cannon's maximum range. (2) The meaning of the cannonball being able to hit the target is that when the flight height $y=3.2$, there exists a solution. Based on the quadratic function having positive roots, we can derive the range of $a$.
    \end{tcolorbox}
    
    \caption{The English translation of the math Q\&A question example in Fig. \ref{fig:math_qa}.}
    \label{fig:math_qa_en}
\end{figure}









\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        问题（解答）：如图所示，在光滑的水平面上，质量$m=5kg$的物体，在水平拉力$F=10N$的作用下，从静止开始运动，运动时间$t=3s$。求：（1）力$F$在$3s$内对物体所做的功；（2）力$F$在$3s$内对物体做功的平均功率；（3）在$3s$末，力$F$对物体做功的瞬时功率。

        \includegraphics[width=\textwidth]{images/PhysicsQAQuestion.png}

        \textbf{Standard Solution}

        （1）由牛顿第二定律可得：$F=ma$，$3 s$内对物体的位移为$x={1\over 2} at^2$，则力$F$在$3 s$内对物体所做的功为$W=Fx$，联立可得：$W=90 J$。
        
        （2）力$F$在$3 s$内对物体做功的平均功率为$\overline{P}={W\over t} =30 W$。
        
        （3）在$3 s$末物体的速度大小为$v=at$，则在$3 s$末，力$F$对物体做功的瞬时功率为$P=Fv$，联立可得：$P=60 W$。
    \end{tcolorbox}

    % \vspace{0.1em}
    
    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        本题主要考查牛顿第二定律和功率公式的选择与计算。

        问题求解：
        
        （1）由牛顿第二定律可算出运动的加速度，便可求出$3s$内对物体的位移，便能算出力$F$在$3s$内对物体所做的功。
        
        （2）根据$\overline{P} = \frac{W}{t}$便可算出力$F$在$3s$内对物体做功的平均功率。
        
        （3）先算出在$3s$末物体的速度大小，根据$P = Fv$便可算出在$3s$末，力$F$对物体做功的瞬时功率。
    \end{tcolorbox}
    
    \caption{An example of a physics Q\&A question.}
    \label{fig:physics_qa}
\end{figure}


\begin{figure}[!htbp]
    \small
    \centering
    % Question Box
    \begin{tcolorbox}[colframe=magenta!50, colback=magenta!10, title=Question \& Standard Solution]
        \textbf{Question}

        Question (Q\&A): As shown in the figure, on a smooth horizontal plane, a mass \( m = 5kg \) object is acted upon by a horizontal force \( F = 10N \) and starts moving from rest. The motion time is \( t = 3s \). Find: (1) The work done by force \( F \) on the object within \( 3s \); (2) The average power of force \( F \) in doing work on the object within \( 3s \); (3) The instantaneous power of force \( F \) in doing work on the object at the end of \( 3s \).

        \includegraphics[width=\textwidth]{images/PhysicsQAQuestion.png}

        \textbf{Standard Solution}

        (1) From Newton's second law, \( F = ma \). The displacement of the object within \( 3s \) is \( x = \frac{1}{2} a t^2 \). Therefore, the work done by force \( F \) on the object within \( 3s \) is \( W = Fx \). Solving these equations yields \( W = 90J \).

        (2) The average power of force \( F \) in doing work on the object within \( 3s \) is \( \overline{P} = \frac{W}{t} = 30W \).

        (3) At the end of \( 3s \), the velocity of the object is \( v = at \). Therefore, the instantaneous power of force \( F \) in doing work on the object at the end of \( 3s \) is \( P = Fv \). Solving these equations yields \( P = 60W \).
    \end{tcolorbox}

    % \vspace{0.1em}

    % Solution Box
    \begin{tcolorbox}[colframe=cyan!50, colback=cyan!10, title=Explanation]
        This problem primarily tests the application and calculation of Newton's second law and power formulas.

        Problem Solving:

        (1) Using Newton's second law, the acceleration of the motion can be calculated, which allows us to find the displacement of the object within \( 3\,\text{s} \). This displacement can then be used to calculate the work done by force \( F \) on the object within \( 3s \).

        (2) Using \( \overline{P} = \frac{W}{t} \), the average power of force \( F \) in doing work on the object within \( 3s \) can be calculated.

        (3) First, calculate the velocity of the object at the end of \( 3s \). Then, using \( P = Fv \), the instantaneous power of force \( F \) in doing work on the object at the end of \( 3s \) can be calculated.
    \end{tcolorbox}
    
    \caption{The English translation of the physics Q\&A question example in Fig. \ref{fig:physics_qa}.}
    \label{fig:physics_qa_en}
\end{figure}

\section{The Distribution of Choices of MCQs}
Table \ref{tab:choice_distribution} shows that over 99\% of MCQs in MMSciBench have 4 choices

\begin{table}[!htbp]
% \setlength{\tabcolsep}{4pt}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Subject} & \textbf{Image} & \textbf{4 Choices} & \textbf{Other} & \textbf{Total} \\
\midrule
Physics & \xmark & 2230 & 27 & 2257 \\
Physics & \cmark & 448 & 2 & 450 \\
Math & \xmark & 500 & 0 & 500 \\
Math & \cmark & 260 & 0 & 260 \\
\midrule
\textbf{Total} & & 3438 & 29 & 3467 \\
\bottomrule
\end{tabular}
\caption{Distribution of choice numbers in MCQs in MMSciBench by subject and image presence.}
\label{tab:choice_distribution}
\end{table}

% \section{Taxonomy}
% The three-level taxonomies used in MMSciBench are shown in Table \ref{tab:math_taxonomy} and Table \ref{tab:physics_taxonomy}.


% % Switch to one-column mode
% \onecolumn

% \begin{longtable}{p{5cm} p{5cm} p{5cm}}
% \caption{The three-level mathematics taxonomy used in MMSciBench, comprising Domain, Module, and Chapter.}
% \label{tab:math_taxonomy} \\
% \toprule
% \textbf{Domain} & \textbf{Module} & \textbf{Chapter} \\
% \midrule
% \endfirsthead

% % \toprule
% % \textbf{Domain} & \textbf{Module} & \textbf{Chapter} \\
% % \midrule
% \endhead

% \midrule
% \multicolumn{3}{r}{\textit{Continued on next page}} \\
% \endfoot

% \bottomrule
% \endlastfoot

% \textbf{Sets} & Prerequisite Knowledge & Sets \\
% \addlinespace

% \textbf{Logic and Reasoning} & Prerequisite Knowledge & Common Logical Terms \\
% \addlinespace

% \textbf{Logic and Reasoning} & Prerequisite Knowledge & Equality and Inequality Relations \\
% \addlinespace

% \textbf{Logic and Reasoning} & Logical Reasoning & Logical Reasoning \\
% \addlinespace

% \textbf{Functions} & Prerequisite Knowledge & Quadratic Equations and Inequalities from the Perspective of Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Concepts and Properties of Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Power Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Exponential Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Logarithmic Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Trigonometric Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Applications of Functions \\
% \addlinespace

% \textbf{Functions} & Functions & Sequences \\
% \addlinespace

% \textbf{Functions} & Functions & Derivatives of Single-Variable Functions and Applications \\
% \addlinespace

% \textbf{Geometry and Algebra} & Geometry and Algebra & Plane Vectors and Applications \\
% \addlinespace

% \textbf{Geometry and Algebra} & Geometry and Algebra & Complex Numbers \\
% \addlinespace

% \textbf{Geometry and Algebra} & Geometry and Algebra & Solid Geometry \\
% \addlinespace

% \textbf{Geometry and Algebra} & Geometry and Algebra & Plane Analytic Geometry \\
% \addlinespace

% \textbf{Geometry and Algebra} & Geometry and Algebra & Space Vectors and Solid Geometry \\
% \addlinespace

% \textbf{Probability and Statistics} & Probability and Statistics & Probability \\
% \addlinespace

% \textbf{Probability and Statistics} & Probability and Statistics & Statistics \\
% \addlinespace

% \textbf{Probability and Statistics} & Probability and Statistics & Counting Principles \\
% \addlinespace

% \textbf{Calculus} & Calculus & Calculus \\
% \addlinespace

% \textbf{Applied Mathematical Modeling and Mathematical Inquiry Activities} & Mathematical Modeling and Mathematical Inquiry Activities & Social Investigation and Data Analysis \\
% \addlinespace

% \textbf{Applied Mathematical Modeling and Mathematical Inquiry Activities} & Mathematical Modeling and Mathematical Inquiry Activities & Mathematical Models \\
% \addlinespace

% \end{longtable}

% % Switch back to two-column mode
% \twocolumn







% % Switch to one-column mode
% \onecolumn

% \begin{longtable}{p{5cm} p{5cm} p{5cm}}
% \caption{The three-level physics taxonomy used in MMSciBench, comprising Domain, Module, and Chapter.}
% \toprule
% \textbf{Domain} & \textbf{Module} & \textbf{Chapter} \\
% \midrule
% \endfirsthead

% \toprule
% \textbf{Domain} & \textbf{Module} & \textbf{Chapter} \\
% \midrule
% \endhead

% \midrule
% \multicolumn{3}{r}{\textit{Continued on next page}} \\
% \endfoot

% \bottomrule
% \endlastfoot

% \textbf{Classical Mechanics} & Mechanical Motion and Physical Models & Modern Experimental Science \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Motion and Physical Models & Particle Models \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Motion and Physical Models & Displacement, Velocity, and Acceleration \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Gravity, Elastic Force, and Friction \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Hooke's Law \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Sliding and Static Friction \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Composition and Decomposition of Forces \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Vectors and Scalars \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Equilibrium Conditions of Concurrent Forces \\
% \addlinespace
% \textbf{Classical Mechanics} & Interactions and Laws of Motion & Understanding Mechanical Units in the International System of Units \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Energy and Conservation Laws & Work and Power \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Energy and Conservation Laws & Kinetic Energy and the Work-Energy Theorem \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Energy and Conservation Laws & Gravitational Potential Energy \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Energy and Conservation Laws & Proving the Conservation of Mechanical Energy \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Curvilinear Motion \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Projectile Motion \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Describing Uniform Circular Motion with Linear Velocity, Angular Velocity, and Period \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Analyzing Centripetal Force in Uniform Circular Motion using Newton's Second Law \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Understanding the Relationship between the Magnitude of Centripetal Force, Radius, Angular Velocity, and Mass in Uniform Circular Motion \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Universal Law of Gravitation \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Calculating the Orbital Velocity of Artificial Earth Satellites \\
% \addlinespace
% \textbf{Classical Mechanics} & Curvilinear Motion and Universal Gravitation & Second and Third Cosmic Velocities \\
% \addlinespace
% \textbf{Classical Mechanics} & Limitations of Newtonian Mechanics and Introduction to Relativity & Limitations of Newtonian Mechanics \\
% \addlinespace
% \textbf{Classical Mechanics} & Limitations of Newtonian Mechanics and Introduction to Relativity & Relativistic Concepts of Spacetime \\
% \addlinespace
% \textbf{Classical Mechanics} & Momentum and Conservation of Momentum & Impulse and Momentum \\
% \addlinespace
% \textbf{Classical Mechanics} & Momentum and Conservation of Momentum & Elastic and Inelastic Collisions \\
% \addlinespace
% \textbf{Classical Mechanics} & Momentum and Conservation of Momentum & Analyzing Physical Problems with Conservation Laws \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Vibrations and Waves & Simple Harmonic Motion \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Vibrations and Waves & Quantitative Relationship between the Period and Length of a Simple Pendulum \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Vibrations and Waves & Forced Vibrations and Resonance \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Vibrations and Waves & Characteristics of Waves \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Vibrations and Waves & Reflection and Refraction of Waves \\
% \addlinespace
% \textbf{Classical Mechanics} & Mechanical Vibrations and Waves & Doppler Effect \\
% \addlinespace

% \textbf{Electrodynamics} & Electrostatic Field & Electrostatic Phenomena \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Atomic Structure Models and Charge Conservation \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Point Charge Models \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Rules for Interaction Between Two Point Charges \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Coulomb's Law \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Electric Field as a Form of Matter \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Electric Field Strength \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Charges in an Electric Field Have Electric Potential Energy \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Solving Electric Potential Energy, Electric Potential, and Potential Difference \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Potential Difference and Electric Field Strength in a Uniform Electric Field \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Motion of Charged Particles in an Electric Field \\
% \addlinespace
% \textbf{Electrodynamics} & Electrostatic Field & Capacitors \\
% \addlinespace
% \textbf{Electrodynamics} & Circuits and Their Applications & Circuit Components \\
% \addlinespace
% \textbf{Electrodynamics} & Circuits and Their Applications & Quantitative Relationship of Metal Conductor Resistance with Material, Length, and Cross-Sectional Area \\
% \addlinespace
% \textbf{Electrodynamics} & Circuits and Their Applications & Ohm's Law in Closed Circuits \\
% \addlinespace
% \textbf{Electrodynamics} & Circuits and Their Applications & Electric Work, Power, and Joule's Law \\
% \addlinespace
% \textbf{Electrodynamics} & Circuits and Their Applications & Simple Problems in Household Circuits \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & Magnetic Phenomena \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & Magnetic Field \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & Magnetic Induction Strength \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & Magnetic Flux \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & Electromagnetic Waves \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & Light as an Electromagnetic Wave \\
% \addlinespace
% \textbf{Electrodynamics} & Preliminary Electromagnetic Field and Electromagnetic Waves & The Energy of Light is Discrete; Introduction to the Quantum Nature of the Microscopic World, using Hydropower, Wind Power, Solar Power, and Nuclear Power \\
% \addlinespace
% \textbf{Electrodynamics} & Magnetic Field & Ampère's Force \\
% \addlinespace
% \textbf{Electrodynamics} & Magnetic Field & Lorentz Force \\
% \addlinespace
% \textbf{Electrodynamics} & Magnetic Field & Using Lorentz Force to Analyze the Circular Motion of Charged Particles in a Uniform Magnetic Field; Understanding the Deflection and Applications of Charged Particles in a Uniform Magnetic Field \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Induction and Its Applications & Direction of Induced Current \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Induction and Its Applications & Faraday's Law of Electromagnetic Induction \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Induction and Its Applications & Self-Induction Phenomena and Eddy Current Phenomena \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Induction and Its Applications & Alternating Current \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Induction and Its Applications & Relationship between Voltage and Number of Turns in Primary and Secondary Coils of Transformers \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Induction and Its Applications & Energy Conversion in the Operation of Generators and Electric Motors \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Oscillations and Electromagnetic Waves & Maxwell's Electromagnetic Field Theory \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Oscillations and Electromagnetic Waves & Electromagnetic Oscillations \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Oscillations and Electromagnetic Waves & Emission, Propagation, and Reception of Electromagnetic Waves \\
% \addlinespace
% \textbf{Electrodynamics} & Electromagnetic Oscillations and Electromagnetic Waves & Electromagnetic Spectrum \\
% \addlinespace
% \textbf{Electrodynamics} & Sensors & Converting Non-Electrical Quantities into Electrical Quantities \\
% \addlinespace
% \textbf{Electrodynamics} & Sensors & Working Principles of Sensors \\
% \addlinespace

% \textbf{Quantum Mechanics} & Light and Its Applications & Law of Refraction of Light \\
% \addlinespace
% \textbf{Quantum Mechanics} & Light and Its Applications & Interference, Diffraction, and Polarization of Light \\
% \addlinespace
% \textbf{Quantum Mechanics} & Light and Its Applications & Lasers \\
% \addlinespace
% \textbf{Quantum Mechanics} & Atoms and Atomic Nuclei & Atoms and Their Structure \\
% \addlinespace
% \textbf{Quantum Mechanics} & Atoms and Atomic Nuclei & Composition of Atomic Nuclei and the Nature of Nuclear Forces \\
% \addlinespace
% \textbf{Quantum Mechanics} & Atoms and Atomic Nuclei & Radioactivity and Nuclear Decay \\
% \addlinespace
% \textbf{Quantum Mechanics} & Atoms and Atomic Nuclei & Binding Energy of Atomic Nuclei; Understanding Nuclear Fission and Nuclear Fusion Reactions \\
% \addlinespace
% \textbf{Quantum Mechanics} & Wave-Particle Duality & Photoelectric Effect Phenomena \\
% \addlinespace
% \textbf{Quantum Mechanics} & Wave-Particle Duality & Einstein's Photoelectric Equation and Its Significance \\
% \addlinespace
% \textbf{Quantum Mechanics} & Wave-Particle Duality & Physical Particles Have Wave Properties \\
% \addlinespace
% \textbf{Quantum Mechanics} & Wave-Particle Duality & Quantum Nature of the Microscopic World \\
% \addlinespace

% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Molecular Kinetic Theory \\
% \addlinespace
% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Diffusion Phenomena \\
% \addlinespace
% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Brownian Motion \\
% \addlinespace
% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Microscopic Structure of Solids \\
% \addlinespace
% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Materials Science \\
% \addlinespace
% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Surface Tension of Liquids \\
% \addlinespace
% \textbf{Thermodynamics} & Solids, Liquids, and Gases & Gas Experimental Laws \\
% \addlinespace
% \textbf{Thermodynamics} & Laws of Thermodynamics & First Law of Thermodynamics \\
% \addlinespace
% \textbf{Thermodynamics} & Laws of Thermodynamics & Law of Conservation of Energy \\
% \addlinespace
% \textbf{Thermodynamics} & Laws of Thermodynamics & Second Law of Thermodynamics \\
% \addlinespace

% \textbf{Applied Physics} & Energy and Sustainable Development & Hydropower, Wind Energy, Solar Energy, and Nuclear Energy \\
% \addlinespace
% \textbf{Applied Physics} & Energy and Sustainable Development & Nuclear Fission and Nuclear Fusion \\
% \addlinespace
% \textbf{Applied Physics} & Energy and Sustainable Development & Different Forms of Energy Can Be Converted into Each Other \\
% \addlinespace
% \textbf{Applied Physics} & Energy and Sustainable Development & Energy Conversion is Directional \\
% \addlinespace
% \textbf{Applied Physics} & Energy and Sustainable Development & Renewable and Non-Renewable Energy Sources \\
% \addlinespace
% \textbf{Applied Physics} & Energy and Sustainable Development & Hazards of Environmental Pollution \\

% \end{longtable}
% % \caption{Physics taxonomy structured using Domain, Module, and Chapter.}
% \label{tab:physics_taxonomy}
% % Switch back to two-column mode
% \twocolumn









\end{CJK*}


\end{document}
