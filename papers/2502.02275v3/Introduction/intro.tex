\section{Introduction}

The Wasserstein distance is acclaimed for its geometric relevance in comparing probability distributions. Having gathered a lot of theoretical work~\citep{santambrogio2015optimal,villani2008optimal}, it has also proved to be relevant in numerous applied domains in the last fifteen years, such as image comparison~\citep{rabin2009statistical}, image registration~\citep{feydy2017optimal}, domain adaptation~\citep{courty2016optimal}, generative modeling~\citep{arjovsky2017wasserstein,gulrajani2017improved,salimans2018improving}, inverse problems in imaging~\citep{hertrich2022wasserstein} or topological data analysis \citep{edelsbrunner09,ensembleBenchmark}, to name just a few. The computational demands of the Wasserstein distance are, however, quite high, since evaluating the distance between two discrete distributions of $N$ samples with traditional linear programming methods incurs a runtime complexity of \(O(N^3 \log N)\)~\citep{peyre2019computational}. This computational burden has motivated the development of alternative metrics sharing some of the {desirable} properties of the Wasserstein distance but with reduced complexity.

The Sliced Wasserstein (SW) 
distance~\citep{Rabin_texture_mixing_sw,bonneel2015sliced}, defined by slicing 
the Wasserstein distance along all possible directions on the hypersphere, is 
one of these efficient alternatives. Indeed, 
% the Sliced Wasserstein (SW) 
{the SW}
distance maintains the core properties of the 
Wasserstein distance but with reduced computational overhead. For compactly 
supported measures, Bonnotte~\citep{bonnotte2013phd} showed for instance that 
the two distances are equivalent. Again, it has been successfully applied in 
various domains, such as domain adaptation~\citep{lee2019sliced}, texture 
synthesis and style transfer~\citep{heitz2021sliced,Elnekave:2022aa}, generative 
modeling~\citep{deshpande2018generative,wu2019sliced}, regularizing 
autoencoders~\citep{kolouri2018sliced}, shape 
matching~\citep{le2024integrating}, and has even been adapted on Riemaniann 
manifolds~\citep{bonet2024sliced}.

The SW distance between two measures $\mu$ and $\nu$ can be written as the expectation of the one dimensional Wasserstein distance between the projections of $\mu$ and $\nu$ on a line whose direction is drawn uniformly on the hypersphere. It benefits from the simplicity of the Wasserstein distance computation in one dimension. In practice, computing the expectation on the hypersphere is unfeasible, so it is estimated thanks to numerical integration. The most common method for approximating the SW distance is to rely on Monte Carlo approximation, by sampling $M$ random directions uniformly on the hypersphere and approximating the integral by an average on these directions. Since the Wasserstein distance in 1D between two measures of $N$ samples can be computed in \(O(N \log N)\), computing this empirical version of Sliced Wasserstein has a runtime complexity of \(O(M N \log N)\). This complexity makes it a compelling alternative to the Wasserstein distance, especially when the number $N$ of samples is high.


As a Monte Carlo approximation, the law of large numbers ensures that this 
empirical Sliced Wasserstein distance converges to the true expectation, with a 
convergence rate of \(O(\frac{1} {\sqrt{M}})\). This convergence speed is slow 
but independent of the space dimension. However,  it is important to keep in 
mind that to preserve some of the properties of the 
% Sliced Wasserstein (SW) 
{SW}
distance, the number $M$ of directions should increase 
with the dimension. For instance,  it has been shown that for the empirical 
distance to almost surely separate discrete distributions (in the sense that if 
the distance between two distributions is zero then the two distributions are 
almost surely equal), the number of directions $M$ must be chosen strictly 
larger than the space dimension~\citep{tanguy2023reconstructing}. 

Classical Monte Carlo with independent samples is not always optimal, since 
independent random samples do not cover the space efficiently, creating clusters 
of points and leaving holes between these clusters. In very low dimension, 
quadrature rules provide efficient alternative methods to classical Monte Carlo. 
On the circle for instance, the simplest solution is to replace the $M$ random 
samples by the roots of unity $\{e^{i \frac{2k\pi}{M}}\ |\ \;0\le k \le M-1\}$: 
 since the function that we wish to integrate is Lipschitz, this 
ensures that 
{the}
integral approximation converges at speed $O(\frac 1 M)$. However, such 
quadrature rules are unsuitable for high-dimensional problems, as they require 
an exponential number of samples to achieve a given level of accuracy. 

Another alternative sampling strategy is to rely on quasi-Monte Carlo (Q.M.C.) methods, which use deterministic, low-discrepancy sequences instead of independent random samples. %These sequences fill the ambiant space better, and improve the Monte Carlo approximation when the number of samples is high and the space dimension is not too large. 
Traditional 
% QMC 
{Q.M.C.}
methods are designed for integration over the unit hypercube $[0, 1]^d$. The 
quality of a {Q.M.C.} sequence is often measured by its discrepancy, which 
measures how uniformly the points cover the space. A lower discrepancy 
correlates with a better approximation, according to the Koksma-Hlawka 
inequality~\citep{brandolini2013koksma}. Examples of low-discrepancy sequences 
for the unit cube include for instance the Halton 
sequence~\citep{halton1964algorithm},  and the Sobol 
sequence~\citep{sobol1967distribution}, and different approaches have been 
investigated to project such sequences on the hypersphere. While quadrature 
rules are recommended for very small dimensions ($d = 1$ or $2$ for instance), 
{Q.M.C.} integration is particularly effective in low to intermediate 
dimensions. A variant of low-discrepancy sequence is one where randomness is 
injected in the sequence while preserving its "low discrepancy" property. Such a 
sequence is called a randomized low-discrepancy sequence, and this is the 
foundation to randomized quasi-Monte Carlo
{(R.Q.M.C.)}
methods ~\citep{owen2019monte}.  
Q.M.C. methods do not only rely on low-discrepancy {sequences}, but can also use 
point sets of a given size directly optimized to have low-discrepancy, such as  
s-Riesz point configurations on the sphere ~\citep{GOTZ200362}. However {Q.M.C.} 
and {R.Q.M.C.} methods on the sphere have {a strong practical downside}: they 
suffer from the curse of dimensionality. Indeed the higher the dimension the 
harder it is to generate samples with {Q.M.C.} and {R.Q.M.C.} approaches. 
Moreover, the higher the dimension, the slower the convergence rate, and the 
more regular the integrand needs to be to ensure fast convergence. 
The recent paper~\citep{nguyen2024quasimonte} already proposes an interesting 
comparison of such {Q.M.C.} methods to approximate  
{the}
Sliced-Wasserstein 
{distance}
in dimension 3, showing that such methods could provide 
better approximations  that conventional {M.C.} in this specific dimensional 
setting. 

All the sampling strategies mentioned above are designed to provide a good coverage of the space. However, they do not take into account the specific structure of the integrand, which is the Wasserstein distance between the one dimensional projections of the two measures $\mu$ and $\nu$. More involved methods to improve Monte Carlo efficiency include importance sampling, control variates or stratification~\citep{asmussen2007stochastic}. Such variance reduction techniques strategies can also be used in conjunction with quasi-Monte Carlo integration.
Control variates have been explored for Sliced Wasserstein approximation in~\citep{Nguyen:2023aa} and \citep{leluc2024slicedwassersteinestimationsphericalharmonics}, showing interesting improvements in intermediate dimensions over classical Monte Carlo. 

The goal of this survey is to provide a detailed comparison of these different sampling strategies for the computation of Sliced-Wasserstein in various dimensional settings. It is intended as a user-guide to help practitioners choose the appropriate sampling strategy for their specific problem, depending on the size and dimension of their data, and the type of experiments to be carried out (whether or not they need to compute numerous SW distances for instance). We will also look at the particularities of the different approaches, some being more appropriate than others depending on whether a given level of accuracy is desired (in which case an approach allowing sequential sampling is preferable to one requiring optimization of a point set) or, on the contrary, a given computation time is imposed.  
We will mainly focus on sampling strategies which are independent of the knowledge of the measures $\mu$ and $\nu$, such as uniform random sampling~\citep{asmussen2007stochastic},
 orthonormal sampling~\citep{rowland2019orthogonal}, low-discrepancy sequences mapped on the sphere \citep{halton1964algorithm,sobol1967distribution}, randomized low-discrepancy sequences mapped on the spheres \citep{owen2019monte}, Fibonacci point sets~\citep{hardin2016comparison} and Riesz configuration point sets~\citep{GOTZ200362}.

For the sake of completeness, we  also include in our comparison the recent  approach~\citep{leluc2024slicedwassersteinestimationsphericalharmonics}, which appears to be the most efficient among recent control variates approaches proposed to approximate Sliced Wasserstein.   


The paper is organized as follows. \autoref{sec:reminderSW} introduces some reminders on the 
Sliced Wasserstein distance such as its definition and some regularity 
properties. \autoref{sec:Sampling} explores all the sampling methods considered 
in this paper, hightlighting their theoretical guarantees, the conditions under 
which they can be used, and identifying which methods suffer from the curse of 
dimensionality. Then \autoref{sec:Exp} provides a comparison of each sampling 
method's experimental results on different datasets. Finally, in 
\autoref{sec:Recommendation} we offer detailed recommendations for choosing and 
using these 
{sampling}
methods effectively in practice.
