\section{Reminders on sampling strategies on the sphere and their theoretical guarantees}\label{sec:Sampling}
{In this section, we present the different 
sampling methods for numerical integration on $\Sd$ considered in this paper, 
before comparing them experimentally in \autoref{sec:Exp}. This paper addresses 
three main types of sampling: random sampling, discrepancy-based sampling, and a 
control variate approach. The first type includes the classical Monte Carlo 
(M.C.) method (\citep{hammersley1964monte}, \citep{lemieux2009monte}) on the 
sphere and its variant called orthonormal sampling 
\citep{rowland2019orthogonal}. The second one relies on a concept called the 
discrepancy (\citep{lemieux2009monte},\citep{dick2010digital}) of a point set, 
which represents the number of points in a unit of volume, and can be divided 
into two categories: low-discrepancy sequences (or digital nets) and point sets 
(or lattices). {Among the former category, we also investigate a method based on a spherical sliced-Wasserstein type discrepancy~\citep{bonet2023sphericalslicedwasserstein}.} The last type details a control variates method 
~\citep{lemieux2009monte} using spherical harmonics ~\citep{Muller1998} for this 
purpose ~\citep{leluc2024slicedwassersteinestimationsphericalharmonics}. We will 
also determine which method, and under which conditions, is theoretically 
suitable based on the regularity properties established in 
\autoref{sec:regularityProp}. \autoref{tab:summarySamplingMethods} presents a 
taxonomy of all the sampling methods explored in this paper. It details which 
method's convergence rate result is \textbf{independent from the dimension} 
({i.e.} the dimension does not appear in the asymptotic rate), which one 
can be \textbf{computed independently} 
({i.e.} each sample can be generated 
independently from the others), and which one can be \textbf{computed and 
stored} in advance.} 

\begin{table}[h!]
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
Sampling types &  
{Dimension independence}
& Independent computation & 
{Possible pre-computation}
\\
\hline
\textbf{Random Sampling} &  &   & \\
\hline
Uniform Sampling & x & x & x  \\
Orthonormal Sampling & x & x  & x \\
\hline
\textbf{Based on discrepancy} & & & \\
\hline
Riesz Point Set / Riesz Point Set Randomized &  &  & x \\
Fibonacci Point Set / Fibonacci Point Set Randomized &  &  & x \\
Sobol / Sobol Randomized mapped on $\Sd$ &  & x & x \\
Halton / Halton Randomized on $\Sd $ &  & x & x \\
{Spherical Sliced Wasserstein Discrepancy} & {x} & & {x}\\ 
\hline
\textbf{Control variates} & & & \\
\hline
Spherical Harmonics Control Variates &  &  & \\
\hline
\end{tabular}}
\caption{{Taxonomy of the three types of sampling methods 
{investigated in this paper.}
}}
\label{tab:summarySamplingMethods}
\end{table}

{\autoref{tab:summaryConvergenceAndComplexity}  
gives a summary of the convergence rate and computational complexity of each sampling method explored in this paper. In this table $n_M = o\bigg(M^{1/\big(2(d-1)\big)}\bigg)$.}

{
\begin{table}[h!]
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
{Sampling types} & 
{Theoretical convergence rate}
& {Time complexity} & {Space complexity}
\\
\hline
{\textbf{Random Sampling}} &  &  & \\
\hline
{Uniform Sampling} & {$\mathcal{O}(1/\sqrt{M})$} & {$\mathcal{O}(M)$}& {$\mathcal{O}(M)$}\\
{Orthonormal Sampling} & {None} & {$\mathcal{O}(M)$}& {$\mathcal{O}(M)$}\\
\hline
{\textbf{Based on discrepancy}} & & &\\
\hline
{Riesz Point Set / Riesz Point Set Randomized} & {$1/M$ on $\mathbb{S}^1$, Not applicable otherwise}  & {$\mathcal{O}(M^2)$} & {$\mathcal{O}(M)$} \\
{Fibonacci Point Set / Fibonacci Point Set Randomized} & {Not applicable} &  {$\mathcal{O}(M)$} & {$\mathcal{O}(M)$}\\
{Sobol / Sobol Randomized mapped on $\Sd$} & {None} & {$\mathcal{O}\big(M \text{log}_b^2(M)\big)$} & {$\mathcal{O}(M)$} \\
{Halton / Halton Randomized on $\Sd $} & {None} & {$\mathcal{O}\big(M \text{log}_b^2(M)\big)$} & {$\mathcal{O}(M)$}\\
{Spherical Sliced Wasserstein Discrepancy} & {None} & {$\mathcal{O}(M\text{log}(M))$} & {$\mathcal{O}(M)$}\\
\hline
{\textbf{Control variates}} & & &\\
\hline
{Spherical Harmonics Control Variates} & {$\mathcal{O}\big(1/(n_M \sqrt{M})\big)$} & {$\mathcal{O}(M)$} & {$\mathcal{O}(M)$}\\
\hline
\end{tabular}}
\caption{{Convergence rate, time complexity and spacial complexity (w.r.t the sampling number) summary of the sampling methods 
studied in this paper.
}}
\label{tab:summaryConvergenceAndComplexity}
\end{table}
}

\subsection{Random samplings} \label{sec:Rand}

{We first explore} classical strategies for randomly generating points on the sphere: uniform sampling \citep{hammersley1964monte} and orthonormal sampling \citep{rowland2019orthogonal}. These strategies are the most commonly used for estimating $\SW$, and their convergence rates do not depend on the dimension of the input measures.


\subsubsection{Classical Monte Carlo} \label{sec:unif}

{The classical Monte Carlo method uses uniform random sampling to generate the projection angles. 
For $(\theta_M)_{M\in\mathbb{N}^*}$ i.i.d. samples of $\sd$~\footnote{In practice, to simulate a random variable $\theta\sim\sd$, one takes a normal random variable $Z\sim\mathcal{N}(0, I_d)\neq 0$ and chooses $\theta = \frac{Z}{\| Z \|} \sim \sd$ \citep{asmussen2007stochastic}.}, we write the Monte Carlo Estimator 
\begin{equation}
    X_M :=\displaystyle\frac{1}{M}\sum\limits_{i = 1}^{M} f(\theta_i) \text{ with } M\in\mathbb{N}^* . 
    \label{eq:mcestimator}
\end{equation}
The law of large numbers ensures that $X_M$ converges a.s. to $\SW(\mu,\nu) = \mathbb{E}_{\theta \sim \sd}[f(\theta)]$ as $M$ goes to infinity. Moreover, the rate of convergence for this unbiased estimator  is given by 
\begin{equation}\label{eq:convUnif}
    \sqrt{\mathbb{V}[X_M]} = \sqrt{\frac {\mathbb{V}[X_1]}{M}} = \frac {{\sigma}}{\sqrt{M}},
    \end{equation}
where $\sigma^2 = \mathbb{V}[f(\theta)] = \displaystyle\int_{\Sd} f^2(\theta) ds_{d-1}(\theta) - \SWsq(\mu,\nu) < +\infty$.
{This convergence rate in \autoref{eq:convUnif} does not depend on the dimension of the input measures}. 
In order to derive confidence intervals for $\SW(\mu,\nu)$, we can rely on the Central Limit Theorem~\citep{fischer2010history} , which states that 
\begin{equation*}\label{eq:CLTSW}
    \sqrt{M}\frac{[X_M - \SW(\mu,\nu)]}{\mathbb{\sigma}} \xrightarrow[M\rightarrow +\infty]{\mathcal{L}} \mathcal{N}(0,1),
\end{equation*}
This allows us to compute confidence intervals for $\SW(\mu,\nu)$ by using the quantiles of the standard normal distribution.}
\noindent {This means that} for $M$ large enough,  $\mathbb{P}\left(X_M - \SW(\mu,\nu) \in \left[  -\frac{\sigma q_{1 - \alpha/2}}{\sqrt{M}} , \frac{\sigma q_{1 - \alpha/2}}{\sqrt{M}}\right]\right) {\xrightarrow[M\rightarrow +\infty]{}} 1 - \alpha$, with $\alpha$ in $[0,1]$  and $q_{1 - \alpha/2}$ the quantile {of level $1 - \alpha /2$} of $\mathcal{N}(0,1)$. 
One strategy for choosing $M$ is taking $M$ such that $\frac{\sigma q_{1 - \alpha/2}}{\sqrt{M}} \leq \varepsilon$ with $\varepsilon\geq 0$ a chosen {precision}.  The value of $\sigma$ being unknown, a possibility is to plug a consistent  estimator of $\sigma^2$, such as
$$\hat{\std}_M^2 = \frac{1}{M}\left[\displaystyle\sum\limits_{i = 1}^M f(\theta_i)^2 - X_M^2\right].$$
\citet{xu2022central} provide an alternative criteria for choosing $M$, however it is quite impractical as it {requires} to compute the Wasserstein distance between $\mu$ and $\nu$.


\subsubsection{Orthonormal sampling} \label{ortho}

{A variant of the uniform sampling covered in \autoref{sec:unif} was introduced by \citep{rowland2019orthogonal}, which presents a simple variant for the previous Monte Carlo estimator $X_M$ by sampling random orthonormal bases. This method is inspired by variance reduction techniques known as stratification \citep{lemieux2009monte}.
Let $O(d)$ be the orthogonal group in $\R^d$. For $(\Theta_P)_{P\in \N^*} \sim \mathcal{U}\big(O(d)\big)$, denoting $\theta_1,\hdots,\theta_M$ all the columns of the matrices $\Theta_1,\hdots,\Theta_K$, we define $Y_M =\displaystyle\frac{1}{M}\sum\limits_{i = 1}^{M} f(\theta_i)$. It is easy to show that each $\theta_i$ follows the uniform distribution on $\Sd$~\citep{rowland2019orthogonal}. As a consequence, the estimator $Y_M$ is still unbiased. Although it is not possible to show that $Y_M$ has a smaller variance than $X_M$ in general, this estimator is most of the time more efficient than $X_M$ in our experiments and show an equivalent or better rate of convergence in practice. This might be due to the fact that the diversity of the samples is increased by the orthonormality constraint.}

\begin{Rk}
 Other fully random point processes on $[0,1]^2$ or $\mathbb{S}^2$ suitable for Monte Carlo integration are studied in the literature. Among them, we can mention Determinantal Point Process (DPP). Recent works, such as \citep{feng2023determinantalpointprocessesspheres}, have proposed DPP methods directly on the sphere $\mathbb{S}^2$. Unfortunately, due to the lack of publicly available implementations, we could not experiment efficiently with these methods.
\end{Rk}


\subsection{Sampling strategies based on discrepancy}

We examine in this section two different types of deterministic sampling based on discrepancy: low-discrepancy sequences (digital nets) and low-discrepancy point sets (lattices). They were developed to replace random sampling, expecting to have a better convergence rate than the classical Monte Carlo method. 

\subsubsection{Low-discrepancy sequences} \label{sub:lds}

Quasi-random sequences, better known as low-discrepancy sequences (L.D.S.), are 
sequences mimicking the behavior of random sequences while being entirely 
deterministic. {To date, these sequences are only defined on the unit 
{hypercube}
$[0,1]^d$. We introduce below a first definition of discrepancy 
(\citep{lemieux2009monte},\citep{dick2010digital})}.
\begin{D}
The discrepancy of a set of points $P = \{u_1, \hdots, u_M\}$ in {$[0,1]^d$} is defined as
$$D_M(P) = \sup_{I\in \mathcal{I}} \Bigg| \frac{| P \cap I |}{M} - \lambdim(I)\Bigg|,$$
where  $|A|$ denotes the cardinal of a set $A$, $\lambdim$ is the $d$-dimensional Lebesgue measure and
 $\mathcal{I} = \{\prod\limits_{i = 1}^d [a_i,b_i[\ |\ 0 \leq a_i < b_i \leq 1\}$. The star-discrepancy $D_M^*(P)$ is defined the same way with $\mathcal{I}^{*} = \{\prod\limits_{i = 1}^d [0,b_i[ \ |\ 0 \leq  b_i \leq 1\}$.
\end{D}

We can now provide a definition of a Low discrepancy sequence (L.D.S.).
\begin{D}
{Let $(u_m)_{m\in\N^*}$ be a sequence in $[0,1]^d$. Denoting $P_M = \{u_1, \hdots, u_M\}$ for any $M\in\N^*$, $u$ is a L.D.S. if
$$D_M^*(P_M) \xrightarrow[M \to +\infty]{} 0.$$}
\end{D}




{The notion of discrepancy is important because it is related to the error made when approximating an integral on the hypercube by its Monte Carlo approximation.} {This relation is made explicit by the Koksma-Hlawka inequality (\citep{lemieux2009monte}; \citep{dick2010digital}; \citep{brandolini2013koksma}).}

This inequality requires to introduce the notion of Hardy-Krause variation $V_h$ of a function $h$ on $[0,1]^d$ \citep{aistleitner2016functions}, which is out of the scope of this paper, but can be broadly understood as a measure of the oscillation of $h$ on the unit cube $[0,1]^d$.
\begin{Prop}[Koksma-Hlawka inequality]
Let {$h:[0,1]^d \rightarrow \mathbb{R}$} have bounded variation $V_h$ on {$[0,1]^d$} in the sense of Hardy-Krause \citep{aistleitner2016functions}. Then for $\{u_1,\hdots,u_M\}$ a point set in {$[0,1]^d$}, we have
\begin{equation}\label{eq:KHIneq}
\Bigg |\frac{1}{M}\displaystyle\sum\limits_{k=1}^M h(u_k) - \displaystyle\int_{S} h(x)d\lambdim(x) \Bigg | \leq V_h D_M^*(u_1,\hdots,u_M).
\end{equation}
\end{Prop}
\noindent The proof of this inequality and basic results on discrepancy theory can be found in\citep{kuipers2012uniform} and \citep{dick2010digital}. \autoref{eq:KHIneq} shows that the absolute error made {by the Monte Carlo approximation} is {upper} bounded {by a term depending only on $h$} and the star discrepancy. Compared to the Central Limit Theoreom, this inequality is not probabilistic and not asymptotic, the bound being valid for every $M \in \mathbb{N}^*$.  An important limitation is the term $V_h$, which is impractical to compute directly. When $d = 1$, this term is exactly the total variation of $h$, but in general, it is only {upper} bounded by the total variation. 
In the case of our function $f$ involved in the estimation of $SW$, $V_f < +\infty$ holds since $f$  is Lipschitz continuous. Another limitation of the previous bound is that the rate of convergence of the star discrepancy $D_M^*$ of a sequence is most of the time not explicit and difficult to compute~\citep{owen2005multidimensional}. 

Nevertheless, this proposition ensures that if the rate of convergence of the star discrepancy of a sequence is better than $O(\frac 1 {\sqrt{M}})$, for $M$ large enough the approximation of the quasi Monte Carlo approximation using this sequence will outperform the one of classical Monte Carlo. 

{In the following, we present two L.D.S. defined on the unit square $[0,1]^d$, and see how their star discrepancy decreases with $M$. We then focus on practical methods to map these sequences from the hypercube to the hypersphere $\Sd$.}

\paragraph{Halton sequence} \label{par:Halton}\mbox{} \\

The Halton sequence $(u_i)_{i\in\N} \in (\Rd)^{\N}$ \citep{halton1964algorithm} is a generalization of the von der Corput sequence \citep{vdc1935}. In the following, we write, for any integer $i$, $c_l(i)$ the coefficients from the expansion of $i$ in base $b$, and we define the radical-inverse function in base $b$ as 
$$
\phi_b(i) = \displaystyle\sum\limits_{l = 0}^{+\infty} c_l(i)b^{{-}l-1},\forall i \in \mathbb{N}.
$$ 
The Halton sequence in dimension $d$ is then defined as
$$
u_i = (\phi_{b_1}(i),\hdots,\phi_{b_d}(i))^T,
$$
where $b_i$ is chosen as the i-th prime number.

\paragraph{Sobol sequence} \label{par:Sobol}\mbox{} \\

This sequence uses the base $b = 2$. To generate the $j$-th coordinate of the $i$-th point $u_i$ in a Sobol sequence \citep{sobol1967distribution}, one needs a primitive polynomial of degree $n_j$ in $\Zq[X]$,
$$
X^{n_j} + a_{1,j}X^{n_j - 1} + a_{2,j}X^{n_j - 2} + \hdots + a_{n_j-1,j}X + 1.
$$
{This polynomial is used to define} a sequence of positive integers $(m_{k,j})$ by recurrence, with $+_{\Zq}$ the inner law of $\Zq$:
$$
m_{k,j} = 2a_{1,j}m_{k-1,j}+_{\Zq} 2^2a_{2,j}m_{k-2,j} +_{\Zq} \hdots +_{\Zq} 2^{n_j}m_{k-n_j,j} +_{\Zq} m_{k-n_j,j}.
$$
The values $m_{k,j}$, {for $1\leq k \leq n_j$,} can be chosen chosen arbitrarily provided that each is odd and less than $2^k$. Then one {generates} what is called direction numbers:
$$v_{k,j} = \frac{m_{k,j}}{2^k}.$$
The $j$-th coordinate of $u_i$ is then obtained as
$$
u_{i,j} = \displaystyle\sum\limits_{k = 1}^{+\infty} c_k(i) v_{k,j}.
$$

\paragraph{Convergence rate of Halton and Sobol sequences} \label{par:convHaltonSobol}

Both sequences (Halton and Sobol) have a star discrepancy which converges to $0$ (which means that they are indeed L.D.S.). The convergence rate is given by the following property~\citep{niederreiter1988low}~\citep{owen2019monte}.
\begin{Prop}
Let $(u_m)_{m\in\mathbb{N}^*}$ be either the Halton sequence or Sobol sequence in $[0,1]^d$. Then for $M\geq 1$, we have
$$D_M^{*}(u_1,\hdots,u_M)\leq c_d \frac{log(M)^d}{M}$$
where $c_d$ is a constant that depends only on the dimension.
\end{Prop}
Thanks to~\autoref{eq:KHIneq}, {for any function $h$ such that $V_h<+\infty$ (which is the case for our function $f$)}, this implies  a convergence rate of the Monte Carlo estimator using these sequences in $\mathcal{O}(\frac{log(M)^d}{M})$, which means $\mathcal{O}(M^{-1+\epsilon})$ for every $\epsilon >0$. This convergence rate  is better than the one of classical Monte Carlo with i.i.d. sequences, {even if the rate of convergence slows down when the dimension increases, because of the term $\log(M)^d$.} 
{
\begin{Rk}
Note that L.D.S. are designed to mimic the behavior of a random uniform sampling in $[0,1]^d$ while being completely deterministic. This deterministic behavior leads to patterns in the sampling; because of those patterns, the higher the dimension, the harder it is for those to fill the "gaps" in $[0,1]^d$.
Moreover, the term $\text{log}(M)^d$ implies that one needs $M$ to be very large (exponential) to get the same level of space coverage in high dimension than in low dimension.
\end{Rk}
}
{
\begin{Rk}
Observe that both for Sobol and Halton sequences, {generating $M$ values} has a complexity in $\mathcal{O}\big(M log_b^2(M)\big)$, where $b$ is the base (or smallest basis for Halton) chosen.
\end{Rk}
}

\paragraph{L.D.S. on the sphere} \label{par:ldsSphere}\mbox{} \\
\noindent To our knowledge, there is no true L.D.S. on the unit sphere $\Sd$ for $d \geq 3$, this question remaining an active research area. Practitioners typically map L.D.S. from the hypercube to the hypersphere, using one of the methods described below: 
\begin{itemize}\label{item:Methods}
\item[$\bullet$] \textbf{Equal area mapping} ~\citep{Aistleitner2012}: this method is only defined for mapping points in the unit square to $\mathbb{S}^2$. Denoting $(z_1,z_2)\in [0,1[^2$,  one gets a point $u = \Phi(2\pi z_1, 1-2z_2)$ on $\mathbb{S}^2$ with:
\begin{equation}\label{eq:EqArea}
\Phi(\eta, \beta) = \left( \sqrt{1 - \beta^2}\tcos(\eta), \sqrt{1 - \beta^2}\tsin(\eta), \beta\right),\ \eta,\beta\in[0,1[.
\end{equation}

\item[$\bullet$] \textbf{Spherical coordinates}~\citep{arfken2011mathematical}: This method maps the points from an L.D.S. in $[0,1]^{d-1}$ to $\Sd$ by using the spherical coordinates.  Unfortunately, we found that the resulting sampling is usually not competitive compared to other sampling methods.

\item[$\bullet$] \textbf{Normalization onto the sphere} ~\citep{basu2016}: {An L.D.S. is generated 
in the $d$-hypercube $[0,1]^d$ and mapped to $\R^d$ using the inverse cumulative 
distribution function of the standard normal distribution (separately on each 
dimension).  Then each point in the 
{resulting}
sequence is normalized by its norm to map it onto $\Sd$. }

\end{itemize}


\textbf{Specific case of $\mathbb{S}^2$.} \\
In the specific case of $\mathbb{S}^2$, it has been shown by~\citet{Aistleitner2012} that if $u$ is an L.D.S in $[0,1]^2$ and $\Phi$ the equal area mapping defined in \autoref{eq:EqArea}, the spherical cap discrepancy $D_{\mathbb{L}_2,M}\big(\Phi(P)\big)$ (see definition \autoref{eq:SphereCal} in the next section) of the mapped sequence is in $\mathcal{O}\left(\frac{1}{M^{1/2}}\right)$. However, their experiments showed that the correct order seems rather to be $\mathcal{O}\left(\frac{log^c (M)}{M^{3/4}}\right)$ for $1/2\leq c \leq 1$.


\subsubsection{Deterministic point sets on $\Sd$} \label{sec:QMCSphere}

This section details different methods to design well distributed point sets on $\Sd$. Contrary to the L.D.S. defined above,  these point sets are defined directly on the sphere, in order to be approximately uniformly distributed on $\Sd$. To measure this uniformity, we can rely on the notion of spherical cap on the sphere: a spherical cap of center $c\in\Sd$ and $t\in [-1,1]$ is defined as
\begin{equation}
C(c,t) = \{ x \in \Sd\ | \ \langle x , c \rangle > t\}.
\end{equation}\label{eq:sphereCap}

In other words, a spherical cap is the intersection of a portion of the sphere and a half-space (see \autoref{fig:sphericalcap} for an illustration). 
\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.25]{ReminderSampling/sphericalcapnew.png}
\caption{Illustration of a spherical cap on $\mathbb{S}^2$. The circle represents the intersection of the plane $\langle x, c \rangle = t$ with the sphere, and the purple colored area is the cap $C(c,t)$ as noted in \autoref{eq:sphereCap}.}
\label{fig:sphericalcap}
\end{center}
\end{figure}


To the best of our knowledge, there is no equivalent to the Koksma-Hlawka inequality for the sphere in full generality~\citep{brauchart2011optimal}. 
A sequence of points $\{u_n\}$ on $\Sd$ is said asymptotically uniformly distributed on $\Sd$ if for every spherical cap $C$, the proportion of points inside the cap, converges to the measure of the cap $\sd(C)$. It can be shown that this assumption is equivalent to assume that for every continuous function $h$, the Monte Carlo approximation $\frac 1 M \sum_{k=1}^M h(u_k)$ converges to $\mathbb{E}_{\theta \sim \sd}[h(\theta)]$. 

In order to get a non asymptotic notion of the uniformity of a point set on $\Sd$, we can rely on different notions of spherical cap discrepancy on the sphere, defined as follows.


\begin{D}
The spherical cap max-discrepancy of a point set $P_M$ of size $M$ is defined as~\citep{marzo2021discrepancy}:
\begin{equation*}
    D_{max}(P_M) = \sup_{c\in \Sd ,t \in [-1,1]}\left\lbrace  \bigg | \frac{|P_M \cap C(c,t)|}{M} - s_{d-1}\big(C(c,t)\big)\bigg| \right\rbrace.
\end{equation*}\label{eq:Spheremax}
The spherical cap $\mathbb{L}_2$-discrepancy of a point set $P_M$ of size $M$ is defined as~\citep{brauchart2011optimal}:
\begin{equation*}
D_{\mathbb{L}_2}^2(P_M) = \left\lbrace \int_{-1}^1 \int_{\Sd} \bigg | \frac{|P_M \cap C(c,t)|}{M} - s_{d-1}\big(C(c,t)\big)\bigg|^2 ds_{d-1}(c) dt\right\rbrace ,
\end{equation*}\label{eq:SphereCal}
where $C(c,t)$ is  a spherical cap of center $c$ and height $t$.
\end{D}
Again, the idea is to compare the proportion of points in $P_M$ that fall inside a spherical cap with the measure of the cap. This comparison is done for all possible caps on the sphere, and $D_{max}$ represents the worst error over all possible caps, while $D_{\mathbb{L}_2}^2$ represents the average squared error over all possible caps.


When using 
{Q.M.C.}
on the hypersphere to approximate the integral of functions $h$, 
another notion often used in the literature is the worst-case (integration) error (W.C.E.) on a Banach space of functions, which is the largest possible error made by the method on the space. 
For instance, on $H^{\alpha}(\Sd)$,
\begin{D}
For $P_M = \{ u_1,\hdots, u_M\}$, for $\alpha \in \N$
\begin{equation*}
WCE\Big(P_M,H^{\alpha}(\Sd)\Big)= \sup_{h\in H^{\alpha}(\Sd)} \bigg |\displaystyle \frac{1}{M} \sum\limits_{m=1}^M h(u_m) - \frac{1}{s_{d-1}(\Sd)}\int_{\Sd} h(w) ds_{d-1}(w) \bigg |.
\end{equation*}
\end{D}
\noindent Under some regularity condition, a sufficient and necessary one being $\alpha \geq \frac{1}{2} + \frac{d - 1}{2}$ for $H^{\alpha}(\Sd)$, \citet{Brauchart_2013} show that optimizing the spherical cap $\mathbb{L}_2$-discrepancy is equivalent to optimizing the W.C.E. thanks to the Stolarsky's invariant principle ~\citep{stolarsky1973}.
In the case of our function $f$, we have seen that $f$ is regular enough in the specific case of $\mathbb{S}^1$, since $f\in H^{\alpha} (\mathbb{S}^1)$ with $\alpha = 1 = \frac{1}{2} + \frac{1}{2}$. However in dimension larger than $3$, this result does not hold anymore since $f$ does not belong to any Sobolev space $H^{\alpha} (\mathbb{S}^d)$ with $\alpha > 1$.

\paragraph{Fibonacci point set on $\mathbb{S}^2$}\mbox{}\\
\noindent {Denoting $\varphi$ the polar angle and $\chi$ the azimuthal angle 
forming the geographical coordinates $(\varphi, \chi)$,  we retrieve the 
Cartesian coordinates $(x,y,z)$ using the spherical coordinates 
(see~\autoref{fig:sphericalcoord} for 
{an}
illustration).} Noting $\phi = \frac{1 + \sqrt{5}}{2}$ the golden 
ratio, {the $m$-th point $u_m = (\varphi_m, \chi_m)$ of the Fibonacci} {point} set is given by
\begin{align*}
\varphi_m &= \tacos\left(\frac{2m}{2M + 1}\right),\\
\chi_m &= 2m\pi \phi^{-2}.
\end{align*}
It is a simple and  efficient way, convergence rate wise, to generate points on $\mathbb{S}^2$ for the quasi-Monte Carlo method {but it} is only defined on $\mathbb{S}^2$. The complexity of the generation is linear in $M$, and according to \citep{Marques2013}, the corresponding convergence rate for the W.C.E. and the $\mathbb{L}_2$-spherical cap discrepancy is in $\mathcal{O}(\frac{1}{M^{3/4}})$. For an extensive list of other popular point configurations on $\mathbb{S}^2$, see \citep{hardin2016comparison}. 


\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.18]{ReminderSampling/sphereCoords2.png}
\caption{Illustration of the spherical coordinates in $\R^3$ for points on the sphere $\mathbb{S}^2$.}
\label{fig:sphericalcoord}
\end{center}
\end{figure}


\paragraph{Equi-distributed points generated by the discrete s-Riesz energy} \label{par:Riesz} \mbox{}\\

Another classical way to define equi-distributed point sets on the hypersphere is to rely on optimization. In such methods, the point set $P_M$ is defined as the minimizer of a certain energy functional $E_s$,
\begin{equation*}
    P^*_M := \argmin_{u_1,\hdots,u_M \in \Sd} E_s(u_1,\hdots, u_M).
\end{equation*}
The most common energy functional is the s-Riesz energy, which is defined as follows. 
\begin{D}
For $s \geq 0$ and $P_M = \{ u_1, \hdots, u_M\}$ a set of points on $\Sd$, the s-Riesz energy of $P$ is defined as
\begin{equation*}
E_s(P_M) = 
\begin{cases}
\displaystyle\sum\limits_{i\neq j} \frac{1}{\| u_i - u_j\|^s} & \text{if} \ s > 0,\\
\displaystyle\sum\limits_{i\neq j} \text{log} \frac{1}{\| u_i - u_j\|} & \text{if} \ s = 0.
\end{cases}
\end{equation*}
\end{D}

The resulting point set is called a minimal $s$-energy configuration. {The s-Riesz energy can also be defined for $s < 0$, in this case the point set $P_M$ is obtained as the maximizer of $E_s= \displaystyle\sum\limits_{i\neq j} \| u_i - u_j\|^s$~\citep{brauchart2011optimal}.} Minimising $E_s$  is non trivial, the functional being not convex, and the problem becomes more complex when the dimension increases. 
Minimal energy configuration points for $E_s$ are called Fekete points and it is known that for $0\leq s < d$, these sets are asymptotically uniformly distributed with respect to the normalized surface measure $s_{d-1}$, which means that Monte Carlo estimates using the Fekete points converge to the integral against $s_{d-1}$~\citep{marzo2021discrepancy}. 

The spherical cap $\mathbb{L}_2$-discrepancy of a point configuration is minimal if and only if the sum of distances in the configuration is maximal. {This would correspond to maximizing a s-Riesz energy for $s=-1$~\citep{brauchart2011optimal}}. However, the link between the configurations of minimal $s$-Riesz energy and the max or $\mathbb{L}_2$ discrepancies of these configurations is in general not straightforward, see~\citep{brauchart2011optimal},~\citep{marzo2021discrepancy}, \citep{GOTZ200362}. For $0\leq s <d$, and $P_M$ a minimizer of size $M$ of the Riesz s-energy on $\Sd$, the authors of ~\citep{marzo2021discrepancy} show that {\[D_{max}(P_M) \lesssim O\left(\max\left(M^{-\frac{2}{d(d-s+1)}},M^{-\frac{2(d-s)}{d(d-s+4)}}\right)\right).\]  
This implies that $D_{max}(P_M) \xrightarrow[M \to +\infty]{} 0$, but the speed of convergence degrades with the dimension $d$}, which means that  the uniformity of these configurations  
{is}
 likely to suffer from the curse of dimensionality. \autoref{fig:rieszAndfig} shows an example of s-Riesz points and Fibonacci points on $\mathbb{S}^2$ with $500$ points.

\begin{Rk}Since computing Riesz point configurations involves optimization (with a non linear complexity), the time needed to generate those points can be impractical. Note that generally the generation of the $s$-Riesz configuration points has a {runtime complexity} of $\mathcal{O}(T M^2)$, where $T$ is the number of iterations of the optimization loop.
\end{Rk}

\noindent \textbf{In the specific case of $S^1$}, the Fekete points are unique up to a rotation, and are the $M$-th unit roots (see \citep{GOTZ200362} and see \autoref{fig:unityroots} for an illustration):
$$\left\lbrace e^{\frac{2ik\pi}{M}} \ | \ k = 0,\hdots,M-1 \right\rbrace.$$
This explains why for 2D discrete measures, a uniform grid on $\mathbb{S}^1$ gives better results than any other sampling method for computing $\SW$, as we will see in \autoref{sec:Exp}.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.35]{ReminderSampling/unity_roots.png}
\caption{{Plot} of the 10-th unity roots, i.e  solutions to the equation $z^{10} = 1$.}
\label{fig:unityroots}
\end{center}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.04]{ReminderSampling/rieszExample.png}
\hspace{1cm}
\includegraphics[scale=0.041]{ReminderSampling/fibExample.png}
\caption{Illustration of s-Riesz points (on the left) and Fibonacci points (on the right) {on $S^2$}, with $500$ points for both configurations. }
\label{fig:rieszAndfig}
\end{figure}


\subsubsection{Random Quasi Monte-Carlo} \label{par:rqmc} 

The principle of Randomized Quasi-Monte Carlo (R.Q.M.C.) methods is to reintroduce stochasticty in Q.M.C. sequences.  Indeed, Q.M.C. methods such as the ones described in Sections~\autoref{sub:lds} and~\autoref{sec:QMCSphere} are deterministic. For a given $M$, the estimator given by one of these methods is always the same. As such, we cannot easily estimate the error or the variance of the Monte Carlo approximation. Besides, while results such as the Koksma-Hlawka inequality ensures that they converge at a certain rate, the different quantities involved in the inequality are much more complex to estimate than the one involved in the Central Limit theorem.  Random Quasi-Monte Carlo methods were especially designed to recover this ability to estimate the error easily.
These sequences are usually defined on $[0,1]^d$.
\begin{D}[\citep{owen2019monte}]
Let $\{\Ranu_i\}_{i \geq 1}$ be a sequence of points in $[0,1]^d$. It is said to be suitable for R.Q.M.C. if $\forall i$, $\Ranu_i \sim \mathcal{U}([0,1]^d)$ and if there exist a finite $ c > 0 $ and $K > 0$ such that for all $M\geq K$, 
\begin{equation*}
\mathbb{P}\left[D_M^*(\RanP_M) < c\frac{log^d(M)}{M}\right] = 1, \;\;\text{where}\;\; \RanP_M = \{\Ranu_1, \hdots, \Ranu_M\}.
\end{equation*}
\end{D}

\noindent {Denoting}  $X_M = \displaystyle\frac{1}{M}\sum\limits_{i = 1}^{M} h(\Ranu_i)$ the  {empirical} estimator of $\mathbb{E}_{\theta\sim \sd}[h(\theta)]$, the assumption $\Ranu_i \sim \mathcal{U}([0,1]^d)$ implies that  $X_M$ is unbiased. Besides, the previous inequality implies that if  $\{\Ranu_i\}_{i \geq 1}$ is suitable for R.Q.M.C., then the variance of $X_M$ is bounded by $c^2 V_h^2 \frac{log^{2d}(M)}{M^2}$. For functions $h$ such that $V_h <\infty$, this yields a convergence rate in $\mathcal{O}\big(\log^{d}(M)/M\big)$, similar to the one of low discrepancy sequences. 

Once a randomization method is chosen (such that it provides suitable R.Q.M.C. sequences), the process can be repeated several times to obtain $K$ independent random estimators $X_M^1,\dots X_M^K$ of $\mathbb{E}_{\theta\sim \sd}[h(\theta)]$. The agregated estimate $ X_{M,K} = \displaystyle\frac{1}{K}\sum\limits_{k = 1}^{K} X_M^K$ has a variance decreasing in $O\big(\log^{d}(M)/(MK^{-1/2}) \big)$. One of the key advantages of this approach is that this variance (or confidence intervals) can be estimated by the empirical variance of the $K$ independent estimators.

\noindent 
There are several  ways to generate sequences from low discrepancy sequences on 
$[0,1]^d$ in order to make them suitable for R.Q.M.C.. One of the most simple 
methods consists in applying the same random shift $U$ to all points in the 
sequence, and taking the result modulo $1$ 
componentwise~\citep{lemieux2009monte}. More involved methods, such as Digital 
shift or Scrambling, are described in~\citep{lemieux2009monte} 
and~\citep{owen2019monte}.  

However, to the best of our knowledge, there is no proper R.L.D.S. on the sphere, as stated by \citet{nguyen2024quasimonte}. 
In practice,  R.L.D.S. on the unit cube are mapped onto the sphere by the methods described in \autoref{par:ldsSphere}. Another possibility, as done in \citet{nguyen2024quasimonte}, is to generate a random rotation matrix and apply it directly on point configurations on $\Sd$, {such as the ones described in~\autoref{sec:QMCSphere}}.

{
\subsection{Spherical Sliced Wasserstein}}

{A sampling method based on a Sliced-Wasserstein type discrepancy on the sphere $\Sd$ was developped by \citet{bonet2023sphericalslicedwasserstein} for $d\geq 3$.
We denote $\C$ the set of great circles of $\Sd$, a great circle being the intersection between a plane of dimension 2 and $\Sd$ \citep{jung2012}. The authors of \citet{bonet2023sphericalslicedwasserstein}
define a pseudo distance, called Spherical Sliced Wasserstein distance, between two probability measures $\Theta,\Xi$ defined on $\Sd$:
\begin{equation}\label{eq:SSW}
SSW_2^2(\Theta,\Xi) = \displaystyle\int_{\C} W_2^2(\pi_C\#\Theta,\pi_C\#\Xi)d\zeta(C),
\end{equation}
where for all $x\in\Sd, \pi_C(x) = \argmin_{y\in C} d_{\Sd}(x,y)$ with $d_{\Sd}(x,y) = \text{arcos}(\langle x,y\rangle)$~\citep{Fletcher2004} and $\zeta$ is the uniform distribution over $\C$.\\
As shown in~\citet{bonet2023sphericalslicedwasserstein}, this distance can be used to sample points on $\Sd$ by minimizing $SSW_2$ between a discrete measure $\Theta = \frac{1}{M}\sum\limits_{i=1}^M \delta_{\theta_i}$ and the uniform measure $\Xi = \sd$ on $\Sd$. 
To this aim, for $C_1,\hdots,C_L$ L independent great circles, they approximate $SSW_2^2(\Theta,\Xi)$  by  its Monte Carlo approximation $Z_L(\Theta,\Xi) =\displaystyle\frac{1}{L}\sum\limits_{l = 1}^L W_2^2(\pi_{C_l}\# \Theta, \pi_{C_l}\# \Xi)$.
Then, they note that $\pi_{C_l}\#\sd = s_1$~\citep{Jung2021} for each $l$, and derive a closed form for $W_2^2(\pi_{C_l}\# \Theta, s_1)$ based on \citet{ delon2010transportationdistancescircleapplications}.
The final distance $SSW_2^2(\Theta,\Xi)$ can then be optimized with respect to the point positions $\theta_i$ with a projected gradient descent.
\begin{Rk}
There are cases in which $SSW$ is a metric:
\begin{itemize}
\item[$\bullet$] Based on \citet{Quellmalz_2023}, $SSW$ is a metric between any two probability measures on $\mathbb{S}^2$.
\item[$\bullet$] A result from \citet{liu2024linearsphericalslicedoptimal} also shows that $SSW$ is a metric between any two absolutely continuous probability measures with continuous density functions on $\Sd$ for $d\geq 3$.
\end{itemize}
\end{Rk}
\begin{Rk}
Noting $T$ the number of iterations for the gradient descent algorithm, and $L$ as above, then the time complexity of this method is in $\mathcal{O}(TLM\text{log}(M))$.
\end{Rk}
\begin{Rk}
Notice that $SSW$'s form is similar to the $\mathbb{L}_2$-spherical cap discrepancy, where instead of averaging the "error" made by the sampling on a spherical cap, it averages the "error" made by the sampling on a great circle.
\end{Rk}
}


\subsection{Variance reduction} \label{sec:CV}
All methods described so far are based on the idea of generating points on the 
sphere in such a way that these points are sufficiently well distributed to be used 
for Monte Carlo integration, and ideally yield faster convergence than 
{M.C.}
with i.i.d. sequences. These point sequences or point sets are defined 
independently of the function to be integrated. 

More involved approaches, such as importance sampling or control variates, use the knowledge of the function to be integrated to improve Monte Carlo estimators by decreasing their variance. 
Recently, two control variates based methods have been developped to estimate the Sliced Wasserstein distance.
A control variate is a centered random vector $Y \in \mathbb{R}^p$, easy to sample, with finite second moments. Assume we want to estimate $\mathbb{E}_{\theta \sim \sd}[f(\theta)]$. Writing $\theta_1,\hdots,\theta_M$ i.i.d. samples of $\theta \sim \sd$ and  {$Y_1,\hdots,Y_M$ $M$} independent copies of the random centered vector $Y$, we consider the following estimator 
\begin{equation*}
     \displaystyle\frac{1}{M}\sum\limits_{i=1}^M [f(\theta_i) - \beta^T Y_i],
\end{equation*}
where $\beta \in \R^p$ is a constant vector to be determined. The variance of this estimator is proportional to $\mathrm{Var}(f(\theta) - \beta^T Y)$.
It follows that if we write $\beta^*$ the parameter minimizing this variance, then the pair $(\mathbb{E}(f(\theta)),\beta^*)$ is solution of the least square problem
\[\min_{(\zeta,\beta)\in \mathbb{R}\times \mathbb{R}^p} \mathbb{E}[(f(\theta) - \zeta - \beta^TY )^2].\]
An empirical version of this quadratic problem on a sample $(\theta_1,\hdots,\theta_M)$ writes
\begin{equation}\label{eq:leastSquares}
    (\widehat{\mathbb{E}(f(\theta))}_M,\beta_M)= \argmin_{\zeta,\beta\in\R\times\R^p} \| \textbf{F} -\zeta \mathbb{1}_M -\textbf{Y}\beta\|_2^2
\end{equation}
where {$\textbf{F} = \big(f(\theta_i)\big)^T_{i = 1,\hdots,M}$, $\mathbb{1}_M = (1,\hdots,1)^T\in\R^M$, and $\textbf{Y} = \big(Y_i^T\big)_{i = 1,\hdots,M}\in \mathbb{R}^{M\times p}$}. 

{Recently, \citet{nguyen2024slicedwassersteinestimationcontrol} introduced a Sliced Wasserstein distance estimation using Gaussian control variates and \citet{leluc2024slicedwassersteinestimationsphericalharmonics} developped a method using spherical harmonics control variates. We focus only on \citet{leluc2024slicedwassersteinestimationsphericalharmonics} here, since their method yields much better experimental results.} 
In their work, \citet{leluc2024slicedwassersteinestimationsphericalharmonics} chose Spherical Harmonics
\citep{Muller1998} as control variates. 
Spherical harmonics are functions which form an orthonormal basis $(\phi_i)$ of the Hilbert space $L^2(\Sd,\sd)$.  
In this setting, the random variable $Y$ is thus chosen as $Y = (\phi_i(\theta))_{i = 1,\hdots,p}$, with $\theta \sim \sd$. In practice, the number $p$  is chosen as $p = L_{n,d} = \displaystyle\sum\limits_{l = 1}^n N(d,2l)$, the number of spherical harmonics of even degree up to $2n$, with $N(d,n) = (2n +d -2)\frac{(n + d -3)!}{(d-2)!n!}$ the number of spherical harmonics of degree $n$ in dimension $d$.




\citet{leluc2024slicedwassersteinestimationsphericalharmonics} then compute the  solution $(SHCV_{M,n}^2,\beta_M)$ of~\eqref{eq:leastSquares} on a sample $(\theta_1,\hdots,\theta_M)$ and use the control variates estimator $SHCV_{M,n}^2$ as  estimator of the (squared) Sliced Wasserstein distance. 
They prove the following convergence property.
\begin{Prop}
Let $\mu,\nu$ be two discrete measures in $\R^d$ with finite moments of order 2 and let $d \geq 2$.
For any sequence of degrees $n = (n_M)_M$ such that $n_M = o\bigg(M^{1/\big(2(d-1)\big)}\bigg)$ as $M\longrightarrow +\infty$, we have
\begin{equation}\label{eq:SHCVConv}
\big|SHCV_{M,n}^2(\mu,\nu) - \SW(\mu,\nu)\big| = \mathcal{O}_{\mathbb{P}}\left(\frac{1}{n M^{1/2}}\right), 
\end{equation}
where the notation $X_n = \mathcal{O}_{\mathbb{P}}(a_n)$ means that the sequence $\frac{X_n}{a_n}$ is stochastically bounded~\footnote{The notation $X_n = \mathcal{O}_{\mathbb{P}}(a_n)$ means that for all $\epsilon > 0$, there exists finite $K >0 $ and $N >0$ such that $\mathbb{P}[|X_n| > K a_n ]< \epsilon$ for all $n > N$.}.\end{Prop}


Notice that since $n_M = o\bigg(M^{1/\big(2(d-1)\big)}\bigg)$, in high dimensions $d$ the global convergence rate is similar to that of the classical Monte Carlo method described in \autoref{sec:unif}.
