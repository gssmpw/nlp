\section{Experimental results} \label{sec:Exp}

This section presents experimental results from all the different 
sampling strategies 
{presented in }
\autoref{sec:Sampling}, on 
{a variety of}
datasets. {To provide representative results, we select
datasets spanning a range of dimensions going from 2 to $28\times 28$. Those 
include a toy dataset and three "real-life" ones.}
We first present results on Gaussian mixtures in the following dimensions 
$\lbrace 2, 3, 5, 10, 20, 50\rbrace$, the six ground truths (true distances) are 
estimated using 
{$10^8$}
projections.
{Secondly, we show some 
{dimensionality}
reduction results on 12 different datasets of persistence diagrams 
{(}for the case of $2$ dimensional discrete measures{)}. Then we 
show some convergence results in the specific case of measures in $3$ 
{dimensions.}
{Specifically, }
we compare different 
{estimations}
of 
{the}
Sliced Wasserstein distance 
between $3$D point 
{clouds}
taken from the ShapeNetCore dataset, see 
\citep{chang2015shapenetinformationrich3dmodel}. Finally we compare
different 
{dimensionality}
reduction results on the MNIST dataset 
\citep{lecun1998mnist}.} For the experiments on the Gaussian mixtures we compare 
the listed strategies with the following sampling numbers $\lbrace 100, 300, 
500, 700, 900, 1100, 2100, 3100, 4100, 5100, 6100, 7100, 8100, 9100, 
10100\rbrace$. Otherwise, we use the following sampling numbers $\{100, 1100, 
2100, 3100, 4100, 5100, 6100, 7100, 8100, 9100, 10100\}${.} 
{\autoref{Tab:legend} displays the acronyms of all the sampling methods 
compared in the following experiments.} 
{{For each sampling method from}
\autoref{Tab:legend}, there are two {variants} finishing 
{with the 
term} "Area Mapped" and two {variants} finishing 
{with the term}
"Normalized Mapped". 
The first one means that we applied the equal area mapping detailed in 
\autoref{par:ldsSphere}. The second one means we normalize each point generated 
by those methods, this normalization method is also detailed in 
\autoref{par:ldsSphere}.}


\begin{table}[h!]
\centering
 \resizebox{0.6\linewidth}{!}{
 \begin{tabular}{ |p{7cm}||r|r|  }
  \hline
  Name &  Legends & {Dimensions}\\
  \hline
  Riesz Randomized & R.R. & {2,3,5,10,20,50}  \\
  Uniform Sampling & U.S. & {2,3,5,10,20,50}\\
  Othornormal Sampling & O.S. & {2,3,5,10,20,50}\\
  Halton Area Mapped & H.A.M. & {2,3}\\
  Halton Randomized Area Mapped & H.R.A.M. & {3}\\
  Halton Normalized Mapped & H.N.M. & {5,10,20,50} \\
  Halton Randomized Normalized Mapped & H.R.N.M & {5,10,20,50} \\
  Fibonacci Point Set & F.P.S. & {3} \\
  Fibonacci Randomized Point Set & F.R.P.S. & {3} \\
  Sobol Area Mapped & S.A.M. & {3}\\
  Sobol Randomized Area Mapped & S.R.A.M. & {3} \\
  Sobol Normalized Mapped & S.N.M. & {5,10,20}\\
  Sobol Randomized Normalized Mapped & S.R.A.M. & {5,10,20} \\
  Spherical Harmonics Control Variates & S.H.C.V. & {3,5,10,20}\\
  {Spherical Sliced Wasserstein Randomized} & {S.S.W.R.} & {3,5,10,20,50}\\
  \hline
 \end{tabular}}
 \caption{{For each method used in this experimental part, associated  acronym, and list of dimensions where this method is used}.}
 \label{Tab:legend}
\end{table}

\subsection{Implementation of the sampling methods} \label{sec:implementation}

This section provides details on the implementations used for the sampling methods, and specifies how the parameters are set. The implementations used are grouped and are available here \url{https://anonymous.4open.science/r/SW-Sampling-Guide-C157/README.md}.
\begin{itemize}

\item \textbf{Classical M.C. methods: }For both methods we used python included functions to sample a Gaussian variable and to sample orthogonal matrices in $d$ dimension. For sampling orthogonal matrices we use the following python library scipy.stats.ortho\_group  \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ortho_group.html}.

\item \textbf{Halton \& Sobol sequences: }In dimension 3 and less, we use python implementations from the library scipy.stats.qmc (\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Halton.html} \& \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Sobol.html}. As for the parameters we set "scramble" to True to get the randomized version.
For high dimensions, we use \citet{leluc2024slicedwassersteinestimationsphericalharmonics}'s implementation available here \url{https://github.com/RemiLELUC/SHCV}.
\begin{Rk}
{
For the Sobol sequence, we noticed that the implementation provided by \citet{leluc2024slicedwassersteinestimationsphericalharmonics} cannot be used in dimension higher than 20.
}
\end{Rk}

\item \textbf{Riesz point configuration: }{We use a code provided by Fran√ßois Clement (\url{https://sites.math.washington.edu/~fclement/}),} implementing a projected gradient 
descent method, where we choose the number of iterations as $T = 10$, the gradient 
step as
{1}
and $s = 0.1$. {The function can be found in the riesz\_noblur.py script in the repository \url{https://anonymous.4open.science/r/SW-Sampling-Guide-C157/README.md}.}

\item {\textbf{Spherical Sliced Wasserstein: } We used the following implementation from \citet{bonet2023sphericalslicedwasserstein} that can be found in POT library (Python Optimal Transport) \url{https://pythonot.github.io/auto_examples/backends/plot_ssw_unif_torch.html}.
For the hyper-parameters we set the number of iteration $T = 250$, the learning rate $\epsilon = 150$ and the number of great circles $L = 500$. For the initialization, we generate $\theta_1,\hdots,\theta_M\sim \sd$ following the method described in \autoref{sec:unif}.}

\item \textbf{Spherical Harmonics Control Variates: }We use the implementation provided by \citet{leluc2024slicedwassersteinestimationsphericalharmonics}, available in \url{https://github.com/RemiLELUC/SHCV}. {They provide two possible functions}  \textbf{SHCV} and \textbf{SW\_CV}, both functions return a value of a SW estimate. {These functions differ in the way they implement the optimization of~\autoref{eq:leastSquares}. Depending on the number of control variates, one of the functions is much more efficient than the other. For this reason, in our experiments, we use both functions and always keep only the minimal error among the two.} 

\end{itemize}

\subsection{Gaussian data} \label{sec:GaussExp}

{This part details the experiments on a toy dataset chosen because it is simple to replicate and simple to understand.}
{We} compare different estimates of $\SW(\mu_d, \nu_d)$ for $d\in \lbrace 
2,3,5,10,20,50\rbrace$. We pick up 
\citet{leluc2024slicedwassersteinestimationsphericalharmonics}'s setting, using 
$\mu_d = \frac{1}{N}\sum\limits_{i=1}^{N} \delta_{x_i}$ and $\nu_d = 
\frac{1}{N}\sum\limits_{i=1}^{N} \delta_{y_i}$ with $x_1,\hdots,x_N\sim 
\mathcal{N}(x, \textbf{X})$, $y_1,\hdots,y_N \sim \mathcal{N}(y, \textbf{Y})$, 
where $N = 1000$. The means are {drawn as} $x,y\sim \mathcal{N}(\mathbb{1}_d, 
I_d)$ and the covariances are $X,Y= \Sigma_x\Sigma_x^T, \Sigma_y\Sigma_y^T$ 
where all entries of the matrices are drawn {using} the standard normal 
distribution. In \autoref{fig:comparisonConvRateGauss}, we show  
convergence curves generated by all the different sampling strategies 
{in all the}
dimensions listed above. 
{\autoref{fig:comparisonTimerRateSWIncludedGauss} reports the 
distance estimation error as a function of computation time (in seconds).}
In 
both figures, both axes are log scaled. We can see in 
\autoref{fig:comparisonConvRateGauss} that up to dimension 5, Q.M.C. methods are 
preferable convergence wise, then the orthonormal sampling is preferable in 
dimension 20 and 50. 
{In contrast,}
we can see in \autoref{fig:comparisonTimerRateSWIncludedGauss} that for 
{dimensions} less than 10,  
{the S.H.C.V. method has a better error,}
with 
similar running time. 
{For}
higher 
{dimensions}, 
{however,}
the orthonormal sampling is 
{much faster,}
{for a given error target.}

\begin{Rk}
One may notice in \autoref{tikz:toyConvRate2} that both the 
{S.H.C.V.}
method and the Q.M.C. method with the s-Riesz points {(R.R.)} reach a 
plateau at around $10^3$ projections. Our hypothesis is that both methods have a 
better estimation of $\SW$ than the simple random sampling with {$10^8$} 
projections that we use as a ground truth. new{We test this hypothesis in a simple case where 
$\SW(\mu,\nu)$ can be computed explicitly. We define $\mu = 
\frac{1}{2}[\delta_{x_1} + \delta_{x_2}]$  and $\nu = \frac{1}{2}[\delta_{y_1} + 
\delta_{y_2}]$, with $ x_1,x_2 = (1,0,0)^T, (0,-1,0)^T$ and $y_1,y_2 = 
(0,0,1)^T,(0,0,-1)^T$.
Simple computations yield $\SW(\mu,\nu) = \frac{2(\pi - 
\sqrt{2})}{3\pi}$. Knowing the true value of $\SW(\mu,\nu)$, we find that with 
$10^4$ points, {the Q.M.C. method with the s-Riesz points configuration and 
the S.H.C.V. methods already have errors one order smaller that ones made by uniform sampling with $10^8$ points.}}
\end{Rk}

\begin{figure}[h!]
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples},
ylabel={Error}, legend pos = outer north east, legend cell align={left},ytick={0.01 , 0.00001 , 0.00000001 , 0.00000000001 } , yticklabels = {$10^{-2}$,$10^{-5}$,$10^{-8}$}, xmode= log, ymode= log]
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/new_errors_riesz_smoothed_in_2D.csv};
\addlegendentry{R.R.}
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_unif_in_2D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_ortho_in_2D.csv};
\addlegendentry{O.S.} 
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonarea_in_2D.csv};
\addlegendentry{H.A.M.} 
\end{axis}
\end{tikzpicture}
\caption{2D}
\label{tikz:toyConvRate1}
\end{subfigure}
\hspace{1.7cm}
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sphereharmonics_concatenated_in_3D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/new_errors_riesz_smoothed_in_3D.csv};
\addlegendentry{R.R.} 
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_unif_SWIncluded_in_3D.csv};
\addlegendentry{U.S.} 
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_ortho_SWIncluded_in_3D.csv};
\addlegendentry{O.S.} 
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonarea_smoothed_SWIncluded_in_3D.csv};
\addlegendentry{H.A.M.}  
\addplot[curve11] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonarearand_SWIncluded_in_3D.csv};
\addlegendentry{H.R.A.M.} 
\addplot[curve12] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_fib_SWIncluded_in_3D.csv};
\addlegendentry{F.P.S.} 
\addplot[curve13] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_fib_SWIncluded_rand_in_3D.csv};
\addlegendentry{F.R.P.S.} 
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobolarea_smoothed_SWIncluded_in_3D.csv};
\addlegendentry{S.A.M.} 
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobolarearand_SWIncluded_in_3D.csv};
\addlegendentry{S.R.A.M.} 
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_SSWrand_in_3D.csv};
\addlegendentry{S.S.W.R.} 
\end{axis}
\end{tikzpicture}
\caption{3D}
\label{tikz:toyConvRate2}
\end{subfigure}
\\

\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples},
ylabel={Error}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sphereharmonics_concatenated_in_5D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/new_errors_riesz_smoothed_in_5D.csv};
\addlegendentry{R.R.}
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_unif_SWIncluded_in_5D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_ortho_SWIncluded_in_5D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_halton_smoothed_leluc_SWIncluded_in_5D.csv};
\addlegendentry{H.N.M.}
\addplot[curve11] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonrand_leluc_in_5D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobol_leluc_in_5D.csv};
\addlegendentry{S.N.M.} 
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobolrand_leluc_in_5D.csv};
\addlegendentry{S.R.N.M.}
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_SSWrand_in_5D.csv};
\addlegendentry{S.S.W.R.} 
\end{axis}
\end{tikzpicture}
\caption{5D}
\label{tikz:toyConvRate3}
\end{subfigure}
\hspace{1.7cm}
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sphereharmonics_concatenated_in_10D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/new_errors_riesz_smoothed_in_10D.csv};
\addlegendentry{R.R.}
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_unif_SWIncluded_in_10D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_ortho_SWIncluded_in_10D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_halton_smoothed_leluc_SWIncluded_in_10D.csv};
\addlegendentry{H.N.M.} 
\addplot[curve11] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonrand_leluc_in_10D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobol_leluc_in_10D.csv};
\addlegendentry{S.N.M.} 
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobolrand_leluc_in_10D.csv};
\addlegendentry{S.R.N.M.}
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_SSWrand_in_10D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{10D}
\label{tikz:toyConvRate4}
\end{subfigure}
\\

\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples},
ylabel={Error}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sphereharmonics_concatenated_in_20D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/new_errors_riesz_smoothed_in_20D.csv};
\addlegendentry{R.R.}
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_unif_SWIncluded_in_20D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_ortho_SWIncluded_in_20D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_halton_smoothed_leluc_SWIncluded_in_20D.csv};
\addlegendentry{H.N.M.} 
\addplot[curve11] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonrand_leluc_in_20D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobol_leluc_in_20D.csv};
\addlegendentry{S.N.M.} 
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_sobolrand_leluc_in_20D.csv};
\addlegendentry{S.R.N.M.} 
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_SSWrand_in_20D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{20D}
\label{tikz:toyConvRate5}
\end{subfigure}
\hspace{1.7cm}
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/new_errors_riesz_smoothed_in_50D.csv};
\addlegendentry{R.R.}
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_unif_in_50D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_ortho_in_50D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_halton_smoothed_leluc_in_50D.csv};
\addlegendentry{H.N.M.} 
\addplot[curve11] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_haltonrand_leluc_in_50D.csv};
\addlegendentry{H.R.N.M.} 
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/ErrorsToy/errors_SSWrand_in_50D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{50D}
\label{tikz:toyConvRate6}
\end{subfigure}
\caption{Comparison of convergence rate results 
{for}
the 
{studied}
sampling methods 
{(Gaussian data, \autoref{sec:GaussExp}).}
}
\label{fig:comparisonConvRateGauss}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h!]
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Timer},
ylabel={Error}, legend pos = outer north east, legend cell align={left}, ytick={0.01 , 0.00001 , 0.00000001 , 0.00000000001 } , yticklabels = {$10^{-2}$,$10^{-5}$,$10^{-8}$,} ,xmode= log, ymode= log]
\addplot[curve3] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__riesz_smoothedin_2D.csv};
\addlegendentry{R.R.}d 
\addplot[curve4] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__unifin_2D.csv};
\addlegendentry{U.S.} 
\addplot[curve5] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__orthoin_2D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__haltonarea_smoothedin_2D.csv};
\addlegendentry{H.A.M.}
\end{axis}
\end{tikzpicture}
\caption{2D}
\label{tikz:toyTimerSWIncludedRate1}
\end{subfigure}
\hspace{1.7cm}
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Timer}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sphereharmonics_concatenated__in_3D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve4] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__unifin_3D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__orthoin_3D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__haltonarea_smoothedin_3D.csv};
\addlegendentry{H.A.M.} 
\addplot[curve11] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__haltonarearandin_3D.csv};
\addlegendentry{H.R.A.M.} 
\addplot[curve12] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__fibin_3D.csv};
\addlegendentry{F.P.S.} 
\addplot[curve13] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__fibrand_in_3D.csv};
\addlegendentry{F.R.P.S.} 
\addplot[curve8] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__sobolarea_smoothedin_3D.csv};
\addlegendentry{S.A.M.} 
\addplot[curve9] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobolarearand_in_3D.csv};
\addlegendentry{S.R.A.M.} 
\addplot[curve7] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SSWrand_in_3D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{3D}
\label{tikz:toyTimerSWIncludedRate2}
\end{subfigure}
\\

\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Timer},
ylabel={Error}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sphereharmonics_concatenated__in_5D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve4] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__unifin_5D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__orthoin_5D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__halton_smoothed_lelucin_5D.csv};
\addlegendentry{H.N.M.}
\addplot[curve11] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_haltonrand_leluc_in_5D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve8] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobol_leluc_in_5D.csv};
\addlegendentry{S.N.M.}
\addplot[curve9] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobolrand_leluc_in_5D.csv};
\addlegendentry{S.R.N.M.}
\addplot[curve7] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SSWrand_in_5D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{5D}
\label{tikz:toyTimerSWIncludedRate3}
\end{subfigure}
\hspace{1.7cm}
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Timer}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sphereharmonics_concatenated__in_10D.csv};
\addlegendentry{S.H.C.V.}
\addplot[curve4] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__unifin_10D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__orthoin_10D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__halton_smoothed_lelucin_10D.csv};
\addlegendentry{H.N.M.}
\addplot[curve11] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_haltonrand_leluc_in_10D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve8] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobol_leluc_in_10D.csv};
\addlegendentry{S.N.M.}
\addplot[curve9] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobolrand_leluc_in_10D.csv};
\addlegendentry{S.R.N.M.}
\addplot[curve7] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SSWrand_in_10D.csv};
\addlegendentry{S.S.W.R.} 
\end{axis}
\end{tikzpicture}
\caption{10D}
\label{tikz:toyTimerSWIncludedRate4}
\end{subfigure}
\\

\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Timer},
ylabel={Error}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve1] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sphereharmonics_concatenated__in_20D.csv};
\addlegendentry{S.H.C.V.} 
\addplot[curve4] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__unifin_20D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__orthoin_20D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__halton_smoothed_lelucin_20D.csv};
\addlegendentry{H.N.M.}
\addplot[curve11] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_haltonrand_leluc_in_20D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve8] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobol_leluc_in_20D.csv};
\addlegendentry{S.N.M.}
\addplot[curve9] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_sobolrand_leluc_in_20D.csv};
\addlegendentry{S.R.N.M.}
\addplot[curve7] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SSWrand_in_20D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{20D}
\label{tikz:toyTimerSWIncludedRate5}
\end{subfigure}
\hspace{1.7cm}
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Timer}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log]
\addplot[curve4] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__unifin_50D.csv};
\addlegendentry{U.S.}
\addplot[curve5] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__orthoin_50D.csv};
\addlegendentry{O.S.}
\addplot[curve10] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SWIncluded__halton_smoothed_lelucin_50D.csv};
\addlegendentry{H.N.M.}
\addplot[curve11] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_haltonrand_leluc_in_50D.csv};
\addlegendentry{H.R.N.M.}
\addplot[curve7] table [x=Timers, y=Error, col sep=comma] {Experiments/ErrorsToy/combined_SSWrand_in_50D.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{50D}
\label{tikz:toyTimerSWIncludedRate6}
\end{subfigure}
\caption{{Distance estimation error as a function of computation time 
(seconds). Computation times include the point generation as well as the 
$\SW$ distance approximation.}}
\label{fig:comparisonTimerRateSWIncludedGauss}
\end{figure}

\begin{Rk}
Note that for the running time curves, we do not include the s-Riesz 
points configuration starting from the dimension 3 because it takes around 
$10^2$ seconds to generate $10^3$ points and $9\times 10^3$ seconds to generate 
$10^4$ points. {However, observe that those points, once generated, can be stored once for all to compute 
other $\SW$ distances or any other Monte Carlo estimation problems for functions defined 
on the unit sphere. This means that these configurations should not be discarded by default. 
For practical applications where the number of $\SW$ distances to compute is large, the computing time for these configurations can be factorized by the number of distances to compute and hence could become a negligible factor} {when the sampling number is moderate.}
\end{Rk}

\begin{Rk}
{Recalling the running time complexity $\mathcal{O}(TM^2)$ in \autoref{par:Riesz} and the running time results above, this shows that one needs to spend $9\times 10^7$ seconds to generate $10^6$ points. This demonstrates the limitation of this sampling method in terms of scalability, in other words when one needs a very large sampling number.}
\end{Rk}


\subsection{Persistence diagrams reduction dimension score}
\label{sec:tda}

{The goal of this section is to evaluate the relevance of the sampling 
methods studied in \autoref{sec:Sampling}, in the context of a concrete use 
case, involving two-dimensional real-life datasets. For that,
we focus in this section on 
\emph{persistence diagrams}, a popular object used in Topological Data Analysis 
\citep{edelsbrunner09}.}
Persistence diagrams are data abstractions encapsulating the features of 
interest {of complex input datasets (e.g. scalar fields)}
into 
simple {two-dimensional} representations. 
{Specifically, we consider an input dataset represented }
as a piecewise linear (PL) scalar field, namely a function $f: \M 
\rightarrow \R$ defined on a PL $(d_{\M})$-manifold $\M$ with $d_{\M} \leq 3$. 
Take a value $a\in\R$, we denote $\subf(a) = f^{-1}(]-\infty,a])$ the sub-level 
set of $f$ at $a$. While increasing $a$, the topology of $\subf(a)$ changes at 
the critical points of $f$ in $\M$. Those critical points are classified by 
their index $\I$: 0 for minima, 1 for 1-saddles, $d_{\M} - 1$ for 
$(d_{\M}-1)$-saddles and $d_{\M}$ for maxima. Following the Elder rule 
\citep{edelsbrunner09}, a topological feature of $\subf(a)$ (connected 
component, cycle, void) is associated with a pair of critical points $(c,c')$ 
such that $f(c) < f(c')$ and $\I_{c} = \I_{c'} - 1$.  This pair corresponds to 
{the}
\textit{birth} and \textit{death} 
{of the topological feature}
during the 
{sweep of the range from $-\infty$ to 
$+\infty$ by $a$,} 
and {it} is called a \textit{persistence pair}. As an example, when two 
connected components of $\subf(a)$ merge at a critical point $c'$, the younger 
one (created last) \textit{dies} to let the older one (created first) live on. 
Then those persistence pair{s} are represented as $2$D points where the 
horizontal coordinate corresponds to the \textit{birth} of a topological feature 
(noted $b = f(c)$) and where the vertical one corresponds to its \textit{death} 
(noted $d = f(c')$). The lifespan of a feature is called \textit{persistence} 
and is simply encoded as $b-d$. This representation is called the 
\textit{Persistence Diagram}, and its popularity in topological data analysis is 
explained by its stability to the addition of noise. See \autoref{fig:diagGauss} 
for a simple example of a persistence diagram.

\begin{Rk}
new{
Two persistence diagrams can have {a} different number of points, so to 
make it a balanced transport problem one has to augment them. Formally, denoting 
$d_1 =
\frac{1}{N_1}\sum\limits_{k=1}^{N_1} \delta_{x_k}$ , $d_2 = \frac{1}{N_2}\sum\limits_{k=1}^{N_2} \delta_{y_k}$ the diagrams, and noting $\Delta_{d_1} = \frac{1}{N_1}\sum\limits_{k=1}^{N_1} \delta_{\pi_{\Delta}(x_k)}$, $\Delta_{d_2} = \frac{1}{N_2}\sum\limits_{k=1}^{N_1} \delta_{\pi_{\Delta}(y_k)}$ their projections on the diagonal $\Delta$, one considers $\mu = \frac{1}{N} [N_1 d_1 + N_2 \Delta_{d_2}]$ and $\nu = \frac{1}{N} [N_2 d_2 + N_1 \Delta_{d_1}]$ as input measures with $N = N_1 + N_2$. Then the Sliced Wasserstein distance can be used to compare persistence diagrams as detailed by \citet{carriere2017slicedwassersteinkernelpersistence}.}
\end{Rk}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{Experiments/IllustDiag.png}
\caption{A simple example of a persistence diagram issued from a gaussian 
mixture {(left).}
{On}
the right you can see that the persistence diagram 
is stable to the addition of noise.}
\label{fig:diagGauss}
\end{figure}

\noindent We present dimensionality reduction results on 12 
{ensembles}
of persistence diagrams \citep{ensembleBenchmark} described in 
\citep{pont_vis21}, which original scalar fields include simulated and acquired 
2D and 3D ensembles from SciVis constests \citep{scivisOverall}. {The 
dimensionality reduction techniques used are MDS \citep{kruskal78} and t-SNE 
\citep{tSNE} applied on distance matrices obtained by the SW estimations 
{between the persistence diagrams}. For a given technique, one 
quantifies its ability to preserve the cluster structure of an ensemble by 
running 
{the}
$k$-means algorithm in the 
{resulting}
2D-layouts.  
Then one evaluates the quality 
of the clustering with the normalized mutual information (NMI) and adjusted rand 
index (ARI){, which should both be equal to $1$ for a clustering that 
is identical to the classification ground-truth}.} \autoref{tab:diagDimReduct} 
shows the average clustering scores of both MDS \citep{kruskal78} and t-SNE 
\citep{tSNE}. First we take the average from distance matrices made by each 
$\SW$ estimates on all sampling number $\{100, 1100, 2100, 3100, 4100, 5100, 
6100, 7100, 8100, 9100, 10100\}$. Then we average again over all the 12 
different 
{ensembles}
of persistence diagrams. One can see that all the methods are 
quite similar. But overall the s-Riesz points configuration, which are just the 
$M$-th unity roots up to a rotation, is slightly better.



\begin{table}[h!]
 \caption{Average NMI and ARI scores for over all 12 
 {ensembles}
 of persistence diagrams.}
 \resizebox{0.45\linewidth}{!}{
 \begin{tabular}{ |p{3cm}||r|r|  }
  \hline
  Method&  MDS NMI & t-SNE NMI \\
  \hline
  Riesz & 0.74 & 0.65 \\
  Uniform & 0.74 & 0.59  \\
  Orthonormal & 0.75 & 0.63   \\
  Halton& 0.74 & 0.58 \\
  \hline
 \end{tabular}}
 \hfill
  \resizebox{0.45\linewidth}{!}{
 \begin{tabular}{ |p{3cm}||r|r|  }
  \hline
  Method&  MDS ARI & t-SNE ARI \\
  \hline
  Riesz & 0.64 & 0.51 \\
  Uniform & 0.64 & 0.44  \\
  Orthonormal & 0.64 & 0.48  \\
  Halton& 0.63 & 0.41 \\
  \hline
 \end{tabular}}
 \label{tab:diagDimReduct}
\end{table}


\subsection{3D Shapenet 55Core Data}

{This part details convergence results on a 3D dataset commonly used 
{as a benchmark when studying}
shape comparison {techniques}.} {So} as in 
\citep{nguyen2024quasimonte} and 
\citep{leluc2024slicedwassersteinestimationsphericalharmonics}, we took three 3D 
point clouds issued from the ShapenetCore dataset introduced by 
\citep{chang2015shapenetinformationrich3dmodel}. Among the different shapes in 
the dataset, we took one lamp, one plane and one bed; with all three of them 
having $N=2048$ points. \autoref{fig:shapes} {displays} the three 
data{sets} 
{considered} for this experiment.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.04]{Experiments/plane.jpg}
\includegraphics[scale=0.04]{Experiments/lamp.jpg}
\includegraphics[scale=0.04]{Experiments/bed.jpg}
\caption{The three 
{point clouds}
taken from the ShapenetCore 
{dataset}
{(}a plane, a lamp and a bed{)}.}
\label{fig:shapes}
\end{figure}



\begin{figure}[h!]
\centering
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples},
ylabel={Error}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log,width = 0.3\textwidth, height = 0.3\textwidth]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_sphereharmonics_shapenet_1.csv};
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_unif_shapenet_1.csv};
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_ortho_shapenet_1.csv};
\addplot[curve6] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_halton_smoothed_shapenet_1.csv};
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_haltonarearand_shapenet_1.csv};
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_fib_shapenet_1.csv};
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_fib_rand_shapenet_1.csv};
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/new_errors_riesz_rand_shapenet_1.csv};
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_SSW_rand_shapenet_1.csv};
\end{axis}
\end{tikzpicture}
\hskip 5pt
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log,width = 0.3\textwidth, height = 0.3\textwidth]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_sphereharmonics_shapenet_2.csv};
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/new_errors_riesz_rand_shapenet_2.csv};
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_unif_shapenet_2.csv};
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_ortho_shapenet_2.csv};
\addplot[curve6] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_halton_smoothed_shapenet_2.csv};
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_haltonarearand_shapenet_2.csv};
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_fib_shapenet_2.csv};
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_fib_rand_shapenet_2.csv};
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_SSW_rand_shapenet_2.csv};
\end{axis}
\end{tikzpicture}
\hskip 5pt
\begin{tikzpicture}
\begin{axis}[group style={group name=plots,},xlabel={Number of samples}, legend pos = outer north east, legend cell align={left}, xmode= log, ymode= log,width = 0.3\textwidth, height = 0.3\textwidth]
\addplot[curve1] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_sphereharmonics_shapenet_3.csv};
\addlegendentry{S.H.C.V.}
\addplot[curve3] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/new_errors_riesz_rand_shapenet_3.csv};
\addlegendentry{R.R}
\addplot[curve4] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_unif_shapenet_3.csv};
\addlegendentry{U.S}
\addplot[curve5] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_ortho_shapenet_3.csv};
\addlegendentry{O.S}
\addplot[curve6] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_halton_smoothed_shapenet_3.csv};
\addlegendentry{H.A.M}
\addplot[curve7] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_haltonarearand_shapenet_3.csv};
\addlegendentry{H.R.A.M}
\addplot[curve8] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_fib_shapenet_3.csv};
\addlegendentry{F.P.S}
\addplot[curve9] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_fib_rand_shapenet_3.csv};
\addlegendentry{F.R.P.S}
\addplot[curve10] table [x=N_sample, y=Error, col sep=comma] {Experiments/Errors3DShape/errors_SSW_rand_shapenet_3.csv};
\addlegendentry{S.S.W.R.}
\end{axis}
\end{tikzpicture}
\caption{Comparison of convergence rate results from the different sampling methods. The first plot shows errors made with respect to the $\SW$ distance between a lamp and a plane. The second one is between a plane and a bed. The last one corresponds to $\SW$ between a plane and a bed.}
\label{fig:comparisonConvRateShape}
\end{figure}

\autoref{fig:comparisonConvRateShape} shows different convergence curves of 
Sliced Wasserstein estimates between the three point clouds.
As in \autoref{sec:GaussExp}, the methods dominating are the Q.M.C., R.Q.M.C., S.S.W.
and 
{S.H.C.V.}
methods, especially the s-Riesz points configuration {and the Spherical Sliced Wasserstein sampling}.

\subsection{MNIST reduction dimension score} \label{sec:MNIST}

{The goal of this section is {twofold}. 
{First, it evaluates the practical convergence of the studied sampling 
methods on real-life high-dimensional datasets. Second, it describes an 
application of the SW distance for high-dimensional data, namely, 
dimensionality reduction.}
{For this, we select the classical MNIST dataset \citep{lecun1998mnist}.} 
{To construct our dataset, we represent each digit image as a point in 
$\R^{28 \times 28}$. For each class $\lbrace 0,1,2,3,4,5,6,7,8,9\rbrace$, we 
select {randomly} 600 digit images and divide them into groups of 200. This results 
in 30 point clouds {of $200$ points each, in $\R^{28 \times 28}$}, with 
{10 ground-truth classes.}
\autoref{fig:MNISTMatrix} illustrates the {$30\times 30$ matrix of $SW$ distances 
between all point clouds in the dataset}. new{We use MDS and t-SNE to produce 2D 
layouts from the distance matrices generated by the various sampling methods 
with different sample sizes. We then apply a clustering algorithm to these 2D 
layouts and average the clustering scores (NMI and ARI{, see 
\autoref{sec:tda}}) on all sampling numbers for all 
the 
{studied}
sampling strategies.} {Results are provided in \autoref{Tab:MNISTScore}. In such high dimension ($d=784$), we see that the performance of L.D.S. collapse, the three
sampling methods standing out being the s-Riesz points configuration, the uniform 
sampling and the orthonormal sampling}. 
\begin{figure}[h!]
\centering
\includegraphics[scale=0.1]{Experiments/distMatrixMNIST.png}
\caption{Sliced Wasserstein distance matrix of our dataset using $10^6$ projections. All 10 classes, $\{0,1,2,3,4,5,6,7,8,9\}$, of 3 members each are well represented in the matrix.}
\label{fig:MNISTMatrix}
\end{figure}

\begin{table}[h!]
 \caption{Average NMI and ARI scores with standard deviation. Higher scores correspond to better clustering.}
 \resizebox{0.45\linewidth}{!}{
 \begin{tabular}{ |p{3cm}||r|r|  }
  \hline
  Method&  MDS NMI & t-SNE NMI \\
  \hline
  Riesz & 1 $\pm$ 0. & 0.98 $\pm$ 2e-2 \\
  Uniform & 1 $\pm$ 0. & 0.97 $\pm$ 4e-2 \\
  Orthonormal & 1 $\pm$ 0. & 0.98 $\pm$ 3e-2  \\
  Halton& 0.91 $\pm$ 1e-1 & 0.91 $\pm$ 9e-2\\
 {S.S.W.} & {1 $\pm$ 0.} & {0.98 $\pm$ 4e-2}\\
  \hline
 \end{tabular}}
 \hfill
  \resizebox{0.45\linewidth}{!}{
 \begin{tabular}{ |p{3cm}||r|r|  }
  \hline
  Method&  MDS ARI & t-SNE ARI \\
  \hline
  Riesz & 1 $\pm$ 0. & 0.95 $\pm$ 7e-2 \\
  Uniform & 1 $\pm$ 0. & 0.91 $\pm$ 1e-1  \\
  Orthonormal & 1 $\pm$ 0. & 0.94 $\pm$ 8e-2  \\
  Halton& 0.75 $\pm$ 2e-1 & 0.76 $\pm$ 2e-1 \\
 {S.S.W.} & {1 $\pm$ 0.} & {0.94 $\pm$ 1e-1}\\
  \hline
 \end{tabular}}
 \label{Tab:MNISTScore}
\end{table}
