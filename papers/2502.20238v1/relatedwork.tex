\section{Related Work}
\paragraph{LLM Reasoning.}
Advancing the reasoning capabilities of large language models is a critical goal in natural language processing~\cite{wos1992automated,yang2018hotpotqa}. 
In recent years, LLMs, combined with prompting techniques such as Chain of Thought~\cite{wei2022chain}, Tree of Thought~\cite{yao2023tree}, and Self-Consistency~\cite{wang2023selfconsistency}, have shown remarkable performance in various reasoning tasks~\cite{cobbe2021training,srivastava2022beyond}.
Current evaluation methods focus mainly on the final accuracy in reasoning-intensive domains, including mathematics~\cite{cobbe2021training,math,chen-etal-2023-theoremqa,rein2023gpqa,VisAidMath}, coding~\cite{chen2021evaluating,austin2021program}, commonsense~\cite{mihaylov-etal-2018-suit,hendrycks2020measuring}, and logical reasoning~\cite{yao2023tree,long2023large}. However, as inference-time scaling gains importance~\cite{snell2024scaling,guo2025deepseek} and models are becoming more capable of reasoning, it is crucial to assess how effectively models perform reflection and correction during reasoning.
While \citet{tyagi-etal-2024-step} manually analyze the reasoning chains in logic puzzles, their approach lacks scalability.
Some studies~\cite{singh2024exposing,zeng2024mrben} evaluate how models handle reasoning mistakes, but these investigations often rely on rule-based mistakes that may be easily resolved by current LLMs. Moreover, these studies only assess reflection on past steps in a static manner.
In our work, we address these limitations by introducing two novel tasks designed to more accurately reflect models' capabilities in dynamic reasoning and error correction.


\paragraph{Puzzle Solving Tasks.}
Logic puzzles, which require deducing solutions from a set of rules~\cite{giadikiaroglou-etal-2024-puzzle}, are ideal for evaluating LLMs' reasoning capabilities as they rely minimally on prior knowledge~\cite{li-etal-2024-assessing-logical}.
Recent studies have explored LLMs on various puzzles with different emphases~\cite{mittal2024puzzlebench}, such as Sudoku~\cite{ishay2023leveraging,long2023large} for strategic thinking, Game of 24 ~\cite{ding2023everything,yao2023tree} for arithmetic calculations. Some investigate grid puzzles~\cite{dziri2024faith,tyagi-etal-2024-step}, crosswords~\cite{yao2023tree}, chess puzzles~\cite{feng2024chessgpt}, mazes~\cite{Noever2021PuzzleSW}, and Minesweeper~\cite{li-etal-2024-assessing-logical}.
However, evaluation remains primarily focused on final accuracy.