\section{Related Work}
\paragraph{LLM Reasoning.}
Advancing the reasoning capabilities of large language models is a critical goal in natural language processing____. 
In recent years, LLMs, combined with prompting techniques such as Chain of Thought____, Tree of Thought____, and Self-Consistency____, have shown remarkable performance in various reasoning tasks____.
Current evaluation methods focus mainly on the final accuracy in reasoning-intensive domains, including mathematics____, coding____, commonsense____, and logical reasoning____. However, as inference-time scaling gains importance____ and models are becoming more capable of reasoning, it is crucial to assess how effectively models perform reflection and correction during reasoning.
While ____ manually analyze the reasoning chains in logic puzzles, their approach lacks scalability.
Some studies____ evaluate how models handle reasoning mistakes, but these investigations often rely on rule-based mistakes that may be easily resolved by current LLMs. Moreover, these studies only assess reflection on past steps in a static manner.
In our work, we address these limitations by introducing two novel tasks designed to more accurately reflect models' capabilities in dynamic reasoning and error correction.


\paragraph{Puzzle Solving Tasks.}
Logic puzzles, which require deducing solutions from a set of rules____, are ideal for evaluating LLMs' reasoning capabilities as they rely minimally on prior knowledge____.
Recent studies have explored LLMs on various puzzles with different emphases____, such as Sudoku____ for strategic thinking, Game of 24 ____ for arithmetic calculations. Some investigate grid puzzles____, crosswords____, chess puzzles____, mazes____, and Minesweeper____.
However, evaluation remains primarily focused on final accuracy.