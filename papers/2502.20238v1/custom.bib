% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{EVANS2003454@in-two-minds,
    title = {In two minds: dual-process accounts of reasoning},
    journal = {Trends in Cognitive Sciences},
    volume = {7},
    number = {10},
    pages = {454-459},
    year = {2003},
    issn = {1364-6613},
    doi = {https://doi.org/10.1016/j.tics.2003.08.012},
    url = {https://www.sciencedirect.com/science/article/pii/S1364661303002250},
    author = {Jonathan St.B.T. Evans},
}

@book{daniel2011@thinking-fast-slow,
  title={Thinking, fast and slow},
  author={Daniel Kahneman},
  year={2011},
  isbn = {9780374275631 0374275637},
  publisher = {Farrar, Straus and Giroux},
}

@inproceedings{
yao2023tree,
title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=5Xc1ecxO1h}
}

@article{qin2024o1,
  title={O1 Replication Journey: A Strategic Progress Report--Part 1},
  author={Qin, Yiwei and Li, Xuefeng and Zou, Haoyang and Liu, Yixiu and Xia, Shijie and Huang, Zhen and Ye, Yixin and Yuan, Weizhe and Liu, Hector and Li, Yuanzhi and others},
  journal={arXiv preprint arXiv:2410.18982},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{o1,
  title={Learning to reason with LLMs},
  author={OpenAI},
  url={https://openai.com/index/learning-to-reason-with-llms/},
  year={2024}
}

@article{gpt3.5,
  title={GPT3.5 Turbo},
  author={OpenAI},
url = "https://platform.openai.com/docs/models/gpt-3-5",
  year={2022}
}

@article{geminiflash,
  title={Gemini-2.0-Flash Family},
    url = "https://deepmind.google/technologies/gemini/flash/",
  author={Google},
  year={2024}
}

@inproceedings{
shinn2023reflexion,
title={Reflexion: language agents with verbal reinforcement learning},
author={Noah Shinn and Federico Cassano and Ashwin Gopinath and Karthik R Narasimhan and Shunyu Yao},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=vAElhFcKW6}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@inproceedings{tyagi-etal-2024-step,
    title = "Step-by-Step Reasoning to Solve Grid Puzzles: Where do {LLM}s Falter?",
    author = "Tyagi, Nemika  and
      Parmar, Mihir  and
      Kulkarni, Mohith  and
      Rrv, Aswin  and
      Patel, Nisarg  and
      Nakamura, Mutsumi  and
      Mitra, Arindam  and
      Baral, Chitta",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1111/",
    doi = "10.18653/v1/2024.emnlp-main.1111",
    pages = "19898--19915"
}


@article{math,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      journal={arXiv preprint arXiv:2103.03874},
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{roelofs2019meta,
  title={A meta-analysis of overfitting in machine learning},
  author={Roelofs, Rebecca and Shankar, Vaishaal and Recht, Benjamin and Fridovich-Keil, Sara and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{DBLP:journals/corr/SpuriousCorrelations24,
  author       = {Daniel Enstr{\"{o}}m and
                  Viktor Kjellberg and
                  Moa Johansson},
  title        = {Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning
                  Shortcuts},
  journal      = {CoRR},
  volume       = {abs/2403.11314},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.11314},
  doi          = {10.48550/ARXIV.2403.11314},
  eprinttype    = {arXiv},
  eprint       = {2403.11314},
  timestamp    = {Mon, 08 Apr 2024 18:24:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-11314.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@inproceedings{
creswell2023selectioninference,
title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
author={Antonia Creswell and Murray Shanahan and Irina Higgins},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3Pf3Wg6o-A4}
}

@inproceedings{
lightman2024lets,
title={Let's Verify Step by Step},
author={Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=v8L0pN6EOi}
}

@inproceedings{wang2024lookahead,
  title={Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation},
  author={Wang, Zihan and Li, Xiangyang and Yang, Jiahao and Liu, Yeqi and Hu, Junjie and Jiang, Ming and Jiang, Shuqiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13753--13762},
  year={2024}
}

@inproceedings{agarwal2019model,
  title={Model learning for look-ahead exploration in continuous control},
  author={Agarwal, Arpit and Muelling, Katharina and Fragkiadaki, Katerina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3151--3158},
  year={2019}
}

@article{suddendorf2006foresight,
  title={Foresight and evolution of the human mind},
  author={Suddendorf, Thomas},
  journal={Science},
  volume={312},
  number={5776},
  pages={1006--1007},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@article{singh2024exposing,
  title={Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning},
  author={Singh, Joykirat and Nambi, Akshay and Vineet, Vibhav},
  journal={arXiv preprint arXiv:2406.10834},
  year={2024}
}

@inproceedings{giadikiaroglou-etal-2024-puzzle,
    title = "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    author = "Giadikiaroglou, Panagiotis  and
      Lymperaiou, Maria  and
      Filandrianos, Giorgos  and
      Stamou, Giorgos",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.646/",
    doi = "10.18653/v1/2024.emnlp-main.646",
    pages = "11574--11591",
    abstract = "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy{---}dividing puzzles into rule-based and rule-less categories{---}to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI`s logical reasoning and creative problem-solving advancements."
}

@inproceedings{li-etal-2024-assessing-logical,
    title = "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study",
    author = "Li, Yinghao  and
      Wang, Haorui  and
      Zhang, Chao",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.4/",
    doi = "10.18653/v1/2024.naacl-long.4",
    pages = "59--81",
    abstract = "Large Language Models (LLMs) have shown remarkable proficiency in language understanding and have been successfully applied to a variety of real-world tasks through task-specific fine-tuning or prompt engineering. Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data. In our research, we introduce a novel task{---}Minesweeper{---}specifically designed in a format unfamiliar to LLMs and absent from their training datasets. This task challenges LLMs to identify the locations of mines based on numerical clues provided by adjacent opened cells. Successfully completing this task requires an understanding of each cell`s state, discerning spatial relationships between the clues and mines, and strategizing actions based on logical deductions drawn from the arrangement of the cells. Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent, multi-step logical reasoning process needed to solve Minesweeper. These findings highlight the need for further research to understand the nature of reasoning capabilities in LLMs under similar circumstances, and to explore pathways towards more sophisticated AI reasoning and planning models."
}

@article{ishay2023leveraging,
  title={Leveraging large language models to generate answer set programs},
  author={Ishay, Adam and Yang, Zhun and Lee, Joohyung},
  journal={arXiv preprint arXiv:2307.07699},
  year={2023}
}

@article{long2023large,
  title={Large language model guided tree-of-thought},
  author={Long, Jieyi},
  journal={arXiv preprint arXiv:2305.08291},
  year={2023}
}

@article{ding2023everything,
  title={Everything of thoughts: Defying the law of penrose triangle for thought generation},
  author={Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2311.04254},
  year={2023}
}


@article{mittal2024puzzlebench,
  title={PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?},
  author={Mittal, Chinmay and Kartik, Krishna and Singla, Parag and others},
  journal={arXiv preprint arXiv:2402.02611},
  year={2024}
}


@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{feng2024chessgpt,
  title={Chessgpt: Bridging policy learning and language modeling},
  author={Feng, Xidong and Luo, Yicheng and Wang, Ziyan and Tang, Hongrui and Yang, Mengyue and Shao, Kun and Mguni, David and Du, Yali and Wang, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{Noever2021PuzzleSW,
  title={Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach},
  author={David A. Noever and Ryerson Burdick},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.02797},
  url={https://api.semanticscholar.org/CorpusID:237431487}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@inproceedings{mihaylov-etal-2018-suit,
    title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
    author = "Mihaylov, Todor  and
      Clark, Peter  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1260/",
    doi = "10.18653/v1/D18-1260",
    pages = "2381--2391",
    abstract = "We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic{---}in the context of common knowledge{---}and the language it is expressed in. Human performance on OpenBookQA is close to 92{\%}, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance."
}

@book{wos1992automated,
  title={Automated reasoning introduction and applications},
  author={Wos, Larry and Overbeek, Ross and Lusk, Ewing and Boyle, Jim},
  year={1992},
  publisher={McGraw-Hill, Inc.}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{
wang2023selfconsistency,
title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=1PL1NIMMrw}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@inproceedings{chen-etal-2023-theoremqa,
    title = "{T}heorem{QA}: A Theorem-driven Question Answering Dataset",
    author = "Chen, Wenhu  and
      Yin, Ming  and
      Ku, Max  and
      Lu, Pan  and
      Wan, Yixin  and
      Ma, Xueguang  and
      Xu, Jianyu  and
      Wang, Xinyi  and
      Xia, Tony",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.489/",
    doi = "10.18653/v1/2023.emnlp-main.489",
    pages = "7889--7901",
    abstract = "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90{\%} accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE{\&}CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4`s capabilities to solve these problems are unparalleled, achieving an accuracy of 51{\%} with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15{\%}, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems."
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}


@inproceedings{
kojima2022large,
title={Large Language Models are Zero-Shot Reasoners},
author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=e2TBb5y0yFf}
}

@article{kimik15,
      title={Kimi k1.5: Scaling Reinforcement Learning with LLMs}, 
      author={Kimi Team},
      year={2025},
      volume={abs/2501.12599},
      journal={ArXiv},
      url={https://arxiv.org/abs/2501.12599}, 
}

@misc{shao2024deepseekmathpushinglimitsmathematical,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03300}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and others},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@inproceedings{
yu2024metamath,
title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng YU and Zhengying Liu and Yu Zhang and James Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=N8N0hgNDRt}
}

@article{gpt4o,
      title={{GPT}-4o System Card}, 
      author={OpenAI and Aaron Hurst and Adam Lerer and Adam P. Goucher and others},
      year={2024},
      volume={abs/2410.21276},
      journal={ArXiv},
      url={https://arxiv.org/abs/2410.21276}, 
}



@article{qwen25,
      title={Qwen2.5 Technical Report}, 
      author={Qwen, An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      volume={abs/2412.15115},
      journal={ArXiv},
      url={https://arxiv.org/abs/2412.15115}, 
}

@incollection{backtracking,
title = {Chapter 4 - Backtracking Search Algorithms},
editor = {Francesca Rossi and Peter {van Beek} and Toby Walsh},
series = {Foundations of Artificial Intelligence},
publisher = {Elsevier},
volume = {2},
pages = {85-134},
year = {2006},
booktitle = {Handbook of Constraint Programming},
issn = {1574-6526},
doi = {https://doi.org/10.1016/S1574-6526(06)80008-8},
url = {https://www.sciencedirect.com/science/article/pii/S1574652606800088},
author = {Peter {van Beek}},
}

@inproceedings{zeng2024mrben,
title={{MR}-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in {LLM}s},
author={Zhongshen Zeng and Yinhong Liu and Yingjia Wan and Jingyao Li and Pengguang Chen and Jianbo Dai and Yuxuan Yao and Rongwu Xu and Zehan Qi and Wanru Zhao and Linling Shen and Jianqiao Lu and Haochen Tan and Yukang Chen and Hao Zhang and Zhan Shi and Bailin Wang and Zhijiang Guo and Jiaya Jia},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=GN2qbxZlni}
}

@article{VisAidMath,
  author       = {Jingkun Ma and
                  Runzhe Zhan and
                  Derek F. Wong and
                  Yang Li and
                  Di Sun and
                  Hou Pong Chan and
                  Lidia S. Chao},
  title        = {VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning},
  journal      = {CoRR},
  volume       = {abs/2410.22995},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.22995},
  doi          = {10.48550/ARXIV.2410.22995},
  eprinttype    = {arXiv},
  eprint       = {2410.22995},
  timestamp    = {Fri, 29 Nov 2024 21:16:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-22995.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
