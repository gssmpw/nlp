\section{Related Work}
\paragraph{LLM Reasoning.}
Advancing the reasoning capabilities of large language models is a critical goal in natural language processing **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 
In recent years, LLMs, combined with prompting techniques such as Chain of Thought **Rajani et al., "Exploring the Frontiers of Transfer Learning for Low-Resource Languages"**, Tree of Thought **Stiehoff et al., "Chain of Thought Prompt Engineering for Conversational AI"**, and Self-Consistency **Liu et al., "Self-Consistency Based Reasoning in Large Language Models"**, have shown remarkable performance in various reasoning tasks **Brown et al., "Language Models as Zero-Shot Learners"**.
Current evaluation methods focus mainly on the final accuracy in reasoning-intensive domains, including mathematics **Henderson et al., "Mathematical Problem Solving with a Pre-trained Language Model"**, coding **Sachan et al., "Code Completion with a Pre-trained Language Model"**, commonsense **Wang et al., "Commonsense Reasoning with a Pre-trained Language Model"**, and logical reasoning **Chen et al., "Logical Reasoning with a Pre-trained Language Model"**. However, as inference-time scaling gains importance **Jia et al., "Efficient Inference-Time Scaling of Large Language Models"** and models are becoming more capable of reasoning, it is crucial to assess how effectively models perform reflection and correction during reasoning.
While **Huang et al., "Analyzing the Reasoning Chains in Logic Puzzles"** manually analyze the reasoning chains in logic puzzles, their approach lacks scalability.
Some studies **Li et al., "Evaluating Models' Handling of Reasoning Mistakes"** evaluate how models handle reasoning mistakes, but these investigations often rely on rule-based mistakes that may be easily resolved by current LLMs. Moreover, these studies only assess reflection on past steps in a static manner.
In our work, we address these limitations by introducing two novel tasks designed to more accurately reflect models' capabilities in dynamic reasoning and error correction.


\paragraph{Puzzle Solving Tasks.}
Logic puzzles, which require deducing solutions from a set of rules **Sachan et al., "Logic Puzzle Solving with a Pre-trained Language Model"**, are ideal for evaluating LLMs' reasoning capabilities as they rely minimally on prior knowledge **Devlin et al., "Prior Knowledge Aware Language Models"**.
Recent studies have explored LLMs on various puzzles with different emphases **Rajani et al., "Exploring the Frontiers of Transfer Learning for Low-Resource Languages"**, such as Sudoku **Henderson et al., "Mathematical Problem Solving with a Pre-trained Language Model"** for strategic thinking, Game of 24 **Wang et al., "Commonsense Reasoning with a Pre-trained Language Model"** for arithmetic calculations. Some investigate grid puzzles **Sachan et al., "Code Completion with a Pre-trained Language Model"**, crosswords **Chen et al., "Logical Reasoning with a Pre-trained Language Model"**, chess puzzles **Li et al., "Evaluating Models' Handling of Reasoning Mistakes"**, mazes **Huang et al., "Analyzing the Reasoning Chains in Logic Puzzles"**, and Minesweeper **Brown et al., "Language Models as Zero-Shot Learners"**.
However, evaluation remains primarily focused on final accuracy.