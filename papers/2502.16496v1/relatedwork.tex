\section{RELATED WORK}
\label{sec:related work}
In this section, we introduce several representative state-of-the-art MARL algorithms, covering both the simultaneous and the sequential decision-making paradigms. 
We also discuss the difference between several types of order optimization in MARL.\looseness=-1

\noindent \textbf{Simultaneous Decision-Making MARL Algorithms.} The vast majority of \textit{Centralized Training Decentralized Execution} (CTDE) \cite{oliehoek2008optimal,foerster2018counterfactual} algorithms in MARL adopt a simultaneous decision-making paradigm. 
Here we introduce two representative ones.
MAPPO \cite{yu2022surprising} is a straightforward policy-based approach that endows the policy network of all agents with a shared set of parameters and utilizes agents' aggregated trajectories to facilitate policy optimization. 
HAPPO \cite{kuba2022trust} is a heterogeneous-agent trust-region method that employs a sequential policy update paradigm. 
During an update in HAPPO, the agents randomly choose an update order and update their own policies over the newly updated policies of previous agents. 
Due to the adoption of the simultaneous decision-making paradigm, both MAPPO and HAPPO suffer from potential action conflicts and lack coordination efficiency guarantee.\looseness=-1

\noindent \textbf{Sequential Decision-Making MARL Algorithm.}
To alleviate potential action conflicts and further enhance multi-agent coordination, \citet{wen2022multi} proposed Multi-agent Transformer (MAT), which presents an auto-regressive sequential decision-making MARL algorithm based on the \textit{Transformer} \cite{vaswani2017attention} architecture. MAT successfully transforms multi-agent joint policy optimization into a sequential decision-making process by generating actions in an agent-by-agent manner, which holds the potential for finer-grained supervision and management of inter-agent dependencies.\looseness=-1

\noindent \textbf{Different Order Optimization in MARL.}
The current MARL algorithms have started to focus on the impact of ``order'' on agent-level or batch-level updates, and several solutions have been proposed.
\citet{wang2023order} proposed an agent-by-agent policy optimization method, A2PO, which adopts a semi-greedy agent selection rule to determine agent update order within a single rollout. 
Furthermore, B2MAPO \cite{zhang2024b2mapo} establishes update batches, further enhancing algorithm efficiency to facilitate joint policy optimization in larger-scale agent clusters. 
These studies on sequential update MARL algorithms offer valuable insights, suggesting that decision order optimization can be achieved incrementally on an item-by-item basis.
Unlike these works, this paper focuses on optimizing agent decision order under the sequential decision-making paradigm, which remains an underexplored area in MARL.\looseness=-1


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%