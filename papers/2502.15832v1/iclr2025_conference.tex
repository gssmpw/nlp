
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{nth}

\definecolor{changran}{RGB}{255,100,0} % Orange color
\definecolor{yunhao}{RGB}{0,0,255} % Green color


\title{DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yi Liu, Changran Xu, Yunhao Zhou, Zeju Li, Qiang Xu \\
Department of Computer Science and Engineering\\
The Chinese University of Hong Kong\\
\texttt{\{yliu22,zjli24,qxu\}@cse.cuhk.edu.hk}\\
\texttt{\{xxuchangran,yunhaoz.cs\}@gmail.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\input{sec/0_abstract}
\input{sec/1_intro}
\input{sec/2_related_work}
\input{sec/3_method_dataset}
\input{sec/4_method_model}
\input{sec/5_experiment}
\input{sec/6_conclusion}
\newpage

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\section*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.
This work was supported in part by the General Research Fund of the Hong Kong Research Grants Council (RGC) under Grant No. 14212422 and 14202824, and in part by National Technology Innovation Center for EDA.

% \bibliography{iclr2025_conference}
% \bibliographystyle{iclr2025_conference}

\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th annual international conference on machine learning}, pp.\  41--48, 2009.

\bibitem[Blocklove et~al.(2023)Blocklove, Garg, Karri, and Pearce]{blocklove2023chip}
Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce.
\newblock Chip-chat: Challenges and opportunities in conversational hardware design.
\newblock In \emph{2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD)}, pp.\  1--6. IEEE, 2023.

\bibitem[Campos(2021)]{campos2021curriculum}
Daniel Campos.
\newblock Curriculum learning for language modeling.
\newblock \emph{arXiv preprint arXiv:2108.02170}, 2021.

\bibitem[Chang et~al.(2023)Chang, Wang, Ren, Wang, Liang, Han, Li, and Li]{chang2023chipgpt}
Kaiyan Chang, Ying Wang, Haimeng Ren, Mengdi Wang, Shengwen Liang, Yinhe Han, Huawei Li, and Xiaowei Li.
\newblock Chipgpt: How far are we from natural language hardware design.
\newblock \emph{arXiv preprint arXiv:2305.14019}, 2023.

\bibitem[Chang et~al.(2024{\natexlab{a}})Chang, Chen, Zhou, Zhu, Xu, Li, Wang, Liang, Li, Han, et~al.]{chang2024natural}
Kaiyan Chang, Zhirong Chen, Yunhao Zhou, Wenlong Zhu, Haobo Xu, Cangyuan Li, Mengdi Wang, Shengwen Liang, Huawei Li, Yinhe Han, et~al.
\newblock Natural language is not enough: Benchmarking multi-modal generative ai for verilog generation.
\newblock \emph{arXiv preprint arXiv:2407.08473}, 2024{\natexlab{a}}.

\bibitem[Chang et~al.(2024{\natexlab{b}})Chang, Wang, Yang, Wang, Jin, Zhu, Chen, Li, Yan, Zhou, et~al.]{chang2024data}
Kaiyan Chang, Kun Wang, Nan Yang, Ying Wang, Dantong Jin, Wenlong Zhu, Zhirong Chen, Cangyuan Li, Hao Yan, Yunhao Zhou, et~al.
\newblock Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework.
\newblock \emph{arXiv preprint arXiv:2403.11202}, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2024)Chen, Chen, Chu, Fang, Ho, Huang, Khan, Li, Li, Liang, et~al.]{chen2024dawn}
Lei Chen, Yiqi Chen, Zhufei Chu, Wenji Fang, Tsung-Yi Ho, Yu~Huang, Sadaf Khan, Min Li, Xingquan Li, Yun Liang, et~al.
\newblock The dawn of ai-native eda: Promises and challenges of large circuit models.
\newblock \emph{arXiv preprint arXiv:2403.07257}, 2024.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Fu et~al.(2023)Fu, Zhang, Yu, Li, Ye, Li, Wan, and Lin]{fu2023gpt4aigchip}
Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan~Celine Lin.
\newblock Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models.
\newblock In \emph{2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)}, pp.\  1--9. IEEE, 2023.

\bibitem[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, et~al.]{guo2024deepseek}
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu~Wu, YK~Li, et~al.
\newblock Deepseek-coder: When the large language model meets programming--the rise of code intelligence.
\newblock \emph{arXiv preprint arXiv:2401.14196}, 2024.

\bibitem[Husain et~al.(2019)Husain, Wu, Gazit, Allamanis, and Brockschmidt]{husain2019codesearchnet}
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
\newblock Codesearchnet challenge: Evaluating the state of semantic code search.
\newblock \emph{arXiv preprint arXiv:1909.09436}, 2019.

\bibitem[Jin et~al.(2023)Jin, Liu, Zheng, Li, Zhao, Zhang, Zheng, Zhou, and Liu]{jin2023adapt}
Bu~Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, and Jingjing Liu.
\newblock Adapt: Action-aware driving caption transformer.
\newblock In \emph{2023 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  7554--7561. IEEE, 2023.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Lin(2004)]{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pp.\  74--81, 2004.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Ene, Kirby, Cheng, Pinckney, Liang, Alben, Anand, Banerjee, Bayraktaroglu, et~al.]{liu2023chipnemo}
Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, et~al.
\newblock Chipnemo: Domain-adapted llms for chip design.
\newblock \emph{arXiv preprint arXiv:2311.00176}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Pinckney, Khailany, and Ren]{liu2023verilogeval}
Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren.
\newblock Verilogeval: Evaluating large language models for verilog code generation.
\newblock In \emph{2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)}, pp.\  1--8. IEEE, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2024)Liu, Fang, Lu, Wang, Zhang, Zhang, and Xie]{liu2024rtlcoder}
Shang Liu, Wenji Fang, Yao Lu, Jing Wang, Qijun Zhang, Hongce Zhang, and Zhiyao Xie.
\newblock Rtlcoder: Fully open-source and efficient llm-assisted rtl code generation technique.
\newblock \emph{IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 2024.

\bibitem[Lu et~al.(2024)Lu, Liu, Zhang, and Xie]{lu2024rtllm}
Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie.
\newblock Rtllm: An open-source benchmark for design rtl generation with large language model.
\newblock In \emph{2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)}, pp.\  722--727. IEEE, 2024.

\bibitem[Na et~al.(2024)Na, Yamani, Lhadj, Baghdadi, et~al.]{na2024curriculum}
Marwa Na, Kamel Yamani, Lynda Lhadj, Riyadh Baghdadi, et~al.
\newblock Curriculum learning for small code language models.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)}, pp.\  531--542, 2024.

\bibitem[Narvekar et~al.(2020)Narvekar, Peng, Leonetti, Sinapov, Taylor, and Stone]{narvekar2020curriculum}
Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew~E Taylor, and Peter Stone.
\newblock Curriculum learning for reinforcement learning domains: A framework and survey.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (181):\penalty0 1--50, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pp.\  311--318, 2002.

\bibitem[Pearce et~al.(2020)Pearce, Tan, and Karri]{pearce2020dave}
Hammond Pearce, Benjamin Tan, and Ramesh Karri.
\newblock Dave: Deriving automatically verilog from english.
\newblock In \emph{Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD}, pp.\  27--32, 2020.

\bibitem[Pei et~al.(2024)Pei, Zhen, Yuan, Huang, and Yu]{pei2024betterv}
Zehua Pei, Hui-Ling Zhen, Mingxuan Yuan, Yu~Huang, and Bei Yu.
\newblock Betterv: Controlled verilog generation with discriminative guidance.
\newblock \emph{arXiv preprint arXiv:2402.03375}, 2024.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{taori2023stanford}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem[Thakur et~al.(2023)Thakur, Ahmad, Fan, Pearce, Tan, Karri, Dolan-Gavitt, and Garg]{thakur2023benchmarking}
Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Benchmarking large language models for automated verilog rtl code generation.
\newblock In \emph{2023 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)}, pp.\  1--6. IEEE, 2023.

\bibitem[Thakur et~al.(2024)Thakur, Ahmad, Pearce, Tan, Dolan-Gavitt, Karri, and Garg]{thakur2024verigen}
Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth Garg.
\newblock Verigen: A large language model for verilog code generation.
\newblock \emph{ACM Transactions on Design Automation of Electronic Systems}, 29\penalty0 (3):\penalty0 1--31, 2024.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Le, Gotmare, Bui, Li, and Hoi]{wang2023codet5+}
Yue Wang, Hung Le, Akhilesh~Deepak Gotmare, Nghi~DQ Bui, Junnan Li, and Steven~CH Hoi.
\newblock Codet5+: Open code large language models for code understanding and generation.
\newblock \emph{arXiv preprint arXiv:2305.07922}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Yue, Lu, Liu, Zhong, Song, and Huang]{wang2023efficienttrain}
Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and Gao Huang.
\newblock Efficienttrain: Exploring generalized curriculum learning for training visual backbones.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  5852--5864, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2024)Wei, Wang, Lu, Xu, Liu, Zhao, Chen, and Wang]{wei2024editable}
Yuxi Wei, Zi~Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, and Yanfeng Wang.
\newblock Editable scene simulation for autonomous driving via collaborative llm-agents.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  15077--15087, 2024.

\bibitem[Williams \& Baxter(2002)Williams and Baxter]{williams2002icarus}
Stephen Williams and Michael Baxter.
\newblock Icarus verilog: open-source verilog more than a year later.
\newblock \emph{Linux Journal}, 2002\penalty0 (99):\penalty0 3, 2002.

\bibitem[Wu et~al.(2024)Wu, He, Zhang, Yao, Zheng, Zheng, and Yu]{wu2024chateda}
Haoyuan Wu, Zhuolun He, Xinyun Zhang, Xufeng Yao, Su~Zheng, Haisheng Zheng, and Bei Yu.
\newblock Chateda: A large language model powered autonomous agent for eda.
\newblock \emph{IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 2024.

\bibitem[Xu et~al.(2020)Xu, Zhang, Mao, Wang, Xie, and Zhang]{xu2020curriculum}
Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang.
\newblock Curriculum learning for natural language understanding.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  6095--6104, 2020.

\bibitem[Yan et~al.(2017)Yan, Liu, Li, Han, and Qiu]{yan2017privmin}
Ziqi Yan, Jiqiang Liu, Gang Li, Zhen Han, and Shuo Qiu.
\newblock Privmin: Differentially private minhash for jaccard similarity computation.
\newblock \emph{arXiv preprint arXiv:1705.07258}, 2017.

\bibitem[Zhang et~al.(2024)Zhang, Yu, Fu, Wan, et~al.]{zhang2024mg}
Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, et~al.
\newblock Mg-verilog: Multi-grained dataset towards enhanced llm-assisted verilog generation.
\newblock \emph{arXiv preprint arXiv:2407.01910}, 2024.

\end{thebibliography}


\newpage
\appendix
% \section{Appendix}

\section{Introduction of Verilog}
\label{appendix:verilog_introduction}

Verilog is the most widely used hardware description language (HDL) for modeling digital integrated circuits. It enables designers to specify both the behavioral and structural aspects of hardware systems, such as processors, controllers, and digital logic circuits. Verilog operates at a relatively low level, focusing on gates, registers, and signal assignments—each representing physical hardware components. While Verilog supports behavioral constructs (\textit{e.g.}, \texttt{if-else}, \texttt{case}) that are somewhat similar to software programming languages, their use is constrained by synthesizable coding styles required for hardware implementation.
Verilog differs from software programming languages like Python and C++ in several key ways:


\begin{enumerate}
    \item \textbf{Parallelism:} Verilog inherently models hardware’s concurrnet nature, with multiple statements executing simultaneously. In contrast, software languages like Python typically follow a sequential execution model.
    \item \textbf{Timing:} Timing is a fundamental concept in Verilog that directly influences how digital circuits are designed and simulated. Verilog relies on clocks to synchronize sequential logic behaviors, enabling the precise modeling of synthronous circuits. In contrast, software programming languages generally do not have an inherent need for explicit timing.
    \item \textbf{Syntax and Constructs:} Verilog’s syntax is tailored to describe the behavior and structure of digital circuits, reflecting the parallel nature of hardware. Key constructs of Verilog include:
    
    \begin{itemize}
        \item {\textbf{Modules:}}
        The basic unit of Verilog, used to define a hardware block or component. Each module in Verilog encapsulates inputs, outputs, and internal logic, and modules can be instantiated within other modules, enabling hierarchical designs that mirror the complexity of real-world systems. And each module instantiation results in the generation of a corresponding circuit block.
        \item {\textbf{Always block:}}
        In an \texttt{always} block, circuit designers can model circuits using high-level behavioral descriptions. However, this does not imply that a broad range of programming language syntax is available. In practice, Verilog supports only a limited subset of programming-like constructs, primarily \texttt{if-else} and \texttt{case} statements. Statements in multiple \texttt{always} blocks are executed in parallel and the resulting circuit continuously performs its operations.
        \item {\textbf{Sensitivity list:}}
        In an \texttt{always} block, the sensitivity list specifies the signals that trigger the block’s execution when they change.
        \item {\textbf{Assign statements:}}
        \texttt{assign} statements are used to describe continuous assignments of signal values in parallel, reflecting the inherent concurrency of hardware.
        \item {\textbf{Registers (\texttt{reg}) and Wires (\texttt{wire}):}}
        \texttt{reg} is used for variables that retain their values (\textit{e.g.}, flip-flops or memory), and \texttt{wire} is used for connections that propagate values through the circuit.
        
    \end{itemize}

    In contrast, software programming languages like C, Python, or Java employ a more conventional syntax for defining algorithms, control flow, and data manipulation. These languages use constructs like loops (\texttt{for}, \texttt{while}), conditionals (\texttt{if}, \texttt{else}), and functions or methods for structuring code, with data types such as integers, strings, and floats for variable storage.

\end{enumerate}


\section{Prompt Details for CoT Annotation}
\label{appendix:prompt}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/prompt_v2.pdf}
    \caption{Detailed prompts used in the CoT annotation process.}
    \label{fig:prompt}
\end{figure}

As shown in Figure~\ref{fig:prompt}, we present the detailed prompts used in our annotation process. 
For each task, we supplement the primary prompt with several human-reviewed input-output pair examples, serving as in-context learning examples to enhance GPT's understanding of task requirements and expectations.
%In addition to the prompt listed, for each task, we will also provide GPT with several human-reviewed input-output pair examples as initial input to help it better understand the task requirements and expectations. 
These examples will serve as guidance for the model to correctly interpret and execute tasks in accordance with the prompt, ensuring more accurate and contextually relevant outputs.

\section{Discarding Verilog Code Exceeding $2048$ Tokens}
\label{appendix:discard}
In the main submission, we state that Verilog modules and blocks exceeding $2048$ tokens are excluded, as $2048$ is the maximum input length supported by CodeT5+. Beyond this limitation, several additional factors motivate this decision:
\newpage

\begin{figure}[ht]
    \centering
    \vspace{-20pt}
    \includegraphics[width=0.9\linewidth]{fig/distribution.jpg}
    \vspace{-10pt}
    \caption{The distribution of the token lengths of the generation benchmark by~\citet{chang2024natural}.}
    \label{fig:distribution}
\end{figure}
\begin{enumerate}
    \item \textbf{Generation Capabilities of Existing LLMs Are Limited to Small Designs}

    Existing benchmarks for Verilog generation, including the one used in our work~\citep{chang2024natural}, do not include designs exceeding $2048$ tokens, with the maximum token length observed in the benchmark being $1851$. As shown in Table~\ref{tab:generation_results} of the main submission, even the state-of-the-art LLM, o1-preview, is capable of accurately generating only simple designs and struggles with more complex ones. 
    Figure~\ref{fig:distribution} illustrates the token length distribution across the benchmark, further justifying our decision to exclude Verilog modules and blocks exceeding $2048$ tokens.

    \item \textbf{Segmentation as a Common Practice}

    Segmenting longer code into smaller chunks that fit within the predefined context window and discarding those that exceed it is a widely accepted practice in both Verilog-related research~\citep{chang2024data,pei2024betterv} and studies on software programming language~\citep{wang2023codet5+}. This approach ensures compatibility with current LLMs while maintaining the integrity and usability of the dataset. It is worth noting that the default maximum sequence length in CodeT5+ is $512$ tokens, and our work extends this limit to $2048$ tokens to better accommodate Verilog designs.

    \item \textbf{Empirical Findings and Practical Challenges}
    
    Our experiments reveal an important empirical observation: existing LLMs, such as GPT-4, consistently produce accurate descriptions for shorter Verilog modules but struggle with correctness when handling longer ones. Specifically, During the annotation process, we divide the dataset into two sections: Verilog designs with fewer than $2048$ tokens, and designs with token lengths between $2048$ and $4096$ tokens. Our human evaluation finds that descriptions for Verilog designs with fewer than $2048$ tokens are approximately 90\% accurate, while descriptions for designs with token lengths between $2048$ and $4096$ tokens have accuracy rates of only 60\%–70\%. And accuracy further decreases for designs exceeding $4096$ tokens. Since our datasets rely on LLM-generated annotations, restricting the dataset to Verilog modules within the $2048$-token limit helps maintain the quality and accuracy of annotations. This, in turn, facilitates higher-quality dataset creation and more efficient fine-tuning. For the potential negative impact of incorporating Verilog designs larger than $2048$ tokens, please refer to Appendix~\ref{appendix:negative_impact}.
    And we examine the impact of varying context window lengths in Appendix~\ref{appendix:varying_context_window_length}.
\end{enumerate}


\section{Standards and Processes for Manual Code Annotation}
\label{appendix:standard}

Given the industrial-grade quality of the proprietary code, we employ professional hardware engineers for manual annotation. We have established the following standards and processes to guide engineers in crafting accurate and detailed descriptions with example annotations shown in Figure~\ref{fig:engineer}:

\begin{enumerate}
    \item \textbf{Standards:} The hardware engineers are required to provide descriptions at both the module and block levels.
    
    \begin{itemize}
        \item For module-level descriptions, two levels are defined:
        \begin{itemize}
            \item[i.] \textbf{H (High-level):} The role of this module in the overall design (IP/Chip).
            \item[ii.] \textbf{D (Detailed):} What functions this module performs (overview) and how it is implemented (implementation details). This description should adhere to a top-down structure and consist of approximately 2-5 sentences.
        \end{itemize}
        \textbf{Note:} If the summary statements for H and D are identical, both must be provided.
        
        \item For block-level descriptions, particularly \texttt{always} blocks, descriptions are required at three distinct levels:
        \begin{itemize}
            \item[i.] \textbf{H (High-level):} The role of this block in the overall design (\textit{e.g.}, across modules).
            \item[ii.] \textbf{M (Medium-detail):} Contextual explanations.
            \item[iii.] \textbf{D (Detailed):} Descriptions specific to the block following a top-down structure. If details are absent, they may be omitted; do not guess based on signal names.
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Processes:} Initially, we provide engineers with a set of descriptions generated by GPT-4 for reference. They are then expected to revise and enhance these GPT-generated descriptions using their expertise and relevant supplementary materials, such as README files and register tables.

\end{enumerate}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/engineer.pdf}
    \caption{Human-annotated examples for the proprietary code.}
    \label{fig:engineer}
\end{figure}

\section{Examples of Verilog Understanding Benchmark}
\label{appendix:benchmark}

To construct a high-quality benchmark, we first remove comments from the original code, and then submit it to experienced hardware engineers for annotation, ultimately producing the code and description pairs as shown in Figure~\ref{fig:verified_data}.


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/verified_data_v2.pdf}
    \caption{Examples from the Verilog understanding benchmark.}
    \label{fig:verified_data}
\end{figure}


\section{Model Selection}
\label{appendix:model_selection}
In this work, we choose CodeT5+, a family of encoder-decoder code foundation models, as the base model for training DeepRTL for two primary reasons. First, as we aim to develop a unified model for Verilog understanding and generation, T5-like models are particularly well-suited due to their ability to effectively handle both tasks, as evidenced by~\citet{wang2023codet5+}. Second, the encoder component of CodeT5+ enables the natural extraction of Verilog representations, which can be potentially utilized for various downstream tasks in EDA at the RTL stage. Examples include PPA (Power, Performance, Area) prediction, which estimates the power consumption, performance, and area of an RTL design, and verification, which ensures that the RTL design correctly implements its intended functionality and meets specification requirements. Both tasks are crucial in the hardware design process. This capability distinguishes it from decoder-only models, which are typically less suited for producing standalone, reusable intermediate representations. In future work, we plan to explore how DeepRTL can further enhance productivity in the hardware design process.

To further demonstrate the superiority of CodeT5+ as a base model, we fine-tune two additional models, deepseek-coder-1.3b-instruct\footnote{\url{https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct}} (deepseek-coder)~\citep{guo2024deepseek} and Llama-3.2-1B-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}} (llama-3.2)~\citep{dubey2024llama}, using the same dataset as DeepRTL and the adopted curriculum learning strategy.

In Table~\ref{tab:understanding_additional} and Table~\ref{tab:decoder_compare}, we present the performance of both the original base models and their fine-tuned counterparts on Verilog understanding and generation tasks. The improvement in performance from the original base models to the fine-tuned models highlights the effectiveness of our dataset and the curriculum learning-based fine-tuning strategy. Compared to the results in Table~\ref{tab:understanding_results} and Table~\ref{tab:generation_results}, the superior performance of DeepRTL-220m on both tasks, despite its smaller model size, underscores the architectural advantages of our approach.


\begin{table}[ht]
\centering
\vspace{-20pt}
\caption{Evaluation results on Verilog understanding using the benchmark proposed in Section~\ref{sec:understanding_benchmark}. BLEU-4 denotes the smoothed BLEU-4 score, and Emb. Sim. represents the embedding similarity metric. Specifically, this table presents the performance of decoder-only models, where ``long'' indicates models fine-tuned on the dataset containing longer Verilog designs, and those fine-tuned specifically on Verilog. $^\dag$ indicates performance evaluated on designs shorter than $512$ tokens.}
\vspace{2pt}
\label{tab:understanding_additional}
% \small{
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l|ccccccc@{}}
    \toprule
Model & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & Emb. Sim. & GPT Score \\
\midrule
deepseek-coder (original) & 1.04 & 21.43 & 4.38 & 19.77 & 0.678 & 0.557 \\
deepseek-coder (fine-tuned) & 11.96 & 40.49 & 19.77 & 36.14 & 0.826 & 0.664 \\
deepseek-coder (long) & 11.27 & 40.28 & 18.95 & 35.93 & 0.825 & 0.649 \\
\midrule
llama-3.2 (original) & 0.88 & 19.26 & 3.60 & 17.64 & 0.615 & 0.449 \\
llama-3.2 (fine-tuned) & 12.11 & 39.95 & 19.47 & 35.29 & 0.825 & 0.620 \\
llama-3.2 (long) & 11.32 & 39.60 & 18.67 & 34.94 & 0.814 & 0.610 \\
\midrule
RTLCoder & 1.08 & 21.83 & 4.68 & 20.30 & 0.687 & 0.561 \\
VeriGen & 0.09 & 6.54 & 0.35 & 6.08 & 0.505 & 0.311 \\
\midrule
DeepRTL-220m-512$^\dag$	& 14.98	& 44.27	& 23.11	& 40.08	& 0.780	& 0.567 \\
DeepRTL-220m$^\dag$	& 18.74	& 48.41	& 29.82	& 45.01	& 0.855	& 0.743 \\
\bottomrule
\end{tabular}%
}
\end{table}


\begin{table}[!ht]
\centering
\vspace{-5pt}
\caption{Evaluation results on Verilog generation. Each cell displays the percentage of code samples,
out of five trials, that successfully pass compilation (syntax column) or functional unit tests (function
column). This table presents the performance of decoder-only models, where ``o'' denotes the original model and ``f'' denotes the fine-tuned model.}
\vspace{5pt}
\label{tab:decoder_compare}
% {\tiny
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|cl|cc|cc|cc|cc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Benchmark}} & \multicolumn{2}{c|}{deepseek-coder (o)} & \multicolumn{2}{c|}{deepseek-coder (f)} & \multicolumn{2}{c|}{llama-3.2 (o)} & \multicolumn{2}{c|}{llama-3.2 (f)} \\ \cline{3-10} 
\multicolumn{2}{|c|}{} & \multicolumn{1}{c|}{syntax} & function & \multicolumn{1}{c|}{syntax} & function & \multicolumn{1}{c|}{syntax} & function & \multicolumn{1}{c|}{syntax} & function \\ \hline
\multicolumn{1}{|c|}{\multirow{10}{*}{Logic}} & Johnson\_Counter & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & alu & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & edge\_detect & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{80\%} & 20\% & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & freq\_div & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & mux & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{60\%} & 60\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & parallel2serial & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & pulse\_detect & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{80\%} & 40\% & \multicolumn{1}{c|}{60\%} & 20\% & \multicolumn{1}{c|}{60\%} & 40\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & right\_shifter & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{80\%} & 80\% & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{40\%} & 40\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & serial2parallel & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & width\_8to16 & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \hline
\multicolumn{1}{|c|}{\multirow{11}{*}{Arithmetic}} & accu & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & adder\_16bit & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{40\%} & 20\% & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & adder\_16bit\_csa & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 20\% & \multicolumn{1}{c|}{0\%} & 20\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & adder\_32bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & adder\_64bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{40\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & adder\_8bit & \multicolumn{1}{c|}{40\%} & 0\% & \multicolumn{1}{c|}{80\%} & 20\% & \multicolumn{1}{c|}{40\%} & 0\% & \multicolumn{1}{c|}{60\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & div\_16bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & multi\_16bit & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & multi\_booth & \multicolumn{1}{c|}{40\%} & 0\% & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{40\%} & 0\% & \multicolumn{1}{c|}{60\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & multi\_pipe\_4bit & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & multi\_pipe\_8bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \hline
\multicolumn{1}{|c|}{\multirow{10}{*}{Advanced}} & 1x2nocpe & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{20\%} & 40\% & \multicolumn{1}{c|}{60\%} & 20\% & \multicolumn{1}{c|}{60\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 1x4systolic & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 2x2systolic & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 4x4spatialacc & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & fsm & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & macpe & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 5state\_fsm & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 20\% & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 3state\_fsm & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{100\%} & 80\% & \multicolumn{1}{c|}{20\%} & 20\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 4state\_fsm & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{100\%} & 40\% & \multicolumn{1}{c|}{80\%} & 20\% & \multicolumn{1}{c|}{100\%} & 20\% \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & 2state\_fsm & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{100\%} & 20\% & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{100\%} & 20\% \\ \hline
\multicolumn{2}{|c|}{Success Rate} & \multicolumn{1}{c|}{44.52\%} & 0.00\% & \multicolumn{1}{c|}{63.87\%} & 25.81\% & \multicolumn{1}{c|}{45.16\%} & 3.23\% & \multicolumn{1}{c|}{58.71\%} & 22.58\% \\ \hline
\multicolumn{2}{|c|}{Pass @ 1} & \multicolumn{1}{c|}{12.90\%} & 0.00\% & \multicolumn{1}{c|}{61.29\%} & 22.58\% & \multicolumn{1}{c|}{12.90\%} & 0.00\% & \multicolumn{1}{c|}{54.84\%} & 19.35\% \\ \hline
\multicolumn{2}{|c|}{Pass @ 5} & \multicolumn{1}{c|}{67.74\%} & 0.00\% & \multicolumn{1}{c|}{80.65\%} & 48.39\% & \multicolumn{1}{c|}{70.97\%} & 16.13\% & \multicolumn{1}{c|}{80.65\%} & 48.39\% \\ \hline
\end{tabular}%
}
% }
\end{table}

\section{Instructions for Different Scenarios}
\label{appendix:instruction}
Figure~\ref{fig:instruction_example} presents detailed instruction samples for different scenarios, following the instruction construction process illustrated in Figure~\ref{fig:instruction}.
Additionally, it includes a special module-level task, which involves completing the source code based on the functional descriptions of varying granularity and the predefined module header.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/instruction_example_v2.pdf}
    \caption{Instruction tuning data samples for different scenarios.}
    \label{fig:instruction_example}
\end{figure}

\section{Further Explanation of the Adopted Curriculum Learning Strategy}
\label{appendix:explanation_curriculum_learning}
Our dataset includes three levels of annotation: line, block, and module, with each level containing descriptions that span various levels of detail—from detailed specifications to high-level functional descriptions. And the entire dataset is utilized for training. To fully leverage the potential of this dataset, we employ a curriculum learning strategy, enabling the model to incrementally build knowledge by starting with simpler cases and advancing to more complex ones.

The curriculum learning strategy involves transitioning from more granular to less granular annotations across hierarchical levels, which can be conceptualized as a tree structure with the following components (as shown in Figure~\ref{fig:tree}):

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/tree.pdf}
    \caption{The adopted curriculum learning strategy visualized as a tree structure. Specifically, the terminals of the tree, enclosed by blue dotted boxes, represent specific training datasets. Our curriculum learning strategy follows a pre-order traversal of this tree structure.}
    \label{fig:tree}
\end{figure}

\begin{enumerate}
    \item \textbf{Hierarchical Levels (First Layer)}
    
    The training process transitions sequentially across the three hierarchical levels—line, block, and module. Each level is fully trained before moving to the next, ensuring a solid foundation at simpler levels before addressing more complex tasks.
    \item \textbf{Granularity of Descriptions (Second Layer)}
    
    Within each hierarchical level, the annotations transition from detailed descriptions to high-level descriptions. This progression ensures that the model learns finer details first and then builds an understanding of higher-level abstractions.
    \newpage
    \item \textbf{Annotation Source Transition (Third Layer)}
    
    At each level and granularity, training starts with GPT-annotated data and is followed by human-annotated data. This sequence leverages large-scale machine-generated annotations first and refines the model with high-quality, human-curated data.
    
    \item \textbf{Instruction Blending}
    
    Each terminal node in this tree represents a specific training dataset, which blends tasks for Verilog understanding and Verilog generation. This enables the model to perform well across diverse tasks.
\end{enumerate}

The training process mirrors a pre-order traversal of this tree structure:
\begin{enumerate}
    \item Starting at the root, training begins with the line level.
    \item The model progresses through the second layer (detailed, medium-detail, and high-level descriptions).
    \item Within each granularity, training transitions through the third layer (GPT-annotated data first, followed by human-annotated data).
    \item Once the line level is complete, the process repeats for the block level and then the module level.
\end{enumerate}

% This progressive training strategy aligns closely with the principles of curriculum learning, where simpler concepts are introduced first, and the knowledge gained is transferred incrementally to handle more complex scenarios.

To validate the effectiveness of this strategy, we conduct an ablation study where the model is trained on the entire dataset all at once without progression. The results, presented in Table~\ref{tab:understanding_results} of the main submission, demonstrate that the curriculum learning strategy significantly outperforms this baseline approach. Moreover, to the best of our knowledge, this is one of the first applications of a curriculum-like training strategy in the code-learning domain. Unlike existing Verilog-related models that establish simple and weak alignments between natural language and Verilog code~\citep{chang2024data}, or general software code datasets like CodeSearchNet\footnote{\url{https://huggingface.co/datasets/code-search-net/code_search_net}}~\citep{husain2019codesearchnet} that only provide single-level docstring annotations, our approach incorporates multi-level and multi-granularity annotations in a structured training process.


\section{Prompt for Calculating GPT Score}
\label{appendix:gpt_score}
To calculate the GPT score, we input the model’s generated descriptions (model\_output) and the ground truth annotations (ground\_truth) to GPT-4, using the prompt displayed in Figure~\ref{fig:gpt_score}. This metric is designed to assess the semantic accuracy of the generated functional descriptions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.72\linewidth]{fig/gptscore.pdf}
    \caption{Prompt used to calculate the GPT score.}
    \label{fig:gpt_score}
\end{figure}

\section{Comparison with Models Specifically Trained on Verilog}
\label{appendix:comparison}

To further demonstrate the superiority of DeepRTL, we conduct experiments comparing it with models specifically trained on Verilog. 
We do not select~\citep{chang2024data,zhang2024mg} for comparison, as their models are not open-sourced, and it is non-trivial to reproduce their experiments. Additionally, the reported performance in their original papers is either comparable to, and in some cases inferior to, that of GPT-3.5. 
In Table~\ref{tab:understanding_additional} and Table~\ref{tab:verilog_specific}, we show the performance of two state-of-the-art Verilog generation models, RTLCoder-Deepseek-v1.1\footnote{\url{https://huggingface.co/ishorn5/RTLCoder-Deepseek-v1.1}} (RTLCoder)~\citep{liu2024rtlcoder} and fine-tuned-codegen-16B-Verilog\footnote{\url{https://huggingface.co/shailja/fine-tuned-codegen-16B-Verilog}} (VeriGen)~\citep{thakur2024verigen} on both Verilog understanding and generation benchmarks. It is noteworthy that RTLCoder is fine-tuned on DeepSeek-coder-6.7B, and VeriGen is fine-tuned on CodeGen-multi-16B, both of which have significantly larger parameter sizes than DeepRTL-220m. Despite this, the superior performance of DeepRTL-220m further underscores the effectiveness of our proposed dataset and the adopted curriculum learning strategy.


\begin{table}[!ht]
\centering
\caption{Evaluation results on Verilog generation. Each cell displays the percentage of code samples,
out of five trials, that successfully pass compilation (syntax column) or functional unit tests (function
column). This table presents the performance of models specifically trained on Verilog.}
\vspace{5pt}
\label{tab:verilog_specific}
{\tiny
\begin{tabular}{|cl|cc|cc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Benchmark}} & \multicolumn{2}{c|}{RTLCoder} & \multicolumn{2}{c|}{VeriGen} \\ \cline{3-6} 
\multicolumn{2}{|c|}{} & \multicolumn{1}{c|}{syntax} & function & \multicolumn{1}{c|}{syntax} & function \\ \hline
\multicolumn{1}{|c|}{\multirow{10}{*}{Logic}} & Johnson\_Counter & \multicolumn{1}{c|}{40\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & alu & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & edge\_detect & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{100\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & freq\_div & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & mux & \multicolumn{1}{c|}{60\%} & 40\% & \multicolumn{1}{c|}{80\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & parallel2serial & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & pulse\_detect & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{40\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & right\_shifter & \multicolumn{1}{c|}{80\%} & 80\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & serial2parallel & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & width\_8to16 & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \hline
\multicolumn{1}{|c|}{\multirow{11}{*}{Arithmetic}} & accu & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_16bit & \multicolumn{1}{c|}{40\%} & 20\% & \multicolumn{1}{c|}{20\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_16bit\_csa & \multicolumn{1}{c|}{80\%} & 80\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_32bit & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_64bit & \multicolumn{1}{c|}{40\%} & 0\% & \multicolumn{1}{c|}{40\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_8bit & \multicolumn{1}{c|}{80\%} & 40\% & \multicolumn{1}{c|}{40\%} & 40\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & div\_16bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_16bit & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_booth & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{20\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_pipe\_4bit & \multicolumn{1}{c|}{60\%} & 20\% & \multicolumn{1}{c|}{80\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_pipe\_8bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \hline
\multicolumn{1}{|c|}{\multirow{10}{*}{Advanced}} & 1x2nocpe & \multicolumn{1}{c|}{40\%} & 40\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 1x4systolic & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 2x2systolic & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 4x4spatialacc & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & fsm & \multicolumn{1}{c|}{100\%} & 60\% & \multicolumn{1}{c|}{80\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & macpe & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 5state\_fsm & \multicolumn{1}{c|}{60\%} & 40\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 3state\_fsm & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{80\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 4state\_fsm & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{80\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 2state\_fsm & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{60\%} & 0\% \\ \hline
\multicolumn{2}{|c|}{Success Rate} & \multicolumn{1}{c|}{48.39\%} & 20.00\% & \multicolumn{1}{c|}{50.97\%} & 12.26\% \\ \hline
\multicolumn{2}{|c|}{Pass @ 1} & \multicolumn{1}{c|}{41.94\%} & 16.13\% & \multicolumn{1}{c|}{48.39\%} & 9.68\% \\ \hline
\multicolumn{2}{|c|}{Pass @ 5} & \multicolumn{1}{c|}{77.42\%} & 35.48\% & \multicolumn{1}{c|}{70.97\%} & 32.26\% \\ \hline
\end{tabular}%
}
\end{table}


\begin{table}[!ht]
\centering
\caption{Evaluation results on Verilog generation. Each cell displays the percentage of code samples,
out of five trials, that successfully pass compilation (syntax column) or functional unit tests (function
column). This table presents the performance of decoder-only models fine-tuned on the dataset containing longer Verilog designs.}
\label{tab:decoder_model_with_longer_designs}
\vspace{5pt}
{\tiny
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{|cl|cc|cc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Benchmark}} & \multicolumn{2}{c|}{deepseek-coder} & \multicolumn{2}{c|}{llama-3.2} \\ \cline{3-6} 
\multicolumn{2}{|c|}{} & \multicolumn{1}{c|}{syntax} & function & \multicolumn{1}{c|}{syntax} & function \\ \hline
\multicolumn{1}{|c|}{\multirow{10}{*}{Logic}} & Johnson\_Counter & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & alu & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & edge\_detect & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & freq\_div & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & mux & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{60\%} & 60\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & parallel2serial & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & pulse\_detect & \multicolumn{1}{c|}{80\%} & 40\% & \multicolumn{1}{c|}{60\%} & 40\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & right\_shifter & \multicolumn{1}{c|}{80\%} & 80\% & \multicolumn{1}{c|}{40\%} & 40\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & serial2parallel & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & width\_8to16 & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \hline
\multicolumn{1}{|c|}{\multirow{11}{*}{Arithmetic}} & accu & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_16bit & \multicolumn{1}{c|}{20\%} & 20\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_16bit\_csa & \multicolumn{1}{c|}{20\%} & 20\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_32bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_64bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & adder\_8bit & \multicolumn{1}{c|}{80\%} & 20\% & \multicolumn{1}{c|}{60\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & div\_16bit & \multicolumn{1}{c|}{20\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_16bit & \multicolumn{1}{c|}{80\%} & 0\% & \multicolumn{1}{c|}{80\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_booth & \multicolumn{1}{c|}{60\%} & 0\% & \multicolumn{1}{c|}{60\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_pipe\_4bit & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & multi\_pipe\_8bit & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \hline
\multicolumn{1}{|c|}{\multirow{10}{*}{Advanced}} & 1x2nocpe & \multicolumn{1}{c|}{40\%} & 40\% & \multicolumn{1}{c|}{60\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 1x4systolic & \multicolumn{1}{c|}{20\%} & 20\% & \multicolumn{1}{c|}{20\%} & 20\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 2x2systolic & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 4x4spatialacc & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & fsm & \multicolumn{1}{c|}{100\%} & 100\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & macpe & \multicolumn{1}{c|}{0\%} & 0\% & \multicolumn{1}{c|}{0\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 5state\_fsm & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 3state\_fsm & \multicolumn{1}{c|}{80\%} & 80\% & \multicolumn{1}{c|}{100\%} & 100\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 4state\_fsm & \multicolumn{1}{c|}{100\%} & 0\% & \multicolumn{1}{c|}{100\%} & 0\% \\ \cline{2-6} 
\multicolumn{1}{|c|}{} & 2state\_fsm & \multicolumn{1}{c|}{100\%} & 20\% & \multicolumn{1}{c|}{100\%} & 20\% \\ \hline
\multicolumn{2}{|c|}{Success Rate} & \multicolumn{1}{c|}{60.00\%} & 20.65\% & \multicolumn{1}{c|}{57.42\%} & 21.94\% \\ \hline
\multicolumn{2}{|c|}{Pass @ 1} & \multicolumn{1}{c|}{38.71\%} & 19.35\% & \multicolumn{1}{c|}{38.71\%} & 19.35\% \\ \hline
\multicolumn{2}{|c|}{Pass @ 5} & \multicolumn{1}{c|}{77.42\%} & 38.71\% & \multicolumn{1}{c|}{77.42\%} & 45.16\% \\ \hline
\end{tabular}%
}
\end{table}


\section{Negative Impact of Incorporating Verilog Designs Exceeding $2048$ Tokens}
\label{appendix:negative_impact}
Notably, the maximum input length for DeepSeek-coder is 16k tokens, while for LLaMA-3.2, it is 128k tokens. To assess the potential negative impact of including Verilog designs exceeding $2048$ tokens, we conduct an ablation study in which we do not exclude such modules for these two models and instead use the dataset containing longer designs for training. As shown in Table~\ref{tab:decoder_model_with_longer_designs}, and by comparing the results in Table~\ref{tab:understanding_additional}, the performance of the fine-tuned models on both Verilog understanding and generation tasks significantly degrades compared to the results in Table~\ref{tab:decoder_compare}, where these models are fine-tuned using the same dataset as DeepRTL. This further validates the rationale behind our decision to exclude Verilog modules and blocks exceeding $2048$ tokens.


\section{Additional Experiments Investigating the Impact of Varying Context Window Lengths}
\label{appendix:varying_context_window_length}
To address concerns regarding the potential bias introduced by excluding examples longer than $2048$ tokens, we investigate the impact of context window length. Specifically, we exclude all Verilog modules exceeding $512$ tokens and use the truncated dataset to train a new model, DeepRTL-220m-512 utilizing the curriculum learning strategy, which has a maximum input length of $512$ tokens. We then evaluate both DeepRTL-220m-512 and DeepRTL-220m on Verilog understanding benchmark samples, where the module lengths are below $512$ tokens, and present the results in Table~\ref{tab:understanding_additional}. For the generation task, DeepRTL-220m-512 shows near-zero performance, with nearly 0\% accuracy for both syntax and functional correctness. This result refutes the concern that ``a model accommodating longer context windows could potentially offer superior performance on the general task, but not for this tailored dataset," as it does not hold true in our case.

\end{document}
