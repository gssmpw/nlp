\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/instruction_v2.pdf}
    \vspace{-12pt}
    \caption{The overview of the instruction construction process and the curriculum learning strategy. For instruction construction, we integrate various settings, \textit{e.g.}, task type, granularity, and comment level, to create tailored instructions for specific scenarios. The curriculum learning strategy involves three hierarchical stages: training progresses from line-level to module-level code (\nth{1} stage), transitioning from detailed to high-level descriptions at each level (\nth{2} stage), and advancing from GPT-annotated to human-annotated descriptions for each granularity (\nth{3} stage).}
    \label{fig:instruction}
\end{figure}


\section{Model and Evaluation}

In this section, we introduce DeepRTL and elaborate on the preparation of our instruction tuning dataset and how we adapt curriculum learning for training.
% our progressive training strategy.
% we need an overview paragraph here
%In this section, we introduce DeepRTL and elaborate on the preparation of our instruction tuning dataset, as well as our progressive training strategy.
%, along with specific training and inference settings that optimize model performance. 
Additionally, we detail the benchmarks and metrics used to evaluate our model's performance in both Verilog understanding and generation tasks.
To accurately assess the semantic precision of the generated descriptions, 
we take the initiative to apply embedding similarity and GPT score for evaluation,
% we introduce two novel metrics: embedding similarity and GPT score, 
which are designed to quantitatively measure the semantic similarity between the model's outputs and the ground truth.

\subsection{Model}
In our work, we have chosen to fine-tune CodeT5+~\citep{wang2023codet5+}, a family of encoder-decoder code foundation LLMs for a wide range of code understanding and generation tasks. CodeT5+ employs a ``shallow encoder and deep decoder" architecture~\citep{li2022competition}, where both encoder and decoder are initialized from pre-trained checkpoints and connected by cross-attention layers. 
We choose to fine-tune CodeT5+ for its extensive pre-training on a vast corpus of software code, 
with the intent to transfer its acquired knowledge to hardware code tasks.
Also, the model's flexible architecture allows for the customization of various training tasks, making it highly adaptable for specific downstream applications. Furthermore, CodeT5+ adopts an efficient fine-tuning strategy where the deep decoder is frozen and only the shallow encoder and cross-attention layers are allowed to train, significantly reducing the number of trainable parameters.
Specifically, we have fine-tuned two versions of CodeT5+, codet5p-220m-bimodal\footnote{\url{https://huggingface.co/Salesforce/codet5p-220m-bimodal}} (CodeT5+-220m) and instructcodet5p-16b\footnote{\url{https://huggingface.co/Salesforce/instructcodet5p-16b}} (CodeT5+-16b), on our dataset, resulting in DeepRTL-220m and DeepRTL-16b, respectively. 
For more information on the model selection, please refer to Appendix~\ref{appendix:model_selection}.



\subsection{Instruction Tuning Dataset}
During the fine-tuning process, we adopt the instruction tuning strategy to enhance the adaptability of LLMs, which is particularly effective when handling diverse types of data and tasks.
Given that our dataset features descriptions at multiple levels and our model is fine-tuned for both Verilog understanding and generation tasks, there is diversity in both the data types and tasks.
To accommodate this diversity, we carefully design specific instructions for each scenario, ensuring the model can adjust its output to align with the intended instructions. Figure~\ref{fig:instruction} illustrates how we combine various settings, \textit{e.g.}, task type, granularity, and comment level, to construct tailored instructions for each specific scenario, fostering a structured approach to instruction-based tuning that optimizes the fine-tuning efficacy. For details on the instructions for different scenarios, please refer to Appendix~\ref{appendix:instruction}.

\subsection{Curriculum Learning for DeepRTL}
We adapt curriculum learning for the fine-tuning process, leveraging our structured dataset that features descriptions of various details across multiple levels.
% For the fine-tuning process, we implement a progressive training strategy, 
%recognizing that the model is more influenced by the data it encounters most recently. 
Initially, the model is fine-tuned on line-level and block-level data, subsequently progressing to module-level data. At each level, we start by aligning the detailed specifications with the code before moving to the high-level functional descriptions. 
And fine-tuning typically starts with GPT-annotated data, followed by human-annotated data for each annotation granularity.
Figure~\ref{fig:instruction} provides an illustration of this process.
We adopt such strategy because a particular focus is placed on aligning Verilog modules with their high-level functional descriptions, which poses the greatest challenge and offers substantial practical applications.
This curriculum learning strategy enables the model to incrementally build knowledge from simpler to more complex scenarios. As a result, the models demonstrate impressive performance across both Verilog understanding and generation benchmarks.
Note that we exclude the cases in the benchmarks from our training dataset.
We primarily follow the instruction tuning script of CodeT5+\footnote{\url{https://github.com/salesforce/CodeT5}} in the fine-tuning process, with a modification to expand the input context length to the maximum of $2048$ tokens.  We utilize the distributed framework, DeepSpeed, to efficiently fine-tune the model across a cluster equipped with eight NVIDIA A800 GPUs, each with 80GB of memory. During inference, we adjust the temperature to 0.8 for understanding tasks and to 0.5 for generation tasks, while other hyperparameters remain at their default settings to ensure optimal performance. 
Further details on the adopted curriculum learning strategy are provided in Appendix~\ref{appendix:explanation_curriculum_learning}.


\subsection{Understanding Evaluation}
\label{sec:understanding_evaluation}
For evaluating LLMs' capabilities in Verilog understanding, we utilize the benchmark introduced in Section~\ref{sec:understanding_benchmark}. The evaluation measures the similarity between the generated descriptions and the ground truth summaries. Previous works usually use BLEU~\citep{papineni2002bleu} and ROUGE~\citep{lin2004rouge} scores for this purpose~\citep{wang2023codet5+}. 
BLEU assesses how many $n$-grams, \textit{i.e.}, sequences of $n$ words, in the machine-generated text appear in the reference text (focusing on precision). In contrast, ROUGE counts how many $n$-grams from the reference appear in the generated text (focusing on recall). 
However, both metrics primarily capture lexical rather than semantic similarity, which may not fully reflect the accuracy of the generated descriptions.
To address this limitation, we take the initiative to apply embedding similarity and GPT score for evaluation.
% we propose two innovative metrics, embedding similarity and GPT score. 
Embedding similarity calculates the cosine similarity between vector representations of generated and ground truth descriptions, using embeddings derived from OpenAI's text-embedding-3-large model. Meanwhile, GPT score uses GPT-4 to quantify the semantic coherence between descriptions by assigning a similarity score from 0 to 1, where 1 indicates perfect semantic alignment.
These metrics provide a more nuanced evaluation by capturing the semantic essence of the descriptions, thus offering a more accurate assessment than previous methods.
For details on the prompt used to calculate the GPT score, please refer to Appendix~\ref{appendix:gpt_score}.

\begin{table}[ht]
    \centering
    \vspace{-10pt}
    \caption{Evaluation results on Verilog understanding using the benchmark proposed in Section~\ref{sec:understanding_benchmark}. BLEU-4 denotes the smoothed BLEU-4 score, and Emb. Sim. represents the embedding similarity metric. Best results are highlighted in bold.}
    \vspace{5pt}
    \label{tab:understanding_results}
    \small{
    \begin{tabular}{@{}l|ccccccc@{}}
    \toprule
       Model  & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & Emb. Sim. & GPT Score\\
    \midrule
       GPT-3.5 & 4.75 & 35.46 & 12.64 & 32.07 & 0.802 & 0.641 \\ 
       GPT-4 & 5.36 & 34.31 & 11.31 & 30.66 & 0.824 & 0.683 \\
       o1-preview & 6.06 & 34.27 & 12.25 & 31.01 & 0.806 & 0.643\\
    \midrule
       CodeT5+-220m & 0.28 & 7.10 & 0.34 & 6.18 & 0.313 & 0.032 \\ 
       CodeT5+-16b & 0.10 & 1.37 & 0.00 & 1.37 & 0.228 & 0.014 \\
       LLaMA2-70B-Chat & 2.86 & 28.15 & 10.09 & 26.12 & 0.633 & 0.500 \\ 
       DeepRTL-220m-direct & 11.99 & 40.05 & 20.56 & 37.09 & 0.793 & 0.572\\ 
       DeepRTL-16b-direct & 11.06 & 38.12 & 18.15 & 34.85 & 0.778 & 0.533 \\ 
    % \midrule
    %    CodeT5+-220m & \\
    %    CodeT5+-16b & \\
    %    LLaMA-70B-Chat & \\
    \midrule
       DeepRTL-220m & 18.66 & \textbf{47.69} & \textbf{29.49} & 44.02 & \textbf{0.837} & \textbf{0.705}\\
       DeepRTL-16b & \textbf{18.94} & 47.27 & 29.46 & \textbf{44.13} & 0.830 & 0.694\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table}



\subsection{Generation Evaluation}
To evaluate LLMs' capabilities in Verilog generation, we adopt the latest benchmark introduced by~\citet{chang2024natural}, which is an expansion based on the previous well-established RTLLM benchmark~\citep{lu2024rtllm}.
The benchmark by~\citet{chang2024natural} encompasses a broad spectrum of complexities across three categories: arithmetic, digital circuit logic, and advanced hardware designs.
This benchmark extends beyond previous efforts by incorporating a wider range of more challenging and practical Verilog designs, thus providing a more thorough assessment of the models' capabilities in generating Verilog code.

The evaluation focuses on two critical aspects: syntax correctness and functional accuracy. We use the open-source simulator iverilog~\citep{williams2002icarus} to assess both syntactic and functional correctness of Verilog code generated by LLMs. 
For the evaluation metric, we adopt the prevalent Pass@$k$ metric, which considers a problem solved if any of the $k$ generated code samples pass the compilation or functional tests~\citep{pei2024betterv}. For this study, we set $k$ values of 1 and 5, where a higher Pass@$k$ score indicates better model performance.
To further delineate the models' capabilities, we track the proportion of cases that pass out of 5 generated samples and compute the average as the success rate. For syntax correctness, this success rate measures the proportion of code samples that successfully compile and, for functional accuracy, the fraction that passes unit tests.


% evaluation benchmark
% the scope of the benchmark is also important
% comparing the results with the software programming language
% we use less data to achieve comparable performance of models trained with abundant software codes
% maybe we could state that the simple functional description is a more challenging case
% some works even do not compare the results with GPT-3.5/GPT-4, like Thakur's work
% we need to state that the evaluation benchmark is not overlapped with the training dataset
% we could mention that the previous software code understanding and generation tasks are relatively simple by showing that the maximum input and output lengths are relatively small for the software code model
% failure case analysis may also be interesting




