\section{Dataset and Understanding Benchmark}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{fig/cot_v6.pdf}
    \vspace{-12pt}
    \caption{The overview of the data annotation process. We employ the CoT approach and the SOTA LLM, GPT-4, for annotation. Annotations span three levels—line, block, and module—providing both detailed specifications and high-level functional descriptions.}
    \label{fig:cot}
\end{figure}
In this section, we introduce our dataset designed to enhance Verilog understanding and generation, which aligns natural language with Verilog code across line, block, and module levels with detailed and high-level descriptions.
By integrating both open-source and proprietary code, the dataset offers a diverse and robust collection that spans a broad spectrum of hardware design complexities.
% By integrating open-source and proprietary code, we ensure a diverse and robust dataset encompassing a wide range of hardware designs. 
We employ GPT-4 along with the CoT approach for annotation, achieving about $90\%$ accuracy in human evaluations, confirming the dataset's high quality.
We also introduce the first benchmark for Verilog understanding, setting a new standard for evaluating LLMs' capabilities in interpreting Verilog code.





\subsection{Dataset Source}
Our dataset comprises both open-source and proprietary Verilog code. For the open-source part, we gather \texttt{.v} files from GitHub repositories using the keyword \texttt{Verilog}.
These files are segmented into individual modules, each representing a distinct functional unit within the Verilog design.
This segmentation is crucial given the limited context length of current LLMs, improving the efficiency and accuracy of the subsequent annotation and fine-tuning processes.
We employ MinHash and Jaccard similarity metrics~\citep{yan2017privmin} to deduplicate these modules and exclude those predominantly made up of comments or lacking complete \texttt{module} and \texttt{endmodule} structures.
Finally, this process results in a total of 61,755 distinct Verilog modules.
For the proprietary portion, we incorporate a set of purchased intellectual properties (IPs) that enhance the variety and functional diversity of our dataset. This component includes a total of 213 high-quality, industry-standard Verilog modules. These IPs not only offer a range of advanced functions but also provide unique insights that complement the open-source data. Integrating these elements ensures a comprehensive dataset that captures a wide spectrum of hardware design practices.


\subsection{Dataset Annotation}
We employ different annotation strategies for open-source and proprietary code. For open-source code, we utilize the CoT approach with the SOTA LLM, GPT-4, to provide annotations at multiple levels. As illustrated in Figure~\ref{fig:cot}, 
%we begin by removing all comments from the original Verilog module (resulting in refined code) and counting the number of tokens using CodeT5+'s tokenizer. 
we initially remove all comments from the original Verilog code (resulting in refined code) to avoid training complications from incorrect or misleading comments.
If the token count of a complete module exceeds $2048$, the maximum context length for CodeT5+, we utilize GPT-4 to segment the module into smaller, manageable blocks such as \texttt{always} blocks. 
If the resulting blocks still exceed $2048$ tokens, we will discard them. 
For modules and blocks with a token count below $2048$ (qualified code), we then use GPT-4 to add informative comments, resulting in commented code (Step 1).
% If the token count is below $2048$, we treat the module as a complete module and use GPT-4 to add informative comments, resulting in commented code (Step 1). 
% From this commented code, we can extract line-level descriptions. To ensure accuracy, GPT-4 is employed to verify that these line-level descriptions are strictly confined to the context of each individual line, avoiding any external or unrelated information. For example, [insert specific examples here].
From this commented code, we can extract line-level descriptions (pairings of single lines of code with natural language descriptions). To guarantee the accuracy and relevance of the inline comments, we use GPT-4o-mini to rigorously check each comment, ensuring that all line-level descriptions are strictly confined to the context of their respective lines without incorporating any extraneous or irrelevant information. For example, consider the line \texttt{"O = I1;"} annotated with \texttt{"Assign the value of I1 to the output O when S is high."}.
Since we cannot deduce from this single line that \texttt{O} is the output and \texttt{S} is related, such descriptions are deemed inaccurate and are consequently excluded from the dataset to maintain training effectiveness.
% we cannot directly infer the information that O is the output and S is related from the single line of code. This type of line-level description is considered ineffective and detrimental to model training, and thus we will discard it.
% \textcolor{changran}{
% Otherwise, we employ GPT-4 to divide the module into smaller manageable code blocks, such as \texttt{always} blocks, which enhances the utilization of the available Verilog code. [state whether certain techniques are used here]. Blocks with token counts below $2048$ are returned to Step 1 for further processing.
% }
% Once we have the commented code, we use GPT-4 to generate a detailed specification for the code (Step 2). This specification comprises of two key components: a summary of the code's functionality (what it does) and a detailed explanation of the implementation process, including data transitions between registers (how it works). This dual-layered specification provides a deeper understanding of the code. [do we only have what+how for specification?]
In Step 2, we use GPT-4 to generate a detailed specification for the commented code from Step1. 
This specification includes two main components: a summary of the code's functionality (what it does) and a comprehensive explanation of the implementation process (how it works). 
% Specifically, for the implementation process, we will require GPT to include descriptions of the input and output ports, an explanation of the internal workings of the module/block, a description of the logical implementation process, an overview of the algorithmic logic used in the implementation, and an explanation of the internally defined signals. 
Finally, in Step 3, we combine the qualified code from Step 1 with the detailed specification generated in Step 2 to create high-level functional descriptions. 
% To ensure precision, we instruct GPT-4 to prioritize the refined code, using the detailed specification as a reference. 
To ensure precision, we instruct GPT-4 to focus on the qualified code, using the detailed specification only as reference. 
% This approach prevents GPT-4 from potentially overlooking some details in lengthy segments of code.
The resulting high-level descriptions succinctly summarize the code's functionality (what it does) and provide a concise overview of the implementation process (how it works).
This annotation phase is the most critical and challenging as it demands that the model captures the code's high-level semantics, requiring a profound understanding of Verilog. In current benchmarks and practical applications, users typically prompt the model with high-level functional descriptions rather than detailed specifications. Otherwise, they would need to invest significant effort in writing exhaustive implementation details, making the process time-consuming and requiring extensive expertise. For detailed prompts used in this annotation process, please refer to Appendix~\ref{appendix:prompt}.
And a detailed explanation of why we discard Verilog modules or blocks exceeding $2048$ tokens can be found in Appendix~\ref{appendix:discard}.


Given the industrial-grade quality of the proprietary code, we engage professional hardware engineers to maintain high annotation standards. Adhering to rigorous industry-level standards, these experts ensure precise and accurate annotations, capturing intricate details and significantly enhancing the dataset's value for advanced applications.
Unlike GPT-generated annotations, these human-annotated ones incorporate an additional layer of granularity with medium-detail block descriptions.
For detailed annotation standards and processes, please refer to Appendix~\ref{appendix:standard}.

% Due to the industrial-grade quality of the proprietary code, we have high annotation standards and thus hire several professional hardware engineers for annotation. 
% We adhere to rigorous industry-level standards for annotation. Consequently, we have engaged several professional hardware engineers to perform the annotation tasks.
% Their expertise ensures precise and accurate annotations, capturing intricate details and enhancing the dataset's overall value for advanced applications. Specifically, [complete the annotation details].

\begin{table}[ht]
\centering
\vspace{-10pt}
\caption{The overall statistics of the annotation results for our dataset.}
\vspace{5pt}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Comment Level} & \textbf{Granularity} & \textbf{Count} \\ \hline
Line Level             & N/A                  & 434697 \\ \hline
\multirow{3}{*}{Block Level}  & High-level Description    & 892    \\ \cline{2-3} 
                              & Medium-Detail Description & 1306    \\ \cline{2-3} 
                              & Detailed Description      & 894    \\ \hline
\multirow{2}{*}{Module Level} & High-level Functional Description    & 59448 \\ \cline{2-3} 
                              & Detailed Specification             & 59503 \\ \hline
\end{tabular}

\label{tab:dataset_statistics}
\end{table}

We present the overall statistics of the annotation results in Table~\ref{tab:dataset_statistics}. 
Additionally, Figure~\ref{fig:comment_example} illustrates an example of our comprehensive annotation for a complete Verilog module. 
Notably, the overall dataset encompassing descriptions of various details across multiple levels is used for training.
A similar work to ours is the MG-Verilog dataset introduced by~\citet{zhang2024mg}, including 11,000 Verilog code samples and corresponding natural language descriptions at various levels of details.
However, it has several limitations compared to ours. Firstly, MG-Verilog is relatively small in size and lacks proprietary Verilog code, which diminishes its diversity and applicability. Secondly, it employs direct annotation rather than the CoT approach, which we have found to enhance annotation accuracy as demonstrated in Section~\ref{sec:dataset_evaluation}. 
Besides, our annotation is more comprehensive than that of MG-Verilog, which lacks granularity. We cover line, block, and module levels with both detailed and high-level descriptions, ensuring a strong alignment between natural language and Verilog code.
%Besides, our annotation is more comprehensive than that of MG-Verilog which lacks granularity, covering line, block, and module levels with both detailed and high-level descriptions, ensuring strong alignment between natural language and Verilog code. 
Lastly, MG-Verilog relies on the open-source LLM LLaMA2-70B-Chat for annotation, whereas we use the SOTA LLM GPT-4. In Section~\ref{sec:understanding_evaluation}, we demonstrate that LLaMA2-70B-Chat has a poor understanding of Verilog code, leading to inferior annotation quality in MG-Verilog.
\vspace{-10pt}



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.88\linewidth]{fig/comment_example.pdf}
    \vspace{-12pt}
    \caption{An example of our comprehensive annotation for a complete Verilog module.}
    \label{fig:comment_example}
\end{figure}

\vspace{-10pt}
\subsection{Dataset Evaluation}
\label{sec:dataset_evaluation}
To ensure the quality of our dataset, we assess annotations generated from the CoT process. We randomly sample 200 Verilog modules and engage four professional Verilog designers to evaluate the accuracy of annotations at various levels. This human evaluation indicates that annotations describing high-level functions achieve an accuracy of $91\%$, while those providing detailed specifications attain an accuracy of $88\%$. For line-level annotations, the accuracy is $98\%$. Additionally, we compare the CoT method with the direct annotation approach, where annotations are generated straightforwardly from the original code. This direct annotation method yields only a $67\%$ accuracy, highlighting the significant advantage of integrating the CoT process.

Recent studies in natural language processing (NLP) have demonstrated that LLMs fine-tuned with synthetic instruction data can better understand natural language instructions and show improved alignment with corresponding tasks~\citep{wang2022self,ouyang2022training,taori2023stanford}.
It is important to note that in our work, we also utilize data generated by language models for fine-tuning, including annotations at various levels. While not all annotations are perfectly accurate, we achieve a commendable accuracy of approximately $90\%$. Motivated by~\citet{wang2022self}, we treat those inaccuracies as data noise, and the fine-tuned model on this dataset still derives significant benefits.

\subsection{Understanding Benchmark}
\label{sec:understanding_benchmark}
As the first work to consider the task of Verilog understanding, we introduce a pioneering benchmark to evaluate LLMs' capabilities in interpreting Verilog code. This benchmark consists of 100 high-quality Verilog modules, selected to ensure comprehensive coverage of diverse hardware functionalities, providing a broad assessment scope across different types of hardware designs. We have engaged four experienced hardware engineers to provide precise annotations on each module’s functionalities and the specific operations involved in their implementations. These initial annotations are then rigorously cross-verified by three additional engineers to guarantee accuracy and establish a high standard for future model evaluations. This benchmark fills a critical gap by providing a standardized means to assess LLMs on interpreting Verilog code and will be released later. For detailed examples included in the benchmark, please refer to Appendix~\ref{appendix:benchmark}.
