\begin{abstract}
\iffalse
Recent advances in large language models (LLMs) have demonstrated significant potential in automating the generation of hardware description language (HDL) code from high-level natural language instructions. 
Researchers have enhanced these models through fine-tuning to improve their capabilities in hardware design.
However, previous works have predominantly focused on Verilog generation, neglecting the crucial task of Verilog understanding. Additionally, the lack of strong alignment between natural language descriptions and Verilog code has impeded improvements in generating high-quality code. To address these challenges, we introduce DeepRTL, a unified representation model designed to enhance both Verilog understanding and generation. With CodeT5+ as a foundation, we fine-tune it on a curated dataset that aligns Verilog code with comprehensive natural language descriptions at multiple levels.
To evaluate LLMs' understanding capabilities of Verilog code, we develop the first benchmark for Verilog understanding and introduce new metrics, including embedding similarity and GPT score, which measure the semantic similarity of natural language descriptions. Compared to traditional metrics like BLEU and ROUGE scores that calculate n-gram overlap, these new metrics provide more accurate evaluations of LLMs' understanding capabilities.
By employing a progressive training strategy, DeepRTL significantly outperforms GPT-4 in understanding tasks and matches the performance of 
OpenAI's o1-preview in generation tasks. 
% This work establishes a new baseline for Verilog code understanding and generation, offering advanced tools for hardware design.
\fi 

Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. 
We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.

% We also introduce the first benchmark for Verilog understanding, alongside two novel metrics, embedding similarity and GPT score, that capture semantic similarity more accurately than traditional metrics like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. DeepRTL's progressive training strategy enables it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks. 

\end{abstract}