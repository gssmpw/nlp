\section{Conclusion}

In this work, we introduce DeepRTL, a novel unified representation model that bridges Verilog understanding and generation. 
It is fine-tuned on a meticulously curated dataset featuring multi-level natural language descriptions of Verilog code, encompassing line, block, and module levels with both detailed and high-level functional descriptions.
DeepRTL not only addresses the gaps in previous methods focused solely on Verilog code generation but also ensures strong alignment between Verilog code and natural language. 
Moreover, we establish the first benchmark for evaluating LLMs' capabilities in Verilog understanding. To overcome the limitations of traditional metrics like BLEU and ROUGE, which primarily assess lexical similarity, 
we apply embedding similarity and GPT score for evaluating the model's understanding capabilities.
% we introduce two novel metrics, embedding similarity and GPT score. 
These metrics are designed to evaluate the semantic similarity of descriptions more accurately, thus better reflecting the precision of generated descriptions. 
By implementing a curriculum learning strategy, DeepRTL demonstrates superior performance in both Verilog understanding and generation tasks. Specifically, it surpasses the SOTA LLM, GPT-4, across all understanding metrics and achieves performance comparable to OpenAI's o1-preview in terms of syntax correctness and functional accuracy for Verilog generation.