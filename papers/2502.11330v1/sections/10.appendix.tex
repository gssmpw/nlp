

% \section{Rubrics of Automatic Evaluation}


\section{Data Statistics}
\label{app:data_statistics_appendix}
\paragraph{Statistics of generated tags.} As we stated in limitations section, we provide the statistics of generate special tag tokens in Table~\ref{app:tag_statistics}.
We find out that most of the <<Role>>, <<Content>>, <<Task>> tokens are annotated in the instances.
Compared to thoses tokens, <<Action>>, <<Style>>, <<Background>>, and <<Format>> depends on the user instructions to be generated.
However, <<Tool>> tokens have shown absolutely low portion to be generated.
We thus want to suggest that properly choosing the public or your own dataset seems to ensure the <<Tool>> tag usages such as selecting searching protocols or function calls. 
\input{table/tag_statistics}

\input{table/data_statistics_appendix}
\paragraph{Statistics of original SFT datasets.}
In Table~\ref{tab:data_statistics_appendix}, we observe that most widely used public datasets either lack a system message entirely or include only a simple one, such as "You are a helpful AI assistant.".
The publicly available data mostly cover mathematics, code problems following some reasoning and logical ones.




\section{Experimental Details}
\label{app:experimental_details}
\paragraph{Computing Resources}
We use 4x8 NVIDIA H100 Tensor Core GPU with 80GB memory to train the open-source models.
We use Deepspeed stage 3~\citep{rajbhandari2020zero} to implement multi-GPU settings and FlashAttention~\citep{dao2022flashattention} for efficient training.
Our code is written in PyTorch~\citep{paszke2019pytorch} and HuggingFace~\citep{wolf2019huggingface}.


\paragraph{Integrating system roles in models that do not support them.}
\label{app:system_role_support}
Through our experiments, we find out that the Gemma-2-9b-it~\citep{team2024gemma} model does not inherently support the system role.
To address this limitation during data generation and training, we modified the chat template in the configuration of tokenization to remove restrictions on the system role.
Interestingly, despite the lack of native support, our findings show that \textsc{SysGen} data can still be utilized effectively to incorporate a system role into these models.

\section{Qualitative analysis of generated instances}
\label{app:qualitative_analysis}

\input{table/qualitative_analysis}
In Table~\ref{app:qualitative_analysis}, we provide the \textsc{SysGen} data by presenting the system messages, user instructions, and new assistant responses.
We observe that providing a specific format such as answer with paragraph format steers the LLM's behavior to answer in step-by-step processes within paragraph.
Also, if conversational example was provided, then the phrase of style tag forces to generate assistant response friendly.
Furthermore, if the system message grant specific roles such as a knowledgeable assistant, then the new assistant responses tend to generate verbose answers to the user instructions.

\section{Prompts}
\label{app:prompt}
To enhance reproducibility and facilitate understanding of the \textsc{SysGen} pipeline, we provide multiple prompts that we utilized.
In Table~\ref{tab:app_prompt_system_generation}, we use three-shot demonstrations to generate useful system messages which are collected through real-world scenarios.
The \textit{Conversational History} written in the prompt is composed of user instructions and original assistant responses.
Thus, given the user instructions and assistant responses, we generate the system messages at a phrase level containing eight functionalities with special tokens such as <<Role>>, <<Content>>, and <<Style>>.

After generating the system messages, in Table~\ref{tab:app_prompt_system_tag_check}, we verify the quality of each tag with three classes: Good, Bad, and None.
We want to note that the \textit{Annotated system messages}, composed of phrases and tags, are used to verify the \textit{Filtered system messages}.
By utilizing LLM-as-a-judge approach, we could save tremendous budgets through self-model feedbacks rather than using proprietary models (i.e., API Calls). 
Through our preliminary experiment, we observe that current open-source models such as Phi-4 or Qwen2.5-14b-instruct could preserve most of the phrases after applying phase 3.

Table~\ref{tab:app_prompt_answer_quality_check} shows the prompt of how we verify the quality of new assistant responses as shown in Figure~\ref{fig:answer_comparison}.
After prompting 1K randomly sampled instances, we observe that new assistant responses were qualified to be better aligned with user instructions.
\input{table/system_generation_prompt}
\input{table/prompt_tag_check}
\input{table/answer_quality_prompt}