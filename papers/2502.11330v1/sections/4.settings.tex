\section{Experimental Settings}

\input{table/data_statistics}
\input{table/experiments}

\subsection{Training Dataset}
In Table~\ref{tab:data_statistics}, we provide the remaining instances after processing each phase of our generated datasets.
We target datasets with three conditions: (1) widely used as SFT datasets; (2) do not contain the system messages; (3) diverse domains are covered. 
We enumerate the selected datasets as follows: 
(1) Capybara~\citep{daniele2023amplify-instruct}, which focuses on information diversity across a wide range of domains.
(2) Airoboros~\citep{airoboros3.1} is composed of multi-step instructions with a diverse structured format.
(3) Orcamath~\citep{mitra2024orcamath} aims to provide various mathematical problem solving.
(4) MetamathQA~\citep{yu2023metamath} is an augmented version of several math instructions.
(5) Magicoder~\citep{luo2023wizardcoder} dataset provides various code generation problems.
We provide detailed statistics in Appendix~\ref{app:data_statistics_appendix}.




\subsection{Evaluation Benchmarks}
% Our primary goal is to ensure that the system messages generated using our \textsc{SysGen} piepline lead to less performance degradation on existing benchmarks while assessing how effectively they function in system-related benchmarks.
% To achieve this, we evaluate performance on  Multifacet~\citep{lee2024aligning}, which requires both a system message and a user instruction to generate the model response.
We evaluate performance on  Multifacet~\citep{lee2024aligning}, which requires both the system messages and the user instructions to generate the assistant responses.
For the source data, the Multifacet benchmark is constructed of approximately 921 samples by incorporating AlpacaEval~\citep{dubois2024length}, FLASK~\citep{ye2023flask}, MT-bench~\citep{bai2024mt}, Koala~\citep{koala_blogpost_2023}, and Self-Instruct~\citep{wang2022self}.
The authors of~\citet{lee2024aligning} set the multiple aspects of evaluating each response with four dimensions: style, background information, harmlessness, and informativeness.
We follow these evaluation settings in our experiments.

Additionally, we aim to investigate the impact of the \textsc{SysGen} data on unseen benchmarks by leveraging the Open LLM Leaderboard 2~\citep{myrzakhan2024open} as a test set.
The test set is composed of MMLU~\citep{hendrycks2020measuring}, MMLU-pro~\citep{wang2024mmlu}, Arc-challenge~\citep{clark2018think}, GPQA~\citep{rein2023gpqa}, HellaSwag~\citep{zellers2019hellaswag}, IFEVAL~\citep{zhou2023instruction}, MATHQA~\citep{amini-etal-2019-mathqa}, and BBH~\citep{suzgun2023challenging}.
We use the publicly available lm-evaluation harness~\citep{eval-harness} as an evaluation tool for a fair comparison.



\subsection{Open-source Models}
Our baseline models are composed of instruction-tuned open-source models and trained with supervised fine-tuning datasets without system messages.
We select and utilize one from each widely used open-source model family: 
(1) Solar-10.7B-instruct~\citep{kim-etal-2024-solar} (2) Gemma-2-9B-instruct~\citep{team2024gemma} (3) LLaMA-3.1-8B-instruct~\citep{meta2024introducing} (4) Qwen2.5-14B-instruct~\citep{yang2025qwen2}, and (5) Phi-4~\citep{abdin2024phi}.