
\section{Experiments}
\input{table/knowledge_distillation}
\input{table/experiments_unseen}
The primary goal of \textsc{SysGen} pipeline is to enhance the utilization of the \emph{system role} while minimizing performance degradation on unseen benchmarks, thereby improving the effectiveness of supervised fine-tuning (SFT).
To validate this, we evaluate how well the models trained on \textsc{SysGen} data generate appropriate assistant responses given both the system messages and user instructions, using the Multifacet~\citep{lee2024aligning} dataset.
For models that cannot generate data independently, we apply knowledge distillation to assess their effectiveness.
Additionally, we leverage the widely used Open LLM Leaderboard 2~\citep{myrzakhan2024open} as an unseen benchmark to determine whether our approach can be effectively integrated into existing SFT workflows.


\paragraph{\textsc{SysGen} provides better system message and assistant response to align with user instructions.}
Given the system messages and user instructions, the assistant's response is evaluated across four dimensions: style, background knowledge, harmlessness, and informativeness.
Each of these four aspects is scored on a scale of 1 to 5 using a rubric, and the average score is presented as the final score for the given instruction.
As shown in Table~\ref{tab:main_experiments}, recent open-source models achieve comparable scores to the proprietary models, indicating that open-source models have already undergone training related to system roles~\citep{meta2024introducing, yang2024qwen2, abdin2024phi}.

When trained on \textsc{SysGen} data, both LLaMA (4.12 → 4.21) and Phi (4.41 → 4.54) show score improvements.
Among the four dimensions, LLaMA exhibits score increases in style (4.15 → 4.32) and harmlessness (4.23 → 4.29).
Similarly, Phi shows the improvements in style (4.42 → 4.61) and informativeness (4.37 → 4.49).
As a result, even open-source models that have already been trained on system roles demonstrate their positive effects on style, informativeness, and harmlessness.



\paragraph{Knowledge distillation through \textsc{SysGen} data.}
If an open-source model does not support the system roles, it may not generate the system messages properly using \textsc{SysGen} pipeline. 
However, the effectiveness of knowledge distillation, using data generated by another open-source model without the limitation, remains uncertain.
To explore this, we train Gemma~\citep{team2024gemma} and Solar~\citep{kim-etal-2024-solar} using data generated by Phi-4~\citep{abdin2024phi}.
We use the Phi-4 data because it preserves most of the data and provides high quality  assistant responses as shown in Table~\ref{tab:statistics_generated_answer} and \ref{tab:data_statistics}.

As shown in Table~\ref{tab:knowledge_distillation}, even for models that do not inherently support system roles, modifying the chat template to incorporate system role and training on knowledge distilled dataset leads to an improvement in Multifacet performance, as observed in Gemma (4.05 → 4.23).
We describe the details in the Appendix~\ref{app:system_role_support}.
Additionally, for the Solar model, which had not been trained on system roles, we observe a dramatic performance improvement (3.19 → 3.76).\footnote{We speculate that Solar model did not properly learn the system role because its initial Multifacet score was low.}
This demonstrates that the data generated by the \textsc{SysGen} pipeline effectively supports the system roles.


\paragraph{\textsc{SysGen} data minimizes the performance degradation in unseen benchmarks.}
When incorporating system messages that were not present in the original SFT datasets and modifying the corresponding assistant responses, it is crucial to ensure that the model’s existing performance should not degrade.
For example, one key consideration in post-training is maintaining the model's original performance.
To assess this, we observed performance difference in unseen benchmark after applying supervised fine-tuning.
As shown in Table~\ref{tab:unseen_experiments}, we use the Open LLM Leaderboard 2 dataset as an unseen benchmark, with performance categorized into four groups:
\begin{itemize}
    \item Performance of existing open-source models (row 1-6)
    \item Performance of fine-tuning with open-source models using SFT datasets (row 7-12)
    \item Performance of fine-tuning with \textsc{SysGen} data (row 13-16)
    \item Performance after applying knowledge distillation using Phi-4 \textsc{SysGen} data (row 17-19)
\end{itemize}
The average performance degradation reflects the scores missing from each open-source model's original performance (row 1-6).

When fine-tuning with independently generated data using \textsc{SysGen}, the performance degradation is significantly lower than fine-tuning with the original SFT datasets selected under the same conditions.
Additionally, even for models that cannot generate data independently (e.g., those that do not support system roles), knowledge distillation helps mitigate performance drops considerably.








