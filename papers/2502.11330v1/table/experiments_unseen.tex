\begin{table*}[t]
\centering
{\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Parameter}\\ \textbf{Scale}\end{tabular}}  & \multicolumn{8}{c}{\textbf{Unseen Benchmarks}} & \multirow{2}{*}{\textbf{Average}} \\  \cmidrule{3-10} 
\multicolumn{1}{c}{} &  & \multicolumn{1}{l}{\textbf{MMLU}} & \multicolumn{1}{l}{\textbf{MMLU-Pro}} & \multicolumn{1}{l}{\textbf{ARC-c}} & \multicolumn{1}{l}{\textbf{GPQA}} & \multicolumn{1}{l}{\textbf{HellaSwag}} & \multicolumn{1}{l}{\textbf{IFEVAL}} & \textbf{MATHQA} & \textbf{BBH} & \\ \midrule
\multicolumn{11}{l}{\textit{Open-Source Models}} \\ \midrule 
Solar-10.7B-instruct & 10.7B  &  63.28 & 30.20 & 63.99 & 30.36 & 86.35 & 38.59 &  36.38 & 37.28 & 48.31 \\
Gemma-2-9b-it & 9B & 73.27 & 32.78 & 67.89 & 31.05 & 81.92 & 74.78 & 38.87 & 41.98 & 55.31 \\ 
LLaMA-3.1-8B-instruct & 8B & 67.95 & 40.87 & 54.95 & 34.60 & 79.18 & 50.71 & 39.53 & 70.85 & 54.83 \\ 
% Mixtral-8x22B-instruct & 8x22B & 75.62 &  52.63 & 67.83 & 36.83 & 87.68 & 60.43 & 50.08 & 83.03 & \\   
Qwen2.5-14B-instruct & 14B &  79.73  & 51.22 & 67.39 & 45.51 & 82.31 & 79.83 & 42.12 & 78.25 & 65.79 \\ 
Phi-4 & 14B & 84.56 & 70.12  & 68.26 & 55.93 & 84.42 & 62.98 & 48.87 & 79.87 & 69.37 \\  
\midrule
\multicolumn{11}{l}{\textit{Open-Source Models (Fine-tuning on original SFT Dataset)}} \\ \midrule
Solar-10.7B-instruct & 10.7B & 62.38 & 29.12 & 58.87 & 29.17 & 81.58 & 31.27 & 37.21 & 32.85 & 45.30 (-3.01) \\ 
Gemma-2-9b-it & 9B & 71.85 & 31.67  & 62.57 & 30.51 & 77.54 & 69.25 & 39.12 & 37.25 & 52.47 (-2.84) \\ 
LLaMA-3.1-8B-instruct & 8B & 65.34 &  36.85 &  
54.18 & 33.93 & 77.98 & 35.64 & 40.03 & 62.83 & 50.85 (-3.98)  \\ 
% Mixtral-8x22B-instruct & 8x22B&  &   & & & & & &  & \\ 
Qwen2.5-14B-instruct & 14B & 75.87  & 49.85  & 66.89 & 43.98 & 80.99 & 62.57 & 43.28 & 71.17 & 61.82 (-3.97) \\ 
Phi-4 & 14B & 80.27 & 66.58  & 66.27 & 52.89 & 83.39 & 55.83 & 49.98 & 75.49 & 66.33 (-6.04) \\ 
\midrule
\multicolumn{11}{l}{\textit{Open-Source Models (Fine-tuning on \textbf{\textsc{SysGen}} dataset)}} \\ \midrule 
LLaMA-3.1-8B-instruct & 8B & 66.89 & 39.77 & 54.55 & 34.21 & 78.89 & 46.75 & 42.11 & 68.98 & 54.02 (-0.81) \\ 
% Gemma-2-9B-instruct & 9B &  &   & & & & & &  & \\ 
% Mixtral-8x22B-instruct & 8x22B&  &   & & & & & &  & \\ 
% Solar-10.7B-instruct & 10.7B & 63.28 & 30.20 & 63.99 & 30.36 & 86.35 & 38.59 &  36.38 & 37.28 & \\ 
Qwen2.5-14B-instruct & 14B & 78.92 & 43.38 & 66.82 & 44.46 & 80.98 & 74.59 & 43.23 & 76.28 & 63.58 (-2.20) \\ 
Phi-4 & 14B & 83.27 & 68.77  & 67.89 & 55.18 & 84.31 & 57.87 & 50.23 & 77.12 & 68.08 (-1.29) \\ 
\midrule
\multicolumn{11}{l}{\textit{Open-source Models} $+$ \textit{Knowledge Distillation (Fine-tuning on \textbf{\textsc{SysGen}} dataset))}} \\
\midrule 
Solar-10.7B-instruct & 10.7B & 59.98  & 29.26  & 62.81 & 30.25 & 85.91 & 34.58 & 38.25 & 35.97 & 47.12 (-1.19) \\ 
Gemma-2-9b-it & 9B & 72.19 & 31.56 & 66.75 & 30.89 & 81.53 & 71.37 & 40.27 & 40.38 & 54.37 (-0.94) \\ 
\bottomrule
\end{tabular}}}
\caption{We utilize the Open LLM Leaderboard 2 score as the unseen benchmark. This reveals the key finding that adding system messages to existing SFT datasets does not lead to significant performance degradation.}
\label{tab:unseen_experiments}
\end{table*}