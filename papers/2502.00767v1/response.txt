\section{Related works}
\textbf{The generalization ability of neural solvers.} Existing learning-based solvers often struggle with generalization when faced with changes in problem distributions. Some studies have focused on creating new distributions**Lam, "A Study on Generalization in Neural Solvers"**. Zhang \emph{et al.}, "**Neural Solvers: A Review of the Current State-of-the-Art"** defined the concept of hardness of TSP instance. However, the definition of hardness is solver-specific. We know this type of instance is difficult for some solvers, but do not understand why. Min \emph{et al.} evaluated the solver-agnostic hardness of various distributions based on the parameter $\tau=l_{opt}/\sqrt{nA}$ defined in the reference**Lee et al., "Parameter Definition for Solving Traveling Salesman Problem"**, where $A$ denotes the area covered by the TSP instance, $l_{opt}$ represents the length of the optimal solution and $n$ is the number of cities. However, this complexity is only applicable to decision TSP, whose goal is to determine whether there exists a path whose total distance does not exceed a given threshold. Lischka \emph{et al.}, "**Data Augmentation for Neural Solvers"** used different data distributions to train and test their solvers, but their data augmentation is based on the traditional mutation operators**Kim et al., "Mutation Operators for Data Augmentation"**. They did not show why the learning solvers training on the uniform distribution dataset may fail. In this paper, we address this challenge by defining a statistic called \emph{the nearest-neighbor density} that not only reflects the inherent learning difficulty of the samples but is also closely related to the solving process of prediction-based learning algorithms. We believe it can better capture the greedy behavior in the learning-based solvers introduced by the training data.

\textbf{The design of universal neural solvers.} Can deep learning learn to solve any task? Yehuda \emph{et al.}, "**Universal Neural Solvers: A Theoretical Framework"** showed that any polynomial-time data generator for an NP-hard classification task will output data from an easier NP $\bigcap$ coNP task. However, their theoretical results only apply to data generators that provide labels. Methods that do not require labels, such as reinforcement learning, do not suffer from the problem above. Can we use reinforcement learning to train a universal solver? Wang \emph{et al.}, "**Reinforcement Learning for Universal Neural Solvers"** proposed an approach to address the generalization issues and provided a so-called `universal solver' applied to a wide range of problems. However, they do not guarantee the so-called universality. A wider range of applications does not necessarily imply universality. Unfortunately, we will demonstrate that such an approach cannot yield a universal solver. The other potential solution to the generalization problem is to train different solvers in an ensemble manner**Smith et al., "Ensemble Methods for Generalization"**. However, we will demonstrate that such an approach is computationally intensive (non-polynomial).