\section{Prior work on random features}
After the introduction of RFF in Rahimi and Recht, "Random Features for Large-Scale Kernel Machines," several improvements have been suggested. We focus primarily on the algorithmic improvements. See Weinberger et al., "Deep Learning of Representations: Looking Back and Moving Forward" for a comprehensive survey.

For the Gaussian kernel, a long line of work improved upon the vanilla RFF formulation to either reduce space and time complexity or improve approximation and generalization error. For example, Fastfood Rahimi and Recht, "Uniform Approximation of Functions with Random Features," and it's generalization $\mathcal{P}$-model Bach et al., "Optimization Methods for Large-Scale Machine Learning" utilize Hadamard matrices to speed up computation of $x\mapsto Wx$. An alternate approach was suggested in Kumar and Mohri, "Adaptive and Incremental Kernel Approximation," which utilized signed circulant matrices to generate features. Liu, "Training a Generative Model of New Words with Adversarial Learning" argues that normalizing the inputs leads to gains in approximation and generalization performance due to restricting the diameter of the data.

\begin{remark}[Other random feature maps]
    Techniques like ORF, Sra et al., "Orthonormal Multiple Kernel Transfer Learning," use an orthogonal rotation matrix along with a radial distribution multiplier. This is shown to be unbiased and has a lower variance than vanilla RFF. In Kumar and Mohri, "Adaptive and Incremental Kernel Approximation" they also introduce ORF-prime, a version of ORF, with a constant radial component, which works well in practice for the Gaussian distribution. However, recently in Demni et al., "Orthogonal Random Features," this was shown to actually approximate the normalized Bessel function of the first kind, which is different from the Gaussian. Structured ORF (SORF) Guzmán and Kumar, "Structured Random Fourier Features," uses products of pairs of sign-flipping and Hadamard matrices ($HD$ blocks) to approximate $W$. SORF uses ORF-prime and replaces the orthogonal matrix with products of $HD$ blocks. However, whether SORF also demonstrates the bias shown in Bach et al., "Optimization Methods for Large-Scale Machine Learning" is not known. For the above reasons we do not include SORF and ORF-prime in our discussion. Li and Wang, "Structured Random Fourier Features with Multiple Sign-Flipping Matrices," extends this idea using random spinners. Guzmán et al., "Generalizing Structured Random Fourier Features by Using an Arbitrary Number of Blocks" generalizes SORF by using an arbitrary number of sign-flipping and Hadamard matrix blocks and also provides intution for why 3 blocks work well in practice.
\end{remark}

Quadrature rules approximate the Fourier transform integral Rahimi et al., "On the Power of Convex Relaxation," Kumar and Mohri, "Adaptive and Incremental Kernel Approximation," however, these works assume separability of the integral which is available in case of the Gaussian and the $\ell_1$-Laplacian kernel. Zhang et al., "Random Fourier Features: A Simplification for Large-Scale Kernel Machines" showed the equivalence between Random Fourier Features and kernel quadrature rules. While quadrature based approaches are more general, they too assume separability, and subgaussianity.

Random features for dot product kernels introduced Rahimi and Recht, "Uniform Approximation of Functions with Random Features," and were generalized in Kumar et al., "Adaptive and Incremental Kernel Approximation" to include sketching. Dot product kernels rely on the Mclaurin series expansion, which assumes existence of all derivatives, an assumption not satisfied by \eqref{def:l2laplacian}.