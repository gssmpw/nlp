@article{kimeldorf1971some,
  title={Some results on Tchebycheffian spline functions},
  author={Kimeldorf, George and Wahba, Grace},
  journal={Journal of mathematical analysis and applications},
  volume={33},
  number={1},
  pages={82--95},
  year={1971},
  publisher={Elsevier}
}

@inproceedings{scholkopf2001generalized,
  title={A generalized representer theorem},
  author={Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J},
  booktitle={International conference on computational learning theory},
  pages={416--426},
  year={2001},
  organization={Springer}
}

@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}

@article{radhakrishnan2024synthetic,
  title={Synthetic lethality screening with Recursive Feature Machines},
  author={Radhakrishnan, Adityanarayanan and Cai, Cathy and Weir, Barbara A and Moy, Christopher and Uhler, Caroline},
  journal={Cancer Research},
  volume={84},
  number={6\_Supplement},
  pages={897--897},
  year={2024},
  publisher={AACR}
}

@article{radhakrishnan2024mechanism,
  title={Mechanism for feature learning in neural networks and backpropagation-free machine learning models},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={Science},
  volume={383},
  number={6690},
  pages={1461--1467},
  year={2024},
  publisher={American Association for the Advancement of Science}
}

@article{aristoff2024fast,
  title={The fast committor machine: Interpretable prediction with kernels},
  author={Aristoff, David and Johnson, Mats and Simpson, Gideon and Webber, Robert J},
  journal={The Journal of chemical physics},
  volume={161},
  number={8},
  year={2024},
  publisher={AIP Publishing}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@inproceedings{rahimi2007random,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2007}
}

@article{sutherland2015error,
  title={On the error of random Fourier features},
  author={Sutherland, Danica J and Schneider, Jeff},
  journal={Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
  year={2015}
}

@article{
demni2024orthogonal,
title={Orthogonal Random Features: Explicit Forms and Sharp Inequalities},
author={Nizar Demni and Hachem Kadri},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=FMtRZ4xzSi},
note={}
}

@article{yu2016orthogonal,
  title={Orthogonal random features},
  author={Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@ARTICLE{RFF_survey,
  author={Liu, Fanghui and Huang, Xiaolin and Chen, Yudong and Suykens, Johan A. K.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond}, 
  year={2022},
  volume={44},
  number={10},
  pages={7128-7148},
  keywords={Kernel;Approximation algorithms;Taxonomy;Scalability;Risk management;Prediction algorithms;Loss measurement;Random features;kernel approximation;generalization properties;over-parameterized models},
  doi={10.1109/TPAMI.2021.3097011}}


@article{QMC_RFF,
  author  = {Haim Avron and Vikas Sindhwani and Jiyan Yang and Michael W. Mahoney},
  title   = {Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {120},
  pages   = {1--38},
  url     = {http://jmlr.org/papers/v17/14-538.html}
}

@inproceedings{NRFF,
author = {Li, Ping},
title = {Linearized GMM Kernels and Normalized Random Fourier Features},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098081},
doi = {10.1145/3097983.3098081},
abstract = {The method of "random Fourier features (RFF)" has become a popular tool for approximating the "radial basis function (RBF)" kernel. The variance of RFF is actually large. Interestingly, the variance can be substantially reduced by a simple normalization step as we theoretically demonstrate. We name the improved scheme as the "normalized RFF (NRFF)", and we provide a technical proof of the asymptotic variance of NRFF, as validated by simulations.We also propose the "generalized min-max (GMM)" kernel as a measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an associate hashing method named "generalized consistent weighted sampling (GCWS)" which linearizes this (nonlinear) kernel. We provide an extensive empirical evaluation of the RBF and GMM kernels on more than 50 datasets. For a majority of the datasets, the (tuning-free) GMM kernel outperforms the best-tuned RBF kernel.We then conduct extensive classification experiments for comparing the linearized RBF kernel using NRFF with the linearized GMM kernel using GCWS. We observe that, in order to reach a similar accuracy, GCWS typically requires substantially fewer samples than NRFF, even on datasets where the original RBF kernel outperforms the original GMM kernel. As the training, storage, and processing costs are directly proportional to the sample size, our experiments can help demonstrate that GCWS would be a more practical scheme for large-scale machine learning applications.The empirical success of GCWS (compared to NRFF) can also be explained theoretically, from at least two aspects. Firstly, the relative variance (normalized by the squared expectation) of GCWS is substantially smaller than that of NRFF, except for the very high similarity region (where the variances of both methods approach zero). Secondly, if we are allowed to make a general model assumption on the data, then we can show analytically that GCWS exhibits much smaller variance than NRFF for estimating the same object (e.g., the RBF kernel), except for the very high similarity region.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {315–324},
numpages = {10},
keywords = {normalized random fourier features, large-scale machine learning, hashing, gmm kernel},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@book{Stein1971,
 ISBN = {9780691080789},
 URL = {http://www.jstor.org/stable/j.ctt1bpm9w6},
 abstract = {The authors present a unified treatment of basic topics that arise in Fourier analysis. Their intention is to illustrate the role played by the structure of Euclidean spaces, particularly the action of translations, dilatations, and rotations, and to motivate the study of harmonic analysis on more general spaces having an analogous structure, e.g., symmetric spaces.},
 author = {Elias M. Stein and Guido Weiss},
 publisher = {Princeton University Press},
 title = {Introduction to Fourier Analysis on Euclidean Spaces (PMS-32)},
 urldate = {2024-12-01},
 year = {1971}
}

@book{rudin2017fourier,
  title={Fourier analysis on groups},
  author={Rudin, Walter},
  year={2017},
  publisher={Courier Dover Publications}
}

@book{steinwart2008support,
  title={Support Vector Machines},
  author={Steinwart, I. and Christmann, A.},
  isbn={9780387772424},
  lccn={2008932159},
  series={Information Science and Statistics},
  url={https://books.google.co.in/books?id=HUnqnrpYt4IC},
  year={2008},
  publisher={Springer New York}
}

@article{Matern,
title = {The Matérn function as a general model for soil variograms},
journal = {Geoderma},
volume = {128},
number = {3},
pages = {192-207},
year = {2005},
note = {Pedometrics 2003},
issn = {0016-7061},
doi = {https://doi.org/10.1016/j.geoderma.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0016706105000911},
author = {Budiman Minasny and Alex. B. McBratney},
abstract = {The variogram is important in pedometrics for describing and quantifying soil spatial variability. Therefore, it is essential to have a model that can describe various spatial processes and to use appropriate techniques for estimating its parameters. The Matérn model is a generalization of several theoretical variogram models that incorporates a smoothness parameter. We show the flexibility of the Matérn model using simulation and apply the Matérn model to some soil data in Australia. Parameters of the Matérn model were determined by restricted maximum likelihood (REML), and weighted nonlinear least-squares (WNLS) on the empirical variogram. The Matérn model is shown to be flexible and can be used to describe many isotropic spatial soil processes. The REML method fits the local spatial process correctly, however the drawback is the lengthy computation. Meanwhile WNLS fits only the shape of the calculated empirical variogram, and parameters estimated from WNLS can be misleading. From this study, the smoothness parameter of soil data from point measurement appears to be in the range of 0.25–0.50 and can be considered to be a rough spatial process.}
}

@book{Kotz_Nadarajah_2004, place={Cambridge}, title={Multivariate T-Distributions and Their Applications}, publisher={Cambridge University Press}, author={Kotz, Samuel and Nadarajah, Saralees}, year={2004}}

@article{joarder1996characteristic,
  title={On the characteristic function of the multivariate t-distribution},
  author={Joarder, Anwar H and Ali, Mir M},
  journal={Pakistan Journal of Statistics},
  volume={12},
  pages={55--62},
  year={1996},
  publisher={Nusrat Press}
}


@article{characteristic_t,
title = "A simple proof of the characteristic function of Student's t-distribution",
abstract = "This note presents a simple proof of the characteristic function of Student's t-distribution. The method of proof, which involves finding a differential equation satisfied by the characteristic function, is applicable to many other distributions.",
keywords = "Student's t-distribution;, characteristic function;, modified Bessel function.",
author = "Robert Gaunt",
year = "2020",
doi = "10.1080/03610926.2019.1702695",
language = "English",
journal = "Communications in Statistics: Theory and Methods",
issn = "0361-0926",
publisher = "Taylor & Francis",
}


@InProceedings{geom_random,
  title = 	 {The Geometry of Random Features},
  author = 	 {Choromanski, Krzysztof and Rowland, Mark and Sarlos, Tamas and Sindhwani, Vikas and Turner, Richard and Weller, Adrian},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1--9},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/choromanski18a/choromanski18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/choromanski18a.html},
  abstract = 	 {We present an in-depth examination of the effectiveness of radial basis function kernel (beyond Gaussian) estimators based on orthogonal random feature maps. We show that orthogonal estimators outperform state-of-the-art mechanisms that use iid sampling under weak conditions for tails of the associated Fourier distributions. We prove that for the case of many dimensions, the superiority of the orthogonal transform can be accurately measured by a property we define called the charm of the kernel, and that orthogonal random features provide optimal (in terms of mean squared error) kernel estimators. We provide the first theoretical results which explain why orthogonal random features outperform unstructured on downstream tasks such as kernel ridge regression by showing that orthogonal random features provide kernel algorithms with better spectral properties than the previous state-of-the-art. Our results enable practitioners more generally to estimate the benefits from applying orthogonal transforms.}
}

@article{Hui2018KernelMB,
  title={Kernel Machines Beat Deep Neural Networks on Mask-based Single-channel Speech Enhancement},
  author={Like Hui and Siyuan Ma and Mikhail Belkin},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.02095},
  url={https://api.semanticscholar.org/CorpusID:53249357}
}

@article{GG_dist,
author = {Saralees Nadarajah},
title = {A generalized normal distribution},
journal = {Journal of Applied Statistics},
volume = {32},
number = {7},
pages = {685--694},
year = {2005},
publisher = {Taylor \& Francis},
doi = {10.1080/02664760500079464},
URL = {https://doi.org/10.1080/02664760500079464},
eprint = {https://doi.org/10.1080/02664760500079464}
}

@article{dytso2018analytical,
  title={Analytical properties of generalized Gaussian distributions},
  author={Dytso, Alex and Bustin, Ronit and Poor, H Vincent and Shamai, Shlomo},
  journal={Journal of Statistical Distributions and Applications},
  volume={5},
  pages={1--40},
  year={2018},
  publisher={Springer}
}

@article{Stable2,
 ISSN = {00206598, 14682354},
 URL = {http://www.jstor.org/stable/2525289},
 author = {Benoit Mandelbrot},
 journal = {International Economic Review},
 number = {2},
 pages = {79--106},
 publisher = {[Economics Department of the University of Pennsylvania, Wiley, Institute of Social and Economic Research, Osaka University]},
 title = {The Pareto-Lévy Law and the Distribution of Income},
 urldate = {2024-12-05},
 volume = {1},
 year = {1960}
}

@book{nolan2020univariate,
  title={Univariate Stable Distributions: Models for Heavy Tailed Data},
  author={Nolan, J.P.},
  isbn={9783030529154},
  book={Springer Series in Operations Research and Financial Engineering},
  year={2020},
  publisher={Springer International Publishing}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@article{nolan2005multivariate,
  title={Multivariate stable densities and distribution functions: general and elliptical case},
  author={Nolan, John},
  year={2005},
  journal={Deutsche Bundesbank's Annual Fall Conference},
  publisher={American University (Washington, DC)}
}

@book{samoradnitsky2017stable,
  title={Stable non-Gaussian random processes: stochastic models with infinite variance},
  author={Samoradnitsky, Gennady},
  year={2017},
  publisher={Routledge}
}

@article{weron1996chambers,
title = {On the Chambers-Mallows-Stuck method for simulating skewed stable random variables},
journal = {Statistics \& Probability Letters},
volume = {28},
number = {2},
pages = {165-171},
year = {1996},
issn = {0167-7152},
doi = {https://doi.org/10.1016/0167-7152(95)00113-1},
url = {https://www.sciencedirect.com/science/article/pii/0167715295001131},
author = {Rafał Weron},
keywords = {Stable distribution, Characteristic function, Random variable generation},
abstract = {In this note, we give a proof to the equality in law of a skewed stable variable and a nonlinear transformation of two independent uniform and exponential variables. The lack of an explicit proof of this formula has led to some inaccuracies in the literature. The Chambers et al. (1976) method of computer generation of a skewed stable random variable is based on this equality}
}

@article{chambers1976method,
author = {J. M. Chambers, C. L. Mallows and B. W. Stuck},
title = {A Method for Simulating Stable Random Variables},
journal = {Journal of the American Statistical Association},
volume = {71},
number = {354},
pages = {340--344},
year = {1976},
publisher = {ASA Website},
doi = {10.1080/01621459.1976.10480344},
URL = { https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480344},
eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1976.10480344}
}

@inproceedings{Fastfood,
author = {Le, Quoc and Sarl\'{o}s, Tam\'{a}s and Smola, Alex},
title = {Fastfood: approximating kernel expansions in loglinear time},
year = {2013},
publisher = {JMLR.org},
abstract = {Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that computing the decision function is typically expensive, especially at prediction time. In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matrices when combined with diagonal Gaussian matrices exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks (Rahimi \& Recht, 2007) and thereby speeding up the computation for a large range of kernel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) computation and storage, without sacrificing accuracy. We prove that the approximation is unbiased and has low variance. Extensive experiments show that we achieve similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster and using 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applications that have large training sets and/or require real-time prediction.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–244–III–252},
location = {Atlanta, GA, USA},
series = {ICML'13}
}


@InProceedings{P_model,
  title = 	 {Recycling Randomness with Structure for Sublinear time Kernel Expansions},
  author = 	 {Choromanski, Krzysztof and Sindhwani, Vikas},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2502--2510},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/choromanski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/choromanski16.html},
  abstract = 	 {We propose a scheme for recycling Gaussian random vectors into structured matrices to ap- proximate various kernel functions in sublin- ear time via random embeddings. Our frame- work includes the Fastfood construction of Le et al. (2013) as a special case, but also ex- tends to Circulant, Toeplitz and Hankel matri- ces, and the broader family of structured matri- ces that are characterized by the concept of low- displacement rank. We introduce notions of co- herence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of ran- dom feature maps that arise within our frame- work. For the case of low-displacement matri- ces, we show how the degree of structure and randomness can be controlled to reduce statis- tical variance at the cost of increased computa- tion and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scal- ing up kernel methods using random features.}
}
@inproceedings{scrf,
author = {Feng, Chang and Hu, Qinghua and Liao, Shizhong},
title = {Random feature mapping with signed circulant matrix projection},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Random feature mappings have been successfully used for approximating non-linear kernels to scale up kernel methods. Some work aims at speeding up the feature mappings, but brings increasing variance of the approximation. In this paper, we propose a novel random feature mapping method that uses a signed Circulant Random Matrix (CRM) instead of an unstructured random matrix to project input data. The signed CRM has linear space complexity as the whole signed CRM can be recovered from one column of the CRM, and ensures loglinear time complexity to compute the feature mapping using the Fast Fourier Transform (FFT). Theoretically, we prove that approximating Gaussian kernel using our mapping method is unbiased and does not increase the variance. Experimentally, we demonstrate that our proposed mapping method is time and space efficient while retaining similar accuracies with state-of-the-art random feature mapping methods. Our proposed random feature mapping method can be implemented easily and make kernel methods scalable and practical for large scale training and predicting problems.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3490–3496},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{Bojarski2016StructuredAA,
  title={Structured adaptive and random spinners for fast machine learning computations},
  author={Mariusz Bojarski and Anna Choromańska and Krzysztof Choromanski and Francois Fagan and C{\'e}dric Gouy-Pailler and Anne Morvan and Nourhan Sakr and Tam{\'a}s Sarl{\'o}s and Jamal Atif},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:7434091}
}

@inproceedings{ROM_,
author = {Choromanski, Krzysztof and Rowland, Mark and Weller, Adrian},
title = {The unreasonable effectiveness of structured random orthogonal embeddings},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel, we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits, and empirical results which suggest that the approach is helpful in a wider range of applications.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {218–227},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{Gauss_quad,
author = {Dao, Tri and Sa, Christopher De and R\'{e}, Christopher},
title = {Gaussian quadrature for kernel features},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that O(ε-2) samples are required to achieve an approximation error of at most ε. We investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any γ > 0, to achieve error ε with O(eeγ + ε-1/γ) samples as ε goes to 0. Our method works particularly well with sparse ANOVA kernels, which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6109–6119},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@inproceedings{Quad_based_feats,
author = {Munkhoeva, Marina and Kapushev, Yermek and Burnaev, Evgeny and Oseledets, Ivan},
title = {Quadrature-based features for kernel approximation},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. Based on an efficient numerical integration technique, we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation. We derive the convergence behaviour and conduct an extensive empirical study that supports our hypothesis.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {9165–9174},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{Bach_equivalence,
author = {Bach, Francis},
title = {On the equivalence between kernel quadrature rules and random feature expansions},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L2- and L∞-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {714–751},
numpages = {38},
keywords = {integral operators, positive-definite kernels, quadrature}
}

@InProceedings{pmlr-v22-kar12,
  title = 	 {Random Feature Maps for Dot Product Kernels},
  author = 	 {Kar, Purushottam and Karnick, Harish},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {583--591},
  year = 	 {2012},
  editor = 	 {Lawrence, Neil D. and Girolami, Mark},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/kar12/kar12.pdf},
  url = 	 {https://proceedings.mlr.press/v22/kar12.html},
  abstract = 	 {Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classifiers and other kernel based learning algorithms. We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces. We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence.}
}
@article{wacker2024improved,
  title={Improved Random Features for Dot Product Kernels},
  author={Wacker, Jonas and Kanagawa, Motonobu and Filippone, Maurizio},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={235},
  pages={1--75},
  year={2024}
}

@article{beaglehole2024average,
  title={Average gradient outer product as a mechanism for deep neural collapse},
  author={Beaglehole, Daniel and S{\'u}ken{\'\i}k, Peter and Mondelli, Marco and Belkin, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{simon2024more,
  title={More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory},
  author={Simon, James B and Karkada, Dhruva and Ghosh, Nikhil and Belkin, Mikhail},
  year={2024},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{ghosh2023universal,
  title={A universal trade-off between the model size, test loss, and training loss of linear predictors},
  author={Ghosh, Nikhil and Belkin, Mikhail},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={4},
  pages={977--1004},
  year={2023},
  publisher={SIAM}
}

@article{beaglehole2023inconsistency,
  title={On the inconsistency of kernel ridgeless regression in fixed dimensions},
  author={Beaglehole, Daniel and Belkin, Mikhail and Pandit, Parthe},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={4},
  pages={854--872},
  year={2023},
  publisher={SIAM}
}

@article{mallinar2024emergence,
  title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
  author={Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2407.20199},
  year={2024}
}

@article{mallinar2022benign,
  title={Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting},
  author={Mallinar, Neil and Simon, James and Abedsoltan, Amirhesam and Pandit, Parthe and Belkin, Misha and Nakkiran, Preetum},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1182--1195},
  year={2022}
}

@inproceedings{abedsoltan2023toward,
  title={Toward large kernel models},
  author={Abedsoltan, Amirhesam and Belkin, Mikhail and Pandit, Parthe},
  booktitle={International Conference on Machine Learning},
  pages={61--78},
  year={2023},
  organization={PMLR}
}

@article{geifman2020similarity,
  title={On the similarity between the laplace and neural tangent kernels},
  author={Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1451--1461},
  year={2020}
}

@article{chen2020deep,
  title={Deep neural tangent kernel and laplace kernel have the same rkhs},
  author={Chen, Lin and Xu, Sheng},
  journal={International Conference on Representation Learning},
  year={2020}
}

@inproceedings{rakhlin2019consistency,
  title={Consistency of interpolation with Laplace kernels is a high-dimensional phenomenon},
  author={Rakhlin, Alexander and Zhai, Xiyu},
  booktitle={Conference on Learning Theory},
  pages={2595--2623},
  year={2019},
  organization={PMLR}
}

@article{abedsoltan2024fast,
  title={Fast training of large kernel models with delayed projections},
  author={Abedsoltan, Amirhesam and Ma, Siyuan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2411.16658},
  year={2024}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{muyskens2024correspondence,
  title={Correspondence of NNGP Kernel and the Mat{\'e}rn Kernel},
  author={Muyskens, Amanda and Priest, Benjamin W and Goumiri, Imene R and Schneider, Michael D},
  journal={arXiv preprint arXiv:2410.08311},
  year={2024}
}

@article{karoui2013asymptotic,
  title={Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators: rigorous results},
  author={Karoui, Noureddine El},
  journal={arXiv preprint arXiv:1311.2445},
  year={2013}
}

@article{sutradhar1986characteristic,
  title={On the characteristic function of multivariate Student t-distribution},
  author={Sutradhar, Brajendra C},
  journal={The Canadian Journal of Statistics/La Revue Canadienne de Statistique},
  pages={329--337},
  year={1986},
  publisher={JSTOR}
}
@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}
@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Annals of Statistics},
  year={2021}
}

@article{bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}

@article{liu2021random,
  title={Random features for kernel approximation: A survey on algorithms, theory, and beyond},
  author={Liu, Fanghui and Huang, Xiaolin and Chen, Yudong and Suykens, Johan AK},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={10},
  pages={7128--7148},
  year={2021},
  publisher={IEEE}
}

@book{abramowitz1948handbook,
  title={Handbook of mathematical functions with formulas, graphs, and mathematical tables},
  author={Abramowitz, Milton and Stegun, Irene A},
  volume={55},
  year={1948},
  publisher={US Government printing office}
}

@article{ma2019kernel,
  title={Kernel machines that adapt to GPUs for effective large batch training},
  author={Ma, Siyuan and Belkin, Mikhail},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={360--373},
  year={2019}
}

@ARTICLE{IEEE_754,
  author={},
  journal={IEEE Std 754-2019 (Revision of IEEE 754-2008)}, 
  title={IEEE Standard for Floating-Point Arithmetic}, 
  year={2019},
  volume={},
  number={},
  pages={1-84},
  keywords={IEEE Standards;Floating-point arithmetic;arithmetic;binary;computer;decimal;exponent;floating-point;format;IEEE 754;interchange;NaN;number;rounding;significand;subnormal.},
  doi={10.1109/IEEESTD.2019.8766229}}
