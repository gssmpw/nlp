\begin{table*}[t]
\caption{Statistics for self-declarations, with bots and non-coherent users removed. The subreddits are only those filtered by the regular expression on the user's attribute label.}
\centering
\resizebox{.85\textwidth}{!}{\input{tables/dataset_stats.tex}}
\label{table:stats}
\end{table*}

This section describes first the data sources we use and then the methods we study in our systematic comparison.

\subsection{Reddit data}
\spara{Sociodemographic proxies.}
As described in the previous section, we identify subreddit participation as the primary source of information for our goals.
To ensure robustness and generalizability, we use the \num{10000} most popular subreddits based on comment activity from 2016 to 2020 as the feature space.
This selection criterion is the same one used by~\citet{waller2021quantifying} to create their embeddings.
It strikes a balance between feature richness and computational feasibility but introduces challenges such as multicollinearity, which can affect model stability.

\spara{Ground truth.}
As a reliable ground truth for sociodemographic attributes, we adopt users' self-declaration.
In fact, as discussed in the previous section, it respects our criteria of authenticity while at the same time being practical to obtain.
To identify the sociodemographic attributes, we define specific regular expressions (regexes).
For partisan affiliation, we use patterns such as ``I'm a democrat/republican'' and ``I am a registered democrat/republican,'' and exclude expressions containing negations.
\Cref{fig:regex-pol} provides an overview of these regex patterns.
For gender and age, we use patterns indicating them (e.g., ``F27'' or ``20M'') when adjacent to first person pronouns (e.g., ``I'', ``my''), following previous work~\citep{de2022social}.

Using these regexes, we label users based on their comments and submissions for age, gender, and partisan affiliation over the period from 2016 to 2020.
We then convert the age attribute into the birth year by taking into account the time of disclosure.
While the age of a person changes with time, the year of birth is invariant, and thus is a more suitable target for classification that uses data over time.
Bots were removed using an external list of identified bots.
Subsequently, we retrieved all comments and submissions from the selected \num{10000} subreddits for all labeled users.
To ensure data reliability, we removed users with inconsistent declarations during the selected time period, which affected approximately $4\%$ of users.

\Cref{table:stats} presents a quantitative description of the dataset.
It includes approximately \num{400000} declarations each for gender and year of birth, while the number of declarations for partisan affiliation is two orders of magnitude smaller.
The classes are roughly balanced, and the number of subreddits with non-zero activity for the labeled users is slightly smaller than the original \num{10000} subreddits selected.
The distributions of sociodemographic disclosures are illustrated in \Cref{fig:regex1,fig:regex3} in the Appendix.

\subsection{Models}
The fundamental task of sociodemographic classification can be described as follows.
We are given a dataset \(\mathcal{D} = \{(\vec{x}^{(i)}, y^{(i)}) \mid i = 1, \dots, n\}\), representing an observed sequence of subreddit activations \(\vec{x}^{(i)} \in \mathbb{N}^d\) across a set of \(d\) subreddits.
A model has to predict a class label \(y^{(i)} \in \{0, 1\}\) for each attribute of user \(i\), based on their participation data.
As previously described, such a model should provide interpretable predictions and reliable uncertainty estimation, while maintaining high classification performance.

We evaluate a range of models that reflect common practices in the field, including Bayesian models, embedding-based models~\citep{waller2021quantifying}, decision tree-based models, semi-supervised approaches, and activity-modeling techniques.



\spara{WA Model \citep{waller2021quantifying}.}
Each subreddit has a pre-trained embedding represented in a unified, high-dimensional vector space.
For each attribute (age, gender, affluence, partisanship), a one-dimensional axis is defined by taking ``extreme'' subreddits as poles for the axis (called ``seeds'' in the original work).
Other subreddits can then be projected onto the one-dimensional axis for each attribute.
We assign a z-score to each subreddit for each dimension.
Users can also be projected if represented as a weighted combination of the subreddits where they participated.
A user's z-score for an attribute is the weighted average of the subreddit z-scores, weighted by the number of comments the user posted in each subreddit.
The resulting z-score is then used to predict the user's class for the attribute.

\spara{Random Forest.}
Random Forest is an ensemble learning method for classification based on decision trees.
It is widely used for tabular data due to its flexibility, strong performance, and resistance to multicollinearity.

\spara{Bayesian Modeling.}
Multinomial Naive Bayes (NB) is a standard Bayesian model that assumes feature independence and provides transparent, interpretable predictions.
It is fast to train, with closed-form estimators for its parameters.
We extend the semi-supervised Multinomial Naive Bayes model introduced by \citet{nigam2000text} and adapt it to our task with some promising extensions to Naive Bayes.

\emph{Semi-Supervised Naive Bayes} (\texttt{SS NB}):
incorporates the Expectation-Maximization (EM) algorithm to enable semi-supervised learning, allowing the model to leverage both labeled and unlabeled data~\cite{sristy2012semi}.

\emph{Log-Normal Naive Bayes} (\texttt{logN NB}):
models a class-dependent user activity by assuming a log-normal distribution.
Activity is measured as the $L_1$-norm of the feature vector.
This approach allows for variations in user engagement.

\emph{Semi-Supervised Log-Normal Naive Bayes} (\texttt{SS logN NB}): combines the semi-supervised EM approach with the log-normal activity model, thereby enhancing performance in scenarios with limited labeled data.

Mathematical details of each of these models are provided in the Appendix \emph{``Modeling and Learning.''}
