
A critical issue in social media studies is understanding the connection between individuals’ online and offline behaviors.
This connection is essential for examining phenomena such as the interplay between online information diets and voting behavior~\citep{bach2021predicting}, or the impact of algorithmic curation on vaccine hesitancy~\citep{sasse2024understanding}.
Sociodemographic attributes provide a key bridge between these two realms.

Individuals of similar demographic groups are likely to share significant parts of their offline environments, such as the places they visit~\citep{fan2023diversity}.
Furthermore, sociodemographic factors are primary determinants of one's experience of reality~\citep{lee2014newsworthy} and opinion formation~\citep{hamilton2011all, capozzi2021clandestino}.
As such a plethora of studies has used these attributes to characterize social media users and how their online behavior connects to their identity~\citep{rivas2020classification,gjurkovic2021pandora,tadesse2019detection}.
Finally, post-stratification techniques on such features might be necessary to mitigate the selection bias to connect the results observed in online cohorts to the general population~\citep{giorgi2022correcting}.

In this context, inferring sociodemographic attributes is a fundamental task in computational social science (CSS). 
Researchers face practical requirements for methods used in these studies, such as \emph{reliability}, \emph{coverage}, \emph{interpretability}, and \emph{scalability}. 
However, this task is inherently challenging due to the lack of ground truth data and the diversity of information available across different social media platforms.
To address these challenges effectively, it is crucial to tailor solutions to specific platforms and data sources.
For instance, popular platforms such as Twitter have been the focus of numerous studies aimed at comparing methods for sociodemographic inference~\citep{chen2015comparative, sadah2016demographic}.
Yet, this task has not drawn the same attention for Reddit.

Self-described as “the front page of the Internet,” Reddit is a top-ten global website and a hub for social news and discussions. 
Organized into thematic fora called subreddits \citep{baumgartner2020pushshift}, it provides a rich source for studying online behavior, where participation often reflects offline sociodemographic characteristics \citep{medvedev2019anatomy}.
Over the years, Reddit has become a prominent platform for academic research.
It has been used as a digital observatory for public health-related issues~\citep{balsamo2019firsthand, lokala2022computational, balsamo2023pursuit}, a testbed for structured public debate~\citep{cinelli2021echo,monti2022language,colacrai2024navigating} and studied as a breeding ground for far-right ideologies and conspiracy theories~\citep{klein2019pathways,rollo2022communities}.
While many studies have incorporated sociodemographic information to explore user behavior and social phenomena~\citep{monti2023evidence}, there remains a lack of systematic evaluation of methodologies for inferring such information from user activity.

One of the most widely-used methods has been devised by~\citet{waller2021quantifying}.
Their method is unsupervised and constructs an embedding of subreddits in a latent space based on co-participation.
Then it finds a projection of demographic attributes by using some seed pairs as reference subreddits.
While this approach provides broad coverage by leveraging the \num{10000} most common subreddits as the feature space, it has several notable limitations.
As a fully unsupervised method, it lacks systematic evaluation to assess its reliability.
Moreover, the approach relies on arbitrarily defined poles for each attribute, which have not been rigorously tested or validated.
The use of a neural embedding-based model (Word2Vec) introduces additional challenges, as it renders the model a black box with components that are not directly interpretable.
This lack of interpretability limits its suitability for applications where transparency and explainability are critical.




\smallskip
The goal of this work is to systematically compare the method proposed by~\citet{waller2021quantifying} with alternative approaches specifically designed for this task. These alternatives are structurally different, rethinking data collection strategies, modeling techniques, and prediction tasks.
We explore supervised learning as a foundation, deriving sociodemographic proxies from Reddit user activity logs along with their self-declared sociodemographic information in comments—specifically for \emph{gender}, \emph{age}, and \emph{partisan affiliation}. 
Using self-declared attributes addresses the scalability challenges of data labeling.
To enhance interpretability, we focus on models based on a Naive Bayes framework, which assume a straightforward probabilistic relationship between subreddit participation and demographic attributes. 
While primarily supervised, we also explore semi-supervised extensions that incorporate unstructured data from the co-participation network to improve robustness. 
Additionally, we include decision tree-based models as a benchmark, given their prominence in handling tabular data~\citep{grinsztajn2022why}. %
To evaluate these methods and identify the strengths and limitations of major strategies in sociodemographic inference, we use self-declarations as the ground truth across two distinct tasks:
($i$) user classification, a widely used approach in many Reddit studies;
and ($ii$)~quantification, which estimates the proportion of users with a given sociodemographic attribute. 
Quantification often serves as a privacy-conscious alternative to classification in downstream tasks.

Our key finding is that a simple and interpretable Naive Bayes (NB) model is a reliable and scalable approach for classifying and quantifying sociodemographics on Reddit.
Through extensive experiments, we demonstrate that Multinomial Naive Bayes (MNB) outperforms the embedding-based WA model~\citep{waller2021quantifying} when using supervised labeling, achieving a ROC AUC above $0.7$, and thus increasing the performances on the classification tasks of age, gender and partisan affiliation of $17\%$, $19\%$, and $6\%$, respectively. %
Surprisingly, even in the distant supervision setting---where training labels are derived from the same assumptions made by~\citet{waller2021quantifying}---MNB proves to be the best-performing model.

Empirically, supervised models require a minimum of \num{1000} labeled data points to perform reliably. 
The benefit of incorporating additional unsupervised co-participation data is limited.
Increasing model complexity, e.g., using decision trees or introducing additional terms in the likelihood, does not significantly improve performance.

We also address the problem of quantification, introducing sociodemographic quantifiers that achieve low mean absolute error (MAE), with errors below 15\% in large-scale data settings and below 18\% in sparse data scenarios. 
These quantifiers effectively estimate the proportion of users in each sociodemographic class.

Finally, we showcase the properties of our classifiers and quantifiers, highlighting their interpretability through calibration and feature importance analysis.











