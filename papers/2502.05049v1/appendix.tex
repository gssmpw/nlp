\subsection{Bayesian Modeling}
\label{sec:app_bayes}
We extend the semi-supervised Multinomial Naive Bayes (MNB) model introduced by~\citet{nigam2000text} by incorporating the conditional probability of subreddit activations given the class label.
First, we review the extended model, followed by a description of the Expectation-Maximization (EM) algorithm used for semi-supervised learning in Naive Bayes as outlined by~\citet{nigam2000text}.

\vspace{2mm}

Each data sample in the dataset $\mathcal{D}=\{(\vec{x}^{(i)}, y^{(i)}) \mid i=1,\ldots,n\}$ consists of an observed sequence of subreddit activations  $\Vec{x}^{(i)} \in \mathbb{N}^d$, where $d$ is the number of subreddits, and a class label $y^{(i)}\in \{0, \ldots,  k-1\}$ for user $i$.

In our case study, $k = 2$ for each attribute (\textit{Year of Birth}, \textit{Gender}, and \textit{Partisan Affiliation}), but we present the model in its general form.
Missing labels are denoted as $y^{(i)}=-1$.
The observed userâ€™s total activity is given by $a^{(i)}=\lvert\vec{x}^{(i)}\rvert_1$.

\vspace{2mm}

In a naive sense, each user's activation over the set of $d$ subreddits is assumed to be independent.
In addition, the probabilities of activity $a$ and activations $\Vec{x}$ we assume to be conditionally independent given the class $y$.
Hence, we define the joint likelihood of the unsupervised as follows, by setting marginalizing out the labels
\begin{align*}
     \mathcal{L}(\vec{\Theta}) = \prod_{i=1}^{n} p(\vec{x}^{(i)})\\
     =  \prod_{i=1}^{n}  \sum_{y=0}^{k-1} p(\vec{x}^{(i)}, y) \\
     =  \prod_{i=1}^{n}  \sum_{y=0}^{k-1}\left( p(y) p(a^{(i)} \mid y) \prod_{j=0}^{d-1} p(x_j^{(i)} \mid y)\right)
\end{align*}
where the 3 groups of learnable parameters, collectively denoted with $\vec{\Theta}$, are: 
\begin{enumerate}
    \item $p(y)$:
    \begin{itemize}
        \item prior probability to observe class $y$;
        \item there is one for each of the $k$ classes;
        \item it satisfies $\sum_{y=0}^{k-1} p(y)=1$.
    \end{itemize}
    \item $p(a \mid y)$:
    \begin{itemize}
        \item probability to observe an activity $a$ given the class $y$:
              \begin{align*}
                  p(a \mid y)=\Phi(a+1;\mu_y, \sigma_y)-\Phi(a;\mu_y, \sigma_y) \\
                  \mathrm{where} \: \: \Phi:= \mathrm{CDF(lognormal(\mu, \sigma))}
              \end{align*}
        \item $(\mu_y, \sigma_y)$ are the learnable parameters;
        \item there is one tuple for each of the $k$ classes.
    \end{itemize}
    \item $p(j \mid y)$: 
    \begin{itemize}
        \item probability to observe an activation over subreddit $j$ given the class $y$;
        \item the probability to observe $x$ independent activations in the subreddit $j$ given the class $y$ is \[
        p(x_j \mid y)=p_j(j \mid y)^{x_j}
        \] 
        \item there is one for each class ($k$) and each subreddit ($d$)
        \item it satisfies $\sum_{j=0}^{d-1} p(j \mid y)=1$.
    \end{itemize}
\end{enumerate}

\newpage
\spara{Models.}  
This formulation leads to three main models considered throughout the paper:  

\begin{enumerate}
    \item Standard Multinomial Naive Bayes (\texttt{NB}):  
    This model is trained using closed-form counting formulas to estimate the probabilities.

    \item Log-Normal Naive Bayes (\texttt{LogN NB}):
    In this model, the parameters of the log-normal (mean and standard deviation) are fitted to the data to model \(p(a \mid y)\).
    
    \item Semi-Supervised Naive Bayes (\texttt{SS NB}):
    This model uses the Expectation-Maximization algorithm to incorporate both labeled and unlabeled data during training.
    
    \item Semi-Supervised Log-Normal Naive Bayes (\texttt{SS LogN NB}):
    This hybrid model combines the semi-supervised approach with the log-normal distribution, fitting the activity (\(p(a \mid y)\)) while leveraging the EM algorithm.
\end{enumerate}
Notice that both \texttt{NB} and \texttt{LogN NB} are trained in a fully supervised manner, assuming all class labels (\(y\)) are given.  


\subsection{Semi-Supervised Learning}  
In semi-supervised learning, the presence of unlabeled data introduces latent variables, making the problem more complex.
To address this issue, parameter estimation relies on the Expectation-Maximization (EM) algorithm, which iteratively refines the parameters by alternating between the following two steps:

\spara{E-step.} We define the conditional class probabilities given the current parameter $\vec{\Theta}^{t}$. For each sample $i$, and each label $y$ the probability is
\begin{align*}
    p(y \mid \vec{x}^{(i)};\vec{\Theta}^{t})= 
    \frac{p^{t}(y) p^{t}(a^{(i)} \mid y) \prod_{j=0}^{d-1} p^{t}_j(x_j^{(i)} \mid y)}{\sum_{y=0}^{k-1} p^{t}(y) p^{t}(a^{(i)} \mid y) \prod_{j=0}^{d-1} p^{t}_j(x_j^{(i)} \mid y)}
\end{align*} where $\sum_y p(y \mid \vec{x}^{(i)};\vec{\Theta}^{t}) = 1$. In the supervised setting, when $y^{(i)}$ is not missing ($y^{(i)}\neq -1$), there is only one term with probability one: $ p(y \mid \vec{x}^{(i)};\vec{\Theta}^{t})=1$ if $y=y^{(i)}, 0$ otherwise.

\spara{M-step.} By MLE, the three sets of parameters are updated in the following way.
\begin{enumerate}
    \item Class probability: $\forall y \in \{0, \ldots , k-1 \}$ 
    \[p^{t+1}(y)=\frac{\alpha_1+\sum_{i=1}^n p(y \mid \vec{x}^{(i)};\vec{\Theta}^{t})}{\alpha_1 k+ n}\]
    where $\alpha_1$ is the regularization term;
    \item conditional activity: $\forall y \in \{0, \ldots , k-1 \}$
    \begin{align*}
         \mu_y=\frac{1}{n}\sum_{i=1}^{n}\log{a^{(i)}} \\
         \sigma_y=\sqrt{\frac{1}{n}\sum_{i=1}^{n} \left( \log{a^{(i)}}- \mu_y \right)^2} 
    \end{align*}
    \item conditional activations: $\forall y \in \{0, \ldots , k-1 \} $ and $\forall j \in \{1, \ldots , d \} $
    \[p^{t+1}(j \mid y)=\frac{\alpha_2+\sum_{i=1}^n x_j^{(i)}p(y \mid \vec{x}^{(i)};\vec{\Theta}^{t})}{\alpha_2 d+ \sum_{j=0}^{d-1} \sum_{i=1}^n x_j^{(i)} p(y \mid \vec{x}^{(i)};\vec{\Theta}^{t})}\]
     where $\alpha_2$ is the regularization term.
\end{enumerate}



\section{Dataset}
The ground truth for socio-demographic attributes is based on the \num{10000} most active subreddits (2016--2020).
The dataset includes approximately \num{400000} declarations each for Gender and Year of Birth, while Partisan Affiliation declarations are two orders of magnitude smaller.

Disclosure patterns vary across subreddits, as shown in \Cref{fig:regex1,fig:regex2}.
For Year of Birth, subreddits such as \texttt{relationships} are highly discriminative, while gaming-related subreddits such as \texttt{gaming} are more prominent for Gender.
In contrast, Partisan Affiliation (\Cref{fig:regex3}) is characterized by subreddits such as \texttt{politics} and \texttt{news}, which reflects the political focus of those communities.

\Cref{fig:class_distr_true} shows the distribution of user-declared labels across the three attributes (Year, Gender, and Partisan), divided into an 80/20 train-test split for the classification task. 
The number of declarations for the Partisan attribute is significantly lower--one order of magnitude smaller--compared to the Year and Gender attributes.


\begin{figure}
\includegraphics[width=.94\columnwidth, trim=0mm 6mm 0mm 7mm, clip]{classification_data_distribution_True_supervision.pdf}
\caption{(True Supervision) Distribution of the user-declared labels on the three attributes, divided into train and test set (80/20) for the classification task. Classes 0: Old, Male, Democrat. Classes 1: Young, Female, Republican.}
\label{fig:class_distr_true}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{\columnwidth}
        \centering
        \includegraphics[width=.94\columnwidth]{activity_distribution_year.pdf}
        \caption{Subreddit names for year of birth disclosure.}
        \label{fig:regex1}
    \end{subfigure}

    \begin{subfigure}[t]{\columnwidth}
        \centering
        \includegraphics[width=.94\columnwidth]{activity_distribution_gender.pdf}
        \caption{Subreddit names for gender disclosure.}
        \label{fig:regex2}
    \end{subfigure}

    \begin{subfigure}[t]{\columnwidth}
        \centering
        \includegraphics[width=.94\columnwidth]{activity_distribution_demo_rep.pdf}
        \caption{Subreddit names for partisan disclosure.}
        \label{fig:regex3}
    \end{subfigure}

    \caption{Subreddit titles for socio-demographic disclosure: year of birth, gender, and partisan affiliation.}
    \label{fig:activity_distribution_combined}
\end{figure}



\begin{figure*}[h!]
\begin{subfigure}[t]{0.67\columnwidth}
\includegraphics[width=\textwidth]{log-odds-year-PERCENTILE990-top25.pdf}
\caption{}
\label{fig:importance-year}
\end{subfigure}
\begin{subfigure}[t]{0.67\columnwidth}
\includegraphics[width=\textwidth]{log-odds-gender-PERCENTILE990-top25.pdf}
\caption{}
\label{fig:importance-gender}
\end{subfigure}
\begin{subfigure}[t]{0.67\columnwidth}
\includegraphics[width=\textwidth]{log-odds-partisan-PERCENTILE990-top25.pdf}
\caption{}
\label{fig:importance-partisan}
\end{subfigure}
\caption{Top 25 most important subreddits for classification
(among the top 1\% most active subreddits).}
\end{figure*}






\begin{figure*}[b]
\includegraphics[width=.94\textwidth]{Matches_per_year_opinion.pdf}
\caption{Distribution of matched regular expressions for partisan affiliation per year.}
\label{fig:regex-pol}
\end{figure*}
