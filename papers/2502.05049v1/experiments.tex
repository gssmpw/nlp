\begin{figure*}[ht!]
\includegraphics[width=.99\textwidth]{ROC-all-dimensions-supervision.pdf}
\centering
\caption{ROC curves for each attribute (Year of Birth, Gender, Partisan Affiliation) and model (Naive Bayes (NB), \citeauthor{waller2021quantifying} (WA)). Models are trained with \textbf{true supervision}, employing random oversampling to address class imbalances, and evaluated using 10-fold stratified cross-validation.}
\label{fig:classif_true}
\end{figure*}


This section outlines the experimental setup, including details about the model benchmarks, training settings, and prediction tasks.
We then present the results for both the classification and quantification tasks, along with an analysis of the properties of the models.

Our study is guided by this overarching research goal: \emph{``How to best infer sociodemographics on Reddit?''}
We articulate this goal into three research questions.
\begin{squishlist}
\item \emph{RQ 1: Classification.}
Which model achieves the best performance in classifying sociodemographic attributes?
\item \emph{RQ2: Quantification.}
Which model achieves the best performance in estimating the prevalence of a demographic class in a set of users?
\item \emph{RQ3: Transparency.}
How do desirable transparency properties of the models impact their usage?
\end{squishlist}

\noindent


\begin{figure*}[h!]
\centering
\includegraphics[width=.99\textwidth]{ROC-all-dimensions-distant-supervision.pdf}
\caption{ROC curves for each attribute (Year of Birth, Gender, Partisan Affiliation) and model (Naive Bayes (NB), \citeauthor{waller2021quantifying} (WA)). Models are trained with \textbf{distant supervision}, employing random oversampling to address class imbalances, and evaluated using 10-fold stratified cross-validation.}
\label{fig:classif_dist}
\end{figure*}


\subsection{Experimental Setup}
We consider two main quantitative prediction tasks: classification and quantification of binary sociodemographic attributes:
\emph{(i) Year of Birth}: whether a user's year of birth is lower than the median of the user base;
\emph{(ii) Gender}: gender, considering only binary categories (Male and Female);
\emph{(iii) Partisan Affiliation}: party alignment with the U.S. two main parties (Democrat and Republican).

In contrast to~\citet{waller2021quantifying}, we mapped the self-declared age to the year of birth for each user.
This transformation ensures that the attribute remains static, and addresses the evolving nature of age over a time span longer than one year, as in our case study.


\label{sec:setup}
\paragraph{Benchmark and Parameters.}
We evaluate the following classifiers:
\emph{(i)} \texttt{Majority Model} (predicts the most frequent class in the dataset. This serves as a reference point for the minimum acceptable performance); 
\emph{(ii)} \texttt{WA Model} \cite{waller2021quantifying}; 
\emph{(iii)} \texttt{Random Forest (RF)} (with 50 estimators and max depth 10);
\emph{(iv)} \texttt{Multinomial Naive Bayes (NB)} (with additive smoothing parameter $\alpha=1$).

For quantification tasks, we apply the following quantification \emph{wrappers} on each classifier above:
\begin{squishlist}
    \item \texttt{Classify \& Count (CC)}: Classifies each data point and returns the raw cardinality of each predicted class. This serves as a simple baseline.
    \item \texttt{Adjusted Classify \& Count (ACC)}: Adjusts the raw class proportions by fitting conditional distributions to correct potential biases in the CC method. This implementation follows the method proposed by \citet{moreo2021quapy}.
\end{squishlist} 


\paragraph{Extensions to Naive Bayes for Quantification.}
To evaluate common practices to enhance the quantification capabilities of the Naive Bayes model, we evaluate the following advanced variants:
\emph{(i)} \texttt{Semi-Supervised Naive Bayes (SS NB)};
\emph{(ii)} \texttt{Log-Normal Naive Bayes (logN NB)}, which models user activity as class-dependent);
\emph{(iii)} \texttt{Semi-Supervised Log-Normal Naive Bayes (SS logN NB)}, a hybrid approach.


\spara{Training Settings.}
While the method by \citet{waller2021quantifying} is unsupervised, the others require a training phase.
We compare two distinct training data settings.

\emph{True Supervision:} We use labels derived directly from explicit user self-declarations on Reddit.
These labels provide a reliable ground truth based on self-declared sociodemographic attributes and serve as a benchmark for evaluating model performance under ideal conditions.

\emph{Distant Supervision:}
We adopt a procedure inspired by~\citet{waller2021quantifying}:
training labels are based on their participation in discriminative subreddits associated with each class of each attribute.
For each attribute (\emph{year}, \emph{gender}, \emph{partisan}), we define two sets of five subreddits—one set corresponding to each label (e.g., \emph{young/old}, \emph{male/female}, \emph{democrat/republican}).
A user is labeled according to a specific class if their participation in one set of subreddits exceeds their participation in the opposite set for that attribute.
To ensure robust and unambiguous labeling, a user is included in the training set only if their participation difference between the two sets exceeds a threshold of three interactions.
This threshold minimizes noise and reduces the likelihood of mislabeling due to ambiguity.

The key distinction between the two data settings lies in their respective strengths and limitations.
Self-declared labels act as direct proxies for users’ sociodemographics, offering high-quality and explicit information.
However, these labels are often associated with a narrower range of subreddit activity, which limits the diversity of user behaviors represented in the training data, as shown in~\Cref{fig:regex1,fig:regex3}.
Conversely, distant supervision broadens the scope of user activity by covering a wider range of users whose activity is orthogonal to the choice to be labeled.
This expands the dataset and captures more heterogeneous patterns of activity.
Nevertheless, distant supervision relies heavily on assumptions about the associations between subreddits and sociodemographic attributes, which may introduce noise and reduce labeling accuracy.

The class distribution for the classification task is depicted in~\Cref{fig:class_distr_true} in the Appendix.
To address class imbalances, we apply a Random Over Sampler, which ensures a balanced representation of both classes during training.
Furthermore, for training with distant supervision, we apply distant labeling to the entire set of users, regardless of the disclosed attributes, thereby enlarging the user base.


\spara{Evaluation Metrics.}
All metrics are computed using self-declarations only (also in the distant-supervision case).
For classification, performance is measured via average ROC AUC and F1-score, calculated over $100$ bootstrapped test samples, each comprising $20\%$ of the data.
For quantification, we use the Mean Absolute Error (MAE).
We divide the dataset into $70\%$ for training the models and $30\%$ held out as unseen data for building test sets, using $50$ different random seeds.
For each seed, we apply the Natural Prevalence Protocol (NPP)~\cite{moreo2021quapy} to generate $50$ test sets, each with class prevalences approximating the natural prevalence of the two classes for each attribute.
We compute Absolute Errors for each test set across all repetitions and seeds, then average across all seeds and repetitions to obtain the MAE.

\begin{table}[t]
\caption{Results for the classification task with true supervision (mean $\pm$ std dev).}
\centering
\resizebox{\columnwidth}{!}{\input{tables/classification_True_supervision_FORMATTED}}
\label{table:classif}
\end{table}

\subsection{Results: Classification Task}

\spara{Simpler models outperform embedding-based models with supervised labeling.}
\Cref{fig:classif_true} shows that Naive Bayes-based models (NB) consistently outperform embedding-based models (WA) by a substantial margin in attributes when a sufficient amount of data is available.
This performance gap is particularly evident in the gender task, which benefits from the largest dataset size (424k declarations).
Naive Bayes achieves an average AUC of $0.80$ compared to $0.67$ for the WA model.

The datasets for gender and year are notably larger than those for the partisan attribute, with almost two orders of magnitude more declarations (\Cref{table:stats}), contributing to the performance of supervised methods in these cases.

\Cref{table:classif} shows details for models trained with \emph{true supervision}.
Naive Bayes consistently outperforms the Majority baseline by a large margin across all attributes and metrics.
Compared to the WA model, NB achieves significant improvements in all metrics, with margins ranging from $0.4$ to $1.8$, depending on the attribute.
Random Forest (RF), despite its higher computational cost compared to NB, demonstrates a competitive advantage only in the \textit{partisan} prediction task, where data scarcity dominates.
In this setting, RF slightly outperforms NB in ROC AUC, although the difference is within the error bars of the two classifiers.


\spara{Simpler Models outperform embedding-based models with distant supervision.}
\Cref{fig:classif_dist} demonstrates that Naive Bayes models (NB) outperform embedding-based methods (WA) when trained with labels derived solely from distant supervision.
These labels are generated based on participation in discriminative subreddits, as defined by \citet{waller2021quantifying}.
This result underscores the capability of Naive Bayes models to effectively capture sociodemographic patterns similar to those assumed in the embedding-based approach while retaining simplicity and interpretability.
Thus, the simpler approach is preferable even when self-declarations are not available.
However, Naive Bayes models trained under distant supervision show lower overall performance, higher variance, and narrower margins than their true-supervised counterparts and the WA model.
This reduced performance highlights the inherent limitations of distant supervision, which relies on noisier and less direct labels derived from coarse assumptions about user participation patterns.

\spara{True supervision labels are better than distant supervision.}
Training on declared labels yields better performance compared to distant supervision, as evidenced by the degraded scores observed from~\Cref{fig:classif_true} (true supervision) to~\Cref{fig:classif_dist} (distant supervision).
This performance gap arises from several factors.
First, declared labels are direct evidence for the target attributes, and thus higher-quality training data.
Training and testing on the same type of data inherently offers an advantage due to consistency in data distributions.
Second, the dataset of declared labels is approximately two orders of magnitude larger than the dataset derived from distant supervision, which allows the supervised models to learn more robust patterns and generalize better.


\begin{figure*}[ht!]
\centering
\includegraphics[width=.9\textwidth]{quantification_true_supervision.pdf}
\caption{Quantification curves. MAE obtained each method vs the number of training samples with true declared labels (true supervision).}
\label{fig:curves}
\end{figure*}


\subsection{Results: Quantification Task}

\spara{Simpler models outperform embedding-based in quantification with supervised labeling.}
\Cref{table:quant-true} presents the performance of the best-performing models and baseline models on the quantification task using true supervision.
The results demonstrate that simpler Bayesian models, such as Multinomial Naive Bayes (NB) and its variants, consistently outperform embedding-based approaches (WA).
For instance, the NB model achieves significantly lower mean absolute error (MAE) compared to the WA model and its variations across all attributes (gender, partisan, and year).

This advantage is particularly pronounced for the gender attribute, where the NB model not only reduces the MAE but also exhibits lower standard deviation, highlighting its robustness and effectiveness in quantification tasks.
Specifically, the MNB achieves absolute errors of approximately $11\%$, making it a reliable choice for this attribute.

While incorporating semi-supervised techniques or fitting a log-normal distribution to model user activity lead to incremental performance improvements, the computational overhead of these methods outweighs their benefits when considering the marginal gains relative to the error bars.

Additionally, adjustments to the WA model, such as applying adjusted classify-and-count (ACC) to refine conditional probabilities, result in better performance compared to the base WA classify-and-count (CC) method.
However, these improvements remain insufficient to match the performance of Naive Bayes models, with discrepancies as high as $15\%$ for gender.


\begin{table}[t]
\caption{Results for the quantification task with true supervision. MAE for each attribute (mean $\pm$ std dev).}
\centering
\resizebox{\columnwidth}{!}{\input{tables/quantification_true_supervision_FORMATTED}}
\label{table:quant-true}
\end{table}

\spara{Simpler bayesian models are  better than embedding-based with distant supervision.}
\Cref{table:quant-dist} shows that simpler Bayesian models, such as Multinomial Naive Bayes (MNB) and its semi-supervised variants, consistently outperform embedding-based models in quantification tasks under distant supervision.
Notably, the best-performing Bayesian model achieves up to a $36\%$ improvement in gender quantification compared to the standard WA model using Classify and Count (CC).

The results also demonstrate the utility of assumptions inherent in distant supervision labeling.
For attributes such as year and gender, distant supervision produces comparable or better results while enabling the inclusion of a larger and more diverse user base in the training set.
This broader dataset captures more heterogeneous activity patterns, unlike self-declared labels, which may be biased toward users active in fewer subreddits or specific communities.

However, for the partisan attribute, distant supervision proves more challenging.
Only more complex models, such as Random Forest, achieve acceptable performance, with MAE values below $0.2$.
These results highlight the limitations of simpler models for this attribute under distant supervision and emphasize the importance of leveraging labeled data to validate the assumptions and ensure the reliability of distant supervision labels.

Finally, these results underline the critical impact of direct sociodemographic proxies.
While distant supervision expands the training dataset, it inherently introduces noise and reduces performance due to its reliance on assumptions about user participation patterns.
In contrast, directly declared labels provide a more accurate and robust foundation for model training, which leads to superior performance.

\begin{table}[t]
\caption{Results for the quantification task results with distant supervision. MAE for each attribute (mean $\pm$ std dev).}
\centering
\resizebox{\columnwidth}{!}{\input{tables/quantification_distance_supervision_FORMATTED}}
\label{table:quant-dist}
\end{table}

\spara{Supervised models require at least 1k data points to outperform pretrained models.}
\Cref{fig:curves} illustrates the reduction in MAE as a function of the number of training samples for the various models.
The WA model (CC), a pretrained approach introduced by \citet{waller2021quantifying}, relies on classification followed by counting the proportions of each class for the given binary attribute.
As it does not depend on the size of the training dataset, its performance remains constant across all data points.
In contrast, all other methods are trained on datasets with true supervision, and the quantifier wrappers (ACC) further refine these models by fitting conditional distributions in the training set to adjust the class proportions.

For year and partisan, the WA Model converges quickly but reaches a performance plateau that falls short of the Naive Bayes models.
Similarly, Random Forest exhibits consistently higher MAE than Naive Bayes across all training set sizes. %
On the gender task in particular, results underscore the importance of quantifiers.
Training quantifiers to estimate conditional probabilities is crucial for achieving accurate quantification in this attribute, as shown by the poor performance of the Classify and Count (CC) aggregation technique.

Finally, the size of the training data emerges as a critical factor in model performance.
Across all attributes, a minimum of \num{1000} training samples is consistently required for supervised models to surpass the performance of the pretrained WA model.


\subsection{Model Transparency}

\begin{figure*}[ht!]
\centering
\includegraphics[width=.99\textwidth]{figures/calibrations-supervision.pdf}
\caption{Calibration curves for the different attributes, showing the alignment of prediction scores with true probabilities.}
\label{fig:calibration-year}
\end{figure*}

By design, Bayesian models inherently provide interpretable characteristics, making them ideal for tasks requiring transparency and explainability.
Here, we discuss key interpretability aspects derived from our models.

\spara{Feature Importance.}
The interpretability of a Naive Bayes (NB) model stems from its ability to assign log-odds weights to features, reflecting their average importance in predictions.
To illustrate how the model enables researchers to derive interpretable insights from feature importance and foster a deeper understanding of sociodemographic patterns, we analyze the log odds and their standard deviations to identify the most influential features for different attributes.

For \textit{gender}, the key features include \texttt{SkincareAddiction}, \texttt{raisedbynarcissists}, and \texttt{buildapc}, as shown in~\Cref{fig:importance-gender}.
For \textit{partisan affiliation}, significant subreddits include \texttt{Conservative}, \texttt{The\_Donald}, and \texttt{SandersForPresident}, as depicted in~\Cref{fig:importance-partisan}.
For \textit{year of birth}, features such as \texttt{teenagers}, \texttt{Minecraft}, and \texttt{dankmemes} dominate in determining the year attribute, as illustrated in~\Cref{fig:importance-year}.

This analysis showcases the practical utility of Naive Bayes models in uncovering interpretable feature importance, enabling researchers to better understand the sociodemographic behaviors embedded in the data.


\spara{Calibration.}
An important aspect of a predictive model is the ability to interpret its predicted scores for each data point as probabilities, which enables meaningful and actionable insights in the downstream tasks.
To maximize coverage of Reddit's user base, we use the most popular subreddits as the feature space.
However, it introduces challenges such as feature redundancy and overconfidence in Naive Bayes predictions, potentially compromising the reliability of the model's predictions.

To address these issues, we use isotonic regression to calibrate the prediction scores (\Cref{fig:calibration-year}).
Calibration ensures that the predicted scores align with true probabilities, and enhances both the interpretability and reliability of the model.
Notably, Naive Bayes provides a well-calibrated baseline compared to more complex methods, and its inherent simplicity makes it easier to calibrate effectively.
This advantage allows Naive Bayes to maintain accurate probability estimates even in scenarios with large and redundant feature spaces.

\spara{Prediction Intervals.}
In the quantification task, data points are independently but not identically distributed, as each data point is sampled according to a Bernoulli distribution with a success probability equal to its calibrated probability score.
Calibration ensures that these scores accurately reflect true probabilities, making them reliable inputs for further statistical modeling.

Consequently, the Poisson-Binomial distribution, which generalizes the Binomial distribution for non-identical probabilities, describes the joint distribution of the data.
By leveraging the calibrated scores, we can use the known second moment of the Poisson-Binomial distribution to estimate prediction intervals for the quantification measure.
These intervals provide a reliable measure of uncertainty around the class proportions, offering interpretable insights into the variability of the predicted outcomes.

\spara{Robustness.}
To evaluate the robustness of the classifiers under varying conditions, we tested their performance by modifying user selection thresholds.
Specifically, we filtered users based on extreme score values, selecting those with scores either greater than a threshold $\tau$ or less than 1 - $\tau$, where $\tau \in [0, 1]$.
This approach examines how classifier performance responds to restricting the test set to users with high confidence predictions.

All classifiers exhibited strong stability across most threshold values.
Performance remained consistent, with minimal variation until $\tau$ dropped below $0.1$.
At this point, scores varied by $6\%$ to $12\%$, depending on the attribute and classifier.
These findings suggest that classifiers are robust under varying confidence thresholds.



\section{Discussion}

Sociodemographic characterization enables researchers to better understand and model the complex interactions between individuals and their online and offline environments and is an invaluable tool when investigating key societal questions.
However, research on Reddit has lacked systematic evaluations and clear guidelines to identify effective methodologies.
This study addresses this gap by comparing different models:
we offer actionable insights for best practices for CSS research on Reddit and remark on the coverage, interpretability, reliability, and scalability of models.

Our pipeline
spans data collection, modeling, and task execution. %
Thanks to the dataset we collect of over \num{850000} self-declarations on age, gender, and partisan affiliation, we can train and evaluate supervised learning methods with high-quality data.
This approach avoids reliance on assumed inferred characteristics and preserves the authenticity of the ground-truth data.
Moreover, the regex-based data collection process is scalable and robust, thus mitigating labeling costs and reducing potential annotation inconsistencies.


Our approach also highlights the value of the Bayesian framework for sociodemographic inference.
By design, this framework provides transparency and aids in probability calibration and interpretability through its composability.
It supports further analysis using log odds and enables uncertainty quantification through prediction intervals, thus offering researchers a reliable measure of the robustness of the model.
These features should empower researchers to conduct their analyses with greater confidence.

Our findings caution researchers against overly engineered and unnecessarily complex models.
Instead, the results emphasize the effectiveness of simpler approaches that respect users’ self-declared information while maintaining methodological rigor.
This balance between simplicity, interpretability, and ethical transparency is critical for fostering trust in computational social science research.

Finally, our evaluation of classification and quantification tasks provides researchers with clear use cases for both user-level and population-level analysis.
The quantification task, in particular, emphasizes aggregated insights over individual predictions, thus offering a privacy-conscious alternative that reduces risks associated with exposing sensitive user information.
The models analyzed in this work achieve both efficiency and accuracy on both tasks, which makes them a robust choice for sociodemographic inference.

Despite the usefulness of our work, it is important to acknowledge some of its limitations.
(\emph{i}) \textit{Risks of Mislabeling.}
Mislabeling in population studies poses significant risks, including flawed analyses, spurious conclusions, and potentially harmful policy recommendations. Our findings emphasize the need for robust, interpretable pipelines to mitigate these risks and ensure the reliability of sociodemographic insights.
(\emph{ii}) \textit{Binary attributes.}
To simplify modeling, we reduced all attributes to binary pairs.
On gender and political affiliation, such a simplification might reduce the complexity of the world and reduce the visibility of minorities.
While the data did not present enough examples to consider other cases, it is important to be conscious of this limitation.
(\emph{iii}) \textit{Dynamic attributes.}
For similar reasons, we excluded ambiguous and inconsistent user declarations. However, this approach opens opportunities for future research into dynamic disclosures, particularly in analyzing how partisan declarations evolve in response to exogenous events.
(\emph{iv}) \textit{Integration of multiple proxies.}
Our study highlights the strengths and limitations of different proxies, including self-declared labels and distant supervision.
Nevertheless, integrating multiple proxies into a unified model remains underexplored and represents a promising avenue for future research.
(\emph{v}) \textit{Data sources.}
Finally, in this work, we focused on the possibilities offered by subreddit participation data on demographic inference, excluding from our scope alternative sources, e.g., textual comments made by users.
While such alternatives are computationally more expensive and less interpretable, it would be important to quantify their effectiveness in future work.
