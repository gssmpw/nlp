\section{INTRODUCTION}
\label{sec: introduction}
Accurate and robust state estimation is crucial for the successful execution of autonomous missions using mobile robots or vehicles. Global navigation satellite system (GNSS) provides reliable estimation in typical outdoor environments, but its reliability degrades or it is unavailable in obstructed areas such as urban canyons or indoor environments. In such cases, alternative state estimation methods such as simultaneous localization and mapping (SLAM) or odometry using exteroceptive sensors (e.g., camera, LiDAR, and radar) are crucial to maintain reliable autonomy.

Over the past 20 years, real-time state estimation methods using light-based sensors such as a camera and a LiDAR have significantly improved accuracy and robustness in diverse environments~\cite{9196524, 9440682, 9341176, 9697912}. Vision-based methods show notable performance across a wide range of conditions, despite relying on small and lightweight sensors. However, their performance significantly decreases in environments with lighting changes or visually featureless surfaces~\cite{zhang2018laser}. In contrast, LiDAR-based methods are resilient to lighting conditions, and can accurately capture the detailed structure of the surrounding environment over long distances. However, they struggle in environments with self-similarity, such as long corridors or structureless areas like flat planes~\cite{10611444}. Moreover, light-based sensors face limitations when exposed to small particles, such as snow, fog, or dust, due to their short wavelength.

\begin{figure}[t]
\centering \includegraphics[width=\linewidth]{figure_1.png} \caption{Illustration of the temporal misalignment between IMU and radar measurement streams, along with the corresponding EKF's propagation and measurement update steps. The time offset between the sensors is denoted as $t_d$. The time $t'$ corresponds to the IMU measurement and $t$ to the radar measurement.} \label{fig1}
\end{figure}

Recently, a radar has gained attention as a promising solution to address these challenges~\cite{10683889}. In particular, a frequency-modulated continuous wave (FMCW) millimeter-wave 4D radar not only penetrates small particles effectively due to its relatively long wavelength but also measures relative speed (i.e., doppler velocity) to surrounding environments through frequency modulation. This radar typically operates at 5-20 Hz, providing raw signal data, 3D point clouds with spatial information and Doppler velocity for each point, enabling the estimation of ego-velocity from a single radar scan~\cite{6728341}. However, due to its relatively low sensor rate, it is necessary to predict the movement between consecutive radar scans. To address this, sensor fusion with an IMU, which operates at a higher sensor rate of over 100 Hz, can be utilized. The IMU complements the movement between consecutive radar scans, improving overall estimation accuracy and robustness. Therefore, there has been increasing interest and active research in radar-inertial odometry (RIO), which tightly couples the IMU with the radar.

Accurate time synchronization between heterogeneous sensors is crucial for data fusion. All sensors inherently experience delays, leading to a discrepancy between the actual time when the event was captured and the time recorded as the sensor measurement, as illustrated in Fig.~\ref{fig1}. While IMU systems typically have minimal latency, radar systems experience more significant delays due to factors such as signal processing steps, including fast Fourier transform, beamforming techniques, detection algorithms, and elevation angle estimation. Additionally, inherent hardware delays, including the analog-to-digital converter start-time and transmission delay, further contribute to the overall latency of radar systems~\cite{10477463}. Differences in sensor delays lead to temporal misalignments, which can pose significant challenges in multi-sensor systems. To address this issue, special hardware triggers or manufacturer software support may be required, but not all sensors provide such functions~\cite{10610666}. The time offset, which represents the difference in delays between IMU and radar systems, can reach up to hundreds of milliseconds, posing significant challenges for accurate data fusion in RIO. Compared with LiDAR-IMU and camera-IMU systems, our experimental results show that radar-IMU systems have a significantly larger time offset, highlighting the importance of temporal calibration.

In this paper, we propose a method for real-time estimation of the time offset between the IMU and the radar in an extended Kalman filter (EKF)-based RIO. Unlike existing RIO studies that rely on hardware/software triggers or do not consider the time offset, our method directly estimates the time offset online by utilizing the radar ego-velocity. The proposed method demonstrates efficient and robust performance across multiple datasets. Additionally, we show that accounting for the time offset in RIO enhances overall accuracy, even when using the same measurement model. The main contributions of this study can be summarized as: 
\begin{enumerate}
    \item We propose an EKF-RIO-TC framework that estimates the time offset between the IMU and the radar in real-time, utilizing the radar ego-velocity estimated from a single radar scan;
    \item The proposed method is validated through both self-collected dataset from real-world environments and open datasets, with and without hardware triggers. The results show that the time offset between sensors is non-negligible and must be accurately estimated to improve RIO performance; and
    \item To benefit the community, the implementation code has been made open-source. The proposed method is easy to implement and applicable, as it utilizes the radar ego-velocity measurement, which is commonly used in most RIO studies. To the best of our knowledge, this is the first work to implement online temporal calibration between sensors in RIO.
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec: related work} reviews the related work relevant to our research. Section~\ref{sec: notation} outlines notations used throughout the paper. Section~\ref{sec: filter description} details the framework of the proposed RIO, accounting for the time offset between sensors. Section~\ref{sec: experiments} validates the proposed method across multiple datasets and analyzes the results. Finally, Section~\ref{sec: conclusion} summarizes the findings and concludes the paper.