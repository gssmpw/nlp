\section{Related Work}
\label{sec: related work}
In this section, we introduce filter-based RIO research using FMCW 4D radar. Because no existing research considers temporal calibration in the radar-IMU systems, we subsequently introduce research into online temporal calibration in other multi-sensor fusion systems.

\subsection{Radar-Inertial Odometry with FMCW 4D Radars}
Doer and Trommer \cite{9235254} proposed an EKF-based RIO framework that fuses IMU data with 3D ego-velocity estimated from radar measurements. They introduced the 3-point RANSAC-LSQ to remove outliers from the noisy radar measurements during the ego-velocity estimation. Later, the same authors extended the filter state to include the extrinsic parameters between the IMU and the radar for online extrinsic estimation \cite{9317343}, and enhanced yaw estimation for indoor environments using Manhattan world assumptions \cite{9470842}. Michalczyk et al. \cite{9981396} focused on the relatively accurate radar measurements, specifically distance and Doppler velocity, utilizing them for filter updates. In addition to ego-velocity updates, they constructed point-to-point residuals between consecutive scans and leveraged the most informative dimension, which is distance, for the measurement update. The same authors extended this approach by incorporating multi-pose into the framework and including long-term observed points in the filter state to further improve performance \cite{10160482}. Zhuang et al. \cite{10100861} adopted graduated non-convexity for ego-velocity estimation and applied distribution-to-multi-distribution constraints for current scan and submap matching when utilizing point residuals for sparse radar points. They also incorporated scancontext for place revisits and loop closures via pose graphs. Kubelka et al. \cite{10610666} compared scan matching-based methods (e.g., ICP, APD-GICP, and NDT) and radar ego-velocity-based methods (e.g., IMU attitude with radar ego-velocity and EKF-RIO \cite{9235254}) across two distinct radar sensor setups. Scan matching-based methods showed better performance in dense point cloud setups, while radar ego-velocity-based methods excelled in sparse point cloud setups. In sparse radar setups, scan matching-based methods experienced random divergence due to incorrect scan matching caused by the low radar point cloud density. Performance degradation in radar ego-velocity-based methods was also observed in specific sections, such as when the vehicle hit a bump, causing discrepancies between the IMU and the radar measurements due to the temporal misalignment. Their analysis on this issue motivated our research.

In \cite{9235254, 9317343, 9470842}, hardware triggers were implemented on a microcontroller board for sensor time synchronization. In \cite{10610666}, and~\cite{9981396, 10160482, 10100861}, the authors did not account for the time offset when constructing their system frameworks, assuming the times of sensor measurements were accurate. Our proposed method estimates the time offset in real-time from radar ego-velocity, without relying on physical triggers. Since the ego-velocity is estimated from a single scan, there is no need for matching between consecutive scans, making the method independent of radar point cloud density, offering greater flexibility for various radar sensors. Furthermore, since all the mentioned works~\cite{10610666, 9317343, 9235254, 9470842, 9981396, 10160482, 10100861} utilize radar ego-velocity in the measurement update, the proposed method can be seamlessly integrated into their frameworks, not only ensuring accurate time synchronization but also potentially improving scan matching accuracy.

\subsection{Online Temporal Calibration for Multi-Sensor Fusion Systems}
Qin and Shen \cite{8593603} proposed an optimization-based method for visual-inertial odometry (VIO) that enables online temporal calibration. They addressed time synchronization by jointly optimizing a prior factor, an IMU propagation factor, and a vision factor that accounts for the time offset between sensors. In particular, they compensated for feature measurements using the feature velocity on the image plane along with the time offset. Li and Mourikis \cite{li2014online} proposed a filter-based method to estimate the time offset between a camera and an IMU by including the time offset variable in the filter state. They formulated the camera measurement models for the 3D feature positions using the filter state including the time offset, thereby effectively estimating the time offset and improving the performance of VIO. Lee et al. \cite{9561254} proposed a filter-based method for multi-sensor fusion odometry involving a LiDAR and an IMU. Since finding point correspondences between LiDAR scans can be challenging, they used plane patches, which contain structural information, to handle the data more efficiently. Their method involved extracting plane patches from the LiDAR point clouds and incorporating the time offset variable into the plane measurement models.

In contrast to previous works \cite{8593603, li2014online, 9561254} which estimate the time offset by matching features between consecutive images or scans, our proposed method estimates the time offset from a single radar scan by formulating the radar ego-velocity measurement models. Unlike the camera and the LiDAR, the radar uniquely measures the Doppler velocity, enabling the direct estimation of ego-velocity. Instead of relying on scan matching, which can be challenging due to the sparse nature of radar point clouds, the proposed method avoids potential inaccuracies associated with correspondence matching.