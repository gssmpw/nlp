        % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{dblfloatfix}
\usepackage{amsmath}
\usepackage{float}
\usepackage{booktabs}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
}


\newcommand{\methodname}{Q-STRUM}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\methodname\ Debate: Query-Driven Contrastive Summarization for Recommendation Comparison}
%\title{\methodname\ Debate Prompting for Query-driven Contrastive Summarization for Recommendation Comparison}
%\title{\methodname\ Debate Prompting for Recommendation Comparison via Query-driven Contrastive Summarization}
%\title{Query-driven Contrastive Summarization via\\ \methodname\ Debate Prompting for Recommendation Comparison}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{George-Kirollos Saad \\
  University of Toronto \\ Toronto, Ontario, Canada \\
  \texttt{g.saad@mail.utoronto.ca} \\\And
  Scott Sanner \\
  University of Toronto \\ Toronto, Ontario, Canada \\
  \texttt{ssanner@mie.utoronto.ca} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

% ========================================
% Scott notes on SIGIR Poster:
% Make the work STRUM-centric to focus your reader's attention and avoid need for excessive comparison to every recommendation explanantion method in the literature.
% Cover STRUM, mention that it is extended to be query driven.  We note that it is not contrastive and that re-engineering its prompts to be more contrastive have limited effect.
% To improve the overall contrastiveness of Q-STRUM based explanations, we introduce a novel methodolog called debate-style prompting that ...
% We show that Q-STRUM-Debate strongly outperforms variants of Q-STRUM ... (use key abstract claims here).
%
% - Query-driven recommendation with unknown items poses a challenge for users to understand why items are appropriate for their needs
% - Query-driven Comparative Summarization (QCS) is a methodology that can address these issue
% - However, existing QCS methods fail to clearly contrast the items to help users decide between them.
% (Need to mention LLMs somewhere -- great tool for NL explanation and summarization)
% - To address this deficit, we introduce a debate promping method for QCS that is intended to help propose more focused contrastive debates on item aspects relevant to the user's interests.
% ========================================

\begin{document}
\maketitle
\begin{abstract}
    % The rapid expansion of natural language processing (NLP) has increased interest in providing grounded explanations for recommendations, particularly in contrastive, query-driven contexts. This work investigates methods for generating explanations that use both user reviews and objective data sources. Inspired by existing literature and discourse theory, we explore various design choices to enhance the quality of these explanations. We introduce a query-driven variant of STRUM-LLM called \methodname. We also introduce debate-style prompting approach to generate contrastive explanations for recommendations. We find that debate-style prompting outperformed \methodname\ for contrastive summarization on two datasets, and for another dataset, either matches or beats the baselines on all criteria. We also examine whether the level of discourse aggressiveness influences explanation quality, finding no significant improvement. Additionally, we assess the reliability of large language models (LLMs) for pairwise Win Rate evaluation through a user study, showing that LLM-based pairwise evaluations align well with human judgment and can thus be considered trustworthy.


Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them.  However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal.  To overcome these limitations, we introduce \methodname\ Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, \methodname\ Debate 
provides enhanced contrastive summaries. Experiments across three datasets demonstrate that \methodname\ Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS. 
%This approach establishes \methodname\ Debate as a robust solution for improving recommendation summarizations and user decision-making.
%  providing clear and contrastive comparisons.

\end{abstract}

% Scott's notes
% - Recommendation is becoming query-driven
% - Recommendation of unknown items is challenging -- hard to understand how of why items satisfy user preference
% ... Among multiple choices, harder to understand relative trade-offs
% - Query-driven comparative explanations serve as a possible solution
% ... Need to give both query and item answers (Bangkok, Melbourne) and then make it clear that's it obvious why these are appropriate, nor how they compare?  Figure 1 provides a much better answer.

% Streamlined
% - Define QCS
% - Discuss Example in Figure 1
% - Discuss existing methods (or obvious adaptations thereof) and what's lacking
% - Discuss our motivation (in terms of discourse and debate) and our contributions (need to add back in aggresiveness) including highlights of our experimental results.
\section{Introduction}

% In today’s information-rich environment, recommendations are increasingly query-driven, with users seeking personalized options that match their specific needs. However, recommending unknown items poses significant challenges, as it is difficult to understand how or why these options align with user preferences. When faced with multiple choices, understanding the relative trade-offs becomes even harder. This difficulty is compounded by the abundance of information from subjective sources like user reviews on TripAdvisor or Amazon and objective ones such as Wikipedia or travel guides \cite{gunel2024strumllmattributedstructuredcontrastive, wen2024elaborative}.

% Query-driven Contrastive Summarization (QCS) offers a potential solution to these challenges. The emergence of large language models (LLMs) has revolutionized the way we process and summarize vast amounts of text, offering more accessible insights and facilitating QCS \cite{chowdhery2023palm, colin2020exploring}. By distilling relevant content into concise comparisons, they help users evaluate options more effectively and make informed decisions. Users not only seek recommendations but also want to understand the reasoning behind these suggestions, ideally grounded in concrete data and clear comparisons \cite{lubos2024llm, pu2006trust}.

In query-driven recommendation settings such as hotels, restaurants, or travel, where items may be {\it a priori} unknown to users, language-based item descriptions can help users make informed choices.
%Users increasingly seek tailored options that align with their specific needs, but recommending unknown items 
%remains difficult due to the lack of clear reasoning behind suggestions. 
%the challenges of personalized recommendations in today’s information-rich environment. 
However, understanding the trade-offs between choices becomes challenging given the abundance of information from objective sources, like Wikipedia or travel guides, and opinion-rich subjective sources, such as TripAdvisor and Amazon reviews~\cite{gunel2024strumllmattributedstructuredcontrastive, wen2024elaborative}. 

Fortunately, Query-driven Contrastive Summarization (QCS) offers a principled solution to these challenges by providing succinct comparative summaries of items.  
However, many traditional QCS methods often rely on complex extraction, ranking, and diversification algorithms that may fail to find clear contrasts, leaving users to sift through extensive information~\cite{qcs_summary}. 

Fortunately, the emergence of large language models (LLMs) has revolutionized 
QCS capabilities~\cite{colin2020exploring,angelidis-etal-2021-extractive,chowdhery2023palm}. 
%revolutionized text processing and summarization, making insights more accessible and facilitating QCS 
By distilling relevant descriptive and review content into concise comparisons, state-of-the-art LLM-based contrastive summarization methods such as STRUM-LLM~\cite{gunel2024strumllmattributedstructuredcontrastive} enable users to comparatively evaluate choices via
%options effectively and make informed decisions. Users not only seek recommendations but also desire 
summaries grounded in concrete data and clear comparisons that are important for decision-making~\cite{lubos2024llm, pu2006trust}.
%Recent structured contrastive summarization and aspect-based summarization~\cite{strum,gunel2024strumllmattributedstructuredcontrastive,angelidis-etal-2021-extractive}
While these LLM-driven approaches arguably improve on their pre-LLM predecessors, they often fall short of their contrastive summary potential as we show in our comparative empirical evaluation.
%of the contrastive level required to make item trade-offs explicit.


% Remove example and refer directly to Figure 1
% Consider a scenario where a user queries for travel destinations with specific amenities.
% \begin{quote}
%     \textit{``Culinary cities for food lovers''}
% \end{quote}
% and the top two recommendations of \emph{Bangkok, Thailand} and \emph{Melbourne, Australia}. 

% Consider the example shown in Figure \ref{fig:good-output}. A simple list of destinations based on the query would be insufficient for most users to make an informed decision on why either option is relevant to the query and the user's personal preferences.  

% While platforms like WikiVoyage provide rich information, the sheer volume of content can overwhelm users. A summarization approach that distills content like this or reviews into comparative insights is essential \cite{hernandez2023explaining}. For instance, a debate-style system could provide a nuanced comparison of options, highlighting key pros and cons effectively, as demonstrated in Figure \ref{fig:good-output

%% Scott: Good content, just don't know where to put it -- currently it breaks the flow
%A simple list of destinations does not help users understand why an option is relevant to their query and preferences. Platforms like WikiVoyage contain extensive information, but their sheer volume can overwhelm users. A summarization approach that condenses this information into comparative insights is essential \cite{hernandez2023explaining}. 

% Scott
% - Reduce to 2 aspects: choose one of diverse or global
% - Reduce to one contrastive sentence per aspect
% ... diverse / global: cuisine types
% ... unique: street food vs. coffee culture
% - Almost suggests a prompt output template of "Option 1 has A, but Option 2 has B"

% move query to caption
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/reduced-example-debate.pdf}
    \caption{Example of an aspect-based contrastive summary for the query: ``culinary cities for food lovers''}
    \label{fig:good-output}
\end{figure*}

This work addresses a critical gap in QCS by leveraging LLMs to prioritize contrast while maintaining relevance and groundedness as motivated by principles of debate that are founded on discourse theory,
%discourse theory 
%that underlies principles of debate, 
including the Elaboration Likelihood Model (ELM) \cite{Petty1986} and Grice's Maxims \cite{Grice2013-GRILA-2}.  Building on the state-of-the-art contrastive summarization STRUM-LLM framework~\cite{gunal-etal-2024-conversational}, we propose that aspect-based \emph{debate prompting} provides a natural framework for improved QCS that we term \methodname\ Debate.  An example output summary demonstrating this approach is shown in Figure~\ref{fig:good-output}. %that provides a structured comparison, which clearly presents key pros and cons.
%***
%LLMs to generate summaries that prioritize contrast, relevance, and groundedness. 

We summarize our key contributions as follows:
\begin{enumerate}
    \item We provide a novel \emph{debate prompting} mechanism to improve contrastiveness in QCS.
    \item We show the resulting \methodname\ Debate matches or outperforms base STRUM-LLM and a contrastive prompt extension on three  domains (hotels, restaurants, and travel). 
    \item We modulate debate prompt aggressiveness and evaluate its impact on summary quality.
\end{enumerate}
%By adopting a debate-driven approach, \methodname\ Debate aims to provide clear and contrastive summaries that allow users to make informed choices.
%understand item trade-offs and 
%contrastiveness to provide data-grounded insights
%% Scott: Oh man, this is so GPT
%%contrastive, data-grounded insights that empower users to navigate complex choices with greater clarity and confidence.





%\cite{gunel2024strumllmattributedstructuredcontrastive, angelidis-etal-2021-extractive}, we propose that structured debate-style prompting, informed by discourse theory, provides a natural framework for producing superior query-driven contrastive explanations. 
% Scott: will edit and put back
% Furthermore, we examine how varying levels of assertiveness in these prompts influence the quality of explanations and evaluate the reliability of LLM-based assessments against human judgments \cite{liu2023gevalnlgevaluationusing, dubois2024length}.


% At the core of QCS, debate-style prompting draws inspiration from discourse theory frameworks, including the Elaboration Likelihood Model (ELM) \cite{Petty1986} and Grice's Maxims \cite{Grice2013-GRILA-2}. ELM emphasizes deeper cognitive engagement through the central route of persuasion, where users critically evaluate substantive arguments. This makes debate-style prompting an ideal approach for fostering informed decision-making. Grice's Maxims, which stress relevance, clarity, and informativeness, further guide the creation of contrastive and grounded explanations. By incorporating these principles, debate-style prompting ensures that QCS generates user-relevant insights that are both clear and actionable, empowering users to make better-informed decisions \cite{miller2018explanationartificialintelligenceinsights}.

% By addressing these challenges, this research aims to advance the effectiveness of LLMs in facilitating query-driven decision-making, empowering users to navigate complex choices with greater clarity and confidence.


\section{STRUM for Contrastive Summarization}

% Scott: need to provide guidance to the reader
% - What are you doing in the paper
% - What is the first you need to cover and why?
% ...
% - Reminder reader that the goal of the paper is to do contrastive summarization with LLMs for query-driven recommendation.  Remark that there are already methods for LLM-based contrastive summarization, namely STRUM, that you build on in this work.  Hence, it is first important to define STRUM and then discuss the enhancements that this work makes to improve contrastive summarization as empirically verified in Section X.
Recommendations often involve presenting users with multiple options, requiring methods that clearly articulate how and why each option aligns with their preferences. STRUM \cite{strum} introduced a seminal and foundational approach to contrastive summarization by leveraging entailment models and hierarchical clustering to extract, merge, and contrast aspects of data. While effective for structured summarization, STRUM faced significant limitations in producing outputs that were sufficiently contrastive and aligned with user-specific queries.

% Scott -- move to Appendix?  Explain aspects extracted in context of Aspect Merge stage (which gives examples?)
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/aspect-extract.pdf}
    \caption{\textit{Aspect Extraction} stage for the query: ``culinary cities for food lovers''}
    \label{fig:aspect-extract}
\end{figure}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figs/aspect-merge.pdf}
    \caption{\textit{Aspect Merge} stage for the query: ``culinary cities for food lovers''}
    \label{fig:aspect-merge}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/filter-stage.pdf}
    \caption{\textit{Filter} stage for the query: ``culinary cities for food lovers''}
    \label{fig:filter-stage}
\end{figure*}

To address these challenges, STRUM-LLM \cite{gunel2024strumllmattributedstructuredcontrastive} integrated large language models (LLMs) to improve attribute extraction, aspect merging, and summarization. The architecture is outlined in Figure \ref{fig:qstrum}(a). STRUM-LLM employs several LLM-driven components:
\begin{itemize}
    \item \textbf{Aspect Extraction}: Identifies aspects and relevant values from source data while attributing them to their origins. An example of this stage is provided in Figure \ref{fig:aspect-extract}.
    \item \textbf{Aspect Merge}: Combines similar aspects to reduce redundancy. An example of this stage is provided in Figure \ref{fig:aspect-merge}. 
    \item \textbf{Value Merge}: Consolidates consistent values for each aspect based on majority opinion.
    \item \textbf{Contrastive Summarizer}: Highlights the most significant and contrasting aspects.
    \item \textbf{Usefulness}: Filters out less useful aspects and identifies errors.
\end{itemize}

While STRUM-LLM provides a state-of-the-art methodology for contrastive summarization, it is not query-driven as originally defined.
%, which can be addressed in an extension we propose called query-driven STRUM (\methodname). 
More critically, we also empirically show that the STRUM-LLM methodology falls short of providing strongly contrastive summaries. 
%These gaps highlight the need for a more robust framework that incorporates both contrastive insights and query-driven adaptability. 
To address these gaps, we next introduce \textbf{\methodname\ Debate}, which builds upon STRUM-LLM to deliver query-driven, contrastive summarization. Central to this improvement will be the introduction of \emph{debate prompting}. %which directly addresses the contrastiveness deficiency observed in STRUM-LLM summaries.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/method-compare.pdf}
    \caption{Comparison of Outputs of All \methodname\ Methodologies}
    \label{fig:outputs}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/q-strum-query.pdf}
    \caption{STRUM-LLM vs. \methodname\ Architectures}
    \label{fig:qstrum}
\end{figure*}

\section{\methodname\ for QCS}
\label{sec:q-strum}

To address the limitations of STRUM-LLM and enable a query-driven system for contrastive summarization, we propose \textbf{\methodname}. This method ensures that the query is passed through all stages of the architecture to generate highly relevant, contrastive outputs tailored to user needs.

% Scott: Should come directly above debate summary (could merge actual figures or just force Latex to stack them).
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/debate-example-td.pdf}
    \caption{\textit{Debate} example for the query: ``culinary cities for food lovers''}
    \label{fig:debate-example}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/debate-summary.pdf}
    \caption{\textit{Debate Summary} example for the query: ``culinary cities for food lovers''}
    \label{fig:debate-json-example}
\end{figure*}

\subsection{Pipeline Overview}
\label{sec:q-strum-base}
\methodname\ employs a structured pipeline with four stages: Aspect Extraction, Aspect Merge, Filter, and Summarizer. Each stage uses the query as an anchor, ensuring alignment with user intent. The Filter stage, unique to \methodname, selects the top three aspects most relevant to the query and extracts exactly 10 concise, informative phrases for each. This reduces noise and redundancy, enabling downstream stages to focus on concise, length-controlled content. Figure \ref{fig:filter-stage} outlines an example of the Filter stage. These modifications make the Value Merge and Usefulness stages from STRUM-LLM redundant and hence allow us to remove them from the pipeline.  
%ensure controlled experimental comparison of all methods. 
Figures \ref{fig:outputs} and \ref{fig:qstrum} illustrate the output and architecture comparisons, respectively.

\begin{figure}[!ht]
    \small
    \begin{lstlisting}[caption={LLM Prompt for Debate Stage}, label={lst:debate}]
Query: {{query}}

Destination 1: {{dest1}}
{{sents1}}

Destination 2: {{dest2}}
{{sents2}}

You must simulate a debate between 2 people, Alice and Bob. 
Alice thinks that {{dest1}} is the best destination for the provided query, whereas Bob thinks {{dest2}} is for the specific aspect of: {{aspect}}. Alice and Bob should emphasize pros of their respective destinations and cons of the other destination. Make it extensive and detailed and try to mention as many sentences and points as possible. 

Perform and output a contrastive debate for each of 2 destinations for the aspect. The debate should include exact phrases from the provided sentences with sentence number citations.
    \end{lstlisting}
\end{figure}



\subsection{Contrastive Prompting}
\label{sec:q-strum-cont}

The Base Summarizer uses a monolithic prompt to produce a general summary of the extracted aspects from the Filter stage. This approach provides a simple, high-level overview of the data. The Contrastive Summarizer builds on this by explicitly instructing the LLM to \textit{``identify the most contrasting and important values.''} This simple yet effective addition produces more detailed and relevant contrastive outputs \cite{gunel2024strumllmattributedstructuredcontrastive}. Prompts for both the Base and Contrastive Summarizers are provided in Appendix \ref{sec:prompts}.



\begin{figure}[!ht]
    \small
    \begin{lstlisting}[caption={LLM Prompt for Debate Summary Stage}, label={lst:debate-json}]
Query: {{query}}
Aspect: {{aspect}}

Destination 1: {{dest1}}
{{sents1}}

Destination 2: {{dest2}}
{{sents2}}

Debate: {{debate}}

Based on the provided sentences and debate, provide a contrastive comparison for each of 2 destinations for only the listed aspect in JSON format.

Requirements are as follows:
- Do not mention Alice or Bob in the output.
- The keys should be the destination names, exactly as provided.
- The output should include summarization, backed by quotes with exact phrases from the provided sentences with sentence number citations.
- The output should be contrastive, specifically mentioning pros and cons of the destination.
- The phrasing of the output should be natural and more explanatory.
- You must include at least 5 points per aspect for each destination.


Output format:
{
 "{{dest1}}": "<extracted phrases> [sentence #]",
 "{{dest2}}": "<extracted phrases> [sentence #]"
}
    \end{lstlisting}
\end{figure}

\subsection{Debate Prompting}
\label{sec:q-strum-debate}

% Scott: ideally every box in the architecture needs a definition of input format, output format, and the prompt / algorithm that makes it happen.
%
% Scott: Need a brief summary of uses of debate in LLMs, but note that it has not been introduced for query-driven contrastive summarization.
% 
% Scott: Need full technical (comparative) explanation of debate methodology with running examples.  Ideally need a table showing 3 prompts (base, contrastive, debate), but George notes that debate is actually quite different.
% - First be crystal clear with Figures on input and output.  But there is an input from the Filter and an output to the aspects/sentences -- be very clear on these input and output formats (ideally there is a figure that gives examples of each, for the output you do have an example).
% - Next explain that base/contrastive summarization simply use a monolithic prompt to produce the output from the input (reference Prompts in the Appendix).
% - Next explain the technical details of Debate prompting, which has two stages.
% ... Reference debate prompt in the main paper, show input and output
% ... Split Debate-JSON into Debate Summary prompt and Debate-JSON light blue box output: show prompt and an (partial) example output of the Debate JSON.  Ideally the final output for Debate is already in the Figure... only need to give content for "Global Culinary Influences".
Debate prompting introduces a multi-stage process to address the shortcomings of monolithic summarization. Recent work demonstrates that inter-LLM debates have been shown to produce more truthful answers by leveraging structured argumentation \cite{khan2024debatingpersuasivellmsleads}. However, debate-style prompting has not been applied to query-driven contrastive summarization until now. \methodname\ Debate leverages this approach to enhance outputs, ensuring they are contrastive and aligned with user queries.


Unlike Base and Contrastive summarizers, which rely on a single prompt, Debate prompting divides the task into distinct stages:

\begin{itemize}
    \item \textbf{Debate Stage}: The LLM simulates a structured argument between two perspectives (e.g., Alice and Bob), where each defends one entity while addressing the other's weaknesses. This ensures balanced, contrastive comparisons by emphasizing both pros and cons. An example of a debate for the query:
    \begin{quote}
        \textit{``Culinary cities for food lovers''}
    \end{quote}
    for destinations Bangkok and Melbourne is provided in Figure \ref{fig:debate-example}. Highlights include pros and cons (green) and references to source data (yellow). The prompt used for this can be found in Listing \ref{lst:debate}.
    \item \textbf{Debate Summary Stage}: The output from the Debate Stage is summarized and then formatted into a structured JSON representation, called Debate-JSON. This step ensures that the information is well-organized and explicitly aligned with the query and extracted aspects. An example of the output of this stage for the same debate in Figure \ref{fig:debate-example}, can be found in Figure \ref{fig:debate-json-example}. The prompt used for this can be found in Listing \ref{lst:debate-json}.
    \item \textbf{Final Contrastive Summary}: The structured data from the Debate-JSON is processed using the same Contrastive Summarizer prompt from Section \ref{sec:q-strum-cont}.
    %This final step condenses the arguments into a concise, user-friendly summary with 3 aspects, each with 3 bullet points.
\end{itemize}


This multi-stage process ensures that the outputs are not only highly contrastive but also grounded and well-structured. Figure \ref{fig:qstrum}(b) and (c) depicts the architecture of \methodname\ Debate alongside Base and Contrastive methodologies. 


\begin{table*}[!ht]
\centering
\small
%\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{\begin{tabular}[c]{@{}c@{}}Dataset \\\ Name\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Number\\ of\\ Queries\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Number\\ of \\ Entities\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Average Number of \\ Reviews / Data Snippets\\ Per Entity\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Average Length of\\ Review / Data Snippet\\ (in characters)\end{tabular}} &
  \textbf{Data Source} \\\midrule
TravelDest  & 50 & 774 & 163.31 & 264.53 & WikiVoyage          \\
Restaurants & 26 & 43  & 94.51  & 441.96 & TripAdvisor Reviews \\
Hotels      & 24 & 29  & 75.76  & 798.61 & TripAdvisor Reviews \\\bottomrule
\end{tabular}%
%}
\caption{Summary of Datasets}
\label{tab:datasetssummary}
\end{table*}

% Scott TODO: Weave debate/discourse motivation back into paper from communication theory and grounded in the actual training data statistics.

% we need to:
% - show example of debate-json output

% Scott: these don't seem to go here, I wonder if we could move brief summaries to the intro as motivation for how to better structure explanations.
% ... Key points: debate is based discourse theory for effective communication and argumentation.  [Grice's maxims are about effective communication and ELM is about persuasive argumentation.]  We argue that this is precisely what we need for contrastive explanation.
% ... LLMs have most likely been trained on debate-style data (e.g., transcripts of debates) -- certainly they are empirically capable of producing debates -- and hence implicitly capture these principles of debate and discourse theory in their training.  
% ... We aim to leverage this inherent debating ability for effective query-driven contrastive explanation in this work and show that empirically it works better than direct contrastive methods of prompting.
% ... Could probably drop sections on the Elaboration Likelihood Model and Grice's Maxims in the intro and focus on the simple streamlined argument that (a) discourse theory/debate serve our needs and (b) LLMs know how to debate b/c they've likely been trained on a variety of debate data.

%% ChatGPT cited sources for LLMs:
% Yes, modern large language models (LLMs) have been trained on diverse sources that help them engage in debates. These sources include:
%* Philosophical Texts & Ethical Discussions – Works from philosophers and ethicists, helping with argument structure and moral reasoning.
%* Academic Papers & Journals – Covering fields like law, political science, economics, and more, providing well-supported arguments.
%* News Outlets & Opinion Pieces – Representing multiple perspectives on current events and historical debates.
%* Public Forums & Discussions – Data from platforms where structured debates occur (e.g., transcripts of debates, policy discussions, moderated forums).
%* Legal and Policy Documents – Offering insight into argumentation in law and governance.
%While LLMs can simulate debate well, they still have limitations, such as potential biases in training data, lack of personal opinions, and challenges in evaluating the strength of arguments beyond pattern recognition. Would you like to test a debate scenario with me?

% Scott: Needs to be heavily streamlined... about 3 pages, all extra detail moved to sections in the Appendix.

% Scott: cite the Appendix... mention up front and mention section.
% **[Up front] We briefly summarize three datasets chosen for experimentation and provide full explanations of data preprocessing in Appendix ZZZ.
% **Should make a new github with code required to process data and reproduce results (need a simple README to provide explanations of where to look for different content and the command line arguments to reproduce results).
% Could put a footnote "Github code repository URL suppressed for anonymous review.", but anonymous4openscience is much more transparent and preferred by reviewers.
% Use https://anonymous.4open.science/
\section{Datasets}
To evaluate \methodname\ Debate, we used three query-driven entity recommendation datasets with diverse and comprehensive query-entity pairs, containing detailed textual data relevant to queries.  We aimed to experiment with both objective entity descriptions as well as highly subjective review-based entity opinions.
%and sufficient granularity for robust comparisons.  
Full data preprocessing details are provided in Appendix \ref{sec:preprocessing}.  Table \ref{tab:datasetssummary} summarizes key statistics of the following datasets: 
%\subsection{Datasets Overview}
\begin{itemize}
    \item \textbf{TravelDest:} Objective (factual, non-review) travel destination descriptions from WikiVoyage. Example query: ``Top cities for music lovers''.
    % Toronto Restaurants
    \item \textbf{Restaurants:} Subjective restaurant reviews from TripAdvisor. Example query: ``I want a romantic restaurant with views of the city''.
    % Toronto Hotels
    \item \textbf{Hotels:} Subjective hotel reviews from TripAdvisor. Example query: ``Find me a family-friendly hotel with enriching activities for kids''.
\end{itemize}


%The preprocessing performed on these datasets is outlined in further detail in the Appendix.


% To evaluate the efficacy of \methodname\ Debate in generating contrastive explanations, the datasets used must fulfill certain critical requirements. Specifically, they need to:
% \begin{itemize}
%     \item Include diverse and comprehensive query-entity pairs.
%     \item Contain detailed textual data relevant to each query.
%     \item Provide sufficient data granularity to allow for robust extraction and comparison of aspects.
% \end{itemize}
% We employed one objective dataset and two subjective review datasets to ensure the model's performance could be evaluated across different types of content.

% \subsection{Datasets Overview}
% We utilized three datasets to evaluate \methodname, each chosen to test its ability to handle both objective and subjective information. The datasets include:

% \begin{itemize}
%     \item \textbf{TravelDest:} This dataset consists of objective information from WikiVoyage, providing structured and factual descriptions of travel destinations. It allows for consistent and reliable comparisons across entities, aligning well with evaluation tasks requiring grounded data \cite{wen2024elaborative}. Some example queries include:
%     \begin{quote}
%         ``Top cities for music lovers'' \\
%         ``Vibrant cities known for their tropical atmosphere''
%     \end{quote}
    
%     \item \textbf{Toronto Restaurants:} Developed using subjective reviews scraped from TripAdvisor, this dataset introduces variability and personal opinions. The user-generated content offers a rich context for assessing how \methodname\ performs on less structured, opinion-driven data \cite{angelidis-etal-2021-extractive}. Some example queries include:
%     \begin{quote}
%         ``I want a restaurant with healthy options like salads'' \\
%         ``I want a romantic restaurant with views of the city''
%     \end{quote}
    
%     \item \textbf{Toronto Hotels:} Similar to the Restaurants dataset, this dataset includes subjective reviews from TripAdvisor. The variability in user perspectives provides a diverse test bed to evaluate \methodname's ability to generate accurate and relevant contrastive explanations from subjective and less structured textual data \cite{angelidis-etal-2021-extractive}. Some example queries include:
%     \begin{quote}
%         ``Find me a family-friendly hotel that offers enriching activities for kids'' \\
%         ``I want a kitchen, access to vending machines, a view of the city''
%     \end{quote}
% \end{itemize}

% A summary of the three datasets is provided in Table \ref{tab:datasetssummary}.



\section{Experimental Design}

% Scott: Important to highlight that all methods are constrained to produce roughly the same amount of explanation (3 aspects, 3 bullet points per aspect), which is an important control to ensure that Win Rate depends on content and not the length of verbosity of different methods.
%\subsection{Baselines}

We empirically compare our novel  \textbf{\methodname\ Debate} (Section \ref{sec:q-strum-debate}) to the query-driven STRUM-LLM extension \textbf{\methodname\ Base} (Section \ref{sec:q-strum-base}) and its  \textbf{\methodname\ Contrastive} extension (Section \ref{sec:q-strum-cont}) to address the following research questions:
% The baselines used in this evaluation are versions of \methodname. Both baselines use the same query-driven architecture up to the Filter stage, as described in Section \ref{sec:q-strum}. The key differences lie in the final summarization stage, which tests the impact of contrastive instructions. The baselines are as follows:

% \begin{itemize}
%     \item \textbf{\methodname\ Base}: This version uses a simple summarizer in the final stage, providing a baseline without explicit contrastive instructions. Details are provided in Section \ref{sec:q-strum}.
%     \item \textbf{\methodname\ Contrastive}: This version incorporates contrastive summarization, instructing the model to emphasize key differences between entities. Details are provided in Section \ref{sec:q-strum-cont}.
% \end{itemize}

% Scott: Discuss RQs in order in results, need more visible structure to match RQs listed here.
%%This work aims to address the following research questions:
    \begin{itemize}
        \item \textbf{RQ1}: Does debate-style prompting improve query-driven contrastive summaries?
        % \item \textbf{RQ2}: How does pairwise LLM Win Rate evaluation of query-driven contrastive explanations compare to human judgments?
        \item \textbf{RQ2}: Does the aggressiveness (or niceness) level in debate prompting impact the quality of contrastive summaries?
    \end{itemize}
% along with a link to a code repository to reproduce all results.


% merge RQs and results (5.5)
\subsection{RQ1: Win Rate Evaluation Metrics}
\label{sec:evalmetrics}

We evaluate Q-STRUM and its baselines using a pairwise LLM Win Rate evaluation approach, leveraging GPT-4o and Claude-3.5-Sonnet to compare summary outputs. %for query-driven contrastive summarization.
We bidirectionally tested both A vs. B and B vs. A to control for potential LLM ordering bias in winner evaluation.
This methodology aligns well with human judgments and is effective for subjective assessment tasks such as explanation evaluation \cite{liu2023gevalnlgevaluationusing,liu2024aligninghumanjudgementrole,wang2023large}. Pairwise evaluation allows for nuanced comparisons, determining a ``winner'' for each summary based on established criteria.

The Win Rate for Method A vs B is defined as:
\begin{equation}
    \text{Win Rate}_A = \frac{\text{times A wins} + 0.5 \times \text{ties}}{\text{Total Comparisons Made}} \times 100\%
\end{equation}

% Scott: Need to be precise on the GPT-4o prompts required for evaluation, reference Appendix.
% - Should mention win rate and define as the percentage of examples where Method A beats Method B.
The evaluation focuses on four key criteria, derived from the existing literature:

\paragraph{Contrastiveness.} Summaries should effectively highlight differences, emphasizing pros and cons to help users make decisions \cite{miller2018explanationartificialintelligenceinsights,castelnovo2023evaluativeitemcontrastiveexplanationsrankings}.

\paragraph{Relevancy.} Outputs must align with the query and address user-specific needs \cite{castelnovo2023evaluativeitemcontrastiveexplanationsrankings,miller2018explanationartificialintelligenceinsights}.

\paragraph{Diversity.} Summaries should provide a variety of points without repetition, offering multiple facets of comparison \cite{Gienapp_2024,castelnovo2023evaluativeitemcontrastiveexplanationsrankings}.

\paragraph{Usefulness.} Summaries must be informative and help users in decision-making \cite{lubos2024llm,hernandez2023explaining}.

\vskip 2mm
For each query, outputs from Q-STRUM and its baselines are compared pairwise across these criteria. The LLM determines a winner for each criterion, or declares a tie, and the results are aggregated into Win Rates. To enhance evaluation quality, the LLM is prompted to justify its decisions, as reasoning summaries are shown to improve consistency \cite{zhang2022automaticchainthoughtprompting}. The prompt used for this evaluation is provided in Appendix \ref{sec:prompts}.

This approach ensures reliable, consistent, and scalable evaluation of all variations of Q-STRUM in a manner aligned with established standards for evaluating both summary and explanation quality~\cite{castelnovo2023evaluativeitemcontrastiveexplanationsrankings,miller2018explanationartificialintelligenceinsights}.

% \subsection{RQ2: User Study}

% To validate the effectiveness of our debate methodology from a human perspective, we conducted a user study designed to complement the LLM evaluation. The study focused on the Restaurants dataset, selecting 10 random aspects from the overall set of 78 aspects available (26 queries with 3 aspects each). Participants were shown a query, a specific aspect, and two explanations. To ensure fairness, the order in which the debate and baseline explanations appeared as A or B was randomized for each participant, as was the order of the aspects presented.

% A total of 10 participants took part, each evaluating the same 10 aspects. Participants used the same evaluation criteria definitions as in the LLM evaluation. They selected the explanation they found superior for each criterion or declared a tie if neither explanation was distinctly better. This mirrored the methodology used for LLM evaluation, enabling a direct comparison between human and LLM judgments. Only consistent LLM wins across both directional evaluations (debate as A vs. baseline as B and vice versa) were counted.

% Qualitative feedback was gathered through an open-ended question to understand user sentiment and identify patterns influencing preferences. Detailed methodology and feedback analysis are included in the Appendix.

% Scott: Can you provide the prompts and comparative examples of standard, nice, aggressive responses for the same debate in the Appendix and reference it?
\subsection{RQ2: Aggressiveness Analysis}

This analysis examined whether varying the tone and assertiveness of debate-style prompting impacts summarization quality. Three prompt variations were tested: `nice,' `aggressive,' and the standard neutral version. The `nice' prompt instructed: ``Alice and Bob should both be nice and polite to each other.'' The `aggressive' prompt instructed: ``Alice and Bob should both be aggressive and assertive with each other.'' All other data and prompt inputs remained consistent across variations.

The standard Q-STRUM Debate was compared to Q-STRUM Debate with the modified `aggressive' and `nice' prompt versions defined above.
Evaluation followed the same pairwise Win Rate comparison over contrast, relevancy, diversity, and usefulness as in Section \ref{sec:evalmetrics}. 
%Each aggressiveness level was again compared across the criteria of contrast, relevancy, diversity, and usefulness with the only difference due to  aggressiveness level. 
Bidirectional Win Rate evaluation mitigated potential bias from aggressiveness and comparison-order interactions.

\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
           & \multicolumn{2}{c}{Restaurants}                & \multicolumn{2}{c}{Hotels}                     & \multicolumn{2}{c}{TravelDest}                 \\ \cline{2-7} 
Criterion  & Debate vs. Contrastive & Debate vs. Base       & Debate vs. Contrastive & Debate vs. Base       & Debate vs. Contrastive & Debate vs. Base       \\ \hline
Contrast   & 0.85 {[}0.78, 0.91{]}  & 0.87 {[}0.81, 0.93{]} & 0.82 {[}0.75, 0.88{]}  & 0.82 {[}0.75, 0.90{]} & 0.64 {[}0.58, 0.71{]}  & 0.78 {[}0.73, 0.84{]} \\
Relevance  & 0.57 {[}0.51, 0.63{]}  & 0.57 {[}0.51, 0.62{]} & 0.62 {[}0.55, 0.70{]}  & 0.59 {[}0.52, 0.66{]} & 0.50 {[}0.46, 0.54{]}  & 0.56 {[}0.51, 0.60{]} \\
Diversity  & 0.83 {[}0.76, 0.91{]}  & 0.84 {[}0.77, 0.91{]} & 0.80 {[}0.72, 0.88{]}  & 0.86 {[}0.79, 0.92{]} & 0.54 {[}0.48, 0.61{]}  & 0.69 {[}0.63, 0.75{]} \\
Usefulness & 0.83 {[}0.76, 0.90{]}  & 0.89 {[}0.83, 0.95{]} & 0.78 {[}0.70, 0.86{]}  & 0.84 {[}0.77, 0.91{]} & 0.61 {[}0.54, 0.68{]}  & 0.72 {[}0.66, 0.78{]} \\ \hline
\end{tabular}
}%
\caption{Pairwise LLM Win Rate (95\% CIs in $[\cdot,\cdot]$) for  Q-STRUM Debate vs. Q-STRUM Baselines (Contrastive, Base) across the Restaurants, Hotels, and TravelDest datasets using GPT-4o.}
\label{tab:debate-merged-4o}
\end{table*}

\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
           & \multicolumn{2}{c}{Restaurants}                & \multicolumn{2}{c}{Hotels}                     & \multicolumn{2}{c}{TravelDest}                 \\ \cline{2-7} 
Criterion  & Debate vs. Contrastive & Debate vs. Base       & Debate vs. Contrastive & Debate vs. Base       & Debate vs. Contrastive & Debate vs. Base       \\ \hline
Contrast   & 0.79 {[}0.71, 0.87{]}  & 0.87 {[}0.81, 0.92{]} & 0.77 {[}0.69, 0.84{]}  & 0.80 {[}0.74, 0.87{]} & 0.70 {[}0.64, 0.75{]}  & 0.75 {[}0.70, 0.79{]} \\
Relevance  & 0.63 {[}0.56, 0.69{]}  & 0.58 {[}0.53, 0.63{]} & 0.62 {[}0.56, 0.69{]}  & 0.64 {[}0.57, 0.71{]} & 0.50 {[}0.46, 0.54{]}  & 0.58 {[}0.54, 0.62{]} \\
Diversity  & 0.75 {[}0.67, 0.84{]}  & 0.84 {[}0.78, 0.91{]} & 0.77 {[}0.69, 0.84{]}  & 0.81 {[}0.74, 0.88{]} & 0.48 {[}0.42, 0.54{]}  & 0.53 {[}0.47, 0.59{]} \\
Usefulness & 0.77 {[}0.70, 0.85{]}  & 0.85 {[}0.78, 0.92{]} & 0.79 {[}0.71, 0.86{]}  & 0.84 {[}0.77, 0.91{]} & 0.54 {[}0.47, 0.60{]}  & 0.63 {[}0.57, 0.69{]} \\ \hline
\end{tabular}
}%
\caption{Pairwise LLM Win Rate (95\% CIs in $[\cdot,\cdot]$) for Q-STRUM Debate vs. Q-STRUM Baselines (Contrastive, Base) across the Restaurants, Hotels, and TravelDest datasets using Claude-3.5-Sonnet.}
\label{tab:debate-merged-claude}
\end{table*}

\begin{table*}[!h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Restaurants}               & \multicolumn{2}{c}{Hotels}                    & \multicolumn{2}{c}{TravelDest}                \\
Criterion            & Aggressive            & Nice                  & Aggressive            & Nice                  & Aggressive            & Nice                  \\ \hline
Contrast             & 0.57 {[}0.48, 0.66{]} & 0.56 {[}0.46, 0.65{]} & 0.48 {[}0.33, 0.63{]} & 0.57 {[}0.49, 0.65{]} & 0.53 {[}0.47, 0.59{]} & 0.47 {[}0.41, 0.53{]} \\
Relevance            & 0.53 {[}0.48, 0.58{]} & 0.51 {[}0.45, 0.56{]} & 0.46 {[}0.37, 0.56{]} & 0.56 {[}0.49, 0.63{]} & 0.50 {[}0.46, 0.53{]} & 0.50 {[}0.47, 0.53{]} \\
Diversity            & 0.54 {[}0.45, 0.63{]} & 0.57 {[}0.48, 0.66{]} & 0.41 {[}0.26, 0.55{]} & 0.55 {[}0.46, 0.63{]} & 0.53 {[}0.47, 0.59{]} & 0.47 {[}0.41, 0.53{]} \\
Usefulness           & 0.58 {[}0.49, 0.67{]} & 0.58 {[}0.48, 0.68{]} & 0.43 {[}0.27, 0.58{]} & 0.57 {[}0.48, 0.65{]} & 0.54 {[}0.47, 0.60{]} & 0.45 {[}0.39, 0.51{]} \\ \hline
\end{tabular}
}
\caption{Pairwise LLM Win Rate (95\% CIs in $[\cdot,\cdot]$) for Q-STRUM Debate (Standard) vs. Q-STRUM Debate (Agressive and Nice) across the Restaurants, Hotels, and TravelDest datasets using GPT-4o.}
\label{tab:aggro-combined}
\end{table*}

\begin{table*}[!h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Restaurants}               & \multicolumn{2}{c}{Hotels}                    & \multicolumn{2}{c}{TravelDest}                \\
Criterion            & Aggressive            & Nice                  & Aggressive            & Nice                  & Aggressive            & Nice                  \\ \hline
Contrast             & 0.55 {[}0.46, 0.65{]} & 0.58 {[}0.49, 0.67{]} & 0.50 {[}0.41, 0.59{]} & 0.58 {[}0.49, 0.67{]} & 0.53 {[}0.46, 0.59{]} & 0.54 {[}0.47, 0.61{]} \\
Relevance            & 0.52 {[}0.46, 0.57{]} & 0.55 {[}0.49, 0.60{]} & 0.47 {[}0.41, 0.53{]} & 0.55 {[}0.49, 0.62{]} & 0.50 {[}0.46, 0.53{]} & 0.50 {[}0.46, 0.53{]} \\
Diversity            & 0.55 {[}0.47, 0.64{]} & 0.49 {[}0.39, 0.59{]} & 0.47 {[}0.39, 0.55{]} & 0.59 {[}0.50, 0.68{]} & 0.58 {[}0.52, 0.64{]} & 0.53 {[}0.47, 0.59{]} \\
Usefulness           & 0.53 {[}0.43, 0.63{]} & 0.56 {[}0.46, 0.65{]} & 0.54 {[}0.45, 0.63{]} & 0.62 {[}0.52, 0.72{]} & 0.57 {[}0.51, 0.63{]} & 0.54 {[}0.48, 0.60{]} \\ \hline
\end{tabular}
}
\caption{Pairwise LLM Win Rate (95\% CIs in $[\cdot,\cdot]$) for Q-STRUM Debate (Standard) vs. Q-STRUM Debate (Agressive and Nice) across the Restaurants, Hotels, and TravelDest datasets using Claude-3.5-Sonnet.}
\label{tab:aggro-combined-claude}
\end{table*}

\section{Experimental Results}

%The results of the pairwise evaluation, user study, and aggressiveness analysis collectively demonstrate the efficacy of debate-style prompting in enhancing the quality of contrastive explanations. 
Below, we summarize key findings for our previous research questions.  Experiments used $\sim$11M tokens of GPT-4o (est. $\sim$200B parameters~\cite{abacha2025medecbenchmarkmedicalerror}) API calls and $\sim$3M tokens of Claude-3.5-Sonnet (175B parameters) API calls. 
 All code and data to reproduce these results are provided in an anonymized code repository.\footnote{\url{https://github.com/D3Mlab/q-strum-debate}}  

\subsection{RQ1: Pairwise Win Rate Evaluation}

Across all datasets and criteria, Q-STRUM Debate outperformed Q-STRUM Base. For the subjective review datasets, Restaurants and Hotels, Debate achieved Win Rates mostly at or above 80\% for contrast, diversity, and usefulness, as shown in Table \ref{tab:debate-merged-4o}. However, the relevance criterion, while still favoring Debate, had lower Win Rates, at or above 57\%, however all confidence intervals were still above 50\%. 
The results for Claude-3.5-Sonnet in Table \ref{tab:debate-merged-claude} are similar.
Overall, these results indicate that while both debate and baseline summaries are relevant, Q-STRUM Debate generally excels in offering greater contrast, diversity, and usefulness. 

For the objective TravelDest dataset, Q-STRUM Debate demonstrated a narrower margin of superiority. The same tables highlight that Q-STRUM Debate (significantly) outperformed Q-STRUM Base in almost all criteria, but achieved mixed results against Q-STRUM Contrastive.  
%In this setting, Contrastive prompting alone (whether a direct prompt or debate-style) 
This suggests that Q-STRUM Debate prompting may be more effective for the \emph{subjective, opinion-rich} TripAdvisor review data in Restaurants and Hotels than the \emph{objective, fact-oriented} WikiVoyage data of TravelDest.

% These tables could all be merged
% \begin{table*}[!ht]
% \centering
% \resizebox{\textwidth}{!}{%
%   \begin{tabular}{lcccccc}
%   \hline
%    & \multicolumn{2}{c}{Restaurants} & \multicolumn{2}{c}{Hotels} & \multicolumn{2}{c}{TravelDest}\\\cline{2-7}
%   Criterion & Debate vs. Contrastive & Debate vs. Simple & Debate vs. Contrastive & Debate vs. Simple & Debate vs. Contrastive & Debate vs. Simple \\\hline
%   Contrast   & 0.86 [0.80, 0.91] & 0.88 [0.83, 0.93] & 0.84 [0.77, 0.90] & 0.83 [0.77, 0.89] & 0.64 [0.58, 0.70] & 0.79 [0.74, 0.84] \\
%   Relevance  & 0.58 [0.52, 0.64] & 0.58 [0.52, 0.64] & 0.59 [0.52, 0.67] & 0.58 [0.51, 0.66] & 0.51 [0.46, 0.56] & 0.57 [0.52, 0.61] \\
%   Diversity  & 0.83 [0.77, 0.90] & 0.85 [0.79, 0.91] & 0.80 [0.72, 0.87] & 0.88 [0.82, 0.94] & 0.55 [0.48, 0.61] & 0.70 [0.64, 0.75] \\
%   Usefulness & 0.84 [0.78, 0.90] & 0.90 [0.85, 0.95] & 0.77 [0.70, 0.85] & 0.85 [0.79, 0.91] & 0.61 [0.55, 0.67] & 0.73 [0.67, 0.78] \\
%   \hline
%   \end{tabular}
% }%
% \caption{Pairwise LLM Win Rate for Debate vs. STRUM-LLM-like Baselines across the Restaurants, Hotels, and TravelDest datasets.}
% \label{tab:debate-merged}
% \end{table*}


% add claude table too



% \begin{table*}[!ht]
%     \small
%     \centering
%     \begin{tabular}{lcc}
%     \hline
%     Criterion &
%      Debate vs. Contrastive Summarizer &
%       Debate vs. Simple Summarizer \\ \hline
%     Contrast   & \textbf{0.86} {[}0.80, 0.91{]} & \textbf{0.88} {[}0.83, 0.93{]} \\
%     Relevance  & \textbf{0.58} {[}0.52, 0.64{]} & \textbf{0.58} {[}0.52, 0.64{]} \\
%     Diversity  & \textbf{0.83} {[}0.77, 0.90{]} & \textbf{0.85} {[}0.79, 0.91{]} \\
%     Usefulness & \textbf{0.84} {[}0.78, 0.90{]} & \textbf{0.90} {[}0.85, 0.95{]} \\ \hline
%     \end{tabular}
%     \caption{Pairwise LLM Win Rate for Debate vs. STRUM-LLM-like Baselines for Restaurants Dataset.}
%     \label{tab:debate-restos}
% \end{table*}

% \begin{table*}[!ht]
%     \small
%     \centering
%     \begin{tabular}{lcc}
%     \hline
%     Criterion &
%       Debate vs. Contrastive Summarizer &
%       Debate vs. Simple Summarizer \\ \hline
%     Contrast   & \textbf{0.84} {[}0.77, 0.90{]} & \textbf{0.83} {[}0.77, 0.89{]} \\
%     Relevance  & \textbf{0.59} {[}0.52, 0.67{]} & \textbf{0.58} {[}0.51, 0.66{]} \\
%     Diversity  & \textbf{0.80} {[}0.72, 0.87{]} & \textbf{0.88} {[}0.82, 0.94{]} \\
%     Usefulness & \textbf{0.77} {[}0.70, 0.85{]} & \textbf{0.85} {[}0.79, 0.91{]} \\ \hline
%     \end{tabular}
%     \caption{Pairwise LLM Win Rate for Debate vs. STRUM-LLM-like Baselines for Hotels Dataset.}
%     \label{tab:debate-hotels}
% \end{table*}

% \begin{table*}[!ht]
%     \small
%     \centering
%     \begin{tabular}{lcc}
%     \hline
%     Criterion &
%       Debate vs. Contrastive Summarizer &
%       Debate vs. Simple Summarizer \\ \hline
%     Contrast   & \textbf{0.64} {[}0.58, 0.70{]} & \textbf{0.79} {[}0.74, 0.84{]} \\
%     Relevance  & \textbf{0.51} {[}0.46, 0.56{]} & \textbf{0.57} {[}0.52, 0.61{]} \\
%     Diversity  & \textbf{0.55} {[}0.48, 0.61{]} & \textbf{0.70} {[}0.64, 0.75{]} \\
%     Usefulness & \textbf{0.61} {[}0.55, 0.67{]} & \textbf{0.73} {[}0.67, 0.78{]} \\ \hline
%     \end{tabular}
%     \caption{Pairwise LLM Win Rate for Debate vs. STRUM-LLM-like Baselines for TravelDest Dataset.}
%     \label{tab:debate-traveldests}
% \end{table*}

% \subsection{RQ2: User Study}


% To validate the effectiveness of debate-style prompting and its alignment with human evaluations, we conducted a user study utilizing Cohen's Kappa to measure inter-rater reliability \cite{mchugh2012interrater}. The agreement was assessed in four ways:

% % Scott: Move evaluation metrics to Experimental Design
% \begin{enumerate}
%     \item \textbf{Agreement Among Human Evaluators:} This metric averaged the consistency in judgments across the 10 human evaluators for pairwise comparisons. Formally, the agreement between two evaluators \(i\) and \(j\) (\(i \neq j\)) is given by \(\kappa_{i,j}\), and the average agreement for evaluator \(i\) with others is defined as:
%     \[
%     \theta_i = \frac{\sum_{j \neq i} \kappa_{i,j}}{n - 1},
%     \]
%     where \(n = 10\) is the total number of evaluators. The overall average agreement is then:
%     \[
%     \kappa_{\text{ua}} = \frac{\sum_{i=1}^n \theta_i}{n}.
%     \]
%     \item \textbf{LLM Agreement with Human Evaluators:} This measured how well the LLM aligned with individual human preferences.
%     \item \textbf{Human Agreement with the Majority Vote:} Evaluated the extent to which individual human judgments matched the majority decision (at least six out of ten evaluators).
%     \item \textbf{LLM Agreement with the Majority Vote:} Gauged the LLM's ability to reflect consensus-based human decisions.
% \end{enumerate}

% \begin{enumerate}
%     \item \textbf{Agreement Among Human Participants (Average):} This metric measured the consistency in judgments among the 10 human evaluators. It allowed us to understand how often humans independently arrived at the same conclusion for a pairwise comparison. Formally, the agreement between two evaluators \(i\) and \(j\) (\(i \neq j\)) is given by \(\kappa_{i,j}\), and the average agreement for evaluator \(i\) with others is defined as:
%     \[
%     \theta_i = \frac{\sum_{j \neq i} \kappa_{i,j}}{n - 1},
%     \]
%     where \(n = 10\) is the total number of evaluators. The overall average agreement is then:
%     \[
%     \kappa_{\text{ua}} = \frac{\sum_{i=1}^n \theta_i}{n}.
%     \]

%     \item \textbf{LLM Agreement with Human Evaluators (Average):} The average agreement between the LLM and individual human evaluators was computed to gauge how closely the LLM aligned with human preferences. This calculation mirrors the method used for human-to-human agreement.

%     \item \textbf{Human Agreement with the Majority Vote (Average):} This metric assessed how often individual human evaluators agreed with the majority decision. The majority vote required at least six out of ten evaluators to select the same winner for a comparison. If no majority was achieved, the result was considered a tie.

%     \item \textbf{LLM Agreement with the Majority Vote:} Lastly, we measured the agreement between the LLM's evaluations and the majority human vote to assess how well the LLM reflected consensus-based human decisions.
% \end{enumerate}

% The summary of these metrics is presented in Table \ref{tab:kappa-summary}. Key observations include:

% \begin{itemize}
%     \item For most criteria, the LLM agreement with human evaluators was slightly lower than average human-to-human agreement, though the values were close. This suggests that the LLM is capable of aligning with human judgments, albeit not perfectly.
%     \item The LLM's agreement with the majority vote often exceeded human-to-human agreement. This highlights the consistency and reliability of LLM-based evaluations, particularly when compared against a consensus-based human decision.
%     \item Among all criteria, diversity showed the highest LLM agreement with the majority vote (\(0.411765\)), surpassing human agreement with the majority for this criterion. This indicates the LLM's strength in identifying and rewarding diverse explanations.
% \end{itemize}

% Scott: compress key findings into one or two main points about whether LLM judgment aligns with human judgment to directly answer RQ and validate results from RQ1.
% Key findings from Table \ref{tab:kappa-summary} include:

% \begin{itemize}
%     \item LLM-human agreement was close to the average human-to-human agreement, showing LLMs can align reasonably well with human judgments.
%     \item The LLM often surpassed human agreement with the majority vote, demonstrating its consistency and reliability.
%     \item For diversity, the LLM achieved the highest agreement with the majority vote (\(0.411765\)), even exceeding human agreement, highlighting its strength in recognizing and rewarding diversity.
% \end{itemize}

% Scott: pairwise comparison with other LLM(s), remove user study discussion throughout paper
%  remove user study altogether, run on claude instead
% \begin{table*}[!h]
% \centering
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{lccccc}
% \toprule
% Criterion & HH (Cohen) & LH (Cohen) & HMV (Cohen) & LMV (Cohen) & H (Fleiss) \\
% \midrule
% Contrast   & \textbf{0.087584} & 0.06483  & \textbf{0.24545}  & 0.210526 & 0.067246 \\
% Relevance  & 0.143305 & \textbf{0.217614} & \textbf{0.399715} & 0.310345 & 0.125931 \\
% Diversity  & 0.112613 & \textbf{0.215235} & 0.23009  & \textbf{0.411765} & 0.104792 \\
% Usefulness & 0.132112 & \textbf{0.206982} & \textbf{0.281494} & 0.268293 & 0.094024 \\
% \bottomrule
% \end{tabular}%
% % }
% \caption{Combined Cohen’s and Fleiss’ Kappa Statistics. 
% HH = Human-Human (Cohen), LH = LLM-Human (Cohen), 
% HMV = Human-Majority Vote (Cohen), 
% LMV = LLM-Majority Vote (Cohen), 
% H = Human (Fleiss).}
% \label{tab:combined-kappa}
% \end{table*}

% \begin{table*}[!h]
% \centering
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{lccc}
% \hline
% Criterion  & Restaurants & Hotels & TravelDest \\ \hline
% Contrast   & 0.262       & 0.296  & 0.207      \\
% Relevance  & 0.080       & 0.255  & 0.152      \\
% Diversity  & 0.212       & 0.128  & 0.222      \\
% Usefulness & 0.188       & 0.274  & 0.292      \\ \hline
% \end{tabular}
% % }
% \caption{Cohen's Kappa Scores for GPT-4o with Claude-3.5-Sonnet}
% \label{tab:combined-kappa}
% \end{table*}

% \begin{table*}[!h]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lcccccc}
% \hline
% \multicolumn{1}{c}{Criterion} & \multicolumn{2}{c}{Hotels} & \multicolumn{2}{c}{Restaurants} & \multicolumn{2}{c}{TravelDest}\\
% & Aggressive & Nice & Aggressive & Nice & Aggressive & Nice \\
% \hline
% Contrast & 0.44 [0.32, 0.57] & \textbf{0.57} [0.49, 0.64] & \textbf{0.58} [0.50, 0.66] & \textbf{0.56} [0.47, 0.64] & \textbf{0.54} [0.50, 0.57] & \textbf{0.51} [0.47, 0.55] \\
% Relevance & 0.46 [0.36, 0.56] & \textbf{0.56} [0.49, 0.62] & \textbf{0.55} [0.49, 0.60] & \textbf{0.56} [0.49, 0.62] & \textbf{0.51} [0.48, 0.55] & 0.50 [0.46, 0.54] \\
% Diversity & 0.42 [0.28, 0.55] & \textbf{0.54} [0.46, 0.62] & \textbf{0.53} [0.44, 0.61] & \textbf{0.56} [0.48, 0.65] & \textbf{0.52} [0.46, 0.58] & \textbf{0.51} [0.46, 0.57] \\
% Usefulness & 0.43 [0.29, 0.56] & \textbf{0.55} [0.47, 0.63] & \textbf{0.59} [0.50, 0.67] & \textbf{0.56} [0.48, 0.64] & \textbf{0.54} [0.49, 0.60] & \textbf{0.51} [0.46, 0.56] \\
% \hline
% \end{tabular}
% }
% \caption{Pairwise LLM Win Rate for Standard Debate vs. Varying Levels of Aggression across the Restaurants, Hotels, and TravelDest datasets.}
% \label{tab:aggro-combined}
% \end{table*}




% \begin{table*}[ht]
%     \centering
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{@{}lcccc@{}}
%     \toprule
%     Criterion &
%       \begin{tabular}[c]{@{}c@{}}Agreement Among \\ Human Evaluators (Average)\end{tabular} &
%       \begin{tabular}[c]{@{}c@{}}LLM Agreement with \\ Human Evaluators (Average)\end{tabular} &
%       \begin{tabular}[c]{@{}c@{}}Agreement of Human \\ Evaluators with Majority \\ Vote (Average)\end{tabular} &
%       \begin{tabular}[c]{@{}c@{}}LLM Agreement with \\ the Majority Vote\end{tabular} \\ \midrule
%     Contrast   & \textbf{0.087584} & 0.06483           & \textbf{0.24545}  & 0.210526          \\
%     Relevance  & 0.143305          & \textbf{0.217614} & \textbf{0.399715} & 0.310345          \\
%     Diversity  & 0.112613          & \textbf{0.215235} & 0.23009           & \textbf{0.411765} \\
%     Usefulness & 0.132112          & \textbf{0.206982} & \textbf{0.281494} & 0.268293          \\ \bottomrule
%     \end{tabular}%
%     }
%     \caption{Summary of Cohen's Kappa Statistics for LLM \& Human Agreement}
%     \label{tab:kappa-summary}
% \end{table*}

% Additionally, Fleiss' Kappa, calculated among all human evaluators, revealed relatively low agreement levels (Table \ref{tab:fleiss-human}), indicating significant variability in individual human judgments. Interestingly, the LLM agreement with humans and the majority vote was on par with, or even exceeded, the average human agreement for certain criteria. These results affirm the robustness of LLM-based evaluations.

% Fleiss' Kappa, summarized in Table \ref{tab:fleiss-human}, showed low agreement among human evaluators, reflecting significant variability in human judgments. Despite this, the LLM's performance was robust, matching or exceeding average human agreement for most criteria.

% Scott: merge with Cohen's Kappa, all column headings now have to say Cohen or Fleiss, Cohen's column labels should reduced in length / abbreviated to fit
% \begin{table}[ht]
%     \small
%     \centering
%     \begin{tabular}{@{}lc@{}}
%     \toprule
%     Criterion & \begin{tabular}[c]{@{}c@{}}Fleiss' Kappa for \\ Human Evaluators\end{tabular} \\ \midrule
%     Contrast   & 0.067246 \\
%     Relevance  & 0.125931 \\
%     Diversity  & 0.104792 \\
%     Usefulness & 0.094024 \\ \bottomrule
%     \end{tabular}%
%     \caption{Fleiss' Kappa for Human Participants}
%     \label{tab:fleiss-human}
% \end{table}


% Participants provided qualitative feedback highlighting a preference for longer, detailed explanations with specific descriptors and examples. They valued diversity and contrast but criticized overly repetitive points. Clear presentation of pros and cons was especially appreciated, aligning with the evaluation criteria. These insights emphasize the need for explanations that are detailed, relevant, and contrastive, further supporting the reliability of debate-style prompting in meeting user expectations.

%% Scott: nice, but not quantative (by definition) so reviewers can complain about how we summarized this feedback without adding our own bias.
% Participants' qualitative feedback emphasized a preference for longer, detailed explanations with clear contrast and specific examples. They appreciated diversity and clarity in presenting pros and cons but criticized repetitive points. These findings reinforce the value of debate-style prompting in generating detailed, relevant, and contrastive explanations, aligning well with user expectations.


\subsection{RQ2: Aggressiveness Analysis}
\label{sec:rq2}

The aggressiveness analysis compared `standard', `aggressive', and `nice' debate prompts. As shown in Tables \ref{tab:aggro-combined} and \ref{tab:aggro-combined-claude}, the standard prompt generally performed best across datasets. The aggressive prompt showed marginal improvements in specific contexts, such as the Hotels dataset, but did not demonstrate consistent advantages elsewhere. 
%These results suggest that while assertiveness can enhance explanation quality in some scenarios, overly aggressive or polite tones may detract from overall effectiveness.
% Scott: Claim -- 

We conjecture that while niceness vs. aggressiveness does affect the subjective debate style and verbiage, this does not ultimately affect the core objective content being contrasted (as one can verify from the examples in Appendix \ref{sec:aggro-analysis}), hence having minimal impact on the results.
%which we conjecture is the reason why increased aggressiveness did not significantly improve Win Rates.




% \begin{table*}[ht]
%     \centering
%     % \resizebox{0.7\textwidth}{!}{%
%     \begin{tabular}{lcc}
%     \hline
%     \multicolumn{1}{c}{Criterion} &
%       \begin{tabular}[c]{@{}c@{}}Standard vs. \\ Aggressive Debate\end{tabular} &
%       \begin{tabular}[c]{@{}c@{}}Standard vs.\\ Nice Debate\end{tabular} \\ \hline
%     Contrast   & 0.44 {[}0.32, 0.57{]} & \textbf{0.57} {[}0.49, 0.64{]} \\
%     Relevance  & 0.46 {[}0.36, 0.56{]} & \textbf{0.56} {[}0.49, 0.62{]} \\
%     Diversity  & 0.42 {[}0.28, 0.55{]} & \textbf{0.54} {[}0.46, 0.62{]} \\
%     Usefulness & 0.43 {[}0.29, 0.56{]} & \textbf{0.55} {[}0.47, 0.63{]} \\ \hline
%     \end{tabular}%
%     % }
%     \caption{Standard vs. Aggression Variations in Debate-style Prompting on the Hotels Dataset. 95\% Confidence Interval provided next to each value.}
%     \label{tab:aggro-hotel}
% \end{table*}

% \begin{table*}[ht]
%     \centering
%     % \resizebox{0.7\textwidth}{!}{%
%     \begin{tabular}{lcc}
%     \hline
%     \multicolumn{1}{c}{Criterion} &
%       \begin{tabular}[c]{@{}c@{}}Standard vs. \\ Aggressive Debate\end{tabular} &
%       \begin{tabular}[c]{@{}c@{}}Standard vs.\\ Nice Debate\end{tabular} \\ \hline
%     Contrast   & \textbf{0.58} [0.50, 0.66] & \textbf{0.56} [0.47, 0.64] \\
%     Relevance  & \textbf{0.55} [0.49, 0.60] & \textbf{0.56} [0.49, 0.62] \\
%     Diversity  & \textbf{0.53} [0.44, 0.61] & \textbf{0.56} [0.48, 0.65] \\
%     Usefulness & \textbf{0.59} [0.50, 0.67] & \textbf{0.56} [0.48, 0.64] \\ \hline
%     \end{tabular}%
%     % }
%     \caption{Standard vs. Aggression Variations in Debate-style Prompting on the Restaurants Dataset. 95\% Confidence Interval provided next to each value.}
%     \label{tab:aggro-restos}
% \end{table*}

% \begin{table*}[ht]
%     \centering
%     % \resizebox{0.7\textwidth}{!}{%
%     \begin{tabular}{lcc}
%     \hline
%     \multicolumn{1}{c}{Criterion} &
%       \begin{tabular}[c]{@{}c@{}}Standard vs. \\ Aggressive Debate\end{tabular} &
%       \begin{tabular}[c]{@{}c@{}}Standard vs.\\ Nice Debate\end{tabular} \\ \hline
%     Contrast   & \textbf{0.54} [0.50, 0.57] & \textbf{0.51} [0.47, 0.55]
%  \\
%     Relevance  & \textbf{0.51} [0.48, 0.55] & 0.50 [0.46, 0.54] \\
%     Diversity  & \textbf{0.52} [0.46, 0.58] & \textbf{0.51} [0.46, 0.57]
%  \\
%     Usefulness & \textbf{0.54} [0.49, 0.60] & \textbf{0.51} [0.46, 0.56] \\ \hline
%     \end{tabular}%
%     % }
%     \caption{Standard vs. Aggression Variations in Debate-style Prompting on the TravelDest Dataset. 95\% Confidence Interval provided next to each value.}
%     \label{tab:aggro-traveldest}
% \end{table*}

\section{Conclusion}

We introduced a novel debate-style prompting framework called Q-STRUM Debate to generate high-quality, query-driven contrastive summaries using LLMs. We demonstrated that debate-style prompting significantly outperforms baselines, particularly for subjective review datasets, by delivering more contrastive, relevant, diverse, and useful summaries. Experiments modulating debate aggressiveness showed marginal impact on results.
%While the aggressiveness of the debate had context-dependent effects, the standard prompting approach consistently performed well across domains.

Future directions include multiple entity comparisons, incorporating multi-modal review content, and adapting it to diverse domains such as medical products or hiking trails. Further exploration of multi-agent debate methodology  could enhance the framework's versatility. Overall, these extensions can build on Q-STRUM's novel debate prompting methodology to improve contrastiveness in a variety of query-driven summarization applications.
%Overall, these extensions can leverage Q-STRUM-Debate's novel debate prompting methodology to improve contrastive quality in query-driven summarization.
%support 
%nuanced decision-making, paving the way for advancements in NLP-driven summarization.

\section{Limitations}

While our research introduces a robust debate model for comparative analysis, certain limitations remain that present opportunities for refinement. First, the current model is limited to pairwise comparisons of entities, which, although effective, may not fully capture the complexity of real-world decision-making scenarios where users often consider multiple options simultaneously. Additionally, our focus on textual datasets restricts the model's applicability to domains where multi-modal data (e.g., images, videos) play a critical role in user decision-making, such as in product or travel reviews.

While subjective review data enhanced the effectiveness of debates, the interpretation of conflicting subjective opinions remains an open challenge. Understanding how the model reconciles divergent viewpoints is important for improving the depth and fairness of contrastive summaries.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

\bibliography{anthology,custom}

\appendix



\section{Data Preprocessing}
\label{sec:preprocessing}

The preprocessing pipeline was designed to prepare the datasets for effective use in query-driven contrastive summarization (QCS). Key steps included:

\paragraph{TravelDest Dataset} The TravelDest dataset includes detailed WikiVoyage descriptions for 774 global destinations. For preprocessing, we employed elaborative query reformulation (EQR) \cite{wen2024elaborative} to generate rich queries. Entities were ranked for each query using dense retrieval via TAS-B embeddings \cite{hofstatter2021efficiently}. 

\paragraph{Restaurants and Hotels Datasets} For the subjective review datasets, we manually created natural language queries related to common user needs. We then scraped reviews from TripAdvisor for hotels and restaurants in Toronto. Dense retrieval was performed using OpenAI's text-embedding-3-small model\footnote{https://openai.com/index/new-embedding-models-and-api-updates/} to compute cosine similarity scores between queries and the review snippets. 

\paragraph{Snippet Extraction} For all three datasets, we extracted the top-50 relevant snippets for each entity, ensuring balanced representation. For TravelDest, these snippets were sourced from WikiVoyage, whereas for the restaurants and hotels datasets, the snippets were drawn from TripAdvisor reviews.

\paragraph{Scoring and Selection} Entity relevance was determined using the arithmetic mean of the top-50 cosine similarity scores. For each query, the top two entities were selected as candidates for comparison, and their corresponding snippets served as input for the \methodname\ pipeline.

This preprocessing approach ensures consistency, relevance, and high-quality textual inputs for evaluating \methodname's ability to generate contrastive summaries for diverse queries and data. % types.

% \section{User Study}

% To validate the effectiveness of our debate methodology from a human perspective, we conducted a user study designed to complement the LLM evaluation. The primary goal was to compare the summaries generated by the debate methodology against the strongest baseline, \methodname\ Contrastive, through pairwise human evaluations. By involving human participants, we aimed to assess how well the LLM pairwise evaluation aligned with human judgment on the evaluation criteria of contrast, relevancy, diversity, and usefulness.

% The study focused on the Restaurants dataset, selecting 10 random aspects from the overall set of 78 aspects available (26 queries with 3 aspects each). Participants were shown a query, a specific aspect, and two explanations (labeled as Comparison A and Comparison B). To ensure fairness, the order in which the debate and baseline explanations appeared as A or B was randomized for each participant, as was the order of the aspects presented. This randomization mitigates any potential ordering bias.

% A total of 10 participants took part in the study, each evaluating the same 10 aspects. Participants were given the same definitions of the evaluation criteria used in the LLM evaluation to ensure consistency. They were instructed to select the explanation they found superior for each criterion or declare a tie if neither explanation was distinctly better. This mirrored the methodology used for LLM evaluation, enabling a direct comparison between human and LLM judgments. To ensure consistency in comparisons, LLM wins were only counted when both directional evaluations (debate as A versus baseline as B and vice versa) agreed on the same winner; otherwise, they were recorded as ties.

% In addition to quantitative evaluations, participants were asked to provide qualitative feedback on the comparisons through an open-ended question. This feedback allowed us to gather insights on patterns or characteristics that influenced participants’ preferences and to understand user sentiment towards the explanations. The feedback provided further contextualized the results and offered valuable insights into the strengths and weaknesses of the debate methodology from a human perspective.

% By incorporating both structured pairwise evaluations and qualitative feedback, the user study provided a robust and comprehensive assessment of the debate methodology’s performance relative to human judgment.

\section{Aggressiveness Analysis}
\label{sec:aggro-analysis}
Figure \ref{fig:aggro-compare-debate} shows an example comparison between `aggressive', `nice' and standard debate outputs as explored in RQ2 of Section~\ref{sec:rq2}. We see that `aggressive' prompts take a more firm stance in their wording but cover similar content as the standard prompt.  In contrast, the `nice' prompt is noticeably more collegial, but also provides more limited argumentation as evidenced by the shorter length.
%% I think the following is true, but it may invite more investigation and comparison to LLMs that don't use human alignment fine-tuning.
%% TODO: I bet reviewers are going to ask why we only included one example even though this is the Appendix and space is unlimited.  If you can quickly throw two other examples into the table, that would be ideal.
%We conjecture that LLM human alignment fine-tuning principles, while the `nice' prompts are noticeably more collegial.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.92\linewidth]{figs/debate-aggro-compare.pdf}
    \caption{Debate output comparison for various aggressiveness levels}
    \label{fig:aggro-compare-debate}
\end{figure*}

\section{Q-STRUM Prompts}
\label{sec:prompts}
%% TODO: this is quick and dirty, would be good to include proper Listing references and more descriptions
We provide the full prompts required to implement the Q-STRUM pipeline components as follows:
\begin{itemize}
\item Listing~\ref{lst:aspectextract}: LLM Prompt for Aspect Extraction Stage in Figure~\ref{fig:aspect-extract} and Section~\ref{sec:q-strum-base}.
\item Listing~\ref{lst:aspectmerge}: LLM Prompt for Aspect Merge Stage in Figure~\ref{fig:aspect-merge} and Section~\ref{sec:q-strum-base}.
\item Listing~\ref{lst:filter}: LLM Prompt for Filter Stage in Figure~\ref{fig:filter-stage} and Section~\ref{sec:q-strum-base}.
\item Listing~\ref{lst:contrast}: LLM Prompt for Contrastive Summarizer Stage in Figure~\ref{fig:qstrum}(b) and Section~\ref{sec:q-strum-cont}.
\item Listing~\ref{lst:llm-eval}: LLM Prompt for Pairwise Evaluation in Section~\ref{sec:evalmetrics} used for evaluation metrics.
\end{itemize}
The prompts for Q-STRUM-Debate were provided in the main paper in Listings~\ref{lst:debate} and~\ref{lst:debate-json} as discussed in Section~\ref{sec:q-strum-debate}.

\begin{figure*}[!ht]
\begin{lstlisting}[caption={LLM Prompt for Aspect Extraction Stage}, label={lst:aspectextract}]
{{destination}}
{{sentences}}

Query: {{query}}

Given the following destination and numbered texts, generate diverse and elaborative aspect phrases that describe what the user might be looking for according to the intent of the query provided and the information provided for the destination. Use the JSON format provided. 

Requirements:
- The aspect phrase must be elaborate, specific, descriptive and detailed.
- You must include the aspect and list of relevant extracted phrases for the destination for that aspect. 
- You must include a citation in a [#] format for the sentence that supports the aspect phrase from the provided sentences. Follow the same numbering as the provided sentences.
- The values must be entire, long phrases extracted exaclty from the provided sentences. 
- You must include exactly 5 aspects.
- For each aspect, you must include at least 10 extracted phrases and each extracted phrase must be highly relevant to the aspect.
- Prioritize relevancy in the extracted phrases over the number of phrases.



Output format:
{
  "<aspect>": ["extracted phrase [sentence #]", extracted phrase [sentence #]", ...],
  "<aspect>": ["extracted phrase [sentence #]", extracted phrase [sentence #]", ...],
  ...
}
    \end{lstlisting}
    \end{figure*}

\begin{figure*}[ht]
    \begin{lstlisting}[caption={LLM Prompt for Aspect Merge Stage}, label={lst:aspectmerge}]
Destination 1: {{dest1}}
Attributes 1: {{attributes1}}

Destination 2: {{dest2}}
Attributes 2: {{attributes2}}

Query: {{query}}

Merge any similar attributes from the attribute lists for each destination. Return a JSON mapping the old attribute names exactly to the new attribute names. Include the old attribute names from both destinations in the output. Ensure the new attributes are common to both destinations.

Output format:

{
  "{{dest1}}": {
    "oldAttr1": "newAttr1",
    "oldAttr2": "newAttr2",
    ...
  },
  "{{dest2}}": {
    "oldAttr3": "newAttr3",
    "oldAttr4": "newAttr4",
    ...
  }
}
    \end{lstlisting}
\end{figure*}

\begin{figure*}[ht]
    \begin{lstlisting}[caption={LLM Prompt for Filter Stage}, label={lst:filter}]
Destination 1: {{dest1}}
{{attributes1}}

Destination 2: {{dest2}}
{{attributes2}} 

Query: {{query}}

Identify the top 3 most informative attributes. For each attribute, identify exactly 10 of the most informative value pharases. You must have exactly 3 attributes per destination and exactly 10 value phrases per attribute, no exceptions. Both destinations must have the exact same 3 attributes. Follow the JSON output format provided exactly.

Output format:
{
    "{{dest1}}": {
        "<attribute1_placeholder>": ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", ...],
        "<attribute2_placeholder>: ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", ...],
        ...
    },
    "{{dest2}}": {
        "<attribute1_placeholder>": ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", ...],
        "<attribute2_placeholder>: ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]",
        ...
    }
}
    \end{lstlisting}
\end{figure*}



\begin{figure*}[ht]
    \begin{lstlisting}[caption={LLM Prompt for Contrastive Summarizer Stage}, label={lst:contrast}]
Destination 1: {{dest1}}
{{attributes1}}

Destination 2: {{dest2}}
{{attributes2}} 

Query: {{query}}

Identify the most contrasting and important values and return a JSON with these attributes and their values.

Requirements are as follows:
- You must return exactly 3 attributes for each destination.
- Each attribute must have exactly 3 bullet points, summarizing both the positives and negatives of the destination for that attribute.
- Each bullet point must be relevant to the attribute and must be supported by a citation.
- The attributes should be identical for both destinations.
- Do not include meaningless attributes like null or N/A.

Output format:
{
    "{{dest1}}": {
        "<attribute1_placeholder>": ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", "<value phrase 3> [<citation>]"],
        "<attribute2_placeholder>: ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", "<value phrase 3> [<citation>]"],
        "<attribute3_placeholder>: ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", "<value phrase 3> [<citation>]"]
    },
    "{{dest2}}": {
        "<attribute1_placeholder>": ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", "<value phrase 3> [<citation>]"],
        "<attribute2_placeholder>: ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", "<value phrase 3> [<citation>]"],
        "<attribute3_placeholder>: ["<value phrase 1> [<citation>]", "<value phrase 2> [<citation>]", "<value phrase 3> [<citation>]"]
    }
}
    \end{lstlisting}    
\end{figure*}

\begin{figure*}[ht]
    \begin{lstlisting}[caption={LLM Prompt for Pairwise Evaluation}, label={lst:llm-eval}]
Query: {{query}}

Explanation A:
{{a}}

Explanation B:
{{b}}

Your role is to evaluate Explanation A and Explanation B as being good contrastive explanations for {{domain}} recommendation. The provided criteria should be used and you should select either "A" or "B" as the winner for each criterion or "tie" if both explanations are the same. You should provide explanations for each of your choices.

Criteria:
contrast - The summarizations should differentiate between the two {{domain}}s well, such as by including pros and cons and details, and help a user choose one {{domain}} instead of the other. 
relevancy - The summarizations provided should be relevant to each aspect and query provided.
diversity - The summarizations should provide multiple different points in support and against the {{domain}} for each aspect. Repetitive points should be penalized and a variety of different points should be rewarded. Additional context that is not repetitive should be rewarded.
usefulness - The summarizations should provide useful information and be informative for a user to make a decision between the two {{domain}}s.

Output in JSON format:
{
    "contrast": "A" or "B" or "tie",
    "contrast_explanation": <explanation>,
    "relevancy": "A" or "B" or "tie",
    "relevancy_explanation": <explanation>,
    "diversity": "A" or "B" or "tie",
    "diversity_explanation": <explanation>,
    "usefulness": "A" or "B" or "tie",
    "usefulness_explanation": <explanation>
}
    \end{lstlisting}    
\end{figure*}

\end{document}
