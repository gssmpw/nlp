% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[ruled,vlined]{algorithm2e}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{amsmath} % for \boldsymbol
\usepackage{bm}      % for \bm
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% add by yf
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots} % 画折线图
\usepackage{scalefnt} %用于设置字体
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor}         % colors
\usepackage{spverbatim}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{FastMCTS: A Simple Sampling Strategy for Data Synthesis}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
 \textbf{Peiji Li\textsuperscript{1,2}\thanks{Equal contribution}},
 \textbf{Kai Lv\textsuperscript{1,2}$^\ast$},
 \textbf{Yunfan Shao\textsuperscript{1,2}},
 \textbf{Yichuan Ma\textsuperscript{1,2}},
\\
 \textbf{Linyang Li\textsuperscript{2}},
 \textbf{Xiaoqing Zheng\textsuperscript{1}},
 \textbf{Xipeng Qiu\textsuperscript{1}},
 \textbf{Qipeng Guo\textsuperscript{2}\thanks{Corresponding Author}},
\\
 \textsuperscript{1}School of Computer Science, Fudan University, Shanghai,
\\
 \textsuperscript{2}Shanghai AI Laboratory, Shanghai,
\\
 \small{
  \href{pjli24@m.fudan.edu.cn}{pjli24@m.fudan.edu.cn},
  % \href{lilinyang@pjlab.org.cn}{lilinyang@pjlab.org.cn},
  \href{lilinyang@pjlab.org.cn}{\{lilinyang, guoqipeng\}@pjlab.org.cn},
 }
}
% \footnotetext[1]{Equal Contribution}
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30\% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9\% across multiple benchmarks. As a lightweight sampling strategy,  FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data. Our code will be released soon.

\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{intro_fig_cn.pdf}
    \caption{Comparison of generation efficiency of three sampling algorithms. "\#Verified Tokens" represents the total tokens in all verified correct trajectories.}

    \label{fig:intro_fig_cn}
\end{figure}



% LLMs are 

% GPT-4~\cite{Achiam2023GPT4TR}

% Reasoning ability is one major 

% importance of RFT 

% current method of major vote 

% MCTS is widely explored 

% success of alpha go

% exclusive feature of LLM different from chess games in NL reasoning 

% reasoning is not a lot of steps, more bfs than dfs
% 12.2 updating
% 结合一些o1 的longcot的 策略进来，强调search policy 的时候用mcts是有优势的，因为longcot的很多信息，信息密度其实是下降了的？ 

% our aim to fast deploy mcts and produce syn data, step level variance 

% in this paper, 

% we aim to make use of all the generated tokens in the autoregressive decoding process of LLMs

% The traditional MCTS process includes four actions: select, expand, simulate, and backpropagate.
% The simulate process is xxx, 
% in the GO process, 
% hundreds of steps 
% In LLM reasoning, specifically, COT process, 
% the reasoning process is step-by-step, 
% despite o1 style models explore long cots
%  most 
% human thought bfs
% error propagation is more common than 

% optimize the expand and simulate process 

% use the gen tokens to build a simulate tree

% revise the UCB score func, to choose whether to expand a brand new branch 

% or explore a leaf node 

% experiments show 

% Large language models (LLMs) have demonstrated exceptional performance on various tasks. Notably, multi-step Chain-of-Thought (CoT) approach has been empirically validated to significantly enhance the reasoning capabilities of language models, both during training and inference phases. Efficiently synthesizing accurate multi-step reasoning trajectories is a significant research area aimed at enhancing the reasoning capabilities of these models. 

% Nevertheless, the acquisition of high-quality CoT reasoning data continues to pose a considerable challenge. Besides relying on human annotations, 
% there are several primary methods for synthesizing multi-step reasoning data using LLMs, including Rejection-Sampling Based Methods or 

% Current methodologies predominantly rely on human annotations or employ Rejection Sampling strategies, wherein language models are utilized to generate synthetic data.

% 这段讲multi step reasoning数据的意义
Large language models (LLMs) have achieved remarkable performance across various domains. 
Reasoning capability plays a crucial role in this success and serves as the foundation for further extending their application scope.
For complex problems, LLMs typically require multi-step reasoning to arrive at final solutions. Synthesizing reasoning trajectories and using them for training has proven to be an effective approach to enhancing their reasoning capabilities.

% 这段讲rejection sampling被用于合成数据时的缺点：采样的独立性以及没有步级别的监督信息。以及引出了MCTS，并指出了MCTS同样不应直接应用于LLM
Currently, rejection sampling~\cite{neal2003slice} is commonly used to synthesize correct trajectories for reasoning tasks. This approach generally involves generating multiple candidate responses through random sampling based on a given problem~\cite{wei2022chain} , and then selecting the correct responses with the corresponding answers as synthetic training data. 
However, this random sampling method handles each attempt independently, constrained by the reasoning capacity of the policy model. As a result, it suffers from inefficiency particularly for long reasoning chains and complex problems, and it fails to provide step-level supervision during the synthesis process.

On the other hand, Monte Carlo Tree Search (MCTS)~\cite{DBLP:conf/cg/Coulom06}, known for its ability to effectively explore state spaces, has been widely adopted in complex tasks such as board games. Some recent studies have also attempted to adapt MCTS for language models. However, the reasoning process of language models differs significantly from those of games like Go or chess. For instance, the state space in language model reasoning is often ill-defined, the computational cost is substantially higher, and the evaluation of reasoning outcomes tends to be more deterministic. As a result, directly applying MCTS to large-scale language generation tasks is less suitable.

% 有待修改


% 这段简单介绍一下我们方法的改进
In this work, we aim to efficiently deploy MCTS for data synthesis. We propose FastMCTS, an MCTS-inspired sample strategy for efficient data synthesis. 
To enhance data synthesis efficiency, we propose a dynamic balance mechanism between exploration and exploitation that adapts to problem complexity.
Specifically, we introduce modifications to the selection phase of MCTS, enabling it to prioritize more valuable nodes rather than being limited to leaf nodes. 
% This search strategy adaptively shifts its focus: when a node has demonstrated successful trajectories in previous simulations, it favors exploration to diversify synthetic reasoning paths; when few or no correct solutions have been discovered through a node, it prioritizes exploitation by exploring proven paths more extensively to identify additional valid solutions.
Furthermore, vanilla MCTS employs a simulation process to evaluate node values. However, conducting complete sampling with LLMs is computationally expensive. To maximize the utility of tokens generated during the autoregressive decoding process of LLMs, we preserve each step of the complete reasoning trajectory generated during simulation as tree nodes, instead of discarding these reasoning steps after simulation. This do not influence the selection of the next most promising node in MCTS but serve as a caching mechanism to prevent redundant generation of reasoning trajectories. Figure \ref{fig:intro_fig_cn} demonstrates the efficiency gains of FastMCTS compared to Rejection Sampling and vanilla MCTS in generating correct trajectory tokens on Chinese high school math data.


% 这段讲了一下实验结果以及我们方法的优越性

Experiments on a wide range of mathematical problems demonstrate the superior data synthesis efficiency of FastMCTS. Compared to vanilla rejection sampling, FastMCTS synthesizes more correct reasoning trajectories, produces more effective tokens, and solves a larger number of problems.
This advantage is particularly pronounced for challenging problems, leading to more balanced synthesis across varying difficulty levels.  Besides, under comparable generation budgets, models trained on FastMCTS-synthesized data outperform those trained on baseline methods across various benchmarks of different complexity.

Further analysis validates the effectiveness of the proposed components and shows that step-level pairwise data constructed through FastMCTS can further boost model performance through methods like step or branch level Direct Preference Optimization. As a lightweight data synthesis strategy, we believe FastMCTS offers a superior alternative to vanilla rejection sampling due to its higher efficiency and ability to provide step-level supervision for multi-step reasoning tasks.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{fastmcts_new.pdf}
    \caption{The overview of one iteration of FastMCTS}

    \label{fig:intro}
\end{figure*}

\section{Related Work}

\paragraph{Synthetic Data for Reasoning Tasks}
% \paragraph{Synthetic Data for Math Reasoning }
Synthetic data has become a key resource for improving the reasoning capabilities of large language models. Several studies~\cite{DBLP:conf/iclr/YuJSYLZKLWL24,DBLP:conf/iclr/XuSZG0FTLJ24} focus on generating new problem sets by rephrasing or augmenting existing training data. Other works~\cite{DBLP:journals/corr/abs-2306-02707,DBLP:journals/corr/abs-2403-04706} leverage strong models, such as GPT-4~\cite{Achiam2023GPT4TR}, to distill high-quality reasoning data, enhancing the reasoning capabilities of smaller models; some of these approaches also utilize code executors to further improve performance~\cite{, DBLP:journals/corr/abs-2309-05653,DBLP:conf/iclr/WangRZLLSZSZ024,DBLP:journals/corr/abs-2402-10176}. Additionally, methods like~\cite{DBLP:conf/acl/WangLSXDLCWS24, DBLP:journals/corr/abs-2406-06592,DBLP:conf/emnlp/WangLWLH0S24} focus on synthesizing multi-step reasoning data and provide step-level supervision without the need for human annotation.

\paragraph{Sampling Strategies for Data Synthesis}

Sampling strategies play a crucial role in enhancing the reasoning and generation capabilities of large language models. Many approaches improve reasoning performance by sampling multiple reasoning paths and selecting the most promising ones. For instance, Self-Consistency~\cite{DBLP:conf/iclr/0002WSLCNCZ23} generates diverse reasoning paths and selects the most consistent answer. Other works~\cite{yuan2023scalingrelationshiplearningmathematical,DBLP:journals/corr/abs-2402-10176,DBLP:journals/corr/abs-2407-13690} use strategies like rejection sampling~\cite{neal2003slice} to generates candidate outputs and filters them based on predefined criteria or a reward model. 
% These methods cannot provide effective step-level supervision.

\paragraph{Tree Search in LLM}
Tree-search strategies have been shown to be highly effective in enhancing the reasoning capacity of large language models, as the nodes of the tree can naturally represent reasoning steps in the chain-of-thought (CoT)~\cite{wei2022chain}. Several studies~\cite{yao2024tree,hao2023reasoning,zhang2024accessing,tian2024toward} have employed tree search during inference to guide multi-step reasoning.
% These methods typically prompt large language models to self-evaluate or use reward models to provide process-level supervision, employing depth-first search, breadth-first search, or Monte-Carlo Tree Search strategies. 
In another stream of research~\cite{feng2023alphazero,DBLP:journals/corr/abs-2405-03553,xie2024monte,zhang2024rest,wang2024towards}, Monte-Carlo Tree Search is used to generate tree-structured data for training, constructing preference data pairs or providing process supervision for CoT steps.

However, in synthetic data scenarios of LLMs, using MCTS can incur significant overhead due to simulation costs or rely on a trained process reward model for step supervision, leading to inefficiencies. To address these limitations, we propose FastMCTS, which efficiently synthesizes tree-structured multi-step reasoning data with high efficiency.
% These tree search-based approaches often face low search efficiency.
% These approaches allow for preference optimization or iterative training of the policy model and value/reward model. 

% However, methods based on Rejection Sampling cannot provide effective step-level supervision, while tree search-based approaches often face low search efficiency and heavily rely on reward models. To address these limitations, we propose Fast-MCTS, a reward-model-free tree search strategy that incorporates Monte Carlo Tree Search (MCTS) principles. Our method synthesizes tree-structured multi-step reasoning data more efficiently than vanilla rejection sampling, without requiring additional reward models.


% 这里是不是要列一下我们的改进与contribution





% \paragraph{Reasoning Path Searching and Learning}

% sc 
% tot
% major vote
% best of n 
% rft 
% dpo 
% step-dpo
% grpo
% mcts dpo etc.




% \paragraph{Monte-Carlo Tree Search}

% alpha zero

% mcts in game 

% mcts in LLM 

% rest mcts etc.



\section{Preliminaries}

\paragraph{Rejection Sampling}

Rejection sampling is a widely used synthetic-data method for obtaining high-quality data to enhance the reasoning capabilities of LLMs. 
% This approach is commonly employed to distill knowledge from stronger models, such as GPT-4. 
Given an input question \( q \), the process involves sampling multiple candidate responses \( \{o^{(j)}\}_{j=1}^N \) from a language model. Each response \( o^{(j)} \) is then evaluated based on predefined criteria, typically by comparing its final answer to a ground-truth solution using a rule-based function. Responses that pass this filtering step are considered correct and used to train the language model.

However, vanilla rejection sampling suffers from several limitations. For instance, the sampled data may exhibit imbalanced distributions~\cite{DBLP:journals/corr/abs-2407-13690}. Moreover, due to the rule-based filtering mechanism, reasoning paths with errors in intermediate steps or those incorrectly discarded due to formatting issues are often excluded~\cite{DBLP:conf/iclr/LightmanKBEBLLS24}. Our work addresses these issues effectively by introducing a more robust sampling strategy while achieving higher efficiency.



\paragraph{Monte Carlo Tree Search} 

% Monte Carlo Tree Search (MCTS) is a decision-making algorithm widely used in games like Go and complex decision processes~\cite{DBLP:journals/nature/SilverHMGSDSAPL16,DBLP:journals/nature/SilverSSAHGHBLB17}. It operates by building a search tree and simulating outcomes to estimate the value of actions. In the context of language models, MCTS serves as a sampling strategy that can be combined with reward models to assist inference or synthesize multi-step reasoning data. By leveraging Monte Carlo estimation, it could provide step-level supervision for reward model training.

% MCTS iteratively builds a search tree through four key phases: selection, expansion, simulation, and backpropagation~\cite{DBLP:journals/tciaig/BrownePWLCRTPSC12}. When applied to LLM inference, the input question \( q \) can be represented as the root node, and each reasoning step in the chain-of-thought (CoT) can be represented as a child node.

% \textbf{Selection} Starting from the root node, MCTS recursively selects child nodes using the Upper Confidence Bound for Trees (UCT) criterion~\cite{DBLP:journals/ml/AuerCF02}:
% \begin{equation}
% \text{UCT}(i) = \frac{w_i}{n_i} + c \cdot \sqrt{\frac{\ln N_i}{n_i}},  
% \label{eq:uct}
% \end{equation}
% where \( n_i \) is the visit count for node \( i \), \( N_i \) is the visit count for its parent, \( w_i \) is the cumulative value (or win count) of all descendant nodes of \( i \), representing the number of times roll-outs from node \( i \) lead to correct answers, and \( c \) is a hyperparameter balancing exploration and exploitation.

% \textbf{Expansion} When a leaf node is reached, the tree is expanded by adding one or more child nodes, representing possible reasoning steps.  

% \textbf{Simulation} From the newly expanded node, a simulation (or "roll-out") is performed using the policy \( \pi_\theta \) to generate a complete reasoning path.  

% \textbf{Backpropagation} The result of the simulation is propagated back through the tree, updating the visit counts and cumulative values of all ancestor nodes.  

% This iterative process enables MCTS to efficiently explore the solution space, identify high-quality reasoning paths, and provide process supervision for reasoning steps through Monte Carlo (MC) evaluation. However, unlike board games such as Go, each roll-out in language models requires autoregressive inference by the model itself, making the simulation process computationally expensive~\cite{DBLP:journals/corr/abs-2405-03553}. As a result, directly applying the MCTS algorithm for data synthesis incurs significant computational overhead.


% new
Monte Carlo Tree Search (MCTS) is a decision-making algorithm widely used in games like Go and complex decision processes~\cite{DBLP:journals/nature/SilverHMGSDSAPL16,DBLP:journals/nature/SilverSSAHGHBLB17}. It builds a search tree through simulations to estimate the value of actions. In the context of language models, MCTS serves as a sampling strategy that can be combined with reward models to assist inference or synthesize multi-step reasoning data, providing step-level supervision for further training.

MCTS iteratively constructs a search tree through four phases: selection, expansion, simulation, and backpropagation~\cite{DBLP:journals/tciaig/BrownePWLCRTPSC12}. When applied to LLM inference, the input question \( q \) is represented as the root node, and each reasoning step in the chain-of-thought (CoT) is represented as a child node. During selection, MCTS uses the Upper Confidence Bound for Trees (UCT) criterion to balance exploration and exploitation:
\begin{equation}
\text{UCT}(i) = \frac{w_i}{n_i} + c \cdot \sqrt{\frac{\ln N_i}{n_i}}  
\label{eq:uct}
\end{equation}
where \( n_i \) is the visit count for node \( i \), \( N_i \) is the visit count for its parent, \( w_i \) is the cumulative value of descendant nodes, and \( c \) is a hyperparameter. 

Unlike board games, each roll-out in language models requires autoregressive inference, making the simulation process computationally expensive~\cite{DBLP:journals/corr/abs-2405-03553}. The results of simulations are often discarded after backpropagation, further reducing sampling efficiency. As a result, directly applying MCTS for data synthesis incurs significant computational overhead.




\section{Method}
% 1. reserve simulation
% 2. stay policy
% 3. dynamic c_puct
% 4. robust: Model Eval + random fewshot




\newcommand{\COMMENTLLAMA}[1]{{\textcolor[HTML]{962C38} {$\triangleright$ {#1}\\}}}
\newcommand{\COMMENTLIGHTGRAY}[1]{\hfill{\textcolor[HTML]{A9A9A9} {$\triangleright$  {#1}\\}}}
\begin{algorithm*}
\caption{Selection phase of FastMCTS}
\label{algo:select}
\KwIn{Current search tree $T$, difficulty thresholds $l_{high}, l_{low}$, UCT constant $c$ }
\KwOut{Selected node in this iteration}
\COMMENTLLAMA{Recursively select node with Adaptive Stay Policy}

% select from root
current\_node $\gets$ root

selected\_node $\gets$ None
    
\While{\textnormal{selected\_node} is None}{

candidate\_children $\gets$ current\_node.children

\If{\textnormal{number of candidate\_children} $<=$ 1 \texttt{or} \COMMENTLIGHTGRAY{\textnormal{Adaptive Stay Policy}}
\quad \textnormal{all candidate\_children are leaf nodes} \texttt{or} \\
\quad current\_node.visit\_count > 1 \texttt{and} current\_node.score $\in (0, l_{\text{low}}] \cup [l_{\text{high}}, 1)$ 
}{
selected\_node $\gets$ current\_node

break
}

\eIf{current\_node.visit\_count > 1}{$c_{current}$ $\gets$ $c \cdot current\_node.score$ \COMMENTLIGHTGRAY{Dynamic Exploration}} 
{$c_{current}$ $\gets$ $c$}


candidate\_node $\gets$ $\arg\max_{node \in candidate\_children} UCT(node,c_{current})$ 

\If{candidate\_node.visit\_count > 1 \texttt{and} candidate\_node.score <= $l_{low}$}{
selected\_node $\gets$ candidate\_node
}

current\_node $\gets$ candidate\_node


}
\end{algorithm*}


% \subsection{Motivation}

% Our work is motivated by several key limitations of existing approaches and the unique characteristics of language model reasoning:

% \paragraph{Dependency on Process Reward Models (PRMs)}

% Previous works applying tree search to LLMs often rely on step-level supervision, either through iterative training of reward/value models or self-evaluation using the LLM itself. Inspired by MATH-SHEPHERD~\cite{DBLP:conf/acl/WangLSXDLCWS24}, we aim to design a lightweight tree-structured data synthesis algorithm that solely relies on Monte Carlo (MC) evaluation for node scoring. Specifically, the score of a node \( i \) is computed as:
% $
%  \text{MC}(i) = \frac{w_i}{n_i},
% $
% where \( w_i \) is the number of successful roll-outs from node \( i \), and \( n_i \) is the total number of visits to node \( i \).

% \paragraph{Preserving Simulation Results}

% Unlike board games such as Go or chess, where the outcome of a random simulation does not necessarily reflect the quality of a specific node, LLM reasoning exhibits a strong correlation between the final answer and the correctness of the entire reasoning path. In most cases, if the final answer is correct, the entire reasoning path is likely to be correct. Therefore, simulation results in LLMs are valuable and should be preserved, rather than discarded as in traditional MCTS. This fundamental difference makes the direct application of vanilla MCTS strategies unsuitable for LLM reasoning tasks.

% \paragraph{Balancing Diversity and Efficiency}

% Traditional MCTS strategies are designed to evaluate the value of different actions, which does not align well with the goals of data synthesis. For example, tree search is unnecessary for problems with high sampling success rates, while for challenging problems, efficiency becomes a critical concern. Our method addresses this by dynamically adjusting the search strategy to balance diversity and efficiency, ensuring optimal resource utilization.


% 1. reserve simulation
% 2. stay policy
% 3. dynamic c_puct
% 4. robust: Model Eval + random fewshot

% \subsection{Fast-MCTS}
\label{method:fastmcts}
% 这段可能去掉或者放在后面，更像是problem formulation
% In our framework for synthetic data generation, we consider an input question as \( q \), and the language model is treated as a policy, denoted as \( \pi_\theta \). 
% Concretely, consider a complete solution consisting of \( T \) reasoning steps. At a given time step \( t \), we represent the partial solution as the state \( s_t \), and the subsequent reasoning step to be taken as the action \( a_{t+1} \). The policy model, implemented by a large language model (LLM), generates the action \( a_t \) based on the current state \( s_t \) and the input question \( q \), i.e.,  
% $
% \pi_\theta(a_{t+1} | s_t, q) = \text{LLM}(a_{t+1} | s_t, q).  
% $  
% The transition from one state to the next is deterministically achieved through the concatenation operation:  
% $
% s_{t+1} = \text{Cat}(s_t, a_t),  
% $  
% where \( s_t = (a_t, a_{t-1}, \dots, a_1) \) represents the sequence of reasoning steps up to time \( t \).
% In our settings, we split the reasoning trajectories to steps by "Step 1", "Step 2" etc.
In our framework for synthetic data generation, for an input question $q$ and its solution with $ T $ reasoning steps, the partial solution at time step $ t $ is represented as state $ s_t $, and the next reasoning step as action $ a_{t+1} $. The language model is treated as a policy model \( \pi_\theta \) and generates actions based on the current state and input question:
\begin{equation}
\pi_\theta(a_{t+1} | s_t) = \text{LLM}(a_{t+1} | s_t)
\end{equation}
The transition to the next state is achieved by concatenating current state and next ction:
\begin{equation}
s_{t+1} = \text{Cat}(s_t, a_{t+1})
\end{equation}
where $ s_t = (a_t, a_{t-1}, \dots, a_1,q) $ represents the sequence of reasoning steps up to time $ t $. We divide the reasoning trajectories into steps based on the strings "Step 1", "Step 2", etc. 

Our proposed method, Fast-MCTS, introduces several key improvements to vanilla MCTS algorithm, tailored for efficient and robust data synthesis in language models. 
In the following, we describe our algorithm in detail.
% the whole algorithm are demonstrated in \ref{algo:fastmcts}.

% For out method, We also define some necessary parameters and notations:

% \begin{itemize}
%     \item \textbf{Iteration Nums}:  number of tree search iterations.
%     \item \textbf{Initial Degree}: number of child nodes at the root.
%     \item \textbf{Expand Degree}: number of child nodes added during expansion.
%     \item \textbf{Node Score}: \( \text{score} = \frac{w\_count}{n\_visit} \) (probability of the model solving the problem correctly at the current state).
%     \item \textbf{Difficulty Thresholds}: we define \( \text{higher\_limit} \) and \( \text{lower\_limit} \). If \( \text{score} < \text{lower\_limit} \), the node is considered unpromising; if \( \text{score} > \text{higher\_limit} \), the node is considered highly confident.
%     \item \textbf{Exploration Threshold}: \( E \). If \( n\_visit > E \), the node is considered fully explored.
% \end{itemize}

% Then we will introduce our Fast-MCTS in detail.

% \paragraph{Selection with Auto Stay Policy}
% % 或者叫Adaptive Selection/Stay? 好听一点
% In the selection phase, Fast-MCTS recursively selects child nodes using the Upper Confidence Bound for Trees (UCT) criterion, as defined in Equation \ref{eq:uct}. To improve efficiency and diversity, we introduce a Auto-stay Policy that dynamically adjusts the selection process based on the node's exploration status and confidence level. During the selection phase, we refer to the node currently under consideration as the "current node," and all its potential child nodes that can be selected as "candidate children." The Auto Stay Policy could be described as follows:

% \begin{itemize}
%     \item If all candidate children are in stopped state(即已经是推理链的最后一步), select current node and terminate selection phase for this iteration. 这是因为所有candidate node已经无法被继续扩展了
%     \item If current node is fully explored (\( n\_visit > E \)) and its score exceeds \( \text{higher\_limit} \), select current node and terminate selection phase. 这是由于，对于正确率很高的节点，没有必要对其分支的子节点进行进一步的树搜索。
%     \item If current node is fully explored (\( n\_visit > E \)) and its score is below \( \text{lower\_limit} \), select the current node and terminate the search. This avoids further exploration of unpromising paths.
%     \item If one candidate child would be selected but it has been fully explored (\( n\_visit > E \)) and its score is below \( \text{lower\_limit} \), fall back to the parent node and terminate the selection phase. This also avoids exploration of unpromising paths.
% \end{itemize}

% This Auto-stay policy ensures that the search process adapts to the difficulty of the problem, balancing exploration and exploitation while maintaining diversity. 从合成数据的角度，对于没有希望的推理分支能够及时放弃以保证效率，对于有把握的step，则避免树搜索以保证多样性。


\subsection{Selection with Adaptive Stay Policy}

\label{sec:adaptive}

In the selection phase, Fast-MCTS recursively selects child nodes using the Upper Confidence Bound for Trees (UCT) criterion, as vanilla MCTS does, as defined in Equation \ref{eq:uct}. However, to improve efficiency and diversity, we introduce an Adaptive Stay Policy that dynamically adjusts the selection process based on the node's exploration status and estimated value. 

% 我们从合成数据效率的角度出发，尽可能的搜索正确且多样的推理路径。
% 在adaptive Stay policy中，与vanilla MCTS不同，select不再需要选择到叶子节点。对于作对期望较低的state会及时放弃。对于模型作对概率极高或极低的状态，也会stay而不是继续select，而是优先保证多样性，或试图至少探索到一条正确的推理路径。

% In Adaptive Stay policy, selection does not necessarily proceed to leaf nodes as in vanilla MCTS. For states where the model's probability of being correct is either very high or very low, the policy opts to "stay" rather than continuing selection. This approach prioritizes diversity for easy problem and ensures that at least one correct reasoning path is explored for hard problem.

In Adaptive Stay policy, selection does not necessarily proceed to leaf nodes as in vanilla MCTS. For states where the likelihood of being correct is either very high or very low, our method opts to "stay" rather than continuing selection. This approach prioritizes diversity for easier problems and attempts to explore at least one correct reasoning path for more challenging problems.

% This Adaptive Stay policy ensures that the search process adapts to the difficulty of the problem, balancing exploration and exploitation while maintaining diversity. From the perspective of data synthesis, it allows for timely abandonment of unpromising reasoning branches to ensure efficiency, while avoiding tree searches for confident steps to maintain diversity.



% During the selection phase, we refer to the node currently under consideration as the "current node," and all its potential child nodes that can be selected as "candidate children." The Adaptive Stay Policy can be described as follows:

% \begin{itemize}
%     \item If the number of candidate children is less than or equal to one (i.e., the current node has no or one child), select the current node and terminate the selection phase. We use this policy instead of "select leaf node" because it integrates well with our Reserve Simulation strategy, which we will introduce later.
%     \item If all candidate children are in a stopped state (i.e., they represent the final step of the reasoning chain), select the current node and terminate the selection phase for this iteration. This is because no further expansion of the candidate nodes is possible.
%     \item If the current node is fully explored (\( n_{\text{visit}} > E \)) and its score exceeds \( \text{higher\_limit} \), select the current node and terminate the selection phase. This avoids unnecessary tree search for highly promising branches.
%     \item If the current node is fully explored (\( n_{\text{visit}} > E \)) and its score is below \( \text{lower\_limit} \), select the current node and terminate the search. This prevents further exploration of unpromising paths.
%     \item If a candidate child would be selected but it has been fully explored (\( n_{\text{visit}} > E \)) and its score is below \( \text{lower\_limit} \), fall back to the parent node and terminate the selection phase. This also avoids exploration of unpromising paths.
% \end{itemize}

% \subsection{Dynamic Exploration}
% To further enhance the search strategy, we dynamically adjust the parameter $c$ in UCT. We define the score of one node as the estimated value of taking one action(step )calculated by Monte Carlo Estimation. which is ($node.score = \frac{node.win\_count}{node.visit\_count}$)
% And we adjust $c$ by
% \[
% c_{current} = c \cdot node.score\quad \text{if} \quad node.visit\_count > E,
% \]
% where $E$ .
% This heuristic encourages exploration in promising states and prioritizes exploitation in less promising ones, aligning with the goal of data synthesis.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \includegraphics[width=\linewidth]{aime_efficiency.pdf}
        \caption{Sampling Efficiency on AIME}
        \label{fig:efficiency_aime}
    \end{subfigure}
    
    % \vspace{1em} % 添加一些垂直间距
    
    \begin{subfigure}[b]{1\linewidth}
        \includegraphics[width=\linewidth]{cn_efficiency.pdf}
        \caption{Sampling Efficiency on CN High School Math}
        \label{fig:efficiency_high}
    \end{subfigure}
    
    \caption{Comparison of sampling efficiency for FastMCTS and Rejection Sampling.}
    \label{fig:efficiency}
\end{figure*}

\subsection{Dynamic Exploration}

To enhance the search strategy, we dynamically adjust the parameter $ c $ in UCT based on node scores. The score of one tree node is defined as the estimated value of taking an action (step), calculated by Monte Carlo Evaluation:
\begin{equation}
   node.score = \frac{node.win\_count}{node.visit\_count} 
\end{equation}
Then we adjuct $c$ by multiplying it with the node's score if the node has been visited more than once.
This approach encourages exploration in promising states and prioritizes exploitation in less promising ones, aligning with the goal of data synthesis. The entire selection phase of the FastMCTS algorithm is demonstrated in Algorithm \ref{algo:select}.

% \paragraph{Reserve Simulation}
% To streamline the process and enhance efficiency, we consolidate the expansion and simulation phases into a single phase. From the selected node, we perform \( \text{expand\_degree} \) 次 roll-outs using the LLM policy \( \pi_\theta \), generating \( \text{expand\_degree} \) 个 new reasoning paths. Unlike vanilla MCTS, which discards simulation results, we preserve all newly generated paths as valid nodes and add to our search tree. This ensures that correct reasoning paths are not wasted, significantly improving efficiency. 这个改进能够很好的与之前的Adaptive Stay Policy结合, 因为 保留了Simulation 的推理路径，意味着一个节点即使未被select expand，也会有一条对应的子孙孩子节点分支对应着一条推理路径。因此select过程是不需要深入到叶子节点。

\subsection{Reserve Simulation}
% 参考一些motivation部分

% Unlike board games such as Go or chess, where the outcome of a random simulation does not necessarily reflect the quality of a specific node, LLM reasoning exhibits a strong correlation between the final answer and the correctness of the entire reasoning path. In most cases, if the final answer is correct, the entire reasoning path is likely to be correct. Therefore, simulation results in LLMs are valuable and should be preserved, rather than discarded as in traditional MCTS. This fundamental difference makes the direct application of vanilla MCTS strategies unsuitable for LLM reasoning tasks.

% Unlike board games such as Go or chess, where the outcome of a random simulation does not necessarily reflect the quality of a specific node, LLM reasoning exhibits a strong correlation between the final answer and the correctness of the entire reasoning path. Therefore, simulation results in LLMs are valuable and should be preserved, rather than discarded as in traditional MCTS.

% 受此启发，we consolidate expansion and simulation into a single phase. Unlike vanilla MCTS, which discards simulation results, we preserve all newly generated paths as valid nodes and add them to our search tree. This ensures that all correct reasoning paths are not wasted, significantly improving efficiency. This improvement also integrates well Adaptive Stay Policy, as all trajectories are stored after the selection process, then there is no  need to dive deeply into leaf nodes in search process.


Unlike board games like Go or chess, where the outcome of one random simulation does not necessarily reflect the quality of a specific state, LLM reasoning shows a strong correlation between the final answer and the correctness of the entire reasoning path. Therefore, simulation results in LLMs are valuable and should be preserved.

Inspired by this, we consolidate expansion and simulation into a single phase. Unlike vanilla MCTS, which discards simulation results, we preserve all newly generated paths as valid nodes and add them to our search tree. This significantly enhances sampling efficiency and integrates well with Adaptive Stay Policy. Since all trajectories are stored after selection, there is no need to delve deeply into leaf nodes during the search process.



\subsection{Robustness Enhancements}

% To address the challenges of answer format variability and logical errors in reasoning paths, we introduce a robustness enhancement mechanism. Instead of relying solely on rule-based answer matching, we use LLM to evaluate the correctness of reasoning paths against the ground-truth answer. Additionally, we require  the correctness of intermediate steps within each reasoning path, ensuring that both the final answer and the intermediate logic are accurate.

% Furthermore, To increase the diversity of generated reasoning paths, we prepend a random set of few-shot examples to the input question during each simulation. Specifically, ext learning (ICL) approach encourages the generation of diverse reasoning paths, further enhancing the robustness of the synthesized data.

To address variability in answer formats and logical errors in reasoning paths, we introduce a robustness enhancement mechanism. Instead of relying solely on rule-based answer matching, we use a LLM to evaluate the correctness of reasoning paths against the ground-truth answer. Additionally, we require the LLM to verify the correctness of intermediate steps within each path, aiming to identify logical errors and exclude trajectories that are guessed answers (e.g., multiple-choice questions).

Furthermore, to increase the diversity of generated reasoning paths, we prepend different random combination of few-shot examples to each input string during simulation. This in-context learning approach promotes diverse reasoning paths, further enhancing data robustness.



\subsection{Tree Construction and Data Utilization}
The search tree is constructed iteratively, starting from the root node. The complete algorithm is outlined in Appendix \ref{apd:fastmcts}, and Figure \ref{fig:intro} illustrates the flow of one iteration of FastMCTS.

We can construct training data from the tree structure. Specifically, correct reasoning paths are used for Supervised Fine-Tuning (SFT). Additionally, different branches within the tree nodes, based on their values, can be transformed into pair data for step-level and branch-level Direct Preference Optimization~\cite{DBLP:conf/nips/RafailovSMMEF23}.






\section{Experiment}
% 首先是关于效率的实验
% 其次是关于效果的实验 包括dpo数据的利用
% 最后abalation关于多样性和难度平衡

% In this Section, 我们首先将在\ref{sec:efficiency}展示FastMCTS相比于Rejection Sampling在合成数据效率上的优势。在\ref{sec:performance}中，我们展示了FastMCTS在与Rejection Sampling在相同的推理开销下合成的数据集，在训练效果上的提升。在\ref{sec:analysis}中，我们用消融实验证明了我们方法改进之处的比较性，并且对FastMCTS合成数据的效果进行了分析。



% Todo: 要在这里提一下，为了更广泛的验证我们的方法有效性，我们在两种不同分布的数据上做了效率的验证以及训练的验证

% In this section, we first demonstrate the efficiency advantages of FastMCTS over Rejection Sampling in synthesizing data in Section \ref{sec:efficiency}. 

% In Section \ref{sec:performance}, we demonstrate the improvements in training performance when using datasets synthesized by FastMCTS compared to those generated by Rejection Sampling, under the same inference cost (i.e., generating the same number of tokens). 

% Finally, in Section \ref{sec:analysis}, we conduct ablation studies to highlight the comparative improvements of our approach and provide an in-depth analysis of the synthesized data quality produced by FastMCTS.

% \subsection{Setup}
% data: hard questions from NuminaMath. qwen2.5-72b for roll out. 

% for efficiency, we compare in \S\ref{sec:efficiency}

% for performance, we train qwen-2.5-7B. evaluated on 3 types of benchmark: competition level, college level, olympiad level. compare 7B-math(math-specilaized model), 72B-instruct(roll out model), numinamath-7b. in \S\ref{sec:performance}




\subsection{Sampling Efficiency Comparison}

\label{sec:efficiency}
% In this section ,我们展示了FastMCTS相比于Vanilla Rejection Sampling在Sampling Efficiency上的提升。We use Qwen2.5-72B-Instruct(cite) as the policy model 用于synthetic data，数据集方面，我们使用了AIME(2023年前)的数据，以及从互联网中收集的中文高中数学数据(cite 文曲星)。我们比较了两种方法合成正确题目的效率。


% In this section, we demonstrate the improvements in sampling efficiency of FastMCTS compared to Vanilla Rejection Sampling. For the dataset, we utilized problems from the USA Mathematical Olympiad-level competition AIME up to the year 2023~\cite{aime}, along with subsets of Chinese high school mathematics problems collected from internet \cite{2024internlm2wqx}. We compare the efficiency of both methods in generating correct problem instances.

% In our Experiment, we use sglang(cite) as our 推理引擎， we employ sampling generation with 
% temperature = 1 to ensure diversity, 对于UCT分数中的常数c我们设置为默认值1.414。We also use Qwen2.5-72B-Instruct as a LLM judger to verify the solution. 这能够有效避免由于格式问题引起的判别错误，并能排除一些中间过程错误但做对的推理路径(如选择题)。 To scale up sampling compute, for FastMCTS, 我们逐步扩大迭代的轮数(从4轮扩展到18轮以上)。for Rejection Sampling，我们扩大采样次数，从3扩大到32。 


In this section, we demonstrate the improvements in sampling efficiency of FastMCTS compared to Rejection Sampling. For our dataset, we utilized problems from the USA Mathematical Olympiad-level competition AIME up to the year 2023~\cite{aime}, along with Chinese high school mathematics problems collected from the internet, referred to as CN High School Math \cite{2024internlm2wqx}. Specifically, we randomly selected 300 problems from AIME and 1000 problems from CN High School Math for our experiments. We then compared the efficiency of both methods in generating correct problem instances. We use the open-sourced LLM Qwen2.5-72B-Instruct~\cite{DBLP:journals/corr/abs-2412-15115} and temperature is set to 1. Detailed generation settings are provided in Appendix \ref{apd:sampling settings}.
% 补充在这里

% On these two datasets, our experiment result is in \ref{fig:efficiency} 所示. we scale up generated tokens in our sampling, and we compare three metrics for Rejection sampling and our FastMCTS. Problem Solving Rate 指的是对于对于一个query，合成了至少一条正确推理路径的平均概率。Correct Path 指的是对于一个query，平均产生了正确推理路径的数量。Effective Token Rate指的是生成的token中属于正确推理路径的token的比例。

% 可以看到，随着采样次数增多，generated tokens 数量上升，两种sampling 策略产生的正确推理路径数量都在增加。但是在token数量足够多时，FastMCTS能够比Rejection Sampling多采样出超过30\%的正确推理路径。证明了我们方法在合成数据上的效率提升。此外，FastMCTS也比Rejection Sampling有更高的Problem Solving Rate。，这是由于在每次simulation过程前，对于expand的不同分支都会拼接不同的few-shot作为context，使产生的推理路径更具多样性，搜索到正确答案的概率也得到提升。


Our experimental results are shown in Figure \ref{fig:efficiency}. We gradually increased the number of generated tokens during sampling and compared three metrics for Rejection Sampling and FastMCTS.
\textbf{Problem Solving Rate} refers to the average probability of generating at least one correct reasoning trajectories for a query.
\textbf{Average Correct Paths} refers to the average number of correct reasoning trajectories generated for a query.
\textbf{Effective Token Rate} refers to the proportion of generated tokens that belong to correct reasoning trajectories.



% As shown in Figure \ref{fig:efficiency}, when the number of generated tokens scales up, FastMCTS can generate over 30\% more correct reasoning paths compared to Rejection Sampling, as well as more effective tokens. This demonstrates the efficiency improvement of FastMCTS in synthesizing data. Additionally, FastMCTS exhibits a higher Problem Solving Rate than Rejection Sampling. This is due to the fact that different few-shot examples are prepended as context for each expanded branch before each simulation, enhancing the diversity of generated reasoning paths and increasing the probability of finding the correct solution.

As shown in Figure \ref{fig:efficiency}, FastMCTS generates over 30\% more correct reasoning paths compared to Rejection Sampling as the number of generated tokens scales up. Additionally, FastMCTS produces more effective tokens, demonstrating its efficiency in data synthesis. Furthermore, FastMCTS achieves a higher Problem Solving Rate than Rejection Sampling. This is because diverse few-shot examples are prepended as context for each expanded branch before simulation, enhancing the diversity of generated reasoning paths and increasing the likelihood of finding the correct solution.



% \paragraph{Difficulty Level}
% 这个放到analysis

\subsection{Training Performance Comparison}
\label{sec:performance}

% 这些部分删一下，放附录。正文说清楚，细节放附录
\subsubsection{Experimental Setup}
In addition to the comparison of sampling efficiency, we also evaluated the training performance on datasets generated using FastMCTS versus those generated using Rejection Sampling, with comparable computational budgets. To facilitate a more general comparison, we conducted experiments on datasets with two different distributions, specifically Chinese and English.

% We synthesized corresponding mathematical reasoning data for both English and Chinese datasets using these two methods.
% 为了使，我们在
% We refer to these two datasets after selection as \textit{EN Math Hard} and \textit{CN High School Math Hard}.

\paragraph{Training Data Generation} For English data, we selected 46,000 problems from a wide range of math data including Numina-Math~\cite{numina_math_datasets}, MetaMath~\cite{yu2023metamath}, and the training set of InternLM-Math~\cite{ying2024internlmmathopenmathlarge}. For Chinese data, we selected 50,000 problems from Chinese high school math problems collected from the Internet\cite{2024internlm2wqx}. We used heuristic strategies and model evaluations to filter out simpler problems, retaining multiple-choice, fill-in-the-blank, and solution-type questions while excluding proof and diagram-drawing problems. We refer to these two datasets after selection as \textit{EN Math Hard} and \textit{CN High School Math Hard}. We used Qwen2.5-72B-Instruct as the policy model and other sampling settings are described in Appendix \ref{apd:sampling settings}. To ensure a fair comparison, we controlled the computational costs of both sampling strategies to be comparable. The specific computational costs for both datasets are detailed in Table~\ref{tab:sample_set}. Under this configuration, FastMCTS generates fewer tokens per query while acquiring more correct reasoning trajectories compared to rejection sampling. 

\paragraph{Baselines}
We use Qwen2.5-7B~\cite{DBLP:journals/corr/abs-2412-15115} as the base model and compare its performance when trained on data synthesized by FastMCTS and Rejection Sampling. For both methods, the synthesized data is constructed into supervised fine-tuning datasets by randomly sampling different maximum limits of correct trajectories. For FastMCTS, we additionally construct preference data from its generated tree structures, including step-level and branch-level pairs, which are used for a second-phase Branch-DPO training. Detailed data construction and training setups are provided in Appendix \ref{apd:data construct} and \ref{apd:training details}.



% \begin{table}[htb]
% \footnotesize
% \caption{Sampling budgets for English and Chinese Datasets}
% % 这里如何更好的表述？
% \centering
% \captionsetup{justification=centering}
% \begin{subtable}{\linewidth}
%   \centering
%   \caption{Sampling Comparison on English Dataset}
%   \label{tab:cost_en}
%   \begin{tabular}{lcc}
%     \toprule
%     & \begin{tabular}[c]{@{}c@{}}\textbf{Rejection Sampling}\\(Sample 30)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{FastMCTS}\\ (3,2,16)\end{tabular} \\
%     \midrule
%     \# Tokens & 27.8K & 26.2K \\
%     \# Trajectories & 3.46  & 5.88  \\
%     \bottomrule
%   \end{tabular}
% \end{subtable}

% \vspace{0.5cm} % 增加间距以区分两个表格

% \begin{subtable}{\linewidth}
%   \centering
%   \caption{Sampling Comparison on Chinese Dataset}
%   \label{tab:cost_cn}
%   \begin{tabular}{lcc}
%     \toprule
%     & \begin{tabular}[c]{@{}c@{}}\textbf{Rejection Sampling}\\(Sample 24)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{FastMCTS}\\ (3,2,16)\end{tabular} \\
%     \midrule
%     \# Tokens & 18.2K & 17.4K \\
%     \# Trajectories & 8.15 & 13.70 \\
%     \bottomrule
%   \end{tabular}
%   \begin{minipage}{\linewidth}
%     \vspace{5pt}
%     \footnotesize
%     \textbf{Note:} The row ``\# Tokens'' refers to the average number of tokens generated per problem during the sampling phase. The row ``\# Trajectories'' refers to the average number of correct reasoning paths acquired per problem.
%   \end{minipage}
% \end{subtable}
% \label{tab:sample_set}
% \end{table}


\begin{table}[t]
\footnotesize
\centering

\begin{tabular}{lcc}
    \toprule
    &  \multicolumn{1}{c}{\textbf{Rejection Sampling}} & \multicolumn{1}{c}{\textbf{FastMCTS}} \\
    \midrule
    \multicolumn{3}{c}{\textit{EN Math Hard}}  \\
    \# Tokens &  27.8K & 26.2K  \\
    \# Trajectories &  3.46  & \textbf{5.88} \\
    \midrule
    \multicolumn{3}{c}{\textit{CN High School Math Hard}}  \\
    \# Tokens &   18.2K  & 17.4K \\
    \# Trajectories &  8.15 & \textbf{13.70} \\
    \bottomrule
\end{tabular}
\caption{Comparison of synthetic data generation costs between Rejection Sampling and FastMCTS under the experimental settings of Section \ref{sec:performance}. The row ``\# Tokens'' indicates the average number of tokens generated per problem during the sampling phase. The row ``\# Trajectories'' indicates the average number of correct reasoning paths acquired per problem.}

\label{tab:sample_set}
\end{table}








% 评测集 中英
\begin{table*}[htb]
\footnotesize
% 好东西 控制行间距
% \renewcommand{\arraystretch}{1.4}

\begin{tabular}{l@{\hskip 5pt}c@{\hskip 0pt}cccc@{\hskip 5pt}c@{\hskip 5pt}cc@{\hskip 5pt}cc}
\toprule
                     & & \multicolumn{1}{c}{\textbf{Base Level}} & \multicolumn{2}{c}{\textbf{High School Level}}                                                                  & \multicolumn{3}{c}{\textbf{Competition Level}} & \multicolumn{2}{c}{\textbf{Olympiad Level}}                         &  \\ 
                     \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-10} 
Method &    \#Data   & GSM8K                     & \begin{tabular}[c]{@{}c@{}}Gaokao\\ Math\end{tabular} & \begin{tabular}[c]{@{}l@{}}SAT\\ Math\end{tabular} & AIME24          & AMC23         & MATH         & \begin{tabular}[c]{@{}c@{}}Olympiad\\ Bench\end{tabular} & OmniMath & \textbf{Avg.}\\ \midrule
Qwen2.5-7B & - & 88.2 & 62.6    &  70.6 &  0 &  47.5 &  66.8 &    26.2 & 35.5 & 49.7 \\ \midrule
\multicolumn{11}{c}{\textit{Training Trajectories per Problem $\le$ 5}}                                                                                                                                                                                              \vspace{1.5pt}               \\  
RS  & 111K  & \underline{89.1} & 62.6 &  70.6 & 6.7 &  52.5 & 72.0 &    27.6 & 38.3 & 52.4 \\
FastMCTS & 132K & 88.9 & \underline{63.6} &  \underline{74.5} & \underline{13.3} &  \underline{57.5} & \underline{73.0} &    \underline{28.1} & \textbf{39.8} & \underline{54.8}     \\ \midrule
\multicolumn{11}{c}{\textit{Training Trajectories per Problem $\le$ 10}}                                                                                                                    \vspace{1.5pt}                                                                                                 \\
RS  & 167K & 89.4 & 62.6 &  72.6 & 6.7 &  50.0 & 70.8 &    26.3 & 37.5 & 52.0 \\
FastMCTS &223K& \textbf{90.0} & \underline{64.0} &  \underline{74.5} & \underline{13.3} &  \underline{57.5} & \underline{72.0} &  \underline{27.3} & \underline{38.7} & \underline{54.7}    \\ \midrule
\multicolumn{11}{c}{\textit{Training Trajectories per Problem $\le$ 16}}                                                            \vspace{1.5pt}                                                                                                                                                         \\
RS &197K  & 87.1 & \textbf{65.1} &  72.6 & 10.0 &  52.5 & 70.0 &  27.1 & 37.2 & 52.7 \\
FastMCTS &288K & 88.9 & 63.8 &  72.6 & \textbf{20.0} & \textbf{60.0} & 74.0 &  27.5 & 38.3 & 55.6  \\ 
+ Branch-DPO & 152K & \underline{89.9} & 65.0 &  \textbf{76.5} & \textbf{20.0} & 57.5 & \textbf{75.4} &  \textbf{29.6} & \underline{39.2} & \textbf{56.6} \\
\bottomrule
\end{tabular}

% \caption{Comparison of training performance on some challenging math benchmarks benchmarks between Rejection Sampling and FastMCTS with comparable synthetic data generation cost.}
\caption{The results of model performance trained on EN Math Hard dataset synthesized by Rejection Sampling and FastMCTS with comparable generation cost. RS refers to synthetic dataset generated though rejection sampling. \textbf{Bold} indicates the best value, and \underline{underlined} indicates the best value within a group.}
%  

\label{tab:train res en}
\end{table*}


\begin{table}[htb]
% \footnotesize
% 好东西 控制行间距
% \renewcommand{\arraystretch}{1.4}

\begin{tabular}{lccc}
\toprule
Method       & \#Data & Gaokao24 & CMATH \\
\midrule
Qwen2.5-7B   & -  &  33.3   &  85.8     \\
\midrule
\multicolumn{4}{c}{\textit{Training Trajectories per Problem $\le$ 5}}   \\
RS           &  158K      &   58.0   &  89.3     \\
FastMCTS     &  198K      &  \underline{59.4}    &  \textbf{90.8}     \\
\midrule
\multicolumn{4}{c}{\textit{Training Trajectories per Problem $\le$ 10}}   \\
RS           &  250K      &  59.4  &   89.3    \\
FastMCTS     &  359K      &  \underline{60.9}   &   \underline{89.5}    \\
\midrule
\multicolumn{4}{c}{\textit{Training Trajectories per Problem $\le$ 16}} \\  
RS           &  305K      &  60.9        & 88.8      \\
FastMCTS     &  502K      &  \textbf{62.3}        &  89.3     \\
+ Branch-DPO &  215K      &  \textbf{62.3}        &  \underline{89.8}     \\
\bottomrule
\end{tabular}

% \caption{Comparison of training performance on some challenging math benchmarks benchmarks between Rejection Sampling and FastMCTS with comparable synthetic data generation cost.}
\caption{The results of model performance trained on CN High School Math Hard dataset synthesized by Rejection Sampling and FastMCTS with comparable generation cost. RS refers to synthetic dataset generated though rejection sampling. \textbf{Bold} indicates the best value, and \underline{underlined} indicates the best value within a group.}


% 是不是要写一下训练参数？？？？
\label{tab:train res cn}
\end{table}





\subsubsection{Main Results}

We evaluated our models across a variety of mathematical benchmarks. All models are assessed in a zero-shot setting, employing greedy decoding for evaluation purposes.

% For models trained on data synthesized from EN Math Hard, we utilize GSM8K~\cite{DBLP:journals/corr/abs-2110-14168} as a baseline assessment, Gaokao Bench Math~\cite{DBLP:conf/icml/TangZWW24} and SAT-Math~\cite{DBLP:conf/icml/TangZWW24} for high school-level evaluations, AIME24~\cite{aime24}, AMC23~\cite{amc}, MATH-500~\cite{hendrycksmath2021,DBLP:conf/iclr/LightmanKBEBLLS24} for competition-level assessments, and Olympiad Bench~\cite{DBLP:conf/acl/HeLBHTSHHHZLQL024}, OmniMath~\cite{DBLP:journals/corr/abs-2410-07985} for olympiad-level challenges. It is important to note that our English training datasets have been carefully curated to exclude any overlap with these evaluation benchmarks. 

% Regarding models trained on data synthesized from CN High School Math Hard, we select 69 text-only problems from the 2024 Chinese Gaokao (National Higher Education Entrance Examination). These problems are both challenging and collected after the training dataset, ensuring no contamination. Additionally, CMATH~\cite{wei2023cmath} is chosen as a foundational benchmark for evaluating basic performance.

For models trained on data synthesized from EN Math Hard, we evaluated on GSM8K~\cite{DBLP:journals/corr/abs-2110-14168} for baseline assessment, Gaokao Bench Math~\cite{DBLP:conf/icml/TangZWW24} and SAT-Math~\cite{DBLP:conf/icml/TangZWW24} for high school-level problems, AIME24~\cite{aime24}, AMC23~\cite{amc}, and MATH-500~\cite{hendrycksmath2021,DBLP:conf/iclr/LightmanKBEBLLS24} for competition-level challenges, and Olympiad Bench~\cite{DBLP:conf/acl/HeLBHTSHHHZLQL024} and OmniMath~\cite{DBLP:journals/corr/abs-2410-07985} for olympiad-level tasks. For models trained on CN High School Math Hard, we evaluated on 69 text-only problems from the 2024 Chinese Gaokao(National Higher Education Entrance Examination) and CMATH~\cite{wei2023cmath} for foundational performance. Our training data are carefully curated to ensure no overlap with these evaluation benchmarks. 


% cite evalua

% The training results are shown in Table \ref{tab:train res en} and \ref{tab:train res cn}. From this, we observe:

% 1. Under the same generation budget, models trained with data sampled from FastMCTS outperform those trained with data from Rejection Sampling.

% 2. The overall performance of models trained with FastMCTS-generated data improves when scales up the number of reasoning trajectories per problem, whereas models trained using Rejection Sampling show 较少的 improvement.(或者说提升不稳定，不明显或者说不显著)

% 3. 由于FastMCTS方法合成的数据具有tree structure(树结构) with score of each step, it could be effective reused for a second stage Branch-DPO Training. Which is capable of further boost model reasoning performance.

% This indicates that, under same computational budge for data synthesis, Data synthesized from FastMCTS is more effective than those from Rejection Sampling. Model performance increases with using more trajectories for training, 同时能够从Branch/Step Level 的DPO数据中获得额外的性能增益。实验表明，FastMCTS合成的数据的的有效性不仅仅来源于其包含更多正确的trajectories, 还由于其动态采样的算法，能够对不同难度的数据所采样的trajectories数量保持较好的平衡性，而Rejection Sampling则会偏向于采样到更多简单题目的推理路径。 We will further analyze this point in Section \ref{sec:analysis}.

% Andthereby more effectively strengthening the model's mathematical reasoning abilities. Data from Reject Sampling would consist of more simpler problems, solve fewer chanllenging problems compared to FastMCTS. We will further analyze this point in Section \ref{5.3.2}.


% The training results are shown in Table~\ref{tab:train res en} and Table~\ref{tab:train res cn}. From these results, we observe the following key findings:

% 1. Under the same generation budget, models trained on data sampled using FastMCTS consistently outperform those trained on data from rejection sampling.

% 2. The overall performance of models trained on FastMCTS-generated data improves as the number of reasoning trajectories per problem increases. In contrast, models trained using rejection sampling exhibit limited and inconsistent improvement.

% 3. Due to the tree structure of the data synthesized by FastMCTS, which includes step-level scores, the data can be effectively reused for a second stage of Branch-DPO training. This further enhances the model's reasoning performance.

% These results indicate that, under the same computational budget for data synthesis, FastMCTS-generated data is more effective than data from rejection sampling. Model performance improves with the use of more trajectories for training, and additional performance gains can be achieved through Branch/Step-Level DPO training. Our experiments demonstrate that the effectiveness of FastMCTS-synthesized data stems not only from its ability to generate more correct trajectories but also from its dynamic sampling algorithm, which maintains a balanced distribution of trajectories across problems of varying difficulty. In contrast, rejection sampling tends to bias towards simpler problems, resulting in an imbalanced distribution of trajectories. We will further analyze this point in Section~\ref{sec:diffifulty}.

% The training results are shown in Table~\ref{tab:train res en} and Table~\ref{tab:train res cn}. Key findings include:

% 1. Under comparable generation budget, models trained on FastMCTS-sampled data consistently outperform those trained on rejection sampling data.

% 2. Performance of models trained on FastMCTS-generated data improves as the number of reasoning trajectories per problem increases, while models trained on data from rejection sampling exhibit limited and inconsistent improvement.

% 3. FastMCTS-generated data could be effectively reused for Branch-DPO training, further enhancing reasoning performance.

% These results demonstrate that FastMCTS-synthesized data is more effective than rejection sampling with a comparable or even less generation budget. Model performance improves with using more trajectories for training, and additional performance gains can be achieved through DPO by utilizing step-level scores from tree-structured data.




The training results are presented in Table~\ref{tab:train res en} and Table~\ref{tab:train res cn}. Key findings include:

1. Under comparable generation budgets, models trained on FastMCTS-sampled data consistently outperform those trained on rejection sampling data.

2. The performance of models trained on FastMCTS-generated data improves as the number of reasoning trajectories per problem increases, while models trained on rejection sampling data show limited and inconsistent improvement.

2. FastMCTS-generated data can be effectively reused for Branch-DPO training, further enhancing reasoning performance.

These results demonstrate that FastMCTS-synthesized data is more effective than rejection sampling, even with a comparable or lesser generation budget. For FastMCTS, model performance improves with an increase in the number of trajectories used for training, and additional gains can be achieved through DPO by utilizing step-level scores from tree-structured data.




\subsection{Analysis}
\label{sec:analysis}

\begin{figure*}[htb]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{en_balance.pdf}
        \caption{Sampling Balance on EN Math Hard}
        \label{fig:balance_en}
    \end{subfigure}
     % \vspace{1em} 
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{cn_balance.pdf}
        \caption{Sampling Balance on CN High School Math Hard}
        \label{fig:efficiency_cn}
    \end{subfigure}
    
    \caption{Comparison of sampling balance across difficulty levels for Rejection Sampling and FastMCTS.}
    \label{fig:balance}
\end{figure*}

\subsubsection{Difficulty-Aware Sampling in FastMCTS}
\label{sec:diffifulty}
% \subsubsection{Difficulty Balance}
% 优先写这个？证明实验结果的有效性
% 用high school easy 和 hard 做一下对比
% 这个图已经包括了简单题目和难题目的区分了

% As described in \ref{method:fastmcts}, FastMCTS ensures that the search
% process adapts to the difficulty of the problem, obtaining a more balance distribution of problems of diverse difficulty. This makes the data generated by FastMCTS not only more in 数量, but also more effective in training.

% To analysis this, for the training data synthesized by both FastMCTS and Rejection Sampling, 我们根据Rejection Sampling时题目采样到正确答案的比例，将所有题目划分为了五个难度级别。之后，我们统计了对于每一种难度的数据，Rejection Sampling 和 FastMCTS所采样到的正确Trajectories数量，实验结果如图\ref{fig:balance}所示。

% 从图\ref{fig:balance}中可以发现，不论是从EN Math Hard合成的英文数据还是从CN High School Math中合成的中文数据，FastMCTS在不同难度的题目上的分布比Rejection Sampling更加平衡。此外，FastMCTS比Rejection Sampling所更多采样的数据集中在更高难度题目上，对于简单的题目(Difficulty Level = 1)，FastMCTS所采样到的数据并没有比Rejection Sampling多很多。这些结果反映了FastMCTS的Difficulty-Aware的特性，在树搜索过程中，随着迭代次数的增加，每一次simulation都会使树节点中由蒙特卡洛估计得到的分数更加准确。对于难度较高的题目，FastMCTS会更多的倾向于采样作对概率更高的branch。对于难度较低的题目，树搜索则是不需要的，FastMCTS会退化为Rejection Sampling，优先保证数据的多样性。

% 以上结果解释了为何使用Rejection Sampling方法采样到的更多trajectories，训练效果没有FastMCTS的数据效果好。尽管FastMCTS得到的数据部分会因为树搜索的特性从而有一些相同的前缀，但是在不同难度的数据之间获得了更平衡性的分布。

As described in Section~\ref{sec:adaptive}, FastMCTS dynamically adapts the search process according to the problem difficulty. This adaptation results in a more balanced distribution of problems across different difficulty levels. Consequently, the data generated by FastMCTS is not only larger in quantity but also more effective for training purposes.

To analyze this, we categorize problems from our dataset into five difficulty levels based on the probability of sampling a correct answer using rejection sampling. We then compare the number of correct trajectories generated by both FastMCTS and rejection sampling for each level.

The results in Figure~\ref{fig:balance} show that FastMCTS achieves a more balanced distribution across difficulty levels than rejection sampling, particularly for higher-difficulty problems. These results highlight FastMCTS's difficulty-aware feature. During tree search, as iterations increase, Monte Carlo-estimated scores become more accurate. For harder problems, FastMCTS tends to sample branches with higher success probabilities, while for easier problems, it degenerates to rejection sampling, mainly focusing on diversity.

\begin{table}[b]
\footnotesize
\begin{tabular}{lcc}
\toprule
Method                          & Solving Rate(\%)    & \#Correct Path  \\
\midrule
Rejection Sampling              & 61.3                   & 7.22                       \\
\midrule
FastMCTS     & \textbf{61.7}             & \textbf{7.95}       \\

\quad w/o stay   & 55.9             & 7.59             \\
\quad w/o dynamic   & \textbf{61.7}             & 7.28                           \\
\quad w/o stay \& dynamic & 55.9    & 7.32                     \\
\bottomrule
\end{tabular}
\caption{Ablation study}
\label{tab:ablation}
\end{table}

These findings explain the effectiveness of data synthesized by FastMCTS. Although tree-search process may reduce diversity due to shared prefixes, FastMCTS achieves a more balanced distribution across problems of varying difficulty levels.





\subsubsection{Ablation Study}
\label{sec:ablation}




% \subsubsection{Preference Optimization}
% We ablate the effectiveness of our methods. In FastMCTS, we 我们对vanilla MCTS 进行了一些改进使其更适合合成数据, 我们通过消融实验证明了Adaptive Stay Policy 以及 Dynamic Exploration 的必要性。

% We choose Rejection Sampling as our baseline, and for FastMCTS, 我们比较了当缺失Adaptive Stay 或  Dynamic Exploratio 时其合成数据效率的变化情况。我们随机抽取了300道AIME的数据用于测试, 生成数据的settings 与\ref{sec:efficiency}相同。对于每种采样方法，for each input我们都生成25个Trajectories，即对于Rejection Sampling，对于每个输入问题我们都采样25次；对于FastMCTS，我们 set initial degree of 3 at the root, expands by adding 2 branches in each expansion phase, and performs 12 iterations of tree search, which also generate 25 (3 + (12-1)*2) trajectoreis for each  input problem. 

% The comparison resuilt is in Table \ref{tab:ablation}，表中的结果是多次实验的均值. We could deduce that:
% 1. Ablation stay policy could 主要影响到 problem solve rate. Because it 能够根据当前节点的难度，决定是否向下搜索，还是在此节点直接扩增新的分支。这有助于对于过于困难或者过于简单的问题动态扩增更多的branch，从而增加搜索的多样性和成功率。

% 2. Dynamic Exploration 主要影响到了搜索正确trajectories的效率。without this, the average correct path decreased from 7.95 to 7.28. 证明了此策略的有效性。

% We use Rejection Sampling as our baseline and compare the efficiency of data synthesis by FastMCTS with and without Adaptive Stay or Dynamic Exploration. We randomly selected 300 problems from AIME for testing, using the same settings as described in Appendix~\ref{apd:sampling settings}. For each input problem, we sampled 25 trajectories for rejection sampling, and
% for FastMCTS, we set an initial degree of 3 at the root, expanded by adding 2 branches in each expansion phase, and performed 12 iterations of tree search, resulting in 25 trajectories per input problem.

% The comparison results are shown in Table~\ref{tab:ablation}, where the values represent the mean of multiple experiments. From these results, we can deduce that:
% \begin{enumerate}
%     \item The Adaptive Stay policy primarily affects the problem solve rate. It decides whether to continue searching deeper or expand new branches based on the current node's score. This helps dynamically increase branch diversity and success rates for both very difficult and very simple problems.
%     \item Dynamic Exploration mainly impacts the efficiency of generating correct trajectories. Without this strategy, the average number of correct paths decreased from 7.95 to 7.28, demonstrating its effectiveness.
% \end{enumerate}

For our ablation study, we compare the efficiency of FastMCTS with and without Adaptive Stay and Dynamic Exploration, using Rejection Sampling as the baseline. Experiments are conducted on 300 randomly selected AIME problems under the same settings provided in Appendix~\ref{apd:sampling settings}. For each problem, we sample 25 trajectories: Rejection Sampling directly generates 25 trajectories, while FastMCTS performs 12 iterations of tree search with an initial degree of 3 and then expands 2 branches per phase, also yielding 25 trajectories.

From results in Table~\ref{tab:ablation} (averaged over multiple runs), we could deduce that
the Adaptive Stay policy primarily affects the problem solving rate. It decides whether to continue searching deeper or expand new branches based on the current node's score. As for Dynamic Exploration, it increases the efficiency of generating correct trajectories, as its absence reduces the average number of correct paths from 7.95 to 7.28. These findings highlight the necessity and effectiveness of our proposed improvements in FastMCTS.

% These findings highlight the necessity and effectiveness of our proposed improvements. Together, these components enable FastMCTS to achieve superior performance in synthesizing multi-step reasoning data.

% These findings highlight the necessity and effectiveness of our proposed improvements. Together, these components enable FastMCTS to achieve superior performance in synthesizing multi-step reasoning data.

% 如果需要篇幅，就在这里提一下，this 证说明了我们方法改进之处的有效性之类的话

% Please add the following required packages to your document preamble:
% \usepackage{multirow}


% 改成单栏 去掉那两列结果不必要的

% 是否可以再放一个难题目的表？

% 定义注释的方法






\section{Conclusion}

In this work, we introduce FastMCTS, an efficient sampling algorithm that leverages Monte Carlo Tree Search to synthesize high-quality multi-step reasoning data for training large language models. Our approach not only improves the efficiency of data synthesis but also promotes a balanced sampling distribution across problems of varying difficulty, while providing step-level supervision for enhanced training like DPO. Experimental results demonstrate that FastMCTS outperforms rejection sampling in both sampling efficiency and training performance under comparable synthetic data budgets. We believe our method offers a practical solution for efficiently generating high-quality multi-step reasoning data and hope it inspires further research on data synthesis for language models.

% \newpage
% without newpage!

\section*{Limitations}
Our work has several limitations. First, although we utilize a diverse range of data sources for data
synthesis, we rely solely on the open-source model Qwen2.5-72B-Instruct for data generation. We do not employ stronger closed-source models like GPT-4 or models specifically fine-tuned for higher reasoning capabilities, such as Qwen-Math~\cite{DBLP:journals/corr/abs-2409-12122}, DeepSeek-R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, or o1~\cite{openai2024openaio1card}. As a result, the performance of the trained models is not state-of-the-art. Additionally, due to computational resource constraints, we only compare the training performance of Qwen2.5-7B on data synthesized by the two methods. Finally, while FastMCTS-synthesized data achieves better training results due to its quantity and balanced distribution, the impact of prefix repetition in reasoning paths caused by the tree structure remains an open question, which we plan to investigate in future work.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

% \clearpage

\bibliography{custom}
\newpage
\appendix

\section{FastMCTS Algorithms}
\label{apd:fastmcts}




% \newcommand{\COMMENTLLAMA}[1]{{\textcolor[HTML]{962C38} {$\triangleright$ {#1}\\}}}
% \newcommand{\COMMENTLIGHTGRAY}[1]{\hfill{\textcolor[HTML]{A9A9A9} {$\triangleright$  {#1}\\}}}
\begin{algorithm*}
\caption{FastMCTS}
\label{algo:fastmcts}
\KwIn{Input query  $q$, ground truth $g$, few shot set $\mathcal{F}$, policy model $\pi_\theta$, verifier model $V_\phi$, initial degree $d_i$, expand degree $d_e$, iterations $N$, difficulty thresholds $l_{high}, l_{low}$, UCT constant $c$ }
\KwOut{The search tree $T$ of input query $q$}

\textbf{Initialize:} search tree $T$ with root $\gets$ $q$

% \COMMENTLIGHTGRAY{test for gray comment}

%\texttt{init\_process\_group(world\_size=2)}\\

% $prefix\_unverified\gets \texttt{None}$\\


\While{iter < N}{
\COMMENTLLAMA{Recursively select node with Adaptive Stay Policy}

% select from root
current\_node $\gets$ root

selected\_node $\gets$ None
    
\While{\textnormal{selected\_node is None}}{

candidate\_children $\gets$ current\_node.children

\If{\textnormal{number of candidate\_children} $<=$ 1 \texttt{or} \COMMENTLIGHTGRAY{\textnormal{Adaptive Stay Policy}}
\quad \textnormal{all candidate\_children are leaf nodes} \texttt{or} \\
\quad current\_node.visit\_count > 1 \texttt{and} current\_node.score $\in (0, l_{\text{low}}] \cup [l_{\text{high}}, 1)$ 
}{
selected\_node $\gets$ current\_node

break
}


\eIf{current\_node.visit\_count > 1}{$c_{current}$ $\gets$ $c \cdot current\_node.score$ \COMMENTLIGHTGRAY{Dynamic Exploration}} 
{$c_{current}$ $\gets$ $c$}

candidate\_node $\gets$ $\arg\max_{node \in candidate\_children} UCT(node,c_{current})$ 

\If{candidate\_node.visit\_count > 1 \texttt{and} candidate\_node.score <= $l_{low}$}{
selected\_node $\gets$ candidate\_node
}

current\_node $\gets$ candidate\_node


}



\COMMENTLLAMA{Expansion and Simulation}
% get $prob_{draft}, $ via inter-process communication\\
Get current state $s_t$ from root to selected\_node: \( s_t = (a_t, a_{t-1}, \dots, a_1,q) \) \\

\eIf{\textnormal{candidate\_node is root} }
{   
    Sample $d_i$ partial trajectories  $\left\{\boldsymbol{\tau}^{(i)}\right\}_{i=1}^{d_i} \sim \pi_{\theta}(\boldsymbol{\tau}\mid s_t, f^{(i)}),\quad f^{(i)} \subseteq \mathcal{F} $ \\
    \COMMENTLIGHTGRAY{Sample with random fewshot}
}
{
    Sample $d_e$ partial trajectories  $\left\{\boldsymbol{\tau}^{(i)}\right\}_{i=1}^{d_e} \sim \pi_{\theta}(\boldsymbol{\tau}\mid s_t, f^{(i)}),\quad f^{(i)} \subseteq \mathcal{F} $
}
\textbf{Split} $\left\{\boldsymbol{\tau}^{(i)}\right\}$ to multi steps 
$\left\{(a_{t+1}^{(i)}, a_{t+2}^{(i)}, \dots, a_{end}^{(i)}) \right\}$ and construct them as new branches of tree nodes $\left\{(node_{t+1}^{(i)}, node_{t+2}^{(i)}, \dots, node_{end}^{(i)}) \right\}$\\
\textbf{Append} these new branches to selected\_node \COMMENTLIGHTGRAY{Reserve all simulation results}

\COMMENTLLAMA{Backup}

Use verifier model $V_\phi$ to judge $\left\{\boldsymbol{\tau}^{(i)}\right\}$  \COMMENTLIGHTGRAY{Use LLM as verifer}

\textbf{Backup} score from newly expanded tree nodes using Monte Carlo Evaluation \\

}

\end{algorithm*}

% 这里可以补充一下，根据那些参数能采样到多少条数据？

The full FastMCTS algorithm is outlined in Algorithm \ref{algo:fastmcts}.


\section{Sampling Settings}
\label{apd:sampling settings}

For all our sampling settings, we use SGLang~\cite{DBLP:journals/corr/abs-2312-07104} as our inference engine and employ sampling generation with a temperature setting of 1 to ensure diversity. In FastMCTS, the constant $c$ in the UCT score is set to its default value of 1.414. Additionally, we utilize Qwen2.5-72B-Instruct as a LLM judger to verify the solutions. 

We use an asynchronous approach in our implementation, allowing different branches of the search tree to be processed concurrently. Although FastMCTS requires multiple iterations to construct a search tree for each problem, this parallel processing allows us to perform inference on a large number of inputs simultaneously, thereby ensuring high efficiency.

In section \ref{sec:efficiency}, to scale up the sampling computation, for FastMCTS, we incrementally increased the number of iterations from 4 to 20, and the expansion degree (i.e., the number of nodes expanded after the selection phase) is varied from 1 to 2. For Rejection Sampling, we expanded the number of generated trajectories per query from 3 to 32.

In section \ref{sec:performance}, to obtain comparable sampling computation, for each query in the original dataset, we sampled multiple times (30 for English data and 24 for Chinese data) using rejection sampling. For FastMCTS, it starts with an initial degree of 3 at the root, expands by adding 2 branches in each expansion phase, and performs 16 iterations of tree search.

\section{Training Data Construction}
\label{apd:data construct}
\paragraph{Supervised Fine-tuning}

After the sampling process, each problem is sampled with varying numbers of solution candidates. To investigate the impact of both training data size and the number of reasoning trajectories per problem, we impose constraints on the maximum number of solutions utilized per problem during the training process. This approach also helps maintain a balance between different problems.

For Rejection Sampling, we selecte correct trajectories for each problem randomly. For FastMCTS, our strategy involves prioritizing the selection of correct trajectories from various branches of the search tree. By doing so, we aim to maximize the diversity of the training data.
% The sizes of the supervised fine-tuning datasets obtained by both sampling methods and their corresponding training results are shown in Table \ref{tab:train res}.

\paragraph{Branch-DPO}

In addition to improving the efficiency of sampling correct reasoning paths, FastMCTS also provides step-level supervision information. Unlike rejection sampling, which generates multiple completely independent trajectories for each problem, FastMCTS constructs a search tree for each problem, where each node stores a score computed through Monte Carlo evaluation. This allows for step-level or branch-level preference optimization based on the scores of tree nodes.

% 这里是否需要放DPO公式？
% 附录是不是要放一下构造dpo数据的算法？因为如何构造dpo数据不算太重要？
Direct Preference Optimization (DPO)~\cite{DBLP:conf/nips/RafailovSMMEF23} has been widely adopted for model optimization due to its efficiency in utilizing pairwise preference data. It has also been applied to step-level preference optimization, as most undesirable trajectories do not initially contain errors~\cite{lai2024stepdpostepwisepreferenceoptimization,xie2024monte,DBLP:journals/corr/abs-2405-03553,wang2024towards}. 

We propose a simple algorithm to construct preference data from the tree structures generated by FastMCTS. Our approach is based on the following assumptions: 

1. For a multi-step reasoning trajectory, if the final result is correct and clear, all intermediate steps are considered correct.

2. If the final result is incorrect, the intermediate steps are not necessarily incorrect. 

However, if a step has been simulated multiple times and its Monte Carlo-estimated score remains zero, it can be considered a \textbf{"low-quality node."} Based on this, we construct step-level or branch-level preference data. For any node in the tree, we examine its child nodes. If a child node is identified as low-quality, we construct step-level preference data between this node and a high-quality node that has led to correct results. If the child branches contain both correct and incorrect results but have only been simulated once, we cannot definitively assess the quality of individual steps and instead construct branch-level preference data.

 In our experiments, for each search tree associated with one problem, we construct up to 5 step-level or branch-level preference pairs, resulting in an additional 152K(on CN High School Math Hard) and 215K(on En Math Hard) preference data points for DPO training. This approach further leverages the tree-structured data generated by FastMCTS.


\section{Training Setups}
\label{apd:training details}
We use Qwen2.5-7B as our base model and perform training on datasets generated by both FastMCTS and rejection sampling. For supervised fine-tuning, the maximum sequence length is set to 4096 tokens, and the global batch size is set to 32. We employ the Adam optimizer with a learning rate of 1e-5 and a linear warmup schedule with a warmup step ratio of 0.1. For all synthetic datasets, we train the model for 3 epochs and select the best checkpoint based on validation performance.

After supervised fine-tuning, we further refine the best checkpoint trained on FastMCTS-generated data using Branch-DPO for 3 epochs. The global batch size for Branch-DPO is set to 16, and the learning rate is set to 1e-6. The hyperparameter \(\beta\) is set to 0.4. We use the AdamW optimizer with a cosine learning rate scheduler and a warmup ratio of 0.1.

\end{document}
