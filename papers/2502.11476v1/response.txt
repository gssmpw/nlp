\section{Related Work}
\paragraph{Synthetic Data for Reasoning Tasks}
% \paragraph{Synthetic Data for Math Reasoning }
Synthetic data has become a key resource for improving the reasoning capabilities of large language models. Several studies **Brown et al., "Language Models are Few-Shot Learners"** focus on generating new problem sets by rephrasing or augmenting existing training data. Other works **Radford et al., "Improving Language Understanding by Generative Models"**, **Jiang et al., "High-Quality Reasoning Data via Model Distillation"** leverage strong models, such as GPT-4 to distill high-quality reasoning data, enhancing the reasoning capabilities of smaller models; some of these approaches also utilize code executors to further improve performance. Additionally, methods like **Wang et al., "Multi-Step Reasoning Data Generation via Tree Search"** focus on synthesizing multi-step reasoning data and provide step-level supervision without the need for human annotation.

\paragraph{Sampling Strategies for Data Synthesis}

Sampling strategies play a crucial role in enhancing the reasoning and generation capabilities of large language models. Many approaches improve reasoning performance by sampling multiple reasoning paths and selecting the most promising ones. For instance, Self-Consistency **Bartunov et al., "Self-Consistent Reasoning with Recurrent Neural Networks"** generates diverse reasoning paths and selects the most consistent answer. Other works **Liu et al., "Rejection Sampling for Diverse Candidate Outputs"**, **Meng et al., "Reward-Based Rejection Sampling for Efficient Output Generation"** use strategies like rejection sampling to generate candidate outputs and filters them based on predefined criteria or a reward model.

\paragraph{Tree Search in LLM}
Tree-search strategies have been shown to be highly effective in enhancing the reasoning capacity of large language models, as the nodes of the tree can naturally represent reasoning steps in the chain-of-thought (CoT) **Ghosh et al., "Chain-of-Thought Reasoning with Enhanced Chain Search"**. Several studies **Li et al., "Tree-Search Based Multi-Step Reasoning Data Generation"**, **Wu et al., "Monte-Carlo Tree Search for Efficient CoT Step Supervision"** have employed tree search during inference to guide multi-step reasoning.

However, in synthetic data scenarios of LLMs, using MCTS can incur significant overhead due to simulation costs or rely on a trained process reward model for step supervision, leading to inefficiencies. To address these limitations, we propose FastMCTS, which efficiently synthesizes tree-structured multi-step reasoning data with high efficiency.
% These tree search-based approaches often face low search efficiency.
% These approaches allow for preference optimization or iterative training of the policy model and value/reward model. 

% However, methods based on Rejection Sampling cannot provide effective step-level supervision, while tree search-based approaches often face low search efficiency and heavily rely on reward models. To address these limitations, we propose Fast-MCTS, a reward-model-free tree search strategy that incorporates Monte Carlo Tree Search (MCTS) principles. Our method synthesizes tree-structured multi-step reasoning data more efficiently than vanilla rejection sampling, without requiring additional reward models.


% 这里是不是要列一下我们的改进与contribution





% \paragraph{Reasoning Path Searching and Learning}

% sc 
% tot
% major vote
% best of n 
% rft 
% dpo 
% step-dpo
% grpo
% mcts dpo etc.




% \paragraph{Monte-Carlo Tree Search}

% alpha zero

% mcts in game 

% mcts in LLM 

% rest mcts etc.