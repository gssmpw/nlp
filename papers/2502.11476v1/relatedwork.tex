\section{Related Work}
\paragraph{Synthetic Data for Reasoning Tasks}
% \paragraph{Synthetic Data for Math Reasoning }
Synthetic data has become a key resource for improving the reasoning capabilities of large language models. Several studies~\cite{DBLP:conf/iclr/YuJSYLZKLWL24,DBLP:conf/iclr/XuSZG0FTLJ24} focus on generating new problem sets by rephrasing or augmenting existing training data. Other works~\cite{DBLP:journals/corr/abs-2306-02707,DBLP:journals/corr/abs-2403-04706} leverage strong models, such as GPT-4~\cite{Achiam2023GPT4TR}, to distill high-quality reasoning data, enhancing the reasoning capabilities of smaller models; some of these approaches also utilize code executors to further improve performance~\cite{, DBLP:journals/corr/abs-2309-05653,DBLP:conf/iclr/WangRZLLSZSZ024,DBLP:journals/corr/abs-2402-10176}. Additionally, methods like~\cite{DBLP:conf/acl/WangLSXDLCWS24, DBLP:journals/corr/abs-2406-06592,DBLP:conf/emnlp/WangLWLH0S24} focus on synthesizing multi-step reasoning data and provide step-level supervision without the need for human annotation.

\paragraph{Sampling Strategies for Data Synthesis}

Sampling strategies play a crucial role in enhancing the reasoning and generation capabilities of large language models. Many approaches improve reasoning performance by sampling multiple reasoning paths and selecting the most promising ones. For instance, Self-Consistency~\cite{DBLP:conf/iclr/0002WSLCNCZ23} generates diverse reasoning paths and selects the most consistent answer. Other works~\cite{yuan2023scalingrelationshiplearningmathematical,DBLP:journals/corr/abs-2402-10176,DBLP:journals/corr/abs-2407-13690} use strategies like rejection sampling~\cite{neal2003slice} to generates candidate outputs and filters them based on predefined criteria or a reward model. 
% These methods cannot provide effective step-level supervision.

\paragraph{Tree Search in LLM}
Tree-search strategies have been shown to be highly effective in enhancing the reasoning capacity of large language models, as the nodes of the tree can naturally represent reasoning steps in the chain-of-thought (CoT)~\cite{wei2022chain}. Several studies~\cite{yao2024tree,hao2023reasoning,zhang2024accessing,tian2024toward} have employed tree search during inference to guide multi-step reasoning.
% These methods typically prompt large language models to self-evaluate or use reward models to provide process-level supervision, employing depth-first search, breadth-first search, or Monte-Carlo Tree Search strategies. 
In another stream of research~\cite{feng2023alphazero,DBLP:journals/corr/abs-2405-03553,xie2024monte,zhang2024rest,wang2024towards}, Monte-Carlo Tree Search is used to generate tree-structured data for training, constructing preference data pairs or providing process supervision for CoT steps.

However, in synthetic data scenarios of LLMs, using MCTS can incur significant overhead due to simulation costs or rely on a trained process reward model for step supervision, leading to inefficiencies. To address these limitations, we propose FastMCTS, which efficiently synthesizes tree-structured multi-step reasoning data with high efficiency.
% These tree search-based approaches often face low search efficiency.
% These approaches allow for preference optimization or iterative training of the policy model and value/reward model. 

% However, methods based on Rejection Sampling cannot provide effective step-level supervision, while tree search-based approaches often face low search efficiency and heavily rely on reward models. To address these limitations, we propose Fast-MCTS, a reward-model-free tree search strategy that incorporates Monte Carlo Tree Search (MCTS) principles. Our method synthesizes tree-structured multi-step reasoning data more efficiently than vanilla rejection sampling, without requiring additional reward models.


% 这里是不是要列一下我们的改进与contribution





% \paragraph{Reasoning Path Searching and Learning}

% sc 
% tot
% major vote
% best of n 
% rft 
% dpo 
% step-dpo
% grpo
% mcts dpo etc.




% \paragraph{Monte-Carlo Tree Search}

% alpha zero

% mcts in game 

% mcts in LLM 

% rest mcts etc.