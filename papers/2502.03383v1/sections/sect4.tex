\vspace{-1em}
\section{Experiments}
\begin{figure*}[t]
    \centering
    % \vspace{-0.5em}
    \includegraphics[width=\linewidth]{imgs/all_results.png}
    % \vspace{-2.5em}
    \caption{
    \textbf{Top: Model performance on data with different number of covariates.
    }
    For both MOIRAI and MOIRAI-relu, we observe their performance behave like least squares.
    As in our construction, the longer the lookback size is, the more examples available for transformers to fit an $\mathtt{AR}$ model.
    Note that our test data has variance $\sigma^2 = 1$, thus the MSE for both models are expected to converge to $1$ as the lookback size increases.
    \textbf{Bottom: Generalization to unseen values of $d, q$.}
    From left to right, we have MOIRAI's generalization performance (pretrained on $d\in\{4,5\}, q\in\{4,5\}$) on high dimensional data ($d=10$), low dimensional data ($d=2$) and high lag step + low dimensional data ($d=3,q=7$).
    Note that high and low is compared with pretraining data.
    We observe that even when MOIRAI did not learn from any time series with $d=10$, it is still able to generalize well and shows even better sample complexity than least squares regression.
    Finally, even when both $q,d$ are unseen, it does not impact MOIRAI's ability to make accuracy predictions.
    }
    \label{fig:icl-results}
    % \vspace{-1em}
\end{figure*}

To verify our analysis, we first train transformers on synthetic datasets generated from $\mathtt{AR}$ process with different parameters.
The goal of this experiment is to verify the existence of a transformer that performs least squares regression on input time series with bounded lag window and number of covariates.
Next we study whether a pretrained transformer is capable of generalize such an ability to unseen values of $d, q$.
More empirical results are in \cref{appendix:additional-exp}.
% \vspace{-1em}
\subsection{Datasets}
\paragraph{Synthetic Data Generation.}
We generate the $\mathtt{AR}$ synthetic data similar to Equation~\eqref{eqn:AR-data} but use normalization to stabilize the values.
Consider a sequence of data 
$\bx \in \R^{d \times T} \coloneqq (\bx_1, \dots, \bx_T)$, where $\bx_t = (x_t^1, \cdots, x_t^d) \in \R^d$.
Assuming our target (variate of interest) is in dimension $1$, we generate our data as follows:
\begin{equation}
    x_{t}^1
    =
    \frac{1}{qd}
    \sum_{i=1}^q
    \sum_{j=1}^d
    a_i^j \cdot x_{t-i}^j
    + \epsilon_t
    ,
\end{equation}
where $\epsilon_t \sim N(0, \sigma^2)$, $a_i^j \sim N(0, 1) \in \R$.
We have $\sigma^2 \sim \text{unif}(0.1, 1)$.
After recursively generating the time series, we remove its first 50 time steps as burn out.
Each $\mathtt{AR}$ time series has the number of covariates between $1$ to $5$ and lag between $1$ to $5$.
For test data, we randomly generate one time series with $5k$ data points with $\sigma^2 = 1$, and evaluate our model on all time steps.
We set $q, d \leq 5$ in our experiments.
In total, we generate $100$ different time series with randomly sampled $d$ and $q$.
We also conduct experiments on synthetic data with seasonality, which can be found in the appendix.
% \vspace{-1em}
\paragraph{Model.}
We use MOIRAI-base, it is a $12$ layer MOIRAI transformer, with hidden dimension $768$.
% We use the pretraining pipeline as used in \cite{woo2024unified}.
The hyperparameters of this experiment can be found in \cref{table:hyperparameters}.
We use AdamW optimizer with linear warm ups.
We use MSE loss for pretraining, comparing to \cite{woo2024unified} using NLL loss, we choose MSE loss to simplify our settings.
% \vspace{-1em}
\paragraph{Training and Evaluation.}
For pretraining, we follow the standard MOIRAI pretraining but set the patch size as $1$ to minimize the impact of patch embedding.
During pretraining, each time series is randomly sampled, and the mask is randomly applied to each time step with probability $0.15$.
We evaluate the pretrained model on our test data with $d = \{3, 4, 5\}$, $q=5$ and $\sigma^2 = 1$ with different input length.
% \vspace{-2em}
\paragraph{Baselines.}
We compare MOIRAI with least squares regression performing different gradient descent steps.
For least squares regression, we assume $q$ is known.
When MOIRAI takes a $T$ length input, the least squares regression is trained on $T-1$ samples with each having $dq$ features.
A more detailed example on how we implement baselines is in \cref{appendix:ls-baseline}.
We also include the standard transformers and MOIRAI with ReLU replacing Softmax, which we term it as MOIRAI-relu.
For standard transformers, we keep the any-variate encoding but replace its attention with standard attention.
In \cite{woo2024unified}, without any-variate attention, the error of MOIRAI-small increases roughly $40\%$.
% \vspace{-2em}
\paragraph{Results.}
Since our test data generation process obeys noise variance = $1$, when fitting a linear model, the expected MSE will converge towards $1$ as lookback size increases.
Based on \cref{thm:any-variate-auto}, the length of input time series also corresponds to the number of examples model perform least squares on via gradient descent.
We observe that as the input length increases, the predictive error of MOIRAI decreases similarly to least squares, which verifies \cref{thm:any-variate-auto}.
Next, when pretrained on diverse dataset, pretrained MOIRAI is able to adapt to different number of covariates and perform least squares accordingly.
Further, when replace softmax with ReLU, the performance gap is negligible.
For standard transformer, while it also behaves similar to other models, it does present higher error comparing to other baselines, indicating the advantages of using any-variate attention.
% \vspace{-1em}
\subsection{Generalization to Unseen $d, q$}
% \vspace{-0.5em}
Here we are interested in whether a pretrained transformer is capable of generalizing to unseen values of $d$ and $q$.
Therefore, we train transformers (MOIRAI) on synthetic data generated with $\mathtt{AR}$ with $d \in \{4, 5\}$, and $q \in \{4, 5\}$.
In our construction, pretrained transformer is compatible with lower order and dimension $\mathtt{AR}$ data.
We evaluate the trained model on data with unseen values of $d$.
We select $d=2, d=10$, to represent the scenario when the number of covariates is lower and higher than pretraining data.
% \vspace{-1em}
\paragraph{Results.}
We observe that even when facing data with unseen number of covariates, MOIRAI is still capable of performing least squares regression effectively.
Note that for $d=10$, least squares require higher sample complexity to obtain similar performance to $d=5$ cases.
However, the pretrained MOIRAI is able to outperform it from such an aspect.
For $d=2$ all models perform well, again verifies our theoretical results.
Finally, when facing data with unseen both $d$ and $q$, it is still capable of performing well.