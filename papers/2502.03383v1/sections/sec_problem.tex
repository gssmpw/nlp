\paragraph{Notations.}
We use the following notation conventions. 
The vector-valued variable is given by boldfaced characters. 
We denote $[n]:=\{1,\ldots,n\}$ and $[i:j]:=\{i,i+1,\ldots, j\}$ for $i<j$. 
The universal constants are given by $C$ and are ad hoc. 
Considering a sequence of vectors $(\bx_1, \cdots, \bx_T)$, we use $\bx$ without index to represent the whole sequence, and $\bx_{i:j}$ represents $(\bx_i, \cdots, \bx_j)$ for $i < j$.
We impose periodic boundary conditions for the negative index, i.e., $\bx_{-1} = \bx_{T}$.
For a vector $\bfa v$ we denote $\Vert\bfa v\Vert_2$ as its $L_2$ norm. 
For a matrix $\bfa A\in  \R^{m\times n}$ we denote its operator norm as $\Vert\bfa A\Vert_2:=\sup_{\bfa v\in \mathbb{S}^{n-1}}\Vert\bfa A\bfa v\Vert_2$. 
Random variables are given by calligraphic characters $\cX$, and elements from a domain set are given by normal font $\text{x}$.
For more details, see Table~\ref{tab:nomenclature}.

% \vspace{-0.5em}
\section{Problem Setup}
This section describes our problem setup.
We introduce the architecture of transformer-based time series foundation models and how we construct our datasets.
% \vspace{-0.5em}
\subsection{Transformers}
We consider a sequence of $N$ input vectors $\{ h_i \}_{i=1}^N \subset \R^D$, where $ \bfa H \coloneqq \left[ h_1, \cdots, h_N \right] \in \R^{D \times N}$.
Given any $ \bfa H \in \R^{D \times N}$, we define the attention layer as follows.
\begin{definition}[Attention layer\normalfont]\label{def:attn}
    {\normalfont
    A self-attention layer with $M$ heads is denoted as $\text{Attn}^{\dagger}_{\bm{\theta}_0}(\cdot)$ with parameters $\bm{\theta}_0 = \{ (\bfa V_m), (\bfa Q_m), (\bfa K_m)  \}_{m \in [M]} \subset \R^{D \times D}$.
    The self-attention layer processes any given input sequence $\bfa H \in \R^{D \times N}$ as
    }
    \begin{align*}
    \text{Attn}_{\bm{\theta}_0}^{\dagger} 
    &\left( \bfa H \right)
    \coloneqq
    \bfa H + 
    \frac{1}{N}
    \sum_{m=1}^M
    (\bfa V_m \bfa H)
    \cdot
   % \CCC{\times} 
    \\
    &\sigma 
    \left( 
    \left( \bfa Q_m \bfa H \right)^\top 
    \left(\bfa K_m \bfa H \right) 
    \right),  
    % \hfill{\text{\R^{D \times N}}}
\end{align*}
where $\sigma \coloneqq t \mapsto \text{ReLU}(t)/N$.
% \CCC{We should not use times for matrix product. times is for outer product.}
\end{definition}
\paragraph{Any-variate Attention.}
Next, we introduce the any-variate attention, where \cite{woo2024unified} uses it to replace the standard attention in transformers.
The any-variate attention introduces two learnable variables: Attention Bias $u_1, u_2 \in \R$,
for disambiguation between variates.
\begin{definition}[Any-variate Attention.]\label{def:any-variate-attn}
    An any-variate attention layer with $M$ heads is denoted as $\text{Attn}_{\bm{\theta}_1}(\cdot)$ with parameters $\bm{\theta}_{1} = \{ (\bfa V_m), (\bfa Q_m), (\bfa K_m), (u_m^1), (u_m^2)  \}_{m \in [M]}$.
    With any input $H \in \R^{D \times N}$, we have
    \begin{align*}
    \text{Attn}_{\bm{\theta}_1} &\left( \bfa H \right)
    \coloneqq
    \bfa H + 
    \frac{1}{N}
    \sum_{m=1}^M
    (\bfa V_m \bfa H)
    \times
    \\
    &\sigma 
    \left( 
    \left( \bfa Q_m \bfa H \right)^\top 
    \left(\bfa K_m \bfa H \right) 
    +
    u_m^1 * \bU
    +
    u_m^2 * \bar{\bU}
    \right),  
    % \hfill{\text{\R^{D \times N}}}
\end{align*}
where $\sigma \coloneqq t \mapsto \text{ReLU}(t)/N$, $\bU \in \R^{N \times N}$ is a block diagonal matrix with block size $T\in \N^+$, such that each block consists of $1$s, $\bar{\bU} = \bI - \bU$, and $*$ denotes a constant multiply to all entries of a matrix.
\end{definition}

\begin{remark}
    In \cite{woo2024unified}, the attention score is calculated with the RoPE embedding \cite{su2024roformer}:
    \[
    \sigma 
    \left( 
    \left( \bfa Q_m \bfa H \right)^\top 
    \bfa R
    \left(\bfa K_m \bfa H \right) 
    +
    u_m^1 * \bU
    +
    u_m^2 * \bar{\bU}
    \right).  
    \]
    We omit the notation of rotary matrix $\bfa R$ as it is not learnable and is invertible and thus merged into $\bfa Q, \bfa K$ in our analysis.
\end{remark}

\begin{definition}[MLP Layer\normalfont]
    {\normalfont
    We denote an MLP layer with hidden state dimension $D^\prime$ as $\text{MLP}_{\bm{\theta}}(\cdot)$ with parameters $\bm{\theta}_2 = ( \bW_1, \bW_2 ) \in \R^{ D^\prime \times D } \times \R^{D \times D^\prime}$.
    The MLP layer processes any given input sequence $\bfa H \in \R^{D \times N}$ as
    }
    \begin{equation*}
        \text{MLP}_{\bm{\theta}_2} (\bfa H)
        \coloneqq
        \bfa H +
        \bW_2 \sigma(\bW_1 \bfa H).
    \end{equation*}
\end{definition}
Finally, we define a transformer with $L \geq 1$ layers, each consisting of any-variate attention and an MLP layer.
\begin{definition}[MOIRAI Transformer\normalfont]\label{def:moirai}
    {
    \normalfont
    We define the $L$-layer MOIRAI transformer \cite{woo2024unified}, $\text{TF}_{\bm{\theta}}(\cdot)$, as
    }
    \begin{equation*}
        \text{TF}_{\bm{\theta}}(\bfa H)
        =
        \text{MLP}_{\bm{\theta}_2^L}
        \left(
        \text{Attn}_{\bm{\theta}_1^L}
        \left(
        \cdot \cdot
        \text{MLP}_{\bm{\theta}_2^1}
        \left(
        \text{Attn}_{\bm{\theta}_1^1}
        (\bfa H)
        \right)
        \right)
        \right)
        .
    \end{equation*}
    Note that this transformer is equipped with any-variate attention instead of the standard attention.
    For transformers with standard attention, we denote it as 
    $\text{TF}_{\bm{\theta}}^{\dagger}(\cdot)$.
\end{definition}

We use $\bm{\theta}$ to denote the vectorization of all parameters in a transformer and super-index $\ell$ to denote the parameter of the $\ell$-th layer.
Thus, the parameter of a transformer is defined by
\begin{equation*}
        \bfa \theta = 
        \left\{
        \left\{
        \left(\{\bfa Q_m^\ell,\bfa K_m^\ell,\bfa V_m^\ell, u_m^{1, \ell}, u_m^{2, \ell}\}_{m\in[M]}, \bfa W_{1}^\ell,\bfa W_{2}^\ell
        \right)
        \right\}_{\ell\in[L]}
        \right\}.
\end{equation*}    

We denote the ``attention-only" transformers with $\bW_1^{(\ell)}, \bW_2^{(\ell)} = 0$, as $\text{TF}_{\bm{\theta}}^0(\cdot)$ for shorthand.
We define the following norm of a MOIRAI transformer as
\begin{align*}
    \Vert\bfa\theta\Vert_{op}
    \coloneqq
    \max_{\ell\in[L]}
    &\Big\{\max_{m\in[M^{\ell}]}
    \Big\{\Vert \bfa Q_m^\ell\Vert_2,\Vert\bfa K_m^\ell\Vert_2,
        \\
        \vert u_m^{1\ell} \vert , \vert u_m^{2\ell} \vert 
    \Big\}
    &+
    \sum_{m=1}^{M^{\ell}}\Vert\bfa V_m^\ell\Vert_2+\Vert\bfa W_1^{\ell}\Vert_2+\Vert\bfa W_2^{\ell}\Vert_2 \Big\}, 
\end{align*}
where $M^{\ell}$ is the number of heads of the $\ell$-th Attention layer.


\subsection{Data Generation}
Here, we first consider the case where we aim to find a multi-layered transformer that performs least squares regression via In-context learning (ICL).
Specifically, we assume our data is generated from an autoregressive process $\mathtt{AR}_d(q)$ as follows, where $q, d$ denotes the steps of lag and number of covariates, respectively. 
Consider a sequence of data 
$\bx \in \R^{d \times T} \coloneqq (\bx_1, \dots, \bx_T)$, where $\bx_t = (x_t^1, \cdots, x_t^d) \in \R^d$.
Assuming our target (variate of interest) is in dimension $1$, we assume the $\mathtt{AR}_d(q)$ process generates $x_t^1$ as follows:
\begin{equation}\label{eqn:AR-data}
    x_{t}^1
    =
    \sum_{i=1}^q
    \sum_{j=1}^d
    a_i^j \cdot x_{t-i}^j
    + \epsilon_t
    =
    \sum_{j=1}^d
    \langle \bw^j , \bx_{t-q: t-1}^j
    \rangle
    +
    \epsilon_t
    ,
\end{equation}
where $\epsilon_t \sim N(0, 1)$, $a_i^j \in \R^1$.
We denote the concatenation of all weights $\bw^\star = (\bw_1, \cdots, \bw^j)\in \R^{qd}$.
We assume bounded features $\norm{\bx_{t-q:t-1}}_2 \leq B_x$ , for all $t = 1,\cdots, T$.
The first equation writes the $\mathtt{AR}$ process in scalar form, and the second writes it in vector form.
In the following chapters, we will start by considering the uni-variate case ($\mathtt{AR}_1(q)$) and then move on to the multi-variate case ($\mathtt{AR}_d(q)$).
\vspace{-1.5em}
\paragraph{Problem Setup.}
Given a function class $\cF: \R^{d\times T}\mapsto \R$, our goal is to find a universal function $f\in\cF$ such that, given any time series generated from any arbitrary $\mathtt{AR}_d(q)$, its prediction error is bounded by some $\varepsilon \geq 0$, i.e.,
\[
\norm{f( \Tilde{\bx}) - x_{T}^1}_2
\leq
\varepsilon,
\]
where $\Tilde{\bx}$ denotes the time series $\bx$ with $x_T^1$ being masked.
\begin{remark}
    In the appendix, we show that even when the autoregressive process follows some non-linear relationship, there still exists a universal $f$ that predicts all non-linear AR process accurately.
\end{remark}
% \paragraph{Multivariate Autoregressive Process.}
% We extend the definition and notation for the \texttt{AR($q$)} process to the multivariate AR process, denoted as \texttt{MAR($q$)}.
% Consider a sequence of data $\bx \in \R^{d \times T} \coloneqq (x_1, \dots, x_T)$, we set the first dimension of $\bx$: $x_t^1$, for $t = 1, \cdots, T$ as target, and the other $d-1$ dimensions of $\bx$ as its covariates.
% We say $\bx$ is generated by a \texttt{MAR($q$)} process if the target of the sequence is generated by the following 
% \begin{equation}\label{eqn:MAR-data}
%     x_{t+1}^1
%     =
%     \sum_{i=1}^q
%     \sum_{j=1}^d
%     \langle
%     \bw_i^\star, x_i^j
%     \rangle
%     +
%     \epsilon_i,
% \end{equation}
% where $\bw_i^\star \in \R^d$ and $\epsilon_i \sim N(0, 1)$, for $i = 1, \cdots , q$,.
% Here we slightly abuse the notation in the \texttt{AR} process of $\bw^\star$, as we will be focusing on \texttt{MAR}($q$) in the rest of the paper.
