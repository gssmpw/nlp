% \vspace{-1em}
\section{Introduction}
% \CCC{We study the approximation and generalization of transformers as universal forecasters. (Perhaps we do not need a first sentence like this? we can just start with saying how important MOIRAI is.)} 
% A recent study \cite{woo2024unified} propose MOIRAI, the first universal forecaster, i.e., a large multi-variate time series foundation model.
% MOIRAI achieves the \emph{one-model-for-all-dataset} schema by solving two key challenges of building a time series foundation model:
% (a) It must be compatible with any arbitrary number of covariates;
% (b) It must be able to generalize well on unseen domains.
% MOIRAI not only addresses the above mentioned setbacks effectively, but also show impressive zero-shot performance on unseen domains, leading to its universality.
% Despite its empirical success, how does MOIRAI's XXX

The advancement of foundation models is reshaping the field of time series forecasting.
Recent studies demonstrate the empirical success of transformer-based time series foundation models \cite{woo2024unified, ansari2024chronos, liang2024foundation, das2023decoder}.
However, a theoretical understanding of how these models succeed is yet missing.
In this paper, we aim to provide a comprehensive analysis of time series foundation models, with a focus on transformer-based models.
We are interested in how these models achieve the \emph{one-model-for-all-datasets} paradigm in time series forecasting.
Specifically, our results cover both uni-variate \cite{ansari2024chronos, das2023decoder, rasul2023lag}, and multi-variate time series foundation models \cite{woo2024unified}\footnote{The model proposed by \cite{woo2024unified} is compatible with an arbitrary number of covariates}.


Our main discovery is twofold.
First, to address universality, we prove that there exists a transformer that fits an autoregressive model \cite{hamilton2020time, mills1990time} on any given uni-variate time series.
% Further, we show that the special design of MOIRAI allows transformers to further handle any arbitrary number of covariates, making it true universal for all multi-variate time series.
Furthermore, we show that the special design of MOIRAI allows transformers to further handle arbitrary number of covariates, making it process any dimension of time series in a principled way.
Second, to address learnability, we establish a generalization bound for pretraining when the data satisfies Dobrushin's condition \cite{Dobrushin1968TheDO, dobrushin1987completely}.
We refer to these two aspects as approximation and generalization, respectively, throughout the rest of this paper, as they form the theoretical foundation for the success of these models.

% To line up with practical scenarios, our theoretical results take MOIRAI's unique attention mechanism and encoding methods into considerations.
% Specifically, we show that those features of MOIRAI allows it to process any arbitrary number of covariates in a principle way.
% This discovery provides an explanation on MOIRAI's predictive power across different time series, justifying the design of MOIRAI.

Our approximation result is inspired by recent studies on in-context learning \cite{bai2024transformers, von2023transformers, li2023transformers, mahankali2023one, ahn2024transformers, zhang2024trained}. 
In-context learning refers to the ability of large foundation models to make accurate predictions for unseen tasks by observing training examples without parameter updates \cite{brown2020language, garg2022can}.
This capability explains how time series foundation models achieve their universality.
By treating the input time series as in-context examples, we show that transformers are able to implement gradient descent to estimate the parameters of the autoregressive model that best explains the given input.


% For generalization, we assume the model is pretrained on a large number of different time series, where each of them satisfies Dobrushin's condition.
% Next, when the data is generated by some autoregressive $\mathtt{AR}$ processes, we are able to further obtain a sharper rate, which reveals the trade-off between model complexity and training error.
% Finally, we present an application of our theorem on $\mathtt{AR}(1)$ process.
% To the best of the authors knowledge, our generalization bound is the first on time series foundation model pretraining.

% In our work, we delve into the theoretical and practical underpinnings of this universal forecastability by examining the core building blocks of MOIRAI. Specifically, we show the existence of a single model that can perform least-square regression across any time series dataset, and we establish pretraining-based generalization bounds to quantify its performance on unseen domains. This investigation clarifies how MOIRAI can guarantee consistent predictive power despite widely varying sources of data and domain-specific nuances. By presenting both a rigorous foundation and empirical insights, our paper sheds light on MOIRAIâ€™s unique ability to unify diverse time series tasks under a single, versatile architecture, offering a pathway for truly universal time series forecasting

% {\color{red}talk about anyvariate encoding, we aim to understand moirai...}

% Our approximation results are inspired by recent studies on in-context learning \cite{bai2024transformers, von2023transformers, li2023transformers, mahankali2023one, ahn2024transformers, zhang2024trained}. 
% In-context learning refers to the ability of large foundation models to make accurate predictions for unseen tasks by observing training examples without parameter updates \cite{brown2020language, garg2022can}. 
% This capability explains MOIRAI's zero-shot performance on unseen domains. 
% By treating the input time series as in-context examples, MOIRAI can infer underlying patterns and predict the next time step. 
% However, existing in-context learning theories fail to fully explain this due to two key challenges: (a) \CCC{difficulty in handling inputs concatenated into a single dimension, and (b) inability to process an arbitrary number of covariates.} \CCC{(these two challenges are not very clear. Perhaps we can write a short paragraph to explain why existing works cannot explain the success of MOIRAI?)}


% Most in-context learning theories rely on the assumption that the input feature and label and formatted into the same column.
% However, MOIRAI's encodes a multi-variate time series into a single long row vector, preventing XX.
% Next, icl theories only focus on a fixed feature dimension.
% Comparing to moirai, the data dimension can be any arbitrary number.


The contribution of this paper is threefold:
\begin{itemize}
    \item 
    From an algorithmic approximation perspective, we prove the existence of a transformer capable of fitting an autoregressive ($\mathtt{AR}$) model on any given uni-variate time series via gradient descent. 
    Extending this to the multi-variate setting, we show that a MOIRAI transformer can automatically adjust the dimensionality of the $\mathtt{AR}$ model to fit time series with an arbitrary number of covariates. 
    Our approximation results not only explain the strong performance of modern models across diverse datasets but also justify the design of MOIRAI.

    \item
    We present the first pretraining generalization bound for time series foundation models.
    We show that when the pretraining data satisfies Dobrushin's condition, the test error can be effectively bounded even when the data does not satisfy the i.i.d. assumption.
    Specifically, when pretraining MOIRAI on $n$ multi-variate time series, the test error decays by a factor of $\nicefrac{1}{\sqrt{n}}$.

    \item 
    Our experimental results match our theories by showing that the prediction error of transformers reduces as the input time series length increases, corresponding to our approximation result.
    % We also empirically show the trade-off between model complexity and test error, corresponding to our generalization bounds.
    
\end{itemize}



% \CCC{The following two paragraphs can be revised into the contribution bullets? }

% \CCC{Is it possible to move the current two paragraphs to contributions, and add a new paragraph before contributions to discuss the goal of the paper, and informally describe the problem setting we consider, highlighting how the problem setting captures the nature of the important tasks of ``process
% an arbitrary number of covariates'', etc? We also need to introduce the concept of ``approximation'' and ``generalization''}

% We demonstrate that MOIRAI's unique encoding and attention mechanisms effectively address key challenges (a) and (b), offering advantages over standard transformers.
% Specifically, we show that MOIRAI can perform variate-wise operations regardless of the number or length of covariates. 
% This advantage allows MOIRAI to perform least square regression on any multi-variate time series.
% Furthermore, when data is generated by autoregressive processes ($\mathtt{AR}$), we show that MOIRAI is capable of estimating the underlying parameter of any multi-variate $\mathtt{AR}$ processes.
% Notably, compared to existing theoretical results \cite{bai2024transformers} on in-context learning, our approximation rate differs by at best a constant factor in the required number of layers.

% For generalization, we demonstrate the learning bound for MOIRAI pretraining.
% When pretraining MOIRAI on a large volume of multi-variate time series satisfying the Dobrushin's condition \cite{Dobrushin1968TheDO}, we are able to obtain a generalization bound.
% Next, our generalization bounds only degrade by no more than log factors in the length of time series comparing to learning a standard transformer \cite{bai2024transformers}.
% Finally, we provide an example on pretraining a MOIRAI transformer on data generated by $\mathtt{AR}$ process with lag size $1$, where with appropriate setup, the test loss is bounded by factor of {\color{red}XXX}.


% The contribution of this paper is threefold
% \begin{itemize}
%     \item 
%     We are the first to provide a comprehensive analysis on large foundation models as universal forecasting.
%     We show that there exists a MOIRAI that learns the underlying pattern of a time series in-context, providing solid explanation on their empirical performance.

%     \item 
%     Our theoretical results gives profound insights to practice as we study the exact setup of moirai.

%     \item 
%     Our generalization bound for pretraining ...

% \end{itemize}


\paragraph{Organization.}
The organization of this paper is as follows:
Section 2 describes the problem setup; Section 3 provides the approximation result; Section 4 analyzes the generalization bound for pretraining; Section 5 reports the numerical simulations; and we leave discussions to Section 6.