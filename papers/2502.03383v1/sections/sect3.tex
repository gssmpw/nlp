\section{Generalization}
In this section, we investigate the generalization bound of pretraining transformer-based time series foundation models.
This section will focus on learning MOIRAI on multi-variate time series, one can easily adapt our proofs into learning uni-variate time series with standard transformers.

Let $\pi$ be a meta distribution, and each distribution drawn from it $ \mathtt{P}^{(T)} \sim \pi$, satisfies Dobrushin's condition \cite{Dobrushin1968TheDO} (which we will introduce shortly).
For pretraining data, we first sample $n$ distributions $\mathtt{P}_j^{(T)}$ i.i.d. from $\pi$, and for each distribution, we sample a time series $(\bx_{1j}, \cdots, \bx_{Tj})$, for $j \in[n]$, and each of them contains no more than $d$ covariates and with lag step no more than $q$.


For each time series, we encode it with any-variate encoding into an input matrix denoted as $\bH \in \R^{D \times N}$, \footnote{Due to any-variate encoding, $N = dT$.}
We define each pretraining sample as $\bz_j \coloneqq \left( \bH_j, y_j \right)$, where $y_j = \bx_{Tj}^1$.
We consider the squared loss between model prediction and the label, i.e.
{\small
\begin{equation*}
    \ell( \bz_t, \bm{\theta} )
    \coloneqq
    \frac{1}{2}
    \Bigg[
    y_t
    -
    \mathtt{Clip}_{B_x}
    \bigg(
    \mathtt{read}_y
    \Big(
    \text{TF}_{\bm{\theta}}^R
    \left(
    \bH
    \right)
    \Big)
    \bigg)
    \Bigg]^2,
\end{equation*}
}
where 
$\mathtt{Clip}_{B_x}(t) \coloneqq \max\{ \min \{ t, B_x \}, -B_x \}$, and $\text{TF}_{\bm{\theta}}^R$ is the MOIRAI transformer defined in \cref{def:moirai} with $\mathtt{Clip}(\cdot)$ applied after each layer.
The pretraining loss and test loss is defined as the following:
\begin{equation}\label{eqn:icl-loss}
    \hat{L}( \bm{\theta} )
    \coloneqq
    \frac{1}{nT}
    \sum_{t=1}^T
    \sum_{j=1}^n
    \ell( \bm{\theta}, \bz_{jt} ),
    \;
    L( \bm{\theta} )
    \coloneqq
    \mathbb{E}_{\bz,\mathtt{P}^{(T)}}
    \left[
    \ell(  \bm{\theta}, \bz )
    \right].
\end{equation}
% where the expectation of test loss is taken over $\mathtt{P}_j^{(T)} \sim \pi, \bz \sim \mathtt{P}^{(T)}_j$.
The goal of our pretraining algorithm is to find an empirical risk minimizer (ERM) over MOIRAI transformers with $L$ layers, $M$ heads, and norm bounded by $B$:
\begin{align}\label{eqn:parameter-regime}
    &\hat{\bm{\theta}}
    \coloneqq
    \underset{ \bm{\theta} \in \Theta_{L,M,D^\prime, B} }{\argmin}
    \hat{L}
    (\bm{\theta}),
    \\
    &\Theta_{L,M,D^\prime,B}
    \coloneqq
    \Bigg\{
    \bm{\theta}
    =
    \left(
    \bm{\theta}_1^{(1:L)},
    \bm{\theta}_2^{(1:L)}
    \right)
    :
    \\
    \max_{\ell\in[L]}
    &M^{(\ell)} \leq M
    , 
    \quad
    \max_{\ell\in[L]}
    % \underset{\ell\in[L]}{\max}
    D^{(\ell)} \leq D^\prime
    ,
    \quad
    \Vert\bfa\theta\Vert_{op}
    \leq 
    B
    \Bigg\}.
\end{align}
% \vspace{-1em}
\subsection{Weakly-Dependent Time Series}
In this scenario, we consider the training data $\bx$ to be drawn from a distribution $\mathtt{P}$ satisfying Dobrushin's condition.
Under this condition, we are able to present several generalization bounds on pretraining.


\begin{definition}[Influence in high dimensional distributions]\label{def:influence}
    Let $\cX = (\cX_1, \cdots , \cX_T)$ be a sequence of random variables over $\cD_{\cX}^{T}$.
    The influence of variable $\cX_j$ on variable $\cX_i$ is defined as
    \begin{align*}
        &\bI_{j \rightarrow i}(\cX)
        \coloneqq
        \max_{ \text{x}_{ -i-j}, \text{x}_j, \text{x}_j^\prime  }
        \\
        % \underset{  z_{-i-j} \in Z^{m-2} }{ z_j, z_j^\prime \in Z}}
        &
        \norm{
        P_{\cX_i | \cX_{-i}} \left( \cdot | \text{x}_{-i-j}, \text{x}_j \right),
        P_{\cX_i | \cX_{-i}} \left( \cdot | \text{x}_{-i-j}, \text{x}_j^\prime \right)
        }_{\texttt{TV}}
        ,
    \end{align*}
    where $\text{x}_{-i-j} \in \cD_{\cX}^{T-2}, \text{x}_j, \text{x}_j^\prime \in \cD_{\cX}$, $\norm{\cdot}_{\texttt{TV}}$ denotes the total variation distance, and $\text{x}_{-i}$ represents the vector \textbf{x} after omitting the $i$-th element.
\end{definition}



\begin{definition}[Dobrushin's Uniqueness Condition]
    Consider a random variable $\cX$ over $\cD_{\cX}^T$.
    The Dobrushin coefficient of $\cX$ is defined as 
    \[
    \alpha(\cX) \coloneqq  \max_{1\leq i \leq T} \sum_{j \neq i} \bI_{j \rightarrow i} (\cX).
    \]
    We say the variable satisfies Dobrushin's uniqueness condition if $\alpha(\cX) < 1$.
    For a distribution $\mathtt{P}$, we denote $\alpha(\mathtt{P}) = \sup_{\cX\sim\mathtt{P}} \alpha(\cX)$.
\end{definition}
\begin{definition}[Log Dobrushin's Coefficients]
        Let $\cX = (\cX_1, \cdots, \cX_T)$ be a random variable over $\cD_{\cX}^T$ and let $\mathtt{P}_z$ denote its density.
        Assume that $\mathtt{P}_{z} > 0$ on all $\Omega^T$.
        For any $i \neq j \in [T]$, the log influence between $j$ and $i$ is defined as:
    {\small
    \begin{equation*}
        I^{\log}_{j, i}(\cX)
        =
        \frac{1}{4}
        \sup
        \log
        \frac{ P
        \left[
        \text{x}_i, \text{x}_j, \text{x}_{-i-j}
        \right] 
        P
        \left[
        \text{x}_i^\prime, \text{x}_j^\prime, \text{x}_{-i-j}
        \right]
        }{
    P
        \left[
        \text{x}_i^\prime, \text{x}_j, \text{x}_{-i-j}
        \right] 
    P
        \left[
        \text{x}_i, \text{x}_j^\prime, \text{x}_{-i-j}
        \right] 
        },
    \end{equation*}
    }
    where the $\sup$ is taken over $\text{x}_{-i-j}, \text{x}_i, \text{x}_i^\prime, \text{x}_j, \text{x}_j^\prime$,
    and the log-coefficient of $\cX$ is defined as $\alpha_{\log}(\cX) = \max_{i \in [T]} \sum_{j \neq i} I^{\log}_{j, i}(\cX)$.
\end{definition}
The coefficient $\alpha(\cdot)$ has a natural bound $0 \leq \alpha(\cdot) \leq T-1$, with $\alpha = 0$, the data reduces to the i.i.d. case.

\begin{remark}
    Dobrushin's condition characterizes a class of distributions whose dependency is mild. 
    However, our empirical evaluation suggests that in certain situations where Dobrushin's condition fails to hold, the Transformers can perform prediction well.
\end{remark}
\subsection{Generalization Bounds of MOIRAI}

\begin{theorem}[Pretraining Generalization Bound]\label{thm:gen-bound-1}
    Let $\Theta_{L,M,D^\prime, B}$ be the parameter space defined in 
    Equation~\ref{eqn:parameter-regime}.
    Assume $\alpha_{\log}( \mathtt{P}^{(T)}) < 1/2$.
    Then with probability at least $1 - \varepsilon$, ERM $\hat{\bm{\theta}}$ satisfies the following:
    \begin{align*}
    L(\hat{\bm{\theta}})
    &\leq 
    \inf_{\bm{\theta} \in \Theta_{L,M,D^\prime, B}} L(\bm{\theta})
    +
    \\
    &
    O
    \left(
    \frac{B_x^2}{1 - \alpha(\mathtt{P}^{(T)}) }
    \sqrt{
    \frac{
    L(MD^2 + D D^\prime) \zeta + \log(\nicefrac{1}{\varepsilon})
    }{n}
    }
    \right),
    \end{align*}
    % where $\iota = \log( 2 + 2(L B_H^{L-1} B_{\Theta}) B \frac{1-\alpha}{B_x})$
    where $C$ is an universal constant, and $\zeta = O(\log(2 + \max \{ B, \mathtt{R}, B_x, T, d \}$. 
\end{theorem}

The proof is in \cref{proof:gen-bound-1}.
Note that when $\alpha(\mathtt{P}) = 0$, the data becomes i.i.d., where the only difference between our generalization and one proposed in \cite{bai2024transformers} is the complexity term. 
The complexity of MOIRAI and standard transformers differs as the complexity of MOIRAI also dependents on the time series length ($T$).
Further, in \cref{thm:gen-bound-1}, we do not assume our data is generated from the $\mathtt{AR}$ process, only its Dobrushin coefficient.
When the data is generated by the $\mathtt{AR}$ process, we are able to give a more explicit bound on the same test loss as described below.

\begin{corollary}[Test Error Bound]\label{thm:test-error-bound-1}
    Following the setup in \cref{thm:gen-bound-1},
    if pretraining samples are generated by some $\mathtt{AR}_d(q)$ process with noise sampled from $N(0, \sigma^2_\epsilon)$\footnote{Here we assume fixed $d, q$ across all samples as one can describe a lower dimension/order $\mathtt{AR}$ process with zero coefficients.}, 
    then with probability $ \Delta(1 - \varepsilon)$, ERM $\hat{\bm{\theta}}$ satisfies the following:
    \begin{align*}
    L(\hat{\bm{\theta}})
    &
    \leq
    O
    \Bigg(
    B_x B_w \exp \left( \frac{-L}{\kappa} \right)
    +
    \\
    &
    \frac{B_x^2}{1 - \alpha(\mathtt{P}^{(T)})}
    \sqrt{ \frac{L(MD^2 + D D^\prime ) \zeta + \log (1 / \varepsilon)}{n} }
    \Bigg).
    \end{align*}
    where $\Delta = O\left(1 - \left(  \nicefrac{\sigma_\epsilon}{B_x  B_w e^{\nicefrac{-L}{2\kappa}}}  \right)^2  \right)$, $C$ is an universal constant, and $\zeta = O(\log(2 + \max \{ B, \mathtt{R}, B_x, T, d \})$. 
\end{corollary}
\begin{remark}
Considering the model parameters ($M,D,D^\prime, d$) are of constant level, one is able to further optimize the bound to
$L(\hat{\bm{\theta}}) \lesssim  n^{-\nicefrac{1}{2}}$, by selecting $L$ appropriately.    
\end{remark}

% \begin{theorem}[Multi-Path Generalization Bound]
%     Let $L_{\mathtt{T}}( \bm{\theta})$, $\hat{L}_{\mathtt{T}})\bm{\theta})$ be the test and empirical loss over samples drawn from a target distribution $\mathtt{T}$.
%     Let $\lambda_k = \inf L_k( \bm{\theta})$, where the $\inf$ is taken over all $\bm{\theta} \in \Theta_{L,M,D^\prime, B}$.
%     Following the constructions above, we have
%     \begin{equation}
%         L_{\mathtt{T}}(\hat{\bm{\theta}})
%         \leq 
%         \frac{1}{K}
%         \sum_{k=1}^K
%         \left(
%         \hat{L}_{\mathtt{T}}( \hat{\bm{\theta}})
%         +
%         C_1 \cdot
%         \hat{\mathfrak{G}}
%         +   
%         \lambda_K
%         \right)
%         +
%         \cdots,
%     \end{equation}
%     where $\lambda_K$ is the average risk of the optimal hypothesis on the combined source and target domain,  $C_1, C_2$ are universal constants whenever $\nicefrac{1}{2} - \alpha_{\log}(D_S^k)$ is bounded away from zero, and $\hat{\mathfrak{G}}$ is the empirical Gaussian complexity.
% \end{theorem}


\subsection{Example: Stationary $\mathtt{AR}(1)$}\label{sec:AR1}
Here we provide an example of the application of \cref{thm:test-error-bound-1} on $\mathtt{AR}(1)$ process with the following form
\begin{equation*}
    \bx_{t+1}
    =
    \langle \bw, \bx_{t} \rangle + \epsilon_t, \quad \epsilon \sim N(0, \sigma_{\epsilon}^2),
\end{equation*}
where $\bx_t \in \R^d$, $\bw \in \R^d, \epsilon \in \R$ and $\by_{t+1} = \bx_{t+1}^1$.

To satisfy the condition of $\alpha( \mathtt{P} ) < \frac{1}{2}$, we assume the following holds
\begin{equation}\label{eqn:condition-weakly-dependent-AR1}
    B_x^2 < \ln \frac{1}{2} + ( \sigma_{\epsilon}^2 ),
    \quad  \norm{\bw}_{\infty} < 1.
\end{equation}
The first condition comes from the fact that we require the pair-wise potential of this time series to be less than $1/2$ (For more details, see \cref{appendix:analysis-ar1}).
The second condition comes from the requirement of it being stationary.
\begin{proposition}[Generalization Bound for Any-Variate Transformer on $\mathtt{AR}(1)$]\label{proposition:ar1}
    Considering an $\mathtt{AR}(1)$ process with Dobrushin's coefficient bounded by $1/2$.
    With probability at least $\delta(1 - \varepsilon)$, ERM $\hat{\bm{\theta}}$ satisfies the following:
    \begin{align*}
    L(\hat{\bm{\theta}})
    &=
    O
    \Bigg(
    \frac{\sigma_\epsilon}{\sqrt{1 - \delta}}
    +
    \frac{\sigma_\epsilon^2}{B_x}
    \exp \left( \frac{-L}{\kappa} \right)
    +
    \\
    &
    \frac{\sigma_\epsilon^2}{1 - \alpha( \mathtt{AR}(1) )}
    \sqrt{ \frac{L(MD^2 + D D^\prime) \zeta + \log (1 / \varepsilon)}{n} }
    \Bigg).
    \end{align*}
    where $\zeta = O(\log(2 + \max \{ B, \mathtt{R}, B_x, d \})$. 
\end{proposition}
If we further optimize the bound by viewing the hyperparameters as constants, the test error obeys $O(e^{-L} + \sqrt{\frac{L}{n}})$ with high probability whenever $\sigma_\epsilon$ is small.
