\section{Related Works}\label{sec:related-work}

\paragraph{Time Series Foundation Models.}
The recent progress in foundation models \cite{touvron2023llama, brown2020language, rai2024strategies} has begun to reshape the field of time series forecasting, a critical task of predicting the future based on history \cite{hamilton2020time}.
However, there are two major challenges in building a time series foundation model:
(a) the model must be able to handle an arbitrary number of covariates, and
(b) the model must generalize to unseen time series domains.
To circumvent (a), several studies simplify the task by considering only univariate time series \cite{ansari2024chronos, rasul2023lag, das2023decoder}.
\cite{das2023decoder} propose a decoder-only transformer pretrained on both real and synthetic datasets.
\cite{rasul2023lag} incorporate lag features and the Llama architecture to pretrain a large uni-variate time series foundation model.
\cite{ansari2024chronos} leverage the power of large language models (LLMs) by using pretrained LLMs backbones.


Recently, \cite{woo2024unified} proposed MOIRAI, the first time series foundation model capable of handling an arbitrary number of covariates.
It addresses (a) by concatenating all covariates into a uni-dimensional sequence, ensuring a consistent input dimension across datasets.
It addresses (b) by pretraining on a large collection of time series datasets \cite{godahewa2021monash, alexandrov2020gluonts, wu2021autoformer, lai2018modeling} spanning domains such as weather, traffic, electricity, and industry.
MOIRAI not only generalizes across a wide range of domains, but its \emph{zero-shot} performance also surpasses several strong supervised learning baselines \cite{liu2023itransformer, nie2022time, zhang2023crossformer}.
However, the machine learning community has yet to provide a suitable explanation for MOIRAIâ€™s impressive performance.
Therefore, this paper is the first to offer theoretical guarantees for MOIRAI as a time series foundation model.



\paragraph{In-Context Learning.}
In-context learning (ICL) is an emerging capability of large foundation models, enabling them to learn diverse and unseen tasks from given examples.
\cite{brown2020language} first provide empirical evidence of ICL in large language models (LLMs); by presenting several examples of 
$(\bx,\by)$ pairs, GPT-3 effectively infers the relationship between $\bx$ and $\by$.
\cite{garg2022can} then conduct quantitative experiments on simple function classes, such as linear regression.
Their results demonstrate that large foundation models can learn the parameters of these function classes.
Subsequently, several theoretical studies \cite{bai2024transformers, von2023transformers, ahn2024transformers, akyrek2023what} have proven that different types of transformers can implement algorithms such as gradient descent.
This discovery provides a theoretical foundation for the empirical findings in \cite{garg2022can}.


The closest studies to this paper are \cite{nichani2024transformers, sander2024transformers}.
However, \cite{sander2024transformers} examines ICL in the context of next-token prediction using a linear transformer.
While their theoretical results relate to in-context learning on sequential data, they are insufficient to explain transformers' success in time series forecasting.
\cite{nichani2024transformers} explores another case where the data is modeled as a Markov chain generated by a transition matrix.
They demonstrate the existence of induction heads that enable transformers to perform next-token prediction.
However, their scenario does not align with multivariate time series, which is where our main contribution lies.

