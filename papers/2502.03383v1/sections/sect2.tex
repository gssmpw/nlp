% \vspace{-0.5em}
\section{Approximation}
We study the algorithmic approximation perspective of transformer-based time series foundation models.
We first investigate transformers as uni-variate time series foundation models as a warm-up.
Next, we will move on to MOIRAI \cite{woo2024unified} and analyze how its unique design and pre-processing methods enable its universality.
% construct multi-layer transformers to implement least square regression on data (1) generated by autoregressive process, and (2) structured in the form of any-variate encoding.
% \vspace{-0.5em}
\subsection{Warm Up: Autoregressive Regression}
We start our analysis with a warm-up example on the $\mathtt{AR}_1(q)$ model.
We show that standard transformers are capable of performing gradient descent via in-context learning on autoregressive data.
Here, we consider an input sequence with the following form
\begin{align}\label{eqn:input-data}
    \bfa H 
    &\coloneqq
    \begin{bmatrix}
        x_1 & x_2 &  \dots & x_{T} & 0
        \\
        \bp_1 & \bp_2 & \dots & \bp_{T} &
        \bp_{T+1}
    \end{bmatrix}
    \in \R^{D \times (T+1)}
    ,
    \\
    \bp_i
    &\coloneqq
    \begin{bmatrix}
        \mathbf{0}_{d^\prime}
        \\
        \be_i
        \\
        1
        \\
        1\{ i < {T} \}
    \end{bmatrix}
    \in \R^{d^\prime + T + 3}
    ,
\end{align}
where $\be_i$ is an one-hot vector with 1 at the $i$-th entry, and $d^\prime + T + 3 = D$.
Here, our goal is to predict $x_{T}$.
\begin{remark}\label{remark:format-assumption}
    Most in-context learning studies \cite{akyrek2023what, bai2024transformers, li2023transformers} make an assumption on the input data, where they assume it is formatted with features and labels in the same column, i.e.,
\begin{equation}\label{eqn:format-assumption}
\begin{bmatrix}
    \bx_1 &  \bx_2 & \dots & \bx_{N} 
    \\ 
    \yb_1 & \by_2 & \dots & \by_{N}
    \\
    \pb_1 & \pb_2 & \dots & \pb_{N}
\end{bmatrix}.
\end{equation}
In contrast, we adopt a natural approach that leverages the raw structure of the data, particularly for the $\mathtt{AR}_d(q)$ process. 
In this setting, each time stepâ€™s label also serves as a feature for future steps. 
Further, the unknown value of $q$ complicates the task of achieving such a format in Equation~\eqref{eqn:format-assumption}.
\end{remark}

Our next lemma shows that transformers are indeed capable of reformatting $\bfa H$ into the form of Equation~\ref{eqn:format-assumption}.
Notably, the following lemma relaxes the assumption in Remark~\ref{remark:format-assumption} of previous studies as well.

\begin{lemma}\label{lem:input-causal}
    Given a sequence of token $\bfa H$ in the form of Equation~\ref{eqn:input-data}, there exists a one-layer, $q_{\max}$ head attention layer, such that for any $q \leq q_{\max}$, the columns of $\text{Attn}_{\bm{\theta}}^{\dagger}( \bfa H )$ has the following form:
    \begin{equation}
    \text{Attn}_{\bm{\theta}_1}^{\dagger}( \bfa H )_i
    \coloneqq
        \begin{bmatrix}
            x_i
            \\
            x_{i-1}
            \\
            \vdots
            \\
            x_{i-q}
            \\
            \bp_i^\prime
        \end{bmatrix},
        \quad
        \bp_i^\prime 
        \coloneqq
        \begin{bmatrix}
        \mathbf{0}_{ d^\prime - q }
        \\
        \be_i
        \\
        1
        \\
        1 \{ i < T \}
        \end{bmatrix}.
    \end{equation}
\end{lemma}
The proof is in \cref{proof:lem-input-casual}.
\cref{lem:input-causal} is crucial in our analysis as it connects the theoretical results in ICL \cite{bai2024transformers} to uni-variate time series forecasting.
% The implication of \cref{lem:input-causal} is highly related to the results in \cite{bai2024transformers}.
When data formats in the form of Equation~\eqref{eqn:format-assumption}, \cite{bai2024transformers} show that there exists a multi-layer transformer that performs linear regression via gradient descent on the first $N-1$ data points and evaluates the $N$-th one.
Thus, \cref{lem:input-causal} implies transformers are also capable of performing linear regression on time series data, which we present in the following paragraph.

This lemma applies to both any-variate attention and standard attention, as the latter can be viewed as a special case of any-variate attention by setting $u^1, u^2 = 0$.
Additionally, the construction of a single layer with $q$ heads is not a strict requirement; the lemma also holds for $c$ layers of  $\frac{q}{c}$ head attention, for any $c$ satisfies $\frac{q}{c} >= 2$.

With Lemma~\ref{lem:input-causal}, we are able to apply the in-context learning results in \cite{bai2024transformers} on the $\mathtt{AR}_1(q)$ case.
Consider the data generated by the $\mathtt{AR}$ process in Equation~\ref{eqn:AR-data}.
Given an input time series $\bx \in \R^{ d \times T}$, we define the least squares estimator as the empirical risk minimizer over the time series, i.e.,
{\small
\begin{align*}
    \ell_{\text{reg}}(\bw, \bx_{t-1:t-q}  )
    &\coloneqq
    \frac{
    \left[
    \langle
    \bw ,
    [ \bx^1_{t-1:t-q} ; \dots ; \bx^d_{t-1:t-q}]
    \rangle
    -
    x_t^1
    \right]^2
    }{2}
    \\
    L_{\text{reg}}
    (\bw, \bx)
    &\coloneqq
    \frac{1}{T-1}
    \sum_{t=1}^{T-1}
    \ell_{\text{reg}}
    \left(
    \bw, \bx_{t-1:t-q}
    \right)
    \\
    \hat{\bw}_{\text{ERM}} 
    &\coloneqq
    \argmin_{\bw\in\R^{dq}}
    \;
    L_{\text{reg}}
    \left(
    \bw, \bx
    \right),
\end{align*}
}
where $[ \bv ; \bu]$ denotes the concatenation between vectors, as $[\bx^1_{t-1:t-q} ; \bx^2_{t-1:t-q}] = ( \bx^1_{t-1},  \bx^1_{t-2},  \cdots, \bx^2_{t-q+1}, \bx^2_{t-q} ) \in \R^{2q}$, $\tilde{\bx}$ denotes masking out the last time step of the target variate, and $L_{reg}$ is a loss, which is $\alpha$-strongly convex, and $\beta$-smooth over $\R^{dq}$.
We make the following assumption and then present our first result on uni-variate time series ($d=1$).
\begin{assumption}\label{assumption:effective-regression}
    The regression problem above $\hat{\bw}_{\text{ERM}}$ is well-conditioned and has a bounded solution.
\end{assumption}
\begin{proposition}[Uni-variate Autoregressive Regression via Transformers]
    Assume \cref{assumption:effective-regression} holds and fix a $q_{\max} > 0$.
    For any $0 \leq \alpha \leq \beta$ with $\kappa \coloneqq \frac{\beta}{\alpha}$, $B_w > 0$, and $\epsilon < B_x B_w / 2$,
    there exists a $L$-layer transformer
     $\text{TF}_{\bm{\theta}}^{0 \dagger} 
    \left( \cdot \right)$, with
    \begin{align*}
        L = L_1 + L_2, \quad
        L_1& = \lceil 2 \kappa \log( \frac{B_x B_w}{2 \epsilon} ) \rceil,
        \quad
        L_2 = \lceil \frac{q_{\max}}{3} \rceil,
        \\
        \text{max}_{\ell \in [L]} 
        M^{(\ell)} 
        &\leq 3,
        \quad
        \norm{ \bm{\theta} }_{\text{op}}
        \leq
        | 4R + 8\beta^{-1} |
        ,
    \end{align*}
    ($R \coloneqq \text{max}\{ B_x B_w, B_x, 1 \}$), the following holds.
    On any input data $\bx$ generated by any $\mathtt{AR}_1(q)$ process such that
    \begin{equation}
        0 < q \leq q_{\max}
        \quad
       \norm{ \hat{\bw}_{\text{ERM}} }_2
       \leq 
       \frac{B_w}{2},
    \end{equation}
    we have
    \begin{equation}
        \lVert
        \hat{\bx}_{T}
        -
        \left\langle
        \hat{\bw}_{\text{ERM}}, 
        [
        \bx_{t-1:t-q}^1 ; \dots 
        ; \bx_{t-1:t-q}^d
        ]
        \right\rangle
        \rVert
        \leq 
        \epsilon,
    \end{equation}
    where 
    $\hat{\bx}_{T} = \mathtt{read}(\text{TF}_{\bm{\theta}}^{0 \dagger}
    \left( \bfa H \right))$.
    The $\mathtt{read}(\bfa H)$ operation reads out the first entry of $T$-th column of $\bfa H$.
\end{proposition}
This proposition follows immediately from \cref{lem:input-causal} and \citep[Theorem~4]{bai2024transformers}.
The above result applies for MOIRAI with $u^1_m, u^2_m = 0$ in all heads and layers.
Further, one can replace the least squares ERM with lasso or ridge ERM and obtain a similar result by applying Theorem 4, 7, and 13 of \cite{bai2024transformers}.

So far, we show that transformers are capable of solving uni-variate autoregressive regression with, at best, one additional layer compared to the results in \cite{bai2024transformers}.
The result above provides insights on transformer-based uni-variate time series foundation models \cite{ansari2024chronos, rasul2023lag, das2023decoder}.
To study MOIRAI, we then include two ingredients into our analysis: the \textit{any-variate encoding} and the \textit{covariates} in the following chapters.

\subsection{Approximation Error of MOIRAI Transformer}
In this subsection, we extend our results to the multivariate autoregressive process ($d > 1$) and the encoding method of MOIRAI.
Note that in the multi-variate case, we only focus on MOIRAI as it is the only transformer-based model that is compatible with arbitrary number of covariates. 
We start by introducing the any-variate encoding.
\paragraph{Any-Variate Encoding.}
\cite{woo2024unified} propose to flatten a $d$-dimensional time series, $\bx \in \R^{d \times T}$, into a $1$-dimensional sequence, i.e., $\bx^\prime \in \R^{1 \times Td}$.
This operation transforms time series with arbitrary number of covariates ($d$), into a long sequence with fixed dimension, enabling consistent input dimension for transformers.
Following the flattening operation, \cite{woo2024unified} also proposes to add two types of indices into the input sequence: the time and variate ID.
We term the above operations as the any-variate encoding, which transforms a multivariate sequence $\bx \in \R^{d \times T}$, as follows:
{    \small
\begin{equation}\label{eqn:AV-encoding}
    \begin{bmatrix}
        x_1^1 & \cdots & x_T^1 
        \\
         x_1^2 & \cdots & x_T^2
         \\
         \vdots & \vdots & \vdots 
         \\
        x_1^d & \cdots & x_T^d    
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        x_1^1 & \cdots & \cellcolor{cyan!20} x_{T}^1 &  \cdots & x_1^d & \cdots & x_{T}^d
        \\
        \bp_1 & \cdots & \bp_{T} &  \cdots & \bp_1 & \cdots & \bp_T
        \\
        \be_1 & \cdots & \be_1 &  \cdots & \be_d & \cdots & \be_d
    \end{bmatrix},
\end{equation}}
where $\be_i$ is the variate index, a one-hot vector with $i$-th entry being $1$, and $\bp_i$ is the time index, which is defined the same as Equation~\eqref{eqn:input-data}.
This is without loss of generality because the discrete-time and variate ID used in \cite{woo2024unified} can be easily transformed into a high-dimensional vector with the embedding layer.
Note that only the target variate has length $T$, we highlight $x_{T}^1$ as it is our prediction target and will be masked as $0$.
% The variate index is defined as
% \begin{equation*}
%     \bp_i
%     \coloneqq
%     \begin{bmatrix}
%         \mathbf{0}_{d^\prime}
%         \\
%         \be_i
%         \\
%         1
%         \\
%         1\{ i < {T+1} \}
%     \end{bmatrix}.
% \end{equation*}

Now we define the history matrix $\mathtt{A}_i(q) \in \R^{q+1 \times T}$ for the $i$-th covariates $(x_1^i, \cdots, x_T^i)$, with order $q$, such that
\begin{equation*}
    \mathtt{A}_i(q)_{\mu, \nu}
    \coloneqq
        x^i_{\nu - \mu + 1},
        \quad
        \text{ for }\mu\in[d], \nu\in[q].
    % \begin{bmatrix}
    %     x_1^i & x_2^i & \cdots & x_t^i & x_{t+1}^i & x_{t+2}^i & \cdots \\
    %     x_T^i & x_{T-1}^i & \cdots & x_{t-1}^i & x_t^i & x_{t+1}^i & \cdots \\
    %     x_{T-1}^i & x_{T-2}^i & \cdots & x_{t-2}^i & x_{t-1}^i & x_{t}^i & \cdots \\
    %     \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots \\
    %     x_{T-q}^i & x_{T-q+1}^i & \cdots & x_{t-q}^i & x_{t-q+1}^i & x_{t-q+2}^i & \cdots \\   
    % \end{bmatrix},
\end{equation*}
where in the $j$-th column of $\mathtt{A}_i(q)$, it contains historical values of $x_j^i$ with lag $q > 0$.
% The matrix form of $\mathtt{A}_i(q)$ is in Equation~\eqref{eqn:history-matrix}.

\begin{lemma}\label{lem:mar-group-wise}
Fix $q_{\max}, D \in \N^+$.
Given any $T > 0, d^\prime > q > 0, d > 0$ such that $T > q$, $q_{\max} \geq q$.
For any input matrix $\bH$ in the form of any-variate encoding in Equation~\ref{eqn:AV-encoding}, such that $\bH \in \R^{ D \times dT}$.
There exists a one layer, $q_{\max}$ head \textbf{any-variate attention} that performs the following operation.
    \begin{align*}
    &
    \begin{bmatrix}
        x_1^1 & \cdots & x_{T}^1 & x_1^2 & \cdots & x_T^2 & \cdots & x_1^d & \cdots & x_T^d
        \\
        \bp_1 & \cdots & \bp_{T} & \bp_1 & \cdots & \bp_T & \cdots & \bp_1 & \cdots & \bp_T
        \\
        \be_1 & \cdots & \be_1 & \be_2 & \cdots & \be_2 & \cdots & \be_d & \cdots & \be_d
    \end{bmatrix}
    \\ 
    &\quad\quad\quad\quad\mapsto
    \begin{bmatrix}
        \mathtt{A}_1(q) & \mathtt{A}_2(q) & \cdots & \mathtt{A}_d(q)
        \\
        \bm{0}_{d^{''} \times T} & \bm{0}_{d^{''} \times T} & \cdots & \bm{0}_{d^{''} \times T}   
        \\
        \ddots & \ddots & \cdots & \ddots 
    \end{bmatrix},
    \end{align*}
    where $d^{''}  = d^\prime - q_{\max}$.
\end{lemma}
The proof is in \cref{proof:any-var-enc}.
Intuitively, the above operation performs the same operation in Lemma~\ref{lem:input-causal} but in a variate-wise fashion.
\cref{lem:mar-group-wise} shows that any-variate attention is capable of organizing the history of each variate efficiently.
To again achieve the format in Equation~\eqref{eqn:format-assumption}, one has to stack all $\mathtt{A}_i(q)$ in the same columns, which can be easily done by a single layer of attention via \cref{lem:input-causal} and \citep[Proposition~A.5]{bai2024transformers} (details in \cref{proof:any-var-enc}).
This lemma serves as a foundation for MOIRAI to handle multi-variate time series with in-context learning which we present as the theorem below.
\begin{remark}
        Comparing to Lemma~\ref{lem:input-causal}, \cref{lem:mar-group-wise} is specifically for any-variate attention in our construction, where we demonstrate that several special mechanisms in any-variate attention enables variate-wise operations efficiently.
\end{remark}
\begin{remark}    
        Lemma~\ref{lem:input-causal} and Lemma~\ref{lem:mar-group-wise} can be generalized to Softmax and linear attention by considering perturbations, making them applicable to a wide range of transformers.    
\end{remark}

\begin{theorem}[Any-variate Autoregressive Regression via MOIRAI]\label{thm:any-variate-auto}
    Assume \cref{assumption:effective-regression} holds.
    For any $0 \leq \alpha \leq \beta$ with $\kappa \coloneqq \frac{\beta}{\alpha}$, $B_w > 0$, and $\epsilon < B_x B_w / 2$.
    there exists an $(L_1+L_2)$-layer of MOIRAI transformer equipped with any-variate Attention, satisfies the following
    \begin{align*}
        &
        L_1 = \lceil  \frac{q_{\max}}{3} \rceil  +1, \quad
        L_2 = \lceil 2 \kappa \log \frac{ B_x B_w}{2\epsilon} \rceil, 
        \quad
        \\
        &\quad\quad\quad \quad\quad\max_{\ell \in [L_1+1, L_2]} M^{(\ell)} \leq 3, 
        \\
        &\vertiii{\bm{\theta}} \leq \lvert 4R + 8 \beta^{-1} \rvert,
        \quad \sum_{ \ell=1 }^{L_1} M^{(\ell)} = d_{\max} + q_{\max},
    \end{align*}
    where $d_{\max} > 0$.
    For any input time series $\bx$ with length $T$ generated from an $\mathtt{AR}_d(q)$ process, where
    \begin{align*}
        \bx \in \R^{d \times T},\quad q \leq q_{\max}, \quad d \leq d_{\max}.
    \end{align*}
    Then there exists a MOIRAI transformer with $D \geq (q+1)d_{\max} + T + 2$, satisfies the following
    \begin{equation}
        \lVert
        \hat{\bx}_{T}^1
        -
        \left\langle
        \bw^\star_i, [\bx_{T-1: T-q}^1; \dots ; \bx_{T-1: T-q}^d]
        \right\rangle
        \rVert
        \leq 
        \epsilon,
    \end{equation}
    where $\hat{\bx}_T^1 = \mathtt{read}(\text{TF}^0_{\bm{\theta}}(\bH))$, and $\bH \in \R^{D\times  N}$ is the any-variate encoding of $\bx$.
\end{theorem}
\begin{remark}
    \cref{thm:any-variate-auto} indicates there exists a MOIRAI transformer that fits an autoregressive model on time series as long as the number of covariates no greater than $d_{\max}$ and lags no greater than $q_{\max}$.
    This shows its ability to infer the underlying AR model in a principled way and provides a possible explanation for its zero-shot performance on a wide range of datasets.
\end{remark}

The proof is in \cref{proof:group-wise}.
Observe that there exists two trade-offs in \cref{thm:any-variate-auto}.
First, $q_{\max} d_{\max}$ is upper bounded by the hyperparameter $D$ (up to constant), which is a natural trade-off in our construction.
Second, the approximation error is roughly $O( e^{-L} )$, suppressed exponentially by the number of layers, as in our analysis, each layer of MOIRAI performs a single step of gradient descent on $L_{\text{reg}}$.

Another popular approach of time series prediction is through probabilistic forecasting, where the model estimates the distribution from input data.
In \cref{thm:moirai-mle}, we show that there also exists a MOIRAI that performs Maximum Likelihood Estimation with a small estimation error.

% Observe that with a fixed number of layers and number of heads each layer, there exists a trade-off between the approximation error and the max number of covariates a model handles.
% {\color{red} add more analysis}

% \paragraph{Data Assumption.}
% We assume a $T$ time step time series $(\bx_t, \by_t)_{t=1}^T$, $\bx_t \in \R^d$, $\by_t \in \R$, is generated from the following stochastic process with lookback window size $q \in \N_+$.
% \begin{equation*}
%     \bx_{t+1} = \sum_{i=1}^q \langle \bw_{t-i} , \by_{t-i} \rangle + \epsilon_{t-i},
%     \quad\text{where }
%     \epsilon_t \sim N(0, \sigma_t^2), \quad
%     \bw_t, \in \R^d, \quad \text{ for all }t \in [T].
% \end{equation*}

% \begin{theorem}[Transformers as  Universal Forecaster: Approximation Bound]\label{thm:any-variate-auto}
%     For any $0 \leq \alpha \leq \beta$ with $\kappa \coloneqq \frac{\beta}{\alpha}$, assume the input data is norm bounded by $B_x$.
%     With any input time series $\{(\bx_t, \by_t)\}_{t=1}^T$ following the above assumption,
%     there exists an $(L)$-layer of Transformer satisfies the following
%     \begin{equation*}
%         \lVert f( \bx_{T+1} ) - \by_{T+1} \rVert 
%         =
%         O
%         \left(
%         \frac{\kappa \log B_x}{L-1}
%         \right).
%     \end{equation*}
% \end{theorem}


% \subsection{Generalization Bound}

% \begin{itemize}
%     \item Multi-path + No-sliding window
%     \item Single Path + Sliding Window
%     \item Multi-path + sliding window
% \end{itemize}

% Next we study the generalization bound for XXX.
% Considering learning from X
