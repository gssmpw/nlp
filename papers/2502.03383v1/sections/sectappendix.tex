\appendix

{
\setlength{\parskip}{-0em}
\startcontents[sections]
\printcontents[sections]{ }{1}{}
}




% \begin{itemize}
%     \item[\ref{sec:tab_notation}] \hyperref[sec:tab_notation]{Table of Notations}\dotfill \pageref{sec:tab_notation}

%     \item[\ref{sec:related-work}] \hyperref[sec:related-work]{Related Works} \dotfill \pageref{sec:related-work}
    
%     \item[\ref{sec:additional-theory}] \hyperref[sec:additional-theory]{Additional Theoretical Background} \dotfill \pageref{sec:additional-theory}

%     \item[\ref{sec:proofs}] \hyperref[sec:proofs]{Proof of Main Context} \dotfill \pageref{sec:proofs}
    
%     \begin{itemize}
%         \item[$\quad$\ref{proof:lem-input-casual}] \hyperref[proof:lem-input-casual]{Proof of \cref{lem:input-causal}} \dotfill \pageref{proof:lem-input-casual}

%         \item[$\quad$\ref{proof:any-var-enc}] \hyperref[proof:lem-input-casual]{Proof of \cref{lem:mar-group-wise}} \dotfill \pageref{proof:any-var-enc}

%         \item[$\quad$\ref{proof:any-var-enc}] \hyperref[proof:any-var-enc]{Proof of \cref{thm:any-variate-auto}} \dotfill \pageref{proof:any-var-enc}

%         \item[$\quad$\ref{proof:tr-lipschitz}] \hyperref[proof:tr-lipschitz]{Proof of \cref{proposition:lipschitz-moirai}} \dotfill \pageref{proof:tr-lipschitz}

%         \item[$\quad$\ref{proof:gen-bound-1}] \hyperref[proof:gen-bound-1]{Proof of \cref{thm:gen-bound-1}} 
%         \dotfill \pageref{proof:gen-bound-1}

%         \item[$\quad$\ref{appendix:analysis-ar1}] \hyperref[appendix:analysis-ar1]{Proof of \cref{proposition:ar1}}
%         \dotfill \pageref{appendix:analysis-ar1}
        
%     \end{itemize}

%     \item[\ref{sec:exp-details}] \hyperref[sec:exp-details]{Experimental Details} \dotfill \pageref{sec:exp-details}

    
% \end{itemize}




\section{Table of Notations}
\label{sec:tab_notation}

\begin{table}[h]
    \caption{Mathematical Notations and Symbols}
    \centering
    % \resizebox{ \textwidth}{!}{ 
    \begin{tabular}{cl}
    \toprule
        Symbol & Description \\
    \midrule
        $\bx_i$ & The $i$-th component of vector $\bx$ \\
        $\Braket{\ba,\bb}$ & Inner product for vectors $\ba,\bb\in \R^d$ \\
        $[I]$ & Index set $\{1,\cdots,I\}$, where $I\in\mathbb{N}^+$ \\
        $\norm{\cdot}$ & Spectral norm, equivalent to the $l_2$-norm when applied to a vector \\
        $\norm{\cdot}_{2, \infty}$ & The largest L2 norm of column vectors of a matrix \\
        $\bA_{ij}$ & The element on the $i$-th row and $j$-th column of matrix $\bA$ \\
        $\bx_{i:j}$ & The sub-sequence of sequence $\bx$ from coordinate $i$ to $j$ \\
        $\oplus$ & Concatenation between column vectors $\bv \oplus \bu \mapsto ( \bv^\top, \bu^\top)^\top$ \\
        $[\bu ; \bv]$ & Concatenation between two row vectors \\
    \midrule
        $N$ & Length of a transformer input sequence \\
        $T$ & Number of time steps of a time series \\
        $M$ & Number of attention heads. \\
        $q$ & Lag of an $\mathtt{AR}$ process. \\
        $d$ & The number of covariates in an $\mathtt{AR}$ process \\
    \midrule
        $\bv$ & Vector (bold lower) \\
        $\bA$ & Matrix (bold upper) \\
        $\cX$ & random variable (calligraphic) \\
        $\text{x}$ & element from a domain set \\
        $\cD_{\cX}$ & Domain of random variable $\cX$ \\
        $\be_i$ & one-hot vector with its $i$-th entry as 1 \\
    \midrule
        $\mathtt{P}_{\cX}$ & Probability distribution of $\cX$ \\
        $P_{\bz | \bw}(z \mid w )$ & The probability $P \left[ \bz = z \mid \bw = w \right]$ \\
    \bottomrule
    \end{tabular}
    % }
     \label{tab:nomenclature}
\end{table}

\clearpage

\input{sections/sect5}





\section{Additional Theoretical Background}\label{sec:additional-theory}
Here, we include several technical lemme that are intensively used throughout our paper.
The Lipschitzness of an MLP layer is obtained in \citep[Lemma~J.1]{bai2024transformers}, which we restate it below
\begin{lemma}[\cite{bai2024transformers}]\label{lem:mlp-lipschitz}
    For a single MLP layer, $\bm{\theta}_2 = (\bW_1, \bW_2)$, we introduce its norm
    \begin{equation*}
        \lvert
        \lvert
        \lvert
        \bm{\theta}_2
        \rvert 
        \rvert 
        \rvert
        =
        \norm{\bW_1}_{\text{op}}
        +
        \norm{\bW_2}_{\text{op}}.
    \end{equation*}
    For any fixed hidden dimension $D^\prime$, we consider
    \begin{equation*}
        \Theta_{2, B}
        \coloneqq
        \{ 
        \bm{\theta}_2
        :
        \lvert
        \lVert
        \bm{\theta}_2
        \rVert
        \rvert
        \leq 
        B
        \}.
    \end{equation*}
    Then for $\bH \in \mathcal{H}_R$, $\bm{\theta}_2 \in \Theta_{2, B}$, the function 
    $(\bm{\theta}_2, \bH) \mapsto \text{MLP}_{\bm{\theta}_2}$ is $(BR)$-Lipschitz w.r.t. $\bm{\theta}_2$
    and $(1 + B^2)$-Lipschitz w.r.t. $\bH$.
\end{lemma}

The following lemma shows any-variate attention is capable of performing variate-wise operation on arbitrary number of covariates under any-variate encoding.

\begin{lemma}[Group-Wise Operation via Any-Variate Attention]\label{lem:moirai-group-wise}

    Let $\norm{\bH}_{2,p} \coloneqq ( \sum_{i=1}^N \norm{\bh_i}_2^p )^{1/p}$ denote the column-wise $(2, p)$-norm of $\bH$.
    For any input matrix $\bH = (\bh_1, \cdots, \bh_T)$ such that $\norm{\bH}_{2, \infty} \leq \mathtt{R}$,
    suppose
    $\psi(\cdot): \R^{D \times T} \rightarrow \R^{D \times T}$ is a sequence-to-sequence function implemented by a single layer standard transformer ($\text{TF}_{\bm{\theta}}^\dagger$) such that 
    \[
    \text{TF}_{\bm{\theta}}^\dagger(\bH)
    \coloneqq
        \psi(\bH).
    \]
    Then there exists a single layer MOIRAI transformer $\text{TF}_{\bm{\theta}}(\cdot)$ such that for any input 
    \[
    \bH^\star
    =
    \begin{bmatrix}
        \bH_1 & \bH_2 & \cdots & \bH_K
    \end{bmatrix},
    \]
    where $\bH_k \in \R^{D \times T}$.
    $\text{TF}_{\bm{\theta}}(\cdot)$ performs
    \[
    \text{TF}_{\bm{\theta}}
    (\bH^\star)
    =
    \begin{bmatrix}
     \psi(\bH_1) & \psi(\bH_2) & \cdots & \psi(\bH_K)   
    \end{bmatrix}.
    \]
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:moirai-group-wise}]\label{proof:group-wise}
    We start by showing the case of a single-head, single-layer standard transformer.
    Let 
    \[
    \text{MLP}_{\bm{\theta}_2} \circ \text{Attn}_{\bm{\theta}}^\dagger
    (\bH)
    =
    \text{MLP}_{\bm{\theta}_2}
    \circ
    \bV \bH
    \sigma
    (
    \Braket{
    \bQ \bH, 
    \bK \bH
    }
    )
    =
    \bV \bH
    \bA_{\bH},
    \]
    where $\bA_{\bH} = \sigma(\Braket{
    \bQ \bH, 
    \bK \bH})$.

    Let $\psi_1(\bH) \coloneqq \bV \bH \bA_{\bH}$, to apply group-wise operation of $\psi_1(\cdot)$ on some input such that
    \[
    \psi_1(
    \bH^\star
    )
    =
    \begin{bmatrix}
        \psi_1(\bH_1) & \psi_1(\bH_2) & \cdots & \psi_1(\bH_K)
    \end{bmatrix}.
    \]

    Let $\bm{0} \in \R^{T \times T}$ be a zero matrix, and $\bm{1} \in \R^{T \times T}$ be a $1$s matrix, for for any input $\norm{\bH^\star}_{2, \infty} \leq \mathtt{R}$, one can find some $u^2 < 0$ to decompose $\psi_1(\cdot)$ into the following form.
    \begin{align*}
    \psi_1(\bH^\star)
    &=
    \bV
    \begin{bmatrix}
        \bH_1 \bA_{\bH_1} & 
        \bH_2 \bA_{\bH_2} &
        \cdots &
        \bH_K \bA_{\bH_K}
    \end{bmatrix}
    \\
    &=
    \bV 
    \bH^\star
    \begin{bmatrix}
        \bA_{\bH_1} & \bm{0} & \cdots  & \bm{0} \\
        \bm{0} & \bA_{\bH_2} & \cdots  & \bm{0} \\
        \vdots & \vdots & \ddots & \vdots \\
        \bm{0} & \bm{0} & \cdots & \bA_{\bH_K}
    \end{bmatrix}
    \\
    &=
    \bV
    \bH^\star
    \times
    \\
    &\sigma
    \left(
    \begin{bmatrix}
        \Braket{\bQ \bH_1, \bK \bH_1} & \Braket{\bQ \bH_1, \bK \bH_2} & \cdots  & \Braket{\bQ \bH_1, \bK \bH_K} \\
        \Braket{\bQ \bH_2, \bK \bH_1} & \Braket{\bQ \bH_2, \bK \bH_2} & \cdots  & \Braket{\bQ \bH_2, \bK \bH_K} \\
        \vdots & \vdots & \ddots & \vdots \\
        \Braket{\bQ \bH_K, \bK \bH_1} & \Braket{\bQ \bH_K, \bK \bH_2} & \cdots & \Braket{\bQ \bH_K, \bK \bH_K}
    \end{bmatrix}
    +
    \begin{bmatrix}
        \bm{0} & u^2 \cdot \bm{1} & \cdots  & u^2 \cdot \bm{1} \\
        u^2 \cdot\bm{1} & \bm{0} & \cdots  & u^2 \cdot \bm{1}\\
        \vdots & \vdots & \ddots & \vdots \\
        u^2 \cdot\bm{1} & u^2\cdot \bm{1} & \cdots & \bm{0}
    \end{bmatrix}
    \right).
    \end{align*}
    Further, observe that operations in an MLP layer are either left multiplication or element-wise operations, which implies group-wise as well. 
    We then finish the proof by setting $u^1 = 0$.

\end{proof}


\begin{theorem}[\text{\citep[Section~5.6]{wainwright2019high}}]\label{thm:generalizd-dudley}
    Suppose $\psi: [0,+\infty_ \rightarrow [0, +\infty)$ is a convex, non-decreasing function satisfying $\psi(x+y) \geq \psi(x) \psi(y)$.
    For any random variable $X$, we consider the Orlicz norm induced by $\psi: \norm{X}_{\psi} \coloneqq \inf \{ K > 0: \mathtt{E}_\psi(|X|/K) \} \leq 1$.
    Suppose that $\{ X_{\theta} \}$ is a zero-mean random process indexed by $\theta\in\Theta$ such that $\norm{ X_{\theta} - X_{\theta^\prime} } \leq \rho(\theta, \theta^\prime)$ for some metric $\rho$ on $\Theta$.
    Then the following holds
    \[
    P
    \left(
    \sup_{\theta, \theta^\prime \in \Theta}
    \vert 
    X_{\theta}
    -
    X_{\theta^\prime}
    \vert 
    \leq 
    8(J+t)
    \right)
    \leq
    \frac{1}{\psi(t/D)},
    \quad
    \text{for all} t \geq 0,
    \]
    where $D$ is the diameter of the metric space $(\Theta, \rho)$, and the generalized Dudley entropy integral $J$ is given by 
    \[
    J
    \coloneqq
    \int_0^D
    \psi^{-1}
    (N(\delta; \Theta, \rho))
    d \delta,
    \]
    where $N(\delta; \Theta, \rho)$ is the $\delta$-covering number of $(\Theta, \rho)$.
\end{theorem}

The next technical lemma is in \cite{bai2024transformers}.
Let $\mathtt{B}^k_{\infty}(R) = [-R, R]^k$ denotes the standard $\ell_{\infty}$ ball in $\R^k$ with radius $R > 0$. 
\begin{definition}[Sufficiently smooth $k$-variable function]
We say a function $g : \R^k \mapsto \R$ is $(R, C_{\ell})$-smooth if for $s = \lceil (k-1)/2 \rceil + 2$, $g$ is a $C^s$ fnction on $\mathtt{B}_{\infty}^k(R)$, and
\[
\sup_{\bz \in \mathtt{B}_{\infty}^k(R)}
\norm{\nabla^i g(\bz)}_{\infty}
=
\sup_{\bz \in \mathtt{B}_{\infty}^k(R)}
\sup_{j_1,\cdots,j_i\in[k]}
| \partial_{x_{j1} \cdots x_{ji}}
g(\bx)
\leq 
L_i
\]
for all $i = 0, 1, \cdots ,s$, with $\max_{0\leq i \leq s} L_i R^i \leq C_{\ell}$.
 
\end{definition}


\begin{lemma}[Approximating smooth $k$-variable functions]\label{lem:approx}
    For any $\varepsilon_{\text{approx}} > 0$, $R \geq 1, C_{\ell} > 0$, we have the following:
    Any $(R, C_{\ell})$-smooth function $g : \R^k \mapsto \R$ is $(\varepsilon_{\text{approx}}, R, M, C)$-approximable by sum of relus with $M \leq C(k) C_{\ell}^2 \log( 1 + C_{\ell}/\varepsilon_{\text{approx}}^2 )$ and $C \leq C(k) C_{\ell}$, where $C(k) > 0$ is a constant that depends only on $k$, i.e.,
    \[
    f(\bz) =
    \sum_{m=1}^M c_m \sigma( \ba_m^\top [\bz ; 1])
    \quad\text{with }
    \sum_{m=1}^M |c_m| \leq C,
    \quad
    \max_{m\in[M]}
    \norm{\ba_m}_1 \leq ,    
    \]
    such that $\sup_{\bz \in [-R,R]^k}
    | f(\bz) - g(\bz) |\leq \varepsilon_{\text{approx}}
    $.
\end{lemma}


% \subsection{Kernel Regression with Transformer} 
% Here we first review the Nadaraya-Watson Kernel Estimator for kernel regression.
% %, and then review the leave-one-out cross validation for bandwidth selection.

% \paragraph{The Nadaraya-Watson Kernel Estimator.}
% Let $h > 0$ be the bandwidth, and $K(\cdot)$ be a smoothing kernel.
% The Nadaraya-Watson kernel estimator is formulated as follows:
% \begin{equation*}
%     \hat{r}(\bx)
%     \coloneqq
%     \sum_{i=1}^n
%     \gamma_i(\bx) \by_i,\quad\text{where }
%     \gamma_i(\bx)
%     \coloneqq
%     \frac{ K\left( \frac{\bx- \bx_i}{h} \right)  }{     \sum_{j=1}^n
%  K\left( \frac{\bx- \bx_i}{h} \right)}.
% \end{equation*}
% To approximate the Nadaraya-Watson Kernel Estimator, we consider the following input format
% \begin{equation*}
%     \bH 
%     =
%     \begin{bmatrix}
%         \bx_1 & \bx_2 & \dots & \bx_n & \bx_{\text{test}}
%         \\
%         \by_1 & \by_2 & \dots & \by_n & \by_{\text{test}}
%         \\
%         \bp_1 & \bp_2 & \dots & \bp_n & \bp_{\text{test}},
%     \end{bmatrix}
% \end{equation*}
% where we have $n$ in-context examples, and the goal is to estimate calculate $\hat{r}(\bx_{\text{test}})$ with those $n$ in-context examples.
% Now we show our first result of kernel regression via Transformer.
% \begin{theorem}[\normalfont Nadaraya-Watson kernel regression via Transformer]\label{thm:kernel-regression}
%     Let $K_h(\cdot) : \R^d \mapsto \R$ be a kernel with bandwidth $h > 0$ that is $(\varepsilon, R, M, C)$-approximable by sum of ReLUs.
%     There exists a 2-layered transformer $\text{TF}_\theta(\cdot)$ with first layer having a single head, second layer having $M$ heads,
%     that approximates kernel regression
%     \begin{equation}
%         \norm{ \texttt{read}\left( \text{TF}_\theta(\bH) \right) - \hat{r}( \bx_{\text{test}} ) }_2
%         \leq 
%         \frac{n B_y \cdot \varepsilon^2}{s},
%     \end{equation}
%     where $s \coloneqq  \sum_{j=1}^N K(\nicefrac{\bx - \bx_j}{h})$, and $\norm{\by_i}_2 \leq B_y$ for all $i \in [n]$.
% \end{theorem}




% \textcolor{red}{
% \subsection{Basics Operations}
% \begin{table}[t]
%     \centering
%     \caption{Caption}
%     \begin{tabular}{l|l}
%         \toprule
%             Operations & Method  \\
%         \hline
%         Zero-out a certain row & left matrix multiplication $\bW \bx$
%         \\
%         Zero-out a certain column  & right matrix multiplication $\bx \bW$
%         \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:operation-summary}
% \end{table}
% }

\clearpage

\section{Proofs}\label{sec:proofs}
\subsection{Proof of Lemma~\ref{lem:input-causal}}\label{proof:lem-input-casual}
Here we prove a slightly simpler result with the positional encoding containing only zero vectors and a one-hot vector.
One can easily extend the proof by padding the weight matrices.

\begin{equation}\label{eqn:input-data2}
    \bfa H 
    \coloneqq
    \begin{bmatrix}
        x_1 & x_2 &  \dots & x_T & 0
        \\
        \bp_1 & \bp_2 & \dots & \bp_T &
        \bp_{T+1}
    \end{bmatrix}
    \in \R^{D \times (T+1)}
    ,
    \quad
    \bp_i
    \coloneqq
    \begin{bmatrix}
        \mathbf{0}_{d^\prime}
        \\
        \be_i
    \end{bmatrix}
    \in \R^{d^\prime + T}
    ,
\end{equation}

\begin{lemma}[Lemma~\ref{lem:input-causal} Restate]
    Given a sequence of token $\bfa H$ in the form of Equation~\ref{eqn:input-data2}, there exists a one-layer, $q-1$ head ReLU attention layer, such that the columns of $\text{Attn}_{\bm{\theta}}( \bfa H )$ has the following form:
    \begin{equation}
    \text{Attn}_{\bm{\theta}_1}^{\dagger}( \bfa H )_i
    \coloneqq
        \begin{bmatrix}
            x_i
            \\
            x_{i-1}
            \\
            \vdots
            \\
            x_{i-q}
            \\
            \bp_i^\prime
        \end{bmatrix},
        \quad
        \text{where }
        \bp_i^\prime 
        \coloneqq
        \begin{bmatrix}
        \mathbf{0}_{ d^\prime - q }
        \\
        1
        \\
        1 \{ i < T + 1 \}
        \end{bmatrix}
        \in \R^{ d^\prime - q + 2 }.
    \end{equation}
\end{lemma}




% \begin{proof}
%     We denote $\mathbf{0}_d \in \R^d$ as a $d$-dimensional zero vector. 
%     Considering the input $(x_1, \dots, x_T) \in \R^{d \times (T+1)}$.
%     Let $x_i \in \R^d$, 
%     $$\bW_V^h x_i \coloneqq 
%     \begin{pmatrix}
%          \mathbf{0}_{hd}
%          \\
%          x_i
%          \\
%          \mathbf{0}_{(H-h)d}
%     \end{pmatrix}
%     \in \R^{(H+1) d},$$
%     where $H$ is the number of heads with $h$ as head index.
%     For example, 
%     $$
%     \bW_V^1 x_i = 
%     \begin{pmatrix}
%         \mathbf{0}_d
%         \\
%         x_i
%         \\
%         \mathbf{0}_{(H-1)d}
%     \end{pmatrix}.
%     $$

%     We now view the attention layer, as the following form
%     \begin{align*}
%         \text{Attn}( \bH )
%         &=
%         \bH
%         +
%         \frac{1}{T}
%         \sum_{m=1}^M
%         \Vb_m \bH
%         \cdot
%         \sigma( 
%         \left(
%         \Qb_m \bH
%         \right)^\top
%         \left(
%         \Kb_m \bH
%         \right)
%         )
%         \\
%         &=
%         \bH+
%         \frac{1}{T}
%         \sum_{m=1}^M
%         \Vb_m \bH
%         \cdot
%         \Ab_m,
%     \end{align*}
%     where $\Ab_m = \sigma( 
%         \left(
%         \Qb_m \bH
%         \right)^\top
%         \left(
%         \Kb_m \bH
%         \right)
%         )$.
%     By doing so, the self-attention layer contains two types of operations: column-wise and row-wise moving,
%     with right and left matrix multiplications, respectively.
%     Therefore, with $\Vb_m$ being the row-moving operator, we now only have to design $\Kb_m, \Qb_m$, such that $\Ab_m$ shifts each column to the right by $m$ columns, which is trivial to construct with the positional encoding.
% \end{proof}


\begin{proof}
    Consider an input of the following form
    \begin{equation*}
        \bx
        =
        \begin{bmatrix}
            x_1 & x_2 & \cdots & x_T
            \\
            \bm{0} & \bm{0} & \cdots & \bm{0} 
            \\
            \be_1 & \be_2 & \cdots & \be_T
        \end{bmatrix},
    \end{equation*}
    where $\bx_t \in \R^{d}, \pb_t \in \R^T$, for all $t = 1, \cdots, T$.
    We construct weights of the $m$-th head $\bW_Q^m, \bW_Q^m$ as following,
    \begin{equation*}
        \bW_K^m
        =
        \begin{bmatrix}
            \bm{0}^\top & \bm{0}^\top & \be_1^\top
            \\
            \bm{0}^\top & \bm{0}^\top & \be_2^\top
            \\
            \vdots & \vdots & \vdots 
            \\
            \bm{0}^\top & \bm{0}^\top & \be_T^\top            
        \end{bmatrix},
        \quad
        \bW_Q^m
        =
        \begin{bmatrix}
            \bm{0}^\top & \bm{0}^\top & \be_{1-m}^\top
            \\
            \bm{0}^\top & \bm{0}^\top & \be_{2-m}^\top
            \\
            \vdots & \vdots & \vdots 
            \\
            \bm{0}^\top & \bm{0}^\top & \be_{T-m}^\top            
        \end{bmatrix},
    \end{equation*}
    where we define the negative index as rotational index, i.e., $\be_{-1} = \be_{T}, \be_{-2} = \be_{T-1}$.
    We have
    \begin{align*}
        \left(
        \bW_K^m
        \Xb
        \right)^\top
        \left(
        \bW_Q^m
        \Xb
        \right)
        &=
        \begin{bmatrix}
            \be_1^\top
            \\
            \be_2^\top
            \\
            \vdots 
            \\
            \be_T^\top
        \end{bmatrix}^\top
        \begin{bmatrix}
            \be_{1-m}^\top
            \\
            \be_{2-m}^\top
            \\
            \vdots
            \\
            \be_{T-m}^\top
        \end{bmatrix}
        \\
        &=
        \bI_T
        \begin{bmatrix}
            \be_{1-m}
            \\
            \be_{2-m}
            \\
            \vdots
            \\
            \be_{T-m}
        \end{bmatrix}.
    \end{align*}
    Note that the result of 
        $\sigma\left(\left(
        \bW_K^m
        \Xb
        \right)^\top
        \left(
        \bW_Q^m
        \Xb
        \right)
        \right)$
    is a rotation matrix, where right multiplication on $\Xb$ will rotate the columns of $\Xb$.
    Therefore, we have 
    $\bW_V^m$ that performs row-wise shifting and the attention matrix 
    $\sigma\left(\left(
        \bW_K^m
        \Xb
        \right)^\top
        \left(
        \bW_Q^m
        \Xb
        \right)
        \right)$
        performs column-wise shifting.
\end{proof}


\subsection{Proof of \cref{thm:any-variate-auto}}\label{proof:any-var-enc}

\paragraph{Autoregressive Linear Regression under Any-Variate Encoding.}
The ultimate goal of this setup is to perform the following mechanism.
Let $\bx$ be the target variate we wish to predict, $\bz^j$ be the $j$-th covariate of $\bx$, for $j \in [M]$.
We denote the lookback window size as $q$, and each covariate has length $T$ ($T$-time steps.).
We denote the time encoding as $\bp_i$ for $i \in [T]$, and the variate encoding as $\bq_{j}$ for $j \in [M]$.
Finally, our goal is to predict $\bx_T$.

\begin{equation*}
\begin{bmatrix}
    x_1^1 & \cdots & x_T^1 & x_1^2 & \cdots & x_T^2 & \cdots & x_1^d & \cdots & x_T^d
    \\
    \bp_1 & \cdots & \bp_T & \bp_1 & \cdots & \bp_T & \cdots & \bp_1 & \cdots & \bp_T
    \\
    \be_1 & \cdots & \be_1 & \be_2 & \cdots & \be_2 & \cdots & \be_d & \cdots & \be_d
\end{bmatrix}
\mapsto
\begin{bmatrix}
    \mathtt{A}_1(q) & \cdots
    \\
    \mathtt{A}_2(q) & \cdots
    \\
    \vdots
    \\
    \mathtt{A}_d(q) & \cdots
    \\
    \vdots & \ddots
\end{bmatrix}.
\end{equation*}

Here, different colors represent different covariates.
The motivation for performing such an operation is to apply the in-context learning property of transformers proved in \cite{bai2024transformers}.


\begin{lemma}[Lemma~\ref{lem:mar-group-wise} Restate]
Define the matrix $\mathtt{A}_i(q)$ for the $i$-th covariates $(x_1^i, \cdots, x_T^i)$, with order $q$, such that
\begin{equation*}
    \mathtt{A}_i(q)
    \coloneqq
    \begin{bmatrix}
        x_1^i & x_2^i & \cdots & x_t^i & x_{t+1}^i & x_{t+2}^i & \cdots \\
        x_T^i & x_{T-1}^i & \cdots & x_{t-1}^i & x_t^i & x_{t+1}^i & \cdots \\
        x_{T-1}^i & x_{T-2}^i & \cdots & x_{t-2}^i & x_{t-1}^i & x_{t}^i & \cdots \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots \\
        x_{T-q}^i & x_{T-q+1}^i & \cdots & x_{t-q}^i & x_{t-q+1}^i & x_{t-q+2}^i & \cdots \\   
    \end{bmatrix},
\end{equation*}
where in the $j$-th column of $\mathtt{A}_i(q)$, it contains historical values of $x_j^i$ with lag $q$.

Given fixed $D, T \in \N^+$, where $T > q$.
For any input matrix $\bH$ in the form of Any-Variate Encoding in Equation~\ref{eqn:AV-encoding}, such that $\bH \in \R^{ D^\prime \times dT^\prime }$, and $D^\prime \leq D$, $T^\prime < T$.
There exists a 1-layer, $q$ head Any-Variate Attention that performs the following operation.
    \begin{equation*}
    \begin{bmatrix}
        x_1^1 & \cdots & x_T^1 & x_1^2 & \cdots & x_T^2 & \cdots & x_1^d & \cdots & x_T^d
        \\
        \bp_1 & \cdots & \bp_T & \bp_1 & \cdots & \bp_T & \cdots & \bp_1 & \cdots & \bp_T
        \\
        \be_1 & \cdots & \be_1 & \be_2 & \cdots & \be_2 & \cdots & \be_d & \cdots & \be_d
    \end{bmatrix}
    \mapsto
    \begin{bmatrix}
        \mathtt{A}_1(q) & \mathtt{A}_2(q) & \cdots & \mathtt{A}_d(q)
        \\
        \ddots & \ddots & \cdots & \ddots 
    \end{bmatrix}
    \end{equation*}
\end{lemma}

\begin{proof}
    The proof comes as a direct corollary of Lemma~\ref{lem:moirai-group-wise} and \citep[Proposition~A.5]{bai2024transformers}.
    By Lemma~\ref{lem:input-causal}, there exists a single layer standard transformer layer (with $\bW_1, \bW_2$ being $0$s) that generates $\mathtt{A}_i(q)$ for each uni-variate (covariate).
    It then left applying Lemma~\ref{lem:moirai-group-wise} for variate-wise operation and applying \citep[Proposition~A.5]{bai2024transformers} to keep the time indices $\bp_t$ unchanged.
        
\end{proof}


\begin{corollary}\label{cor:second-step-enc}
    There exists a $d_{\max}$ head standard attention layer that performs the following
    \[
        \begin{bmatrix}
            \mathtt{A}_1(q) & \mathtt{A}_2(q) & \cdots & \mathtt{A}_d(q)
            \\
            \ddots & \ddots & \cdots & \ddots 
        \end{bmatrix}
        \mapsto
        \begin{bmatrix}
            \mathtt{A}_1(q) & \cdots 
            \\
            \tilde{\mathtt{A}}_2(q) & \cdots
            \\
            \vdots
            \\
            \tilde{\mathtt{A}}_d(q) & \cdots
            \\
            \ddots & \ddots 
        \end{bmatrix},
        \quad\text{for any }d \leq d_{\max},
    \]
    where $\tilde{\mathtt{A}}_i(q)$ is $\mathtt{A}_i(q)$ without the first row.
\end{corollary}
\begin{proof}
Note that this operation in \cref{cor:second-step-enc} is straightforward with \cref{lem:input-causal} and \citep[Proposition~A.5]{bai2024transformers}.
As for each $i \in [d]$, $i \neq 1$, the attention layer performs two operations to each element of $\mathtt{A}_i(q)$:

\begin{align*}
    \begin{cases}
        iT \text{ columns to the left } & \text{ right multiplication} \\
        q_{\max} \text{ rows below } & \text{ left multiplication} \\
        \text{zero out} & \text{if in first row (left multiplication)}
    \end{cases}.
\end{align*}
Note that one can simply construct weight matrices to perform the above permutations and masking.
In total, we need $d_{\max}$ heads to perform such operations for each $\mathtt{A}_i(q)$, for any $d \leq d_{\max}$.
For $q < q_{\max}$, the remaining entries will be zero padded.
Finally, with at best $2$ layers of $d_{\max}$ head any-variate attention, we then obtain
\[
\tilde{H}^{(2)}
\coloneqq
    \begin{bmatrix}
        \mathtt{A}_1(q) & \cdots 
        \\
        \tilde{\mathtt{A}}_2(q) & \cdots
        \\
        \vdots
        \\
        \tilde{\mathtt{A}}_d(q) & \cdots
        \\
        \bp
        \\
        \be
    \end{bmatrix}
    =
    \begin{bmatrix}
     \cdots & x_{T-1}^1 & \mathcolor{red}{x_T^1} &\cdots 
    \\
    \cdots & x_{T-2}^1 & \mathcolor{blue}{x_{T-1}^1} &\cdots
    \\
    \vdots & \vdots & \mathcolor{blue}\vdots &\cdots
    \\
    \cdots & x_{T-q}^1 & \mathcolor{blue}{x_{T-q}^1} &\cdots
    \\
    \cdots & x_{T-1}^d & \mathcolor{blue}{x_{T-1}^d}
    \\
    \cdots & x_{T-2}^d & \mathcolor{blue}{x_{T-2}^d}
    \\
    \cdots & \vdots & \mathcolor{blue}\vdots \\
    \\
    \cdots & x_{T-q}^d & \mathcolor{blue}{x_{T-q}^d}
    \\
    \cdots & \bp_{T-1} & \bp_{T}
    \\
    \cdots & \be_1 & \be_1
    \end{bmatrix},
\]
where $\bp$ is the matrix of $(\bp_1, \cdots, \bp_T)$, $\be$ is the matrix of $(\be_1, \cdots, \be_1)$.

Note that $x_T^1$ in red is the target we wish to predict (masked as 0 initially), and the entries in blue is considered the input feature of our AR model (a linear regression model in this case),
and we are able to directly apply several theoretical results in \cite{bai2024transformers} with input $\tilde{\bH}^{(2)}$.
Specifically, for \cref{thm:any-variate-auto}, it follows directly from \citep[Theorem~4]{bai2024transformers} by setting $\lambda = 0$.

\end{proof}

Next, we present several approximation results from \cite{bai2024transformers}, which our approximation results follows immediately from.
Considering the general form of autoregressive data:
$\bx \in \R^{d \times T} \coloneqq (\bx_1, \dots, \bx_T)$, where $\bx_t = (x_t^1, \cdots, x_t^d) \in \R^d$.
Assuming our target (variate of interest) is in dimension $1$, we assume the autoregressive process generates $x_t^1$ as follows:
\begin{equation}
    x_{t}^1
    =
    f( \bx_{t-q: t-1}^{1:d})
    +
    \epsilon_t
    ,
\end{equation}
where $\epsilon_t \sim N(0, \sigma^2)$, $a_i^j \in \R^1$, and $f$ is a function of interest.
% Note that $f : \R^{q\times d} \mapsto \R$ is a function of interest.
We then present several results when $f$ varies.
% \paragraph{Generalized Linear Models.}
% Following the above setup, let $f : \R \mapsto \R$ be a link function that is non-decreasing and $C^2$-smooth that operates element-wise on input matrices.
% Considering the following convex ERM problem:
% \begin{align*}
% \hat{\bw}_{\text{GLM}}
% &\coloneqq
% \argmin_{\bw \in \R^{dq}}
% \hat{L}_{GLM} (\bw)
% \coloneqq
% \frac{1}{T-1}
% \sum_{t=1}^{T-1}
% \ell
% \left(
% \langle 
% \bw ,
% [\bx_{t-1:t-q}^1 ; \cdots ; \bx_{t-1:t-q}^d]
% \right),
% \end{align*}
% where $\ell(t, j) = -yt + \int_0^t f(s) ds$ is the convex loss associated with $f$.

% \begin{proposition}[Generalized Linear Models ICL with MOIRAI]
% For any $0 < \alpha < \beta$ with $\kappa= \nicefrac{\beta}{\alpha}$, assume \cref{assumption:effective-regression} holds for the above problem, $B_w > 0$, $B_x > 0$, $\kappa_w$
\paragraph{Non-Linear AR.}
% \end{proposition}
Here we analyze that when the autoregressive process is generated by a 2 layer ReLU network with look back window size $q$.
Suppose the prediction function $\text{pred}(\bx, \bw) \coloneqq \sum_{k=1}^K u_k r ( \bv_k^\top \bx)$ is given by a two-layer neural network, parameterized by $\bw = [\bw_k, u_k]_{k\in[K]} \in \R^{K(d+1)}$.
Consider the ERM problem:
\[
\min_{\bw \in \cW}
\hat{L}_N(\bw)
\coloneq
\frac{1}{2N}
\sum_{i=1}^N
\ell( \text{pred}(\bx_i, \bw), y_i )
=
\frac{1}{2N}
\sum_{i=1}^N
\ell
\left(
\sum_{k=1}^K
u_k r(\bv_k^\top \tilde{\bx}_i), x_T^1 
\right),
\]
where $\cW$ is a bounded domain and $\tilde{\bx}_i \in \R^{qd}$ is a flatten version of $\bx_{t-q:t-q} \in \R^{d\times q}$.
\begin{proposition}
    Fix any $B_w, B_u > 0$, $L \geq 3, \nu > 0$, and $\varepsilon > 0$.
    Suppose that
    \begin{enumerate}
        \item Both the activation function $r$ and the loss function $\ell$ is $C^4$-smooth.
        \item $\cW$ is a closed domain such that $\cW \subset \left\{ \bw = [\bv_k;u_k]_{k\in[K]} \in \R^{K(d+1)} : \norm{\bv_k}_2 \leq B_v, |u_K| \leq B_u \right\}$, and $\text{Proj}_{\cW} = \text{MLP}_{\bm{\theta_2}}$ for some MLP layer with hidden dimension $D_w$ and $\norm{\bm{\theta}_2}_{\text{op}} \leq C_w$.
    \end{enumerate}
    Then there exists a $(L_1 + 2 L_2)$-layer MOIRAI transformer with 
    \begin{align*}
    \max_{\ell\in[L_1+1,2L_2]}
    M^{(\ell)} \leq \tilde{O}(\varepsilon^{-2}), \quad
    \max_{\ell\in[L_1+1,2L_2]}
    D^{(\ell)} \leq \tilde{O}(\varepsilon^{-2}) + D_w, \quad
    \\
    \norm{\bm{\theta}}_{\text{op}} \leq O(1+\eta) + C_w, \quad
    \sum_{\ell=1}^{L_1} M^{(\ell)} = d_{\max} + q_{\max}.
    \end{align*}
    where we hide the constants $K, B_x, B_u, B_v, C^4$,
    satisfies the following
    \[
    \norm{ \hat{\bw} - \bw^{L}_{\text{GD}} }_2
    \leq 
    L_f^{-1} ( 1 + \eta L_f)^L \varepsilon,
    \]
    where $L_f = \sup_{w\in\cW} \norm{ \nabla^2 \hat{L}_N(\bw) }_2$.
    
\end{proposition}

\paragraph{Maximum Likelihood Estimation (Gaussian) via Transformers.}
The next result shows that MOIRAI transformers are also capable of performing maximum likelihood estimation on any input multi-variate time series.
Given a data generated by some $\mathtt{AR}_d(q)$ process with parameter $(\bw_1, \cdots, \bw_q) \subset \R^{d}$: 
$(\bx_1, \cdots, \bx_T) \subset \R^d$, the conditional likelihood $f(\cdot)$ of observing $\bx_t$ is
\[
f(\bx_t \mid \bx_{t-1}, \cdots, \bx_{t-q} )
\coloneq
\frac{1}{\sqrt{2\pi \sigma^2}}
\exp
\left(
\frac{-( \bx_t - \sum_{i=1}^q \langle \bw_i,  \bx_{t-i} \rangle )^2 }{2 \sigma^2}
\right).
\]
The goal is to estimate the mean vector $(\bw_1, \cdots, \bw_q)$ and the variance $\sigma^2$ by minimizing the negative log-likelihood loss.
Note that with $n \geq d$, the loss is strongly convex.
The optimization over the NLL Loss has two steps: estimating the mean vector: $\hat{\bw}$, and then derive the variance $\hat{\sigma}^2$ with the following closed-form solution:
\[
\sigma^2
=
\frac{1}{T}
\sum_{t=1}^T
\left(
\bx_t - \sum_{i=1}^q
\langle \hat{\bw}_i, \bx_{t-i} \rangle
\right)^2.
\]

\begin{theorem}\label{thm:moirai-mle}
    Given a set of input data generated by some $\mathtt{AR}_d(q)$ process: $\bX, \in \R^{n \times d}, \bY \in \R^n$, considering the following negative log-likelihood loss, the goal is to find a set of parameters $\bw \in \R^d, \sigma^2 \in \R^+$ to minimize the following loss
    \[
    L_{\text{NLL}}
    ( \bw, \sigma )
    \coloneq
    \frac{n}{2}
    \log (2 \pi \sigma^2) +
    \frac{1}{2\sigma^2}
    \sum_{t=q+1}^T
    \left(
    \bx_t - \sum_{i=1}^q
    \langle \bw_i, \bx_{t-i} \rangle
    \right)^2
    \]
    We denote $\bw^\star, \sigma^\star$ as the ERM satisfying the NLL Loss.
    There exists a $(L_1 + L_2 + 2)$-layer MOIRAI Transformer such that its first $L_1+L_2$ layers follow the same requirement in \cref{thm:any-variate-auto}, and the last two layers each has two and one heads, it estimates $\bw, \bm{\sigma}$ with bounded error:
    \[
    \norm{ \hat{\bw} - \bw^\star } \leq
    \varepsilon,
    \]
    and the estimated variance is bounded by
    \[
    \left\vert \hat{\sigma}^2 - {\sigma^\star}^2 \right\vert
    \leq
    2 E B_x \varepsilon + B_x^2 \varepsilon
    =
    \tilde{O}(\varepsilon + \varepsilon^2),
    \]
    where $E \leq B(1+B_w)$, and $\tilde{O}$ hides the values dependent on $B_x, B_w$.
\end{theorem}


\begin{proof}[Proof of \cref{thm:moirai-mle}]
    
\[
L_{\text{NLL}}
( \bw, \sigma )
\coloneq
\frac{n}{2}
\log (2 \pi \sigma^2) +
\frac{1}{2\sigma^2}
\sum_{t=1}^T
\left(
\bx_t - \sum_{i=1}^q
\langle \bw_i, \bx_{t-i} \rangle
\right)^2.
\]
Following \cref{thm:any-variate-auto}, the first $L_1 + L_2$ layers of MOIRAI obtains $\hat{\bw}$ such that the $L_1+L_2+1$-th layer takes the following as input
% Finally, by following the proof in \citep[Proposition~C.2]{bai2024transformers}, we get
\begin{align*}
    \tilde{\bh}_i^{(L_1+L_2)}
    =    \left[
    x^1_i ; 
    \bx_{i-1:i-q}^1
    ;
    \bx_{i-1:i-q}^2
    ;
    \cdots 
    ;
    \bx_{i-1:i-q}^d
    ; 
    \bw^\star + \bm{\varepsilon}
    ; \bm{0} ; 1; t_i
    \right],
\end{align*}
where $\bw^\star + \bm{\varepsilon} \in \R^{qd}$ is the flatten mean vectors.
For the simplicity of notations, for the $i$-th column, we denote $x^1_i$ with $\tilde{\by}_i$, and denote $[\bx_{i-1:i-q}^1
    ;
    \bx_{i-1:i-q}^2
    ;
    \cdots 
    ;
    \bx_{i-1:i-q}^d]$ as $\tilde{\bx}_i \in \R^{qd}$, as they correspond to the label and feature of our AR model, respectively.
$\bm{\varepsilon} \in \R^{dq}$ satisfies 
\[
\norm{\bm{\varepsilon}}
\leq
\varepsilon \cdot (\eta B_x).
\]

Now we start to construct the $(L_1+L_2+1)$-th layer.
One can then construct
\begin{align*}
    \bQ_1^{L+1} \bh^L_i &= [ \bm{0} ; \tilde{\bx}_i ; \bm{0} ], \quad \bK^{L+1}_1 \bh^L_j = [ \bm{0} ; \hat{\bw} ; \bm{0} ], \quad \bV^{L+1}_1 \bh_k^L = [   \bm{0} ; 1 ; \bm{0} ]
    \\
    \bQ_2^{L+1} \bh^L_i &= [ \bm{0} ; \tilde{\bx}_i ; \bm{0} ], \quad \bK^{L+1}_2 \bh^L_j = [ \bm{0} ; -\hat{\bw} ; \bm{0} ], \quad \bV^{L+1}_2 \bh_k^L = [ \bm{0} ; -1 ; \bm{0} ].
\end{align*}

The above construction gives us
\begin{align*}
    \bh_i^{L+1}
    &=
    \bh_i^{L}
    +
    \frac{1}{n}
    \sum_{j=1}^{n}
    \sum_{m=1}^2
    \sigma\left(
    \langle 
    \bQ^{L+1}_m \bh_{n+1}^L,
    \bK_m^{L+1}
    \bh_{j}^L
    \rangle
    \right)
    \bV_m^{L+1}
    \bh_j^L
    \\
    &=
    [ \tilde{\by}_i ; \tilde{\bx}_i; \hat{\bw} ; \bm{0} ; 1 ; t_i]
    +
    \left(
    \sigma
    (
    \langle \hat{\bw}, \bx_i \rangle
    )
    -
    \sigma 
    (
    -
    \langle \hat{\bw}, \bx_i \rangle
    )
    \right)
    \cdot [ \bm{0} ; 1 ; \bm{0} ]
    \\
    &=
    [ \tilde{\by}_i ; \tilde{\bx}_i; \hat{\bw} ;  \langle \hat{\bw} , \tilde{\bx_i} \rangle,  \bm{0} ; 1 ; t_i].
\end{align*}

Next, we construct the last layer as
\begin{align*}
    \bQ_1^{L+1} \bh^L_i &= [ ... ; \tilde{\by}_i -  \langle \hat{\bw}, \tilde{\bx}_i \rangle  ; ... ], \quad \bK^{L+1}_1 \bh^L_j = [ ... ; \tilde{\by}_j -  \langle \hat{\bw}, \tilde{\bx}_j \rangle ;  ... ], \quad \bV^{L+1}_1 \bh_k^L = [ \bm{0} ; 1 ; \bm{0} ]
    % \\
    % \bQ_1^{L+1} \bh^L_i &= [ \bm{0} ; \bx_i ; \bm{0} ], \quad \bK^{L+1}_1 \bh^L_j = [ \bm{0} ; -\hat{\bw} ; \bm{0} ], \quad \bV^{L+1}_1 \bh_k^L = [ \bm{0} ; 1 ; \bm{0} ].
\end{align*}

Finally, the result becomes 
\begin{align*}
    \bh_{i}
    =
    \frac{1}{n}
    [ ... ; \sum_{\mu=1}^n ( \by_\mu - \langle \bx_\mu , \hat{\bw} \rangle )^2; ... ]
    =
    [ ... ; \hat{\sigma^2}; ... ].
\end{align*}

% \begin{align*}
%     \langle \hat{\bw}, \tilde{\bx}_t \rangle
%     &=
%     \langle \bw^\star + \bm{\varepsilon}, \tilde{\bx}_t \rangle
%     \\
%     &=
%     \langle \bw^\star, \tilde{\bx}_t \rangle
%     +
%     \langle \bm{\varepsilon}, \tilde{\bx}_t \rangle
%     \\
%     &\leq
%     B_w B_x \cdot d) +
%     \varepsilon \eta B_x^2 (d)
% \end{align*}
Thus, we complete the proof.
\end{proof}

% \subsection{Proof of Kernel Regression}

% \begin{proof}
% We first apply our Lemma~\ref{lem:input-causal} to transform the input sequence into the following using a 1-layer attention layer
% \begin{equation*}
%     \tilde{\bH}^{(1)}
%     =
%     \begin{bmatrix}
%         \bx_1 & \bx_2 & \dots & \bx_{\text{test}}
%         \\
%         \bx_{\text{test}} & \bx_{\text{test}} & \dots & \bx_{\text{test}}
%         \\
%         \by_1 & \by_2 & \dots & \by_{\text{test}}
%         \\
%         \bp_1 & \bp_2 & \dots & \bp_{\text{test}}
%     \end{bmatrix}.
% \end{equation*}
%     We then set the $\bW^{(0)}_1, \bW^{(0)}_2$ to be zero matrices.
%     Note that the $\by$ and $\bp$ rows are remained unchanged due to  \cite[Proposition~A.5]{bai2024transformers}\footnote{ (Joining parallel single-layer transformers}.
    
%     Next we set 
%     \begin{equation*}
%         \tilde{\bW}_Q^{(m)}
%         =
%         \begin{bmatrix}
%             \frac{1}{h} I_{2d}
%             & \mathbf{0}
%             \\ 
%             \mathbf{0} &
%             I
%         \end{bmatrix}
%         +
%         \begin{bmatrix}
%             0 & -1/h & 0 & \dots & 0
%             \\
%             0 & -1/h & 0 & \dots & 0
%             \\
%             \vdots & \vdots & \vdots & \dots & \vdots
%             \\
%             0 & -1 & 0 & \dots & 0
%         \end{bmatrix},
%     \end{equation*}
%     where entries in the first $2d$ rows and columns is multiplied by $1/h$.

%     By setting $\bW_Q$ as following (we omit the layer index here for simplicity)
%     \begin{equation*}
%         \bW^\prime_Q 
%         \tilde{\bW}_Q
%         \tilde{\bH}^{(2)}
%         =
%         \begin{bmatrix}
%             a_m \cdot \frac{\bx_1 - \bx_{\text{test}}}{h}
%             &
%             a_m \cdot\frac{\bx_2 - \bx_{\text{test}}}{h}
%             & 
%             \dots 
%             &
%             \bx_{\text{test}}
%             \\
%          \by_1 & \by_2 & \dots & \by_{\text{test}}
%         \\
%         \bp_1 & \bp_2 & \dots & \bp_{\text{test}}
%         \end{bmatrix},
%     \end{equation*}
%     and set $\bW_K$ as 
%     \begin{equation*}
%         \bW_K \tilde{\bH}^{(2)}
%         =
%         I,
%     \end{equation*}
%     we get
%     \begin{align*}
%         \sum_{m=1}^M
%         \bW_V^{(m)}
%         \tilde{\bH}^{(m)}
%         \sigma \left( \left\langle \bW_Q^{(m)} \tilde{\bH}^{(m)},  \bW_K^{(m)} \tilde{\bH}^{(m)} \right\rangle \right)
%         &=
%         \sum_{m=1}^M
%         c_m
%         \sigma
%         \left(
%         a_m \cdot 
%         \frac{\bx_m - \bx_{\text{test}}}{h}
%         \right)
%         \\
%         &=
%         \begin{bmatrix}
%             f\left(\frac{\bx_1 - \bx_{\text{test}}}{h}\right)
%             &
%             f\left(\frac{\bx_2 - \bx_{\text{test}}}{h}\right)
%             & 
%             \dots 
%             &
%             \bx_{\text{test}}
%             \\
%         \by_1 & \by_2 & \dots & \by_{\text{test}}
%         \\
%         \bp_1 & \bp_2 & \dots & \bp_{\text{test}}
%         \end{bmatrix}
%         \\
%         &=
%         \begin{bmatrix}
%             \epsilon + K\left(\frac{\bx_1 - \bx_{\text{test}}}{h}\right)
%             &
%             \epsilon + K\left(\frac{\bx_2 - \bx_{\text{test}}}{h}\right)
%             & 
%             \dots 
%             &
%             \bx_{\text{test}}
%             \\
%         \by_1 & \by_2 & \dots & \by_{\text{test}}
%         \\
%         \bp_1 & \bp_2 & \dots & \bp_{\text{test}}
%         \end{bmatrix}.
%     \end{align*}

%     The approximation error is then bounded by
%     \begin{align*}
%         \norm{
%         \hat{r}
%         (\bx)
%         -
%         \frac{\sum_{i=1}^N
%         f( \frac{\bx - \bx_i}{h} ) \by_i}{\sum_{i=1}^N
%         f( \frac{\bx - \bx_i}{h} )}
%         }
%         &=
%         \norm{
%         \sum_{i=1}^N
%         \frac{
%         K( \frac{\bx - \bx_i}{h} ) \by_i}{\sum_{j=1}^N
%         K( \frac{\bx - \bx_j}{h} )}
%         -
%         \sum_{i=1}^N
%         \frac{
%         f( \frac{\bx - \bx_i}{h} ) \by_i}{\sum_{j=1}^N
%         f( \frac{\bx - \bx_j}{h} )}
%         }    
%         \\
%         &=
%         \norm{
%         \sum_{i=1}^N
%         \left[
%         \frac{
%         K( \frac{\bx - \bx_i}{h} ) \by_i}{\sum_{j=1}^N
%         K( \frac{\bx - \bx_j}{h} )}
%         -
%         \frac{
%         f( \frac{\bx - \bx_i}{h} ) \by_i}{\sum_{j=1}^N
%         f( \frac{\bx - \bx_j}{h} )}
%         \right]
%         }    
%         \\
%         &=
%         \norm{
%         \sum_{i=1}^N
%         \left[
%         \langle \gamma, \by \rangle
%         -
%         \langle \gamma_f, \by \rangle
%         \right]
%         }    
%         \\
%         &\leq
%         \sum_{i=1}^N
%         \norm{
%         \langle \gamma, \by \rangle
%         -
%         \langle \gamma_f, \by \rangle
%         }    
%         \\
%         &=
%         \sum_{i=1}^N
%         \norm{
%         \langle \gamma - \gamma_f, \by \rangle
%         }    
%         \\
%         &\leq
%         \sum_{i=1}^N
%         \norm{
%         \gamma - \gamma_f
%         }
%         \norm{\by}
%         \\
%         &\leq
%         \frac{n B_y \varepsilon^2}{s},
%     \end{align*}
%     where $s \coloneqq  \sum_{j=1}^N K(\nicefrac{\bx - \bx_j}{h})$, and the last equality comes from Lemma~\ref{lem:prob-perturbation}.
% \end{proof}


\clearpage


\subsection{Proof of the Lipschitzness of Any-Variate Transformers}\label{proof:tr-lipschitz}

We first show the Lipschitzness of each component in an Any-Variate Transformer.
For any $p \in [1, \infty]$, let $\norm{\bH}_{2,p} \coloneqq ( \sum_{i=1}^N \norm{\bh_i}_2^p )^{1/p}$ denote the column-wise $(2, p)$-norm of $\bH$.
For any radius $\mathtt{R} > 0$, we denote 
$\cH_{\mathtt{R}} \coloneqq \{ \bH : \norm{\bH}_{2, \infty} \leq \mathtt{R} \} $ be the ball of radius $\mathtt{R}$ under norm $\norm{\cdot}_{2, \infty}$.


\begin{lemma}\label{lem:attn-lipschitz}
    For a single Any-Variate attention layer, $\bm{\theta}_1 = \{(\bV_m, \bQ_m, \bK_m, u^1_m, u^2_m)\}_{m\in[M]}$, we introduce its norm
    \begin{equation*}
        \Vert
        \bm{\theta}_1
        \Vert
        \coloneqq
        \max_{m\in[M]}
        \{ 
            \norm{\bQ_m}_{\text{op}},
            \norm{\bK_m}_{\text{op}},
            \vert u^1_m \vert,
            \vert u^2_m \vert
        \}
        +
        \sum_{m=1}^M \norm{\bV_m}_{\text{op}}
    \end{equation*}
    For any fixed hidden dimension $D^\prime$, we consider
    \begin{equation*}
        \Theta_{1, B}
        \coloneqq
        \{ 
        \bm{\theta}_{1}
        :
        \lvert \lvert \lvert
        \bm{\theta}_1
        \rvert \rvert \rvert
        \leq 
        B
        \}.
    \end{equation*}
    Then for $\bH \in \cH_{\mathtt{R}}$, $\bm{\theta}_1 \in \Theta_{1, B}$, the function 
    $(\bm{\theta}_1, \bH) \mapsto \text{Attn}_{ \bm{\theta}_1}$ is $(1+\iota)$-Lipschitz w.r.t. $\bm{\theta}_1$, where $\iota = \max \{  B^2\mathtt{R}^2 +T + (T-1)d, B(T-1)d  \}$,
    and $(1 + B^3 \mathtt{R}^2)$-Lipschitz w.r.t. $\bH$.
\end{lemma}

\begin{proof}
    Given some $\epsilon > 0$, some set $X$ and a function class $\mathcal{F}$.
    If $\mathcal{F}$ is $L$-Lipschitzness, i.e., 
    \begin{equation*}
        \norm{f(x_1) - f(x_2)} \leq
        L \norm{x_1 - x_2}, \quad \text{for all }f \in \mathcal{F}.
    \end{equation*}
    Then, the following holds
    \begin{equation*}
        N(\epsilon, \mathcal{F}, \norm{\cdot})
        \leq
        N( \nicefrac{\epsilon}{L}, X, \norm{\cdot} ).
    \end{equation*}

    Define 
    \begin{equation*}
        \Theta_{\text{attn}, B}
        \coloneqq
        \{ \theta_{\text{attn}}:  \lvert \lVert \theta_{\text{attn}} \rVert \rvert  \leq B \}.
    \end{equation*}

    The output of the Any-Variate Attention $[\tilde{h}_i]$ is given by
    \begin{equation*}
        \tilde{h}_i
        =
        h_i
        +
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \sigma
        \left(
        \Braket{ \Qb_m h_i, \Kb_m h_j }
        \cdot \Vb_m h_j
        +
        u_m^1
        \star 
        \Ub
        +
        u_m^2
        \star
        \bar{\Ub}
        \right).
    \end{equation*}
    We also define $\theta^\prime_{\text{attn}} = \{ \Vb_m^\prime, \Qb_m^\prime, \Kb_m^\prime, u_m^{1\prime}, u_m^{2 \prime} \}_{m \in [M]}$.
    $\tilde{h}^\prime_i$ as
    \begin{equation*}
        \tilde{h}_i^\prime
        =
        h_i
        +
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \sigma
        \left(
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        \cdot \Vb_m^\prime h_j
        +
        u_m^{1\prime}
        \star 
        \Ub
        +
        u_m^{2 \prime}
        \star
        \bar{\Ub}
        \right).
    \end{equation*}

    Now we bound 
    $
    \norm{
    \text{Attn}_{\bm{\theta}_1}( \bH )
    -
    \text{Attn}_{\bm{\theta}_1^\prime}(\bH)
    }_{2, \infty}
    =
    \max_i 
    \norm{ \tilde{\bh}_i - \tilde{\bh}_i^\prime }_2 
    $ as follows
    \begin{align*}
        \norm{ \tilde{\bh}_i - \tilde{\bh}_i^\prime }_2 
        &=
        \Bigg\Vert
        \sum_{m=1}^M
        \frac{1}{N}
        \Bigg[
        \sum_{j=1}^N
        \sigma
        \left(
        \Braket{ \Qb_m h_i, \Kb_m h_j }
        +
        u^1_m
        \star 
        \Ub
        +
        u^2_m
        \star
        \bar{\Ub}
        \right)
        \Vb_m h_j
        -
        \\
        &\quad\quad
        \sum_{j=1}^N
        \sigma
        \left(
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        +
        u_m^{1\prime}
        \star 
        \Ub
        +
        u_m^{2 \prime}
        \star
        \bar{\Ub}
        \right)
        \Vb_m^\prime h_j
        \Bigg]
        \Bigg\Vert_2
        \\
        &\leq
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \Big\Vert
        \sigma
        \left(
        \Braket{ \Qb_m h_i, \Kb_m h_j }
        +
        u^1_m
        \star 
        \Ub
        +
        u^2_m
        \star
        \bar{\Ub}
        \right)
        \Vb_m h_j 
        \\
        &\quad\quad
        -
        \sigma
        \left(
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        +
        u_m^{1\prime}
        \star 
        \Ub
        +
        u_m^{2 \prime}
        \star
        \bar{\Ub}
        \right)
        \Vb_m^\prime h_j
        \Big\Vert_2
        \\
        &\leq
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \norm{h_j}_2
        \Big\Vert
        \sigma
        \left(
        \Braket{ \Qb_m h_i, \Kb_m h_j }
        +
        u_m^1
        \star 
        \Ub
        +
        u_m^2
        \star
        \bar{\Ub}
        \right)
        \Vb_m
        \\
        &\quad\quad
        -
        \sigma
        \left(
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        +
        u_m^{1\prime}
        \star 
        \Ub
        +
        u_m^{2 \prime}
        \star
        \bar{\Ub}
        \right)
        \Vb_m^\prime
        \Big\Vert_{\text{op}}.
    \end{align*}


    Let 
    \begin{align*}
        A &= 
        \Braket{ \Qb_m h_i, \Kb_m h_j }
        +
        u^1_m
        \star 
        \Ub
        +
        u_m^2
        \star
        \bar{\Ub}
        \\
        B &= 
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        +
        u_m^{1\prime}
        \star 
        \Ub
        +
        u_m^{2 \prime}
        \star
        \bar{\Ub}.
    \end{align*}

    By triangle inequality, we have
    \[
    \norm{
    \sigma(A) V_m - \sigma(B) V_m^\prime
    }
    \leq 
    \norm{\sigma(A)}_{\text{op}}
    \norm{\bV_m - \bV_m^\prime}_{\text{op}}
    +
    \norm{\sigma(A) - \sigma(B)}_{\text{op}}
    \norm{V^\prime_m}_{\text{op}}.
    \]

    Note that $\sigma(\cdot)$ is $1$-Lipschitz, we get
    \begin{align*}
        \norm{\sigma(A) - \sigma(B)}_{\text{op}}
        &\leq
        \norm{A-B}_{\text{op}}
        \\
        &=
        \norm{
        \Braket{ \Qb_m h_i, \Kb_m h_j } 
        -
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        +
        ( u^1_m - u_m^{1\prime} ) \bU
        +
        ( u^2_m - u_m^{2 \prime} ) \hat{\bU}
        }_{\text{op}}
        \\
        &\leq
        \norm{
        \big\vert
            \Braket{ \Qb_m h_i, \Kb_m h_j } 
        -
        \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
        \big\vert
        +
        \bigg\vert
        \norm{
        ( u^1_m - u_m^{1\prime} ) \star \bU
        }
        \bigg\vert
        +
        \bigg\vert
        \norm{
        ( u^2_m - u_m^{2 \prime} ) \star \bar{\bU}
        }
        \bigg\vert
        }
        .
    \end{align*}

    For the first term in the last inequality, we have
    \begin{align*}
    \Braket{ \Qb_m h_i, \Kb_m h_j } 
        -
    \Braket{ \Qb_m^\prime h_i, \Kb_m^\prime h_j }
    &\leq
    \norm{
    \bQ_m - \bQ_m^\prime
    }
    \norm{h_i}
    \norm{h_j}
    \norm{\bK_m}
    +
    \norm{
    \bK_m - \bK_m^\prime
    }
    \norm{h_i}
    \norm{h_j}
    \norm{\bQ_m}
    \\
    &=
    \mathtt{R}^2 B
    \left(
    \norm{
    \bQ_m - \bQ_m^\prime
    }
    +
    \norm{
    \bK_m - \bK_m^\prime
    }
    \right).
    \end{align*}
    Further, we have
    \[
    \norm{ (u^1_m - u_m^{1\prime}) \star \bU }
    \leq
    \vert 
    u^1_m - u_m^{1\prime}
    \vert 
    \norm{\bU}
    \leq
    T    \vert 
    u^1_m - u_m^{1\prime}
    \vert ,
    \]
    where $T$ is the length of each variate (lookback window size).
    \[
    \norm{ (u^2_m - u_m^{2 \prime}) \star \bar{\bU} }
    \leq
    \vert 
    u^2_m - u_m^{2 \prime}
    \vert 
    \norm{\bar{\bU}}
    \leq
    (T-1)d
    \vert 
    u^2_m - u_m^{2 \prime}
    \vert 
    ,
    \]
    where $d$ is the number of variates.
    
    Thus, we have
    \begin{align*}
        \norm{\sigma(A) - \sigma(B)}_{\text{op}}
        \norm{\bV_m^\prime}_{\text{op}}
        &\leq
        B 
        \left( 
        \mathtt{R}^2 B
        \left(
        \norm{
        \bQ_m - \bQ_m^\prime
        }
        +
        \norm{
        \bK_m - \bK_m^\prime
        }
        \right)
        +
        T \left( \vert u^1_m - u_m^{1\prime} \vert  \right)
        +
        (T-1)d
        \left(
        \vert 
        u^2_m - u_m^{2 \prime}
        \vert
        \right)
        \right)
        \\
        &\leq
        B \cdot \max \{ \mathtt{R}^2 B, (T-1) d \}
        \mathcolor{red} \cdot
        \left(
        \norm{\bQ_m - \bQ^\prime_m}
        +
        \norm{\bK_m - \bK^\prime_m}
        +
        \vert u^1_m - u_m^{1\prime} \vert 
        +
        \vert u^2_m - u_m^{2 \prime} \vert 
        \right)
        .
    \end{align*}
    Next, we bound
    \begin{align*}
        \norm{\sigma(A)}_{\text{op}}
        \leq
        \norm{
        A
        }_{\text{op}}
        \leq
        B^2 \mathtt{R}^2 + ( T + (T-1)d ),
    \end{align*}
    due to the fact that 
    \[
    \norm{A}
    \leq
    \norm{
    \bQ_m h_i
    }
    \norm{
    \bK_m h_j
    }
    \norm{
    u^1_m \bU
    }
    \norm{
    u^2_m \bar{\bU}
    }.
    \]
    Overall, the Any-Variate Attention is 
    $
    \max \{  B^2\mathtt{R}^2 +T + (T-1)d, B(T-1)d  \}
    $-Lipshcitz in $\bm{\theta}_1$.
\end{proof}

\begin{proof}
    We start by considering $\bH^\prime = [\bh_i^\prime]$ and
    \begin{equation*}
        \tilde{\bh}_i^\prime
        =
        \bh^\prime_i
        +
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \sigma
        \left(
        \langle
        \Qb_m \bh^\prime_i,
        \Kb_m \bh^\prime_j
        \rangle
        +
        u^1_m \cdot \Ub
        +
        u^2_m \cdot \bar{\Ub}
        \right)
        \cdot 
        \Vb_m \bh_j^\prime.
    \end{equation*}
    We then bound 
    \begin{align*}
        &\norm{ (\tilde{\bh}_i^\prime - \bh_i^\prime)
        -
        ( \tilde{\bh}_i - \bh_i )
        }_2
        \\
        &=
        \norm{
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \left[
        \sigma
        \left(
        \langle
        \Qb_m \bh^\prime_i,
        \Kb_m \bh^\prime_j
        \rangle
        +
        u^1_m \cdot \Ub
        +
        u^2_m \cdot \bar{\Ub}
        \right)
        \Vb_m \bh_j
        -
        \left(
        \langle
        \Qb_m \bh^\prime_i,
        \Kb_m \bh^\prime_j
        \rangle
        +
        u^1_m \cdot \Ub
        +
        u^2_m \cdot \bar{\Ub}
        \right)
        \Vb_m \bh_j^\prime
        \right]
        }_2
        \\
        &\leq
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \norm{\Vb_m}_{\text{op}}
        \norm{
        \sigma
        \left(
        \langle
        \Qb_m \bh^\prime_i,
        \Kb_m \bh^\prime_j
        \rangle
        +
        u^1_m \cdot \Ub
        +
        u^2_m \cdot \bar{\Ub}
        \right)
         \bh_j
        -
        \left(
        \langle
        \Qb_m \bh^\prime_i,
        \Kb_m \bh^\prime_j
        \rangle
        +
        u^1_m \cdot \Ub
        +
        u^2_m \cdot \bar{\Ub}
        \right)
        \bh_j^\prime
        }_2
        \\
        &\leq
        \sum_{m=1}^M 
        \frac{1}{N}
        \sum_{j=1}^N
        \norm{\Vb_m}_{\text{op}}
        \Big\{
        \vert 
        \sigma
        \left( \langle \Qb_m \bh_i, \Kb_m \bh_j \rangle + u^1_m \Ub + u^2_m \bar{\Ub} \right)
        \vert
        \cdot \norm{\bh_j - \bh_j^\prime}_2
        \\
        &\quad\quad\quad\quad+
        \vert
        \sigma
        \left( \langle \Qb_m \bh_i, \Kb_m \bh_j \rangle + u^1_m \Ub + u^2_m \bar{\Ub} \right)
        -
        \sigma
        \left( \langle \Qb_m \bh_i^\prime, \Kb_m \bh_j \rangle + u^1_m \Ub + u^2_m \bar{\Ub} \right)
        \vert
        \cdot
        \norm{\bh_j^\prime}_2
        \\
        &\quad\quad\quad\quad+
        \vert
        \sigma
        \left( \langle \Qb_m \bh_i, \Kb_m \bh_j \rangle + u^1_m \Ub + u^2_m \bar{\Ub} \right)
        -
        \sigma
        \left( \langle \Qb_m \bh_i^\prime, \Kb_m \bh_j^\prime \rangle + u^1_m \Ub + u^2_m \bar{\Ub} \right)
        \vert
        \norm{\bh_j^\prime}_2
        \Big\}
        \\
        &\leq
        \sum_{m=1}^M
        \frac{1}{N}
        \sum_{j=1}^N
        \norm{\Vb_m}_{\text{op}}
        \cdot
        3
        \norm{\Qb_m}_{\text{op}}
        \norm{\Kb_m}_{\text{op}}
        \mathtt{R}^2
        \norm{\bh_j - \bh_j^\prime}_2
        \\
        &\leq
        B^3 \mathtt{R}^2 \norm{\bH - \bH^\prime}_{2, \infty}.
    \end{align*}
    Where the third inequality comes from the fact that ReLU is $1$-Lipschitzness, and the fourth and fifth inequality comes from the AM-GM inequality.
    For more details, refer \citep[Section~J.2]{bai2024transformers}
\end{proof}

\begin{corollary}[Lipschitz Constant of Single Layer Moirai Transformer]
    For a fixed number of heads $M$ and hidden dimension $D^\prime$, we consider
    \[
    \Theta_{\text{TF}, 1, B}
    =
    \{
    \bm{\theta}
    =
    ( \bm{\theta}_1, \bm{\theta}_2 )
    \}
    :
    M \text{ heads, }
    \text{hidden dimension }
    D^\prime,
    \norm{\bm{\theta}}_{\text{op}}
    \leq 
    B.
    \]
    Then for the function $\text{TF}^{\mathtt{R}}$ given by
    \[
    \text{TF}^{\mathtt{R}}
    :
    ( \bm{\theta}, \bH )
    \mapsto
    \texttt{clip}_{\mathtt{R}}
    (
    \text{MLP}_{\bm{\theta}_2}
    (
    \text{Attn}_{\bm{\theta}_1}
    (
    \bH
    )
    )),
    \quad
    \bm{\theta} \in 
    \Theta_{\text{TF}, 1, B},
    \bH \in \cH_{\mathtt{R}}.
    \]
    $\text{TF}^{\mathtt{R}}$ is $B_{\Theta}$-Lipschitz w.r.t. $\bm{\theta}$ and $B_H$-Lipschitz w.r.t. $\bH$,
    where 
    $B_{\Theta} = (1 + B^2) ( 1 + \iota ) + B\mathtt{R}(1 + B^3\mathtt{R}^2)$
    and 
    $B_H = (1 + B^2)(1 + B^3 \mathtt{R}^2 )$.
\end{corollary}


\begin{proposition}[Lipschitz Constant of Moirai Transformer]\label{proposition:lipschitz-moirai}
    For a fixed number of heads $M$ and hidden dimension $D^\prime$, we consider
    \[
    \Theta_{\text{TF}, L, B}
    =
    \{
    \bm{\theta}
    =
    ( \bm{\theta}_1^{(1:L)}, \bm{\theta}_2^{(1:L)} )
    \}
    :
    M^{(\ell)} = M,
    D^{(\ell)}
    =
    D^\prime,
    \norm{\bm{\theta}}_{\text{op}}
    \leq 
    B.
    \]
    Then for the function $\text{TF}^{\mathtt{R}}$ is $(L B_H^{L-1} B_{\Theta})$-Lipschitz in $\bm{\theta} \in \Theta_{\text{TF}, L, B}$ for any fixed $\bH$.
\end{proposition}


\begin{proof}
    For any $\bm{\theta} = ( \bm{\theta}_1, \bm{\theta}_2 )$, 
    $\bH \in \cH_{\mathtt{R}}$, and $\theta^\prime = (\theta^\prime_1, \theta^\prime_2)$, we have
    \begin{align*}
        \norm{
        \text{TF}_{\bm{\theta}}(\bH)
        -
        \text{TF}_{\theta^\prime}(\bH)
        }_{2, \infty}
        &\leq
        \norm{
        \text{MLP}_{\bm{\theta}_2}
        (
        \text{Attn}_{\bm{\theta}_1}
        (\bH)
        )
        -
        \text{MLP}_{\bm{\theta}_2}
        (
        \text{Attn}_{\theta^\prime_1}
        (\bH)
        )
        }_{2, \infty}
        +
        \\
        &\quad\quad
        \norm{
        \text{MLP}_{\bm{\theta}_2}
        (
        \text{Attn}_{\theta^\prime_1}
        (\bH)
        )
        -
        \text{MLP}_{\theta^\prime_2}
        (
        \text{Attn}_{\theta^\prime_1}
        (\bH)
        )
        }_{2, \infty}
        \\
        &\leq
        (1 + B^2)
        \norm{
        \text{Attn}_{\theta^\prime_1}
        (\bH)
        -
        \text{Attn}_{\theta^\prime_1}
        (\bH)
        }_{2, \infty}
        +
        B \bar{\mathtt{R}}
        \norm{ \bm{\theta}_2 - \theta_2^\prime }_{\text{op}}
        \\
        &\leq
        (1 + B^2) ( 1 + \iota ) \norm{\bm{\theta}_1 - \theta_1^\prime}_{\text{op}}
        +
        B \bar{\mathtt{R}}
        \norm{\bm{\theta}_2 - \theta_2^\prime}_{\text{op}}
        \leq 
        B_{\Theta} \norm{\bm{\theta} - \theta^\prime}_{\text{op}},
    \end{align*}
    where $\bar{\mathtt{R}} = \mathtt{R} + B^3 \mathtt{R}^3$, $\iota = \max \{  B^2\mathtt{R}^2 +T + (T-1)d, B(T-1)d  \}$.
    The second inequality comes from the fact $\norm{\text{Attn}_{\bm{\theta}}(\bH)} \leq \mathtt{R} + B^3 \mathtt{R}^3$.

    Further, for $\bH^\prime \in \cH_{\mathtt{R}}$, we have
    \begin{align*}
    \norm{
        \text{TF}_{\bm{\theta}}(\bH)  
        -
        \text{TF}_{\bm{\theta}}(\bH^\prime)
    }_{2, \infty}
    &\leq 
    (1 + B^2) \norm{
    \text{Attn}_{\bm{\theta}_1}(\bH)
    -
    \text{Attn}_{\bm{\theta}_1}(\bH^\prime)
    }
    \\
    &\leq
    (1 + B^2)(1 + B^3 \mathtt{R}^2 ) \norm{
    \bH - \bH^\prime
    }_{2, \infty}.
    \end{align*}

    For the multi-layer case, one can simply follow \citep[Proposition~J.1]{bai2024transformers} to conclude the proof.
    
\end{proof}


\clearpage


\subsection{Proof of \cref{thm:gen-bound-1}}\label{proof:gen-bound-1}

Let $\pi$ be a meta distribution, and each distribution drawn from $ \mathtt{P}^{(T)} \sim \pi$ satisfies the Dobrushin's condition.
We then define the single-path average loss as
\[
Y_{\theta, \mathtt{P}^{(T)}}
\coloneqq
\frac{1}{T}
\sum_{t=1}^T
\ell( \theta, \bz_t)
-
\mathbb{E}_{\bz \sim \mathtt{P}^{(T)}}
\left[
\ell( \theta, \bz)
\right].
\]

Now, we assume our pretraining data is generated by the following
\begin{enumerate}
    \item Sample $n$ distributions from $\pi$ i.i.d. to get $\mathtt{P}^{(T)}_j$, for $j = 1,\cdots,n$
    \item For each distribution $\mathtt{P}^{(T)}_j$, we sample $(\bz_{j,1}, \cdots, \bz_{j,T})$
\end{enumerate}


% For the pretraining data, we consider two cases when the following assumptions hold or not hold
\begin{assumption}\label{assumption:stationary}
    We assume that for each $j \in [n]$, $(z_{j, t})$ has marginals equal to some distribution $D$ for $t = 1, \cdots ,T$.
\end{assumption}

We first present several lemma and theorems that will be used later.
\begin{lemma}[\text{\citep[Example~5.8]{wainwright2019high}}]\label{lem:covering-number-unit-ball}
    Given any well-defined norm $\norm{\cdot}^\prime$.
    Let $\mathbb{B}$ be the $\R^d$ unit-ball in $\norm{\cdot}^\prime$, i.e. 
    $\mathbb{B} = \{ \theta \in \R^d \mid \norm{\theta}^\prime \leq 1 \}$,
    we have
    \[
    \log N( \delta, \mathbb{B}, \norm{\cdot}^\prime )
    \leq
    d 
    \log
    \left( 1 + \frac{2}{\delta} \right).
    \]
\end{lemma}

\begin{theorem}[\text{\citep[Theorem~5.3]{dagan2019learning}}]\label{thm:gen-bound-dobrushin}
    Given a function class $\cF$, such that $\vert f \vert \leq B$, for all $f \in \cF$.
    Let $\mathtt{P}^{(T)}$ be a distribution over some domain $Z^{(T)}$, assuming \cref{assumption:stationary} holds and $\alpha_{\log}( \mathtt{P}^{(T)} ) < \nicefrac{1}{2}$.
    Then for all $t > 0$,
    \[
    P_{\bz \sim \mathtt{P}^{(T)}}
    \left(
    \sup_{f\in\cF}
    \left\vert 
        \frac{1}{T}
        \sum_{i=1}^T
        f(z_i)
        -
        \mathbb{E}_{z}
        [
        f(z)
        ]
    \right\vert
    >
    C
    \left(
        \mathfrak{G}_{\mathtt{P}^{(T)}}( \cF)
        +
        \frac{B t}{\sqrt{T}}
    \right)
    \right)
    \leq
    e^{-t^2/2},
    \]
    for some universal constant whenever $1/2 - \alpha_{\log}(\mathtt{P}^{(T)})$ is bounded away from zero.
\end{theorem}

The following theorem is from \cite{dagan2019learning, kulske2003concentration}.
\begin{theorem}\label{thm:dobrushin-concentration}
    Let $\mathtt{P}^{(T)}_{\bz}$ be a distribution satisfying the Dobrushin's condition with coefficient $\alpha(\mathtt{P}_{\bz}^{(T)})$.
    Let $(\bz_1,\cdots,\bz_T) \sim \mathtt{P}^{(T)}$, and let $f: \bZ^{(T)} \rightarrow \R$ be a real-valued function with the following bounded difference property, with parameters $\lambda_1, \cdots, \lambda_T \geq 0$:
    \[
    \vert f(\bz) - f(\bz^\prime) \vert 
    \leq
    \sum_{t=1}^T
    \mathbbm{1}_{\bz_t\neq \bz_t^\prime}
    \lambda_t.
    \]
    Then for all $t > 0$,
    \[
    P
    \left(
        \vert 
        f(\bz)
        -
        \mathbb{E}
        [ f(\bz) ]
        \geq
        t
    \right)
    \leq
    2 \exp
    \left(
    -
    \frac{(1 - \alpha)t^2}{2 \sum_t \lambda_t^2}.
    \right)
    \]
\end{theorem}
The following corollary directly follows from the above result
\begin{corollary}\label{cor:subgaussian-loss}
    Following \cref{thm:dobrushin-concentration}, let
    \[
    \ell( \bz)
    \coloneqq
    \frac{1}{T}
    \sum_{t=1}^T
    \ell( \bz_t),
    \]
    where $0 \leq \ell(\bz_t) \leq B$ for all $t=1,\cdots,T$ and all $\bz \sim \mathtt{P}_{\bz}$.
    Then the variance of $\ell(\cdot)$ is bounded by
    \[
    \vert 
        \ell(\bz)
        -
        \ell(\bz^\prime)
    \vert
    \leq
    B.
    \]
    Then, the following holds
    \[
        P
        \left(
        \left\vert
        \ell(\bz)
        -
        \mathtt{E}
        [ \ell(\bz) ]
        \right\vert
        \geq
        t
        \right)
        \leq
        2
        \exp
        \left(
        \frac{- (1 - \alpha) t^2 }{2 \sum_t B^2}
        \right)
        .
    \]
\end{corollary}

\paragraph{Direct Application of \cref{thm:gen-bound-dobrushin}.}
By \cref{thm:gen-bound-dobrushin}, if \cref{assumption:stationary} holds, with probability over $1 - e^{-t^2/2}$, for any $\theta \in \Theta$, $\alpha_{\log}(\mathtt{P}^{(T)}_j) < 1/2$ we have

\[
\sup_{\theta \in \Theta }
\left\vert
Y_{\theta, \mathtt{P}^{(T)}}
\right\vert
\leq
C 
\left[
\mathfrak{G}_{\mathtt{P}^{(T)}_j}(\ell(\Theta))
+
\frac{2tB_x^2}{\sqrt{T}}
\right],
\]
where $\ell(\Theta)$ denotes the function class of $\ell( \theta, )$, for all $\theta\in\Theta$, and $C > 0$ is an universal constant.
Note that the above bound presents the naive learning bound for learning a single time series, which is a direct result from \cite{dagan2019learning}.



\begin{proof}
    

We then define a random process 
$\{ X_\theta \}$ as
\begin{align*}
X_{\theta}
\coloneqq
\frac{1}{n}
\sum^n_{j=1}
Y_{\theta, \mathtt{P}_j^{(T)}}
&=
\frac{1}{n}
\sum^n_{j=1}
\left[
\frac{1}{T}
\sum_{t=1}^T
\ell( \theta, \bz_{j,t})
-
\mathbb{E}_{\bz \sim \mathtt{P}^{(T)}_j}
\left[
\ell( \theta, \bz)
\right]
\right]
\\
&=
\left[
\frac{1}{nT}
\sum^n_{j=1}
\sum_{t=1}^T
\ell( \theta, \bz_{j,t})
\right]
-
\mathbb{E}_{\bz \sim \mathtt{P}^{(T)}_j, \mathtt{P}^{(T)}_j \sim \pi}
\left[
\ell( \theta, \bz)
\right].
\end{align*}

Now, to take supremum over $X_\theta$, we get
\begin{align*}
    \sup_{\theta \in \Theta}
    X_\theta
    &=
    \sup_{\theta \in \Theta}
    \frac{1}{n}
    \sum_{j=1}^n
    Y_{\theta, \mathtt{P}_j^{(T)}}
    \\
    &\leq
    \frac{1}{n}
    \sum_{j=1}^n
    \sup_{\theta \in \Theta}
    Y_{\theta, \mathtt{P}_j^{(T)}}.
\end{align*}

To upper bound $\sup | X_{\theta} |$,   we take a similar approach to \citep[Proposition~A.4]{bai2024transformers}.

Assuming the index set $\Theta$ is equipped with a distance metric $\rho$ and diameter $D$.
We assume that for any ball $\Theta^\prime$ of radius $r$ in $\Theta$, there exists some constant $C_1$ such that the covering number admits upper bound
\[
\log
N( \delta, \Theta^\prime, \rho)
\leq
d \log ( 2 A r / \delta),
\]
for all $0 < \delta \leq 2r$.

Now we select $\Theta_0$ such that it is a $(D_0/2)$-covering of $\Theta$.
The above assumption guarantees us that we can have a $\Theta_0$ such that $\log | \Theta_0 | \leq d \log (2AD/D_0)$.
By \cref{cor:subgaussian-loss}, $X_{\bm{\theta}}$ is a $\nicefrac{2 B_x^2}{(1 - \alpha)}$-subgaussian ($\alpha = \alpha( \mathtt{P}^{(T)})$).
Then, with probability at least $1 - \delta/2$, 
\[
\sup_{\theta \in \Theta_0}
| X_\theta |
\leq
C \frac{2 B_x^2}{(1 - \alpha)}
\sqrt{
d \log (2 A D/ D_0)
+
\log ( 2 / \delta)
}.
\]

Note that the uniform bound for independent subgaussian random variables still applies here as for each $\theta$, we are re-sampling a new chain from a new distribution sampled from $\pi$.

Assume that $\Theta_0 = \{ \theta_1, \cdots, \theta_n\}$.
Now for each $j\in[m]$, we consider $\Theta_j$ is the ball centered at $\theta_j$ of radius $D_0$ in $(\Theta, \rho)$.
With \cref{thm:generalizd-dudley}, for each process $\{ X_{\theta} \}_{\theta \in\Theta_j}$, then
\[
\psi = \psi_2,
\quad
\norm{
X_{\theta}
-
X_{\theta^\prime}
}_{\psi}
\leq
\frac{B^1}{\sqrt{n}}
\rho(\theta, \theta^\prime),
\]
where $\ell(\theta, \bz) - \ell(\theta^\prime, \bz)$ is a $B^1 \rho(\theta, \theta^\prime)$-subgaussian random variable.

We then get
\[
P
\left(
    \sup_{\theta, \theta^\prime \in \Theta_j}
    \vert 
    X_{\theta}
    -
    X_{\theta^\prime}
    \vert 
    \leq
    C^\prime 
    B^1
    D_0
    \left(
    \sqrt{
    \frac{d\log(2A)}{n}
    }
    +t
    \right)
\right)
\leq
2 \exp( -nt^2),
\quad\text{ for all }t \geq 0.
\]

If we further take $t \leq \sqrt{\log(2m/\delta)/n}$, then with probability at least $1 - \delta/2$, it holds that for all $j\in[m]$,
\[
    \sup_{\theta, \theta^\prime \in \Theta_j}
    \vert 
    X_{\theta}
    -
    X_{\theta^\prime}
    \vert 
    \leq
    C^\prime
    B^1
    D_0
    \sqrt{
    \frac{
    2d \log (2AD/D_0) + \log(4/\delta)
    }{n}
    }.
\]

By chaining, we have
\[
|X_{\theta}|
\leq 
|X_{\theta_j}|
+
|X_{\theta}
-
X_{\theta_j}|.
\]

Hence with probability at least $1 - \delta$, it holds that

\[
\sup_{\theta\in\Theta}
|X_{\theta}|
\leq
\sup_{\theta\in\Theta_0} | X_{\theta} |
+
\sup_j
\sup_{\theta \in \Theta_j}
| X_{\theta} - X_{\theta_j} |
\leq 
C^{\prime\prime}
(
\frac{2 B_x^2}{(1 - \alpha)}
+
B^1 D_0
)
\sqrt{
\frac{
d \log (2AD/D_0) + \log(2/\delta)
}{n}
}.
\]

Next by taking $D_0 = D/\kappa, \kappa= 1 + B^1D \frac{(1 - \alpha)}{2 B_x^2}$, we get

\[
\sup_{\theta\in\Theta}
|X_{\theta}|
\leq
C^{\prime\prime}
(
\frac{2 B_x^2}{(1 - \alpha)}
+
B^1 D \kappa
)
\sqrt{
\frac{
d \log (2A \kappa) + \log(2/\delta)
}{n}
}.
\]

Last, we check whether the assumptions we make above hold for our function class $\ell_\Theta$. 
Below, we slightly abuse our notation by using $D$ as the dimension for weight matrices in $\text{TF}_{\bm{\theta}}$.
By \cref{lem:covering-number-unit-ball}, it holds that 
\[
\log N (\delta, B_{\norm{\cdot}_{\text{op}}}(r), \norm{\cdot}_{\text{op}} )
\leq
L(3MD^2 + D D^\prime + 2) \log ( 1 + 2r/\delta),
\]
where $B_{\norm{\cdot}_{\text{op}}}(r)$ is a ball of radius $r$ under norm $\norm{\cdot}_{\text{op}}$.

We check that
\[
\norm{
\ell(\theta, \bz)
-
\ell(\theta^\prime, \bz)
}
\leq
B_x(L B_H^{L-1} B_{\Theta}) \norm{\theta - \theta^\prime}_{\text{op}},
\]
where it is a direct result from \cref{proposition:lipschitz-moirai}.
By plugging all the parameters, we get

\[
\sup_{\theta\in\Theta}
|X_{\theta}|
\leq
C
(
\frac{B_x^2}{(1 - \alpha)}
)
\sqrt{
\frac{
L(3MD^2 + D D^\prime + 2) \iota + \log(2/\delta)
}{n}
},
\]

where $\iota = \log( 2 + 2(L B_H^{L-1} B_{\Theta}) B \frac{1-\alpha}{B_x})$


Finally, by plugging the ERM $\hat{\bm{\theta}}$, we get
\[
L(\hat{\bm{\theta}})
\leq
\inf_{\theta} L(\theta)
+
2 \sup_{\theta} | X_{\theta} |.
\]


\end{proof}

% Our goal is to bound the following,
% \begin{align*}
%     X_{\bm{\theta}}
%     \coloneqq
%     \frac{1}{T}
%     \sum_{t=1}^T
%     \ell(\bm{\theta}, \bz_{t})
%     -
%     \mathtt{E}_{\bz\sim \mathtt{P}_{\bz}}
%     \left[
%     \hat{\ell( \bm{\theta}, \bz )}
%     \right],
% \end{align*}
% where $(\bz_1, \cdots, \bz_T)$ satisfies the Dobrushin's condition such that $\alpha( \mathtt{P}_{\bz} ) < 1$.

% The following Lemma is from \citep[Lemma~5.2]{van2014probability}.
% \begin{lemma}[Maximal Tail Inequality]\label{lemma:max-tail}
%     Suppose that $\log \mathbb{E}[ e^{\lambda X_t} ] \leq \psi(\lambda)$ for all $\lambda \geq 0$ and $t \in [T]$, where $\psi$ is convex and $\psi(0) = \psi^\prime(0) = 0$.
%     Then
%     \[
%         P
%         \left(
%             \sup_{t \in T} X_t \geq \psi^{* -1}( \log |T| + u)
%         \right)
%         \leq 
%         e^{-u}
%         \quad
%         \text{for all }u \geq 0.
%     \]
%     Further, if $X_t$ is $\sigma^2$-subgaussian for every $t \in T$, we have
%     \[
%         P
%         \left(
%             \sup_{t\in T} X_t
%             \geq
%             \sqrt{
%             2 \sigma^2 \log |T|
%             }
%             +x
%         \right)
%         \leq
%         e^{-x^2/2\sigma^2},
%         \quad
%         \text{for all }
%         x \geq 0.
%     \]
% \end{lemma}


% The following theorem is from \cite{dagan2019learning, kulske2003concentration}.
% \begin{theorem}\label{thm:dobrushin-concentration}
%     Let $\mathtt{P}^{(T)}_{\bz}$ be a distribution satisfying the Dobrushin's condition with coefficient $\alpha(\mathtt{P}_{\bz})$.
%     Let $(\bz_1,\cdots,\bz_T) \sim \mathtt{P}^{(T)}$, and let $f: \bZ^{(m)} \rightarrow \R$ be a real valued function with the following bounded difference property, with parameters $\lambda_1, \cdots, \lambda_T \geq 0$:
%     \[
%     \vert f(\bz) - f(\bz^\prime) \vert 
%     \leq
%     \sum_{t=1}^T
%     \mathbbm{1}_{\bz_t\neq \bz_t^\prime}
%     \lambda_t.
%     \]
%     Then for all $t > 0$,
%     \[
%     P
%     \left(
%         \vert 
%         f(\bz)
%         -
%         \mathbb{E}
%         [ f(\bz) ]
%         \geq
%         t
%     \right)
%     \leq
%     2 \exp
%     \left(
%     -
%     \frac{(1 - \alpha)t^2}{2 \sum_t \lambda_t^2}.
%     \right)
%     \]
% \end{theorem}
% The following corollary directly follows from the above result
% \begin{corollary}
%     Following \cref{thm:dobrushin-concentration}, let
%     \[
%     \ell( \bz)
%     \coloneqq
%     \frac{1}{T}
%     \sum_{t=1}^T
%     \ell( \bz_t),
%     \]
%     where $0 \leq \ell(\bz_t) \leq B$ for all $t=1,\cdots,T$ and all $\bz \sim \mathtt{P}_{\bz}$.
%     Then the variance of $\ell(\cdot)$ is bounded by
%     \[
%     \vert 
%         \ell(\bz)
%         -
%         \ell(\bz^\prime)
%     \vert
%     \leq
%     B.
%     \]
%     Then the following holds
%     \[
%         P
%         \left(
%         \left\vert
%         \ell(\bz)
%         -
%         \mathtt{E}
%         [ \ell(\bz) ]
%         \right\vert
%         \geq
%         t
%         \right)
%         \leq
%         2
%         \exp
%         \left(
%         \frac{- (1 - \alpha) t^2 }{2 \sum_t B^2}
%         \right)
%         .
%     \]
% \end{corollary}



% \begin{theorem}[\text{\citep[Theorem~6.7]{dagan2019learning}}] \label{thm:gen-bound-dobrushin}
%     Let $\mathtt{P}^{(m)}$ be a distribution satisfying $\alpha_{\log}(\mathtt{P}^{(m)}) < 1/2$ and let $\cF \coloneqq \bZ \rightarrow \R$ be a function class.
%     For every $f \in \cF$, $\vert f(s) \vert \leq B$.
%     Then for any $t > 0$,
%     \[
%     P_{\bZ \sim \mathtt{P}^{(m)}}
%     \left(
%     \sup_{f \in \cF }
%     \left\vert
%     \sum_{i=1}^m
%     f(\bz_i)
%     -
%     \mathbb{E}_{\bZ}
%     \left[
%     \sum_{i=1}^m
%     f(\bz_i)
%     \right]
%     \right\vert
%     >
%     \frac{C \mathfrak{G}_{\mathtt{P}^{(m)}}(\cF)}{
%     \sqrt{1 - 2 \alpha_{\log}(\mathtt{P}^{(m)})}
%     }
%     +
%     C B \sqrt{m t}
%     \right)
%     \leq 
%     e^{-t^2/2},
%     \]
%     for some universal constant $C > 0$.
% \end{theorem}



% \begin{proof}
%     Considering the quantity $\sup_{\bm{\theta}\in\Theta}| X_{\bm{\theta}}|$.

%     Assuming the index set $\Theta$ is equipped with a distance metric $\rho$ and diameter $D$.
%     We assume that for any ball $\Theta^\prime$ of radius $r$ in $\Theta$, there exists some constant $C_1$ such that the covering number admits upper bound
%     \[
%     \log
%     N( \delta, \Theta^\prime, \rho)
%     \leq
%     d \log ( 2 A r / \delta),
%     \]
%     for all $0 < \delta \leq 2r$.

%     Now we select $\Theta_0$ such that it is a $(D_0/2)$-covering of $\Theta$.
%     The above assumption guarantees us that we can have a $\Theta_0$ such that $\log | \Theta_0 | \leq d \log (2AD/D_0)$.
%     Since $X_{\bm{\theta}}$ is a $\nicefrac{B_x^2}{2}$-subgaussian, by \cref{lemma:max-tail}, we have
%     \[
%         P
%         \left(
%             \sup_{ \bm{\theta} \in \Theta_0} X_{\bm{\theta}}
%             \geq
%             {B_x}
%             \sqrt{
%              \log | \Theta_0 |
%             }
%             +u
%         \right)
%         \leq
%         e^{-u^2/B_x^2},
%         \quad
%         \text{for all }
%         u \geq 0.
%     \]
%     The above result is also equivalent to, with probability at least $1 - \delta$, the following holds
%     \begin{align*}
%         \sup_{ \bm{\theta} \in \Theta_0} X_{\bm{\theta}}
%         &\leq
%         B_x \sqrt{ \log | \Theta_0| }
%         +
%         \sqrt{ B_x^2 \log(\frac{1}{\delta})}
%         \\
%         &\leq
%         B_x\left(
%         \sqrt{ d \log(2AD/D_0)}
%         +
%         \log(\frac{1}{\delta})
%         \right).
%     \end{align*}



% \end{proof}


% \subsection{Proof of XXX}

% We first restate some lemma and theorem.
% % There to bound the following quantity
% % \[
% % \sup_{\bm{\theta} \in \Theta_{L, M, D^\prime, B}}
% % \big\vert
% % \hat{L}(\bm{\theta})
% % -
% % L(\bm{\theta})
% % \big\vert,
% % \]
% % we proceed XXX.


% % From \cite{bai2024transformers}
% % \begin{theorem}
% %     Suppose that $\psi : [0, +\infty) \mapsto [0, +\infty)$ is a convex, non-decreasing function that satisfies 
% %     $\psi(x+y) \geq \psi(x) \psi(y)$.
% %     For any random variable $\cX$, we consider the Orlicz norm induced by $\psi: \norm{\cX}_{\psi}\coloneqq \inf \{ K > 0: \mathbb{E}_{\psi}[ |\cX|/K ]\} \leq 1$.
% %     Suppose that $\{ \cX_{\theta}\}$ is a zero-mean random process indexed by $\theta \in \Theta$ such that 
% %     $\norm{\cX_\theta - \cX_{\theta^\prime}} \leq \rho(\theta, \theta^\prime)$ for some metric $\rho$ on space $\Theta$.
% %     Then it holds that 
% %     \begin{equation*}        
% %     P
% %     \left(
% %     \sup_{\theta, \theta^\prime \in \Theta}
% %     \vert 
% %     \cX_\theta - \cX_{\theta^\prime}
% %     \vert
% %     \leq 
% %     8 (J + t)
% %     \right)
% %     \leq
% %     \frac{1}{\psi(t/D)},
% %     \quad
% %     \forall t \geq 0,
% %     \end{equation*}

% %     where $D$ is the diameter of the metric space $(\Theta, \rho)$, and the generalized Dudley entropy integral $J$ is given by
% %     \[
% %     J \coloneqq
% %     \int_0^D
% %     \psi^{-1}
% %     (N(\delta ; \Theta, 
% %     rho) )
% %     d \delta,
% %     \]
% %     where $N(\delta ; \Theta, \rho)$ is the $\delta$-covering number of $(\Theta, \rho)$.
% % \end{theorem}

% % By , we have 
% % \begin{lemma}[\citep[Example~5.8]{wainwright2019high}]\label{lem:covering-number-unit-ball}
% %     Given any well-defined norm $\norm{\cdot}^\prime$.
% %     Let $\mathbb{B}$ be the $\R^d$ unit-ball in $\norm{\cdot}^\prime$, i.e. 
% %     $\mathbb{B} = \{ \theta \in \R^d \mid \norm{\theta}^\prime \leq 1 \}$,
% %     we have
% %     \[
% %     \log N( \delta, \mathbb{B}, \norm{\cdot}^\prime )
% %     \leq
% %     d 
% %     \log
% %     \left( 1 + \frac{2}{\delta} \right).
% %     \]
% % \end{lemma}


% Now we are interested in bounding the following complexity
% \[
% \mathfrak{G}_{\mathtt{P}_{\bZ}}( \ell(\Theta) )=
% \mathbb{E}_{\bZ \sim \mathtt{P}_{\bZ}}
% \left[
% \mathbb{E}_{\tau}
% \left[
% \sup_{ \bm{\theta} \in \Theta_{L,M,D^\prime, B} }
% \frac{1}{T}
% \sum_{i=1}^T
% \tau_i
% \ell ( \bZ_i, \bm{\theta} )
% \right]
% \right],
% \quad
% \tau_i \sim N(0, 1), \text{ for } i = 1, \cdots , T.
% \]

% Note that by Dudley's metric entropy bound, the above Gaussian complexity is upper bounded by

% \[
% \mathfrak{G}_{\mathtt{P}_{\bZ}}( \ell(\Theta) )
% \leq
% C
% \int_0^{\infty}
% \sqrt{ \log N( \delta, \ell(\Theta), L_2) }
% d \delta.
% \]

% Note that the function class $\ell(\Theta)$ is parameterized by $\bm{\theta}$, and the whole function class is $L_1$-Lipschitz in $\bm{\theta}$.
% This implies to cover $\ell(\Theta)$ at scale $\delta$, it is sufficient to cover $\Theta$ at scale $\delta/L_1$.

% By Lemma~\ref{lem:covering-number-unit-ball}, we have
% \[
% \log N( \delta, \ell(\Theta), L_2)
% \leq
% d
% \log
% \left(
% 1 + 
% \frac{2 L_1}{\delta}
% \right).
% \]

% Now consider Dudley's metric entropy bound, we get
% \begin{align*}
%     \mathfrak{G}
%     &\leq
%     C
%     \int_0^{2B_x^2}
%     \sqrt{
%     \frac{
%     \log N( \delta, \ell(\Theta), L_2)
%     }{n}
%     }
%     d \delta
%     \\
%     &\leq 
%     C
%     \int_0^{2B_x^2}
%     \sqrt{
%     \frac{
%     d
%     \log
%     \left(
%     1 + 
%     \frac{2 L_1}{\delta}
%     \right)   
%     }{n}
%     }
%     d \delta
%     \\
%     &=
%     C \sqrt{\frac{d}{n}}
%     \int_0^{2B_x^2}
%     \sqrt{
%     \log( 1 + \frac{2L_1}{\delta} )
%     }
%     d \delta
%     \\
%     &=
%     O
%     \left(
%     \sqrt{
%     \frac
%     {L( MD^2 + D D^\prime)}
%     {n}
%     }
%     \left(
%     B_x^2 + 
%     L ( B_H^{L-1} B_{\Theta} ) 
%     B_x
%     \right)
%     \right).
% \end{align*}


% Now with Theorem~\ref{thm:gen-bound-dobrushin},
% for any $t > 0$,
% with probability $1 - e^{t^2/2}$,
% the ERM $\hat{\bm{\theta}}$ satisfies
% \begin{align*}
%     L ( \hat{\bm{\theta}}
%     -
%     \hat{L}( \hat{\bm{\theta}})
%     &>
%     \frac{C \mathfrak{G}_{\mathtt{P}^{(m)}}(\cF)}{
%     \sqrt{1 - 2 \alpha_{\log}(\mathtt{P}^{(m)})}
%     }
%     +
%     C B \sqrt{m t}
%     \\
%     &\geq 
%     O
%     \left(
%     \sqrt{
%     \frac
%     {L( MD^2 + D D^\prime)}
%     {n ( 1 - 2 \alpha_{\log})}
%     }
%     \left(
%     B_x^2 + 
%     L ( B_H^{L-1} B_{\Theta} ) 
%     B_x
%     \right)
%     \right)
%     +
%     C B_x^2 \sqrt{n t}
% \end{align*}



% The above theorem serves as a general generalization bound for learning from weakly dependent data satisfying the Dobrushin's condition.

% \begin{proof}
%     By Theorem~\ref{thm:gen-bound-dobrushin}, the ERM $\hat{\bm{\theta}}$ satisfies
%     \[
%     P_{\bZ \sim \mathtt{P}^{(m)}}
%     \left(
%     \hat{
%     L}( \hat{\bm{\theta}} )    
%     -
%     L(
%     \hat{\bm{\theta}}
%     )
%     >
%     \frac{C \mathfrak{G}_{\mathtt{P}^{(m)}}( L(\Theta) )}{
%     \sqrt{1 - 2 \alpha_{\log}(\mathtt{P}^{(m)})}
%     }
%     +
%     C B \sqrt{m t}
%     \right)
%     \leq 
%     e^{-t^2/2},
%     \]
%     where $L(\Theta)$ is a function class of $L \circ \cF$.

%     By general bound for Gaussian Complexity, we have
%     \[
%     O ( \sqrt{L (M D^2 + D D^\prime)} L B B^{L-1}_H B_{\Theta})  ),
%     \]

%     By plugging the bound for Gaussian complexity, we have
%     \[
%     P_{\bZ \sim \mathtt{P}^{(m)}}
%     \left(
%     L(
%     \hat{\bm{\theta}}
%     )
%     >
%     O 
%     \left(
%     \frac{\sqrt{L (M D^2 + D D^\prime)} L B_x B^{L-1}_H B_{\Theta}}{n ( 1 - \alpha)}
%     +
%     B_x \cdot \exp\left( \frac{-L}{\kappa} \right)
%     \right)
%     +
%     C B \sqrt{m t}
%     \right)
%     \leq 
%     e^{-t^2/2},
%     \]
    
% \end{proof}


% \begin{proof}[Proof of Theorem~\ref{thm:gen-bound-1}]
%     Considering the following random process
%     \[
%     \cX_{\theta}
%     \coloneqq
%     \frac{1}{N} \sum_{i=1}^N f(z_i, \theta)
%     -
%     \mathbb{E}_z[ f(z, \theta) ],
%     \]
%     where $z_1, \cdots, z_N$ are sampled from a distribution $\mathtt{P}_z$ satisfies the Dobrushin's condition with $\alpha( \mathtt{P}) < 1$.

%     By \citep[Theorem~2.3]{dagan2019learning} (also \cite{kulske2003concentration}), $\{ \cX_\theta \}$ is a $O(1/(1-\alpha(\mathtt{P}_z  )$ subGaussian process for every $\theta \in \Theta$.

%     By \cite{wainwright2019high}, we are able to show that
%     \[
%     L(\hat{\bm{\theta}}) \leq
%     \inf_{\bm{\theta} \in \Theta_{ L,B,M, }}
%     \mathbb{E}
%     \left[
%     L(\bm{\theta})
%     \right]
%     +
%     2 \sup_{\bm{\theta} \in \Theta_{L,B,M}}
%     \vert \cX_{\bm{\theta}} \vert .
%     \]




%     To further bound the above inequality, the tail bound for empirical process to verify a few conditions on the function $L$ and the set $\Theta$ as follows
%     \begin{itemize}
%         \item[1]
%         The metric entropy of an operator norm ball
%         \[
%         \log N ( \delta, B_{\norm{\cdot}_{\text{op}}} (\delta),
%         \norm{\cdot}_{\text{op}}
%         \leq
%         CL (M D^2 + DD^\prime) \log(1 + 2r  \delta)
%         \]
%         \item[2]
%         $L(\bm{\theta})$ is $2 B_x^2$-subGaussian
%         \item[3]
%         The Moirai transformer is Lipschitz in $\bm{\theta}$.
%     \end{itemize}

%     Note that [1] follows immediately from J.2 in \cite{bai2024transformers} (with $D > 2$), 
%     [2] follows immediately by the $\texttt{clip}$ operation and [3] is shown by Proposition~\ref{proposition:lipschitz-moirai}.

%     $(L B_H^{L-1} B_{\Theta})$

%     Finally, by \citep[Proposition~A.4]{bai2024transformers}, with probability at least $1 - \varepsilon$, we have
%     \[
%     \sup_{\bm{\theta} \in \Theta}
%     \big\vert 
%     \cX_{\bm{\theta}}
%     \big\vert 
%     \leq 
%     C B_x^2
%     \sqrt{ L(M D^2 + D D^\prime) \zeta + \log(1/\varepsilon) },
%     \]
%     where $\zeta = \log ( 2 + B(L B_H^{L-1} B_{\Theta} ) / 2B_x ) = O( 2 + \max\{ B, \mathtt{R}, B_x, T, d \})$, with $T$ is the number of time steps of each covariates, and $d$ is the number of covariates.

%     Finally, we get the final result by plugging XXX.
%     \[
%     L(\hat{\bm{\theta}})
%     \leq 
%     \inf_{\bm{\theta} \in \Theta}
%     \mathbb{E}[ L(\bm{\theta})]
%     +
%     C B_x^2 \sqrt{ \frac{L(MD^2 + D D^\prime) \zeta + \log (1 / \varepsilon)}{n} }.
%     \]


    
% \end{proof}


% % \subsubsection{The i.i.d. Case}
% % Let $\cF$ be a function class such that for all $f \in \cF$, $f$ is $L$-Lipschitz on some domain $\cX$, w.r.t. L2 norm, i.e.
% % \[
% % \norm{ f(x) - f(y)}
% % \leq 
% % L
% % \norm{x-y},
% % \quad
% % \text{for all }x, y \in \cX.
% % \]

% % Given a sample of points $X = x_1, \cdots, x_n \in \cX$, the empirical Gaussian complexity of $\cF$ on these points $\hat{\mathfrak{G}}_X( \cF )$ is defined as
% % \[
% % \hat{\mathfrak{G}}_X( \cF )
% % \coloneqq
% % \mathbb{E}_{\gamma}
% % \left[
% % \frac{1}{n}
% % \sum_{i=1}^n
% % \gamma_i f(x_i)
% % \right], \quad \text{where } \gamma_1, \cdots \gamma_n \sim N(0, 1) \text{ i.i.d.}
% % \]


% % Based on the Dudley's inequality \cite{dudley1967sizes} (the chaining bound), the above empirical Gaussian complexity is bounded by
% % \[
% % \hat{\mathfrak{G}}_X( \cF )
% % \leq 
% % \frac{1}{\sqrt{n}}
% % \int_0^D
% % \sqrt{\log N(\epsilon, \cF, d) }
% % d \epsilon,
% % \]
% % where $D \geq \sup_{f,g \in \cF} d(f, g)$ for metric $d$. $d$ is defined as
% % \[
% % d(f, g) \coloneqq 
% % \sqrt{
% % \frac{1}{n}
% % \sum_{i=1}^n
% % \left(
% % f(x_i) - g(x_i)
% % \right)^2
% % }.
% % \]
% % Note that $\cF$ is $L$-Lipschitz by assumption.
% % Assuming $\norm{x_i} < B_x$ for $i = 1, \cdots, n$, $D$ has a natural upper bound
% % \[
% % D \leq 2 L B_x.
% % \]

% % {\color{red} Below needs to be revised}
% % The metric entropy of a Lipschitz function class is then bounded by
% % \[
% % \log
% % N(\epsilon, \cF, d)
% % \leq
% % d_x \log\left( \frac{LB_x}{\epsilon} \right),
% % \]
% % where $d_x$ is the dimension of $\cX$.
% % Finally, the empirical Gaussian complexity of a $L$-Lipschitz function with $d_x$ dimensional input is bounded by
% % \begin{equation}\label{eqn:gaus-complexity-bound}
% %     \hat{\mathfrak{G}}_X (\cF)
% %     \leq
% %     \frac{1}{n}
% %     \int_0^{L B_x}
% %     \sqrt{d_x \log
% %     \left(
% %     \frac{L B_x}{\epsilon}
% %     \right)
% %     d \epsilon
% %     }
% %     =
% %     \cO\left( \frac{L B_x}{\sqrt{n}} \sqrt{ d_x \log (n L B_x) }  \right).
% % \end{equation}

% % \subsubsection{The Weakly Dependent Case}

% % We will use Dudley's inequality to prove XXX. 
% % First, we prove some lemmas. 
% % The first shows that by partitioning the data sequence into block, then with high probability, these blocks behaves like i.i.d. samples.

% % \begin{lemma}[Block-wise Dependency of Weakly Dependent Data]\label{lem:block-dependence}
% %     Let $\cX = (\cX_1, \cdots, \cX_n)$ be random variables over $\cD_{\cX}^n$, where $\alpha(\cX) < 1$.
% %     Let $B \subset \{ 1, \cdots, n \}$, and $ \vert B \vert = b$.
% %     We the subscript $\cX_B$ to denote the subset of $\cX$ selected with indices $B$,  with $B^c \cap B = [n]$.
% %     We have
% %     \[
% %         d_{\text{TV}}
% %         \left(
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}_{B^c} )
% %         ,
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}^\prime_{B^c} )
% %         \right)
% %         =
% %         O
% %         \left(\alpha( \cX )^b\right).
% %     \]
% % \end{lemma}

% % \begin{proof}[Proof of Lemma~\ref{lem:block-dependence}]
% %     Considering the sequence
% %     $\cX = (\cX_1, \cdots, \cX_n)$ over $\cD_{\cX}^n$, we say $\cX$ satisfies the Dobrushin's condition if
% %     \[
% %     \max_{1\leq i \leq T} \sum_{j \neq i} \bI_{j \rightarrow i} (\cX) < 1,
% %     \]
% %     where $\bI$ denotes the influence between \emph{elements} in different coordinates.
% %     To be concrete, we have
% %     \[
% %         \bI_{j \rightarrow i}(\cX)
% %         \coloneqq
% %         \max_{ \text{x}_{-i-j}, \text{x}_j, \text{x}_j^\prime  }
% %         % \underset{  z_{-i-j} \in Z^{m-2} }{ z_j, z_j^\prime \in Z}}
% %         d_{\text{TV}}
% %         \left(
% %         \mathbb{P}_{\cX_i | \cX_{-i}} \left( \cdot | \text{x}_{-i-j}, \text{x}_j \right),
% %         \mathbb{P}_{\cX_i | \cX_{-i}} \left( \cdot | \text{x}_{-i-j}, \text{x}_j^\prime \right)
% %         \right).
% %     \]

% %     % We now proceed the \emph{blocking} technique by grouping subsets of indices into blocks.
% %     Let $B \subset \{ 1, \cdots, n \}, \vert B \vert = b$.
% %     We the subscript $\cX_B$ to denote the subset of $\cX$ selected with indices $B$.
% %     The \emph{between-block} influence is then defined by
% %     \begin{equation}\label{eqn:block-influence}
% %         d_{\text{TV}}
% %         \left(
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}_{B^c} )
% %         ,
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}^\prime_{B^c} )
% %         \right),
% %     \end{equation}
% %     where $B^c$ is the complement of $B$. 

% %     Observe
% %     \begin{align*}
% %         &\mathbb{P}
% %         \left(
% %         \cX_{i_1}, \cdots, \cX_{i_b}
% %         \mid 
% %         \cX_{B^c}
% %         =
% %         \text{x}_{B^c}
% %         \right)
% %         \\
% %         &=
% %         \mathbb{P}
% %         \left(
% %         \cX_{i_1}
% %         \mid 
% %         \cX_{B^c}=
% %         \text{x}_{B^c}
% %         \right)
% %         \times
% %         \mathbb{P}
% %         \left(
% %          \cX_{i_2}
% %         \mid 
% %         \cX_{i_1},
% %         \cX_{B^c}=
% %         \text{x}_{B^c}
% %         \right)
% %         \cdots 
% %         \times
% %         \mathbb{P}
% %         \left(
% %          \cX_{i_b}
% %         \mid 
% %         \cX_{i_1}, \cdots, \cX_{i_{b-1}},
% %         \cX_{B^c}=
% %         \text{x}_{B^c}
% %         \right)
% %     \end{align*}

% %     Define 
% %     \[
% %     q_k
% %     \coloneqq
% %     \mathbb{P}
% %     \left( \cX_{i_k} \mid \left( \cX_{i_1}, \cdots, \cX_{k-1} \right) = u, \cX_{B^c}=
% %         \text{x}_{B^c} \right),
% %     \]
% %     for $k=1, \cdots, b$.
% %     Thus,
% %     \[
% %     \mathbb{P}
% %         \left(
% %         \cX_{i_1}, \cdots, \cX_{i_b}
% %         \mid 
% %         \cX_{B^c}
% %         =
% %         \text{x}_{B^c}
% %         \right)
% %         =
% %         \prod_{k=1}^b
% %         q_k,
% %         \quad
% %     \mathbb{P}
% %         \left(
% %         \cX_{i_1}, \cdots, \cX_{i_b}
% %         \mid 
% %         \cX_{B^c}
% %         =
% %         \text{x}^\prime_{B^c}
% %         \right)
% %         =
% %         \prod_{k=1}^b
% %         q_k^\prime.
% %     \]
% %     Let 
% %     \[
% %     P_k 
% %     \coloneqq
% %     \left(
% %     \prod_{j=1}^k
% %     q_j^\prime
% %     \right)
% %     \times
% %     \left(
% %     \prod_{j=k+1}^b
% %     q_j
% %     \right),
% %     \]

% %     we then have
% %     \begin{align*}
% %         \Delta
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}_{B^c} )
% %         ,
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}^\prime_{B^c} )
% %         \right)
% %         \\
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         \prod_{k=1}^b
% %         q_k,
% %         \prod_{k=1}^b
% %         q_k^\prime,
% %         \right)
% %         \\
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         P_0
% %         ,
% %         P_b
% %         \right)
% %         \\
% %         &\leq
% %         \sum_{k=1}^b
% %         d_{\text{TV}}
% %         \left(
% %         P_{k-1}
% %         ,
% %         P_k
% %         \right),
% %     \end{align*}
% %     where the last inequality comes from the triangle inequality with summation over consecutive differences.

% %     Observe 
% %     \begin{align*}
% %         d_{\text{TV}}
% %         \left(
% %         P_{k-1}
% %         ,
% %         P_k
% %         \right)
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         \prod_{j=1}^{k-1}
% %         q^\prime_j
% %         \times 
% %         (q_k)
% %         \times 
% %         \prod_{j=k+1}^{b}
% %         q_j
% %         ,
% %         \prod_{j=1}^{k-1}
% %         q^\prime_j
% %         \times 
% %         (q_k^\prime)
% %         \times 
% %         \prod_{j=k+1}^{b}
% %         q_j
% %         \right)
% %         \\
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         \left(
% %         \prod_{j=1}^{k-1}
% %         q^\prime_j
% %         \prod_{j=k+1}^{b}
% %         q_j
% %         \right)
% %         \cdot
% %         \colorbox{cyan!20}{$\left(
% %         q_k , q^\prime_k
% %         \right)$}
% %         \right)
% %         .
% %     \end{align*}

% %     Note that the factor $\left(
% %         \prod_{j=1}^{k-1}
% %         q^\prime_j
% %         \prod_{j=k+1}^{b}
% %         q_j
% %         \right)$ is a probability measure for other coordinates.
% %         Therefore, the above equation implies that the total variation between $P_{k-1}, P_k$ is controlled by the total variation between $q_k, q_k^\prime$.
        
% %         Thus, we have
% %     \[
% %         d_{\text{TV}}
% %         \left(
% %         P_{k-1}
% %         ,
% %         P_k
% %         \right)
% %         =
% %         d_{\text{TV}}
% %         \left(
% %         \left(
% %         \prod_{j=1}^{k-1}
% %         q^\prime_j
% %         \prod_{j=k+1}^{b}
% %         q_j
% %         \right)
% %         \cdot
% %         \colorbox{cyan!20}{$\left(
% %         q_k , q^\prime_k
% %         \right)$}
% %         \right)
% %         \leq\
% %         \sup
% %         d_{\text{TV}}
% %         \left(
% %         \colorbox{cyan!20}{$
% %         q_k , q^\prime_k
% %         $}
% %         \right),
% %     \]
% %     where the supremum is taken over all possible $({\text{x}_{i_1}, \cdots \text{x}_{i_{k-1}}, \text{x}_{i_{k+1}}, \cdots, \text{x}_{i_b}})$.

% %     Note that by Dobrushin's condition, 
% %     \[
% %     d_{\text{TV}}
% %     (q_k, q_k^\prime)
% %     \leq
% %     a^b.
% %     \]

% %     Therefore, we arrive 
% %     \begin{align*}
% %         \Delta
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}_{B^c} )
% %         ,
% %         \mathbb{P}_{ \cX_B \mid \cX_{B^c} }
% %         ( \cdot
% %         \mid \text{x}^\prime_{B^c} )
% %         \right)
% %         \\
% %         &=
% %         d_{\text{TV}}
% %         \left(
% %         P_0, P_b
% %         \right)
% %         \\
% %         &\leq 
% %         \sum_{k=1}^b
% %         d_{\text{TV}}(P_{k-1}, P_k)
% %         \\
% %         &\leq
% %         \sum_{k=1}^b
% %         d_{\text{TV}}
% %         (q_k, q^\prime_k)
% %         \\
% %         &\leq
% %         b \cdot a^b = \cO(a^b).
% %     \end{align*}
% % \end{proof}

% % The next lemma bounds the empirical Gaussian complexity of a function class $\cF$ over variables satisfying the Dobrushin's condition.

% % \begin{lemma}[Metric Entropy Bound on Weakly Dependent Data]\label{lem:weak-data-complexity}
% %     Following the above notations,
% %     let $(\text{x}_1, \cdots, \text{x}_n) \sim \cD_{\cX}$ be a sequence sampled from a distribution satisfies the Dobrushin's condition, i.e., $\alpha(\cX) < 1$.
% %     Given a function class $\cF$, with some $b > 1 \in \N^+$ such that $\alpha(\cX)^b \ll 1$, we have
% %     \[
% %     \hat{\mathfrak{G}}_X(\cF)
% %     \leq 
% %     \sqrt{\frac{b}{n}}
% %     \int_0^R \sqrt{\log N(\epsilon, \cF, L_2) } d \epsilon
% %     +
% %     \cO\left( \alpha(\cX)^b \right),
% %     \]
% %     where $R \geq \sup_{f,g \in \cF} L_2(f, g)$, $L_2$ is the L2 distance and $N(\cdot)$ is the covering number of $\cF$.
% % \end{lemma}

% % \begin{corollary}\label{cor:complexity-lipschitz}
% %     Following Lemma~\ref{lem:weak-data-complexity}, with $x_i \in \R^d$, for $i = 1, \cdots, n$, and $\cF$ being a $L$-Lipschitz function, we have
% %     \[
% %     \hat{\mathfrak{G}}_X(\cF)
% %     \leq
% %     \cO\left( \alpha(D_x)^b \right)
% %     +
% %     C_0
% %     \left(
% %     \frac{L R}{\epsilon}
% %     \right)^d,
% %     \]
% %     where $C_0$ is a universal constant.
% % \end{corollary}

% % \begin{proof}[Proof of Lemma~\ref{lem:weak-data-complexity}]

% %     We first partition the sequence $(\text{x}_1, \cdots, \text{x}_n)$ into blocks with size $b$.
% %     The empirical Gaussian complexity can be written as
% %     \begin{align*}
% %     \hat{\mathfrak{G}}_X( \cF )
% %     &\coloneqq
% %     \mathbb{E}_{\gamma}
% %     \left[
% %     \frac{1}{n}
% %     \sum_{i=1}^n
% %     \gamma_i f(x_i)
% %     \right]
% %     \\
% %     &=
% %     \mathbb{E}_{\gamma}
% %     \left[
% %     \frac{b}{n}
% %     \sum_{i=1}^{ \lfloor n/b \rfloor}
% %     \sum_{j=1}^{b}
% %     \gamma_i f( x_{ b(j-1) + j} )
% %     \right]
% %     \end{align*}    

% %     We slightly abuse our notations and define
% %     \[
% %         \sum_{j=1}^{b}
% %         \gamma_i f( x_{ b(j-1) + j} )
% %         \coloneqq
% %         \gamma_i f( B_i ).
% %     \]
% %     Then the empirical Gaussian complexity over sequence $(\text{x}_1, \cdots, \text{x}_n)$ can be written as the empirical Gaussian complexity as \emph{super-samples} $( \text{x}_{B_1}, \cdots, \text{x}_{B_{ \lfloor n/b \rfloor }  })$.
% %     We then have

% %     \begin{align*}
% %         \hat{\mathfrak{G}}_X( \cF )
% %         =
% %         \mathbb{E}_{\gamma}
% %         \left[
% %         \frac{1}{n}
% %         \sum_{i=1}^n
% %         \gamma_i f(\text{x}_{B_i}).
% %         \right]
% %     \end{align*}    

% %     We define the event $A$ as the case where each super-samples behaves like i.i.d. samples and $\bar{A}$ denotes the case whenever $A$ fails, i.e. $\mathbb{P}(A) + \mathbb{P}(\bar{A}) = 1$.

% %     \begin{align*}
% %         \mathbb{E}_{\gamma}
% %         \left[
% %         \frac{1}{n}
% %         \sum_{i=1}^n
% %         \gamma_i f(\text{x}_{B_i}).
% %         \right]
% %         =
% %         \mathbb{P}(A) \cdot
% %         \mathbb{E}_A[ \hat{\mathfrak{G}}_X( \cF ) ]
% %         +
% %         \mathbb{P}(\bar{A})
% %         \mathbb{E}_{\bar{A}}[ \hat{\mathfrak{G}}_X( \cF ) ].
% %     \end{align*}    

% %     By Lemma~\ref{lem:block-dependence}, $\mathbb{P}(A) \leq 1 - b \cdot \alpha(\cX)^b$.
% %     Thus, we have
% %     \begin{align*}
% %         \mathbb{E}_{\gamma}
% %         \left[
% %         \frac{1}{n}
% %         \sum_{i=1}^n
% %         \gamma_i f(\text{x}_{B_i}).
% %         \right]
% %         &=
% %         \mathbb{P}(A) \cdot
% %         \mathbb{E}_A[ \hat{\mathfrak{G}}_X( \cF ) ]
% %         +
% %         \mathbb{P}(\bar{A})
% %         \mathbb{E}_{\bar{A}}[ \hat{\mathfrak{G}}_X( \cF ) ]
% %         \\
% %         &\leq
% %         (1 - b \cdot \alpha(\cX)^b)
% %         \sqrt{\frac{b}{n}}
% %         \int_0^R
% %         \sqrt{ \log N\left( \epsilon, \cF, L_2 \right) } d \epsilon
% %         +
% %         (b \cdot \alpha(\cX)^b)
% %         \mathbb{E}_{\bar{A}}[ \hat{\mathfrak{G}}_X( \cF ) ].
% %     \end{align*}    

% %     Note that we then simply bound the term  
% %     \[
% %     \mathbb{E}_{\bar{A}}[ \hat{\mathfrak{G}}_X( \cF ) ]
% %     \leq
% %     2R,
% %     \]
% %     which is dominated by $(b \cdot \alpha(\cX)^b) = O(\alpha(\cX)^b)$.

% %     Thus, we arrive the final result
% %     \begin{equation*}
% %         \hat{\mathfrak{G}}_X( \cF )
% %         \leq
% %         \sqrt{\frac{b}{n}}
% %         \int_0^R
% %         \sqrt{ \log N\left( \epsilon, \cF, L_2 \right) } d \epsilon
% %         +
% %         O(\alpha(\cX)^b).
% %     \end{equation*}

% %     Additionally, Corollary~\ref{cor:complexity-lipschitz} is a direct corollary of Lemma~\ref{lem:weak-data-complexity}.

% % \end{proof}






% \subsection{Proof of the Multi-path Generalization}

\clearpage

\subsection{Analysis of Section~\ref{sec:AR1}}\label{appendix:analysis-ar1}

\begin{definition}[Markov Random Field (MRF) with pairwise potentials]
The random vector $\mathcal{Z} = (\cZ_1, \cdots, \cZ_d)$ over $Z^d$ is an MRF with pariwise potentials if there exist functions $\psi_i : Z \rightarrow \R$ and $\varphi_{ij}: Z^2 \rightarrow \R$ for $i \neq j \in \{ 1, \cdots, d \}$ such that for all $z \in Z^d$,
\begin{equation*}
    \mathbb{P}_{z \sim \mathtt{P}^d }
    \left[ 
    \cZ = z
    \right]
    =
    \prod_{i=1}^d
    e^{\psi(\cZ_i)}
    \prod_{1 \leq i < j \leq d}
    e^{\varphi_{ij}(\cZ_i, \cZ_j)}
\end{equation*}
The functions $\psi_i$ are called as element-wise potentials and $\phi_{ij}$ are pairwise potentials.  
\end{definition}

\begin{definition}
Given an MRF $\cZ$ with potentials $\{ \varphi_i \}$ and $\{ \psi_{ij}\}$, we define
    \begin{equation*}
        \beta_{i,j}(\cZ)
        \coloneqq
        \sup_{\cZ_i, \cZ_j \in Z}
        \lvert
        \varphi_{ij}( \cZ_i, \cZ_j )
        \rvert;
        \quad
        \beta(\cZ)
        \coloneqq
        \max_{1 \leq i \leq d}
        \sum_{j \neq i}
        \beta_{ij}
        ( \mathtt{P}^d ).
    \end{equation*}
\end{definition}

\begin{lemma}\label{lem:beta-bound}
    Given an MRF $\bz$ with pairwise potentials, for any $i \neq j$, $I_{j \rightarrow i}(\bz) \leq \beta_{j,i}(\bz)$.
    $ \bI_{j\rightarrow i}(\cZ)
    \leq
    \bI^{\log}_{j,i}(\cZ) 
    \leq 
    \beta_{j,i}(\cZ)$
    
    % Hence, $\alpha(\bz) \leq \beta(\bz)$.
    % Further, $\bI_{j,i}^{\log}(\bz) \leq \beta_{j,i}(\bz)$.
\end{lemma}
\cref{lem:beta-bound} implies that to satisfy the condition $\alpha^{\log}(\cdot) < 1/2$, it is sufficient to show that $\beta(\cdot) < 1/2$, leading to the following condition.
\begin{equation}
    \langle \bw \bx_{t}, \bx_{t+1} \rangle < \ln \frac{1}{2} + ( \sigma_{\epsilon}^2 ).
    % \quad \text{where}  \norm{\bw}_{\infty} < 1.
\end{equation}

Observe that
\begin{align*}
        \langle \bw \bx_{t}, \bx_{t+1} \rangle 
        &\leq
        \norm{\bw} \cdot \max_t \norm{\bx_t}
        \\
        &= B_w B_x
        \\&< \ln \frac{1}{2} + ( \sigma_{\epsilon}^2 )
        \sim 0.3
        .
\end{align*}

\subsection{Additional Details}

\paragraph{The History Matrix.}
The matrix form of $\mathtt{A}_i(q)$ is presented below
\begin{equation}\label{eqn:history-matrix}
    \mathtt{A}_i(q)
    \coloneqq
    \begin{bmatrix}
        x_1^i & x_2^i & \cdots & x_t^i & x_{t+1}^i & x_{t+2}^i & \cdots \\
        x_T^i & x_{T-1}^i & \cdots & x_{t-1}^i & x_t^i & x_{t+1}^i & \cdots \\
        x_{T-1}^i & x_{T-2}^i & \cdots & x_{t-2}^i & x_{t-1}^i & x_{t}^i & \cdots \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots \\
        x_{T-q}^i & x_{T-q+1}^i & \cdots & x_{t-q}^i & x_{t-q+1}^i & x_{t-q+2}^i & \cdots \\   
\end{bmatrix}
\end{equation}

\clearpage

\section{Experimental Details}\label{sec:exp-details}

\subsection{Environment}
We mostly train our model on NVIDIA-H100 GPUs with 2 cores each with 128GB RAM.
2 GPUs are sufficient for all of our experiments.
We use PyTorch 2.1 and our code is based on the open source published by \cite{woo2024unified}.
Training and evaluate takes roughly $12$ hours for one run.


\subsection{Model Architecture}
For most of our experiments, we use MOIRAI-base model.
The hyperparameters are listed in \cref{table:hyperparameters}.

\begin{table}[h]
        \centering
        \caption{Hyperparameters
        }
        % \resizebox{ \textwidth}{!}{  
        \begin{tabular}{l*{1}{c}}
        \toprule
            \bf{parameter} & \multicolumn{1}{c}{\bf{values}}  \\ 
            \midrule
            batch size & $64$ \\ 
            initial learning rate
            & $1\text{e-}3$ \\
            learning rate decay
            & cosine annealing \\
            hidden dimension
            & $768$ \\
            MLP dimension
            & $3072$ \\
            number of heads
            & $12$\\
            training steps
            & $20k$ \\
            max sequence length & $512$ \\
            \midrule
            optimizer
            & AdamW \\
            beta ($\beta_1, \beta_2$)
            & $(0.9, 0.98)$ \\
            weight decay & $1\text{e-}1$ \\
            warm up steps (linear) & $10k$ \\
            \bottomrule
        \end{tabular}
    \label{table:hyperparameters}
\end{table} 




\subsection{Synthetic Data Generation}
We generate the $\mathtt{AR}$ synthetic data similar to Equation~\eqref{eqn:AR-data} but use normalization to stabilize the values.
The parameters of synthetic data are in \cref{table:data-param}.
Consider a sequence of data 
$\bx \in \R^{d \times T} \coloneqq (\bx_1, \dots, \bx_T)$, where $\bx_t = (x_t^1, \cdots, x_t^d) \in \R^d$.
Assuming our target (variate of interest) is in dimension $1$, we assume the $\mathtt{AR}_d(q)$ process generates $x_t^1$ as follows:
\begin{equation}
    x_{t}^1
    =
    \frac{1}{qd}
    \sum_{i=1}^q
    \sum_{j=1}^d
    a_i^j \cdot x_{t-i}^j
    + \epsilon_t
    ,
\end{equation}
where $\epsilon_t \sim N(0, 1)$, $a_i^j \sim N(0, 1) \in \R$.
After recursively generating the time series, we remove its first 50 time steps as burnout.
Each $\mathtt{AR}$ time series has a number of covariates between $1$ to $5$.
For training data, we sampled $100$ different time series, each with $20k$ time steps.
For test data, we randomly generate one time series with time step $5k$, and evaluate our model on all time steps.
We set $q, d \leq 5$ in our experiments.

\paragraph{Seasonality.}
We also conduct experiments on datasets with seasonality information.
Specifically, we consider monthly information.
After generating a multi-variate time series with $T$ time steps $\bx \in \R^{d \times T}$, we then add the seasonality information.
For each time step $t$, its seasonal information is
\[
a \cdot \sin{ \frac{2 \pi T}{f}} \in \R,
\]
where $a \in \R$ is the amplitude, $f \in \N^+$ is the frequency which is $30$ for monthly information.
The whole seasonal information will be added to the time series.

\begin{table}[h]
        \centering
        \caption{Parameter of Synthetic Data
        }
        % \resizebox{ \textwidth}{!}{  
        \begin{tabular}{l*{1}{c}}
        \toprule
            \bf{parameter} & \multicolumn{1}{c}{\bf{values}}  \\ 
            \midrule
            lag size & $\{ 1, 2, 3, 4, 5 \}$ \\ 
            variance
            &  $\text{unif}(0.1, 1)$ \\
            length ($T$)
            & $20k$ \\
            number of covariates ($d$)
            & $\{1, 2, 3, 4, 5 \}$ \\
            \midrule
            amplitude &  $\text{unif}(0, 1.5)$ \\
            frequency &  $30$ \\
            \bottomrule
        \end{tabular}
    \label{table:data-param}
\end{table} 


\subsection{Baselines}

\paragraph{Least Squares Regression.}\label{appendix:ls-baseline}
Consider MOIRAI taking an input AR sequence $\bx \in \R^{d \times T}$, to match our theoretical results (\cref{thm:any-variate-auto}), we transform $\bx$ into the following input-label pairs
\begin{align*}
    \tilde{\bx}_1 &= 
    \left(
    ( \bx_1, \cdots \bx_q ),
    \bx_{q+1}
    \right)
    \\
    \tilde{\bx}_2 &= 
    \left(
    ( \bx_2, \cdots \bx_{q+1} ),
    \bx_{q+2}
    \right)...
\end{align*}
After fitting least squares on this transformed dataset with $T-q$ samples, it predicts the $T+1$-th time step with the following input
\[
\tilde{\bx}_{\text{test}} = 
\left(
\bx_{T-q+1}, \cdots \bx_{T}
\right).
\]
For least squares, we use learning rate as $0.1$, and perform full gradient descent with $50, 100$ iterations.

% \section{Additional Theoretical Background}


% \begin{lemma}[Error bound on Probability Distribution Perturbation]\label{lem:prob-perturbation}
%     Given $\bz = (\bz_1, \dots, \bz_d) \in \R^d$, $\bz^\prime = \bz + \epsilon$, where $\epsilon \in \R^d$.
%     Assume the following holds,
%     $$
%     \vert 
%     \bz_i - \bz^\prime_i
%     \vert
%     \leq 
%     \varepsilon,
%     $$
%     we have
%     \begin{equation*}
%         \norm{n(\bz) - n(\bz^\prime)}_2
%         \leq 
%         \frac{1}{s}
%         \norm{\epsilon}_2,
%     \end{equation*}
%     where 
%     $n(\bz) = (  \nicefrac{\bz_1}{\sum_{i=1} \bz_i}, \dots, \nicefrac{\bz_d}{\sum_{i=1} \bz_i})$ is a normalization operator.    
% \end{lemma}

% \todo[inline]{Revise this paragraph}

% \begin{proof}

%     The goal is to find the operation norm of $J$, which is the Jacobian of the function $n$, which
%     $\norm{J}$ satisfies 
%     $$
%     \norm{ n(\bz) - n(\bz+\epsilon) } 
%     \simeq
%     \norm{J \epsilon}
%     \leq 
%     \norm{J} \cdot \norm{\epsilon}.
%     $$

%     The normalization function $n(\bz)$ defines the component-wise operation such that
%     \begin{equation*}
%         n(\bz)_i
%         =
%         \frac{\bz_i}{s},
%         \quad\text{where }
%         s = \sum_{j=1}^d \bz_j.
%     \end{equation*}
%     Let $s \geq \delta > 0$.

%     We first calculate the Jacobian of the normalization function such that
%     $$
%     J_{ij}
%     =
%     \frac{\partial n(\bz)_i}{\partial \bz_j}
%     =
%     \frac{\delta_{ij} s - \bz_i}{s^2},
%     $$
%     where
%     \begin{equation*}
%         \delta_{ij}
%         =
%         \begin{cases}
%             1 & \text{if } i = j 
%             \\
%             0 & \text{otherwise}
%         \end{cases}.
%     \end{equation*}
%     The Jacobian can be written as
%     \begin{equation*}
%         J 
%         =
%         \frac{1}{s}
%         (\mathbf{I} 
%         - 
%         \bv e^\top
%         ),
%     \end{equation*}
%     where $e^\top$ is a row vector with all ones, and $\bv = \frac{\bz}{s}$.
%     Note that the Jacobian is symmetric.
%     Therefore, its norm is bounded by the absolute value of its largest eigenvalue.
%     For eigenvectors $\bu$ that are orthogonal to $\bv$, we have
%     \begin{equation}
%         J \bu
%         =
%         \frac{1}{s}
%         (\mathbf{I} 
%         - 
%         \bv e^\top
%         )
%         \bu
%         =
%         \frac{1}{s} \bu.
%     \end{equation}
%     For eigenvectors along $\bv$,
%     \begin{equation*}
%         J \bv
%         =
%         \frac{1}{s}
%         (\mathbf{I} 
%         - 
%         \bv e^\top
%         )
%         \bv
%         =
%         0.
%     \end{equation*}

%     Therefore, given a perturbation vector 
%     \begin{equation*}
%         \norm{J \epsilon}_2
%         \leq 
%         \frac{1}{s} \norm{\epsilon}_2
%     \end{equation*}

    
% \end{proof}



\subsection{Additional Experiments}\label{appendix:additional-exp}



\paragraph{Seasonality Data.}
Here we present the experimental results on training transformers on seasonality data.
The data generation is the same as described above.
We use the same setup for seasonality data, where our training data comes from time series with $d \in \{ 1, 2, 3, 4, 5 \}$, and $q = \{ 1, 2, 3, 4, 5\}$.
The evaluation results on seasonality data is in \cref{fig:seasonal}.
We observe that transformers are capable of inferring data with seasonality.
Note that transformers are capable of achieving nearly optimal performance, while least squares regression fails, indicating that transformers are capable of fitting a more complicated model than $\mathtt{AR}$ on a given time series.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/figure_5season.png}
    \caption{We observe that when least squares regression fails to obtain the optimal error rate for prediction, transformers are capable of having their MSE converge towards $1$ as the lookback size increases.
    This indicates that these models are capable of fitting a more complex model other than linear regression on a given time series.
    }
    \label{fig:seasonal}
\end{figure}

% \paragraph{Strongly Dependent Data.}
% Here we construct a dataset with strongly dependent data and pretrain a MOIRAI transformer on it.
% We generate a dataset with $\mathtt{AR}$ processes plus non-ergodic drift.

% \paragraph{Evaluation on Unseen $d, q$.}
% Here we conduct experiments on transformers to see if they can generalize to time series with unseen values of $d, q$.
% Therefore, we train our models on $d \in \{ 4, 5 \}$, and $q \in \{ 4, 5 \}$.
% We evaluate these models on several scenarios
% \begin{itemize}
%     \item[1] Generalization to higher order data
%     \item[2] Generalization to lower order data
%     \item[3] Generalization to higher dimension data
%     \item[4] Generalization to lower dimension data
% \end{itemize} 
