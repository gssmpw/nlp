% \vspace{-1em}
\section{Conclusion}
In this paper, we investigate the theoretical foundations of transformers as time series foundation models.
First, we show that there exists a multi-layer transformer capable of performing least squares regression on any input uni-variate time series.
Next, when considering MOIRAI, we demonstrate the existence of a multi-layer MOIRAI that adapts to the dimensionality $d$ of the input (i.e., the number of covariates) and fits different autoregressive models based on $d$.
When the data is generated by an autoregressive process, such a transformer benefits from its prediction error being exponentially suppressed by the number of layers.
We then establish a generalization bound for pretraining when the data satisfies Dobrushin's condition.
When the pretraining data is sampled from $\mathtt{AR}$ processes, we derive a more explicit bound on the test loss, with a trade-off controlled by the number of layers.
Our analysis not only provides the first theoretical justification for the design and performance of MOIRAI but also represents the first theoretical framework for constructing a time series foundation model.
% \vspace{-0.5em}

\paragraph{Limitations.}
One limitation in our analysis is that we consider ReLU instead of softmax in attention mechanisms.
While the same approach also is in theoretical \cite{bai2024transformers, lin2023transformers, he2025learning} and empirical works \cite{wortsman2023replacing, zhang2021sparse, shen2023study}, one might obtain a different approximation bound comparing to \cref{thm:any-variate-auto}.
However, in our generalization analysis, the difference is small as softmax does not affect the model complexity too much.
Another aspect is that we mainly focus on $\mathtt{AR}$ processes.
While in the appendix, we do show the approximation result for non-linear $\mathtt{AR}$ processes generated by a ReLU network, to achieve universal forecasting, a more general assumption on data is required.

\paragraph{Impact Statement.}
This paper studies the theoretical aspect of transformers as time series foundation models.
No negative societal impacts that the authors feel should be specifically highlighted here.