We give a comprehensive analysis of transformers as time series foundation models, focusing on their approximation and generalization capabilities. 
First, we demonstrate that there exist transformers that fit an autoregressive model on input univariate time series via gradient descent. 
We then analyze MOIRAI \cite{woo2024unified}, a multivariate time series foundation model capable of handling an arbitrary number of covariates. 
We prove that it is capable of automatically fitting autoregressive models with an arbitrary number of covariates, offering insights into its design and empirical success.   
For generalization, we establish bounds for pretraining when the data satisfies Dobrushinâ€™s condition. 
Experiments support our theoretical findings, highlighting the efficacy of transformers as time series foundation models.
