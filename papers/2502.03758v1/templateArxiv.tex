\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


% 这部分是自己添加的包
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{wrapfig}
% 这部分是自己添加的包



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Preprint.}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Improving Adversarial Robustness via Phase and Amplitude-aware Prompting 
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Yibo Xu \thanks{Equal contributions.}\\
  Xidian University \\
  \texttt{ybxu.xidian@gmail.com} \\
  %% examples of more authors
   \And
  Dawei Zhou \footnotemark[1]\\
  Xidian University \\
  \texttt{dwzhou.xidian@gmail.com} \\
  \AND
  Decheng Liu \\
  Xidian University \\
  % Address \\
  \texttt{dchliu@xidian.edu.cn} \\
  \And
  Nannan Wang \thanks{Corresponding author.} \\
  Xidian University \\
  % Address \\
  \texttt{nnwang@xidian.edu.cn} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
% \lipsum[1]
Deep neural networks are found to be vulnerable to adversarial noises. The prompt-based defense has been increasingly studied due to its high efficiency. However, existing prompt-based defenses mainly exploited mixed prompt patterns, where critical patterns closely related to object semantics lack sufficient focus. The phase and amplitude spectra have been proven to be highly related to specific semantic patterns and crucial for robustness. To this end, in this paper, we propose a Phase and Amplitude-aware Prompting (PAP) defense. Specifically, we construct phase-level and amplitude-level prompts for each class, and adjust weights for prompting according to the model's robust performance under these prompts during training. During testing, we select prompts for each image using its predicted label to obtain the prompted image, which is inputted to the model to get the final prediction. Experimental results demonstrate the effectiveness of our method.
\end{abstract}


% keywords can be removed
\keywords{Adversarial attack \and Adversarial robustness \and Prompt-based defense}


\section{Introduction}
% \lipsum[2]
% \lipsum[3]

Deep Neural Networks (DNNs) have been found to be vulnerable to adversarial noises \cite{szegedy2013intriguing,xiao2018spatially,
yang2023semantic}. This vulnerability has posed a significant threat to many deep learning applications \cite{jaiswal2022two,mi2023adversarial,shukla2024exploring}, promoting the development of defenses \cite{madry2017towards,zhou2022adversarial,zhao2024adversarial,xia2024inspector}.

Recently, prompt-based defenses have been increasingly investigated \cite{huang2023improving,chen2023visual}. It is of interest since it does not retrain target models like adversarial training does \cite{wu2020adversarial,wei2023cfa,singh2024revisiting}, and does not perform major modifications on data as in denoising methods \cite{nie2022diffusion,zhou2023eliminating}. However, existing prompt-based defenses mainly focus on mixed patterns, such as pixel and frequency domains (see Figure~\ref{fig_problem}). These patterns cannot explicitly reflect specific patterns like structures and textures. To this end, we seek to disentangle the mixed patterns, and construct prompts for stabilizing model predictions by utilizing the specific patterns closely related to the object semantics.


% \begin{wrapfigure}{r}{0.5\columnwidth}
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.5\columnwidth]{fig_problem}}
% \caption{Differences between previous defenses and our defense. Previous method use mixed patterns like pixel or frequency domains for prompting. However, they do not explicitly focus on specific semantic patterns. The phase and amplitude spectra can reflect structures and textures specifically. Our method utilizes these specific patterns for prompting, further improving the robustness.}
% \label{fig_problem}
% \end{center}
% \vskip -0.2in
% \end{endfigure}




The amplitude and phase spectra of the data have been proven to be able to reflect the specific semantic patterns. Previous studies indicated the amplitude spectrum holds texture patterns \cite{randen1999filtering,sidhu2005texture}, while the phase spectrum reflects structural patterns \cite{kovesi2000phase,zhang2011fsim}. Besides, cognitive sciences reveal that people tend to recognize objects by utilizing the phase spectrum \cite{freeman2011metamers,gladilin2015role}, which can also help improve the model's generalization ability \cite{chen2021amplitude}. Also, the amplitude spectrum has been proven to be easily manipulated by noises and thus further processes are needed to mitigate this problem for robustness \cite{chen2021amplitude}. To this end, \textit{constructing prompts using amplitude and phase spectra is expected to provide positive effects for prompt-based defenses} (see Figure~\ref{fig_problem}).



Motivated by the above studies, we propose a \textit{Phase and Amplitude-aware Prompting} (PAP) defense mechanism, which constructs phase and amplitude prompts to stabilize the model's predictions during testing. 
\begin{wrapfigure}{r}{0.55\columnwidth}
  \vskip 0.2in
  \centering
  \includegraphics[width=0.55\columnwidth]{fig_problem} % 图片宽度稍小于 wrapfig 宽度
  \caption{Differences between previous defenses and our defense. Previous method use mixed patterns like pixel or frequency domains for prompting. However, they do not explicitly focus on specific semantic patterns. The phase and amplitude spectra can reflect structures and textures specifically. Our method utilizes these specific patterns for prompting, further improving the robustness.}
  \label{fig_problem}
  \vskip -0.2in
\end{wrapfigure}
We learn a phase-level prompt and an amplitude-level prompt for each class, since it can help learn more precise semantic patterns while reducing computational costs compared with learning prompts for each instance. Naturally, a question arises here: \textit{Do amplitude-level prompts and phase-level prompts have the same effect on the model robustness?} To answer it, we utilize phase and amplitude spectra of natural examples to replace the corresponding spectra of adversarial examples respectively for testing as in Table~\ref{tab_weight_veri_1}. It shows phase and amplitude spectra have different effects on model's predictions. Furthermore, we construct phase-level and amplitude-level prompts, training them under different prompting weights. Table~\ref{tab_weight_veri_2} shows different weights lead to different robustness, and thus \textit{we need to adjust their weights appropriately}.




Based on these analyses, we propose a weighting method for our prompts. Since different weights for prompting lead to different robust performances, we adjust their weights based on their influences on robustness during training. We adjust the weight for amplitude-level prompts by the ratio of accuracy under adversarial training examples with amplitude-level prompts to that with phase-level prompts, since the ratio can reflect the relative importance of amplitude-level prompts compared to phase-level prompts.




During testing, we select prompts for each image according to the model's predicted label for it. Previous method \cite{chen2023visual} traverses all the prompts for different classes for testing, causing great time consumptions especially on datasets with many classes. To alleviate this problem, we directly select prompts for tested images according to their predicted labels. To further reduce the negative effect of mismatches between images and selected prompts, we design a loss that helps images with prompts not coming from their ground-truth labels to still be correctly classified. 
% Our code is available at \url{https://anonymous.4open.science/r/PAP-4D8D}.




Our contributions can be summarized as follows:
\begin{itemize}
\item Considering the amplitude and phase spectra are closely related to specific semantic patterns and crucial for robustness, we seek to design phase-level and amplitude-level prompts to provide positive gains for prompt-based defenses.

\item We propose a \textit{Phase and Amplitude-aware Prompting} (PAP) defense. Specifically, we propose a weighting method for prompts based on their impacts on the model's robust performances for training, and propose to directly select the prompts for images based on their predicted labels for testing.

\item We evaluate the effectiveness of our method for both naturally and adversarially pre-trained models against general attacks and adaptive attacks. Experimental results reveal that our method outperforms state-of-the-art methods and achieves superior transferability.
\end{itemize}

% \medskip
% \verb|\icmltitlerunning{...}|

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.99\textwidth]{fig_framework}}
\caption{The framework of our method. First, We construct phase-level and amplitude-level prompts, and adjust the weights of amplitude-level prompts according to their influences on robustness for training. Then, we select prompts from predicted labels to get prompted images using the finally adjusted weights for testing.}
\label{fig_main_framework}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Related Work}
\subsection{Adversarial Attacks}
Adversarial attacks craft malicious noises to mislead target models. White-box attacks like Projected Gradient Descent (PGD) \cite{madry2017towards}, AutoAttack (AA) \cite{croce2020reliable}, Carlini\&Wagner (C\&W) \cite{carlini2017towards} and Decoupling Direction and Norm (DDN) \cite{rony2019decoupling} craft noises through accessing and utilizing models' intrinsic information like structures and parameters. For black-box attacks like transfer-based attacks and query-based attacks \cite{andriushchenko2020square}, attackers have no access to the models' internal information, and thus attack only by interacting with model's inputs and outputs.
% 经过检查，基本没有大的与以往论文重复的可能性

\subsection{Adversarial Defenses}
Adversarial training methods (ATs) \cite{madry2017towards,zhang2019theoretically,wang2019improving} aim at augmenting training examples through adversarial noises for training. However, ATs require modifying parameters of models and crafting noises for training, consuming significant resources. In addition, denoising methods \cite{jin2019ape,zhou2023eliminating} purify images before feeding them into target models. It introduces an additional module for substantially modifying data to remove noises, thereby also consuming great computational resources.

To alleviate this problem, prompt-based defenses has attracted more and more interests due to its efficiency. C-AVP \cite{chen2023visual} trains pixel-level prompts for each class, and traverses all the prompts for testing. However, it requires high computation costs on datasets with numerous classes. Frequency Prompting (Freq) \cite{huang2023improving} aims at mitigating the vulnerability of models in the high-frequency domain by a masked prompting strategy. However, it does not explicitly focus on specific semantic patterns, where the semantic pattern which C-AVP focused on is also mixed (\textit{i.e.}, the pixel domain). Differently, \textit{we focus on specific textures and structures by prompting on the amplitude and phase spectra}. Also, our method does not traverse all the prompts for testing, \textit{achieving superior performances efficiently through selecting prompts using predicted labels}.
% 经过检查，基本没有大的与以往论文重复的可能性


\section{Methodology}

\subsection{Preliminary}
In this paper, we focus on classification tasks under adversarial settings. Given a model $h_\theta$ with parameters $\theta$ and natural data $(x,y)$, the adversarial example $\tilde{x}$ is crafted for misleading $h_\theta$. Since we focus on images, we utilize Discrete Fourier Transform (DFT) and its inverse version (IDFT), denoted as $\mathcal{F}(\cdot)$ and $\mathcal{F}^{-1}(\cdot, \cdot)$, respectively. The phase and amplitude spectra are derived as $\phi_x = \mathcal{F}_\phi(x)$ and $\xi_x = \mathcal{F}_\xi(x)$. We use $\phi_x$ and $\xi_x$ to denote phase and amplitude spectra of a natural image $x$, while $\phi_{\tilde{x}}$ and $\xi_{\tilde{x}}$ denote the corresponding spectra of $\tilde{x}$. In addition, the process to recover an image from its phase and amplitude spectra is expressed as $x = \mathcal{F}^{-1}(\phi_x, \xi_x)$. \textit{Our goal is to design prompts to assist $h_\theta$ in making accurate predictions without the need of the model retraining.}
% 优化完毕，防止与以前论文重复

\subsection{Motivation}
DNNs can be easily fooled by adversarial noises. Defenses like adversarial training and denoising methods all improve robustness with a high computational cost, promoting the development of prompt-based defenses due to the efficiency. However, existing prompt-based defenses focus on mixed patterns like pixel or frequency information, which cannot capture specific patterns like structures and textures. Thus, we seek to further disentangle these patterns for robustness.

The phase and amplitude spectra have been proven to be able to reflect specific semantic patterns. Through Fourier transform, image signals in the pixel domain can be converted into the frequency domain, which can be further decoupled into phase and amplitude spectra. The phase spectrum can reflect structures \cite{kovesi2000phase,zhang2011fsim}, while the amplitude spectrum carries textures \cite{randen1999filtering,sidhu2005texture}. Cognitive sciences indicate people tend to recognize objects by leveraging structures from the phase spectrum \cite{freeman2011metamers,gladilin2015role}, which has been proven to be able to help DNNs improve their generalization performances \cite{chen2021amplitude}. Also, the amplitude spectrum has been analyzed to be easily manipulated by noises, indicating the necessity to mitigate this problem for robustness \cite{chen2021amplitude}. \textit{To this end, constructing phase-level and amplitude-level prompts to disentangle the mixed patterns is considered to be beneficial for improving prompt-based defenses} (see Figure~\ref{fig_problem}).
% 有待补充针对图的引用
% 有待补充针对图的引用
% 有待补充针对图的引用


\subsection{Defense}
Based on the above analyses, we introduce the designed \textit{Phase and Amplitude-aware Prompting} (PAP) defense. We first construct prompts and adjust the weights of amplitude-level prompts based on their influences on robustness. During testing, we select prompts from predicted labels for prompting. The framework is shown in Figure~\ref{fig_main_framework}.
% 介绍自己的方法的整体流程，说明测试阶段的prompt是怎么选取的
% 介绍自己的方法的整体流程，说明测试阶段的prompt是怎么选取的
% 介绍自己的方法的整体流程，说明测试阶段的prompt是怎么选取的




\subsubsection{Prompt Construction and Training}
\label{sec_prompt_construct_train}
We firstly construct and train a phase-level prompt and an amplitude-level prompt for each class, since it can help learn precise natural semantic patterns for each class while reducing computational costs compared with learning prompts for each instance. We randomly sample a natural example $x$ from class $y$, and obtain its phase spectrum $\phi_x = \mathcal{F}_\phi(x)$ and amplitude spectrum $\xi_x = \mathcal{F}_\xi(x)$ as the prompt initialization for class $y$. The prompt initialization for other classes is performed following the above operation. Then, the initialized phase-level and amplitude-level prompts are denoted as $\{p_{\phi_i}\}_{i=0}^{c-1}$ and $\{p_{\xi_i}\}_{i=0}^{c-1}$, where $c$ is the number of classes. The prompted image $x^p$ for $x$ is obtained as follows:
\begin{equation}
x^p = \mathcal{F}^{-1}(\phi_{x}+p_{\phi_y}, \xi_{x}+p_{\xi_y}),
\label{eq_ori_prom}
\end{equation}
where $p_{\phi_y}$ and $p_{\xi_y}$ denote the phase-level prompt and the amplitude-level prompt corresponding to the ground-truth label $y$ of $x$. We perform prompting for the adversarial example $\tilde{x}$ following the same way as Equation~\ref{eq_ori_prom}. Then, the designed training losses are introduced as follows:




\textbf{Classification Loss.} To enforce our prompts to stabilize the model's predictions, we promote our prompts to learn to help correct wrong predictions of models, and thus exploit the prompted examples to construct the classification loss:
\begin{equation}
\mathcal{L}_{adv} = -\frac{1}{N}\sum_{i=1}^{N}[y_i log(h_\theta(\tilde{x}_i^p))],
\label{eq_loss_ce_adv}
\end{equation}
where $N$ is the number of examples, and $\tilde{x}_i^p$ denotes the prompted image of the adversarial example $\tilde{x}_i$ using the prompts from its ground-truth label $y_i$. Then, the classification loss for the natural prompted data is presented as:
\begin{equation}
\mathcal{L}_{nat} = -\frac{1}{N}\sum_{i=1}^{N}[y_i log(h_\theta(x_i^p))],
\label{eq_loss_ce_nat}
\end{equation}
where $x_i^p$ denotes the prompted image of the natural example $x_i$ using the prompts from its ground-truth label $y_i$.

\textbf{Reconstruction Loss.}
The constructed prompt could modify the phase and amplitude spectra during prompting. To ensure these modifications do not severely disrupt the original semantic patterns, we design a reconstruction loss between the prompted adversarial images and natural images as:
\begin{equation}
\mathcal{L}_{sim}=\frac{1}{N \times H \times W} \sum_{i=1}^{N}\sum_{j=1}^{H}\sum_{k=1}^{W}e^{\vert \tilde{m}_{j,k}^p-m_{j,k} \vert},
\label{eq_loss_sim}
\end{equation}
where $\tilde{m}_{j,k}^p$ and $m_{j,k}$ denote the pixel value of $\tilde{x}_i^p$ and the pixel value of $x_i$ in the j-th row and k-th colum respectively, and $H, W$ denote the height and weight of the image.
% 这里还缺一个介绍，即为什么用指数形式


% 这块的介绍用这个形式
% 我们为每个类分别学一个幅度prompt和一个相位prompt，由于我们是根据伪标签来选prompt（这个前面总起的时候必须要说，已在前面标注），测试阶段可能存在prompt与测试样本不匹配 
% 以往工作表明通过…..方法改善了/缓解了这个问题
% 因此我们用......







\textbf{Data-prompt Mismatching Loss.} Note that we learn a phase-level prompt and an amplitude-level prompt for each class. Also, we select prompts according to predicted labels during testing. Therefore, there exist mismatches between test images and selected prompts when testing. Previous studies \cite{chen2023visual} indicate we can get prompted images using prompts from classes different from ground-truth labels, and enforce their outputs on ground-truth labels to be larger than those on other labels, so that these images can still be correctly classified to some extent. To this end, we construct a data-prompt mismatching loss as:
\begin{equation}
\mathcal{L}_{mis} = \frac{1}{N}\sum_{i=1}^{N} \{ max\{ h_\theta^{y'_i}(\tilde{x}_i^{p'}) - h_\theta^{y_i}(\tilde{x}_i^{p'}), -\tau \} \},
\label{eq_loss_mismatch}
\end{equation}
where $\tilde{x}_i^{p'}$ is the prompted adversarial example using prompts from $y'_i$, which is a randomly selected label different from its ground-truth label $y_i$. $h_\theta^{y'_i}(\cdot)$ and $h_\theta^{y_i}(\cdot)$ denote outputs on $y'_i$ and $y_i$, and $\tau$ is a threshold set as 0.1 in this paper.


\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{fig_weight}}
\caption{The weighting method for amplitude-level prompts. We use amplitude-level and phase-level prompts respectively for prompting, and adjust weights by the ratio of accuracy of images with amplitude-level prompts to that with phase-level prompts.}
\label{fig_weight}
\end{center}
\vskip -0.2in
\end{figure}



% 使用 wraptable 将表格靠右
\begin{wraptable}{r}{0.5\columnwidth}
  \caption{The impact of different spectra on robustness. \textit{Adv. All} denotes normal noises. \textit{Nat. Pha./Amp.} indicate we replace phase/amplitude spectra with corresponding natural spectra.}
  \label{tab_weight_veri_1}
  \vskip 0.15in
  \centering
  \begin{small}
    \begin{tabular}{l|ccc}
      \hline
          & Adv. All & Nat. Pha. & Nat. Amp. \\ \hline
      NAT & 0.00     & 47.81    & 13.41    \\
      AT  & 46.82    & 69.00    & 70.10   \\ \hline
    \end{tabular}
  \end{small}
  \vskip -0.1in
\end{wraptable}





% 这一部分就是4段，第一段总起，第二段第一个实验，第三段第二个实验，最后介绍权重如何设计
\subsubsection{Weighting Method}
% Based on our explorations, we note that the amplitude spectrum and the phase spectrum are both closely related to the specific semantic information and are expected to be beneficial for robustness. To this end, we aim at constructing and optimizing amplitude-level and phase-level prompts for improving robustness.
The designed prompts with the corresponding training procedure focus on helping the model make predictions accurately during testing. However, it is natural for us to question \textit{whether the amplitude-level and phase-level prompts have the same influence on the robustness}. To this end, we conduct several experimental analyses to answer it.

We firstly replace the amplitude and phase spectra of adversarial examples with the corresponding spectrum of natural examples respectively. As shown in Table~\ref{tab_weight_veri_1}, for different models, the natural amplitude spectrum and natural phase spectrum contribute differently to the model's robust performances. It indicates that \textit{the amplitude and phase spectra have different influences on the model's robustness}.
% 相位幅度有不同的影响，所以有不同的权重  
% 相位幅度有不同的影响，所以有不同的权重
% 所以我们分配不同权重做下面的验证实验
% 所以我们分配不同权重做下面的验证实验
% 所以我们分配不同权重做下面的验证实验


% 使用 wraptable 将表格靠右
\begin{wraptable}{r}{0.5\columnwidth}
  \caption{Robust accuracy (percentage) under different weights. $\alpha$ and $\beta$ are weights of phase-level and amplitude-level prompts.}
  \label{tab_weight_veri_2}
  \vskip 0.15in
  \centering
  \begin{small}
    \begin{tabular}{cc|cc|cc}
    \hline
    \multirow{2}{*}{$\alpha$} & \multirow{2}{*}{$\beta$} & \multicolumn{2}{c|}{NAT} & \multicolumn{2}{c}{AT} \\ \cline{3-6}
    & & None& PGD& None& PGD\\ \hline
    1& 0.01& 88.82& 34.44& 84.31& 47.69\\
    0.01& 1& 91.11& 5.64& 73.84& 50.87\\ \hline
    \end{tabular}
  \end{small}
  \vskip -0.1in
\end{wraptable}


Due to their different influences on robustness, we may need to assign different weights for phase-level prompts and amplitude-level prompts to further improve the robustness. To show this, we assign different weights for them to obtain the prompted images for training and testing. As shown in Table~\ref{tab_weight_veri_2}, it is clear that different weight assignments result in different robust performances. Therefore, \textit{we need to design a strategy which can appropriately adjust their weights}.
% 所以我们要设计自适应的权重，接到下面这句话
% 所以我们要设计自适应的权重，接到下面这句话
% 所以我们要设计自适应的权重，接到下面这句话
% 所以我们要设计自适应的权重，接到下面这句话




Based on it, we enforce prompts to assign weights for themselves according to their influences on the robustness. 
% 这句不需要，你已经在上面结合实验介绍了为什么要调节权重了
% Due to the different effects of amplitude-level prompts and phase-level prompts on the robustness, directly treating these two kinds of prompts equally may affect the model's performances negatively. Therefore, adjusting their weights appropriately is considered to be beneficial for robustness. 
% 下面这段解释是根据讨论后的结果修改的，就这么写就行
The robust accuracy after prompting can explicitly reflect the influence of these prompts on robustness, which has been proven to be suitable for measuring the importance of them and adjusting their weights \cite{wang2019improving,wei2023cfa}.
% 这里要说一下为什么要用比值的形式，一句话就行：比值可以体现幅度相对于相位的重要性，直接说相位的固定为1，其他的不用多说
\textit{Therefore, we use the robust accuracy under these prompts to adjust their weights.}
Specifically, during training, we obtain robust accuracies in training data using amplitude-level and phase-level prompts for prompting respectively. Then, the weights for amplitude-level prompts are adjusted by the ratio of accuracy under amplitude-level prompts to accuracy under phase-level prompts, since it can reflect the relative importance of amplitude-level prompts compared with phase-level prompts for robustness. The weight strategy is specified as:
\begin{equation}
w_t = w_{t-1} \times \frac{\sum_{i=1}^{N}\mathbb{I}(f(\tilde{x}_i^{p_\xi})=y_i)}{\sum_{i=1}^{N}\mathbb{I}(f(\tilde{x}_i^{p_\phi})=y_i)},
\label{eq_w_adjust}
\end{equation}
where $\tilde{x}_i^{p_\xi}=\mathcal{F}^{-1}(\phi_{\tilde{x}_i}, \xi_{\tilde{x}_i}+w_{t-1}p_{\xi_{y_i}})$ and $\tilde{x}_i^{p_\phi}=\mathcal{F}^{-1}(\phi_{\tilde{x}_i}+p_{\phi_{y_i}}, \xi_{\tilde{x}_i})$, and $y_i$ is the ground-truth label of the adversarial example $\tilde{x}_i$. $\mathbb{I}(\cdot)$ denotes the indicator function, and $w_t$ is the weight during the t-th epoch. The Equation~\ref{eq_ori_prom} is then incorporated with the designed weight as:
\begin{equation}
x^p = \mathcal{F}^{-1}(\phi_{x}+p_{\phi_y}, \xi_{x}+w_tp_{\xi_y}),
\label{eq_new_prom}
\end{equation}
where we use Equation~\ref{eq_new_prom} for training, and we adjust the weight every 5 epochs for the efficiency. The finally learned weight $w^*$ is utilized for testing, and is shown in Table~\ref{tab_learned_w}. The weighting strategy is illustrated in Figure~\ref{fig_weight}.
% 这里说明一下对前面的式子的重构
% 这里说明一下对前面的式子的重构
% 这里说明一下对前面的式子的重构




\begin{algorithm}[t]\small
   \caption{Phase and Amplitude-aware Prompting (PAP).}
   \label{alg:alg_1}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} The target model $h_\theta$, training dataset $\mathcal{D}$, batch size $n$, the number of batches $M$, epoch number $T$, perturbation budget $\epsilon$, the initialized phase-level prompts $\{p_{\phi_i}\}_{i=0}^{c-1}$ and amplitude-level prompts $\{p_{\xi_i}\}_{i=0}^{c-1}$.\\
   \FOR{$t=1$ {\bfseries to} $T$}
   \FOR{$m=1$ {\bfseries to} $M$}
   \STATE Read mini-batch $\mathcal{B}=\{x_i\}_{i=1}^{n}$ from training set $\mathcal{D}$;
   \STATE Craft corresponding adversarial samples $\tilde{\mathcal{B}}=\{\tilde{x}_i\}_{i=1}^{n}$ at the given perturbation budget $\epsilon$;
   \STATE Calculate $L_{all}$ by Equation~\ref{eq_loss_all} to optimize $\{p_{\phi_i}\}_{i=0}^{c-1}$ and $\{p_{\xi_i}\}_{i=0}^{c-1}$;
   \ENDFOR
   \IF{$t\bmod 5=0$}
   \STATE update $w_t$ via Equation~\ref{eq_w_adjust};
   \ENDIF
   \ENDFOR
\end{algorithmic}
\end{algorithm}






% 这段和测试阶段选prompt的这两小节对调一下位置
% 这段和测试阶段选prompt的这两小节对调一下位置
% 这段和测试阶段选prompt的这两小节对调一下位置
% 这段和测试阶段选prompt的这两小节对调一下位置
\subsubsection{Overall Defense Procedure}
To improve the overall effectiveness of our combined defense, we incorporate the the weighting strategy into the training process. The overall loss function is denoted as:
\begin{equation}
\mathcal{L}_{all}=\mathcal{L}_{adv}+\lambda_1\mathcal{L}_{nat}+\lambda_2\mathcal{L}_{sim}+\lambda_3\mathcal{L}_{mis},
\label{eq_loss_all}
\end{equation}
where $\lambda_1$, $\lambda_2$, $\lambda_3$ are hyper-parameters.

The overall defense procedure is presented in Algorithm~\ref{alg:alg_1}. Specifically, during training, for each mini-batch $\mathcal{B}$, we craft adversarial examples $\tilde{\mathcal{B}}$. Then, we forward-pass $\mathcal{B}$ and $\tilde{\mathcal{B}}$ to calculate $L_{all}$ using Equation~\ref{eq_loss_all}, and further optimize phase-level prompts $\{p_{\phi_i}\}_{i=0}^{c-1}$ and amplitude-level prompts $\{p_{\xi_i}\}_{i=0}^{c-1}$. The weight for amplitude-level prompts is adjusted by Equation~\ref{eq_w_adjust}. Through iteratively optimizing the prompts and adjusting the weights, the prompts are expected to provide superior robustness gains.

\begin{table*}[t]
\caption{Robust accuracy (percentage) of defenses against adversarial attacks on CIFAR-10 and Tiny-ImageNet. The target models are ResNet18 and WRN28-10. We present the most successful defense results with \textbf{bold}.}
\label{tab_main_white}
\renewcommand\tabcolsep{3.8pt}
\renewcommand{\arraystretch}{1.1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cccc|cccc}
\hline
\multirow{2}{*}{Defense} & \multicolumn{4}{c|}{CIFAR-10 (ResNet18)}& \multicolumn{4}{c}{Tiny-ImageNet (WRN28-10)}
\\ \cline{2-9}
& None & AA & C\&W & DDN & None & AA & C\&W & DDN 
\\
\hline
None  & \textbf{94.83$\pm$0.05} & 0.00$\pm$0.00  & 0.00$\pm$0.00  & 0.00$\pm$0.00   & \textbf{66.62$\pm$0.11}  & 0.00$\pm$0.00 & 0.00$\pm$0.00 & 0.02$\pm$0.00 \\
\hspace{0.2cm}+Freq  & 94.50$\pm$0.21  & 0.44$\pm$0.08 & 11.57$\pm$0.15 & 4.60$\pm$0.07  & 60.43$\pm$0.17 & 2.56$\pm$0.03  & 14.82$\pm$0.20  & 10.51$\pm$0.22 \\
\hspace{0.2cm}+C-AVP   & 92.67$\pm$0.51 & 0.61$\pm$0.11  & 1.93$\pm$0.07 & 1.05$\pm$0.17 & 66.52$\pm$0.02 & 0.39$\pm$0.00 & 5.83$\pm$0.07  & 4.19$\pm$0.25  \\
\hspace{0.2cm}+PAP(Ours)   & 87.12$\pm$0.21  & \textbf{37.34$\pm$0.11}  & \textbf{80.27$\pm$0.28}   & \textbf{66.22$\pm$0.21}  & 57.30$\pm$0.15  & \textbf{5.33$\pm$0.07} & \textbf{42.14$\pm$0.36}  & \textbf{33.27$\pm$0.41}  \\ \cdashline{1-9}
AT  & 84.22$\pm$0.21    & 44.94$\pm$0.44  & 0.84$\pm$0.18 & 2.97$\pm$0.34  & 51.39$\pm$0.17 & 18.29$\pm$0.42  & 0.19$\pm$0.02   & 11.48$\pm$0.27  \\
\hspace{0.2cm}+Freq   & 78.26$\pm$0.11  & 51.50$\pm$0.31   & 35.67$\pm$0.35  & 35.12$\pm$0.17  & 44.84$\pm$0.28 & 22.93$\pm$0.26  & 19.25$\pm$0.41 & 21.76$\pm$0.20  \\
\hspace{0.2cm}+C-AVP  & 84.28$\pm$0.24  & 45.79$\pm$0.33 & 11.00$\pm$0.14   & 10.35$\pm$0.24  & 51.16$\pm$0.07   & 19.82$\pm$0.21 & 12.42$\pm$0.19  & 17.45$\pm$0.22 \\
\hspace{0.2cm}+PAP(Ours)  & \textbf{84.34$\pm$0.12}  & \textbf{52.31$\pm$0.19}   & \textbf{66.66$\pm$0.18} & \textbf{60.29$\pm$0.07}  & \textbf{51.40$\pm$0.09}  & \textbf{23.44$\pm$0.24} & \textbf{35.76$\pm$0.34} & \textbf{34.98$\pm$0.16} \\ \cdashline{1-9}
TRADES  & 81.59$\pm$0.21  & 48.98$\pm$0.23  & 0.74$\pm$0.10 & 5.03$\pm$0.09  & \textbf{48.98$\pm$0.07} & 17.87$\pm$0.07 & 0.15$\pm$0.00 & 14.29$\pm$0.09  \\
\hspace{0.2cm}+Freq  & 75.62$\pm$0.22 & 52.87$\pm$0.24  & 30.98$\pm$0.15  & 30.74$\pm$0.05  & 41.80$\pm$0.11    & 21.64$\pm$0.33  & 15.47$\pm$0.07  & 21.36$\pm$0.20 \\
\hspace{0.2cm}+C-AVP  & 81.58$\pm$0.17  & 49.44$\pm$0.24 & 4.47$\pm$0.20& 7.97$\pm$0.35& 48.59$\pm$0.19& 19.15$\pm$0.34& 9.18$\pm$0.17& 18.57$\pm$0.13\\
\hspace{0.2cm}+PAP(Ours)& \textbf{81.62$\pm$0.14}& \textbf{54.36$\pm$0.24}& \textbf{63.89$\pm$0.27}& \textbf{57.58$\pm$0.20}& 48.03$\pm$0.24& \textbf{22.73$\pm$0.34}&\textbf{30.43$\pm$0.14}& \textbf{31.81$\pm$0.26}\\ \cdashline{1-9}
MART& \textbf{80.31$\pm$0.24}& 46.95$\pm$0.24& 0.75$\pm$0.07& 3.85$\pm$0.15& \textbf{44.29$\pm$0.04}& 19.18$\pm$0.17& 0.34$\pm$0.02& 15.31$\pm$0.07\\
\hspace{0.2cm}+Freq& 74.14$\pm$0.18& 52.60$\pm$0.22& 34.40$\pm$0.16& 32.79$\pm$0.19& 37.76$\pm$0.28& 22.80$\pm$0.27& 15.46$\pm$0.24& 21.75$\pm$0.24\\
\hspace{0.2cm}+C-AVP& 80.29$\pm$0.25& 47.25$\pm$0.27& 3.51$\pm$0.45& 6.11$\pm$0.08& 43.86$\pm$0.27& 20.66$\pm$0.20& 10.78$\pm$0.33& 20.09$\pm$0.27\\
\hspace{0.2cm}+PAP(Ours)& 79.49$\pm$0.20& \textbf{53.79$\pm$0.11}& \textbf{60.36$\pm$0.22}& \textbf{56.66$\pm$0.30}& 43.71$\pm$0.23& \textbf{23.74$\pm$0.27}& \textbf{30.39$\pm$0.09}& \textbf{32.18$\pm$0.08}\\
\hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



% 经过确认，这段这样介绍是可以的，不用加动机介绍为什么要用伪标签
% 经过确认，这段这样介绍是可以的，不用加动机介绍为什么要用伪标签
% 经过确认，这段这样介绍是可以的，不用加动机介绍为什么要用伪标签
% 经过确认，这段这样介绍是可以的，不用加动机介绍为什么要用伪标签
% 把这段放到最后面去，删掉算法里对应的step 2 testing
% 把这段放到最后面去，删掉算法里对应的step 2 testing
% 把这段放到最后面去，删掉算法里对应的step 2 testing
\subsubsection{Prompt Selection for Testing}
After acquiring our prompts, we need to explore an effective prompt selection method during testing. Previous methods \cite{chen2023visual} traverse all the prompts from all the classes for testing on naturally pre-trained models, which sets the label with the largest output among all the prompting cases as the final prediction (see Appendix~\ref{sec_sup_cavp_prompt_select}). However, it can easily cause high computational costs for testing on large datasets with numerous classes. To address it, we promote the test image to choose prompts corresponding to its predicted label directly. Incorporated with the learned weight, we obtain the prompted image for testing as:
\begin{equation}
\begin{gathered}
x_{test}^p = \mathcal{F}^{-1}(\phi_{x_{test}}+p_{\phi_{y_{pred}}}, \xi_{x_{test}}+w^*p_{\xi_{y_{pred}}}),
\label{eq_our_test}
\end{gathered}
\end{equation}
where $p_{\phi_{y_{pred}}}$ and $p_{\xi_{y_{pred}}}$ are selected prompts from the predicted label $y_{pred}$. Since this strategy may result in mismatches between images and selected prompts, we introduce a data-prompt mismatching loss to alleviate its negative effects on robustness, which can be seen in Section~\ref{sec_prompt_construct_train}.
% 这里最终的用于测试的w需要设定一个特定的字母，并介绍一下是用于测试的权重

% To address this problem, we seek to select prompts based on the model's predicted labels, without the demand of traversing all the prompts from different classes. However, in such a selection strategy, there usually exist some examples that will be misclassified by the model, which can cause unmatched prompt assignment for tested examples. To this end, it is necessary for us to alleviate the negative effect of such wrong assignments on the model's performance.

% Inspired by the previous defense, we can enforce the prompted images with prompts from other classes to be correctly classified through controlling the difference of logits between different prompts. Since the main goal is to optimize the prompts of each class through enforcing training data with prompts from their ground-truth labels to be correctly predicted, directly promoting data with prompts from wrong labels to be correctly predicted for training prompts may conflict with the main goal. To this end, ... propose to.....






\begin{table}[t]
\caption{Robust accuracy (percentage) of defenses against adversarial attacks on CIFAR-10 using the prompt selection method of C-AVP. The target model is ResNet18.}
\label{tab_main_white_cavptest}
\renewcommand\tabcolsep{15pt}
\renewcommand{\arraystretch}{1.1}
\vskip 0.15in
\begin{center}
\begin{small}

\begin{tabular}{l|cccc}
\hline
Defense & None & AA & C\&W & DDN \\ \hline
NAT     & \textbf{94.83}& 0.00& 0.00& 0.00\\
\hspace{0.2cm}+Freq   & 56.80& 7.54& 49.22& 30.25\\
\hspace{0.2cm}+C-AVP  & 52.17& 32.26& 46.78& 39.51\\
\hspace{0.2cm}+PAP(Ours)   & 86.59& \textbf{38.33}& \textbf{83.88}& \textbf{70.24} \\
\hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\begin{table}[t]
\caption{The learned weights for the amplitude-level prompts. We show the results of ResNet18 and WRN28-10.}
\label{tab_learned_w}
\renewcommand\tabcolsep{15pt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|l|cccc}
\hline
Model& Dataset& None & AT & TRADES & MART \\ \hline
ResNet18& CIFAR-10& 0& 0.3054& 0.2572& 0.3258\\
WRN28-10 & Tiny-ImageNet & 0& 0.2702& 0.3022& 0.2848\\
\hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\begin{table*}[t]
\caption{Robust accuracy (percentage) of defenses on different models. All the prompt-based defenses are trained on ResNet18, and then applied to the VGG19 and WRN28-10 respectively. We present the most successful defense results with \textbf{bold}.}
\label{tab_transfer}
\renewcommand\tabcolsep{10pt}
\renewcommand{\arraystretch}{1.2}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{c|l|cccc|cccc}
\hline
\multirow{2}{*}{Model}& \multirow{2}{*}{Defense} & \multicolumn{4}{c|}{CIFAR-10}& \multicolumn{4}{c}{Tiny-ImageNet}
\\ \cline{3-10}
&   & None & AA & C\&W & DDN & None & AA & C\&W & DDN
\\ \hline
\multirow{8}{*}{VGG19}& NAT & \textbf{93.13}& 0.00& 0.00& 0.00 & \textbf{59.40} & 0.00  & 0.00 & 0.03
\\
& \hspace{0.2cm}+Freq  & 90.77& 1.69& 21.08& 11.07& 47.79& 4.27& 16.58& 12.55
\\
& \hspace{0.2cm}+C-AVP & 21.60& 10.78& 15.58& 12.32& 50.95& 3.00& 17.97& 13.61
\\
& \hspace{0.2cm}+PAP(Ours)  & 87.65& \textbf{34.28}& \textbf{81.33}& \textbf{65.65}& 49.30& \textbf{7.39}& \textbf{38.66}& \textbf{31.26}
\\ \cdashline{2-10}
& AT& \textbf{80.12}& 42.61& 0.38& 3.42&\textbf{38.99}& 10.73& 0.13& 5.40
\\
& \hspace{0.2cm}+Freq  & 73.78& 49.04& 34.08&33.10& 30.11& 15.69& 16.23& 16.93
\\
& \hspace{0.2cm}+C-AVP & 79.99& 43.60& 10.67& 10.41& 38.86& 14.20& 16.85& 16.60
\\
& \hspace{0.2cm}+PAP(Ours)  & 79.53& \textbf{49.43}& \textbf{56.26}& \textbf{51.22}& 38.37& \textbf{16.55}& \textbf{29.98}& \textbf{27.76}
\\ \hline
\multirow{8}{*}{WRN28-10} & NAT& \textbf{95.42}& 0.00& 0.00& 0.00& \textbf{66.62}& 0.00& 0.00& 0.02
\\
& \hspace{0.2cm}+Freq  & 94.53& 0.98& 18.29& 7.95& 54.47& 3.67& 18.29& 13.78
\\
& \hspace{0.2cm}+C-AVP & 10.24& 9.95& 10.32& 10.14& 61.83& 2.07& 19.11& 13.49
\\
& \hspace{0.2cm}+PAP(Ours)  & 87.83& \textbf{41.81}& \textbf{81.88}& \textbf{69.19}& 54.93& \textbf{6.88}& \textbf{40.90}& \textbf{34.05}
\\ \cdashline{2-10}
& AT     & \textbf{87.85}& 49.45& 1.16& 4.62& \textbf{51.50}& 18.27& 0.19& 11.48
\\
& \hspace{0.2cm}+Freq  & 82.50& 54.40& 34.79& 34.54& 44.29& 23.32& 19.88& 22.18
\\
& \hspace{0.2cm}+C-AVP & 87.74& 50.50& 12.01& 12.93& 51.18& 20.03& 15.31& 19.31
\\
& \hspace{0.2cm}+PAP(Ours)  & 87.26& \textbf{54.99}& \textbf{70.29}& \textbf{63.62}& 51.27& \textbf{23.71}& \textbf{36.27}& \textbf{35.41}
\\ \hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\section{Experiments}

\subsection{Experimental Settings}

\textbf{Datasets and Models.} We use CIFAR-10 \cite{krizhevsky2009learning} and Tiny-ImageNet \cite{le2015tiny} for evaluations. CIFAR-10 has 10 classes with 50,000 training images and 10,000 testing images, and Tiny-ImageNet has 200 classes with 100,000 training images, 10,000 validation images and 10,000 testing images. All the images are normalized into $[0,1]$. We use ResNet18 \cite{he2016deep} and WideResNet28-10 (WRN28-10) \cite{zagoruyko2016wide} as target models, and use WRN28-10, VGG19 \cite{simonyan2014very} and popular Swin Transformer \cite{liu2021swin} for evaluating the transferability.
% 模型的说明待改动

\textbf{Attack Settings.}
We use various attacks across two norms for evaluations. We utilize $L_\infty$-norm AA \cite{croce2020reliable}, $L_2$-norm C\&W \cite{carlini2017towards} and $L_2$-norm DDN \cite{rony2019decoupling}. The iteration number of $L_2$-norm DDN is 20, while that of $L_2$-norm C\&W is 50. The perturbation budget for $L_\infty$-norm AA is $8/255$.
% 待添加黑盒攻击的介绍

\textbf{Defense Settings.} We use prompt-based defenses C-AVP \cite{chen2023visual} and Freq \cite{huang2023improving} as baselines, where they are designed only for defending on naturally pre-trained models. We use natural training (NAT), AT \cite{madry2017towards}, TRADES \cite{zhang2019theoretically} and MART \cite{wang2019improving} to obtain pre-trained models. We use PGD with perturbation budget 8/255, perturb step 10 and step size 2/255 for training. We train prompts by SGD \cite{andrew2007scalable} for 100 epochs, where the initial learning rate is 0.1 and is divided by 10 at the 75-th epoch. The batch size is 512 for CIFAR-10, and 256 for Tiny-ImageNet. We set $\lambda_1\!=\!3$, $\lambda_2\!\!=\!\!400$, $\lambda_3\!=\!4$ for naturally pre-trained models, and $\lambda_1\!=\!1$, $\lambda_2\!=\!5000$, $\lambda_3\!=\!4$ for adversarially pre-trained models. We omit deviations in several tables due to their small values ($\leq\!\!0.60\%$). More details can be found in Appendix~\ref{sec_sup_setting}.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{fig_prompted}}
\caption{Visualizations of prompted images for input examples. For each pair of images, the left part denotes the prompted image, while the right part denotes the difference heatmap compared to the original input (\textit{i.e.}, adversarial) example.}
\label{fig_prompted_images}
\end{center}
\vskip -0.2in
\end{figure}







\subsection{Defending against General Attacks}

\textbf{Defending against White-box Attacks.} We apply various attacks to evaluate the robustness. The average accuracies with the deviations are presented in Table~\ref{tab_main_white}.




% 使用 wraptable 将表格靠右
\begin{wraptable}{r}{0.5\columnwidth}
  \caption{Robust accuracy (percentage) of defenses against adaptive attacks on CIFAR-10. The target model is ResNet18.}
  \label{tab_ada_att}
  \renewcommand{\arraystretch}{1.2}
  \vskip 0.15in
  \centering
  \begin{small}
    \begin{tabular}{l|ccc}
    \hline
    Defense& None & AdaA20 & AdaA40 \\ \hline
    NAT+Freq  & \textbf{94.35}& 0.68& 0.88\\
    NAT+C-AVP & 77.74& 0.01& 0.01\\
    NAT+PAP(Ours)  & 90.44& \textbf{27.95}& \textbf{17.64}\\ \cdashline{1-4}
    AT+Freq   & 78.30\textbf&{} 32.44& 32.22\\
    AT+C-AVP  & 83.08& 45.32& 45.08\\
    AT+PAP(Ours)   & \textbf{84.70}& \textbf{47.94}& \textbf{47.04}\\ \hline
    \end{tabular}
  \end{small}
  \vskip -0.1in
\end{wraptable}



Figure~\ref{fig_prompted_images} shows our method can preserve complete semantic patterns after prompting. Quantitative analyses in Table~\ref{tab_main_white} show that our method improves the robustness by a large margin on various attacks compared with existing defenses. On AutoAttack, our PAP helps increase the robust accuracy by about 37\% on CIFAR-10 and 4\% on Tiny-ImageNet for naturally pre-trained models, and achieves better robustness on adversarially pre-trained models from both datasets. Although Frequency Prompting improves robustness against AA for adversarially pre-trained models to some degrees, it decreases natural accuracy by about 7\% on these models. On C\&W and DDN, our method provides great positive effects for robustness. In addition, although our method sacrifies natural accuracy on naturally pre-trained models to some extent, it greatly improves robustness against all of these attacks (e.g., 80.27\% and 42.14\% against C\&W on CIFAR-10 and Tiny-ImageNet).
% 后续要进一步完善优势的介绍
% 后续要进一步完善优势的介绍
% 后续要进一步完善优势的介绍







\textbf{Defending against Black-box Attacks.} We apply transfer-based attacks using VGG19 as the surrogate model and query-based attack Square \cite{andriushchenko2020square} for evaluations. Table~\ref{tab_supp_black} in Appendix~\ref{sec_sup_black} shows our method achieves superior performances, verifying the practicality of our defense in real scenarios.


\textbf{Defenses on the Prompt Selection Method of C-AVP.} To further verify the stability of our method, we use the prompt selection strategy of C-AVP on the naturally pre-trained ResNet18 on CIFAR-10 for evaluations. Table~\ref{tab_main_white_cavptest} shows although previous methods achieve some improvements on robustness, they reduce natural accuracy by a large margin. In comparison, our method can still protect models more effectively without losing natural accuracy too much, achieving more stable performances.

\begin{wraptable}{r}{0.5\columnwidth}
  \caption{The impact of different losses on CIFAR-10. We report average robust accuracies due to space limitations.}
  \label{tab_abla_remove_loss}
  \vskip 0.15in
  \centering
  \begin{small}
    \begin{tabular}{ccc|cc|cc}
    \hline
    \multicolumn{3}{c|}{Losses}& \multicolumn{2}{c|}{NAT} & \multicolumn{2}{c}{AT} \\ \hline
    $\mathcal{L}_{nat}$ & $\mathcal{L}_{sim}$ & $\mathcal{L}_{mis}$ & None& Avg& None& Avg\\ \hline
    $\times$&$\surd$&$\surd$& 48.27& 41.35& 84.32& \textbf{59.77}\\
    $\surd$& $\times$& $\surd$& 83.64& \textbf{61.56}& 83.20& 54.19\\
    $\surd$&$\surd$& $\times$& 10.01& 10.43& 84.34& 59.48\\
    $\surd$& $\surd$& $\surd$& \textbf{87.12}& 61.27& \textbf{84.34}& 59.57\\
    \hline
    \end{tabular}
  \end{small}
  \vskip -0.1in
\end{wraptable}

\subsection{Defense Transferability}
To evaluate the transferability across different models, we applied our method trained on ResNet18 to other target models, \textit{i.e.}, WRN28-10, VGG19 and Swin Transformer. Table~\ref{tab_transfer} and Table~\ref{tab_supp_transfer_vit} (see Appendix~\ref{sec_sup_transfer_vit}) show our PAP can effectively help defend against various attacks across both convolutional neural networks and vision transformers. It indicates that we can train our prompts only once and directly apply them to other models for defenses effectively.



\subsection{Defending against Adaptive Attacks}
Since we achieve defenses by prompting, the prompts could be leaked to attackers for performing adaptive attacks (AdaA). In this case, attackers focus on crafting adversarial noises for misleading predictions after prompting as:
\begin{equation}
\underset{\delta}{max}\ell_{ce}(\mathcal{F}^{-1}(\mathcal{F}_\phi(x\!+\!\delta)+p_{\phi_y}, \mathcal{F}_\xi(x\!+\!\delta)+w_tp_{\xi_y}),y),
\label{eq_ada_att}
\end{equation}
where $\ell_{ce}$ denotes the cross-entropy loss and $\delta$ is the perturbation. For fairness, we retrain our PAP on this attack and apply the same adaptive attack strategy on baselines using their own prompts to retrain them. Then, the retrained defenses are evaluated on adaptive attacks. The iteration number of attack for training is 10, while that for testing is 20 and 40. Table~\ref{tab_ada_att} shows our method achieves better robust accuracy, verifying the effectiveness of our method.


\begin{wraptable}{r}{0.5\columnwidth}
  \caption{The effectiveness of the weight for prompting. We report average robust accuracies on both datasets.}
  \label{tab_abla_weight}
  \vskip 0.15in
  \centering
  \begin{small}
    \begin{tabular}{c|cc|cc}
    \hline
    \multirow{2}{*}{weight} & \multicolumn{2}{c|}{CIFAR-10}& \multicolumn{2}{c}{Tiny-ImageNet} \\ \cline{2-5}
    & None& Avg & None& Avg\\ \hline
    $\times$& 75.92 & 57.27& 41.59& 28.70\\
    $\surd$& \textbf{84.34} & \textbf{59.57}\textbf&{} \textbf{51.40}& \textbf{31.39}\\ 
    \hline
    \end{tabular}
  \end{small}
  \vskip -0.1in
\end{wraptable}



\subsection{Ablation Studies}

\textbf{Loss Functions.} We explore impacts of losses with different hyper-parameters. Table~\ref{tab_abla_remove_loss} shows removing any of these losses will damage performances, such as the extremely accuracy drop when removing $\mathcal{L}_{nat}$ or $\mathcal{L}_{mis}$ on NAT. Also, Appendix~\ref{sec_sup_hyper} shows natural and robust accuracies vary differently in various hyper-parameter settings for these losses. As a whole, the hyper-parameters we set achieve superior performances in both natural and robust accuracies.
% 这块缺的有点多
% 这块缺的有点多
% 这块缺的有点多
% 这块缺的有点多

\textbf{The Weighting Strategy.} To verify the effectiveness of the weighting strategy, we remove it for evaluations. As shown in Table~\ref{tab_abla_weight}, the performances drop a lot when removing the weighting strategy. Therefore, this strategy for dealing with different effects of phase-level and amplitude-level prompts on robustness is necessary and rational.





\begin{table}[t]
\caption{The effectiveness of learning prompts for each class on CIFAR-10 using ResNet18 compared with \textbf{universal} prompts.}
\label{tab_abla_uni}
\renewcommand\tabcolsep{9pt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cc||l|cc}
\hline
Defense& None  & AA & Defense& None& AA \\ \hline
NAT& \textbf{94.83} & 0.00 & AT & 84.22 & 44.94 \\
\hspace{0.2cm}+Universal & 87.54 & 31.81& \hspace{0.2cm}+Universal & \textbf{84.56} & 51.92 \\
\hspace{0.2cm}+PAP(Ours)& 87.12 & \textbf{37.34} & \hspace{0.2cm}+PAP& 84.34 & \textbf{52.31} \\
\hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[t]
\caption{The effectiveness of defenses with Gaussian Blur on CIFAR-10. The target model is ResNet18.}
\label{tab_abla_gaussian_blur}
\renewcommand\tabcolsep{18pt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cccc}
\hline
Defense & None  & AA& C\&W& DDN\\ \hline
NAT& \textbf{70.57}& 25.92 & 56.79 & 40.10 \\
\hspace{0.2cm}+Freq& 70.50 & 26.51 & 57.94 & 40.88 \\
\hspace{0.2cm}+C-AVP& 65.83 & 25.75 & 53.01 & 37.04 \\
\hspace{0.2cm}+PAP(Ours)& 67.83 & \textbf{46.57} & \textbf{65.38} & \textbf{58.85} \\ \cdashline{1-5}
AT& \textbf{78.55} & 55.51 & 57.78 & 54.77 \\
\hspace{0.2cm}+Freq& 72.72 & 55.94 & 57.57 & 55.61 \\
\hspace{0.2cm}+C-AVP& 78.09 & 55.68 & 59.16 & 55.95 \\
\hspace{0.2cm}+PAP(Ours)& 77.01 & \textbf{57.41} & \textbf{64.57} & \textbf{61.68} \\ \hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\textbf{Comparison with Universal Prompts.} To verify the superiority of learning prompts for each class, we train a universal phase-level prompt and a universal amplitude-level prompt for comparisons. Table~\ref{tab_abla_uni} shows performances under universal prompts are worse than those of ours, indicating that our method helps enhance the robustness.


\textbf{Effectiveness When Blurring the Edges.} Some attacks like DDN tend to disrupt edges of objectives. Therefore, it's natural to question whether robustness gains from our PAP come from edge blurring. To this end, we apply Gaussian Blur on the tested image for evaluations. As shown in Table~\ref{tab_abla_gaussian_blur}, when blurring edges, our method can still achieve superior defenses, indicating the effectiveness of PAP does not come from edge blurring.

\section{Conclusion}
In this paper, we focus on specific semantic patterns for improving prompt-based defenses. It has been proven that phase and amplitude spectra reflect structures and textures, and both of them need to be manipulated for robustness. Therefore, we construct prompts using these spectra, and propose a Phase and Amplitude-aware Prompting (PAP) defense, which learns a phase-level prompt and an amplitude-level prompt for each class. Considering different influences of phase-level and amplitude-level prompts for robustness, we design a weighting method for them according to the robustness under these prompts. To perform testing efficiently, we select prompts according to predicted labels, and design a data-prompt mismatching loss to mitigate the negative effects of mismatches between images and their selected prompts. Experimental results demonstrate our method helps defend against general attacks and adaptive attacks, achieving superior transferability. The limitation of our work is that we sacrifice some natural accuracy when prompting on naturally pre-trained models. We will address it in the future such as using Contrastive Learning \cite{xu2024enhancing}. Overall, our defense explores specific semantic patterns to improve performances of prompt-based defenses.


% \section{Headings: first level}
% \label{sec:headings}

% \lipsum[4] See Section \ref{sec:headings}.




% \subsection{Headings: second level}
% \lipsum[5]
% \begin{equation}
% \xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
% \end{equation}

% \subsubsection{Headings: third level}
% \lipsum[6]

% \paragraph{Paragraph}
% \lipsum[7]

% \section{Examples of citations, figures, tables, references}
% \label{sec:others}
% \lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
  % \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations
% appropriate for use in inline text.  For example,
% \begin{verbatim}
   % \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
  % Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% \begin{center}
  % \url{https://www.ctan.org/pkg/booktabs}
% \end{center}


% \subsection{Figures}
% \lipsum[10] 
% See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
% \lipsum[11] 

% \begin{figure}
  % \centering
  % \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  % \caption{Sample figure caption.}
  % \label{fig:fig1}
% \end{figure}

% \subsection{Tables}
% \lipsum[12]
% See awesome Table~\ref{tab:table}.

% \begin{table}
%  \caption{Sample table title}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:table}
% \end{table}

% \subsection{Lists}
% \begin{itemize}
% \item Lorem ipsum dolor sit amet
% \item consectetur adipiscing elit. 
% \item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
% \end{itemize}


% \section{Conclusion}
% Your conclusion here

% \section*{Acknowledgments}
% This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{templateArxiv}  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Preliminary}
\textbf{Notation.} We use capital letters like $X$ and $Y$ to represent random variables. Correspondingly, lower-case letters such as $x$ and $y$ are presented as the realizations of $X$ and $Y$. We use $\mathbb{B}(x,\epsilon)$ to denote the neighborhood of $x$: $\{\!\tilde{x}\!:\!\Vert x-\tilde{x} \Vert \!\leq\! \epsilon \}$, where $\epsilon$ is the perturbation budget. Here, $\Vert \cdot \Vert$ represents the norm, which can be specified as $L_\infty$-norm $\Vert \cdot \Vert_\infty$ and $L_2$-norm $\Vert \cdot \Vert_2$. We define $f$ : $\chi\! \rightarrow\! \{1,2,...,C\}$ as a classification function, where the $f$ can be parameterized by a deep neural network $h_\theta$ with the parameter $\theta$.
% 优化完毕，防止与以前论文重复

\textbf{Problem Setting.} In this paper, the task we focus on is the classification under adversarial settings, which means target models may be misled by adversarial noises. We sample natural data $\{(x_i,y_i)\}_{i=1}^n$ based on the distribution of $(X,Y)$, where $X$ and $Y$ are the variables of natural instances and their ground-truth labels. Here, $(X,Y) \in \chi \times \{1,2,...,c\}$ and $c$ is the number of classes. Given a deep neural network $h_\theta$ and a pair of natural data $(x,y)$, the adversarial example $\tilde{x}$ is crafted following such a constraint:
\begin{equation}
h_\theta(x) \neq y \quad s.t. \quad \Vert x-\tilde{x} \Vert \leq \epsilon,
\label{eq_supp_problem_set}
\end{equation}
where $\tilde{x}=x+\delta$ and $\delta$ represents the adversarial noises. Since our focus is on attacking and defending for images, we utilize the Discrete Fourier Transform (DFT) and its inverse version (IDFT), denoted as $\mathcal{F}(\cdot)$ and $\mathcal{F}^{-1}(\cdot, \cdot)$, respectively. The phase and amplitude spectra are derived as $\phi_x = \mathcal{F}_\phi(x)$ and $\xi_x = \mathcal{F}_\xi(x)$. Specifically, we use $\phi_x$ and $\xi_x$ to denote the phase and amplitude spectra of a natural image $x$, while $\phi_{\tilde{x}}$ and $\xi_{\tilde{x}}$ represent the corresponding spectra of an adversarial example $\tilde{x}$. In addition, the process to recover an image from its phase and amplitude spectra is expressed as $x = \mathcal{F}^{-1}(\phi_x, \xi_x)$. Our goal is to design a set of prompts to assist the classification model $h_\theta$ in making accurate predictions. These prompts are trained without the need of model retraining, and are further utilized during testing.
% 优化完毕，防止与以前论文重复

\section{Prompt Selection Method for Testing from C-AVP}
\label{sec_sup_cavp_prompt_select}
C-AVP \cite{chen2023visual} aims at utilizing pixel domains for prompting. It trains a prompt for each class, and designs a prompt selection method that traverses all the prompts from all the classes to get the final predictions for testing on naturally pre-trained models especially for CIFAR-10, which can be formulated as:
\begin{equation}
p = p_{i^*}, i^*=argmax_{i \in \mathcal{C}}h_\theta^i(x_{test}+p_i),
\label{eq_cavp_test}
\end{equation}
where $p$ is the selected prompt for the test image $x_{test}$, while $p_{i}$ is the prompt of class $i$ and $h_\theta^i$ is the output of class $i$, and $\mathcal{C}$ denotes the set of classes. Clearly, when the number of class becomes large, this strategy for testing can easily cause extremely high computational costs. Also, when testing using it, C-AVP indeed achieves some robustness, but sacrifices natural accuracy a lot, which can be seen in Table~\ref{tab_main_white_cavptest}.


\section{Experimental Settings}
\label{sec_sup_setting}
\textbf{Datasets and Models.} In this paper, we consider two popular benchmark datasets CIFAR-10 \cite{krizhevsky2009learning} and Tiny-ImageNet \cite{le2015tiny}. CIFAR-10 has 10 classes of images with a resolution of $32\times32$, which contains 50,000 training images and 10,000 testing images. The larger dataset Tiny-ImageNet has 200 classes with a resolution of $64\times64$ and has 100,000 training images, 10,000 validation images and 10,000 testing images. Images in all of these datasets are regarded as natural examples. We normalize all the images into the range of $[0,1]$. Data augmentations including random crop and random horizontal flip are performed for all the data in the training stage. For the target model, we use ResNet18 \cite{he2016deep} and WideResNet28-10 (WRN28-10) \cite{zagoruyko2016wide} for these datasets. We use WRN28-10, VGG19 \cite{simonyan2014very} and a popular vision transformer architecture Swin Transformer \cite{liu2021swin} for evaluating the defense transferability across different models. The Swin Transformer is trained following previous studies about evaluating its defense performances \cite{liu2023exploring}.

\textbf{Attack Settings.}
We introduce white-box attacks and black-box attacks to evaluate the defense. For white-box attack, we utilize $L_\infty$-norm AA \cite{croce2020reliable}, $L_2$-norm C\&W \cite{carlini2017towards} and $L_2$-norm DDN \cite{rony2019decoupling}. The iteration number of $L_2$-norm DDN is set to 20, while that of $L_2$-norm C\&W is 50. The perturbation budget for $L_\infty$-norm AA is $8/255$. For $L_\infty$-norm C\&W, the learning rate is 0.01 and the confidence is 0. All the attacks mentioned above are set as non-targeted attacks. For black-box attacks, we apply transfer-based attacks under $L_\infty$-norm AA, $L_2$-norm DDN using VGG19 as the surrogate model and query-based attacks under Square \cite{andriushchenko2020square}. The number of queries for Square is set to 200.

\textbf{Defense Settings.} We introduce two recently proposed prompt-based defenses C-AVP \cite{chen2023visual} and Freq \cite{huang2023improving} as baselines, which utilize the pixel domain and the frequency domain for prompting respectively. In addition, for the pre-trained models for optimizing and evaluating our prompts, we introduce natural training, AT \cite{madry2017towards}, TRADES \cite{zhang2019theoretically} and MART \cite{wang2019improving}. Note that all the pre-trained models are fixed without participating in any prompt training procedure. For the attack during training, we use PGD, where the perturbation budget and perturb step are 8/255 and 10, and the step size is 2/255. We train them using SGD \cite{andrew2007scalable} for 100 epochs. The initial learning rate is 0.1 with batch size 512 for CIFAR-10, and batch size 256 for Tiny-ImageNet. The initial learning rate is divided by 10 at the 75-th epoch. We set $\lambda_1=3$, $\lambda_2=400$, $\lambda_3=4$ for naturally pre-trained models, and $\lambda_1=1$, $\lambda_2=5000$, $\lambda_3=4$ for adversarially pre-trained models.



\section{Defending against Black-box Attacks}
\label{sec_sup_black}
We perform black-box attacks under transfer-based attacks and query-based attacks. The results are shown in Table~\ref{tab_supp_black}. It is shown that our method achieve superior robust performances under black-box settings compared with baselines.

\begin{table*}[h]
\caption{Robust accuracy (percentage) of defenses against black-box attacks on CIFAR-10. The target model is ResNet18, and the surrogate model is VGG19. we perform AA and DDN as the transfer-based attack strategies.}
\label{tab_supp_black}
\renewcommand\tabcolsep{16pt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cccc}
\hline
Defense & None  & AA & DDN& Square \\ \hline
NAT     & \textbf{94.83} & 16.91& 53.46 & 22.10  \\
\hspace{0.2cm}+Freq   & 94.50 & 19.37& 53.85 & 26.43  \\
\hspace{0.2cm}+C-AVP  & 92.67 & 17.27& 52.87 & 22.64  \\
\hspace{0.2cm}+PAP(Ours)    & 87.12 & \textbf{51.90}& \textbf{74.79} & \textbf{61.38}  \\ \hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\section{Defense Transferability to Vision Transformers}
\label{sec_sup_transfer_vit}
We further transfer our prompts trained on ResNet18 to popular Swin Transformer for evaluating the defense transferability of our method. As shown in Table~\ref{tab_supp_transfer_vit}, our method can be transferred well to vision transformers for improving their robustness, verifying the superior transferability across both convolutional neural networks and vision transformers.

\begin{table}[h]
\caption{Robust accuracy (percentage) of our prompts transferred to vision transformers. The prompts are trained on ResNet18, and the vision transformer we introduced is Swin Transformer.}
\label{tab_supp_transfer_vit}
\renewcommand\tabcolsep{16pt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cccc}
\hline
Defense & None  & AA    & C\&W    & DDN   \\ \hline
NAT     & \textbf{88.98} & 0.00  & 0.00  & 0.00  \\
\hspace{0.2cm}+Freq   & 82.77 & 3.47  & 27.55 & 15.50 \\
\hspace{0.2cm}+C-AVP  & 30.33 & 9.00  & 17.28 & 12.90 \\
\hspace{0.2cm}+PAP(Ours)    & 84.82 & \textbf{10.13} & \textbf{77.97} & \textbf{49.15} \\ \hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\section{Hyper-parameter Studies}
\label{sec_sup_hyper}
We perform several ablation studies for losses with different hyper-parameters as follows. For each hyper-parameter, it varies within a certain range while other hyper-paremeters are fixed. It can be seen that the natural and robust accuracies vary under different settings, and the hyper-parameters we set can achieve superior performances in both natural accuracy and robust accuracy.

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{fig_nat_hyper}}
\caption{The impact of losses with different hyper-parameters on naturally pre-trained ResNet18 in CIFAR-10. For each hyper-parameter, it varies within a certain range while other hyper-parameters are fixed. We show the natural accuracy and robust accuracy against AA.}
\label{fig_nat_hyper}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{fig_at_hyper}}
\caption{The impact of losses with different hyper-parameters on adversarially pre-trained ResNet18 in CIFAR-10. For each hyper-parameter, it varies within a certain range while other hyper-parameters are fixed. We show the natural accuracy and robust accuracy against AA.}
\label{fig_at_hyper}
\end{center}
\vskip -0.2in
\end{figure}




\section{Visualizations of Prompted Images}
We present additional visualized results of prompted images using our prompts, which are presented as follows. It can be seen that our method retains complete and natural semantic patterns after prompting.

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{fig_sup_prompted_cifar10_natres18_pgd}}
\caption{Visualizations of prompted images for input examples on CIFAR-10. The target model is naturally pre-trained ResNet18. For each pair of images, the left part denotes the prompted image, while the right part denotes the difference heatmap compared to the original input (\textit{i.e.}, adversarial) example.}
\end{center}
\vskip -0.2in
\end{figure}

% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=1\columnwidth]{fig_sup_prompted_cifar10_ATres18_pgd}}
% \caption{Visualization of prompted images for adversarial examples using PGD on \textit{CIFAR-10}. The target model is adversarially pre-trained ResNet18. For each pair of images, the left part denotes the prompted image, while the right part denotes the difference heatmap compared to the original image.}
% \end{center}
% \vskip -0.2in
% \end{figure}


\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{fig_sup_prompted_tiny_natwrn_pgd}}
\caption{Visualizations of prompted images for input examples on Tiny-ImageNet. The target model is naturally pre-trained WRN28-10. For each pair of images, the left part denotes the prompted image, while the right part denotes the difference heatmap compared to the original input (\textit{i.e.}, adversarial) example.}
\end{center}
\vskip -0.2in
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}
