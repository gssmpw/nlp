\section{Related Work}
\subsection{Indoor Light Estimation}
Directly estimating indoor light from an indoor image is an ill-posed problem. Under natural illumination, the indoor spatially-varying light depends on the outdoor context,  scene geometry, and material properties. 
Given a 2D perspective image, some studies focus on predicting panoramic HDR environment maps (\cite{gardner2017learning}), lighting representation (\cite{gardner2019deep}), and estimating HDR panoramas from LDR images (\cite{gkitsas2020deep,legendre2019deeplight,gardner2017learning}). Extensive research efforts have been devoted to estimating HDR from LDR images using inverse tone mapping algorithms (\cite{rempel2007ldr2hdr,banterle2006inverse,banterle2007framework,reinhard2002photographic}). The estimated HDR panorama is used as a global light source for high-quality relighting and inserting new virtual 3D objects into the scene. Previous studies have also explored indoor lighting editing (\cite{li2022physically}), material property estimation (\cite{yeh2022photoscene}), and recovery of spatially-varying lighting (\cite{li2020inverse,garon2019fast,srinivasan2020lighthouse}) from images. User inputs provide additional information to define scene geometry and light location and direction (\cite{karsch2011rendering}). Through user-annotated light positions, \cite{zhang2016emptying} recover indoor illumination and room geometry from a complete RGB-D scan of a scene.

Since the input 2D perspective image is in LDR format, while the estimated HDR environment map from previous work serves as a global light source, it often fails to recover the accurate scene radiance of the real world. More importantly, earlier studies that directly estimate environment maps from images do not construct a complete light transport model. Such a model would ideally account for outdoor lighting, scene geometry, and material properties and present a complete interaction between light and the environment.


\subsection{Outdoor Light Estimation}
Outdoor light changes with the appearance of the sky, the surrounding context, and the time of day. The luminance and angular distribution of the sky also vary with cloud levels, times of year, and geographic locations. Perez sky model (\cite{perez1993all}) and the CIE standard sky model (\cite{cie2003spitial}) are commonly used to estimate outdoor light. Due to the nuanced difference between cloud cover and sunlight intensity, generalized sky models cannot fully describe the realistic sky appearance (\cite{darula2002cie}). Recent studies have focused on estimating the appearance of the spectral sky. A spectral sky represents chromatic changes based on time, sky turbidity, and geographic locations (\cite{hosek2012analytic}). Furthermore, analytical sky models (\cite{lalonde2012estimating,preetham1999practical}) approximate full-spectrum daylight under various atmospheric conditions.  

In addition to generating sky model images, some studies aim to estimate outdoor lighting directly from 2D images. Learning-based method estimates 360$^{\circ}$ outdoor environment map from a single 2D photograph (\cite{hold2017deep,lalonde2014lighting}). \cite{lalonde2014lighting} developed a method to estimate light direction and intensity from a single Low Dynamic Range (LDR) image, and the generated HDR sky models are used for relighting new objects. \cite{zhang2017learning} recover linear HDR outdoor images from a single LDR panorama. \cite{sunkavalli2008color} recover time-varying color variation for the outdoor scene under direct sunlight and ambient skylight. The recent study by \cite{dastjerdi2023everlight} directly adds, removes, and edits the indoor and outdoor illumination sources in the panoramic representation. However, those studies focus on estimating outdoor environment maps for relighting new virtual objects; the outdoor light source is not used to estimate indoor light transport.

\subsection{Thermal Imaging Technique}
Above absolute zero temperature, indoor objects radiate infrared energy in the Long-Wave Infrared (LWIR) spectrum (wavelengths between 8 $\mu\text{m}$ and 15 $\mu\text{m}$), resulting in heat on the object's surface. The radiometric quantity of the heat transfer can be measured by infrared thermal imaging technique (\cite{holst2000common,kaplan2007practical,vollmer2020infrared,planinsic2011infrared}).  
Thermal imaging has gained increasing attention in computer vision research and leads to applications based on the thermal spectrum, such as object segmentation (\cite{huo2023glass}), 3D estimation (\cite{eren2009scanning,shin2023deep,narayanan2025shape}), and material property estimation (\cite{dashpute2023thermal}).

Thermography has been extensively used to collect surface heat in building spaces. In outdoor applications, recent studies have focused on identifying thermal leakage on exterior facades (\cite{motayyeb2023fusion}) and integrating thermal attributes with 3D point clouds (\cite{jarzkabek2020supervised}). In indoor scenarios, recent research has combined 3D point clouds with 2D thermography for generating 3D building models (\cite{adan2017fusion,schmoll2022method}), detecting faults on interior surfaces (\cite{garrido2018autonomous}), and segmenting targeted thermal regions on building surfaces (\cite{adan2020temporal}). Additionally, heat maps can be captured by a thermal-infrared camera from multiple indoor locations to analyze the heat generated on vertical wall planes (\cite{lopez2017thermographic}), support 3D modeling (\cite{adan2017fusion}), and combine with RGB-D camera for spatial mapping (\cite{vidas20133d}).
 
Existing studies on indoor thermography primarily focus on capturing temporal heat maps at specific time points. There are several limitations. First, 2D thermography captures only a limited field of view. Second, capturing multiple 2D perspective images requires additional effort in data post-processing, such as image registration and alignment. Third, implementing infrared thermal cameras with additional devices like RGB-D cameras or LiDAR adds complexity and increases data acquisition costs.