\appendix
\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thetable}{\Alph{table}}

\section{Implementation details}

\medskip\noindent{\textbf{Collection process}}. 
Queries and positives are created/collected by a group of 16 collectors who are well-informed about the task objectives. 
Most of the images consist of photographs taken by the collectors for the purpose of this work, while a smaller part is downloaded from online repositories with a permissive license.
All collected images are manually filtered and curated by the authors. Regarding the selection of the objects, the collectors are advised to opt for objects with distinct, uncommon features--such as unique shapes, colors, or textures--that set them apart within their category, \ie prioritize items with rare modifications. 
As mentioned in the main paper, objects that are created or share parts with other objects created before 2014 do not qualify as query objects. Fig.~\ref{fig:rejected} illustrates some of the objects rejected during the selection process. Fig.~\ref{fig:rej_building} is the Kuggen building, whose construction finished in 2011. Fig.~\ref{fig:rej_coaster} is a newly bought coaster that displays a well-known van Gogh painting. Fig.~\ref{fig:rej_cutlery} is a newly bought cutlery holder whose design is rather generic with no distinctive detail; hence, very similar (close to identical) objects may exist in YFCC100M. 
Furthermore, the collectors are provided with older camera models used in YFCC100M. This simulates similar camera distribution for the query and positive images with the distractors. Tab.~\ref{tab:cameras} shows the distribution of the most used cameras. Older-generation cameras are used for the majority of the collected images.
The collectors are instructed to avoid using the same camera for both the query and the positives of an object to avoid any possible shortcuts learned by pre-trained models.

\medskip\noindent\textbf{Downloading and storing images.}
To acquire the YFCC100M~\cite{tsf+16}, we download images based on the Flickr URLs provided by the original authors.
Approximately $\sim$82M images are downloaded. The remaining images are downloaded from the AWS S3 data bucket provided by the authors.
We opt for downloading the images from Flickr to ensure that identical preprocessing has been applied to the distractor dataset and the collected query and positive sets in \ours.
The collected images in \ours are also uploaded to and downloaded from Flickr. We use the ``medium'' option to download all images, which resizes images to 500px based on their larger side.
All images are stored with 90 JPEG compression quality with 4:4:4 chroma subsampling. Following~\cite{gonzalez09,ab19}, white balancing is applied on all images.
All personal details (\eg human faces, license plates) that are displayed in the collected images of \ours are either blurred or cropped.

\begin{figure}[t]
    \centering
    \input{fig/rejected}
    \vspace{-5pt}
    \caption{\textbf{Rejected objects.} Example of objects that are disregarded during the selection process.
    \label{fig:rejected}
    \vspace{5pt}
    }
\end{figure}

\begin{table}[t]
  \centering
  \scalebox{0.99}{
    \input{tab/cameras}
  }
  \vspace{-5pt}
  \caption{\textbf{Most frequently used camera models in \ours.} Cameras used for more than 100 images are displayed. Information about release date and type of camera is provided.
  \label{tab:cameras}
  \vspace{-5pt}
  }
\end{table}

\medskip\noindent\textbf{\miniours composition.} We consider the 88 text category labels from \ours taxonomy to generate text queries, manually expanded with 132 terms that are synonyms or fine-grained descriptions of the original labels. The collected labels are combined with 43 templates used in the original CLIP~\cite{clip} to generate a list of 9,976 text queries. Examples of the templates used are in Tab.~\ref{tab:templates}. We do not consider domain-specific templates. We use the large model variants of SigLIP, OpenCLIP, and EVA-CLIP to compute the ensemble text-image similarities between the text queries and each image of YFCC100M.

\medskip\noindent\textbf{Text query generation.} 
We generate text queries using the GPT-4o~\cite{chatgpt}. The prompt displayed in Fig.~\ref{fig:chagpt_prompt} is first provided to the LLM. 
Then, a query image of one of the objects in \ours and its corresponding category is provided to the model to generate a textual description. For object category, we use mid level category from taxonomy. If it is not available, we use the coarser level category. The generated text queries are manually edited by the authors to fix errors, insufficient descriptions, or nuances of the model.

\begin{table}[t]
    \centering
    \begin{tabular}{l}
        \texttt{a close-up photo of the *.}\\
        \texttt{a good photo of the *.}\\
        \texttt{a photo of a cool *.}\\
        \texttt{a low resolution photo of the *.}\\
        \texttt{a bad photo of the *.}\\
        \texttt{a cropped photo of the *.}\\
        \texttt{a photo of a hard to see *.}\\
        \texttt{a bright photo of a *.}\\
        \texttt{a photo of a clean *.}\\
        \texttt{a photo of a dirty *.}\\
        \texttt{a dark photo of the *.}\\
        \texttt{a photo of my *.}\\
        \texttt{a photo of the cool *.}\\
        \texttt{a close-up photo of a *.}\\
        \texttt{a bright photo of the *.}
    \end{tabular}
    \vspace{-6pt}
    \caption{\textbf{Examples of templates} used for the text query generation for the creation of \miniours. The \texttt{*} symbol is replaced with a taxonomy term.
    \label{tab:templates}
    \vspace{-9pt}
    }
\end{table}

\medskip\noindent\textbf{Global representations.}
For the implementation of global representation models, we rely on public resources available on PyTorch~\cite{pgm+19}. We use the timm\footnote{\rurl{github.com/rwightman/pytorch-image-models}} and torchvision\footnote{\rurl{github.com/pytorch/vision}} libraries that provide relevant code and weights for the majority of the models. For the models not included there, we use the relevant code from the official github repositories provided by the authors, \ie
\cite{odm+24}\footnote{\rurl{github.com/facebookresearch/dinov2}},
\cite{ctm+21}\footnote{\rurl{github.com/facebookresearch/dino}},
\cite{cmm+20}\footnote{\rurl{github.com/facebookresearch/swav}},
\cite{hfw+20}\footnote{\rurl{github.com/facebookresearch/moco-v3}},
\cite{ptm+22}\footnote{\rurl{github.com/yash0307/recallatk_surrogate}},
\cite{kjk23}\footnote{\rurl{github.com/tjddus9597/hier-cvpr23}},
\cite{lsl+22}\footnote{\rurl{github.com/sungonce/cvnet}},
\cite{sck+23}\footnote{\rurl{github.com/shihaoshao-gh/superglobal}},
\cite{ady+23}\footnote{\rurl{github.com/deepglint/unicom}},
\cite{swl+24}\footnote{\rurl{github.com/naver/unic}},
\cite{ycc+23}\footnote{\rurl{github.com/nikosips/universal-image-embeddings}},
\cite{yca+24}\footnote{\rurl{github.com/nikosips/udon}}. Model weights that are not publicly available are provided to us by the original authors. 
For t2i, we use the image encoders from timm and the text encoders from huggingface\footnote{\rurl{huggingface.co}} and OpenCLIP\footnote{\rurl{github.com/mlfoundations/open_clip}}. We include only base and large model variants in our benchmark. Tab.~\ref{tab:supp_all_models} and~\ref{tab:supp_all_text_models} contain more information, including model checkpoints. Regarding image preprocessing, following instance-level retrieval literature~\cite{sck+23,rtc19,lsl+22}, the images are resized based on their largest side respecting their aspect ratio, \ie isotropic rescaling. Image resolution is dictated by each model's specifications together with the rule setting resolution one level higher than those used during training. This rule is empirically created based on experiments presented in Sec.~\ref{sec:addan}. We normalize the image tensors with the mean and standard deviation statistics according to model specifications. For all ViT-based models, bicubic interpolation of the position embeddings is performed. Unicom~\cite{ady+23} requires fixed-size tensors in the backbone output, which goes through a projection head; hence, we use adaptive average pooling to fix the spatial dimensions of the output feature tensors. For UDON~\cite{yca+24} and USCRR~\cite{ycc+23} models, we use the representation before projection due to the low dimensionality of the latter. For AlexNet~\cite{ksh12} and VGG~\cite{sz14} models, we extract descriptors based on the feature maps of the last convolutional layer by applying GeM pooling~\cite{rtc19}. For the rest of the models, the extraction process used in the original methods is employed. All global descriptors are \l2 normalized.

\begin{figure}[t]
    \centering
    \begin{tcolorbox}[colback=gray!15, colframe=black, width=\linewidth]
        \hspace{-8pt}
        \begin{tabular}{p{1.03\linewidth}}
            {\scriptsize \texttt{You are a system generating descriptions of objects shown in an image.}} \\
            {\scriptsize \texttt{Provided with an image and a category in which the item shown in the image belongs to, you will describe the main item that you see in the image, giving enough details to unambiguously describe the object.}} \\
            {\scriptsize \texttt{You can describe unambiguously what the item is and its material, color, and style if clearly identifiable.}} \\
            {\scriptsize \texttt{Please do not describe anything about the background.}}
        \end{tabular}
    \end{tcolorbox}
    \vspace{-10pt}
    \caption{\textbf{Prompt} used for the initial generation of text queries.
    \label{fig:chagpt_prompt}
    \vspace{-6pt}
    }
\end{figure}

\noindent\textbf{Linear adaptation.}
The single linear adaptation layer is trained on a 1M random subset of UnED~\cite{ycc+23}.
The training follows the UJCDS~\cite{ycc+23} method that learns a linear classifier on all classes in the UnED subset (191,513 classes). The classifier gets the \l2 normalized features output from the linear adaptation layer.
During training, the Normalized Softmax loss~\cite{zw18} is minimized, and no balancing across UnED domains is performed.
The linear layer and classifier are trained for 2 epochs with 128 batch size. We use Adam~\cite{kb15} optimizer with 10\textsuperscript{-3} learning rate and 10\textsuperscript{-6} weight decay. The scale of the Normalized Softmax loss is 16.

\begin{figure}[t]
    \centering
    \input{fig/fig_number_of_distractors}
    \vspace{-10pt}
    \caption{\textbf{Impact of the number of distractors.} mAP@1k of five models for varying db size. $\dagger$ indicates results with the linear adaptation. $b$ and $l$: base and large model variants.
    \label{fig:number_of_distractors}
    \vspace{-13pt}
    }
\end{figure}

\noindent\textbf{Local representations.}
Following AMES~\cite{ski+24}, local descriptors are extracted based on the base variant of DINOv2 with registers~\cite{odm+24,doj+23}. Local descriptors are selected based on their weights estimated by a feature detector~\cite{cas20}. We use the pre-trained network trained on the corresponding descriptors. The local descriptor extraction, the pre-trained models, and inference configurations are publicly-available\footnote{\rurl{github.com/pavelsuma/ames}}. To ensure a fair comparison between re-ranking methods, we use the same local descriptors for other methods but with  different binarization. AMES consists of a binarization layer initialized with ITQ~\cite{glg+12,ktp+22} and finetuned during model training. Hence, for Chamfer Similarity (CS)~\cite{rsa+14} and Spatial verification (SP)~\cite{pci+07}, the descriptors are binarized with the same ITQ weights.
For SP, we follow the standard practice in retrieval with fast spatial matching~\cite{cas20} and use single correspondence hypotheses, which is translation in our case, and LO-RANSAC~\cite{cmk03} for affine-transformation. Due to the single scale local descriptors, departing from the single correspondence hypothesis and sampling correspondence pairs or triplets can potentially provide better results despite being slower.
Tentative inlier correspondences are extracted based on the nearest neighbor of each query local descriptor, using a threshold of 32 Hamming distance. Local similarity for re-ranking is estimated based on the number of inliers detected by RANSAC, with a minimum threshold of 5 inliers.
Also, the final AMES similarity is an ensemble of local and global similarity. For a fair comparison, the same ensembling scheme is also used for CS and SP, following the same validation process. The ensemble hyper-parameters are tuned on the public split of the GLDv2 dataset.
In the default settings, \ie 100 binarized local descriptors for db images, the total memory requirements for storing local descriptors is $\sim$149GB, which is to be compared with $\sim$95GB needed for 512D global descriptors stored in half-precision.
Note that we do not consider compression techniques for the global descriptors, which can decrease the memory footprint by an order of magnitude with an insignificant performance loss~\cite{gar+17, ski+24}.

\section{Additional experiments}
Similar to the main paper, unless stated otherwise, we use the large ViT model variants with the largest resolution available, \eg we use SigLIP ViT-L trained with 384 resolution. In the case of various architectures for the same method, we use the best-performing one, \eg we use the large variant of ConvNext architecture for OpenCLIP.

\subsection{Additional analysis}
\label{sec:addan}

\begin{table}[t]
  \centering
  \scalebox{0.95}{
  \hspace{-7pt}
  \input{tab/resolution}}
  \vspace{-5pt}
  \caption{\textbf{Impact of resolution.} Performance (mAP@1k) by testing at different resolutions. The underline indicates the resolution selected for each model based on our rule. Linear adaptation is not used. Top: base models. Bottom: large models.
  \label{tab:resolution}
  \vspace{-10pt}
  }
\end{table}

\begin{figure*}[t]
  \centering
  \vspace{-15pt}
  \scalebox{0.9}{
    \hspace{10pt}
    \input{fig/instre_ilias_adapt}
    \hspace{10pt}
    \input{fig/gld_ilias_adapt} 
    \hspace{10pt}
    \input{fig/sop_ilias_adapt} 
  }
  \vspace{-10pt}
  \caption{\textbf{Comparison with other instance-level retrieval datasets} via reporting mAP@1k. Results with linear adaptation. INSTRE: 27.3K db size, multi-domain. GLDv2: 762K db size, single-domain. SOP:  60.5K db size, single-domain. Different network types are color-coded. For GLDv2 and SOP, models fine-tuned on these domains with the corresponding training sets are highlighted.
  \label{fig:instre_gld_sop_adapt}
  \vspace{-15pt}
  }
\end{figure*}

\noindent{\textbf{Impact of number of distractors}}.
Fig.~\ref{fig:number_of_distractors} presents the performance of five models under varying numbers of distractors. Performance declines as more distractors are added; however, significantly increasing the dataset's difficulty requires an exponential growth in the number of distractors. Notably, the ranking of models changes considerably when comparing performance with no distractors to that with 100M distractors. For example, DINOv2 demonstrates strong robustness to distractor increases, ranking last with no distractors but surpassing two models at 100M distractors and reaching others. Also, several crossings between models are observed. Therefore, evaluation at a large scale, provided by \ours, is important.

\noindent\textbf{Impact of image resolution.}
In Tab.~\ref{tab:resolution}, we investigate the impact of resolution and validate the rule of using as test resolution one up from the training one. Linear adaptation is not used in this experiment. It is clear that the vast majority of models achieve the best performance following the imposed rule; test at a resolution one level larger than the training resolution. Interestingly, SigLIP collapses when used with a resolution much lower than the training one. 

\noindent{\textbf{Impact of background clutter}}. To quantify the impact of background clutter, we experiment with masking out areas outside object bounding boxes in the positives during descriptor extraction. This approach improves SigLIP$^\dagger$ performance from 28.9 to 62.4.
Fig.~\ref{fig:heatmap_db_rank_vs_clutter} also presents the impact of clutter, \ie number of segments detected by SAM in a positive image outside of the object bounding box, on the ranking of this positive.
This experiment provides insight about the type of positives, according to clutter, that populate the top and bottom ranks.
Positives with less clutter, \ie low number of segments, are the most common in the higher ranks; while, positives with more clutter, \ie high number of segments, are the most common in the lower ranks.

\noindent{\textbf{Impact of object scale}}. Flowing the same strategy as above, but object bounding boxes are cropped and rescaled instead of masking, performance further improves to 69.4. However, although this does not reflect solely the impact of scale changes due to potential partial views and drastic viewpoint changes, it still gives a good insight into the limitations of the current models regarding scale changes.
Fig.~\ref{fig:heatmap_db_rank_vs_scale} presents the impact of relative scale, \ie percentage of the bounding box area within the image area, on the ranking of positive. This experiment provides insight into the type of positives, according to relative image coverage, that populate the corresponding rank ranges. Positives where the object covers a large area are the most common in the higher ranks; while, positives with a small area coverage are the most common in lower.

\begin{figure}[t]
    \centering
    \scalebox{0.82}{
    \begin{tabular}{cc}
        \hspace{-10pt}
        \begin{subfigure}{0.28\textwidth}
            \centering
            \input{fig/fig_heatmap_db_rank_vs_clutter}
            \vspace{-12pt}
            \caption{\label{fig:heatmap_db_rank_vs_clutter}}
        \end{subfigure}
    &
        \hspace{-5pt}
        \begin{subfigure}{0.28\textwidth}
            \centering
            \input{fig/fig_heatmap_db_rank_vs_scale} 
            \vspace{-12pt}
            \caption{\label{fig:heatmap_db_rank_vs_scale}}
        \end{subfigure}
    \end{tabular}
    }
    \vspace{-10pt}
    \caption{\textbf{Impact of clutter and area coverage.} Percentage of images per ranking range based on SigLIP$^\dagger$ and grouped based on (a) clutter, \ie number of segments detected by SAM, (b) scale, \ie area of object bounding box in images. Column bins contain the same number of positives. Normalization per row is applied.
    \label{fig:heatmaps_scale}
    \vspace{-6pt}
    }
\end{figure}

\begin{figure*}[t]
    \centering
    \scalebox{1.}{
       \input{fig/fig_performance_per_domain}
    }
    \vspace{-11pt}
    \caption{\textbf{Performance comparison per primary category.} mAP@1k  averaged over objects in the same primary-level category size, sorted by SigLIP$^\dagger$+AMES performance. Comparison between SigLIP with and without adaptation, SigLIP combined with AMES reranking, SigLIP t2i, and DINOv2. $\dagger$ indicates results with the linear adaptation.
    \label{fig:performance_barplot}
    \vspace{-10pt}
    }
\end{figure*}

\noindent{\textbf{Multi-scale and multi-rotation extraction}}. 
A common approach to address scale variation is multi-scale feature extraction, as widely adopted in the literature~\cite{rtc19,sck+23}.
Applying multi-scale extraction asymmetrically, \ie only on the queries, yields an average 0.4 performance improvement across benchmarked models. SigLIP is marginally improved by 0.1. Multi-rotation is also tested in a similar manner, which, however, leads to an average drop of 0.3. Yet, SigLIP is marginally improved by 0.2.

\noindent{\textbf{Comparison with other datasets using linear adaptation}}. Fig.~\ref{fig:instre_gld_sop_adapt} presents the performance of global representation models with linear adaptation. Similar conclusions derive as in the case without adaptation. Only SigLIP achieves a competitive performance in SOP datasets out of the models not trained in-domain.

\noindent{\textbf{Performance per domain}}. Fig.~\ref{fig:performance_barplot} shows the average performance of objects grouped based on coarser taxonomy level.

\noindent{\textbf{Qualitative examples}}. Fig.~\ref{fig:fig_dataset_supplementary_i2i} and \ref{fig:fig_dataset_supplementary_t2i} show examples of retrieved images based on i2i and t2i retrieval, respectively.


\subsection{Linear adaptation}

\noindent\textbf{Comparison with other approaches.}
Tab.~\ref{tab:adaptation} compares the proposed linear adaptation with other linear projection methods trained on UnED for three models. 
All methods project the off-the-shelf descriptors to 512D ones.
The unsupervised PCA whitening (PCA$_w$)~\cite{jc12} and the supervised learnable whitening (L$_w$)~\cite{rtc19} approaches are evaluated. 
The proposed linear adaptation scheme achieves the best performance, typically with a large margin. It is the only one that does not drop off-the-shelf DINOv2 performance.

\begin{table}[t]
  \centering
  \input{tab/adaptation}
  \vspace{-7pt}
  \caption{\textbf{Performance comparison for linear adaptation via mAP@1k.} Label requirement is indicated. Performance before adaptation is provided for reference.
  \vspace{-7pt}
  \label{tab:adaptation}}
\end{table}

\noindent\textbf{Impact of multi-domain linear adaptation.}
Tab.~\ref{tab:uned_subdomains} illustrates the performance of several models with linear adaptation trained on the four largest single-domain datasets of UnED, as well as the entire UnED. Training on a single domain typically increases the performance of VLMs, except in the case of Met, where performance drops dramatically. DINOv2 performance decreases consistently with single-domain training. Nevertheless, the margin with multi-domain training is significant, indicating that multi-domain training on the whole UnED is best suited for \ours.

\begin{table}[t]
  \centering
  \scalebox{0.87}{
    \input{tab/uned_subdomains}
  }
  \vspace{-5pt}
  \caption{\textbf{Performance comparison of single- and multi-domain linear adaptation.} mAP@1k of models with linear adaptation trained on different dataset setups based on UnED. Performance before adaptation is provided for reference.
  \vspace{-5pt}
  \label{tab:uned_subdomains}}
\end{table}

\noindent\textbf{Impact of descriptor dimensionality.}
Fig.~\ref{fig:descriptor_dimensionality} illustrates the performance of five models linearly adapted on UnED with varying descriptor dimensionalities. For all models, performance saturates at a descriptor dimensionality of 256D, with only marginal improvements observed for most models beyond this point.


\begin{figure}[t]
    \centering
    \vspace{-12pt}
    \input{fig/fig_descriptor_dimensionality}
    \vspace{-11pt}
    \caption{\textbf{Impact of descriptor dimensionality.} mAP@1k of five models with the linear adaptation of various dimensionalities.
    \label{fig:descriptor_dimensionality}
    \vspace{-15pt}
    }
\end{figure}

\noindent\textbf{Robustness.} 
We conduct three independent runs using different random seeds to evaluate the robustness of the linear adaptation. 
Across five global descriptors, the proposed scheme exhibits strong robustness, with a maximum standard deviation of 0.2 and a minimum of 0 across runs.


\subsection{Re-ranking with local representations}

\noindent\textbf{Impact of top-M re-ranked images and number of local descriptors.}
Fig.~\ref{fig:ames_num_desc} illustrates the performance of SigLIP with re-ranking when an increasing number of re-ranked images and local descriptors, translated to memory per image, are used. Performance increases as both variables increase. In the default scenario of top-1k and 100 descriptors, the performance is 35.6, which requires 0.6sec per query and approximately 150GB of memory. In an unconstrained scenario, the top performance is 38.8, requiring 20sec and almost 900GB.

\noindent\textbf{Combination with various global representations.}
Tab.~\ref{tab:ames_models} presents the performance with and without re-ranking on \ours and \miniours using various models for global representation. mAP@1k is improved by more than 6 when re-ranking is applied for all models and datasets.

\begin{figure}[t]
    \centering
    \input{fig/ames_num_desc}
    \vspace{-7pt}
    \caption{\textbf{Impact of the re-ranking shortlist size and required memory for local descriptors.} Text above each point denotes the number of local descriptors per DB image. The shortlist size is indicated in the legend. Results are with the linear adaptation.
    \label{fig:ames_num_desc}
    \vspace{-7pt}
    }
\end{figure}

\begin{table}[t]

  \centering
  \input{tab/ames_models}
  \vspace{-7pt}
  \caption{\textbf{Re-ranking on top of different global representations.} mAP@1k and oracle re-ranking on \ours and \miniours. + indicates re-ranking with AMES. $\dagger$ indicates results with the linear adaptation.
  \label{tab:ames_models}
  \vspace{-7pt}
  }
\end{table}

\begin{figure}[t]
  \centering
  \scalebox{0.55}{
    \input{fig/fig_heatmap_spatial_location}
  }
  \caption{\textbf{Distribution of object bounding boxes in positives.}
  \label{fig:heatmap_spatial_location}
  \vspace{-10pt}
  }
\end{figure}

\begin{figure}[t]
  \centering
  \vspace{10pt}
  \input{fig/ames_rerank}
  \vspace{-15pt}
  \caption{\textbf{Re-ranking with AMES.} Queries with the most significant AP increase from re-ranking. The number of negatives ranked above positives is reported on top, as before $\rightarrow$ after re-ranking.
  \label{fig:ames_rerank}
  }
\end{figure}

\noindent\textbf{Qualitative examples.} Fig.~\ref{fig:ames_rerank} presents some queries with the largest AP improvement from re-ranking with AMES. Several cases of severe clutter, scale changes, and partial views are successfully retrieved with re-ranking.




\section{Dataset extras}

\noindent{\textbf{Spatial location of objects in positives}}. 
Fig.~\ref{fig:heatmap_spatial_location} illustrates the spatial location of the object in the positives. Center bias in \ours is much less prominent in comparison with INSTRE~\cite{wj15} dataset. 

\noindent{\textbf{Taxonomy}}. Fig.~\ref{fig:taxonomy} illustrate the defined categories for the three taxonomy levels.

\noindent{\textbf{Query and positive examples}}. Fig.~\ref{fig:fig_dataset_supplementary_obj} provides visual examples of the collected queries and positives of several query objects.

\noindent{\textbf{Benchmarked models}}. Tab.~\ref{tab:supp_all_models} and \ref{tab:supp_all_text_models} provide details and performance on \ours and \miniours of all models.

\section{Dataset hosting, sharing and license}
\ours is hosted in our servers in its entirety (\ie collected images and the downloaded YFCC100M) to assert its long-term availability to the broader public. All collected images are shared under the permissive CC-BY 4.0 license. The downloaded images are distributed under their original license. All collectors have signed a consent form for the distribution of their images under this license.

\newpage

\begin{table*}[t]
  \centering
  \scalebox{0.67}{
    \input{tab/supp_all_models}
  }
  \caption{\textbf{Benchmarked model details and mAP@1k on \ours and \miniours for global representation models for i2i}. Model details include the year of publication, repository used, architecture (arch), model descriptor dimensions (dims), training scheme (train), training data, and train/test resolution. 5M and 100M correspond to the mini and full versions of the dataset, respectively. For fine-tuned models, only the fine-tuning dataset is considered. Repo indicates the framework used to acquire model weights, \ie torchvision, timm, or official github. $*$ indicates non-publicly available models provided by the original author. $\dagger$ indicates results with the linear adaptation. sup, ssl, dist, vla: supervised learning, self-supervised learning, distillation, vision-language alignment. R50, R101, CN: ResNet50, ResNet101 and ConvNext.
  \label{tab:supp_all_models}
  }
\end{table*}

\newpage

\begin{table*}[t]
  \centering
  \scalebox{0.7}{
    \input{tab/supp_all_text_models}
  }
  \caption{\textbf{Benchmarked model details and mAP@1k on \ours and \miniours for global representation models for t2i}. Model details include the year of publication, repository used, architecture (arch), model descriptor dimensions (dims), training data, and train/test resolution. 5M and 100M correspond to the mini and full versions of the dataset, respectively. Repo indicates the framework used to acquire model weights, \ie timm for the image encoders and huggingface (hf) or OpenCLIP (oc) for the text encoders. R50, CN: ResNet50 and ConvNext.
  \label{tab:supp_all_text_models}
  }
\end{table*}

\newpage

\begin{figure*}[p]
    \centering
    \scalebox{1.8}{
        \input{fig/fig_taxonomy}
    }
    \caption{\textbf{The \ours taxonomy} with a 3 level hierarchy.
     The number of objects is displayed for categories with more than 5 objects.
    The taxonomy is used to summarize the objects' diversity and distribution and to report performance per category without affecting the ground truth, which is defined at the instance level.
    \label{fig:taxonomy}
    }
\end{figure*}

\newpage
\begin{figure*}[t]
  \centering
  \scalebox{1.0}{
    \input{fig/fig_dataset_supplementary_i2i}
  }
  \caption{\textbf{Additional examples of queries, positives, and hard negatives within the distractor set based on i2i retrieval.} Average Precision per query and rank of the negatives and positives are reported using SigLIP$^\dagger$. {\color{lightblue}\textbf{Gray:}} queries. {\color{lightgreen}\textbf{Green:}} positives. {\color{lightred}\textbf{Red:}} distractors.
  \label{fig:fig_dataset_supplementary_i2i}
  }
\end{figure*}

\newpage

\begin{figure*}[t]
  \centering
  \hspace{-55pt}
  \scalebox{0.9}{
    \input{fig/fig_dataset_supplementary_t2i}
  }
  \vspace{-10pt}
  \caption{\textbf{Examples of text queries, positives, and hard negatives within the distractor set based on t2i retrieval.} Average Precision per text query, and rank of the negatives and positives is reported using SigLIP. 
  {\color{lightblue}\textbf{Gray:}} text queries. {\color{lightgreen}\textbf{Green:}} positives. {\color{lightred}\textbf{Red:}} distractors.
  \label{fig:fig_dataset_supplementary_t2i}
  }
\end{figure*}

\newpage

\begin{figure*}[t]
  \centering
  \input{fig/fig_dataset_supplementary_obj}
  \caption{\textbf{Examples of collected query objects.} Queries and multiple positives are displayed. 
  {\color{lightblue}\textbf{Gray:}} queries.
  \label{fig:fig_dataset_supplementary_obj}
  }
\end{figure*}

