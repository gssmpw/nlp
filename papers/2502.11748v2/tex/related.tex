\section{Related work}

\begin{table*}[t]
  \centering
  \vspace{-10pt}
  \input{tab/dataset_compare}
  \vspace{-7pt}
  \caption{\textbf{Comparison with other instance-level datasets.} Datasets are compared based on their size (object, query, positives, database), the accuracy of the ground truth (gt), type of class definition, domain, supplementary annotations (bbox) and accessibility (online, license). N/A: not available. FP/FN: false positives/negatives. FN?: possibility of false negatives. Mix: combination of clean and noisy datasets. IL: instance-level. FG: fine-grained. Partial IL: instance-level with subtle variations among same class objects. CC: Creative Commons.
  \label{tab:sota}
  \vspace{-13pt}
  }
\end{table*}

In this section, we review the related work in terms of existing datasets and benchmarks in the literature.

\noindent\textbf{Datasets. }
Tab.~\ref{tab:sota} presents the datasets from the image retrieval literature related to \ours. The datasets can be compared based on five main axes: (i) \emph{Class definition adopted}. Many datasets~\cite{nister2006scalable,jed+08,rit+18,nas+17,wac+20} adopt a strict definition very similar to ours, satisfying instance-level requirements. Others~\cite{sxj+15,wj15,az11,zwd+21} adopt a more relaxed definition, where some minor variations are permitted, \eg color changes in objects of the same class. Even more relaxed are the fine-grained definitions~\cite{eproduct}, where the object of the very same type is considered related, \eg same product with different variant. (ii) \emph{Domain of the dataset}. Most datasets are tailored for a specific domain. Landmarks are among the most popular domains~\cite{az11,jed+08,rit+18,nas+17,wac+20}. Other domains include products~\cite{nister2006scalable,zwd+21,rp2k} and fashion~\cite{sxj+15,liu2016deepfashion}. Some datasets cover multiple domains, either being standalone~\cite{wj15} or bundle of repurposed datasets~\cite{ycc+23,gpr1200}. (iii) \emph{Scale of database}. Most of the datasets are small-scale, counting a few thousand images~\cite{wj15,liu2016deepfashion,zwd+21}. Larger ones~\cite{rit+18,eproduct,ycc+23} expend slightly above a million. None satisfies large-scale requirements. (iv) \emph{Noise in ground truth}. Most datasets consist of clean annotations, except for a few cases that contain inaccuracies, including false positives~\cite{wac+20}, \ie images wrongly annotated as relevant, false negatives~\cite{nister2006scalable,rit+18}, \ie relevant images that have not been annotated as positives, or the possibility of false positives~\cite{rit+18}. (v) \emph{Availability}. Most datasets are publicly available with permissive licenses, with few exceptions of partial~\cite{az11,jed+08} or no~\cite{eproduct,nas+17} availability.
To this end, no publicly available dataset fits the strict instance-level definition, contains objects from multiple domains, ensures error-free labeling and is large scale. This gap is filled with \ours satisfying all the aforementioned requirements.

\noindent\textbf{Evaluation benchmarks.}
Benchmarking~\cite{sws+00} tracks the progress in the field, which is even more necessary with the emergence of foundation models.
Several benchmarks papers~\cite{zheng2017sift,kmo21,ko2019benchmark} exists in the instance-level retrieval literature, investigating the impact of learning scheme, post-processing, model ensembling, query expansion, and whitening.
The most relevant benchmark to \ours is UnED~\cite{ycc+23} that combines existing datasets to create a union that evaluates models performance across various domains.
Due to its wide variety, UnED serves as the training dataset for linear adaptation. 

Regarding the evaluation of foundation models, the most common practice~\cite{odm+24,fwx+23,evaclip} is measuring classification performance on top of frozen models on ImageNet~\cite{dds+09}. 
This is performed either with or without the training of a classifier via linear probing or k-NN search. 
Furthermore, models are usually evaluated on dense prediction tasks~\cite{odm+24} and several multiple-downstream single-domain tasks~\cite{ady+23}. For VLMs, zero-shot classification and retrieval serve as the primary benchmarks~\cite{clip,siglip,metaclip}, utilizing class text labels. 
In this work, we provide similar evaluation protocols tailored for instance-level retrieval. 
One can test the raw model capabilities or adapt for the instance-level task via linear adaptation on UnED.
Text-to-image retrieval is also facilitated for the evaluation of VLMs.
