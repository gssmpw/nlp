\section{Benchmark methods}
We describe the methods and foundation models we evaluate on \ours, which are grouped according to their type of representations used for retrieval in global (i2i) representations, re-ranking with global (i2i) representations, re-ranking with local (i2i) representations, and text-to-image. A detailed list of models, their performance, and implementation details are in the supplementary materials.

\begin{figure*}[t]
  \vspace{-10pt}
  \centering
  \scalebox{0.9}{
    \hspace{10pt}
    \input{fig/instre_ilias} 
    \hspace{10pt}
    \input{fig/gld_ilias}
    \hspace{10pt}
    \input{fig/sop_ilias} 
  }
  \vspace{-7pt}
  \caption{\textbf{Comparison with other instance-level retrieval datasets} via reporting mAP@1k. INSTRE: 27.3K db size, multi-domain. GLDv2: 762K db size, single-domain. SOP:  60.5K db size, single-domain. Different network types are color-coded. For GLDv2 and SOP, models fine-tuned on these domains with the corresponding training sets are highlighted. No linear adaptation is used.
  \label{fig:instre_gld_sop}
  \vspace{-12pt}
  }
\end{figure*}

\noindent\textbf{Image-to-image retrieval with global representations.} Global representation methods use image encoders to map images to global descriptors and rank db images based on cosine similarity. 
We evaluate legacy and recent foundation models, varying in architecture, descriptor dimensionality, training scheme, training data, and input resolution. Foundation models~\cite{bha+} are the models trained with a training set on the scale of a hundred million. 
Particularly, 23 CNN~\cite{ksh12,sz14,slj+15,hzr+16,zvs+18,tl19,convnext} and 45 ViT~\cite{dbk+21,fwx+23} models, trained with supervision~\cite{wtj+21,vit-augreg,tcd+21,kjk23}, self-supervision~\cite{odm+24,cmm+20,hfw+20,ctm+21}, distillation~\cite{ady+23,tcd+21,swl+24}, or visual-language alignment~\cite{clip,metaclip,siglip,cbw+23,evaclip,siglip2} are benchmarked. 
Most of the non-foundation models are trained on ImageNet~\cite{dds+09}. There are models trained on single specific domains~\cite{sck+23,lsl+22,ptm+22}, \ie landmarks or products on GLDv2 or SOP.
Universal models~\cite{ycc+23,yca+24,ady+23,swl+24} trained on multi-domains or multi-task schemes are included.
The full list of models and results is provided in supplementary materials.

To mitigate the differences in training resolution, we use three widely-used resolutions, \ie 384, 512, and 724 and resize images so that their larger dimension matches one of the three.
The test resolution is defined to be one resolution above the one used for training, \eg a network trained with 224 or 384 is tested with 384 or 512, respectively.
The vast majority of models achieve best performance under this rule. Similar behavior is observed in the literature~\cite{tvd+19,st23}.

\noindent\textbf{Linear adaptation for i2i retrieval.} Pre-trained foundation models, as well as legacy models, are trained to extract representations that are applicable to various tasks; not all encoded features are directly relevant to instance-level retrieval.
To adapt the representation to the task at hand, we propose to train a single linear layer (projection) on top of frozen backbones.
The recently introduced Universal Embeddings (UnED) dataset~\cite{ycc+23} is used for learning the linear adaptation. 
UnED contains images from 8 different domains with fine-grained and/or instance-level class annotation.
In our experiments, the linear layer that converts the backbone output to a 512D descriptor is trained on a uniformly sampled subset of 1M images from UnED.
The linear adaptation layer is trained with the UJCDS~\cite{ycc+23} method. 

\noindent\textbf{Text-to-image retrieval.}
Text-to-image retrieval is performed using Vision-Language Models (VLMs) trained to align the two modalities. 
Retrieval is performed based on cosine similarity between the text query and db image descriptors that are extracted using the textual and visual encoder, respectively.
We evaluate 17 VLM models.

\noindent\textbf{Re-ranking with global representations.} Such methods rely on global descriptors for exhaustive search during the initial ranking, but also for a second refinement stage that issues a new query.
We experiment with $\alpha$QE~\cite{rtc19}, the adaptive variant of average Query Expansion~\cite{cps+07}. After the initial ranking, the descriptors of the top-ranked images are aggregated with the query via weighted average pooling.
The weights are derived from the similarity to the query in the power of $\alpha$. We don't have a validation set; hence, we use a fixed value $\alpha=1$.

\noindent\textbf{Re-ranking with local representations.}
These re-ranking methods rely on global descriptors for exhaustive search during the initial ranking but estimate query-to-db image similarity based on local descriptors for a second refinement stage of the ranked list of images.
We experiment with three methods: (i) Chamfer Similarity (CS)~\cite{rsa+14, btb+77} on the similarity matrix between local descriptors across the image pair. We use the asymmetric variant of CS with max over db descriptors and sum over query descriptors. (ii) Spatial Verification (SP)~\cite{fib+81, pci+07,cas20}, a common re-ranking method where point correspondences are processed with a RANSAC-like process and the number of inliers is used for re-ranking. (iii) AMES~\cite{ski+24}, a recent transformer-based network to estimate the similarity between sets of local descriptors. 
Due to the scale of \ours database, we use only 100 binary local descriptors for each database image and 600 for the query image.
Local descriptors are extracted using the base variant of DINOv2 with registers~\cite{odm+24,doj+23} and selected based the local descriptor detector used in AMES~\cite{ski+24}. Top-1k retrieved images are re-ranked.