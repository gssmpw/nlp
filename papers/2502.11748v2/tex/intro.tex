\section{Introduction}
The ability to recognize and differentiate every unique object instance in the physical world represents one of the ultimate goals for foundation representation models~\cite{clip,bha+,odm+24,siglip}. 
This work aims to assess this capability through the lens of instance-level image retrieval at a very large scale.
Instance-level image retrieval corresponds to searching for images of particular objects within large collections. 
All images of a particular object form their own instance-level class.
This is an important information retrieval task due to its numerous real-world applications in robotics~\cite{lcj23,sth+24}, e-commerce~\cite{eproduct, zwd+21}, and cultural heritage~\cite{sll+21,ff22}, to name just a few. 
The task faces challenges because of the substantial variations among positive examples, such as illumination/viewpoint~\cite{jc19,tas+15} changes and background clutter~\cite{mbe18,bad+21}.
An additional difficulty is the high similarity among negatives, which is driven by the extremely fine granularity in the class definitions. 
It becomes even more challenging at a real-world scale, where searching through millions or even billions of images requires handling an open-world setup with countless unseen objects spanning diverse and complex domains.


\begin{figure}[t]
  \centering
  \input{fig/fig_performance_year}
  \vspace{-24pt}
  \caption{\textbf{Performance timeline on \ours}. Curves indicate best performance in chronological order for {\color{appleblue}\textbf{image-to-image}} and {\color{appleorange}\textbf{text-to-image}} retrieval, showing a significant boost with the release of foundation models. Representations are {\color{applegreen}\textbf{linearly adapted}} via multi-domain learning on UnED~\cite{ycc+23}. {\color{applepurple}\textbf{Re-ranking with local descriptors}} achieves the best results by a significant margin.
  \label{fig:sota_scatter}
  \vspace{-15pt}
  }
\end{figure}

Benchmarking instance-level retrieval under real-world challenges is currently limited by the lack of suitable datasets. Constructing a dataset with instance-level class definitions necessitates huge development effort, reflected by the many shortcomings of existing datasets.
Shortcomings exist in several key aspects, such as dataset size~\cite{wj15}, domain diversity~\cite{rit+18,sxj+15}, and ground-truth accuracy~\cite{wac+20}, which suffers from both false positives and false negatives.
Popular datasets are typically limited to landmarks~\cite{rit+18}, and as dataset scale increases, ground-truth quality tends to decline~\cite{wac+20,syh+24}. 
This is a consequence of automating the ground-truth creation process to facilitate scaling up.
To address such limitations, we introduce the Instance-Level Image retrieval At Scale (\ours) evaluation dataset.

The creation of our dataset has two key elements. First, query and positive images are manually captured to ensure challenging variations, covering 1,000 objects across diverse domains. 
Second, to expand the dataset size without ground-truth errors or additional annotation effort, we leverage a key technique: distractor images, collected in 2014 from YFCC100M, are combined with query objects verified not to have publicly existed until after 2014. This distractor set includes 100 million images, two orders of magnitude larger than the largest existing dataset~\cite{rit+18}.
Notably, all images have a permissive license, allowing us to ensure long-term online availability to the full extent.

\ours includes both image and text queries. The latter is in the form of detailed descriptions of objects and their distinctive features. 
The dataset is designed to support future research in image-to-image and text-to-image retrieval for particular objects, and additionally serves as a large-scale benchmark for evaluating representations of foundation vision and language models (VLM)~\cite{clip,siglip}. 
To facilitate faster experimentation, we provide a mini, but challenging, version (5M) of the distractor set.

We perform an extensive evaluation comparison, including many foundation image-to-image and text-to-image models, and establish a comprehensive testbed that enables future comparisons.
The provided evaluation includes retrieval with global image representation but also re-ranking techniques that use local representations~\cite{ras+14,pci+07,ski+24} and query expansion~\cite{cps+07,rtc19}.
We observe the following:
\begin{itemize}
\item Performance of standard 10-year-old models, measured by mean Average Precision, is as low as 1.3\%, while the best-performing model achieves 31.3\%, as shown in Fig.~\ref{fig:sota_scatter}. This points out the vast progress of representation models and the high challenging factors of \ours.
\item VLMs are the top-performing models.
\item Smaller (ViT-B) models trained/tested on large resolution (512/724) outperform larger models (ViT-L) trained/tested on small resolution (256/384). 
\item Using Universal Embedding Dataset (UnED)~\cite{ycc+23} to learn a linear adaptation layer on top of frozen models improves performance of most models, making it a candidate training set to couple with \ours. Notably, VLMs demonstrate the largest benefits, presumably because their training stage does not optimize image-to-image relations. 
\item In contrast to the current belief~\cite{sck+23}, local representation is a key ingredient, while global representation, despite being efficient and compact, performs much lower. 
\item The performance gap between image-to-image and text-to-image models is surprisingly small. Therefore, detailed text queries are a reasonable proxy in the absence of image queries, even at the instance level.
\end{itemize}