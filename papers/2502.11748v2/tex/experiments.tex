\section{Experiments}
\label{sec:experiments}

\begin{table*}[t]
  \vspace{-10pt}
  \centering
  \scalebox{0.83}{
    \input{tab/foundational}
  }
  \vspace{-7pt}
  \caption{\textbf{Performance comparison using mAP@1k on \ours and \miniours for global representation models for i2i and t2i}. Comparison of model architecture (arch), training scheme (train), training data, and train/test resolution. $\dagger$ indicates results with the linear adaptation. 5M and 100M correspond to the mini and full versions of the dataset, respectively. sup, ssl, dist, vla: supervised learning, self-supervised learning, distillation and vision-language alignment. R50, CN: ResNet50 and ConvNext.
  \vspace{-15pt}
  \label{tab:foundational}
  }
\end{table*}

We evaluate all the above models and methods, extracting useful insights regarding the factors that boost retrieval performance. \ours is compared with other existing datasets for instance-level image retrieval. We analyze the performance of selected models\footnote{Although the very recent SigLIP2 is the best-performing model, we conduct most experiments with SigLIP.} to break down the impact of different \ours attributes, such as domains, clutter, and scale. Unless stated otherwise, we use the large model variants with the largest resolution available, \eg in our analysis we use SigLIP ViT-L trained with 384 resolution. 


\subsection{Comparison with other instance-level datasets}
\label{sec:comparison_datasets}

In Fig.~\ref{fig:instre_gld_sop}, \ours is compared with other instance-level retrieval datasets based on evaluation of the same models. Linear adaptation is not applied as parts of GLDv2 and SOP are included in the UnED dataset. 
Only for the sake of this comparison, and for no other experiment in this work, we use models fine-tuned on specific domains (in-domain models), \ie on the training sets of SOP and GLDv2.
INSTRE, which is also multi-domain, shows a correlation to \ours, but its performance is saturated due to its small size.
For single-domain datasets, in-domain models outperform others by a large margin, with few exceptions, \ie DINOv2, which includes the trainset of GLDv2 in its training data.
Several multi-domain models perform well on SOP since their training set is usually included in the training data. 
However, in-domain and multi-domain models face challenges on \ours, highlighting the diversity of our dataset.


\subsection{Method comparison}
\label{sec:methods_comparison}

\noindent\textbf{Image-to-image retrieval with global representations.}
Tab.~\ref{tab:foundational} presents the performance of global descriptor models on \ours. Selected models are presented to highlight useful comparisons, while many other models are included in the supplementary material. The main factors that improve performance are the size of the training set, training resolution, and model architecture, which aligns with the literature. The impact of dataset size is apparent in various model combinations, \eg CLIP with \texttt{openai} and \texttt{laion2b}. This is also pronounced by the dominance of foundation models. Training with large resolution brings significant gains and consistently improves mAP@1k. In some cases of SigLIP, smaller models trained with large resolutions outperform larger ones trained with small resolutions. For the models of the same resolution, it is a common trend for larger model variants to bring corresponding performance gains. 
In general, VLMs perform the best. From non-VLMs, only DINOv2 and Unicom achieve competitive performance. Masked Image Modeling (MIM) and supervised models are not performing well.
Our linear adaptation scheme is very effective, improving most models. The boost is more pronounced in the case of VLMs. A possible explanation for such improvements is that image-to-image relations are not optimized during the training of VLMs.

\noindent\textbf{Text-to-image retrieval.}
Following results in Tab.~\ref{tab:foundational}, similar conclusions are derived for the t2i case. Retrieval performance improves with the scaling of the training data. The larger model achieves significantly better results, \ie compare the base with large variants. Finally, it is noteworthy that the best performance achieved by SigLIP2 is very close to the i2i performance when no adaptation is used. Note that t2i includes 1k text queries in total, with one query per object, while i2i 1,232 image queries.

\begin{table}[t]
  \centering
  \scalebox{0.85}{
    \input{tab/mini}
  }
  \vspace{-7pt}
  \caption{\textbf{A challenging distractor subset for \miniours.} mAP@1k evaluated for different distractor sets, 100M: the full dataset, 5M-mini: \miniours subset, 5M-rand: random subset. We report the mean and std of 3 randomly sampled subsets. $\dagger$ indicates results with the linear adaptation.
  \label{tab:mini}}
  \vspace{-10pt}
\end{table}
\begin{table}[h!]
  \centering
  \scalebox{0.85}{
    \input{tab/reranking}
  }
  \vspace{-7pt}
  \caption{\textbf{Performance comparison for re-ranking methods.} Oracle represents the performance of perfect re-ranking at the top-1k images. Top: query expansion with global descriptors. Bottom: re-ranking with local descriptors. $\dagger$: results with linear adaptation.
  \label{tab:reranking}}
  \vspace{-20pt}
\end{table}

\noindent\textbf{Evaluation of \miniours selection.} Tab.~\ref{tab:mini} shows performance on \miniours for five models with linear adaptation. 
The selected subset is significantly more challenging than a random selection of 5M images. 
More precisely, a set of approximately $\sim$26M random images matches the performance of \miniours. 

\noindent\textbf{Retrieval with re-ranking.} Tab.~\ref{tab:reranking} shows the performance of re-ranking methods applied on top of SigLIP with and without linear adaptation on \ours. Complementary to mAP@1k, an oracle-based top-1k re-ranking metric is reported as the upper bound of a re-ranking method that processes the top 1k images.
Local similarity estimated by a learned model proves to be very effective for re-ranking. Nevertheless, the oracle re-ranking performance indicates that there is a lot more space for improvements. Re-ranking with QE is useful when the number of aggregated neighbors is low and drops below the baseline when the number of neighbors is increased. 
Notably, global re-ranking affects and, interestingly, decreases oracle performance since the whole db is re-ranked; while local re-ranking does not affect it since it is performed only on a shortlist of images.


\begin{figure*}[t]
\vspace{-10pt}
\hspace{-15pt}
  \centering
  \scalebox{0.91}{
    \input{fig/fig_performance_per_subdomain}
  }
  \vspace{-16pt}
  \caption{\textbf{Performance comparison per category.} mAP@1k  averaged over objects in the same mid-level taxonomy category,   organized by their primary-level category size, with sorting within each group by SigLIP$^\dagger$+AMES performance. Comparison between SigLIP with and without adaptation, SigLIP combined with AMES reranking, SigLIP t2i, and DINOv2. $\dagger$ indicates results with the linear adaptation.
  \label{fig:subdomains}
  \vspace{-16pt}
  }
\end{figure*}

\begin{figure}[t]
  \centering
  \vspace{-3pt}
  \scalebox{0.88}{
    \input{fig/pairwise}
  }
  \vspace{-14pt}
  \caption{\textbf{Performance comparison reporting AP per query for different approaches with SigLIP.} Pearson correlation reported in parenthesis. $\dagger$ indicates results with the linear adaptation.}
  \label{fig:pairwise}
  \vspace{-16pt}
\end{figure}

\subsection{Analysis}
\label{sec:analysis}

\noindent\textbf{Performance per domain.} Fig.~\ref{fig:subdomains} shows the performance per taxonomy categories. The taxonomy annotations allow a fine-grained view of the results, which can possibly allow us to capture imbalanced improvements in future work.
For example, DINOv2, despite being overall inferior to SigLIP, is outperforming it in categories like architecture and sculptures or is quite similar in categories like public art and paper art.
This is possibly attributed to the curation and composition of the DINOv2 training set, which includes artwork and landmark datasets. Also, some categories are hurt by re-ranking with AMES, with some demonstrating big drops, \ie sport, gaming, perfume. These categories deviate significantly from the domain AMES is trained, \ie landmarks, which could potentially justify such drops.

\noindent\textbf{Per query comparisons.} Fig.~\ref{fig:pairwise} shows the AP per query for various methods. Linear adaptation boosts most queries, \ie performance drop only for 192 queries. Image- and text-based retrieval are not strongly correlated despite performing similarly, which is good evidence~\cite{tky+24} for the effectiveness of model ensembles. Indeed, ensembling i2i and t2i by averaging similarities brings +6.1 improvement over i2i retrieval. Query expansion improves the queries with at least some positives at top positions, \ie AP greater than 20. However, it harms many low-performing queries by aggregating descriptors irrelevant to the query. AMES improves the majority of the queries; however, many are harmed, indicating that there is plenty of room for improvement.

\noindent\textbf{Impact of clutter and scale in positives.}
To quantify the impact of background clutter and scale changes, Fig.~\ref{fig:scale_clutter_groups} presents the performance for different groups of positives. Dealing with small objects and multi-object scenes form major weaknesses of existing models. Notably, t2i beats i2i without adaptation in small-scale groups.

\begin{figure}[t]
    \vfill
    \centering
    \begin{tabular}{cc}
        \hspace{-10pt}
        \begin{subfigure}{0.48\textwidth}
            \input{fig/fig_performance_per_scale}
        \end{subfigure}
        &
        \hspace{-130pt}
        \begin{subfigure}{0.48\textwidth}
            \input{fig/fig_performance_per_clutter}
            \vspace{-0.3pt}
        \end{subfigure}
    \end{tabular}
    \vspace{-10pt}
    \caption{\textbf{Performance evaluation (mAP@1k) across different amounts of object area coverage and background clutter.} Positives across all queries are jointly ranked based on coverage or clutter and split into 4 equal size groups. 
    Queries with no positive in the corresponding group are discarded. No. of queries per group is in parentheses.
    $\dagger$ indicates results with the linear adaptation.
    \vspace{-8pt}
    \label{fig:scale_clutter_groups}}
\end{figure}
