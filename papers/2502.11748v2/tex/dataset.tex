\section{ILIAS dataset}
\label{sec:ilias}

\begin{figure*}[t]
  \centering
  \scalebox{0.83}{
    \input{fig/fig_dataset_main}
  }
  \vspace{-21pt}
  \caption{\textbf{Examples of query, positive and hard negatives within the distractor set.} Average Precision per query and rank of the negatives and positives is reported using SigLIP~\cite{siglip} model. {\color{lightblue}\textbf{Gray:}} queries. {\color{lightgreen}\textbf{Green:}} positives. {\color{lightred}\textbf{Red:}} distractors.
  \label{fig:fig_dataset_instances}
  \vspace{-16pt}
  }
\end{figure*}

\subsection{Composition and collection}
\label{sec:composition}

\noindent{\textbf{Instance-level class definition.}} Following an instance-level class definition~\cite{rit+18,ygg+21}, we consider \emph{all indistinguishable object instances of the real world to form their own class}. 
Nevertheless, we add a restriction to consider a pair of images as relevant to each other only if there is a view overlap. Other cases are explicitly not included in the dataset, contrasting the existing work~\cite{liu2016deepfashion,sxj+15,wac+20}. Therefore, models should mostly rely on estimating the visual similarity and less on shortcuts through semantics.

\noindent\textbf{Overview.}
\ours supports both image-to-image (i2i) and text-to-image (t2i) retrieval and follows the standard setup for retrieval datasets, consisting of two main parts: (i) \emph{query} images and text, and (ii) \emph{database} (db) images. The objective is to rank \emph{positives} -- db images relevant to the query -- at the top ranks. 
The collected objects cover a wide range of categories and are not restricted to specific domains. An overview of some collected objects is provided in Fig.~\ref{fig:fig_dataset_instances}.

Queries and positives are created/collected by a group of \emph{collectors} that are well-informed about the task objectives. 
In addition to positives, in the database, we include numerous \emph{distractors} -- irrelevant (negative) images to the queries -- that make retrieval more challenging. Following previous work~\cite{rit+18}, adding a large, uncurated set of random images achieves this. The larger the set, the higher the chances of hard negatives -- images with similar appearance or semantics to the queries. To this end, we select the YFCC100M~\cite{tsf+16} dataset to serve as the source of distractors due to its size and permissive license.

\noindent\textbf{Selected objects.}
Ensuring that distractor images include no false negatives cannot be performed in a scalable or accurate way if one relies on human annotation or metadata. 
Instead, we take advantage of the fact that YFCC100M was crawled from Flickr in 2014.
Hence, an object qualifies in \ours if it could not have appeared on Flickr before 2014. To verify this, we rely either on publicly available information, \eg objects known to be created/manufactured after 2014, or on the collector's knowledge about the object not being publicly available. Additionally, we opt for objects with distinctive and unique features that set them apart from others within the same category. For example, we avoid recent smartphones that look like plain black screens or new objects with distinctive parts closely resembling older ones. \looseness=-1

\noindent\textbf{Queries and positives.}
Query images depict the instance on a clean or uniform background. When this is not feasible (\eg buildings or statues), background blurring or cropping is applied. This is performed to avoid including background objects in the query that do not have corresponding positives in our ground truth information. 
Positives are images featuring the query object in challenging conditions, such as clutter, scale changes, occlusions, and partial views. 
Prior work~\cite{rit+18} reveals that easy positives dominate performance metrics. Thus, we specifically opt for challenging cases that cannot be easily retrieved by the models.
To avoid taking advantage of camera identification, most query and positive images are captured with at least two different camera models to introduce diversity. We also incorporate older camera models that are used in YFCC100M.

Each text query consists of a detailed and fine-grained textual description of an object. 
Descriptions are initially created by a large language model prompted to provide highly detailed depictions of the object shown in query images. Generated descriptions are manually edited to fix errors, insufficient descriptions, or nuances of the model.

\noindent\textbf{Distractors.}
The YFCC100M dataset was chosen for the distractor set due to its large scale and diverse range of concepts. It consists of 100 million Flickr images, collected without specific filtering, aside from being shared under a permissive CC-BY license.

\noindent\textbf{Bounding box annotation.} We include supplementary bounding boxes that specify the precise location of objects in query and positive images. 
They provide statistics about the position and size of object areas, assist our analysis of the dataset challenges, and support future research in instance-level localization.

\noindent\textbf{Evaluation metric.}
Retrieval performance is evaluated via mean Average Precision (mAP), a widely used metric in instance-level image retrieval~\citep{rtc19,pci+07,pci+08}. Specifically, we adopt mAP@1k~\cite{wac+20}, which assesses the ranking of the top-1k nearest neighbors for each query, treating any positive not ranked among the top-1k as not retrieved. 
We estimate the area under the curve using rectangles and not trapezoids.


\subsection{Statistics}
\label{sec:statistics}

\noindent\textbf{Dataset size.} The final \ours dataset includes 1,000 object instances captured in 5,947 images, of which 1,232 are queries and 4,715 are positives.
Fig.~\ref{fig:dist_pos} shows the distribution of positives per object. Also, 99,144,315 images from YFCC100M are downloaded. All images (queries, positives, distractors) are transferred through Flickr to ensure the same pre-processing.

\noindent\textbf{Taxonomy.} A hierarchical 3-level taxonomy is composed for \ours. All instances are assigned across one to three categories of different granularity levels. The taxonomy consists of 8 categories on the coarser level, 42 on the mid level, and 38 on the finer level. The categories are derived through manual labeling of the objects based on their semantic content. To form the coarser-level categories, we use domain definitions borrowed from prior work~\cite{wac+20,sxj+15,liu2016deepfashion} to align with the literature, \ie art, landmarks, products, fashion. We also define novel categories based on the objects that do not fit into any existing domain. The distribution of objects across categories is uneven, \eg ranging from 168 and 162 for art and landmarks to 83 for products. Each mid- and finer-level category contains at least 4 instances. Note that taxonomy is given to provide statistics about the domains of objects and assist our analysis instead of being leveraged as ground truth. The distribution of taxonomy categories can be inferred by Fig.~\ref{fig:subdomains}, and a detailed figure is provided in the supplementary material.

\noindent\textbf{Bounding box analysis.} A total number of 6,117 bounding boxes are annotated for both queries and positives. Note that positives may display multiple objects of near identical appearance to the query; in such cases, bounding boxes are drawn on all indistinguishable objects. There are 235 images with more than one bounding box.
%
Based on the annotated bounding boxes, we compute the area covered in the image by the object instances to derive its relative scale. Fig.~\ref{fig:dist_scale} shows the distribution of the scale ratio for queries and positives. Most objects in queries cover the largest area of the images; while in the vast majority of positives, the object covers a small area of less than half the image. It is a result of the severe scale changes and partial views. \looseness=-1
%
Moreover, we use the Segment Anything Model (SAM)~\cite{kmr+23,rgh+24} to extract object segments from positives. The number of detected segments outside the query object's bounding boxes is computed. This indicates clutter from other items in the positives. Fig.~\ref{fig:dist_seg} shows the segment number distribution, with most images containing multiple segments due to clutter.

\begin{figure}[t]
    \input{fig/fig_dataset_distributions}
    \vspace{-15pt}
    \caption{\textbf{\ours statistics}. (a) number of positives per object, (b) positive distribution by the SAM segments outside the bounding box, (c) image distribution by the relative bounding box area.
    \label{fig:statistics}
    %
    \vspace{-15pt}
    }
    %
\end{figure}

\subsection{\miniours}
\label{sec:mini}
We provide a small version of \ours, called \miniours, to facilitate quick experimentations. 
It consists of the query and positive images collected for \ours, and a subset of the YFCC100M distractors.
Instead of randomly subsampling YFCC100M, we construct a challenging subset with the help of VLMs.
We aim at selecting distractors displaying objects of similar categories as the query objects.
We use the text category labels of the taxonomy as text queries. We also extend them with standard templates used for zero-shot recognition~\cite{clip}, which resulted in several thousands. T2i similarity between each text query and each distractor image is estimated. A similarity score for each distractor is derived based on its maximum similarity over the text queries. We ensemble the scores of 3 VLMs to rank images. The top-5M ranked distractors compose the final \miniours. 
Our experiments indicate that this subset is significantly more challenging than a random subset of the same size.
