\section{Related Work}
% This section focuses on multimodal fake news detection methods relevant to the proposed approach, while a detailed description of unimodal methods is provided in the appendix \ref{unimodality works}.

\subsection{News Content-based Methods}
The lack of correlation between textual and visual content in news is a key characteristic of certain types of multimodal fake news. Consequently, several studies have focused on measuring multimodal consistency to verify the credibility of news. Zhou et al. ____ utilized a pre-trained image captioning model to convert images into text which achieved cross-modal semantic space alignment, then assessed the multimodal consistency between the original text and the generated image captions. Similarly, Xue et al. ____ employed shared weights to enforce the alignment of textual and visual representations within a shared semantic space, calculating the similarity of the transformed multimodal representations. To mitigate the semantic gap between modalities, Jiang et al. ____ leveraged CLIP ____ to extract features from news text and images, subsequently using cosine similarity to guide multimodal fusion. Chen et al. ____ introduced cross-modal contrastive learning as an auxiliary task, and then used the Kullback-Leibler (KL) divergence to measure the ambiguity score between the latent distributions of text and image sampled from the autoencoder. Qi et al. ____ measured multimodal entity inconsistency by calculating the similarity between entities in the news text and the corresponding visual content. 

In the fusion of multimodal information, the news text and images are not simply modeled through operations such as addition or concatenation. Instead, their high-level semantic interactions are leveraged as crucial features, capturing the intricate relationships between modalities. Qian et al. ____ extracted text features at different hierarchical levels and fused them with visual features using a contextual transformer to learn cross-modal contextual features and supplementary information. Wu et al. ____ extracted spatial and frequency domain features from images and progressively fused them with textual features through stacked cross-attention mechanisms ____.
However, their approaches primarily focus on exploring inter-modal relationships, often neglecting the role of inherent modal properties in fake news detection. Moreover, capturing high-level semantic interactions based on latent consistency cues between modalities is typically performed in a black-box manner.

\subsection{External Knowledge-based Methods}
Due to limitations in text length or the emergence of novel terms, discrepancies in understanding news content may arise. As a result, several studies have incorporated external knowledge extracted from social media ____, knowledge graphs ____ or internet retrieval ____ to enhance the performance of fake news detection. Zheng et al. ____ employed an attention mechanism to combine social graph features, composed of user and comment data, with textual and visual features of news, generating discriminative features for fake news detection. Wu et al. ____ performed the deep fusion of text and image features based on three distinct reading patterns while leveraging the relationship between news comments and content to capture semantic inconsistencies. Zhang et al. ____ leveraged news text entities as prompts to guide a large-scale vision-language model in generating image entities that enhance the semantic knowledge of the content. Zhang et al. ____ converted news images and externally relevant knowledge into pure textual representations, which were then combined with the original text and fed into a prompt-based large language model to predict the authenticity of the news. While some methods enhance the representation of news content by incorporating external knowledge, this simultaneously introduces the risk of noise, which is crucial for the accuracy of fake news detection. These challenges significantly hinder the progress of multimodal fake news detection.



    
%