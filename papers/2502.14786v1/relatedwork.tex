\section{Related work}
\label{sec:related}

Contrastive pretraining as popularized by CLIP~\cite{clip} and ALIGN~\cite{align} has become the dominant approach for learning high-level, semantic, visual representations that perform well on classification and retrieval, as vision encoders for VLMs~\cite{qwen-vl, blip2, peng2023kosmos, llava1, beyer2024paligemma, mm1, cambrian1} and open-vocabulary tasks including detection~\cite{minderer2022simple, kuo2023open, owlvitv2} and segmentation~\cite{ding2022decoupling, catseg}. Besides the original CLIP release, several projects have released open-weight contrastive models~\cite{ilharco2021open, sun2023eva, siglip, li2023clipa, fang2024dfn, xu2024demystifying}. At a high level, these works follow training methods that are relatively close to the original CLIP method, mainly \cite{siglip} proposing modified loss functions and \cite{fang2024dfn, xu2024demystifying} targeting data quality and filtering.


More generally, a large number of modifications and improvements  to contrastive training have been proposed in the literature. \cite{gadre2024datacomp, fang2024dfn, xu2024demystifying, evansdata, udandarao2024active} study filtering techniques to improve data quality. With a similar motivation, \cite{fan2023improving, nguyen2024improving, lai2024veclip, maninis2024tips} re-caption training images with VLMs to improve the caption quality and hence the quality of the training signal. Another promising area has been to modify or augment the loss function. \cite{mu2022slip, naeem2024silc, maninis2024tips} combine CLIP with self-supervised losses. Another popular approach is to add a language decoder to train with captioning as an auxiliary task~\cite{yu2022coca, blip2}. Captioning as a standalone representation learning task has attracted less attention, but can produce visual representations competitive with contrastive training~\cite{wang2021simvlm, cappa, locca, fini2024multimodal}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/cultural_diversity.pdf}
    \caption{10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP~2 consistently performs better than SigLIP (see Table~\ref{tbl:app_cultural_diversity} for additional results).}
    \label{fig:cultural_diversity}
\end{figure}