\section{Full PaliGemma results}
% https://colab.corp.google.com/drive/1UExIPclPIuug0bMJ45yBuR0-Rtnxu5Qp
\vspace*{\fill}
\begin{table*}[h]
  \centering
  \footnotesize
  \include{tables/paligemma}
  \caption{The first three columns compare Large-sized models with 256 tokens each (that's 224px for the AIMv2 model with patch size 14, and 256px for the SigLIP models with patch size 16). The last four columns compare So400M-sized SigLIP models with patch size 14 at two different resolutions (and hence tokens). Same data as in Figure~\ref{fig:paligemma}.}
  \label{tbl:paligemma}
\end{table*}
\vspace*{\fill}

\clearpage

\section{Full NaFlex results}
\vspace*{\fill}
\begin{table*}[h]
  \centering
  \footnotesize
  \include{tables/app_naflex_table}
  \caption{Comparing the NaFlex (supporting native aspect ratio and variable sequence length (Seq.)) and the standard square-input SigLIP variants which use a separate checkpoint per sequence length. Numerical data corresponding to the plots in Fig.~\ref{fig:naflex}. TC: TextCaps, HT: HierText, SC: SciCap, S2W: Screen2Words.}
  \label{tbl:app_naflex}
\end{table*}
\vspace*{\fill}

\clearpage

\section{Full cultural diversity and fairness results}\label{app:cultural_diversity}
\vspace*{\fill}
\begin{table*}[h]
  \centering
  \footnotesize
  \include{tables/app_cultural_diversity}
  \caption{10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP 2 consistently outperforms SigLIP on most benchmarks.}
  \label{tbl:app_cultural_diversity}
\end{table*}
\vspace*{\fill}


\begin{table*}
  \centering
  \footnotesize
  \include{tables/app_disparity_rep_bias}
  \caption{Disparity: Corresponds to the maximum difference in 0-shot accuracy on Dollar Street when disaggregating the accuracy by income level: We observe that SigLIP 2 slightly reduces the performance disparity. Rep. bias: Representation bias; lower values are better. SigLIP2, which is trained on de-biased data, exhibits significantly reduced representation bias than its predecessor. In addition, larger models are better than smaller models, in agreement with the earlier findings in~\cite{alabdulmohsin2024clip}.}
  \label{tbl:app_rb}
\end{table*}