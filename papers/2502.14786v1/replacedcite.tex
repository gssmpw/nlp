\section{Related work}
\label{sec:related}

Contrastive pretraining as popularized by CLIP____ and ALIGN____ has become the dominant approach for learning high-level, semantic, visual representations that perform well on classification and retrieval, as vision encoders for VLMs____ and open-vocabulary tasks including detection____ and segmentation____. Besides the original CLIP release, several projects have released open-weight contrastive models____. At a high level, these works follow training methods that are relatively close to the original CLIP method, mainly ____ proposing modified loss functions and ____ targeting data quality and filtering.


More generally, a large number of modifications and improvements  to contrastive training have been proposed in the literature. ____ study filtering techniques to improve data quality. With a similar motivation, ____ re-caption training images with VLMs to improve the caption quality and hence the quality of the training signal. Another promising area has been to modify or augment the loss function. ____ combine CLIP with self-supervised losses. Another popular approach is to add a language decoder to train with captioning as an auxiliary task____. Captioning as a standalone representation learning task has attracted less attention, but can produce visual representations competitive with contrastive training____.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/cultural_diversity.pdf}
    \caption{10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP~2 consistently performs better than SigLIP (see Table~\ref{tbl:app_cultural_diversity} for additional results).}
    \label{fig:cultural_diversity}
\end{figure}