\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.88
\input{math_commands.tex}

% \usepackage{subcaption}
% \usepackage{subfig}

\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{comment}

%  ========================== Packages ==================
\usepackage{makecell}
\usepackage{bbm}
\usepackage{bm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{svg}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}

\usepackage{color}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{Theorem}{\textbf{Theorem}}
\newtheorem{Lemma}{\textbf{Lemma}}
\newtheorem{Corollary}{\textbf{Corollary}}
\newtheorem*{Remark}{\textbf{Remark}}
\newtheorem*{Proof}{\textbf{Proof}}
\newtheorem{Assumption}{\textbf{Assumption}}
\newtheorem{Proposition}{\textbf{Proposition}}
\newtheorem{Example}{Example}
\newtheorem{Case}{Case}

\title{Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization}


\author{Jiajun Fan$^{1 *}$, Shuaike Shen$^{2 \dagger}$, Chaoran Cheng$^1$, Yuxin Chen$^{3 \dagger}$, Chumeng Liang$^{4 \dagger}$, Ge Liu$^{1 *}$ \\
$^1$ University of Illinois Urbana-Champaign, $^2$ Zhejiang University.\\
$^3$ Tsinghua University,  $^4$ University of Southern California. \\
$^1$ \texttt{\{jiajunf3, chaoran7, geliu\}@illinois.edu} \\
$^{2,3,4}$ \texttt{\{shenshuaike256, itsme.yxchen, caradryan2022\}@gmail.com} \\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}



\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors.}
\footnotetext[2]{Work done as research interns at University of Illinois Urbana-Champaign.}

\maketitle

\begin{abstract}
Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as policy collapse from overoptimization and the prohibitively high computational cost of likelihoods in continuous-time flows. In this paper, we propose an easy-to-use and theoretically sound RL fine-tuning method, which we term Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2). Our method integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions, without relying on gradients of rewards or filtered datasets. By introducing an online reward-weighting mechanism, our approach guides the model to prioritize high-reward regions in the data manifold. To prevent policy collapse and maintain diversity, we incorporate Wasserstein-2 (W2) distance regularization into our method and derive a tractable upper bound for it in flow matching, effectively balancing exploration and exploitation of policy optimization. We provide theoretical analyses to demonstrate the convergence properties and induced data distributions of our method, establishing connections with traditional RL algorithms featuring Kullback-Leibler (KL) regularization and offering a more comprehensive understanding of the underlying mechanisms and learning behavior of our approach. Extensive experiments on tasks including target image generation, image compression, and text-image alignment demonstrate the effectiveness of our method, where our method achieves optimal policy convergence while allowing controllable trade-offs between reward maximization and diversity preservation.
\end{abstract}


\section{Introduction}
Generative models have achieved remarkable success in producing high-fidelity data across various domains, including text and images \citep{instructed_rlhf, sd3}. Among these, continuous flow-based models have gained attention for their more concise and flexible design of the ODE-based denoising process to model complex data distributions \citep{otcfm, fmgm}. However, fine-tuning such flow-based models to align with arbitrary user-defined reward objectives remains challenging. While recent advancements in reinforcement learning (RL) have demonstrated considerable success in fine-tuning diffusion-based generative models \citep{dpo}, their application to continuous flow-based models remains underexplored. 
The primary challenges lie in lacking computationally efficient divergence measurement and tractable methods to avoid overoptimization \citep{ddpo} and policy collapse in fine-tuning continuous-time flow matching models. Particularly, traditional policy gradient methods \citep{ddpo} struggle in this continuous domain due to the high computation cost of computing the exact ODE likelihood for continuous normalizing flows (CNFs), which requires solving intricate transport dynamics and tracking probability flows with model divergence calculation across time (See App. \ref{app: likelihood cal}).


Furthermore, existing diffusion-based RLHF methods, such as DPO \citep{dpo} and DDPO \citep{ddpo}, face significant limitations when applied to CNFs. DPO relies on pairwise reward comparisons and filtered datasets, making it unsuitable for optimizing arbitrary reward functions.  DDPO adopts a PPO-style policy optimization that depends on frequent likelihood calculations, which are computationally intractable for CNFs.  Moreover, methods for diffusion often rely on tractable ELBO likelihood approximation or MDP formulation, which does not apply to CNFs. To tackle these challenges, we propose a novel RL fine-tuning framework that bypasses the need for explicit likelihood estimation by focusing on \textit{reward-weighted objectives} tailored specifically for continuous flow-based models, which we call Reward-Weighted Conditional Flow Matching (RW-CFM).  Our approach significantly reduces computational overhead while maintaining optimal performance.

\begin{wrapfigure}{r}{0.64\columnwidth}
    \vspace{-1.0em}
    \includegraphics[width=0.64\columnwidth]{figure/new_main/rl.pdf}
    \vspace{-2.0em}
    \caption{A General Architecture of Our Method.}
    \label{fig:main_pipeline}
    \vspace{-1.0em}
\end{wrapfigure}

In the meanwhile, many existing reward-weighted RL fine-tuning methods require train on offline datasets manually collected by humans \citep{ReFT} or derived from pre-trained models \citep{offline}.  While this approach ensures stable learning, it restricts the model's ability to explore optimally in an online setting, inducing what is known as the \textit{online-offline gap} \citep{on_off_gap} (See Fig. \ref{fig:main_pipeline} and Figs. \ref{fig: tau control mnist}). This limitation reduces the model's flexibility and can lead to sub-optimal convergence. In contrast, online RL enables the generative model to continuously update its training distribution, allowing for greater adaptability and diverse exploration within the reward space. Despite the success in traditional RL domains, the convergence behavior of online RL in fine-tuning generative models, particularly continuous flow-based models, has not yet been widely studied.  To leverage the advantages of online RL for generative models, we propose an online reward-weighted fine-tuning method called Online Reward-Weighted Conditional Flow Matching (ORW-CFM). In this paper, we theoretically demonstrate that in online reward-weighted settings, the continuous updates of the training data distribution of the fine-tuned generative model based on reward feedback can cause the learned policy to collapse into a delta distribution (i.e., becoming extremely exploitative; see Lemma \ref{lemma: limiting case}). Similar phenomena have been experimentally observed in mode collapse of GAN  \citep{gan2014} and over-optimization in diffusion models \citep{ddpo}. Nevertheless, this issue has not yet been fully addressed or theoretically demonstrated in previous works \citep{ddpo}.

To address these challenges and avoid policy collapse, we introduce a divergence regularization approach to bound the distance between the fine-tuned model and the pre-trained reference model into our online RL method. This regularization ensures that the policy updates remain within a controllable and stable range, preventing collapse while still allowing the model to explore and improve. One of the major challenges in fine-tuning CNFs lies in finding a computationally tractable distance measure between different flow matching models, especially considering the KL divergence used in classic RL methods is inefficient in CNFs. To overcome this, we instead propose to use the Wasserstein-2 (W2) distance \citep{wgan} to measure the distance between the fine-tuned model and the reference model. In this paper, we derive a tractable upper bound of W2 distance in flow matching (See Theorem \ref{theorem: W2 Bound for Flow Matching}) and incorporate it into our online RL method, inducing a reward-distance trade-off in fine-tuning flow matching models. We demonstrate both theoretically and empirically that W2 distance regularization allows for a controllable and stable fine-tuning process, enabling our agent to balance the reward objective and generative diversity.

In summary, for the first time, we propose an online RL method to fine-tune continuous flow-based generative models towards arbitrary reward objectives with sufficient theoretical guarantees and detailed analysis of the convergence behavior in both online and offline settings. Our approach can leverage classifier-guided reward models, compression rate, and text-image similarity score with CLIP models \citep{clip_models}, eliminating the need for time-consuming likelihood calculations that involve the integration of the model divergence across the entire continuous trajectory. Our novel framework significantly reduces computational costs and enhances the feasibility of using RL to fine-tune continuous flow-based generative models. The main contributions of our paper are:

\begin{itemize}
    \item \textbf{A Novel RL-Based Fine-Tuning Framework for CNFs}: We propose the Online Reward-Weighted Conditional Flow Matching (ORW-CFM) method, a novel RL approach for fine-tuning continuous flow-based generative models. Our method enables optimization with user-defined reward functions without the need for likelihood calculation \citep{ddpo}, filtered dataset \citep{dpo} or differentiable reward \citep{Draft}, expanding the applicability of flow-based models in various downstream generative tasks.
    
    \item \textbf{Tractable Wasserstein Regularization}: To prevent policy collapse and maintain the diversity of the fine-tuned models, we introduce Wasserstein-2 (W2) distance regularization into our online method, and derive a tractable upper bound of it in flow matching. This regularization effectively balances exploration and exploitation by bounding the distance between the fine-tuned model and the pre-trained reference model, inducing a theoretically and empirically controllable convergent behavior of fine-tuning flow matching models.

    \item \textbf{Theoretical Analysis and RL Connections:} We provide comprehensive theoretical analyses of our method, including convergence properties and the induced data distributions. We establish connections between our approach and traditional RL algorithms with KL regularization, offering new insights into fine-tuning flow matching generative models.

    \item \textbf{Empirical Validation:} Through extensive experiments on tasks such as target image generation, image compression, and text-image alignment, we empirically demonstrate the effectiveness of our method. Our approach achieves optimal policy convergence, allowing controllable trade-offs between reward maximization and generative diversity.
\end{itemize}

\section{Related Works}
\paragraph{Conditional Flow Matching} Conditional Flow Matching (CFM) \citep{fmgm,otcfm} and related methods have been pivotal in transforming simple data distributions into complex, task-specific ones. \cite{fmgm} and \cite{otcfm} focus on training flow-based models by conditioning the generative flow on known distributions, allowing for more precise control over the generative process. However, while these models achieve great success in generation tasks, how to fine-tune FM models to fit arbitrary user-defined objectives has not yet been widely studied.



\paragraph{Fine-tuning From Human Feedback} Recent works typically leverage DPO \citep{dpo} or policy gradient RL methods \citep{ddpo,dpok} to achieve RLHF \citep{instructed_rlhf}. However, since the likelihood of the generative policy cannot be easily calculated in continuous-time flow models, none of these approaches can be easily adapted to fine-tune flow matching models. Additionally, DPO \citep{dpo} relies heavily on a filtered dataset, which does not work with a general reward model, while DPOK \citep{dpok} relies on KL divergence, which is computationally inefficient and lacks effective evidence lower bound (ELBO) in FM.


\paragraph{Fine-Tuning Generative Models with RL}
RL has been increasingly adopted to fine-tune generative models, allowing them to adapt to specific downstream tasks by optimizing for user-defined reward objectives. \cite{ddpo,dpok,offline} used RL to fine-tune diffusion models to align generated data with task-specific rewards. \cite{offline} proposed an offline reward-weighted method,  while \cite{ddpo} proposed an online version without divergence regularization, which either failed to converge into optimal policy  or collapsed into an extremely greedy policy without diversity \citep{ddpo}. Other reward-weighted methods like ReFT, ReST \citep{ReFT,Rest} are almost  limited to the offline settings with given datasets and lack theoretical analysis of the convergent behavior.  Importantly, how to adopt online RL methods to fine-tune FM models has not yet been widely studied in both theory and practice.





\section{Preliminaries}

\subsection{Flow Matching Models}
Flow Matching (FM) is a promising technique for training generative models by transforming a simple initial distribution $p\left(x_0\right)$ like Gaussian noises, into a complex target distribution $p\left(x_1\right)$, which represents real-world data \citep{sd3}. This transformation works continuously over time, governed by a time-dependent vector field $u_t(x)$ and the flow ordinary differential equation (ODE):$\frac{d x}{d t}=u_t(x)$. The goal of FM is to learn a vector field that transports samples from the base distribution to the target distribution by minimizing the difference between the learned vector field and the true dynamics of the probability flow. This process can be  formalized by the Flow Matching loss: $ \mathcal{L}_{\mathrm{FM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}(0,1), x \sim p_t(x)}[\|v_\theta(t, x)-u_t(x)\|^2]$.

However, the original FM objective is generally intractable because the time-varying distribution $p_t(x)$ and the true vector field $u_t(x)$ are often intractable. To address this, Conditional Flow Matching (CFM) has been proposed \citep{fmgm}, which simplifies the task by conditioning the flow on target samples to derive a tractable objective. The CFM loss is defined as follows:
\begin{equation}
\label{equ: cfm loss}
    \mathcal{L}_{\mathrm{CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}(0,1), x_1 \sim q\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[\left\|v_\theta(t, x)-u_t\left(x \mid x_1\right)\right\|\right]^2
\end{equation}
wherein $u_t\left(x \mid x_1\right)$ becomes tractable by defining explicit conditional probability paths from $x_0$ to $x_1$, such as OT-paths \citep{otcfm} or linear interpolation paths \citep{fmgm}. In this paper, we adopt OT-paths for all experiments based on TorchCFM \citep{torchcfm2} and Diffusers \citep{von-platen-etal-2022-diffusers}.

\subsection{Reward Weighted Regression}
Reward Weighted Regression (RWR) \citep{rwr,ddpo} is an approach used in RL for policy optimization. In this framework, actions that yield higher rewards are assigned greater importance, and the policy is updated by re-weighting actions based on the rewards they produce. The key idea behind RWR is to adjust the probability of selecting actions proportionally to their associated rewards, encouraging the agent to focus on high-reward actions.

Specifically, the policy $\pi(a \mid s)$ is updated by re-weighting the actions via $\pi_{\mathrm{new}}(a \mid s) \propto \pi_{\mathrm{old}}(a \mid s) \cdot \exp (\tau * r(s, a))$, where $r(s, a)$ is the reward for taking action $a$ in state $s$, and $\tau$ is a temperature parameter that controls the trade-off between exploration and exploitation. Higher rewards lead to higher probabilities for the corresponding actions, allowing the agent to improve its policy over time by favoring actions that maximize cumulative rewards: $    \pi^*=\arg \max _\pi \mathbb{E}_\pi[\sum_{t=0}^T \gamma^t r(s_t, a_t)]$.


\section{Methods}


\subsection{Problem Statement}

In this paper, we aim to fine-tune continuous flow-based models using a reward-weighted RL method \citep{rwr,awr} to optimize a user-defined reward function $r\left(x_1\right)$. Specifically, our objective is to find the optimal generative policy model, denoted as $\pi^*$, which maximizes the expected reward throughout data generation:
\begin{equation}
\label{equ: max reward objectives}
    \pi^*=\arg \max _\pi J(\pi)=\arg \max _\pi \mathbb{E}_{x_1 \sim \pi\left(x_1\right)}\left[r\left(x_1\right)\right]
\end{equation}
where the policy $\pi$ refers to the parameterized flow matching data generative model $p^{\theta}(x_1)$, determining how data points (e.g., images) are generated, which is learned solely from scalar rewards, without ground-truth labels or filtered dataset. In fine-tuning tasks, we may also consider constrained optimization, where the distance between the pre-trained model and fine-tuned model will be constrained \citep{instructed_rlhf}. Considering the computational inefficiency of KL in continuous flow models, we instead propose a Wasserstein-2 distance to achieve that.


\subsection{Fine-Tuning Flow Matching Models with Offline Reward-Weighting}


To find the optimal data generation policy in \eqref{equ: max reward objectives}, in this section, we extend the standard CFM framework by introducing a reward-weighting mechanism to focus the model’s learning on more important or highly rewarded regions of the data space, which thus improve its performance towards user-defined reward objectives $r(x_1)$.

Let $\mathcal{X} \subseteq \mathbb{R}^d$ be the data space, and let $q(x_1)$ be the probability density function (pdf) of the random variable $x_1 \in \mathcal{X}$. We consider a continuous-time flow over $t \in [0, 1]$, where $t$ is sampled from the uniform distribution $\mathcal{U}[0,1]$. The conditional pdf of $x$ at time $t$, given $x_1$, is denoted by $p_t(x \mid x_1)$. The true conditional vector field is $u_t(x \mid x_1)$, and the model’s learned vector field is $v_t(x; \theta)$, where $\theta$ represents the model parameters. In CFM \citep{otcfm}, the goal is to learn $v_t(x; \theta)$ such that it closely approximates $u_t(x \mid x_1)$ with equal importance via \eqref{equ: cfm loss}. Based on that, we introduce a weighting function $w(x_1) \propto r(x_1): \mathcal{X} \rightarrow [0, \infty)$ into CFM loss, inducing a reward-weighted CFM method (RW-CFM) as \eqref{equ: RW-CFM loss}. Then, we can have the Theorem \ref{theorem: rwr-cfm} (Proof in App. \ref{app sec: offline RW CFM}):

\begin{Theorem}
\label{theorem: rwr-cfm}
Let $w: \mathcal{X} \rightarrow[0, \infty)$ be a measurable weighting function such that $0<Z=$ $\int_{\mathcal{X}} w\left(x_1\right) q\left(x_1\right) d x_1<\infty$ and $w(x_1) \propto r(x_1)$. Define the reward weighted CFM loss function:
\begin{equation}
\label{equ: RW-CFM loss}
    \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim q\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[w\left(x_1\right)\left\|v_t(x;\theta)-u_t\left(x \mid x_1\right)\right\|^2\right]
\end{equation}
where $w\left(x_1\right)$ is a weighting function, $q\left(x_1\right)$ is the original data distribution (e.g., data distribution induced by well-learned pre-trained models), $p_t\left(x \mid x_1\right)$ is the conditional distribution at time $t, v_t(x;\theta)$ is the model's vector field, and $u_t\left(x \mid x_1\right)$ is the true vector field conditioned on $x_1$. Then, training the model using this RW-CFM loss leads the model to learn a new data distribution:

\begin{equation}
\setlength{\abovedisplayskip}{0.5pt}
\setlength{\belowdisplayskip}{0.5pt}
    p^{\text {new }}\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}
\end{equation}

where $Z=\int w\left(x_1\right) q\left(x_1\right) d x_1$ is the normalization constant.
\end{Theorem}

Based on Theorem \ref{theorem: rwr-cfm}, it is obvious that the model trained with the RW-CFM loss will focus on high-reward regions compared to the original data distribution $q(x_1)$. However, since the training data is sampled from a static dataset $q(x_1)$, which is equivalent to an offline RL setting \citep{ddpo}, the model may not find the optimal data distribution $p^{*}$ (i.e., optimal policy $\pi^*$) that maximizes the expected reward (i.e., online and offline gap, see Fig. \ref{fig: tau control mnist}). To address this, we propose using the fine-tuned flow matching models as the data sampling policy (i.e., $q(x_1)=p^{\theta}(x_1)$), enabling an online generation of training data and thereby inducing an online RW-CFM method.

\subsection{Fine-Tuning Flow Matching Models with Online Reward-Weighting}


As discussed, to mitigate the online-offline gap and achieve better performance, we can instead adopt an online RL algorithm, where the fine-tuned FM models are used as a \textbf{data generation/sampling policy} to continuously generate new training data and explore high-value regions, inducing what we call the Online Reward-Weighted Conditional Flow Matching (ORW-CFM) method:

\begin{equation}
\label{equ: orw-cfm loss}
    \mathcal{L}_{\text{ORW-CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim p^{\theta}\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[w\left(x_1\right)\left\|v_t(x;\theta)-u_t\left(x \mid x_1\right)\right\|^2\right],
\end{equation}

In the online setting, the model can adaptively sample from the current learned data distribution, and fine-tune itself based on the newly generated samples, inducing an online policy iteration that promotes more efficient exploration of the high-reward areas in the data space. In general, the data distribution learned by the ORW-CFM method follows Theorem \ref{theorem: Online Reward Weighted CFM} (Proof in App. \ref{app sec: online RW-CFM}):

\begin{Theorem}[Online Reward Weighted CFM]
\label{theorem: Online Reward Weighted CFM}
    Let $q\left(x_1\right)$ be the initial data distribution, and $w\left(x_1\right)$ be a non-negative weighting function integrable with respect to $q\left(x_1\right)$, and proportional to user-defined reward $r(x_1)$. Assume that at each epoch $n$, the model $v_\theta$ perfectly learns the distribution implied by the ORW-CFM loss in \eqref{equ: orw-cfm loss}. Then, the learned data distribution after $N$ epochs is given by:

\begin{equation}
    q_\theta^N\left(x_1\right)=\frac{w\left(x_1\right)^N q\left(x_1\right)}{Z_N}
\end{equation}

where $Z_N=\int_{\mathcal{X}} w\left(x_1\right)^N q\left(x_1\right) d x_1$ is the normalization constant ensuring $q_\theta^N\left(x_1\right)$ is a valid probability distribution.
\end{Theorem}
Based on Theorem \ref{theorem: Online Reward Weighted CFM}, we can easily derive the limiting behavior of ORW-CFM method (Proof in App. \ref{app sec: online RW-CFM}) as the training converges (i.e., the learned generative policy collapses into an extremely exploitative/greedy policy without diversity as the case where $\alpha=0$ in Fig. \ref{fig: alpha control mnist}):
\begin{Lemma}[Limiting Case]
\label{lemma: limiting case}
    We now consider the limiting cases where $N \rightarrow \infty$:
    
    If $w\left(x_1\right) \propto r(x_1)>0$ for all $x_1 \in \mathcal{X}$ and attains its maximum rewards at $x_1^*$, then as $N \rightarrow \infty$, the learned data distribution $q_\theta^N\left(x_1\right)$ converges to a Dirac delta function centered at $x_1^*$ :


\begin{equation}
    \lim _{N \rightarrow \infty} q_\theta^N\left(x_1\right)=\delta\left(x_1-x_1^*\right)
\end{equation}
\end{Lemma}


From Lemma \ref{lemma: limiting case}, in the online setting, as training progresses, the ORW-CFM method may converge to the optimal policy that yields maximum reward without any diversity, potentially leading to a policy collapse problem (See Figs. \ref{fig: alpha control mnist}, \ref{fig: main ab study in Text-Image Alignment} and \ref{fig: text cifar alpha} where $\alpha=0$):

\begin{Corollary}[Policy Collapse/Overoptimization]
\label{corollary: main Overoptimization}
Overemphasizing high-reward regions can lead to a lack of diversity in generated samples, similar to mode collapse in GANs \citep{gan2014}, overoptimization in fine-tuning diffusion models \citep{ddpo}, or policy collapse in RL \citep{sac,rlsutton}. 
\end{Corollary}


Based on  Lemma \ref{lemma: limiting case} and Theorem \ref{theorem: Online Reward Weighted CFM}, assuming an exponential weighting function $w(x_1)=\exp(\tau * r(x_1))$ case, we can have $w\left(x_1\right)^N=\exp(N *\tau * r(x_1))$, where $\tau$ can control the convergence/collapse speed of learned policy, and $\tau \to \infty$ also induces a policy collapse problem (See Fig. \ref{fig: tau control mnist}).  To mitigate the policy collapse problem, we introduce Wasserstein-2 regularization into our method, which can bound the distance between the current learned model and the pre-trained model. This helps maintain diversity and prevents the model from collapsing into an extremely greedy policy, inducing an exploration-exploitation trade-off in fine-tuning flow matching models.

\subsection{Wasserstein-2 Regularization in Flow Matching}

To address the  policy collapse problem in the ORW-CFM method, we introduce Wasserstein-2 (W2) regularization, which helps maintain diversity by bounding the distance between the current learned model and a pre-trained reference model. This regularization induces an exploration-exploitation trade-off, allowing the model to avoid focusing solely on high-reward regions at the expense of diversity. In general, we can write the Wasserstein-2 Distance as follows \citep{wgan}:
\begin{definition}[Wasserstein-2 Distance]
    Given two probability measures $\mu$ and $\nu$ on $\mathbb{R}^n$, the squared Wasserstein- 2 distance between $\mu$ and $\nu$ is defined as:
\begin{equation}
    W_2^2(\mu, \nu)=\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathbb{R}^n \times \mathbb{R}^n}\|x-y\|^2 d \gamma(x, y)
\end{equation}
where $\Pi(\mu, \nu)$ denotes the set of all couplings of $\mu$ and $\nu$; that is, all joint distributions $\gamma$ on $\mathbb{R}^n \times \mathbb{R}^n$ with marginals $\mu$ and $\nu$.
\end{definition}


However, it is intractable to directly introduce W2 distance into our loss function. Therefore, we instead derive and introduce its upper bound as our distance regularizer (Proof in App. \ref{app sec: W2 distance}):
\begin{Theorem}[W2 Bound for Flow Matching]
\label{theorem: W2 Bound for Flow Matching}
We consider two flow matching models parameterized by $\theta_1$ and $\theta_2$, inducing time-evolving data distributions $p_t^{\theta_1}(x)$ and $p_t^{\theta_2}(x)$, respectively. The models define vector fields $v^{\theta_1}(t, x)$ and $v^{\theta_2}(t, x)$ that transport an initial distribution $p_0(x)$ to final distributions $p_1^{\theta_1}(x)$ and $p_1^{\theta_2}(x)$ at time $t=1$. Assume that $v^{\theta_2}(t, x)$ is Lipschitz continuous in $x$ with Lipschitz constant L. Then, the squared Wasserstein-2 distance between the distributions $p_1^{\theta_1}$ and $p_1^{\theta_2}$ induced by the flow matching models at time $t=1$ satisfies:
\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq e^{2 L} \int_0^1 \mathbb{E}_{x \sim p_s^{\theta_1}}\left[\left\|v^{\theta_1}(s, x)-v^{\theta_2}(s, x)\right\|^2\right] d s
\end{equation}
\end{Theorem}

Since the integral can be approximated by Monte Carlo sampling, this bound allows us to constrain the discrepancy between the learned and reference models, effectively preventing the model from collapsing into a greedy policy. By controlling the Wasserstein-2 distance, the model maintains a balance between exploring new areas of the data space and exploiting known high-reward regions in a constrained neighborhood from the pre-trained model, thus preserving diversity.




\subsection{Fine-Tuning Flow Matching Models with W2 Regularization}
The offline version of the reward-weighted method can be considered as the first step of the online version \citep{ddpo}. For simplicity, we only derive the ORW-CFM method with W2 distance regularization (i.e., ORW-CFM-W2 method) in this section, and left the offline version (i.e., RW-CFM with W2 distance regularization, namely RW-CFM-W2 method) in App. \ref{app: sec RW-CFM with W2 Distance Bound} for ease of reading.

To further improve the stability of the ORW-CFM method and prevent policy collapse, we incorporate W2 regularization into our method, which has not been widely studied \citep{ddpo}. This regularization introduces a balance between exploration and exploitation by penalizing divergence from the reference model, thus maintaining diversity in the learned model. The W2 distance bound can be  directly incorporated  into the ORW-CFM loss, inducing an ORW-CFM-W2 method:
\begin{equation}
\label{equ: orw-cfm-w2 loss}
\begin{aligned}
            \mathcal{L}_{\text{ORW-CFM-W2}} =  \mathbb{E}_{t \sim U(0,1),x_1 \sim q(x_1;\theta_{\text{ft}}), x \sim p_t(x|x_1)} [&
    w\left(x_1\right) \left\|v_t(x ; \theta_{\text{ft}})-u_t\left(x \mid x_1\right)\right\|^2 \\
    & + 
    \alpha * \left\|v^{\theta_{\text{ft}}}(x, t)-v^{\theta_{\text{ref}}}(x, t)\right\|^2 ],
\end{aligned}
\end{equation}
wherein $\alpha$ is a trade-off coefficient, $v^{\theta_{\text{ft}}}(x, t) = v_t(x ; \theta_{\text{ft}})$  and $v^{\theta_{\text{ref}}}(x, t)$ is the reference model. Then, the learned data distribution after n epochs follows the Theorem \ref{theorem: induced data distribution of ORW-CFM-W2} (Proof in App. \ref{app: sec ORW-CFM with W2 Distance Bound}):


\begin{Theorem}
\label{theorem: induced data distribution of ORW-CFM-W2}
    Under the online RL setup with ORW-CFM-W2 loss, the data distribution after n epochs evolves according to the following rules:
    \begin{equation}
        q_\theta^n\left(x_1\right) \propto\left[w\left(x_1\right) q_\theta^{n-1}\left(x_1\right) \exp \left(-\beta D^{n-1}\left(x_1\right)\right)\right],
    \end{equation}
where, $D^{n-1}\left(x_1\right)=\mathbb{E}_{t, x \sim p_t\left(x \mid x_1\right)}\left[\left\|v^{\theta^{n-1}}(t, x)-v^{\theta_{\text{ref}}}(t, x)\right\|^2\right]$, $\beta=\gamma \alpha, \gamma>0$ is a scaling constant, $\theta^{n-1}$ denotes the fine-tuned model parameters after epoch $n-1$.
\end{Theorem}


The W2 regularization term prevents the model from solely 
 collapsing into high-reward regions and ensures diversity in the learned policy (See cases where $\alpha>0$ in Figs.\ref{fig: alpha control mnist}, \ref{fig: reward-distance trade-off cifar com} and \ref{fig: text cifar alpha}), also inducing the exploration-exploitation trade-off.

In practice, we often use an exponential function or Boltzmann distribution to formulate the weighting function as $w(x_1)=\exp(\tau * r(x_1))$, where $\tau$ is an entropy coefficient that controls the entropy of the induced policy \citep{sac}. Thus, we can derive the induced data distribution for the exponential case under the ORW-CFM-W2 loss (Proof in App. \ref{app: sec exp}):

\begin{Theorem}
\label{theorem: exp case}
    Given $w\left(x_1\right)=\exp \left(\tau* r\left(x_1\right)\right)$, the induced data distribution after $N$ epochs of training under ORW-CFM-W2 Loss as \eqref{equ: orw-cfm-w2 loss} is:
\begin{equation}
    q_\theta^N\left(x_1\right) \propto \exp \left(\tau N  r\left(x_1\right)-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right) q\left(x_1\right)
\end{equation}
where, $D^{n-1}\left(x_1\right)=\mathbb{E}_{t, x \sim p_t\left(x \mid x_1\right)}\left[\left\|v^{\theta^{n-1}}(t, x)-v^{\theta_{\text {ref }}}(t, x)\right\|^2\right]$, $\beta=\gamma \alpha$, $\gamma>0$ is a scaling constant,  $\theta^{n-1}$ denotes the model parameters after epoch $n-1$.
\end{Theorem}


Based on this, we can derive the learning behavior of ORW-CFM-W2 under two limiting cases:


\begin{Case}[Dominant Regularization Term: $\alpha \to \infty$ or $\tau=0$] If $D^{n-1}\left(x_1\right)$ grows rapidly with $N$ or $\alpha$ is large, since $\beta \propto \alpha$,  the regularization term dominates, namely $\beta \sum_{n=1}^N D^{n-1}\left(x_1\right) \gg \tau* r\left(x_1\right)$. The induced distribution is heavily penalized for deviations from the reference model, potentially leading to a distribution similar to the initial distribution $q\left(x_1\right)$ (See $\tau=0$ in Fig. \ref{fig: tau control mnist}).
\end{Case}

\begin{Case}[Dominant Reward Term: $\alpha=0$ or $\tau \to \infty$]
If $D^{n-1}\left(x_1\right)$ grows slowly with $N$ or $\alpha$ is small, since $\beta \propto \alpha$, the reward term dominates the regularization term, namely $\tau* r\left(x_1\right) \gg \beta \sum_{n=1}^N D^{n-1}\left(x_1\right)$. The induced distribution $q_\theta^N\left(x_1\right)$ concentrates on the $x_1$ maximizing $r\left(x_1\right)$, similar to the case without considering $D^{n-1}\left(x_1\right)$ as Lemma \ref{lemma: limiting case} (See $\alpha=0$ in Fig. \ref{fig: alpha control mnist}).
\end{Case}

Though the Boltzmann distribution is commonly used for the weighting function to handle the normalization constant $Z$, the derivation is similar to the exponential form. For completeness, we provide the derivation of the Boltzmann form weighting function in App. \ref{app: sec Boltzmann case} for ease of reading.


Interestingly, we can interpret our ORW-CFM-W2 method from the perspective of policy iteration in RL \citep{rlsutton}, which can connect our method to KL-constrained policy optimization \citep{instructed_rlhf,awr}.  See App. \ref{app: sec rl perspective of orw-cfm-w2} for more detailed discussion. 


\section{Experiments}
\label{sec: exp}
We empirically evaluate our ORW-CFM-W2 method in \eqref{equ: orw-cfm-w2 loss} by fine-tuning small-scale FM models with a U-net architecture from TorchCFM \citep{otcfm} and large-scale FM models like Stable Diffusion 3 (SD3) \citep{sd3} from diffusers \citep{von-platen-etal-2022-diffusers} for reproducibility. In all experiments, we use the exponential weighting function described in Theorem \ref{theorem: exp case}. We also explore the impact of each component of our method and conducted detailed analyses of convergent behavior in various hyperparameter settings. More experimental details can be found in App. \ref{sec:app Experiment Details} and \ref{app: Hyper-parameters} with our Pseudocode in App. \ref{app: Pseudocode}. In general, we focus on the following questions:
\begin{itemize}
    \item \textbf{Optimal Policy Convergence:}  Can our online RL methods converge to an optimal generative data distribution that maximizes various user-defined reward objectives as we derived in Theorem \ref{theorem: Online Reward Weighted CFM} and Lemma \ref{lemma: limiting case} while the offline baseline method converges to a sub-optimal? 
    \item \textbf{Reward-Diversity Trade-off:}  Does W2 regularization effectively prevent policy collapse by maintaining a balance between reward maximization and diversity as Theorem \ref{theorem: exp case}?
    \item \textbf{Stable and Controllable Policy Optimization:} Can our method achieve a stable and controllable fine-tuning process of FM models in different settings, and control the convergent behavior of learned policy by adjusting entropy coefficient $\tau$ and W2 coefficient  $\alpha$?
\end{itemize}

\begin{figure}[!ht]
\vspace{-0.1in}
	\centering
 \subfigure[Reward Curve]{
 \includegraphics[width=0.275\linewidth]{figure/new_main/mnist/tau/reward_curve.pdf}
    }
  \subfigure[$\tau=0$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/mnist/tau_control/gen_figure/0.png}
    }
 \subfigure[$\tau=0.01$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/mnist/tau_control/gen_figure/0.01.png}
    }
    \subfigure[$\tau=10$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/mnist/tau_control/gen_figure/10.png}
    }
	\caption{Learning curve and generated images in target image generation task with different $\tau$ while $\alpha=0$. As $\tau$ increases, the convergent policy becomes increasingly greedy, and the diversity decreases. $\tau=0$ remains the similar distribution as pre-trained  model (See Theorem \ref{theorem: exp case}).}
	\label{fig: tau control mnist}
\end{figure}


\begin{figure}[!ht]
\vspace{-0.1in}
	\centering
    \subfigure[Reward Curve]{
 \includegraphics[width=0.275\linewidth]{figure/new_main/mnist/alpha/reward_curve.pdf}
    }
   \subfigure[$\alpha=0$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/mnist/alpha_control/0.png}
    }
  \subfigure[$\alpha=0.3$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/mnist/alpha_control/0.3.png}
    }
 \subfigure[$\alpha=0.8$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/mnist/alpha_control/0.8.png}
    }
	\caption{Learning curve and generated images in target image generation task with different $\alpha$ while $\tau=10$. From $\alpha=0$ to $\alpha=0.8$,  the diversity of the convergent generative distribution   increases without sacrificing  too much performance. $\alpha=0$ collapse to  a Delta Distribution as Lemma \ref{lemma: limiting case}.}
	\label{fig: alpha control mnist}
 \vspace{-0.1in}
\end{figure}

\subsection{Target Image Generation}

At first, we evaluate our ORW-CFM method, with and without W2 regularization, by fine-tuning a pre-trained model of MNIST \citep{mnist} to generate only even numbers using reward signals. The reward function is defined as $r\left(x_1\right)=p_{\text {even }}\left(x_1\right)-p_{\text {odd }}\left(x_1\right)$, calculated by a pre-trained binary classifier. An optimal expected return of 1 indicates the model exclusively generates even numbers.


Figs. \ref{fig: tau control mnist} and \ref{fig: alpha control mnist} demonstrate that our method quickly reaches near-optimal performance across different settings of the entropy coefficient $\tau$ and regularization coefficient $\alpha$. From Fig. \ref{fig: tau control mnist}, increasing $\tau$ leads to a more greedy, reward-maximizing policy at the cost of diversity, consistent with the theoretical limits in Theorem \ref{theorem: exp case} and Lemma \ref{lemma: limiting case}, while $\tau=0$ remains a similar performance as the pre-trained model. Besides, from Fig. \ref{fig: alpha control mnist}, W2 regularization controlled by $\alpha$ remains diverse by controlling how far the fine-tuned model diverges from the pre-trained model, enabling a balance between exploration and exploitation. In general, ORW-CFM, especially with W2 regularization, achieves a stable fine-tuning process via maximizing the rewards in a constrained neighborhood of the pre-trained model, inducing a flexible balance between reward and diversity, avoiding extremely exploitative policies.


\subsection{Image Compression}

\begin{figure}[!ht]
\vspace{-0.1in}
	\centering
   \subfigure[$\alpha=0$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/cifar_compression/alpha_control/0.png}
    }
  \subfigure[$\alpha=0.5$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/cifar_compression/alpha_control/0.5.png}
    }
 \subfigure[$\alpha=10$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/cifar_compression/alpha_control/10.png}
    }
       \subfigure[pre-trained model]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/cifar_compression/alpha_control/ref.png}
    }

    \subfigure[Reward Curve]{
 \includegraphics[width=0.3\linewidth]{figure/new_main/cifar_com/alpha/reward_tradeoff.pdf}
    }
  \subfigure[W2 Curve]{
 \includegraphics[width=0.3\linewidth]{figure/new_main/cifar_com/alpha/w2_tradeoff.pdf}
    }
    \vspace{-0.1in}
	\caption{Reward-Diversity Trade-off and W2 Distance Control via $\alpha$ in Image Compression Task \citep{ddpo} of CIFAR-10 \citep{cifar10} with $\tau=1$. W2 distance is estimated by its upper bound (see App. \ref{app sec: W2 distance}). Each point in (e) and (f) corresponds to one group of experiments with $\alpha$ varying from 0 to 1. As $\alpha$ increases, the final reward decays while distance between the fine-tuned model and the reference model become closer, inducing a controllable fine-tuning process.}
	\label{fig: reward-distance trade-off cifar com}
\end{figure}


The effect of W2 regularization and exploration-exploitation trade-off is particularly evident in the image compression task using the model pre-trained in CIFAR-10, as shown in Figs. \ref{fig: reward-distance trade-off cifar com}. In Fig. \ref{fig: reward-distance trade-off cifar com}, we see that the introduction of W2 regularization prevents the policy from collapsing into an extremely greedy solution. As $\alpha$ increases, the model explores optimal solutions within a constrained neighborhood of the pre-trained model,  preserving diversity while still optimizing the reward.


The reward-distance trade-off curve in Fig. \ref{fig: reward-distance trade-off cifar com} further highlights this balance. Varying $\alpha$ provides an explicit trade-off between maximizing the reward and minimizing the divergence from the pre-trained model. As $\alpha$ decreases, the policy becomes more exploitative, potentially collapsing into a Delta distribution, as described in Lemma \ref{lemma: limiting case}. Conversely, increasing $\alpha$ encourages broader exploration of the data space, ensuring a more diverse generative policy. Therefore, proper W2 regularization effectively balances exploration and exploitation, avoiding policy collapse.



\begin{figure}[!ht]
	\centering
 \includegraphics[width=0.65\linewidth, height=0.2\textheight]{figure/app/sd3/dpok/positional/postional_prompts_final.png}
	\caption{General Comparison of Different Fine-tuning Methods on SD3 via CLIP Rewards.}
    \label{fig: main Text-Image Alignment with Spatial Understanding}
\end{figure}




\subsection{Text-Image Alignment}



\paragraph{Aligning Large-Scale Flow Matching Models} To further validate our method's effectiveness, we evaluate our ORW-CFM-W2 method on fine-tuning Stable Diffusion 3 (SD3) \citep{sd3}. Besides, we adopt LoRA \citep{lora} for efficient fine-tuning. As shown in Figure \ref{fig: main Text-Image Alignment with Spatial Understanding}, we compare our method against RAFT \citep{Raft} and ReFT \citep{ReFT} on spatial relationship prompts used in DPOK \citep{dpok}. Our approach demonstrates better positional relationship control while maintaining image quality and semantic coherence. On challenging prompts like "a banana on the left of an apple" and "a cat on the left of a dog", our method consistently generates images with correct object placement and natural appearances, outperforming previous approaches in terms of both alignment accuracy and generation quality. Results on these complex compositional prompts further demonstrate our approach's capability in handling multiple semantic constraints while maintaining generation diversity. See App. \ref{app: Text-Image Alignment Experiments of Stable Diffusion 3} for more details and additional results.


\begin{figure}[!ht]
	\centering
 \includegraphics[width=0.65\linewidth, height=0.2\textheight]{figure/app/sd3/cat/multi_baseline/cat_multi_method.png}
	\caption{Ablation Studies on Text-Image Alignment Tasks of SD3. All use CLIP rewards.}
    \label{fig: main ab study in Text-Image Alignment}
\end{figure}

\paragraph{Ablation Results} 
The ablation studies in Figure \ref{fig: main ab study in Text-Image Alignment} demonstrate several key findings that align with our theoretical predictions. Our ORW-CFM method, even without W2 regularization ($\alpha=0$), achieves better semantic alignment than other online methods like RAFT and ReFT, though all methods without W2 regularization exhibit clear signs of policy collapse, generating nearly identical images despite achieving high rewards - empirically validating our theoretical prediction in Lemma \ref{lemma: limiting case}. The introduction of W2 regularization (RAFT+W2, ReFT+W2, and our ORW-CFM-W2 method, $\alpha=1$) successfully prevents this collapse while maintaining performance, validating our tractable W2 distance bound in Theorem \ref{theorem: W2 Bound for Flow Matching}. Our method achieves the best balance between reward optimization and diversity preservation, with generated images showing consistent semantic understanding of "cat in sky" while maintaining natural variations in compositions. See App. \ref{app: Policy Collapse in Text-Image Alignment} for more details.



\section{Conclusion}
In this paper, we present the first theoretically-grounded online RL framework for fine-tuning flow matching models - Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2) without requiring calculations of ELBO, exact likelihood or KL divergence, which are intractable in ODE-based flow matching models. Our work provides the first theoretical analysis of convergent behavior in flow matching online fine-tuning, proving that unregularized methods inevitably converge to a Delta distribution (Lemma \ref{lemma: limiting case}), inducing policy collapse that maximizes rewards at the cost of diversity. This theoretical discovery motivates our second major contribution - the derivation of a tractable Wasserstein-2 regularization that can be directly computed from vector fields, providing an efficient way to prevent collapse while maintaining diversity. Extensive experiments across target image generation, image compression, and text-image alignment demonstrate that our method achieves optimal convergence while maintaining stable learning, with ablation studies validating both the online reward-weighting mechanism for optimal convergence and W2 regularization for diversity preservation. See App. \ref{app: Discussion and Limitation} for more discussions.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}


\clearpage
\appendix


\section{Text-Image Alignment Experiments of Stable Diffusion 3}
\label{app: Text-Image Alignment Experiments of Stable Diffusion 3}

In this section, to further empirically demonstrate our performance on fine-tuning large-scale generative models like the stable diffusion series \citep{sd3}, we conduct extensive experiments on fine-tuning Stable Diffusion 3 (SD3) \citep{sd3}, which adopts a flow matching architecture \citep{fmgm,sd3}. For reproducibility, we built our code on the open-sourced diffuser codebase \citep{von-platen-etal-2022-diffusers}, and adopted LoRA \citep{lora} and fp16 precision to reduce GPU memory requirements. Unless otherwise specified, we use $\alpha=1, \tau=0.1$ for all experiments in this section.

To verify the effectiveness of our method, we primarily use the raw CLIP score (i.e., logit) \citep{clip} as our rewards for text-image alignment for SD3 fine-tuning. We also demonstrate our method's adaptability by extending it to various other text-image alignment reward models, including HPS-V2 \citep{hpsv2}, Pick Score \citep{pic_a_score}, and Alpha Clip \citep{Alpha_clip}. Notably, our method learns entirely from self-generated data \citep{Raft,ai_collapse_nature} without requiring manually collected \citep{instructed_rlhf} or filtered datasets \citep{dpo}, while effectively preventing policy collapse through our theoretically-derived W2 regularization (See App. \ref{app sec: W2 distance}).



\subsection{Text-Image Alignment with Spatial Understanding}
\label{app: Text-Image Alignment with Spatial Understanding}

\begin{figure}[!ht]
	\centering
 \includegraphics[width=\linewidth]{figure/app/sd3/dpok/positional/postional_prompts_final.png}
	\caption{General comparison of positional relationship understanding between different fine-tuning methods on SD3 \citep{sd3}. We compare our proposed ORW-CFM-W2 method against RAFT \citep{Raft}, ReFT (online variants, training on self-generated data) \citep{ReFT}, and the original SD3 model \citep{sd3} across various positional relationship prompts from DrawBench \citep{drawbench} used in DPOK \citep{dpok}. Each column shows generations for a specific prompt testing spatial understanding (e.g., ``left of'', ``on top of'', ``underneath''). The results demonstrate that our method achieves better positional relationship control while maintaining image quality and semantic coherence. This suggests that our Wasserstein-2  regularized online reward-weighted approach effectively fine-tunes flow-based models to better understand and generate complex spatial relationships between objects. Model generations shown are sampled with the same random seeds across methods for fair comparison.}
    \label{fig: Text-Image Alignment with Spatial Understanding}
\end{figure}


In the first experiments, we aim to improve the model's understanding of spatial relationships between objects, a challenging task that requires both semantic understanding and precise positional control.

To demonstrate the effectiveness of our method, we use the raw CLIP score (i.e., logits) \citep{clip,ImageReward,Raft} directly as our text-image alignment reward model, eliminating the need for probability conversion or complex reward shaping \citep{r2d2}.  This straightforward approach further demonstrates our method's ability to optimize directly from raw similarity scores, making it more practical for real-world applications \citep{Raft,clip}.

Figure \ref{fig: Text-Image Alignment with Spatial Understanding} (i.e., Same as Figure \ref{fig: main Text-Image Alignment with Spatial Understanding}, but with more detailed discussion for ease of reading.) compares our method against several baselines: RAFT \citep{Raft}, ReFT (online variants, training on self-generated data) \citep{ReFT}, and the original SD3 model. The results demonstrate several key advantages of our approach:

\begin{enumerate}
    \item Spatial Semantic Coherence/Alignment: Our method shows superior understanding of spatial relationships (``left of'', ``on top of'', ``underneath'') while maintaining high image quality. This validates our theoretical analysis that ORW-CFM-W2 can effectively optimize arbitrary reward functions without compromising generation quality.
    
    \item Controlled Optimization: As shown in  Figure \ref{fig: alpha control mnist} and Figure \ref{fig: Text-Image Alignment with Spatial Understanding}, online RL methods trained on self-generated data \citep{Raft,ReFT,ddpo,ai_collapse_nature} normally face great challenges of policy collapse, where models converge to limited, homogeneous outputs that maximize rewards but lack diversity - a phenomenon theoretically characterized in Lemma \ref{lemma: limiting case}. To address this challenge, we introduce W2 regularization, which effectively prevents over-optimization and policy collapse (Lemma \ref{lemma: limiting case}). Our theoretical analysis in Theorem \ref{theorem: exp case} demonstrates that this regularization enables a controlled trade-off between reward maximization and output diversity (See Figure \ref{fig: reward-distance trade-off cifar com}). The experimental results validate this prediction, showing consistent spatial relationships while maintaining natural variations in object appearances, styles, and compositions.
    
    \item Easy-to-Use and Stable Fine-tuning Method: Our method achieves these improvements without requiring filtered datasets \citep{dpo}, likelihood calculations \citep{ddpo} or differentiable rewards \citep{adjoint_matching}, demonstrating the practical advantages of our reward-weighted flow matching framework. This aligns with our theoretical framework that bypasses the computational challenges of likelihood estimation in continuous flow models.
\end{enumerate}


In general, these results further validate our theoretical contributions in a practical, large-scale setting. The successful fine-tuning of SD3 demonstrates that our method can effectively balance between reward maximization and diversity preservation, as predicted by our reward-distance trade-off analysis (Theorem \ref{theorem: induced data distribution of ORW-CFM-W2} and \ref{theorem: exp case}). Furthermore, the stable learning behavior supports our theoretical analysis of the convergence properties under W2 regularization. These experiments also highlight the practical significance of our two key innovations: the online reward-weighting mechanism and the W2 regularization. The online approach allows the model to continuously explore and improve spatial understanding, while the W2 regularization prevents the collapse into simplistic or degenerate solutions - a common challenge in fine-tuning large generative models \citep{Raft,ddpo,ai_collapse_nature}.




\subsection{Ablation Studies And Policy Collapse Analysis}
\label{app: Policy Collapse in Text-Image Alignment}



\begin{figure}[!ht]
	\centering
 \includegraphics[width=\linewidth]{figure/app/sd3/cat/multi_baseline/cat_multi_method.png}
	\caption{Comparison of different fine-tuning methods on fine-tuning SD3 for the prompt ``a cat in the sky'' (reward is also the CLIP score \cite{clip,ImageReward,Raft}). The results demonstrate that policy collapse frequently occurs during online RL fine-tuning \citep{ddpo,Raft,ai_collapse_nature}. The baseline SD3 \cite{sd3} shows limited spatial/semantic understanding for unusual prompts (i.e., hard to see in our normal life). While RAFT \citep{Raft} and ReFT \citep{ReFT} without W2 regularization achieve high rewards (e.g., successful cat placement), they exhibit clear signs of policy collapse, generating nearly identical images with limited diversity. Our method without W2 regularization (Ours no W2) similarly suffers from policy collapse (also see Figure \ref{fig: tau control mnist}), aligning with our theoretical prediction of the convergent behavior in Lemma \ref{lemma: limiting case}. In contrast, methods incorporating W2 regularization (RAFT+W2, ReFT+W2, and Ours) maintain generation diversity while successfully positioning cats in the sky. Our approach is particularly good at balancing reward optimization with diversity preservation, demonstrating the effectiveness of our theoretically-derived W2 regularization bound (Theorem \ref{theorem: W2 Bound for Flow Matching}) in preventing policy collapse while maintaining high-quality generations. The results empirically validate our theoretical analysis of the reward-diversity trade-off in Theorem \ref{theorem: exp case}.}
    \label{fig: Policy Collapse in Text-Image Alignment}
\end{figure}







In the second part of SD3 fine-tuning experiments, to empirically validate our theoretical analysis of policy collapse in online RL fine-tuning as Lemma \ref{lemma: limiting case} and demonstrate the effectiveness of our W2 regularization via ablation, we further fine-tune SD3 models to align out-of-distribution (OOD) prompt "a cat in the sky". This setup provides an ideal test case for examining both the policy collapse phenomenon and our method's ability to maintain diversity while optimizing rewards in different online methods. The ablation results have been concluded in Figure \ref{fig: Policy Collapse in Text-Image Alignment}.

\paragraph{Effectiveness of Online Reward Weighting (ORW-CFM)} The ablation studies in Figure \ref{fig: Policy Collapse in Text-Image Alignment} demonstrate that our ORW-CFM method, even without W2 regularization ($\alpha=0$), achieves better semantic alignment than other online methods like RAFT \citep{Raft} and ReFT \citep{ReFT}. This superior performance validates our theoretical framework for online reward-weighted fine-tuning of flow matching models (See Lemma Optimal Convergent Behavior in Lemma \ref{lemma: limiting case}). The method successfully optimizes for the desired semantic content, achieving higher-quality generations that better match the prompt "cat in sky" compared to baseline approaches.


\paragraph{Policy Collapse Phenomenon} However, in Figure \ref{fig: Policy Collapse in Text-Image Alignment}, all methods without W2 regularization (RAFT, ReFT, and Ours without W2) exhibit clear signs of policy collapse \citep{ai_collapse_nature,ddpo}, generating nearly identical images despite achieving high rewards. This empirically validates our theoretical prediction in Lemma \ref{lemma: limiting case} that unregularized online reward-weighted methods naturally converge to a delta distribution at the maximum reward point, sacrificing diversity for reward maximization.


\textbf{Effectiveness of W2 Regularization} Based on Figure \ref{fig: Policy Collapse in Text-Image Alignment} and Table \ref{tab:cat_ab_study}, the introduction of W2 regularization (RAFT+W2, ReFT+W2, and our ORW-CFM-W2 method, $\alpha=1$) demonstrates the effectiveness of our theoretically-derived tractable W2 distance bound from Theorem \ref{theorem: W2 Bound for Flow Matching}. This regularization successfully prevents policy collapse while maintaining performance across all methods, enabling controlled divergence from the reference model. The consistent improvement in output diversity across different baseline methods validates our theoretical framework for controlling model behavior through W2 regularization in flow matching architectures.



\paragraph{Optimal Balance Through ORW-CFM-W2} In Figure \ref{fig: Policy Collapse in Text-Image Alignment}, our ORW-CFM-W2 method achieves the best balance between reward optimization and diversity preservation - generated images show consistent high-level semantic understanding of "cat in sky" while maintaining natural variations in cat appearances, sky conditions, and overall compositions. While both RAFT+W2 and ReFT+W2 show diversity improvements from W2 regularization, their outputs still exhibit less variation in style and composition compared to ORW-CFM-W2. This improved performance aligns with our theoretical analysis in Theorem \ref{theorem: exp case} regarding the reward-diversity trade-off in the exponential weighting case. The results also validate our optimization framework's ability to achieve stable convergence without collapsing into extreme exploitation.

\paragraph{Theoretical and Practical Validation} These experimental results in Figure \ref{fig: Policy Collapse in Text-Image Alignment} provide strong empirical validation of both key components of our approach. First, they demonstrate the ORW-CFM method's ability to achieve optimal policy convergence, though with potential collapse if unregularized (as predicted by our theoretical analysis of online reward-weighted learning). Second, they show how W2 regularization effectively prevents this collapse while preserving output quality and diversity - a capability derived from our tractable W2 distance bound for flow matching. The comprehensive improvement across different baseline methods \citep{Raft,ReFT} illustrates how our theoretical framework enables stable fine-tuning that maximizes rewards while maintaining the rich generative capabilities of flow-based models \citep{fmgm,sd3}.






\begin{table}[!ht]
\caption{ Performance and Diversity comparison of different fine-tuning methods on text-image alignment using SD3. 'CLIP Score' measures alignment with the text prompt using the CLIP model's similarity score (higher is better). 'Diversity Score' quantifies the diversity of generated images by measuring the mean pairwise distance between CLIP embeddings of generated samples (higher indicates more diverse outputs). We use Euclidean distance to calculate the distance between any two embeddings when calculating mean pairwise distance. Our method achieves the highest CLIP score while maintaining strong diversity comparable to the base SD3 model. The addition of W2 regularization ('+W2' variants) helps preserve generation diversity across all methods, with our full approach (Ours) achieving the best balance between alignment (35.46) and diversity (4.21) compared to baseline methods RAFT and ReFT. All scores are calculated by averaging across 64 samples.}
\begin{center}
\begin{tabular}{lccc}
\toprule
Method & CLIP Score $\uparrow$  & Diversity Score $\uparrow$&  \\
\midrule
\multicolumn{3}{c}{\textit{Effectiveness of ORW-CFM without W2}}\\
SD3 (Baseline)     & 28.69  &  \textbf{4.78}    \\
ORW-CFM (Ours w/o W2) & \textbf{33.63} & 3.47  \\
RAFT    & 29.30 & 2.05    \\ 
ReFT    & 29.32 & 3.26   \\
\midrule
\multicolumn{3}{c}{\textit{Effectiveness of W2 regularization}}\\
ORW-CFM-W2 (Ours)    & \textbf{35.46} &  \textbf{4.21} \\
RAFT + W2 & 30.88 & 2.81  \\
ReFT + W2 & 32.03 & 3.63  \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:cat_ab_study}
\end{table}



\subsection{Adaptability Across Different Reward Models}


\begin{figure}[!ht]
	\centering
 \includegraphics[width=\linewidth]{figure/app/sd3/dpok/multi_rewards/multi_prompt_multi_reward.png}
	\caption{Comparison of fine-tuning methods for the prompt ``a train on top of a surfboard'', demonstrating our method's adaptability across different reward models (HPS-V2 \cite{hpsv2}, Pick Score \cite{pic_a_score}, and Alpha Clip scores \cite{Alpha_clip}). The results validate our theoretical analysis that ORW-CFM-W2 can effectively optimize diverse reward functions while maintaining generation quality through W2 regularization. 1) Top row: Using HPS-V2 reward shows our method successfully aligns locomotive placement while preserving realistic wave interactions. 2) Middle row: Pick Score reward further demonstrates robust spatial understanding across varying train models and surfboard configurations. 3) Bottom row: Alpha Clip reward showcases consistent performance even with different text-image alignment rewards, validating the reward-agnostic nature/property of our approach. This comprehensive evaluation across multiple reward models empirically confirms our theoretical prediction that the W2 regularization (Theorem \ref{theorem: W2 Bound for Flow Matching}) enables stable fine-tuning regardless of the underlying reward mechanism. The consistent high-quality generations across all three reward models demonstrate the practical advantages of our reward-weighted flow matching framework, which achieves strong performance without requiring filtered datasets \citep{dpo}, likelihood calculations \citep{ddpo},  reward-specific optimizations \citep{Raft,ReFT}, or differentiable rewards \citep{adjoint_matching}. These results particularly highlight how our method's reward-diversity trade-off (Theorem \ref{theorem: exp case}) generalizes effectively across different reward models while preventing policy collapse (Lemma \ref{lemma: limiting case}).}
    \label{fig: Adaptability Across Different Reward Models}
\end{figure}


In addition to the three reward functions in Section \ref{sec: exp}, to demonstrate the broad applicability and reward-agnostic nature/property of our methods, we further evaluate our performance using three different text-image alignment reward models: HPS-V2 \citep{hpsv2}, Pick Score \citep{pic_a_score}, and Alpha Clip \citep{Alpha_clip}. Using the challenging Spatial prompt ``a train on top of a surfboard,'' we examine how our method adapts to different reward signals/models while maintaining generation quality and semantics coherence.


In general, the results shown in Figure \ref{fig: Adaptability Across Different Reward Models} demonstrate several key strengths of our approach:

\textbf{Reward Adaptability:} Our method shows consistent performance improvement across all three reward models, successfully positioning trains on surfboards while maintaining realistic scene composition. This empirically validates our theoretical framework's ability to optimize arbitrary reward functions without requiring reward-specific modifications \citep{Raft} or filtered datasets \citep{dpo}. The high-quality generations across different reward models demonstrate that our W2 regularization (Theorem \ref{theorem: W2 Bound for Flow Matching}) provides effective stabilization regardless of the underlying reward mechanism (It is worth mentioning that appropriately increasing the $\alpha$ value can be used to adjust the distance from the reference model, which is also demonstrated in Figure \ref{fig: reward-distance trade-off cifar com}).

\textbf{Spatial Understanding:} Across all reward models, our method demonstrates robust spatial understanding:

\begin{enumerate}
    \item \textbf{HPS-V2:} Achieves precise locomotive placement while capturing realistic wave interactions.
    \item \textbf{Pick Score:} Maintains consistent spatial relationships across varying train models and surfboard configurations.
    \item \textbf{Alpha Clip:} Shows stable performance even with a different similarity metric framework.
\end{enumerate}


\textbf{Diversity Preservation:} Consistent with our theoretical analysis in Theorem \ref{theorem: exp case}, the generated images maintain natural variations in train appearances, surfboard designs, and ocean conditions while adhering to the spatial constraints. This demonstrates how our W2 regularization effectively prevents policy collapse (Lemma \ref{lemma: limiting case}) regardless of the reward models used.

\textbf{Stable Fine-tuning: }The consistent quality/alignment improvement across reward models validates our theoretical contribution regarding the convergence behavior of ORW-CFM-W2 (See App. \ref{app: sec ORW-CFM with W2 Distance Bound}). As predicted by our analysis of the exponential weighting case (Theorem \ref{theorem: exp case}), the method achieves stable learning without collapsing into extreme exploitation, even when faced with different reward scales and architectures. It is also worth mentioning that the reward model is very important for the success of the fine-tuning task \citep{Raft,ddpo,dpo,ImageReward}, so how to select the best reward model still needs further exploration. 

All in all, the successful adaptation across multiple reward models showcases how our online reward-weighting mechanism and W2 regularization work together to enable robust, stable fine-tuning without requiring reward-specific optimizations. This generality makes our method particularly valuable for real-world applications where different reward models might be needed for different tasks or domains \citep{ddpo,ai_collapse_nature}.

Furthermore, these results address a key challenge in flow matching fine-tuning: \textbf{the ability to optimize diverse reward objectives without compromising generation quality or diversity while training on self-generated data}. Our method's consistent performance improvement across reward models demonstrates that the reward-distance trade-off formulation (See Theorem \ref{theorem: exp case}) provides a easy-to-use and theoretically-grounded approach to balancing task-specific optimization with output diversity, regardless of how the reward is computed.


\subsection{Compositional and Complex Semantic Understanding}
\label{app: Compositional Semantic Understanding}

\begin{figure}[!ht]
	\centering
 \includegraphics[width=\linewidth]{figure/app/sd3/dpok/Semantic_details/Semantic_details.png}
	\caption{ Additional evaluation of ORW-CFM-W2's semantic understanding and compositional generation capabilities on complex structured prompts from DrawBench \citep{drawbench} used in DPOK \citep{dpok}. Each column demonstrates our method's ability to handle multi-faceted requirements including spatial relationships, color specifications (red, green), and detailed object attributes. The figure compares baseline SD3 \citep{sd3} samples with our ORW-CFM-W2 results across three challenging compositional prompts: stacked colored cubes, stacked colored plates, and a customized panda emoji. Our method shows consistently better semantic alignment while maintaining generation diversity, validating our theoretical framework's ability to prevent mode collapse \citep{ai_collapse_nature} through W2 regularization.}
    \label{fig: Compositional Semantic Understanding}
\end{figure}

In the fourth experiments of fine-tuning SD3 models \citep{sd3}, to further validate our method's capability in handling complex semantic relationships and demonstrate the practical benefits of our theoretical framework, we evaluate ORW-CFM-W2 on a set of challenging compositional prompts that test multiple aspects of semantic understanding. These experiments not only demonstrate our method's effectiveness on large-scale flow matching models like SD3, but also empirically validate our theoretical predictions about controlled optimization and diversity preservation. We use CLIP score as our text-image alignment reward model \citep{clip,ImageReward,Raft}.

\textbf{Compositional Attribute Control:} As shown in Figure \ref{fig: Compositional Semantic Understanding}, our method successfully handles prompts with multiple interleaved requirements spanning \textbf{spatial relationships, color specifications, and object attributes}. In the cube-stacking example, the model demonstrates precise control over both spatial ("on top", "in the middle", "on the bottom") and color attributes (two red cubes on a green cube), while maintaining physically plausible stacking arrangements. This validates that our ORW-CFM-W2 can effectively achieve fine-grained fine-tuning without falling into degenerate solutions.

\textbf{Spatial Color Relationships:} The stacked plates example particularly highlights our method's ability to jointly optimize multiple constraints - positional relationships (top, middle, bottom) and color attributes (blue plates on top/middle, green plate on bottom). The consistent high-quality results validate our theoretical prediction that our methods enables stable fine-tuning even when dealing with complex, spatially-dependent color constraints.

\textbf{Multi-attribute Character Specification:} The panda emoji example demonstrates fine-grained control over multiple clothing attributes (red hat, green gloves, red shirt, green pants) while maintaining the core semantic concept and cartoon-style consistency. This success in handling detailed attribute specifications without semantic drift empirically supports our analysis of the reward-diversity trade-off (Theorem \ref{theorem: induced data distribution of ORW-CFM-W2}), showing how our method balances attribute-level precision with overall coherence.

In general, across all test cases, our method achieves notably better semantic alignment compared to the baseline SD3 samples while maintaining image quality and stylistic diversity. This improvement is particularly evident in cases requiring precise attribute combinations or spatial relationships, where baseline samples often exhibit attribute mixing or spatial inconsistencies. The results demonstrate that our theoretically-grounded approach effectively addresses the policy collapse problem (Lemma \ref{lemma: limiting case}) through W2 regularization, enabling precise semantic control without sacrificing generation diversity.
These results offer strong empirical validation of our theoretical framework. The successful handling of complex compositional prompts demonstrates that our online reward-weighting mechanism and W2 regularization work together to enable robust, stable fine-tuning of large-scale flow matching models. This generality makes our method particularly valuable for real-world applications requiring precise semantic control and attribute specification while maintaining natural variations in the generated outputs.
All in all, the consistent performance across diverse prompt types demonstrates our ability to optimize multiple semantic objectives without compromising generation quality or diversity. Our method's success in handling complex compositional prompts \citep{drawbench,dpok} while maintaining coherent generations provides strong empirical support for our theoretical contributions while demonstrating the practical utility of our approach in real-world applications.



\clearpage





\section{Background}

\subsection{Reinforcement Learning}

\subsubsection{RL Formulation}
In RL \citep{rlsutton}, an agent interacts with an environment by taking actions $a \in \mathcal{A}$ in states $s \in \mathcal{S}$, aiming to maximize cumulative rewards over time. The agent's behavior is governed by a policy $\pi(a \mid s)$, which represents the probability of taking action $a$ given state $s$. The main goal of RL is to optimize the policy to maximize the expected return:

\begin{equation}
    J(\pi)=\mathbb{E}_\pi\left[\sum_{t=0}^T \gamma^t r\left(s_t, a_t\right)\right]
\end{equation}

where $\gamma \in[0,1]$ is the discount factor that prioritizes early rewards, $T$ is the time horizon (which could be infinite), $r\left(s_t, a_t\right)$ is the reward at time step $t$.

The core problem in RL is how to learn the optimal policy $\pi^*(a \mid s)$ that maximizes the expected return $J(\pi)$. A special case of RL, where there is no states but only actions for each agent, is known as the multi-armed bandit problem \citep{rlsutton}.

\subsubsection{Reward-Weighted Regression}

In Reward-Weighted Regression (RWR), the policy update is done by re-weighting the probability of actions based on the rewards they produce. The updated policy is given by:

\begin{equation}
    \pi_{\text {new }}(a \mid s) \propto \pi_{\text {old }}(a \mid s) \cdot \exp (\tau * r(s, a))
\end{equation}

where $\tau$ is a temperature parameter controlling the trade-off between exploration and exploitation.

\subsubsection{Advantage-Weighted Regression}
Advantage-Weighted Regression (AWR) refines RWR by weighting actions based on their advantage, which measures how much better an action is compared to the expected value of other actions:

\begin{equation}
    \pi_{\text {new }}(a \mid s) \propto \pi_{\text {old }}(a \mid s) \cdot \exp (\tau A(s, a))
\end{equation}

where $A(s, a)=r(s, a)-V(s)$ is the advantage function.

In practice, we can also call the data-collection policy, the behavior policy $\mu$ \citep{rlsutton,ppo,awr}, and the optimal target policy $\pi^{\star}$.

Then, the update rules of AWR becomes:
\begin{equation}
        \pi_{\text {new }}(a \mid s) \propto \mu(a \mid s) \cdot \exp (\tau A(s, a))
\end{equation}

And the optimal policy is therefore given by \citep{awr},
\begin{equation}
    \pi^*(\mathbf{a} \mid \mathbf{s})=\frac{1}{Z(\mathbf{s})} \mu(\mathbf{a} \mid \mathbf{s}) \exp \left(\tau *A^\mu\left(\mathbf{s}_t, \mathbf{a}_t\right)\right)
\end{equation}
where $Z(s)$ is the partition function that normalizes
the conditional action distribution.




\subsection{Flow Matching}
\subsubsection{Flow Matching Formulation}
Flow Matching \citep{fmgm} is a methodology for training continuous normalizing flows, which are generative models that transform a simple base distribution into a complex target distribution through a continuous, invertible mapping. The core idea of FM is to define a continuous flow that gradually transports samples from an initial distribution $p_0(x)$ (usually a simple distribution like a standard Gaussian) to a target distribution $p_1(x)$ (the data distribution we aim to model).

This transformation is governed by an ordinary differential equation:

\begin{equation}
    \frac{d x}{d t}=u_t(x)
\end{equation}

where $x \in \mathbb{R}^d$ is a data point in $d$-dimensional space, $t \in[0,1]$ is the time parameter, and $u_t(x)$ is a time-dependent vector field that dictates the dynamics of the flow at each point in time and space.

The solution to this ODE defines a flow $\phi_t(x)$ that maps a point $x$ at time $t$ to another point at time $t+\delta t$. The transformation over the entire time interval $[0,1]$ effectively transports the entire distribution $p_0(x)$ to $p_1(x)$.

The evolution of the probability density $p_t(x)$ over time is described by the continuity equation:

\begin{equation}
    \frac{\partial p_t(x)}{\partial t}+\nabla \cdot\left(p_t(x) u_t(x)\right)=0\label{eqn:continuity}
\end{equation}

where $\nabla$. denotes the divergence operator.
The goal in FM is to learn the vector field $u_t(x)$ such that the flow induced by $u_t(x)$ transforms $p_0(x)$ into $p_1(x)$ as $t$ progresses from 0 to 1.

\subsubsection{Conditional Flow Matching}

Conditional Flow Matching \citep{fmgm} is an extension of FM that introduces conditioning to simplify the learning process. Instead of modeling the entire distribution transformation directly, CFM focuses on modeling the conditional dynamics of the flow given a target sample $x_1$ from the data distribution.

In CFM, we consider pairs $\left(x_0, x_1\right)$, where $x_0 \sim p_0(x)$ and $x_1 \sim p_1(x)$. The conditional flow transports $x_0$ to $x_1$ over time $t \in[0,1]$ using a vector field $u_t\left(x \mid x_1\right)$ that depends on $x_1$ :

\begin{equation}
    \frac{d x}{d t}=u_t\left(x \mid x_1\right)
\end{equation}

The objective in CFM is to learn a parametric model $v_\theta(t, x)$ that approximates $u_t\left(x \mid x_1\right)$. The loss function used to train the model is:

\begin{equation}
    \mathcal{L}_{\mathrm{CFM}}(\theta)=\mathbb{E}_{t, x_1, x}\left[\left\|v_\theta(t, x)-u_t\left(x \mid x_1\right)\right\|^2\right]
\end{equation}

where the expectation is taken over $t \sim \mathcal{U}(0,1), x_1 \sim p_1(x)$, and $x \sim p_t\left(x \mid x_1\right)$, the conditional distribution at time $t$ given $x_1$.

By conditioning on $x_1$, CFM reduces the complexity of the learning problem, as the model only needs to learn how to transform $x_0$ to $x_1$ for given pairs, rather than modeling the entire joint distribution.


\subsubsection{OT-CFM}

Our baseline code is built upon torchcfm \citep{otcfm}, where we use OT-CFM loss to learn flow matching models (i.e., OT-paths). Optimal Transport theory provides a framework for finding the most efficient way to transport one probability distribution to another \citep{otcfm}. OT-CFM integrates optimal transport into the conditional flow matching framework to define vector fields based on optimal transport maps.

In OT-CFM \citep{otcfm}, the vector field $u_t\left(x \mid x_1\right)$ is derived from the optimal transport plan that minimizes the cost of transporting $p_0(x)$ to $p_1(x)$. The cost is typically defined in terms of the expected squared Euclidean distance between transported points.

By using optimal transport, OT-CFM ensures that the flow from $x_0$ to $x_1$ follows the most efficient path, which can improve the convergence and quality of the generative model. The OT-CFM loss function remains similar to the CFM loss but incorporates the optimal transport-based vector field:

\begin{equation}
    \mathcal{L}_{\mathrm{OT}-\mathrm{CFM}}(\theta)=\mathbb{E}_{t, x_1, x}\left[\left\|v_\theta(t, x)-u_t^{\mathrm{OT}}\left(x \mid x_1\right)\right\|^2\right]
\end{equation}

where $u_t^{\mathrm{OT}}\left(x \mid x_1\right)$ represents the optimal transport vector field.

\subsubsection{Likelihood Calculation}
\label{app: likelihood cal}

As a special case of CNF, flow matching models share the benefit of exact likelihood calculation. Specifically, the learned vector field induces a probability path induced by the pushforward:
\begin{equation}
    p_t:=(\psi_t)_*p_0
\end{equation}
where $p_0$ is the initial noise distribution and $\psi_t(x):=x_t$ is the flow governed by the learned vector field and the flow ODE as $\frac{dx_t}{dt}=v_t(x_t)$. The continuity equation in \eqref{eqn:continuity} allows us to calculate such change-of-probability term with the estimation of the model divergence as \citep{fmgm}:
\begin{equation}
    \log p_1(x_1)=\log p_0(x_0) + \int_1^0 \operatorname{div}(v_t)(x_t)dt
\end{equation}
The trajectory is generated reverse through time from $t=1$ (arbitrary data) to $t=0$ via the learned vector field. Direct calculation of the model divergence $\operatorname{div}(v_t)$ is prohibitively expensive. Instead, Hutchinson's trace estimator is used to efficiently approximate the model divergence as \citep{hutchinson1989stochastic}:
\begin{equation}
    \operatorname{div}(v_t)(x)=\mathbb{E}_{\varepsilon\sim\mathcal{N}(0,I)}[\varepsilon^\top \nabla v_t(x)\varepsilon]
\end{equation}

In contrast to diffusion models that rely on variational bounds that can be calculated in closed form as the summation of KL divergence \citep{ho2020denoising}, \textbf{the likelihood for flow-based models requires expensive simulation and the calculation of vector-Jacobian product to estimate the model divergence}. Therefore, the likelihood-based RL formulation for diffusion models in \citet{ddpo,ppo,dpo,dpok} is non-trivial and  computationally expensive for flow models.


\clearpage


\clearpage
\section{Proofs}

\newtheorem*{nitheorem}{Theorem}


\subsection{Induced Data Distribution}
\label{app: induced data distribution}

In practice, in generative model, we are normally very interested in the induced data distribution $p_{new}(x_1)$ after learning under a design loss function $\mathcal{L}$. In order to facilitate the subsequent derivation, in this section, we first derive a general Theorem of Induced Data Distribution.

\begin{Theorem}[Induced Data Distribution]
\label{theorem: Induced Data Distribution}
Given the loss function:

\begin{equation}
    \mathcal{L}_{\text {total }}(\theta)=\mathbb{E}_{x_1 \sim q\left(x_1\right)}\left[\tilde{L}\left(x_1\right)\right]
\end{equation}

where $\tilde{L}\left(x_1\right)$ is any per-sample loss function, the induced data distribution over $x_1$ is:

\begin{equation}
    p_{\text {new }}\left(x_1\right) \propto q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)
\end{equation}

with $\gamma>0$ being a positive constant.
\end{Theorem}

\begin{proof}[Proof of Theorem \ref{theorem: Induced Data Distribution}, from RL Perspective]
In fact, most previous RLHF \citep{zie2020,stiennon2020,ouyang2022} and RL works \citep{awr} have found that assume a reward model $r(x)$ that captures human preferences, with the goal of modifying the base generative model $q(x)$ such that it generates the following tilted distribution:
\begin{equation}
    p_{new}(x_1) = p^*(x_1) \propto q(x_1) \exp (\tau * r(x_1))
\end{equation}

just set the  reward model as the negative of per-sample loss $\tilde{L}\left(x_1\right)$, namely:

\begin{equation}
    r(x_1) = - \tilde{L}\left(x_1\right)
\end{equation}

and set $\tau = \gamma$, which is a constant, and we can obtain: 

\begin{equation}
    p_{\text {new }}\left(x_1\right) = p^*\left(x_1\right)  \propto q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)
\end{equation}

Then, the proof concludes.
\end{proof}

\begin{proof}[Proof of Theorem \ref{theorem: Induced Data Distribution}, from Energy Function Perspective]
    In standard EBMs, the probability density over $x_1$ is defined using an energy function $E\left(x_1\right)$ :

\begin{equation}
    p_{\text {EBM }}\left(x_1\right)=\frac{1}{Z} \exp \left(-\beta E\left(x_1\right)\right)
\end{equation}

where $\beta>0$ is the inverse temperature, and $Z$ is the partition function ensuring normalization:

\begin{equation}
    Z=\int \exp \left(-\beta E\left(x_1\right)\right) d x_1
\end{equation}


This formulation assumes a uniform base measure over $x_1$.

We consider the per-sample loss $\tilde{L}\left(x_1\right)$ with the energy function $E\left(x_1\right)$, and set $\beta=\gamma$.
Therefore:

\begin{equation}
    p_{\text {new }}\left(x_1\right)=\frac{1}{Z} \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)
\end{equation}



When the data distribution $q\left(x_1\right)$ is not uniform, or when we have prior knowledge about $x_1$ like $q(x_1)$, it is appropriate to include $q\left(x_1\right)$ in the model \citep{lecun2006tutorial}. This leads to a modified probability density:

\begin{equation}
    p_{\text {new }}\left(x_1\right)=\frac{1}{Z} q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)
\end{equation}


Here, $q\left(x_1\right)$ acts as a base measure or prior distribution, and $\tilde{L}\left(x_1\right)$ corresponds to the energy function $E\left(x_1\right)$. The inclusion of $q\left(x_1\right)$ is necessary to accurately represent the underlying distribution of the data when it is not uniform.

\begin{Remark}
    The total loss function is:

\begin{equation}
    \mathcal{L}_{\text {total }}(\theta)=\int q\left(x_1\right) \tilde{L}\left(x_1\right) d x_1
\end{equation}


Minimizing $\mathcal{L}_{\text {total }}(\theta)$ with respect to $\theta$ corresponds to adjusting the model parameters to reduce the expected loss over the data distribution $q\left(x_1\right)$.

We can interpret this minimization as seeking a distribution $p_{\text {new }}\left(x_1\right)$ that places higher probability on samples with lower $\tilde{L}\left(x_1\right)$. This aligns with the principle of maximum likelihood estimation in EBMs, where we aim to maximize the likelihood of observed data under the model.
\end{Remark}

Then, the proof concludes.
\end{proof}

\begin{proof}[Proof of Theorem \ref{theorem: Induced Data Distribution}, from Bayesian Inference Perspective]
In Bayesian inference, the posterior distribution $p_{\text {new }}\left(x_1 \mid \mathcal{D}\right)$ over a latent variable $x_1$ given data $\mathcal{D}$ is proportional to the product of the prior $q\left(x_1\right)$ and the likelihood $p\left(\mathcal{D} \mid x_1\right)$ :

\begin{equation}
    p_{\text {new }}\left(x_1 \mid \mathcal{D}\right)=\frac{1}{Z} q\left(x_1\right) p\left(\mathcal{D} \mid x_1\right)
\end{equation}

where $Z$ is the normalization constant:

\begin{equation}
    Z=\int q\left(x_1\right) p\left(\mathcal{D} \mid x_1\right) d x_1
\end{equation}

In statistical modeling and machine learning, the loss function $\tilde{L}\left(x_1\right)$ quantifies the discrepancy between the model's predictions and the observed data. It is common to relate the loss function to the likelihood function through the negative log-likelihood:

\begin{equation}
    \tilde{L}\left(x_1\right)=-\frac{1}{\gamma} \log p\left(\mathcal{D} \mid x_1\right)
\end{equation}

Rearranging this equation gives:

\begin{equation}
    p\left(\mathcal{D} \mid x_1\right)=\exp \left(-\gamma \tilde{L}\left(x_1\right)\right)
\end{equation}

The posterior distribution becomes:

\begin{equation}
    p_{\text {new }}\left(x_1\right)=\frac{1}{Z} q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)
\end{equation}

where:

\begin{equation}
    Z=\int q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right) d x_1
\end{equation}

which ensures that $p_{\text {new }}\left(x_1\right)$ integrates to one.

Then, the proof concludes.
\end{proof}

\begin{Corollary}[Limiting Behavior of Well Learned Model]
\label{corollary: Limiting Behavior of Well Learned Model}
    As $\tilde{L}\left(x_1\right) \rightarrow 0$, the exponential term approaches 1 :
    
\begin{equation}
    \lim _{\tilde{L}\left(x_1\right) \rightarrow 0} \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)=1
\end{equation}

Thus, $p_{\text {new }}\left(x_1\right) \propto q\left(x_1\right)$, meaning the induced distribution is the same as the data sampling distribution $q(x_1)$.
\end{Corollary}

\begin{Corollary}[Weighted Prior Distribution]
\label{corollary: Weighted Prior Distribution}
Considering a weighed prior sampling distribution $p_{eff}(x_1) = w(x_1) q(x_1)$, given the loss function: 
\begin{equation}
    \mathcal{L}_{\text {total }}(\theta)=\mathbb{E}_{x_1 \sim p_{eff}\left(x_1\right)}\left[\tilde{L}\left(x_1\right)\right]
\end{equation}
where $\tilde{L}\left(x_1\right)$ is any per-sample loss function, the induced data distribution over $x_1$ is:

\begin{equation}
\begin{aligned}
        p_{\text {new }}\left(x_1\right) 
 & \propto p_{eff}\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)\\
 & \propto w(x_1)q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right) \\
\end{aligned}
\end{equation}

with $\gamma>0$ being a positive constant.
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: Weighted Prior Distribution}]
    Substituting $q(x_1) = p_{eff}(x_1)$ into the Theorem \ref{theorem: Induced Data Distribution}, we can have the induced data distribution:
    \begin{equation}
\begin{aligned}
        p_{\text {new }}\left(x_1\right) 
 & \propto p_{eff}\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)\\
 & \propto w(x_1)q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right) \\
\end{aligned}
\end{equation}
then, the proof concludes.
\end{proof}

\begin{Corollary}[Constant Loss]
    If $\tilde{L}\left(x_1\right)=c$ for all $x_1$, then:

\begin{equation}
    p_{\text {new }}\left(x_1\right) \propto q\left(x_1\right) \exp (-\gamma c)=q\left(x_1\right) \times \text { constant. }
\end{equation}

Thus, $p_{\text {new }}\left(x_1\right)$ is proportional to $q\left(x_1\right)$, and the induced distribution remains unchanged.
\end{Corollary}
\clearpage

\subsection{Reward Weighted Conditional  Flow Matching}
\label{app sec: offline RW CFM}

We start from a conditional flow matching setting, where a model parameterized by $\theta$ defines a vector field $v_{\theta}(t,x)$ that transports samples from a base distribution to a target distribution over time t $\in[0,1]$. The standard CFM loss function is as follows:
\begin{equation}
        \mathcal{L}_{\text{CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}(0,1), q\left(x_1\right), p_t\left(x \mid x_1\right)}\left\|v_{\theta}(t,x)-u_t\left(x \mid x_1\right)\right\|^2,
\end{equation}
which aims to align $v_{\theta}(t,x)$ with the true conditional vector field $u_t(x|x_1)$ by minimizing the expected squared error over the data distribution $q(x_1)$.

In practical applications, it is often desirable to focus the model's learning capacity on specific regions of the data space. This can be achieved by incorporating a weighting function $w\left(x_1\right)$ into the loss function, effectively reweighting the data distribution. However, the theoretical implications of this modification on the learned distribution have not been fully studied.

In this part, we rigorously analyze the impact of using a weighted regression loss in CFM training. We prove that incorporating a weighting function into the CFM loss leads the model to learn a new distribution over $x_1$, given by $p^{\text {new }}\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}$, where $q\left(x_1\right)$ is the original data distribution and $Z$ is a normalization constant. Our analysis provides theoretical support for the use of weighting functions to control the focus of generative models, with implications for target data generation, RLHF \citep{instructed_rlhf} and RL fine-tuning \citep{ddpo,dpo}.


Let $\mathcal{X} \subseteq \mathbb{R}^d$ denote the data space, and let $q\left(x_1\right)$ be the probability density function (pdf) of the random variable $x_1 \in \mathcal{X}$. We consider a continuous-time flow over $t \in[0,1]$, where $t$ is sampled from the uniform distribution $\mathcal{U}[0,1]$. The conditional pdf of $x$ at time $t$ given $x_1$ is denoted by $p_t\left(x \mid x_1\right)$.

The true conditional vector field is $u_t\left(x \mid x_1\right)$, and the model's vector field is $v_t(x ; \theta)$, where $\theta$ represents the model parameters. The goal is to learn $v_t(x ; \theta)$ such that it approximates $u_t(x \mid$ $\left.x_1\right)$

We introduce a weighting function $w: \mathcal{X} \rightarrow[0, \infty)$, which is measurable and integrable with respect to $q\left(x_1\right)$.

Then, we can have the following Theorem (For  ease of reading, we rewrite the Theorem \ref{theorem: rwr-cfm} to be proved in this section as follows):
\begin{nitheorem}
Let $w: \mathcal{X} \rightarrow[0, \infty)$ be a measurable weighting function such that $0<Z=$ $\int_{\mathcal{X}} w\left(x_1\right) q\left(x_1\right) d x_1<\infty$. Define the reward weighted CFM loss function:
\begin{equation}
\label{equ: origin weighted rw-cfm}
    \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim q\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[w\left(x_1\right)\left\|v_t(x;\theta)-u_t\left(x \mid x_1\right)\right\|^2\right]
\end{equation}
where $w\left(x_1\right)$ is a weighting function, $q\left(x_1\right)$ is the original data distribution, $p_t\left(x \mid x_1\right)$ is the conditional distribution at time $t, v_t(x;\theta)$ is the model's vector field, and $u_t\left(x \mid x_1\right)$ is the true vector field conditioned on $x_1$. Then, training the model using this weighted loss function leads the model to learn a new data distribution:

\begin{equation}
    p^{\text {new }}\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}
\end{equation}

where $Z=\int w\left(x_1\right) q\left(x_1\right) d x_1$ is the normalization constant.
\end{nitheorem}


\begin{proof}[Proof of Theorem \ref{theorem: rwr-cfm}]

We first aim to show that incorporating the weighting function $w\left(x_1\right)$ into the loss function effectively changes the distribution over $x_1$ from $q\left(x_1\right)$ to $p^{\text {new }}\left(x_1\right)$.

\begin{Lemma}
\label{lemma: rwr new distribution}
     The weighting function modifies the data distribution  over $x_1$ from $q(x_1)$ to:
     \begin{equation}
         p^{\text {new }}\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}
     \end{equation}
\end{Lemma}

\begin{proof}[Proof of Lemma \ref{lemma: rwr new distribution}]
The weighted loss function can be explicitly written as:

\begin{equation}
    \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=\int_0^1 \int_{\mathcal{X}} w\left(x_1\right) q\left(x_1\right) \int_{\mathcal{X}}\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2 p_t\left(x \mid x_1\right) d x d x_1 d t
\end{equation}

Then, we can define the reweighted distribution $p^{\text {new }}\left(x_1\right)$ as:


\begin{equation}
    p^{\text {new }}\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}, \quad \text { where } \quad Z=\int_{\mathcal{X}} w\left(x_1\right) q\left(x_1\right) d x_1
\end{equation}

Since $w\left(x_1\right) \geq 0$ and $Z<\infty, p^{\text {new }}\left(x_1\right)$ is a valid pdf over $\mathcal{X}$.


Substituting $p^{\text {new }}\left(x_1\right)$ into the loss function, we have:

\begin{equation}
    \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=Z \int_0^1 \int_{\mathcal{X}} p^{\mathrm{new}}\left(x_1\right) \int_{\mathcal{X}}\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2 p_t\left(x \mid x_1\right) d x d x_1 d t
\end{equation}


This can be expressed as an expectation:

\begin{equation}
    \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=Z \mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim p^{\mathrm{new}}\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2\right]
\end{equation}

The gradient of $\mathcal{L}_{\mathrm{RW-CFM}}(\theta)$ is:

\begin{equation}
\label{equ: p_new of rw-cfm}
    \nabla_\theta \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=Z \mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim p^{\mathrm{new}}\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[\nabla_\theta\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2\right]
\end{equation}

Since $Z$ is constant with respect to $\theta$, it does not affect the optimization process. Therefore, minimizing $\mathcal{L}_{\mathrm{RW-CFM}}(\theta)$ is equivalent to minimizing the expected loss under the distribution $p^{\text {new }}\left(x_1\right)$
\begin{Remark}
    According to  Corollary \ref{corollary: Weighted Prior Distribution}, set $p_{eff}(x_1)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}$, we have:
    \begin{equation}
        \begin{aligned} p_{\text {new }}\left(x_1\right) & \propto p_{e f f}\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right) \\ & \propto w\left(x_1\right) q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right)\end{aligned}
    \end{equation}
    When the model fits the data well (i.e., attain convergence), and $\tilde{L}\left(x_1\right) \to 0$, then according to Corollary \ref{corollary: Limiting Behavior of Well Learned Model}, we have:
    \begin{equation}
        \begin{aligned} p_{\text {new }}\left(x_1\right) & \propto w\left(x_1\right) q\left(x_1\right) \exp \left(-\gamma \tilde{L}\left(x_1\right)\right) \\
        & \propto w\left(x_1\right) q\left(x_1\right)
        \end{aligned}
    \end{equation}
\end{Remark}
\begin{Remark}
    During optimization, the model minimizes the expected squared difference between the model's vector field and the true vector field, but crucially, this expectation is now taken with respect to the reweighted distribution $p^{\text{new}}(x_1)$. As a result, the model prioritizes accurately modeling the flow dynamics in regions of higher weight, effectively learning to generate data according to $p^{\text{new}}(x_1)$.
\end{Remark}

The proof concludes.
\end{proof}

Based on Lemma \ref{lemma: rwr new distribution}, it's obvious that Theorem \ref{theorem: rwr-cfm} holds.
\end{proof}
Based on \eqref{equ: p_new of rw-cfm}, the reward weighted conditional flow matching loss in \eqref{equ: origin weighted rw-cfm} actually learn a weighted distribution $p^{new}(x_1)$:
\begin{equation}
    p^{\text {new }}\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z}
\end{equation}


\begin{Remark}[Normalization Constant $Z$ ]
$Z$ is a  finite normalization constant ensuring $p^{\text {new }}\left(x_1\right)$ is a valid probability distribution (i.e., integrates to 1). In practice, during training, the normalization constant can be ignored because it does not affect the optimization process since it is constant with respect to $\theta$.
\end{Remark}

\begin{Lemma}[Finite Normalization Constant $Z$]
\label{lemma: finite Z}
    The normalization constant $Z$ is finite if $w\left(x_1\right)$ is integrable with respect to $q\left(x_1\right)$.
\end{Lemma}

\begin{proof}[Proof of Lemma \ref{lemma: finite Z}]
    Since $w\left(x_1\right)$ is a weighting function and $q\left(x_1\right)$ is a probability density function, the integral:
\begin{equation}
    Z=\int_{\mathcal{X}} w\left(x_1\right) q\left(x_1\right) d x_1
\end{equation}
is finite if $w\left(x_1\right)$ does not introduce singularities and is integrable over $\mathcal{X}$.
\end{proof}

\begin{Remark}[Data Traversal]
In fact, to optimize the reward weighted CFM loss in \eqref{equ: origin weighted rw-cfm} requires the agent to nearly traverse almost possible samples $x_1$, which may also similar to the exploration problem in RL, namely how to explore the data space $\mathcal{X}$. Therefore, maintaining the ability to continuously explore the data space is important for RL method \citep{goexplore,sac}. 
\end{Remark}


\clearpage

\subsection{Online Reward Weighted Conditional  Flow Matching}
\label{app sec: online RW-CFM}

In App. \ref{app sec: offline RW CFM} we have discussed offline version of Reward Weighted Conditional Flow Matching, which is learned by optimizing the following loss:
\begin{equation}
    \mathcal{L}_{\mathrm{RW-CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim q\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[w\left(x_1\right)\left\|v_t(x;\theta)-u_t\left(x \mid x_1\right)\right\|^2\right],
\end{equation}
wherein $q\left(x_1\right)$ is the data distribution approximated by the training dataset or sampled from pre-trained model, $p_t\left(x \mid x_1\right)$ is the conditional pdf of $x$ at time $t$ given $x_1$ \citep{otcfm}. 

We consider a N epoch online fine-tuning scenario, where our vector field model $v_{\theta_{\text{ft}}}$ is initialized by a pre-trained model $v_{\theta_{\text{ref}}}$ that can capture the origin data distribution $q(x_1)$, namely the data distribution $p^{ref}(x_1)$ induced by pre-trained model $v_{\theta_{\text{ref}}}$ can meet $p^{\theta_{\text{ref}}}(x_1) \approx q(x_1)$. At each epoch of training, the agent will sample data from reweighted data distribution $p^{\theta_{\text{ft}}}(x_1)$ induced by $v_{\theta_{\text{ft}}}$, and use the newly sampled data to train the model under the Reward Weighted CFM Loss, then the online reward weighted CFM loss can be writen as:
\begin{equation}
    \mathcal{L}_{\mathrm{ORW-CFM}}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim p^{\theta_{\text{ft}}}\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}\left[w\left(x_1\right)\left\|v_t(x;\theta)-u_t\left(x \mid x_1\right)\right\|^2\right],
\end{equation}
wherein $p^{\theta_{\text{ft}}}\left(x_1\right)$ is the re-weighted data distribution given by fine-tuned vector fields $v_{\theta_{\text{ft}}}$. Then, we can have the following (For ease of reading, we rewrite the Theorem \ref{theorem: Online Reward Weighted CFM} as follows):

\begin{nitheorem}[Online Reward Weighted CFM]
    Let $q\left(x_1\right)$ be the initial data distribution, and $w\left(x_1\right)$ be a non-negative weighting function integrable with respect to $q\left(x_1\right)$. Assume that at each epoch $n$, the model $v_\theta$ perfectly learns the distribution implied by the Online Reward-Weighted CFM loss. Then, the data distribution after $N$ epochs is given by:

\begin{equation}
    q_\theta^N\left(x_1\right)=\frac{w\left(x_1\right)^N q\left(x_1\right)}{Z_N}
\end{equation}

where $Z_N=\int_{\mathcal{X}} w\left(x_1\right)^N q\left(x_1\right) d x_1$ is the normalization constant ensuring $q_\theta^N\left(x_1\right)$ is a valid probability distribution.
\end{nitheorem}

\begin{proof}[Proof of Theorem \ref{theorem: Online Reward Weighted CFM}]
We can use induction to present the proof.

\begin{Example}[Base Case $N=1$]
We first consider the basic case, where $N=1$, namely single epoch of learning, which equals to offline RW-CFM. Then, The initial data distribution is $q_\theta^0\left(x_1\right)=q\left(x_1\right)$. 

According to Theorem \ref{theorem: rwr-cfm}, after applying the online reward weighted CFM loss, we can obtain the new distribution is:

\begin{equation}
    q_\theta^1\left(x_1\right)=\frac{w\left(x_1\right) q\left(x_1\right)}{Z_1}
\end{equation}

where $Z_1=\int_{\mathcal{X}} w\left(x_1\right) q\left(x_1\right) d x_1$
\end{Example}

Assume that after $n-1$ epochs, the distribution is:

\begin{equation}
    q_\theta^{n-1}\left(x_1\right)=\frac{w\left(x_1\right)^{n-1} q\left(x_1\right)}{Z_{n-1}}
\end{equation}

with $Z_{n-1}=\int_{\mathcal{X}} w\left(x_1\right)^{n-1} q\left(x_1\right) d x_1$.

According to Theorem \ref{theorem: rwr-cfm}, at epoch $n$, the weighted loss leads to an effective distribution:

\begin{equation}
    q_\theta^n\left(x_1\right)=\frac{w\left(x_1\right) q_\theta^{n-1}\left(x_1\right)}{Z_n}
\end{equation}
where $Z_n=\int_{\mathcal{X}} w\left(x_1\right) q_\theta^{n-1}\left(x_1\right) d x_1$.
Substituting $q_\theta^{n-1}\left(x_1\right)$ :

\begin{equation} q_\theta^n\left(x_1\right)=\frac{w\left(x_1\right)\left(\frac{w\left(x_1\right)^{n-1} q\left(x_1\right)}{Z_{n-1}}\right)}{Z_n}=\frac{w\left(x_1\right)^n q\left(x_1\right)}{Z_{n-1} Z_n}
\end{equation}

Define $\tilde{Z}_n=Z_{n-1} Z_n$, then:

\begin{equation}
    q_\theta^n\left(x_1\right)=\frac{w\left(x_1\right)^n q\left(x_1\right)}{\tilde{Z}_n}
\end{equation}


By induction, $\tilde{Z}_n=\prod_{k=1}^n Z_k$, and we can write:

\begin{equation}
    q_\theta^n\left(x_1\right)=\frac{w\left(x_1\right)^n q\left(x_1\right)}{\prod_{k=1}^n Z_k}
\end{equation}

Then, the proof concludes.

\end{proof}

\begin{Corollary}[Normalization Constant of ORW-CFM]
\label{corollary:N Epoch Normalization Constant}
The normalization constant after $N$ epochs is:
\begin{equation}
    Z_N=\int_{\mathcal{X}} w\left(x_1\right)^N q\left(x_1\right) d x_1
\end{equation}
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary:N Epoch Normalization Constant}]
    By induction on the normalization constants $Z_n$ and the fact that $q_\theta^n\left(x_1\right)$ must integrate to 1 , we have:
\begin{equation}
    Z_N=\prod_{k=1}^N Z_k=\int_{\mathcal{X}} w\left(x_1\right)^N q\left(x_1\right) d x_1
\end{equation}
\end{proof}

\begin{Remark}[RL Perspective of ORW-CFM]
 From a RL perspective, this iterative reweighting process can be interpreted as an analog to policy iteration:
 \begin{itemize}
     \item Policy Representation: The data distribution $q_\theta^n\left(x_1\right)$ represents the policy at epoch $n$.
     \item Policy Improvement: The application of the weighting function $w\left(x_1\right) \propto r(x_1)$ corresponds to evaluating and improving the policy based on the rewards.
     \item Value Estimation: The weighting function can be seen as the expected return or value function guiding the policy update.
     \item  Exploration Exploitation Trade-off: Similar to the exploration-exploitation trade-off, there is a balance between focusing on high-reward regions and maintaining diversity.
 \end{itemize}

This process also aligns with the concept of importance sampling in RL, where the probability of selecting certain actions (data points) is adjusted based on their estimated value, and  enhance the high-rewarded probability area and reduce the low-rewarded probability area.
\end{Remark}

\begin{Corollary}[Convergence Behavior]
According to \ref{theorem: Online Reward Weighted CFM}, as $N \rightarrow \infty$, the distribution $q_\theta^N\left(x_1\right)$ may become increasingly concentrated on the subset of $\mathcal{X}$ where $w\left(x_1\right)$ is maximized.
\end{Corollary}

\begin{Corollary}[Mode Collapse Risk/Overoptimization]
\label{Corollary: Overoptimization}
 Overemphasis on high-reward regions can lead to a lack of diversity in generated samples, similar to mode collapse in GANs \citep{gan2014} or overoptimization in fine-tuning diffusion models \citep{ddpo}.
\end{Corollary}


In this paper, we introduce two methods to handle the Overoptimization and ease the mode collapse risk in online RW-CFM algorithms. The first is to introduce a W2 distance bound between fine-tuned model $\theta_{\text{ft}}$ and reference model $\theta_{\text{ref}}$, which works similar to the KL bound in previous RL \citep{ppo} and RLHF methods \citep{ddpo,dpo}. The second is to incorporate a  entropy regularization (i.e., $w(x_1) = \exp(\tau * x_1)$) or smoothing the weighting function (i.e., $w(x_1) = \operatorname{Softmax}(\tau * x_1)$)  can mitigate excessive concentration or extremely greedy policy.

\begin{Lemma}[Limiting Case]
    We now consider the limiting cases where $N \rightarrow \infty$:
    
    If $w\left(x_1\right)>0$ for all $x_1 \in \mathcal{X}$ and attains its maximum at $x_1^*$, then as $N \rightarrow \infty$, the distribution $q_\theta^N\left(x_1\right)$ converges to a Dirac delta function centered at $x_1^*$ :

\begin{equation}
    \lim _{N \rightarrow \infty} q_\theta^N\left(x_1\right)=\delta\left(x_1-x_1^*\right)
\end{equation}
\end{Lemma}

\begin{proof}[Proof of Lemma \ref{lemma: limiting case}]
Given that $w\left(x_1\right)>0$ for all $x_1 \in \mathcal{X}$ and attains its maximum at $x_1^*$, we define:
\begin{equation}
    \epsilon\left(x_1\right)=\frac{w\left(x_1\right)}{w\left(x_1^*\right)}
\end{equation}
which means, $\epsilon\left(x_1^*\right)=1$ since $w\left(x_1^*\right) / w\left(x_1^*\right)=1$, and $0 \leq \epsilon\left(x_1\right)<1$ for $x_1 \neq x_1^*$, since $w\left(x_1\right)<w\left(x_1^*\right)$.

Then, we can rewrite $q_{\theta}^N(x_1)$ using $\epsilon(x_1)$ as:
\begin{equation}
q_\theta^N\left(x_1\right)=\frac{\left[w\left(x_1^*\right)\right]^N \epsilon\left(x_1\right)^N q\left(x_1\right)}{Z_N}
\end{equation}

Then for $x_1 \neq x_1^*$, we can have:
$\epsilon(x_1)^N \to 0$ as $N \to \infty$  since $\epsilon(x_1) <1$. Thus $q_{\theta}^N(x_1)\to 0, \forall x_1 \neq x_1^{*}$.

And for $x_1 = x_1^*$, we can have $\epsilon(x_1^*)^N=1$, and $q^N_{\theta}(x_1^*) = \frac{\left[w\left(x_1^*\right)\right]^N q\left(x_1\right)}{Z_N}$.

And we can have the normalization constant as follows:
\begin{equation}
    Z_N=\int_{\mathcal{X}} w\left(x_1\right)^N q\left(x_1\right) d x_1=\left[w\left(x_1^*\right)\right]^N \int_{\mathcal{X}} \epsilon\left(x_1\right)^N q\left(x_1\right) d x_1.
\end{equation}

Similarly, we can obtain $Z_N \approx\left[w\left(x_1^*\right)\right]^N q\left(x_1^*\right)$ as $N \to \infty$.

Then, we can have the limit behavior:

For $x_1 \neq x_1^*:$
\begin{equation}
q_\theta^N\left(x_1\right)=\frac{\left[w\left(x_1^*\right)\right]^N \epsilon\left(x_1\right)^N q\left(x_1\right)}{\left[w\left(x_1^*\right)\right]^N q\left(x_1^*\right)}=\frac{\epsilon\left(x_1\right)^N q\left(x_1\right)}{q\left(x_1^*\right)} \rightarrow 0.
\end{equation}

For $x_1 = x_1^*:$
\begin{equation}
q_\theta^N\left(x_1^*\right)=\frac{\left[w\left(x_1^*\right)\right]^N q\left(x_1^*\right)}{\left[w\left(x_1^*\right)\right]^N q\left(x_1^*\right)}=1
\end{equation}

Then, we can have:
\begin{equation}
    \lim _{N \rightarrow \infty} q_\theta^N\left(x_1\right)=\delta\left(x_1-x_1^*\right).
\end{equation}
\end{proof}



According to Lemma \ref{lemma: limiting case}, iteratively utilize the ORW-CFM loss to fine-tune the flow matching model without bounding the distance between reference model and fine-tuned model may lead us to a greedy policy over $x_1$, which may induce the  overoptimization problem \citep{ddpo}. In this paper we introduce W2 Distance into the ORW-CFM training to bound the distance between reference model and fine-tuned model.
\clearpage

\subsection{Wasserstein-2 Distance for Flow Matching}
\label{app sec: W2 distance}

We consider two flow matching models parameterized by $\theta_1$ and $\theta_2$, inducing time-evolving data distributions $p_t^{\theta_1}(x)$ and $p_t^{\theta_2}(x)$, respectively. The models define vector fields $v^{\theta_1}(t, x)$ and $v^{\theta_2}(t, x)$ that transport an initial distribution $p_0(x)$ to final distributions $p_1^{\theta_1}(x)$ and $p_1^{\theta_2}(x)$ at time $t=1$.

We aim to bound the Wasserstein-2 distance between $p_1^{\theta_1}(x)$ and $p_1^{\theta_2}(x)$ in terms of the difference between $v^{\theta_1}$ and $v^{\theta_2}$.

First, we can write down the Wasserstein-2 Distance as follows (For ease of reading, we rewrite the Theorem \ref{theorem: W2 Bound for Flow Matching} to be proved in this section as follows):

\begin{definition}[Wasserstein-2 Distance]
    Given two probability measures $\mu$ and $\nu$ on $\mathbb{R}^n$, the squared Wasserstein- 2 distance between $\mu$ and $\nu$ is defined as:
\begin{equation}
    W_2^2(\mu, \nu)=\inf _{\gamma \in \Pi(\mu, \nu)} \int_{\mathbb{R}^n \times \mathbb{R}^n}\|x-y\|^2 d \gamma(x, y)
\end{equation}
where $\Pi(\mu, \nu)$ denotes the set of all couplings of $\mu$ and $\nu$; that is, all joint distributions $\gamma$ on $\mathbb{R}^n \times \mathbb{R}^n$ with marginals $\mu$ and $\nu$.
\end{definition}

\begin{nitheorem}[W2 Bound for Flow Matching]
Assume that $v^{\theta_2}(t, x)$ is Lipschitz continuous in $x$ with Lipschitz constant L. Then, the squared Wasserstein-2 distance between the distributions $p_1^{\theta_1}$ and $p_1^{\theta_2}$ induced by the flow matching models at time $t=1$ satisfies:
\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq e^{2 L} \int_0^1 \mathbb{E}_{x \sim p_s^{\theta_1}}\left[\left\|v^{\theta_1}(s, x)-v^{\theta_2}(s, x)\right\|^2\right] d s
\end{equation}
\end{nitheorem}

Before providing the full proof of Theorem \ref{theorem: W2 Bound for Flow Matching}, we first want to prove the Differential Flow Discrepancy Lemma:
\begin{Lemma}[Differential Flow Discrepancy Lemma]
\label{lemma: Differential Flow Discrepancy Lemma}
Let $\phi_t^{\theta_i}\left(x_0\right)$ be the flow map induced by $v^{\theta_i}$ starting from $x_0$. The difference $\Delta_t\left(x_0\right)=$ $\phi_t^{\theta_1}\left(x_0\right)-\phi_t^{\theta_2}\left(x_0\right)$ satisfies the differential inequality:
\begin{equation}
    \frac{d}{d t}\left\|\Delta_t\left(x_0\right)\right\| \leq\left\|v^{\theta_1}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)-v^{\theta_2}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)\right\|+L\left\|\Delta_t\left(x_0\right)\right\|
\end{equation}
\end{Lemma}
\begin{proof}[Proof of Lemma \ref{lemma: Differential Flow Discrepancy Lemma}]
Let $\Delta_t\left(x_0\right)=\phi_t^{\theta_1}\left(x_0\right)-\phi_t^{\theta_2}\left(x_0\right)$. Then:

\begin{equation}
    \frac{d}{d t} \Delta_t\left(x_0\right)=v^{\theta_1}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)-v^{\theta_2}\left(t, \phi_t^{\theta_2}\left(x_0\right)\right)
\end{equation}


Adding and subtracting $v^{\theta_2}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)$ :

\begin{equation}
    \frac{d}{d t} \Delta_t\left(x_0\right)=\underbrace{v^{\theta_1}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)-v^{\theta_2}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)}_{\delta_v(t)}+\underbrace{v^{\theta_2}\left(t, \phi_t^{\theta_1}\left(x_0\right)\right)-v^{\theta_2}\left(t, \phi_t^{\theta_2}\left(x_0\right)\right)}_{\delta_\phi(t)} .
\end{equation}

\begin{Assumption}[Lipschitz Continuity]Assume that $v^{\theta_{2}}(t, x)$ is Lipschitz continuous in $x$ with Lipschitz constant $L_v$ :
\begin{equation}
    \left\|v^{\theta_{2}}(t, x)-v^{\theta_{2}}(t, y)\right\| \leq L_v\|x-y\|
\end{equation}
\end{Assumption}
By the Lipschitz continuity of $v^{\theta_2}$ :

\begin{equation}
    \left\|\delta_\phi(t)\right\| \leq L\left\|\Delta_t\left(x_0\right)\right\|
\end{equation}

Thus:

\begin{equation}
    \left\|\frac{d}{d t} \Delta_t\left(x_0\right)\right\| \leq\left\|\delta_v(t)\right\|+L\left\|\Delta_t\left(x_0\right)\right\|
\end{equation}

Then, the proof of Lemma \ref{lemma: Differential Flow Discrepancy Lemma} concludes. 


\end{proof}


\begin{proof}[Proof of Theorem \ref{theorem: W2 Bound for Flow Matching}]
   Using Lemma \ref{lemma: Differential Flow Discrepancy Lemma}, define $f(t)=\left\|\Delta_t\left(x_0\right)\right\|$. Then:
   \begin{equation}
       \frac{d}{d t} f(t) \leq\left\|\delta_v(t)\right\|+L f(t)
   \end{equation}
Multiplying both sides by $e^{-L t}$ :
\begin{equation}
    \frac{d}{d t}\left(e^{-L t} f(t)\right) \leq e^{-L t}\left\|\delta_v(t)\right\|
\end{equation}
Integrating from 0 to $t$ :
\begin{equation}
    e^{-L t} f(t)-f(0) \leq \int_0^t e^{-L s}\left\|\delta_v(s)\right\| d s
\end{equation}

\begin{Assumption}
    Assume that the initial noise distribution $x_0 \sim p(x_0)$ are shared in two flow matching models, as thus  $f(0)=\left\|\Delta_0\left(x_0\right)\right\|=0$
\end{Assumption}

Since $f(0)=0$ :

\begin{equation}
    f(t) \leq e^{L t} \int_0^t e^{-L s}\left\|\delta_v(s)\right\| d s
\end{equation}

At $t=1$ :
\begin{equation}
    f(1) \leq e^L \int_0^1 e^{-L s}\left\|v^{\theta_1}\left(s, \phi_s^{\theta_1}\left(x_0\right)\right)-v^{\theta_2}\left(s, \phi_s^{\theta_1}\left(x_0\right)\right)\right\| d s
\end{equation}

Taking the expectation over $x_0 \sim p_0$ and applying Jensen's inequality:
\begin{equation}
    \mathbb{E}_{x_0}\left[f(1)^2\right] \leq e^{2 L}\left(\int_0^1 \mathbb{E}_{x \sim p_s^{\theta_1}}\left[\left\|v^{\theta_1}(s, x)-v^{\theta_2}(s, x)\right\|^2\right] d s\right)
\end{equation}

\begin{Lemma}
\label{equ: W2 upper bound}
Let $\phi_t^{\theta_i}\left(x_0\right)$ be the flow map induced by $v^{\theta_i}$ starting from $x_0$. The flow matching models define vector fields $v^{\theta_1}(t, x)$ and $v^{\theta_2}(t, x)$ that transport an initial distribution $p_0(x)$ to final distributions $p_1^{\theta_1}(x)$ and $p_1^{\theta_2}(x)$ at time $t=1$. Let $\Delta_t\left(x_0\right)=$ $\phi_t^{\theta_1}\left(x_0\right)-\phi_t^{\theta_2}\left(x_0\right)$ and $f(t)=\left\|\Delta_t\left(x_0\right)\right\|$. Then, the squared Wasserstein-2 distance between the distributions $p_1^{\theta_1}$ and $p_1^{\theta_2}$ induced by the flow matching models at time $t=1$ satisfies:
\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq \mathbb{E}_{x_0}\left[f(1)^2\right]
\end{equation}
\end{Lemma}

According to Lemma \ref{equ: W2 upper bound}, we have:
\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq \mathbb{E}_{x_0}\left[f(1)^2\right] \leq e^{2 L} \int_0^1 \mathbb{E}_{x \sim p_s^{\theta_1}}\left[\left\|v^{\theta_1}(s, x)-v^{\theta_2}(s, x)\right\|^2\right] d s
\end{equation}

Then, the proof of Theorem \ref{theorem: W2 Bound for Flow Matching} concludes.
\end{proof}

\begin{proof}[Proof of Lemma \ref{equ: W2 upper bound}]
Recall the definition of the Wasserstein-2 distance:
\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right)=\inf _{\gamma \in \Pi\left(p_1^{\theta_1}, p_1^{\theta_2}\right)} \int_{\mathbb{R}^n \times \mathbb{R}^n}\|x-y\|^2 d \gamma(x, y)
\end{equation}
We construct a specific coupling $\gamma$ between $p_1^{\theta_1}$ and $p_1^{\theta_2}$ by mapping the same initial sample $x_0 \sim$ $p_0$ through both flow maps:
\begin{itemize}
    \item $x_1^{\theta_1}=\phi_1^{\theta_1}\left(x_0\right)$.
    \item $x_1^{\theta_2}=\phi_1^{\theta_2}\left(x_0\right)$.
\end{itemize}
This coupling $\gamma$ is defined via the joint distribution of $\left(x_1^{\theta_1}, x_1^{\theta_2}\right)$.

By the definition of the Wasserstein-2 distance, for any coupling $\gamma$ :
\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq \int_{\mathbb{R}^n \times \mathbb{R}^n}\|x-y\|^2 d \gamma(x, y)
\end{equation}

Using our constructed coupling:

\begin{equation}
    \int_{\mathbb{R}^n \times \mathbb{R}^n}\|x-y\|^2 d \gamma(x, y)=\mathbb{E}_{x_0}\left[\left\|\phi_1^{\theta_1}\left(x_0\right)-\phi_1^{\theta_2}\left(x_0\right)\right\|^2\right]=\mathbb{E}_{x_0}\left[f(1)^2\right]
\end{equation}

Thus, we have:

\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq \mathbb{E}_{x_0}\left[f(1)^2\right]
\end{equation}

Then, the Proof concludes.
\end{proof}

\begin{Corollary}[Uniform Vector Field Deviation Bound]
\label{corollary: Uniform Vector Field Deviation Bound}
If there exists a constant $\varepsilon>0$ such that for all $s \in[0,1]$ :

\begin{equation}
    \mathbb{E}_{x \sim p_s^{\theta_1}}\left[\left\|v^{\theta_1}(s, x)-v^{\theta_2}(s, x)\right\|^2\right] \leq \varepsilon^2
\end{equation}

then the Wasserstein-2 distance between the induced distributions is bounded by:

\begin{equation}
    W_2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq \varepsilon e^L
\end{equation}

\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: Uniform Vector Field Deviation Bound}]
Given the assumption in Corollary \ref{corollary: Uniform Vector Field Deviation Bound}, from Theorem \ref{theorem: W2 Bound for Flow Matching}:

\begin{equation}
    W_2^2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq e^{2 L} \int_0^1 \varepsilon^2 d s=\varepsilon^2 e^{2 L}
\end{equation}


Thus:

\begin{equation}
    W_2\left(p_1^{\theta_1}, p_1^{\theta_2}\right) \leq \varepsilon e^L
\end{equation}

\end{proof}


\begin{Corollary}[W2 Bound for Flow Matching Fine-tuning]
\label{corollary: W2 Bound for Flow Matching Fine-tuning}
Considering we have two flow matching models, the fine-tuned  model $v^{\theta_{\mathrm{ft}}}$  and the reference/pre-trained model $v^{\theta_{\text {ref }}}$. According to Theorem \ref{theorem: W2 Bound for Flow Matching}, we can bound the Wasserstein-2 $\left(W_2\right)$ distance between the distributions $p_{\theta_{\mathrm{ft}}}\left(x_1\right)$ and $p_{\theta_{\text {ref }}}\left(x_1\right)$ induced by these two flow matching models:
\begin{equation}
\mathbb{E}_{x_0}\left[\left\|\Delta_1\left(x_0\right)\right\|^2\right] \leq e^{2 L_v} \int_0^1 \mathbb{E}_{x \sim p_s(x;\theta_{\text{ft}})}\left[\left\|v^{\theta_{\mathrm{ft}}}(s, x)-v^{\theta_{\mathrm{ref}}}(s, x)\right\|^2\right] d s
\end{equation}
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: W2 Bound for Flow Matching Fine-tuning}]
Set $\theta_1 = \theta_{\text{ft}}$ and $\theta_2 = \theta_{\text{ref}}$, according to the Theorem \ref{theorem: W2 Bound for Flow Matching}, the proof concludes.
\end{proof}

\begin{Corollary}[Monte Carlo Sampling for W2 Bound in Flow Matching]
\label{Corollary: Monte Carlo Sampling for W2 Bound}
However, the integral of time $t$ in Theorem \ref{theorem: W2 Bound for Flow Matching} is normally intractable and hard to obtain. In practice, we can use Monte Carlo sampling to approximate the integral:
\begin{align*}
            \int_0^1 &\mathbb{E}_{x \sim p_s(x;\theta_1)}\left[\left\|v^{\theta_{1}}(s, x)-v^{\theta_{2}}(s, x)\right\|^2\right] d s \\
            =&\mathbb{E}_{t \sim U(0,1),x_1 \sim q(x_1;\theta_1), x \sim p_t(x|x_1)}\left[\left\|v^{\theta_{1}}(t, x)-v^{\theta_{2}}(t, x)\right\|^2\right]
\end{align*}
\end{Corollary}


According to Corollary \ref{Corollary: Monte Carlo Sampling for W2 Bound}, we can obtain the W2 bound RW-CFM loss as follows:
\begin{align*}
    \mathcal{L}_{RW-CFM-W2} =  \mathbb{E}_{t \sim U(0,1),x_1 \sim q(x_1), x \sim p_t(x|x_1)} [&
    w\left(x_1\right) \left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2 \\
    & + 
    \alpha * \left\|v_{\theta_{\text{ft}}}(t, x)-v_{\theta_{\text{ref}}}(t, x)\right\|^2 ],
\end{align*}
wherein $\theta_{\text{ft}}=\theta$ is our fine-tuned model, and $\theta_{\text{ref}}$ is reference/pre-trained model,  $\alpha$ is the trade-off coefficient, indicates how close we want the final convergence point to be to the reference model.


Also, according to Corollary \ref{Corollary: Monte Carlo Sampling for W2 Bound}, we can obtain the W2 bound ORW-CFM loss as follows:
\begin{align*}
    \mathcal{L}_{ORW-CFM-W2} =  \mathbb{E}_{t \sim U(0,1),x_1 \sim q(x_1;\theta_{\text{ft}}), x \sim p_t(x|x_1)} [&
    w\left(x_1\right) \left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2 \\
    & + 
    \alpha * \left\|v_{\theta_{\text{ft}}}(t, x)-v_{\theta_{\text{ref}}}(t, x)\right\|^2 ],
\end{align*}
wherein $\alpha$ is the trade-off coefficient, indicates how close we want the final convergence point to be to the reference model (See the Divergence Controlled by $\alpha$ in \ref{fig: reward-distance trade-off cifar com}).
\clearpage

\subsection{RW-CFM with W2 Distance Bound}
\label{app: sec RW-CFM with W2 Distance Bound}

In this section, we provide a detailed analysis of how the inclusion of a Wasserstein regularization term in the Reward-Weighted Conditional Flow Matching (RW-CFM) loss affects the induced data distribution $p_{\text {new }}\left(x_1\right)$. Specifically, we prove that the new distribution becomes proportional to $w\left(x_1\right) q\left(x_1\right) \exp \left(-\alpha D\left(x_1\right)\right)$, where $D\left(x_1\right)$ is a divergence measure between the fine-tuned model and the reference model.



\begin{Theorem}[Learned Distribution Induced by RW-CFM-W2 Loss]
\label{theorem: Learned Distribution Induced by RW-CFM-W2 Loss}
 Let $w: \mathcal{X} \rightarrow[0, \infty)$ be a measurable weighting function, $q\left(x_1\right)$ be the original data distribution, and $\theta_{\text {ref }}$ be a fixed reference model. Consider the loss function:

\begin{equation}
\begin{aligned}
        \mathcal{L}_{\text {RW-CFM-W2 }}(\theta)= \mathbb{E}_{t \sim \mathcal{U}[0,1], x_1 \sim q\left(x_1\right), x \sim p_t\left(x \mid x_1\right)}[ & w\left(x_1\right)\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2 \\
        +&\alpha\left\|v_t(x ; \theta)-v_t\left(x ; \theta_{\text {ref }}\right)\right\|^2]
\end{aligned}
\end{equation}

where $u_t\left(x \mid x_1\right)$ is the true vector field conditioned on $x_1, v_t(x ; \theta)$ is the model's vector field, and $\alpha>0$ is the regularization coefficient. Then, minimizing $\mathcal{L}_{\text {RW-CFM-W2 }}(\theta)$ over $\theta$ induces a new data distribution:

\begin{equation}
p_{\text {new }}\left(x_1\right) \propto w\left(x_1\right) q\left(x_1\right) \exp \left(-\beta D\left(x_1\right)\right)
\end{equation}

where $D\left(x_1\right)=\mathbb{E}_{t, x \sim p_t\left(x \mid x_1\right)}\left[\left\|v^\theta(t, x)-v^{\theta_{\text {ref }}}(t, x)\right\|^2\right]$, $\beta=\gamma \alpha$, $\gamma>0$ is a scaling constant.

\end{Theorem}


\begin{proof}[Proof of Theorem \ref{theorem: Learned Distribution Induced by RW-CFM-W2 Loss}]
    We can re-write the RW-CFM-W2 Loss as follows:
\begin{equation}
    \mathcal{L}_{\text {RW-CFM-W2 }}(\theta)=\mathbb{E}_{t, x_1, x}\left[w\left(x_1\right)\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2+\alpha\left\|v^\theta(t, x)-v^{\theta_{\text {ref }}}(t, x)\right\|^2\right],
\end{equation}
where $\alpha$ controls the regularization strength.

Furthermore, we can write the loss function as an expectation over $x_1$ :

\begin{equation}
    \mathcal{L}_{\text {total }}(\theta)=\int q\left(x_1\right)\left[w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha L_{\mathrm{reg}}\left(x_1\right)\right] d x_1
\end{equation}

where the first term  gives $L_{\text {cfm }}\left(x_1\right)=\mathbb{E}_{t, x \sim p_t\left(x \mid x_1\right)}\left[\left\|v_t(x ; \theta)-u_t\left(x \mid x_1\right)\right\|^2\right]$, and the second term  gives $L_{\mathrm{reg}}\left(x_1\right)=\mathbb{E}_{t, x \sim p_t\left(x \mid x_1\right)}\left[\left\|v^\theta(t, x)-v^{\theta_{\mathrm{ref}}}(t, x)\right\|^2\right]$.

\begin{Assumption}
    Assuming that the expectation $L_{\mathrm{reg}}\left(x_1\right)$ has an upper bound $\varepsilon$ :

\begin{equation}
    L_{\mathrm{reg}}\left(x_1\right) \leq \varepsilon, \quad \forall x_1
\end{equation}

Since we're interested in how $L_{\mathrm{reg}}\left(x_1\right)$ varies with $x_1$, we can let $D\left(x_1\right)=L_{\mathrm{reg}}\left(x_1\right)$.
\end{Assumption}


Then, our total loss is:

\begin{equation}
    \mathcal{L}_{\text {total }}(\theta)=\int q\left(x_1\right)\left[w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)\right] d x_1
\end{equation}

We consider the optimization over $\theta$, which indirectly affects the distribution $p_{\text {new }}\left(x_1\right)$ that the model learns.

Then, consider the model as implicitly assigning higher importance to values of $x_1$ that minimize the total loss per $x_1$ :

\begin{equation}
    \tilde{L}\left(x_1\right)=w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)
\end{equation}


\begin{Remark}[Probabilistic Perspective]
    In optimization, samples $x_1$ with lower $\tilde{L}\left(x_1\right)$ have a higher impact on training. To make this explicit, consider a probabilistic model where the likelihood of $x_1$ is proportional to $\exp \left(-\gamma \tilde{L}\left(x_1\right)\right)$, where $\gamma$ is a positive constant \citep{hinton2002}.
\end{Remark}

Therefore, the induced distribution over $x_1$ becomes (detailed proof can see App. \ref{app: induced data distribution}):

\begin{equation}
    p_{\text {new }}\left(x_1\right) \propto q\left(x_1\right) \exp \left(-\gamma\left[w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)\right]\right)
\end{equation}


We can also induce the similar results from the perspective of energy function \citep{lecun2006tutorial}.
\begin{Lemma}[Energy Function Perspective]
\label{lemma: energy function}
    The per-sample loss contribution $\tilde{L}\left(x_1\right)$ can be interpreted as an energy function $E\left(x_1\right)$, and the induced distribution follows a Boltzmann distribution \citep{lecun2006tutorial}:
    \begin{equation}
    \begin{aligned}
        p_{\text {new }}\left(x_1\right) 
        & \propto q\left(x_1\right) \exp \left(-E\left(x_1\right)\right) \\
        & \propto q\left(x_1\right) \exp \left(-\gamma\left[w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)\right]\right)
    \end{aligned}
    \end{equation}
\end{Lemma}



\begin{proof}[Proof of Lemma \ref{lemma: energy function}]
    By interpreting the per-sample loss as an energy $E\left(x_1\right)$, we have:

\begin{equation}
\begin{aligned}
E\left(x_1\right)&=\tilde{L}\left(x_1\right)\\ 
&=w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)
\end{aligned}
\end{equation}

The induced distribution is then:

\begin{equation}
    \begin{aligned}
        p_{\text {new }}\left(x_1\right) 
        & \propto q\left(x_1\right) \exp \left(-E\left(x_1\right)\right) \\
        & \propto q\left(x_1\right) \exp \left(-\gamma\left[w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)\right]\right)
    \end{aligned}
\end{equation}

This aligns with the Boltzmann distribution in statistical mechanics, where higher energy states (higher loss) are less probable.
\end{proof}

Besides, based on Theorem \ref{theorem: Induced Data Distribution}, we can have:
\begin{equation}
    \begin{aligned} p_{\text {new }}\left(x_1\right) & \propto q\left(x_1\right) \exp \left(-\gamma*\tilde{L}\left(x_1\right)\right) \\ & \propto q\left(x_1\right) \exp \left(-\gamma\left[w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)+\alpha D\left(x_1\right)\right]\right)\end{aligned}
\end{equation}

\begin{Assumption}
\label{assump: less data error}
    Assuming $L_{\text {cfm }}\left(x_1\right)$ is approximately constant or negligible compared to $D\left(x_1\right)$, namely:
\begin{itemize}
    \item  When the model fits the data well, $L_{\text {cfm }}\left(x_1\right)$ is small. (i.e., $w\left(x_1\right)$ dominates)
\item The term $w\left(x_1\right) L_{\text {cfm }}\left(x_1\right)$ becomes proportional to $w\left(x_1\right)$.
\end{itemize}
\end{Assumption}
\begin{Remark}
    Generally speaking, if we fine-tune  from a well-learned pre-trained model, the model can usually fit the data well, so $L_{\text {cfm }}\left(x_1\right)$  is rather small. In practice, $w\left(x_1\right)$ can generally dominates.
\end{Remark}


Thus, we can write:

\begin{equation}
    p_{\text {new }}\left(x_1\right) \propto q\left(x_1\right) w\left(x_1\right) \exp \left(-\beta D\left(x_1\right)\right)
\end{equation}

where $\beta=\gamma \alpha$ absorbs the constants. Then the proof concludes.




\end{proof}

\begin{Remark}[Importance Sampling Perspective]
    We can also derive similar results from the perspective of importance sampling. We start from the total loss function: 
    \begin{equation}
    \label{equ: importance sampling perspective}
        \begin{aligned}
            \mathcal{L}_{\text {total }}(\theta)&=\int w\left(x_1\right) q\left(x_1\right)\left[ L_{\text {cfm }}\left(x_1\right)+\frac{\alpha}{w\left(x_1\right)} D\left(x_1\right)\right] d x_1 \\
            &=E_{x \sim w\left(x_1\right) q\left(x_1\right)}\left[\left[ L_{\text {cfm }}\left(x_1\right)+\frac{\alpha}{w\left(x_1\right)} D\left(x_1\right)\right]\right]
        \end{aligned}
    \end{equation}

We denote the reward weighted distribution as $p_w(x)$:
\begin{equation}
    p_w(x) = w\left(x_1\right) q\left(x_1\right)
\end{equation}

Then the loss function can be re-write as follows:
\begin{equation}
    \mathcal{L}_{\text {total }}(\theta) =E_{x \sim p_w(x)}\left[\left[ L_{\text {cfm }}\left(x_1\right)+\frac{\alpha}{w\left(x_1\right)} D\left(x_1\right)\right]\right]
\end{equation}

Then the induced data distribution $p_{new}(x)$ becomes:

\begin{equation}
    p_{\text {new }}\left(x_1\right) \propto p_{w}\left(x_1\right) \exp \left(-\gamma\left[ L_{\text {cfm }}\left(x_1\right)+\frac{\alpha}{w\left(x_1\right)}  D\left(x_1\right)\right]\right)
\end{equation}

Under Assumption \ref{assump: less data error}, we can obtain:
\begin{equation}
    \begin{aligned}
        p_{\text {new }}\left(x_1\right) & \propto p_{w}\left(x_1\right) \exp \left(-\frac{\gamma * \alpha}{w\left(x_1\right)}  D\left(x_1\right)\right )\\
        & \propto p_{w}\left(x_1\right) \exp \left(-\frac{\beta}{w\left(x_1\right)}  D\left(x_1\right)\right ) \\
        & \propto w(x_1) q\left(x_1\right) \exp \left(-\frac{\beta}{w\left(x_1\right)}  D\left(x_1\right)\right )
    \end{aligned}
\end{equation}

In practice, we can assume that $\forall x_1, \beta \gg w(x_1)$, then $\beta$ dominates, and we can obtain: 
\begin{equation}
    p_{\text {new }}\left(x_1\right) \propto w(x_1) q\left(x_1\right) \exp \left(- \beta  D\left(x_1\right)\right )
\end{equation}

Also, we can set $\alpha = \alpha_0 * w(x_1)$, then:
\begin{equation}
    \exp \left(-\frac{\beta}{w\left(x_1\right)}  D\left(x_1\right)\right ) = \exp \left(-\beta_0  D\left(x_1\right)\right )
\end{equation}
wherein $\beta_0=\alpha_0 * \gamma$.

\end{Remark}

\begin{Corollary}
\label{corollary: rw-cfm-w2 alpha 0}
    As $\alpha \rightarrow 0, \beta \rightarrow 0$, the induced distribution $p_{\text {new }}\left(x_1\right)$ approaches $w\left(x_1\right) q\left(x_1\right) / Z$, recovering the standard RW-CFM result.
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: rw-cfm-w2 alpha 0}]
When $\alpha \rightarrow 0, \beta \rightarrow 0$, the exponential term $\exp \left(-\beta D\left(x_1\right)\right) \rightarrow 1$, so:

\begin{equation}
    p_{\text {new }}\left(x_1\right) \rightarrow \frac{w\left(x_1\right) q\left(x_1\right)}{Z}
\end{equation}
where $Z$ is the normalization constant ensuring that $p_{\text {new }}\left(x_1\right)$ integrates to 1.
\end{proof}

\begin{Corollary}
\label{corollary: rw-cfm-w2 alpha inf}
    As $\alpha \rightarrow \infty, \beta \rightarrow \infty$, the induced distribution $p_{\text {new }}\left(x_1\right)$ concentrates on minimizing $D\left(x_1\right)$, effectively aligning with the reference model's distribution (See cases with increased $\alpha$, such as $\alpha=10$,  in Fig. \ref{fig: reward-distance trade-off cifar com}).
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: rw-cfm-w2 alpha inf}]
    When $\alpha \rightarrow \infty, \beta \rightarrow \infty$, the exponential term $\exp \left(-\beta D\left(x_1\right)\right)$ approaches zero unless $D\left(x_1\right)=0$. Therefore, $p_{\text {new }}\left(x_1\right)$ concentrates on the set of $x_1$ where $v_t\left(x ; \theta_{\text {ref }}\right)$ matches $u_t\left(x \mid x_1\right)$. Namely, the fine-tuned model remains unchanged from the reference model.
\end{proof}

\begin{Remark}
    The theorem \ref{theorem: Learned Distribution Induced by RW-CFM-W2 Loss} demonstrates that incorporating the Wasserstein regularization term into the RW-CFM loss results in an induced data distribution that not only depends on the reward weighting $w\left(x_1\right)$ but also exponentially penalizes data points based on their divergence $D\left(x_1\right)$ from the reference model (See Trade-off curve via traversing $\alpha$ Fig. \ref{fig: reward-distance trade-off cifar com}). This provides a mechanism to control the trade-off between fitting the reward weighted data and adhering to the behavior of a pre-trained or reference model (i.e., reward-diversity trade-off).

The divergence $D\left(x_1\right)$ acts as a measure of how much the true dynamics $u_t\left(x \mid x_1\right)$ differ from the reference model's dynamics $v_t\left(x ; \theta_{\text {ref }}\right)$ along trajectories starting from $x_1$. By adjusting $\alpha$, we can control the extent to which the model prioritizes matching the reference model versus fitting the reward-weighted data.
\end{Remark}

\clearpage
\subsection{ORW-CFM with W2 Distance Bound}
\label{app: sec ORW-CFM with W2 Distance Bound}


Similar to Theorem \ref{theorem: Learned Distribution Induced by RW-CFM-W2 Loss}, we can also derive the induced data distribution learned under the online reward weighted CFM loss and W2 Distance Bound.



According to Theorem \ref{theorem: W2 Bound for Flow Matching}, We can directly introduce the W2 Distance Bound into ORW-CFM loss \eqref{equ: orw-cfm loss}:
\begin{equation}
\begin{aligned}
            \mathcal{L}_{ORW-CFM-W2} =  \mathbb{E}_{t \sim U(0,1),x_1 \sim q(x_1;\theta_{\text{ft}}), x \sim p_t(x|x_1)} [&
    w\left(x_1\right) \left\|v_t(x ; \theta_{\text{ft}})-u_t\left(x \mid x_1\right)\right\|^2 \\
    & + 
    \alpha * \left\|v_{\theta_{\text{ft}}}(t, x)-v_{\theta_{\text{ref}}}(t, x)\right\|^2 ],
\end{aligned}
\end{equation}
wherein $\alpha$ is a trade-off coefficient.

For ease of reading, we rewrite the Theorem \ref{theorem: induced data distribution of ORW-CFM-W2} to be proved in this section as follows:

\begin{nitheorem}
    Under the online reinforcement learning setup with ORW-CFM-W2 loss, the data distribution after n epochs evolves according to:
    \begin{equation}
        q_\theta^n\left(x_1\right) \propto\left[w\left(x_1\right) q_\theta^{n-1}\left(x_1\right) \exp \left(-\beta D^{n-1}\left(x_1\right)\right)\right],
    \end{equation}
where, $D^{n-1}\left(x_1\right)=\mathbb{E}_{t, x \sim p_t\left(x \mid x_1\right)}\left[\left\|v^{\theta^{n-1}}(t, x)-v^{\theta_{\text {ref }}}(t, x)\right\|^2\right]$, $\beta=\gamma \alpha$, $\theta^{n-1}$ denotes the model parameters after epoch $n-1$.
\end{nitheorem}


\begin{proof}[Proof of Theorem \ref{theorem: induced data distribution of ORW-CFM-W2}]
Similar to the proof of \ref{theorem: Online Reward Weighted CFM}, we also proceed by induction.

\begin{Case}[Base Case $(n=1)$]
At $n=1, q_\theta^1\left(x_1\right)$ is obtained by training on data sampled from $q\left(x_1\right)$. From previous analysis, we have:

\begin{equation}
    q_\theta^1\left(x_1\right) \propto w\left(x_1\right) q\left(x_1\right) \exp \left(-\beta D^0\left(x_1\right)\right)
\end{equation}

where $D^0\left(x_1\right)$ measures the discrepancy between the initial model (before training) and the reference model.
\end{Case}



Assume that at epoch $n-1$, the data distribution is $q_\theta^{n-1}\left(x_1\right)$. During epoch $n$, the model is trained using data sampled from $q_\theta^{n-1}\left(x_1\right)$. By applying ORW-CFM-W2 Loss, compared to RW-CFM-W2 Loss, the loss function remains the same, but the data distribution changes to $q_\theta^{n-1}\left(x_1\right)$.

The total loss associated with $x_1$ at epoch $n$ is:

\begin{equation}
    \tilde{L}^n\left(x_1\right)=w\left(x_1\right) L_{\text{cfm}}^n\left(x_1\right)+\alpha D^{n-1}\left(x_1\right)
\end{equation}

where $L_{\text {cfm }}^n\left(x_1\right)$ is the data fitting loss of CFM at epoch $n$, and $D^{n-1}\left(x_1\right)$ is the discrepancy from the reference model using parameters $\theta^{n-1}$.

Similarly, we can assume that the probability of $x_1$ being selected for training at epoch $n$ is proportional to $\exp \left(-\gamma \tilde{L}^n\left(x_1\right)\right)$. Therefore:

\begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(-\gamma \tilde{L}^n\left(x_1\right)\right)
\end{equation}

Substituting $\tilde{L}^n\left(x_1\right)$ :

\begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(-\gamma\left[w\left(x_1\right) L_{\text {cfm }}^n\left(x_1\right)+\alpha D^{n-1}\left(x_1\right)\right]\right) .
\end{equation}


Assuming $L_{\text {cfm }}^n\left(x_1\right)$ is approximately constant or negligible compared to $\alpha D^{n-1}\left(x_1\right)$ (i.e., well learned), we simplify:

\begin{equation}
    q_\theta^n\left(x_1\right) \propto w\left(x_1\right) q_\theta^{n-1}\left(x_1\right) \exp \left(-\beta D^{n-1}\left(x_1\right)\right)
\end{equation}


Then, the induction step completes. This concludes the proof.

Besides, we can also derive the proof via the Importance Sampling Perspective as \eqref{equ: importance sampling perspective}. The process is very similar to the offline cases. We just need to substitute $\tilde{L}^n\left(x_1\right)$ into  \eqref{equ: importance sampling perspective}, take out the weights $w(x)$ to form $w(x)q^{n-1}_{\theta}(x_1)$ and reuse Theorem \ref{theorem: Induced Data Distribution}.
\end{proof}


\begin{Corollary}[Recursive Formulation]
\label{corollary: Recursive Formulation of ORW-CFM-W2}
    The data distribution after $N$ epochs training under $ \mathcal{L}_{ORW-CFM-W2}$ is given by:
    \begin{equation}
    \label{equ: close-form of ORW-CFM-W2}
        q_\theta^N\left(x_1\right) \propto\left[w\left(x_1\right)\right]^N q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
    \end{equation}
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: Recursive Formulation of ORW-CFM-W2}]
    By recursively applying Theorem \ref{theorem: induced data distribution of ORW-CFM-W2}, we have:
\begin{equation}
    \begin{aligned}
q_\theta^N\left(x_1\right) & \propto w\left(x_1\right) q_\theta^{N-1}\left(x_1\right) \exp \left(-\beta D^{N-1}\left(x_1\right)\right) \\
& \propto w\left(x_1\right)\left(w\left(x_1\right) q_\theta^{N-2}\left(x_1\right) \exp \left(-\beta D^{N-2}\left(x_1\right)\right)\right) \exp \left(-\beta D^{N-1}\left(x_1\right)\right) \\
& \propto\left[w\left(x_1\right)\right]^N q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
\end{aligned}
\end{equation}
\end{proof}

\begin{Corollary}[Convergence Behavior]
\label{corollary: Convergence Behavior of ORW-CFM-W2}
If $D^n\left(x_1\right)$ decreases with each epoch and $w\left(x_1\right)$ is constant (i.e., $w\left(x_1\right)=w$ ), then $q_\theta^N\left(x_1\right)$ converges towards a distribution that places higher probability on data points where the model aligns closely with the reference model (See $\tau=0$ cases in Fig. \ref{fig: tau control mnist}).
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: Convergence Behavior of ORW-CFM-W2}]
    As $D^n\left(x_1\right)$ decreases, the exponential term $\exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)$ increases for those $x_1$ where the model aligns better with the reference model. With $w\left(x_1\right)$ constant, the weighting does not favor any specific $x_1$, so the distribution $q_\theta^N\left(x_1\right)$ is increasingly influenced by the cumulative discrepancy term, leading to higher probabilities for data points where the model matches the reference model.
\end{proof}



\clearpage
\subsection{Exponential Weight Function}
\label{app: sec exp}
As we discussed, normally we can have $w(x_1) \propto r(x_1)$ and $w(x_1) \propto \mathcal{F}(r(x_1))$. In practice, we normally have several form of the weighting function $\mathcal{F}$ to enable $w(x_1) >0$ \citep{awr,rwr,ddpo}, which maps from reward function to weight. In this section, based on the previous derivation, we focus on a special cases of fine-tuning flow matching model with ORW-CFM-W2 loss, where $w(x_1) \propto \exp (\tau * r(x_1))$, wherein $\tau$ is the temperature coefficient. 

Then we can simply have the following Theorem (For ease of reading, we rewrite the Theorem \ref{theorem: exp case} to be proved in this section as follows):
\begin{nitheorem}
    Given $w\left(x_1\right)=\exp \left(\tau r\left(x_1\right)\right)$, the induced data distribution after $N$ epochs of training under ORW-CFM-W2 Loss is:
\begin{equation}
    q_\theta^N\left(x_1\right) \propto \exp \left(\tau N  r\left(x_1\right)-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right) q\left(x_1\right)
\end{equation}
\end{nitheorem}

\begin{proof}[Proof of Theorem \ref{theorem: exp case}]
    According to Corollary \ref{corollary: Recursive Formulation of ORW-CFM-W2}, we can re-write the final induced data distribution learned under ORW-CFM-W2 loss function as:
\begin{equation}
\label{equ: Recursive Formulation of ORW-CFM-W2}
    q_\theta^N\left(x_1\right) \propto\left[w\left(x_1\right)\right]^N q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
\end{equation}

Let $r\left(x_1\right)$ be a reward function, and $\tau>0$ a temperature coefficient. The weighting function becomes:

\begin{equation}
w\left(x_1\right)=\exp \left(\tau * r\left(x_1\right)\right)
\end{equation}
Substituting into \eqref{equ: Recursive Formulation of ORW-CFM-W2}:
\begin{equation}
    q_\theta^N\left(x_1\right) \propto q\left(x_1\right) \exp \left(\tau N r\left(x_1\right)-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
\end{equation}
Then, the proof concludes.
\end{proof}

\begin{Corollary}[Limiting Behavior]
\label{corollary: Limiting Behavior of exp linear}
    If $\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)$ grows proportionally with $N$, i.e., $\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)=$ $N \delta\left(x_1\right)$, the induced data distribution becomes:

\begin{equation}
    q_\theta^N\left(x_1\right) \propto \exp \left(N\left(\tau r\left(x_1\right)-\delta\left(x_1\right)\right)\right) q\left(x_1\right)
\end{equation}


This suggests that the effective reward is adjusted by $-\delta\left(x_1\right)$ due to the W2 distance bound.
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: Limiting Behavior of exp linear}]
Given the proportional growth, we can write:
\begin{equation}
    \beta \sum_{n=1}^N D^{n-1}\left(x_1\right)=N \delta\left(x_1\right)
\end{equation}

Substituting into the expression for $q_\theta^N\left(x_1\right)$ :

\begin{equation}
    q_\theta^N\left(x_1\right) \propto \exp \left(N \tau* r\left(x_1\right)-N \delta\left(x_1\right)\right) q\left(x_1\right)=\exp \left(N\left(\tau *r\left(x_1\right)-\delta\left(x_1\right)\right)\right) q\left(x_1\right)
\end{equation}

\end{proof}

\begin{Corollary}[The Effect of W2 Discrepancy]
\label{corollary: The Effect of W2 Discrepancy}
    The W2 distance bound $D^{n-1}\left(x_1\right)$ serves as a regularization term that penalizes the discrepancy between the current model $v^{\theta^{n-1}}$ and a reference model $v^{\theta_{\text {ref }}}$. This term affects the induced data distribution by effectively adjusting the reward: $\tilde{r}\left(x_1\right)=r\left(x_1\right)-\frac{\delta\left(x_1\right)}{\tau}$.

The induced distribution then becomes:

\begin{equation}
    q_\theta^N\left(x_1\right) \propto \exp \left(\tau N  \tilde{r}\left(x_1\right)\right) q\left(x_1\right)
\end{equation}

This adjustment means that the W2 distance bound reduces the attractiveness of certain samples based on how much the model deviates from the reference model when conditioned on those samples (See reward-distance reward-off in Figs. \ref{fig: reward-distance trade-off cifar com}).
\end{Corollary}

According to different hyper-parameter settings, we discuss the limiting behavior of the method of the following cases:

\begin{Case}[Dominant Reward Term]
\label{case: exp reward dominant}If $D^{n-1}\left(x_1\right)$ grows slowly with $N$ or $\beta$ is small, the reward term dominates the regularization term, namely $\tau* r\left(x_1\right) \gg \beta \sum_{n=1}^N D^{n-1}\left(x_1\right)$. The induced distribution $q_\theta^N\left(x_1\right)$ concentrates on the $x_1$ maximizing $r\left(x_1\right)$, similar to the case without considering $D^{n-1}\left(x_1\right)$ (See $\tau=10$ in Fig. \ref{fig: tau control mnist}).
\end{Case}


\begin{Case}[Dominant Regularization Term] If $D^{n-1}\left(x_1\right)$ grows rapidly with $N$ or $\beta$ is large,  the regularization term dominates, namely $\beta \sum_{n=1}^N D^{n-1}\left(x_1\right) \gg \tau* r\left(x_1\right)$. The induced distribution is heavily penalized for deviations from the reference model, potentially leading to a distribution similar to the initial distribution $q\left(x_1\right)$ (See $\alpha=10$ in Fig. \ref{fig: alpha control mnist}).
\end{Case}

\begin{Case}[Balanced Terms]
When both terms are balanced,  namely $\tau* r\left(x_1\right) \approx \beta \sum_{n=1}^N D^{n-1}\left(x_1\right)$, the induced distribution reflects a trade-off between maximizing rewards and staying close to the reference model. The model achieves a compromise, focusing on high-reward samples that do not deviate significantly from the reference.
\end{Case}

\begin{Lemma}[Growth Rate of $D^{n-1}\left(x_1\right)$]
\label{lemma: Growth Rate of D}
    Assuming that $D^{n-1}\left(x_1\right) \leq D_{\max }$ for all $n$ and $x_1$, the total W2 distance bound over $N$ epochs satisfies:

\begin{equation}
    \beta \sum_{n=1}^N D^{n-1}\left(x_1\right) \leq N \beta D_{\max }
\end{equation}

\end{Lemma}


\begin{proof}[Proof of Lemma \ref{lemma: Growth Rate of D}]
    Since $D^{n-1}\left(x_1\right) \leq D_{\text {max }}$, summing over $n$ :
\begin{equation}
    \beta \sum_{n=1}^N D^{n-1}\left(x_1\right) \leq \beta N D_{\max }
\end{equation}
\end{proof}

Based on the above derivation, we can discuss the regularization effect of $D^{n-1}\left(x_1\right)$ and how to tune the trade-off coefficient $\beta$.
The W2 distance bound term acts as a regularizer that:
\begin{itemize}
    \item Penalizes Model Drift: Prevents the model from deviating excessively from the reference model $v^{\theta_{\text {ref }}}$.
    \item Encourages Smooth Updates: Ensures that updates to the model parameters are gradual, promoting stability during training.
    \item Balances Exploration and Exploitation: Mitigates the risk of overoptimization to high-reward samples by considering the model's consistency.
\end{itemize}

Besides, the parameter $\beta$ controls the strength of the regularization:
\begin{itemize}
    \item  Large $\beta$ : Strong regularization, the model remains close to the reference, potentially underexploiting high-reward regions and inducing more diversity/exploration (See $\tau=1$ in Fig. \ref{fig: text cifar alpha}.).
    \item Small $\beta$ : Weak regularization, the model prioritizes reward maximization, possibly at the cost of stability and induce the over-optimization problem \citep{ddpo} (See $\tau=0$ in Fig. \ref{fig: text cifar alpha}. )
\end{itemize}

\begin{Corollary}[Optimal $\beta$ for Exploration-Exploitation Trade-off]
    \label{corollary: exp optimal beta}
    An optimal $\beta^*$ exists that balances the influence of the reward term and the W2 distance bound, satisfying:

\begin{equation}
    \beta^*=\frac{\tau \bar{r}}{\bar{D}}
\end{equation}

where $\bar{r}$ is the average reward and $\bar{D}$ is the average W2 distance bound.
\end{Corollary}

\begin{proof}
Setting the magnitudes of the reward and regularization terms equal, we have:
\begin{equation}
    N \tau \bar{r}=N \beta^* \bar{D} \Longrightarrow \beta^*=\frac{\tau \bar{r}}{\bar{D}}
\end{equation}
where $\bar{r}$ is the average reward and $\bar{D}$ is the average W2 distance bound.
\end{proof}

\begin{Remark}
    The regularization terms are crucial for ensuring stable learning dynamics, preventing catastrophic forgetting \citep{forget}. Besides, Regularization terms also offer a trust region constraint like TRPO \citep{trpo} or policy update in proximal  area like PPO \citep{ppo}.
\end{Remark} 

\clearpage
\subsection{Boltzmann Weight Function}
\label{app: sec Boltzmann case}
 In this section, based on the previous derivation, we focus on a special cases of fine-tuning flow matching model with ORW-CFM-W2 loss, where $w(x_1) \propto \operatorname{Softmax} (\tau * r(x_1))$, wherein $\tau$ is the temperature coefficient. 

According to Corollary \ref{corollary: Recursive Formulation of ORW-CFM-W2}, we can also re-write the final induced data distribution learned under ORW-CFM-W2 loss function as:
\begin{equation}
    q_\theta^N\left(x_1\right) \propto\left[w\left(x_1\right)\right]^N q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
\end{equation}

\begin{definition}[Boltzmann Weighting Function]
    The Boltzmann weighting function over the set of all possible $x_1$ is defined as:

\begin{equation}
    w\left(x_1\right)=\operatorname{softmax}\left(\tau r\left(x_1\right)\right)=\frac{\exp \left(\tau r\left(x_1\right)\right)}{Z}
\end{equation}

where $\tau \in \mathbb{R}$ is the temperature parameter, $Z=\sum_{x_1^{\prime}} \exp \left(\tau r\left(x_1^{\prime}\right)\right)$ is the partition function ensuring that $\sum_{x_1} w\left(x_1\right)=1$.
\end{definition}

Then, we can have the following:
\begin{Theorem}
\label{theorem: softmax ORW-CFM-W2}
With $w\left(x_1\right)=\operatorname{softmax}\left(\tau r\left(x_1\right)\right)$, the induced data distribution after $N$ epochs is:
\begin{equation}
    q_\theta^N\left(x_1\right) \propto\left(\frac{\exp \left(\tau r\left(x_1\right)\right)}{Z}\right)^N q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
\end{equation}
\end{Theorem}

\begin{proof}[Proof of Theorem \ref{theorem: softmax ORW-CFM-W2}]
Starting from the recursive formulation:
\begin{equation}
    q_\theta^N\left(x_1\right) \propto w\left(x_1\right) q_\theta^{N-1}\left(x_1\right) \exp \left(-\beta D^{N-1}\left(x_1\right)\right)
\end{equation}


By induction, assume the expression holds for $N-1$ :

\begin{equation}
    q_\theta^{N-1}\left(x_1\right) \propto\left(\frac{\exp \left(\tau r\left(x_1\right)\right)}{Z}\right)^{N-1} q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^{N-1} D^{n-1}\left(x_1\right)\right) .
\end{equation}


At epoch $N$ :

\begin{equation}
    \begin{aligned}
q_\theta^N\left(x_1\right)  &\propto w\left(x_1\right) q_\theta^{N-1}\left(x_1\right) \exp \left(-\beta D^{N-1}\left(x_1\right)\right) \\
& \propto\left(\frac{\exp \left(\tau r\left(x_1\right)\right)}{Z}\right)\left(\frac{\exp \left(\tau r\left(x_1\right)\right)}{Z}\right)^{N-1} q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^{N} D^{n-1}\left(x_1\right)\right) \\
& =\left(\frac{\exp \left(\tau r\left(x_1\right)\right)}{Z}\right)^N q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right)
\end{aligned}
\end{equation}

Thus, the expression holds for $N$. Then, the proof concludes.
\end{proof}

\begin{Corollary}[Simplifying Induced Distribution]
\label{corollary: softmax simple induced distribution}
   The induced data distribution can be expressed as:

\begin{equation}
    q_\theta^N\left(x_1\right) \propto \exp \left(N \tau r\left(x_1\right)-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right) q\left(x_1\right)
\end{equation}

up to a normalization constant independent of $x_1$.
\end{Corollary}

\begin{proof}[Proof of Corollary \ref{corollary: softmax simple induced distribution}]
Since $Z$ is constant with respect to $x_1$, we can absorb $Z^N$ into the proportionality constant:

\begin{equation}
\begin{aligned}
        q_\theta^N\left(x_1\right) & \propto \frac{\exp \left(N \tau r\left(x_1\right)\right)}{Z^N} q\left(x_1\right) \exp \left(-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right) \\
& \propto \exp \left(N \tau r\left(x_1\right)-\beta \sum_{n=1}^N D^{n-1}\left(x_1\right)\right) q\left(x_1\right) .
\end{aligned}
\end{equation}

\end{proof}
Despite the presence of the partition function $Z$, the induced data distribution $q_\theta^N\left(x_1\right)$ under softmax weighting ultimately depends on the unnormalized exponentiated rewards, similar to the exponential weighting case. The key difference lies in the normalization:
\begin{itemize}
    \item Exponential Weighting: Does not normalize $w\left(x_1\right)$, leading to potential numerical instability.
    \item Softmax Weighting: Normalizes $w\left(x_1\right)$ at each epoch, ensuring $\sum_{x_1} w\left(x_1\right)=1$.
\end{itemize}

However, when considering the evolution over multiple epochs, the normalization constants $Z^N$ accumulate but do not affect the proportionality of $q_\theta^N\left(x_1\right)$ with respect to $x_1$. And, in practice, softmax weighting normally requires a large batch size for efficient learning.

\begin{Corollary}[Exploration Exploitation Trade-Off]
    Proper tuning of $\tau$ and $\beta$ is essential to balance exploration (sampling diverse $x_1$ ) and exploitation (focusing on high-reward $x_1$ ):
\begin{itemize}
    \item High $\tau$ : Increases emphasis on high-reward samples.
    \item  High $\beta$ : Increases the penalty for deviations from the reference model.
\end{itemize}
\end{Corollary}

\begin{Remark}
    In practice, the Boltzmann Weight Function can help stabilize the learning process and  keep numerical stability. However, in practice, when the batch size is smaller, the Boltzmann Weight Function methods normally fail to learn a optimal results in similar training steps as Exponential weighting function methods.
\end{Remark}

\clearpage
\subsection{RL Perspectives of ORW-CFM-W2}
\label{app: sec rl perspective of orw-cfm-w2}

In this section, we start to offer a different perspective of the learning behavior of models under ORW-CFM-W2 loss from the perspective of RL.

We first do the following parallels: 1) Data Points $x_1$ as Actions: In our setup, each data point $x_1$ corresponds to an action $a$ in RL. 2) Discrepancy Term as Regularizer: The discrepancy term $D^{n-1}\left(x_1\right)$ acts as a regularizer, similar to entropy regularization or KL divergence penalties in RL.

Then, we define the objective that our RL agents want to optimize:

\begin{definition}[RL Objective of ORW-CFM]
    \begin{equation}
    \mathcal{J}(\theta)=\mathbb{E}_{x_1 \sim q_\theta^n\left(x_1\right)}\left[r\left(x_1\right)\right]-\frac{1}{\lambda} D_{\mathrm{KL}}\left(q_\theta^n\left(x_1\right) \| q_\theta^{n-1}\left(x_1\right)\right)
\end{equation}

where $q_\theta^n\left(x_1\right)$ is the data distribution at epoch $n$. $q_\theta^{n-1}\left(x_1\right)$ is the data distribution at epoch $n-1$. $D_{\mathrm{KL}}$ acts as a regularizer to keep the new distribution close to the previous one. $\lambda$ controls the strength of the regularization.
\end{definition}

We can obtain the following Theorem (For ease of reading, we rewrite the Theorem \ref{theorem: RL perspective of RWR} to be proved in this section as follows):
\begin{nitheorem}
\label{theorem: RL perspective of RWR}
    In RL settings, We aim to find $q_\theta^n\left(x_1\right)$ that maximizes $\mathcal{J}(\theta)$. This involves solving:
\begin{equation}
    q_\theta^n\left(x_1\right)=\arg \max _q \mathbb{E}_{x_1 \sim q}\left[r\left(x_1\right)\right]-\frac{1}{\lambda} D_{\mathrm{KL}}\left(q\left(x_1\right) \| q_\theta^{n-1}\left(x_1\right)\right)
\end{equation}
And the optimal data distribution (policy) $q_\theta^n\left(x_1\right)$ can be obtained as:
    \begin{equation}
        q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)\right)
    \end{equation}
\end{nitheorem}

\begin{proof}[Proof of Theorem \ref{theorem: RL perspective of RWR}]
    The optimization problem can be solved using variational methods. The KL divergence between $q\left(x_1\right)$ and $q_\theta^{n-1}\left(x_1\right)$ is:

\begin{equation}
    D_{\mathrm{KL}}\left(q\left(x_1\right) \| q_\theta^{n-1}\left(x_1\right)\right)=\int q\left(x_1\right) \log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)} d x_1
\end{equation}


Introducing a Lagrangian to enforce the normalization constraint $\int q\left(x_1\right) d x_1=1$ :

\begin{equation}
    \mathcal{L}=\int q\left(x_1\right)\left[r\left(x_1\right)-\frac{1}{\lambda} \log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}\right] d x_1-\eta\left(\int q\left(x_1\right) d x_1-1\right) .
\end{equation}

Then, we take the functional derivative of $\mathcal{L}$ with respect to $q\left(x_1\right)$ and set it to zero:

\begin{equation}
    \frac{\delta \mathcal{L}}{\delta q\left(x_1\right)}=r\left(x_1\right)-\frac{1}{\lambda}\left(\log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}+1\right)-\eta=0
\end{equation}

Simplifying:

\begin{equation}
    r\left(x_1\right)-\frac{1}{\lambda} \log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}-\left(\frac{1}{\lambda}+\eta\right)=0
\end{equation}


Rewriting the equation:

\begin{equation}
    \log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}=\lambda\left(r\left(x_1\right)-C\right)
\end{equation}

where $C=\frac{1}{\lambda}+\eta$ is a constant independent of $x_1$.

Exponentiating both sides:

\begin{equation}
    \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}=\exp \left(\lambda r\left(x_1\right)-\lambda C\right)
\end{equation}


Thus:

\begin{equation}
    q_\theta^n\left(x_1\right)=q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)\right) e^{-\lambda C}
\end{equation}


Since $e^{-\lambda C}$ is a normalization constant, we have:

\begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)\right)
\end{equation}

Then, the proof concludes.
\end{proof}



Similarly, if we introduce the Regularization Term $D^{n-1}\left(x_1\right)$, we can have the following Theorem (For ease of reading, we rewrite the Theorem \ref{theorem: RL perspective of RWR-W2} to be proved in this section as follows):
\begin{nitheorem}
    \label{theorem: RL perspective of RWR-W2}
    Considering an additional discrepancy term $D^{n-1}\left(x_1\right)$ that acts as a regularizer. We can include this term in the objective function:
\begin{equation}
\label{equ: close form RWR with W2}
    \mathcal{J}(\theta)=\mathbb{E}_{x_1 \sim q}\left[r\left(x_1\right)-\beta D^{n-1}\left(x_1\right)\right]-\frac{1}{\lambda} D_{\mathrm{KL}}\left(q\left(x_1\right) \| q_\theta^{n-1}\left(x_1\right)\right)
\end{equation}
Then, the RL agents aim to find $q_\theta^n\left(x_1\right)$ that maximizes $\mathcal{J}(\theta)$. This involves solving:
\begin{equation}
    q_\theta^n\left(x_1\right)=\arg \max _q \mathbb{E}_{x_1 \sim q}\left[r\left(x_1\right)-\beta D^{n-1}\left(x_1\right)\right]-\frac{1}{\lambda} D_{\mathrm{KL}}\left(q\left(x_1\right) \| q_\theta^{n-1}\left(x_1\right)\right)
\end{equation}
And the optimal data distribution (policy) $q_\theta^n\left(x_1\right)$ can be obtained as:
\begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)-\lambda \beta D^{n-1}\left(x_1\right)\right).
\end{equation}
\end{nitheorem}

\begin{proof}[Proof of Theorem \ref{theorem: RL perspective of RWR-W2}]
Following similar steps, the functional derivative becomes:

\begin{equation}
    \frac{\delta \mathcal{L}}{\delta q\left(x_1\right)}=r\left(x_1\right)-\beta D^{n-1}\left(x_1\right)-\frac{1}{\lambda}\left(\log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}+1\right)-\eta=0
\end{equation}

Simplifying:

\begin{equation}
    \log \frac{q\left(x_1\right)}{q_\theta^{n-1}\left(x_1\right)}=\lambda\left(r\left(x_1\right)-\beta D^{n-1}\left(x_1\right)-C\right)
\end{equation}


Exponentiating:

\begin{equation}
    q_\theta^n\left(x_1\right)=q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)-\lambda \beta D^{n-1}\left(x_1\right)\right) e^{-\lambda C}
\end{equation}


Therefore, the updated data distribution is:

\begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)-\lambda \beta D^{n-1}\left(x_1\right)\right)
\end{equation}

Then, the proof concludes.
\end{proof}


\begin{Corollary}[ORW-CFM-W2 As Reinforcement Learning]
    Based on Theorem \ref{theorem: RL perspective of RWR-W2}, we can rewrite the induced optimal policy in  \eqref{equ: close form RWR with W2} as follows:
    \begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda r\left(x_1\right)-\lambda \beta D^{n-1}\left(x_1\right)\right)
\end{equation}
By setting $\beta^{\prime}=\lambda * \beta $, $w(x_1)=\exp (\lambda r(x_1))$, we can obtain:
    \begin{equation}
    q_\theta^n\left(x_1\right) \propto w(x_1) q_\theta^{n-1}\left(x_1\right) \exp \left(-\beta^{\prime} D^{n-1}\left(x_1\right)\right)
\end{equation}
which is the same form of induced data distribution over $x_1$ in Theorem \ref{theorem: induced data distribution of ORW-CFM-W2}.

Therefore, ORW-CFM with W2 Distance Regularization is actually solving a Reinforcement Learning Problem, where the objective is a trade-off between reward and discrepancy. 
\end{Corollary}

The update rule for $q_\theta^n\left(x_1\right)$ mirrors the policy update in RL methods like AWR, where the new policy is proportional to the old policy multiplied by the exponentiated advantage.

 The discrepancy $D^{n-1}\left(x_1\right)$ acts as a penalty term, discouraging the model from deviating too far from the \textbf{reference model}, similar to how KL divergence or entropy regularization is used in RL \citep{awr,ppo} or RLHF \citep{ddpo,dpo} to maintain policy stability and keep the fine-tuned model close to pre-trained model.

\begin{Remark}[Connection to AWR]
AWR \citep{awr} also updates the policy by weighting actions according to the exponentiated advantage. Similarly, our update rule similarly weights data points $x_1$ according to $\exp \left(\lambda * r\left(x_1\right)\right)$.
\end{Remark}

\begin{Remark}[Connection to PPO]
PPO \citep{ppo} uses a clipped objective to prevent large policy updates. In our setup, the regularization term $D^{n-1}\left(x_1\right)$ prevents the data distribution from changing too drastically. Besides, $D^{n-1}\left(x_1\right)$ can also used to limit the distance between ref model and fine-tuned model.
\end{Remark}
\begin{Remark}[Exploration and Exploitation Trade-Off] The term $\exp \left(\lambda * r\left(x_1\right)\right)$ encourages the model to prioritize data points with higher rewards. While the regularization term $\exp \left(-\beta^{\prime} D^{n-1}\left(x_1\right)\right)$ ensures the model does not deviate excessively from the reference, promoting stability and diversity (e.g., avoid the policy collapses \citep{sac})
\end{Remark}

\begin{Corollary}[Learning Flow Matching as Reinforcement Learning]
    In general, by adopting a reinforcement learning perspective, we have derived the evolution of the data distribution $q_\theta^n\left(x_1\right)$ under self-training with the RW-CFM-W2 loss. The derivation shows that:

\begin{equation}
    q_\theta^n\left(x_1\right) \propto q_\theta^{n-1}\left(x_1\right) \exp \left(\lambda * r\left(x_1\right)-\beta^{\prime} D^{n-1}\left(x_1\right)\right)
\end{equation}

This update rule mirrors policy updates in RL methods like Advantage Weighted Regression, where the new policy is updated based on the exponentiated advantage while incorporating a regularization term to ensure stability.
\end{Corollary}

\clearpage


\section{Experimental Details}
\label{sec:app Experiment Details}

\subsection{Implementation Details}
\label{sec:app Implementation Details}

To ensure reproducibility, we built our code upon the open-source flow matching code base called TorchCFM \citep{otcfm,torchcfm2} to demonstrate that our method is general enough to be applied  to fine-tuning existing flow matching based methods, which provides efficient and theoretical sound tools for training continuous normalizing flow models. TorchCFM offers a simulation-free training objective for Conditional Flow Matching (CFM), enabling conditional generative modeling and accelerating both training and inference processes.

We utilized the existing unconditional flow matching training baselines provided by TorchCFM on datasets such as MNIST and CIFAR-10. Our implementations of the RW-CFM, ORW-CFM, and ORW-CFM-W2 methods are based on the TorchCFM codebase, and we adopt the same network architecture—a U-Net \citep{unet}—for the image generation tasks. The pre-trained models used in our experiments are trained with the standard CFM loss as defined in Equation~\eqref{equ: cfm loss} on CIFAR-10 and MNIST datasets.

For the image compression task, we directly adopt the reward function from DDPO \citep{ddpo}. In the even-number generation task, we trained a binary classifier to distinguish between even and odd digits, which serves as the reward signal. For the text-image alignment task, we employ the CLIP model from OpenAI \citep{clip} to calculate the similarity between text and images; the CLIP model is pre-trained and provides a probability score representing the alignment between the generated image and the given text prompt.

The reward functions for each task can be found in App. \ref{app: reward functions}. And the hyper-parameters can be found in App. \ref{app: Hyper-parameters}.

\subsection{Resources Used}
\label{app: Resources Used}

All experiments were conducted on a system with one worker equipped with an 8-core CPU and, an NVIDIA A10 GPU, and memory of 32 GB. The MNIST task cost about 4-8 hours for training, while image compression and text-image alignment task costs about 2-3 days for training. 



\clearpage


\section{Hyper-parameters}
\label{app: Hyper-parameters}

In general, in this paper, we propose a general and easy-to-use RL  method for fine-tuning flow matching generative models. Our algorithm is directly built upon the flow matching training code in TorchCFM \citep{otcfm}. To facilitate reproducibility and highlight the effectiveness of our proposed RL fine-tuning algorithm, we maintain the same flow matching training parameters and the calculation of $u_t$ in OT paths as in the TorchCFM examples. We have conducted detailed ablation experiments in the main text and appendix on the two most important hyperparameters in our method: the entropy control parameter $\tau$ and the W2 distance regularization parameter $\alpha$.

ORW-CFM-W2 method involves only a few hyperparameters, and we have carefully ablated those that affect the convergence behavior in our experiments of Sec. \ref{sec: exp}, namely $\tau$ and $\alpha$. Specifically, $\tau$ controls the convergence speed of the fine-tuning method, while $\alpha$ has a significant impact on the convergence point.

Massive experiments demonstrate that the parameter $\tau$ effectively controls the convergence speed of the policy model. In the absence of W2 regularization, $\tau$ directly influences the rate at which the policy collapses to a delta distribution, as clearly demonstrated in the theoretical analysis of our exponential form weighting form as \ref{theorem: exp case}.

Interestingly, previous methods such as DDPO \citep{ddpo} did not include KL regularization terms or other explicit distance constraints, making it difficult to control the divergence between the optimized policy and the pre-trained reference model. As a result, these methods often struggle to avoid over-optimization and policy collapse. Therefore, when employing our method, we strongly recommend setting $\alpha > 0$ to provide sufficient W2 regularization and prevent policy collapse.


\begin{Remark}
    In practice, we encourage users/readers to first use a small $\alpha$ to fine-tune the model with our ORW-CFM-W2 method, along with a relatively large $\tau$. Then, different values of $\alpha$ can be tried to customize the optimal generative models that fit the user's needs for reward and diversity. 
\end{Remark} 

For all experiments, we use a batch size of 64 and employ a separate test process to evaluate our performance.

In general, the proposed ORW-CFM-W2 method effectively fine-tunes flow matching models. We can control the convergence speed through $\tau$ and adjust the diversity of convergence points and the final reward through $\alpha$. This provides sufficient flexibility for customizing the convergence policy to meet user needs in various scenarios and enables us to achieve stable and efficient RL fine-tuning using a simple reward-weighted method.





\clearpage

\section{Reward Functions}
\label{app: reward functions}

In this section, we will describe the reward function we used in our experiments to demonstrate the effectiveness and generality of our method.

\subsection{Target Image Generation}

In the MNIST target image generation task, our objective is to fine-tune the model to generate images of even numbers. We used a pre-trained binary classifier $C(x)$, where $x$ is the generated image, to determine whether the image corresponds to an even or odd number. The classifier outputs the probability of the image being even or odd:

\begin{equation}
    C_{\text {even }}(x) = p(\operatorname{even} \mid x), \quad C_{\text {odd }}(x) = p(\text { odd } \mid x)
\end{equation}


The reward function is then defined as the difference between the classifier's probabilities of the image being even versus odd:

\begin{equation}
    r(x)=p(\text { even } \mid x)-p(\operatorname{odd} \mid x)
\end{equation}


Where $p($ even $\mid x)=C_{\text {even }}(x)$ is the probability that the generated image $x$ is classified as an even digit (i.e., digits $\{0,2,4,6,8\}$ ), and $p($ odd $\mid x)=C_{\text {odd }}(x)$ is the probability that the generated image $x$ is classified as an odd digit (i.e., digits $\{1,3,5,7,9\}$ ).

This reward encourages the model to maximize $p(\operatorname{even} \mid x)$ while minimizing $p(\operatorname{odd} \mid x)$, guiding the model to generate even digits more frequently. The fine-tuning objective can be formalized as:

\begin{equation}
    \max _\theta \mathbb{E}_{x \sim q_\theta(x)}[r(x)]=\mathbb{E}_{x \sim q_\theta(x)}\left[C_{\text {even }}(x)-C_{\text {odd }}(x)\right]
\end{equation}


This reward function enables the model to be fine-tuned to produce even numbers without needing explicit labels or a dataset directly composed of even digits.

Furthermore, it is worth noting that for this type of binary classifier-guided reward, we can easily modify the task to generate the opposite class of images by simply using the negative of the reward function. For example, to generate odd digits, we define the reward as $r(x)=p$ (odd $\mid$ $x)-p($ even $\mid x)$. The results for generating odd digits are provided in App. \ref{app: gen odd mnist}.

\subsection{Image Compression}
 For the image compression task, we followed the reward function from DDPO \citep{ddpo}, where the goal was to either minimize  the file size of the images after JPEG compression. The reward $r(x)$ is proportional to the compression rate. We controlled the balance between compression and diversity using the regularization parameter $\alpha$, which induces varying degrees of divergence between the fine-tuned model and the reference model. A lower $\alpha$ prioritizes compressibility, leading the model to generate images that occupy minimal storage space after compression, while higher $\alpha$ increases diversity in the generated outputs.


\subsection{Text-Image Alignment Using CLIP}
For the text-image alignment task, we utilized the CLIP model, which computes the similarity between text prompts and generated images. The reward function in this case was based on the probability score generated by the CLIP model. Given an image $x$ and a text description $t$, the CLIP model outputs a probability $p_{\text {clip }}(x, t)$, which indicates the alignment between the image and the text. The reward function for this task was defined as:

\begin{equation}
    r(x, t)=p_{\text {clip }}(x, t)
\end{equation}


This reward allows the model to fine-tune its output images to better match the provided text prompts, optimizing for higher similarity as measured by the CLIP model. In practice, we will use a difference form in probability to promote training like $r(x, \text{"cat"})=p_{\text {clip }}(x, \text{"an image of cat"})-p_{\text {clip }}(x, \text{"not an image of cat"})$.



\clearpage

\section{Additional Experimental Results}


\subsection{Aligning Small-Scale Flow Matching Models}
\label{app: Aligning Small-Scale Flow Matching Models}

We first evaluate by guiding an unconditional generative model pre-trained on CIFAR-10 \citep{torchcfm2} to maximize the text-image similarity score based on CLIP \citep{clip}, specifically aligning the generated images with a given text prompt via reward (e.g., "an image of cat"). As shown in Figure \ref{fig: text cifar alpha}, without W2 regularization (i.e., $\alpha=0$), the ORW-CFM method tends to fall into a greedy policy that maximizes reward but sacrifices diversity. However, with W2 regularization, our agent achieves a more diverse convergent optimal policy while maintaining similar final performance.

\begin{figure}[!ht]
\vspace{-0.1in}
	\centering
   \subfigure[$\alpha=0$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/cifar_text/alpha_control/0.png}
    }
  \subfigure[$\alpha=1$]{
 \includegraphics[width=0.2\linewidth]{figure/main_exp/cifar_text/alpha_control/1.png}
    }
      \subfigure[Reward Curve]{
\includegraphics[width=0.25\linewidth,height=0.125\textheight]{figure/new_main/cifar_text/alpha/reward_curve.pdf}
    }
    \subfigure[W2 Curve]{
\includegraphics[width=0.25\linewidth,height=0.125\textheight]{figure/new_main/cifar_text/alpha/w2.pdf}}
    
	\caption{Diversity Control with different $\alpha$ while $\tau=1$ and reward is Text-Image Similarity Probability Score (e.g., $p_{\text {clip }}(x, \text{"an image of cat"})$ for a cat image). W2 distance is estimated by its upper bound (see App. \ref{app sec: W2 distance}). In practice, we will use a difference form in probability to promote training like $r(x, \text{"cat"})=p_{\text {clip }}(x, \text{"an image of cat"})-p_{\text {clip }}(x, \text{"not an image of cat"})$.}
 \label{fig: text cifar alpha}
 \vspace{-0.1in}
\end{figure}



\subsection{Reward Shaping}
\label{app: gen odd mnist}
To demonstrate the adaptability of our model to various reward objectives, we can directly modify the reward function to change the fine-tuning target. For instance, we transformed the original even-preference reward function, $r = p(even) - p(odd)$, into $r = p(odd) - p(even)$ to generate odd numbers instead. The results show that our method successfully fine-tunes the flow model according to the new reward objective, leading to the generation of only odd numbers.

We set $w(x)$ as $w(x) = \exp(\tau \cdot r(x))$ with $\tau = 1$ (except for the last one with $\tau=0$ as control group), and experimented with different values of $\alpha$ to control the divergence between the fine-tuned model and the reference model, which induces diversity in the convergent fine-tuned model. As illustrated in Fig. \ref{fig: odd gen alpha control}, without W2 regularization, our method converges to the optimal generative policy, though with limited diversity. However, as the W2 regularization increases with higher values of $\alpha$, our model still performs the odd-number generation task effectively, while also showing increased diversity in the generated samples.



\begin{figure}[!ht]
	\centering
   \subfigure[$\alpha=0$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.png}
    }
  \subfigure[$\alpha=0.1$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.1.png}
    }
 \subfigure[$\alpha=0.2$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.2.png}
    }
       \subfigure[$\alpha=0.3$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.3.png}
    }

    \subfigure[$\alpha=0.4$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.4.png}
    }
  \subfigure[$\alpha=0.5$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.5.png}
    }
 \subfigure[$\alpha=0.6$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.6.png}
    }
       \subfigure[$\alpha=0.7$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.7.png}
    }

    \subfigure[$\alpha=0.8$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.8.png}
    }
  \subfigure[$\alpha=0.9$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/0.9.png}
    }
 \subfigure[$\alpha=1$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/alpha_control/1.png}
    }
       \subfigure[$\tau=0$]{
 \includegraphics[width=0.23\linewidth]{figure/app/mnist/odd/tau_control/0.png}
    }
    \label{fig: odd gen alpha control}
    \caption{Odd Number Generation in MNIST.}
\centering
\end{figure}

\subsection{More Text-Image Alignment Results}
In this section, we provide more text-image alignment results:



\begin{figure}[!ht]
	\centering
   \subfigure[$\alpha=0$]{
 \includegraphics[width=0.3\linewidth]{figure/app/text/image/alpha_control/dog/0.png}
    }
      \subfigure[$\alpha=1$]{
\includegraphics[width=0.3\linewidth]{figure/app/text/image/alpha_control/dog/1.png}
    }
	\caption{Diversity Control with different $\alpha$ while $\tau=1$ and reward is Text-Image Similarity Probability Score (e.g., $p_{\text {clip }}(x, \text{"an image of dog"})$ for a dog image). In practice, we will use a difference form in probability to promote training like $r(x, \text{"dog"})=p_{\text {clip }}(x, \text{"an image of dog"})-p_{\text {clip }}(x, \text{"not an image of dog"})$.}
\end{figure}


\begin{figure}[!ht]
	\centering
   \subfigure[$\alpha=0$]{
 \includegraphics[width=0.3\linewidth]{figure/app/text/image/alpha_control/bird/0.png}
    }
      \subfigure[$\alpha=1$]{
\includegraphics[width=0.3\linewidth]{figure/app/text/image/alpha_control/bird/1.png}
    }
	\caption{Diversity Control with different $\alpha$ while $\tau=1$ and reward is Text-Image Similarity Probability Score (e.g., $p_{\text {clip }}(x, \text{"an image of bird"})$ for a bird image). In practice, we will use a difference form in probability to promote training like $r(x, \text{"bird"})=p_{\text {clip }}(x, \text{"an image of bird"})-p_{\text {clip }}(x, \text{"not an image of bird"})$.}
\end{figure}



\clearpage



\section{Algorithm Pseudocode}
\label{app: Pseudocode}

In fact, we can implement the proposed fine-tuning algorithm by making minor changes based on the conditional flow matching implementation in OT-CFM \citep{otcfm}. In this section, we will demonstrate the Pseudocode of each fine-tuning algorithm proposed in this paper. For the origin CFM pipeline we refer the readers to \citep{otcfm}.




\begin{algorithm}[H]
\caption{Reward Weighted Conditional Flow Matching (RW-CFM in Theorem \ref{theorem: rwr-cfm})}
\begin{algorithmic}
\REQUIRE Pre-trained model $v_{\theta_{\text{ref}}}$ and induced sample distribution $q_{\theta_{\text{ref}}}(x_1)$, efficiently samplable $p_t(x|x_1)$,  computable $u_t(x|x_1)$, initial network $v_{\theta}$, reward function $r(x_1)$, weighting function $w(x_1)=\mathcal{F}(r(x_1))$ where $w(x_1) \propto r(x_1)$, entropy hyper-parameter $\tau$, $\mathcal{F}$ can be $\exp$ or $\operatorname{Softmax}$
\STATE $\theta \gets \theta_{\text{ref}}$
\WHILE{Training}
    \STATE $x_1 \sim q_{\theta_{\text{ref}}}(x_1)$; \quad $t \sim \mathcal{U}(0, 1)$; \quad $x \sim p_t(x|x_1)$ 
    \STATE $r \gets r(x_1)$ ;\quad $w = \mathcal{F}(\tau * r)$
    \STATE $\mathcal{L}_{\text{RW-CFM}}(\theta) \gets $ \eqref{equ: RW-CFM loss}
    \STATE $\theta \gets \text{Update}(\theta, \nabla_{\theta} \mathcal{L}_{\text{RW-CFM}}(\theta))$
\ENDWHILE
\RETURN $v_{\theta}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Online Reward Weighted Conditional Flow Matching (ORW-CFM in Theorem \ref{theorem: Online Reward Weighted CFM})}
\begin{algorithmic}
\REQUIRE Parameterized efficiently samplable $q_{\theta}(x_1), p_t(x|x_1)$,  computable $u_t(x|x_1)$, initial network $v_{\theta}$, reward function $r(x_1)$, weighting function $w(x_1)=\mathcal{F}(r(x_1))$ where $w(x_1) \propto r(x_1)$, entropy hyper-parameter $\tau$, $\mathcal{F}$ can be $\exp$ or $\operatorname{Softmax}$, pre-trained model $v_{\theta_{\text{ref}}}$ and induced sample distribution $q_{\theta_{\text{ref}}}(x_1)$
\STATE  $\theta \gets \theta_{\text{ref}}$; \quad $q_{\theta}(x_1) \gets q_{\theta_{\text{ref}}}(x_1)$
\WHILE{Training}
    \STATE $x_1 \sim q_{\theta}(x_1)$; \quad $t \sim \mathcal{U}(0, 1)$; \quad $x \sim p_t(x|x_1)$ 
   \STATE $r \gets r(x_1)$ ;\quad $w = \mathcal{F}(\tau * r)$
    \STATE $\mathcal{L}_{\text{ORW-CFM}}(\theta) \gets$ \eqref{equ: orw-cfm loss}
    \STATE $\theta \gets \text{Update}(\theta, \nabla_{\theta} \mathcal{L}_{\text{ORW-CFM}}(\theta))$
\ENDWHILE
\RETURN $v_{\theta}$
\end{algorithmic}
\end{algorithm}




\begin{algorithm}[H]
\caption{Online Reward Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2 in Theorem \ref{theorem: induced data distribution of ORW-CFM-W2})}
\begin{algorithmic}
\REQUIRE Parameterized efficiently samplable $q_{\theta}(x_1), p_t(x|x_1)$,  computable $u_t(x|x_1)$, initial network $v_{\theta}$, reward function $r(x_1)$, weighting function $w(x_1)=\mathcal{F}(r(x_1))$ where $w(x_1) \propto r(x_1)$, entropy hyper-parameter $\tau$ and W2 hyper-parameter $\alpha$, $\mathcal{F}$ can be $\exp$ or $\operatorname{Softmax}$, pre-trained model $v_{\theta_{\text{ref}}}$ and induced sample distribution $q_{\theta_{\text{ref}}}(x_1)$
\STATE  $\theta \gets \theta_{\text{ref}}$; \quad $q_{\theta}(x_1) \gets q_{\theta_{\text{ref}}}(x_1)$
\WHILE{Training}
    \STATE $x_1 \sim q_{\theta}(x_1)$; \quad $t \sim \mathcal{U}(0, 1)$; \quad $x \sim p_t(x|x_1)$
    \STATE $r \gets r(x_1)$ ;\quad $w = \mathcal{F}(\tau * r)$
    \STATE $\mathcal{L}_{\text{ORW-CFM-W2}}(\theta) \gets$ \eqref{equ: orw-cfm-w2 loss}
    \STATE $\theta \gets \text{Update}(\theta, \nabla_{\theta} \mathcal{L}_{\text{ORW-CFM-W2}}(\theta))$
\ENDWHILE
\RETURN $v_{\theta}$
\end{algorithmic}
\end{algorithm}



\clearpage


\section{Discussion and Limitation}
\label{app: Discussion and Limitation}

Current RL fine-tuning methods, like DPO \citep{dpo} or many reward-based RLHF methods for large language models (LLM) \citep{offline,ReFT,Rest} normally rely on a well-collected or filtered offline dataset, requiring massive manual assistance and high costs, which is far from automated fine-tuning. However, traditional online fine-tuning methods are usually too complicated, with too many hyper-parameters, and it is difficult for us to find the relationship between each hyperparameter and the convergent policy, making the overoptimization problem hard to handle. 




In fact, the existing LLM model and large text-image generative models has a very strong data sampling capability, but traditional online RL fine-tuning methods may easily cause the model to forget old knowledge, completely collapse to a local greedy policy, and fail to jump out of the local optimum. However, if we can control the collapse speed of the policy (via $\tau$) and the distance between the converged policy and the reference policy (via $\alpha$), then the online RL fine-tuning method can not only converge to a good and diverse policy without the need for additional manually labeled/filtered data, but also avoid over-optimization \citep{ddpo}.

In general, one of the primary motivations of our paper is to propose an easy-to-use and controllable reinforcement learning algorithm for fine-tuning continuous flow matching generative models toward arbitrary reward objectives. 
Although flow matching has demonstrated powerful generative capabilities in models like Stable Diffusion 3 \citep[SD3]{sd3}, an easy-to-use and theoretically comprehensive RL fine-tuning method for continuous flow matching models has not been widely studied. Previous methods are either offline \citep{offline}, rely on filtered datasets and cannot be applied to arbitrary reward models \citep{dpo}, or lack an explicit divergence bound between the learned policy and the reference model, which may be incapable of addressing over-optimization and policy collapse issues \citep{ddpo}. Furthermore, most methods have not been extensively applied or studied in continuous flow matching fine-tuning tasks. To address these limitations, we propose an online reward-weighted conditional flow matching with the Wasserstein-2 regularization (ORW-CFM-W2) method based on reward-weighted RL \citep{rwr,awr}. Our goal is to achieve a \textit{controllable RL fine-tuning algorithm} that allows us to control both the convergence speed and the convergence point (e.g., managing the trade-off between reward and diversity, see Fig. \ref{fig: reward-distance trade-off cifar com}).

To this end, we conducted a detailed analysis of the optimal policy distribution induced by our algorithm and experimentally verified the influence of the entropy coefficient $\tau$ and the regularization coefficient $\alpha$ on the convergence speed and convergence point. 

Interestingly, we established the similarity between our method and traditional RL methods with KL regularization in policy updates. We found that $\tau$ effectively controls the speed (or range) of the policy updates, thereby affecting the convergence speed, and when the W2 regularization is absent, it influences the rate of policy collapse. The coefficient $\alpha$ directly affects the diversity of the final convergent policy by controlling the divergence from the reference model—assuming, of course, that the pre-trained model is not a purely greedy policy.

In practice, we recommend using the ORW-CFM-W2 method with at least $\alpha > 0$ during fine-tuning to effectively avoid issues such as policy collapse.

Additionally, since our method does not make any assumptions about the reward function, it can readily adapt to new tasks by simply adjusting the reward, \textbf{without the need to modify other hyper-parameters or collect new datasets manually}. For example, in the MNIST dataset, we can encourage the generation of images of odd digits by defining the reward as $- (p(\text{even}) - p(\text{odd}))$, thereby penalizing even digits and rewarding odd ones (See App. \ref{app: gen odd mnist}). In other words, our method shifts the focus from fine-tuning tasks to designing more interesting reward functions, enabling the creation of various flow matching generative models that meet specific user needs without carefully designed offline dataset.

It is important to mention that there are still many unresolved issues in RL for flow matching fine-tuning tasks, such as data efficiency, more general reward shaping methods, off-policy online fine-tuning methods, and online fine-tuning of large text-image generative models like SD3 \citep{sd3}. These topics warrant further investigation in future work.





\clearpage






\end{document}
