\section{Related Works}
\paragraph{Conditional Flow Matching} Conditional Flow Matching (CFM) ____ and related methods have been pivotal in transforming simple data distributions into complex, task-specific ones. ____ and ____ focus on training flow-based models by conditioning the generative flow on known distributions, allowing for more precise control over the generative process. However, while these models achieve great success in generation tasks, how to fine-tune FM models to fit arbitrary user-defined objectives has not yet been widely studied.



\paragraph{Fine-tuning From Human Feedback} Recent works typically leverage DPO ____ or policy gradient RL methods ____ to achieve RLHF ____. However, since the likelihood of the generative policy cannot be easily calculated in continuous-time flow models, none of these approaches can be easily adapted to fine-tune flow matching models. Additionally, DPO ____ relies heavily on a filtered dataset, which does not work with a general reward model, while DPOK ____ relies on KL divergence, which is computationally inefficient and lacks effective evidence lower bound (ELBO) in FM.


\paragraph{Fine-Tuning Generative Models with RL}
RL has been increasingly adopted to fine-tune generative models, allowing them to adapt to specific downstream tasks by optimizing for user-defined reward objectives. ____ used RL to fine-tune diffusion models to align generated data with task-specific rewards. ____ proposed an offline reward-weighted method,  while ____ proposed an online version without divergence regularization, which either failed to converge into optimal policy  or collapsed into an extremely greedy policy without diversity ____. Other reward-weighted methods like ReFT, ReST ____ are almost  limited to the offline settings with given datasets and lack theoretical analysis of the convergent behavior.  Importantly, how to adopt online RL methods to fine-tune FM models has not yet been widely studied in both theory and practice.