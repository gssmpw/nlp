%\subsection{The MILP constraints for Matching Dependencies and Similarity Encoding} 
%\label{sec:our_approach_milp_MD_SE} 
%In this section, we describe our approach to leveraging the closeness between the classifier $D$ and the hyper-network $D_M^\#$ to reduce the overall complexity of solving the \propa MILP problem. 
\paragraph{Difference intervals} The matching dependencies and relax-if-similar techniques rely on the difference intervals.  
%These constraints bound the differences between the continuous optimization variables of $\mathcal{N}$ and $\mathcal{N}^\#$, effectively guiding the optimizer into computing the optimal solution efficiently. 
%Additionally, these constraints allows reducing the number of boolean variables in $\mathcal{N}^\#$, which in turn decreases the overall complexity of solving the \propa MILP problem. 
A difference interval $I_{z_{m,k}} = [\Delta^l_{z_{m,k}}, \Delta^u_{z_{m,k}}] \in \mathbb{R}^2$ is defined for every matching neurons and is computed by bound propagation and interval arithmetic.
To formally define it, %for every $m\in [L],k\in[k_m],k'\in [k_{m-1}]$, 
we define for each weight $w_{m,k,k'}$ an interval capturing the difference: $I_{w_{m,k,k'}}=w^\#_{m,k,k'}-[w_{m,k,k'},w_{m,k,k'}]$, where $w_{m,k,k'}$ is the real-valued weight in $N$ and $w^\#_{m,k,k'}$ is the weight interval in $N^\#_S$. Similarly, $I_{b_{m,k}}=b^\#_{m,k}-[b_{m,k},b_{m,k}]$. 
We define the difference intervals inductively on $m$.
%For each layer, \tool computes these intervals by propagating the closeness intervals from previous layer through the affine and ReLU functions. Accordingly, it adds the 
%\begin{itemize}[nosep,nolistsep]
%\item 
For the input layer, $\forall k\in[d].\ I_{z_{0,k}}=[0,0]$. 
%\item 
For the other layers, for all $m>0$ and $k\in[k_m]$: 
\begin{align*}
I_{\hat{z}_{m,k}}=  
\left( {b_{m,k}^\#}+\sum_{k'=1}^{k_{m-1}}{w^\#_{m,k,k'}}\cdot  z^\#_{m-1,k'}\right) - \hat{z}_{m,k} =\phantom{aaaaaaa}\\
b_{m,k}+I_{b_{m,k}}+\sum_{k'=1}^{k_{m-1}}(w_{m,k,k'}+I_{w_{m,k,k'}})\cdot  (z_{m-1,k'}+I_{z_{m-1,k'}}) - \hat{z}_{m,k}\\
=I_{b_{m,k}}+\sum_{k'=1}^{k_{m-1}} w_{m,k,k'} \cdot I_{z_{m-1,k'}} + I_{w_{m,k,k'}}\cdot  (z_{m-1,k'}+I_{z_{m-1,k'}}) 
\end{align*}
Here, $+$ and $\cdot$ are the standard addition and multiplication of two intervals (when writing a variable $z$, we mean the interval $[z,z]$).  
     For the ReLU computation, we bound the difference as follows:
%     To illustrate, we start from the output of the previous layer: $z^\#_{m-1,k'} = z_{m-1,k'} + I_{z_{m-1,k}}$. This output is then propagated through the affine layer represented by $w_{m,k,k'} + I_{w_{m,k,k'}}$, resulting in $\hat{z}^\#_{m,k} = \hat{z}_{m,k} + I_{\hat{z}_{m,k}}$. \\
 %$I_{z_{m,k}}=[\max(0,\Delta^l_{\hat{z}_{m,k}}),\max(0,\Delta^u_{\hat{z}_{m,k}}))]$, since ReLU$(x)=\max(0,x)$. 
%
\begin{align*}
\text{ReLU}(\hat{z}^\#_{m,k}) - \text{ReLU}(\hat{z}_{m,k})=
\text{ReLU}(\hat{z}_{m,k} + I_{\hat{z}_{m,k}})- \text{ReLU}(\hat{z}_{m,k})\leq \\
\text{ReLU}(\hat{z}_{m,k}) + \text{ReLU}(\Delta^u_{\hat{z}_{m,k}})- \text{ReLU}(\hat{z}_{m,k})=\max(0,\Delta^u_{\hat{z}_{m,k}})
\end{align*}
Similarly, 
\begin{align*}
\text{ReLU}(\hat{z}^\#_{m,k}) - \text{ReLU}(\hat{z}_{m,k})=
\text{ReLU}(\hat{z}_{m,k} + I_{\hat{z}_{m,k}})- \text{ReLU}(\hat{z}_{m,k})\geq 
%\text{ReLU}(\hat{z}_{m,k} + \Delta^l_{\hat{z}_{m,k}})- \text{ReLU}(\hat{z}_{m,k})=\\
 \text{ReLU}(\hat{z}_{m,k} - (-\Delta^l_{\hat{z}_{m,k}}) )- \\ \text{ReLU}(\hat{z}_{m,k}) \geq
 \text{ReLU}(\hat{z}_{m,k}) - \text{ReLU}(-\Delta^l_{\hat{z}_{m,k}})- \text{ReLU}(\hat{z}_{m,k})=
  -\max(0,-\Delta^l_{\hat{z}_{m,k}})
\end{align*}
The inequality transitions follow from the triangle inequalities for ReLU.
Overall,
$I_{z_{m,k}}=[-\max(0,-\Delta^l_{\hat{z}_{m,k}}),\max(0,\Delta^u_{\hat{z}_{m,k}})]$. 
It is possible to compute a tighter difference interval~\cite{ref_1,ref_2}, but with a longer computation, and in practice it is not required in our setting. %, our difference intervals are effective. %in practice.
% This bounds are because, $ReLU(\hat{z}_{m,k} + I_{\hat{z}_{m,k}})\leq  ReLU(\hat{z}_{m,k} + \Delta^u_{\hat{z}_{m,k}})\leq ReLU(\hat{z}_{m,k}) + ReLU(\Delta^u_{\hat{z}_{m,k}})=ReLU(\hat{z}_{m,k}) + max(0,\Delta^u_{\hat{z}_{m,k}})$. Alternatively,  $ReLU(\hat{z}_{m,k} + I_{\hat{z}_{m,k}})\geq  ReLU(\hat{z}_{m,k} + \Delta^l_{\hat{z}_{m,k}})\geq ReLU(\hat{z}_{m,k}) - |\Delta^l_{\hat{z}_{m,k}}|$.
%\end{itemize} 


 

%To propagate these intervals into the affine layer, \tool rephrases the parameters of the affine layers in the hyper-networks, defined by intervals, by the parameters of the original network's affine layer, and an interval representing the difference between them as follows: $(w^\#_{m,k,k'}, b^\#_{m,k}) = (w_{m,k,k'} + \Delta w^\#_{m,k,k'}, b_{m,k} + \Delta b^\#_{m,k})$. 
%For propagation through the ReLU function, \tool leverages the monotonicity of the ReLU function. Next we formalize.

%\begin{lemma}~\label{lem2}
%Given a layer $m$ with the original classier parameters $(w_{m,k,k'}, b_{m,k})$, the hyper-network parameters $(w_{m,k,k'} + \Delta w^\#_{m,k,k'}, b_{m,k} + \Delta b^\#_{m,k})$, and the closeness intervals of previous layer $I_{m-1,k}$ the closeness intervals of layer $m$ are $I_{m-1,k}=[\text{min},\text{max}](\Delta b^\#_{m,k}+\sum_{k'=1}^{k_{m-1}}{\Delta w^\#}_{m,k,k'}(\cdot{z^\#}_{m-1,k'}+I_{m-1,k}))$.
%\end{lemma} 
%We provide the proof in~\Cref{sec:appex_proofs}. 
%\tool utilizes the propagation lemma to compute closeness intervals for all neurons. Using these intervals, it formulates two types of constraints:
%(1) Matching dependencies $\phi_{MD}$, derived directly from the intervals,such that for each pair of neurons $(n,k)$ it encode $\phi_{MD}(n,k)=\{z^\#_{n,k} \geq z_{n,k}+\Delta^l_{m,n}, z^\#_{n,k} \leq z_{n,k}+\Delta^u_{m,n}\}$.   

\paragraph{Matching dependencies}
 The matching dependencies are linear constraints encoding the difference intervals to prune the MILP's search space. 
%These constraints bound the differences between the continuous optimization variables of $\mathcal{N}$ and $\mathcal{N}^\#$, effectively guiding the optimizer into computing the optimal solution efficiently. 
%Additionally, these constraints allows reducing the number of boolean variables in $\mathcal{N}^\#$, which in turn decreases the overall complexity of solving the \propa MILP problem. 
Given the intervals, the combined constraint is: $$\phi_{MD}=\bigwedge_{m\in[L],k\in [k_m]} \left ( z^\#_{m,k} \geq z_{m,k}+\Delta^l_{z_{m,k}}\wedge  z^\#_{m,k} \leq z_{m,k}+\Delta^u_{z_{m,k}}\right )$$

\paragraph{Relax-if-similar} To reduce the MILP's complexity, relax-if-similar overapproximates neurons in the hyper-network whose difference interval is small (its size is below a small threshold $\tau$). The overapproximation replaces their ReLU constraints defined over a boolean variable by linear constraints overapproximating the ReLU computation as proposed by~\citet{Ehlers17}.
%To keep the overapproximation error small, \boundtool employs this step only for neurons whose difference interval is small (up to 
%a small threshold $\tau$). %, which identifies neurons that are sufficiently close and substitutes the precise encoding in $\mathcal{N}^\#$ with linear constraints. Namely, it introduces a small hyperparamter variable $\tau$ 
Namely, it replaces the constraints of $z^\#_{m,k}$ for these neurons (Eq. (\ref{eq3:row7})) by:
\begin{align*}
\phi_{RiS}=\bigwedge_{\Delta^u_{z_{m,k}}-\Delta^l_{z_{m,k}}\leq \tau }
 \left(z^\#_{m,k} \geq 0 \wedge z^\#_{m,k} \geq \hat{z}^\#_{m,k}\wedge z^\#_{m,k} \leq \frac{u^\#_{m,k}}{u^\#_{m,k} - l^\#_{m,k}} (\hat{z}^\#_{m,k} - l^\#_{m,k}) \right) 
\end{align*}
 %satisfying $|\Delta^u_{z_{m,k}}-\Delta^l_{z_{m,k}}|\leq \tau$.
%(1) $ z^\#_{m,k} \geq 0, $ 
%(2) $ z^\#_{m,k} \geq \hat{z}^\#_{m,k}, $ 
%(3) $ z^\#_{m,k} \leq \frac{u^\#_{m,k}}{u^\#_{m,k} - l^\#_{m,k}} (\hat{z}^\#_{m,k} - l^\#_{m,k})$,  


\begin{comment}
\begin{algorithm}[t]
\caption{MD and SE constraints}
\label{alg:ME_SE}
\DontPrintSemicolon
\KwIn{The classifier $D$, the hyper-network $D^\#$.}
\KwOut{$\phi_{MD}$, $\phi_{SE}$}
\end{algorithm}
\end{comment} 