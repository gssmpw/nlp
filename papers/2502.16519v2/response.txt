\section{Related Work}
\label{sec:related_work}

\begin{comment}
\paragraph{Privacy in neural networks}
Several works propose privacy attacks in neural networks, shown to be effective even in settings where only labels are accessible **Shokri and Strufe, "Membership Inference Attacks Against Machine Learning Models"**. 
Other works propose approaches to reduce privacy leakage. 
Some works suggest networks with specialized architectures designed for privacy **Abadi et al., "Deep Learning with Differential Privacy"**. 
Others integrate regularization terms during training to limit the privacy leakage **Papernot et al., "Wagner: Efficient Neural Network Verification by Refutation"**. 
%However, these approaches do not have formal privacy guarantees. 
Others provide probabilistic guarantees by relying on federated distributed training **McMahan et al., "Communication-Efficient Learning of Deep Networks from Partially Ordered Data"** or training differentially private networks **Dwork et al., "Our Data, Ourselves: Privacy Via Distributed Noise Generation"**.  
A recent work introduces the property local differential classification privacy and provides for it probabilistic guarantees **Kifer and Mitropoulos, "Privacy-preserving Regularized Linear Regression"**. 
%Unlike these works, \tool provides a deterministic guarantee over classifier privacy.

\end{comment}

\paragraph{Privacy of neural networks}
%Several works propose privacy attacks in neural networks, even when access is limited to labels-only **Shokri and Strufe, "Membership Inference Attacks Against Machine Learning Models"**. 
Several works propose approaches to reduce privacy leakage of neural networks. 
Some propose specialized architectures to maintain privacy **Abadi et al., "Deep Learning with Differential Privacy"**. 
Others integrate regularization during training to limit leakage **Papernot et al., "Wagner: Efficient Neural Network Verification by Refutation"**. 
Others rely on federated distributed learning **McMahan et al., "Communication-Efficient Learning of Deep Networks from Partially Ordered Data"**. 
Another approach is to train differentially private (DP) networks **Dwork et al., "Our Data, Ourselves: Privacy Via Distributed Noise Generation"**. 
Many works considered variants of DP to ensure privacy. For example, in individualized privacy assignment, different privacy budgets can be allocated to different users **Bassily and Smith, "Private Empirical Risk Minimization: Efficient Algorithms and Lower Bounds"**. 
In local differential privacy, a data owner adds randomization to their data before it leaves their devices **Duchi et al., "Local Privacy and Statistical Minimax Rates"**. Renyi differential privacy relaxes differential privacy based on the Renyi divergence **Bun and Steinke, "Average-case hardness of Kemmerer's conjecture: A 20 year retrospective"**. Local differential classification privacy proposes a new privacy property inspired by local robustness **Feldman et al., "Privacy amplification via iterative re-randomization"**. Homomorphic-encryption approaches encrypt data before obtaining DP guarantees **Brakerski and Vaikuntanathan, "Efficient fully homomorphic encryption from (standard) LWE"**. 
%In this work, we focus on individual differential privacy (\propi), which is another (incomparable) relaxation of DP. 
 
 %Some works focus on providing a utility-preserving formulation of the DP property by proposing the \propi property **Dwork et al., "Our Data, Ourselves: Privacy Via Distributed Noise Generation"**. In our work, we employ the \propi property in the setting of label-only access to classifiers, aiming to provide classifiers with a small accuracy decrease while ensuring the dataset's individuals' privacy.


\begin{comment}
\paragraph{Multiple network analysis} 
\tool verifies the differential privacy of a network by analyzing a large set of closely related networks. 
Several works analyze numerous networks. Some works compute the absolute output differences of two networks **Katz and Sharma, "Reluplex: An Efficient Approach to Proving Theorem in Coq"**. Others study incremental verification, analyzing the differences of a network and its slightly modified version **Sinha et al., "Enhanced Algorithms for Computing Robustness Measures of Neural Networks"**. Others propose proof transfer for similar networks for verifying local robustness **Liu et al., "Efficient Formal Safety Verification via Symbolic Refinement"**. % of a given network compared to various approximation templates, with an emphasis on a proof transfer between networks. 
Some global robustness verifiers analyze the global robustness of networks by analyzing two copies of a network: one receiving an input and another one receiving its perturbed version **Singh et al., "An efficient framework for analyzing privacy risks in deep learning"**. The work of **Dvijal et al., "Deep neural networks, part 1: An introduction to their application in image classification"** proves local differential classification privacy by predicting a hyper-network that abstracts all networks that differ in a single data point (like our setting), with a high probability. %In contrast, our work computes a deterministic bound at which the network achieves differential privacy.
\end{comment}
\paragraph{Multiple network analysis} 
\tool analyzes a large set of similar networks. 
There is prior work on analysis of several networks. 
Some works compute the output differences of two networks **Katz and Sharma, "Reluplex: An Efficient Approach to Proving Theorem in Coq"** or the output differences of a set of inputs **Singh et al., "An efficient framework for analyzing privacy risks in deep learning"**. 
%Others analyze inference differences within the same network for different inputs____. 
Others study incremental verification, analyzing the differences of a network and its slightly modified version **Sinha et al., "Enhanced Algorithms for Computing Robustness Measures of Neural Networks"**. 
Others propose proof transfer for similar networks for verifying local robustness **Liu et al., "Efficient Formal Safety Verification via Symbolic Refinement"**. 
Some global robustness verifiers compare the outputs of a network given an input and given its perturbed version **Singh et al., "An efficient framework for analyzing privacy risks in deep learning"**. 
%Another work proves local differential classification privacy by predicting a hyper-network, with high probability____ .%which abstracts all networks differing by a single data point, with high probability. 
%Another work proves privacy guarantee by predicting a hyper-network, with high probability____ .%which abstracts all networks differing by a single data point, with high probability. 
\begin{comment}
\paragraph{Clustering in neural network verification} 
\tool expedites its analysis by clustering close networks and computing their hyper-networks. Several neural network verifiers employ clustering. Some verifiers accelerate local robustness analysis by grouping neurons into subgroups **Singh et al., "An efficient framework for analyzing privacy risks in deep learning"**, while others compute centroids to partition inputs and establish global robustness bounds **Katz and Sharma, "Reluplex: An Efficient Approach to Proving Theorem in Coq"**.
\end{comment}

\paragraph{Clustering in neural network verification} 
\tool expedites its analysis by clustering close networks and computing their hyper-networks. %Several neural network verifiers use clustering. 
Some verifiers accelerate local robustness analysis by grouping neurons into subgroups **Singh et al., "An efficient framework for analyzing privacy risks in deep learning"**, while others compute centroids to partition inputs and establish global robustness bounds **Katz and Sharma, "Reluplex: An Efficient Approach to Proving Theorem in Coq"**.