\section{Insight: The Individual Differential Privacy Deterministic Bound}
\label{sec:formalization}
In this section, we present our main insight:
identifying inputs with local sensitivity zero using the  
\emph{network's \propl} (\propa). 
The \propa overapproximates the leaking inputs (whose local sensitivity is not zero) using the network's classification confidence.
Every input that the network classifies with confidence over this bound satisfies 0-\propi. %This enables \tool to repair by adding noise only to inputs whose confidence is below this bound. %and thereby keep the accuracy decrease small.
%The key idea is to deterministically identify all inputs that can potentially violate the differential privacy property. 
%Once identified, these inputs can be treated differently by the classifier's designer to improve the overall classifier's privacy preservation. 
We next provide the definitions, illustrate the \propa, and discuss the challenges in computing it. %begin by describing the 
%attack model and the inputs that leak privacy. We then define a classifier's confidence and our \propa property. with several definitions describing our settings, followed by the definition of our bound and a motivation connecting it to the probabilistic definition of differential privacy.

%\paragraph{Privacy leaking inputs} 
%We begin with formally defining the inputs leaking information. This definition depends on our attack model.
%The definition of the privacy leaking inputs depends on the attack model. 
%We focus on a label-only %(black-box) 
%attack model, 
%where an attacker does not have access to the network's internals and can only query it to obtain the label of a given input $c=\text{argmax}(N(x))$
%(there is no limit on the number of the attacker's queries). %Formally, 
%We focus on a neural network classifier $D: [0,1]^{d} \rightarrow \{0,1\}$. 
%The classifier is trained with an algorithm $\mathcal{A}$ on a training dataset $\mathcal{S}$. 
%We assume a black-box access to the classifier, where a malicious attacker can infer information about the training dataset by querying the classifier's input domain with 
%the attack can query about any input $x\in[0,1]^{d}$ and obtain its classification $c=\text{argmax}(D(x))$. 
%In this setting, an input $x$ causes a classifier $N$ to leak private information, if there 
%is a data point in the training set $x_D\in D$ such that the network $N_{-x_D}$, trained with the same architecture as $N$, the same training algorithm, and the same dataset $D$ except for $x_D$, classifies $x$ differently from $N$: $\text{argmax}(N(x))\neq \text{argmax}(N_{-x_D}(x))$. %a but over the training dataset $\mathcal{S} \setminus \{s_i\}$, where $i \in [\mid\mathcal{S}\mid]$. We call these input queries privacy leaking inputs and formalize them as follows:

%\begin{definition}[Privacy Leaking Input]
%Given a training set $\mathcal{S}$, a training algorithm $\mathcal{A}$, and a classifier $D$ trained by $\mathcal{A}$ over $\mathcal{S}$, an input $x \in [0,1]^d$ is \textit{privacy leaking} if there exists at least one classifier $D'_{i \in [\mid\mathcal{S}\mid]}$ trained using $\mathcal{A}$ over $\mathcal{S} \setminus \{s_i\}$, such that $\arg\max(D(x)) \neq \arg\max(D'_{i \in [\mid\mathcal{S}\mid]}(x))$.
%\end{definition}

%Our main goal is to identify all privacy leaking inputs. 
%Since our focus lies in detecting the inputs associated with different classification results $D(x)$ and $D^{'}_{i \in [\mid\mathcal{S}\mid]}$, we focus on comparing the decision boundaries of the classifiers. 

\paragraph{Leaking inputs}
%To satisfy DP, it is not required to noise the output of inputs that do not leak privacy.  
%Thus, by dynamically determining whether a given input to the network leaks private information, we can keep the added noise to minimum and not lose accuracy because of adding unnecessary noise to inputs that either way do not leak private information.
The set of leaking inputs can be defined by the decision boundaries of all classifiers in $\{N\}\cup \{N_{-x_D}\mid x_D \in D\}$, where $N$ is the classifier trained on the dataset $D$ and $N_{-x_D}$ is the classifier trained on $D\setminus\{x_D\}$.  
%Our main goal is to identify the privacy leaking inputs. %, which are the ones lead to different classification results between $D$ and $D'_{i \in [|\mathcal{S}|]}$. 
%To achieve this, we focus on comparing the decision boundaries between classifiers. 
%Intuitively, if the decision boundaries of all classifies are identical, then no input leaks private information. This follows since all inputs are classified the same, and thus by the DP definition, the training (or repair) algorithm is DP.
 %In practice, there are differences in the decision boundaries of these classifiers. 
 %Intuitively, if an input is on the one side of a decision boundary of $D$ (i.e., classified as some class $c$) and on the other side of a decision boundary of a classifier $D_{-s}$ (i.e., classifies as another class $c'$), for some $s\in \mathcal{S}$, it is a leaking input. %is the space of inputs that leak private information.
The decision boundaries of a classifier partition the input space into subspaces of inputs classified to the same class. %Given the decision boundaries, 
The leaking inputs are the inputs that for $N$ are contained in a subspace labeled as some class and 
for some $N_{-x_D}$, where $x_D \in D$, are contained in a subspace labeled as another class.
%Namely, we can identify the set of privacy leaking inputs by computing the classifiers' decision boundaries. 
%Computing the decision boundaries is highly complex. 
 %differences in the decision boundaries indicate the existence of inputs leading to differential privacy violation. %, typically these inputs can be detected around the decision boundaries of the classifiers. 
We formalize this definition using \emph{classification confidences}. %, which we define for every possible class. 
%
%\paragraph{Classification confidence} 
Given a classifier $N$, an input $x$, and a label $c$,
the classification confidence is the difference between the score of $c$ and the highest score of the other classes: 
%We define the classification confidence for a given classifier $D$, a class $c$ and an input $x$. The classification confidence is the difference of the score of $c$ and the maximal score of any other label $c'$: 
%
  %To obtain the decision boundaries of the classifiers, we begin by representing the network's classification result through the concept of classification sensitivity. Next, we formalize.
%
%\begin{definition}[Classification Confidence]
%Given a classifier $D$, an input $x$, and a label $c$, the classification confidence $\mathcal{C}$ of $D$ for $x$ and $c$ is: 
$\mathcal{C}_{N}^c(x) = N(x)_c - \max_{c' \neq c} N(x)_{c'}.$
%\end{definition}
%
%The classification sensitivity is a term measuring the difference between the classifier's confidence in label $c$ and its maximum confidence in any other label. 
If $\mathcal{C}_N^c(x)$ is positive, then $N$ classifies $x$ as $c$.
Otherwise, $N$ does not classify $x$ as $c$. 
%If it is zero, $x$ is on the decision boundary (such inputs exist due to the continuity of neural networks). %Inputs on the decision boundary of $D$ satisfy $S_{D}^c(x)=0$. 
%Such inputs exist due to the continuity of neural network classifiers. %comprising continues activation functions.     
Given this notation, the set of leaking inputs of class $c$ contains all inputs classified by $N$ as $c$ 
and by one of the other classifiers not as $c$:
$\{x\in [0,1]^d \mid \mathcal{C}_{N}^c(x) > 0 \land \bigvee_{x_D \in D} \mathcal{C}_{N_{-x_D}}^c(x) \leq 0\}$. 
Computing this set requires computing the decision boundaries, which is highly complex. 
Instead, we overapproximate this set. %using \emph{classification confidences}.

\paragraph{The \propa}
%Since computing the exact space of privacy leaking inputs is as complex as computing the exact decision boundaries of all classifiers, we  rely on the classification confidence to overapproximate this space of inputs. 
 We overapproximate the set of leaking inputs by the maximal classification confidence $\beta^*$ of any leaking input. That is, any input whose classification confidence is higher than $\beta^*$ satisfies 0-\propi \emph{without added noise}. Our overapproximation is sound: it does not miss any leaking input. However, it may be imprecise: there may be inputs that are not leaking whose classification confidence is smaller or equal to $\beta^*$. Importantly, adding noise based on our overapproximation enables \tool to satisfy \propi: every leaking input will be noised. Due to the 
  overapproximation, \tool may add noise to inputs that are not leaking, however, as we show in~\Cref{sec:eval}, this approach enables \tool to obtain label-only access to a network with a small accuracy decrease. 
    We call $\beta^*$ the \emph{\propl}, since inputs whose confidence is above $\beta^*$  deterministically satisfy \propi, without adding random noise.
We next formally define it.

%To bound all such inputs, we start by considering inputs from the decision boundaries of the classifier $D$, where the classification difference between the classifier $D$ and the classifiers set $D(x)_{i\in[|\mathcal{S}|]}$ may begin. 
%We then expand to include more inputs, until no classification difference is remained.  
%To obtain this, we propose the bound $\beta$, a positive real number added as a bias to the sensitivity function, such that $S^{\beta}_{D}(x) = \max(0, S_{D}(x) - \beta)$. 

\begin{definition}[Individual Differential Privacy Deterministic Bound (\propa)]~\label{def:main_problem}
Given a dataset~$D$, a training algorithm $\mathcal{T}$, and a network architecture $\widetilde{N}$, %and the classifiers $D_{-s}$, for every $s\in \mathcal{S}$, of the same architecture as $D$ and trained using $\mathcal{A}$ over $\mathcal{S} \setminus \{s\}$, 
the \emph{\propa} is: %$\beta^*\in \mathbb{R}$ is the maximal classification confidence satisfying:
$$%\begin{align*}
\beta^*=\text{argmax}_\beta \exists x \in [0,1]^d\ \exists c\in C \ %\phantom{aaaaaaaaa}\\
%\phantom{aaaaaaaaa}\hspace{1cm}
\left( 
\mathcal{C}_{N}^c(x) \geq \beta \land \bigvee_{ x_D \in D}\ \mathcal{C}_{N_{-x_D}}^c(x) \leq 0\right )
%\end{align*}
$$
%S_{D}^{\beta}(x,c)>0 \Rightarrow  S_{D'_i}(x,c)>0$.
where $N=\mathcal{T}(D,\widetilde{N})$ and $N_{-x_D}=\mathcal{T}(D\setminus \{x_D\},\widetilde{N})$, for $x_D\in D$.
\end{definition}

We note the following.
First, the \propa exists for every network, since there is a maximum confidence for every label~$c$. In the worst case scenario, this bound is equal to the maximum confidence over all labels, in which case the set of leaking inputs is overapproximated by all inputs. Consequently, every input will be noised by \tool.
In the (highly unlikely) best case scenario, this bound is zero, indicating that all classifiers classify the same every input. %In this case, the above optimization problem is infeasible and thus we define $\beta^*\equiv 0$. 
%By definition, $\beta^*$ overapproximates the set of leaking inputs since $\beta^*$ is the maximum confidence over all leaking inputs. 
Second, every higher classification confidence $\beta>\beta^*$ also overapproximates the set of leaking inputs. However, it adds a higher than necessary overapproximation error, which would lead \tool to add unnecessary noise.

To reduce the overapproximation error, we define the \propa for every class $c\in C$:
\begin{align}\label{dpdbc}
\beta^*_c=\text{argmax}_\beta \exists x \in [0,1]^d
\left( 
\mathcal{C}_{N}^c(x) \geq \beta \land \bigvee_{ x_D \in D}\ \mathcal{C}_{N_{-x_D}}^c(x) \leq 0\right )
\end{align}
Computing the \propa for every class $c$ enables \tool to lower the decrease in the network's accuracy: a joint bound for all classes $\beta^*=\max_c \beta^*_c$ would lead \tool to add unnecessary noise to inputs that the network classifies as $c$ with confidence 
in $(\beta_c^*,\beta^*]$.


\paragraph{Illustration}
%We can use the \propa to noise only the inputs whose confidence is below this bound.
We illustrate the \propa on the classifier considered in \Cref{fig::intro}(a),
trained on a 2D synthetic training set comprising 7,000 data points. A labeled data point $(x_D,y_D)$ is $x_D=(x_1,x_2)\in [0,1]^2$ and $y_D\in \{0,1\}$. 
Its \propa-s are $\beta_0^*=6.6$ and $\beta_1^*=15$ and thus $\beta^*=15$.
Our idea is to make label-only access to a network $N$ \propi by adding noise to every input $x$ whose confidence is at most $\beta^*_c$, where $c$ is the class $N$ predicts for $x$.
To visualize the set of inputs overapproximated by the \propa (which will be noised by \tool), we consider an extended definition of the classification confidence: $\mathcal{C}^{c,\beta}_{N}(x) = \max(0, \mathcal{C}_{N}^c(x) - \beta)$. That is, classification confidence up to $\beta$ is set to zero so inputs classified with such confidence are on the extended decision boundary. 
\Cref{fig::formalization_beta} visualizes the extended classification confidence for $\beta\in\{0, \beta_0^*, \beta_1^*, 30\}$. For each $\beta$, it shows the extended classification confidence as a function of the input $(x_1,x_2)$, along with the decision boundaries of the 7,000 classifiers in $\{N_{-x_D}\mid x_D \in D \}$ (in colored lines). For $\beta=0$, the decision boundary of $N$ does not cover all the other classifiers' decision boundaries. Namely, there are leaking inputs not on the extended boundary and thus $\beta=0$ is an underapproximation of the set of leaking inputs. Adding noise only to these inputs will not make the label-only access to the network \propi. For $\beta=\beta_0^*$, all the leaking inputs classified as $0$ are on the extended decision boundary, but there are leaking inputs classified as $1$ not on it. For $\beta_1^*$, the extended decision boundary covers all decision boundaries (i.e., it covers all classification differences) and 
thus $\beta^*_1$ overapproximates the set of leaking inputs.
It is also the minimal bound covering all decision boundaries. Adding noise to these inputs will make the label-only access to the network \propi (in fact, for inputs that the network classifies as $0$, adding noise is required only if their confidence is at most $\beta_0^*$). For $\beta=30$, all decision boundaries are covered by the extended decision boundary and it also provides an overapproximation of the leaking inputs. While adding noise to all these inputs will make the label-only access to the network \propi, it results in an unnecessary decrease in accuracy.
%
%For small $\beta$ values (\Cref{fig::formalization_beta}(a-b)), the analysis proves unsound as it fails to bound all privacy leaking inputs, thereby in this case the bound is not differentially-private. In contrast, at a very high $\beta$ value (\Cref{fig::formalization_beta}(d)), the analysis suffers from significant overapproximation error, erroneously considering inputs as privacy leaking when they are not. Computing the minimal differentially-private bound $\beta_{MIN}$ is therefore ensures the analysis's soundness while minimizing overapproximation errors.
 %
%Conceptually, we view this bound  expand the decision boundaries of the classifier $D$ such that it identify privacy leaking inputs as on the decision boundaries. Thereby, we can conceptually limit the noise to inputs in the extended decision boundary region and make the overall repair DP.
%
%The bias term can be viewed as a parameter allowing to expand the decision boundaries of the classifier $D$ such that it excludes inputs close to the decision boundaries.  That is the larger the value of $\beta$ the more inputs to have zero sensitivity, i.e., to be part of the decision boundaries.  
%Using the $\beta$ bound we are able define our deterministic differentially-private bound as follows:
%When the $\beta$ bound is differentially-private, it partitions the input domain into two disjoint set. The first set consists of inputs defined by $\{ x \mid S_D^{\beta}(x,c) = 0\}$ bounding all privacy leaking inputs. In contrast, the second group is $\{x \mid S_D^{\beta}(x,c)>0\}$, and it has no privacy leaking inputs. 
%It is important to note that from one side when $\beta$ is exceedingly small, it is probable that our differentially-private property fails to be upheld. 
%From the other side, an excessively large $\beta$ ensures differential privacy; however, it also determines a large number of inputs as privacy leaking when, practically,  they do not leak privacy, resulting in a significant overapproximation error. 
%Consequently, our objective is to compute the minimal differentially-private bound $\beta_{MIN}$ that precisely bound the group of privacy leaking inputs. We next define the minimal deterministic differentially-private bound.  
%
%\begin{definition}[The minimal deterministic differentially-private bound]
%Given a training set $\mathcal{S}$, a training algorithm $\mathcal{A}$, a classifier $D$ trained by $\mathcal{A}$ over $\mathcal{S}$, and the classifiers $D'_{i \in [|\mathcal{S}|] }$ trained using $\mathcal{A}$ over $\mathcal{S} \setminus \{s_i\}$, the deferentially-private bound is minimal $\beta_{MIN}$ if:
%\begin{itemize}
%    \item For each $\beta < \beta_{MIN}$ there is a privacy leaking input $x$ at which $S_D^{\beta}(x,c)>0$.   
%    \item For each $\beta \geq \beta_{MIN}$ there is no privacy leaking input $x$ at which $S_D^{\beta}(x,c)>0$.  
%\end{itemize}
%\end{definition}
 %  \Cref{fig::formalization_beta} illustrates the extended decision boundaries. %for different differentially-private deterministic bounds. 
%To illustrate the importance of determining the minimal differentially-private bound, consider~\Cref{fig::formalization_beta}. 
%This figure considers the same classifier introduced in \Cref{fig::intro}(b), for different values of $\beta$. For small $\beta$ values (\Cref{fig::formalization_beta}(a-b)), the analysis proves unsound as it fails to bound all privacy leaking inputs, thereby in this case the bound is not differentially-private. In contrast, at a very high $\beta$ value (\Cref{fig::formalization_beta}(d)), the analysis suffers from significant overapproximation error, erroneously considering inputs as privacy leaking when they are not. Computing the minimal differentially-private bound $\beta_{MIN}$ is therefore ensures the analysis's soundness while minimizing overapproximation errors.
%
 \begin{figure}[t]
    \centering
  \includegraphics[width=1\linewidth, trim=0 295 0 0, clip,page=4]{images/figures.pdf}
    \caption{The conceptually extended decision boundaries as a function of the input $(x_1,x_2)\in[0,1]^2$, for different confidences, including the \propa.}
    \label{fig::formalization_beta}
\end{figure}  
  %
%
%\paragraph{Motivation for our deterministic differentially-private bound}
%Differential privacy is defined in a protoblastic manner bounding the likelihood of observing differences in the outputs of a classifier when trained on two datasets, $\mathcal{S}$ and $\mathcal{S'}$, that differ by a single element. This bounding is defined by two parameters: the privacy loss parameter $\epsilon$, and a probability margin error $\delta$. 
Conceptually, our \propa %enables us a new approach for making a network DP. 
partitions the input space into two parts: a subspace of inputs that deterministically satisfy \propi (if the attacker only queries them, the label-only access is $0$-\propi) and a subspace 
%This ensures absolute privacy within this region, as no differential change in output can be observed.  
that overapproximates the leaking inputs. % of the inputs  whose privacy leakage has to be mitigated. %, in contrast, is designed to contain all potential output differences, providing a controlled environment where privacy breaches can be controlled and mitigated by the classifier's designer.  
%The network designer can opt for different noise functions to ensure \propi in this subspace (demonstrated in~\Cref{sec:eval}). %Note, that our approach is orthogonal to the common approach of integrating random noise into the data processing pipeline to preserve privacy. However, it is able to  guide the classifier designer where to add or to not add that noise. Namely, our approach opens the door for other types of privacy preservation schemes (as we explain in section~\ref{sec:eval}). 
To ensure \propi in this subspace, we employ the exponential mechanism.
\sloppy
\paragraph{Challenges} Computing the \propa is very challenging. First, it is a global property, requiring to analyze the network's computation for \emph{every} input (not confined to a particular dataset). 
%requires computing the network classifier's outputs across a large input domain. 
This analysis has shown to be highly complex~\citep{ref_51,ref_52}, as it involves precise modeling of a network classifier (a highly non-convex function) over a very large space of inputs. 
Second, \propa is defined over a very large number of classifiers, $|D|+1$, where $D$ is the dataset, which tends to be large (several thousand). This amplifies the complexity of the previous challenge. % due to the significant increase in the number of comparisons required. 
Third, \propa is the \emph{maximal} bound $\beta$ satisfying 
$\exists x \in [0,1]^d\ \exists c\in C \ \left( 
\mathcal{C}_{N}^c(x) \geq \beta \land \bigvee x_D \in D.\ \mathcal{C}_{N_{-x_D}}^c(x) \leq 0\right )$, where $\beta$ is a real number, which 
increases the problem's complexity. %In the following section, we introduce our ideas to address these challenges.


  
  
  
  









