\section{Problem Definition: Guarding Label-Only Access to Networks}
\label{sec:IDP} 
%Several works show that neural network classifiers are vulnerable to privacy attacks, and in particular attacks that extract private information of the individuals participating in the classifier's training set~\citep{ref_26, ref_20, ref_28,ref_13}.
%Further, some of these attacks can expose individuals' information even if the network only provides a label-only access (i.e., it only returns the predicted label of any given input)~\citep{ref_12,ref_13,ref_14}.
%We focus on label-only classifiers, where users can query the network for any input and obtain the predicted label. 
In this section, we define the problem of protecting label-only access to neural network classifiers using individual differential privacy. 
We begin by defining the functions that may leak private information in a label-only setting. We then define the problem of creating a randomized mechanism for them that introduces a minor accuracy decrease. Lastly, we describe two naive approaches. %and their limitations. 

%describe how we employ the Individual Differential Privacy (IDP) framework, as described in~\citep{ref_88}, in our neural network classifier settings. IDP provides general framework for interactive privacy protection, where a user-queried function $f$ is performed on the actual dataset $\mathcal{S}$, and then a protected version of the results, $f(\mathcal{S})$, is returned to the user. 

\subsection{Label-Only Queries} 

In this section, we formalize the functions that access the dataset (in our setting, it is also called the training set), thus may leak private information, in our label-only access setting. 

Label-only access to a neural network $N$ maps an input $x\in [0,1]^d$ to a label $c\in C$, where  
$N$ is the network trained by a training algorithm $\mathcal{T}$ given a dataset $D$ and an architecture %(i.e., a neural network with random weights and biases) 
$\widetilde{N}$.
One might consider the training algorithm as the function that leaks private information, since it accesses the dataset to compute the network. 
However, this definition is too coarse for our label-only access setting, where 
users (including attackers) indirectly access the dataset by obtaining the network's predicted label of any input and do not have access to the network's internal parameters.
 Further, making the training algorithm private requires adding random noise to its computations, regardless of the inputs that are later used for querying the network.  
%
Our formalization removes this separation of the network's training and its querying. It defines the functions that access the dataset as an infinite set of functions, one for each possible input $x\in [0,1]^d$ (not only inputs in the dataset). This refined definition enables us later to add noise only to the functions that potentially leak private information. 
%We next define this set of functions.
Formally, given a training algorithm $\mathcal{T}$ and an architecture $\widetilde{N}$,
the set of \emph{label-only queries} is: $\mathcal{F}_{\mathcal{T},\widetilde{N}} = \{f_{x,\mathcal{T},\widetilde{N}} \mid x\in [0,1]^d\}$, where for every $x\in [0,1]^d$ the function $f_{x,\mathcal{T},\widetilde{N}}(D)$ maps a dataset $D$ to the label predicted for $x$ by the classifier trained by $\mathcal{T}$ on $\widetilde{N}$ and $D$. 
To illustrate, consider stochastic gradient descent as the training algorithm $\mathcal{T}_\text{SGD}$ and as a network architecture $\widetilde{N}_{3,5,2}$ a fully-connected network with three layers, consisting of three neurons, five neurons, and two neurons, respectively. 
Then, for example the function $f_{(1,0.2,0.1)}\in \mathcal{F}_{\mathcal{T}_\text{SGD},\widetilde{N}_{3,5,2}}$ maps a dataset 
$D$ to one of the two labels by (1)~training the network $N=\mathcal{T}_\text{SGD}(D,\widetilde{N}_{3,5,2})$, (2)~computing $N((1,0.2,0.1))$ and (3)~returning the label with the maximal score $c=\text{argmax}(N(1,0.2,0.1))$.

Our definition may raise two questions: (1)~is it equivalent to the common practice where a neural network is computed once and then queried? and (2)~is it useful in practice if it (supposedly) requires to retrain the network from scratch upon every new input? 
The answer to both questions is yes. First, it is equivalent because given a dataset $D$ and an  architecture $\widetilde{N}$,
  the training algorithm $\mathcal{T}$ is \emph{deterministic}. We note that if a training algorithm $\mathcal{T}$ makes random choices, we assume we are given its random seed, which makes $\mathcal{T}$ a deterministic function. Namely, every function $f_{x,\mathcal{T},\widetilde{N}}(D)$ computes the exact same network given $\mathcal{T}$, $D$, and $\widetilde{N}$ and thus our definition is equivalent to the common practice. 
  Second, in practice, we do not retrain the network from scratch for every input, we define these functions only for the sake of mathematically characterizing the functions that may leak private information, but eventually the network is trained once (as we explain later). 
    %with training hyperparameters $\theta$ and initial classifier parameters' values $\widetilde{D}$, the classifier $D$ is a deterministic function of the training set. Under our assumptions, each parameter of the classifier is a deterministic function of the training set. Consequently, the classifier, which is a composition of these parameters (functions), is itself a deterministic function of the training set.

%In our neural network classifier settings, we assume a classifier $D$ which is trained on the training set $\mathcal{S}$ using the training algorithm $\mathcal{T}$. The user then queries the classifier with an input $x \in [0,1]^d$ to obtain a classification result $c \in [C]$. The dataset that needs to be protected is the training set $\mathcal{S}$. Each time the user submits an input $x$ to the classifier $D$, the classifier effectively accesses the training set itself. This is because, given a training algorithm $\mathcal{T}$ with training hyperparameters $\theta$ and initial classifier parameters' values $\widetilde{D}$, the classifier $D$ is a deterministic function of the training set. Under our assumptions, each parameter of the classifier is a deterministic function of the training set. Consequently, the classifier, which is a composition of these parameters (functions), is itself a deterministic function of the training set.


%To formalize this, we define a deterministic function $\mathcal{T}_{\theta,\widetilde{D}}$ that maps a training set to a classifier, such that $D = \mathcal{T}_{\theta,\widetilde{D}}(\mathcal{S})$ (assuming standard deterministic training operations). Given an input $x' \in [0,1]^d$, a query function $f_{x'}$ is composed of an input layer defined by $h_0 = x'$, followed by $\mathcal{T}_{\theta,\widetilde{D}}$, such that $f_{x'} = \mathcal{T}_{\theta,\widetilde{D}}(\cdot) \circ h_0$. Finally, applying this query on the training dataset provides the query response $f_{x'}(\mathcal{S}) = \mathcal{T}_{\theta,\widetilde{D}}(\mathcal{S}) \circ h_0$. This query response is, in fact, the classification result $D(x') \in \{0,1\}^{|C|}$. 

\subsection{Problem Definition: \propi Label-Only Queries with a Minor Accuracy Decrease}
In this section, we present our problem of creating a randomized mechanism for our label-only queries, while minimally decreasing their accuracy. 

As explained in~\Cref{sec:preliminary}, creating a randomized mechanism for our label-only queries is possible by adding noise determined by the local sensitivity. We next define the local sensitivity of a label-only query. 
As defined in~\Cref{def:local_sensitivity}, given a function $f_{x,\mathcal{T},\widetilde{N}}$
%$x\in [0,1]^d$, $\mathcal{T}$, $\widetilde{N}$, 
and a dataset $D$,  
the local sensitivity is the maximum difference of $f_{x,\mathcal{T},\widetilde{N}}$ with respect to $D$ and any of its adjacent datasets. 
Since a label-only query returns a label, we define the difference over their one-hot encoding. That is, if $f_{x,\mathcal{T},\widetilde{N}}(D)=c$, then the one-hot encoding is a vector of dimension $|C|$ where the entry of $c$ is one and the other entries are zeros. By this definition, the local sensitivity is zero if for \emph{every} adjacent dataset $D'$, the same label is returned: $f_{x,\mathcal{T},\widetilde{N}}(D)=f_{x,\mathcal{T},\widetilde{N}}(D')$. 
In this case, $f_{x,\mathcal{T},\widetilde{N}}$ is 0-\propi.
It is not zero if there exists an adjacent dataset $D'$ that returns a different label: $f_{x,\mathcal{T},\widetilde{N}}(D)\neq f_{x,\mathcal{T},\widetilde{N}}(D')$.

To illustrate the label-only queries with sensitivity zero, consider \Cref{fig::intro}(a).
As described, in this example, we focus on a binary network classifier trained on a 2D synthetic training set of size 7,000. 
The figure shows the decision boundaries of this classifier along with the decision boundaries of the networks trained on its adjacent datasets. The figure shows that the decision boundaries are not identical but close. Thus, many inputs are classified the same by all classifiers (shown in light blue background). The respective label-only queries of these inputs have local sensitivity of zero. The other inputs are classified differently by at least one network trained on an adjacent dataset. Namely, the local sensitivity of their respective label-only queries is not zero. 

While we could design an \propi randomized version of the label-only queries by adding noise that assumes the maximum local sensitivity, it would add unnecessary noise to queries with local sensitivity zero and unnecessarily decrease their accuracy. 
For example, in \Cref{fig::intro}(a) the local sensitivity of most label-only queries is zero, and thus these queries are 0-\propi without any noise addition. By adding noise only to queries with non-zero local sensitivity, we can obtain an \propi label-only access to the network with overall a minor decrease in its accuracy.
%This allows maintaining the accuracy of the classification while safeguarding the privacy of the individuals in the training dataset. 
%Instead, we wish to add the minimal noise necessary to guarantee that all label-only queries are IDP. 
This is our problem:
 

%In order to achieve $(\epsilon,\delta)$-IDP protection over the query $f_{x'}$, its query response $f_{x'}(\mathcal{S})$ needs to be randomized using noise that is calibrated to the local sensitivity $S_{L}(f_{x'})$. To compute the local sensitivity of this query function, it is required to compute the maximal difference between the responses of running the function $\mathcal{T}_{\theta,\widetilde{D}} \circ h_0$ over the full training dataset $\mathcal{S}$ and all other datasets which are equal to the training dataset but missing a single data point $\mathcal{S} \setminus \{s_{\mathcal{T}}\}$. This is equivalent to finding the maximal difference between the outputs at input $x'$ of the classifier trained over the whole training set, $D(x')$, and all other classifiers trained over the whole training set except for a single entry, $D_{-s}(x')$. Next, we formalize.

\begin{definition}[Problem Definition: Minimal Noise \propi Label-Only Queries]~\label{def:NN_local_sensitivity}
Given a dataset $D$, a training algorithm $\mathcal{T}$, a network architecture $\widetilde{N}$, and a privacy budget $\varepsilon$, our goal is to compute for every label-only query $f_{x,\mathcal{T},\widetilde{N}}$, for $x\in [0,1]^d$, the minimal noise required to make it $\varepsilon$-\propi. 
%Given an input $x'\in[0,1]^d$, a classifier $D$ trained over the whole training set and a set of all classifiers trained over the whole training set except for a single entry, $D_{-s}(x')$, the local sensitivity is given by $max\;||D(x')-D_{-s}(x')||\leq S_{L}(f_{x'})$. 
\end{definition} 
Note that we have access to $D$, $\mathcal{T}$, and $\widetilde{N}$; only the users have label-only access to the trained network (as common in machine-learning-as-a-service platforms).
In the following, for brevity,  
%Since every input $x\in[0,1]^d$ corresponds to a label-only query $f_{x,\mathcal{T},\widetilde{N}}(D)$, we sometimes abbreviate and 
%when we write that an input $x$ satisfies/violates \propi we mean that its label-only query $f_{x,\mathcal{T},\widetilde{N}}(D)$ satisfies/violates \propi. Similarly, when we refer to the local sensitivity of an input $x$, we mean the local sensitivity of its label-only query $f_{x,\mathcal{T},\widetilde{N}}(D)$.
we say that an input $x$ satisfies/violates \propi if its label-only query $f_{x,\mathcal{T},\widetilde{N}}(D)$ satisfies/violates \propi. Similarly, the local sensitivity of $x$ is the local sensitivity of its label-only query $f_{x,\mathcal{T},\widetilde{N}}(D)$.
 We call an input $x$ whose local sensitivity is not zero a \emph{leaking input}, since its label-only query is not 0-\propi. %querying for the label of this input leaks information of individuals in the training set.  

%Depending on the sensitivity function, it is possible to calibrate the required noise level to meet the IDP privacy budget for any input $x\in[0,1]^d $. 
%For some inputs, the local sensitivity is zero, meaning that the network trained on the full training dataset and those missing a data point output the same result. In this case, no noise needs to be added as the user cannot detect any difference between the output results. This ensures that the intuitive definition of DP, captured by IDP, is maintained even without adding noise. 
%Only for inputs with non-zero sensitivity should noise be added to preserve privacy. 

\subsection{Naive Algorithms} \label{sec:naive}
In this section, we describe two naive algorithms that solve our problem but are inefficient: either in terms of accuracy or in terms of time and space. 

\paragraph{The naive noise algorithm} The first naive algorithm invokes the training algorithm $\mathcal{T}$ on the given dataset $D$ and the network architecture $\widetilde{N}$ to compute $N$. Its label-only access is defined as follows. Given an input $x\in [0,1]^d$,
 it computes $f_{x,\mathcal{T},\widetilde{N}}(D)$
 by passing $x$ through $N$. Then, it ensures $\varepsilon$-\propi by employing the \emph{exponential mechanism} with the maximum local sensitivity. %which selects the label to return 
 %to ensure $\varepsilon$-\propi.  
We next provide background on the exponential mechanism and explain how this algorithm uses it. %to guarantee $\varepsilon$-\propi.

The exponential mechanism is a popular approach to provide DP guarantees for non-numerical algorithms, such as classification algorithms~\citep{ref_91,Dwork06,ref_92}. In these scenarios, typically, the output range is discrete and denoted $\mathcal{R}$.  
%. is typically discrete and denoted by $\mathcal{R}$, where $r$ represents a possible discrete output. 
%Over the utility function $u$ and any two adjacent datasets $D, D' \in \mathcal{D}$, 
The exponential mechanism defines the \emph{global} sensitivity with respect to a given
\emph{utility} function $u:\mathcal{D} \times \mathcal{R} \rightarrow \mathbb{R}$, 
mapping a dataset and an output to a score.
The global sensitivity is: $\Delta u = \max_{r \in \mathcal{R}} \max_{D,D' s.t. ||D - D'|| \leq 1} |u(D,r) - u(D',r)|$.
To ensure $\varepsilon$-DP, the exponential mechanism returns an output $r \in \mathcal{R}$, 
with a probability proportional to $\exp(\varepsilon \cdot u(D,r) / (2\Delta u))$ (we refer to this process as adding noise to the output $r$). 

\sloppy
Given a query 
$f_{x,\mathcal{T},\widetilde{N}}: \mathcal{D} \rightarrow C$ (mapping a dataset to a class), 
we define a utility function $u_{x,\mathcal{T},\widetilde{N}} : \mathcal{D} \times C \rightarrow \{0,1\}$ (we sometimes abbreviate by $u$ to simplify notation).
The utility function $u$ maps a dataset $D$ and a class $c$ to $1$ if $f_{x,\mathcal{T},\widetilde{N}}(D)=c$ and to $0$ otherwise.
%Formally, $\forall D\in \mathcal{D}\forall c\in C$,  
%$u(D,c) = c=f_{x,\mathcal{T},\widetilde{N}}(D)? 1 : 0$. %if $c=f_{x,\mathcal{T},\widetilde{N}}(D)$ and $u(D,c) = 0$, otherwise.
For example, if $f_{x,\mathcal{T},\widetilde{N}}(D)=c_0$, then  $u_{x,\mathcal{T},\widetilde{N}}(D,c_0)=1$ and   $u_{x,\mathcal{T},\widetilde{N}}(D,c)=0$, for $c\in C\setminus\{c_0\}$.
%its corresponding value in the one-hot vector representation of the classification result $c$ from the query $f_{x,\mathcal{T},\widetilde{N}}(D)$. 
%Specifically, for a given classification result $c$, $u(D,c') = \delta_{c,c'}$, where $\delta_{c,c'}$ is the Kronecker delta. 
Given a dataset $D$, the local sensitivity of $f_{x,\mathcal{T},\widetilde{N}}$ is $\Delta u = \max_{c \in C} \max_{D' s.t. ||D - D'|| \leq 1} |u(D,c) - u(D',c)|$. 
Note that $\Delta u\in\{0,1\}$. 
If our algorithms identify $\Delta u=0$, they do not invoke the exponential mechanism.
That is, they always invoke it with $\Delta u=1$. Thus, our exponential mechanism
%In the zero case the query returns the same label for $D$ and any adjacent dataset.
%This sensitivity can take on only two possible values: 'zero' if the classification results for the query over the entire training set and any training set differing by a single input are identical, and 'one' otherwise. 
%When the sensitivity is zero, the privacy property $\propi$ holds, and there is no need to apply any noise mechanism. 
returns a class $c \in C$ with a probability proportional to $\exp(\varepsilon\cdot u(D,c) / 2)$.  %thereby ensuring $\varepsilon$-differential privacy for the query function. In our specific case, $\Delta u = 1$, so the probability is $\exp(\varepsilon u(D,c') / 2)$. Next we formalize (Proof is in~\Cref{sec:exponential_mechanism}) 
Our next lemma states that this mechanism guarantees $\varepsilon$-\propi:

\begin{restatable}[]{lemma}{ftb}
\label{lemma::exponential_mechanism}
Given a dataset $D$, a query $f_{x,\mathcal{T},\widetilde{N}}$, and a privacy budget $\varepsilon$, the exponential mechanism with our utility function $u_{x,\mathcal{T},\widetilde{N}}$ 
%and a sensitivity $\Delta u = 1$ that outputs a classification $c' \in [C]$ with probability proportional to $\exp(\varepsilon u(D,c') / 2)$, ensures 
is $\varepsilon$-\propi.
\end{restatable}
%\Cref{sec:proofs} 
The proof \ifthenelse{\EXTENDEDVER<0}{(\Cref{sec:proofs})}{\cite[Appendix A]{ref_105}} is similar to the proof showing that this mechanism is DP (with respect to the global sensitivity).
  This lemma implies that the naive noise algorithm solves \Cref{def:NN_local_sensitivity}.
However, it introduces a very high noise since for any input it invokes the exponential mechanism with the maximum local sensitivity. As we show in~\Cref{sec:eval}, its accuracy decrease is even higher than DP training algorithms.
In fact, in this case, the maximum local sensitivity is equal to the maximum global sensitivity, thus the naive noise algorithm is DP. 


\paragraph{The naive \propi algorithm} This naive algorithm improves the accuracy of the previous algorithm by identifying the local sensitivity of each query, instead of bounding it by the maximum local sensitivity.  
It begins by %Given a dataset $D$, a training algorithm $\mathcal{T}$, a network architecture $\widetilde{N}$, and a privacy budget $\varepsilon$, the naive algorithm 
training the set of classifiers for $D$ and all its adjacent datasets.
That is, it computes $\mathcal{N}=\{N\}\cup \{N_{-x_D}\mid x_D\in D \}$, where $N=\mathcal{T}(D,\widetilde{N}) $ and $N_{-x_D}=\mathcal{T}(D\setminus\{x_D\},\widetilde{N}) $.
Its label-only access is defined as follows. Given an input $x\in [0,1]^d$, to compute $f_{x,\mathcal{T},\widetilde{N}}(D)$ it passes $x$ through all networks in $\mathcal{N}$. If all networks return the same label, it returns the label as is. 
Otherwise, it employs the exponential mechanism, exactly as the naive noise algorithm does. %which selects the label to return in a way that ensures $\varepsilon$-\propi. %adds noise to $N$'s predicted label and returns the noised label. 
%We next provide background on the exponential mechanism and explain how the naive algorithm uses it to guarantee $\varepsilon$-\propi.  
Namely, it employs the exponential mechanism only for inputs whose local sensitivity is not zero and thus  
obtains $\varepsilon$-\propi with minimum accuracy decrease.
%since it identifies correctly the local sensitivity of every label-only query and employs the exponential mechanism only if necessary. 
However, as we show in~\Cref{sec:eval}, its inference time is high, 
because every input passes through $|D|+1$ classifiers.
Also it poses a significant space overhead, because it requires to store in the memory all $|D|+1$ classifiers throughout its execution.

%our goal is to compute for every label-only query $f_{x,\mathcal{T},\widetilde{N}}$, for $x\in [0,1]^d$, the minimal noise required to make it $\varepsilon$-IDP. 
%Our extension of IDP to neural network classifiers can be naively implemented by running $|\mathcal{S}| + 1$ classification tasks for each user query, and then comparing the outputs of all classifiers to determine the appropriate noise to add before returning the result to the user. Although this approach is straightforward, it is impractical due to the significant overhead it introduces in terms of time complexity and computational power needed to run the input of all the classifiers, and memory requirements needed to store all classifiers. To address these challenges, we propose the idea of \propl (\propa), which we describe next.
