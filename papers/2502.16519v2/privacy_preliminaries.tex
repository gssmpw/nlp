\section{Preliminaries}
\label{sec:preliminary}
This section provides background on neural network classifiers and individual differential privacy.
  
\paragraph{Neural network classifiers}
%We focuse a $\text{ReLU}$ classifiers accepting a one-dimensional input vectors and classifies it into two classes.
%Given an input vector $[0,1]^{d}$ and a set of classes $C=\{0,1\}$, a classifier maps this to a score vector over the possible classes $N:[0,1]^{d}\to \mathbb{R}^2$.
A classifier maps an input into a score vector over a set of labels (also called classes) $C$, i.e., %For simplicity's sake, we focus on binary classifiers, where an input is mapped into two scores, denoted $C=\{0,1\}$, but our definitions extend to multi-label classifiers. 
it is a function: $N:[0,1]^d\to \mathbb{R}^{|C|}$. 
Given an input $x$, its classification is the label with the maximal score: $c= \text{argmax}(N(x))$. 
We focus on classifiers implemented as neural networks.
A neural network consists of an input layer followed by $L$ layers, where the last layer is the output layer. 
%The layers are functions, denoted $h_0, h_1, \dots, h_{L}$, each taking as input the output of the preceding layer. The network's function is the composition of the layers: $N(x)=h_{L}\circ h_{L-1}\cdots \circ h_0 =h_{L}(h_{L-1}(\cdots(h_0(x))))$.
%Layers consist of neurons, where neurons are connected to neurons (some or all) in the next layer. 
Layers consist of neurons, each connected to some or all neurons in the next layer. 
A neuron computes a function, which is typically a linear combination of its input neurons followed by a non-linear activation function. 
The output layer consists of $|C|$ neurons, each outputs the score of one of the labels.
%
We denote by $z_{m,k}$ the $k^\text{th}$ neuron in layer $m$, for $m\in \{0,\ldots,L\}$ (where $0$ denotes the input layer) and $k\in [k_m]=\{1,\ldots,k_m\}$. 
The input layer $z_0$ passes the input $x$ to the next layer (i.e., $z_{0,k}=x_k$ for all $k\in [d]$).
%The last layer outputs a vector, denoted $D(x)$, consisting of a score for each class in $C$. The classification of the network for input $x$ is the class with the highest score, $c= \text{argmax}(D(x))$.
 %The layers are functions, denoted $n_1, n_2, \dots, n_{L}$, each taking as input the output of the preceding layer. 
 %The network's function is the composition of the layers: $D(x)=n_{L}(n_{L-1}(\cdots(n_1(x))))$.
%A layer $m$ consists of $k_m$ neurons, denoted $z_{m,1},\ldots,z_{m,k_m}$. 
 The neurons compute a function determined by the layer type. %, e.g., fully-connected, convolutional. %The function of layer $m$ is defined by its neurons, i.e., it outputs the vector $(z_{m,1},\ldots,z_{m,k_m})^T$.
  Our definitions focus on fully-connected layers, but our implementation also supports convolutional layers and it is easily extensible to other layers, such as max-pooling layers~\citep{ref_41} and residual layers~\citep{ref_40}. 
  In a fully-connected layer, a neuron $z_{m,k}$ 
   gets as input the outputs of all neurons in the preceding layer. 
   It has a weight for each input ${w}_{m,k,k'}$ and a bias $b_{m,k}$. Its function is the weighted sum of its inputs and bias followed by an activation function. We focus on the ReLU activation function. 
   Formally, the function of a non-input neuron is $z_{m,k}=\text{ReLU}(\hat{z}_{m,k})=\max(0,\hat{z}_{m,k})$, where $\hat{z}_{m,k}$
   is the weighted sum:
 $\hat{z}_{m,k}=b_{m,k}+\sum_{k'=1}^{k_{m-1}}{w}_{m,k,k'}\cdot{z}_{m-1,k'}$. 
%
%\paragraph{Training}
The network's parameters -- the weights and biases -- are determined by a training algorithm. We focus on supervised learning, where the training algorithm~$\mathcal{T}$ is provided with an initial network $\widetilde{N}$ (e.g., with random parameter values), called the network architecture, and a dataset $D \subseteq [0,1]^d \times C$ of labeled data points $(x_D,y_D)$. In the following, we abbreviate a pair $(x_D,y_D)$ by $x_D$. 
%The training algorithm $\mathcal{T}$ minimizes a loss function $\mathcal{L}$ capturing the penalty for misclassification. The goal of the training is to find network parameters $w$ and $b$ (the weights and biases) that yield an acceptably small loss, hopefully the smallest loss. 
%In our work, we focus on the SGD optimizer. Using the SGD optimizer, the training algorithm updates the network parameters based on their gradients with respect to the average of the loss over the training set $\{s_1, \ldots, s_N\}$, so $L(w,b) = \frac{1}{N} \sum_i L(w,b, s_i)$, with each iteration called a training epoch. Specifically, at each epoch $i$, the parameters are updated as follows:
%$ w_i = w_{i-1} - \nabla_{w_{i-1}} \left( \frac{1}{N} \sum_i L(w_{i-1}, b_{i-1}, s_i) \right)$
%with $w_0 = \widetilde{w}$. 
%For neural networks, the loss function $L$ is usually non-convex and difficult to minimize. Therefore, in practice, the minimization is often done by the mini-batch stochastic gradient descent (SGD) algorithm. In this algorithm, at each step, a batch $B$ of random training samples is formed, and the gradient $\frac{1}{|B|} \sum_{x \in B} \nabla_w L(w, b, x)$ is computed as an estimation of the gradient $\nabla_w L(w, b)$. The parameters $w$ are then updated based on the gradient towards a local minimum. Namely the minim-batch algorithm splits the epoch into several batches. 
The exact computation of the training algorithm  $\mathcal{T}$  is irrelevant to our approach. In particular, it may run a long series of iterations, each consisting of a large number of computations (e.g., over the parameters' gradients).  
The important point in our context is that given a network architecture $\widetilde{N}$ and a random seed (in case $\mathcal{T}$ has randomized choices), the training algorithm is a deterministic function mapping a training set $D$ into a neural network.  
%Note that given an initial values of the classifier parameters, the training algorithm along with its hyperparameters, and a random seed which specifies the order and the selection of the training samples within the mini-batch approach, the network paramters are a deterministic function of the training inputs. 
  



%where each sample is represented as an input vector $[0,1]^d$ associated with an output label $C=\{0,1\}$. %The algorithm $\mathcal{A}$ utilizes the dataset $\mathcal{S}$ to train a neural network classifier $D$ tasked with classifying input vectors into two classes. 
%Unlike SGD-DP, our approach treats training as a black-box computation. Thus, we present it at high-level. A training procedure determines the network's parameters by numerical optimization, which minimizes a given loss. For example, the cross-entropy loss captures the number of differences between the network's classification and the true-label, over the training set.

\begin{comment}
\paragraph{Differential privacy}
Differential pr9999-/ivacy (DP) is a popular privacy property that quantifies the maximal privacy leakage of a randomized algorithm~\cite{Dwork06}. 
In the context of neural networks, a randomized algorithm (a training or a repair algorithm) is DP if the inclusion or exclusion of a single data point from the training set does not significantly affect the output~\cite{ref_22,ref_36,ref_37,ref_58}.
Formally, a randomized algorithm $\mathcal{A}$ satisfies $(\varepsilon, \delta)$-differential privacy if for any two adjacent training sets $\mathcal{S}$ and $\mathcal{S}'$ differing by one data point, the following holds for all sets of observed outputs $\mathcal{O} \subseteq \text{Range}(\mathcal{A})$:
\begin{equation}\label{dp}
\Pr[\mathcal{A}(\mathcal{S}) \in \mathcal{O}] \leq e^{\varepsilon} \cdot \Pr[\mathcal{A}(\mathcal{S}') \in \mathcal{O}]+ \delta
\end{equation}
where $\varepsilon,\delta$ are small non-negative numbers: $\varepsilon$ bounds the privacy loss and $\delta$ bounds the probability of a privacy failure (ideally, it is close to 0).
The type of observed outputs depends on the attack model. In a white-box setting, the observed output of a training or a repair algorithm is the neural network. In a label-only setting, which is our focus, the observed outputs are the labels that the network assigns to inputs.
%In our work, we focus on analyzing differential privacy (DP) for neural network classifiers with black-box access, i.e., the attacker has access to the inputs and the outputs (labels) of the classifier. 
Formally, in this setting, a training or a repair algorithm $\mathcal{A}$ is DP if, for any training sets $\mathcal{S}$ and $\mathcal{S}'$ that differ by one data point, the networks trained on $\mathcal{S}$ and $\mathcal{S}'$ by $\mathcal{A}$ return the same label for any input with the probability defined by~\Cref{dp}. 
\end{comment}
\paragraph{Differential privacy} 
Differential privacy (DP)~\cite{Dwork06} is a popular privacy property designed to protect the privacy of individual data points in a dataset, which is being accessed by one or more functions $f$. 
The original DP definition assumes that $f$ is invoked interactively by the user. 
%The query-response process is defined with respect to a function over a dataset $S$. 
To avoid privacy leakage, $f$ is protected by a mechanism $\mathcal{A}$, which is a randomized version of $f$ adding random noise to its computations.
The added noise ensures that the inclusion or exclusion of any single data point in the dataset does not significantly affect the function's output. Formally:

\begin{definition}[Differential Privacy]~\label{def:differential_privacy}
A randomized mechanism $\mathcal{A}$, over a domain of datasets $\mathcal{D}$, is $\varepsilon$-differentially private if for any two adjacent datasets $D,D'\in \mathcal{D}$ (differing by one data point), for all sets of observed outputs $\mathcal{O} \subseteq \text{Range}(\mathcal{A})$ we have:  $\Pr[\mathcal{A}(D) \in \mathcal{O}] \leq e^{\varepsilon} \cdot \Pr[\mathcal{A}(D') \in \mathcal{O}]$. 
\end{definition}
In this definition, $\varepsilon$ is a small non-negative number, bounding the privacy loss. %and $\delta$ bounds the probability of a privacy failure (ideally, it is close to 0). 
A common approach to create a randomized version of $f$ is by adding noise that depends on $f$'s \emph{global sensitivity}
%Given a query function $f$ running over a database $\mathcal{S} \in \mathcal{S}^d$ and outputting a query result $r \in \mathbb{R}^n$ (for example, $f$ which queries the sum of a database $\mathcal{S}$), the DP model aims to find a randomized mechanism $\mathcal{A}_f$ that computes a randomized version of the response $f(\mathcal{S})$ such that the differential privacy property holds. A common approach to obtain $\mathcal{A}_f$ is by using additive noise calibrated to the global sensitivity of the query function 
$S(f)$~\citep{Dwork06,ref_22,ref_58,ref_36,ref_37}. The global sensitivity is the maximum difference of $f$ over any two adjacent datasets in $\mathcal{D}$. Formally: 

\begin{definition}[Global Sensitivity]~\label{def:globall_sensitivity}
The global sensitivity of a function $f:\mathcal{D}\rightarrow \mathbb{R}^n$ is \\
$S(f)=\max_{D,D'\in \mathcal{D} s.t. |D-D'|= 1}{||f(D)-f(D')||}$.
%$ if for any two adjacent datasets $\mathcal{S}$ and $\mathcal{S}'$ differing by one data point: $||f(\mathcal{S})-f(\mathcal{S}')||\leq S(f)$. 
\end{definition}

Differential privacy and global sensitivity are defined with respect to any two adjacent datasets over the dataset domain $\mathcal{D}$. 
While such definitions are useful in many real-world scenarios, in the context of machine learning, it may be too restrictive. 
While there may be scenarios where network designers wish to design a DP training algorithm, protecting the privacy of any dataset, in other scenarios, network designers may only care about protecting the privacy of data points in a \emph{given} dataset. 
For example, a network designer that has collected private information of patients towards designing a network predicting whether a new person has a certain disease may not care whether their training algorithm does not leak private information of the MNIST image dataset~\citep{ref_41} (or any other dataset for this matter). However, even if they do not care about these datasets, ensuring DP may force them 
to add a significantly higher noise because these irrelevant datasets may increase the global sensitivity of the training algorithm. Consequently, the accuracy of the network that the designer cares about can be substantially reduced. In fact, as we demonstrate in~\Cref{sec:eval}, in some cases, the network's classification accuracy can be reduced to that of a random classifier.
In such scenarios, it may be better to relax DP and ensure the privacy of data points in a \emph{given} dataset. This property is called \emph{individual differential privacy}.  
  

\paragraph{Individual differential privacy} 
Individual Differential Privacy (\propi)~\citep{ref_88} is a relaxation of DP, designed to protect the privacy of individual data points in a \emph{given} dataset. 
%an alternative formulation of differential privacy that aims to relax the strict (and often more-than-required to satisfy the intuitive privacy guarantee that DP seeks) DP requirement of indistinguishability of results between \emph{any pair} of adjacent datasets. 
Similarly to DP, a randomized mechanism satisfies \propi if its output is not significantly affected 
by the inclusion or exclusion of any single data point in the given dataset. Formally:
%Instead, IDP focuses only on the \emph{actual dataset} and its adjacent datasets. 
%IDP maintains the same intuitive privacy guarantees for individuals as standard DP, ensuring that the presence or absence of any individual in a dataset does not significantly affect the results of data analyses. 
%Specifically, it guarantees that the output of any query remains indistinguishable between the actual dataset and its adjacent datasets. 
\begin{definition}[Individual Differential Privacy]~\label{def:individual_differential_privacy}
Given a dataset $D\in \mathcal{D}$, a randomized mechanism $\mathcal{A}$, over a domain of datasets $\mathcal{D}$, is $\varepsilon$-individually differentially private if for any dataset $D'$ adjacent to $D$ (differing by one data point), for all sets of observed outputs $\mathcal{O} \subseteq \text{Range}(\mathcal{A})$ we have:  \\
 $e^{-\varepsilon} \cdot \Pr[\mathcal{A}(D') \in \mathcal{O}] \leq\Pr[\mathcal{A}(D) \in \mathcal{O}] \leq e^{\varepsilon} \cdot \Pr[\mathcal{A}(D') \in \mathcal{O}]$. 
\end{definition}
Similarly to DP, a randomized mechanism for $f$ can be obtained by adding noise that depends on $f$'s \emph{local sensitivity} 
$S_L(f)$. The local sensitivity is the maximum difference of $f$ with respect to a given dataset $D$ and any of its adjacent datasets. Formally: 
%IDP reduces data distortion by calibrating the noise required to obtain the $(\epsilon,\delta)$-IDP guarantee to the actual dataset's $L_1$ local sensitivity rather than the global sensitivity, thus significantly improving the accuracy of the results of the randomized mechanism.


\begin{definition}[Local Sensitivity]~\label{def:local_sensitivity}
Given a dataset $D\in \mathcal{D}$, 
the local sensitivity of a function $f:\mathcal{D}\rightarrow \mathbb{R}^n$ is 
$S_L(f)=\max_{D'\in \mathcal{D} s.t. |D-D'|= 1}{||f(D)-f(D')||}$.
%$S_L(f)$ is the local sensitivity of a function $f:\mathcal{S}^d \rightarrow \mathbb{R}^n$ if for any adjacent dataset $\mathcal{S'}$ to $\mathcal{S}$ differing by one data point: $||f(\mathcal{S})-f(\mathcal{S}')||\leq S_L(f)$. 
\end{definition}

%Note that, except for the fact that IDP considers the actual dataset and its adjacent datasets while DP considers any pair of adjacent datasets, the structure of the two properties is similar. Thanks to this, 
\propi has been shown to have several important properties that hold for DP, including parallel and sequential composition~\citep{ref_88}.
Additionally, several DP noise mechanisms naturally extend to \propi by replacing the global sensitivity with the local sensitivity (e.g., the Laplace noise mechanism~\citep{ref_88}). 

%Several DP noise mechanisms have been shown to naturally extend to \propi by replacing the global sensitivity with the local sensitivity (e.g., the Laplace mechanism~\citep{ref_88}). 

%To illustrate, consider the popular Gaussian noise mechanism~\citep{ref_87}, where given the function $f$, the required added noise to obtain a randomized version of $f$ that is $(\epsilon, \delta)$-DP is normally distributed with mean 0 and standard deviation of $S(f) \cdot \sigma$. This is such that $\mathcal{A}_f(\mathcal{S}) = f(\mathcal{S}) + \mathcal{N}(0, S(f)^2 \cdot \sigma^2)$, and $\sigma = \sqrt{2 \cdot \ln(1.25/\delta)} \cdot \frac{1}{\epsilon}$. On the other hand, to obtain $(\epsilon, \delta)$-IDP, the randomized mechanism is $\mathcal{A}_f(\mathcal{S}) = f(\mathcal{S}) + \mathcal{N}(0, S_L(f)^2 \cdot \sigma^2)$ with a similar $\sigma$ to DP.


 % for any given input with high probability
%, adhering to the $(\varepsilon, \delta)$-differential privacy constraints. 
%Conversely, $\mathcal{A}$ is not DP if the probabilities that the networks return a different classification violate~\Cref{dp}.

%The main method for implementing DP involves introducing randomness into the learning process, typically through noise addition. 
%This can be done either by adding noise directly to the training data~\citep{ref_39} or, more sophisticatedly, through techniques like Differentially Private Stochastic Gradient Descent (DP-SGD), where noise is added to the gradients during training backpropagation~\citep{ref_22}. 

%We focus on a Differential Privacy (DP) over neural network classifiers with black-box access, i.e., the attacker has access to the inputs and the outputs (labels) of the classifier. 
%Implementing DP involves balancing trade-offs between privacy protection and model performance; higher noise levels for increased privacy can reduce model accuracy, while lower noise might compromise privacy. 




\begin{comment}
 \begin{figure*}[t]
    \centering
  \includegraphics[width=1\linewidth, trim=0 320 35 0, clip,page=3]{images/figures.pdf}
    \caption{Example. }
    \label{fig::pre}
\end{figure*}

\subsection{Example} \anan{Remove no need for ccs} In this example, we consider a neural network classifier designed with two inputs, two ReLU neurons, and two outputs. 
Our focus is on a dataset $\mathcal{S}$, comprising four inputs: (0,0), (0,1), (1,0), and (1,1), with corresponding labels (0,0,1,1). 
This network is trained using a standard Stochastic Gradient Descent (SGD) algorithm over 10 epochs $\mathcal{A}$ . 
Figure 1(a) describes this classifier. In contrast, Figure 1(b) describes a second classifier with an identical architecture, trained using the same algorithm  $\mathcal{A}$  and number of steps. 
However, the only difference is in the training dataset $\mathcal{S}'$, which excludes the input (0,0) from $\mathcal{S}$. 
Figure 1(b) describes this second classifier. 
We evaluate both classifiers with two inputs: (0.5,0) and (0,0.5). 
Both classifiers assign the input (0.5,0) to the same class, 0, indicating no differential privacy leakage concerning the input (0,0). 
However, the input (0,0.5) is classified differently by the two classifiers, suggesting differential privacy violation.

\end{comment} 