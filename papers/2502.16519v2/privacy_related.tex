\section{Related Work}
\label{sec:related_work}
%In this section, we discuss the closest related work to ours.

\begin{comment}
\paragraph{Privacy in neural networks}
Several works propose privacy attacks in neural networks, shown to be effective even in settings where only labels are accessible~\citep{ref_12,ref_13,ref_14}. %,ref_15}. 
Other works propose approaches to reduce privacy leakage. 
Some works suggest networks with specialized architectures designed for privacy~\cite{ref_16,ref_17}. 
Others integrate regularization terms during training to limit the privacy leakage~\cite{ref_18, ref_19, ref_20}. 
%However, these approaches do not have formal privacy guarantees. 
Others provide probabilistic guarantees by relying on federated distributed training~\cite{ref_21,ref_34,ref_57} or training differentially private networks~\cite{ref_22,ref_36,ref_37,ref_58,ref_59,ref_60,ref_61,ref_62}.  
A recent work introduces the property local differential classification privacy and provides for it probabilistic guarantees~\cite{ref_8}. 
%Unlike these works, \tool provides a deterministic guarantee over classifier privacy.

\end{comment}


\paragraph{Privacy of neural networks}
%Several works propose privacy attacks in neural networks, even when access is limited to labels-only~\citep{ref_12,ref_13,ref_14}. %,ref_15}. 
Several works propose approaches to reduce privacy leakage of neural networks. 
Some propose specialized architectures to maintain privacy~\cite{ref_16,ref_17}. 
Others integrate regularization during training to limit leakage~\cite{ref_18, ref_19}. 
Others rely on federated distributed learning~\cite{ref_21,ref_34,ref_57}. 
Another approach is to train differentially private (DP) networks~\cite{ref_22,ref_36,ref_37,ref_58,ref_59,ref_60}. 
Many works considered variants of DP to ensure privacy. For example, in individualized privacy assignment, different privacy budgets can be allocated to different users~\cite{ref_94,ref_95}. 
In local differential privacy, a data owner adds randomization to their data before it leaves their devices~\cite{ref_60,ref_97}. Renyi differential privacy relaxes differential privacy based on the Renyi divergence~\cite{ref_98,ref_99}. Local differential classification privacy proposes a new privacy property inspired by local robustness~\cite{ref_8}. Homomorphic-encryption approaches encrypt data before obtaining DP guarantees~\cite{ref_100,ref_101}. 
%In this work, we focus on individual differential privacy (\propi), which is another (incomparable) relaxation of DP. 
 
 %Some works focus on providing a utility-preserving formulation of the DP property by proposing the \propi property~\citep{ref_88,ref_90}. In our work, we employ the \propi property in the setting of label-only access to classifiers, aiming to provide classifiers with a small accuracy decrease while ensuring the dataset's individuals' privacy.


\begin{comment}
\paragraph{Multiple network analysis} 
\tool verifies the differential privacy of a network by analyzing a large set of closely related networks. 
Several works analyze numerous networks. Some works compute the absolute output differences of two networks~\cite{ref_1,ref_2}. Others study incremental verification, analyzing the differences of a network and its slightly modified version~\cite{ref_3}. Others propose proof transfer for similar networks for verifying local robustness~\cite{ref_4}. % of a given network compared to various approximation templates, with an emphasis on a proof transfer between networks. 
Some global robustness verifiers analyze the global robustness of networks by analyzing two copies of a network: one receiving an input and another one receiving its perturbed version~\cite{ref_5,ref_6,ref_7}. The work of~\citet{ref_8} proves local differential classification privacy by predicting a hyper-network that abstracts all networks that differ in a single data point (like our setting), with a high probability. %In contrast, our work computes a deterministic bound at which the network achieves differential privacy.
\end{comment}
\paragraph{Multiple network analysis} 
\tool analyzes a large set of similar networks. 
There is prior work on analysis of several networks. 
Some works compute the output differences of two networks~\cite{ref_1,ref_2}
or the output differences of a set of inputs~\cite{ref_102}. 
%Others analyze inference differences within the same network for different inputs~\cite{ref_102}. 
Others study incremental verification, analyzing the differences of a network and its slightly modified version~\cite{ref_3}. 
Others propose proof transfer for similar networks for verifying local robustness~\cite{ref_4}. 
Some global robustness verifiers compare the outputs of a network given an input and given its perturbed version~\cite{ref_6,ref_7,ref_5}. 
%Another work proves local differential classification privacy by predicting a hyper-network, with high probability~\cite{ref_8} .%which abstracts all networks differing by a single data point, with high probability. 
%Another work proves privacy guarantee by predicting a hyper-network, with high probability~\cite{ref_8} .%which abstracts all networks differing by a single data point, with high probability. 
\begin{comment}
\paragraph{Clustering in neural network verification} 
\tool expedites its analysis by clustering close networks and computing their hyper-networks. Several neural network verifiers employ clustering. Some verifiers accelerate local robustness analysis by grouping neurons into subgroups~\cite{ref_9, ref_10}. A different work divides a dataset's inputs into subgroups, computes a centroid point for each, and computes a global robustness bound for each centroid point~\cite{ref_11}.
\end{comment}


\paragraph{Clustering in neural network verification} 
\tool expedites its analysis by clustering close networks and computing their hyper-networks. %Several neural network verifiers use clustering. 
Some verifiers accelerate local robustness analysis by grouping neurons into subgroups~\cite{ref_9, ref_10}, while others compute centroids to partition inputs and establish global robustness bounds~\cite{ref_11}. 