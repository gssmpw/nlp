\appendix

\section{Proofs}\label{sec:proofs}
\ftb*
\begin{proof}
Generally, given a dataset $D$, a set of outputs $\mathcal{R}$ and a utility function $u:\mathcal{D} \times \mathcal{R} \rightarrow \mathbb{R}$,
the exponential mechanism $\mathcal{A}$ samples and returns $r\in \mathcal{R}$ with probability $\frac{\text{exp}(\varepsilon u(D,r)/(2\Delta u))}{\sum_{r'\in \mathcal{R}} \text{exp}(\varepsilon u(D,r')/(2\Delta u))}$.
In our setting, the set of outputs is the label set $\mathcal{R}=C$.
To prove \propi, we show that for any dataset $D'$ adjacent to $D$ (i.e., differing by one data point) and for every possible observed output $\mathcal{O} \subseteq \text{Range}(\mathcal{A})$, the following holds:
$e^{-\varepsilon} \cdot \Pr[\mathcal{A}(D') \in \mathcal{O}] \leq\Pr[\mathcal{A}(D) \in \mathcal{O}] \leq e^{\varepsilon} \cdot \Pr[\mathcal{A}(D') \in \mathcal{O}]$.
In our setting, the observed outputs $\mathcal{O} \subseteq \text{Range}(\mathcal{A})$ are sets of a single class $\{c\}\subseteq C$.
 Thus, we prove that for any dataset $D'$ adjacent to $D$ and for every $c \in C$, the following holds:  
 $$e^{-\varepsilon} \cdot \Pr[\mathcal{A}(D',u_{x,\mathcal{T},\widetilde{N}},C) =c] \leq\Pr[\mathcal{A}(D,u_{x,\mathcal{T},\widetilde{N}},C) =c] \leq e^{\varepsilon} \cdot \Pr[\mathcal{A}(D',u_{x,\mathcal{T},\widetilde{N}},C) =c]$$ 
We prove $\Pr[\mathcal{A}(D,u_{x,\mathcal{T},\widetilde{N}},C) =c] \leq e^{\varepsilon} \cdot \Pr[\mathcal{A}(D',u_{x,\mathcal{T},\widetilde{N}},C) =c]$ (the proof for the other inequality is similar).
The proof is very similar to the proof of~\cite[Theorem 3.10]{ref_87}.
To simplify notation, we write $u\triangleq u_{x,\mathcal{T},\widetilde{N}}$. 
We remind that $\mathcal{A}$ is invoked with $\Delta u=1$.

%The exponential mechanism $\mathcal{A}$ outputs a classification $c'\in C$ on the training set $D$ and any other adjacent training set $D'$.

\begin{multline*}
\frac{\Pr[\mathcal{A}(D,u, C) = c]}{\Pr[\mathcal{A}(D',u,C) = c]}=\frac{\left( \frac{\text{exp}(\varepsilon u(D,c)/2)}{\sum_{c'\in C} \text{exp}(\varepsilon u(D,c')/2)} \right) }{\left( \frac{\text{exp}(\varepsilon u(D',c)/2)}{\sum_{c'\in C} \text{exp}(\varepsilon u(D',c')/2)} \right) }=\\ 
\text{exp}\left(\frac{\varepsilon(u(D,c)-u(D',c))}{2}\right)\cdot\left(\frac{\sum_{c'\in C} \text{exp}(\varepsilon u(D',c')/2)}{\sum_{c'\in C} \text{exp}(\varepsilon u(D,c')/2)}\right)
\leq %\text{exp} \left(\frac{\varepsilon}{2}\right) \cdot \text{exp}\left(\frac{\varepsilon}{2}\right) = 
\text{exp}(\varepsilon)
\end{multline*}
%Similarly, $\frac{\Pr[\mathcal{A}(D,u,C) = c']}{\Pr[\mathcal{A}(D',u,C) = c']}\geq \text{exp}(-\varepsilon)$. 
The last transition follows from:
\begin{itemize}[nosep,nolistsep]
  \item The left term is equal to $\text{exp}\left(\frac{\varepsilon}{2}\right)$: This follows since by the definition of our utility function, for every $D'$ and $c$, we have $u(D,c)-u(D',c) \leq 1$.
%As mentioned, our exponential mechanism is invoked under the worst case value, i.e., $(u(D,c')-u(D',c') \leq 1$. 
\item The right term is equal to  $\text{exp}\left(\frac{\varepsilon}{2}\right)$: This follows by 
the quotient inequality which states that if we have $K$ values $\alpha_1,\ldots,\alpha_K$ and $K$ respective values $\beta_1,\ldots,\beta_K$ and for every $i\in[K]$, we have $\alpha_i / \beta_i \leq \gamma$, for some $\gamma$, then
$\frac{\alpha_1+\ldots + \alpha_K} {\beta_1+\ldots+\beta_K} \leq \gamma$. In our right term, we have for every $c'\in C$,  $\alpha_{c'}=\exp(\epsilon u(D',c')/2)$ and $\beta_{c'}= \exp(\epsilon u(D,c')/2)$. 
That is  $\frac{\alpha_{c'}}{\beta_{c'}} = \frac{\exp(\epsilon u(D',c')/2)}{\exp(\epsilon u(D,c')/2)}=
 \exp(\epsilon (u(D',c')- u(D,c'))/2)$. Like in the left term, since  $u(D',c')-u(D,c') \leq 1$
 we get $\alpha_{c'}/\beta_{c'} \leq \exp(\epsilon/2)\triangleq \gamma$, and the claim follows. 
\end{itemize}
 \end{proof}

\ftc*
\begin{proof}[Proof Sketch]
By the correctness of \boundtool, its computed \propa provides a sound overapproximation to the set of leaking inputs. Thus, if an input $x$ has classification confidence above the respective \propa, its query satisfies $0$-\propi and thus also $\varepsilon$-\propi. For such inputs, \tool does not add noise, and thereby does not lose accuracy.  
%Each input $x$ can define either a leaking input or a non-leaking input. %If the input is non-leaking, then the $\varepsilon$-\propi definition holds trivially with $\varepsilon = 0$, which is upper bounded by any $\varepsilon$. 
Otherwise, the exponential mechanism is employed and thereby $\varepsilon$-\propi is guaranteed by~\Cref{lemma::exponential_mechanism}. %to ensure $\varepsilon$-\propi. The exponential mechanism selects and outputs a class $c' \in [C]$ with probability proportional to $\exp(\varepsilon u(D,c') / 2)$, where $u$ is the utility function and $\Delta u = 1$. Since an input can be either leaking or non-leaking, the response mechanism \reptool ensures $\varepsilon$-\propi in both scenarios. Thus, for any query, the access provided is $\varepsilon$-\propi.
%Minimizing the accuracy decrease by adding noise only to inputs with confidence less than~\propa is obtained by the tightness of~\propa.  
\end{proof}



\begin{comment}
\section{\propa MILP encoding}\label{sec:appex_full_encoding}
In this section, we provide the MILP encoding of the \propa of a set $S$. 
%Overall, the MILP encoding of the \propa of a set $S$ is defined as follows.  
Given a classifier $N$, a hyper-network $N^\#_S$ defined by the network parameters intervals $w_{m,k,k'}^\#=[\underline{w^\#}_{m,k,k'},\overline{w^\#}_{m,k,k'}]$ and $b_{m,k}^\#=[\underline{b^\#}_{m,k},\overline{b^\#}_{m,k}]$, and a class $c$, our encoding is:

%Given a classifier $D$, a hyper-network $D^\#_S$ defined by the network parameters intervals $w_{m,k,k'}^\#=[\underline{w^\#}_{m,k,k'},\overline{w^\#}_{m,k,k'}]$ and $b_{m,k}^\#=[\underline{b^\#}_{m,k},\overline{b^\#}_{m,k}]$, and a class $c$, our encoding is: 

\begin{equation*}\label{main_problem}
  \begin{gathered}
     \max \beta_{c,S} \hspace{0.5cm}\text{subject to}\\
      x\in[0,1]^d;\hspace{0.27cm} \phi_{MD}; \hspace{0.27cm} \phi_{RiS} \hspace{0.27cm}\\
      \forall {c'\neq c}.\ z_{L,c}-z_{L,c'}\geq\beta_{c,S};\hspace{0.27cm}\;\;\;
      %\forall c''\neq c_t.\ z^p_{L,c_t}-z^p_{L,c''}>0;\\
      \forall {c' \neq c}.\  z^\#_{L,c}-z^\#_{L,c'} \leq M\cdot(1-\alpha_{c'});\;\;\sum_{c' \neq c} \alpha_{c'} \geq 1\\
     \forall k:\hspace{0.9cm}\;z_{0,k}=x_{k};\hspace{1.0cm}z^\#_{0,k}=x_{k}\\
     \forall m>0, \forall k:\hspace{0.5cm}\;\hat{z}_{m,k}=b_{m,k}+\sum_{k'=1}^{k_{m-1}}{w}_{m,k,k'}\cdot{z}_{m-1,k'} \hspace{0.5cm}\\
     \hat{z}^\#_{m,k}\geq \underline{b^\#}_{m,k} +\sum_{k'=1}^{k_{m-1}}\underline{w^\#}_{m,k,k'}\cdot{z^\#}_{m-1,k'};\hspace{0.5cm}
     \hat{z}^\#_{m,k}\leq \overline{b^\#}_{m,k}+\sum_{k'=1}^{k_{m-1}}\overline{w^\#}_{m,k,k'}\cdot{z^\#}_{m-1,k'}\\
      {z}_{m,k}\geq0; \hspace{0.5cm} {z}_{m,k}\geq \hat{z}_{m,k}; \hspace{0.5cm} {z}_{m,k} \leq u_{m,k}\cdot a_{m,k}; \hspace{0.5cm}
      {z}_{m,k} \leq \hat{z}_{m,k}-l_{m,k}(1-a_{m,k})\\
     {z}^\#_{m,k}\geq0;\hspace{0.5cm} {z}^\#_{m,k}\geq \hat{z}^\#_{m,k};\hspace{0.5cm}
    {z}^\#_{m,k} \leq u^\#_{m,k}\cdot a^\#_{m,k};\hspace{0.5cm}
    {z}^\#_{m,k} \leq \hat{z}^\#_{m,k}-l^\#_{m,k}(1-a^\#_{m,k})\\
    \end{gathered}
\end{equation*}
We note that we express the maximum function (in the constraint $z^\#_{L,c} - \max_{c'\neq c} z^\#_{L,c'} \leq 0$) using the Big M method~\cite{ref_73,ref_75}. 
\end{comment}
\begin{comment}
\section{Visual illustration of the \propa}\label{sec:appex_visual_illustration}
\paragraph{Illustration of the \propa}
We next illustrate how the \propa enables \tool to repair a network to be DP with a minor accuracy decrease.
We consider the 50x2 network for Twitter, and let \tool compute its \propa for each class separately ($\beta_0$ and $\beta_1$).
 \Cref{fig::eval1} plots the classification confidence of each input in the test set, where inputs labeled as $0$ are shown on the left  plot and inputs labeled as $1$ are shown on the right plot. The plots show in a green dashed line the \propa, in blue points the test points whose confidence is greater than the \propa and in red points the test points whose confidence is smaller or equal to the \propa. The plots 
 show that the confidence of the vast majority of points in the test set is over the \propa. Recall that for these points, \tool does not add noise, and thus it obtains an overall a minor accuracy decrease.
%In this figure, we show the minimal differentially private bounds of the 50x2 network trained on the Twitter dataset (represented by the dashed-green lines). 
%We compute the classification sensitivity for each input in the test set. 
%The input samples whose sensitivity exceeds the bounds (represented by blue points) do not leak privacy, and therefore, we do not add noise to them. 
%Conversely, the input samples with sensitivity below the bounds (represented by red points) are considered to be privacy-leaking, and thus, we add noise to these inputs. This added noise consequently impacts the overall test accuracy of the network classifier. 
 \begin{figure*}[t]
    \centering
  \includegraphics[width=1\linewidth, trim=0 387 0 0, clip,page=5]{images/figures.pdf}
    \caption{Illustration of the \propa and the confidence of test samples with respect to it for the Twitter 50x2 network. }
    \label{fig::eval1}
\end{figure*}
\end{comment} 
\newpage
\section{Evaluation: Dataset Description}\label{sec:appeval}
In this section, we describe our evaluated datasets. % and the ablation study.


\paragraph{Datasets}
We evaluate \tool over four datasets:% studied by previous DP-SGD papers\Dana{correct? if yes, add citations}:
 \begin{itemize}[nosep,nolistsep]
    \item Cryptojacking~\cite{ref_43} (Crypto): %Websites engaging in cryptojacking are harmful webpages that abuse users' computational resources for cryptocurrency mining. 
    %The dataset, comprising 2000 malicious websites and 2000 benign websites, and is divided into a 70\% training set and a 30\% test set. 
    %Consequently, the dataset includes 2800 samples for training and 1200 samples for testing. Each sample comprises 7 input features (e.g., websocket and messageloop), and one output label of malicious/benign.  
    Cryptojacking refers to websites that exploit user resources for cryptocurrency mining. This dataset includes 2,000 malicious websites and 2,000 benign websites, where 2,800 are used for training and 1,200 for testing. Each website has seven features (e.g., websocket) and is labeled as malicious or benign.
    \item Twitter Spam Accounts~\cite{ref_44} (Twitter): %Twitter spam accounts are profiles that distribute unsolicited, irrelevant, or inappropriate messages, often automatically, to a large number of users on the platform. 
    %The dataset, comprising 20,000 spam accounts, and 20,000 benign twitter users accounts. The dataset is divied into 36,000 samples for training and 4000 samples for testing. Each sample comprises 15 input features (e.g., number of followers and number of tweets) and one output label of spam/bengin.  
 Twitter spam accounts refers to accounts that distribute unsolicited messages to users. This dataset includes 20,000 spam accounts and 20,000 benign accounts, split into 36,000 accounts for training and 4,000 for testing. Each data point has 15 features (e.g., followers and tweets) and is labeled as spam or benign.
    \item Adult Census~\cite{ref_45} (Adult): %The Adult Census dataset is designed to analyze the demographic characteristics of adults to predict whether their income exceeds a certain threshold. The dataset comprises 48,842 samples, with 32,561 samples allocated for training and 16,281 samples for testing. Each sample includes 14 input features (e.g., age, education level, occupation, and hours per week worked) and one output label indicating whether the individual's annual income is above or below \$50,000. 
    %\item Bank Marketing~\cite{ref_46}.
 This dataset is used for predicting whether an adult's annual income is over \$50,000. It includes 48,842 data points, split into 32,561 for training and 16,281 for testing. Each data point has 14 features (e.g., age, education, occupation, and working hours) and is labeled as yes or no. %, labels for annual incomes above or below \$50,000.
    \item Default of Credit Card Clients~\cite{ref_47} (Credit): %This dataset concerns individuals' credit card payment defaults in Taiwan, consisting of data from 30,000 clients. It includes information on payment history, amount of bill statement, and previous payment details over six months, along with demographic factors like age, education, and marital status. The dataset is split into 24,000 samples for training and 6,000 samples for testing. Each sample features 23 input variables, such as the amount of given credit, gender, education, marital status, age, history of past payment, and the bill and payment amounts over the last six months, along with one binary output variable indicating default/no default.
The Taiwan Credit Card Default dataset is used for predicting whether the default payment will be paid. It 
 includes 30,000 client records, split into 24,000 for training and 6,000 for testing.
 A data point has 23 features (e.g., bill amounts, age, education, marital status) and is labeled as yes or no.
  \end{itemize} 
 
