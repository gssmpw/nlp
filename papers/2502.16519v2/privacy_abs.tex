
\begin{abstract}
Neural networks are susceptible to privacy attacks that can extract private information of the training set. To cope, several training algorithms guarantee differential privacy (DP) by adding noise to their computation. 
However, DP requires to add noise 
considering \emph{every possible} training set. %Often, network designers care about a \emph{given} training set, in which case the DP requirement leads to a higher noise than necessary, resulting in a significant decrease in the network's accuracy. 
This leads to a significant decrease in the network's accuracy.
%We focus on a more relaxed privacy guarantee, called 
Individual DP (\propi) restricts DP to a \emph{given} training set. %by adding noise to the network's predicted labels independently for each input.
%Satisfying \propi enables us to add noise to the network's predicted labels independently for each input. 
We observe that some inputs deterministically satisfy \propi \emph{without any noise}. By identifying them, we can provide \propi label-only access to the network with a minor decrease to its accuracy. %while protecting the privacy of individuals in the given training set. 
However, identifying the inputs that satisfy \propi without any noise is highly challenging. 
Our key idea is to compute the \emph{\propi deterministic bound} (\propa), which overapproximates the set of inputs that do not satisfy \propi, and add noise only to their predicted labels. 
To compute the tightest \propa, which enables to guard the label-only access with minimal accuracy decrease, we propose \tool, which leverages several formal verification techniques.
%We introduce \tool, a system that leverages formal verification to compute the \propa and accordingly define \propi label-only access to the network. \tool's verification relies on several ideas.
First, it encodes the problem as a mixed-integer linear program, defined over a network and over every network trained identically but without a unique data point. 
Second, 
it abstracts a set of networks using a \emph{hyper-network}. 
Third, it eliminates the overapproximation error via a novel branch-and-bound technique. 
Fourth, it bounds the differences of matching neurons in the network and the hyper-network, encodes them as linear constraints to prune the search space, and employs linear relaxation if they are small.  
We evaluate \tool on fully-connected and convolutional networks for four datasets and compare the results to existing DP training algorithms, which in particular provide \propi guarantees.  
We show that \tool can provide classifiers with a perfect individuals' privacy guarantee ($0$-\propi) -- which is infeasible for DP training algorithms -- with an accuracy decrease of 1.4\%. For more relaxed $\varepsilon$-\propi guarantees, \tool has an accuracy decrease of 1.2\%.  
In contrast, existing DP training algorithms that obtain $\varepsilon$-DP guarantees, and in particular $\varepsilon$-\propi guarantees, reduce the accuracy by 12.7\%. 
\end{abstract} 


