\section{Conclusion}
\label{sec:conclusions_and_discussion}
We present \tool, a system that creates \propi label-only access for a neural network classifier with a minor accuracy decrease.   
%making neural network classifiers differentially private (DP) against an adversary with a label-only access. % for neural network classifiers with minimal decrease on accuracy, under the assumption of adversary black-box access. 
\tool computes the \propa, which overapproximates the set of inputs that violate \propi %bounding all inputs where classifiers, %trained on the complete dataset and its variants missing a single sample, differ in classification. 
%and repairs by adding noise 
and adds noise only to the labels of these inputs. %optimizing the privacy-accuracy trade-off. 
To compute the \propa, \tool 
relies on several techniques: constraint solving, hyper-networks abstracting a large set of networks, and a novel branch-and-bound technique. %and on a MILP-based verifier that computes the \propa of the classifier and a hyper-network, abstracting a set of networks. %each trained on the same training set except for a unique data point.
%encodes the problem as MILP over the classifier and every variant trained on the full dataset except for a single data point. To scale, \tool abstracts a set of networks using a hyper-network. To remove the precision loss, it employs 
%a novel branch-and-bound. %To reduce the number of bounds, it abstracts close networks, which are identified by clustering.
%(2) Hyper-Networks for simultaneous bound computation across clusters, 
To further scale, it prunes the search space by
computing matching dependencies %bounding the differences of matching neurons in the network and the hyper-network, and when the differences are very small, it encodes neurons in the hyper-network using 
and employing linear relaxation. %to reduce the MILP's complexity. %to decrease the number of Boolean variables needed, albeit adding a slight overapproximation. 
Our experimental evaluation shows that our verification analysis enables \tool to provide a $0$-\propi guarantee with an accuracy decrease of 1.4\%. For more relaxed \propi guarantees, \tool can reduce the accuracy decrease to 1.2\%, whereas existing DP approaches lead to an accuracy decrease of 12.7\% to provide the same \propi guarantees. %a privacy protection that is not amenable by the baseline methods, and within a given privacy loss budget, our approach provides classifiers with x\% higher accuracy.  
\section*{Acknowledgements} We thank the anonymous reviewers for their feedback. This research was supported by the Israel Science Foundation (grant No. 2605/20). 

\section*{Data-Availability Statement}
Our code is available at \url{https://github.com/ananmkabaha/LUCID.git}. 
