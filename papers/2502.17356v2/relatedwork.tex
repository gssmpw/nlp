\section{Related Works}
\textbf{Random variation:} Even after controlling for critical hyperparameters such as learning rate, model performance remains sensitive to stochastic aspects of the training process---specifically, random initialization and the order of training examples. Prior studies have documented consistent performance differences across various stress test sets~\citep{d2022underspecification, naik2018stress} and observed that these differences persist throughout training, not just at the final checkpoint~\citep{zhou2020curse}. \citet{dodge2020fine} compared the impacts of weight initialization and data ordering, concluding that both contribute equally to variations in out-of-sample performance. 
Additionally, \citet{sellammultiberts} demonstrated that different BERT models independently pre-training  from scratch can differ in downstream task performance and social biases. Existing work has also found model runs can cluster in out-of-distribution behavior \citep{juneja_linear_2023} and in training dynamics \citep{qin2024itree,hu_latent_2023}, hinting at multimodal variation. Our work connects known these random clustering effects to the phenomenon of emergence at scale.

\textbf{Length generalization and compositionality with transformers:} Several studies have highlighted the challenge of length generalization in transformers~\citep{anil2022exploring, deletangneural, gontier2020measuring, hupkes2020compositionality, schwarzschild2021can, zhang2022unveiling}. Various remedies have been proposed to address length generalization failures. Some methods focus on alternative positional encodings~\citep{shaw2018self, presstrain, su2024roformer, kazemnejad2024impact, jelassirepeat}. Others modify the dataset format, either adding scratchpad or Chain-of-Thought formats~\citep{anil2022exploring} or incorporating padding and index hints for specific arithmetic tasks~\citep{jelassi2023length, zhoualgorithms}. Regarding random variation, \citet{zhoualgorithms} and \citet{zhou2024transformers} provide evidence of variability in length generalization across random seeds, which we further investigate and analyze across a range of model scales.

\textbf{Emergent abilities of LLMs:} In large language models, emergent abilities are behaviors that arise unexpectedly as models are scaled up in size or trained on larger datasets~\citep{hestness2017deeplearningscalingpredictable,rosenfeld2019constructivepredictiongeneralizationerror,brown2020language,kaplan2020scaling}. These abilities are characterized by unpredictable and abrupt performance improvements on specific benchmarks at certain scales~\citep{wei2022emergent, ganguli2022predictability, srivastava2023beyond}. Understanding the conditions and mechanisms underlying emergence is a key area of research. Recent studies suggest that emergent abilities may stem from the choice of evaluation metrics rather than fundamental changes in model behavior with increased scale~\citep{schaeffer2024emergent}. Nonetheless, some breakthrough capabilities remain emergent. One direction to better understand the mechanisms underlying emergence is through studying language models trained on algorithmic tasks exhibiting similar behavior; for instance, \citet{gopalaniabrupt} show that BERT models learn the low-rank matrix completion problem with a sudden drop in the loss before interpreting components of the model before and after this transition.  \citet{snell2024predicting} found that some scales exhibit earlier emergence if finetuned explicitly on an emergent task, suggesting that smaller models may have the capacity for that task but are limited by its scarcity in the training corpus.  In a similar vein, we show that emergent capabilities can arise from multimodal random variation using synthetic length generalization tasks as a case study.

\textbf{Depth versus Width Scaling:} Despite scaling laws offering a smooth extrapolation for model performance, downstream performance can vary depending on architecture shape and not just model size~\citep{tayscale}. For compositional tasks, deeper models often generalize better up to a certain point, but for a fixed compute budget, it may be more advantageous to train a shallower, wider model~\citep{petty2024impact}. Various works have proposed explanations for the role of width versus depth in scaling behavior; for instance, \citet{edelman2024pareto} show that increasing network width offers more `parallel queries' over randomized subnetworks which learn sparse features more efficiently.  \citet{levine2020limits} use a border rank argument to establish a width-dependent depth threshold, beyond which additional depth yields diminishing returns. In this work, we specifically investigate how independently scaling width and depth influences the random variation distribution in compositional tasks.