% motivation1：1、注意力计算量太大，对长序列不友好，计算少量的token；2、如何选择少量的token，节省计算量的方式；3、围绕方法的优化
% motivation2：一类方法遵循上述原则设计，但是缺点是（infllm将token分为block，缺少flexibility），我们的做法是自适应选择方式
Large language models (LLMs) have demonstrated remarkable capabilities across a variety of natural language processing (NLP) tasks.
One of the emerging trends in this area is the increasing emphasis on extending the context length of LLMs to handle tasks like contextual learning and retrieval-augmented generation \citep{dong2022survey,gao2023retrieval}. 

%RoPE不需要过多描述，challenges也要简洁一些。
We focus on decoder-only Language Models (LLMs) with Rotary Position Embedding (RoPE) extrapolated to longer sequence tasks during inference \citep{su2024roformer}. 
Currently, there are two main challenges with the application of these models on long sequences.
First, When the inference length exceeds the training length, the positional encodings encounter values beyond those during training, referred to as Out-of-Distribution (OOD) issues.
Second, the quadratic computational complexity of the attention mechanism significantly reduces the efficiency of long sequence inference, known as efficiency limitations.

% 需要部分放到related work里面
Techniques adopting scaling on positional embeddings are instrumental in addressing the OOD issues \citep{bloc97ntk, peng2023yarn}.
To further enhance inference efficiency, several methods have been developed focusing on selective attention, leveraging the inherent sparsity of attention matrices\citep{zhang2024h2o, Jiang:2024}. 
Early methods like H2O, ScissorHands and FastGen  \citep{zhang2024h2o,liu2024scissorhands,ge2023model} perform token selection only in the decoding stage, facing high latency and enormous memory demand during prefilling.
Another line of work relies on permanently evicting certain tokens to manage memory usage and reduce computation \citep{xiao2023efficient, han2023lm, li2024snapkv}. While effective in some scenarios, these token eviction strategies can result in significant information loss. 
Recent innovations, such as chunk-wise selection methodologies like InfLLM, LongHeads and Landmark Attention \citep{xiao2024infllm, lu2024longheads, mohtashami2023landmark}, have aimed to extend the context length by managing long contexts through chunking input prompts into fixed-size blocks. 
While these methods reduce computational complexity, the selection of tokens from a few high-priority chunks for attention computation lacks flexibility in selection.
In contrast, token-level selection methods provide more flexibility by selectively attending to tokens at a finer granularity, but they are usually computationally expensive or require further training and modification on the architecture \citep{wu2022memorizing, bertsch2024unlimiformer}. 

We propose ESA, an efficient token-level selective attention algorithm. Our method achieves extrapolation without incremental training of parameters of LLMs. Specifically, our method consists of two steps: efficient selection and attention computation. 
In the first step, we introduce a query-aware token-level selection mechanism that adaptively identifies the most crucial tokens.
To reduce the computational cost, we project the query and key vectors in the attention heads into low-dimensional spaces when measuring the importance of individual tokens. A learnable approach is adopted to derive the projection matrices.
We reduce the computational complexity of selecting tokens by utilizing compressed queries and keys.
Additionally, we found that directly selecting a few high-priority tokens can lead to a decrease for some tasks. We propose proximity influence, where the surrounding tokens are incorporated when calculating the importance score of a particular token.
In the step of attention computation, we utilize the complete keys and values of the selected tokens, rather than using all tokens that precede the query. Our approach reduces the complexity of traditional attention computation from quadratic to linear. The trade-off is the introduction of computational overhead for token selection, which is significantly less than the complexity of traditional attention computation.

To demonstrate the effectiveness of our approach, we conduct extensive evaluations on LongBench \citep{bai2023longbench},
$\infty$BENCH \citep{zhang2024bench}, NeedleBench \citep{li2024needlebench}, and Counting-Stars \citep{song2024counting} with Mistral-7B-Instruct-v0.2 (\texttt{Mistral}) \citep{jiang2023mistral} and Llama-3-8B-Instruct (\texttt{Llama}) \citep{llama3modelcard}.

Our contributions are summarized as follows. 1. We propose a new token-level selective attention method that enables an extension without incremental training of parameters of LLMs. We introduce proximity influence to enhance the semantic continuity of selected tokens, mitigating the performance degradation associated with directly selecting tokens of high priority in certain tasks. 2. We utilize the reduced-dimensionality queries and keys to select the tokens used for computing attention, which effectively reduces the complexity. We enhance the inference efficiency by computing attention for the selected tokens rather than the full attention. 3. We propose strategies for memory consumption and computational complexity. We introduce a cache for storing compressed keys, the size of which is negligible compared to the original KV cache. It eliminates the need to repeatedly compress the keys of the preceding tokens, thereby reducing computational load.
% \begin{enumerate}
%   % 不需要具体的 *4 1/32
%   \item We propose a new token-level selective attention method that enables an extension without incremental training of parameters of LLMs. We introduce proximity influence to enhance the semantic continuity of selected tokens, mitigating the performance degradation associated with directly selecting tokens of high priority in certain tasks.
%   \item We utilize the reduced-dimensionality queries and keys to select the tokens used for computing attention, which effectively reduces the complexity. We enhance the inference efficiency by computing attention for the selected tokens rather than the full attention.
%   \item We propose strategies for memory consumption and computational complexity. We introduce a cache for storing compressed keys, the size of which is negligible compared to the original KV cache. It eliminates the need to repeatedly compress the keys of the preceding tokens, thereby reducing computational load.
% \end{enumerate}