[
  {
    "index": 0,
    "papers": [
      {
        "key": "su2024roformer",
        "author": "Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng",
        "title": "Roformer: Enhanced transformer with rotary position embedding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2023extending",
        "author": "Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong",
        "title": "Extending context window of large language models via positional interpolation"
      },
      {
        "key": "kaiokendev",
        "author": "kaiokendev",
        "title": "Things i\u2019m learning while training superhot."
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bloc97ntk",
        "author": "bloc97",
        "title": "NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"
      },
      {
        "key": "Rozire2023CodeLO",
        "author": "Baptiste Rozi{\\`e}re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Tan and Yossi Adi and Jingyu Liu and Tal Remez and J{\\'e}r{\\'e}my Rapin and Artyom Kozhevnikov and I. Evtimov and Joanna Bitton and Manish P Bhatt and Cristian Cant{\\'o}n Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre D'efossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "key": "Liu2023ScalingLO",
        "author": "Xiaoran Liu and Hang Yan and Shuo Zhang and Chen An and Xipeng Qiu and Dahua Lin",
        "title": "Scaling Laws of RoPE-based Extrapolation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "peng2023yarn",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "Yarn: Efficient context window extension of large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2023clex",
        "author": "Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong",
        "title": "Clex: Continuous length extrapolation for large language models"
      },
      {
        "key": "Ding2024LongRoPEEL",
        "author": "Yiran Ding and Li Lyna Zhang and Chengruidong Zhang and Yuanyuan Xu and Ning Shang and Jiahang Xu and Fan Yang and Mao Yang",
        "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "JianlinSu",
        "author": "Jianlin Su",
        "title": "Rectified rotary position embeddings."
      },
      {
        "key": "Jin2024LLMML",
        "author": "Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-yuan Chang and Huiyuan Chen and Xia Hu",
        "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"
      },
      {
        "key": "An2024TrainingFreeLS",
        "author": "Chen An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong",
        "title": "Training-Free Long-Context Scaling of Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2023clex",
        "author": "Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong",
        "title": "Clex: Continuous length extrapolation for large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "JianlinSu",
        "author": "Jianlin Su",
        "title": "Rectified rotary position embeddings."
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Beltagy2020LongformerTL",
        "author": "Iz Beltagy and Matthew E. Peters and Arman Cohan",
        "title": "Longformer: The Long-Document Transformer"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Zaheer2020BigBT",
        "author": "Manzil Zaheer and Guru Guruganesh and Kumar Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Onta{\\~n}{\\'o}n and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed",
        "title": "Big Bird: Transformers for Longer Sequences"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "han2023lm",
        "author": "Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong",
        "title": "Lm-infinite: Simple on-the-fly length generalization for large language models"
      },
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2024h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      },
      {
        "key": "liu2024scissorhands",
        "author": "Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali",
        "title": "Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time"
      },
      {
        "key": "ge2023model",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      },
      {
        "key": "han2023lm",
        "author": "Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong",
        "title": "Lm-infinite: Simple on-the-fly length generalization for large language models"
      },
      {
        "key": "li2024snapkv",
        "author": "Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming",
        "title": "Snapkv: Llm knows what you are looking for before generation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xiao2024infllm",
        "author": "Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong",
        "title": "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lu2024longheads",
        "author": "Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing",
        "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "bertsch2024unlimiformer",
        "author": "Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew",
        "title": "Unlimiformer: Long-range transformers with unlimited length input"
      },
      {
        "key": "ahrabian2024adaptation",
        "author": "Ahrabian, Kian and Benhaim, Alon and Patra, Barun and Pujara, Jay and Singhal, Saksham and Song, Xia",
        "title": "On the Adaptation of Unlimiformer for Decoder-Only Transformers"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wu2022memorizing",
        "author": "Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian",
        "title": "Memorizing transformers"
      },
      {
        "key": "bertsch2024unlimiformer",
        "author": "Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew",
        "title": "Unlimiformer: Long-range transformers with unlimited length input"
      }
    ]
  }
]