% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amssymb}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{algorithm} % 提供 algorithm 环境
\usepackage{algpseudocode} % 提供算法伪代码的命令
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{float}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Unshackling Context Length: An Efficient Selective Attention Approach through Query-Key Compression}
\author{
	Haoyu Wang\textsuperscript{\rm 1}$^*$, Tong Teng \textsuperscript{\rm 1}\thanks{Equal Contribution.}, Tianyu Guo\textsuperscript{\rm 1}, An Xiao\textsuperscript{\rm 1}, Duyu Tang\textsuperscript{\rm 2},   \\
	\textbf{Hanting Chen\textsuperscript{\rm 1}, Yunhe Wang\textsuperscript{\rm 1}\thanks{Corresponding Author.}} \\
	\textsuperscript{\rm 1}Huawei Noah’s Ark Lab ~~~\textsuperscript{\rm 2}Huawei CBG  \\
	\texttt{\small \{wanghaoyu50, tengtong1, tianyu.guo, an.xiao, tangduyu, chenhanting, yunhe.wang\}@huawei.com}
}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

% Handling long-context sequences efficiently remains a significant challenge in large language models (LLMs). Existing methods for token selection in sequence extrapolation either employ a permanent eviction strategy or select tokens in chunks which may lead to the loss of critical information. We propose Efficient Selective Attention (ESA), a novel approach that achieves extension by efficiently selecting the most critical tokens at the token level to compute attention. ESA reduces the computational complexity of token selection by compressing the query and key vectors into lower-dimensional representations. We evaluate our method on a variety of long sequence benchmarks with maximum lengths reaching up to 256k using open-source LLMs with context lengths of 8k and 32k. ESA outperforms other selective attention methods, particularly in challenging tasks that require the retrieval of multiple pieces of information. Compared to full-attention extrapolation methods, ESA achieves comparable performance across a variety of tasks, even with superior results in certain tasks.
Handling long-context sequences efficiently remains a significant challenge in large language models (LLMs). Existing methods for token selection in sequence extrapolation either employ a permanent eviction strategy or select tokens by chunk, which may lead to the loss of critical information. We propose Efficient Selective Attention (ESA), a novel approach that extends context length by efficiently selecting the most critical tokens at the token level to compute attention. ESA reduces the computational complexity of token selection by compressing query and key vectors into lower-dimensional representations. We evaluate ESA on long sequence benchmarks with maximum lengths up to 256k using open-source LLMs with context lengths of 8k and 32k. ESA outperforms other selective attention methods, especially in tasks requiring the retrieval of multiple pieces of information, achieving comparable performance to full-attention extrapolation methods across various tasks, with superior results in certain tasks. 
% Notably, ESA retains original length capability for short sequence retrieval tasks in the 4-8k range, even outperforming full-attention approaches.

% Furthermore, experimental results on multiple benchmarks demonstrate that our approach achieves competitive accuracy, making it highly scalable for real-world applications. 
\end{abstract}

\section{Introduction}
\input{intro_v2}

\section{Related Work}
\paragraph{Position Extrapolation.}
%Position extrapolation methods address longer contexts by scaling position embeddings (PE).  Early work primarily focused on achieving length extrapolation by modifying relative PEs during the pre-training phase. 
Following the introduction of RoPE \citep{su2024roformer}, great efforts have been imposed to extend the context length by modifying the position embeddings (PE).
Position interpolation  \cite{chen2023extending, kaiokendev} extends the context length by interpolating positional indices within the constraints of pre-training. 
The NTK-aware method \citep{bloc97ntk,Rozire2023CodeLO,Liu2023ScalingLO} introduces a nonlinear interpolation strategy by increasing the base parameter of RoPE. YaRN \citep{peng2023yarn} proposes a method for frequency interpolation of RoPE dimensions, where higher frequency dimensions are extrapolated, and lower frequency dimensions are interpolated. Further improvements \citep{chen2023clex,Ding2024LongRoPEEL} exploit the dynamics in position extrapolation.
Another group of work redesigns the relative position matrix to overcome the OOD issue \citep{JianlinSu,Jin2024LLMML,An2024TrainingFreeLS}. 
%CLEX generalizes PE scaling to model the transition of frequency basis continuously \citep{chen2023clex}.
These methods extend the context length but still compute the full attention matrix for inference thus fail to reduce the computational cost. To achieve better performance, some require fine-tuning with a certain amount of long-context data.
%ReRoPE proposes to maintain the position encodings within the window unchanged and truncate the position encodings that extend beyond the window \citep{JianlinSu}. 

\paragraph{Selective Attention.}
Selective attention mechanisms aim to mitigate the computational cost of processing long sequences by selecting only the most relevant tokens for attention computation. Approaches like Longformer \citep{Beltagy2020LongformerTL} and BigBird \citep{Zaheer2020BigBT} use fixed or adaptive sparse patterns, while \citealp{han2023lm, xiao2023efficient} introduce $\Lambda$-shaped windows that evict middle tokens. Although these methods lower costs, they often compromise global context understanding due to restricted attention over all tokens.
Some methods aim at compressing the KV cache, usually perform token selection only in the decoding stage \citep{zhang2024h2o,liu2024scissorhands,ge2023model} or permanently evicting certain tokens \citep{xiao2023efficient, han2023lm, li2024snapkv}. While effective, they may lose critical contextual information.
As for chunk-based methods, \citealp{xiao2024infllm} uses an efficient contextual memory mechanism to select the most relevant chunks for computing attention, and \citealp{lu2024longheads} selects the most relevant chunks for each head separately considering the variance among different heads.
Unlimiformer and its adaptation \citep{bertsch2024unlimiformer,ahrabian2024adaptation} segment input during the pre-filling stage, using external memory blocks, but remain computationally expensive and require additional training or architecture modifications.
In contrast, our method performs efficient token-level selection in both prefilling and decoding stages, without discarding tokens permanently.

%These methods reduce memory usage and computational complexity by limiting the size of the attention computation window, and mitigate the decline in accuracy by selectively focusing on the most critical tokens.
% % they are usually computationally expensive or require further training and modification on the architecture \citep{wu2022memorizing, bertsch2024unlimiformer}. 
% but may lose valuable contextual information. Compared to these methods, our proposed approach does not require chunking of the context, thus ensuring the flexibility of token selection and the integrity of contextual information.
\section{Method}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{\textwidth}
    \centering
    % 图的宽高为16.76、4.47；页面宽高为33.88、19.05 left bottom right top
    \includegraphics[trim=0cm 14.5cm 17.1cm 0cm, clip, width=\textwidth]{ESAa.pdf}
    \vspace{-20pt}
    \caption{}
    \end{subfigure}

    % \label{fig:wide-image-a}
    % 图的宽高为9.17、4.41；页面宽高为33.88、19.05 left bottom right top
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[trim=0cm 14.6cm 24.7cm 0cm, clip, width=\textwidth]{ESAb.pdf}
    \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    % 图的宽高为8.78、4.3；页面宽高为33.88、19.05 left bottom right top
    \includegraphics[trim=0cm 14.6cm 24.7cm 0cm, clip, width=\textwidth]{ESAc.pdf}
    \caption{}
    \end{subfigure}
    \caption {(a) In long-context scenarios, the number of middle tokens occupies the majority, while the lengths of the other three parts of tokens are fixed. The importance scores between current tokens and middle tokens are utilized to select the top-k middle tokens. The selected tokens replace the middle tokens for computing attention. (b) The queries from current tokens and keys from middle tokens are compressed into smaller tensors through a linear layer respectively. The dot product of the compressed queries and keys serves as the importance scores. (c) The priority of a middle token being selected is determined by the maximum importance score among itself and several surrounding tokens.}
    \label{fig:illust}
\end{figure*}



% 为了高效长序列推理，提出xxx方式，对每个部分说明，我们的方法好处，motivation,然后放prefilling的chunk by chunk或者在3.1前面也可以
Conventionally, each token attends to all preceding tokens to compute the attention output during LLM inference, leading to significant computational overhead when processing long-context sequences. 
We introduce ESA, which effectively identifies a subset of the most crucial tokens for attention computation, thereby improving the efficiency of the model.
%The computational and memory costs of LLMs become prohibitive with the growing of context length. To mitigate this issue, 
%we exploit the sparsity nature in attention matrices \citep{zhang2024h2o, Jiang:2024, lu2024longheads, singhania2024loki}, and
%To mitigate this issue, our method selects a constant number of the most relevant preceding tokens with respect to the current tokens in each step for 
%, so that to reduce the computational complexity associated with inferencing on long-context sequences. (too long)

\subsection{Token Selection}
Following previous work \citep{xiao2023efficient, han2023lm, xiao2024infllm} on extending the context window of LLMs, the preceding tokens ($P$) are segmented into three parts, as depicted in Figure \ref{fig:illust}: initial tokens ($I$), middle tokens ($M$), and local tokens ($L$) with respective lengths denoted as $l_I$, $l_M$, and $l_L$. With a massive number of preceding tokens, $l_M$ is considerably larger than $l_I$ and $l_L$.
The initial tokens and local tokens with fixed length will always be selected due to their significance, as highlighted by \citet{xiao2023efficient} and \citet{han2023lm}. We then apply ESA to select the top-$k$ critical tokens $M_k$ from the middle tokens $M$.
Specifically, LLM inference consists of two stages: prefilling and decoding. In the prefilling stage, all the input tokens are encoded, and the keys as well as values are computed and cached. We adopt the chunked-prefill method \citep{agrawal2023sarathi}.
%, where tokens in the same chunk shares the selected tokens during the encoding of the input.
Compared to the original prefilling method, chunked-prefill approach effectively reduces memory usage. In the decoding stage, the model generates tokens one by one autoregressively. 
% 下面的删掉也没什么影响
Denote the \emph{current tokens} (i.e., the current input chunk being processed or the most recently decoded token) as $C$, the attention output computed in each step between the current tokens and the selected tokens is defined as:
\begin{equation} \label{attention_eq}
	\mathbf{O} = \text{Attn}(\mathbf{Q}_C,\mathbf{K}_{[I,M_k,L,C]}, \mathbf{V}_{[I,M_k,L,C]})
\end{equation}
%\begin{equation}
%	\mathbf{O} = \text{Attn}(\mathbf{Q}_C,[\mathbf{K}_I;\mathbf{K}_{M_k};\mathbf{K}_L,\mathbf{K}_C], [\mathbf{V}_I;\mathbf{V}_{M_k};\mathbf{V}_L;\mathbf{V}_C])
%\end{equation}
where, $\mathbf{Q}_C$ represents the queries of the current tokens, $\mathbf{K}_{[I,M_k,L,C]}$ and $\mathbf{V}_{[I,M_k,L,C]}$ respectively denote the concatenated keys and values of the selected and current tokens.


%During the prefilling and decoding stages, we compute the attention scores between the current tokens and the preceding tokens (including initial tokens, middle tokens, local tokens, and current tokens themselves). Without considering the positional encoding, the output of attention is:
%\begin{equation}
%O_{i} \triangleq Attn(q_{i, [C]}, k_{i,[I,M,L,C]}, v_{i,[I,M,L,C]}) \label{initial_attention}
%\end{equation} 
%where, $q_{i, [C]}$ represents $i$-th head of the queries of the current tokens, $k_{i, [I,M,L,C]}$ and $v_{i, [I,M,L,C]}$ denote $i$-th head of the concatenated keys and values (kvs) of the initial, middle, local, and current tokens, respectively.
%
%The quantity of $M$ constitutes a significant proportion in the context of long-sequence inputs. In this section, we focus on selecting top-k critical tokens ($M_k$) from key-value (KV) cache of $M$ for attention computation. We substitute $M$ in Equation~\ref{initial_attention} with $M_k$:
%\begin{equation} 
%O_{i} = Attn(q_{i, [C]}, k_{i,[I,M_k,L,C]}, v_{i,[I,M_k,L,C]}) \label{topk_attention_score}
%\end{equation}
\paragraph{Importance Score.} 
Successfully identifying the most crucial tokens requires a function to precisely measure the importance of each token in $M$ w.r.t $C$. Let $m \in M$ be a specific token, we denote the importance score of $m$ as $F_s(m; C)$.
Given a predefined number $k < =l_M$, $k$ tokens with the highest scores will be selected, as formalized bellow:
\begin{equation}
	M_k = \arg \text{topK}(M; F_s, C, k)
\end{equation}
For an individual token $c \in C$, the degree to which it attends to a preceding token $m$ can be determined as following:
\begin{equation}
	f_s(m; c) = \sum_{h=1}^{H}\mathbf{q}_{h,c}\mathbf{k}_{h,m}^{T}
	\label{dot-p}
\end{equation}
where, $\mathbf{q}_{h,c},\mathbf{k}_{h, m} \in \mathbb{R}^d$ denote the query of $c$ and the key of $m$, respectively, for the $h$-th head. All of the $H$ $d$-dimensional attention heads are incorporated, and the selected tokens are shared across all the heads in a certain layer. This score can be used directly in the decoding stage for token selection as $C$ consists of only one token (i.e., \( l_C = 1 \)). While at the prefilling stage, $l_C$ is the chunk size,
and every token in $C$ contributes to the score $F_s(m; C)$.
To derive a unified form of importance score, we first regularize the score w.r.t each $c$ and then take the maximum value across all tokens in $C$.
Eventually, the importance score is formulated as
\begin{equation}
	F_s(m; C) = \max_{c \in C} \left(f_s(m;c) - \max_{m^{\prime} \in M} f_s(m^{\prime};c)\right)
        \label{F_s_1}
\end{equation}
1. The expression \(f_s(m;c) - \max_{m^{\prime} \in M} f_s(m^{\prime};c)\) indicates that each token in \(C\) is constrained relative to the maximum score in \(M\), preventing any individual token in \(C\) from exerting a dominating influence on the selection process.
2. In the prefilling stage, our goal is to ensure that high-scoring tokens in \(M\) are not overlooked. Therefore, we select the highest score of each token in \(C\) to represent the score in \(M\) by applying \(\max_{c \in C}\).
% 1. prefilling阶段 \left(f_s(m;c) - \max_{m^{\prime} \in M} f_s(m^{\prime};c)\right) 表示每个C中的token相对于M中的最大分数是被限制的，避免C中的个别token对选择有统治的影响。
% 2. 我们选择tokens的目标是不漏掉分数高的tokens in M，因此选择C中每个token最高的分数作为M中的分数 by 应用\max_{c \in C}.

%Based on the preceding discussion, We define $q_{i, [C]}$ and $k_{i, [M]}$ using the following equations, respectively:
%\begin{align}  
%q_{i, [C]} &\triangleq \{q_{i, j} \mid j \in \mathbb{N}, l_{I,M,L} \le j < l_{I,M,L,C} \} \label{q_i_C_define} \\  
%k_{i, [M]} &\triangleq \{k_{i, j} \mid j \in \mathbb{N}, l_I \le j < l_{I,M}\}   \label{k_i_M_define}
%\end{align}
%where, $q_{i, j},k_{i, j} \in \mathbb{R}^d$ denote the query and key, respectively, for the $i$-th head of the $j$-th token. we denote $l_{I,M,L} \triangleq l_I + l_M + l_L$, $l_{I,M} \triangleq l_I + l_M$, $l_{I,M,L,C} \triangleq l_I + l_M + l_L + l_C$. $d$ represents the dimensionality of a single head. The importance score of the $j$-th token in $M$ being selected is presented in Equation~\ref{j_token-score}:
%\begin{align} 
%% s_max有512个取值，和q的长度一致
%\label{j_token-score}
% s_j &= \max\bigg( \Big\{ \big(\sum_{i=1}^{H}{q_{i,x} \cdot k_{i,j}^{T}}\big)-s_{x}^{max} \mid  x\in  \mathbb{N}, \nonumber\\ 
%     & l_{I,M,L} \le x < l_{I,M,L,C} \Big\}\bigg)  \\
%\label{s_max_define}
% s_{x}^{max} &\triangleq \max(\{\sum_{i=1}^{H}{q_{i,x} \cdot k_{i,y}^{T}} \mid y\in  \mathbb{N}, \nonumber\\ &l_I \le y < l_{I,M}\}) 
%\end{align}


%Equation~\ref{j_token-score} and~\ref{s_max_define} have the following two implications:
% 小维度
\paragraph{Efficient Selection through Query-Key Compression.} 
The aforementioned scoring method of employing dot product across a considerable number tokens is computational expensive. To achieve efficient selection, we perform dimensionality reduction on keys and queries.
The right-hand side of Equation~\ref{dot-p} is equivalent to concatenating $H$ heads followed by performing dot product. That is, $ f_s(m;c)=\mathbf{q}_{c}\mathbf{k}_{m}^{T}$ where $\mathbf{q}_{c} = [\mathbf{q}_{1,c};\mathbf{q}_{2,c};... ;\mathbf{q}_{H,c}]$, $\mathbf{k}_{m} = [\mathbf{k}_{1,m};\mathbf{k}_{2,m};... ;\mathbf{k}_{H,m}]$, and
$\mathbf{q}_{c},\mathbf{k}_{ m} \in \mathbb{R}^{d_{H}}$, $d_{H}=H \times d$.
%explain why not reduce dim on each head ...
Denote the dimensionality reduction on queries and keys as follows:
\begin{equation} \label{reduce_dim_formula}
\begin{aligned}
\mathbf{q}_{c}^{\prime} &\triangleq f_{\theta_q}(\mathbf{q}_{c}),\\
\mathbf{k}_{m}^{\prime} &\triangleq f_{\theta_k}(\mathbf{k}_{m}), 
\end{aligned}
\end{equation}
where, $\mathbf{q}_{c}^{\prime}, \mathbf{k}_{m}^{\prime} \in \mathbb{R}^{d^{\prime}}$, $d^{\prime} < d_{H}$. 
The dimension-reduced representation $\mathbf{k}_{m}^{\prime}$ will be cached and reused during the subsequent computation steps.
With the lower-dimensional surrogates of queries and keys, the importance score is approximated with
\begin{equation} \label{reduce_dim_dot}
    f_s(m; c) \approx \mathbf{q}^\prime_{c}\mathbf{k}^{\prime T}_{m}
\end{equation}
The computational cost is therefore reduced compared with using the full-dimensional queries and keys. 
To maintain accuracy, the lower-dimensional representations should retrain the token order as faithfully as possible.
To this end, we perform a one-time offline procedure to learn $f_{\theta_q}$ and $f_{\theta_k}$ in each layer by minimizing the discrepancy between the importance scores before and after dimensionality reduction, formally: 
\begin{equation} \label{train_theta}
\begin{aligned}
\min_{\theta_q, \theta_k}  \sum_{c \in C, m \in M} \left\lVert \mathbf{q}_{c} \mathbf{k}_{m}^{T} - f_{\theta_q}(\mathbf{q}_{c}) f_{\theta_k}(\mathbf{k}_{m})^{T} \right\rVert_{2}^{2}
\end{aligned}
\end{equation}
%[感觉需要一个algorithm][这里没有改]
We model $f_{\theta_q}$ and $f_{\theta_k}$ jointly, where each is a linear layer that projects a high-dimensional input to a low-dimensional output.
In preparation of the training data for the neural networks, we first perform token selection with full-dimensional queries and keys with a calibration dataset. All the queries and keys calculated during the process are saved.
Subsequently, we use the saved queries and keys to train \(f_{\theta_q}\) and \(f_{\theta_k}\) for each layer.
The learnt $f_{\theta_q}$ and $f_{\theta_k}$ will be utilized in our ESA to compress queries and keys with dimensionality reduction.
Since the low-dimensional keys are cached and reused, the additional computational load introduced by Equation~\ref{reduce_dim_formula} is marginal compared to the reduction achieved by Equation~\ref{reduce_dim_dot}. We conducted a quantitative analysis of efficiency in Section \ref{Complexity_Analysis}.
% todo Proximity Influence
\paragraph{Proximity Influence.}
Considering proximity influence, if a token in $M$ has a high importance score, it affects the scores of the surrounding tokens. We propose adaptive selection which is achieved by updating the importance score of a token based on its neighbors. Specifically, the score of the $j$-th token, where $j\in [l_I,l_I+l_M-1]$, is determined as the maximum score within a window of size $ \epsilon $ surrounding it. The importance score of the $j$-th token, computed using the low-dimensional query and key, is denoted by $ s_j $. The updated score is given by 
\begin{equation} \label{proximity_influence_eq}
    s_j^\prime = \max_{w=\max(j-\epsilon, l_I)}^{\min(j+\epsilon,l_I+l_M-1)}\{s_{w}\}
\end{equation}
where, $\epsilon$ represents the proximity influence distance, which is used to control the continuity of the selected tokens.
%$F_s(m;C)$ 
% 1.描述用一个单层的MLP来降维（采取calibration dataset），然后topk选择中间token再加上周围的token，具体选择token
% 连续语义信息
% infinite-topk 选择中间的几个；Unlimiformer：input分成了好多个段落，然后也是token-level选择，不够efficient
% 复用之前一些文章的结论，attention计算是很稀疏的，但是只能算出点乘以后才能看分数，我们提出一个cheap计算attention的function,只要排序，不要精度，具体实现用rank MLP，有个框图
% 2.不考虑head展开的32*128 --> 4096降维到128；prefill和decoding
% 3.连续语义：NLP语义信息不是单个token能表示完整的，需要保持连续性，邻距影响力下的adaptive，token-level的更小的block
% q*A*A^{T}*k

% \subsection{prefill和Decoding}

\subsection{Position Encoding}
We follow the extrapolated position encoding settings as described in \citep{su2023rerope, xiao2024infllm}.
The attention weights in Equation~\ref{attention_eq} can be divided into 
%long-distance attention and local attention.
% According to Equation~\ref{attention_eq}, we compute the attention weights of $C$ with respect to $I,M_k,L$, and itself. These weights can be divided into global weights and local weights.
(a) Long-distance attention of $C$ with respect to $I,M$: The positions of tokens in $C$ are all modified to a fixed position $w$, and the positions of tokens from $I,M$ are set to 0. The relative position will always be $w$;
and (b) Local attention of $C$ with respect to $L$ and itself: The query and key positions from $L,C$ are arranged in order of their relative positions (i.e., 0, 1, 2, ..., $l_L + l_C - 1$).
To normalize the attention weights, we fuse the two categories of attention based on their respective weights. 
The specific algorithm for computing attention is shown in Appendix~\ref{sec:pseudocode_attn}.
% Due to the distinct position encoding of tokens in $C$ within global attention and local attention, we compute the queries and transposed keys dot products separately for each. This is followed by a concatenation operation and softmax operation.
% 可以考虑增加一个flash attention实现的说明

\subsection{Complexity Analysis} \label{Complexity_Analysis}
% 短序列的情况下采取单卡或多卡加载，对于超长数据例如700k+采取off load，从CPU到GPU搬运，kv cache改为hidden states cache，通过增加线性的计算量，减小搬运量，减小topk时间
% 不讲搬运的问题
% 和下面的合一

\paragraph{Computational Complexity Analysis.}
%In the prefilling and decoding stages, we have $l_C = N$ and $l_C = 1$, respectively. In the prefilling stage, the input sequence is divided into blocks of length $N$, with each block selecting keys of length $k$ from $M$. In the decoding stage, the current individual token is treated as a single block. Considering the long sequence condition where $M$ occupies the majority of the tokens, the compression of complexity is as follows:

When inferring over a long-context sequence of total length $S$, using full-dimensional queries and keys for computing the importance score in Equation~\ref{dot-p} incurs a time complexity of $O(S^2d_H)$. By utilizing low-dimensional queries and keys, the computation is reduced to $O(S^2d^\prime)$. The additional computation for dimensionality reduction is $O(Sd_Hd^\prime+Sd^\prime)$, which scales linearly with context length and thus marginal to the quadratic savings.

In each step, ESA computes the attention matrix for a constant number of selected tokens. Considering the long sequence scenario where $M$ occupies the majority of the tokens, this approach significantly reduces computational costs, with only a minor overhead for token selection. Compared to vanilla full attention, the complexity of computing one step of attention can be reduced by ratio \( r \) in Equation~\ref{reduction_ratio_complexity}. The derivation can be found in Appendix~\ref{sec:Complexity_Analysis_Proof}.
\begin{equation} \label{reduction_ratio_complexity}
  r=\frac{2 d^{\prime} +1}{4d_{H} + 3H}
\end{equation} 
% The derivation of Equation~\ref{reduction_ratio_complexity} can be found in Appendix~\ref{sec:Complexity_Analysis_Proof}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Since we cache hidden states rather than kvs, we need to compute the following formula omitting considering GQA and the matrix operations for partitioning heads:
% \begin{align}
% q \cdot k^T &= (h_s \cdot W_q) \cdot (h_s \cdot W_k)^T \label{calculate_qk_analysis1} \\
% q \cdot k^T &= (h_s \cdot W_q \cdot W_k^T) \cdot h_s^T \label{calculate_qk_analysis2}
% \end{align}
% where, $h_s$ is hidden states, $W_q$ and $W_k$ represent the weights for computing the query and key, respectively. We compute attention using Equation~\ref{calculate_qk_analysis1} and~\ref{calculate_qk_analysis2} during the prefilling and decoding stages, respectively, which allows for a slight reduction in computational complexity. 
% The overall reduction ratio of complexity is as follows:
% \begin{equation} \label{reduction_ratio_complexity}
%   r=\frac{2 d^{\prime} +1}{4d_{H} + 3H}
% \end{equation} 
% The derivation process of Equation~\ref{reduction_ratio_complexity} can be found in Appendix~\ref{sec:Complexity_Analysis_Proof}

\paragraph{Cache Analysis.} 
We introduce an additional cache alongside the traditional KV cache, which stores the dimension-reduced keys from $M$. By incorporating a model that applies GQA \citep{ainslie2023gqa}, a widely used technique for reducing the KV cache, we analyze the impact of our approach on memory usage. Assuming the number of heads is denoted as \( H_G \) in GQA, the total dimensions of the kvs are given by \( d_{G} = H_G \times d \).
Given that $l_M \gg l_{I},l_{L},l_{C}$ for long sequences, we focus our analysis on the memory usage related to $M$. 
The cache size increased by the dimension-reduced keys is $\frac{d^{\prime}}{2 d_{G}}$ of the traditional KV cache.

\section{Experiments} \label{section_Experiments}
\subsection{Experimental Setup} 
\paragraph{Baselines and Benchmarks.} ESA can be applied to all decoder-only LLMs that utilize RoPE as their position encoding. We evaluate ESA using Mistral-7B-Instruct-v0.2 and Llama-3-8B-Instruct, with context lengths of 32k and 8k, respectively. We select the following three selective attention methods as our baselines: (a) InfLLM, (b) LM-Infinite (Infinite), (c) StreamingLLM (Stream). Additionally, we also choose two methods with position extrapolation: (a) NTK-aware (NTK), (b) YaRN. We conduct extensive evaluations on LongBench, $\infty$BENCH, NeedleBench, and Counting-Stars.

%Mistral-7B-Instruct-v0.2 (\texttt{Mistral}) \citep{jiang2023mistral} and Llama-3-8B-Instruct (\texttt{Llama}) 
\paragraph{Calibration Dataset.}
We employ a small subset of Books3 data from Pile \citep{gao2020pile} as our calibration dataset to train the networks $f_{\theta_q}$ and $f_{\theta_k}$. There are 50k tokens in total and therefore 50k concatenated query-key pairs for training the networks in each layer.
%The sample length for generating the calibration dataset is 30k, and we ultimately collected approximately 50k samples. 
The learning rate and global batch size is $0.0005$ and 128, respectively. We trained the dimensionality reduction networks for 10 epochs.

\paragraph{Parameters.}
% The model parameters for Mistral and Llama are as follows: 
The number of attention heads ($H$) is 32.
We compress the original size of query and key from \( d_H = 4096 \) to \( d' = 128 \).
% The complexity for computing the importance scores is reduced to 3.13\% of the original. 
Since the number of GQA heads is 8, the additional size required for the reduced-dimensionality keys is 6.25\% of the original KV cache.
Compared to computing full attention, the computational complexity is reduced to up to 1.56\% in each step according to Equation~\ref{reduction_ratio_complexity}.
ESA and three other baselines with selective attention select the same number of tokens. The length of initial tokens (\( l_I \)) is 128. InfLLM and ESA both choose the lengths of middle tokens and local tokens to be 2048 and 4096, respectively.
%by our method is kept consistent with those of InfLLM, Infinite, and Stream.
\subsection{Results on LongBench}
LongBench includes six different categories of tasks, with an average context length range from less than 10k to around 32k. We adjust the scaling factors of NTK and YaRN to accommodate the benchmark. The context length for Mistral is 32k, which does not necessitate any modification to the model's scaling factor. Consequently, we omit the NTK and YaRN for Mistral in this section. 
The results of the 16 English tasks are presented in Table~\ref{tab:res_longbench}. We draw the following conclusions: 
(a) Our method achieves improvement over the best baselines of selective attention (including Infinite, Stream, InfLLM) for both Llama and Mistral across a variety of tasks. Particularly, our method outperforms other methods of selective attention on the PassageRetrieval-en significantly, demonstrating the benefit of token-level selection.
(b) Our method is comparable to the baselines that compute full attention (including Origin, NTK, YaRN). The gap between our method and the best among these approaches is within 1 percentage point.
% \input{res-table-longbench-t}
\input{res-table-longbench}
\subsection{Results on $\infty$BENCH} \label{results_infinitebench}
We select 6 tasks from $\infty$BENCH with an average length up to around 200k, encompassing open-form question answering (QA), code, retrieval tasks, and other domains. We set the scaling factor for NTK and YaRN to accommodate contexts of up to 200k. The results of the tasks are presented in Table~\ref{tab:res_infinitebench}. 
Firstly, our method slightly outperforms the scores of InfLLM.
The performance of our method exhibits minimal differences in retrieval tasks compared to InfLLM.
This may be due to the fact that retrieval tasks only require the model to focus on a single relevant piece of information.
InfLLM retrieves the most relevant chunks from the context, which is sufficient for solving long-text tasks that require attention to only a small amount of local information.
Our method, on the other hand, opts for retrieval at a finer granularity, resulting in performance that is close to that of InfLLM on such tasks. In other tasks, our method outperforms other selective attention methods. 
Secondly, our method outperforms NTK and YaRN, especially on Llama. This superiority may arise from the fact that methods with position embedding scaling tend to suffer a significant decline when extrapolated to excessively long contexts, such as a 8-fold extension. It demonstrates that our approach can extrapolate to significantly longer contexts, even up to a $\times 25$ extension for Llama.

%%%%%%%%%%%%%%%% infiniteBench 2 col
\input{res-table-infbench-2col.tex}


\subsection{Results on NeedleBench and Counting-Stars}
NeedleBench and Counting-Stars evaluate the ability of LLMs to retrieve multiple pieces of related information. The two benchmarks places higher demands on the model's capacity to handle long context. Sample examples from the benchmarks are provided in Appendix~\ref{sec:NeedleBench and Counting-Stars}. The context length for these two benchmarks ranges from 4k to 256k, assessing the model's capability to retrieve multiple pieces of information across varying lengths. We uniformly set the scaling factor for NTK and YaRN to accommodate contexts of up to 200k tokens. We follow \citep{li2024needlebench, song2024counting} to use the recall accuracy as a metric to evaluate the performance of the models.


% needle说明每条样本有3个针，且位置随机 
Our method exhibits great strength in extracting critical information distributed across the context.
The experimental results on Counting-Stars and NeedleBench are shown in Table~\ref{tab:res_count_stars} and ~\ref{tab:res_needle}, respectively. Details of the Counting-Stars are provided in Appendix~\ref{sec:Counting-Stars-Results}. 
Firstly, when multiple pieces of relevant information need to be retrieved, our method significantly outperforms Infinite, Stream, and InfLLM. 
This is attributed to our method's flexibility in selecting middle tokens at the token level. 
Secondly, the performance of ESA is comparable to that of NTK and YaRN. NTK and YaRN achieve promising performance by computing full attention when extrapolation is limited.
When extrapolated to tasks involving longer sequences, NTK and YaRN may experience performance degradation.
Lastly, within the original training lengths, ESA does not exhibit significant performance degradation, whereas the NTK and YaRN show a noticeable decline.
% 需要增加分析，对应的组合起来的图
%%%%%%%% Counting stars
\input{res-table-counting-star-2col}
%%%%%%%%%%%%%% multi needle
\input{res-table-needle}


\subsection{Ablation Study}

\paragraph{Effectiveness of the proximity influence distance \( \epsilon \).} The parameter \( \epsilon \) in Equation~\ref{proximity_influence_eq} controls the continuity of the selected tokens. As demonstrated in Table~\ref{tab:proximity_influence_distance_ablation}, we find this parameter to be crucial for the model, especially with regard to its retrieval capabilities. Furthermore, we observe that in Retrieve.KV, when \( \epsilon = 0,1 \), even when the model's predictions are incorrect, it is still able to retrieve parts of the correct values. For instance, the answer is "49c65968-6319-44fc-b286-feb249694b07", while the model's prediction is "49c65968-6319-44fc-\textcolor{red}{9021-cfa198896071}". 
Retrieve.KV and NeedleBench exhibit different optimal values for \( \epsilon \). 
We speculate that the underlying reason may be the difference in the number of positions where answers are to be retrieved.
In Retrieve.KV, there is typically only one segment that requires retrieval, and increasing \( \epsilon \) may enhance the completeness of the retrieved answer.
In contrast, NeedleBench involves the retrieval of answers from multiple positions. Increasing \( \epsilon \) might lead to an over-concentration of attention on a limited number of positions. 

\begin{table}[]
\centering
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{@{}l|cccc@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Task}} & \multicolumn{4}{c}{$\epsilon$}                \\
\multicolumn{1}{c|}{}                      & 0     & 1              & 3    & 5             \\ \midrule
Retrieve.KV                                & 66.6  & 82             & 91.6 & \textbf{95.6} \\
NeedleBench                                & 69.67 & \textbf{71.33} & 68   & 58.67         \\ \bottomrule
\end{tabular}%
}
\caption{The ablation study results of \( \epsilon \) on InfiniteBench's Retrieve.KV and NeedleBench with Mistral. NeedleBench in the table represents the average scores across lengths ranging from 4k to 200k, with the specific scores detailed in Appendix~\ref{appen_proxi_influ}.} 
  % 除了我们的方法，其他的在接近200k的时候不行
  \label{tab:proximity_influence_distance_ablation}
\end{table}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{graphicx}
% \begin{table}[]
%   \begin{tabular}{l|cccc}
%   \hline
%   \multicolumn{1}{c|}{\multirow{2}{*}{Task}} & \multicolumn{4}{c}{$\epsilon$} \\
%   \multicolumn{1}{c|}{}                      & 0      & 1     & 3     & 5     \\ \hline
%   Retrieve.KV                                & 66.6   & 82  & 91.6  & \textbf{95.6}  \\
%   NeedleBench                        & 69.67   & \textbf{71.33}  & 68  & 58.67  \\ \hline
%   \end{tabular}
%   \caption{The ablation study results of \( \epsilon \) on InfiniteBench's Retrieve.KV and NeedleBench with Mistral. NeedleBench in the table represents the average scores across lengths ranging from 4k to 200k, with the specific scores detailed in Appendix~\ref{appen_proxi_influ}.} 
%   % 除了我们的方法，其他的在接近200k的时候不行
%   \label{tab:proximity_influence_distance_ablation}
%   \end{table}
  
% 最好将longbench替换为needle

\paragraph{Uniform Token Selection for All Heads.}
Our method does not select tokens individually for each head but rather chooses tokens based on the average importance scores across all heads. This approach is beneficial for compressing the size of queries and keys, thereby enhancing inference efficiency. To verify whether there is a significant performance degradation, we design two experiments with Llama on LongBench as shown in Table~\ref{tab:uniform_individual_token_selection}. "Individual" refers to the importance score of each token being the maximum value among the scores of all heads, meaning that each head votes for the scores. This approach ensures that the selection process takes into account all heads. "Uniform" in Table~\ref{tab:uniform_individual_token_selection} denotes our method of selecting tokens without dimensionality reduction. The scores for each subtask of LongBench are depicted in Appendix~\ref{sec:Token_Selection_for_Heads}. We extrapolate Llama from its original 8k to an average length of 32k on LongBench, and the performance on various category tasks for both token selection methods is very close. 
% It demonstrates that our method of averaging the scores from different heads and then uniformly selecting tokens has a minimal impact on the model's performance.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\centering
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{@{}l|cc@{}}
\toprule
\multicolumn{1}{c|}{Task} & individual & uniform \\ \midrule
LongBench scores          & 44.7       & 44.8    \\ \bottomrule
\end{tabular}%
}
\caption{We employ Llama to validate different token selection strategies for heads. The LongBench scores represent the average scores across 16 subtasks in LongBench.} 
  % 除了我们的方法，其他的在接近200k的时候不行
  \label{tab:uniform_individual_token_selection}
\end{table}
%%%%%%%%%%%%%%
% \begin{table}[]
%   \begin{tabular}{l|cc}
%     \hline
%     \multicolumn{1}{c|}{Task} & individual & uniform \\ \hline
%     LongBench scores          & 44.7       & 44.8    \\ \hline
%     \end{tabular}
%   \caption{We employ Llama to validate different token selection strategies for heads. The LongBench scores represent the average scores across 16 subtasks in LongBench.} 
%   % 除了我们的方法，其他的在接近200k的时候不行
%   \label{tab:uniform_individual_token_selection}
%   \end{table}


\paragraph{Dimension Reduction of Queries and Keys.}
We calculate the importance scores of tokens using the reduced-dimensionality queries and keys. To evaluate the impact of dimensionality reduction, we analyse experiments on LongBench with Llama using the full-dimensional query and key, as well as their reduced-dimensionality counterparts. As demonstrated in Table~\ref{tab:uniform_individual_token_selection} and Table~\ref{tab:res_longbench}, their respective scores are 44.8 and 44.41. The difference between the two scores is only 0.39.
Furthermore, we select samples from Books3 in Pile and employ Mistral to validate the recall rate of the top-k retrieval subsequent to dimensionality reduction. 
The ground truth is determined using the top-k tokens derived from the full-dimensional query and key. A total of 2,000 tokens are selected for this analysis, spanning positions from 23,000 to 25,000. 
In parallel, we execute comparative experiments utilizing principle component analysis (PCA) for dimensionality reduction inspired by \citep{singhania2024loki}. 
The experimental results are depicted in Figure~\ref{fig:compare_pca_mlp}. It can be observed that our dimensionality reduction method achieves a recall rate of over 90\% for the majority of the model's layers, whereas PCA exhibits a recall rate below 90\% in approximately half of the layers. 
% It demonstrates that our method is capable of selecting the vast majority of the most important tokens.
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{pca_compared.pdf}
  \caption{Recall rates of each layer for selecting the top 2,000 tokens after dimensionality reduction.}
  \label{fig:compare_pca_mlp}
\end{figure}

% 周围token的选择+和PCA的对比+每个head单独选token+Infinite-topk

\section{Conclusions and Future work}
In this paper, we propose an efficient token-level selective attention method for extending the context length of LLMs without incremental training of LLMs parameters.
The insight is to select a constant number of the most important tokens at each step for computing attention at the token level, leveraging the sparsity of the attention matrix. 
When the input sequence is sufficiently long, we are able to reduce the computational complexity to up to nearly $1.56\%$ of the original by compressing the queries and keys into low-dimensional representations. 
Our empirical evaluations demonstrate that ESA can effectively handle sequences of lengths up to $4 \times$ and even $25 \times$ the training length for various types of long sequence tasks. 
%Future work can explore how to more accurately and efficiently select important tokens in extrapolatory tasks. 
Future work could explore more accurate and efficient methods for selecting important tokens in extrapolation tasks.
% Additionally, we will leverage the characteristics of RoPE to explore better position encoding for application in selective attention methods.
\section{Limitations}
Our method has the following limitations: 1. We apply the same compression ratio to the queries and keys across different layers; employing varying compression ratios for different layers may yield better performance. 2. There may exist more effective token selection methods and compression techniques that warrant further exploration. 3. A more suitable positional encoding for our approach may exist and requires further investigation.
% \clearpage
\bibliography{custom}

\appendix

% 1.没有尝试中文等其他语言，需要进一步验证多语言的可能性
% 2.不同层可能需要不同的压缩比例，而不是统一设置为固定压缩比例
% 3.在prefill的时候是分block的，虽然FLOPS有所降低，但是prefill开始不能充分利用显存，后续工程优化需要在prefill的时候batch size逐渐减小
% 4.目前还没有和flash attention、VLLM等加速推理方法结合，需要利用以后的加速推理工具进一步提升
% 5.可能存在更好的选择token的方式，无论效率还是准确度
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
% \input{appendix}
\input{appendix_kualan}
%\input{count_star}
\end{document}