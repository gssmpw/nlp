\section{Related Work}
\paragraph{Position Extrapolation.}
%Position extrapolation methods address longer contexts by scaling position embeddings (PE).  Early work primarily focused on achieving length extrapolation by modifying relative PEs during the pre-training phase. 
Following the introduction of RoPE \citep{su2024roformer}, great efforts have been imposed to extend the context length by modifying the position embeddings (PE).
Position interpolation  \cite{chen2023extending, kaiokendev} extends the context length by interpolating positional indices within the constraints of pre-training. 
The NTK-aware method \citep{bloc97ntk,Rozire2023CodeLO,Liu2023ScalingLO} introduces a nonlinear interpolation strategy by increasing the base parameter of RoPE. YaRN \citep{peng2023yarn} proposes a method for frequency interpolation of RoPE dimensions, where higher frequency dimensions are extrapolated, and lower frequency dimensions are interpolated. Further improvements \citep{chen2023clex,Ding2024LongRoPEEL} exploit the dynamics in position extrapolation.
Another group of work redesigns the relative position matrix to overcome the OOD issue \citep{JianlinSu,Jin2024LLMML,An2024TrainingFreeLS}. 
%CLEX generalizes PE scaling to model the transition of frequency basis continuously \citep{chen2023clex}.
These methods extend the context length but still compute the full attention matrix for inference thus fail to reduce the computational cost. To achieve better performance, some require fine-tuning with a certain amount of long-context data.
%ReRoPE proposes to maintain the position encodings within the window unchanged and truncate the position encodings that extend beyond the window \citep{JianlinSu}. 

\paragraph{Selective Attention.}
Selective attention mechanisms aim to mitigate the computational cost of processing long sequences by selecting only the most relevant tokens for attention computation. Approaches like Longformer \citep{Beltagy2020LongformerTL} and BigBird \citep{Zaheer2020BigBT} use fixed or adaptive sparse patterns, while \citealp{han2023lm, xiao2023efficient} introduce $\Lambda$-shaped windows that evict middle tokens. Although these methods lower costs, they often compromise global context understanding due to restricted attention over all tokens.
Some methods aim at compressing the KV cache, usually perform token selection only in the decoding stage \citep{zhang2024h2o,liu2024scissorhands,ge2023model} or permanently evicting certain tokens \citep{xiao2023efficient, han2023lm, li2024snapkv}. While effective, they may lose critical contextual information.
As for chunk-based methods, \citealp{xiao2024infllm} uses an efficient contextual memory mechanism to select the most relevant chunks for computing attention, and \citealp{lu2024longheads} selects the most relevant chunks for each head separately considering the variance among different heads.
Unlimiformer and its adaptation \citep{bertsch2024unlimiformer,ahrabian2024adaptation} segment input during the pre-filling stage, using external memory blocks, but remain computationally expensive and require additional training or architecture modifications.
In contrast, our method performs efficient token-level selection in both prefilling and decoding stages, without discarding tokens permanently.

%These methods reduce memory usage and computational complexity by limiting the size of the attention computation window, and mitigate the decline in accuracy by selectively focusing on the most critical tokens.
% % they are usually computationally expensive or require further training and modification on the architecture \citep{wu2022memorizing, bertsch2024unlimiformer}. 
% but may lose valuable contextual information. Compared to these methods, our proposed approach does not require chunking of the context, thus ensuring the flexibility of token selection and the integrity of contextual information.