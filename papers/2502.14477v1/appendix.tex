
\section{Complexity Analysis Derivation}
\label{sec:Complexity_Analysis_Proof}


Denote $l_{I,M,L,C} = l_{I} + l_{M} + l_{L} + l_{C}$, the full attention in each step is computed as
\begin{equation} \label{attention_eq_full}
	\mathbf{O} = \text{Attn}(\mathbf{Q}_C,\mathbf{K}_{[I,M,L,C]}, \mathbf{V}_{[I,M,L,C]})
\end{equation}
The complexity of computing full attention consists of the following parts: 
\begin{enumerate}
\item The dot product of queries and keys: 
\begin{equation} \label{attn_comple_dot}
  2 \cdot d_{H} \cdot l_{C} \cdot l_{I,M,L,C} 
\end{equation}
\item The softmax operation (including exponential calculation, summation, and normalization): 
\begin{equation} \label{attn_comple_softmax}
  3 \cdot H \cdot l_{C} \cdot l_{I,M,L,C}
\end{equation}
\item The dot product of attention weights and values: 
\begin{equation} \label{attn_comple_wei_val}
  2 \cdot d_{H} \cdot l_{C} \cdot l_{I,M,L,C}
\end{equation}
\end{enumerate}

The overall complexity is
\begin{equation} \label{attn_full}
  (4 \cdot d_{H} + 3 \cdot H) \cdot l_{C} \cdot l_{I,M,L,C} 
\end{equation}

The complexity of our method comprises the following components:
\begin{enumerate}
    \item Reduction of the dimensions of the query and key in Equation~(\ref{reduce_dim_formula}):
    % 这里分别对C的query和key降维，经过一个MLP
    \begin{equation}
      2 \cdot 2 \cdot l_C \cdot d_{H} \cdot d' + 2 \cdot l_C \cdot d'
    \end{equation}
    \item The complexity of token selection in Equation~(\ref{F_s_1}) and~(\ref{proximity_influence_eq}) is
    $2 \cdot l_M \cdot l_C \cdot d' + l_{M} \cdot l_C + h_{\text{max}}(2 \cdot l_C \cdot l_M + (1 + 2 \epsilon) \cdot l_M)$. The complexity of taking the maximum operation on multi-dimensional vectors is denoted as \( h_{\text{max}} \). Since (a) the complexity is lower than that of an equivalent number of Floating Point Operations (FLOPS)  for the same scale; and (b) $2 \cdot l_C \cdot l_M + (1 + 2 \epsilon) \cdot l_M < 2 \cdot l_M \cdot l_C \cdot d'$, we neglect the impact of the max operation. Therefore, the complexity of token selection is:
    \begin{equation}
      2 \cdot l_M \cdot l_C \cdot d' + l_{M} \cdot l_C
    \end{equation}
    % Following the same rationale, we can deduce the complexity associated with Equation~\ref{adaptive selection},~\ref{j_token-score-final} and~\ref{calculate_qk_analysis2}: 
    % \begin{equation}
    %   2 \cdot l_C \cdot d_{H} \cdot d_{H} + 2 \cdot H \cdot l_C \cdot d_{H} \cdot k + l_M \cdot l_C
    % \end{equation}
    \item Following the analogy of Equation~(\ref{attn_comple_dot}),~(\ref{attn_comple_softmax}) and~(\ref{attn_comple_wei_val}), after selecting $k$ tokens from $M$, the complexity of computing sparse attention is as follows:
    \begin{equation}
      (4 \cdot d_{H} + 3 \cdot H ) \cdot l_{C} \cdot (l_{I,L,C}+k)
    \end{equation}
    where, $l_{I,L,C} \triangleq l_{I}+l_{L}+l_{C}$.
\end{enumerate}
% 不需要考虑GQA
% 需要再推导一下
Given that $l_M >> l_{I},l_{L},l_{C},k,d_{H},d^{\prime}$, the overall reduction ratio of complexity is:
\begin{align} \label{reduction_ratio_complexity_infer}
% r = \frac{4 l_c d_{H} d^{\prime} + l_c d^{\prime} + 2 l_M l_C d^{\prime} + l_M l_C + 4 k d_{H}^{2} + (4 d_{H} + 3H) (l_I + k + l_L + l_C) l_C }{(4 d_{H} + 3 H)l_{M}l_{C}}
r = &\frac{l_I + k + l_L + l_C}{l_I + l_M + l_L + l_C} + \nonumber \\
&\frac{4 d_{H} d^{\prime} + 2 d^{\prime} + 2 d^{\prime} l_M + l_M}{(4 d_{H} + 3H) (l_I + l_M + l_L + l_C)} \nonumber \\
&\approx \frac{2 d^{\prime} +1}{4d_{H} + 3H}
\end{align}
% Adopting different attention computation methods during the prefilling and decoding stages can slightly reduce the computational complexity. however, this does not significantly alter the compression ratio for long sequence computations. Consequently, the compression ratio during the decoding stage remains consistent with \( r \) as presented in Equation~\ref{reduction_ratio_complexity_infer}.

\section{Model Loading}
\label{sec:load_model_appendix}
All of our experiments were conducted on a device equipped with 4 $\times$ A800 GPUs. The evaluated models were partitioned across the 4 GPUs by layer using the \texttt{accelerate} \citep{accelerate}. Our model loading approach is capable of supporting the execution of all the aforementioned experiments. 
Additionally, we support loading the model onto a single A800 GPU by offloading the original KV cache to the CPU, while the dimension-reduced keys are always kept on the GPU. 
During each attention computation, a small number of keys and values in KV cache are loaded onto the GPU. 
By employing this cache management approach, we are able to perform inference on long sequence tasks with lengths of 700k+ on a single A800 GPU.
For NTK and YaRN, we employ vLLM \citep{kwon2023efficient} for inference on a device equipped with 4 $\times$ A800 GPUs.

\section{NeedleBench and Counting-Stars}
\label{sec:NeedleBench and Counting-Stars}

\paragraph{NeedleBench}
A sample from NeedleBench is shown below, where xxxxxx represents noise:
\begin{quote}
  You are an intelligent AI assistant skilled in answering user questions.\\ 
  Please keep your answers concise and clear. Do not talk about irrelevant topics or repeat your answers.\\
  The document given to you by the user is May 2001 xxxxxxxxxxxxxxxxxxxxx \textbf{Hidden on Hell Island is the legendary Dream Bubble.}  xxxxxxxx\\
  \textbf{Hidden on Emerald Island is the legendary Ghost Pearl.}   xxxxxxxx \\
  \textbf{Hidden on Sand Island is the legendary Stardust Shard.}xxxxxx\\
  Now, the questions are: \textbf{What legendary item is hidden on Hell Island?What legendary item is hidden on Emerald Island?What legendary item is hidden on Sand Island?}Before answering, please consider what in the document is most relevant to this question. Please answer in the format of 'The legendary item hidden on the Hell Island is\underline{$\phantom{\rule{1cm}{0ex}}$}. The legendary item hidden on the Emerald Island is\underline{$\phantom{\rule{1cm}{0ex}}$}. The legendary item hidden on the Sand Island is\underline{$\phantom{\rule{1cm}{0ex}}$}
\end{quote}

\paragraph{Counting-Stars}
A sample from Counting-Stars is shown below, where \textellipsis{} indicates omitted content.
\begin{quote}
  xxxxxxxxx \\ 
  \textbf{The little penguin counted 15 $\star$} xxxxxxxxx \\
  \textbf{The little penguin counted 117 $\star$} xxxxxxxxx \\
  \textbf{The little penguin counted 42 $\star$} xxxxxxxxx \\
  ...... \\
  \textbf{The little penguin counted 29 $\star$}  \\ \\
  On this moonlit and misty night, the little penguin is looking up at the sky and concentrating on counting $\star$. Please help the little penguin collect the number of $\star$, for example: \textbf{\{"little\_penguin": [x, x, x,...]\}}. The summation is not required, and the numbers in [x, x, x,...] represent the counted number of $\star$ by the little penguin. Only output the results in JSON format without any explanation. 
  ```json \\
  \{"little\_penguin": [
\end{quote}




\section{Proximity Influence Distance: Specific Experiments} \label{appen_proxi_influ}
We validate the ablation study results of the proximity influence distance, with the detailed findings presented in Table~\ref{tab:proximity_influence_distance_details}. 
When \( \epsilon \) is set to 0 and 1, Mistral demonstrates superior performance across all subtasks ranging from 4k to 200k. 
As \( \epsilon \) is further increased to 3 and 5, the scores on the short-sequence subtasks (4k-16k) remain comparable to the previous results. 
However, the model's performance exhibits a significant decline on the long-sequence subtasks (48k-200k).
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\begin{tabular}{l|cccc}
\hline
\multirow{2}{*}{Context Length} & \multicolumn{4}{c}{$\epsilon$}                 \\
                                & 0              & 1              & 3              & 5     \\ \hline
4k                              & 100            & 100            & 100            & 100   \\
8k                              & 83.33          & 83.33          & 83.33          & 83.33 \\
16k                             & \textbf{73.33} & \textbf{73.33} & \textbf{73.33} & 70    \\
48k                             & 80             & \textbf{86.67} & 70          & 70 \\
80k                             & \textbf{70}    & 63.33          & 66.67          & 40    \\
112k                            & 60 & \textbf{70} & 63.33          & 40    \\
128k                            & 66.67          & \textbf{73.33} & 60             & 56.67 \\
144k                            & \textbf{70}    & 56.67             & 63.33             & 36.67 \\
176k                            & 46.67             & \textbf{60} & 53.33          & 50    \\
200k                            & \textbf{46.67} & \textbf{46.67}             & \textbf{46.67}          & 40 \\ \hline
Average                         & 69.67             & \textbf{71.33}    & 68          & 58.67 \\ \hline
\end{tabular}
  \caption{We validate the performance across all subtasks of NeedleBench with varying \( \epsilon \) using Mistral.}
  \label{tab:proximity_influence_distance_details}
  \end{table}

\section{Token Selection for Heads: Specific Experiments}
\label{sec:Token_Selection_for_Heads}
We employ Llama to investigate the impact of different token selection methods for heads. 
We extrapolate Llama's original context length of 8k to 32k and conduct experiments using LongBench, which has an average length of 32k.
The scores for each subtask are presented in Table~\ref{tab:token_selection_head_subtask}.
\begin{table}[]
  \begin{tabular}{l|cc}
  \hline
  \multicolumn{1}{c|}{Task} & individual     & uniform        \\ \hline
  NarrativaQA               & 24.52          & \textbf{25.1}  \\
  Qasper                    & \textbf{44.69} & 44.51          \\
  MultiFieldQA-en           & 49.18          & 49.18          \\
  MuSiQue                   & 25.55          & \textbf{27.58} \\
  HotpotQA                  & \textbf{49.57} & 49.26          \\
  2WikiMultihopQA           & \textbf{38.1}  & 37.44          \\
  GovReport                 & \textbf{31.06} & 30.99          \\
  QMSum                     & \textbf{22.91} & 22.75          \\
  MultiNews                 & 27.41          & \textbf{27.45} \\
  TREC                      & 73.5           & 73.5           \\
  TriviaQA                  & 91.19          & 91.19          \\
  SAMSum                    & \textbf{42.87} & 42.7           \\
  PassageRetrieval-en       & 86.5           & \textbf{87.5}  \\
  PassageCount              & 8.17           & 7.17           \\
  LCC                       & 58.32          & 58.32          \\
  RepoBench-P               & 41.7           & \textbf{42.6}  \\ \hline
  Average                   & 44.7           & \textbf{44.8}           \\ \hline
  \end{tabular}
  \caption{Llama's specific experiments on LongBench using different token selection methods for heads. "Individual" refers to the importance score of each token being the maximum value among the scores of all heads, meaning that each head votes for the scores.This approach ensures that the selection process takes into account all heads. "Uniform" in the table denotes our method of selecting tokens without dimensionality reduction.}
  \label{tab:token_selection_head_subtask}
  \end{table}

\section{Pseudocode for Computing Attention}
\label{sec:pseudocode_attn}
We support the computation of local and global attention using either Flash Attention \citep{dao2205fast} or PyTorch operators. The pseudocode for computing a step of attention with Flash Attention is shown in Algorithm~\ref{alg:Pseudocode for Attention Computation}. It is worth noting that we omit the exp-normalize trick in Step 12 of Algorithm~\ref{alg:Pseudocode for Attention Computation} to avoid numerical overflow. We employ the function \texttt{flash\_attn\_func} provided by Flash Attention, which returns the logarithm of the softmax normalization factor as its second result. In environments where Flash Attention is not supported, we can replace the function \texttt{flash\_attn\_func} with PyTorch operators.
\begin{algorithm*} 
\caption{Pseudocode for Attention Computation with Flash Attention}
\begin{algorithmic}[1]
\State \textbf{Input:}
\State $l\_q$: Queries from $C$ with position encoding $(l_L, l_L+1, l_L+2, \ldots, l_L + l_C - 1)$
\State $g\_q$: Queries from $C$ with position encoding $(w, w, w, \ldots, w)$
\State $l\_k$: Concatenated keys from $L$ and $C$ with position encoding $(0, 1, 2, \ldots, l_L + l_C - 1)$
\State $g\_k$: Selected keys from $M$ and $I$ with position encoding $(0, 0, 0, 0, \ldots)$
\State $l\_v$: Concatenated values from $L$ and $C$
\State $g\_v$: Selected values from $M$ and $I$
\State
\State \textbf{Procedure:}
\State $(l\_attn, l\_lse, \_) \leftarrow \text{flash\_attn\_func}(l\_q, l\_k, l\_v, \mathrm{causal}=\mathrm{True})$
\State $(g\_attn, g\_lse, \_) \leftarrow \text{flash\_attn\_func}(g\_q, g\_k, g\_v, \mathrm{causal}=\mathrm{False})$
\State $se \leftarrow \exp([l\_lse, g\_lse])$
\State $fac \leftarrow se / \sum se$
\State $attn \leftarrow [l\_attn, g\_attn] \cdot fac$
\State
\State \textbf{Output:}
\State $attn$
\end{algorithmic}
\label{alg:Pseudocode for Attention Computation}
\end{algorithm*}


\section{Experimental Results on Counting-Stars}
\label{sec:Counting-Stars-Results}

%%% counting star results
%\input{count_star}