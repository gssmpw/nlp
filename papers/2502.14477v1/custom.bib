% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").







% start
@article{Jiang:2024,
  title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
  author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2407.02490},
  year={2024}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lu2024longheads,
  title={LongHeads: Multi-Head Attention is Secretly a Long Context Processor},
  author={Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.10685},
  year={2024}
}

@article{singhania2024loki,
  title={Loki: Low-Rank Keys for Efficient Sparse Attention},
  author={Singhania, Prajwal and Singh, Siddharth and He, Shwai and Feizi, Soheil and Bhatele, Abhinav},
  journal={arXiv preprint arXiv:2406.02542},
  year={2024}
}

@article{han2023lm,
  title={Lm-infinite: Simple on-the-fly length generalization for large language models},
  author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  journal={arXiv preprint arXiv:2308.16137},
  year={2023}
}

@article{xiao2024infllm,
  title={Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.04617},
  year={2024}
}


@misc{su2023rerope,
  author       = "{Su, Jianlin}",
  title        = "{ReRoPE}",
  howpublished = "\url{https://kexue.fm/archives/9708}",
  year         = "{2023}"
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{su2024roformer,
	title={Roformer: Enhanced transformer with rotary position embedding},
	author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
	journal={Neurocomputing},
	volume={568},
	pages={127063},
	year={2024},
	publisher={Elsevier}
}

@article{chen2023extending,
	title={Extending context window of large language models via positional interpolation},
	author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	journal={arXiv preprint arXiv:2306.15595},
	year={2023}
}


@misc{kaiokendev,
	title={Things iâ€™m learning while training superhot.},
	author={kaiokendev},
	url={https://kaiokendev.github.io/til#extending-context-to-8k./},
	year={2023}
}
@misc{bloc97ntk,
title = {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation
},
url = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
},
author = {bloc97},
year = {2023}
}
@article{peng2023yarn,
	title={Yarn: Efficient context window extension of large language models},
	author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	journal={arXiv preprint arXiv:2309.00071},
	year={2023}
}
@article{chen2023clex,
  title={Clex: Continuous length extrapolation for large language models},
  author={Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.16450},
  year={2023}
}

@article{JianlinSu,
	title={Rectified rotary position embeddings.},
	author={Jianlin Su},
	journal={https://github.com/bojone/rerope},
	year={2023}
}

@article{xiao2023efficient,
	title={Efficient streaming language models with attention sinks},
	author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal={arXiv preprint arXiv:2309.17453},
	year={2023}
}
@article{bertsch2024unlimiformer,
  title={Unlimiformer: Long-range transformers with unlimited length input},
  author={Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{ahrabian2024adaptation,
  title={On the Adaptation of Unlimiformer for Decoder-Only Transformers},
  author={Ahrabian, Kian and Benhaim, Alon and Patra, Barun and Pujara, Jay and Singhal, Saksham and Song, Xia},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={12395--12402},
  year={2024}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}
@article{mohtashami2023landmark,
  title={Landmark attention: Random-access infinite context length for transformers},
  author={Mohtashami, Amirkeivan and Jaggi, Martin},
  journal={arXiv preprint arXiv:2305.16300},
  year={2023}
}

@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022}
}
@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}
@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@inproceedings{zhang2024bench,
  title={$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15262--15277},
  year={2024}
}
@article{li2024needlebench,
  title={NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?},
  author={Li, Mo and Zhang, Songyang and Liu, Yunxin and Chen, Kai},
  journal={arXiv preprint arXiv:2407.11963},
  year={2024}
}
@article{song2024counting,
  title={Counting-stars: A simple, efficient, and reasonable strategy for evaluating long-context large language models},
  author={Song, Mingyang and Zheng, Mao and Luo, Xuan},
  journal={arXiv preprint arXiv:2403.11802},
  year={2024}
}
@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}
@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}
@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}
@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
% end


@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@article{agrawal2023sarathi,
  title={SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills},
  author={Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.16369},
  url={https://api.semanticscholar.org/CorpusID:261395577}
}
@article{Adnan2024KeyformerKC,
  title={Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference},
  author={Muhammad Adnan and Akhil Arunkumar and Gaurav Jain and Prashant J. Nair and Ilya Soloveychik and Purushotham Kamath},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.09054},
  url={https://api.semanticscholar.org/CorpusID:268385087}
}
@article{Ding2024LongRoPEEL,
  title={LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  author={Yiran Ding and Li Lyna Zhang and Chengruidong Zhang and Yuanyuan Xu and Ning Shang and Jiahang Xu and Fan Yang and Mao Yang},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.13753},
  url={https://api.semanticscholar.org/CorpusID:267770308}
}
@inproceedings{Yang2024PyramidInferPK,
  title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
  author={Dongjie Yang and Xiaodong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269930254}
}
@article{Cai2024PyramidKVDK,
  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
  author={Zefan Cai and Yichi Zhang and Bofei Gao and Yuliang Liu and Tianyu Liu and Keming Lu and Wayne Xiong and Yue Dong and Baobao Chang and Junjie Hu and Wen Xiao},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.02069},
  url={https://api.semanticscholar.org/CorpusID:270226243}
}
@inproceedings{Lee2024InfiniGenEG,
  title={InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management},
  author={Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim},
  booktitle={USENIX Symposium on Operating Systems Design and Implementation},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270845627}
}
@article{Jin2024LLMML,
  title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning},
  author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-yuan Chang and Huiyuan Chen and Xia Hu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.01325},
  url={https://api.semanticscholar.org/CorpusID:266725385}
}
@article{An2024TrainingFreeLS,
  title={Training-Free Long-Context Scaling of Large Language Models},
  author={Chen An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.17463},
  url={https://api.semanticscholar.org/CorpusID:268032518}
}
@article{Singhania2024LokiLK,
  title={Loki: Low-Rank Keys for Efficient Sparse Attention},
  author={Prajwal Singhania and Siddharth Singh and Shwai He and Soheil Feizi and Abhinav Bhatele},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.02542},
  url={https://api.semanticscholar.org/CorpusID:270226131}
}
@article{Yu2024EffectivelyCK,
  title={Effectively Compress KV Heads for LLM},
  author={Hao Yu and Zelan Yang and Shen Li and Yong Li and Jianxin Wu},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.07056},
  url={https://api.semanticscholar.org/CorpusID:270380364}
}
@article{Wu2024RetrievalHM,
  title={Retrieval Head Mechanistically Explains Long-Context Factuality},
  author={Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.15574},
  url={https://api.semanticscholar.org/CorpusID:269330144}
}
@article{Dong2024GetMW,
  title={Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference},
  author={Harry Dong and Xinyu Yang and Zhenyu (Allen) Zhang and Zhangyang Wang and Yuejie Chi and Beidi Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.09398},
  url={https://api.semanticscholar.org/CorpusID:267657553}
}
@article{Chang2024PaluCK,
  title={Palu: Compressing KV-Cache with Low-Rank Projection},
  author={Chi-Chih Chang and Wei-Cheng Lin and Chien-Yu Lin and Chong-Yan Chen and Yu-Fang Hu and Pei-Shuo Wang and Ning-Chi Huang and Luis Ceze and Kai-Chiang Wu},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.21118},
  url={https://api.semanticscholar.org/CorpusID:271571616}
}
@article{Saxena2024EigenAA,
  title={Eigen Attention: Attention in Low-Rank Space for KV Cache Compression},
  author={Utkarsh Saxena and Gobinda Saha and Sakshi Choudhary and Kaushik Roy},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.05646},
  url={https://api.semanticscholar.org/CorpusID:271855637}
}
@article{Rozire2023CodeLO,
  title={Code Llama: Open Foundation Models for Code},
  author={Baptiste Rozi{\`e}re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Tan and Yossi Adi and Jingyu Liu and Tal Remez and J{\'e}r{\'e}my Rapin and Artyom Kozhevnikov and I. Evtimov and Joanna Bitton and Manish P Bhatt and Cristian Cant{\'o}n Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre D'efossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.12950},
  url={https://api.semanticscholar.org/CorpusID:261100919}
}
@article{Liu2023ScalingLO,
  title={Scaling Laws of RoPE-based Extrapolation},
  author={Xiaoran Liu and Hang Yan and Shuo Zhang and Chen An and Xipeng Qiu and Dahua Lin},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.05209},
  url={https://api.semanticscholar.org/CorpusID:263828829}
}
@article{Choromanski2020RethinkingAW,
  title={Rethinking Attention with Performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tam{\'a}s Sarl{\'o}s and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy J. Colwell and Adrian Weller},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.14794},
  url={https://api.semanticscholar.org/CorpusID:222067132}
}
@article{dao2205fast,
  title={fast and memory-efficient exact attention with IO-awareness. arXiv; 2022},
  author={Dao, T and Fu, DY and Ermon, S and Rudra, A and R{\'e}, C FlashAttention},
  journal={arXiv preprint arXiv:2205.14135}
}
@article{Beltagy2020LongformerTL,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.05150},
  url={https://api.semanticscholar.org/CorpusID:215737171}
}
@Misc{accelerate,
  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year =         {2022}
}
@article{Zaheer2020BigBT,
  title={Big Bird: Transformers for Longer Sequences},
  author={Manzil Zaheer and Guru Guruganesh and Kumar Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Onta{\~n}{\'o}n and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.14062},
  url={https://api.semanticscholar.org/CorpusID:220831004}
}