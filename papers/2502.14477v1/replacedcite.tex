\section{Related Work}
\paragraph{Position Extrapolation.}
%Position extrapolation methods address longer contexts by scaling position embeddings (PE).  Early work primarily focused on achieving length extrapolation by modifying relative PEs during the pre-training phase. 
Following the introduction of RoPE ____, great efforts have been imposed to extend the context length by modifying the position embeddings (PE).
Position interpolation  ____ extends the context length by interpolating positional indices within the constraints of pre-training. 
The NTK-aware method ____ introduces a nonlinear interpolation strategy by increasing the base parameter of RoPE. YaRN ____ proposes a method for frequency interpolation of RoPE dimensions, where higher frequency dimensions are extrapolated, and lower frequency dimensions are interpolated. Further improvements ____ exploit the dynamics in position extrapolation.
Another group of work redesigns the relative position matrix to overcome the OOD issue ____. 
%CLEX generalizes PE scaling to model the transition of frequency basis continuously ____.
These methods extend the context length but still compute the full attention matrix for inference thus fail to reduce the computational cost. To achieve better performance, some require fine-tuning with a certain amount of long-context data.
%ReRoPE proposes to maintain the position encodings within the window unchanged and truncate the position encodings that extend beyond the window ____. 

\paragraph{Selective Attention.}
Selective attention mechanisms aim to mitigate the computational cost of processing long sequences by selecting only the most relevant tokens for attention computation. Approaches like Longformer ____ and BigBird ____ use fixed or adaptive sparse patterns, while ____ introduce $\Lambda$-shaped windows that evict middle tokens. Although these methods lower costs, they often compromise global context understanding due to restricted attention over all tokens.
Some methods aim at compressing the KV cache, usually perform token selection only in the decoding stage ____ or permanently evicting certain tokens ____. While effective, they may lose critical contextual information.
As for chunk-based methods, ____ uses an efficient contextual memory mechanism to select the most relevant chunks for computing attention, and ____ selects the most relevant chunks for each head separately considering the variance among different heads.
Unlimiformer and its adaptation ____ segment input during the pre-filling stage, using external memory blocks, but remain computationally expensive and require additional training or architecture modifications.
In contrast, our method performs efficient token-level selection in both prefilling and decoding stages, without discarding tokens permanently.

%These methods reduce memory usage and computational complexity by limiting the size of the attention computation window, and mitigate the decline in accuracy by selectively focusing on the most critical tokens.
% % they are usually computationally expensive or require further training and modification on the architecture ____. 
% but may lose valuable contextual information. Compared to these methods, our proposed approach does not require chunking of the context, thus ensuring the flexibility of token selection and the integrity of contextual information.