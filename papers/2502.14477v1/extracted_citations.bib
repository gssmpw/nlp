@article{An2024TrainingFreeLS,
  title={Training-Free Long-Context Scaling of Large Language Models},
  author={Chen An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.17463},
  url={https://api.semanticscholar.org/CorpusID:268032518}
}

@article{Beltagy2020LongformerTL,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.05150},
  url={https://api.semanticscholar.org/CorpusID:215737171}
}

@article{Ding2024LongRoPEEL,
  title={LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  author={Yiran Ding and Li Lyna Zhang and Chengruidong Zhang and Yuanyuan Xu and Ning Shang and Jiahang Xu and Fan Yang and Mao Yang},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.13753},
  url={https://api.semanticscholar.org/CorpusID:267770308}
}

@article{JianlinSu,
	title={Rectified rotary position embeddings.},
	author={Jianlin Su},
	journal={https://github.com/bojone/rerope},
	year={2023}
}

@article{Jin2024LLMML,
  title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning},
  author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-yuan Chang and Huiyuan Chen and Xia Hu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.01325},
  url={https://api.semanticscholar.org/CorpusID:266725385}
}

@article{Liu2023ScalingLO,
  title={Scaling Laws of RoPE-based Extrapolation},
  author={Xiaoran Liu and Hang Yan and Shuo Zhang and Chen An and Xipeng Qiu and Dahua Lin},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.05209},
  url={https://api.semanticscholar.org/CorpusID:263828829}
}

@article{Rozire2023CodeLO,
  title={Code Llama: Open Foundation Models for Code},
  author={Baptiste Rozi{\`e}re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Tan and Yossi Adi and Jingyu Liu and Tal Remez and J{\'e}r{\'e}my Rapin and Artyom Kozhevnikov and I. Evtimov and Joanna Bitton and Manish P Bhatt and Cristian Cant{\'o}n Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre D'efossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.12950},
  url={https://api.semanticscholar.org/CorpusID:261100919}
}

@article{Zaheer2020BigBT,
  title={Big Bird: Transformers for Longer Sequences},
  author={Manzil Zaheer and Guru Guruganesh and Kumar Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Onta{\~n}{\'o}n and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.14062},
  url={https://api.semanticscholar.org/CorpusID:220831004}
}

@inproceedings{ahrabian2024adaptation,
  title={On the Adaptation of Unlimiformer for Decoder-Only Transformers},
  author={Ahrabian, Kian and Benhaim, Alon and Patra, Barun and Pujara, Jay and Singhal, Saksham and Song, Xia},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={12395--12402},
  year={2024}
}

@article{bertsch2024unlimiformer,
  title={Unlimiformer: Long-range transformers with unlimited length input},
  author={Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{bloc97ntk,
title = {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation
},
url = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
},
author = {bloc97},
year = {2023}
}

@article{chen2023clex,
  title={Clex: Continuous length extrapolation for large language models},
  author={Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.16450},
  year={2023}
}

@article{chen2023extending,
	title={Extending context window of large language models via positional interpolation},
	author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	journal={arXiv preprint arXiv:2306.15595},
	year={2023}
}

@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@article{han2023lm,
  title={Lm-infinite: Simple on-the-fly length generalization for large language models},
  author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  journal={arXiv preprint arXiv:2308.16137},
  year={2023}
}

@misc{kaiokendev,
	title={Things iâ€™m learning while training superhot.},
	author={kaiokendev},
	url={https://kaiokendev.github.io/til#extending-context-to-8k./},
	year={2023}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{peng2023yarn,
	title={Yarn: Efficient context window extension of large language models},
	author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	journal={arXiv preprint arXiv:2309.00071},
	year={2023}
}

@article{su2024roformer,
	title={Roformer: Enhanced transformer with rotary position embedding},
	author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
	journal={Neurocomputing},
	volume={568},
	pages={127063},
	year={2024},
	publisher={Elsevier}
}

@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022}
}

@article{xiao2023efficient,
	title={Efficient streaming language models with attention sinks},
	author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal={arXiv preprint arXiv:2309.17453},
	year={2023}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

