Large language models (LLMs) have demonstrated remarkable capabilities across a variety of natural language processing (NLP) tasks.
One of the emerging trends in this area is the increasing emphasis on extending the context length of LLMs to handle tasks like contextual learning and retrieval-augmented generation \citep{dong2022survey,gao2023retrieval}. However, this extension comes with its own set of challenges, particularly in terms of the Out-Of-Distribution (OOD) extrapolation on context length, massive memory demanded by key-value (KV) cache and the quadratic computational complexity of the attention mechanism. In this paper, we focus on mitigating the OOD issues and reducing the computational cost of LLM inference on long-context sequences, especially for models which adopt Rotary Position Embedding (RoPE) \citep{su2024roformer}.

%RoPE不需要过多描述，challenges也要简洁一些。
%We focus on decoder-only Language Models (LLMs) with Rotary Position Embedding (RoPE) extrapolated to longer sequence tasks during inference \citep{su2024roformer}. 
%Currently, there are two main challenges with the application of these models on long sequences.
%First, When the inference length exceeds the training length, the positional encodings encounter values beyond those during training, referred to as Out-of-Distribution (OOD) issues.
%Second, the quadratic computational complexity of the attention mechanism significantly reduces the efficiency of long sequence inference, known as efficiency limitations.

Extrapolation methods based on positional embeddings address the OOD issues \citep{chen2023extending, bloc97ntk, peng2023yarn}, but they are not targeted at reducing computational cost.
To improve inference efficiency, methods focusing on \emph{selective attention} are developed, exploiting the inherent sparsity of attention matrices \citep{zhang2024h2o, Jiang:2024}. These methods usually restrict the number of selected tokens to within the pre-training context length. Early approaches focus on local attention, sometimes introduce global tokens, often neglecting long-distance tokens \citep{Beltagy2020LongformerTL,xiao2023efficient, han2023lm}. Recent chunk-based methods, such as InfLLM, LongHeads, and Landmark Attention \citep{xiao2024infllm, lu2024longheads, mohtashami2023landmark}, have aimed to extend the context length by chunking input prompts into fixed-size blocks and selecting the top-ranked chunks. 
These approaches, while reducing computational complexity, often suffer from a lack of flexibility. 
%Fixed-size chunks may not always align with the natural semantic boundaries of the input text, leading to suboptimal performance.
%While these methods reduce computational complexity, the selection of tokens from a few top-ranked chunks for attention computation lacks flexibility in selection.
In contrast, token-level selection methods provide more flexibility by selectively attending to tokens at a finer granularity.
However, existing methods usually perform token selection only in the decoding stage and permanently evict certain tokens to manage memory usage as well as reduce computation, facing high latency during prefilling and significant information loss \citep{zhang2024h2o,liu2024scissorhands,ge2023model,li2024snapkv,Cai2024PyramidKVDK}.
To maintain accuracy while reducing computation, it is essential to perform token selection over all preceding tokens during both prefilling and decoding. Nevertheless, selecting tokens individually could introduce significant computational overhead, raising the challenge of how to efficiently identify the most crucial tokens while minimizing cost. This presents a key research problem: balancing flexibility and efficiency for selective attention approach.


To this end, we propose ESA, an efficient token-level selective attention algorithm that enables length extrapolation without the need for incremental training of LLM parameters. Specifically, our method consists of two steps: efficient selection and attention computation. 
In the first step, we introduce a query-aware token-level selection mechanism that adaptively identifies the most crucial tokens.
To reduce the computational cost, we compress the query and key vectors in the attention heads into low-dimensional representations when evaluating the importance of individual tokens. A learnable approach is employed to derive these compression functions.
%We reduce the computational complexity of selecting tokens by utilizing compressed queries and keys.
Additionally, we found that directly selecting the top-ranked tokens can lead to performance degradation in certain tasks. Therefore, we propose \emph{proximity influence}, where the surrounding tokens are incorporated when calculating the importance score of a particular token.
In the step of attention computation, we utilize the full-sized keys and values of the selected tokens, rather than those of all the preceding tokens. This reduces the complexity of traditional attention computation from quadratic to linear. While there is an added computational cost for token selection, it is significantly lower than the complexity of standard attention computation.

To demonstrate the effectiveness of our approach, we conduct extensive evaluations on LongBench \citep{bai2023longbench},
$\infty$BENCH \citep{zhang2024bench}, NeedleBench \citep{li2024needlebench}, and Counting-Stars \citep{song2024counting} with Mistral-7B-Instruct-v0.2 (\texttt{Mistral}) \citep{jiang2023mistral} and Llama-3-8B-Instruct (\texttt{Llama}) \citep{llama3modelcard}.

Our contributions are summarized as follows: 1. We propose a novel token-level selective attention method for extending the context length, without incremental training of parameters of LLMs. By introducing proximity influence, we improve semantic continuity among selected tokens, addressing performance degradation caused by directly selecting top-ranked tokens in certain tasks. 2. We reduce computational complexity by leveraging low-dimensional queries and keys for token selection, achieving competitive accuracy at a significantly lower cost.

%3. We propose strategies for memory consumption and computational complexity. We introduce a cache for storing compressed keys, the size of which is negligible compared to the original KV cache. It eliminates the need to repeatedly compress the keys of the preceding tokens, thereby reducing computational load.