\section{Related Work}
\label{sec:related_work}

%\subsection{Vision-LMs}

%\todo[inline]{evolution of VLM.}



Various types of benchmarks have been proposed since the raise of VLMs. From the single \emph{task-oriented} benchmarks (e.g.~\citet{antol:vqa15,visdial}), attention has now moved to task collections~\cite{lvlmehubbenchmark,lee:vhelm24} in which models show impressive performance. As in the early phase~\cite{johnson2017clevr,shekhar-etal-2017-foil,suhr-etal-2017-corpus}, such success is mitigated by the use of \emph{diagnostic} benchmarks, such as~\citet{valse,thrush:winoground22,chen-etal-2023-bla, bugliarello-etal-2023-measuring,bianchi2024devil} and \emph{carefully curated} benchmarks such as~\citet{xio:nextGQA24,tong2024eyes} . The third type of benchmarks available focus on the VLMs \emph{competence} in a holistic fashion,  evaluating advanced perception and reasoning with domain-specific knowledge \cite{mmmlu23}. A similar picture emerges for video-based VLMs. Here as well, early surveys call for careful evaluation (e.g.~\citet{zhong-etal-2022-video}), task-oriented benchmarks show impressive performance~\cite{GrundeMcLaughlin2021AGQA,zhou:anetqa23}, while fine-grained ones pinpoint important weaknesses \cite{vilma}, and competence-based analysis highlight there is significant room for improvement in multimodal video understanding \citep{patraucean2023perception}. Moreover, LLMs have been shown to suffer from the Generative AI Paradox: they are better at generating than at understanding text~\cite{west2024the}. We verify such statement in our controlled setting.  Finally, both~\citet{tong2024eyes} for images and \citet{vilma} for videos manage to highlight VLMs shortcomings by imposing a more stringent task-accuracy metric that account for model consistency across very similar data or correlated competencies. Thanks to the richness of MAIA data collection, we adopt such severe, and hence robust, evaluation code and propose a novel aggregate metric.


Both closed VLMs, GPT-4, and open-sourced ones have been used as evaluators for VL understanding tasks and fine-grained evaluation \cite{fang2024mmbenchvideo,lee-etal-2024-prometheus}. The application of these models has paved the way for a novel evaluation paradigm in open-ended Visual QA tasks that goes beyond traditional yes/no datasets \cite{vqamatter, okvqa} and underscores the importance of using comprehensive sets of reference answers \citep{vqa_evaluation_manos}. 
%\todo{Say how we relate to such literature, what we continue/merge/improve}

%\todo[inline]{Potrebbe andar bene una cosa del genere?  B: da accorciare. Manca riferimento a pairing understanding and generation -> ho messo una frase alla fine ora}
%In this complex landscape, zero-shot evaluations in \citet{valse, vilma} have revealed critical issues such as the high presence of distributional and plausibility biases, with VLMs often over-relying on linguistic cues, showing a phenomenon known as \textit{Unimodal Collapse}. \citet{mmshap} address this by employing a multimodality score using shapley values to quantify modality usage in text-image scenarios.
Building on this literature, MAIA extends existing approaches by focusing on a low-resource language and the less explored domain of videos. While benchmarks like \citet{exams} and \citet{m3exam} include a minority native Italian component through multiple-choice tasks, to the best of our knowledge, no competence-oriented benchmarks specifically target high-level reasoning in Italian video contexts, nor do they analyze the influence of distributional biases on VLM behavior in such settings. Similarly, existing evaluation frameworks do not yet combine paired understanding and generation tasks that challenge models from multiple perspectives, thereby enforcing a robust evaluation strategy.

%\todo[inline]{B: questa parte andrebbe elaborata un po' e lasciata qui, in coda a related work.

Finally, our selection of categories differs from other benchmarks, such as  the widely used AGQA~\cite{GrundeMcLaughlin2021AGQA} or the more recent MVBench~\cite{li:mvbench24}. They focus on the explicit visual structure (entity, action, and the spatio-temporal reasoning involving them), while MAIA  aims to capture the relation expressed  between language and vision when the two modalities are clearly aligned or their relation is implicit and must be inferred, a dimension neglected so far in Video QA benchmarks.  


%or  \citet{} who propose a Perception Test  to evaluate the perception and reasoning skills of pre-trained multimodal models, focusing on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual).



%COMPETENCE: "MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations." . VHELM: an holistic evaluation/ VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety \cite{lee:vhelm24}.  

%AGQA \cite{} propose a  paradigm to generate QA pairs automatically from pre-annotated scene graphs, enabling it to measure diverse reasoning abilities with granular control.  NetQA \cite{} larger and more fine-grained.