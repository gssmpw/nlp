\section{Experimental Setting}
\label{sec:exp_sett}

 We run several experiments to test state-of-the-art VLMs on the MAIA benchmark in a zero-shot setting. Experiments are designed to reveal both strengths and limitations of current VLMs in handling complex visual reasoning  tasks on video data. To capture different VLM behaviors, we defined two tasks: a multiple choice task, \textit{Visual Statement Verification}, and a generative task, \textit{Open-ended Visual Question Answering}.
 
 
% while also establishing informative baselines for further comparisons and analyzing key phenomena that affect VLMs (see Section \ref{sec:related_work}). 
 

\subsection{Task1: Visual Statement Verification}
\label{task1}
Visual Statement Verification (VSV) is a multiple-choice task where a model is presented with a true and a false statement related to a MAIA question (see section \ref{subsec: Dataset}) for a given video, and has to select the true statement. The two statements are randomly assigned to two labels, A and B, and the model is asked to generate only the label. We chose the prompt through an extensive evaluation of $32$ variants ($16$ in Italian and $16$ in English), with the best-performing Italian prompt ultimately selected. 
Performance for VSV is measured with accuracy, i.e.,  the proportion of correctly selected true statements over the total statement pairs. 

%Two  scores are used: (1) Single Item Accuracy, which measures the model's ability to correctly identify the TS for each individual item, and (2) Consistency Score, which assesses whether the model is consistent in its evaluation across the 8-pool of True-False Statement pairs.

\subsection{Task2: Open-ended VQA}
\label{task1}
Open-ended Visual Question Answering (OEVQA) is a generative task, where models are tested on their ability to provide correct open-ended answers to a question related to video content. The model receives as input  a prompt question and a video, and is tasked with generating a correct answer. The prompt used in the experiments was selected as the best-performing among $10$ tested variants ($5$ in Italian and $5$ in English). 
Generated responses are then  evaluated according to two approaches: similarity-based  and LLM as a judge.

 
\paragraph{Similarity-based metrics}  compare a  response against the pool of eight reference answers available in MAIA.
We used five token-level metrics: \textit{ROUGE} \cite{lin-2004-rouge}, \textit{BLEU} \cite{10.3115/1073083.1073135}, \textit{BERT-Score} \cite{zhang2020bertscoreevaluatingtextgeneration}, \textit{METEOR} \cite{10.5555/1626355.1626389} and \textit{CIDEr} \cite{vedantam2015ciderconsensusbasedimagedescription}. 


\paragraph{LLM as a judge.} While similarity-based metrics  provide a reasonable ranking among VLMs, they fail to validate the correctness of a model generated answer.  To this end, we add a LLM as a judge metric \cite{gu2025surveyllmasajudge}, where the correctness of an answer is judged by a LLM, i.e., \textit{GPT-4o}.
In this context,  an answer to a question is considered correct if it is semantically consistent with at least one of the eight reference answers provided by MAIA for that question. Following~\citet{bavaresco:llm24}  we validate the LLM-as-a-judge metric through the correlation with human judgments on 100 samples. Two human labelers and GPT-4o have been asked to annotate the correct answers, confirming a strong agreement with a Fleiss' Kappa score of 0.82.

%This approach ensures both the reliability and reproducibility of the experiment. 

%For this task we used five token-level metrics: \textit{ROUGE} \cite{lin-2004-rouge} a metric originally introduced for summarization, \textit{BLEU} \cite{10.3115/1073083.1073135}, used for machine translation, \textit{BERT-Score} \cite{zhang2020bertscoreevaluatingtextgeneration} based on embeddings, \textit{METEOR} \cite{10.5555/1626355.1626389} and \textit{CIDEr} \cite{vedantam2015ciderconsensusbasedimagedescription}. Such surface form- and similarity-based metrics, which facilitate performance comparisons among models, were then integrated by an NLI task performed by \textit{GPT-4o}. An answer is considered correct if it entails at least one of the $8$ reference answers and is provided in Italian; responses in any other language are considered incorrect. 

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lc|c|c|c|c|c|c|c|cc|cc|cc}
\toprule
 & \textbf{Models} & \textbf{Avg.} & \textbf{Causal} & \textbf{Counterfactual}  & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment}&  \textbf{Uncertainty} & \multicolumn{2}{c|}{\textbf{Implicit}} & \multicolumn{2}{c|}{\textbf{Spatial}} & \multicolumn{2}{c}{\textbf{Temporal}} \\
 &  &  &  &   &  &  &  & & \textit{Partial} & \textit{Total} & \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
 \midrule

 
\textbf{Most Probable} &  {} & 0.56 & 0.45 & 0.73   & 0.55 & 0.68 & 0.53 & 0.79 & 0.50 & 0.51 & 0.48 & 0.51 & 0.53 & 0.48 \\ 
\midrule

\multirow{4}{*}{\textbf{Black video}} & \textit{InternVL2} & 0.68 & 0.64 & \underline{\textbf{0.88}}   & 0.89 & 0.69 & 0.61& 0.96 & 0.52 & 0.59 & 0.54 & 0.55 & 0.67 & 0.60 \\
 & \textit{Llava-Next-Video} & 0.50 & 0.49 & 0.52   & 0.48 & 0.51 & 0.52 & 0.45 & 0.50 & 0.51 & 0.51 & 0.49 & 0.51 & 0.48 \\
 & \textit{Llava-oneVision} & 0.59 & 0.55 & 0.48   & \underline{\textbf{0.92}} & 0.61 & 0.38 & 0.97 & 0.52 & 0.53 & 0.50 & 0.52 & 0.60 & 0.51 \\
 & \textit{Qwen-2.5-VL} & \textbf{0.73} & \textbf{0.68} & 0.86   & 0.87 & \textbf{0.75} & \textbf{0.72}& \underline{\textbf{0.98}} & \textbf{0.56} & \textbf{0.62} & \textbf{0.61} & \textbf{0.65} & \textbf{0.73} & \textbf{0.70} \\
\midrule

\multirow{4}{*}{\textbf{1-Frame}} & \textit{InternVL2} & 0.75 & 0.80 & \textbf{0.87}   & 0.69 & 0.71 & 0.77 & 0.89 & 0.65 & 0.80 & 0.65 & 0.82 & 0.67 & 0.69 \\
 & \textit{Llava-Next-Video} & 0.61 & 0.68 & 0.76   & 0.49 & 0.63 & 0.75 & 0.40 & 0.59 & 0.65 & 0.59 & 0.70 & 0.56 & 0.53 \\
 & \textit{Llava-oneVision} & 0.76 & 0.79 & 0.78   & 0.89 & 0.74 & 0.76 & 0.91 & 0.68 & 0.81 & 0.64 & 0.81 & \textbf{0.70} & 0.67 \\
 & \textit{Qwen-2.5-VL} & \textbf{0.79} & \textbf{0.81} & 0.81   & \textbf{0.91} & \textbf{0.75} & \textbf{0.82}& \textbf{0.70} & \underline{\textbf{0.82}}& \underline{\textbf{0.96}} & \textbf{0.67} & \textbf{0.82} & 0.69 & \textbf{0.73} \\ 
 

 

 \midrule
 \midrule

 
\multirow{4}{*}{\textbf{32-Frames}} & \textit{InternVL2} & {0.79} & 0.83 &  \textbf{0.85}   &  {0.77} &  {0.75} &  {0.84}&  {0.84} &  {0.75} &  {0.83} &  {0.69} &  {0.86} &  {0.66} &  {0.76} \\
 &  \textit{Llava-Next-Video} &  {0.52} &  0.56 &  {0.57}   &  {0.42} &  {0.57} &  {0.61} &  {0.32}&  {0.52} &  {0.59}&  {0.52} &  {0.56} &  {0.51} &  {0.48} \\
 &  \textit{Llava-oneVision} &  {0.81} &  0.87 &  {0.78}   &  {0.88} &  {0.76} &  \underline{\textbf{0.88}}&  {0.85} &  {0.80} &  {0.85} &  {0.71} &  {0.87} &  {0.65} &  {0.80} \\
 &  \textit{Qwen-2.5-VL} &  \underline{\textbf{0.84}} &  \underline{\textbf{0.89}} &  {0.80}   &  \textbf{0.89} &  \underline{\textbf{0.78}} &  {0.86}&  \textbf{0.92} &  \underline{\textbf{0.82}} &  \textbf{0.88}&  \underline{\textbf{0.83}} &  \underline{\textbf{0.90}} &  \underline{\textbf{0.75}} &  \underline{\textbf{0.81}} \\ 
 \bottomrule
\end{tabular}%
}
\caption{\label{tab:task1_res}
    Visual Statement Verification (Task $1$): accuracy of correct choices across reasoning categories. Best values for each setting are in bold and best overall are underlined.
  }
\end{table*}

\subsection{Baselines} 
We implemented three baselines for our tasks. 

\paragraph{Most probable}  baseline applies to Task 1, and selects the most probable statement in a TS-FS pair. It is intended as a language unimodal baseline, based on distributional biases in VLMs. Probabilities of TS and FS are first estimated on five open-weight LLMs that have shown good performance on a variety of Italian tasks  \cite{magnini2025evalitallmbenchmarkinglargelanguage}: Llama-3.1 ($8$B-Instruct), LLaMAntino-2 ($7$B), LLaMAntino-3-ANITA ($8$B-Instruct), Gemma ($7$B) and Qwen2.5 ($7$B-Instruct). 
Then probabilities are combined,  taking the highest probability from the five models for each case. 
%rather than averaging them, to prevent potential negative bias.



\paragraph{Black video} substitutes the MAIA videos with a completely black video. Since, for some VLMs, the input needs to be an [image/video - text] pair, we use a black video as a proxy for a no-video configuration. The goal is to limit as much as possible the capacity of the VLM to capture meaningful visual features, which, as a consequence, should mostly relay on the language component. 

\paragraph{1-frame.} This baseline considers only one selected frame for each MAIA video, this way  reducing the capacity of a VLM to capture visual features. The one-frame “static appearance bias”  has been discussed in  \cite{lei-etal-2023-revealing}.  For all models the selected frame is the first one in the video. 
The intuition is that, particularly for e.g.,  temporal  questions, models' performance on the one-frame baseline should significantly drop.


\subsection{Vision-Language-Models.}
We benchmarked four recent VLMs sourced from the Hugging Face Hub, representing state-of-the-art approaches in Vision-Language tasks\footnote{More details about VLMs are reported in Appendix \ref{sec:models_app} }: InternVL2 (8B, \citet{internvl}), LLaVA-NeXT-Video (7B, \citet{llavanextvideo}), LLaVa-OneVision (7B, \citet{llavaonevisioneasyvisualtask}), and Qwen2.5-VL (7B, \citet{qwen2.5technicalreport}). All models accept a [video, text] pair as input, and uniformly sample 32 frames from the video.




\label{ref:results}
%Figure \ref{Table: Result_task1} illustrates the relationship between VLMs and out baseline for each semantic category. Interestingly, text-only LLMs sometimes achieve higher accuracy in selecting the correct statement than VLMs, despite lacking visual input. For instance, in the \textit{Counterfactual} category the baseline values closely match those of the VLMs, while in the \textit{Uncertainty} one the baseline even outperforms both InternVL and LLaVA-NeXT-Video. Overall, the accuracy gap between our baseline and the VLMs is generally smaller than one might expect or theoretically anticipate.\\








% \begin{table}
%   \centering
%   %\small
%   \resizebox{0.6\linewidth}{!}{%

%   \renewcommand{\arraystretch}{1.2}
%   \setlength{\tabcolsep}{4pt}
%   \begin{tabular}{ll|c}
%     \toprule
%         & Case & \%\\
%                \midrule
%                \midrule
    
%     \multirow{4}{*}{\textbf{InternVL2}} &CaseA & 00\%   \\ 
%     &CaseB &  00\%   \\ 
%     &CaseC &  00\%   \\ 
%     &CaseD &  00\%   \\ 
% \midrule
%     \multirow{4}{*}{\textbf{Llava-Next-Video}} &CaseA &  00\%   \\ 
%     &CaseB &  00\%   \\ 
%     &CaseC &  00\%   \\ 
%     &CaseD &  00\%   \\ 
%     \midrule
%      \multirow{4}{*}{\textbf{Llava-oneVision}} &CaseA &  00\%   \\ 
%     &CaseB &  00\%   \\ 
%     &CaseC &  00\%   \\ 
%     &CaseD &  00\%   \\ 
% \midrule

%     \multirow{4}{*}{\textbf{Qwen-2.5-VL}} &CaseA & 00\%   \\ 
%     &CaseB &  00\%   \\ 
%     &CaseC &  00\%   \\ 
%     &CaseD &  00\%   \\
%     \bottomrule
%   \end{tabular}%
%   }
%   \caption{\label{Table: Result_task2}
%     Results VLM accuracy across LLM
%   }
% \end{table}




