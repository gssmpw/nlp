
\begin{table*}[]
%\caption{Open-ended Visual Question Answering (Task $2$). Accuracy scores obtained with LLM-as-a-judge}
%\label{tab:Task2_llm_as_judge}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lc|c|c|c|c|c|c|c|cc|cc|cc}
\toprule
 & \textbf{Models} & \textbf{Avg.} & \textbf{Causal} & \textbf{Counterfactual}  & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment}&  \textbf{Uncertainty} & \multicolumn{2}{c|}{\textbf{Implicit}} & \multicolumn{2}{c|}{\textbf{Spatial}} & \multicolumn{2}{c}{\textbf{Temporal}} \\
 &  &  &  &   &  &  &  & & \textit{Partial} & \textit{Total} & \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
 \midrule

\multirow{4}{*}{\textbf{Black video}} & \textit{InternVL2} & 0.37 & 0.42 & 0.60  & 0.30 & \underline{\textbf{0.68}} & 0.43& 0.08 & 0.21 & 0.23  & \textbf{0.36} & \textbf{0.25} & \textbf{0.55} & \textbf{0.25} \\
 & \textit{Llava-Next-Video} & 0.27 & 0.43 & 0.47   & \textbf{0.30} & 0.40 & 0.33& \textbf{0.51}& 0.21 & 0.12 & 0.16 & 0.04 & 0.12 & 0.16 \\
 & \textit{Llava-oneVision} & \textbf{0.40} & \textbf{0.66} & 0.68   & 0.29 & 0.60 & 0.60& 0.28& \textbf{0.26} & \textbf{0.24} & \textbf{0.36} & 0.18 & 0.37 & 0.23 \\
 & \textit{Qwen-2.5-VL} & 0.35 & 0.36 & \textbf{0.69}   & 0.08 & 0.54 & \textbf{0.74}& 0.23& 0.24 & 0.19 & 0.30 & 0.19 & 0.41 & 0.20 \\
 \midrule
 
\multirow{4}{*}{\textbf{1-Frame}} & \textit{InternVL2} & 0.44 & 0.57 & 0.65   & 0.21 & 0.65 & 0.60& 0.10& 0.33 & 0.38 & 0.39 & 0.47 & 0.53 & 0.35  \\
 & \textit{Llava-Next-Video} & 0.32 & 0.30 & 0.56   & 0.20 & 0.46 & 0.60& 0.29& 0.18 & 0.32& 0.24 & 0.27 & 0.20 & 0.19 \\
 & \textit{Llava-oneVision} & 0.50 & \textbf{0.59} & \textbf{0.78}  & 0.15 & \textbf{0.66} & 0.74  & 0.37& \textbf{0.39} & \textbf{0.54}& \textbf{0.39} & 0.51 & 0.54 & 0.33 \\
 & \textit{Qwen-2.5-VL} & \textbf{0.51} & 0.56 & 0.76   & \textbf{0.32} & 0.60 & \textbf{0.79}& \textbf{0.40} & 0.35 & 0.50 & 0.38 & \textbf{0.53} & \textbf{0.55} & \textbf{0.38} \\ 
 \midrule
  \midrule
 
\multirow{4}{*}{\textbf{32-Frames}} & \textit{InternVL2} & 0.49 & 0.54 &  0.68   &  0.28 &  0.64 &  0.62&  0.11 &  0.45  &  0.48&  0.47 &  0.51 &  \underline{\textbf{0.57}} &  0.46 \\
 &  \textit{Llava-Next-Video} &  0.33 &  0.37 &  0.38   &  0.16 &  0.42 &  0.48&  0.27 &  0.24 &  0.37 &  0.29 &  0.39 &  0.32 &  0.29 \\
 &  \textit{Llava-oneVision} &  0.53 &  0.67 &  0.79  &  0.11 &  \textbf{0.65} &  0.79&  0.23 &  \underline{\textbf{0.55}} &  0.56 &  \underline{\textbf{0.51}} &  0.62 &  0.40 &  0.46 \\
 &  \textit{Qwen-2.5-VL} &  \underline{\textbf{0.61}} &  \underline{\textbf{0.71}} &  \underline{\textbf{0.80}}   &  \underline{\textbf{0.43}} &  0.60 &  \underline{\textbf{0.85}}&  \underline{\textbf{0.55}}&  \underline{\textbf{0.55}} &  \underline{\textbf{0.60}} &  0.50 &  \underline{\textbf{0.70}} &  0.54 &  \underline{\textbf{0.53}} \\ 
 \bottomrule
\end{tabular}%
}
 \caption{\label{tab:Task2_llm_as_judge}
    Open-ended Visual Question Answering (Task $2$): accuracy of correct answers with LLM-as-a-judge. Best values for each setting are in bold and best overall are underlined.
  }
\end{table*}

\begin{table}[t!]
\centering
\resizebox{\linewidth}{!}{%
  \begin{tabular}{lc|cc|cc}
  \toprule
    & \textbf{Models} & \multicolumn{2}{|c|}{\textbf{Total Agreement}} & \multicolumn{2}{|c}{\textbf{Majority Agreeement}}  \\
    & &  \textit{TS} & \textit{FS} & \textit{TS} & \textit{FS} \\
    \midrule
    \midrule

       \multirow{4}{*}{\textbf{Black video}} & \textit{InternVL} & 17.83\% &			0.58\% &		    63.79\% &		17.80\%  \\
     & \textit{LLaVa-neXT-Video} & 0.21\% &		0.29\% &		    63.20\% &		    36.30\%  \\
     & \textit{LLava-oneVision} & 13.91\% &		1.71\% &	             57.20\% &	           27.18\%  \\
     & \textit{Qwen2.5-VL} & \textbf{22.25\%} &			0.54\% &			\textbf{66.25\%} &		    10.96\%   \\

     \midrule
  
         \multirow{4}{*}{\textbf{1-Frame}} & \textit{InternVL} & 27.58\% &		0.70\% &	    60.42\% &		11.30\% \\
     & \textit{LLaVa-neXT-Video} & 8.30\% &		0.96\% &			\textbf{68.70\%} &		    22.04\%  \\
     & \textit{LLava-oneVision} & 31.55\% &	0.71\% &		     56.62\% &		   11.12\%    \\
     & \textit{Qwen2.5-VL} & \textbf{35.92\%} & 		0.25\% &		     55.33\% &		    8.50\%   \\

    \midrule
   \midrule
     
     \multirow{4}{*}{\textbf{32-Frame}} & \textit{InternVL} & 31.25 \% & 0.30\% & 60.04\% & 8.41\% \\
     & \textit{LLaVa-neXT-Video} & 2.79\% & 1.46\% & \textbf{62.25\%} & 33.5\%  \\
     & \textit{LLava-oneVision} & 37.71\% & 0.21\% & 54.33\% & 7.75\%   \\
     & \textit{Qwen2.5-VL} & \textbf{43.62\%}  & 0.21\% &  50.92\% & 5.25\%  \\
    \bottomrule
  \end{tabular}%
  }
  \caption{\label{tab:task1-consistency}
Task 1: consistency of TS and FS across the  8-answer pool. Total agreement is 8/8,  majority  is between 4 and 7. There are $0$ cases with agreement < 4.
  }
\end{table}



\section{Results} \label{sec:results}
This Section  reports the main results obtained in our experiments for Task 1 and Task 2.

\subsection{Results on Visual Statement Verification}
\label{ref:vvs} 

\begin{comment}
\begin{table*}
  \centering
  %\small
  \resizebox{\textwidth}{!}{%

  \renewcommand{\arraystretch}{1.2}
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{lc|cc|c|cc|c|c|c|c|cc|cc|c}
    \toprule
       Model & \textbf{Avg. Acc}. & \multicolumn{2}{|c|}{\textbf{Causal}} & \textbf{Counterfactual} & \multicolumn{2}{c|}{\textbf{Implicit}} & \textbf{Uncertainty} & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment} & \multicolumn{2}{c}{\textbf{Spatial}} & \multicolumn{2}{|c|}{\textbf{Temporal}} & Correlation w/ Baseline\\
               & &  \textit{Explicit}
  & \textit{Implicit} & & \textit{Partial} & \textit{Total} & & & & &  \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} & \\ 
            \midrule
            \midrule

    LLMs Ensemble &  0.56  &  0.43    &  0.48   &  0.73  &  0.50  &  0.51  &  0.79  &  0.55  &  0.68  &  0.53  &  0.48  &  0.51  &  0.53  &  0.48 & //  \\
    \midrule
    \midrule
    \textit{InternVL2} &   0.79  &  0.82   &  0.84  &  \textbf{0.85}  &  0.75  &  0.83  &  0.84  &  0.77  &  0.75  &  0.84  &  0.69  &  0.86  &  0.66  & 0.76 & 0.229 \\ 
     \textit{LLava-NeXT-Video} &   0.52\footnotemark[1]   & 0.54  & 0.57  &  0.57 &  0.52 & 0.59 & 0.32 & 0.42 &  0.57 &  0.61 & 0.52 & 0.56 & 0.51 & 0.48 & -0.436 \\ 
     
    \textit{LLava-oneVision} & 0.81  &  0.86 & 0.88 & 0.78 & 0.80 & 0.85 & 0.85 & 0.88 &  0.76 & \textbf{0.88} & 0.71 & 0.87 & 0.65 & 0.80 & -0.068 \\ 
   
    \textit{Qwen-2.5-VL} & \textbf{0.84} & \textbf{0.89} & \textbf{0.90} & 0.80 & \textbf{0.82} & \textbf{0.88} & \textbf{0.92} & \textbf{0.89} & \textbf{0.78} & 0.86 & \textbf{0.73} & \textbf{0.90} & \textbf{0.75} & \textbf{0.81}  & 0.030\\  
    \bottomrule
  \end{tabular}%
  }
  \caption{\label{Table: Result_task1}
    Accuracy performance of 4 VLMs on the MAIA benchmark for Visual Statement Verification (Task 1).
  }
\end{table*}
\footnotetext[1]{For this  model, 193 cases of out-of-domain responses (i.e., neither A nor B) were recorded.}
\end{comment}


% \begin{table*}
%   \centering
%   \scriptsize
%   \begin{tabular}{l|cccc}
%     \toprule
%     & \textit{InternVL2} & \textit{LLava-NeXT-Video} & \textit{Qwen-2.5-VL} & \textit{LLava-OneVision} \\
%     \midrule
%     \textbf{Black Video} & 0.6797 & 0.4989 & Ongoing & 0.5904 \\
%     \textbf{1 Frame} & 0.7511 & 0.6104 & 0.7915 & 0.7652 \\
%     \bottomrule
%   \end{tabular}
%   \caption{\label{Table:model_results_baselines} Model accuracy results for different visual conditions.}
% \end{table*}


\begin{comment}
\begin{table*}
  \centering
  %\small
  \resizebox{\textwidth}{!}{%

  \renewcommand{\arraystretch}{1.2}
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{lcc|cc|c|cc|c|c|c|c|cc|cc}
    \toprule
       Metric & Model & Mean & \multicolumn{2}{c|}{\textbf{Causal}} & \textbf{Counterfactual} & \multicolumn{2}{c|}{\textbf{Implicit}} & \textbf{Uncertainty} & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment} & \multicolumn{2}{c}{\textbf{Spatial}} & \multicolumn{2}{|c}{\textbf{Temporal}}\\
               & & & \textit{Explicit} & \textit{Implicit} & & \textit{Partial} & \textit{Total} & & & & &  \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
               \midrule
               \midrule
    
    \multirow{4}{*}{\textbf{1-Frame}} &\textit{InternVL2} & 0.75 & 0.77  & 0.82  &  0.87 & 0.65 & 0.80 & 0.89 & 0.69 & 0.71 & 0.77 & 0.65 & 0.82 & 0.67 & 0.69 \\ 
    
     &\textit{Llava-Next-Video} & 0.61 & 0.67 & 0.70  &  0.76 &  0.59 &  0.65 &  0.40 & 0.49 & 0.63 & 0.75 &  0.59 & 0.70 & 0.56 & 0.53 \\ 
     
    &\textit{Llava-oneVision} & 0.76 & 0.75  & 0.83  & 0.78 & 0.68 &  0.81 & 0.91 & 0.89 & 0.74 & 0.76 & 0.64 & 0.81 &  0.70 & 0.67 \\ 
   
    &\textit{Qwen-2.5-VL} & 0. & 0. & 0.  &  0. & 0. &  0. & 0. &  0. & 0.  & 0. &  0. &  0. &  0. & 0. \\  
    \midrule
     \multirow{4}{*}{\textbf{Black video}} &\textit{InternVL2} & 0.68 & 0.61 & 0.67 & 0.88 & 0.52 &  0.59 & 0.96 & 0.89 & 0.69 & 0.61 & 0.54 & 0.55 & 0.67 & 0.60 \\ 
    
     &\textit{Llava-Next-Video} & 0.50 & 0.48 & 0.50 & 0.52 & 0.50 & 0.51 & 0.45 & 0.48 & 0.51 & 0.52 &0.51 & 0.49 & 0.51 & 0.48  \\ 
     
    &\textit{Llava-oneVision} & 0.59 & 0.52 & 0.57 &  0.48 & 0.52  &  0.53 & 0.97 & 0.92 & 0.61 & 0.38 & 0.50 & 0.52 & 0.60 & 0.51  \\ 
   
    &\textit{Qwen-2.5-VL} &  0. & 0.  & 0.  & 0. & 0. &  0. & 0.  &  0. & 0. &  0. & 0. & 0. & 0. & 0. \\  
    \midrule
  \end{tabular}%
  }
  \caption{\label{Table:model_results_baselines} Model accuracy results for different visual conditions.}
\end{table*}
    
\end{comment}








% \begin{table*}
%   \centering
%   \scriptsize
%   %\resizebox{\textwidth}{!}{%

%   %\renewcommand{\arraystretch}{1.2}
%   %\setlength{\tabcolsep}{4pt}
%   \begin{tabular}{l|ccccccc}
%     \toprule
%     & Black video & 1 Frame & Most probable & \textit{InternVL2} & \textit{LLava-NeXT-Video} & \textit{LLava-oneVision} & \textit{Qwen-2.5-VL} \\
%     \midrule
%     \textbf{Avg. Acc} & & & 0.56 & 0.79 & 0.52\footnotemark[1] & 0.81 & \textbf{0.84} \\
%     \midrule
%     \textbf{Causal Explicit} & & & 0.43 & 0.82 & 0.54 & 0.86 & \textbf{0.89} \\
%     \textbf{Causal Implicit} & & & 0.48 & 0.84 & 0.57 & 0.88 & \textbf{0.90} \\
%         \midrule
%     \textbf{Counterfactual} & & & 0.73 & \textbf{0.85} & 0.57 & 0.78 & 0.80 \\
%         \midrule
%     \textbf{Implicit Partial} & & & 0.50 & 0.75 & 0.52 & 0.80 & \textbf{0.82} \\
%     \textbf{Implicit Total} & & & 0.51 & 0.83 & 0.59 & 0.85 & \textbf{0.88} \\
%     \midrule
%     \textbf{Uncertainty} & & & 0.79 & 0.84 & 0.32 & 0.85 & \textbf{0.92} \\
%         \midrule
%     \textbf{Out-of-Scope} & & & 0.55 & 0.77 & 0.42 & 0.88 & \textbf{0.89} \\
%         \midrule
%     \textbf{Planning} & & & 0.68 & 0.75 & 0.57 & 0.76 & \textbf{0.78} \\
%         \midrule
%     \textbf{Sentiment} & & & 0.53 & 0.84 & 0.61 & \textbf{0.88} & 0.86 \\
%     \midrule
%     \textbf{Spatial Partial} & & & 0.48 & 0.69 & 0.52 & 0.71 & \textbf{0.73} \\
%     \textbf{Spatial Total} & & & 0.51 & 0.86 & 0.56 & 0.87 & \textbf{0.90} \\
%     \midrule
%     \textbf{Temporal Duration} & & & 0.53 & 0.66 & 0.51 & 0.65 & \textbf{0.75} \\
%     \textbf{Temporal Partial} & & & 0.48 & 0.76 & 0.48 & 0.80 & \textbf{0.81} \\
%     \midrule
%     \textbf{Corr. w/ Most probable} & & & // & 0.229 & -0.436 & -0.068 & 0.030 \\
%     \bottomrule
%   \end{tabular}%
%   %}
%   \caption{\label{Table: Result_task1_transposed}
%     VLM accuracy on the MAIA benchmark for  Visual Statement Verification (Task 1).
%   }
% \end{table*}
% \footnotetext[1]{For this model, 193 cases of out-of-domain responses (i.e., neither A nor B) were recorded.}

\begin{comment}
    
\begin{table*}
  \centering
  \scriptsize
  %\resizebox{\textwidth}{!}{%

  %\renewcommand{\arraystretch}{1.2}
  %\setlength{\tabcolsep}{4pt}
  \begin{tabular}{lc|ccc||cccc}
    \toprule
    & & Black video & 1 Frame & Most probable & \textit{InternVL2} & \textit{LLava-NeXT-Video} & \textit{LLava-oneVision} & \textit{Qwen-2.5-VL} \\
    \midrule
    \midrule
    \textbf{Avg. Acc} & & & & 0.56 & 0.79 & 0.52\footnotemark[1] & 0.81 & \textbf{0.84} \\
    \midrule
    \multirow{2}{*}{\textbf{Causal}} &\textit{Explicit} & & & 0.43 & 0.82 & 0.54 & 0.86 & \textbf{0.89} \\
    & \textit{Implicit} & & & 0.48 & 0.84 & 0.57 & 0.88 & \textbf{0.90} \\
        \midrule
    \textbf{Counterfactual}&  & & & 0.73 & \textbf{0.85} & 0.57 & 0.78 & 0.80 \\
        \midrule
    \multirow{2}{*}{\textbf{Implicit}}& \textit{Partial} & & & 0.50 & 0.75 & 0.52 & 0.80 & \textbf{0.82} \\
    & \textit{Total} & & & 0.51 & 0.83 & 0.59 & 0.85 & \textbf{0.88} \\
    \midrule
    \textbf{Uncertainty}&  & & & 0.79 & 0.84 & 0.32 & 0.85 & \textbf{0.92} \\
        \midrule
    \textbf{Out-of-Scope} & & & & 0.55 & 0.77 & 0.42 & 0.88 & \textbf{0.89} \\
        \midrule
    \textbf{Planning}&  & & & 0.68 & 0.75 & 0.57 & 0.76 & \textbf{0.78} \\
        \midrule
    \textbf{Sentiment}&  & & & 0.53 & 0.84 & 0.61 & \textbf{0.88} & 0.86 \\
    \midrule
    \multirow{2}{*}{\textbf{Spatial}} & \textit{Partial} & & & 0.48 & 0.69 & 0.52 & 0.71 & \textbf{0.73} \\
    & \textit{Total} & & & 0.51 & 0.86 & 0.56 & 0.87 & \textbf{0.90} \\
    \midrule
    \multirow{2}{*}{\textbf{Temporal}} & \textit{Duration} & & & 0.53 & 0.66 & 0.51 & 0.65 & \textbf{0.75} \\
    & \textit{Partial} & & & 0.48 & 0.76 & 0.48 & 0.80 & \textbf{0.81} \\
    \midrule
    \textbf{Corr. w/ Most probable} & & & & // & 0.229 & -0.436 & -0.068 & 0.030 \\
    \bottomrule
  \end{tabular}%
  %}
  \caption{\label{Table: Result_task1_transposed}
    VLM accuracy on the MAIA benchmark for  Visual Statement Verification (Task 1).
  }
\end{table*}
\footnotetext[1]{For this model, 193 cases of out-of-domain responses (i.e., neither A nor B) were recorded.}
\end{comment}

Table \ref{tab:task1_res} reports the VLM accuracy for the three configurations (black video, 1-frame and 32-frames) across the reasoning categories. Qwen-2.5-VL is the best performing model, with 0.84 average accuracy for 32-frames.  Llava-Next-Video achieves its highest accuracy (0.61) in the 1-frame configuration: this is explainable because the model incorporates Vicuna-7B, a LLM that was poorly pre-trained on Italian.

%\paragraph{Baselines.}
All models significantly surpass the most probable baseline (0.56 accuracy), with improvements ranging from a minimum of $23$ points (\textit{InternVL2}) to a maximum of $28$ points (\textit{Qwen2.5-VL}). This means that all models are able to take advantage of visual features in order to mitigate the distributional biases of the baseline. The 0.56 accuracy of the baseline highlights that TS and FS have similar probabilities (distribution across categories is not uniform, see Table \ref{tab:task1_res}) in MAIA.
In addition, the correlation between the 32-frames and the most probable baseline across the reasoning categories is close to zero for all models but Llava-Next-Video, which is negative: the absence of correlation means that VLMs select the correct statement independently from the probability assigned by the LLM.

%\paragraph{Reasoning categories.}
Difficult categories for all models are \textsc{causal} (average accuracy 51.5) \textsc{temporal duration} (62.75), \textsc{spatial partial} (62.16) and 
\textsc{implicit partial} (64.4). For all of them the relevant information is visible only for a fragment of the video, making them particularly challenging for VLMs.
\textsc{Counterfactual}, \textsc{Uncertainty}, and \textsc{Out-of-Scope}  show the highest  accuracy. This is in line with \cite{lei-etal-2023-revealing}, where it is shown that large pretrained VLMs can mitigate the static appearance bias of the 1-frame configuration.

%\paragraph{Model consistency.}
Table \ref{tab:task1-consistency} reports the consistency  of models across the same pool of TF-FS pairs in Task $1$. As we can see, imposing a fair and rigid evaluation regime, viz. evaluating a model positively only when consistent across datapoints (total agreement), the 1-frame bias is mitigated for almost all models. The most consistent model  is \textit{Qwen2.5-VL}, while  \textit{LLaVA-NeXT-Video} performs best when a simple majority (at least $4$/$8$ consistent decisions) is considered. 
%We also analyzed the accuracy variability across the $8$ pools. An analysis for each category reveals significant fluctuations, indicating a lack of stability in the models (See Appendix  \ref{sec:app_task1}).




%for almost every model when compared to the \textit{32-Frame} setting. This is counterintuitive but can be explained by the fact that these questions expect answers that may not be strongly related to the video content and may instead benefit from plausible responses that align just to the question content. This consideration is further supported by the \textit{Black video} setting case, which shows the same accuracy trend of the \textit{1-Frame} case for the just mentioned question categories. For all the other questions, the models with 32 frames inputs performs the strongest, followed by the \textit{1-Frame} setting and finally by the \textit{Black video}.

%This trend is consistent across all  models, except for \textit{LLaVA-NeXT-Video}, which drops even 4 points below our most probable baseline. This may be due to the llm-backbone \textit{Vicuna-7B} which has been finetuned just on English text and may have poor Italian understanding. 

%\textit{Qwen2.5-VL} confirm itself as the best-performing model in this task, reaching accuracy levels up to $0.92$ and delivering the highest scores in almost all categories, with few exceptions in the \textit{Counterfactual} and \textit{Sentiment} categories. 

%Furthermore, a correlation analysis between the models' performance and the baseline revealed generally low correlations, indicating little dependency, except for \textit{LLaVA-NeXT-Video}, which exhibited a significant negative correlation.




\subsection{Results on Open-ended Generation}
\label{ref:Open_gen}

According to the LLM-as-a-judge metric (Table  \ref{tab:Task2_llm_as_judge}), Qwen-2.5-VL is the best performing model, with a correctness accuracy of 0.61 for 32-frame. This is significantly less than 0.84, the accuracy  on the statement verification, hinting to the fact that generating answers is more complex than selecting the correct option. On average, the four VLMs show a  34\% drop  in  answer generation  with respect to statement verification.

%\paragraph{Baselines.} 
Table \ref{tab:Task2_llm_as_judge} shows that the black video  setting performs  poorly, with  Spatial and Temporal reasoning being particularly penalized.  While the average drop between 32-frame and 1-frame is $9.7\%$, the gap is not uniformly distributed across  reasoning categories: for  \textsc{partial temporal} the \textit{Qwen2.5-VL} gap is 15  points, the same for \textsc{uncertainty}, and 20 points for \textsc{partial implicit}. As an example, the  \textsc{partial implicit} question \textit{How the man in the flannel shirt wears his hair?} is correctly answered with
 \textit{The man in the flannel shirt has shaved head.} by the 32-frames model, while the 
1-frame  answers \textit{The man in the flannel shirt keeps his hair in a braid.}, which is an  hallucination.

\begin{table}[t!]
  \centering
  \resizebox{\linewidth}{!}{%
  \renewcommand{\arraystretch}{1.2}
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{l|c|c|c|c|cc} 
    \toprule
       \textbf{Models} & ROUGE & BertScore & BLEU & METEOR & CIDEr \\ % Corretto l'intestazione
    \midrule
    \midrule
    \textit{InternVL2} & \textbf{0.61} & \textbf{0.84} & 0.38 & 0.59 & \textbf{1.18}\\ 
    \textit{LLaVa-NeXT-Video} & 0.46 & 0.79 & 0.21  & 0.45 & 0.65\\ 
    \textit{LLava-oneVision} & 0.58 & 0.83  & \textbf{0.40} & 0.55 & 1.08\\ 
    \textit{Qwen-2.5-VL} & 0.58 & 0.83 & 0.38 & \textbf{0.61} & 0.98\\ 
    \bottomrule
  \end{tabular}%
  }
  \caption{{\label{tab:similarity_metrics_vqa}}VLM performance for open-ended VQA (Task 2) according to similarity-based metrics.}
  

\end{table}

\begin{table*}[h]
%\caption{Open-ended Visual Question Answering (Task $2$). Accuracy scores obtained with LLM-as-a-judge}
%\label{tab:Task2_llm_as_judge}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lc|c|c|c|c|c|c|c|cc|cc|cc}
\toprule
 & \textbf{Model} & \textbf{Avg.} & \textbf{Causal} & \textbf{Counterfactual}  & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment}&  \textbf{Uncertainty} & \multicolumn{2}{c|}{\textbf{Implicit}} & \multicolumn{2}{c|}{\textbf{Spatial}} & \multicolumn{2}{c}{\textbf{Temporal}} \\
 &  &  &  &   &  &  &  & & \textit{Partial} & \textit{Total} & \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
 \midrule

\multirow{1}{*}{\textbf{Black video}} 
 & \textit{Qwen-2.5-VL} & 0.09 & 0.05 & \textbf{0.29} & 0.05 & 0.09&  0.14 & 0.02 & 0.02 & 0.20 & 0.01 & 0.03 & 0.10 & 0.04 \\
 \midrule
 
\multirow{1}{*}{\textbf{1-Frame}} 
 & \textit{Qwen-2.5-VL} & 0.22 & 0.25 & 0.23   & 0.25 & 0.11 & 0.40 & 0.33 & 0.16 & 0.28  & 0.06 & 0.28 & 0.17 & 0.14 \\ 
 \midrule
  \midrule
 
\multirow{1}{*}{\textbf{32-Frames}} 
 &  \textit{Qwen-2.5-VL} &  \textbf{0.31} &  \textbf{0.41} &  0.26   &  \textbf{0.31} &  \textbf{0.16} &  \textbf{0.48} &  \textbf{0.41} &  \textbf{0.31} &  \textbf{0.38} &  \textbf{0.19} &  \textbf{0.44} &  \textbf{0.22} &  \textbf{0.19} \\ 
 \bottomrule
\end{tabular}%
}
 \caption{\label{tab:tasks_consistency_threshold8}
    Aggregate accuracy on Task $1$ and Task $2$ on Qwen-2.5-VL across reasoning categories.
  }
\end{table*}


%\paragraph{Similarity-based metrics vs LLM-as-a-judge.} 
Results in Table  \ref{tab:similarity_metrics_vqa}  reveal a notable distance between  similarity-based metrics and answer correctness  with LLM-as-a-judge. \textit{InternVL2} achieves the highest scores on ROUGE ($0.61$), BERTScore ($0.84$),  and CIDEr ($1.18$), indicating that the generated responses closely match the reference answers on a surface level. However, the same model scores $0.49$ on LLM-as-a-judge, suggesting that while \textit{InternVL2} produces superficially similar responses, \textit{Qwen2.5-VL} delivers answers that are more semantically accurate. 
\textit{LLaVA-NeXT-Video} consistently underperforms across all metrics, proving its relative weakness also in this task. 

%A notable  difficulty is shown when processing answers for the category of \textit{Uncertainty} (e.g., $0.11$ the worst model and $0.55$ the best), \textit{Out-of-Scope} (e.g., $0.28$ the worst model and $0.43$ the best).



%More details on category-specific performance are in the Appendix, see Figure \ref{Table: Result_task2_app} \ref{sec:app_task2}.




\subsection{Discussion}

The experiments above show that the neat contribution of the actual information extracted from videos is unequally distributed across reasoning categories. For instance, Table \ref{tab:task1_res} reveals that for some categories (e.g., \textsc{counterfactual}, \textsc{planning}, \textsc{sentiment}, etc.) the black video setting can reach a performance close to and something better than the 32-frame one, suggesting that in these cases the task can  be tackled with the model linguistic information only. This means that, even if the question is to describe the sentiment of a person appearing in a video, the model might actually rely on information different from sentiment-related features extracted from the visual input. Instead, other kinds of reasoning (e.g., spatial one) can be performed only by accessing information available in the video. Therefore, the MAIA categories are extremely useful to factorize the contribution of the visual vs. linguistic components of VLMs, favoring a more nuanced analysis of their actual abilities.

A second noteworthy fact is the effect of the task design. Open-ended answer generation is surely harder than multiple choice selection. However, the performance drop in Table \ref{tab:Task2_llm_as_judge} is impressive. Interestingly, its magnitude is not the same for all reasoning categories. For instance, the \textsc{sentiment} and \textsc{counterfactual} classes in the 32-frame configuration are left untouched, while for other categories (e.g., \textsc{spatial}) the drop can even reach 30 points. Though this phenomenon will require further investigation, a suggestive hypothesis is that it might also be related to the different contribution of linguistic vs. visual information, since in the multiple choice task models can exploit the linguistic cues in the provided answers. Since open-ended VQA is the most natural application setting for VLMs, our results warn against focusing VLM performance estimation only on ``in vitro'' evaluations like multiple choice QA, prompting the need for more ``ecological'' designs like answer generation.   





















\begin{comment}
\begin{figure*}[h]
\small
    \centering
  \includegraphics[width=\linewidth]{media/acc_cat_all.png}
  \caption{Accuracy of 4 VLMs compared with our most-probable baseline with reference to each category in MAIA}
  \label{fig:MAIA_overview}
\end{figure*}
\end{comment}

