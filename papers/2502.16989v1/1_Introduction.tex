\section{Introduction}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.42\textwidth]{media/MAIA.flow.png}
    \caption{Structure of the MAIA benchmark. For each video there are two questions related to 12 reasoning categories. For each question there is a pool of 8 answers, which are associated to a true-false statement pair.}
    \label{fig:maia_structure}
\end{figure}

Vision and Language models entered the Computer Vision and Natural Language Processing scenes more than a decade ago pushed by theoretical (e.g.,~\citealp{baro:groun15}) and application-oriented~ (e.g.,~\citealp{bigham:vizwiz10}) motivations. Their success on combining image and text has been monitored and summarized in various surveys (e.g., from the earlier ones on Visual Question Answering (VQA)~\citet{bern:ling21} to the more recent ones focusing on Visually grounded Large Language Models (e.g.,~\citealp{caffagni-etal-2024-revolution,li:mmfm24}). Researchers have always felt the need to target Vision and Language  Models (VLMs) shortcomings, developing carefully designed benchmarks consisting of a suit of VL tasks to evaluate a variety of capabilities~\cite{kafle:chall19}.  The minimal pair task, contrasting a caption with its foil, proposed in~\citet{shekhar-etal-2017-foil} has been applied to large-scale linguistic phenomena~\cite{valse} and extended to highlight weakness of VLMs on Video QA. This trend focuses on visually grounded natural language understanding~\cite{vilma}. 

Today VLMs are trained to generate text and are known to excel at it. We argue that the evaluation of Natural Language Generation (NLG) and Natural Language Understanding (NLU) competence should always be pursued together: An agent that can answer questions about an event must understand it too. 
Moreover, success should be claimed only through demanding evaluation regimes in which consistency across answers is required.  

Previous work focused separately on either caption-foil or open-ended question answering: we propose paired caption-foil and open-ended QA as core components of our evaluation framework coupling VLMs comprehension and generation abilities. This allows us to investigate the relations between the two tasks and to better capture strengths and weaknesses of current VLMs. We propose a competence-oriented benchmark consisting of two paired tasks: \textbf{Multiple-choice visual statement verification} and \textbf{open-ended visual question answering}, with native Italian video QA with a large coverage of reasoning semantic categories/phenomena and with a pool of multiple answers for each question. Figure \ref{fig:maia_structure} visualizes the richness of MAIA interleaved data and how it captures the tied connection between NLU and NLG, two aspects   that allow for a reliable and robust evaluation, and obtain an \textit{all-in-one} Multimodal AI Assessment Italian benchmark. To the best of our knowledge, this is the first benchmark for the Italian language on videos. 


\paragraph{Contributions.} In the paper, we: (i) introduce MAIA, the first Italian benchmark designed to assess the reasoning abilities of VLMs on videos; (ii) evaluate multiple VLMs, demonstrating that while they perform well on the visual statement verification task, their effectiveness varies across different reasoning categories; (iii) show that generating open-ended answers significantly reduces performance compared to the visual statement verification task; and (iv) propose a metric that unveils models abilities by evaluating visually grounded comprehension and generation simultaneously.

\begin{comment}
Our experiments on  MAIA, show that VLMs: 
\begin{itemize}
\item  are able to avoid the trap of distributional LLM bias by grounding text on the visual input;
\item achieve high performance on challenging statement verification tasks; 
\item When the "severe" consistency metric is applied, their accuracy drops significantly
%\item  when tasked with generating open-ended responses for the same caption-foil cases, LMM performance drops down significantly.
\end{itemize}
%call for more the research trend of studying beyond factoid VideoQA to inference VideoQA, as well as towards the robustness and interoperability.
\end{comment}



%\begin{itemize}
    
   % \item Motivations
    % \begin{itemize}
 %       \item  previous work focused separately on either caption-foil or open-ended question answering: we propose paired caption-foil and open-ended QA. This allows to investigate the relations between the two tasks and to better capture strengths and weaknesses of current VLMs.
  %      \item previous work on VQA focused on generic video understanding, without capturing relevant aspects of different reasoning types.
   %      \item Peculiarities of videos with respect to images (e.g., temporal durations, cause-effect relations); 
    %     mostly Q\&A about entities, much less about events
     %    \item need for competence rather than task oriented benchmarks
     %RBL I think these motivations do not hold. There are several datasets with different types of reasoning and focusing on competence.
     %\end{itemize}
     
    
%\item Short intro to MAIA
 %   \begin{itemize}  
  %      \item \textit{all-in-one}: two paired tasks: Multiple-choice visual statement verification and open-ended visual question answering
   %     \item a competence-oriented  benchmark
    %     \item Native Italian  (no translation)
     %   \item describe Figure \ref{fig:maia_structure}
      %  \item Large coverage of reasoning semantic categories/phenomena --> challenging dataset  
    %\end{itemize}


%\item Open issues and research questions
 %   \begin{itemize}
  %       \item how do distributional biases affect VLM performance?
   %      \item how performance vary across reasoning categories?
    %     \item how the multiple choice and the answer generation tasks are related?  
    %\end{itemize}     
        
%\item  Contributions and findings: We present MAIA, a new benchmark allowing fine grained investigations on the reasoning abilities of visual language models. To the best of our knowledge, this is the first benchmark for the Italian language on videos. Our experiments on  MAIA, show that: (i) recent LMMs achieve high performance on challenging statement verification tasks; (ii) the  visual component is able to significantly mitigate distributional biases of the LLMs; (iii) when tasked with generating open-ended responses for the same caption-foil cases, LMM performance drops down significantly.
    
%\end{itemize}

