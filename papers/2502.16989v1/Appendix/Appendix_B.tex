\section{Experiments}
\label{sec:appendixB}
This appendix section will contain additional details on our experimental settings, including a description of the Vision-Language Models used, as well as graphs and tables summarizing the results for Tasks 1 and 2 of MAIA. These supplementary materials provide a more comprehensive view of our findings, which could not be included in the main paper due to space limitations.\\
Note that in contrast to the initial experiments for creating and validating the synthetic data of the MAIA dataset, where we used \textit{OpenAI}'s \textit{GPT-4o} API, the experiments on MAIA were conducted using A$100$ GPUs ($40$GB). Overall, the total computational budget was on the order of \~{$1,000$} GPU hours.

\subsection{Models tested}
\label{sec:models_app}

\paragraph{Vision-Language Models}
We benchmarked four recent VLMs,  sourced from the \textit{Hugging Face} hub, representing state-of-the-art approaches in Vision-Language tasks. Both experiments and related quantitative evaluation have been done using \textit{lmms-eval} \citep{lmmevaluation}, a framework for the evaluation of multimodal models.
\par \textbf{InternVL2} \cite{internvl}: $8$B parameter transformer-based multimodal model employing advanced cross-attention; pre-trained on large-scale image-text and video datasets for diverse multimodal tasks and  instruction-tuned. It uses \textit{InternLM2.5} as open-sourced-$7$B parameter chat model.\\ \textit{Hugging Face} model: OpenGVLab/InternVL2-8B
\par \textbf{LLaVA-NeXT-Video} \cite{llavanextvideo}: $7$B parameter model built on the LLaVA framework, optimized for video understanding with mechanisms to capture temporal dynamics; fine-tuned on video instruction data. Base LLM: \textit{Vicuna}-$7$B (v$1.5$).\\ \textit{Hugging Face} model: llava-hf/LLaVA-NeXT-Video-7B-hf
\par \textbf{LLaVa-OneVision} \cite{llavaonevisioneasyvisualtask}: $7$B parameter model that builds on the LLaVA framework with a Qwen2 LLM backbone to serve as a general-purpose vision-language assistant; pre-trained on extensive multimodal data to deliver robust cross-modal reasoning.\\ \textit{Hugging Face} model: lmms-lab/llava-onevision-qwen2-7b-ov
\par \textbf{Qwen2.5-VL} \cite{qwen2.5technicalreport}: the latest $7$B parameter VLM of the Qwen family using the \textit{Qwen}$2.5$ LLM decoder; key enhancements are related to grounding, working with longer videos and capturing events. It was pre-trained on comprehensive visual and textual datasets and fine-tuned for detailed, context-aware responses.
\\ \textit{Hugging Face} model: Qwen/Qwen2.5-VL-7B-Instruct 
\paragraph{Unimodal models.} As described in Section \ref{sec:exp_sett}, we used MAIA for benchmarking also some LLMs in a text-only evaluation.   We used five open-weight LLMs which have shown good performance on a variety of  tasks on Italian \cite{magnini2025evalitallmbenchmarkinglargelanguage}. For conducting these experiments, we used \textit{Minicons} library \cite{misra2022minicons}, a high-level wrapper around \textit{Hugging Face} for investigating predictive behavior of transformer models. Specifically, probabilities were computed adding a normalization parameter to take into account the different length of sentences in terms of tokens. Models used in the \textit{Hugging face} Hub are: \textbf{Llama-$3.1$} ($8$B-Instruct), \textbf{
LLaMAntino-$2$} ($7$B), \textbf{
LLaMAntino-$3$-ANITA} ($8$B-Instruct), \textbf{
Gemma} ($7$B) and \textbf{Qwen$2.5$} ($7$B-Instruct). 
\subsection{Task1: Visual Statement Verification}
\label{sec:app_task1}

As described in Section \ref{ref:results}, the performance of the models in this task was compared against our most probable baseline. Figure \ref{fig:MAIA_Task1_VLM_vs_LLMs_app} illustrates this relationship for each semantic category. Interestingly, text-only LLMs sometimes achieve higher accuracy in selecting the correct statement than VLMs, despite lacking visual input. For instance, in the counterfactual category the baseline values closely match those of the VLMs, while in the uncertainty category the baseline even outperforms both InternVL and LLaVA-NeXT-Video. Overall, the accuracy gap between our baseline and the VLMs is generally smaller than one might expect or theoretically anticipate. Table \ref{tab:task1_res_app} presents the results of Task $1$, providing a detailed analysis for the Causal category, distinguishing between \textit{Explicit} and \textit{Implicit}. The same analysis is conducted in Table \ref{tab:Task2_llm_as_judge_app} for Task $2$ taking into account LLM's judgements.

\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lc|c|cc|c|c|c|c|c|cc|cc|cc}
\toprule
 & \textbf{Models} & \textbf{Avg.} & \multicolumn{2}{c|}{\textbf{Causal}} & \textbf{Counterfactual}  & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment}&  \textbf{Uncertainty} & \multicolumn{2}{c|}{\textbf{Implicit}} & \multicolumn{2}{c|}{\textbf{Spatial}} & \multicolumn{2}{c}{\textbf{Temporal}} \\
 &  &  & \textit{Explicit} & \textit{Implicit} &   &  &  &  & & \textit{Partial} & \textit{Total} & \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
 \midrule

 
\textbf{Most Probable} &  {} & 0.56 & 0.43 & 0.48 & 0.73   & 0.55 & 0.68 & 0.53 & 0.79 & 0.50 & 0.51 & 0.48 & 0.51 & 0.53 & 0.48 \\ 
\midrule

\multirow{4}{*}{\textbf{Black video}} & \textit{InternVL2} & 0.68 & 0.61 & 0.67 & \textbf{0.88}   & 0.89 & 0.69 & 0.61& 0.96 & 0.52 & 0.59 & 0.54 & 0.55 & 0.67 & 0.60 \\
 & \textit{Llava-Next-Video} & 0.50 & 0.48 & 0.50 & 0.52   & 0.48 & 0.51 & 0.52 & 0.45 & 0.50 & 0.51 & 0.51 & 0.49 & 0.51 & 0.48 \\
 & \textit{Llava-oneVision} & 0.59 & 0.52 & 0.57 & 0.48   & \textbf{0.92} & 0.61 & 0.38 & 0.97 & 0.52 & 0.53 & 0.50 & 0.52 & 0.60 & 0.51 \\
 & \textit{Qwen-2.5-VL} & \textbf{0.73} & \textbf{0.66} & \textbf{0.70} & 0.86   & 0.87 & \textbf{0.75} & \textbf{0.72}& \textbf{0.98} & \textbf{0.56} & \textbf{0.62} & \textbf{0.61} & \textbf{0.65} & \textbf{0.73} & \textbf{0.70} \\
\midrule

\multirow{4}{*}{\textbf{1-Frame}} & \textit{InternVL2} & 0.75 & 0.77 & 0.82 & \textbf{0.87}   & 0.69 & 0.71 & 0.77 & 0.89 & 0.65 & 0.80 & 0.65 & 0.82 & 0.67 & 0.69 \\
 & \textit{Llava-Next-Video} & 0.61 & 0.67 & 0.70 & 0.76   & 0.49 & 0.63 & 0.75 & 0.40 & 0.59 & 0.65 & 0.59 & 0.70 & 0.56 & 0.53 \\
 & \textit{Llava-oneVision} & 0.76 & 0.75 & \textbf{0.83} & 0.78   & 0.89 & 0.74 & 0.76 & 0.91 & 0.68 & 0.81 & 0.64 & 0.81 & \textbf{0.70} & 0.67 \\
 & \textit{Qwen-2.5-VL} & \textbf{0.79} & \textbf{0.79} & \textbf{0.83} & 0.81   & \textbf{0.91} & \textbf{0.75} & \textbf{0.82}& \textbf{0.70} & \textbf{0.82}& \textbf{0.96} & \textbf{0.67} & \textbf{0.82} & 0.69 & \textbf{0.73} \\ 
 

 

 \midrule
 \midrule

 
\multirow{4}{*}{\textbf{32-Frames}} & \textit{InternVL2} & {0.79} & {0.82} &  {0.84} &  \textbf{0.85}   &  {0.77} &  {0.75} &  {0.84}&  {0.84} &  {0.75} &  {0.83} &  {0.69} &  {0.86} &  {0.66} &  {0.76} \\
 &  \textit{Llava-Next-Video} &  {0.52} &  {0.54} &  {0.57} &  {0.57}   &  {0.42} &  {0.57} &  {0.61} &  {0.32}&  {0.52} &  {0.59}&  {0.52} &  {0.56} &  {0.51} &  {0.48} \\
 &  \textit{Llava-oneVision} &  {0.81} &  {0.86} &  {0.88} &  {0.78}   &  {0.88} &  {0.76} &  \textbf{0.88}&  {0.85} &  {0.80} &  {0.85} &  {0.71} &  {0.87} &  {0.65} &  {0.80} \\
 &  \textit{Qwen-2.5-VL} &  \textbf{0.84} &  \textbf{0.89} &  \textbf{0.90} &  {0.80}   &  \textbf{0.89} &  \textbf{0.78} &  {0.86}&  \textbf{0.92} &  \textbf{0.82} &  \textbf{0.88}&  \textbf{0.83} &  \textbf{0.90} &  \textbf{0.75} &  \textbf{0.81} \\ 
 \bottomrule
\end{tabular}%
}
\caption{\label{tab:task1_res_app}
    Visual statement verification (Task $1$): accuracy of correct choices across reasoning categories.
  }
\end{table*}



\begin{figure*}[h]

    \centering
  \includegraphics[width=\linewidth]{media/acc_cat_all.png}
  \caption{Accuracy of 4 VLMs compared with our most-probable baseline with reference to each category in MAIA}
  \label{fig:MAIA_Task1_VLM_vs_LLMs_app}
\end{figure*}

\begin{table*}[]
%\caption{Open-ended Visual Question Answering (Task $2$). Accuracy scores obtained with LLM-as-a-judge}
%\label{tab:Task2_llm_as_judge}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lc|c|cc|c|c|c|c|c|cc|cc|cc}
\toprule
 & \textbf{Models} & \textbf{Avg.} & \multicolumn{2}{c|}{\textbf{Causal}} & \textbf{Counterfactual}  & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment}&  \textbf{Uncertainty} & \multicolumn{2}{c|}{\textbf{Implicit}} & \multicolumn{2}{c|}{\textbf{Spatial}} & \multicolumn{2}{c}{\textbf{Temporal}} \\
 &  &  & \textit{Explicit} & \textit{Implicit} &   &  &  &  & & \textit{Partial} & \textit{Total} & \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
 \midrule

\multirow{4}{*}{\textbf{Black video}} & \textit{InternVL2} & 0.37 & 0.41 & 0.43 & 0.60  & 0.30 & \textbf{0.68} & 0.43& 0.08 & 0.21 & 0.23  & \textbf{0.36} & \textbf{0.25} & \textbf{0.55} & \textbf{0.25} \\
 & \textit{Llava-Next-Video} & 0.27 & 0.40 & 0.47 & 0.47   & \textbf{0.30} & 0.40 & 0.33& \textbf{0.51}& 0.21 & 0.12 & 0.16 & 0.04 & 0.12 & 0.16 \\
 & \textit{Llava-oneVision} & \textbf{0.40} & \textbf{0.60} & \textbf{0.73} & 0.68   & 0.29 & 0.60 & 0.60& 0.28& \textbf{0.26} & \textbf{0.24} & \textbf{0.36} & 0.18 & 0.37 & 0.23 \\
 & \textit{Qwen-2.5-VL} & 0.35 & 0.35 & 0.37 & \textbf{0.69}   & 0.08 & 0.54 & \textbf{0.74}& 0.23& 0.24 & 0.19 & 0.30 & 0.19 & 0.41 & 0.20 \\
 \midrule
 
\multirow{4}{*}{\textbf{1-Frame}} & \textit{InternVL2} & 0.44 & 0.48 & 0.67 & 0.65   & 0.21 & 0.65 & 0.60& 0.10& 0.33 & 0.38 & 0.39 & 0.47 & 0.53 & 0.35  \\
 & \textit{Llava-Next-Video} & 0.32 & 0.24 & 0.36 & 0.56   & 0.20 & 0.46 & 0.60& 0.29& 0.18 & 0.32& 0.24 & 0.27 & 0.20 & 0.19 \\
 & \textit{Llava-oneVision} & 0.50 & \textbf{0.56} & \textbf{0.62} & \textbf{0.78}  & 0.15 & \textbf{0.66} & 0.74  & 0.37& \textbf{0.39} & \textbf{0.54}& \textbf{0.39} & 0.51 & 0.54 & 0.33 \\
 & \textit{Qwen-2.5-VL} & \textbf{0.51} & 0.50 & 0.62 & 0.76   & \textbf{0.32} & 0.60 & \textbf{0.79}& \textbf{0.40} & 0.35 & 0.50 & 0.38 & \textbf{0.53} & \textbf{0.55} & \textbf{0.38} \\ 
 \midrule
  \midrule
 
\multirow{4}{*}{\textbf{32-Frames}} & \textit{InternVL2} & 0.49 & 0.50 &  0.58 &  0.68   &  0.28 &  0.64 &  0.62&  0.11 &  0.45  &  0.48&  0.47 &  0.51 &  \textbf{0.57} &  0.46 \\
 &  \textit{Llava-Next-Video} &  0.33 &  0.33 &  0.42 &  0.38   &  0.16 &  0.42 &  0.48&  0.27 &  0.24 &  0.37 &  0.29 &  0.39 &  0.32 &  0.29 \\
 &  \textit{Llava-oneVision} &  0.53 &  0.63 &  0.71 &  0.79  &  0.11 &  \textbf{0.65} &  0.79&  0.23 &  0.55 &  0.56 &  \textbf{0.51} &  0.62 &  0.40 &  0.46 \\
 &  \textit{Qwen-2.5-VL} &  \textbf{0.61} &  \textbf{0.68} &  \textbf{0.75} &  \textbf{0.80}   &  \textbf{0.43} &  0.60 &  \textbf{0.85}&  \textbf{0.55}&  \textbf{0.55} &  \textbf{0.60} &  0.50 &  \textbf{0.70} &  0.54 &  \textbf{0.53} \\ 
 \bottomrule
\end{tabular}%
}
 \caption{\label{tab:Task2_llm_as_judge_app}
    Open-ended Visual Question Answering (Task $2$): accuracy of correct answers with LLM-as-a-judge.
  }
\end{table*}

\paragraph{Models consistency.}
In Task 1, model consistency and robustness in making uniform decisions within the same pool of True-False Statement pairs were investigated through a quantitative analysis, as outlined in Section \ref{ref:vvs}. We further examined the accuracy variability across the 8 pools, with Figure \ref{fig:model_cons_all} providing a detailed examination of trends within each category. This analysis revealed notable fluctuations in accuracy for certain categories such as Uncertainty, Spatial, and Temporal categories, pointing to a lack of stability and highlighting the complexity of tasks . For instance, some models, like \textit{LLaVA-NeXT-Video}, exhibited more significant variability, showcasing their challenge in handling these categories. Despite these fluctuations, \textit{Qwen2.5-VL} emerged as the top performer overall, maintaining the most consistent accuracy across pools.
\begin{figure*}[t]
  \centering
  % Prima riga: due immagini affiancate
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{media/internvl_cons.png}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{media/llavaNext_cons.png}
  \end{minipage}
  
  \vspace{1em} % Spazio verticale fra la prima e la seconda riga
  
  % Seconda riga: altre due immagini affiancate
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{media/llava1V_cons.png}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{media/qwen_cons.png}
  \end{minipage}
  
  \caption{Performance trends of the four models. Each plot shows the accuracy trend as a function of the 8 different pools.}
  \label{fig:model_cons_all}
\end{figure*}












% \subsection{Task2: Open-ended VQA}
% \label{sec:app_task2}

% Table results: \ref{Table: Result_task2_app}

% \begin{table*}
%   \centering
%   %\small
%   \resizebox{\textwidth}{!}{%

%   \renewcommand{\arraystretch}{1.2}
%   \setlength{\tabcolsep}{4pt}
%   \begin{tabular}{lcc|cc|c|cc|c|c|c|c|cc|cc}
%     \toprule
%        Metric & Model & Mean & \multicolumn{2}{c|}{\textbf{Causal}} & \textbf{Counterfactual} & \multicolumn{2}{c|}{\textbf{Implicit}} & \textbf{Uncertainty} & \textbf{Out-of-Scope} & \textbf{Planning} & \textbf{Sentiment} & \multicolumn{2}{c}{\textbf{Spatial}} & \multicolumn{2}{|c}{\textbf{Temporal}}\\
%                & & & \textit{Explicit} & \textit{Implicit} & & \textit{Partial} & \textit{Total} & & & & &  \textit{Partial} & \textit{Total} & \textit{Duration} & \textit{Partial} \\ 
%                \midrule
%                \midrule
    
%     \multirow{4}{*}{\textbf{ROUGE}} &\textit{InternVL2} & \textbf{0.61} & 0.61  & 0.60  &  0.57 & 0.68 & 0.64 & 0.54 & 0.53 & 0.67 & 0.63 & 0.68 & 0.58 & 0.65 & 0.58 \\ 
    
%      &\textit{Llava-Next-Video} & 0.46 &0.49 & 0.46  &  0.48 &  0.45 &  0.43 &  0.38 & 0.47 & 0.51 & 0.43 &  0.52 & 0.44 & 0.44 & 0.44 \\ 
     
%     &\textit{Llava-oneVision} & 0.58 & 0.54  & 0.48  & 0.60 & 0.65 &  0.60 & 0.51 & 0.45 & 0.63 & 0.61 & 0.62 & 0.60 &  0.66 & 0.52 \\ 
   
%     &\textit{Qwen-2.5-VL} & 0.58 & 0.54 & 0.51  &  0.53 & 0.69 &  0.57 & 0.52 &  0.53 & 0.57  & 0.56 &  0.68 &  0.58 &  0.66 & 0.52 \\  
%     \midrule
%      \multirow{4}{*}{\textbf{BertScore}} &\textit{InternVL2} & \textbf{0.84} & 0.84 & 0.83 & 0.83 & 0.86 &  0.85 & 0.78 & 0.81 & 0.85 & 0.83 & 0.87 & 0.85 & 0.87 & 0.84 \\ 
    
%      &\textit{Llava-Next-Video} & 0.79 & 0.80 & 0.78 & 0.80 & 0.80 & 0.78 & 0.75 & 0.78 & 0.80 & 0.78 &0.81 & 0.80 & 0.78 & 0.79  \\ 
     
%     &\textit{Llava-oneVision} & 0.83 & 0.80 & 0.79 &  0.84 & 0.85  &  0.83 & 0.77 & 0.78 & 0.84 & 0.83 & 0.85 & 0.85 & 0.86 & 0.82  \\ 
   
%     &\textit{Qwen-2.5-VL} &  0.83 & 0.82  & 0.81  & 0.82 & 0.86 &  0.83 & 0.79  &  0.80 & 0.83 &  0.82 & 0.87 & 0.84 & 0.87 & 0.82 \\  
%     \midrule

%     \multirow{4}{*}{\textbf{BLEU}} &\textit{InternVL2} & \textbf{0.38} & 0.42 & 0.40  &  0.40 &  0.37 & 0.40 &  0.24 & 0.09 & 0.44 & 0.38 & 0.44 & 0.27 &  0.40 & 0.37 \\ 
    
%      &\textit{Llava-Next-Video} & 0.21 & 0.27 & 0.27 & 0.30 &  0.17 &  0.17 &  0.15 & 0.10 &  0.25 &  0.17 & 0.29  & 0.17 & 0.20 & 0.18  \\ 
     
%     &\textit{Llava-oneVision} & 0.40 & 0.29& 0.24 & 0.45 & 0.32 & 0.34  &  0.22 &  0.07 & 0.38 & 0.31 & 0.40 & 0.30 & 0.39 & 0.33  \\ 
   
%     &\textit{Qwen-2.5-VL} &  0.34 & 0.35  & 0.28  & 0.35 & 0.38 & 0.31 & 0.23 & 0.15 &  0.28 &  0.26 & 0.46 & 0.29 & 0.40 & 0.32\\  
%     \midrule

%     \multirow{4}{*}{\textbf{METEOR}} &\textit{InternVL2} & 0.59 & 0.59 & 0.56  & 0.60 &  0.65 & 0.62 & 0.45 & 0.49 &  0.60 & 0.66 & 0.65 &  0.56 &  0.64 &  0.56 \\ 
    
%      &\textit{Llava-Next-Video} & 0.45 & 0.48  & 0.46 &  0.56 & 0.43 & 0.43 & 0.33 & 0.44 &  0.50 &  0.50 & 0.49 & 0.44 & 0.44 & 0.43  \\ 
     
%     &\textit{Llava-oneVision} & 0.55  & 0.45  & 0.40  & 0.63 &  0.61 &  0.58 & 0.42  & 0.40 & 0.60 & 0.62 & 0.58 & 0.60 &  0.65 & 0.54  \\ 
   
%     &\textit{Qwen-2.5-VL} &  \textbf{0.61} & 0.59  & 0.56 & 0.62 & 0.69 & 0.61 &  0.52 & 0.54 &0.56 & 0.63 &  0.69 & 0.64  & 0.7 &  0.56 \\  
%     \midrule

%     \multirow{4}{*}{\textbf{CIDEr}} &\textit{InternVL2} & \textbf{1.18} & / & / & / & / & / & / & / & / & / & / & / & / & /   \\ 
    
%      &\textit{Llava-Next-Video} &  0.65 & / & / & / & / & / & / & / & / & / & / & / & / & /  \\ 
     
%     &\textit{Llava-oneVision} & 1.08 & / & / & / & / & / & / & / & / & / & / & / & / & /  \\ 
   
%     &\textit{Qwen-2.5-VL} &  0.98 & / & / & / & / & / & / & / & / & / & / & / & / & /\\  
%     \midrule

%     \multirow{4}{*}{\textbf{Entailment (NLI)}} &\textit{InternVL2} & 0.37  & 0.45 & 0.46 & 0.48 & 0.39 & 0.42 &  0.06 & 0.26 & 0.49 & 0.47 &  0.35 &  0.42 & 0.35 &  0.22 \\ 
    
%      &\textit{Llava-Next-Video} & 0.22 & 0.33  & 0.26  & 0.26  & 0.17 & 0.28 &  0.12 & 0.11 & 0.27 & 0.33 & 0.23 & 0.25 & 0.18 & 0.18  \\ 
     
%     &\textit{Llava-oneVision} & 0.38  &  0.60 &  0.55 & 0.61 & 0.44 & 0.46 &  0.05 & 0.09 & 0.44 & 0.56 & 0.39 & 0.45 & 0.18 &  0.26 \\ 
   
%     &\textit{Qwen-2.5-VL} & \textbf{0.47} &  0.57 & 0.61  &  0.61 & 0.46 & 0.52 & 0.48 & 0.35 & 0.43 &  0.68 & 0.34 & 0.49 & 0.36 & 0.31\\  
%     \midrule
%     \bottomrule
%   \end{tabular}%
%   }
%   \caption{\label{Table: Result_task2_app}
%      VLM performance  on the MAIA benchmark for open-endend VQA (Task 2). Standard metrics are compared with the Natural Language Inference (NLI) task.
%   }
% \end{table*}










