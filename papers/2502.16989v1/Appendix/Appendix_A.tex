

\section{MAIA: Benchmark Creation Process}
\label{sec:appendixA}


This appendix provides a detailed explanation of the step-by-step process involved in developing the dataset of the benchmark. Each sub-section corresponds to a specific stage, outlining the methodologies, decisions, and criteria that guided the creation of MAIA as an evaluation framework.
Figure \ref{fig:Flowchart_MAIA} illustrates the underlying logic of the evaluation framework, outlining its construction process and the tasks involved. Similarly, Table \ref{Table: MAIA} shows MAIA's dataset structure after the creation and validation process which is accurately described in the following Section. The final structure of the dataset is shown in Figure \ref{fig:example_overview_app}, which accurately provides  examples of aligned question, answer, TS, and FS. 

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{media/Worflow_tot.png}
    \caption{Workflow of the MAIA evaluation framework, integrating dataset construction with its application to the two main benchmark tasks.}
    \label{fig:Flowchart_MAIA}
\end{figure*}








\begin{table}[ht]
\centering
\resizebox{0.8\linewidth}{!}{%
  \begin{tabular}{lll}
  \toprule
    Feature & n & \\
    \midrule
     Videos & $100$ \\
     Semantic Categories & $12$\footnotemark[1] & ($9$ Macro-Cat.) \\
     Questions & $2,400$ & ($2$ x Category)\\
     8-Answers Pool & $2,400$ \\
     Answers & $19,200$ \\
     True Statements & $19,200$ \\
     False Statements & $19,200$\\
     
    \bottomrule
  \end{tabular}%
  }
  \caption{\label{Table: MAIA}
    MAIA Statistics
  }
\end{table}



\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{media/all_cat_examples_maia.png}
    \caption{Overview of  MAIA reasoning categories. For each of the 100 videos, it contains 2 questions for each of the 12 categories; for each question, it has 8 answers, and each of these answers has a corresponding True and False statement pair.}
    \label{fig:example_overview_app}
\end{figure*}



\subsection{Video Selection}
The first step in creating the benchmark involved the selection of a culturally representative dataset of $100$ short videos coming from \textit{YouTube}, each lasting approximately $30$ seconds. To reflect Italian culture authentically and to ensure richness and variety, the selection process tried to be focused on the following thematic areas:

\begin{itemize}
    \item \emph{Locations}
    \item \emph{Food}
    \item \emph{Sport}
    \item \emph{Job}
    \item \emph{Nature}
    \item \emph{Activities}
\end{itemize}

Such topics allowed us to collect a dataset showing locations (indoor and outdoor), iconic Italian cities, and daily activities such as crossing the street, enjoying breakfast at a café, cooking pasta, attending a soccer match or even important events such as Italian local festivals or typical Italian weddings. Priority was given to videos featuring people, particularly with close-up shots, to enhance visual and contextual relevance.\\
The video collection process was facilitated by an automated script that scraped videos from YouTube  matching specific keywords tied to these thematic areas. Additional filters ensured that the content came from YouTube Italy and respected the \textit{Creative Commons licensing} for the content use and reproducibility. Once the videos were identified, as many of them were too long, the most engaging $30$-second segments were extracted and cut to create a uniform set of $100$ short clips, representing the base of our Italian-native dataset.

\subsection{Semantic Categories Definition}
\label{subsec:Categories}
This step focused on establishing and refining the semantic categories and phenomena that would form the foundation for analyzing the previously selected videos. This process aimed to ensure that the benchmark comprehensively addressed a wide range of linguistic reasoning categories, providing a robust framework for evaluating vision-language models.
Each of these categories was carefully tailored to align with the cultural and contextual richness of the selected video dataset and resulted from a long period of discussion and refinements. Initial categories were proposed and systematically tested against sample videos to assess their applicability and coverage. Feedback from these tests was used to refine definitions, ensuring clarity and reducing overlap between categories. The final set of semantic categories represents a broad spectrum of linguistic and semantic reasoning phenomena, designed to challenge VLMs in interpreting the intricate interaction between visual and linguistic cues in diverse, real-world contexts. These categories form the backbone of the benchmark, allowing the models' reasoning capabilities to be extensively tested on culturally representative Italian scenes.
Next, the following paragraphs present in detail the nine final macro-categories, along with their definitions and some examples of usage. These categories and descriptions were also used to administer the \textit{Google forms} for collecting data from the annotators.
\paragraph{{\textbf{Causal}}}
This category\footnote{Note that in MAIA there are four macro-categories with two fine-grained specifications (i.e., subcategories). The only exception is the \textit{Causal} category, in which explicit and implicit items are equally represented (100 each). However, we do not consider them subcategories in the same way as the others, since in those cases, the subcategories express entirely different aspects of the same domain.} includes two subtypes: \textit{Implicit Causal} and \textit{Explicit Causal}, both aimed at reasoning about the causes or effects of events depicted in the video. Thus, it includes reasoning tasks involving both visible and inferred causal relationships, offering a comprehensive test of a model's ability to infer and describe causality within events.

\begin{itemize}
    \item \underline{Implicit Causal}: This type of question targets the inferred cause of an event, object, or human action visible in the video. The focus is on an implicit cause that cannot be directly observed but must be deduced from the effect presented in the scene. Typical responses involve a logical inference explaining the implicit cause behind the visible effect.\\
    Example: Suppose a video shows a person at home grabbing an umbrella while going out.\\
    
    \begin{quote}
        Italian:\\
Q: \textit{Per quale motivo la persona prende l'ombrello?}\\
A: \textit{Perchè potrebbe piovere fuori.}\\

    \end{quote}
    
    \begin{quote}
        English: \\
Q: \textit{Why does the person take the umbrella?}\\
A: \textit{Because it might be raining outside.}\\

    \end{quote}
In this example, the action of grabbing the umbrella is visible, but the reason (bad weather) is not explicit in the video and must be inferred.

    \item \underline{Explicit Causal}: This type of question addresses direct cause-and-effect relationships visible within the video. The focus is on identifying a specific event, object, or human action (the cause) that led to another event, situation, or state (the effect) or vice versa. Typical responses clearly describe either the cause or the effect based on what is directly observable in the video.\\
    Example: Suppose a video shows an angry person throwing a glass on the floor, which subsequently shatters.\\
    
    \begin{quote}
       Italian:\\
Q: \textit{Perchè il bicchiere si è rotto?}\\
A: \textit{Perchè la persona lo ha gettato a terra}.\\
    \end{quote}
    \begin{quote}
        English:\\
Q: \textit{Why did the glass break?}\\
A: \textit{Because the person threw it on the ground}.\\

    \end{quote}
    Here, both the cause (throwing the glass) and the effect (the glass breaking) are visible in the video and can be used to provide a direct response.



\end{itemize}



\paragraph{{\textbf{Counterfactual}}}
This category focuses on questions about hypothetical scenarios that do not actually occur in the video but could take place under specific conditions. These questions explore the consequences of an event or situation that might happen in the video if a certain condition were met. A key requirement is that the hypothetical condition must be based on entities or events visible in the video.
Consequently, this category tests a model's ability to reason about hypothetical scenarios grounded in the context of the video while deriving logical and plausible outcomes from such scenarios.\\
Example: Suppose a video shows an outdoor concert.
\begin{quote}
        Italian:\\
Q: \textit{Cosa succederebbe al concerto se arrivasse un forte temporale?}\\
A: \textit{Il concerto verrebbe interrotto all'istante.}\\

    \end{quote}
    
\begin{quote}
        English:\\
Q: \textit{What would happen to the concert if a violent thunderstorm started?}\\
A: \textit{The concert would be immediately interrupted.}\\

    \end{quote}
    
In this example, the focus of the question (the concert) is visible in the video, while the condition (a violent thunderstorm) is not. The consequence (the concert being interrupted) is not shown in the video but can be reasonably inferred.


\paragraph{{\textbf{Implicit}}}
The implicit category includes questions about entities, events, or their attributes that are not explicitly visible in the video. However, their presence or properties can be reasonably inferred from the context. This category evaluates a model’s ability to infer implicit details based on context, whether the target information was never shown or was previously visible but later obscured.

\begin{itemize}
    \item \underline{Total Implicit}:
These questions focus on entities or events that are never directly visible in the video but can be inferred from observable details. A typical answer provides the requested information based on logical inference.\\
Example: Suppose a video shows the interior of a house, and suddenly the front door opens, revealing a person soaking wet with a dripping closed umbrella.

\begin{quote}
   Italian: \\     
Q: \textit{Che tempo fa fuori?}\\
A: \textit{Piove molto forte.}\\

    \end{quote}


\begin{quote}
   English: \\     
Q: \textit{What’s the weather like outside?}\\
A: \textit{It’s raining heavily.}\\

    \end{quote}
In this case, the focus of the question (the weather outside) is not visible at any point in the video. However, details such as the wet person and dripping umbrella allow for a reasonably confident inference (heavy rain).

    \item \underline{Partial Implicit}:
These questions address entities or events that were visible earlier in the video but are no longer visible due to a shift in the scene or because they have moved out of the frame.\\
Example: Suppose a video shows a man placing a pen in a drawer and then closing it.

\begin{quote}
    Italian: \\   
Q: \textit{Dove si trova la penna?}\\
A: \textit{La penna è nel cassetto.}\\

    \end{quote}

\begin{quote}
    English: \\   
Q: \textit{Where is the pen?}\\
A: \textit{The pen is inside the drawer.}\\

    \end{quote}
In this example, the focus of the question (the pen) is no longer visible in the video. However, earlier information (the man placing the pen in the drawer) allows for a logical and confident answer (the pen is in the drawer).
\end{itemize}




\paragraph{{\textbf{Out-of-scope}}}
Such a category involves questions about entities or events that are not present in the video at all, asking for properties or details about these non-existent entities or events. A typical response to an out-of-scope question is a negation, stating that the entity or event in question is not present. This category tests the ability of a model to identify and handle irrelevant or non-existent entities within the video content, appropriately responding with a negation when the requested object or event is absent. Thus, it represents an indirect way to test the models on possible multimodal hallucinations and their tendency to be assertive in their responses.\\
Example: Suppose a video shows a dog and its owner playing in the park, but there are no cars in the scene.
\begin{quote}
        Italian:\\
Q: \textit{Di che colore è la macchina?}\\
A: \textit{Non ci sono auto (nella scena).}\\

    \end{quote}
\begin{quote}
        English:\\
Q: \textit{What color is the car?}\\
A: \textit{There is no car (in the scene).}\\

    \end{quote}
In this example, the focus of the question (the car) is not physically present in the video, nor can its presence be reasonably inferred. When trying to answer the question, no useful information about a car can be found, and the expected response would be a negation, such as "There is no car."



\paragraph{{\textbf{Planning}}}
This category involves questions that request the actions needed to achieve a specific goal related to the video. The typical response to a planning question is a sequence of actions that someone should perform, based on the situation presented in the video, in order to reach the desired outcome. Such a category assesses the model's ability to infer and plan the necessary steps to accomplish a goal based on the visual cues provided in the video.\\
Example: Suppose a video shows a dog and its owner playing with a ball in a park, and the owner throws the ball onto a bench.

\begin{quote}
        Italian:\\
Q: \textit{Cosa dovrebbe fare il cane per continuare a giocare col padrone?}\\
A: \textit{Dovrebbe correre verso la palla, saltare sulla pacchina, prendere la palla e riportarla al padrone.}\\
    \end{quote}


\begin{quote}
        English:\\
Q: \textit{What should the dog do to continue playing with its owner?}\\
A: \textit{The dog should run toward the ball, jump onto the bench, grab the ball, and bring it back to the owner.}\\
    \end{quote}
In this example, the focus of the question (the dog) is visible in the video. To answer the question, one can use the information in the video (the ball on the bench) to deduce the series of actions the dog should take (running, jumping, grabbing, and returning the ball) in order to continue the game.



\paragraph{{\textbf{Sentiment}}}
The category involves questions that focus on the sentiment, mood, attitude, or emotion displayed by one or more characters in the video (i.e., animated beings) toward other entities or events in the scene, throughout the entire video. A typical response to a sentiment question may describe a specific sentiment, attitude, or emotion, or it may reflect a neutral stance. This category represents a tool for evaluating model's ability to recognize and identify the emotional state or attitude of characters based on visual cues, reflecting their reaction or feelings toward the events and other entities in the video.\\
Example: Suppose a video shows children who appear bored at a birthday party.

\begin{quote}
        Italian:\\
Q: \textit{Che atteggiamento hanno i bimbi?}\\
A: \textit{Sono annoiati.}\\
    \end{quote}

    
\begin{quote}
        English:\\
Q: \textit{What is the attitude of the children?}\\
A: \textit{They are bored.}\\
    \end{quote}
In this example, the focus of the question (the children) is visible in the video. To answer the question, one can use the visual cues present in the video (expressions and behaviors of the children) to infer the sentiment (boredom) displayed by the characters.

\paragraph{{\textbf{Spatial}}}
Such a category involves questions related to the spatial relationships between entities, objects, or events depicted in the video. It aims at assessing the model’s ability to infer both stable and time-dependent spatial relationships, as well as the ability to determine relative positioning in space and to rely on grounding competencies.

\begin{itemize}
    \item \underline{Total Spatial}: This question asks about the position of entities in space (including their relation to other entities) that remains constant throughout the entire video, disregarding any temporal variations or minimal movements of the entity at different moments in the video. A typical response to this type of question provides specific spatial information valid for the entire duration of the video.\\
    Example: Suppose a video shows a school lesson with a teacher and students in a classroom.

    \begin{quote}
        Italian:\\
Q: \textit{Dov'è l'insegnante?}\\
A: \textit{L'insegnante è in classe.}\\
    \end{quote}

    
    \begin{quote}
        English:\\
Q: \textit{Where is the teacher?}\\
A: \textit{The teacher is in the classroom.}\\
    \end{quote}
In this example, the focus of the question (the teacher) is visible throughout the video. To answer the question, one can use visible information from the video (classroom, desk) to provide the entity’s spatial position (behind the desk) throughout the video’s duration.

    \item \underline{Partial Spatial}: This question asks about the position of entities in space, but in relation to the time and/or other events occurring in the scene. It may also request the position of one entity relative to another, with a temporal aspect taken into account. A typical response provides spatial information that is specific only to the requested time range in the video.\\
    Example: Suppose a video shows a school lesson with a teacher and students in a classroom.

    \begin{quote}
       Italian:\\
Q: \textit{Dove si trova l'insegnante all'inizio del video?}\\
A: \textit{All'inizio del video, l'insegnante è di fronte la cattedra.}\\
    \end{quote}
    
    \begin{quote}
        English:\\
Q: \textit{Where is the teacher at the beginning of the video?}\\
A: \textit{At the beginning of the video, the teacher is standing in front of the desk.}\\
    \end{quote}
In this example, the focus of the question (the teacher) is visible in the video. To answer the question, one would use the visual information visible in the specific part of the video (classroom, desk) to provide the spatial position (in front of the desk) relative to the time frame requested (at the beginning of the video).
\end{itemize}




\paragraph{{\textbf{Temporal}}}
The category includes questions that focus on temporal information. This category studies the model's ability to infer temporal relationships, sequence of events, and durations from visual content in a coherent manner.
\begin{itemize}
    \item \underline{Partial Temporal}: This question focuses on the temporal properties and relationships of events in the video. The questions may request any type of temporal information about the events or their temporal relationships, except for their duration. For example, asking when something happens or if something happens before or after another event. A typical response provides the event with the specific temporal information requested by the question.\\
    Example: Suppose a video shows a rock band concert.

    \begin{quote}
        Italian:\\
Q: \textit{Che succede dopo che il chitarrista inizia a suonare?}\\
A: \textit{Il cantante inizia a cantare.}\\
    \end{quote}

    
    \begin{quote}
        English:\\
Q: \textit{What happens after the guitarist starts playing?}\\
A: \textit{The singer starts singing.}\\
    \end{quote}
In this example, the focus of the question (what happens after the guitarist starts playing) is visible at a specific moment in the video. To answer the question, one can use the visible information in that portion of the video (the singer starts singing) to provide the event (the singer starting to sing) as a response.

    \item \underline{Duration Temporal}: This question focuses on a specific property of events in the video: their duration. A typical response provides the specific temporal information required by the question regarding the event’s duration.\\
    Example: Suppose a video shows a room with a light on and a person switching it off.

     \begin{quote}
        Italian:\\
Q: \textit{Per quanto tempo la luce rimane accesa?}\\
A: \textit{Per circa 15 secondi.}\\
    \end{quote}

    
    \begin{quote}
        English:\\
Q: \textit{How long was the light on?}\\
A: \textit{For about 15 seconds.}\\
    \end{quote}
In this example, the focus of the question (the light on) is visible in the video. To answer the question, one can use the temporal information visible in the video (the person switching the light off) to provide a duration (about 15 seconds) as response.
\end{itemize}


\paragraph{{\textbf{Uncertainty}}}
This question refers to entities and events that are part of the situation represented in the video, but the scene does not provide enough information to give a precise answer. Therefore, uncertainty questions involve a certain degree of ambiguity in the response, which cannot be fully derived from the video content. The answer may refer to a range of values, state that a precise answer cannot be given, or mention that the answer is a guess and might not be correct. This category tests the model's ability to recognise and deal with situations in which the available information is insufficient or ambiguous, leading to a response that reflects the uncertainty of the scene and indirectly testing the hypothetical assertive behaviour of such models in answering.\\
Example: Suppose a video shows a dog.


\begin{quote}
        Italian:\\
Q: \textit{Quanti anni ha il cane?}\\
A: \textit{Difficile da dire. / Il cane è probabilmente giovane, ma non si può esserne certi.}\\
    \end{quote}

    \begin{quote}
        English:\\
Q: \textit{How old is the dog?}\\
A: \textit{It's hard to say. / The dog is probably young, but it's not certain.}\\
    \end{quote}
In this example, the focus of the question (the dog) is visible in the video. However, if one tries to answer the question, only partial information about the dog’s age is available in the video. As a result, an uncertain answer (e.g., “It's difficult to tell”) is expected.

\subsection{Data collection}
\label{subsec:Datacoll}
This section outlines the data collection process and the methodologies adopted for this step of the research which plays a crucial role in the development of this evaluation framework.
This section is divided into two main parts: Questions Collection, which describe the procedure for creating relevant questions based on video content, and Answers Collection, which tells how annotators provided diverse responses to those questions.
\paragraph{Questions Collection}
After defining the Semantic Categories on which we based the subsequent analysis, we created $12$ different sets of guidelines (one for each subcategory\footnote{The original subcategories were $13$, but due to the difficulty of the causal category, we decided to merge the category of explicit and implicit causals into one group. The category was then assigned to a student who annotated the first $50$ videos with explicit causals and the last $50$ with implicit causals to allow the subcategories to be balanced within the same group.}) that were administered to the same number of annotators via \textit{Google Forms}.
\begin{figure*}[h]
    \centering
  \includegraphics[width=\linewidth]{media/Esempio_Form_Questions.png}
  \caption{Example of a \textit{Google Form} used for collecting 2 Questions for each video for each of the assigned category}
  \label{fig:esempio_form_questions}
\end{figure*}
\noindent All the selected annotators were PhD students under $30$ with specializations in Linguistics and Computational Linguistics. They were paid with $100$ euros for the generation task, which was executed in multiple stages through the administration of $1,200$ different forms ($10$ forms per annotator). After carefully reading the rules and instructions in the form, their task was to watch the video as much as they wanted, and to compulsorily generate $2$ questions (i.e., Question\_A and Question\_B) for each of the $100$ videos in the dataset, as shown in Figure \ref{fig:esempio_form_questions} . To ensure variability between the pair of questions about that video, in the second question, the annotators were asked to try as far as possible to change the entities and/or events involved in the first one previously formulated. The final result corresponds to a total of $2,400$ questions ($24$ per video). Figure \ref{fig:Video+Q} shows an example of a video with $12$ out of $24$ questions for each semantic category.\\
\begin{figure*}[h]
    \centering
  \includegraphics[width=\linewidth]{media/12Q.png}
  \caption{Example of a video with its $12$ Questions, each one for a sub-category.}
  \label{fig:Video+Q}
\end{figure*}
Each form provided contained both the definition of the assigned semantic category with examples (See section \ref{subsec:Categories}), and also general rules to be followed. Each question had to be generated naturally and as an open-ended question. Questions involving a ‘Yes/No’ answer (e.g. ‘Is there a car in the video?’) were not allowed. Finally, for the correct execution of the task, the audio of the video had to be ignored, as the Vision Language Models to be tested could only work on the visual part.\\
Subsequently, these questions were manually reviewed to ensure quality and category alignment. Thus, while verifying whether the questions generated could actually belong to the specific category and whether they complied with the rules presented in the forms, one third of the generated questions were modified or refined.

\paragraph{Answers Collection}
Once the questions were obtained and checked, they were used to set up the answer collection process through Prolific, a platform designed to help researchers recruit participants for high-quality data collection. To follow the initial goal of creating an Italian-native resource that accurately represents Italian culture, we implemented a preliminary screening process to ensure that participants would provide answers from the point of view of an Italian person. For this data collection step, we accepted only annotators aged $25$ to $80$ who were born in Italy, spoke Italian as their first language, and had spent the majority of their first 18 years of life in Italy. \\
As with the question collection step, we used \textit{Google Forms} (Figure \ref{fig:Google_form_answers}) to provide the task to the annotators that was paid £$7$ per hour. Each form included $10$ videos, and for each video, the annotators were asked to answer $12$ questions\footnote{Some annotators were given the $12$ Questions\_A for that video and others the $12$-Questions\_B group} (one per category). The forms provided detailed guidelines on how to answer and specified the criteria for an acceptable response, which was necessary for the annotators to receive payment. Each response needed to be a complete answer (i.e., not short answers with missing noun / prepositional phrases), rely only on visible or deducible information from the video, and exclude any information derived from the video's audio. Annotators were also encouraged to use their own world knowledge when interpreting the visual content.
\begin{figure*}[h]
    \centering
  \includegraphics[width=\linewidth]{media/Esempio_Form_Answers.png}
  \caption{Example of a Google Form used for collecting answers for each video}
  \label{fig:Google_form_answers}
\end{figure*}
The initial acceptability of the task was determined by electing $2$ control questions from the $12$ semantic categories: the \textit{Out-of-scope} and the \textit{Uncertainty} question. Both questions had predefined acceptable answers, which made them easy to use for this purpose: the former required the negation of what asked in the question, while the latter required an expression of uncertainty in the response. To evaluate the acceptability of these control question answers, we employed the \textit{GPT-4o} model in a few-shot prompting setting. During each annotation task, annotators worked with $10$ videos, each containing the $12$ questions,including the control questions. Annotation work was accepted only if the annotators correctly answered at least $90\%$ of the control questions (i.e., $18$ out of $20$ correct answers); otherwise, their submissions were rejected, and the task was reassigned to another annotator.\\
Our goal was to collect 8 different answers for each question to ensure not only accuracy but also variability in responses. To achieve this, we presented the same question to $8$ different annotators, collecting their individual answers. In total, $2,400$ questions were paired with 8 answers each, resulting in $19,200$ responses.\\
These responses then were features by a semi-automated review and validation process divided into two main steps, both facilitated by \textit{GPT-$4$o} with few-shot prompting. 
\begin{enumerate}
    \item \textbf{Semantic Consistency check}. Each response was evaluated for semantic consistency with the corresponding question (Figure \ref{fig:Prompt_AnswersCheck}A). In cases where inconsistencies were detected, the responses were manually reviewed to determine whether the question should be re-answered by another annotator or the response could still be accepted. Real inconsistencies were found to be minimal (i.e., fewer than $100$ out of $19,200$ responses).
    \item \textbf{Contradiction test}. By using \textit{GPT-4o} \cite{openai2024gpt4ocard}, we checked whether, within each pool of $8$ responses to the same question, any of the response contradicted the others (Figure \ref{fig:Prompt_AnswersCheck}B). Cases where contradictions were identified amounted to $234$ out of $19,200$ responses and they were manually reviewed.
\end{enumerate}

\begin{figure*}[t]
    %\centering
    \small
  \includegraphics[width=\linewidth]{media/Prompt1-1.png}
  \caption{Prompts used for Answers' semi-automatic validation. Prompts were provided with some examples (i.e., few-shot prompting) but here in the Figure only the main instructions are presented.}
  \label{fig:Prompt_AnswersCheck}
\end{figure*}


After completing these reviews, we obtained a dataset of $2,400$ questions, each paired with a pool of $8$ high-quality responses, resulting in a total of $19,200$ validated responses. %Such data serves as a robust foundation for the subsequent steps in the creation of the benchmark.\\


\subsection{Answers Post-Processing}
\label{subsec: A_post_proc}
Since the data obtained up to this point will form the solid basis for the subsequent benchmarking phases (\ref{subsec:captionGen} and \ref{subsec:FoilGen}), we decided to set up a post-processing phase to assess and check that a certain degree of variability was ensured within each of the $2,400$ $8$-Answers pool.
This work was carried out in several incremental stages, each of which involved an initial analysis of the potential redundancy of responses within the pool. Each of these analyses was then followed by a specific phase of semi-automatic expansion of these responses through the introduction of lexical variability.
\begin{enumerate}
    \item \textbf{Equal strings}: a classic check to see if there were any pairs of equal answers. Table \ref{Table: Equal Strings_answers} shows the number of identified equal pairs divided by semantic category, highlighting a higher score for \textit{Out-of-scope} Answers. Based on these results, \textit{GPT-4o} was used in a Sentence Rephrasing task in a zero-shot prompting setting (Figure \ref{fig:Prompt_Loverlap}A). The only exception was the \textit{Out-of-scope} category where all 1600 responses were manually edited.
    \item \textbf{Lexical Overlap}: we tokenized each answer, checked the number of Type-Tokens shared between answer pairs within the pool of 8 and selected those that shared more than $50$\% of them. This phase was followed for these selected items by an automatic Sentence Rephrasing task in which \textit{GPT-4o} generated a new Answer with a Lexical overlap < $50$\% with reference to the other $7$ Answers in the pool (Figure \ref{fig:Prompt_Loverlap}B). Table \ref{Table: loverlap_TTR_answers} shows the lexical overlap statistics before and after reformulation. The final average lexical overlap score within a pool($18.74$ \%) represents the most balanced situation achievable. Indeed, it was noted that further attempts at lexical modifications within the pool would increase this percentage rather than make it lower. Finally, by focusing only on tokens represented by content words (i.e. adjectives, nouns, verbs and adverbs), it was noted that the lexical overlap index drops even further to $17.60$\%.
\end{enumerate}



\begin{figure*}[h]
    %\centering
    \small
  \includegraphics[width=\linewidth]{media/Prompt2-1.png}
  \caption{Prompts used for Answers' post-processing with GPT4-o   }
  \label{fig:Prompt_Loverlap}
\end{figure*}
\noindent These two checks were finally followed by an analysis performed using the \textit{Type-Token Ratio} (TTR) as a measure to quantify lexical richness (Table \ref{Table: loverlap_TTR_answers}). The final results showed that, considering only content words, only $633$/$2400$ pools had a TTR < $0.5$ and that the average TTR per pool stands at $0.55$.


\begin{table}
  \small
  \begin{tabular}{ll}
    \toprule
       Category & Equal Pairs \\ \hline
    \textit{Causal} & 21 \\
    \textit{Counterfactual} & 2 \\
    \textit{Partial Implicit} & 133 \\
    \textit{Total Implicit} & 93 \\
    \textit{Uncertainty} & 112 \\
    \textit{Out-of-scope} & 414 \\
    \textit{Planning} & 2 \\
    \textit{Sentiment} & 9 \\
    \textit{Partial Spatial} & 77 \\ 
    \textit{Total Spatial} & 101 \\
    \textit{Temporal Duration} & 91 \\
    \textit{Partial Temporal} & 9 \\
    \bottomrule
    \textbf{Tot.} & 1064 \\
     & (2128 / 19200 Answers) \\

     
  \end{tabular}
  \caption{\label{Table: Equal Strings_answers}
    Number of equal answers pairs in our \\ dataset before post-processing divided by semantic categories.
  }
\end{table}

\begin{table}
  
  \resizebox{\linewidth}{!}{%

  

  \begin{tabular}{ll|ll}
    \toprule
       & \textbf{Before Rephrasing} & \multicolumn{2}{|c}{\textbf{After Rephrasing}} \\ 
               &         &  & \textit{CW-only} \\ \midrule 
               & & & \\
    Pool Avg. Lexical Overlap & $22.95$\% & $18.74$\% & $17.60$\%\\ 
    Pool with Lexical Overlap > $50$\% & $11$ & $0$ & $0$ \\
     & & & \\
    \midrule
     & & & \\
    Avg TTR for pool &  & $0.50$ & $0.55$ \\ 
    Pool with TTR < $50$\% (Avg.)    &   & $1197$/$2400$ & $633$/$2400$ \\ 
    & & & \\
     
    \bottomrule
  \end{tabular}%
  }
  \caption{\label{Table: loverlap_TTR_answers}
    Lexical Overlap and TTR Statistics (Type-token) before and after Automatic Sentence Rephrasing with GPT-4o for $19,200$ Answers. Content-words (CW only) statistics are shown only for the After Rephrasing part.
  }
\end{table}
After these steps, Figure \ref{fig:Video+Q+8A} represents a valuable example of a high-quality question with a pool of $8$ answers in which we can find a significant level of lexical variability.
\begin{figure*}[t]
    \centering
  \includegraphics[width=\linewidth]{media/8-A-pool.png}
  \caption{Example of a video with a Questions (Implicit category) and its 8-Answers pool.}
  \label{fig:Video+Q+8A}
\end{figure*}

\subsection{True Statement Generation}
\label{subsec:captionGen}

After the collection and validation of the questions and answers, the next step involved the automatic generation of true statements (TS) which consists of descriptive declarative sentences aligned with the visual content of the videos. Just as a caption describes an image, a TS describes a video (or part of it) from different perspectives according to the specific semantic category. \\
For example, suppose a video shows a boy who is initially in a kitchen and while he is cooking he hears a loud noise and runs away to another room. An example of TS for the \textit{Spatial} category could be:
\begin{quote}
    Italian:
    \textit{Nel video il ragazzo è in cucina prima di scappare via.}
\end{quote}

\begin{quote}
    English:
    \textit{In the video the boy is in the kitchen before running away.}
\end{quote}
In contrast a \textit{Causal} TS:
\begin{quote}
    \textit{In the scene the boy runs away because of the loud noise.}
\end{quote}

To create such statements, we used, as in the previous steps, \textit{GPT-4o} (see Figure \ref{fig:Prompt_StatementGen}A for the prompt used). Using the information conveyed by the single question and its answer, we were able to combine them, generating $19,200$ true statements (TSs). As with the answers, the TS are also organised into $2,400$ pools of $8$ items. Each of these (TSs) in a pool will express the same event with different words and/or points of view. 



\subsection{True Statement post-processing}
Just like with the pools of 8 responses, we also performed checks and semi-automatic interventions to ensure lexical variability within the 2400 pools of true statements (TS).\\ The steps followed were the same, except that for the TS we used the Type-Token Ratio (TTR) as an additional tool to enhance lexical variability within the pools. First, we identified $326$ pairs of identical TS and addressed this by regenerating the sentences using \textit{GPT-4o}, as already done in \ref{subsec:A-postprocessing}, with the prompt shown in Figure \ref{fig:Prompt_Loverlap}A. Then, we analyzed the lexical overlap within the pools, focusing on statements with an overlap greater than $50$\% ($354$ cases). This reduced the average overlap percentage of a pool from $39.34$\% to $30.51$\% (Type-Token) as shown in Table \ref{Table: loverlap_TTR_Captions}.\\
\noindent As a final step, we used the TTR to increase lexical richness within individual pools. We identified pools with a TTR < $0.5$ and replaced content words with synonyms to ensure that each TS contained different words. This approach allowed us to achieve a TTR of $0.50$ ($0.55$ when considering only content words), which is an excellent result given that, unlike responses, TS have a more fixed and repetitive structure (e.g., "In the video X happens" "The video shows X" etc.).


\begin{table}
  \resizebox{\linewidth}{!}{%

  \begin{tabular}{lll|ll}
    \toprule
       & \multicolumn{2}{c}{\textbf{Before Rephrasing}} & \multicolumn{2}{|c}{\textbf{After Rephrasing}} \\ 
               & \textit{Type-Token} & \textit{CW-only}         &  \textit{Type-Token} & \textit{CW-only} \\ \midrule
               & & & \\
    Pool Avg. Lexical Overlap & $39.34$\% & $38.04$\% & $30.51$\% & $26.81$\%\\ 
    Pool with Lexical Overlap > $50$\% & $354$ & $344$ & $0$ & $0$ \\
     & & & \\
    \hline
     & & & \\
    Avg TTR for pool & $0.37$ & $0.41$ & $0.50$ & $0.55$ \\ 
    Pool with TTR < $50$\% (Avg.)  &  $2288$ & $1964$   & $1196$ & $634$ \\ 
    & & & \\
     
    \bottomrule
  \end{tabular}%
  }
  \caption{\label{Table: loverlap_TTR_Captions}
    Lexical Overlap and TTR Statistics (Type-token and Content word-only) before and after Automatic Sentence Rephrasing with \textit{GPT-4o} for 19200 TSs.
  }
\end{table}


\subsection{False Statement Generation}
\label{subsec:FoilGen}

The automatic generation of true statements represents the final phase of data creation, ensuring all elements are ready to begin the actual benchmark task. Starting from the notion of the dichotomy of Caption and Foil —where a caption is a brief and correct descriptive sentence about a scene, and a foil is a minimally altered version of the caption that turns it into an incorrect description— we developed our False Statements (FSs).\\ The final goal of this work is to create minimal pairs for each semantic category, enabling controlled experiments and precise analysis of a model's behavior with respect to the specific category.\\ As for TSs, the FSs were also automatically generated using $GPT-4o$. Figure \ref{fig:Prompt_StatementGen}B illustrates the prompt used during this phase, where the task was presented to the model using the Caption/Foil terminology rather than True/False Statements to enhance the effectiveness of the output. Given a semantic category and a caption, the task was to generate the corresponding foil by editing only the elements of the sentence related to that semantic category (e.g., If the category is Spatial, the prepositional phrase conveying spatial or location information is the block to be edited for creating the corrisponding foil).

\begin{figure*}[h]
    %\centering
    \small
  \includegraphics[width=\linewidth]{media/prompt3-2.png}
  \caption{Prompts used for True (A) and False (B) Statements generation with GPT-4o. Prompt B is representative of the 12 different prompts used to generate False Statements, each tailored to a specific semantic category.}
  \label{fig:Prompt_StatementGen}
\end{figure*}

\subsection{False Statement Validation}
\label{subsec:FoilVal}

As the last step of this work phase concerning the generation of data, we focused on the validation of the false statements we had just obtained. This review was characterised by the presence of two different semi-automatic checks based on the actual definition of foil: A description of the visual content minimally different from the caption, such that this difference falsifies its veracity. In both cases, \textit{GPT-4o} was used as a model for the implementation of the task in a semi-automatic fashion.
\begin{itemize}
    \item \textbf{Structural check}: a minimal prompt (see Figure \ref{fig:Prompt_FFsValidation}A) was constructed to check the correctness of a foil with respect to the related caption according to the specific category (e.g., if the category is Spatial, investigate whether, given a caption and a foil, the foil is correct with respect to the spatial information). Overall, $2.5$\% of the foils generated turned out to be incorrect and then manually fixed. Table \ref{Table: FFs validation output} shows for each semantic category the number of foils that resulted as incorrect according to \textit{GPT-4o}, as opposed to those that actually were not and that were then manually corrected. 
    \item \textbf{Contradiction test}: based on the notion that the correct foil must be in contradiction with the relevant caption, we then ran an NLI task to identify cases of Entailment, Neutrality and Contradiction between our pairs of True and False Statements(Figure \ref{fig:Prompt_FFsValidation}B). After a qualitative analysis of these cases, we realised that the cases identified by the model as Neutral (1287), were in fact correct and already partly defined a contradiction as in the next example.

\begin{quote}
    Italian:\\
        TS: I bambini toccano la lana per capirne \underline{la consistenza}.\\
        FS: I bambini toccano la lana per capirne \underline{il colore}.
    \end{quote}

   \begin{quote}
    English:\\
        TS: The children touch the wool to check its \underline{texture}.\\
        FS: The children touch the wool to check its \underline{colour}.
    \end{quote}


For this reason, the focus shifted only to the cases defined as Entailment ($170$). Within this group, only $93$ turned out to be true Entailments and they were subsequently manually modified. 
\end{itemize}
This last revision allow us to obtain a packet of $19,200$ high-quality FFs with respect to its relative TSs.
\begin{table}
  \resizebox{\linewidth}{!}{%

  \begin{tabular}{ll|ll}
    \toprule
       Semantic Category & FSs & \multicolumn{2}{c}{Incorrect FSs}\\
        & & \textit{GPT-4o}  & \textit{Human}\\
    \midrule

    \textit{Causal} & 1600 & 80 & 54\\
    \textit{Counterfactual} & 1600 & 61 & 15 \\
    \textit{Partial Implicit} & 1600 & 94 & 38  \\
    \textit{Total Implicit} & 1600 & 99 & 63  \\
    \textit{Uncertainty} & 1600 & 27 & 20 \\
    \textit{Out-of-scope} & 1600 & 48 & 9  \\
    \textit{Planning} & 1600 & 159 & 117 \\
    \textit{Sentiment} & 1600 & 10 & 7\\
    \textit{Partial Spatial} & 1600 & 149 & 58\\ 
    \textit{Total Spatial} & 1600 & 33 & 14\\
    \textit{Temporal Duration} & 1600 & 33 & 29 \\
    \textit{Partial Temporal} & 1600 & 71 & 55 \\
    \bottomrule
    \textbf{Tot.} & 19200 & 864 & 479 \\
    
  \end{tabular}%
  }
  \caption{\label{Table: FFs validation output} Results of Semi-automatic False Statements Validation (Structural check)}
\end{table}


 \begin{figure*}[h]
    %\centering
    \small
  \includegraphics[width=\linewidth]{media/Prompt4-2.png}
  \caption{Prompts used for False Statement Validation: Structural check (A) and Contradition Test (B)}
  \label{fig:Prompt_FFsValidation}
\end{figure*}