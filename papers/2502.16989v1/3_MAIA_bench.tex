\section{The MAIA Benchmark}
%MAIA is an evaluation framework specifically designed to assess  reasoning and grounding capabilities of VLMs in video-based contexts. To the best of our knowledge, it is the first Italian-native benchmark of its kind, providing a structured evaluation protocol and adopting a competence-oriented approach. It is built upon a carefully structured dataset (see \ref{subsec: Dataset}), a set of fine-grained semantic categories that cover different reasoning skills and levels of abstraction (see \ref{sec:semcat}) and two specular tasks with allow the investigation of models' stability and robustness. Figure \ref{fig:Flowchart_MAIA} illustrates the underlying logic of the evaluation framework, outlining its construction process and the tasks involved.

\begin{figure*}[h]
    \centering
  \includegraphics[width=\linewidth]{media/12Cat.png}
  \caption{Overview of  MAIA reasoning categories (we illustrate 9/12 categories, see the Appendix for the examples about the missing ones). For each of the 100 videos, it contains 2 questions for each of the 12 categories; for each question, it has 8 answers, and each of these answers has a corresponding True and False statement pair.}
  \label{fig:MAIA_overview}
\end{figure*}

%\todo[inline]{Davide potresti aggiungere gli esempi per avere le *12* categories?, cosi si capisce anche meglio la differenza}

MAIA (Multimodal AI Assessment) is an evaluation framework designed to assess the reasoning  capabilities of VLMs in video-based contexts. 
%To the best of our knowledge, it is the first Italian-native benchmark on videos, with a    competence-based evaluation protocol. 
%MAIA is built on a carefully designed dataset, fine-grained semantic categories covering various reasoning skills (see \ref{sec:semcat}), and two paired tasks that test model stability and robustness. 

%Figure \ref{fig:Flowchart_MAIA} outlines its structure and evaluation process.



 
% \begin{figure*}[ht!]
%     \small
%     \centering
%     \includegraphics[width=\textwidth]{media/Worflow_tot.png}
%     \caption{\todo[inline]{da sostituire con figura di Alessandro}}
%     \label{fig:Flowchart_MAIA}
% \end{figure*}



\subsection{Dataset}
\label{subsec: Dataset}
%All the steps presented within the next paragraphs led to the collection, generation, and validation of the data composing MAIA\footnote{The examples referring to the dataset presented in this section have been provided in English to facilitate readability and comprehension. For examples in Italian (with corresponding English translations), please refer to Appendix \ref{sec:appendixA}.}. The dataset  is currently structured as shown in Table \ref{Table: MAIA}. For further details check Appendix \ref{sec:appendixA}.

We outline the steps for collecting, generating, and validating the MAIA dataset. The validation step consists in a qualitative analysis and revision of the data, when necessary.\footnote{Examples in this section are in English for readability. Appendix \ref{sec:appendixA} provides more details, Italian examples, and translations.} 
%The dataset structure is shown in Figure \ref{fig:maia_structure}.

\textbf{Video Collection.} We gathered 100 short (ca. $30$s) videos from \textit{YouTube} Italy. The selection covers various aspects of Italian culture, including cities, art, food, sports, and daily activities (e.g., cooking pasta, having coffee, or watching a soccer match). Preference was given to videos featuring people and close-up shots. An automated script retrieved videos using thematic keywords and ensured \textit{Creative Commons} compliance.

%\textbf{Reasoning Categories.} We defined nine reasoning categories, and three of them  are split in two sub-categories (see Sec. %\ref{sec:semcat}) in order to test more challenging reasoning abilities.  Categories and sub-categories are balanced, i.e. we collect the %same amount of data for each sub-category as we do for categories. 


\textbf{Reasoning Categories.} We defined twelve reasoning categories aiming to capture the relation between language and vision and highlight when one of the two modalities  might be sufficient for carrying out the task, or when instead both of them are essential to succeed. 
%All categories contain the same amount of data, 
%This structure assesses linguistic and cognitive competencies in multimodal models while providing a strong basis for evaluating reasoning and grounding. 
These categories form the benchmark’s backbone, enabling thorough testing in culturally relevant Italian contexts.

\textbf{Questions and Answers Collection.} The annotation process was carried out in two phases.
In the first phase (\emph{question creation}), $12$ qualified annotators wrote $2$ open-ended questions\footnote{Yes/No and audio-based questions were prohibited.} per video for each category, ensuring diversity in entities and events.\footnote{Annotators were paid €100 for their work.} %A total of 2,400 questions were collected (24 per video), and a 
A manual review verified adherence to guidelines and semantic categories.
In the second phase (\emph{answer collection}), we used Prolific\footnote{\url{https://www.prolific.com/}} to solve the task, targeting Italian-native participants with specific cultural criteria (Appendix \ref{subsec:Datacoll}). Each annotator answered 12 out of 24 questions per video\footnote{Annotators were paid £7 per hour.}, focusing on detailed, visually grounded responses. Each question was answered by eight annotators, resulting in $19,200$ responses. 
Two semi-automatic validation checks were applied to the output: ($1$) semantic consistency with the corresponding question, and ($2$) contradiction tests within each answer pool (Appendix \ref{subsec:Datacoll}). A post-processing phase reduced redundancy and ensured lexical diversity through semi-automatic rephrasing (Appendix \ref{subsec: A_post_proc}).

\textbf{Statement Collection.} 
%In this phase, True and False Statements were generated for each question-answer pair, following a caption-foil style \citep{shekhar-etal-2017-foil, rosenberg-etal-2021-vqa, bitton-etal-2021-automatic}. Captions corrrectly describe visual content, while foils are minimally altered, incorrect descriptions. The generation of these pairs was fully automated using OpenAI's \textit{GPT-4o} model \cite{gpt4}, with few-shot prompting. The generated data underwent a semi-automatic review process, combining automatic checks with \textit{GPT-4o} and manual verification.\todo{I am confused: I understood caption=TS and foil=FS. Could someone clarify the difference?}
As shown in Figure \ref{fig:MAIA_overview}, True Statements (TSs) are descriptive declarative sentences that accurately align with the visual content of videos. Similar to captions, TSs describe videos  from different semantic perspectives, according to MAIA semantic categories. %For instance, in a video where a boy is in a kitchen, hears a loud noise, and runs away, a TS focusing on causal relationships might state: “\textit{In the scene, the boy runs away \underline{because of the loud noise}.}”. 
TSs were generated using \textit{GPT-4o} (prompts are reported in Figure \ref{fig:Prompt_StatementGen} of the Appendix) by combining the content of questions and each of the eight corresponding answers. Post-processing techniques ensured high lexical variability within each pool reducing lexical overlap. False Statements (FSs) are incorrect descriptions created by modifying elements related to a  reasoning category while maintaining the sentence structure. 
%For example, the  FS obtained from the previous example would be “\textit{In the scene, the boy runs away \underline{because of the bad smell}.}”. 
FSs were validated through two semi-automatic checks: a  check to ensure minimal modification while maintaining incorrectness for the semantic category, and a test to confirm that FSs contradict their corresponding TSs. This process produced $19,200$ high-quality FSs aligned with their corresponding TSs.
%\paragraph{a. Video Collection.} We collected a set of $100$ short videos   from \textit{YouTube} Italy, each lasting approximately 30 seconds. The selection focused on diverse themes dealing with Italian culture, such as iconic cities, art, food, sports, and daily activities (e.g., cooking pasta, enjoying coffee at a café, or attending a soccer match). To ensure relevance, priority was given to videos featuring people and close-up shots. An automated script facilitated the retrieval of videos based on thematic keywords, applying filters to ensure compliance with \textit{Creative Commons} licensing. %Longer videos were cut to extract the most engaging $30$-second segments, resulting in a diverse and visually rich dataset tailored for the benchmark.

% \paragraph{b. Reasoning Categories.}  We defined nine semantic macro-categories designed to cover a broad range of reasoning abilities (see Sec. \ref{sec:semcat}). 
% %The final framework consists of nine carefully curated macro-categories and their respective subcategories (see Section \ref{sec:semcat} for more details), ensuring a comprehensive coverage of key reasoning skills (e.g., Spatial, Causal, Temporal, etc.). 
% This structure was specifically designed to probe the linguistic and cognitive competencies underlying multimodal models while offering a robust basis for evaluating their performance in reasoning and grounding settings. At the same time, the reasoning categories form the backbone of the benchmark, as they allow models' reasoning capabilities to be extensively tested in culturally representative Italian contexts.

% \paragraph{c. Questions and Answers  Collection.} 
% %This phase represented a core moment for the entire project, as the collected data laid the foundation for building the architecture of this evaluation framework. For this reason, the data collection process was accompanied by equally crucial steps of data validation to ensure their quality and consistency.\\
%  %First step of this process began with the creation of a set of guidelines tailored to each of the twelve semantic subcategories, which were defined in alignment with the research goals. 
%  A group of twelve qualified under 30 annotators was provided with detailed instructions via \textit{Google Forms}, %tasked with generating two open-ended questions per video (trying to vary entities and events between the two) and paid €$100$.
%  and paid € $100$ for writing two open-ended questions per video, trying to vary entities and events between the two.
%  Questions requiring simple ‘Yes/No’ answers or relying on audio content were explicitly prohibited. As shown in Table \ref{Table: MAIA}, a total of $2,400$ questions were collected across all videos ($24$ per video since each of the twelve categories has $2$ different questions), and a manual review process was carried out to verify adherence to the guidelines and a correct match with semantic categories. \\
% %Approximately one-third of the questions were refined or modified during this quality assurance phase.\\
% Answers were collected through \textit{Prolific}\footnote{\url{https://www.prolific.com/}} by targeting Italian-native participants who matched specific cultural and linguistic criteria (Appendix \ref{subsec:Datacoll}). For each video, each annotator answered $12$ out the $24$ available questions, with guidelines emphasizing the need for detailed, visually grounded responses while excluding audio-based information. Every set of these $12$ questions was administered to eight different annotators, with the aim of collecting an 8-Reference answers pool. As a result, the dataset comprises $19,200$ responses. Annotators were paid £ $7$ pounds per hour for successfully completing the task.
% %To ensure response quality, some control check was used during the collection phase inside the platform itself and then these responses underwent 
% We applied two main semi-automatic validation steps: (1) a semantic consistency check with the corresponding question, and (2) a contradiction test within each pool of eight answers (Appendix \ref{subsec:Datacoll}). 
% %Minimal inconsistencies (fewer than $100$) and contradictions ($234$ cases) were manually resolved, ensuring high-quality validated data. 
% A post-processing phase was then implemented to reduce lexical redundancy and ensure lexical richness within each $8$-Answer pools, by employing semi-automatic methods for sentence rephrasing (Appendix \ref{subsec: A_post_proc}).

% \paragraph{d. Statement Collection.} In this phase, for each question-answer pair, we collect True and False Statements, based on the dichotomous concept of caption and foil \citep{shekhar-etal-2017-foil, rosenberg-etal-2021-vqa, bitton-etal-2021-automatic}. The former represents the correct description of a visual content, while the latter is a minimally altered, incorrect description. The generation of these minimal pairs was fully automatic, exploiting OpenAI's APIs, specifically the \textit{GPT-4o} model \cite{gpt4}, in few-shot prompting contexts. These synthetic data were further ensured through semi-automatic review processes, involving automatic checks with \textit{GPT-4o} followed by manual verification and final adjustments.\\
% \textbf{True Statements} (TSs) are descriptive declarative sentences that accurately align with the visual content of videos. Similar to captions describing images, TSs describe videos or specific scenes within them from different semantic perspectives (i.e., according to our semantic categories). For instance, in a video where a boy is in a kitchen, hears a loud noise, and runs away, a TS focusing on causal relationships might state: “\textit{In the scene, the boy runs away \underline{because of the loud noise}.}”. These statements were generated using \textit{GPT-4o} (prompts are reported in Figure \ref{fig:Prompt_AnswersCheck} of the Appendix) by combining the information from questions and each of the eight relative answers, resulting in $19,200$ TSs organized into $2,400$ pools of $8$ semantically similar sentences ($8$-TSs pool). Post-processing techniques ensured high lexical variability within each pool by addressing identical sentences and minimizing lexical overlap.
% \\
% \textbf{False Statements} (FSs) are false descriptions generated by modifying only the elements relevant to a specific semantic category while preserving the overall sentence structure. For instance, taking the causal example in the previous paragraph, the correspondent FS would be “\textit{In the scene, the boy runs away \underline{because of the bad smell}.}”, with the prepositional phrase conveying causality being altered to falsify the TS. To ensure the goodness of our data, we validated FSs by doing two semi-automatic checks: (1) a structural check investigating whether foils were minimally modified yet incorrect for the target semantic category, and (2) a contradiction test by verifying that FSs contradicted their corresponding TSs. This rigorous process produced $19,200$ high-quality FSs aligned with their corresponding TSs, ensuring robust data for benchmarking tasks.

% \begin{table}
% \centering
% \resizebox{0.8\linewidth}{!}{%
%   \begin{tabular}{lll}
%   \toprule
%     Feature & n & \\
%     \midrule
%      Videos & $100$ \\
%      Semantic Categories & $12$\footnotemark[1] & ($9$ Macro-Cat.) \\
%      Questions & $2,400$ & ($2$ x Category)\\
%      8-Answers Pool & $2,400$ \\
%      Answers & $19,200$ \\
%      True Statements & $19,200$ \\
%      False Statements & $19,200$\\
     
%     \bottomrule
%   \end{tabular}%
%   }
%   \caption{\label{Table: MAIA}
%     MAIA Statistics
%   }
% \end{table}

%\footnotetext[1]{Note that there are $12$ sub-categories rather than $13$, as the Causal category is equally represented by explicit and implicit specifications that are not treated as separate sub-categories. Nonetheless, we report results for both.\textcolor{red}{Can we avoid adding this info or clarify it?.}}

\subsection{Reasoning Categories}
\label{sec:semcat}
We report  the reasoning categories  in MAIA. Figure \ref{fig:MAIA_overview} provides  examples of aligned question, answer, TS, and FS. More  details in Appendix \ref{subsec:Categories}. 
%Sub-categores are highlighted in \textit{italic}.

\begin{description}[style=unboxed,leftmargin=0cm,noitemsep]
    \item[Causal.] {Focuses on questions about the cause or effect of an event. It provides a comprehensive test of a model's ability to infer and describe causality within events.  It can address either explicit (observable in the video) or implicit (inferred from the visible effect) causes/effects. }
    \item[Counterfactual.]{Focuses on hypothetical events that do not occur in the video but could happen under certain conditions. It tests a model's ability to reason about plausible scenarios grounded in the video's context.}
    \item[Implicit.] Involves questions about entities or events that are either not explicitly visible in the video (\textit{Total Implicit}) or no longer visible (\textit{Partial Implicit}), but can still be reasonably inferred. It evaluates a model's ability to deduce implicit details based on context (e.g., information never shown or previously visible and later obscured).
    \item[Out-of-scope.] Assumes the presence of entities or events not actually shown in the video, asking about properties of these nonexistent elements. It tests the model's ability to handle multimodal hallucinations and its tendency to make assertive, yet incorrect, responses.
    \item[Planning.] Inquires about the sequence of actions needed to achieve a specific goal related to the video. It assesses the model's ability to infer and plan the necessary steps based on the visual cues provided in the video.
    \item[Sentiment.] Focuses on sentiment, mood, attitude, or emotions of characters towards other entities or events in the video. It evaluates the model's ability to recognize and identify the emotional cues.
    \item[Spatial.] Focuses on the location of entities in space, either applicable to the entire video (\textit{Total Spatial}) or specific moments and events (\textit{Partial Spatial}). It assesses the model’s ability to infer stable and time-dependent spatial relationships, determine relative positioning, and demonstrate grounding competencies.
    \item[Temporal.] Relies on when something happens, either in relation to other events (\textit{Partial Temporal}) or the duration of an event (\textit{Duration}). It evaluates the model's ability to infer temporal relationships, event sequences, and durations from visual content in a coherent manner.
    \item[Uncertainty.] Arises when insufficient information is provided in the video to give a precise answer. It tests the model's ability to recognize and handle situations with ambiguous or incomplete information, indirectly assessing its tendency to make assertive (rather than uncertain) responses.
\end{description}

% \noindent{Example}: A video showing someone taking an umbrella while leaving their home.\\
% Q: \textit{Why does the person take the umbrella?} \\
% A: \textit{Because the weather outside could be bad.}
%\textbf{Counterfactual}. 
% \noindent{Example}: A video showing an outdoor concert.\\
% Q: \textit{What would happen (at the concert) if a violent storm started?} \\
% A: \textit{If a violent storm started, the concert would be immediately interrupted.}



 
% \noindent{Example A}: A video showing a man putting a pen in a drawer and then closing the drawer.\\
% Q: \textit{Where is the pen?} \\
% A: \textit{The pen is inside the drawer.}\\
% \noindent{Example B}: A video showing the inside of a house, and suddenly the front door opens and a person enters, soaking wet with a closed umbrella dripping water.\\
% Q: \textit{What’s the weather like outside?} \\
% A: \textit{It's raining heavily.}
% \textbf{Out-of-scope}. These questions assume the presence of entities or events not actually shown in the video, asking about properties of these nonexistent elements. It tests the model's ability to handle multimodal hallucinations and its tendency to make assertive, yet incorrect, responses.

% \noindent{Example}: A video showing a dog and its owner playing in the park, and there is no car visible in the video.\\
% Q: \textit{What color is the car?} \\
% A: \textit{There is no car (in the scene).}
% \textbf{Planning}. These questions inquire about the sequence of actions needed to achieve a specific goal related to the video. It assesses the model's ability to infer and plan the necessary steps based on the visual cues provided in the video.

% % Q: \textit{What should the dog do to continue playing with the owner?}\\
% % A: \textit{The dog should run towards the ball, jump onto the bench, grab the ball and return it to the owner.}
% \textbf{Sentiment}. These questions focus on the sentiment, mood, attitude, or emotions of characters towards other entities or events in the video. This category evaluates the model's ability to recognize and identify the emotional state or attitude of characters based on visual cues, reflecting their reactions or feelings toward the events and entities in the video.

% % \noindent{Example}: Suppose a video shows children bored at a birthday party.\\
% % Q: \textit{What is the attitude of the children?}\\
% % A: \textit{They are bored.}
% \textbf{Spatial}. These questions focus on the location of entities in space, either applicable to the entire video (\textit{Total Spatial}) or specific moments and events (\textit{Partial Spatial}). This category assesses the model’s ability to infer stable and time-dependent spatial relationships, determine relative positioning, and demonstrate grounding competencies.

% % \noindent{Example A}:\\
% % Q: \textit{Where is the teacher?}\\
% % A: \textit{The teacher is in the classroom.}\\
% % \noindent{Example B}:\\
% % Q: \textit{Where is the teacher at the beginning of the video?}\\
% % A: \textit{At the beginning of the video, the teacher is standing in front of their desk.}
% \textbf{Temporal}. These questions focus on when something happens, either in relation to other events (\textit{Partial Temporal}) or the duration of an event (\textit{Duration}). It evaluates the model's ability to infer temporal relationships, event sequences, and durations from visual content in a coherent manner.
% % \noindent{Example A}:\\
% % Q: \textit{What happens after the guitarist starts playing?}
% % A: \textit{The singer starts singing.}\\
% % \noindent{Example B}:\\
% % Q: \textit{How long was the light on?}\\
% % A: \textit{For about 15 seconds.}
% \textbf{Uncertainty}. These questions arise when insufficient information is provided in the video to give a precise answer. This category tests the model's ability to recognize and handle situations with ambiguous or incomplete information, indirectly assessing its tendency to make assertive (rather than uncertain) responses.

% % \noindent{Example}:\\
% % Q: \textit{How old is the dog?}\\
% % A: \textit{The dog is probably young, but it is not certain.}



  



