

\section{Conclusion}

We introduce MAIA, a benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA allows to activate two aligned tasks: a visual caption-foil task, and a open-ended visual question answering task, both on the same set of video related questions.  First, we provided a in-depth analysis of the two tasks independently, finding that Open-ended VQA is more challenging than Visual Statement Verification. Then, we couple comprehension and generation in an aggregated evaluation framework, arguing that the aggregated "all-in-one" understanding\&generation task is a  challenging and natural 
setting toward a more objective VLM evaluation.  As for the future, it would be interesting  to see whether our framework promote models that undergo learning paradigms tightly integrating these two capabilities, as in~\citet{gul-artzi-2024-cogen}.


%Contrary to previous literature, we find that distributional biases do not affect the accuracy of current language visual models, meaning that the visual component is able to balance biases of the language component. 


%We could connect our work to this one by Yoav Artzi: "CoGen: Learning from Feedback with Coupled Comprehension and Generation"

%\url{https://scholar.google.com/citations?view_op=view_citation&hl=en&user=XuQW7ogAAAAJ&sortby=pubdate&citation_for_view=XuQW7ogAAAAJ:9pM33mqn1YgC}