







%\begin{table*}[ht!]
%%  \centering
%  %\small
%  \resizebox{0.6\textwidth}{!}{%

%  \renewcommand{\arraystretch}{1.2}
%  \setlength{\tabcolsep}{4pt}
%  \begin{tabular}{l|c|c|c|c}
%    \toprule
%        Case & \textbf{InternVL2} & \textbf{LLaVA-NeXT-Video} &\textbf{LLava-oneVision} & \textbf{Qwen-2.5-VL} \\
%               \midrule
%               \midrule
    
%    A: $P_{(TS)}$ > $P_{(FS)}$ + VLM chooses TS&   45\% & 29\% & 46\% & 48\%  &
%    B: $P_{(FS)}$ > $P_{(TS)}$ + VLM chooses TS&   34\% & 23\% & 35\% &36\%  &
%    C: $P_{(TS)}$ > $P_{(FS)}$ + VLM chooses FS&  11\% & 27\% & 10\% & 8\%  &
%    D: $P_{(FS)}$ > $P_{(TS)}$ + VLM chooses FS&   10\% & 21\% & 9\% & 8\%  &
%    \bottomrule
%  \end{tabular}%
%  }
%  \caption{\label{Table: VLMs Vs. LLMs}
%    Distributional biases: VLMs accuracy across LLMs' case-studies.
%  }
%\end{table*}

%\todo[inline]{Shall we remove Majority?}




\section{Aggregating Understanding and Generation}
\label{sec:Understanding-Generation}
A unique feature of MAIA is that the VSV task and the OEVQA task are paired, i.e., they depend from the same question. While in Section \ref{sec:results} we presented results of the two tasks independently, we now aggregate them in a single task, proposing an evaluation framework where both comprehension and generation abilities VLMs are considered. Specifically, we define a metric that evaluates models abilities in visually grounded comprehension and generation simultaneously.
Intuitively, given a question $q$ on a video $v$ , we want to reward the ability of a VLM to both select a correct answer $TF$ from a $TF-FS$ pair related to $q$, and to generate a correct answer $a$ to $q$. This is under the assumption that the model should leverage overlapping abilities (i.e., knowledge and reasoning) for the two aligned tasks, and that producing consistent results is a desired behavior. For instance, we ideally expect that a VLM looking at the  video in Figure \ref{fig:maia_structure} (i.e., a pizza is cooking in a wood-fired oven) is able to answer the question \textit{What is the cake made of?} both selecting the statement \textit{There is no cake in the video}, and generating an answer like \textit{I do not see any cake in the video.} or \textit{In the video there is a pizza.} 

We  introduce \textit{Aggregate Accuracy} (\textit{Agg-Acc}), in order to reward the ability of a VLM to both understand (task $1$) and generate (task $2$). \textit{Agg-Acc} is defined as follows:

\begin{comment}
\[
Agg\text{-}Acc(M, q) = 
\begin{cases} 
1 &  \, \text{for all }  TS-FS a_M = \text{$TS$}  \\
  &  and \,  a_M  = \text{ correct} \\
0 & \text{otherwise}
\end{cases}
\]
\end{comment}

\[
Agg\text{-}Acc(M, q) = 
\begin{cases} 
1 & \text{if } \forall (TS, FS) \in S_q, \\
  & a_M(TS, FS) = TS \\
  & \text{and } a_M(q) \text{ is correct} \\
0 & \text{otherwise}
\end{cases}
\]


\noindent
where $M$ is a VLM, $q$ is a question in a VQA task, $S_q$ is the set of all the true and false statement pairs defined on $q$, and $a_M$ is an answer to $q$ generated by $M$.
\textit{Agg-Acc} for a question $q$ is correct if: (i) the model selects a true statement TS for all the 8-true-false pairs for $q$; and (ii) if the answer $a$ to question $q$ generated by the model is correct. 

Table \ref{tab:tasks_consistency_threshold8} reports the \textit{Agg-Acc} for the best-performing VLM, i.e., \textit{Qwen2.5-VL}, in the three configurations. The model achieves 0.31 accuracy on  32-frames, 0.22 on 1-Frame and 0.09 on black video, showing that the aggregate task is more challenging than the single tasks, and that a single frame is not that robust. As expected, black video is close to 0 accuracy in all categories but  \textsc{counterfactual}, where reasonable answers can be produced without visual features. Achieving a consensus among the two tasks is particularly hard for \textsc{planning} (0.16), \textsc{spatial partial} (0.19), \textsc{temporal duration} (0.22) and \textsc{temporal partial} (0.19), where even with 32-frames the model struggles to exploit its visual-language knowledge.

In light of the presented results, we believe that the aggregated understanding\&generation task proposed in MAIA is the more challenging and natural setting toward a more objective VLM evaluation.







