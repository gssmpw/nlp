\section{Conclusion and Limitations}
\label{sec:conclusion}
In this paper, we presented EgoAgent, a joint predictive agent model designed to simultaneously learn to represent world observations, predict future states, and generate actions based on egocentric RGB input. EgoAgent integrates three critical abilities—perception, prediction, and interaction—within a unified transformer architecture. By aligning the representational spaces of these abilities and leveraging a teacher-student training mechanism, EgoAgent enables mutual reinforcement across tasks, facilitating improved learning and performance.

Through extensive experiments, we demonstrated that EgoAgent outperforms existing state-of-the-art methods in image classification, future state prediction, and 3D human motion prediction. However, we identify a key limitation as the sparse human skeleton representation, which excludes finger movements which are essential for precise tasks like object manipulation. Additionally, our model currently lacks long-term memory, limiting its ability to leverage historical context for tasks requiring extended temporal dependencies. Future work will explore further model improvements and scalability in more complex environments.