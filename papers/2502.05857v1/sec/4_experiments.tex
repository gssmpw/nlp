\section{Experiments}
\label{sec:exp}

\subsection{Datasets}
As humans perceive the world through the first-view perspective, to imitate the learning process of humans, we train the proposed EgoAgent on two egocentric video datasets, \emph{i.e.}, WalkingTours (WT)~\cite{venkataramanan2023imagenet} and Ego-Exo4D~\cite{grauman2024ego}. WT is a video-only dataset, which comprising approximately 1.5 million high-resolution frames captured in various cities worldwide. Ego-Exo4D has 221.26 egocentric video hours with 5035 takes, accompanied by 376K manually labeled 3D body poses and 9.2M automatically generated 3D body poses. We undistort the raw sensor data captured from fisheye cameras into pinhole cameras to eliminate the imaging projection difference between Ego-Exo4D and WT. We apply a 20-frame sliding window filter to ensure continuous pose sequences, resulting in 1,410,119 clips with synchronized egocentric videos and 3D body poses. Based on the EgoPose division in Ego-Exo4D, we reserve the validation set of Ego-Exo4D-v2\footnote{The whole Ego-Exo4D dataset is divided into Ego-Exo4D-v1 and Ego-Exo4D-v2 on the original website (https://ego-exo4d-data.org).} for evaluation and use the remaining clips for training, yielding 1,378,672 training clips and 31,447 evaluation clips.

% \noindent\textbf{Evaluation datasets}: 

\begin{table}[t]
  \centering
  \caption{$k$-NN evaluation results on ImageNet-100 and ImageNet-1K. Top1 and Top5 accuracy (\%) are reported.}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{llcccc}
    \toprule
    \multirow{2}[0]{*}{Method} & \multirow{2}[0]{*}{Training Dataset} & \multicolumn{2}{c}{ImgNet-100} & \multicolumn{2}{c}{ImgNet-1K} \\
     \cmidrule(r){3-4}   \cmidrule(r){5-6}  &       & Top1  & Top5  & Top1  & Top5 \\
          \midrule
    \multicolumn{5}{l}{\textit{Representation Models}}   \\
    \midrule
    R3M~\cite{nair2022r3m}  & Ego4D & 4.82  & 14.42 & 0.77 & 0.77 \\
    VIP~\cite{ma2022vip}   & Ego4D & 8.04  & 21.14 & 1.59 & 1.59 \\
    DINO~\cite{caron2021emerging}  & WT    & -     & -    & 31.1  & - \\
    DoRA~\cite{venkataramanan2023imagenet}  & WT   & 55.08 & 78.06 & 34.52 & 52.50 \\
    \midrule
    \multicolumn{5}{l}{\textit{World Models}}   \\
    \midrule
    OpenSora (V1.1) & Hybrid Dataset & 33.44 & 58.4  & 16.03 & 29.05 \\
    \midrule
     \multicolumn{5}{l}{\textit{Agent Models}}   \\
    \midrule   
    EgoAgent-300M & WT+Ego-Exo4D & 55.14 & 76.56 & 34.65 & 51.42 \\
    EgoAgent-1B & WT+Ego-Exo4D & \textbf{56.48} & \textbf{78.12}  & \textbf{35.84} & \textbf{53.03} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:rep}%
  \vspace{-0.5em}
\end{table}%

\subsection{Implementation Details}
% frame rates, video horizon, motion horizon, human skeleton, LLM architecture (InternLM), pretrain stage-1, dataset, loss weights ...

To alleviate the training burden, we sample one image frame every 5 frames from the video clips, while keeping all 3D body skeletons. Specifically, for each clip with 20 frames, we divide it into $T_{s}=$4 time steps and sample the first image frame from each time step. This results in every time step consisting of one image frame paired with five frames of 3D body poses. For representation learning on egocentric videos, we follow the practice in~\cite{venkataramanan2023imagenet} to avoid cropping noisy positive pairs. Following DINO~\cite{caron2021emerging}, we adopt two global crops and six local crops.

We use IntermLM as the backbone in all our experiments. Following the configurations in~\cite{guo2024data}, we train EgoAgent with two model sizes, \emph{i.e.}, InternLM-300M and InternLM-1B. We adopt the same Adam optimizer as that in IntermLM with $\beta_{1}=0.9$, $\beta_{2}=0.999$. We train EgoAgent on the datasets for 72,000 iters with a linear warm-up of 1800 iters. We set the base learning rate to $6\times 10^{-4}$ with a cosine decay scheduler. The loss weights are set as $\lambda_{rep}=2, \lambda_{pred}=1,  \lambda_{act}=3$, respectively. All models are trained from scratch using FP16 to speed up. The whole training takes 25, 60 hours in total with a batch size of 1920 on 32, 48 NVIDIA A100 GPUs for EgoAgent-300M, and EgoAgent-1B, respectively.


% Describe the implementation details.
% 第四段：为了降低计算量，每五帧取一帧
% 1. In practice, 每五帧image取一帧。
% 2. 每个time step的token segment包含一帧image tokens和五帧human action tokens。

\subsection{Represent: Image Classification}
We examine the representative capability of EgoAgent, measuring the performance of classification on ImageNet-100 and ImageNet-1K~\cite{deng2009imagenet} by $k$-NN. Concretely, we freeze the model and extract features of images from train/validation sets, then utilize a $k$-nearest neighbor classifier with $k=20$. 


% Comparison: DINO on Egovideo, DoRA
% 1. Trained on egocentric videos from scratch, our achieves the best representation. 
\vspace{1.5mm}
\noindent{\textbf{Results.}} As illustrated in Table~\ref{tab:rep}, trained on egocentric videos from scratch, our EgoAgent achieves the best representation performance. Specifically, EgoAgent-1B outperforms DoRA by \textbf{+1.40}\% and \textbf{+1.32}\% Top1 accuracy on ImageNet-100 and ImageNet-1K, respectively, demonstrating that the task of representing the world can be well integrated into learning of world models. Notable, compared with OpenSora, which adopts a much larger dataset ($\times$3 of EgoAgent training set) for training, EgoAgent-1B shows remarkable performance improvement(\textbf{+19.81}\%) on ImageNet-1K Top1 accuracy. This indicates that learning a world model on the feature space is more effective for learning powerful representations than on the latent space.
\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/pred_final.pdf}
        \vspace{-0.5em}
    \caption{Retrieval results for egocentric future state prediction. Correct retrieval images are marked with green boundaries.}
    \label{fig:pred}
\end{figure*}

\subsection{Predict: Egocentric Future State Prediction} 
To evaluate the effectiveness of EgoAgent in predicting future states of the world, we perform an egocentric video prediction task with feature retrieval. Specifically, given the egocentric frame and 3D human skeleton sequences as actions at the current time step, the query set stores the features of the egocentric frame at the next time step predicted by models, while the gallery set contains the features directly extracted from the next frames. At the time step $t$, if the predicted future state $S_{t+1}'$ in the query set can correctly retrieve scene state $S_{t+1}$ features in the gallery, we 
treat it as a successful future state prediction.
For representation models, there are no actions to modify the input image features, the predicted future state $S_{t+1}'$ is exactly the scene state $S_{t}$ at time step $t$. To avoid retrieving the exact same features, when querying with input image $I_t$, we ignore the scene state $S_{t}$ in the gallery set.  
% As the time gap between every time step is quite short, \emph{i.e.}, 5 frames (1/6 seconds), the query/gallery features of two adjacent frames will be extremely similar, we sample two frames. 
Following common practice in retrieval tasks, we use Top1 accuracy and mAP as metrics. 

\begin{table}[tbp]
  \centering
  \caption{Feature retrieval results on predicted features of the next frame and features extracted from the next frame. Averaged Top-1 accuracy and mAP on total timesteps $T_{s}=4$ are reported.}
      \resizebox{\linewidth}{!}{
    \begin{tabular}{lllcc}
    \toprule
    \multicolumn{1}{l}{Method} &       & \multicolumn{1}{l}{Training Dataset} & \multicolumn{1}{c}{Top1} & \multicolumn{1}{c}{mAP} \\
    \midrule
         \multirow{4}[0]{*}{Representation}         & VIP~\cite{ma2022vip}   & \multicolumn{1}{l}{Ego4D} &   1.67    & 6.10 \\
           & R3M~\cite{nair2022r3m}   & \multicolumn{1}{l}{Ego4D} &   24.42    & 37.26 \\
          & DINO~\cite{caron2021emerging}  & \multicolumn{1}{l}{WT} &   28.24    & 43.42 \\
     & DoRA~\cite{venkataramanan2023imagenet}  & \multicolumn{1}{l}{WT} &   30.15    &  45.01\\

          \midrule
    % \multirow{4}[0]{*}{Video prediction}      & OpenSora &       &       &  \\
    %  & OpenSora-FT &       &       &  \\
     \multirow{2}[0]{*}{Agent Models}      & EgoAgent-300M & \multicolumn{1}{l}{WT+Ego-Exo4D} &   43.01    & 58.06 \\
          & EgoAgent-1B &   \multicolumn{1}{l}{WT+Ego-Exo4D}    &   \textbf{46.43}    & \textbf{61.96} \\
          \bottomrule
    \end{tabular}%
    }
    \vspace{-0.5em}
  \label{tab:pred}%
\end{table}%

\vspace{1.5mm}
\noindent{\textbf{Results.}} We summarize the egocentric future state prediction results in Table~\ref{tab:pred}. First, 
EgoAgent outperforms representation models by a large margin. Concretely, EgoAgent-300M improves the performance of DoRA by \textbf{+12.86}\% Top1 accuracy and \textbf{+13.05}\% mAP, showing that EgoAgent predicts the future states not only based on semantic similarity. On the contrary, representation models can only retrieve future state features using semantic similarity, making it hard for them to deal with visually similar frames in egocentric videos. Second, EgoAgent-1B achieves further performance gains on EgoAgent-300M with \textbf{+3.42}\% Top1 accuracy and \textbf{+3.90}\% mAP, indicating the potential of scaling up the world models learned on feature space. 

We also visualize the retrieving results in Figure~\ref{fig:pred}. Representation methods like DoRA, which rely on only semantic similarities, always fail to retrieve future states when humans have large movements. 
% \yz{As video generation methods like OpenSora take only the past observations as input and neglect human motion, they can not adjust to quick viewpoint changes caused by head movements.} 
On the contrary, after formulating the relation between egocentric observations and human skeletons, EgoAgent can predict the correct future state features, and then retrieve the real future image.



% Dataset: Ego-Exo4D
% Method: by feature retrieval
% Comparison: DINO, DoRA, OpenSora, OpenSora (finetuned), Ours (Uncond), Ours (Grounded)
\begin{table}[tbp]
  \centering
  \caption{3D human motion prediction results on Ego-Exo4D. We provide the results of predicting human skeletons with a time gap of 1/30 and 1/10 seconds, respectively. MPJPE and MPJVE are in cm and cm/s, respectively.}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}[0]{*}{Method} & \multicolumn{2}{c}{Predict@30fps}        & \multicolumn{2}{c}{Predict@10fps}  \\
          & MPJPE$\downarrow$ & MPJVE$\downarrow$ & MPJPE$\downarrow$ & MPJVE$\downarrow$ \\
    \midrule
    \multicolumn{5}{l}{\textit{Video-based Motion Generation Models}}   \\
    \midrule
Diffusion Policy-C~\cite{chi2023diffusion} & 28.07 & 206.96   & 27.95  & 114.75 \\
    Diffusion Policy-T~\cite{chi2023diffusion} & 25.92 & 353.24   & 25.85 & 148.82 \\
    \midrule
    \multicolumn{5}{l}{\textit{Motion Prediction Models}}   \\
    \midrule
    HumanMAC~\cite{chen2023humanmac} & 19.21 & 94.22    & 17.68 & 77.43 \\
    siMLPe~\cite{guo2023back} & 13.33 & 81.94    & 12.2  & 60.65 \\
    \midrule
        \multicolumn{5}{l}{\textit{Agent Models}}   \\
    \midrule
    EgoAgent-300M & 12.92 & 82.18 & 11.89 & 59.96 \\
    EgoAgent-1B &    \textbf{12.51}   &   \textbf{81.45}    &    \textbf{11.65}   & \textbf{58.99} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:act}%
  \vspace{-0.5em}
\end{table}%


\subsection{Act: 3D Human Motion Prediction}
We compare our method with state-of-the-art video-based motion generation Diffusion Policy~\cite{chi2023diffusion} and human motion prediction models~\cite{guo2023back} using the egocentric human skeletons from Ego-Exo4D~\cite{grauman2024ego}. 
Every testing sample with time steps $T_{s}=4$ contains 4 frames of egocentric images and 20 frames of 3D human skeleton. 
To compare these two types of motion prediction models, we set the prediction targets as the last 15 frames of 3D human skeletons. 4 frames of egocentric images and the first 5 frames of 3D human skeletons are adopted as inputs for video-action models and human motion prediction models, respectively.
For EgoAgent, both egocentric images and the first 5 frames of human skeletons are given to predict future actions. Following Ego-Exo4D~\cite{grauman2024ego}, the mean per joint position error (MPJPE) in centimeters (cm) and the mean per joint velocity error (MPJVE) in centimeters per second (cm/s) for 3D joint positions and velocities are adopted as evaluation criteria.


\vspace{1.5mm}
\noindent\textbf{Results}. Quantitatively, as shown in Table~\ref{tab:act}, our EgoAgent-300M achieves state-of-the-art performance with the lowest MPJPE and competitive MPJVE. Specifically, EgoAgent-300M improves the siMLPe~\cite{guo2023back} by \textbf{-0.41} and \textbf{-0.31} MPJPE (cm), on the evaluation set of 30fps and 10 fps, respectively. When scaling up EgoAgent to 1B, the prediction errors can be further decreased. These results demonstrate that world models can also predict actions accurately when given egocentric observations and past actions.

We also qualitatively evaluate the 3D human skeleton prediction results. As illustrated in Figure~\ref{fig:3D motion}, given the starting 3D human skeleton, our EgoAgent shows relatively small errors compared with the ground truth. Compared with the video-based motion generation method Diffusion Policy~\cite{chi2023diffusion}, EgoAgent achieves better prediction accuracy on non-visible body keypoints. Specifically, on the beginning frame (see Figure~\ref{fig:3D motion} $t_2$), missing hands in the image leads to large prediction errors on Diffusion Policy, while EgoAgent learns from previous human motions and generates accurate predictions of the position of hands. Compared with motion-only motion generation methods, EgoAgent shows smaller accelerated errors. As shown in Figure~\ref{fig:3D motion} $t_3$, after predicting several frames, the predicted skeletons start to lean against the floor. On the contrary, EgoAgent can integrate the information of the visible body parts in the egocentric observation and modify the accelerated errors.

\begin{figure*}[th]
    \centering
\includegraphics[width=.9\linewidth]{figures/pose.pdf}
\vspace{-0.5em}
    \caption{Visualizations of the 3D motion prediction tasks. After observing the egocentric frames, EgoAgent can generate accurate human skeletons even though most of the 3D body points are not visible in the input frame.}
    \label{fig:3D motion}
\end{figure*}

\begin{table*}[thp]
  \centering
  \caption{Ablation of removing one task from the training and learning world in feature space. All variants are trained with 14,400 iterations.}
  \resizebox{.9\textwidth}{!}{
    \begin{tabular}{lccccccccc}
    \toprule
    \multirow{4}[0]{*}{Method} & \multicolumn{3}{c}{\multirow{2}[0]{*}{Tasks}} & \multicolumn{2}{c}{Future State Prediction} & \multicolumn{2}{c}{Representation} & \multicolumn{2}{c}{Action Prediction} \\
        \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10}  & \multicolumn{3}{c}{}  & \multicolumn{2}{c}{Ego-Exo4D} & \multicolumn{1}{c}{ImgNet-100} & \multicolumn{1}{c}{ImgNet-1K} & \multicolumn{2}{c}{Ego-Exo4D (@30fps)} \\
         
       \cmidrule(r){2-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10}  & \multicolumn{1}{c}{Represent} & \multicolumn{1}{c}{Predict} & \multicolumn{1}{c}{  Act  } & \multicolumn{1}{l}{Top1} & \multicolumn{1}{c}{mAP} & \multicolumn{1}{c}{Top1} & \multicolumn{1}{c}{Top1} & \multicolumn{1}{c}{MPJPE $\downarrow$} & \multicolumn{1}{c}{MPJVE $\downarrow$} \\
          \midrule
    Baseline &   \checkmark    & \checkmark      &   \checkmark    & \textbf{37.77}      &  \textbf{53.72}     & \textbf{41.64} & \textbf{22.28} & \textbf{14.49} & \textbf{88.61} \\
    \midrule
    (a) w/o $\mathcal{L}_{rep}$  &       &    \checkmark   &    \checkmark   &    25.90   &  40.17     & 1.00     & 0.10   & \textbf{14.49} & 88.82 \\
    (b) w/o $\mathcal{L}_{pred}$  &   \checkmark    &       &  \checkmark     &    0.01   &   0.07    & 39.12  & 20.97 & 14.70 & 89.62 \\
        (c) w/o $\mathcal{L}_{act}$  &   \checkmark    &   \checkmark    &      &   34.86    &  49.13      & 39.92 &   21.31    & 107.03   & - \\
        \midrule
           (d) Latent &       &   latent   & \checkmark     & 20.62 & 37.12 & 1.00     & 0.10   & 13.57 & 84.86 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:remove_one}%
\end{table*}%

\subsection{Ablation Study}

To demonstrate the effectiveness of special designs in EgoAgent, we conduct several ablations with a short learning schedule of 14,400 iterations using EgoAgent-300M.

\vspace{1.5mm}
\noindent\textbf{Integration of represent, predict and act.} As EgoAgent achieves promising performance in representing, predicting, and acting, we explore how these three tasks influence each other during training. We conduct leave-one-out experiments (as shown in Table~\ref{tab:remove_one}), where we remove one task from the training process each time. First, removing any task will lead to worse performances, indicating that these three tasks, as foundation tasks of human learning from the world, can contribute to each other. Specifically, on video prediction tasks, removing the representation loss leads to a significant performance decrease with \textbf{-11.87}\% Top1 and \textbf{-13.55}\% mAP, demonstrating that representation is essential for predicting future states. Correspondingly, removing the prediction supervision also leads to the representation performance decrease of \textbf{-2.52}\% and \textbf{-1.31}\% Top1 accuracy on ImageNet-100 and ImageNet-1K, respectively. This result demonstrates that the representation ability can be strengthened by predicting future states, which is similar to the process of how humans learn from the world. Second, the motion prediction task is less sensitive to the removal of the representation learning task than the future state prediction task. 
This result indicates that EgoAgent can formulate the internal causal relationship between future state prediction and action prediction tasks, bringing additional improvement when jointly learning these two tasks.

\vspace{1.5mm}
\noindent\textbf{Train world model on feature space.} To ablate the effectiveness of training world models on feature space, we adopt a pretrained VQGAN~\cite{esser2021taming} as the image tokenizer to formulate the entire task as the next-token prediction task on image and pose tokens. As shown in Table~\ref{tab:remove_one}(d), training EgoAgent on the VQGAN latent space leads to significant performance drops on both future state prediction and image classification tasks, showing the effectiveness of training a world model on the feature space. We also find that the latent EgoAgent obtains better action prediction capabilities, which might brought by the unification of task formulation. However, the performance drops on the other two tasks make training EgoAgent on the latent space not a desirable solution.

