\begin{abstract}
This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. 
Previous methods usually train separate models for these three abilities, leading to information silos among them, which prevents these abilities from learning from each other and collaborating effectively.
In this paper, we propose a joint predictive agent model, named {EgoAgent}, that simultaneously learns to represent the world, predict future states, and take reasonable actions with a single transformer. 
EgoAgent unifies the representational spaces of the three abilities by mapping them all into a sequence of continuous tokens. 
Learnable query tokens are appended to obtain current states, future states, and next actions. 
With joint supervision, our agent model establishes the internal relationship among these three abilities and effectively mimics the human inference and learning processes.
Comprehensive evaluations of EgoAgent covering image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. 
The code and trained model will be released for reproducibility. 
\end{abstract}