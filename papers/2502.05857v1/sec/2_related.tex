\section{Related Work}
\label{sec:related}

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/frame_work.pdf}
    \vspace{-1.5em}
    \caption{The overall framework of EgoAgent. EgoAgent adopts the Joint Embedding-Action-Prediction Architecture to embed input egocentric video frames and 3D human pose sequences into image tokens $i$ and action tokens $a$ with corresponding projectors respectively. Learnable action query token $q_a$ and future state query token $q_s$ are appended after the image and action tokens to stimulate EgoAgent to predict the next action $A'$ and scene state $S'$. An action prediction loss $\mathcal{L}_{act}$ is adopted to guide the training process of image-based action anticipation. Since EgoAgent predicts scene states $S'$ in an image feature space, the target future state $S$ is obtained from a momentum teacher network $\Theta_{tea}$. Features from the student and teacher branches are aligned through a state prediction loss $\mathcal{L}_{pred}$. When 3D skeleton sequences are not paired with the video, EgoAgent randomly crops different views from egocentric observations and learns representative features only with image tokens $i$ and state query tokens $q_s$ by a representation loss $\mathcal{L}_{rep}$. For easier understanding, we only illustrate the supervisions on predicting the scene state and action at time step $t$. }
    \label{fig:framework}
    \vspace{-0.5em}
\end{figure*}

\subsection{Egocentric Visual Representation Learning}
Traditional visual representation methods learn the features on large-scale curated image datasets, \emph{e.g.}, ImageNet~\cite{deng2009imagenet}, using supervised~\cite{he2016deep,khosla2020supervised,wang2022revisiting} or self-supervised methods~\cite{he2020momentum,caron2021emerging,chen2020simple}. 
As curated datasets consume lots of resources, which limits the training scale, researchers turn to more accessible data, \emph{i.e.}, egocentric videos. R3M~\cite{nair2022r3m} and VIP~\cite{ma2022vip} captured the temporal relationships among egocentric videos, learning generic representations on Ego4D~\cite{grauman2022ego4d} for robotics. Recently, Venkataramanan et al.~\cite{venkataramanan2023imagenet} proposed DoRA to learn object-level representation on their proposed egocentric video dataset, WalkingTours. Aiming to mimic the process of how humans learn from the egocentric world, we proposed a joint predictive agent model that can not only learn representative features using egocentric videos but also predict future states and generate informed actions.

\subsection{World Models}
World models aim to predict future world states based on previous observations and actions. These models can be broadly classified into two main categories: generative and predictive models.
Generative world models~\cite{ha2018world,alonso2024diffusion,xiang2024pandora} often employ an autoencoder framework, training to capture state transitions within the input image or video space.
Recently, these models have demonstrated success in applications such as autonomous driving~\cite{wang2024driving}, robotics~\cite{ha2018recurrent}, and game control~\cite{bamford2020neural}. 
Notably, GAIA-1~\cite{hu2023gaia} mapped multimodal driving signals into discrete tokens, which are then encoded into a unified representation using an autoregressive transformer.
UniSim~\cite{yang2023learning} learned a universal simulator from diverse human demonstrations for robotic manipulation with a diffusion model. 
Predictive world models~\cite{lecun2022path,assran2023self,bardes2024revisiting,garrido2024learning} focus on capturing the dependencies between two action-connected states without explicitly generating images or videos.
MC-JEPA~\cite{bardes2023mc} encompassed self-supervised learning with optical flow estimation to jointly learn content features and motion information.
SWIM~\cite{mendonca2023structured} utilized large-scale Internet human videos
to pretrain a world model to build a structured action space for robots.


\subsection{3D Human Motion Prediction}
Human motion prediction models~\cite{liu2021aggregated,xue2020location,wang2021pvred} aim to forecast future body movements given historical poses.
% For this spatiotemporal prediction problem, 
Aksan et al.~\cite{aksan2021spatio} proposed a transformer-based architecture 
% with a dual attention mechanism
to capture spatial-temporal dependencies of human motion
% , producing plausible motions across 
over short and long horizons. Cui et al.~\cite{cui2020learning} 
% focused on the inherent connectivity of human joints, introduced a generative model that 
represents the human skeleton as a dynamic graph to adaptively learn joint connection strengths for better prediction accuracy.
Other works~\cite{maeda2022motionaug,zhang2024incorporating} incorporate physics-based priors, such as inverse kinematics, to ensure the generated motions adhere to physical principles. 
Recent research has integrated multimodal cues like textual instructions~\cite{mao2022weakly}, eye gaze~\cite{zheng2022gimo}, and 3D objects~\cite{yan2024forecasting} as conditions for human motion prediction.
A growing body of works~\cite{corona2020context,hassan2021stochastic,wang2021scene} highlighted the importance of scene-awareness in this task. Notably, Cao et al.~\cite{cao2020long} proposed a three-stage framework that consists of goal prediction, path planning, and pose finalization, effectively leveraging scene context to enhance goal-oriented motion prediction.
