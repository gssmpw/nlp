\section{Introduction}
\label{sec:intro}

% core contribution:
% 1. 任务：human egocentric vision，包括为感知、预判、交互
% 2. 技术：提出了一个框架，实现了3件事同时做
% 3. evaluation and performance

% 第一段：介绍任务：人类能感知、预判、交互这个世界。
% 1. 任务定义：This paper aims to build a virtual character acting like humans, which输入egocentric RGB observations，可以同时感知、预判、交互这个世界。
% 人有一个内生的world model Human 
This paper aims to build an agent model, as defined by David Ha and Jurgen Schmidhuber~\cite{ha2018world}, inspired by the human cognition system.
% Built upon world models that focus on predicting the environment, the agent model contains an additional decision-making component, emphasizing the capability of jointly learning and acting.
Built upon world models that focus on predicting the environment, the agent model contains an additional action proposal component, emphasizing the capability to take actions in response to predicted states.
% obtaining more general capabilities through 
Motivated by the acquisition process of human commonsense, we design our agent model to take egocentric RGB observations as input and learn to perceive, predict, and interact with the world.
% 2. 任务的应用：Such ability is essential for applications in robotics, gaming, and virtual reality, where the character 需要感知世界现有状态、预判世界的dynamics，从而支撑interaction with the world.
Such abilities are essential for applications in robotics, gaming, and virtual reality, where the agent needs to understand what is happening in the world, anticipate what will happen next, and decide what actions to take to achieve its objectives.
% 3. 任务的挑战：It is challenging to 构建一个character像人类一样掌握这三个能力，考虑到两个方面。
It is challenging to build a model that can possess these three abilities like humans, considering two key aspects.
% 4. 推理方面：人类的三个能力有紧密合作关系，通过感知与预判为交互提供信息，而交互又能让character主动地获取更多信息。
First, the three abilities of humans are closely intertwined, where perception and prediction provide information for interaction, and interaction allows the model to better understand the world.
% 5. 学习方面：三项能力的学习也是紧密联系的。人类通过observation和interaction进行学习，这两个事情是紧密联系，互相促进。
Second, the learning of these three abilities is also closely related. Humans develop internal models of the world through an ongoing cycle of perception and interaction, which reinforce each other.

% 第二段：讨论related works只单独做这三件事，引出我们任务上的贡献，讨论做这个任务的好处
% 1. Previous methods typically 专注于建立三种能力中的一种，相应的典型任务为: (1) Visual representation learning, which xx; (2) World model, which xx; (3) Motion prediction, which xx.
Previous methods typically focus on building one of the three abilities, with typical vision tasks including (1) Visual representation learning~\cite{caron2021emerging,he2022masked, chen2020simple, khosla2020supervised,wang2022revisiting}, which encodes high-level states of human observations, \emph{i.e.}, images and videos; (2) Action prediction~\cite{martinez2017human,cao2020long}, which forecasts future human actions according to past actions; (3) World model~\cite{mendonca2023structured,yang2023learning}, which predict world state transitions based on observations and actions.
% 2. However, we argue that 分别学习perception, prediction, interaction这三个能力，会导致三个模型之间的信息孤岛，使得它们无法互相促进学习，也无法让它们更好地协作。
However, we argue that training separate models for perception, prediction, and interaction leads to information silos between the models, preventing them from learning from each other and collaborating effectively.
% 3. It is crucial to 建立三个能力的关联，在推理和学习上，让它们互相促进。
It is crucial to establish the relationship between these three abilities, allowing them to reinforce each other in both inference and learning processes.
% Given that world models aim to capture the relationship between \textit{observations} and \textit{actions}, we propose that these abilities can be jointly learned within a world model architecture.


% 第三段：介绍我们的技术贡献
% 1. In this paper, we propose a novel framework, named xx, for xx.
In this paper, we propose a novel joint predictive agent model in egocentric worlds, named \emph{EgoAgent}, that simultaneously learns to represent world observations, predict future states, and act based on learned representations (Figure~\ref{fig:teaser-image}).
% 2. Our core innovation lies in a single unified transformer, unify 三个能力的 representational spaces by mapping them into a sequence of continuous tokens.
Our core innovation lies in a single transformer that unifies the representational spaces of the three abilities by mapping them into a sequence of continuous tokens.
% 3. Specifically, 给定egocentric observations and history character actions, 通过projection layers把它们映射为high-dimensional feature vectors
Specifically, given egocentric observations and historical human actions, we map them into high-dimensional feature vectors using projection layers. 
% 4. 然后，这些feature vectors和一组预定义的query tokens一起feed到transformer architecture中，获得current state feature vector, future state feature vector和next actions。
Then, these feature vectors are fed into a transformer along with a set of pre-defined query tokens to obtain the current state features, future state features, and the following actions.
% 5. 使用teacher-student mechanism训练Visual representation learning和World model。
We adopt a teacher-student mechanism to train the agent model, which jointly learns visual representation extraction, future state prediction, and action prediction.


% 第四段：讨论技术优势
% 1. Our unified model brings three benefits.
Our unified agent model brings three benefits. 
% 1. thanks to the attention mechanism of the transformer, 用transformer可以建立各个任务之间的因果关系 
First, thanks to the attention mechanism of the transformer, our model naturally establishes internal relationships among the three abilities.
% 2. 将representation对齐以后，current state feature和future state feature对next action prediction的帮助也更大 
% Second, by constructing a unified representational space, the underlying representations of three abilities are implicitly aligned, enabling the perception and prediction to better facilitate the action generation.
% 3. 通过我们的模型，可以通过action训练world percetion and future prediction。maintain full parameter-sharing，unified representational spaces，互相促进学习。
Second, aligning representation space allows the transformer to be compatible with multi-modal data and maintain full parameter-sharing, increasing training samples for three sub-tasks.  
Third, our proposed model enables us to leverage the task of action generation to reinforce the learning of world perception and future prediction, effectively mimicking the human learning process.

% 第五段：实验
Extensive experiments on egocentric video and human motion dataset~\cite{grauman2024ego} demonstrate that EgoAgent can well handle tasks on perception, prediction, and action, even pushing the performance limits of the existing state-of-the-art methods. Specifically, EgoAgent outperforms the leading egocentric video pretraining method~\cite{venkataramanan2023imagenet} by \textbf{+1.40}\% and \textbf{+1.32}\% Top1 accuracy on ImageNet-100 and ImageNet-1K for the image classification task, \textbf{+16.28}\% Top1 accuracy and \textbf{+16.95}\% mAP on Ego-Exo4D~\cite{grauman2024ego} future state prediction task, respectively. Furthermore, on the 3D human motion prediction task, EgoAgent surpasses both video-based motion generation models and motion prediction methods, achieving \textbf{-0.82} MPJPE (cm) performance improvement at 30 fps prediction rate. 

% 6. Contributions:
Our contributions are two-fold: (1) We develop a joint predictive agent model in egocentric worlds named EgoAgent, which can simultaneously learn to represent the observation, predict future states, and generate informed actions.
(2) We evaluate our EgoAgent on egocentric video-action datasets and show its superior abilities in image classification, egocentric future status prediction, and 3D human motion prediction tasks.
