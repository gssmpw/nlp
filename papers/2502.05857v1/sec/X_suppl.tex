\clearpage
\pagenumbering{gobble}
\maketitlesupplementary

\section{Additional Results on Embodied Tasks}

To evaluate the broader applicability of our EgoAgent's learned representation beyond video-conditioned 3D human motion prediction, we test its ability to improve visual policy learning for embodiments other than the human skeleton.
Following the methodology in~\cite{majumdar2023we}, we conduct experiments on the TriFinger benchmark~\cite{wuthrich2020trifinger}, which involves a three-finger robot performing two tasks: reach cube and move cube. 
We freeze the pretrained representations and use a 3-layer MLP as the policy network, training each task with 100 demonstrations.

\begin{table}[h]
\centering
\caption{Success rate (\%) on the TriFinger benchmark, where each model's pretrained representation is fixed, and additional linear layers are trained as the policy network.}
\label{tab:trifinger}
\resizebox{\linewidth}{!}{%
\begin{tabular}{llcc}
\toprule
Methods       & Training Dataset & Reach Cube & Move Cube \\
\midrule
DINO~\cite{caron2021emerging}         & WT Venice        & 78.03     & 47.42     \\
DoRA~\cite{venkataramanan2023imagenet}          & WT Venice        & 81.62     & 53.76     \\
DoRA~\cite{venkataramanan2023imagenet}          & WT All           & 82.40     & 48.13     \\
\midrule
EgoAgent-300M & WT+Ego-Exo4D      & 82.61    & 54.21      \\
EgoAgent-1B   & WT+Ego-Exo4D      & \textbf{85.72}      & \textbf{57.66}   \\
\bottomrule
\end{tabular}%
}
\end{table}

As shown in Table~\ref{tab:trifinger}, EgoAgent achieves the highest success rates on both tasks, outperforming the best models from DoRA~\cite{venkataramanan2023imagenet} with increases of +3.32\% and +3.9\% respectively.
This result shows that by incorporating human action prediction into the learning process, EgoAgent demonstrates the ability to learn more effective representations that benefit both image classification and embodied manipulation tasks.
This highlights the potential of leveraging human-centric motion data to bridge the gap between visual understanding and actionable policy learning.



\section{Additional Results on Egocentric Future State Prediction}

In this section, we provide additional qualitative results on the egocentric future state prediction task. Additionally, we describe our approach to finetune video diffusion model on the Ego-Exo4D dataset~\cite{grauman2024ego} and generate future video frames conditioned on initial frames as shown in Figure~\ref{fig:opensora_finetune}.

\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{figures/opensora_finetune.pdf}
    \caption{Comparison of OpenSora V1.1 first-frame-conditioned video generation results before and after finetuning on Ego-Exo4D. Fine-tuning enhances temporal consistency, but the predicted pixel-space future states still exhibit errors, such as inaccuracies in the basketball's trajectory.}
    \label{fig:opensora_finetune}
\end{figure}

\subsection{Visualizations and Comparisons}

More visualizations of our method, DoRA, and OpenSora in different scenes (as shown in Figure~\ref{fig:supp pred}). For OpenSora, when predicting the states of $t_k$, we use all the ground truth frames from $t_{0}$ to $t_{k-1}$ as conditions. As OpenSora takes only past observations as input and neglects human motion, it performs well only when the human has relatively small motions (see top cases in Figure~\ref{fig:supp pred}), but can not adjust to large movements of the human body or quick viewpoint changes (see bottom cases in Figure~\ref{fig:supp pred}).

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/supp_pred.pdf}
    \caption{Retrieval and generation results for egocentric future state prediction. Correct and wrong retrieval images are marked with green and red boundaries, respectively.}
    \label{fig:supp pred}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/motion_prediction.pdf}
    \vspace{-0.5mm}
    \caption{Motion prediction results in scenes with minor changes in observation.}
    \vspace{-1.5mm}
    \label{fig:motion_prediction}
\end{figure*}

\subsection{Finetuning OpenSora on Ego-Exo4D}

OpenSora V1.1~\cite{opensora}, initially trained on internet videos and images, produces severely inconsistent results when directly applied to infer future videos on the Ego-Exo4D dataset, as illustrated in Figure~\ref{fig:opensora_finetune}.
To address the gap between general internet content and egocentric video data, we fine-tune the official checkpoint on the Ego-Exo4D training set for 50 epochs.
OpenSora V1.1 proposed a random mask strategy during training to enable video generation by image and video conditioning. We adopted the default masking rate, which applies: 75\% with no masking, 2.5\% with random masking of 1 frame to 1/4 of the total frames, 2.5\% with masking at either the beginning or the end for 1 frame to 1/4 of the total frames, and 5\% with random masking spanning 1 frame to 1/4 of the total frames at both the beginning and the end.

As shown in Fig.~\ref{fig:opensora_finetune}, despite being trained on a large dataset, OpenSora struggles to generalize to the Ego-Exo4D dataset, producing future video frames with minimal consistency relative to the conditioning frame. While fine-tuning improves temporal consistency, the moving trajectories of objects like the basketball and soccer ball still deviate from realistic physical laws. Compared with our feature space prediction results, this suggests that training world models in a reconstructive latent space is more challenging than training them in a feature space.


\section{Additional Results on 3D Human Motion Prediction}

We present additional qualitative results for the 3D human motion prediction task, highlighting a particularly challenging scenario where egocentric observations exhibit minimal variation. This scenario poses significant difficulties for video-conditioned motion prediction, as the model must effectively capture and interpret subtle changes. As demonstrated in Fig.~\ref{fig:motion_prediction}, EgoAgent successfully generates accurate predictions that closely align with the ground truth motion, showcasing its ability to handle fine-grained temporal dynamics and nuanced contextual cues.

\section{OpenSora for Image Classification}

In this section, we detail the process of extracting features from OpenSora V1.1~\cite{opensora} (without fine-tuning) for an image classification task. Following the approach of~\cite{xiang2023denoising}, we leverage the insight that diffusion models can be interpreted as multi-level denoising autoencoders. These models inherently learn linearly separable representations within their intermediate layers, without relying on auxiliary encoders. The quality of the extracted features depends on both the layer depth and the noise level applied during extraction.


\begin{table}[h]
\centering
\caption{$k$-NN evaluation results of OpenSora V1.1 features from different layer depths and noising scales on ImageNet-100. Top1 and Top5 accuracy (\%) are reported.}
\label{tab:opensora-knn}
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Timesteps} & \multicolumn{2}{c}{First Layer} & \multicolumn{2}{c}{Middle Layer} & \multicolumn{2}{c}{Last Layer} \\
\cmidrule(r){2-3}   \cmidrule(r){4-5}  \cmidrule(r){6-7}  & Top1           & Top5           & Top1            & Top5           & Top1           & Top5          \\
\midrule
32        &  6.10           & 18.20             & 34.04               & 59.50             & 30.40             & 55.74             \\
64        & 6.12              & 18.48              & 36.04               & 61.84              & 31.80         & 57.06         \\
128       & 5.84             & 18.14             & 38.08               & 64.16              & 33.44       & 58.42 \\
256       & 5.60             & 16.58              & 30.34               & 56.38              &28.14          & 52.32        \\
512       & 3.66              & 11.70            & 6.24              & 17.62              & 7.24              & 19.44  \\ 
\bottomrule
\end{tabular}%
}
\end{table}

As shown in Table~\ref{tab:opensora-knn}, we first evaluate $k$-NN classification performance on the ImageNet-100 dataset using three intermediate layers and five different noise scales. We find that a noise timestep of 128 yields the best results, with the middle and last layers performing significantly better than the first layer.
We then test this optimal configuration on ImageNet-1K and find that the last layer with 128 noising timesteps achieves the best classification accuracy.

\section{Data Preprocess}
For egocentric video sequences, we utilize videos from the Ego-Exo4D~\cite{grauman2024ego} and WT~\cite{venkataramanan2023imagenet} datasets.
The original resolution of Ego-Exo4D videos is 1408×1408, captured at 30 fps. We sample one frame every five frames and use the original resolution to crop local views (224×224) for computing the self-supervised representation loss. For computing the prediction and action loss, the videos are downsampled to 224×224 resolution.
WT primarily consists of 4K videos (3840×2160) recorded at 60 or 30 fps. Similar to Ego-Exo4D, we use the original resolution and downsample the frame rate to 6 fps for representation loss computation.
As Ego-Exo4D employs fisheye cameras, we undistort the images to a pinhole camera model using the official Project Aria Tools to align them with the WT videos.

For motion sequences, the Ego-Exo4D dataset provides synchronized 3D motion annotations and camera extrinsic parameters for various tasks and scenes. While some annotations are manually labeled, others are automatically generated using 3D motion estimation algorithms from multiple exocentric views. To maximize data utility and maintain high-quality annotations, manual labels are prioritized wherever available, and automated annotations are used only when manual labels are absent.
Each pose is converted into the egocentric camera's coordinate system using transformation matrices derived from the camera extrinsics. These matrices also enable the computation of trajectory vectors for each frame in a sequence. Beyond the x, y, z coordinates, a visibility dimension is appended to account for keypoints invisible to all exocentric views. Finally, a sliding window approach segments sequences into fixed-size windows to serve as input for the model. Note that we do not downsample the frame rate of 3D motions.

\section{Training Details}
\subsection{Architecture Configurations}
In Table~\ref{tab:arch}, we provide detailed architecture configurations for EgoAgent following the scaling-up strategy of InternLM~\cite{team2023internlm}. To ensure the generalization, we do not modify the internal modules in InternML, \emph{i.e.}, we adopt the RMSNorm and 1D RoPE. We show that, without specific modules designed for vision tasks, EgoAgent can perform well on vision and action tasks.

\begin{table}[ht]
  \centering
  \caption{Architecture configurations of EgoAgent.}
  \resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{lcc}
    \toprule
          & EgoAgent-300M & EgoAgent-1B \\
          \midrule
    Depth & 22    & 22 \\
    Embedding dim & 1024  & 2048 \\
    Number of heads & 8     & 16 \\
    MLP ratio &    8/3   & 8/3 \\
    $\#$param.  & 284M & 1.13B \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:arch}%
\end{table}%

Table~\ref{tab:io_structure} presents the detailed configuration of the embedding and prediction modules in EgoAgent, including the image projector ($\text{Proj}_i$), representation head/state prediction head ($\text{MLP}_i$), action projector ($\text{Proj}_a$) and action prediction head ($\text{MLP}_a$).
Note that the representation head and the state prediction head share the same architecture but have distinct weights.

\begin{table}[t]
\centering
\caption{Architecture of the embedding ($\text{Proj}_i$, $\text{Proj}_a$) and prediction ($\text{MLP}_i$, $\text{MLP}_a$) modules in EgoAgent. For details on module connections and functions, please refer to Fig.~2 in the main paper.}
\label{tab:io_structure}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcl}
\toprule
       & \multicolumn{1}{c}{Norm \& Activation} & \multicolumn{1}{c}{Output Shape}  \\
\midrule
\multicolumn{3}{l}{$\text{Proj}_i$ (\textit{Image projector})} \\
\midrule
Input image  & -          & 3$\times$224$\times$224 \\
Conv 2D (16$\times$16) & -       & Embedding dim$\times$14$\times$14    \\
\midrule
\multicolumn{3}{l}{$\text{MLP}_i$ (\textit{State prediction head} \& \textit{Representation head)}} \\
\midrule
Input embedding  & -          & Embedding dim \\
Linear & GELU       & 2048          \\
Linear & GELU       & 2048          \\
Linear & -          & 256           \\
Linear & -          & 65536     \\
\midrule
\multicolumn{3}{l}{$\text{Proj}_a$ (\textit{Action projector})} \\
\midrule
Input pose sequence  & -          & 4$\times$5$\times$17 \\
Conv 2D (5$\times$17) & LN, GELU   & Embedding dim$\times$1$\times$1    \\
\midrule
\multicolumn{3}{l}{$\text{MLP}_a$ (\textit{Action prediction head})} \\
\midrule
Input embedding  & -          & Embedding dim$\times$1$\times$1 \\
Linear & -          & 4$\times$5$\times$17     \\
\bottomrule
\end{tabular}%
}
\end{table}


\subsection{Training Configurations}
In Table~\ref{tab:training hyper}, we provide the detailed training hyper-parameters for experiments in the main manuscripts.

\begin{table}[ht]
  \centering
  \caption{Hyper-parameters for training EgoAgent.}
  \resizebox{0.86\linewidth}{!}{%
    \begin{tabular}{lc}
    \toprule
    Training Configuration & EgoAgent-300M/1B \\
    \midrule
    Training recipe: &  \\
    optimizer & AdamW~\cite{loshchilov2017decoupled} \\
    optimizer momentum & $\beta_1=0.9, \beta_2=0.999$ \\
    \midrule
    Learning hyper-parameters: &  \\
    base learning rate & 6.0E-04 \\
    learning rate schedule & cosine \\
    base weight decay & 0.04 \\
    end weight decay & 0.4 \\
    batch size & 1920 \\
    training iters & 72,000 \\
    lr warmup iters & 1,800 \\
    warmup schedule & linear \\
    gradient clip & 1.0 \\
    data type & float16 \\
    norm epsilon & 1.0E-06 \\
    \midrule
    EMA hyper-parameters: &  \\
    momentum & 0.996 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:training hyper}%
\end{table}%

\clearpage
