\section{Method}
\label{sec:method}

% 1. 任务定义：This paper aims to build a virtual character acting like humans, which 可以同时感知、预判、交互这个世界。
% 2. 任务的应用：Such ability is essential for applications in robotics, gaming, and virtual reality, where the character 需要感知世界现有状态、预判世界的dynamics，从而支撑interaction with the world.
% 任务的挑战：怎么在一个模型中实现这三件事，并且三个能力在学习过程中会相互促进，就像人类interact to learn and learn to interact一样。

% 1. In this paper, we propose a novel framework, named xx, for xx.
% 2. Our core innovation lies in a single unified transformer, unify 三个能力的 representational spaces by mapping them into a sequence of continuous tokens.
% 3. Specifically, 给定egocentric observations and history character actions, 通过projection layers把它们映射为high-dimensional feature vectors。定义了query tokens，通过transformer architecture，获得current state feature vector, future state feature vector和next actions。
% 4. 使用teacher-student mechanism训练Visual representation learning和World model。
% 5. 讨论技术优势：maintain full parameter-sharing，unified representational spaces，互相促进学习 (0)用transformer可以建立各个任务之间的因果关系 (1)将representation对齐以后，current state feature和future state feature对next action prediction的帮助也更大 (2)因为action generation condition在了future prediction上，所以可以通过action训练future prediction

% Overview
% 1. 任务: 学习一个world model，很好地预测future scene state
% 2. Key insight: 人类通过observe and interact来认知世界，因此同时预测了future scene state and future character motion的world model
% 3. 各个subsection的内容: 首先讨论Predictive world model的base model，以及如何预测future scene state and future character motion；然后讨论如何训练；最后介绍我们world model的applications
Given the egocentric observations and history human actions, 
our paper aims to learn an agent model that 
can represent the scene state, predict future states, and predict the following actions. As humans learn to understand the world's underlying dynamics by observing and interacting with it, we believe these three tasks share internal relations, so that can be jointly learned and be beneficial to each other.
% not only can accurately predict future scene states but also the following actions. 
% Our key observation is that humans learn to understand the world's underlying dynamics by observing and interacting with it, which motivates us to equip the world model with an action module, 
% enabling it to not only estimate the near-future scene but also generate the character's next-step action. 

The overview of our proposed architecture is presented in Figure~\ref{fig:framework}.
We first introduce the base model of our joint embedding-action predictive architecture and describe how to represent the current scene state, predict future scene state and human action with the base model (Section~\ref{sec:joint embedding-action predictive world model}).
Then, we introduce how it can be learned in Section~\ref{sec:learning_world_model}.
% In Section~\todo{}, we discuss the training strategies, including the loss functions and the training scheduler.
Finally, we demonstrate the applications of our agent model in various tasks in Section~\ref{sec:applications}.



\subsection{Joint Embedding-Action-Prediction Architecture for EgoAgent Model}
\label{sec:joint embedding-action predictive world model}
% Subsection 1: 模型架构
% Motivation

% Module design
\vspace{1.5mm}
\noindent{\textbf{Base model.}}
% 为什么我们用LLM作为base model
% 1. 我们选择LLM作为backbone model，在其之上修改进行future state和future motion的预测，以及image feature的提取。
We employ a Large Language Model (LLM)~\cite{team2023internlm,touvron2023llama,sun2024moss,jiang2024mixtral,bai2023qwen} as the foundational architecture for our agent model and adapt it to jointly predict future scene states and human actions, as well as to extract image features.
% 2. 这个design choice有三个方面的考虑。
This design choice is motivated by two main reasons.
% 3. 我们的target是预测the near-future world，和LLM的next-token prediction很相似。
First, our objective of near-future world prediction aligns closely with LLMs' inherent next-token prediction capability.
% 4. LLM模型的结构比较完善，也比较简单，容易拓展。
Second, the well-structured and straightforward architecture of LLMs provides a flexible and extensible foundation for our specific tasks.

% \sd{The basic information of InternLM.}{}
% 介绍LLM的基本信息
% 1. Given previous texts, LLM first 使用tokenizer将它们转为tokens，使用一个embedding layer将每个token映射到一个d维的向量，然后将这些向量输入到一个Transformer中，预测下一个token的概率分布。

% Given previous texts, LLM first uses a tokenizer to convert them into tokens, maps each token to a feature vector using an embedding layer, and feeds these vectors into a transformer to predict the next token.

% 2. In practice, we adopt the InternLM as the backbone architecture.
In practice, we adopt the InternLM~\cite{team2023internlm} as the backbone,
% 3. 我们模型的transformer和InternLM保持一致，只将输入修改为image和actions，将输出改为feature和actions，以及修改了corresponding projection layers and prediction heads。
% Our model's transformer is consistent with InternLM, except that we modify 
with adjustments made to the input and output formats: we modify the input to be the observed images and actions, and the output to the scene states and human actions.
% Meanwhile, we adjust the corresponding network layers to accommodate these changes.
To accommodate these changes, we adapt relevant network layers accordingly.
Notably, instead of using pretrained tokenizers like VQVAE~\cite{van2017neural} to convert images and actions into discrete tokens, we utilize learnable convolutional layers to project them into continuous feature representations.

\vspace{1.5mm}
\noindent{\textbf{Jointly predict and act.}}
% 第一段：讲清楚input、output，及其motivation
Given egocentric observations $V=\{I_0, I_1, ...I_t\}$ and history human actions $A=\{A_0, A_1, ...A_t\}$,
our agent model aims to predict the future scene state $S_{t+1}$ corresponding to the anticipated observation $I_{t+1}$ and to generate plausible human action $A_{t+1}$ by understanding the current scene state $S_t$ at the same time.
The scene state is represented as a set of high-dimensional feature vectors, whereas the human actions are represented as sequences of 3D human skeletons.
In contrast to previous methods~\cite{xiang2024pandora,bruce2024genie,yang2023learning} that represent the scene states using raw images or videos, our feature vector embedding allows the model to focus on higher-level concepts.
This abstraction aligns more closely with human perception and predictive process in the real world, as highlighted in IWM~\cite{garrido2024learning}.

% 第一部分：怎么转为feature vectors
% 1. Specifically, we first separately encode the egocentric video and character actions into feature vectors.
Specifically, we begin by encoding the egocentric video $V$ and human action sequence $A$ into feature vectors.
% 2. For egocentric video, similar to ViT, our approach utilizes a convolutional layer to process each video frame to get the feature map, which is then subdivided into a set of image feature vectors.
For egocentric video, following a similar approach to ViT~\cite{dosovitskiy2020image}, we use a convolutional layer to process each video frame $I_t$, producing a feature map that is then subdivided into a set of image feature vectors. These features are flattened to form the input image tokens $i_0, i_1, ... i_t$.
% 3. 我们利用linear projection layer \tocite{} 将human skeletons投影为action feature vectors.
For human actions, we utilize a convolutional layer with a Layer Normalization (LN) and a Gaussian Error Linear Unit (GeLU) activation layer to map the human skeletons $A_t$ into action features $a_t$.
% 第二部分：怎么输入给LLM
The Transformer network within the LLM then processes these feature vectors to produce the target outputs.
% 1. According to LLM的做法，我们将input转为a sequence of tokens的形式。
As illustrated in Figure~\ref{fig:framework}, the input sequence is formatted as a sequence of image, action, and query tokens, in line with LLM practices.
% 2. Different to text index tokens，our tokens是high-dimensional feature vectors.
Different from text index tokens, our tokens are continuous feature vectors.
% 3. 对于每一time step，我们组合一段tokens，including image tokens, current state query token, action tokens, future action query token, future state query token.

% As shown in Figure~\ref{fig:framework}, 
At each time step $t$, we assemble a fragmented sequence of tokens, including image tokens $i_t$, an action query token $q_{a}$, action tokens $a_t$, and a future state query token $q_{s}$, where the query tokens are represented as learnable embeddings. 
The future state query token $q_{s}$ prompts the model to take previous image tokens $i_{0:t}$ and action tokens $a_{0:t}$ into consideration. Notably, similar to next-token prediction, we insert the action query token $q_{a}$ right after the image tokens $i_t$ and before the input action tokens $a_t$, ensuring that the model relies solely on past observations and actions to predict subsequent actions.
% 4. 组合4个time steps作为input token sequence.
% The final input token sequence is composed of four time steps.
% 第三部分：怎么输出
% 1. Given the input token sequence, the transformer network within the LLM process them to output current state tokens, future action tokens, and future state tokens.
Subsequently, the transformer network processes the input token sequence and outputs action embeddings $a_{t}^{'}$ and future state embeddings $s_{t+1}^{'}$ in response to the query tokens.
% 2. Finally, we leverages separate MLP networks to convert these tokens to current state feature vectors, future actions, future state feature vectors.
Finally, we utilize separate MLP networks to map these embeddings to the predicted actions $A_{t}^{'}$ and future scene states $S_{t+1}^{'}$.

\subsection{Learning EgoAgent in Feature Space}
\label{sec:learning_world_model}

% Motivation：我们在feature level进行supervision，所以我们设计了一个asymmetric teacher branch
% \paragraph{Asymmetric teacher branch.}
% \paragraph{Loss functions}: feature prediction loss, action loss.
% \paragraph{self-supervised learning of teacher branch.}: 这里先讲一下motivation：self-supervise学到的特征可以帮助学习world model。dino loss

\vspace{1.5mm}
\noindent{\textbf{Asymmetric feature branch.}
% Motivation
% 1. Similar to JEPA, we train our world model in the feature level.
Similar to IWM~\cite{garrido2024learning}, we train our agent model at the feature level.
% 2. To this end, our approach additionally designs a asymmetric branch, which extracts image features only and acts as a guidance for the main branch, as illustrated in Figure~\todo{}.
To this end, we introduce an additional asymmetric feature branch as a teacher network, which extracts only image features and provide guidance for the student branch, as illustrated in Figure~\ref{fig:framework}.
% 3. Thanks to the flexible structure of transformer network, we can easily achieve this by adjusting the input as current image tokens and query token, as illustrated in …
Thanks to the flexible structure of the LLM architecture, this feature branch can be easily implemented by adjusting the network input to a reduced sequence of image tokens $i_{t}$ and a state query token $q_s$ compared to the main network.
% 4. Then, the transformer network outputs the current state embeddings, which is fed into an MLP network to obtain the current state feature vectors.
The model then outputs a world state represented as an image embedding, which is fed into an MLP network to obtain the scene state feature $S_{t}$.

\vspace{1.5mm}
\noindent{\textbf{Supervisions for predicting scene states and actions.}}
% 1. Given egocentric observations $V=\{I_0, I_1, ...I_t\}$ and history character actions $A=\{M_0, M_1, ...M_t\}$, the main branch predicts the future state feature vectors and future actions, and the teacher branch extract the image feature vectors.
% Given egocentric observations $V=\{I_0, I_1, ...I_t\}$ and history character actions $A=\{A_0, A_1, ...A_t\}$, 
The student branch predicts the future world states $S_{t+1}^{'}$ and actions $A_{t}^{'}$, while the asymmetric feature branch (teacher) takes only the egocentric image $I_{t+1}$ to extract the future scene states $S_{t+1}$.
% 2. 描述dino feature loss和action loss
Given the input token sequence at time step $t$, the loss function for training the model to predict future states and actions is defined as:
% \begin{equation}
    \begin{align}
       \mathcal{L}_{act}(t)&=\mathcal{L}_1(A_t',A_t), \\
    \mathcal{L}_{pred}(t)&=\mathcal{L}_{dino}(S_{t+1}', sg[S_{t+1}]),       
    \end{align}
% \end{equation}
where $sg[\cdot]$ denotes the stop-gradient operation, $\mathcal{L}_1$ and $\mathcal{L}_{dino}$ represent the L1 loss and the DINO loss~\cite{caron2021emerging}, respectively. 
% 3. stop gradient
% 4. ema
Following common practices in self-supervised learning~\cite{caron2021emerging, he2020momentum}, sg[$\cdot$] is applied to block the gradients from back-propagating to the teacher branch.
The weights of the teacher branch are updated in each iteration using an Exponential Moving Average (EMA) of the student branch.

\vspace{1.5mm}
\noindent{\textbf{Self-supervision for learning powerful representations.}}
% 1. Motivation: 
% \todo{Describe the motivation of additionally learning feature branch.}
When humans learn to interact with the environment, they first develop an understanding of the observed scene and objects, which then aids them in predicting future states of the world and making appropriate responses. Inspired by this process, we introduce an additional self-supervised learning loss on EgoAgent to facilitate learning representative features from egocentric videos from scratch:
\begin{align}
    \mathcal{L}_{rep}(t)&=\mathcal{L}_{dino}(\Theta_{stu}(I_t^{v1})
    , \Theta_{tea}(I_t^{v2})).  
\end{align}
% \begin{align}
%     S_t^{v1}&=\Theta_{stu}(I_t^{v1}), \\
%     S_t^{v2}&=\Theta_{tea}(I_t^{v2}), \\
%     \mathcal{L}_{rep}(t)&=L^{DINO}(S_t^{v1}
%     , sg[S_t^{v2}]).  
% \end{align}
Here, $\Theta_{stu}$ and $\Theta_{tea}$ denote the student and teacher networks of EgoAgent, respectively, $I_t^{v1}$ and $I_t^{v2}$ denote the two different views derived from the egocentric image $I_t$. 
% We first randomly crop a relatively small region from the large egocentric images. Two views of the region, $I_t^{v1}$, are then generated and fed into the student and teacher branches in REPA. 

% % 2. 做法：We perform the self-supervised learning of the feature branch through:
% We perform the self-supervised learning of the feature branch through:
% % 3. 描述dino feature loss
% \begin{equation}
%     L_{dino}(\Theta_{stu}(), sg[\Theta_{tea}()]),
%     \text{\todo{describe the dino loss}}
% \end{equation}
% where $\Theta_{stu}$ and $\Theta_{tea}$ means the student and teacher models.
% Both of them share the same architecture as the feature branch.
% % 4. ema
% The weights of the teacher branch are updated through the exponential moving average of the student branch weights in each training iteration.

\vspace{1.5mm}
\noindent{\textbf{Overall objective function.}}
Given an input token sequence containing $t$ time steps, we have the overall objective function $\mathcal{L}$ defined as:
\begin{equation}
    \mathcal{L} = \frac{1}{t}\sum_{k=0}^t{(
    \lambda_{rep}\mathcal{L}_{rep} + \lambda_{pred}\mathcal{L}_{pred} +
    \lambda_{act}\mathcal{L}_{act}),
    }
\end{equation}
where $\lambda_{rep}$, $\lambda_{pred}$, and $\lambda_{act}$ are the corresponding loss weights for representing the world, predicting future scene states and actions, respectively.

\subsection{Applications}
\label{sec:applications}
After training, EgoAgent inherently possesses three valuable applications.

\vspace{1.5mm}
\noindent\textbf{Visual representation learning}: EgoAgent learns scene states as feature representations, allowing it to extract meaningful features from input images
by $ S= \Theta(I),$
where $\Theta$ denotes the EgoAgent model.
These features can be directly applied to representation tasks, \emph{e.g.}, image classification.

\vspace{1.5mm}
\noindent\textbf{Future state prediction:} given egocentric video clips with historical human action sequence, EgoAgent is capable of predicting future scene states $S_{t+1}$ by
$        S_{t+1} = \Theta(I_0, A_0, ..., I_{t}, A_t). 
$
Once predicted, the future scene image can then be retrieved by measuring similarity within the learned feature space, enabling applications that require foresight into upcoming visual states.

\vspace{1.5mm}
\noindent\textbf{Human action prediction:} from an egocentric perspective and informed by previous actions, EgoAgent can generate plausible future human actions $A_t$ as a sequence of 3D skeletons by
$
A_t=\Theta(I_0, A_0, ..., A_{t-1}, I_{t}).
$
This predictive capability is essential for applications in humanoid robotics, virtual environments, and interactive gaming, where anticipating human-like movements is crucial.