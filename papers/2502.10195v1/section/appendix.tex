\clearpage

% \section{Implementation details}
% \paragraph{Measure of camera bias}

\section{Statistics of benchmarks}
\begin{table}[H]
    \scriptsize
    \centering
    \caption{
        Statistics of datasets used in our experiments. In PersonX, each identity has 36 images for each camera.
    }
    {
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|cccc}
        \hline
        Dataset & \# identities & \# images & \# cameras & Scene \\
        \hline  \hline
        CHUK03-NP & 1,467 & 14,097 & 2 & Indoor \\
        Market-1501 & 1,501 &  32,668 & 6 & Outdoor \\
        MSMT17 & 4,101 & 126,441 & 15 & Indoor/outdoor \\
        PersonX & 1,266 & 273,465 & 6 & Synthetic \\
        VeRi-776 & 200 & 51,003 & 20 & Outdoor \\
        \hline
    \end{tabular}
    }
    \label{tab:dataset_statistics}
\end{table}

\section{Additional discussions on feature space}
\label{sup:features_space}

\subsection{Experimental details}
\label{sup:features_space_details}
Here, we explain how we calculate the displacement vectors from feature pairs of two different cameras with the same identity in Section~\ref{subsec:features_space}.
Suppose a labeled dataset is given.
For the $i$-th identity, we denote its $k$-th image in the $j$-th camera by ${\x}_{k}^{(i,j)}$.
We denote the number of images of the $i$-th identity from the $j$-th camera by $N^{(i, j)}$, \ie, the $i$-th identity has $N^{(i, j)}$ images from the $j$-th camera.
For a pretrained encoder $f_\theta$, the feature of ${\x}_{k}^{(i,j)}$ is given by ${\f}_{k}^{(i,j)} = f_\theta({\x}_{k}^{(i,j)})$.
Then, we compute an average representation of the $i$-th identity in the $j$-th camera, ${\s}^{(i,j)}$, as follows:
\begin{equation}
    {\s}^{(i,j)} 
    = {\mathop{\mathbb{E}}}_{k} [{\f}_{k}^{(i,j)}]
    = \frac{1}{N^{(i, j)}} \sum_{k=1}^{N^{(i, j)}} {\f}_{k}^{(i,j)}.
\end{equation}
The displacement vector of the $i$-th identity between the $j$-th camera and the $l$-th camera, $\bm{d}_{i}^{j \rightarrow l}$, is given by
\begin{equation}
    \bm{d}_{i}^{j \rightarrow l} = {\s}^{(i,l)} - {\s}^{(i,j)}.
\end{equation}
The displacement vector $\bm{d}_{i}^{j \rightarrow l}$ can be thought as the motion of the feature due to the camera change from the $j$-th camera to the $l$-th camera.
The average cosine similarity between the displacement vectors is computed by
\begin{equation}
    {\mathop{\mathbb{E}}}_{p, q}
    {\mathop{\mathbb{E}}}_{m,n} [Sim(
        \bm{d}_{m}^{p \rightarrow q}, 
        \bm{d}_{n}^{p \rightarrow q}
    )],
\end{equation}
where $(p, q)$ and $(m, n)$ denote a pair of different cameras and a pair of different identities, respectively, and $Sim$ denotes the cosine similarity function.


\subsection{Additional results}
\label{sup:features_space_additional}

We additionally analyze the feature space of other models, PPLR-CAM and SOLIDER, following the experimental setup of Section~\ref{subsec:features_space}.
The results are presented in Figures~\ref{fig:dominant_pplr_cam} and \ref{fig:dominant_solider}.
The tendencies previously discussed for TransReID-SSL are also observed for these models.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{figure/feature_space_pplr_msmt_cam.pdf}
  \newcolumntype{E}{@{}c@{}}% begin-/end-column definitions
  \begin{tabular}{ccc}
    (a) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ & 
    (b) & 
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  (c) \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Analysis on the 2048-dimensional embedding space of PPLR-CAM~\citep{cho2022part}.
    (a) Variance of each dimension of camera mean features.
    (b) Cosine similarity of displacement vectors between feature pairs of the same identity from different cameras for selected dimensions.
    (c) Result of camera-specific centering on the features for selected dimensions.
  }
 %\vspace{-2mm}
  \label{fig:dominant_pplr_cam}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{figure/feature_space_solider_msmt.pdf}
  \newcolumntype{E}{@{}c@{}}% begin-/end-column definitions
  \begin{tabular}{ccc}
    (a) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ & 
    (b) & 
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  (c) \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Analysis on the 1024-dimensional embedding space of SOLIDER~\citep{chen2023beyond}.
    (a) Variance of each dimension of camera mean features.
    (b) Cosine similarity of displacement vectors between feature pairs of the same identity from different cameras for selected dimensions.
    (c) Result of camera-specific centering on the features for selected dimensions.
  }
 %\vspace{-2mm}
  \label{fig:dominant_solider}
\end{figure}



\section{Additional discussions on low-level image properties}
\label{sup:low_level}

\subsection{Experimental details}

\begin{figure}[H]
  \centering
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Brightness_dark.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Brightness_light.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Blur.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Sharp.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Contrast_up.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Contrast_down.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Saturation_up.pdf}
  \includegraphics[width=.45\linewidth]{figure/lowlevel/Saturation_down.pdf}
  \caption{
    Examples of applying the transformation functions to an image with four strength levels.
  }
  \label{fig:lowlevel_examles}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{figure/cuhk_lowlevel_statistics.pdf}
  \caption{
    Statistics of low-level properties of images used in our experiments on each low-level property.
    Note that all images have the same contrast value.
  }
  \label{fig:cuhk_statistics}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{figure/lowlevel_centering/nmi.pdf}
  \caption{
    The Normalized Mutual Information (NMI) scores between property group labels and camera labels for each property.
    A weak correlation between them is observed, except the contrast.
  }
  \label{fig:cuhk_nmi}
\end{figure}

\subsection{Additional results}

\begin{figure}[H]
  \centering
  \setlength\tabcolsep{2pt}  % 4.5pt
  \begin{tabular}{cc}
    \includegraphics[width=.4\linewidth]{figure/lowlevel/di_dj_sim.pdf} &
    \includegraphics[width=.4\linewidth]{figure/lowlevel/dbar_di_sim.pdf} \\
    (a) & (b) \\
    \includegraphics[width=.4\linewidth]{figure/lowlevel/d1i_d2i_sim.pdf} &
    \includegraphics[width=.4\linewidth]{figure/lowlevel/d1bar_d2bar_sim.pdf} \\
    (c) & (d) \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Cosine similarity of displacement vectors of the features due to low-level transformations.
  }
 %\vspace{-2mm}
  \label{fig:lowlevel_metrics}
\end{figure}

Here, we analyze the movements of the features due to low-level image transformations in multiple aspects.
In the experiments, we define four levels of transformation strength for several low-level transformation functions, as shown in  Figure~\ref{fig:lowlevel_examles}.
For a transformation function, we denote the feature of the $i$-th image and the feature of its transformed image at level $k$ by ${\f_i}^{(0)}$ and ${\f_i}^{(k)}$, respectively.
For example, for the blurring function, ${\f_i}^{(4)}$ denotes the feature when the $i$-th image is most strongly blurred.
Then, we denote the displacement vector of the feature after applying the transformation to the images for a level $k$ by ${\bm{d}_i}^{(k)} = {\f_i}^{(k)} - {\f_i}^{(k-1)}$.
We also denote the average displacement vector in the level $k$ by ${\m}^{(k)} = \mathop{\mathbb{E}}_{i} [{\bm{d}_i}^{(k)}]$.
We investigate the tendency of the movements of the features by computing the following cosine similarity between the displacement vectors:
\begin{itemize}
    \item (a) $\mathop{\mathbb{E}}_{i,j} [Sim({d_i}^{(k)}, {d_j}^{(k)})]$: 
    How similar are the movements of features to each other under a transformation?
    \item (b) $\mathop{\mathbb{E}}_{i,j} [Sim({d_i}^{(k)}, {m}^{(k)})]$:
    How similar are the movements of features to the average movement under a transformation?
    \item (c) $\mathop{\mathbb{E}}_{i,j} [Sim({d_i}^{(k)}, {d_i}^{(k+1)})]$:
    How similar are the movements of features to their previous motion when a stronger transformation is applied?
    \item (d) $\mathop{\mathbb{E}}_{i,j} [Sim({m}^{(k)}, {m}^{(k+1)})]$:
    How consistent is the average movement when a stronger transformation is applied?
\end{itemize}
The results are presented in Figure~\ref{fig:lowlevel_metrics}.
It can be observed that the motions of the features due to specific low-level transformations, \eg, ``Contrast\_down'', are similar to each other overall.



% \section{Additional results of camera-adaptive feature debiasing}
% \label{sup:fd}


% \section{Additional discussions on background}
% \label{sup:background}

% \subsection{Experimental details}
% \begin{figure}[H]
%   \centering
%   \includegraphics[height=.25\linewidth]{figure/background/ex1_ori.png} ~~~~~~
%   \includegraphics[height=.25\linewidth]{figure/background/ex1_rm.jpg}  ~~~~~~
%   \includegraphics[height=.25\linewidth]{figure/background/ex2_ori.png} ~~~~~~
%   \includegraphics[height=.25\linewidth]{figure/background/ex2_rm.jpg}  ~~~~~~
%   \includegraphics[height=.25\linewidth]{figure/background/ex3_ori.png}  ~~~~~~
%   \includegraphics[height=.25\linewidth]{figure/background/ex3_rm.jpg}  
%   \caption{
%     Examples of the image matting result using the extracted background masks. 
%   }
%   \label{fig:background_mask_examples}
% \end{figure}

% \begin{figure}[H]
%   \centering
%   \setlength\tabcolsep{10pt}  % 4.5pt
%   \begin{tabular}{cccccc}
%     \sf{\small Original} & 
%     \sf{\small \vtop{\hbox{\strut Random}\hbox{\strut uniform}}} &
%     \sf{\small \vtop{\hbox{\strut Random}\hbox{\strut noise}}} &
%     \sf{\small 0-filled} &
%     \sf{\small 127-filled} &
%     \sf{\small 255-filled} \\
%     \includegraphics[width=.08\linewidth]{figure/background/ori.png} &
%     \includegraphics[width=.08\linewidth]{figure/background/random.png} &
%     \includegraphics[width=.08\linewidth]{figure/background/noise.png} &
%     \includegraphics[width=.08\linewidth]{figure/background/black.png} &
%     \includegraphics[width=.08\linewidth]{figure/background/gray.png} &
%     \includegraphics[width=.08\linewidth]{figure/background/white.png} \\
%   \end{tabular}
%   \vspace{-2mm}
%   \caption{
%     Examples of the synthesized backgrounds.
%   }
%  %\vspace{-2mm}
%   \label{fig:background_synthesis_examples}
% \end{figure}


% We extract the background masks of the test images using a background removal tool~\citep{removebg}.
% We obtain almost accurate background masks, as shown in Figure~\ref{fig:background_mask_examples}.
% Using the masks, we synthesize the backgrounds of the images in multiple manners, as shown in Figure~\ref{fig:background_synthesis_examples}.

% \subsection{Additional results}
% \begin{table}[H]
%     \scriptsize
%     \centering
%     \caption{
%         Additional result on the influence of background. 
%         The numbers denote the performance before/after applying the camera-specific normalization.
%     }
%     \vspace{1mm}
% {
%     \scriptsize
%     \renewcommand{\arraystretch}{1.1}
%     \begin{tabular}{l|cc}
%         \hline
%         \multirow{2}{*}{Background} &
%         \multirow{2}{*}{mAP} &
%         \multirow{2}{*}{R1} \\
        
%         \multicolumn{1}{c|}{} & 
%         \multicolumn{1}{c}{} & 
%         \multicolumn{1}{c}{} \\
%         \hline \hline
        
%         Original & 64.8 / \textbf{76.0} &  63.0 / \textbf{74.1} \\
%         \hline
%         Random uniform & 37.9 / \textbf{45.5} &  43.7 / \textbf{51.1}  \\
%         Random noise &  62.6 / \textbf{74.7} &  58.5 / \textbf{74.1}  \\
%         0-filled &  47.5 / \textbf{62.1} &  43.7 / \textbf{60.0}  \\
%         127-filled & 64.5 / \textbf{77.5} &  58.5 / \textbf{77.0} \\
%         255-filled &  62.6 / \textbf{74.1} &  57.0 / \textbf{74.1}  \\
%         \hline
%     \end{tabular}
% }
%     \label{tab:background_additional_result}
% \end{table}

% \begin{figure}[H]
%   \centering
%   \setlength\tabcolsep{5pt}  % 4.5pt
%   \begin{tabular}{cc}
%     \sf{Original features} & 
%     \sf{Debiased features} \\
%     \includegraphics[width=.3\linewidth]{figure/background/graybg_vis_ori.pdf} &
%     \includegraphics[width=.3\linewidth]{figure/background/graybg_vis_deb.pdf} \\
%   \end{tabular}
%   \vspace{-2mm}
%   \caption{
%     Visualization results of features from the images with the 127-filled backgrounds. Each color denotes an unique camera.
%   }
%  %\vspace{-2mm}
%   \label{fig:background_gray_vis}
% \end{figure}

% We provide experimental results for additional backgrounds in Table~\ref{tab:background_additional_result}.
% The visualization results of the original features and debiased features from the 127-filled backgrounds are presented in Figure~\ref{fig:background_gray_vis}.
% It is observed that the original features are clustered with respect to cameras, although there is no background difference among the images. 
% The camera-specific normalization successfully mitigate this camera bias.




\section{Experimental details on body angle}
\label{sup:body_angle}

\begin{figure}[H]
  \centering
  \setlength\tabcolsep{10pt}  % 4.5pt
  \begin{tabular}{ccc}
    \sf{Front} & 
    \sf{Back} &
    \sf{Side} \\
    \includegraphics[width=.12\linewidth]{figure/body_angle/front.jpg} &
    \includegraphics[width=.12\linewidth]{figure/body_angle/back.jpg} &
    \includegraphics[width=.12\linewidth]{figure/body_angle/side.jpg} \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Templates of three body angle classes defined in our experiment. The blue and orange points denote the right and left body parts, respectively.
  }
 %\vspace{-2mm}
  \label{fig:body_angle_examples}
\end{figure}

\begin{table}[H]
% \tiny
% \scriptsize
% \vspace{-10pt}
    \caption{
        Testset statistics of the experiments on body angle.
        The number of images in each case is shown.
    }
    \label{tab:body_angle_stats}
    \centering
{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|ccccccc}
        \hline
        Class & CAM 1 & CAM 2 & CAM 3 & CAM 4 & CAM 5 & CAM 6 & All \\
        \hline  \hline
        
        Front & 406 & 557 & 639 & 101 & 91 & 243 & 2,037 \\
        Back & 410 & 235 & 639 & 121 & 95 & 191 & 1,691 \\
        Side & 120 & 63 & 222 & 102 & 44 & 108 & 659 \\
        \hline
        Total & 936 & 855 & 1,500 & 324 & 230 & 542 & 4,387 \\
        \hline
    \end{tabular}
}
\end{table}

We define three body angle classes of front, back, and side, and construct a test set which is a subset of Market-1501.
As shown in Figure~\ref{fig:body_angle_examples}, we define a template of body keypoints to each class.
To obtain the labels of test images, we extract the body keypoints of the images using MediaPipe~\citep{mediapipe}, and classify the images through a template-based nearest neighbor classification.
The statistics of the constructed dataset are presented in Table~\ref{tab:body_angle_stats}.

\section{Influence of camera balance in training data}
\label{sup:camera_imbalance_in_training}

\begin{table}[H]
    % \scriptsize
    \caption{
        Evaluation results of the camera-specific normalization for CC trained on PersonX with the ground truth labels.
    }
    \vspace{3pt}
    \label{tab:camera_imbalance}
    \centering
    {
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{cc|cc|cc|cc}
        \hline
        \multicolumn{2}{c|}{Market-1501} & 
        \multicolumn{2}{c|}{MSMT17} & 
        \multicolumn{2}{c|}{CUHK03-NP} &
        \multicolumn{2}{c}{PersonX} \\
        
        \cline{1-8}
        mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 \\ 
        \hline \hline
        
        12.7 / \textbf{18.8} & 31.6 / \textbf{39.3} & 1.2 / \textbf{2.3} & 4.0 / \textbf{6.6} & 4.5 / \textbf{7.0} & 4.4  / \textbf{6.5} & 87.8 / \textbf{88.8} & 95.4 / \textbf{95.9} \\
        \hline
    \end{tabular}
    }
\end{table}

In the widely used training datasets, using Market-1501, MSMT17 and CUHK03-NP, the number of samples of an identity varies for each camera view.
In other words, there is an inherent camera imbalance in these datasets, which may induce the camera bias into the model during training.
Then, if this imbalance is corrected, would the camera bias be resolved?
To find out, CC is trained on PersonX with ground truth labels, where all identities have the same number of samples in each camera.
Table~\ref{tab:camera_imbalance} presents the evaluation results of the camera-specific normalization on the model for several benchmarks.
It is observed that the effect of debiasing is still definite, suggesting that the camera bias still exists even when there is no camera imbalance in the training data.
In other words, more effort is required for debiasing beyond balancing the training dataset.


\section{Feature visualization result}

\begin{figure}[H]
  \centering
  \setlength\tabcolsep{2pt}  % 4.5pt
  \begin{tabular}{cc}
    \textsf{Original features} & \textsf{Normalized features} \\
    \includegraphics[width=.4\linewidth]{figure/vis_ori.pdf} &
    \includegraphics[width=.4\linewidth]{figure/vis_deb.pdf} \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    The t-SNE result of features of PPLR-CAM trained on MSMT17 using samples from Market-1501. 
    Different colors are used for each camera.
  }
 %\vspace{-2mm}
  \label{fig:feat_vis}
\end{figure}

Figure~\ref{fig:feat_vis} presents the t-SNE result of features of PPLR-CAM trained on the MSMT dataset using samples from the Market dataset. 
It is observed that the features from the same camera tend to cluster more than the features from the different cameras in the left plot. 
This camera bias is effectively mitigated by the normalization as shown in the right plot.

\section{Additional result of camera-specific normalization}
\label{sup:spcl_uda}

\begin{table}[H]
    \scriptsize
    \caption{
        Evaluation result of SPCL trained in an unsupervised domain adaptive manner, with Market-1501 and MSMT17 as the source domain and target domain, respectively. 
        The numbers denote the performance before/after the camera-specific normalization.
    }
    \vspace{3pt}
    \label{tab:ice_cam}
    \centering
    {
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{cc|cc|cc|cc}
        \hline
        \multicolumn{2}{c|}{Market-1501} & 
        \multicolumn{2}{c|}{MSMT17} & 
        \multicolumn{2}{c|}{CUHK03-NP} &
        \multicolumn{2}{c}{PersonX} \\
        
        \cline{1-8}
        mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 \\ 
        \hline \hline
        
        \textbf{86.8} / 86.1 & \textbf{94.7} / 93.9 &  26.8 / \textbf{28.5} & 53.7 / \textbf{56.1} & 13.9 / \textbf{18.9} & 13.3 / \textbf{18.1} & 36.1 / \textbf{45.4} & 59.2 / \textbf{68.9} \\
        \hline
    \end{tabular}
    }
\end{table}

\section{Number of discarded training samples by our USL strategy}
\label{sup:usl_discarded_ratio}


\begin{table}[H]
% \tiny
% \scriptsize/
% \vspace{-10pt}
    \caption{
        The proportion of discarded training samples by our training strategy.
    }
    \label{tab:discarded_ratio}
    \centering
{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{c|cccccc}
        \hline
        Epoch & 0  & 20 & 40 & 60 & 80 & 100 \\
        \hline  \hline
        
        Discarded samples & 63.5\% & 5.8\% & 3.0\% & 2.6\% & 2.8\% & 2.8\% \\
        \hline
    \end{tabular}
}
\end{table}

Discarding biased clusters in Section~\ref{subsec:strategy_usl} reduces the effective number of training samples.
Table~\ref{tab:discarded_ratio} presents the ratio of the discarded samples (\ie, the samples of single-camera clusters) during training of CC with our training strategy. 
We observe a drastic discarding ratio in the initial epoch of model training. 
However, the proportion rapidly reduces; only approximately 3\% of the total samples are excluded in the last epochs. 
As a result, the risk is reduced by excluding many samples in the early training stages, and as the model converges, it learns enough knowledge from almost all samples. Note that, the suggested learning strategy is an effective and easy-to-implement solution, which significantly improves the mAP of this model by 19.3\%.


% \section{Additional results of main experiments}

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=.7\linewidth]{figure/sim_dist_deb1_v2.pdf}
%   \caption{
%     Cosine similarity distribution of debiased features from Figure~\ref{fig:motivation}(b) and (c).
%     The left plot shows the similarity between positive samples, while the right plot shows the similarity between negative samples.
%     The less separable distributions compared to the original distributions indicate the mitigation of the camera bias.
%   }
%  %\vspace{-2mm}
%   \label{fig:sim_dist_deb}
% \end{figure}




\section{Influence of clustering parameter in USL}
\label{sup:usl_clusteirng_parameter}

\begin{table}[H]
% \tiny
% \scriptsize
% \vspace{-10pt}
    \caption{
        Training results of CC with the varying $\epsilon$ parameter of the DBSCAN algorithm.}
    \label{tab:clustering_parameter}
    \centering
{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|cccccc}
        \hline
        \multicolumn{1}{c|}{Method} & 
        mAP & R1 & R5 & R10 & Bias & \# training clusters\\ 
        \hline \hline
        
        \multicolumn{1}{l|}{(a) Without our training strategies} \\
        % \hdashline
        $\epsilon = 0.4$    & 18.3 & 39.2 & 49.7 & 54.8 &  33.7 & 2495 \\
        $\epsilon = 0.5$    & 21.8 & 45.8 & 56.5 & 61.3 & 32.0 & 2090 \\
        $\epsilon = 0.6$    & 29.8 & 57.1 & 68.5 & 72.8 & 32.5 & 1564 \\
        $\epsilon = 0.7$    & 32.0 & 58.6 & 71.1 & 75.9 & 30.5 & 1108 \\
        $\epsilon = 0.8$    & 8.2 & 19.1 & 27.9 & 32.9 & 40.7 & 291 \\

        \hline
         \multicolumn{1}{l|}{(b) With our training strategies} \\
        % \hdashline
        $\epsilon = 0.4$    & 44.5 & 74.3 & 84.0 & 86.6 & 24.6 & 1708 \\
        $\epsilon = 0.5$    & 46.9 & 75.1 & 84.6 & 87.2 & 24.4 & 1516 \\
        $\epsilon = 0.6$    & 49.1 & 76.5 & 85.6 & 88.3 & 23.8 &  1245 \\
        $\epsilon = 0.7$    & 46.2 & 73.5 & 83.5 & 86.8 & 24.0 & 974 \\
        $\epsilon = 0.8$    & - & - & - & - & - & - \\
        
        \hline
    \end{tabular}
}
\end{table}
Since the clustering result depends on the parameter settings of the clustering algorithm, it is possible that the camera bias of the model varies as well.
Here, we investigate the influence of the most important parameter of DBSCAN, $\epsilon$, which is the maximum distance between two samples to be neighborhood.  
The training results of CC on MSMT using several $\epsilon$ values are presented in Table~\ref{tab:clustering_parameter}, where the number of clusters at the last training epoch of each model is also shown.

In Table~\ref{tab:clustering_parameter}(a), it is observed that the larger $\epsilon$ values tend to roughly decrease the camera bias, while the too large value ($\epsilon$ = 0.8) results in the severe performance degradation. 
A larger $\epsilon$ value can make it easier for samples to cluster together, leading to fewer clusters and larger cluster sizes. 
Thus, it seems that as the $\epsilon$ value increases, the diversity of cameras in each cluster benefits from the increased cluster size up to a certain point. 
However, the samples are indiscriminately clustered for a too large value, causing poor clustering quality. 
In this context, setting an appropriate value for $\epsilon$ is crucial for model training. 
We find that $\epsilon$ = 0.7 yields the best result, with the number of clusters (1108) most similar to the number of identities (1041) in the training data. 
In Table~\ref{tab:clustering_parameter}(b), it is shown that the performance of the models are considerably improved by our training strategies.
The result of $\epsilon$ = 0.8 is omitted since the number of the generated clusters was extremely low, so the mini-batches were not produced properly with the same data sampler used in the previous experiments.





\section{Additional discussions}
\label{sup:discussion}
% Because the camera bias turns out to be a complex combination of many visual domain gaps between different cameras, it seems that more efficient to select debiasing criteria based on trained feature, not hand-crafted features. We remain this work for further research.

\paragraph{Limitations}
The camera-specific feature normalization requires additional computation of mean and variance for each camera, followed by the normalization on the features.
The expected running time of these operations increases linearly with the number of data.
The calculation of the statistics can be a memory-exhaustive process if the number of samples per a camera is large. 
To solve the problem of computational cost, for example, general under-sampling techniques can be adopted.
This issue is left as a topic for future work.


\paragraph{Broader impacts}
Our work focuses on the re-identification technology, which is widely used in real-world applications such as surveillance systems and traffic management solutions.
By mitigating bias in target camera domains with a simple approach at the inference level, deploying these applications becomes easier, leading to broader use of AI-powered solutions.
As a negative societal impact of the work, an improvement in the re-identification could be used to surveil people in negative manners.

\paragraph{Reproducibility}
The code for reproducing the experiment results is provided in the supplementary material.


% \section{Algorithm detail}
