\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figure/feature_space.pdf}
  \newcolumntype{E}{@{}c@{}}% begin-/end-column definitions
  \begin{tabular}{ccc}
    (a) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ & 
    (b) & 
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  (c) \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Analysis on the 384-dimensional embedding space of a ReID model.
    We measure the similarity of displacement vectors and mAP results increasing the number of feature dimensions following different orders.
    (a) Variance of each dimension of camera mean features.
    (b) Cosine similarity of displacement vectors between samples of the same identities from different cameras along selected dimensions.
    (c) Result of camera-specific feature centering for selected dimensions.
  }
 %\vspace{-2mm}
  \label{fig:und1}
\end{figure}


\section{Understanding camera bias and feature normalization}
\label{sec:understanding}

\subsection{Camera-specific feature normalization}
\label{subsec:feature_debiasing}
In Section~\ref{sec:motivation}, we observed that the ReID models have a large camera bias on unseen domains.
As a straightforward debiasing method, we introduce camera-specific feature normalization which postprocesses embedding vectors leveraging camera labels at test time.
It is performed as follows.

Suppose that a test dataset $\mathcal{X} = \{ (\x_1, \y_1), (\x_2, \y_2), \cdots, (\x_N, \y_N) \}$ with $N$ samples is given, where $\x_i$ and $\y_i$ denote the image and camera label of each sample, respectively.
A pretrained encoder $f_\theta$ is used to extract embedding features $\mathcal{F} = \{ \f_1, \f_2, \cdots, \f_N \}$, where $\f_i = f_\theta(\x_i)$.
We split $\mathcal{F}$ into $M$ subsets $\mathcal{F}_1, \mathcal{F}_2, \cdots, \text{and} ~  \mathcal{F}_M$ depending on the camera labels, where the number of cameras is denoted by $M$.
Then, the mean and standard deviation vectors for each camera, $\m_c$ and $\bsigma_c$, are computed as follows:
\begin{equation}
    \m_c = \frac{1}{|\mathcal{F}_c|} \sum_{\f_i \in \mathcal{F}_c}^{} \f_i ~~~ \text{and} ~~~
    \bsigma_c = \sqrt{\frac{1}{|\mathcal{F}_c|} \sum_{\f_i \in \mathcal{F}_c}^{} (\f_i - \m_c) \odot (\f_i - \m_c)},
\end{equation}
where $\odot$ denotes the element-wise multiplication.
% The camera-specific feature normalization on $\f_i$ from the $c$-th camera is given by: 
% \begin{equation}
%     \label{eq:fd}
%     \fhat_i = \frac{\f_i - \m_c}{\bsigma_c}.
%     % \z_i = \frac{\f_i - \m_c}{\bsigma_c}  ~~~ \text{and} ~~~  \fhat_i = \frac{\z_i}{||\z_i||_2}.
% \end{equation}
The camera-specific feature normalization on $\f_i$ with the camera label $\y_i$ is given by: 
\begin{equation}
    \label{eq:fd}
    \fhat_i = \frac{\f_i - \m_{\y_i}}{\bsigma_{\y_i}}.
\end{equation}

% This technique can be used for any pretrained ReID models at test time without additional training or heavy computation.

This operation has been used as modified forms in camera mean subtraction~\citep{gu20201st,luo2021empirical} and camera-specific batch normalization~\citep{zhuang2020rethinking}. 
In \citet{zhuang2020rethinking}, the normalization is followed by an affine transformation learned during training. 
% Importantly, the previous studies have employed this operation without sufficient justification for its debiasing effects.
However, how does the simple camera-specific feature normalization have a debiasing effect?
We revisit the camera-specific feature normalization by empirically analyzing why it mitigates the camera bias and demonstrating its generalizability through comprehensive experiments.


\subsection{Analysis on feature space}
\label{subsec:features_space}
We dive deeply into the feature space of a ReID model~\citep{luo2021self} trained on MSMT17 using CUHK03-NP samples, to understand why the normalization can play a role of debiasing.

\paragraph{Sensitivity to camera variations differs across dimensions}
We first find that the sensitivity of each dimension of the feature space to camera variations is quite different from each other.
We compute mean features of each camera view and present the element-wise variances of the mean features in the descending order in Figure~\ref{fig:und1}(a).
It is shown that some dimensions have a relatively large variation, which might be largely related to the camera bias of the model.

\paragraph{Movements of features due to camera variations}
We indirectly investigate features, movements due to camera changes using the identity labels and camera labels of the samples.
We obtain displacement vectors from feature pairs of two different cameras with the same identities (details in Appendix~\ref{sup:features_space_details}) and compute their average cosine similarity in selected dimensions, with increasing the number of selected dimensions.
Three selecting orders are used: 
(1) ``Dimension index'' follows the original index order of the dimensions, 
(2) ``Camera sensitive'' follows the descending order of 
the element-wise variances of the camera means,
and (3) ``Camera insensitive'' follows the reverse order of (2).
From Figure~\ref{fig:und1}(b), we observe that the similarities of the displacement vectors in the camera-sensitive dimensions are relatively large.
In other words, the features tend to move consistently in the camera-sensitive dimensions depending on a camera variation, implying that the effect of a camera change appears as translation on these embedding dimensions.


\paragraph{Sensitive dimensions dominate debiasing effects}
% \paragraph{Normalizing for sensitive dimensions is dominant}
Then, can we debias the features by subtracting the camera mean features for those sensitive dimensions?
To find out, we apply a camera-specific centering on selected dimensions in Figure~\ref{fig:und1}(c).
Note that there is a clear difference in the improvement rate of ReID performance depending on the selecting order.
The performance gains are actually dominated in the camera-sensitive dimensions.
For example, centering on top-50 dimensions (about 13\%) of higher variances  achieves approximately a half of the total gains, while centering on top-50 dimensions of lower variances shows almost no gain.
For the low-variance dimensions, a half of the total gains requires centering of as many as 350 dimensions (about 91\%). 
Similar results are obtained for other models in Appendix~\ref{sup:features_space_additional}.





\begin{figure}[t]
  \centering
  \setlength\tabcolsep{-1pt}  % 4.5pt
  \begin{tabular}{ccc}
    \includegraphics[height=.23\linewidth]{figure/lowlevel/di_dj_sim.pdf} &
    \includegraphics[height=.23\linewidth]{figure/lowlevel_centering/group.pdf} &
    \includegraphics[height=.23\linewidth]{figure/lowlevel_centering/group_camera.pdf} \\
    % \includegraphics[height=.23\linewidth]{figure/und_tmp/property_norm.png} & 
    % \includegraphics[height=.23\linewidth]{figure/und_tmp/property_camera_norm.png} \\
    (a) & 
    (b) &
    (c) \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Analysis on low-level properties.
    (a) Cosine similarity of displacement vectors by image transformations.
    (b) Property group-specific feature normalization.
    The dashed line indicates the performance without normalization.
    (c) (Property group, camera)-specific feature normalization.
    The dashed line indicates the performance with camera-specific (and property-agnostic) normalization.
  }
 \vspace{-1mm}
  \label{fig:und2}
\end{figure}


\subsection{Analysis on detailed bias factors}

We explore the feature normalization for detailed bias factors of ReID models, including image properties and body angle of images.
The model~\citep{luo2021self} trained on MSMT17 is used.

\paragraph{Movements of features due to image transformations}
Given the fine-grained nature of person ReID, the camera bias of a model might be closely related to the difference in low-level image properties between cameras.
Here, we analyze the changes of features due to image transformations applied to samples from CUHK03-NP, using eight low-level transformation functions with four levels of transformation strength as shown in Figure~\ref{fig:lowlevel_examles}.
The feature of the $i$-th image and the feature of its transformed image at level $k$ are denoted by ${\f_i}^{(0)}$ and ${\f_i}^{(k)}$, respectively.
For example, for a blurring function, ${\f_i}^{(4)}$ denotes the feature when the $i$-th image is most strongly blurred.
Then, we compute the average cosine similarity between displacement vectors of the features after applying a transformation to the images for each level $k$, which is given by $\mathop{\mathbb{E}}_{i,j} [Sim({\f_i}^{(k)} - {\f_i}^{(k-1)}, {\f_j}^{(k)} - {\f_j}^{(k-1)})]$.
The result is shown in Figure~\ref{fig:und2}(a).
We observe that, for certain transformations such as decreasing brightness, 
the displacement vector (${\f_i}^{(k)} - {\f_i}^{(k-1)}$) due to the transformation is similar across different images to some extent
, which is analogous to the effect of camera variations.

\paragraph{Normalization for image properties}
Then, can we reduce biases of the model towards the low-level properties by utilizing the feature normalization?
To find out, we calculate the brightness, sharpness, contrast, and area of all samples, as visualized in Figure~\ref{fig:cuhk_statistics}. 
Note that all samples in the dataset have almost same contrast values.
We divide the samples into $N$ groups of equal size for each property.
For example, when dividing the samples into $N=2$ groups based on the brightness, we use the median brightness value as the threshold for group assignment.
Here, a small but meaningful correlation between these group labels and camera labels is observed as shown in Figure~\ref{fig:cuhk_nmi}.
Then, we perform a group-specific feature normalization on the features using the property group labels.
As presented in Figure~\ref{fig:und2}(b), the normalization on the features based on the property groups is effective for brightness, sharpness, and area.
It does not work for contrast since all contrast values are almost equal.
In addition, we subdivide each property group into multiple groups based on the camera labels and present the result of group-specific normalization with the subdivided group labels in Figure~\ref{fig:und2}(c).
Interestingly, the (property group, camera)-specific normalization outperforms the camera-specific normalization for proper $N$ values.
For example, compared to the camera-specific normalization, (area group, camera)-specific normalization exhibits about 1.5 mAP improvements.
This implies that further considerations of other bias types of ReID models along with the camera bias are needed.
Experimental details and additional results are provided in Appendix~\ref{sup:low_level}.

\paragraph{Normalization for body angle}
It has been shown that ReID models have a bias towards the body angle of an image~\citep{personx}.
This bias is likely to be closely related with the camera bias, since
the distribution of body angles would be different for each camera orientation.
Then, can we reduce the bias of the model towards the body angle by using the feature normalization?
To find out, we define three body angle classes (front, back, and side) and construct four angle-labeled datasets from Market-1501, including front-only, side-only, back-only, and all-angle dataset.
Then, we perform an angle-specific feature normalization on the features using the angle labels, as well as (angle, camera)-specific normalization like previous paragraph.
As shown in Table~\ref{tab:angle_norm}, the angle-specific normalization works and the performance is further improved by using the camera labels together.
Details of the datasets are described in Appendix~\ref{sup:body_angle}.

In summary, we observed consistent feature movements due to image transformations and confirmed the applicability of the feature normalization to detailed bias factors such as low-level image properties and body angles.
It is encouraging that considering both camera labels and other factors in normalization can achieve performance beyond only considering the camera labels, highlighting the need for further research into biases of ReID models beyond the camera bias. 
The normalization methods could serve as an easy tool for such exploration.



\begin{table}[t]
% \tiny
\centering
\vspace{-10pt}
    \caption{
    Feature normalization for body angle.
    ``All'' includes images of front, back, and side angles.
    }
    \label{tab:angle_norm}
    {
    \scriptsize
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|cc|cc|cc}
        \hline
        \multicolumn{1}{c|}{\multirow{2}{*}{Normalization}} & 
        \multicolumn{2}{c|}{All} & 
        \multicolumn{2}{c|}{Front-only} & 
        \multicolumn{2}{c}{Side-only} \\

        \cline{2-7}
        \multicolumn{1}{c|}{} & 
        mAP & R1 & mAP & R1 & mAP & R1 \\ 
        \hline \hline
        
        None  & 68.3 & 76.0 & 84.1 & 84.8 & 84.3 & 83.3 \\
        \hline
        Angle-specific  & 75.0 & 81.1 & - & - & - & - \\
        Camera-specific  & 76.2 & 84.1 & \textbf{91.1} & \textbf{91.5} & \textbf{89.4} & \textbf{91.3} \\
        (Angle, Camera)-specific  & \textbf{80.5} & \textbf{87.2} & - & - & - & - \\
        \hline
    \end{tabular}
    }
\end{table}


\subsection{More empirical results}

\paragraph{Generalizability}

\begin{table}[t]
    \tiny
    % \scriptsize
    % \small
    % \vspace{-5pt}
    \caption{
        Evaluation results of the camera-specific feature normalization for various ReID models.
        The numbers denote the performance before/after normalization.
        ``SL'' and ``CA'' denote the supervised learning and camera-aware methods, respectively.
        ``\textdagger'' indicates transformer backbones.
        ``*'' indicates our reproduced results.
        ISR is trained on external videos and the others are trained on MSMT17.
    }
    \vspace{3pt}
    \label{tab:main_fd_real}
    % \centering
    % \setlength{\tabcolsep}{0.855em}
    \setlength{\tabcolsep}{0.61em}
    {\scriptsize (a) ReID performance }\\
    {
    % \renewcommand{\arraystretch}{1.2}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcc|cc|cc|cc|cc}
        \hline
        \multicolumn{1}{l}{\multirow{2}{*}{Method}} & 
        \multicolumn{1}{c}{\multirow{2}{*}{SL}} & 
        \multicolumn{1}{c|}{\multirow{2}{*}{CA}} & 
        \multicolumn{2}{c|}{Market-1501} & 
        \multicolumn{2}{c|}{MSMT17} & 
        \multicolumn{2}{c|}{CUHK03-NP} &
        \multicolumn{2}{c}{PersonX} \\
        
        \cline{4-11}
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &  
        \multicolumn{1}{c|}{} & 
        mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 \\ 
        \hline \hline
        
        SPCL~\citep{ge2020self} & \xmark & \xmark & 16.0 / \textbf{21.9} & 37.1 / \textbf{43.7} & \cellcolor{black!12} 19.1 / \textbf{20.3} & \cellcolor{black!12}	42.4 / \textbf{44.4} & 6.1 / \textbf{9.1} &  5.1 / \textbf{8.1} & 20.4 / \textbf{31.0} & 41.2 / \textbf{53.7} \\
        % CC*~\citep{dai2022cluster} & \xmark & \xmark & 24.9 / \textbf{32.9} & 50.8 / \textbf{59.3} & \cellcolor{black!12} 30.1 / \textbf{32.6} & \cellcolor{black!12} 57.0 / \textbf{60.2} & 10.8 / \textbf{15.0} & 10.5 / \textbf{15.6} & 27.7 / \textbf{39.2} & 54.4 / \textbf{64.2} \\
        CC*~\citep{dai2022cluster} & \xmark & \xmark & 22.5 / \textbf{30.0} & 47.3 / \textbf{56.4} & \cellcolor{black!12} 29.8 / \textbf{32.2} & \cellcolor{black!12} 57.1 / \textbf{60.4} & 8.4 / \textbf{13.5} & 8.3 / \textbf{13.1} & 24.7 / \textbf{36.8} & 51.4 / \textbf{62.1} \\
        % ICE*~\citep{dai2022cluster} & \xmark & \xmark &  &  & \cellcolor{black!12} & \cellcolor{black!12}	&  &  &  & \\
        PPLR~\citep{cho2022part} & \xmark & \xmark     & 25.2 / \textbf{31.8} & 53.7 / \textbf{61.6} & \cellcolor{black!12} 30.6 / \textbf{31.7} & \cellcolor{black!12} 59.5 / \textbf{62.4} & 10.1 / \textbf{14.2} & 9.4 / \textbf{13.5} & 30.7 / \textbf{39.4} & 57.3 / \textbf{68.2} \\
        TransReID-SSL\textsuperscript{\textdagger}~\citep{luo2021self} & \xmark & \xmark     &  53.6 / \textbf{62.3} & 78.1 / \textbf{83.5} & \cellcolor{black!12} 49.5 / \textbf{53.0}	& \cellcolor{black!12} 75.0 / \textbf{77.3} & 27.2 / \textbf{35.7} & 25.4 / \textbf{34.4} & 45.4 / \textbf{59.6} & 65.7 / \textbf{79.0} \\
        ISR\textsuperscript{\textdagger}~\citep{dou2023identity} & \xmark & \xmark  & 70.2 / \textbf{71.9} & 87.0 / \textbf{87.8} & 32.5 / \textbf{34.2} & 58.8 / \textbf{60.8} & 38.6 / \textbf{42.3} & 37.1 / \textbf{40.5} & 66.4 / \textbf{70.2} & 83.1 / \textbf{85.3} \\
        \hline

        CAP*~\citep{wang2021camera} & \xmark & \checkmark & 30.8 / \textbf{36.6} & 58.9 / \textbf{65.3} & \cellcolor{red!15} 36.3 / \textbf{36.6} & \cellcolor{red!15} 67.5 / \textbf{67.7}	& 15.5 / \textbf{17.9} & 16.3 / \textbf{18.7} & 36.9 / \textbf{45.0} & 64.6 / \textbf{72.7} \\
        ICE-CAM*~\citep{chen2021ice} & \xmark & \checkmark & 25.9 / \textbf{34.9} & 53.4 / \textbf{63.3} & \cellcolor{red!15} 37.8 / \textbf{37.9}  & \cellcolor{red!15} 66.9 / \textbf{67.5} & 12.2 / \textbf{17.2} &  11.9 / \textbf{16.4} & 26.2 / \textbf{39.8} &  52.2 / \textbf{66.7} \\
        PPLR-CAM~\citep{cho2022part} & \xmark & \checkmark & 28.4 / \textbf{34.5} & 58.3 / \textbf{65.1} & \cellcolor{red!15} \textbf{42.2} / 41.3	& \cellcolor{red!15} 73.2 / \textbf{73.3} & 12.0 / \textbf{16.2} & 12.0 / \textbf{15.9} & 31.0 / \textbf{39.6} & 57.4 / \textbf{68.6} \\
        CAJ~\citep{chen2024caj} & \xmark & \checkmark & 30.6 / \textbf{36.9} & 61.3 / \textbf{68.1} & \cellcolor{red!15} \textbf{44.3} / 42.8 & \cellcolor{red!15} \textbf{75.1} / 74.4 & 14.1 / \textbf{18.1} & 14.6 / \textbf{19.1} & 32.5 / \textbf{40.9} & 59.5 / \textbf{70.0} \\
        \hline
        			
        PAT\textsuperscript{\textdagger*}~\citep{pat} & \checkmark & \xmark  & 43.8 / \textbf{52.9} & 70.4 / \textbf{76.8} & \cellcolor{red!15} \textbf{54.8} / 54.1 & \cellcolor{red!15} 78.0 / \textbf{78.3} & 24.5 / \textbf{29.8} & 24.2 / \textbf{29.9} & 50.0 / \textbf{59.8} & 72.8 / \textbf{80.4} \\
        SOLIDER\textsuperscript{\textdagger}~\citep{chen2023beyond} & \checkmark & \xmark &  72.4 / \textbf{79.2} &	89.0 / \textbf{91.7}	& \cellcolor{red!15} \textbf{77.1} / 77.0	& \cellcolor{red!15} \textbf{90.7} / 90.6	& 53.9 / \textbf{58.7}	& 53.8 / \textbf{59.1} & 	55.4 / \textbf{63.4}	& 79.5 / \textbf{84.8} \\
        \hline
        
        TransReID\textsuperscript{\textdagger}~\citep{he2021transreid} & \checkmark & \checkmark & 43.1 / \textbf{52.5}	& 69.5 / \textbf{76.1}	& \cellcolor{red!15} \textbf{67.8} / 66.7	& \cellcolor{red!15} \textbf{85.4} / 85.0	& 29.9 / \textbf{34.5}	& 28.8 / \textbf{34.7}	& 57.7 / \textbf{65.8}  	& 76.9 / \textbf{82.8} \\
        \hline
    \end{tabular}
    }
    {\vspace{2px}
    \\
    \scriptsize (b) Clustering result} \\
    {
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcc|cc|cc|cc|cc}
        \hline
        \multicolumn{1}{l}{\multirow{2}{*}{Method}} & 
        \multicolumn{1}{c}{\multirow{2}{*}{SL}} & 
        \multicolumn{1}{c|}{\multirow{2}{*}{CA}} & 
        \multicolumn{2}{c|}{Market-1501} & 
        \multicolumn{2}{c|}{MSMT17} & 
        \multicolumn{2}{c|}{CUHK03-NP} &
        \multicolumn{2}{c}{PersonX} \\
        
        \cline{4-11}
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &  
        \multicolumn{1}{c|}{} & 
        Bias & Accuracy & Bias & Accuracy & Bias & Accuracy & Bias & Accuracy \\ 
        \hline \hline
        
        SPCL~\citep{ge2020self} & \xmark & \xmark & 22.5 / \textbf{15.1} & 75.0 / \textbf{79.5} & \cellcolor{black!12} 34.7 / \textbf{32.2} & \cellcolor{black!12} 84.0 / \textbf{84.5} & 18.2 / \textbf{14.7}  & 71.2 / \textbf{74.0} & 22.0 / \textbf{13.2} & 74.3 / \textbf{80.6} \\
        % CC*~\citep{dai2022cluster} & \xmark & \xmark  & 15.9 / \textbf{10.5} & 78.5 / \textbf{81.5} & \cellcolor{black!12} 32.2 / \textbf{29.4} & \cellcolor{black!12} 87.8 / \textbf{88.9} & 14.6 / \textbf{10.0} & 76.4 / \textbf{79.2} & 19.2 / \textbf{9.4} & 80.5 / \textbf{86.0} \\
        CC*~\citep{dai2022cluster} & \xmark & \xmark  & 17.1 / \textbf{11.2} & 81.0 / \textbf{84.7} & \cellcolor{black!12} 32.5 / \textbf{29.7} & \cellcolor{black!12} 88.0 / \textbf{89.0} & 17.6 / \textbf{10.8} &74.6 / \textbf{78.4} & 20.6 / \textbf{9.5} & 78.9 / \textbf{85.1} \\
        PPLR~\citep{cho2022part} & \xmark & \xmark     & 	15.6 / \textbf{9.9} & 	81.7 / \textbf{85.2} &  \cellcolor{black!12} 30.2 / \textbf{27.0} & \cellcolor{black!12} 89.0 / \textbf{89.9} & 	15.9 / \textbf{10.1} & 	77.4 / \textbf{80.5} & 	15.3 / \textbf{6.3} & 	82.0 / \textbf{87.5} \\
        TransReID-SSL\textsuperscript{\textdagger}~\citep{luo2021self} & \xmark & \xmark & 9.7 / \textbf{7.2}	& 92.2 / \textbf{94.3}	& \cellcolor{black!12} 27.1 / \textbf{25.4}	& \cellcolor{black!12} 92.8 / \textbf{93.6} & 7.0 / \textbf{4.0}	& 84.2 / \textbf{86.9}	& 12.5 / \textbf{3.9}	& 88.8 / \textbf{93.5} \\
        ISR\textsuperscript{\textdagger}~\citep{dou2023identity} & \xmark & \xmark  & 9.7 / \textbf{9.4} & 95.8 / \textbf{96.0} & 30.3 / \textbf{29.6} & 89.4 / \textbf{89.9} & 5.4 / \textbf{4.5} &  87.7 / \textbf{88.9} & 6.1  / \textbf{4.5} &  94.9 / \textbf{95.9} \\
        \hline		

	
        CAP*~\citep{wang2021camera} & \xmark & \checkmark & 12.3 / \textbf{8.2} & 84.0 / \textbf{86.9} & \cellcolor{red!15} 25.5 / \textbf{24.8} & \cellcolor{red!15} 90.1 / 90.1 & 8.0 / \textbf{6.5} & 78.8 / \textbf{80.6} & 9.9 / \textbf{5.2} & 86.0 / \textbf{89.6} \\ 
        ICE-CAM*~\citep{chen2021ice} & \xmark & \checkmark & 13.9 / \textbf{9.1} & 79.2 / \textbf{82.9} & \cellcolor{red!15} 26.8 / \textbf{25.4} & \cellcolor{red!15} 90.6 /  \textbf{90.8} & 12.7 / \textbf{7.3} & 77.5 / \textbf{80.7} & 18.1 / \textbf{7.4} & 78.3 / \textbf{86.1} \\
        PPLR-CAM~\citep{cho2022part} & \xmark & \checkmark  & 14.3 / \textbf{9.7} & 84.1 / \textbf{87.2} & \cellcolor{red!15} 26.7 / \textbf{25.5} & \cellcolor{red!15} 92.4	  / \textbf{92.5} & 13.7 / \textbf{10.0} & 78.4	  / \textbf{82.6} & 14.6 / \textbf{6.4}	& 81.8  / \textbf{88.0} \\	
        CAJ~\citep{chen2024caj} & \xmark & \checkmark & 12.2 / \textbf{8.9} & 82.1 / \textbf{84.2} & \cellcolor{red!15} 25.2 / \textbf{24.3} & \cellcolor{red!15} \textbf{92.7} / 92.4  & 11.6 / \textbf{8.3} & 80.8 / \textbf{83.3} & 12.2 / \textbf{5.7} &  83.3 / \textbf{88.0} \\
        \hline
        
        PAT\textsuperscript{\textdagger*}~\citep{pat} & \checkmark & \xmark  &  10.8 / \textbf{9.1} & 90.8 / \textbf{93.3} & \cellcolor{red!15} 24.0 / \textbf{23.4} & \cellcolor{red!15} 94.3 / \textbf{94.6} & 6.4 / \textbf{4.4} & 85.3 / \textbf{86.5} & 7.7 / \textbf{3.5} & 90.8 / \textbf{93.9} \\
        SOLIDER\textsuperscript{\textdagger}~\citep{chen2023beyond} & \checkmark & \xmark & 7.3 / \textbf{6.9} & 96.5 / \textbf{97.5} & \cellcolor{red!15}	\textbf{21.3}	/ 21.4 & \cellcolor{red!15}	96.9	/ 96.9 &  1.6	/ 1.6 & 	90.8	/ \textbf{92.8} & 	2.8	/ \textbf{1.5} & 	93.8	/ \textbf{95.1}   \\
        \hline
        
        TransReID\textsuperscript{\textdagger}~\citep{he2021transreid} & \checkmark & \checkmark &  13.6 / \textbf{10.9} & 89.8 / \textbf{92.2} & \cellcolor{red!15}	23.6 / \textbf{22.6} & \cellcolor{red!15} 94.5 / \textbf{95.2} & 3.9 / \textbf{3.6} & 84.7 / \textbf{86.8} & 6.6 / \textbf{3.3} & 92.7 / \textbf{95.1} \\
        \hdashline
        
        Ground Truth  & - & - & 6.4 & - & 19.2 & - & 0.1 & - & 0.0 & - \\
        \hline
    \end{tabular}
    }
\end{table}

\begin{table}[t]
% \vspace{-3px}
\begin{minipage}[c]{0.5\linewidth}
    \centering
    \includegraphics[width=.8\linewidth]{figure/iclr/pplr_cam_market_msmt_deb_right.pdf}
    \vspace{-8px}
    \captionof{figure}{Normalization result of Figure~\ref{fig:motivation}(b).}
    \label{fig:vis_debiased}
\end{minipage}\hfill
\begin{minipage}[c]{0.5\linewidth}
\centering
    {
    \scriptsize
    \renewcommand{\arraystretch}{1.1}
    \captionof{table}{
        Ablation study of feature normalization.
    }
    \label{tab:ablation_fd}
    \begin{tabular}{l|cc|cc}
        \hline
        \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & 
        \multicolumn{2}{c|}{Entire} &
        \multicolumn{2}{c}{Camera-specific} \\

        \cline{2-5}
        \multicolumn{1}{c|}{} & 
        mAP & R1 & mAP & R1 \\ 
        \hline \hline
        
        Baseline & 27.2 & 25.4  & 27.2 & 25.4 \\
        \hline
        + Centering   & 27.8  & 25.8 & 34.7 & 33.0 \\
        + Scaling   & 27.7 & 25.6 & 27.2 & 25.1 \\
        + Centering + Scaling   & \textbf{28.8} & \textbf{26.6} &  \textbf{35.7} & 34.4  \\
        \hline
        + ZCA whitening   & 18.7 & 19.1 & 30.1 & \textbf{35.1} \\
        \hline
    \end{tabular}
    }
    \vspace{4pt}
  \end{minipage}
\end{table}

We present the evaluation results of the camera-specific feature normalization on multiple ReID models in Table~\ref{tab:main_fd_real}.
The mean average precision (mAP) and cumulative matching characteristics (CMC) Rank-1 (R1) are used for evaluation.
The NMI scores of clustering results are also reported as in Section~\ref{sec:motivation}.
Note that ISR and PAT~\citep{pat} are domain generalized methods.
The feature normalization significantly improves the performance of all models on the unseen domain (white background in the table), regardless of training methods or backbones architectures.
For example, on Market-1501, CC~\citep{dai2022cluster} 
exhibits about 7.5\% improvement in mAP and about 5.9\% reduction in camera bias, and TransReID~\citep{he2021transreid} 
% with a transformer backbone learned in a camera-aware supervised manner 
shows about 9.4\% improvement in mAP and about 2.7\% reduction in camera bias.
For the seen domain, the camera-agnostic unsupervised models show slight improvement (gray background), while the camera-aware or supervised models exhibit no improvement (red background).
It is likely because the camera bias of these models is already relatively small on the seen domain, \eg, SOLIDER~\citep{chen2023beyond} has an almost identical bias value to the ground truth.
The normalization results of Figure~\ref{fig:motivation}(b) are shown in Figure~\ref{fig:vis_debiased},
% and~\ref{fig:sim_dist_deb}, 
where the less separable distributions are observed.
The feature visualization result is illustrated in Figure~\ref{fig:feat_vis}.

\paragraph{Ablation study}
The camera-specific feature normalization consists of (1) camera-specific, (2) mean centering, and (3) scaling by standard deviation on the features.
We investigate the effectiveness of each component 
% and additionally compare to ZCA whitening 
for CUHK03-NP in Table~\ref{tab:ablation_fd}, using TransReID-SSL~\citep{luo2021self} trained on MSMT17.
ZCA whitening is also evaluated to check the effectiveness of rotation related to covariance across feature dimensions.
There are some gains from the entire transforms, but the camera-specific transforms outperform them. 
It is observed that the camera-specific mean centering has a dominant effect and the scaling operation provides a small but additional gain.
The rotation by the ZCA whitening does not exhibit definite gains compared to the normalization.


\paragraph{Image volume for computing normalization parameters}
We explore the impact of the number of samples used to compute the statistics for normalization in Figure~\ref{fig:sample_num}.
% , using TransReID-SSL trained on MSMT17.
Equal numbers of samples are randomly sampled from each camera in Market-1501. TransReID-SSL trained on MSMT17 is used in this experiments.
The performance is degraded when using too few samples per camera (\eg, five samples).
Interestingly, using more than 25 samples per camera leads to positive effects and the gains start to be saturated over 100 samples.
This suggests that the number of samples needed to represent camera features is not very large.


\paragraph{Combination with other postprocessing methods}
We test whether the camera-specific feature normalization is still effective when used in conjunction with other postprocessing algorithms, using TransReID-SSL trained on MSMT17.
As shown in Table~\ref{tab:combination}, the normalization brings about 9\% gains in mAP for all methods.
In other words, the problem of camera bias remains after applying the conventional postprocessing methods.
Note that unlike other methods resulting in decreases in R5 and R10, the normalization consistently improves all metrics.


\begin{table}[t]
% \vspace{-3px}
\begin{minipage}[c]{0.48\linewidth}
    \centering
    \includegraphics[width=.95\linewidth]{figure/iclr/sample_num_bar.pdf}
    \vspace{-5px}
    \captionof{figure}{
        Results based on the number of samples used to calculate normalization parameters.
        % The gray and red dashed lines denote the performance without normalization and the performance with normalization using all samples, respectively.
    }
    \label{fig:sample_num}
\end{minipage}\hfill
\begin{minipage}[c]{0.48\linewidth}
\centering
    {
    \scriptsize
    \renewcommand{\arraystretch}{1.1}
    \captionof{table}{
         Combination results with other feature postprocessing methods on Market-1501.
    }
    \label{tab:combination}
    \setlength{\tabcolsep}{0.7em}
    \begin{tabular}{l|cccc}
        \hline
        \multirow{2}{*}{Postprocessing} &
        \multirow{2}{*}{mAP} &
        \multirow{2}{*}{R1} &
        \multirow{2}{*}{R5} &
        \multirow{2}{*}{R10} \\
        
        \multicolumn{1}{c|}{} & 
        \multicolumn{1}{c}{} & 
        \multicolumn{1}{c}{} & 
        \multicolumn{1}{c}{} & 
        \multicolumn{1}{c}{} \\
        \hline \hline
        
        None & 53.6 & 78.1 & 89.2 & 92.3 \\
        + Normalization & \textbf{62.3} & \textbf{83.5} & \textbf{92.8} & \textbf{95.5} \\
        \hline
        DBA~\citep{gordo2017dba} & 59.2 & 77.9 & 88.4 & 91.8 \\
        + Normalization & \textbf{68.6} & \textbf{84.2} & \textbf{92.7} & \textbf{95.1} \\
        \hline
        AQE~\citep{chum2007aqe} & 61.1 & 78.9 & 87.5 & 90.7 \\
        + Normalization & \textbf{70.6} & \textbf{85.1} & \textbf{91.8} & \textbf{94.3} \\
        \hline
        Reranking~\citep{zhong2017reranking} & 67.7 & 78.4 & 85.2 & 88.6 \\
        + Normalization & \textbf{76.3} & \textbf{84.7} & \textbf{90.7} & \textbf{93.0} \\
        \hline
    \end{tabular}
    }
    \vspace{4pt}
  \end{minipage}
\end{table}

