\begin{table}[t]
    \tiny
    % \scriptsize
    % \small
    % \vspace{-5pt}
    \caption{
        Camera bias and accuracy of various state-of-the-art ReID models based on clustering results. 
        ``SL'' and ``CA'' denote the supervised learning and camera-aware method, respectively.
        ``Bias'' and ``Accuracy'' denote the Normalized Mutual Information (NMI) scores between cluster labels and camera labels, and between cluster labels and identity labels, respectively, in $\times$100 scale.
        ISR is trained on external videos and the other models are trained on MSMT17-Train.
    }
    \vspace{3pt}
    \label{tab:motivation}
    % \setlength{\tabcolsep}{0.79em}  % 0.75em
    \setlength{\tabcolsep}{0.61em}  % 0.75em
    \centering
    {
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lccc|cc|cc|cc|cc|cc}
        \hline
        \multicolumn{1}{l}{\multirow{2}{*}{Method}} & 
        \multicolumn{1}{c}{\multirow{2}{*}{SL}} & 
        \multicolumn{1}{c}{\multirow{2}{*}{CA}} & 
        \multicolumn{1}{c|}{\multirow{2}{*}{Backbone}} & 
        \multicolumn{2}{c|}{MSMT17-Train} & 
        \multicolumn{2}{c|}{MSMT17-Test} & 
        \multicolumn{2}{c|}{Market-1501} &
        \multicolumn{2}{c|}{CUHK03-NP} &
        \multicolumn{2}{c}{PersonX} \\
        
        \cline{5-14}
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &  
        \multicolumn{1}{c|}{} & 
        Bias & Accuracy & Bias & Accuracy & Bias & Accuracy & Bias & Accuracy  & Bias & Accuracy \\ 
        \hline \hline
        
        % CC~\citep{dai2022cluster} & \xmark & \xmark & R50 & 34.4 & 89.4 & 32.2 & 87.8	& 15.9 & 78.5 & 14.6 & 76.4 \\
        CC~\citep{dai2022cluster} & \xmark & \xmark & R50 & 34.7 & 89.3 & 32.5 & 88.0 & 17.1 & 81.0 & 17.6 & 74.6 & 20.6 & 78.9 \\
        PPLR~\citep{cho2022part} & \xmark & \xmark & R50 & 31.8 & 90.3 & 30.2 & 89.0	& 15.6 & 81.7 & 15.9 & 77.4 & 15.3 & 82.0 \\
        TransReID-SSL~\citep{luo2021self} & \xmark & \xmark & ViT & 29.3 &	93.1 &	27.1 &	92.8	& 9.7	& 92.2	& 7.0	& 84.2 & 12.5 & 88.8 \\
        ISR~\citep{dou2023identity} & \xmark & \xmark & ViT & 31.8 & 90.5 & 30.3 & 89.4 & 9.7 & 95.8 & 5.4 & 87.7 & 6.1 & 94.9 \\
        PPLR-CAM~\citep{cho2022part} & \xmark & \cmark & R50 & 29.3 & 92.8 & 26.7 & 92.4	& 14.3 & 84.1 & 13.7 & 78.4 & 14.6 & 81.8 \\
        TransReID~\citep{he2021transreid} & \cmark & \cmark & ViT & 24.4 & 98.3 & 23.6	& 94.5 & 13.6 & 89.8	& 3.9 & 84.7 & 6.6 & 92.7 \\
        SOLIDER~\citep{chen2023beyond} & \cmark & \xmark & ViT & 23.2 & 98.7 & 21.3 & 96.9	& 7.3 & 96.5 & 1.6 & 90.8 & 2.8 & 93.8 \\
        \hdashline
        Ground Truth & - & - & - & 21.1 & - & 19.2 & -	& 6.4 & - & 0.1 & - & 0.0 & - \\
        % \bottomrule
        \hline
    \end{tabular}
    }
    % \vspace{-2mm}
\end{table}




\section{Quantitative analysis on camera bias}
\label{sec:motivation}

In this section, we quantitatively investigate the camera bias in existing ReID models.
The camera bias is the phenomenon where the feature distribution is biased towards the camera labels of the samples, which degrades ReID performance.
Many camera-aware methods have been proposed to address this problem.
However, the scope of the discussion has been primarily limited to training domain and the camera bias on unseen domains has not been thoroughly explored.
We focus on the camera bias of ReID models on unseen domains, examining various types of models including camera-aware/agnostic, supervised/unsupervised, and domain generalizable approaches, with the widely used backbones such as ResNet~\citep{he2016deep} and ViT~\citep{dosovitskiy2020vit}.

To measure the bias, we utilize Normalized Mutual Information (NMI) which quantifies the shared information between two clustering results.
We extract the features of samples and perform clustering to them using InfoMAP~\citep{infomap}.
Then, the camera bias is computed by NMI between cluster labels and camera labels of the samples.
The accuracy of the clusters are measured by NMI between the cluster labels and the identity labels.
The results on MSMT17, Market-1501, CUHK03-NP~\citep{cuhk_np}, and PersonX~\citep{personx} are shown in Table~\ref{tab:motivation}, where the bias of the ground truth (\ie, NMI between the identity labels and the camera labels) indicates the inherent imbalance in a dataset.
All models except ISR~\citep{dou2023identity} are trained on MSMT17, hence the other datasets are unseen domains for them.
For ISR, all datasets are unseen domains.

We make two notable observations from the results.
First, the existing ReID models have a large camera bias on the unseen domains, regardless of their training setups or backbones.
% The bias of PPLR~\citep{cho2022part} is mitigated by the camera-aware learning of PPLR-CAM~\citep{cho2022part}, but its bias is still significantly higher than the ground truth.
Second, the unsupervised models have a large camera bias on the seen domain, 
even on their training data.
These imply that debiasing methods for unseen domains are needed in general, and there is room for performance improvement of unsupervised methods by reducing the camera bias during training.
Relatively, the recent supervised models exhibit less debiased results on the training domain.