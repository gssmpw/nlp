\section{Risk of camera bias in unsupervised learning}
\label{sec:risk_usl}


\subsection{Risk of biased pseudo labels}
\label{subsec:biased_pseudo_labels}

In Section~\ref{sec:motivation}, we observed that the ReID models learned in unsupervised manners have a large camera bias even on their training data.
We argue that the existing USL algorithms have two limitations introducing the camera bias into the models.
First, the pseudo labels of training data are biased towards the camera labels.
In USL, a model is supervised by the pseudo labels of the training samples which are usually generated by clustering of the features extracted by the model.
However, as we have seen, the clustering result is already camera-biased, hence using them for training would make the model dependent on the camera-related information.
Second, camera-biased clusters with few cameras are used in training without sufficient consideration.
For example, consider a cluster only consisting of samples from one camera.
Since most of the samples of this cluster may share similar camera-related information (\eg, background), utilizing them as positive training samples can lead the model to pay more attention to the common camera-related information.
Also, the samples in that cluster were likely grouped together incorrectly due to the shared camera information, which is expected to be more common in the early stage of model training.

\subsection{Toy example results}
\label{subsec:toy_examples}

We investigate the risks of biased training data toward cameras using toy examples.
ResNet50 models are trained on the toy datasets using the cluster contrastive loss~\citep{dai2022cluster} in the experiments.

Figure~\ref{fig:usl_toy}(a) compares the training results with the different levels of camera bias and accuracy of pseudo labels for the same training samples.
We constructed a dataset of 7500 samples by randomly selecting 500 identities from Market-1501, where each identity has 5 samples per camera with 3 cameras.
To generate pseudo labels with varying degrees of camera bias at the same accuracy, a certain proportion of identities were divided into three equally-sized clusters for each identity based either on camera labels (``Camera’’) or random selection (``Random’’).
Five splitting ratios of 0\%, 25\%, 50\%, 75\%, and 100\% were used.
For example, the pseudo labels generated by splitting 50\% of the identities consist of 250 original clusters and 750 split clusters, totaling 1000 clusters.
The bias of the pseudo labels is measured by calculating the mean entropy of the camera labels within each cluster~\citep{lee2023camera}.
It is observed that, at the same pseudo label accuracy, models trained with ``Camera'' consistently perform worse than those trained with ``Random''.
Moreover, ``Random'' with 91.9\% accuracy outperforms ``Camera'' with 93.8\% accuracy, despite having lower accuracy.
These results suggest that greater camera bias of pseudo labels has a detrimental effect on model training, and pseudo labels with lower accuracy but less camera bias can provide more benefits than those with higher accuracy but greater camera bias.

Figure~\ref{fig:usl_toy}(b) illustrates the impact of camera diversity of training data, using ground truth labels.
We constructed five datasets of 11821 samples of 1041 identities from MSMT17, where the maximum numbers of cameras per identity are different.
As expected, the model performance declines as the maximum number of cameras decreases.
Notably, a significant performance drop is observed when the model is trained with samples from only a single camera for each identity.
This suggests that using single-camera clusters for training can degrade the model performance.
In addition, the influence of clustering parameter on the camera bias is investigated in Appendix~\ref{sup:usl_clusteirng_parameter}.

\begin{figure}[t]
  \centering
  \setlength\tabcolsep{5pt}  % 4.5pt
  \vspace{-1.5mm}
  \begin{tabular}{cc}
    \includegraphics[height=.28\linewidth]{figure/iclr/usl_toy_pseudo_label_accuracy.pdf} &
    \includegraphics[height=.25\linewidth]{figure/iclr/usl_toy_camera_num.pdf} \\
    (a) & 
    (b) \\
  \end{tabular}
  \vspace{-2mm}
  \caption{
    Risk of biased clusters.
    (a) Training results with varying pseudo label qualities for the same training samples. Two pseudo label generation methods, ``Random'' and ``Camera'', are used.
    The colored numbers denote the average camera entropy of the clusters.
    (b) Training results with varying maximum number of cameras per identity for the same number of samples. 
  }
 \vspace{-1mm}
  \label{fig:usl_toy}
\end{figure}


\subsection{Simple strategies for debiased unsupervised learning}
\label{subsec:strategy_usl}
To reduce the explored risk of camera bias in unsupervised learning, we present two simple training strategies applicable to existing USL algorithms;
\textbf{(1) debiased pseudo labeling}: clustering on the debiased features computed by Equation~\ref{eq:fd} instead of the original features when generating pseudo labels, and 
\textbf{(2) discarding biased clusters}: discarding the single-camera clusters in training data.
We present an example of applying the proposed strategies to unsupervised learning in Algorithm~\ref{alg:strategies}.
With these minor modifications, we observe significant performance improvements in next section.

\begin{algorithm}[t]
\scriptsize
\caption{\small Unsupervised learning algorithm for Person ReID with simple modificaitons} 
\label{alg:strategies}
\begin{algorithmic}
% \REQUIRE Training images $\mathcal{X}$ and corresponding camera labels $\mathcal{Y}$
% \REQUIRE Initialized backbone encoder $f_\theta$ \\
\REQUIRE Initialized backbone encoder $f_\theta$ and training samples with camera labels $\mathcal{X}$ \\
\textbf{for} n in $[1, \text{num\_epochs}]$ \textbf{do}\\
\quad Extract features $\mathcal{F}$ from $\mathcal{X}$ by $f_\theta$.  \\
\quad \textbf{(1) Debiased pseudo labeling:}\\
\quad \quad Transform $\mathcal{F}$ to $\hat{\mathcal{F}}$ by applying the camera-specific feature normalization.  \\
\quad \quad Generate pseudo labels by clustering $\hat{\mathcal{F}}$.  \\
\quad \textbf{(2) Discarding biased clusters:}\\
\quad \quad Collect the images belong to the clusters of single camera as $\mathcal{B}$.  \\
\quad \quad Reconstruct training images by $\mathcal{X'} = \mathcal{X} - \mathcal{B}$.   \\
\quad Prepare for training iterations (\eg, initialization of feature memory).  \\
\quad \textbf{for} i in $[1, \text{num\_iterations}]$ \textbf{do}\\
\quad \quad Sample a mini-batch from the reconstructed data $\mathcal{X'}$. \\
\quad \quad Compute loss (\eg, contrastive loss). \\
\quad \quad Update the encoder $f_\theta$. \\
\quad \quad Update auxiliary modules (\eg, update of feature memory). \\
\quad \textbf{end for}\\
\textbf{end for}
\end{algorithmic}
\end{algorithm}


\subsection{Empirical results}
\label{subsec:strategy_usl_result}

\begin{table}[t]
    % \tiny
    \scriptsize
    % \small
    % \vspace{-5pt}
    \caption{
        % Evaluation of our debiased unsupervised learning method with the SOTA models.
        Results of modifying the existing USL algorithms based on our training strategies.
        "*" indicates our reproduced result with the official code.
    }
    \label{tab:main_usl}
    \vspace{-3pt}
    \setlength{\tabcolsep}{0.72em}
    \centering
    {
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|cccc|cccc|cccc}
        \hline
        \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & 
        \multicolumn{4}{c|}{Market-1501} & 
        \multicolumn{4}{c|}{MSMT17} & 
        \multicolumn{4}{c}{VeRi-776} \\
        
        \cline{2-13}
        \multicolumn{1}{c|}{} & 
        mAP & R1 & R5 & R10 & mAP & R1 & R5 & R10 & mAP & R1 & R5 & R10 \\ 
        \hline \hline
        											
        \multicolumn{13}{l}{\it Camera-agnostic unsupervised} \\
        \hline
        SPCL~\citep{ge2020self} & 73.1 & 88.1 & 95.1 & 97.0 & 19.1 & 42.3 & 55.6 & 61.2 & 36.9 & 79.9 & 86.8 & 89.9 \\
        ICE~\citep{chen2021ice} & 79.5 & 92.0 & 97.0 & 98.1 & 29.8 & 59.0 & 71.7 & 77.0 & - & - & - & - \\
        PPLR~\citep{cho2022part} & 81.5 & 92.8 & 97.1 & 98.1 & 31.4 & 61.1 & 73.4 & 77.8 & 41.6	& 85.6 & 91.1 & 93.4 \\
        CC~\citep{dai2022cluster} & 83.0 & 92.9	& 97.2 & 98.0 & 33.0 & 62.0 & 71.8 & 76.7 & 40.8 & 86.2 & 90.5 & 92.8 \\
        \hline

    
        

        \multicolumn{13}{l}{\it Camera-aware unsupervised} \\
        \hline
        CAP~\citep{wang2021camera} & 79.2 & 91.4 & 96.3 & 97.7 & 36.9 & 67.4 & 78.0 & 81.4 & - & - & - & - \\
        ICE-CAM~\citep{chen2021ice} & 82.3 & 93.8 & 97.6&98.4&38.9&70.2&80.5&84.4& - & - & - & -  \\
        PPLR-CAM~\citep{cho2022part} & 84.4 & 94.3 & 97.8 & 98.6 & 42.2 & 73.3&83.5&86.5&43.5&88.3&92.7&94.4 \\
        \hline
        \hline

        PPLR*~\citep{cho2022part} & 77.4 & 89.6 & 96.1 & 97.4 & 27.2 & 55.7 & 67.1 & 71.8 & 41.5 & 85.6 & 91.4 & 93.2 \\
        PPLR*~\citep{cho2022part} + Ours & \textbf{84.6} & \textbf{93.9} & \textbf{97.8} & \textbf{98.6} & \textbf{40.7} & \textbf{71.4} & \textbf{82.3} & \textbf{85.4} & \textbf{43.2} & \textbf{86.7} & \textbf{91.7} & \textbf{93.7} \\
        \hline
        PPLR-CAM*~\citep{cho2022part} & 84.1 & \textbf{94.0} & 97.7 & 98.6 & 40.7 & 71.8 & 82.6 & 85.7 & 43.3 & 88.1 & 92.2 & 94.2 \\
        PPLR-CAM*~\citep{cho2022part} + Ours & \textbf{84.3} & 93.8 & \textbf{98.1} & \textbf{98.8} & \textbf{44.4} & \textbf{75.8} & \textbf{84.9} & \textbf{87.7} & \textbf{43.7} & \textbf{88.2} & \textbf{92.8} & \textbf{94.5} \\
        \hline
        CC*~\citep{dai2022cluster} & 82.6 & 91.8 & 96.7 & 97.8 & 29.8 & 57.1 & 68.5 & 72.8 & 38.2 & 79.8 & 83.9 & 86.9 \\
        CC*~\citep{dai2022cluster} + Ours & \textbf{85.2} & \textbf{93.5} & \textbf{97.3} & \textbf{98.2} & \textbf{49.1} & \textbf{76.5} & \textbf{85.6} & \textbf{88.3} & \textbf{45.3} & \textbf{89.8} & \textbf{93.9} & \textbf{95.3}  \\
        \hline							
    \end{tabular}
    }
\end{table}

% \paragraph{Main result}
We validate the suggested training strategies on the SOTA camera-agnostic methods, CC and PPLR, and the SOTA camera-aware method, PPLR-CAM.
A vehicle ReID dataset, VeRi-776~\citep{veri}, is additionally used and the person and vehicle images are resized to $384\times128$ and $256\times256$, respectively, following the setup of PPLR.
The models are trained on a H100 GPU with batch size 256 and 100 training epochs, with DBSCAN~\citep{dbscan} to obtain pseudo labels. Our strategies effectively improves all methods as presented in Table~\ref{tab:main_usl}.
In particular, outstanding performance gains are obtained on the challenging benchmark, MSMT17, 
% whose camera gap is relatively large, 
\eg, 19.3\% mAP increase for CC.
The gains for PPLR-CAM are relatively small, which is likely because it uses a camera-aware loss function. 
In addition, the number of discarded training samples by our strategy is discussed in Appendix~\ref{sup:usl_discarded_ratio}.


\begin{figure}[t]
\vspace{-2mm}
\begin{minipage}[c]{0.55\linewidth}
\centering
{
    % \tiny
    \scriptsize
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l|ccccc}
        \hline
        \multicolumn{1}{c|}{Method} & 
        mAP & R1 & R5 & R10 & Bias \\ 
        \hline \hline
        
        Baseline & 29.8 & 57.1 & 68.5 & 72.8 & 32.5  \\
        \hline
        
        \multicolumn{1}{l|}{(a) Ablation study} \\
        % \hdashline
        + (1) Debiased pseudo labeling    & 44.6 & 71.9 & 82.0 & 85.1 & 26.5  \\
        + (2) Discarding biased clusters &  45.4 & 73.7 & 83.4 & 86.6 & 25.4 \\
        + Both of (1) and (2)     & \textbf{49.1} & \textbf{76.5} & \textbf{85.6} & \textbf{88.3} & \textbf{23.8} \\

        \hline
         \multicolumn{1}{l|}{(b) Cluster-weighted loss} \\
        % \hdashline 
        + CD loss~\citep{lee2023camera}   & 40.7 & 69.4 & 80.2 & 83.8 & 25.1 \\
        % + Relaxed CD loss   & 42.6 & 70.2 & 80.1 & 83.9 &   \\
        + Binary weighting   & 40.1 & 69.3 & 79.9 & 83.3 & 25.9 \\
        
        \hdashline
        Ground Truth & - & - & - & - & 19.2 \\
        
        \hline
    \end{tabular}
    }
    \vspace{4pt}
  \end{minipage}\hfill
\begin{minipage}[c]{0.55\linewidth}
\centering
 {\renewcommand{\arraystretch}{0}
  \begin{tabular}{c}
      \includegraphics[width=0.65\linewidth]{figure/exp/usl_ablation_single_cam.pdf} 
      \\
      \includegraphics[width=0.65\linewidth]{figure/exp/usl_ablation_single_accuracy.pdf} \\
  \end{tabular}}
\end{minipage}
\vspace{-2mm}
\caption{
    Results of our training strategies for debiased unsupervised learning on MSMT17.
}
\label{fig:usl_ablation}
\end{figure}

\paragraph{Ablation study}
Table(a) of Figure~\ref{fig:usl_ablation} investigates the individual effect of our strategies, using CC.
Both of the suggested strategies contribute to the performance improvements by reducing the camera bias.
The ratio of the single-camera clusters and clustering accuracy during training are illustrated in the plot of Figure~\ref{fig:usl_ablation}.
It is observed that the baseline has unusually high single-camera cluster rates (from about 80\% to about 35\%), which is effectively mitigated by the proposed methods.


 
\paragraph{Comparison to weighted loss}
We experiment with weighted loss methods related to discarding biased clusters in Table(b) in Figure~\ref{fig:usl_ablation}.
The camera diversity (CD) loss~\citep{lee2023camera} weights a sample proportionally to the diversity of cameras within the cluster it belongs to, with 0-weight for single-camera clusters.
The binary weighting is a variant of it, where we assign 1-weight to clusters of more than one camera.
% without differential weighting of others.
It is observed that the performance improvement of the CD loss is dominated by the 0-weight setting for single-camera clusters.
In our opinion, the CD loss has a side effect of randomizing the mini-batch size or learning rate, since the effective number of samples involved in a model update is randomly changed at each training iteration.
Discarding the biased clusters in training is a simpler method free from such drawbacks, outperforming the CD loss.





