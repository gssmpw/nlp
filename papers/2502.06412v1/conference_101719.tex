\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{standalone}
\usepackage{myPlotStyle}
\usepackage{booktabs}
\usepackage{url}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\renewcommand{\baselinestretch}{0.94}
\begin{document}

\title{Toolbox for Developing Physics Informed Neural Networks for Power Systems Components \\ 
\thanks{This work was supported by the ERC Starting Grant VeriPhIED, funded by the European Research Council, Grant Agreement 949899.}}

\author{\IEEEauthorblockN{Ioannis Karampinis\IEEEauthorrefmark{1}, Petros Ellinas, Ignasi Ventura Nadal, Rahul Nellikkath, Spyros Chatzivasileiadis}
\IEEEauthorblockA{\textit{Department of Wind and Energy Systems
} \\
\textit{Technical University of Denmark (DTU)
}\\
Kgs. Lyngby, Denmark \\
{\IEEEauthorrefmark{1}iokar}@dtu.dk}
}
\maketitle
\begin{abstract}

This paper puts forward the vision of creating a library of neural-network-based models for power system simulations. Traditional numerical solvers struggle with the growing complexity of modern power systems, necessitating faster and more scalable alternatives. Physics-Informed Neural Networks (PINNs) offer promise to solve fast the ordinary differential equations (ODEs) governing power system dynamics. This is vital for the reliability, cost optimization, and real-time decision-making in the electricity grid. Despite their potential, standardized frameworks to train PINNs remain scarce. This poses a barrier for the broader adoption and reproducibility of PINNs; it also does not allow the streamlined creation of a PINN-based model library. This paper addresses these gaps. It introduces a Python-based toolbox for developing PINNs tailored to power system components, available on GitHub \url{https://github.com/radiakos/PowerPINN}. Using this framework, we capture the dynamic characteristics of a 9th-order system, which is probably the most complex power system component trained with a PINN to date, demonstrating the toolbox capabilities, limitations, and potential improvements. The toolbox is open and free to use by anyone interested in creating PINN-based models for power system components.

% %In modern power systems, efficiently solving the differential equations that govern system dynamics is essential for maintaining reliability, optimizing operational costs, and enabling real-time decision-making. However, traditional numerical solvers can be computationally expensive, particularly as the complexity of the components of the power system increases due to the ongoing energy transition. This growing computational burden requires the development of faster and more scalable alternatives. 
% Solving fast the ordinary differential equations (ODEs) governing power system dynamics is vital for the reliability, cost optimization, and real-time decision-making in the electricity grid. However, traditional numerical solvers struggle with the growing complexity of modern power systems, necessitating faster and more scalable alternatives.
% % A promising approach to overcoming this limitation is the use of Physics-Informed Neural Networks (PINNs), which embed the underlying ordinary differential equations (ODEs) of the system into the learning process, thereby reducing reliance on large datasets while preserving physical consistency. Yet, there are no clearly established hybrid data-driven and model-driven PINNs.
% Machine Learning (ML) models offer promise but heavily depend on data quality and quantity. Physics-Informed Neural Networks (PINNs) address this by embedding the ODEs into the learning process, reducing data dependency while ensuring physical consistency. Despite their potential, hybrid data- and model-driven standardized frameworks remain scarce.
% %In this paper, we introduce a standardized pipeline for developing such PINN-based solutions tailored to power system modeling. We developed a Python-based framework, which has been made publicly available on GitHub\footnote{\url{https://github.com/radiakos/PowerPINN}}, enabling automated training and evaluation of PINNs based on a given set of ODEs and a predefined selection of parameters and hyperparameters. 
% This paper introduces a Python-based pipeline for developing PINNs tailored to power system components, available on GitHub \footnote{\url{https://github.com/radiakos/PowerPINN}}. %, enabling automated training and evaluation of PINNs based on a given set of ODEs and a predefined selection of parameters and hyperparameters. 
% %We use this framework to capture the dynamic characteristics of a 9th order system. Through this study we aim to showcase the capabilities of the proposed framework and also unveil the limitations of various approaches. Finally, we present potential improvements-extensions of the proposed pipeline.
% Using this framework, we capture the dynamic characteristics of a ninth-order system, demonstrating its capabilities, limitations, and potential improvements.


%In modern power systems, efficient and accurate simulation of system components is critical for maintaining reliability, reducing costs, and guiding operational decisions. Although the accessible measurement data allow for the deployment of machine learning models, such approach is heavily dependent on the quality and quantity of the available data. The solution is given by the field of Physics-Informed Neural Networks (PINNs), that underlying ordinary differential equations(ODEs) of the simulated system and makes the dependence of data optional. Yet, there is no clearly established hybrid data-driven and model-driven PINNs. In this paper, we introduce a standardized pipeline for training and testing such PINN solutions. We developed in Python the respective framework and made it public on Github. Given the set of ODEs and selecting a series of parameters and hyperparameters, the automated training and testing of the respective PINN models are enabled. We use this framework to capture the dynamic characteristics of different synchronous machine models with purely data-driven, purely model-driven and hybrid approaches. Through this study we aim to showcase the capabilities of the proposed framework and also unveil the limitations of various approaches. Finally, we present potential improvements-extensions of the proposed pipeline.
\end{abstract}

\begin{IEEEkeywords}
Physics-informed machine learning, neural networks, power system dynamic behavior, transient analysis.
\end{IEEEkeywords}




%, which will then serve as a fertile ground for research. 
%In this paper, we aim to establish the roadmap for creating physics-informed machine learning (PIML) models to simulate power system dynamics. With power systems transitioning toward sustainable energy, there is an urgent need for computationally efficient models that can approximate complex dynamic behaviors. Machine learning models, such as Neural Networks(NNs) have proven their ability to identify complex pattern, capturing the desired behavior. The new emerging field, that of PIML exploits the physical laws within machine learning models offering the potential for faster and more accurate dynamic simulations. In this paper, we introduce a standardized pipeline for training and testing such models. Additionally, we offer the framework that 
%Additionally,  We present methods for model reduction and validation, introducing novel correctness verification techniques and an interpretability-driven architecture, Kolmogorov-Arnold Networks (KANs), to enhance trust in NN approximations. This work contributes to the design of transparent, interpretable, and rigorously validated PIML frameworks to accelerate the deployment of reliable AI tools in power systems.

\section{Introduction}

%Decarbonization-integration of RES, low inertia systems - tremendous unexplored challenges - increases the complexity of modelling the simulation in power system - need for novel solutions 
%NNs promising tools, ... emerging field that of PINNs, data hungry combine physics agnostic methods with data agnostic.
%Although, there has been research in power systems there was no automated and easy to use tool for training PINNs to approximate the behavior of dynamical systems in power systems. 
%Toolboxes in PINNs.
%Old1
%The increasing integration of renewable energy sources (RES) into modern power systems is a key driver of decarbonization. However, this shift toward more sustainable energy introduces significant challenges to the stability of the system as a result of the resulting reduced inertia of RES and the high complexity of the inverter controls. Traditionally, power systems have relied heavily on synchronous machines to maintain frequency and voltage stability. With the growing adoption of inverter-based resources (IBR), such as RES, replacing conventional synchronous machines, the dynamics of power grids are evolving rapidly, but also become more non-linear \cite{Erdiwansyah2021, Alimi2020}.
%The increasing integration of renewable energy sources (RES) into modern power systems is a crucial step towards decarbonization. However, this transition introduces significant challenges to system stability because unlike  due to the resulting reduced inertia of RES. Conventional power systems relied heavily on synchronous machines to provide frequency and voltage stability, but with inverter-based resources (IBR) replacing conventional synchronous machines, the dynamics of power grids is evolving rapidly \cite{Erdiwansyah2021, Alimi2020}. 
%old1
%As a result, accurate and fast time-domain simulations are essential to analyze the stability and transient behaviors of the power system under various operating conditions.
%new1
The increasing integration of renewable energy sources (RES) into modern power systems has significantly altered system dynamics, reducing inertia and introducing non-linear behaviors due to inverter-based resources (IBR). These changes pose new challenges in ensuring system stability and reliability \cite{Erdiwansyah2021, Alimi2020}, demanding computational methods that are both accurate and scalable. Power system components exhibit dynamic behavior that can be mathematically represented using ordinary differential equations (ODEs), which describe the evolution of system states over time. Solving these equations efficiently in terms of accuracy and time is crucial for analyzing transient stability, frequency control, and fault responses.

Traditional simulation approaches primarily rely on numerical solvers such as Runge-Kutta methods to approximate the system dynamics governed by ODEs \cite{sauerandpi}. %new2
As power systems grow in complexity, conventional techniques struggle to meet the demand for fast and efficient simulations, particularly when dealing with high-order models or complex non-linear interactions.

%old2
%While these methods offer high accuracy, they suffer from high computational costs, particularly when approximating complex systems. Furthermore, they are limited to predefined system models and do not leverage any real-world sensor data. 
On the one hand, Machine Learning (ML) techniques, particularly Neural Networks (NNs), have been explored to accelerate power system simulations by learning complex system behaviors. However, such
solutions are characterized by their high dependence on data to ensure generalizability and trustworthiness \cite{Hamilton2022}. 

%new3
A promising alternative is Physics-Informed Neural Networks (PINNs), which integrate physical laws into the learning process. By embedding ODEs directly into the network architecture, PINNs reduce data dependency while preserving physical accuracy 
%old3
%Physics Informed Neural Networks (PINNs) have emerged as a promising alternative by incorporating the governing physical laws into the learning process 
\cite{lagaris, RAISSI2019686}. 
%old4
%As a result, PINNs can effectively learn system dynamics with significantly reduced data requirements. 
Recent studies have demonstrated the potential of PINNs in power system applications \cite{Huang2023}, including transient stability analysis \cite{stiasny2023transientstabilityanalysisphysicsinformed} and parameter \& state estimation  \cite{chertkov}. 
%old5
%However, despite their potential, the practical deployment of PINNs that approximate the behavior of power system components is still in its early stages. A major limitation is the lack of standardized frameworks for defining, training, and evaluating PINNs for specific power system components, such as higher-order synchronous machine models. Establishing systematic methodologies for individual power system components is essential to ensure accuracy and computational efficiency while making PINNs practical for real-world applications.
However, applying PINNs to an entire power system remains infeasible due to excessive training time and computational complexity. Instead, a more practical approach is to use PINNs for modeling individual power system components, which can then be integrated into conventional simulators \cite{integratingprevious} or incorporated into a dedicated PINN-based simulation engine, such as PINNSim \cite{pinnsim}. Our vision is for users to be able to easily create a library of PINN-based models, which they can then use with the simulator of their choice; this is similar to the model library of conventional simulation software.


%old5
%In this work, we present a framework that automates and standardizes the training pipeline of PINNs for power system components, ensuring efficient and reproducible results. 
Despite their potential, there is currently no standardized framework for defining, training, and evaluating PINNs for power system components. This lack of standardization hinders broader adoption and reproducibility. It also does not allow the streamlined creation of such a PINN-based model library. This paper addresses these gaps. We introduce a modular and automated framework that streamlines the PINN training process, enabling efficient and reproducible simulations of power system components. Our contributions can be summarized as follows:
\begin{enumerate}
    \item We introduce a modular, automated, and open-source framework to train PINNs for a wide range of power system components.
    %\item We developed an efficient, unified pipeline tailored for training PINN models on power system components.
    %\item Created a comprehensive toolbox for power system components, featuring synchronous machines, designed to seamlessly integrate as plug-and-play modules into our pipeline. 
    %old7
    %We introduce a modular and automated framework for training PINNs to simulate the dynamic behavior of power system components.
    \item We demonstrate the effectiveness of our approach by applying it to a 9th-order system consisted of a synchronous machine (SM) with an Automatic Voltage Regulator (AVR) and a Governor, which is probably the most complex power system component trained with a PINN to date. The code is open-source and available to anyone wishing to create a library of PINN models for their simulations.
    
    %old8
    %We demonstrate the effectiveness of our approach by applying it to a 9th order system consisted of a synchronous machine (SM) with an Automatic Voltage Regulator (AVR) and a Governor, highlighting its accuracy and computational efficiency.
\end{enumerate}

Looking ahead, we envision this framework as a foundation for a future library of PINN-based models, enabling seamless integration into power system simulations, similar to existing model libraries in traditional simulation tools.
%In this work, we present a framework that , , . standardize the pipeline of training efficiently the PINNs for power systems components.
%We summarize the contribution of the paper in 1,2,3,
\section{Physics-Informed Neural Networks}

Power system components exhibit dynamic behavior that can be mathematically represented using ODEs. These equations describe the evolution of state variables over time and are typically framed as initial value problems (IVPs). An IVP refers to finding a function $\mathbf{x}(t)$ solution to a differential equation subject to given initial conditions. The uniqueness of the solution is ensured by the Picard-Lindelöf theorem, assuming that the function $f$ governing the ODE is Lipschitz continuous. A system of ODEs can be expressed as:
\begin{equation}
\frac{d}{dt} \mathbf{x} = f (t, \mathbf{x}, \mathbf{u}, \mu), \quad \mathbf{x}(t_0) = \mathbf{x}_0.
\label{odes}
\end{equation}


where, the function 
$f$ governs the time evolution of the system state vector $\mathbf{x}(t;\mathbf{x}_0, \mathbf{u}, \mu)$, vector $\mathbf{u}$ the inputs of the system, vector $\mu$ consists of the system parameters to be determined and $ \mathbf{x}_0 $ denotes the initial conditions.
%where, $\mathbf{x}(t,\mathbf{x}_0, \mathbf{u}, \mu)$ represents the state vector that evolves over time $t$, $f$ is a given function, mapping the system parameters to the states, vector $\mathbf{u}$ the inputs of the system and vector $\mu$ the system parameters that are to be determined and $ \mathbf{x}_0 $ represents the initial conditions.

The goal is to find the function $\mathbf{x}(t;\mathbf{x}_0, \mathbf{u}, {\mu})$ that satisfies \eqref{odes}, providing the time-domain response of the system. This function will offer the unique solution along time for the given $\mathbf{x}_0, \mathbf{u}$ and $\mu$, which we refer to as a trajectory.


However, due to the nonlinear nature of the problem, it is impossible to derive the closed-form solution of $\mathbf{x}(t)$ and therefore we rely on approximations:
\begin{equation}
\hat{\mathbf{x}}(t; \mathbf{x}_0, \mathbf{u}, \mu) \approx \mathbf{x}(t; \mathbf{x}_0, \mathbf{u}, \mu) 
\label{eq:approximation}
\end{equation}


\subsection{Classical Ordinary differential equations solvers}
Classical numerical solvers, such as the Runge-Kutta methods, are widely used to solve ODEs. These methods approximate the solution trajectory over a specified time interval \( T \) by iteratively computing discrete updates based on derivative evaluations at small time steps. However, this process can be computationally expensive, particularly for complex systems with many state variables or stiff equations.



A promising alternative approach is the use of ML models, e.g. NNs, which can serve as function approximators.

\subsection{Neural Networks as function approximators}

The standard feed-forward Neural Networks are theoritically capable of appoximating any function \cite{hornik1991approximation}. NNs consist of multiple interconnected layers, each containing a number of nodes. These networks include a set of trainable parameters, namely the weights and biases of the connections between nodes, which are updated iteratively until they can approximate the target function with a given accuracy, with the help of an optimizer. Activation functions are applied at each node to introduce non-linearity, enabling the network to approximate complex, non-linear functions as in \eqref{eq:approximation}.


\subsection{Training Neural Networks with data}
NNs are typically trained using a dataset of input-output pairs (labeled data). The training process enables the network to generalize and predict outputs for unseen inputs, provided that the training data sufficiently represent the target domain. Through an iterative procedure, the trainable parameters of the NN are continuously updated to align their outputs with the desired ones. However, this procedure depends entirely on the training data's quality and availability. In our case, such data can be offered with the help of ODE solvers. By simulating a set of trajectories and structuring them into the required input-output format, we obtain the dataset necessary for training, validation, and testing of neural networks. Specifically, we construct a dataset of simulated trajectories within the input domain $\Omega_d$, consisting of a total of $N_d$ data points, represented as: $ \{ ({\mathbf{x}_{0}^{(i)}}, t^{(i)}), \mathbf{x}^{(i)} \}_{i=1}^{N_d} $. The Mean Squared Error (MSE) between the known state $\mathbf{x}^{(i)}$  and the NN output $ \mathbf{\hat{x}}^{(i)}$ for a given initial condition $(\mathbf{x}_{0},t)$, computed across
$N_d$ data points in the dataset, is given by the following loss term:
\begin{equation} \label{eq:data_loss}
\mathcal{L}_{\text{data}} = \frac{1}{N_d} \sum_{i=1}^{N_d} \left| \mathbf{\hat{x}}^{(i)} - \mathbf{x}^{(i)} \right|^2
\end{equation}

\subsubsection{Mathematical foundation of Physics Informed Machine Learning}
The fusion of well-established physical laws described by differential equations and the
capabilities of ML has offered a novel approach to modeling complex
physical systems. The proposed ML algorithms are characterized by their capability to combine data-driven and physics-driven solutions. The automatic differentiation(AD) function of deep learning models, such as NNs, provides the derivative of the predicted output with respect to a known input, e.g. time. In our case, this value represents the left-hand side of the ODE \eqref{odes}. For the right-hand side, the known output $\mathbf{x}$ from the aforementioned dataset is used as a reference value, e, while all other parameters remain known. As a result, we can introduce a physics-informed loss that enforces these two sides to be equal \eqref{eq:physics_data_loss}, ensuring that the trained ML model satisfies the underlying physics laws.
The physics-informed loss that is based on simulated data can be formed as follows:
\begin{equation} \label{eq:physics_data_loss}
\mathcal{L}_{\text{physics data}} = \frac{1}{N_d} \sum_{i=1}^{N_d} \left| \underbrace{\frac{d \mathbf{\mathbf{\hat{x}}}^{(i)}}{d t}}_{\text{ \tiny Derivative of NN output}} - \underbrace{f(t^{(i)}, \mathbf{x}^{(i)}, \mathbf{u}, \mu)}_\text{ \tiny Source term evaluation} \right|^2
\end{equation}

\textbf{Training Neural Networks without data}:
due to the nature of the ODEs, and the form of the above physics-informed loss, no labeled data are required for training. To enforce initial conditions, the neural network output must match the known values of the state variables at $t_0$. 
\begin{equation}
\label{eq:col_ic}
\mathcal{L}_\text{ic col} = \frac{1}{N_{ic}}\sum_{i=1}^{N_{ic}}\big|\mathbf{\hat{x}}^{(i)}(\mathbf{t}_0) - \mathbf{x}_0^{(i)}\big|^2
\end{equation}
where $N_{ic}$ is the number of distinct initial conditions considered
%This results in an initial condition loss that penalizes deviations at these points. The NN output must meet initial conditions at $t_0$, resulting in a loss for any initial condition points within the input space. 
Additionally, the NN must satisfy the differential equation at arbitrary points within the input domain $\Omega$, leading to a physics-informed loss that does not require labeled data.
Conversely, using the NN output for the ODE's right-hand side creates a label-free physics loss. To compute this physics-based loss, we introduce collocation points, which are sampled points $\{(\mathbf{x_0}^{(i)}, t^{(i)}) \}$ within the input domain $\Omega_c$. These points allow us to evaluate the residuals of the ODE, ensuring that the NN adheres to the underlying physics. The physics loss is computed as:
%Any points $\mathbf{x_0}$ within the input domain $\Omega$, which we will refer to as collocation points, can be exploited to calculate the losses of the following form at a selected set of collocation points $\{(\mathbf{x}^{(i)}, t^{(i)}) \}_{i=1}^{N_c}$ within the input domain $\Omega_c$:

\begin{equation} \label{eq:physics_col_loss}
 \mathcal{L}_{\text{physics col}} = \frac{1}{N_c} \sum_{i=1}^{N_c} \left| \underbrace{\frac{d \mathbf{\mathbf{\hat{x}}}^{(i)}}{d t}}_{\text{ \tiny Derivative of NN output}} - \underbrace{f(t^{(i)}, \mathbf{\hat{x}}^{(i)}, \mathbf{u}, \mu)}_\text{ \tiny NN approximation term evaluation} \right|^2
\end{equation}

where $N_{c}$ is the number of the collocation points considered.

\textbf{Hybrid training}

Training PINNs involves optimizing a loss function that combines both data-driven and physics-informed terms. These losses can originate from two sources: 
\begin{enumerate}
\item{Labeled Data Loss}, that ensures match between the NN output and simulated trajectories \eqref{eq:data_loss}, along with the corresponding physics-based penalty at these points \eqref{eq:physics_data_loss} 
\item{Collocation Loss}, which enforces the differential equation at thecollocation points where no labeled data are available \eqref{eq:physics_col_loss}, while also ensuring the NN satisfies the respective initial conditions \eqref{eq:col_ic}.
\end{enumerate}
The total loss function is then formulated as:
%The two distinct sources(labeled data and collocation points) of loss functions and their corresponding physics-informed aspects must be carefully managed to ensure their proper integration into the training process of PINNs. Specifically, during each training step, a single loss value is required. To achieve this, the weighted sum of all available loss terms is used to construct the objective function for the training procedure. Due to differences in scaling and the varying quality of data-driven losses compared to collocation point-driven losses, appropriately scaled weights $\lambda$ are crucial to balance the contributions of each loss component.
\begin{equation} \label{eq:total_loss}
\mathcal{L} = \lambda_d\mathcal{L}_{\text{data}}+\lambda_{dp}\mathcal{L}_{\text{data physics}}+\lambda_{cp}\mathcal{L}_{\text{col physics}} + \lambda_{ic}\mathcal{L}_{\text{ic col}}
\end{equation}
where $\lambda$ values balance contributions from data consistency and physics compliance. By minimizing 
$\mathcal{L}$, the network learns a solution that aligns with both empirical data and the governing equations, enhancing generalization in physics-informed learning.

\section{Training PINNs - Challenges}

The primary objective of this work is to develop an efficient and robust pipeline for training PINNs to simulate various dynamic components, accompanied by the corresponding Python code. The pipeline is designed to be fully parameterizable and easily customizable at every step. In the following sections, we outline the key stages of the methodology and provide a detailed justification for the approach taken in each stage.


%The ultimate goal of this work is to establish an efficient and robust pipeline for training PINNs to simulate different dynamic components and to provide the respective code in python. Each step, is fully parametrizable and editable, and below we try to present the main steps and justify our chosen approach for each of them.

\subsection{Setting up the ODEs}
The first step is to define the ODEs that govern the examined power system dynamic component without replacing any value. The inputs of the system $\mathbf{u}$ and the system parameters $\mu$ in \eqref{odes} are considered static and are stored separately. 
%\textcolor{red}{In the scenario where these values are dynamic, the most efficient way is to embed them into the system of ODEs with zero derivative.} 
In that way, it is easy to test the power system components across a range of values for each parameter, e.g. different inertia for a synchronous machine.
%In that way, we enable efficient testing of multiple dynamic components with different parameters, e.g. different inertia for a synchronous machine. 
These ODEs will be utilized by the ODE solver for dataset generation and for the loss function formulation during the NNs training. 
%In case, we want to consider dynamic inputs of the system $\mathbf{u}$, the ODEs need to be reformulated accordingly 

\subsection{Generating the Dataset}
The next step is to generate a dataset of simulated trajectories and one of collocation points, which will be used in training and testing phase of the PINN model. If real sensor data are available for the entire examined system, a portion of the simulated trajectories can be disregarded.

\textbf{Sampling the initial conditions}: 
We aim to approximate the behavior of a system within a specific input domain $\Omega_d$. We start by choosing the bounds of that domain, and also the number of different samples of each state. Different sampling methods are available, such as random, linear or \textbf{Latin Hypercube sampling} (LHS) \cite{mckay2000samplingcomparison}. LHS is preferred due to its ability to provide comprehensive coverage of the input space with fewer samples. This approach generates a diverse set of initial conditions for the system. Similarly, we will sample the initial conditions for the collocation points.

\subsection{Generating the trajectories}
As previously discussed, obtaining a closed-form solution is not feasible, necessitating the use of numerical methods to approximate the solution. Given the stiffness of the power system's equations, the higher-order \textbf{Runge-Kutta 4-5} method provides satisfactory accuracy with a computational cost that is negligible during the training phase. 
Subsequently, defining the \textbf{time horizon} and the \textbf{number of points} within the time interval enables the generation of the desired number of trajectories. 
%As mentioned above, we cannot obtain the closed-form solution, thus relying on numerical methods to approximate the solution. Based on the stiffness of the power system's equations, the higher-order method \textbf{Runge Kutta 4-5} offers satisfactory accuracy with a computational cost that is negligible in that phase. 
%Next, defining the \textbf{time horizon} and the \textbf{number of points} within the time interval, allows for the generation of the selected number of different trajectories. 

\subsection{Preprocessing the Dataset}
% NNs require pairs of data, as mentioned earlier, and in our case, they function as approximators. 
For given initial conditions and the time $t$, NNs must approximate the state of the system at that time. As a result, it is essential to preprocess the generated trajectories to create this input-output format. These labeled data are used during training through the respective labeled losses \eqref{eq:data_loss},\eqref{eq:physics_data_loss} and for testing the NNs. For the collocation points, the desired input format is created by pairing each sampled initial condition with all the corresponding time points within the domain.


Additionally, it was observed that skipping certain points from the same trajectories 
significantly accelerate the training process while not compromising the model accuracy by using fewer points per trajectory. In other words, to maintain a high model accuracy it is important to have as many trajectories as possible (i.e. a broad range of initial conditions), but it is not necessary to have too many points along each trajectory.%can significantly accelerate the training process without compromising the model's accuracy. This approach enables the exploration of broader input domains while minimizing computational cost.


%Finally, additional methods such as input and output standardization or normalization have been applied. These techniques enhance the generalization capabilities of the trained NN and improve both training efficiency and test performance, addressing the challenges posed by the varying scales of the states $\mathbf{x}$.








\subsection{The architecture of Neural Network}

The complexity of the examined power system component directly affects the architecture of the developed NN. The more complex this system is, the more neurons and/or layers must be included. As for the input layer, it must have K+1 inputs and K outputs as presented in Fig.~\ref{fig:nn_architecture}, where K is equal to the different dynamic states of the examined system. The extra input expresses the time for which the NN will offer the predicted state. Additionally, initialization weights strategies such as xavier can be included in order to promote stable NN training. Based on the accuracy and time inference needs, ML models can be implemented, such as Kolmogorov-Arnold
Networks (KANs) \cite{Ellinas2024}.

\begin{figure}[t]
\centering
% \includegraphics[width=0.5\textwidth]{images/plot_2d_metrics.png}
\includestandalone{Plots/NNs/nn_architecture}
% \includegraphics[]{Plots/PINNS/results_point.pdf}
\caption{Architecture of the proposed PINN that approximates the state \( \mathbf{x}(t) \) at time \( t = t_0 + h \) based on the initial state \( \mathbf{x}_0 \) and the time t.}
\label{fig:nn_architecture}
\end{figure}

\subsection{Training challenges of the Neural Networks}

The training of the PINNs stands out as the most intricate process in this study, which
encompasses numerous components and a multitude of hyperparameters. Extensive experimentation was conducted to refine the training process and its hyperparameters. For training, the MSE loss function was selected due to its sensitivity to larger errors, which helps to effectively minimize significant deviations during the learning process. Regarding optimizers, various options were explored, with Adam and LBFG-S demonstrating superior performance, and a fixed
learning rate is preferred. In addition, early stopping can be integrated into the training pipeline to prevent overfitting. 

The choice of loss function weights $\lambda$ in \eqref{eq:total_loss} plays a crucial role in the efficient training of PINNs and can be either static or dynamic. To determine optimal initial weight magnitudes, we first perform a preliminary training phase using only $\mathcal{L}_{\text{data}}$. This step helps identify proper weight magnitudes, ensuring robust optimization and improved overall performance. In the actual training phase, a dynamic weighting strategy can be applied, where the weights of each loss component are adjusted based on either the training epoch or a strategy that considers the convergence behavior of individual loss terms. Inspired from \cite{heydari2019softadapt}, we explored a soft-adaptive weighting method, which falls under gradient-based approaches and dynamically adjusts loss weights based on their optimization dynamics. However, since this method resulted in comparable weight scaling and only slightly improved overall performance, we report results primarily using the static approach for clarity and conciseness.

%Then, for the dynamic approaches, the weights of each loss component can be adjusted based on the training epoch or on a strategy about the convergence behavior of each individual loss term. As a benchmark for this study, we used the static weights, but also other dynamic strategies are offered, from which the proposed soft adaptive strategy seemed to performing the best. 

%The loss function L is a weighted sum of four objectives, as established in (16). Hence, appropriate weights need to be selected for each component. Failing to do so may trap the training process in local minima, consequently leading to suboptimal performance. To achieve an optimal balance among the loss components, we carefully adjusted their contributions, initially assigning a contribution of λdata = 1 to the data loss, as error in this component could propagate and negatively impact the physics loss. Finally, the empirically selected weights are λboundary = λrhs = 10−3 for the boundary and rhs loss, and λphysics = 10−4 for the physics loss. These values were selected empirically to prioritize the data loss while ensuring balanced contributions from the other terms, leading to robust optimization and improved performance. In addition to the aforementioned method, we also experimented with two dynamic weighting methods, which adjust the weights of each loss component based on the convergence behavior of each individual term. These methods are grounded in gradient-based attention techniques and Neural Tangent Theory [37]. Our tests suggested that the scaling of the weights was consistent across all three methods, resulting in comparable weights for the different loss components. Therefore, considering that the results are similar across all three weighting methods, for the sake of readability, in the remainder of this Section, we report results obtained only with the first weighting method




\subsection{Performance assessment}
The developed PINN model will be evaluated in two distinct categories, accuracy and computation time:

\textbf{Accuracy}: A subset of the simulated data will be used as ground truth data. We calculate the MSE, the Mean Absolute Error (MAE), and the Maximum Absolute Error (Max AE) to evaluate the PINN's performance. MAE provides a linear measure of the average magnitude of errors. The Max AE measures the largest absolute error, drawing attention to the model's worst-case prediction performance. 

\textbf{Computation time}: In terms of time efficiency for approximating trajectories, it is crucial to highlight the reduced computation time required by PINNs compared to traditional numerical methods. As a benchmark, the time needed to obtain the solution for a single set and 100 sets of initial conditions is examined. This metric aims to demonstrate the efficiency of PINNs and their ability to parallelize computations effectively, making them well-suited for real-time applications and computationally demanding tasks.


Additionally, visualizations of the solutions and benchmarks can be highly valuable, providing insights into potential weaknesses of the proposed PINN model, particularly in approximating stiff trajectories.

\section{Experimental evaluation - Demonstration}

%We demonstrate the proposed methodology on a 4th order Synchronous Machine with Automatic Voltage Regulator and Governor Infinite-Bus system, which we detail below. This section also specifies the simulated dataset and NN parameters, and the training setup with its hyperparameters.
We apply the proposed methodology to a 9th- order system: a SM model equipped with an AVR and a Governor connected to an Infinite-Bus system. The following sections provide a detailed description of the system, including the parameters of the simulated dataset and the NN. Additionally, we outline the training setup along with its associated hyperparameters.

\subsection{CASE STUDY: The 4th-order Synchronous Machine with Automatic Voltage Regulator and Governor}

This section briefly presents the examined power system component. %The fourth-order SM model is fundamental for analyzing the dynamic behavior of power systems, and it is simpler and better understood than inverter-based resources (IBR). 
This study considers a 4th-order SM with an AVR and a governor connected to an infinite bus. The AVR and governor are represented by 3rd-order and 2nd-order models, respectively, thereby increasing the overall model complexity. 
%is simpler and more well-understood than converter-interfaced generation. The SM connects to an infinite bus. To increase the complexity of the tested system, we will add an AVR and a Governor, which have an order of 3 and 2 respectively. 
As a result, the final system will be a 9th-order system, with the following representation found in \cite{sauerandpi}:
% \begin{align}

\begin{multline}
\hspace{-1.2em}
% \noindent
\scriptsize
\begin{bmatrix}
\scriptsize
1 \\
\frac{2H}{\Omega_{B}} \\
T'_{do} \\
T'_{qo} \\
T_E \\
T_F \\
T_A \\
T_{CH} \\
T_{SV}
\end{bmatrix}
\hspace{-0.2em}\frac{d}{dt} \hspace{-0.2em}
\begin{bmatrix}
\delta \\
\omega \\
E'_q \\
E'_d \\
E_{fd} \\
R_f \\
V_R \\
P_M \\
P_{SV}
\end{bmatrix}
\hspace{-0.2em}= \hspace{-0.2em}
\begin{bmatrix}
\scriptsize
\omega \\
P_m - E'_d I_d - E'_q I_q - (X'_q - X'_d)I_d I_q - D\omega \\
-E'_q - (X_d - X'_d)I_d + E_{fd} \\
-E'_d + (X_q - X'_q)I_q \\
-\left(K_E + S_E(E_{fd})\right) E_{fd} + V_R \\
-R_f + \frac{K_F}{T_F} E_{fd} \\
K_A R_f - \frac{K_A K_F}{T_F} E_{fd} + K_A \left(V_{\text{ref}} - V_t\right)  -V_R \\
-P_M + P_{SV} \\
-P_{SV} + P_C - \frac{1}{R_D} \left(\frac{\omega}{\Omega_B}\right)
\end{bmatrix}
\end{multline}
% \end{align}
and 
\begin{align}
\begin{bmatrix}
(R_s + R_e) & -(X_q + X_e) \\
(X'_d + X_e) & (R_s + R_e)
\end{bmatrix}
\begin{bmatrix}
I_d \\ I_q
\end{bmatrix}
&=
\begin{bmatrix}
V_s \sin(\delta - \theta_{vs}) \\
V_s \cos(\delta - \theta_{vs}) \\
\end{bmatrix}
\end{align}
\begin{align}
\begin{bmatrix}
V_d \\ V_q \\ V_t \\ S_E(E_{fd})
\end{bmatrix}
&=
\begin{bmatrix}
R_e I_d - X_{ep} I_q + V_s \sin(\delta - \theta_{vs}) \\ 
R_e I_q - X_{ep} I_d + V_s \cos(\delta - \theta_{vs}) \\ 
\sqrt{V_d^2 + V_q^2} \\
0.098 * e^{(0.55*E_{fd})})
\end{bmatrix}
\end{align}
\begin{align}
\begin{bmatrix}
V_R^{\min} \\
0
\end{bmatrix}
\leq
\begin{bmatrix}
V_R \\
P_{SV}
\end{bmatrix}
\leq
\begin{bmatrix}
V_R^{\max} \\
P_{SV}^{\max}
\end{bmatrix}
\end{align}




The parameters used for the SM modeling are as follows: the damping factor is \( D = 2 \),  the inertia constant is \( H = 5.06 \) s and the stator resistance is \( R_s = 0 \) p.u. The direct-axis transient time constants are \( T_d' = 4.75 \) s, respectively, whereas the quadrature-axis transient time constant is \( T_q' = 1.6 \) s. 

The direct-axis synchronous, transient reactances are \( X_d = 1.25 \) p.u., \( X_d' = 0.232 \) p.u., respectively. Similarly, the quadrature-axis synchronous transient  reactances are \( X_q = 1.22 \) p.u., \( X_q' = 0.715 \) p.u.
The line and system parameters are as follows: the reactance of the line between the SM and the bus is \( X_{ep} = 0.1 \) p.u., while the resistance of the line is \( R_e = 0 \) p.u. The angular frequency of the system is \( \Omega_b = 314.159 \) rad/s, and the system input voltage magnitude is set to \( V_s = 1 \) p.u. The voltage phase angle is \( \theta_{vs} = 0 \) rad.

%can be found in \cite{Ellinas2024}. 
As for the AVR, the following parameters were used: the gain of the regulator is \( K_A = 20 \), and its time constant is \( T_A = 0.2 \, \text{s} \). The feedback gain is \( K_F = 0.063 \), with a feedback time constant of \( T_F = 0.35 \, \text{s} \). The excitation system gain is \( K_E = 1.0 \), and the excitation system time constant is \( T_E = 0.314 \, \text{s} \). The reference voltage is set to \( V_{\text{ref}} = 1.095 \). The output of the AVR is constrained within the limits \( V_R^{\min} = 0.8 \) and \( V_R^{\max} = 8 \).
The parameters used for the governor modeling are as follows: the steady-state power command is \( P_c = 0.7  \,\text{pu} \), and the regulation droop is \( R_d = 0.05 \, \text{pu}\). The governor time constant is \( T_{CH} = 0.4 \, \text{s} \), and the servo time constant is \( T_{SV} = 0.2 \, \text{s} \). The governor output is limited by a maximum servo power value of \( P_{SV}^{\max} = 1.0  \,\text{pu} \).
%\textcolor{red}{mention how to store them? and or that there is an established process to define the odes and the parameters and then the input domain for the simulation and the collocation points. WHY? Because we can assess the performance under different settings} SOSOSOS
\subsubsection{Input domain}
The next step is to define the input domain $\Omega_d$ from which we will sample different initial conditions for our simulations. The input domain is specified as follows: $\theta \in [-2, 2]$ rad, $\omega \in [-1, 1]$ rad/s, $E_d'$ = 0 pu, $E_q' \in [0.9, 1.1]$ pu, $R_F $ = 1 pu, $V_r $ = 1.105 pu, $E_{fd}$ = 1.08 pu, $P_{sv}$ = 0.7048 pu, and $P_m$= 0.7048 pu. 500 different sets of initial conditions were sampled within this state space with LHS sampling. The initial conditions for the collocation points were also sampled using the same input domain and sampling method.
\subsubsection{Obtaining the final datasets}
Next, we want to approximate the behavior of the SM under transient conditions, and therefore we will consider that the
solver was operated with a 1 ms timestep and a simulation period of 1 s. The classical RK45-solver as implemented in \texttt{scipy.integrate} will be deployed to generate the simulated trajectories. The subsequent processing of these outputs results in the set $\mathcal{X}_d$ represented in the input-output form $(\mathbf{x}_0,t),\mathbf{x} $, which contains $N_d$ = 500'000 data points. 

The initial conditions for collocation points are sampled with LHS independently from those used in the labeled data. However, both datasets share the same time instances as in $\mathcal{X}_d$ to maintain consistency in the temporal domain. 
%Using the corresponding sampled initial conditions of the collocation points and the same time points as in $\mathcal{X}_d$ 
The resulting second dataset $\mathcal{X}_c$ is represented in the input form $(\mathbf{x}_0,t)$, which contains $N_c$ = 500'000 data points. 
\begin{subequations}\label{eq:T1T2}
\begin{align}
\mathcal{X}_d & = \{(\mathbf{x}_0^{(i)}, t^{(i)}), \mathbf{x}^{(i)} \}_{i=1}^{N_{d}=500'000} \\
\mathcal{X}_c & =  \{ ({\mathbf{x}_0^{(i)}}, t^{(i)}) \}_{i=1}^{N_{c}=500'000}  
\end{align}
\end{subequations}

The dataset $\mathcal{X}_d$ is used for training, validation, and testing of the NNs through the respective data losses, with a trajectory distribution of 80\%, 10\%, and 10\%, respectively. In contrast, $\mathcal{X}_c$ is used exclusively for training purposes. If needed, training can be performed using only one of these datasets, depending on the specific requirements of the task. As mentioned earlier, for efficient training, we sample at regular intervals across the same trajectory. The sampling interval is 23 steps for data points and 19 steps for collocation points.

\subsection{Training the PINN}

A NN with four hidden layers and 64 nodes with the hyperbolic tangent (tanh) activation function in each layer is used to approximate the state of the above system. ML algorithms were implemented using PyTorch \cite{pytorch}. The LBFG-S optimizer \cite{LBFGS}  with a learning rate of 0.001 was used to update the trainable parameters within 750 epochs, using the MSE loss criterion. As for the weights of the losses, we used the following static weights: $\lambda_d = 1$, $\lambda_{dp} = 0.01$, $\lambda_{cp}= 0.001$ and $\lambda_{ic} = 0.01$. WandB \cite{wandb} was used to monitor and tune the hyperparameters, providing insightful information regarding the training and testing performance. All experiments were conducted on the High-Performance Computing (HPC) cluster at the Technical University of Denmark, utilizing a 16-core Intel Xeon 6226R processor with 256 GB of RAM and an NVIDIA V100 GPU with 16 GB of exclusive GPU memory. 
%We also incorporated the PI-KANs that have been implemented in \cite{Ellinas2024} in order to show the flexibility of the proposed framework to work with different NN architectures. The PI-KAN architecture consisted of 2 hidden layers with 14 neurons each.




%PINNs consistently demonstrate superior performance, offering significant speedups for both individual and large-scale predictions. This makes them particularly well-suited for real-time applications and computationally demanding tasks.


\subsection{Results}

%In this section we evaluate the performance of the implement PIML models that approximate the behavior of a 4th order SM with an AVR and a Governor. The aim was to assess the generalizability of the proposed framework for a higher-order system. We compare the performance of the PIML models with the ODE solver in the test set which is consisted of 50 unique trajectories.

In this section, we evaluate the performance of the implemented PINN models in approximating the behavior of a 9th-order system. The objective is to assess the generalizability of the proposed framework when applied to a higher-order system. To this end, we compare the performance of the PINN models against the ODE solver on a test set comprising 50 unique trajectories.


In terms of accuracy, the results highlight the capability of PINNs to simulate the
dynamic behavior of higher-order system with 9 state variables. PINN approximated the states of the system with a MAE score below $2.26 \times 10^{-3}$ and Max AE $44.85 \times 10^{-3}$, which was noted in the very first moments according to Fig.~\ref{fig:accuracy_results}. 
%Both PINN and PI-KAN approximated the state with a MAE score below $2.26 \times 10^{-3}$ and Max AE $44.91 \times 10^{-3}$, which was noted in the very first moments according to Fig.~\ref{fig:accuracy_results}. 
Although this level of accuracy may be sufficient for certain applications, the current project has highlighted areas where further refinement is possible.

\begin{table}[t]
\centering
\caption{Accuracy of PINN}
\label{time_table}
\scriptsize
\centering
\setlength{\tabcolsep}{3pt} % Adjust column separation
\renewcommand{\arraystretch}{0.9} % Adjust row separation
\begin{tabular}{p{1.5cm}cc}
\toprule
 MAE & MSE & Max AE  \\
\midrule
 $2.26 \times 10^{-3}$ & $10.04 \times 10^{-6}$ & $44.85 \times 10^{-3}$ \\
 %PI-KAN & $2.14 \times 10^{-3}$ & $9.34 \times 10^{-6}$ & $44.91 \times 10^{-3}$  \\

\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
% \includegraphics[width=0.5\textwidth]{images/plot_2d_metrics.png}
\includestandalone{Plots/accuracy_all}
% \includegraphics[]{Plots/PINNS/results_point.pdf}
\caption{Metric scores of a PINN approximating the states of a 9th-order SM. The error metrics are benchmarked against the ODE solutions.}
\label{fig:accuracy_results}
\end{figure}


In terms of time efficiency, it is crucial to highlight the reduced computation time
required by PINNs compared to traditional numerical methods. 

Table~\ref{time_table} shows that as the inference state space grows, the advantages of PINNs become even more pronounced. Such approaches excel in their ability to parallelize outputs, allowing for efficient assessment of a broad range of initial parameters.
%



\begin{table}[t]
\centering
\caption{Inference time (in ms).}
\label{time_table}
\scriptsize
\centering
\setlength{\tabcolsep}{3pt} % Adjust column separation
\renewcommand{\arraystretch}{0.9} % Adjust row separation
\begin{tabular}{p{1.5cm}ccc}
\toprule
 Used Method & Single Trajectory & 50 Trajectories & 500 Trajectories  \\
\midrule
 ODE solver & 10.81  & 54.06  & 5406.13 \\
 PINN & 1.952 &  3.82 & 8.59 \\
 %PI-KAN & 9.26 &  25.90 & 50.53 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[t]

% \includegraphics[width=0.5\textwidth]{images/plot_2d_metrics.png}
\includestandalone{Plots/trajectories}
% \includegraphics[]{Plots/PINNS/results_point.pdf}
\caption{Results for SM variables from the ODE solver and the trained PINN approximation for 3 different sets of initial conditions}
\label{fig:example_results}
\end{figure}

Finally, Fig.~\ref{fig:example_results} illustrates the trajectories of the state variables for the SM, generated using three randomly selected sets of test initial conditions. The results highlight that PINNs successfully capture the flow of the trajectories under varying initial conditions, adhering closely to the governing physical laws.








%this is for 50 and single trajectory
%Model: SM_AVR_GOV NN: DynamicNN 50 trajectories Mean time: 0.003820 std time: 0.024922 1 trajectory Mean time: 0.001952 std time: 0.003475

%this is for 500 trajectories
%Mean time for 500 traj 0.008591442108154297 std time for 50 traj 0.02136968628530365 Mean time for 1 traj 0.018186798095703127 std time for 1 traj 0.012260337777971655

%time for single from ode, just multiple, the time for 500 verifies it from logs
%0.010812268257141114  and std:  0.003751192115677172


\section{Conclusion and Future Work}
This work developed an efficient, unified pipeline tailored for training PINN models on power system components and introduced a comprehensive toolbox featuring synchronous machines, designed to seamlessly integrate as plug-and-play modules.  The proposed approach demonstrated that PINNs can effectively approximate high-order system dynamics while significantly reducing computational costs compared to traditional solvers. Our test case showcased the ability of PINNs to model a 9th-order SM with AVR and Governor controllers, highlighting their potential for higher-order power system components.

Despite these advantages, several challenges remain. PINNs' accuracy depends on training data quality, loss function weighting, and hyperparameter selection. In addition, future work will explore adaptive sampling strategies and improve the selection of collocation points \cite{lau2024pinnacle} to enhance model accuracy and training efficiency. Expanding the component library to include emerging technologies such as inverter-based resources (IBRs) will further broaden their applicability. Additionally, integrating real-world measurement data with simulated datasets can improve robustness and generalization. Finally, incorporating PINNs into commercial simulation platforms, such as PowerFactory or PSCAD, will facilitate industrial adoption and practical deployment.


%The scalability of the proposed pipeline allows seamless integration with existing power system simulation tools, making it a promising approach for future applications.

%Despite the demonstrated advantages, several challenges remain. The accuracy of PINNs is highly dependent on the choice of training data, loss function weighting, and NNs hyperparameters. While our implementation leveraged a static loss weighting strategy, further research into adaptive weighting mechanisms could enhance training efficiency and model accuracy. 


%Easy to train - adaptive sampling, collocation points, expand component library with emerging components such as inverters. Integration with commercial tools like power factory, to train and use PINNs. This approach
%demonstrated the scalability and validity of PINNs principles, underscoring their
%potential across broader applications.


%The level of accuracy and time efficiency of PINNs can be particularly advantageous in real-world scenarios where initial conditions are imprecise or vary. 
%would combine training data that effectively represent the state space with available real-world data.
%Incorporating real-world data could provide additional insights and improve model
%accuracy by capturing practical complexities and variations.
%making PINNs a superior choice for time-sensitive and large-scale simulations.
%which will then serve as a fertile ground for research. 

%Contributions:
%\begin{itemize}
    %\item We developed an efficient, unified pipeline tailored for training PINN models on power system components.
    %\item Created a comprehensive toolbox for power system components, featuring synchronous machines, designed to seamlessly integrate as plug-and-play modules into our pipeline.
    %\item Pioneered the training of a PINN to effectively simulate the dynamic behavior of a synchronous machine equipped with AVR and Governor controllers for the first time, showcasing their capabilities for higher order modeling.
%\end{itemize}

%This standardization ensures a consistent and reproducible approach, enabling researchers to build upon established methodologies and advance the application of PINNs in power systems analysis
%Framework and the code
%Train Pinns for SM AVR Governor for the first time, showcasing their capabilities for higher order modeling

%Future work will focus on improving the framework in several key aspects. First, adaptive sampling strategies and collocation point selection techniques will be explored to enhance model accuracy while reducing training costs. Second, the component library will be expanded to include emerging power system components, such as IBRs, to better address the evolving dynamics of modern grids. Third, integration with commercial simulation tools, such as PowerFactory or PSCAD, will be pursued to facilitate broader adoption of PINNs in industrial applications. 

%Another promising direction is the incorporation of real-world measurement data to complement simulated datasets. By combining physics-driven learning with real-world observations, PINNs can improve generalization and robustness, making them a promising alternative for real-time and large-scale simulations. 

Overall, this study underscores the potential of PINNs in power system dynamic modeling and transient analysis while highlighting areas for further research to enhance their practical applicability.


\bibliographystyle{IEEEtran}
\bibliography{bibliography}




\end{document}
