

\section{Introduction}

% 背景介绍

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{fig/tokenizer-4.drawio.pdf}
    \caption{Illustration of general pipeline and applications. Discrete tokenizers can be regarded as the \textbf{interface} between different input modalities and downstream tasks.}
    \label{fig:enter-label}
    \vspace{-3mm}
\end{figure*}


The rapid advancement of AI systems and Large Language Models (LLMs) has underscored the critical role of tokenizers as indispensable components in modern machine learning systems. Tokenizers serve as the bridge between raw, unstructured data from diverse modalities 
 % text, images, audio, and video  
and the discrete tokens that LLMs process. By transforming continuous or heterogeneous inputs into a sequence of discrete symbols, tokenizers enable LLMs to operate effectively across a wide range of tasks. As AI continues to evolve, the ability of LLMs to generate and comprehend complex information hinges significantly on the quality and design of their tokenization mechanisms. In this survey, we focus specifically on discrete tokenizers, which are uniquely suited for integration with LLMs due to their ability to discretize information while preserving semantic fidelity \cite{team2024chameleon,chen2024next}.


% 现有方法和进展
Tokenizers play multifaceted roles in various domains, including generation, understanding, and recommendation tasks. 
In generative applications, tokenizers decompose input signals (e.g., pixels in videos or words in text) into discrete tokens. Tokenizers enable autoregressive modeling for video synthesis \cite{tian2024visual} and text generation \cite{guo2025deepseek}, determining how well an LLM can produce coherent and contextually relevant outputs.
For comprehension tasks, tokenizers~\cite{team2024chameleon,chen2025janus} map raw modality inputs into the unified token format, directly influencing the token-based multimodal models' capacity to understand different modalities.
In recommender systems, tokenizers~\cite{rajput2023tiger,singh2024better} transform semantic embedding of user preferences and item attributes into semantic IDs, improving the generalization of personalized recommendations in short-video platforms and online shopping.
Despite their pivotal role and pervasive importance, there remains a notable gap in the literature: no comprehensive survey has systematically reviewed the design, application, and challenges of discrete tokenizers across these tasks.
% ~\gjt{do we need to illustrate the relation between tokenizer and discrete tokenizer?}
A thorough examination of discrete tokenizers is essential not only to trace the evolutionary trajectory and provide an overview of SOTA methods but also to highlight their current limitations and guide future research efforts toward addressing these challenges.


The significance of surveying discrete tokenizers cannot be overstated. 
First, generation and comprehension tasks form the cornerstone of AI applications, while recommender systems are integral to modern digital economies, powering applications such as e-commerce and content delivery platforms. 
Second, state-of-the-art approaches in generation \cite{chen2024next}, comprehension~\cite{yang2023teal}, and recommendation tasks \cite{li2023large} rely heavily on the capabilities of LLMs, which necessitate the discretization of input data through tokenizers. 
Third, tokenizers act as a universal interface that harmonizes different modalities (text, image, video, audio) \cite{guo2025deepseek,tian2024visual,tan2024sweettokenizer,zhang2023speechtokenizer} at both syntactic and semantic levels, ensuring seamless interaction between raw data and LLMs. 
Finally, the performance ceiling of any LLM-based system is inherently constrained by the quality of its tokenizer; suboptimal tokenization can lead to significant degradation in downstream task outcomes \cite{yang2023teal,yu2023magvit-v2}. Given these considerations, a detailed exploration of discrete tokenizers is imperative to advance our understanding of their impact and potential.




Our survey differs significantly from existing reviews in the field and aims to provide a comprehensive framework for understanding semantic tokenizers in contemporary AI systems. While previous works have focused on specific aspects -- traditional vector quantization techniques for compression~\cite{wu2019vector}, quantization methods in recommender systems~\cite{liu2024vector}, and VQ-VAE variants for representation learning~\cite{zheng2023online} -- this survey examines semantic tokenizers through the perspective of modern neural architectures and their role in enabling unified multimodal processing applications. 
Given the increasing centrality of tokenization~\cite{wang2024tokenization} in foundation LLMs and the rapid emergence of novel multimodal architectures, a thorough analysis of semantic tokenization techniques becomes essential for guiding future research directions. 

This survey is structured to provide a comprehensive overview of discrete tokenizers from the perspectives of mechanisms, applications, and challenges.
In Sec.~\ref{sec:mechanism}, we begin by formally defining tokenizers and delineating the key components involved in their design, with a particular emphasis on discrete quantization.
Sec.~\ref{sec:application} explores the practical applications of tokenizers in generation, comprehension, recommendation, and information retrieval tasks, highlighting the unique contributions of SOTA methods in each domain. 
In Sec.~\ref{sec:challenge}, we critically examine the current challenges and open problems associated with discrete tokenizers, offering insights into promising directions for future research. 
Through this systematic review, we aim to equip researchers and practitioners with a deeper understanding of discrete tokenizers and inspire innovations that address existing limitations in the field.


















