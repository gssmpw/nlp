\section{Challenges \& Future Directions} \label{sec:challenge}

As discrete tokenizers continue to evolve and find new applications in multimodal understanding and generation, several key challenges and opportunities emerge. 


\subsection{Challenges}

\paragraph{Trade-offs Between Compression and Fidelity.}
Existing visual tokenizers (such as VQ-GAN~\cite{esser2021vqgan}) achieve efficient generation by heavily compressing the input resolution (e.g., from 256$\times$256 to 16$\times$16), leading to inevitably loss of fine-grained information. 
On the other hand, increasing the token count can enhance quality, but it results in an exponential increase in sequence length and computational cost.
Striking an optimal balance between compression rate and generation quality remains an open challenge in the field.

\paragraph{Trade-offs Between Understanding and Generation.}
A single visual encoder faces challenges in simultaneously optimizing for both understanding and generation tasks. 
The DeepSeek Janus~\cite{chen2025janus} experiment highlights this issue, showing that when a VQ Tokenizer is used, multimodal understanding performance is significantly lower compared to using a specialized semantic tokenizer. 
To address this, Janus-Pro decouples the visual encoder into specialized tokenizers for understanding and generation. While this approach mitigates the issue, it also increases the model's complexity.


\paragraph{Codebook Collapse and Utilization.} Due to the reliance of discrete tokenizers on a finite-sized codebook, a fundamental challenge in training semantic tokenizers is the codebook collapse problem, where only a small subset of the codebook entries are effectively utilized while others remain "dead" or unused. 
This issue particularly affects VQ-VAE-based approaches and their variants, limiting the model's capacity to capture diverse semantic representations.
Recent lookup-free quantization methods~\cite{mentzer2023fsq}~\cite{yu2023magvit-v2} are emerging to adress this problem.

\paragraph{Cross-Modal Alignment and Consistency.} 
Ensuring consistent semantic alignment between tokens from different modalities presents a significant challenge in multimodal systems. 
Current methods often struggle to maintain semantic consistency when processing multiple modalities simultaneously (e.g. text, image, video), particularly in real-time scenarios. 
In tasks such as video summarization~\cite{li2023blip}, where both temporal and spatial elements need to be aligned, ensuring synchronization across tokens becomes even more challenging.


\paragraph{Integration with Foundation Models.} 
While semantic tokenizers show promise in bridging different modalities with large language models (LLMs), their integration presents several challenges. 
These include aligning token vocabularies with LLM architectures, managing computational overhead, and ensuring effective knowledge transfer between foundation models and tokenized representations.
Additionally, the mixed input of multimodal tokens can introduce distributional discrepancies between modalities,  which can cause instability during training.


\subsection{Future Directions}

\paragraph{Adaptive and Dynamic Tokenization.} Future research should focus on developing more flexible tokenization frameworks that can dynamically adapt to different types of input data and task requirements. This includes investigating methods for automatic vocabulary size adjustment and hierarchical tokenization structures that can capture both fine-grained details and high-level semantic concepts.

\paragraph{Efficient Training and Inference.} Development of more efficient training and inference methods represents a crucial direction for future research. This includes exploring lightweight architectures specifically designed for resource-constrained environments and investigating parameter-efficient adaptation methods while maintaining high-quality representations.


\paragraph{Architectural Innovations.}
Unified tokenization frameworks~\cite{yang2023teal} aim to simplify modality-specific encoders. Byte-level models like BLT~\cite{pagnoni2024blt} bypass tokenization entirely by processing raw bytes, potentially resolving alignment issues.
More innovation can be made in model architecture and problem definition to enable the tokenizer to have better efficiency and generalization.


