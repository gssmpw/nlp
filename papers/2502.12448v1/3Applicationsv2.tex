\section{Applications} \label{sec:application}


Discrete tokenization was initially widely adopted in the field of Natural Language Processing (NLP) for segmenting sentences into subwords \cite{bpe2016,wu2016google,kudo2018sentencepiece}. Building on this foundation, tokenization techniques have since been extended to non-textual modalities, garnering significant attention and achieving substantial advancements \cite{team2024chameleon,xie2024show,chen2025janus}. These developments have paved the way for constructing fully token-in-token-out multimodal large language models \cite{yin2023survey}, enabling seamless integration of diverse data types within a unified framework. 
Meanwhile, discrete tokenizers applied in LLM-based generative recommendation models \cite{li2023large} have gradually garnered attention as a means to complement or even replace traditional ID features in conventional recommender systems.

In this section, we discuss the recent advances of discrete tokenizers applied in generation, comprehension, recommendation, and information retrieval, and propose a unified framework to summarize and categorize the existing studies systematically.



\subsection{Generation}


Tokenizers, exemplified by Byte Pair Encoding (BPE), were initially widely adopted in the field of generation task of NLP to transform sentences into discrete tokens, thereby aligning with the requirements of transformer-based backbones in large language models (LLMs).

Inspired by discrete representations in NLP, VQ-VAE \cite{van2017vqvae} and VQ-GAN \cite{esser2021vqgan} employ vector quantization (VQ) to partition raw pixels into a fixed number of tokens as the pioneers in image generation. To reduce the sequence length of the quantized feature map and improve the image fidelity for autoregressive modeling, \cite{lee2022rqvae} introduces the residual quantization (RQ) mechanism \cite{juang1982multiple} to represent the latent space in a residual way. 
% omnitokenizer, titok，sweettokenizer，cosmos，vidtok，
To overcome the limitations of 2D physical patch positions in prior tokenizers, SEED \cite{ge2023planting} introduces a VQ-based image tokenizer that generates discrete visual codes with 1D causal dependency and high-level semantic representations.
As a contemporaneous work, TiTok \cite{yu2024titok} introduces the 1-dimensional tokenizer to learn the compact latent tokens independent of the input resolution and reduce the inherent redundancy in adjacent image pixels. To facilitate the training efficiency, a two-stage training strategy including supervision with proxy code and decoder fine-tuning is also proposed. 


MAGVIT proposes the 3D-VQ architecture to construct the spatial-temporal tokenizer, which models the temporal dynamics in the video generation task. To further improve the 
generation quality and solve the codebook collapse of the VQVAE, MAGVITv2 \cite{yu2023magvit-v2} propose the Lookup-Free Quantization (LFQ) as the replacement of the VQ. It also improves the tokenizer's encoder and decoder architecture, such as causal 3D-CNN and sampler operation, to achieve a visual tokenizer capable of tokenizing images and videos using a shared codebook.
Instead of using 3D-CNN as the backbone, Omnitokenizer \cite{wang2024omnitokenizer} adopts the spatial-temporal decoupled transformer and a progressive training schedule to joint image and video tokenization.
To further break the limitation of the rasterization order in 2D patches and achieve a higher compression ratio, SweetTokenizer \cite{tan2024sweettokenizer} encodes the original image/frame patches into the token sequence with the cross-attention query auto-encoder (CQAE), significantly reducing the number of tokens required for discretization.
Instead of using LFQ like MAGVITv2, Cosmos \cite{agarwal2025cosmos} and VIDTOK \cite{tang2024vidtok} introduce the Finite Scalar Quantization (FSQ) \cite{mentzer2023fsq} to generate discrete tokens, where each dimension is quantized to a small, fixed set of values.


% \paragraph{Audio Generation.} 
Audio generation also faces the same challenge as image generation, which is how to compress the original signal into low bit-rate streams with high fidelity. 
SoundStream~\cite{zeghidour2021soundstream} adopts a fully causal convolution architecture in tokenizer's backbone structure, which ensures that the network encodes and decodes audio signals based solely on previous samples.
iRVQGAN~\cite{kumar2024improvedRVQGAN} decouples code lookup and code embedding by performing code lookup in a low-dimensional space, while the code embeddings reside in a higher-dimensional space. Additionally, it applies L2-normalization to the codebook vectors, converting Euclidean distance into cosine similarity, which improves both stability and quality.
HiFi-Codec \cite{yang2023hificodec} adopts grouped residual vector quantization (GRVQ) to address the issue in RQ models, where the first layer contains most of the information while subsequent layers contain less. 



\subsection{Comprehension}

Autoregressive (AR) large language models have ushered in a new era in the field of artificial intelligence, marking a significant milestone in the advancement of generative and comprehension capabilities. Tokenizers, discretizing any modality into a token sequence, facilitate the development of universal interfaces that bridge diverse data types and enable seamless interaction with transformer-based LLM.
TEAL \cite{yang2023teal} and AngGPT \cite{zhan2024anygpt} adopt modality-specific tokenizers to discretize image, text, and audio information into one discrete token sequence for token-in-token-out multimodal LLM.
To solve the restriction of encoding images/videos to a fixed number of tokens irrespective of the original visual content, 
LaViT \cite{jin2024lavit} and ElasticTok \cite{yan2024elastictok} generate dynamic discrete visual tokens maintaining high-level semantics. 
With the tokenizer as the universal interfaces of text and visual modalities and large-scale pretraining, Chameleon \cite{team2024chameleon} and Show-O \cite{xie2024show} demonstrate the broad and general capabilities and achieve promising performance in various downstream applications of generation and comprehending tasks.




As for audio comprehension, recent works increasingly focus on reconstructing audio signals from multiple informational perspectives, not only emphasizing the fidelity of local detail reconstruction but also considering the semantic information of the discretized tokens.
RepCodec~\cite{huang2023repcodec} learns a vector quantization codebook by reconstructing speech representations from speech encoders like HuBERT~\cite{hsu2021hubert}.
Furthermore, SpeechTokenizer~\cite{zhang2023speechtokenizer} unifies both semantic and acoustic tokens, hierarchically disentangling different aspects of speech information across multiple RQ layers, which boosts the alignment of speech and language tokens.
More comprehensively, NeuralSpeech-3~\cite{ju2024naturalspeech} introduces a factorized neural speech codec that decomposes complex speech waveforms into separate subspaces representing attributes like content, prosody, timbre, and acoustic details, and then reconstructs high-quality speech from these factors.



\subsection{Recommendation}


Existing recommender systems (RS) predominantly rely on unique IDs randomly assigned to users and items, such as ItemIDs or UserIDs. The ID representation lacks inherent semantic information and heavily relies on extensive historical interactions to learn the ID embeddings.
Consequently, the acquired recommendation model struggles to generalize to unseen data, resulting in suboptimal performance, particularly in cold-start scenarios and long-tail user recommendations. Notably, in the recommendation domain, semantic IDs are commonly used to represent discrete semantic tokens for items or users. Therefore, we use semantic ID and semantic token interchangeably in this part.

To address these critical limitations of traditional ID-based approaches, integrating semantic IDs into recommender systems has garnered increasing attention from both industries and academics.
First, semantic IDs offer a robust solution to the challenges of data sparsity, cold-start problems, and long-tail distribution. Unlike conventional IDs, which rely solely on interaction data (e.g., user clicks), semantic IDs leverage content semantics -- such as textual descriptions or visual features --to generate meaningful initial representations for new users or items, thereby mitigating the above issue. 
Second, semantic IDs facilitate cross-domain and cross-modal recommendations by acting as a bridge between different domains, enabling knowledge transfer and improving generalization across heterogeneous data sources. 
Finally, semantic IDs align seamlessly with the input requirements of large language models (LLMs), enabling the seamless integration of these identifiers into LLM-driven systems. This compatibility not only facilitates the inheritance of key properties of LLMs, such as scaling laws, but also allows recommendation systems to benefit from the continuous improvements in model capacity and generalization that arise from scaling up data and model size. By leveraging the intrinsic connection between semantic IDs and LLMs, recommendation systems can achieve enhanced performance, scalability, and adaptability, paving the way for more robust and intelligent personalized recommendation paradigms.

As the pioneer, TIGER~\cite{rajput2023tiger} first proposes the generative recommendation framework, including the content embedding generator, semantic ID tokenizer, and Transformer-based autoregressive recommender. The content embedding is extracted from the item text content by the text encoder. The core component -- semantic ID tokenizer -- is based on RQVAE, which is introduced to quantize the content embedding into sequential semantic tokens with a hierarchical structure. Item semantic tokens are chronologically organized to the user historical interaction sequence as the input of the autoregressive recommender. Inspired by the success in the academic datasets, \cite{singh2024better} applies the semantic tokens generated by TIGER and replaces the original video ID in the real-world ranking model of the YouTube platform. The online industry results show that the semantic ID scheme improves the generalization in cold-start items and achieves comparable performance in the overall items.

For incorporating high-order collaborative knowledge in LLM-based recommendations, TokenRec \cite{qu2024tokenrec} proposes a masked vector-quantized tokenizer (MQ-Tokenizer), which quantizes collaborative embeddings of user and item into discrete tokens. The MQ-Tokenizer randomly masks the input collaborative embeddings to enhance the token generalization capability.
To enhance the transferability of sequence modeling and improve its performance in cross-domain scenarios, VQ-Rec \cite{hou2023learning} quantize the BERT embedding of item text based on product quantization (PQ) and conduct code-embedding alignment to achieve transfer capacity in downstream domains.
Instead of utilizing unstructured VQ and PQ, LC-Rec \cite{zheng2024adapting}, LETTER \cite{wang2024learnable}, and CoST \cite{zhu2024cost} adopts the RQ mechanism to create discrete hierarchical item tokens based on LLaMA embedding~\cite{touvron2023llama}.
Different from the above methods, ColaRec \cite{wang2024content}, SEATER \cite{si2024generative}, and QARM \cite{luo2024qarm} use k-means algorithm to conduct hierarchical clustering and generate tree-structured item tokens from the content or collaborative embedding.

\subsection{Information Retrieval}

With the advancement of pre-trained language models, generative information retrieval \cite{li2024matching} has emerged as a novel paradigm, gaining increasing attention in recent years. For the sake of terminological consistency, the concept of doc identifiers in the Information Retrieval (IR) domain is also referred to as the doc token in the following discussion.


DSI \cite{tay2022transformer} explores the semantically structured doc token by conducting the hierarchical k-means algorithm on doc embeddings, allowing semantically similar documents to share the same prefix tokens.
Ultron \cite{zhou2022ultron} introduces the PQ mechanism to compress the dense vector space from the pre-trained encoder into compact discrete token space.
GenRet~\cite{sun2024learning} and LMINDEXER~\cite{jin2023language} utilize the encoder-decoder transformer to autoregressively generate the semantic continuous representation, which is quantized into discrete tokens based on maximum inner-product with learnable codebook tables.
To further minimize the distortion error between the original and approximated representations, RIPOR \cite{zeng2024scalable} adopts the RQ mechanism to capture the hierarchical document structure and reduce the doc token length for inference efficiency.