\section{Taxonomy of Discrete Tokenizer} \label{sec:mechanism}



\begin{table*}[h!]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Methods} & \textbf{Modality} & \textbf{Backbone} & \textbf{Quantizer} & \textbf{Task} \\ \midrule
VQVAE~\cite{van2017vqvae} & Image & CNN & VQ & Generation \\
VQGAN~\cite{esser2021vqgan} & Image & CNN & VQ & Generation \\
ViT-VQGAN~\cite{yu2021vit-vqgan} & Image & Transformer & VQ & Generation \\
RQVAE~\cite{lee2022rqvae} & Image & CNN & RQ & Generation \\
LQAE~\cite{liu2024lqae} & Image & CNN & VQ & Generation \\
SEED~\cite{ge2023seed} & Image & Transformer & VQ & Generation, Comprehension \\
TiTok~\cite{yu2024titok} & Image & Transformer & VQ & Generation \\
MAGVIT~\cite{yu2023magvit} & Image, Video & CNN & VQ & Generation \\
MAGVIT-v2~\cite{yu2023magvit-v2} & Image, Video & CNN & LFQ & Generation \\
OmniTokenizer~\cite{wang2024omnitokenizer} & Image, Video & Transformer & VQ & Generation \\
SweetTokenizer~\cite{tan2024sweettokenizer} & Image, Video & Transformer & VQ & Generation, Comprehension \\
Cosmos~\cite{agarwal2025cosmos} & Video & CNN, Transformer & FSQ & Generation \\
VidTok~\cite{tang2024vidtok} & Video & CNN & FSQ & Generation \\
TEAL~\cite{yang2023teal} & Image, Audio, Text & Transformer & VQ & Comprehension \\
AnyGPT~\cite{zhan2024anygpt} & Image, Audio, Text & Transformer & VQ & Generation, Comprehension \\
LaViT~\cite{jin2024lavit} & Image & Transformer & VQ & Generation \& Comprehension \\
Video-LaViT~\cite{jin2024video-lavit} & Video & Transformer & VQ & Generation \& Comprehension \\
ElasticTok~\cite{yan2024elastictok} & Image, Video & Transformer & VQ, FSQ & Generation, Comprehension \\
Chameleon~\cite{team2024chameleon} & Image, Text & CNN, Transformer & VQ & Generation, Comprehension \\
ShowO~\cite{xie2024show} & Image, Text & CNN, Transformer & LFQ & Generation, Comprehension \\
SoundStream~\cite{zeghidour2021soundstream} & Audio & CNN & RQ & Generation \\
iRVQGAN~\cite{kumar2024improvedRVQGAN} & Audio & CNN & RQ & Generation \\
HiFiCodec~\cite{yang2023hificodec} & Audio & CNN & GRVQ & Generation \\
RepCodec~\cite{huang2023repcodec} & Audio & CNN, Transformer & RQ & Comprehension \\
SpeechTokenizer~\cite{zhang2023speechtokenizer} & Audio & CNN, Transformer & RQ & Generation, Comprehension \\
NeuralSpeech-3~\cite{ju2024naturalspeech} & Audio & CNN, Transformer & VQ & Generation, Comprehension \\
TIGER~\cite{rajput2023tiger} & Text & MLP & RQ & Recommendation \\
SPM-SID~\cite{singh2024better} & Text & MLP & RQ & Recommendation \\
TokenRec~\cite{qu2024tokenrec} & Text & MLP & VQ & Recommendation \\
VQ-Rec~\cite{hou2023learning} & Text & MLP & PQ & Recommendation \\
LC-Rec~\cite{zheng2024adapting} & Text & MLP & RQ & Recommendation \\
LETTER~\cite{wang2024learnable} & Text & MLP & RQ & Recommendation \\
CoST~\cite{zhu2024cost} & Text & MLP & RQ & Recommendation \\ 
ColaRec~\cite{wang2024content} & Text & MLP & VQ & Recommendation \\
SEATER~\cite{si2024generative} & Text & MLP & VQ & Recommendation \\
QARM~\cite{luo2024qarm} & Text & MLP & VQ & Recommendation  \\
DSI~\cite{tay2022transformer} & Text & Transformer & VQ & Information Retrieval \\
Ultron~\cite{zhou2022ultron} & Text & Transformer & PQ & Information Retrieval \\
GenRet~\cite{sun2024learning} & Text & Transformer & VQ & Information Retrieval \\
LMINDEXER~\cite{jin2023language} & Text & Transformer & VQ & Information Retrieval \\
RIPOR~\cite{zeng2024scalable} & Text & Transformer & RQ & Information Retrieval \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{
Taxonomy and summarization of discrete tokenizers across various modalities for tasks such as generation, comprehension, recommendation and information retrieval. 
It highlights the use of different quantization strategies and model architectures in addressing diverse tasks.
}
\vspace{-3mm}
\end{table*}



\subsection{General Pipeline}
\label{sec:procedure}



The concept of discrete tokenizer originates from natural language processing tasks, which aims to break down the input into several finest tokens from a pre-defined dictionary~\cite{bpe2016}.
However, for modalities containing much denser information (e.g.\ image, video, audio), directly tokenizing raw input is not feasible. 
Therefore, a standardized process must be established to discretize various modalities universally.
To this end, we summarize the discrete tokenization pipeline, which consists of three steps:


\paragraph{(1) Encoding.}
An encoder $Enc$ is required to map input data $\boldsymbol{x}$ from a lower-dimensional tensor to a higher-dimensional latent vector $\boldsymbol{z} \in \mathbb{R}^{d}$:
\begin{equation}\label{equ:encoding}
\begin{gathered}
    \boldsymbol{z} = Enc(\boldsymbol{x})
\end{gathered}
\end{equation}

This transformation allows the model to capture complex patterns and relationships within the data. 
By leveraging various architectures, such as neural networks or transformers, the encoder learns to represent the input in a space that highlights relevant features.



\paragraph{(2) Quantization.}
Given a set of continuous vectors $\boldsymbol{z} \in \mathbb{R}^d$ forming a matrix $\boldsymbol{Z} \in \mathbb{R}^{n\times d}$, vector quantization aims to learn a codebook matrix $\boldsymbol{C} \in \mathbb{R}^{m\times d}$ containing $m$ discrete codes, where typically $m \ll n$. The quantization process can be formalized as a mapping function $Q: \boldsymbol{Z} \rightarrow \boldsymbol{C}$ that assigns each input vector $\boldsymbol{z}$ to its nearest code $\boldsymbol{c}_j$ in the codebook:

\begin{equation}\label{equ:quantization}
\begin{gathered}
    [j, \boldsymbol{c}_j] = Q(\boldsymbol{z})\\
    j = \underset{k=1,...,m}{\argmin} D(\boldsymbol{z}, \boldsymbol{c}_k),
\end{gathered}
\end{equation}
where $D$ measures the distance between vectors, $j$ is the quantized codeword, and $\boldsymbol{c}_j$ is the quantized representation.

\paragraph{(3) Supervision.}
A decoder $Dec$ is adopted to reconstruct the original input from the discretized representation $\boldsymbol{c}$. 
\begin{equation}\label{equ:decoding}
\begin{gathered}
    \hat{\boldsymbol{x}} = Dec(\boldsymbol{c})
\end{gathered}
\end{equation}

By minimizing the reconstruction error between the original input $\boldsymbol{x}$ and its reconstructed version $\hat{\boldsymbol{x}}$, the model is encouraged to preserve important information in the tokenized form.
\begin{equation}
\mathcal{L}_{rec}=\left\|\boldsymbol{x}-\boldsymbol{\hat{x}}\right\|_2^2
\end{equation}

Straight-Through Estimator (STE)~\cite{van2017vqvae} allows gradient flow through the discrete quantization operation and the codebook to be learned and updated automatically. 
STE simply copies the gradients directly:

\begin{equation}
\frac{\partial \boldsymbol{c}}{\partial \boldsymbol{z}} \approx \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{z}}=\mathbf{I}
\end{equation}


Incorporated with commitment loss $\mathcal{L}_{cmt}$, the whole process is differentiable and the encoder output is limited around codebook vectors:
\begin{equation}
\mathcal{L}_{cmt}=\left\|\operatorname{sg}\left[\boldsymbol{z}\right]-\boldsymbol{c}\right\|_2^2+\left\|\boldsymbol{z}-\operatorname{sg}[\boldsymbol{c}]\right\|_2^2,
\end{equation}
where $\operatorname{sg}$ stands for the stopgradient operator.

Through the tokenization process, the inputs with similar contents are mapped to closer discrete, low-dimensional tokens. 
As a result, these quantized representations, characterized by their semantic, discrete, and low-dimensional properties, are increasingly employed in token-prediction models, particularly large language models (LLMs).

\subsection{Backbone Network}

\paragraph{MLP-based.}
MLP-based approaches were commonly used in early stages. However, in recent years, many methods in the recommendation domain have also adopted MLP as the backbone. 
This shift is primarily due to the fact that in recommender systems, each entity (e.g.\ user, item) is typically represented by an embedding vector. 
To effectively model and process these embeddings, MLP is employed to perform latent space mapping~\cite{rajput2023tiger,singh2024better}, enabling more sophisticated feature interactions and enhancing recommendation performance.

\paragraph{CNN-based.}
In the context of image modalities, CNN-based network architectures, such as UNet~\cite{ronneberger2015unet}, are widely adopted as the backbone for feature extraction~\cite{esser2021vqgan}. 
Similarly, for audio modalities, CNNs can be effectively utilized by converting audio signals into spectrograms~\cite{zeghidour2021soundstream}, allowing the network to process the spectral features. 

When handling even higher-dimensional modalities, such as video, these architectures are extended to 3D-CNNs
, which incorporate an additional temporal dimension alongside the spatial dimensions, enabling the capture of both spatial and temporal information for more comprehensive analysis~\cite{yu2023magvit}.


\paragraph{Transformer-based.}
The development of attention mechanisms and transformer models~\cite{dosovitskiy2020vit} has demonstrated significant enhance performance across various tasks. 
For text information, we can use the transformer architecture to extract relevant information and perform sequence-to-sequence reconstruction~\cite{sun2024learning}.
For 2D inputs, such as images, the data is typically divided into patches, and these patches are then treated as a sequence, fed into the transformer's encoder-decoder framework, as seen in~\cite{yu2021vit-vqgan}. 
For 3D inputs, such as videos, the data is sliced along the temporal dimension, converting 2D patches into 3D tubes, as seen in~\cite{wang2024omnitokenizer}.
The key advantage of transformers lies in their scalability, allowing for the efficient handling of large parameter sizes. As a result, many of the latest state-of-the-art methods in various fields now adopt transformers as the backbone to leverage their ability to capture complex data features~\cite{yu2024titok}.


More recently, the transformer-based structure has also been adopted to reduce token number through cross-attention querying.
For example, SweetTokenizer~\cite{tan2024sweettokenizer} adopts the approach of Q-Former~\cite{li2023blip} by learning a set of fixed input queries to reduce redundant information embedded in neighbor patches.


\subsection{Quantization Method}

\paragraph{Vanilla Quantization.}
Pioneered by Shannon et al.~\cite{shannon1959coding}, vanilla vector quantization encompasses methods that construct codebooks with explicit structures such as clusters~\cite{nister2006scalable} or trees~\cite{babenko2015tree}. 
The quantization procedure can be described by Eq~\eqref{equ:quantization}.

Recent works argue that lexical tokens inherently contain higher-level semantic information, and thus, the latent space of text tokens can come to the rescue for image discretization.
For example, LQAE~\cite{liu2024lqae} quantizes image embeddings using a pretrained language codebook (e.g., BERT~\cite{devlin2018bert}), aligning images and text without paired data, thus enabling few-shot image classification with LLM.

\paragraph{Level-wise Quantization.}
Vanilla Quantization often introduces rough errors during the quantization process, raising the challenge of improving the approximation of vectors using a codebook. 

One possible solution is the level-wise quantization approach.
This method suggests that after quantizing at each level, the codebook from the subsequent level is used to approximate the quantization error of the current level, as seen in RQ~\cite{juang1982multiple}. 
It employs a new sequential quantization process where each stage k quantizes the residual from previous stages:


\begin{equation}\label{equ:rvq}
\boldsymbol{r}_{s+1} = \boldsymbol{r}_{s} - C_s(\boldsymbol{r}_s)
\end{equation}
where $\boldsymbol{r}_s$ denotes the $s_{th}$ stage of residual vector with $\boldsymbol{r}_0=\boldsymbol{x}$, $C_s \in C$ corresponds to the $s_{th}$ codebook, and a quantization result of $[C_0(\boldsymbol{r}_0), ..., C_s(\boldsymbol{r}_s), ..., C_S(\boldsymbol{r}_S)]$ could be obtained with a total of $S$ codewords instead of a single codeword. 
By iteratively accumulating these corrections across multiple levels, the overall approximation becomes more precise, leading to a reduction in quantization error and enhancing the quality of the results. This approach has been explored as an effective means to refine vector quantization.


RQ adopts a greedy approach by choosing the nearest neighbor within the current layer, but this method doesn't guarantee a global optimum. To overcome this, Additive Quantization (AQ)~\cite{babenko2014additive} improves upon it by using beam search to aggregate one selected code per codebook, representing vectors as sums of $S$ codewords, and applying joint optimization for enhanced performance:

\begin{equation}
\boldsymbol{x} \approx \sum_{s=1}^S C_s(\boldsymbol{x})
\end{equation}


\paragraph{Group-wise Quantization.}
Another approach, known as group-wise quantization, suggests that splitting a vector into multiple subcomponents and quantizing each part separately can also help reduce the quantization error. By handling smaller portions of the vector independently, this method allows for more accurate representation and finer control over the quantization process. 
For example, Product Quantization (PQ)~\cite{jegou2010product} established another parallel approach by decomposing high-dimensional vectors into $S$ separately orthogonal quantized subvectors:

\begin{equation}
C(\boldsymbol{x}) = (C_1(\boldsymbol{x}_1), ..., C_s(\boldsymbol{x}_s), ..., C_S(\boldsymbol{x}_S)),
\end{equation}
where $\boldsymbol{x} = \text{concat}(\boldsymbol{x}_1, ..., \boldsymbol{x}_s, ..., \boldsymbol{x}_S)$. This decomposition also enables efficient distance computation through smaller lookup tables. 

Optimized Product Quantization (OPQ)~\cite{ge2013optimized} extends this by learning an optimal rotation matrix R that minimizes quantization distortion while maintaining subspace independence. 

\paragraph{Lookup-free Quantization.}
The methods discussed above typically involve maintaining a codebook and performing lookups. However, recent research~\cite{yu2023magvit-v2} has shown that as the codebook size increases, optimization becomes more challenging. This is due to the fact that many entries in a larger codebook remain unused, which can hinder the optimization process and degrade performance.
As a result, some approaches have been proposed that eliminate the need for lookup tables or traditional codebooks. 

Finite Scalar Quantization (FSQ) \cite{mentzer2023fsq} projects the 
% VAE~\gjt{Variational Auto Encoder (VAE)}
encoded representation to a few dimensions via transformation $f$, with each dimension rounded to a small set of fixed values, forming an implicit codebook:
\begin{equation}
    Q(\boldsymbol{z}) = \text{round}(f(\boldsymbol{z}))
\end{equation}

By carefully bounding each channel, FSQ can create a codebook of any desired size. 
For a vector \( \boldsymbol{z} \) with \( d \) channels, mapping each entry \( \boldsymbol{z}_i \) to \( L \) values (e.g., \( \boldsymbol{z}_i \mapsto \lfloor L/2 \rfloor \tanh(\boldsymbol{z}_i) \) followed by rounding) results in  \( L^d \) possible vectors.

More aggressively, Lookup-Free Quantization (LFQ)~\cite{yu2023magvit-v2} decomposes the latent space into binary dimensions, and each dimension is quantized independently with $\boldsymbol{c}_i = \{-1,1\}$. 
% This enables efficient handling of large vocabularies while maintaining differentiability. 
The latent space of LFQ is decomposed as the Cartesian product \( C = \prod_{i=1}^{\log_2 K} C_i \). Given \( \boldsymbol{z} \in \mathbb{R}^{\log_2 K} \),
The \( \arg \min \) can be computed using the sign function as:
\begin{equation}
Q(\boldsymbol{z}_i) = \text{sign}(\boldsymbol{z}_i) = -1 \cdot (\boldsymbol{z}_i \leq 0) + 1 \cdot (\boldsymbol{z}_i > 0).
\end{equation}


Experiments show that LFQ can grow the vocabulary size in a way
benefiting the generation quality of language models.







