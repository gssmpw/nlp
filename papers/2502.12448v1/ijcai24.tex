%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
% \usepackage{cite}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{mydef}
% \usepackage{biblatex}
% \usepackage{natbib} 
% \usepackage[style=authoryear,maxnames=3,minnames=2]{biblatex}

\newcommand{\argmax}{\mathop{\arg\max}\hspace{0.3em}} \newcommand{\argmin}{\mathop{\arg\min}\hspace{0.3em}}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

% \title{Discrete Latent Representations: A Survey on Discrete Tokenizers in Multimodal Understanding, Generation, and Recommendation}

\newcommand{\gjt}[1]{{\color{red} [gjt: #1]}}


\title{From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval}

% \title{Discrete Tokenizers in AI: A Unified Perspective on Generation, Comprehension, Recommendation, and Information Retrieval}


% Single author syntax
% \author{
%     Jian Jia
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Jian Jia$^{1*}$
\and
Jingtong Gao$^{2*}$\and
Ben Xue$^1$\thanks{These authors contributed equally to this work.} \and
Junhao Wang$^1$\and
Qingpeng Cai$^1$\and
Quan Chen$^1$\and \\
Xiangyu Zhao$^2$\thanks{Corresponding author.}\and
Peng Jiang$^1$\And
Kun Gai$^1$ \\
\affiliations
$^1$Kuaishou Technology\\
$^2$City University of Hong Kong\\
\emails
\{jiajian, xueben, wangjunhao05, caiqingpeng, chenquan06, jiangpeng, yuyue06\}@kuaishou.com,
jt.g@my.cityu.edu.hk,
xianzhao@cityu.edu.hk
}
% \fi

\begin{document}

\maketitle

\begin{abstract}

Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). 
These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. 
Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. 
This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. 
We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design.
Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. 
Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. 
By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.



\end{abstract}


\input{1introductionv2}
\input{2VectorQuantization}
\input{3Applicationsv2}
\input{4Challenges}
\input{5Conclusion}


% \clearpage
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
% \bibliographystyle{apalike}
\bibliography{ijcai24}

\end{document}

