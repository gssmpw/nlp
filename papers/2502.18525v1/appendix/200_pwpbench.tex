\section{\bench{}}

\paragraph{Metrics}
\label{app:metrics}

We use individual metrics mentioned in the original datasets. When reporting results on \bench{}, we report marco average of all these metrics. In particular, 11/15 used Accuracy as their metric. However, due to complexity of dataset, these often goes beyond simple accuracy metrics and in some cases, the dataset is evaluated on multiple orthogonal metrics, instead of one. We detail, these metrics for each of the datasets.  

\begin{itemize}
    \item \textbf{SWT-Bench} evaluates generated tests by the agent, and reports 6 different metrics: Applicability, Success Rate, F-\>X, F-\>P, P-\>P, and Coverage. We report the average of all 6 metrics.
    \item \textbf{ChartMimic} evaluates generated code on various metrics such as accuracy of text, colors used, legend etc. We average all metrics similar to the original dataset.
    \item \textbf{Design2Code} evaluates generated code on various metrics such as accuracy of text, position, clip score, etc. We average all metrics similar to the original dataset. 
    \item \textbf{DSBench} has two categories, one containing MCQ questions, while the other containing generating code for Kaggle Competitions. We use 10/10 instances from each category in \bench{}-Lite. While MCQ questions are evaluated using Accuracy, the code generation part is evaluated using linear normalization between the baseline score (of the competition) and the score of the winner of competition.
\end{itemize}
