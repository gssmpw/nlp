\section{Conclusion}

In this work, we introduce \ours{}, a unified environment that challenges the prevailing tool-based paradigm by enabling direct interaction with IDEs through basic computer-use actions like typing and clicking. This approach allows for a wide range of tasks to be modeled without language- or domain-specific modifications.
Second, we introduce \bench{}, a unification of existing SWE datasets evaluated in \ours{}.
We find that general-purpose computer-use agents can approach or outperform previous state-of-the-art results, without any task-specific modifications. This suggests that the dominant paradigm of building specialized text-based tools for SWE agents may be superseded by end-to-end computer-use agents. However, our analysis reveals that state-of-the-art agents are still incapable of using the extensive set of tools available in \ours{}, and could perform better if they could use them.
Our work opens up an exciting new direction of development of computer-use agents for SWE tasks, an important step towards reaching truly general-purpose SWE agents. Finally, \ours{} provides a common platform that supports existing agents and benchmarks,  making it a foundation for future research on SWE agents.

\paragraph{Acknowledgments.} We thank Daniel Fried, Graham Neubig, and Zora Wang, Saujas Vaduguru, Atharva Naik, Riyaz Ahuja, Weihua Du for helpful feedback, Google and Open AI credit programs, and Convergent Research.
