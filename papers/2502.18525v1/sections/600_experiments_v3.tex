\section{Experiments}
\label{sec:experiments}

\input{tables/category_results}
\input{figures/successfull_tool_use}


\paragraph{Experimental setup.}

We evaluate two categories of baseline agents as described in Section~\ref{sec:agent}.
For each configuration, we test five state-of-the-art vision-language models (VLMs): Gemini-Flash-1.5, Gemini-Pro-1.5, GPT-4o, GPT-4o-mini, and Claude-3.5 Sonnet.
With the exception of Claude—which is natively trained for UI interaction—the remaining models are provided with SoM.

At each timestep, an agent receives an observation (with the observation space determined by its category) and returns an action.
The complete history of observations and actions is incorporated into the model's context.
For each task instance, the maximum number of iterations is capped at 20 steps; if the agent either exhausts these steps or issues a stop command, the environment's final state is evaluated using task-specific metrics (see Appendix~\ref{app:metrics} for full details).
Notably, the agent design remains the same throughout all tasks.
Due to computational and budget constraints, we evaluate on \bench{}-Lite, which has 300 task instances.

\subsection{Results}

\input{figures/resq/resq}
\input{figures/grounding}


Table~\ref{tab:model_performance} summarizes performance across different agent architectures and base models over the four categories of \bench{}.
As seen in the top half of the table, when using only primitive keyboard and mouse actions most agents have poor performance, with a maximum overall average of 10.2\%.
We attribute this poor performance primarily to limited visual grounding and an inability to interact effectively with the IDE—particularly for file editing and tool usage (see Section~\ref{sec:analysis} for further analysis).

In contrast, when agents are granted access to file editing and bash operations through API calls rather than relying solely on UI interactions, we observe consistent improvements across all categories, with maximum average accuracy reaching 46.8\%.
Among the evaluated models, the Claude computer-use agent performs best, likely because it is specifically trained for UI interactions.
We found that this agent is able to leverage basic IDE tools—such as HTML live preview, chart visualization, and file navigation—to boost performance on tasks requiring visual understanding and IDE navigation.
Importantly, we find for the first time that a single computer-use agent can achieve performance comparable to and often surpassing (See Appendix~\ref{app:results}) state-of-the-art methods across a wide variety of software engineering (SWE) tasks—encompassing multiple languages, modalities, and domains—while operating within a single unified environment and interface, and no specific hand-crafted tools, unlike existing SoTA methods.


Nonetheless, as detailed in Section~\ref{sec:agents_tools}, the models struggle to fully take advantage of the IDE tooling.
This is evidenced by the poor performance on the `General SWE' dataset, where tasks are often as simple as editing IDE settings and often require fewer than four clicks to complete.
As we show in Section~\ref{sec:assisted_analysis}, these tasks become simpler if the models could use the IDE tooling more effectively.

Overall, while the results point toward a promising direction for developing general computer-use SWE agents, significant improvements are still needed in visual grounding, tool usage, and planning. We analyze these next.

\subsection{Analysis}
\label{sec:analysis}

\paragraph{Agents Demonstrate Poor Visual Grounding Capabilities.}



Our qualitative analysis across multiple VLMs on \bench{} reveals significant limitations in visual understanding—even for basic IDE interactions.
We identify two primary failure modes.
First, models frequently fail to correctly identify the UI elements intended for interaction, as demonstrated in Figure~\ref{fig:grounding-1},~\ref{fig:grounding-2}.
In agents using set-of-marks (SoM), this issue manifests as incorrect element selection, while without SoM it leads to inaccurate mouse positioning.
Second, models struggle to comprehend the current UI state.
As shown in Figure~\ref{fig:uielements-1},~\ref{fig:uielements-2}, they consistently fail to recognize highlighted elements, cannot detect linter errors indicated by wavy underlines (Figure~\ref{fig:fileediting-1}), and often confuse active panels—resulting, for example, in typing into search bars rather than file editors (Figure~\ref{fig:uielements-1}).
While similar issues have been documented in web and OS domains~\cite{koh2024visualwebarenaevaluatingmultimodalagents, xie2024osworldbenchmarkingmultimodalagents}, these limitations were primarily observed in models without UI-specific training. However, our work, shows even models explicitly trained for UI interaction~\cite{anthropic2024developing}, including Claude-Computer Use, exhibit these issues in \ours{}—likely due to the increased complexity of the IDE interface.

\textbf{Agents Fail to Edit Files.}
File editing is a basic capability required in most SWE tasks.
However, we find that the deficiencies in visual grounding significantly impact the file editing capabilities of current agents that use basic actions (clicking and typing).
For example, even when provided with cursor location information in textual form, these models struggle to interpret such data amid complex UI elements.
Models fine-tuned for UI interactions still commit basic editing errors—such as incorrect indentation and text misplacement—and are unable to recover from these errors (see Appendix for examples). 
We speculate these limitations could stem from two factors: (i) model overfitting to user interfaces in their training domains, or (ii) the increased complexity of the \ours{} IDE interface, which contains substantially more interactable elements than typical web or OS environments. Addressing these limitations represents an important direction for future work.
Although direct file access via tool operations is available, UI-based editing confers unique advantages for tasks such as editing Jupyter notebooks, comparing changes, or modifying specific sections of large files.
These results underscore two limitations: (i) current VLMs are challenged by complex UI interactions beyond simple web/OS interfaces~\cite{xie2024osworldbenchmarkingmultimodalagents,koh2024visualwebarenaevaluatingmultimodalagents}, and (ii) the inability to effectively perform UI-based editing prevents agents from leveraging valuable IDE features that could have improved their performance.

\input{figures/ui_elements}



\paragraph{Claude Computer-Use Agent Demonstrates Basic IDE Tool Proficiency.}


Our analysis shows that the Claude Computer-Use agent successfully uses fundamental IDE functionalities, including file explorer navigation, file editing, search, browser-based live preview, and image generation and visualization capabilities. Figure~\ref{fig:successful_tool_use-1} demonstrates the agent's effective use of browser tools in UI replication tasks. Similarly, Figure~\ref{fig:resq_example} illustrates the agent's ability to coordinate multiple tools while editing specific lines in a repository, relying solely on screenshot observations and primitive keyboard/mouse actions.

Second, we hypothesize that the agent has additional abilities to use tools that can be uncovered through prompting or fine-tuning. 
To investigate this, we considered the symbol renaming task in our `General-SWE' benchmark, in which Claude initially achieves 0\% accuracy when attempting the task. 
However, when explicitly instructed to use the renaming tool, its accuracy improves to 50\% (see Appendix~\ref{app:results}). 

\paragraph{Agents Struggle to Use IDE Functionality.}
\label{sec:agents_tools}

Despite these successes with basic IDE functionality, computer-use agents demonstrate significant limitations when interfacing with more complicated IDE tools. We observe no successful instances of debugger usage or symbol listing operations. Models without specific UI interaction training struggle even with fundamental tools such as HTML live preview, image visualization, and graph generation capabilities. While Claude demonstrates competency with basic tools, it fails to effectively use advanced tools like profilers or debuggers.

To further evaluate these capabilities, we developed the `General-SWE' dataset, which focuses on software engineering activities (e.g., profiling, refactoring, debugging) that can be completed without direct code modification. Although these tasks sometimes require only 4-5 steps when using appropriate IDE tools, agents achieve minimal performance, highlighting substantial room for improvement.

\paragraph{Training Models to Use IDE Tools Better Would Improve Performance.}
\label{sec:assisted_analysis}


While a single computer-use agent design can perform well across a wide variety of tasks, our results indicate that these models do not fully exploit domain-specific tools.
As an indication of the potential for performance gains \textit{if} the agent was able to effectively use the IDE, we perform an ``assisted'' experiment.

For the assisted experiment, we manually engineer a set of API calls that are useful for the tasks.
For example, in Design2Code, the assisted agents have an API call for a live HTML preview, while for SweBench it has API calls for retrieving repository structure and symbol outlines.
Importantly, each API call is achievable using basic operations in the IDE, meaning that in principle an agent could learn to perform it.
To ensure that each API call is achievable in the IDE, we implement each API call by executing a fixed sequence of low-level IDE actions, with the details abstracted away from the agent.
A complete list of tools available in different environments is in Table~\ref{tab:assisted_tools} (see Appendix~\ref{app:results}).

Table~\ref{tab:assisted_comparison} compares the performance of these assisted agents with that of standard computer-use agents across four datasets for which we manually created tools.
The assisted agents achieved up to a 13.3\% improvement in average scores relative to the non-assisted agents.
This suggests that training agents to explore and use the built-in IDE functionality would yield performance gains.
It also suggests that in the near term, we can get performance gains by introducing hand-engineered tools into the computer-use agent and incorporating existing agent designs in our \ours{} environment.


\input{tables/assisted_comparison}




\input{figures/file_editing}






\textbf{Agents Are Incapable of Recovering from Errors.}
Next, we find that current agents
show limited error recovery capabilities.
When an action fails to execute correctly, models tend to persistently repeat the same failed action without exploring alternatives.
Similarly, if an agent selects an incorrect action, it continues along an erroneous solution path without recognizing or correcting the mistake.
In an experiment designed to probe this behavior, we deliberately suppressed one of the model's actions.
Despite the environment's screenshot clearly showing an unchanged state, the models proceeded with their planned action sequence as though the suppressed action had succeeded.
This behavior suggests a heavy reliance on memorized action sequences rather than dynamic responses to visual feedback, resulting in exponentially increasing errors and poor performance.


