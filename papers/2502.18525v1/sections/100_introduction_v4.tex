\section{Introduction}
\label{sect:intro}

\input{figures/teaser_figure}
Human software developers possess a remarkable ability to work across a wide range of programming tasks, seamlessly adapting to new languages, tools, and problem domains. 
Realizing a single, general-purpose agent with similar versatility is the overarching goal of many recent efforts in code generation and software engineering automation~\cite{Jiang2024ASO,Jin2024FromLT,wang2024openhandsopenplatformai}. 
However, most software engineering agents (SWE agents) still rely on a \textit{tool-based paradigm},
where an agent takes actions using hand-engineered functions (e.g., search repository, run Python code) exposed through a text API~\cite{yang2024sweagentagentcomputerinterfacesenable,yang2024swebenchmultimodalaisystems,wang2024executablecodeactionselicit,wang2024openhandsopenplatformai}.
This fundamentally limits generalization, since tool-based agents can only perform tasks using the predefined actions.
For example, an agent designed to manage GitHub pull requests lacks debugging abilities unless it is programmed into the agent's API.
Furthermore, the tool-based paradigm lacks scalability, as hand-engineering complex tools requires significant human effort and may not be bug-free.
As a result, it remains unclear whether the tool-based paradigm scales to the diversity of software engineering tasks, which spans multiple languages, modalities, and task types.

Our motivating hypothesis is that achieving general-purpose SWE agents requires a shift to \textit{computer-use agents}~\cite{anthropic2024developing} that interact with computers as humans do: by observing the screen, typing, and clicking. 
To this end, we recast agentic software engineering as interacting directly with an \textit{integrated development environment (IDE)} by observing its visual state and using basic actions such as clicking and typing.
This allows the agent to perform any task possible in an IDE and leverage all of the IDE's tools—from debuggers to web browsers—without requiring specialized APIs.
However, despite promising results in  web navigation~\cite{anthropic2024developing} and open-ended computer tasks~\cite{xie2024osworldbenchmarkingmultimodalagents}, the ability of computer-use agents to perform software engineering remains underexplored and we lack a dedicated environment for software engineering.



To close this gap, we introduce \textit{Programming with Pixels (\ours{})}, the first software engineering agent environment aimed at general-purpose computer-use agents. 
The \ours{} environment is a VSCode-based IDE where 
agents perceive the screen and use primitive actions such as typing, pointing, and clicking.
\ours{} fulfills two key properties.
First, the environment is \textit{expressive}, allowing agents to complete any software engineering task achievable in an IDE, without language- or domain-specific modifications. 
Second, agents naturally interact with \textit{any tools available in the IDE}--including debuggers, linters, and code suggestions while handling diverse data types such as images, videos, and PDFs--through basic actions such as clicking and typing.
This notion of tool use fundamentally differs from hand-engineered tool APIs, offering scalability and reducing the effort needed to hand-engineer tools for AI agents.
Namely, \ours{} lets agents take advantage of the rich tools already accessible to humans in an IDE, rather than reinventing the wheel.
Finally, computer-use agents reduce the need for complex tool pipelines (e.g., tool-specific prompts), opening up a simplified approach to general-purpose SWE agents.


To further evaluate agents developed for computer-use we construct \bench{}, a unified benchmark of 15 tasks spanning a variety of software engineering activities, including code generation, pull request resolution, UI development, and DevOps workflows.
We show for the first time that general-purpose computer-use agents achieve non-trivial performance on a wide variety of SWE tasks, often approaching or exceeding state-of-the-art tool-based agents.


However, our analysis reveals substantial opportunities for future work. 
First, even state-of-the-art computer-use agents suffer from visual grounding issues. 
Second, we show that current agents lack the ability to use many of the tools available in the IDE, including ones that could make their tasks trivial. 
This suggests that training agents to explore and use the tools available in the IDE is a fruitful future direction.
Finally, we find that only one model, Claude~\citep{anthropic2024developing}, performs well, highlighting the need for further research into training and improving computer-use agents.

In summary, our contributions are as follows.
First, we introduce \Ours{} (\ours{}), the first software engineering-focused environment for evaluating computer-use agents.
Second, we introduce \bench{}, a benchmark spanning 15 diverse SWE domains, allowing for systematic comparison of software engineering agents.
Third, we demonstrate, for the first time, that computer-use agents can perform a wide variety of software engineering tasks without additional hand-engineering of their action or observation spaces.
Fourth, we analyze the limitations of current computer-use agents, identifying the need for models that better leverage IDE tooling and highlighting agent training as a key future direction.
Finally, both existing agents and benchmarks can be easily incorporated into our unified environment, positioning \ours{} to serve as a common platform for developing future SWE agents.
Overall, \ours{} challenges the prevailing tool-based paradigm for SWE agents and provides a platform for developing more general agents that interact directly with IDEs.

