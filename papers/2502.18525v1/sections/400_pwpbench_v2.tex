
\section{\bench{}}
\label{sec:bench}

We introduce \bench{}, a benchmark comprising 15 diverse software engineering tasks that span 8 programming languages and multiple modalities. Each task provides agents with access to the complete suite of tools available in the \ours{} environment. The purpose of \bench{} is to evaluate how well agents handle a broad range of software engineering (SWE) activities, thereby testing the generality of their code generation and SWE capabilities.

\paragraph{Tasks.}
\label{sec:bench_tasks}
\bench{} contains \benchsize{} instances covering 15 tasks, sourced from 13 existing code-generation datasets and 2 newly created by us. 
These tasks are designed to be representative of the breadth of software engineering—including tasks beyond conventional code generation to capture real-world complexities—and can be expanded as models excel in newer tasks. 
We followed three guiding principles: (1) tasks should require significant interaction with various SWE tools, (2) each task should necessitate multiple steps to complete, and (3) the overall benchmark should span multiple programming languages and modalities. 
Based on these principles, we collected diverse tasks and grouped them into four categories:

\begin{itemize}[leftmargin=*]
    \item \textbf{Code Generation and Editing:} Evaluates the ability of agents to generate and edit code. This category includes datasets such as HumanEval for code completion, SWE-Bench~\cite{Jimenez2023SWEbenchCL} and SWE-Bench-Java~\cite{zan2024swebenchjavagithubissueresolving} for resolving pull requests, DSBench~\cite{jing2024dsbench} for data science tasks, and Res-Q~\cite{labash2024resqevaluatingcodeeditinglarge} or CanITEdit~\cite{cassano2024editevaluatingabilitylarge} for code editing. Each dataset benefits from different tools. For example, SWE-Bench can take advantage of debuggers and linters, while DSBench may leverage an IPython kernel and tools for analyzing large data files. Code editing tasks can leverage refactoring utilities and repository searches, covering varied input-output formats and end goals.

    \item \textbf{Multimodal Code Synthesis:} Involves creating code based on input images or other visual data. Examples include Design2Code~\cite{Si2024Design2CodeHF} for UI development, Chart2Mimic~\cite{Shi2024ChartMimicEL} for generating Python code from chart images, SWE-Bench-MM~\cite{yang2024swebenchmultimodalaisystems} for multimodal code editing, and DSBench tasks that rely on images or PDF documents during data analysis.

    \item \textbf{Domain-Specific Programming:} Focuses on specialized fields such as ethical hacking (CTF)~\cite{yang2023intercodestandardizingbenchmarkinginteractive} and interactive theorem proving (miniCTX)~\cite{hu2024minictxneuraltheoremproving}. These tasks demand significant interactivity with IDE components. For example, theorem proving requires continuously inspecting states via the IDE, while CTF tasks often involve analyzing images, running executables, or installing VSCode extensions (e.g., hexcode readers).

    \item \textbf{IDE-Specific and General SWE Tasks:} Recognizing that code generation is only one aspect of software engineering, we introduce two novel task sets to evaluate broader SWE skills. The first, \textbf{IDE Configuration}, evaluates an agent's ability to modify IDE settings—such as themes, extension installations, and preferences—that are critical for effective tool use in a complex environment. The second, which we term \textbf{General-SWE}, targets non-code activities such as profiling, designing UI mockups, managing Kanban boards, and project refactoring. These tasks capture essential operational skills typically required by human developers but largely absent from conventional code generation benchmarks.

\end{itemize}

Figure~\ref{fig:task_distribution} shows the distribution of tasks across all categories. An agent that succeeds across these tasks demonstrates strong potential for automating a wide range of software engineering activities.
For instance, \bench{} covers Python, Java, JavaScript, HTML, CSS, Bash, SQL, and Lean, and requires agents to work with text, images, data files, and other data types. Furthermore, effective interaction with IDE tools is essential.


\input{figures/task_distribution}

\input{tables/bench_comparison}


\paragraph{Benchmarking Design and Task Setup.}

Every task is evaluated within the \ours{} environment. Unlike traditional benchmarks that provide structured, well-formatted context (for example, supplying all relevant schemas in text-to-SQL), \bench{} presents agents with an IDE containing a codebase rich in information. Specifically, an agent is provided with an initial environment state $S_i$ and an instruction $I$. The agent's goal is to update the codebase to satisfy $I$ and transition to a final state $S_f$. Only $S_f$ is evaluated, using execution-based criteria (e.g., running unit tests). This setup requires agents to autonomously discover and extract relevant information from files, directories, and other resources—mirroring the challenges faced in real-world software development.

Many tasks in \bench{} require extensive multi-turn interactions and can be time-consuming. To support large-scale evaluations, \ours{} enables parallelized testing in a sandboxed environment, ensuring both security and reproducibility. Tasks can also be configured to restrict or partially allow internet access based on experimental needs. 

Furthermore, as software engineering tasks evolve with advancements in model capabilities, our benchmark is designed to grow over time. New tasks can be incorporated by creating simple setup scripts that define the IDE's initial state and evaluation logic, ensuring that \bench{} remains modular and adaptable.

\paragraph{\bench{}-Lite.}

Because \bench{} contains more than \benchsize{} instances in total, running a full evaluation can be computationally expensive. To address this, we also provide \bench{}-Lite—a smaller subset of 300 instances, made up of 20 random samples per task. This subset preserves the overall difficulty and distribution while ensuring equal representation for each task, thereby making rapid experimentation more accessible.

