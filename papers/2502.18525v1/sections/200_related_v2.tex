
\section{Related Work}

\subsection{Task-specific SWE benchmarks}

Early neural code generation approaches were typically evaluated on fixed input-output pairs—for example, generating code from docstrings~\cite{chen2021evaluatinglargelanguagemodels} or from general textual descriptions~\cite{austin2021programsynthesislargelanguage}. 
Subsequent benchmarks extended these evaluations to interactive settings, such as resolving GitHub pull requests or writing unit tests for real-world code repositories~\cite{Jimenez2023SWEbenchCL, zan2024swebenchjavagithubissueresolving, mündler2025swtbenchtestingvalidatingrealworld}. 
More recently, efforts have broadened the scope of code generation to include multimodal tasks, where vision models must interpret images to generate correct code or edits~\cite{Si2024Design2CodeHF, Shi2024ChartMimicEL, jing2024dsbench, yang2024swebenchmultimodalaisystems}. However, each of these benchmarks is confined to specific languages, modalities, or task types. In contrast, our proposed \bench{} unifies these diverse evaluations into a single framework, encompassing multimodal and multilingual challenges that require interaction with a broad suite of IDE tools. Using this unified approach we reproduce the performance of established benchmarks and encourage the development of general-purpose agents capable of handling a variety of new software engineering tasks. We further compare our work with previous efforts in Tables~\ref{tab:env_comparison_pwp} and \ref{tab:bench_comparison_pwp}.


\subsection{Software Engineering (SWE) Agents}

Recent work has explored “code agents” that move beyond single-step neural code generation toward interactive methods, where intermediate feedback from tools informs subsequent actions. However, many of these approaches specialize in particular tools or programming languages~\cite{Jin2024FromLT,yang2024swebenchmultimodalaisystems}, limiting their broader applicability. For example, Agentless~\cite{xia2024agentlessdemystifyingllmbasedsoftware} relies on a tool that parses files into Python-specific class and function structures.
This fails to perform well in other languages or settings~\cite{yang2024swebenchmultimodalaisystems} without manual modifications. Similarly, the SWE-agent requires modifications to adapt to different tasks~\cite{abramovich2024enigmaenhancedinteractivegenerative,yang2024swebenchmultimodalaisystems}. In contrast, agents designed for \ours{} are inherently task and language-agnostic due to the expressive action and observation spaces mandated by our environment. Moreover, the diverse tasks in \bench{} require agents to generalize across a wide range of SWE challenges rather than excel in one narrowly defined area such as resolving pull requests.

Many existing agents also depend on hand-engineered tools that require human effort to implement and are susceptible to bugs. For instance, Agentless~\cite{xia2024agentlessdemystifyingllmbasedsoftware} leverages tools for parsing files into Python-specific structures; CodeAct relies on an IPython kernel~\cite{wang2024executablecodeactionselicit}; SWE-Agent uses dedicated search and file editing tools~\cite{yang2024sweagentagentcomputerinterfacesenable}; AutoCodeRover requires a linter~\cite{zhang2024autocoderoverautonomousprogramimprovement}; SWE-Agent EnIGMA develops specialized tools for CTF-style competitions~\cite{abramovich2024enigmaenhancedinteractivegenerative}; and SWE-Bench-MM~\cite{yang2024swebenchmultimodalaisystems} implements a browser view. 
In \ours{}, these tools are inherently available within the IDE (as detailed in  Table~\ref{tab:assisted_tools}), and the agent's task is to effectively use them rather than being explicitly guided on which tool to use for each specific task.

Finally, current approaches often blur the line between the agent and the environment, as each agent is designed with its own specified action and observation spaces within a self-created environment. \Ours{} addresses this issue by unifying existing environments into a single, general-purpose platform on which agents operate. This clear separation of environment design from agent design standardizes evaluation and also allows any existing agent to be modeled within our framework, making it an important testbed for both current and future SWE agents.


\subsection{Visual Agents and Computer-Use Agents}

A family of recent multimodal agent benchmarks require agents to operate user interfaces using a predefined, limited set of actions (e.g., \texttt{new\_tab}, \texttt{go\_back}, \texttt{click [element id]})~\cite{koh2024visualwebarenaevaluatingmultimodalagents, deng2023mind2webgeneralistagentweb, zheng2024gpt} . 
These \textit{visual agents} typically rely on additional prompting—such as set-of-marks techniques that supply an HTML accessibility tree containing textual and positional information—to overcome their inherent poor visual grounding capabilities~\cite{yang2023setofmarkpromptingunleashesextraordinary}. 
Despite such aids, these agents often fail when faced with the complex and dense IDE interfaces found in our environment.

A separate family of \textit{computer-use agents}~\cite{anthropic2024developing,openai2025introducing,gou2024navigatingdigitalworldhumans} are trained to operate with an expressive action and observation space using primitive operations like clicks and keystrokes, without the need for external accessibility elements. However, there is no SWE-specific environment for evaluating and further training these agents. \ours{} fills this gap by providing a unified, expressive IDE platform that challenges computer-use agents with realistic and diverse SWE tasks.

\subsection{Expressive Agent Environments}

Prior work on expressive agent environments has predominantly targeted the web domain~\cite{koh2024visualwebarenaevaluatingmultimodalagents, deng2023mind2webgeneralistagentweb}, entire operating systems~\cite{xie2024osworldbenchmarkingmultimodalagents, bonatti2024windowsagentarenaevaluating, NEURIPS2023_bbbb6308}, or other general scenarios~\cite{xu2024theagentcompanybenchmarkingllmagents}. Some of these environments, such as OSWorld~\cite{xie2024osworldbenchmarkingmultimodalagents}, feature general action and observation spaces similar to ours. However, although these benchmarks are capable of expressing a wide range of tasks, they do not focus on the unique challenges inherent to software engineering within an IDE. For example, while OSWorld offers a broad set of tasks, it is not specifically designed for SWE, resulting in increased computational overhead. Software engineering is a diverse and important domain that merits its own dedicated environment.

Additionally, we design \ours{} so that existing tool-based software engineering agents can be readily incorporated into our framework. Specifically, we modify the source code of the IDE to open up API calls that let us test current tool-based agents. 
Furthermore, \bench{} is tailored specifically for multimodal SWE tasks within an IDE, encompassing activities such as pull-request handling, debugging, and image-based code generation across multiple programming languages. We also observe that existing agents built for generic UI control often struggle in the \ours{} environment, as they must interact with a richer set of tools and achieve precise visual grounding within a complex interface containing a large number of interactive elements. We further distinguish \ours{} from other environments in Table~\ref{tab:env_comparison_pwp}.
