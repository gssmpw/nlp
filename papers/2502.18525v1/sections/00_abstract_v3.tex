\begin{abstract}

Recent advancements in software engineering (SWE) agents have largely followed a \textit{tool-based paradigm}, where agents interact with hand-engineered tool APIs to perform specific tasks. 
While effective for specialized tasks, these methods fundamentally lack generalization, as they require predefined tools for each task and do not scale across programming languages and domains.
We introduce \Ours{} (\ours{}), an agent environment that unifies software development tasks by enabling \textit{computer-use agents}â€”agents that operate directly within an IDE through visual perception, typing, and clicking, rather than relying on predefined tool APIs. 
To systematically evaluate these agents, we propose \bench{}, a benchmark that unifies existing SWE benchmarks spanning tasks across multiple programming languages, modalities, and domains under a task-agnostic state and action space.
Our experiments demonstrate that general-purpose computer-use agents can approach or even surpass specialized tool-based agents on a variety of SWE tasks without the need for hand-engineered tools.
However, our analysis shows that current models suffer from limited visual grounding and fail to exploit many IDE tools that could simplify their tasks. 
When agents can directly access IDE tools, without visual interaction, they show significant performance improvements, highlighting the untapped potential of leveraging built-in IDE capabilities.
Our results establish \ours{} as a scalable testbed for building and evaluating the next wave of software engineering agents.\footnote{Carnegie Mellon University. We release code and data at \url{programmingwithpixels.com}.}
    
\end{abstract}
