\section{Related Works}
\subsection{Preference-Based Reward Learning}
\vspace{-5pt}
Learning rewards from preferences falls in the category of learning utility functions in Preference-based Reinforcement Learning (PbRL) \cite{wirth2017survey}, and was early studied in  \cite{zucker2010optimization,akrour2012april,akrour2014programming}. In those early works, typically a set of weights for linear features is learned. With neural network representations, a scalable PbRL framework of learning reward is developed in \cite{christiano2017deep}, recently used for fine tuning large language models \cite{bakker2022fine,achiam2023gpt}.  In those works, human preference is modelled using  Bradley-Terry formulation \cite{bradley1952rank}.
% . Backbone RL algorithms like PPO \cite{schulman2017proximal} and SAC \cite{haarnoja2018soft} is used to optimize the learned reward to achieve good control performance. 
Numerous variants of this framework have been developed \cite{hejna2023few,hejna2023contrastive,pmlr-v164-myers22a,10160439} over time. Recently, progress has been made to improve human data complexity. PEBBLE \cite{pmlr-v139-lee21i} used unsupervised pretraining and experience re-labelling to achieve better human query efficiency. SURF \cite{park2022surf} applied data augmentation and semi-supervised learning to get a more diverse preference dataset. RUNE \cite{liang2022reward} encourages agent exploration with an intrinsic reward measuring the uncertainty of reward prediction. MRN \cite{liu2022meta} jointly optimizes the reward network and the pair of Q/policy networks in a bi-level framework. Despite the above progress, most methods are vulnerable to erroneous human preference. As shown in \cite{lee2021b}, PEBBLE suffers from a 20\% downgrade when there exists a 10\% error rate in preference data. In real-world applications of the PbRL, non-expert human users tend to make mistakes when providing feedback, due to human mistakes,  malicious interference, or difficulty distinguishing between equally undesirable motions. 


% In this paper, the proposed method can perform well with the existence of the false preference labels by introducing a voting mechanism in the update of the hypothesis spaces, suggesting the potential robustness and adaptiveness in aligning the reward function with human.






\vspace{-5pt}
\subsection{Robust Learning from Noisy Labels}
\vspace{-5pt}
% The noisy labels present in the real-world data \cite{song2022learning}, which imposes an essential need to perform robust deep learning using such data with a certain rate of the noisy labels. To improve the robustness of deep learning, multiple robust learning approaches has been proposed. The categories of these approaches include Robust Network Architectures \cite{yao2018deep,lee2019robust}, Robust Regularization \cite{lukasik2020does,zhang2017mixup}, Robust Loss Functions \cite{amid2019robust,ma2020normalized} and Sample Selection \cite{jiang2018mentornet,zhou2020robust}.

To tackle erroneous human preferences, recent PbRL methods draw on the methods in robust deep learning \cite{yao2018deep,lee2019robust,lukasik2020does,zhang2017mixup,amid2019robust,ma2020normalized,jiang2018mentornet,zhou2020robust}. For example, \cite{xue2023reinforcement} uses an encoder-decoder structure within the reward network to mitigate the impact of preference outliers. \cite{cheng2024rime} trains a discriminator to differentiate between correct and false preference labels, filtering the data to create a cleaner dataset. However, these methods rely on prior knowledge of the correct human preference distribution for latent correction or require additional training.  \cite{heo2025mixing} proposes a robust learning approach based on the mixup technique \cite{zhang2017mixup}, which augments preference data by generating linear combinations of preferences. While this approach eliminates the need for additional training, false human preferences can propagate through the augmentation process, ultimately degrading learning performance. 

The proposed method differs from existing methods in three key aspects: (1) it requires no prior distribution assumptions for correct or false human preferences, (2) it avoids additional classification training to filter false preferences, (3) without explicitly identifying unknown false preferences, it still ensures robust learning performance.

\vspace{-5pt}
\subsection{Active Learning in Hypothesis Space}
\vspace{-5pt}
Active learning based on Bayesian approaches has been extensively studied \cite{daniel2014active, biyik2018batch, Sadigh2017ActivePL, biyik2020active, houlsby2011bayesian, biyik2024batch}, and many of these methods can be interpreted through the lens of hypothesis space removal. In particular, the works of \cite{Sadigh2017ActivePL, biyik2018batch, biyik2024batch} are closely related to ours. They iteratively update a reward hypothesis space using a Bayesian approach through (batches of) active preference queries. However, their methods are limited to reward functions that are linear combinations of preset features. More recently, \cite{jin2022learning, xie2024safe} have explored learning reward or constraint functions via hypothesis space cutting, where the hypothesis space is represented as a convex polytope, and human feedback introduces linear cuts to this space. Still, these approaches are restricted to linear parameterizations of rewards.

In addition to their limitations to weight-feature parametric rewards, the aforementioned methods do not address robustness in the presence of erroneous human preference data. In this paper, we show that the absence of robust design makes these methods highly vulnerable to false preference data.