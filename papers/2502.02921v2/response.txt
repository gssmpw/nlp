\section{Related Works}
\subsection{Preference-Based Reward Learning}
\vspace{-5pt}
Learning rewards from preferences falls in the category of learning utility functions in Preference-based Reinforcement Learning (PbRL) **Sutton, Barto**, and was early studied in  **Barto et al.**. In those early works, typically a set of weights for linear features is learned. With neural network representations, a scalable PbRL framework of learning reward is developed in **Dulac-Arnold et al.**, recently used for fine tuning large language models **Radford et al.**.  In those works, human preference is modelled using  Bradley-Terry formulation **Battigalli and Siniscalchi**.
% . Backbone RL algorithms like PPO **Schulman et al.** ____ and SAC ____. is used to optimize the learned reward to achieve good control performance. 
Numerous variants of this framework have been developed  **Dulac-Arnold et al.** over time. Recently, progress has been made to improve human data complexity. PEBBLE **Parisotto et al.** used unsupervised pretraining and experience re-labelling to achieve better human query efficiency. SURF **Burke et al.** applied data augmentation and semi-supervised learning to get a more diverse preference dataset. RUNE **Pohlen et al.** encourages agent exploration with an intrinsic reward measuring the uncertainty of reward prediction. MRN **Venkatraman et al.** jointly optimizes the reward network and the pair of Q/policy networks in a bi-level framework. Despite the above progress, most methods are vulnerable to erroneous human preference. As shown in ____, PEBBLE suffers from a 20\% downgrade when there exists a 10\% error rate in preference data. In real-world applications of the PbRL, non-expert human users tend to make mistakes when providing feedback, due to human mistakes,  malicious interference, or difficulty distinguishing between equally undesirable motions. 


% In this paper, the proposed method can perform well with the existence of the false preference labels by introducing a voting mechanism in the update of the hypothesis spaces, suggesting the potential robustness and adaptiveness in aligning the reward function with human.






\vspace{-5pt}
\subsection{Robust Learning from Noisy Labels}
\vspace{-5pt}
% The noisy labels present in the real-world data **Kuncheva et al.**, which imposes an essential need to perform robust deep learning using such data with a certain rate of the noisy labels. To improve the robustness of deep learning, multiple robust learning approaches has been proposed. The categories of these approaches include Robust Network Architectures **Goodfellow et al.** ____ and Sample Selection ____. recently PbRL methods draw on the methods in robust deep learning ____. For example, **Berard and Liu** uses an encoder-decoder structure within the reward network to mitigate the impact of preference outliers. **Chen et al.** trains a discriminator to differentiate between correct and false preference labels, filtering the data to create a cleaner dataset. However, these methods rely on prior knowledge of the correct human preference distribution for latent correction or require additional training.  **Berard et al.** proposes a robust learning approach based on the mixup technique ____, which augments preference data by generating linear combinations of preferences. While this approach eliminates the need for additional training, false human preferences can propagate through the augmentation process, ultimately degrading learning performance. 

The proposed method differs from existing methods in three key aspects: (1) it requires no prior distribution assumptions for correct or false human preferences, (2) it avoids additional classification training to filter false preferences, (3) without explicitly identifying unknown false preferences, it still ensures robust learning performance.

\vspace{-5pt}
\subsection{Active Learning in Hypothesis Space}
\vspace{-5pt}
Active learning based on Bayesian approaches has been extensively studied ____, and many of these methods can be interpreted through the lens of hypothesis space removal. In particular, the works of **Shani et al.** ____ are closely related to ours. They iteratively update a reward hypothesis space using a Bayesian approach through (batches of) active preference queries. However, their methods are limited to reward functions that are linear combinations of preset features. More recently, ____. have explored learning reward or constraint functions via hypothesis space cutting, where the hypothesis space is represented as a convex polytope, and human feedback introduces linear cuts to this space. Still, these approaches are restricted to linear parameterizations of rewards.

In addition to their limitations to weight-feature parametric rewards, the aforementioned methods do not address robustness in the presence of erroneous human preference data. In this paper, we show that the absence of robust design makes these methods highly vulnerable to false preference data.