%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{colortbl}
\setlist{leftmargin=1.5mm}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\Hfunc}{\mathrm{\mathbf{H}}}
\newcommand{\sig}[1]{\mathrm{\mathbf{sig}}_{#1}}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Robust Reward Alignment via  Hypothesis Space Batch Cutting}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhixian Xie}{asu}
\icmlauthor{Haode Zhang}{sjtu}
\icmlauthor{Yizhe Feng}{sjtu}
\icmlauthor{Wanxin Jin}{asu}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
\icmlaffiliation{asu}{Arizona State University, Tempe AZ, United States}
\icmlaffiliation{sjtu}{Shanghai Jiao Tong University, Shanghai, China}

\icmlcorrespondingauthor{Zhixian Xie}{zxxie@asu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Learning from Human Feedback, Inverse Reinforcement Learning, Preference Based Reinforcement Learning, Robust Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reward design for reinforcement learning and optimal control agents is challenging. Preference-based alignment addresses this by enabling agents to learn rewards from ranked trajectory pairs provided by humans. However, existing methods often struggle from poor robustness to unknown false human preferences. In this work, we propose a robust and efficient reward alignment method based on a novel and geometrically interpretable perspective: hypothesis space batched cutting. Our method iteratively refines the reward hypothesis space through “cuts” based on batches of human preferences. Within each batch, human preferences, queried based on disagreement, are grouped using a voting function to determine the appropriate cut, ensuring a bounded human query complexity.
To handle unknown erroneous preferences, we introduce a conservative cutting method within each batch, preventing erroneous human preferences from making overly aggressive cuts to the hypothesis space. This guarantees provable robustness against false preferences. We evaluate our method in a model predictive control setting across diverse tasks, including DM-Control, dexterous in-hand manipulation, and locomotion. The results demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing method
when handling high percentage of erroneous human preferences.

\end{abstract}
\vspace{-25pt}
\section{Introduction}
\vspace{-5pt}
Reinforcement learning and optimal control have shown great success in generating intricate behavior of an agent/robot, such as dexterous manipulation \cite{qi2023hand,handa2023dextreme, yin2023rotating} and agile locomotion \cite{radosavovic2024real,tsounis2020deepgait, hoeller2024anymal}. In these applications,  reward/cost functions are essential, as they dictate agent behavior by rewarding/penalizing different aspects of motion in a balanced manner. However, designing different reward terms and tuning their relative weights is nontrivial, often requiring extensive domain knowledge and laborious trial-and-error. This is further amplified in user-specific applications, where agent behavior must be tuned to align with individual user preferences.

Reward alignment automates reward design by enabling an agent to learn its reward function directly from intuitive human feedback \cite{casper2023open}. In preference-based feedback, a human ranks a pair of the agent’s motion trajectories, and then the agent infers a reward function that best aligns with human judgment. Numerous alignment methods have been proposed \cite{christiano2017deep,ibarz2018reward,hejna2023few}, with most relying on the Bradley-Terry model \cite{bradley1952rank}, which formalizes the rationality of human preferences. While effective, these models are vulnerable to false human preferences, where rankings may be irrational or inconsistent due to human errors, mistakes, malicious interference, or difficulty distinguishing between equally undesirable trajectories. When a significant portion of human feedback is erroneous, the learned reward function degrades substantially, leading to poor agent behavior.


In this paper, we propose a  robust reward alignment framework based on a novel and interpretable perspective: hypothesis space batch cutting. We maintain a hypothesis space of reward models during learning, where each batch of human preferences introduces a nonlinear cut, removing portions of the hypothesis space. Within each batch, human preferences, queried based on disagreement over the current hypothesis space, are grouped using a proposed voting function to determine the appropriate cut. This process ensures a certifiable upper bound on the number of human preferences required. To tackle false human preferences, we introduce a conservative cutting method within each batch. This prevents false human preferences from making overly aggressive cuts to the hypothesis space and ensures provable robustness. Extensive experiments demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing approaches when handling high rates of erroneous human preferences.



 % Our method can also be extended to the robust setting, in which there are false preference pairs in the data. Certain cuts in one batch can be relaxed by a voting process to mitigate the false preference labels, and ensures the robustness of our algorithm against false preference pairs. In experiments, our method has similar performance with the state-of-the-art baseline based on Bradley-Terry loss in vanilla setting with purely rational human, and greatly outperforms it in both feedback efficiency and robustness with human irrationality related false feedback. 



\section{Related Works}
\subsection{Preference-Based Reward Learning}
\vspace{-5pt}
Learning rewards from preferences falls in the category of learning utility functions in Preference-based Reinforcement Learning (PbRL) \cite{wirth2017survey}, and was early studied in  \cite{zucker2010optimization,akrour2012april,akrour2014programming}. In those early works, typically a set of weights for linear features is learned. With neural network representations, a scalable PbRL framework of learning reward is developed in \cite{christiano2017deep}, recently used for fine tuning large language models \cite{bakker2022fine,achiam2023gpt}.  In those works, human preference is modelled using  Bradley-Terry formulation \cite{bradley1952rank}.
% . Backbone RL algorithms like PPO \cite{schulman2017proximal} and SAC \cite{haarnoja2018soft} is used to optimize the learned reward to achieve good control performance. 
Numerous variants of this framework have been developed \cite{hejna2023few,hejna2023contrastive,pmlr-v164-myers22a,10160439} over time. Recently, progress has been made to improve human data complexity. PEBBLE \cite{pmlr-v139-lee21i} used unsupervised pretraining and experience re-labelling to achieve better human query efficiency. SURF \cite{park2022surf} applied data augmentation and semi-supervised learning to get a more diverse preference dataset. RUNE \cite{liang2022reward} encourages agent exploration with an intrinsic reward measuring the uncertainty of reward prediction. MRN \cite{liu2022meta} jointly optimizes the reward network and the pair of Q/policy networks in a bi-level framework. Despite the above progress, most methods are vulnerable to erroneous human preference. As shown in \cite{lee2021b}, PEBBLE suffers from a 20\% downgrade when there exists a 10\% error rate in preference data. In real-world applications of the PbRL, non-expert human users tend to make mistakes when providing feedback, due to human mistakes,  malicious interference, or difficulty distinguishing between equally undesirable motions. 


% In this paper, the proposed method can perform well with the existence of the false preference labels by introducing a voting mechanism in the update of the hypothesis spaces, suggesting the potential robustness and adaptiveness in aligning the reward function with human.






\vspace{-5pt}
\subsection{Robust Learning from Noisy Labels}
\vspace{-5pt}
% The noisy labels present in the real-world data \cite{song2022learning}, which imposes an essential need to perform robust deep learning using such data with a certain rate of the noisy labels. To improve the robustness of deep learning, multiple robust learning approaches has been proposed. The categories of these approaches include Robust Network Architectures \cite{yao2018deep,lee2019robust}, Robust Regularization \cite{lukasik2020does,zhang2017mixup}, Robust Loss Functions \cite{amid2019robust,ma2020normalized} and Sample Selection \cite{jiang2018mentornet,zhou2020robust}.

To tackle erroneous human preferences, recent PbRL methods draw on the methods in robust deep learning \cite{yao2018deep,lee2019robust,lukasik2020does,zhang2017mixup,amid2019robust,ma2020normalized,jiang2018mentornet,zhou2020robust}. For example, \cite{xue2023reinforcement} uses an encoder-decoder structure within the reward network to mitigate the impact of preference outliers. \cite{cheng2024rime} trains a discriminator to differentiate between correct and false preference labels, filtering the data to create a cleaner dataset. However, these methods rely on prior knowledge of the correct human preference distribution for latent correction or require additional training.  \cite{heo2025mixing} proposes a robust learning approach based on the mixup technique \cite{zhang2017mixup}, which augments preference data by generating linear combinations of preferences. While this approach eliminates the need for additional training, false human preferences can propagate through the augmentation process, ultimately degrading learning performance. 

The proposed method differs from existing methods in three key aspects: (1) it requires no prior distribution assumptions for correct or false human preferences, (2) it avoids additional classification training to filter false preferences, (3) without explicitly identifying unknown false preferences, it still ensures robust learning performance.

\vspace{-5pt}
\subsection{Active Learning in Hypothesis Space}
\vspace{-5pt}
Active learning based on Bayesian approaches has been extensively studied \cite{daniel2014active, biyik2018batch, Sadigh2017ActivePL, biyik2020active, houlsby2011bayesian, biyik2024batch}, and many of these methods can be interpreted through the lens of hypothesis space removal. In particular, the works of \cite{Sadigh2017ActivePL, biyik2018batch, biyik2024batch} are closely related to ours. They iteratively update a reward hypothesis space using a Bayesian approach through (batches of) active preference queries. However, their methods are limited to reward functions that are linear combinations of preset features. More recently, \cite{jin2022learning, xie2024safe} have explored learning reward or constraint functions via hypothesis space cutting, where the hypothesis space is represented as a convex polytope, and human feedback introduces linear cuts to this space. Still, these approaches are restricted to linear parameterizations of rewards.

In addition to their limitations to weight-feature parametric rewards, the aforementioned methods do not address robustness in the presence of erroneous human preference data. In this paper, we show that the absence of robust design makes these methods highly vulnerable to false preference data.


\section{Problem Formulation}
\vspace{-5pt}
We formulate a reward-parametric Markov Decision Process (MDP) as follows $(S,A,r_{\boldsymbol{\theta}},P,p_0)$, in which $S$ is the state space, $A$ is the action space, $r_{\boldsymbol{\theta}}:S\times A\times \Theta \rightarrow \mathbb{R}$ is a reward function parameterized by $\boldsymbol{\theta}\in\Theta$, $P:S\times A \rightarrow S$ is the dynamics and $p_0$ is the initial state distribution. The parametric reward $r_{\boldsymbol{\theta}}$ can be a neural network.


We call the entire parametric space of the reward function, ${\Theta}\subseteq\mathbb{R}^r$, the \emph{reward hypothesis space}. Given any specific reward $\boldsymbol{\theta}\in{\Theta}$, the agent plans its action sequence $\{\boldsymbol{a}_{0:T-1}\}$ with a starting state $\boldsymbol{s}_0$  maximizes the cumulated reward 
\begin{equation}
    J_{\boldsymbol{\theta}}(\boldsymbol{a}_{0:T-1}) =\mathbb{E}_{\boldsymbol{\xi} = \{\boldsymbol{s}_0, \boldsymbol{a}_0, \dots, \boldsymbol{s}_{T}\}} \sum\nolimits_{t=0}^{T-1} r_{\boldsymbol{\theta}}(\boldsymbol{s}_t,\boldsymbol{a}_t)
\end{equation}
over a time horizon $T$.  Here, the expectation is with respect to initial state distribution, probabilistic dynamics, etc. The action and the rollout state sequence forms an agent trajectory  $\boldsymbol{\xi} = \{\boldsymbol{s}_0, \boldsymbol{a}_0, \dots, \boldsymbol{s}_{T}\}$. With a slight abuse of notation, we below denote the reward of trajectory  $\boldsymbol{\xi}$ also as $J_{\boldsymbol{\theta}}(\boldsymbol{\xi})$.



Suppose a human user's preference of agent behavior corresponds to an implicit reward function in the hypothesis space $\boldsymbol{\theta}_H \in \boldsymbol{\Theta}$. The agent does not know human reward  $\boldsymbol{\theta}_H$, but can query for the human's preference over a trajectory pair $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1)$. Human returns a  preference label $y^{\text{true}}$ for $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1)$ base on the rationality assumption \cite{shivaswamy2015coactive,christiano2017deep,jin2022learning}:
\begin{equation}\label{equ.truelabel}
y^{\text{true}}=
\begin{cases}
1 &
     J_{\boldsymbol{\theta}_{H}}(\boldsymbol{\xi}^0)\leq  J_{\boldsymbol{\theta}_{H}}(\boldsymbol{\xi}^1) \quad \\
     0 & \text{otherwise}
\end{cases}.
\end{equation}
We call $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{true}})$  a \emph{true human preference}. Oftentimes, the human can give erroneous preference due to mistakes, errors,  intentional attacks, or misjudgment
of two trajectories that look very similar, this leads to a \emph{false human preference}, defined as $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{false}})$, where 
\begin{equation}\label{equ.falselabel}
y^{\text{false}}=
\begin{cases}
0 &
     J_{\boldsymbol{\theta}_{H}}(\boldsymbol{\xi}^0)\leq  J_{\boldsymbol{\theta}_{H}}(\boldsymbol{\xi}^1) \quad \\
     1 & \text{otherwise}
\end{cases}.
\end{equation}


\textbf{Problem statement:} We consider human preferences are queried and received incrementally, $(\boldsymbol{\xi}^0_k, \boldsymbol{\xi}^1_k, y_k)$, $k=1,2,3,...K$, where $y_k=y_k^{\text{true}}$ or $y_k=y_k^{\text{false}}$ and $k$ is the human query time. We seek to  answer two questions:

\textbf{Q1:} With all true human preferences, i.e., $y_k=y_k^{\text{true}}$ for $k=1,2,...$,  how to develop a query-efficient learning process, such that the agent can quickly learn $\theta_H$ with a certifiable upper bound for human query count $K$?
    
\textbf{Q2:} When \emph{unknown} false human preferences (\ref{equ.falselabel}) are included, i.e., $y_k=y_k^{\text{true}}$ or  $y_k^{\text{false}}$, $k=1,2,...$,  how to establish a provably robust learning process, such that the agent can still learn $\theta_H$ regardless of false preferences?


\section{Method of Hypothesis Space Batch Cutting}
In this section, we present an overview of our proposed Hypothesis Space Batch Cutting (HSBC) method and address the question of \textbf{Q1}. The proofs for all lemmas and theorems can be found in Appendix \ref{sec.appendix_lemma_proof} and \ref{sec.appendix_proof}.

\vspace{-5pt}
\subsection{Overview}\label{sec.overview}
\vspace{-5pt}



We start with first showing how a human preference leads to a cut to the hypothesis space. Given a true or false human preference  $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y)$, we can always define the  function
\begin{equation}
    f(\boldsymbol{\theta}, \boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y) = (1-2y) \big(J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^0) - J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^1)\big)
\end{equation}
For a true human preference $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{true}})$, it is easy to verify that (\ref{equ.truelabel}) will lead to the following constraints on $\boldsymbol{\theta}_H$
\begin{equation}\label{equ.cut_cstr}
    \boldsymbol{\theta}_H\in\{ \boldsymbol{\theta}\in\Theta | f(\boldsymbol{\theta}, \boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{true}}) \geq 0\}
\end{equation}
This means that a true human preference will result in a constraint (or a cut) in (\ref{equ.cut_cstr}) on the hypothesis space $\Theta$, and the true human reward $\boldsymbol{\theta}_H$ satisfies such constraint. 

With the above premises, we present the overview of the method of Hypothesis Space Batch Cutting (HSBC) below, including three key steps.

\begin{tcolorbox}[title=\textbf{HSBC Algorithm},left=1mm, right=0.7mm, bottom=1.5mm]
With initial hypothesis space $\Theta_0\subseteq\Theta$, perform the following three steps at iteration $i=0,1,2,...I$
\begin{itemize}
    \item[] \textbf{Step 1}: [Hypothesis sampling] Sample an ensemble $\mathcal{E}_{i}$ of  reward parameters $\boldsymbol{\theta}$s  from the latest hypothesis space $\Theta_i$ for trajectory generation.
    \item[] \textbf{Step 2}: [Disagreement-based human query] Use the ensemble $\mathcal{E}_{i}$ to generate  $N$ trajectory pairs based on disagreement for active human query and obtain a batch of human preferences $\mathcal{B}_i=\{{(\boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})}\}^N_{j=1}$.     
    \item[] \textbf{Step 3}: [Hypothesis space cutting] Calculate the constraint set (batch cuts)  $\mathcal{C}_i$: 
\begin{equation}
    \mathcal{C}_i= \{ \boldsymbol{\theta}| f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j}) \geq 0, \, j=1,...,N\} \label{equ.batch_constraint_set}.
\end{equation}
 and  update  hypothesis space by 
 \begin{equation}
     \Theta_{i+1}=\Theta_{i}\cap \mathcal{C}_i.
     \label{equ.cut}
 \end{equation}
\end{itemize}
\end{tcolorbox}

\begin{figure}[!htpb]
\centering
    \includegraphics[width=1.5in]{Figs/Cuts/cut_noerr.jpg}
    \caption{\small Illustration of one update from $\Theta_i$ to $\Theta_{i+1}$. Three constraints are induced from a human preference batch of size 3, with the simplified notation $f_{i,j}(\boldsymbol{\theta}) = f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})$. Red arrows are the direction of the constraints, i.e., the  $\boldsymbol{\theta}$ region satisfying $\{\boldsymbol{\theta}|f_{i,j}(\boldsymbol{\theta}) \geq 0\}$. New $\Theta_{i+1}$ is the regions in $\Theta_i$ satisfying all constraints.}
    \label{fig.cut_single}
    \vspace{-5pt}
\end{figure}


As shown in Fig. \ref{fig.cut_single}, the intuitive idea of the HSBC algorithm is to maintain and update a hypothesis space $\Theta_i$, starting from an initial set $\Theta_0$, based on batches of human preferences. Each batch of human preferences (\textbf{Step 2}) is actively queried. The human preference batch is turned into a batch cut (\textbf{Step 3}), removing portion of the current hypothesis space, leading to the next hypothesis space. 

From (\ref{equ.cut}) in (\textbf{Step 3}), it follows $\Theta_0 \supseteq \Theta_1 \supseteq \Theta_2 \supseteq \dots$. This means that the hypothesis space is not increasing. More importantly, we have the following assertion, saying the human reward is always contained in the current hypothesis if all human preferences are true in a sense of (\ref{equ.truelabel}).

\begin{lemma}
     If all human preferences are true, i.e., $y_{i,j} = y^{\text{true}}_{i,j},\ \forall i,j$  and $\boldsymbol{\theta}_H\in\Theta_0$, then following HSBC Algorithm,  one has $\boldsymbol{\theta}_H \in \Theta_i$ for all $i=1,2,...,I$.
    \label{lemma.cut_in}
\end{lemma}

\subsection{Voting Function for Human Preference Batch}
\vspace{-5pt}
In the HSBC algorithm, directly handling batch cut $\mathcal{C}_i$ and hypothesis space $\Theta_i$  can be computationally difficult, especially for high-dimensional parameter space e.g., neural networks. Thus, we will introduce a novel ``voting function" to represent both.

Specifically, we define a voting function for  a batch of human preferences $\mathcal{B}_i=\{{(\boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})}\}^N_{j=1}$ as follows
\begin{equation}\label{equ.vote_fn}
    V_i(\boldsymbol{\theta}) = \sum\nolimits_{j=1}^{N} \mathrm{\mathbf{H}}(f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})),
\end{equation}
where $H$ is the Heaviside step function defined as $\mathrm{\mathbf{H}}(x) =1$ if $x \ge 0$ and $\mathrm{\mathbf{H}}(x) =1$ otherwise. Intuitively, any point $\boldsymbol{\theta}\in \Theta$ in the hypothesis space will get a "+1" vote if it satisfies $f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j}) \geq 0$ and ``0" vote otherwise. Therefore, $V_i(\boldsymbol{\theta})$ represents the total votes of any point in the hypothesis space after the $i$th batch of human preferences. 

Thus, $\boldsymbol{\theta} \in \mathcal{C}_i$ in (\ref{equ.batch_constraint_set}) if and only if $V_i(\boldsymbol{\theta}) = N$ because  $\boldsymbol{\theta}$s need to satisfy all $N$ constraints $f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j}) \geq 0$ for $j=1,2,...,N$. As the number of votes in (\ref{equ.vote_fn}) only takes integers, the batch cut $\mathcal{C}_i$ can be equivalently written as:
\begin{equation}
    \mathcal{C}_i     = \{\boldsymbol{\theta}| V_i(\boldsymbol{\theta}) \geq N -0.5 \}  .
    \label{equ.C_i}
\end{equation}
Furthermore, the indicator  of  batch cut $\mathcal{C}_i$  can be written as:
\begin{equation}
    \mathbbm{1}_{\mathcal{C}_i}(\boldsymbol{\theta}) = \Hfunc(V_i(\boldsymbol{\theta}) - N+0.5).
\end{equation}

Following \eqref{equ.cut} in the HSBC, since $\Theta_i = \Theta_0 \cap \bigcap_{k=0}^{i-1} \mathcal{C}_k$, we can define the indicator function of the hypothesis space at $i$-th iteration: 
\begin{equation}
    \mathbbm{1}_{\Theta_i}(\boldsymbol{\theta}) = \mathbbm{1}_{\Theta_0}(\boldsymbol{\theta}) \prod\nolimits_{k=0}^{i-1} \mathbbm{1}_{\mathcal{C}_k}(\boldsymbol{\theta}),\quad i\geq 1
    \label{equ.indicator}
\end{equation}
with $\mathbbm{1}_{\Theta_0}(\boldsymbol{\theta})$ be the indicator function of initial hypothesis space $\Theta_0$. With the indicator representation of $\Theta_i$ in \eqref{equ.indicator}, later we will show that  $\boldsymbol{\theta}$ can be sampled from $\Theta_i$ as stated in (\textbf{Step 1}) in the HSBC algorithm.

\subsection{Disagreement-Based Query and its Complexity}
\vspace{-5pt}
In the HSBC algorithm, not all batch cuts have similar effectiveness, i.e., some batch cut $\mathcal{C}_{i}$ can be redundant without removing any volume of hypothesis space,  leading to an unnecessarily human query. To achieve effective cutting, each trajectory pair $(\boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})$ should attain certain disagreement on the current hypothesis space $\Theta_i$, before sent to the human for preference query.

Specifically,  in collecting human preference batch $\mathcal{B}_i$, we only query human preferences using    trajectory pairs $(\boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})$ that satisfy 
\begin{multline}
        \exists \,\, \boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \Theta_i,  f(\boldsymbol{\theta}_1, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j}) \geq 0 \\
        \text{and}\ f(\boldsymbol{\theta}_2, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j}) \leq 0,\ \forall y_{i,j}
    \label{equ.method_disagree}
\end{multline}
Intuitively, the disagreement-based query can ensure at least some portion of the hypothesis space is removed by the constraint $f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j}) \geq 0$ induced by $(\boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})$, no matter what preference label  $y_{i,j}$ human will provide. A geometric illustration is shown in  Fig. \ref{fig.cut_disagree}, with the simplified notation $f_{i,j}(\boldsymbol{\theta}) = f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y_{i,j})$. The above disagreement-based query strategies is also related to prior work \cite{christiano2017deep,pmlr-v139-lee21i}.

\begin{figure}[!htpb]
\centering
     \subfigure
    {
        \label{fig.disagree_1}
        \includegraphics[width=1.5in]{Figs/Cuts/disagree_1.jpg}
    } 
    \subfigure
    {
        \label{fig.disagree_2}
        \vspace*{-3cm}
        \includegraphics[width=1.5in]{Figs/Cuts/disagree_2.jpg}
    } 
    \caption{\small Illustration of disagreement-based preference query. Regardless of the human actual preference label (which only determines which side is to cut),  the disagreement condition will always guarantee a least a portion of hypothesis space is removed.}
    \label{fig.cut_disagree}
    \vspace{-5pt}
\end{figure}


With the disagreement-based preference query in each batch, the following assertion provides an upper bound on the total human query complexity of the HSBC algorithm given all true human preferences.

\begin{theorem}
\label{thm.main}
     In the HSBC algorithm, suppose all true human preferences. Let $P(\boldsymbol{\xi}^0 ,\boldsymbol{\xi}^1, y^{\text{true}})$  be an unknown distribution of true human preferences. 
    Define $\mathrm{err}(r_{\boldsymbol{\theta}}) = P(f(\boldsymbol{\theta}, \boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{true}}) < 0)$, which is  the probability that $r_{\boldsymbol{\theta}}$ makes different predictions than human. For any $\epsilon, \delta$, the probability 
    \begin{equation}
        P(\mathrm{err}(r_{\boldsymbol{\theta}})\leq\epsilon) \geq 1 - \delta
    \end{equation}
    holds with human query complexity:
    \begin{equation}
        K=NI \leq O(\zeta(d\log\zeta + \log\big(\log\frac{1}{\epsilon}/\delta\big))\log\frac{1}{\epsilon})
        \label{equ.theorem_complexity}
    \end{equation}
    where $N$ and $I$ are the batch size and the iteration number,$\zeta$ is the disagreement coefficient related to the learning problem defined in Appendix \ref{sec.appendix_disagree}, and $d$ is the VC-dimension \cite{vapnik1998statistical} of the reward model defined in Appendix \ref{sec.appendix_classifier}.
\end{theorem}
% \begin{proof}
%     Please refer to the proof in Appendix \ref{sec.appendix_proof}.
% \end{proof}
The above theorem shows that the proposed HSBC algorithm can achieve Probably Approximately Correct (PAC) learning of the human reward. The learned reward function $r_{\boldsymbol{\theta}}$ can make preference prediction with arbitrarily low error rate $\epsilon$ under arbitrary high probability $1- \delta$, under the human preference data complexity in (\ref{equ.theorem_complexity}).

\section{Robust Alignment}
In this section, we extend the HSBC algorithm to handle false human preference data. We will establish a provably robust learning process, such that the agent can still learn $\boldsymbol{\theta}_H$ regardless of the false human preference.

First, for a false human preference $(\boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{false}})$, it is easy to verify that (\ref{equ.falselabel}) will lead to 
\begin{equation}
    \boldsymbol{\theta}_H\notin\{ \boldsymbol{\theta}\in\Theta | f(\boldsymbol{\theta}, \boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{false}}) \geq 0\}
\end{equation}
This means that a false human preference will also result in a constraint that $\boldsymbol{\theta}_H$ does not satisfy.

In the HSBC algorithm, suppose a batch $\mathcal{B}_m$ of human preference includes both true human preference labels and unknown false ones, i.e., $y_{m,j}$ is $y_{m,j}^{\text{true}}$ or $y_{m,j}^{\text{false}}$.  We have the following lemma stating the failure of the method.



\begin{lemma}
    Following the HSBC Algorithm, if there exists one false preference in $m$-th batch $\mathcal{B}_m$ of human preferences, then $\boldsymbol{\theta}_H \notin \mathcal{C}_m$ and for all $i > m$, $\boldsymbol{\theta}_H \notin \Theta_i$.
    \label{lemma.fail}
\end{lemma}
% \begin{proof}
%     Please refer to the proof in \ref{proof.fail}
% \end{proof}
Geometrically, Lemma \ref{lemma.fail} shows that any false human preference in preference batch $\mathcal{B}_m$ will make the batch cut $\mathcal{C}_m$ mistakenly ``cut out" $\boldsymbol{\theta}_H$, i.e., $\boldsymbol{\theta}_H\in\mathcal{C}_m$.  In the following, we show that the HSBC Algorithm will become provably robust by a slight modification of \eqref{equ.batch_constraint_set} or  (\ref{equ.C_i}).


\vspace{-5pt}
\subsection{Cutting with Conservativeness Level $\gamma$}
\vspace{-5pt}

Our idea is to adopt a worst-case, conservative cut to handle unknown false human preferences in the HSBC algorithm. Specifically, let \emph{conservativeness level} $\gamma \in [0,1]$ denote the ratio of the maximum number of false human preferences in a batch of $N$ human preferences.  In other words, with  $\gamma$, we suppose a preference batch has \textit{at most} $\lceil \gamma N \rceil$ ($\lceil \cdot \rceil$ is the round-up operator) false preference.  $\gamma$ is a worst-case or conservative estimate of false preference ratio and is not necessarily equal to the real error rate, which is unknown.

With the above conservativeness level $\gamma$, one can change (\ref{equ.batch_constraint_set}) or (\ref{equ.C_i}) into following 
% To maintain this property, the definition of the cutting set can be changed in the robust setting to:
\begin{equation}    \mathcal{C}_i = \{\boldsymbol{\theta}| V_i(\boldsymbol{\theta}) > \lfloor(1-\gamma) N\rfloor  -0.5\} 
    \label{equ.C_i_robust}
\end{equation}
where $\lfloor \cdot \rfloor$ is the round-down operator, and the indicator function of the batch cut $\mathcal{C}_i$  thus can be expressed
\begin{equation}\label{equ.relaxed_indicator}
    \mathbbm{1}_{\mathcal{C}_i}(\boldsymbol{\theta}) = \Hfunc(V_i(\boldsymbol{\theta}) - \lfloor(1-\gamma) N\rfloor +0.5)
\end{equation}

The following important lemma states that only by the above modification of $\mathcal{C}_i $ without any other changes, the proposed HSBC algorithm can still achieve the provable robustness against false human preference.
\begin{lemma}
    With the conservativeness level $\gamma$ and replacing (\ref{equ.batch_constraint_set}) with  $\mathcal{C}_i$ in \eqref{equ.C_i_robust}, the proposed HSBC Algorithm will have 
    $\boldsymbol{\theta}_H \in \mathcal{C}_i$ and $\boldsymbol{\theta}_H \in \Theta_i$, i=1,2,3...I, regardless of false human preference.
    \label{lemma.robust}
\end{lemma}
The lemma means that by simply replacing expression  \eqref{equ.batch_constraint_set} or \eqref{equ.C_i} with $\eqref{equ.C_i_robust}$ while keeping other settings unchanged, the hypothesis cutting learning be performed in presence of the human false label. Actually, \eqref{equ.C_i} is a special case of $\eqref{equ.C_i_robust}$ since when $\gamma=0$, $\lfloor(1-\gamma) N\rfloor = \lfloor N\rfloor = N$. A geometric interpretation of Lemma \ref{lemma.robust} is given below.  

\vspace{-5pt}
\subsection{Geometric Interpretation}
\vspace{-5pt}
Fig. \ref{fig.cuts} shows a 2D geometric interpretation of the robust hypothesis cutting from $\Theta_{i}$ to $\Theta_{i+1}$ in the presence of false human preferences for batch $\mathcal{B}_i$. Suppose  batch size $N=3$ and the third human preference $(\boldsymbol{\xi}^0_{i,3}, \boldsymbol{\xi}^1_{i,3}, y_{i,3}^{\text{false}})$ is false, while other two are true. Thus, we set $\gamma=1/3$ in \eqref{equ.relaxed_indicator}.


As shown in Fig. \ref{fig.cuts}, since  $(\boldsymbol{\xi}^0_{i,3}, \boldsymbol{\xi}^1_{i,3}, y_{i,3}^{\text{false}})$ is a false human preference, it will induce a constraint $f_{i,3}(\boldsymbol{\theta}):=f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,3}, \boldsymbol{\xi}^1_{i,3}, y_{i,3}^{\text{false}}) \geq 0$ which  $\boldsymbol{\theta}_H$ does not satisfy. As a result, simply taking the intersection of all constraints $\{\boldsymbol{\theta}|f_{i,j}(\boldsymbol{\theta}) \geq 0, j=1,2,3\}$ will rule out $\boldsymbol{\theta}_H$ by mistake. However, in this case, one can still perform a correct hypothesis space cutting (containing $\boldsymbol{\theta}_H$) using \eqref{equ.relaxed_indicator}. Replacing $N=3$ and $\gamma = 1/3$ in \eqref{equ.relaxed_indicator}, the indicator function for the batched cut is $\mathbbm{1}_{\mathcal{C}_i}(\boldsymbol{\theta}) = \Hfunc(V_i(\boldsymbol{\theta}) - 1.5)$. This means by keeping  the region in $\Theta_i$ with voting value $V_i (\boldsymbol{\theta})$ to be 2 or 3, $\boldsymbol{\theta}_H \in \Theta_{i+1}$. 

\begin{figure}[!htpb]
\vspace{-5pt}
    \centering
    \subfigure
    {
        \label{fig.cut_out}
        \includegraphics[height=0.9in]{Figs/Cuts/cut_out.png}
    } 
    \subfigure
    {
        \label{fig.cut_err}
        \includegraphics[height=0.9in]{Figs/Cuts/cut_robust.png}
    } 
    \caption{\small Geometry Interpretation of the robust HSBC with false preferences. Red arrows are the direction of the constraints, i.e., the  $\boldsymbol{\theta}$ region satisfying $\{\boldsymbol{\theta}|f_{i,j}(\boldsymbol{\theta}) \geq 0\}$. Left: $y_{i,3}^{\text{false}}$ is false human preference, and thus simply taking the intersection will cut out $\boldsymbol{\theta}_H$. Right: in robust HSBC, the regions with $V_i (\boldsymbol{\theta}) \geq 2$  are kept, thus  $\boldsymbol{\theta}_H$ is still contained in the new hypothesis space $\Theta_{i+1}$.}
    \label{fig.cuts}
\end{figure}

\vspace{-10pt}
\section{Detailed Implementation}
\vspace{-5pt}
By viewing all true human preferences as a special case of robust cutting with conservativeness level $\gamma=0$, we present the implementation of the HSBC algorithm in Algorithm \ref{alg.main}.  The details are presented below. 

\begin{algorithm}[tb]
\small
   \caption{Implementation for HSBC Algorithm}
   \label{alg.main}
\begin{algorithmic}
   \STATE {\bfseries Input:} Batch size $N$, ensemble size $M$, conservativeness level $\gamma$,  disagreement threshold $\eta$, segment count per trajectory $Z$.
    % softness parameter $\alpha,\beta$, $\nu$.

   \FOR{$i=0$ {\bfseries to} $I$}
   
    % \STATE $\widehat{\mathcal{E}_{j}} \leftarrow \mathrm{Optimize\_Sample}(\mathcal{L}_j)$;
       \setlength{\fboxsep}{2pt} % Adjust padding inside the box if needed
       \colorbox{red!10}{\parbox{\dimexpr\linewidth-2\fboxsep}{%
           \STATE Sample an assemble $\mathcal{E}_i$ from current hypothesis space (if $i=0$, randomly initialize $\mathcal{E}_0$) with the method in Section \ref{sec.samp_opt}.
           % with all previous preference batches $\mathcal{B}_0,\dots,\mathcal{B}_{i-1}$;
           Filter and Densify $\mathcal{E}_i$ with the method in Section \ref{sec.filter};
           \STATE \hfill //\textit{Hypothesis Sampling}
           }}
               
    \colorbox{green!10}{\parbox{\dimexpr\linewidth-2\fboxsep}{%
           \STATE Based on $\mathcal{E}_i$, generate trajectory using sampling-based MPC (Section \ref{sec.planning}). 
 Select trajectory pairs based on disagreement score, and acquire a preference batch $\mathcal{B}_i$ (Section \ref{sec.planning});
           \STATE \hfill //\textit{Disagreement-based human query}
           }}
    \colorbox{blue!10}{\parbox{\dimexpr\linewidth-2\fboxsep}{%
           \STATE Update the sampling objective function with \eqref{equ.obj_func} and  \eqref{equ.opt_indicator_fn2} using the new batch $\mathcal{B}_i$;
           \STATE \hfill //\textit{Hypothesis space Cutting}
           }}

   \ENDFOR
   \STATE \textbf{return} $\mathcal{E}_{I}$
\end{algorithmic}
\end{algorithm}

\vspace{-5pt}
\subsection{Hypothesis Space Sampling}
\vspace{-5pt}
\label{sec.samp_opt}

The step of hypothesis space sampling is to sample an ensemble of $M$ parameters, $\mathcal{E}_{i}=\{\boldsymbol{\theta}^k_i\}_{k=1}^{M}$,   from the current hypothesis space $\Theta_i$. We let  the initial hypothesis space ${\Theta}_0=\mathbb{R}^r$, that is,
\begin{equation}
    \mathbbm{1}_{\Theta_0}=1
\end{equation}
which guarantees  possible $\boldsymbol{\theta}_H$ is contained in $\Theta_0$. In our implementation, we use neural networks as our reward functions, thus   $\mathcal{E}_{0}$ is randomly initialized common neural-network initialization distributions.



For $i>0$, sampling $\mathcal{E}_{i}$ can be reformulated as an optimization with the indicator function (\ref{equ.indicator}), i.e.,
\begin{align}
    \mathcal{E}_{i} \subseteq \arg\max_{\boldsymbol{\theta}} \mathbbm{1}_{\Theta_i}(\boldsymbol{\theta})=\arg\max_{\boldsymbol{\theta}}\prod\nolimits_{k=0}^{i-1} \mathbbm{1}_{\mathcal{C}_k}(\boldsymbol{\theta})
    \label{equ.opt_indicator_fn}
\end{align}
To use a gradient-based optimizer, (\ref{equ.opt_indicator_fn}) needs to be differentiable.  Recall  in (\ref{equ.relaxed_indicator}) the only non-differentiable operation is $\Hfunc$. Thus, we use the smooth sigmoid function $\sig{\rho}(\cdot)$ with temperature $\rho$ to approximate $\Hfunc$. From (\ref{equ.relaxed_indicator}), $\mathbbm{1}_{\mathcal{C}_i}$  will be replaced by the smooth approximation:
\begin{equation}\label{equ.obj_func}
    \hat{\mathbbm{1}}_{\mathcal{C}_i}(\boldsymbol{\theta}) \approx \texttt{\textbf{sig}}_{\beta}\bigg(  \sum_{j=1}^{N} \sig{\alpha}(f_{i,j}(\boldsymbol{\theta})){-} \nu(1{-}\gamma) N\bigg)
\end{equation}
where $\alpha, \beta, \nu$ are the tunable learning parameters.
Therefore,  sampling $\mathcal{E}_{i}$ can be performed through 
\begin{equation}
    \mathcal{E}_{i} \subseteq \arg\max_{\boldsymbol{\theta}}\prod\nolimits_{k=0}^{i-1} \hat{\mathbbm{1}}_{\mathcal{C}_k}(\boldsymbol{\theta})
    \label{equ.opt_indicator_fn2}
\end{equation}
To get  diverse  $\boldsymbol{\theta}$ samples in  $\mathcal{E}_{i}$, we optimize (\ref{equ.opt_indicator_fn2}) in parallel with diverse initial guesses. 
% At $i$th iteration, $N$ parameters with different initial values is optimized individually to minimize object $\mathcal{L}_j(\theta, \alpha, \beta, \gamma, \nu)$. 
 For $i>0$, the optimization is warm-started using the previous  $\mathcal{E}_{i-1}$ as initial values. 

\vspace{-5pt}
\subsection{Sample Filtering and Densification}
\label{sec.filter}
\vspace{-5pt}
As  ${\mathcal{E}_{i}}$ are solutions to a smoothed optimization  (\ref{equ.opt_indicator_fn2}), there is no guarantee that the samples are all in $\Theta_i$. Thus, we use the original indicator function $\mathbbm{1}_{\Theta_i}(\boldsymbol{\theta})$ to filter the collected samples: remove $\boldsymbol{\theta}$s that are not satisfying $\mathbbm{1}_{\Theta_i}(\boldsymbol{\theta})$. Since ${\mathcal{E}_{i}}$ after filtering may not have $M$ samples,  we propose a densification step to duplicate some $\boldsymbol{\theta}$s (with also adding some noise) in ${\mathcal{E}_{i}}$  to maintain the same sample size $M$. 

\vspace{-5pt}
\subsection{Reward Ensemble Trajectory Generation}
\label{sec.planning}
\vspace{-5pt}
With the ensemble $\mathcal{E}_i$ sampled from $\Theta_i$, the reward used for planning and control is defined by  taking the mean value predicted of the reward ensemble, i.e.
\begin{equation}
    r_{\mathcal{E}_i}(\boldsymbol{s},\boldsymbol{a}) = \mathbb{E}_{\boldsymbol{\theta} \in \mathcal{E}_i}r_{\boldsymbol{\theta}}(\boldsymbol{s},\boldsymbol{a}).
\end{equation}
% The cumulative  reward is  $J_{\mathcal{E}_j}=\mathbb{E}_{\boldsymbol{s}_{0:T-1}} \sum_{t=0}^{T-1} r_{\mathcal{E}_j}(\boldsymbol{s}_t,\boldsymbol{a}_t)$. 
In our implementation, we use sampling-based model predictive control (MPC) in the MJX simulator \cite{todorov2012mujoco}  for trajectory generation. Half of the action plans generated from the  MPC  are perturbed with a decaying noise for exploration in the initial stage of learning. It is possible to using reinforcement learning for trajectory generation.

\vspace{-5pt}
\subsection{Disagreement Scoring and Query}\label{sec.query}
\vspace{-5pt}
In our algorithm, the generated trajectories are stored in a buffer. When a new trajectory is generated, we segment the new or each of the old trajectories (in the buffer) into $Z$ segments and randomly picking segment pairs $(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1})$ for disagreement test. Here, $\boldsymbol{\xi}^{0}$  is   from the new trajectory and $\boldsymbol{\xi}^{1}$  from the buffered trajectory. 

For any segment pair $(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1})$, we measure their disagreement on the current hypothesis space using the reward ensemble $\mathcal{E}_i$. We define the following disagreement score:
\begin{equation}
    \texttt{\textbf{DIS}}_{\mathcal{E}_i}(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1}) = {4n_{\mathcal{E}_i}^+ n_{\mathcal{E}_i}^-}/{N^2}
    \label{equ.mu}
\end{equation}
where $n_{\mathcal{E}_i}^+$ is the number of $\boldsymbol{\theta}$s in  $\mathcal{E}_i$ such that $J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^{0}) > J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^{1})$ and $n_{\mathcal{E}_i}^- = N - n_{\mathcal{E}_i}^+$. When there is no disagreement in $\mathcal{E}_i$, i.e.,  $\boldsymbol{\theta} \in \mathcal{E}_i$ makes consensus judgment  on $(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1})$, $\texttt{\textbf{DIS}}_{\mathcal{E}_i}(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1})=0$. When the disagreement is evenly split in , the score yields 1.    In our implementation, a disagreement threshold $\eta \in (0,1)$ is defined to select valid disagreement pairs for human queries. A  segment pair $(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1})$  is rejected if $\texttt{\textbf{DIS}}_{\mathcal{E}_i}(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1}) \leq \eta$; otherwise, the human is queried for a preference label $y$ for $(\boldsymbol{\xi}^{0} ,\boldsymbol{\xi}^{1})$.
Trajectory planning and the disagreement-based query are performed iteratively until  $\mathcal{B}_{i}$ contains $N$ pairs. 





\vspace{-5pt}
\section{Experiments}
\vspace{-5pt}
% \subsection{Experiment Settings}

We test our  method on 6 different tasks, as shown in Fig. \ref{fig.tasks}:

\begin{figure}[h]
\vspace{-10pt}
    \centering 
    \subfigure
    {
        \includegraphics[width=0.85in]{Figs/tasks/cartpole.png}
    }
    \subfigure
    {
        \includegraphics[width=0.85in]{Figs/tasks/walker.png}
    } 
    \subfigure
    {
        \includegraphics[width=0.85in]{Figs/tasks/humanoid.png}
    }
    
    \subfigure
    {
        \includegraphics[width=0.85in]{Figs/tasks/cube.png}
    } 
    \subfigure 
    {
        \includegraphics[width=0.85in]{Figs/tasks/bunny.png}
    }
    \subfigure
    {
        \includegraphics[width=0.85in]{Figs/tasks/unitree_stand_2.png}
    } 
    \vspace{-5pt}
    \caption{Task environments of experiments.}
    \label{fig.tasks}
    \vspace{-5pt}
\end{figure}


\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=0.4\textwidth]{Figs/legend_only.png} % Include the legend as a separate image

    \vspace{0.1cm} % Add vertical space between the legend and the 
   \begin{tabular}{c c c c c} % Table layout (1st column for labels, 4 image columns)
        \rotatebox{90}{\quad\qquad\scriptsize{\textbf{Cartpole}}} &  
        \includegraphics[width=1.3in]{Figs/err_0/cartpole.jpg} &
        \includegraphics[width=1.3in]{Figs/err_1/cartpole.jpg} &
        \includegraphics[width=1.3in]{Figs/err_2/cartpole.jpg} &
        \includegraphics[width=1.3in]{Figs/err_3/cartpole.jpg} \\
        
        \rotatebox{90}{\quad\qquad\scriptsize{\textbf{Walker}}} & 
        \includegraphics[width=1.3in]{Figs/err_0/walker.jpg} &
        \includegraphics[width=1.3in]{Figs/err_1/walker.jpg} &
        \includegraphics[width=1.3in]{Figs/err_2/walker.jpg} &
        \includegraphics[width=1.3in]{Figs/err_3/walker.jpg} \\
        
        \rotatebox{90}{\qquad\scriptsize{\textbf{Humanoid}}} &  
        \includegraphics[width=1.3in]{Figs/err_0/humanoid.jpg} &
        \includegraphics[width=1.3in]{Figs/err_1/humanoid.jpg} &
        \includegraphics[width=1.3in]{Figs/err_2/humanoid.jpg} &
        \includegraphics[width=1.3in]{Figs/err_3/humanoid.jpg} \\
    \end{tabular}
    \vspace{-5pt}
    \begin{minipage}{1.0\textwidth}
    \small
        \hspace{2cm}
        (a) False Rate $=0$ \hspace{1.3cm}
        (b) False Rate $=10\%$ \hspace{0.8cm}
        (c) False Rate $=20\%$ \hspace{0.6cm}
        (d) False Rate $=30\%$ 
    \end{minipage}
    
    \caption{\small Learning curves of the dm-control tasks under different rates of false human preferences in different columns: (a) False Rate $ = 0$; (b) False Rate $= 10\%$; (c) False Rate $= 20\%$; (d) False Rate $ = 30\%$. The conservativeness level in HSBC algorithms is the same as the actual human false rate. All results are reported over 5 runs. The results show that our method significantly outperforms the baseline when the false rate is high, and has a comparable performance when the false rate is 0 (no false human preference). 
    }
    \vspace{-10pt}
    \label{fig.res_dm}
\end{figure*}
\begin{figure}[h]
    \centering 
    \begin{tabular}{c c c} % Table layout (1st column for labels, 4 image columns)
        \rotatebox{90}{\qquad\scriptsize{\textbf{DexMan-Cube}}} &  
        \includegraphics[width=1.4in]{Figs/err_0/cube.jpg} &
        \includegraphics[width=1.4in]{Figs/err_2/cube.jpg} \\
        \vspace{-5pt}
        \rotatebox{90}{\qquad\scriptsize{\textbf{DexMan-Bunny}}} &  
        \includegraphics[width=1.4in]{Figs/err_0/bunny.jpg} &
        \includegraphics[width=1.4in]{Figs/err_2/bunny} \\
        \vspace{-5pt}
        \rotatebox{90}{\qquad\scriptsize{\textbf{Go2-Standup}}} &  
        \includegraphics[width=1.4in]{Figs/err_0/go2} &
        \includegraphics[width=1.4in]{Figs/err_2/go2.jpg} \\
    \end{tabular}
    \vspace{-0pt}
    \begin{minipage}{1.0\textwidth}
    \small
    \vspace{3pt}
        \hspace{1.5cm}
        (a) False Rate $=0$ \hspace{1.3cm}
        (b) False Rate $=20\%$ \hspace{0.8cm}
    \end{minipage} 
    \vspace{-15pt}
    \caption{\small Learning curves for dexterous manipulation and locomotion tasks under different rates of false human preferences: (a) False Rate $ = 0$; (b) False Rate $= 10\%$. The results are over 5 runs.}
    \vspace{-20pt}
    \label{fig.res_robot}
\end{figure}

 \textbf{3 dm-control tasks:} including Cartpole-Swingup, Walker-Walk,  Humanoid-Standup.

\textbf{2 in-hand dexterous manipulation tasks:} An Allegro hand in-hand reorientate a cube and bunny object to any given target. The two objects demands different reward designs due to the geometry-dependent policy. We use DexMan-Cube and DexMan-Bunny to denote two tasks.

\textbf{Quadruped robot stand-up task}:  
A Go2 quadruped robot learns a reward to stand up with two back feet and raise the two front feet. We use Go2-Standup to denote the task. 

 More setting details are in Appendix \ref{sec.appendix_tasks_loco}. Similar to MJPC \cite{howell2022} but to better support GPU-accelerated sampling-based MPC, we re-implemented all environments in MJX \cite{todorov2012mujoco} with MPPI \cite{williams2015model} as MPC policy. 
 



 \begin{figure*}[!htpb]
    \centering
    \subfigure[Ablation  of $\gamma$]
    {
        \includegraphics[width=1.6in]{Figs/Ablation/gamma.jpg}
        \label{fig.ablation_gamma}
    }
    \hspace{0.5cm}
        \subfigure[Ablation  of $\eta$]
    {
        \includegraphics[width=1.6in]{Figs/Ablation/disagree.jpg}
        \label{fig.ablation_eta}
    }
    \hspace{0.5cm}
    \subfigure[Ablation  of $N$]
    {
        \includegraphics[width=1.6in]{Figs/Ablation/batchsize.jpg}
        \label{fig.ablation_M}
    }    
    \caption{Ablation study in Walker-Walk task for the choices of (a): conservativeness level $\gamma$,  (b) disagreement threshold $\eta$,  (c): batch size $N$. The learning curves are reported across 5 individual runs.}
    \label{fig.ablation}
\end{figure*}


\begin{table*}[t]
\caption{\small Results on dm-control Tasks over 5 runs}
\vskip -0.2in
\label{tbl.result_dm}
\begin{center}
\begin{tiny}

\begin{sc}
\begin{tabular}{c|c|cc|cc|cc|cc}
\toprule

\multirow{2}{4em}{Tasks} &  & \multicolumn{2}{c|}{\scriptsize 
  False Rate 0 \%} & \multicolumn{2}{c|}{\scriptsize  False Rate 10\%} & \multicolumn{2}{c|}{\scriptsize  False Rate 20\%} & \multicolumn{2}{c}{\scriptsize  False Rate 30\%}\\

\cline{3-10}
\noalign{\vskip 1pt} 
& Oracle & Baseline & Ours & Baseline & Ours& Baseline & Ours& Baseline & Ours\\
\midrule
Cartpole-Swingup & 148.6 & \cellcolor{red!10} \textbf{143.3}  \scalebox{.8}{$\pm$8.3} & 137.8 \scalebox{.8}{$\pm$7.9} & 101.4 \scalebox{.8}{$\pm$39.2} & \cellcolor{red!10}\textbf{137.0} \scalebox{.8}{$\pm$8.5} & 52.3 \scalebox{.8}{$\pm$26.8} &\cellcolor{red!10} \textbf{130.7} \scalebox{.8}{$\pm$2.0} & 42.8 \scalebox{.8}{$\pm$23.3} & \cellcolor{red!10}\textbf{111.3} \scalebox{.8}{$\pm$16.8}\\
Walker-Walk  & 472.9 & 444.4 \scalebox{.8}{$\pm$13.4} & \cellcolor{red!10}\textbf{459.4} \scalebox{.8}{$\pm$3.9} & 421.2 \scalebox{.8}{$\pm$20.6} & \cellcolor{red!10}\textbf{441.4} \scalebox{.8}{$\pm$15.0} & 401.8 \scalebox{.8}{$\pm$37.6} & \cellcolor{red!10}\textbf{447.1} \scalebox{.8}{$\pm$14.4} & 277.0 \scalebox{.8}{$\pm$62.3} &\cellcolor{red!10} \textbf{417.2} \scalebox{.8}{$\pm$12.3}\\
Humanoid-Standup & 210.1 &  107.0 \scalebox{.8}{$\pm$28.6} &\cellcolor{red!10} \textbf{134.7} \scalebox{.8}{$\pm$30.5} &  85.6 \scalebox{.8}{$\pm$25.9} &\cellcolor{red!10} \textbf{148.0} \scalebox{.8}{$\pm$14.9}  &  84.2 \scalebox{.8}{$\pm$15.2} & \cellcolor{red!10}\textbf{117.9} \scalebox{.8}{$\pm$19.9} & 69.3 \scalebox{.8}{$\pm$12.0} & \cellcolor{red!10}\textbf{106.5} \scalebox{.8}{$\pm$27.0}\\
\bottomrule
\end{tabular}
\end{sc}
\end{tiny}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table}[t]
\caption{\small Results on Dexterous Manipulation \& Locomotion Tasks}
\vskip -0.2in
\label{tbl.result_robot}
\begin{center}
\begin{tiny}

\begin{sc}
\begin{tabular}{c|c|cc|cc}
\toprule

\multirow{2}{4em}{Tasks} &  
& \multicolumn{2}{c|}{\scriptsize 
  False Rate 0\%} & \multicolumn{2}{c}{\scriptsize  False Rate 20\%}\\
%\hline
\cline{3-6}
\noalign{\vskip 1pt} 
 & Oracle & Baseline & Ours & Baseline & Ours\\
\midrule

DexMan-Cube  & -.14 & -.28\scalebox{.8}{$\pm$.08} &\cellcolor{red!10} \textbf{-.20}\scalebox{.8}{$\pm$.04} & -.36\scalebox{.8}{$\pm$.13} & \cellcolor{red!10}\textbf{-.24}\scalebox{.8}{$\pm$.06} \\
DexMan-Bunny  & -.16 &  -.35\scalebox{.8}{$\pm$.09} &\cellcolor{red!10} \textbf{-.22}\scalebox{.8}{$\pm$.04} & -.40\scalebox{.8}{$\pm$.10} & \cellcolor{red!10}\textbf{-.24}\scalebox{.8}{$\pm$.03}\\
Go2-Standup   & -31 & -62\scalebox{.8}{$\pm$23} &\cellcolor{red!10} \textbf{-44}\scalebox{.8}{$\pm$4} &   -101\scalebox{.8}{$\pm$18} & \cellcolor{red!10}\textbf{-76}\scalebox{.8}{$\pm$22}\\
\bottomrule
\end{tabular}
\end{sc}
\end{tiny}
\end{center}
\vspace{-20pt}
\end{table}


 \textbf{Simulated (false) human preference:}
 Similar to \cite{pmlr-v139-lee21i, cheng2024rime}, a ``simulated human" is implemented as a computer program to provide preference for the reward learning. The simulated human uses the ground-truth reward $r_{\boldsymbol{\theta}_H}$ (see the ground truth reward of each task in Appendix \ref{sec.appendix_tasks}) to rank trajectory pairs. To simulate different rates of false human preferences, in each batch, a random selection of human preference labels is flipped. 
 
\textbf{Baseline} Throughout the experiment, we compare our method against a baseline implemented using the reward learning approach from PEBBLE (details in Appendix \ref{sec.appendix_baseline}).

\textbf{HSBC Algorithm settings: } In all tasks, reward functions are  MLP models. In our algorithm, unless specially mentioned, batch size $N=10$, ensemble size $M=16$,  
% $\nu=0.75$ 
and disagreement threshold $\eta=0.75$. 
% Each task also has different MPPI controller settings in training and evaluation stage. 
In dm-control tasks and Go2-Standup,  trajectory pairs of the first few batches are generated by random policy for better exploration.
%The number of these batches are 5 in Cartpole-Swingup and Walker-Walk, 20 in Humanoid-Standup and 10 in Go2-Standup. 
Refer to Appendix \ref{sec.appendix_model_param} for other settings.

\vspace{-5pt}
\subsection{Results}
\vspace{-5pt}
For each task, we test our method and the baseline each for 5 independent runs with different random seeds. During the learning progress,  the reward ensemble $\mathcal{E}_{i}$ at the checkpoint iteration $i$ is saved for evaluation. The evaluation involves using the saved reward ensemble $\mathcal{E}_{i}$ to generate a new trajectory, for which the performance is calculated using the ground-truth reward.  Each evaluation is performed in 5 runs with different random seeds. Evaluation performance at different learning checkpoints will yield the learning curve. We show the learning curve in Fig. \ref{fig.res_dm} for dm-control tasks, Fig. \ref{fig.res_robot} for DexMan and Go2-Standup tasks. The mean and standard deviation across different runs are reported. To better show the human query complexity, we set the x-axis as the number of queries, and the y-axis is the performance of $\mathcal{E}_{i}$ control evaluated by ground-truth reward. Quantitatively, Table \ref{tbl.result_dm} and Table \ref{tbl.result_robot} show the evaluation reward results of the dm-control tasks and DexMan/Go2-Standup tasks.


From both Fig. \ref{fig.res_dm} and Table \ref{tbl.result_dm}, we observe a comparable performance of our proposed method and the baseline for zero false human preferences. Both methods successfully learn reward functions to complete different tasks. As the rate of false preference increases, from $ 10\%$ to $30\%$,  we observe a significant performance drop of the baseline method. In contrast, the drop of the proposed method over a high rate of false human preference is not significant. 
These results demonstrate the robustness of the proposed method for reward learning against a high rate of false human preferences. 

\vspace{-5pt}
\subsection{Ablation Study}



\paragraph{Conservativeness level $\gamma$} Given a fixed rate of false human preference, We evaluate how different settings of conservativeness levels $\gamma = 0, 10\%,20\%,30\%$ affect the performance of the proposed method. Here, the actual human preferences have a fixed false rate of $20\%$. The ablation is tested for the Walker task, and the results over 5 runs are reported in Fig. \ref{fig.ablation_gamma}. % Specifically, the parameter $\nu$ is set to 0.95 magnify the significance of this setting. 

The results indicate that when $\gamma = 0$ (i.e., the algorithm aggressively cuts the hypothesis space without conservatism), the learning performance drops significantly. Conversely, when the conservativeness level $\gamma = 30\%$ exceeds the actual false rate ($20\%$) but remains within a reasonable range, it has little impact on learning performance. However, we postulate that, in general, a higher $\gamma$ increases query complexity, as less of the hypothesis space is cut per iteration.

\vspace{-10pt}
\paragraph{Disagreement threshold $\eta$} We evaluate the performance of our algorithm under different disagreement thresholds $\eta$ given all true human preferences. The test is in Walker's task. We set $\eta$ to 0, 0.5, 0.75 and show corresponding learning curves in Fig. \ref{fig.ablation_eta}. The results show that disagreement-based query can accelerate the learning convergence.

\vspace{-10pt}
\paragraph{Batch size $N$} In Fig. \ref{fig.ablation_M}, we show the reward learning performance for the Walker task with different choices of preference batch sizes, $N = 5,10,15,20$, under a fixed rate $20\%$ of false human preference.   The results indicate a similar performance, suggesting that the learning performance is not sensitive to the setting of preference batch sizes.



% \subsection{Comparison with Other  Methods}



\section{Conclusion}
In this paper, we propose the Hypothesis Space Batch Cutting (HSBC) method for robust reward alignment from human preference. Our method actively selects batches of preference data based on disagreement and uses them to constrain the reward function. Constraints in one batch are then grouped with a voting function, forming cuts to update the hypothesis space. To mitigate the false label in preference data, a conservative cutting method based on the voting function value is used to prevent overly aggressive cuts. HSBC is geometrically interpretable and provides a certifiable bound on query complexity. Tested across diverse control tasks, it matches baseline performance with clean data and outperforms it under high false preference rates.


% geometrically interpretable expression of a cut on the latest hypothesis space. The disagreement-based human query guarantees a provable upper bound for human query complexity. In cases of robust alignment, where false human preference data exists, HSBC can still effectively learn a human-intended reward function, with a conservative cutting method based on vote functions. The proposed method is tested on multiple robot manipulation and locomotion tasks, the result shows that this approach can significantly outperform the baseline under a high rate of false preference. Ablation studies demonstrate that the two core design aspects, the disagreement-based active query and the conservative cutting method, are both crucial for learning efficiency and robustness.

% In this paper, we propose the Hypothesis Space Batch Cutting (HSBC) method for robust reward alignment from human preference. The method iteratively updates the reward hypothesis space with batches of human preference, by performing hypothesis sampling, disagreement-based human query and hypothesis space cutting. The disagreement-based human query guarantees a provable upper bound for human query complexity. In cases of robust alignment, where false human preference data exists, HSBC can still effectively learn a human-intended reward function, with a conservative cutting method based on vote functions. The proposed method is tested on multiple robot manipulation and locomotion tasks, the result shows that this approach can significantly outperform the baseline under a high rate of false preference. Ablation studies demonstrate that the two core design aspects, the disagreement-based active query and the conservative cutting method, are both crucial for learning efficiency and robustness.

% \paragraph{Limitations and Future Works}
% The ultimate goal of the preference-based reward learning, especially in robotics, is to do reward alignment for real humans in the real-world robotic systems. Although our method performs well in simulated robotic tasks with a simulated irrational human, there are still some gaps towards successful real-world application. First, the simulated irrationality of randomly flipping the preference label may not be the true pattern of the human irrationality. More delicate models for human irrationality needs to be tested, perhaps some mechanisms related to the absolute and relative reward values of the pairs. In addition, the simulated tasks are simplified compared to real hardwares, without considering uncertainties such as the sensor noises and communication delay. The performance of our method in real-world applications thus remains as a question. In future works we will implement and evaluate our method on real-world systems with real human feedback to fill this gap.



\section*{Impact Statement}
The HSBC method enhances interpretability and robustness in reward alignment by providing a geometric view of hypothesis updates and a certifiable bound on human query complexity. Its conservative cutting strategy ensures stable learning even in the presence of false preferences, making it resilient to noisy or adversarial data. These properties make HSBC particularly valuable in applications such as robotics, autonomous systems, and human-in-the-loop decision-making, where false data frequently present and reliable reward alignment with human intent is essential. By improving robustness while maintaining interpretability, HSBC offers a principled approach to preference-based learning in uncertain environments.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Baseline Method for Comparison}
\label{sec.appendix_baseline}
We present the details of the baseline method used for comparison. It is inspired by the method of PEBBLE \cite{pmlr-v139-lee21i} on training the reward model, while uses the same setting to plan the trajectories and collect the preferences. The only differences between the baseline and our method is the ensemble size and the objective function. An ensemble of 3 reward models is maintained. After $i$ batches of preference, the reward models are tuned to optimize the Bradley Terry loss function of all previous preferences:
\begin{equation}
    \mathcal{L} = \sum_{k=0}^{i-1}\sum_{j=0}^{N-1} (1-y_{k,j}) P_{\boldsymbol{\theta}}^{BT}(\boldsymbol{\xi}^{1}_{k,j} \succ \boldsymbol{\xi}^{0}_{k,j}) + y_{k,j}P_{\boldsymbol{\theta}}^{BT}(\boldsymbol{\xi}^{1}_{k,j} \succ \boldsymbol{\xi}^{0}_{k,j})
\end{equation}
where
\begin{equation}
    P_{\boldsymbol{\theta}}^{BT}(\boldsymbol{\xi}^{0} \succ \boldsymbol{\xi}^{1}) = \frac{\exp{\alpha_{base}J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^0)}}{\exp{\alpha_{base}J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^0)}+\exp{\alpha_{base}J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^1)}}
\end{equation}
The parameter $\alpha_{base}$ is the temperature of the Bradley-Terry model. In the dexterous manipulation tasks, $\alpha_{base}=0.5$. In all other tasks, $\alpha_{base}=3$. In the experiments of cartpole swingup, $\alpha_{base}=10$. The preferences are selected with disagreement threshold $\eta=0.8$, which is similar to our approach. 

\section{Detailed Description of the Tasks}
\label{sec.appendix_tasks}
\subsection{DM-Control Tasks}
\label{sec.appendix_tasks_dm}
We re-implemented three dm-control tasks in MJX to perform sampling-based MPC, similar to \cite{howell2022}.

\paragraph{Cartpole} We directly use the environment xml file in dm-control, with the same state and action definition in original library. The goal is to swing up the pole to an upright position and balance the system in the middle of the trail. Use $\varphi,\dot{\varphi}$ to denote the angle and angular velocity of the pole, $x,\dot{x}$ to denote the horizontal position and velocity of the cart. The system is initialized with $x=0$ and $\varphi = \pi$, along with a normal noise with standard deviation 0.01 imposed on all joint positions and velocities. $\boldsymbol{s} = (x,\sin\varphi,\cos\varphi,\dot{x},\dot{\varphi})$ and $\boldsymbol{a}$ is a 1D control scalar signal for the horizontal force applied on the cart. We use a ground truth reward function:
\begin{equation}
    r(\boldsymbol{s},\boldsymbol{a}) = \mathrm{upright}(\boldsymbol{s})*\mathrm{middle}(\boldsymbol{s})*\mathrm{small\_ctrl}(\boldsymbol{a}) * \mathrm{small\_vel}(\boldsymbol{s})
\end{equation}
where $\mathrm{upright}(\boldsymbol{s}) = (\cos\varphi + 1)/2$, $\mathrm{middle}(\boldsymbol{s}) = \exp(-x^2)$, $\mathrm{small\_ctrl}(\boldsymbol{a}) = (4 + \exp(-4 \boldsymbol{a}^2))/5$, $\mathrm{small\_vel}(\boldsymbol{s}) = (1 + \exp(-0.5\dot{x}^2))/2$.

\paragraph{Walker-Walk} 
We directly use the environment xml file in dm-control. The goal is to make the agent walk forward with a steady speed.The system is initialized at a standing position and a normal noise with standard deviation 0.01 is imposed on all joint positions and velocities. Use $z$ to denote the sum of $z-$coordinate of the torso and a bias $-1.2$, thus the value $z=0$ when the walker stands up-straight. $\varphi_y$ to denote the torso orientation in $xz-$plane, $q$ to denote all joints between links and $\dot{x}$ to denote the torso linear velocity on $x-$axis. $\boldsymbol{s} = (z, \varphi_y, q, \dot{x},\dot{z}, \dot{\varphi_y},\dot{q})$ and $\boldsymbol{a} \in [0,1]^6$ stands for torques applied on all joints. We use a ground truth reward function:
\begin{equation}
    r(\boldsymbol{s},\boldsymbol{a}) = \frac{3*\mathrm{standing}(\boldsymbol{s}) + \mathrm{upright}(\boldsymbol{s})}{4} * \mathrm{move}(\boldsymbol{s})
\end{equation}
where $\mathrm{standing}(\boldsymbol{s}) = \mathrm{clip}(1- |z|,0,1)$, $\mathrm{upright}(\boldsymbol{s}) = (\cos\varphi_y + 1)/2$, $\mathrm{move}(\boldsymbol{s}) = \mathrm{clip}(\dot{x},0,1)$.

\paragraph{Humanoid-Standup}
We directly use the environment xml file in dm-control. The goal is to stand up from a lying position. The system is initialized at a lying position and a uniform noise between -0.01 and 0.01 is imposed on all joint positions and velocities. Use $z, quat$ to denote the $z-$coordinate and quaternion orientation of the humanoid torso. Use $v = [v_x,v_y,v_z],w = [w_x,w_y,w_z] $ to denote 3D linear and angular velocity of the torso. Use $q$ to denote all of the joints in humanoid agent. $\boldsymbol{s} = [z, quat,q,v,w,\dot{q}]$ and $\boldsymbol{a} \in [0,1]^{17}$ stands for torques applied on all joints. We use a ground truth reward function:
\begin{equation}
    r(\boldsymbol{s},\boldsymbol{a}) = \mathrm{standing}(\boldsymbol{a}) * \mathrm{small\_vel}(\boldsymbol{s})
\end{equation}
where $\mathrm{standing}(\boldsymbol{s}) = \mathrm{clip}(z/1.2,0,1)$ and $\mathrm{small\_vel}(\boldsymbol{s}) = \exp(-0.1 * \sqrt{v_x^2 + v_y^2} - 0.3 \norm{w})$. 

\subsection{In-hand Dexterous Manipulation}
\label{sec.appendix_tasks_mani}
We mainly focus on Allegro robot in-hand re-orientation of two different objects, a cube and a bunny. An allegro hand and an object are simulated in the environment. The allegro hand is tilted with quaternion $[0.0, 0.82,0.0,0.57]$. The objects has initial position of $[0.0,0.0,0.05]$ with quaternion $[0,0,0,1]$ In the beginning of every trajectory planning during training, a target orientation in the form of an axis-angle is randomly selected. The axis is uniformly randomly selected from $x,y,z$ axis and the angle is uniformly randomly selected from the set $\{\pm\frac{\pi}{4}, \pm \frac{\pi}{2}, \pm \frac{3\pi}{4}, \pi\}$. This angle-axis rotation is then converted to target quaternion $q_{targ}$. The goal is to use the fingers of the robotic hand to rotate the object to the target orientation, as well as remain the object in the center of the hand. Use $q$ to denote the joint positions of the robot hand, $p$ to denote the object position, $\Delta quat$ to denote the quaternion different between current object pose and target pose ($\Delta quat = 1 - (q_{curr}^T q_{targ})^2$) and $ff_{tip}, mf_{tip}, rf_{tip}, th_{tip}$ to denote the positions of four fingertips (forefinger, middle finger, ring finger and thumb). The object and fingertip positions are multiplied by 10 to balance the scale. $\boldsymbol{s}=[q,p,\Delta quat, ff_{tip}, mf_{tip}, rf_{tip}, th_{tip}]$. $\boldsymbol{a} \in [0,1]^{12}$ is the delta-position command on all joints and is multiplied by 0.2 to generate actual control signals. The ground truth reward is designed to be:
\begin{equation}
    r(\boldsymbol{s},\boldsymbol{a}) = -\mathrm{cost\_quat}(\boldsymbol{s}) - 0.4\mathrm{cost\_pos}(\boldsymbol{s}) - 0.05\mathrm{cost\_contact}(\boldsymbol{s})
\end{equation}
Where $\mathrm{cost\_quat}(\boldsymbol{s}) = \Delta quat$, $\mathrm{cost\_pos}(\boldsymbol{s}) = \norm{p-(0.0,0.0,0.01) * 10}^2$ and $\mathrm{cost\_contact}(\boldsymbol{s}) = \norm{p - ff_{tip}}^2 +\norm{p - mf_{tip}}^2 + \norm{p - rf_{tip}}^2 + \norm{p - th_{tip}}^2$.

In evaluation, 6 trajectories are planned by setting different target of $\pm \frac{\pi}{2}$ rotation on $x,y,z$ axis. The mean reward per trajectory per time step is calculated and recorded.

% \begin{figure}[!htpb]
%     \centering
%     \subfigure[DexMan-Cube]
%     {
%         \includegraphics[height=1.2in]{Figs/tasks/cube.png}
%     }
%     \hspace{0.5cm}
%     \subfigure[DexMan-Bunny]
%     {
%         \includegraphics[height=1.2in]{Figs/tasks/bunny.png}
%     }  
%     \caption{Illustration of the dexterous manipulation task.}
%     \label{fig.DexMan}
% \end{figure}

\subsection{Go2-Standup}
\label{sec.appendix_tasks_loco}
In this task, a go2 quadruped robot is simulated and the goal is to stand up on two back feet to keep the torso height on $0.6$, and raise two front feet to the height of $1.2$. The initial pose and the target pose is shown in Fig. \ref{fig.Go2}. The system is initialized at a standing position on four legs and a uniform noise between -0.01 and 0.01 is imposed on all joint positions and velocities. Use $z_{t}, quat$ to denote the $z-$coordinate and quaternion orientation of the robot torso. Use $z_{ff},z_{rf}$ to denote $z-$coordinates of two front feet and two rear feet. To balance the scale, all features related to the $z$-coordinates are multiplied by 10. Use $v = [v_x,v_y,v_z],w = [w_x,w_y,w_z] $ to denote 3D linear and angular velocity of the torso. Use $q$ to denote all of the joint positions. $\boldsymbol{s} = [z_t,quat,q,z_{ff},z_{rf},v,w,\dot{q}]$ and $\boldsymbol{a} \in [0,1]^{12}$ stands for the desired delta-position on all joints. We define the ground truth reward function as:
\begin{equation}
    r(\boldsymbol{s},\boldsymbol{a}) = -2.5\mathrm{height\_cost}(\boldsymbol{s})  - 0.5\mathrm{feet\_cost}(\boldsymbol{s}) - 10^{-6} \mathrm{ang\_vel\_cost}(\boldsymbol{s}) - 10^{-2} \mathrm{vel\_cost}(\boldsymbol{s})
\end{equation}
where $\mathrm{height\_cost}(\boldsymbol{s}) = (z-0.6)^2$, $\mathrm{feet\_cost}(\boldsymbol{s}) = \norm{z_{ff} - 1.2}^2 + \norm{z_{rf}}^2$, $\mathrm{ang\_vel\_cost}(\boldsymbol{s}) = \norm{w}^2$ and $\mathrm{vel\_cost}(\boldsymbol{s}) = \norm{v}^2$.

\begin{figure}[!htpb]
    \centering
    \subfigure[Initial Position]
    {
        \includegraphics[height=1.2in]{Figs/tasks/unitree_stand.png}
    }
    \hspace{0.5cm}
    \subfigure[Stand-up Position]
    {
        \includegraphics[height=1.2in]{Figs/tasks/unitree_stand_2.png}
    }  
    \caption{Illustration of the Go2-Standup task, the goal is to reach a stand-up posture with two feet.}
    \label{fig.Go2}
\end{figure}


\section{Model and Parameters in Experiment}
\label{sec.appendix_model_param}
\paragraph{Reward Neural Networks}
For all tasks, we use an MLP with 3 hidden layers of 256 hidden units to parameterize reward functions. The activation function of the network is ReLU. In the dm-control tasks, the input is a concatenated vector of state $\boldsymbol{s}$ and action $\boldsymbol{a}$. In dexterous manipulation and quadruped locomotion tasks, the input is purely the state $\boldsymbol{s}$ to perform faster learning. The output scalar value of the reward is scaled to the interval $[-1,1]$ using the Tanh function. In particular, in the dexterous manipulation tasks, an intrinsic reward $-0.5\mathrm{cost\_contact}(\boldsymbol{s})$ is imposed to the network output, forming a predicted reward to encourage grasping the object and bootstrapping the learning.

\paragraph{Learning Settings}
The parameters $\alpha, \beta, Z,I,T$, segment length $T_{seg}$ and evaluation trajectory length $T_{eval}$ slightly varies from the tasks, see Table \ref{tbl.learning_param}. In the Table, the two number of batch number $K$ corresponds to the number with irrationality 0 and 20\%. The Adam \cite{kingma2014adam} optimizer parameter is identical for all dm-control and Go2-Standup tasks, the learning rate is set to be 0.005 with a weight decay coefficient 0.001. In dexterous manipulation tasks, the learning rate is 0.002 with a weight decay coefficient 0.001.

\begin{table}[t]
\caption{Learning Parameter in Different Tasks}
\label{tbl.learning_param}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Task & $\alpha$ & $\beta$ & $Z$ & $I$ & $T$ & $T_{eval}$ & $T_{seg}$\\
\midrule
Cartpole-Swingup    & 10.0 & 3.0 & 2 & 50/80 & 100 & 200 & 50 \\
Walker-Walk    & 5.0 & 3.0 & 2 & 80/120 & 150 & 500 & 50 \\
Humanoid-Standup    & 5.0 & 3.0 & 3 & 100/150 & 200 & 300 & 50 \\
DexMan-Cube & 5.0 & 3.0 & 3 & 30/40 & 100 & 150 & 20 \\
DexMan-Bunny & 5.0 & 3.0 & 3 & 30/40 & 100 & 150 & 20 \\
Go2-Standup    & 5.0 & 3.0 & 3 & 80/120 & 150 & 100 & 25 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{MPPI Parameter}
The number of samples ($\mathrm{Num}$), planning horizon ($\mathrm{Hor}$), temperature ($\lambda$) and the standard deviation ($\mathrm{Std}$) of the normal sampling distribution determines the quality of trajectories planned by MPPI. These parameters varies from different tasks, see Table \ref{tbl.mppi_param}. In the table, two numbers on each entry denotes the different setting in training and evaluation stage.

\begin{table}[t]
\caption{MPPI Parameter in Different Tasks}
\label{tbl.mppi_param}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccr}
\toprule
Task & $\mathrm{Num}$ & $\mathrm{Hor}$ & $\lambda$ & $\mathrm{Std}$ \\
\midrule
Cartpole-Swingup    & 256/512 & 20/25 & 0.01/0.01 & 1.0/0.75 \\
Walker-Walk    & 512/1024 & 25/30 & 0.01/0.01 & 1.0/0.75  \\
Humanoid-Standup    & 1024/1024 & 25/25 & 0.01/0.01 & 1.2/0.75  \\
DexMan-Cube    & 1024/1024 & 5/5 & 0.01/0.01 & 1.0/1.0  \\
DexMan-Bunny    & 1024/1024 & 5/5 & 0.01/0.01 & 1.0/1.0  \\
Go2-Standup    & 1024/1024 & 25/25 & 0.005/0.01 & 1.0/0.75 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\section{Proof of the Lemmas}
\label{sec.appendix_lemma_proof}
\subsection{Proof of Lemma \ref{lemma.cut_in}}
\label{proof.cut_in}
\begin{proof}
    By the definition of $y^{\text{true}}$ in \eqref{equ.truelabel}, 
    \begin{equation}
        f(\boldsymbol{\theta}, \boldsymbol{\xi}^0_{i,j}, \boldsymbol{\xi}^1_{i,j}, y^{\text{true}}_{i,j}) \geq 0
    \end{equation}
    combining the definition of $\mathcal{C}_{i}$ in \eqref{equ.batch_constraint_set}, $\boldsymbol{\theta}_H \in \mathcal{C}_{i},\ \forall i$. Hypothesis space $\Theta_i = \Theta_0 \cap \mathcal{C}_{0} \cap \cdots \cap \mathcal{C}_{i-1}$. Combining $\boldsymbol{\theta}_H \in \Theta_0$ and $\boldsymbol{\theta}_H \in \mathcal{C}_{i},\ \forall i$, $\boldsymbol{\theta}_H \in \Theta_i,\ \forall i$.
\end{proof}
\subsection{Proof of Lemma \ref{lemma.fail}}
\label{proof.fail}
\begin{proof}
    Without loss of generality, suppose $j$th preference in $m$th batch $(\xi^{(0)}_{m,j} ,\xi^{(1)}_{m,j},y^{\text{false}}_{m,j})$ has false label, Thus, 
    \begin{equation}
        f_{m,j}(\boldsymbol{\theta}_H) < 0
    \end{equation}
    Which means $\boldsymbol{\theta}_H \notin \mathcal{C}_{m}$. With definition of $\mathcal{C}_m$ and $\Theta_i$, for all $i> m$,
    \begin{equation}
        \Theta_i \subseteq \Theta_m \subseteq  \mathcal{C}_m
    \end{equation}
    that leads to $\boldsymbol{\theta}_H \notin \Theta_i$ for all $i > m$.
\end{proof}
\subsection{Proof of Lemma \ref{lemma.robust}}
\label{proof.robust}
\begin{proof}
    With the definition of $\gamma$, there are at most $\lceil \gamma N \rceil$ incorrect labels and at least $\lfloor(1-\gamma) N\rfloor$ correct labels in one batch $\mathcal{B}_i$. For all $\boldsymbol{\theta}_H \in \Theta_H$, $f_{i,j}(\theta_H) \geq 0$ holds for all tuples  with correct labels $(\xi^{0}_{i,j} ,\xi^{1}_{i,j},y^{\text{true}}_{i,j})$. As a result, in the voting process of $V_i(\boldsymbol{\theta}_H)$ shown in \eqref{equ.vote_fn}, there will be at least $\lfloor(1-\gamma) N\rfloor$ "+1" votes and $V_i(\boldsymbol{\theta}_H) \geq \lfloor(1-\gamma) N\rfloor$. With the modified definition of $\mathcal{C}_i$ in \eqref{equ.C_i_robust}, $\boldsymbol{\theta}_H \in \mathcal{C}_i$ and we have $\boldsymbol{\theta}_H \in \Theta_i$
\end{proof}

\section{Proof of the Theorem \ref{thm.main}}
\label{sec.appendix_proof}
The proof of the theorem follows the pipeline in Chapter 6 in \cite{ActiveBook}. In order for the paper to be self-contained, we include the complete proof here.
\subsection{Reward-Induced Classifier}
\label{sec.appendix_classifier}
The preference alignment can be viewed as a classification task. Use the new notation $x=(\boldsymbol{\xi}^0,\boldsymbol{\xi}^1)$ to denote a trajectory pair. This pair is fed to the classifier to predict a 0-1 label $y$ of which trajectory is better in human's sense. For any parameterized reward model $r_{\boldsymbol{\theta}}$, a classifier $h_{\boldsymbol{\theta}}$ can be induced by defining:
\begin{equation}
    h_{\theta}(x) = \begin{cases}
        0, J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^0) \geq J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^1) \\
        1, J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^0) < J_{\boldsymbol{\theta}}(\boldsymbol{\xi}^1) \\
    \end{cases}
\end{equation}
Recall that function $g$ is used to calculate the average reward on any trajectory. All of the classifiers induced by $r_\theta$ parameterized by $\theta \in 
\Theta_0$ forms a concept class $\mathcal{H} = \{h_\theta | \theta \in \mathbb{R}^r\}$. We assume the VC dimension of $\mathcal{H}$ is a finite number $d$. 

Specifically, there is a human classifier correspond to $r_{\boldsymbol{\theta}_H}$ which defines the label $y$ corresponds to $x$:
\begin{equation}
    y = h_{H}(x) = \begin{cases}
        0, J_{\boldsymbol{\theta}_H}(\boldsymbol{\xi}^0) \geq J_{\boldsymbol{\theta}_H}(\boldsymbol{\xi}^1) \\
        1, J_{\boldsymbol{\theta}_H}(\boldsymbol{\xi}^0) < J_{\boldsymbol{\theta}_H}(\boldsymbol{\xi}^1) \\
    \end{cases}
\end{equation}
Using $P_{XY}$ to denote the data-label joint distribution and use $P_X$ to define the marginal distribution of $x$ for simplicity. There exists one aligned classifier $h^* \in \mathcal{H}$ such that:
\begin{equation}
    \forall x,y \sim P_{XY}, h^*(x) = y
\end{equation}

Define the error rate of any $h \in \mathcal{H}$ as:
\begin{equation}
    \mathrm{err}(h) = P_{X}(h(x) \neq h^*(x)),
\end{equation}
it is equivalent to express the preference prediction error rate $\mathrm{err}(r_{\boldsymbol{\theta}}) = P(f(\boldsymbol{\theta}, \boldsymbol{\xi}^0, \boldsymbol{\xi}^1, y^{\text{true}}) < 0)$ in Theorem as $\mathrm{err}(r_{\boldsymbol{\theta}}) = \mathrm{err}(h_{\boldsymbol{\theta}})$.

\subsection{Definition of some Helper Functions, Sets and Coefficients}
In this section we establish some definitions of helper functions, sets or coefficients, which are used in the main proof of Theorem \ref{thm.main}. 

\subsubsection{Version Space}
Similar to the definition of $\mathcal{H}$, the classifier set generated by every $\Theta_i$ can be notes as $\mathcal{V}_i = \{h_{\boldsymbol{\theta}} | {\boldsymbol{\theta}}\in \Theta_i\}$, which is typically called \textit{version space} in the context of active learning. By the definition of $\Theta_i$, for all $h \in \mathcal{V}_i$, $h$ can make perfect classfication of all data $(x,y)$ formed by the trajectory pairs and labels in all previous batches $\mathcal{B}_0,\dots\mathcal{B}_{i-1} $. Also, since $\Theta_H \subseteq \Theta_i$, $h^* \in \mathcal{V}_i$ for all the time. Because the hypothesis space for parameters is shrinking, i.e., $\Theta_i \supseteq \Theta_{i+1}$, the version space is also shrinking such that $\mathcal{V}_i \supseteq \mathcal{V}_{i+1}$.

\subsubsection{Difference between Classifiers on $\Xi$ and Corresponding Balls}
The data distribution provides straightforward measure between two classifiers $h_1,h_2 \in \mathcal{H}$, by directly examine the porpotion of data that they behaves differently. A difference $\Delta(h_1,h_2)$ can be defined as the probability of two classifiers behaves differently, under the data distribution $\Xi$:
\begin{equation}
    \Delta(h_1,h_2) = P_{X}(h_1(x) \neq h_2(x))
\end{equation}
With this different as a distance measure, an $\rho-$ball with radius $\rho$ and center $h_c$ can be defined as:
\begin{equation}
    B(h_c, \rho) = \{h \in \mathcal{H}|\Delta(h_c,h) \leq \rho\}
\end{equation}

\subsubsection{Region of Disagreement and Disagreement Coefficient}
\label{sec.appendix_disagree}
For a given version space $\mathcal{V}$ and use $\mathcal{X}$ to denote the instance space of $x$, the region of disagreement is a subset of the instance space:
\begin{equation}
    \mathrm{DIS}(\mathcal{V}) = \{x\in\mathcal{X}|\exists h_1,h_2 \in \mathcal{V}: h_1(x) \neq h_2(x)\}
\end{equation}
With the definition of the region of disagreement, a \textit{disagreement coefiicient} was propose by \cite{hanneke2007bound} to characterize the difficulty of active learning, which is defined with the expression
\begin{equation}
    \zeta = \sup_{\rho>0} \frac{P_{X}(\mathrm{DIS}(B(h^*, \rho)))}{\rho}
\end{equation}
The value $\zeta$ is determined by $\mathcal{H}$ and $\Xi$. The greater $\zeta$ is, the more the volume of the disagreement region of the $\rho-$ball around $h^*$ scales with $\rho$, which signified greater learning difficulty.

\subsection{Passive PAC Sample Complexity Bound}
\label{sec.append_pac}
Since the training set from the distribution $\Xi_{XY}$ is perfectly seperateble as in, using the famous probably approximately correct (PAC) bound for passive learning, with arbitary $\epsilon$ and $\delta$, $P(\mathrm{err}(h) \leq \epsilon) \geq 1 - \delta$ can be achieved by perfectly fit training data with size $L_{PASS}$:
\begin{equation}
    L_{PASS} \leq O(\frac{1}{\epsilon}(d\log(\frac{1}{\epsilon}) + \log \frac{1}{\delta})) \simeq O(\frac{d}{\epsilon})
    \label{equ.l_pass}
\end{equation}

\subsection{Formal Proof of Theorem \ref{thm.main}}
\begin{proof}
    For arbitary $\epsilon$ and $\delta$, define batch size $N$ to be:
    \begin{equation}
        N = \lceil c\zeta(d\log\zeta + \log\frac{1}{\delta'}) \rceil
    \end{equation}
    with $c$ as a constant and  a sepcialy chosen $\delta'$ (the definition of which is shown later in the proof). With probability $1-\delta'$ and for all $h \in \mathcal{V}_{i+1}$:
    \begin{equation}
        P_{X}(h(x) \neq h^*(x)|x\in\mathrm{DIS}(\mathcal{V}_{i})) \leq \frac{c'}{N}(d\log\frac{N}{d} + \log\frac{1}{\delta'})
        \label{equ.proof_error}
    \end{equation}
    This is because data batch $\mathcal{B}_i$ is selected by disagreement \eqref{equ.method_disagree}. By the definition of $\mathcal{V}_{i+1}$, all of the data $\mathcal{B}_i$ comes from the set $\mathrm{DIS}(\mathcal{V}_{i})$ and all of the classifiers in $\mathcal{V}_{i+1}$ perfectly fits this batch. By switching the PAC bound $L_{PASS}$ in Appendix \ref{sec.append_pac} with $M$ and solves for $\epsilon$ we get the RHS of the inequality.

    With proper choice of $c, c'$, $\log\frac{N}{d} \leq \log\zeta$ and the RHS of \eqref{equ.proof_error}:
    \begin{equation}
        \frac{c'}{N}(d\log\frac{N}{d} + \log\frac{1}{\delta'}) \leq \frac{c'}{c\zeta} \leq \frac{1}{2\zeta}
    \end{equation}

    As a result, for all $h \in \mathcal{V}_{i+1}$,
    \begin{align}
        \mathrm{err}(h) = &P_{X}(h(x) \neq h^*(x)) \\
        = &P_{X}(h(x) \neq h^*(x)|x\in\mathrm{DIS}(\mathcal{V}_{i}))P_X(x\in\mathrm{DIS}(\mathcal{V}_{j})) + \\ 
        & P_{X}(h(x) \neq h^*(x)|x\notin\mathrm{DIS}(\mathcal{V}_{i}))P_X(x\notin\mathrm{DIS}(\mathcal{V}_{j}))       
    \end{align}

    By the property $\mathcal{V}_{i+1} \subseteq \mathcal{V}_{i}$, $h \in \mathcal{V}_{i}$ and combining with $h^* \in \mathcal{V}_{i}$, $P_{X}(h(x) \neq h^*(x)|x\notin\mathrm{DIS}(\mathcal{V}_{i})) = 0$ and
    \begin{equation}
        \mathrm{err}(h) \leq \frac{P_X(x\in\mathrm{DIS}(\mathcal{V}_{i}))}{2\zeta} = \frac{P_X(\mathrm{DIS}(\mathcal{V}_{i}))}{2\zeta}
    \end{equation}
    This means $\mathcal{V}_{i+1} \subseteq B(h^*,\frac{P_X(\mathrm{DIS}(\mathcal{V}_{i}))}{2\zeta})$, with the definition of the disagreement coefficient $\zeta$,
    \begin{equation}
        P_X(\mathrm{DIS}(\mathcal{V}_{i+1})) \leq P_X(\mathrm{DIS}(B(h^*,\frac{P_X(\mathrm{DIS}(\mathcal{V}_{i}))}{2\zeta}))) \leq \frac{P_X(\mathrm{DIS}(\mathcal{V}_{i}))}{2}
    \end{equation}
    Thus, let $U_0 = P_X(\mathrm{DIS}(\mathcal{V}_{0}))/ 2\zeta$, after $I= \log_2(U_0 / \epsilon)$ batches of data and with $\delta' = \delta/I$, for all $h \in \mathcal{V}_I$, The relationship
    \begin{equation}
        \mathrm{err}(h) \leq U_0 2^{-I} = \epsilon
    \end{equation}
    holds with a union bound probability $1-I\delta' = 1- \delta$. With the definition of $\mathcal{V}_I$, for all $\theta \in \Theta_I$, $\mathrm{err}(r_\theta) = \mathrm{err}(h_\theta \in \mathcal{V}_I) \leq \epsilon$. Thus, the total number of the quries to perform PAC alignment can be expressed as:
    \begin{equation}
        K = IN = O(\zeta(d\log\zeta + \log\frac{\log 1/\epsilon}{\delta}) \log\frac{1}{\epsilon})
    \end{equation}
    This is the bound shown in Theorem \ref{thm.main} and completes the proof. 
\end{proof}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
