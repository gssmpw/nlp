\section{Related Works}
\subsection{Preference-Based Reward Learning}
\vspace{-5pt}
Learning rewards from preferences falls in the category of learning utility functions in Preference-based Reinforcement Learning (PbRL) ____, and was early studied in  ____. In those early works, typically a set of weights for linear features is learned. With neural network representations, a scalable PbRL framework of learning reward is developed in ____, recently used for fine tuning large language models ____.  In those works, human preference is modelled using  Bradley-Terry formulation ____.
% . Backbone RL algorithms like PPO ____ and SAC ____ is used to optimize the learned reward to achieve good control performance. 
Numerous variants of this framework have been developed ____ over time. Recently, progress has been made to improve human data complexity. PEBBLE ____ used unsupervised pretraining and experience re-labelling to achieve better human query efficiency. SURF ____ applied data augmentation and semi-supervised learning to get a more diverse preference dataset. RUNE ____ encourages agent exploration with an intrinsic reward measuring the uncertainty of reward prediction. MRN ____ jointly optimizes the reward network and the pair of Q/policy networks in a bi-level framework. Despite the above progress, most methods are vulnerable to erroneous human preference. As shown in ____, PEBBLE suffers from a 20\% downgrade when there exists a 10\% error rate in preference data. In real-world applications of the PbRL, non-expert human users tend to make mistakes when providing feedback, due to human mistakes,  malicious interference, or difficulty distinguishing between equally undesirable motions. 


% In this paper, the proposed method can perform well with the existence of the false preference labels by introducing a voting mechanism in the update of the hypothesis spaces, suggesting the potential robustness and adaptiveness in aligning the reward function with human.






\vspace{-5pt}
\subsection{Robust Learning from Noisy Labels}
\vspace{-5pt}
% The noisy labels present in the real-world data ____, which imposes an essential need to perform robust deep learning using such data with a certain rate of the noisy labels. To improve the robustness of deep learning, multiple robust learning approaches has been proposed. The categories of these approaches include Robust Network Architectures ____, Robust Regularization ____, Robust Loss Functions ____ and Sample Selection ____.

To tackle erroneous human preferences, recent PbRL methods draw on the methods in robust deep learning ____. For example, ____ uses an encoder-decoder structure within the reward network to mitigate the impact of preference outliers. ____ trains a discriminator to differentiate between correct and false preference labels, filtering the data to create a cleaner dataset. However, these methods rely on prior knowledge of the correct human preference distribution for latent correction or require additional training.  ____ proposes a robust learning approach based on the mixup technique ____, which augments preference data by generating linear combinations of preferences. While this approach eliminates the need for additional training, false human preferences can propagate through the augmentation process, ultimately degrading learning performance. 

The proposed method differs from existing methods in three key aspects: (1) it requires no prior distribution assumptions for correct or false human preferences, (2) it avoids additional classification training to filter false preferences, (3) without explicitly identifying unknown false preferences, it still ensures robust learning performance.

\vspace{-5pt}
\subsection{Active Learning in Hypothesis Space}
\vspace{-5pt}
Active learning based on Bayesian approaches has been extensively studied ____, and many of these methods can be interpreted through the lens of hypothesis space removal. In particular, the works of ____ are closely related to ours. They iteratively update a reward hypothesis space using a Bayesian approach through (batches of) active preference queries. However, their methods are limited to reward functions that are linear combinations of preset features. More recently, ____ have explored learning reward or constraint functions via hypothesis space cutting, where the hypothesis space is represented as a convex polytope, and human feedback introduces linear cuts to this space. Still, these approaches are restricted to linear parameterizations of rewards.

In addition to their limitations to weight-feature parametric rewards, the aforementioned methods do not address robustness in the presence of erroneous human preference data. In this paper, we show that the absence of robust design makes these methods highly vulnerable to false preference data.