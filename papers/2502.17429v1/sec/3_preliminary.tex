\section{Preliminaries}


\subsection{Transformer-based Segmentation}
We adopt a transformer-based instance segmentation method based on Mask3D \cite{Schult23ICRA}. Specifically, transformer-based segmentation model $\Phi$ is employed for point cloud instance segmentation. Given an input point cloud $p$, the model predicts $\hat{y} = \{(\hat{m_j}, \hat{c_j})\}_{j=1}^J$, which consists of mask predictions and class probabilities for each instance.
The segmentation process begins by quantizing the input point cloud $p$ into voxels $V$, creating voxelized representations of size $\mathbb{R}^{M_0 \times 3}$. Each voxel is assigned an average RGB color computed from the points within that voxel, serving as its initial feature representation. The feature backbone network generates a high-resolution output feature map $\mathbf{F}_0 \in \mathbb{R}^{M_0}$. Additionally, intermediate feature maps are extracted from the decoder layers of the backbone network.
For each intermediate feature map ($r \geq 0$), a set of $K_r$ voxels is selected, and their features are linearly projected to a fixed dimension $D$, yielding feature maps $\mathbf{F}_r \in \mathbb{R}^{M_{r} \times D}$. 

The Transformer decoder initiates with a set of K instance queries and iteratively improves them using L Transformer decoder layers. These layers employ cross-attention to refine the instance queries, incorporating information from point cloud features. The decoder attends to a specific feature map obtained from the corresponding feature backbone layer at each layer, employing conventional cross-attention mechanisms. This process enables the decoder to reason at the instance level through self-attention, resulting in the generation of accurate and contextually relevant instance queries tailored to the specific scene. %: 
% \begin{equation}
% X = \mathrm{Softmax}({QK^T}/{\sqrt{D}})V
% \end{equation}

To achieve this, the voxel features $\mathbf{F}_r \in \mathbb{R}^{M_{r} \times D}$ are transformed into sets of keys $\mathbf{K} \in \mathbb{R}^{M_{r} \times D}$ and values $\mathbf{V} \in \mathbb{R}^{M_{r} \times D}$ through linear projection. The instance queries $\mathbf{Z}$ are also projected to create the queries $\mathbf{Q}$. This enables cross-attention, allowing the queries to gather relevant information from the voxel features. Following cross-attention, a self-attention step occurs among the queries, facilitating information exchange and refinement. The learned queries are then used to make $K$ class and mask predictions, which are matched with ground truth labels through bipartite matching, resulting in $\hat{y} = \{(\hat{m}_j, \hat{c}_j)\}_{j=1}^J$. The model is optimized based on the ground truth label, mask, and class predictions:
% \alert{
\begin{equation}
\label{eq:detr_loss}
\mathcal{L}_\mathrm{Seg}(y_j, \hat{y}_j) = \mathcal{L}_\mathrm{mask}(m_j, \hat{m}_j) + \lambda_\mathrm{cls}\mathcal{L}_\mathrm{cls}(c_j, \hat{c}_j)
\end{equation}
% }

where, mask loss $\mathcal{L}_\mathrm{mask} = \lambda_\mathrm{ce}\mathcal{L}_\mathrm{ce}(y_j, \hat{y}_j) + \lambda_\mathrm{dice}\mathcal{L}_\mathrm{dice}(y_j, \hat{y}_j)$ and $\lambda_\mathrm{cls}\mathcal{L}_\mathrm{cls}$ is classification loss.

The traditional setup assumes all categories are available and well-balanced during training. However, in scenarios where only a subset of categories is present, training the model in multiple phases is required. Unfortunately, such multi-phase models often suffer from forgetting previous tasks. To address this issue, we employ incremental learning strategies which will be discussed in the next section.

% \subsection{Incremental Settings}
% In incremental (or continual) learning (IL) a subset of data $\mathcal{D}^{t} = (\mathcal{X}^{t}, \mathcal{Y}_{t})$ is introduced to the model at time $t = \{1, 2, \dots, T\}$. Based on the input and output distribution of data it can be categorized into four settings: task-incremental, class-incremental, domain-incremental, and task-free IL \cite{hsu2019reevaluating}. Task-incremental settings require task-id at test, as they have isolated output heads $\mathcal{Y}^{t}$ for each task and $P(\mathcal{Y}^{t}) \neq P(\mathcal{Y}^{t+1})$. The class-incremental setting assumes that the output label space expands with each task, where $\mathcal{Y}^{t} \subset \mathcal{Y}^{t+1}$ and $P(\mathcal{Y}^{t}) \neq P(\mathcal{Y}^{t+1})$.  The domain-incremental setting involves different input distributions ($P(\mathcal{X}^{t}) \neq P(\mathcal{X}^{t+1}$), but the output label space remains the same $\mathcal{Y}^{t} = \mathcal{Y}^{t+1}$ for every task. Task-free IL is a more challenging setting where the task data changes smoothly, and the task identity is unknown \cite{De_Lange_2021_ICCV}. In this work, we utilize class incremental setting to introduce new classes in phases over a period of time for 3D instance segmentation.