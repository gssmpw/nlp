% \clearpage
% \setcounter{page}{1}
\maketitlesupplementary

\appendix 
\renewcommand{\thesection}{Appendix \Alph{section}}

In this supplementary material, we first demonstrate the performance gains on rare classes achieved by incorporating the IC module in \ref{app:rare_eval}. Next, we provide detailed split information for all scenarios, based on class names, in \ref{app:split_fullview}. Finally, we present a qualitative comparison between the baseline method and our proposed approach in \ref{app:qual_results}.
% \input{tab/common_rare_eval}

\section{Evaluation on Rare Categories}
\label{app:rare_eval}
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to address the performance gap for rare classes. To assess its impact, we compare its performance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model encounters infrequently compared to more common classes.

\input{tab/common_rare_eval}

The results, shown in \Cref{tab:common_eval} and \Cref{tab:tail_eval}, correspond to evaluations on \fsplit~ for \textit{Phase 2} and \textit{Phase 3}, respectively. In \textit{Phase 2}, we evaluate classes seen 1–20 times per epoch, while \textit{Phase 3} targets even less frequent classes, with observations limited to 1–10 times per epoch. 
% Note that \textit{Phase 2} utilizes a standard split of the ScanNet200 dataset, which includes both frequent and rare classes, justifying the 1–20 observation threshold.

As illustrated in \Cref{tab:common_eval}, the IC module substantially improves performance on rare classes in terms of \mapft~ in Phase 2 of \fsplit. For instance, classes like \texttt{recycling bin} and \texttt{trash bin}, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied. Overall, the IC module provides an average boost of 8.32\%, highlighting its effectiveness in mitigating class imbalance.

\input{tab/tail_rare_eval}
Similarly, \Cref{tab:tail_eval} presents results for \textit{Phase 3}, demonstrating significant gains for infrequent classes. For example, even though the classes such as \texttt{piano}, \texttt{bucket}, and \texttt{laundry basket} are observed only once, IC module improves the performance by 52.30\%, 10.40\%, and 13.60\%, respectively. The ER+KD module does not focus on rare classes like \texttt{shower} and \texttt{toaster} which results in low performance, but the IC module compensates for this imbalance by focusing on underrepresented categories. On average, the addition of the proposed IC module into the framework outperforms ER+KD by 12.13\%.

\input{tab/split_fullview}
\section{Incremental Scenarios Phases}
\label{app:split_fullview}
\Cref{tab:split_fullview} presents the task splits for each proposed scenario introduced in Section 4.3 using the ScanNet200 dataset. The three scenarios, \fsplit, \ssplit, and \rsplit, are each divided into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is random. 


\section{Qualitative Results}
\label{app:qual_results}
In this section, we present a qualitative comparison of the proposed framework with the baseline method. \Cref{fig:fsplit_qual} illustrates the results on the \fsplit~evaluation after learning all tasks, comparing the performance of the baseline method and our proposed approach. As shown in the figure, our method demonstrates superior instance segmentation performance compared to the baseline. For example, in row 1, the baseline method fails to segment the \texttt{sink}, while in row 3, the \texttt{sofa} instance is missed. Overall, our framework consistently outperforms the baseline, with several missed instances by the baseline highlighted in red circles.

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=\linewidth]{fig/freq_qual_grid_annot.pdf}
  \caption{Qualitative comparison of ground truth, the baseline method, and our proposed framework on the \fsplit~ evaluation after learning all tasks.}
  \label{fig:fsplit_qual}
\end{figure*}

In \Cref{fig:ssplit_qual}, we present the results on \ssplit, highlighting instances where the baseline method underperforms, marked with red circles. For example, in row 2, the baseline method incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing machine is segmented into two instances by the baseline. In contrast, the proposed method delivers results that closely align with the ground truth, demonstrating its superior performance

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=\linewidth]{fig/sema_qual_grid_annot.pdf}
  \caption{Qualitative comparison of ground truth, the baseline method, and our proposed framework on the \ssplit~ evaluation after learning all tasks.}
  \label{fig:ssplit_qual}
\end{figure*}

Similarly, \Cref{fig:rsplit_qual} highlights the results on \rsplit, where classes are encountered in random order. The comparison emphasizes the advantages of our method, as highlighted by red circles. The baseline method often misses instances or splits a single instance into multiple parts. In contrast, our approach consistently produces results that are closely aligned with the ground truth, further underscoring its effectiveness.

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=\linewidth]{fig/rand_qual_grid_annot.pdf}
  \caption{Qualitative comparison of ground truth, the baseline method, and our proposed framework on the \rsplit~ evaluation after learning all tasks.}
  \label{fig:rsplit_qual}
\end{figure*}