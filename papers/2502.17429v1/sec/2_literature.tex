\section{Related Work}
\label{sec:related_work}
This section reviews the current literature on 3D instance segmentation and incremental learning methods, including the limited work addressing incremental learning for 3D scene-level tasks.

\subsection{3D Instance Segmentation}
% weak supervision: 
Various approaches have been proposed for 3D instance segmentation. One common approach adopts a bottom-up pipeline, in which an embedding in the latent space is learned to facilitate the clustering of object points \cite{wang2018sgpn,lahoud20193d,jiang2020pointgroup,zhang2021point,chen2021hierarchical,han2020occuseg,he2021dyco3d,liang2021instance}. 
These methods are also known as grouping-based or clustering-based methods.
Other methods use a top-down approach, also known as proposal-based methods, where 3D bounding boxes are first detected, then the object region is segmented within the box \cite{yang2019learning, hou20193dsis,liu2020learning,yi2019gspn,engelmann20203d}.
Recently, the transformer architecture \cite{vaswani2017attention} has also been used for the task of 3D instance segmentation \cite{Schult23ICRA,sun2022superpoint}, motivated by work in 2D \cite{cheng2022masked,cheng2021per}. 
While these methods propose various models for improving the quality of the object segments, they rely on the availability of annotations for all object categories. On the other hand, we target learning in a progressive manner, in which new semantic annotation is provided and past data is inaccessible.
 
In order to reduce the annotation cost for 3D instance segmentation, various methods propose weakly supervised alternatives to methods that use dense annotations \cite{xie2020pointcontrast, hou2021exploring,chibane2022box2mask}.
While these methods improve the ability to learn from a small set of annotated examples, they rely on a fixed set of semantic labels, so they are prone to catastrophic forgetting in an incremental setting.

\subsection{Incremental Learning}
 Incremental, lifelong, or continual learning methods aim to train a machine learning model sequentially to avoid ``catastrophic forgetting" which is caused by training the model on a set of data and later training on another set of data. There are several methods have been proposed for this paradigm, these methods can be divided into three categories: (i) Model Regularization \cite{kirkpatrick2017overcoming, aljundi2018memory, li2017learning} methods limit the plasticity of model parameters to avoid catastrophic forgetting of previous tasks. These methods include weight regularization such as EWC \cite{serra2018overcoming} and function regularization such as knowledge distillation \cite{hinton2015distilling}. (ii) Exemplar replay approaches either create a subset of the past task data or generate samples using generative models to avoid privacy concerns and save those in memory to replay while learning new tasks. This method is effective in more challenging settings and datasets \cite{rebuffi2017icarl,kamra2017deep, buzzega2020dark, cha2021co2l}. (iii) Dynamic network expansion-based method learns a new task by either dynamically expanding the model \cite{rusu2016progressive, li2019learn, zhao2022deep} or by creating a subset of the model \cite{zhao2022deep, wang2020learn,ke2020continual, rajasegaran2019random} to learn to cater for a new task. 
 % In this work, we are making use of ER and KD.

Recent approaches to 3D class-incremental segmentation, such as \cite{Yang_2023_CVPR} and \cite{su2024balanced}, have made some initial contributions. However, these methods often fall short in performance as they do not leverage state-of-the-art 3D segmentation models and are primarily focused on semantic segmentation, while our work emphasizes object-level instance segmentation. Kontogianni et al. \cite{kontogianni2024continual} propose a general online continual learning framework and evaluate it on 3D dataset segmentation. Similarly, \cite{boudjoghra20243d} addresses the open-world 3D incremental learning problem but relies heavily on an extensive memory buffer. 
% Additionally, their distillation strategy is designed to retain knowledge of unknown classes for future tasks. 
In contrast, our work introduces a dedicated continual learning framework for 3D instance segmentation, with a focus on effective knowledge transfer from previous tasks, while also accounting for the challenges posed by infrequent class occurrences.
