
\section{Experiments}
We evaluate our method across three scenarios specifically designed to capture the complexities of real-world settings, where new categories emerge incrementally alongside class imbalance, as discussed in \Cref{sec:scenarios}. Experimental results demonstrate that our approach effectively handles the gradual introduction of new classes and mitigates the impact of class imbalance within these scenarios. The following sections detail the datasets, evaluation metrics, incremental scenarios, and implementation procedures, followed by a thorough analysis and comparison of the results.

\subsection{Experimental Setup}

\cpara{Datasets} We evaluate our method using the ScanNet200 dataset \cite{rozenberszki2022language}, which includes 200 object categories and exhibits inherent class imbalance, making it well-suited for simulating and evaluating real-world scenarios. Additionally, we benchmark our method against existing incremental learning approaches using the original ScanNet dataset \cite{dai2017scannet} in a semantic segmentation setting. We follow the standard training and validation splits as defined in prior works. % to ensure consistency with the broader research community and facilitate direct comparisons with existing methods.

\cpara{Evaluation Metrics}
We evaluate our method using mean Average Precision (mAP), a standard metric for 3D instance segmentation that provides a comprehensive measure of segmentation quality, accounting for both precision and recall.
For comparison with existing semantic incremental learning approaches, we report mean Intersection over Union (mIoU), which measures the overlap between predicted and ground truth instances, offering a detailed evaluation of segmentation accuracy.
To assess the model’s ability to mitigate catastrophic forgetting in continual learning scenarios, we use the Forgetting Percentage Points (FPP), as defined in \cite{liu2023continual}. This metric quantifies performance degradation by measuring the accuracy drop between the initial and final training phases, on the categories observed in the first training phase.

\cpara{Incremental Scenarios} 
As discussed in \Cref{sec:scenarios}, we design three incremental scenarios: \fsplit, \ssplit, and \rsplit, each consisting of three tasks, which are grouped based on object occurrence frequency, semantic similarity, and random grouping, respectively. In \fsplit, the frequency of object categories progressively decreases through the tasks. This scenario follows the head, common, and tail splits present in the ScanNet200 dataset, with class distributions of \texttt{66-68-66} in each split. In \ssplit, we partition the classes into \texttt{74-50-76} based on semantic similarity, which is calculated using the CLIP \cite{radford2021learning} text encoder, followed by clustering using K-Means. Finally, in \rsplit, the classes are shuffled and split into three sets, resulting in \texttt{67-67-66} categories per split. These scenarios allow for a comprehensive evaluation of our approach under varying conditions, facilitating a deeper understanding of its performance and generalization in diverse real-world settings.

\cpara{Implementation Details} We utilize the transformer-based model for 3D instance segmentation proposed in \cite{Schult23ICRA}, designed to iteratively attend to hierarchical feature representations. The model processes the 4 coarsest levels of a ResNet-based U-Net backbone across three iterations, progressively refining from coarse to fine, resulting in $L=12$ transformer decoder layers. Each transformer decoder layer shares weights across iterations and consists of a standard transformer layer utilizing self-attention and masked cross-attention mechanisms. The feature backbone employed is Minkowski Res16UNet34C \cite{choy20194d}.

\cpara{Training Details} We adopt the data augmentation, hyperparameters, and training strategy described in \cite{Schult23ICRA}. For joint training (Row 1, \Cref{tab:ablation}), the model is trained for 600 epochs using the AdamW optimizer \cite{loshchilov2017decoupled} with a one-cycle learning rate scheduler \cite{smith2018superconvergence}, and results are evaluated on the entire validation set. In incremental training, we retain the same hyperparameters, adjusting only the number of epochs, and we use a memory buffer size of 50 scenes. Training is divided into three phases, introducing one split per phase across the three designed scenarios. After each phase, the resulting model is evaluated on all classes encountered up to that point.% using the ScanNet evaluation set \cite{dai2017scannet}.

\subsection{Results and Discussion}
\input{tab/er_comparison}
To evaluate our proposed method, we conduct a comparative analysis using exemplar replay (ER) for instance segmentation and \cite{Yang_2023_CVPR} for semantic segmentation as baselines. As shown in \Cref{tab:er_comparison}, our method, which integrates exemplar replay, knowledge distillation, and an imbalance correction (IC) module, achieves notable improvements over the baseline in terms of mAP and FPP. Specifically, in the \fsplit~ scenario, our approach significantly enhances overall performance. We observe an improvement of 19.23\%, 31.05\%, and 12.28\% for \mapft, \maptf, and overall mAP, respectively, while reducing forgetting by 47.86\% as measured by \mapft.
% 
For the \ssplit~ scenario as well, CLIMB-3D demonstrates a consistent performance boost over the baseline, significantly improving mAP and reducing forgetting, which is lowered to 5.52\% compared to 46.21\% in the baseline for AP50.
% 
Likewise, in the \rsplit~ scenario, CLIMB-3D enhances both learning efficiency and forgetting reduction, achieving a performance of 26.78\% in \mapft~ and reducing forgetting by 20.95\% compared to baseline. These results across scenarios underscore the effectiveness of our approach.

\input{tab/sseg_comparison}
% \Cref{tab:sseg_compare} presents a comparative analysis with existing methods for class-incremental semantic segmentation on the ScanNet V2 dataset. 
Although our method focuses on segmenting individual objects (instance segmentation), we also demonstrate its performance in semantic segmentation by presenting a comparative analysis with existing methods for class-incremental semantic segmentation on the ScanNet V2 dataset (\Cref{tab:sseg_compare}). Using our predicted labels, we assign each point the label corresponding to the highest confidence mask and exclude background labels (floor and wall), as these are not part of the object-level segmentation.
Following the dataset splits established by \cite{Yang_2023_CVPR}, we report results for both training phases. Our proposed method achieves a substantial improvement over prior methods, with a gain of 35.23\% in Phase 1 and approximately 19.1\% in Phase 2. Overall, our method reaches a mIoU of 59.38\%, significantly outperforming previous baselines, which achieve a lower mIoU of around 30\%.

\input{tab/ph_comparison}
We extend the analysis from \Cref{tab:er_comparison} to \Cref{tab:ph_comparison} to highlight the impact of our proposed method on individual splits across various scenarios. The results clearly demonstrate that our model consistently retains knowledge of previous tasks better than the baseline. For \fsplit~, our model shows improvement throughout the phase. In Phase 3 of (\textbf{s2}), although both the baseline and our method exhibit a performance drop, our method reduces forgetting significantly compared to the baseline.
% \alert{
The \ssplit~ scenario, while more complex than \fsplit, achieves comparable results due to semantic similarity among classes within the same task. In Phase 2, our model achieves overall all 43.13\% \mapft~compared to 24.53\% on baseline, a similar trend is observed in Phase 3, where our method not only consistently improves learning but also enhances retention of previous information. After all three tasks, our method achieves an overall performance of 31.56\% AP50, compared to 15.07\% for the baseline.
% }
% 
In the \rsplit~ scenario, the first-stage model struggles due to the increased complexity introduced by random grouping. In Phase 2, while the baseline focuses on learning the current task, it suffers from severe forgetting of prior knowledge. Conversely, our method balances new task learning with the retention of earlier information. By Phase 3, the model effectively consolidates \textbf{s1} and maintains strong performance across all task splits. Overall, our proposed method improves mAP by 5.6\%.


% \cpara{Ablation}
\subsection{Ablation}

To assess the effectiveness of each component in our proposed framework, we perform an ablation study. Initially, we establish an upper-bound performance by jointly training the model on the complete dataset using a transformer-based architecture, such as Mask3D \cite{Schult23ICRA}, referred to as the \textit{Oracle}. For the incremental learning setup, we generate training splits according to the scenarios outlined earlier. In this study, we first train the model naively across phases and then sequentially integrate each module to evaluate its individual contribution to performance. \Cref{tab:ablation} summarizes the results for the \fsplit~ scenario, using both the \mapft~ and FPP metrics.

\input{tab/ablation}

\cpara{Na\"ive Training} In the naive incremental training setup, where no dedicated modules are incorporated, the model learns the current task but suffers from catastrophic forgetting of the previously learned tasks, as expected. This behavior is evident in row 2, where, upon transitioning to phase 2, the model entirely forgets the classes learned during phase 1. A similar trend is observed in phase 3, and this pattern is also reflected in the FPP metric.

\cpara{Effect of Exemplar Replay} To mitigate catastrophic forgetting, we incorporate exemplar replay, which stores and replays examples from previous tasks. As shown in row 3, exemplar replay improves average precision by 18.5\% for \textbf{s1} in phase 2 and 10.38\% in phase 3. It also reduces forgetting for \textbf{s2} by 9.43\% in phase 3, while slightly improving learning on the current task. However, substantial forgetting persists, as reflected in the FPP metric, highlighting the limitations of exemplar replay alone.

\cpara{Effect of Knowledge Distillation} The addition of knowledge distillation (KD), which retains a copy of the model from previous tasks, facilitates the preservation of past task knowledge while enabling forward knowledge transfer. As shown in row 4, KD considerably reduces forgetting and boosts performance on the current task. Specifically, for \textbf{s1}, KD improves \mapft~by 31.49\% in phase 2 and by 39.40\% in phase 3, compared to exemplar replay. Overall, KD leads to a 15.00\% increase in performance while reducing forgetting by 39.40\% after all tasks have been learned.

\cpara{Effect of Imbalance Correction} The imbalance correction module addresses the class imbalance in the dataset by re-weighting the teacher model’s predictions during KD based on class frequency. As highlighted in row 5 of \Cref{tab:ablation}, this addition further improves performance. Specifically, for \textbf{s1}, imbalance correction reduces forgetting by 4.67\% and 4.41\% in phases 2 and 3, respectively, compared to the results without this module (row 4). For \textbf{s2}, while a slight decrease in current task performance is observed in phase 2, this is likely due to the module’s prioritization of mitigating forgetting less frequent classes in previous tasks. In phase 3, performance on \textbf{s2} and \textbf{s3} improves. Overall, imbalance correction significantly reduces forgetting, achieving improvements of 4.41\% and 43.81\% over KD and exemplar replay, respectively.