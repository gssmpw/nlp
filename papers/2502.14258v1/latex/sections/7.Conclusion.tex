\section{Conclusion}
\label{sec:conclusion}
We systematically investigate how LLMs can handle \emph{temporal knowledge}, focusing on time-dependent facts. 
Through our experiments, we uncovered \emph{Temporal Heads} that selectively mediate the activation of time-variant knowledge.
Ablating these heads leads to temporal mismatches while leaving time-invariant knowledge and general QA performance unaffected.
Note that these heads are also activated under textual conditioning, and using their value for editing successfully changes the models' responses with minimal intervention.

As a foundational step, our work explores how LLMs can actively manage temporal information rather than merely integrating temporal context.
We believe our analysis offers valuable insights into the inner mechanisms of LLMs and can inspire future approaches for \emph{time-aware model alignment} and \emph{precise temporal updates} by selectively targeting \emph{temporal heads}, rather than relying on global retraining. 
