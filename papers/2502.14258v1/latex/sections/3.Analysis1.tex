\section{Knowledge Circuit Deciphers Temporal Head in LLMs}
\label{sec:knw-circuit-reuse}
We now explore how \emph{knowledge circuits}, extracted via EAP-IG pruning, can reveal specialized \emph{Temporal Heads} in large language models (LLMs). 
We extend knowledge circuits in \ref{subsec:knowledge_circuit} to 
\emph{temporal knowledge circuits} by analyzing how the same subject--relation pair can produce different objects across multiple time points. 
Specifically, we seek to identify which edges encode time-dependent specificity, such that an edge $e_i$ is crucial for predicting the time-relevant object $o_k$ at period $T_k$.
Given a knowledge circuit score $S(e_i)$ (Eq.~\ref{eq:knw-circuit-score}), 
we define its temporal variant as follows:
\begin{equation}
\begin{split}
S(e_i, T_k) = &\; \log p_G(o_k \mid s,r,T_k) \\
& - \log p_{G/e_i}(o_k \mid s,r,T_k) > \tau.
\end{split}
\end{equation}
where $T_k$ indicates a specific time (or period), and $o_k$ is the corresponding object for subject $s$ and relation $r$ at time $T_k$. 
Thus, $S(e_i, T_k)$ measures the contribution of edge $e_i$ to correctly predicting $o_k$ under time $T_k$.
For highlighting importance and simplifying graphs, edges retained in the temporal circuit satisfy $S(e_i, T_k) > \tau$, ensuring they encode time-dependent knowledge. 
Here, we decide to attach temporal conditioning in front of subject, following prior insight from causal tracing (\S\ref{sec:causal_tracing_theme}) and details in Appendix~\ref{sec:temporal-influence}.

\subsection{Implementations}
\label{subsec:impl}
We conduct experiments primarily on three LLMs: Llama-2-7b-chat-hf~\citep{llama2}, Qwen1.5-7B-Chat~\citep{qwen, qwen1.5}, Phi-3-mini-4k-instruct~\citep{phi3}.
We adopt transformer lens~\citep{transformerlens} to intercept and ablate model components, enabling \textbf{EAP-IG}-based circuit discovery.
We mainly illustrate results on Llama2, though similar trends emerge in the other models.
More details are described in Appendix~\ref{sec:detail_in_eapig}.

\subsubsection{Circuit Reproduction Score}
To evaluate how well a pruned circuit reproduces the full model’s behavior, we define the \emph{Circuit Reproduction Score} (CRS), ranging from $0$ to $100$. 
Let $B$ be the baseline performance of the full model on time-conditioned prompts, and $P$ be the performance of the pruned circuit. 
If the pruned circuit maintains or exceeds the baseline performance ($P \geq B$ when $B>0$), we assign it the maximum CRS as follows:
\begin{equation}
\mathrm{CRS}(B,P) = 100.
\end{equation}
Otherwise, the score follows an exponential decay:
\begin{equation}
\mathrm{CRS}(B,P) = 100 \times \sigma \exp\left(-\alpha \frac{d}{|B|} \right),
\end{equation}
where $d = \max\{B, 0\}$.
The factor $\sigma \in (0,1]$ accounts for sign mismatches, adjusting for cases where the pruned circuit’s output deviates in direction from the full model.
A higher CRS indicates better reproduction of the full model’s predictions. Details on hyperparameters and adjustments are deferred to the Appendix~\ref{sec:detail_in_crs}.

\subsection{Dataset}
\label{subsec:dataset}
Our dataset comprises (statistics in Appendix~\ref{sec:dataset_details}):
\begin{itemize}
    \item \textbf{Temporal Knowledge}: Various categories of knowledge samples that embed a specific year (e.g., \emph{1999}, \emph{2004}, and \emph{2009}) alongside a factual statement (e.g., which sports team or president is correct in that year) based on Wikidata~\citep{wikidata}.
    \item \textbf{Time-Invariant Knowledge}: Commonsense data from LRE~\citep{lre} (e.g., \emph{object superclass}, \emph{fruit inside color}), plus newly implemented numerical facts embedded in subject/object (e.g., \emph{geometric shape} or \emph{roman numerals}). 
    These tasks assume no explicit time-based shift.
    \item \textbf{Unstructured QA}: We utilize TriviaQA~\citep{triviaqa} and Math~\citep{mathkg} QA in ChroKnowledge~\citep{chroknowledge} for unstructured, general QA to verify the ablation effect with basic LLM's tasks.
\end{itemize}
For each data point, we run both a \emph{clean} prompt and a \emph{corrupted} prompt, following EAP-IG guidelines.
We focus on the first token(s) that differ, capturing the key transition that determines correctness.
In the QA setting, we evaluate models using standard TriviaQA validation metrics, including exact match (EM) and F1 scores. 
For Math ChroKnowledge, we employ a multiple-choice QA (MCQA) template, scoring responses based on probability (\%). 
Given that models possess some degree of inherent knowledge~\citep{KC}, we assess their performance under zero-shot and greedy decoding.

\begin{table}[t]
% \vspace{-10pt}
\centering
\vspace{-5pt}
{\resizebox{\columnwidth}{!}{
\begin{tabular}{llllll}
\toprule
\multicolumn{2}{l}{\textbf{Category}} & \textbf{Knowledge} & \textbf{\#Node} & \textbf{\#Edge} & \textbf{CRS} \\ 
\midrule
\multicolumn{6}{l}{\textbf{\textit{Temporal}}} \\ 
\midrule
Sports     &            & Nicolas Anelka     & 29  & 37  & 74.14 \\ 
    &            & David Beckham       & 43  & 80  & 39.53 \\ 
Presidents &            & Argentina          & 42  & 102 & 60.97 \\ 
 &            & South Korea        & 46  & 110 & 65.55 \\ 
CEO        &            & Hewlett-Packard    & 52  & 115 & 53.49 \\ 
       &            & Chrysler           & 51  & 97  & 57.10 \\ 
Defense    &            & United States      & 50  & 137 & 48.08 \\ 
 &            & China              & 19  & 19  & 37.62 \\ 
\midrule
\multicolumn{3}{l}{\textbf{Avg}} & \textbf{42} & \textbf{87} & \textbf{54.56} \\ 
\midrule
\multicolumn{6}{l}{\textbf{\textit{Time-Invariant}}} \\ 
\midrule
CommonSense             &            & Object Superclass  & 43  & 56  & 44.47 \\ 
Conditional CS &            & Fruit Inside Color & 76  & 131 & 53.08 \\ 
Num in Obj     &            & Geometric Shape    & 52  & 118 & 76.09 \\ 
Num in Sub     &            & Roman Numerals     & 43  & 135 & 95.70 \\ 
\midrule
\multicolumn{3}{l}{\textbf{Avg}} & \textbf{54} & \textbf{110} & \textbf{67.33} \\ 
\bottomrule
\end{tabular}}}{}
\caption{Statistics of temporal knowledge circuits for Llama2, both temporal and time-invariant knowledge.
For temporal knowledge, each type of knowledge is reproduced with three selected years: \textbf{1999, 2004, and 2009}.
The numbers of nodes, edges and CRS is the average of each knowledge's yearly circuits.
}
\label{table:statistic_crs}
\vspace{-10pt}
\end{table}

\subsection{Evaluation}
\label{subsec:eval}
After pruning less-contributory nodes via EAP-IG, we measure how well the \emph{resulting subgraph} preserves the model’s original performance on each knowledge type. 
Table~\ref{table:statistic_crs} and~\ref{table:statistic_crs_qwen}--\ref{table:statistic_crs_phi} shows the average number of \textbf{nodes} and \textbf{edges} in these pruned circuits, along with their \textbf{CRS}.
We then apply threshold \(\tau\) to remove edges/nodes that contribute marginally to object prediction, retaining only edges with scores above \(\tau\) and their corresponding nodes.

In Llama2, both temporal and time-invariant knowledge circuits effectively capture the model’s internal knowledge flow, with average CRS exceeding 50 in both cases. 
However, temporal circuits exhibit more variability, likely due to the inherent complexity of year-based facts. 
These tasks demand precise temporal conditioning, adding an extra difficulty, not just simply generating any possible objects. 
Even when models are expected to retain such knowledge, the increased complexity underscores the nuanced nature of temporal reasoning compared to time-invariant knowledge.

\subsection{Findings}
\label{subsec:findings}
We now identify \textbf{common nodes} in all circuits (e.g., \texttt{[input]}, \texttt{[logits]}, MLP \texttt{m2}, \texttt{m24}, \texttt{m30}, etc.) and a set of \textbf{temporal-only} nodes that appear exclusively in circuits for year-dependent prompts as in Figure~\ref{fig:overview}.
Firstly, most MLP nodes were appeared both temporal and time-invariant knowledge, as they are activated for storing knowledge~\citep{geva2021mlp, KN, whatKN}.

What is impressive stood out in the attention heads.
\emph{\textbf{Temporal Heads}}, appearing in almost every temporal knowledge circuits but not time invariants, are shown: \verb|a15.h0|, \verb|a18.h3| in Llama2.
Those temporal heads reoccur across multiple year-specific circuits, and it is different for other model's cases like \verb|a17.h15| for Qwen 1.5 in Table~\ref{table:temporal_heads}.
Visualizing their attention maps in Figure~\ref{fig:overview} (C) indicates a strong focus on \emph{“In 19xx”} and subsequent subject phrases, as key tokens revolve around temporal conditions with queries hooking into the subjects.
This pattern corroborates the idea that these heads facilitate year-subject binding—justifying the label “temporal”, as this kind of task specific attention heads were previously suggested by~\citealp{ioi, circuitcolor, subhead, rethead} and ~\citealp{headsurvey}.

When lowering the ratio of exhibition (e.g., 70-80\%), additional heads (e.g., \verb|a0.h15|, \verb|a20.h17|, \verb|a31.h25|) emerge.
These \emph{Backup Temporal Heads} are also exclusive to temporal knowledge circuits, though their emerging varies different among types of knowledge and years.
But interestingly, even at high ratio, no heads are exclusive in time-invariant knowledge circuits.
This suggests that many “general knowledge” heads overlap with or are reused by knowledge recalling tasks, whereas certain specialized heads exist \emph{only} for time-based tasks.

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=1\textwidth]{latex/fig/log_prob_sample6.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Log probability results with temporal knowledge; \textit{In XXXX, the president of South Korea was}.
(A) shows prediction probability change among results of Llama2.
The effect of head ablation reacts differently for each selected year with the same prompt.
Each subplot in (A) represents the probability distribution of correct (\textcolor{green!60!blue}{green}) and incorrect (\textcolor{red!80!white}{red}) predictions, where the x-axis denotes probability values and the y-axis differentiates between target and non-target responses.
Total results for each model are in Figures~\ref{fig:log_prop_app1}--\ref{fig:log_prop_app2} in Appendix.
(B) illustrates the performance degradation trends across various years.
As averaging the result of ablation, the \textcolor{gray}{\textbf{gray space}} between two line plots represent degradation level pointed out by \textcolor{red!70!black}{\textbf{red arrows}} (which becomes darker and bigger when the gap is wider).
The background shows how objects were changed in the time range between 1999 to 2009.}
\label{fig:log_prob}
\vspace{-10pt}
\end{figure*}

In the next (\S\ref{sec:indepth-analysis}), we delve into further ablation experiments to verify that ablating temporal heads indeed degrades year-specific predictions, reinforcing their role as the crucial channel through which the model recall knowledge conditioned on time.
