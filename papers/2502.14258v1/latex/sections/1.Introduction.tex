\section{Introduction}
\begin{quote}
    \emph{``Remembrance of things past is not necessarily the remembrance of things as they were.''~\citep{proust}}
\end{quote}
This profound and intricate relationship between memory and truth resonates deeply with one of the central challenges in modern artificial intelligence. 
While large language models (LLMs) like GPTs~\citep{chatgpt, gpt4omini, gpto1} and LLaMA familiess~\citep{llama, llama2, llama3} have demonstrated remarkable capabilities in leveraging factual knowledge, they face a unique challenge that mirrors human memory: 
the accurate representation of \emph{temporal knowledge}—facts that transform across different time points.

Unlike static facts (e.g., “The capital of France is Paris”), many real-world facts change over time (e.g., a politician’s term in office, a sports player’s team membership in a given year). 
This time-evolving nature necessitates that LLMs accurately capture such change.
To do so, they must not only track newly updated facts within a specific timeline, but also retain historical information across different time periods~\citep{temporalwiki}.
This presents a significant challenge, as models must contend with tracking and reasoning over temporal changes in knowledge~\citep{realtime}.
However, beyond prompting~\citep{serac, chroknowledge} or retrieval-augmentated generation~\citep{rag, hipporag}, the internal mechanisms by which models adapt to temporally evolving facts remain relatively underexplored.

\begin{figure}[t]
% \vspace{-10pt}
\begin{center}
    \includegraphics[width=\columnwidth]{latex/fig/sample_prompt5.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Temporal Heads exist within various TKCs at different times $T_k$.
Ablating them disrupts the model's temporal alignment, yielding incorrect objects.
}
% \caption{Temporal Heads can be identified within various Temporal Knowledge Circuits (TKCs) across different time points $T_k$. 
% Ablating these heads disrupts the model's temporal alignment, resulting in temporally incorrect objects coming out.
% }
\label{fig:intro_sample}
\vspace{-20pt}
\end{figure}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=1\textwidth]{latex/fig/overview5.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Overview of temporal knowledge circuit analysis.
(A): Construct temporal knowledge circuits (TKCs), and compare it with general knowledge circuits (KCs) using time-invariant knowledge.
Circuits reproduce residual streams for time~\textbf{\textcolor{cyan!70!gray}{T}}, subject~\textbf{\textcolor{pink!80!gray}{S}} and relation~\textbf{\textcolor{yellow!80!gray}{R}}.
This verifies temporal heads only found in each different TKCs of various year $T_k$.
(B): Example of simplified TKC.
Here, basic knowledge nodes is colored \textcolor{violet}{violet}, (common in both), while \textbf{\textcolor{blue!60!cyan!80!black}{Temporal Heads}} is highlited.
(C): Attention map for temporal heads.
\textbf{a15.h0} means the 15th layer's first attention head.
Each head's attention pattern is represented as the output logits of the hean by mapping to vocabulary space.
Queries are input tokens focusing on others, while keys are the tokens being focused on.
Values represent attention weights, indicating the strength of this focus.
Total results are in Figures~\ref{fig:total_circuit}--\ref{fig:total_circuit2} and \ref{fig:full_attn}--\ref{fig:full_attn_phi}.
}
\label{fig:overview}
\vspace{-10pt}
\end{figure*}

Empirical observations suggest that LLMs already possess some level of temporal awareness~\citep{nylund2023time, dyknow}.
This raises the question of whether the model is inherently capable of encoding and utilizing temporal knowledge.
For instance, when prompted with time-specific queries like ``\textit{In 1999, [X] was a member of sports team}'', the model may generate the correct team \textit{[Y]} relevant to that year, indicating that certain time-conditional links are embedded in its internal parameters. 
The key puzzle, however, is how this temporal knowledge is organized and recalled.
Do LLMs internally have a place for \textbf{Time}, adjusting their factual outputs based on the input time condition?
If so, where within the model architecture—among the attention heads and feed-forward layers—does this mechanism reside?

To address them, we apply Circuit Analysis~\citep{ elhage2021mathematical, ioi} to reconstruct the model’s computations via localized subgraphs of attention heads, feed-forward networks (FFN), and residual streams.
Especially, by systematic ablating (zeroing out) attention heads or MLP components, it pinpoints which parts are responsible for eliciting knowledge in each knowledge recalling inference~\citep{KC}.
These knowledge circuits enable to measure how much each nodes or edges in subgraph contribute to processing facts.

We extend it into temporal dimension, capturing how models reacts to time-evolving attributes using Temporal Knowledge Circuits (Figure~\ref{fig:intro_sample} (A)).
We then identify \textbf{\textit{Temporal Heads}}, such as \textit{a15.h0} and \textit{a18.h3}, which are exclusively activated for temporal knowledge while remaining inactive for time-invariant information.
Each model have its own temporal heads, which exhibit a strong influence on temporal input tokens in attention maps.
Moreover, ablating these heads significantly reduces time-specific factual accuracy, leading to temporal mismatches as suggested in Figure~\ref{fig:intro_sample} (B).

One step further, we explore in-depth impacts of temporal heads among different years, knowledge and conditioning types.
Ablating them exclusively affects temporal information, while having negligible impact on time-invariant knowledge and general QA performance.
Notably, these temporal heads are activated for both numerical expressions (“In 2004”) and textual conditions (“In the year the Summer Olympics were held in Athens”), indicating that they encode a broader temporal dimension beyond simple numerical representation.
Building on this, we demonstrate that \textit{temporal knowledge editing}-selectively adding their activations-enables direct intervention in year-conditioned factual recall.
Through this targeted manipulation, our experiments demonstrate that the temporal heads serve as key subcomponents for encoding and modifying time-sensitive knowledge.
