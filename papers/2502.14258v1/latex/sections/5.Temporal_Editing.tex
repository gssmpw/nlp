\section{Temporal Knowledge Editing}
\label{subsubsec:temp-edit}
Lastly, we explore an approach to confirm that injecting or amplifying \emph{temporal head}'s attention value can effectively \emph{“edit”} year-specific knowledge as in Figure~\ref{fig:editing}. 
Given a \texttt{source\_prompt} (where the model is confident about a certain year’s fact) and a \texttt{target\_prompt} (where it confuses the same year) based on log probability results, we:
\begin{enumerate}
    \item \textbf{Extract} the value of attention head \(\mathbf{a}_{\mathrm{src}}\) from the \texttt{source\_prompt} at a chosen layer/head (e.g.\ \verb|a18.h3|).
    \item \textbf{Average} over total source prompts (e.g., "In 2009, the name of president of South Korea was").
    \item \textbf{Inject} the modified attention value into the \texttt{target\_prompt} at the corresponding temporal token position, scaled by a coefficient \(\lambda\):
\end{enumerate}
Details of adding an attention is in Appendix~\ref{appendix:temp-edit}.

\begin{figure}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=\columnwidth]{latex/fig/editing3.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Example Of Temporal Knowledge Editing.
From the source prompt, we catch the specific attention value of model's head, for example, \textbf{a18.h3}.
By simply adding it to target prompt, the model's output is changed into temporally correct answer from temporally wrong answer.
The headmap below denotes the number of success in editing for every combination of layers and heads.
The most successful case in here is temporal heads \textbf{a18.h3} as highlighted, following other heads such as backup temporal heads \textbf{a20.h17}.
}
\label{fig:editing}
\vspace{-10pt}
\end{figure}

This modification is applied dynamically using a forward hook mechanism at inference time, preserving the overall model parameters while selectively influencing time-conditioned factual recall.  
We test it with model wrong answer in a normal condition, varying the injection coefficient across three cases (\(\lambda = 1, 3, 6\)), following~\citealp{actadd, actaddllama2}, which emphasized its impact.

Remarkably, the model’s completions shift from a temporally incorrect response (\textit{“changed to Vladimir Putin”}) to the correct one (\textit{“Dmitry Medvedev”}), aligning with the known presidency timeline.
The heatmap in Figure~\ref{fig:editing} further supports this by visually representing the effectiveness of temporal knowledge editing across all layers and heads.  
While certain attention heads can influence the model’s response, the \textbf{most successful cases} are consistently linked to temporal heads, with \verb|a18.h3| exhibiting the highest success rate.  
Additionally, backup temporal heads, such as \verb|a20.h17|, also rank among the top-performing heads, reinforcing their critical role in preserving and modifying time-conditioned knowledge.  
This highlights that temporal factual recall is not arbitrarily distributed but is instead concentrated in specialized subcomponents.
Other results are in Figure~\ref{fig:editing_app}.

This targeted intervention remains minimally invasive, as it does not require global fine-tuning but instead modulates the value of a single specialized head, thereby preserving most of the model’s prior knowledge.  
Taken together, these findings reinforce the hypothesis that LLMs harbor a \emph{temporal subcomponent} within specialized attention heads.  
By intercepting or amplifying these temporal heads, we can selectively alter time-conditioned responses, strengthening the claim that these heads are integral to the reinforcement of year-based factual knowledge.
