\section{Appendix}
\subsection{Effective Attribution Pruning-Integrated Gradients (EAP-IG)}
\label{sec:EAP-IG}
We perform \textbf{Effective Attribution Pruning} (EAP) by ablating (zeroing) candidate edges and measuring the drop in correct predictions following~\citealp{eapig}. 
In tandem, we use \textbf{Integrated Gradients} (IG) to capture gradient-based contributions:
\begin{equation}
\mathrm{IG}(\mathbf{z},\mathbf{z}') = \int_{0}^{1} 
\frac{\partial}{\partial \mathbf{z}} \mathcal{L}(\mathbf{z}' + \alpha(\mathbf{z}-\mathbf{z}'))
\,\mathrm{d}\alpha,
\end{equation}
where $\mathcal{L}$ is the loss (e.g., negative log-likelihood), and $\mathbf{z}'$ a baseline embedding or activation. 
Furthermore, not just combining signals to rank each node/edge by its importance, we extend EAP-IG to \emph{time-sensitive} knowledge.
We construct \textbf{temporal knowledge circuits} by analyzing variations across different years $T_k$. 
For a given $(s,r)$ pair:
\begin{itemize}
    \item \textbf{Clean input}: $(s,r,o_t)$ where $o_t$ is correct at $T_t$.
    \item \textbf{Corrupted inputs}: $(s,r,o_{t'})$ where $o_{t'}$ is the correct object for a different time $T_{t'} \neq T_t$.
\end{itemize}
Rather than treating $o_{t'}$ as incorrect, we leverage the contrast between different valid temporal associations to isolate time-dependent components. An edge $e_i$ is retained in the temporal circuit if:
\begin{equation}
\begin{split}
S(e_i, T_k) = &\; \log p_G(o_k \mid s,r,T_k) \\
& - \log p_{G/e_i}(o_k \mid s,r,T_k) > \tau.
\end{split}
\end{equation}
This identifies edges that encode temporal specificity rather than general factual associations.
By ablating edges across different $T_k$, we verify if disruptions occur primarily at the corresponding time while preserving outputs for other years. This ensures the extracted circuits genuinely reflect temporal dependencies.

\subsubsection{Implementation Details in EAP-IG}
\label{sec:detail_in_eapig}
In each model’s configuration, we set \texttt{split\_qkv\_input} to true in transformer lens~\citep{transformerlens}, ensuring attention heads are disentangled enough for targeted pruning.
The \texttt{ig\_steps} for integrated gradients, we set it as 100.
We use \texttt{top\_n} 5000 settings and the $\tau$ for simplified threshold, we use 0.1 as a predefined value for every models to cutting out unimportant edges and nodes.
The experiments are all done with one NVIDIA A100 GPUs(80GB), less than 30 minutes per each runs.

\subsection{Causal Tracing}
\label{sec:causal_tracing_theme}
Causal Tracing~\citep{causal, rome} aims to reveal which hidden states in an autoregressive Transformer \emph{cause} correct recall of a fact. 
Let a fact be $(s,r,o)$ (e.g., $(\texttt{L. Messi}, \texttt{sports\_team}, \texttt{Newell's Old Boys})$), and time $T$ (e.g., $\texttt{In 1999}$). 
We construct a prompt $p$ (e.g., \textit{“In 1999, Lionel Messi was a member of sports team …”}) and measure the model’s probability of generating $o$ at output:
\begin{equation}
p_{\mathrm{clean}}(o) = G(p),
\end{equation}
where $G$ is the Transformer. Next, we create a \emph{corrupted} prompt $p^\prime$ (e.g., replacing “Lionel Messi” with a fake name). Denote the model’s probability,
\begin{equation}
p_{\mathrm{corr}}(o) = G(p^\prime).
\end{equation}
Because key information is obfuscated, $p_{\mathrm{corr}}(o)$ typically drops. 
Finally, in the \emph{corrupted-with-restoration} run, we overwrite certain hidden states in the corrupted run with their clean-run counterparts:
\begin{equation}
p_{\mathrm{restored}}(o) = G_{\mathrm{restore}}\Bigl(p^\prime,\{\mathbf{h}^{(l)}_{\mathrm{clean}}\}\Bigr),
\end{equation}
where $\mathbf{h}^{(l)}_{\mathrm{clean}}$ are layer-$l$ hidden states from the clean run. 
If restoring layer $l$ significantly boosts $p_{\mathrm{restored}}(o)$, those states at layer $l$ are \emph{causally important} for retrieving the fact. 
Applying this procedure to time-conditioned facts (e.g., specifying “In 1999,” “In 2009,” etc.) localizes \emph{temporal} knowledge within specific tokens and layers.
\input{latex/sections/10.Causal_Tracing}

\begin{table}[t]
% \vspace{-10pt}
\centering
\vspace{-5pt}
{\resizebox{\columnwidth}{!}{
\begin{tabular}{llllll}
\toprule
\multicolumn{2}{l}{\textbf{Category}} & \textbf{Knowledge} & \textbf{\#Node} & \textbf{\#Edge} & \textbf{CRS} \\ 
\midrule
\multicolumn{6}{l}{\textbf{\textit{Temporal}}} \\ 
\midrule
Sports     &            & Nicolas Anelka     & 27  & 26  & 88.81 \\ 
    &            & David Beckham       & 42  & 59  & 26.50 \\ 
Presidents &            & Argentina          & 38  & 64 & 43.99 \\ 
 &            & South Korea        & 51  & 104 & 53.18 \\ 
CEO        &            & Hewlett-Packard    & 31  & 34 & 40.36 \\ 
       &            & Chrysler           & 26  & 22  & 28.14 \\ 
Defense    &            & United States      & 8  & 5 & 25.60 \\ 
 &            & China              & 13  & 9 & 25.82 \\ 
\midrule
\multicolumn{3}{l}{\textbf{Avg}} & \textbf{30} & \textbf{40} & \textbf{41.44} \\ 
\midrule
\multicolumn{6}{l}{\textbf{\textit{Time-Invariant}}} \\ 
\midrule
CommonSense             &            & Object Superclass  & 72  & 127  & 42.61 \\ 
Conditional CS &            & Fruit Inside Color & 43  & 49 & 64.83 \\ 
Num in Obj     &            & Geometric Shape    & 60  & 127 & 62.94 \\ 
Num in Sub     &            & Roman Numerals     & 57  & 108 & 71.18 \\ 
\midrule
\multicolumn{3}{l}{\textbf{Avg}} & \textbf{58} & \textbf{103} & \textbf{60.39} \\ 
\bottomrule
\end{tabular}}}{}
\caption{Statistics of temporal knowledge circuits for Qwen 1.5, both temporal and time-invariant knowledge.
For temporal knowledge, each type of knowledge is reproduced with three selected years: \textbf{1999, 2004, and 2009}.
The numbers of nodes, edges and CRS is the average of each knowledge's yearly circuits.
We simplified total circuits with $\tau = 0.1$, same as Llama2.
}
\label{table:statistic_crs_qwen}
\vspace{-10pt}
\end{table}

\subsection{Details of Circuit Reproduction Score}
\label{sec:detail_in_crs}
CRS condenses relative performance differences and sign alignment into a single, intuitive 0–100 metric, offering a streamlined assessment of circuit quality.

\subsubsection{Motivation}
Existing approaches such as \textit{logit diff} or \textit{MatchNLL}~\citep{conmy2023towards, KC} evaluate circuits by reporting two separate numbers: the \textbf{baseline performance} of the original model and the \textbf{circuit’s performance}. However, this can obscure direct comparisons, especially when values are of different scales or signs. To address this, we introduce the \textbf{Circuit Reproduction Score (CRS)}, a unified metric that normalizes these comparisons onto a \textbf{0–100 scale}. A score of 0 indicates a circuit that fails to retain meaningful model behavior, while 100 signifies equal or superior performance compared to the original model.

\subsubsection{Definition}
Let $B$ represent the \textbf{baseline performance} of the original model and $P$ the \textbf{circuit’s performance}. CRS is computed as:
\begin{equation}
CRS(B, P) = 100 \times S(B,P) \times D(B,P),
\end{equation}
where:
\begin{itemize}
    \item $S(B,P) \in (0,1]$ is a sign-based adjustment factor.
    \item $D(B,P) = \exp(-\alpha R)$ scales based on deviation $R$.
    \item $\alpha$ controls the sensitivity to deviations.
\end{itemize}

The deviation $R$ is defined as:
\begin{equation}
R = \frac{\text{dist}(B,P)}{|B|},
\end{equation}
where $\text{dist}(B,P)$ measures how far $P$ deviates from $B$.

If the circuit’s performance meets or exceeds the baseline ($B > 0$ and $P \geq B$), CRS is set to:
\begin{equation}
CRS(B, P) = 100.
\end{equation}

\begin{table}[t]
% \vspace{-10pt}
\centering
\vspace{-5pt}
{\resizebox{\columnwidth}{!}{
\begin{tabular}{llllll}
\toprule
\multicolumn{2}{l}{\textbf{Category}} & \textbf{Knowledge} & \textbf{\#Node} & \textbf{\#Edge} & \textbf{CRS} \\ 
\midrule
\multicolumn{6}{l}{\textbf{\textit{Temporal}}} \\ 
\midrule
Sports     &            & Nicolas Anelka     & 5  & 3  & 64.51 \\ 
    &            & David Beckham       & 22  & 22  & 42.24 \\ 
Presidents &            & Argentina          & 53  & 127 & 91.19 \\ 
 &            & South Korea        & 55  & 142 & 81.47 \\ 
CEO        &            & Hewlett-Packard    & 12  & 9 & 35.55 \\ 
       &            & Chrysler           & 9  & 7  & 73.98 \\ 
Defense    &            & United States*      & 3  & 1 & 73.03 \\ 
 &            & China*              & 2  & 1 & 72.85 \\ 
\midrule
\multicolumn{3}{l}{\textbf{Avg}} & \textbf{20} & \textbf{39} & \textbf{66.85} \\ 
\midrule
\multicolumn{6}{l}{\textbf{\textit{Time-Invariant}}} \\ 
\midrule
CommonSense             &            & Object Superclass  & 73  & 135  & 61.49 \\ 
Conditional CS &            & Fruit Inside Color & 24  & 44 & 49.48 \\ 
Num in Obj     &            & Geometric Shape    & 16  & 20 & 39.98 \\ 
Num in Sub     &            & Roman Numerals     & 78  & 153 & 74.04 \\ 
\midrule
\multicolumn{3}{l}{\textbf{Avg}} & \textbf{48} & \textbf{88} & \textbf{56.25} \\ 
\bottomrule
\end{tabular}}}{}
\caption{Statistics of temporal knowledge circuits for Phi 3 mini, both temporal and time-invariant knowledge.
For temporal knowledge, each type of knowledge is reproduced with three selected years: \textbf{1999, 2004, and 2009}.
The numbers of nodes, edges and CRS is the average of each knowledge's yearly circuits.
We simplified total circuits with $\tau = 0.1$, same as Llama2, except knowledge in Defense where at least 30\% lower $\tau$ is needed.
Interestingly, Phi 3 mini shows better CRS of temporal knowledge than time-invariant ones, though their overall simplified nodes and edges are less than same cases of other models.
}
\label{table:statistic_crs_phi}
\vspace{-10pt}
\end{table}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lll}
\toprule
\textbf{Category} & \textbf{Time Range} & \textbf{\# of Cases} \\ 
\midrule
\multicolumn{3}{l}{\textbf{\textit{Temporal Knowledge~\citep{wikidata}}}} \\ 
\midrule
Sports             & 1996-2020  & 81  \\ 
Presidents         & 1999-2009  & 65  \\ 
CEO                & 1999-2009  & 65  \\ 
Defense            & 1999-2009  & 77  \\ 
Movies             & 1999-2009  & 33  \\ 
GDP                & 1999-2009  & 33  \\ 
Inflations         & 1999-2009  & 33  \\ 
\midrule
\multicolumn{3}{l}{\textbf{\textit{Time Invariant Knowledge~\citep{lre}}}} \\ 
\midrule
Object Superclass  & -          & 36  \\ 
Fruit Inside Color & -          & 76  \\ 
Geometric Shape    & -          & 28  \\ 
Roman Numerals     & -          & 31  \\ 
\bottomrule
\end{tabular}
}
\caption{Statistics of dataset used for circuits.
}
\label{table:statistic_dataset1}
\end{table}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{llll}
\toprule
\textbf{Dataset}            & \textbf{Format} & \textbf{Test} & \textbf{Source} \\
\midrule
TriviaQA           & MCQA   & 11,313  & \citealp{triviaqa}         \\
Math ChroKnowledge & MCQA   & 2,585  & \citealp{mathkg, chroknowledge}          \\
\bottomrule
\end{tabular}
}
\caption{Statistics of dataset used general QA.
}
\label{table:statistic_dataset2}
\end{table}

\paragraph{Handling Positive and Negative Baselines}
\begin{itemize}
    \item If $B > 0$ and $P \geq B$, CRS is 100, indicating that the circuit fully retains or improves upon original performance.
    \item If $P < B$, the CRS score is exponentially reduced based on the relative performance gap.
    \item If $B < 0$ (indicating the original model performed poorly), less negative performance is treated as an improvement.
    \item If $B$ and $P$ differ in sign, CRS applies an intermediate weighting (e.g., 0.6–0.8) to avoid misleadingly high scores.
\end{itemize}

\subsubsection{Implementation}
We compute:
\begin{align}
B &= \text{eval\_baseline}(G, \mathcal{D}_{val}, \text{logit\_diff}), \\
P &= \text{eval\_graph}(G, P, \mathcal{D}_{val}, \text{logit\_diff}).
\end{align}
These yield average performance values, which are then converted into:
\begin{equation}
CRS = \text{one\_score}(B, P; \alpha, S) \in [0,100].
\end{equation}

The resulting CRS provides a concise and interpretable measure of circuit faithfulness:
\begin{itemize}
    \item \textbf{Both negative:} The circuit’s score is capped (e.g., at most $100 \times 0.5$).
    \item \textbf{Both positive:} The circuit may reach 100 if it fully retains baseline performance.
    \item \textbf{Mixed sign:} An intermediate factor (e.g., 0.6–0.8) prevents inflated scores if the circuit behaves in an unintended manner.
\end{itemize}

\subsubsection{Hyperparameters}
The CRS computation relies on several hyperparameters that modulate its sensitivity to deviations and its handling of different sign scenarios:
\begin{itemize}
    \item $\alpha$: Sensitivity to deviation, controlling how sharply CRS decreases as the circuit deviates from the baseline. Default: 1.0.
    \item $sf_{\text{bothpos}}$: Sign factor when both baseline and circuit performance are positive ($B > 0$, $P > 0$). Default: 1.0.
    \item $sf_{\text{bothneg}}$: Sign factor when both baseline and circuit performance are negative ($B < 0$, $P < 0$). Default: 0.5.
    \item $sf_{\text{bneg\_cpos}}$: Sign factor when the baseline is negative but the circuit is positive ($B < 0$, $P > 0$). Default: 0.8.
    \item $sf_{\text{bpos\_cneg}}$: Sign factor when the baseline is positive but the circuit is negative ($B > 0$, $P < 0$). Default: 0.6.
    \item $\epsilon$: Small constant for numerical stability, ensuring nonzero denominators and preventing division errors. Default: $10^{-9}$.
\end{itemize}

\subsection{Details and Statistics of Dataset}
\label{sec:dataset_details}
Table~\ref{table:statistic_dataset1} and~\ref{table:statistic_dataset2} present the statistical details of the knowledge datasets used in our evaluation. 
For temporal knowledge, we utilize open-sourced WikiData as referenced. 
These datasets encompass a variety of knowledge categories, each consisting of multiple objects along with their associated time ranges.

\subsubsection{Categorization of Knowledge Datasets}
Each dataset category represents a specific type of structured knowledge:

\paragraph{Temporal Knowledge.} This category contains knowledge that varies over time, requiring temporal awareness for accurate retrieval. 
The definitions for each subcategory are as follows:
\begin{itemize}
    \item \textbf{Sports}: The teams associated with specific athletes over time.
    \item \textbf{Presidents}: The names of country leaders for given years.
    \item \textbf{CEO}: The chief executive officers of major companies in a given year.
    \item \textbf{Defense}: The national defense budget of different countries.
    \item \textbf{Movies}: The highest-grossing films by country for specific years.
    \item \textbf{GDP}: The annual Gross Domestic Product (GDP) of various countries.
    \item \textbf{Inflation}: The inflation rate of different countries for given years.
\end{itemize}

\begin{table*}[]
\centering
{\scalebox{0.9}{%
\begin{tabular}{llllllllll}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Settings}}} & \multicolumn{7}{c}{\textbf{Temporal Knowledge (\%)}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Average}}} \\ \cmidrule(lr){2-8}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Sports}} & \multicolumn{1}{c}{\textbf{Presidents}} & \multicolumn{1}{c}{\textbf{CEO}} & \multicolumn{1}{c}{\textbf{Defense}} & \multicolumn{1}{c}{\textbf{Movies}} & \multicolumn{1}{c}{\textbf{GDP}} & \multicolumn{1}{c}{\textbf{Inflations}} & \multicolumn{1}{c}{} \\ \midrule
\rowcolor[HTML]{BFD9EC} % 연한 하늘색
\multicolumn{9}{c}{\textbf{\textit{Llama-2-7b-chat-hf - a18,h3, a15.h0}}} \\ \midrule
Baseline & 41.9 & 80.7 & 27.5 & 13.5 & 23.1 & 10.4 & 10.8 & 29.7 \\
Ablation & \textcolor{red}{40.0} & 75.6 & \textcolor{red}{21.3} & 13.3 & \textcolor{red}{9.37} & 10.7 & 9.34 & 25.6 \\ \midrule
\rowcolor[HTML]{D4BFE1} % 연한 보라색
\multicolumn{9}{c}{\textbf{\textit{Qwen1.5-7B-Chat - a17.h15}}} \\ \midrule
Baseline & 32.4 & 57.2 & 19.6 & 11.5 & 16.7 & 9.58 & 10.0 & 22.4 \\
Ablation & 32.0 & \textcolor{red}{49.4} & 16.6 & 10.3 & 10.8 & 9.50 & 10.3 & 19.8 \\ \midrule
\rowcolor[HTML]{8EDB8A}
\multicolumn{9}{c}{\textbf{\textit{Phi-3-mini-4k-instruct - a10.h13}}} \\ \midrule
Baseline & 24.4 & 72.1 & 30.8 & 73.7 & 21.4 & 12.2 & 13.5 & 35.4 \\
Ablation & 24.8 & 69.6 & 30.7 & \textcolor{red}{11.5} & 21.6 & \textcolor{red}{11.7} & \textcolor{red}{11.8} & \textcolor{red}{26.0} \\ \bottomrule
\end{tabular}}}{}
\caption{Total results of temporal knowledge across multiple models.
Each scores were measured in probability (\%) with averaging effect of multiple heads ablation results.
The most dropped score for each column is colored red.}
\label{table:total_result1}
\end{table*}

\paragraph{Time-Invariant Knowledge.} Unlike temporal knowledge, this category consists of facts that do not change over time.
The specific subcategories are defined as follows:
\begin{itemize}
    \item \textbf{Object Superclass}: General commonsense knowledge that categorizes objects into broader superclasses.
    \item \textbf{Fruit Inside Color}: Commonsense knowledge conditioned on the phrase ``On the inside,'' focusing on the internal color of fruits.
    \item \textbf{Geometric Shape}: Knowledge where objects are associated with numerical properties, such as shape classifications based on the presence of numbers.
    \item \textbf{Roman Numerals}: Cases where numerical values appear in the subject itself, typically involving Roman numeral representations.
\end{itemize}

\subsubsection{General Question Answering (QA) Datasets}
In addition to the structured knowledge datasets, we also utilize benchmark QA datasets for evaluation. 
The test or validation sets provided by these benchmarks are used in our experiments. 
All evaluations are conducted under the \textbf{Multiple-Choice Question Answering (MCQA)} setting.
Statistics are following Table~\ref{table:statistic_dataset2}.

\subsection{Details of Log Probability Check}
\label{sec:log_evaluation_details}
Our evaluation follows the paradigm outlined in \citealt{logprob}, focusing on log probability variation rather than direct answer accuracy.  
Standard multiple-choice evaluations often overestimate model difficulty by testing answers in isolation rather than in comparative contexts.
Instead, we analyze how ablation affects probability distributions across all candidate objects, providing a more granular view of temporal knowledge representation.
By using per-object probability tracking, we reveal a more precise representation of how temporal information is encoded and manipulated within the model.
\paragraph{Notations}
Let \( M \) be the transformer model under evaluation, and let \( O \) be the set of all candidate objects (e.g., teams, presidents).  
For a given input, the model assigns a probability \( p(o | s,r,T) \) to each object \( o \in O \) with given subject $s$, relation $r$ and time $T$, representing its likelihood of being the correct answer.  
The object assigned the highest probability is labeled \texttt{Target} if it corresponds to the correct temporal fact, or \texttt{Non-Target} otherwise.
\paragraph{Per-Choice Probability Assessment}
Unlike conventional approaches, which focus solely on the final prediction, we track probability variations across all objects.  
This ensures that we capture nuanced knowledge shifts caused by ablation, rather than just observing whether the top-ranked answer changes.

\begin{table*}[]
\centering
{\scalebox{0.8}{%
\begin{tabular}{llllllll}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Settings}}} & \multicolumn{5}{c}{\textbf{Time Invariant Knowledge (\%)}} & \multicolumn{2}{c}{\textbf{General QA (F1 \& \%)}} \\ \cmidrule(lr){2-6} \cmidrule(lr){7-8}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Obj-Super}} & \multicolumn{1}{c}{\textbf{Fruit In-Color}} & \multicolumn{1}{c}{\textbf{Geo-Shape}} & \multicolumn{1}{c}{\textbf{Roman-Num}} & \multicolumn{1}{c}{\textbf{Average}} & \multicolumn{1}{c}{\textbf{TriviaQA}} & \multicolumn{1}{c}{\textbf{Math}} \\ \midrule
\rowcolor[HTML]{BFD9EC} 
\multicolumn{8}{c}{\textbf{\textit{Llama-2-7b-chat-hf - a18,h3, a15.h0}}} \\ \midrule
Baseline & 49.7 & 75.6 & 68.5 & 53.5 & 61.8 & 55.4 & 45.4 \\
Ablation & 50.2 & 75.6 & 68.1 & 53.0 & 61.7 & 54.9 & 45.3 \\ \midrule
\rowcolor[HTML]{D4BFE1} 
\multicolumn{8}{c}{\textbf{\textit{Qwen1.5-7B-Chat - a17.h15}}} \\ \midrule
Baseline & 48.0 & 72.0 & 69.4 & 61.5 & 62.7 & 49.7 & 77.0 \\
Ablation & 47.8 & 72.0 & 69.3 & 61.1 & 62.6 & 49.5 & 77.0 \\ \midrule
\rowcolor[HTML]{8EDB8A}
\multicolumn{8}{c}{\textbf{\textit{Phi-3-mini-4k-instruct - a10.h13}}} \\ \midrule
Baseline & 21.8 & 76.0 & 68.3 & 73.2 & 59.8 & 46.8 & 80.8 \\
Ablation & 23.2 & 76.4 & 69.1 & 73.7 & 60.6 & 46.2 & 81.2 \\ \bottomrule
\end{tabular}}}{}
\caption{Total results of time invariant knowledge and general QA across multiple models.
For TriviaQA, we test the unfiltered, no-context validation set (11.3k).
Each scores were measured in probability (\%) or f1 score with averaging effect of multiple heads ablation results.
Most of cases, the scores remain stable or even goes up such as \emph{Object Superposition}.}
\label{table:total_result2}
\end{table*}

\paragraph{Head Ablation and Probability Recalculation}
To examine the role of temporal attention heads, we zero out selected heads \( \hat{H} \) and measure how the model's probability distribution over \( O \) shifts.  
The recalculated probability after ablation is given by:
\begin{align}
    z_o &= \log p_\text{ablate}(o|s,r,T),
    \\ \hat{p}_o &= \frac{\exp(z_o)}{\sum_{o' \in O} \exp(z_{o'})},
\end{align}
where \( p_{\text{ablate}} \) denotes the log-probability computed by forward pass of model, with ablation of corresponding heads in \( \hat{H} \).
Unlike standard evaluation, this method isolates the impact of specific attention heads on temporal knowledge retention.


\subsection{Total Result Each Datasets}
\label{app:total_qa}
Table~\ref{table:total_result1}--\ref{table:total_result2} indicates total result of time variant, invariant and general QA for all three models.
We additionally deal with the case of Movies (which movie is the most popular in each year for each countries), GDP (how much GDP for each year for each countries) and Inflation (the inflation rate of each countries).
As colored in red, temporal knowledge drops more drastically than time invariant knowledge or general QA.


\subsection{Details of Temporal Knowledge Editing}
\label{appendix:temp-edit}

\subsubsection{Attention Value Extraction and Injection}
We employ a direct attention value addition method to influence the model’s temporal knowledge representation.
Though we inspired by activation addition or patching methods like ~\citealp{actaddllama2, iti, cast, saevector} and especially \citealp{actadd}, which computes an activation difference between positive and negative prompts, our method directly extracts value of attention heads from the \texttt{source\_prompt} and injects them into the \texttt{target\_prompt}.

\paragraph{Extracting Value of Attention Head}
For a given \texttt{source\_prompt}, we extract the value from a specific attention head \((l,h)\) at the token position corresponding to the temporal entity:
\begin{equation}
    \mathbf{a}_{\mathrm{src}} = \text{AttnV}(x_{\text{src}}, l, h),
\end{equation}
where \( x_{\text{src}} \) is the tokenized \texttt{source\_prompt} and \( \text{AttnV}(x, l, h) \) returns the attention value at layer \( l \) and head \( h \).

To obtain a stable representation across multiple \texttt{source\_prompt}s, we compute the mean value:
\begin{equation}
    \mathbf{a}_{\mathrm{src}} = \frac{1}{N} \sum_{i=1}^{N} \text{AttnV}(x_{\text{src}}^{(i)}, l, h),
\end{equation}

\paragraph{Identifying Temporal Token Position}
In the \texttt{target\_prompt}, we locate the last token index of the temporal condition to determine where the \text{AttnV} injection should occur.

\paragraph{Attention Value Injection}
Once the temporal token index \( t_{\text{subj}} \) is found, we inject the extracted \text{AttnV}:
\begin{equation}
    \mathbf{a}_{\mathrm{tgt}} = \text{AttnV}(x_{\text{tgt}}, l, h),
\end{equation}
\begin{equation}
    \mathbf{a}_{\mathrm{tgt}}^{\text{new}} = \mathbf{a}_{\mathrm{tgt}} + \lambda \mathbf{a}_{\mathrm{src}},
\end{equation}
where \( x_{\text{tgt}} \) is the tokenized \texttt{target\_prompt}, \( \lambda \) is the injection coefficient (\(\lambda \in \{1, 3, 6\}\)), and \( \mathbf{a}_{\mathrm{tgt}}^{\text{new}} \) is the modified value.
This modification is applied dynamically using a forward hook:
\begin{equation}
    \text{Hook}(\mathbf{a}) = \mathbf{a} + \lambda \mathbf{a}_{\mathrm{src}}, %, \quad \text{where } t = t_{\text{temp}}
\end{equation}
where $t = t_{\text{temp}}$ and \( x_{\text{temp}} \) is the tokenized temporal condition (e.g., "In 2009").

\subsubsection{Evaluation Metrics}
To assess the impact of attenion value injection, we introduce two evaluation criteria.

\paragraph{First-Token Prediction Shift}
We measure whether the injected value shifts the model’s predicted first token. 
Given the target prompt \( x_{\text{tgt}} \), we compare the probability of the correct response \( w^* \) before and after injection:
\begin{equation}
    P(w^* | x_{\text{tgt}}) < P(w^* | x_{\text{tgt}}^{\text{new}}),
\end{equation}
where \( P(w^* | x_{\text{tgt}}) \) is the original probability of generating the correct token and \( P(w^* | x_{\text{tgt}}^{\text{new}}) \) is the probability after attention value injection.

This probability shift is measured using log-probabilities from the model's output distribution.

\paragraph{Full-Text Response Validation}
To further verify the efficacy of our method, we check whether the model’s full generated response contains the expected factual entity. Specifically, we count the number of experiments where the correct answer appears in the model's output (e.g., "Dmitry Medvedev" for the name of president of Russia in 2009).

\begin{figure*}[t]
\vspace{-20pt}
\begin{center}
    \includegraphics[width=1\textwidth]{latex/fig/causal_tracing.pdf}
    \vspace{-20pt}
    \includegraphics[width=1\textwidth]{latex/fig/causal_tracing_rel.pdf}
    % \vspace{-20pt}
    \includegraphics[width=1\textwidth]{latex/fig/causal_tracing_obj.pdf}
\end{center}%
\vspace{-20pt}%
\caption{Results of Causal Tracing for all position(subject, relation, object), six plots for each cases from the top to middle and bottom. 
The restoring part is set to each temporal conditioning, in two different age: 1999 and 2004. 
(Illustrative) Causal tracing heatmaps showing how restoring different layers (x-axis) after temporal corruption affects $\mathrm{p}(\text{New})$ or $\mathrm{p}(\text{Barcelona})$. 
For the object position, we set a simulated \emph{[Object]} for the place holder.
Each figure's left column represents single-layer restoration; the center and right columns reflect MLP vs.\ attention intervals. 
Restoring subject+year at mid layers yields pronounced differences (dark regions).
On the other hand, restoring relation+year or object+year yields trivial differences as their range is overlap significantly.
}
\label{fig:causal_tracing}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.7\textwidth]{latex/fig/total_circuit.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Temporal knowledge circuit of Llama2.
It is simplified version of total circuit by its importance of each nodes using $\tau = 0.1$ as threshold.
}
\label{fig:total_circuit}
\vspace{10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.8\textwidth]{latex/fig/total_circuit_qwen.pdf}
\end{center}%
\vspace{20pt}
\begin{center}
    \includegraphics[width=0.8\textwidth]{latex/fig/total_circuit_phi.pdf}
\end{center}%
\vspace{10pt}%
\caption{Temporal knowledge circuit of Qwen 1.5 and Phi 3 mini.
Those are simplified version of total circuit according to each nodes and edges' importance of using same $\tau = 0.1$ as threshold.
}
\label{fig:total_circuit2}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-20pt}
\begin{center}
    \includegraphics[width=0.85\textwidth]{latex/fig/full_attention.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Total map of attention with Llama2-7b-chat-hf, for each temporal heads and backup temporal heads.
The left side of border line is the attention map of \textbf{Temporal Heads}, and the other side is the result of \textbf{Backup Temporal Heads}.
}
\label{fig:full_attn}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.85\textwidth, trim=0 80 0 80, clip]{latex/fig/full_attention_qwen.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Total map of attention with Qwen1.5-7B-Chat, for each temporal heads and backup temporal heads.
The left side of border line is the attention map of \textbf{Temporal Heads}, and the other side is the result of \textbf{Backup Temporal Heads}.
}
\label{fig:full_attn_qwen}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.85\textwidth]{latex/fig/full_attention_phi.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Total map of attention with Phi-3-mini-4k-instruct, for each temporal heads and backup temporal heads.
The left side of border line is the attention map of \textbf{Temporal Heads}, and the other side is the result of \textbf{Backup Temporal Heads}.
}
\label{fig:full_attn_phi}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.7\textwidth]{latex/fig/log_prob_app_1.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Total results of Llama2-7b-chat-hf, head ablation inference with log probability.
}
\label{fig:log_prop_app1}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.7\textwidth]{latex/fig/log_prob_app_2.pdf}
\end{center}%
\vspace{-10pt}%
\caption{Total results of Qwen1.5-7B-Chat and Phi-3-mini-4k-instruct, head ablation inference with log probability.
(A) denotes the result of Qwen 1.5 and (B) represents the result of Phi 3 mini.
}
\label{fig:log_prop_app2}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.8\textwidth]{latex/fig/alias_app.pdf}
\end{center}%
\vspace{20pt}
\begin{center}
    \includegraphics[width=0.8\textwidth]{latex/fig/alias_app2.pdf}
\end{center}%
% \vspace{-10pt}%
\caption{Temporal knowledge circuit from textual temporal conditioned prompt.
Here, we change the temporal condition \emph{"In 1999"} into \emph{"In the year the Champions League final was held in Barcelona"}, which model already correctly recalls the answer \textit{Malmö FF}.
The temporal knowledge circuit successfully catches temporal conditioning even with alias based on event based textual conditioning, with correctly showing off temporal knowledge heads and some backup temporal heads.
Figure of downside is the attention maps for each temporal heads and backup temporal heads.
Each of those figures highlight various tokens in conditioning part of prompt.
}
\label{fig:alias_app}
\vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\vspace{-10pt}
\begin{center}
    \includegraphics[width=0.8\textwidth]{latex/fig/editing_app.pdf}
\end{center}%
% \vspace{-10pt}%
\caption{Result Of temporal knowledge editing in Qwen 1.5 7B Chat and Phi 3 mini 4k instruct.
From the source prompt, we catch the attention value of each model's temporal head, \textbf{a17.h15} and \textbf{a10.h13}.
The model's output is changed into temporally correct answer from temporally wrong answer.
The headmap below denotes the number of success in editing for every combination of layers and heads.
Though the most successful case of editing is the temporal head \textbf{a17.h15} in Qwen 1.5 7B Chat, Phi 3 mini 4k instruct shows that adding attention had minimal impact, and temporal heads failed to enable effective editing. 
This suggests that the model, constrained by its small parameter size (3.8B), requires a more sophisticated vector steering mechanism rather than relying on a single attention head value modification.
}
\label{fig:editing_app}
\vspace{-10pt}
\end{figure*}
