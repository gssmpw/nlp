


In the experiment, we benchmark different guidance methods for flow matching models in different tasks including synthetic datasets, generative decision-making, and image inverse problems. These tasks all fall into the category of energy guidance \citep{janner_planning_2022,lu_contrastive_nodate} or posterior sampling \cite{chung_diffusion_2024}. 
With these experiments, we aim to answer the following questions:
    \textbf{(1)} Can the proposed learning-based exact guidance method $g_\phi$ learn the correct guidance VF $g_t$ for general (including non-\diffusionpath) flow matching?
    \textbf{(2)} For the asymptotically exact MC guidance method, can it produce the correct guidance in non-Gaussian flow matching, and is it exact assuming a sufficiently large sample budget?
    \textbf{(3)} On the practical aspect, how do different types of guidance methods (approximate/exact, training-free/training-based) perform on more realistic tasks, and how do we choose the appropriate flow matching guidance in different tasks?

\input{tables/image_main}

\subsection{Synthetic Dataset}
We first compare different guidance methods on 2-dimensional synthetic datasets where the source distributions are different from Gaussian. 
The base flow matching model is trained to learn the flow with source distributions other than Gaussian. 
During inference, different guidance methods are applied to perform guided sampling with different objective functions $J$ for each dataset. All of the source distributions are non-Gaussian, so traditional diffusion guidance should not be applied. In Figure \ref{fig:toy}, we compare the performance of $g^{\text{MC}}$, $g_\phi$, an exact diffusion guidance called contrastive energy guidance (CEG) proposed by \citet{lu_contrastive_nodate}, and approximate guidance $g^{\text{cov-A}}$, $g^{\text{cov-G}}$, and $g^{\text{sim-MC}}$. The original target distributions (w/o $g_t$) and the $J$-weighted distributions (ground truth) are shown in the first and second columns. 
The details of the experiment are provided in Appendix \ref{app:exp}.

It can be seen from Figure \ref{fig:toy} that $g^{\text{MC}}$ and $g_{\phi}$ generated samples that almost exactly match the ground truth distribution and the performance is consistent across different datasets. 
Note that the generated samples maintain the correct data manifold instead of concentrating on some points as gradient-based approximate guidance methods do.
As has been mentioned in Section \ref{sec:method}, CEG is essentially $\nabla_{x_t}\log Z_t$ which is exact under the \diffusionpath~assumption \ref{assumption:uncoupled_affine_gaussian_path}. However, none of the flow matching paths we have here are \diffusionpath s, so exact diffusion guidance performs poorly compared to $g_\phi$ and $g^{\text{MC}}$, showing a largely distorted generated distribution.


Also, we investigated the asymptotic exactness of $g^{\text{MC}}$. To quantitatively see the increasing guidance precision as the sample size increases, we show the Wasserstein-2 distance between the guided generation distribution and the ground truth target distribution, estimated using $1000$ samples. As shown in Figure \ref{fig:asymptotic}, the error decreases as the number of samples for computing $g^{\text{MC}}$ ($N$ in Algorithm \ref{alg:mc_estimation_on_g}) increases from $5$ to $10^4$, and eventually approaches or surpasses the error of the learned generative model for the original distribution, which can be seen as an approximate lower-bound of the guided generation error.


\subsection{Planning}

We also conduct experiments on offline RL tasks where generative models have been used as planners \citep{janner_planning_2022,chen_flow_2024}. The planning process is realized through sampling from $\frac{1}{Z}p(x_1)e^{R(x_1)}$ \citep{levine_reinforcement_2018}, which aligns with the goal of our guidance by setting $J=-R$, and $R$ being the return.
We report experiment results on the Locomotion tasks in the D4RL dataset \citep{d4rl}, with experiment setting details and complete results provided in Appendix \ref{app:exp_rl}.



The average normalized scores across five seeded runs of each guidance method are reported in Table \ref{tab:rl} where score $=100$ corresponds to the scores of the expert. The conclusions for CFM and OT-CFM are consistent: for gradient-based methods, $g^{\text{cov-G}}$ is generally better than $g^{\text{cov-A}}$ with an average increase in score of $8.0$. $g^{\text{sim-MC}}$ has a performance between $g^{\text{cov-A}}$ and $g^{\text{cov-G}}$.
The improved performance of $g^{\text{cov-G}}$ comes at a higher computation cost of differentiation through the VF model, though. 
The MC-based guidance, although being gradient-free, outperforms the second-best method $g^{\text{cov-G}}$ by $3.5$ on average and is the best method in $7$ out of $9$ tasks.
For $g_\phi$, we report the result of the best losses $\ell$, but their performance is still relatively weak, falling behind the best by $10.4$ on average. We attribute this to the unstable joint training of two networks and provide the complete results in Appendix \ref{app:exp_rl}.

To investigate the effectiveness of $g^{\text{MC}}$, we collect an ensemble of plans that are generated under guidance, compute the critic-predicted objective function value (estimated return $R$), and then plot the density distribution of the estimated return $R$. An ideal guidance will result in the $R$ distribution to be $p(R)e^{R}$ where $p(R)$ is the distribution generated without guidance (Appendix \ref{app:exp_rl}). As can be seen from Figure \ref{fig:rl_ablation_J_dist}, the samples generated with $g^{\text{MC}}$ have a density distribution that best matches the ground-truth target distribution. 

\subsection{Image Inverse Problems}

We conduct experiments on the image inverse problems on the CelebA-HQ (256$\times$256) dataset to reflect the guidance performances on higher dimensional generative tasks. We consider three different types of noised inverse problems: box inpainting, super-resolution, and Gaussian deburring, and compute the metrics FID, LPIPS, PSNR, and SSIM on 3000 samples from the test set. The details of the settings and result visualizations are included in Appendix \ref{app:exp_image}. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\linewidth]{image/return_distributions/ablation_generated_return_hopper-medium-expert-v2_scale_0.05_small.pdf}
    \vskip -0.12in
    \caption{$R$ distribution of generated trajectories in Locomotion. $g^{\text{MC}}$ matches the target gray dashed line well.}
    \label{fig:rl_ablation_J_dist}
    \vspace{-16pt}
\end{figure}

The results demonstrate that $\Pi$GDM is generally better on all tasks, being the best in $8$ out of $12$ metrics. $g^{\text{cov-G}}$ has a similar but slightly worse performance than $\Pi$GDM, being the best or the runner-up in all $4$ metrics of the super-resolution task if ranking the results of CFM and OT-CFM separately and $3$ out of $4$ metrics in the deblurring task. $g^{\text{sim-A}}$ that does not involve the Jacobian shows remarkable performance in inpainting and deblurring, with a lower FID score than $\Pi$GDM by $1.5$ on average, and LPIPS, PSNR, and SSIM within $3\%$ relative difference compared to the best or the runner-up method in 5 out of 6 metrics, especially when considering the efficiency of $g^{\text{sim-A}}$ that no backpropagation through the model is needed. $g^{\text{cov-A}}$ is the worst excluding $g^{\text{MC}}$, being the second worst or the worst in 11 out of 12 metrics and ranking CFM and OT-CFM separately. It should be noted that $g^{\text{MC}}$ performs poorly here, as $J$ in the inverse problem is highly complex, thus requiring an infeasibly large sample budget to compute accurate $g^{\text{MC}}$. A more detailed explanation is deferred to Appendix \ref{app:exp_image}. 




