

This section is organized as follows: Section \ref{sec:general_g} proposes the general flow matching guidance framework. In Section \ref{sec:g_mc}, we derive a new guidance method $g^{\text{MC}}$ based on Monte Carlo estimation, which is asymptotically exact. In Section \ref{sec:method_localized_posterior}, we derive a guidance $g_t^{\text{local}}$ proportional to the gradient of the energy function $J$ by approximating $g_t$ with Taylor expansion. Then, we introduce the affine path assumption (Assumption \ref{assumption:affine_path}) to obtain a tractable $g^{\text{cov}}\approx g_t^{\text{local}}$, and show that under the stronger \diffusionpath~assumption (Assumption \ref{assumption:uncoupled_affine_gaussian_path}),~$g^{\text{local}}$ covers classical diffusion guidance methods. Section \ref{sec:method_approximate_simple_posterior} introduces an alternative approximation that $p(z|x_t)$ is a Gaussian. Under this approximation and the affine path assumption (Assumption \ref{assumption:affine_path}), we derive a derivative-free guidance $g^{\text{sim-MC}}$, and a specialized guidance of $g^{\text{sim-A}}$ for inverse problems. Finally, Section \ref{sec:g_learned} provides different training losses for training-based exact guidance $g_\phi$. Figure \ref{fig:fig1} visualizes the above outline and the relations between these guidance methods.

\subsection{General Guidance Vector Fields}\label{sec:general_g}
\begin{figure*}[t]
    \centering    \includegraphics[width=0.9\linewidth]{image/fig1.pdf}
    \vskip -0.1in
    \caption{Overview of guidance methods in the paper. We start with a unified guidance expression and derive different guidance methods, including training-free and training-based methods, and cover many classical diffusion guidances.}
    \label{fig:fig1}
    \vskip -0.1in
\end{figure*}

Conditional generative modeling aims to generate samples according to specific requirements. One of the major methods to achieve this is energy-guided sampling, shown in Appendix \ref{app:posterior_sampling_and_energy_guided_sampling} to be equivalent to posterior sampling. Given an energy function $J: \mathbb{R}^d\to \mathbb{R}$ and a pre-trained generative model for $p(x)$, energy-guided samples follow $x\sim p'(x) = \frac{1}{Z}p(x)e^{-J(x)}$, where $Z=\int p(x)e^{-J(x)}dx $ is the normalizing constant. Samples with lower energy values $J$ are more likely to be generated. Thus, the problem of guidance for flow-matching models becomes:

\textit{How can we alter the original vector field (VF) $v_t$ that generates $p(x)$ such that the new VF $v_t'$\footnotemark can generate samples from the new distribution $p'(x) = \frac{1}{Z}p(x)e^{-J(x)}$?}

\footnotetext{The prime symbol $'$ denotes probability distributions and vector fields corresponding to the \emph{new} distribution $p'(x)$.}

A natural choice that is commonly used in diffusion models \citep{dhariwal_diffusion_2021} is to add a guidance VF $g_t(x_t):[0,1] \times \mathbb{R}^d \to \mathbb{R}^d$
to the original VF $v_t(x_t)$, such that the new VF $v'_t(x_t) = g_t(x_t) + v_t(x_t)$ is a VF formed with the same flow matching hyperparameters but generates the new probability path arriving at the new distribution $p'(x)$. Therefore, 
\vspace{-5pt}
\begin{equation}
    \vspace{-7pt}
    \nonumber g_t(x_t) = \int v'_{t|z}(x_t|z) p'(z|x_t) dz - \int v_{t|z}(x_t|z) p(z|x_t) dz,
\end{equation}
where $p(z|x_t) = \frac{p_t(x_t|z)p(z)}{p(x_t)}$ and $p'(z|x_t) = \frac{p'_t(x_t|z)p'(z)}{p'(x_t)}$.
Recall that new VF $v'_t(x_t)$ has the conditional probability path and conditional VF as that of the original VF, \emph{i.e.}, $v'_t(x_t|z)=v_t(x_t|z)$, $p'_t(x_t|z) = p_t(x_t|z)$.
Then, we have the following theorem (proof in Appendix \ref{app:general_guidance}).
\begin{theorem}\label{theorem:general_guidance}
    Adding the guidance VF $g_t(x_t)$ to the original VF $v_t(x_t)$ will form VF $v'_t(x_t)$ that generates $p_t'(x_t) = \int p_t(x_t|z)p'(z)dz$, as long as $g_t(x_t)$ follows:
    \vspace{-5pt}
    \begin{align}\label{eq:general_guidance}
        &g_t(x_t) = \int  \left({\mathcal{P}}\frac{e^{-J(x_1)}}{Z_t(x_t)} - 1\right) v_{t|z}(x_t|z) p(z|x_t) dz, \\
        \vspace{-5pt}
        \label{eq:general_guidance_z}
        &\text{where }Z_t(x_t) = \int e^{-J(x_1)} p(z|x_t)dz,
    \end{align}
    $\mathcal{P} = \frac{\coupling'(x_0|x_1)}{\coupling(x_0|x_1)}$ is the reverse coupling ratio, where $\coupling'(x_0|x_1)$ is the reverse data coupling for the new VF, i.e., the distribution of $x_0$ given $x_1$ sampled from the target distribution.
\end{theorem}


In this paper, we consider the case where $\mathcal{P}$ is or can be approximated as 1. $\mathcal{P}$ is exactly $1$ when the coupling is independent $\coupling(x_0,x_1)=p(x_0)p(x_1)$, resulting in $\coupling'(x_0|x_1)=\coupling(x_0|x_1) = p(x_0)$. $\mathcal{P}=1$ is also reasonable when couplings have little impact on the marginal source distribution, such as mini-batch optimal transport (OT) conditional flow matching \citep{tong_improving_2024} with a small batch size\footnote{Given the high dimensionality of the sample space, mini-batch OT likely makes $\mathcal{P}$ close to 1. However, if the coupling is an exact mapping like an exact OT plan, the impact of $\mathcal{P}$ is non-negligible, which we leave for future work.}.
Later, we will show this choice covers many existing guidance techniques for flow matching or diffusion models.


Uncoupled affine Gaussian path flow matching, where $\coupling(x_0,x_1)=p(x_0)p(x_1)$, $v_{t|1}(x_t|x_1)=\alpha_t x_t + \beta_t x_1$, and $p(x_0)=\mathcal{N}(x_0;\mu,\Sigma)$, is known to be equivalent to diffusion models, with little difference in the noise schedule \citep{zheng_guided_2023,ma_sit_2024}. In this case, our general guidance for flow matching Eq. \eqref{eq:general_guidance} can be reduced to a commonly used guidance term in diffusion models:
$
    g_t(x_t)\propto \nabla_{x_t} \log Z_t(x_t)\nonumber
$
(proof in Appendix \ref{app:affine_gaussian_guidance_matching}).
Thus, most existing works that only consider \diffusionpath~flow matching essentially apply the same guidance techniques for diffusion models \citep{dhariwal_diffusion_2021,song_loss-guided_2023,chung_diffusion_2024}.

Next, we will explore more challenging scenarios of flow matching guidance, which are substantially different from diffusion guidance: either the coupling is dependent, or the source distribution is non-Gaussian. 

\subsection{Monte Carlo Estimation, $g_t^{\text{MC}}$}\label{sec:g_mc}



To start with, we discuss the Monte Carlo (MC) method to estimate the guidance $g_t(x_t)$ in Eq. \eqref{eq:general_guidance}, which is asymptotically exact while being training-free. The MC estimation of the integrals in Eq. \eqref{eq:general_guidance} and \eqref{eq:general_guidance_z} requires sampling from intractable $p(z|x_t)$, but they can be converted into:
\begin{align}\nonumber
&    \bm{g_t}^{\textbf{MC}}(x_t) \defg \mathbb{E}_{x_1,x_0\sim p(z)} 
\left[
(\frac{e^{-J(x_1)}}{Z_t} - 1) v_{t|z}(x_t|z) \frac{p_t(x_t|z)}{p_t(x_t)}
\right],
\\
&\nonumber    Z_t(x_t) = \mathbb{E}_{x_1,x_0\sim p(z)} 
\left[
e^{-J(x_1)} \frac{p_t(x_t|z)}{p_t(x_t)} 
\right],
\end{align}
where only $p_t(x_t)$ remains intractable. We can use the same MC samples to self-normalize the distribution $p_t(x_t|z)$, using $
    p_t(x_t) = \mathbb{E}_{x_1,x_0\sim p(z)}[p_t(x_t|z)]
$.
In the expressions above, $p_t(x_t|z)$ is usually designed to be a simple, known distribution \citep{tong_improving_2024} and $p(z)$ can be sampled either using the learned generative model or from the training distribution if accessible.
The above method can be understood as executing importance sampling to convert the expectation under the intractable distribution $\mathbb{E}_{z\sim p(z|x_t)}[\cdot]$ to that under a tractable distribution $\mathbb{E}_{z\sim p(z)}[\frac{p(x_t|z)}{p(x_t)}(\cdot)]$.
The pseudocode for computing $g^{\text{MC}}_t(x_t)$ can be found in Algorithm \ref{alg:mc_estimation_on_g}, and
a simplified version of $g^{\text{MC}}$ under the assumption of independent coupling is provided in Appendix \ref{app:independent_mc}.



It should be noted that this method is unbiased and applicable to \textit{any} source distribution.
This enables the guidance for flow matching with different source distributions such as uniform \citep{chen_flow_2024} and others that have been adopted in flow-based models \citep{papamakarios_masked_2018,mathieu_riemannian_2020,stimper_resampling_2022}. This method can also be applied to flow matching with dependent couplings of $x_1,x_0$, \textit{e.g.}, optimal transport couplings \citep{tong_improving_2024} and rectified flow \citep{liu_flow_2022}. 

To our knowledge, $g^{\text{MC}}_t$ is the \emph{first} to provide an asymptotically exact training-free estimation of the guidance VF for flow matching whose source distribution is non-Gaussian. However, due to the high variance of MC given a limited number of samples, $g^{\text{MC}}$ is more suitable for tasks where the energy function $J$ varies gently and the data is relatively low-dimensional.






\subsection{Localized $p(x_1|x)$}\label{sec:method_localized_posterior}


Many practical guidance methods rely on approximations \citep{song_loss-guided_2023,pokle_training-free_2024}, so contrary to the unbiased MC estimation of $g_t$, we investigate approximate (biased) guidance methods in this subsection.
We start from an intuitive assumption that the probability mass of $p(x_1|x_t)$ is centered around its mean. Following this, it is natural to approximate the integrals in Eq.~\eqref{eq:general_guidance} with the Taylor expansion that captures local behaviors around the mean. Thus, Eq. \eqref{eq:general_guidance} can be simplified and becomes tractable. 

First, we approximate the normalizing constant $Z_t(x_t)$ in Eq. \eqref{eq:general_guidance_z} as:
\begin{align}
Z_t(x_t) = \int p(z|x_t) e^{-J(x_1)} dz  
\approx e^{-J(\hat{x}_1)}\label{eq:approx_1order_z_uncoupled_approximation}
\end{align}
where $\hat{x}_1 \coloneqq \mathbb{E}_{x_0,x_1\sim p(z|x_t)}[x_1]$. 
Likewise, Eq. \eqref{eq:general_guidance} can be approximated as $g_t(x_t) \approx g_t^{\text{local}}$, and:
\begin{align}
    &\bm{g_t}^{\textbf{local}}(x_t)\nonumber
\label{eq:local_approx_guidance_initial}    \defg -\mathbb{E}_{z \sim p(z|x_t)}
    \left[
     (x_1 - \hat{x}_1)v_{1|t}(x_t|z)
    \right] \nabla_{\hat{x}_1} J(\hat{x}_1),
\end{align}
where $\hat{x}_1$ is defined as above. The approximation error $\|\delta g\|_2^2$ can be bounded by the covariance of the distribution $p(x_1|x_t)$, \textit{i.e.}, $\|\delta g\|^2_2 = (\lambda_h \sigma_1 d)^2(C_1+C_2)$ (Appendix \ref{app:localized_approximation}), where $\lambda_h$ is the maximum eigenvalue of the Hessian of $e^{-J(x)}$, $\sigma_1$ is the $L_2$ norm of the covariance matrix of the distribution $p(x_1|x_t)$, and $d$ is the dimensionality of the data. Therefore, the error is small when $J$ varies gently, \textit{i.e.}, guidance is mild, or when $x_1$ is almost entirely determined given $x_t$, \textit{e.g.}, at the end of the flow $t\rightarrow 1$.

The gradient in $g_t^{\text{local}}$ is a natural outcome of the Taylor expansion near the mean of $p(x_1|x_t)$. $g^{\text{local}}$ is not only applicable to more general flow matching but also originates differently from diffusion guidance \citep{dhariwal_diffusion_2021} where the gradient naturally emerges from the score function $\nabla_{x_t}\log p(x_t)$. Moreover, the error bound we provide here is more practical compared to those previously proposed for diffusion guidance, \textit{e.g.}, the Jensen gap in diffusion posterior sampling (DPS) \citep{chung_diffusion_2024}, which only bounds the error in $\mathbb{E}[e^{-J(x_1)}]$, but it is $\nabla_{x_t}\log Z_t$ that is the guidance VF, whose error is not bounded. 

In order to obtain $\hat{x}_1$, we need the following assumption:
\begin{assumption}\label{assumption:affine_path}
    \textbf{The affine path assumption.} We assume the conditional probability path to be affine, \textit{i.e.}, $x_t = \alpha_t x_1 + \beta_t x_0 + \sigma_t\varepsilon$, where $\varepsilon$ is a random noise and $\sigma_t$, $\dot\sigma_t$ are both sufficiently small.\footnotemark
\end{assumption}
\footnotetext{This choice is a widely used one \cite{lipman_flow_2023,tong_improving_2024}. With a small random noise $\sigma_t \varepsilon$, $x_t$ under conditional VF flows is almost exactly from $x_0$ to $x_1$. }



Note that this assumption does not prevent the samples from having dependent coupling: $\coupling(x_0|x_1) \neq p(x_0)$.
Under Assumption \ref{assumption:affine_path}, we can use the $x_1$-parameterization \citep{lipman_flow_2024} to express $\hat{x}_1$ with the VF $v_t$ that is learned by the model $v_\theta$ (Appendix \ref{appendix:x1_parameterization}):
\vspace{-5pt}
\begin{equation}\label{eq:local_approx_guidance_affine_path_estimate_x1}
    \hat{x}_1 \approx -\frac{\dot\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} x_t + \frac{\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} v_t.
\end{equation}
With the commonly chosen schedule $\alpha_t = t, \beta_t = 1 - t$ \citep{lipman_flow_2023,tong_improving_2024}, $\hat{x}_1 = x_t + (1 - t) v_t$ coincides with the 1-step generated $x_1$ under the Euler scheme.
Also under this affine path assumption, $g^{\text{local}}_t$ can be expressed with the covariance matrix of $p(x_1|x_t)$, $\Sigma_{1|t}$, and the gradient $\nabla_{\hat{x}_1 J(\hat{x}_1)}$ (proof in Appendix \ref{appendix:affine_path_cov_local_approx_guidance}): 
\begin{equation}\label{eq:g_cov}
    \bm{g_t^{\textbf{cov}}} \defg -\underbrace{\frac{\dot\alpha_t\beta_t - \dot\beta_t\alpha_t}{\beta_t}}_{\text{schedule}} \Sigma_{1|t} \nabla_{\hat{x}_1}J(\hat{x}_1).
    \vspace{-5pt}
\end{equation}
Intuitively, the guidance is the gradient of estimated $J$ preconditioned with the covariance matrix of $p(z|x_t)$. The preconditioning squeezes the guidance vector into the $p(z|x_t)$ manifold. Next, we discuss different ways to obtain this covariance term, resulting in $g^{\text{cov-A}}$, $g^{\text{cov-L}}$, and $g^{\text{cov-G}}$.

The simplest way is to approximate the covariance matrix with a manually set schedule $\lambda'_t I$, resulting in $g_t^{\text{cov-A}}$. This allows us to tune the guidance's schedule, a common practice in diffusion model guidance \citep{song_loss-guided_2023,song_pseudoinverse-guided_2022}.
\begin{equation}\nonumber
    \bm{g_t}^{\textbf{cov-A}} \defg -\lambda^{\text{cov-A}}_t \nabla_{\hat{x}_1}J(\hat{x}_1),
\end{equation}
where the original schedule in $g^{\text{cov}}_t$ is already included in the hyperparameter $\lambda^{\text{cov-A}}_t$. Since $p(x_1|x_t)$ is localized when $t\rightarrow 1$, a general guideline is to set $\lambda^{\text{cov-A}}_t$ decaying.
$g_t^{\text{cov-A}}$ only assumes affine path (Assumption \ref{assumption:affine_path}), and is thus theoretically justified for mini-batch optimal transport conditional flow matching \citep{tong_improving_2024} for which few existing guidance methods have been proposed.
Besides, $g_t^{\text{cov-A}}$ is efficient as its computation involves no extra number of function evaluations (NFE) than unguided sampling. 

Alternatively, we can use Proposition \ref{proposition:matching_anything} to train a model to fit the actual covariance matrix of $p(x_1|x_t)$ and acquire $g_t^{\text{cov-L}}$. Note that this covariance matrix is determined by the original distribution and is agnostic to the energy function $J$. Affine path flow matching learns $v_{t}$ which can be seen as the expectation of $x_1$ under $p(x_1|x_t)$, but to achieve approximate conditional generation, the covariance of the distribution $p(x_1|x_t)$ also needs to be learned.








The important special case where flow matching is a diffusion model with a new schedule is formalized with:
\begin{assumption}
    \label{assumption:uncoupled_affine_gaussian_path}
    \textbf{The \diffusionpath~assumption}.
    In addition to Assumption \ref{assumption:affine_path}, the source distribution $p(x_0)$ is a standard Gaussian and the coupling is independent $\coupling(x_0,x_1) = p(x_0)p(x_1)$.
\end{assumption}
We will demonstrate that under Assumption \ref{assumption:uncoupled_affine_gaussian_path}, $g_t^{\text{cov}}$ simplifies into $g_t^{\text{cov-G}}$ (Eq. (\ref{eq:g_cov_G})), which covers classical guidance methods in diffusion models.
Specifically, under Assumption \ref{assumption:uncoupled_affine_gaussian_path}, we can relate the VF to the score function. Using the second-order Tweedie's formula, we can express the covariance matrix $\Sigma_{1|t}$ in Eq. \eqref{eq:g_cov} in terms of the Hessian of the log-probability, $\nabla_{x_t} \nabla_{x_t} \log p(x_t)$ \citep{rozet_learning_2024,boys_tweedie_2024,ye_tfg_2024}. 
This relationship is affine (linearly related) to the derivative of the VF $v_t(x_t,t)$, since the VF is affine to $\nabla_{x_t}\log p(x_t)$. Furthermore, the Jacobian $\frac{\partial \hat{x}_1}{\partial x_t}$ can be shown affine to (and in fact, proportional to) the covariance matrix, following from the fact that the Jacobian is affine to differentiating through the VF. 
We refer to this relationship as the Jacobian trick (proof in Appendix \ref{appendix:jacobian_trick}):
\begin{proposition} \textbf{The Jacobian trick}. Under Assumption \ref{assumption:uncoupled_affine_gaussian_path}, the inverse covariance matrix of $p(x_1|x_t)$, $\Sigma_{1|t}$, is affine to the Jacobian of the VF $\frac{\partial v_t}{\partial x_t}$, and is proportional to the Jacobian $\frac{\partial \hat{x}_1}{\partial x_t}$:
\vspace{-5pt}
\begin{equation}\nonumber
     \Sigma_{1|t}= \frac{\beta_t^2}{\alpha_t(\dot\alpha_t\beta_t - \dot\beta_t\alpha_t)} (-\dot\beta_t+ \beta_t\frac{\partial v_t}{\partial x_t} )
     = \frac{\beta_t^2}{\alpha_t} \frac{\partial \hat{x}_1}{\partial x_t}.
     \vspace{-10pt}
\end{equation}\label{proposition:jacobian_trick}
\end{proposition}
Inserting back to Eq. \eqref{eq:g_cov}, we have:
\begin{align}\label{eq:g_cov_G}
    \bm{g_t}^{\textbf{cov-G}}\defg&\lambda^{\text{cov-G}}_t\nabla_{x_t}J(\hat{x}_1),
\end{align}
where $\lambda^{\text{cov-G}}_t = -{\beta_t(\dot\alpha_t\beta_t - \dot\beta_t\alpha_t)}/{\alpha_t}$ is the schedule. 
$g_t^{\text{cov-G}}$ covers classical diffusion model guidance including loss-guided diffusion\footnote{We refer to the simplest version of LGD-MC($n=1$) as LGD here. LGD-MC with $n>1$ will be covered in Section \ref{sec:method_approximate_simple_posterior}.} (LGD) \citep{song_loss-guided_2023} and diffusion posterior sampling (DPS) \citep{chung_diffusion_2024}. In diffusion models the guidance can be expressed as $\nabla_{x_t} \log \mathbb{E}_{z\sim p(z|x_t)}[e^{-J(x_1)}]$, and DPS approximates this by neglecting the gap in the Jensen inequality \citep{chung_diffusion_2024} to move the expectation into $e^{J(\cdot)}$, and LGD approximates the expectation with a point estimate. Both methods arrive at $-\nabla_{x_t}J(\hat{x}_1)$, which is covered by our Eq. \eqref{eq:g_cov_G}. 


















\begin{figure*}[htb]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.8\linewidth]{image/toy.png}
    \vspace{-7pt}
    \caption{Results of the synthetic dataset with different source (blue) and target (red) distributions. We visualize the start/end points and the flow trajectories. $g^{\text{MC}}$ and $g_\phi$ yield the best guidance across different settings while diffusion guidance fails.}
    \label{fig:toy}
    \vspace{-7pt}
\end{figure*}


\subsection{Gaussian Approximation of $p(z|x_t)$}\label{sec:method_approximate_simple_posterior}

Instead of approximating guidance by expanding it near its mean, we can alternatively approximate
 $p(z|x_t)$ with a Gaussian distribution $\tilde p(z|x_t)=\mathcal{N}(z;\mu_t(x_t),\Sigma_t(x_t))$, and get $g_t\approx g_t^{\text{sim}}$.
To minimize the KL divergence between the approximate distribution and the original distribution, we need to choose $\mu_t(x_t)$ and $\Sigma_t(x_t)$ to that of the actual distribution $p(z|x_t)$ \citep{rozet_learning_2024,boys_tweedie_2024}. Under Assumption \ref{assumption:affine_path}, $\hat{x}_1$ can be estimated using Eq. \eqref{eq:local_approx_guidance_affine_path_estimate_x1}, $\hat{x}_0$ can be similarly estimated (Appendix \ref{appendix:x1_parameterization}), and $\Sigma_t(x_t)$ can be either set as a hyperparameter as in $g_t^{\text{cov-A}}$ or computed as in $g_t^{\text{cov-G}}$ in \diffusionpath~flow matching.

Now that if $J$ has an unknown expression, we can use Monte Carlo (MC) sampling to estimate $g_t^{\text{sim}}\approx g_t^{\text{sim-MC}}$, since we can sample from $\tilde{p}(z|x_t)$ which is a Gaussian distribution:
\vspace{-5pt}
\begin{equation}
\vspace{-5pt}
    \bm{g_t}^{\textbf{sim-MC}} \defg \sum_i^N \left(\frac{e^{-J(x_1^i)}}{\tilde{Z}_t} - 1\right) v_{t|z}(x_t|z^i),
\end{equation}
where $\tilde{Z}_t\coloneqq \frac{1}{N} \sum_i^N e^{-J(x_1^i)} $ is an estimated normalizing constant. This shares the spirit of LGD-MC \citep{song_loss-guided_2023}, which uses MC to estimate $Z_t$ and then computes diffusion guidance $\nabla_{x_t}\log Z_t$.

In some guided generation tasks like inverse problems where $J$ is known analytically, we can derive analytical expressions of $g_t^{\text{sim}}$. For example, when the measurement involves a known degradation operator $H$ applied to $x$ and then adding Gaussian noise with scale $\sigma_y$ to $Hx$, we have $J(x_1) \propto \frac{1}{2\sigma_y^2}\|y - Hx\|_2^2$. Thus we can derive $g_t^{\text{sim}}$ under the affine path assumption (Assumption \ref{assumption:affine_path}), and propose a practical approximation $g_t^{\text{sim-inv-A}}$ (Appendix \ref{appendix:approximate_simple_posterior_pigdm_like}):
\vspace{-5pt}
\begin{equation}
\vspace{-5pt}
    \bm{g_t}^{\textbf{sim-inv-A}} \defg -\lambda_t \Big{(}\frac{\sigma_y^2}{r_t^2} + H^TH\Big{)}^{-1}
    H^T\left(y - {H} \hat{x}_1\right),
\end{equation}
where $\lambda_t$ and $r_t$ are both hyperparameters. $g_t^{\text{sim-inv-A}}$ extends $\Pi$GDM \cite{song_pseudoinverse-guided_2022} to affine~flow matching but requires a further approximation $\frac{\partial \hat{x}_1}{\partial x_t}\approx I$, which is accurate when $t\rightarrow 1$.


In \diffusionpath~(Assumption \ref{assumption:uncoupled_affine_gaussian_path}), $g_t^{\text{sim}}$ in our framework can exactly cover $\Pi$GDM and OT-ODE \citep{song_pseudoinverse-guided_2022,pokle_training-free_2024}
(Appendix \ref{appendix:approximate_simple_posterior_pigdm_like}). 
Note that our $g_t^{\text{sim}}$ is theoretically justified for dependent couplings, such as optimal transport conditional flow matching (OT-CFM) \citep{tong_improving_2024}.

\input{tables/rl_main}
\subsection{Training-based Guidance $g_\phi$}
\label{sec:g_learned}




Previously, we have discussed training-free guidance methods. In this subsection, we discuss how to train a neural network $g_\phi$ to fit guidance $g_t$.
To construct a tractable training loss, we extend the conditional loss in flow matching to arbitrary conditional variables in the following proposition (proof in Appendix \ref{app:proposition_match_anything}).
\begin{proposition}\label{proposition:matching_anything}
    Any marginal variable $f(x_t,t)\coloneqq \mathbb{E}_{z\sim p_t(z|x_t)}[f_{t|z}(x_t,z,t)],~z=(x_0,x_1)$ has an intractable marginal loss:
    \vspace{-5pt}
    \begin{equation}\nonumber
    \vspace{-5pt}
    \mathcal{L}_t=\mathbb{E}_{x_t\sim p(x_t)}\left[\left\|f_{{\theta}}(x_t,t) - \mathbb{E}_{z\sim p_t(z|x_t)}[f_{t|z}(x_t,z,t)]\right\|_2^2\right],
    \end{equation}
    whose gradient w.r.t. $\theta$ is identical to that of the tractable conditional loss:
    \vspace{-5pt}
    \begin{equation}\nonumber
    \mathcal{L}_{t|z}=\mathbb{E}_{x_t,z\sim p(x_t,z)}\left[\left\|f_{\theta}(x_t,t) - f_{t|z}(x_t,z,t)\right\|_2^2\right].
    \vspace{-5pt}
    \end{equation}
\end{proposition}
\paragraph{Guidance Matching.} Based on proposition \ref{proposition:matching_anything}, We can train $g_\phi$ by first learning a surrogate model $Z_{\phi_Z}(x_t,t) \approx Z_t $, and then train $g_\phi\approx g_t$ with the following guidance matching (GM) losses.
\begin{align}\label{eq:guidance_matching_loss_sum}
    \mathcal{L}_{\phi_Z,\phi} &= 
    \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)} [\ell_{\phi_Z} + \ell_{\phi}^{\text{GM}}],\\
    \nonumber
    \ell_{\phi}^{\text{GM}} &= 
    \left\|g_{\phi}(x_t,t) - \left(\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} - 1\right) v_{t|z}(x_t|z)\right\|_2^2,
    \\
    \label{eq:guidance_matching_loss_g_1}
    \ell_{\phi_Z} &= 
    \left\|Z_{\phi_Z}(x_t,t) - e^{-J(x_1)}\right\|_2^2,
\end{align}
where $sg$ denotes stopping the gradient in automatic differentiation.
We prove in Appendix \ref{app:guidance_matching} that the minimizer of $\mathcal{L}_{\phi_Z,\phi}$ is indeed $Z_{\phi_Z} = Z_t$ and $g_{\phi} = g_t$.


In fact, there are other methods to learn $Z_{\phi_Z}$. In the literature on diffusion models, \citet{lu_contrastive_nodate} proposed to use contrastive learning to obtain $Z_t(x_t)$ that can be extended to include the general flow matching path (Appendix \ref{appendix:other_ways_to_learn_z}). Alternatively, Monte Carlo estimation can be applied to obtain $Z_{\phi_Z}$ (Appendix \ref{appendix:other_ways_to_learn_z}).
As for learning $g_\phi$,
alternative to the loss $\ell_{\phi}^{\text{GM}}$ in Eq. \eqref{eq:guidance_matching_loss_g_1}, 
there are other losses $\ell^{\text{VGM}}$, $\ell^{\text{RGM}}$, and $\ell^{\text{MRGM}}$ which, when substituted into Eq. \eqref{eq:guidance_matching_loss_sum}, will produce the same minimizer. We provide detailed analysis and proof for all these losses in Appendix \ref{app:guidance_matching}.
Notably, $\ell^{\text{MRGM}}$ that we derive is identical to a newly proposed training-based flow matching guidance loss in \citet{anonymous2025energyweighted}. For \diffusionpath~flow matching (Assumption \ref{assumption:uncoupled_affine_gaussian_path}), since the guidance is $g_t\propto\nabla_{x_1}\log Z_t(x_t)$ (Appendix \ref{app:affine_gaussian_guidance_matching}), learning $Z_{\phi_Z}$ in Eq. \eqref{eq:guidance_matching_loss_g_1} is adequate, and \citet{lu_contrastive_nodate} also learns $Z_{\phi_Z}$.





In this subsection, we have proposed guidance matching methods for training-based guidance. We derive different losses for learning $Z_t$ and $g_t$, which all provide unbiased estimations of the gradient and can be utilized without specific assumptions on flow matching. 
These losses open the design space of the training loss for classifier-free guidance of flow matching \citep{ho_classifier-free_2022,zhang_adding_2023}.




