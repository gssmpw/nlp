



\section{Complete Theoretical Derivations and Proofs}

Here, we provide the theoretical analysis that is deferred from the main text, including the following subsections:
Appendix \ref{app:posterior_sampling_and_energy_guided_sampling} includes proof of how energy-guided sampling from $p(x)e^{-J(x)} / Z$ is equivalent to conditional sampling from $p(x|y)$.
Appendix \ref{app:general_guidance} proves Theorem \ref{theorem:general_guidance} by showing $g_t+v_t$ is equal to $v'_t$ which generates the correctly guided probability path.
Appendix \ref{app:affine_gaussian_guidance_matching} Proof of that under uncoupled affine Gaussian paths, our general guidance $g_t$ is equivalent to the diffusion guidance $\nabla_{x_t} \log Z_t$.
Appendix \ref{app:proposition_match_anything} is proof of Proposition \ref{proposition:matching_anything}.
Appendix \ref{appendix:other_ways_to_learn_z} discusses other ways to obtain $Z_t$, including using contrastive learning and Monte Carlo estimation.
In appendix \ref{app:guidance_matching}, we propose three other training losses $\ell^{VGM}$, $\ell^{RGM}$, and $\ell^{MRGM}$ for $g_\phi$ and prove that the losses will produce the correct gradient.
Appendix \ref{app:independent_mc} includes a more detailed explanation of $g^{\text{MC}}$ and a variant of it under uncoupled paths.
Appendix \ref{app:localized_approximation} derives $g^{\text{local}}$ and proves its error bound.
Appendix \ref{appendix:x1_parameterization} shows how to estimate $\hat{x}_1$ using $x_t$ and $v_\theta(x_t,t)$ under the affine path assumption.
Appendix \ref{appendix:affine_path_cov_local_approx_guidance} proves that $g^{\text{local}}$ becomes $g^{\text{cov}}$ under affine paths.
Appendix \ref{appendix:jacobian_trick} includes the proof of the Jacobian trick (Proposition \ref{appendix:jacobian_trick}).
Appendix \ref{appendix:approximate_simple_posterior_pigdm_like} includes a proof of $g_t^{\text{sim-inv}}$ for image inverse problem, how to derive $g^{text{sum-inv-A}}$ and how to recover $\Pi$GDM under the uncoupled affine Gaussian path assumption.


\subsection{Energy Guided Sampling as Posterior Sampling}
\label{app:posterior_sampling_and_energy_guided_sampling}

There exists a $J(x)$ such that sampling from $\frac{1}{Z}p(x)e^{-J(x)}$ is equivalent to sampling from $p(x|y)$.

Simply take $J=-\log \frac{p(y|x)}{p(y)}$. Plug it in to get
\begin{equation}
    \frac{1}{Z}p(x)e^{-J(x)}=\frac{p(x)e^{\log \frac{p(y|x)}{p(y)}}}{\int p(x)e^{\log \frac{p(y|x)}{p(y)}}dx}=p(x|y).
\end{equation}

Similar approaches have been used in probability inference reinforcement learning \citep{levine_reinforcement_2018}, and Diffuser \citep{janner_planning_2022} uses this to convert return-conditioned sampling into energy-guided sampling.


\subsection{General Guidance}
\label{app:general_guidance}
We prove Theorem \ref{theorem:general_guidance} here.
\begin{theorem}
    Adding the guidance $g_t(x_t)$, where
    \begin{align}\label{eq:general_guidance_appendix}
        g_t(x_t) & = \int (\frac{e^{-J(x_1)}}{Z_t} - 1) v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} dz\\
        Z_t & = \int e^{-J(x_1)} p(z|x_t)dz
    \end{align}
    to $v_t(x_t)$ that generates $p_t(x_t) = \int p_t(x_t|z)p(z)dz$, will form a vector field $v'_t(x_t)$ that generates $p_t(x_t) = \int p_t(x_t|z)p'(z)dz$, where 
    $p'(z) = p(x_0|x_1) \frac{1}{Z}p(x_1)e^{-J(x_1)}$ has the same reverse coupling as in $p(z)$.
\end{theorem}

\textbf{Proof.}
We can subtract $v_t(x_t)$ from $v'_t(x_t)$ to construct $g_t(x_t)$:
\begin{align}
    g_t(x_t) & =  v'_t(x_t) - \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} dz.
\end{align}
One possible $v'_t(x_t)$ to generate $p_t(x_t) = \int p_t(x_t|z)p'(z)dz$ is
\begin{equation}
    v'_t(x_t) = \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p'(z)}{p'(x_t)} dz,
\end{equation}
where $p'(z) = p(x_0|x_1) \frac{1}{Z}p(x_1)e^{-J(x_1)}$, which follows from conditional flow matching, \emph{i.e.}, a VF marginalizing a conditional VF will generate the corresponding marginal probability path \citep{lipman_flow_2023,tong_improving_2024}.
Then, we have a possible form of $g_t(x_t)$
\begin{align}
    \nonumber g_t(x_t) & = \int v_{t|z}(x_t|z) (\frac{p_t(x_t|z)p'(z)}{p'_t(x_t)} - \frac{p_t(x_t|z)p(z)}{p_t(x_t)}) dz \\
    \nonumber& = \int v_{t|z}(x_t|z) (\frac{p_t(x_t|z)p(x_0|x_1)\frac{1}{Z}p(x_1)e^{-J(x_1)}}{p'_t(x_t)} - \frac{p_t(x_t|z)p(z)}{p_t(x_t)}) dz \\ 
    \nonumber& = \int v_{t|z}(x_t|z) (\frac{p_t(x_t|z)p(z)\frac{1}{Z}e^{-J(x_1)}}{p'_t(x_t)} - \frac{p_t(x_t|z)p(z)}{p_t(x_t)}) dz \\ 
    & = \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} (\frac{1}{Z}e^{-J(x_1)}\frac{p_t(x_t)}{p'_t(x_t)} - 1) dz, \label{eq:appendix:general_guidance_theorem_intermediate}
\end{align}
where $p_t(x_t) = \int p(x_t,z)dz$ and $p'(x_t) = \int p'(x_t,z)dz$.
Since
\begin{equation}
    \int p(x_t,z)dz = p(x_1) \int p(z|x_1)dz\label{eq:appendix:general_guidance_theorem_marginal_pt}
\end{equation}
and 
\begin{align}
     \nonumber\int p'(x_t,z)dz &= \int p'(x_t|z) p'(z)dz  \\
     \nonumber& = \int p(x_t|z) p(x_0|x_1) \frac{1}{Z} p(x_1) e^{-J(x_1)}dz \\ 
     \nonumber& = \int p(x_t|z) p(z) \frac{1}{Z} e^{-J(x_1)}dz \\ 
     \nonumber& = \int p(x_t,z) \frac{1}{Z} e^{-J(x_1)}dz \\ 
     & = p(x_t) \int p(z|x_t) \frac{1}{Z} e^{-J(x_1)}dz .\label{eq:appendix:general_guidance_theorem_marginal_ptprime}
\end{align}
Plugging Eq. \eqref{eq:appendix:general_guidance_theorem_marginal_pt} and Eq. \eqref{eq:appendix:general_guidance_theorem_marginal_ptprime} into Eq. \eqref{eq:appendix:general_guidance_theorem_intermediate}, we get 
\begin{align}
    \nonumber g_t(x_t) & = \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} (\frac{1}{Z}e^{-J(x_1)}\frac{p_t(x_t)}{p'_t(x_t)} - 1) dz \\ 
    \nonumber& = \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} (\cancel{\frac{1}{Z}}e^{-J(x_1)}\frac{\cancel{p(x_t)} \int p(z|x_t)dz}{\cancel{p(x_t)} \int p(z|x_t) \cancel{\frac{1}{Z}} e^{-J(x_1)}dz} - 1) dz \\ 
    \nonumber& = \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} (e^{-J(x_1)}\frac{1}{\int p(z|x_t) e^{-J(x_1)}dz}\underbrace{\int p(z|x_t)dz}_{=1} - 1) dz \\ 
    & = \int v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} (\frac{e^{-J(x_1)}}{\mathbb{E}_{z\sim p(z|x_t)}[e^{-J(x_1)}]} - 1) dz. 
\end{align}
Finally, denote $Z_t = \mathbb{E}_{z\sim p(z|x_t)}[e^{-J(x_1)}]$ to complete the proof.

\textit{Remark.} The theorem states that $v'_t = g_t + v_t$ not only generates the desired terminal distribution $\frac{1}{Z}p(x_1)e^{-J(x_1)}$ at time $t=1$, but also generates a probability path $p'_t(x_t)$ that is similar to the original one. Specifically, their "noising process" $p(x_t|x_1)$, and the conditional vector fields $v(x_t|x_1)$, and the reverse coupling $p(x_0|x_1)$ are the same. These are all hyperparameters of flow matching, as one can choose an arbitrary conditional vector field satisfying boundary conditions and the conditional vector field uniquely determines the conditional probability path; the reverse coupling, given target (dataset) distribution $p(x_1)$ or $\frac{1}{Z}p(x_1)e^{-J(x_1)}$, composes the data coupling $p(x_0,x_1) = p(z)$ for flow matching training.

It should be noted that the $g_t$ and $v'_t$ we construct here is only one of infinitely many \citep{lipman_flow_2023} possible vector fields to generate $\frac{1}{Z}p(x)e^{-J(x_1)}$ at $t=1$. It remains an interesting question whether there exists better $v'_t$ that, for example, simplifies $g_t$ or improves the vector field by straightening the flow.



\subsection{Uncoupled Affine Gaussian Guidance}
\label{app:affine_gaussian_guidance_matching}

Here we prove that $\nabla_{x_t} \log Z_t(x_t)$ is proportional to the guidance $g_t$ in Eq. \eqref{eq:general_guidance}. 
Note that the term $\nabla_{x_t} \log Z_t(x_t)$ is widely used as guidance in the diffusion model literature \citep{dhariwal_diffusion_2021,ho_classifier-free_2022,song_score-based_2021,song_loss-guided_2023,song_pseudoinverse-guided_2022,janner_planning_2022,ajay_is_2023}. Therefore, we prove that our general flow matching guidance exactly covers the original diffusion guidance under the affine Gaussian path assumption, \emph{i.e.}, when flow matching falls back to the diffusion model. Our proof here also elucidates how the gradient $\nabla_{x_t}$ emerges from the original expression of the general guidance for flow matching in Eq. \eqref{eq:general_guidance} where there is no apparent gradient.

We restate Eq. \eqref{eq:general_guidance} here:
\begin{align}\label{eq:appendix_restate:general_guidance}
    g_t(x_t) & = \int (\frac{e^{-J(x_1)}}{Z_t(x_t)} - 1) v_{t|z}(x_t|z) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} dz \\
    Z_t(x_t) & =  \int e^{-J(x_1)} \frac{p_t(x_t|z)p(z)}{p_t(x_t)} dz
\end{align}

Assuming the flow matching to be of uncoupled affine path, we have
\begin{equation}
    x_t = \sigma_t x_0 + \alpha_t x_1,
\end{equation}
where $\sigma_t$ and $\alpha_t$ are schedulers satisfying boundary conditions $\sigma_0 = \alpha_1 = 1$, $\sigma_1 = \alpha_0 = 0$.
Thus, 
\begin{align}
\nonumber
    v_{t|z}(x_t|z) & = \dot\sigma_t x_0 + \dot\alpha_t x_1 
    \\
    \label{eq:appendix:general_guidance_and_diffusion_guidance_affine_path}
    & =  \underbrace{\frac{\dot\sigma_t}{\sigma_t}}_{a_t} x_t + \underbrace{\frac{1}{\sigma_t} (\dot\alpha_t \sigma_t - \dot\sigma_t \alpha_t)}_{b_t} x_1 
\end{align}
where $\dot f_t \coloneqq \frac{d f}{d t}$ denotes derivative to time $t$, and we define $a_t\coloneqq \frac{\dot\sigma_t}{\sigma_t}$, $b_t\coloneqq \frac{1}{\sigma_t} (\dot\alpha_t \sigma_t - \dot\sigma_t \alpha_t)$.

First, we demonstrate a useful technique for the proof later.
Inserting Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_affine_path} into $g_t(x_t)$ in Eq. \eqref{eq:appendix_restate:general_guidance} and we have
\begin{align}\label{eq:appendix:general_guidance_expanded_v}
    g_t(x_t) = \int (\frac{e^{-J(x_1)}}{Z_t} - 1) (a_t x_t + b_t x_1) \frac{p_t(x_t|z)p(z)}{p_t(x_t)} dz.
\end{align}
Since $Z_t = \mathbb{E}_{z\sim p(z|x_t)}[e^{-J(x_1)}]$, 
\begin{equation}
    \int (\frac{e^{-J(x_1)}}{Z_t} - 1) a_t x_t \frac{p_t(x_t|z)p(z)}{p_t(x_t)} dz = 0.
    \label{eq:appendix:general_guidance_and_diffusion_guidance_integrate_to_zero}
\end{equation}
That is to say, $x_t$ inside the integral of Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_integrate_to_zero} will integrate to zero, and we can freely remove or add it to construct desired terms.

Recall the assumption of uncoupled Gaussian path, \emph{i.e.} $p(x_0,x_1) = p(x_0) p(x_1)$, $p(x_0) = \mathcal{N}(x_0;0,I)$. We can utilize the important fact that the conditional probability path for affine Gaussian path flows satisfies $x_t \sim \mathcal{N}(x_t;\alpha_t x_1, \sigma_t^2 I)$, which allows us to connect the conditional score to $x_1$
\begin{equation}\label{eq:appendix:general_guidance_and_diffusion_guidance_score_and_affine_path}
    \nabla_{x_t} \log p(x_t|x_1) = -\frac{x_t - \alpha_t x_1}{\sigma_t ^2}.
\end{equation}
Using Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_integrate_to_zero} and Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_score_and_affine_path}, Eq. \eqref{eq:appendix:general_guidance_expanded_v} can be further converted to 
\begin{align}\label{eq:appendix:general_guidance_and_diffusion_guidance_convert_conditional_v_to_score}
    g_t(x_t)& = \int (\frac{e^{-J(x_1)}}{Z_t} - 1) (b_t x_1 \underbrace{- \frac{b_t}{\alpha_t} x_t}_{\text{Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_integrate_to_zero}}}) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1 \\ \nonumber
    & = \frac{b_t \sigma_t^2}{\alpha_t} \int (\frac{e^{-J(x_1)}}{Z_t} - 1) \nabla_{x_t} \log p(x_t|x_1) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1 \\\nonumber
    & = \frac{b_t \sigma_t^2}{\alpha_t} \int (\frac{e^{-J(x_1)}}{Z_t} - 1) \left(\underbrace{\nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(x_1 | x_t)}_{\text{Bayes' rule}}\right) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1
    \\
    & = \frac{b_t \sigma_t^2}{\alpha_t} \int (\frac{e^{-J(x_1)}}{Z_t} - 1) \left(\underbrace{\cancel{\nabla_{x_t} \log p(x_t)}}_{\text{Integrates to zero as in Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_integrate_to_zero}}} + \nabla_{x_t} \log p(x_1 | x_t)\right) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1 \\
    & = \frac{b_t \sigma_t^2}{\alpha_t} \int (\frac{e^{-J(x_1)}}{Z_t} - 1)  \nabla_{x_t} \log p(x_1 | x_t) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1. \label{eq:appendix:general_guidance_and_diffusion_guidance_samplified_score}
\end{align}
Notice in Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_samplified_score} that 
\begin{align}\label{eq:appendix:general_guidance_and_diffusion_guidance_canceling_expectation_of_score}
    \nonumber
    &\int \nabla_{x_t} \log p(x_1 | x_t) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1 \\
    \nonumber= & \int \underbrace{p(x_1|x_t)  \nabla_{x_t} \log p(x_1 | x_t)}_{\text{Since } f \nabla  \log f = \nabla f}  dx_1 \\
    \nonumber= & \int \nabla_{x_t} p(x_1 | x_t)  dx_1 \\
    \nonumber= &\, \nabla_{x_t} \int p(x_1 | x_t)  dx_1 \\
    = &\,0.
\end{align}
Therefore 
\begin{align}
    \nonumber g_t(x_t) & = -\frac{b_t \sigma_t^2}{\alpha_t} \int (\frac{e^{-J(x_1)}}{Z_t} \cancel{-1})  \nabla_{x_t} \log p(x_1 | x_t) \frac{p_t(x_t|x_1)p(x_1)}{p_t(x_t)} dx_1 \\
    \nonumber& = \frac{b_t \sigma_t^2}{\alpha_t} \int \frac{e^{-J(x_1)}}{Z_t(x_t)} \underbrace{p_t(x_1|x_t) \nabla_{x_t} \log p(x_1 | x_t)}_{\text{Using again $f \nabla \log f = \nabla f$}}  dx_1 \\
    \nonumber& = \frac{b_t \sigma_t^2}{\alpha_t} \frac{1}{Z_t(x_t)} \int e^{-J(x_1)} \nabla_{x_t} p(x_1 | x_t)  dx_1 \\
    \nonumber& = \frac{b_t \sigma_t^2}{\alpha_t} \frac{1}{Z_t(x_t)}\underbrace{\nabla_{x_t} \int e^{-J(x_1)} p(x_1 | x_t)  dx_1}_{\text{Absorb $e^{-J(x_1)}$ for it is independent of $x_1$, and exchange with integral}} \\
    \nonumber& = \frac{b_t \sigma_t^2}{\alpha_t} \frac{1}{Z_t(x_t)} \nabla_{x_t} \underbrace{Z_t(x_t)}_{\text{$Z_t$'s definition in Eq. \eqref{eq:general_guidance}}} \\
    & = \frac{b_t \sigma_t^2}{a_t} \nabla_{x_t} \log Z_t(x_t).
\end{align}

Another possible way to derive this is to first prove the vector field in affine Gaussian path flow matching to be affine to the marginal score, and we direct interested readers to \citep{zheng_guided_2023}. 

\begin{remark}
    The above derivation opens the possibility of using diffusion guidance into affine Gaussian path flow matching, \emph{i.e.}, by multiplying a scheduler $-\frac{b_t \sigma_t^2}{\alpha_t} = \frac{\sigma_t(\dot\alpha_t\sigma_t - \dot \sigma_t\alpha_t)}{\alpha_t} $ to the diffusion classifier guidance. The most common scheduler for flow matching is $\sigma_t = 1 - t, \alpha_t = t$ \citep{lipman_flow_2023,pokle_training-free_2024,zheng_guided_2023,tong_improving_2024,liu_flow_2022,lipman_flow_2024}. In this case, the guidance scheduler is $\frac{(1-t)}{t}$. It should be noted that this scheduler explodes near $t=0$, thus being unstable. The flow matching schedule $\sigma_t$ and $\alpha_t$ can be chosen as other ways to avoid this instability.
\end{remark} 

\begin{remark}
    Note that this guidance cannot be applied to coupled paths. Central to the proof is that in uncoupled affine Gaussian paths, we can convert the conditional vector field to the conditional score.
    If we could do this in coupled paths, we would require (1) $p_t(x|z)$ is Gaussian $\mathcal{N}(x;\mu_t,\sigma_t I)$, such that $\nabla_{x_t} \log p_t(x_t|z) \propto x_t - \mu_t$.  and (2) $v_{t|z}=\dot\mu_t + \dot \sigma_t (x_t - \mu_t) \propto \mu_t$, such that in Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_convert_conditional_v_to_score} the conditional vector field can be converted to the conditional score. Therefore, the following equation must hold \emph{for any $x_t,x_0,x_1$} 
    \begin{equation}
        \dot\mu_t + \dot \sigma_t (x_t - \mu_t) \propto \mu_t
    \end{equation}
    inside the integral of Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_convert_conditional_v_to_score}
    where $\mu_t = \xi_t x_0 + \eta_t x_1$. This equivalent to that
    \begin{equation}
        (\dot\xi_t - \dot \sigma_t \xi) x_0 + (\dot\eta_t - \dot \sigma_t \eta_t) x_1 +  \dot \sigma_t x_t \propto \xi_t x_0 + \eta_t x_1
    \end{equation}
    must hold \emph{for any $x_t,x_0,x_1$} inside the integral of Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_convert_conditional_v_to_score}. According to Eq. \eqref{eq:appendix:general_guidance_and_diffusion_guidance_integrate_to_zero}, $x_t$ terms will integrate to zero, thus
    \begin{equation}
        \frac{\dot\xi_t - \dot \sigma_t \xi}{\dot\eta_t - \dot \sigma_t \eta_t} = \frac{\xi_t}{\eta_t}
    \end{equation}
    which cannot hold: because of the boundary conditions $\xi_0 = \eta_1=1$ and $\xi_1=\eta_0 = 0$, $\exists t\in(0,1),\frac{d \log \xi_t}{ dt} \neq \frac{d \log \eta_t}{ dt}$. %
    It can be observed that the reason why this guidance does not apply to coupled paths is that $x_0,x_1$, and $x_t$ are all independent variables here, preventing us from canceling two of $x_0$ to avoid matching the schedulers' ratio.
    
    
\end{remark}




\subsection{Proof of Proposition \ref{proposition:matching_anything}}
\label{app:proposition_match_anything}
We prove proposition \ref{proposition:matching_anything} here.
\begin{proposition}
    Any \emph{marginal variable} $f(x_t,t)\coloneqq \mathbb{E}_{z\sim p_t(z|x_t)}[f_{t|z}(x_t,z,t)],~z=(x_0,x_1)$ has an intractable \emph{marginal loss}
    \begin{equation}\label{eq:app_proposition_matching_anything_marginal_loss}
        \mathcal{L}=\mathbb{E}_{x_t\sim p(x_t)}\left[\left\|f_{{\theta}}(x_t,t) - \mathbb{E}_{z\sim p_t(z|x_t)}[f_{t|z}(x_t,z,t)]\right\|_2^2\right],
    \end{equation}
    whose gradient is identical to the tractable \emph{conditional loss}
    \begin{equation}
        \mathcal{L}_{t|z}=\mathbb{E}_{x_t,z\sim p(x_t,z)}\left[\left\|f_{\theta}(x_t,t) - f_{t|z}(x_t,z,t)\right\|_2^2\right].
    \end{equation}
\end{proposition}

\textbf{Proof.} Expand and take gradient w.r.t. Eq. \eqref{eq:app_proposition_matching_anything_marginal_loss} to get
\begin{align}
    \nonumber\nabla_\theta \mathcal{L}_t
    &=\nabla_\theta \mathbb{E}_{x_t\sim p(x_t)}\left[\left\|f_{{\theta}}(x_t,t) - \mathbb{E}_{z\sim p_t(z|x_t)}[f(x_t,z,t)]\right\|_2^2\right] \\
    \nonumber&=\mathbb{E}_{x_t\sim p(x_t)}\left[\nabla_\theta \left\|f_{{\theta}}(x_t,t) - \mathbb{E}_{z\sim p_t(z|x_t)}[f(x_t,z,t)]\right\|_2^2\right] \\
    \nonumber&=\int \nabla_\theta p(x_t) \left\|f_{{\theta}}(x_t,t) - \mathbb{E}_{z\sim p_t(z|x_t)}[f(x_t,z,t)]\right\|_2^2 dx_t \\
    \nonumber&=\int \nabla_\theta p(x_t) \left(\|f_{{\theta}}(x_t,t)\|^2 - 2 \langle f_{{\theta}}(x_t,t),  \int p_t(z|x_t) dz f(x_t,z,t) \rangle\right)  dx_t \\ 
    \nonumber&=\int \nabla_\theta  p_t(z|x_t) p(x_t) \left(\|f_{{\theta}}(x_t,t)\|^2 - 2 \langle f_{{\theta}}(x_t,t),  f(x_t,z,t) \rangle\right)  dx_t dz \\ 
    \nonumber&=\int  p_t(z|x_t) p(x_t) \nabla_\theta  \left(\|f_{{\theta}}(x_t,t)\|^2 - 2 \langle f_{{\theta}}(x_t,t),  f(x_t,z,t) \rangle\right)  dx_t dz \\ 
    \nonumber&=\mathbb{E}_{z,x_t\sim p_t(z|x_t) p(x_t)} \left[ \nabla_\theta \left\|f_{{\theta}}(x_t,t) - f(x_t,z,t) \right\|^2 \right]\\ 
    &=\nabla_\theta \underbrace{\mathbb{E}_{z,x_t\sim p_t(z|x_t) p(x_t)} \left[ \left\|f_{{\theta}}(x_t,t) - f(x_t,z,t) \right\|^2 \right]}_{\mathcal{L}_{1|z}}.
\end{align}
Thus, the gradient of the \emph{marginal loss} $\mathcal{L}_{t}$ is identical to the gradient of the \emph{conditional loss} $\mathcal{L}_{t|z}$.


\subsection{Other Ways to Obtain $Z_{\phi_Z}$}
\label{appendix:other_ways_to_learn_z}

\citet{lu_contrastive_nodate} proposed to use contrastive learning to train $Z_{\phi_Z}$. The proof already applies to any uncoupled path, and we show that $Z_{\phi_Z}$ does not depend on the coupling
\begin{align}
    Z_t =& \mathbb{E}_{x_1\sim p(x_0,x_1|x_t)}[e^{-J(x_1)}]= \int e^{-J(x_1)}p(x_0|x_1,x_t)p(x_1|x_t) dx_0 dx_1 \\
    =& \int e^{-J(x_1)}p(x_1|x_t) dx_1 = \mathbb{E}_{x_1\sim p(x_1|x_t)}[e^{-J(x_1)}].
\end{align}
That is, instead of actually sampling from $p(x_0,x_1|x_t)$, sampling from $p(x_1|x_t)$ will result in the same $Z_t$. In the case of the coupled path, the marginalized distribution is identical to the uncoupled path case. Therefore, the contrastive learning method can be readily applied to train $Z_{\phi_Z}$.

Besides training-based $Z_{\phi_Z}$, we can also use Monte Carlo estimation to obtain $Z_t$. Notice that by using importance sampling, we have
\begin{equation}
     Z_t = \mathbb{E}_{x_1\sim p(x_0,x_1|x_t)}[e^{-J(x_1)}] = \mathbb{E}_{x_1\sim p(x_0,x_1)}\left[\frac{p(x_t|x_0,x_1)}{p(x_t)}e^{-J(x_1)}\right].
\end{equation}
As long as $p(x_t|x_0,x_1)$ is known (which is often the case \citep{lipman_flow_2023,tong_improving_2024}), we can estimate $Z_t$ by sampling $N$ pairs of $x_0^i,x_1^i$ from $p(x_0,x_1)$ and estimate
\begin{equation}
    \tilde{Z}_t = \sum_i^N \left(\frac{p(x_0^i,x_1^i|x_t)}{\sum_j^N p(x_0^j,x_1^j|x_t)}e^{-J(x_1^i)}\right).
\end{equation}
A similar technique is used in Section \ref{sec:g_mc}.


\subsection{Guidance Matching Losses}
\label{app:guidance_matching}
Here, we prove that the loss in guidance matching is correct and show there are three other equivalent training losses $\ell^{\text{VGM}},\ell^{\text{RGM}},\ell^{\text{MRGM}}$.
The expressions of different losses are summarized below, and their proof follows.

\paragraph{VF-added Guidance Matching (VGM) Loss.} By utilizing the learned VF $v_\theta(x_t, t)$ into Eq. \eqref{eq:guidance_matching_loss_g_1}, we have 
\begin{equation}
    \ell_\phi^{\text{VGM}} = \left\|g_{\phi}(x_t,t) + v_\theta(x_t,t) - \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\right\|_2^2.
\end{equation}

\paragraph{Reweight Guidance Matching (RGM) Loss.} $\ell_\phi^{\text{VGM}}$ can be further shown equivalent to 
\begin{equation}\label{eq:guidance_matching_loss_g_3_loss}
    \ell_\phi^{\text{RGM}}=\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} \left\|g_{\phi}(x_t,t) + v_\theta(x_t,t) - v_{t|z}(x_t|z)\right\|_2^2.
\end{equation}
This training loss steers $g_\phi$ to where $e^{-J(x_1)}$ is larger by assigning a large loss to steer $g_t$ towards high $e^{-J(x_1)}$ regions.

\paragraph{Marginalized Reweight Guidance Matching (MRGM) Loss.} The above loss can be re-assigned a weight, which will result in the same optimal $g_{\phi_2}(x_t,t)$ as in Eq. \eqref{eq:guidance_matching_loss_g_1}. Specifically, by changing $Z_{\phi_Z,sg}(x_t,t)$ to its expectation under $p_t(x_t)$, we have the following equivalent loss
\begin{equation}\label{eq:guidance_matching_loss_g_4_loss}
    \ell_\phi^{\text{MRGM}}=\frac{e^{-J(x_1)}}{Z} \|g_{\phi_2}(x_t,t) + v_\theta(x_t,t) - v_{t|z}(x_t|z)\|_2^2,
\end{equation}
where $Z = \mathbb{E}_{x_1\sim p(x_1)}[e^{-J(x_1)}]$.
$\ell_\phi^{\text{MRGM}}$ is identical to a newly proposed fine-tuning loss in \citet{anonymous2025energyweighted}. It can also be derived via importance sampling in Eq. \ref{eq:guidance_matching_loss_sum} and
similar reweighting-based fine-tuning losses have been studied in the literature of diffusion models \citep{fan_dpok_2023}.


\paragraph{(1) Guidance Matching Loss $\ell^{\text{GM}}$}
By using proposition \ref{proposition:matching_anything}, the following conditional loss
\begin{equation}\label{eq:appendix:guidance_matching_loss_g_1}
    \mathcal{L}_{\phi}^{\text{GM}} = \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}\left[\underbrace{\left\|g_{\phi}(x_t,t) - (\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} - 1) v_{t|z}(x_t|z)\right\|_2^2}_{=\ell^{\text{GM}}}\right]
\end{equation}
has a gradient that is equivalent to the marginal loss
\begin{equation}
    \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z)x_t \sim p(x_t|z)}\left[\left\|g_{\phi}(x_t,t) - \underbrace{\mathbb{E}_{z\sim p(z|x_t)}\left[(\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} - 1) v_{t|z}(x_t|z)\right]}_{=g_t(x_t)}\right\|_2^2\right].
\end{equation}
Therefore, using the loss $\mathcal{L}_{\phi_Z}$ we can train $g_\phi$ to matching $g_t$. Recall that $\mathcal{L}$ in Eq. \eqref{eq:guidance_matching_loss_g_1} is identical
to $\mathcal{L}_{\phi_Z}$, and we proved the validity of the guidance matching training.

\paragraph{(2) VF-added Guidance Matching Loss $\ell^{\text{VGM}}$.} By replacing the learned VF $v_\theta(x_t, t)$ into Eq. \eqref{eq:appendix:guidance_matching_loss_g_1}, we show that  
\begin{equation}
\label{eq:appendix:guidance_matching_loss_g_2}
    \mathcal{L}_{\phi}^{\text{VGM}} = \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}\left[\left\|g_{\phi}(x_t,t) + v_\theta(x_t,t) - \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\right\|_2^2\right],
\end{equation}
has a gradient equal to that of $\mathcal{L}_{\phi}$ in Eq. \eqref{eq:appendix:guidance_matching_loss_g_1}.

Expand Eq. \eqref{eq:appendix:guidance_matching_loss_g_1} to get
\begin{align}
    \nonumber\mathcal{L}_{\phi}^{\text{GM}} & = \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}\left[\left\|g_{\phi}(x_t,t) - (\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} - 1) v_{t|z}(x_t|z)\right\|_2^2\right] \\
    \nonumber & = \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}[\|\underbrace{g_{\phi}(x_t,t)\|_2^2}_{\text{dependent on }\phi} + \|\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\|_2^2 + \|v_{t|z}(x_t|z)\|_2^2 \\
    \label{eq:appendix:guidance_matching_loss_g1_expanded}& \underbrace{-2\langle g_{\phi}(x_t,t), \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\rangle}_{\text{dependent on }\phi} - 2\langle\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z), v_{t|z}(x_t|z)\rangle \underbrace{- 2\langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle}_{\text{dependent on }\phi} ]. 
\end{align}
After taking gradient w.r.t. $\phi$, only the terms 
\begin{equation}
    \nabla_{\phi}\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}
    \left[
    \|g_{\phi}(x_t,t)\|_2^2 - 2\langle g_{\phi}(x_t,t), \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\rangle
    - 2\langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle
    \right]
\end{equation}
survive. Therefore, by assuming a perfectly learned $v_\theta(x_t,t)$, \emph{i.e.}, 
\begin{equation}
    v_\theta(x_t,t) = \mathbb{E}_{z\sim p(z|x_t)}\left[ v_{t|z}(x_t|z) \right],
\end{equation}
we have 
\begin{align}
\nonumber&\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}
\left[
    \langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle
\right]  \\
\nonumber= &
\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z|x_t),x_t \sim p(x_t)}
\left[
    \langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle
\right] \\
\nonumber= & \mathbb{E}_{\tilde{z}\sim p(\tilde{z}|x_t)}\left[\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z|x_t),x_t \sim p(x_t)}
\left[
    \langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle
\right]
\right] \\
\nonumber= & \mathbb{E}_{\tilde{z}\sim p(\tilde{z}|x_t)}\left[\mathbb{E}_{t\sim\mathcal{U}(0,1),x_t \sim p(x_t)}
\left[
    \langle \mathbb{E}_{z\sim p(z|x_t)} [v_{t|z}(x_t|z)], g_{\phi}(x_t,t)\rangle
\right]
\right] \\
\nonumber= & \mathbb{E}_{z\sim p(z|x_t)}\left[\mathbb{E}_{t\sim\mathcal{U}(0,1),x_t \sim p(x_t)}
\left[
    \langle \mathbb{E}_{\tilde{z}\sim p(\tilde{z}|x_t)} [v_{t|z}(x_t|z)], g_{\phi}(x_t,t)\rangle
\right]
\right] \\
\label{eq:appendix:guidance_matching_loss_g_2_v_term}
= & \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z|x_t),x_t \sim p(x_t)}
\left[
    \langle v_\theta(x_t,t), g_{\phi}(x_t,t)\rangle
\right], 
\end{align}
so by adding back terms that the gradient is agnostic to, we can see that the new loss  $\mathcal{L}_{\phi}^{(1)}$ in Eq. \eqref{eq:appendix:guidance_matching_loss_g_2} is equivalent to $\mathcal{L}_{\phi}$ in Eq. \eqref{eq:appendix:guidance_matching_loss_g_1}
\begin{align}
    \nonumber\nabla_{\phi}{\mathcal{L}}_{\phi}^{\text{GM}} & = \nabla_{\phi}\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}\left[\left\|g_{\phi}(x_t,t) - (\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} - 1) v_{t|z}(x_t|z)\right\|_2^2\right] \\
    \nonumber& = \nabla_{\phi}\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}[\|{g_{\phi}(x_t,t)\|_2^2} + \|\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\|_2^2 + \underbrace{\|v_{\theta}(x_t,t)\|_2^2}_{\text{Vanishes after $\nabla_\phi$}} \\
    & {-2\langle g_{\phi}(x_t,t), \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\rangle} - 2\langle\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z), v_{t|z}(x_t|z)\rangle \underbrace{- 2\langle v_\theta(x_t,t), g_{\phi}(x_t,t)\rangle}_{\text{Changed $v_{t|z}$ to $v_\theta$ using Eq. \eqref{eq:appendix:guidance_matching_loss_g_2_v_term}}} ]\\
    \nonumber& =\nabla_{\phi} \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}\left[\underbrace{\left\|g_{\phi}(x_t,t) + v_\theta(x_t,t) - \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\right\|_2^2}_{\coloneqq\ell^{\text{VGM}}}\right]\\
    & = \nabla_{\phi} {\mathcal{L}}_{\phi}^{\text{VGM}}.
\end{align}


\paragraph{(3) Reweighted Guidance Matching Loss $\ell^{\text{RGM}}$.} Eq. Replacing $\ell^{\text{GM}}$ in \eqref{eq:appendix:guidance_matching_loss_g_1} with $\ell^{\text{VGM}}$: 
\begin{equation}\label{eq:appendix:guidance_matching_loss_g_3_loss}
    \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} \|g_{\phi}(x_t,t) + v_\theta(x_t,t) - v_{t|z}(x_t|z)\|_2^2,
\end{equation}
and the loss $\mathcal{L}^{\text{GM}}_\phi$ becomes $\mathcal{L}^{\text{VGM}}_\phi$, which are shown equivalent in the following.

Starting from Eq. \eqref{eq:appendix:guidance_matching_loss_g1_expanded}, we can extract $\frac{e^{-J(x_1)}}{Z_{\phi_Z}}$ from the three terms depended on $\phi$, and thus showing the resulting loss is indeed $\ell^{\text{RGM}}$.
Notice that because $Z_{\phi_Z} = \mathbb{E}_{z\sim p(z|x_t)}[e^{-J(z)}]$,
\begin{equation}
    \mathbb{E}_{z\sim p(z|x_t)}[\frac{e^{-J(z)}}{Z_{\phi_Z}(x_t)}f(x_t,t)] = f(x_t,t).
\end{equation}
Thus, we have:
\begin{align}
    \nonumber \mathcal{L}^{\text{GM}}_\phi& = \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}[\|\underbrace{g_{\phi}(x_t,t)\|_2^2}_{\text{dependent on }\phi} + \|\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\|_2^2 + \|v_{t|z}(x_t|z)\|_2^2 \\
    \nonumber& \underbrace{-2\langle g_{\phi}(x_t,t), \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\rangle}_{\text{dependent on }\phi} - 2\langle\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z), v_{t|z}(x_t|z)\rangle \underbrace{- 2\langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle}_{\text{dependent on }\phi} ] \\
   \nonumber & =\mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)}[\|\frac{e^{-J(x_1)}}{Z_{\phi_Z}(x_t)}{g_{\phi}(x_t,t)\|_2^2} + \|\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\|_2^2 + \|v_{t|z}(x_t|z)\|_2^2 \\
    \nonumber& {-2\langle g_{\phi}(x_t,t), \frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z)\rangle} - 2\langle\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)}v_{t|z}(x_t|z), v_{t|z}(x_t|z)\rangle - 2\frac{e^{-J(x_1)}}{Z_{\phi_Z}(x_t)}\langle v_{t|z}(x_t|z), g_{\phi}(x_t,t)\rangle ] \\
    & = \mathbb{E}_{t\sim\mathcal{U}(0,1),z\sim p(z),x_t \sim p(x_t|z)} \left[\underbrace{\frac{e^{-J(x_1)}}{Z_{\phi_Z,sg}(x_t,t)} \|g_{\phi}(x_t,t) + v_\theta(x_t,t) - v_{t|z}(x_t|z)\|_2^2}_{=\ell^{\text{RGM}}}\right]\\
    &=\mathcal{L}_\phi^{\text{RGM}}.
\end{align}
Where we used the conclusion of Eq. \eqref{eq:appendix:guidance_matching_loss_g_2_v_term}, and inserted terms that vanish after $\nabla_{\phi}$ to make $\ell^{\text{RGM}}$.

\paragraph{(3) Marginalized Reweighted Guidance Matching Loss $\ell^{\text{MRGM}}$.} The above loss Eq. \eqref{eq:appendix:guidance_matching_loss_g_3_loss} can be re-assigned a weight, which will result in the same optimal $g_{\phi}(x_t,t)$ as in Eq. \eqref{eq:guidance_matching_loss_g_1}. Specifically, by changing $\frac{1}{Z_{\phi_Z,sg}(x_t,t)}$ to $\frac{1}{\mathbb{E}_{x_t\sim p(x_t)}[Z_{\phi_Z,sg}(x_t,t)]}=\frac{1}{\int p(x_t)Z_{\phi_Z,sg}(x_t,t) dx_t}$, we have 
\begin{equation}\label{eq:appendix:guidance_matching_loss_g_4_loss_appendix}
    \frac{e^{-J(x_1)}}{\int e^{-J(x_1)} p(z)dz} \|g_{\phi}(x_t,t) + v_\theta(x_t,t) - v_{t|z}(x_t|z)\|_2^2.
\end{equation}
We only need to prove that
\begin{equation}
    \int p(x_t)Z_{t}(x_t) dx_t = \int e^{-J(x_1)} p(z)dz.
\end{equation}
Recall  that 
\begin{equation}
    Z_{t}(x_t) = \int p(z|x_t) e^{-J(x_1)} dz,
\end{equation}
so 
\begin{align}
    &\int p(x_t)Z_{t}(x_t) dx_t
    = \int  p(x_t)\int p(z|x_t) e^{-J(x_1)} dx_t dz \\
    =& \int p(z)e^{-J(x_1)}dz = Z.
\end{align}

Eq. \eqref{eq:guidance_matching_loss_g_4_loss} can also be derived by applying importance sampling $\mathbb{E}_{z\sim \frac{1}{Z}p(z)e^{-J(x_1)}}\left[\|\cdot\|_2^2 \right]=\mathbb{E}_{z\sim p(z)}\left[ \frac{e^{-J(x_1)}}{Z} \|\cdot\|_2^2  \right]$ to the flow matching objective of the new VF for the new target distribution $p'(x_t) = \frac{1}{Z} p(x_t) e^{-J(x_t)}$.


\paragraph{Discussions}
The losses have the same expected gradient, but their performance may differ. Among the four losses, $\ell_\phi^{\text{MRGM}}$ is the only one that does not require the auxiliary model $Z_{\phi_Z}$. However, $\ell_\phi^{\text{RGM}}$ assigns loss weight dependent on $x_t$. The weight is emphasized when the expectation of $e^{-J(x_1)}$ under $p(x_1|x_t)$ is small. Compared to these two losses, $\ell_\phi^{\text{GM}},\ell_\phi^{\text{VGM}}$ do not reweight the loss. The variance of $\ell_\phi^{\text{GM}}$ will be smaller if $J$ is smooth, while $\ell_\phi^{\text{VGM}}$ is better when $v_t$ is more complex.

\subsection{Algorithm Details and Variants of $g^{\text{MC}}$}\label{app:independent_mc}
The pseudocode for computing $g^{\text{MC}}$ is as follows.
\begin{algorithm}[h]
\caption{Monte Carlo estimation of the guidance $g_t(x_t)$}
\label{alg:mc_estimation_on_g}
\begin{algorithmic}[1]
\REQUIRE Current $t$, $x_t$, known $p_t(x_t|z)$.

\STATE Sample $z_i \sim p(z)$, where $i=1,2,...,N$ 
{\color{gray} // Recall $z_i = (x_{1}^i,x_{0}^i)$}
\STATE $\tilde{p}_t(x_t) \gets \frac{1}{N} \sum_i p_t(x_t|z_i)$
\STATE $\tilde{Z}_t(x_t) \gets \frac{1}{N} \sum_i e^{-J(x_{1}^i)} \frac{p_t(x_t|z_i)}{\tilde{p}(x_t)}$
\STATE ${g}^{\text{MC}}_t(x_t) \gets \frac{1}{N} \sum_i (\frac{e^{-J(x_{1}^i)}}{\tilde{Z}_t(x_t)} - 1) v_{t|z}(x_t|z_i) \frac{p_t(x_t|z_i)}{\tilde{p}_t(x_t)}$
\STATE \textbf{return} ${g}^{\text{MC}}_t(x_t)$
\end{algorithmic}
\end{algorithm}

\paragraph{Independent Couplings.}
Although we introduced the flow matching using the condition $z=(x_0,x_1)$, it can also be chosen as $x_0$ or $x_1$ \citep{lipman_flow_2024}. When we $z\coloneqq x_1$, Algorithm \ref{alg:mc_estimation_on_g} can be readily adopted for the $x_0$ condition. This way, the Monte Carlo estimation reduces the integration region dimensionality to half of the original one, thus becoming more efficient. 

In the case where $z=(x_0,x_1)$ and the data coupling is independent $\pi(x_0|x_1) = p(x_0)$, we show here that the MC estimation can be simplified to the $x_1$-conditioned case that is more efficient:
\begin{align}
&    \bm{g_t}^{\textbf{MC-$x_1$}}(x_t) \defg \mathbb{E}_{x_1\sim p(x_1)} 
\left[
(\frac{e^{-J(x_1)}}{Z_t} - 1) v_{t|x_1}(x_t|x_1) \frac{p_t(x_t|x_1)}{p_t(x_t)}
\right],
\\
& Z_t^{\text{MC-$x_1$}}(x_t) = \mathbb{E}_{x_1\sim p(x_1)} 
\left[
e^{-J(x_1)} \frac{p_t(x_t|x_1)}{p_t(x_t)} 
\right].
\end{align}
Obviously, as $Z_t = \int p(x_0,x_1|x_t) e^{-J(x_1)}dx_0dx_1 = \int p(x_0|x_1,x_t)p(x_1|x_t) e^{-J(x_1)}dx_0dx_1 $, integrating out $x_0$ gives $Z_t = Z^{\text{MC-$x_1$}}_t$. Therefore, to prove the above simplification, we only need to prove that:
\begin{equation}
    \mathbb{E}_{x_0,x_1\sim p(x_0,x_1|x_t)}\left[(\frac{e^{-J(x_1)}}{Z_t} - 1)  v(x_t|x_0,x_1)\right] = \mathbb{E}_{x_1\sim p(x_1|x_t)}\left[(\frac{e^{-J(x_1)}}{Z_t} - 1)  v(x_t|x_1)\right].
\end{equation}
The proof is simply integrating out $x_0$:
\begin{align}
    \nonumber&\int p(x_0,x_1|x_t) (\frac{e^{-J(x_1)}}{Z_t} - 1)  v(x_t|x_0,x_1) dx_0 dx_1 \\
    \nonumber=& \int \underbrace{\int p(x_0|x_1,x_t)v(x_t|x_0,x_1)dx_0}_{\coloneqq v(x_t|x_1)} (\frac{e^{-J(x_1)}}{Z_t} - 1) p(x_1|x_t) dx_1 \\
    =& \int v(x_t|x_1) (\frac{e^{-J(x_1)}}{Z_t} - 1) p(x_1|x_t) dx_1.
\end{align}
It should be noted that $v(x_t|x_1)$ is defined to be generally different from $v(x_t|x_0,x_1)$, and to do MC estimation via importance sampling, we need the forward probability path $p(x_t|x_1)$ to have a known density. The variance-reducing variant of $g^{\text{MC}}$ is summarized in Algorithm \ref{alg:mc_uncoupled}.

\begin{algorithm}[H]
\caption{Monte Carlo estimation of the guidance $g_t(x_t)$}
\label{alg:mc_uncoupled}
\begin{algorithmic}[1]
\REQUIRE Current $t$, $x_t$, known $p_t(x_t|x_1)$.

\STATE Sample $x_1^i \sim p(x_1)$, where $i=1,2,...,N$
\STATE $\tilde{p}_t(x_t) \gets \frac{1}{N} \sum_i p_t(x_t|x_1^i)$
\STATE $\tilde{Z}_t(x_t) \gets \frac{1}{N} \sum_i e^{-J(x_{1}^i)} \frac{p_t(x_t|x_1^i)}{\tilde{p}(x_t)}$
\STATE $\tilde{g}_t(x_t) \gets \frac{1}{N} \sum_i (\frac{e^{-J(x_{1}^i)}}{\tilde{Z}_t(x_t)} - 1) v_{t|z}(x_t|x_1^i) \frac{p_t(x_t|x_1^i)}{\tilde{p}_t(x_t)}$
\STATE \textbf{return} $\tilde{g}_t(x_t)$
\end{algorithmic}
\end{algorithm}

\subsection{Localized Approximation}
\label{app:localized_approximation}

To get $g^{\text{local}}$, we presume $p(z|x_t)$ is localized, and we can use a point estimation to approximate $Z_t$:
\begin{align}
Z_t(x_t) = \int p(z|x_t) e^{-J(x_1)} dz  
\approx e^{-J(\hat{x}_1)}
\end{align}
where $\hat{x}_1 \coloneqq \mathbb{E}_{x_0,x_1\sim p(z|x_t)}[x_1]$,
and then expanding $g_t$ to the first order
\begin{align}\nonumber
    g_t(x_t) \approx g_t(x_t)^{\text{cov}} &= \mathbb{E}_{z \sim p(z|x_t)} 
    \left[
    (\frac{e^{-J(x_1)}}{e^{-J(\hat{x}_1)}} - 1)v_{t|z}(x_t|z)
    \right] \\
    \nonumber &\approx \mathbb{E}_{z \sim p(z|x_t)} \left[
    (\frac{e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}} - 1)v_{t|z}(x_t|z)
    \right] \\
    &= -\mathbb{E}_{x_1 \sim p(x_1|x_t)}
    \left[
     (x_1 - \hat{x}_1)v_{t|z}(x_t|z)
    \right] \nabla_{\hat{x}_1} J(\hat{x}_1).
\end{align}




To quantify the approximation error, we have
\begin{align}
    \nonumber \|\delta g\|^2&\coloneqq \|g_t - g^{\text{local}}\|^2_2 \\
    \nonumber&=\bigg{\|}\mathbb{E}_{z \sim p(z|x_t)} 
    \left[
    (\frac{e^{-J(x_1)}}{Z_t(x_t)} - 1)v_{t|z}(x_t|z)
    \right] \\
    \nonumber&- \mathbb{E}_{z \sim p(z|x_t)} 
    \left[
    (\frac{e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}} - 1)v_{t|z}(x_t|z)
    \right] \bigg{\|}_2^2\\
    &=\left\|\mathbb{E}_{z \sim p(z|x_t)} 
    \left[
    (\frac{e^{-J(x_1)}}{Z_t(x_t)}-\frac{e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}})v_{t|z}(x_t|z)
    \right] \right\|_2^2
\end{align}
where 
\begin{equation}
    Z_t(x_t) = \mathbb{E}_{z\sim p(z|x_t)} [e^{-J(x_1)}].
\end{equation}
We start by computing the error bound of approximating $Z_t$ with $e^{-J(\hat{x}_1)}$. Using Taylor expansion and the Taylor Remainder Theorem \footnote{The notations here neglect the order of vector/matrix products, but this does not matter as all of them will be scaled using the triangle inequality.}, 
\begin{align}\nonumber
    \left\|Z_t(x_t)-e^{-J(\hat{x}_1)}\right\|^2_2 &= \left\|\mathbb{E}_{z\sim p(z|x_t)}[\sum_{k=2} \frac{1}{k!}D_{x_1}^k e^{-J(x)}\big{|}_{x_1=\hat{x}_1} (x_1 - \hat{x}_1)^k] \right\|_2^2 \\
    &\le \mathbb{E}_{z\sim p(z|x_t)}
    \left
    [\left\|\frac{1}{2} (x_1 - \hat{x}_1)^T \underbrace{\nabla_{x_1} \nabla_{x_1} e^{-J(x)}\big{|}_{x_1=\hat{x}_1 + t (x_1 - \hat{x}_1)}}_{\coloneqq h^{(J)}_t} (x_1 - \hat{x}_1)
    \right\|_2^2
    \right] ,
\end{align}
where $t\in[0, 1]$.


If we set the L2 norm of the covariance matrix $\mathbb{E}_{z\sim p(z|x_t)}[ (x_1 - \hat{x}_1)(x_1 - \hat{x}_1)^T]$ as $\sigma_1$, and the largest eigenvalue of $\max_{t,x_1}h^{(J)}_t$ to be $\lambda_{h}$, we can show that  
\begin{align}
    \nonumber \left\|Z_t(x_t)-e^{-J(\hat{x}_1)}\right\|^2_2 \le &\mathbb{E}_{z\sim p(z|x_t)}[(x_1 - \hat{x}_1)^T h^{(J)}_t(x_1 - \hat{x}_1)]\\
    \nonumber \le &\mathbb{E}_{z\sim p(z|x_t)}[(x_1 - \hat{x}_1)^T \lambda_h(x_1 - \hat{x}_1)]\\
    \nonumber \le &\lambda_h\mathbb{E}_{z\sim p(z|x_t)}[ (x_1 - \hat{x}_1)^T(x_1 - \hat{x}_1)]\\
    \nonumber \le& \lambda_h \mathbf{tr}\Sigma_{11} \\
    \le&\lambda_h \sigma_1 d,
\end{align}
where $\Sigma_{11}$ is the covariance matrix of $p(x_1|x_t)$, $d$ is the dimensionality of $x \in \mathbb{R}^d$. The last inequality follows from $\mathbf{tr}A = \sum_i^n \lambda_i \le n \max_i\lambda_i $, and the L2 norm of a matrix is its largest singular value, \emph{i.e.}, for the covariance matrix, that is the largest eigenvalue.

Then, 
\begin{align}
    \nonumber \delta g &= \mathbb{E}_{z \sim p(z|x_t)} 
    \left[
    \left(\frac{e^{-J(x_1)}}{Z_t(x_t)}-\frac{e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}}\right)v_{t|z}
    \right] \\
    & = \mathbb{E}_{z \sim p(z|x_t)} \left[ 
    \left(\underbrace{\frac{e^{-J({x}_1)}}{Z_t(x_t)} - \frac{e^{-J(x_1)}}{e^{-J(\hat{x}_1)}}}_{\text{Using the error bound of } Z_t} + \frac{e^{-J(x_1)}}{e^{-J(\hat{x}_1)}} -\frac{e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}}\right)v_{t|z}
    \right].
\end{align}
Therefore,
\begin{align}
    \nonumber \|\delta g\|_2^2& \le \left\|-\frac{\lambda_h d}{Z_t(x_t)e^{-J(\hat{x}_1)}} {\mathbb{E}_{z \sim p(z|x_t)} [e^{-J(x_1)}v_{t|z}]}\sigma_1\right\|_2^2 \\
    \nonumber &+ \left\|\mathbb{E}_{z \sim p(z|x_t)} \left[ 
    \frac{e^{-J(x_1)} - e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}} v_{t|z}
    \right] \right\|_2^2\\
    \nonumber & = \bigg{\|} -\frac{\lambda_h d}{Z_t(x_t)e^{-J(\hat{x}_1)}} {\mathbb{E}_{z \sim p(z|x_t)} [e^{-J(x_1)}v_{t|z}]}\sigma_1\bigg{\|}_2^2 \\
    & + \bigg{\|}\mathbb{E}_{z \sim p(z|x_t)} \left[ 
    \frac{e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1)) + R_2 - e^{-J(\hat{x}_1)}(1 - \nabla_{\hat{x}_1} J(\hat{x}_1) (x_1 - \hat{x}_1))}{e^{-J(\hat{x}_1)}} v_{t|z}
    \right]\bigg{\|}_2^2.
\end{align}
By using the Taylor Remainder Theorem again, we have 
\begin{equation}
R_2 = \frac{1}{2}(x_1 - \hat{x}_1)^T \underbrace{\nabla_{\xi}\nabla_{\xi} e^{-J(\xi)}\big{|}_{\xi=\hat{x}_1 + t (x_1 - \hat{x}_1)}}_{= h^{(J)}_t}(x_1 - \hat{x}_1).
\end{equation}
Thus,
\begin{align}
    \nonumber \|\delta g\|_2^2\le&\left\|  \frac{\mathbb{E}[e^{-J(x_1)}v(x_t|z)]\lambda_h\sigma_1 d}{\mathbb{E}[e^{-J(x_1)}]e^{-J(\hat{x}_1)}} \right\|_2^2 + \big{\|}\mathbb{E}_{z \sim p(z|x_t)}[\frac{1}{2}(x_1 - \hat{x}_1)^T h_t^{(J)}(x_1 - \hat{x}_1) 
    v_{t|z}
    ]\big{\|}_2^2\\
    \nonumber \le&\left\|  \frac{\mathbb{E}[e^{-J(x_1)}v(x_t|z)]\lambda_h\sigma_1 d}{\mathbb{E}[e^{-J(x_1)}]e^{-J(\hat{x}_1)}} \right\|_2^2 \\
    \nonumber +&
        \left[
            \mathbb{E}_{z \sim p(z|x_t)} 
            \left\|
            \frac{1}{2}(x_1 - \hat{x}_1)^T h_t^{(J)}(x_1 - \hat{x}_1)
            \right\|_2^2
        \right]
        \left[
            \mathbb{E}_{z \sim p(z|x_t)} 
            \left\|
            v(x_t|z)
            \right\|_2^2
        \right]\\
    \le & \left\|  \frac{\mathbb{E}[e^{-J(x_1)}v(x_t|z)]\lambda_h\sigma_1 d}{\mathbb{E}[e^{-J(x_1)}]e^{-J(\hat{x}_1)}} \right\|_2^2 + \left\|\frac{\lambda_h \sigma_1 d}{2} \right\|^2_2 \mathbb{E}\left[\|v(x_t|z)\|_2^2\right]
\end{align}

Then we have
\begin{align}
    \nonumber \|\delta g\|^2_2 &\le \left\|  \frac{\mathbb{E}[e^{-J(x_1)}v(x_t|z)]\lambda_h\sigma_1 d}{\mathbb{E}[e^{-J(x_1)}]e^{-J(\mathbb{E}[x_1])}} \right\|_2^2
    + \left\|\frac{\lambda_h \sigma_1 d}{2} \right\|^2_2 \mathbb{E}\left[\|v(x_t|z)\|_2^2\right] \\
    & = (\lambda_h \sigma_1 d)^2
    \left( 
    \underbrace{\left\|  \frac{\mathbb{E}[e^{-J(x_1)}v(x_t|z)]}{\mathbb{E}[e^{-J(x_1)}]e^{-J(\mathbb{E}[{x}_1])}} \right\|_2^2}_{C_1} + \underbrace{\mathbb{E}\left[\|v(x_t|z)\|_2^2\right]}_{C_2}
    \right),
\end{align}
where we omit $z\sim p(z|x_t)$ in $\mathbb{E}_{z\sim p(z|x_t)}$ for simplicity.
Therefore, the approximation error of $g^{\text{local}}$ is bounded by $(\lambda_h \sigma_1 d)^2 (C_1 + C_2)$, where $\lambda_h$ is the largest eigenvalue of $h_t^{(J)}$, the Hessian matrix of the objective function $e^{-J}$, $\sigma_1$ is the L2 norm of the covariance matrix, $d$ is the sample dimensionality, $C_1$ is a constant that has to do with the norm of the new VF, and $C_2$ is the variance of the original \emph{conditional} VF. Some intuitions can be emphasized:
\begin{enumerate}
    \item The error is small as $J$ is smooth, in which case the Hessian of $e^{-J}$ will approach zero. This corresponds to the mild guidance, where approximation-based $g^{\text{local}}$ works well.
    \item The error is small as $\sigma_1$ is small, \emph{i.e.} the covariance matrix $\Sigma_{11}$ has a small Frobenius norm. This is the case when the flow time $t\rightarrow 1$ (and $\sigma_t = 0$), where $x_t$ predicts $x_1$ well.
    \item The cases where $C_1$ and $C_2$ are small are of less practical usage since with a small norm of the VF, the error in the guidance VF will likely cause a larger deviation due to increased relative error.
\end{enumerate}


\subsection{Estimation of $\hat{x}_1$}
\label{appendix:x1_parameterization}
Under the \emph{affine path assumption} (Assumption \ref{assumption:affine_path}), we can estimate the expectation of $x_1$ under the distribution $p(z|x_t)$. This is a well-known trick \citep{lipman_flow_2024,pokle_training-free_2024}, but our analysis includes the dependent coupling case.

Since the flow matching model learns
\begin{equation}
    v_{\theta}(x_t,t)\approx v_t(x_t) = \mathbb{E}_{z\sim p(z|x_t)}\left[
    v(x_t|z)
    \right],
\end{equation}
using the \emph{affine path assumption} ($x_t = \alpha_t x_1 + \beta_t x_0 + \sigma_t\dot\sigma_t\varepsilon$), 
\begin{equation}
    v(x_t|z) = \frac{d}{dt}x_t  = (\dot\alpha x_1 + \dot\beta_t x_0 + \dot\sigma_t\varepsilon),
\end{equation}
so
\begin{equation}\label{eq:appendix:x1_parameterization_v}
    v_t(x_t) = \mathbb{E}_{z\sim p(z|x_t)}\left[
    \dot\alpha x_1 + \dot\beta_t x_0 +\dot \sigma_t\varepsilon
    \right].
\end{equation}

Meanwhile, taking the expectation of $x_t$ under $p(z|x_t)$ yields
\begin{align}\label{eq:appendix:x1_parameterization_x}
    \underbrace{\mathbb{E}_{z\sim p(z|x_t)}[x_t] = x_t}_{\text{because} \int z p(z|x_t) dz = 1} = \mathbb{E}_{z\sim p(z|x_t)}[\alpha_t x_1 + \beta_t x_0 + \sigma_t\varepsilon].
\end{align}

Then, by using Eq. \ref{eq:appendix:x1_parameterization_v} and \ref{eq:appendix:x1_parameterization_x}, we can eliminate either $\hat{x}_0$ or $\hat{x}_1$ in each other's expression:
\begin{align}
    \hat{x}_0 \coloneqq \mathbb{E}_{z\sim p(z|x_t)}[x_0] & = 
    \frac{\dot\alpha_t x_t - \alpha_t v_t(x_t)}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}
    +
    \underbrace{\mathbb{E}_{z\sim p(z|x_t)}\left[
    \frac{-\dot\alpha_t\sigma_t\varepsilon + \alpha_t\dot\sigma_t\varepsilon}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}
    \right]}_{\coloneqq \zeta^0_t}
    \\
    \hat{x}_1 \coloneqq \mathbb{E}_{z\sim p(z|x_t)}[x_1] & = 
    \frac{-\dot\beta_t x_t + \beta_t v_t(x_t)}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t} + \underbrace{\mathbb{E}_{z\sim p(z|x_t)}\left[
    \frac{\dot\beta_t\sigma_t\varepsilon - \beta_t\dot\sigma_t\varepsilon}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}
    \right]}_{\coloneqq \zeta^1_t}.
\end{align}
It should be noted that we have assumed that $\sigma_t$ is small and thus $\zeta_t^0$ and $\zeta_t^1$ are also small in the \emph{affine path assumption} (Assumption \ref{assumption:affine_path}):
\begin{align}
    \nonumber \zeta_t^0 &= \frac{-\dot\alpha_t\sigma_t + \alpha_t\dot\sigma_t}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}\mathbb{E}_{z\sim p(z|x_t)}[\varepsilon] \\
    \nonumber &=\frac{-\dot\alpha_t\sigma_t + \alpha_t\dot\sigma_t}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}
    \int \frac{p(x_t|x_0,x_1)\pi(x_0,x_1)}{p(x_t)}\varepsilon dx_0dx_1 \\
    &=\frac{-\dot\alpha_t\sigma_t + \alpha_t\dot\sigma_t}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}\int\frac{1}{p(x_t)}
    \mathbb{E}_{\varepsilon\sim p_{\varepsilon}(\varepsilon)} \left[
    \pi(x_0(x_t,x_1,\varepsilon),x_1)\varepsilon \right] dx_1.
\end{align}
Since $\pi(x_0(x_t,x_1,\varepsilon),x_1)$ is a probability distribution that is assumed to be bounded, we denote $\max_{\varepsilon}\|\pi(x_0(x_t,x_1,\varepsilon),x_1)\|\le \mathcal{M}(x_1,x_t)$, and thus 
\begin{equation}
    \lim_{\sigma_t\rightarrow 0, \dot\sigma_t\rightarrow 0}\zeta_t^0
    \le\lim_{\sigma_t\rightarrow 0, \dot\sigma_t\rightarrow 0}\left|\frac{-\dot\alpha_t\sigma_t + \alpha_t\dot\sigma_t}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t}\right|\cdot \int 
    \left\|
    \frac{1}{p(x_t)}
    \right\|_2^2
    \cdot
    \left\|
    \mathbb{E}_{\varepsilon\sim p_{\varepsilon}(\varepsilon)}
    \|\varepsilon\|_2
    \mathcal{M}(x_1,x_t)
    \right\|_2^2dx_1 = 0.
\end{equation}
Since everything in the integral is independent of $\varepsilon$, $x_0$, or $\sigma_t$, as $\sigma_t\rightarrow 0$ $\zeta^0$ simply converges to zero. A similar approach can prove that $\lim_{\sigma_t\rightarrow 0, \dot\sigma_t\rightarrow 0}\zeta_2^1$ is also zero.

Next, we explain why we specifically care about the case where the small $\sigma_t$ assumption holds.
In independent coupling flow matching, $\sigma_t\varepsilon$ is exactly zero since we can use two of $x_t,x_0$, and $x_1$ to express the third one. In dependent coupling flow matching, this assumption also holds for famous methods such as optimal transport conditional flow matching or Schrodinger Bridge conditional flow matching \cite{tong_improving_2024}, where $\varepsilon\sim\mathcal{N}(0,I)$ and $\sigma_t$ is set as a small constant. Therefore, the assumption that $\sigma_t\varepsilon$ is small in \emph{affine path assumption} is general and applies to many existing flow matching methods. Hence, by approximating $\zeta_t^0$ and $\zeta_t^1$ as zero, we have the final estimation of $\hat{x}_1$
\begin{align}
    &\hat{x}_0 \approx 
    \frac{\dot\alpha_t x_t - \alpha_t v_\theta(x_t,t)}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t} \\
    &\label{eq:appendix:local_approx_guidance_affine_path_estimate_x1}\hat{x}_1 \approx 
    \frac{-\dot\beta_t x_t + \beta_t v_t(x_t)}{\beta_t\dot\alpha_t - \dot\beta_t\alpha_t},
\end{align}
where Eq. \eqref{eq:appendix:local_approx_guidance_affine_path_estimate_x1} is just Eq. \eqref{eq:local_approx_guidance_affine_path_estimate_x1}. Note the approximations become exact under Assumption \ref{assumption:uncoupled_affine_gaussian_path}.


\subsection{Proof of $g^{\text{cov}}$}
\label{appendix:affine_path_cov_local_approx_guidance}
Here, we prove that under the affine path assumption (Assumption \ref{assumption:affine_path}), Eq. \eqref{eq:g_cov}
\begin{equation}\label{eq:restate_g_cov}
    g_t^{\text{local}}\approx g_t^{\text{cov}} = -\underbrace{\frac{\dot\alpha_t\beta_t - \dot\beta_t\alpha_t}{\beta_t}}_{\text{schedule}} \Sigma_{1|t} \nabla_{\hat{x}_1}J(\hat{x}_1),
\end{equation}
where 
\begin{equation}
    g_t^{\text{local}} = -\mathbb{E}_{z \sim p(z|x_t)}
    \left[
     (x_1 - \hat{x}_1)v_{1|t}(x_t|z)
    \right] \nabla_{\hat{x}_1} J(\hat{x}_1).
\end{equation}

Under the affine path $x_t = \alpha_t x_1 + \beta_t x_0 + \sigma_t \varepsilon$ the conditional vector field $v_{1|t}$ follows
\begin{equation}
    v_{1|t}(x_t) = \dot \alpha_t x_1 + \dot\beta_t x_0 + \dot\sigma_t \varepsilon.
\end{equation}
Plugging this into the definition of $g^{\text{local}}$ and we get
\begin{align}
    \nonumber g_t^{\text{local}} = & -\mathbb{E}_{z \sim p(z|x_t)}[\underbrace{(x_1 - \hat{x}_1)(\dot \alpha_t x_1 + \dot\beta_t x_0 + \dot\sigma_t\varepsilon}_{\text{substitute } x_0 \text{ with } x_1,~ \sigma_t\varepsilon, \text{ and } x_t})] \nabla_{\hat{x}_1} J(\hat{x}_1)\\
    \nonumber =& -\mathbb{E}_{z \sim p(z|x_t)}[(x_1 - \hat{x}_1)(\dot \alpha_t x_1 + \frac{\dot\beta_t}{\beta_t} (x_t - \alpha_t x_1 - \sigma_t \varepsilon) + \dot\sigma_t\varepsilon)] \nabla_{\hat{x}_1} J(\hat{x}_1)\\
    \nonumber =& -\mathbb{E}_{z \sim p(z|x_t)}\left[
    (x_1 - \hat{x}_1)\left(
    \left(\frac{\beta_t\dot \alpha_t - \alpha_t\dot\beta_t}{\beta_t}\right) x_1 +  \cancel{x_t}  + (\dot\sigma_t - \sigma_t)\varepsilon
    \right)
    \right] 
    \nabla_{\hat{x}_1} J(\hat{x}_1)\\
    = & -{\frac{\dot\alpha_t\beta_t - \dot\beta_t\alpha_t}{\beta_t}} \Sigma_{1|t} \nabla_{\hat{x}_1}J(\hat{x}_1)
    + \underbrace{(\sigma_t - \dot\sigma_t)\mathbb{E}_{z \sim p(z|x_t)}[(x_1-\hat{x}_1)\varepsilon]\nabla_{\hat{x}_1}J(\hat{x}_1)}_{\coloneqq\Upsilon,~\lim_{\sigma_t\rightarrow 0, \dot\sigma_t \rightarrow 0}\|\Upsilon\|_2^2=0 },
\end{align}
where the $x_t$ term is canceled out because $\int p(z|x_t) (x_1 - \mathbb{E}_{z\sim p(z|x_t)}[x_1])dz = \int p(z|x_t) x_1 dz -\mathbb{E}_{z\sim p(z|x_t)}[x_1] = 0$, $\Sigma_{1|t}\coloneqq \mathbb{E}_{z \sim p(z|x_t)}\left[
(x_1 - \hat{x}_1)(x_1 - \hat{x}_1)\right]$, and the residual term that characterizes the approximation error (denoted as $\|\Upsilon\|^2_2$) in Eq. \eqref{eq:g_cov} (restated in Eq. \eqref{eq:restate_g_cov}) is
\begin{align}
    \nonumber \Upsilon = &(\sigma_t - \dot\sigma_t)\mathbb{E}_{z \sim p(z|x_t)}[(x_1-\hat{x}_1)\varepsilon]\nabla_{\hat{x}_1}J(\hat{x}_1) \\
    \nonumber =&(\sigma_t - \dot\sigma_t)\nabla_{\hat{x}_1}J(\hat{x}_1)\int \frac{p(x_t|z)p(z)}{p(x_t)}(x_1-\hat{x}_1)\varepsilon dx_1dx_0 \\
    \nonumber =&(\sigma_t - \dot\sigma_t)\nabla_{\hat{x}_1}J(\hat{x}_1)\int \frac{p(\sigma_t\varepsilon)\pi(x_0|x_1)p(x_1)}{p(x_t)}(x_1-\hat{x}_1)\frac{1}{\sigma_t}(x_t - \alpha_t x_1 - \beta_t x_0) dx_1dx_0 \\
    \nonumber =&(\sigma_t - \dot\sigma_t)\nabla_{\hat{x}_1}J(\hat{x}_1)\int \frac{p(x_1)}{p(x_t)}(x_1-\hat{x}_1) dx_1 \int p(\sigma_t\varepsilon)\pi(x_0|x_1) \varepsilon dx_0 \\
    =&(\sigma_t - \dot\sigma_t)\nabla_{\hat{x}_1}J(\hat{x}_1)\int \frac{p(x_1)}{p(x_t)}(x_1-\hat{x}_1) 
    \mathbb{E}_{\varepsilon\sim p_\varepsilon(\varepsilon)}
    \left[{\pi\left(\frac{1}{\beta_t}(x_t - \alpha_t x_1 - \sigma_t \varepsilon)\mid x_1\right)} \varepsilon \right]dx_1 ,
\end{align}
where $p_\varepsilon(\varepsilon)$ is the marginal distribution of $\varepsilon$.
Suppose $\|\pi\left(\frac{1}{\beta_t}(x_t - \alpha_t x_1 - \sigma_t \varepsilon)\mid x_1\right)\|^2_2 = \|\pi\left(x_0 \mid x_1\right)\|^2_2\le \mathcal{M}(x_1,x_t)$ (which is a function independent of $\varepsilon$, then
\begin{align}
    \nonumber &\|\Upsilon\|^2_2 \\
    \nonumber \le& \left\|(\sigma_t - \dot\sigma_t)\nabla_{\hat{x}_1}J(\hat{x}_1)\right\|_2^2\cdot \left\|\int\frac{p(x_1)}{p(x_t)}(x_1-\hat{x}_1)\mathbb{E}_{\varepsilon\sim p_\varepsilon(\varepsilon)}
    \left[{\pi\left(\frac{1}{\beta_t}(x_t - \alpha_t x_1 - \sigma_t \varepsilon)\mid x_1\right)} \varepsilon \right]dx_1\right\|^2_2 \\
    \nonumber \le& |(\sigma_t - \dot\sigma_t)|\underbrace{\left\|\nabla_{\hat{x}_1}J(\hat{x}_1)\right\|_2^2}_{\coloneqq \mathcal{G}}\cdot \int
    \underbrace{\left\|\frac{p(x_1)}{p(x_t)}(x_1-\hat{x}_1)\right\|^2_2}_{\coloneqq\mathcal{Q}}
    \cdot
    \underbrace{\left\|\mathbb{E}_{\varepsilon\sim p_\varepsilon(\varepsilon)}
    \left[{\pi\left(\frac{1}{\beta_t}(x_t - \alpha_t x_1 - \sigma_t \varepsilon)\mid x_1\right)} \varepsilon \right]\right\|^2_2}_{\le \left(\mathcal{M}\mathbb{E}_{\varepsilon\sim p_\varepsilon(\varepsilon)}[\|\varepsilon\|_2]\right)^2 \le \mathcal{M}^2 \text{Var}_{p_\varepsilon}}dx_1 \\
    \le& |(\sigma_t - \dot\sigma_t)|\mathcal{G}(x_t)\int \mathcal{Q}(x_1,x_t))\mathcal{M}^2(x_1,x_t)\text{Var}_{p_\varepsilon} dx_1,
\end{align}
all of which are independent on $x_0$ or $\sigma_t$. Thus, 
\begin{equation}
    \lim_{\sigma\rightarrow 0,\sigma_t\rightarrow 0}\|\Upsilon\|^2_2 = 0.
\end{equation}


\subsection{The Jacobian Trick}\label{appendix:jacobian_trick}
We prove the Jacobian Trick here. 
\begin{proposition} The Jacobian trick. Under Assumption \ref{assumption:uncoupled_affine_gaussian_path}, the inverse covariance matrix of $p(x_1|x_t)$, $\Sigma_{1|t}$, is affine to the Jacobian of the VF $\frac{\partial v_t}{\partial x_t}$, and is proportional to the Jacobian $\frac{\partial \hat{x}_1}{\partial x_t}$:
\begin{align}\nonumber
     \Sigma_{1|t}= \frac{\beta_t^2}{\alpha_t(\dot\alpha_t\beta_t - \dot\beta_t\alpha_t)} (-\dot\beta_t+ \beta_t\frac{\partial v_t}{\partial x_t} )
     = \frac{\beta_t^2}{\alpha_t} \frac{\partial \hat{x}_1}{\partial x_t}.
\end{align}
\end{proposition}

\textbf{Proof.}


To begin with, we prove $\Sigma_{1|t}=\frac{\beta_t^2}{\alpha_t} \frac{\partial \hat{x}_1}{\partial x_t}$. A similar conclusion has been proved in \citet{ye_tfg_2024}. We generalize their proof to affine Gaussian path flow matching:

Recall from Eq. \eqref{eq:appendix:local_approx_guidance_affine_path_estimate_x1} that
\begin{equation}
    \hat{x}_1 = -\frac{\dot\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} x_t + \frac{\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} v_t
\end{equation}
and \begin{equation}
    \hat{x}_0 = \frac{\dot\alpha_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} x_t - \frac{\alpha_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} v_t.
\end{equation}
So we have the Jacobian trick
\begin{equation}
    \frac{\partial \hat{x}_1}{\partial x_t} = -\frac{\dot\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} + \frac{\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} \frac{\partial v_t(x_t)}{\partial x_t},
\end{equation}
and because the VF is associated with the score
\begin{equation}\label{eq:appendix:score_and_vector_field}
    v_t(x_t) = \frac{\beta_t(\dot\alpha_t\beta_t - \dot \beta_t\alpha_t)}{\alpha_t} \nabla_{x_t}\log p_t(x_t) + \frac{\dot\alpha_t}{\alpha_t}x_t.
\end{equation}

Next, we try to prove 
\begin{equation}\label{eq:appendix:score_derivative_and_covariance}
    \nabla_{x_t}\nabla_{x_t}\log p_t(x_t) = -\frac{1}{\beta_t^2} + \frac{\alpha_t^2}{\beta_t^4} \Sigma_{x_1x_1},
\end{equation}
which allows us to connect the derivative of the score $\nabla^2_{x_t}\log p(x_t)$\footnotemark with the covariance matrix $\Sigma_{1|t}$.
\footnotetext{We use $\nabla \nabla$ and $\nabla^2$ interchangeably, with a little abuse of notation. It should not cause confusion since the size of the terms in the equations must match.}

\begin{align}
    \nonumber \nabla_{x_t}^2\log p(x_t) = 
    \nonumber & \frac{\nabla_{x_t}^2 p(x_t) }{p(x_t)} - \nabla_{x_t}\log p(x_t) \nabla_{x_t}\log p(x_t)\\
    \nonumber =& \frac{1}{p(x_t)} \int p(x_1)
    \underbrace{\nabla_{x_t}^2 p(x_t|x_1)}_{\text{using }\nabla^2 p = p\nabla^2 \log p + p(\nabla \log p)^2}
    dx_1
    - \nabla_{x_t}\log p(x_t) \nabla_{x_t}\log p(x_t) \\
    \nonumber =& \frac{1}{p(x_t)} \int p(x_1) (p(x_t|x_1)\nabla_{x_t}^2\log p(x_t|x_1) + p(x_t|x_1)\nabla_{x_t}\log p(x_t|x_1)\nabla_{x_t}\log p(x_t|x_1)) dx_1\\
    \nonumber &
    - \nabla_{x_t}\log p(x_t) \nabla_{x_t}\log p(x_t)\\
    \nonumber =&\mathbb{E}_{x_1\sim p(x_1|x_t)}\left[
        \nabla_{x_t}^2\log p(x_t|x_1)+ \nabla_{x_t}\log p(x_t|x_1)\nabla_{x_t}\log p(x_t|x_1)
    \right]
    - \nabla_{x_t}\log p(x_t) \nabla_{x_t}\log p(x_t) \\
    \nonumber =&\mathbb{E}_{x_1\sim p(x_1|x_t)}\left[
        -\frac{1}{\beta_t^2}
        +\left(
            \frac{x_t - \alpha_t x_1}{\beta_t^2}
        \right)^2
    \right]
    -\left(
        \frac{x_t - \alpha_t \mathbb{E}_{x_1\sim p(x_1|x_t)}[x_1]}{\beta_t^2}
    \right)^2 \\
    \nonumber =&-\frac{1}{\beta_t^2} + \frac{\alpha_t^2}{\beta_t^4} 
    \left(\mathbb{E}[x_1 x_1^T] - \mathbb{E}[x_1]\mathbb{E}[x_1]^T\right) \\
    =& -\frac{1}{\beta_t^2} + \frac{\alpha_t^2}{\beta_t^4} \Sigma_{x_1x_1}.
\end{align}

Then by combining Eq. \eqref{eq:appendix:score_and_vector_field} and \eqref{eq:appendix:score_derivative_and_covariance} we have
\begin{align}
    \nonumber \frac{\partial \hat{x}_1}{\partial x_t} &=  -\frac{\dot\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} + \frac{\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} 
    \left(
    \frac{\beta_t(\dot\alpha_t\beta_t - \dot \beta_t\alpha_t)}{\alpha_t} \nabla_{x_t}\nabla_{x_t}\log p_t(x_t) + \frac{\dot\alpha_t}{\alpha_t}
    \right) \\
    \nonumber & = -\frac{\dot\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} + \frac{\beta_t}{\dot \alpha_t \beta_t - \dot \beta_t \alpha_t} 
    \left(
    \frac{\beta_t(\dot\alpha_t\beta_t - \dot \beta_t\alpha_t)}{\alpha_t} (-\frac{1}{\beta_t^2} + \frac{\alpha_t^2}{\beta_t^4} \Sigma_{x_1x_1}) +\frac{\dot\alpha_t}{\alpha_t}
    \right) \\
    &=\frac{\alpha_t}{\beta_t^2}\Sigma_{x_1x_1}.
\end{align}

Inserting back Eq. \eqref{eq:appendix:local_approx_guidance_affine_path_estimate_x1} and we prove 
\begin{equation}
    \Sigma_{1|t}= \frac{\beta_t^2}{\alpha_t(\dot\alpha_t\beta_t - \dot\beta_t\alpha_t)} (-\dot\beta_t+ \beta_t\frac{\partial v_t}{\partial x_t} ).
\end{equation}



\subsection{Proof for $g_t^{\text{sim-inv}}$}\label{appendix:approximate_simple_posterior_pigdm_like}
We begin with
\begin{align}
    &g^{\text{sim-inv}}_t(x_t) = \int \left(\frac{e^{-J(x_1)}}{\tilde{Z}_t} - 1\right) v_{t|z}(x_t|z) \tilde{p}(z|x_t) dz, \\
    &\text{where }\tilde{Z}_t = \int e^{-J(x_1)} \tilde{p}(z|x_t)dz,
\end{align}
and approximate $p(x_1|x_t)$ with $\mathcal{N}(x_1;\hat{x}_1, \Sigma_t)$ where $\hat{x}_1\coloneqq \mathbb{E}_{z\sim p(z|x_t)}[x_1]$ and $\Sigma_t$ is already known.

With Assumption \ref{assumption:affine_path} and assuming $e^{-J(x_1)} = \mathcal{N}(Hx_1;y, \sigma_y I)$, we have 
\begin{align}
    \tilde{Z}_t &= \int e^{-J(x_1)} \tilde{p}(z|x_t)dz \nonumber\\
    &=\int e^{-\frac{1}{2\sigma_y^2}\|y-Hx_1\|^2_2 - \frac{1}{2}(z-\hat{z})^T\Sigma_t^{-1}(z-\hat{z})} dz
\end{align}
where $\hat{z} = (\hat{x}_0, \hat{x}_1)$ is the expectation of $z$ under $p(z|x_t)$. Note $H$ operates on $x_1$ only, and we pad the blocks related to $x_0$ with zero in $H$.


Then, by inserting the Gaussian approximation
\begin{align}
    g^{\text{sim-inv}}_t(x_t) &\approx \int \left(\frac{e^{-J(x_1)}}{\tilde{Z}_t} - 1\right) (\dot\alpha_t x_1 + \dot \beta_t x_0) \underbrace{\tilde{p}(z|x_t)}_{\text{Gaussian}} dz \\
    & = \int \underbrace{\frac{1}{\tilde{Z}_t}\exp\left(-\frac{1}{2\sigma_y^2}\|y-Hx_1\|^2_2 - \frac{1}{2}(z-\hat{z})^T\Sigma_t^{-1}(z-\hat{z})\right)}_{\coloneqq \tilde{\tilde{p}}(z|x_t)} (\dot\alpha_t x_1 + \dot \beta_t x_0)  dz - v_t(x_t).\label{eq:appendix:gaussian_approx_affine_integral_of_g}
\end{align}

\begin{remark}
    Note that $\Sigma_t^{-1}$ couples $x_0$ and $x_1$. This is a fundamental feature of dependent couplings $\pi(x_0,x_1)$. However, it may seem tempting to further assume that $\Sigma_t^{-1}$ is diagonal or even a scalar. It should be noted that this assumption completely discards the dependency of $x_0$ and $x_1$ in the coupling, and thus, we try to avoid that in the dependent coupling case.
\end{remark}

For clarity, we need to express $\Sigma_t^{-1}$ with 
\begin{equation}
    \Sigma_t^{-1} \overset{\Delta}{=} \begin{pmatrix}
        \Xi_{00} & \Xi_{01} \\
        \Xi_{10} & \Xi_{11}
    \end{pmatrix}.
\end{equation}

Then, the distribution $\exp\left(-\frac{1}{2\sigma_y^2}\|y-Hx_1\|^2_2 - \frac{1}{2}(z-\hat{z})^T\Sigma_t^{-1}(z-\hat{z})\right)$ is still a Gaussian, and to estimate the expectation of $z=(x_0,x_1)$ we need to simply the probability density of this Guassian into a standard form.
\begin{align}
    \nonumber {\tilde{Z}_t}\tilde{\tilde{p}}(z|x_t) = &\exp\left(-\frac{1}{2\sigma_y^2}\|y-Hx_1\|^2_2 - \frac{1}{2}(z-\hat{z})^T\Sigma_t^{-1}(z-\hat{z})\right) \\
    \nonumber =&\exp\left(-\frac{1}{2\sigma_y^2}
    \left(
        \|y\|^2- 2\langle y, Hx_1 \rangle + \|Hx_1\|^2_2
    \right)
    -
    \frac{1}{2}z^T\Sigma_t^{-1}z - \frac{1}{2}\hat{z}^T{\Sigma_t^{-1}}\hat{z} + \underbrace{\langle z,\Sigma_t^{-1} \hat{z} \rangle}_{\text{since } \Sigma_t^{-1} = {\Sigma_t^{-1}}^T}\right) \\
    =&\exp\left(-\frac{1}{2}\left(
        z^T\left(\frac{H^TH}{\sigma_y^2} + \Sigma_t^{-1}\right)z
        -2\langle
            z, \frac{H^T}{\sigma_y^2}y + \Sigma_t^{-1}\hat{z}
        \rangle
        +\left(
            \frac{1}{2\sigma_y^2}\|y\|^2 + \frac{1}{2}\hat{z}^T{\Sigma_t^{-1}}\hat{z}
        \right)
        \right)
    \right),
\end{align}
It is obvious that the mean of this Gaussian is
\begin{equation}
    \mu = \Bigg{(}\underbrace{\frac{H^TH}{\sigma_y^2} + \Sigma_t^{-1}}_{\coloneqq P}\Bigg{)}^{-1}
    \left(
    \frac{H^T}{\sigma_y^2}y + \Sigma_t^{-1}\hat{z}
    \right),
\end{equation}
where we can find $P$'s blocks to be 
\begin{equation}
    P = \begin{pmatrix}
        \Xi_{00} & \Xi_{01} \\
        \Xi_{10} & \Xi_{11} + \frac{H^TH}{\sigma_y^2}
    \end{pmatrix}.
\end{equation}

Then, by computing $\mu = (\hat{\hat{x}}_0,\hat{\hat{x}}_1)$ we can compute $g_t^{\text{sim-inv}} + v_t$, where $\hat{\hat{x}}_0,\hat{\hat{x}}_1$ are the $x_0$ and $x_1$ term in the integral of Eq. \eqref{eq:appendix:gaussian_approx_affine_integral_of_g}, because
\begin{equation}
     g_t^{\text{sim-inv}} + v_t = \mathbb{E}_{z\sim\tilde{\tilde{p}}(z|x_t)} [\dot \alpha_t x_1 + \dot\beta x_0].
\end{equation}

To simplify, insert back $v_t$ to get
\begin{align}
     \nonumber g_t^{\text{sim-inv}} &= \mathbb{E}_{z\sim\tilde{\tilde{p}}(z|x_t)} [\dot \alpha_t x_1 + \dot\beta x_0] - \mathbb{E}_{z\sim \tilde{p}(z|x_t)}[\dot \alpha_t x_1 + \dot\beta x_0] \\
     \nonumber &=
     \begin{pmatrix}
     \dot\beta_t I & \dot\alpha_t I
     \end{pmatrix}
     P^{-1}
     \left(
    \begin{pmatrix}0 \\ \frac{H^T}{\sigma_y^2}y\end{pmatrix}
        +
     \left(
        \begin{pmatrix}
            \Xi_{00} & \Xi_{01} \\
            \Xi_{10} & \Xi_{11}
        \end{pmatrix}
        -
        P
    \right)
     \begin{pmatrix}\hat{x}_0 \\ \hat{x}_1 \end{pmatrix}
     \right)\\
     \nonumber &=
     \begin{pmatrix}
     \dot\beta_t I & \dot\alpha_t I
     \end{pmatrix}
     P^{-1}\left(
    \begin{pmatrix}0 \\ \frac{H^T}{\sigma_y^2}y\end{pmatrix}
        +
    \begin{pmatrix}
            0 & 0 \\
            0 & -\frac{H^TH}{\sigma_y^2}
    \end{pmatrix}
    \begin{pmatrix}\hat{x}_0 \\ \hat{x}_1 \end{pmatrix}
    \right) \\
    & = 
    \begin{pmatrix}
     \dot\beta_t I & \dot\alpha_t I
     \end{pmatrix}
     P^{-1}
    \begin{pmatrix}0 \\ \frac{H^T}{\sigma_y^2}y - \frac{H^TH}{\sigma_y^2} \hat{x}_1\end{pmatrix}.
\end{align}
Usually, $\begin{pmatrix}\dot\beta_t I & \dot\alpha_t I\end{pmatrix}P^{-1}$ is difficult to obtain:
\begin{equation}
    g_t^{\text{sim-inv}} = (\dot\beta_t P^{-1}_{01} + \dot\alpha_t P^{-1}_{11})\left(\frac{H^T}{\sigma_y^2}y - \frac{H^TH}{\sigma_y^2} \hat{x}_1\right), 
\end{equation}
where $P_{01}^{-1}$ and $P_{11}^{-1}$ requires computing the inversion of $P$ and thus in general intractable. 
Using block matrix inversion, we have
\begin{align}
    g_t^{\text{sim-inv}} = \Bigg{(}-\dot\beta_t \Xi_{11}^{-1}\Xi_{01} \Big{(}\Xi_{11} + \frac{H^TH}{\sigma_y^2}-\Xi_{10}\Xi_{11}^{-1}\Xi_{01}\Big{)}^{-1}
    +\dot\alpha_t \Big{(}\Xi_{11} + \frac{H^TH}{\sigma_y^2}-\Xi_{10}\Xi_{11}^{-1}\Xi_{01}\Big{)}^{-1}\Bigg{)}
    \left(\frac{H^T}{\sigma_y^2}y - \frac{H^TH}{\sigma_y^2} \hat{x}_1\right).
\end{align}

For general (possibly coupled) affine path flow matching, we can make approximations and set the \emph{blocks} in $\Sigma_t^{-1}$ to scalars. It should be noted that this Gaussian assumption can still capture some coupling between $x_0$ and $x_1$ since the off-diagonal blocks $\Xi_{01}$ and $\Xi_{10}$ are not set to zero.
Specifically, we have
\begin{equation}
    g_t^{\text{sim-inv-A}} = -\lambda_t \Big{(}\frac{\sigma_y^2}{r_t^2} + H^TH\Big{)}^{-1}
    H^T\left(y - {H} \hat{x}_1\right),
\end{equation}
where $\lambda_t$ and $r_t^2$ are hyperparameters. $\lambda_t$ approximates $\dot\alpha_t-\dot\beta_t \Xi_{11}^{-1}\Xi_{01}$, absorbing the flow schedule.

\textbf{Special case: the non-coupled affine Gaussian path}

Next, we prove that $g_t^{\text{sim-inv}}$ covers $\Pi$GDM \citep{song_pseudoinverse-guided_2022} and OT-ODE \citep{pokle_training-free_2024} as special cases.
Under the uncoupled affine Gaussian path assumption (Assumption \ref{assumption:uncoupled_affine_gaussian_path}), one may think that the covariance matrix is block diagonal, but it is false: $x_0$ and $x_1$ are still dependent on each other in the distribution $p(z|x_t) = p(x_0,x_1|x_t)$ even if the coupling is independent. In the uncoupled case, the probability graph is $x_0 \rightarrow x_t \leftarrow x_1$, so although $x_0$ and $x_1$ are marginally independent ($\pi(x_0,x_1)= p(x_0)p(x_1)$), their conditional can be dependent $p(x_0,x_1|x_t)\neq p(x_0|x_t)p(x_1|x_t)$.
Then, we notice the uncoupled path is 
\begin{equation}
    x_t = \alpha_t x_1 + \beta_t x_0,
\end{equation}
so we actually should not have approximated the distribution $p_{z|x_t}$ as a Gaussian in the uncoupled case. Fortunately, there is a workaround to make $x_0$ almost entirely dependent on $x_1$. We can set $x_0 = -\frac{\alpha_t}{\beta_t}x_1 + \frac{1}{\beta_t}x_t + \xi \epsilon$, where $\epsilon \sim \mathcal{N}(0, I)$, and setting $\xi \rightarrow 0$ gives our desired uncoupled path results. The covariance matrix of $x_0$ and $x_1$ to:
\begin{equation}
    \Sigma_t = \begin{pmatrix}
        \frac{\alpha_t^2}{\beta_t^2}\Sigma_{x_1 x_1} + \xi^2 I & -\frac{\alpha_t}{\beta_t} \Sigma_{x_1 x_1} \\
        -\frac{\alpha_t}{\beta_t} \Sigma_{x_1 x_1} & \Sigma_{x_1 x_1}
    \end{pmatrix}
\end{equation}
Note that $\Sigma_{x_1 x_1}^{-1} \neq \Xi_{11}$ as $\Xi_{11}$ is a block in the inversion of the larger matrix $\Sigma_t$. Next we compute $\Sigma_t^{-1}$:
\begin{align}
    \nonumber \Sigma_t^{-1} =& \begin{pmatrix}
    \frac{1}{\xi^2} I & -\frac{a}{\xi^2} I \\
    -\frac{a}{\xi^2} I  & \Big{(} \Sigma_{x_1 x_1} - a^2\Sigma_{x_1 x_1} (a^2\Sigma_{x_1 x_1} +\xi^2 I)^{-1} \Sigma_{x_1 x_1} \Big{)}^{-1}
    \end{pmatrix} \\
    = &\begin{pmatrix}
    \frac{1}{\xi^2} I & \frac{\alpha_t}{\beta_t\xi^2} I  \\
    \frac{\alpha_t}{\beta_t\xi^2} I  & \Big{(} \Sigma_{x_1 x_1} - \Sigma_{x_1 x_1} (\Sigma_{x_1 x_1} +\frac{\beta_t^2}{\alpha_t^2}\xi^2 I)^{-1} \Sigma_{x_1 x_1} \Big{)}^{-1}
    \end{pmatrix}.
\end{align}
where $a = -\frac{\alpha_t}{\beta_t}$. Therefore, $\Xi_{11} \rightarrow \infty$, $\Xi_{01}=\Xi_{10}=\frac{\alpha_t}{\beta_t\xi^2} I \rightarrow \infty$, and $\Xi_{00} = \frac{1}{\xi^2} I\rightarrow \infty $. Thus, we need more detailed calculations to get the result:

\begin{align}
    \nonumber g_t^{\text{sim-inv-diffusion}} =& \Bigg{(}-\dot\beta_t \underbrace{\Xi_{11}^{-1}\Xi_{01}}_{\rightarrow \frac{\alpha_t}{\beta_t} I } \Big{(}\underbrace{\Xi_{11}}_{\rightarrow \infty} + \frac{H^TH}{\sigma_y^2}-\underbrace{\Xi_{10}\Xi_{11}^{-1}\Xi_{01}}_{\rightarrow \infty}\Big{)}^{-1}
    +\dot\alpha_t \Big{(}\underbrace{\Xi_{11}}_{\rightarrow \infty} + \frac{H^TH}{\sigma_y^2}-\underbrace{\Xi_{10}\Xi_{11}^{-1}\Xi_{01}}_{\rightarrow \infty} \Big{)}^{-1}\Bigg{)}\\
    &\left(\frac{H^T}{\sigma_y^2}y - \frac{H^TH}{\sigma_y^2} \hat{x}_1\right).
\end{align}
Obviously we want to find the finite term left in $\Xi_{11} - \Xi_{10}\Xi_{11}^{-1}\Xi_{01}$:
\begin{align}
    \nonumber &\lim_{\xi\rightarrow 0}\Xi_{11} - \Xi_{10}\Xi_{11}^{-1}\Xi_{01} \\
    \nonumber =&\lim_{\xi\rightarrow 0}\Big{(} \Sigma_{x_1 x_1} - \Sigma_{x_1 x_1} (\Sigma_{x_1 x_1} +\frac{\beta_t^2}{\alpha_t^2}\xi^2 I)^{-1} \Sigma_{x_1 x_1} \Big{)}^{-1} - \Xi_{10}\Xi_{11}^{-1}\Xi_{01}\\
    \nonumber =&\lim_{\xi\rightarrow 0}\Big{(} \Sigma_{x_1 x_1} - \Sigma_{x_1 x_1} (\Sigma_{x_1 x_1} +\frac{\beta_t^2}{\alpha_t^2}\xi^2 I)^{-1} \Sigma_{x_1 x_1} \Big{)}^{-1} - \frac{\alpha_t^2}{\beta_t^2\xi^2} \\
    \nonumber =&\lim_{\xi\rightarrow 0}\left( \Sigma_{x_1 x_1}\left(
    \Sigma_{x_1 x_1} +\frac{\beta_t^2}{\alpha_t^2}\xi^2 I\right)^{-1}
    \left(
    (\cancel{\Sigma_{x_1 x_1}} +\frac{\beta_t^2}{\alpha_t^2}\xi^2 I)
    - \cancel{\Sigma_{x_1 x_1}}
    \right)
    \right)^{-1} - \frac{\alpha_t^2}{\beta_t^2\xi^2}\\
    \nonumber =&\lim_{\xi\rightarrow 0}
    \frac{\alpha_t^2}{\beta_t^2\xi^2}
    \left(
    \Sigma_{x_1 x_1}\left(
    \Sigma_{x_1 x_1} +\frac{\beta_t^2}{\alpha_t^2}\xi^2 I\right)^{-1}
    -1
    \right)
    \\
    =&\Sigma_{x_1 x_1}^{-1}. 
\end{align}

Now we have 
\begin{equation}
    g_t^{\text{sim-inv-diffusion}} = \frac{\dot \alpha_t \beta_t - \dot\beta_t\alpha_t}{\beta_t} \left(
    \Sigma_{x_1 x_1}^{-1} + \frac{H^TH}{\sigma_y^2}
    \right)^{-1}
    \left(
    \frac{H^T}{\sigma_y^2}y - \frac{H^TH}{\sigma_y^2} \hat{x}_1
    \right).
\end{equation}

This is essentially the same formulation as in $\Pi$GDM \citep{song_pseudoinverse-guided_2022} and OT-ODE \citep{pokle_training-free_2024}. Next, we will make some trivial conversions to cover the formulations exactly. 


In diffusion paths (Assumption \ref{proposition:jacobian_trick}) we proved that $\frac{\partial\hat{x}_t}{\partial x_t} = \frac{\alpha_t}{\beta_t^2}\Sigma_{1|t}$ where $\Sigma_{1|t}$ is just what we denote $\Sigma_{x_1 x_1}$ here. Equivalently, \begin{equation}
    \frac{\partial x_t}{\partial \hat{x}_1} = \frac{\beta_t^2}{\alpha_t}\Sigma_{x_1x_1}^{-1}.
\end{equation}

Thus,
\begin{align}
    \nonumber g_t^{\text{sim-inv-diffusion}} =& \frac{\dot \alpha_t \beta_t - \dot\beta_t\alpha_t}{\beta_t} \left(
    \Sigma_{x_1 x_1}^{-1} + \frac{H^TH}{\sigma_y^2}
    \right)^{-1}
    \left(
    \frac{H^T}{\sigma_y^2}y - \frac{H^TH}{\sigma_y^2} \hat{x}_1
    \right) \\
    \nonumber =&\frac{\dot \alpha_t \beta_t - \dot\beta_t\alpha_t}{\beta_t}
    \Sigma_{x_1 x_1}
    \left(
    {\sigma_y^2}I + \Sigma_{x_1 x_1} H^TH
    \right)^{-1}
    H^T
    \left(
    y - H \hat{x}_1
    \right) \\
    \nonumber =&\frac{\dot \alpha_t \beta_t - \dot\beta_t\alpha_t}{\beta_t}
    \frac{\beta_t^2}{\alpha_t}\frac{\partial \hat{x}_1}{\partial x_t}
    \left(
        {\sigma_y^2}I + \Sigma_{x_1 x_1}H^TH 
        \right)^{-1}
        \left(
        H^T
        y - H^TH \hat{x}_1
    \right) \\
    \nonumber =&\frac{\beta_t(\dot \alpha_t \beta_t - \dot\beta_t\alpha_t)}{\alpha_t}
    \left(
        \frac{\partial \hat{x}_1}{\partial x_t}
        \left(
        {\sigma_y^2}I + \Sigma_{x_1 x_1}H^TH 
        \right)^{-1}
        \left(
        H^T
        y - H^TH \hat{x}_1
        \right)
    \right)\\
    =&\frac{\beta_t(\dot \alpha_t \beta_t - \dot\beta_t\alpha_t)}{\alpha_t}
    \left(
        \left(
        y - H \hat{x}_1
        \right)^TH
        \left(
        {\sigma_y^2}I + \Sigma_{x_1 x_1} H^T H
        \right)^{-1}
        \frac{\partial \hat{x}_1}{\partial x_t}
    \right)^T.
\end{align}

Now we make the same approximation in $\Pi$GDM that $\Sigma_{x_1x_1} = r_t^2 I$. Then
by noticing that 
\begin{align}
    \nonumber \left(
        {\sigma_y^2}I + r_t^2  H H^T
    \right)H
    =&
    H\left(
        {\sigma_y^2}I + r_t^2  H^T H
    \right)
    \\
    H\left(
        {\sigma_y^2}I + r_t^2  H^T H
    \right)^{-1}
    =&
    \left(
        {\sigma_y^2}I + r_t^2  H H^T
    \right)^{-1}   
    H
\end{align}
We exactly cover 
\begin{equation}
    g_t^{\text{sim-inv-$\Pi$GDM}} = \frac{\beta_t(\dot \alpha_t \beta_t - \dot\beta_t\alpha_t)}{\alpha_t}
    \left(
        \left(
        y - H \hat{x}_1
        \right)^T
        \left(
        {\sigma_y^2}I + r_t^2 H^T H
        \right)^{-1}H
        \frac{\partial \hat{x}_1}{\partial x_t}
    \right)^T,
\end{equation}
and the scheduler $\frac{\beta_t(\dot \alpha_t \beta_t - \dot\beta_t\alpha_t)}{\alpha_t}$ in the path $\alpha_t=t,\beta_t=1-t$ becomes $\frac{1-t}{t}$, which exactly covers the schedule in OT-ODE which takes the same path.
In addition, we can also directly compute $\Sigma_{x_1x_1}$ using $\frac{\partial \hat{x}_1}{\partial x_t}$ instead of approximating it with $r_t$. This corresponds to the approach in \citet{boys_tweedie_2024}, which uses the Jacobian to acquire the covariance and then remove the approximation error in computing $\left(
{\sigma_y^2}I + \Sigma_{x_1 x_1} H^T H
\right)^{-1}$.

\begin{remark}
    Starting from the more general assumption of affine path flow matching, we derived the guidance compatible with dependent coupling flow matching, including OT-CFM. The fact that our guidance can exactly cover classical diffusion guidance like $\Pi$GDM and affine Gaussian path flow matching guidance like OT-ODE verifies the validity of our theory.
\end{remark}