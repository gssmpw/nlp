\section{Related Work}
\noindent \textbf{Continual Learning (CL).} CL aims to effectively acquire new task knowledge during ongoing training while preserving knowledge from previously learned tasks. Traditional CL methods can be broadly classified into three categories: \textit{rehearsal-based}, \textit{regularization-based}, and \textit{architecture-based} approaches. Rehearsal-based methods mitigate catastrophic forgetting (CF) by selectively retaining samples \cite{GCRGC, inscl, seekr} or pseudo-generative examples \cite{ContinualLW, Sun2019LAMOLLM} from prior tasks. In contrast, regularization-based methods employ quadratic regularization terms to constrain the update of weights critical to previous tasks \cite{Zhu2024ModelTM, migu}, thereby stabilizing learned knowledge. Architecture-based methods introduce extra task-specific parameters for each new task \cite{olora, sapt}, ensuring that previously acquired knowledge remains intact. These methods typically rely on access to old task data or involve discrete component learning, which presents significant challenges for the CL. In contrast, DATA leverages representations from diverse rank spaces and harnesses the inherent capabilities of LLMs to enable more effective CL \cite{cotta}.

\begin{figure*}[t]
\centerline{\includegraphics[width=0.95\textwidth]{div.pdf}}
% \vspace{-0.45cm}
\caption{\textcolor{red}{(a)} We perform a t-SNE distribution analysis of different adapter representations on Order 1(4 tasks). The low-rank branch shows a consistent distribution across the target tasks and the high-rank branch exhibits substantial distribution differences across the target tasks. \textcolor{red}{(b)} We calculate the divergence of different branches in Order 4 (15 tasks). In comparison to the source model, low-rank adapters effectively alleviates inter-task divergence across all 14 task transitions, while the high-rank adapters significantly enhances intra-task feature aggregation.
% The low-rank branch shows a consistent distribution across the target tasks, highlighting its effectiveness in minimizing the impact of dynamic distribution shifts. In contrast, the high-rank branch exhibits substantial distribution differences across the target tasks, indicating its primary focus on extracting task-specific knowledge.
}
\label{div}
% \vspace{-0.65cm}
\end{figure*}

\noindent \textbf{Parameter Efficient Fine-Tuning (PEFT).} LLMs have shown remarkable adaptability across a wide range of downstream tasks. However, traditional full fine-tuning for each task can incur \textit{significant computational and storage overheads}, often resulting in overfitting \cite{slora}. To mitigate these challenges, researchers have increasingly turned to PEFT methods, which can achieve comparable or even superior performance and generalization by fine-tuning only a small subset of parameters.
For instance, \textit{adapters} \cite{adapter} incorporate additional modules into various layers of the Transformer architecture, while \textit{prompt-tuning} \cite{prompt} and \textit{prefix-tuning} \cite{prefix} introduce learnable soft tokens at different layers of the Transformer input. \textit{Low-rank adaptation} (LoRA) \cite{hu2022lora} inserts low-rank branches into pre-trained weights and fine-tunes only those branches. Although these common PEFT techniques offer certain advantages, they are generally confined to static single-task or multi-task learning scenarios. In contrast, DATA leveraging the effectiveness and efficiency of LoRA enables continual adaptation by dynamically combining LoRAs of two different ranks \cite{vida} utilizing attention-based weights.

%