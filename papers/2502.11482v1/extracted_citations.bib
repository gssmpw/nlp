@inproceedings{ContinualLW,
  title={Continual Learning with Deep Generative Replay},
  author={Hanul Shin and Jung Kwon Lee and Jaehong Kim and Jiwon Kim},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:1888776}
}

@article{GCRGC,
  title={GCR: Gradient Coreset based Replay Buffer Selection for Continual Learning},
  author={Rishabh Tiwari and Krishnateja Killamsetty and Rishabh K. Iyer and Pradeep Shenoy},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={99-108},
  url={https://api.semanticscholar.org/CorpusID:244478041}
}

@inproceedings{Sun2019LAMOLLM,
  title={LAMOL: LAnguage MOdeling for Lifelong Language Learning},
  author={Fan-Keng Sun and Cheng-Hao Ho and Hung-yi Lee},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:209475822}
}

@article{Zhu2024ModelTM,
  title={Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models},
  author={Didi Zhu and Zhongyi Sun and Zexi Li and Tao Shen and Ke Yan and Shouhong Ding and Kun Kuang and Chao Wu},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.12048},
  url={https://api.semanticscholar.org/CorpusID:267751282}
}

@article{adapter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.00751},
  url={https://api.semanticscholar.org/CorpusID:59599816}
}

@article{cotta,
  title={Continual Test-Time Domain Adaptation},
  author={Qin Wang and Olga Fink and Luc Van Gool and Dengxin Dai},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={7191-7201},
  url={https://api.semanticscholar.org/CorpusID:247748613}
}

@inproceedings{inscl,
    title = "{I}ns{CL}: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
    author = "Wang, Yifan  and
      Liu, Yafei  and
      Shi, Chufan  and
      Li, Haoling  and
      Chen, Chen  and
      Lu, Haonan  and
      Yang, Yujiu",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.37/",
    doi = "10.18653/v1/2024.naacl-long.37",
    pages = "663--677",
    abstract = "Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL. When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay."
}

@inproceedings{migu,
  title={Unlocking Continual Learning Abilities in Language Models},
  author={Wenyu Du and Shuang Cheng and Tongxu Luo and Zihan Qiu and Zeyu Huang and Ka Chun Cheung and Reynold Cheng and Jie Fu},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270710911}
}

@inproceedings{olora,
    title = "Orthogonal Subspace Learning for Language Model Continual Learning",
    author = "Wang, Xiao  and
      Chen, Tianze  and
      Ge, Qiming  and
      Xia, Han  and
      Bao, Rong  and
      Zheng, Rui  and
      Zhang, Qi  and
      Gui, Tao  and
      Huang, Xuanjing",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.715/",
    doi = "10.18653/v1/2023.findings-emnlp.715",
    pages = "10658--10671",
    abstract = "Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks."
}

@inproceedings{prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353/",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {\textquotedblleft}virtual tokens{\textquotedblright}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training."
}

@article{prompt,
  title={Learning How to Ask: Querying LMs with Mixtures of Soft Prompts},
  author={Guanghui Qin and Jas' Eisner},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.06599},
  url={https://api.semanticscholar.org/CorpusID:233231453}
}

@inproceedings{sapt,
  title={SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models},
  author={Weixiang Zhao and Shilong Wang and Yulin Hu and Yanyan Zhao and Bing Qin and Xuanyu Zhang and Qing Yang and Dongliang Xu and Wanxiang Che},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267027714}
}

@inproceedings{seekr,
    title = "{SEEKR}: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models",
    author = "He, Jinghan  and
      Guo, Haiyun  and
      Zhu, Kuan  and
      Zhao, Zihan  and
      Tang, Ming  and
      Wang, Jinqiao",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.190/",
    doi = "10.18653/v1/2024.emnlp-main.190",
    pages = "3254--3266",
    abstract = "Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1{\%}. The code is available at https://github.com/jinghan1he/SEEKR."
}

@article{vida,
  title={ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation},
  author={Jiaming Liu and Senqiao Yang and Peidong Jia and Ming Lu and Yandong Guo and Wei Xue and Shanghang Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.04344},
  url={https://api.semanticscholar.org/CorpusID:259096014}
}

