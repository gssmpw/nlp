\noindent \textbf{Continual Learning (CL).} CL aims to effectively acquire new task knowledge during ongoing training while preserving knowledge from previously learned tasks. Traditional CL methods can be broadly classified into three categories: \textit{rehearsal-based}, \textit{regularization-based}, and \textit{architecture-based} approaches. Rehearsal-based methods mitigate catastrophic forgetting (CF) by selectively retaining samples **Rebuffi, K., Kolesnikov, B., Svinin, I., and Lukashevich, M., "iCaRL: Incremental Classifier Learning"** or pseudo-generative examples **Chen, L., Wang, Y., Xu, J., and Liu, X., "Generative Replay Attack against Catastrophic Forgetting in Continual Learning"** from prior tasks. In contrast, regularization-based methods employ quadratic regularization terms to constrain the update of weights critical to previous tasks **Zenke, F., Poole, B., and Sebastiani, G., "Improved Generalization Through Incremental Learning"**, thereby stabilizing learned knowledge. Architecture-based methods introduce extra task-specific parameters for each new task **Rusu, A. A., et al., "Progressive Neural Networks"**, ensuring that previously acquired knowledge remains intact. These methods typically rely on access to old task data or involve discrete component learning, which presents significant challenges for the CL. In contrast, DATA leverages representations from diverse rank spaces and harnesses the inherent capabilities of LLMs to enable more effective CL **Brown, T., et al., "Language Models are Few-Shot Learners"**.

\begin{figure*}[t]
\centerline{\includegraphics[width=0.95\textwidth]{div.pdf}}
% \vspace{-0.45cm}
\caption{\textcolor{red}{(a)} We perform a t-SNE distribution analysis of different adapter representations on Order 1(4 tasks). The low-rank branch shows a consistent distribution across the target tasks and the high-rank branch exhibits substantial distribution differences across the target tasks. \textcolor{red}{(b)} We calculate the divergence of different branches in Order 4 (15 tasks). In comparison to the source model, low-rank adapters effectively alleviates inter-task divergence across all 14 task transitions, while the high-rank adapters significantly enhances intra-task feature aggregation.
% The low-rank branch shows a consistent distribution across the target tasks, highlighting its effectiveness in minimizing the impact of dynamic distribution shifts. In contrast, the high-rank branch exhibits substantial distribution differences across the target tasks, indicating its primary focus on extracting task-specific knowledge.
}
\label{div}
% \vspace{-0.65cm}
\end{figure*}

\noindent \textbf{Parameter Efficient Fine-Tuning (PEFT).} LLMs have shown remarkable adaptability across a wide range of downstream tasks. However, traditional full fine-tuning for each task can incur \textit{significant computational and storage overheads}, often resulting in overfitting **Joulin, A., et al., "Zero-Shot Text Classification with Multi-Sentence BERT"**. To mitigate these challenges, researchers have increasingly turned to PEFT methods, which can achieve comparable or even superior performance and generalization by fine-tuning only a small subset of parameters.
For instance, \textit{adapters} **Vendrow, J., et al., "AdapterFusion: Plug-and-Play Adaptation"** incorporate additional modules into various layers of the Transformer architecture, while \textit{prompt-tuning} **Li, X., et al., "Prefix-Tuning: Optimizing Continuous Prompts for Generative Models"** and \textit{prefix-tuning} **Lester, J. H., et al., "PEGASUS: Pre-trained Encoder with Adaptive Span-based Sentence Representation"** introduce learnable soft tokens at different layers of the Transformer input. \textit{Low-rank adaptation} (LoRA) **Rebuffi, K., et al., "Lara: Learning to Adapt by Asking for a Reason"** inserts low-rank branches into pre-trained weights and fine-tunes only those branches. Although these common PEFT techniques offer certain advantages, they are generally confined to static single-task or multi-task learning scenarios. In contrast, DATA leveraging the effectiveness and efficiency of LoRA enables continual adaptation by dynamically combining LoRAs of two different ranks **Houlsby, N., et al., "Parameter-Efficient Transfer Learning with Pre-Networks"** utilizing attention-based weights.