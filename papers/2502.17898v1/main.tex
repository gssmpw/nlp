%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%\documentclass[manuscript,screen,review]{acmart}
%\documentclass[sigconf]{acmart}
%\documentclass[sigconf]{acmart}
%\documentclass[sigconf,review,anonymous]{acmart}
%\documentclass[manuscript, review, anonymous]{acmart}
\documentclass[sigconf, screen]{acmart}
%\documentclass[sigconf, screen , review, anonymous]{acmart}

\newcommand{\eg}[0]{\textit{e.g., }}
\newcommand{\ie}[0]{\textit{i.e., }}
\newcommand{\christine}[1]{{\color{purple} #1}}
\newcommand{\bilge}[1]{{\color{orange} #1}}
\newcommand{\revision}[1]{{\color{black} #1}}
\newcommand{\revisionNew}[1]{{\color{black} #1}}

\newcommand{\ours}{\texttt{\tool{}}}
%\newcommand{\ours}{\texttt{\tool{}}}


\usepackage{comment}
\usepackage{stfloats}
\usepackage{microtype}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2023}
% \acmYear{2023}
% \acmDOI{XXXXXXX.XXXXXXX}
%\acmSubmissionID{8938}

% \setcopyright{acmcopyright}
% \copyrightyear{2024}
% \acmYear{2024}
% \acmDOI{XXXXXXX.XXXXXXX}
% \acmSubmissionID{6627}


%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[CSCW'23]{Make sure to enter the correct
%  conference title from your rights confirmation email}{October 13--18, 2023}{Minneapolis, MN}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{1211}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\usepackage[autostyle,german=guillemets]{csquotes}
\makeatletter
%Take the original environment definition and change the leftmargin to 1cm
% \renewenvironment*{displayquote}
%   {\begingroup\setlength{\leftmargini}{1cm}\csq@getcargs{\csq@bdquote{}{}}}
%   {\csq@edquote\endgroup}
% \makeatother
%Hooks
%Use single spacing, set 10pt font, set italics, and beginning quotes
\renewcommand{\mkbegdispquote}
    {\selectfont\itshape}%\setquotestyle{quote}
%End displayquote environment with ending quotes
% \renewcommand{\mkenddispquote}{\textcoquote}

\usepackage{hyperref,quoting}
\usepackage{url}
\quotingsetup{vskip=0pt,font={itshape,raggedright},rightmargin=0pt}
\usepackage{tikz}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{gensymb}
\usepackage{float}
\usepackage{stfloats}
\usepackage{tikz}
\usepackage{xcolor}
%\usepackage{tcolorbox} 
% \newcommand{\redcircle}[1]{%
%     \begin{tikzpicture}[baseline=(char.base)]
%         \node[draw=red!80!black, fill=red!80!black, circle, inner sep=.6pt, text=white] (char) {\textbf{#1}};
%     \end{tikzpicture}%
% }
\definecolor{purple}{RGB}{153, 102, 255}
\newcommand{\purplesquare}[1]{%
    \begin{tikzpicture}[baseline=(char.base)]
        \node[draw=purple, fill=purple, rectangle, rounded corners=2pt, minimum height=10pt, minimum width=10pt, inner sep=0pt, text=white] (char) {\textbf{#1}};
    \end{tikzpicture}%
}
\definecolor{skyblue}{RGB}{25, 189, 255} 
\newcommand{\skybluesquare}[1]{%
    \begin{tikzpicture}[baseline=(char.base)]
        \node[draw=skyblue, fill=skyblue, rectangle, rounded corners=2pt, minimum height=10pt, minimum width=10pt, inner sep=0pt, text=white] (char) {\textbf{#1}};
    \end{tikzpicture}%
}
\definecolor{pink}{RGB}{220, 37, 255} 
\newcommand{\pinksquare}[1]{%
    \begin{tikzpicture}[baseline=(char.base)]
        \node[draw=pink, fill=pink, rectangle, rounded corners=2pt, minimum height=10pt, minimum width=10pt, inner sep=0pt, text=white] (char) {\textbf{#1}};
    \end{tikzpicture}%
}
\definecolor{darkgrey}{RGB}{85, 85, 85} 
\newcommand{\darkgreysquare}[1]{%
    \begin{tikzpicture}[baseline=(char.base)]
        \node[draw=darkgrey, fill=darkgrey, rectangle, rounded corners=2pt, minimum height=10pt, minimum width=10pt, inner sep=0pt, text=white] (char) {\textbf{#1}};
    \end{tikzpicture}%
}
%\newcommand{\UPDATEDdarkgreysquareNEW}[1]{\textbf{\textcolor{darkgrey}{\fbox{#1}}}}

\newcommand{\UPDATEDdarkgreysquareNEW}[1]{%
    \fcolorbox{darkgrey}{darkgrey}{\textcolor{white}{\textbf{#1}}}%
}
% \newcommand{\UPDATEDdarkgreysquareNEW}[1]{%
%     \fcolorbox{darkgrey}{darkgrey}{\makebox[0.5em][c]{\textcolor{white}{\textbf{#1}}}}%
% }

\newcommand{\tool}[1]{\texttt{VeriPlan}}
\makeatletter
\def\@ACM@copyright@check@cc{}
\makeatother
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by}
\acmConference[CHI '25]{CHI Conference on Human Factors in Computing Systems}{April 26-May 1, 2025}{Yokohama, Japan}
\acmBooktitle{CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan}\acmDOI{10.1145/3706598.3714113}
\acmISBN{979-8-4007-1394-1/25/04}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.

\title{\texttt{\tool{}}: Integrating Formal Verification and LLMs into End-User Planning}

%user-centered verification for llm-based planning tools

%integrating formal verification and LLMs into end-user planning tools

%verp
%\tool{}
%surePlan

%Integrating Formal Verification for LLM-Based Planning Tools

% user-centric verification for llm-based planning tools

%Applying formal verification to LLMs for enhanced user control
%Verifying LLMs Enhancing User Control in Complex Task Planning with Temporal Constraints
%VERL: Integrating Formal Verification and User Control for Effective LLM-Based Planning Tools

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Christine P Lee}
\orcid{0000-0003-0991-8072}
\affiliation{%
  \institution{Department of Computer Sciences University of Wisconsin--Madison}
  \country{Madison, Wisconsin, USA}
}
\email{cplee5@cs.wisc.edu}

\author{David Porfirio}
\orcid{0000-0001-5383-3266}
\affiliation{%
  \institution{U.S. Naval Research Laboratory}
  \country{Washington, DC, USA}
}
\email{david.j.porfirio2.civ@us.navy.mil}

\author{Xinyu Jessica Wang}
\orcid{0009-0002-5519-8432}
\affiliation{%
  \institution{Department of Computer Sciences University of Wisconsin--Madison}
  \country{Madison, Wisconsin, USA}
}
\email{xwang2775@wisc.edu}

\author{Kevin Zhao}
\orcid{0009-0008-6349-2862}
\affiliation{%
  \institution{Department of Computer Sciences University of Wisconsin--Madison}
  \country{Madison, Wisconsin, USA}
}
\email{kczhao@wisc.edu}

\author{Bilge Mutlu}
\orcid{0000-0002-9456-1495}
\affiliation{%
  \institution{Department of Computer Sciences University of Wisconsin--Madison}
  \country{Madison, Wisconsin, USA}
}
\email{bilge@cs.wisc.edu}
\renewcommand{\shortauthors}{}


\begin{abstract}
Automated planning is traditionally the domain of experts, utilized in fields like manufacturing and healthcare with the aid of expert planning tools. Recent advancements in LLMs have made planning more accessible to everyday users due to their potential to assist users with complex planning tasks. However, LLMs face several application challenges within end-user planning, including consistency, accuracy, and user trust issues. This paper introduces \texttt{\tool{}}, a system that applies formal verification techniques, specifically model checking, to enhance the reliability and flexibility of LLMs for end-user planning. In addition to the LLM planner, \texttt{\tool{}} includes three additional core features---a rule translator, flexibility sliders, and a model checker---that engage users in the verification process. Through a user study ($n=12$), we evaluate \texttt{\tool{}}, demonstrating improvements in the perceived quality, usability, and user satisfaction of LLMs. Our work shows the effective integration of formal verification and user-control features with LLMs for end-user planning tasks.

%Our results lead to a set of design implications for integrating formal verification techniques and user-control features for LLMs as user-centered planning tools for everyday tasks.



%Planning has traditionally been the domain of experts, with utilization in fields like manufacturing and healthcare and with the aid of expert planning tools. Recent advancements in artificial intelligence (AI), specifically large language models (LLMs), have made planning tools more accessible to everyday users. While LLMs have the potential to assist with complex planning tasks, they face several challenges, including issues with consistency, accuracy, and user trust. This paper introduces \texttt{\tool{}}, a system that applies formal verification techniques, specifically model checking, to enhance the reliability and flexibility of LLMs for end-user planning. \tool{} includes three core features---a rule translator, flexibility sliders, and a model checker---that engage users in the verification process. Through a user study ($n=12$), we evaluate the effectiveness of \texttt{\tool{}}, demonstrating improvements in the perceived performance of LLMs, usability, and user satisfaction. Finally, we present design implications for integrating formal verification techniques and user control features for LLMs as user-centered planning tools for everyday tasks.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
 % <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003124.10010870</concept_id>
       <concept_desc>Human-centered computing~Natural language interfaces</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003122.10010854</concept_id>
       <concept_desc>Human-centered computing~Usability testing</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003129</concept_id>
       <concept_desc>Human-centered computing~Interactive systems and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003122.10003334</concept_id>
       <concept_desc>Human-centered computing~User studies</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Natural language interfaces}
\ccsdesc[300]{Human-centered computing~Usability testing}
\ccsdesc[500]{Human-centered computing~Interactive systems and tools}
\ccsdesc[300]{Human-centered computing~User studies}



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{large-language models; verification; human-in-the-loop; human-centered AI}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

% \begin{teaserfigure}
% %     \includegraphics[width=\textwidth]{figures/teaser new.pdf}
% %    \vspace{-12pt}
% %   \caption{\textit{Outline of the MAP framework and \ours{} ---} In this paper, we draw from lifelong learning theories of the education domain to design a framework that supports AI systems in providing personalization to multiple users across three stages: \textit{reflection}, \textit{articulation}, and \textit{feedback}. We implement our framework using multiple LLM agents that operate within a hierarchical structure, employing retrieval-augmented generation (RAG) techniques.
% % }
%   \label{fig:teaser}
% \end{teaserfigure}
\begin{teaserfigure}
    \includegraphics[width=\textwidth]{figures/VERLteaser.pdf}
   \vspace{-12pt}
  \caption{\textit{\ours{} --- }In this work, we present \ours{}, a system that applies formal verification techniques to LLM outputs for end-user planning tasks. The figure compares a user's interaction with an LLM without \ours{} (left) and with \ours{} (right). In both cases, the user provides a prompt requesting a plan with specific requirements (depicted as step \protect\darkgreysquare{A}). The LLM generates an initial planning attempt from the prompt (step \protect\darkgreysquare{B}). With \ours{}, however, rules are automatically extracted from the user's prompt, verified, and refined through direct user involvement (step \protect\darkgreysquare{C}). These rules are then sent to the model checker, which verifies whether the LLM's output adheres to the user-defined rules (step \protect\darkgreysquare{D}). The validation result, along with any rule violations, is shared with both the user and the LLM to refine future planning outputs (step \protect\darkgreysquare{E}). 
  }
  \label{fig:teasor}
\end{teaserfigure}
\maketitle

   
%Reliability = quality of outputs; user experience = user control and transparency. Got it.

\section{Introduction}


%Complex planning and scheduling have traditionally been the domain of professionals operating in highly specialized environments. From staff scheduling in hospitals to resource management in manufacturing operations, these tasks are mission-critical, often requiring significant user expertise and access to advanced tools. Over the years, entire industries have been built around the development and refinement of scheduling and planning tools, with specialized software designed to handle the intricate web of variables and constraints that define these tasks. The precision and reliability required in these contexts underscore the importance of these tools, as they play a vital role in ensuring the smooth operation of complex systems.
%\item Planning: planning algorithms (in this case an \emph{automated} planning, or \emph{task} planning), search for a sequence of actions that will lead the system from the initial state to the goal state. (\cite{kress2018synthesis}) 

%With the advent of advanced artificial intelligence (AI) tools, particularly large language models (LLMs), there is a new opportunity to bring these complex planning and scheduling capabilities into the hands of everyday users. This represents a significant shift from professional expertise areas to everyday user areas, as novice end users have not traditionally engaged in such complex planning activities. Currently, in everyday novice user domains, day-to-day scheduling or task planning is a matter of doing their best by hand or with limited tools and often without considering the multitude of factors that professionals must manage. For example, in everyday tasks, users might utilize planning when needing to cook multiple recipes simultaneously in the most optimized order, when planning to pick up and drop off children at different locations and different schedules, or even when scheduling multiple meetings, events, and personal tasks throughout the week. During these tasks, it is not easy for users to consider the intertwined constraints or specifications that must be adhered to; one recipe might need a certain chilling time, one child might not like riding with another child on top of schedule constraints, or one might need to fit in a consecutive block of time in the midst of other tasks within each day dedicated to cleaning. The introduction of LLMs changes this dynamic, offering the potential for end users to apply sophisticated scheduling strategies to everyday tasks.


%This emergence of LLMs as tools for end-user planning raises new challenges and questions. Unlike the highly specialized tools used in industrial domains, LLMs are imperfect and not designed with the specific needs of end users in mind. As a result, there is a critical need to develop resources, interfaces, and support systems that enable users to effectively leverage these tools. A key aspect of this challenge is ensuring that users can trust and control the outputs generated by LLMs, which requires new approaches to user interaction and system design. In this context, the role of human-computer interaction (HCI) becomes paramount. HCI professionals must design tools and environments that support verification and control, ensuring that these AI-driven systems are not only accessible but also trustworthy and effective in the context of everyday life. This paper explores these challenges and opportunities, offering insights into how verification can be integrated into LLMs to better support end-user tasks, making these technologies more reliable and user-friendly.

%A promising approach to enhancing user engagement and control in planning with LLMs is the integration of verification mechanisms. \textit{Formal verification}, a method grounded in mathematical and logical principles to ensure that a system's behavior adheres to its predefined specificationsis particularly useful for ensuring that the timing and sequences of events within a system is correct. In particular, \textit{model checking} is a common formal verification technique that systematically explores the state space of a system to ensure that all temporal properties are upheld under various scenarios. In this work, by incorporating model checking into our LLM pipeline, we enable users to modify and evaluate temporal constraints within the planning process interactively. This approach is aimed to increase the reliability of LLM outputs but also support users by providing them with a deeper level of control and engagement. In this work, we develop and implement a verification pipeline that integrates LLMs with model checking, using a "temporal constraint template" as a foundation, and evaluate its effectiveness through a user study.

\textit{Automated planning}---the search for sequences of actions that guide an autonomous agent from an initial state to a goal state \cite{kress2018synthesis}---has traditionally been the domain of experts. Planning has been applied in professional settings, including production planning in manufacturing, medical resource planning in healthcare, project planning in construction, and route and fleet planning in transportation \cite{bourne2011recent,leonetti2016synthesis, lewis2020retrieval}. Automated planning is inherently complex, as the problem space involves managing numerous contingencies, constraints, and variables such as resource limitations, timing dependencies, and evolving preferences or changing conditions. Given the complexity and critical nature of these tasks, entire research communities and industries have dedicated themselves to building and utilizing planning tools (\textit{e.g.,} \cite{fox2003pddl2, nau2021gtpyhop, kapellosaiplan4eu}) that support foresight, decision-making, and the intricate coordination required for effective outcomes.


While these planning tools have traditionally been designed for expert use in professional settings, people increasingly need similar planning support in their everyday lives. People often manage multiple complex planning tasks in their everyday lives, such as coordinating pickup schedules for three children's school and after-school activities, hosting a family dinner party, preparing multiple meals simultaneously, and still setting aside time for personal tasks like writing a book and working out. Despite this need, they often lack effective tools to assist them, relying instead on manual methods or basic calendar apps. 
\revision{Traditional planning tools are often inaccessible for everyday users, as they require expertise in low-level planning languages, complex semantics, or detailed domain specifications for the task environment.}
Recent advances in artificial intelligence (AI), particularly large language models (LLMs), present an opportunity to bridge this gap. By understanding context, adapting flexibly, managing constraints, and automating decision-making, LLMs can make complex planning support more accessible and effective for everyday users.
%Traditional planning tools are often inaccessible for everyday users, as they require expertise in low-level planning languages (\eg Planning Domain Definition Language (PDDL)), complex semantics (\eg Linear Temporal Logic (LTL)) properties), or detailed domain specifications for the task environment.
%Recent advances in artificial intelligence (AI), particularly large language models (LLMs), offer the ability to understand context, enable flexible adaptation, manage constraints, and automate decision-making, enabling them to support users in managing these complex tasks.


%Recent advances in artificial intelligence (AI) have made planning more accessible to everyday users. People often manage complex planning tasks in their daily lives but lack effective tools to assist them. For example, a person might need to coordinate pickup schedules for three children's school and after-school activities, host a family dinner party, prepare multiple meals simultaneously, and still set aside time for personal tasks like writing a book and working out. Traditionally, people handled these planning tasks manually or with basic calendar tools. Recently developed AI capabilities, specifically large language models (LLMs), offer the ability to understand context, enable flexible adaptation, manage constraints, and automate decision-making, enabling them to support users in managing these complex tasks.

Despite their potential, end-users have yet to utilize LLMs effectively for such planning tasks. First, it is unclear whether LLMs can, out of the box, offer users solutions that adhere to user expectations, especially in highly constrained planning problems. Existing work has shown that, despite the increasing attention LLMs are receiving as planning tools, they are insufficient for planning and self-verification, particularly in the planning domain \cite{pallagani2024prospects, wang2023describe, huang2024understanding, valmeekam2023planning, kambhampati2024llms, valmeekam2022large}. Recent research has also highlighted several challenges, including difficulties with prompt input and navigation, limitations of text-only interfaces, and issues with evaluating LLMs' consistency and accuracy in meeting user needs \cite{tankelevitch2024metacognitive, subramonyam2024bridging, khurana2024and, sarkar2023exploring}. Finally, LLMs are prone to ``hallucinations''---coherent but incorrect information---that undermine user trust, usability, and satisfaction \cite{leiser2023chatgpt}. These technical limitations and user-centered barriers make it difficult for end-users to rely on LLMs for effective planning.

To address these challenges and enable the effective use of LLMs in end-user planning, LLM-based planning systems must not only be designed to be \textit{reliable}, but the user-LLM interaction must also be designed to support \textit{correction} when the system produces incorrect or unacceptable output---a core principle of human-centered AI \cite{amershi2019guidelines, lee2024rex}. To these ends, LLM-based planning systems must be designed to be (1) verifiable and (2) to keep the user in the loop during verification. Achieving these design principles necessitates combining interaction design with \textit{formal verification}, a set of techniques grounded in mathematical and logical principles to ensure that a system's behavior meets predefined specifications.

In this work, we apply formal verification to LLMs, in order to enable their use as effective end-user planning tools. Specifically, we leverage \textit{model checking}, a formal verification technique, to verify LLM outputs against user-defined constraints. Crucially, we explore how to involve users in the verification process and support user control and flexible adaptation to their needs. Based on this goal, we pose the following research questions: 
\begin{enumerate}
    \item How can formal verification methods, specifically model checking, be effectively applied to LLMs?
    \item How can we engage humans in the process of model checking to improve (1) the quality of outputs from LLMs and (2) the user's experience?
    \item At what stage of the model-checking process should users be engaged to maximize the effectiveness of integrating verification approaches with LLMs?
\end{enumerate}
To address these research questions, we present \tool{}, which integrates a formal verification-based approach to verifying plans generated by LLMs. \tool{} consists of three key features: a rule translator, flexibility sliders, and a model checker, which enables user control throughout the verification process. To evaluate our system, we conducted a user study that ablates different features to assess its effectiveness and impact on users. Our findings indicate that model checking improves the user experience with LLMs in planning tasks, particularly in terms of perceived output quality, user control, and transparency. Additionally, user control over constraint verification enforces rigidity in LLMs, while control over the strictness of constraints enables flexibility and creativity in planning. Finally, we offer design implications for integrating verification methods and user control features into LLM design to make them more useful and applicable for everyday planning tasks.
Our work makes the following contributions: 
\begin{enumerate}
    \item \textit{System contributions:} We present \tool{}, a verification-based approach involving the use of model checking against LLM outputs with multiple user control features. \tool{} includes three key features: rule translator, flexibility adjuster, and model checker. 
    \item \textit{Empirical contributions:} We evaluate \tool{} through a user study $(n=12)$ to understand its effectiveness and the specific contributions of its key features.
    \item \textit{Conceptual contributions:} We present a template-based approach to categorizing temporal constraints for verifying LLM outputs, instantiated and validated within a finite set of scenarios.
    \item \textit{Design implications:} Based on our findings, we present design insights on how to integrate formal verification techniques and user control into the design of LLMs for effective application for end-user planning.
\end{enumerate}

% Contributions are the following:
% \begin{enumerate}
%     \item Conceptual contribution: we take a template-based approach to categorize temporal constraints for verifying LLM outputs, instantiated and validated within a finite set of scenarios
%     \item Systems contribution: providing a \textbf{verification approach} for LLMs against temporal constraints: this toolkit includes templates of temporal constraints that are to be used to check LLM outputs, a model checker to check LLM outputs against these temporal constraints, and user-controllable features to enter and modify the constraints to influence the LLM's output. 
%     \item Empirical contribution: we show a pipeline of how this toolkit can be used in an actual LLM pipeline, for six temporal constraint scenarios driven from our template, including scheduling, recipe following, and planning with conflicting user preferences. We evaluate this system with users. 
    
% \end{enumerate}

\section{Related Works}
%\christine{Do we need a planning section RW?}
%- LLMs have challenges as they are increasingly used with general users and deployed in real-world applications
%- in response, research and industry has focused on making LLMs human-centered
%In this section, we review research on the challenges that users face in using LLMs and prior work that has aimed at addressing these challenges as well as provide background on methods in automated planning and model checking.

\revision{In this section, we provide background on automated planning for end-users and discuss the challenges they face when using LLMs. Next, we review existing verification approaches for LLMs, both broadly and within the context of automated planning. Finally, we provide background on model checking and its use in our verification approach.}

\subsection{Automated Planning for End-users}
%%% definition of automated planning
\textit{Automated planning} refers to automated techniques that decide \textit{what} an agent does, namely the steps that it takes to achieve a goal, rather than \textit{how} it performs each step \cite{ghallab2016automated}.
Numerous languages and libraries exist that enable users to interact with planning algorithms, such as the \textit{Planning Domain Definition Language} (PDDL) \cite{fox2003pddl2}, the \textit{GTPyhop} planner \cite{nau2021gtpyhop}, and the extensive \textit{Unified Planning} library \cite{kapellosaiplan4eu}, to name a few examples. Although planning tools are typically intended for expert users, recent work has engaged novice users in the planning process through visualization \cite{DePellegrin_Petrick_2024} and plan creation \cite{porfirio2024polaris}.
\revision{However, these planning tools pose significant challenges for end-users due to their reliance on complex formal languages and abstract logic formulas \cite{ebbinghaus1994mathematical, schoen2020authr, hurnaus2010programming}, which are difficult to learn and apply. The technical interfaces often lack intuitiveness, providing rigid workflows and low-level feedback  \cite{helmert2009concise, peer2004pddl, shah2013knowledge}. Moreover, users must invest significant effort in creating detailed system models, specifying states, transitions, and probabilities \cite{porfirio2018authoring, sauer2022structure, porfirio2020transforming}---tasks that demand technical expertise and are highly time-consuming. Designed with a focus on theoretical rigor and correctness, these tools often neglect practical usability, leaving them to fall short in addressing the dynamic and high-level goals of end-users.}

LLMs possess great potential to further increase the accessibility of automated planning for novice users.
Given a natural language prompt or set of prompts, LLMs are demonstrably capable planners \cite{silverLLM2024, songLLMPlan2023, lu2023plug} without requiring the user to directly interact with low-level planning languages or libraries. Still, LLMs are insufficient as standalone planners, requiring external support to verify and improve planning output \cite{pmlr-v235-kambhampati24a}.
To this end, \citet{gundawar2024robustplanningllmmoduloframework} contributes an \textit{LLM-Modulo Framework} that checks LLM-produced plans against a set of \textit{critics}, which provide feedback to the LLM to iterate. In our work, we envision the novice user as a critical component of the verification-feedback loop, akin to recent work in human-LLM interaction for text annotation tasks \cite{wang2024LLM}. For planning tasks, there is a research gap on designing systems to engage novice users in the verification-replanning process, which this work aims to address.

\subsection{End-user Challenges with LLMs}
\revision{
As LLMs are increasingly deployed in everyday applications and engage directly with end-users, they demonstrate great potential but also present significant human-centered challenges, particularly in terms of \textit{usability} and \textit{reliability}. 


\textit{Usability} remains a critical issue as users frequently struggle with crafting effective prompts and engaging with systems beyond the input stage. Studies highlight the difficulty users face in formulating prompts that elicit desired responses \cite{zamfirescu2023johnny, khurana2024and, tankelevitch2024metacognitive, subramonyam2024bridging, liu2024we}. Additionally, the cognitive demands placed on users---such as monitoring and deciding on strategies for prompting and interaction---exacerbate these challenges \cite{tankelevitch2024metacognitive, subramonyam2024bridging}. 
Another usability barrier is users' difficulty understanding how prompts influence outputs and building accurate mental models of the system's behavior and the reasoning behind it \cite{bhatt2021uncertainty, sun2022investigating, vasconcelos2023generation}. 
%User engagement is often confined to the input stage, making it difficult for users to influence outputs effectively. 
In response to these challenges, engaging users during the interaction process to steer the LLM's behavior, and support user's understanding of the reasoning has gained increasing attention. 
Strategies like co-creation, where users and AI collaboratively refine outputs, have been proposed to expand engagement and improve interaction intuitiveness \cite{schellaert2023your}. Similarly, interactive environments with user-controllable parameters enable experimentation, helping users build a better understanding of LLM capabilities \cite{louie2020novice, ma2024beyond, suh2023sensecape, jiang2023graphologue}. 
In addition, approaches like enhancing explainability and introducing customizable interaction options aim to reduce cognitive load and improve user experience \cite{tankelevitch2024metacognitive, teufelberger2024llm}. 
While engaging users and providing control to address usability challenges is a promising direction, further work is needed to understand \textit{how} and \textit{when} to involve users throughout the interaction process with LLMs. Such exploration can reveal ways to gather direct input and feedback that help LLMs accommodate evolving preferences and more effectively meet diverse user needs.



The \textit{reliability} of the output is another significant challenge. LLMs are prone to generating text that appears structurally coherent but contains factual inaccuracies or nonsensical information, a phenomenon known as hallucination \cite{rawte2023survey, bender2021dangers, ji2023survey, maynez2020faithfulness}. The lack of interpretability further complicates users' safe reliability, as users often struggle to understand the reasoning behind the output of the LLM \cite{mathews2019explainable, zhao2024explainability, yang2024harnessing, mirchandani2023large, liu2024we}. These issues are especially concerning in safety or mission-critical domains, such as healthcare or military applications, where reliance on incorrect outputs can have severe consequences \cite{koga2023exploring, lee2023benefits, sallam2023chatgpt}. These issues can further lead to risks of users over-relying on LLM-generated outputs without sufficient critical evaluation, underscoring the need for mechanisms that support users' safe and reliable interactions with LLMs \cite{ji2023survey, maynez2020faithfulness}. 
}

\subsection{Verification Approaches for LLMs}
\revision{
The advancements in LLMs have unlocked unprecedented capabilities in sense-making, language use, and interaction, enabling precise inference of user needs and applications across diverse domains \cite{kim2024understanding, minaee2024large, zhao2023survey}.
As these systems advance, ensuring their safety, reliability, trustworthiness, and alignment with user needs has become a pressing focus. To address this, a substantial body of work has emerged on verifying LLM outputs, which we broadly categorize into two directions.

The first direction focuses on enhancing user trust through explanations and interface design. Existing approaches generate explanations to support users in understanding and trusting LLM outputs \cite{li2022explanations, krishna2024post, huang2023can, marasovic2021few, wiegreffe2021reframing}. Others have explored designing interfaces and tools that help users deconstruct textual components, evaluate LLM outputs, and act upon them effectively \cite{ma2024beyond, suh2023sensecape, jiang2023graphologue}.

The second direction focuses on ensuring the validity of LLM outputs. One notable direction includes using LLMs for evaluation \cite{desmond2024evalullm, zhang2023wider, zheng2024judging} or orchestrating multi-agent systems to verify outputs \cite{mostajabdaveh2024optimization, liang2024improving, hassan2024llm, chan2023chateval}. These methods have been applied to complex tasks such as mathematical reasoning \cite{wu2024mathchat, zhang2025mathverse, li2023making}, semantic reasoning \cite{chen2023teaching, ni2023lever, liu2024speak}, and data annotation \cite{wang2024human}. Additionally, other approaches involve humans in evaluating and correcting outputs \cite{wang2024human, shankar2024validates}. Finally, a growing area of research incorporates constraint-based approaches, such as applying constraints to planning in robotics \cite{yang2024plug}, creating datasets with constraints for evaluation \cite{zhang2024cfbench}, or generating plans that adhere to multiple constraints \cite{xie2024travelplanner}. However, constraint-based approaches often utilize predefined datasets and can suffer from the lack of mechanisms for dynamically incorporating user preferences, needs, or evolving contexts.

Despite recent advancements, challenges persist in relying on LLMs for verification. Using LLMs to verify their own outputs risks critical flaws. Studies highlight their deficiencies in error detection, correction mechanisms, and adherence to constraints, as well as their tendency to hallucinate or retrieve inaccurate context \cite{ji2024testing, yao2023llm, liu2024exploring}. 
For instance, in the planning domain, despite extended context windows and few-shot learning, \citet{xie2024travelplanner} and \citet{chen2024can} demonstrate that LLMs struggle to generate plans and feedback for complex scenarios or adhere to predefined constraints. Similarly, \citet{valmeekam2023planning} reports that GPT-4 achieves an average success rate of 12\% in planning tasks, highlighting the inadequacy of LLMs in handling intricate requirements independently.
Other works have highlighted how utilizing LLMs for evaluation can suffer from bias based on the order, appearance, or length of the content, aspect-specific evaluation, scalability, and effectiveness in diverse contexts \cite{wang2023large, huang2024empirical, koo2023benchmarking, son2024llm, park2024offsetbias}. 
These limitations have led to heuristic and modular approaches as verification mechanisms to address such shortcomings \cite{kambhampati2024llms, valmeekam2022large}. Moreover, LLM reasoning and explanations, such as chain-of-thought reasoning, can be influenced by biased contexts, raising further caution about their reliability \cite{turpin2024language}. Consequently, developing methods to verify LLM outputs without relying on LLMs is critical to ensure validity, particularly for high-stakes, real-world applications.

}



%To support users in verifying LLM outputs, enhancing explainability and transparency in decision-making processes have been employed to improve users' ability to critically evaluate outputs and foster trust \cite{bhattacharjee2024towards}.

%Human-centered approaches that emphasize interpretability and adaptability have been proposed to address these risks, making LLMs more suitable for diverse populations and contexts \cite{schellaert2023your, sarkar2022like, xiao2024human}. Interactive interfaces that allow users to provide real-time feedback or adapt outputs dynamically based on their input represent a promising direction for aligning outputs with user expectations \cite{fakhoury2024llm, zhang2023visar}.



\subsection{Model Checking in Formal Verification and LTL constraints}\label{hello}

Model checking is a formal verification technique used to determine whether a software or hardware system satisfies requirements expressed in formal logic \cite{baier2008principles}. By systematically exploring all possible states that a system may encounter or produce, model checking exhaustively examines system behavior against these requirements, making it essential for proving the behavior of highly complex systems. Linear Temporal Logic (LTL) is a commonly used representation to express requirements, or \textit{properties}, in domains such as assistive robotics \cite{dixon2014fridge} and autonomous navigation \cite{liu2023grounding}. LTL allows users to specify and compose temporal constraints in the form of sequencing (\ie{} ``event A must occur before event B''), eventuality (\ie{} ``event C must eventually happen.''), and safety (\ie{} ``event D will never occur''), to name a few examples. This expressiveness makes LTL suitable for real-world tasks such as scheduling, safety protocols, and workflow management, where the timing and the order of actions are critical. %Building on LTL's representativeness in real-world scenarios, we focus in this work on applying model checking to time-based conditions using LTL properties.

\revision{In summary, our work builds on existing approaches to verify and validate LLM outputs, with a particular focus on constraint-based methods. We extend these methods by directly involving human engagement to define and refine constraints that align with users' needs and preferences. Our features for human engagement are designed to support varying levels of user control and involvement, for users to effectively guide the LLM's behavior. We leverage the significant potential of LLMs as end-user planning tools while addressing their shortcomings and user challenges through the implementation of an external verification approach using model checking, a formal verification technique.}


\input{figure_interface}

% \christine{\textbf{need translator backend figure}}
% \christine{\textbf{need model checker/LLM to PRISM figure}}
% \christine{\textbf{need mapping figure}}
\section{Technical Approach}\label{sec:tech}
This section introduces the technical approach of \ours, illustrating how it utilizes model checking on LLM outputs. We begin by outlining the three core features of \ours, followed by a detailed explanation of the technical approach for each feature, accompanied by an illustrative example. All LLM agents used in our implementation are powered by GPT-4 \cite{gpt-4}. Specific information on prompts used for LLM agents and the source code for our implementation can be found in the supplementary materials.\footnote{The supplementary materials can be found at \url{https://osf.io/va6d5/?view_only=8d74c81f765746908420e63479f6f36d}} 
%The source code for our implementation can be found in the anonymized repository. \footnote{The anonymized source code for our robot implementation can be found at \url{https://anonymous.4open.science/r/RAG-with-Verification-FBC4}}



\subsection{Patient Navigation Planning Scenario} \label{sec:scenario}

Throughout this section, we use the scenario of a user using an LLM to plan patient navigation for a counseling session while following conflict-prevention rules to illustrate how \ours{} assists with complex planning tasks.

\begin{quote}
    \textit{You (P1) are a family counselor preparing to hold a family therapy session. You are aware that certain family members have deeper conflicts with some more than others. You believe that a group session could be beneficial, allowing you to use established procedures to help heal family tensions. However, to avoid conflict before the group session begins, you decide to escort each member separately to the counseling room (L2) based on the severity of their conflicts. All family members are currently in the waiting room (L1) with you. Due to hospital safety protocols, all family members (P2, P3, P4) must be escorted by you, and only one person can be escorted at a time. However, because of ongoing tensions, P2 and P3 cannot be left alone together, and similarly, P3 and P4 cannot be left alone together.}
\end{quote}

Using this scenario, we demonstrate how \ours{} assists the user in iteratively solving the navigation planning task using its three features for model checking---rule translator, flexibility slider, and model checker---until a successful planning solution is reached.


\subsection{Features}
The verification approach implemented in \ours{} includes the following features: the (1) LLM planner; (2) rule translator; (3) flexibility slider; (4) model checker; and (5) refined LLM planner. %Below, we provide a brief summary of each feature, with detailed descriptions in Sections 3.2 to 3.4. 
\paragraph{LLM Planner}
The LLM receives the initial user input in the form of a natural language prompt, which includes the user's request, context, and constraints. Based on this input, the single-agent LLM will attempt to create a plan according to the provided prompt.

\paragraph{Rule Translator}
The rule translator converts the user's initial natural language input into formal language properties that are interpretable for the model checker to use during verification. 
%LTL properties and PRISM code for model checking. 
The translation is then translated back into natural language and presented to the user, who provides feedback to verify whether the translation is accurate.

\paragraph{Flexibility Slider}\label{hello}
Once the correctness of the rules is verified, the user can adjust the strictness of each rule using the flexibility sliders, defining the level of enforcement. This strictness determines the extent to which the model checker will insist on adhering to the rules during model checking. 

\paragraph{Model Checker}
\revision{\ours{} employs an external verification process, using a formal verification technique called model checking (see \S\ref{hello} for more). For model checking, we use an off-the-shelf probabilistic model checker, to systematically inspect every state within the system to confirm whether a set of behavioral properties are satisfied.} The model checker uses the user-defined constraints to evaluate the LLM planner's planning attempts, ensuring they align with the specified requirements. After completing the evaluation, the model checker provides feedback to the user and LLM on whether the plan is valid or which constraints are violated.

\paragraph{Refined LLM Planner}
Once feedback is provided, the LLM planner will iteratively regenerate a plan based on this feedback until it either reaches a valid solution or the maximum number of iterations specified in the program. At the end of the iterations, based on feedback from the model checker, the user can adjust the constraints using the rule translator or flexibility sliders before rerunning the LLM planner to reach a satisfying solution.

%In the following sections, we describe the technical approaches for our system. We guide this section with a leading example described in Section 4.1. We then describe the technical approach in the flow of the interaction for this example. The requirements at the beginning of the interaction are described in Section 4.2. The input of the system is described at Section 4.3, the planning and verification procedure throughout Section 3.3 to 3.5, and the feedback procedure and final output in Section 3.6 and 3.7, respectively. 

%\input{figure_interface}
\input{figure_llmPlanner}

%\input{figure_pipeline}
\subsection{LLM Planner}\label{sec:llmPlanner}
The front-end interface of \ours{} is shown in Figure \ref{fig:interface}.
In the example scenario, the user inputs their full planning requests and constraints through the input panel (depicted as step \skybluesquare{1}), and the request is reflected on the interface (step \skybluesquare{2}). 

\subsubsection{How It Works}
The beginning of the pipeline for \ours{}, including the LLM planner, is presented in Figure \ref{fig:LLMplanner}.
At the start of the interaction, as the user inputs their prompt (step \purplesquare{a}), an LLM agent generates an initial plan based on the user's request (step \darkgreysquare{f}). This plan is then later to be checked by the model checker, using the constraints defined by the rule translator and the flexible slider features discussed below.

\input{figure_pipeline}
\subsection{Rule Translator}\label{sec:ruleTranslator}

The role of the rule translator is to extract constraints from the user's prompt that a correct plan must follow.
%In the backend, it first uses an LLM agent to extract and translate them into LTL properties and PRISM format, and retranslates them into natural language for the user to verify and confirm. 
\revision{The rule translator presents the extracted results to the user, allowing them to review the extracted constraints and either confirm them or request regeneration. For confirmation, the user selects the correct version of the constraint using the check box (step \skybluesquare{3}).} If the presented constraints are unsatisfactory, the user can ask \revision{the rule translator} to regenerate translations for the constraints using the input panel.


\subsubsection{How It Works}
The pipeline of the rule translator is shown in Figure \ref{fig:pipeline}.
Receiving the user prompt (step \purplesquare{a}) which includes the user's planning request and desired constraints, \revision{an LLM-based mapping agent extracts content from the prompt and maps it to the appropriate categories in the} \textit{temporal constraint template} described below (step \purplesquare{b}). The mapping agent is bound to select from the seven categories and has been prompt-engineered with examples for mapping accuracy.
\input{figureTable}
\paragraph{Template of Temporal Constraints}
To ensure that the rule translator can accurately convert user input into rules for model checking, it uses a predefined temporal constraint template. 
For the model checker to function, the rules must be specified in LTL logic. However, since users input rules in natural language, manually translating them into LTL formulas is challenging. \revision{Unlike fixed algorithms that require rigid input formats, LLMs can interpret and categorize variable natural language inputs into temporal categories by understanding context and intent, guided by examples from prompt engineering. This adaptability allows complex or unconventional rules to be mapped to predefined LTL constraint templates, reducing the need for extensive manual refinement in rule translation.}

%and relying on LLMs for translation may lead to inconsistencies or inaccuracies. 
To address this, we developed a template of LTL properties which are fed into an LLM for translation, covering six temporal categories: (1) fixed time blocks, (2) sequential order, (3) concurrent events, (4) conditional constraints, (5) exclusive constraints, and (6) global constraints. Each category includes a template for converting natural language into LTL properties, which are fed into the LLM. 
\revision{In the constraint template, LTL provides \textit{modal operators} to formalize such statements. The global operator, $G$, specifies conditions that must hold in every state. The future operator, $F$, checks for events that must occur at some point in the future. The until operator, $U$, specifies that an event $\phi$ must remain true until another specified event $\psi$ occurs, and that $\psi$ must indeed happen.}
The detailed templates are provided in \revision{Table \ref{fig:template}}.
%and relying on LLMs that have not undergone fine-tuning or specific prompting for translation may lead to inconsistencies or inaccuracies.
%\subsubsection{How it works}

%The pipeline of the rule translator is shown in Figure \ref{fig:pipeline}.
%At the start of the interaction, the user inputs their prompt (depicted as step \purplesquare{a}), which includes the user's planning request and desired constraints. An LLM-based mapping agent then extracts the rules from the prompt and maps them to the appropriate categories from the temporal constraint template (step \purplesquare{b}). The mapping agent is bound to select from the five categories and has been prompt-engineered with examples for mapping accuracy.


Once the mapping is complete, it is sent to the LTL translator (step \purplesquare{c}). The LLM-based LTL translator uses the template to convert the mapped outputs into LTL properties, guided by prompt engineering to determine the appropriate conditionals for each constraint. The translator then generates an LTL formula for the constraint.

%Once the mapping is complete, it is sent to the LTL translator (step \purplesquare{c}). The LTL translator uses the template to convert the mapped outputs into LTL properties, based on prompt engineering which notifies the LTL translator which conditionals are appropriate for each scenario. The LTL translator then outputs a generalized formula for the scenario. 
%The LTL translator also employs a prompting strategy to guide the Language Model (LLM) in creating appropriate variables and translating natural language into Linear Temporal Logic (LTL). This strategy involves instructing the LLM on how to correctly generate variables that represent various states and conditions. For instance, when translating a statement like "I write every weekday," the LTL translator prompts the creation of variables such as \textit{writing\_active}, where a value of $1$ indicates writing is occurring and $0$ indicates it is not. Similarly, it introduces a \textit{global\_day} variable, where values $0$ through $6$ represent days from Monday to Sunday, respectively.

These LTL translations are then sent to the LLM-based PRISM translator, for converting the LTL properties into an interpretable format for the model checker (step \purplesquare{d}). Our verification approach utilizes the PRISM Model Checker \cite{kwiatkowska2011prism} (discussed in detail in \S\ref{sec:modelChecker}) to format LTL properties, which requires that properties be expressed in the PRISM language. \revisionNew{While an algorithmic approach could perform this translation, an LLM was chosen for its seamless integration and demonstrated feasibility during system design.} Our PRISM translator utilizes manual examples for prompt engineering to convert LTL expressions into the PRISM format, covering state representations, rule violations, and temporal logic translations.



%. These cover basic state representations, rule violations, and temporal logic translations, strategically designed to cover all possible combinations of initial and final states for multi-agent movement rules. 
%The PRSIM code are designed to be interpretable and verifiable by our model checker, serving as input for subsequent verification processes.
%It utilizes around 30 manual examples for prompt engineering, ensuring accurate conversion of LTL expressions into the PRISM format while adhering strictly to 'and' and 'or' logic. These examples encompass a comprehensive range of scenarios, including basic state representations, rule violations, and complex temporal logic translations. The examples are strategically designed to cover all possible combinations of initial and final states for multi-agent movement rules. The PRSIM code are designed to be interpretable and verifiable by our model checker, serving as input for subsequent verification processes.

These two sets of translations are then sent to the user for final verification and confirmation of each constraint. Before being presented to the user, each translation is converted back into natural language by the PRISM and LTL translator for user readability (step \purplesquare{e}).
\revision{The translated rules are presented to the user in natural language for review. The user can verify their correctness and make adjustments if needed. If a rule aligns with the user's expectations and goals, the user can confirm it by marking the checkbox next to it; otherwise, they can provide feedback to regenerate the rule using the rule translator. Only the rules with marked checkboxes will be included in the final set. Once all desired rules are confirmed, the user finalizes the process by selecting the `submit' button.}
%The user then reviews these translations and selects the rule that best aligns with their expectations and goals. 
Based on user input, the final set of rules to be used for model checking is finalized along with the corresponding LTL properties and PRISM code. The final set of constraints is then passed to the flexibility sliders for strictness adjustment. 

%The LTL translator is prompt-engineered with both manual examples and datasets of natural language to LTL translations, while the PRISM translator is prompt-engineered using manual examples. Examples and datasets used for prompt engineering can be found in the supplementary materials. 


%\subsubsection{Illustrating Rule Translator}

%The frontend demonstration of \ours{} with the example scenario is shown in Figure \ref{fig:interface}. In this scenario, after the user inputs their full planning requests and constraints through the input panel (depicted as steps \skybluesquare{1} and \skybluesquare{2}), the Rule Translator extracts the constraints, translates them into LTL properties and PRISM format, and retranslates them into natural language for the user to verify and confirm. At this stage, the user selects the correct version of the rule if applicable (step \skybluesquare{3}). If the presented rules are unsatisfactory, the user can ask the LLM to regenerate translations for the constraints using the input panel.


%and these constraints are mapped to the ``Exclusive Constraint'' category from the temporal constraint template.  the following constraints: (1) all four people must be at L2 by the end of the solution; (2) P2 and P3 cannot be at the same location without P1; (3) P3 and P4 cannot be at the same location without P1; (4) no one can travel alone except P1; (5) only two people can travel at once between locations. These constraints are mapped to the 'exclusive constraint' category from the temporal constraint template. The LTL translator then uses this template to convert the natural language into LTL properties, as shown in Table 3. 

%Once translated into LTL in two versions, the outputs are further translated into PRISM format for the model checker and back into natural language to be presented to the user as a checklist for verification (see dark purple box 1 in Figure 2). When the user selects the correct rule for verification, the final set of rules for model checking is confirmed. These rules are then passed to the next feature, the flexibility sliders (see dark purple box 2 in Figure 2).



\subsection{Flexibility Sliders}
As shown in Figure \ref{fig:interface}, once users have verified the correctness of the constraints, they can specify the strictness of each constraint using the flexibility sliders (step \skybluesquare{4}). In the given example, the user initially believes that all the rules should be treated as hard constraints, as they pertain to hospital protocols and are crucial for avoiding conflicts among patients. Consequently, they set the sliders to 100\% for each rule and submitted the adjustments. After the first few attempts fail, the user decides to set the strictness of rule four to 50\%, reasoning that P1 might be able to travel with both P2 and P4. Throughout the interaction, users can freely modify the strictness of individual rules after reviewing the outputs from the LLM and model checker. Once the strictness levels are finalized, the complete set of constraints, verified and customized by the user, is sent to the model checker.

\subsubsection{How It Works}
Constraints that are verified by the user from the rule translator are then sent to the flexibility sliders. These sliders allow users to adjust the strictness of each rule, where strictness defines how rigidly the model checker will enforce the rule. Strictness includes both ``soft'' and ``hard'' constraints: hard constraints \textit{must} be satisfied for a plan to be valid, and any plan that violates a hard constraint is immediately rejected. Soft constraints, while preferred, are not strictly necessary and their violation does not invalidate the plan. \revision{If a soft constraint is violated, unlike hard constraints, the plan will not be immediately rejected. Instead, the plan with the violated soft constraint will be marked as valid, and the user will be notified of the violation.}
Constraints are then weighted based on hardness, and \ours{} samples from the weighted constraints, with lower-weighted constraints (corresponding to ``softer'' constraints) being less likely to be sampled. The model checker then checks the plan against the sampled constraints.
%After the user defines the strictness of each rule, we use random sampling based on the probabilities set by the user, to select the final set of rules for the model checker to enforce.

%\subsubsection{Illustrating Flexibility Sliders}




\input{figure_modelChecker}
\subsection{Model Checker}\label{sec:modelChecker}
Once the correctness and strictness of the rules are defined by the user, the model checker uses these rules to check the initial plan generated by the LLM agent. In the interface, the user can view the initial planning attempt generated by the LLM (step \skybluesquare{5}). The model checker then performs model checking on this plan, comparing each state against the specified constraints. Based on the verification, the model checker provides feedback, which includes a list of broken rules or confirmation of the plan's validity (step \skybluesquare{6}). This feedback is then sent to the user to explain the system's status and to the LLM for regenerating the plan based on the feedback. 

\subsubsection{How It Works}
The pipeline of the model checker is shown in Figure \ref{fig:modelChecker}. 
Similar to LTL translation, the initial plan generated by the LLM agent based on the user's request (depicted as step \darkgreysquare{f}) is also translated into the PRISM language format for the model checker to process (step \purplesquare{g}). In this work, we use the PRISM Model Checker \cite{kwiatkowska2011prism} and Stormpy for verification. Stormpy is a Python API for Storm \cite{hensel2022probabilistic} that enables model checking and property verification within a Python environment. At this point, since the model checker has (a) a set of LTL-expressed rules, and (b) the LLM-generated plan expressed in the PRISM language, it evaluates the plan against these rules (step \pinksquare{h}). During verification, the model checker examines each state of the plan for rule violations. Any rule violations will result in an invalid plan. The validity of the plan, along with any rules that were violated are sent to both the user and the LLM agents to refine their future solutions (step \pinksquare{i}).

%In our system, the model checker operates through the following workflow: (1) It receives the plan and rules translated into PRISM format. (2) Using Stormpy, it constructs the corresponding probabilistic model. (3) The rules are transformed into temporal logic formulas. (4) The model is traversed, checking each state for compliance with the rule requirements. (5) If states violating the rules are discovered, relevant information is recorded. (6) Finally, a verification report is generated, encompassing the plan's validity and any rules that were violated. This approach enables systematic verification of LLM-generated plans, ensuring their adherence to predefined rules and constraints.


\subsection{Refined LLM Planner}
Receiving the feedback from the model checker, the process of the LLM regenerating a plan and the model checker verifying it against the user-defined rules is iterated two additional times, allowing for a total of three iterations, as defined by the system parameters.
Between iterations, the user can adjust the strictness of the constraints to explore different planning solutions (step \skybluesquare{4}). 
Once all iterations are complete, the user can choose to inquire about aspects such as the constraints, the decision-making procedure, the logic of the model checker, or the system status through the input panel (step \skybluesquare{1}). Additionally, the user can modify the constraints using the flexibility sliders (step \skybluesquare{4}), or modify the constraints through the rule translator through the input panel before initiating a new interaction (step \skybluesquare{1}).

\subsubsection{How It Works}
Upon receiving feedback from the model checker, this information is provided as updated requirements to the LLM, which is then asked to regenerate a plan. The regenerated plan is checked by the model checker for rule violations using the user-defined rules. If no violations are found and a correct plan is generated, the interaction ends. If a correct plan is not generated by the end of the iterations, the system prompts the user to adjust the constraints or their strictness for additional iterations.
%\subsubsection{Illustrating Model Checkers}




%\subsection{After Model Checking}
%If an optimal plan is generated, then the model checker conveys the plan to the user and the interaction is complete. However, if a sub-optimal plan is generated, the model checker provides feedback to the LLM agent and the user based on which constraint was violated. Then, the LLM agent will regenerate a new plan, the model checker will check it, and output the violated properties of the new plan. This iterative process is done until the LLM agent generates a plan where all constraints are adhered to. If no satisfactory solution is generated after a user-defined amount of tries, the system stops the verification procedure, and presents all sub-optimal plans it had previously generated.

%Our system applies formal verification techniques and includes features for user feedback for planning tasks involving temporal constraints with LLMs. Our system includes a model checker, that checks the LLM's generated plan against specified constraints. includes two features for user input: verifying extracted constraints, and specificying the strictness of the constraints. 



%automatically extracts constraints from user prompts, verifies LLMs outputs against these constraints using a formal verification technique called model checking, and enables users to adjust the strictness of such constraints for better alignment to user needs. 

% \subsection{Preparation for Model Checking}
% \subsubsection{User Input: Prompt and Constraints}
% In the system's interface, the user writes a prompt containing both a planning goal and constraints that the goal's solution must adhere to, as shown in the example in Section 4.2. Users input their planning requests and constraints in natural language as a prompt. 

% \subsubsection{Mapping to Template of Temporal Constraints}
% In the example, there are a number of constraints that must be adhered to, including: (1) at one time, only one family member can be escorted with P1; (2) P1, P2, P3, and P4 must arrive at L2; (3) P2 and P3 cannot be left alone together; and (4) P3 and P4 cannot be left alone together.
% To successfully transform natural language constraints into properties that can be used for model checking, our system holds a template of temporal constraints. We developed this template to map constraints written in natural language into properties for model checking. The template contains five temporal constraints in LTL format, derived colloquially, including (1) fixed time blocks; (2) sequential order; (3) repetitive patterns; (4) conditional constraints; and (5) exclusive constraints. Each constraint contains a template that the translator can use to transform natural language into linear temporal logic (LTL) properties. The detailed template of temporal constraints can be found in Table 3. 



% An LLM agent is assigned with the task of mapping the natural language constraints to the most appropriate type of temporal constraint in the template. The LLM agent is bound to select from one of the ten types, and is prompt-engineered with mapping examples for accuracy and consistent mapping. 
% In the example, the constraints would be mapped to the "exclusive constraints" of the template. 

% %understand the mapping. 
% %With enough details from user inputs, the LLM agent is accurate and consistent in its mapping.

% %\subsubsection{Verifying Constraints}
% %\paragraph{Mapping natural language to LTL constraint}

% \subsubsection{Translating Mapping to Model Checking Properties}
% Once the natural language constraint is mapped to a template type, two LLM-based agents are involved in translating the natural language constraints: the LTL translator and the PRISM translator. The LTL translator maps the natural language constraints to the most relevant LTL properties from the temporal constraint template. Taking the LTL properties, the PRISM translator translates these properties into PRISM format, to be able to be used by the PRISM model checker. 

% When the LTL translator generates an output for translation and the PRISM translator generates the corresponding PRISM format as well, the LTL and PRISM translations are then given as input to a final LLM agent to be translated back into natural language. This natural language is used for the translated constraints to be shown to the users once again, to obtain feedback for correctness. During the translation process, two translation attempts are made for each rule. The two are then presented back to the user, where the user can select the translation that accurately reflects the inputted rule. 

% Thus in the example, the four constraints will be translated as:
% LTL: 
% PRISM:

% Each translator is trained with prompt engineering techniques that involve inputting vast examples of translation. 

% %The two translators collaborate together to map the user's input constraints to its corresponding PRISM code format, and outputs the matched constraints back to the interface as checkbox. This allows the user to select the correct PRISM-code-formatted temporal constraint from the interface, avoiding errors produced by the language model and ensuring a complete and accurate translation of constraints. %Once the translation is complete, these constraints are sent to the model checker to be used against the LLM's output for generating plans. 

% \subsubsection{Verifying the Translations}

% As a result of the translation process, two options for translation for each constraint will appear on the interface as a checklist. At this point, the users are involved to verify and select which constraints are correct and adhere to their intended rules, and if not, users re-input their constraints. Once the user selects the correct constraints, these constraints are sent to the model checker to be used against the LLM's output for generating plans. 
% Figure 4 shows the selected constraints for the example scenario.

% \subsubsection{Setting the strictness of the constraints}
% Once the constraints are verified by the user, the system once again invites users to set the strictness of the constraints. The strictness involves the "softness" and "hardness" of the constraints, meaning how strictly the constraint will be enforced by the model checker. The user can specify the strictness using the sliders in the interface for each rule. Once specified, the system uses random sampling for the model checker to enforce the constraint. 
% Figure 5 shows the strictness of the constraints being adjusted in the example scenario. 

% %\subsection{Model Checking LLM Outputs}

% \subsubsection{LLM generated plan}
% As the user inputs the request, an independent LLM agent generates a plan to fulfill the request. 
% This LLM agent is a single agent using OpenAI GPT-4. The plan generated by this LLM agent will be checked against the model checker, to make sure the plan is adhering to all user-defined specifications. 

% The plan generated by the LLM agent in the example scenario is the following: 

% \subsubsection{Model checking against LLM plan}
% Using the preparation specified in Section 4.3, the model checker will verify the planning output generated by the LLM. 
% We use a PRISM model checker that uses Stormpy.  
% The PRISM model checker will compare each state against the constraints. 

% Another LLM agent parses the planning goal given by the user and translates the goal into PRISM code. This PRISM code is combined with the PRISM constraints, and both sections are shipped to the model checker to verify if the plan is satisfactory given the constraints.

% \subsubsection{Iterative Feedback to LLM and User}
% If an optimal plan is generated, then it conveys the plan to the user and the interaction is complete. However, if a sub-optimal plan is generated, the model checker provides feedback to the LLM agent based on which constraint was violated. Then, the LLM agent will regenerate a new plan, the model checker will check it, and output the violated properties of the new plan. This iterative process is done until the LLM agent generates a plan where all constraints are adhered to. If no satisfactory solution is generated after a user-defined amount of tries, the system stops the verification procedure, and presents all sub-optimal plans it had previously generated.

%\subsection{Planner Interface}
%The planning interface is used in tandem with the Input Interface to complete the system's user interface. It entails the users verifying the constraints, the system verifying the integrity of the planning goal through model checking, and the users modifying the constraints. 


%\subsubsection{Generating Plans}
%Another LLM agent parses the planning goal given by the user and translates the goal into PRISM code. This PRISM code is combined with the PRISM constraints, and both sections are shipped to the model checker to verify if the plan is satisfactory given the constraints.

%\subsubsection{Model Checking}
%If an optimal plan is generated, then it conveys the plan to the user and the interaction is complete. However, if a sub-optimal plan is generated, the model checker provides feedback to the LLM agent based on which constraint was violated. Then, the LLM agent will regenerate a new plan, the model checker will check it, and output the violated properties of the new plan. This iterative process is done until the LLM agent generates a plan where all constraints are adhered to. If no satisfactory solution is generated after a user-defined amount of tries, the system stops the verification procedure, and presents all sub-optimal plans it had previously generated.

%\subsubsection{Modifying Constraints Between Iterations}
%When a sub-optimal plan is generated, users can give feedback to the LLM by adjusting the strictness of each constraint. In the interface, each constraint has a slider which entails how strict the constraint should be adhered to by the model checker. Values close to 100\% mean it is a "hard" constraint which should be enforced when generating a plan. On the other hand, values close to 0\% indicate "softer" constraints, which are less likely to be considered by the LLM agent. Internally, the system uses random sampling with the slider values to determine which constraints should be adhered to. Once the constraints are adjusted, the planner will start generating plans aligning to the selected constraints as described in Section 3.4.

%\subsubsection{Final generated plan}
%The final generated plan is the plan that has been validated by the model checker against the defined constraints is $100\%$ accurate. If the generated plan violates any of the established constraints, the system will notify the user of the specific constraint that has been broken. Conversely, if the generated plan adheres to all the constraints without any violations, the system will output a message stating, "The attempt is valid." This process ensures that the final generated plan is accurate and reliable. The transparency of this approach allows any potential errors produced by the LLM to be identified and corrected, thereby ensuring that the LLM's output (the final generated plan) is fully reliable and achieving $100\%$ confidence in the generated plans.

%\input{figure_modelChecker}
\section{User Study}
\subsection{Scenarios}\label{sec:scenarios}
We design three scenarios that incorporate the temporal constraints illustrated in Table \ref{fig:template}. One of these scenarios is the ``patient navigation in hospital'' example discussed in \S\ref{sec:scenario}. Below, we describe the remaining two scenarios.

\paragraph{Optimizing Cooking Procedures}
The user is hosting a dinner party on Wednesday at 6:00 PM with multiple guests, requiring the preparation of various dishes to accommodate different dietary preferences, such as vegetarian and gluten-free. The user plans to make spaghetti and meatballs as the main dish and cheesecake for dessert, with meat, vegetarian, and gluten-free versions of each. The user must plan how to cook these dishes simultaneously, ensuring they are ready on time while optimizing the cooking process.

\paragraph{Scheduling Multiple Events}
The user is trying to schedule multiple events for the week. These include three hour writing blocks for her book, a dinner party on Wednesday at 6:00 PM, meetings with colleagues on Tuesdays and Wednesdays, tennis lessons on Fridays at 3:00 PM, child pickup and playtime, household chores, and personal routines (\textit{e.g.,} listening to music while writing or having coffee in the morning). Every Sunday evening, she creates a weekly plan to organize and fit all these tasks into her schedule.

\subsection{Study Design}
This study aimed to understand the importance and impact of \ours{}'s verification approach and user control features, specifically evaluating how these elements influenced user reliance, usability, satisfaction, and the perceived performance of LLM outputs. 
We conducted an ablation study using a within-subjects design, where different ablation conditions served as the within-subjects variable.
%At the start of the study, participants were introduced to various planning scenarios before interacting with the system. We briefly discussed with them the complexity of these scenarios and the tools they would typically use to solve such tasks. 
%After this initial discussion, participants began interacting with \ours. 
%\revision{For each scenario, participants engaged in all four conditions, and the order of the conditions were randomized.} 
In Condition 1, participants engaged with \ours, which included the rule translator, flexibility sliders, and model checker. Condition 2 removed the flexibility slider, leaving only the rule translator and model checker. Condition 3 removed the rule translator, including only the flexibility slider and model checker. In Condition 4, all three features, including the rule translator, flexibility slider, and model checker, were removed as neither the rule translator nor the flexibility sliders can function without the model checker. For consistency, we denote these conditions with \textit{C1 (Full)}, \textit{C2 ($\neg$Slider)}, \textit{C3 ($\neg$Translator)}, \textit{C4 (None)} in the remainder of the paper.
%In this condition, all features were removed as the rule translator and flexibility slider cannot function without the model checker. 
%In each condition, participants interacted with a single LLM agent to generate initial plans powered by GPT-4 \cite{gpt-4}. For consistency, we denote these conditions with \textit{C1 (Full)}, \textit{C2 ($\neg$Slider)}, \textit{C3 ($\neg$Translator)}, \textit{C4 (None)} in the remainder of the paper.
\revision{During the study, participants were randomly assigned to two of the three scenarios. In each scenario, participants engaged in all four conditions in a randomized order. After each condition, participants completed the quantitative scales. At the end of their interaction with each scenario, semi-structured interviews were conducted.} The entire study lasted 1.5 hours. Questionnaires used during the study can be found in the supplementary materials.\footnote{The supplementary materials can be found at \url{https://osf.io/va6d5/?view_only=8d74c81f765746908420e63479f6f36d}} 
%: hospital patient navigation and scheduling multiple events. They were also encouraged to explore the optimizing cooking procedure scenario if time permitted. \revision{Participants engaged only within the given scenarios.}
%Two participants (P8, P12) had additional time and engaged with a third scenario, for which we did not collect quantitative data. 


\subsection{Measures}
To evaluate the participants' experiences with the system, we employed the Usefulness, Satisfaction, and Ease (USE) scale \cite{article} to measure three key dimensions: usefulness (Cronbach's $\alpha = 0.94$), ease of use (Cronbach's $\alpha = 0.83$), and satisfaction (Cronbach's $\alpha = 0.95$). We also used the performance questionnaire from the fairness, accountability, transparency, and explainability (FATE) scale developed by \citet{shin2021effects} to measure participants' perceived quality of the LLM's output (Cronbach's $\alpha = 0.91$). Both scales were placed on a seven-point Likert scale.
%After each condition, we conducted post-task semistructured interviews to further assess user experience.


\subsection{Participants}
12 participants were recruited for our user study. Participants were required to be in the United States, fluent in English, and at least 18 years old. All participants were recruited through university mailing lists. 
\revision{While our sample size is not large, the within-subjects study design achieves an acceptable level of statistical power for significant results \cite{bellemare2014statistical}.} 
%\revision{The within-subjects study design with our sample size provided sufficient data for significant results and in-depth qualitative insights.}
Participants age ranged from 19--48 ($M = 25$, $SD = 7.9$). 50\% of the participants identified as female and 50\% as male. 50\% of our participants were White, 41.6\% were Asian, and 8.4\% were American Indian or Alaska Native. After the study, participants were compensated \$15.00 per hour. We refer to participants as P1--P12, using the notation P\textit{i} to indicate participants, where \textit{i} indicates participant ID number.
\revision{In the recruitment survey, we also collected participants' experiences with LLMs, asking them to select a category that best described their familiarity: ``not familiar or none,'' ``occasional use,'' or ``regular use.'' Five participants (P7--P11) selected ``not familiar or none,'' four (P1, P4, P6, P12) selected ``occasional use,'' and three (P2, P3, P5) selected ``regular use.'' Those who reported occasional or regular use mentioned using LLMs for tasks such as brainstorming, search engines, writing assistance, and planning tools (\eg scheduling assistance, task management, project coordination, and itinerary planning.)}

\subsection{Analysis}

%For the quantitative data, we first used factorial repeated-measures analysis of variance (ANOVA) and Tukey analysis to examine all pairwise comparisons. Tukey analysis, or Tukey's HSD test, identifies significant mean differences between multiple groups post-ANOVA, controlling errors in pairwise comparisons, and accommodating data variability. 
%\revision{Additionally, we conducted a Dunnett test to evaluate the effects of the experimental groups (C2, C3, C4) relative to the control group (C1) in our ablation study. Dunnett's test compares the means of several experimental conditions to the mean of a single control condition. The test was performed with an alpha level of 0.05.}
\revision{For the quantitative data, we conducted a Dunnett test to compare the means of the ablation groups (C2, C3, C4) to the mean of the full system (C1). Dunnett's test compares the mean of several experimental conditions to a control condition, in which for our study, the full \ours{} system (C1) is considered to be the control. The test was performed with an alpha level of 0.05.}

%To further elucidate the value of the full system when compared to each individual component, we conducted a Dunnett's test as a supplement to our Tukey analysis. This Dunnett's test provides greater statistical power in comparing the means of the ablation groups (C2, C3, C4) to the mean of the full system. The test was performed with a significance level of 0.05. Similar to the results from our Tukey analysis, the Dunnet test revealed that the full system (C1) significantly outperformed the \textit{C3 ($\neg$Translator)} $(p=.0011)$ and \textit{C4 (None)} $(p=.0013)$ conditions; was significantly more useful than the \textit{C2 ($\neg$Slider)} $(p=.047)$, \textit{C3 ($\neg$Translator)} $(p=.009)$, and \textit{C4 (None)} $(p=.0257)$ conditions; and was significantly more satisfying than the \textit{C3 ($\neg$Translator)} $(p=.007)$ and \textit{C4 (None)} $(p=.0101)$ conditions. 


%given our focus on evaluating the effects of specific conditions relative to the control group (Condition 1) from the ablation study, we performed a Dunnett test to compare each group directly to the control group. Dunnett's test compares the means of several experimental conditions to the mean of a single control condition.
%A Dunnett test was conducted to compare each group to the control group (C1), with an alpha level of 0.05.

%Additionally, our setup in the ablation study, condition 1 (C1) was set as the control group, with Conditions 2, 3, and 4 (C2, C3, C4) as experimental groups. To evaluate the effects of the experimental groups relative to the control, we conducted a Dunnett test, which compares the means of multiple groups to a single control. The test was performed with an alpha level of 0.05.

For qualitative data, we conducted a Thematic Analysis (TA) on the interviews. The coding of the responses was conducted by deriving representative themes from transcriptions~\cite{clarke2014thematic, McDonald19}. During open coding, the first author coded for significant concepts in the data. Concepts were then categorized into clusters, further being grouped into themes. These themes were iteratively discussed between the whole research team, recategorizing the groups and revising the themes upon disagreement until a consensus was reached. %Interview questions can be found in the supplementary materials.

\input{figure_quant}
\section{Results}
Our analysis aimed to understand the impact of our verification-based approach and its features on the effectiveness of and user experience with LLMs in planning tasks. The results of our quantitative data are shown in Figure~\ref{fig:userStudyQuant}. 
\revision{Overall, the Dunnett test revealed that the full system (C1) significantly outperformed the \textit{C3 ($\neg$Translator)} $(p=.0011)$ and \textit{C4 (None)} $(p=.0013)$ conditions; was significantly more useful than the \textit{C2 ($\neg$Slider)} $(p=.047)$, \textit{C3 ($\neg$Translator)} $(p=.009)$, and \textit{C4 (None)} $(p=.0257)$ conditions; and was significantly more satisfying than the \textit{C3 ($\neg$Translator)} $(p=.007)$ and \textit{C4 (None)} $(p=.0101)$ conditions.} 


%Overall, \textit{C1 (Full)} and \textit{C3 ($\neg$Translator)} had a significant effect on the participants's perceived performance of the LLM, $F(3, 33)=5.69, p=.003$, usefulness, $F(3, 33)=4.38, p=.0106$, and satisfaction, $F(3, 33)=6.56, p=.0013$. There were no other statistically significant results observed in \textit{C2 ($\neg$Slider)} or in the measure of ease of use. 

%The Tukey test showed that \textit{C1 (Full)} and \textit{C3 ($\neg$Translator)} had a significant effect on the participants' perceived performance of the LLM, $F(3, 33)=5.69, p=.003$, usefulness, $F(3, 33)=4.38, p=.0106$, and satisfaction, $F(3, 33)=6.56, p=.0013$. There were no other statistically significant results observed in \textit{C2 ($\neg$Slider)} or in the measure of ease of use. 

%The Dunnett test revealed that \textit{C3 ($\neg$Translator)} $(p=.0011)$ and \textit{C4 (None)} $(p=.0013)$ had significantly lower mean scores than the control group \textit{C1 (Full)} in the participants' perceived performance, while \textit{C2 ($\neg$Slider)} did not differ significantly. In participants' usefulness, \textit{C2 ($\neg$Slider)} $(p=.047)$, \textit{C3 ($\neg$Translator)} $(p=.009)$, and \textit{C4 (None)} $(p=.0257)$ had significantly lower mean scores than the control group \textit{C1 (Full)}. The measure of ease of use did not show any significant differences. Finally, \textit{C3 ($\neg$Translator)} $(p=.007)$ and \textit{C4 (None)} $(p=.0101)$ had significantly lower mean scores than the control group \textit{C1 (Full)} in the participants' satisfaction, while \textit{C2 ($\neg$Slider)} did not show significant differences. The results of our Dunnett test data are shown in Figure~\ref{fig:userStudyQuant}.

%\textit{C1 (Full)} \textit{C2 ($\neg$Slider)}, \textit{C3 ($\neg$Translator)}, \textit{C4 (None)} in the remainder of the paper.


Below, we present our findings in \revision{four} key themes that emerged in our analysis. For the first three themes, we present quantitative findings first, followed by qualitative insights that reveal differences in use patterns and user perceptions across conditions, providing a deeper understanding of our system's impact.
\revision{For the fourth theme, we present findings derived from the qualitative analysis, focusing on participants' interaction experiences with \ours{}.}

\subsection{Rule Translator Improved Performance, Satisfaction, and Usefulness of LLMs}
Participants' scores in \textit{C3 ($\neg$Translator)} were significantly lower than those in \textit{C1 (Full)} in measures of performance, usefulness, and satisfaction. Our qualitative analysis provides further insight into these results.

\subsubsection{Verification Enabled Users to Control Rigidity}
All twelve participants noted that the ability to verify translations and adjust rules provided a sense of user control. This control allowed them to set deterministic boundaries, creating a level of rigidity within which the LLM could operate when generating plans. This rigidity ensured that the LLM's outputs were aligned with their personalized, user-defined constraints, matching their needs and goals. As P10 explained: \textit{``I know LLMs work probabilistically, so having these rigid boundaries felt like it was getting better accuracy. And because I defined those boundaries, they were useful to me. I liked that I was giving the algorithm more defined limits to create plans that fit me.''}


Seven participants (P2, P5--P8, P10, P11) emphasized that being involved in the verification process was core to ensuring the validity and correctness of the user-specified rules, which directly improved the system's usability and reliability. In contrast, when they were unable to participate in defining the rules, users were less confident that the system accurately reflected their goals or needs, leading to reduced trust in the output. As P6 noted: \textit{``I liked being involved in planning the output, knowing these are my rules, not the computer's. It helps with the validity of what's being spit out. Especially in personal situations, where I want more input opportunities to ensure the system doesn't misinterpret what I want.''}



%P3: Yeah, I think the verification process helps you use AI as a tool. Because it will tell you what the broken constraints are, and then like you still like, you intuitively know, like, what's best, or like, how to adjust it, and things like that, instead of replacing you entirely. I think that's nice, because I still want control over my stuff.

%P11: No, I did like that there were multiple options for each rule because then I was able to, like, drill down on what exactly I was wanting the system to do, instead of just being like, yep, we do these four things.


In Condition 3 where the rule translator was ablated, five participants (P3, P7, P9, P11, P12) decided to stop interacting after at most two attempts because the system failed to correctly translate their prompts into rules, and adjustments were unsuccessful. As P3 explained: \textit{``Because I know the constraints are wrong, I don't want to do anything after this. So then every solution it generates, I'm gonna have to double-check anyway, so it's probably more efficient for me to just use my brain to generate my own solution. I can't verify that the constraints are 100\% correct, so I can't trust the material it produces. It's not very helpful.''}

\subsubsection{Verification Helped Align Expectations and Refine Prompts}

By being involved in the validation of the rules, eight participants (P1, P2, P3, P5, P7--P9, P11) found this procedure helpful in ensuring that the LLM's rules and inputs aligned with their expectations and goals. Through the translation and verification process, participants aimed to understand how the system interpreted their prompts, minimizing gaps or misunderstandings during translation. As one participant explained:
\textit{P8: ``I think it [using the rule translator] was more like fact-checking for reliance and trust, ensuring that it [LLM] is listening to what I'm saying and will actually give me a plan that adheres to my facts.''}

Five participants (P1, P3, P4, P7, P12) also noted that the verification process was effective in mitigating the impact of messy or unorganized prompts. They described their prompts as often being a text dump from their minds, sometimes lacking key details. The system's translation and presentation of prompts as rules helped participants organize their thoughts and check for completeness of including their needs. They described that this process reduced the mental load of creating careful and precise prompts in the initial interactions. As P4 explained, \textit{``The verification process gave me freedom from being so constrained or stressed about making my prompts detailed. I don't always put in a prompt the same way, so it was like guidance, checking to ensure I put in good prompts.''}

\subsubsection{The Need for Contextual Memory for Iterative Rule Verification}

Three participants (P3, P9, P12) suggested improving the rule verification process by enabling the translator to remember past contexts or interactions. Currently, \tool{} only supports single interactions, translating a list of rules based on the initial prompt and resetting the translation process with each new interaction. As a result, if participants wanted to adjust a rule, they had to re-enter their set of instructions with every prompt. They felt that allowing the translator to remember previous rules and iteratively build upon them would be more efficient and helpful in adjusting the rules on the fly and giving feedback to improve individual rules. 

\subsection{Flexibility Sliders Showed Potential to Improve Usefulness of LLMs}
Participants' scores in the \textit{C2 ($\neg$Sliders)} \revision{were significantly lower in usefulness,} and lower in perceived performance, ease of use, and satisfaction compared to \textit{C1 (Full)}. 
%Out of the measures, a larger gap was observed in the usefulness measure than in others. While there were no significant differences, users' scores in \textit{C2 ($\neg$Sliders)} were similar to that of \textit{C1 (Full)} in measures of satisfaction, usefulness, and performance. 
While there were no significant differences, the gap in participants' scores for the usefulness measure between \textit{C2 ($\neg$Sliders)} and \textit{C1 (Full)} was larger than in other measures. Our qualitative analysis presents further insights into these results.

\subsubsection{Sliders Were Found Essential for Flexibility in Adaptive Scenarios}
Seven participants (P1, P2, P4, P5, P8, P9, P11) found the sliders particularly useful and necessary in scenarios requiring greater personalization and flexibility in the rules, such as complex scheduling and event planning. They noted that contextual factors, preferences, and priorities often evolve based on user needs, making it essential to consider these variables during planning. In such cases, participants saw the sliders as crucial for managing the complexity of the rules and arriving at a workable solution. However, for tasks such as hospital navigation or recipe planning, which involved stricter rules, participants felt that the sliders were less relevant. Although they appreciated the flexibility sliders provided, they viewed them as secondary to the verification of the translator or model checker. P8 elaborated on this point by \textit{``I think it was just this scenario [hospital navigation], where it felt like these are pretty hard rules. But when you're talking about scheduling, or even personal life, like who gets the kids on what days, there's a lot of flexibility in that, and it would be a lot easier to make decisions. That fits better with life. Like for certain days, you're going to have harder deadlines.''}

%P2: Um, because I feel like some rules, they contradict itself, like more so if I, like, soften them like a little bit, maybe they can, like, kind of like both of them can be a little bit wrong.


\subsubsection{Users Leveraged Sliders to Improve LLM Adaptability and Accuracy}

Participants primarily described two key benefits of the sliders: enabling flexibility in creating plans and helping them understand the capabilities of the system. First, participants highlighted that the sliders allowed for flexibility by adjusting rules based on priorities, preferences, and trade-offs. Ten participants (P1--P6, P8--P11) agreed that the sliders helped them reflect their specific needs, such as safety concerns, reasonable compromises, and personal preferences. One user described this usefulness as \textit{P3: ``There are factors that maybe the AI might not understand---like, for me, family is really important. The previous version without the sliders seemed to prioritize work over family.''}
Some participants (P2, P6, P7, P11) felt that the sliders allowed them to effectively communicate their priorities to the system, conveying their nuances and preferences. One user, P11, described their use of the sliders with the following example, \textit{P11: ``I set rules one and two at 100\%, extremely strict, because I wanted to prioritize patient safety. For rule three, I set it at 70\%---which was about no one traveling without me. I was trying to negotiate, rather than having a binary choice, like in real-life decisions.''}


%P10: It's, you know, I think, would then give the algorithm more options to create plans for you with flexibility and things that are rigid. It would then give it, you know, fewer options, but then I think, more, I guess, what's, I guess, creativity for coming up with a plan, you know, that is functional for difficult constraints, right?


%Six users also used the sliders to improve system capabilities. Users used the sliders to emphasize the rules that the LLM was not correctly focusing on or struggling to adhere to, to emphasize the priority or focus on the rule for it to be adhered when generating the solution. By emphasizing the rule that the LLM was struggling to adhere to, users aimed to improve the accuracy and performance of LLMs generating the plan.

Additionally, six participants (P1, P3, P4, P6--P8) used the sliders to improve the system's performance by emphasizing the strictness of the rules that the LLM overlooked or was struggling to follow. By increasing the strictness of these rules and relaxing them for well-adhered rules, they aimed to enhance the LLM's output accuracy by directing the system's attention more appropriately. One participant described this intent as \textit{P5: ``The constraints really helped me understand what to emphasize more, based on what the system struggled with. It made me focus on what I wanted the system to prioritize for its own performance when regenerating a plan.''}

%Additionally, users also used the sliders to experiment the LLM's capabilities. They aimed to understand how the LLM would react to different strictness specifications and options of different combinations of specifications, to understand the system logic and capabilities of responding to the constraint specifications. Testing through the capabilities helped users understand capabilities of the system and rules that were difficult, leading them to adapt their strictness and negotiate for the satisfying and optimal plan. 

%P12: The sliders allowed for a certain level of, like, re experimentation, right? It gives me multiple options, and from that I can see what the system can do.

\subsubsection{Ambiguity in the Impact of Constraint Strictness}

Four participants (P2, P5, P7, P11) noted that the impact of the slider's strictness was unclear. They were unsure how the specific percentage affected the output or how the system's logic changed based on their specification of strictness. This lack of clarity made it difficult for participants to determine how much to adjust the sliders to reach their planning goal. As a result, some participants felt that it was a trial-and-error process when determining the appropriate level of strictness, forcing them to guess the impact of their choices.
One participant described this ambiguity as \textit{P2: ``Even though I noticed that they have different impacts, and I can try different combinations, it feels a bit up in the air. I don't exactly know what percentage leads to different outcomes, so I wasn't sure how much to change.''}


\subsection{Model Checker Improved Performance, Usefulness, and Satisfaction of LLMs}

Participants' scores in \textit{C1 (Full)} were significantly higher in performance and satisfaction compared to \textit{C3 ($\neg$Translator)} and \textit{C4 (none)}. \revision{In usefulness, \textit{C1 (Full)} was significantly higher than \textit{C2 ($\neg$Slider)}, \textit{C3 ($\neg$Translator)}, and \textit{C4 (none)}.} However, no significant difference was observed in ease of use. Our qualitative findings provide further insights into these results.


\subsubsection{Model Checking for Efficiency and Transparency} 

Nine participants (P2, P3, P5, P6, P8--P12) highlighted that the model checker significantly improved their efficiency by reducing planning time for complex tasks and supporting a constructive trial-and-error process to reach satisfying solutions. They noted that specifying user-centered needs through verified rules and adjusting constraints, followed by the model checker assessing the quality of outputs, greatly improved problem-solving for planning tasks. One participant compared their experience to that of not having model checking, stating, \textit{P10: ``If it hadn't asked to verify things, it would've resulted in more failures, increasing re-do times. The ability to set rules and goals, and then optimize in as few iterations as possible, helped me achieve goals feasibly that would have taken much longer otherwise, you know, figuring out how to bend this way and that.''}

In addition, six participants (P1--P3, P5, P6, P10) emphasized the role of transparency in driving efficiency. The feedback provided by the model checker on system status and errors, along with the input panel for navigating further questions, was particularly useful. One participant described using the input panel for clarification, describing, \textit{P6: ``The input panel was especially useful when I felt like the model checker was assuming something, so then I could ask questions about why it acted that way, and then adjust the rules. So it wasn't that big of a problem.''}


In \textit{C4 (None)} where participants interacted solely with an LLM agent, they reported difficulties in efficiently achieving a correct solution. Seven participants (P2, P4--P6, P10--P12) noted that the LLM often provided ``a'' answer instead of ``the'' answer that best aligned with their needs. They struggled with enforcing rules, as the system did not always capture the specified requirements, leading to inefficiency or failure in achieving the correct outcome. One participant described their experience without the model checker being \textit{P2: ``It felt like it [LLM] was just putting out an answer as fast as possible. I felt more like it was producing an answer, versus trying its best to produce a better answer with its honest, real best effort.''}

Eight participants (P1, P2, P4, P5, P7, P8, P10, P12) also faced challenges with monitoring errors and manually verifying outputs, which added a significant burden, explaining \textit{P10: ``Sometimes speed isn't everything, because all it really did was produce the wrong answer faster. Producing the wrong answer faster just made it more inefficient. I had to keep correcting its logic over and over, which made the process very inefficient.''}
This process raised concerns regarding over-reliance and blindly accepting incorrect results, as another user stated \textit{P4: ``If I wasn't paying as much attention, I would have been possibly just accepted its answer, because I assume that it's an intelligent machine or something like that.''}
Finally, transparency of the system was also an issue as participants struggled to understand the system's logic and rule inputs, leading to skepticism and decreased trust in the system's outputs. One participant described their challenges in navigating the system's decision-making process, \textit{P8: ``When I reviewed the plan, I could quickly see it had missed something, but without knowing exactly what it considered, it was harder to trust. It made me more skeptical about whether it was accounting for all of my priorities.''}





\subsubsection{Feedback from the Model Checker Enabled Creativity in Action Planning}

Eight participants (P1, P3--P5, P8--P10, P12) highlighted that multiple planning attempts, facilitated by rule verification and iterations, allowed them to creatively generate plans that adhered to their predefined rules. Users described how the model checker enabled them to experiment with different levels of constraint strictness and rule adaptations, while ensuring safety through verification. The model checker acted as a safety net, allowing users to be exploratory and creative, which they found helpful in identifying optimal plans. As P10 noted, \textit{``The algorithm now has more options to create plans for me, because it has that determined list of rules that it will be checked against. This allows me, or the LLM, to have more options or creativity for coming up with a plan that is functional for difficult constraints.''}

In addition, participants found that the feedback provided by the model checker on broken rules helped them to gain insight for their next steps, even after unsuccessful attempts. Seven participants (P2--P4, P6, P9, P11, P12) explained how this feedback revealed details that the system missed, highlighted errors, and demonstrated how well the system was interpreting their rules. This understanding enabled them to refine their prompts and adjust constraints to guide the system toward generating the correct solution. Five participants (P1--P3, P5, P12) further emphasized how the feedback revealed alternative solutions they had not previously considered, which helped them compromise, prioritize rules, and understand trade-offs.
P5 illustrated this experience, stating, \textit{``I was able to bounce ideas from its [plan] suggestions and get creative. Like, I never thought you could bring people back to the waiting room and then into the counseling room. That was something I hadn't considered, but it became part of the solution. I learned something from it and added something to my own ideas.''}

%Furthermore, three users described that the input panel provided additional support for users to navigate the next action plans. 

%P5: so kind of the constraints gave you the starting point, and chatbot gave you, like, the more specific directions on how to navigate the problem.

% \subsubsection{Transparency of system status improving user confidence and reliability}

% Four users described how the involvement the feedback led to (i.e., adjusting the rules or strictness of the features, re-evaluating their priorities), led to the procedure feeling more incremental and constructive, making them feel confidence that the system trying to plan and move towards the user's goal. 

% P8: Yeah, I I like the workflow of it, where it goes through. You know, the rules. Do we have these right? Here are your constraints. This is our suggestion. And then, kind of like fact checking, checking itself. 

% Even when the system presented incorrect plans, users described the model checker provided resilience through the input panel.

% P10: So yeah, it was a little bit of rule, assuming it seemed that way, okay, and the decisions is made, but then I guess, with a bit of just coaching through it, so wasn't that big of a problem. Being able to ask the system questions was definitely really great, especially since it then remembered those answers to those questions.

\subsubsection{Users Wanted Actionable Suggestions from the Model Checker}
Ten participants (P1--P5, P7, P9--P12) emphasized the importance of the system providing actionable suggestions alongside its planning attempts, which they suggested might enhance usability and efficiency. These suggestions could include adjustments to constraints (\textit{e.g.,} \textit{P12: ``Loosen the time constraint slightly to make the recipe more manageable''}) or guidance on how to rephrase rules to improve model checker comprehension. Users also envisioned the system offering multiple options for resolving issues, allowing them to select the most appropriate adjustment.

One participant suggested that the system highlight broken rules and provide potential fixes. P2 noted, \textit{``What if the system showed you broken rules and said, `Here's how you can fix it,' offering hypotheses for the changes you could try?''} This sentiment was echoed by another participant, who emphasized the importance of prioritization in such suggestions. P3 explained, \textit{``It could inform you about the constraints that are broken and suggest which ones you can adjust, but also warn you about those that are too critical to change, like if someone's safety is at risk.''}

Furthermore, several participants (P3, P5, P6, P9) expressed a desire for the system to take into account the strictness of constraints when offering action plans. P3 elaborated, \textit{``The system could suggest changes to constraints, and I could review the options---maybe rule four isn't that important, so I could go with a solution that adjusts it.''}


\revision{
\subsection{\ours{} Interface Supported Usability}
Our qualitative analysis shows that the design of the front-end interface supported users' interactions with \ours{} in terms of understanding the planning context, applying feedback to user control features, and organizing plans based on user preferences. 

\subsubsection{Understanding Planning Content}
Six participants (P2--P4, P6, P7, P11) highlighted that the \textit{P6: ``mind map layout''} of \ours{} helped them better understand the LLM's reasoning and functionality. \ours{}'s layout organizes key information---such as rules, inputs, outputs, and conflicts---into blocks connected within a visual map. Participants found this compartmentalized structure significantly more intuitive for interpreting planning content compared to conventional text-based LLM interactions.

They explained that text-based interfaces often present a ``wall of text,'' making it difficult to quickly or efficiently extract information about the system's reasoning or conflicts, thereby hindering the system's transparency for user understanding. As one participant noted,
\textit{P3: ``This structure makes it much clearer to see what the plan was, where the conflict happened, and why it occurred. It's all laid out logically, so I can address it right away. With ChatGPT, I'd have to sift through a wall of text and ask multiple follow-up questions just to figure out what went wrong, which takes a lot more effort.''}
%comparison for performance


\subsubsection{Applying Feedback to User Control Features}
The mind map-based layout was also described to support participants in effectively applying the model checker's feedback. Four participants (P2, P7, P9, P10) described how having all components---the rule translator, flexibility sliders, user's input, planning output, and conflicts from the model checker---in one view and interconnected made it intuitive to apply modifications while monitoring feedback and conflicts in the output. As P9 explained: \textit{``Getting the feedback, I could tweak a slider or update a rule and immediately see how it shifted the output---like adjusting dials on a machine and watching it respond.''}
Three participants (P1, P2, P7) also emphasized that \ours{}'s structure displaying multiple planning iterations in one view helped them track their rule modifications, compare the impact of different rules and adjustments on the LLM's performance, and make their modifications incrementally.% P7 explained: \textit{``Being able to track these changes for each plan meant I could compare everything at a glance and make changes incrementally, based on how I want to kind of play with it.''}




\subsubsection{Organizing Plans Based on User Preferences}
During interactions with \ours{}, multiple participants (P2, P5, P10, P11) used the mind map structure to organize iterations or plans based on their preferences or perceived efficiency. For example, one participant (P5) engaged with the weekly scheduling scenario and described preferring meetings in the morning. They grouped plans with morning meetings into a ``preferred'' category, separating them from plans that scheduled meetings later in the day. They also created a ``less favored'' group for plans where meetings followed their workout sessions, as they disliked feeling sweaty or tired during meetings.
Similarly, another participant (P11), working with the cooking optimization scenario, used the mind map to prioritize plans they found more efficient. For instance, they preferred plans that consolidated ingredient preparation at the beginning rather than doing it separately for each dish, describing as 
\textit{P11: ``That doesn't seem that efficient to me. I like to use my cutting board once and then clean it up. So I prioritized plans like that.''}
Participants described the ability to categorize plans was helpful in selecting or ranking their preferred options and gaining insights into creating the most optimal plan for their needs.
}


\section{Discussion}
\revision{
In this work, we present \ours, which applies formal verification, specifically model checking, to LLMs for complex end-user planning tasks. \ours{} includes three core features---the rule translator, flexibility sliders, and a model checker---and engages users throughout the verification process. Our user study demonstrates that \ours{} enhances users' perceived performance of the LLM, as well as its usefulness, satisfaction, and reliability. Below, we discuss how our findings address the research questions and present design implications for integrating verification processes and user control features into future systems.

%\subsection{Design Implications}

\subsection{Formal Verification for Deterministic Boundaries in LLMs}
%RQ1. How Can Formal Verification Methods, Specifically Model Checking, Be Effectively Applied to LLMs?

LLMs have made automated planning more accessible to end-users by removing many of the barriers traditionally associated with planning tools. Existing tools often require users to understand complex formal languages, interpret low-level feedback, build detailed system models, and work within rigid workflows. These challenges are compounded by scalability issues, language barriers, and misalignment with end-user objectives, making them less adaptable to practical, real-world contexts. While LLMs address these accessibility issues for automated planning, their probabilistic nature introduces new risks, including unpredictability in their outputs. This inherent variability can lead to errors and failures, posing notable challenges for ensuring reliability and user confidence \cite{kambhampati2024llms}.

In this work, we aim to combine the strengths of both approaches: LLMs to enhance accessibility of planning tools to end-users, and formal verification methods to ensure safety, reliability, and trustworthiness. Quantitative findings show that LLMs incorporating verification approaches (C1) significantly improves users' perceived performance, usefulness, and satisfaction compared to those without (C4). 
Qualitative insights further illustrate how the model checker, guided by user-defined constraints, effectively aligns the LLM planner's outputs with user needs and goals.
Participants described the model checker as a ``problem solver'' that identified conflicts on their behalf and helped propose solutions, allowing them to achieve their goals more efficiently and reducing concerns about undetected errors. Moreover, participants referred to the model checker as a ``safety net,'' particularly valuable when experimenting with exploratory inputs or modifications related to the rules. By employing an external verification process on plans using user-defined constraints, the model checker alleviates the cognitive burden of manually reviewing constraints and comparing them with the generated outputs, while fostering increased reliability in interactions with the LLM.


These results suggest that formal verification, particularly model checking, can provide deterministic boundaries for the inherently probabilistic nature of LLM systems. By systematically exploring all possible states of a system, model checkers verify whether logical properties are satisfied and, if not, identify violations. This capability allows model checkers to act as external guardrails for LLMs, detecting errors caused by inaccuracies, hallucinations, or misaligned outputs. As the complexity of planning states and constraints increases, such verification becomes essential to ensure the reliable use of LLMs as planning tools.

Looking ahead, the integration of formal verification processes can play an increasingly critical role as LLMs and AI systems are increasingly used for planning contexts \cite{xie2024travelplanner, song2023llm,yao2022react}. As LLMs become increasingly used as tools for beyond planning contexts (\eg personalization \cite{shang2024personalized,ning2024user}), verification methods can enable users to safely and effectively guide, collaborate with, and customize these systems to meet their specific needs. By providing a deterministic mechanism for error detection, formal verification methods can help LLMs adapt to practical, real-world applications while maintaining safety and reliability.



\subsection{User Control with Model Checking for Improved LLM Outputs and User Experience}
%RQ2. How Can We Engage Humans in the Process of Model Checking to Improve (1) the Quality of Outputs From LLMs and (2) the User's Experience?
\revisionNew{As described above, model checking can be particularly beneficial for LLMs by imposing deterministic boundaries on their probabilistic nature. However, for model checking to effectively support LLMs in achieving personalization, it must acquire user-specific preferences, constraints, and needs. Traditionally, involving users in this specification process has been an arduous task requiring domain-specific expertise. However, with LLMs enabling natural language interactions---such as the translators in \ours{} that convert LTL properties into natural language---users can engage at a higher level without needing prior knowledge of model checking properties or complex programming language concepts. Instead, they can define and refine their specifications in a user-centered, understandable manner.

Thus, combining model checking and LLMs creates a symbiotic relationship: model checking enables formal verification for LLMs, while LLMs lower the technical barrier for users to engage in model checking. This relationship further establishes an environment where users can actively contribute at different points in the decision-making process. Rather than being passive recipients of AI-generated outputs, users can assume an active role in specifying their needs, preferences, and constraints, thereby guiding and refining LLM outputs in an adaptive manner.

The user's role of driving high-level control throughout the system's decision-making process is critical for enhancing both the quality of LLM outputs and the overall user experience. The importance of granting users agency to shape system behavior is well recognized; recent work in LLMs has increasingly focused on enabling human control, such as segmenting queries into sub-tasks for users to specify personal contexts and preferences \cite{ma2024beyond} or adapting outputs based on user feedback \cite{kirk2023past}. However, our insights from \ours{} emphasize that involving users more directly in the system's decision-making process---particularly in the stages of defining system parameters and behavioral factors---can enable more effective and efficient personalization. Compared to involvement solely at the output level, this approach ensures that user needs are clearly defined from the outset, making adaptation more direct and refinement more targeted within a narrowed space.

To fully leverage the benefits of combining LLMs and model checking, system designs should integrate high-level user control at multiple stages of the decision-making process. Future system designs can take inspiration from \ours{}'s approach, which demonstrated effective strategies for high-level user control:

\paragraph{User-defined specifications and iterative refinement} Users defined constraints and preferences for model checking in natural language and refined them iteratively until the rules aligned with their expectations. This early engagement before the system's final output showed effectiveness in reflecting user needs and fostering trust. Rather than limiting personalization to post-hoc feedback on system outputs, which can lead to abstract inference of user preferences, this approach embeds user input in the system's foundation, enabling direct and meaningful refinement.

\paragraph{Flexible endorsement through high-level adjustments} Users interacted with the slider bars to balance constraints based on their priorities. Interestingly, while users did not specify precise numerical values, they intuitively assigned abstract priority levels, which most considered sufficient for preference specification. This suggests that effective user control may not require fine-grained precision but rather a structured way to articulate high-level preferences.

\paragraph{Seamless interaction through intuitive interfaces} The system interface played a crucial role in streamlining user engagement. By abstracting complex model checking and planning processes, it provided a structured yet intuitive workflow, allowing users to easily understand, categorize, and refine their constraints and plans. Ensuring that the planning and verification procedure feels as seamless and accessible as interacting with LLMs is essential to maintaining a positive user experience and lowering the barrier to adoption for end-user planning.


By designing systems that integrate high-level user control throughout different stages of the decision-making process, model checking specifications can be more accurately tailored to user needs, leading to more effective personalization and improved alignment between LLM outputs and user expectations.

}
% ---------------------
% In our work, users interacted with two key features of the system: the rule translator and the flexibility slider. This section explores how user engagement with features like these during model checking can enhance the quality of LLM outputs and improve the overall user experience.

% \paragraph{Rule Translator}
% %The rule translator extracts constraints from user input for planning and allows users to verify and adjust them. Quantitative findings show that the rule translator can improve the perceived performance and user experience with the LLM, as its removal resulted in significantly lower performance, usefulness, and satisfaction scores compared to the full \ours{} system. Qualitative findings further reveal that this improvement stems from enhanced reliability in the LLM's outputs, driven by user engagement in defining and validating constraints.

% well what do you call this, specification of evolving conditions, and personal preferences and constraints?
% Our findings suggest that engaging users through features like the rule translator during model checking can enhance their perception of system performance. By actively involving users in the rule translation process, they can specify constraints tailored to their unique preferences and expectations. This direct specification helps define and validate rules aligned with their needs, reducing the perceived unpredictability often associated with LLM outputs. Furthermore, such engagement fosters outputs that feel more predictable and controllable, as research shows that these characteristics enhance users' trust and confidence \cite{daronnat2021inferring,shneiderman2020human}. By enabling users to directly define and manage constraints through features like the rule translator, systems can instill greater confidence in adherence to user-defined rules, ultimately improving users' overall perception of LLM performance.

% In addition to improving the perceived system performance, the integration of features like the rule translator for user engagement can also enhance user experience.
% By allowing natural language interaction and intuitive components such as check boxes for validation, the rule translator eliminates the steep learning curve typically associated with low-level planning languages used in formal verification. Additionally, natural language-based interactions for inputting, reviewing, refining, and ensuring the completeness of rules promote system transparency. Presenting rules in an interpretable manner helps address common challenges with LLMs, such as misunderstandings, misaligned expectations, incorrect assumptions, and ambiguous prompts \cite{zamfirescu2023johnny, tankelevitch2024metacognitive, vasconcelos2023generation, maynez2020faithfulness, zhao2024explainability}. Such transparent rule descriptions can enable users to predict system performance, identify refinement needs, and take appropriate actions, thereby fostering trust and satisfaction with the system.


% \paragraph{Flexibility Slider}
% well what do you call this, alignment to evolving conditions, and personal preferences and constraints?

% The flexibility slider, based on rules confirmed by the user, allows adjustment of each rule's strictness, determining how strongly it is enforced during the model checker's verification process. Quantitative findings suggest that the flexibility sliders may enhance users' perception of the LLM's performance and user experience, as its removal resulted in lower scores compared to the full VeriPlan system in performance, usefulness, and satisfaction. Qualitative findings further reveal that users relied on the slider to align outputs with their needs and values, while enabling flexible adaptation to context-specific factors and evolving preferences.

% Our findings suggest that engaging users through features like the flexibility slider during model checking can enhance their perception of the system's performance. The flexibility slider allows users to adjust constraints beyond simply defining rules, enabling them to specify the strictness based on personal priorities or explore the system's capabilities by experimenting with different rule combinations (\eg users can loosen certain rules to accommodate alternative plans or tighten others when the system struggles to meet specific constraints.) This control over the level of adaptiveness helps tailor system outputs to better align with users' unique goals. These outputs can further guide users toward better options, help them settle on viable alternatives, or provide insights into their priorities, enabling them to adapt constraints to reflect their preferences and needs more effectively.

% The integration of features like flexibility sliders for user engagement can not only support the perceived system performance, but also improve user experience. Everyday scenarios involve shifting priorities, evolving preferences, and unexpected events, making flexibility and adaptability essential for ensuring a system's usefulness and successful integration \cite{lieberman2000out}. Flexibility sliders, or features that allow users to adjust the strictness of rules based on their changing needs, can provide the adaptive capability required. For instance, in weekly scheduling scenarios, users can shift preferences from having meetings spread throughout the day to scheduling them exclusively in the morning. Similarly, a user might relax an eight-hour work rule for a special occasion, such as their child's birthday. In another case, they could test experimental plans, like determining if a set of meals could be prepared within an hour by adjusting cooking time constraints. These features can allow users to dynamically express and modify their needs, enabling them to adapt to changing circumstances and achieve satisfying outcomes.



\subsection{Stages of User Engagement for LLM Verification and Alignment}
%RQ3. At What Stage of the Model-Checking Process Should Users Be Engaged to Maximize the Effectiveness of Integrating Verification Approaches with LLMs?
%process of decision-making and weights etc., rather than just feedback at the decision stage

Our findings indicate that user engagement with the core features, the rule translator and flexibility sliders, can effectively steer the direction of the LLM and refine it to align with the user's diverse needs and preferences. Based on insights from our findings, we identify two general stages in a user's interaction process with an LLM where user engagement can be beneficial: (1) the initial definition of guidelines and rules for LLM performance, and (2) iterative refinement based on model-checking outputs.

In the initial stage, users can set general guidelines, such as constraints, preferences, or protocols, to align the LLM's performance with their unique needs. Existing research shows that user input on preferences and domain knowledge during initial interactions is important to effectively guide system behavior \cite{kumar2024applications}. Without explicit user input at this stage, LLMs may have difficulty in inferring distinctive user preferences or goals, which are essential for tailoring the system's outputs. This stage can also be particularly effective for user engagement, as users might find it easier to define high-level preferences compared to specifying granular operational details, or they may not always have detailed insights or clarity into their needs and preferences \cite{lee2024ai, simonson2005determinants}. Our results support this notion, as the LLM-generated plans often inspired participants by presenting novel, creative, or efficient approaches they had not previously considered. This highlights a symbiotic relationship where user-defined boundaries enable the system to explore within those parameters, leveraging its computational capabilities to deliver outputs that align or may exceed user expectations.


The second stage of user engagement can occur after the model checker identifies conflicts and provides feedback. By this stage, users have outlined their general preferences and know their interaction goals. What remains unclear are the specific details of their preferences, priorities, or contextual needs, which may vary between users and are challenging for an LLM to infer. However, when presented with a plan or actionable feedback, it can be easier for users to effectively identify and articulate their desired adjustments or more granular preferences. Prior research shows that allowing users to refine and adjust system behavior based on initial feedback improves outcomes and enhances user experience \cite{fang2022framework, schellaert2023your}. Therefore this stage can be particularly valuable for user engagement, as it allows users to refine the LLM's behavior based on personalized intricacies or distinct nuances using tools like the flexibility slider. These refinements can also enable the LLM to capture additional cues and information for future adaptation.


This two-stage approach---initial user input to define the system's scope and subsequent refinement based on feedback---leverages an effective balance between user involvement and the system's autonomy. By allowing the system to independently generate solutions within user-defined boundaries, users can benefit from its ability to propose innovative and comprehensive outputs. Simultaneously, user-driven inputs and refinements can ensure that the system remains responsive to individual needs and evolving goals. Ultimately, this interplay between user input and system capabilities can foster a more effective and user-centered verification process.


%In addition to defining the overall direction for the LLM's performance, user engagement during the model checking procedure is also important to provide updates and modifications based on the model checking feedback. As discussed above, in the beginning of a task or when asked to provide their preferences, constraints, or rules, users may only have a general picture. However, once the LLM produces plans, the model checker checks for conflicts and provides feedback, the user now has information to build off from, further refining their preferences, identifying new rules, recalling rules they forgot and so on. Thus, in such cases, it is important to enable users with mechanisms to react upon this feedback, whether that is adjusting the rules entirely or adjusting the strictness of the rules. Our findings further support this suggestion, as users increasingly engage with the flexibility sliders once receiving feedback from the model checker, to adjust the strictness of the rules based on priorities, adaptability needs, and supporting system performance. Based on observing the plans and feedback that were presented, users also added new rules or made adjustments to existing rules. Therefore, once users had the LLM and model checker outputs to base off on, they actively responded with more specific directions or adjustments to guide the LLM's behavior. Thus, once the system or model checking procedure provides outputs that users can react upon, user engagement or control to adjust the content of constraints or preferences, or strictness of these properties can be effective to refine and adapt the system's behavior to respond to shortcomings in alignments or better align with user needs and goals. 


%\subsection{RQ1. How Can Formal Verification Methods, Specifically Model Checking, Be Effectively Applied to LLMs?}
%As interactions with probabilistic AI systems become more prevalent in everyday task environments, users benefit from the decision-making assistance and real-time adaptability of LLMs. However, these systems also introduce significant risks due to the inherent unpredictability of their outputs. The probabilistic nature of LLMs can lead to an infinite space of potential errors and failures, especially when users depend on these systems for critical or complex tasks. Addressing these issues requires grounded strategies to ensure that users can collaborate effectively with such AI systems while maintaining confidence in their outputs \cite{kambhampati2024llms}.

%Our work proposes the use of verification techniques to address this challenge, to manage the uncertainty, inaccuracy, and inconsistency in LLM outputs. We focus on enabling verification by implementing three features: the rule translator, flexibility adjuster, and model checker. The rule translator directly extracts a deterministic set of constraints from the user prompt, verifies it with the user, and also enables users to define the strictness of the constraints to be used by the model checker. The model checker then uses these user-defined rules to check whether the LLM output adheres to the specifications. Through our evaluation, users described that the verification approach acts as a safety net, ensuring that LLM outputs adhere to clearly specified user expectations. Our findings indicate that users found this deterministic, rigidity particularly useful, as it enabled them to verify and adjust accordingly within the interaction process, helping them catch errors or preventing LLM output deviations from the rules, leading to a more reliable interaction with the probabilistic system. Moreover, the introduction of model checking not only improved reliability but also had a positive impact on users' perceived system performance, usability, and satisfaction. Users consistently emphasized the importance of being able to engage with the system to validate its logic and confirm that it was adhering to rules tailored to their specific needs. This was particularly significant when comparing their experience to using traditional LLMs without verification, where users found it challenging to predict outputs and difficult to manually diagnose and correct errors.

%Critical to our verification approach was user-centeredness, specifically the involvement of users in providing feedback, shaping the rules, and adjusting their inputs based on the model checker's feedback. This engagement allowed users to not only verify, but also clarify and understand the system's interpretation of their prompts, aligning their expectations with the system's behavior. Moreover, by receiving feedback on what the system may have misinterpreted, users were able to refine their prompts, fostering a more resilient interaction. This iterative and interactive process was critical for enabling effective verification, as it enabled users involvement, transparency, and control over LLMs. Given the probabilistic nature of LLM outputs, it is clear that users require tools to control the uncertainty or unpredictable nature of LLMs. The use of deterministic tools, such as those enabled by our verification approach, proved valuable in helping users plan and execute tasks with greater confidence. These findings underscore the importance of integrating formal verification methods into the design of LLM-based systems to provide a balance between probabilistic flexibility and deterministic rigidity. By doing so, designers can offer users more control and reliability, ensuring AI systems meet their real-world needs.

%\revision{Once users defined the constraints, having an external process to automatically verify the plan based on these constraints were described to increase the user's \textit{reliability} on the plan and the LLM's perceived performance. The model checker freed users from the mental load of having to sift through the list of constraints and compare the generated plan, or be concerned about undetected conflicts or inaccurate judgements.
%ultimately veriplan is enhacing reliabilty, thus making LLMs for accessible as planning tools -- you need to better incorporate this in RQ1 discussion.}

% \subsection{RQ2. How Can We Engage Humans in the Process of Model Checking to Improve (1) the Quality of Outputs From LLMs and (2) the User's Experience?}

% Our findings show that leveraging user control within verification approaches and the interaction procedure is critical to improve the perceived performance of the LLM and the users satisfaction, usefulness, and reliability. 

% Throughout the interaction with \ours, user control emerged as a critical factor in enhancing the overall user experience when interacting with LLM systems. By being involved at different points throughout the verification procedure, users were able to exert control over the system, such as by verifying and adjusting rules, asking questions for system transparency, adjusting the flexibility of each rule, and adapting their needs based on the model checker's feedback. This controllability was essential in improving usability, satisfaction, and perceived performance, as LLM outputs were difficult to perform exactly to user expectations in every scenario. To address this misalignment in expectations, it is important to develop user control methods that allow users to adjust, guide, and customize LLM outputs as needed. \ours{} offered different types of control through its three distinct features, each of which contributed to better performance and user experience compared to using LLMs without model checking.

% Among the different types of user control provided in the three features of our verification approach, the rule translator feature proved to be the most impactful. This feature provided users with the rigidity and validity needed to ensure that the system adhered to their specific needs, thereby enhancing reliability. Nonetheless, the flexibility offered by the sliders, which allowed users to adjust system behavior based on their preferences and priorities, was also important. This feature was particularly beneficial for tasks involving personal context or subjective needs, where adaptability was crucial. c

% \subsection{RQ3. At What Stage of the Model-Checking Process Should Users Be Engaged to Maximize the Effectiveness of Integrating Verification Approaches with LLMs?}

% All three features exceeded user experiences compared to an LLM without our verification approach. Of the three, the model checker equipped with the rule translator was the most effective. Users emphasized that verifying constraints was crucial, as aligning these constraints with their expectations was the key to generating the right solution from the LLM. The rule translator added validity by accurately representing users' expectations and applying them to the LLM's output. Given the importance of correct constraints, users felt it was essential to be involved in this process to ensure all rules were accurate.

% The rule translator also helped mitigate misunderstandings, incorrect assumptions by the LLM, and unclear user prompts. Users noted that these issues often created barriers when interacting with standard LLMs, leading to ineffective outputs. However, by filtering user prompts and presenting its interpretation, the rule translator acted as a useful resting point, allowing users to check the quality of the interpretation and rules, identify any missing details, and make adjustments accordingly.


% Following the rule translator, the model checker with the flexibility slider was the next most effective feature. Our findings suggest that the sliders served as a valuable tool for personalizing user interactions and fostering creative action planning with the LLM. As the rule translator defined strict boundaries, conveying nuanced or context-specific preferences was often limited with this feature. The inclusion of sliders offered users the flexibility to adjust the strictness of these rules, enabling them to tailor the system's behavior to their individual needs and preferences.

% This flexibility was particularly significant for tasks requiring personal adaptation. Especially in scenarios such as weekly scheduling, users described the sliders as crucial for reflecting their personal preferences and priorities. For instance, in these situations, users could prioritize family time over work or focus on efficiency over quality. By using the sliders, they could easily modify the system's rule enforcement to better align with their needs. In contrast, tasks like recipe creation and patient navigation, which were more rule-bound, saw less frequent use of the sliders. When they were used, it was often to either soften specific rules to reach a solution or to apply stricter constraints to rules that the system struggled to follow.


% The ability to adjust rule strictness gave users a sense of control and allowed them to feel more involved and empowered in the system's decision-making process. Users emphasized that the sliders helped them reflect their unique preferences through the model checker, ensuring that the LLM could adapt in a way that suited their personal and contextual needs. This feature not only enhanced usability but also fostered greater satisfaction by allowing users to define the balance between flexibility and rigidity in the system's behavior. Thus, our findings suggest that tools that enable users flexibility---such as sliders or other mechanisms to adjust rule enforcement---can greatly improve the adaptability of LLM systems in real-world contexts. 

% On top of building the three features for the verification approach, involving users throughout the verification was core for its effectiveness, to facilitate understanding of the system's status, verifying important constraints and priorities for users, adapting to unique and changing user needs, and giving them control over outputs for their plan. Designers can further investigate and understand different points throughout the verification and interaction procedure on where and how to engage users, to make LLMs more effective for a range of personalized tasks and ever-changing user needs.

\subsection{Design Implications}
Below, we present design implications for incorporating verification processes and user control features into future systems and interaction design.

%\subsubsection{The Value of \ours{} is Greater than the Sum of its Components}

\subsubsection{Considerations for Integrating Verification Methods into LLMs}

\revisionNew{Our findings demonstrate that formal verification methods, such as model checking, can serve as effective guardrails when applied to LLMs, enhancing perceived performance and user experience. Not limited to model checking, LLM designers should consider integrating various external verification techniques into system designs to ensure reliable verification. Beyond using LLMs for verification, prior work has explored how external verifiers can complement LLM capabilities to address their inherent lack of reliable self-verification. \citet{kambhampati2024llms} introduced a modulo framework, which combines the generative strengths of LLMs with external ``critics'' or verifiers. In this framework, LLMs generate candidate plans and ideas, which are then evaluated by specialized critics leveraging formal domain models and planning algorithms. In other work, \citet{zhang2023controlling} verify LLM outputs by comparing the present state with historical trajectories extracted from a memory module, enabling evaluation and learning. \citet{gou2023critic} employ external tools (\eg knowledge bases, code interpreters, search engines, and calculators) to critique and refine LLM outputs.
Therefore like model checking, LLM designers should explore non-LLM-based verification methods that align with their specific task goals to ensure reliability and effectiveness.}


%These results suggest that formal verification procedures, such as model checking, can provide effective guidelines and guardrails for LLM behavior, making it safer, more reliable, and better aligned with user needs.
% System designers should incorporate verification techniques into LLM systems. As LLMs become increasingly capable of reasoning, inference, and task-solving, and are deployed across diverse applications with varying levels of user expertise, implementing safety guardrails is critical to prevent inaccurate or harmful outcomes. Future designs could adopt strict, automated verification for high-stakes tasks or extreme constraints (\eg regulated content or suggestions for children) or lighter verification approaches tailored to user-defined constraints (\eg housecleaning rules), as exemplified by \ours{}.

% Given the probabilistic nature of LLMs, designers should also prioritize deterministic measures like model checking over additional probabilistic agents for verification (\eg \cite{chen2024can, kambhampati2024llms}). Future research could expand on how deterministic approaches can augment probabilistic systems, such as in both the planning and execution phases. For example, constraints could be applied deterministically before the LLM generates its output, ensuring that only outputs meeting the specific conditions are produced.  %to ensure verification is robust and reliable.


\subsubsection{Considerations for Incorporating Multiple Dimensions of User Control}

\revisionNew{In \ours{}, both the rule translator and flexibility sliders provide distinct yet complementary dimensions of control: the rule translator allows users to define and refine strict verification boundaries, ensuring alignment with their needs, while the flexibility sliders enable users to adjust the relative weights of rules, adapting the system based on context, evolving preferences, and user priorities. Together, they balance rigid rule definition with nuanced customization to meet diverse user requirements.

Similarly, LLM designers should incorporate appropriate levels and opportunities for user control to optimize system performance, usefulness, and satisfaction. For example, systems could proactively infer user preferences from behavior and engage users for verification, or dynamically adjust the user's autonomy during interactions when the system determines that user control is unnecessary. User control has long been recognized as a critical feature in human-computer interaction, influencing user experience and outcome quality \cite{shneiderman2010designing, nielsen1999designing, harambam2019designing, harper2015putting, andjelkovic2016moodplay}. However, \citet{jin2017different} caution that excessive control can increase cognitive load, emphasizing the need to tailor control to task and user characteristics, such as familiarity and domain knowledge, for balanced usability and effectiveness. Therefore, multi-dimensional control mechanisms should be carefully designed to enable LLM systems to gather richer inputs and seamlessly integrate human knowledge and preferences into their decision-making processes.}

%As demonstrated in \ours{}, future systems should incorporate diverse user control mechanisms to address various tasks and interaction stages, allowing users to guide and adapt the system to their goals. Beyond verification contexts, these features could support contexts like preference extraction and interaction goal inference. For example, user preferences, that aren't limited to temporal properties like \ours{}, could be defined and validated through the rule translator, then weighted via sliders to tailor the decision-making process. Such multi-dimensional control mechanisms enable systems to gather richer inputs, seamlessly integrating human knowledge and preferences into AI processes. 






%This leads to a key design implication: \textbf{the value of combining the slider and translator for LLM-based planners is greater than the sum of their individual contributions.}
%Our quantitative results from the Tukey analysis shows that, the full version of \ours{} (C1) often outperforms the individual baselines in terms of performance, usefulness, and satisfaction measures. However, there is no significant difference between the ablation conditions (C2-4), suggesting that adding either the slider or translator component individually has less impact on these measures. Instead, the notable effect appears to arise from the combination of these components.

%To further elucidate the value of the full system when compared to each individual component, we conducted a Dunnett's test as a supplement to our Tukey analysis. This Dunnett's test provides greater statistical power in comparing the means of the ablation groups (C2, C3, C4) to the mean of the full system. The test was performed with a significance level of 0.05. Similar to the results from our Tukey analysis, the Dunnet test revealed that the full system (C1) significantly outperformed the \textit{C3 ($\neg$Translator)} $(p=.0011)$ and \textit{C4 (None)} $(p=.0013)$ conditions; was significantly more useful than the \textit{C2 ($\neg$Slider)} $(p=.047)$, \textit{C3 ($\neg$Translator)} $(p=.009)$, and \textit{C4 (None)} $(p=.0257)$ conditions; and was significantly more satisfying than the \textit{C3 ($\neg$Translator)} $(p=.007)$ and \textit{C4 (None)} $(p=.0101)$ conditions. 

%These findings suggest that both the rule translator and flexibility sliders are necessary to achieve effective user control in verification processes for LLM-based planning tools. The effectiveness from the combination of these components is likely because they provide distinct but synergistic dimensions of user control. The rule translator enables users to define strict boundaries for verification, involving them to directly evaluate translation outputs, align them with their needs, and refine them accordingly. Meanwhile, flexibility sliders provide user control at a more nuanced dimension by enabling users to assign relative weights to rules, which helps adapt the system's performance to contextual factors, evolving preferences, and distinct priorities. In other words, while the rule translator excels in defining rigid boundaries for verification, the flexibility sliders address subtle or context-specific needs, offering a complementary refinement process.


%Leveraging the appropriate level of user control to achieve optimal performance, system usefulness, and user satisfaction is a critical aspect of system design. User control has long been recognized as an important feature in human-computer interaction, significantly influencing the quality of the user experience \cite{shneiderman2010designing, nielsen1999designing, harambam2019designing}. Research shows that lower levels of user control negatively affect user's satisfaction, experience, and perceived quality of outcomes \cite{harper2015putting,parra2015user,pu2012evaluating, andjelkovic2016moodplay}. However, \citet{jin2017different} highlight that higher user control is not always beneficial, as it can increase cognitive load. The authors further suggest that the optimal level of user control depends on the user's familiarity and domain knowledge, thus adapting the level of control to align with task and user characteristics is essential for balancing usability and effectiveness. For \ours{}, the system provides varying levels of user control in different stages during the interaction to guide the verification process. Similarly, future systems can aim to incorporate diverse dimensions of user control features that address different tasks and stages throughout interaction, enabling users to guide and adapt the system according to their goals and needs.

%Moreover, the integration of user control features providing different levels of control can extend beyond verification approaches for LLMs. Similar approaches can be applied to extract user preferences or infer interaction goals, which can then be refined and adjusted by the user. For instance, user preferences could be defined and confirmed through the rule translator then weighted through the sliders, tailoring the system's decision-making process. Leveraging such multi-dimensional user control mechanisms allows systems to gather richer inputs and cues, facilitating seamless adaptation and integration of human knowledge into AI processes.


%Thus, future systems should be designed to create specific pipelines that provide users with control across different stages and dimensions, so they can guide and adapt the system based on their needs and goals. 
%The integration of such user control features resembling the rule translator or flexibility sliders are not just limited to verification approaches for LLMs. Similar approaches can involve extracting user preferences or inferring user interaction goals and confirming and refining them with the user. These properties can then be sent to features similar to the flexibility slider for the user to adjust weights or the strictness of enforcing them to the system's decision-making procedure. The leverage of different dimensions of user control can help the system gather information and cues that enable the system to effectively adapt to the system needs and integrate human input or knowledge seamlessly and effectively. 

%The value of user control was evident across various points in the interaction. Our findings suggest that leveraging multiple control features throughout the design process can significantly benefit users. By offering control over different stages of the interaction, the model checker enabled users to experience greater reliability and customizability. This resulted in a more flexible and dependable system, which increased user satisfaction and trust.

%\subsection{Updated System Design based on Findings}
%\subsection{impact of user control on reliability, satisfaction, quality improvement}

%\subsection{impact of verification on LLMs}

%\subsection{best feature among the three: when and where to involve users?}

\subsubsection{Considerations for Designing Flexibility Sliders Based on Task and Constraint Characteristics}

Our findings show that the usage pattern for flexibility sliders largely depended on the characteristics of the constraints. When participants worked with organizational or strict rules that allowed little room for negotiation, they used the slider to guide the system to best adhere to all the constraints, ultimately aiming to efficiently obtain an accurate, verified output. In contrast, for constraints reflecting personal preferences or priorities, participants employed the slider as a representation of their values, adjusting it to adaptively align the system's behavior with their evolving needs. %This allowed the system to adapt flexibly, even generating outputs that could be both new and creative while satisfying users' changing expectations.

Designers could adapt flexibility sliders to play various roles, tailored to the task and constraint characteristics. For instance, in workplace contexts that use AI systems for decision-making or plan generation such as the healthcare domain (\eg ensuring diagnosis or treatment plans adhere to medical protocols \cite{hosny2018artificial}) or financial underwriting (\eg creating financial plans while complying with regulatory rules \cite{mayer2020unintended}), sliders can emphasize under-adhered-to rules or prioritize task-specific constraints. Additionally, sliders can allow the worker to explore alternative outcomes, such as loosening a ``no student loans'' rule to assess how it affects a mortgage decision, providing the worker insights for financial advising. Conversely, in personalized contexts like movie recommendations, sliders can enable users to dynamically adjust preferences, such as exploring genres based on their current mood. Thus, future systems should leverage flexibility sliders to support diverse roles, such as facilitating efficient task completion in structured environments or fostering adaptive outputs in more flexible, personalized settings. 


%Our qualitative findings reveal two primary reasons users employed the flexibility sliders: to steer the system's performance for better rule adherence (\eg loosening well-followed rules while strengthening strictness for frequently missed ones) or to adapt rule strictness based on changing priorities and preferences. These usage patterns largely depended on the characteristics of the constraints.

%When users worked with organizational or strict rules that allowed little room for negotiation, they used the slider to guide the system toward understanding and adhering to these critical constraints, ultimately aiming to efficiently obtain an accurate, verified output. In contrast, for constraints reflecting personal preferences or priorities, users employed the slider as a representation of their values, adjusting it to adaptively align the system's behavior with their evolving needs. This allowed the system to adapt flexibly, even generating outputs that could be both new and creative while satisfying users' changing expectations.

%These findings suggest that flexibility sliders can play varied roles in systems, tailored to the task and constraint characteristics. For instance, in workplace contexts that use AI systems for decision-making or plan generation such as the healthcare domain (\eg ensuring diagnosis or treatment plans adhere to medical protocols \cite{hosny2018artificial}) or financial underwriting (\eg creating financial plans while complying with regulatory rules \cite{mayer2020unintended}), sliders can emphasize under-adhered-to rules or prioritize task-specific constraints. Additionally, sliders might allow the worker's exploration of alternative outcomes, such as loosening a ``no student loans'' rule to assess how it affects a mortgage decision, providing the worker insights for financial advising. Conversely, in personalized contexts like movie recommendations, sliders empower users to dynamically adjust preferences, such as exploring genres based on their current mood. Thus, future systems could leverage flexibility sliders to support diverse rolesenabling efficient task completion in structured environments or fostering adaptive outputs in more fluid, personalized settings. 


%In \ours{}, user control was provided at two primary levels: rule validation and rule flexibility adjustment. Our user study findings suggest that instead of offering user control universally, tailoring it based on task characteristics and stakes can be more effective. For instance, participants indicated that flexibility sliders were unnecessary in the hospital navigation scenario, where stakes were high, and strict adherence to rules was critical. In contrast, flexibility sliders proved valuable in the scheduling and cooking scenarios, where users wanted to explore alternative outcomes by adjusting rule flexibility.

%This contrast suggests that high-level control, such as rule validation, is generally beneficial across scenarios. However, lower-level control, like flexibility adjustments, may depend on the task's goal and stakes. Designers can apply this insight to different contexts. For example, in workplace settings like healthcare (\ie diagnosis or treatment plans that adhere to medical protocols) or financial underwriting (\ie underwriting that adheres to regulatory and financial rules), lower-level flexibility control might be unnecessary, as users prioritize receiving verified outputs that strictly adhere to rules. In contrast, in low-stakes settings such as movie recommendations, flexibility controls allow users to adjust preferences dynamically, such as exploring genres based on their mood.

%Even in high-stakes settings, users might occasionally want to explore alternative outcomes, such as for action planning (\ie a loan officer exploring financial advising plans to help a loan-denied customer gain approval). In such cases, making lower-level flexibility controls accessible but optional can meet diverse user needs. Designers can strategically determine which levels of control to incorporate and how to present them, based on the specific characteristics of the task.

\subsubsection{Considerations for Designing Effective Interfaces for LLMs}

Users reported that the interface enhanced their understanding of planning content, facilitated effective feedback and modifications through user control features, and helped organize generated plans to align with their preferences. They particularly appreciated interactive elements such as rule checkboxes and flexibility sliders for providing input beyond text, as well as the mind map-based layout for managing plan generation and incorporating feedback from the model checker. These features improved users' ability to interpret system outputs, reorganize plans efficiently, and gain actionable insights for the next steps.

To design future systems that incorporate verification approaches for LLMs, designers should consider visual, intuitive, and interactive interfaces to enhance usability and satisfaction. Recent studies have emphasized the value of interfaces and visualizations in helping users better understand, organize, and utilize information from LLMs \cite{ma2024beyond, suh2023sensecape, jiang2023graphologue, wang2024farsight}. For complex tasks like planning, where users must manage multiple constraints or variables and compare outputs, text-based interactions alone may be insufficient. Instead, systems could integrate features such as tools for saving and retrieving plans that effectively align with user-defined preferences, drag-and-drop interfaces for reorganizing plan components, timeline views for tracking evolving changes in needs or preferences, or dashboard summaries for visualizing comparisons. 

%These tools can support user assessment, facilitate engagement, and streamline the system's decision-making process while ensuring alignment with user needs.


%Our qualitative findings highlight the usefulness of VeriPlan's interface in enhancing usability and satisfaction. Users reported that the interface improved their understanding of planning content, facilitated effective feedback and modifications through user control features, and helped organize generated plans according to their preferences.

%Participants emphasized that VeriPlan's front-end interface significantly improved the user experience compared to existing generative AI tools or text-based applications like ChatGPT. They appreciated interactive features such as rule checkboxes and flexibility sliders for providing input beyond text, as well as the mind map-based layout for managing plan generation and model checking. These features enhanced users' ability to interpret system outputs, reorganize plans efficiently, and gain actionable insights for next steps.

%These findings suggest that the design of the front-end interface plays a crucial role in the user experience of LLM-based applications. Recent works have increasingly focused on creating interfaces and visualizations to help users better understand, organize, and utilize information from LLMs \cite{ma2024beyond, suh2023sensecape, jiang2023graphologue, wang2024farsight}. Text-based interactions may fall short for tasks like planning, where users often need to manage multiple constraints or variables and cross-compare multiple outputs. Visual and interactive interfaces, such as those in \ours{}, can better address these needs by providing intuitive tools for engagement and decision-making.

%As LLM-based applications are increasingly applied to complex tasks that require user collaboration and active participation in decision-making, rich interface designs beyond text-based interactions become more essential. Future systems should integrate diverse visual and interactive features tailored to the specific tasks and interaction goals, to ensure a more effective and engaging user experience.

}
%A key factor in facilitating users' interpretation of verification results, engagement in the verification process, and follow-up actions based on model-checking outputs was the mind map-based interface layout. This structure compartmentalized each iteration, helping users visualize differences and improvements between model-checking cycles. It also allowed users to easily retrieve information about violated rules when providing subsequent instructions to the LLM.

%Users appreciated the ability to move map boxes, organizing them based on preferences---bringing favored boxes closer and less relevant ones further away---to streamline input for future iterations. Participants noted that this visual layout was easier to read and navigate compared to traditional text-based outputs found in many current LLM interfaces. Additionally, interactive elements such as checkboxes, buttons, and sliders were described as intuitive and user-friendly, further enhancing their involvement in the verification process. Thus when designing LLM applications that involve user participation in verification, designers should prioritize intuitive, visually organized UI features that improve usability and encourage adoption.



\section{Limitations \& Future Work}

While our proposed system offers valuable contributions to integrating formal verification with LLMs, several limitations exist that suggest areas for future improvement. First, the types of temporal constraints available in our template represent a limited subset of potential constraints. Users could benefit from greater flexibility, particularly the ability to define their own temporal constraints, or through enhanced capabilities of LLMs that could be trained or fine-tuned to handle a wider range of temporal constraints.

Additionally, the limitations of the current modeling framework, specifically using PRISM and Stormpy for verification, restrict the types of temporal constraints or logical expressions that can be formulated in LTL. Future research should explore alternative model-checking and formal verification approaches to enhance the expressiveness and applicability of formal verification in conjunction with LLMs.
Future work could also focus on improving the output of the model checker within \ours{} by introducing proactive suggestions and actions. This improvement could be achieved through program repair techniques, such as automatic or interactive repair. 


\revision{The evaluation of \ours{} is limited to three scenarios, focusing on planning tasks that end-users can describe in natural language. Future work could explore and test the effectiveness of \ours{} in various domain-specific planning tasks, such as those in healthcare or manufacturing.

Finally, our sample size is limited to 12 participants. Larger scale studies can be conducted to validate and expand the results and reveal additional insights beyond those identified in this paper.}

%which we envision operating on two levels: automated and interactive. Automated program repair would allow the system to resolve issues autonomously within its predefined rules and knowledge base. In cases where uncertainty arises, interactive program repair would prompt user feedback to guide the system in making more informed repairs.

%Moreover, by combining model checking with emerging machine learning techniques such as Retrieval-Augmented Generation (RAG) \cite{lewis2020retrieval} and Reinforcement Learning from Human Feedback (RLHF) \cite{christiano2017deep}, the system could retrieve relevant user preferences, safety guidelines, or past experiences to aid in more tailored decision-making and repairs. Once the necessary information is gathered, it could undergo a synthesis and verification process using model checking to ensure compliance with user-defined constraints, further improving the reliability and adaptability of the system.

%We clarify that our current findings represent key effects observed in our sample, and we encourage readers to interpret these results as indicative of broader trends that future studies with larger samples may further explore.} 


\section{Conclusion}
This paper introduces \ours, a system that integrates formal verification techniques with LLMs to enhance their reliability and usability for end-user planning tasks. Our evaluation shows that the core features of \ours---the rule translator, flexibility sliders, and model checker---improved users' perceptions of performance, usability, satisfaction, and reliability in LLM outputs. These findings emphasize the value of incorporating formal verification methods in LLMs for everyday users, providing rigidity and deterministic boundaries to mitigate the probabilistic nature of LLMs, making them more reliable for planning tasks. The integration of user-controlled flexibility in verification further enhanced creativity in plan generation and aligned outputs with personal preferences and evolving contexts. Finally, our results underscore the importance of user control in the model-checking process, which significantly improves the reliability and usability of LLM outputs. Based on these insights, \ours{} offers valuable implications for LLMs as end-user planning tools,  highlighting the need for verification methods and user control features to ensure reliability, user-centered adaptability, and alignment with complex real-world needs.
%% the bibliography file.
%\balance


\begin{acks}
We thank the reviewers for their helpful comments. This work was supported by the National Science Foundation awards 1925043 and 2152163. This research was also partially supported by the U.S. Naval Research Laboratory (NRL) and an NRC Postdoctoral Research Associateship awarded to DP at NRL. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Navy.
\end{acks}
\balance
\bibliographystyle{ACM-Reference-Format}
% \balance
\bibliography{bibliography}


\end{document}
\endinput

%%
%% End of file `sample-manuscript.tex'.
