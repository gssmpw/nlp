\section{Related Works}
%\christine{Do we need a planning section RW?}
%- LLMs have challenges as they are increasingly used with general users and deployed in real-world applications
%- in response, research and industry has focused on making LLMs human-centered
%In this section, we review research on the challenges that users face in using LLMs and prior work that has aimed at addressing these challenges as well as provide background on methods in automated planning and model checking.

\revision{In this section, we provide background on automated planning for end-users and discuss the challenges they face when using LLMs. Next, we review existing verification approaches for LLMs, both broadly and within the context of automated planning. Finally, we provide background on model checking and its use in our verification approach.}

\subsection{Automated Planning for End-users}
%%% definition of automated planning
\textit{Automated planning} refers to automated techniques that decide \textit{what} an agent does, namely the steps that it takes to achieve a goal, rather than \textit{how} it performs each step \cite{ghallab2016automated}.
Numerous languages and libraries exist that enable users to interact with planning algorithms, such as the \textit{Planning Domain Definition Language} (PDDL) \cite{fox2003pddl2}, the \textit{GTPyhop} planner \cite{nau2021gtpyhop}, and the extensive \textit{Unified Planning} library \cite{kapellosaiplan4eu}, to name a few examples. Although planning tools are typically intended for expert users, recent work has engaged novice users in the planning process through visualization \cite{DePellegrin_Petrick_2024} and plan creation \cite{porfirio2024polaris}.
\revision{However, these planning tools pose significant challenges for end-users due to their reliance on complex formal languages and abstract logic formulas \cite{ebbinghaus1994mathematical, schoen2020authr, hurnaus2010programming}, which are difficult to learn and apply. The technical interfaces often lack intuitiveness, providing rigid workflows and low-level feedback  \cite{helmert2009concise, peer2004pddl, shah2013knowledge}. Moreover, users must invest significant effort in creating detailed system models, specifying states, transitions, and probabilities \cite{porfirio2018authoring, sauer2022structure, porfirio2020transforming}---tasks that demand technical expertise and are highly time-consuming. Designed with a focus on theoretical rigor and correctness, these tools often neglect practical usability, leaving them to fall short in addressing the dynamic and high-level goals of end-users.}

LLMs possess great potential to further increase the accessibility of automated planning for novice users.
Given a natural language prompt or set of prompts, LLMs are demonstrably capable planners \cite{silverLLM2024, songLLMPlan2023, lu2023plug} without requiring the user to directly interact with low-level planning languages or libraries. Still, LLMs are insufficient as standalone planners, requiring external support to verify and improve planning output \cite{pmlr-v235-kambhampati24a}.
To this end, \citet{gundawar2024robustplanningllmmoduloframework} contributes an \textit{LLM-Modulo Framework} that checks LLM-produced plans against a set of \textit{critics}, which provide feedback to the LLM to iterate. In our work, we envision the novice user as a critical component of the verification-feedback loop, akin to recent work in human-LLM interaction for text annotation tasks \cite{wang2024LLM}. For planning tasks, there is a research gap on designing systems to engage novice users in the verification-replanning process, which this work aims to address.

\subsection{End-user Challenges with LLMs}
\revision{
As LLMs are increasingly deployed in everyday applications and engage directly with end-users, they demonstrate great potential but also present significant human-centered challenges, particularly in terms of \textit{usability} and \textit{reliability}. 


\textit{Usability} remains a critical issue as users frequently struggle with crafting effective prompts and engaging with systems beyond the input stage. Studies highlight the difficulty users face in formulating prompts that elicit desired responses \cite{zamfirescu2023johnny, khurana2024and, tankelevitch2024metacognitive, subramonyam2024bridging, liu2024we}. Additionally, the cognitive demands placed on users---such as monitoring and deciding on strategies for prompting and interaction---exacerbate these challenges \cite{tankelevitch2024metacognitive, subramonyam2024bridging}. 
Another usability barrier is users' difficulty understanding how prompts influence outputs and building accurate mental models of the system's behavior and the reasoning behind it \cite{bhatt2021uncertainty, sun2022investigating, vasconcelos2023generation}. 
%User engagement is often confined to the input stage, making it difficult for users to influence outputs effectively. 
In response to these challenges, engaging users during the interaction process to steer the LLM's behavior, and support user's understanding of the reasoning has gained increasing attention. 
Strategies like co-creation, where users and AI collaboratively refine outputs, have been proposed to expand engagement and improve interaction intuitiveness \cite{schellaert2023your}. Similarly, interactive environments with user-controllable parameters enable experimentation, helping users build a better understanding of LLM capabilities \cite{louie2020novice, ma2024beyond, suh2023sensecape, jiang2023graphologue}. 
In addition, approaches like enhancing explainability and introducing customizable interaction options aim to reduce cognitive load and improve user experience \cite{tankelevitch2024metacognitive, teufelberger2024llm}. 
While engaging users and providing control to address usability challenges is a promising direction, further work is needed to understand \textit{how} and \textit{when} to involve users throughout the interaction process with LLMs. Such exploration can reveal ways to gather direct input and feedback that help LLMs accommodate evolving preferences and more effectively meet diverse user needs.



The \textit{reliability} of the output is another significant challenge. LLMs are prone to generating text that appears structurally coherent but contains factual inaccuracies or nonsensical information, a phenomenon known as hallucination \cite{rawte2023survey, bender2021dangers, ji2023survey, maynez2020faithfulness}. The lack of interpretability further complicates users' safe reliability, as users often struggle to understand the reasoning behind the output of the LLM \cite{mathews2019explainable, zhao2024explainability, yang2024harnessing, mirchandani2023large, liu2024we}. These issues are especially concerning in safety or mission-critical domains, such as healthcare or military applications, where reliance on incorrect outputs can have severe consequences \cite{koga2023exploring, lee2023benefits, sallam2023chatgpt}. These issues can further lead to risks of users over-relying on LLM-generated outputs without sufficient critical evaluation, underscoring the need for mechanisms that support users' safe and reliable interactions with LLMs \cite{ji2023survey, maynez2020faithfulness}. 
}

\subsection{Verification Approaches for LLMs}
\revision{
The advancements in LLMs have unlocked unprecedented capabilities in sense-making, language use, and interaction, enabling precise inference of user needs and applications across diverse domains \cite{kim2024understanding, minaee2024large, zhao2023survey}.
As these systems advance, ensuring their safety, reliability, trustworthiness, and alignment with user needs has become a pressing focus. To address this, a substantial body of work has emerged on verifying LLM outputs, which we broadly categorize into two directions.

The first direction focuses on enhancing user trust through explanations and interface design. Existing approaches generate explanations to support users in understanding and trusting LLM outputs \cite{li2022explanations, krishna2024post, huang2023can, marasovic2021few, wiegreffe2021reframing}. Others have explored designing interfaces and tools that help users deconstruct textual components, evaluate LLM outputs, and act upon them effectively \cite{ma2024beyond, suh2023sensecape, jiang2023graphologue}.

The second direction focuses on ensuring the validity of LLM outputs. One notable direction includes using LLMs for evaluation \cite{desmond2024evalullm, zhang2023wider, zheng2024judging} or orchestrating multi-agent systems to verify outputs \cite{mostajabdaveh2024optimization, liang2024improving, hassan2024llm, chan2023chateval}. These methods have been applied to complex tasks such as mathematical reasoning \cite{wu2024mathchat, zhang2025mathverse, li2023making}, semantic reasoning \cite{chen2023teaching, ni2023lever, liu2024speak}, and data annotation \cite{wang2024human}. Additionally, other approaches involve humans in evaluating and correcting outputs \cite{wang2024human, shankar2024validates}. Finally, a growing area of research incorporates constraint-based approaches, such as applying constraints to planning in robotics \cite{yang2024plug}, creating datasets with constraints for evaluation \cite{zhang2024cfbench}, or generating plans that adhere to multiple constraints \cite{xie2024travelplanner}. However, constraint-based approaches often utilize predefined datasets and can suffer from the lack of mechanisms for dynamically incorporating user preferences, needs, or evolving contexts.

Despite recent advancements, challenges persist in relying on LLMs for verification. Using LLMs to verify their own outputs risks critical flaws. Studies highlight their deficiencies in error detection, correction mechanisms, and adherence to constraints, as well as their tendency to hallucinate or retrieve inaccurate context \cite{ji2024testing, yao2023llm, liu2024exploring}. 
For instance, in the planning domain, despite extended context windows and few-shot learning, \citet{xie2024travelplanner} and \citet{chen2024can} demonstrate that LLMs struggle to generate plans and feedback for complex scenarios or adhere to predefined constraints. Similarly, \citet{valmeekam2023planning} reports that GPT-4 achieves an average success rate of 12\% in planning tasks, highlighting the inadequacy of LLMs in handling intricate requirements independently.
Other works have highlighted how utilizing LLMs for evaluation can suffer from bias based on the order, appearance, or length of the content, aspect-specific evaluation, scalability, and effectiveness in diverse contexts \cite{wang2023large, huang2024empirical, koo2023benchmarking, son2024llm, park2024offsetbias}. 
These limitations have led to heuristic and modular approaches as verification mechanisms to address such shortcomings \cite{kambhampati2024llms, valmeekam2022large}. Moreover, LLM reasoning and explanations, such as chain-of-thought reasoning, can be influenced by biased contexts, raising further caution about their reliability \cite{turpin2024language}. Consequently, developing methods to verify LLM outputs without relying on LLMs is critical to ensure validity, particularly for high-stakes, real-world applications.

}



%To support users in verifying LLM outputs, enhancing explainability and transparency in decision-making processes have been employed to improve users' ability to critically evaluate outputs and foster trust \cite{bhattacharjee2024towards}.

%Human-centered approaches that emphasize interpretability and adaptability have been proposed to address these risks, making LLMs more suitable for diverse populations and contexts \cite{schellaert2023your, sarkar2022like, xiao2024human}. Interactive interfaces that allow users to provide real-time feedback or adapt outputs dynamically based on their input represent a promising direction for aligning outputs with user expectations \cite{fakhoury2024llm, zhang2023visar}.



\subsection{Model Checking in Formal Verification and LTL constraints}\label{hello}

Model checking is a formal verification technique used to determine whether a software or hardware system satisfies requirements expressed in formal logic \cite{baier2008principles}. By systematically exploring all possible states that a system may encounter or produce, model checking exhaustively examines system behavior against these requirements, making it essential for proving the behavior of highly complex systems. Linear Temporal Logic (LTL) is a commonly used representation to express requirements, or \textit{properties}, in domains such as assistive robotics \cite{dixon2014fridge} and autonomous navigation \cite{liu2023grounding}. LTL allows users to specify and compose temporal constraints in the form of sequencing (\ie{} ``event A must occur before event B''), eventuality (\ie{} ``event C must eventually happen.''), and safety (\ie{} ``event D will never occur''), to name a few examples. This expressiveness makes LTL suitable for real-world tasks such as scheduling, safety protocols, and workflow management, where the timing and the order of actions are critical. %Building on LTL's representativeness in real-world scenarios, we focus in this work on applying model checking to time-based conditions using LTL properties.

\revision{In summary, our work builds on existing approaches to verify and validate LLM outputs, with a particular focus on constraint-based methods. We extend these methods by directly involving human engagement to define and refine constraints that align with users' needs and preferences. Our features for human engagement are designed to support varying levels of user control and involvement, for users to effectively guide the LLM's behavior. We leverage the significant potential of LLMs as end-user planning tools while addressing their shortcomings and user challenges through the implementation of an external verification approach using model checking, a formal verification technique.}


\input{figure_interface}

% \christine{\textbf{need translator backend figure}}
% \christine{\textbf{need model checker/LLM to PRISM figure}}
% \christine{\textbf{need mapping figure}}