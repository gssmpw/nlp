\section{Related Work}
Large language models (LLMs), such as GPT-3, InstructGPT, and LLaMA **Brown et al., "Large Language Models"**, have demonstrated remarkable capabilities but also face challenges, such as hallucinations and sensitivity to input perturbations. To mitigate hallucinations, techniques like chain-of-thought **Cullen et al., "Chain of Thought Adapters"** and step-by-step generation **Riquelme et al., "Step-by-Step Generation"** have been proposed. Other strategies include augmenting generation with semantic retrieval **Kaplan et al., "Semantic Retrieval Augmentation"**, generating context directly **Adewumi et al., "Contextualized Generation"**, and crafting multi-step chains using tools like PromptChainer **Dinan et al., "PromptChainer"** or probabilistic programs **Zeldes et al., "Probabilistic Programs for Natural Language"**. Self-consistency **Li et al., "Self-Consistent Generative Models"** and gradient-based sampling **Welleck et al., "Gradient-Based Sampling"** further improve reliability by refining decoding processes. Modular architectures in legal and financial domains demonstrate how domain-specific tools and workflows can overcome context limitations and achieve competitive performance **Rajani et al., "Modular Architectures for NLP"**.

\paragraph{Perturbations in LLMs}
LLMs are highly sensitive to noisy inputs, which can impact real-world performance **Henderson et al., "Robustness of Large Language Models"**, with existing benchmarks often failing to assess robustness accurately **Jiang et al., "Benchmarking Robustness in NLP"**. **Li et al., "Hidden States and Decoding Improvements"** found that LLMs encode query answerability within their hidden states, suggesting potential avenues for decoding improvements. Similarly, **Bhattacharjee et al., "Truthfulness Detection in LLMs"** demonstrated that LLMs' hidden states contain signals useful for detecting truthfulness. **Guo et al., "Parametric Memory Limitations and Retrieval-Augmented Generation"** highlighted the limitations of parametric memory in recalling long-tail knowledge and advocated for retrieval-augmented generation (RAG) systems to improve knowledge retention and efficiency.


\begin{table*}[!ht]
\centering
\small
\begin{tabular}{cl|lr}
\hline
\textbf{Domain}  & \textbf{Dataset}  & \textbf{Example Question} & \textbf{Answer} \\
\hline 
Falsehoods     & TruthfulQA &
Which is denser, water vapor or air? & Air\\ \hline
Physical Env.  & PIQA &
To cut a length of PVC to size, you can? & Use a saw\\ \hline
Yes/No         & BoolQ &
Are there 51 states in the United States? & No \\ \hline
STEM   & \multirow{3}{*}{MMLU} &
The strongest base in liquid ammonia is? & $\text{NH}_2^-$ \\
Humanities & &
When did the first pharaohs emerge in Egypt? & 5100 B.P.\\
Social Sciences &  &
The government measures inflation using? & CPI
\\ \hline
\multirow{4}{*}{Science} & OpenBookQA &
What raw material is consumed by chloroplast? & $\text{CO}_2$\\
& SciQ & 
Which is the final step of cell division? & Cytokinesis \\
& ARC - (C)  &
How many valence electrons does selenium have?	& $6$ \\
& ARC - (E)  &
Where is water most likely to be brackish? & Estuary\\ \hline
Mathematics & MathQA & 
If $n = 2^{0.15}$ and $n^b = 8$ , $b$ must equal? & $20$\\ \hline
\multirow{3}{*}{Wikipedia} & SQuADv2 &
Where is the Mona Lisa housed? & The Louvre \\
& WikiQA & 
What is korean money called? & The won\\
& HotpotQA & 
EMU and Ugg boots both originated from where? & Australia\\ \hline
General & TriviaQA & 
In an opera, whose lover was Cavaradossi? & Tosca\\
\hline
\end{tabular}
\caption{\label{tab:overview}
Overview of the 12 question-answering datasets studied in this work, the domain coverage, and examples of the question-answer format. These datasets span traditional QA formats such as {\color{BrickRed} \textbf{Extractive}}, {\color{ForestGreen} \textbf{Multiple Choice}}, and {\color{Mulberry} \textbf{Abstractive}}. Our experiments treat all scenarios as text generation tasks, albeit with different prompting templates to align responses with the ground truth answer.
}
\vspace{-3mm}
\end{table*}