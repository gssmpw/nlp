[
  {
    "index": 0,
    "papers": [
      {
        "key": "NEURIPS2020_1457c0d6",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "key": "ouyang2022training",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "touvron2023llama",
        "author": "Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2023chainofthought",
        "author": "Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "nye2021work",
        "author": "Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena",
        "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2021makes",
        "author": "Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen",
        "title": "What Makes Good In-Context Examples for GPT-$3$?"
      },
      {
        "key": "reimers-2019-sentence-bert",
        "author": "Reimers, Nils and Gurevych, Iryna",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yu2023generate",
        "author": "Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng",
        "title": "Generate rather than retrieve: Large language models are strong context generators"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "10.1145/3491101.3519729",
        "author": "Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J",
        "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dohan2022language",
        "author": "David Dohan and Winnie Xu and Aitor Lewkowycz and Jacob Austin and David Bieber and Raphael Gontijo Lopes and Yuhuai Wu and Henryk Michalewski and Rif A. Saurous and Jascha Sohl-dickstein and Kevin Murphy and Charles Sutton",
        "title": "Language Model Cascades"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2023selfconsistency",
        "author": "Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kumar-etal-2022-gradient",
        "author": "Kumar, Sachin  and\nParia, Biswajit  and\nTsvetkov, Yulia",
        "title": "Gradient-based Constrained Sampling from Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "watson-etal-2025-law",
        "author": "Watson, William  and\nCho, Nicole  and\nSrishankar, Nishan  and\nZeng, Zhen  and\nCecchi, Lucas  and\nScott, Daniel  and\nSiddagangappa, Suchetha  and\nKaur, Rachneet  and\nBalch, Tucker  and\nVeloso, Manuela",
        "title": "{LAW}: Legal Agentic Workflows for Custody and Fund Services Contracts"
      },
      {
        "key": "10.1145/3677052.3698597",
        "author": "Cho, Nicole and Srishankar, Nishan and Cecchi, Lucas and Watson, William",
        "title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning"
      },
      {
        "key": "10.1145/3604237.3626908",
        "author": "Zeng, Zhen and Watson, William and Cho, Nicole and Rahimi, Saba and Reynolds, Shayleen and Balch, Tucker and Veloso, Manuela",
        "title": "FlowMind: Automatic Workflow Generation with LLMs"
      },
      {
        "key": "watson-etal-2023-hiddentables",
        "author": "Watson, William  and\nCho, Nicole  and\nBalch, Tucker  and\nVeloso, Manuela",
        "title": "{H}idden{T}ables and {P}y{QT}ax: A Cooperative Game and Dataset For {T}able{QA} to Ensure Scale and Data Privacy Across a Myriad of Taxonomies"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2022interpretingrobustnessneuralnlp",
        "author": "Yunxiang Zhang and Liangming Pan and Samson Tan and Min-Yen Kan",
        "title": "Interpreting the Robustness of Neural NLP Models to Textual Perturbations"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "moradi2021evaluatingrobustnessneurallanguage",
        "author": "Milad Moradi and Matthias Samwald",
        "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "slobodkin-etal-2023-curious",
        "author": "Slobodkin, Aviv  and\nGoldman, Omer  and\nCaciularu, Avi  and\nDagan, Ido  and\nRavfogel, Shauli",
        "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "azaria2023internalstatellmknows",
        "author": "Amos Azaria and Tom Mitchell",
        "title": "The Internal State of an LLM Knows When It's Lying"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mallen-etal-2023-trust",
        "author": "Mallen, Alex  and\nAsai, Akari  and\nZhong, Victor  and\nDas, Rajarshi  and\nKhashabi, Daniel  and\nHajishirzi, Hannaneh",
        "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories"
      }
    ]
  }
]