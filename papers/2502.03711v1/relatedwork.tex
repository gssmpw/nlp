\section{Related Work}
Large language models (LLMs), such as GPT-3, InstructGPT, and LLaMA \citep{NEURIPS2020_1457c0d6, ouyang2022training, touvron2023llama}, have demonstrated remarkable capabilities but also face challenges, such as hallucinations and sensitivity to input perturbations. To mitigate hallucinations, techniques like chain-of-thought \citep{wei2023chainofthought} and step-by-step generation \citep{nye2021work} have been proposed. Other strategies include augmenting generation with semantic retrieval \citep{liu2021makes,reimers-2019-sentence-bert}, generating context directly \citep{yu2023generate}, and crafting multi-step chains using tools like PromptChainer \citep{10.1145/3491101.3519729} or probabilistic programs \citep{dohan2022language}. Self-consistency \citep{wang2023selfconsistency} and gradient-based sampling \citep{kumar-etal-2022-gradient} further improve reliability by refining decoding processes. Modular architectures in legal and financial domains demonstrate how domain-specific tools and workflows can overcome context limitations and achieve competitive performance \citep{watson-etal-2025-law, 10.1145/3677052.3698597, 10.1145/3604237.3626908, watson-etal-2023-hiddentables}.

\paragraph{Perturbations in LLMs}
LLMs are highly sensitive to noisy inputs, which can impact real-world performance \citep{zhang2022interpretingrobustnessneuralnlp}, with existing benchmarks often failing to assess robustness accurately \citep{moradi2021evaluatingrobustnessneurallanguage}. \citet{slobodkin-etal-2023-curious} found that LLMs encode query answerability within their hidden states, suggesting potential avenues for decoding improvements. Similarly, \citet{azaria2023internalstatellmknows} demonstrated that LLMs' hidden states contain signals useful for detecting truthfulness. \citet{mallen-etal-2023-trust} highlighted the limitations of parametric memory in recalling long-tail knowledge and advocated for retrieval-augmented generation (RAG) systems to improve knowledge retention and efficiency.


\begin{table*}[!ht]
\centering
\small
\begin{tabular}{cl|lr}
\hline
\textbf{Domain}  & \textbf{Dataset}  & \textbf{Example Question} & \textbf{Answer} \\
\hline 
Falsehoods     & TruthfulQA &
Which is denser, water vapor or air? & Air\\ \hline
Physical Env.  & PIQA &
To cut a length of PVC to size, you can? & Use a saw\\ \hline
Yes/No         & BoolQ &
Are there 51 states in the United States? & No \\ \hline
STEM   & \multirow{3}{*}{MMLU} &
The strongest base in liquid ammonia is? & $\text{NH}_2^-$ \\
Humanities & &
When did the first pharaohs emerge in Egypt? & 5100 B.P.\\
Social Sciences &  &
The government measures inflation using? & CPI
\\ \hline
\multirow{4}{*}{Science} & OpenBookQA &
What raw material is consumed by chloroplast? & $\text{CO}_2$\\
& SciQ & 
Which is the final step of cell division? & Cytokinesis \\
& ARC - (C)  &
How many valence electrons does selenium have?	& $6$ \\
& ARC - (E)  &
Where is water most likely to be brackish? & Estuary\\ \hline
Mathematics & MathQA & 
If $n = 2^{0.15}$ and $n^b = 8$ , $b$ must equal? & $20$\\ \hline
\multirow{3}{*}{Wikipedia} & SQuADv2 &
Where is the Mona Lisa housed? & The Louvre \\
& WikiQA & 
What is korean money called? & The won\\
& HotpotQA & 
EMU and Ugg boots both originated from where? & Australia\\ \hline
General & TriviaQA & 
In an opera, whose lover was Cavaradossi? & Tosca\\
\hline
\end{tabular}
\caption{\label{tab:overview}
Overview of the 12 question-answering datasets studied in this work, the domain coverage, and examples of the question-answer format. These datasets span traditional QA formats such as {\color{BrickRed} \textbf{Extractive}}, {\color{ForestGreen} \textbf{Multiple Choice}}, and {\color{Mulberry} \textbf{Abstractive}}. Our experiments treat all scenarios as text generation tasks, albeit with different prompting templates to align responses with the ground truth answer.
}
\vspace{-3mm}
\end{table*}