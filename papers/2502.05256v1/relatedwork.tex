\section{Related work}
Query optimization is a long-standing problem in the databases community. System R~\cite{system_r} proposed the heuristic query optimization scheme now used in most production databases~\cite{cascades}, consisting of a cost model, cardinality estimates, and a dynamic programming search.
Conventional query optimizers are designed according to the ``query optimization contract''~\cite{chaudhuri2009_rethinking_qo}, which expects query optimizers to produce plans quickly (within hundreds of milliseconds, as the actual execution of the plan might be very quick); this work proposed that in order to improve query optimizers, we should consider ``breaking'' this contract in a number of ways, such as allowing the optimizer to intrusively examine the base data (as opposed to keeping cheap histograms), spend a long time on optimization, or even adaptively change the query plan during execution. We believe our work falls into this ``breaking the contract'' category. Older work~\cite{fittest_qo} considered amortizing the cost of searching parts of the plan space across multiple executions, but did not consider offline execution. Query \emph{reoptimization}, perhaps the first ``contract breaker,'' is the task of proactively modifying or recreating a query plan during execution, based on information found during execution, with the overall goal of minimizing total latency~\cite{reopt,rio_reopt,inc_reopt}.


A related concept from the compilers literature is ``superoptimization''~\cite{superopt_coined}, in which a program compiler, which traditionally follows a similar ``contract'' as a query optimizer (i.e., fast compilation times), instead uses a large time budget to produce the best possible sequence of assembly instructions for a given program. Our work can be considered a sort of ``superoptimization for query plans''. GenesisDB~\cite{genesisdb} represents a similar effort, focusing on developing fast implementations over relational operators, instead of entire query plans (thus GenesisDB is mostly orthogonal to the work presented here). Kepler~\cite{kepler} uses a genetic algorithm and exhaustive execution to map the plan space for parameterized queries, which can be viewed as a type of superoptimization. SlabCity~\cite{slabcity} takes an approach similar to traditional superoptimization, by considering SQL-level semantic rewrites of queries to improve performance (e.g., query simplification). Finally, DataFarm~\cite{datafarm} \edit{and HitTheGym~\cite{hitthegym}} investigated how to best produce datasets for machine learning powered database components, including query optimizers.

In recent years, the databases community has been increasingly engaged in applying machine learning techniques to query optimization, including latency prediction~\cite{learning_latency,zeroshot_latency_model,qppnet,contender,jennie_sigmod11,stage}, cardinality estimation~\cite{deep_card_est2, flowloss, neurocard, quicksel, geom_card_est, gridar_lce, asm_lce,alece_lce, robopt}, and cost models~\cite{learn_cost}. Other works have attempted to either augment existing optimizers with learned components (e.g.,~\cite{bao,fastgres,workload_reopt,leon_qo,hybrid_lqo,opengauss}) or entirely replace query optimizers with reinforcement learning (e.g.,~\cite{neo,balsa,lero,pilotscope, eraser_lqo, roq, loger, qo_rank, q_transformer, mcts_qo, lstm_jo, skinnerdb}). Most of these works are focused on the online optimization setting: they must complete quickly while avoiding performance regressions relative to traditional heuristic-based optimizers. Most of these works also employ reinforcement learning, seeking to manage regret from exploring alternatives instead of exploiting the current known-best plan. In comparison, we apply Bayesian optimization to the superoptimization problem because we are principally concerned with finding the query plan with the best possible latency, and ignore suboptimal plans. In the superoptimization setting, bad plans are only bad insofar as executing them until the timeout consumes part of the optimization time budget.

\edit{Bayesian optimization is not the only sample-efficient learning technique. For example, NeuroCARD~\cite{neurocard} learns join distributions efficiently by uniformly sampling tuples from the full outer join of all tables in a schema. Reiner et al.~\cite{geom_card_est} show how domain knowledge can be incorporated into learned models to improve sample efficiency via geometric deep learning. LlamaTune~\cite{kanellis_llamatune} uses database documentation to accelerate DBMS knob-tuning.}


\edit{While our plan encoding was inspired by work in molecular dynamics (i.e, SELFIES~\cite{selfies} strings as used by Maus et al.~\cite{bayes_latent}), a representation with similar goals for query plans was presented by Reiner et al.~\cite{geom_card_est}. Our approaches mainly differ in what we are trying to represent: as our format only seeks to encode join orderings, it does not encode predicates. Furthermore, while Reiner et al. use invariances to give joins with the same cardinality the same representation, our encoding format may have multiple representations for the same join ordering. Further motivation for this design choice is given in ~\Cref{sec:string-format}. Other works have also looked at non-string representations of queries based on graphs~\cite{geom_card_est}, trees~\cite{neo}, and recurrences~\cite{tree_lstm}, typically by using neural network architectures which model these structures.}

% Learned query optimizers (neo, bao, lero, balsa, whatever other billion of them) which introduce a feedback loop, but do not superoptimize. Difference between RL and BO: RL seeks to minimize *sum* of all attempted latencies, BO only seeks to minmize the minimal latency.

The random search heuristic we presented in ~\Cref{sec:eval} can be considered a modified version of QuickPick~\cite{quickpick}. Instead of sampling random query plans and then using a cost model to evaluate their quality, we simply evaluate the quality of the random plans by actually executing them. Such a suggestion would seem ludicrous in the original context of~\cite{quickpick}, but for offline optimization, executing terrible query plans is \emph{not} off the table, if it eventually leads to a better plan! 

Since Bayesian optimization is a relatively old technique, it may be reasonable to ask ``why now?'' Recent innovations in the machine learning community have made it practical to apply Bayesian optimization to \emph{structured} (i.e. non-continuous) inputs with high dimensionality~\cite{lolbo,high_dim_bo,bayes_local}, which was previously impossible. The key innovation that enabled this advancement was attention transformer models~\cite{attention}, which allowed sequences to be efficiently and accurately mapped into vector spaces. In the databases literature, Bayesian optimization has been most frequently applied to tuning configuration knobs~\cite{database_hyperparameter_optimization, cgptuner, gptuner}. To our knowledge, this is the first work to apply BO directly to the optimization of individual queries.

\edit{Perhaps most similar to this work is LimeQO~\cite{limeqo}, a system that uses offline query execution to find the best query hint for each query in a workload. LimeQO can be viewed as a practical way of finding an optimal Bao~\cite{bao} model for a given workload. LimeQO is arguably much simpler than the present work, requiring only linear methods (e.g., no VAE or Bayesian optimization). However, LimeQO only considers a finite set of query hints to apply to each query in a workload, whereas we fully construct query plans. As a result, the present work can potentially find better plans. Additionally, LimeQO focuses on optimizing an entire workload of queries at once (i.e., considering which queries are best to explore next), whereas we focus on optimizing only a single user-specified queries.}