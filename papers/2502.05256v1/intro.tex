
\section{Introduction}

Query optimization is a long-standing problem in the database community~\cite{system_r,howgood,qo_unsolved}. Recent advancements in \emph{learned} query optimization (LQO)~\cite{balsa,lero,roq,loger,hybrid_lqo,lstm_jo,qo_state_rep,rejoin,hero_qo,concurrent_lqo,leon_qo} have shown significant promise, often delivering  2-10x improvements in query runtime. However, deploying LQO is complicated due to two main challenges: (1) query regressions (``my query was fast yesterday, why is it slow today?'') and (2) the tight integration of machine learning components into the core query processing pipeline (which are generally engineered with different levels of reliability in mind).

Despite various efforts to address these challenges~\cite{bao,fastgres}, most real-world deployments of LQO (such as at Meta~\cite{autosteer}, Microsoft~\cite{bao_scope, bao_scope2}, and Alibaba~\cite{eraser_lqo,pilotscope}) have separated learned query optimization two components, an offline component and an online component. The \emph{offline component} tests new query plans and caches those plans that perform better than the plans produced by the traditional optimizer. The \emph{online component} then checks this cache for a plan; if a cached plan is not found, it calls the traditional optimizer instead.

This compromise --- spending additional resources offline to find good query plans for specific queries --- solves issues with query regressions and avoids the need to put ML primitives into the query processing pipeline, but it is also motivated by the nature of analytic workloads. In many analytic systems, the majority of compute resources are spent executing repetitive report generation~\cite{superopt_vision} or dashboarding queries~\cite{redshift_pred_cache}, with some queries being executed hundreds of times per day~\cite{stage} or hundreds of thousands of times per year~\cite{superopt_vision}. \edit{Recent studies of Amazon Redshift showed that, for the median database, 60\% of all queries executed were repeated queries (verbatim)~\cite{wu_stage_redshift} and that roughly 10\% of all Redshift clusters have their entire workload consisting of queries that repeated within the last day ~\cite{redshift_workload}.} \footnote{\edit{Some unknown proportion of these verbatim repeats may involve staging tables or views, for which the underlying  query may be changing.}} Substantial repetition means that even a small improvement in query latency can be amplified many times over, making it worthwhile to invest additional optimization resources.

We call the goal of these offline components the \textbf{offline query optimization problem}: to find the best query plan using as few offline resources (i.e., offline query time) as possible. Unlike traditional query optimizers, which generally seek to be so fast that optimization time amortizes to zero compared to execution time, an offline query optimizer is expected to take many times longer than a single query execution. 

Learned query optimizers using the ``offline/online'' compromise are implicitly performing offline query optimization. Current systems must solve two fundamental problems: first, offline query optimizers must have a \emph{search strategy} to decide which plans to test. Second, offline query optimizers must have a \emph{timeout strategy} to deal with query plans that take too long to execute. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/bayes_vs_rl-crop.pdf}
    \caption{A comparison of reinforcement learning (RL, left) and Bayesian optimization (BO, right). BO is a better match for the offline query optimization problem because we do not care about ``regressions'' in the offline search phase; we only care about the quality of the best-discovered plan.}
    \label{fig:bayes_vs_rl}
\end{figure}

\sparagraph{Search strategy} Existing techniques either use a coarse-grained search of predefined alternative plans~\cite{autosteer,bao_scope2}, or adopt a fine-grained reinforcement learning procedure~\cite{pilotscope,eraser_lqo}. The biggest drawback of the coarse-grained approach is that the set of plans explored is limited, and significant improvements may be outside of the search space. The drawback of the RL approach is more subtle.

\emph{Reinforcement learning (RL) is fundamentally the ``wrong tool for the job'' of offline query optimization.} Specifically, the objective function of reinforcement learning is poorly aligned with offline query optimization. Consider the latency of a query over time as the query is optimized, as depicted on the left side of Figure~\ref{fig:bayes_vs_rl}. RL seeks to minimize the shaded area, representing latency (negative reward) over time. This corresponds to balancing exploration and exploitation~\cite{rl_book}: the optimizer is penalized each time it chooses a bad plan, so the optimizer must frequently choose ``lower risk'' plans that lower the area under the curve, but might not be as informative as ``higher risk'' plans. This is perfectly aligned with the goals of \emph{online} query optimization, however, in the context of \emph{offline} query optimization, what we really care about is finding the best query plan observed during the time-bounded optimization phase (that is, the minimum of the plotted function, not the area under the curve), as shown on the right side of Figure~\ref{fig:bayes_vs_rl}.

\sparagraph{Trouble with timeouts} An important, but often overlooked, dimension of learned query optimization is query timeouts: since some query plans are orders of magnitude worse than others~\cite{howgood}, any learned query optimizer (either online or offline) has a non-zero chance of hitting a poor-performing plan. Current approaches solve this fundamental dilemma in ad-hoc ways, often by ``timing out'' (termination prior to completion) proposed query plans after a fixed threshold. These timed-out values are problematic because (1) they represented a large amount of wasted time (a poor query plan was selected~\cite{balsa}) and (2) no new information was gained (since updating an RL model with the timed-out value would cause the model to strictly underestimate the cost of the timed-out query~\cite{neo}). Thus, a successful offline query optimizer must have a strategy for \emph{selecting timeout values} and for \emph{learning from timed-out queries}.


\sparagraph{Bayesian optimization (BO).} We propose an offline query optimizer, \sysname, that closely mirrors recent work in computational drug discovery~\cite{lolbo}: we use a learned encoder and decoder to translate query plans to and from vectors (called the \emph{latent space}), such that similar query plans are mapped to nearby vectors. Then, off-the-shelf and well-studied Bayesian optimization techniques manipulate the encoded vectors in the latent space, using query execution as a reward signal. Existing coarse-grained techniques that generate a fixed set of plan variants for each query can be used to initialize the process, ensuring that the best-found plan is at least as good as the best plan in the initialization set.  

We show that off-the-shelf BO techniques~\cite{bayes_survey} can be easily adapted to the offline query optimization objective (that is, finding the fastest possible plan in the least amount of time). Furthermore, we show that the framework of Bayesian optimization enables robust \emph{learning from timeouts} as well as \emph{selecting timeouts} on a learned, plan-by-plan basis. In other words, we can meaningfully represent ``query latency $> x$'' within the learned model as a \emph{censored observation}, and our model's confidence intervals provide a robust way of selecting timeouts to maximize information gain. Finally, we show how cross-query information can be incorporated into \sysname by fine-tuning a language model to provide database-specific initialization points for the BO search process.


In our experiments, we show that offline optimization can yield 10-100x performance improvements over prior learned query optimization for some queries, and we demonstrate that our approach can find modest improvements for nearly every query in several benchmarks. Since our system targets repetitive analytic queries, even modest gains can be significantly amplified in practical settings. Our contributions include:


%Analytic queries are highly repetitive, with some being run 100K+ times per year over slightly-shifting data (e.g., dashboards). Surprisingly, a wide class of these queries are exactly identical (not even different parameterizations of the same template). \ryan{citations} Examples at Meta (autosteer), Amazon (Kipf's paper or one of the other 6 Redshift had at SIGMOD), on-premise warehouses (superopt paper). These queries are attractive targets for more aggressive optimization, as any gains in query performance can be amplified by 100K-$\times$. For example, if a dashboard is refreshed once every hour for a year and curently takes 5 minutes to execute the same query each time it refreshes, we can easily spend tens of compute-hours improving the runtime to 4 minutes and still save in overall computation time. Spending large amounts of time to eek out every last bit of performance from an important task is often called \emph{superoptimization}.

% Moreover, it is not currently possible to utilize existing query optimization algorithms to approach this problem. Conventional optimization algorithms deployed in state-of-the-art DBMSs (henceforth, ``traditional optimizers'') try to minimize plan runtime by applying a cost model to candidate plans. These algorithms have been designed for \emph{online} optimization, just before the resulting optimized plan is to be executed to return results, so the optimization process itself must run quickly so as not to impose too much performance overhead over short-running queries. Once they have emitted the predicted lowest-cost plan, they are finished and cannot be asked to work harder on the problem to find a more optimized plan. 

% It is well-known \cite{job} that traditional optimizers using cardinality estimation frequently fail to find the optimal plan, and that the plan of lowest estimated cost can have a runtime several orders of magnitude greater than the optimum. Optimal plans cannot be found via these traditional algorithms. Thus, when superoptimizing queries \emph{offline}, it is helpful to execute query plans and measure their actual run times; this is what the majority of the computational time budget of a query superoptimization algorithm will be spent on.

% Recently-developed learned techniques for query optimization are also typically focused on online optimization. \jeff{Citations, why they aren't competitive superoptimizers}


%In this work, we present the following contributions:

\begin{enumerate}[leftmargin=*]
    \item We formalize the problem of offline query optimization,
    \item We implement \sysname, an offline query optimizer that applies Bayesian optimization techniques,
    %\item We present an encoding format for query plans in order to define a latent space that Bayesian optimization can be performed in
    \item We show  how recent developments in Bayesian optimization that accommodate high-dimensionality and censored observations can be integrated into \sysname,
    \item We show how fine-tuning a language model can be used to incorporate cross-query information into \sysname,
    \item We show that \sysname can outperform online learned query optimization techniques in an offline setting.
\end{enumerate}
