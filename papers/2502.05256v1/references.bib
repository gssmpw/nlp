% =========
% Bayes Opt
% =========

@article{distill_bayesopt,
  author = {Agnihotri, Apoorv and Batra, Nipun},
  title = {Exploring Bayesian Optimization},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/bayesian-optimization},
  doi = {10.23915/distill.00026}
}

@misc{hutter2013_bocensored,
      title={Bayesian Optimization With Censored Response Data}, 
      author={Frank Hutter and Holger Hoos and Kevin Leyton-Brown},
      year={2013},
      eprint={1310.1947},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1310.1947}, 
}

@misc{eggensperger2020_censored,
      title={Neural Model-based Optimization with Right-Censored Observations}, 
      author={Katharina Eggensperger and Kai Haase and Philipp Müller and Marius Lindauer and Frank Hutter},
      year={2020},
      eprint={2009.13828},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2009.13828}, 
}

@inproceedings{Weighted_Retraining,
    author    = {Austin Tripp and
               Erik A. Daxberger and
               Jos{\'{e}} Miguel Hern{\'{a}}ndez{-}Lobato},
    title     = {Sample-Efficient Optimization in the Latent Space of Deep Generative
               Models via Weighted Retraining},
    booktitle = {Advances in Neural Information Processing Systems 33},
    year      = {2020},
}

@article{ladder,
  author    = {Aryan Deshwal and
               Janardhan Rao Doppa},
  title     = {Combining Latent Space and Structured Kernels for {Bayesian} Optimization
               over Combinatorial Spaces},
  journal   = {CoRR},
  volume    = {abs/2111.01186},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.01186},
  eprinttype = {arXiv},
  eprint    = {2111.01186},
  timestamp = {Fri, 05 Nov 2021 15:25:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-01186.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gomez2018automatic,
  title={Automatic chemical design using a data-driven continuous representation of molecules},
  author={G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n},
  journal={ACS central science},
  volume={4},
  number={2},
  pages={268--276},
  year={2018},
  publisher={ACS Publications}
}

@article{Huawei,
  author    = {Antoine Grosnit and
               Rasul Tutunov and
               Alexandre Max Maraval and
               Ryan{-}Rhys Griffiths and
               Alexander Imani Cowen{-}Rivers and
               Lin Yang and
               Lin Zhu and
               Wenlong Lyu and
               Zhitang Chen and
               Jun Wang and
               Jan Peters and
               Haitham Bou{-}Ammar},
  title     = {High-Dimensional {Bayesian} Optimisation with Variational Autoencoders
               and Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/2106.03609},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2106.03609},
  timestamp = {Tue, 15 Jun 2021 11:51:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-03609.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{eissman2018bayesian,
  title={Bayesian optimization and attribute adjustment},
  author={Eissman, Stephan and Levy, Daniel and Shu, Rui and Bartzsch, Stefan and Ermon, Stefano},
  booktitle={Proc. 34th Conference on Uncertainty in Artificial Intelligence},
  year={2018}
}

@article{selfies,
	title = {Self-{Referencing} {Embedded} {Strings} ({SELFIES}): {A} 100\% robust molecular string representation},
	volume = {1},
	issn = {2632-2153},
	shorttitle = {Self-{Referencing} {Embedded} {Strings} ({SELFIES})},
	url = {http://arxiv.org/abs/1905.13741},
	doi = {10.1088/2632-2153/aba947},
	abstract = {The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering -- generally denoted as inverse design -- was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100{\textbackslash}\% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.},
	language = {en},
	number = {4},
	urldate = {2023-10-17},
	journal = {Machine Learning: Science and Technology},
	author = {Krenn, Mario and Häse, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alán},
	month = dec,
	year = {2020},
	note = {arXiv:1905.13741 [physics, physics:quant-ph, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Chemical Physics, Quantum Physics},
	pages = {045024},
	annote = {Comment: 6+3 pages, 6+1 figures},
	file = {Krenn et al. - 2020 - Self-Referencing Embedded Strings (SELFIES) A 100.pdf:/home/jtao/Zotero/storage/SGXFHI67/Krenn et al. - 2020 - Self-Referencing Embedded Strings (SELFIES) A 100.pdf:application/pdf},
}

@inproceedings{kajino2019molecular,
  title={Molecular hypergraph grammar with its application to molecular optimization},
  author={Kajino, Hiroshi},
  booktitle={International Conference on Machine Learning},
  pages={3183--3191},
  year={2019},
  organization={PMLR}
}

@inproceedings{lolbo,
author = {Maus, Natalie T. and Jones, Haydn T. and Moore, Juston S. and Kusner, Matt J. and Bradshaw, John and Gardner, Jacob R.},
title = {Local latent space Bayesian optimization over structured inputs},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian optimization over the latent spaces of deep autoencoder models (DAEs) has recently emerged as a promising new approach for optimizing challenging black-box functions over structured, discrete, hard-to-enumerate search spaces (e.g., molecules). Here the DAE dramatically simplifies the search space by mapping inputs into a continuous latent space where familiar Bayesian optimization tools can be more readily applied. Despite this simplification, the latent space typically remains high-dimensional. Thus, even with a well-suited latent space, these approaches do not necessarily provide a complete solution, but may rather shift the structured optimization problem to a high-dimensional one. In this paper, we propose LOL-BO, which adapts the notion of trust regions explored in recent work on high-dimensional Bayesian optimization to the structured setting. By reformulating the encoder to function as both an encoder for the DAE globally and as a deep kernel for the surrogate model within a trust region, we better align the notion of local optimization in the latent space with local optimization in the input space. LOL-BO achieves as much as 20 times improvement over state-of-the-art latent space Bayesian optimization methods across six real-world benchmarks, demonstrating that improvement in optimization strategies is as important as developing better DAE models.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2500},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{KingmaW13,
  author       = {Diederik P. Kingma and
                  Max Welling},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Auto-Encoding Variational Bayes},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6114},
}

@inproceedings{eriksson2019scalable,
  title = {Scalable Global Optimization via Local {Bayesian} Optimization},
  author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {5496--5507},
  year = {2019},
  url = {http://papers.nips.cc/paper/8788-scalable-global-optimization-via-local-bayesian-optimization.pdf},
}

@inproceedings{pie,
    title={\href{https://openreview.net/pdf?id=ix7rLVHXyY}{Learning Performance-Improving Code Edits}},
    author={Shypula, Alexander and Madaan, Aman and Zeng, Yimeng and Alon, Uri and Gardner, Jacob and Hashemi, Milad and Neubig, Graham and Ranganathan, Parthasarathy and Bastani, Osbert and Yazdanbakhsh, Amir},
    booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
    year={2024},
    URL = {https://openreview.net/pdf?id=ix7rLVHXyY},
}

% ================
% Bayes Opt Usages
% ================

@article{gptuner, author = {Lao, Jiale and Wang, Yibo and Li, Yufei and Wang, Jianping and Zhang, Yunjia and Cheng, Zhiyuan and Chen, Wanghu and Tang, Mingjie and Wang, Jianguo}, title = {GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian Optimization}, year = {2024}, issue_date = {April 2024}, publisher = {VLDB Endowment}, volume = {17}, number = {8}, issn = {2150-8097}, url = {https://doi.org/10.14778/3659437.3659449}, doi = {10.14778/3659437.3659449}, abstract = {Modern database management systems (DBMS) expose hundreds of configurable knobs to control system behaviours. Determining the appropriate values for these knobs to improve DBMS performance is a long-standing problem in the database community. As there is an increasing number of knobs to tune and each knob could be in continuous or categorical values, manual tuning becomes impractical. Recently, automatic tuning systems using machine learning methods have shown great potentials. However, existing approaches still incur significant tuning costs or only yield sub-optimal performance. This is because they either ignore the extensive domain knowledge available (e.g., DBMS manuals and forum discussions) and only rely on the runtime feedback of benchmark evaluations to guide the optimization, or they utilize the domain knowledge in a limited way. Hence, we propose GPTuner, a manual-reading database tuning system that leverages domain knowledge extensively and automatically to optimize search space and enhance the runtime feedback-based optimization process. Firstly, we develop a Large Language Model (LLM)-based pipeline to collect and refine heterogeneous knowledge, and propose a prompt ensemble algorithm to unify a structured view of the refined knowledge. Secondly, using the structured knowledge, we (1) design a workload-aware and training-free knob selection strategy, (2) develop a search space optimization technique considering the value range of each knob, and (3) propose a Coarse-to-Fine Bayesian Optimization Framework to explore the optimized space. Finally, we evaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics (throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to the state-of-the-art approaches, GPTuner identifies better configurations in 16x less time on average. Moreover, GPTuner achieves up to 30\% performance improvement (higher throughput or lower latency) over the best-performing alternative.}, journal = {Proc. VLDB Endow.}, month = {may}, pages = {1939–1952}, numpages = {14} }

@article{cgptuner, author = {Cereda, Stefano and Valladares, Stefano and Cremonesi, Paolo and Doni, Stefano}, title = {CGPTuner: a contextual gaussian process bandit approach for the automatic tuning of IT configurations under varying workload conditions}, year = {2021}, issue_date = {April 2021}, publisher = {VLDB Endowment}, volume = {14}, number = {8}, issn = {2150-8097}, url = {https://doi.org/10.14778/3457390.3457404}, doi = {10.14778/3457390.3457404}, abstract = {Properly selecting the configuration of a database management system (DBMS) is essential to increase performance and reduce costs. However, the task is astonishingly tricky due to a large number of tunable configuration parameters and their inter-dependencies. Also, the optimal configuration depends upon the workload to which the DBMS is exposed. To extract the full potential of a DBMS, we must also consider the entire IT stack on which the DBMS is running, comprising layers like the Java virtual machine, the operating system and the physical machine. Each layer offers a multitude of parameters that we should take into account. The available parameters vary as new software versions are released, making it impractical to rely on historical knowledge bases. We present a novel tuning approach for the DBMS configuration auto-tuning that quickly finds a well-performing configuration of an IT stack and adapts it to workload variations, without having to rely on a knowledge base. We evaluate the proposed approach using the Cassandra and MongoDB DBMSs, showing that it adjusts the suggested configuration to the observed workload and is portable across different IT applications. We try to minimise the memory consumption without increasing the response time, showing that the proposed approach reduces the response time and increases the memory requirements only under heavy-load conditions, reducing it again when the load decreases.}, journal = {Proc. VLDB Endow.}, month = {apr}, pages = {1401–1413}, numpages = {13} }

@article{database_hyperparameter_optimization, author = {Zhang, Xinyi and Chang, Zhuo and Li, Yang and Wu, Hong and Tan, Jian and Li, Feifei and Cui, Bin}, title = {Facilitating database tuning with hyper-parameter optimization: a comprehensive experimental evaluation}, year = {2022}, issue_date = {May 2022}, publisher = {VLDB Endowment}, volume = {15}, number = {9}, issn = {2150-8097}, url = {https://doi.org/10.14778/3538598.3538604}, doi = {10.14778/3538598.3538604}, abstract = {Recently, using automatic configuration tuning to improve the performance of modern database management systems (DBMSs) has attracted increasing interest from the database community. This is embodied with a number of systems featuring advanced tuning capabilities being developed. However, it remains a challenge to select the best solution for database configuration tuning, considering the large body of algorithm choices. In addition, beyond the applications on database systems, we could find more potential algorithms designed for configuration tuning. To this end, this paper provides a comprehensive evaluation of configuration tuning techniques from a broader perspective, hoping to better benefit the database community. In particular, we summarize three key modules of database configuration tuning systems and conduct extensive ablation studies using various challenging cases. Our evaluation demonstrates that the hyper-parameter optimization algorithms can be borrowed to further enhance the database configuration tuning. Moreover, we identify the best algorithm choices for different modules. Beyond the comprehensive evaluations, we offer an efficient and unified database configuration tuning benchmark via surrogates that reduces the evaluation cost to a minimum, allowing for extensive runs and analysis of new techniques.}, journal = {Proc. VLDB Endow.}, month = {may}, pages = {1808–1821}, numpages = {14} }

# ==============================
# Traditional Query Optimization
# ==============================

@inproceedings{quickpick,
  title={Join Order Selection - Good Enough Is Easy},
  author={F. Michael Waas and Arjan Pellenkoft},
  booktitle={British National Conference on Databases},
  year={2000},
  url={https://api.semanticscholar.org/CorpusID:15111246}
}

@article{job,
author = {Leis, Viktor and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
title = {How good are query optimizers, really?},
year = {2015},
issue_date = {November 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850583.2850594},
doi = {10.14778/2850583.2850594},
abstract = {Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {204–215},
numpages = {12}
}

@inproceedings{chaudhuri2009_rethinking_qo,
author = {Chaudhuri, Surajit},
title = {Query optimizers: time to rethink the contract?},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559955},
doi = {10.1145/1559845.1559955},
abstract = {Query Optimization is expected to produce good execution plans for complex queries while taking relatively small optimization time. Moreover, it is expected to pick the execution plans with rather limited knowledge of data and without any additional input from the application. We argue that it is worth rethinking this prevalent model of the optimizer. Specifically, we discuss how the optimizer may benefit from leveraging rich usage data and from application input. We conclude with a call to action to further advance query optimization technology.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {961–968},
numpages = {8},
keywords = {cardinality estimation, query optimizer},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@article{looking_glass_job,
author = {Leis, Viktor and Radke, Bernhard and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
title = {Query optimization through the looking glass, and what we found running the Join Order Benchmark},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0480-7},
doi = {10.1007/s00778-017-0480-7},
abstract = {Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark that works on real-life data riddled with correlations and introduces 113 complex join queries. We experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. For this purpose, we describe cardinality-estimate injection and extraction techniques that allow us to compare the cardinality estimators of multiple industrial SQL implementations on equal footing, and to characterize the value of having perfect cardinality estimates. Our investigation shows that all industrial-strength cardinality estimators routinely produce large errors: though cardinality estimation using table samples solves the problem for single-table queries, there are still no techniques in industrial systems that can deal accurately with join-crossing correlated query predicates. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. We investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the suboptimal cardinality estimates. Finally, we extend our investigation from main-memory only, to also include disk-based query processing. Here, we find that though accurate cardinality estimation should be the first priority, other aspects such as modeling random versus sequential I/O are also important to predict query runtime.},
journal = {The VLDB Journal},
month = {oct},
pages = {643–668},
numpages = {26},
keywords = {Cardinality estimation, Cost models, Join ordering, Query optimization}
}

@article{system_r,
author = {Astrahan, M. M. and Blasgen, M. W. and Chamberlin, D. D. and Eswaran, K. P. and Gray, J. N. and Griffiths, P. P. and King, W. F. and Lorie, R. A. and McJones, P. R. and Mehl, J. W. and Putzolu, G. R. and Traiger, I. L. and Wade, B. W. and Watson, V.},
title = {System R: relational approach to database management},
year = {1976},
issue_date = {June 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/320455.320457},
doi = {10.1145/320455.320457},
abstract = {System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.},
journal = {ACM Trans. Database Syst.},
month = {jun},
pages = {97–137},
numpages = {41},
keywords = {relational model, recovery, nonprocedural language, locking, index structures, database, data structures, authorization}
}

@article{cascades,
  author    = {Goetz Graefe},
  title     = {The Cascades Framework for Query Optimization},
  journal   = {IEEE Data Eng. Bull.},
  volume    = {18},
  number    = {3},
  year      = {1995},
  pages     = {19-29},
  ee        = {db/journals/debu/Graefe95a.html},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

% ===========================
% Learned  Query Optimization
% ===========================

@inproceedings{balsa,
	address = {Philadelphia PA USA},
	title = {Balsa: {Learning} a {Query} {Optimizer} {Without} {Expert} {Demonstrations}},
	isbn = {978-1-4503-9249-5},
	shorttitle = {Balsa},
	url = {https://dl.acm.org/doi/10.1145/3514221.3517885},
	doi = {10.1145/3514221.3517885},
	abstract = {Query optimizers are a performance-critical component in every database system. Due to their complexity, optimizers take experts months to write and years to refine. In this work, we demonstrate for the first time that learning to optimize queries without learning from an expert optimizer is both possible and efficient. We present Balsa, a query optimizer built by deep reinforcement learning. Balsa first learns basic knowledge from a simple, environmentagnostic simulator, followed by safe learning in real execution. On the Join Order Benchmark, Balsa matches the performance of two expert query optimizers, both open-source and commercial, with two hours of learning, and outperforms them by up to 2.8× in workload runtime after a few more hours. Balsa thus opens the possibility of automatically learning to optimize in future compute environments where expert-designed optimizers do not exist.},
	language = {en},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Yang, Zongheng and Chiang, Wei-Lin and Luan, Sifei and Mittal, Gautam and Luo, Michael and Stoica, Ion},
	month = jun,
	year = {2022},
	pages = {931--944},
	file = {Yang et al. - 2022 - Balsa Learning a Query Optimizer Without Expert D.pdf:/home/jtao/Zotero/storage/VYWZBCP4/Yang et al. - 2022 - Balsa Learning a Query Optimizer Without Expert D.pdf:application/pdf},
}

@inproceedings{bao,
author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
title = {Bao: Making Learned Query Optimization Practical},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452838},
doi = {10.1145/3448016.3452838},
abstract = {Recent efforts applying machine learning techniques to query optimization have shown few practical gains due to substantive training overhead, inability to adapt to changes, and poor tail performance. Motivated by these difficulties, we introduce Bao (the underlineBa ndit underlineo ptimizer). Bao takes advantage of the wisdom built into existing query optimizers by providing per-query optimization hints. Bao combines modern tree convolutional neural networks with Thompson sampling, a well-studied reinforcement learning algorithm. As a result, Bao automatically learns from its mistakes and adapts to changes in query workloads, data, and schema. Experimentally, we demonstrate that Bao can quickly learn strategies that improve end-to-end query execution performance, including tail latency, for several workloads containing long-running queries. In cloud environments, we show that Bao can offer both reduced costs and better performance compared with a commercial system.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1275–1288},
numpages = {14},
keywords = {machine learning, query optimization, reinforcement learning},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{flowloss,
author = {Negi, Parimarjan and Marcus, Ryan and Kipf, Andreas and Mao, Hongzi and Tatbul, Nesime and Kraska, Tim and Alizadeh, Mohammad},
title = {Flow-Loss: Learning Cardinality Estimates That Matter},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476259},
doi = {10.14778/3476249.3476259},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2019–2032},
numpages = {14}
}

@article{neo,
author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
title = {Neo: a learned query optimizer},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342644},
doi = {10.14778/3342263.3342644},
abstract = {Query optimization is one of the most challenging problems in database systems. Despite the progress made over the past decades, query optimizers remain extremely complex components that require a great deal of hand-tuning for specific workloads and datasets. Motivated by this shortcoming and inspired by recent advances in applying machine learning to data management challenges, we introduce Neo (Neural Optimizer), a novel learning-based query optimizer that relies on deep neural networks to generate query executions plans. Neo bootstraps its query optimization model from existing optimizers and continues to learn from incoming queries, building upon its successes and learning from its failures. Furthermore, Neo naturally adapts to underlying data patterns and is robust to estimation errors. Experimental results demonstrate that Neo, even when bootstrapped from a simple optimizer like PostgreSQL, can learn a model that offers similar performance to state-of-the-art commercial optimizers, and in some cases even surpass them.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1705–1718},
numpages = {14}
}

@article{lero,
author = {Zhu, Rong and Chen, Wei and Ding, Bolin and Chen, Xingguang and Pfadler, Andreas and Wu, Ziniu and Zhou, Jingren},
title = {Lero: A Learning-to-Rank Query Optimizer},
year = {2023},
issue_date = {February 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3583140.3583160},
doi = {10.14778/3583140.3583160},
abstract = {A recent line of works apply machine learning techniques to assist or rebuild cost-based query optimizers in DBMS. While exhibiting superiority in some benchmarks, their deficiencies, e.g., unstable performance, high training cost, and slow model updating, stem from the inherent hardness of predicting the cost or latency of execution plans using machine learning models. In this paper, we introduce a learning-to-rank query optimizer, called Lero, which builds on top of a native query optimizer and continuously learns to improve the optimization performance. The key observation is that the relative order or rank of plans, rather than the exact cost or latency, is sufficient for query optimization. Lero employs a pairwise approach to train a classifier to compare any two plans and tell which one is better. Such a binary classification task is much easier than the regression task to predict the cost or latency, in terms of model efficiency and accuracy. Rather than building a learned optimizer from scratch, Lero is designed to leverage decades of wisdom of databases and improve the native query optimizer. With its non-intrusive design, Lero can be implemented on top of any existing DBMS with minimal integration efforts. We implement Lero and demonstrate its outstanding performance using PostgreSQL. In our experiments, Lero achieves near optimal performance on several benchmarks. It reduces the plan execution time of the native optimizer in PostgreSQL by up to 70\% and other learned query optimizers by up to 37\%. Meanwhile, Lero continuously learns and automatically adapts to query workloads and changes in data.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {1466–1479},
numpages = {14}
}










@article{weininger_smiles,
author = {Weininger, David},
title = {SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
journal = {Journal of Chemical Information and Computer Sciences},
volume = {28},
number = {1},
pages = {31-36},
year = {1988},
doi = {10.1021/ci00057a005},
URL = {https://doi.org/10.1021/ci00057a005},
eprint = {https://doi.org/10.1021/ci00057a005}
}


@article{ding_dsb,
author = {Ding, Bailu and Chaudhuri, Surajit and Gehrke, Johannes and Narasayya, Vivek},
title = {DSB: a decision support benchmark for workload-driven and traditional database systems},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484234},
doi = {10.14778/3484224.3484234},
abstract = {We describe a new benchmark, DSB, for evaluating both workload-driven and traditional database systems on modern decision support workloads. DSB is adapted from the widely-used industrial-standard TPC-DS benchmark. It enhances the TPC-DS benchmark with complex data distribution and challenging yet semantically meaningful query templates. DSB also introduces configurable and dynamic workloads to assess the adaptability of database systems. Since workload-driven and traditional database systems have different performance dimensions, including the additional resources required for tuning and maintaining the systems, we provide guidelines on evaluation methodology and metrics to report. We show a case study on how to evaluate both workload-driven and traditional database systems with the DSB benchmark. The code for the DSB benchmark is open sourced and is available at https://aka.ms/dsb.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {3376–3388},
numpages = {13}
}

@inproceedings{wu_stage_redshift,
author = {Wu, Ziniu and Marcus, Ryan and Liu, Zhengchun and Negi, Parimarjan and Nathan, Vikram and Pfeil, Pascal and Saxena, Gaurav and Rahman, Mohammad and Narayanaswamy, Balakrishnan and Kraska, Tim},
title = {Stage: Query Execution Time Prediction in Amazon Redshift},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653391},
doi = {10.1145/3626246.3653391},
abstract = {Query performance (e.g., execution time) prediction is a critical component of modern DBMSes. As a pioneering cloud data warehouse, Amazon Redshift relies on an accurate execution time prediction for many downstream tasks, ranging from high-level optimizations, such as automatically creating materialized views, to low-level tasks on the critical path of query execution, such as admission, scheduling, and execution resource control. Unfortunately, many existing execution time prediction techniques, including those used in Redshift, suffer from cold start issues, inaccurate estimation, and are not robust against workload/data changes.In this paper, we propose a novel hierarchical execution time predictor: the Stage predictor. The Stage predictor is designed to leverage the unique characteristics and challenges faced by Redshift. The Stage predictor consists of three model states: an execution time cache, a lightweight local model optimized for a specific DB instance with uncertainty measurement, and a complex global model that is transferable across all instances in Redshift. We design a systematic approach to use these models that best leverages optimality (cache), instance-optimization (local model), and transferable knowledge about Redshift (global model). Experimentally, we show that the Stage predictor makes more accurate and robust predictions while maintaining a practical inference latency and memory overhead. Overall, the Stage predictor can improve the average query execution latency by 20\% on these instances compared to the prior query performance predictor in Redshift.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {280–294},
numpages = {15},
keywords = {AWS redshift, query performance prediction},
location = {Santiago AA, Chile},
series = {SIGMOD/PODS '24}
}

@article{wang_ceda,
author = {Wang, Zilong and Zeng, Qixiong and Wang, Ning and Lu, Haowen and Zhang, Yue},
title = {CEDA: Learned Cardinality Estimation with Domain Adaptation},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611589},
doi = {10.14778/3611540.3611589},
abstract = {Cardinality Estimation (CE) is a fundamental but critical problem in DBMS query optimization, while deep learning techniques have made significant breakthroughs in the research of CE. However, apart from requiring sufficiently large training data to cover all possible query regions for accurate estimation, current query-driven CE methods also suffer from workload drifts. In fact, retraining or fine-tuning needs cardinality labels as ground truth and obtaining the labels through DBMS is also expensive. Therefore, we propose CEDA, a novel domain-adaptive CE system. CEDA can achieve more accurate estimations by automatically generating workloads as training data according to the data distribution in the database, and incorporating histogram information into an attention-based cardinality estimator. To solve the problem of workload drifts in real-world environments, CEDA adopts a domain adaptation strategy, making the model more robust and perform well on an unlabeled workload with a large difference from the feature distribution of the training set.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3934–3937},
numpages = {4}
}

  

@article{yang_neurocard,
author = {Yang, Zongheng and Kamsetty, Amog and Luan, Sifei and Liang, Eric and Duan, Yan and Chen, Xi and Stoica, Ion},
title = {NeuroCard: one cardinality estimator for all tables},
year = {2020},
issue_date = {September 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3421424.3421432},
doi = {10.14778/3421424.3421432},
abstract = {Query optimizers rely on accurate cardinality estimates to produce good execution plans. Despite decades of research, existing cardinality estimators are inaccurate for complex queries, due to making lossy modeling assumptions and not capturing inter-table correlations. In this work, we show that it is possible to learn the correlations across all tables in a database without any independence assumptions. We present NeuroCard, a join cardinality estimator that builds a single neural density estimator over an entire database. Leveraging join sampling and modern deep autoregressive models, NeuroCard makes no inter-table or inter-column independence assumptions in its probabilistic modeling. NeuroCard achieves orders of magnitude higher accuracy than the best prior methods (a new state-of-the-art result of 8.5x maximum error on JOB-light), scales to dozens of tables, while being compact in space (several MBs) and efficient to construct or update (seconds to minutes).},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {61–73},
numpages = {13}
}

  

@article{kanellis_llamatune,
author = {Kanellis, Konstantinos and Ding, Cong and Kroth, Brian and M\"{u}ller, Andreas and Curino, Carlo and Venkataraman, Shivaram},
title = {LlamaTune: sample-efficient DBMS configuration tuning},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551844},
doi = {10.14778/3551793.3551844},
abstract = {Tuning a database system to achieve optimal performance on a given workload is a long-standing problem in the database community. A number of recent works have leveraged ML-based approaches to guide the sampling of large parameter spaces (hundreds of tuning knobs) in search for high performance configurations. Looking at Microsoft production services operating millions of databases, sample efficiency emerged as a crucial requirement to use tuners on diverse workloads.This motivates our investigation in LlamaTune, a tuner design that leverages domain knowledge to improve the sample efficiency of existing optimizers. LlamaTune employs an automated dimensionality reduction technique based on randomized projections, a biased-sampling approach to handle special values for certain knobs, and knob values bucketization, to reduce the size of the search space. LlamaTune compares favorably with the state-of-the-art optimizers across a diverse set of workloads. It identifies the best performing configurations with up to 11X fewer workload runs, and reaching up to 21\% higher throughput. We also show that benefits from LlamaTune generalize across both BO-based and RL-based optimizers, as well as different DBMS versions.While the journey to perform database tuning at cloud-scale remains long, LlamaTune goes a long way in making automatic DBMS tuning practical at scale.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2953–2965},
numpages = {13}
}

  

@misc{dao_flashattention,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}