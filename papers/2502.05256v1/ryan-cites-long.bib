
@article{url-stxbtree,
	title = {{STX} {B}+ {Tree}, https://panthema.net/2007/stx-btree/},
	url = {https://panthema.net/2007/stx-btree/},
	note = {tex.key= 1},
}

@inproceedings{fast,
	series = {{SIGMOD} '10},
	title = {{FAST}: fast architecture sensitive tree search on modern {CPUs} and {GPUs}},
	shorttitle = {{FAST}: fast architecture sensitive tree},
	doi = {10.1145/1807167.1807206},
	booktitle = {Proceedings of the 2010 {International} {Conference} on {Management} of {Data}},
	author = {Kim, Changkyu and Chhugani, Jatin and Satish, Nadathur and Sedlar, Eric and Nguyen, Anthony D. and Kaldewey, Tim and Lee, Victor W. and Brandt, Scott A. and Dubey, Pradeep},
	year = {2010},
	keywords = {Binary tree, CAS latency, Central processing unit, Database, Fastest, Futures studies, Graphics processing unit, Memory bandwidth, Page (computer memory), Parallel computing, SIMD, Tree traversal, X-tree},
	file = {Kim et al. - 2010 - FAST fast architecture sensitive tree search on m.pdf:/home/ryan/Zotero/storage/NSINNDB9/Kim et al. - 2010 - FAST fast architecture sensitive tree search on m.pdf:application/pdf},
}

@inproceedings{sosd,
	series = {{MLForSystems} @ {NeurIPS} '19},
	title = {{SOSD}: {A} {Benchmark} for {Learned} {Indexes}},
	copyright = {All rights reserved},
	shorttitle = {{SOSD}},
	booktitle = {{ML} for {Systems} at {NeurIPS}},
	author = {Kipf, Andreas and Marcus, Ryan and van Renen, Alexander and Stoian, Mihail and Kemper, Alfons and Kraska, Tim and Neumann, Thomas},
	month = dec,
	year = {2019},
	file = {Andreas Kipf et al. - SOSD A Benchmark for Learned Indexes.pdf:/home/ryan/Zotero/storage/R9NG4C5T/Andreas Kipf et al. - SOSD A Benchmark for Learned Indexes.pdf:application/pdf},
}

@inproceedings{fiting_tree,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {{FITing}-{Tree}: {A} {Data}-aware {Index} {Structure}},
	isbn = {978-1-4503-5643-5},
	shorttitle = {{FITing}-{Tree}},
	url = {http://doi.acm.org/10.1145/3299869.3319860},
	doi = {10.1145/3299869.3319860},
	urldate = {2019-09-24},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Galakatos, Alex and Markovitch, Michael and Binnig, Carsten and Fonseca, Rodrigo and Kraska, Tim},
	year = {2019},
	pages = {1189--1206},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/LUVV7IM3/Galakatos et al. - 2019 - FITing-Tree A Data-aware Index Structure.pdf:application/pdf},
}

@article{url-auctus,
	title = {{NYU} {Auctus}, https://datamart.d3m.vida-nyu.org/},
	url = {https://datamart.d3m.vida-nyu.org/},
	note = {tex.key= 1},
}

@article{feature_selection,
	series = {{CEE} '14},
	title = {A survey on feature selection methods},
	volume = {40},
	issn = {0045-7906},
	url = {http://www.sciencedirect.com/science/article/pii/S0045790613003066},
	doi = {10.1016/j.compeleceng.2013.11.024},
	language = {en},
	number = {1},
	urldate = {2020-02-22},
	journal = {Computers \& Electrical Engineering},
	author = {Chandrashekar, Girish and Sahin, Ferat},
	month = jan,
	year = {2014},
	pages = {16--28},
	file = {Chandrashekar and Sahin - 2014 - A survey on feature selection methods.pdf:/home/ryan/Zotero/storage/NE2C4VE5/Chandrashekar and Sahin - 2014 - A survey on feature selection methods.pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/NW9HSZRB/S0045790613003066.html:text/html},
}

@techreport{bias_variance,
	title = {Bias, {Variance}, and {Arcing} {Classifiers}},
	author = {Breiman, Leo},
	year = {1996},
	file = {Citeseer - Full Text PDF:/home/ryan/Zotero/storage/J47YUCBK/Breiman - 1996 - Bias, Variance, and Arcing Classifiers.pdf:application/pdf;Citeseer - Snapshot:/home/ryan/Zotero/storage/BBGLJ3LV/summary.html:text/html},
}

@inproceedings{autoperf,
	title = {A {Zero}-{Positive} {Learning} {Approach} for {Diagnosing} {Software} {Performance} {Regressions}},
	url = {http://papers.nips.cc/paper/9337-a-zero-positive-learning-approach-for-diagnosing-software-performance-regressions.pdf},
	urldate = {2020-02-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Alam, Mejbah and Gottschlich, Justin and Tatbul, Nesime and Turek, Javier S and Mattson, Tim and Muzahid, Abdullah},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch√©-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {11627--11639},
	file = {NIPS Full Text PDF:/home/ryan/Zotero/storage/MUM72HCZ/Alam et al. - 2019 - A Zero-Positive Learning Approach for Diagnosing S.pdf:application/pdf;NIPS Snapshot:/home/ryan/Zotero/storage/3J4JNS4J/9337-a-zero-positive-learning-approach-for-diagnosing-software-performance-regressions.html:text/html},
}

@article{large_discrete,
	title = {Deep {Reinforcement} {Learning} in {Large} {Discrete} {Action} {Spaces}},
	url = {http://arxiv.org/abs/1512.07679},
	urldate = {2020-02-13},
	journal = {arXiv:1512.07679 [cs, stat]},
	author = {Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
	month = apr,
	year = {2016},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/E5JAPAN3/Dulac-Arnold et al. - 2016 - Deep Reinforcement Learning in Large Discrete Acti.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/H8W64SYV/1512.html:text/html},
}

@inproceedings{pillars,
	address = {Philadelphia, PA, USA},
	series = {{MAPL} 2018},
	title = {The three pillars of machine programming},
	isbn = {978-1-4503-5834-7},
	url = {https://doi.org/10.1145/3211346.3211355},
	doi = {10.1145/3211346.3211355},
	urldate = {2020-02-13},
	booktitle = {Proceedings of the 2nd {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Gottschlich, Justin and Solar-Lezama, Armando and Tatbul, Nesime and Carbin, Michael and Rinard, Martin and Barzilay, Regina and Amarasinghe, Saman and Tenenbaum, Joshua B. and Mattson, Tim},
	month = jun,
	year = {2018},
	keywords = {adaptation, intention, invention, machine programming, program synthesis, software development, software maintenance},
	pages = {69--80},
	file = {Full Text PDF:/home/ryan/Zotero/storage/5B5GBKM5/Gottschlich et al. - 2018 - The three pillars of machine programming.pdf:application/pdf},
}

@article{stitch,
	title = {Plan stitch: harnessing the best of many plans},
	volume = {11},
	issn = {2150-8097},
	shorttitle = {Plan stitch},
	url = {https://doi.org/10.14778/3231751.3231761},
	doi = {10.14778/3231751.3231761},
	number = {10},
	urldate = {2020-02-13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ding, Bailu and Das, Sudipto and Wu, Wentao and Chaudhuri, Surajit and Narasayya, Vivek},
	month = jun,
	year = {2018},
	pages = {1123--1136},
	file = {Ding et al. - 2018 - Plan stitch harnessing the best of many plans.pdf:/home/ryan/Zotero/storage/SZNLPJL9/Ding et al. - 2018 - Plan stitch harnessing the best of many plans.pdf:application/pdf},
}

@article{neurovec,
	title = {{NeuroVectorizer}: {End}-to-{End} {Vectorization} with {Deep} {Reinforcement} {Learning}},
	shorttitle = {{NeuroVectorizer}},
	url = {http://arxiv.org/abs/1909.13639},
	urldate = {2020-02-14},
	journal = {arXiv:1909.13639 [cs]},
	author = {Haj-Ali, Ameer and Ahmed, Nesreen K. and Willke, Ted and Shao, Sophia and Asanovic, Krste and Stoica, Ion},
	month = jan,
	year = {2020},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/J7IIEQAC/Haj-Ali et al. - 2020 - NeuroVectorizer End-to-End Vectorization with Dee.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/68GF9UKR/1909.html:text/html},
}

@inproceedings{gc_rf,
	address = {Phoenix, AZ, USA},
	series = {{ISMM} 2019},
	title = {Learning when to garbage collect with random forests},
	isbn = {978-1-4503-6722-6},
	url = {https://doi.org/10.1145/3315573.3329983},
	doi = {10.1145/3315573.3329983},
	urldate = {2020-02-13},
	booktitle = {Proceedings of the 2019 {ACM} {SIGPLAN} {International} {Symposium} on {Memory} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Jacek, Nicholas and Moss, J. Eliot B.},
	month = jun,
	year = {2019},
	keywords = {garbage collection, machine learning},
	pages = {53--63},
	file = {Jacek and Moss - 2019 - Learning when to garbage collect with random fores.pdf:/home/ryan/Zotero/storage/AFGWDWFG/Jacek and Moss - 2019 - Learning when to garbage collect with random fores.pdf:application/pdf},
}

@inproceedings{deep_bayes_bandits,
	series = {{ICLR} '18},
	title = {Deep {Bayesian} {Bandits} {Showdown}: {An} empirical comparison of bayesian deep networks for thompson sampling},
	shorttitle = {Deep {Bayesian} {Bandits} {Showdown}},
	url = {https://openreview.net/forum?id=SyYe6k-CW},
	booktitle = {International conference on learning representations},
	author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
	year = {2018},
}

@inproceedings{thompson_dropout,
	address = {New York, NY, USA},
	series = {{ICML}'16},
	title = {Dropout as a {Bayesian} approximation: representing model uncertainty in deep learning},
	shorttitle = {Dropout as a {Bayesian} approximation},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2020-01-30},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 48},
	publisher = {JMLR.org},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2016},
	pages = {1050--1059},
}

@article{thompson_bootstrap,
	title = {Bootstrapped {Thompson} {Sampling} and {Deep} {Exploration}},
	url = {http://arxiv.org/abs/1507.00300},
	abstract = {This technical note presents a new approach to carrying out the kind of exploration achieved by Thompson sampling, but without explicitly maintaining or sampling from posterior distributions. The approach is based on a bootstrap technique that uses a combination of observed and artificially generated data. The latter serves to induce a prior distribution which, as we will demonstrate, is critical to effective exploration. We explain how the approach can be applied to multi-armed bandit and reinforcement learning problems and how it relates to Thompson sampling. The approach is particularly well-suited for contexts in which exploration is coupled with deep learning, since in these settings, maintaining or generating samples from a posterior distribution becomes computationally infeasible.},
	urldate = {2020-01-30},
	journal = {arXiv:1507.00300 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin},
	month = jul,
	year = {2015},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/88HFG5JJ/Osband and Van Roy - 2015 - Bootstrapped Thompson Sampling and Deep Exploratio.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/PVXIXPZL/1507.html:text/html},
}

@article{bandit_survey,
	title = {A {Survey} on {Contextual} {Multi}-armed {Bandits}},
	url = {http://arxiv.org/abs/1508.03326},
	abstract = {In this survey we cover a few stochastic and adversarial contextual bandit algorithms. We analyze each algorithm's assumption and regret bound.},
	urldate = {2020-01-30},
	journal = {arXiv:1508.03326 [cs]},
	author = {Zhou, Li},
	month = feb,
	year = {2016},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/C6US2JV9/Zhou - 2016 - A Survey on Contextual Multi-armed Bandits.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/2DF62924/1508.html:text/html},
}

@article{suc_elim,
	series = {{JMLR} '06},
	title = {Action {Elimination} and {Stopping} {Conditions} for the {Multi}-{Armed} {Bandit} and {Reinforcement} {Learning} {Problems}},
	volume = {7},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v7/evendar06a.html},
	number = {Jun},
	urldate = {2020-01-30},
	journal = {Journal of Machine Learning Research},
	author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
	year = {2006},
	pages = {1079--1105},
	file = {Full Text PDF:/home/ryan/Zotero/storage/HGEEKLBQ/Even-Dar et al. - 2006 - Action Elimination and Stopping Conditions for the.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/5J76JWNP/evendar06a.html:text/html},
}

@article{opt_viz,
	title = {The {Picasso} database query optimizer visualizer},
	volume = {3},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/1920841.1921027},
	doi = {10.14778/1920841.1921027},
	number = {1-2},
	urldate = {2020-02-01},
	journal = {Proceedings of the VLDB Endowment},
	author = {Haritsa, Jayant R.},
	month = sep,
	year = {2010},
	pages = {1517--1520},
	file = {Full Text PDF:/home/ryan/Zotero/storage/8AUGLIVC/Haritsa - 2010 - The Picasso database query optimizer visualizer.pdf:application/pdf},
}

@inproceedings{opt_prov,
	series = {{ICDE} '17},
	title = {Provenance-{Aware} {Query} {Optimization}},
	doi = {10.1109/ICDE.2017.104},
	booktitle = {2017 {IEEE} 33rd {International} {Conference} on {Data} {Engineering}},
	author = {Niu, Xing and Kapoor, Raghav and Glavic, Boris and Gawlick, Dieter and Liu, Zhen Hua and Krishnaswamy, Vasudha and Radhakrishnan, Venkatesh},
	month = apr,
	year = {2017},
	keywords = {Algebra, auditing, Big Data, big data analytics, CBO, cloud computing, cloud environments, cost estimation, cost-based optimization framework, data analysis, data auditing, data integrity, data provenance, Databases, DBMS, Encoding, GProM provenance system, instrumented queries, Instruments, LOC, optimisation, Optimization, Pipelines, provenance specific optimizations, provenance-aware query optimization, query processing, query results debugging, Semantics},
	pages = {473--484},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/EFP2WTUV/7930000.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/XVJG9R8T/Niu et al. - 2017 - Provenance-Aware Query Optimization.pdf:application/pdf},
}

@inproceedings{coko,
	address = {Madison, Wisconsin},
	series = {{SIGMOD} '02},
	title = {Visual {COKO}: a debugger for query optimizer development},
	isbn = {978-1-58113-497-1},
	shorttitle = {Visual {COKO}},
	url = {https://doi.org/10.1145/564691.564770},
	doi = {10.1145/564691.564770},
	urldate = {2020-02-01},
	booktitle = {Proceedings of the 2002 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {Association for Computing Machinery},
	author = {Abadi, Daniel J. and Cherniack, Mitch},
	month = jun,
	year = {2002},
	pages = {617},
	file = {Full Text PDF:/home/ryan/Zotero/storage/C3THCQ2T/Abadi and Cherniack - 2002 - Visual COKO a debugger for query optimizer develo.pdf:application/pdf},
}

@article{xai,
	series = {Access '18},
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	doi = {10.1109/ACCESS.2018.2870052},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	keywords = {AI-based systems, artificial intelligence, Biological system modeling, black-box models, black-box nature, Conferences, explainable AI, explainable artificial intelligence, Explainable artificial intelligence, fourth industrial revolution, interpretable machine learning, Machine learning, Machine learning algorithms, Market research, Prediction algorithms, XAI},
	pages = {52138--52160},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/LSKBF8AF/8466590.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/WBWEV9F6/Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf:application/pdf},
}

@inproceedings{local_card_est,
	address = {Amsterdam, Netherlands},
	series = {{aiDM} '19},
	title = {Cardinality estimation with local deep learning models},
	isbn = {978-1-4503-6802-5},
	url = {https://doi.org/10.1145/3329859.3329875},
	doi = {10.1145/3329859.3329875},
	urldate = {2020-01-30},
	booktitle = {Proceedings of the {Second} {International} {Workshop} on {Exploiting} {Artificial} {Intelligence} {Techniques} for {Data} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Woltmann, Lucas and Hartmann, Claudio and Thiele, Maik and Habich, Dirk and Lehner, Wolfgang},
	month = jul,
	year = {2019},
	pages = {1--8},
	file = {Full Text PDF:/home/ryan/Zotero/storage/EX47RCJJ/Woltmann et al. - 2019 - Cardinality estimation with local deep learning mo.pdf:application/pdf},
}

@article{containment_rates,
	title = {Improved {Cardinality} {Estimation} by {Learning} {Queries} {Containment} {Rates}},
	url = {http://arxiv.org/abs/1908.07723},
	abstract = {The containment rate of query Q1 in query Q2 over database D is the percentage of Q1's result tuples over D that are also in Q2's result over D. We directly estimate containment rates between pairs of queries over a specific database. For this, we use a specialized deep learning scheme, CRN, which is tailored to representing pairs of SQL queries. Result-cardinality estimation is a core component of query optimization. We describe a novel approach for estimating queries result-cardinalities using estimated containment rates among queries. This containment rate estimation may rely on CRN or embed, unchanged, known cardinality estimation methods. Experimentally, our novel approach for estimating cardinalities, using containment rates between queries, on a challenging real-world database, realizes significant improvements to state of the art cardinality estimation methods.},
	urldate = {2020-01-30},
	journal = {arXiv:1908.07723 [cs]},
	author = {Hayek, Rojeh and Shmueli, Oded},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/KVTJT7JZ/Hayek and Shmueli - 2019 - Improved Cardinality Estimation by Learning Querie.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/ZUCJEQQA/1908.html:text/html},
}

@article{learned_card_eval,
	series = {{arXiv} '19},
	title = {An {Empirical} {Analysis} of {Deep} {Learning} for {Cardinality} {Estimation}},
	url = {http://arxiv.org/abs/1905.06425},
	urldate = {2020-01-30},
	journal = {arXiv:1905.06425 [cs]},
	author = {Ortiz, Jennifer and Balazinska, Magdalena and Gehrke, Johannes and Keerthi, S. Sathiya},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Databases},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/A794YQAC/Ortiz et al. - 2019 - An Empirical Analysis of Deep Learning for Cardina.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/V9UF5JHS/1905.html:text/html},
}

@article{learn_cost,
	series = {{VLDB} '19},
	title = {An end-to-end learning-based cost estimator},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3368289.3368296},
	doi = {10.14778/3368289.3368296},
	number = {3},
	urldate = {2020-01-30},
	journal = {Proceedings of the VLDB Endowment},
	author = {Sun, Ji and Li, Guoliang},
	month = nov,
	year = {2019},
	pages = {307--319},
	file = {Full Text PDF:/home/ryan/Zotero/storage/GMZTLQ77/Sun and Li - 2019 - An end-to-end learning-based cost estimator.pdf:application/pdf},
}

@article{deep_cmab,
	series = {{arXiv} '18},
	title = {Deep {Contextual} {Multi}-armed {Bandits}},
	url = {http://arxiv.org/abs/1807.09809},
	urldate = {2020-01-30},
	journal = {arXiv:1807.09809 [cs, stat]},
	author = {Collier, Mark and Llorens, Hector Urdiales},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/CUQ6WD86/Collier and Llorens - 2018 - Deep Contextual Multi-armed Bandits.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/TJJMT8RS/1807.html:text/html},
}

@article{bao_arxiv,
	title = {Bao online appendix, https://rm.cab/bao\_appendix},
	url = {https://rm.cab/bao_appendix},
}

@inproceedings{multitask_prior,
	series = {{ECCV} '18},
	title = {Dynamic {Task} {Prioritization} for {Multitask} {Learning}},
	url = {http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Focus_on_the_ECCV_2018_paper.html},
	urldate = {2020-01-23},
	author = {Guo, Michelle and Haque, Albert and Huang, De-An and Yeung, Serena and Fei-Fei, Li},
	year = {2018},
	pages = {270--287},
	file = {Full Text PDF:/home/ryan/Zotero/storage/HBX487MB/Guo et al. - 2018 - Dynamic Task Prioritization for Multitask Learning.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/8D6XUI64/Michelle_Guo_Focus_on_the_ECCV_2018_paper.html:text/html},
}

@article{audio_weight,
	title = {{DNN} and {CNN} with {Weighted} and {Multi}-task {Loss} {Functions} for {Audio} {Event} {Detection}},
	url = {http://arxiv.org/abs/1708.03211},
	urldate = {2020-01-23},
	journal = {arXiv:1708.03211 [cs]},
	author = {Phan, Huy and Krawczyk-Becker, Martin and Gerkmann, Timo and Mertins, Alfred},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/MFP5BWV2/Phan et al. - 2017 - DNN and CNN with Weighted and Multi-task Loss Func.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/6J7SEW9M/1708.html:text/html},
}

@article{rec_weight_loss,
	title = {{DeepRec}: {A} deep neural network approach to recommendation with item embedding and weighted loss function},
	volume = {470},
	issn = {0020-0255},
	shorttitle = {{DeepRec}},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025518306431},
	doi = {10.1016/j.ins.2018.08.039},
	language = {en},
	urldate = {2020-01-23},
	journal = {Information Sciences},
	author = {Zhang, Wen and Du, Yuhang and Yoshida, Taketoshi and Yang, Ye},
	month = jan,
	year = {2019},
	keywords = {Deep neural network, DeepRec, Item embedding, Recommender system, Weighted loss function},
	pages = {121--140},
	file = {ScienceDirect Snapshot:/home/ryan/Zotero/storage/3RT5MN39/S0020025518306431.html:text/html},
}

@article{dqn_prior2,
	title = {Distributed {Prioritized} {Experience} {Replay}},
	url = {http://arxiv.org/abs/1803.00933},
	urldate = {2020-01-23},
	journal = {arXiv:1803.00933 [cs]},
	author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/VKGQMLVD/Horgan et al. - 2018 - Distributed Prioritized Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/AVG3PVRI/1803.html:text/html},
}

@article{card_est_cloud,
	title = {Towards a learning optimizer for shared clouds},
	volume = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3291264.3291267},
	doi = {10.14778/3291264.3291267},
	number = {3},
	urldate = {2020-01-24},
	journal = {Proceedings of the VLDB Endowment},
	author = {Wu, Chenggang and Jindal, Alekh and Amizadeh, Saeed and Patel, Hiren and Le, Wangchao and Qiao, Shi and Rao, Sriram},
	month = nov,
	year = {2018},
	pages = {210--222},
	file = {Full Text PDF:/home/ryan/Zotero/storage/ZZIBL3Y8/Wu et al. - 2018 - Towards a learning optimizer for shared clouds.pdf:application/pdf},
}

@article{batch_weight_heart,
	title = {A robust deep convolutional neural network with batch-weighted loss for heartbeat classification},
	volume = {122},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417418308054},
	doi = {10.1016/j.eswa.2018.12.037},
	language = {en},
	urldate = {2020-01-23},
	journal = {Expert Systems with Applications},
	author = {Sellami, Ali and Hwang, Heasoo},
	month = may,
	year = {2019},
	pages = {75--84},
	file = {ScienceDirect Full Text PDF:/home/ryan/Zotero/storage/PBBSEUKR/Sellami and Hwang - 2019 - A robust deep convolutional neural network with ba.pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/LQ4NY4Y3/S0957417418308054.html:text/html},
}

@inproceedings{dqn_prior1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Q}-{Learning} with {Prioritized} {Sampling}},
	isbn = {978-3-319-46687-3},
	doi = {10.1007/978-3-319-46687-3_2},
	language = {en},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Zhai, Jianwei and Liu, Quan and Zhang, Zongzhang and Zhong, Shan and Zhu, Haijun and Zhang, Peng and Sun, Cijia},
	editor = {Hirose, Akira and Ozawa, Seiichi and Doya, Kenji and Ikeda, Kazushi and Lee, Minho and Liu, Derong},
	year = {2016},
	keywords = {Deep Learning, Deep Reinforcement Learning, Policy control, Prioritized sampling, Reinforcement Learning},
	pages = {13--22},
}

@inproceedings{hot,
	address = {New York, NY, USA},
	series = {{SIGMOD} ‚Äô18},
	title = {{HOT}: {A} height optimized trie index for main-memory database systems},
	isbn = {978-1-4503-4703-7},
	shorttitle = {{HOT}: {A} height optimized trie index},
	url = {https://doi.org/10.1145/3183713.3196896},
	doi = {10.1145/3183713.3196896},
	booktitle = {Proceedings of the 2018 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Binna, Robert and Zangerle, Eva and Pichl, Martin and Specht, G√ºnther and Leis, Viktor},
	year = {2018},
	keywords = {height optimized trie, index, main memory, simd},
	pages = {521--534},
	file = {Binna et al. - 2018 - HOT A height optimized trie index for main-memory.pdf:/home/ryan/Zotero/storage/38U477GR/Binna et al. - 2018 - HOT A height optimized trie index for main-memory.pdf:application/pdf},
}

@article{alex,
	series = {{arXiv} '19},
	title = {{ALEX}: {An} {Updatable} {Adaptive} {Learned} {Index}},
	shorttitle = {{ALEX}: {An} {Updatable} {Adaptive} {Learned} {Index}},
	url = {http://arxiv.org/abs/1905.08898},
	urldate = {2020-01-03},
	journal = {arXiv:1905.08898 [cs]},
	author = {Ding, Jialin and Minhas, Umar Farooq and Zhang, Hantian and Li, Yinan and Wang, Chi and Chandramouli, Badrish and Gehrke, Johannes and Kossmann, Donald and Lomet, David},
	month = may,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/D7TDEPE9/Ding et al. - 2019 - ALEX An Updatable Adaptive Learned Index.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/QGN3ZVVB/1905.html:text/html},
}

@inproceedings{systemr,
	address = {San Francisco (CA)},
	series = {{SIGMOD} '79},
	title = {Access {Path} {Selection} in a {Relational} {Database} {Management} {System}},
	isbn = {978-0-934613-53-8},
	shorttitle = {Access {Path} {Selection}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780934613538500388},
	doi = {10.1016/B978-0-934613-53-8.50038-8},
	urldate = {2018-02-27},
	booktitle = {{SIGMOD} '79},
	publisher = {Morgan Kaufmann},
	author = {Selinger, P. Griffiths and Astrahan, M. M. and Chamberlin, D. D. and Lorie, R. A. and Price, T. G.},
	editor = {Mylopolous, John and Brodie, Michael},
	year = {1979},
	pages = {511--522},
	file = {ScienceDirect Snapshot:/home/ryan/Zotero/storage/SNNYZKT7/B9780934613538500388.html:text/html;Selinger et al. - 1989 - Access Path Selection in a Relational Database Man.pdf:/home/ryan/Zotero/storage/QVRPPHXL/Selinger et al. - 1989 - Access Path Selection in a Relational Database Man.pdf:application/pdf},
}

@inproceedings{flood,
	series = {{MLForSystems} @ {NeurIPS} '19},
	title = {Learning {Multi}-dimensional {Indexing}},
	copyright = {All rights reserved},
	booktitle = {{ML} for {Systems} at {NeurIPS}},
	author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
	month = dec,
	year = {2019},
}

@article{url-rmi,
	title = {{MIT} {RMI}, https://learned.systems/rmi},
	url = {https://learned.systems/rmi},
}

@article{url-weka,
	title = {Weka 3, http://cs.waikato.ac.nz/ml/weka/},
	url = {http://cs.waikato.ac.nz/ml/weka/},
	note = {tex.key= 1},
}

@misc{AdaptiveRadixTree,
	title = {The adaptive radix tree {\textbar} {Proceedings} of the 2013 {IEEE} {International} {Conference} on {Data} {Engineering} ({ICDE} 2013)},
	url = {https://dl.acm.org/doi/abs/10.1109/ICDE.2013.6544812},
	language = {EN},
	urldate = {2019-12-29},
	file = {Snapshot:/home/ryan/Zotero/storage/KFYGCSYV/ICDE.2013.html:text/html},
}

@inproceedings{park_ws,
	address = {Long Beach, CA},
	series = {{RL4RealLife} @ {ICML} '19},
	title = {Park: {An} {Open} {Platform} for {Learning}-{Augmented} {Computer} {Systems}},
	copyright = {All rights reserved},
	shorttitle = {Park},
	booktitle = {Reinforcement {Learning} for {Real} {Life}},
	author = {Mao, Hongzi and Negi, Parimarjan and Narayan, Akshay and Wang, Hanrui and Yang, Jiacheng and Wang, Haonan and Marcus, Ryan and Addanki, Ravichandra and Khani, Mehrdad and He, Songtao and Nathan, Vikram and Cangialosi, Frank and Venkatakrishnan, Shaileshh Bojja and Weng, Wei-Hung and Han, Song and Kraska, Tim and Alizadeh, Mohammad},
	year = {2019},
	note = {Award: 'best workshop paper award'},
	file = {Mao et al. - 2019 - Park An Open Platform for Learning-Augmented Comp.pdf:/home/ryan/Zotero/storage/BX3TEBQA/Mao et al. - 2019 - Park An Open Platform for Learning-Augmented Comp.pdf:application/pdf},
}

@inproceedings{wisedb-clouddm,
	series = {{CloudDM} @ {ICDE} '16},
	title = {Workload {Management} for {Cloud} {Databases} via {Machine} {Learning}},
	copyright = {All rights reserved},
	url = {http://ieeexplore.ieee.org/abstract/document/7495611/},
	doi = {10.1109/ICDEW.2016.7495611},
	booktitle = {Workshop on {Cloud} {Data} {Management} and the {IEEE} {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	year = {2016},
	file = {Marcus and Papaemmanouil - 2016 - Workload Management for Cloud Databases via Machin.pdf:/home/ryan/Zotero/storage/IC3ZPWRN/Marcus and Papaemmanouil - 2016 - Workload Management for Cloud Databases via Machin.pdf:application/pdf},
}

@inproceedings{pca,
	address = {Berlin, Heidelberg},
	series = {{IESS} '11},
	title = {Principal {Component} {Analysis}},
	isbn = {978-3-642-04898-2},
	url = {https://doi.org/10.1007/978-3-642-04898-2_455},
	doi = {10.1007/978-3-642-04898-2_455},
	language = {en},
	urldate = {2018-11-22},
	booktitle = {International {Encyclopedia} of {Statistical} {Science}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jolliffe, Ian},
	editor = {Lovric, Miodrag},
	year = {2011},
	pages = {1094--1096},
	file = {Jolliffe - 2011 - Principal Component Analysis.pdf:/home/ryan/Zotero/storage/WM5NDDBN/Jolliffe - 2011 - Principal Component Analysis.pdf:application/pdf},
}

@inproceedings{develops,
	title = {Devel-op: {An} optimizer development environment},
	isbn = {978-1-4799-2555-1},
	shorttitle = {Devel-op},
	url = {http://ieeexplore.ieee.org/document/6816760/},
	doi = {10.1109/ICDE.2014.6816760},
	urldate = {2018-05-21},
	publisher = {IEEE},
	author = {Peng, Zhibo and Cherniack, Mitch and Papaemmanouil, Olga},
	month = mar,
	year = {2014},
	pages = {1278--1281},
	file = {Peng et al. - 2014 - Devel-op An optimizer development environment.pdf:/home/ryan/Zotero/storage/6M3V9P37/Peng et al. - 2014 - Devel-op An optimizer development environment.pdf:application/pdf},
}

@inproceedings{deep_card_est2,
	series = {{CIDR} '19},
	title = {Learned {Cardinalities}: {Estimating} {Correlated} {Joins} with {Deep} {Learning}},
	shorttitle = {Learned {Cardinalities}},
	url = {http://arxiv.org/abs/1809.00677},
	abstract = {We describe a new deep learning approach to cardinality estimation. MSCN is a multi-set convolutional network, tailored to representing relational query plans, that employs set semantics to capture query features and true cardinalities. MSCN builds on sampling-based estimation, addressing its weaknesses when no sampled tuples qualify a predicate, and in capturing join-crossing correlations. Our evaluation of MSCN using a real-world dataset shows that deep learning significantly enhances the quality of cardinality estimation, which is the core problem in query optimization.},
	urldate = {2018-09-17},
	booktitle = {9th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Kipf, Andreas and Kipf, Thomas and Radke, Bernhard and Leis, Viktor and Boncz, Peter and Kemper, Alfons},
	year = {2019},
	keywords = {Computer Science - Databases},
	file = {arXiv.org Snapshot:/home/ryan/Zotero/storage/YFUPWDD7/1809.html:text/html;Kipf et al. - 2019 - Learned Cardinalities Estimating Correlated Joins.pdf:/home/ryan/Zotero/storage/ETXAITVT/Kipf et al. - 2019 - Learned Cardinalities Estimating Correlated Joins.pdf:application/pdf},
}

@inproceedings{deep_card_est,
	address = {Riverton, NJ, USA},
	series = {{CASCON} '15},
	title = {Cardinality {Estimation} {Using} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2886444.2886453},
	abstract = {Database query optimizers benefit greatly from accurate cardinality estimation; however, this is hard to achieve on tables with correlated and/or skewed columns. We present a novel approach using neural networks to learn and approximate selectivity functions that take a bounded range on each column as input, effectively estimating selectivities for all relational operators. Experimental results with a simplified prototype show a significant improvement over state-of-the-art cardinality estimators on constructed datasets in terms of accuracy, efficiency, and amount of user input required.},
	urldate = {2018-09-17},
	booktitle = {Proceedings of the 25th {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Liu, Henry and Xu, Mingbin and Yu, Ziting and Corvinelli, Vincent and Zuzarte, Calisto},
	year = {2015},
	keywords = {machine learning, cardinality estimation, neural network, relational database},
	pages = {53--59},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/T9CHHDDA/Liu et al. - 2015 - Cardinality Estimation Using Neural Networks.pdf:application/pdf},
}

@inproceedings{repr,
	series = {{CVPR} '19},
	title = {{RePr}: improved training of convolutional filters},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Prakash, Aaditya and Storer, James and Florencio, Dinei and Zhang, Cha},
	year = {2019},
	pages = {10666--10675},
}

@inproceedings{siptip,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {Efficiently {Searching} {In}-{Memory} {Sorted} {Arrays}: {Revenge} of the {Interpolation} {Search}?},
	isbn = {978-1-4503-5643-5},
	shorttitle = {Efficiently {Searching} {In}-{Memory} {Sorted} {Arrays}},
	url = {http://doi.acm.org/10.1145/3299869.3300075},
	doi = {10.1145/3299869.3300075},
	urldate = {2019-09-10},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Van Sandt, Peter and Chronis, Yannis and Patel, Jignesh M.},
	year = {2019},
	keywords = {binary search, in-memory search, interpolation search},
	pages = {36--53},
	file = {Van Sandt et al. - 2019 - Efficiently Searching In-Memory Sorted Arrays Rev.pdf:/home/ryan/Zotero/storage/JWU9PVXI/Van Sandt et al. - 2019 - Efficiently Searching In-Memory Sorted Arrays Rev.pdf:application/pdf},
}

@article{neo,
	series = {{VLDB} '19},
	title = {Neo: {A} {Learned} {Query} {Optimizer}},
	volume = {12},
	copyright = {All rights reserved},
	issn = {2150-8097},
	doi = {10.14778/3342263.3342644},
	number = {11},
	journal = {PVLDB},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
	year = {2019},
	pages = {1705--1718},
	file = {Marcus et al. - 2019 - Neo A Learned Query Optimizer.pdf:/home/ryan/Zotero/storage/K979KMUD/Marcus et al. - 2019 - Neo A Learned Query Optimizer.pdf:application/pdf},
}

@article{qppnet,
	series = {{VLDB} '19},
	title = {Plan-{Structured} {Deep} {Neural} {Network} {Models} for {Query} {Performance} {Prediction}},
	volume = {12},
	copyright = {All rights reserved},
	doi = {10.14778/3342263.3342646},
	number = {11},
	journal = {PVLDB},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	year = {2019},
	pages = {1733--1746},
	file = {Marcus and Papaemmanouil - 2019 - Plan-Structured Deep Neural Network Models for Que.pdf:/home/ryan/Zotero/storage/5GPHK22S/Marcus and Papaemmanouil - 2019 - Plan-Structured Deep Neural Network Models for Que.pdf:application/pdf},
}

@article{som_cardinality2,
	series = {{TKDD} '17},
	title = {Query-{Driven} {Learning} for {Predictive} {Analytics} of {Data} {Subspace} {Cardinality}},
	volume = {11},
	issn = {1556-4681},
	url = {http://doi.acm.org/10.1145/3059177},
	doi = {10.1145/3059177},
	number = {4},
	urldate = {2019-07-30},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Anagnostopoulos, Christos and Triantafillou, Peter},
	month = jun,
	year = {2017},
	keywords = {analytics selection queries, data subspace exploration, optimal stopping theory, Predictive analytics, predictive learning, vector regression quantization},
	pages = {47:1--47:46},
	file = {Anagnostopoulos and Triantafillou - 2017 - Query-Driven Learning for Predictive Analytics of .pdf:/home/ryan/Zotero/storage/B8V4YNHH/Anagnostopoulos and Triantafillou - 2017 - Query-Driven Learning for Predictive Analytics of .pdf:application/pdf},
}

@article{url-amazonRDS,
	title = {Amazon {RDS}, https://aws.amazon.com/rds/},
	url = {https://aws.amazon.com/rds/},
	note = {tex.key= 1},
}

@inproceedings{nashdb_demo,
	series = {{VLDB} '19},
	title = {{NashDB}: {Fragmentation}, {Replication}, and {Provisioning} using {Economic} {Methods}},
	copyright = {All rights reserved},
	doi = {10.14778/3352063.3352077},
	booktitle = {{PVLDB}},
	author = {Marcus, Ryan and Zhang, Chi and Yu, Shuai and Kao, Geoffrey and Papaemmanouil, Olga},
	year = {2019},
	note = {props: demo},
	file = {Marcus et al. - 2019 - NashDB Fragmentation, Replication, and Provisioni.pdf:/home/ryan/Zotero/storage/SLUGXTH8/Marcus et al. - 2019 - NashDB Fragmentation, Replication, and Provisioni.pdf:application/pdf},
}

@article{url-pari_embedding,
	title = {Embedding tools, https://github.com/parimarjan/db-embedding-tools},
	url = {https://github.com/parimarjan/db-embedding-tools},
	note = {tex.key= 1},
}

@article{decima,
	series = {{arXiv} '18},
	title = {Learning {Scheduling} {Algorithms} for {Data} {Processing} {Clusters}},
	url = {http://arxiv.org/abs/1810.01963},
	urldate = {2019-06-01},
	journal = {arXiv:1810.01963 [cs, stat]},
	author = {Mao, Hongzi and Schwarzkopf, Malte and Venkatakrishnan, Shaileshh Bojja and Meng, Zili and Alizadeh, Mohammad},
	year = {2018},
	note = {arXiv: 1810.01963},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1810.01963 PDF:/home/ryan/Zotero/storage/SYCSA2SM/Mao et al. - 2018 - Learning Scheduling Algorithms for Data Processing.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/B6R6Q8L2/1810.html:text/html},
}

@article{url-ejob,
	title = {Ext-{JOB} queries, https://git.io/extended\_job},
	url = {https://git.io/extended_job},
	note = {tex.key= 1},
}

@article{lottery_ticket,
	series = {{ICLR} '19},
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	urldate = {2019-05-31},
	journal = {International Conference on Learning Representations},
	author = {Frankle, Jonathan and Carbin, Michael},
	year = {2019},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1803.03635 PDF:/home/ryan/Zotero/storage/U4G37MN6/Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/72N2Z3VY/1803.html:text/html},
}

@article{manifolds,
	series = {{TNNLS} '16},
	title = {Why {Deep} {Learning} {Works}: {A} {Manifold} {Disentanglement} {Perspective}},
	volume = {27},
	issn = {2162-237X},
	shorttitle = {Why {Deep} {Learning} {Works}},
	doi = {10.1109/TNNLS.2015.2496947},
	number = {10},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Brahma, P. P. and Wu, D. and She, Y.},
	month = oct,
	year = {2016},
	keywords = {Machine learning, Data models, deep hierarchical representations, Deep learning, deep learning works, disentanglement, disentanglement perspective, Kernel, learning (artificial intelligence), machine learning applications, manifold learning, manifold shaped data flattening, Manifolds, multilayer neural networks, neural nets, Neural networks, Nonhomogeneous media, Principal component analysis, unsupervised feature transformation, unsupervised pretraining},
	pages = {1997--2008},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/RABQXVES/7348689.html:text/html},
}

@article{holodetect,
	series = {{arXiv} '19},
	title = {{HoloDetect}: {Few}-{Shot} {Learning} for {Error} {Detection}},
	shorttitle = {{HoloDetect}},
	url = {http://arxiv.org/abs/1904.02285},
	doi = {10.1145/3299869.3319888},
	urldate = {2019-05-30},
	journal = {arXiv:1904.02285 [cs]},
	author = {Heidari, Alireza and McGrath, Joshua and Ilyas, Ihab F. and Rekatsinas, Theodoros},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02285},
	keywords = {Computer Science - Databases},
	file = {arXiv\:1904.02285 PDF:/home/ryan/Zotero/storage/WVH2F4MK/Heidari et al. - 2019 - HoloDetect Few-Shot Learning for Error Detection.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/ZC5T2F87/1904.html:text/html},
}

@article{neo_arxiv,
	series = {{arXiv} '19},
	title = {Neo: {Towards} {A} {Learned} {Query} {Optimizer}},
	shorttitle = {Neo},
	url = {http://arxiv.org/abs/1904.03711},
	urldate = {2019-05-30},
	journal = {arXiv:1904.03711 [cs]},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03711},
	keywords = {Computer Science - Databases},
	file = {arXiv\:1904.03711 PDF:/home/ryan/Zotero/storage/JDJIGZUT/Marcus et al. - 2019 - Neo A Learned Query Optimizer.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/BGGNLKPE/1904.html:text/html},
}

@article{nn_loss,
	series = {{TFML} '17},
	title = {On {Loss} {Functions} for {Deep} {Neural} {Networks} in {Classification}},
	url = {http://arxiv.org/abs/1702.05659},
	urldate = {2019-05-27},
	journal = {Theoretical Foundations of Machine Learning},
	author = {Janocha, Katarzyna and Czarnecki, Wojciech Marian},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.05659},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1702.05659 PDF:/home/ryan/Zotero/storage/2QZE7NTX/Janocha and Czarnecki - 2017 - On Loss Functions for Deep Neural Networks in Clas.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/ATAM3PGX/1702.html:text/html},
}

@inproceedings{jvm_db,
	series = {{ICDE} '06},
	title = {Compiled {Query} {Execution} {Engine} using {JVM}},
	doi = {10.1109/ICDE.2006.40},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering} ({ICDE}'06)},
	author = {{Jun Rao} and Pirahesh, H. and Mohan, C. and Lohman, G.},
	month = apr,
	year = {2006},
	keywords = {Database systems, Engines, Java, Optimizing compilers, Prototypes, Query processing, Relational databases, Runtime, Support vector machines, Virtual machining},
	pages = {23--23},
	file = {Jun Rao et al. - 2006 - Compiled Query Execution Engine using JVM.pdf:/home/ryan/Zotero/storage/WDMZYUVB/Jun Rao et al. - 2006 - Compiled Query Execution Engine using JVM.pdf:application/pdf},
}

@inproceedings{eddies,
	address = {New York, NY, USA},
	series = {{SIGMOD} '00},
	title = {Eddies: {Continuously} {Adaptive} {Query} {Processing}},
	isbn = {978-1-58113-217-5},
	shorttitle = {Eddies},
	url = {http://doi.acm.org/10.1145/342009.335420},
	doi = {10.1145/342009.335420},
	urldate = {2019-05-27},
	booktitle = {Proceedings of the 2000 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Avnur, Ron and Hellerstein, Joseph M.},
	year = {2000},
	note = {event-place: Dallas, Texas, USA},
	pages = {261--272},
	file = {Avnur and Hellerstein - 2000 - Eddies Continuously Adaptive Query Processing.pdf:/home/ryan/Zotero/storage/99X7Y42V/Avnur and Hellerstein - 2000 - Eddies Continuously Adaptive Query Processing.pdf:application/pdf},
}

@article{deep_schedule,
	series = {{arXiv} '18},
	title = {Learning {Scheduling} {Algorithms} for {Data} {Processing} {Clusters}},
	url = {http://arxiv.org/abs/1810.01963},
	urldate = {2019-05-21},
	journal = {arXiv:1810.01963 [cs, stat]},
	author = {Mao, Hongzi and Schwarzkopf, Malte and Venkatakrishnan, Shaileshh Bojja and Meng, Zili and Alizadeh, Mohammad},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1810.01963 PDF:/home/ryan/Zotero/storage/AXRLCLZD/Mao et al. - 2018 - Learning Scheduling Algorithms for Data Processing.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/MX5TXBG3/1810.html:text/html},
}

@inproceedings{sideways_vert,
	series = {{ICDE} '13},
	title = {Materialization strategies in the {Vertica} analytic database: {Lessons} learned},
	shorttitle = {Materialization strategies in the {Vertica} analytic database},
	doi = {10.1109/ICDE.2013.6544909},
	booktitle = {2013 {IEEE} 29th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Shrinivas, L. and Bodagala, S. and Varadarajan, R. and Cary, A. and Bharathan, V. and Bear, C.},
	month = apr,
	year = {2013},
	keywords = {query processing, Data models, Engines, Query processing, bookkeeping complexity, column store database, Complexity theory, Containers, Context, data compression, database management systems, late materialization, materialization strategy, memory, sideways information passing technique, spilling joins, storage management, TPC-H query, tuple reconstruction strategy, Vertica analytic database},
	pages = {1196--1207},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/4NWLVU8M/6544909.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/52QK3APJ/Shrinivas et al. - 2013 - Materialization strategies in the Vertica analytic.pdf:application/pdf},
}

@article{url-tpch-skewed,
	title = {{TPC}-{H} skewed, https://www.microsoft.com/en-us/download/details.aspx?id=52430},
	url = {https://www.microsoft.com/en-us/download/details.aspx?id=52430},
	note = {tex.key= 1},
}

@inproceedings{sideways_orig,
	series = {{ICDE} '08},
	title = {Sideways {Information} {Passing} for {Push}-{Style} {Query} {Processing}},
	doi = {10.1109/ICDE.2008.4497486},
	booktitle = {2008 {IEEE} 24th {International} {Conference} on {Data} {Engineering}},
	author = {Ives, Z. G. and Taylor, N. E.},
	month = apr,
	year = {2008},
	keywords = {query processing, Engines, Query processing, Runtime, adaptive information passing, bushy query, Concurrent computing, data management, decision making, Decision making, Delay, Distributed databases, general runtime decision-making technique, information management, information passing, Information science, multithreaded pipelined hash, Peer to peer computing, push-style query processing, sideways information, Switches, Web},
	pages = {774--783},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/M2FX6DJJ/4497486.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/URC3ABNU/Ives and Taylor - 2008 - Sideways Information Passing for Push-Style Query .pdf:application/pdf},
}

@inproceedings{tpch_analyzed,
	address = {Berlin, Heidelberg},
	series = {{TPC} '14},
	title = {{TPC}-{H} {Analyzed}: {Hidden} {Messages} and {Lessons} {Learned} from an {Influential} {Benchmark}},
	isbn = {978-3-319-04935-9},
	shorttitle = {{TPC}-{H} {Analyzed}},
	url = {https://doi.org/10.1007/978-3-319-04936-6_5},
	doi = {10.1007/978-3-319-04936-6_5},
	urldate = {2019-05-20},
	booktitle = {Revised {Selected} {Papers} of the 5th {TPC} {Technology} {Conference} on {Performance} {Characterization} and {Benchmarking} - {Volume} 8391},
	publisher = {Springer-Verlag},
	author = {Boncz, Peter and Neumann, Thomas and Erling, Orri},
	year = {2014},
	pages = {61--76},
	file = {Submitted Version:/home/ryan/Zotero/storage/DUI3MVMF/Boncz et al. - 2014 - TPC-H Analyzed Hidden Messages and Lessons Learne.pdf:application/pdf},
}

@inproceedings{jennie_sigmod11,
	address = {Athens, Greece},
	series = {{SIGMOD} '11},
	title = {Performance {Prediction} for {Concurrent} {Database} {Workloads}},
	isbn = {978-1-4503-0661-4},
	url = {http://doi.acm.org/10.1145/1989323.1989359},
	doi = {10.1145/1989323.1989359},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Duggan, Jennie and Cetintemel, Ugur and Papaemmanouil, Olga and Upfal, Eli},
	year = {2011},
	note = {tex.acmid= 1989359
tex.numpages= 12},
	keywords = {concurrency, query performance prediction},
	pages = {337--348},
	file = {Duggan et al. - 2011 - Performance Prediction for Concurrent Database Wor.pdf:/home/ryan/Zotero/storage/MQARVVKS/Duggan et al. - 2011 - Performance Prediction for Concurrent Database Wor.pdf:application/pdf},
}

@inproceedings{q-cop,
	series = {{ICDE} '10},
	title = {Q-{Cop}: {Avoiding} bad query mixes to minimize client timeouts under heavy loads},
	doi = {10.1109/ICDE.2010.5447850},
	booktitle = {Data {Engineering} ({ICDE}), 2010 {IEEE} 26th {International} {Conference} on},
	author = {Tozer, S. and Brecht, T. and Aboulnaga, A.},
	month = mar,
	year = {2010},
	keywords = {admission control, learning based},
	pages = {397--408},
	file = {Tozer et al. - 2010 - Q-Cop Avoiding bad query mixes to minimize client.pdf:/home/ryan/Zotero/storage/ETEDAHWN/Tozer et al. - 2010 - Q-Cop Avoiding bad query mixes to minimize client.pdf:application/pdf},
}

@inproceedings{thompson_bound_time,
	series = {{ALT} '12},
	title = {Thompson sampling: {An} asymptotically optimal finite-time analysis},
	booktitle = {International {Conference} on {Algorithmic} {Learning} {Theory}},
	author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R√©mi},
	year = {2012},
	file = {1205.4217.pdf:/home/ryan/Zotero/storage/BR6D8Q3C/1205.4217.pdf:application/pdf},
}

@article{thompson_infotheory,
	series = {Machine {Learning} {Research} '14},
	title = {An information-theoretic analysis of {Thompson} sampling},
	journal = {Journal of Machine Learning Research},
	author = {Russo, Daniel and Roy, Benjamin Van},
	year = {2014},
	file = {14-087.pdf:/home/ryan/Zotero/storage/WA2MJ6D7/14-087.pdf:application/pdf},
}

@article{beam_search_related,
	title = {Beam-{ACO}: {Hybridizing} {Ant} {Colony} {Optimization} with {Beam} {Search}: {An} {Application} to {Open} {Shop} {Scheduling}},
	volume = {32},
	issn = {0305-0548},
	url = {http://dx.doi.org/10.1016/j.cor.2003.11.018},
	doi = {10.1016/j.cor.2003.11.018},
	number = {6},
	journal = {Comput. Oper. Res.},
	author = {Blum, Christian},
	month = jun,
	year = {2005},
	note = {tex.acmid= 1241785
tex.issue\_date= June 2005
tex.numpages= 27},
	pages = {1565--1591},
	file = {Beam-ACO hybridizing ant colony optimization.pdf:/home/ryan/Zotero/storage/N44U52WA/Beam-ACO hybridizing ant colony optimization.pdf:application/pdf},
}

@inproceedings{restless_thompson,
	series = {{ICMLA} '11},
	title = {Thompson sampling for dynamic multi-armed bandits},
	volume = {1},
	booktitle = {2011 10th {International} {Conference} on {Machine} {Learning} and {Applications} and {Workshops} ({ICMLA})},
	publisher = {IEEE},
	author = {Gupta, Neha and Granmo, Ole-Christoffer and Agrawala, Ashok},
	year = {2011},
	pages = {484--489},
	file = {gupta2011.pdf:/home/ryan/Zotero/storage/27WA3TGR/gupta2011.pdf:application/pdf},
}

@inproceedings{hedger,
	series = {{ICML} '00},
	title = {Practical reinforcement learning in continuous spaces},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Smart, William and Kaelbling, Leslie},
	year = {2000},
	pages = {903--910},
	file = {bf8bbf69510fa15595064fe21fe4b2e69b16.pdf:/home/ryan/Zotero/storage/GXZIEFFK/bf8bbf69510fa15595064fe21fe4b2e69b16.pdf:application/pdf},
}

@inproceedings{thompson_bound,
	series = {{AISTATS} '13},
	title = {Further {Optimal} {Regret} {Bounds} for {Thompson} {Sampling}},
	booktitle = {The {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Agrawal, Shipra and Goyal, Navin},
	year = {2013},
	file = {agrawal13a.pdf:/home/ryan/Zotero/storage/GB3G26F4/agrawal13a.pdf:application/pdf},
}

@article{onlinebinpacking,
	series = {{JACM} '02},
	title = {On the {Online} {Bin} {Packing} {Problem}},
	volume = {49},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/585265.585269},
	doi = {10.1145/585265.585269},
	number = {5},
	journal = {Journal of the ACM},
	author = {Seiden, Steven},
	month = sep,
	year = {2002},
	note = {tex.acmid= 585269
tex.issue\_date= September 2002
tex.numpages= 32},
	keywords = {Bin packing, online algorithms},
	pages = {640--671},
	file = {p640-s_seiden.pdf:/home/ryan/Zotero/storage/Q2GBQU2K/p640-s_seiden.pdf:application/pdf},
}

@inproceedings{bootstrapping,
	series = {Maching {Learning} '96},
	title = {Bagging {Predictors}},
	booktitle = {Machine {Learning}},
	author = {Breiman, Leo},
	year = {1996},
	file = {Breiman1996.pdf:/home/ryan/Zotero/storage/CJZ7GTC4/Breiman1996.pdf:application/pdf},
}

@book{os,
	title = {Operating system concepts},
	volume = {4},
	publisher = {Addison-Wesley Reading},
	author = {Silberschatz, Abraham and Galvin, Peter and Gagne, Greg and Silberschatz, A.},
	year = {1998},
}

@inproceedings{sparrow,
	address = {Farminton, Pennsylvania},
	series = {{SOSP} '13},
	title = {Sparrow: {Distributed}, {Low} {Latency} {Scheduling}},
	isbn = {978-1-4503-2388-8},
	url = {http://doi.acm.org/10.1145/2517349.2522716},
	doi = {10.1145/2517349.2522716},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Ousterhout, Kay and Wendell, Patrick and Zaharia, Matei and Stoica, Ion},
	year = {2013},
	note = {tex.acmid= 2522716
tex.numpages= 16},
	keywords = {placement, uses latency prediction},
	pages = {69--84},
	file = {p69-ousterhout.pdf:/home/ryan/Zotero/storage/XSPSZWHZ/p69-ousterhout.pdf:application/pdf},
}

@phdthesis{beam_search_original,
	type = {phdthesis},
	title = {The {Harpy} {Speech} {Recognition} {System}.},
	school = {Stanford University},
	author = {Lowerre, Bruce T.},
	year = {1976},
	file = {rq916rn6924.pdf:/home/ryan/Zotero/storage/BAF4JQV3/rq916rn6924.pdf:application/pdf},
}

@article{weka,
	series = {{SIGKDD} '09},
	title = {The {WEKA} {Data} {Mining} {Software}: {An} {Update}},
	volume = {11},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/1656274.1656278},
	doi = {10.1145/1656274.1656278},
	number = {1},
	journal = {The Association for Computing Machinery's Special Interest Group on Knowledge Discovery and Data Mining Exploration Newsletter},
	author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
	month = nov,
	year = {2009},
	note = {tex.acmid= 1656278
tex.issue\_date= June 2009
tex.numpages= 9},
	pages = {10--18},
	file = {The_WEKA_data_mining_software_An_update.pdf:/home/ryan/Zotero/storage/FFU6WF4R/The_WEKA_data_mining_software_An_update.pdf:application/pdf},
}

@article{q,
	series = {Machine learning '92},
	title = {Q-learning},
	volume = {8},
	number = {3-4},
	journal = {Machine learning},
	author = {Watkins, Christopher JCH and Dayan, Peter},
	year = {1992},
	pages = {279--292},
	file = {10.1.1.466.7149.pdf:/home/ryan/Zotero/storage/TGRCCSUQ/10.1.1.466.7149.pdf:application/pdf},
}

@inproceedings{sqlvm,
	series = {{CIDR} '13},
	title = {{SQLVM}: {Performance} {Isolation} in {Multi}-{Tenant} {Relational} {Database}-as-a-{Service}},
	url = {http://research.microsoft.com/apps/pubs/default.aspx?id=184020},
	booktitle = {6th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Narasayya, Vivek and Das, Sudipto and Syamala, Manoj and Chandramouli, Badrish and Chaudhuri, Surajit},
	month = jan,
	year = {2013},
	keywords = {placement},
	file = {CIDR13_Paper25.pdf:/home/ryan/Zotero/storage/SM344SEJ/CIDR13_Paper25.pdf:application/pdf},
}

@book{appalg,
	title = {Approximation algorithms},
	publisher = {Springer Science \& Business Media},
	author = {Vazirani, Vijay},
	year = {2013},
	file = {book.pdf:/home/ryan/Zotero/storage/A58ESCSX/book.pdf:application/pdf},
}

@article{thompson,
	series = {Biometrika '33},
	title = {On the {Likelihood} that {One} {Unknown} {Probability} {Exceeds} {Another} in {View} of the {Evidence} of {Two} {Samples}},
	journal = {Biometrika},
	author = {Thompson, William R.},
	year = {1933},
	file = {2332286.pdf:/home/ryan/Zotero/storage/E62IABGJ/2332286.pdf:application/pdf},
}

@inproceedings{qlearning_cont,
	series = {{AKCAI} '99},
	title = {Q-learning in continuous state and action spaces},
	booktitle = {Australasian {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Gaskett, Chris and Wettergreen, David and Zelinsky, Alexander},
	year = {1999},
	pages = {417--428},
	file = {99ai.kambara.pdf:/home/ryan/Zotero/storage/F4WJ3G2C/99ai.kambara.pdf:application/pdf},
}

@inproceedings{thompson_intro,
	series = {{NIPS}'11},
	title = {An empirical evaluation of {Thompson} sampling},
	booktitle = {Advances in neural information processing systems},
	author = {Chapelle, Olivier and Li, Lihong},
	year = {2011},
	file = {4321-an-empirical-evaluation-of-thompson-sampling.pdf:/home/ryan/Zotero/storage/R72AHPEZ/4321-an-empirical-evaluation-of-thompson-sampling.pdf:application/pdf},
}

@inproceedings{thompson_complex,
	series = {{ICML} '14},
	title = {Thompson {Sampling} for {Complex} {Online} {Problems}.},
	volume = {14},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
	year = {2014},
	pages = {100--108},
	file = {gopalan14.pdf:/home/ryan/Zotero/storage/JNCV64RU/gopalan14.pdf:application/pdf},
}

@inproceedings{nonstationary_thompson,
	series = {{NIPS} '14},
	title = {Stochastic multi-armed-bandit problem with non-stationary rewards},
	booktitle = {Advances in neural information processing systems},
	author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
	year = {2014},
	pages = {199--207},
	file = {5378-beta-negative-binomial-process-and-exchangeable-random-partitions-for-mixed-membership-modeling.pdf:/home/ryan/Zotero/storage/M7ZNEUXP/5378-beta-negative-binomial-process-and-exchangeable-random-partitions-for-mixed-membership-modeling.pdf:application/pdf},
}

@inproceedings{bayes_indep,
	series = {{ECML} '98},
	title = {Naive ({Bayes}) at forty: {The} independence assumption in information retrieval},
	booktitle = {European conference on machine learning},
	publisher = {Springer},
	author = {Lewis, David D.},
	year = {1998},
	pages = {4--15},
	file = {15a220ce74badf755aae870fa0b69ee2b82a.pdf:/home/ryan/Zotero/storage/2D93HHFH/15a220ce74badf755aae870fa0b69ee2b82a.pdf:application/pdf},
}

@book{chi2,
	title = {A guide to chi-squared testing},
	volume = {280},
	publisher = {John Wiley \& Sons},
	author = {Greenwood, Priscilla E. and Nikulin, Michael S.},
	year = {1996},
}

@inproceedings{coevolution,
	address = {London, UK, UK},
	series = {{PPSN} '94},
	title = {A {Cooperative} {Coevolutionary} {Approach} to {Function} {Optimization}},
	isbn = {3-540-58484-6},
	url = {http://dl.acm.org/citation.cfm?id=645822.670374},
	booktitle = {Proceedings of the {International} {Conference} on {Evolutionary} {Computation}. {The} {Third} {Conference} on {Parallel} {Problem} {Solving} from {Nature}: {Parallel} {Problem} {Solving} from {Nature}},
	publisher = {Springer-Verlag},
	author = {Potter, Mitchell A. and Jong, Kenneth A. De},
	year = {1994},
	note = {tex.acmid= 670374
tex.numpages= 9},
	pages = {249--257},
	file = {Potter and Jong - 1994 - A Cooperative Coevolutionary Approach to Function .pdf:/home/ryan/Zotero/storage/GZPUCHSP/Potter and Jong - 1994 - A Cooperative Coevolutionary Approach to Function .pdf:application/pdf},
}

@inproceedings{noisy_neighbor,
	address = {Phoenix, Arizona, USA},
	series = {{MMSys} '10},
	title = {Empirical {Evaluation} of {Latency}-sensitive {Application} {Performance} in the {Cloud}},
	isbn = {978-1-60558-914-5},
	url = {http://doi.acm.org/10.1145/1730836.1730842},
	doi = {10.1145/1730836.1730842},
	booktitle = {Proceedings of the {First} {Annual} {ACM} {SIGMM} {Conference} on {Multimedia} {Systems}},
	publisher = {ACM},
	author = {Barker, Sean Kenneth and Shenoy, Prashant},
	year = {2010},
	note = {tex.acmid= 1730842
tex.numpages= 12},
	keywords = {multimedia, resource isolation, virtualization},
	pages = {35--46},
	file = {mmsys10-latency.pdf:/home/ryan/Zotero/storage/74UKGDZ3/mmsys10-latency.pdf:application/pdf},
}

@inproceedings{montecarlo_cont,
	series = {{NIPS} '07},
	title = {Reinforcement learning in continuous action spaces through sequential {Monte} {Carlo} methods},
	booktitle = {Advances in neural information processing systems},
	author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
	year = {2007},
	pages = {833--840},
	file = {NIPS2007_959.pdf:/home/ryan/Zotero/storage/KP9QIVBH/NIPS2007_959.pdf:application/pdf},
}

@article{bootstrapping-nonbias,
	series = {American {Statistical} {Association} '87},
	title = {Better {Bootstrap} {Confidence} {Intervals}},
	volume = {82},
	number = {397},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	year = {1987},
	file = {BCa.pdf:/home/ryan/Zotero/storage/IBUG95DK/BCa.pdf:application/pdf},
}

@article{astar,
	series = {{SSC} '68},
	title = {A {Formal} {Basis} for the {Heuristic} {Determination} of {Minimum} {Cost} {Paths}},
	volume = {4},
	issn = {0536-1567},
	doi = {10.1109/TSSC.1968.300136},
	number = {2},
	journal = {Systems Science and Cybernetics, IEEE Transactions on},
	author = {Hart, P. E. and {N.J.Nilsson} and B, Raphael},
	month = jul,
	year = {1968},
	keywords = {Automatic control, Automatic programming, Chemical technology, Costs, Functional programming, Gradient methods, Instruction sets, Mathematical programming, Minimax techniques, Minimization methods},
	pages = {100--107},
	file = {Hart et al. - 1968 - A Formal Basis for the Heuristic Determination of .pdf:/home/ryan/Zotero/storage/ECJR93ES/Hart et al. - 1968 - A Formal Basis for the Heuristic Determination of .pdf:application/pdf},
}

@article{avl,
	series = {Soviet {Mathematics} '62},
	title = {An algorithm for the organization of information},
	volume = {146},
	journal = {Proceedings of the USSR Academy of Sciences},
	author = {Adelson-Velsky, Georgy and Landis, Evgenii},
	year = {1962},
	pages = {1259--1263},
	file = {Adelson-Velsky and Landis - 1962 - An algorithm for the organization of information.pdf:/home/ryan/Zotero/storage/4ZUZ8N56/Adelson-Velsky and Landis - 1962 - An algorithm for the organization of information.pdf:application/pdf},
}

@article{url-demovideo,
	title = {Video, http://youtu.be/{YAKRxSoUs18}},
	url = {http://youtu.be/YAKRxSoUs18},
	note = {tex.key= 1},
}

@article{dstat,
	title = {dstat, http://dag.wiee.rs/home-made/dstat/},
	url = {http://dag.wiee.rs/home-made/dstat/},
	note = {tex.key= 1},
}

@book{moderncol,
	address = {Hanover, MA, USA},
	title = {The {Design} and {Implementation} of {Modern} {Column}-{Oriented} {Database} {Systems}},
	isbn = {1-60198-754-4 978-1-60198-754-9},
	publisher = {Now Publishers Inc.},
	author = {Abadi, Daniel and Boncz, Peter and Harizopoulos, Stavros and Idreos, Stratos and Madden, Samuel},
	year = {2013},
	file = {abadi-column-stores.pdf:/home/ryan/Zotero/storage/TV3IUCBQ/abadi-column-stores.pdf:application/pdf},
}

@article{url-google,
	title = {Google {Cloud} {Platform}, https://cloud.google.com/},
	url = {https://cloud.google.com/},
	note = {tex.key= 1},
}

@article{url-tpch,
	title = {The {TPC}-{H} benchmark, http://www.tpc.org/tpch/},
	url = {http://www.tpc.org/tpch/},
	note = {tex.key= 1},
}

@article{url-ms_pivot,
	title = {{SQL} {Server} {PIVOT}, https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot},
	url = {https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot},
	note = {tex.key= 1},
}

@article{url-linode,
	title = {Linode, https://linode.com},
	url = {https://linode.com},
	note = {tex.key= 1},
}

@article{emd,
	series = {{TCS} '11},
	title = {Sublinear time algorithms for earth mover's distance},
	volume = {48},
	number = {2},
	journal = {Theory of Computing Systems},
	author = {Ba, Khanh Do and Nguyen, Huy L. and Nguyen, Huy N. and Rubinfeld, Ronitt},
	year = {2011},
	pages = {428--442},
	file = {0904.0292.pdf:/home/ryan/Zotero/storage/BQG6MB6B/0904.0292.pdf:application/pdf},
}

@inproceedings{pslas,
	series = {{CIDR} '15},
	title = {Changing the {Face} of {Database} {Cloud} {Services} with {Personalized} {Service} {Level} {Agreements}},
	booktitle = {7th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Ortiz, Jennifer and Almeida, Victor Teixeira de and Balazinska, Magdalena},
	year = {2015},
	file = {ortiz-cidr15.pdf:/home/ryan/Zotero/storage/SIJMJVQS/ortiz-cidr15.pdf:application/pdf},
}

@inproceedings{ernest,
	series = {{NSDI} '16},
	title = {Ernest: efficient performance prediction for large-scale advanced analytics},
	booktitle = {13th {USENIX} {Symposium} on {Networked} {Systems} {Design} and {Implementation}},
	author = {Venkataraman, Shivaram and Yang, Zongheng and Franklin, Michael and Recht, Benjamin and Stoica, Ion},
	year = {2016},
	pages = {363--378},
	file = {Venkataraman et al. - 2016 - Ernest efficient performance prediction for large.pdf:/home/ryan/Zotero/storage/PZBJ9EJF/Venkataraman et al. - 2016 - Ernest efficient performance prediction for large.pdf:application/pdf},
}

@inproceedings{sci_place,
	address = {San Jose, California, USA},
	series = {{DIDC} '11},
	title = {Integrated {Data} {Placement} and {Task} {Assignment} for {Scientific} {Workflows} in {Clouds}},
	isbn = {978-1-4503-0704-8},
	url = {http://doi.acm.org/10.1145/1996014.1996022},
	doi = {10.1145/1996014.1996022},
	booktitle = {Proceedings of the {Fourth} {International} {Workshop} on {Data}-intensive {Distributed} {Computing}},
	publisher = {ACM},
	author = {Catalyurek, Umit V. and Kaya, Kamer and Ucar, Bora},
	year = {2011},
	note = {tex.acmid= 1996022
tex.numpages= 10},
	keywords = {placement, uses latency prediction, scheduling},
	pages = {45--54},
	file = {Integrated_data_placement_and_task_assignment_for_.pdf:/home/ryan/Zotero/storage/QQ767XGF/Integrated_data_placement_and_task_assignment_for_.pdf:application/pdf},
}

@inproceedings{consolidation,
	series = {{SIGMOD} '11},
	title = {Workload-aware database monitoring and consolidation},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Curino, Carlo and Jones, Evan and Madden, Samuel and Balakrishnan, Hari},
	year = {2011},
	keywords = {placement, uses latency prediction, provisioning},
	pages = {313--324},
	file = {74218.pdf:/home/ryan/Zotero/storage/VS2A93AS/74218.pdf:application/pdf},
}

@inproceedings{smartsla,
	series = {{ICDE} '11},
	title = {Intelligent management of virtualized resources for database systems in cloud environment},
	booktitle = {{IEEE} 27th {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {Xiong, Pengcheng and Chi, Yun and Zhu, Shenghuo and Moon, Hyun Jin and Pu, Calton and Hacigumus, Hakan},
	year = {2011},
	keywords = {learning based, uses latency prediction, provisioning},
	pages = {87--98},
	file = {11icde_smartsla.pdf:/home/ryan/Zotero/storage/BUF4HEEF/11icde_smartsla.pdf:application/pdf},
}

@inproceedings{cost_wait,
	series = {{CLOUD} '11},
	title = {Cost-wait trade-offs in client-side resource provisioning with elastic clouds},
	booktitle = {2011 {IEEE} international conference on {Cloud} computing},
	author = {Genaud, St√©phane and Gossa, Julien},
	year = {2011},
	keywords = {placement, uses latency prediction, provisioning},
	file = {cloudSched.pdf:/home/ryan/Zotero/storage/62CQG8Q6/cloudSched.pdf:application/pdf},
}

@article{tpch,
	series = {{SIGMOD} '00},
	title = {New {TPC} {Benchmarks} for {Decision} {Support} and {Web} {Commerce}},
	volume = {29},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/369275.369291},
	doi = {10.1145/369275.369291},
	number = {4},
	journal = {SIGMOD Records},
	author = {Poess, Meikel and Floyd, Chris},
	month = dec,
	year = {2000},
	note = {tex.acmid= 369291
tex.issue\_date= Dec. 2000
tex.numpages= 8},
	pages = {64--71},
}

@inproceedings{learning_latency,
	series = {{ICDE} '12},
	title = {Learning-based query performance modeling and prediction},
	booktitle = {2012 {IEEE} 28th {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {{Mert Akdere} and {Ugur Cetintemel}},
	year = {2012},
	pages = {390--401},
	file = {cs11-01.pdf:/home/ryan/Zotero/storage/BAJ6U72X/cs11-01.pdf:application/pdf},
}

@inproceedings{perfenforce_demo,
	address = {San Francisco, California, USA},
	series = {{SIGMOD} '16},
	title = {{PerfEnforce} {Demonstration}: {Data} {Analytics} with {Performance} {Guarantees}},
	isbn = {978-1-4503-3531-7},
	url = {http://doi.acm.org/10.1145/2882903.2899402},
	doi = {10.1145/2882903.2899402},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Ortiz, Jennifer and Lee, Brendan and Balazinska, Magdalena},
	year = {2016},
	note = {tex.acmid= 2899402
tex.numpages= 4},
	keywords = {learning based, provisioning},
	pages = {2141--2144},
	file = {SIGMOD-Demo-2016.pdf:/home/ryan/Zotero/storage/R8PMUS6W/SIGMOD-Demo-2016.pdf:application/pdf},
}

@article{opennebula,
	series = {{IEEE} {IC} '09},
	title = {Virtual {Infrastructure} {Management} in {Private} and {Hybrid} {Clouds}},
	volume = {13},
	issn = {1089-7801},
	doi = {10.1109/MIC.2009.119},
	number = {5},
	journal = {Internet Computing, IEEE},
	author = {Sotomayor, B. and Montero, Ruben and Llorente, I. and Foster, I.},
	month = sep,
	year = {2009},
	keywords = {placement, scheduling},
	pages = {14--22},
	file = {Virtual_infrastructure_management_in_pri.pdf:/home/ryan/Zotero/storage/WB73IUSD/Virtual_infrastructure_management_in_pri.pdf:application/pdf},
}

@inproceedings{slos,
	series = {{ICDE} '14},
	title = {Towards {Multi}-{Tenant} {Performance} {SLOs}},
	volume = {26.6},
	doi = {10.1109/TKDE.2013.74},
	booktitle = {{IEEE} {Transactions} on {Knowledge} and {Data} {Engineering}},
	author = {Lang, W. and Shankar, S. and Patel, J. and Kalhan, A.},
	month = jun,
	year = {2014},
	keywords = {placement, uses latency prediction},
	pages = {1447--1463},
	file = {10.1.1.456.3924.pdf:/home/ryan/Zotero/storage/2975BDEP/10.1.1.456.3924.pdf:application/pdf},
}

@inproceedings{contender,
	series = {{EDBT} '14},
	title = {Contender: {A} {Resource} {Modeling} {Approach} for {Concurrent} {Query} {Performance} {Prediction}},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Extending} {Database} {Technology}},
	author = {Duggan, Jennie and Papaemmanouil, Olga and Cetintemel, Ugur and Upfal, Eli},
	year = {2014},
	pages = {109--120},
	file = {Duggan et al. - 2014 - Contender A Resource Modeling Approach for Concur.pdf:/home/ryan/Zotero/storage/UG4D9BCQ/Duggan et al. - 2014 - Contender A Resource Modeling Approach for Concur.pdf:application/pdf},
}

@article{icbs,
	series = {{VLDB} '11},
	title = {{iCBS}: {Incremental} {Cost}-based {Scheduling} {Under} {Piecewise} {Linear} {SLAs}},
	volume = {4},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2002938.2002942},
	doi = {10.14778/2002938.2002942},
	number = {9},
	journal = {PVLDB},
	author = {Chi, Yun and Moon, Hyun Jin and Hacigumus, Hakan},
	year = {2011},
	note = {tex.acmid= 2002942
tex.issue\_date= June 2011
tex.numpages= 12},
	keywords = {uses latency prediction, scheduling},
	pages = {563--574},
	file = {iCBS_Incremental_cost-based_scheduling_u.pdf:/home/ryan/Zotero/storage/HPJBEITQ/iCBS_Incremental_cost-based_scheduling_u.pdf:application/pdf},
}

@inproceedings{delphi_pythia,
	address = {New York, New York, USA},
	series = {{SIGMOD} '13},
	title = {Characterizing {Tenant} {Behavior} for {Placement} and {Crisis} {Mitigation} in {Multitenant} {DBMSs}},
	isbn = {978-1-4503-2037-5},
	url = {http://doi.acm.org/10.1145/2463676.2465308},
	doi = {10.1145/2463676.2465308},
	booktitle = {Proceedings of the 2013 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Elmore, Aaron J. and Das, Sudipto and Pucher, Alexander and Agrawa, Divyakant and Abbadi, Amr El and Yan, Xifeng},
	year = {2013},
	note = {tex.acmid= 2465308
tex.numpages= 12},
	keywords = {learning based, placement, uses latency prediction},
	pages = {517--528},
	file = {Elmore-Pythia.pdf:/home/ryan/Zotero/storage/PFX8NEPJ/Elmore-Pythia.pdf:application/pdf},
}

@article{url-msazure,
	title = {Microsoft {Azure} {Services}, http://www.microsoft.com/azure/},
	url = {http://www.microsoft.com/azure/},
	note = {tex.key= 1},
}

@misc{url-postgres,
	title = {{PostgreSQL} database, http://www.postgresql.org/},
	url = {http://www.postgresql.org/},
	year = {2024},
}

@inproceedings{sla-tree,
	address = {Uppsala, Sweden},
	series = {{EDBT} '11},
	title = {{SLA}-tree: {A} {Framework} for {Efficiently} {Supporting} {SLA}-based {Decisions} in {Cloud} {Computing}},
	isbn = {978-1-4503-0528-0},
	url = {http://doi.acm.org/10.1145/1951365.1951383},
	doi = {10.1145/1951365.1951383},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {ACM},
	author = {Chi, Yun and Moon, Hyun Jin and Hacigumus, Hakan and Tatemura, Junichi},
	year = {2011},
	note = {tex.acmid= 1951383
tex.numpages= 12},
	keywords = {uses latency prediction, scheduling},
	pages = {129--140},
	file = {7466b729b64fde626605f3a9e6e45734f7f6.pdf:/home/ryan/Zotero/storage/G3QPHPUJ/7466b729b64fde626605f3a9e6e45734f7f6.pdf:application/pdf},
}

@inproceedings{hadoop_place,
	series = {{GRID} '12},
	title = {Minimizing {Cost} of {Virtual} {Machines} for {Deadline}-{Constrained} {MapReduce} {Applications} in the {Cloud}},
	doi = {10.1109/Grid.2012.19},
	booktitle = {Grid {Computing} ({GRID}), 2012 {ACM}/{IEEE} 13th {International} {Conference} on},
	author = {Hwang, Eunji and Kim, Kyong Hoon},
	month = sep,
	year = {2012},
	keywords = {uses latency prediction, provisioning},
	pages = {130--138},
	file = {06319163.pdf:/home/ryan/Zotero/storage/KE8XP8WR/06319163.pdf:application/pdf},
}

@inproceedings{pmax,
	address = {Genoa, Italy},
	series = {{EDBT} '13},
	title = {{PMAX}: {Tenant} {Placement} in {Multitenant} {Databases} for {Profit} {Maximization}},
	isbn = {978-1-4503-1597-5},
	url = {http://doi.acm.org/10.1145/2452376.2452428},
	doi = {10.1145/2452376.2452428},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {ACM},
	author = {Liu, Ziyang and Hacigumus, Hakan and Moon, Hyun Jin and Chi, Yun and Hsiung, Wang-Pin},
	year = {2013},
	note = {tex.acmid= 2452428
tex.numpages= 12},
	keywords = {placement, uses latency prediction, provisioning},
	pages = {442--453},
	file = {10.1.1.308.8881.pdf:/home/ryan/Zotero/storage/FBBSNVCB/10.1.1.308.8881.pdf:application/pdf},
}

@inproceedings{cloudoptimizer,
	address = {Genoa, Italy},
	series = {{EDBT} '13},
	title = {{CloudOptimizer}: {Multi}-tenancy for {I}/{O}-bound {OLAP} {Workloads}},
	isbn = {978-1-4503-1597-5},
	url = {http://doi.acm.org/10.1145/2452376.2452386},
	doi = {10.1145/2452376.2452386},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {ACM},
	author = {Mahmoud, Hatem and Moon, Hyun Jin and Chi, Yun and Hacigumus, Hakan and Agrawal, Divyakant and El-Abbadi, Amr},
	year = {2013},
	note = {tex.acmid= 2452386
tex.numpages= 12},
	keywords = {placement},
	pages = {77--88},
	file = {10.1.1.298.4249.pdf:/home/ryan/Zotero/storage/ZZN8I2DR/10.1.1.298.4249.pdf:application/pdf},
}

@book{ffd,
	address = {New York, NY, USA},
	title = {Knapsack {Problems}: {Algorithms} and {Computer} {Implementations}},
	isbn = {0-471-92420-2},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Martello, Silvano and Toth, Paolo},
	year = {1990},
	file = {KnapsackProblems.pdf:/home/ryan/Zotero/storage/G6TK7F55/KnapsackProblems.pdf:application/pdf},
}

@inproceedings{generic_prov,
	series = {{ICDEW} '10},
	title = {A generic auto-provisioning framework for cloud databases},
	booktitle = {{IEEE} 26th {International} {Conference} on {Data} {Engineering} {Workshops}},
	publisher = {IEEE},
	author = {Rogers, Jennie and Papaemmanouil, Olga and Cetintemel, Ugur},
	year = {2010},
	keywords = {uses latency prediction, provisioning},
	pages = {63--68},
	file = {3e4951a8f198ba011e218ee46cbf13e8bfd9.pdf:/home/ryan/Zotero/storage/JNTI2EMU/3e4951a8f198ba011e218ee46cbf13e8bfd9.pdf:application/pdf},
}

@inproceedings{activesla,
	address = {Cascais, Portugal},
	series = {{SoCC} '11},
	title = {{ActiveSLA}: {A} {Profit}-oriented {Admission} {Control} {Framework} for {Database}-as-a-{Service} {Providers}},
	isbn = {978-1-4503-0976-9},
	url = {http://doi.acm.org/10.1145/2038916.2038931},
	doi = {10.1145/2038916.2038931},
	booktitle = {Proceedings of the 2nd {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Xiong, Pengcheng and Chi, Yun and Zhu, Shenghuo and Tatemura, Junichi and Pu, Calton and Hacigumus, Hakan},
	year = {2011},
	note = {tex.acmid= 2038931
tex.articleno= 15
tex.numpages= 14},
	keywords = {machine learning, admission control, learning based, uses latency prediction},
	pages = {15:1--15:14},
	file = {Xiong et al. - 2011 - ActiveSLA A Profit-oriented Admission Control Fra.pdf:/home/ryan/Zotero/storage/74C6SDA2/Xiong et al. - 2011 - ActiveSLA A Profit-oriented Admission Control Fra.pdf:application/pdf},
}

@article{wisedb-long,
	series = {Technical {Report}, {arXiv}.org},
	title = {{WiSeDB}: {A} {Learning}-based {Workload} {Management} {Advisor} for {Cloud} {Databases}},
	journal = {Technical Report, arXiv:1601.08221 [cs.DB]},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	file = {bcdd6e98b3bdbb24882e21a6241344d6ae0e.pdf:/home/ryan/Zotero/storage/CKCN5GQT/bcdd6e98b3bdbb24882e21a6241344d6ae0e.pdf:application/pdf},
}

@inproceedings{azar,
	series = {{SPAA} '13},
	title = {Cloud scheduling with setup cost},
	booktitle = {Proceedings of the twenty-fifth annual {ACM} symposium on {Parallelism} in algorithms and architectures},
	author = {Azar, Yossi and Ben-Aroya, Naama and Devanur, Nikhil R. and Jain, Navendu},
	year = {2013},
	keywords = {placement, uses latency prediction, provisioning},
	file = {schedule6.pdf:/home/ryan/Zotero/storage/D4QRWX5T/schedule6.pdf:application/pdf},
}

@inproceedings{leitner,
	series = {{CLOUD} '12},
	title = {Cost-{Efficient} and {Application} {SLA}-{Aware} {Client} {Side} {Request} {Scheduling} in an {Infrastructure}-as-a-{Service} {Cloud}},
	doi = {10.1109/CLOUD.2012.21},
	booktitle = {Proceedings of the 2012 {IEEE} {Fifth} {International} {Conference} on {Cloud} {Computing}},
	author = {Leitner, Philipp and Hummer, Waldemar and Satzger, Benjamin and Inzinger, Christian and Dustdar, Schahram},
	year = {2012},
	keywords = {placement, uses latency prediction, provisioning},
	file = {Cloud 2012 Ph. Leitner Cost-Efficient.pdf:/home/ryan/Zotero/storage/VBXAFTBJ/Cloud 2012 Ph. Leitner Cost-Efficient.pdf:application/pdf},
}

@article{learned_xaction,
	series = {{VLDB} '11},
	title = {On {Predictive} {Modeling} for {Optimizing} {Transaction} {Execution} in {Parallel} {OLTP} {Systems}},
	volume = {5},
	doi = {10.14778/2078324.2078325},
	number = {2},
	journal = {PVLDB},
	author = {Pavlo, Andrew and P. C. Jones, Evan and Zdonik, Stan},
	year = {2011},
	pages = {86--96},
	file = {Pavlo et al. - 2011 - On Predictive Modeling for Optimizing Transaction .pdf:/home/ryan/Zotero/storage/LL6E86M5/Pavlo et al. - 2011 - On Predictive Modeling for Optimizing Transaction .pdf:application/pdf},
}

@article{wangLiftingHazeCloud2016,
	title = {Lifting the {Haze} off the {Cloud}: {A} {Consumer}-centric {Market} for {Database} {Computation} in the {Cloud}},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {Lifting the {Haze} off the {Cloud}},
	url = {http://dl.acm.org/citation.cfm?id=3025111.3025119},
	number = {4},
	urldate = {2016-12-17},
	journal = {PVLDB},
	author = {Wang, Yue and Meliou, Alexandra and Miklau, Gerome},
	year = {2016},
	pages = {373--384},
	file = {p373-wang.pdf:/home/ryan/Zotero/storage/H4G3WSAV/p373-wang.pdf:application/pdf},
}

@article{khanNewIntrusionDetection2007,
	series = {{VLDB} '07},
	title = {A new intrusion detection system using support vector machines and hierarchical clustering},
	volume = {16},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-006-0002-5},
	doi = {10.1007/s00778-006-0002-5},
	language = {en},
	number = {4},
	urldate = {2018-11-22},
	journal = {PVLDB},
	author = {Khan, Latifur and Awad, Mamoun and Thuraisingham, Bhavani},
	year = {2007},
	keywords = {Anomaly Detection, Reference Vector, Support Vector, Support Vector Machine, Training Time},
	pages = {507--521},
	file = {Khan et al. - 2007 - A new intrusion detection system using support vec.pdf:/home/ryan/Zotero/storage/8BPLXAAM/Khan et al. - 2007 - A new intrusion detection system using support vec.pdf:application/pdf},
}

@inproceedings{garberCompactRepresentationsDynamic2019,
	address = {Snowbird, Utah},
	series = {{DCC} '19},
	title = {Compact {Representations} of {Dynamic} {Video} {Background} {Using} {Motion} {Sprites}},
	copyright = {All rights reserved},
	doi = {10.1109/DCC.2019.00052},
	booktitle = {2019 {Data} {Compression} {Conference} ({DCC})},
	author = {Garber, Solomon and Prakash, Aaditya and Marcus, Ryan and DiLillo, Antonella and Storer, James},
	year = {2019},
	file = {Solomon Garber et al. - 2019 - Compact Representations of Dynamic Video Backgroun.pdf:/home/ryan/Zotero/storage/2IU5BGHC/Solomon Garber et al. - 2019 - Compact Representations of Dynamic Video Backgroun.pdf:application/pdf},
}

@article{frankleLotteryTicketHypothesis2018,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	urldate = {2019-05-31},
	journal = {arXiv:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1803.03635 PDF:/home/ryan/Zotero/storage/KNTP2RYE/Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/JGLW4ENX/1803.html:text/html},
}

@article{sivaStochasticModelsLoad2012,
	series = {{INFOCOM} '12},
	title = {Stochastic {Models} of {Load} {Balancing} and {Scheduling} in {Cloud} {Computing} {Clusters}},
	doi = {10.1109/INFCOM.2012.6195815},
	journal = {Proceedings - IEEE INFOCOM},
	author = {Siva, Theja and Maguluri, R and , Srikant and Ying, Lei},
	month = mar,
	year = {2012},
	file = {Siva et al. - 2012 - Stochastic Models of Load Balancing and Scheduling.pdf:/home/ryan/Zotero/storage/WL2Y5VMS/Siva et al. - 2012 - Stochastic Models of Load Balancing and Scheduling.pdf:application/pdf},
}

@inproceedings{semantic-queries,
	series = {{DEEM} '17},
	title = {Using {Word} {Embedding} to {Enable} {Semantic} {Queries} in {Relational} {Databases}},
	booktitle = {Proceedings of the 1st {Workshop} on {Data} {Management} for {End}-to-{End} {Machine} {Learning} ({DEEM})},
	author = {Bordawekar, Rajesh and Shmueli, Oded},
	year = {2017},
	pages = {5:1--5:4},
}

@techreport{giakoumakisTestingSQLServer2008,
	title = {Testing {SQL} {Server}'s {Query} {Optimizer}: {Challenges}, {Techniques} and {Experiences}},
	shorttitle = {Testing {SQL} {Server}'s {Query} {Optimizer}},
	abstract = {Query optimization is an inherently complex problem, and va lidating the correctness and effectiveness of a query optimizer can be a task of comparable complexity. The overall process of measuring query optimization quality becomes increasingly challenging as mode rn query optimizers provide more advanced optimization strategies and adaptive techniques. In this p aper we present a practitioner‚Äôs account of query optimization testing. We discuss some of the unique is s s in testing a query optimizer, and we provide a high-level overview of the testing techniques use d to validate the query optimizer of Microsoft‚Äôs SQL Server. We offer our experiences and discuss a few ongoin g challenges, which we hope can inspire additional research in the area of query optimization and DB MS testing.},
	author = {Giakoumakis, Leo and Galindo-Legaria, C√©sar A.},
	year = {2008},
	keywords = {Correctness (computer science), High- and low-level, Mathematical optimization, Microsoft SQL Server, Query optimization},
	pages = {36--43},
}

@misc{InnatenessAlphaZeroArtificial,
	title = {Innateness, {AlphaZero}, and {Artificial} {Intelligence}},
	url = {https://www.researchgate.net/publication/322567895_Innateness_AlphaZero_and_Artificial_Intelligence},
	abstract = {Download Citation on ResearchGate {\textbar} Innateness, AlphaZero, and Artificial Intelligence {\textbar} The concept of innateness is rarely discussed in the context of artificial intelligence. When it is discussed, or hinted at, it is often the context of trying to reduce the amount of innate machinery in a given system. In this paper, I consider as a test case a recent series...},
	language = {en},
	urldate = {2019-02-16},
	journal = {ResearchGate},
	file = {Snapshot:/home/ryan/Zotero/storage/V5NUZLHM/322567895_Innateness_AlphaZero_and_Artificial_Intelligence.html:text/html},
}

@article{lorenzobossiSystemProfilingMonitoring2014,
	series = {{ToSE}  '14},
	title = {A {System} for {Profiling} and {Monitoring} {Database} {Access} {Patterns} by {Application} {Programs} for {Anomaly} {Detection}},
	volume = {43},
	issn = {0098-5589},
	doi = {10.1109/TSE.2016.2598336},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {{Lorenzo Bossi} and {Elisa Bertino} and {Syed Rafiul Hussain}},
	month = may,
	year = {2014},
	pages = {415--431},
	file = {Bossi et al. - 2017 - A System for Profiling and Monitoring Database Acc.pdf:/home/ryan/Zotero/storage/G2RQ7PDY/Bossi et al. - 2017 - A System for Profiling and Monitoring Database Acc.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/WFQNF8I6/7534833.html:text/html},
}

@inproceedings{bottouLargeScaleMachineLearning2010,
	series = {{COMPSTAT} '10},
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	isbn = {978-3-7908-2603-6 978-3-7908-2604-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-7908-2604-3_16},
	doi = {10.1007/978-3-7908-2604-3_16},
	language = {en},
	urldate = {2017-11-08},
	booktitle = {Proceedings of {COMPSTAT}'2010},
	publisher = {Physica-Verlag HD},
	author = {Bottou, L√©on},
	year = {2010},
	pages = {177--186},
	file = {Snapshot:/home/ryan/Zotero/storage/LP7N3WA3/978-3-7908-2604-3_16.html:text/html},
}

@inproceedings{snyderDeepNeuralNetworkbased2016,
	series = {{SLT} '16},
	title = {Deep neural network-based speaker embeddings for end-to-end speaker verification},
	doi = {10.1109/SLT.2016.7846260},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Snyder, D. and Ghahremani, P. and Povey, D. and Garcia-Romero, D. and Carmiel, Y. and Khudanpur, S.},
	month = dec,
	year = {2016},
	keywords = {neural nets, Neural networks, Computational modeling, Computer architecture, deep neural network-based speaker embeddings, deep neural networks, different-speaker pair, EER, end-to-end text-independent speaker verification system, end-to-end training, equal error-rate, Linear programming, Mathematical model, same-speaker pair, speaker recognition, speaker verification, Speech, text-dependent verification, Training},
	pages = {165--170},
}

@inproceedings{binnigInteractiveCurationAutomatic2018,
	address = {New York, NY, USA},
	series = {{DEEM} '18},
	title = {Towards {Interactive} {Curation} \& {Automatic} {Tuning} of {ML} {Pipelines}},
	isbn = {978-1-4503-5828-6},
	url = {http://doi.acm.org/10.1145/3209889.3209891},
	doi = {10.1145/3209889.3209891},
	urldate = {2018-11-22},
	booktitle = {Proceedings of the {Second} {Workshop} on {Data} {Management} for {End}-{To}-{End} {Machine} {Learning}},
	publisher = {ACM},
	author = {Binnig, Carsten and Buratti, Benedetto and Chung, Yeounoh and Cousins, Cyrus and Kraska, Tim and Shang, Zeyuan and Upfal, Eli and Zeleznik, Robert and Zgraggen, Emanuel},
	year = {2018},
	pages = {1:1--1:4},
	file = {Binnig et al. - 2018 - Towards Interactive Curation & Automatic Tuning of.pdf:/home/ryan/Zotero/storage/TZNXN8SM/Binnig et al. - 2018 - Towards Interactive Curation & Automatic Tuning of.pdf:application/pdf},
}

@article{goldbergPrimerNeuralNetwork2016,
	series = {{JAIR} '16},
	title = {A {Primer} on {Neural} {Network} {Models} for {Natural} {Language} {Processing}},
	volume = {57},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/11030},
	doi = {10.1613/jair.4992},
	language = {en-US},
	urldate = {2018-09-25},
	journal = {Journal of Artificial Intelligence Research},
	author = {Goldberg, Yoav},
	month = nov,
	year = {2016},
	pages = {345--420},
	file = {Full Text PDF:/home/ryan/Zotero/storage/4C3JNAIZ/Goldberg - 2016 - A Primer on Neural Network Models for Natural Lang.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/AK4AGYFR/11030.html:text/html},
}

@inproceedings{leeOperatorQueryProgress2016,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Operator and {Query} {Progress} {Estimation} in {Microsoft} {SQL} {Server} {Live} {Query} {Statistics}},
	isbn = {978-1-4503-3531-7},
	url = {http://doi.acm.org/10.1145/2882903.2903728},
	doi = {10.1145/2882903.2903728},
	urldate = {2018-09-18},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Lee, Kukjin and K√∂nig, Arnd Christian and Narasayya, Vivek and Ding, Bolin and Chaudhuri, Surajit and Ellwein, Brent and Eksarevskiy, Alexey and Kohli, Manbeen and Wyant, Jacob and Prakash, Praneeta and Nehme, Rimma and Li, Jiexing and Naughton, Jeff},
	year = {2016},
	keywords = {database administration, databases query progress estimation},
	pages = {1753--1764},
	file = {Lee et al. - 2016 - Operator and Query Progress Estimation in Microsof.pdf:/home/ryan/Zotero/storage/FY32TPJH/Lee et al. - 2016 - Operator and Query Progress Estimation in Microsof.pdf:application/pdf},
}

@article{oordNeuralDiscreteRepresentation2017,
	series = {{NIPS} '17},
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2018-09-18},
	journal = {Neural Information Processing},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00937},
	keywords = {Computer Science - Machine Learning},
	file = {Oord et al. - 2017 - Neural Discrete Representation Learning.pdf:/home/ryan/Zotero/storage/4BL8WPY9/Oord et al. - 2017 - Neural Discrete Representation Learning.pdf:application/pdf},
}

@article{demulderSurveyApplicationRecurrent2015,
	series = {{CSL} '15},
	title = {A survey on the application of recurrent neural networks to statistical language modeling},
	volume = {30},
	issn = {0885-2308},
	url = {http://www.sciencedirect.com/science/article/pii/S088523081400093X},
	doi = {10.1016/j.csl.2014.09.005},
	abstract = {In this paper, we present a survey on the application of recurrent neural networks to the task of statistical language modeling. Although it has been shown that these models obtain good performance on this task, often superior to other state-of-the-art techniques, they suffer from some important drawbacks, including a very long training time and limitations on the number of context words that can be taken into account in practice. Recent extensions to recurrent neural network models have been developed in an attempt to address these drawbacks. This paper gives an overview of the most important extensions. Each technique is described and its performance on statistical language modeling, as described in the existing literature, is discussed. Our structured overview makes it possible to detect the most promising techniques in the field of recurrent neural networks, applied to language modeling, but it also highlights the techniques for which further research is required.},
	number = {1},
	urldate = {2018-09-18},
	journal = {Computer Speech \& Language},
	author = {De Mulder, Wim and Bethard, Steven and Moens, Marie-Francine},
	month = mar,
	year = {2015},
	keywords = {Language modeling, Machine translation, Natural language processing, Recurrent neural networks, Speech recognition},
	pages = {61--98},
	file = {De Mulder et al. - 2015 - A survey on the application of recurrent neural ne.pdf:/home/ryan/Zotero/storage/MGJNM7YF/De Mulder et al. - 2015 - A survey on the application of recurrent neural ne.pdf:application/pdf},
}

@inproceedings{liGSLPICostBasedQuery2012,
	series = {{ICDE} '12},
	title = {{GSLPI}: {A} {Cost}-{Based} {Query} {Progress} {Indicator}},
	shorttitle = {{GSLPI}},
	doi = {10.1109/ICDE.2012.74},
	abstract = {Progress indicators for SQL queries were first published in 2004 with the simultaneous and independent proposals from Chaudhuri et al. and Luo et al. In this paper, we implement both progress indicators in the same commercial RDBMS to investigate their performance. We summarize common cases in which they are both accurate and cases in which they fail to provide reliable estimates. Although there are differences in their performance, much more striking is the similarity in the errors they make due to a common simplifying uniform future speed assumption. While the developers of these progress indicators were aware that this assumption could cause errors, they neither explored how large the errors might be nor did they investigate the feasibility of removing the assumption. To rectify this we propose a new query progress indicator, similar to these early progress indicators but without the uniform speed assumption. Experiments show that on the TPC-H benchmark, on queries for which the original progress indicators have errors up to 30X the query running time, the new progress indicator is accurate to within 10 percent. We also discuss the sources of the errors that still remain and shed some light on what would need to be done to eliminate them.},
	booktitle = {2012 {IEEE} 28th {International} {Conference} on {Data} {Engineering}},
	author = {Li, J. and Nehme, R. V. and Naughton, J.},
	month = apr,
	year = {2012},
	keywords = {Pipelines, query processing, Benchmark testing, cost-based query progress indicator, costing, error source, Estimation error, GSLPI, Indexes, query running time, RDBMS, relational databases, Servers, SQL, SQL queries, TPC-H benchmark},
	pages = {678--689},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/D5XDN7D3/6228124.html:text/html;Li et al. - 2012 - GSLPI A Cost-Based Query Progress Indicator.pdf:/home/ryan/Zotero/storage/8BJ2YQEI/Li et al. - 2012 - GSLPI A Cost-Based Query Progress Indicator.pdf:application/pdf},
}

@inproceedings{xiePIGEONProgressIndicator2015,
	series = {{ICDE} '15},
	title = {{PIGEON}: {Progress} indicator for subgraph queries},
	shorttitle = {{PIGEON}},
	doi = {10.1109/ICDE.2015.7113409},
	abstract = {Subgraph queries have been a fundamental query for retrieving patterns from graph data. Due to the well known NP hardness of subgraph queries, those queries may sometimes take a long time to complete. Our recent investigation on real- world datasets revealed that the performance of queries on graphs generally varies greatly. In other words, query clients may occasionally encounter ‚Äúunexpectedly‚Äù long execution from a subgraph query processor. This paper aims to demonstrate a tool that alleviates the problem by monitoring subgraph query progress. Specifically, we present a novel subgraph query progress indicator called PIGEON that exploits query-time information to report to users accurate estimated query progress. In the demonstration, users may interact with PIGEON to gain insights on the query evaluation, which include the following: Users are enabled to (i) monitor query progress; (ii) analyze the causes of long query times; and (iii) abort queries that run abnormally long, which may sometimes contain human errors.},
	booktitle = {2015 {IEEE} 31st {International} {Conference} on {Data} {Engineering}},
	author = {Xie, X. and Fan, Z. and Choi, B. and Yi, P. and Bhowmick, S. S. and Zhou, S.},
	month = apr,
	year = {2015},
	keywords = {Databases, Optimization, query processing, Runtime, Cascading style sheets, computational complexity, Estimation, graph data, graph theory, long query time cause analysis, NP hardness, pattern retrieval, PIGEON, query abortion, query evaluation, query-time information, subgraph query progress indicator, subgraph query progress monitoring, user interfaces, Visualization, YouTube},
	pages = {1492--1495},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/KHWWMALM/7113409.html:text/html;Xie et al. - 2015 - PIGEON Progress indicator for subgraph queries.pdf:/home/ryan/Zotero/storage/754U3L2N/Xie et al. - 2015 - PIGEON Progress indicator for subgraph queries.pdf:application/pdf},
}

@inproceedings{luoProgressIndicatorDatabase2004,
	address = {New York, NY, USA},
	series = {{SIGMOD} '04},
	title = {Toward a {Progress} {Indicator} for {Database} {Queries}},
	isbn = {978-1-58113-859-7},
	url = {http://doi.acm.org/10.1145/1007568.1007658},
	doi = {10.1145/1007568.1007658},
	abstract = {Many modern software systems provide progress indicators for long-running tasks. These progress indicators make systems more user-friendly by helping the user quickly estimate how much of the task has been completed and when the task will finish. However, none of the existing commercial RDBMSs provides a non-trival progress indicator for long-running queries. In this paper, we consider the problem of supporting such progress indicators. After discussing the goals and challenges inherent in this problem, we present a set of techniques sufficient for implementing a simple yet useful progress indicator for a large subset of RDBMS queries. We report an initial implementation of these techniques in PostgreSQL.},
	urldate = {2018-09-18},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Luo, Gang and Naughton, Jeffrey F. and Ellmann, Curt J. and Watzke, Michael W.},
	year = {2004},
	pages = {791--802},
	file = {Luo et al. - 2004 - Toward a Progress Indicator for Database Queries.pdf:/home/ryan/Zotero/storage/RXYSJ39Y/Luo et al. - 2004 - Toward a Progress Indicator for Database Queries.pdf:application/pdf},
}

@inproceedings{mortonParaTimerProgressIndicator2010,
	address = {New York, NY, USA},
	series = {{SIGMOD} '10},
	title = {{ParaTimer}: {A} {Progress} {Indicator} for {MapReduce} {DAGs}},
	isbn = {978-1-4503-0032-2},
	shorttitle = {{ParaTimer}},
	url = {http://doi.acm.org/10.1145/1807167.1807223},
	doi = {10.1145/1807167.1807223},
	abstract = {Time-oriented progress estimation for parallel queries is a challenging problem that has received only limited attention. In this paper, we present ParaTimer, a new type of time-remaining indicator for parallel queries. Several parallel data processing systems exist. ParaTimer targets environments where declarative queries are translated into ensembles of MapReduce jobs. ParaTimer builds on previous techniques and makes two key contributions. First, it estimates the progress of queries that translate into directed acyclic graphs of MapReduce jobs, where jobs on different paths can execute concurrently (unlike prior work that looked at sequences only). For such queries, we use a new type of critical-path-based progress-estimation approach. Second, ParaTimer handles a variety of real systems challenges such as failures and data skew. To handle unexpected changes in query execution times due to runtime condition changes, ParaTimer provides users with not only one but with a set of time-remaining estimates, each one corresponding to a different carefully selected scenario. We implement our estimator in the Pig system and demonstrate its performance on experiments running on a real, small-scale cluster.},
	urldate = {2018-09-18},
	booktitle = {Proceedings of the 2010 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Morton, Kristi and Balazinska, Magdalena and Grossman, Dan},
	year = {2010},
	keywords = {mapreduce, parallel database, progress estimation},
	pages = {507--518},
	file = {Morton et al. - 2010 - ParaTimer A Progress Indicator for MapReduce DAGs.pdf:/home/ryan/Zotero/storage/EHFX7ZJN/Morton et al. - 2010 - ParaTimer A Progress Indicator for MapReduce DAGs.pdf:application/pdf},
}

@inproceedings{chenAdaptiveSelectivityEstimation1994,
	address = {New York, NY, USA},
	series = {{SIGMOD} '94},
	title = {Adaptive {Selectivity} {Estimation} {Using} {Query} {Feedback}},
	isbn = {978-0-89791-639-4},
	shorttitle = {Adaptive {Selectivity} {Estimation}},
	url = {http://doi.acm.org/10.1145/191839.191874},
	doi = {10.1145/191839.191874},
	abstract = {In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantage of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering.},
	urldate = {2018-04-20},
	booktitle = {Proceedings of the 1994 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Chen, Chungmin Melvin and Roussopoulos, Nick},
	year = {1994},
	pages = {161--172},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/7X3B2M5G/Chen and Roussopoulos - 1994 - Adaptive Selectivity Estimation Using Query Feedba.pdf:application/pdf},
}

@inproceedings{aboulnagaSelftuningHistogramsBuilding1999,
	address = {New York, NY, USA},
	series = {{SIGMOD} '99},
	title = {Self-tuning {Histograms}: {Building} {Histograms} {Without} {Looking} at {Data}},
	isbn = {978-1-58113-084-3},
	shorttitle = {Self-tuning {Histograms}},
	url = {http://doi.acm.org/10.1145/304182.304198},
	doi = {10.1145/304182.304198},
	abstract = {In this paper, we introduce self-tuning histograms. Although similar in structure to traditional histograms, these histograms infer data distributions not by examining the data or a sample thereof, but by using feedback from the query execution engine about the actual selectivity of range selection operators to progressively refine the histogram. Since the cost of building and maintaining self-tuning histograms is independent of the data size, self-tuning histograms provide a remarkably inexpensive way to construct histograms for large data sets with little up-front costs. Self-tuning histograms are particularly attractive as an alternative to multi-dimensional traditional histograms that capture dependencies between attributes but are prohibitively expensive to build and maintain. In this paper, we describe the techniques for initializing and refining self-tuning histograms. Our experimental results show that self-tuning histograms provide a low-cost alternative to traditional multi-dimensional histograms with little loss of accuracy for data distributions with low to moderate skew.},
	urldate = {2018-04-20},
	booktitle = {Proceedings of the 1999 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Aboulnaga, Ashraf and Chaudhuri, Surajit},
	year = {1999},
	pages = {181--192},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/7M543Y65/Aboulnaga and Chaudhuri - 1999 - Self-tuning Histograms Building Histograms Withou.pdf:application/pdf},
}

@inproceedings{kumarDataManagementMachine2017,
	address = {New York, NY, USA},
	series = {{SIGMOD} '17},
	title = {Data {Management} in {Machine} {Learning}: {Challenges}, {Techniques}, and {Systems}},
	isbn = {978-1-4503-4197-4},
	shorttitle = {Data {Management} in {Machine} {Learning}},
	url = {http://doi.acm.org/10.1145/3035918.3054775},
	doi = {10.1145/3035918.3054775},
	abstract = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
	urldate = {2018-04-05},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
	year = {2017},
	keywords = {machine learning, data management},
	pages = {1717--1722},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/AKA9SD3D/Kumar et al. - 2017 - Data Management in Machine Learning Challenges, T.pdf:application/pdf},
}

@misc{marcusSharedMemoryManyCore2015,
	address = {Los Alamos National Lab},
	title = {Shared {Memory} for {Many}-{Core} {Hydrodynamics} {Codes}},
	copyright = {All rights reserved},
	author = {Marcus, Ryan},
	collaborator = {Nystrom, William and Gunter, David and Wright, Cornell},
	year = {2015},
	note = {LA-UR-15-25303},
	file = {Marcus - 2015 - Shared Memory for Many-Core Hydrodynamics Codes.pdf:/home/ryan/Zotero/storage/FIKC56Y9/Marcus - 2015 - Shared Memory for Many-Core Hydrodynamics Codes.pdf:application/pdf},
}

@techreport{marcusTechniquesAutomatedPerformance2014,
	title = {Techniques for {Automated} {Performance} {Analysis}},
	copyright = {All rights reserved},
	url = {https://www.osti.gov/biblio/1154980-techniques-automated-performance-analysis},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {LA-UR-14-26577},
	urldate = {2018-03-25},
	institution = {Los Alamos National Lab},
	author = {Marcus, Ryan},
	year = {2014},
	doi = {10.2172/1154980},
	file = {Marcus - 2014 - Techniques for Automated Performance Analysis.pdf:/home/ryan/Zotero/storage/YNAYPHP9/Marcus - 2014 - Techniques for Automated Performance Analysis.pdf:application/pdf},
}

@techreport{marcusMCMiniMonteCarlo2012,
	title = {{MCMini}: {Monte} {Carlo} on {GPGPU}},
	copyright = {All rights reserved},
	shorttitle = {{MCMini}},
	url = {https://www.osti.gov/biblio/1047072-mcmini-monte-carlo-gpgpu},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {LA-UR-12-23206},
	urldate = {2018-03-25},
	institution = {Los Alamos National Lab},
	author = {Marcus, Ryan},
	year = {2012},
	doi = {10.2172/1047072},
	file = {Marcus - 2012 - MCMini Monte Carlo on GPGPU.pdf:/home/ryan/Zotero/storage/2I9NKJNW/Marcus - 2012 - MCMini Monte Carlo on GPGPU.pdf:application/pdf},
}

@techreport{marcusDPFastMedian2013,
	title = {{DP}: a {Fast} {Median} {Filter} {Approximation}},
	copyright = {All rights reserved},
	shorttitle = {{DP}},
	url = {https://www.osti.gov/biblio/1088342-dp-fast-median-filter-approximation},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {LA-UR-13-25331},
	urldate = {2018-03-25},
	institution = {Los Alamos National Lab},
	author = {Marcus, Ryan and Ward, William C.},
	year = {2013},
	doi = {10.2172/1088342},
	file = {Marcus and Ward - 2013 - DP a Fast Median Filter Approximation.pdf:/home/ryan/Zotero/storage/BUVD77KG/Marcus and Ward - 2013 - DP a Fast Median Filter Approximation.pdf:application/pdf},
}

@techreport{coxDevelopingMonteCarlo2011,
	title = {Developing a {Monte} {Carlo} mini-app for exascale co-design},
	copyright = {All rights reserved},
	url = {https://www.osti.gov/biblio/1074563},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {LA-UR-11-06085; LA-UR-11-6085},
	urldate = {2018-03-25},
	institution = {Los Alamos National Lab},
	author = {Cox, Lawrence J. and Marcus, Ryan},
	year = {2011},
	file = {Full Text PDF:/home/ryan/Zotero/storage/DI5E6LAV/Cox and Marcus - 2011 - Developing a Monte Carlo mini-app for exascale co-.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/4FG9LGCV/1074563.html:text/html},
}

@techreport{marcusEfficientAlgorithmMonte2014,
	title = {An {Efficient} {Algorithm} and {Monte} {Carlo} {Methods} for {Inferring} {Functional} {Dependencies}},
	copyright = {All rights reserved},
	institution = {University of Arizona},
	author = {Marcus, Ryan and Lavine, Shaughan},
	year = {2014},
	file = {Marcus and Lavine - 2014 - An Efficient Algorithm and Monte Carlo Methods for.pdf:/home/ryan/Zotero/storage/4Q6VIVDR/Marcus and Lavine - 2014 - An Efficient Algorithm and Monte Carlo Methods for.pdf:application/pdf},
}

@inproceedings{gollerLearningTaskdependentDistributed1996,
	title = {Learning task-dependent distributed representations by backpropagation through structure},
	volume = {1},
	isbn = {978-0-7803-3210-2},
	url = {http://ieeexplore.ieee.org/document/548916/},
	doi = {10.1109/ICNN.1996.548916},
	urldate = {2017-11-07},
	publisher = {IEEE},
	author = {Goller, C. and Kuchler, A.},
	year = {1996},
	pages = {347--352},
	file = {6ed81d21f1bf32a0fd3be05c44c1fa362688.pdf:/home/ryan/Zotero/storage/QLUWHVM5/6ed81d21f1bf32a0fd3be05c44c1fa362688.pdf:application/pdf},
}

@article{xiaoAutomaticScalingInternet2014,
	title = {Automatic {Scaling} of {Internet} {Applications} for {Cloud} {Computing} {Services}},
	volume = {63},
	issn = {0018-9340},
	doi = {10.1109/TC.2012.284},
	abstract = {Many Internet applications can benefit from an automatic scaling property where their resource usage can be scaled up and down automatically by the cloud service provider. We present a system that provides automatic scaling for Internet applications in the cloud environment. We encapsulate each application instance inside a virtual machine (VM) and use virtualization technology to provide fault isolation. We model it as the Class Constrained Bin Packing (CCBP) problem where each server is a bin and each class represents an application. The class constraint reflects the practical limit on the number of applications a server can run simultaneously. We develop an efficient semi-online color set algorithm that achieves good demand satisfaction ratio and saves energy by reducing the number of servers used when the load is low. Experiment results demonstrate that our system can improve the throughput by 180\% over an open source implementation of Amazon EC2 and restore the normal QoS five times as fast during flash crowds. Large scale simulations demonstrate that our algorithm is extremely scalable: the decision time remains under 4 s for a system with 10 000 servers and 10 000 applications. This is an order of magnitude improvement over traditional application placement algorithms in enterprise environments.},
	number = {5},
	journal = {IEEE Transactions on Computers},
	author = {Xiao, Z. and Chen, Q. and Luo, H.},
	month = may,
	year = {2014},
	keywords = {cloud computing, Switches, virtualization, Computer architecture, Servers, Amazon EC2, auto scaling, automatic scaling property, CCBP, CCBP problem, Central Processing Unit, class constrained bin packing, cloud computing services, cloud environment, cloud service provider, Color, decision time, demand satisfaction ratio, energy saving, enterprise environments, fault diagnosis, fault isolation, flash crowds, green computing, Internet applications, open source implementation, public domain software, QoS, quality of service, resource usage, semionline color set algorithm, set theory, virtual machine, virtual machines, virtualisation, virtualization technology, VM},
	pages = {1111--1123},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/AG3QQL8I/6365627.html:text/html;Xiao et al. - 2014 - Automatic Scaling of Internet Applications for Clo.pdf:/home/ryan/Zotero/storage/UL8MF8ND/Xiao et al. - 2014 - Automatic Scaling of Internet Applications for Clo.pdf:application/pdf},
}

@article{alexaOptimalDiscreteSlicing2017,
	title = {Optimal {Discrete} {Slicing}},
	volume = {36},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/2999536},
	doi = {10.1145/2999536},
	abstract = {Slicing is the procedure necessary to prepare a shape for layered manufacturing. There are degrees of freedom in this process, such as the starting point of the slicing sequence and the thickness of each slice. The choice of these parameters influences the manufacturing process and its result: The number of slices significantly affects the time needed for manufacturing, while their thickness affects the error. Assuming a discrete setting, we measure the error as the number of voxels that are incorrectly assigned due to slicing. We provide an algorithm that generates, for a given set of available slice heights and a shape, a slicing that is provably optimal. By optimal, we mean that the algorithm generates sequences with minimal error for any possible number of slices. The algorithm is fast and flexible, that is, it can accommodate a user driven importance modulation of the error function and allows the interactive exploration of the desired quality/time tradeoff. We demonstrate the practical importance of our optimization on several three-dimensional-printed results.},
	number = {1},
	urldate = {2017-09-10},
	journal = {ACM Trans. Graph.},
	author = {Alexa, Marc and Hildebrand, Kristian and Lefebvre, Sylvain},
	month = jan,
	year = {2017},
	keywords = {additive manufacturing, Computer numerical control, direct digital manufacturing, dynamic programming, slicing},
	pages = {12:1--12:16},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/6GVIKZUU/Alexa et al. - 2017 - Optimal Discrete Slicing.pdf:application/pdf},
}

@article{segevLearnSourceRefine2017,
	title = {Learn on {Source}, {Refine} on {Target}: {A} {Model} {Transfer} {Learning} {Framework} with {Random} {Forests}},
	volume = {39},
	issn = {0162-8828},
	shorttitle = {Learn on {Source}, {Refine} on {Target}},
	doi = {10.1109/TPAMI.2016.2618118},
	abstract = {We propose novel model transfer-learning methods that refine a decision forest model \$M\$ learned within a ‚Äúsource‚Äù domain using a training set sampled from a ‚Äútarget‚Äù domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Segev, N. and Harel, M. and Mannor, S. and Crammer, K. and El-Yaniv, R.},
	month = sep,
	year = {2017},
	keywords = {Data models, Computational modeling, Training, Adaptation models, Companies, decision tree, Decision trees, model transfer, random forest, Transfer learning, Vegetation},
	pages = {1811--1824},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/CHXPRBGY/7592407.html:text/html;Segev et al. - 2017 - Learn on Source, Refine on Target A Model Transfe.pdf:/home/ryan/Zotero/storage/XDY9EYXL/Segev et al. - 2017 - Learn on Source, Refine on Target A Model Transfe.pdf:application/pdf},
}

@inproceedings{nehmeAutomatedPartitioningDesign2011,
	series = {{SIGMOD} '11},
	title = {Automated partitioning design in parallel database systems},
	isbn = {978-1-4503-0661-4},
	url = {http://portal.acm.org/citation.cfm?doid=1989323.1989444},
	doi = {10.1145/1989323.1989444},
	language = {en},
	urldate = {2017-03-21},
	publisher = {ACM Press},
	author = {Nehme, Rimma and Bruno, Nicolas},
	year = {2011},
	pages = {1137},
	file = {Nehme and Bruno - 2011 - Automated partitioning design in parallel database.pdf:/home/ryan/Zotero/storage/S2ZMREG4/Nehme and Bruno - 2011 - Automated partitioning design in parallel database.pdf:application/pdf},
}

@inproceedings{tatarowiczLookupTablesFineGrained2012,
	series = {{ICDE} '12},
	title = {Lookup {Tables}: {Fine}-{Grained} {Partitioning} for {Distributed} {Databases}},
	shorttitle = {Lookup {Tables}},
	doi = {10.1109/ICDE.2012.26},
	booktitle = {2012 {IEEE} 28th {International} {Conference} on {Data} {Engineering}},
	author = {Tatarowicz, A. L. and Curino, C. and Jones, E. P. C. and Madden, S.},
	month = apr,
	year = {2012},
	keywords = {Indexes, data distribution layer, data mining, distributed databases, distributed OLTP DBMS, distributed queries, distributed transactions, fine-grained partitioning, lookup tables, meta data, metadata, order-fulfillment, Routing protocols, Social network services, social networking, table lookup, Throughput},
	pages = {102--113},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/DKDCMRJ8/6228076.html:text/html;Tatarowicz et al. - 2012 - Lookup Tables Fine-Grained Partitioning for Distr.pdf:/home/ryan/Zotero/storage/AFF3PWJA/Tatarowicz et al. - 2012 - Lookup Tables Fine-Grained Partitioning for Distr.pdf:application/pdf},
}

@article{jonesComparativeAdvantageTheory1961,
	title = {Comparative {Advantage} and the {Theory} of {Tariffs}: {A} {Multi}-{Country}, {Multi}- {Commodity} {Model}},
	volume = {28},
	issn = {00346527},
	shorttitle = {Comparative {Advantage} and the {Theory} of {Tariffs}},
	url = {https://academic.oup.com/restud/article-lookup/doi/10.2307/2295945},
	doi = {10.2307/2295945},
	number = {3},
	urldate = {2017-04-02},
	journal = {The Review of Economic Studies},
	author = {Jones, Ronald W.},
	month = jun,
	year = {1961},
	pages = {161},
	file = {Jones - 1961 - Comparative Advantage and the Theory of Tariffs A.pdf:/home/ryan/Zotero/storage/PZ48T9B7/Jones - 1961 - Comparative Advantage and the Theory of Tariffs A.pdf:application/pdf},
}

@inproceedings{pavloSkewawareAutomaticDatabase2012,
	title = {Skew-aware automatic database partitioning in shared-nothing, parallel {OLTP} systems},
	isbn = {978-1-4503-1247-9},
	url = {http://dl.acm.org/citation.cfm?doid=2213836.2213844},
	doi = {10.1145/2213836.2213844},
	language = {en},
	urldate = {2017-03-21},
	publisher = {ACM Press},
	author = {Pavlo, Andrew and Curino, Carlo and Zdonik, Stanley},
	year = {2012},
	pages = {61},
}

@inproceedings{nehmeAutomatedPartitioningDesign2011a,
	address = {New York, NY, USA},
	series = {{SIGMOD} '11},
	title = {Automated {Partitioning} {Design} in {Parallel} {Database} {Systems}},
	isbn = {978-1-4503-0661-4},
	url = {http://doi.acm.org/10.1145/1989323.1989444},
	doi = {10.1145/1989323.1989444},
	abstract = {In recent years, Massively Parallel Processors (MPPs) have gained ground enabling vast amounts of data processing. In such environments, data is partitioned across multiple compute nodes, which results in dramatic performance improvements during parallel query execution. To evaluate certain relational operators in a query correctly, data sometimes needs to be re-partitioned (i.e., moved) across compute nodes. Since data movement operations are much more expensive than relational operations, it is crucial to design a suitable data partitioning strategy that minimizes the cost of such expensive data transfers. A good partitioning strategy strongly depends on how the parallel system would be used. In this paper we present a partitioning advisor that recommends the best partitioning design for an expected workload. Our tool recommends which tables should be replicated (i.e., copied into every compute node) and which ones should be distributed according to specific column(s) so that the cost of evaluating similar workloads is minimized. In contrast to previous work, our techniques are deeply integrated with the underlying parallel query optimizer, which results in more accurate recommendations in a shorter amount of time. Our experimental evaluation using a real MPP system, Microsoft SQL Server 2008 Parallel Data Warehouse, with both real and synthetic workloads shows the effectiveness of the proposed techniques and the importance of deep integration of the partitioning advisor with the underlying query optimizer.},
	urldate = {2017-03-21},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Nehme, Rimma and Bruno, Nicolas},
	year = {2011},
	keywords = {databases, distributed},
	pages = {1137--1148},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/33DP6ISF/Nehme and Bruno - 2011 - Automated Partitioning Design in Parallel Database.pdf:application/pdf},
}

@book{brualdiCombinatorialMatrixTheory1991,
	title = {Combinatorial {Matrix} {Theory}},
	isbn = {978-0-521-32265-2},
	abstract = {This book, first published in 1991, is devoted to the exposition of combinatorial matrix theory. This subject concerns itself with the use of matrix theory and linear algebra in proving results in combinatorics (and vice versa), and with the intrinsic properties of matrices viewed as arrays of numbers rather than algebraic objects in themselves.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Brualdi, Richard A. and Ryser, Herbert J.},
	month = jul,
	year = {1991},
	note = {Google-Books-ID: vVTdbb6930EC},
	keywords = {Mathematics / Algebra / General, Mathematics / Discrete Mathematics},
}

@article{birkhoffThreeObservationsLinear1946,
	title = {Three observations on linear algebra},
	volume = {5},
	journal = {Univ. Nac. Tucum√°n. Revista A},
	author = {Birkhoff, Garrett},
	year = {1946},
	pages = {147--151},
}

@article{bittencourtHCOCCostOptimization2011,
	series = {{JISA} '11},
	title = {{HCOC}: a cost optimization algorithm for workflow scheduling in hybrid clouds},
	volume = {2},
	issn = {1867-4828, 1869-0238},
	shorttitle = {{HCOC}},
	url = {http://link.springer.com/article/10.1007/s13174-011-0032-0},
	doi = {10.1007/s13174-011-0032-0},
	abstract = {Workflows have been used to represent a variety of applications involving high processing and storage demands. As a solution to supply this necessity, the cloud computing paradigm has emerged as an on-demand resources provider. While public clouds charge users in a per-use basis, private clouds are owned by users and can be utilized with no charge. When a public cloud and a private cloud are merged, we have what we call a hybrid cloud. In a hybrid cloud, the user has elasticity provided by public cloud resources that can be aggregated to the private resources pool as necessary. One question faced by the users in such systems is: Which are the best resources to request from a public cloud based on the current demand and on resources costs? In this paper we deal with this problem, presenting HCOC: The Hybrid Cloud Optimized Cost scheduling algorithm. HCOC decides which resources should be leased from the public cloud and aggregated to the private cloud to provide sufficient processing power to execute a workflow within a given execution time. We present extensive experimental and simulation results which show that HCOC can reduce costs while achieving the established desired execution time.},
	language = {en},
	number = {3},
	urldate = {2017-01-08},
	journal = {Journal of Internet Services and Applications},
	author = {Bittencourt, Luiz Fernando and Madeira, Edmundo Roberto Mauro},
	month = dec,
	year = {2011},
	pages = {207--227},
	file = {Bittencourt and Madeira - 2011 - HCOC a cost optimization algorithm for workflow s.pdf:/home/ryan/Zotero/storage/2JAFEKK3/Bittencourt and Madeira - 2011 - HCOC a cost optimization algorithm for workflow s.pdf:application/pdf},
}

@inproceedings{poolaRobustSchedulingScientific2014,
	title = {Robust {Scheduling} of {Scientific} {Workflows} with {Deadline} and {Budget} {Constraints} in {Clouds}},
	doi = {10.1109/AINA.2014.105},
	abstract = {Dynamic resource provisioning and the notion of seemingly unlimited resources are attracting scientific workflows rapidly into Cloud computing. Existing works on workflow scheduling in the context of Clouds are either on deadline or cost optimization, ignoring the necessity for robustness. Robust scheduling that handles performance variations of Cloud resources and failures in the environment is essential in the context of Clouds. In this paper, we present a robust scheduling algorithm with resource allocation policies that schedule workflow tasks on heterogeneous Cloud resources while trying to minimize the total elapsed time (make span) and the cost. Our results show that the proposed resource allocation policies provide robust and fault-tolerant schedule while minimizing make span. The results also show that with the increase in budget, our policies increase the robustness of the schedule.},
	booktitle = {2014 {IEEE} 28th {International} {Conference} on {Advanced} {Information} {Networking} and {Applications}},
	publisher = {AINA '14},
	author = {Poola, D. and Garg, S. K. and Buyya, R. and Yang, Y. and Ramamohanarao, K.},
	month = may,
	year = {2014},
	pages = {858--865},
	file = {Poola et al. - 2014 - Robust Scheduling of Scientific Workflows with Dea.pdf:/home/ryan/Zotero/storage/7DFEREKJ/Poola et al. - 2014 - Robust Scheduling of Scientific Workflows with Dea.pdf:application/pdf},
}

@inproceedings{malawskiCostDeadlineconstrainedProvisioning2012,
	address = {Los Alamitos, CA, USA},
	series = {{SC} '12},
	title = {Cost- and {Deadline}-constrained {Provisioning} for {Scientific} {Workflow} {Ensembles} in {IaaS} {Clouds}},
	isbn = {978-1-4673-0804-5},
	url = {http://dl.acm.org/citation.cfm?id=2388996.2389026},
	abstract = {Large-scale applications expressed as scientific workflows are often grouped into ensembles of inter-related workflows. In this paper, we address a new and important problem concerning the efficient management of such ensembles under budget and deadline constraints on Infrastructure- as-a-Service (IaaS) clouds. We discuss, develop, and assess algorithms based on static and dynamic strategies for both task scheduling and resource provisioning. We perform the evaluation via simulation using a set of scientific workflow ensembles with a broad range of budget and deadline parameters, taking into account uncertainties in task runtime estimations, provisioning delays, and failures. We find that the key factor determining the performance of an algorithm is its ability to decide which workflows in an ensemble to admit or reject for execution. Our results show that an admission procedure based on workflow structure and estimates of task runtimes can significantly improve the quality of solutions.},
	urldate = {2017-01-08},
	booktitle = {Proceedings of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE Computer Society Press},
	author = {Malawski, Maciej and Juve, Gideon and Deelman, Ewa and Nabrzyski, Jarek},
	year = {2012},
	pages = {22:1--22:11},
	file = {Malawski et al. - 2012 - Cost- and Deadline-constrained Provisioning for Sc.pdf:/home/ryan/Zotero/storage/5XHW83UK/Malawski et al. - 2012 - Cost- and Deadline-constrained Provisioning for Sc.pdf:application/pdf},
}

@article{kwokDynamicCriticalpathScheduling1996,
	title = {Dynamic critical-path scheduling: an effective technique for allocating task graphs to multiprocessors},
	volume = {7},
	issn = {10459219},
	shorttitle = {Dynamic critical-path scheduling},
	url = {http://ieeexplore.ieee.org/document/503776/},
	doi = {10.1109/71.503776},
	number = {5},
	urldate = {2016-12-31},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Kwok, Y and Ahmad, I.},
	month = may,
	year = {1996},
	pages = {506--521},
	file = {Kwok and Ahmad - 1996 - Dynamic critical-path scheduling an effective tec.pdf:/home/ryan/Zotero/storage/4J7RFZ7Z/Kwok and Ahmad - 1996 - Dynamic critical-path scheduling an effective tec.pdf:application/pdf},
}

@inproceedings{gonzalezMultiphaseProactiveCloud2016,
	title = {Multi-phase proactive cloud scheduling framework based on high level workflow and resource characterization},
	isbn = {978-1-5090-3216-7},
	url = {http://ieeexplore.ieee.org/document/7778591/},
	doi = {10.1109/NCA.2016.7778591},
	urldate = {2016-12-29},
	publisher = {IEEE},
	author = {Gonzalez, Nelson Mimura and de Brito Carvalho, Tereza Cristina Melo and Miers, Charles Christian},
	month = oct,
	year = {2016},
	pages = {43--47},
}

@article{leeResourceefficientWorkflowScheduling2015,
	title = {Resource-efficient workflow scheduling in clouds},
	volume = {80},
	issn = {09507051},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705115000556},
	doi = {10.1016/j.knosys.2015.02.012},
	language = {en},
	urldate = {2016-12-28},
	journal = {Knowledge-Based Systems},
	author = {Lee, Young Choon and Han, Hyuck and Zomaya, Albert Y. and Yousif, Mazin},
	month = may,
	year = {2015},
	pages = {153--162},
	file = {Lee et al. - 2015 - Resource-efficient workflow scheduling in clouds.pdf:/home/ryan/Zotero/storage/8W3I5FJB/Lee et al. - 2015 - Resource-efficient workflow scheduling in clouds.pdf:application/pdf},
}

@article{liSchedulingPrecedenceConstrained2015,
	title = {Scheduling {Precedence} {Constrained} {Stochastic} {Tasks} on {Heterogeneous} {Cluster} {Systems}},
	volume = {64},
	issn = {0018-9340},
	url = {http://ieeexplore.ieee.org/document/6636894/},
	doi = {10.1109/TC.2013.205},
	number = {1},
	urldate = {2016-12-28},
	journal = {IEEE Transactions on Computers},
	author = {Li, Kenli and Tang, Xiaoyong and Veeravalli, Bharadwaj and Li, Keqin},
	month = jan,
	year = {2015},
	pages = {191--204},
	file = {Li et al. - 2015 - Scheduling Precedence Constrained Stochastic Tasks.pdf:/home/ryan/Zotero/storage/6AICZ94X/Li et al. - 2015 - Scheduling Precedence Constrained Stochastic Tasks.pdf:application/pdf},
}

@article{topcuogluPerformanceeffectiveLowcomplexityTask2002,
	title = {Performance-effective and low-complexity task scheduling for heterogeneous computing},
	volume = {13},
	issn = {1045-9219},
	doi = {10.1109/71.993206},
	abstract = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics},
	number = {3},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
	month = mar,
	year = {2002},
	pages = {260--274},
	file = {Topcuoglu et al. - 2002 - Performance-effective and low-complexity task sche.pdf:/home/ryan/Zotero/storage/C4PV4FGI/Topcuoglu et al. - 2002 - Performance-effective and low-complexity task sche.pdf:application/pdf},
}

@inproceedings{wautersFastPermutationLearning2012,
	address = {New York, NY, USA},
	series = {{LION} 6},
	title = {Fast {Permutation} {Learning}},
	isbn = {978-3-642-34412-1},
	url = {http://dx.doi.org/10.1007/978-3-642-34413-8_21},
	doi = {10.1007/978-3-642-34413-8_21},
	abstract = {Permutations occur in a great variety of optimization problems, such as routing, scheduling and assignment problems. The present paper introduces the use of learning automata for the online learning of good quality permutations. Several centralized and decentralized methods using individual and common rewards are presented. The performance, memory requirement and scalability of the presented methods is analyzed. Results on well known benchmark problems show interesting properties. It is also demonstrated how these techniques are successfully applied to multi-project scheduling problems.},
	urldate = {2016-12-17},
	booktitle = {Revised {Selected} {Papers} of the 6th {International} {Conference} on {Learning} and {Intelligent} {Optimization} - {Volume} 7219},
	publisher = {Springer-Verlag New York, Inc.},
	author = {Wauters, Tony and Verbeeck, Katja and Causmaecker, Patrick and Berghe, Greet},
	year = {2012},
	pages = {292--306},
	file = {Wauters et al. - 2012 - Fast Permutation Learning.pdf:/home/ryan/Zotero/storage/HCI7DKGC/Wauters et al. - 2012 - Fast Permutation Learning.pdf:application/pdf},
}

@article{hypergraph_tasks,
	series = {{JPDS} '15},
	title = {Hypergraph partitioning for multiple communication cost metrics: {Model} and methods},
	volume = {77},
	issn = {0743-7315},
	shorttitle = {Hypergraph partitioning for multiple communication cost metrics},
	url = {http://www.sciencedirect.com/science/article/pii/S0743731514002275},
	doi = {10.1016/j.jpdc.2014.12.002},
	abstract = {We investigate hypergraph partitioning-based methods for efficient parallelization of communicating tasks. A good partitioning method should divide the load among the processors as evenly as possible and minimize the inter-processor communication overhead. The total communication volume is the most popular communication overhead metric which is reduced by the existing state-of-the-art hypergraph partitioners. However, other metrics such as the total number of messages, the maximum amount of data transferred by a processor, or a combination of them are equally, if not more, important. Existing hypergraph-based solutions use a two phase approach to minimize such metrics where in each phase, they minimize a different metric, sometimes at the expense of others. We propose a one-phase approach where all the communication cost metrics can be effectively minimized in a multi-objective setting and reductions can be achieved for all metrics together. For an accurate modeling of the maximum volume and the number of messages sent and received by a processor, we propose the use of directed hypergraphs. The directions on hyperedges necessitate revisiting the standard partitioning heuristics. We do so and propose a multi-objective, multi-level hypergraph partitioner called UMPa. The partitioner takes various prioritized communication metrics into account, and optimizes all of them together in the same phase. Compared to the state-of-the-art methods which only minimize the total communication volume, we show on a large number of problem instances that UMPa produces better partitions in terms of several communication metrics.},
	urldate = {2016-12-16},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Deveci, Mehmet and Kaya, Kamer and U√ßar, Bora and √áataly√ºrek, √úmit V.},
	month = mar,
	year = {2015},
	pages = {69--83},
	file = {deveci_jpdc.pdf:/home/ryan/Zotero/storage/ZBUJN59M/deveci_jpdc.pdf:application/pdf},
}

@article{grandlHardStuffFirst2016,
	title = {Do the {Hard} {Stuff} {First}: {Scheduling} {Dependent} {Computations} in {Data}-{Analytics} {Clusters}},
	shorttitle = {Do the {Hard} {Stuff} {First}},
	url = {http://arxiv.org/abs/1604.07371},
	abstract = {We present a scheduler that improves cluster utilization and job completion times by packing tasks having multi-resource requirements and inter-dependencies. While the problem is algorithmically very hard, we achieve near-optimality on the job DAGs that appear in production clusters at a large enterprise and in benchmarks such as TPC-DS. A key insight is that carefully handling the long-running tasks and those with tough-to-pack resource needs will produce good-enough schedules. However, which subset of tasks to treat carefully is not clear (and intractable to discover). Hence, we offer a search procedure that evaluates various possibilities and outputs a preferred schedule order over tasks. An online component enforces the schedule orders desired by the various jobs running on the cluster. In addition, it packs tasks, overbooks the fungible resources and guarantees bounded unfairness for a variety of desirable fairness schemes. Relative to the state-of-the art schedulers, we speed up 50\% of the jobs by over 30\% each.},
	urldate = {2016-12-16},
	journal = {arXiv:1604.07371 [cs]},
	author = {Grandl, Robert and Kandula, Srikanth and Rao, Sriram and Akella, Aditya and Kulkarni, Janardhan},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.07371},
	file = {Grandl et al. - 2016 - Do the Hard Stuff First Scheduling Dependent Comp.pdf:/home/ryan/Zotero/storage/87S8WVQU/Grandl et al. - 2016 - Do the Hard Stuff First Scheduling Dependent Comp.pdf:application/pdf},
}

@inproceedings{jalapartiNetworkAwareSchedulingDataParallel2015,
	address = {New York, NY, USA},
	series = {{SIGCOMM} '15},
	title = {Network-{Aware} {Scheduling} for {Data}-{Parallel} {Jobs}: {Plan} {When} {You} {Can}},
	isbn = {978-1-4503-3542-3},
	shorttitle = {Network-{Aware} {Scheduling} for {Data}-{Parallel} {Jobs}},
	url = {http://doi.acm.org/10.1145/2785956.2787488},
	doi = {10.1145/2785956.2787488},
	abstract = {To reduce the impact of network congestion on big data jobs, cluster management frameworks use various heuristics to schedule compute tasks and/or network flows. Most of these schedulers consider the job input data fixed and greedily schedule the tasks and flows that are ready to run. However, a large fraction of production jobs are recurring with predictable characteristics, which allows us to plan ahead for them. Coordinating the placement of data and tasks of these jobs allows for significantly improving their network locality and freeing up bandwidth, which can be used by other jobs running on the cluster. With this intuition, we develop Corral, a scheduling framework that uses characteristics of future workloads to determine an offline schedule which (i) jointly places data and compute to achieve better data locality, and (ii) isolates jobs both spatially (by scheduling them in different parts of the cluster) and temporally, improving their performance. We implement Corral on Apache Yarn, and evaluate it on a 210 machine cluster using production workloads. Compared to Yarn's capacity scheduler, Corral reduces the makespan of these workloads up to 33\% and the median completion time up to 56\%, with 20-90\% reduction in data transferred across racks.},
	urldate = {2016-12-16},
	booktitle = {Proceedings of the 2015 {ACM} {Conference} on {Special} {Interest} {Group} on {Data} {Communication}},
	publisher = {ACM},
	author = {Jalaparti, Virajith and Bodik, Peter and Menache, Ishai and Rao, Sriram and Makarychev, Konstantin and Caesar, Matthew},
	year = {2015},
	pages = {407--420},
	file = {Jalaparti et al. - 2015 - Network-Aware Scheduling for Data-Parallel Jobs P.pdf:/home/ryan/Zotero/storage/3ET2T2UP/Jalaparti et al. - 2015 - Network-Aware Scheduling for Data-Parallel Jobs P.pdf:application/pdf},
}

@article{red-black,
	series = {Acta '72},
	title = {Symmetric binary {B}-{Trees}: {Data} structure and maintenance algorithms},
	volume = {1},
	issn = {0001-5903, 1432-0525},
	shorttitle = {Symmetric binary {B}-{Trees}},
	url = {https://link.springer.com/article/10.1007/BF00289509},
	doi = {10.1007/BF00289509},
	abstract = {SummaryA class of binary trees is described for maintaining ordered sets of data. Random insertions, deletions, and retrievals of keys can be done in time proportional to log N where N is the cardinality of the data-set. Symmetric B-Trees are a modification of B-trees described previously by Bayer and McCreight. This class of trees properly contains the balanced trees.},
	language = {en},
	number = {4},
	urldate = {2017-04-24},
	journal = {Acta Informatica},
	author = {Bayer, Rudolf},
	month = dec,
	year = {1972},
	pages = {290--306},
	file = {Bayer - 1972 - Symmetric binary B-Trees Data structure and maint.pdf:/home/ryan/Zotero/storage/6PDF59JP/Bayer - 1972 - Symmetric binary B-Trees Data structure and maint.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/834G6RDI/10.html:text/html},
}

@article{interval_tree,
	series = {Journal of {Computer} {Math} '83},
	title = {A {New} {Approach} to {Rectangle} {Intersections}},
	volume = {13},
	journal = {International Journal of Computer Math},
	author = {Edelsbrunner, Herbert},
	year = {1983},
	pages = {221--229},
	file = {Edelsbrunner - 1983 - A New Approach to Rectangle Intersections.pdf:/home/ryan/Zotero/storage/CDBPRI4T/Edelsbrunner - 1983 - A New Approach to Rectangle Intersections.pdf:application/pdf},
}

@article{kuhn,
	series = {{NRLQ} '55},
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	issn = {1931-9193},
	url = {http://onlinelibrary.wiley.com.ezproxy2.library.arizona.edu/doi/10.1002/nav.3800020109/abstract},
	doi = {10.1002/nav.3800020109},
	abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the ‚Äúassignment problem‚Äù is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
	language = {en},
	number = {1-2},
	urldate = {2017-04-15},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	month = mar,
	year = {1955},
	pages = {83--97},
	file = {Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:/home/ryan/Zotero/storage/RFUETZU7/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/7FJCJF7H/abstract.html:text/html},
}

@article{n3_kuhn,
	series = {Networks '71},
	title = {On some techniques useful for solution of transportation network problems},
	volume = {1},
	issn = {1097-0037},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/net.3230010206/abstract},
	doi = {10.1002/net.3230010206},
	abstract = {This paper presents an efficient algorithm for solving transportation problems. The improvement over the existing algorithms of the ‚Äúprimal-dual‚Äù type [3], [5] consists in modifying the ‚Äúpotential-raising‚Äù stages of the solution process in such a way that negative-cost arcs are removed so that the Dijkstra's algorithm may be applied. Especially, the algorithm requires at most n3 additions and comparisons when applied to an n-by-n assignment problem, as compared with the theoretical upper bound proportional to n4 for the number of such operations required by currently available methods. Furthermore, auxiliary techniques of simplifying the original network by means of ‚Äúreduction‚Äù and ‚Äúinduction‚Äù are also introduced as useful tools to treat large-scale problems and specially-structured problems with.},
	language = {en},
	number = {2},
	urldate = {2017-04-15},
	journal = {Networks},
	author = {Tomizawa, N.},
	month = jan,
	year = {1971},
	pages = {173--194},
	file = {Snapshot:/home/ryan/Zotero/storage/VF3WRIZV/abstract.html:text/html},
}

@inproceedings{squall,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Squall: {Fine}-{Grained} {Live} {Reconfiguration} for {Partitioned} {Main} {Memory} {Databases}},
	isbn = {978-1-4503-2758-9},
	shorttitle = {Squall},
	url = {http://doi.acm.org/10.1145/2723372.2723726},
	doi = {10.1145/2723372.2723726},
	abstract = {For data-intensive applications with many concurrent users, modern distributed main memory database management systems (DBMS) provide the necessary scale-out support beyond what is possible with single-node systems. These DBMSs are optimized for the short-lived transactions that are common in on-line transaction processing (OLTP) workloads. One way that they achieve this is to partition the database into disjoint subsets and use a single-threaded transaction manager per partition that executes transactions one-at-a-time in serial order. This minimizes the overhead of concurrency control mechanisms, but requires careful partitioning to limit distributed transactions that span multiple partitions. Previous methods used off-line analysis to determine how to partition data, but the dynamic nature of these applications means that they are prone to hotspots. In these situations, the DBMS needs to reconfigure how data is partitioned in real-time to maintain performance objectives. Bringing the system off-line to reorganize the database is unacceptable for on-line applications. To overcome this problem, we introduce the Squall technique for supporting live reconfiguration in partitioned, main memory DBMSs. Squall supports fine-grained repartitioning of databases in the presence of distributed transactions, high throughput client workloads, and replicated data. An evaluation of our approach on a distributed DBMS shows that Squall can reconfigure a database with no downtime and minimal overhead on transaction latency.},
	urldate = {2017-04-11},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Elmore, Aaron J. and Arora, Vaibhav and Taft, Rebecca and Pavlo, Andrew and Agrawal, Divyakant and El Abbadi, Amr},
	year = {2015},
	keywords = {load-balancing, migration, reconfiguration},
	pages = {299--313},
	file = {Elmore et al. - 2015 - Squall Fine-Grained Live Reconfiguration for Part.pdf:/home/ryan/Zotero/storage/JU6B7XZX/Elmore et al. - 2015 - Squall Fine-Grained Live Reconfiguration for Part.pdf:application/pdf},
}

@article{accordion,
	series = {{VLDB} '14},
	title = {Accordion: {Elastic} {Scalability} for {Database} {Systems} {Supporting} {Distributed} {Transactions}},
	volume = {7},
	issn = {2150-8097},
	shorttitle = {Accordion},
	url = {http://dx.doi.org/10.14778/2732977.2732979},
	doi = {10.14778/2732977.2732979},
	number = {12},
	urldate = {2017-04-11},
	journal = {PVLDB},
	author = {Serafini, Marco and Mansour, Essam and Aboulnaga, Ashraf and Salem, Kenneth and Rafiq, Taha and Minhas, Umar Farooq},
	year = {2014},
	pages = {1035--1046},
	file = {Serafini et al. - 2014 - Accordion Elastic Scalability for Database System.pdf:/home/ryan/Zotero/storage/NTSHMJSW/Serafini et al. - 2014 - Accordion Elastic Scalability for Database System.pdf:application/pdf},
}

@article{schism,
	series = {{VLDB} '14},
	title = {Schism: a workload-driven approach to database replication and partitioning},
	volume = {3},
	issn = {21508097},
	shorttitle = {Schism},
	url = {http://dl.acm.org/citation.cfm?doid=1920841.1920853},
	doi = {10.14778/1920841.1920853},
	language = {en},
	number = {1},
	urldate = {2017-03-21},
	journal = {PVLDB},
	author = {Curino, Carlo and Jones, Evan and Zhang, Yang and Madden, Sam},
	year = {2010},
	keywords = {discontinuous fragments, query span},
	pages = {48--57},
	file = {Curino et al. - 2010 - Schism a workload-driven approach to database rep.pdf:/home/ryan/Zotero/storage/A9GBKI5X/Curino et al. - 2010 - Schism a workload-driven approach to database rep.pdf:application/pdf},
}

@inproceedings{adaptive_science,
	series = {{ICDE} '08},
	title = {Adaptive {Segmentation} for {Scientific} {Databases}},
	doi = {10.1109/ICDE.2008.4497573},
	abstract = {In this paper we explore database segmentation in the context of a column-store DBMS targeted at a scientific database. We present a novel hardware- and scheme-oblivious segmentation algorithm, which learns and adapts to the workload immediately. The approach taken is to capitalize on (intermediate) query results, such that future queries benefit from a more appropriate data layout. The algorithm is implemented as an extension of a complete DBMS and evaluated against a real-life workload. It demonstrates significant performance gains without DBA assistance.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Data} {Engineering}},
	author = {Ivanova, M. and Kersten, M. L. and Nes, N.},
	month = apr,
	year = {2008},
	pages = {1412--1414},
	file = {Ivanova et al. - 2008 - Adaptive Segmentation for Scientific Databases.pdf:/home/ryan/Zotero/storage/BPENDRNV/Ivanova et al. - 2008 - Adaptive Segmentation for Scientific Databases.pdf:application/pdf},
}

@article{dyfram,
	series = {Distrib {Parallel} {DB} '10},
	title = {{DYFRAM}: dynamic fragmentation and replica management in distributed database systems},
	volume = {28},
	issn = {0926-8782, 1573-7578},
	shorttitle = {{DYFRAM}},
	url = {http://link.springer.com.ezproxy1.library.arizona.edu/article/10.1007/s10619-010-7068-1},
	doi = {10.1007/s10619-010-7068-1},
	language = {en},
	number = {2-3},
	urldate = {2016-12-21},
	journal = {Distributed and Parallel Databases},
	author = {Hauglid, Jon Olav and Ryeng, Norvald H. and N√∏rv√•g, Kjetil},
	month = dec,
	year = {2010},
	keywords = {continuous fragments},
	pages = {157--185},
	file = {Hauglid et al. - 2010 - DYFRAM dynamic fragmentation and replica manageme.pdf:/home/ryan/Zotero/storage/Q6KGII86/Hauglid et al. - 2010 - DYFRAM dynamic fragmentation and replica manageme.pdf:application/pdf},
}

@article{oldai_fragment,
	series = {{IEEE} {Software} {Engineering} '91},
	title = {Fragmenting relations horizontally using a knowledge-based approach},
	volume = {17},
	issn = {00985589},
	url = {http://ieeexplore.ieee.org/document/92906/},
	doi = {10.1109/32.92906},
	number = {9},
	urldate = {2016-12-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Shin, Dong-Guk and Irani, Keki},
	month = sep,
	year = {1991},
	pages = {872--883},
	file = {Shin and Irani - 1991 - Fragmenting relations horizontally using a knowled.pdf:/home/ryan/Zotero/storage/NPP9SZN5/Shin and Irani - 1991 - Fragmenting relations horizontally using a knowled.pdf:application/pdf},
}

@article{cost_func_frag,
	series = {{IJCA} '10},
	title = {A {New} {Technique} for {Database} {Fragmentation} in {Distributed} {Systems}},
	volume = {5},
	issn = {09758887},
	url = {http://www.ijcaonline.org/volume5/number9/pxc3871318.pdf},
	doi = {10.5120/940-1318},
	number = {9},
	urldate = {2016-12-21},
	journal = {International Journal of Computer Applications},
	author = {Khan, Shahidul Islam and Hoque, Dr. A. S. M. Latiful},
	month = aug,
	year = {2010},
	pages = {20--24},
	file = {Khan and Hoque - 2010 - A New Technique for Database Fragmentation in Dist.pdf:/home/ryan/Zotero/storage/A6ESA9AQ/Khan and Hoque - 2010 - A New Technique for Database Fragmentation in Dist.pdf:application/pdf},
}

@inproceedings{mariposa,
	series = {{ICDE} '96},
	title = {Data replication in {Mariposa}},
	doi = {10.1109/ICDE.1996.492198},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Data} {Engineering}},
	author = {Sidell, J. and Aoki, P. M. and Sah, A. and Staelin, C. and Stonebraker, M. and Yu, A.},
	month = feb,
	year = {1996},
	pages = {485--494},
	file = {Sidell et al. - 1996 - Data replication in Mariposa.pdf:/home/ryan/Zotero/storage/25BIENH5/Sidell et al. - 1996 - Data replication in Mariposa.pdf:application/pdf},
}

@article{sword,
	series = {{VLDBJ} '14},
	title = {{SWORD}: {Workload}-aware {Data} {Placement} and {Replica} {Selection} for {Cloud} {Data} {Management} {Systems}},
	volume = {23},
	issn = {1066-8888},
	shorttitle = {{SWORD}},
	url = {http://dx.doi.org/10.1007/s00778-014-0362-1},
	doi = {10.1007/s00778-014-0362-1},
	number = {6},
	urldate = {2016-12-16},
	journal = {The VLDB Journal},
	author = {Kumar, K. Ashwin and Quamar, Abdul and Deshpande, Amol and Khuller, Samir},
	month = dec,
	year = {2014},
	keywords = {discontinuous fragments, query span},
	pages = {845--870},
	file = {Kumar et al. - 2014 - SWORD Workload-aware Data Placement and Replica S.pdf:/home/ryan/Zotero/storage/GRQ839T9/Kumar et al. - 2014 - SWORD Workload-aware Data Placement and Replica S.pdf:application/pdf},
}

@inproceedings{dag_lc,
	series = {{ICCP} '88},
	title = {A general approach to mapping of parallel computation upon multiprocessor architectures},
	booktitle = {Proceedings of {International} {Conference} on {Parallel} {Processing}},
	author = {Kim, S.J. and Browne},
	month = aug,
	year = {1988},
	keywords = {latency},
	file = {Kim and Browne - 1988 - A general approach to mapping of parallel computat.pdf:/home/ryan/Zotero/storage/D4M97TJG/Kim and Browne - 1988 - A general approach to mapping of parallel computat.pdf:application/pdf},
}

@article{dag_md,
	title = {Hypertool: {A} {Programming} {Aid} for {Message}-{Passing} {Systems}},
	volume = {1},
	shorttitle = {Hypertool},
	abstract = {As both the number of processors and the complexity of problems to be solved increase, programming multiprocessing systems becomes more difficult and error-prone. This paper discusses programming assistance and automation concepts and their application to a program development tool for message-passing systems called Hypertool. It performs scheduling and handles the communication primitive insertion automatically. Two algorithms, based on the critical-path method, are presented for scheduling processes statically. Hypertool also generates the performance estimates and other program quality measures to help programmers in improving their algorithms and programs.},
	journal = {Ieee Trans. on Parallel and Distributed Systems},
	author = {Wu, Min-you and Gajski, Daniel D.},
	year = {1990},
	keywords = {latency},
	pages = {330--343},
	file = {Wu and Gajski - 1990 - Hypertool A Programming Aid for Message-Passing S.pdf:/home/ryan/Zotero/storage/AW5E7D5W/Wu and Gajski - 1990 - Hypertool A Programming Aid for Message-Passing S.pdf:application/pdf},
}

@article{dag_dsc,
	title = {{DSC}: {Scheduling} {Parallel} {Tasks} on an {Unbounded} {Number} of {Processors}},
	volume = {5},
	issn = {1045-9219},
	shorttitle = {{DSC}},
	url = {http://dx.doi.org/10.1109/71.308533},
	doi = {10.1109/71.308533},
	abstract = {We present a low-complexity heuristic, named the dominant sequence clusteringalgorithm (DSC), for scheduling parallel tasks on an unbounded number of completelyconnected processors. The performance of DSC is on average, comparable to, or evenbetter than, other higher-complexity algorithms. We assume no task duplication andnonzero communication overhead between processors. Finding the optimum solution forarbitrary directed acyclic task graphs (DAG's) is NP-complete. DSC finds optimalschedules for special classes of DAG's, such as fork, join, coarse-grain trees, and somefine-grain trees. It guarantees a performance within a factor of 2 of the optimum forgeneral coarse-grain DAG's. We compare DSC with three higher-complexity generalscheduling algorithms: the ETF by J.J. Hwang, Y.C. Chow, F.D. Anger, and C.Y. Lee(1989); V. Sarkar's (1989) clustering algorithm; and the MD by M.Y. Wu and D. Gajski(1990). We also give a sample of important practical applications where DSC has beenfound useful.},
	number = {9},
	urldate = {2016-12-15},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Yang, T. and Gerasoulis, A.},
	month = sep,
	year = {1994},
	keywords = {latency},
	pages = {951--967},
	file = {Yang and Gerasoulis - 1994 - DSC Scheduling Parallel Tasks on an Unbounded Num.pdf:/home/ryan/Zotero/storage/2PD73GKD/Yang and Gerasoulis - 1994 - DSC Scheduling Parallel Tasks on an Unbounded Num.pdf:application/pdf},
}

@article{dag_twopass,
	series = {Parallel {Computing} '94},
	title = {A {Two}-pass {Scheduling} {Algorithm} for {Parallel} {Programs}},
	volume = {20},
	issn = {0167-8191},
	url = {http://dx.doi.org/10.1016/0167-8191(94)90121-X},
	doi = {10.1016/0167-8191(94)90121-X},
	number = {6},
	urldate = {2016-12-15},
	journal = {Parallel Computing},
	author = {Kim, Dongseung and Yi, Byung-Guoen},
	month = jun,
	year = {1994},
	keywords = {latency},
	pages = {869--885},
	file = {Kim and Yi - 1994 - A Two-pass Scheduling Algorithm for Parallel Progr.pdf:/home/ryan/Zotero/storage/8M9PCDT2/Kim and Yi - 1994 - A Two-pass Scheduling Algorithm for Parallel Progr.pdf:application/pdf},
}

@inproceedings{dag_dsc,
	series = {{ICCP} '94},
	title = {A {Static} {Scheduling} {Algorithm} {Using} {Dynamic} {Critical} {Path} for {Assigning} {Parallel} {Algorithms} onto {Multiprocessors}},
	volume = {2},
	doi = {10.1109/ICPP.1994.46},
	abstract = {An algorithm for compile-time static scheduling of task graphs onto multiprocessors is proposed. The proposed algorithm, which is called Dynamic Critical Path (DCP) scheduling algorithm, is different from previously reported algorithms in a number of ways. First, it determines the critical path of the task graph and selects the next node to be scheduled in a dynamic fashion. Second, it rearranges the schedule on each processor dynamically in the sense that the positions of the nodes in the partial schedules are not fixed until all nodes have been considered. Third, it uses an intelligent way to select a suitable processor for a node by looking ahead the potential start times of the remaining critical nodes on that processor and by scheduling relatively less important nodes to the processors already in use. Four related scheduling algorithms are also discussed. Although these algorithms are efficient in general, they possess drawbacks which can lead to poor performance. The proposed DCP algorithm overcomes the drawbacks of these algorithms and outperforms them by a considerable margin. Despite having a number of new features, the DCP algorithm is fast, efficient in terms of the number of processors used and is equally suitable for different types of graph structures.},
	booktitle = {1994 {Internatonal} {Conference} on {Parallel} {Processing} {Vol}. 2},
	author = {Kwok, Y. K. K. Yu-Kwong and Ahmad, I.},
	month = aug,
	year = {1994},
	keywords = {latency},
	pages = {155--159},
	file = {Kwok and Ahmad - 1994 - A Static Scheduling Algorithm Using Dynamic Critic.pdf:/home/ryan/Zotero/storage/F2U8RDQD/Kwok and Ahmad - 1994 - A Static Scheduling Algorithm Using Dynamic Critic.pdf:application/pdf},
}

@phdthesis{dag_ez,
	address = {Stanford, CA, USA},
	title = {Partitioning and {Scheduling} {Parallel} {Programs} for {Execution} on {Multiprocessors}},
	abstract = {There are three fundamental problems to be solved in the execution of a parallel program on a multiprocessor--identifying the parallelism in the program, partitioning the program into tasks and scheduling the tasks on processors. Whereas the problem of identifying parallelism is a programming language issue, the partitioning and scheduling problems are intimately related to parameters of the target multiprocessor, like the number of processors and synchronisation and communication overhead. It is desirable for the partitioning and scheduling to be performed automatically, so that the same parallel program can execute efficiently on different multiprocessors. This dissertation presents two solutions to the partitioning and scheduling problems. The first approach is based on a macro-dataflow model, where the program is partitioned into tasks at compile-time and the tasks are scheduled on processors at run-time. The second approach is based on a compile-time scheduling model, where the partitioning of the program and the scheduling of tasks on processors are both performed at compile-time.Both approaches have been implemented to partition programs written in the single-assignment language, SISAL. The inputs to the partitioning and scheduling algorithms are a graphical representation of the program and a list of parameters describing the target multiprocessor. Execution profile information is used to derive compile-time estimates of execution times and data sizes in the program. Both the macro-dataflow and compile-time scheduling problems are expressed as optimisation problems, which are proved to be NP-complete in the strong sense. This dissertation presents efficient approximation algorithms for these problems. The effectiveness of the partitioning and scheduling algorithms is studied by multiprocessor simulations of various SISAL benchmark programs for different target multiprocessor parameters.},
	school = {Stanford University},
	author = {Sarkar, V.},
	year = {1989},
	keywords = {latency},
	file = {Sarkar - 1989 - Partitioning and Scheduling Parallel Programs for .pdf:/home/ryan/Zotero/storage/ACIGNMZK/Sarkar - 1989 - Partitioning and Scheduling Parallel Programs for .pdf:application/pdf},
}

@article{dag_supercomputing,
	series = {{JSC} '16},
	title = {Cost-aware {DAG} scheduling algorithms for minimizing execution cost on cloud resources},
	volume = {72},
	issn = {0920-8542, 1573-0484},
	url = {http://link.springer.com/10.1007/s11227-016-1637-7},
	doi = {10.1007/s11227-016-1637-7},
	language = {en},
	number = {3},
	urldate = {2016-12-15},
	journal = {The Journal of Supercomputing},
	author = {Convolbo, Mo√Øse W. and Chou, Jerry},
	month = mar,
	year = {2016},
	keywords = {cost},
	pages = {985--1012},
	file = {Convolbo and Chou - 2016 - Cost-aware DAG scheduling algorithms for minimizin.pdf:/home/ryan/Zotero/storage/UAANDC25/Convolbo and Chou - 2016 - Cost-aware DAG scheduling algorithms for minimizin.pdf:application/pdf},
}

@article{dag_survey,
	series = {{CSUR} '99},
	title = {Static {Scheduling} {Algorithms} for {Allocating} {Directed} {Task} {Graphs} to {Multiprocessors}},
	volume = {31},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/344588.344618},
	doi = {10.1145/344588.344618},
	abstract = {Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse   assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.},
	number = {4},
	urldate = {2016-12-15},
	journal = {ACM Computing Surveys},
	author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
	month = dec,
	year = {1999},
	keywords = {latency},
	pages = {406--471},
	file = {Kwok and Ahmad - 1999 - Static Scheduling Algorithms for Allocating Direct.pdf:/home/ryan/Zotero/storage/SZ2FMUWV/Kwok and Ahmad - 1999 - Static Scheduling Algorithms for Allocating Direct.pdf:application/pdf},
}

@article{powerof2,
	series = {{IEEE} {Parallel} {Distrib}. {Sys}. '01},
	title = {The {Power} of {Two} {Choices} in {Randomized} {Load} {Balancing}},
	volume = {12},
	issn = {1045-9219},
	url = {http://dx.doi.org/10.1109/71.963420},
	doi = {10.1109/71.963420},
	number = {10},
	journal = {IEEE Transactions on Parallel Distributed Systems},
	author = {Mitzenmacher, Michael},
	month = oct,
	year = {2001},
	note = {tex.acmid= 504343
tex.issue\_date= October 2001
tex.numpages= 11},
	keywords = {distributed systems, limiting systems, Load balancing, queuing theory},
	pages = {1094--1104},
	file = {10.1.1.57.4019.pdf:/home/ryan/Zotero/storage/BNXHAU7C/10.1.1.57.4019.pdf:application/pdf},
}

@inproceedings{adaptiveastar,
	series = {{ICAPS} '06},
	title = {A {New} {Principle} for {Incremental} {Heuristic} {Search}: {Theoretical} {Results}},
	booktitle = {Proceedings of the {International} {Conference} on {Automated} {Planning} and {Scheduling}},
	author = {Koenig, Sven and Likhachev, Maxim},
	year = {2006},
	pages = {402--405},
	file = {aaprinciple_icaps06.pdf:/home/ryan/Zotero/storage/DJ8HG4C4/aaprinciple_icaps06.pdf:application/pdf},
}

@inproceedings{pred_multiple,
	series = {{ICDE} '09},
	title = {Predicting {Multiple} {Metrics} for {Queries}: {Better} {Decisions} {Enabled} by {Machine} {Learning}},
	shorttitle = {Predicting {Multiple} {Metrics} for {Queries}},
	doi = {10.1109/ICDE.2009.130},
	abstract = {One of the most challenging aspects of managing a very large data warehouse is identifying how queries will behave before they start executing. Yet knowing their performance characteristics - their runtimes and resource usage - can solve two important problems. First, every database vendor struggles with managing unexpectedly long-running queries. When these long-running queries can be identified before they start, they can be rejected or scheduled when they will not cause extreme resource contention for the other queries in the system. Second, deciding whether a system can complete a given workload in a given time period (or a bigger system is necessary) depends on knowing the resource requirements of the queries in that workload. We have developed a system that uses machine learning to accurately predict the performance metrics of database queries whose execution times range from milliseconds to hours. For training and testing our system, we used both real customer queries and queries generated from an extended set of TPC-DS templates. The extensions mimic queries that caused customer problems. We used these queries to compare how accurately different techniques predict metrics such as elapsed time, records used, disk I/Os, and message bytes. The most promising technique was not only the most accurate, but also predicted these metrics simultaneously and using only information available prior to query execution. We validated the accuracy of this machine learning technique on a number of HP Neoview configurations. We were able to predict individual query elapsed time within 20\% of its actual time for 85\% of the test queries. Most importantly, we were able to correctly identify both the short and long-running (up to two hour) queries to inform workload management and capacity planning.},
	booktitle = {2009 {IEEE} 25th {International} {Conference} on {Data} {Engineering}},
	author = {Ganapathi, A. and Kuno, H. and Dayal, U. and Wiener, J. L. and Fox, A. and Jordan, M. and Patterson, D.},
	month = mar,
	year = {2009},
	keywords = {machine learning, Databases, query processing, Machine learning, learning (artificial intelligence), Capacity planning, Computer science, Data engineering, Data warehouses, database performance prediction, database queries, Measurement, Milling machines, operational business intelligence, performance metrics, software metrics, System testing, USA Councils, very large data warehouse, very large databases},
	pages = {592--603},
	file = {Ganapathi et al. - 2009 - Predicting Multiple Metrics for Queries Better De.pdf:/home/ryan/Zotero/storage/WBD9BCET/Ganapathi et al. - 2009 - Predicting Multiple Metrics for Queries Better De.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/PQSIYUVJ/4812438.html:text/html},
}

@article{pollack_ram,
	series = {{AI} '90},
	title = {Recursive distributed representations},
	volume = {46},
	issn = {00043702},
	url = {http://linkinghub.elsevier.com/retrieve/pii/000437029090005K},
	doi = {10.1016/0004-3702(90)90005-K},
	language = {en},
	number = {1-2},
	urldate = {2017-11-07},
	journal = {Artificial Intelligence},
	author = {Pollack, Jordan B.},
	month = nov,
	year = {1990},
	pages = {77--105},
	file = {Pollack - 1990 - Recursive distributed representations.pdf:/home/ryan/Zotero/storage/6XJ4CM49/Pollack - 1990 - Recursive distributed representations.pdf:application/pdf},
}

@inproceedings{priority,
	series = {{ICDE} '04},
	title = {Priority mechanisms for {OLTP} and transactional {Web} applications},
	isbn = {978-0-7695-2065-0},
	url = {http://ieeexplore.ieee.org/document/1320025/},
	doi = {10.1109/ICDE.2004.1320025},
	urldate = {2017-10-15},
	publisher = {IEEE Comput. Soc},
	author = {McWherter, D.T. and Schroeder, B. and Ailamaki, A. and Harchol-Balter, M.},
	year = {2004},
	pages = {535--546},
	file = {McWherter et al. - 2004 - Priority mechanisms for OLTP and transactional Web.pdf:/home/ryan/Zotero/storage/R8J7D6ZT/McWherter et al. - 2004 - Priority mechanisms for OLTP and transactional Web.pdf:application/pdf},
}

@inproceedings{forecast,
	series = {{NOMS} '12},
	title = {Workload characterization and prediction in the cloud: {A} multiple time series approach},
	isbn = {978-1-4673-0269-2 978-1-4673-0267-8 978-1-4673-0268-5},
	shorttitle = {Workload characterization and prediction in the cloud},
	url = {http://ieeexplore.ieee.org/document/6212065/},
	doi = {10.1109/NOMS.2012.6212065},
	urldate = {2017-10-15},
	publisher = {IEEE},
	author = {Khan, Arijit and {Xifeng Yan} and {Shu Tao} and Anerousis, N.},
	month = apr,
	year = {2012},
	pages = {1287--1294},
	file = {Khan et al. - 2012 - Workload characterization and prediction in the cl.pdf:/home/ryan/Zotero/storage/XKZ4FB22/Khan et al. - 2012 - Workload characterization and prediction in the cl.pdf:application/pdf},
}

@article{bffd,
	series = {Theoretical {Computer} {Science} '08},
	title = {The class constrained bin packing problem with applications to video-on-demand},
	volume = {393},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397508000273},
	doi = {10.1016/j.tcs.2008.01.001},
	number = {1},
	urldate = {2017-09-13},
	journal = {Theoretical Computer Science},
	author = {Xavier, E. C. and Miyazawa, F. K.},
	month = mar,
	year = {2008},
	keywords = {Data placement},
	pages = {240--259},
	file = {ScienceDirect Snapshot:/home/ryan/Zotero/storage/CMXT9JKV/S0304397508000273.html:text/html;Xavier and Miyazawa - 2008 - The class constrained bin packing problem with app.pdf:/home/ryan/Zotero/storage/HIVTVCC2/Xavier and Miyazawa - 2008 - The class constrained bin packing problem with app.pdf:application/pdf},
}

@article{url-jgrapht,
	title = {{JGraphT}, http://jgrapht.org/},
	url = {https://aws.amazon.com/rds/},
	note = {tex.key= 1},
}

@article{dp_pca_orig,
	title = {Best piecewise constant approximation of a function of single variable},
	volume = {7},
	issn = {01676377},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0167637788900302},
	doi = {10.1016/0167-6377(88)90030-2},
	language = {en},
	number = {4},
	urldate = {2017-09-10},
	journal = {Operations Research Letters},
	author = {Konno, Hiroshi and Kuno, Takahito},
	month = aug,
	year = {1988},
	pages = {205--210},
	file = {Konno and Kuno - 1988 - Best piecewise constant approximation of a functio.pdf:/home/ryan/Zotero/storage/R5UX2B6Z/Konno and Kuno - 1988 - Best piecewise constant approximation of a functio.pdf:application/pdf},
}

@article{dp_pca,
	series = {Information {Systems} '17},
	title = {A scalable dynamic programming scheme for the computation of optimal k -segments for ordered data},
	issn = {03064379},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0306437916300990},
	doi = {10.1016/j.is.2016.08.002},
	language = {en},
	urldate = {2017-09-10},
	journal = {Information Systems},
	author = {Mahlknecht, Giovanni and Dign√∂s, Anton and Gamper, Johann},
	month = sep,
	year = {2017},
	file = {Mahlknecht et al. - 2016 - A scalable dynamic programming scheme for the comp.pdf:/home/ryan/Zotero/storage/4S2Q9M75/Mahlknecht et al. - 2016 - A scalable dynamic programming scheme for the comp.pdf:application/pdf},
}

@article{running_mean,
	series = {Technometrics '62},
	title = {Note on a {Method} for {Calculating} {Corrected} {Sums} of {Squares} and {Products}},
	volume = {4},
	number = {3},
	journal = {Technometrics},
	author = {Welford, BP},
	year = {1962},
	pages = {419--420},
	file = {Welford - 1962 - Note on a Method for Calculating Corrected Sums of.pdf:/home/ryan/Zotero/storage/DXHHKZD5/Welford - 1962 - Note on a Method for Calculating Corrected Sums of.pdf:application/pdf},
}

@book{cart,
	address = {Boca Raton},
	title = {Classification and regression trees},
	isbn = {978-0-412-04841-8},
	language = {eng},
	publisher = {Chapman \& Hall},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard and Stone, Charles},
	year = {1998},
	keywords = {Baum, Diskriminante, Diskriminanzanalyse, Graph, Regressionsanalyse},
}

@article{kernel_methods,
	title = {Kernel {Methods} in {Machine} {Learning}},
	volume = {36},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/25464664},
	doi = {10.2307/25464664},
	abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
	number = {3},
	urldate = {2017-08-18},
	journal = {The Annals of Statistics},
	author = {Hofmann, Thomas and Sch√∂lkopf, Bernhard and Smola, Alexander J.},
	year = {2008},
	pages = {1171--1220},
	file = {Hofmann et al. - 2008 - Kernel Methods in Machine Learning.pdf:/home/ryan/Zotero/storage/F7993C9D/Hofmann et al. - 2008 - Kernel Methods in Machine Learning.pdf:application/pdf},
}

@article{rbf,
	title = {Universal {Approximation} {Using} {Radial}-{Basis}-{Function} {Networks}},
	volume = {3},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.1991.3.2.246},
	doi = {10.1162/neco.1991.3.2.246},
	number = {2},
	urldate = {2017-08-10},
	journal = {Neural Computation},
	author = {Park, J. and Sandberg, I. W.},
	month = jun,
	year = {1991},
	pages = {246--257},
	file = {Neural Computation Snapshot:/home/ryan/Zotero/storage/BSABD67M/neco.1991.3.2.html:text/html;Park and Sandberg - 1991 - Universal Approximation Using Radial-Basis-Functio.pdf:/home/ryan/Zotero/storage/4X9MX6LC/Park and Sandberg - 1991 - Universal Approximation Using Radial-Basis-Functio.pdf:application/pdf},
}

@article{concept_drift,
	title = {Incremental learning from noisy data},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/BF00116895},
	doi = {10.1007/BF00116895},
	abstract = {Induction of a concept description given noisy instances is difficult and is further exacerbated when the concepts may change over time. This paper presents a solution which has been guided by psychological and mathematical results. The method is based on a distributed concept description which is composed of a set of weighted, symbolic characterizations. Two learning processes incrementally modify this description. One adjusts the characterization weights and another creates new characterizations. The latter process is described in terms of a search through the space of possibilities and is shown to require linear space with respect to the number of attribute-value pairs in the description language. The method utilizes previously acquired concept definitions in subsequent learning by adding an attribute for each learned concept to instance descriptions. A program called STAGGER fully embodies this method, and this paper reports on a number of empirical analyses of its performance. Since understanding the relationships between a new learning method and existing ones can be difficult, this paper first reviews a framework for discussing machine learning systems and then describes STAGGER in that framework.},
	language = {en},
	number = {3},
	journal = {Machine Learning},
	author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
	month = sep,
	year = {1986},
	pages = {317--354},
	file = {Schlimmer and Granger - 1986 - Incremental learning from noisy data.pdf:/home/ryan/Zotero/storage/YLY7PM49/Schlimmer and Granger - 1986 - Incremental learning from noisy data.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/RCYQB6QS/BF00116895.html:text/html},
}

@misc{practical_rf_kernel,
	title = {The {Random} {Forest} {Kernel} and other kernels for big data from random partitions},
	url = {https://arxiv.org/abs/1402.4293},
	abstract = {We present Random Partition Kernels, a new class of kernels derived by demonstrating a natural connection between random partitions of objects and kernels between those objects. We show how the construction can be used to create kernels from methods that would not normally be viewed as random partitions, such as Random Forest. To demonstrate the potential of this method, we propose two new kernels, the Random Forest Kernel and the Fast Cluster Kernel, and show that these kernels consistently outperform standard kernels on problems involving real-world datasets. Finally, we show how the form of these kernels lend themselves to a natural approximation that is appropriate for certain big data problems, allowing O(N) inference in methods such as Gaussian Processes, Support Vector Machines and Kernel PCA.},
	publisher = {arXiv},
	author = {Davies, Alex and Ghahramani, Zoubin},
	month = feb,
	year = {2014},
	file = {Davies and Ghahramani - 2014 - The Random Forest Kernel and other kernels for big.pdf:/home/ryan/Zotero/storage/WENNLTAN/Davies and Ghahramani - 2014 - The Random Forest Kernel and other kernels for big.pdf:application/pdf},
}

@article{rf_kernel_theory,
	title = {Extremely randomized trees},
	volume = {63},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/s10994-006-6226-1},
	doi = {10.1007/s10994-006-6226-1},
	abstract = {This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	month = apr,
	year = {2006},
	pages = {3--42},
	file = {Geurts et al. - 2006 - Extremely randomized trees.pdf:/home/ryan/Zotero/storage/3Q3VEPQW/Geurts et al. - 2006 - Extremely randomized trees.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/H64WDDVA/10.html:text/html},
}

@techreport{init_rf_kernel,
	title = {Some infinity theory for predictor ensembles},
	institution = {Technical Report 579, Statistics Dept. UCB},
	author = {Breiman, Leo},
	year = {2000},
	file = {Breiman - 2000 - Some infinity theory for predictor ensembles.pdf:/home/ryan/Zotero/storage/59WU39YN/Breiman - 2000 - Some infinity theory for predictor ensembles.pdf:application/pdf},
}

@article{set_cover,
	series = {{MOR} '79},
	title = {A {Greedy} {Heuristic} for the {Set}-{Covering} {Problem}},
	volume = {4},
	issn = {0364-765X},
	url = {http://www.jstor.org/stable/3689577},
	abstract = {Let A be a binary matrix of size m √ó n, let {\textless}tex-math{\textgreater}\$c{\textasciicircum}\{T\}\${\textless}/tex-math{\textgreater} be a positive row vector of length n and let e be the column vector, all of whose m components are ones. The set-covering problem is to minimize c$^{\textrm{T}}$x subject to Ax ‚â• e and x binary. We compare the value of the objective function at a feasible solution found by a simple greedy heuristic to the true optimum. It turns out that the ratio between the two grows at most logarithmically in the largest column sum of A. When all the components of {\textless}tex-math{\textgreater}\$c{\textasciicircum}\{T\}\${\textless}/tex-math{\textgreater} are the same, our result reduces to a theorem established previously by Johnson and Lovasz.},
	number = {3},
	urldate = {2017-04-24},
	journal = {Mathematics of Operations Research},
	author = {Chvatal, V.},
	year = {1979},
	pages = {233--235},
	file = {Chvatal - 1979 - A Greedy Heuristic for the Set-Covering Problem.pdf:/home/ryan/Zotero/storage/TNDECKGJ/Chvatal - 1979 - A Greedy Heuristic for the Set-Covering Problem.pdf:application/pdf},
}

@article{e-store,
	series = {{VLDB} '15},
	title = {E-{Store}: {Fine}-grained {Elastic} {Partitioning} for {Distributed} {Transaction} {Processing} {Systems}},
	volume = {8},
	issn = {2150-8097},
	shorttitle = {E-store},
	url = {http://dx.doi.org/10.14778/2735508.2735514},
	doi = {10.14778/2735508.2735514},
	number = {3},
	urldate = {2017-04-11},
	journal = {PVLDB},
	author = {Taft, Rebecca and Mansour, Essam and Serafini, Marco and Duggan, Jennie and Elmore, Aaron J. and Aboulnaga, Ashraf and Pavlo, Andrew and Stonebraker, Michael},
	year = {2014},
	pages = {245--256},
	file = {Taft et al. - 2014 - E-store Fine-grained Elastic Partitioning for Dis.pdf:/home/ryan/Zotero/storage/D88Q698A/Taft et al. - 2014 - E-store Fine-grained Elastic Partitioning for Dis.pdf:application/pdf},
}

@inproceedings{selfdrivingcidr,
	series = {{CIDR} '17},
	title = {Self-{Driving} {Database} {Management} {Systems}},
	booktitle = {8th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Pavlo, Andrew and Angulo, Gustavo and Arulraj, Joy and Lin, Haibin and Lin, Jiexi and Ma, Lin and Menon, Prashanth and Mowry, Todd C. and Perron, Matthew and Quah, Ian and Santurkar, Siddharth and Tomasic, Anthony and Toor, Skye and Aken, Dana Van and Wang, Ziqi and Wu, Yingjun and Xian, Ran and Zhang, Tieying},
	year = {2017},
	file = {Pavlo et al. - 2017 - Self-Driving Database Management Systems.pdf:/home/ryan/Zotero/storage/RQ4F8NKQ/Pavlo et al. - 2017 - Self-Driving Database Management Systems.pdf:application/pdf},
}

@article{ccbp,
	series = {J. of {Scheduling} '01},
	title = {Polynomial time approximation schemes for class-constrained packing problems},
	volume = {4},
	issn = {1099-1425},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/jos.86/abstract},
	doi = {10.1002/jos.86},
	language = {en},
	number = {6},
	urldate = {2017-02-07},
	journal = {Journal of Scheduling},
	author = {Shachnai, Hadas and Tamir, Tami},
	month = nov,
	year = {2001},
	keywords = {algorithms, class constraints, multiple knapsack, polynomial time approximation schemes},
	pages = {313--338},
	file = {Shachnai and Tamir - 2001 - Polynomial time approximation schemes for class-co.pdf:/home/ryan/Zotero/storage/9TRCMSQH/Shachnai and Tamir - 2001 - Polynomial time approximation schemes for class-co.pdf:application/pdf},
}

@article{ccbpr,
	series = {Theoretical {Computer} {Science} '10},
	title = {Class constrained bin packing revisited},
	volume = {411},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397510002549},
	doi = {10.1016/j.tcs.2010.04.037},
	abstract = {We study the following variant of the bin packing problem. We are given a set of items, where each item has a (non-negative) size and a color. We are also given an integer parameter k, and the goal is to partition the items into a minimum number of subsets such that for each subset S in the solution, the total size of the items in S is at most 1 (as in the classical bin packing problem) and the total number of colors of the items in S is at most k (which distinguishes our problem from the classical version). We follow earlier work on this problem and study the problem in both offline and online scenarios.},
	number = {34},
	urldate = {2017-02-07},
	journal = {Theoretical Computer Science},
	author = {Epstein, Leah and Imreh, Csan√°d and Levin, Asaf},
	month = jul,
	year = {2010},
	keywords = {AFPTAS, Online algorithms},
	pages = {3073--3089},
	file = {Epstein et al. - 2010 - Class constrained bin packing revisited.pdf:/home/ryan/Zotero/storage/7KTCWIKH/Epstein et al. - 2010 - Class constrained bin packing revisited.pdf:application/pdf},
}

@article{cloud_pareto,
	title = {Cost-efficient task scheduling for executing large programs in the cloud},
	volume = {39},
	issn = {01678191},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167819113000355},
	doi = {10.1016/j.parco.2013.03.002},
	language = {en},
	number = {4-5},
	urldate = {2016-12-29},
	journal = {Parallel Computing},
	author = {Su, Sen and Li, Jian and Huang, Qingjia and Huang, Xiao and Shuang, Kai and Wang, Jie},
	month = apr,
	year = {2013},
	keywords = {cost},
	pages = {177--188},
	file = {Su et al. - 2013 - Cost-efficient task scheduling for executing large.pdf:/home/ryan/Zotero/storage/KIXDRWZ7/Su et al. - 2013 - Cost-efficient task scheduling for executing large.pdf:application/pdf},
}

@article{cca,
	title = {{CCA}: a deadline-constrained workflow scheduling algorithm for multicore resources on the cloud},
	issn = {0920-8542, 1573-0484},
	shorttitle = {{CCA}},
	url = {http://link.springer.com/10.1007/s11227-016-1789-5},
	doi = {10.1007/s11227-016-1789-5},
	language = {en},
	urldate = {2016-12-29},
	journal = {The Journal of Supercomputing},
	author = {Deldari, Arash and Naghibzadeh, Mahmoud and Abrishami, Saeid},
	month = jun,
	year = {2016},
	keywords = {cost},
	file = {Deldari et al. - 2016 - CCA a deadline-constrained workflow scheduling al.pdf:/home/ryan/Zotero/storage/M3RP6PVF/Deldari et al. - 2016 - CCA a deadline-constrained workflow scheduling al.pdf:application/pdf},
}

@article{permelearn,
	title = {Learning {Permutations} with {Exponential} {Weights}},
	volume = {10},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1577069.1755841},
	abstract = {We give an algorithm for the on-line learning of permutations. The algorithm maintains its uncertainty about the target permutation as a doubly stochastic weight matrix, and makes predictions using an efficient method for decomposing the weight matrix into a convex combination of permutations. The weight matrix is updated by multiplying the current matrix entries by exponential factors, and an iterative procedure is needed to restore double stochasticity. Even though the result of this procedure does not have a closed form, a new analysis approach allows us to prove an optimal (up to small constant factors) bound on the regret of our algorithm. This regret bound is significantly better than that of either Kalai and Vempala's more efficient Follow the Perturbed Leader algorithm or the computationally expensive method of explicitly representing each permutation as an expert.},
	urldate = {2016-12-17},
	journal = {J. Mach. Learn. Res.},
	author = {Helmbold, David P. and Warmuth, Manfred K.},
	month = dec,
	year = {2009},
	pages = {1705--1736},
	file = {Helmbold and Warmuth - 2009 - Learning Permutations with Exponential Weights.pdf:/home/ryan/Zotero/storage/IGJGXP4Z/Helmbold and Warmuth - 2009 - Learning Permutations with Exponential Weights.pdf:application/pdf},
}

@article{vertica,
	series = {{VLDB} '12},
	title = {The {Vertica} {Analytic} {Database}: {C}-store 7 {Years} {Later}},
	volume = {5},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2367502.2367518},
	doi = {10.14778/2367502.2367518},
	number = {12},
	journal = {PVLDB},
	author = {Lamb, Andrew and Fuller, Matt and Varadarajan, Ramakrishna and Tran, Nga and Vandiver, Ben and Doshi, Lyric and Bear, Chuck},
	year = {2012},
	note = {tex.acmid= 2367518
tex.issue\_date= August 2012
tex.numpages= 12},
	pages = {1790--1801},
	file = {1208.4173.pdf:/home/ryan/Zotero/storage/ERNMCVCV/1208.4173.pdf:application/pdf},
}

@inproceedings{wordvec2,
	series = {H:{T} '11},
	title = {Learning word vectors for sentiment analysis},
	volume = {1},
	isbn = {978-1-932432-87-9},
	url = {http://dl.acm.org/citation.cfm?id=2002472.2002491},
	urldate = {2018-11-24},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	month = jun,
	year = {2011},
	pages = {142--150},
	file = {Full Text PDF:/home/ryan/Zotero/storage/5JANW25G/Maas et al. - 2011 - Learning word vectors for sentiment analysis.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/HR5WP99Y/citation.html:text/html},
}

@article{gigo,
	series = {Comm. {ACM} '12},
	title = {A {Few} {Useful} {Things} to {Know} {About} {Machine} {Learning}},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2347736.2347755},
	doi = {10.1145/2347736.2347755},
	abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
	number = {10},
	urldate = {2018-11-24},
	journal = {Commun. ACM},
	author = {Domingos, Pedro},
	month = oct,
	year = {2012},
	pages = {78--87},
	file = {Domingos - 2012 - A Few Useful Things to Know About Machine Learning.pdf:/home/ryan/Zotero/storage/82YGXT56/Domingos - 2012 - A Few Useful Things to Know About Machine Learning.pdf:application/pdf},
}

@inproceedings{wordvec1,
	series = {{HLT} '13},
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	year = {2013},
	keywords = {Language model, Word embedding},
	file = {Full Text PDF:/home/ryan/Zotero/storage/U6EW2LW9/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:application/pdf},
}

@inproceedings{caffe,
	address = {New York, NY, USA},
	series = {{MM} '14},
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	isbn = {978-1-4503-3063-3},
	shorttitle = {Caffe},
	url = {http://doi.acm.org/10.1145/2647868.2654889},
	doi = {10.1145/2647868.2654889},
	urldate = {2018-11-22},
	booktitle = {Proceedings of the {22Nd} {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = {2014},
	keywords = {machine learning, computer vision, neural networks, open source, parallel computation},
	pages = {675--678},
}

@article{helix,
	series = {{VLDB} '18},
	title = {Helix: {Accelerating} {Human}-in-the-loop {Machine} {Learning}},
	volume = {11},
	issn = {2150-8097},
	shorttitle = {Helix},
	url = {https://doi.org/10.14778/3229863.3236234},
	doi = {10.14778/3229863.3236234},
	number = {12},
	urldate = {2018-11-22},
	journal = {PVLDB},
	author = {Xin, Doris and Ma, Litian and Liu, Jialin and Macke, Stephen and Song, Shuchen and Parameswaran, Aditya},
	year = {2018},
	pages = {1958--1961},
	file = {Submitted Version:/home/ryan/Zotero/storage/EDQBXKU8/Xin et al. - 2018 - Helix Accelerating Human-in-the-loop Machine Lear.pdf:application/pdf},
}

@article{northstar,
	series = {{VLDB} '18},
	title = {Northstar: {An} {Interactive} {Data} {Science} {System}},
	volume = {11},
	issn = {2150-8097},
	shorttitle = {Northstar},
	url = {https://doi.org/10.14778/3229863.3240493},
	doi = {10.14778/3229863.3240493},
	number = {12},
	urldate = {2018-11-22},
	journal = {PVLDB},
	author = {Kraska, Tim},
	year = {2018},
	pages = {2150--2164},
	file = {Kraska - 2018 - Northstar An Interactive Data Science System.pdf:/home/ryan/Zotero/storage/EHRUQL5X/Kraska - 2018 - Northstar An Interactive Data Science System.pdf:application/pdf},
}

@article{ringtail,
	series = {{VLDB} '13},
	title = {Ringtail: {A} {Generalized} {Nowcasting} {System}},
	volume = {6},
	issn = {2150-8097},
	shorttitle = {Ringtail},
	url = {http://dx.doi.org/10.14778/2536274.2536315},
	doi = {10.14778/2536274.2536315},
	number = {12},
	urldate = {2018-11-22},
	journal = {PVLDB},
	author = {Antenucci, Dolan and Li, Erdong and Liu, Shaobo and Zhang, Bochun and Cafarella, Michael J. and R√©, Christopher},
	year = {2013},
	pages = {1358--1361},
	file = {Antenucci et al. - 2013 - Ringtail A Generalized Nowcasting System.pdf:/home/ryan/Zotero/storage/9NS8N5FA/Antenucci et al. - 2013 - Ringtail A Generalized Nowcasting System.pdf:application/pdf},
}

@inproceedings{brainwash,
	address = {Asilomar, California},
	series = {{CIDR} '13},
	title = {Brainwash: {A} {Data} {System} for {Feature} {Engineering}},
	shorttitle = {Brainwash},
	booktitle = {6th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Anderson, Michael and Antenucci, Dolan and Bittorf, Victor and Burgess, Matthew and Cafarella, Michael and Kumar, Arun and Niu, Feng and Park, Yongjoo and R√©, Christopher and Zhang, Ce},
	year = {2013},
	file = {Anderson et al. - 2013 - Brainwash A Data System for Feature Engineering.pdf:/home/ryan/Zotero/storage/SXU6HFHC/Anderson et al. - 2013 - Brainwash A Data System for Feature Engineering.pdf:application/pdf;Citeseer - Snapshot:/home/ryan/Zotero/storage/ILGQ338M/summary.html:text/html},
}

@article{tsne,
	series = {{JMLR} '08},
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
	number = {Nov},
	urldate = {2018-11-21},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:/home/ryan/Zotero/storage/CK5ZZL8B/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/BXLW6HG8/vandermaaten08a.html:text/html},
}

@inproceedings{zombie,
	title = {Input selection for fast feature engineering},
	doi = {10.1109/ICDE.2016.7498272},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Anderson, M. R. and Cafarella, M.},
	month = may,
	year = {2016},
	keywords = {machine learning, Big Data, learning (artificial intelligence), Training, Indexes, data handling, Data mining, Data processing, data processing task, fast feature engineering, Feature extraction, feature selection, input selection, Learning systems, Monitoring, supervised learning tasks, Zombie data-centric system},
	pages = {577--588},
	file = {Anderson and Cafarella - 2016 - Input selection for fast feature engineering.pdf:/home/ryan/Zotero/storage/URCF6GN7/Anderson and Cafarella - 2016 - Input selection for fast feature engineering.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/JB2JPTP7/7498272.html:text/html},
}

@article{layer_norm,
	series = {{arXiv} '16},
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	urldate = {2018-11-21},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1607.06450 PDF:/home/ryan/Zotero/storage/T8AAIAY6/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/XYUETSHX/1607.html:text/html},
}

@inproceedings{cyberrank,
	address = {New York, NY, USA},
	series = {{CIKM} '16},
	title = {{CyberRank}: {Knowledge} {Elicitation} for {Risk} {Assessment} of {Database} {Security}},
	isbn = {978-1-4503-4073-1},
	shorttitle = {{CyberRank}},
	url = {http://doi.acm.org/10.1145/2983323.2983896},
	doi = {10.1145/2983323.2983896},
	urldate = {2018-11-22},
	booktitle = {Proceedings of the 25th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Grushka - Cohen, Hagit and Sofer, Oded and Biller, Ofer and Shapira, Bracha and Rokach, Lior},
	year = {2016},
	keywords = {cold start, cyber security, preference elicitation, ranking, risk assessment, semi supervised},
	pages = {2009--2012},
	file = {Grushka - Cohen et al. - 2016 - CyberRank Knowledge Elicitation for Risk Assessme.pdf:/home/ryan/Zotero/storage/8HUSN6C9/Grushka - Cohen et al. - 2016 - CyberRank Knowledge Elicitation for Risk Assessme.pdf:application/pdf},
}

@article{knn,
	series = {Information {Theory} '67},
	title = {Nearest neighbor pattern classification},
	volume = {13},
	issn = {0018-9448},
	doi = {10.1109/TIT.1967.1053964},
	number = {1},
	journal = {IEEE Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	month = jan,
	year = {1967},
	keywords = {Pattern classification},
	pages = {21--27},
	file = {Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:/home/ryan/Zotero/storage/JY7ITDD6/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/QPD2KEV6/1053964.html:text/html},
}

@article{relight_msr,
	title = {Image {Based} {Relighting} {Using} {Neural} {Networks}},
	volume = {34},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/2766899},
	doi = {10.1145/2766899},
	number = {4},
	urldate = {2018-11-13},
	journal = {ACM Trans. Graph.},
	author = {Ren, Peiran and Dong, Yue and Lin, Stephen and Tong, Xin and Guo, Baining},
	month = jul,
	year = {2015},
	keywords = {neural network, clustering, image based relighting, light transport},
	pages = {111:1--111:12},
	file = {Ren et al. - 2015 - Image Based Relighting Using Neural Networks.pdf:/home/ryan/Zotero/storage/52STJP26/Ren et al. - 2015 - Image Based Relighting Using Neural Networks.pdf:application/pdf},
}

@article{DVN/QIIAPS_2018,
	title = {{JOB} {Queries} {Executed} by {PostgreSQL} 10.5},
	url = {https://doi.org/10.7910/DVN/QIIAPS},
	doi = {10.7910/DVN/QIIAPS},
	author = {Marcus, Ryan},
	year = {2018},
	note = {tex.publisher= Harvard Dataverse},
}

@article{rnn,
	series = {{arXiv} '15},
	title = {A {Critical} {Review} of {Recurrent} {Neural} {Networks} for {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1506.00019},
	urldate = {2018-10-05},
	journal = {arXiv:1506.00019 [cs]},
	author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
	month = may,
	year = {2015},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1506.00019 PDF:/home/ryan/Zotero/storage/FK8UTV9W/Lipton et al. - 2015 - A Critical Review of Recurrent Neural Networks for.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/M2LC3QQW/1506.html:text/html},
}

@article{opt_est_par,
	series = {{VLDB} '13},
	title = {Towards {Predicting} {Query} {Execution} {Time} for {Concurrent} and {Dynamic} {Database} {Workloads}},
	volume = {6},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2536206.2536219},
	doi = {10.14778/2536206.2536219},
	number = {10},
	urldate = {2018-10-05},
	journal = {PVLDB},
	author = {Wu, Wentao and Chi, Yun and Hac√≠g√ºm√º≈ü, Hakan and Naughton, Jeffrey F.},
	year = {2013},
	pages = {925--936},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/JG9WBNWE/Wu et al. - 2013 - Towards Predicting Query Execution Time for Concur.pdf:application/pdf},
}

@inproceedings{opt_est,
	address = {Washington, DC, USA},
	series = {{ICDE} '13},
	title = {Predicting {Query} {Execution} {Time}: {Are} {Optimizer} {Cost} {Models} {Really} {Unusable}?},
	isbn = {978-1-4673-4909-3},
	shorttitle = {Predicting {Query} {Execution} {Time}},
	url = {http://dx.doi.org/10.1109/ICDE.2013.6544899},
	doi = {10.1109/ICDE.2013.6544899},
	abstract = {Predicting query execution time is useful in many database management issues including admission control, query scheduling, progress monitoring, and system sizing. Recently the research community has been exploring the use of statistical machine learning approaches to build predictive models for this task. An implicit assumption behind this work is that the cost models used by query optimizers are insufficient for query execution time prediction. In this paper we challenge this assumption and show while the simple approach of scaling the optimizer's estimated cost indeed fails, a properly calibrated optimizer cost model is surprisingly effective. However, even a well-tuned optimizer cost model will fail in the presence of errors in cardinality estimates. Accordingly we investigate the novel idea of spending extra resources to refine estimates for the query plan after it has been chosen by the optimizer but before execution. In our experiments we find that a well calibrated query optimizer model along with cardinality estimation refinement provides a low overhead way to provide estimates that are always competitive and often much better than the best reported numbers from the machine learning approaches.},
	urldate = {2018-10-05},
	booktitle = {Proceedings of the 2013 {IEEE} {International} {Conference} on {Data} {Engineering} ({ICDE} 2013)},
	publisher = {IEEE Computer Society},
	author = {Wu, Wentao and Hacigumus, Hakan and Chi, Yun and Zhu, Shenghuo and Tatemura, Junichi and Naughton, Jeffrey F.},
	year = {2013},
	pages = {1081--1092},
	file = {Hacigumus et al. - 2013 - Predicting Query Execution Time Are Optimizer Cos.pdf:/home/ryan/Zotero/storage/JFUB7ZCU/Hacigumus et al. - 2013 - Predicting Query Execution Time Are Optimizer Cos.pdf:application/pdf},
}

@article{no_percent_error,
	series = {{JORS} '14},
	title = {A {Better} {Measure} of {Relative} {Prediction} {Accuracy} for {Model} {Selection} and {Model} {Estimation}},
	volume = {2015},
	url = {https://papers.ssrn.com/abstract=2635088},
	language = {en},
	number = {66},
	urldate = {2018-09-16},
	journal = {Journal of the Operational Research Society},
	author = {Tofallis, Chris},
	month = jul,
	year = {2014},
	pages = {1352--1362},
	file = {Full Text PDF:/home/ryan/Zotero/storage/NAM3TVMR/Tofallis - 2014 - A Better Measure of Relative Prediction Accuracy f.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/552IR6A4/papers.html:text/html},
}

@article{svm,
	series = {{ISA} '98},
	title = {Support vector machines},
	volume = {13},
	issn = {1094-7167},
	doi = {10.1109/5254.708428},
	number = {4},
	journal = {IEEE Intelligent Systems and their Applications},
	author = {Hearst, M. A. and Dumais, S. T. and Osuna, E. and Platt, J. and Scholkopf, B.},
	month = jul,
	year = {1998},
	keywords = {machine learning, Machine learning, Kernel, learning (artificial intelligence), Neural networks, Support vector machines, Algorithm design and analysis, Character recognition, computational learning theory, computational linguistics, face detection, face recognition, learning algorithms, Pattern recognition, Polynomials, real-world applications, Reuters collection, support vector machines, text categorization, Training data, Web pages},
	pages = {18--28},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/JCVT6PGK/708428.html:text/html},
}

@article{howgood,
	series = {{VLDB} '15},
	title = {How {Good} {Are} {Query} {Optimizers}, {Really}?},
	volume = {9},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2850583.2850594},
	doi = {10.14778/2850583.2850594},
	number = {3},
	urldate = {2018-02-27},
	journal = {PVLDB},
	author = {Leis, Viktor and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
	year = {2015},
	pages = {204--215},
	file = {Leis et al. - 2015 - How Good Are Query Optimizers, Really.pdf:/home/ryan/Zotero/storage/B44GEKI5/Leis et al. - 2015 - How Good Are Query Optimizers, Really.pdf:application/pdf},
}

@article{sgd,
	series = {{arXiv} '16},
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2017-11-07},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1609.04747 PDF:/home/ryan/Zotero/storage/74QFNUHG/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/CIKHHTE4/1609.html:text/html},
}

@article{tree_lstm,
	series = {{arXiv} '15},
	title = {Improved {Semantic} {Representations} {From} {Tree}-{Structured} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1503.00075},
	abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
	urldate = {2017-11-06},
	journal = {arXiv:1503.00075 [cs]},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	month = feb,
	year = {2015},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning},
	file = {arXiv\:1503.00075 PDF:/home/ryan/Zotero/storage/B5EVNA97/Tai et al. - 2015 - Improved Semantic Representations From Tree-Struct.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/8F7BSEGP/1503.html:text/html},
}

@article{scikit-learn,
	series = {{JMLR} '11},
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1532-4435},
	shorttitle = {Scikit-learn},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2078195},
	urldate = {2017-09-10},
	journal = {J. Mach. Learn. Res.},
	author = {Pedregosa, Fabian and Varoquaux, Ga√´l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, √âdouard},
	month = nov,
	year = {2011},
	pages = {2825--2830},
	file = {Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:/home/ryan/Zotero/storage/F89QTWTI/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@article{testing_mssql,
	series = {Data {Eng} '08},
	title = {Testing {SQL} {Server}'s {Query} {Optimizer}: {Challenges}, {Techniques} and {Experiences}},
	volume = {31},
	journal = {IEEE Data Eng. Bull.},
	author = {Giakoumakis, Leo and Galindo-Legaria, C√©sar A.},
	year = {2008},
	pages = {36--43},
}

@book{inside_mssql,
	title = {Inside the {SQL} {Server} {Query} {Optimizer}},
	isbn = {978-1-906434-60-1},
	abstract = {The SQL Server Query Optimizer is perceived by many to be a magic black box, transforming SQL queries into high performance execution plans in the blink of an eye through some unknowable process. The truth is that, while the Query Optimizer is indeed the highly-complex result of decades of research, learning how it works its magic is not only possible, but immensely useful to¬†database developers and administrators alike. A better understanding of what the Query Optimizer does behind the scenes can help you to improve the performance of your databases and applications, and this book explains the core concepts behind how the SQL Server Query Optimizer works. With this knowledge, you'll be able to write superior queries, provide the Query Optimizer with all the information it needs to produce efficient execution plans, and troubleshoot the cases when the Query Optimizer is not giving you the best plan possible. With over 15 years of experience in the use of Relational Databases (including SQL Server since version 6.5), Benjamin has watched the SQL Server¬†Relational Engine grow and evolve. His insight will leave you with an excellent foundation in the practicalities of the Query Optimizer, and everything you need to know to start tuning your queries to perfection.},
	language = {English},
	publisher = {Red Gate books},
	author = {Nevarez, Benjamin},
	month = mar,
	year = {2011},
	file = {Nevarez - 2011 - Inside the SQL Server Query Optimizer.pdf:/home/ryan/Zotero/storage/83VQLD4E/Nevarez - 2011 - Inside the SQL Server Query Optimizer.pdf:application/pdf},
}

@article{qppnet_long,
	series = {{arXiv} '19},
	title = {Plan-{Structured} {Deep} {Neural} {Network} {Models} for {Query} {Performance} {Prediction}},
	url = {http://arxiv.org/abs/1902.00132},
	urldate = {2019-02-20},
	journal = {arXiv:1902.00132 [cs]},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Databases},
	file = {arXiv\:1902.00132 PDF:/home/ryan/Zotero/storage/67KLBE8Q/Marcus and Papaemmanouil - 2019 - Plan-Structured Deep Neural Network Models for Que.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/WHZZWNIB/1902.html:text/html},
}

@article{inductive_bias_auto,
	series = {{JAIR} '00},
	title = {A {Model} of {Inductive} {Bias} {Learning}},
	volume = {12},
	issn = {1076-9757},
	url = {http://dl.acm.org/citation.cfm?id=1622248.1622254},
	abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
	number = {1},
	urldate = {2019-02-16},
	journal = {Journal of Artificial Intelligence Research},
	author = {Baxter, Jonathan},
	month = mar,
	year = {2000},
	pages = {149--198},
	file = {Baxter - 2000 - A Model of Inductive Bias Learning.pdf:/home/ryan/Zotero/storage/M3JM8Q7Z/Baxter - 2000 - A Model of Inductive Bias Learning.pdf:application/pdf},
}

@inproceedings{termite,
	series = {{aiDM} '19},
	title = {Termite: {A} {System} for {Tunneling} {Through} {Heterogeneous} {Data}},
	author = {Fernandez, Raul Castro and Madden, Samuel},
	year = {2019},
}

@article{skinnerdb,
	series = {{VLDB} '18},
	title = {{SkinnerDB}: {Regret}-bounded {Query} {Evaluation} via {Reinforcement} {Learning}},
	volume = {11},
	issn = {2150-8097},
	shorttitle = {{SkinnerDB}},
	url = {https://doi.org/10.14778/3229863.3236263},
	doi = {10.14778/3229863.3236263},
	number = {12},
	urldate = {2019-01-31},
	journal = {PVLDB},
	author = {Trummer, Immanuel and Moseley, Samuel and Maram, Deepak and Jo, Saehan and Antonakakis, Joseph},
	year = {2018},
	pages = {2074--2077},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/WSQYA4D5/Trummer et al. - 2018 - SkinnerDB Regret-bounded Query Evaluation via Rei.pdf:application/pdf},
}

@article{boosting,
	series = {Machine {Learning} '90},
	title = {The {Strength} of {Weak} {Learnability}},
	volume = {5},
	issn = {0885-6125},
	url = {https://doi.org/10.1023/A:1022648800760},
	doi = {10.1023/A:1022648800760},
	abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error Œµ.},
	number = {2},
	urldate = {2019-01-30},
	journal = {Machine Learning},
	author = {Schapire, Robert E.},
	month = jul,
	year = {1990},
	keywords = {Machine learning, learnability theory, learning from examples, PAC learning, polynomial-time identification},
	pages = {197--227},
	file = {Full Text:/home/ryan/Zotero/storage/9NC65EI4/Schapire - 1990 - The Strength of Weak Learnability.pdf:application/pdf},
}

@inproceedings{sagedb,
	series = {{CIDR} '19},
	title = {{SageDB}: {A} {Learned} {Database} {System}},
	booktitle = {9th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Kraska, Tim and Alizadeh, Mohammad and Beutel, Alex and {Ed Chi} and {Ani Kristo} and {Guillaume Leclerc} and {Samuel Madden} and {Hongzi Mao} and {Vikram Nathan}},
	year = {2019},
}

@inproceedings{sql_embed,
	series = {{CIDR} '19},
	title = {Database-{Agnostic} {Workload} {Management}},
	booktitle = {9th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {{Shrainik Jain} and {Jiaqi Yan} and {Thiery Cruanes} and {Bill Howe}},
	year = {2019},
	file = {Shrainik Jain et al. - 2019 - Database-Agnostic Workload Management.pdf:/home/ryan/Zotero/storage/553TCBLH/Shrainik Jain et al. - 2019 - Database-Agnostic Workload Management.pdf:application/pdf},
}

@article{svm_rbf,
	series = {{TOSP} '97},
	title = {Comparing support vector machines with {Gaussian} kernels to radial basis function classifiers},
	volume = {45},
	issn = {1053-587X},
	doi = {10.1109/78.650102},
	abstract = {The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Scholkopf, B. and Sung, Kah-Kay and Burges, C. J. C. and Girosi, F. and Niyogi, P. and Poggio, T. and Vapnik, V.},
	month = nov,
	year = {1997},
	keywords = {Machine learning, Kernel, Neural networks, Support vector machines, Polynomials, support vector machines, backpropagation, Backpropagation algorithms, Clustering algorithms, error backpropagation, expected test error, feedforward neural nets, Gaussian kernels, Gaussian processes, handwritten digits, hybrid system, image classification, learning machine, polynomial classifiers, polynomials, radial basis function classifiers, RBF case, recognition accuracy, statistical analysis, Statistical learning, statistical learning theory, Support vector machine classification, SV algorithm, United States postal service database, Upper bound, X-means clustering},
	pages = {2758--2765},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/UTEA9BEA/650102.html:text/html;Scholkopf et al. - 1997 - Comparing support vector machines with Gaussian ke.pdf:/home/ryan/Zotero/storage/KELUED79/Scholkopf et al. - 1997 - Comparing support vector machines with Gaussian ke.pdf:application/pdf},
}

@article{momentum,
	series = {{NN} '99},
	title = {On the {Momentum} {Term} in {Gradient} {Descent} {Learning} {Algorithms}},
	volume = {12},
	issn = {0893-6080},
	url = {http://dx.doi.org/10.1016/S0893-6080(98)00116-6},
	doi = {10.1016/S0893-6080(98)00116-6},
	number = {1},
	urldate = {2018-12-19},
	journal = {Neural Networks},
	author = {Qian, Ning},
	month = jan,
	year = {1999},
	keywords = {critical damping, damped harmonic oscillator, gradient descent learning algorithm, learning rate, momentum, speed of convergence},
	pages = {145--151},
	file = {Qian - 1999 - On the Momentum Term in Gradient Descent Learning .pdf:/home/ryan/Zotero/storage/GCRLTCZ9/Qian - 1999 - On the Momentum Term in Gradient Descent Learning .pdf:application/pdf;Submitted Version:/home/ryan/Zotero/storage/V9C2MB67/Qian - 1999 - On the Momentum Term in Gradient Descent Learning .pdf:application/pdf},
}

@inproceedings{wordvec3,
	address = {New Orleans, Louisiana},
	series = {{NAACL} '18},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {http://aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year = {2018},
	pages = {2227--2237},
}

@book{logistic,
	title = {Applied {Logistic} {Regression}},
	isbn = {978-0-470-58247-3},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Hosmer Jr, David W and Lemeshow, Stanley and Sturdivant, Rodney X.},
	month = apr,
	year = {2013},
	keywords = {Mathematics / Probability \& Statistics / Regression Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Medical / Biostatistics, Medical / Public Health},
}

@inproceedings{cidr_dlqo,
	series = {{CIDR} '19},
	title = {Towards a {Hands}-{Free} {Query} {Optimizer} through {Deep} {Learning}},
	copyright = {All rights reserved},
	booktitle = {9th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	year = {2019},
	file = {Marcus and Papaemmanouil - Towards a Hands-Free Query Optimizer through Deep .pdf:/home/ryan/Zotero/storage/RQUDH9S7/Marcus and Papaemmanouil - Towards a Hands-Free Query Optimizer through Deep .pdf:application/pdf},
}

@article{evo_nn,
	series = {{arXiv} '17},
	title = {Evolving {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.00548},
	urldate = {2018-12-16},
	journal = {arXiv:1703.00548 [cs]},
	author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00548},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1703.00548 PDF:/home/ryan/Zotero/storage/47GHM6FW/Miikkulainen et al. - 2017 - Evolving Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/PZBS957Z/1703.html:text/html},
}

@inproceedings{rl_arch,
	series = {{ICLR} '17},
	title = {Designing {Neural} {Network} {Architectures} using {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.02167},
	urldate = {2018-12-16},
	booktitle = {International {Conference} on {Algorithmic} {Learning} {Theory}},
	author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1611.02167 PDF:/home/ryan/Zotero/storage/FD9TC4LM/Baker et al. - 2016 - Designing Neural Network Architectures using Reinf.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/JZMUEYYB/1611.html:text/html},
}

@article{dbsafe,
	series = {Systems '17},
	title = {{DBSAFE}‚Äî{An} {Anomaly} {Detection} {System} to {Protect} {Databases} {From} {Exfiltration} {Attempts}},
	volume = {11},
	issn = {1932-8184},
	doi = {10.1109/JSYST.2015.2487221},
	number = {2},
	journal = {IEEE Systems Journal},
	author = {{Asmaa Sallam} and {Elisa Bertino} and {Syed Rafiul Hussain} and {David Landers} and {R Michael Lefler} and {Donald Steiner}},
	month = jun,
	year = {2017},
	pages = {483--493},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/B4GBB3QJ/7307113.html:text/html;Sallam et al. - 2017 - DBSAFE‚ÄîAn Anomaly Detection System to Protect Data.pdf:/home/ryan/Zotero/storage/MRG4AZBK/Sallam et al. - 2017 - DBSAFE‚ÄîAn Anomaly Detection System to Protect Data.pdf:application/pdf},
}

@article{deep_survey,
	series = {Neurocomputing '17},
	title = {A survey of deep neural network architectures and their applications},
	volume = {234},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231216315533},
	doi = {10.1016/j.neucom.2016.12.038},
	urldate = {2018-09-20},
	journal = {Neurocomputing},
	author = {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E.},
	month = apr,
	year = {2017},
	keywords = {Deep learning, Autoencoder, Convolutional neural network, Deep belief network, Restricted Boltzmann machine},
	pages = {11--26},
	file = {ScienceDirect Full Text PDF:/home/ryan/Zotero/storage/UD2L4LX3/Liu et al. - 2017 - A survey of deep neural network architectures and .pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/ASPJBMHF/S0925231216315533.html:text/html},
}

@article{sanjay_wat,
	series = {{arXiv} '18},
	title = {Learning to {Optimize} {Join} {Queries} {With} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1808.03196},
	language = {en},
	urldate = {2018-09-11},
	journal = {arXiv:1808.03196 [cs]},
	author = {Krishnan, Sanjay and Yang, Zongheng and Goldberg, Ken and Hellerstein, Joseph and Stoica, Ion},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.03196},
	keywords = {Computer Science - Databases},
	file = {Krishnan et al. - 2018 - Learning to Optimize Join Queries With Deep Reinfo.pdf:/home/ryan/Zotero/storage/ZMNBEA47/Krishnan et al. - 2018 - Learning to Optimize Join Queries With Deep Reinfo.pdf:application/pdf},
}

@article{rf,
	series = {Machine {Learning} '01},
	title = {Random {Forests}},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	pages = {5--32},
	file = {Breiman - 2001 - Random Forests.pdf:/home/ryan/Zotero/storage/M88HIWBN/Breiman - 2001 - Random Forests.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/MPLRGVMV/10.html:text/html},
}

@inproceedings{msr_paper,
	series = {{SIGMOD} '19},
	title = {{AI} {Meets} {AI}: {Leveraging} {Query} {Executions} to {Improve} {Index} {Recommendations}},
	copyright = {All rights reserved},
	doi = {10.1145/3299869.3324957},
	booktitle = {38th {ACM} {Special} {Interest} {Group} in {Data} {Management}},
	author = {Ding, Bailu and Das, Sudipto and Marcus, Ryan and Wu, Wentao and Chaudhuri, Surajit and Narasayya, Vivek R.},
	year = {2019},
	file = {Bailu Ding et al. - 2019 - AI Meets AI Leveraging Query Ex-ecutions to Impro.pdf:/home/ryan/Zotero/storage/5WH37SRA/Bailu Ding et al. - 2019 - AI Meets AI Leveraging Query Ex-ecutions to Impro.pdf:application/pdf},
}

@inproceedings{softdefsoft,
	series = {{ICDCS} '18},
	title = {Software-{Defined} {Software}: {A} {Perspective} of {Machine} {Learning}-{Based} {Software} {Production}},
	shorttitle = {Software-{Defined} {Software}},
	doi = {10.1109/ICDCS.2018.00126},
	abstract = {As the Moore's Law is ending, and increasingly high demand of software development continues in the human society, we are facing two serious challenges in the computing field. First, the general-purpose computing ecosystem that has been developed for more than 50 years will have to be changed by including many diverse devices for various specialties in high performance. Second, human-based software development is not sustainable to respond the requests from all the fields in the society. We envision that we will enter a time of developing high quality software by machines, and we name this as Software-defined Software (SDS). In this paper, we will elaborate our vision, the goals and its roadmap.},
	booktitle = {2018 {IEEE} 38th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Lee, R. and Wang, H. and Zhang, X.},
	month = jul,
	year = {2018},
	keywords = {machine learning, Optimization, Deep Learning, learning (artificial intelligence), Automatic programming, general purpose computers, general-purpose computing, Hardware, human-based software development, Libraries, Machine Learning, Production, Programming, SDS, Software, software development management, software production, software quality, Software-defined Software},
	pages = {1270--1275},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/D2PZ2HUA/8416389.html:text/html;Lee et al. - 2018 - Software-Defined Software A Perspective of Machin.pdf:/home/ryan/Zotero/storage/87QS3XM7/Lee et al. - 2018 - Software-Defined Software A Perspective of Machin.pdf:application/pdf},
}

@article{url-amazonS3,
	title = {Amazon {S3}, https://aws.amazon.com/s3/},
	url = {http://aws.amazon.com/},
	note = {tex.key= 1},
}

@inproceedings{sloorchestrator,
	series = {{USENIX} '18},
	title = {{SLAOrchestrator}: {Reducing} the {Cost} of {Performance} {SLAs} for {Cloud} {Data} {Analytics}},
	isbn = {978-1-931971-44-7},
	url = {https://www.usenix.org/conference/atc18/presentation/ortiz},
	booktitle = {2018 {USENIX} {Annual} {Technical} {Conference} ({USENIX} {ATC} 18)},
	publisher = {USENIX Association},
	author = {Ortiz, Jennifer and Lee, Brendan and Balazinska, Magdalena and Gehrke, Johannes and Hellerstein, Joseph L.},
	year = {2018},
	pages = {547--560},
	file = {Ortiz et al. - 2018 - SLAOrchestrator Reducing the Cost of Performance .pdf:/home/ryan/Zotero/storage/UHMDRPDK/Ortiz et al. - 2018 - SLAOrchestrator Reducing the Cost of Performance .pdf:application/pdf},
}

@inproceedings{alexnet,
	address = {USA},
	series = {{NIPS} '12},
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	urldate = {2019-03-01},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105},
}

@inproceedings{cnn_seg,
	series = {{CVPR} '15},
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = jun,
	year = {2015},
}

@article{bellman,
	series = {{IU} {Math} '57},
	title = {A {Markovian} {Decision} {Process}},
	volume = {6},
	issn = {0022-2518},
	number = {4},
	journal = {Indiana University Mathematics Journal},
	author = {Bellman, Richard},
	year = {1957},
	pages = {679--684},
}

@book{rl_book,
	address = {Cambridge, MA, USA},
	edition = {1st},
	title = {Introduction to {Reinforcement} {Learning}},
	isbn = {978-0-262-19398-6},
	abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
}

@inproceedings{retrofit_wv,
	series = {{NAACL} '15},
	title = {Retrofitting {Word} {Vectors} to {Semantic} {Lexicons}},
	url = {http://aclweb.org/anthology/N/N15/N15-1184.pdf},
	booktitle = {The 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay Kumar and Dyer, Chris and Hovy, Eduard H. and Smith, Noah A.},
	year = {2015},
	pages = {1606--1615},
}

@inproceedings{gensim,
	series = {{LREC} '10},
	title = {Software {Framework} for {Topic} {Modelling} with {Large} {Corpora}},
	language = {English},
	booktitle = {Proceedings of the {LREC} 2010 {Workshop} on {New} {Challenges} for {NLP} {Frameworks}},
	publisher = {ELRA},
	author = {≈òeh≈Ø≈ôek, Radim and Sojka, Petr},
	month = may,
	year = {2010},
	pages = {45--50},
}

@article{refine_wv,
	series = {{TASLP} '18},
	title = {Refining {Word} {Embeddings} {Using} {Intensity} {Scores} for {Sentiment} {Analysis}},
	volume = {26},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2017.2788182},
	abstract = {Word embeddings that provide continuous low-dimensional vector representations of words have been extensively used for various natural language processing tasks. However, existing context-based word embeddings such as Word2vec and GloVe typically fail to capture sufficient sentiment information, which may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. To tackle this problem, recent studies have suggested learning sentiment embeddings to incorporate the sentiment polarity (positive and negative) information from labeled corpora. This study adopts another strategy to learn sentiment embeddings. Instead of creating a new word embedding from labeled corpora, we propose a word vector refinement model to refine existing pretrained word vectors using real-valued sentiment intensity scores provided by sentiment lexicons. The idea of the refinement model is to improve each word vector such that it can be closer in the lexicon to both semantically and sentimentally similar words (i.e., those with similar intensity scores) and further away from sentimentally dissimilar words (i.e., those with dissimilar intensity scores). An obvious advantage of the proposed method is that it can be applied to any pretrained word embeddings. In addition, the intensity scores can provide more fine-grained (real-valued) sentiment information than binary polarity labels to guide the refinement process. Experimental results show that the proposed refinement model can improve both conventional word embeddings and previously proposed sentiment embeddings for binary, ternary, and fine-grained sentiment classification on the SemEval and Stanford Sentiment Treebank datasets.},
	number = {3},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Yu, L. and Wang, J. and Lai, K. R. and Zhang, X.},
	month = mar,
	year = {2018},
	keywords = {Semantics, Neural networks, Speech, Context modeling, dissimilar intensity scores, fine-grained sentiment classification, labeled corpora, low-dimensional vector representations, natural language processing tasks, opposite sentiment polarity, pretrained word embeddings, pretrained word vectors, refinement process, sentiment analysis, Sentiment analysis, sentiment embeddings, sentiment intensity scores, sentiment lexicons, sentimentally dissimilar words, Speech processing, sufficient sentiment information, word embeddings, word vector refinement, word vector refinement model, Word2vec},
	pages = {671--681},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/HUNKQLMR/8241844.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/DQNQPF4I/Yu et al. - 2018 - Refining Word Embeddings Using Intensity Scores fo.pdf:application/pdf},
}

@inproceedings{rl_search,
	series = {{NIPS} '17},
	title = {Thinking {Fast} and {Slow} with {Deep} {Learning} and {Tree} {Search}},
	url = {http://papers.nips.cc/paper/7120-thinking-fast-and-slow-with-deep-learning-and-tree-search},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	author = {Anthony, Thomas and Tian, Zheng and Barber, David},
	year = {2017},
	pages = {5366--5376},
	file = {Full Text PDF:/home/ryan/Zotero/storage/JQLME8SD/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf:application/pdf},
}

@inproceedings{vertica_opt,
	series = {{ICDE} '14},
	title = {The {Vertica} {Query} {Optimizer}: {The} case for specialized query optimizers},
	shorttitle = {The {Vertica} {Query} {Optimizer}},
	doi = {10.1109/ICDE.2014.6816727},
	abstract = {The Vertica SQL Query Optimizer was written from the ground up for the Vertica Analytic Database. Its design and the tradeoffs we encountered during its implementation argue that the full power of novel database systems can only be realized with a carefully crafted custom Query Optimizer written specifically for the system in which it operates.},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	author = {Tran, N. and Lamb, A. and Shrinivas, L. and Bodagala, S. and Dave, J.},
	month = mar,
	year = {2014},
	keywords = {Encoding, Optimization, query processing, Data models, Engines, Indexes, SQL, Dictionaries, novel database systems, specialized query optimizers, vertica analytic database, vertica SQL query optimizer},
	pages = {1108--1119},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/68ZTPC7W/6816727.html:text/html},
}

@article{anytime,
	series = {{AAAI} '96},
	title = {Using {Anytime} {Algorithms} in {Intelligent} {Systems}},
	volume = {17},
	issn = {2371-9621},
	url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1232},
	doi = {10.1609/aimag.v17i3.1232},
	language = {en},
	number = {3},
	urldate = {2019-02-25},
	journal = {AI Magazine},
	author = {Zilberstein, Shlomo},
	month = mar,
	year = {1996},
	pages = {73--73},
	file = {Full Text PDF:/home/ryan/Zotero/storage/4UF49Z5I/Zilberstein - 1996 - Using Anytime Algorithms in Intelligent Systems.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/SJY8D3BD/1232.html:text/html},
}

@article{best_first,
	series = {{ACM} '85},
	title = {Generalized {Best}-first {Search} {Strategies} and the {Optimality} of {A}*},
	volume = {32},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/3828.3830},
	doi = {10.1145/3828.3830},
	abstract = {This paper reports several properties of heuristic best-first search strategies whose scoring functions ∆í depend on all the information available from each candidate path, not merely on the current cost g and the estimated completion cost h. It is shown that several known properties of A* retain their form (with the minmax of f playing the role of the optimal cost), which helps establish general tests of admissibility and general conditions for node expansion for these strategies. On the basis of this framework the computational optimality of A*, in the sense of never expanding a node that can be skipped by some other algorithm having access to the same heuristic information that A* uses, is examined. A hierarchy of four optimality types is defined and three classes of algorithms and four domains of problem instances are considered. Computational performances relative to these algorithms and domains are appraised. For each class-domain combination, we then identify the strongest type of optimality that exists and the algorithm for achieving it. The main results of this paper relate to the class of algorithms that, like A*, return optimal solutions (i.e., admissible) when all cost estimates are optimistic (i.e., h ‚â§ h*). On this class, A* is shown to be not optimal and it is also shown that no optimal algorithm exists, but if the performance tests are confirmed to cases in which the estimates are also consistent, then A* is indeed optimal. Additionally, A* is also shown to be optimal over a subset of the latter class containing all best-first algorithms that are guided by path-dependent evaluation functions.},
	number = {3},
	urldate = {2019-02-25},
	journal = {J. ACM},
	author = {Dechter, Rina and Pearl, Judea},
	month = jul,
	year = {1985},
	pages = {505--536},
	file = {Dechter and Pearl - 1985 - Generalized Best-first Search Strategies and the O.pdf:/home/ryan/Zotero/storage/UL5UPACK/Dechter and Pearl - 1985 - Generalized Best-first Search Strategies and the O.pdf:application/pdf},
}

@article{op_embed,
	series = {{arXiv} '19},
	title = {Flexible {Operator} {Embeddings} via {Deep} {Learning}},
	url = {http://arxiv.org/abs/1901.09090},
	abstract = {Integrating machine learning into the internals of database management systems requires significant feature engineering, a human effort-intensive process to determine the best way to represent the pieces of information that are relevant to a task. In addition to being labor intensive, the process of hand-engineering features must generally be repeated for each data management task, and may make assumptions about the underlying database that are not universally true. We introduce flexible operator embeddings, a deep learning technique for automatically transforming query operators into feature vectors that are useful for a multiple data management tasks and is custom-tailored to the underlying database. Our approach works by taking advantage of an operator's context, resulting in a neural network that quickly transforms sparse representations of query operators into dense, information-rich feature vectors. Experimentally, we show that our flexible operator embeddings perform well across a number of data management tasks, using both synthetic and real-world datasets.},
	urldate = {2019-02-25},
	journal = {arXiv:1901.09090 [cs]},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases},
	file = {arXiv\:1901.09090 PDF:/home/ryan/Zotero/storage/JM6Y83KJ/Marcus and Papaemmanouil - 2019 - Flexible Operator Embeddings via Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/T7F327ZK/1901.html:text/html},
}

@inproceedings{cords,
	series = {{SIGMOD} '04},
	title = {{CORDS}: {Automatic} {Discovery} of {Correlations} and {Soft} {Functional} {Dependencies}},
	booktitle = {{ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Ilyas, Ihab F. and Markl, Volker and Haas, Peter and Brown, Paul and Aboulnaga, Ashraf},
	year = {2004},
	pages = {647--658},
}

@inproceedings{qo_unsolved,
	series = {{ACM} {Blog} '14},
	title = {Is {Query} {Optimization} a "{Solved}" {Problem}?},
	url = {https://wp.sigmod.org/?p=1075},
	booktitle = {{ACM} {SIGMOD} {Blog}},
	author = {Lohman, Guy},
	year = {2014},
}

@inproceedings{overview98,
	series = {{SIGMOD} '98},
	title = {An {Overview} of {Query} {Optimization} in {Relational} {Systems}},
	booktitle = {{ACM} {SIGMOD} {Symposium} on {Principles} of {Database} {Systems}},
	author = {Chaudhuri, Surajit},
	year = {1998},
	pages = {34--43},
}

@inproceedings{column_embed,
	series = {{CIDR} '19},
	title = {Exploiting {Latent} {Information} in {Relational} {Databases} via {Word} {Embedding} and {Application} to {Degrees} of {Disclosure}},
	booktitle = {Biennial {Conference} on {Innovative} {Data} {Systems} {Research} ({CIDR})},
	author = {Bordawekar, Rajesh and Shmueli, Oded},
	year = {2019},
}

@article{quicksel,
	title = {{QuickSel}: {Quick} {Selectivity} {Learning} with {Mixture} {Models}},
	shorttitle = {{QuickSel}},
	url = {http://arxiv.org/abs/1812.10568},
	abstract = {Estimating the selectivity of a query is a key step in almost any cost-based query optimizer. Most of today's databases rely on histograms or samples that are periodically refreshed by re-scanning the data as the underlying data changes. Since frequent scans are costly, these statistics are often stale and lead to poor selectivity estimates. As an alternative to scans, query-driven histograms have been proposed, which refine the histograms based on the actual selectivities of the observed queries. Unfortunately, these approaches are either too costly to use in practice---i.e., require an exponential number of buckets---or quickly lose their advantage as they observe more queries. For example, the state-of-the-art technique requires 318,936 buckets (and over 8 seconds of refinement overhead per query) after observing only 300 queries. In this paper, we propose a selectivity learning framework, called QuickSel, which falls into the query-driven paradigm but does not use histograms. Instead, it builds an internal model of the underlying data, which can be refined significantly faster (e.g., only 1.9 milliseconds for 300 queries). This fast refinement allows QuickSel to continuously learn from each query and yield increasingly more accurate selectivity estimates over time. Unlike query-driven histograms, QuickSel relies on a mixture model and a new optimization algorithm for training its model. Our extensive experiments on two real-world datasets confirm that, given the same target accuracy, QuickSel is on average 254.6x faster than state-of-the-art query-driven histograms, including ISOMER and STHoles. Further, given the same space budget, QuickSel is on average 57.3\% and 91.1\% more accurate than periodically-updated histograms and samples, respectively.},
	urldate = {2019-02-25},
	journal = {arXiv:1812.10568 [cs]},
	author = {Park, Yongjoo and Zhong, Shucheng and Mozafari, Barzan},
	month = dec,
	year = {2018},
	keywords = {Computer Science - Databases},
	file = {arXiv\:1812.10568 PDF:/home/ryan/Zotero/storage/Z25RZ658/Park et al. - 2018 - QuickSel Quick Selectivity Learning with Mixture .pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/LPQB4DLP/1812.html:text/html},
}

@inproceedings{bicyclegan,
	series = {{NIPS} '17},
	title = {Toward {Multimodal} {Image}-to-{Image} {Translation}},
	url = {http://papers.nips.cc/paper/6650-toward-multimodal-image-to-image-translation.pdf},
	urldate = {2019-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {465--476},
	file = {NIPS Full Text PDF:/home/ryan/Zotero/storage/IXG76VWB/Zhu et al. - 2017 - Toward Multimodal Image-to-Image Translation.pdf:application/pdf;NIPS Snapshot:/home/ryan/Zotero/storage/VV9NRSX5/6650-toward-multimodal-image-to-image-translation.html:text/html},
}

@article{alphago,
	series = {Nature '16},
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2019-02-24},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Full Text:/home/ryan/Zotero/storage/LE6VUP2T/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@article{dewitt_clause,
	series = {Review of {Litigation} '06},
	title = {{DeWitt} clauses: can we protect purchasers without hurting {Microsoft}},
	volume = {25},
	shorttitle = {{DeWitt} {Clauses}},
	journal = {Rev. Litig.},
	author = {Read, Anthony G},
	year = {2006},
	pages = {387},
	file = {Read - 2006 - DeWitt clauses can we protect purchasers without .pdf:/home/ryan/Zotero/storage/X25AL8R2/Read - 2006 - DeWitt clauses can we protect purchasers without .pdf:application/pdf},
}

@article{li_robust,
	series = {{VLDB} '12},
	title = {Robust estimation of resource consumption for {SQL} queries using statistical techniques},
	volume = {5},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2350229.2350269},
	doi = {10.14778/2350229.2350269},
	language = {en},
	number = {11},
	urldate = {2017-11-06},
	journal = {PVLDB},
	author = {Li, Jiexing and K√∂nig, Arnd Christian and Narasayya, Vivek and Chaudhuri, Surajit},
	year = {2012},
	pages = {1555--1566},
	file = {Li et al. - 2012 - Robust estimation of resource consumption for SQL .pdf:/home/ryan/Zotero/storage/PEKTWZJ8/Li et al. - 2012 - Robust estimation of resource consumption for SQL .pdf:application/pdf},
}

@article{url-sqlite,
	title = {{SQLite} database, https://www.sqlite.org},
	url = {https://www.sqlite.org},
	note = {tex.key= 1},
}

@article{shared_weights,
	title = {Convolutional networks for images, speech, and time series},
	url = {http://dl.acm.org/citation.cfm?id=303568.303704},
	urldate = {2019-05-13},
	journal = {The Handbook of Brain Theory and Neural Networks},
	author = {LeCun, Yann and Bengio, Yoshua},
	editor = {Arbib, Michael A.},
	year = {1998},
	pages = {255--258},
}

@article{url-mysql,
	title = {{MySQL} database, https://www.mysql.com/},
	url = {https://www.mysql.com/},
	note = {tex.key= 1},
}

@article{hyper,
	series = {{VLDB} '11},
	title = {Efficiently {Compiling} {Efficient} {Query} {Plans} for {Modern} {Hardware}},
	volume = {4},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2002938.2002940},
	doi = {10.14778/2002938.2002940},
	number = {9},
	urldate = {2019-05-13},
	journal = {PVLDB},
	author = {Neumann, Thomas},
	year = {2011},
	pages = {539--550},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/KQZ7N7PM/Neumann - 2011 - Efficiently Compiling Efficient Query Plans for Mo.pdf:application/pdf},
}

@article{infinite-horizon,
	series = {{JAIR} '01},
	title = {Infinite-{Horizon} {Policy}-{Gradient} {Estimation}},
	volume = {15},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10289},
	doi = {10.1613/jair.806},
	urldate = {2019-04-23},
	journal = {Journal of Artificial Intelligence Research},
	author = {Baxter, J. and Bartlett, P. L.},
	month = nov,
	year = {2001},
	pages = {319--350},
	file = {Baxter and Bartlett - 2001 - Infinite-Horizon Policy-Gradient Estimation.pdf:/home/ryan/Zotero/storage/NLWHHAML/Baxter and Bartlett - 2001 - Infinite-Horizon Policy-Gradient Estimation.pdf:application/pdf},
}

@inproceedings{region-selection,
	series = {{INFOCOM} '12},
	title = {Network aware resource allocation in distributed clouds},
	doi = {10.1109/INFCOM.2012.6195847},
	booktitle = {2012 {Proceedings} {IEEE} {INFOCOM}},
	author = {Alicherry, M. and Lakshman, T. V.},
	month = mar,
	year = {2012},
	keywords = {cloud computing, Switches, virtual machines, 2-approximation algorithm, approximation theory, bandwidth cost reduction, cloud computing resource deployment, communication cost minimization, communication latency minimization, computer centres, distributed cloud systems, network aware resource allocation, optimal data center selection, rack selection, resource allocation, resource needs, server selection, wide area network},
	pages = {963--971},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/LIA92MBP/6195847.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/8AHCIA2N/Alicherry and Lakshman - 2012 - Network aware resource allocation in distributed c.pdf:application/pdf},
}

@article{job2,
	series = {{VLDBJ} '18},
	title = {Query optimization through the looking glass, and what we found running the {Join} {Order} {Benchmark}},
	issn = {1066-8888, 0949-877X},
	url = {https://link.springer.com/article/10.1007/s00778-017-0480-7},
	doi = {10.1007/s00778-017-0480-7},
	language = {en},
	urldate = {2018-03-04},
	journal = {The VLDB Journal},
	author = {Leis, Viktor and Radke, Bernhard and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
	year = {2017},
	pages = {1--26},
	file = {Leis et al. - 2017 - Query optimization through the looking glass, and .pdf:/home/ryan/Zotero/storage/K928J2MD/Leis et al. - 2017 - Query optimization through the looking glass, and .pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/BL6GGADK/s00778-017-0480-7.html:text/html},
}

@inproceedings{exodus,
	address = {New York, NY, USA},
	series = {{SIGMOD} '87},
	title = {The {EXODUS} {Optimizer} {Generator}},
	isbn = {978-0-89791-236-5},
	url = {http://doi.acm.org/10.1145/38713.38734},
	doi = {10.1145/38713.38734},
	abstract = {This paper presents the design and an initial performance evaluation of the query optimizer generator designed for the EXODUS extensible database system. Algebraic transformation rules are translated into an executable query optimizer, which transforms query trees and selects methods for executing operations according to cost functions associated with the methods. The search strategy avoids exhaustive search and it modifies itself to take advantage of past experience. Computational results show that an optimizer generated for a relational system produces access plans almost as good as those produced by exhaustive search, with the search time cut to a small fraction.},
	urldate = {2018-03-04},
	booktitle = {Proceedings of the 1987 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Graefe, Goetz and DeWitt, David J.},
	year = {1987},
	pages = {160--172},
	file = {sigmod87.pdf:/home/ryan/Zotero/storage/SBSHAH75/sigmod87.pdf:application/pdf},
}

@inproceedings{opt_rules,
	address = {New York, NY, USA},
	series = {{SIGMOD} '96},
	title = {Rule {Languages} and {Internal} {Algebras} for {Rule}-based {Optimizers}},
	isbn = {978-0-89791-794-0},
	url = {http://doi.acm.org/10.1145/233269.233356},
	doi = {10.1145/233269.233356},
	abstract = {Rule-based optimizers and optimizer generators use rules to specify query transformations. Rules act directly on query representations, which typically are based on query algebras. But most algebras complicate rule formulation, and rules over these algebras must often resort to calling to externally defined bodies of code. Code makes rules difficult to formulate, prove correct and reason about, and therefore compromises the effectiveness of rule-based systems.In this paper we present KOLA: a combinator-based algebra designed to simplify rule formulation. KOLA is not a user language, and KOLA's variable-free queries are difficult for humans to read. But KOLA is an effective internal algebra because its combinator-style makes queries manipulable and structurally revealing. As a result, rules over KOLA queries are easily expressed without the need for supplemental code. We illustrate this point, first by showing some transformations that despite their simplicity, require head and body routines when expressed over algebras that include variables. We show that these transformations are expressible without supplemental routines in KOLA. We then show complex transformations of a class of nested queries expressed over KOLA. Nested query optimization, while having been studied before, have seriously challenged the rule-based paradigm.},
	urldate = {2018-03-04},
	booktitle = {Proceedings of the 1996 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Cherniack, Mitch and Zdonik, Stanley B.},
	year = {1996},
	pages = {401--412},
	file = {Cherniack-1996-RLI.pdf:/home/ryan/Zotero/storage/EXAK47UX/Cherniack-1996-RLI.pdf:application/pdf},
}

@inproceedings{relu,
	address = {Fort Lauderdale, FL, USA},
	series = {{PMLR} '11},
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	volume = {15},
	url = {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	editor = {Gordon, Geoffrey and Dunson, David and Dud√≠k, Miroslav},
	month = apr,
	year = {2011},
	pages = {315--323},
}

@article{perfenforce,
	series = {{arXiv} '16},
	title = {{PerfEnforce}: {A} {Dynamic} {Scaling} {Engine} for {Analytics} with {Performance} {Guarantees}},
	shorttitle = {{PerfEnforce}},
	url = {http://arxiv.org/abs/1605.09753},
	abstract = {In this paper, we present PerfEnforce, a scaling engine designed to enable cloud providers to sell performance levels for data analytics cloud services. PerfEnforce scales a cluster of virtual machines allocated to a user in a way that minimizes cost while probabilistically meeting the query runtime guarantees offered by a service level agreement. With PerfEnforce, we show how to scale a cluster in a way that minimally disrupts a user's query session. We further show when to scale the cluster using one of three methods: feedback control, reinforcement learning, or perceptron learning. We find that perceptron learning outperforms the other two methods when making cluster scaling decisions.},
	urldate = {2018-02-27},
	journal = {arXiv:1605.09753 [cs]},
	author = {Ortiz, Jennifer and Lee, Brendan and Balazinska, Magdalena and Hellerstein, Joseph L.},
	month = may,
	year = {2016},
	note = {arXiv: 1605.09753},
	keywords = {Computer Science - Databases},
	file = {arXiv\:1605.09753 PDF:/home/ryan/Zotero/storage/XW6QRB4K/Ortiz et al. - 2016 - PerfEnforce A Dynamic Scaling Engine for Analytic.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/YUKC6CR4/1605.html:text/html},
}

@inproceedings{ml_tuning,
	address = {New York, NY, USA},
	series = {{SIGMOD} '17},
	title = {Automatic {Database} {Management} {System} {Tuning} {Through} {Large}-scale {Machine} {Learning}},
	isbn = {978-1-4503-4197-4},
	url = {http://doi.acm.org/10.1145/3035918.3064029},
	doi = {10.1145/3035918.3064029},
	urldate = {2018-02-27},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Van Aken, Dana and Pavlo, Andrew and Gordon, Geoffrey J. and Zhang, Bohan},
	year = {2017},
	keywords = {machine learning, database management systems, autonomic computing, database tuning},
	pages = {1009--1024},
	file = {Van Aken et al. - 2017 - Automatic Database Management System Tuning Throug.pdf:/home/ryan/Zotero/storage/QNQAZUGY/Van Aken et al. - 2017 - Automatic Database Management System Tuning Throug.pdf:application/pdf},
}

@inproceedings{dqfd,
	address = {New Orleans},
	series = {{AAAI} '18},
	title = {Deep {Q}-learning from {Demonstrations}},
	url = {http://arxiv.org/abs/1704.03732},
	abstract = {Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
	urldate = {2018-02-27},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artifical} {Intelligence}},
	publisher = {IEEE},
	author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.03732},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
	file = {arXiv\:1704.03732 PDF:/home/ryan/Zotero/storage/5F2YZUS4/Hester et al. - 2017 - Deep Q-learning from Demonstrations.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/FJIUGVPF/1704.html:text/html},
}

@article{adaptive_qp_rl,
	series = {Technical {Report}, 08},
	title = {A {Reinforcement} {Learning} {Approach} for {Adaptive} {Query} {Processing}},
	journal = {Technical Reports},
	author = {Tzoumas, Kostas and Sellis, Timos and Jensen, Christian},
	month = jun,
	year = {2008},
	file = {Tzoumas et al. - 2008 - A Reinforcement Learning Approach for Adaptive Que.pdf:/home/ryan/Zotero/storage/IWHGDNA9/Tzoumas et al. - 2008 - A Reinforcement Learning Approach for Adaptive Que.pdf:application/pdf},
}

@article{dbml,
	series = {{SIGMOD} {Rec} '16},
	title = {Database {Meets} {Deep} {Learning}: {Challenges} and {Opportunities}},
	volume = {45},
	issn = {0163-5808},
	shorttitle = {Database {Meets} {Deep} {Learning}},
	url = {http://doi.acm.org/10.1145/3003665.3003669},
	doi = {10.1145/3003665.3003669},
	abstract = {Deep learning has recently become very popular on account of its incredible success in many complex datadriven applications, including image classification and speech recognition. The database community has worked on data-driven applications for many years, and therefore should be playing a lead role in supporting this new wave. However, databases and deep learning are different in terms of both techniques and applications. In this paper, we discuss research problems at the intersection of the two fields. In particular, we discuss possible improvements for deep learning systems from a database perspective, and analyze database applications that may benefit from deep learning techniques.},
	number = {2},
	urldate = {2018-02-27},
	journal = {SIGMOD Rec.},
	author = {Wang, Wei and Zhang, Meihui and Chen, Gang and Jagadish, H. V. and Ooi, Beng Chin and Tan, Kian-Lee},
	month = sep,
	year = {2016},
	pages = {17--22},
	file = {Wang et al. - 2016 - Database Meets Deep Learning Challenges and Oppor.pdf:/home/ryan/Zotero/storage/RYWBENYE/Wang et al. - 2016 - Database Meets Deep Learning Challenges and Oppor.pdf:application/pdf},
}

@article{ituned,
	series = {{VLDB} '09},
	title = {Tuning {Database} {Configuration} {Parameters} with {iTuned}},
	volume = {2},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/1687627.1687767},
	doi = {10.14778/1687627.1687767},
	number = {1},
	urldate = {2018-02-27},
	journal = {PVLDB},
	author = {Duan, Songyun and Thummala, Vamsidhar and Babu, Shivnath},
	year = {2009},
	pages = {1246--1257},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/DA8CTXPB/Duan et al. - 2009 - Tuning Database Configuration Parameters with iTun.pdf:application/pdf},
}

@inproceedings{a3c,
	series = {{ICML} '16},
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	booktitle = {{ICML}},
	author = {Mnih, Volodymyr and Badia, Adri√† Puigdom√®nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	year = {2016},
}

@inproceedings{ddqn,
	series = {{AAAI} '16},
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-{Learning}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented.  In this paper, we answer all these questions affirmatively.  In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.  We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation.  We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	language = {en},
	urldate = {2018-02-27},
	booktitle = {Thirtieth {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
	month = mar,
	year = {2016},
	file = {Full Text PDF:/home/ryan/Zotero/storage/TWC4NILT/Hasselt et al. - 2016 - Deep Reinforcement Learning with Double Q-Learning.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/RWK94DA4/12389.html:text/html},
}

@article{ac,
	series = {{ICLR} '17},
	title = {Sample {Efficient} {Actor}-{Critic} with {Experience} {Replay}},
	url = {http://arxiv.org/abs/1611.01224},
	abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
	urldate = {2018-02-27},
	journal = {arXiv:1611.01224 [cs]},
	author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01224},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1611.01224 PDF:/home/ryan/Zotero/storage/3V9LSZQ3/Wang et al. - 2016 - Sample Efficient Actor-Critic with Experience Repl.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/UES33FBG/1611.html:text/html},
}

@inproceedings{trpo,
	series = {{ICML} '15},
	title = {Trust {Region} {Policy} {Optimization}},
	booktitle = {{ICML}},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	year = {2015},
}

@article{ppo,
	series = {{arXiv} '17},
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2018-02-27},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1707.06347 PDF:/home/ryan/Zotero/storage/U2TBYXQU/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/VP6FQDUG/1707.html:text/html},
}

@article{url-amazonEBS,
	title = {Amazon {EBS}, https://aws.amazon.com/ebs/},
	url = {https://aws.amazon.com/ebs/},
	note = {tex.key= 1},
}

@inproceedings{dp_histogram,
	series = {{VLDB} '98},
	title = {Optimal {Histograms} with {Quality} {Guarantees}},
	booktitle = {{VLDB}},
	author = {Jagadish, H and Poosala, Viswanath and Koudas, Nick and Sevcik, Ken and Muthukrishnan, S. and Suel, Torsten},
	year = {1998},
	pages = {275--286},
	file = {Citeseer - Snapshot:/home/ryan/Zotero/storage/RX4HS8P8/summary.html:text/html;Jagadish et al. - 1998 - Optimal Histograms with Quality Guarantees.pdf:/home/ryan/Zotero/storage/QAQGAC34/Jagadish et al. - 1998 - Optimal Histograms with Quality Guarantees.pdf:application/pdf},
}

@article{clay,
	series = {{VLDB} '16},
	title = {Clay: {Fine}-grained {Adaptive} {Partitioning} for {General} {Database} {Schemas}},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {Clay},
	url = {https://doi.org/10.14778/3025111.3025125},
	doi = {10.14778/3025111.3025125},
	number = {4},
	urldate = {2017-05-30},
	journal = {PVLDB},
	author = {Serafini, Marco and Taft, Rebecca and Elmore, Aaron J. and Pavlo, Andrew and Aboulnaga, Ashraf and Stonebraker, Michael},
	year = {2016},
	pages = {445--456},
	file = {Serafini et al. - 2016 - Clay Fine-grained Adaptive Partitioning for Gener.pdf:/home/ryan/Zotero/storage/EW6EF884/Serafini et al. - 2016 - Clay Fine-grained Adaptive Partitioning for Gener.pdf:application/pdf},
}

@article{dqn,
	series = {Nature '15},
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	number = {7540},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg},
	year = {2015},
	pages = {529--533},
	file = {nature14236.pdf:/home/ryan/Zotero/storage/JBM3TD7T/nature14236.pdf:application/pdf},
}

@inproceedings{grid2,
	title = {A distributed resource management architecture that supports advance reservations and co-allocation},
	doi = {10.1109/IWQOS.1999.766475},
	abstract = {The realization of end-to-end quality of service (QoS) guarantees in emerging network-based applications requires mechanisms that support first dynamic discovery and then advance or immediate reservation of resources that will often be heterogeneous in type and implementation and independently controlled and administered. We propose the Globus Architecture for Reservation and Allocation (GARA) to address these four issues. GARA treats both reservations and computational elements such as processes, network flows, and memory blocks as first-class entities, allowing them to be created, monitored, and managed independently and uniformly. It simplifies management of heterogeneous resource types by defining uniform mechanisms for computers, networks, disk, memory, and other resources. Layering on these standard mechanisms, GARA enables the construction of application-level co-reservation and co-allocation libraries that applications can use to dynamically assemble collections of resources, guided by both application QoS requirements and the local administration policy of individual resources. We describe a prototype GARA implementation that supports three different resource type-parallel computers, individual CPU under control of the dynamic soft real-time scheduler, and integrated services networks, and provide performance results that quantify the costs of our techniques},
	booktitle = {1999 {Seventh} {International} {Workshop} on {Quality} of {Service}, 1999. {IWQoS} '99},
	author = {Foster, I. and Kesselman, C. and Lee, C. and Lindell, B. and Nahrstedt, K. and Roy, A.},
	year = {1999},
	keywords = {scheduling, Computer architecture, quality of service, Libraries, advance reservations, Application software, application-level co-reservation, Assembly, co-allocation, computer network management, Computer network management, Computer networks, Computerized monitoring, CPU control, distributed resource management architecture, dynamic discovery, dynamic soft real-time scheduler, Globus Architecture for Reservation and Allocation, heterogeneous resource types, integrated services networks, local administration policy, management, Memory management, network flows, performance, performance evaluation, QoS guarantees, Quality of service, Resource management, telecommunication traffic, uniform mechanisms},
	pages = {27--36},
	file = {Foster et al. - 1999 - A distributed resource management architecture tha.pdf:/home/ryan/Zotero/storage/4HHGKBEJ/Foster et al. - 1999 - A distributed resource management architecture tha.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/NHJ9KUEV/766475.html:text/html},
}

@inproceedings{grid1,
	address = {Washington, DC, USA},
	series = {{HPDC} '99},
	title = {Resource {Co}-{Allocation} in {Computational} {Grids}},
	isbn = {978-0-7695-0287-8},
	url = {http://dl.acm.org/citation.cfm?id=822084.823250},
	abstract = {Applications designed to execute on "computational grids" frequently require the simultaneous co-allocation of multiple resources in order to meet performance requirements. For example, several computers and network elements may be required in order to achieve real-time reconstruction of experimental data, while a large numerical simulation may require simultaneous access to multiple supercomputers. Motivated by these concerns, we have developed a general resource management architecture for Grid environments, in which resource co-allocation is an integral component. In this paper, we examine the co-allocation problem in detail and present mechanisms that allow an application to guide resource selection during the co-allocation process; these mechanisms address issues relating to the allocation, monitoring, control, and configuration of distributed computations. We describe the implementation of co-allocators based on these mechanisms and present the results of microbenchmark studies and large-scale application experiments that provide insights into the costs and practical utility of our techniques.},
	urldate = {2018-04-05},
	booktitle = {Proceedings of the 8th {IEEE} {International} {Symposium} on {High} {Performance} {Distributed} {Computing}},
	publisher = {IEEE Computer Society},
	author = {Czajkowski, Karl and Foster, Ian and Kesselman, Carl},
	year = {1999},
	pages = {37--},
	file = {Czajkowski et al. - 1999 - Resource Co-Allocation in Computational Grids.pdf:/home/ryan/Zotero/storage/4L68VQQC/Czajkowski et al. - 1999 - Resource Co-Allocation in Computational Grids.pdf:application/pdf},
}

@article{universal_approx,
	series = {Neural {Networks} '89},
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	number = {5},
	urldate = {2018-04-05},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
	file = {ScienceDirect Full Text PDF:/home/ryan/Zotero/storage/DVTTQKZS/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/S9U48JSV/0893608089900208.html:text/html},
}

@article{deep_learning,
	series = {Nature '15},
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2018-04-05},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/home/ryan/Zotero/storage/QQBHVJ53/LeCun et al. - 2015 - Deep learning.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/Q5QGDVZW/nature14539.html:text/html},
}

@inproceedings{mdp_elastic,
	series = {Big {Data} '17},
	title = {Elastic management of cloud applications using adaptive reinforcement learning},
	isbn = {978-1-5386-2715-0},
	url = {http://ieeexplore.ieee.org/document/8257928/},
	doi = {10.1109/BigData.2017.8257928},
	urldate = {2018-04-03},
	publisher = {IEEE},
	author = {Lolos, Konstantinos and Konstantinou, Ioannis and Kantere, Verena and Koziris, Nectarios},
	month = dec,
	year = {2017},
	pages = {203--212},
	file = {Lolos et al. - 2017 - Elastic management of cloud applications using ada.pdf:/home/ryan/Zotero/storage/6G5TWKYS/Lolos et al. - 2017 - Elastic management of cloud applications using ada.pdf:application/pdf},
}

@article{nash,
	series = {{PNAS} '50},
	title = {Equilibrium points in n-person games},
	volume = {36},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/36/1/48},
	doi = {10.1073/pnas.36.1.48},
	abstract = {One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms ‚Ä¶},
	language = {en},
	number = {1},
	urldate = {2018-03-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nash, John F.},
	month = jan,
	year = {1950},
	pmid = {16588946},
	pages = {48--49},
	file = {Nash - 1950 - Equilibrium points in n-person games.pdf:/home/ryan/Zotero/storage/AT4E5BP4/Nash - 1950 - Equilibrium points in n-person games.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/G6L22PI5/48.html:text/html},
}

@article{url-pgeo,
	title = {{PostgreSQL} {Documentation},  "{Controlling} the {Planner} with {Explicit} {JOIN} {Clauses}", https://www.postgresql.org/docs/10/static/explicit-joins.html},
	url = {https://aws.amazon.com/rds/},
	note = {tex.key= 1},
}

@article{url-imdbgithub,
	title = {Premade {VM} for replication, http://git.io/imdb},
	url = {https://aws.amazon.com/rds/},
	note = {tex.key= 1},
}

@article{tail2,
	series = {Comm. {ACM} '13},
	title = {The {Tail} at {Scale}},
	volume = {56},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2408776.2408794},
	doi = {10.1145/2408776.2408794},
	abstract = {Software techniques that tolerate latency variability are vital to building responsive large-scale Web services.},
	number = {2},
	urldate = {2018-03-06},
	journal = {Commun. ACM},
	author = {Dean, Jeffrey and Barroso, Luiz Andr√©},
	month = feb,
	year = {2013},
	pages = {74--80},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/MKMFE4JV/Dean and Barroso - 2013 - The Tail at Scale.pdf:application/pdf},
}

@inproceedings{tail1,
	address = {New York, NY, USA},
	series = {{SOCC} '14},
	title = {Tales of the {Tail}: {Hardware}, {OS}, and {Application}-level {Sources} of {Tail} {Latency}},
	isbn = {978-1-4503-3252-1},
	shorttitle = {Tales of the {Tail}},
	url = {http://doi.acm.org/10.1145/2670979.2670988},
	doi = {10.1145/2670979.2670988},
	abstract = {Interactive services often have large-scale parallel implementations. To deliver fast responses, the median and tail latencies of a service's components must be low. In this paper, we explore the hardware, OS, and application-level sources of poor tail latency in high throughput servers executing on multi-core machines. We model these network services as a queuing system in order to establish the best-achievable latency distribution. Using fine-grained measurements of three different servers (a null RPC service, Memcached, and Nginx) on Linux, we then explore why these servers exhibit significantly worse tail latencies than queuing models alone predict. The underlying causes include interference from background processes, request re-ordering caused by poor scheduling or constrained concurrency models, suboptimal interrupt routing, CPU power saving mechanisms, and NUMA effects. We systematically eliminate these factors and show that Memcached can achieve a median latency of 11 Œºs and a 99.9th percentile latency of 32 Œºs at 80\% utilization on a four-core system. In comparison, a na√Øve deployment of Memcached at the same utilization on a single-core system has a median latency of 100 Œºs and a 99.9th percentile latency of 5 ms. Finally, we demonstrate that tradeoffs exist between throughput, energy, and tail latency.},
	urldate = {2018-03-06},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Li, Jialin and Sharma, Naveen Kr. and Ports, Dan R. K. and Gribble, Steven D.},
	year = {2014},
	keywords = {predictable latency, Tail latency},
	pages = {9:1--9:14},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/KZUSJ6NU/Li et al. - 2014 - Tales of the Tail Hardware, OS, and Application-l.pdf:application/pdf},
}

@article{url-sqlserver,
	title = {{SQL} {Server}, https://microsoft.com/sql-server/},
	url = {https://microsoft.com/sql-server/},
	note = {tex.key= 1},
}

@article{bound_card,
	series = {{VLDB} '09},
	title = {Preventing {Bad} {Plans} by {Bounding} the {Impact} of {Cardinality} {Estimation} {Errors}},
	volume = {2},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/1687627.1687738},
	doi = {10.14778/1687627.1687738},
	number = {1},
	urldate = {2018-03-05},
	journal = {PVLDB},
	author = {Moerkotte, Guido and Neumann, Thomas and Steidl, Gabriele},
	year = {2009},
	pages = {982--993},
	file = {Moerkotte et al. - 2009 - Preventing Bad Plans by Bounding the Impact of Car.pdf:/home/ryan/Zotero/storage/S9F725RP/Moerkotte et al. - 2009 - Preventing Bad Plans by Bounding the Impact of Car.pdf:application/pdf},
}

@inproceedings{leftdeep_vs_bushy,
	address = {New York, NY, USA},
	series = {{SIGMOD} '91},
	title = {Left-deep vs. {Bushy} {Trees}: {An} {Analysis} of {Strategy} {Spaces} and {Its} {Implications} for {Query} {Optimization}},
	isbn = {978-0-89791-425-3},
	shorttitle = {Left-deep vs. {Bushy} {Trees}},
	url = {http://doi.acm.org/10.1145/115790.115813},
	doi = {10.1145/115790.115813},
	urldate = {2018-03-05},
	booktitle = {Proceedings of the 1991 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Ioannidis, Yannis E. and Kang, Younkyung Cha},
	year = {1991},
	pages = {168--177},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/WUU9JUQB/Ioannidis and Kang - 1991 - Left-deep vs. Bushy Trees An Analysis of Strategy.pdf:application/pdf},
}

@inproceedings{volcano,
	address = {Washington, DC, USA},
	series = {{ICDE} '93},
	title = {The {Volcano} {Optimizer} {Generator}: {Extensibility} and {Efficient} {Search}},
	isbn = {978-0-8186-3570-0},
	shorttitle = {The {Volcano} {Optimizer} {Generator}},
	url = {http://dl.acm.org/citation.cfm?id=645478.757691},
	urldate = {2018-03-04},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Graefe, Goetz and McKenna, William J.},
	year = {1993},
	pages = {209--218},
	file = {Graefe and McKenna - 1993 - The Volcano Optimizer Generator Extensibility and.PDF:/home/ryan/Zotero/storage/4I32DQU9/Graefe and McKenna - 1993 - The Volcano Optimizer Generator Extensibility and.PDF:application/pdf},
}

@inproceedings{robust_qo,
	address = {New York, NY, USA},
	series = {{SIGMOD} '05},
	title = {Towards a {Robust} {Query} {Optimizer}: {A} {Principled} and {Practical} {Approach}},
	isbn = {978-1-59593-060-6},
	shorttitle = {Towards a {Robust} {Query} {Optimizer}},
	url = {http://doi.acm.org/10.1145/1066157.1066172},
	doi = {10.1145/1066157.1066172},
	abstract = {Research on query optimization has focused almost exclusively on reducing query execution time, while important qualities such as consistency and predictability have largely been ignored, even though most database users consider these qualities to be at least as important as raw performance. In this paper, we explore how the query optimization process can be made more robust, focusing on the important subproblem of cardinality estimation. The robust cardinality estimation technique that we propose allows for a user- or application-specified trade-off between performance and predictability, and it captures multi-dimensional correlations while remaining space- and time-efficient.},
	urldate = {2018-03-04},
	booktitle = {Proceedings of the 2005 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Babcock, Brian and Chaudhuri, Surajit},
	year = {2005},
	pages = {119--130},
	file = {Babcock and Chaudhuri - 2005 - Towards a Robust Query Optimizer A Principled and.pdf:/home/ryan/Zotero/storage/ULC7ZMBI/Babcock and Chaudhuri - 2005 - Towards a Robust Query Optimizer A Principled and.pdf:application/pdf},
}

@inproceedings{joe_complexity,
	series = {{VLDB} '90},
	title = {Measuring the {Complexity} of {Join} {Enumeration} in {Query} {Optimization}},
	isbn = {978-1-55860-149-9},
	shorttitle = {Measuring the {Complexity} of {Join} {Enumeration}},
	url = {http://dl.acm.org/citation.cfm?id=645916.671976},
	urldate = {2018-03-04},
	booktitle = {{VLDB}},
	author = {Ono, Kiyoshi and Lohman, Guy M.},
	year = {1990},
	pages = {314--325},
	file = {Ono and Lohman - 1990 - Measuring the Complexity of Join Enumeration in Qu.PDF:/home/ryan/Zotero/storage/5XQ5GUME/Ono and Lohman - 1990 - Measuring the Complexity of Join Enumeration in Qu.PDF:application/pdf},
}

@article{tensorforce,
	series = {https://github.com/reinforceio/tensorforce},
	title = {{TensorForce}: {A} {TensorFlow} library for applied reinforcement learning},
	url = {https://github.com/reinforceio/tensorforce},
	author = {Schaarschmidt, Michael and Kuhnle, Alexander and Fricke, Kai},
	year = {2017},
	note = {tex.howpublished= Web page},
}

@inproceedings{reinforce,
	series = {Machine {Learning} '92},
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	abstract = {Abstract. This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	booktitle = {Machine {Learning}},
	author = {Williams, Ronald J.},
	year = {1992},
	pages = {229--256},
	file = {Citeseer - Full Text PDF:/home/ryan/Zotero/storage/G3J7XWUS/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf:application/pdf;Citeseer - Snapshot:/home/ryan/Zotero/storage/RC8IIQB4/summary.html:text/html},
}

@article{lstm,
	series = {Neural {Computation} '97},
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	language = {en},
	number = {8},
	urldate = {2017-11-06},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, J√ºrgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {lstm.pdf:/home/ryan/Zotero/storage/S4KHY8PB/lstm.pdf:application/pdf},
}

@article{dnn,
	series = {{NN} '15},
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep learning in neural networks},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	language = {en},
	urldate = {2017-11-06},
	journal = {Neural Networks},
	author = {Schmidhuber, J√ºrgen},
	month = jan,
	year = {2015},
	pages = {85--117},
	file = {1-s2.0-S0893608014002135-main.pdf:/home/ryan/Zotero/storage/HLSWVSVK/1-s2.0-S0893608014002135-main.pdf:application/pdf},
}

@inproceedings{wisedb-icde,
	address = {San Diego, CA},
	series = {{ICDE} '17},
	title = {A {Learning}-{Based} {Service} for {Cost} and {Performance} {Management} of {Cloud} {Databases}},
	copyright = {All rights reserved},
	isbn = {978-1-5090-6543-1},
	url = {http://ieeexplore.ieee.org/document/7930073/},
	doi = {10.1109/ICDE.2017.177},
	urldate = {2017-08-05},
	booktitle = {Proceedings of the {IEEE} 33rd {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {Marcus, Ryan and Semenova, Sofiya and Papaemmanouil, Olga},
	year = {2017},
	note = {props: demo},
	pages = {1361--1362},
	file = {Marcus et al. - 2017 - A Learning-Based Service for Cost and Performance .pdf:/home/ryan/Zotero/storage/KSFPUW3X/Marcus et al. - 2017 - A Learning-Based Service for Cost and Performance .pdf:application/pdf},
}

@inproceedings{wisedb-cidr,
	address = {San Jose, CA},
	series = {{CIDR} '17},
	title = {Releasing {Cloud} {Databases} from the {Chains} of {Performance} {Prediction} {Models}},
	copyright = {All rights reserved},
	booktitle = {8th {Biennial} {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	year = {2017},
	note = {tex.authors= Ryan Marcus and Olga Papaemmanouil},
	keywords = {learning based, placement, scheduling, provisioning},
	file = {Marcus and Papaemmanouil - 2017 - Releasing Cloud Databases from the Chains of Perfo.pdf:/home/ryan/Zotero/storage/73QSVFCG/Marcus and Papaemmanouil - 2017 - Releasing Cloud Databases from the Chains of Perfo.pdf:application/pdf},
}

@inproceedings{bazaar,
	address = {San Jose, California},
	series = {{SoCC} '12},
	title = {Bridging the {Tenant}-provider {Gap} in {Cloud} {Services}},
	isbn = {978-1-4503-1761-0},
	shorttitle = {Bridging the {Tenant}-provider {Gap}},
	url = {http://doi.acm.org/10.1145/2391229.2391239},
	doi = {10.1145/2391229.2391239},
	booktitle = {Proceedings of the {Third} {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Jalaparti, Virajith and Ballani, Hitesh and Costa, Paolo and Karagiannis, Thomas and Rowstron, Ant},
	year = {2012},
	note = {tex.acmid= 2391239
tex.articleno= 10
tex.numpages= 14},
	keywords = {uses latency prediction, provisioning},
	pages = {10:1--10:14},
	file = {socc12-bazaar.pdf:/home/ryan/Zotero/storage/HMUC568I/socc12-bazaar.pdf:application/pdf},
}

@article{wisedb-vldb,
	series = {{VLDB} '16},
	title = {{WiSeDB}: {A} {Learning}-based {Workload} {Management} {Advisor} for {Cloud} {Databases}},
	volume = {9},
	copyright = {All rights reserved},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2977797.2977804},
	doi = {10.14778/2977797.2977804},
	number = {10},
	journal = {PVLDB},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	year = {2016},
	note = {tex.acmid= 2977804
tex.issue\_date= June 2016
tex.numpages= 12},
	keywords = {learning based, placement, uses latency prediction, scheduling, provisioning},
	pages = {780--791},
	file = {p780-marcus.pdf:/home/ryan/Zotero/storage/NXXXBTVF/p780-marcus.pdf:application/pdf},
}

@article{pretrain_demonstration,
	series = {{arXiv} '17},
	title = {Pre-training {Neural} {Networks} with {Human} {Demonstrations} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1709.04083},
	abstract = {Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using a deep neural network as its function approximator and by learning directly from raw images. A drawback of using raw images is that deep RL must learn the state feature representation from the raw images in addition to learning a policy. As a result, deep RL can require a prohibitively large amount of training time and data to reach reasonable performance, making it difficult to use deep RL in real-world applications, especially when data is expensive. In this work, we speed up training by addressing half of what deep RL is trying to solve --- learning features. Our approach is to learn some of the important features by pre-training deep RL network's hidden layers via supervised learning using a small set of human demonstrations. We empirically evaluate our approach using deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600 games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training with human demonstrations in a supervised learning manner is better at discovering features relative to pre-training naively in DQN, and 2) initializing a deep RL network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of human demonstrations.},
	urldate = {2018-08-07},
	journal = {arXiv:1709.04083 [cs]},
	author = {Cruz Jr, Gabriel V. de la and Du, Yunshu and Taylor, Matthew E.},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04083},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv\:1709.04083 PDF:/home/ryan/Zotero/storage/G8XPQGYD/Cruz Jr et al. - 2017 - Pre-training Neural Networks with Human Demonstrat.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/CWYUGX8S/1709.html:text/html},
}

@article{deep_blue,
	series = {{AI} '02},
	title = {Deep {Blue}},
	volume = {134},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370201001291},
	doi = {10.1016/S0004-3702(01)00129-1},
	abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: ‚Ä¢a single-chip chess search engine,‚Ä¢a massively parallel system with multiple levels of parallelism,‚Ä¢a strong emphasis on search extensions,‚Ä¢a complex evaluation function, and‚Ä¢effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
	number = {1},
	urldate = {2018-08-07},
	journal = {Artificial Intelligence},
	author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
	month = jan,
	year = {2002},
	keywords = {Computer chess, Evaluation function, Game tree search, Parallel search, Search extensions, Selective search},
	pages = {57--83},
	file = {Campbell et al. - 2002 - Deep Blue.pdf:/home/ryan/Zotero/storage/RAIFTLYJ/Campbell et al. - 2002 - Deep Blue.pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/9SD2X599/S0004370201001291.html:text/html},
}

@article{alpha_beta,
	series = {{AIM}-30 '61},
	title = {The {Alpha}-{Beta} {Heuristic}},
	volume = {Memo 30},
	url = {http://dspace.mit.edu/handle/1721.1/6098},
	abstract = {The Alpha-Beta heuristic is a method for pruning unneeded branches from the move tree of a game. The algorithm makes use of information gained about part of the tree to reject those branches which will not affect the principle variation.},
	language = {en\_US},
	urldate = {2018-08-07},
	journal = {Artificial Intelligence Project},
	author = {Edwards, D. J. and Hart, T. P.},
	month = dec,
	year = {1961},
	file = {Edwards and Hart - 1961 - The Alpha-Beta Heuristic.pdf:/home/ryan/Zotero/storage/V8BR2FTU/Edwards and Hart - 1961 - The Alpha-Beta Heuristic.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/KUJRVR25/6098.html:text/html},
}

@article{near_opt_rl,
	series = {Machine {Learning} '01},
	title = {Near-{Optimal} {Reinforcement} {Learning} in {Polynomial} {Time}},
	volume = {49},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1017984413808},
	doi = {10.1023/A:1017984413808},
	abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
	language = {en},
	number = {2-3},
	urldate = {2018-07-23},
	journal = {Machine Learning},
	author = {Kearns, Michael and Singh, Satinder},
	month = nov,
	year = {2002},
	pages = {209--232},
	file = {Kearns and Singh - 2002 - Near-Optimal Reinforcement Learning in Polynomial .pdf:/home/ryan/Zotero/storage/6P7CSHP7/Kearns and Singh - 2002 - Near-Optimal Reinforcement Learning in Polynomial .pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/YYMZL4EV/A1017984413808.html:text/html},
}

@inproceedings{demonstration,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'96},
	title = {Learning from {Demonstration}},
	url = {http://dl.acm.org/citation.cfm?id=2998981.2999127},
	abstract = {By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For teaming control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based reinforcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor.},
	urldate = {2018-07-23},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Schaal, Stefan},
	year = {1996},
	pages = {1040--1046},
	file = {Schaal - 1996 - Learning from Demonstration.pdf:/home/ryan/Zotero/storage/E565YAQG/Schaal - 1996 - Learning from Demonstration.pdf:application/pdf},
}

@inproceedings{incremental_rl,
	address = {New York, NY, USA},
	series = {{AGENTS} '01},
	title = {Incremental {Reinforcement} {Learning} for {Designing} {Multi}-agent {Systems}},
	isbn = {978-1-58113-326-4},
	url = {http://doi.acm.org/10.1145/375735.375826},
	doi = {10.1145/375735.375826},
	urldate = {2018-07-21},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Autonomous} {Agents}},
	publisher = {ACM},
	author = {Buffet, Olivier and Dutech, Alain and Charpillet, Fran√ßois},
	year = {2001},
	pages = {31--32},
	file = {Buffet et al. - 2001 - Incremental Reinforcement Learning for Designing M.pdf:/home/ryan/Zotero/storage/PGQ9WMS5/Buffet et al. - 2001 - Incremental Reinforcement Learning for Designing M.pdf:application/pdf},
}

@inproceedings{slaorchestrator,
	address = {Boston, MA},
	series = {{USENIX} {ATX}'18},
	title = {{SLAOrchestrator}: {Reducing} the {Cost} of {Performance} {SLAs} for {Cloud} {Data} {Analytics}},
	isbn = {978-1-931971-44-7},
	url = {https://www.usenix.org/conference/atc18/presentation/ortiz},
	booktitle = {2018 {USENIX} {Annual} {Technical} {Conference} ({USENIX} {ATC} 18)},
	publisher = {USENIX Association},
	author = {Ortiz, Jennifer and Lee, Brendan and Balazinska, Magdalena and Gehrke, Johannes and Hellerstein, Joseph L.},
	year = {2018},
	pages = {547--560},
	file = {Ortiz et al. - 2018 - SLAOrchestrator Reducing the Cost of Performance .pdf:/home/ryan/Zotero/storage/MTFJGR3F/Ortiz et al. - 2018 - SLAOrchestrator Reducing the Cost of Performance .pdf:application/pdf},
}

@article{cuttlefish,
	series = {{arXiv} '18},
	title = {Cuttlefish: {A} {Lightweight} {Primitive} for {Adaptive} {Query} {Processing}},
	shorttitle = {Cuttlefish},
	url = {https://arxiv.org/abs/1802.09180},
	language = {en},
	urldate = {2018-07-19},
	journal = {arXiv preprint},
	author = {Kaftan, Tomer and Balazinska, Magdalena and Cheung, Alvin and Gehrke, Johannes},
	month = feb,
	year = {2018},
	file = {Kaftan et al. - 2018 - Cuttlefish A Lightweight Primitive for Adaptive Q.pdf:/home/ryan/Zotero/storage/2WCXUFFV/Kaftan et al. - 2018 - Cuttlefish A Lightweight Primitive for Adaptive Q.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/2HMERRPP/1802.html:text/html},
}

@inproceedings{batchnorm,
	address = {Lille, France},
	series = {{ICML}'15},
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	urldate = {2018-07-05},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	pages = {448--456},
	file = {Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:/home/ryan/Zotero/storage/D746F34N/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@article{dex,
	series = {{arXiv} '18},
	title = {Dex: {Incremental} {Learning} for {Complex} {Environments} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Dex},
	url = {http://arxiv.org/abs/1706.05749},
	abstract = {This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.},
	urldate = {2018-07-04},
	journal = {arXiv:1706.05749 [cs, stat]},
	author = {Erickson, Nick and Zhao, Qi},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.05749},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1706.05749 PDF:/home/ryan/Zotero/storage/QFXDZUK9/Erickson and Zhao - 2017 - Dex Incremental Learning for Complex Environments.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/X3Q6PW9S/1706.html:text/html},
}

@inproceedings{step,
	address = {New York, NY, USA},
	series = {{SoCC} '16},
	title = {{STeP}: {Scalable} {Tenant} {Placement} for {Managing} {Database}-as-a-{Service} {Deployments}},
	isbn = {978-1-4503-4525-5},
	shorttitle = {{STeP}},
	url = {http://doi.acm.org/10.1145/2987550.2987575},
	doi = {10.1145/2987550.2987575},
	abstract = {Public cloud providers with Database-as-a-Service offerings must efficiently allocate computing resources to each of their customers. An effective assignment of tenants both reduces the number of physical servers in use and meets customer expectations at a price point that is competitive in the cloud market. For public cloud vendors like Microsoft and Amazon, this means packing millions of users' databases onto hundreds or thousands of servers. This paper studies tenant placement by examining a publicly released dataset of anonymized customer resource usage statistics from Microsoft's Azure SQL Database production system over a three-month period. We implemented the STeP framework to ingest and analyze this large dataset. STeP allowed us to use this production dataset to evaluate several new algorithms for packing database tenants onto servers. These techniques produce highly efficient packings by collocating tenants with compatible resource usage patterns. The evaluation shows that under a production-sourced customer workload, these techniques are robust to variations in the number of nodes, keeping performance objective violations to a minimum even for high-density tenant packings. In comparison to the algorithm used in production at the time of data collection, our algorithms produce up to 90\% fewer performance objective violations and save up to 32\% of total operational costs for the cloud provider.},
	urldate = {2018-07-01},
	booktitle = {Proceedings of the {Seventh} {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Taft, Rebecca and Lang, Willis and Duggan, Jennie and Elmore, Aaron J. and Stonebraker, Michael and DeWitt, David},
	year = {2016},
	pages = {388--400},
	file = {Taft et al. - 2016 - STeP Scalable Tenant Placement for Managing Datab.pdf:/home/ryan/Zotero/storage/7DPYEDQS/Taft et al. - 2016 - STeP Scalable Tenant Placement for Managing Datab.pdf:application/pdf},
}

@inproceedings{pytorch,
	series = {{NIPS}-{W} '17},
	title = {Automatic differentiation in {PyTorch}},
	booktitle = {Neural information processing workshops},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
}

@article{rnn_nlp,
	series = {{JMLR} '11},
	title = {Natural {Language} {Processing} ({Almost}) from {Scratch}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/collobert11a.html},
	number = {Aug},
	urldate = {2018-04-22},
	journal = {Journal of Machine Learning Research},
	author = {Collobert, Ronan and Weston, Jason and Bottou, L√©on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	year = {2011},
	pages = {2493--2537},
	file = {Full Text PDF:/home/ryan/Zotero/storage/N4LESXFK/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/K58U2K8K/collobert11a.html:text/html},
}

@article{tensorflow,
	series = {{arXiv} '16},
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	urldate = {2018-04-22},
	journal = {arXiv:1603.04467 [cs]},
	author = {Abadi, Mart√≠n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
	file = {arXiv\:1603.04467 PDF:/home/ryan/Zotero/storage/XPA6QNZF/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/2BQNVGWB/1603.html:text/html},
}

@inproceedings{qo_state_rep,
	series = {{DEEM} '18},
	title = {Learning {State} {Representations} for {Query} {Optimization} with {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1803.08604},
	language = {en},
	urldate = {2018-04-18},
	booktitle = {2nd {Workshop} on {Data} {Managmeent} for {End}-to-{End} {Machine} {Learning}},
	author = {Ortiz, Jennifer and Balazinska, Magdalena and Gehrke, Johannes and Keerthi, S. Sathiya},
	year = {2018},
	file = {Ortiz et al. - 2018 - Learning State Representations for Query Optimizat.pdf:/home/ryan/Zotero/storage/C7LSS74H/Ortiz et al. - 2018 - Learning State Representations for Query Optimizat.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/QELUL6WB/1803.html:text/html},
}

@inproceedings{leo,
	series = {{VLDB} '01},
	title = {{LEO} - {DB2}'s {LEarning} {Optimizer}},
	url = {http://dl.acm.org/citation.cfm?id=645927.672349},
	urldate = {2018-04-18},
	booktitle = {{VLDB}},
	author = {Stillger, Michael and Lohman, Guy M. and Markl, Volker and Kandil, Mokhtar},
	year = {2001},
	pages = {19--28},
	file = {Stillger et al. - 2001 - LEO - DB2's LEarning Optimizer.pdf:/home/ryan/Zotero/storage/Q44DFYY4/Stillger et al. - 2001 - LEO - DB2's LEarning Optimizer.pdf:application/pdf},
}

@article{kcca,
	series = {{IJNS} '00},
	title = {Kernel and nonlinear canonical correlation analysis},
	volume = {10},
	number = {05},
	journal = {International Journal of Neural Systems},
	author = {Lai, Pei Ling and Fyfe, Colin},
	year = {2000},
	pages = {365--377},
}

@article{svr,
	series = {{NIPS} '07},
	title = {Support vector regression},
	volume = {11},
	number = {10},
	journal = {Neural Information Processing},
	author = {Basak, Debasish and Pal, Srimanta and Patranabis, Dipak Chandra},
	year = {2007},
	pages = {203--224},
	file = {Basak et al. - 2007 - Support vector regression.pdf:/home/ryan/Zotero/storage/XE2TEYIF/Basak et al. - 2007 - Support vector regression.pdf:application/pdf},
}

@article{rejoin_arxiv,
	title = {Deep {Reinforcement} {Learning} for {Join} {Order} {Enumeration}},
	url = {http://arxiv.org/abs/1803.00055},
	abstract = {Join order selection plays a significant role in query performance. However, modern query optimizers typically employ static join enumeration algorithms that do not receive any feedback about the quality of the resulting plan. Hence, optimizers often repeatedly choose the same bad plan, as they do not have a mechanism for "learning from their mistakes". In this paper, we argue that existing deep reinforcement learning techniques can be applied to address this challenge. These techniques, powered by artificial neural networks, can automatically improve decision making by incorporating feedback from their successes and failures. Towards this goal, we present ReJOIN, a proof-of-concept join enumerator, and present preliminary results indicating that ReJOIN can match or outperform the PostgreSQL optimizer in terms of plan quality and join enumeration efficiency.},
	urldate = {2018-04-05},
	journal = {arXiv:1803.00055 [cs]},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Databases, Computer Science - Learning},
	file = {arXiv\:1803.00055 PDF:/home/ryan/Zotero/storage/6LHKIE4E/Marcus and Papaemmanouil - 2018 - Deep Reinforcement Learning for Join Order Enumera.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/XNQCSGST/1803.html:text/html},
}

@article{deep_rl,
	series = {{IEEE} {Signal} {Processing} '17},
	title = {A {Brief} {Survey} of {Deep} {Reinforcement} {Learning}},
	volume = {34},
	issn = {1053-5888},
	shorttitle = {Brief survey of {DRL}},
	url = {http://arxiv.org/abs/1708.05866},
	doi = {10.1109/MSP.2017.2743240},
	abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
	number = {6},
	urldate = {2018-02-27},
	journal = {IEEE Signal Processing Magazine},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.05866},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {26--38},
	file = {arXiv\:1708.05866 PDF:/home/ryan/Zotero/storage/YKJ64HLN/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/TDXG7SWX/1708.html:text/html},
}

@inproceedings{quickpick,
	series = {{BNCD} '00},
	title = {Join {Order} {Selection} ({Good} {Enough} {Is} {Easy})},
	isbn = {978-3-540-67743-7 978-3-540-45033-7},
	url = {https://link.springer.com/chapter/10.1007/3-540-45033-5_5},
	doi = {10.1007/3-540-45033-5_5},
	abstract = {Uniform sampling of join orders is known to be a competitive alternative to transformation-based optimization techniques. However, uniformity of the sampling process is difficult to establish and only for a restricted class of join queries techniques are known.In this paper, we investigate non-uniform sampling devising a simple yet powerful algorithm that is generally applicable. The key element of the algorithm is a mapping of randomly generated sequences of join predicates to query plans. We take advantage of the bottom-up constructing of query plans by simultaneously computing the costs and discarding partial plans as soon as they exceed the best costs found so far, which implements a highly effective cost-bound pruning component.Sampling does not produce the optimal plan but a near-optimal solution which is fully sufficient as the cost function grows more and more inaccurate with increasing query size. In return, our algorithm establishes a well-balanced trade-off between result quality and time invested in the optimization process.},
	language = {en},
	urldate = {2018-02-27},
	booktitle = {Advances in {Databases}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Waas, Florian and Pellenkoft, Arjan},
	month = jul,
	year = {2000},
	pages = {51--67},
	file = {Snapshot:/home/ryan/Zotero/storage/LQ7ZU38R/3-540-45033-5_5.html:text/html;Waas and Pellenkoft - 2000 - Join Order Selection ( Good Enough Is Easy ).pdf:/home/ryan/Zotero/storage/KRLWSTV3/Waas and Pellenkoft - 2000 - Join Order Selection ( Good Enough Is Easy ).pdf:application/pdf},
}

@article{sparse_bad,
	series = {Econometrics '08},
	title = {Sparse estimators and the oracle property, or the return of {Hodges}‚Äô estimator},
	volume = {142},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407607001273},
	doi = {10.1016/j.jeconom.2007.05.017},
	abstract = {We point out some pitfalls related to the concept of an oracle property as used in Fan and Li [2001. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association 96, 1348‚Äì1360; 2002. Variable selection for Cox's proportional hazards model and frailty model. Annals of Statistics 30, 74‚Äì99; 2004. New estimation and model selection procedures for semiparametric modeling in longitudinal data analysis. Journal of the American Statistical Association 99, 710‚Äì723] which are reminiscent of the well-known pitfalls related to Hodges‚Äô estimator. The oracle property is often a consequence of sparsity of an estimator. We show that any estimator satisfying a sparsity property has maximal risk that converges to the supremum of the loss function; in particular, the maximal risk diverges to infinity whenever the loss function is unbounded. For ease of presentation the result is set in the framework of a linear regression model, but generalizes far beyond that setting. In a Monte Carlo study we also assess the extent of the problem in finite samples for the smoothly clipped absolute deviation (SCAD) estimator introduced in Fan and Li [2001. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association 96, 1348‚Äì1360]. We find that this estimator can perform rather poorly in finite samples and that its worst-case performance relative to maximum likelihood deteriorates with increasing sample size when the estimator is tuned to sparsity.},
	number = {1},
	urldate = {2018-09-18},
	journal = {Journal of Econometrics},
	author = {Leeb, Hannes and P√∂tscher, Benedikt M.},
	month = jan,
	year = {2008},
	keywords = {Bridge estimator, Hard-thresholding, Hodges‚Äô estimator, Lasso, Maximal absolute bias, Maximal risk, Nonuniform limits, Oracle property, Penalized least squares, Penalized maximum likelihood, SCAD, Sparsity},
	pages = {201--211},
	file = {Leeb and P√∂tscher - 2008 - Sparse estimators and the oracle property, or the .pdf:/home/ryan/Zotero/storage/TML34I3E/Leeb and P√∂tscher - 2008 - Sparse estimators and the oracle property, or the .pdf:application/pdf},
}

@inproceedings{pred_xml,
	series = {{VLDB} '05},
	title = {Statistical {Learning} {Techniques} for {Costing} {XML} {Queries}},
	isbn = {978-1-59593-154-2},
	url = {http://dl.acm.org/citation.cfm?id=1083592.1083628},
	urldate = {2018-09-18},
	booktitle = {{VLDB}},
	author = {Zhang, Ning and Haas, Peter J. and Josifovski, Vanja and Lohman, Guy M. and Zhang, Chun},
	year = {2005},
	pages = {289--300},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/CIVDSQ4L/Zhang et al. - 2005 - Statistical Learning Techniques for Costing XML Qu.pdf:application/pdf},
}

@inproceedings{tree_conv,
	address = {Phoenix, Arizona},
	series = {{AAAI} '16},
	title = {Convolutional {Neural} {Networks} over {Tree} {Structures} for {Programming} {Language} {Processing}},
	url = {http://dl.acm.org/citation.cfm?id=3015812.3016002},
	abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.},
	urldate = {2018-09-18},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
	year = {2016},
	pages = {1287--1293},
	file = {Mou et al. - 2016 - Convolutional Neural Networks over Tree Structures.pdf:/home/ryan/Zotero/storage/K7NVI7A6/Mou et al. - 2016 - Convolutional Neural Networks over Tree Structures.pdf:application/pdf},
}

@inproceedings{pred_xml2,
	series = {{ICDE} '06},
	title = {{XSEED}: {Accurate} and {Fast} {Cardinality} {Estimation} for {XPath} {Queries}},
	isbn = {978-0-7695-2570-9},
	shorttitle = {{XSEED}},
	url = {doi.ieeecomputersociety.org/10.1109/ICDE.2006.178},
	doi = {10.1109/ICDE.2006.178},
	abstract = {We propose XSEED, a synopsis of path queries for  cardinality estimation that is accurate, robust, efficient, and  adaptive to memory budgets. XSEED starts from a very  small kernel, and then incrementally updates information  of the synopsis. With such an incremental construction, a  synopsis structure can be dynamically configured to accommodate  different memory budgets. Cardinality estimation  based on XSEED can be performed very efficiently and  accurately. Extensive experiments on both synthetic and  real data sets show that even with less memory, XSEED  could achieve accuracy that is an order of magnitude better  than that of other synopsis structures. The cardinality  estimation time is under 2\% of the actual querying time for  a wide range of queries in all test cases.},
	urldate = {2018-09-18},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering}},
	author = {Ozsu, M. T. and Zhang, N. and Aboulnaga, A. and Ilyas, I. F.},
	year = {2006},
	keywords = {null},
	pages = {61},
	file = {Ozsu et al. - 2006 - XSEED Accurate and Fast Cardinality Estimation fo.pdf:/home/ryan/Zotero/storage/M3KZPV8N/Ozsu et al. - 2006 - XSEED Accurate and Fast Cardinality Estimation fo.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/GM594XRJ/25700061-abs.html:text/html},
}

@inproceedings{card_sampling,
	address = {New York, NY, USA},
	series = {{SIGMOD} '07},
	title = {Cardinality {Estimation} {Using} {Sample} {Views} with {Quality} {Assurance}},
	isbn = {978-1-59593-686-8},
	url = {http://doi.acm.org/10.1145/1247480.1247502},
	doi = {10.1145/1247480.1247502},
	abstract = {Accurate cardinality estimation is critically important to high-quality query optimization. It is well known that conventional cardinality estimation based on histograms or similar statistics may produce extremely poor estimates in a variety of situations, for example, queries with complex predicates, correlation among columns, or predicates containing user-defined functions. In this paper, we propose a new, general cardinality estimation technique that combines random sampling and materialized view technology to produce accurate estimates even in these situations. As a major innovation, we exploit feedback information from query execution and process control techniques to assure that estimates remain statistically valid when the underlying data changes. Experimental results based on a prototype implementation in Microsoft SQL Server demonstrate the practicality of the approach and illustrate the dramatic effects improved cardinality estimates may have.},
	urldate = {2018-09-18},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Larson, Per-Ake and Lehner, Wolfgang and Zhou, Jingren and Zabback, Peter},
	year = {2007},
	keywords = {cardinality estimation, query optimization, sample views, sequential sampling, statistical quality control},
	pages = {175--186},
	file = {Larson et al. - 2007 - Cardinality Estimation Using Sample Views with Qua.pdf:/home/ryan/Zotero/storage/WVD2NCAX/Larson et al. - 2007 - Cardinality Estimation Using Sample Views with Qua.pdf:application/pdf},
}

@inproceedings{tpcds,
	address = {Seoul, Korea},
	series = {{VLDB} '06},
	title = {The {Making} of {TPC}-{DS}},
	url = {http://dl.acm.org/citation.cfm?id=1182635.1164217},
	urldate = {2018-09-16},
	booktitle = {{VLDB}},
	publisher = {VLDB Endowment},
	author = {Nambiar, Raghunath Othayoth and Poess, Meikel},
	year = {2006},
	pages = {1049--1058},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/9QRUCT86/Nambiar and Poess - 2006 - The Making of TPC-DS.pdf:application/pdf},
}

@inproceedings{adam,
	address = {San Diego, CA},
	series = {{ICLR} '15},
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {https://arxiv.org/abs/1412.6980},
	language = {en},
	urldate = {2018-09-16},
	booktitle = {3rd {International} {Conference} for {Learning} {Representations}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2015},
	file = {Full Text PDF:/home/ryan/Zotero/storage/PMUFTRA8/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/E8CPXFMR/1412.html:text/html},
}

@article{word2vec,
	series = {{arXiv} '13},
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2018-09-16},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1301.3781 PDF:/home/ryan/Zotero/storage/J9CNCS4S/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/6WTVWX8G/1301.html:text/html},
}

@inproceedings{deep_entity,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {Deep {Learning} for {Entity} {Matching}: {A} {Design} {Space} {Exploration}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {Deep {Learning} for {Entity} {Matching}},
	url = {http://doi.acm.org/10.1145/3183713.3196926},
	doi = {10.1145/3183713.3196926},
	abstract = {Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can significantly outperform them on textual and dirty EM. For practitioners, this suggests that they should seriously consider using DL for textual and dirty EM problems. Finally, we analyze DL's performance and discuss future research directions.},
	urldate = {2018-08-21},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
	year = {2018},
	keywords = {deep learning, entity matching, entity resolution},
	pages = {19--34},
	file = {Mudgal et al. - 2018 - Deep Learning for Entity Matching A Design Space .pdf:/home/ryan/Zotero/storage/FK23BJF2/Mudgal et al. - 2018 - Deep Learning for Entity Matching A Design Space .pdf:application/pdf},
}

@inproceedings{transfer,
	series = {{ICML} {WUTL} '12},
	title = {Deep {Learning} of {Representations} for {Unsupervised} and {Transfer} {Learning}},
	url = {http://proceedings.mlr.press/v27/bengio12a.html},
	abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defi...},
	language = {en},
	urldate = {2018-08-11},
	booktitle = {Proceedings of {ICML} {Workshop} on {Unsupervised} and {Transfer} {Learning}},
	author = {Bengio, Yoshua},
	month = jun,
	year = {2012},
	pages = {17--36},
	file = {Full Text PDF:/home/ryan/Zotero/storage/PJYYMQG4/Bengio - 2012 - Deep Learning of Representations for Unsupervised .pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/LBRBBI69/bengio12a.html:text/html},
}

@article{transfer_rl,
	series = {{JMLR} '09},
	title = {Transfer {Learning} for {Reinforcement} {Learning} {Domains}: {A} {Survey}},
	volume = {10},
	issn = {ISSN 1533-7928},
	shorttitle = {Transfer {Learning} for {Reinforcement} {Learning} {Domains}},
	url = {http://www.jmlr.org/papers/v10/taylor09a.html},
	number = {Jul},
	urldate = {2018-08-11},
	journal = {Journal of Machine Learning Research},
	author = {Taylor, Matthew E. and Stone, Peter},
	year = {2009},
	pages = {1633--1685},
	file = {Full Text PDF:/home/ryan/Zotero/storage/4U828CW2/Taylor and Stone - 2009 - Transfer Learning for Reinforcement Learning Domai.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/JKWS4CK5/taylor09a.html:text/html},
}

@inproceedings{transfer2,
	address = {Cambridge, MA, USA},
	series = {{NIPS} '14},
	title = {How {Transferable} {Are} {Features} in {Deep} {Neural} {Networks}?},
	url = {http://dl.acm.org/citation.cfm?id=2969033.2969197},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	urldate = {2018-08-11},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {MIT Press},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
	pages = {3320--3328},
	file = {Yosinski et al. - 2014 - How Transferable Are Features in Deep Neural Netwo.pdf:/home/ryan/Zotero/storage/P4JFJDLX/Yosinski et al. - 2014 - How Transferable Are Features in Deep Neural Netwo.pdf:application/pdf},
}

@article{url-amazonAWS,
	title = {Amazon {Web} {Services}, http://aws.amazon.com/},
	url = {http://aws.amazon.com/},
	note = {tex.key= 1},
}

@inproceedings{cenLEALearnedEncoding2021,
	address = {New York, NY, USA},
	series = {{aiDM} @ {SIGMOD} '21},
	title = {{LEA}: {A} {Learned} {Encoding} {Advisor} for {Column} {Stores}},
	copyright = {All rights reserved},
	isbn = {978-1-4503-8535-0},
	shorttitle = {{LEA}},
	url = {https://doi.org/10.1145/3464509.3464885},
	doi = {10.1145/3464509.3464885},
	urldate = {2021-11-17},
	booktitle = {Fourth {Workshop} in {Exploiting} {AI} {Techniques} for {Data} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Cen, Lujing and Kipf, Andreas and Marcus, Ryan and Kraska, Tim},
	month = jun,
	year = {2021},
	pages = {32--35},
	file = {Full Text PDF:/home/ryan/Zotero/storage/4LKTDW8F/Cen et al. - 2021 - LEA A Learned Encoding Advisor for Column Stores.pdf:application/pdf},
}

@inproceedings{learned_benchmark,
	series = {{SMDB} @ {ICDE} '21},
	title = {Towards a {Benchmark} for {Learned} {Systems}},
	copyright = {All rights reserved},
	doi = {10.1109/ICDEW53142.2021.00029},
	booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Bindschaedler, Laurent and Kipf, Andreas and Kraska, Tim and Marcus, Ryan and Minhas, Umar Farooq},
	month = apr,
	year = {2021},
	note = {ISSN: 2473-3490},
	keywords = {machine learning, Databases, Conferences, data management, Training, Benchmark testing, Throughput, Measurement, benchmark, cost of ownership, instance-optimized system, learned system, Manuals, metrics},
	pages = {127--133},
	file = {Bindschaedler et al. - 2021 - Towards a Benchmark for Learned Systems.pdf:/home/ryan/Zotero/storage/QQXIF6FJ/Bindschaedler et al. - 2021 - Towards a Benchmark for Learned Systems.pdf:application/pdf},
}

@inproceedings{plex,
	series = {{aiDB} @ {VLDB} '21},
	title = {Towards {Practical} {Learned} {Indexing}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2108.05117},
	urldate = {2022-07-07},
	author = {Stoian, Mihail and Kipf, Andreas and Marcus, Ryan and Kraska, Tim},
	month = nov,
	year = {2021},
	note = {bibtex: plex},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases},
	file = {Stoian et al. - 2021 - Towards Practical Learned Indexing.pdf:/home/ryan/Zotero/storage/KX4JLWXK/Stoian et al. - 2021 - Towards Practical Learned Indexing.pdf:application/pdf},
}

@article{marcusBaoMakingLearned2022,
	series = {{SIGMOD} {Rec} '22},
	title = {Bao: {Making} {Learned} {Query} {Optimization} {Practical}},
	volume = {51},
	copyright = {All rights reserved},
	shorttitle = {Bao},
	url = {https://sigmodrecord.org/2022/05/02/bao-making-learned-query-optimization-practical/},
	language = {en-US},
	number = {1},
	urldate = {2022-05-09},
	journal = {SIGMOD Record},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
	month = mar,
	year = {2022},
	note = {Award: 'research highlight'},
	pages = {6--13},
	file = {Ryan Marcus et al. - 2022 - Bao Making Learned Query Optimization Practical.pdf:/home/ryan/Zotero/storage/A8ZBEP7T/Ryan Marcus et al. - 2022 - Bao Making Learned Query Optimization Practical.pdf:application/pdf},
}

@inproceedings{kipfLSILearnedSecondary2022,
	series = {{aiDM} @ {SIGMOD} '22},
	title = {{LSI}: {A} {Learned} {Secondary} {Index} {Structure}},
	copyright = {All rights reserved},
	author = {Kipf, Andreas and Horn, Dominik and Pfeil, Pascal and Marcus, Ryan and Kraska, Tim},
	month = jun,
	year = {2022},
	file = {Kipf et al. - 2022 - LSI A Learned Secondary Index Structure.pdf:/home/ryan/Zotero/storage/3N96F9MJ/Kipf et al. - 2022 - LSI A Learned Secondary Index Structure.pdf:application/pdf},
}

@article{flowloss,
	series = {{VLDB} '21},
	title = {Flow-loss: {Learning} cardinality estimates that matter},
	volume = {14},
	copyright = {All rights reserved},
	url = {http://www.vldb.org/pvldb/vol14/p2019-negi.pdf},
	doi = {10.14778/3476249.3476259},
	number = {11},
	journal = {Proc. VLDB Endow.},
	author = {Negi, Parimarjan and Marcus, Ryan and Kipf, Andreas and Mao, Hongzi and Tatbul, Nesime and Kraska, Tim and Alizadeh, Mohammad},
	year = {2021},
	pages = {2019--2032},
	file = {Negi et al. - 2021 - Flow-loss Learning cardinality estimates that mat.pdf:/home/ryan/Zotero/storage/V6TELXL4/Negi et al. - 2021 - Flow-loss Learning cardinality estimates that mat.pdf:application/pdf},
}

@article{other_lis_eval,
	title = {A {Critical} {Analysis} of {Recursive} {Model} {Indexes}},
	url = {http://arxiv.org/abs/2106.16166},
	urldate = {2022-04-11},
	journal = {arXiv:2106.16166 [cs]},
	author = {Maltry, Marcel and Dittrich, Jens},
	month = nov,
	year = {2021},
	note = {bibtex: other\_lis\_eval},
	keywords = {Computer Science - Databases, H.3.1},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/N5CWLXZP/Maltry and Dittrich - 2021 - A Critical Analysis of Recursive Model Indexes.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/IZWG23BI/2106.html:text/html},
}

@article{micro_alex,
	title = {Micro-architectural {Analysis} of a {Learned} {Index}},
	url = {http://arxiv.org/abs/2109.08495},
	urldate = {2022-04-11},
	journal = {arXiv:2109.08495 [cs]},
	author = {Andersen, Mikkel M√∏ller and T√∂z√ºn, Pƒ±nar},
	month = sep,
	year = {2021},
	note = {bibtex: micro\_alex},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/5L5UDWGG/Andersen and T√∂z√ºn - 2021 - Micro-architectural Analysis of a Learned Index.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/K5NPUEWV/2109.html:text/html},
}

@inproceedings{logloss,
	address = {New York, NY, USA},
	series = {{aiDM} '21},
	title = {A {Tailored} {Regression} for {Learned} {Indexes}: {Logarithmic} {Error} {Regression}},
	isbn = {978-1-4503-8535-0},
	shorttitle = {A {Tailored} {Regression} for {Learned} {Indexes}},
	url = {https://doi.org/10.1145/3464509.3464891},
	doi = {10.1145/3464509.3464891},
	urldate = {2022-04-10},
	booktitle = {Fourth {Workshop} in {Exploiting} {AI} {Techniques} for {Data} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Eppert, Martin and Fent, Philipp and Neumann, Thomas},
	month = jun,
	year = {2021},
	note = {bibtex: logloss},
	pages = {9--15},
}

@article{shift_table,
	title = {Shift-{Table}: {A} {Low}-latency {Learned} {Index} for {Range} {Queries} using {Model} {Correction}},
	shorttitle = {Shift-{Table}},
	url = {https://arxiv.org/abs/2101.10457v1},
	doi = {10.48550/arXiv.2101.10457},
	language = {en},
	urldate = {2022-04-11},
	author = {Hadian, Ali and Heinis, Thomas},
	month = jan,
	year = {2021},
	note = {bibtex: shift\_table},
	file = {Full Text PDF:/home/ryan/Zotero/storage/LHCAKJPG/Hadian and Heinis - 2021 - Shift-Table A Low-latency Learned Index for Range.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/HGYR4RMF/2101.html:text/html},
}

@inproceedings{lis_theory,
	series = {{PMLR} '20},
	title = {Why are learned indexes so effective?},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/ferragina20a.html},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {PMLR},
	author = {Ferragina, Paolo and Lillo, Fabrizio and Vinciguerra, Giorgio},
	editor = {III, Hal Daum√© and Singh, Aarti},
	month = jul,
	year = {2020},
	note = {bibtex: lis\_theory},
	pages = {3123--3132},
	file = {Ferragina et al. - 2020 - Why are learned indexes so effective.pdf:/home/ryan/Zotero/storage/SASR3DRE/Ferragina et al. - 2020 - Why are learned indexes so effective.pdf:application/pdf},
}

@article{indexable_dict,
	series = {{TALG} '07},
	title = {Succinct indexable dictionaries with applications to encoding k-ary trees, prefix sums and multisets},
	volume = {3},
	number = {4},
	journal = {ACM Transactions on Algorithms},
	author = {Raman, Rajeev and Raman, Venkatesh and Satti, Srinivasa Rao},
	year = {2007},
	note = {bibtex: indexable\_dict},
	pages = {43},
}

@phdthesis{succinct,
	type = {{PhD} {Thesis}},
	title = {Succinct static data structures},
	url = {https://www.proquest.com/dissertations-theses/succinct-static-data-structures/docview/303540527/se-2?accountid=12492},
	language = {English},
	author = {Jacobson, Guy J.},
	year = {1988},
	note = {bibtex: succinct},
	keywords = {Computer science, 0984:Computer science, Applied sciences},
}

@inproceedings{histtree,
	series = {{CIDR} '21},
	title = {Hist-{Tree}: {Those} {Who} {Ignore} {It} {Are} {Doomed} to {Learn}},
	booktitle = {11th {Annual} {Conference} on {Innovative} {Data} {Systems} {Researc}},
	author = {{Andrew Crotty}},
	year = {2021},
	note = {bibtex: histtree},
	file = {Andrew Crotty - 2021 - Hist-Tree Those Who Ignore It Are Doomed to Learn.pdf:/home/ryan/Zotero/storage/N9DALXHP/Andrew Crotty - 2021 - Hist-Tree Those Who Ignore It Are Doomed to Learn.pdf:application/pdf},
}

@article{wuUpdatableLearnedIndex2021,
	series = {{VLDB} '21},
	title = {Updatable learned index with precise positions},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3457390.3457393},
	doi = {10.14778/3457390.3457393},
	number = {8},
	urldate = {2022-04-11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Wu, Jiacheng and Zhang, Yong and Chen, Shimin and Wang, Jin and Chen, Yu and Xing, Chunxiao},
	month = apr,
	year = {2021},
	pages = {1276--1288},
	file = {Submitted Version:/home/ryan/Zotero/storage/8TJYSJ3X/Wu et al. - 2021 - Updatable learned index with precise positions.pdf:application/pdf},
}

@inproceedings{nashdb,
	address = {Houston, TX},
	series = {{SIGMOD} '18},
	title = {{NashDB}: {An} {End}-to-{End} {Economic} {Method} for {Elastic} {Database} {Fragmentation}, {Replication}, and {Provisioning}},
	copyright = {All rights reserved},
	shorttitle = {{NashDB}},
	doi = {https://doi.org/10.1145/3183713.3196935},
	booktitle = {Proceedings of the 37th {ACM} {Special} {Interest} {Group} in {Data} {Management}},
	author = {Marcus, Ryan and Papaemmanouil, Olga and Semenova, Sofiya and Garber, Solomon},
	year = {2018},
	file = {Ryan Marcus et al. - 2018 - NashDB An Economic Approach to Fragmentation, Rep.pdf:/home/ryan/Zotero/storage/35KTECTC/Ryan Marcus et al. - 2018 - NashDB An Economic Approach to Fragmentation, Rep.pdf:application/pdf},
}

@inproceedings{garberLowBitrateCompression2020,
	address = {Snowbird, Utah},
	series = {{DCC} '20},
	title = {Low {Bitrate} {Compression} of {Video} with {Dynamic} {Backgrounds}},
	copyright = {All rights reserved},
	doi = {10.1109/DCC47342.2020.00088},
	booktitle = {2020 {Data} {Compression} {Conference} ({DCC})},
	author = {Garber, Solomon and Marcus, Ryan and DiLillo, Antonella and Storer, James},
	year = {2020},
	file = {Garber et al. - 2020 - Low Bitrate Compression of Video with Dynamic Back.pdf:/home/ryan/Zotero/storage/LZSHT7SE/Garber et al. - 2020 - Low Bitrate Compression of Video with Dynamic Back.pdf:application/pdf},
}

@inproceedings{bao_scope,
	address = {Virtual Event China},
	series = {{SIGMOD} '21},
	title = {Steering {Query} {Optimizers}: {A} {Practical} {Take} on {Big} {Data} {Workloads}},
	copyright = {All rights reserved},
	isbn = {978-1-4503-8343-1},
	shorttitle = {Steering {Query} {Optimizers}},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457568},
	doi = {10.1145/3448016.3457568},
	language = {en},
	urldate = {2021-06-24},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Negi, Parimarjan and Interlandi, Matteo and Marcus, Ryan and Alizadeh, Mohammad and Kraska, Tim and Friedman, Marc and Jindal, Alekh},
	month = jun,
	year = {2021},
	note = {Award: 'best paper honorable mention'},
	pages = {2557--2569},
	file = {Negi et al. - 2021 - Steering Query Optimizers A Practical Take on Big.pdf:/home/ryan/Zotero/storage/FJSNTK9F/Negi et al. - 2021 - Steering Query Optimizers A Practical Take on Big.pdf:application/pdf},
}

@inproceedings{bao,
	address = {China},
	series = {{SIGMOD} '21},
	title = {Bao: {Making} {Learned} {Query} {Optimization} {Practical}},
	copyright = {All rights reserved},
	isbn = {978-1-4503-8343-1},
	doi = {10.1145/3448016.3452838},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
	month = jun,
	year = {2021},
	note = {Award: 'best paper award'},
	file = {Ryan Marcus et al. - 2021 - Bao Making Learned Query Optimization Practical.pdf:/home/ryan/Zotero/storage/6IHZS47B/Ryan Marcus et al. - 2021 - Bao Making Learned Query Optimization Practical.pdf:application/pdf},
}

@article{bao_online,
	title = {Bao: {Learning} to {Steer} {Query} {Optimizers}},
	shorttitle = {Bao},
	url = {http://arxiv.org/abs/2004.03814},
	urldate = {2020-05-21},
	journal = {arXiv:2004.03814 [cs]},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Databases},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/QBYXMZHF/Marcus et al. - 2020 - Bao Learning to Steer Query Optimizers.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/7W5RRD69/2004.html:text/html},
}

@article{pandas,
	series = {{PHPSC} '11},
	title = {pandas: a foundational {Python} library for data analysis and statistics},
	volume = {14},
	number = {9},
	journal = {Python for High Performance and Scientific Computing},
	author = {McKinney, Wes},
	year = {2011},
	pages = {1--9},
}

@article{url-zstd,
	title = {{ZSTD} {Compression} {Library}, https://facebook.github.io/zstd/},
	url = {https://facebook.github.io/zstd/},
	note = {tex.key= 1},
}

@article{bao_github,
	title = {Bao for {PostgreSQL} prototype, https://learned.systems/bao},
	url = {https://learned.systems/bao},
}

@inproceedings{tableauworkload,
	series = {{DBTest}@{SIGMOD} '18},
	title = {Get {Real}: {How} {Benchmarks} {Fail} to {Represent} the {Real} {World}},
	url = {https://doi.org/10.1145/3209950.3209952},
	doi = {10.1145/3209950.3209952},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Testing} {Database} {Systems}},
	publisher = {ACM},
	author = {Vogelsgesang, Adrian and Haubenschild, Michael and Finis, Jan and Kemper, Alfons and Leis, Viktor and M√ºhlbauer, Tobias and Neumann, Thomas and Then, Manuel},
	editor = {B√∂hm, Alexander and Rabl, Tilmann},
	year = {2018},
	pages = {1:1--1:6},
}

@inproceedings{sqlshare,
	series = {{SIGMOD} '16},
	title = {{SQLShare}: {Results} from a {Multi}-{Year} {SQL}-as-a-{Service} {Experiment}},
	url = {https://doi.org/10.1145/2882903.2882957},
	doi = {10.1145/2882903.2882957},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Jain, Shrainik and Moritz, Dominik and Halperin, Daniel and Howe, Bill and Lazowska, Ed},
	editor = {√ñzcan, Fatma and Koutrika, Georgia and Madden, Sam},
	year = {2016},
	pages = {281--293},
}

@article{query2vec,
	series = {{arXiv} '18},
	title = {{Query2Vec}: {An} {Evaluation} of {NLP} {Techniques} for {Generalized} {Workload} {Analytics}},
	shorttitle = {{Query2Vec}},
	url = {http://arxiv.org/abs/1801.05613},
	urldate = {2020-11-21},
	journal = {arXiv:1801.05613 [cs]},
	author = {Jain, Shrainik and Howe, Bill and Yan, Jiaqi and Cruanes, Thierry},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Databases, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/GIUX74EP/Jain et al. - 2018 - Query2Vec An Evaluation of NLP Techniques for Gen.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/2JK24GC5/1801.html:text/html},
}

@inproceedings{salt_pareto,
	series = {{OSDI} '14},
	title = {Salt: {Combining} \{{ACID}\} and \{{BASE}\} in a {Distributed} {Database}},
	isbn = {978-1-931971-16-4},
	shorttitle = {Salt},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/xie},
	language = {en},
	urldate = {2020-11-21},
	author = {Xie, Chao and Su, Chunzhi and Kapritsos, Manos and Wang, Yang and Yaghmazadeh, Navid and Alvisi, Lorenzo and Mahajan, Prince},
	year = {2014},
	pages = {495--509},
	file = {Full Text PDF:/home/ryan/Zotero/storage/28F2NQNB/Xie et al. - 2014 - Salt Combining ACID and BASE in a Distributed.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/SZQE4ZZA/xie.html:text/html},
}

@inproceedings{som_cardinality3,
	address = {USA},
	series = {{ICDM} '15},
	title = {Learning {Set} {Cardinality} in {Distance} {Nearest} {Neighbours}},
	isbn = {978-1-4673-9504-5},
	url = {https://doi.org/10.1109/ICDM.2015.17},
	doi = {10.1109/ICDM.2015.17},
	urldate = {2020-11-21},
	booktitle = {Proceedings of the 2015 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	publisher = {IEEE Computer Society},
	author = {Anagnostopoulos, Christos and Triantafillou, Peter},
	month = nov,
	year = {2015},
	pages = {691--696},
	file = {Accepted Version:/home/ryan/Zotero/storage/7KYSJX5W/Anagnostopoulos and Triantafillou - 2015 - Learning Set Cardinality in Distance Nearest Neigh.pdf:application/pdf},
}

@inproceedings{som_cardinality1,
	series = {Big {Data} '15},
	title = {Learning to accurately {COUNT} with query-driven predictive analytics},
	doi = {10.1109/BigData.2015.7363736},
	booktitle = {2015 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Anagnostopoulos, C. and Triantafillou, P.},
	month = oct,
	year = {2015},
	keywords = {machine learning, Big Data, big data analytics, optimisation, query processing, Data models, learning (artificial intelligence), Data mining, Aggregates, aggregation operator, aggregation operators, aggregation-query analytics, Big data, COUNT predictive learning, data system optimisation, data-centric method, Histograms, incremental learning, multidimensional histograms, Predictive models, query similarity, query-driven analytics, query-driven ML model, query-driven predictive analytics, range queries, sampling, Sampling methods, self-organized map regression, self-tuning histogram, set-cardinality},
	pages = {14--23},
	file = {Anagnostopoulos and Triantafillou - 2015 - Learning to accurately COUNT with query-driven pre.pdf:/home/ryan/Zotero/storage/Z2Q73KIH/Anagnostopoulos and Triantafillou - 2015 - Learning to accurately COUNT with query-driven pre.pdf:application/pdf},
}

@article{anagnostopoulosQueryDrivenLearningPredictive2017,
	series = {{TKDD} '17},
	title = {Query-{Driven} {Learning} for {Predictive} {Analytics} of {Data} {Subspace} {Cardinality}},
	volume = {11},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3059177},
	doi = {10.1145/3059177},
	number = {4},
	urldate = {2020-11-21},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Anagnostopoulos, Christos and Triantafillou, Peter},
	month = jun,
	year = {2017},
	keywords = {analytics selection queries, data subspace exploration, optimal stopping theory, Predictive analytics, predictive learning, vector regression quantization},
	pages = {47:1--47:46},
	file = {Accepted Version:/home/ryan/Zotero/storage/5UIVZMZA/Anagnostopoulos and Triantafillou - 2017 - Query-Driven Learning for Predictive Analytics of .pdf:application/pdf},
}

@article{benchmark_lis,
	series = {{VLDB} '21},
	title = {Benchmarking {Learned} {Indexes}},
	volume = {14},
	copyright = {All rights reserved},
	doi = {10.14778/3421424.3421425},
	number = {1},
	journal = {PVLDB},
	author = {Marcus, Ryan and Kipf, Andreas and Renen, Alexander Van and Stoian, Mihail and Misra, Sanchit and Kemper, Alfons and Neumann, Thomas and Kraska, Tim},
	month = sep,
	year = {2020},
	note = {Citation Key: benchmark\_lis},
	pages = {1--13},
	file = {Marcus et al. - 2020 - Benchmarking Learned Indexes.pdf:/home/ryan/Zotero/storage/V6XRI8K5/Marcus et al. - 2020 - Benchmarking Learned Indexes.pdf:application/pdf},
}

@inproceedings{park,
	series = {{NeurIPS} '19},
	title = {Park: {An} {Open} {Platform} for {Learning}-{Augmented} {Computer} {Systems}},
	copyright = {All rights reserved},
	shorttitle = {Park},
	url = {http://papers.nips.cc/paper/8519-park-an-open-platform-for-learning-augmented-computer-systems.pdf},
	doi = {10.5555/3454287.3454511},
	urldate = {2019-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Mao, Hongzi and Negi, Parimarjan and Narayan, Akshay and Wang, Hanrui and Yang, Jiacheng and Wang, Haonan and Marcus, Ryan and addanki, ravichandra and Khani Shirkoohi, Mehrdad and He, Songtao and Nathan, Vikram and Cangialosi, Frank and Venkatakrishnan, Shaileshh and Weng, Wei-Hung and Han, Song and Kraska, Tim and Alizadeh, Mohammad},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alche-Buc, F. d and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {2490--2502},
	file = {Mao et al. - 2019 - Park An Open Platform for Learning-Augmented Comp.pdf:/home/ryan/Zotero/storage/Z7PPGK5Q/Mao et al. - 2019 - Park An Open Platform for Learning-Augmented Comp.pdf:application/pdf;NIPS Snapshot:/home/ryan/Zotero/storage/XHEYF8ZC/8519-park-an-open-platform-for-learning-augmented-computer-systems.html:text/html},
}

@inproceedings{ml_index,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {The {Case} for {Learned} {Index} {Structures}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {Learned {Index} {Structures}},
	url = {http://doi.acm.org/10.1145/3183713.3196909},
	doi = {10.1145/3183713.3196909},
	abstract = {Indexes are models: a {\textbackslash}btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term {\textbackslash}em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
	urldate = {2018-08-21},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
	year = {2018},
	keywords = {b-tree, bloom-filter, cdf, hash-map, index structures, learned data structures, learned index, learned index structure, linear regression, mixture of experts, neural net},
	file = {Kraska et al. - 2018 - The Case for Learned Index Structures.pdf:/home/ryan/Zotero/storage/RUSRH445/Kraska et al. - 2018 - The Case for Learned Index Structures.pdf:application/pdf},
}

@inproceedings{lstm_jo,
	series = {{ICDE} '20},
	title = {Reinforcement {Learning} with {Tree}-{LSTM} for {Join} {Order} {Selection}},
	doi = {10.1109/ICDE48307.2020.00116},
	booktitle = {2020 {IEEE} 36th {International} {Conference} on {Data} {Engineering}},
	author = {Yu, Xiang and Li, Guoliang and Chai, Chengliang and Tang, Nan},
	month = apr,
	year = {2020},
	note = {ISSN: 2375-026X},
	keywords = {Databases, query processing, Machine learning, learning (artificial intelligence), Neural networks, database management systems, Training, Benchmark testing, SQL, dynamic programming, Vegetation, complicated SQL queries, database query optimizers, database schema, deep reinforcement learning, DRL-based approaches, fixed-length feature vectors, fixed-length handtuned feature vectors, Forestry, JOB, join order selection, join tree, LSTM, oin order benchmark, poor join plans, recurrent neural nets, RTOS, solution space, TPC-H, traditional optimizers, Tree-LSTM, tree-structured, trees (mathematics)},
	pages = {1297--1308},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/48BWECK5/9101694.html:text/html},
}

@inproceedings{plan_loss,
	series = {{SMDB} @ {ICDE} '20},
	title = {Cost-{Guided} {Cardinality} {Estimation}: {Focus} {Where} it {Matters}},
	copyright = {All rights reserved},
	booktitle = {Workshop on {Self}-{Managing} {Databases}},
	author = {Negi, Parimarjan and Marcus, Ryan and Mao, Hongzi and Tatbul, Nesime and Kraska, Tim and Alizadeh, Mohammad},
	year = {2020},
	file = {Negi et al. - 2020 - Cost-Guided Cardinality Estimation Focus Where it.pdf:/home/ryan/Zotero/storage/RXSMJHCT/Negi et al. - 2020 - Cost-Guided Cardinality Estimation Focus Where it.pdf:application/pdf},
}

@article{neurocard,
	series = {{arXiv} '20},
	title = {{NeuroCard}: {One} {Cardinality} {Estimator} for {All} {Tables}},
	shorttitle = {{NeuroCard}},
	url = {http://arxiv.org/abs/2006.08109},
	urldate = {2020-07-05},
	journal = {arXiv:2006.08109 [cs]},
	author = {Yang, Zongheng and Kamsetty, Amog and Luan, Sifei and Liang, Eric and Duan, Yan and Chen, Xi and Stoica, Ion},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08109},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases},
	file = {arXiv Fulltext PDF:/home/ryan/Zotero/storage/TJXPSCR6/Yang et al. - 2020 - NeuroCard One Cardinality Estimator for All Table.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/ZML2NWPC/2006.html:text/html},
}

@article{oracle_sql_analyzer,
	series = {{CSTC} '08},
	title = {Oracle's {SQL} {Performance} {Analyzer}},
	journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
	author = {{Khaled Yagoub} and {Pete Belknap} and {Benoit Dageville} and {Karl Dias} and {Shantanu Joshi} and {Hailing Yu}},
	year = {2008},
	file = {Khaled Yagoub et al. - 2008 - Oracle's SQL Performance Analyzer.pdf:/home/ryan/Zotero/storage/2Y8XFNJ3/Khaled Yagoub et al. - 2008 - Oracle's SQL Performance Analyzer.pdf:application/pdf},
}

@incollection{ms_query_store,
	address = {Berkeley, CA},
	title = {What {Is} {Query} {Store}?},
	isbn = {978-1-4842-5004-4},
	url = {https://doi.org/10.1007/978-1-4842-5004-4_1},
	language = {en},
	urldate = {2020-07-05},
	booktitle = {Query {Store} for {SQL} {Server} 2019: {Identify} and {Fix} {Poorly} {Performing} {Queries}},
	publisher = {Apress},
	author = {Boggiano, Tracy and Fritchey, Grant},
	editor = {Boggiano, Tracy and Fritchey, Grant},
	year = {2019},
	doi = {10.1007/978-1-4842-5004-4_1},
	pages = {1--29},
}

@inproceedings{rejoin_tail,
	address = {Portland, Oregon},
	series = {{aiDM} '20},
	title = {Research challenges in deep reinforcement learning-based join query optimization},
	isbn = {978-1-4503-8029-4},
	url = {https://doi.org/10.1145/3401071.3401657},
	doi = {10.1145/3401071.3401657},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on {Exploiting} {Artificial} {Intelligence} {Techniques} for {Data} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Runsheng Benson and Daudjee, Khuzaima},
	month = jun,
	year = {2020},
	pages = {1--6},
	file = {Full Text PDF:/home/ryan/Zotero/storage/87Z2CWQR/Guo and Daudjee - 2020 - Research challenges in deep reinforcement learning.pdf:application/pdf},
}

@article{linear_reg_lookup,
	title = {Efficient parallel lists intersection and index compression algorithms using graphics processing units},
	volume = {4},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2002974.2002975},
	doi = {10.14778/2002974.2002975},
	number = {8},
	urldate = {2020-06-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ao, Naiyong and Zhang, Fan and Wu, Di and Stones, Douglas S. and Wang, Gang and Liu, Xiaoguang and Liu, Jing and Lin, Sheng},
	month = may,
	year = {2011},
	pages = {470--481},
	file = {Full Text PDF:/home/ryan/Zotero/storage/Z2Q4YL84/Ao et al. - 2011 - Efficient parallel lists intersection and index co.pdf:application/pdf},
}

@article{url-cpp-lb,
	title = {C++ lower\_bound, http://cplusplus.com/reference/algorithm/lower\_bound/},
	url = {http://cplusplus.com/reference/algorithm/lower_bound/},
	note = {tex.key= 1},
}

@inproceedings{learned_gc,
	series = {{MAPL} @ {PLDI} '20},
	title = {Learned {Garbage} {Collection}},
	copyright = {All rights reserved},
	doi = {https://doi.org/10.1145/3394450.3397469},
	booktitle = {Proceedings of the 4th {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages}},
	publisher = {ACM},
	author = {Cen, Lujing and Marcus, Ryan and Mao, Hongzi and Gottschlich, Justin and Alizadeh, Mohammad and Kraska, Tim},
	year = {2020},
	file = {Cen et al. - 2020 - Learned Garbage Collection.pdf:/home/ryan/Zotero/storage/ML4JSXJA/Cen et al. - 2020 - Learned Garbage Collection.pdf:application/pdf},
}

@inproceedings{radix-spline,
	address = {Portland, Oregon},
	series = {{aiDM} @ {SIGMOD} '20},
	title = {{RadixSpline}: a single-pass learned index},
	copyright = {All rights reserved},
	isbn = {978-1-4503-8029-4},
	shorttitle = {{RadixSpline}},
	url = {https://doi.org/10.1145/3401071.3401659},
	doi = {10.1145/3401071.3401659},
	urldate = {2020-06-05},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on {Exploiting} {Artificial} {Intelligence} {Techniques} for {Data} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Kipf, Andreas and Marcus, Ryan and van Renen, Alexander and Stoian, Mihail and Kemper, Alfons and Kraska, Tim and Neumann, Thomas},
	month = jun,
	year = {2020},
	pages = {1--5},
	file = {Kipf et al. - 2020 - RadixSpline a single-pass learned index.pdf:/home/ryan/Zotero/storage/B8IE7K6Y/Kipf et al. - 2020 - RadixSpline a single-pass learned index.pdf:application/pdf},
}

@inproceedings{rejoin,
	address = {Houston, TX},
	series = {{aiDM} @ {SIGMOD} '18},
	title = {Deep {Reinforcement} {Learning} for {Join} {Order} {Enumeration}},
	copyright = {All rights reserved},
	shorttitle = {{ReJOIN}},
	booktitle = {First {International} {Workshop} on {Exploiting} {Artificial} {Intelligence} {Techniques} for {Data} {Management}},
	author = {Marcus, Ryan and Papaemmanouil, Olga},
	year = {2018},
	file = {Marcus and Papaemmanouil - 2018 - Deep Reinforcement Learning for Join Order Enumera.pdf:/home/ryan/Zotero/storage/7M68MFPC/Marcus and Papaemmanouil - 2018 - Deep Reinforcement Learning for Join Order Enumera.pdf:application/pdf;Marcus and Papaemmanouil - 2018 - Deep Reinforcement Learning for Join Order Enumera.pdf:/home/ryan/Zotero/storage/J2AI6J55/Marcus and Papaemmanouil - 2018 - Deep Reinforcement Learning for Join Order Enumera.pdf:application/pdf},
}

@inproceedings{arda,
	series = {{VLDB} '20},
	title = {{ARDA}: {Automatic} {Relational} {Data} {Augmentation} for {Machine} {Learning}},
	volume = {13(9)},
	copyright = {All rights reserved},
	doi = {https://doi.org/10.14778/3397230.3397237},
	booktitle = {{PVLDB}},
	author = {Chepurko, Nadiia and Marcus, Ryan and Zgraggen, Emanuel and Fernandez, Raul Castro and Kraska, Tim and Karger, David},
	year = {2020},
	file = {Nadiia Chepurko et al. - 2020 - ARDA Automatic Relational Data Augmentation for M.pdf:/home/ryan/Zotero/storage/3BNY5LMZ/Nadiia Chepurko et al. - 2020 - ARDA Automatic Relational Data Augmentation for M.pdf:application/pdf},
}

@article{url-sosd,
	title = {Searching on sorted data benchmark, https://learned.systems/sosd},
	url = {https://learned.systems/sosd},
	note = {tex.key= 1},
}

@inproceedings{cache-tables,
	address = {San Francisco},
	series = {{VLDB} '03},
	title = {Cache {Tables}: {Paving} the {Way} for an {Adaptive} {Database} {Cache}},
	isbn = {978-0-12-722442-8},
	shorttitle = {Cache {Tables}},
	url = {http://www.sciencedirect.com/science/article/pii/B9780127224428500690},
	doi = {10.1016/B978-012722442-8/50069-0},
	language = {en},
	urldate = {2020-05-25},
	booktitle = {Proceedings 2003 {VLDB} {Conference}},
	publisher = {Morgan Kaufmann},
	author = {Altinel, Mehmet and Bornh√∂vd, Christof and Krishnamurthy, Sailesh and Mohan, C. and Pirahesh, Hamid and Reinwald, Berthold},
	editor = {Freytag, Johann-Christoph and Lockemann, Peter and Abiteboul, Serge and Carey, Michael and Selinger, Patricia and Heuer, Andreas},
	month = jan,
	year = {2003},
	pages = {718--729},
	file = {ScienceDirect Snapshot:/home/ryan/Zotero/storage/XQB5VNJN/B9780127224428500690.html:text/html},
}

@misc{stanfordblog,
	title = {Don't {Throw} {Out} {Your} {Algorithms} {Book} {Just} {Yet}: {Classical} {Data} {Structures} {That} {Can} {Outperform} {Learned} {Indexes} (blog post), https://dawn.cs.stanford.edu/2018/01/11/index-baselines/},
	shorttitle = {Don't {Throw} {Out} {Your} {Algorithms} {Book} {Just} {Yet}},
	url = {https://dawn.cs.stanford.edu//2018/01/11/index-baselines/},
	language = {en},
	urldate = {2020-05-25},
	author = {{Peter Bailis} and {Kai Sheng Tai} and {Pratiksha Thaker} and {Matei Zaharia}},
	year = {2018},
	note = {Library Catalog: dawn.cs.stanford.edu},
	file = {Snapshot:/home/ryan/Zotero/storage/ZWTXYW4R/index-baselines.html:text/html},
}

@misc{caseforbtree,
	title = {The {Case} for {B}-{Tree} {Index} {Structures} (blog post), http://databasearchitects.blogspot.com/2017/12/the-case-for-b-tree-index-structures.html},
	url = {http://databasearchitects.blogspot.com/2017/12/the-case-for-b-tree-index-structures.html},
	urldate = {2020-05-25},
	journal = {Database Architects},
	author = {{Peter Boncz} and {Thomas Neumann}},
	year = {2017},
	note = {tex.url: http://databasearchitects.blogspot.com/2017/12/the-case-for-b-tree-index-structures.html},
}

@inproceedings{locally-connected,
	series = {Connectionism '89},
	title = {Generalization and network design strategies},
	url = {https://nyuscholars.nyu.edu/en/publications/generalization-and-network-design-strategies},
	language = {English (US)},
	urldate = {2020-05-25},
	booktitle = {Connectionism in perspective},
	author = {Lecun, Yann},
	year = {1989},
	file = {Snapshot:/home/ryan/Zotero/storage/I9NAUA7P/generalization-and-network-design-strategies.html:text/html},
}

@inproceedings{mem-cache,
	address = {Paris, France},
	series = {{SIGMOD} '04},
	title = {Buffering databse operations for enhanced instruction cache performance},
	isbn = {978-1-58113-859-7},
	url = {https://doi.org/10.1145/1007568.1007592},
	doi = {10.1145/1007568.1007592},
	urldate = {2020-05-25},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Jingren and Ross, Kenneth A.},
	month = jun,
	year = {2004},
	pages = {191--202},
	file = {Full Text PDF:/home/ryan/Zotero/storage/LVWAU9X4/Zhou and Ross - 2004 - Buffering databse operations for enhanced instruct.pdf:application/pdf},
}

@inproceedings{cache-augment,
	address = {New York, New York},
	series = {{DBSocial} '13},
	title = {Cache augmented database management systems},
	isbn = {978-1-4503-2191-4},
	url = {https://doi.org/10.1145/2484702.2484709},
	doi = {10.1145/2484702.2484709},
	urldate = {2020-05-25},
	booktitle = {Proceedings of the {ACM} {SIGMOD} {Workshop} on {Databases} and {Social} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Ghandeharizadeh, Shahram and Yap, Jason},
	month = jun,
	year = {2013},
	keywords = {middle-tier caches, physical data independence},
	pages = {31--36},
	file = {Full Text PDF:/home/ryan/Zotero/storage/9BN5TFAB/Ghandeharizadeh and Yap - 2013 - Cache augmented database management systems.pdf:application/pdf},
}

@article{url-rocksdb,
	title = {{RocksDB}, https://rocksdb.org/},
	url = {https://rocksdb.org/},
	note = {tex.key= 1},
}

@article{naru,
	series = {{VLDB} '19},
	title = {Deep unsupervised cardinality estimation},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3368289.3368294},
	doi = {10.14778/3368289.3368294},
	number = {3},
	urldate = {2020-01-30},
	journal = {Proceedings of the VLDB Endowment},
	author = {Yang, Zongheng and Liang, Eric and Kamsetty, Amog and Wu, Chenggang and Duan, Yan and Chen, Xi and Abbeel, Pieter and Hellerstein, Joseph M. and Krishnan, Sanjay and Stoica, Ion},
	month = nov,
	year = {2019},
	pages = {279--292},
	file = {Full Text PDF:/home/ryan/Zotero/storage/HXGMNBZJ/Yang et al. - 2019 - Deep unsupervised cardinality estimation.pdf:application/pdf},
}

@techreport{inductive_bias_ml,
	title = {The {Need} for {Biases} in {Learning} {Generalizations}},
	author = {Mitchell, Tom M.},
	year = {1980},
	file = {Citeseer - Snapshot:/home/ryan/Zotero/storage/7MF3K975/summary.html:text/html;Mitchell - 1980 - The Need for Biases in Learning Generalizations.pdf:/home/ryan/Zotero/storage/IREAWEKR/Mitchell - 1980 - The Need for Biases in Learning Generalizations.pdf:application/pdf},
}

@article{inductive_bias_rl,
	series = {{arXiv} '18},
	title = {Innateness, {AlphaZero}, and {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1801.05667},
	urldate = {2019-02-16},
	journal = {arXiv:1801.05667 [cs]},
	author = {Marcus, Gary},
	month = jan,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, 97R40, I.2.0, I.2.6},
	file = {arXiv.org Snapshot:/home/ryan/Zotero/storage/24YIZHPM/1801.html:text/html;Marcus - 2018 - Innateness, AlphaZero, and Artificial Intelligence.pdf:/home/ryan/Zotero/storage/DHEX6QYZ/Marcus - 2018 - Innateness, AlphaZero, and Artificial Intelligence.pdf:application/pdf},
}

@inproceedings{lis-survey,
	series = {Studies in {Computational} {Intelligence}},
	title = {Learned data structures},
	volume = {896},
	isbn = {978-3-030-43883-8},
	booktitle = {Recent {Trends} in {Learning} {From} {Data}},
	publisher = {Springer},
	author = {Ferragina, Paolo and Vinciguerra, Giorgio},
	year = {2020},
	file = {Ferragina and Vinciguerra - 2020 - Learned data structures.pdf:/home/ryan/Zotero/storage/P39XUJ9N/Ferragina and Vinciguerra - 2020 - Learned data structures.pdf:application/pdf},
}

@inproceedings{faster,
	address = {Houston, TX, USA},
	series = {{SIGMOD} '18},
	title = {{FASTER}: {A} {Concurrent} {Key}-{Value} {Store} with {In}-{Place} {Updates}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {{FASTER}},
	url = {https://doi.org/10.1145/3183713.3196898},
	doi = {10.1145/3183713.3196898},
	urldate = {2020-05-22},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Chandramouli, Badrish and Prasaad, Guna and Kossmann, Donald and Levandoski, Justin and Hunter, James and Barnett, Mike},
	month = may,
	year = {2018},
	keywords = {concurrent, hash index, high performance, key-value store, latch-free, log-structured storage},
	pages = {275--290},
	file = {Full Text PDF:/home/ryan/Zotero/storage/WS23MKBD/Chandramouli et al. - 2018 - FASTER A Concurrent Key-Value Store with In-Place.pdf:application/pdf},
}

@article{lsh1,
	title = {{SK}-{LSH}: an efficient index structure for approximate nearest neighbor search},
	volume = {7},
	issn = {2150-8097},
	shorttitle = {{SK}-{LSH}},
	url = {https://doi.org/10.14778/2732939.2732947},
	doi = {10.14778/2732939.2732947},
	number = {9},
	urldate = {2020-05-22},
	journal = {Proceedings of the VLDB Endowment},
	author = {Liu, Yingfan and Cui, Jiangtao and Huang, Zi and Li, Hui and Shen, Heng Tao},
	month = may,
	year = {2014},
	pages = {745--756},
	file = {Full Text PDF:/home/ryan/Zotero/storage/WT2LYNYF/Liu et al. - 2014 - SK-LSH an efficient index structure for approxima.pdf:application/pdf},
}

@article{peephole,
	series = {{CACM} '65},
	title = {Peephole optimization},
	volume = {8},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/364995.365000},
	doi = {10.1145/364995.365000},
	abstract = {Redundant instructions may be discarded during the final stage of compilation by using a simple optimizing technique called peephole optimization. The method is described and examples are given.},
	number = {7},
	urldate = {2020-05-22},
	journal = {Communications of the ACM},
	author = {McKeeman, W. M.},
	month = jul,
	year = {1965},
	pages = {443--444},
	file = {Full Text PDF:/home/ryan/Zotero/storage/BS7FHQYK/McKeeman - 1965 - Peephole optimization.pdf:application/pdf},
}

@inproceedings{hash_index,
	address = {Carlsbad, CA, USA},
	series = {{OSDI}'18},
	title = {Write-optimized and high-performance hashing index scheme for persistent memory},
	isbn = {978-1-931971-47-8},
	urldate = {2020-05-22},
	booktitle = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Zuo, Pengfei and Hua, Yu and Wu, Jie},
	month = oct,
	year = {2018},
	pages = {461--476},
}

@article{slimdb,
	title = {{SlimDB}: a space-efficient key-value storage engine for semi-sorted data},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {{SlimDB}},
	url = {https://doi.org/10.14778/3151106.3151108},
	doi = {10.14778/3151106.3151108},
	number = {13},
	urldate = {2020-05-22},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ren, Kai and Zheng, Qing and Arulraj, Joy and Gibson, Garth},
	month = sep,
	year = {2017},
	pages = {2037--2048},
	file = {Full Text PDF:/home/ryan/Zotero/storage/ASWBNWLB/Ren et al. - 2017 - SlimDB a space-efficient key-value storage engine.pdf:application/pdf},
}

@article{lsh2,
	title = {{PM}-{LSH}: {A} fast and accurate {LSH} framework for high-dimensional approximate {NN} search},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {{PM}-{LSH}},
	url = {https://doi.org/10.14778/3377369.3377374},
	doi = {10.14778/3377369.3377374},
	number = {5},
	urldate = {2020-05-22},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zheng, Bolong and Zhao, Xi and Weng, Lianggui and Hung, Nguyen Quoc Viet and Liu, Hang and Jensen, Christian S.},
	month = jan,
	year = {2020},
	pages = {643--655},
	file = {Full Text PDF:/home/ryan/Zotero/storage/IBW5UW5F/Zheng et al. - 2020 - PM-LSH A fast and accurate LSH framework for high.pdf:application/pdf},
}

@article{dash,
	title = {Dash: scalable hashing on persistent memory},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {Dash},
	url = {https://doi.org/10.14778/3389133.3389134},
	doi = {10.14778/3389133.3389134},
	number = {8},
	urldate = {2020-05-22},
	journal = {Proceedings of the VLDB Endowment},
	author = {Lu, Baotong and Hao, Xiangpeng and Wang, Tianzheng and Lo, Eric},
	month = apr,
	year = {2020},
	pages = {1147--1161},
	file = {Full Text PDF:/home/ryan/Zotero/storage/XUIXGYX4/Lu et al. - 2020 - Dash scalable hashing on persistent memory.pdf:application/pdf},
}

@article{hashing_eval,
	series = {{VLDB} '15},
	title = {A seven-dimensional analysis of hashing methods and its implications on query processing},
	volume = {9},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2850583.2850585},
	doi = {10.14778/2850583.2850585},
	number = {3},
	urldate = {2020-05-22},
	journal = {Proceedings of the VLDB Endowment},
	author = {Richter, Stefan and Alvarez, Victor and Dittrich, Jens},
	month = nov,
	year = {2015},
	pages = {96--107},
	file = {Full Text PDF:/home/ryan/Zotero/storage/JXDXMARP/Richter et al. - 2015 - A seven-dimensional analysis of hashing methods an.pdf:application/pdf},
}

@article{exponential-search,
	series = {{IPL} '76},
	title = {An almost optimal algorithm for unbounded searching},
	volume = {5},
	issn = {0020-0190},
	url = {http://www.sciencedirect.com/science/article/pii/0020019076900715},
	doi = {10.1016/0020-0190(76)90071-5},
	language = {en},
	number = {3},
	urldate = {2020-05-22},
	journal = {Information Processing Letters},
	author = {Bentley, Jon Louis and Yao, Andrew Chi-Chih},
	month = aug,
	year = {1976},
	keywords = {analysis of algorithms, optimal algorithms, Ordered table searching, representation of integers},
	pages = {82--87},
	file = {ScienceDirect Snapshot:/home/ryan/Zotero/storage/SFL4NYZJ/0020019076900715.html:text/html;Submitted Version:/home/ryan/Zotero/storage/BQBPWYDG/Bentley and Yao - 1976 - An almost optimal algorithm for unbounded searchin.pdf:application/pdf},
}

@inproceedings{deep_sketch,
	address = {Amsterdam, Netherlands},
	series = {{SIGMOD} '19},
	title = {Estimating {Cardinalities} with {Deep} {Sketches}},
	isbn = {978-1-4503-5643-5},
	url = {https://doi.org/10.1145/3299869.3320218},
	doi = {10.1145/3299869.3320218},
	urldate = {2020-05-21},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Kipf, Andreas and Vorona, Dimitri and M√ºller, Jonas and Kipf, Thomas and Radke, Bernhard and Leis, Viktor and Boncz, Peter and Neumann, Thomas and Kemper, Alfons},
	month = jun,
	year = {2019},
	keywords = {cardinality estimation, ml for databases},
	pages = {1937--1940},
	file = {Full Text PDF:/home/ryan/Zotero/storage/GTY7JH54/Kipf et al. - 2019 - Estimating Cardinalities with Deep Sketches.pdf:application/pdf},
}

@article{url-tsl-robin,
	title = {{RobinMap}, https://github.com/{Tessil}/robin-map},
	url = {https://github.com/Tessil/robin-map},
	note = {tex.key= 1},
}

@article{url-simd-cuckoo,
	title = {{SIMD} {Cuckoo} {Hash}, https://github.com/stanford-futuredata/index-baselines},
	url = {https://github.com/stanford-futuredata/index-baselines},
	note = {tex.key= 1},
}

@inproceedings{lis_func_interp,
	series = {{DTA} '20},
	title = {Function {Interpolation} for {Learned} {Index} {Structures}},
	booktitle = {Database {Theory} and {Applications}},
	author = {Setiawan, Naufal and Rubinstein, Benjamin and Borovica-Gajic, Renata},
	year = {2020},
	file = {Setiawan et al. - Function Interpolation for Learend Index Structure.pdf:/home/ryan/Zotero/storage/6RCXGADQ/Setiawan et al. - Function Interpolation for Learend Index Structure.pdf:application/pdf},
}

@inproceedings{wormhole,
	address = {Dresden, Germany},
	series = {{EuroSys} '19},
	title = {Wormhole: {A} {Fast} {Ordered} {Index} for {In}-memory {Data} {Management}},
	isbn = {978-1-4503-6281-8},
	shorttitle = {Wormhole},
	url = {https://doi.org/10.1145/3302424.3303955},
	doi = {10.1145/3302424.3303955},
	urldate = {2020-05-19},
	booktitle = {Proceedings of the {Fourteenth} {EuroSys} {Conference} 2019},
	publisher = {Association for Computing Machinery},
	author = {Wu, Xingbo and Ni, Fan and Jiang, Song},
	month = mar,
	year = {2019},
	pages = {1--16},
	file = {Full Text PDF:/home/ryan/Zotero/storage/4ZISKUPG/Wu et al. - 2019 - Wormhole A Fast Ordered Index for In-memory Data .pdf:application/pdf},
}

@inproceedings{ibtree,
	address = {Chicago, Illinois},
	series = {{DaMoN} '06},
	title = {B-tree indexes, interpolation search, and skew},
	isbn = {978-1-59593-466-6},
	url = {https://doi.org/10.1145/1140402.1140409},
	doi = {10.1145/1140402.1140409},
	urldate = {2020-05-19},
	booktitle = {Proceedings of the 2nd international workshop on {Data} management on new hardware},
	publisher = {Association for Computing Machinery},
	author = {Graefe, Goetz},
	month = jun,
	year = {2006},
	file = {Full Text PDF:/home/ryan/Zotero/storage/U36C9K6U/Graefe - 2006 - B-tree indexes, interpolation search, and skew.pdf:application/pdf},
}

@article{pgm-index,
	title = {The {PGM}-index: a fully-dynamic compressed learned index with provable worst-case bounds},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {The {PGM}-index},
	url = {https://doi.org/10.14778/3389133.3389135},
	doi = {10.14778/3389133.3389135},
	number = {8},
	urldate = {2020-05-08},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ferragina, Paolo and Vinciguerra, Giorgio},
	month = apr,
	year = {2020},
	pages = {1162--1175},
	file = {Full Text PDF:/home/ryan/Zotero/storage/XENHWD4R/Ferragina and Vinciguerra - 2020 - The PGM-index a fully-dynamic compressed learned .pdf:application/pdf},
}

@inproceedings{surf,
	address = {Houston, TX, USA},
	series = {{SIGMOD} '18},
	title = {{SuRF}: {Practical} {Range} {Query} {Filtering} with {Fast} {Succinct} {Tries}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {{SuRF}},
	url = {https://doi.org/10.1145/3183713.3196931},
	doi = {10.1145/3183713.3196931},
	urldate = {2020-05-07},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Huanchen and Lim, Hyeontaek and Leis, Viktor and Andersen, David G. and Kaminsky, Michael and Keeton, Kimberly and Pavlo, Andrew},
	month = may,
	year = {2018},
	keywords = {fast succinct tries, lsm-trees, range filter, succinct data structures, surf},
	pages = {323--336},
	file = {Full Text PDF:/home/ryan/Zotero/storage/KSHICW5U/Zhang et al. - 2018 - SuRF Practical Range Query Filtering with Fast Su.pdf:application/pdf},
}

@article{opt_binary_search,
	title = {An eight-dimensional systematic evaluation of optimized search algorithms on modern processors},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3236187.3236205},
	doi = {10.14778/3236187.3236205},
	number = {11},
	urldate = {2020-04-29},
	journal = {Proceedings of the VLDB Endowment},
	author = {Schulz, Lars-Christian and Broneske, David and Saake, Gunter},
	month = jul,
	year = {2018},
	pages = {1550--1562},
	file = {Full Text PDF:/home/ryan/Zotero/storage/EBGUIWWF/Schulz et al. - 2018 - An eight-dimensional systematic evaluation of opti.pdf:application/pdf},
}

@article{url-instagram-gc,
	title = {Instagram engineering, https://instagram-engineering.com/dismissing-python-garbage-collection-at-instagram-4dca40b29172},
	url = {https://instagram-engineering.com/dismissing-python-garbage-collection-at-instagram-4dca40b29172},
	author = {{Instagram}},
	year = {2017},
	note = {tex.key= 1},
}

@article{plr,
	title = {Maximum error-bounded {Piecewise} {Linear} {Representation} for online stream approximation},
	volume = {23},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-014-0355-0},
	doi = {10.1007/s00778-014-0355-0},
	language = {en},
	number = {6},
	urldate = {2020-04-27},
	journal = {The VLDB Journal},
	author = {Xie, Qing and Pang, Chaoyi and Zhou, Xiaofang and Zhang, Xiangliang and Deng, Ke},
	month = dec,
	year = {2014},
	pages = {915--937},
}

@inproceedings{cdfshop,
	address = {Portland, OR},
	series = {{SIGMOD} '20},
	title = {{CDFShop}: {Exploring} and {Optimizing} {Learned} {Index} {Structures}},
	copyright = {All rights reserved},
	shorttitle = {{CDFShop}},
	doi = {https://doi.org/10.1145/3318464.3384706},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Marcus, Ryan and Zhang, Emily and Kraska, Tim},
	month = jun,
	year = {2020},
	note = {props: demo},
	file = {Ryan Marcus et al. - 2020 - CDFShop Exploring and Optimizing Learned Index St.pdf:/home/ryan/Zotero/storage/DHDZ8QBT/Ryan Marcus et al. - 2020 - CDFShop Exploring and Optimizing Learned Index St.pdf:application/pdf},
}

@article{inductive_bias_dl,
	series = {{arXiv} '18},
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {https://arxiv.org/abs/1806.01261v3},
	language = {en},
	urldate = {2019-02-20},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = jun,
	year = {2018},
	file = {Full Text PDF:/home/ryan/Zotero/storage/PZBJCIIS/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/DZX7MFV7/1806.html:text/html},
}

@article{url-mssql_hints,
	title = {{SQL} {Server} hints, https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-query},
	url = {https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-query},
	note = {tex.key= 1},
}

@article{url-mysql_hints,
	title = {{MySQL} hints, https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html\#sysvar\_optimizer\_switch},
	url = {https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_optimizer_switch},
	note = {tex.key= 1},
}

@misc{url-pg_hints,
	title = {{PostgreSQL} hints, https://www.postgresql.org/docs/current/runtime-config-query.html},
	url = {https://www.postgresql.org/docs/current/runtime-config-query.html},
	author = {{PostgreSQL Developers}},
	year = {2024},
	note = {tex.key= 1},
}

@article{elastras,
	series = {{TODS} '13},
	title = {{ElasTraS}: {An} {Elastic}, {Scalable}, and {Self}-managing {Transactional} {Database} for the {Cloud}},
	volume = {38},
	issn = {0362-5915},
	shorttitle = {{ElasTraS}},
	url = {http://doi.acm.org/10.1145/2445583.2445588},
	doi = {10.1145/2445583.2445588},
	number = {1},
	urldate = {2017-04-11},
	journal = {ACM Trans. Database Syst.},
	author = {Das, Sudipto and Agrawal, Divyakant and El Abbadi, Amr},
	month = apr,
	year = {2013},
	keywords = {cloud computing, ACID, elastic data management, fault-tolerance, scalability, transactions},
	pages = {5:1--5:45},
	file = {ACM Full Text PDF:/home/ryan/Zotero/storage/636HRMSV/Das et al. - 2013 - ElasTraS An Elastic, Scalable, and Self-managing .pdf:application/pdf},
}

@article{gc_gen,
	series = {{CACM} '83},
	title = {A real-time garbage collector based on the lifetimes of objects},
	url = {https://doi.org/10.1145/358141.358147},
	urldate = {2020-03-31},
	journal = {Communications of the ACM},
	author = {Lieberman, Henry and Hewitt, Carl},
	month = jun,
	year = {1983},
	keywords = {algorithms, performance, languages, lisp, LISP, object-oriented programming, parallel processing, real-time garbage collection, reference counting, virtual memory},
	file = {Lieberman and Hewitt - 1983 - A real-time garbage collector based on the lifetim.pdf:/home/ryan/Zotero/storage/MXAJWP7G/Lieberman and Hewitt - 1983 - A real-time garbage collector based on the lifetim.pdf:application/pdf},
}

@article{url-pythongc,
	title = {{CPython} {GC} module, https://docs.python.org/3/library/gc.html},
	url = {https://docs.python.org/3/library/gc.html},
	note = {tex.key= 1},
}

@article{gc_server,
	series = {{PPPJ} '08},
	title = {Garbage collection: {Java} application servers‚Äô {Achilles} heel},
	volume = {70},
	issn = {0167-6423},
	shorttitle = {Garbage collection},
	url = {http://www.sciencedirect.com/science/article/pii/S0167642307001682},
	doi = {10.1016/j.scico.2007.07.008},
	language = {en},
	number = {2},
	urldate = {2020-03-29},
	journal = {Science of Computer Programming},
	author = {Xian, Feng and Srisa-an, Witawas and Jiang, Hong},
	month = feb,
	year = {2008},
	keywords = {Application servers, Garbage collection, Performance analysis, Throughput degradation},
	pages = {89--110},
	file = {ScienceDirect Full Text PDF:/home/ryan/Zotero/storage/E6DUHQMV/Xian et al. - 2008 - Garbage collection Java application servers‚Äô Achi.pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/MFXQ3L4V/S0167642307001682.html:text/html},
}

@inproceedings{gc_gpu,
	address = {Pittsburgh, Pennsylvania},
	series = {{CASES} '16},
	title = {{FastCollect}: offloading generational garbage collection to integrated {GPUs}},
	isbn = {978-1-4503-4482-1},
	shorttitle = {{FastCollect}},
	url = {https://doi.org/10.1145/2968455.2968520},
	doi = {10.1145/2968455.2968520},
	urldate = {2020-03-28},
	booktitle = {Proceedings of the {International} {Conference} on {Compilers}, {Architectures} and {Synthesis} for {Embedded} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jangda, Abhinav and Nasre, Rupesh},
	month = oct,
	year = {2016},
	keywords = {generational garbage collection, GPU, mark and sweep garbage collection, parallel garbage collection, SIMT},
	pages = {1--10},
	file = {Jangda and Nasre - 2016 - FastCollect offloading generational garbage colle.pdf:/home/ryan/Zotero/storage/JJ3SMNEF/Jangda and Nasre - 2016 - FastCollect offloading generational garbage colle.pdf:application/pdf},
}

@inproceedings{gc_api,
	series = {{SIGPLAN} '16},
	title = {Prioritized garbage collection: explicit {GC} support for software caches},
	volume = {51},
	shorttitle = {Prioritized garbage collection},
	url = {https://doi.org/10.1145/3022671.2984028},
	urldate = {2020-03-28},
	booktitle = {{ACM} {SIGPLAN} {Notices}},
	publisher = {Association for Computing Machinery},
	author = {Nunez, Diogenes and Guyer, Samuel Z. and Berger, Emery D.},
	month = oct,
	year = {2016},
	keywords = {garbage collection, soft references, software caching},
	file = {Full Text PDF:/home/ryan/Zotero/storage/8GIG6I23/Nunez et al. - 2016 - Prioritized garbage collection explicit GC suppor.pdf:application/pdf},
}

@inproceedings{java_gc2,
	address = {Atlanta, Georgia, USA},
	series = {{VEE} '16},
	title = {Performance {Analysis} and {Optimization} of {Full} {Garbage} {Collection} in {Memory}-hungry {Environments}},
	isbn = {978-1-4503-3947-6},
	url = {https://doi.org/10.1145/2892242.2892251},
	doi = {10.1145/2892242.2892251},
	urldate = {2020-03-28},
	booktitle = {Proceedings of the12th {ACM} {SIGPLAN}/{SIGOPS} {International} {Conference} on {Virtual} {Execution} {Environments}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Yang and Lei, Tianyang and Zhang, Weihua and Chen, Haibo and Zang, Binyu},
	month = mar,
	year = {2016},
	keywords = {compaction, full garbage collection, hotspot jvm, incremental query},
	pages = {123--130},
}

@inproceedings{gc_v8,
	address = {Santa Barbara, CA, USA},
	series = {{PLDI} '16},
	title = {Idle time garbage collection scheduling},
	isbn = {978-1-4503-4261-2},
	url = {https://doi.org/10.1145/2908080.2908106},
	doi = {10.1145/2908080.2908106},
	urldate = {2020-03-28},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Degenbaev, Ulan and Eisinger, Jochen and Ernst, Manfred and McIlroy, Ross and Payer, Hannes},
	month = jun,
	year = {2016},
	keywords = {Browser Technology, Garbage Collection, JavaScript, Memory Management, Scheduling, Virtual Machines, Web Applications},
	pages = {570--583},
	file = {Full Text PDF:/home/ryan/Zotero/storage/VA2B2UEN/Degenbaev et al. - 2016 - Idle time garbage collection scheduling.pdf:application/pdf},
}

@inproceedings{reward_shaping,
	address = {San Francisco, CA, USA},
	series = {{ICML} '99},
	title = {Policy {Invariance} {Under} {Reward} {Transformations}: {Theory} and {Application} to {Reward} {Shaping}},
	isbn = {978-1-55860-612-8},
	shorttitle = {Policy {Invariance} {Under} {Reward} {Transformations}},
	urldate = {2020-03-27},
	booktitle = {Proceedings of the {Sixteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
	month = jun,
	year = {1999},
	pages = {278--287},
}

@inproceedings{gc_search_opt,
	series = {{TOPS} '19},
	title = {Optimal {Choice} of {When} to {Garbage} {Collect}},
	volume = {41},
	url = {https://doi.org/10.1145/3282438},
	urldate = {2020-03-26},
	booktitle = {{ACM} {Transactions} on {Programming} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jacek, Nicholas and Chiu, Meng-Chieh and Marlin, Benjamin M. and Moss, J. Eliot B.},
	month = jan,
	year = {2019},
	keywords = {garbage collection, Automatic storage management, optimal schedules},
}

@article{mark_and_sweep,
	series = {{ACM} '60},
	title = {Recursive functions of symbolic expressions and their computation by machine, {Part} {I}},
	url = {https://doi.org/10.1145/367177.367199},
	urldate = {2020-03-26},
	journal = {Commun. ACM},
	author = {McCarthy, John},
	month = apr,
	year = {1960},
}

@inproceedings{java_gc,
	address = {Vancouver, BC, Canada},
	series = {{ISMM} '04},
	title = {Garbage-first garbage collection},
	isbn = {978-1-58113-945-7},
	url = {https://doi.org/10.1145/1029873.1029879},
	doi = {10.1145/1029873.1029879},
	urldate = {2020-03-26},
	booktitle = {Proceedings of the 4th international symposium on {Memory} management},
	publisher = {Association for Computing Machinery},
	author = {Detlefs, David and Flood, Christine and Heller, Steve and Printezis, Tony},
	month = oct,
	year = {2004},
	keywords = {garbage collection, parallel garbage collection, concurrent garbrage collection, garbage-first garbage collection, soft real-time garbage collection},
	pages = {37--48},
}

@article{lsm_survey,
	series = {{VLDB} '20},
	title = {{LSM}-based storage techniques: a survey},
	volume = {29},
	issn = {0949-877X},
	shorttitle = {{LSM}-based storage techniques},
	url = {https://doi.org/10.1007/s00778-019-00555-y},
	doi = {10.1007/s00778-019-00555-y},
	language = {en},
	number = {1},
	urldate = {2020-03-25},
	journal = {PVLDB},
	author = {Luo, Chen and Carey, Michael J.},
	month = jan,
	year = {2020},
	pages = {393--418},
	file = {Submitted Version:/home/ryan/Zotero/storage/22B6SEZ8/Luo and Carey - 2020 - LSM-based storage techniques a survey.pdf:application/pdf},
}

@article{lsm,
	series = {Acta {Informatica} '96},
	title = {The log-structured merge-tree ({LSM}-tree)},
	volume = {33},
	issn = {1432-0525},
	shorttitle = {{LSM}-tree},
	url = {https://doi.org/10.1007/s002360050048},
	doi = {10.1007/s002360050048},
	language = {en},
	number = {4},
	urldate = {2020-03-25},
	journal = {Acta Informatica},
	author = {O‚ÄôNeil, Patrick and Cheng, Edward and Gawlick, Dieter and O‚ÄôNeil, Elizabeth},
	month = jun,
	year = {1996},
	pages = {351--385},
}

@article{lift,
	series = {{arXiv} '18},
	title = {{LIFT}: {Reinforcement} {Learning} in {Computer} {Systems} by {Learning} {From} {Demonstrations}},
	shorttitle = {{LIFT}},
	url = {http://arxiv.org/abs/1808.07903},
	urldate = {2018-09-08},
	journal = {arXiv:1808.07903 [cs, stat]},
	author = {Schaarschmidt, Michael and Kuhnle, Alexander and Ellis, Ben and Fricke, Kai and Gessert, Felix and Yoneki, Eiko},
	month = aug,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1808.07903 PDF:/home/ryan/Zotero/storage/JJGQC83D/Schaarschmidt et al. - 2018 - LIFT Reinforcement Learning in Computer Systems b.pdf:application/pdf;arXiv.org Snapshot:/home/ryan/Zotero/storage/ZFHB2KER/1808.html:text/html},
}

@inproceedings{art,
	address = {USA},
	series = {{ICDE} ‚Äô13},
	title = {The adaptive radix tree: {ARTful} indexing for main-memory databases},
	isbn = {978-1-4673-4909-3},
	shorttitle = {The {Adaptive} {Radix} {Tree}},
	url = {https://doi.org/10.1109/ICDE.2013.6544812},
	doi = {10.1109/ICDE.2013.6544812},
	booktitle = {Proceedings of the 2013 {IEEE} international conference on data engineering},
	publisher = {IEEE Computer Society},
	author = {Leis, Viktor and Kemper, Alfons and Neumann, Thomas},
	year = {2013},
	pages = {38--49},
	file = {Leis et al. - 2013 - The adaptive radix tree ARTful indexing for main-.pdf:/home/ryan/Zotero/storage/KIGUBVVL/Leis et al. - 2013 - The adaptive radix tree ARTful indexing for main-.pdf:application/pdf},
}

@inproceedings{spline,
	series = {{BNCOD} '08},
	title = {Smooth interpolating histograms with error guarantees},
	url = {https://doi.org/10.1007/978-3-540-70504-8_12},
	doi = {10.1007/978-3-540-70504-8\_12},
	booktitle = {Sharing data, information and knowledge, 25th {British} {National} {Conference} on {Databases}},
	author = {Neumann, Thomas and Michel, Sebastian},
	year = {2008},
	pages = {126--138},
	file = {Neumann and Michel - 2008 - Smooth interpolating histograms with error guarant.pdf:/home/ryan/Zotero/storage/4KY9I7XY/Neumann and Michel - 2008 - Smooth interpolating histograms with error guarant.pdf:application/pdf},
}

@inproceedings{golynskiRankSelectOperations2006,
	address = {Miami, Florida},
	title = {Rank/select operations on large alphabets: a tool for text indexing},
	isbn = {978-0-89871-605-4},
	shorttitle = {Rank/select operations on large alphabets},
	url = {http://portal.acm.org/citation.cfm?doid=1109557.1109599},
	doi = {10.1145/1109557.1109599},
	language = {en},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the seventeenth annual {ACM}-{SIAM} symposium on {Discrete} algorithm  - {SODA} '06},
	publisher = {ACM Press},
	author = {Golynski, Alexander and Munro, J. Ian and Rao, S. Srinivasa},
	year = {2006},
	pages = {368--373},
	file = {Golynski et al. - 2006 - Rankselect operations on large alphabets a tool .pdf:/home/ryan/Zotero/storage/VL64JCL7/Golynski et al. - 2006 - Rankselect operations on large alphabets a tool .pdf:application/pdf},
}

@inproceedings{balsa,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {Balsa: {Learning} a {Query} {Optimizer} {Without} {Expert} {Demonstrations}},
	isbn = {978-1-4503-9249-5},
	shorttitle = {Balsa},
	url = {https://doi.org/10.1145/3514221.3517885},
	doi = {10.1145/3514221.3517885},
	urldate = {2022-09-10},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Zongheng and Chiang, Wei-Lin and Luan, Sifei and Mittal, Gautam and Luo, Michael and Stoica, Ion},
	month = jun,
	year = {2022},
	keywords = {learned query optimization, machine learning for systems},
	pages = {931--944},
	file = {Yang et al. - 2022 - Balsa Learning a Query Optimizer Without Expert D.pdf:/home/ryan/Zotero/storage/3PVX7PHV/Yang et al. - 2022 - Balsa Learning a Query Optimizer Without Expert D.pdf:application/pdf},
}

@article{q_transformer,
	series = {{PVLDB} '22},
	title = {{QueryFormer}: a tree transformer model for query plan representation},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {{QueryFormer}},
	url = {https://doi.org/10.14778/3529337.3529349},
	doi = {10.14778/3529337.3529349},
	number = {8},
	urldate = {2022-09-10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhao, Yue and Cong, Gao and Shi, Jiachen and Miao, Chunyan},
	month = apr,
	year = {2022},
	pages = {1658--1670},
	file = {Zhao et al. - 2022 - QueryFormer a tree transformer model for query pl.pdf:/home/ryan/Zotero/storage/ID4VGRGK/Zhao et al. - 2022 - QueryFormer a tree transformer model for query pl.pdf:application/pdf},
}

@article{policy_grad_survey,
	title = {A {Survey} of {Actor}-{Critic} {Reinforcement} {Learning}: {Standard} and {Natural} {Policy} {Gradients}},
	volume = {42},
	issn = {1558-2442},
	shorttitle = {A {Survey} of {Actor}-{Critic} {Reinforcement} {Learning}},
	doi = {10.1109/TSMCC.2012.2218595},
	abstract = {Policy-gradient-based actor-critic algorithms are amongst the most popular algorithms in the reinforcement learning framework. Their advantage of being able to search for optimal policies using low-variance gradient estimates has made them useful in several real-life applications, such as robotics, power control, and finance. Although general surveys on reinforcement learning techniques already exist, no survey is specifically dedicated to actor-critic algorithms in particular. This paper, therefore, describes the state of the art of actor-critic algorithms, with a focus on methods that can work in an online setting and use function approximation in order to deal with continuous state and action spaces. After starting with a discussion on the concepts of reinforcement learning and the origins of actor-critic algorithms, this paper describes the workings of the natural gradient, which has made its way into many actor-critic algorithms over the past few years. A review of several standard and natural actor-critic algorithms is given, and the paper concludes with an overview of application areas and a discussion on open issues.},
	number = {6},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel A. D. and Babuska, Robert},
	month = nov,
	year = {2012},
	keywords = {Optimization, Actor-critic, Approximation algorithms, Approximation methods, Convergence, Equations, natural gradient, policy gradient, reinforcement learning (RL), Standards},
	pages = {1291--1307},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/2BYR7HKV/6392457.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/RLI6F2CC/Grondman et al. - 2012 - A Survey of Actor-Critic Reinforcement Learning S.pdf:application/pdf},
}

@inproceedings{attention,
	series = {{NeurIPS} '17},
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2022-09-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/ryan/Zotero/storage/7MUDEVK6/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{coatnet,
	series = {{NeurIPS} '21},
	title = {{CoAtNet}: {Marrying} {Convolution} and {Attention} for {All} {Data} {Sizes}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 34},
	author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V. and Tan, Mingxing},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman},
	year = {2021},
	pages = {3965--3977},
}

@book{bonaCombinatoricsPermutations2022,
	edition = {3rd edition},
	title = {Combinatorics of {Permutations}},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Bona, Miklos},
	month = may,
	year = {2022},
	file = {Bona - 2022 - Combinatorics of Permutations.pdf:/home/ryan/Zotero/storage/JNUQ2TFF/Bona - 2022 - Combinatorics of Permutations.pdf:application/pdf},
}

@book{butlerFundamentalAlgorithmsPermutation1991,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Fundamental {Algorithms} for {Permutation} {Groups}},
	volume = {559},
	isbn = {978-3-540-54955-0 978-3-540-46607-9},
	url = {http://link.springer.com/10.1007/3-540-54955-2},
	urldate = {2022-11-05},
	publisher = {Springer},
	editor = {Butler, Gregory and Goos, Gerhard and Hartmanis, Juris},
	year = {1991},
	doi = {10.1007/3-540-54955-2},
	keywords = {Graph, algorithms, Algebraic Algorithms, Algebraische Algorithmen, algorithm analysis and problem complexity, combinatorics, Computational Complexity, Computational Group Theory, data structures, Graph Algorithms, Graph-Algorithmen, Group theory, Komplexit√§t und Algorithmen, Permutatioen und Kombinationen, Permutations and Combinations},
	file = {Butler - 1991 - Fundamental Algorithms for Permutation Groups.pdf:/home/ryan/Zotero/storage/VUT7U9DX/Butler - 1991 - Fundamental Algorithms for Permutation Groups.pdf:application/pdf},
}

@article{kapil_pqo,
	series = {{PVLDB} '22},
	title = {Leveraging query logs and machine learning for parametric query optimization},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3494124.3494126},
	doi = {10.14778/3494124.3494126},
	number = {3},
	urldate = {2022-11-01},
	journal = {Proceedings of the VLDB Endowment},
	author = {Vaidya, Kapil and Dutt, Anshuman and Narasayya, Vivek and Chaudhuri, Surajit},
	month = feb,
	year = {2022},
	pages = {401--413},
	file = {Vaidya et al. - 2022 - Leveraging query logs and machine learning for par.pdf:/home/ryan/Zotero/storage/MSQU9SA2/Vaidya et al. - 2022 - Leveraging query logs and machine learning for par.pdf:application/pdf},
}

@article{bcgnn,
	series = {Algorithmica '14},
	title = {Efficient {Fully}-{Compressed} {Sequence} {Representations}},
	volume = {69},
	issn = {1432-0541},
	url = {https://doi.org/10.1007/s00453-012-9726-3},
	doi = {10.1007/s00453-012-9726-3},
	language = {en},
	number = {1},
	urldate = {2022-12-31},
	journal = {Algorithmica},
	author = {Barbay, J√©r√©my and Claude, Francisco and Gagie, Travis and Navarro, Gonzalo and Nekrich, Yakov},
	month = may,
	year = {2014},
	note = {bibtex: bcgnn},
	keywords = {Compact data structures, Compressed sequence representations, Compressed text indexing, Entropy-bounded structures, Rank and select on sequences},
	pages = {232--268},
	file = {Barbay et al. - 2014 - Efficient Fully-Compressed Sequence Representation.pdf:/home/ryan/Zotero/storage/XBXH7TCB/Barbay et al. - 2014 - Efficient Fully-Compressed Sequence Representation.pdf:application/pdf},
}

@techreport{poison,
	title = {Targeted {Backdoor} {Attacks} on {Deep} {Learning} {Systems} {Using} {Data} {Poisoning}},
	url = {http://arxiv.org/abs/1712.05526},
	number = {arXiv:1712.05526},
	urldate = {2023-01-15},
	institution = {arXiv},
	author = {Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
	month = dec,
	year = {2017},
	note = {arXiv:1712.05526 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Chen et al. - 2017 - Targeted Backdoor Attacks on Deep Learning Systems.pdf:/home/ryan/Zotero/storage/D4J22WSC/Chen et al. - 2017 - Targeted Backdoor Attacks on Deep Learning Systems.pdf:application/pdf},
}

@article{sagedb_vldb,
	series = {{VLDB} '22},
	title = {{SageDB}: {An} {Instance}-{Optimized} {Data} {Analytics} {System}},
	volume = {15},
	copyright = {All rights reserved},
	doi = {10.14778/3565838.3565857},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ding, Jialin and Marcus, Ryan and Kipf, Andreas and Nathan, Vikram and Nrusimha, Aniruddha and Vaidya, Kapil and Renen, Alexander van and Kraska, Tim},
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {4062--4078},
	file = {Jialin Ding et al. - 2022 - SageDB An Instance-Optimized Data Analytics Syste.pdf:/home/ryan/Zotero/storage/BXXPYIYZ/Jialin Ding et al. - 2022 - SageDB An Instance-Optimized Data Analytics Syste.pdf:application/pdf},
}

@article{tsunami,
	title = {Tsunami: a learned multi-dimensional index for correlated data and skewed workloads},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {Tsunami},
	url = {https://dl.acm.org/doi/10.14778/3425879.3425880},
	doi = {10.14778/3425879.3425880},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ding, Jialin and Nathan, Vikram and Alizadeh, Mohammad and Kraska, Tim},
	month = oct,
	year = {2020},
	pages = {74--86},
	file = {Ding et al. - 2020 - Tsunami a learned multi-dimensional index for cor.pdf:/home/ryan/Zotero/storage/KA4ZMYJI/Ding et al. - 2020 - Tsunami a learned multi-dimensional index for cor.pdf:application/pdf},
}

@article{zeroshot_latency_model,
	series = {{VLDB} '22},
	title = {Zero-shot cost models for out-of-the-box learned cost prediction},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3551793.3551799},
	doi = {10.14778/3551793.3551799},
	number = {11},
	urldate = {2023-02-28},
	journal = {Proceedings of the VLDB Endowment},
	author = {Hilprecht, Benjamin and Binnig, Carsten},
	month = jul,
	year = {2022},
	pages = {2361--2374},
	file = {Submitted Version:/home/ryan/Zotero/storage/52STFNG4/Hilprecht and Binnig - 2022 - Zero-shot cost models for out-of-the-box learned c.pdf:application/pdf},
}

@article{sched_eval,
	series = {{TODS} '92},
	title = {Scheduling real-time transactions: a performance evaluation},
	volume = {17},
	issn = {0362-5915},
	shorttitle = {Scheduling real-time transactions},
	url = {https://doi.org/10.1145/132271.132276},
	doi = {10.1145/132271.132276},
	number = {3},
	urldate = {2023-02-28},
	journal = {ACM Transactions on Database Systems},
	author = {Abbott, Robert K. and Garcia-Molina, Hector},
	month = sep,
	year = {1992},
	keywords = {deadlines, locking protocols, real-time systems},
	pages = {513--560},
	file = {Full Text PDF:/home/ryan/Zotero/storage/8U7BUVZU/Abbott and Garcia-Molina - 1992 - Scheduling real-time transactions a performance e.pdf:application/pdf},
}

@article{realtime_sched,
	series = {{VLDB} '93},
	title = {Value-based scheduling in real-time database systems},
	volume = {2},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/BF01232184},
	doi = {10.1007/BF01232184},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {The VLDB Journal},
	author = {Haritsa, Jayant R. and Canrey, Michael J. and Livny, Miron},
	month = apr,
	year = {1993},
	keywords = {priority and concurrency algorithms, priority mapping, resource and data contention, Transaction values and deadlines},
	pages = {117--152},
	file = {Haritsa et al. - 1993 - Value-based scheduling in real-time database syste.pdf:/home/ryan/Zotero/storage/8CYKB9HM/Haritsa et al. - 1993 - Value-based scheduling in real-time database syste.pdf:application/pdf},
}

@inproceedings{redshift,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {Amazon {Redshift} {Re}-invented},
	isbn = {978-1-4503-9249-5},
	url = {https://doi.org/10.1145/3514221.3526045},
	doi = {10.1145/3514221.3526045},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Armenatzoglou, Nikos and Basu, Sanuj and Bhanoori, Naga and Cai, Mengchu and Chainani, Naresh and Chinta, Kiran and Govindaraju, Venkatraman and Green, Todd J. and Gupta, Monish and Hillig, Sebastian and Hotinger, Eric and Leshinksy, Yan and Liang, Jintian and McCreedy, Michael and Nagel, Fabian and Pandis, Ippokratis and Parchas, Panos and Pathak, Rahul and Polychroniou, Orestis and Rahman, Foyzur and Saxena, Gaurav and Soundararajan, Gokul and Subramanian, Sriram and Terry, Doug},
	month = jun,
	year = {2022},
	keywords = {analytics, autonomics, cloud data warehouse, data lake, elasticity, integration, OLAP, redshift, serverless},
	pages = {2205--2217},
	file = {Full Text PDF:/home/ryan/Zotero/storage/BZVRZ6ND/Armenatzoglou et al. - 2022 - Amazon Redshift Re-invented.pdf:application/pdf},
}

@article{quickstep,
	series = {{VLDB} '18},
	title = {Quickstep: a data platform based on the scaling-up approach},
	volume = {11},
	issn = {2150-8097},
	shorttitle = {Quickstep},
	url = {https://doi.org/10.14778/3184470.3184471},
	doi = {10.14778/3184470.3184471},
	number = {6},
	urldate = {2023-02-28},
	journal = {Proceedings of the VLDB Endowment},
	author = {Patel, Jignesh M. and Deshmukh, Harshad and Zhu, Jianqiao and Potti, Navneet and Zhang, Zuyu and Spehlmann, Marc and Memisoglu, Hakan and Saurabh, Saket},
	month = feb,
	year = {2018},
	pages = {663--676},
	file = {Patel et al. - 2018 - Quickstep a data platform based on the scaling-up.pdf:/home/ryan/Zotero/storage/6MDFLUPS/Patel et al. - 2018 - Quickstep a data platform based on the scaling-up.pdf:application/pdf},
}

@inproceedings{umbra_schedule,
	address = {New York, NY, USA},
	series = {{SIGMOD} '21},
	title = {Self-{Tuning} {Query} {Scheduling} for {Analytical} {Workloads}},
	isbn = {978-1-4503-8343-1},
	url = {https://doi.org/10.1145/3448016.3457260},
	doi = {10.1145/3448016.3457260},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Wagner, Benjamin and Kohn, Andr√© and Neumann, Thomas},
	month = jun,
	year = {2021},
	keywords = {database systems, parallelism, query scheduling, self-tuning},
	pages = {1879--1891},
	file = {Wagner et al. - 2021 - Self-Tuning Query Scheduling for Analytical Worklo.pdf:/home/ryan/Zotero/storage/5NJHNHLN/Wagner et al. - 2021 - Self-Tuning Query Scheduling for Analytical Worklo.pdf:application/pdf},
}

@inproceedings{bao_scope2,
	address = {Philadelphia PA USA},
	series = {{SIGMOD} '22},
	title = {Deploying a {Steered} {Query} {Optimizer} in {Production} at {Microsoft}},
	isbn = {978-1-4503-9249-5},
	url = {https://dl.acm.org/doi/10.1145/3514221.3526052},
	doi = {10.1145/3514221.3526052},
	language = {en},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zhang, Wangda and Interlandi, Matteo and Mineiro, Paul and Qiao, Shi and Ghazanfari, Nasim and Lie, Karlen and Friedman, Marc and Hosn, Rafah and Patel, Hiren and Jindal, Alekh},
	month = jun,
	year = {2022},
	pages = {2299--2311},
	file = {Zhang et al. - 2022 - Deploying a Steered Query Optimizer in Production .pdf:/home/ryan/Zotero/storage/528PFVLE/Zhang et al. - 2022 - Deploying a Steered Query Optimizer in Production .pdf:application/pdf},
}

@inproceedings{lis_bigtable,
	address = {Vancouver, BC, Canada},
	series = {{MLForSystems} @ {NeurIPS} '20},
	title = {Learned {Indexes} for {Google}-scale {Disk}-based {Database}},
	booktitle = {Machine {Learning} for {Systems} {Workshop} at {NeurIPS} 2020},
	author = {{Hussam Abu-Libdeh} and {Deniz Altinbuken} and {Alex Beutel} and {Ed Chi} and {Lyric Doshi} and {Tim Kraska} and {Xiaozhou Li} and {Andy Ly} and {Christopher Olston}},
	year = {2020},
	file = {Hussam Abu-Libdeh et al. - 2020 - Learned Indexes for Google-scale Disk-based Databa.pdf:/home/ryan/Zotero/storage/2KDSJI33/Hussam Abu-Libdeh et al. - 2020 - Learned Indexes for Google-scale Disk-based Databa.pdf:application/pdf},
}

@inproceedings{buffer_sched,
	address = {Tokyo, Japan},
	series = {{AIDB}@{VLDB} '20},
	title = {Buffer {Pool} {Aware} {Query} {Scheduling} via {Deep} {Reinforcement} {Learning}},
	copyright = {All rights reserved},
	url = {https://drive.google.com/file/d/1trNYAcQ3S71SHu5dbtkBR2hjcK-VWFSx/view?usp=sharing},
	booktitle = {2nd {International} {Workshop} on {Applied} {AI} for {Database} {Systems} and {Applications}},
	author = {Zhang, Chi and Marcus, Ryan and Kleiman, Anat and Papaemmanouil, Olga},
	editor = {He, Bingsheng and Reinwald, Berthold and Wu, Yingjun},
	year = {2020},
	file = {Zhang et al. - 2020 - Buffer Pool Aware Query Scheduling via Deep Reinfo.pdf:/home/ryan/Zotero/storage/UAFU83V2/Zhang et al. - 2020 - Buffer Pool Aware Query Scheduling via Deep Reinfo.pdf:application/pdf},
}

@inproceedings{lsched,
	address = {New York, NY, USA},
	series = {{SIGMOD} '22},
	title = {{LSched}: {A} {Workload}-{Aware} {Learned} {Query} {Scheduler} for {Analytical} {Database} {Systems}},
	isbn = {978-1-4503-9249-5},
	shorttitle = {{LSched}},
	url = {https://doi.org/10.1145/3514221.3526158},
	doi = {10.1145/3514221.3526158},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Sabek, Ibrahim and Ukyab, Tenzin Samten and Kraska, Tim},
	month = jun,
	year = {2022},
	keywords = {machine learning, databases, query scheduling and execution, reinforcement learning},
	pages = {1228--1242},
	file = {Full Text PDF:/home/ryan/Zotero/storage/LPASQH2V/Sabek et al. - 2022 - LSched A Workload-Aware Learned Query Scheduler f.pdf:application/pdf},
}

@article{wrr_modern,
	series = {{TON} '03},
	title = {Fair scheduling with tunable latency: {A} round-robin approach},
	volume = {11},
	issn = {1063-6692},
	shorttitle = {Fair scheduling with tunable latency},
	url = {http://ieeexplore.ieee.org/document/1224458/},
	doi = {10.1109/TNET.2003.815290},
	language = {en},
	number = {4},
	urldate = {2023-02-28},
	journal = {IEEE/ACM Transactions on Networking},
	author = {Chaskar, Hemant M. and Madhow, Upamanyu},
	month = aug,
	year = {2003},
	pages = {592--601},
}

@article{wrr_original,
	series = {Selected {Areas} of {Communications} '91},
	title = {Weighted round-robin cell multiplexing in a general-purpose {ATM} switch chip},
	volume = {9},
	issn = {07338716},
	url = {http://ieeexplore.ieee.org/document/105173/},
	doi = {10.1109/49.105173},
	number = {8},
	urldate = {2023-02-28},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Katevenis, Manolis and Sidiropoulos, Stefanos and Courcoubetis, Costas},
	month = oct,
	year = {1991},
	pages = {1265--1279},
}

@inproceedings{multiprogramming_level,
	address = {Atlanta, GA, USA},
	series = {{ICDE} '06},
	title = {How to {Determine} a {Good} {Multi}-{Programming} {Level} for {External} {Scheduling}},
	isbn = {978-0-7695-2570-9},
	url = {http://ieeexplore.ieee.org/document/1617428/},
	doi = {10.1109/ICDE.2006.78},
	urldate = {2023-02-28},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {Schroeder, B. and Harchol-Balter, M. and Iyengar, A. and Nahum, E. and Wierman, A.},
	year = {2006},
	pages = {60--60},
	file = {Accepted Version:/home/ryan/Zotero/storage/PW7TCGKI/Schroeder et al. - 2006 - How to Determine a Good Multi-Programming Level fo.pdf:application/pdf},
}

@inproceedings{xgboost,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	url = {http://doi.acm.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	keywords = {large-scale machine learning},
	pages = {785--794},
}

@inproceedings{datafarm,
	series = {{ICDE} '22},
	title = {Farming {Your} {ML}-based {Query} {Optimizer}'s {Food}},
	doi = {10.1109/ICDE53745.2022.00294},
	abstract = {Machine learning (ML) is becoming a core component in query optimizers, e.g., to estimate costs or cardinalities. This means large heterogeneous sets of labeled query plans or jobs (i.e., plans with their runtime or cardinality output) are needed. However, collecting such a training dataset is a very tedious and time-consuming task: It requires both developing numerous jobs and executing them to acquire ground-truth labels. We demonstrate Datafarm,a novel framework for efficiently generating and labeling training data for ML-based query optimizers to overcome these issues. Datafarmenables generating training data tailored to users' needs by learning from their existing workload patterns, input data, and computational resources. It uses an active learning approach to determine a subset of jobs to be executed and encloses the human into the loop, resulting in higher quality data. The graphical user interface of Datafarmallows users to get informative details of the generated jobs and guides them through the generation process step-by-step. We show how users can intervene and provide feedback to the system in an iterative fashion. As an output, users can download both the generated jobs to use as a benchmark and the training data (jobs with their labels).},
	booktitle = {2022 {IEEE} 38th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Van De Water, Robin and Ventura, Francesco and Kaoudi, Zoi and Quian√©-Ruiz, Jorge-Arnulfo and Markl, Volker},
	month = may,
	year = {2022},
	note = {ISSN: 2375-026X},
	keywords = {Machine learning, Runtime, Training, Training data, active learning, data generation, human in the loop, Iterative methods, Labeling, Maximum likelihood estimation, ML based query optimization, training data},
	pages = {3186--3189},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/A9E95XMZ/9835175.html:text/html;Van De Water et al. - 2022 - Farming Your ML-based Query Optimizer's Food.pdf:/home/ryan/Zotero/storage/2DMX6I3W/Van De Water et al. - 2022 - Farming Your ML-based Query Optimizer's Food.pdf:application/pdf},
}

@article{synthesis_layout,
	series = {{OOPSLA} '20},
	title = {Deductive optimization of relational data storage},
	volume = {4},
	url = {https://doi.org/10.1145/3428238},
	doi = {10.1145/3428238},
	number = {OOPSLA},
	urldate = {2023-03-08},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Feser, John and Madden, Sam and Tang, Nan and Solar-Lezama, Armando},
	month = nov,
	year = {2020},
	keywords = {databases, data representation synthesis, deductive synthesis},
	pages = {170:1--170:30},
	file = {Feser et al. - 2020 - Deductive optimization of relational data storage.pdf:/home/ryan/Zotero/storage/FQJ9XWCV/Feser et al. - 2020 - Deductive optimization of relational data storage.pdf:application/pdf},
}

@inproceedings{data_calculator,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {The {Data} {Calculator}: {Data} {Structure} {Design} and {Cost} {Synthesis} from {First} {Principles} and {Learned} {Cost} {Models}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {The {Data} {Calculator}},
	url = {https://doi.org/10.1145/3183713.3199671},
	doi = {10.1145/3183713.3199671},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Idreos, Stratos and Zoumpatianos, Kostas and Hentschel, Brian and Kester, Michael S. and Guo, Demi},
	month = may,
	year = {2018},
	keywords = {data structure synthesis, learned cost models},
	pages = {535--550},
	file = {Full Text PDF:/home/ryan/Zotero/storage/NAF2X5CQ/Idreos et al. - 2018 - The Data Calculator Data Structure Design and Cos.pdf:application/pdf},
}

@article{roaring,
	series = {{SP}\&{E} '16},
	title = {Consistently faster and smaller compressed bitmaps with {Roaring}},
	volume = {46},
	issn = {0038-0644},
	url = {https://doi.org/10.1002/spe.2402},
	doi = {10.1002/spe.2402},
	number = {11},
	urldate = {2023-03-08},
	journal = {Software‚ÄîPractice \& Experience},
	author = {Lemire, Daniel and Ssi-Yan-Kai, Gregory and Kaser, Owen},
	month = nov,
	year = {2016},
	keywords = {performance, bitmap index, index compression, measurement},
	pages = {1547--1569},
	file = {Accepted Version:/home/ryan/Zotero/storage/2KK8YB87/Lemire et al. - 2016 - Consistently faster and smaller compressed bitmaps.pdf:application/pdf},
}

@inproceedings{wanderjoin,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Wander {Join}: {Online} {Aggregation} via {Random} {Walks}},
	isbn = {978-1-4503-3531-7},
	shorttitle = {Wander {Join}},
	url = {https://doi.org/10.1145/2882903.2915235},
	doi = {10.1145/2882903.2915235},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Li, Feifei and Wu, Bin and Yi, Ke and Zhao, Zhuoyue},
	month = jun,
	year = {2016},
	keywords = {joins, online aggregation, random walks},
	pages = {615--629},
	file = {Full Text PDF:/home/ryan/Zotero/storage/8NN3CYEY/Li et al. - 2016 - Wander Join Online Aggregation via Random Walks.pdf:application/pdf},
}

@article{kepler,
	series = {{SIGMOD} '23},
	title = {Kepler: {Robust} {Learning} for {Faster} {Parametric} {Query} {Optimization}},
	volume = {1},
	copyright = {All rights reserved},
	url = {https://rm.cab/kepler},
	doi = {https://doi.org/10.1145/3588963},
	number = {1},
	journal = {Proceedings of the 2023 ACM SIGMOD Conference},
	author = {Doshi, Lyric and Zhuang, Vincent and Jain, Gaurav and Marcus, Ryan and Huang, Haoyu and Altinbuken, Deniz and Brevdo, Eugene and Fraser, Campbell},
	month = may,
	year = {2023},
	pages = {109},
	file = {Doshi et al. - 2023 - Kepler Robust Learning for Faster Parametric Quer.pdf:/home/ryan/Zotero/storage/3QCQGBU4/Doshi et al. - 2023 - Kepler Robust Learning for Faster Parametric Quer.pdf:application/pdf},
}

@inproceedings{bayes_latent,
	series = {{NeurIPS} '22},
	title = {Local {Latent} {Space} {Bayesian} {Optimization} over {Structured} {Inputs}},
	url = {https://openreview.net/forum?id=nZRTRevUO-},
	language = {en},
	urldate = {2023-03-10},
	author = {Maus, Natalie and Jones, Haydn Thomas and Moore, Juston and Kusner, Matt and Bradshaw, John and Gardner, Jacob R.},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/home/ryan/Zotero/storage/H3L86SCS/Maus et al. - 2022 - Local Latent Space Bayesian Optimization over Stru.pdf:application/pdf},
}

@inproceedings{bayes_local,
	address = {Red Hook, NY, USA},
	series = {{NeurIPS} '19},
	title = {Scalable global optimization via local {Bayesian} optimization},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob R and Turner, Ryan and Poloczek, Matthias},
	month = dec,
	year = {2019},
	pages = {5496--5507},
	file = {Full Text PDF:/home/ryan/Zotero/storage/VT23AUX7/Eriksson et al. - 2019 - Scalable global optimization via local Bayesian op.pdf:application/pdf},
}

@article{bottleneck,
	series = {{ALT} '10},
	title = {Learning and generalization with the information bottleneck},
	volume = {411},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S030439751000201X},
	doi = {10.1016/j.tcs.2010.04.006},
	language = {en},
	number = {29},
	urldate = {2023-03-10},
	journal = {Theoretical Computer Science},
	author = {Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
	month = jun,
	year = {2010},
	keywords = {Information bottleneck, Information theory, Statistical learning theory, Sufficient statistics},
	pages = {2696--2711},
	file = {Full Text:/home/ryan/Zotero/storage/BCSQYNCF/Shamir et al. - 2010 - Learning and generalization with the information b.pdf:application/pdf;ScienceDirect Snapshot:/home/ryan/Zotero/storage/UAH5I7JE/S030439751000201X.html:text/html},
}

@article{bayes_opt1,
	series = {{OR} '20},
	title = {Parallel {Bayesian} {Global} {Optimization} of {Expensive} {Functions}},
	volume = {68},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/10.1287/opre.2019.1966},
	doi = {10.1287/opre.2019.1966},
	language = {en},
	number = {6},
	urldate = {2023-03-10},
	journal = {Operations Research},
	author = {Wang, Jialei and Clark, Scott C. and Liu, Eric and Frazier, Peter I.},
	month = nov,
	year = {2020},
	pages = {1850--1865},
	file = {Full Text:/home/ryan/Zotero/storage/5ZC2M9FP/Wang et al. - 2020 - Parallel Bayesian Global Optimization of Expensive.pdf:application/pdf},
}

@inproceedings{bayes_opt2,
	series = {{NeurIPS} '16},
	title = {Bayesian {Optimization} with {Robust} {Bayesian} {Neural} {Networks}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/a96d3afec184766bfeca7a9f989fc7e7-Abstract.html},
	urldate = {2023-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Springenberg, Jost Tobias and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
	year = {2016},
	file = {Full Text PDF:/home/ryan/Zotero/storage/ZYNQKMUZ/Springenberg et al. - 2016 - Bayesian Optimization with Robust Bayesian Neural .pdf:application/pdf},
}

@inproceedings{qo_rank,
	series = {{BTW} '23},
	title = {Learn {What} {Really} {Matters}: {A} {Learning}-to-{Rank} {Approach} for {ML}-based {Query} {Optimization}},
	doi = {10.18420/BTW2023-25},
	booktitle = {Database {Systems} for {Business}, {Technology}, and the {Web} 2023},
	publisher = {Gesellschaft f√ºr Informatik e.V.},
	author = {Behr, Henriette and Markl, Volker and Kaoudi, Zoi},
	editor = {K√∂nig-Ries, Birgitta and Scherzinger, Stefanie and Lehner, Wolfgang and Vossen, Gottfried},
	year = {2023},
	file = {Behr et al. - 2023 - Learn What Really Matters A Learning-to-Rank Appr.pdf:/home/ryan/Zotero/storage/5BFH6IAG/Behr et al. - 2023 - Learn What Really Matters A Learning-to-Rank Appr.pdf:application/pdf},
}

@inproceedings{controlflag,
	address = {New York, NY, USA},
	series = {{MAPS} '21},
	title = {{ControlFlag}: a self-supervised idiosyncratic pattern detection system for software control structures},
	isbn = {978-1-4503-8467-4},
	shorttitle = {{ControlFlag}},
	url = {https://doi.org/10.1145/3460945.3464954},
	doi = {10.1145/3460945.3464954},
	urldate = {2023-03-14},
	booktitle = {Proceedings of the 5th {ACM} {SIGPLAN} {International} {Symposium} on {Machine} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Hasabnis, Niranjan and Gottschlich, Justin},
	month = jun,
	year = {2021},
	keywords = {self-supervised learning, Source-code mining},
	pages = {32--42},
	file = {Full Text PDF:/home/ryan/Zotero/storage/KQEMIARI/Hasabnis and Gottschlich - 2021 - ControlFlag a self-supervised idiosyncratic patte.pdf:application/pdf},
}

@inproceedings{perf_reg_zeroshot,
	address = {Red Hook, NY, USA},
	series = {{NeurIPS} '19},
	title = {A zero-positive learning approach for diagnosing software performance regressions},
	urldate = {2023-03-14},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Alam, Mejbah and Gottschlich, Justin and Tatbul, Nesime and Turek, Javier and Mattson, Timothy and Muzahid, Abdullah},
	month = dec,
	year = {2019},
	pages = {11627--11639},
	file = {Full Text PDF:/home/ryan/Zotero/storage/7MY4XCFC/Alam et al. - 2019 - A zero-positive learning approach for diagnosing s.pdf:application/pdf},
}

@article{bayes_survey,
	series = {{IEEE} '16},
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization}},
	volume = {104},
	issn = {1558-2256},
	shorttitle = {Taking the {Human} {Out} of the {Loop}},
	doi = {10.1109/JPROC.2015.2494218},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
	month = jan,
	year = {2016},
	keywords = {Optimization, decision making, Decision making, Linear programming, Big data, Bayes methods, design of experiments, Design of experiments, Genomes, genomic medicine, optimization, response surface methodology, Statistical analysis, statistical learning},
	pages = {148--175},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/359IHK2S/7352306.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/LJRDPFSG/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf:application/pdf},
}

@techreport{ergalics,
	address = {Tucson, AZ},
	type = {Position},
	title = {Ergalics: {A} {Natural} {Science} of {Computation}},
	institution = {University of Arizona},
	author = {Snodgrass, Richard},
	month = feb,
	year = {2010},
	file = {Snodgrass - 2010 - Ergalics A Natural Science of Computation.pdf:/home/ryan/Zotero/storage/3SHTCACD/Snodgrass - 2010 - Ergalics A Natural Science of Computation.pdf:application/pdf},
}

@inproceedings{ergalics_db,
	address = {New York, NY, USA},
	series = {{SIGMOD} '13},
	title = {{DBMS} metrology: measuring query time},
	isbn = {978-1-4503-2037-5},
	shorttitle = {{DBMS} metrology},
	url = {https://doi.org/10.1145/2463676.2465331},
	doi = {10.1145/2463676.2465331},
	urldate = {2023-03-14},
	booktitle = {Proceedings of the 2013 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Currim, Sabah and Snodgrass, Richard T. and Suh, Young-Kyoon and Zhang, Rui and Johnson, Matthew Wong and Yi, Cheng},
	month = jun,
	year = {2013},
	keywords = {accuracy, repeatability, tucson protocol},
	pages = {421--432},
	file = {Full Text PDF:/home/ryan/Zotero/storage/KBLWE8SE/Currim et al. - 2013 - DBMS metrology measuring query time.pdf:application/pdf},
}

@article{superopt_coined,
	series = {{SIGARCH} '87},
	title = {Superoptimizer: a look at the smallest program},
	volume = {15},
	issn = {0163-5964},
	shorttitle = {Superoptimizer},
	url = {https://doi.org/10.1145/36177.36194},
	doi = {10.1145/36177.36194},
	number = {5},
	urldate = {2023-03-15},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Massalin, Alexia},
	month = oct,
	year = {1987},
	pages = {122--126},
	file = {Full Text PDF:/home/ryan/Zotero/storage/ANF9VECT/Massalin - 1987 - Superoptimizer a look at the smallest program.pdf:application/pdf},
}

@article{ppqo,
	series = {{TKDE} '09},
	title = {Progressive {Parametric} {Query} {Optimization}},
	volume = {21},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2008.160},
	number = {4},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Bizarro, Pedro and Bruno, Nicolas and DeWitt, David J.},
	month = apr,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Databases, Query processing, Scalability, Cost function, Database Management, Piecewise linear techniques, Proposals},
	pages = {582--594},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/JVYP3V2M/4585381.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/BN4BS57R/Bizarro et al. - 2009 - Progressive Parametric Query Optimization.pdf:application/pdf},
}

@article{pqo_orig,
	series = {{VLDBJ} '97},
	title = {Parametric query optimization},
	volume = {6},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s007780050037},
	doi = {10.1007/s007780050037},
	language = {en},
	number = {2},
	urldate = {2023-03-15},
	journal = {The VLDB Journal},
	author = {Ioannidis, Yannis E. and Ng, Raymond T. and Shim, Kyuseok and Sellis, Timos K.},
	month = may,
	year = {1997},
	keywords = {Actual Parameter, Buffer Size, Database System, Main Approach, Optimization Time},
	pages = {132--151},
	file = {Full Text PDF:/home/ryan/Zotero/storage/5JI642DJ/Ioannidis et al. - 1997 - Parametric query optimization.pdf:application/pdf},
}

@article{millwheel,
	series = {{VLDB} '13},
	title = {{MillWheel}: fault-tolerant stream processing at internet scale},
	volume = {6},
	issn = {2150-8097},
	shorttitle = {{MillWheel}},
	url = {https://doi.org/10.14778/2536222.2536229},
	doi = {10.14778/2536222.2536229},
	number = {11},
	urldate = {2023-03-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Akidau, Tyler and Balikov, Alex and Bekiroƒülu, Kaya and Chernyak, Slava and Haberman, Josh and Lax, Reuven and McVeety, Sam and Mills, Daniel and Nordstrom, Paul and Whittle, Sam},
	month = aug,
	year = {2013},
	pages = {1033--1044},
	file = {Full Text PDF:/home/ryan/Zotero/storage/YTYZQ58V/Akidau et al. - 2013 - MillWheel fault-tolerant stream processing at int.pdf:application/pdf},
}

@article{bigtable,
	series = {{TOCS} '08},
	title = {Bigtable: {A} {Distributed} {Storage} {System} for {Structured} {Data}},
	volume = {26},
	issn = {0734-2071},
	shorttitle = {Bigtable},
	url = {https://doi.org/10.1145/1365815.1365816},
	doi = {10.1145/1365815.1365816},
	number = {2},
	urldate = {2023-03-15},
	journal = {ACM Transactions on Computer Systems},
	author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C. and Wallach, Deborah A. and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E.},
	month = jun,
	year = {2008},
	keywords = {Large-Scale Distributed Storage},
	pages = {4:1--4:26},
	file = {Full Text PDF:/home/ryan/Zotero/storage/WJVWFE5T/Chang et al. - 2008 - Bigtable A Distributed Storage System for Structu.pdf:application/pdf},
}

@inproceedings{deep_card_est_koudas,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Deep {Learning} {Models} for {Selectivity} {Estimation} of {Multi}-{Attribute} {Queries}},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3389741},
	doi = {10.1145/3318464.3389741},
	urldate = {2023-03-14},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Hasan, Shohedul and Thirumuruganathan, Saravanan and Augustine, Jees and Koudas, Nick and Das, Gautam},
	month = may,
	year = {2020},
	keywords = {cardinality estimation, deep learning, density estimation, made, neural autoregressive models, selectivity estimation},
	pages = {1035--1050},
}

@inproceedings{zero_shot_streaming,
	address = {New York, NY, USA},
	series = {{DEBS} '22},
	title = {Zero-shot cost models for distributed stream processing},
	isbn = {978-1-4503-9308-9},
	url = {https://doi.org/10.1145/3524860.3539639},
	doi = {10.1145/3524860.3539639},
	urldate = {2023-03-14},
	booktitle = {Proceedings of the 16th {ACM} {International} {Conference} on {Distributed} and {Event}-{Based} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Heinrich, Roman and Luthra, Manisha and Kornmayer, Harald and Binnig, Carsten},
	month = jul,
	year = {2022},
	keywords = {cost models, stream processing, zero-shot learning},
	pages = {85--90},
	file = {Submitted Version:/home/ryan/Zotero/storage/4998RCHT/Heinrich et al. - 2022 - Zero-shot cost models for distributed stream proce.pdf:application/pdf},
}

@misc{PlayBoardGames,
	title = {Play board games online from your browser},
	url = {https://en.boardgamearena.com/},
	abstract = {The world's \#1 platform for playing board games online. Play hundreds of board games from your browser for free.},
	language = {en},
	urldate = {2023-04-02},
	journal = {Board Game Arena},
	file = {Snapshot:/home/ryan/Zotero/storage/DA6SEM8L/account.html:text/html},
}

@inproceedings{gauravsaxenaAutoWLMMachineLearning,
	title = {Auto-{WLM}: {Machine} {Learning} {Enhanced} {Workload} {Managment} for in {Amazon} {Redshift}},
	author = {{Gaurav Saxena} and {Mohammad Rahman}},
}

@misc{CreateWolframID,
	title = {Create a {Wolfram} {ID} - {Wolfram}{\textbar}{Alpha}},
	url = {https://account.wolfram.com/login/create},
	urldate = {2023-04-14},
	file = {Create a Wolfram ID - Wolfram|Alpha:/home/ryan/Zotero/storage/TUL9ZK3U/create.html:text/html},
}

@inproceedings{parametric_plan_cache,
	series = {{ICDE} '12},
	title = {Parametric {Plan} {Caching} {Using} {Density}-{Based} {Clustering}},
	doi = {10.1109/ICDE.2012.57},
	booktitle = {2012 {IEEE} 28th {International} {Conference} on {Data} {Engineering}},
	author = {Alu√ß, Gunes and DeHaan, David E. and Bowman, Ivan T.},
	month = apr,
	year = {2012},
	keywords = {Optimization, Prediction algorithms, Query processing, Computational modeling, Clustering algorithms, Couplings, History},
	pages = {402--413},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/D8NXWVKW/6228101.html:text/html},
}

@article{robust_card_est,
	series = {{VLDB} '23},
	title = {Robust {Query} {Driven} {Cardinality} {Estimation} under {Changing} {Workloads}},
	volume = {16},
	copyright = {All rights reserved},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3583140.3583164},
	doi = {10.14778/3583140.3583164},
	number = {6},
	urldate = {2023-05-08},
	journal = {Proceedings of the VLDB Endowment},
	author = {Negi, Parimarjan and Wu, Ziniu and Kipf, Andreas and Tatbul, Nesime and Marcus, Ryan and Madden, Sam and Kraska, Tim and Alizadeh, Mohammad},
	month = apr,
	year = {2023},
	pages = {1520--1533},
	file = {Negi et al. - 2023 - Robust Query Driven Cardinality Estimation under C.pdf:/home/ryan/Zotero/storage/S7896LJL/Negi et al. - 2023 - Robust Query Driven Cardinality Estimation under C.pdf:application/pdf},
}

@inproceedings{auto-wlm,
	series = {{SIGMOD} '23},
	title = {Auto-{WLM}: {Machine} {Learning} {Enhanced} {Workload} {Management} in {Amazon} {Redshift}},
	copyright = {All rights reserved},
	doi = {https://doi.org/10.1145/3555041.3589677},
	booktitle = {Companion of the 2023 {International} {Conference} on {Management} of {Data}},
	author = {Saxena, Gaurav and Rahman, Mohammad and Chainani, Naresh and Lin, Chunbin and Caragea, George and Chowdhury, Fahim and Marcus, Ryan and Kraska, Tim and Pandis, Ippokratis and Narayanaswamy, Balakrishnan (Murali)},
	month = jun,
	year = {2023},
	file = {Gaurav Saxena et al. - 2023 - Auto-WLM Machine Learning Enhanced Workload Manag.pdf:/home/ryan/Zotero/storage/CU9Z4YMT/Gaurav Saxena et al. - 2023 - Auto-WLM Machine Learning Enhanced Workload Manag.pdf:application/pdf},
}

@article{autosteer,
	series = {{VLDB} '23},
	title = {{AutoSteer}: {Learned} {Query} {Optimization} for {Any} {SQL} {Database}},
	volume = {14},
	copyright = {All rights reserved},
	issn = {2150-8097},
	doi = {10.14778/3611540.3611544},
	number = {1},
	journal = {PVLDB},
	author = {Anneser, Christoph and Tatbul, Nesime and Cohen, David and Xu, Zhenggang and Pandian, Prithvi and Leptev, Nikolay and Marcus, Ryan},
	month = aug,
	year = {2023},
	file = {Anneser et al. - 2023 - AutoSteer Learned Query Optimization for Any SQL .pdf:/home/ryan/Zotero/storage/K2NZ32C4/Anneser et al. - 2023 - AutoSteer Learned Query Optimization for Any SQL .pdf:application/pdf},
}

@article{adachain,
	series = {{VLDB} '23},
	title = {{AdaChain}: {A} {Learned} {Adaptive} {Blockchain}},
	volume = {16},
	copyright = {All rights reserved},
	issn = {150-8097},
	doi = {10.14778/3594512.3594531},
	number = {8},
	journal = {PVLDB},
	author = {Wu, Chenyuan and Mehta, Bhavana and Amiri, Mohammad Javad and Marcus, Ryan and Loo, Boon Thau},
	year = {2023},
	pages = {2033--2046},
	file = {Wu et al. - 2023 - AdaChain A Learned Adaptive Blockchain.pdf:/home/ryan/Zotero/storage/6V8J2EL3/Wu et al. - 2023 - AdaChain A Learned Adaptive Blockchain.pdf:application/pdf},
}

@inproceedings{pbft,
	address = {USA},
	series = {{OSDI} '99},
	title = {Practical {Byzantine} fault tolerance},
	isbn = {978-1-880446-39-3},
	urldate = {2023-06-09},
	booktitle = {Proceedings of the third symposium on {Operating} systems design and implementation},
	publisher = {USENIX Association},
	author = {Castro, Miguel and Liskov, Barbara},
	month = feb,
	year = {1999},
	pages = {173--186},
	file = {Castro and Liskov - 1999 - Practical Byzantine fault tolerance.pdf:/home/ryan/Zotero/storage/FD7ZVNCY/Castro and Liskov - 1999 - Practical Byzantine fault tolerance.pdf:application/pdf},
}

@inproceedings{umbra,
	series = {{CIDR} '20},
	title = {Umbra: {A} {Disk}-{Based} {System} with {In}-{Memory} {Performance}},
	url = {http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf},
	booktitle = {10th {Conference} on {Innovative} {Data} {Systems} {Research}, {CIDR} 2020, {Amsterdam}, {The} {Netherlands}, {January} 12-15, 2020, {Online} {Proceedings}},
	publisher = {www.cidrdb.org},
	author = {Neumann, Thomas and Freitag, Michael J.},
	year = {2020},
	file = {Neumann and Freitag - 2020 - Umbra A Disk-Based System with In-Memory Performa.pdf:/home/ryan/Zotero/storage/C8WT78EI/Neumann and Freitag - 2020 - Umbra A Disk-Based System with In-Memory Performa.pdf:application/pdf},
}

@techreport{genesisdb,
	address = {Ithaca. NY},
	type = {Technical {Report}},
	title = {{GenesisDB}: {Synthesizing} {Customized} {SQL} {Execution} {Engines} from {Natural} {Language} {Instructions} {Using} {GPT}-3 {Codex}},
	url = {https://rm.cab/genesisdb},
	abstract = {GenesisDB uses generative AI to synthesize customized operator
implementations for SQL processing. Users customize GenesisDB

by providing natural language instructions, describing desired be-
havior, as well as optional operator code samples. For instance,

this enables lay users to obtain customized output for debugging,
non-standard progress updates during processing, or links to SQL
tutorials related to operators in the current query plan. Advanced
users can generate engines that use customized data structures or
processing methods.

Based on the GPT-3 Codex model, GenesisDB provides a frame-
work that controls code synthesis, optimally schedules test cases,

and automatically detects and fixes bugs by re-synthesizing spe-
cific operators. Its run time component decomposes complex SQL

queries, including all queries of the TPC-H benchmark, into process-
ing steps that are realized by custom operators. The experiments

show that GenesisDB is able to generate custom code for a majority
of operators in a variety of scenarios (while automatically switching
to default implementations for remaining ones).},
	institution = {Cornell},
	author = {{Immanuel Trummer}},
	year = {2022},
}

@article{xml_join_bitmap,
	series = {{WWW} '15},
	title = {Configuring bitmap materialized views for optimizing {XML} queries},
	volume = {18},
	issn = {1573-1413},
	url = {https://doi.org/10.1007/s11280-013-0272-y},
	doi = {10.1007/s11280-013-0272-y},
	abstract = {In recent years the inverted lists evaluation model along with holistic stack-based algorithms have been established as the most prominent techniques for evaluating XML queries on large persistent XML data. In this framework, we are using materialized views for optimizing XML queries. We consider a novel approach which instead of materializing the answer of a view materializes exactly the inverted sublists that are necessary for computing the answer of the view. This originality allows storing view materializations as compressed bitmaps, a solution that minimizes the materialization space and empowers performing optimization operations as CPU-efficient bitwise operations. To realize the potential of bitmap materialized views in optimizing query performance, we define and address the following problem (view configuration problem): given an XML tree and its schema find a template of tree-pattern views (view configuration) such that: (a) the views of this configuration can answer all the queries that can be issued against the schema, (b) their materialization fits in the space provided, and (c) evaluating the queries using these views minimizes the overall query evaluation cost. We consider an instance of this problem for tree pattern queries. Our intension is to find view configurations whose materializations are small enough to be stored in main memory. We find two candidate solution configurations and we identify cases where views can be excluded from materialization in a configuration without affecting query performance. In order to compare our approach with an approach which also can support the optimization of every query on the schema, we implemented an improvement of a state-of-the-art approach which is based on structural indexes. Our experimental results show that our approach is stable, greatly improves evaluating queries without materialized views, outperforms the structural index approach on all test cases and is very close to the optimal. These results characterize our approach as the best candidate for supporting the optimization of queries in the framework of the inverted lists model.},
	language = {en},
	number = {3},
	urldate = {2023-07-19},
	journal = {World Wide Web},
	author = {Wu, Xiaoying and Theodoratos, Dimitri and Kementsietsidis, Anastasios},
	month = may,
	year = {2015},
	keywords = {Materialized views, View configuration, XML, XPath query evaluation},
	pages = {607--632},
	file = {Full Text PDF:/home/ryan/Zotero/storage/L2F3QL4E/Wu et al. - 2015 - Configuring bitmap materialized views for optimizi.pdf:application/pdf},
}

@article{hybrid_lqo,
	series = {{VLDB} '22},
	title = {Cost-{Based} or {Learning}-{Based}? {A} {Hybrid} {Query} {Optimizer} for {Query} {Plan} {Selection}},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Cost-{Based} or {Learning}-{Based}?},
	url = {https://dl.acm.org/doi/10.14778/3565838.3565846},
	doi = {10.14778/3565838.3565846},
	abstract = {Traditional cost-based optimizers are efficient and stable to generate optimal plans for simple SQL queries, but they may not generate high-quality plans for complicated queries. Thus learning-based optimizers have been proposed recently that can learn high-quality plans based on past experiences. However, learning-based optimizers cannot work well for dynamic workloads that have different distributions with training examples. In this paper, we propose a hybrid optimizer that adopts the advantages and avoids the shortcomings of these two types of optimizers, which first generates high-quality candidate plans from each type of optimizers and then selects the best plan from the candidates. There are two challenges. (1) How to generate high-quality candidates? We propose a hint-based candidate generation method that leverages the learning-based method to generate highly beneficial hints and then uses a cost-based method to supplement the hints to generate complete plans as candidates. (2) How to evaluate different candidate plans and select the best one? We propose an uncertainty-based optimal plan selection model, which predicts the execution time and the uncertainty for each plan. The uncertainty reflects the confidence of the execution time prediction. We select the plan using the uncertainty model. Experiment results on real datasets showed that our method outperformed the state-of-the-art baselines, and reduced the total latency by 25\% and the tail latency by 65\% compared to PostgreSQL.},
	number = {13},
	urldate = {2023-07-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Yu, Xiang and Chai, Chengliang and Li, Guoliang and Liu, Jiabin},
	month = sep,
	year = {2022},
	pages = {3924--3936},
	file = {Yu et al. - 2022 - Cost-Based or Learning-Based A Hybrid Query Optim.pdf:/home/ryan/Zotero/storage/NBE9N5AF/Yu et al. - 2022 - Cost-Based or Learning-Based A Hybrid Query Optim.pdf:application/pdf},
}

@article{mcts_qo,
	series = {{VLDB} '21},
	title = {A learned query rewrite system using {Monte} {Carlo} tree search},
	volume = {15},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3485450.3485456},
	doi = {10.14778/3485450.3485456},
	number = {1},
	urldate = {2023-07-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhou, Xuanhe and Li, Guoliang and Chai, Chengliang and Feng, Jianhua},
	month = sep,
	year = {2021},
	pages = {46--58},
	file = {Full Text PDF:/home/ryan/Zotero/storage/7BRZ5M27/Zhou et al. - 2021 - A learned query rewrite system using Monte Carlo t.pdf:application/pdf},
}

@inproceedings{qo_contract,
	address = {New York, NY, USA},
	series = {{SIGMOD} '09},
	title = {Query optimizers: time to rethink the contract?},
	isbn = {978-1-60558-551-2},
	shorttitle = {Query optimizers},
	url = {https://dl.acm.org/doi/10.1145/1559845.1559955},
	doi = {10.1145/1559845.1559955},
	urldate = {2023-07-19},
	booktitle = {Proceedings of the 2009 {ACM} {SIGMOD} {International} {Conference} on {Management} of data},
	publisher = {Association for Computing Machinery},
	author = {Chaudhuri, Surajit},
	month = jun,
	year = {2009},
	keywords = {cardinality estimation, query optimizer},
	pages = {961--968},
	file = {Full Text PDF:/home/ryan/Zotero/storage/VSWPZ63U/Chaudhuri - 2009 - Query optimizers time to rethink the contract.pdf:application/pdf},
}

@article{google_mesa,
	title = {Mesa: a geo-replicated online data warehouse for {Google}'s advertising system},
	volume = {59},
	issn = {0001-0782},
	shorttitle = {Mesa},
	url = {https://dl.acm.org/doi/10.1145/2936722},
	doi = {10.1145/2936722},
	abstract = {Mesa is a highly scalable analytic data warehousing system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy a complex and challenging set of user and systems requirements, including near real-time data ingestion and retrieval, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Specifically, Mesa handles petabytes of data, processes millions of row updates per second, and serves billions of queries that fetch trillions of rows per day. Mesa is geo-replicated across multiple datacenters and provides consistent and repeatable query answers at low latency, even when an entire datacenter fails. This paper presents the Mesa system and reports the performance and scale that it achieves.},
	number = {7},
	urldate = {2023-08-09},
	journal = {Communications of the ACM},
	author = {Gupta, Ashish and Yang, Fan and Govig, Jason and Kirsch, Adam and Chan, Kelvin and Lai, Kevin and Wu, Shuo and Dhoot, Sandeep and Kumar, Abhilash Rajesh and Agiwal, Ankur and Bhansali, Sanjay and Hong, Mingsheng and Cameron, Jamie and Siddiqi, Masood and Jones, David and Shute, Jeff and Gubarev, Andrey and Venkataraman, Shivakumar and Agrawal, Divyakant},
	month = jun,
	year = {2016},
	pages = {117--125},
	file = {Full Text PDF:/home/ryan/Zotero/storage/KAR5RV4Y/Gupta et al. - 2016 - Mesa a geo-replicated online data warehouse for G.pdf:application/pdf},
}

@article{ms_scope,
	series = {{VLDB} '08},
	title = {{SCOPE}: easy and efficient parallel processing of massive data sets},
	volume = {1},
	issn = {2150-8097},
	shorttitle = {{SCOPE}},
	url = {https://dl.acm.org/doi/10.14778/1454159.1454166},
	doi = {10.14778/1454159.1454166},
	abstract = {Companies providing cloud-scale services have an increasing need to store and analyze massive data sets such as search logs and click streams. For cost and performance reasons, processing is typically done on large clusters of shared-nothing commodity machines. It is imperative to develop a programming model that hides the complexity of the underlying system but provides flexibility by allowing users to extend functionality to meet a variety of requirements. In this paper, we present a new declarative and extensible scripting language, SCOPE (Structured Computations Optimized for Parallel Execution), targeted for this type of massive data analysis. The language is designed for ease of use with no explicit parallelism, while being amenable to efficient parallel execution on large clusters. SCOPE borrows several features from SQL. Data is modeled as sets of rows composed of typed columns. The select statement is retained with inner joins, outer joins, and aggregation allowed. Users can easily define their own functions and implement their own versions of operators: extractors (parsing and constructing rows from a file), processors (row-wise processing), reducers (group-wise processing), and combiners (combining rows from two inputs). SCOPE supports nesting of expressions but also allows a computation to be specified as a series of steps, in a manner often preferred by programmers. We also describe how scripts are compiled into efficient, parallel execution plans and executed on large clusters.},
	number = {2},
	urldate = {2023-08-09},
	journal = {Proceedings of the VLDB Endowment},
	author = {Chaiken, Ronnie and Jenkins, Bob and Larson, Per-√Öke and Ramsey, Bill and Shakib, Darren and Weaver, Simon and Zhou, Jingren},
	month = aug,
	year = {2008},
	pages = {1265--1276},
	file = {Full Text PDF:/home/ryan/Zotero/storage/237PJ99P/Chaiken et al. - 2008 - SCOPE easy and efficient parallel processing of m.pdf:application/pdf},
}

@phdthesis{sketch_thesis,
	address = {United States -- California},
	type = {Dissertation},
	title = {Program synthesis by sketching},
	url = {https://www.proquest.com/docview/304698243/abstract/1F48A120BE794FFAPQ/1},
	language = {English},
	urldate = {2023-08-09},
	school = {University of California, Berkeley},
	author = {Solar-Lezama, Armando},
	year = {2008},
	note = {ISBN: 9781109097450},
	keywords = {Applied sciences, Program synthesis, Sketching, Software tools},
}

@article{halide_lifting,
	series = {{TOG} '19},
	title = {Automatically translating image processing libraries to halide},
	volume = {38},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3355089.3356549},
	doi = {10.1145/3355089.3356549},
	number = {6},
	urldate = {2023-08-09},
	journal = {ACM Transactions on Graphics},
	author = {Ahmad, Maaz Bin Safeer and Ragan-Kelley, Jonathan and Cheung, Alvin and Kamil, Shoaib},
	month = nov,
	year = {2019},
	keywords = {machine programming, stencil computation, verified lifting},
	pages = {204:1--204:13},
	file = {Full Text PDF:/home/ryan/Zotero/storage/XUEQYAMC/Ahmad et al. - 2019 - Automatically translating image processing librari.pdf:application/pdf},
}

@article{halide,
	series = {{SIGPLAN} '13},
	title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
	volume = {48},
	issn = {0362-1340},
	shorttitle = {Halide},
	url = {https://dl.acm.org/doi/10.1145/2499370.2462176},
	doi = {10.1145/2499370.2462176},
	abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
	number = {6},
	urldate = {2023-08-09},
	journal = {ACM SIGPLAN Notices},
	author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr√©do and Amarasinghe, Saman},
	month = jun,
	year = {2013},
	keywords = {parallelism, optimization, autotuning, compiler, domain specific language, gpu, image processing, locality, redundant computation, vectorization},
	pages = {519--530},
	file = {Full Text PDF:/home/ryan/Zotero/storage/5KMQGCDD/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing par.pdf:application/pdf},
}

@inproceedings{synthesis_concurrent,
	address = {New York, NY, USA},
	series = {{POPL} '10},
	title = {Abstraction-guided synthesis of synchronization},
	isbn = {978-1-60558-479-9},
	url = {https://dl.acm.org/doi/10.1145/1706299.1706338},
	doi = {10.1145/1706299.1706338},
	abstract = {We present a novel framework for automatic inference of efficient synchronization in concurrent programs, a task known to be difficult and error-prone when done manually. Our framework is based on abstract interpretation and can infer synchronization for infinite state programs. Given a program, a specification, and an abstraction, we infer synchronization that avoids all (abstract) interleavings that may violate the specification, but permits as many valid interleavings as possible. Combined with abstraction refinement, our framework can be viewed as a new approach for verification where both the program and the abstraction can be modified on-the-fly during the verification process. The ability to modify the program, and not only the abstraction, allows us to remove program interleavings not only when they are known to be invalid, but also when they cannot be verified using the given abstraction. We implemented a prototype of our approach using numerical abstractions and applied it to verify several interesting programs.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 37th annual {ACM} {SIGPLAN}-{SIGACT} symposium on {Principles} of programming languages},
	publisher = {Association for Computing Machinery},
	author = {Vechev, Martin and Yahav, Eran and Yorsh, Greta},
	month = jan,
	year = {2010},
	keywords = {concurrency, abstract interpretation, synthesis},
	pages = {327--338},
	file = {Full Text PDF:/home/ryan/Zotero/storage/NJYGQEBG/Vechev et al. - 2010 - Abstraction-guided synthesis of synchronization.pdf:application/pdf},
}

@article{codexdb,
	series = {{VLDB} '22},
	title = {{CodexDB}: synthesizing code for query processing from natural language instructions using {GPT}-3 codex},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {{CodexDB}},
	url = {https://dl.acm.org/doi/10.14778/3551793.3551841},
	doi = {10.14778/3551793.3551841},
	abstract = {CodexDB enables users to customize SQL query processing via natural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model which translates text into code. It is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language. Processing steps are enriched with user-provided instructions and descriptions of database properties. Codex translates the resulting text into query processing code. An early prototype of CodexDB is able to generate correct code for up to 81\% of queries for the WikiSQL benchmark and for up to 62\% on the SPIDER benchmark.},
	number = {11},
	urldate = {2023-08-09},
	journal = {Proceedings of the VLDB Endowment},
	author = {Trummer, Immanuel},
	month = jul,
	year = {2022},
	pages = {2921--2928},
	file = {Full Text PDF:/home/ryan/Zotero/storage/A5E4V3WW/Trummer - 2022 - CodexDB synthesizing code for query processing fr.pdf:application/pdf},
}

@inproceedings{hypergraph_partition,
	address = {New York, NY, USA},
	series = {{DAC} '99},
	title = {Multilevel k-way hypergraph partitioning},
	isbn = {978-1-58113-109-3},
	url = {https://dl.acm.org/doi/10.1145/309847.309954},
	doi = {10.1145/309847.309954},
	urldate = {2023-08-19},
	booktitle = {Proceedings of the 36th annual {ACM}/{IEEE} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Karypis, George and Kumar, Vipin},
	month = jun,
	year = {1999},
	pages = {343--348},
	file = {Full Text PDF:/home/ryan/Zotero/storage/DIEZTPQ5/Karypis and Kumar - 1999 - Multilevel k-way hypergraph partitioning.pdf:application/pdf},
}

@article{zone_maps,
	series = {{VLDB} '17},
	title = {Dimensions based data clustering and zone maps},
	volume = {10},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3137765.3137769},
	doi = {10.14778/3137765.3137769},
	abstract = {In recent years, the data warehouse industry has witnessed decreased use of indexing but increased use of compression and clustering of data facilitating efficient data access and data pruning in the query processing area. A classic example of data pruning is the partition pruning, which is used when table data is range or list partitioned. But lately, techniques have been developed to prune data at a lower granularity than a table partition or sub-partition. A good example is the use of data pruning structure called zone map. A zone map prunes zones of data from a table on which it is defined. Data pruning via zone map is very effective when the table data is clustered by the filtering columns. The database industry has offered support to cluster data in tables by its local columns, and to define zone maps on clustering columns of such tables. This has helped improve the performance of queries that contain filter predicates on local columns. However, queries in data warehouses are typically based on star/snowflake schema with filter predicates usually on columns of the dimension tables joined to a fact table. Given this, the performance of data warehouse queries can be significantly improved if the fact table data is clustered by columns of dimension tables together with zone maps that maintain min/max value ranges of these clustering columns over zones of fact table data. In recognition of this opportunity of significantly improving the performance of data warehouse queries, Oracle 12c release 1 has introduced the support for dimension based clustering of fact tables together with data pruning of the fact tables via dimension based zone maps.},
	number = {12},
	urldate = {2023-08-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ziauddin, Mohamed and Witkowski, Andrew and Kim, You Jung and Potapov, Dmitry and Lahorani, Janaki and Krishna, Murali},
	month = aug,
	year = {2017},
	pages = {1622--1633},
	file = {Full Text PDF:/home/ryan/Zotero/storage/9UDFSMZY/Ziauddin et al. - 2017 - Dimensions based data clustering and zone maps.pdf:application/pdf},
}

@article{sgx_sandbox,
	series = {{TOCS} '18},
	title = {Ryoan: {A} {Distributed} {Sandbox} for {Untrusted} {Computation} on {Secret} {Data}},
	volume = {35},
	issn = {0734-2071},
	shorttitle = {Ryoan},
	url = {https://dl.acm.org/doi/10.1145/3231594},
	doi = {10.1145/3231594},
	abstract = {Users of modern data-processing services such as tax preparation or genomic screening are forced to trust them with data that the users wish to keep secret. Ryoan1 protects secret data while it is processed by services that the data owner does not trust. Accomplishing this goal in a distributed setting is difficult, because the user has no control over the service providers or the computational platform. Confining code to prevent it from leaking secrets is notoriously difficult, but Ryoan benefits from new hardware and a request-oriented data model. Ryoan provides a distributed sandbox, leveraging hardware enclaves (e.g., Intel‚Äôs software guard extensions (SGX) [40]) to protect sandbox instances from potentially malicious computing platforms. The protected sandbox instances confine untrusted data-processing modules to prevent leakage of the user‚Äôs input data. Ryoan is designed for a request-oriented data model, where confined modules only process input once and do not persist state about the input. We present the design and prototype implementation of Ryoan and evaluate it on a series of challenging problems including email filtering, health analysis, image processing and machine translation.},
	number = {4},
	urldate = {2023-08-19},
	journal = {ACM Transactions on Computer Systems},
	author = {Hunt, Tyler and Zhu, Zhiting and Xu, Yuanzhong and Peter, Simon and Witchel, Emmett},
	month = dec,
	year = {2018},
	keywords = {enclaves, Intel SGX, private computation, sandboxing, untrusted OS},
	pages = {13:1--13:32},
	file = {Hunt et al. - 2018 - Ryoan A Distributed Sandbox for Untrusted Computa.pdf:/home/ryan/Zotero/storage/8HBX93ZC/Hunt et al. - 2018 - Ryoan A Distributed Sandbox for Untrusted Computa.pdf:application/pdf},
}

@inproceedings{sgx_containers,
	address = {USA},
	series = {{OSDI} '16},
	title = {{SCONE}: secure {Linux} containers with {Intel} {SGX}},
	isbn = {978-1-931971-33-1},
	shorttitle = {{SCONE}},
	abstract = {In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6√ó-1.2√ó of native throughput.},
	urldate = {2023-08-19},
	booktitle = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Arnautov, Sergei and Trach, Bohdan and Gregor, Franz and Knauth, Thomas and Martin, Andre and Priebe, Christian and Lind, Joshua and Muthukumaran, Divya and O'Keeffe, Dan and Stillwell, Mark L. and Goltzsche, David and Eyers, David and Kapitza, R√ºdiger and Pietzuch, Peter and Fetzer, Christof},
	month = nov,
	year = {2016},
	pages = {689--703},
	file = {Arnautov et al. - 2016 - SCONE secure Linux containers with Intel SGX.pdf:/home/ryan/Zotero/storage/GX53E5PH/Arnautov et al. - 2016 - SCONE secure Linux containers with Intel SGX.pdf:application/pdf},
}

@inproceedings{rlshard_workshop,
	address = {Vancouver, BC, Canada},
	series = {{AIDB}@{VLDB} '23},
	title = {Towards {Adaptive} {Fault}-{Tolerant} {Sharded} {Databases}},
	copyright = {All rights reserved},
	booktitle = {Joint {Workshops} at 49th {International} {Conference} on {Very} {Large} {Data} {Bases}},
	author = {Mehta, Bhavana and Kumar, Neelesh Chinnakonda Ashok and Iyer, Prashanth S and Amiri, Mohammad Javad and Loo, Boon Thau and Marcus, Ryan},
	month = sep,
	year = {2023},
	file = {Mehta et al. - 2023 - Towards Adaptive Fault-Tolerant Sharded Databases.pdf:/home/ryan/Zotero/storage/AAG2JG9F/Mehta et al. - 2023 - Towards Adaptive Fault-Tolerant Sharded Databases.pdf:application/pdf},
}

@inproceedings{qo_insight,
	address = {Vancouver, BC, Canada},
	series = {{VLDB} '23},
	title = {{QO}-{Insight}: {Inspecting} {Steered} {Query} {Optimizers}},
	volume = {16},
	copyright = {All rights reserved},
	doi = {10.14778/3611540.3611586},
	abstract = {Steered query optimizers address the planning mistakes of tradi-
tional query optimizers by providing them with hints on a per-query
basis, thereby guiding them in the right direction. This paper in-
troduces QO-Insight, a visual tool designed for exploring query
execution traces of such steered query optimizers. Although steered
query optimizers are typically perceived as black boxes, QO-Insight
empowers database administrators and experts to gain qualitative
insights and enhance their performance through visual inspection
and analysis.},
	booktitle = {Proceedings of the {VLDB} {Endowment}},
	author = {Anneser, Christoph and Petruccelli, Mario and Tatbul, Nesime and Cohen, David and Xu, Zhenggang and Pandian, Prithviraj and Laptev, Nikolay and Marcus, Ryan and Kemper, Alfons},
	month = aug,
	year = {2023},
	note = {props: demo},
	file = {Anneser et al. - 2023 - QO-Insight Inspecting Steered Query Optimizers.pdf:/home/ryan/Zotero/storage/M7H5AGWV/Anneser et al. - 2023 - QO-Insight Inspecting Steered Query Optimizers.pdf:application/pdf},
}

@article{multi_agent_packets,
	series = {{COMST} '22},
	title = {Applications of {Multi}-{Agent} {Reinforcement} {Learning} in {Future} {Internet}: {A} {Comprehensive} {Survey}},
	volume = {24},
	issn = {1553-877X},
	shorttitle = {Applications of {Multi}-{Agent} {Reinforcement} {Learning} in {Future} {Internet}},
	doi = {10.1109/COMST.2022.3160697},
	abstract = {Future Internet involves several emerging technologies such as 5G and beyond 5G networks, vehicular networks, unmanned aerial vehicle (UAV) networks, and Internet of Things (IoTs). Moreover, the future Internet becomes heterogeneous and decentralized with a large number of involved network entities. Each entity may need to make its local decision to improve the network performance under dynamic and uncertain network environments. Standard learning algorithms such as single-agent Reinforcement Learning (RL) or Deep Reinforcement Learning (DRL) have been recently used to enable each network entity as an agent to learn an optimal decision-making policy adaptively through interacting with the unknown environments. However, such an algorithm fails to model the cooperations or competitions among network entities, and simply treats other entities as a part of the environment that may result in the non-stationarity issue. Multi-agent Reinforcement Learning (MARL) allows each network entity to learn its optimal policy by observing not only the environments but also other entities‚Äô policies. As a result, MARL can significantly improve the learning efficiency of the network entities, and it has been recently used to solve various issues in the emerging networks. In this paper, we thus review the applications of MARL in emerging networks. In particular, we provide a tutorial of MARL and a comprehensive survey of applications of MARL in next-generation Internet. In particular, we first introduce single-agent RL and MARL. Then, we review a number of applications of MARL to solve emerging issues in the future Internet. The issues consist of network access, transmit power control, computation offloading, content caching, packet routing, trajectory design for UAV-aided networks, and network security issues. Finally, we discuss the challenges, open issues, and future directions related to the applications of MARL in the future Internet.},
	number = {2},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Li, Tianxu and Zhu, Kun and Luong, Nguyen Cong and Niyato, Dusit and Wu, Qihui and Zhang, Yang and Chen, Bing},
	year = {2022},
	keywords = {Deep learning, Decision making, future Internet, Internet of Things, Internet technologies, Markov processes, Multi-agent reinforcement learning, network access, network security, packet routing, Power control, Reinforcement learning, task offloading, Tutorials},
	pages = {1240--1279},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/T97JWMXQ/9738819.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/FKFTHUYZ/Li et al. - 2022 - Applications of Multi-Agent Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{superopt_vision,
	address = {Vancouver, BC, Canada},
	series = {{AIDB}@{VLDB} '23},
	title = {Learned {Query} {Superoptimization}},
	copyright = {All rights reserved},
	abstract = {Traditional query optimizers are designed to be fast and stateless: each query is quickly optimized using approximate statistics,
sent off to the execution engine, and promptly forgotten. Recent work on learned query optimization have shown that it is
possible for a query optimizer to ‚Äúlearn from its mistakes,‚Äù correcting erroneous query plans the next time a plan is produced.
But what if query optimizers could avoid mistakes entirely? This paper presents the idea of learned query superoptimization. A
new generation of query superoptimizers could autonomously experiment to discover optimal plans using exploration-driven
algorithms, iterative Bayesian optimization, and program synthesis. While such superoptimizers will take significantly longer
to optimize a given query, superoptimizers have the potential to massively accelerate a large number of important repetitive
queries being executed on data systems today.},
	booktitle = {Joint {Workshops} at 49th {International} {Conference} on {Very} {Large} {Data} {Bases}},
	publisher = {CEUR Workshop Proceedings},
	author = {Marcus, Ryan},
	month = aug,
	year = {2023},
	file = {Marcus - 2023 - Learned Query Superoptimization.pdf:/home/ryan/Zotero/storage/6NQN3S6F/Marcus - 2023 - Learned Query Superoptimization.pdf:application/pdf},
}

@inproceedings{abadiColumnstoresVsRowstores2008,
	address = {Vancouver Canada},
	series = {{SIGMOD} '08},
	title = {Column-stores vs. row-stores: how different are they really?},
	isbn = {978-1-60558-102-6},
	shorttitle = {Column-stores vs. row-stores},
	url = {https://dl.acm.org/doi/10.1145/1376616.1376712},
	doi = {10.1145/1376616.1376712},
	abstract = {There has been a signiÔ¨Åcant amount of excitement and recent work on column-oriented database systems (‚Äúcolumn-stores‚Äù). These database systems have been shown to perform more than an order of magnitude better than traditional row-oriented database systems (‚Äúrow-stores‚Äù) on analytical workloads such as those found in data warehouses, decision support, and business intelligence applications. The elevator pitch behind this performance difference is straightforward: column-stores are more I/O efÔ¨Åcient for read-only queries since they only have to read from disk (or from memory) those attributes accessed by a query.},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM},
	author = {Abadi, Daniel J. and Madden, Samuel R. and Hachem, Nabil},
	month = jun,
	year = {2008},
	pages = {967--980},
	file = {Abadi et al. - 2008 - Column-stores vs. row-stores how different are th.pdf:/home/ryan/Zotero/storage/NNU8HX24/Abadi et al. - 2008 - Column-stores vs. row-stores how different are th.pdf:application/pdf},
}

@inproceedings{hentschelColumnSketchesScan2018,
	address = {Houston TX USA},
	series = {{SIGMOD} '18},
	title = {Column {Sketches}: {A} {Scan} {Accelerator} for {Rapid} and {Robust} {Predicate} {Evaluation}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {Column {Sketches}},
	url = {https://dl.acm.org/doi/10.1145/3183713.3196911},
	doi = {10.1145/3183713.3196911},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Hentschel, Brian and Kester, Michael S. and Idreos, Stratos},
	month = may,
	year = {2018},
	pages = {857--872},
	file = {Hentschel et al. - 2018 - Column Sketches A Scan Accelerator for Rapid and .pdf:/home/ryan/Zotero/storage/J3NE46IZ/Hentschel et al. - 2018 - Column Sketches A Scan Accelerator for Rapid and .pdf:application/pdf},
}

@inproceedings{abadiIntegratingCompressionExecution2006,
	address = {Chicago IL USA},
	series = {{SIGMOD} '06},
	title = {Integrating compression and execution in column-oriented database systems},
	isbn = {978-1-59593-434-5},
	url = {https://dl.acm.org/doi/10.1145/1142473.1142548},
	doi = {10.1145/1142473.1142548},
	abstract = {Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2006 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM},
	author = {Abadi, Daniel and Madden, Samuel and Ferreira, Miguel},
	month = jun,
	year = {2006},
	pages = {671--682},
	file = {Abadi et al. - 2006 - Integrating compression and execution in column-or.pdf:/home/ryan/Zotero/storage/K36G86IB/Abadi et al. - 2006 - Integrating compression and execution in column-or.pdf:application/pdf},
}

@article{neumannEfficientlyCompilingEfficient2011,
	series = {{VLDB} '11},
	title = {Efficiently compiling efficient query plans for modern hardware},
	volume = {4},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2002938.2002940},
	doi = {10.14778/2002938.2002940},
	abstract = {As main memory grows, query performance is more and more determined by the raw CPU costs of query processing itself. The classical iterator style query processing technique is very simple and Ô¨Çexible, but shows poor performance on modern CPUs due to lack of locality and frequent instruction mispredictions. Several techniques like batch oriented processing or vectorized tuple processing have been proposed in the past to improve this situation, but even these techniques are frequently out-performed by hand-written execution plans.},
	language = {en},
	number = {9},
	urldate = {2023-08-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Neumann, Thomas},
	month = jun,
	year = {2011},
	pages = {539--550},
	file = {Neumann - 2011 - Efficiently compiling efficient query plans for mo.pdf:/home/ryan/Zotero/storage/8F6DB3E4/Neumann - 2011 - Efficiently compiling efficient query plans for mo.pdf:application/pdf},
}

@article{kerstenEverythingYouAlways2018,
	series = {{VLDB} '18},
	title = {Everything you always wanted to know about compiled and vectorized queries but were afraid to ask},
	volume = {11},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3275366.3284966},
	doi = {10.14778/3275366.3275370},
	abstract = {The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-speciÔ¨Åc choices obstruct a direct comparison of architectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We Ô¨Ånd that both are efÔ¨Åcient, but have different strengths and weaknesses. Vectorization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which beneÔ¨Åts cacheresident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.},
	language = {en},
	number = {13},
	urldate = {2023-08-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
	month = sep,
	year = {2018},
	pages = {2209--2222},
	file = {Kersten et al. - 2018 - Everything you always wanted to know about compile.pdf:/home/ryan/Zotero/storage/MG9YU7QZ/Kersten et al. - 2018 - Everything you always wanted to know about compile.pdf:application/pdf},
}

@incollection{garcia-molinaQueryCompiler2009,
	address = {Upper Saddle River},
	edition = {2},
	title = {The {Query} {Compiler}},
	isbn = {0-13-187325-3},
	language = {English},
	booktitle = {Database {Systems}: {The} {Complete} {Book}},
	publisher = {Pearson},
	author = {Garcia-Molina, Hector and Ullman, Jeffery D. and Widom, Jennifer},
	year = {2009},
	pages = {759--842},
	file = {Garcia-Molina et al. - 2009 - The Query Compiler.pdf:/home/ryan/Zotero/storage/Z4ECSNR7/Garcia-Molina et al. - 2009 - The Query Compiler.pdf:application/pdf},
}

@inproceedings{bandlePartitionNotPartition2021,
	address = {Virtual Event China},
	title = {To {Partition}, or {Not} to {Partition}, {That} is the {Join} {Question} in a {Real} {System}},
	isbn = {978-1-4503-8343-1},
	url = {https://dl.acm.org/doi/10.1145/3448016.3452831},
	doi = {10.1145/3448016.3452831},
	abstract = {An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance.},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Bandle, Maximilian and Giceva, Jana and Neumann, Thomas},
	month = jun,
	year = {2021},
	pages = {168--180},
	file = {Bandle et al. - 2021 - To Partition, or Not to Partition, That is the Joi.pdf:/home/ryan/Zotero/storage/GWNETFI4/Bandle et al. - 2021 - To Partition, or Not to Partition, That is the Joi.pdf:application/pdf},
}

@article{albutiuMassivelyParallelSortmerge2012,
	series = {{VLDB} '12},
	title = {Massively parallel sort-merge joins in main memory multi-core database systems},
	volume = {5},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2336664.2336678},
	doi = {10.14778/2336664.2336678},
	abstract = {Two emerging hardware trends will dominate the database system technology in the near future: increasing main memory capacities of several TB per server and massively parallel multi-core processing. Many algorithmic and control techniques in current database technology were devised for diskbased systems where I/O dominated the performance. In this work we take a new look at the well-known sort-merge join which, so far, has not been in the focus of research in scalable massively parallel multi-core data processing as it was deemed inferior to hash joins. We devise a suite of new massively parallel sort-merge (MPSM) join algorithms that are based on partial partition-based sorting. Contrary to classical sort-merge joins, our MPSM algorithms do not rely on a hard to parallelize Ô¨Ånal merge step to create one complete sort order. Rather they work on the independently created runs in parallel. This way our MPSM algorithms are NUMA-aÔ¨Éne as all the sorting is carried out on local memory partitions. An extensive experimental evaluation on a modern 32-core machine with one TB of main memory proves the competitive performance of MPSM on large main memory databases with billions of objects. It scales (almost) linearly in the number of employed cores and clearly outperforms competing hash join proposals ‚Äì in particular it outperforms the ‚Äúcutting-edge‚Äù Vectorwise parallel query engine by a factor of four.},
	language = {en},
	number = {10},
	urldate = {2023-08-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Albutiu, Martina-Cezara and Kemper, Alfons and Neumann, Thomas},
	month = jun,
	year = {2012},
	pages = {1064--1075},
	file = {Albutiu et al. - 2012 - Massively parallel sort-merge joins in main memory.pdf:/home/ryan/Zotero/storage/5X2KDFE3/Albutiu et al. - 2012 - Massively parallel sort-merge joins in main memory.pdf:application/pdf},
}

@article{freitagAdoptingWorstcaseOptimal2020,
	series = {{VLDB} '20},
	title = {Adopting worst-case optimal joins in relational database systems},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3407790.3407797},
	doi = {10.14778/3407790.3407797},
	abstract = {Worst-case optimal join algorithms are attractive from a theoretical point of view, as they offer asymptotically better runtime than binary joins on certain types of queries. In particular, they avoid enumerating large intermediate results by processing multiple input relations in a single multiway join. However, existing implementations incur a sizable overhead in practice, primarily since they rely on suitable ordered index structures on their input. Systems that support worst-case optimal joins often focus on a specific problem domain, such as read-only graph analytic queries, where extensive precomputation allows them to mask these costs. In this paper, we present a comprehensive implementation approach for worst-case optimal joins that is practical within general-purpose relational database management systems supporting both hybrid transactional and analytical workloads. The key component of our approach is a novel hash-based worst-case optimal join algorithm that relies only on data structures that can be built efficiently during query execution. Furthermore, we implement a hybrid query optimizer that intelligently and transparently combines both binary and multi-way joins within the same query plan. We demonstrate that our approach far outperforms existing systems when worst-case optimal joins are beneficial while sacrificing no performance when they are not.},
	language = {en},
	number = {12},
	urldate = {2023-08-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Freitag, Michael and Bandle, Maximilian and Schmidt, Tobias and Kemper, Alfons and Neumann, Thomas},
	month = aug,
	year = {2020},
	pages = {1891--1904},
	file = {Freitag et al. - 2020 - Adopting worst-case optimal joins in relational da.pdf:/home/ryan/Zotero/storage/QKW6AKQB/Freitag et al. - 2020 - Adopting worst-case optimal joins in relational da.pdf:application/pdf},
}

@inproceedings{nathanLearningMultidimensionalIndexes2020,
	title = {Learning {Multi}-dimensional {Indexes}},
	url = {http://arxiv.org/abs/1912.01668},
	doi = {10.1145/3318464.3380579},
	abstract = {Scanning and filtering over multi-dimensional tables are key operations in modern analytical database engines. To optimize the performance of these operations, databases often create clustered indexes over a single dimension or multidimensional indexes such as R-Trees, or use complex sort orders (e.g., Z-ordering). However, these schemes are often hard to tune and their performance is inconsistent across different datasets and queries. In this paper, we introduce Flood, a multi-dimensional in-memory read-optimized index that automatically adapts itself to a particular dataset and workload by jointly optimizing the index structure and data storage layout. Flood achieves up to three orders of magnitude faster performance for range scans with predicates than state-of-theart multi-dimensional indexes or sort orders on real-world datasets and workloads. Our work serves as a building block towards an end-to-end learned database system.},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
	month = jun,
	year = {2020},
	note = {arXiv:1912.01668 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases, Computer Science - Data Structures and Algorithms},
	pages = {985--1000},
	file = {Nathan et al. - 2020 - Learning Multi-dimensional Indexes.pdf:/home/ryan/Zotero/storage/F7FNY7TT/Nathan et al. - 2020 - Learning Multi-dimensional Indexes.pdf:application/pdf},
}

@inproceedings{kristoCaseLearnedSorting2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {The {Case} for a {Learned} {Sorting} {Algorithm}},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389752},
	doi = {10.1145/3318464.3389752},
	abstract = {Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases not just for sorting query results but also as part of joins (i.e., sort-merge-join) or indexing. In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data. Our algorithm uses a model to efficiently get an approximation of the scaled empirical CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a totally sorted order. We compared this algorithm against common sorting approaches and measured its performance for up to 1 billion normally-distributed double-precision keys. The results show that our approach yields an average 3.38x performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 1.49x improvement over sequential Radix Sort, and 5.54x improvement over a C++ implementation of Timsort, which is the default sorting function for Java and Python.},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Kristo, Ani and Vaidya, Kapil and √áetintemel, Ugur and Misra, Sanchit and Kraska, Tim},
	month = may,
	year = {2020},
	keywords = {CDF, learned algorithm, linear interpolation, linear models, ML for systems, RMI, sorting, sorting algorithm},
	pages = {1001--1016},
	file = {Kristo et al. - 2020 - The Case for a Learned Sorting Algorithm.pdf:/home/ryan/Zotero/storage/CUTKDH3X/Kristo et al. - 2020 - The Case for a Learned Sorting Algorithm.pdf:application/pdf},
}

@article{sabekCanLearnedModels2022,
	title = {Can {Learned} {Models} {Replace} {Hash} {Functions}?},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3570690.3570702},
	doi = {10.14778/3570690.3570702},
	abstract = {Hashing is a fundamental operation in database management, playing a key role in the implementation of numerous core database data structures and algorithms. Traditional hash functions aim to mimic a function that maps a key to a random value, which can result in collisions, where multiple keys are mapped to the same value. There are many well-known schemes like chaining, probing, and cuckoo hashing to handle collisions. In this work, we aim to study if using learned models instead of traditional hash functions can reduce collisions and whether such a reduction translates to improved performance, particularly for indexing and joins. We show that learned models reduce collisions in some cases, which depend on how the data is distributed. To evaluate the effectiveness of learned models as hash function, we test them with bucket chaining, linear probing, and cuckoo hash tables. We find that learned models can (1) yield a 1.4x lower probe latency, and (2) reduce the non-partitioned hash join runtime with 28\% over the next best baseline for certain datasets. On the other hand, if the data distribution is not suitable, we either do not see gains or see worse performance. In summary, we find that learned models can indeed outperform hash functions, but only for certain data distributions.},
	language = {en},
	number = {3},
	urldate = {2023-08-26},
	journal = {Proceedings of the VLDB Endowment},
	author = {Sabek, Ibrahim and Vaidya, Kapil and Horn, Dominik and Kipf, Andreas and Mitzenmacher, Michael and Kraska, Tim},
	month = nov,
	year = {2022},
	pages = {532--545},
	file = {Sabek et al. - 2022 - Can Learned Models Replace Hash Functions.pdf:/home/ryan/Zotero/storage/SD43RXES/Sabek et al. - 2022 - Can Learned Models Replace Hash Functions.pdf:application/pdf},
}

@inproceedings{raoMakingTreesCache2000,
	address = {Dallas Texas USA},
	series = {{SIGMOD} '00},
	title = {Making {B}+- trees cache conscious in main memory},
	isbn = {978-1-58113-217-5},
	url = {https://dl.acm.org/doi/10.1145/342009.335449},
	doi = {10.1145/342009.335449},
	language = {en},
	urldate = {2023-08-26},
	booktitle = {Proceedings of the 2000 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM},
	author = {Rao, Jun and Ross, Kenneth A.},
	month = may,
	year = {2000},
	pages = {475--486},
	file = {Rao and Ross - 2000 - Making B+- trees cache conscious in main memory.pdf:/home/ryan/Zotero/storage/DS47U88Q/Rao and Ross - 2000 - Making B+- trees cache conscious in main memory.pdf:application/pdf},
}

@article{fastgres,
	series = {{VLDB} '23},
	title = {{FASTgres}: {Making} {Learned} {Query} {Optimizer} {Hinting} {Effective}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {{FASTgres}},
	url = {https://dl.acm.org/doi/10.14778/3611479.3611528},
	doi = {10.14778/3611479.3611528},
	number = {11},
	urldate = {2023-09-29},
	journal = {Proceedings of the VLDB Endowment},
	author = {Woltmann, Lucas and Thiessat, Jerome and Hartmann, Claudio and Habich, Dirk and Lehner, Wolfgang},
	month = aug,
	year = {2023},
	pages = {3310--3322},
	file = {Full Text PDF:/home/ryan/Zotero/storage/6URAVGEV/Woltmann et al. - 2023 - FASTgres Making Learned Query Optimizer Hinting E.pdf:application/pdf},
}

@article{wavelet_for_all,
	title = {Wavelet trees for all},
	volume = {25},
	issn = {15708667},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1570866713000610},
	doi = {10.1016/j.jda.2013.07.004},
	abstract = {The wavelet tree is a versatile data structure that serves a number of purposes, from string processing to computational geometry. It can be regarded as a device that represents a sequence, a reordering, or a grid of points. In addition, its space adapts to various entropy measures of the data it encodes, enabling compressed representations. New competitive solutions to a number of problems, based on wavelet trees, are appearing every year. In this survey we give an overview of wavelet trees and the surprising number of applications in which we have found them useful: basic and weighted point grids, sets of rectangles, strings, permutations, binary relations, graphs, inverted indexes, document retrieval indexes, full-text indexes, XML indexes, and general numeric sequences.},
	language = {en},
	urldate = {2023-11-01},
	journal = {Journal of Discrete Algorithms},
	author = {Navarro, Gonzalo},
	month = mar,
	year = {2014},
	pages = {2--20},
	file = {Navarro - 2014 - Wavelet trees for all.pdf:/home/ryan/Zotero/storage/W5SQND7T/Navarro - 2014 - Wavelet trees for all.pdf:application/pdf},
}

@article{wavelet_matrix,
	title = {The wavelet matrix: {An} efficient wavelet tree for large alphabets},
	volume = {47},
	issn = {03064379},
	shorttitle = {The wavelet matrix},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306437914000945},
	doi = {10.1016/j.is.2014.06.002},
	abstract = {The wavelet tree is a flexible data structure that permits representing sequences S¬Ω1; n¬ä of symbols over an alphabet of size œÉ, within compressed space and supporting a wide range of operations on S. When œÉ is significant compared to n, current wavelet tree representations incur in noticeable space or time overheads. In this article we introduce the wavelet matrix, an alternative representation for large alphabets that retains all the properties of wavelet trees but is significantly faster. We also show how the wavelet matrix can be compressed up to the zero-order entropy of the sequence without sacrificing, and actually improving, its time performance. Our experimental results show that the wavelet matrix outperforms all the wavelet tree variants along the space/time tradeoff map.},
	language = {en},
	urldate = {2023-11-01},
	journal = {Information Systems},
	author = {Claude, Francisco and Navarro, Gonzalo and Ord√≥√±ez, Alberto},
	month = jan,
	year = {2015},
	pages = {15--32},
	file = {Claude et al. - 2015 - The wavelet matrix An efficient wavelet tree for .pdf:/home/ryan/Zotero/storage/SXZ72UD5/Claude et al. - 2015 - The wavelet matrix An efficient wavelet tree for .pdf:application/pdf},
}

@article{bcgnn_perm,
	title = {Efficient {Fully}-{Compressed} {Sequence} {Representations}},
	volume = {69},
	issn = {0178-4617, 1432-0541},
	url = {http://link.springer.com/10.1007/s00453-012-9726-3},
	doi = {10.1007/s00453-012-9726-3},
	abstract = {We present a data structure that stores a sequence s[1..n] over alphabet [1..œÉ] in nH0(s) + o(n)(H0(s)+1) bits, where H0(s) is the zero-order entropy of s. This structure supports the queries access, rank and select, which are fundamental building blocks for many other compressed data structures, in worst-case time O (lg lg œÉ) and average time O (lg H0(s)). The worst-case complexity matches the best previous results, yet these had been achieved with data structures using nH0(s) + o(n lg œÉ) bits. On highly compressible sequences the o(n lg œÉ) bits of the redundancy may be signiÔ¨Åcant compared to the the nH0(s) bits that encode the data. Our representation, instead, compresses the redundancy as well. Moreover, our average-case complexity is unprecedented.},
	language = {en},
	number = {1},
	urldate = {2023-11-01},
	journal = {Algorithmica},
	author = {Barbay, J√©r√©my and Claude, Francisco and Gagie, Travis and Navarro, Gonzalo and Nekrich, Yakov},
	month = may,
	year = {2014},
	pages = {232--268},
	file = {Barbay et al. - 2014 - Efficient Fully-Compressed Sequence Representation.pdf:/home/ryan/Zotero/storage/9TTTT39Q/Barbay et al. - 2014 - Efficient Fully-Compressed Sequence Representation.pdf:application/pdf},
}

@inproceedings{wavelet_orig,
	series = {{ACM} {SIAM} '03},
	title = {High-order entropy-compressed text indexes},
	language = {English},
	author = {Grossi, R. and Gupta, A. and Vitter, J.S.},
	year = {2003},
	pages = {841--850},
	file = {Grossi et al. - 2003 - High-order entropy-compressed text indexes.pdf:/home/ryan/Zotero/storage/JBWINU9W/Grossi et al. - 2003 - High-order entropy-compressed text indexes.pdf:application/pdf},
}

@article{yannakakisAlgorithmsAcyclicDatabase,
	series = {{VLDB} '81},
	title = {Algorithms {For} {Acyclic} {Database} {Schemes}},
	language = {en},
	author = {Yannakakis, Mihalis},
}

@inproceedings{yannakakis,
	series = {{VLDB} '81},
	title = {Algorithms for {Acyclic} {Database} {Schemes}},
	url = {https://www.semanticscholar.org/paper/Algorithms-for-Acyclic-Database-Schemes-Yannakakis/fe0b45175713c4637486956f63f9234685a88c1c},
	urldate = {2023-11-09},
	booktitle = {Very {Large} {Data} {Bases} {Conference}},
	author = {Yannakakis, M.},
	month = sep,
	year = {1981},
	pages = {82--94},
	file = {Yannakakis - 1981 - Algorithms for Acyclic Database Schemes.pdf:/home/ryan/Zotero/storage/3KE5IWLI/Yannakakis - 1981 - Algorithms for Acyclic Database Schemes.pdf:application/pdf},
}

@inproceedings{adapt_bft,
	series = {{IPDPS} '15},
	title = {Making {BFT} {Protocols} {Really} {Adaptive}},
	url = {https://ieeexplore.ieee.org/document/7161576},
	doi = {10.1109/IPDPS.2015.21},
	urldate = {2023-11-14},
	booktitle = {2015 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	author = {Bahsoun, Jean-Paul and Guerraoui, Rachid and Shoker, Ali},
	month = may,
	year = {2015},
	note = {ISSN: 1530-2075},
	pages = {904--913},
	file = {IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/VBPNBL2A/7161576.html:text/html;IEEE Xplore Full Text PDF:/home/ryan/Zotero/storage/YF7VMGJ8/Bahsoun et al. - 2015 - Making BFT Protocols Really Adaptive.pdf:application/pdf},
}

@article{zhuLookingAheadMakes2017,
	title = {Looking ahead makes query plans robust: making the initial case with in-memory star schema data warehouse workloads},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {Looking ahead makes query plans robust},
	url = {https://doi.org/10.14778/3090163.3090167},
	doi = {10.14778/3090163.3090167},
	abstract = {Query optimizers and query execution engines cooperate to deliver high performance on complex analytic queries. Typically, the optimizer searches through the plan space and sends a selected plan to the execution engine. However, optimizers may at times miss the optimal plan, with sometimes disastrous impact on performance. In this paper, we develop the notion of robustness of a query evaluation strategy with respect to a space of query plans. We also propose a novel query execution strategy called Lookahead Information Passing (LIP) that is robust with respect to the space of (fully pipeline-able) left-deep query plan trees for in-memory star schema data warehouses. LIP ensures that execution times for the best and the worst case plans are far closer than without LIP. In fact, under certain assumptions of independent and uniform distributions, any plan in that space is theoretically guaranteed to execute in near-optimal time. LIP ensures that the execution time for every plan in the space is nearly-optimal. In this paper, we also evaluate these claims using workloads that include skew and correlation. With LIP we make an initial foray into a novel way of thinking about robustness from the perspective of query evaluation, where we develop strategies (like LIP) that collapse plan sub-spaces in the overall global plan space.},
	number = {8},
	urldate = {2023-11-11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhu, Jianqiao and Potti, Navneet and Saurabh, Saket and Patel, Jignesh M.},
	month = apr,
	year = {2017},
	pages = {889--900},
	file = {Zhu et al. - 2017 - Looking ahead makes query plans robust making the.pdf:/home/ryan/Zotero/storage/8WKVR5CM/Zhu et al. - 2017 - Looking ahead makes query plans robust making the.pdf:application/pdf},
}

@article{gcn,
	series = {{arXiv} '16},
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1609.02907},
	doi = {10.48550/ARXIV.1609.02907},
	urldate = {2023-11-27},
	author = {Kipf, Thomas N. and Welling, Max},
	year = {2016},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{ready_for_lce,
	series = {{VLDB} '21},
	title = {Are we ready for learned cardinality estimation?},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3461535.3461552},
	doi = {10.14778/3461535.3461552},
	language = {en},
	number = {9},
	urldate = {2023-12-01},
	journal = {Proceedings of the VLDB Endowment},
	author = {Wang, Xiaoying and Qu, Changbo and Wu, Weiyuan and Wang, Jiannan and Zhou, Qingqing},
	month = may,
	year = {2021},
	pages = {1640--1654},
	file = {Wang et al. - 2021 - Are we ready for learned cardinality estimation.pdf:/home/ryan/Zotero/storage/EGASDBRK/Wang et al. - 2021 - Are we ready for learned cardinality estimation.pdf:application/pdf},
}

@inproceedings{polyjuice,
	series = {{OSDI} '21},
	title = {Polyjuice: \{{High}-{Performance}\} {Transactions} via {Learned} {Concurrency} {Control}},
	isbn = {978-1-939133-22-9},
	shorttitle = {Polyjuice},
	url = {https://www.usenix.org/conference/osdi21/presentation/wang-jiachen},
	language = {en},
	urldate = {2023-12-05},
	author = {Wang, Jiachen and Ding, Ding and Wang, Huan and Christensen, Conrad and Wang, Zhaoguo and Chen, Haibo and Li, Jinyang},
	year = {2021},
	pages = {198--216},
	file = {Full Text PDF:/home/ryan/Zotero/storage/8QHHRZGJ/Wang et al. - 2021 - Polyjuice High-Performance Transactions via Lea.pdf:application/pdf},
}

@article{lero,
	series = {{VLDB} '23},
	title = {Lero: {A} {Learning}-to-{Rank} {Query} {Optimizer}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {Lero},
	url = {https://doi.org/10.14778/3583140.3583160},
	doi = {10.14778/3583140.3583160},
	number = {6},
	urldate = {2023-12-05},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhu, Rong and Chen, Wei and Ding, Bolin and Chen, Xingguang and Pfadler, Andreas and Wu, Ziniu and Zhou, Jingren},
	month = feb,
	year = {2023},
	pages = {1466--1479},
	file = {Submitted Version:/home/ryan/Zotero/storage/LTLSKCPS/Zhu et al. - 2023 - Lero A Learning-to-Rank Query Optimizer.pdf:application/pdf},
}

@inproceedings{stage,
	address = {Santiago, Chile},
	series = {{SIGMOD} '24},
	title = {Stage: {Query} {Execution} {Time} {Prediction} in {Amazon} {Redshift}},
	copyright = {All rights reserved},
	doi = {10.48550/arXiv.2403.02286},
	booktitle = {Proceedings of the 2024 {International} {Conference} on {Management} of {Data} ({SIGMOD} ‚Äô24)},
	author = {Wu, Ziniu and Marcus, Ryan and Liu, Zhengchun and Negi, Parimarjan and Nathan, Vikram and Pfeil, Pascal and Saxena, Gaurav and Rahman, Mohammad and Narayanaswamy, Balakrishnan and Kraska, Tim},
	month = jun,
	year = {2024},
	file = {Wu et al. - 2024 - Stage Query Execution Time Prediction in Amazon R.pdf:/home/ryan/Zotero/storage/J27E3V6N/Wu et al. - 2024 - Stage Query Execution Time Prediction in Amazon R.pdf:application/pdf},
}

@article{slabcity,
	series = {{VLDB} '23},
	title = {{SlabCity}: {Whole}-{Query} {Optimization} {Using} {Program} {Synthesis}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {{SlabCity}},
	url = {https://dl.acm.org/doi/10.14778/3611479.3611515},
	doi = {10.14778/3611479.3611515},
	abstract = {Query rewriting is often a prerequisite for effective query optimization, particularly for poorly-written queries. Prior work on query rewriting has relied on a set of "rules" based on syntactic pattern-matching. Whether relying on manual rules or auto-generated ones, rule-based query rewriters are inherently limited in their ability to handle new query patterns. Their success is limited by the quality and quantity of the rules provided to them. To our knowledge, we present the first synthesis-based query rewriting technique, SlabCity, capable of whole-query optimization without relying on any rewrite rules. SlabCity directly searches the space of SQL queries using a novel query synthesis algorithm that leverages a new concept called query dataflows. We evaluate SlabCity on four workloads, including a newly curated benchmark with more than 1000 real-life queries. We show that not only can SlabCity optimize more queries than state-of-the-art query rewriting techniques, but interestingly, it also leads to queries that are significantly faster than those generated by rule-based systems.},
	number = {11},
	urldate = {2023-12-17},
	journal = {Proceedings of the VLDB Endowment},
	author = {Dong, Rui and Liu, Jie and Zhu, Yuxuan and Yan, Cong and Mozafari, Barzan and Wang, Xinyu},
	month = jul,
	year = {2023},
	pages = {3151--3164},
	file = {Dong et al. - 2023 - SlabCity Whole-Query Optimization Using Program S.pdf:/home/ryan/Zotero/storage/9ZB8YVBP/Dong et al. - 2023 - SlabCity Whole-Query Optimization Using Program S.pdf:application/pdf},
}

@article{lia_equiv,
	series = {{SIGMOD} '23},
	title = {Proving {Query} {Equivalence} {Using} {Linear} {Integer} {Arithmetic}},
	volume = {1},
	url = {https://doi.org/10.1145/3626768},
	doi = {10.1145/3626768},
	number = {4},
	urldate = {2023-12-18},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Ding, Haoran and Wang, Zhaoguo and Yang, Yicun and Zhang, Dexin and Xu, Zhenglin and Chen, Haibo and Piskac, Ruzica and Li, Jinyang},
	month = dec,
	year = {2023},
	keywords = {LIA, LIA*, linear integer arithmetic, linear integer arithmetic with stars, SQL query equivalence, SQL solver},
	pages = {227:1--227:26},
}

@article{cloud_analytics_benchmark,
	series = {{VLDB} '23},
	title = {Cloud {Analytics} {Benchmark}},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3583140.3583156},
	doi = {10.14778/3583140.3583156},
	abstract = {The cloud facilitates the transition to a service-oriented perspective. This affects cloud-native data management in general, and data analytics in particular. Instead of managing a multi-node database cluster on-premise, end users simply send queries to a managed cloud data warehouse and receive results. While this is obviously very attractive for end users, database system architects still have to engineer systems for this new service model. There are currently many competing architectures ranging from self-hosted (Presto, PostgreSQL), over managed (Snowflake, Amazon Redshift) to queryas-a-service (Amazon Athena, Google BigQuery) offerings. Benchmarking these architectural approaches is currently difficult, and it is not even clear what the metrics for a comparison should be. To overcome these challenges, we first analyze a real-world query trace from Snowflake and compare its properties to that of TPC-H and TPC-DS. Doing so, we identify important differences that distinguish traditional benchmarks from real-world cloud data warehouse workloads. Based on this analysis, we propose the Cloud Analytics Benchmark (CAB). By incorporating workload fluctuations and multi-tenancy, CAB allows evaluating different designs in terms of user-centered metrics such as cost and performance.},
	language = {en},
	number = {6},
	urldate = {2023-12-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Van Renen, Alexander and Leis, Viktor},
	month = feb,
	year = {2023},
	pages = {1413--1425},
	file = {Van Renen and Leis - 2023 - Cloud Analytics Benchmark.pdf:/home/ryan/Zotero/storage/X9JZU4Y2/Van Renen and Leis - 2023 - Cloud Analytics Benchmark.pdf:application/pdf},
}

@inproceedings{duckdb,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {{DuckDB}: an {Embeddable} {Analytical} {Database}},
	isbn = {978-1-4503-5643-5},
	shorttitle = {{DuckDB}},
	url = {https://dl.acm.org/doi/10.1145/3299869.3320212},
	doi = {10.1145/3299869.3320212},
	abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
	urldate = {2024-01-14},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Raasveldt, Mark and M√ºhleisen, Hannes},
	month = jun,
	year = {2019},
	pages = {1981--1984},
	file = {Full Text PDF:/home/ryan/Zotero/storage/W8LRXEJY/Raasveldt and M√ºhleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf:application/pdf},
}

@misc{MyPrintCenter,
	title = {My {Print} {Center}},
	url = {https://seasuniprint.seas.upenn.edu/myprintcenter/},
	urldate = {2024-01-23},
	file = {My Print Center:/home/ryan/Zotero/storage/GQV8ZJ9M/myprintcenter.html:text/html},
}

@misc{PennWebLogin,
	title = {Penn {WebLogin}},
	url = {https://weblogin.pennkey.upenn.edu/idp/profile/SAML2/Redirect/SSO?execution=e1s1},
	urldate = {2024-01-25},
	file = {Penn WebLogin:/home/ryan/Zotero/storage/XBEGN2SN/SSO.html:text/html},
}

@article{presto_meta,
	series = {{SIGMOD} '23},
	title = {Presto: {A} {Decade} of {SQL} {Analytics} at {Meta}},
	volume = {1},
	shorttitle = {A {Decade} of {SQL} {Analytics} at {Meta}},
	url = {https://dl.acm.org/doi/10.1145/3589769},
	doi = {10.1145/3589769},
	abstract = {Presto is an open-source distributed SQL query engine that supports analytics workloads involving multiple exabyte-scale data sources. Presto is used for low-latency interactive use cases as well as long-running ETL jobs at Meta. It was originally launched at Meta in 2013 and donated to the Linux Foundation in 2019. Over the last ten years, upholding query latency and scalability with the hyper growth of data volume at Meta as well as new SQL analytics requirements have raised impressive challenges for Presto. A top priority has been ensuring query reliability does not regress with the shift towards smaller, more elastic container allocation, which requires queries to run with substantially smaller memory headroom and can be preempted at any time. Additionally, new demands from machine learning, privacy, and graph analytics have driven Presto maintainers to think beyond traditional data analytics. In this paper, we discuss several successful evolutions in recent years that have improved Presto latency as well as scalability by several orders of magnitude in production at Meta. Some of the notable ones are hierarchical caching, native vectorized execution engines, materialized views, and Presto on Spark. With these new capabilities, we have deprecated or are in the process of deprecating various legacy query engines so that Presto becomes the single piece to serve interactive, ad-hoc, ETL, and graph processing workloads for the entire data warehouse.},
	number = {2},
	urldate = {2024-01-27},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Sun, Yutian and Meehan, Tim and Schlussel, Rebecca and Xie, Wenlei and Basmanova, Masha and Erling, Orri and Rosa, Andrii and Fan, Shixuan and Zhong, Rongrong and Thirupathi, Arun and Collooru, Nikhil and Wang, Ke and Agarwal, Sameer and Gupta, Arjun and Logothetis, Dionysios and Xirogiannopoulos, Kostas and Dutta, Amit and Gajjala, Varun and Jain, Rohit and Palakuzhy, Ajay and Pandian, Prithvi and Pershin, Sergey and Saikia, Abhisek and Shankhdhar, Pranjal and Somanchi, Neerad and Tailor, Swapnil and Tan, Jialiang and Viswanadha, Sreeni and Wen, Zac and Chattopadhyay, Biswapesh and Fan, Bin and Majeti, Deepak and Pandit, Aditi},
	month = jun,
	year = {2023},
	keywords = {data analytics, data warehouse, distributed database, etl, olap, presto, sql},
	pages = {189:1--189:25},
	file = {Full Text PDF:/home/ryan/Zotero/storage/W5RXIXE8/Sun et al. - 2023 - Presto A Decade of SQL Analytics at Meta.pdf:application/pdf},
}

@article{dbtune_workload_shift,
	title = {{CGPTuner}: a contextual gaussian process bandit approach for the automatic tuning of {IT} configurations under varying workload conditions},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {{CGPTuner}},
	url = {https://dl.acm.org/doi/10.14778/3457390.3457404},
	doi = {10.14778/3457390.3457404},
	language = {en},
	number = {8},
	urldate = {2024-02-01},
	journal = {Proceedings of the VLDB Endowment},
	author = {Cereda, Stefano and Valladares, Stefano and Cremonesi, Paolo and Doni, Stefano},
	month = apr,
	year = {2021},
	pages = {1401--1413},
	file = {Cereda et al. - 2021 - CGPTuner a contextual gaussian process bandit app.pdf:/home/ryan/Zotero/storage/UNJBMX9H/Cereda et al. - 2021 - CGPTuner a contextual gaussian process bandit app.pdf:application/pdf},
}

@article{db_tune_survery,
	title = {Facilitating database tuning with hyper-parameter optimization: a comprehensive experimental evaluation},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Facilitating database tuning with hyper-parameter optimization},
	url = {https://dl.acm.org/doi/10.14778/3538598.3538604},
	doi = {10.14778/3538598.3538604},
	number = {9},
	urldate = {2024-02-01},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhang, Xinyi and Chang, Zhuo and Li, Yang and Wu, Hong and Tan, Jian and Li, Feifei and Cui, Bin},
	month = may,
	year = {2022},
	pages = {1808--1821},
	file = {Full Text PDF:/home/ryan/Zotero/storage/IS2KWW8D/Zhang et al. - 2022 - Facilitating database tuning with hyper-parameter .pdf:application/pdf},
}

@inproceedings{dbtune,
	title = {Practical design space exploration},
	url = {https://ieeexplore.ieee.org/abstract/document/8843094/},
	urldate = {2024-02-01},
	booktitle = {2019 {IEEE} 27th {International} {Symposium} on {Modeling}, {Analysis}, and {Simulation} of {Computer} and {Telecommunication} {Systems} ({MASCOTS})},
	publisher = {IEEE},
	author = {Nardi, Luigi and Koeplinger, David and Olukotun, Kunle},
	year = {2019},
	pages = {347--358},
}

@inproceedings{spark,
	address = {USA},
	series = {{HotCloud}'10},
	title = {Spark: cluster computing with working sets},
	shorttitle = {Spark},
	abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
	urldate = {2024-02-22},
	booktitle = {Proceedings of the 2nd {USENIX} conference on {Hot} topics in cloud computing},
	publisher = {USENIX Association},
	author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
	month = jun,
	year = {2010},
	pages = {10},
	file = {Zaharia et al. - 2010 - Spark cluster computing with working sets.pdf:/home/ryan/Zotero/storage/PKD9B67H/Zaharia et al. - 2010 - Spark cluster computing with working sets.pdf:application/pdf},
}

@inproceedings{sparksql,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Spark {SQL}: {Relational} {Data} {Processing} in {Spark}},
	isbn = {978-1-4503-2758-9},
	shorttitle = {Spark {SQL}},
	url = {https://dl.acm.org/doi/10.1145/2723372.2742797},
	doi = {10.1145/2723372.2742797},
	abstract = {Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.},
	urldate = {2024-02-22},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J. and Ghodsi, Ali and Zaharia, Matei},
	month = may,
	year = {2015},
	keywords = {machine learning, databases, data warehouse, hadoop, spark},
	pages = {1383--1394},
	file = {Armbrust et al. - 2015 - Spark SQL Relational Data Processing in Spark.pdf:/home/ryan/Zotero/storage/B3D653ZA/Armbrust et al. - 2015 - Spark SQL Relational Data Processing in Spark.pdf:application/pdf},
}

@article{loger,
	title = {{LOGER}: {A} {Learned} {Optimizer} {Towards} {Generating} {Efficient} and {Robust} {Query} {Execution} {Plans}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {{LOGER}},
	url = {https://dl.acm.org/doi/10.14778/3587136.3587150},
	doi = {10.14778/3587136.3587150},
	abstract = {Query optimization based on deep reinforcement learning (DRL) has become a hot research topic recently. Despite the achieved promising progress, DRL optimizers still face great challenges of robustly producing efficient plans, due to the vast search space for both join order and operator selection and the highly varying execution latency taken as the feedback signal. In this paper, we propose LOGER, a learned optimizer towards generating efficient and robust plans, aiming at producing both efficient join orders and operators. LOGER first utilizes Graph Transformer to capture relationships between tables and predicates. Then, the search space is reorganized, in which LOGER learns to restrict specific operators instead of directly selecting one for each join, while utilizing DBMS built-in optimizer to select physical operators under the restrictions. Such a strategy exploits expert knowledge to improve the robustness of plan generation while offering sufficient plan search flexibility. Furthermore, LOGER introduces Œµ-beam search, which keeps multiple search paths that preserve promising plans while performing guided exploration. Finally, LOGER introduces a loss function with reward weighting to further enhance performance robustness by reducing the fluctuation caused by poor operators, and log transformation to compress the range of rewards. We conduct experiments on Join Order Benchmark (JOB), TPC-DS and Stack Overflow, and demonstrate that LOGER can achieve a performance better than existing learned query optimizers, with a 2.07x speedup on JOB compared with PostgreSQL.},
	number = {7},
	urldate = {2024-03-21},
	journal = {Proceedings of the VLDB Endowment},
	author = {Chen, Tianyi and Gao, Jun and Chen, Hedui and Tu, Yaofeng},
	month = mar,
	year = {2023},
	pages = {1777--1789},
	file = {Full Text PDF:/home/ryan/Zotero/storage/TWNRZRDM/Chen et al. - 2023 - LOGER A Learned Optimizer Towards Generating Effi.pdf:application/pdf},
}

@article{roq,
	title = {Roq: {Robust} {Query} {Optimization} {Based} on a {Risk}-aware {Learned} {Cost} {Model}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Roq},
	url = {https://arxiv.org/abs/2401.15210},
	doi = {10.48550/ARXIV.2401.15210},
	abstract = {Query optimizers in relational database management systems (RDBMSs) search for execution plans expected to be optimal for a given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select execution plans that are suboptimal at runtime, when these estimates and assumptions are not valid, which may result in poor query performance. Therefore, query optimizers do not sufficiently support robust query optimization. Recent years have seen a surge of interest in using machine learning (ML) to improve efficiency of data systems and reduce their maintenance overheads, with promising results obtained in the area of query optimization in particular. In this paper, inspired by these advancements, and based on several years of experience of IBM Db2 in this journey, we propose Robust Optimization of Queries, (Roq), a holistic framework that enables robust query optimization based on a risk-aware learning approach. Roq includes a novel formalization of the notion of robustness in the context of query optimization and a principled approach for its quantification and measurement based on approximate probabilistic ML. It also includes novel strategies and algorithms for query plan evaluation and selection. Roq also includes a novel learned cost model that is designed to predict query execution cost and the associated risks and performs query optimization accordingly. We demonstrate experimentally that Roq provides significant improvements to robust query optimization compared to the state-of-the-art.},
	urldate = {2024-03-21},
	author = {Kamali, Amin and Kantere, Verena and Zuzarte, Calisto and Corvinelli, Vincent},
	year = {2024},
	keywords = {FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Databases (cs.DB)},
}

@article{PilotScopeSteeringDatabases,
	title = {{PilotScope}: {Steering} {Databases} with {Machine} {Learning} {Drivers}},
}

@article{pilotscope,
	series = {{VLDB} '24},
	title = {{PilotScope}: {Steering} {Databases} with {Machine} {Learning} {Drivers}},
	volume = {17},
	doi = {10.14778/3641204.3641209},
	abstract = {Learned databases, or AI4DB techniques, have rapidly developed in the last decade. Deploying machine learning (ML) and AI4DB algorithms into actual databases is the gold standard to examine their performance in practice. However, due to the complexity of database systems, the difference between ML and DB programming paradigms, and the diversity of ML models, the tasks of developing and deploying AI4DB algorithms into databases are prohibitively difficult. Most previous works focus on specific AI4DB algorithms and ML models whose deployment requires close cooperation between ML and DB developers and heavy engineering cost. In this paper, we design and implement PilotScope, an AI4DB middleware with a programming model that largely reduces such difficulties. With a novel abstraction of AI4DB algorithms for, e.g., knob tuning and query optimization, PilotScope consists of two classes of components, AI4DB drivers and DB interactors, with different programming paradigms and roles in AI4DB tasks. ML developers focus on designing and implementing AI4DB drivers, which are algorithmic workflows that collect statistics from databases, train ML models, make decisions and optimize databases using learned models. AI4DB drivers interact with databases via DB interactors (e.g., for collecting data and enforcing actions in databases). DB developers focus on implementing these interactors on one or more database engines, with the interaction details hindered from ML developers. PilotScope supports a variety of AI4DB tasks, and the implementation of an AI4DB algorithm on PilotScope can be deployed in different databases with only minimum modifications. PilotScope is effective in benchmarking these AI4DB algorithms in real-world scenarios. We hope that PilotScope could significantly accelerate iterating AI4DB research and make AI4DB techniques truly applicable in production.},
	language = {en},
	number = {5},
	journal = {PVLDB},
	author = {Zhu, Rong and Weng, Lianggui and Wei, Wenqing and Wu, Di and Peng, Jiazhen and Wang, Yifan and Ding, Bolin and Lian, Defu and Zheng, Bolong and Zhou, Jingren},
	year = {2024},
	pages = {980--993},
	file = {Zhu et al. - PilotScope Steering Databases with Machine Learni.pdf:/home/ryan/Zotero/storage/CHY4IPUK/Zhu et al. - PilotScope Steering Databases with Machine Learni.pdf:application/pdf},
}

@article{eraser_lqo,
	series = {{VLDB} '24},
	title = {Eraser: {Eliminating} {Performance} {Regression}  on {Learned} {Query} {Optimizer}},
	volume = {17},
	doi = {10.14778/3641204.3641205},
	abstract = {Efficient query optimization is crucial for database management systems. Recently, machine learning models have been applied in query optimizers to generate better plans, but the unpredictable performance regressions prevent them from being truly applicable. To be more specific, while a learned query optimizer commonly outperforms the traditional query optimizer on average for a workload of queries, its performance regression seems inevitable for some queries due to model under-fitting and difficulty in generalization. In this paper, we propose a system called Eraser to resolve this problem. Eraser aims at eliminating performance regressions while still attaining considerable overall performance improvement. To this end, Eraser applies a two-stage strategy to estimate the model accuracy for each candidate plan, and helps the learned query optimizer select more reliable plans. The first stage serves as a coarse-grained filter that removes all highly risky plans with feature values that are seen for the first time. The second stage clusters plans in a more fine-grained manner and evaluates each cluster according to the prediction quality of learned query optimizers for selecting the final execution plan. Eraser can be deployed as a plugin on top of any learned query optimizer. We implement Eraser and demonstrate its superiority on PostgreSQL and Spark. In our experiments, Eraser eliminates most of the regressions while bringing very little negative impact on the overall performance of learned query optimizers, no matter whether they perform better or worse than the traditional query optimizer. Meanwhile, it is adaptive to dynamic settings and generally applicable to different database systems.},
	language = {en},
	number = {5},
	journal = {PVLDB},
	author = {Weng, Lianggui and Zhu, Rong and Wu, Di and Ding, Bolin and Zheng, Bolong and Zhou, Jingren},
	year = {2024},
	pages = {926--938},
	file = {Weng et al. - Eraser Eliminating Performance Regression  on Lea.pdf:/home/ryan/Zotero/storage/LH8JTZQG/Weng et al. - Eraser Eliminating Performance Regression  on Lea.pdf:application/pdf},
}

@inproceedings{resource_predict,
	address = {New York, NY, USA},
	series = {{SOSP} '17},
	title = {Resource {Central}: {Understanding} and {Predicting} {Workloads} for {Improved} {Resource} {Management} in {Large} {Cloud} {Platforms}},
	isbn = {978-1-4503-5085-3},
	shorttitle = {Resource {Central}},
	url = {https://dl.acm.org/doi/10.1145/3132747.3132772},
	doi = {10.1145/3132747.3132772},
	abstract = {Cloud research to date has lacked data on the characteristics of the production virtual machine (VM) workloads of large cloud providers. A thorough understanding of these characteristics can inform the providers' resource management systems, e.g. VM scheduler, power manager, server health manager. In this paper, we first introduce an extensive characterization of Microsoft Azure's VM workload, including distributions of the VMs' lifetime, deployment size, and resource consumption. We then show that certain VM behaviors are fairly consistent over multiple lifetimes, i.e. history is an accurate predictor of future behavior. Based on this observation, we next introduce Resource Central (RC), a system that collects VM telemetry, learns these behaviors offline, and provides predictions online to various resource managers via a general client-side library. As an example of RC's online use, we modify Azure's VM scheduler to leverage predictions in oversubscribing servers (with oversubscribable VM types), while retaining high VM performance. Using real VM traces, we then show that the prediction-informed schedules increase utilization and prevent physical resource exhaustion. We conclude that providers can exploit their workloads' characteristics and machine learning to improve resource management substantially.},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the 26th {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {Association for Computing Machinery},
	author = {Cortez, Eli and Bonde, Anand and Muzio, Alexandre and Russinovich, Mark and Fontoura, Marcus and Bianchini, Ricardo},
	month = oct,
	year = {2017},
	keywords = {machine learning, Cloud workloads, predictive management},
	pages = {153--167},
	file = {Full Text PDF:/home/ryan/Zotero/storage/CV5XRMCM/Cortez et al. - 2017 - Resource Central Understanding and Predicting Wor.pdf:application/pdf},
}

@article{srivastavaDropoutSimpleWay,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {OverÔ¨Åtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overÔ¨Åtting is a serious problem in such networks. Large networks are also slow to use, making it diÔ¨Écult to deal with overÔ¨Åtting by combining the predictions of many diÔ¨Äerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diÔ¨Äerent ‚Äúthinned‚Äù networks. At test time, it is easy to approximate the eÔ¨Äect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiÔ¨Åcantly reduces overÔ¨Åtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiÔ¨Åcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, GeoÔ¨Ärey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/home/ryan/Zotero/storage/HTVV594F/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}

@article{dropout,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {Dropout},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	month = jan,
	year = {2014},
	keywords = {neural networks, deep learning, model combination, regularization},
	pages = {1929--1958},
	file = {Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf:/home/ryan/Zotero/storage/SUV2HEW7/Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf:application/pdf},
}

@article{wuFullStackAdaptivity2024,
	series = {{VLDB} '24},
	title = {Towards {Full} {Stack} {Adaptivity} in {Permissioned} {Blockchains}},
	volume = {17},
	copyright = {All rights reserved},
	doi = {10.14778/3641204.3641216},
	abstract = {This paper articulates our vision for a learning-based untrustworthy
distributed database. We focus on permissioned blockchain systems
as an emerging instance of untrustworthy distributed databases
and argue that as novel smart contracts, modern hardware, and new
cloud platforms arise, future-proof permissioned blockchain sys-
tems need to be designed with full-stack adaptivity in mind. At the
application level, a future-proof system must adaptively learn the
best-performing transaction processing paradigm and quickly adapt
to new hardware and unanticipated workload changes on the fly.
Likewise, the Byzantine consensus layer must dynamically adjust it-
self to the workloads, faulty conditions, and network configuration
while maintaining compatibility with the transaction processing
paradigm. At the infrastructure level, cloud providers must enable
cross-layer adaptation, which identifies performance bottlenecks
and possible attacks, and determines at runtime the degree of re-
source disaggregation that best meets application requirements.
Within this vision of the future, our paper outlines several research
challenges together with some preliminary approaches.},
	number = {5},
	journal = {PVLDB},
	author = {Wu, Chenyuan and Amiri, Mohammad Javad and Qin, Haoyun and Mehta, Bhavana and Marcus, Ryan and Loo, Boon Thau},
	year = {2024},
	pages = {1073--1080},
	file = {Wu et al. - 2024 - Towards Full Stack Adaptivity in Permissioned Bloc.pdf:/home/ryan/Zotero/storage/K2P5MAC9/Wu et al. - 2024 - Towards Full Stack Adaptivity in Permissioned Bloc.pdf:application/pdf},
}

@inproceedings{limeqo,
	address = {Santiago, Chile},
	series = {{aiDM} @ {SIGMOD} '24},
	title = {Low {Rank} {Approximation} for {Learned} {Query} {Optimization}},
	copyright = {All rights reserved},
	doi = {https://doi.org/10.1145/3663742.3663974},
	abstract = {We present LimeQO, a learned steering query optimizer based on
linear me thods, such as matrix completion, for repetitive workloads.
LimeQO can forgo expensive neural networks by taking advantage
of the low-rank structure of query workloads. Using offline exe-
cution, LimeQO can accelerate workloads by up to 2x with zero
regressions in just a few hours, while using 100-1000x fewer com-
putational resources than deep learning techniques.},
	booktitle = {International {Workshop} on {Exploiting} {Artificial} {Intelligence} {Techniques} for {Data} {Management}},
	publisher = {ACM},
	author = {Yi, Zixuan and Tian, Yao and Ives, Zachary G. and Marcus, Ryan},
	month = jun,
	year = {2024},
	file = {Yi et al. - 2024 - Low Rank Approximation for Learned Query Optimizat.pdf:/home/ryan/Zotero/storage/Z7HNQF9P/Yi et al. - 2024 - Low Rank Approximation for Learned Query Optimizat.pdf:application/pdf},
}

@inproceedings{cloud_coplace,
	series = {{USENIX} {Security} '15},
	title = {A {Placement} {Vulnerability} {Study} in \{{Multi}-{Tenant}\} {Public} {Clouds}},
	isbn = {978-1-939133-11-3},
	url = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/varadarajan},
	language = {en},
	urldate = {2024-05-05},
	author = {Varadarajan, Venkatanathan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
	year = {2015},
	pages = {913--928},
	file = {Full Text PDF:/home/ryan/Zotero/storage/TV23MZZ8/Varadarajan et al. - 2015 - A Placement Vulnerability Study in Multi-Tenant .pdf:application/pdf},
}

@inproceedings{liAlgorithmicImprovementsFast2014,
	address = {Amsterdam The Netherlands},
	title = {Algorithmic improvements for fast concurrent {Cuckoo} hashing},
	isbn = {978-1-4503-2704-6},
	url = {https://dl.acm.org/doi/10.1145/2592798.2592820},
	doi = {10.1145/2592798.2592820},
	abstract = {Fast concurrent hash tables are an increasingly important building block as we scale systems to greater numbers of cores and threads. This paper presents the design, implementation, and evaluation of a high-throughput and memory-efÔ¨Åcient concurrent hash table that supports multiple readers and writers. The design arises from careful attention to systems-level optimizations such as minimizing critical section length and reducing interprocessor coherence trafÔ¨Åc through algorithm re-engineering. As part of the architectural basis for this engineering, we include a discussion of our experience and results adopting Intel‚Äôs recent hardware transactional memory (HTM) support to this critical building block. We Ô¨Ånd that naively allowing concurrent access using a coarse-grained lock on existing data structures reduces overall performance with more threads. While HTM mitigates this slowdown somewhat, it does not eliminate it. Algorithmic optimizations that beneÔ¨Åt both HTM and designs for Ô¨Åne-grained locking are needed to achieve high performance.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of the {Ninth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Li, Xiaozhou and Andersen, David G. and Kaminsky, Michael and Freedman, Michael J.},
	month = apr,
	year = {2014},
	pages = {1--14},
	file = {Li et al. - 2014 - Algorithmic improvements for fast concurrent Cucko.pdf:/home/ryan/Zotero/storage/4UZP6776/Li et al. - 2014 - Algorithmic improvements for fast concurrent Cucko.pdf:application/pdf},
}

@article{kipfCuckooIndexLightweight2020,
	title = {Cuckoo index: a lightweight secondary index structure},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {Cuckoo index},
	url = {https://dl.acm.org/doi/10.14778/3424573.3424577},
	doi = {10.14778/3424573.3424577},
	abstract = {In modern data warehousing, data skipping is essential for high query performance. While index structures such as Btrees or hash tables allow for precise pruning, their large storage requirements make them impractical for indexing secondary columns. Therefore, many systems rely on approximate indexes such as min/max sketches (ZoneMaps) or Bloom Ô¨Ålters for cost-e‚Üµective data pruning. For example, Google PowerDrill skips more than 90\% of data on average using such indexes.},
	language = {en},
	number = {13},
	urldate = {2024-05-23},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kipf, Andreas and Chromejko, Damian and Hall, Alexander and Boncz, Peter and Andersen, David G.},
	month = sep,
	year = {2020},
	pages = {3559--3572},
	file = {Kipf et al. - 2020 - Cuckoo index a lightweight secondary index struct.pdf:/home/ryan/Zotero/storage/295XFQZZ/Kipf et al. - 2020 - Cuckoo index a lightweight secondary index struct.pdf:application/pdf},
}

@article{tianLearnedCuckooFilter2023,
	title = {A {Learned} {Cuckoo} {Filter} for {Approximate} {Membership} {Queries} over {Variable}-sized {Sliding} {Windows} on {Data} {Streams}},
	volume = {1},
	url = {https://doi.org/10.1145/3626758},
	doi = {10.1145/3626758},
	abstract = {Designing a space-efficient data structure to answer membership queries while ensuring high accuracy and real-time response is a challenging task in the field of stream processing. Many techniques have been developed to answer these queries in a sliding windows manner. However, assuming the user will conduct the query with the presupposed window size is not always practical. In this paper, we introduce a novel data structure called Learned Cuckoo Filter (LCF). It can provide satisfactory results for the approximate membership query on data streams, regardless of the user-defined query windows. LCF operates by adaptively maintaining cuckoo filters with the assistance of a well-trained oracle that learned the frequency feature of the data within the stream. To further enhance memory utilization, we develop a compact version of LCF (denoted by LCF\_C), which selectively removes redundant information to reduce space consumption without compromising query accuracy. Furthermore, we conduct a thorough theoretical analysis of query accuracy and provide detailed guidelines for optimal parameter selection (denoted by LCF\_O). Extensive experimental studies on synthetic and real-world datasets demonstrate the superiority of the proposed methods in terms of both space consumption and accuracy. Compared to the state-of-the-art algorithms, LCF\_O can reduce up to 61\% of space cost at the same error level, and achieve up to 12√ó improved accuracy with the same space cost.},
	number = {4},
	urldate = {2024-05-23},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Tian, Yao and Yan, Tingyun and Zhang, Ruiyuan and Huang, Kai and Zheng, Bolong and Zhou, Xiaofang},
	month = dec,
	year = {2023},
	keywords = {machine learning, approximate membership query, cuckoo filter, data stream, variable-sized sliding windows},
	pages = {264:1--264:26},
	file = {Tian et al. - 2023 - A Learned Cuckoo Filter for Approximate Membership.pdf:/home/ryan/Zotero/storage/25EU4CTU/Tian et al. - 2023 - A Learned Cuckoo Filter for Approximate Membership.pdf:application/pdf},
}

@inproceedings{morsel,
	address = {Snowbird Utah USA},
	title = {Morsel-driven parallelism: a {NUMA}-aware query evaluation framework for the many-core age},
	isbn = {978-1-4503-2376-5},
	shorttitle = {Morsel-driven parallelism},
	url = {https://dl.acm.org/doi/10.1145/2588555.2610507},
	doi = {10.1145/2588555.2610507},
	abstract = {With modern computer architecture evolving, two problems conspire against the state-of-the-art approaches in parallel query execution: (i) to take advantage of many-cores, all query work must be distributed evenly among (soon) hundreds of threads in order to achieve good speedup, yet (ii) dividing the work evenly is difÔ¨Åcult even with accurate data statistics due to the complexity of modern out-of-order cores. As a result, the existing approaches for ‚Äúplandriven‚Äù parallelism run into load balancing and context-switching bottlenecks, and therefore no longer scale. A third problem faced by many-core architectures is the decentralization of memory controllers, which leads to Non-Uniform Memory Access (NUMA).},
	language = {en},
	urldate = {2024-06-07},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Leis, Viktor and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
	month = jun,
	year = {2014},
	pages = {743--754},
	file = {Leis et al. - 2014 - Morsel-driven parallelism a NUMA-aware query eval.pdf:/home/ryan/Zotero/storage/A9QI2G7U/Leis et al. - 2014 - Morsel-driven parallelism a NUMA-aware query eval.pdf:application/pdf},
}

@inproceedings{multi_agent_rl,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {Independent} vs. {Cooperative} {Agents}},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-1-55860-307-3},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603073500496},
	doi = {10.1016/B978-1-55860-307-3.50049-6},
	abstract = {Semantic Scholar extracted view of "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents" by Ming Tan},
	language = {en},
	urldate = {2024-06-10},
	publisher = {Elsevier},
	author = {Tan, Ming},
	year = {1993},
	doi = {10.1016/B978-1-55860-307-3.50049-6},
	note = {Book Title: Machine Learning Proceedings 1993},
	pages = {330--337},
	file = {Tan - 1993 - Multi-Agent Reinforcement Learning Independent vs.pdf:/home/ryan/Zotero/storage/X7UYG7V8/Tan - 1993 - Multi-Agent Reinforcement Learning Independent vs.pdf:application/pdf},
}

@article{weiROMERobustQuery2024,
	series = {{SIGMOD} '24},
	title = {{ROME}: {Robust} {Query} {Optimization} via {Parallel} {Multi}-{Plan} {Execution}},
	volume = {2},
	shorttitle = {{ROME}},
	url = {https://dl.acm.org/doi/10.1145/3654973},
	doi = {10.1145/3654973},
	abstract = {We present a non-intrusive approach to robust query processing that can be used on top of any SQL execution engine. To reduce the risk of selecting highly sub-optimal query plans, we execute multiple plans in parallel. Query processing finishes once the first of these plans finishes execution. Plans are selected to be complementary in terms of the intermediate results they generate. This increases robustness to cardinality estimation errors, making cost prediction hard, that concern a subset of candidate results. We present multiple cost-based approaches to selecting plans for robust execution. The first approach uses a simple cost model, based on diversity of intermediate results. The second approach features a probabilistic model, approximating expected execution overheads, given uncertainty on true intermediate result sizes. We present greedy and exhaustive algorithms to select optimal plans according to those cost models. The experiments demonstrate that executing multiple plans in parallel is preferable over executing single plans that are occasionally sub-optimal, as well as over several baselines.},
	number = {3},
	urldate = {2024-06-10},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Wei, Ziyun and Trummer, Immanuel},
	month = may,
	year = {2024},
	keywords = {query optimization, integer linear programming, multiplan execution, robust performance},
	pages = {170:1--170:25},
	file = {Wei and Trummer - 2024 - ROME Robust Query Optimization via Parallel Multi.pdf:/home/ryan/Zotero/storage/FBPVFB3Z/Wei and Trummer - 2024 - ROME Robust Query Optimization via Parallel Multi.pdf:application/pdf},
}

@article{qinBFTGymInteractivePlayground2024,
	series = {{VLDB} '24},
	title = {{BFTGym}: {An} {Interactive} {Playground} for {BFT} {Protocols}},
	volume = {17},
	copyright = {All rights reserved},
	doi = {10.14778/3685800.3685850},
	abstract = {Byzantine Fault Tolerant (BFT) protocols serve as a fundamental yet intricate component of distributed data management systems in untrustworthy environments. BFT protocols exhibit different design principles and performance characteristics under varying workloads and fault scenarios. The proliferation of BFT protocols and their growing complexity have made it increasingly challenging to analyze the performance and possible application scenarios of each protocol. This demonstration showcases BFTGym, an interactive platform that allows audience members to (1) evaluate, compare, and gather insights into the performance of various BFT protocols under a wide range of conditions, and (2) prototype new BFT protocols rapidly.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Qin, Haoyun and Wu, Chenyuan and Amiri, Mohammad Javad and Marcus, Ryan and Loo, Boon Thau},
	year = {2024},
	note = {props: demo},
	file = {Qin et al. - 2024 - BFTGym An Interactive Playground for BFT Protocol.pdf:/home/ryan/Zotero/storage/ASASTM2F/Qin et al. - 2024 - BFTGym An Interactive Playground for BFT Protocol.pdf:application/pdf},
}

@inproceedings{filter_rep,
	address = {New York, NY, USA},
	series = {{DAMON} '21},
	title = {Filter {Representation} in {Vectorized} {Query} {Execution}},
	isbn = {978-1-4503-8556-5},
	url = {https://doi.org/10.1145/3465998.3466009},
	doi = {10.1145/3465998.3466009},
	abstract = {Advances in memory technology have made it feasible for database management systems (DBMS) to store their working data set in main memory. This trend shifts the bottleneck for query execution from disk accesses to CPU efficiency. One technique to improve CPU efficiency is batch-oriented processing, or vectorization, as it reduces interpretation overhead. For each vector (batch) of tuples, the DBMS must track the set of valid (visible) tuples that survive all previous processing steps. To that end, existing systems employ one of two data structures, or filter representations: selection vectors or bitmaps. In this work, we analyze each approach's strengths and weaknesses and offer recommendations on how to implement vectorized operations. Through a wide range of micro-benchmarks, we determine that the optimal strategy is a function of many factors: the cost of iterating through tuples, the cost of the operation itself, and how amenable it is to SIMD vectorization. Our analysis shows that bitmaps perform better for operations that can be vectorized using SIMD instructions and that selection vectors perform better on all other operations due to cheaper iteration logic.},
	urldate = {2024-07-05},
	booktitle = {Proceedings of the 17th {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {Association for Computing Machinery},
	author = {Ngom, Amadou and Menon, Prashanth and Butrovich, Matthew and Ma, Lin and Lim, Wan Shen and Mowry, Todd C. and Pavlo, Andrew},
	month = jun,
	year = {2021},
	pages = {1--7},
	file = {Ngom et al. - 2021 - Filter Representation in Vectorized Query Executio.pdf:/home/ryan/Zotero/storage/TGK278DD/Ngom et al. - 2021 - Filter Representation in Vectorized Query Executio.pdf:application/pdf},
}

@article{redshift_workload,
	series = {{VLDB} '24},
	title = {Why {TPC} is not enough: {An} analysis of the {Amazon} {Redshift} fleet},
	url = {https://www.amazon.science/publications/why-tpc-is-not-enough-an-analysis-of-the-amazon-redshift-fleet},
	journal = {Proceedings of the VLDB Endowment},
	author = {van Renen, Alexander and Horn, Dominik and Pfeil, Pascal and Vaidya, Kapil Eknath and Dong, Wenjian and Narayanaswamy, Murali and Liu, Zhengchun and Saxena, Gaurav and Kipf, Andreas and Kraska, Tim},
	year = {2024},
	file = {van Renen et al. - 2024 - Why TPC is not enough An analysis of the Amazon R.pdf:/home/ryan/Zotero/storage/4G8A9PGA/van Renen et al. - 2024 - Why TPC is not enough An analysis of the Amazon R.pdf:application/pdf},
}

@inproceedings{redshift_pred_cache,
	address = {New York, NY, USA},
	series = {{SIGMOD} '24},
	title = {Predicate {Caching}: {Query}-{Driven} {Secondary} {Indexing} for {Cloud} {Data} {Warehouses}},
	isbn = {979-8-4007-0422-2},
	shorttitle = {Predicate {Caching}},
	url = {https://dl.acm.org/doi/10.1145/3626246.3653395},
	doi = {10.1145/3626246.3653395},
	abstract = {Cloud data warehouses are today's standard for analytical query processing. Multiple cloud vendors offer state-of-the-art systems, such as Amazon Redshift. We have observed that customer workloads experience highly repetitive query patterns, i.e., users and systems frequently send the same queries. In order to improve query performance on these queries, most systems rely on techniques like result caches or materialized views.However, these caches are often stale due to inserts, deletes, or updates that occur between query repetitions. We propose a novel secondary index, predicate caching, to improve query latency for repeating scans and joins. Predicate caching stores ranges of qualifying tuples of base table scans. Such an index can be built on the fly, is lightweight, and can be kept online without recomputation.We implemented a prototype of this idea in the cloud data warehouse Amazon Redshift. Our evaluation shows that predicate caching improves query runtimes by up to 10x on selected queries with negligible build overhead.},
	urldate = {2024-07-30},
	booktitle = {Companion of the 2024 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Schmidt, Tobias and Kipf, Andreas and Horn, Dominik and Saxena, Gaurav and Kraska, Tim},
	month = jun,
	year = {2024},
	pages = {347--359},
	file = {Full Text PDF:/home/ryan/Zotero/storage/JZZRA4R2/Schmidt et al. - 2024 - Predicate Caching Query-Driven Secondary Indexing.pdf:application/pdf},
}

@article{bonczMonetDBX100HyperPipelining2005,
	series = {{CIDR} '05},
	title = {{MonetDB}/{X100}: {Hyper}-{Pipelining} {Query} {Execution}},
	url = {https://www.cidrdb.org/cidr2005/papers/P19.pdf},
	abstract = {Database systems tend to achieve only low IPC (instructions-per-cycle) eÔ¨Éciency on modern CPUs in compute-intensive application areas like decision support, OLAP and multimedia retrieval. This paper starts with an in-depth investigation to the reason why this happens, focusing on the TPC-H benchmark. Our analysis of various relational systems and MonetDB leads us to a new set of guidelines for designing a query processor.},
	language = {en},
	author = {Boncz, Peter and Zukowski, Marcin and Nes, Niels},
	year = {2005},
	file = {Boncz et al. - MonetDBX100 Hyper-Pipelining Query Execution.pdf:/home/ryan/Zotero/storage/S2YHC7KU/Boncz et al. - MonetDBX100 Hyper-Pipelining Query Execution.pdf:application/pdf},
}

@article{wuTrulyAdaptiveByzantine2024,
	series = {{SIGOPS} '24},
	title = {Towards {Truly} {Adaptive} {Byzantine} {Fault}-{Tolerant} {Consensus}},
	volume = {58},
	copyright = {All rights reserved},
	issn = {0163-5980},
	doi = {10.1145/3689051.3689055},
	abstract = {To acheive maximum performance, Byzantine fault-tolerant (BFT) systems must be manually tuned when hardware, network, or workload properties change. This paper presents our vision for a reinforcement learning (RL) based Byzantine fault-tolerant (BFT) system that adjusts effectively in realtime to changing fault scenarios and workloads. We identify several variables that can impact the performance of a BFT protocol, and show how these variables can serve as features in an RL engine in order to choose the context-dependent bestperforming BFT protocol in real-time. We further outline a decentralized RL approach capable of tolerating adversarial data pollution, where nodes share local metering values and reach the same learning output by consensus.},
	number = {1},
	urldate = {2024-08-16},
	journal = {SIGOPS Oper. Syst. Rev.},
	author = {Wu, Chenyuan and Qin, Haoyun and Javad Amiri, Mohammad and Thau Loo, Boon and Malkhi, Dahlia and Marcus, Ryan},
	month = aug,
	year = {2024},
	pages = {15--22},
	file = {Wu et al. - 2024 - Towards Truly Adaptive Byzantine Fault-Tolerant Co.pdf:/home/ryan/Zotero/storage/B3CRWFN5/Wu et al. - 2024 - Towards Truly Adaptive Byzantine Fault-Tolerant Co.pdf:application/pdf},
}

@misc{wangHARMONICHarnessingLLMs2024,
	title = {{HARMONIC}: {Harnessing} {LLMs} for {Tabular} {Data} {Synthesis} and {Privacy} {Protection}},
	shorttitle = {{HARMONIC}},
	url = {https://arxiv.org/abs/2408.02927v1},
	abstract = {Data serves as the fundamental foundation for advancing deep learning, particularly tabular data presented in a structured format, which is highly conducive to modeling. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Hence, exploring how to effectively use models like LLMs to generate realistic and privacy-preserving synthetic tabular data is urgent. In this paper, we take a step forward to explore LLMs for tabular data synthesis and privacy protection, by introducing a new framework HARMONIC for tabular data generation and evaluation. In the tabular data generation of our framework, unlike previous small-scale LLM-based methods that rely on continued pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular data and enhance privacy. Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to discover inter-row relationships. Then, with fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. In the evaluation part of our framework, we develop specific privacy risk metrics DLT for LLM synthetic data generation, as well as performance evaluation metrics LLE for downstream LLM tasks. Our experiments find that this tabular data generation framework achieves equivalent performance to existing methods with better privacy, which also demonstrates our evaluation framework for the effectiveness of synthetic data and privacy risks in LLM scenarios.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Wang, Yuxin and Feng, Duanyu and Dai, Yongfu and Chen, Zhengyu and Huang, Jimin and Ananiadou, Sophia and Xie, Qianqian and Wang, Hao},
	month = aug,
	year = {2024},
	file = {Full Text PDF:/home/ryan/Zotero/storage/TQI4KA3U/Wang et al. - 2024 - HARMONIC Harnessing LLMs for Tabular Data Synthes.pdf:application/pdf},
}

@article{llm_disrupt_db,
	series = {{VLDB} '23},
	title = {How {Large} {Language} {Models} {Will} {Disrupt} {Data} {Management}},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3611479.3611527},
	doi = {10.14778/3611479.3611527},
	abstract = {Large language models (LLMs), such as GPT-4, are revolutionizing software's ability to understand, process, and synthesize language. The authors of this paper believe that this advance in technology is significant enough to prompt introspection in the data management community, similar to previous technological disruptions such as the advents of the world wide web, cloud computing, and statistical machine learning. We argue that the disruptive influence that LLMs will have on data management will come from two angles. (1) A number of hard database problems, namely, entity resolution, schema matching, data discovery, and query synthesis, hit a ceiling of automation because the system does not fully understand the semantics of the underlying data. Based on large training corpora of natural language, structured data, and code, LLMs have an unprecedented ability to ground database tuples, schemas, and queries in real-world concepts. We will provide examples of how LLMs may completely change our approaches to these problems. (2) LLMs blur the line between predictive models and information retrieval systems with their ability to answer questions. We will present examples showing how large databases and information retrieval systems have complementary functionality.},
	number = {11},
	urldate = {2024-09-05},
	journal = {Proc. VLDB Endow.},
	author = {Fernandez, Raul Castro and Elmore, Aaron J. and Franklin, Michael J. and Krishnan, Sanjay and Tan, Chenhao},
	month = jul,
	year = {2023},
	pages = {3302--3309},
	file = {Full Text PDF:/home/ryan/Zotero/storage/N3NIF3LW/Fernandez et al. - 2023 - How Large Language Models Will Disrupt Data Manage.pdf:application/pdf},
}

@misc{xuAreLLMsNaturally2024,
	title = {Are {LLMs} {Naturally} {Good} at {Synthetic} {Tabular} {Data} {Generation}?},
	url = {https://arxiv.org/abs/2406.14541v2},
	abstract = {Large language models (LLMs) have demonstrated their prowess in generating synthetic text and images; however, their potential for generating tabular data -- arguably the most common data type in business and scientific applications -- is largely underexplored. This paper demonstrates that LLMs, used as-is, or after traditional fine-tuning, are severely inadequate as synthetic table generators. Due to the autoregressive nature of LLMs, fine-tuning with random order permutation runs counter to the importance of modeling functional dependencies, and renders LLMs unable to model conditional mixtures of distributions (key to capturing real world constraints). We showcase how LLMs can be made to overcome some of these deficiencies by making them permutation-aware.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Xu, Shengzhe and Lee, Cho-Ting and Sharma, Mandar and Yousuf, Raquib Bin and Muralidhar, Nikhil and Ramakrishnan, Naren},
	month = jun,
	year = {2024},
	file = {Full Text PDF:/home/ryan/Zotero/storage/MAB65XUC/Xu et al. - 2024 - Are LLMs Naturally Good at Synthetic Tabular Data .pdf:application/pdf},
}

@article{llm_data_engineering,
	series = {{TADA}@{VLDB} '24},
	title = {{LLMs} for {Data} {Engineering} on {Enterprise} {Data}},
	volume = {1},
	abstract = {A recent line of work applies Large Language Models (LLMs) to data engineering tasks on tabular data, suggesting they can solve a broad spectrum of tasks with high accuracy. However, existing research primarily uses datasets based on tables from web sources such as Wikipedia, calling the applicability of LLMs for real-world enterprise data into question. In this paper, we perform a first analysis of LLMs for solving data engineering tasks on a real-world enterprise dataset. As an exemplary task, we apply recent LLMs to the task of column type annotation to study how the data characteristics affect the LLMs‚Äô accuracy and find that LLMs have severe limitations when dealing with enterprise data. Based on these findings, we point towards promising directions for adapting LLMs to the enterprise context.},
	language = {en},
	number = {1},
	journal = {VLDB 2024 Workshop: Tabular Data Analysis Workshop},
	author = {Bodensohn, Jan-Micha and Brackmann, Ulf and Vogel, Liane and Urban, Matthias and Sanghi, Anupam and Binnig, Carsten},
	month = aug,
	year = {2024},
	file = {Bodensohn et al. - LLMs for Data Engineering on Enterprise Data.pdf:/home/ryan/Zotero/storage/R53ZTDQL/Bodensohn et al. - LLMs for Data Engineering on Enterprise Data.pdf:application/pdf},
}

@misc{wangHARMONICHarnessingLLMs2024a,
	title = {{HARMONIC}: {Harnessing} {LLMs} for {Tabular} {Data} {Synthesis} and {Privacy} {Protection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{HARMONIC}},
	url = {https://arxiv.org/abs/2408.02927},
	doi = {10.48550/ARXIV.2408.02927},
	abstract = {Data serves as the fundamental foundation for advancing deep learning, particularly tabular data presented in a structured format, which is highly conducive to modeling. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Hence, exploring how to effectively use models like LLMs to generate realistic and privacy-preserving synthetic tabular data is urgent. In this paper, we take a step forward to explore LLMs for tabular data synthesis and privacy protection, by introducing a new framework HARMONIC for tabular data generation and evaluation. In the tabular data generation of our framework, unlike previous small-scale LLM-based methods that rely on continued pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular data and enhance privacy. Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to discover inter-row relationships. Then, with fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. In the evaluation part of our framework, we develop specific privacy risk metrics DLT for LLM synthetic data generation, as well as performance evaluation metrics LLE for downstream LLM tasks. Our experiments find that this tabular data generation framework achieves equivalent performance to existing methods with better privacy, which also demonstrates our evaluation framework for the effectiveness of synthetic data and privacy risks in LLM scenarios.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Wang, Yuxin and Feng, Duanyu and Dai, Yongfu and Chen, Zhengyu and Huang, Jimin and Ananiadou, Sophia and Xie, Qianqian and Wang, Hao},
	year = {2024},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Cryptography and Security (cs.CR)},
	file = {Wang et al. - 2024 - HARMONIC Harnessing LLMs for Tabular Data Synthes.pdf:/home/ryan/Zotero/storage/GQZDY9S2/Wang et al. - 2024 - HARMONIC Harnessing LLMs for Tabular Data Synthes.pdf:application/pdf},
}

@misc{xuAreLLMsNaturally2024a,
	title = {Are {LLMs} {Naturally} {Good} at {Synthetic} {Tabular} {Data} {Generation}?},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2406.14541},
	doi = {10.48550/ARXIV.2406.14541},
	abstract = {Large language models (LLMs) have demonstrated their prowess in generating synthetic text and images; however, their potential for generating tabular data -- arguably the most common data type in business and scientific applications -- is largely underexplored. This paper demonstrates that LLMs, used as-is, or after traditional fine-tuning, are severely inadequate as synthetic table generators. Due to the autoregressive nature of LLMs, fine-tuning with random order permutation runs counter to the importance of modeling functional dependencies, and renders LLMs unable to model conditional mixtures of distributions (key to capturing real world constraints). We showcase how LLMs can be made to overcome some of these deficiencies by making them permutation-aware.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Xu, Shengzhe and Lee, Cho-Ting and Sharma, Mandar and Yousuf, Raquib Bin and Muralidhar, Nikhil and Ramakrishnan, Naren},
	year = {2024},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {Xu et al. - 2024 - Are LLMs Naturally Good at Synthetic Tabular Data .pdf:/home/ryan/Zotero/storage/TWXWHJSW/Xu et al. - 2024 - Are LLMs Naturally Good at Synthetic Tabular Data .pdf:application/pdf},
}

@inproceedings{photon,
	address = {Philadelphia, PA, USA},
	series = {{SIGMOD} '22},
	title = {Photon: {A} {Fast} {Query} {Engine} for {Lakehouse} {Systems}},
	isbn = {978-1-4503-9249-5},
	shorttitle = {Photon},
	url = {https://dl.acm.org/doi/10.1145/3514221.3526054},
	doi = {10.1145/3514221.3526054},
	abstract = {Many organizations are shifting to a data management paradigm called the "Lakehouse," which implements the functionality of structured data warehouses on top of unstructured data lakes. This presents new challenges for query execution engines. The engine needs to provide good performance on the raw uncurated datasets that are ubiquitous in data lakes, and excellent performance on structured data stored in popular columnar file formats like Apache Parquet. Toward these goals, we present Photon, a vectorized query engine for Lakehouse environments that we developed at Databricks. Photon can outperform existing warehouses on SQL workloads and also supports the Apache Spark API. We discuss the design choices we made in Photon (e.g., vectorization vs. code generation) and describe its integration with our existing SQL and Apache Spark runtimes, its task model, and its memory manager. Photon has accelerated some customer workloads by over 10x and has recently allowed Databricks to set a new audited performance record for the official 100TB TPC-DS benchmark.},
	language = {en},
	urldate = {2024-09-09},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Behm, Alexander and Palkar, Shoumik and Agarwal, Utkarsh and Armstrong, Timothy and Cashman, David and Dave, Ankur and Greenstein, Todd and Hovsepian, Shant and Johnson, Ryan and Sai Krishnan, Arvind and Leventis, Paul and Luszczak, Ala and Menon, Prashanth and Mokhtar, Mostafa and Pang, Gene and Paranjpye, Sameer and Rahn, Greg and Samwel, Bart and Van Bussel, Tom and Van Hovell, Herman and Xue, Maryann and Xin, Reynold and Zaharia, Matei},
	month = jun,
	year = {2022},
	pages = {2326--2339},
}

@inproceedings{lakehouse,
	address = {Virtual Event},
	series = {{CIDR} '21},
	title = {Lakehouse: {A} {New} {Generation} of {Open} {Platforms} that {Unify} {Data} {Warehousing} and {Advanced} {Analytics}},
	shorttitle = {Lakehouse},
	url = {https://www.semanticscholar.org/paper/Lakehouse%3A-A-New-Generation-of-Open-Platforms-that-Zaharia-Ghodsi/451cf5fc9786ed4f7e1d9877f08d00f8b1262121},
	abstract = {This paper argues that the data warehouse architecture as we know it today will wither in the coming years and be replaced by a new architectural pattern, the Lakehouse, which will (i) be based on open direct-access data formats, such as Apache Parquet, (ii) have first-class support for machine learning and data science, and (iii) offer state-of-the-art performance. Lakehouses can help address several major challenges with data warehouses, including data staleness, reliability, total cost of ownership, data lock-in, and limited use-case support. We discuss how the industry is already moving toward Lakehouses and how this shift may affect work in data management. We also report results from a Lakehouse system using Parquet that is competitive with popular cloud data warehouses on TPC-DS.},
	urldate = {2024-09-09},
	author = {Zaharia, M. and Ghodsi, A. and Xin, Reynold and Armbrust, Michael},
	year = {2021},
	file = {Full Text PDF:/home/ryan/Zotero/storage/FTA7FNS8/Zaharia et al. - 2021 - Lakehouse A New Generation of Open Platforms that.pdf:application/pdf},
}

@article{seiden,
	series = {{VLDB} '23},
	title = {Seiden: {Revisiting} {Query} {Processing} in {Video} {Database} {Systems}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {Seiden},
	url = {https://dl.acm.org/doi/10.14778/3598581.3598599},
	doi = {10.14778/3598581.3598599},
	abstract = {State-of-the-art video database management systems (VDBMSs) often use lightweight proxy models to accelerate object retrieval and aggregate queries. The key assumption underlying these systems is that the proxy model is an order of magnitude faster than the heavyweight oracle model. However, recent advances in computer vision have invalidated this assumption. Inference time of recently proposed oracle models is on par with or even lower than the proxy models used in state-of-the-art (SoTA) VDBMSs. This paper presents Seiden, a VDBMS that leverages this radical shift in the runtime gap between the oracle and proxy models. Instead of relying on a proxy model, Seiden directly applies the oracle model over a subset of frames to build a query-agnostic index, and samples additional frames to answer the query using an exploration-exploitation scheme during query processing. By leveraging the temporal continuity of the video and the output of the oracle model on the sampled frames, Seiden delivers faster query processing and better query accuracy than SoTA VDBMSs. Our empirical evaluation shows that Seiden is on average 6.6 x faster than SoTA VDBMSs across diverse queries and datasets.},
	number = {9},
	urldate = {2024-09-10},
	journal = {Proc. VLDB Endow.},
	author = {Bang, Jaeho and Kakkar, Gaurav Tarlok and Chunduri, Pramod and Mitra, Subrata and Arulraj, Joy},
	month = may,
	year = {2023},
	pages = {2289--2301},
	file = {Full Text PDF:/home/ryan/Zotero/storage/MTZZYDCS/Bang et al. - 2023 - Seiden Revisiting Query Processing in Video Datab.pdf:application/pdf},
}

@article{mlog,
	title = {{MLog}: towards declarative in-database machine learning},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {{MLog}},
	url = {https://dl.acm.org/doi/10.14778/3137765.3137812},
	doi = {10.14778/3137765.3137812},
	abstract = {We demonstrate MLog, a high-level language that integrates machine learning into data management systems. Unlike existing machine learning frameworks (e.g., TensorFlow, Theano, and Caffe), MLog is declarative, in the sense that the system manages all data movement, data persistency, and machine-learning related optimizations (such as data batching) automatically. Our interactive demonstration will show audience how this is achieved based on the novel notion of tensoral views (TViews), which are similar to relational views but operate over tensors with linear algebra. With MLog, users can succinctly specify not only simple models such as SVM (in just two lines), but also sophisticated deep learning models that are not supported by existing in-database analytics systems (e.g., MADlib, PAL, and SciDB), as a series of cascaded TViews. Given the declarative nature of MLog, we further demonstrate how query/program optimization techniques can be leveraged to translate MLog programs into native TensorFlow programs. The performance of the automatically generated Tensor-Flow programs is comparable to that of hand-optimized ones.},
	number = {12},
	urldate = {2024-09-10},
	journal = {Proc. VLDB Endow.},
	author = {Li, Xupeng and Cui, Bin and Chen, Yiru and Wu, Wentao and Zhang, Ce},
	month = aug,
	year = {2017},
	pages = {1933--1936},
	file = {Full Text PDF:/home/ryan/Zotero/storage/SV3JILQM/Li et al. - 2017 - MLog towards declarative in-database machine lear.pdf:application/pdf},
}

@article{aligraph,
	series = {{VLDB} '19},
	title = {{AliGraph}: a comprehensive graph neural network platform},
	volume = {12},
	issn = {2150-8097},
	shorttitle = {{AliGraph}},
	url = {https://dl.acm.org/doi/10.14778/3352063.3352127},
	doi = {10.14778/3352063.3352127},
	abstract = {An increasing number of machine learning tasks require dealing with large graph datasets, which capture rich and complex relationship among potentially billions of elements. Graph Neural Network (GNN) becomes an effective way to address the graph learning problem by converting the graph data into a low dimensional space while keeping both the structural and property information to the maximum extent and constructing a neural network for training and referencing. However, it is challenging to provide an efficient graph storage and computation capabilities to facilitate GNN training and enable development of new GNN algorithms. In this paper, we present a comprehensive graph neural network system, namely
              AliGraph
              , which consists of distributed graph storage, optimized sampling operators and runtime to efficiently support not only existing popular GNNs but also a series of in-house developed ones for different scenarios. The system is currently deployed at Alibaba to support a variety of business scenarios, including product recommendation and personalized search at Alibaba's E-Commerce platform. By conducting extensive experiments on a real-world dataset with 492.90 million vertices, 6.82 billion edges and rich attributes,
              AliGraph
              performs an order of magnitude faster in terms of graph building (5 minutes vs hours reported from the state-of-the-art PowerGraph platform). At training,
              AliGraph
              runs 40\%-50\% faster with the novel caching strategy and demonstrates around 12 times speed up with the improved runtime. In addition, our in-house developed GNN models all showcase their statistically significant superiorities in terms of both effectiveness and efficiency (e.g., 4.12\%--17.19\% lift by F1 scores).},
	language = {en},
	number = {12},
	urldate = {2024-09-10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhu, Rong and Zhao, Kun and Yang, Hongxia and Lin, Wei and Zhou, Chang and Ai, Baole and Li, Yong and Zhou, Jingren},
	month = aug,
	year = {2019},
	pages = {2094--2105},
}

@article{increm_cnn,
	series = {{TODS} '20},
	title = {Incremental and {Approximate} {Computations} for {Accelerating} {Deep} {CNN} {Inference}},
	volume = {45},
	issn = {0362-5915, 1557-4644},
	url = {https://dl.acm.org/doi/10.1145/3397461},
	doi = {10.1145/3397461},
	abstract = {Deep learning now offers state-of-the-art accuracy for many prediction tasks. A form of deep learning called deep convolutional neural networks (CNNs) are especially popular on image, video, and time series data. Due to its high computational cost, CNN inference is often a bottleneck in analytics tasks on such data. Thus, a lot of work in the computer architecture, systems, and compilers communities study how to make CNN inference faster. In this work, we show that by elevating the abstraction level and re-imagining CNN inference as
              queries
              , we can bring to bear database-style query optimization techniques to improve CNN inference efficiency. We focus on tasks that perform CNN inference
              repeatedly
              on inputs that are only
              slightly different
              . We identify two popular CNN tasks with this behavior:
              occlusion-based explanations
              (OBE) and
              object recognition in videos
              (ORV). OBE is a popular method for ‚Äúexplaining‚Äù CNN predictions. It outputs a heatmap over the input to show which regions (e.g., image pixels) mattered most for a given prediction. It leads to many re-inference requests on locally modified inputs. ORV uses CNNs to identify and track objects across video frames. It also leads to many re-inference requests. We cast such tasks in a unified manner as a novel instance of the
              incremental view maintenance
              problem and create a comprehensive algebraic framework for incremental CNN inference that reduces computational costs. We produce
              materialized views
              of features produced inside a CNN and connect them with a novel
              multi-query optimization
              scheme for CNN re-inference. Finally, we also devise novel OBE-specific and ORV-specific approximate inference optimizations exploiting their semantics. We prototype our ideas in Python to create a tool called
              Krypton
              that supports both CPUs and GPUs. Experiments with real data and CNNs show that
              Krypton
              reduces runtimes by up to 5√ó (respectively, 35√ó) to produce exact (respectively, high-quality approximate) results without raising resource requirements.},
	language = {en},
	number = {4},
	urldate = {2024-09-10},
	journal = {ACM Transactions on Database Systems},
	author = {Nakandala, Supun and Nagrecha, Kabir and Kumar, Arun and Papakonstantinou, Yannis},
	month = dec,
	year = {2020},
	pages = {1--42},
	file = {Full Text:/home/ryan/Zotero/storage/YNZHHESY/Nakandala et al. - 2020 - Incremental and Approximate Computations for Accel.pdf:application/pdf},
}

@article{talukdarLearningCreateDataintegrating2008,
	series = {{VLDB} '08},
	title = {Learning to create data-integrating queries},
	volume = {1},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1453856.1453941},
	doi = {10.14778/1453856.1453941},
	abstract = {The number of potentially-related data resources available for querying --- databases, data warehouses, virtual integrated schemas --- continues to grow rapidly. Perhaps no area has seen this problem as acutely as the life sciences, where hundreds of large, complex, interlinked data resources are available on fields like proteomics, genomics, disease studies, and pharmacology. The schemas of individual databases are often large on their own, but users also need to pose queries across multiple sources, exploiting foreign keys and schema mappings. Since the users are not experts, they typically rely on the existence of pre-defined Web forms and associated query templates, developed by programmers to meet the particular scientists' needs. Unfortunately, such forms are scarce commodities, often limited to a single database, and mismatched with biologists' information needs that are often context-sensitive and span multiple databases.
            
              We present a system with which a non-expert user can author new query templates and Web forms, to be reused by anyone with related information needs. The user poses keyword queries that are matched against source relations and their attributes; the system uses sequences of associations (e.g., foreign keys, links, schema mappings, synonyms, and taxonomies) to create multiple ranked queries linking the matches to keywords; the set of queries is attached to a Web query form. Now the user and his or her associates may pose specific queries by filling in parameters in the form. Importantly, the answers to this query are ranked and annotated with data provenance, and the user provides
              feedback
              on the utility of the answers, from which the system ultimately learns to assign costs to sources and associations according to the user's specific information need, as a result changing the ranking of the queries used to generate results. We evaluate the effectiveness of our method against "gold standard" costs from domain experts and demonstrate the method's scalability.},
	language = {en},
	number = {1},
	urldate = {2024-09-10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Talukdar, Partha Pratim and Jacob, Marie and Mehmood, Muhammad Salman and Crammer, Koby and Ives, Zachary G. and Pereira, Fernando and Guha, Sudipto},
	month = aug,
	year = {2008},
	pages = {785--796},
}

@inproceedings{ivesAdaptingSourceProperties2004,
	address = {Paris France},
	series = {{SIGMOD} '04},
	title = {Adapting to source properties in processing data integration queries},
	isbn = {978-1-58113-859-7},
	url = {https://dl.acm.org/doi/10.1145/1007568.1007613},
	doi = {10.1145/1007568.1007613},
	abstract = {An effective query optimizer finds a query plan that exploits the characteristics of the source data. In data integration, little is known in advance about sources' properties, which necessitates the use of adaptive query processing techniques to adjust query processing on-the-fly. Prior work in adaptive query processing has focused on compensating for delays and adjusting for mis-estimated cardinality or selectivity values. In this paper, we present a generalized architecture for adaptive query processing and introduce a new technique, called adaptive data partitioning (ADP), which is based on the idea of dividing the source data into regions, each executed by different, complementary plans. We show how this model can be applied in novel ways to not only correct for underestimated selectivity and cardinality values, but also to discover and exploit order in the source data, and to detect and exploit source data that can be effectively pre-aggregated. We experimentally compare a number of alternative strategies and show that our approach is effective.},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM},
	author = {Ives, Zachary G. and Halevy, Alon Y. and Weld, Daniel S.},
	month = jun,
	year = {2004},
	pages = {395--406},
	file = {Submitted Version:/home/ryan/Zotero/storage/WJW2CLT9/Ives et al. - 2004 - Adapting to source properties in processing data i.pdf:application/pdf},
}

@article{talukdarAutomaticallyIncorporatingNew2010,
	series = {{SIGMOD} '10},
	title = {Automatically incorporating new sources in keyword search-based data integration},
	url = {https://dl.acm.org/doi/10.1145/1807167.1807211},
	doi = {10.1145/1807167.1807211},
	abstract = {Scientific data offers some of the most interesting challenges in data integration today. Scientific fields evolve rapidly and accumulate masses of observational and experimental data that needs to be annotated, revised, interlinked, and made available to other scientists. From the perspective of the user, this can be a major headache as the data they seek may initially be spread across many databases in need of integration. Worse, even if users are given a solution that integrates the current state of the source databases, new data sources appear with new data items of interest to the user. Here we build upon recent ideas for creating integrated views over data sources using keyword search techniques, ranked answers, and user feedback [32] to investigate how to automatically discover when a new data source has content relevant to a user's view - in essence, performing automatic data integration for incoming data sets. The new architecture accommodates a variety of methods to discover related attributes, including label propagation algorithms from the machine learning community [2] and existing schema matchers [11]. The user may provide feedback on the suggested new results, helping the system repair any bad alignments or increase the cost of including a new source that is not useful. We evaluate our approach on actual bioinformatics schemas and data, using state-of-the-art schema matchers as components. We also discuss how our architecture can be adapted to more traditional settings with a mediated schema.},
	language = {en},
	urldate = {2024-09-10},
	journal = {Proceedings of the 2010 ACM SIGMOD International Conference on Management of data},
	author = {Talukdar, Partha Pratim and Ives, Zachary G. and Pereira, Fernando},
	month = jun,
	year = {2010},
	note = {Conference Name: SIGMOD/PODS '10: International Conference on Management of Data
ISBN: 9781450300322
Place: Indianapolis Indiana USA
Publisher: ACM},
	pages = {387--398},
}

@inproceedings{jacobSharingWorkKeyword2011,
	address = {Athens Greece},
	series = {{SIGMOD} '11},
	title = {Sharing work in keyword search over databases},
	isbn = {978-1-4503-0661-4},
	url = {https://dl.acm.org/doi/10.1145/1989323.1989384},
	doi = {10.1145/1989323.1989384},
	abstract = {An important means of allowing non-expert end-users to pose ad hoc queries whether over single databases or data integration systems is through keyword search. Given a set of keywords, the query processor finds matches across different tuples and tables. It computes and executes a set of relational sub-queries whose results are combined to produce the k highest ranking answers. Work on keyword search primarily focuses on single-database, single-query settings: each query is answered in isolation, despite possible overlap between queries posed by different users or at different times; and the number of relevant tables is assumed to be small, meaning that sub-queries can be processed without using cost-based methods to combine work. As we apply keyword search to support ad hoc data integration queries over scientific or other databases on the Web, we must reuse and combine computation. In this paper, we propose an architecture that continuously receives sets of ranked keyword queries, and seeks to reuse work across these queries. We extend multiple query optimization and continuous query techniques, and develop a new query plan scheduling module we call the ATC (based on its analogy to an air traffic controller). The ATC manages the flow of tuples among a multitude of pipelined operators, minimizing the work needed to return the top-k answers for all queries. We also develop techniques to manage the sharing and reuse of state as queries complete and input data streams are exhausted. We show the effectiveness of our techniques in handling queries over real and synthetic data sets.},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of data},
	publisher = {ACM},
	author = {Jacob, Marie and Ives, Zachary},
	month = jun,
	year = {2011},
	pages = {577--588},
	file = {Submitted Version:/home/ryan/Zotero/storage/QWT97GV7/Jacob and Ives - 2011 - Sharing work in keyword search over databases.pdf:application/pdf},
}

@article{rita,
	title = {{RITA}: {Group} {Attention} is {All} {You} {Need} for {Timeseries} {Analytics}},
	volume = {2},
	issn = {2836-6573},
	shorttitle = {{RITA}},
	url = {https://dl.acm.org/doi/10.1145/3639317},
	doi = {10.1145/3639317},
	abstract = {Timeseries analytics is important in many real-world applications. Recently, the Transformer model, popular in natural language processing, has been leveraged to learn high quality feature embeddings from timeseries: embeddings are key to the performance of various timeseries analytics tasks such as similarity-based timeseries queries within vector databases. However, quadratic time and space complexities limit Transformers' scalability, especially for long timeseries. To address these issues, we develop a timeseries analytics tool, RITA, which uses a novel attention mechanism, named group attention, to address this scalability issue. Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity. It thus significantly reduces the time and space complexity, yet provides a theoretical guarantee on the quality of the computed attention. The dynamic scheduler of RITA continuously adapts the number of groups and the batch size in the training process, ensuring group attention always uses the fewest groups needed to meet the approximation quality requirement. Extensive experiments on various timeseries datasets and analytics tasks demonstrate that RITA outperforms the state-of-the-art in accuracy and is significantly faster --- with speedups of up to 63X.},
	language = {en},
	number = {1},
	urldate = {2024-09-10},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Liang, Jiaming and Cao, Lei and Madden, Samuel and Ives, Zack and Li, Guoliang},
	month = mar,
	year = {2024},
	pages = {1--28},
	file = {Full Text:/home/ryan/Zotero/storage/X9VC5R77/Liang et al. - 2024 - RITA Group Attention is All You Need for Timeseri.pdf:application/pdf},
}

@misc{HttpsWwwvldborgPvldb,
	title = {https://www.vldb.org/pvldb/vol17/p4546-madden.pdf},
	url = {https://www.vldb.org/pvldb/vol17/p4546-madden.pdf},
	urldate = {2024-09-10},
}

@article{db_unbound,
	series = {{VLDB} '24},
	title = {Databases {Unbound}: {Querying} {All} of the {World}'s {Bytes} with {AI}},
	volume = {17},
	doi = {10.14778/3685800.3685916},
	abstract = {Over the past five decades, the relational database model has proven
to be a scaleable and adaptable model for querying a variety of struc-
tured data, with use cases in analytics, transactions, graphs, stream-
ing and more. However, most of the world‚Äôs data is unstructured.
Thus, despite their success, the reality is that the vast majority
of the world‚Äôs data has remained beyond the reach of relational
systems.
The rise of deep learning and generative AI offers an opportu-
nity to change this. These models provide a stunning capability to
extract semantic understanding from almost any type of document,
including text, images, and video, which can extend the reach of
databases to all the world‚Äôs data. In this paper we explore how
these new technologies will transform the way we build database
management software, creating new that systems that can ingest,
store, process, and query all data. Building such systems presents
many opportunities and challenges. In this paper we focus on three:
scalability, correctness, and reliability, and argue that the declara-
tive programming paradigm that has served relational systems so
well offers a path forward in the new world of AI data systems as
well. To illustrate this, we describe several examples of such declar-
ative AI systems we have built in document and video processing,
and provide a set of research challenges and opportunities to guide
research in this exciting area going forward.
And lovely apparitions,‚Äìdim at first,
Then radiant, as the mind arising bright
From the embrace of beauty (whence the forms
Of which these are the phantoms) casts on them
The gathered rays which are reality‚Äì
Shall visit us the progeny immortal
Of Painting, Sculpture, and rapt Poesy,
And arts, though unimagined, yet to be;
Prometheus Unbound, Percy Bysshe Shelley},
	number = {12},
	journal = {PVLDB},
	author = {{Samuel Madden} and {Michael Cafarella} and {Michael Franklin} and {Tim Kraska}},
	year = {2024},
	pages = {4546--4554},
}

@article{discover_nested,
	series = {{VLDB} '24},
	title = {Searching {Data} {Lakes} for {Nested} and {Joined} {Data}},
	volume = {17},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3681954.3682005},
	doi = {10.14778/3681954.3682005},
	abstract = {Exploratory data science is driving new platforms that assist data scientists with everyday tasks, such as integration and wrangling, to assemble training datasets. Such tools take scientists' work-in-progress data as a search object (table or JSON) and find relevant supplementary data from an organizational data lake, which can be unioned or joined with the current data. Existing data lake search tools find single, relational tables to match or join with a search object. Yet many data science applications revolve around hierarchical data, which can only be matched by creating views that simultaneously join and transform several tables in the data lake. In this paper, we extend the Juneau data lake search system [46] for this broader class of matches at scale. Our contribution is a general framework for efficiently merging ranked results to match hierarchical data, leveraging novel techniques for indexing and sketching, and incorporating existing single-table search techniques and ranking functions. We experimentally validate our methods' benefits and broad applicability using real data from data science computational notebooks. Our results indicate that, with different ranking functions, our approach can return the optimal set of views up to 4.8x faster and 43\% more related compared to heuristics, and increase the data domain coverage by up to 28\%. In a case study to show the utility of our results to data science downstream tasks, we reduce regression error by up to 6.6\%, and improve classification accuracy by up to 19.5\%.},
	number = {11},
	urldate = {2024-09-10},
	journal = {Proc. VLDB Endow.},
	author = {Zhang, Yi and Chen, Peter Baile and Ives, Zachary G.},
	month = aug,
	year = {2024},
	pages = {3346--3359},
	file = {Full Text PDF:/home/ryan/Zotero/storage/X8VN5SGT/Zhang et al. - 2024 - Searching Data Lakes for Nested and Joined Data.pdf:application/pdf},
}

@inproceedings{discover_interactive,
	address = {Portland OR USA},
	series = {{SIGMOD} '20},
	title = {Finding {Related} {Tables} in {Data} {Lakes} for {Interactive} {Data} {Science}},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389726},
	doi = {10.1145/3318464.3389726},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zhang, Yi and Ives, Zachary G.},
	month = jun,
	year = {2020},
	pages = {1951--1966},
	file = {Accepted Version:/home/ryan/Zotero/storage/WHRWFTRG/Zhang and Ives - 2020 - Finding Related Tables in Data Lakes for Interacti.pdf:application/pdf},
}

@techreport{aisp_hiv,
	address = {Philadelphia, PA, USA},
	title = {Yes, {No}, {Maybe}? {Legal} \& {Ethical} {Considerations} for {Informed} {Consent} in {Data} {Sharing} and {Integration}.},
	url = {https://aisp.upenn.edu/wp-content/uploads/2023/05/Consent-Brief-Appx.pdf},
	abstract = {Data sharing and integration are increasingly commonplace at every level of government, as cross-program and
cross-sector data provide valuable insights to inform resource allocation, guide program implementation, and evaluate
policies. Data sharing, while routine, is not without risks, and clear legal frameworks for data sharing are essential to
mitigate those risks, protect privacy, and guide responsible data use. In some cases, federal privacy laws offer clear
consent requirements and outline explicit exceptions where consent is not required to share data. In other cases, the
law is unclear or silent regarding whether consent is needed for data sharing. Importantly, consent can present both
ethical and logistical challenges, particularly when integrating cross-sector data. This brief will frame out key concepts
related to consent; explore major federal laws governing the sharing of administrative data, including individually
identifiable information; and examine important ethical implications of consent, particularly in cases when the law is
silent or unclear. Finally, this brief will outline the foundational role of strong governance and consent frameworks in
ensuring ethical data use and offer technical alternatives to consent that may be appropriate for certain data uses.},
	urldate = {2024-09-10},
	institution = {University of Pennsylvania},
	author = {{Deja Kemp} and {Amy Hawn Nelson} and {Della Jenkins}},
	month = may,
	year = {2023},
}

@techreport{aisp_ri,
	address = {Philadelphia, PA, USA},
	title = {Finding a {Way} {Forward}: {How} to {Create} a {Strong} {Legal} {Framework} for {Data} {Integration}},
	url = {https://aisp.upenn.edu/wp-content/uploads/2022/06/AISP_Finding-A-Way-Forward_Final_6.16.2022.pdf},
	urldate = {2024-09-10},
	institution = {University of Pennsylvania},
	author = {{Amy Hawn Nelson} and {Deja Kemp} and {Della Jenkins} and {Jessie Rios Benitez} and {Emily Berkowitz} and {TC Burnett} and {Kristen Smith} and {Sharon Zanti} and {Dennis Culhane}},
	month = jun,
	year = {2022},
}

@inproceedings{hash_sort_agg,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Cache-{Efficient} {Aggregation}: {Hashing} {Is} {Sorting}},
	isbn = {978-1-4503-2758-9},
	shorttitle = {Cache-{Efficient} {Aggregation}},
	url = {https://doi.org/10.1145/2723372.2747644},
	doi = {10.1145/2723372.2747644},
	abstract = {For decades researchers have studied the duality of hashing and sorting for the implementation of the relational operators, especially for efficient aggregation. Depending on the underlying hardware and software architecture, the specifically implemented algorithms, and the data sets used in the experiments, different authors came to different conclusions about which is the better approach. In this paper we argue that in terms of cache efficiency, the two paradigms are actually the same. We support our claim by showing that the complexity of hashing is the same as the complexity of sorting in the external memory model. Furthermore we make the similarity of the two approaches obvious by designing an algorithmic framework that allows to switch seamlessly between hashing and sorting during execution. The fact that we mix hashing and sorting routines in the same algorithmic framework allows us to leverage the advantages of both approaches and makes their similarity obvious. On a more practical note, we also show how to achieve very low constant factors by tuning both the hashing and the sorting routines to modern hardware. Since we observe a complementary dependency of the constant factors of the two routines to the locality of the input, we exploit our framework to switch to the faster routine where appropriate. The result is a novel relational aggregation algorithm that is cache-efficient---independently and without prior knowledge of input skew and output cardinality---, highly parallelizable on modern multi-core systems, and operating at a speed close to the memory bandwidth, thus outperforming the state-of-the-art by up to 3.7x.},
	urldate = {2024-09-15},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {M√ºller, Ingo and Sanders, Peter and Lacurie, Arnaud and Lehner, Wolfgang and F√§rber, Franz},
	month = may,
	year = {2015},
	pages = {1123--1136},
	file = {M√ºller et al. - 2015 - Cache-Efficient Aggregation Hashing Is Sorting.pdf:/home/ryan/Zotero/storage/EU5Y6TAX/M√ºller et al. - 2015 - Cache-Efficient Aggregation Hashing Is Sorting.pdf:application/pdf},
}

@article{concurrent_hash,
	title = {Concurrent {Hash} {Tables}: {Fast} and {General}(?)!},
	volume = {5},
	issn = {2329-4949, 2329-4957},
	shorttitle = {Concurrent {Hash} {Tables}},
	url = {https://dl.acm.org/doi/10.1145/3309206},
	doi = {10.1145/3309206},
	abstract = {Concurrent hash tables are one of the most important concurrent data structures, which are used in numerous applications. For some applications, it is common that hash table accesses dominate the execution time. To efficiently solve these problems in parallel, we need implementations that achieve speedups in highly concurrent scenarios. Unfortunately, currently available concurrent hashing libraries are far away from this requirement, in particular, when adaptively sized tables are necessary or contention on some elements occurs.
            Our starting point for better performing data structures is a fast and simple lock-free concurrent hash table based on linear probing that is, however, limited to word-sized key-value types and does not support dynamic size adaptation. We explain how to lift these limitations in a provably scalable way and demonstrate that dynamic growing has a performance overhead comparable to the same generalization in sequential hash tables.
            We perform extensive experiments comparing the performance of our implementations with six of the most widely used concurrent hash tables. Ours are considerably faster than the best algorithms with similar restrictions and an order of magnitude faster than the best more general tables. In some extreme cases, the difference even approaches four orders of magnitude.
            All our implementations discussed in this publication can be found on github [17].},
	language = {en},
	number = {4},
	urldate = {2024-09-15},
	journal = {ACM Transactions on Parallel Computing},
	author = {Maier, Tobias and Sanders, Peter and Dementiev, Roman},
	month = dec,
	year = {2018},
	pages = {1--32},
	file = {Maier et al. - 2018 - Concurrent Hash Tables Fast and General()!.pdf:/home/ryan/Zotero/storage/5VTT6B2Q/Maier et al. - 2018 - Concurrent Hash Tables Fast and General()!.pdf:application/pdf},
}

@article{ajax,
	series = {{SIGMOD} {Rec} '00},
	title = {{AJAX}: an extensible data cleaning tool},
	volume = {29},
	issn = {0163-5808},
	shorttitle = {{AJAX}},
	url = {https://dl.acm.org/doi/10.1145/335191.336568},
	doi = {10.1145/335191.336568},
	abstract = {@@@@ groups together matching pairs with a high similarity value by applying a given grouping criteria (e.g. by transitive closure). Finally, ging collapses each individual cluster into a tuple of the resulting data source. AJAX provides @@@@ for specifying data cleaning programs, which consists of SQL statements enriched with a set of specific primitives to express these transformations.AJAX also @@@@. It allows the user to interact with an executing data cleaning program to handle exceptional cases and to inspect intermediate results. Finally, AJAX provides @@@@ @@@@ that permits users to determine the source and processing of data for debugging purposes.We will present the AJAX system applied to two real world problems: the consolidation of a telecommunication database, and the conversion of a dirty database of bibliographic references into a set of clean, normalized, and redundancy free relational tables maintaining the same data.},
	number = {2},
	urldate = {2024-09-13},
	journal = {SIGMOD Rec.},
	author = {Galhardas, Helena and Florescu, Daniela and Shasha, Dennis and Simon, Eric},
	month = may,
	year = {2000},
	pages = {590},
	file = {Full Text PDF:/home/ryan/Zotero/storage/DTQDL5BF/Galhardas et al. - 2000 - AJAX an extensible data cleaning tool.pdf:application/pdf},
}

@inproceedings{cleaning_tutorial,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Data {Cleaning}: {Overview} and {Emerging} {Challenges}},
	isbn = {978-1-4503-3531-7},
	shorttitle = {Data {Cleaning}},
	url = {https://doi.org/10.1145/2882903.2912574},
	doi = {10.1145/2882903.2912574},
	abstract = {Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.},
	urldate = {2024-09-12},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan},
	month = jun,
	year = {2016},
	pages = {2201--2206},
	file = {Chu et al. - 2016 - Data Cleaning Overview and Emerging Challenges.pdf:/home/ryan/Zotero/storage/GUJ7JPVP/Chu et al. - 2016 - Data Cleaning Overview and Emerging Challenges.pdf:application/pdf},
}

@article{holoclean,
	series = {{VLDB} '17},
	title = {{HoloClean}: holistic data repairs with probabilistic inference},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {{HoloClean}},
	url = {https://dl.acm.org/doi/10.14778/3137628.3137631},
	doi = {10.14778/3137628.3137631},
	abstract = {We introduce HoloClean, a framework for holistic data repairing driven by probabilistic inference. HoloClean unifies qualitative data repairing, which relies on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, HoloClean automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over HoloClean's probabilistic model scales to instances with millions of tuples. We show that HoloClean finds data repairs with an average precision of ‚àº 90\% and an average recall of above ‚àº 76\% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2√ó against state-of-the-art methods.},
	language = {en},
	number = {11},
	urldate = {2024-09-13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F. and R√©, Christopher},
	month = aug,
	year = {2017},
	pages = {1190--1201},
	file = {Full Text PDF:/home/ryan/Zotero/storage/566RI8UA/Rekatsinas et al. - 2017 - HoloClean holistic data repairs with probabilisti.pdf:application/pdf},
}

@article{rayAntecedentsFatalOverdose2023,
	title = {Antecedents of fatal overdose in an adult cohort identified through administrative record linkage in {Indiana}, 2015‚Äì2022},
	volume = {247},
	issn = {0376-8716},
	url = {https://www.sciencedirect.com/science/article/pii/S0376871623001291},
	doi = {10.1016/j.drugalcdep.2023.109891},
	abstract = {Background
The United States continues to experience unprecedented rates of overdose mortality and need to identify effective policies or practices that can be implemented. This study aims to measure the prevalence, frequency, timing, and rate of touchpoints that occurred prior to a fatal overdose where communities might intervene.
Methods
In collaboration with Indiana state government, we conducted record-linkage of statewide administrative datasets to vital records (January 1, 2015, through August 26, 2022) to identify touchpoints (jail booking, prison release, prescription medication dispensation, emergency department visits, and emergency medical services). We examined touchpoints within 12-months prior to a fatal overdose among an adult cohort and explored variation over time and by demographic characteristics.
Results
Over the 92-month study period there were 13,882 overdose deaths (89.3\% accidental poisonings, X40-X44) in our adult cohort that were record-linked to multiple administrative datasets and revealed nearly two-thirds (64.7\%; n=8980) experienced an emergency department visit, the most prevalent touchpoint followed by prescription medication dispensation, emergency medical services responses, jail booking, and prison release. However, with approximately 1 out of every 100 returning citizens dying from drug overdose within 12-months of release, prison release had the highest touchpoint rate followed by emergency medical services responses, jail booking, emergency department visits, and prescription medication dispensation.
Conclusion
Record-linking administrative data from routine practice to vital records from overdose mortality is a viable means of identifying where resources should be situated to reduce fatal overdose, with potential to evaluate the effectiveness of overdose prevention efforts.},
	urldate = {2024-09-20},
	journal = {Drug and Alcohol Dependence},
	author = {Ray, Bradley and Christian, Kaitlyn and Bailey, Timothy and Alton, Madison and Proctor, Alison and Haggerty, John and Lowder, Evan and Aalsma, Matthew C.},
	month = jun,
	year = {2023},
	keywords = {Emergency department, Emergency medical services, Incarceration, Nonfatal overdose, Prescription opioid dispensation},
	pages = {109891},
	file = {ScienceDirect Snapshot:/home/ryan/Zotero/storage/9U6DTRU5/S0376871623001291.html:text/html},
}

@inproceedings{debugging_sql,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Debugging {Database} {Queries}: {A} {Survey} of {Tools}, {Techniques}, and {Users}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Debugging {Database} {Queries}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376485},
	doi = {10.1145/3313831.3376485},
	abstract = {Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gathani, Sneha and Lim, Peter and Battle, Leilani},
	month = apr,
	year = {2020},
	pages = {1--16},
	file = {Full Text PDF:/home/ryan/Zotero/storage/UI78M67B/Gathani et al. - 2020 - Debugging Database Queries A Survey of Tools, Tec.pdf:application/pdf},
}

@techreport{ri_report,
	address = {Rhode Island, RI},
	type = {House {Finance} {Report}},
	title = {Establishing a {Healthy} {School} {Meals} for {All} {Program} in {Rhode} {Island}: {Exploring} {Policy} {Gaps} and {Recommendations}},
	url = {https://rilegislature.gov/Special/comdoc/House%20Finance%202024/03-07-2024--H7225--Article%208--Jason%20Kashdan.pdf},
	abstract = {Edited by Caroline Kistin and Jocelyn Antonio},
	language = {English},
	number = {H7225},
	urldate = {2024-09-21},
	institution = {RI State Legislature},
	author = {Kashdan, Jason},
	month = mar,
	year = {2024},
	file = {Kashdan - 2024 - Establishing a Healthy School Meals for All Progra.pdf:/home/ryan/Zotero/storage/UHSGBFEF/Kashdan - 2024 - Establishing a Healthy School Meals for All Progra.pdf:application/pdf},
}

@article{andersonPermutationTestsUnivariate2001,
	title = {Permutation tests for univariate or multivariate analysis of variance and regression},
	volume = {58},
	copyright = {http://www.nrcresearchpress.com/page/about/CorporateTextAndDataMining},
	issn = {0706-652X, 1205-7533},
	url = {http://www.nrcresearchpress.com/doi/10.1139/f01-004},
	doi = {10.1139/f01-004},
	language = {en},
	number = {3},
	urldate = {2024-10-02},
	journal = {Canadian Journal of Fisheries and Aquatic Sciences},
	author = {Anderson, Marti J},
	month = mar,
	year = {2001},
	pages = {626--639},
	file = {Anderson - 2001 - Permutation tests for univariate or multivariate a.pdf:/home/ryan/Zotero/storage/QLFBZBTF/Anderson - 2001 - Permutation tests for univariate or multivariate a.pdf:application/pdf},
}

@inproceedings{disagg_mem,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2023},
	title = {Pond: {CXL}-{Based} {Memory} {Pooling} {Systems} for {Cloud} {Platforms}},
	isbn = {978-1-4503-9916-6},
	shorttitle = {Pond},
	url = {https://dl.acm.org/doi/10.1145/3575693.3578835},
	doi = {10.1145/3575693.3578835},
	abstract = {Public cloud providers seek to meet stringent performance requirements and low hardware cost. A key driver of performance and cost is main memory. Memory pooling promises to improve DRAM utilization and thereby reduce costs. However, pooling is challenging under cloud performance requirements. This paper proposes Pond, the first memory pooling system that both meets cloud performance goals and significantly reduces DRAM cost. Pond builds on the Compute Express Link (CXL) standard for load/store access to pool memory and two key insights. First, our analysis of cloud production traces shows that pooling across 8-16 sockets is enough to achieve most of the benefits. This enables a small-pool design with low access latency. Second, it is possible to create machine learning models that can accurately predict how much local and pool memory to allocate to a virtual machine (VM) to resemble same-NUMA-node memory performance. Our evaluation with 158 workloads shows that Pond reduces DRAM costs by 7\% with performance within 1-5\% of same-NUMA-node VM allocations.},
	urldate = {2024-10-15},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 2},
	publisher = {Association for Computing Machinery},
	author = {Li, Huaicheng and Berger, Daniel S. and Hsu, Lisa and Ernst, Daniel and Zardoshti, Pantea and Novakovic, Stanko and Shah, Monish and Rajadnya, Samir and Lee, Scott and Agarwal, Ishwar and Hill, Mark D. and Fontoura, Marcus and Bianchini, Ricardo},
	month = jan,
	year = {2023},
	pages = {574--587},
	file = {Full Text PDF:/home/ryan/Zotero/storage/LSP8DWL6/Li et al. - 2023 - Pond CXL-Based Memory Pooling Systems for Cloud P.pdf:application/pdf},
}

@inproceedings{cost_model,
	address = {Hong Kong, China},
	series = {{VLDB} '02},
	title = {Generic database cost models for hierarchical memory systems},
	abstract = {Accurate prediction of operator execution time is a prerequisite for database query optimization. Although extensively studied for conventional disk-based DBMSs, cost modeling in main-memory DBMSs is still an open issue. Recent database research has demonstrated that memory access is more and more becoming a significant-- if not the major--cost component of database operations. If used properly, fast but small cache memories--usually organized in cascading hierarchy between CPU and main memory--can help to reduce memory access costs. However, they make the cost estimation problem more complex.In this article, we propose a generic technique to create accurate cost functions for database operations. We identify a few basic memory access patterns and provide cost functions that estimate their access costs for each level of the memory hierarchy. The cost functions are parameterized to accommodate various hardware characteristics appropriately. Combining the basic patterns, we can describe the memory access patterns of database operations. The cost functions of database operations can automatically be derived by combining the basic patterns' cost functions accordingly.To validate our approach, we performed experiments using our DBMS prototype Monet. The results presented here confirm the accuracy of our cost models for different operations.Aside from being useful for query optimization, our models provide insight to tune algorithms not only in a main-memory DBMS, but also in a disk-based DBMS with a large main-memory buffer cache.},
	urldate = {2024-10-15},
	booktitle = {Proceedings of the 28th international conference on {Very} {Large} {Data} {Bases}},
	publisher = {VLDB Endowment},
	author = {Manegold, Stefan and Boncz, Peter and Kersten, Martin L.},
	month = aug,
	year = {2002},
	pages = {191--202},
	file = {Full Text PDF:/home/ryan/Zotero/storage/VNK988K8/Manegold et al. - 2002 - Generic database cost models for hierarchical memo.pdf:application/pdf},
}

@article{logic_bugs_qo,
	series = {{SIGMOD} '23},
	title = {Detecting {Logic} {Bugs} of {Join} {Optimizations} in {DBMS}},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3588909},
	doi = {10.1145/3588909},
	abstract = {Generation-based testing techniques have shown their effectiveness in detecting logic bugs of DBMS, which are often caused by improper implementation of query optimizers. Nonetheless, existing generation-based debug tools are limited to single-table queries and there is a substantial research gap regarding multi-table queries with join operators. In this paper, we propose TQS, a novel testing framework targeted at detecting logic bugs derived by queries involving multi-table joins. Given a target DBMS, TQS achieves the goal with two key components: Data-guided Schema and Query Generation (DSG) and Knowledge-guided Query Space Exploration (KQE). DSG addresses the key challenge of multi-table query debugging: how to generate ground-truth (query, result) pairs for verification. It adopts the database normalization technique to generate a testing schema and maintains a bitmap index for result tracking. To improve debug efficiency, DSG also artificially inserts some noises into the generated data. To avoid repetitive query space search, KQE forms the problem as isomorphic graph set discovery and combines the graph embedding and weighted random walk for query generation. We evaluated TQS on four popular DBMSs: MySQL, MariaDB, TiDB and PolarDB. Experimental results show that TQS is effective in finding logic bugs of join optimization in database management systems. It successfully detected 115 bugs within 24 hours, including 31 bugs in MySQL, 30 in MariaDB, 31 in TiDB, and 23 in PolarDB respectively.},
	number = {1},
	urldate = {2024-10-15},
	journal = {Proc. ACM Manag. Data},
	author = {Tang, Xiu and Wu, Sai and Zhang, Dongxiang and Li, Feifei and Chen, Gang},
	month = may,
	year = {2023},
	pages = {55:1--55:26},
	file = {Full Text PDF:/home/ryan/Zotero/storage/4P7NFIFC/Tang et al. - 2023 - Detecting Logic Bugs of Join Optimizations in DBMS.pdf:application/pdf},
}

@inproceedings{high_dim_bo,
	title = {High-dimensional {Bayesian} optimization with sparse axis-aligned subspaces},
	url = {https://proceedings.mlr.press/v161/eriksson21a.html},
	abstract = {Bayesian optimization (BO) is a powerful paradigm for efficient optimization of black-box objective functions. High-dimensional BO presents a particular challenge, in part because the curse of dimensionality makes it difficult to define‚Äîas well as do inference over‚Äîa suitable class of surrogate models. We argue that Gaussian process surrogate models defined on sparse axis-aligned subspaces offer an attractive compromise between flexibility and parsimony. We demonstrate that our approach, which relies on Hamiltonian Monte Carlo for inference, can rapidly identify sparse subspaces relevant to modeling the unknown objective function, enabling sample-efficient high-dimensional BO. In an extensive suite of experiments comparing to existing methods for high-dimensional BO we demonstrate that our algorithm, Sparse Axis-Aligned Subspace BO (SAASBO), achieves excellent performance on several synthetic and real-world problems without the need to set problem-specific hyperparameters.},
	language = {en},
	urldate = {2024-10-15},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Eriksson, David and Jankowiak, Martin},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {493--503},
	file = {Full Text PDF:/home/ryan/Zotero/storage/YT57WYW3/Eriksson and Jankowiak - 2021 - High-dimensional Bayesian optimization with sparse.pdf:application/pdf;Supplementary PDF:/home/ryan/Zotero/storage/FWNJ8TJE/Eriksson and Jankowiak - 2021 - High-dimensional Bayesian optimization with sparse.pdf:application/pdf},
}

@article{deep_active_learning_survey,
	title = {A {Survey} of {Deep} {Active} {Learning}},
	volume = {54},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3472291},
	doi = {10.1145/3472291},
	abstract = {Active learning (AL) attempts to maximize a model‚Äôs performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
	number = {9},
	urldate = {2024-10-15},
	journal = {ACM Comput. Surv.},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
	month = oct,
	year = {2021},
	pages = {180:1--180:40},
	file = {Full Text PDF:/home/ryan/Zotero/storage/NJ25DQTR/Ren et al. - 2021 - A Survey of Deep Active Learning.pdf:application/pdf},
}

@article{fleeing_from_knowledge,
	series = {{VLDBJ} '07},
	title = {Consistent selectivity estimation via maximum entropy},
	volume = {16},
	issn = {1066-8888},
	url = {https://dl.acm.org/doi/10.1007/s00778-006-0030-1},
	doi = {10.1007/s00778-006-0030-1},
	abstract = {Cost-based query optimizers need to estimate the selectivity of conjunctive predicates when comparing alternative query execution plans. To this end, advanced optimizers use multivariate statistics to improve information about the joint distribution of attribute values in a table. The joint distribution for all columns is almost always too large to store completely, and the resulting use of partial distribution information raises the possibility that multiple, non-equivalent selectivity estimates may be available for a given predicate. Current optimizers use cumbersome ad hoc methods to ensure that selectivities are estimated in a consistent manner. These methods ignore valuable information and tend to bias the optimizer toward query plans for which the least information is available, often yielding poor results. In this paper we present a novel method for consistent selectivity estimation based on the principle of maximum entropy (ME). Our method exploits all available information and avoids the bias problem. In the absence of detailed knowledge, the ME approach reduces to standard uniformity and independence assumptions. Experiments with our prototype implementation in DB2 UDB show that use of the ME approach can improve the optimizer‚Äôs cardinality estimates by orders of magnitude, resulting in better plan quality and significantly reduced query execution times. For almost all queries, these improvements are obtained while adding only tens of milliseconds to the overall time required for query optimization.},
	number = {1},
	urldate = {2024-10-16},
	journal = {The VLDB Journal},
	author = {Markl, V. and Haas, P. J. and Kutsch, M. and Megiddo, N. and Srivastava, U. and Tran, T. M.},
	month = jan,
	year = {2007},
	pages = {55--76},
	file = {Full Text PDF:/home/ryan/Zotero/storage/6C5S2YIU/Markl et al. - 2007 - Consistent selectivity estimation via maximum entr.pdf:application/pdf},
}

@article{erikssonHighDimensionalBayesianOptimization,
	title = {High-{Dimensional} {Bayesian} {Optimization} with {Sparse} {Axis}-{Aligned} {Subspaces}},
	abstract = {Bayesian optimization (BO) is a powerful paradigm for efÔ¨Åcient optimization of black-box objective functions. High-dimensional BO presents a particular challenge, in part because the curse of dimensionality makes it difÔ¨Åcult to deÔ¨Åne‚Äîas well as do inference over‚Äîa suitable class of surrogate models. We argue that Gaussian process surrogate models deÔ¨Åned on sparse axis-aligned subspaces offer an attractive compromise between Ô¨Çexibility and parsimony. We demonstrate that our approach, which relies on Hamiltonian Monte Carlo for inference, can rapidly identify sparse subspaces relevant to modeling the unknown objective function, enabling sample-efÔ¨Åcient high-dimensional BO. In an extensive suite of experiments comparing to existing methods for high-dimensional BO we demonstrate that our algorithm, Sparse AxisAligned Subspace BO (SAASBO), achieves excellent performance on several synthetic and realworld problems without the need to set problemspeciÔ¨Åc hyperparameters.},
	language = {en},
	author = {Eriksson, David and Jankowiak, Martin},
	file = {Eriksson and Jankowiak - High-Dimensional Bayesian Optimization with Sparse.pdf:/home/ryan/Zotero/storage/KKJ8E6FI/Eriksson and Jankowiak - High-Dimensional Bayesian Optimization with Sparse.pdf:application/pdf},
}

@misc{bayes_local_structure,
	title = {Train {Short}, {Test} {Long}: {Attention} with {Linear} {Biases} {Enables} {Input} {Length} {Extrapolation}},
	shorttitle = {Train {Short}, {Test} {Long}},
	url = {http://arxiv.org/abs/2108.12409},
	doi = {10.48550/arXiv.2108.12409},
	abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
	month = apr,
	year = {2022},
	note = {arXiv:2108.12409},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/ryan/Zotero/storage/GYJIKQ5V/Press et al. - 2022 - Train Short, Test Long Attention with Linear Bias.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/USJ8D7MA/2108.html:text/html},
}

@article{reopt,
	series = {{ICDE} '19},
	title = {How {I} {Learned} to {Stop} {Worrying} and {Love} {Re}-optimization},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	url = {https://ieeexplore.ieee.org/document/8731491/},
	doi = {10.1109/ICDE.2019.00191},
	abstract = {Cost-based query optimizers remain one of the most important components of database management systems for analytic workloads. Though modern optimizers select plans close to optimal performance in the common case, a small number of queries are an order of magnitude slower than they could be. In this paper we investigate why this is still the case, despite decades of improvements to cost models, plan enumeration, and cardinality estimation. We demonstrate why we believe that a re-optimization mechanism is likely the most cost-effective way to improve end-to-end query performance. We find that even a simple re-optimization scheme can improve the latency of many poorly performing queries. We demonstrate that re-optimization improves the end-to-end latency of the top 20 longest running queries in the Join Order Benchmark by 27\%, realizing most of the benefit of perfect cardinality estimation.},
	urldate = {2024-10-16},
	journal = {2019 IEEE 35th International Conference on Data Engineering (ICDE)},
	author = {Perron, Matthew and Shang, Zeyuan and Kraska, Tim and Stonebraker, Michael},
	month = apr,
	year = {2019},
	note = {Conference Name: 2019 IEEE 35th International Conference on Data Engineering (ICDE)
ISBN: 9781538674741
Place: Macao, Macao
Publisher: IEEE},
	pages = {1758--1761},
	file = {Submitted Version:/home/ryan/Zotero/storage/Z57YTL3U/Perron et al. - 2019 - How I Learned to Stop Worrying and Love Re-optimiz.pdf:application/pdf},
}

@article{udf_compile,
	series = {{VLDB} '15},
	title = {An architecture for compiling {UDF}-centric workflows},
	volume = {8},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2824032.2824045},
	doi = {10.14778/2824032.2824045},
	abstract = {Data analytics has recently grown to include increasingly sophisticated techniques, such as machine learning and advanced statistics. Users frequently express these complex analytics tasks as workflows of user-defined functions (UDFs) that specify each algorithmic step. However, given typical hardware configurations and dataset sizes, the core challenge of complex analytics is no longer sheer data volume but rather the computation itself, and the next generation of analytics frameworks must focus on optimizing for this computation bottleneck. While query compilation has gained widespread popularity as a way to tackle the computation bottleneck for traditional SQL workloads, relatively little work addresses UDF-centric workflows in the domain of complex analytics.In this paper, we describe a novel architecture for automatically compiling workflows of UDFs. We also propose several optimizations that consider properties of the data, UDFs, and hardware together in order to generate different code on a case-by-case basis. To evaluate our approach, we implemented these techniques in Tupleware, a new high-performance distributed analytics system, and our benchmarks show performance improvements of up to three orders of magnitude compared to alternative systems.},
	number = {12},
	urldate = {2024-10-17},
	journal = {Proc. VLDB Endow.},
	author = {Crotty, Andrew and Galakatos, Alex and Dursun, Kayhan and Kraska, Tim and Binnig, Carsten and Cetintemel, Ugur and Zdonik, Stan},
	month = aug,
	year = {2015},
	pages = {1466--1477},
	file = {Full Text PDF:/home/ryan/Zotero/storage/EZJY8RJK/Crotty et al. - 2015 - An architecture for compiling UDF-centric workflow.pdf:application/pdf},
}

@article{udf_tutorial,
	series = {{VLDB} '23},
	title = {Efficient {Execution} of {User}-{Defined} {Functions} in {SQL} {Queries}},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3611540.3611574},
	doi = {10.14778/3611540.3611574},
	abstract = {User-defined functions (UDFs) have been widely used to overcome the expressivity limitations of SQL and complement its declarative nature with functional capabilities. UDFs are particularly useful in today's applications that involve complex data analytics and machine learning algorithms and logic. However, UDFs pose significant performance challenges in query processing and optimization, largely due to the mismatch of the UDF execution and SQL processing environments. In this tutorial, we present state-of-the-art methods and systems towards efficient execution of UDFs in SQL queries. We focus on low-level techniques for physical optimization and compilation of UDF queries, describe and compare the core, recent approaches in the area, discuss their advantages and limitations, identify critical gaps in theory and practice, and propose promising future research directions.},
	number = {12},
	urldate = {2024-10-17},
	journal = {Proc. VLDB Endow.},
	author = {Foufoulas, Yannis and Simitsis, Alkis},
	month = aug,
	year = {2023},
	pages = {3874--3877},
	file = {Full Text PDF:/home/ryan/Zotero/storage/3L3TDIN7/Foufoulas and Simitsis - 2023 - Efficient Execution of User-Defined Functions in S.pdf:application/pdf},
}

@article{udf_partition,
	series = {{VLDB} '21},
	title = {Lachesis: automatic partitioning for {UDF}-centric analytics},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {Lachesis},
	url = {https://dl.acm.org/doi/10.14778/3457390.3457392},
	doi = {10.14778/3457390.3457392},
	abstract = {Partitioning is effective in avoiding expensive shuffling operations. However, it remains a significant challenge to automate this process for Big Data analytics workloads that extensively use user defined functions (UDFs), where sub-computations are hard to be reused for partitionings compared to relational applications. In addition, functional dependency that is widely utilized for partitioning selection is often unavailable in the unstructured data that is ubiquitous in UDF-centric analytics. We propose the
              Lachesis
              system, which represents UDF-centric workloads as workflows of analyzable and reusable sub-computations.
              Lachesis
              further adopts a deep reinforcement learning model to infer which sub-computations should be used to partition the underlying data. This analysis is then applied to automatically optimize the storage of the data across applications to improve the performance and users' productivity.},
	language = {en},
	number = {8},
	urldate = {2024-10-17},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zou, Jia and Das, Amitabh and Barhate, Pratik and Iyengar, Arun and Yuan, Binhang and Jankov, Dimitrije and Jermaine, Chris},
	month = apr,
	year = {2021},
	pages = {1262--1275},
}

@article{udf_neumann,
	series = {{VLDB} '22},
	title = {User-defined operators: efficiently integrating custom algorithms into modern databases},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {User-defined operators},
	url = {https://dl.acm.org/doi/10.14778/3510397.3510408},
	doi = {10.14778/3510397.3510408},
	abstract = {In recent years, complex data mining and machine learning algorithms have become more common in data analytics. Several specialized systems exist to evaluate these algorithms on ever-growing data sets, which are built to efficiently execute different types of complex analytics queries.
            However, using these various systems comes at a price. Moving data out of traditional database systems is often slow as it requires exporting and importing data, which is typically performed using the relatively inefficient CSV format. Additionally, database systems usually offer strong ACID guarantees, which are lost when adding new, external systems. This disadvantage can be detrimental to the consistency of the results.
            Most data scientists still prefer not to use classical database systems for data analytics. The main reason why RDBMS are not used is that SQL is difficult to work with due to its declarative and set-oriented nature, and is not easily extensible.
            We present User-Defined Operators (UDOs) as a concept to include custom algorithms into modern query engines. Users can write idiomatic code in the programming language of their choice, which is then directly integrated into existing database systems. We show that our implementation can compete with specialized tools and existing query engines while retaining all beneficial properties of the database system.},
	language = {en},
	number = {5},
	urldate = {2024-10-17},
	journal = {Proceedings of the VLDB Endowment},
	author = {Sichert, Moritz and Neumann, Thomas},
	month = jan,
	year = {2022},
	pages = {1119--1131},
	file = {Full Text PDF:/home/ryan/Zotero/storage/B5HJRXIQ/Sichert and Neumann - 2022 - User-defined operators efficiently integrating cu.pdf:application/pdf},
}

@article{udf_debug,
	series = {{SIGMOD} '23},
	title = {Udon: {Efficient} {Debugging} of {User}-{Defined} {Functions} in {Big} {Data} {Systems} with {Line}-by-{Line} {Control}},
	volume = {1},
	shorttitle = {Udon},
	url = {https://dl.acm.org/doi/10.1145/3626712},
	doi = {10.1145/3626712},
	abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala to process large amounts of data efficiently, while data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. In this paper, we propose Udon, a novel debugger to support fine-grained debugging of UDFs. Udon encapsulates the modern line-by-line debugging primitives, such as the ability to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. It includes a novel debug-aware UDF execution model to ensure the responsiveness of the operator during debugging. It utilizes advanced state-transfer techniques to satisfy breakpoint conditions that span across multiple UDFs. It incorporates various optimization techniques to reduce the runtime overhead. We conduct experiments with multiple UDF workloads on various datasets and show its high efficiency and scalability.},
	number = {4},
	urldate = {2024-10-17},
	journal = {Proc. ACM Manag. Data},
	author = {Huang, Yicong and Wang, Zuozhi and Li, Chen},
	month = dec,
	year = {2023},
	pages = {225:1--225:26},
	file = {Full Text PDF:/home/ryan/Zotero/storage/W5BEKM46/Huang et al. - 2023 - Udon Efficient Debugging of User-Defined Function.pdf:application/pdf},
}

@article{low_data_lqo_benchmark,
	series = {{VLDB} '24},
	title = {Is {Your} {Learned} {Query} {Optimizer} {Behaving} {As} {You} {Expect}? {A} {Machine} {Learning} {Perspective}},
	volume = {17},
	issn = {2150-8097},
	shorttitle = {Is {Your} {Learned} {Query} {Optimizer} {Behaving} {As} {You} {Expect}?},
	url = {https://dl.acm.org/doi/10.14778/3654621.3654625},
	doi = {10.14778/3654621.3654625},
	abstract = {The current boom of learned query optimizers (LQO) can be explained not only by the general continuous improvement of deep learning (DL) methods but also by the straightforward formulation of a query optimization problem (QOP) as a machine learning (ML) one. The idea is often to replace dynamic programming approaches, widespread for solving QOP, with more powerful methods such as reinforcement learning. However, such a rapid "game change" in the field of QOP could not pass without consequences - other parts of the ML pipeline, except for predictive model development, have large improvement potential. For instance, different LQOs introduce their own restrictions on training data generation from queries, use an arbitrary train/validation approach, and evaluate on a voluntary split of benchmark queries.In this paper, we attempt to standardize the ML pipeline for evaluating LQOs by introducing a new end-to-end benchmarking framework. Additionally, we guide the reader through each data science stage in the ML pipeline and provide novel insights from the machine learning perspective, considering the specifics of QOP. Finally, we perform a rigorous evaluation of existing LQOs, showing that PostgreSQL outperforms these LQOs in almost all experiments depending on the train/test splits.},
	number = {7},
	urldate = {2024-10-17},
	journal = {Proc. VLDB Endow.},
	author = {Lehmann, Claude and Sulimov, Pavel and Stockinger, Kurt},
	month = may,
	year = {2024},
	pages = {1565--1577},
	file = {Full Text PDF:/home/ryan/Zotero/storage/A4U2NXUP/Lehmann et al. - 2024 - Is Your Learned Query Optimizer Behaving As You Ex.pdf:application/pdf},
}

@article{mc_reco,
	series = {{BDMA} '18},
	title = {A survey of matrix completion methods for recommendation systems},
	volume = {1},
	issn = {2097-406X},
	url = {https://ieeexplore.ieee.org/document/8400447},
	doi = {10.26599/BDMA.2018.9020008},
	abstract = {In recent years, the recommendation systems have become increasingly popular and have been used in a broad variety of applications. Here, we investigate the matrix completion techniques for the recommendation systems that are based on collaborative filtering. The collaborative filtering problem can be viewed as predicting the favorability of a user with respect to new items of commodities. When a rating matrix is constructed with users as rows, items as columns, and entries as ratings, the collaborative filtering problem can then be modeled as a matrix completion problem by filling out the unknown elements in the rating matrix. This article presents a comprehensive survey of the matrix completion methods used in recommendation systems. We focus on the mathematical models for matrix completion and the corresponding computational algorithms as well as their characteristics and potential issues. Several applications other than the traditional user-item association prediction are also discussed.},
	number = {4},
	urldate = {2024-10-17},
	journal = {Big Data Mining and Analytics},
	author = {Ramlatchan, Andy and Yang, Mengyun and Liu, Quan and Li, Min and Wang, Jianxin and Li, Yaohang},
	month = dec,
	year = {2018},
	note = {Conference Name: Big Data Mining and Analytics},
	keywords = {Prediction algorithms, Computational modeling, Mathematical model, Computer science, Predictive models, Collaboration, collaborative filtering, matrix completion, Matrix decomposition, recommendation systems},
	pages = {308--323},
	file = {Full Text PDF:/home/ryan/Zotero/storage/LR7T4NS4/Ramlatchan et al. - 2018 - A survey of matrix completion methods for recommen.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ryan/Zotero/storage/8YNJLX7M/8400447.html:text/html},
}

@article{survival_analysis,
	title = {Machine {Learning} for {Survival} {Analysis}: {A} {Survey}},
	volume = {51},
	issn = {0360-0300},
	shorttitle = {Machine {Learning} for {Survival} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3214306},
	doi = {10.1145/3214306},
	abstract = {Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called censoring can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.},
	number = {6},
	urldate = {2024-10-18},
	journal = {ACM Comput. Surv.},
	author = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
	month = feb,
	year = {2019},
	pages = {110:1--110:36},
	file = {Full Text PDF:/home/ryan/Zotero/storage/AINA46ZV/Wang et al. - 2019 - Machine Learning for Survival Analysis A Survey.pdf:application/pdf},
}

@misc{active_learning_as_mab,
	title = {Building {Bridges}: {Viewing} {Active} {Learning} from the {Multi}-{Armed} {Bandit} {Lens}},
	shorttitle = {Building {Bridges}},
	url = {http://arxiv.org/abs/1309.6830},
	doi = {10.48550/arXiv.1309.6830},
	abstract = {In this paper we propose a multi-armed bandit inspired, pool based active learning algorithm for the problem of binary classification. By carefully constructing an analogy between active learning and multi-armed bandits, we utilize ideas such as lower confidence bounds, and self-concordant regularization from the multi-armed bandit literature to design our proposed algorithm. Our algorithm is a sequential algorithm, which in each round assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for the label of this sampled point. The design of this sampling distribution is also inspired by the analogy between active learning and multi-armed bandits. We show how to derive lower confidence bounds required by our algorithm. Experimental comparisons to previously proposed active learning algorithms show superior performance on some standard UCI datasets.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Ganti, Ravi and Gray, Alexander G.},
	month = sep,
	year = {2013},
	note = {arXiv:1309.6830},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/ryan/Zotero/storage/KTFN39P7/Ganti and Gray - 2013 - Building Bridges Viewing Active Learning from the.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/RLYZYM48/1309.html:text/html},
}

@article{active_learning_pool,
	series = {{AAAI} '15},
	title = {Active {Learning} by {Learning}},
	volume = {29},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9597},
	doi = {10.1609/aaai.v29i1.9597},
	abstract = {Pool-based active learning is an important technique that helps reduce labeling efforts within a pool of unlabeled instances. Currently, most pool-based active learning strategies are constructed based on some human-designed philosophy; that is, they reflect what human beings assume to be ‚Äúgood labeling questions.‚Äù However, while such human-designed philosophies can be useful on specific data sets, it is often difficult to establish the theoretical connection of those philosophies to the true learning performance of interest. In addition, given that a single human-designed philosophy is unlikely to work on all scenarios, choosing and blending those strategies under different scenarios is an important but challenging practical task. This paper tackles this task by letting the machines adaptively ‚Äúlearn‚Äù from the performance of a set of given strategies on a particular data set. More specifically, we design a learning algorithm that connects active learning with the well-known multi-armed bandit problem. Further, we postulate that, given an appropriate choice for the multi-armed bandit learner, it is possible to estimate the performance of different strategies on the fly. Extensive empirical studies of the resulting ALBL algorithm confirm that it performs better than state-of-the-art strategies and a leading blending algorithm for active learning, all of which are based on human-designed philosophy.},
	language = {en},
	number = {1},
	urldate = {2024-10-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Hsu, Wei-Ning and Lin, Hsuan-Tien},
	month = feb,
	year = {2015},
	note = {Number: 1},
	keywords = {machine learning},
	file = {Full Text PDF:/home/ryan/Zotero/storage/SM2A5MMX/Hsu and Lin - 2015 - Active Learning by Learning.pdf:application/pdf},
}

@inproceedings{cmab_active_learning,
	address = {Cham},
	series = {{NeurIPS} '14},
	title = {Contextual {Bandit} for {Active} {Learning}: {Active} {Thompson} {Sampling}},
	isbn = {978-3-319-12637-1},
	shorttitle = {Contextual {Bandit} for {Active} {Learning}},
	doi = {10.1007/978-3-319-12637-1_51},
	abstract = {The labelling of training examples is a costly task in a supervised classification. Active learning strategies answer this problem by selecting the most useful unlabelled examples to train a predictive model. The choice of examples to label can be seen as a dilemma between the exploration and the exploitation over the data space representation. In this paper, a novel active learning strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. We propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label. Experimental comparison to previously proposed active learning algorithms show superior performance on a real application dataset.},
	language = {en},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Bouneffouf, Djallel and Laroche, Romain and Urvoy, Tanguy and Feraud, Raphael and Allesiardo, Robin},
	editor = {Loo, Chu Kiong and Yap, Keem Siah and Wong, Kok Wai and Teoh, Andrew and Huang, Kaizhu},
	year = {2014},
	keywords = {Active learning, Contextual Bandits, Thompson sampling},
	pages = {405--412},
	file = {Full Text PDF:/home/ryan/Zotero/storage/44SLHJC9/Bouneffouf et al. - 2014 - Contextual Bandit for Active Learning Active Thom.pdf:application/pdf},
}

@misc{fair_active_learning,
	title = {Falcon: {Fair} {Active} {Learning} using {Multi}-armed {Bandits}},
	shorttitle = {Falcon},
	url = {http://arxiv.org/abs/2401.12722},
	doi = {10.48550/arXiv.2401.12722},
	abstract = {Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from "target groups" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood of postponing due to undesired label prediction, and the optimal balance varies per dataset. We capture the trade-off between informativeness and postpone rate as policies and propose to automatically select the best policy using adversarial multi-armed bandit methods, given their computational efficiency and theoretical guarantees. Experiments show that Falcon significantly outperforms existing fair active learning approaches in terms of fairness and accuracy and is more efficient. In particular, only Falcon supports a proper trade-off between accuracy and fairness where its maximum fairness score is 1.8-4.5x higher than the second-best results.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Tae, Ki Hyun and Zhang, Hantian and Park, Jaeyoung and Rong, Kexin and Whang, Steven Euijong},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12722},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/ryan/Zotero/storage/3BVJILI5/Tae et al. - 2024 - Falcon Fair Active Learning using Multi-armed Ban.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/JEXKEBV8/2401.html:text/html},
}

@misc{langchain,
	title = {{LangChain}},
	url = {https://github.com/langchain-ai/langchai},
	author = {{Chase Harrison}},
	month = oct,
	year = {2022},
}

@misc{selfies,
	title = {Self-{Referencing} {Embedded} {Strings} ({SELFIES}): {A} 100\% robust molecular string representation},
	shorttitle = {Self-{Referencing} {Embedded} {Strings} ({SELFIES})},
	url = {http://arxiv.org/abs/1905.13741},
	doi = {10.48550/arXiv.1905.13741},
	abstract = {The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering -- generally denoted as inverse design -- was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100{\textbackslash}\% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.},
	urldate = {2024-11-30},
	publisher = {arXiv},
	author = {Krenn, Mario and H√§se, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Al√°n},
	month = mar,
	year = {2020},
	note = {arXiv:1905.13741},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Chemical Physics, Quantum Physics},
	file = {Preprint PDF:/home/ryan/Zotero/storage/L5MXR38C/Krenn et al. - 2020 - Self-Referencing Embedded Strings (SELFIES) A 100% robust molecular string representation.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/8HLK76I6/1905.html:text/html},
}

@article{geom_card_est,
	series = {{VLDB} '24},
	title = {Sample-{Efficient} {Cardinality} {Estimation} {Using} {Geometric} {Deep} {Learning}},
	volume = {17},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3636218.3636229},
	doi = {10.14778/3636218.3636229},
	abstract = {In database systems, accurate cardinality estimation is a cornerstone of effective query optimization. In this context, estimators that use machine learning have shown significant promise. Despite their potential, the effectiveness of these learned estimators strongly depends on their ability to learn from small training sets.This paper presents a novel approach for learned cardinality estimation that addresses this issue by enhancing sample efficiency. We propose a neural network architecture informed by geometric deep learning principles that represents queries as join graphs. Furthermore, we introduce an innovative encoding for complex predicates, treating their encoding as a feature selection problem. Additionally, we devise a regularization term that employs equalities of the relational algebra and three-valued logic, augmenting the training process without requiring additional ground truth cardinalities. We rigorously evaluate our model across multiple benchmarks, examining q-errors, runtimes, and the impact of workload distribution shifts. Our results demonstrate that our model significantly improves the end-to-end runtimes of PostgreSQL, even with cardinalities gathered from as little as 100 query executions.},
	number = {4},
	urldate = {2024-11-30},
	journal = {Proc. VLDB Endow.},
	author = {Reiner, Silvan and Grossniklaus, Michael},
	month = mar,
	year = {2024},
	pages = {740--752},
	file = {Full Text PDF:/home/ryan/Zotero/storage/BHNEYDM5/Reiner and Grossniklaus - 2024 - Sample-Efficient Cardinality Estimation Using Geometric Deep Learning.pdf:application/pdf},
}

@inproceedings{llm_steer_qo,
	address = {Vancouver, BC, Canada},
	series = {{MLForSystems} @ {NeurIPS} '24},
	title = {The {Unreasonable} {Effectiveness} of {LLMs} for {Query} {Optimization}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2411.02862},
	doi = {10.48550/arXiv.2411.02862},
	abstract = {Recent work in database query optimization has used complex machine learning strategies, such as customized reinforcement learning schemes. Surprisingly, we show that LLM embeddings of query text contain useful semantic information for query optimization. Specifically, we show that a simple binary classifier deciding between alternative query plans, trained only on a small number of labeled embedded query vectors, can outperform existing heuristic systems. Although we only present some preliminary results, an LLM-powered query optimizer could provide significant benefits, both in terms of performance and simplicity.},
	urldate = {2024-12-06},
	author = {Akioyamen, Peter and Yi, Zixuan and Marcus, Ryan},
	month = dec,
	year = {2024},
	note = {arXiv:2411.02862 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases},
	file = {Preprint PDF:/home/ryan/Zotero/storage/N7AVH4AL/Akioyamen et al. - 2024 - The Unreasonable Effectiveness of LLMs for Query O.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/RJCR4WQZ/2411.html:text/html},
}

@misc{qo_analysis_pass,
	title = {Enabling {Data} {Dependency}-based {Query} {Optimization}},
	url = {http://arxiv.org/abs/2406.06886},
	doi = {10.48550/arXiv.2406.06886},
	abstract = {Data dependency-based query optimization techniques can considerably improve database system performance: we apply three such optimization techniques to five database management systems (DBMSs) and observe throughput improvements between 5 \% and 33 \%. We address two key challenges to achieve these results: (i) efficiently identifying and extracting relevant dependencies from the data, and (ii) making use of the dependencies through SQL rewrites or as transformation rules in the optimizer. First, the schema does not provide all relevant dependencies. We present a workload-driven dependency discovery approach to find additional dependencies within milliseconds. Second, the throughput improvement of a state-of-the-art DBMS is 13 \% using only SQL rewrites, but 20 \% when we integrate dependency-based optimization into the optimizer and execution engine, e. g., by employing dependency propagation and subquery handling. Using all relevant dependencies, the runtime of four standard benchmarks improves by up to 10 \% compared to using only primary and foreign keys, and up to 22 \% compared to not using dependencies. The dependency discovery overhead amortizes after a single workload execution.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Lindner, Daniel and Ritter, Daniel and Naumann, Felix},
	month = jun,
	year = {2024},
	note = {arXiv:2406.06886 [cs]},
	keywords = {Computer Science - Databases},
	file = {Preprint PDF:/home/ryan/Zotero/storage/LLV8BAPF/Lindner et al. - 2024 - Enabling Data Dependency-based Query Optimization.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/8BA5PQ5M/2406.html:text/html},
}

@misc{timeseries_sql,
	title = {{TIMESERIES} clause},
	url = {https://docs.vertica.com/en/sql-reference/statements/select/timeseries-clause/},
	abstract = {Provides gap-filling and interpolation (GFI) computation, an important component of time series analytics computation.},
	language = {en},
	urldate = {2025-01-01},
	journal = {Vertica 24.2.x},
	author = {{OpenText}},
	note = {Section: sql-reference},
	file = {Snapshot:/home/ryan/Zotero/storage/JJFFG98V/timeseries-clause.html:text/html},
}

@misc{redshift_ml,
	title = {Machine learning - {Amazon} {Redshift}},
	url = {https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.html},
	urldate = {2025-01-01},
	author = {{AWS}},
	file = {Machine learning - Amazon Redshift:/home/ryan/Zotero/storage/BMIV9R5H/machine_learning.html:text/html},
}

@misc{redshift_array,
	title = {array function - {Amazon} {Redshift}},
	url = {https://docs.aws.amazon.com/redshift/latest/dg/r_array.html},
	urldate = {2025-01-01},
	author = {{AWS}},
	file = {array function - Amazon Redshift:/home/ryan/Zotero/storage/GKM7W7DT/r_array.html:text/html},
}

@article{dm-trend,
	title = {Business analytics in managerial decision-making: top management perceptions},
	volume = {ahead-of-print},
	issn = {1368-3047},
	shorttitle = {Business analytics in managerial decision-making},
	url = {https://www.emerald.com/insight/content/doi/10.1108/mbe-09-2023-0130/full/html},
	doi = {10.1108/MBE-09-2023-0130},
	abstract = {Firms seek to improve their decision-making and enable more ‚Äúfact-based‚Äù decisions by using business analytics. While the benefits of using business analytics to monitor, develop and improve daily operations have been reported by many scholars, using it in more complex top management decisions has received less attention. Building on the resource-based view of the firm, this study aims to investigate top management perceptions of using business analytics for making decisions on firm resources.,This study uses semi-structured interviews to collect perceptions of 12 top managers in large firms on when and why they use business analytics in their decision-making.,Top managers use business analytics output as their main source of information for monitoring ongoing business performance against set targets and taking corrective actions. Concerning future-oriented planning and strategic decision-making involving more complex changes on the firms‚Äô resource base, top managers proactively complement knowledge derived via business analytics with other sources of knowledge, such as stakeholder and expert opinions. Moreover, top managers use of business analytics depends on their own expectations of its value potential and on the expectations of their organization.,This study adds to the extant literature on the business value of business analytics by outlining the purposes and reasons for top management business analytics use. By demonstrating when and why top managers apply business analytics when making decisions on the firm‚Äôs current and future resource base, this study contributes to the discussion on the resource-based view and decision-making practices of the firm.},
	language = {en},
	number = {ahead-of-print},
	urldate = {2025-01-01},
	journal = {Measuring Business Excellence},
	author = {Orjatsalo, Johanna and Hussinki, Henri and Stoklasa, Jan},
	month = oct,
	year = {2024},
	note = {Publisher: Emerald Publishing Limited},
	file = {Snapshot:/home/ryan/Zotero/storage/RWC4XISP/html.html:text/html},
}

@article{shortest_path_qo,
	series = {{SIGMOD} '23},
	title = {Efficiently {Computing} {Join} {Orders} with {Heuristic} {Search}},
	volume = {1},
	issn = {2836-6573},
	url = {https://dl.acm.org/doi/10.1145/3588927},
	doi = {10.1145/3588927},
	abstract = {Join order optimization is one of the most fundamental problems in processing queries on relational data. It has been studied extensively for almost four decades now. Still, because of its NP hardness, no generally efficient solution exists and the problem remains an important topic of research. The scope of algorithms to compute join orders ranges from exhaustive enumeration, to combinatorics based on graph properties, to greedy search, to genetic algorithms, to recently investigated machine learning. A few works exist that use heuristic search to compute join orders. However, a theoretical argument why and how heuristic search is applicable to join order optimization is lacking.
            In this work, we investigate join order optimization via heuristic search. In particular, we provide a strong theoretical framework, in which we reduce join order optimization to the shortest path problem. We then thoroughly analyze the properties of this problem and the applicability of heuristic search. We devise crucial optimizations to make heuristic search tractable. We implement join ordering via heuristic search in a real DBMS and conduct an extensive empirical study. Our findings show that for star- and clique-shaped queries, heuristic search finds optimal plans an order of magnitude faster than current state of the art. Our suboptimal solutions further extend the cost/time Pareto frontier.},
	language = {en},
	number = {1},
	urldate = {2025-01-01},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Haffner, Immanuel and Dittrich, Jens},
	month = may,
	year = {2023},
	pages = {1--26},
	file = {Haffner and Dittrich - 2023 - Efficiently Computing Join Orders with Heuristic S.pdf:/home/ryan/Zotero/storage/LCPSMW9W/Haffner and Dittrich - 2023 - Efficiently Computing Join Orders with Heuristic S.pdf:application/pdf},
}

@inproceedings{dpcpp,
	address = {Seoul, Korea},
	series = {{VLDB} '06},
	title = {Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products},
	abstract = {Two approaches to derive dynamic programming algorithms for constructing join trees are described in the literature. We show analytically and experimentally that these two variants exhibit vastly diverging runtime behaviors for different query graphs. More specifically, each variant is superior to the other for one kind of query graph (chain or clique), but fails for the other. Moreover, neither of them handles star queries well. This motivates us to derive an algorithm that is superior to the two existing algorithms because it adapts to the search space implied by the query graph.},
	urldate = {2025-01-03},
	booktitle = {Proceedings of the 32nd international conference on {Very} large data bases},
	publisher = {VLDB Endowment},
	author = {Moerkotte, Guido and Neumann, Thomas},
	month = sep,
	year = {2006},
	pages = {930--941},
}

@misc{bayesdb_bench,
	title = {posteriordb: {Testing}, {Benchmarking} and {Developing} {Bayesian} {Inference} {Algorithms}},
	shorttitle = {posteriordb},
	url = {http://arxiv.org/abs/2407.04967},
	doi = {10.48550/arXiv.2407.04967},
	abstract = {The generality and robustness of inference algorithms is critical to the success of widely used probabilistic programming languages such as Stan, PyMC, Pyro, and Turing.jl. When designing a new general-purpose inference algorithm, whether it involves Monte Carlo sampling or variational approximation, the fundamental problem arises in evaluating its accuracy and efficiency across a range of representative target models. To solve this problem, we propose posteriordb, a database of models and data sets defining target densities along with reference Monte Carlo draws. We further provide a guide to the best practices in using posteriordb for model evaluation and comparison. To provide a wide range of realistic target densities, posteriordb currently comprises 120 representative models and has been instrumental in developing several general inference algorithms.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Magnusson, M√•ns and Torgander, Jakob and B√ºrkner, Paul-Christian and Zhang, Lu and Carpenter, Bob and Vehtari, Aki},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04967 [stat]},
	keywords = {Statistics - Computation},
	file = {Preprint PDF:/home/ryan/Zotero/storage/E3NGYDYF/Magnusson et al. - 2024 - posteriordb Testing, Benchmarking and Developing Bayesian Inference Algorithms.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/7GG7WUYP/2407.html:text/html},
}

@incollection{prob_prog,
	title = {A survey of probabilistic logic programming},
	volume = {20},
	isbn = {978-1-970001-99-0},
	url = {https://doi.org/10.1145/3191315.3191319},
	abstract = {The combination of logic programming and probability has proven useful for modeling domains with complex and uncertain relationships among elements. Many probabilistic logic programming (PLP) semantics have been proposed; among these, the distribution semantics has recently gained increased attention and has been adopted by many languages such as the Independent Choice Logic, PRISM, Logic Programs with Annotated Disjunctions, ProbLog, and P-log.This chapter reviews the distribution semantics, beginning with the simplest case with stratified Datalog programs, and showing how the definition is extended to programs that include function symbols and non-stratified negation. The languages that adopt the distribution semantics are also discussed and compared both to one another and to Bayesian networks.We then survey existing approaches for inference in PLP languages that follow the distribution semantics. We concentrate on the PRISM, ProbLog, and PITA systems. The PRISM system was one of the first and can be applied when certain restrictions on the program hold. ProbLog introduced the use of Binary Decision Diagrams that provide a computational basis for removing these restrictions and so performing inference over more general classes of logic programs. PITA speeds up inference by using tabling and answer subsumption. It supports general probabilistic programs, but can easily be optimized for simpler settings and even possibilistic uncertain reasoning. The chapter also discusses the computational complexity of the various approaches together with techniques for limiting it by resorting to approximation.},
	urldate = {2025-02-03},
	booktitle = {Declarative {Logic} {Programming}: {Theory}, {Systems}, and {Applications}},
	publisher = {Association for Computing Machinery and Morgan \& Claypool},
	author = {Riguzzi, Fabrizio and Swift, Theresa},
	month = sep,
	year = {2018},
	pages = {185--228},
	file = {Full Text PDF:/home/ryan/Zotero/storage/5ZBUHBQ5/Riguzzi and Swift - 2018 - A survey of probabilistic logic programming.pdf:application/pdf},
}

@inproceedings{fittest_qo,
	address = {Zurich, Switzerland},
	series = {{VLDB} '95},
	title = {The {Fittest} {Survives}: {An} {Adaptive} {Approach} to {Query} {Optimization}},
	shorttitle = {The {Fittest} {Survives}},
	url = {https://www.semanticscholar.org/paper/The-Fittest-Survives%3A-An-Adaptive-Approach-to-Query-Lu-Tan/3d2625f15445adc9dd23324d4839c5dd364630fa},
	abstract = {Traditionally, optimizers are ‚Äúprogrammed‚Äù to optimize queries following a set of buildin procedures. However, optimizers should be robust to its changing environment to generate the fittest query execution plans. To realize adaptiveness, we propose and design an adaptive optimizer with two features. First, the search space and search strategy of the optimizer can be tuned by parameters to allow the optimizer to pick the one that fits best during the optimization process. Second, the optimizer features a ‚Äúlearning‚Äù capability for canned queries that allows existing plans to be incrementally replaced by ‚Äúfitter‚Äù ones. An experimental study on large multijoin queries based on an analytical model is used to demonstrate the effectiveness of such an approach.},
	urldate = {2025-02-02},
	author = {Lu, Hongjun and Tan, K. and Dao, S.},
	month = sep,
	year = {1995},
}

@article{workload_reopt,
	series = {{VLDB} '19},
	title = {Guided automated learning for query workload re-optimization},
	volume = {12},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3352063.3352120},
	doi = {10.14778/3352063.3352120},
	abstract = {Query optimization is a hallmark of database systems. When an SQL query runs more expensively than is viable or warranted, determination of the performance issues is usually performed manually in consultation with experts through the analysis of query's execution plan (QEP). However, this is an excessively time consuming, human error-prone, and costly process. GALO is a novel system that automates this process. The tool automatically learns recurring problem patterns in query plans over workloads in an offline learning phase, to build a knowledge base of plan-rewrite remedies. It then uses the knowledge base online to re-optimize queries often quite drastically.
            
              GALO's knowledge base is built on RDF and SPARQL, W3C graph database standards, which is well suited for manipulating and querying over SQL query plans, which are graphs themselves. GALO acts as a third-tier of re-optimization, after query rewrite and cost-based optimization, as a
              query plan rewrite
              . For generality, the context of knowledge base problem patterns, including table and column names, is abstracted with canonical symbol labels. Since the knowledge base is not tied to the context of supplied QEPs, table and column names are matched automatically during the re-optimization phase. Thus, problem patterns learned over a particular query workload can be applied in other query workloads. GALO's knowledge base is also an invaluable tool for database experts to debug query performance issues by tracking to known issues and solutions as well as refining the optimizer with new tuned techniques by the development team. We demonstrate an experimental study of the effectiveness of our techniques over synthetic TPC-DS and real IBM client query workloads.},
	language = {en},
	number = {12},
	urldate = {2025-02-02},
	journal = {Proceedings of the VLDB Endowment},
	author = {Damasio, Guilherme and Corvinelli, Vincent and Godfrey, Parke and Mierzejewski, Piotr and Mihaylov, Alex and Szlichta, Jaroslaw and Zuzarte, Calisto},
	month = aug,
	year = {2019},
	pages = {2010--2021},
	file = {Full Text PDF:/home/ryan/Zotero/storage/M94TP57B/Damasio et al. - 2019 - Guided automated learning for query workload re-optimization.pdf:application/pdf},
}

@inproceedings{robopt,
	address = {New York, NY, USA},
	series = {{SIGMOD} '24},
	title = {{RobOpt}: {A} {Tool} for {Robust} {Workload} {Optimization} {Based} on {Uncertainty}-{Aware} {Machine} {Learning}},
	isbn = {979-8-4007-0422-2},
	shorttitle = {{RobOpt}},
	url = {https://doi.org/10.1145/3626246.3654755},
	doi = {10.1145/3626246.3654755},
	abstract = {Relational database management systems (RDBMSs) employ query optimizers to search for execution plans deemed optimal for specific queries. Classical optimizers rely on inaccurate parameter estimates and assumptions that may not hold true in real-world scenarios. Consequently, suboptimal execution plans may be chosen, leading to poor query execution performance. Recent proposals of learned query optimizers that leverage Machine Learning suffer also from the selection of suboptimal plans. In order to fill this gap, we have created Robust Workload Optimization (RobOpt), a prototype tool that facilitates robust execution of a query workload in RDBMSs. It implements a novel technique that takes workload logs as input, generates training samples, and trains a risk-aware learned cost model. It optimizes risk-aware plan selection strategies to achieve a desired level of runtime performance and robustness. In addition, it analyzes a workload according to its training samples and determines an optimal plan selection strategy either at the workload or query level. Ultimately, it enables the robust execution of any workload by determining an optimal plan selection strategy per query. RobOpt can work on top of any RDBMS.},
	urldate = {2025-02-02},
	booktitle = {Companion of the 2024 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Kamali, Amin and Kantere, Verena and Zuzarte, Calisto and Corvinelli, Vincent},
	month = jun,
	year = {2024},
	pages = {468--471},
}

@inproceedings{bayesdb2,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {A probabilistic programming approach to probabilistic data analysis},
	isbn = {978-1-5108-3881-9},
	abstract = {Probabilistic techniques are central to data analysis, but different approaches can be challenging to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include discriminative machine learning, hierarchical Bayesian models, multivariate kernel methods, clustering algorithms, and arbitrary probabilistic programs. We demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling definition language and structured query language. The practical value is illustrated in two ways. First, the paper describes an analysis on a database of Earth satellites, which identifies records that probably violate Kepler's Third Law by composing causal probabilistic programs with non-parametric Bayes in 50 lines of probabilistic code. Second, it reports the lines of code and accuracy of CGPMs compared with baseline solutions from standard machine learning libraries.},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Saad, Feras and Mansinghka, Vikash},
	month = dec,
	year = {2016},
	pages = {2019--2027},
	file = {Full Text PDF:/home/ryan/Zotero/storage/USSRBGK7/Saad and Mansinghka - 2016 - A probabilistic programming approach to probabilistic data analysis.pdf:application/pdf},
}

@misc{bayesdb1,
	title = {Probabilistic {Search} for {Structured} {Data} via {Probabilistic} {Programming} and {Nonparametric} {Bayes}},
	url = {http://arxiv.org/abs/1704.01087},
	doi = {10.48550/arXiv.1704.01087},
	abstract = {Databases are widespread, yet extracting relevant data can be difficult. Without substantial domain knowledge, multivariate search queries often return sparse or uninformative results. This paper introduces an approach for searching structured data based on probabilistic programming and nonparametric Bayes. Users specify queries in a probabilistic language that combines standard SQL database search operators with an information theoretic ranking function called predictive relevance. Predictive relevance can be calculated by a fast sparse matrix algorithm based on posterior samples from CrossCat, a nonparametric Bayesian model for high-dimensional, heterogeneously-typed data tables. The result is a flexible search technique that applies to a broad class of information retrieval problems, which we integrate into BayesDB, a probabilistic programming platform for probabilistic data analysis. This paper demonstrates applications to databases of US colleges, global macroeconomic indicators of public health, and classic cars. We found that human evaluators often prefer the results from probabilistic search to results from a standard baseline.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Saad, Feras and Casarsa, Leonardo and Mansinghka, Vikash},
	month = apr,
	year = {2017},
	note = {arXiv:1704.01087 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Databases},
	file = {Preprint PDF:/home/ryan/Zotero/storage/VG7HR2KX/Saad et al. - 2017 - Probabilistic Search for Structured Data via Probabilistic Programming and Nonparametric Bayes.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/E69HGIHL/1704.html:text/html},
}

@article{deepdb,
	series = {{VLDB} '20},
	title = {{DeepDB}: learn from data, not from queries!},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {{DeepDB}},
	url = {https://dl.acm.org/doi/10.14778/3384345.3384349},
	doi = {10.14778/3384345.3384349},
	abstract = {The typical approach for learned DBMS components is to capture the behavior by running a representative set of queries and use the observations to train a machine learning model. This workload-driven approach, however, has two major downsides. First, collecting the training data can be very expensive, since all queries need to be executed on potentially large databases. Second, training data has to be recollected when the workload or the database changes. To overcome these limitations, we take a diÔ¨Äerent route and propose a new data-driven approach for learned DBMS components which directly supports changes of the workload and data without the need of retraining. Indeed, one may now expect that this comes at a price of lower accuracy since workload-driven approaches can make use of more information. However, this is not the case. The results of our empirical evaluation demonstrate that our data-driven approach not only provides better accuracy than state-ofthe-art learned components but also generalizes better to unseen queries.},
	language = {en},
	number = {7},
	urldate = {2025-01-30},
	journal = {Proceedings of the VLDB Endowment},
	author = {Hilprecht, Benjamin and Schmidt, Andreas and Kulessa, Moritz and Molina, Alejandro and Kersting, Kristian and Binnig, Carsten},
	month = mar,
	year = {2020},
	pages = {992--1005},
	file = {PDF:/home/ryan/Zotero/storage/JGDTN2I2/Hilprecht et al. - 2020 - DeepDB learn from data, not from queries!.pdf:application/pdf},
}

@inproceedings{ce_entropy,
	address = {New York, NY, USA},
	series = {{PODS} '10},
	title = {Understanding cardinality estimation using entropy maximization},
	isbn = {978-1-4503-0033-9},
	url = {https://doi.org/10.1145/1807085.1807095},
	doi = {10.1145/1807085.1807095},
	abstract = {Cardinality estimation is the problem of estimating the number of tuples returned by a query; it is a fundamentally important task in data management, used in query optimization, progress estimation, and resource provisioning. We study cardinality estimation in a principled framework: given a set of statistical assertions about the number of tuples returned by a fixed set of queries, predict the number of tuples returned by a new query. We model this problem using the probability space, over possible worlds, that satisfies all provided statistical assertions and maximizes entropy. We call this the Entropy Maximization model for statistics (MaxEnt). In this paper we develop the mathematical techniques needed to use the MaxEnt model for predicting the cardinality of conjunctive queries.},
	urldate = {2025-01-29},
	booktitle = {Proceedings of the twenty-ninth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	publisher = {Association for Computing Machinery},
	author = {R√©, Christopher and Suciu, Dan},
	month = jun,
	year = {2010},
	pages = {53--64},
}

@misc{gridar_lce,
	title = {Grid-{AR}: {A} {Grid}-based {Booster} for {Learned} {Cardinality} {Estimation} and {Range} {Joins}},
	shorttitle = {Grid-{AR}},
	url = {http://arxiv.org/abs/2410.07895},
	doi = {10.48550/arXiv.2410.07895},
	abstract = {We propose an advancement in cardinality estimation by augmenting autoregressive models with a traditional grid structure. The novel hybrid estimator addresses the limitations of autoregressive models by creating a smaller representation of continuous columns and by incorporating a batch execution for queries with range predicates, as opposed to an iterative sampling approach. The suggested modification markedly improves the execution time of the model for both training and prediction, reduces memory consumption, and does so with minimal decline in accuracy. We further present an algorithm that enables the estimator to calculate cardinality estimates for range join queries efficiently. To validate the effectiveness of our cardinality estimator, we conduct and present a comprehensive evaluation considering state-of-the-art competitors using three benchmark datasets‚Äîdemonstrating vast improvements in execution times and resource utilization.},
	language = {en},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Gjurovski, Damjan and Davitkova, Angjela and Michel, Sebastian},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07895 [cs]},
	keywords = {Computer Science - Databases},
	file = {PDF:/home/ryan/Zotero/storage/QC7VGAFF/Gjurovski et al. - 2024 - Grid-AR A Grid-based Booster for Learned Cardinality Estimation and Range Joins.pdf:application/pdf},
}

@article{asm_lce,
	series = {{SIGMOD} '24},
	title = {{ASM}: {Harmonizing} {Autoregressive} {Model}, {Sampling}, and {Multi}-dimensional {Statistics} {Merging} for {Cardinality} {Estimation}},
	volume = {2},
	issn = {2836-6573},
	shorttitle = {{ASM}},
	url = {https://dl.acm.org/doi/10.1145/3639300},
	doi = {10.1145/3639300},
	abstract = {Recent efforts in learned cardinality estimation (CE) have substantially improved estimation accuracy and query plans inside query optimizers. However, achieving decent efficiency, scalability, and the support of a wide range of queries at the same time, has remained questionable. Rather than falling back to traditional approaches to trade off one criterion with another, we present a new learned approach that achieves all these. Our method, called ASM, harmonizes autoregressive models for per-table statistics estimation, sampling for merging these statistics for join queries, and multi-dimensional statistics merging that extends the sampling for estimating thousands of sub-queries, without assuming independence between join keys. Extensive experiments show that ASM significantly improves query plans under a similar or smaller overhead than the previous learned methods and supports a wider range of queries.},
	language = {en},
	number = {1},
	urldate = {2025-01-24},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Kim, Kyoungmin and Lee, Sangoh and Kim, Injung and Han, Wook-Shin},
	month = mar,
	year = {2024},
	pages = {1--27},
	file = {Full Text:/home/ryan/Zotero/storage/X87BZ4I7/Kim et al. - 2024 - ASM Harmonizing Autoregressive Model, Sampling, and Multi-dimensional Statistics Merging for Cardin.pdf:application/pdf},
}

@article{alece_lce,
	series = {{VLDB} '23},
	title = {{ALECE}: {An} {Attention}-based {Learned} {Cardinality} {Estimator} for {SPJ} {Queries} on {Dynamic} {Workloads}},
	volume = {17},
	issn = {2150-8097},
	shorttitle = {{ALECE}},
	url = {https://doi.org/10.14778/3626292.3626302},
	doi = {10.14778/3626292.3626302},
	abstract = {For efficient query processing, DBMS query optimizers have for decades relied on delicate cardinality estimation methods. In this work, we propose an Attention-based LEarned Cardinality Estimator (ALECE for short) for SPJ queries. The core idea is to discover the implicit relationships between queries and underlying dynamic data using attention mechanisms in ALECE's two modules that are built on top of carefully designed featurizations for data and queries. In particular, from all attributes in the database, the data-encoder module obtains organic and learnable aggregations which implicitly represent correlations among the attributes, whereas the query-analyzer module builds a bridge between the query featurizations and the data aggregations to predict the query's cardinality. We experimentally evaluate ALECE on multiple dynamic workloads. The results show that ALECE enables PostgreSQL's optimizer to achieve nearly optimal performance, clearly outperforming its built-in cardinality estimator and other alternatives.},
	number = {2},
	urldate = {2025-01-24},
	journal = {Proc. VLDB Endow.},
	author = {Li, Pengfei and Wei, Wenqing and Zhu, Rong and Ding, Bolin and Zhou, Jingren and Lu, Hua},
	month = oct,
	year = {2023},
	pages = {197--210},
	file = {Submitted Version:/home/ryan/Zotero/storage/9NKZ5FIQ/Li et al. - 2023 - ALECE An Attention-based Learned Cardinality Estimator for SPJ Queries on Dynamic Workloads.pdf:application/pdf},
}

@article{hitthegym,
	series = {{VLDB} '24},
	title = {Hit the {Gym}: {Accelerating} {Query} {Execution} to {Efficiently} {Bootstrap} {Behavior} {Models} for {Self}-{Driving} {Database} {Management} {Systems}},
	volume = {17},
	issn = {2150-8097},
	shorttitle = {Hit the {Gym}},
	url = {https://dl.acm.org/doi/10.14778/3681954.3682030},
	doi = {10.14778/3681954.3682030},
	abstract = {Autonomous database management systems (DBMSs) aim to optimize themselves automatically without human guidance. They rely on machine learning (ML) models that predict their run-time behavior to evaluate whether a candidate configuration is beneficial without the expensive execution of queries. However, the high cost of collecting the training data to build these models makes them impractical for real-world deployments. Furthermore, these models are instance-specific and thus require retraining whenever the DBMS's environment changes. State-of-the-art methods spend over 93\% of their time running queries for training versus tuning.
            To mitigate this problem, we present the Boot framework for automatically accelerating training data collection in DBMSs. Boot utilizes macro- and micro-acceleration (MMA) techniques that modify query execution semantics with approximate run-time telemetry and skip repetitive parts of the training process. To evaluate Boot, we integrated it into a database gym for PostgreSQL. Our experimental evaluation shows that Boot reduces training collection times by up to 268√ó with modest degradation in model accuracy. These results also indicate that our MMA-based approach scales with dataset size and workload complexity.},
	language = {en},
	number = {11},
	urldate = {2025-01-24},
	journal = {Proceedings of the VLDB Endowment},
	author = {Lim, Wan Shen and Ma, Lin and Zhang, William and Butrovich, Matthew and Arch, Samuel and Pavlo, Andrew},
	month = jul,
	year = {2024},
	pages = {3680--3693},
}

@article{concurrent_lqo,
	title = {Lemo: {A} {Cache}-{Enhanced} {Learned} {Optimizer} for {Concurrent} {Queries}},
	volume = {1},
	shorttitle = {Lemo},
	url = {https://dl.acm.org/doi/10.1145/3626734},
	doi = {10.1145/3626734},
	abstract = {With the expansion of modern database services, multi-user access has become a crucial feature in various practical application scenarios, including enterprise applications and e-commerce platforms. However, if multiple users submit queries within a short time frame, it can result in potential issues such as redundant computation and query concurrency. Unfortunately, most existing multi-query optimization methods, which aim to enhance query processing efficiency, have not adequately addressed these two problems, especially in the setting where multiple queries are being executed concurrently. To this end, we propose a novel method named Lemo for the multi-query optimization problem. Specifically, we propose a novel value network to predict latencies of concurrent queries as the foundation model for query plan generation. Furthermore, we introduce a shared buffer manager component to cache the intermediate results of sub-queries. The shared buffer manager applies a novel replacement policy to maintain the cached buffer with the objective of maximizing the opportunity for the reuse of the cached sub-queries. Based on the shared buffer, our proposed value network can incorporate the cached results into cost estimation to further guide Lemo in generating query plans, thus avoiding redundant computation. Lemo has been integrated into PostgreSQL and experiments conducted on real datasets with PostgreSQL show that it outperforms all the baselines in efficiency.},
	number = {4},
	urldate = {2025-01-24},
	journal = {Proc. ACM Manag. Data},
	author = {Mo, Songsong and Chen, Yile and Wang, Hao and Cong, Gao and Bao, Zhifeng},
	month = dec,
	year = {2023},
	pages = {247:1--247:26},
	file = {Full Text PDF:/home/ryan/Zotero/storage/U6LHKB8K/Mo et al. - 2023 - Lemo A Cache-Enhanced Learned Optimizer for Concurrent Queries.pdf:application/pdf},
}

@misc{genjoin_qo,
	title = {{GenJoin}: {Conditional} {Generative} {Plan}-to-{Plan} {Query} {Optimizer} that {Learns} from {Subplan} {Hints}},
	shorttitle = {{GenJoin}},
	url = {http://arxiv.org/abs/2411.04525},
	doi = {10.48550/arXiv.2411.04525},
	abstract = {Query optimization has become a research area where classical algorithms are being challenged by machine learning algorithms. At the same time, recent trends in learned query optimizers have shown that it is prudent to take advantage of decades of database research and augment classical query optimizers by shrinking the plan search space through different types of hints (e.g. by specifying the join type, scan type or the order of joins) rather than completely replacing the classical query optimizer with machine learning models. It is especially relevant for cases when classical optimizers cannot fully enumerate all logical and physical plans and, as an alternative, need to rely on less robust approaches like genetic algorithms. However, even symbiotically learned query optimizers are hampered by the need for vast amounts of training data, slow plan generation during inference and unstable results across various workload conditions. In this paper, we present GenJoin - a novel learned query optimizer that considers the query optimization problem as a generative task and is capable of learning from a random set of subplan hints to produce query plans that outperform the classical optimizer. GenJoin is the first learned query optimizer that significantly and consistently outperforms PostgreSQL as well as state-of-the-art methods on two well-known real-world benchmarks across a variety of workloads using rigorous machine learning evaluations.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Sulimov, Pavel and Lehmann, Claude and Stockinger, Kurt},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04525 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases},
	file = {Preprint PDF:/home/ryan/Zotero/storage/827SQT6W/Sulimov et al. - 2024 - GenJoin Conditional Generative Plan-to-Plan Query Optimizer that Learns from Subplan Hints.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/NCUI4T9C/2411.html:text/html},
}

@misc{hero_qo,
	title = {{HERO}: {Hint}-{Based} {Efficient} and {Reliable} {Query} {Optimizer}},
	shorttitle = {{HERO}},
	url = {http://arxiv.org/abs/2412.02372},
	doi = {10.48550/arXiv.2412.02372},
	abstract = {We propose a novel model for learned query optimization which provides query hints leading to better execution plans. The model addresses the three key challenges in learned hint-based query optimization: reliable hint recommendation (ensuring non-degradation of query latency), efficient hint exploration, and fast inference. We provide an in-depth analysis of existing NN-based approaches to hint-based optimization and experimentally confirm the named challenges for them. Our alternative solution consists of a new inference schema based on an ensemble of context-aware models and a graph storage for reliable hint suggestion and fast inference, and a budget-controlled training procedure with a local search algorithm that solves the issue of exponential search space exploration. In experiments on standard benchmarks, our model demonstrates optimization capability close to the best achievable with coarse-grained hints. Controlling the degree of parallelism (query dop) in addition to operator-related hints enables our model to achieve 3x latency improvement on JOB benchmark which sets a new standard for optimization. Our model is interpretable and easy to debug, which is particularly important for deployment in production.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Zinchenko, Sergey and Iazov, Sergey},
	month = dec,
	year = {2024},
	note = {arXiv:2412.02372 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases},
	file = {Preprint PDF:/home/ryan/Zotero/storage/CCPHLXRF/Zinchenko and Iazov - 2024 - HERO Hint-Based Efficient and Reliable Query Optimizer.pdf:application/pdf;Snapshot:/home/ryan/Zotero/storage/XSMVI8AC/2412.html:text/html},
}

@inproceedings{bayesqo,
	title = {Learned {Offline} {Query} {Planning} via {Bayesian} {Optimization}},
	author = {{Authors anonymized for review}},
	year = {2025},
}

@article{index_advisor_study,
	series = {{VLDB} '24},
	title = {Breaking {It} {Down}: {An} {In}-{Depth} {Study} of {Index} {Advisors}},
	volume = {17},
	issn = {2150-8097},
	shorttitle = {Breaking {It} {Down}},
	url = {https://dl.acm.org/doi/10.14778/3675034.3675035},
	doi = {10.14778/3675034.3675035},
	abstract = {Index advisors aim to improve workload performance by judiciously selecting an appropriate set of indexes. Various heuristic-based and learning-based methods have been proposed. However, there lacks a comprehensive assessment of existing index advisors, i.e., their advantages, limitations, and application scenarios. In this work, we conduct an in-depth study of existing index advisors in five key aspects. First, we initiate an end-to-end analysis, i.e., a completed analysis throughout the entire workflow of index advisors. We decompose index advisors into three essential building blocks, establish a taxonomy to classify methods used in each block, and analyze the strengths and weaknesses of these methods. Second, we develop a unified open-source testbed, implementing seventeen index advisors across eleven open-source or real-world datasets. We enable customizable configurations to meet diverse testing requirements. Third, we conduct an extensive assessment of index advisors across database systems in various scenarios. We evaluate their adaptability and robustness, identifying practical application scenarios. Fourth, we undertake a fine-grained ablation study by investigating variants of each building block. We identify effective variants and pinpoint significant factors impacting index advisors‚Äô performance via explainable machine-learning techniques. Lastly, we consolidate our findings that could shed light on research directions to advance the future development of index advisors.},
	language = {en},
	number = {10},
	urldate = {2025-01-24},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhou, Wei and Lin, Chen and Zhou, Xuanhe and Li, Guoliang},
	month = jun,
	year = {2024},
	pages = {2405--2418},
	file = {PDF:/home/ryan/Zotero/storage/KRW4935M/Zhou et al. - 2024 - Breaking It Down An In-Depth Study of Index Advisors.pdf:application/pdf},
}

@article{learned_shift,
	series = {{SIGMOD} '24},
	title = {Modeling {Shifting} {Workloads} for {Learned} {Database} {Systems}},
	volume = {2},
	issn = {2836-6573},
	url = {https://dl.acm.org/doi/10.1145/3639293},
	doi = {10.1145/3639293},
	abstract = {Learned database systems address several weaknesses of traditional cost estimation techniques in query optimization: they learn a model of a database instance, e.g., as queries are executed. However, when the database instance has skew and correlation, it is nontrivial to create an effective training set that anticipates workload shifts, where query structure changes and/or different regions of the data contribute to query answers. Our predictive model may perform poorly with these out-of-distribution inputs. In this paper, we study how the notion of a replay buffer can be managed through online algorithms to build a concise yet representative model of the workload distribution --- allowing for rapid adaptation and effective prediction of cardinalities and costs. We experimentally validate our methods over several data domains.},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Wu, Peizhi and Ives, Zachary G.},
	month = mar,
	year = {2024},
	pages = {1--27},
}

@article{leon_qo,
	series = {{VLDB} '23},
	title = {{LEON}: {A} {New} {Framework} for {ML}-{Aided} {Query} {Optimization}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {{LEON}},
	url = {https://dl.acm.org/doi/10.14778/3598581.3598597},
	doi = {10.14778/3598581.3598597},
	abstract = {Query optimization has long been a fundamental yet challenging topic in the database field. With the prosperity of machine learning (ML), some recent works have shown the advantages of reinforcement learning (RL) based learned query optimizer. However, they suffer from fundamental limitations due to the data-driven nature of ML. Motivated by the ML characteristics and database maturity, we propose LEON-a framework for ML-aidEd query OptimizatioN. LEON improves the expert query optimizer to self-adjust to the particular deployment by leveraging ML and the fundamental knowledge in the expert query optimizer. To train the ML model, a pairwise ranking objective is proposed, which is substantially different from the previous regression objective. To help the optimizer to escape the local minima and avoid failure, a ranking and uncertainty-based exploration strategy is proposed, which discovers the valuable plans to aid the optimizer. Furthermore, an ML model-guided pruning is proposed to increase the planning efficiency without hurting too much performance. Extensive experiments offer evidence that the proposed framework can outperform the state-of-the-art methods in terms of end-to-end latency performance, training efficiency, and stability.},
	number = {9},
	urldate = {2025-02-07},
	journal = {Proc. VLDB Endow.},
	author = {Chen, Xu and Chen, Haitian and Liang, Zibo and Liu, Shuncheng and Wang, Jinghong and Zeng, Kai and Su, Han and Zheng, Kai},
	month = may,
	year = {2023},
	pages = {2261--2273},
	file = {Full Text PDF:/home/ryan/Zotero/storage/UDGH366N/Chen et al. - 2023 - LEON A New Framework for ML-Aided Query Optimization.pdf:application/pdf},
}

@inproceedings{inc_reopt,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Enabling {Incremental} {Query} {Re}-{Optimization}},
	isbn = {978-1-4503-3531-7},
	url = {https://dl.acm.org/doi/10.1145/2882903.2915212},
	doi = {10.1145/2882903.2915212},
	abstract = {As declarative query processing techniques expand to the Web, data streams, network routers, and cloud platforms, there is an increasing need to re-plan execution in the presence of unanticipated performance changes. New runtime information may affect which query plan we prefer to run. Adaptive techniques require innovation both in terms of the algorithms used to estimate costs, and in terms of the search algorithm that finds the best plan. We investigate how to build a cost-based optimizer that recomputes the optimal plan incrementally given new cost information, much as a stream engine constantly updates its outputs given new data. Our implementation especially shows benefits for stream processing workloads. It lays the foundations upon which a variety of novel adaptive optimization algorithms can be built. We start by leveraging the recently proposed approach of formulating query plan enumeration as a set of recursive datalog queries; we develop a variety of novel optimization approaches to ensure effective pruning in both static and incremental cases. We further show that the lessons learned in the declarative implementation can be equally applied to more traditional optimizer implementations.},
	urldate = {2025-02-07},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Mengmeng and Ives, Zachary G. and Loo, Boon Thau},
	month = jun,
	year = {2016},
	pages = {1705--1720},
	file = {Full Text PDF:/home/ryan/Zotero/storage/NW35KVAX/Liu et al. - 2016 - Enabling Incremental Query Re-Optimization.pdf:application/pdf},
}

@inproceedings{rio_reopt,
	address = {New York, NY, USA},
	series = {{SIGMOD} '05},
	title = {Proactive re-optimization with {Rio}},
	isbn = {978-1-59593-060-6},
	url = {https://dl.acm.org/doi/10.1145/1066157.1066294},
	doi = {10.1145/1066157.1066294},
	abstract = {Traditional query optimizers rely on the accuracy of estimated statistics of intermediate subexpressions to choose good query execution plans. This design often leads to suboptimal plan choices for complex queries since errors in estimates grow exponentially in the presence of skewed and correlated data distributions. We propose to demonstrate the Rio prototype database system that uses proactive re-optimization to address the problems with traditional optimizers. Rio supports three new techniques:1. Intervals of uncertainty are considered around estimates of statistics during plan enumeration and costing2. These intervals are used to pick execution plans that are robust to deviations of actual values of statistics from estimated values, or to defer the choice of execution plan until the uncertainty in estimates can be resolved3. Statistics of intermediate subexpressions are collected quickly, accurately, and efficiently during query executionThese three features are fully functional in the current Rio prototype which is built using the Predator open-source DBMS [5]. In this proposal, we first describe the novel features of Rio, then we use an example query to illustrate the main aspects of our demonstration.},
	urldate = {2025-02-07},
	booktitle = {Proceedings of the 2005 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {Association for Computing Machinery},
	author = {Babu, Shivnath and Bizarro, Pedro and DeWitt, David},
	month = jun,
	year = {2005},
	pages = {936--938},
	file = {Full Text PDF:/home/ryan/Zotero/storage/LJKWKAJQ/Babu et al. - 2005 - Proactive re-optimization with Rio.pdf:application/pdf},
}

@article{opengauss,
	series = {{VLDB} '21},
	title = {{openGauss}: an autonomous database system},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {{openGauss}},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476380},
	doi = {10.14778/3476311.3476380},
	abstract = {Although learning-based database optimization techniques have been studied from academia in recent years, they have not been widely deployed in commercial database systems. In this work, we build an autonomous database framework and integrate our proposed learning-based database techniques into an open-source database system openGauss. We propose effective learning-based models to build learned optimizers (including learned query rewrite, learned cost/cardinality estimation, learned join order selection and physical operator selection) and learned database advisors (including self-monitoring, self-diagnosis, self-configuration, and self-optimization). We devise an effective validation model to validate the effectiveness of learned models. We build effective training data management and model management platforms to easily deploy learned models. We have evaluated our techniques on real-world datasets and the experimental results validated the effectiveness of our techniques. We also provide our learnings of deploying learning-based techniques.},
	language = {en},
	number = {12},
	urldate = {2025-02-07},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Guoliang and Zhou, Xuanhe and Sun, Ji and Yu, Xiang and Han, Yue and Jin, Lianyuan and Li, Wenbo and Wang, Tianqing and Li, Shifu},
	month = jul,
	year = {2021},
	pages = {3028--3042},
}
