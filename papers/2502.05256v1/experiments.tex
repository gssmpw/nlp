\section{Experiments} \label{sec:eval}

We sought to answer the following research questions about \sysname:

\begin{enumerate}[label=\textbf{RQ\arabic*.}, leftmargin=*]
    \item How effectively does \sysname reduce query latency for a workload given a certain time budget? (\Cref{sec:eval-overall}, \Cref{sec:eval-case-studies}) \label{rq:optimization-tradeoff}
    \item How important are our modifications to Bayesian optimization to the performance of \sysname? (\Cref{sec:eval-timeout-ablation}) \label{rq:bo-ablation}
    \item How robust are plans generated by \sysname to data drift? Can previously optimized plans help jump-start reoptimization? (\Cref{sec:eval-drift}) \label{rq:drift}
    \item Can we train an LLM from offline optimization results to generalize to unseen queries? (\Cref{sec:eval-llm}) \label{rq:llm}
\end{enumerate}

% Key experimental questions (some rough ideas):

% 1) How effectively can \sysname reduce query latency for a given time budget?
% 2) How robust are \sysname's plans to data drift?
% 3) How important is the VAE reconstruction accuracy?
% 4) How important are censored observations?
% 5) ... other ablations?

\subsection{Setup}
We evaluate \sysname over four sets of queries:
\begin{enumerate}[leftmargin=*]
    \item \textbf{JOB}: The entire Join Order Benchmark (introduced by Leis et al. \cite{job}), which consists of 113 queries over the IMDB dataset.
    \item \textbf{CEB}: A subset of the Cardinality Estimation Benchmark (introduced by Negi et al. in ``Flow-Loss'' \cite{flowloss}), which consists of $\sim3000$ queries divided across 16 query templates over the IMDB dataset.  We select the top 100 and bottom 100 queries by improvement of the optimal hint set vs. the PostgreSQL default plan and the 100 queries with longest-running PostgreSQL default plans. There is some overlap between these categories, resulting in 234 total queries representing 13 templates.
    \item \textbf{Stack}: A subset of the StackOverflow benchmark (introduced by Marcus et al. in ``Bao'' \cite{bao}), which consists  of $\sim6000$ queries divided across 16 query templates. We selected the longest-running queries from each template in equal proportion (excluding templates consisting entirely of queries that took less than 1 second), producing a list of 200 queries.
    \edit{
    \item \textbf{DSB}: 3 generated queries from each of 30 templates, based on TPC-DS but enhanced with more complex data distributions and query templates (introduced by Ding et al. ~\cite{ding_dsb}). We use generated queries from the ``agg'' and ``spj'' template sets. Following Wu et al.~\cite{learned_shift}, we use a scale factor of 50.
    }
\end{enumerate}

Workload characteristics are summarized in \Cref{table:workloads}.

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c}
        \textbf{Name} & \textbf{Size on Disk}  & \textbf{Queries} & \textbf{Median joins per query} \\
        \hline
         JOB & 8GB & 113 & 7 \\
         CEB & 8GB & 234 & 10 \\
         Stack & 64GB & 200 & 6 \\
         DSB & 89GB & 90 & 5 \\
    \end{tabular}
    \caption{Characteristics for the four evaluation workloads.}
    \label{table:workloads}
\end{table}

The offline query planning setting has not received much attention in the query optimization literature. To our knowledge, this work is one of the first to demonstrate an offline optimization technique. As such, we compare plans generated by \sysname to plans from PostgreSQL, Bao, Balsa, and the random non-cross-join plan generation technique described in \cref{sec:initialization_strategies}, which we refer to as \random. \edit{The \random strategy can be seen as representing the gains from performing offline optimization at all, and we use it to understand whether our technique brings further improvement when rethinking the query optimization contract~\cite{chaudhuri2009_rethinking_qo}.}

Instead of actually running Bao, we instead execute all hint sets (49 total, comprised of all combinations of join and scan hints) and take the hint set with the fastest runtime. This is the best plan that Bao could ever produce, since it focuses on steering PostgreSQL's existing optimizer using hints. We choose this baseline as representing the best that traditional heuristic-based query optimizers can do in the offline query optimization setting. Unless stated otherwise, all BO runs in the following section are initialized using these 49 hinted ``Bao'' plans and runtimes.


We choose Balsa~\cite{balsa} as a baseline representing reinforcement learning-based systems, which are also not originally designed for the offline query optimization setting. We use the default configuration for Balsa, except that we set $S = 1.5$ (the multiplier on query timeout values), which we found to universally improve results in our experimental setup. We note that the comparison to Balsa is not entirely fair: as a reinforcement learning-powered query optimizer, Balsa seeks to minimize \emph{regret}, whereas our BO-based approach seeks to minimize the \emph{best latency found}. This distinction is illustrated in Figure~\ref{fig:bayes_vs_rl}, but the most important experimental consequence is that Balsa will occasionally repeat a query plan that it considers to be ``good'' in order to maximize reward, which is obviously suboptimal in an offline optimization scenario. In these experimental results, we use a plan cache to avoid actually executing any exactly-duplicated query plans.

\random can be thought of as an offline optimization technique that purely explores the space of possible plans. It learns from feedback only insofar as it decreases the time spent on bad plans by settings its timeout to the runtime of the best plan seen (as, unlike with BO, there is no point in executing plans worse than the best-seen). The initial timeout greatly affects how many plans can be tried within a given time budget, as a lower timeout results in more plans tried within a particular time period, but we do not know a priori what the runtime of the fastest possible plan is. We initialize the \random plan generation process with the PostgreSQL default plan runtime as the initial timeout.

All queries are executed against PostgreSQL version 16.3. For all workloads, we disable JIT and set \texttt{join\_collapse\_limit} to 1. Physical operator hints are specified using \texttt{pg\_hint\_plan}. We configure PostgreSQL with 32GB shared buffers and 16MB work memory. For the Stack queries, we disabled GEQO as it was causing high variability in plan performance.

We create indexes on all join keys on the IMDB, Stack, \edit{and DSB} datasets. For Stack, some tables have compound join keys. We build all indexes such that for each set of join predicates between two tables present in all queries in the workload, an index exists containing all of the referenced columns on each side.

Our VAE is based on the transformer VAE architecture introduced in \cite{lolbo}. For each database, the set of training query plans were divided into an 80\%/20\% train-test split, over which the VAE was trained for $800,000$ steps. To determine an appropriate latent space size, VAEs were trained with varying latent dimensionality on the IMDB dataset to evaluate the trade-off between compression and reconstruction, results of which are shown in \Cref{table:vae_accuracy}. A latent space of $64$ dimensions was chosen as it represented a good balance between latent dimension and reconstruction accuracy. Each VAE was trained on 2 Ampere A6000s for $\sim24$ hours. On the GPU cloud that we were using, this cost roughly $\$24$ for each VAE.

\begin{table}
    \centering
    \begin{tabular}{c|c}
        \textbf{Latent Dimension} & \textbf{Reconstruction Accuracy} \\
        \hline
        128 & 97.93\% \\
        64 & 89.67\% \\
        32 & 58.71\% \\
        16 & 24.79\% \\
        8 & 8.49\% \\
        \end{tabular}
    \caption{VAE reconstruction accuracy on the validation set at different latent dimensions, higher is better.}
    \label{table:vae_accuracy}
\end{table}




\subsection{Plan Optimization} \label{sec:eval-overall}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/cdf_sep.pdf}
    \caption{Best plans found at the end of optimization with each technique for each workload. Towards the top right corner is better. \circleOne On each workload, \random fails to find any improvement over the best Bao plan for 30-50\% of queries. \circleTwo \sysname always finds the most improvement compared to the other methods, with this difference being more pronounced on JOB and Stack than on CEB. On Stack, \sysname finds over 2\texttimes~ improvement on 50\% of queries. \circleThree Balsa underperforms when used as an offline optimizer on Stack due to longer query runtimes limiting exploration by the RL algorithm. \circleFour \sysname finds improvement on the \textasciitilde 25\% of Stack queries where the other two techniques find none.}
    \label{fig:workload-summaries}
\end{figure*}

In order to answer \labelcref{rq:optimization-tradeoff}, we executed each of our baseline optimization techniques for several hours for each query in each of our three workloads. \Cref{fig:workload-summaries} visualizes the results at the end of optimization for \bo, Balsa, and \random across our three workloads by comparing the cumulative distribution of queries that achieve at least a certain percentage improvement in plan runtime compared to Bao. ``\% improvement over Bao'' refers to the percentage reduction in plan runtime for a certain query compared to the runtime of the optimal Bao plan (e.g. a reduction in runtime from 1 second to 200 milliseconds would be an improvement of 80\%). The visual separation between the series for the different techniques trending towards the top right of each plot indicates gaps in performance in which one technique is finding plans for more queries with better performance than another technique.

We strove to make comparisons between techniques fair by giving each optimization technique the same optimization budget, since offline optimization techniques can hypothetically be executed for an indefinite amount of time to find potentially faster plans. We only considered time spent executing proposed plans against the database as consuming budget and exclude the overhead of executing each technique (the overhead for \sysname is analyzed in \Cref{sec:overhead}). Each optimization technique was executed for 4000 plan executions. We choose this because query runtimes span from tens of milliseconds to tens of seconds, and we did not want to optimize some queries with $1000\times$ more observations than others.

These plots make obvious the fact that \sysname is finding more plans with better performance compared to the baselines across all three workloads. JOB and Stack are highly differentiated, while all techniques perform similarly on CEB. \edit{DSB is notable in its proportion of queries for which no technique finds much improvement over Bao, but for those queries where improvement can be found, \sysname is moderately differentiated from the baselines.}



\subsection{Case Studies} \label{sec:eval-case-studies}
\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/STACK_Q2-025.pdf}
        \caption{STACK\_Q2-025: \bo finds a better plan than any of the other techniques}
        \label{fig:case-study-best}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/CEB_11A102.pdf}
        \caption{CEB\_11A102: All techniques find good plans, though \bo takes notably longer.}
        \label{fig:case-study-moderate}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/JOB_1B.pdf}
        \caption{JOB\_1B: All techniques converge to the same (very small) runtime.}
        \label{fig:case-study-worst}
    \end{subfigure}
    \caption{Case studies showing optimization time vs. plan performance for individual queries. Lower is better.}
    \label{fig:case-studies}
\end{figure*}

We select three queries from the optimization workloads to illustrate the different optimization outcomes for \sysname. The optimization runs for these three queries are visualized in \Cref{fig:case-studies}. All three plots visualize the runtime of the best plan found so far over the course of an optimization run across all of the optimization techniques. Bao is visualized as a horizontal line showing the runtime of the best plan because it cannot improve after its hint sets are executed. The light blue ``Bayes (latest)'' line illustrates the runtime of the plan run most-recently by BO, hence its constant fluctuation as BO explores the space of possible plans. The $x$ axis in each of these plots captures cumulative execution time on the database and ignores time spent in the rest of the optimization algorithms such as plan proposal and model updates.

In \Cref{fig:case-study-best}, we highlight a case where the advantages of BO are most obvious. In the first hour, BO is exploring the space immediately around its initialization points, executing plans for slightly longer than the best Bao initialization due to uncertainty-based timeouts. Around 1.5 hours into the optimization run, it finds a plan within a trust region that has substantially better performance and rapidly exploits this new information by trying nearby plans. Note that in most of the queries made afterwards, timeouts are lowered to be closer to the new optimal as the BO surrogate model no longer needs to know if plans are much worse than the new optimal. We also note that neither \random nor Balsa manages to find a plan better than the Bao optimal.

In \Cref{fig:case-study-moderate}, all optimization strategies converge to the same best plan runtime after several hours of execution. BO takes notably longer than \random and Balsa to find this plan. That \random finds an optimized plan so quickly suggests that the space of possible plans contains many good plans that perform approximately this well, but they may be quite different from the initialization points given to BO. We also note that despite the fact that we give Balsa training examples including the Bao optimal plan, it begins its search with plans that are considerably worse before eventually passing the Bao optimal.

\Cref{fig:case-study-worst} shows a query for which all techniques converge relatively quickly to the same plan runtime and do not make any progress afterwards. We note that this optimized plan executes in 2ms, which is short enough to be indistinguishable from noise in our experimental setup. We observe that Balsa takes the longest time to arrive at this plan whereas the other techniques find it nearly instantly, which we take as further evidence of the unsuitability of RL-based algorithms for offline optimization. The plateaus in Balsa's progress occur when it is exploiting its best known plan in order to minimize regret instead of trying to find a faster plan.

\subsection{Timeout Ablation Study} \label{sec:eval-timeout-ablation}
\begin{figure}
    \centering
    % \begin{subfigure}[t]{0.48\linewidth}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/job_task1_timeout_ablation.pdf}
        \caption{Optimization time vs. best plan runtime for different timeout schemes when running BO.}
        \label{fig:bo-ablation-timeout}
    \end{subfigure}
    \hfill
    % \begin{subfigure}[t]{0.48\linewidth}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/job_task1_tr_ablation.pdf}
        \caption{Optimization time vs. best plan runtime with and without the local ``Trust Region'' optimization.}
        \label{fig:bo-ablation-trust-region}
    \end{subfigure}
    \caption{Ablation study of our novel BO scheme.}
    \label{fig:bo-ablation}
\end{figure}

To answer \labelcref{rq:bo-ablation}, we justify our usage of a novel timeout strategy, as well as the choice to perform local BO based on trust regions, \footnote{\edit{Which, despite the name, is a \emph{global} optimization scheme: see \Cref{sec:bo-background}.}} via an ablation study visualized in \Cref{fig:bo-ablation}. As described in \Cref{sec:bo-background}, we utilize timeouts in order to manage the impact of executing terrible plans that take many orders of magnitude longer to execute than the optimal plan, wasting optimization budget for little gain.

In \Cref{fig:bo-ablation-timeout}, we show the results of using different timeout schemes when optimizing a single JOB query. Intuitively, using timeouts longer than the runtime of the current best-seen plan (i.e. the 0th percentile timeout) allows the surrogate model to learn more about regions of the space of plans that it is less certain about. Since we use censored observations, two plans (and their surrounding regions of plan space) will look exactly the same if they both timed out after 1 second, even if one plan would have executed for 1.2 seconds and the other for 2 hours. By using longer timeouts, the surrogate model gains greater confidence in whether a particular region of the space is still promising to explore or if it is clearly terrible. As shown in the plot, our uncertainty-based method for determining the timeout threshold results in BO finding faster plans while consuming less optimization budget. In fact, using the best-seen runtime as the timeout causes BO to find the worst plan by the end of optimization, perhaps due to the artificially low timeout uniformly discouraging BO from exploring the space of plans. 

We also justify the choice to use trust region-based local BO instead of global BO by performing another ablation, shown in \Cref{fig:bo-ablation-trust-region}. Our choice to represent the space of plans via the latent space of a VAE comes at the cost of high dimensionality (64). Though it is possible in principle that local BO will miss globally optimal plans, in our experiments we find that local BO initialized with plans derived from the PostgreSQL optimizer can find highly-optimized plans for many queries. In the ablation plots, we can see that even after many hours of optimization, global BO does not catch up to the quality of plans found by local BO due to the exponentially larger space of plans that it must explore.

\subsection{Data Drift After Optimization} \label{sec:eval-drift}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/drift_figure.pdf}
    \caption{Left: Plans from the past vs. plans optimized in the future. Middle: BO run results when using the VAE from the past vs. the VAE retrained in the future. Right: Optimization speed of BO initialized with Bao vs. those including the past plan.}
    \label{fig:drift}
\end{figure*}
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/plan_degradation.pdf}
%     \caption{Percentage difference in best plan runtime found per-query by BO before and after data drift (executed against the post-drift dataset).  \textcolor{blue}{Negative} indicates that the past plan was faster than the best plan found after running BO on the future dataset.}
%     \label{fig:plan-degradation}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/stack_shift_figure.pdf}
    \caption{Left: Performance of plans optimized in the past vs. plans optimized in the future executed on dates in between. Shaded regions show the 25th to 75th percentile runtimes. Right: The top 3 longest plan runtimes per date.}
    \label{fig:drift_over_dates}
\end{figure}

In order to model data drift, we modified the StackOverflow dataset by deleting all rows in all tables with timestamps after 2017, as well as the transitive closure of all rows whose foreign keys became invalidated as a result of deleting those rows. This reduced the overall dataset size by roughly 20\%, with individual tables decreasing in row count between 0\% and 28\%. This deletion effectively restored the database to a snapshot from the end of 2017, while the original StackOverflow dataset snapshot was taken in late 2019. \edit{We present this two year shift as a worst-case scenario for data drift, expecting that if plan performance were to degrade due to data drift, it would degrade more over a longer period of time.} For the rest of this section, we will refer to this 2017 snapshot as the ``past'' and the original 2019 snapshot as the ``future''.

We sought to answer three questions about the impact of data drift:

\begin{enumerate}[label=\textbf{RQ3.\arabic*.}]
    \item How do the past plans perform in comparison to plans produced by reoptimizing from scratch on the future dataset? \label{rq:drift-degradation}
    \item Is it important to retrain the VAE for the future dataset before performing BO? \label{rq:drift-vae}
    \item Does including the past plan as an initialization point in the future BO process speed up optimization? \label{rq:drift-reoptimize}
\end{enumerate}

To answer \labelcref{rq:drift-degradation}, we trained a VAE for the past dataset using the procedure described in \Cref{sec:technique-vae}, planning the sampled queries using PostgreSQL with the past dataset, akin to conducting the full \sysname offline optimization process in the past. We then performed BO against the past dataset using this past version of the VAE and a reduced set of 50 (out of our original 200) queries. We then took the past query plans and executed them against the future dataset, effectively simulating the real-world use case of continuing to use previously optimized query plans well past when data drift may have rendered those plans suboptimal.

The results of executing these past plans against the future dataset are visualized in the left plot of \Cref{fig:drift}. We compare past plans against the Bao optimal plans for the future dataset and the future plans produced by BO from our initial experiment. We also compare them to the results of performing BO for a short time (about 1 hour) initialized with the past plan in addition to the Bao plans, discussed further below. Despite the substantial data drift, past plans perform about as well as if we had performed BO for the first time on the future dataset and continue to perform much better than the best Bao hint. This suggests that the optimality of most of the plans found by BO is not affected much data drift.

\edit{We also executed these past and future plans on dates in between the ``past'' and ``future'' endpoints, shown in \Cref{fig:drift_over_dates}. For the vast majority of queries, there is not a significant difference in runtime between the past and future plans, but as shown by the dramatic increase in runtime of the longest-running past plan, it is indeed possible for data drift to render a plan suboptimal.}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/vae_drift_comparison.pdf}
%     \caption{Percentage change in plan runtime when performing BO using the VAE trained on the past dataset vs. the VAE trained on the future dataset. \textcolor{blue}{Negative} indicates that the BO run using the past VAE found a faster plan.}
%     \label{fig:vae-drift}
% \end{figure}

To answer \labelcref{rq:drift-vae}, we performed a set of BO runs against the future dataset using the VAE trained on the past dataset, simulating the real-world use case of attempting to perform offline optimization in the future without retraining the VAE. The results are visualized in the middle plot of \Cref{fig:drift}. We observe that keeping the VAE up-to-date has a non-negligible effect on the optimality of plans found by BO. Given the small cost of training the VAE compared to the rest of the BO process, it seems worthwhile to periodically retrain the VAE to account for data drift.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/reoptimize_comparison.pdf}
%     \caption{Percentage difference in best plan runtime when reoptimizing using the past plan as an initialization point vs. optimizing from scratch. \textcolor{blue}{Negative} indicates that the run including the past plan found a faster plan.}
%     \label{fig:reoptimization}
% \end{figure}

To answer \labelcref{rq:drift-reoptimize}, we performed a set of BO runs against the future dataset initialized with both the Bao initialization and the optimized past plan, simulating reoptimization when a query had previously been optimized in the past. For these BO runs, we used the VAE trained on the future dataset. The aggregate results in red for reoptimization in the left plot of \Cref{fig:drift} show not only that reoptimization to account for data drift is viable, but that it tends to produce the best plans across the entire workload. In the right side of the same figure, we observe that the BO runs converge to optimized plans much more quickly than a run started from scratch. Reoptimizations ran for an average of 1.5 hours compared to from-scratch optimization runs, which ran for an average of 8.2 hours. This suggests that plans generated by \sysname can be brought up-to-date to account for data drift while consuming much less optimization budget than the initial offline optimization run.


\subsection{Few-Shot LLM from BO Results} \label{sec:eval-llm}
\begin{figure}
    % \centering
    % \begin{subfigure}{0.48\linewidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{figures/within_template_50.pdf}
    %     \caption{LLM Within-template: Percentage difference of best out of 50 plans per-query generated by LLM vs. Bao optimal plan. The LLM was fine-tuned with BO-optimized plans, including queries of the same template as the test set.}
    %     \label{fig:llm-experiments-within-template}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{0.48\linewidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{figures/cross_template_50.pdf}
    %     \caption{LLM Cross-template: Percentage difference of best out of 50 plans per-query generated by LLM vs. Bao optimal plan. The LLM was fine-tuned with BO-optimized plans from the same schema, but excluding queries of the same template as the test set.}
    %     \label{fig:llm-experiments-cross-template}
    % \end{subfigure}
    \includegraphics[width=\linewidth]{figures/llm.pdf}
    \caption{Top: LLM trained on plans of the same template. Bottom: LLM trained on plans not of the same template. Lower (more blue) is better.}
    \label{fig:llm-experiments}
\end{figure}

As a byproduct of performing this experimental evaluation, we generated plans that, to our knowledge, are the best plans that can currently be found for the queries in our evaluation workloads. We hypothesized that a large language model fine-tuned using these high-quality plans could potentially be a better few-shot plan optimizer than existing techniques. Here, we evaluate the LLM's ability to generate initialization points for \sysname and defer offline optimization initialized using the LLM outputs to future work.

In order to answer \labelcref{rq:llm}, we performed two experiments. In the first experiment, visualized in the top plot of \Cref{fig:llm-experiments}, we fine-tuned GPT-4o mini on the fastest 10 optimized plans for each query from running BO on the CEB workload as described in \Cref{sec:initialization_strategies}. This training set included queries from all query templates present in the workload. We then compared the best runtime out of 50 plans (giving it as many plans as Bao) per-query generated by the LLM for a particular query against the runtime of the optimal Bao plan.

In the second experiment, we performed the same process, but withheld BO results from two query templates from the fine-tuning process. We then performed the same test, comparing the best runtime out of 50 plans per-query from the LLM against the optimal Bao plans. The results are visualized in the bottom plot of \Cref{fig:llm-experiments}. The LLM clearly does not perform as well when it has not been fine-tuned with results from the same query template.






\subsection{\sysname Overhead} \label{sec:overhead}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/overheads.pdf}
    \caption{Overheads per iteration of BO. Top: 1x simultaneous BO run. Bottom: 5x simultaneous BO runs.}
    \label{fig:overheads}
\end{figure}

We measured the overhead of running \sysname under multiple conditions in order to better understand its resource requirements and to determine if a GPU is necessary to perform optimization. We recorded time spent in each part of the optimization loop when executing the BO components on both CPU and GPU while varying the number of simultaneous BO runs. The results are visualized in \Cref{fig:overheads}. We find that with only one BO run, while overhead on a CPU is worse than on a GPU, the absolute time spent on overhead (i.e., everything but query execution) is in the single-digit seconds range. For sufficiently long-running queries, this CPU overhead may be tolerable. However, even with just 5 simultaneous runs, \edit{we note that VAE sampling scales significantly better on GPUs than CPUs -- this is attributable to the GPU's hardware support for scaled dot product attention~\cite{dao_flashattention}}.

\subsection{Comparison to LimeQO} \label{sec:limeqo-comparison}
\begin{figure}
    \centering
    % \includegraphics[width=0.9\linewidth]{figures/limeqo_comparison.pdf}
    \includegraphics[width=0.9\linewidth]{figures/limeqo_agg_comparison.pdf}
    % \caption{Optimization using \sysname vs. LimeQO on the three longest-running JOB queries. Lower is better.}
    \caption{Optimization using \sysname vs. LimeQO across the entire JOB. Lower is better. The x-axis is log scale so both final performance and the initial improvement is visible.}
    \label{fig:limeqo-comparison}
\end{figure}

\edit{
LimeQO ~\cite{limeqo} is another work that performs offline optimization for repetitive workloads, but LimeQO and chiefly differs from \sysname in that its potential optimizations are limited to finding optimal hints from a small set, as opposed to \sysname which constructs entire join orders. LimeQO uses the hint sets from Bao~\cite{bao}, which \sysname also uses to initiate the Bayesian optimization process. As shown in \Cref{fig:limeqo-comparison}, both techniques explore all of the Bao hints: once LimeQO has exhausted all of the hints, there are no remaining avenues for further optimization, whereas BayesQO continues exploring and finds better plans.
}