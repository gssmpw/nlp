\section{Related Work}
\label{sec:related}

\paragraph{Author Characteristics and Linguistic Theory}
Linguistic research demonstrates how author characteristics influence writing style.  Functional linguistics and discourse analysis show that registers and genres vary in vocabulary, syntax, and pragmatics \citep{biber1998corpus}, creating distinctions between AI- and human-generated text. Sociolinguistics and Second Language Acquisition (SLA) research reveals textual differences (e.g., lexical diversity, grammatical complexity) between native and non-native speakers \citep{hyland2000disciplinary, lantolf2000sociocultural, long1996role}.  \citet{hyland2000disciplinary} specifically highlights how disciplinary norms in academic writing create stylistic variations.  "World Englishes" theory \citep{kachru1985standards, jenkins2003world} further emphasizes the diverse forms of English usage across cultures.  This linguistic diversity complicates AI text detection, potentially introducing bias if detectors are trained on limited data and fail to generalize across varied social and cultural backgrounds.


\paragraph{AI Text Detection Datasets and Benchmarks}

As LLMs continue to advance in various NLP tasks, the need to distinguish AI-generated text from human-authored text has become a hot research topic. Early work explored supervised learning or feature engineering, but as models scale, attention has shifted to multi-scenario, multi-model, and multilingual detection benchmarks.

Examples include CUDRT, which focuses on polished and post-processed LLM text \citep{tao2024reliabledetectionllmgeneratedtexts}; RAID, a comprehensive benchmark with multiple models, domains, languages, and adversarial samples \citep{dugan2024raidsharedbenchmarkrobust}; MAGE, which systematically combines models and domains for in- and out-of-domain evaluations \citep{Li2024mage}; M4GT-Bench, for multilingual, black-box detection \citep{wang2024m4gtbenchevaluationbenchmarkblackbox}; and HC3, which compares ChatGPT and human expert outputs \citep{guo2023closechatgpthumanexperts}.

Despite this progress in creating diverse detection benchmarks, the human dimension—specifically, the demographic and identity attributes of authors—has received limited attention. The influence of factors like gender, native language, academic discipline, and language proficiency on detection accuracy and potential bias remains largely unexplored.  This study addresses this gap by analyzing how these key author attributes affect AI text detection.
