\section{Detector}
\label{sec:detector}
\subsection{Detector Selection}
Building upon the methodology of \citet{dugan2024raidsharedbenchmarkrobust} (RAID), we evaluate detectors from two categories: \textbf{classifier-based}, \textbf{metric-based}. Classifier-based detectors typically involve fine-tuning a pre-trained language model such as RoBERTa \cite{liu2019roberta}, while metric-based detectors compute a value based on generative model probabilities. 

Specifically, following RAID’s unified pipeline and detector set, we tested the detectors summarized in Table~\ref{tab:detectors}.

\begin{table}[ht]
\centering
\small
\begin{tabular}{l p{0.6\linewidth}}
\toprule
\textbf{Category} & \textbf{Detectors} \\
\midrule
\textbf{Classifier-Based} &
  RoBERTa-Base (GPT2) \cite{solaiman-etal-2019}\\
  & RoBERTa-Large (GPT2) \cite{liu2019roberta}\\
  & RoBERTa-Base (ChatGPT) \cite{guo2023close}\\
  & RADAR \cite{hu2023radar}\\
\midrule
\textbf{Metric-Based} &
  GLTR \cite{gehrmann-etal-2019-gltr} \\
  & Binoculars \cite{hans2024spotting}\\
  & Fast DetectGPT \cite{bao2023fastdetectgpt}\\
  & DetectGPT \cite{mitchell-etal-2023-detectgpt} \\
  & LLMDet \cite{wu-etal-2023-llmdet}\\
\bottomrule
\end{tabular}
\caption{Overview of Detectors Evaluated}
\label{tab:detectors}
\end{table}

In contrast to \citet{Li2024mage}, we adopt RAID’s principle of analyzing off-the-shelf models without further fine-tuning them on our target dataset \citep{dugan2024raidsharedbenchmarkrobust}. This setup directly assesses the generalization capability of these pre-trained detectors. Meanwhile, for the metric-based detectors, we utilize the default generative models within each repository to replicate realistic usage scenarios.\footnote{For additional detector details, please refer to Appendix~\ref{app:detectors}.}

\subsection{Detector Evaluation}
In keeping with RAID’s evaluation paradigm, each detector produces a scalar score for a given sequence, and we transform this score into a binary AI/human judgment by setting a threshold $\tau$. We then calibrate $\tau$ so that the false positive rate (FPR) against human-authored text remains at 5\%. Accuracy at a fixed 5\% FPR thus measures how effectively each detector identifies machine-generated text without unfairly penalizing human writers. This aligns with recent trends in robustness evaluations \citep{hans2024spotting, krishna2023paraphrasing, soto2024fewshot}.\footnote{\citet{dugan2024raidsharedbenchmarkrobust} discusses detailed rationale for choosing FPR-based calibration over traditional precision/recall/F1 metrics.}

Table~\ref{tab:fpr_results} replicates the RAID benchmark’s illustration of false positive rates at naive thresholds. Evidently, failing to calibrate or disclose thresholds can cause prohibitive false positive rates—a risk we aim to mitigate by following RAID’s FPR-based framework.


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|c|c|c|c}
    \toprule
    &$\tau$=0.25&$\tau$=0.5&$\tau$=0.75&$\tau$=0.95\\
    \midrule
    R-B GPT2&8.71\%&6.59\%&5.18\%&3.38\%\\
    R-L GPT2&6.14\%&2.91\%&1.46\%&0.25\%\\
    R-B CGPT&21.6\%&15.8\%&15.1\%&10.4\%\\
    RADAR&7.48\%&3.48\%&2.17\%&1.23\%\\
    \midrule
    GLTR&100\%&99.3\%&21.0\%&0.05\%\\
    F-DetectGPT&47.3\%&23.2\%&13.1\%&1.70\%\\
    LLMDet&97.9\%&96.0\%&92.0\%&75.3\%\\
    Binoculars&0.07\%&0.00\%&0.00\%&0.00\%\\
    \bottomrule
    \end{tabular}
    \caption{False Positive Rates for detectors on RAID at naive choices of threshold ($\tau$). We see that, for open-source detectors, thresholding naively results in unacceptably high false positive rates.}
    \label{tab:fpr_results}
\end{table}

