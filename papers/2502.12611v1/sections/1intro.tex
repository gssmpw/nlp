\section{Introduction}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\textwidth]{figure/pipeline.png}
    \caption{Research workflow: ICNALE data and LLM-generated text with author attributes are used for out-of-domain evaluation of AI text detectors and subsequent bias analysis using multi-factor ANOVA and weighted OLS.}
    \label{fig:workflow}
\end{figure*}

Large Language Models (LLMs) have transformed the way we produce and consume written communication, driving remarkable advancements in text generation across a wide array of domains. From social media content moderation to automated report writing in academia, the breadth and sophistication of these models have brought renewed focus to a pressing challenge: detecting AI-generated text. Although recent efforts have expanded detection benchmarks to include multiple LLMs, multilingual settings, and adversarial perturbations \citep{tao2024reliabledetectionllmgeneratedtexts, dugan2024raidsharedbenchmarkrobust, Li2024mage, wang2024m4gtbenchevaluationbenchmarkblackbox}, the \emph{human dimension} of text production has received far less scrutiny. Traditional detection pipelines predominantly center on model-level and data-centric strategies---e.g., sampling, prompting, and adversarial augmentation---while overlooking social and linguistic factors tied to the authors themselves.

Yet decades of sociolinguistic and applied linguistics research underscore that writing style varies systematically by gender, language proficiency, disciplinary norms, and cultural context \citep{hyland2000disciplinary, lantolf2000sociocultural, long1996role}. Such differences manifest in vocabulary choices, syntactic complexity, and rhetorical conventions, which can inadvertently bias AI text detectors if not carefully addressed. For example, non-native speakers or lower-proficiency writers might be wrongly flagged as AI-generated due to errors or unidiomatic expressions that differ from native-level training data. Conversely, sophisticated second-language learners might produce polished texts that resemble those generated by larger LLMs, thereby complicating detection efforts. In the absence of benchmarks that account for such diversity, existing detectors risk disproportionately penalizing particular demographic or educational groups.

Motivated by this gap, we pose the question: \emph{``Who is writing?''} Specifically, we investigate whether and how author-level attributes---including gender, Common European Framework of Reference (CEFR) proficiency, academic field, and language environment (e.g., native vs.\ EFL/ESL)---systematically influence AI (or AI-generated) text detection outcomes. To this end, we craft a comprehensive end-to-end analysis. First, we source human-authored texts with rich metadata from the ICNALE corpus, a well-established learner corpus that spans multiple Asian regions and diverse proficiency levels \citep{Ishikawa2013icnale}. Next, we generate parallel AI-written texts using a range of modern LLMs (e.g., Qwen, LLaMA, Mistral). We then evaluate cutting-edge off-the-shelf detectors under out-of-domain conditions---a scenario closely mirroring real-world deployments where detectors must generalize beyond their training distributions. Figure~\ref{fig:workflow} offers a concise overview of our end-to-end methodology, culminating in an out-of-domain evaluation that mirrors real-world use cases.

Our findings, drawn from rigorous \emph{t}-tests and ANOVA analyses, show that \textbf{CEFR proficiency level} and \textbf{language environment} exert consistent and sizable effects on detection accuracy across nearly all detectors. By contrast, gender- and academic field-related biases tend to be more detector-dependent, with some models demonstrating marked variability and others remaining largely agnostic. These results suggest a strong need to incorporate demographic and linguistic attributes into AI text detection benchmarks and training protocols. Without doing so, state-of-the-art detectors are liable to unfairly penalize certain writer populations or misclassify legitimate texts.

In summary, our work highlights the \emph{human-centered} challenges that arise when AI text detection collides with real-world diversity. By shifting the focus toward the people behind the text, we aim to catalyze a broader conversation on socially responsible model development, enhanced debiasing strategies, and inclusive evaluation frameworks. We hope that this renewed emphasis will pave the way for fairer, more robust, and ultimately more trustworthy detection systems. 

Our contributions are as follows \footnote{Our code and data will be released to the community to facilitate future research.}: 
\begin{itemize}
\item We introduce a 67K-text dataset that combines human-authored ICNALE essays with parallel AI outputs from 12 modern LLMs, each labeled with detailed persona metadata. This resource enables rigorous, human-centered analysis in AI text detection. 
\item We propose a multi-factor WLS framework with Type~II ANOVA, controlling for multiple attributes to isolate each factor’s influence on detection errors and pinpoint where bias emerges. This approach can also be extended to future bias analysis tasks that consider additional demographic or linguistic variables.
\item We demonstrate that human-level attributes—particularly language proficiency and environment—can overshadow model-based differences, causing significant biases in AI text detection. 
\end{itemize}

