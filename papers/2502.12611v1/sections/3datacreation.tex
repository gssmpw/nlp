\section{Dataset Creation}

\subsection{Overview}
We construct our dataset from 2,569 learners in the International Corpus Network of Asian Learners of English (ICNALE)~\citep{Ishikawa2013icnale}, each providing short essays on two prompts: banning smoking in restaurants and the importance of part-time jobs. This yields 5,138 human-authored texts. ICNALE also includes rich demographic and identity data (e.g., sex, academic genre, proficiency), as shown in Table~\ref{tab:attribute_dist}.

To capture a broader range of writing styles, we pair each human-authored text with parallel responses from 12 modern LLMs. Each model thus produces another 5,138 essays, adding up to 61,656 AI-generated texts. Overall, our corpus contains 66,794 samples. We acquired the ICNALE data under its official license, following all guidelines. The consistent task prompts and detailed metadata enable controlled investigations of how AI models handle varied writer profiles.

Table~\ref{tab:all-datasets} compares our dataset to other publicly available resources of AI-generated text. While many incorporate multiple models, none (to our knowledge) matches our depth of persona annotations. Our corpus thus serves as a valuable benchmark for research on model fairness and sociolinguistic bias. Detailed ICNALE task prompts appear in Appendix~\ref{appendix:task_prompt}.


\begin{table}[ht!]
\centering
\resizebox{\columnwidth}{!}{%
  \small
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Name} & \textbf{Size} & \textbf{MultiModel} & \textbf{Persona}\\
    \midrule
    TuringBench~\cite{uchendu2021turingbench}        & 200k  & \ding{51} & \ding{55}\\
    RuATD~\cite{Shamardina_2022}                     & 215k  & \ding{51} & \ding{55}\\
    HC3~\cite{guo2023close}                          & 26.9k & \ding{55} & \ding{55}\\
    MGTBench~\cite{he2023mgtbench}                   & 2817  & \ding{51} & \ding{55}\\
    MULTITuDE~\cite{macko-etal-2023-multitude}       & 74.1k & \ding{51} & \ding{55}\\
    AuText2023~\cite{sarvazyan2023overview}          & 160k  & \ding{55} & \ding{55}\\
    M4~\cite{wang2023m4}                             & 122k  & \ding{51} & \ding{55}\\
    CCD~\cite{wang2023evaluating}                    & 467k  & \ding{55} & \ding{55}\\
    IMDGSP~\cite{mosca-etal-2023-distinguishing}     & 29k   & \ding{51} & \ding{55}\\
    HC-Var~\cite{xu2023generalization}               & 145k  & \ding{55} & \ding{55}\\
    HC3 Plus~\cite{su2024hc3}                        & 210k  & \ding{55} & \ding{55}\\
    MAGE~\cite{Li2024mage}                           & 447k  & \ding{51} & \ding{55}\\
    RAID~\cite{dugan2024raidsharedbenchmarkrobust}   & 6.2M  & \ding{51} & \ding{55}\\
    \midrule
    \textbf{Ours}                                    & 67k   & \ding{51} & \ding{51}\\
    \bottomrule
  \end{tabular}
}
\caption{Comparison of publicly available AI-generated text datasets. \emph{MultiModel} indicates whether each benchmark includes texts from multiple LLMs (\ding{51}) or only one (\ding{55}). \emph{Persona} indicates whether the dataset provides demographic or identity metadata.}
\label{tab:all-datasets}
\end{table}



\subsection{Author Attribute Construction}
In selecting author attributes, we consider both data feasibility and theoretical grounding. Practically, we choose attributes with minimal missing values, relatively balanced distributions, and manageable category sizes. Theoretically, we prioritize attributes that are well-studied in sociolinguistics, second-language acquisition (SLA), and academic discourse, so they can reveal deeper insights into potential biases. Accordingly, we focus on four key dimensions:


\paragraph{Gender (Sex).}
As indicated by \citet{cameron2005language,gal2012language}, gender is shaped by societal and cultural constructions, potentially giving rise to “gendered” linguistic features. Table~\ref{tab:attribute_dist} shows that 1,430 learners self-identified as female (\emph{F}) and 1,139 as male (\emph{M}), for a total of 2,569.

\paragraph{Academic Field (Acad.\ Genre).}
Rooted in genre theory \citep{swales2014genre} and Bourdieu’s concept of academic capital \citep{bourdieu1991language}, different disciplines (e.g., Humanities, Social Sciences) may exhibit distinct rhetorical styles \citep{hyland2000disciplinary}. Our dataset includes four such academic fields, with Sciences \& Technology (1,034) being the largest group (Table~\ref{tab:attribute_dist}).

\paragraph{CEFR Proficiency Level (CEFR).}
The Common European Framework of Reference for Languages \citep{council2001common} provides a graded scale for language proficiency (A2, B1, B2, etc.), widely used in SLA research \citep{krashen2006input,long1996role,lantolf2000sociocultural}. As shown in Table~\ref{tab:attribute_dist}, the most common levels in our dataset are B1\_1 (914) and B1\_2 (881), with smaller counts at A2\_0 (470), B2\_0 (231), and XX\_0 (73; native speakers).

\paragraph{Language Environment (NS/EFL/ESL).}
Following Kachru’s notion of “World Englishes” \citep{kachru1985standards} and Jenkins’ work on English as a lingua franca \citep{jenkins2003world}, we distinguish between native speakers (NS, 73 learners), EFL (English as a Foreign Language, 1,886 learners), and ESL (English as a Second Language, 610 learners). Such differences in exposure and context can shape writing styles that AI text detectors may treat unevenly.

\begin{table}[t]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Attribute} & \textbf{Value} & \textbf{Count} \\
\midrule
\multirow{5}{*}{CEFR}   
 & B1\_1 & 914 \\
 & B1\_2 & 881 \\
 & A2\_0 & 470 \\
 & B2\_0 & 231 \\
 & XX\_0 & 73 \\
\midrule
\multirow{4}{*}{Acad.\ Genre}  
 & Sciences \& Tech. & 1,034 \\
 & Social Sciences   & 762 \\
 & Humanities        & 674 \\
 & Life Sciences     & 99 \\
\midrule
\multirow{3}{*}{Lang.\ Env.}  
 & EFL & 1,886 \\
 & ESL & 610 \\
 & NS  & 73 \\
\midrule
\multirow{2}{*}{Sex}
 & F   & 1,430 \\
 & M   & 1,139 \\
\bottomrule
\end{tabular}
\caption{Distribution of author attributes across 2,569 learners.}
\label{tab:attribute_dist}
\end{table}







These four attributes are both available at scale (with minimal missing data) and theoretically grounded in prior linguistic research. By focusing on them, we can systematically examine how AI detectors handle diverse writer profiles and detect potential biases. Table~\ref{tab:attribute_dist} shows the distribution of each category in our dataset.


\paragraph{Definition of Subgroup.}  For our analysis, we define a subgroup based on the values of a categorical author attribute.  Let \(A\) represent a categorical attribute (e.g., CEFR level, Language Environment, Academic Genre, Sex).  This attribute \(A\) can take on a set of  \(k\) possible values, denoted as  \(\{\mathcal{A}_1, \ldots, \mathcal{A}_k\}\).  Each \(\mathcal{A}_i\) (where \(i \in \{1, 2, \ldots, k\}\)) represents a specific value of the attribute \(A\) and defines a distinct subgroup. For example, if \(A\) is CEFR level, then \(\mathcal{A}_1\) might be A2\_0, \(\mathcal{A}_2\) might be B1\_1, and so on.  All texts associated with a particular attribute value \(\mathcal{A}_i\) constitute that subgroup.

\subsection{Generators}

We employ a diverse set of Large Language Models (LLMs) to generate texts, spanning different architectures, training sets, and parameter sizes (0.5B–72B). Our lineup includes the Qwen2.5 family \citep{qwen2,qwen2.5} from Alibaba DAMO Academy, recognized for strong instruction-following performance and extended-context capabilities; the LLaMA3.1 and LLaMA3.2 models \citep{dubey2024llama3herdmodels}, noted for their open-source availability and efficient pre-training; and the compact yet high-performing Mistral \citep{jiang2023mistral7b}.

Table~\ref{tab:LLMs} lists the specific model versions used. Each model was prompted with carefully designed instructions to emulate learners with diverse gender, academic fields, CEFR proficiency levels, and language environments (see Appendix~\ref{sec:prompt-content}). By covering a wide range of parameter scales and training paradigms, our approach seeks to capture heterogeneous AI-generated outputs and enable us to evaluate detection robustness across various author attributes.




\begin{table}[t]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{LLM Name} & \textbf{Series} & \textbf{Params} \\
\midrule
Qwen2.5-0.5B-Instruct & Qwen2.5 & 0.5B \\
Qwen2.5-1.5B-Instruct & Qwen2.5 & 1.5B \\
Qwen2.5-3B-Instruct   & Qwen2.5 & 3B \\
Qwen2.5-7B-Instruct   & Qwen2.5 & 7B \\
Qwen2.5-14B-Instruct  & Qwen2.5 & 14B \\
Qwen2.5-32B-Instruct  & Qwen2.5 & 32B \\
Qwen2.5-72B-Instruct  & Qwen2.5 & 72B \\
\midrule
Llama3.1-8b-instruct  & Llama3.1 & 8B \\
Llama3.1-70b-instruct & Llama3.1 & 70B \\
\midrule
Llama3.2-1b-instruct  & Llama3.2 & 1B \\
Llama3.2-3b-instruct  & Llama3.2 & 3B \\
\midrule
Mistral-Small-Instruct-2409 & Mistral & 22B \\
\bottomrule
\end{tabular}
\caption{List of employed LLMs}
\label{tab:LLMs}
\end{table}

