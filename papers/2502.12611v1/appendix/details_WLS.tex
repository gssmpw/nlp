% \clearpage
\section{Details of multi-factor WLS}
\label{appendix:hyp_testing_wls}

To rigorously assess potential bias in each detector’s predictions across various author attributes, we adopt a \textbf{multi-factor (multivariate) analysis} with a significance threshold of \(\alpha = 0.05\). This reveals whether a focal attribute (e.g., \emph{gender}) retains an effect on detection outcomes once we \emph{control} for other variables that might jointly influence performance (e.g., \emph{CEFR level}, \emph{language environment}).

\paragraph{Weighted Least Square for Detector Accuracy.}
In our implementation, each row in the dataset often represents an aggregated outcome (e.g., mean accuracy) across several underlying samples. Let \(w_i\) be the total sample size for the \(i\)-th aggregated row, and let \(\text{accuracy}_i \in [0,1]\) be the observed detection accuracy for that group. We fit a WLS model
\begin{equation*}
\label{eq:weighted-ols}
  \min_{\boldsymbol{\beta}}
  \sum_{i=1}^{n}
  w_i \;(\text{accuracy}_i \;-\; \beta_0 \;-\; \sum_{k=1}^p \beta_k x_{ik})^{2}.
\end{equation*}
In Python \texttt{statsmodels}, this is accomplished by specifying \texttt{weights=}\(w_i\) in a model such as
\[
  \texttt{accuracy} \;\sim\; C(\texttt{gender}) \;+\; C(\texttt{CEFR}) \;+\; \dots
\]
where each predictor is treated as a categorical variable \(C(\cdot)\). If we let \(\hat{y}_i\) be the model’s prediction, then the weighted sum of squares is
\[
  \text{WSSE} 
  \;=\;
  \sum_{i=1}^n 
    w_i \;\bigl(\text{accuracy}_i - \hat{y}_i\bigr)^2.
\]
This penalizes errors in proportion to the sample size \(w_i\).

\paragraph{Multi-Factor Type~II ANOVA.}
After estimating \(\boldsymbol{\beta}\), we evaluate each attribute’s \emph{unique} contribution via \textbf{Type~II ANOVA}. Concretely, for each predictor (e.g., \emph{gender}), we compare:
\[
\begin{aligned}
  \text{RSS}_{\mathrm{reduced}} &\;(\text{model without gender}) \\
  &\quad \text{vs.} \\
  \text{RSS}_{\mathrm{full}} &\;(\text{model with gender}),
\end{aligned}
\]
where \(\text{RSS}\) is the \emph{weighted} residual sum of squares. The partial \(F\)-test for that attribute is
\begin{equation*}
\label{eq:partialF}
F 
\;=\;
\frac{\bigl(\text{RSS}_{\mathrm{reduced}} - \text{RSS}_{\mathrm{full}}\bigr)/\Delta p}
     {\;\text{RSS}_{\mathrm{full}} / \bigl(n - p_{\mathrm{full}}\bigr)},
\end{equation*}
where \(\Delta p\) is the difference in number of parameters between the two models, and \(p_{\mathrm{full}}\) is the total parameter count. If the resulting \(p\)-value falls below \(\alpha\), we conclude that \emph{gender} explains additional variance not captured by the other attributes.

\paragraph{Partial \(R^2\).}
As an effect-size measure, we may compute a \emph{partial \(R^2\)} for each predictor:
\begin{align*}
R^2_{\mathrm{partial}}
&= \frac{\text{RSS}_{\mathrm{reduced}} - \text{RSS}_{\mathrm{full}}}
         {\text{RSS}_{\mathrm{reduced}}} \\
&= 1 - \frac{\text{RSS}_{\mathrm{full}}}{\text{RSS}_{\mathrm{reduced}}}.
\end{align*}
This indicates what fraction of the previously unexplained variation is accounted for by reintroducing the focal attribute.

\paragraph{Pseudo-Code Sketch for Multi-Factor Analysis}
Algorithm~\ref{alg:wls-anova} provides a pseudocode flow that mirrors our Python \texttt{statsmodels} procedure for fitting a WLS model with multiple categorical predictors and performing \textbf{Type~II ANOVA} to assess each predictor’s significance.


By controlling for multiple attributes at once, our multi-factor framework helps isolate each attribute’s \emph{direct} influence on detection accuracy, thereby mitigating spurious correlations and confounding. This process is implemented in Python using \texttt{statsmodels} (for WLS and Type~II ANOVA) alongside supporting libraries for data preparation and hypothesis testing.

\paragraph{Partitioning the Variance}
\label{sec:variance-partition}

When we treat accuracy as a continuous outcome, we can write the total sum of squares (TSS) as
\[
  \text{TSS} 
  \;=\;
  \sum_{i=1}^n 
    w_i \,\Bigl(\text{accuracy}_i - \overline{\text{accuracy}}\Bigr)^{2},
\]
where \(\overline{\text{accuracy}}\) is the weighted grand mean. Fitting the model in Equation~\ref{eq:weighted-ols} yields predictions \(\hat{y}_i\), letting us define
\[
  \text{WSR} 
  = \sum_{i=1}^n w_i 
  \bigl(\hat{y}_i - \overline{\text{accuracy}}\bigr)^2.
\]
\noindent
\textit{Here, WSR is the “weighted sum of regression”.}
and
\[
  \text{WSSE}
  \;=\;
  \sum_{i=1}^n w_i
   \Bigl(\text{accuracy}_i - \hat{y}_i\Bigr)^2.
\]
Hence, \(\text{TSS} = \text{WSR} + \text{WSSE}\). In a multi-factor model, the partial \(F\)-tests correspond to whether \(\text{WSSE}\) drops sufficiently when an attribute is included.









\paragraph{Implementation.}
We implement all these procedures in Python. We leverage \texttt{statsmodels} for WLS fits and Type~II ANOVA, \texttt{scipy.stats} for complementary \emph{t}-tests and distribution checks, and custom matching/down-sampling routines to handle partial confounds. Part~\ref{sec:variance-partition} details how variance in accuracy is decomposed into regression (WSR) and residual (WSSE) sums of squares, while Tables~\ref{tab:bias_table} present the resulting \(p\)-values and significance decisions at \(\alpha=0.05\). Through this process, we can identify which author factors (e.g., \emph{gender}, \emph{CEFR}, \emph{environment}) exhibit genuine biases within the AI text detection pipeline, and which factors do not remain significant once confounds are accounted for.


\begin{algorithm}[!h]
\caption{Multi-Factor WLS + Type II ANOVA}
\label{alg:wls-anova}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
    \REQUIRE Dataset of \(n\) texts, each with:
             \begin{itemize}
                 \item A binary classification \(y_i \in \{0, 1\}\) (0: human, 1: AI).
                 \item An attribute vector \(\mathbf{x}_i\) (e.g., CEFR=B1, Sex=F, ...).
             \end{itemize}
             Significance level \(\alpha\) (e.g., 0.05).
    \ENSURE ANOVA table (F-statistics, \(p\)-values) and LSMeans.

    \STATE \textbf{Data Preparation:}
        \begin{itemize}
            \item Group texts by unique attribute combinations (\(\mathbf{x}_i\)).
            \item For each group \(j\), compute the mean accuracy \(a_j\) and weight \(w_j\) (number of texts in group).
            \item Treat each attribute as a factor.
        \end{itemize}
    \STATE \textbf{Fit WLS Model:} Estimate parameters \(\boldsymbol{\beta}\) by minimizing the weighted sum of squared errors:
    \[
    \min_{\boldsymbol{\beta}} \sum_{j=1}^g w_j\,(a_j - \mathbf{x}_j^T \boldsymbol{\beta})^2,
    \]
    where \(g\) is the number of groups, and \(\mathbf{x}_j\) is a representative attribute vector for group \(j\).

    \STATE \textbf{Perform Type II ANOVA:} For each attribute:
        \begin{itemize}[leftmargin=1.5em]
        \item Construct a reduced model by removing the attribute.
        \item Refit the WLS model to the reduced model, obtaining \(\text{RSS}_{\text{reduced}}\).
        \item Compute the \(F\)-statistic:
        \[F = \frac{(\text{RSS}_{\text{reduced}} - \text{RSS}_{\text{full}}) / \Delta p}{\text{RSS}_{\text{full}} / (n - p_{\text{full}})},\]
         where \(\Delta p\) is the difference in the number of parameters between the full and reduced models, and \(p_{\text{full}}\) is the number of parameters in the full model.
        \item Calculate the \(p\)-value from the \(F\)-distribution.
        \end{itemize}

    \STATE \textbf{Calculate LSMeans:} Compute LSMeans for each attribute level using the full WLS model.
    \STATE \RETURN ANOVA table and LSMeans.
\end{algorithmic}
\end{algorithm}



