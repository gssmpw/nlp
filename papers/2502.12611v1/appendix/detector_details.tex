% \clearpage
\section{Detector Details}
\label{app:detectors}

\paragraph{RoBERTa (GPT-2)} \cite{solaiman-etal-2019} This detector is a RoBERTa model \cite{liu2019roberta} fine-tuned on outputs from GPT-2.  The training dataset consists of GPT-2 generations from various decoding strategies (greedy, top-k=50, and random sampling) in an open-domain setting.  This detector has been a long-standing baseline in the field. We use both the base and large versions, obtained from OpenAI\footnote{\url{https://openaipublic.azureedge.net/gpt-2/detector-models/v1/detector-large.pt}}.

\paragraph{RoBERTa (ChatGPT)} \cite{guo2023close} This RoBERTa-base \cite{liu2019roberta} detector is fine-tuned on the HC3 dataset, which contains approximately 27,000 question-answer pairs, with answers generated by both humans and ChatGPT.  The questions span diverse domains, including Reddit, medicine, finance, and law.  We access the detector via HuggingFace Datasets: \texttt{Hello-SimpleAI/chatgpt-detector-roberta}.

\paragraph{RADAR} \cite{hu2023radar} This detector is a fine-tuned Vicuna 7B model (itself a fine-tune of LLaMA 7B), trained in a generative adversarial setting.  The training involved a paraphraser model designed to fool the detector, and the detector was trained to distinguish between paraphraser outputs, human-written text from the WebText dataset, and generations from the original language model. We access RADAR via HuggingFace: \texttt{TrustSafeAI/RADAR-Vicuna-7B}.

\paragraph{GLTR} \cite{gehrmann-etal-2019-gltr} Originally designed as an interactive tool to aid human detection of generated text, GLTR has become a standard baseline in detector robustness evaluations.  It analyzes the likelihood of text under a language model, binning tokens based on their predicted probabilities. These bins then serve as features for detection. We use the default GLTR settings\footnote{\url{https://github.com/HendrikStrobelt/detecting-fake-text}}: a cutoff rank of 10 and GPT-2 small as the language model.

\paragraph{DetectGPT} \cite{mitchell-etal-2023-detectgpt} This zero-shot detector leverages the observation that LM-generated text often resides in regions of negative curvature within the model's log probability function. DetectGPT compares the log probability of an input text, computed by the target LM, to the average log probability of slightly perturbed versions of the text (generated using a separate masked language model like T5). A significant drop in log probability for the perturbed text indicates a higher likelihood of machine generation.

\paragraph{FastDetectGPT} \cite{bao2023fastdetectgpt} This detector is an optimized version of DetectGPT \cite{mitchell-etal-2023-detectgpt}, achieving a 340x speedup without sacrificing accuracy.  Following the original implementation, we use GPT-Neo-2.7B as the scoring model and GPT-J-6B as the reference model for generating perturbations.  Neither of these models was used to generate the continuations in our dataset.


\paragraph{FastDetectLLM}  Referenced as \texttt{fastdetectllm} in the RAID benchmark code \citep{dugan2024raidsharedbenchmarkrobust}, this zero-shot detector is conceptually related to FastDetectGPT \cite{mitchell-etal-2023-detectgpt}.  FastDetectLLM directly uses the average log-rank of input tokens, predicted by a scoring language model (default: GPT-Neo-2.7B), as its detection metric. Lower average log-ranks suggest a higher likelihood of machine generation. This approach bypasses FastDetectGPT's perturbation and sampling steps, significantly improving speed.


\paragraph{Binoculars} \cite{hans2024spotting} This detector uses the ratio of perplexity to cross-entropy between two similar language models as its detection metric. We use the official code and default models (Falcon 7B and Falcon 7B Instruct \cite{almazrouei2023falcon}), which, like FastDetectGPT, were not used for text generation in our dataset.

\paragraph{LLMDet} \cite{wu-etal-2023-llmdet} This detector uses the "proxy-perplexity" from 10 small language models as features. Proxy-perplexity approximates true perplexity by sampling n-grams, avoiding full model execution. None of the models used for proxy-perplexity calculation were involved in generating text for our dataset.