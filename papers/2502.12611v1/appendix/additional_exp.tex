\section{Additional Experiments}
In this section, we treat AI text detection as a binary classification problem and conduct additional experiments using classical machine learning methods to explore potential biases.

\subsection{Training Data}
We adopt the HC3 dataset \citep{guo2023closechatgpthumanexperts} as our training corpus. HC3 contains 24.3k prompts, each accompanied by both human and ChatGPT responses, spanning diverse domains (e.g., Reddit Q\&A, medical, finance, law). Widely used in existing research, HC3 provides a representative overview of differences between LLM-generated and human-authored text.

\subsection{Feature Engineering}
We extract a range of textual features to train the classical models. Table~\ref{tab:features_explanations} summarizes the categories and specific features, including basic lexical counts, syntactic information, and readability metrics.

\begin{table*}[h]
\small
\centering
\begin{tabular}{c|c|c}
\toprule
\textbf{Category} & \textbf{Feature} & \textbf{Explanation} \\
\midrule
\multirow{4}{*}{Basic}
 & Word Count         & Total words in a text \\
 & Unique Words       & Number of distinct words \\
 & Character Count    & Total characters \\
 & Sentence Count     & Number of sentences \\
\midrule
\multirow{2}{*}{Lexical}
 & Avg Word Length    & (Characters minus special characters) / total words \\
 & Type-Token Ratio   & Distinct words / total words \\
\midrule
Syntactic / POS
 & N/V/Adj Ratio      & Proportion of nouns, verbs, adjectives \\
\midrule
\multirow{2}{*}{Readability}
 & Flesch Reading Ease & Formula-based readability score \\
 & Gunning Fog Index   & Index using complex words and sentence length \\
\bottomrule
\end{tabular}
\caption{Features and their explanations.}
\label{tab:features_explanations}
\end{table*}

\subsection{Machine Learning Detectors}
To complement neural-based approaches, we train a suite of classical machine learning models (Table~\ref{tab:classical_ml_models}) on the extracted features. By evaluating their performance, we aim to assess whether biases might arise from specific model families or feature sets.

\begin{table*}[htbp]
\centering
\small
\resizebox{1.7\columnwidth}{!}{%
\begin{tabular}{lp{7cm}}
\toprule
\textbf{Category} & \textbf{Models} \\
\midrule
\textbf{Linear Models} 
& Logistic Regression, Ridge Classifier, Perceptron, Passive Aggressive Classifier \\
\textbf{Support Vector Machines (SVM)}
& SVC (Support Vector Classifier), NuSVC \\
\textbf{Naive Bayes} 
& Multinomial NB, Bernoulli NB, Gaussian NB, Complement NB \\
\textbf{Neighbor-Based Methods}
& KNN (K-Nearest Neighbors), Nearest Centroid \\
\textbf{Discriminant Analysis}
& LDA (Linear Discriminant Analysis), QDA (Quadratic Discriminant Analysis) \\
\textbf{Tree-Based Models}
& Decision Tree, Random Forest, Extra Tree \\
\textbf{Boosting}
& Gradient Boosting, XGBoost, AdaBoost \\
\textbf{Ensemble Learning}
& Bagging, Voting Classifier, Stacking Classifier \\
\textbf{Neural Networks}
& MLP (Multi-Layer Perceptron) \\
\textbf{Stochastic Methods}
& Stochastic Gradient Descent (SGD) Classifier \\
\bottomrule
\end{tabular}%
}
\caption{Summary of machine learning models used.}
\label{tab:classical_ml_models}
\end{table*}




\subsection{Evaluation Metrics}

We treat AI text detection as a binary classification task and use \textbf{Accuracy} as the primary metric:

\begin{equation}
\mathrm{Accuracy}=\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{TP}+\mathrm{TN}+\mathrm{FP}+\mathrm{FN}}
\end{equation}

\noindent where TP is true positive (\emph{AI text correctly identified}), TN is true negative (\emph{human text correctly identified}), FP is false positive (\emph{human text misidentified as AI}), and FN is false negative (\emph{AI text misidentified as human}).

We compare results under two distinct scenarios:
\paragraph{In-domain Testing (HC3-based).} The testing data share the same distribution as the training set. 
\paragraph{Out-of-domain Testing (ICNALE + LLM).} The testing data come from a different distribution (ICNALE plus newly generated LLM texts).

This setup clarifies how well the detectors \emph{generalize} to previously unseen text distributions.


\subsection{Results}

\paragraph{In-domain Results.} Figure~\ref{fig:in_domain} shows the in-domain performance on HC3. Most detectors achieve accuracy scores above 0.90, with some models approaching 0.99. This indicates that, when the test data distribution is similar to training, current deep learning detectors can effectively distinguish AI from human texts. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figure/in_domain.png}
    \caption{In-domain test results (HC3). Accuracy on the y-axis.}
    \label{fig:in_domain}
\end{figure}

\paragraph{Out-of-domain Results.} 
Figure~\ref{fig:out_domain} presents the out-of-domain results using our ICNALE + LLM dataset. Performance drops significantly compared to in-domain, with accuracy scores generally in the 0.50--0.65 range, and some models falling as low as 0.40. This underlines the limited capacity of detectors to transfer knowledge when encountering distributions different from their training data. Such a gap highlights the need for more diverse training corpora and advanced adaptation or domain-transfer techniques.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figure/out_domain.png}
    \caption{Out-of-domain test results (ICNALE + LLM). Accuracy on the y-axis.}
    \label{fig:out_domain}
\end{figure}

In sum, while machine learning detectors show encouraging performance on in-distribution data, they often struggle to maintain high accuracy once faced with new domains or author attributes. Real-world AI text detection frequently involves such diverse and shifting distributions, underscoring the importance of robust generalization strategies.

\subsection{Bias Analysis}
\paragraph{Hypothesis Test Setting}
In order to investigate whether different features exhibit significant differences in the performance of various machine learning detectors, this section adopts appropriate statistical tests based on the number of possible values of each feature. Specifically, for features that have only two values (e.g., gender), we use the independent samples t-test; for features that contain more than two values (e.g., CEFR, academic writing style, language environment), we conduct ANOVA to evaluate whether there are significant differences across different feature levels.

\paragraph{Hypothesis Test Results}
Figure~\ref{fig:hypothesis_test_results} shows the t-test and ANOVA results for the four main features (gender, CEFR level, academic genre, and language environment). Each row corresponds to a different model, and each column corresponds to one of the four features. The results are indicated by ``1'' for significant (p < 0.05) and ``0'' for non-significant (p \(\ge\) 0.05). From these results, we can draw the following conclusions:

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figure/bias_analysis.png}
    \caption{Hypothesis test results (t-test and ANOVA)}
    \label{fig:hypothesis_test_results}
\end{figure}

\paragraph{Gender (Sex, T-test)} 
For the binary feature of gender (Female vs.\ Male), most detectors (e.g., KNeighborsClassifier, SVC, RandomForestClassifier, etc.) exhibit significant differences (p < 0.05), indicating that gender-based distinctions have statistical significance in these detectors’ predictive performance. However, a few detectors such as LogisticRegression, XGBClassifier, and NearestCentroid do not show significant differences, suggesting that for these detectors, gender has a weaker or non-significant effect on the detection results.

\paragraph{CEFR Level (cefr, ANOVA)} 
All detectors display significant differences in the ANOVA results for the multi-class feature of CEFR level, indicating that the graded language proficiency (ranging from beginner to advanced) exerts distinct and statistically significant effects on the detectors’ predictions. This further demonstrates that CEFR levels possess strong predictive discriminative power and significantly influence the outcomes of various detectors.

\paragraph{Academic Genre (academic\_genre, ANOVA)} 
For academic genre, some detectors (e.g., XGBClassifier, RandomForestClassifier, AdaBoostClassifier, etc.) exhibit significant differences in the ANOVA, whereas others (e.g., LogisticRegression, SVC, DecisionTreeClassifier, etc.) do not. This finding suggests that different detectors vary in their sensitivity to academic writing styles; some are better at capturing and leveraging stylistic differences, while others do not reflect clear statistical differences in their predictions.

\paragraph{Language Environment (language\_env, ANOVA)} 
With respect to language environment, the ANOVA results show that the vast majority of detectors exhibit significant differences, indicating that different language environments (e.g., native vs.\ second language settings) have a substantial impact on the predictive outcomes of these detectors. However, certain detectors (e.g., SVC, DecisionTreeClassifier, GradientBoostingClassifier, etc.) do not reach the threshold of statistical significance on this feature, suggesting that their ability to differentiate language environments is insufficient to yield statistically significant results.

Taken together, the analysis reveals that detectors vary in their sensitivity and discriminative power with regard to gender, CEFR level, academic writing style, and language environment. Most detectors demonstrate high discriminative power for CEFR level and language environment, while sensitivity to gender and academic writing style depends on the specific detector. These findings provide a statistical perspective on the influence of various features in AI-based text detection and offer valuable insights for optimizing both detectors and feature selection in future research.


\begin{table*}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Detector} & \textbf{t-value} & \textbf{p-value} & \textbf{Significance (p<0.05)} \\
\midrule
KNeighborsClassifier            & -3.8818 & 1.0812e-04 & Significant \\
LogisticRegression              & -0.2408 & 8.0976e-01 & Not Significant \\
XGBClassifier                   & -1.4438 & 1.4900e-01 & Not Significant \\
SVC                             & -3.9214 & 9.1954e-05 & Significant \\
LinearSVC                       & -2.8893 & 3.9144e-03 & Significant \\
DecisionTreeClassifier          & -3.1055 & 1.9351e-03 & Significant \\
RandomForestClassifier          & -4.8084 & 1.6770e-06 & Significant \\
GradientBoostingClassifier      & -3.8380 & 1.2898e-04 & Significant \\
AdaBoostClassifier              & -4.2847 & 1.9443e-05 & Significant \\
BaggingClassifier               & -3.5719 & 3.6564e-04 & Significant \\
ExtraTreesClassifier            & -5.5451 & 3.4535e-08 & Significant \\
NearestCentroid                 & -1.3941 & 1.6347e-01 & Not Significant \\
RidgeClassifier                 & -0.7179 & 4.7293e-01 & Not Significant \\
SGDClassifier                   & -0.5139 & 6.0741e-01 & Not Significant \\
Perceptron                      & -1.3458 & 1.7856e-01 & Not Significant \\
PassiveAggressiveClassifier     & -3.2496 & 1.2048e-03 & Significant \\
MLPClassifier                   & -3.5214 & 4.4255e-04 & Significant \\
LinearDiscriminantAnalysis      & -1.0359 & 3.0043e-01 & Not Significant \\
QuadraticDiscriminantAnalysis   & -5.1645 & 2.7226e-07 & Significant \\
BernoulliNB                     & -4.2733 & 2.0588e-05 & Significant \\
GaussianNB                      & -3.3843 & 7.3276e-04 & Significant \\
ExtraTreeClassifier             & -5.4473 & 5.9765e-08 & Significant \\
VotingClassifier                & -3.4404 & 5.9649e-04 & Significant \\
\bottomrule
\end{tabular}
\caption{T-test results for Feature 1: Sex}
\end{table*}

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Detector} & \textbf{F-value} & \textbf{p-value} & \textbf{Significance (p<0.05)} \\
\midrule
KNeighborsClassifier           & 9.7256   & 8.9973e-08   & Significant \\
LogisticRegression             & 10.2101  & 3.6569e-08   & Significant \\
XGBClassifier                  & 14.5891  & 1.0422e-11   & Significant \\
SVC                            & 3.3956   & 8.9432e-03   & Significant \\
LinearSVC                      & 11.2302  & 5.4780e-09   & Significant \\
DecisionTreeClassifier         & 9.4865   & 1.4025e-07   & Significant \\
RandomForestClassifier         & 47.3126  & 1.2241e-37   & Significant \\
GradientBoostingClassifier     & 9.9970   & 5.4343e-08   & Significant \\
AdaBoostClassifier             & 15.2355  & 3.1203e-12   & Significant \\
BaggingClassifier              & 9.3998   & 1.6475e-07   & Significant \\
ExtraTreesClassifier           & 51.9283  & 3.5969e-41   & Significant \\
NearestCentroid                & 3.0469   & 1.6277e-02   & Significant \\
RidgeClassifier                & 37.2302  & 8.2609e-30   & Significant \\
SGDClassifier                  & 66.5123  & 4.0857e-52   & Significant \\
Perceptron                     & 45.3916  & 3.6934e-36   & Significant \\
PassiveAggressiveClassifier    & 3.0908   & 1.5361e-02   & Significant \\
MLPClassifier                  & 20.9005  & 8.1949e-17   & Significant \\
LinearDiscriminantAnalysis     & 36.4892  & 3.1519e-29   & Significant \\
QuadraticDiscriminantAnalysis  & 39.3666  & 1.7583e-31   & Significant \\
BernoulliNB                    & 98.4916  & 5.1139e-75   & Significant \\
GaussianNB                     & 45.5083  & 3.0021e-36   & Significant \\
ExtraTreeClassifier            & 8.2371   & 1.4190e-06   & Significant \\
VotingClassifier               & 26.9124  & 1.2214e-21   & Significant \\
\bottomrule
\end{tabular}
\caption{ANOVA results for Feature 2: CEFR}
\end{table*}

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Detector} & \textbf{F-value} & \textbf{p-value} & \textbf{Significance (p<0.05)} \\
\midrule
LogisticRegression             & 0.9484  & 4.1638e-01   & Not Significant \\
XGBClassifier                  & 3.8680  & 9.0291e-03   & Significant \\
SVC                            & 2.3311  & 7.2526e-02   & Not Significant \\
LinearSVC                      & 8.5848  & 1.1803e-05   & Significant \\
DecisionTreeClassifier         & 2.1278  & 9.4821e-02   & Not Significant \\
RandomForestClassifier         & 5.2634  & 1.2955e-03   & Significant \\
GradientBoostingClassifier     & 2.3275  & 7.2877e-02   & Not Significant \\
AdaBoostClassifier             & 3.7601  & 1.0475e-02   & Significant \\
BaggingClassifier              & 7.3502  & 6.8191e-05   & Significant \\
ExtraTreesClassifier           & 1.6179  & 1.8329e-01   & Not Significant \\
KNeighborsClassifier           & 8.5072  & 1.3181e-05   & Significant \\
NearestCentroid                & 0.2623  & 8.5258e-01   & Not Significant \\
RidgeClassifier                & 9.0998  & 5.6698e-06   & Significant \\
SGDClassifier                  & 5.1594  & 1.4985e-03   & Significant \\
Perceptron                     & 7.5843  & 4.8926e-05   & Significant \\
PassiveAggressiveClassifier    & 1.1490  & 3.2840e-01   & Not Significant \\
MLPClassifier                  & 6.3252  & 2.9084e-04   & Significant \\
LinearDiscriminantAnalysis     & 7.2302  & 8.0842e-05   & Significant \\
QuadraticDiscriminantAnalysis  & 44.5179 & 1.2424e-27   & Significant \\
BernoulliNB                    & 22.5459 & 2.6820e-14   & Significant \\
GaussianNB                     & 1.3856  & 2.4545e-01   & Not Significant \\
ExtraTreeClassifier            & 7.4089  & 6.2747e-05   & Significant \\
VotingClassifier               & 2.4313  & 6.3496e-02   & Not Significant \\
\bottomrule
\end{tabular}
\caption{ANOVA results for Feature 3: Academic Genre}
\end{table*}

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Detector} & \textbf{F-value} & \textbf{p-value} & \textbf{Significance (p<0.05)} \\
\midrule
LogisticRegression             & 15.4867  & 2.1791e-07  & Significant \\
XGBClassifier                  & 6.2824   & 1.9152e-03  & Significant \\
SVC                            & 1.7646   & 1.7158e-01  & Not Significant \\
LinearSVC                      & 12.8361  & 2.9476e-06  & Significant \\
DecisionTreeClassifier         & 0.7064   & 4.9355e-01  & Not Significant \\
RandomForestClassifier         & 98.8694  & 3.2184e-41  & Significant \\
GradientBoostingClassifier     & 0.5297   & 5.8886e-01  & Not Significant \\
AdaBoostClassifier             & 12.1365  & 5.8702e-06  & Significant \\
BaggingClassifier              & 14.2985  & 6.9971e-07  & Significant \\
ExtraTreesClassifier           & 124.3115 & 6.4051e-51  & Significant \\
KNeighborsClassifier           & 3.0683   & 4.6771e-02  & Significant \\
NearestCentroid                & 1.5409   & 2.1451e-01  & Not Significant \\
RidgeClassifier                & 44.5998  & 1.4114e-19  & Significant \\
SGDClassifier                  & 159.0237 & 9.8119e-64  & Significant \\
Perceptron                     & 47.5704  & 8.5035e-21  & Significant \\
PassiveAggressiveClassifier    & 0.2646   & 7.6755e-01  & Not Significant \\
MLPClassifier                  & 26.3431  & 5.5349e-12  & Significant \\
LinearDiscriminantAnalysis     & 43.3016  & 4.8325e-19  & Significant \\
QuadraticDiscriminantAnalysis  & 59.9251  & 7.9565e-26  & Significant \\
BernoulliNB                    & 334.0978 & 4.9281e-122 & Significant \\
GaussianNB                     & 42.1002  & 1.5120e-18  & Significant \\
ExtraTreeClassifier            & 15.4280  & 2.3082e-07  & Significant \\
VotingClassifier               & 64.4107  & 1.2380e-27  & Significant \\
\bottomrule
\end{tabular}
\caption{ANOVA results for Feature 4: Language Environment}
\end{table*}