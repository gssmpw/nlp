\section{Related Work}

\subsection{LLMs for Legal Applications}
Recently, LLMs have profoundly impacted the legal domain, significantly enhancing the efficiency, accuracy, and scalability of legal services~\cite{lai2024large,li2024bladeenhancingblackboxlarge}. 
For instance, Daniel Martin et al.~\cite{katz2024gpt} demonstrate that GPT-4 successfully passes the Uniform Bar Examination (UBE), outperforming both previous models and human test-takers. This highlights its potential to enhance legal services and advance NLP in the legal domain.
Despite the impressive performance, general LLMs encounter significant challenges in complex legal reasoning and specialized tasks, primarily due to their limited domain-specific legal knowledge. 
Therefore, researchers worked to improve the legal adaptability of LLMs  through continue pretraining or fine-tuning. For instance, LexiLaw~\cite{LexiLaw} enhances its expertise and performance in legal consultation and support through fine-tuning on legal domain datasets. ChatLaw~\cite{cui2023chatlaw} integrates knowledge graphs and manual curation to build a high-quality legal dataset for training MoE models, boosting the reliability and accuracy of AI-driven legal services.


\subsection{Benchmarks in the Legal Domain}
LLMs have shown great potential in the legal domain. However, their inherent limitations emphasize the urgent need for comprehensive evaluation. In response, researchers have developed various evaluation criteria and benchmarks.
For instance, LegalBench~\cite{guha2024legalbench} is a collaboratively developed legal benchmark comprising 162 tasks, designed to assess legal reasoning in English LLMs.  Similarly, LawBench~\cite{fei2023lawbench} and LAiW~\cite{dai2023laiw} leverage existing datasets to evaluate the Chinese legal LLMs, fostering community advancement.
LexEval~\cite{lilexeval} presents a taxonomy of legal cognitive abilities and organized 14,150 tasks to systematically evaluate LLMs in the legal domain.
Moreover, Li et al.~\cite{li2024legalagentbench} introduced LegalAgentBench, which provides 37 tools for interacting with external knowledge and evaluates LLM agents in the legal domain. 
Despite these advancements, a dedicated benchmark for legal case document generation is still lacking.
This paper introduces CaseGen, which fills this gap by providing a comprehensive benchmark for multi-stage legal case documents generation in the Chinese legal domain.
