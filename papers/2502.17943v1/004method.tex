\section{CaseGen}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{example.pdf}
\caption{An task example in CaseGen (translated from Chinese).}
\label{figure:example}
\end{figure}





\begin{figure*}[t]
\centering
\vspace{-5mm}
\includegraphics[width=0.9\linewidth]{pipeline.pdf}
% \vspace{-3mm}
\caption{The overview of CaseGen. CaseGen includes four key generation tasks and uses LLMs-as-a-judge as the primary evaluation method.}
\label{figure:task}
\vspace{-5mm}
\end{figure*}

Developed from high-quality, real-world legal cases, CaseGen comprises 500 instances, each structured into seven distinct sections: \textbf{Prosecution}, \textbf{Defense}, \textbf{Evidence}, \textbf{Events}, \textbf{Trial Fact}, \textbf{Reasoning}, and \textbf{Judgment}.
The Prosecution is a formal document filed by the plaintiff to initiate litigation, detailing the claims and supporting facts.
The Defense is the responds to the Prosecution, in which the defendant challenges the plaintiff's claims and presents their own arguments.
The Evidence includes all expert-annotated case-related evidence details, with each piece corresponding to an event in the trial facts.
The Facts, Reasoning, and Judgment sections form the core components of a legal case document. A more detailed description can be found in Section \ref{sec:pre}. We provide a task sample of CaseGen in Figure \ref{figure:example}.




\subsection{Task Definition}
CaseGen includes four key tasks: (1) drafting defense statements, (2) writing trial facts, (3) composing legal reasoning, and (4) generating judgment results. 
These tasks reflect different stages in the document creation process, each with its own writing logic and evaluation criteria, enabling a more precise and comprehensive assessment of LLMs.


\subsubsection{Drafting Defense Statements}
The task of drafting defense statements involves systematically responding to the claims in the prosecution based on the provided evidence list. An effective defense should be clear and logically organized.  Furthermore, it should directly address each claim while integrating relevant legal knowledge and supporting evidence.


\subsubsection{Writing Trial Facts}
The task of writing trial facts can be defined as verifying the true course of events and identifying the key facts based on the provided evidence list, prosecution, and defense statement. 
Since the factual statements in the prosecution and defense may be incomplete or even contradictory, the court must evaluate the evidence to establish the trial facts.
High-quality trial facts should be presented in a clear narrative structure, with a complete timeline and evidentiary chain. Furthermore, all information should be directly relevant to the legal proceedings, with unnecessary details kept to a minimum.

\subsubsection{Composing Legal Reasoning}
Legal reasoning refers to the process by which judges analyze case facts and apply legal principles to justify their rulings. High-quality legal reasoning should clearly identify all key issues in dispute and present the corresponding judicial perspectives. Since legal reasoning requires balancing multiple legal arguments and precisely applying legal provisions, it is one of the most challenging task of legal case documents generation.


\subsubsection{Generating Judgment Results}
Generating a judgment results involves formulating the final ruling based on established trial facts and legal reasoning. This section typically cites relevant legal articles and specifies the corresponding penalties. A well-crafted judgment must be legally sound, enforceable, and logically reasoned, ensuring judicial integrity and fairness.


Figure \ref{figure:task} illustrates the relationship between different tasks in CaseGen.
To effectively prevent error accumulation, each subtask uses authentic documents as input rather than model-generated content.
For example, the input for writing trial facts is the authentic defense statement, not the model-generated defense from the previous task.
This multi-stage generation approach allows for a more precise evaluation of the strengths and weaknesses of the current LLM in legal document drafting tasks.
Due to space constraints, additional task examples and the prompts used are provided in Appendix \ref{sec:more task}.











\subsection{Data Construction}


\subsubsection{Data Source and Processing}


CaseGen is built on high-quality legal documents. We collected hundreds of thousands of legal case documents from China Judgments Online~\footnote{\url{https://wenshu.court.gov.cn/}} and implemented rigorous data filtering and processing techniques to ensure data integrity and quality.

Specifically, we first filter out cases where the fact section contains fewer than 50 chinese characters or involves simplified procedures, as these cases are considered too simplistic. Additionally, we exclude cases with incomplete structures or formatting errors to maintain data consistency.
During filtering, we found that not all legal documents fully record both the plaintiff's claims and the defendant's defenses. Therefore, we carefully selected 50,000 cases that explicitly include both.
To further enhance the diversity and representativeness of the dataset, we then use BGE-base-zh~\cite{bge_embedding} to generate case embeddings and apply K-Means~\cite{ahmed2020k} clustering to group similar documents. From these clusters, we select 500 representative cases evenly as the core dataset for CaseGen.


Then, we utilized regular expressions and text parsing techniques to extract key structural information from legal case documents. The extracted data is structured in JSON format. For cases that are difficult to parse automatically, we manually extract the various sections and conduct thorough verification.



\subsubsection{Data Annotation}
Although high-quality legal case documents generally contain well-structured information, the full details of evidence are often not publicly disclosed. These case documents usually list the names of the submitted evidence without providing their content. 
To ensure data completeness and usability, we recruit legal experts to annotate the content of the evidence.


The annotation follows three core principles: (1) \textbf{Authenticity.} Since LLMs cannot independently verify the authenticity of evidence, all annotated evidence is authentic, excluding any uncertain or potentially falsified information.
(2) \textbf{Completeness.}  The annotated evidence must accurately align with the content of the legal case document, ensuring that the entire trail fact can be reconstructed from the provided evidence.
(3) \textbf{Textual Representation.} All evidence is presented in textual form. For non-text evidence, such as audio recordings or images, experts provide descriptive textual summaries to ensure clarity and usability.
Additionally, annotation experts need to convert litigation claims and defense arguments from the Procedure section into structured prosecution and defense statements. For more detailed annotation guidelines, please refer to Appendix~\ref{sec:guid}.


Our annotation team comprises five legal experts, all of whom have passed the National Unified Legal Professional Qualification Examination and possess a strong legal background. The team includes two male and three female experts, all based in China. To protect the rights and interests of annotators, we established legally binding agreements with all team members before the annotation work began.
These agreements ensure compliance with legal standards and protect the experts' rights throughout the annotation process.


To ensure data quality, all annotators must complete comprehensive training. We first provide a detailed explanation of the task objectives, data formatting requirements, and applicable legal standards. Subsequently, some example cases are provided to help annotators understand the required format and standards.
We provided several hours of in-depth training to ensure annotators fully understood the annotation standards. Following this, each annotator was required to complete five pilot annotation tasks.  Our gold annotator, who hold a Ph.D. in law, conducted cross-check evaluations to review and verify the accuracy of the pilot annotations. Only annotators with an approval rate of 90\% or higher were permitted to proceed with formal annotations.



For each annotated dataset, we employ a dual verification process using both LLMs and human experts.
We first employ an LLM for automated review to verify evidence completeness. Then, legal experts conduct cross-checks to ensure legal compliance and accuracy. The detailed review process can be found in Appendix ~\ref{sec:guid}.
For each successfully reviewed example, we paid \$10.95 to the legal annotators. A total of 500 examples were annotated, amounting to a total payment of \$5,475. 



\begin{table}[t]
\centering
\begin{tabular}{cc}
\hline
\textbf{Statistic}        & \textbf{\#Number} \\ \hline
Total Legal Case Document & 500               \\
Avg. Full Case Length     & 5,223             \\
Avg. Complaint Length     & 1,187             \\
Avg. Defense Length       & 1,100             \\
Avg. Fact Length          & 1,057             \\
Avg. Reasoning Length     & 1,241             \\
Avg. Judgement Length     & 450               \\
Avg. Evidence per Query   & 7.92              \\
Avg. Evidence Length      & 706               \\ \hline
\end{tabular}
\caption{Basic statistic of CaseGen.}
\label{table:sta}
\vspace{-5mm}
\end{table}



\subsection{Data Statistics}
After careful manual verification, CaseGen consists of four types of tasks, with each task containing 500 test samples. Table \ref{table:sta} presents the basic statistical information.
Compared to general-domain texts, legal case documents are significantly longer. On average, each case contains 7.92 pieces of evidence, with each piece averaging 706 characters in length. Additionally, the generated texts can reach lengths of up to 1,000 characters. This poses a significant challenge for LLMs in handling long-text processing effectively.



\subsection{Evaluation Pipeline}
Evaluating legal case documents is a challenging task. Traditional evaluation metrics, such as BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and BERTScore~\cite{zhang2019bertscore}, fail to capture key aspects like fluency, logical coherence, and factuality. 
While human evaluation is reliable, it is time-consuming and labor-intensive, making it difficult to scale for large-scale assessments.
Therefore, we adopt LLM-as-a-judge as the core evaluation method in CaseGen. Recently, LLMs have gained widespread recognition for their effectiveness as evaluators, achieving a high level of consistency with human annotations~\cite{li2024llms}. Compared to traditional automated evaluation metrics, LLM-as-a-judge enables a more fine-grained, multi-dimensional assessment~\cite{10.1145/3627673.3679677,li2024calibraeval}.


However, evaluating legal case documents poses even greater challenges for LLM judges, requiring not only domain-specific expertise but also strict logical reasoning. Moreover, each section follows distinct evaluation criteria, further complicating the evaluation process.
Following Wang et al.,~\cite{wang2024user}, we developed a multi-dimensional automated evaluation framework for legal case documents generation, ensuring both professionalism and reliability. 
As shown in Figure~\ref{figure:task}, the evaluation framework includes the following four key features:

\textbf{Pointwise Scoring.} 
We employ a pointwise scoring method, which offers greater flexibility compared to pairwise comparisons. Specifically, LLM judges perform a multi-dimensional analysis of the generated documents and assign a final score from 1 to 10, with higher scores indicating better quality.


\textbf{Task-Oriented Criteria.}
Different sections of legal case documents require distinct evaluation criteria.
To address these variations, we establish fine-grained evaluation criteria based on expert-defined standards, covering multiple dimensions such as accuracy, logical consistency, completeness, and legal applicability.
For each task, we provide specific evaluation dimensions with detailed explanations to ensure LLM judges accurately reflects the quality of the generated documents.
Additionally, we establish scoring standards for the LLM judges, with each 2-point increment representing a different rating level. 



\textbf{Chain-of-Thought Reasoning.}
To enhance the reliability of LLM judges, we incorporate Chain of Thought (CoT) reasoning, allowing the LLMs to assess the generated content step by step rather than assigning a score directly. Specifically, the LLM judge first compares the generated output with the reference answer, then assigns scores for each evaluation dimension, and finally consider all dimensions to determine the overall score.

\textbf{Reference-Based Evaluation.}
Evaluating legal case documents requires extensive legal expertise. To address this, we adopt the reference-based evaluation approach, where the ground truth is provided as part of the input to the LLM judges. This allows the LLM to contextually compare the generated text with authoritative references, ensuring a more informed and precise evaluation.


More detailed explanations and examples are provided in Appendix \ref{sec:more evaluation}. We further validate the effectiveness of our evaluation framework through human annotations in Section \ref{sec:human}.


\subsection{Legal and Ethical Considerations}
Due to the sensitivity of the legal domain, we have conducted a thorough review of this benchmark. All the open-source datasets we use are licensed. We have also carefully screened and filtered the datasets to avoid any content containing personal identifiable information, discriminatory material, explicit, violent, or offensive content. A more detailed discussion can be found in Appendix \ref{sec:dis}.