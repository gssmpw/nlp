
% 13650
\section{Experiment}

\begin{table*}[t]
\small
\begin{tabular}{lcccccccccccc}
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Defense}}                     & \multicolumn{3}{c}{\textbf{Fact}}                       & \multicolumn{3}{c}{\textbf{Reasoning}}                   & \multicolumn{3}{c}{\textbf{Judgement}}                   \\
                       & ROU.           & BS.            & LLM           & ROU.           & BS.            & LLM           & ROU.           & BS.            & LLM           & ROU.           & BS.            & LLM           \\ \hline
LexiLaw                & 6.18           & 62.38          & 1.17          & 8.16           & 59.70          & 1.18          & 8.27           & 67.65          & 2.36          & 13.20          & 66.17          & 2.22          \\
ChatLaw                & 6.44           & 64.03          & 2.09          & 25.62          & 70.19          & 2.43          & 9.53           & 69.35          & 3.41          & 21.80          & 67.54          & 2.27          \\
GLM-4-flash            & \textbf{25.28} & 74.07          & 4.26          & 39.59          & 75.05          & 3.82          & 19.71          & 71.57          & 5.01          & 26.28          & 72.71          & 3.42          \\
GLM-4                  & 23.05          & 73.31          & 4.47          & 39.55          & 74.82          & 4.32          & 18.68          & 71.58          & 5.39          & \textbf{26.69} & \textbf{75.85} & 3.59          \\
Qwen2.5-72b-instruct   & 22.39          & \textbf{75.45} & \textbf{4.97} & 46.32          & 76.47          & 4.58          & 23.50          & 71.33          & \textbf{6.19} & 19.45          & 74.23          & \textbf{4.46} \\
Llama-3.3-70b-instruct & 21.11          & 70.68          & 4.07          & 37.43          & 74.57          & 3.58          & 23.54          & 72.72          & 4.87          & 21.65          & 70.69          & 4.05          \\
GPT-3.5-turbo          & 19.89          & 71.67          & 4.90          & 38.22          & 73.98          & 4.31          & 22.59          & 71.18          & 5.90          & 17.78          & 70.71          & 3.99          \\
GPT-4o-mini            & 20.84          & 71.35          & 4.83          & 36.00          & 73.69          & 3.99          & 22.46          & 71.50          & 5.66          & 18.52          & 71.03          & 3.88          \\
Claude-sonnet          & 23.60          & 73.31          & 4.91          & \textbf{53.03} & \textbf{77.92} & \textbf{4.75} & \textbf{25.16} & \textbf{72.74} & 5.77          & 25.62          & 77.00          & 4.00          \\ \hline
\end{tabular}
\caption{The main results of the four tasks in CaseGen. ``ROU.'' represents the ROUGE-L score (\%), ``BS.'' stands for BERTScore (\%), and ``LLM'' refers to the scores assigned by the LLM Judge. The best results are highlighted in bold.}
\label{table:main}
\end{table*}



\subsection{Experimental Settings}
We evaluated several popular commercial and open-source models, including GLM-4-flash~\cite{glm2024chatglm}, GLM-4~\cite{glm2024chatglm}, Claude-3.5-sonnet, GPT-3.5-turbo~\cite{achiam2023gpt}, GPT-4o-mini~\cite{achiam2023gpt}, Qwen2.5-72B-Instruct~\cite{yang2024qwen2}, and LLaMA-3.3-70B-Instruct~\cite{touvron2023llama}. Additionally, we assessed legal-specific LLMs, including ChatLaw~\cite{cui2023chatlaw} and LexiLaw~\cite{LexiLaw}.


To ensure reproducibility, we set the temperature of all LLMs to 0. 
All LLMs are evaluated with the same prompt to ensure a fair comparison.
When the input text exceeds the LLM's maximum context window, we truncate the input sequence from the middle since the front and end of the
input may contain crucial information.
We use GPT-4o as the LLM judge to evaluate the performance of other LLMs. In addition to LLM Judge scores, we also provide ROUGE-L~\cite{lin2004rouge} and BERTScore~\cite{zhang2019bertscore} as reference metrics. Due to space limitations, more implementation details are provided in Appendix \ref{sec:exper}.




\subsection{Main Result}
The performance comparison of different LLMs is presented in Table \ref{table:main}. We derive the following observations from the experiment results.

\begin{itemize}[leftmargin=*]
\item \textbf{Legal-specific LLMs exhibit suboptimal performance.}
Despite additional training on legal datasets, legal-specific LLMs such as Lexilaw and ChatLaw perform worse than general LLMs in legal case document generation tasks. 
This may be attributed to two key reasons.
First, the performance of legal-specific LLMs may be limited by the constraints of their base models, which often lack the advanced comprehension and long-text processing capabilities of state-of-the-art general LLMs such as GPT-3.5 and Qwen2.5. For example, Lexilaw has a maximum input length of only 2048 tokens, which may lead to information loss when processing lengthy legal case documents, significantly impacting the quality of the generated context.
Another possible reason is that continuous training on legal corpora may reduce the reasoning abilities inherited from the original base model, limiting its overall effectiveness in generating complex legal cases.
This suggests that legal-specific LLMs need further optimization of training strategies to improve legal reasoning capabilities.




\item \textbf{Open-Source LLMs Demonstrate Competitive Performance in Legal Case Documents Generation.}
Compared to closed-source models like GPT-3.5-turbo and Claude-sonnet, open-source LLMs have achieved competitive performance in legal case documents generation tasks.
Qwen2.5-72B-Instruct achieved the highest LLM judge scores in drafting defense statements, writing trial facts, and generating judgment results.
These results highlight the potential of open-source LLMs as a viable alternative to commercial LLMs.
With continued improvements, open-source LLMs are expected to play an increasingly important role in legal AI applications, making further exploration and development essential.




\item \textbf{Existing LLMs Still Struggle with Legal Case Documents Generation.}
Across multiple tasks evaluated by CaseGen,  most LLMs achieve unsatisfactory scores (below 6 points), indicating that they fail to meet the basic quality standards required for legal case documents. 
This highlights the significant challenges that existing LLMs still face in handling complex legal reasoning.
These LLMs often struggle to generate text that is not only legally precise but also logically coherent. 
Furthermore, it emphasizes the value and challenges of CaseGen as a benchmark for legal document generation, providing clear guidance for the future development of legal AI and specialized LLMs.

\end{itemize}





% 在不同 LLM 的实验中，GPT-3.5 和 LLaMA3.1- 8B 表现不佳，在 LegalAgentBench 上的成功率低于 30%。这可能是因为他们有效使用工具的能力有限，限制了他们在复杂法律任务中解决问题的能力。GLM4、GLM4-Plus、Qwen-Max 和 GPT-4o-mini 的代币消耗量相近。不过，GLM4-Plus 和 Qwen-Max 的性能更胜一筹。Claude-Sonnet 也取得了有竞争力的结果，在 P-S 和 P-E 两种方法下性能最佳。不过，与其他 LLM 相比，它往往需要更多的令牌。在 ReAct 方法下，GPT-4o 以相对较少的令牌取得了最佳性能，成功率达到 78.75%。总的来说，LegalA- gentBench 有效地区分了法律知识管理的能力，并为法律知识管理提供了一种新的方法。


% 通过比较不同的方法，我们发现 ReAct 对于多跳问题通常能产生更好的结果。然而，这种优势伴随着更高的令牌消耗，这表明允许更多的推理时间可以提高性能。此外，我们发现当 LLM 能力有限时，P-E 方法并不总是优于 P-S。GLM-4、LLaMa3.1-8B 和 GPT-3.5 等 LLM 也呈现出类似的趋势。这可能是由于计划更新增加了上下文长度，从而降低了注意力机制的有效性。对于相同的 LLM，不同推理方法之间的性能差距可达 65%，这说明有效的方法能更好地利用 LLM 的潜力。此外，在设计有效的推理方法时，平衡模型能力、推理时间和令牌消耗也是至关重要的。



\subsubsection{Human Evaluation on CaseGen}
\label{sec:human}

In this section, we recruit legal experts to evaluate LLM-generated texts and assess the consistency between LLM judges and human annotations.
Due to cost limitations, we randomly select 50 cases from CaseGen. For each question, we obtain the response from three LLMs: Qwen2.5-72b-instruct, GPT-4o-mini, and Claude-sonnet, as these LLMs demonstrate competitive performance on CaseGen.
Each LLM completes four tasks from CaseGen, generating a total of 600 samples to be evaluated.

We recruit three legal experts, all of whom have passed the National Unified Legal Professional Qualification Examination, to carry out the annotation tasks.
We convert each sample into a input-response-reference triple and present it to human annotators.
To prevent potential bias, annotators were unaware of which LLM generated the response, and the responses were provided in random order.
The annotation criteria provided to the experts align with those given to the LLM judges, ensuring a fair comparison.
We use the Kappa statistic~\cite{warrens2015five} to measure the consistency and quality of the human annotations. The Kappa values~\cite{warrens2015five} for the three annotators across the four tasks are 0.428/0.488/0.539/0.494, respectively, indicating the reliability of our annotations. We paid \$0.21 for each annotation example, totaling \$385.20.


\begin{table}[t]
\small
\centering
\begin{tabular}{lcccc}
\hline
Model                & Defense       & Fact          & Reasoning     & Judgement     \\ \hline
Qwen2.5-72b & \textbf{4.76} & 4.98          & \textbf{5.56} & \textbf{6.02} \\
GPT-4o-mini          & 4.16          & 5.5           & 5.32          & 5.78          \\
Claude-sonnet        & 4.66          & \textbf{5.98} & 5.46          & 5.64          \\ \hline
\end{tabular}
\caption{Results of Human Annotation. The best results are highlighted in bold.}
\label{human}
\end{table}


Table \ref{human} presents the results of the human annotations.
For legal experts, the legal case documents generated by LLMs are still unsatisfactory (below 6 points). 
Even the most advanced LLMs still cannot generate legal case documents that are truly suitable for practical use.
Moreover, we observe that legal experts gave slightly higher average ratings for the tasks of writing trial facts and generating judgment results compared to the LLM judges.
This may be because legal experts can better understand the context and nuances of legal provisions, allowing them to make more accurate judgments based on real-world cases.
On the other hand, LLM judges face limitations in accuracy and logical rigor when dealing with complex legal relationships and dynamic statutes, as they can only compare responses to reference answers.
Further improvements are still needed in the performance of LLM judges within the legal field.


Then, we calculated Spearman's rank correlation coefficient~\cite{zar2005spearman}, Kendall rank correlation coefficient~\cite{abdi2007kendall}, and Pearson correlation coefficient~\cite{sedgwick2012pearson} between the automated metrics and human evaluation results. 
Since the evaluation dimensions vary across tasks, we calculated the consistency for each task separately and then averaged the results.
Table \ref{coefficient} presents the consistency result.
We observe that the LLM judge score demonstrates the highest level of consistency with human annotations, with a Spearman's rank correlation coefficient reaching 75\%. 
In contrast, Rough-L, which relies on lexical matching, demonstrated lower consistency.
BERTScore, which compresses context into vectors to calculate similarity, results in the loss of important details and thus demonstrates the lowest consistency with human annotations.
In conclusion, our evaluation pipeline shows high consistency with human assessments, making it a reliable alternative for large-scale evaluations.


\begin{table}[t]
\centering
\begin{tabular}{lccc}
\hline
Metrics  & \multicolumn{1}{c}{LLM Score} & \multicolumn{1}{c}{Rouge-L} & \multicolumn{1}{c}{BERTScore} \\ \hline
Kendall  & \textbf{0.667}                & 0.333                       & 0.166                         \\
Pearson  & \textbf{0.726}                & 0.264                       & 0.239                         \\
Spearman & \textbf{0.750}                & 0.375                       & 0.250                         \\ \hline
\end{tabular}
\caption{The consistency between different automated metrics and human annotations. he best results are highlighted in bold}
\label{coefficient}
\end{table}







