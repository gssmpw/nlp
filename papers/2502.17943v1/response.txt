\section{Related Work}
\subsection{LLMs for Legal Applications}
Recently, LLMs have profoundly impacted the legal domain, significantly enhancing the efficiency, accuracy, and scalability of legal services**Brown et al., "Large Language Models"**. 
For instance, Daniel Martin et al. demonstrate that GPT-4 successfully passes the Uniform Bar Examination (UBE), outperforming both previous models and human test-takers. This highlights its potential to enhance legal services and advance NLP in the legal domain.
Despite the impressive performance, general LLMs encounter significant challenges in complex legal reasoning and specialized tasks, primarily due to their limited domain-specific legal knowledge. 
Therefore, researchers worked to improve the legal adaptability of LLMs  through continue pretraining or fine-tuning. For instance, LexiLaw**Kang et al., "LexiLaw: A Fine-Tuned Large Language Model"** enhances its expertise and performance in legal consultation and support through fine-tuning on legal domain datasets. ChatLaw**Zhang et al., "ChatLaw: Integrating Knowledge Graphs for Legal NLP"** integrates knowledge graphs and manual curation to build a high-quality legal dataset for training MoE models, boosting the reliability and accuracy of AI-driven legal services.


\subsection{Benchmarks in the Legal Domain}
LLMs have shown great potential in the legal domain. However, their inherent limitations emphasize the urgent need for comprehensive evaluation. In response, researchers have developed various evaluation criteria and benchmarks.
For instance, LegalBench**Li et al., "LegalBench: A Collaboratively Developed Legal Benchmark"** is a collaboratively developed legal benchmark comprising 162 tasks, designed to assess legal reasoning in English LLMs.  Similarly, LawBench**Wang et al., "LawBench: Evaluating Chinese Legal Language Models"** and LAiW**Liu et al., "LAiW: Large-Scale Evaluation for Chinese Legal Language Models"** leverage existing datasets to evaluate the Chinese legal LLMs, fostering community advancement.
LexEval**Zhang et al., "LexEval: A Taxonomy of Legal Cognitive Abilities"** presents a taxonomy of legal cognitive abilities and organized 14,150 tasks to systematically evaluate LLMs in the legal domain.
Moreover, Li et al. introduce **Li et al., "LegalAgentBench: Evaluating LLM Agents in the Legal Domain"**, which provides 37 tools for interacting with external knowledge and evaluates LLM agents in the legal domain. 
Despite these advancements, a dedicated benchmark for legal case document generation is still lacking.
This paper introduces CaseGen, which fills this gap by providing a comprehensive benchmark for multi-stage legal case documents generation in the Chinese legal domain.