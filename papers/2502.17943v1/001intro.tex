\section{Introduction}
Legal case documents are the official records of judicial proceedings, containing factual determinations, legal rationale, judgment outcomes, and other relevant details~\cite{li2024deltapretraindiscriminativeencoder,li2023sailer}.
The quality of legal case documents directly affects both judicial fairness and trial efficiency~\cite{li2024lecardv2}. 
Generally, drafting a high-quality legal case document involves extracting relevant information from extensive evidence, identifying key points of contention, and ensuring logical consistency. Legal professionals must devote significant time and effort, often spending dozens of hours to complete a legal case document~\cite{branting1998techniques,10.1145/3626093}. 
With the explosive growth in legal cases, manually drafting legal case documents now faces pressure from tight deadlines and heavy workloads, making it challenging to balance efficiency and accuracy.


The rise of large language models (LLMs) presents a promising alternative to manually drafting legal case documents~\cite{lai2024large}. These models, trained on vast text corpora, have shown a remarkable ability to understand and generate human-like text~\cite{achiam2023gpt}. Despite their potential, applying LLMs to generate legal case documents continues to present significant challenges.  
Legal case documents require a high level of professionalism and accuracy.
However, probability-based LLMs are prone to hallucinations~\cite{perkovic2024hallucinations}, which means they cannot guarantee the correctness and interpretability of their outputs. If an LLM generates low-quality or misleading legal case documents, it not only increases the workload of legal professionals but also may significantly undermine the fairness of the judgment~\cite{lilexeval}.



The potential and risks of LLMs in generating legal case documents highlight the urgent need to evaluate their professional performance.
However, there is currently no representative benchmark that covers all aspects of legal case documents generation.
Existing datasets in general domains primarily focus on general text processing tasks, such as summarization and open-ended question answering, providing limited guidance for specialized fields like law~\cite{huang2024c,wang2024mmlu}.
Furthermore, existing legal datasets concentrate on relatively straightforward tasks, such as judgment prediction~\cite{xiao2018cail2018} or legal case retrieval~\cite{li2024towards}.
These tasks are typically discriminative with limited output spaces, failing to fully capture the complexity and diversity involved in drafting real-world legal case documents.



To fill this gap, we propose CaseGen, a comprehensive benchmark for multi-stage legal case documents generation in the Chinese legal domain.
Built on high-quality, real-world legal case documents and expert annotations, CaseGen includes 500 instances, each consisting of seven sections: Prosecution, Defense, Evidence, Events, Facts, Reasoning, and Judgment. 
It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results.
CaseGen provides a comprehensive evaluation platform for assessing the strengths and limitations of LLMs in generating legal case documents.

Specifically, CaseGen is unique from the following three perspectives:


\begin{enumerate} 
\item \textbf{First Comprehensive Legal Case Documents Generation Benchmark.}  
To the best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs in legal case document generation. 
It covers all key stages, from the initial complaint to evidence and judgment, providing a complete framework for assessing LLM performance.


\item \textbf{Multi-Stage Generation Task Support.} 
Instead of directly generating entire case documents, CaseGen follows the structure and writing process of real-world legal case documents, designing multi-stage generation tasks. It includes four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. Each task has its own writing logic and evaluation criteria, enabling a more detailed and nuanced assessment.

\item \textbf{Automated Evaluation Framework.}
Relying on human evaluation for quality evaluation is both costly and time-consuming. To achieve efficient automated evaluation, CaseGen adopts the LLM-as-a-judge scoring approach~\cite{li2024llms}. 
The LLM judges assign pointwise scores based on task-oriented criteria, referencing the ground truth and employing Chain-of-Thought (CoT) reasoning~\cite{wei2022chain}. Human evaluations confirm the effectiveness of this method.
\end{enumerate}

We conduct a systematic evaluation of various commercial and open-source LLMs. 
The results show that current LLMs do not achieve satisfactory performance in legal case document generation.
Additionally, human annotations show that our evaluation framework aligns closely with legal expert annotations. We also provide an in-depth analysis of the strengths and limitations of LLMs, highlighting key areas for future development.



