\onecolumn
% \newgeometry{left=2cm,bottom=2cm}
\section{Proof of Lemma \ref{lem:remove}} \label{apdx:remove}
\begin{proof}
    Let $\mR$ denotes $\mR_x\mR_y^\top=\mU\mathbf{\Sigma}\mV^\top$, then the original product is $\mA\mB^\top = \mQ_x \mR \mQ_y^\top$. When the first snapshot dumped, the change in $\mR$ can be expressed as: $\overline{\mR} = \mR-\sigma_1\vu_1\vv_1^\top$. The product after this dump is $\overline{\mA\mB^\top}=\mQ_x\overline{\mR}\mQ_y^\top$. Now, define $\overline{\mR}_x = \mR_x-\vu_1\vu_1^\top\mR_x,\overline{\mR}_y = \mR_y-\vv_1\vv_1^\top\mR_y$. We can prove $\overline{\mR}_x\overline{\mR}_y^\top=\overline{\mR}$ as below,
   \begin{align*}
         \overline{\mR}_x\overline{\mR}_y^\top &= (\mI-\vu_1\vu_1^\top)\mR_x\mR_y^\top(\mI-\vv_1\vv_1^\top)  \\
         &= \mR-\vu_1\vu_1^\top\mR-\mR\vv_1\vv_1^\top+\vu_1\vu_1^\top\mR\vv_1\vv_1^\top   \\
         &= \mR-\sigma_1\vu_1\vv_1^\top 
    \end{align*}
    where the last equality holds due to $\vu_1 \vu_1^\top \mR = \vu_1 \vu_1^\top \sum_{i} \sigma_i \vu_i \vv_i^\top = \sigma_1 \vu_1 \vv_1^\top$, and similarly for the terms $\mR\vv_1\vv_1^\top$ and $\vu_1\vu_1^\top\mR\vv_1\vv_1^\top$. Therefore, $\overline{\mA\mB^\top}=\mQ_x\overline{\mR}_x\overline{\mR}_y^\top\mQ_y^\top$. Now we can reconstruct $\overline{\mA} = \mQ_x\overline{\mR}_x$ and $\overline{\mB} = \mQ_y\overline{\mR}_y$. Apparently, they are equivalent to $\mC$ and $\mD$ with the first column removed. 
\end{proof}

\section{Proof of Theorem \ref{thm:hds}}\label{apdx:hds}
\begin{proof}
    
Let $\mX_{T-N,T}\mY_{T-N,T}^\top$ be the original product within the window $[T-N,T]$. The algorithm \ref{alg:hdscod} returns an approximation $\mA_{T-N,T}\mB_{T-N,T}^\top+\hat{\mC}_{(k-1)N+1,T}\hat{\mD}_{(k-1)N+1,T}^\top$, where $\mA_{T-N,T}\mB_{T-N,T}^\top$ denotes the key component of the product core consisting of snapshots, and $\hat{\mC}_{(k-1)N+1,T}\hat{\mD}_{(k-1)N+1,T}^\top$ represents the sketch residual product.  Here, $k=\operatorname*{max}(1,\lfloor(T-1)/N\rfloor)$ and $(k-1)N+1$ represents the last timestamp of the N-restart. Define $\mC_{T-N,T}\mD_{T-N,T}^\top = \mX_{T-N,T}\mY_{T-N,N}^\top-\mA_{T-N,T}\mB_{T-N,T}^\top$ to be the real residual product. For simplicity, we denote $\hat{\mC}_{(k-1)N+1,T}$ as $\hat{\mC}_T$, and similarly for the other terms. The correlation error can be expressed as:
\begin{align}
    &\quad \|    \mX_{T-N,T}\mY_{T-N,T}^\top-(\mA_{T-N,T}\mB_{T-N,T}^\top+\hat{\mC}_{(k-1)N+1,T}\hat{\mD}_{(k-1)N+1,T}^\top)    \|_2 \nonumber\\
    &= \|     \mX_T\mY_T^\top - \mX_{T-N}\mY_{T-N}^\top - \mA_T\mB_T^\top +\mA_{T-N}\mB_{T-N}^\top  -  \hat{\mC}_T\hat{\mD}_T^\top     \|_2 \nonumber\\
    &= \|     (\mC_T\mD_T^\top-\hat{\mC}_T\hat{\mD}_T^\top) - (\mC_{T-N}\mD_{T-N}^\top - \hat{\mC}_{T-N}\hat{\mD}_{T-N}^\top) +   \hat{\mC}_{T-N}\hat{\mD}_{T-N}^\top   \|_2 \nonumber\\
    &\leq \| \mC_T\mD_T^\top-\hat{\mC}_T\hat{\mD}_T^\top \|_2 + \| \mC_{T-N}\mD_{T-N}^\top - \hat{\mC}_{T-N}\hat{\mD}_{T-N}^\top \|_2 + \| \hat{\mC}_{T-N}\hat{\mD}_{T-N}^\top \|_2
\end{align}
Define the iterative error $\Delta_t = \vx_t\vy_t^\top + \hat{\mC}_{t-1}\hat{\mD}_{t-1}^\top - (\hat{\mC}_{t}\hat{\mD}_{t}^\top + \va_t \vb_t^\top)$. Sum it up from $(k-1)N+1$ to $T$: 
\[
    \sum_{t=(k-1)N+1}^\top{\Delta_t}  = \mX_T\mY_T^\top - \hat{\mC}_{T}\hat{\mD}_{T}^\top - \mA_T\mB_T^\top = \mC_T\mD_T^\top - \hat{\mC}_{T}\hat{\mD}_{T}^\top.
\]
By the definition of COD, we have
\begin{align}
    \Delta_t &= \mQ_x^{(t)}\mU_t\mathbf{\Sigma}_t\mV_t^\top\mQ_y^{(t)T} - \mQ_x^{(t)}\mU_t \cdot \max{(\mathbf{\Sigma_t}-\sigma_{\ell}^{(t)} \mI,0)} \cdot \mV_t^\top\mQ_y^{(t)\top} \nonumber\\
    &= \mQ_x^{(t)} \mU_t \cdot \min{(\mathbf{\Sigma_t},\sigma_{\ell}^{(t)} \mI)} \cdot \mV_t^\top\mQ_y^{(t)\top} \nonumber
\end{align}
Thus, 
\[
\| \mC_T\mD_T^\top - \hat{\mC}_{T}\hat{\mD}_{T}^\top \|_2 \leq \sum_{t=(k-1)N+1}^\top \| \Delta_t \|_2  = \sum_{t=(k-1)N+1}^\top \sigma_\ell^{(t)}.
\]
According to the proof of theorem 2 of \cite{mroueh2017co}, we can deduce that $\sum_{t=(k-1)N+1}^\top \sigma_\ell^{(t)} \leq \frac{2}{\ell}\|\mX_T\|_F\|\mY_T\|_F $.

Suppose the maximum squared norm of column within the window is $2^i, i\in[0,\log R]$, we have
\begin{gather}
    \| \mC_T\mD_T^\top-\hat{\mC}_T\hat{\mD}_T^\top \|_2 \leq 2\varepsilon\|\mX_T\|_F\|\mY_T\|_F  \leq 2\varepsilon \cdot2^i(T-(k-1)N) \nonumber \\
    \| \mC_{T-N}\mD_{T-N}^\top - \hat{\mC}_{T-N}\hat{\mD}_{T-N}^\top \|_2 \leq 2\varepsilon\|\mX_{T-N}\|_F\|\mY_{T-N}\|_F  \leq 2\varepsilon \cdot2^i(T-N-(k-1)N) \nonumber \\
    \| \hat{\mC}_{T-N}\hat{\mD}_{T-N}^\top\| \leq 2\varepsilon \cdot2^iN \nonumber.
\end{gather}

Therefore, Equation (2) can be bounded by
\[
    4\varepsilon \cdot 2^i(T-(k-1)N) \leq 2^{i+3}\varepsilon N.
\]
To satisfy error $2^{i+3}\varepsilon N \leq \alpha\varepsilon \|\mX_{T-N,T}\|_F\|\mY_{T-N,T}\|_F$, we choose the level $i$:
\[
\log_2{\frac{\alpha\|\mX_{T-N,T}\|_F\|\mY_{T-N,T}\|_F}{8N}}-1 \leq i \leq \log_2{\frac{\alpha\|\mX_{T-N,T}\|_F\|\mY_{T-N,T}\|_F}{8N}}
\]
, where $i$ always exist when $\alpha\geq8$.
\end{proof}

\section{Complexity analysis of Algorithm \ref{alg:hdscod}} \label{apdx:comlexity}
\paragraph{Space Complexity.} Each DS-COD sketch contains a snapshot queue $S$ and a pair of residual matrices $\mA$ and $\mB$. In the hierarchical structure, each snapshot queues is bounded by $O(\ell)$ column vectors, and each residual matrix contains at most $O(\ell)$ columns. Since there are $\log R+1$ levels, the total space complexity is $O\left(\frac{m_x + m_y}{\varepsilon} (\log R+1)\right)$.
\paragraph{Time Complexity.} The time complexity of the DS-COD algorithm is mainly driven by its update operations. In case 1, the amortized cost of each CS operation, performed every $\ell$ columns, is $O((m_x + m_y) \ell)$. The rank-1 update operations for maintaining the covariance matrices $\mK_A$ and $\mK_B$ cost $O((m_x + m_y) \ell)$, while LDL and SVD decompositions each require $O(\ell^3)$.
Lines 16-23 of the algorithm may produce multiple snapshots, but the total number of snapshots in any window is bounded by the window size $N$, so this cost can be amortized over each update. The cost of computing the snapshots $\va_j$ and $\vb_j$ is $O((m_x + m_y) \ell)$. The updates to the matrices $\overline{\mA}$ and $\overline{\mB}$, which are used to remove the influence of the snapshots, take the following form:
$$\overline{\mA} = \mA - \frac{1}{\sigma_1} \mA (\mR_y^\top \vv_1)(\vu_1^\top \mR_x), \quad \overline{\mB} = \mB - \frac{1}{\sigma_1} \mB (\mR_x^\top \vu_1)(\vv_1^\top \mR_y),$$
which requires $O((m_x + m_y) \ell)$.
Next, the update to the covariance matrix $\overline{\mK}_A$ is calculated as:
\begin{align}
    \overline{\mK}_A &=\mK_A-\mK_A\mP_A-\mP_A^\top\mK_A+\mP_A^\top\mK_A\mP_A \nonumber\\
    &= \mK_A-(\mA^\top\mA\mP_A+\mP_A^\top\mA^\top\mA-\mP_A^\top\mA^\top\mA\mP_A) \nonumber\\
    &= \mK_A-(\mA^\top\mA\mP_A+\mP_A^\top\mA^\top\overline{\mA}) \nonumber\\
    &= \mK_A-(\mA^\top\mA-\mA^\top\overline{\mA}+\mP_A^\top\mA^\top\overline{\mA}) \nonumber\\
    &= \mA^\top\overline{\mA} - \mP_A^\top\mA^\top\overline{\mA} \nonumber&
\end{align}
Where the term $\mA^\top \overline{\mA}$ is computed as:
$$\mA^\top \overline{\mA} = \mA^\top (\mA - \mA \mP_A) = \mK_A - \mK_A \mP_A = \mK_A - \frac{1}{\sigma_1} \mK_A (\mR_y^\top \vv_1)(\vu_1^\top \mR_x),$$
which costs $O(\ell^2)$. Similarly, the term $\mP_A^\top \mA^\top \overline{\mA}$ requires $O(\ell^2)$ as well, since:
$$\mP_A^\top \mA^\top \overline{\mA} = \frac{1}{\sigma_1} \mR_x^\top \vu_1 \left( (\vv_1^\top \mR_y) (\mA^\top \overline{\mA}) \right).$$
% Thus, the total cost of removing the influence of the snapshots from the buffer and covariance matrices is O((mx+my+ℓ)ℓ)O((m_x + m_y + \ell) \ell).
Thus, the amortized time complexity for a single update step in Algorithm \ref{alg:update} is $O((m_x + m_y)\ell + \ell^3)$. Assuming $m_x + m_y = \Omega(\ell^2)$, the update cost simplifies to $O((m_x + m_y)\ell)$. Given that there are $\log{R}+1$ levels to update, the total cost of DS-COD is $O\left( \frac{m_x + m_y}{\varepsilon} (\log{R}+1) \right)$.

\section{Proof of Theorem \ref{thm:lowerbound}}\label{apdx:lowerbound}
\begin{proof}
Without loss of generality, we suppose that $m_x\le m_y$ and let $\ell=1/\varepsilon$. By Lemma \ref{lem:matrix_set}, we can construct a set of matrix pairs $\fZ_{\ell/4}$ of cardinality $2^{\Omega((m_x+m_y)\ell)}$, where (i) each pair $(\mX^{(i)},\mY^{(i)})$ satisfies $\mathbf{X}^{(i)}\in\mathbb{R}^{m_x \times \frac{\ell}{4}}$,  $\mathbf{Y}^{(i)}\in\mathbb{R}^{m_y \times \frac{\ell}{4}}$ and $\mathbf{X}^{(i)\top} \mathbf{X}^{(i)} = \mathbf{Y}^{(i)\top} \mathbf{Y}^{(i)} = \mathbf{I}_{\ell/4}$; (ii) $\left\|\mathbf{X}^{(i)} \mathbf{Y}^{(i)\top} - \mathbf{X}^{(j)} \mathbf{Y}^{(j)\top}\right\|_2 > \frac{1}{2}$ for any $i\ne j$. We can select $\log{R} + 1$ matrix pairs from the set $\mathcal{Z}_{\ell/4}$ with repetition, arrange them in a sequence and label them from right to left as $0,1,2,\dots,\log{R}$, making total number of distinct arrangements is $S=2^{\Omega((m_x+m_y)\ell(\log R+1))}$. We call the matrix pair labeled $i$ as block $i$. Then, we adjust these blocks to construct a worst-case instance through the following steps:
\begin{enumerate}
    \item For each block $i$, we scale their unit column vectors by a factor of $\sqrt{\frac{2^i N}{\ell}}$ and thus the size of block $i$ satisfies $\|\mX_{i}\|_F\|\mY_{i}\|_F = \frac{2^i N}{4}$.
    
    \item For blocks where $i > \log_2\left(\frac{\ell R}{N}\right)$, to ensure that all column pairs  satisfies $1\le\|\vx\|\|\vy\|\le R$. Thus we increase the number of columns of these blocks from $\frac{\ell}{4}$ to $\frac{\ell}{4} \cdot 2^{i - \log_2 \frac{\ell R}{N}}$ by duplicate the matrix for $2^{i - \log_2 \frac{\ell R}{N}}$ times and scale the squared norm of each column to $R$. 
    Consequently, the total number of columns can be computed as 
        \[
        \frac{\ell}{4} \cdot \left(\log_2{\frac{\ell R}{N}}+1\right) + \frac{\ell}{4}\cdot\sum_{i=\log\frac{\ell R}{N}+1}^{\log_2 R} 2^{i - \log_2 \frac{\ell R}{N}}= \frac{\ell}{4} \left(\log_2{\frac{\ell R}{N}}+1\right)+\frac{\ell}{2}\left( 2^{\log_2{R} - \log_2{\left(\frac{\ell R}{N}\right)}} - 1 \right) = \frac{\ell}{4} \log{\frac{\ell R}{2N}} + \frac{N}{2}.
        \]
    Since we have assumed that $N \geq \frac{1}{2\varepsilon} \log{\frac{R}{\varepsilon}}$ and $\ell=\frac{1}{\varepsilon}$, then we have $\frac{\ell}{4} \log{\frac{\ell R}{2N}} + \frac{N}{2}\le N$, which means a window can contain all selected blocks.
    %Since the window size is bounded by $N$, we have $\frac{\ell}{4} \log\left(\frac{\ell R}{2N}\right)+\frac{N}{2} \leq N$, which implies that $N \geq \frac{\ell}{2} \log{\ell R}$.
    
    \item All columns outside the aforementioned blocks, including those that have not yet arrived, are set to zero vectors. Additionally, we append an extra dimension to each vector, setting its value to $1$. 
\end{enumerate}

We define $(\mX_W^i,\mY_W^i)$ as the active data over the sliding window of length $N$ at the moment when $i+1,i+2,\dots,\log{R}$ blocks have expired. Suppose a sliding window algorithm provides $\frac{\varepsilon}{9}$-correlation sketch $(\mA_W^i,\mB_W^i)$ and $(\mA_W^{i-1},\mB_W^{i-1})$ for $(\mX_W^i,\mY_W^i)$ and $(\mX_W^{i-1},\mY_W^{i-1})$, respectively. The guarantee of the algorithm indicates
\begin{align}
    \| \mX_W^i{\mY_W^{i}}^\top - \mA_W^i{\mB_W^{i}}^\top \|_2 &\leq \frac{\varepsilon}{9} \|\mX_W^i\|_F\|\mY_W^i\|_F \leq  \frac{\varepsilon}{9} \sum_{j=0}^i{\|\mX^{(j)}\|_F\|\mY^{(j)}\|_F} = \frac{\varepsilon}{9}\left( \frac{N}{4}\cdot2^{i+1}+\frac{3N}{4} \right)\nonumber
\end{align}
Similarly, we can obtain the following result:  
\[
\| \mX_W^{i-1} {\mY_W^{i-1}}^\top - \mA_W^{i-1} {\mB_W^{i-1}}^\top \|_2 \leq \frac{\varepsilon}{9} n \left( \frac{N}{4} \cdot 2^i + \frac{3N}{4} \right).
\]
Therefore, we can approximate block $i$ with $\mA_i\mB_i^\top=\mA_W^i{\mB_W^{i}}^\top-\mA_W^{i-1} {\mB_W^{i-1}}^\top$. Then we have
\begin{align}
    &\| \mX_i{\mY_{i}}^\top - \mA_{i}{\mB_{i}}^\top \|_2 \nonumber\\
    = &\| (\mX_W^i{\mY_W^{i}}^\top- \mX_W^{i-1}{\mY_W^{i-1}}^\top) - (\mA^i{\mB^{i}}^\top - \mA^{i-1}{\mB^{i-1}}^\top) \|_2 \nonumber\\
    \leq & \| \mX_W^i{\mY_W^{i}}^\top- \mA^i{\mB^{i}}^\top\|_2 + \| \mX_W^{i-1}{\mY_W^{i-1}}^\top - \mA^{i-1}{\mB^{i-1}}^\top \|_2 \nonumber\\
    \leq &\frac{\varepsilon}{9} \left( \frac{3}{4}N\cdot2^i+\frac{3}{2}N \right) \nonumber\\
    \leq &  \frac{1}{\ell} \cdot\frac{2^i N}{4} \nonumber
\end{align}

Notice that if we use two different matrix pairs $(\mathbf{X}^{(i)}, \mathbf{Y}^{(i)})$ and $(\mathbf{X}^{(j)}, \mathbf{Y}^{(j)})$ from $\fZ_{\ell/4}$ to construct block $i$, we always have 
$$\left\|\mathbf{X}^{(i)} \mathbf{Y}^{(i)\top} - \mathbf{X}^{(j)} \mathbf{Y}^{(j)\top}\right\|_2 > \frac{1}{2}\cdot \frac{2^i N}{\ell}.$$ 
Hence the algorithm can encode a sequence of $\log R+1$ blocks. Since we can have $S=2^{\Omega((m_x+m_y)\ell(\log R+1))}$ sequence in total, which means the lower bound of space complexity is $\log S=\Omega((m_x+m_y)\ell(\log R+1))$.

%According to Lemma \ref{lem:lwb_cod}, any approximation algorithm requires at least $\Omega(\ell(m_x + m_y))$ bits of space to approximate any single block within the window. To accurately approximate the entire window, the sliding window algorithm must utilize at least $\Omega(\ell(m_x + m_y) \log{R})$ bits of space.
\end{proof}

\section{Proof of Theorem \ref{thm:lowerbound_time}}\label{apdx:lowerbound_time}

\begin{proof}
    Similar to the proof of Theorem \ref{thm:lowerbound}, we first construct a set of matrix pairs \( \mathcal{Z}_{\ell/4} \). We then select \( \log{(NR/\ell)} +1\) matrix pairs with repetition, arrange them in a sequence, and label them from right to left as \( 0, 1, \dots, \log{(NR/\ell)} \), resulting in \( S = 2^{\Omega((m_x + m_y)\ell \log{(NR/\ell)})} \) distinct arrangements. Finally, we construct a worst-case instance using the following steps:
    \begin{enumerate}
        \item For each block \( i \), we scale the unit column vectors by \( \sqrt{2^i} \), so the size of block \( i \) satisfies \( \|\mX_{i}\|_F \|\mY_{i}\|_F = \frac{2^i \ell}{4} \).
        
        \item For blocks where \( i > \log_2{R} \), we increase the number of columns from \( \frac{\ell}{4} \) to \( \frac{\ell}{4} \cdot 2^{i - \log_2{R}} \) to ensure \( 1 \leq \|\mathbf{x}\| \|\mathbf{y}\| \leq R \). The total number of columns, \( \frac{\ell}{4} \log{\frac{N}{2}} \), should be bounded by \( N \), leading to \( N \geq \frac{\ell}{2} \log{\frac{R}{2}} \).
        
        \item Columns outside the mentioned blocks, including those not yet arrived, are set to zero vectors.
    \end{enumerate}

    We define \( (\mX_W^i, \mY_W^i) \) as the active data in the sliding window of length \( N \) after \( i+1, i+2, \dots, \log{(NR/\ell)} \) blocks have expired. Suppose a sliding window algorithm provides a \( \frac{\varepsilon}{3} \)-correlation sketch \( (\mA_W^i, \mB_W^i) \) and \( (\mA_W^{i-1}, \mB_W^{i-1}) \) for \( (\mX_W^i, \mY_W^i) \) and \( (\mX_W^{i-1}, \mY_W^{i-1}) \), respectively. The algorithm's guarantee indicates:
    \begin{align}
        \| \mX_W^i{\mY_W^{i}}^\top - \mA_W^i{\mB_W^{i}}^\top \|_2 &\leq \frac{\varepsilon}{3} \|\mX_W^i\|_F\|\mY_W^i\|_F \nonumber\\
        &\leq  \frac{\varepsilon}{3} \sum_{j=0}^i{\|\mX^{(j)}\|_F\|\mY^{(j)}\|_F} \nonumber\\
        &= \frac{\varepsilon}{3}\left( \frac{\ell}{4}\cdot2^{i+1}-\frac{\ell}{4} \right)\nonumber
    \end{align}
    Similarly, we obtain:
    \[
    \| \mX_W^{i-1} {\mY_W^{i-1}}^\top - \mA_W^{i-1} {\mB_W^{i-1}}^\top \|_2 \leq \frac{\varepsilon}{3}  \left( \frac{\ell}{4} \cdot 2^i - \frac{\ell}{4} \right).
    \]
    Therefore, we approximate block \( i \) as \( \mA_i \mB_i^\top = \mA_W^i{\mB_W^{i}}^\top - \mA_W^{i-1} {\mB_W^{i-1}}^\top \). Then we have:
    \begin{align}
        &\| \mX_i {\mY_{i}}^\top - \mA_{i}{\mB_{i}}^\top \|_2 \nonumber\\
        = &\| (\mX_W^i {\mY_W^{i}}^\top - \mX_W^{i-1} {\mY_W^{i-1}}^\top) - (\mA^i {\mB^{i}}^\top - \mA^{i-1} {\mB^{i-1}}^\top) \|_2 \nonumber\\
        \leq & \| \mX_W^i {\mY_W^{i}}^\top - \mA^i {\mB^{i}}^\top \|_2 + \| \mX_W^{i-1} {\mY_W^{i-1}}^\top - \mA^{i-1} {\mB^{i-1}}^\top \|_2 \nonumber\\
        \leq & \frac{\varepsilon}{3} \left( \frac{3}{4}\ell\cdot2^i - \frac{1}{2}\ell \right) \nonumber\\
        \leq &  \frac{1}{\ell} \cdot \frac{2^i \ell}{4} \nonumber
    \end{align}
    
    Notice that when using two different matrix pairs \( (\mathbf{X}^{(i)}, \mathbf{Y}^{(i)}) \) and \( (\mathbf{X}^{(j)}, \mathbf{Y}^{(j)}) \) from \( \mathcal{Z}_{\ell/4} \) to construct block \( i \), we always have:
    $$\left\|\mathbf{X}^{(i)} \mathbf{Y}^{(i)\top} - \mathbf{X}^{(j)} \mathbf{Y}^{(j)\top}\right\|_2 > \frac{1}{2} \cdot 2^i.$$ 
    Hence, the algorithm can encode a sequence of \( \log{(NR/\ell)} +1 \) blocks. Since the total number of distinct sequences is \( S = 2^{\Omega((m_x+m_y)\ell \log{(NR/\ell)})} \), the lower bound for the space complexity is \( \log{S} = \Omega((m_x + m_y)\ell \log{(NR/\ell)}) \).
\end{proof}
