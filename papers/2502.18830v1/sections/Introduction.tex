Large-scale matrix multiplication is a computational bottleneck in many machine learning and data analysis tasks, such as regression~\cite{woodruff2014sketching,naseem2010linear}, clustering~\cite{cohen2015dimensionality,dhillon2001co}, online learning~\cite{TPAMI2022Wan,luo2019robust,duchi2011adaptive,agarwal2019efficient}, and principal component analysis (PCA)~\cite{abdi2010principal,hasan2021review,greenacre2022principal}. For PCA, the product of a high-dimensional data covariance matrix and its eigenvectors typically has a computational complexity of $O(n^3)$, which becomes prohibitive as data size increases. Approximate Matrix Multiplication (AMM)~\cite{drineas2006fast,woodruff2014sketching,ye2016frequent,mroueh2017co} provides an efficient tradeoff between precision and space-time complexity. AMM has proven effective across various tasks~\cite{cohen1999approximating,kyrillidis2014approximate,ye2016frequent,gupta2018oversketch,plancher2019application}. A current trend is to consider AMM in the streaming model, where matrix data is generated in real-time and processed on-the-fly to maintain up-to-date approximations. Among these approaches, the COD algorithm~\cite{mroueh2017co} stands out for its optimal space-error tradeoff, along with superior efficiency and robustness.

% In many machine learning and data analysis tasks, large-scale matrix multiplication often becomes an unavoidable computational bottleneck. For instance, matrix multiplication plays a central role in regression~\cite{woodruff2014sketching,naseem2010linear}, clustering~\cite{cohen2015dimensionality,dhillon2001co}, online learning~\cite{TPAMI2022Wan,luo2019robust,duchi2011adaptive,agarwal2019efficient}, and principal component analysis (PCA)~\cite{abdi2010principal,hasan2021review,greenacre2022principal}. When performing PCA on large datasets, it is necessary to compute the product of a high-dimensional data covariance matrix and its eigenvectors, with a computational complexity typically of $O(n^3)$. As the data size increases, such computations become prohibitively expensive. Approximate Matrix Multiplication (AMM)~\cite{drineas2006fast,woodruff2014sketching,ye2016frequent,mroueh2017co} is a class of algorithms that strike a balance between precision and space-time complexity. These algorithms provide a bounded approximation result efficiently within limited space. Many studies~\cite{cohen1999approximating,kyrillidis2014approximate,ye2016frequent,gupta2018oversketch,plancher2019application} have shown that AMM, as a foundational subroutine, performs excellently across various tasks. A current trend is to consider AMM in the streaming model, where matrix data is generated in real-time, and the algorithm processes it on-the-fly while maintaining the most up-to-date approximation. Among the various AMM approaches, the COD algorithm~\cite{mroueh2017co} stands out as a deterministic method with the best space-error tradeoff, along with superior efficiency and robustness.

In real-world applications, recent data is often prioritized over outdated data. The time sensitivity is common in data analysis tasks, such as analyzing trending topics on social media~\cite{becker2011beyond} over the past week or monitoring network traffic~\cite{joshi2015review} in the last 24 hours. The AMM problem must also account for this time-sensitive nature. For example, in user behavior analysis, researchers are particularly interested in the correlation between user searches and ad clicks in the most recent period.  
The sliding window model~\cite{datar2002maintaining} is well-suited to time-sensitive applications. It processes a data stream by maintaining a window of size $N$, focusing only on the most recent $N$ data items (sequence-based) or the data generated within the most recent $N$ units of time (time-based). The goal of sliding window algorithms is to process only the active data within the window while efficiently discarding outdated information. In most cases, achieving perfectly accurate results requires storing the entire content of the window. This becomes impractical when $N$ or the data dimension is large. Therefore, transitioning from the streaming model to the sliding window model demands novel techniques.

In the AMM over sliding window problem, we have two input matrices $\mX \in \mathbb{R}^{m_x \times n}$ and $\mY \in \mathbb{R}^{m_y \times n}$, and the matrices are received column by column. The active data within the window are represented by $\mX_W \in \mathbb{R}^{m_x \times N}$ and $\mY_W \in \mathbb{R}^{m_y \times N}$. The algorithm processes the incoming columns sequentially while maintaining two low-rank approximations, $\mA \in \mathbb{R}^{m_x \times \ell}$ and $\mB \in \mathbb{R}^{m_y \times \ell}$, where $\ell$ is much smaller than $N$. Its goal is to ensure that the product $\mA\mB^\top$ closely approximates $\mX_W\mY_W^\top$ (i.e., $\mA\mB^\top \approx \mX_W\mY_W^\top$) through the progress, while minimizing both space and time costs. \citet{yao2024approximate} were the pioneers in investigating streaming AMM algorithms over sliding windows. Their proposed EH-COD and DI-COD algorithms are based on the exponential histogram framework~\cite{datar2002maintaining} and the dyadic intervals framework~\cite{arasu2004approximate}. The space complexities of EH-COD and DI-COD are $O\left(\frac{m_x+m_y}{\varepsilon^2}\log{(\varepsilon NR)}\right)$  and $O\left(\frac{m_x+m_y}{\varepsilon}R\log^2{\frac{R}{\varepsilon}}\right)$ respectively, where $R$ represents the upper bound of the squared norm of the data columns. Although their methods are computationally efficient, whether they achieve optimal space complexity remains an open question. \citet{yin2024optimal} proposed an optimal algorithm for matrix sketching over sliding windows, which can be regarded as a symmetric case of AMM in this context; however, developing an optimal algorithm for general AMM over sliding windows continues to be a challenge.

\begin{table}[t]
    \belowrulesep=0pt
    \aboverulesep=0pt
    \centering
    \renewcommand{\arraystretch}{1.5} 
    \caption{Comparison of space complexity of AMM algorithms over sliding windows, with upper bound of relative correlation error $\varepsilon$. Here, $N$ is the window size, $m_x$ and $m_y$ are the dimensions of input matrices $\mX$ and $\mY$, and $R$ is an upper bound on column norms, satisfying $1 \leq \|\vx\|_2\|\vy\|_2 \leq R$.}
    \label{table:comparison}
    \begin{tabular}{|c|c|c|}
        \toprule  
        Methods & Sequence-based & Time-based \\ \hline
        Sampling~\cite{yao2024approximate} & $\frac{m_{x}+m_{y}}{\varepsilon^2} \log (NR)$ & $\frac{m_{x}+m_{y}}{\varepsilon^2} \log (NR)$ \\ 
        EH-COD~\cite{yao2024approximate} & $\frac{m_{x}+m_{y}}{\varepsilon^2} \log (\varepsilon NR)$ & $\frac{m_{x}+m_{y}}{\varepsilon^2} \log (\varepsilon NR)$ \\ 
        DI-COD~\cite{yao2024approximate} & $\frac{m_{x}+m_{y}}{\varepsilon} R \log^2{\frac{R}{\varepsilon}}$ & - \\ 
        \hline
        DS-COD (ours) & $\frac{m_{x}+m_{y}}{\varepsilon} (\log R+1)$ & $\frac{m_{x}+m_{y}}{\varepsilon} \log (\varepsilon NR)$ \\ 
        Lower bound (ours) & $\frac{m_{x}+m_{y}}{\varepsilon} (\log R+1)$ & $\frac{m_{x}+m_{y}}{\varepsilon} \log (\varepsilon NR)$ \\
        \bottomrule
    \end{tabular}
\end{table}



In this paper, we establish a space lower bound for the AMM over the sliding window problem. We prove that any deterministic algorithm for sequence-based sliding windows that achieves an \( \varepsilon \)-approximation error bound requires at least \( \Omega\left(\frac{m_x + m_y}{\varepsilon} (\log{R}+1)\right) \) space. We also propose a new deterministic algorithm, called Dump Snapshot Co-occurring Directions (DS-COD), which utilizes the concept of \( \lambda \)-snapshots \cite{lee2006simpler} and employs a hierarchical structure to handle data from windows with different distributions. Our theoretical analysis shows that in the sequence-based model, the DS-COD algorithm achieves the \( \varepsilon \)-approximation error bound while requiring only \( O\left(\frac{m_x + m_y}{\varepsilon} (\log{R}+1)\right) \) space, demonstrating that it is optimal in terms of space complexity. In the time-based sliding windows, DS-COD requires at most \( O\left(\frac{m_x + m_y}{\varepsilon} \log{(\varepsilon N R)}\right) \) space, matching the space lower bound established for time-based model.
Table~\ref{table:comparison} summarize the comparison of space complexity of existing AMM algorithms over sliding windows.
% Additionally, its time complexity of $O\left(\frac{m_x + m_y}{\varepsilon} \log{R}\right)$ is more efficient than existing algorithms.
We also propose an improved algorithm that replaces the multi-level structure with a dynamic adjustment technique, resulting in enhanced performance in experiments and eliminating the need for prior knowledge of $R$. Our main contributions are summarized as follows:
\begin{itemize}
    \item We develop  the DS-COD algorithm to address the AMM over sliding window problem, providing both error bounds and a detailed analysis of space and time complexities.
    \item We establish the lower bounds on the space required by any deterministic algorithm to achieve an $\varepsilon$-approximation error bound in both sequence-based and time-based AMM over sliding window problem. The lower bounds demonstrate that our DS-COD algorithm is optimal in terms of the space-error tradeoff.
    \item Additionally, we propose an enhanced algorithm, aDS-COD, with a dynamic structure that significantly improves the practical computational efficiency, demonstrating strong and stable performance in experiments.
    \item We conduct extensive experiments on both synthetic and real-world datasets, validating our theoretical analysis and demonstrating the effectiveness of our methods.
\end{itemize}
