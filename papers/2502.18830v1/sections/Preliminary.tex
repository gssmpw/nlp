In this section, we first present the notation and the problem definition. Then we introduce the COD algorithm and its theoretical guarantees.

\subsection{Notations}
Let $\mI_n$ denote the identity matrix of size $n \times n$, and $\mathbf{0}_{m \times n}$ represent the $m \times n$ matrix filled with zeros. A matrix $\mX$ of size $m \times n$ can be expressed as $\mX = [\vx_1, \vx_2, \dots, \vx_n]$, where each $\vx_i \in \mathbb{R}^m$ is the $i$-th column of $\mX$. The notation $[\mX_1\quad \mX_2]$ represents the concatenation of matrices $\mX_1$ and $\mX_2$ along their column dimensions. For a vector $\vx \in \mathbb{R}^d$, we define its $\ell_2$-norm as $\|\vx\| = \sqrt{\sum_{i=1}^d x_i^2}$. For a matrix $\mX \in \mathbb{R}^{m \times n}$, its spectral norm is defined as $\|\mX\|_2 = \max_{\vu: \|\vu\| = 1} \|\mX \vu\|$, and its Frobenius norm is $\|\mX\|_F = \sqrt{\sum_{i=1}^n \|\vx_i\|^2}$, where $\vx_i$ is the $i$-th column of $\mX$. The condensed singular value decomposition (SVD) of $\mX$, written as SVD$(\mX)$, is given by $\mU \mSigma \mV^\top$, where $\mU \in \mathbb{R}^{m \times r}$ and $\mV \in \mathbb{R}^{n \times r}$ are orthonormal column matrices, and $\mSigma$ is a diagonal matrix containing the nonzero singular values $\sigma_1(\mX) \geq \sigma_2(\mX) \geq \dots \geq \sigma_r(\mX) > 0$. The QR decomposition of $\mX$, denoted as QR$(\mX)$, is given by $\mQ \mR$, where $\mQ \in \mathbb{R}^{m \times n}$ is an orthogonal matrix with orthonormal columns, and $\mR \in \mathbb{R}^{n \times n}$ is an upper triangular matrix. The LDL decomposition is a variant of the Cholesky decomposition that decomposes a positive semidefinite symmetric matrix \( \mX \in \mathbb{R}^{n\times n}\) into \( \mL \mD \mL^\top = \operatorname{LDL}(\mX) \), where \( \mL \) is a unit lower triangular matrix and \( \mD \) is a diagonal matrix. By defining \( \bar{\mL} = \sqrt{\mD} \mL^\top \), we obtain the triangular matrix decomposition \( \mX = \bar{\mL} \bar{\mL}^\top \).
%The columns of $\mQ$ form an orthonormal basis for the column space of $\mX$, and $\mR$ contains the coefficients that represent $\mX$ in this orthonormal basis.
% We let $\mI_n$ be the $n \times n$ identity matrix, and $\bf{0}_{m \times n}$ be the $m \times n$ matrix of all zeros. We can denote a $m \times n$ matrix as $\mX = [\vx_1, \vx_2, \dots, \vx_n]$, where $\vx_i \in \BR^{m}$ is the $i$-th column of $\mX$. We use $[ \mX_1, \mX_2 ] $ to denote their concatenation
% on their column dimensions. For a vector $\vx\in\BR^d$, we let $\norm{\vx}=\sqrt{\sum_{i=1}^d x_i^2}$ be its $\ell_2$-norm. For a matrix $\mX\in\BR^{m\times n}$, we let $\Norm{\mX} =  \max_{\vu:\Norm{\vu} = 1}\Norm{\mX \vu}$ be its spectral norm and  $\Norm{\mX}_F = \sqrt{\sum_{i = 1}^{n}{\Norm{\vx_i}^2}}$ be its Frobenius norm. The condensed singular value decomposition (SVD) of matrix $\mX$, written as SVD$(\mX)$, is defined as $\mU \mSigma \mV^T$ where $\mU \in \BR^{m \times r}$ and $\mV \in \BR^{n \times r}$ are column orthonormal and $\mSigma$ is a diagonal matrix with nonzero singular values $\sigma_1(\mX) \geq \sigma_2(\mX) \geq \dots \geq \sigma_r(\mX)>0$. We use $\textrm{nnz}(\mX)$ to denote number of nonzero elements of matrix $\mX$.

\subsection{Problem Setup}
We first provide the definition of correlation sketch as follows:
\begin{defn}[\cite{mroueh2017co}]
Let $\mX \in \mathbb{R}^{m_x \times n}$, $\mY \in \mathbb{R}^{m_y \times n}$, $\mA \in \mathbb{R}^{m_x \times \ell}$ and $\mB \in \mathbb{R}^{m_y \times \ell}$ where $n\ge\max(m_x,m_y)$ and $\ell \leq \min(m_x,m_y)$. We call the pair $(\mA,\mB)$ is an $\varepsilon$-correlation sketch of $(\mX,\mY)$ if the correlation error satisfies
\[ \text{corr-err}\left(\mX \mY^\top, \mA \mB^\top\right)\triangleq\frac{\norm{\mX\mY^\top-\mA\mB^\top}}{\Norm{\mX}_F \Norm{\mY}_F}\leq \varepsilon. \]
\end{defn}
This paper addresses the problem of approximate matrix multiplication (AMM) in the context of sliding windows. At each time step $t$, the algorithm receives column pairs $(\vx_t, \vy_t)$ from the original matrices $\mX$ and $\mY$. Let $N$ denote the window size. The submatrices within current window are denoted as $\mX_W$ and $\mY_W$. The goal of the algorithm is to maintain a pair of low-rank matrices $(\mA, \mB)$, which is an $\varepsilon$-correlation sketch of the matrices $(\mX_W, \mY_W)$. Similar to \cite{wei2016matrix}, we assume that the squared norms of the data columns are normalized to the range \([1, R]\) for both \( \mX \) and \( \mY \). Therefore, for any column pair \( (\vx, \vy) \), the condition \( 1 \leq \|\vx\| \|\vy\| \leq R \) holds.


\subsection{Co-occurring Directions}
Co-occurring directions (COD)~\cite{mroueh2017co} is a deterministic algorithm for correlation sketching. The core step of COD are summarized in Algorithm \ref{alg:cs}, which we call it the correlation shrinkage (CS) procedure.
\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Correlation Shrinkage (CS)}
	\label{alg:cs}
	\begin{algorithmic}[1]
        \Require
            $\mathbf{A} \in \mathbb{R}^{m_x \times \ell'}, \mathbf{B} \in \mathbb{R}^{m_y \times \ell'}, \text{sketch size }\ell $.
        \State $[\mathbf{Q}_x, \mathbf{R}_x] \leftarrow \text{QR}(\mathbf{A})$,
         $[\mathbf{Q}_y, \mathbf{R}_y] \leftarrow \text{QR}(\mathbf{B})$.
        \State $[\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}] \leftarrow \text{SVD}(\mathbf{R}_x \mathbf{R}_y^\top)$.
        \State $\mC \leftarrow \mQ_x\mU\sqrt{\mathbf{\Sigma}},\mD \leftarrow \mQ_y\mV\sqrt{\mathbf{\Sigma}}$ \Comment{$\mC$ and $\mD$ not computed.}
        \State $\delta \leftarrow \sigma_{\ell} (\mathbf{\Sigma})$,
        $\mathbf{\hat{\Sigma}} \leftarrow \text{max}(\mathbf{\Sigma} - \delta \mathbf{I}_{\ell'}, \mathbf{0})$.
        \State $\mathbf{A} \leftarrow \mathbf{Q}_x \mathbf{U} \sqrt{\mathbf{\hat{\Sigma}}}$,  $\mathbf{B} \leftarrow \mathbf{Q}_y \mathbf{V} \sqrt{\mathbf{\hat{\Sigma}}}$.
        \Ensure 
            $\mathbf{A} \text{ and } \mathbf{B}$.
	\end{algorithmic}  
\end{algorithm}

The COD algorithm initially set $\mA = \mathbf{0}_{m_x \times \ell}$ and $\mB = \mathbf{0}_{m_y \times \ell}$. Then, it processes the i-th column of X and Y as follows
\begin{flalign}
    &\text{Insert $\vx_i$ into a zero valued column of $\mA$} \nonumber\\
    &\text{Insert $\vy_i$ into a zero valued column of $\mB$} \nonumber\\
    &\text{\textbf{if} $\mA$ or $\mB$ has no zero valued columns \textbf{then}} \nonumber\\
    &\text{\quad\quad$[\mA,\mB] = \operatorname{CS}(\mA,\mB, \ell/2)$} \nonumber
\end{flalign}


The COD algorithm runs in $O(n(m_x + m_y)\ell)$ time and requires a space of $O((m_x+m_y)\ell)$. It returns the final sketch $\mA\mB^\top$ with correlation error bounded as:
\[
\norm{\mX\mY^\top-\mA\mB^\top} \leq \frac{2}{\ell}\|\mX\|_F\|\mY\|_F.
\]

For the convenience of expression, we present the following definition:
\begin{defn}
% We call matrix pair $(\mC,\mD)$ is an aligned pair of $(\mA,\mB)$ if it satisfies $\mC\mD^\top=\mA\mB^\top$, $\mC=\bar{\mU}\bar{\mSigma}$ and $\mD=\bar{\mV}\bar{\mSigma}$, where $\bar{\mU}$ and $\bar{\mV}$ are orthonormal matrices and $\bar{\mSigma}$ is a diagonal matrix with descending diagonal elements.
We call matrix pair $(\mC,\mD)$ is an aligned pair of $(\mA,\mB)$ if it satisfies $\mC\mD^\top=\mA\mB^\top$, $\mC=\mQ_x\mU\sqrt{\mathbf{\Sigma}}$ and $\mD=\mQ_y\mV\sqrt{\mathbf{\Sigma}}$, where $\mQ_x$, $\mQ_y$, $\mU$ and $\mV$ are orthonormal matrices and $\mathbf{\Sigma}$ is a diagonal matrix with descending diagonal elements.
\end{defn}
Notice that the line 1-3 of the CS procedure generate an aligned pair $(\mC,\mD)$ for $(\mA,\mB)$. In addition, The output of CS algorithm is a shrinked variant of the aligned pair.

%Here we define a new concept: reconstructed multipliers. With the symbols used in the CS algorithm, we have $\mA\mB^\top=\mQ_x\mU\mathbf{\Sigma}\mV^\top\mQ_y^\top = \mC\mD^\top$. We refer to the pair of matrices $\mC = \mQ_x\mU\sqrt{\mathbf{\Sigma}}$ and $\mD = \mQ_y\mV\sqrt{\mathbf{\Sigma}}$ as the reconstructed multipliers of $\mA\mB^\top$. The columns of $\mC$ and $\mD$ are orthogonal and sorted in descending order of their norms ($\|\vc_i\|=\|\vd_i\| \geq \|\vc_{i+1}\|= \|\vd_{i+1}\|$). The output of CS algorithm is a shrinked variant of the reconstructed multipliers. From a global perspective, for our target $\mX\mY^\top = \mQ_x \mU \mathbf{\Sigma} \mV^\top \mQ_y^\top$,  
% (where $[\mQ_x, \mR_x] = \operatorname{QR}(\mX)$, $[\mQ_y, \mR_y] = \operatorname{QR}(\mY)$, and $[\mU, \mathbf{\Sigma}, \mV] = \operatorname{SVD}(\mR_x \mR_y^\top)$),
%the approximate result provided by COD is $\mA\mB^\top = \mQ_x \mU \hat{\mathbf{\Sigma}} \mV^\top \mQ_y^\top$. We refer to $\mU \mathbf{\Sigma} \mV^\top$ as the product core of $\mX\mY^\top$, and correspondingly, $\mU \hat{\mathbf{\Sigma}} \mV^\top$ as the product core of $\mA\mB^\top$.