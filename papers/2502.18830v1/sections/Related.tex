In this section, we present the background and related algorithms for AMM over sliding window.
\paragraph{Streaming AMM}
Over the past few decades, extensive research on AMM has led to the development of various approaches. Randomized methods, such as random sampling~\cite{drineas2006fast} and random projection~\cite{sarlos2006improved, cohen2015optimal}, are simple and effective algorithms that offer better time complexity. Deterministic algorithms, such as FD-AMM~\cite{ye2016frequent} and COD~\cite{mroueh2017co}, base on singular value decomposition and generally achieve smaller approximation errors. In recent years, several variants and extensions of the COD algorithm have been developed. \citet{wan2021approximate} and \citet{luo2021revisiting} proposed optimizations for the sparse data, which improved the costs of time and space. \citet{blalock2021multiplying} introduced learning-augmented methods, providing a better speed-quality tradeoff. \citet{francis2022practical} proposed a variant that is more robust to noise.
\paragraph{Sliding window methods}
The sliding window problem has been a long-standing research focus. One of the most well-known frameworks, proposed by \citet{datar2002maintaining}, uses exponential histograms. By creating a logarithmic hierarchy, it effectively approximates various statistical tasks within the window, such as counting, summing, and computing vector norms. \citet{arasu2004approximate} introduced a tree-based framework with significant practical value. \citet{lee2006simpler} developed a sampling method named $\lambda$-snapshot to approximate frequent items in the sliding window. \citet{braverman2020near} applied random sampling to solve numerical linear algebra problems like spectral and low-rank approximations in the sliding window, achieving near-optimal space efficiency. \citet{wei2016matrix} were the first to study matrix sketching over sliding windows, offering an $\varepsilon$-error approximation using logarithmic space. Recently, \citet{yin2024optimal} proposed a new matrix sketching algorithm based on the $\lambda$-snapshot idea, optimizing matrix sketching over sliding windows.

\paragraph{AMM over sliding windows}
Research on AMM in sliding window models is limited. \citet{yao2024approximate} combined classic sliding window frameworks with AMM, presenting the first comprehensive solution. The EH-COD algorithm, based on the Exponential Histogram idea~\cite{datar2002maintaining}, approximates the matrix product within the window by dividing it into blocks and creating a logarithmic hierarchy with exponentially increasing norms through merge operations. EH-COD works for both sequence-based and time-based windows, requiring $O\left(\frac{m_x+m_y}{\varepsilon^2}\log{(\varepsilon NR)}\right)$ space for sequence-based windows. Its amortized runtime per column is $O\left(\frac{m_x+m_y}{\varepsilon}\log{(\varepsilon NR)}\right)$.

The DI-COD algorithm, using the Dyadic Interval method~\cite{arasu2004approximate}, maintains $L = \log{\frac{R}{\varepsilon}}$ parallel levels to approximate the matrix product with different block granularities. Its space complexity is $O\left(\frac{m_x+m_y}{\varepsilon}R\log^2{\frac{R}{\varepsilon}}\right)$, with an amortized runtime of $O\left(\frac{m_x+m_y}{\varepsilon}\log{\frac{R}{\varepsilon}}\right)$. DI-COD offers better space efficiency than EH-COD when $R$ is small. However, it requires prior knowledge of $R$ and cannot handle variable-size windows, limiting its application to time-based windows. While both EH-COD and DI-COD effectively solve the AMM over sliding window problem, their space efficiency is not optimal.

