In this section, we present a novel method that addresses AMM over sliding window, termed Dump Snapshots Co-occurring Directions (DS-COD) which leverage the idea from the $\lambda$-snapshot method~\cite{lee2006simpler} for the frequent item problem in the sliding window model and combine it with the COD algorithm. The main idea of our DS-COD method in figure \ref{fig:idea}. Notice that in the COD algorithm, each CS procedure produces a shrinked aligned pair $\mQ_x \mU \sqrt{\hat{\mathbf{\Sigma}}}$ and $\mQ_y \mV \sqrt{\hat{\mathbf{\Sigma}}}$. Our idea is to examine whether the singular values of $\hat{\mathbf{\Sigma}}$ is large enough. Once a singular value $\sigma_j$ exceeds the threshold $\theta$, we dump the corresponding columns of the aligned pair, i.e. $\sqrt{\sigma_j} \mQ_x \vu_j$ and $\sqrt{\sigma_j} \mQ_y \vv_j$, attaching a timestamp to form a snapshot. %We first consider the case where all columns of the input matrices are normalized and describe our DS-COD algorithm in Section~\ref{sec:alg}. Then we show how to extend DS-COD to the case where the squared norms of the columns within the range $[1,R]$.

% \paragraph{Snapshot Method.} The $\lambda$-snapshot method~\cite{lee2006simpler} extends the classical MG algorithm to the sliding window model. The main idea is that once an item appears more than $\varepsilon N$ times, it is recorded with the current timestamp as a snapshot, and these snapshots are then promptly expired within a sliding window.
% \paragraph{Sketching over Sliding Window.}~\citet{yin2024optimal} combined Frequent Directions(FD) with the $\lambda$-snapshot method. For the matrix $\mA = \mU \mathbf{\Sigma} \mV^\top$, FD maintains an approximate sketch $\mB = \hat{\mathbf{\Sigma}} \mV^\top$. For each FD update, after performing $[\mU, \mathbf{\Sigma}, \mV] = \operatorname{SVD}([\mB_{t-1}; \va_t])$, the $j$-th row of $\mB$ is dumped as a snapshot if the $j$-th singular value exceeds a threshold $\theta$.
% \paragraph{AMM over Sliding Window.} 

% The product core in this snapshot, $\mQ_x \vu_j \sigma_j \vv_j^\top \mQ_y^\top$, is exactly a rank-1 component of the original product core. What we need to do is to promptly dump the sufficiently large rank-1 components from the matrix product core as snapshots. 
%Similar to frequency estimation and sketching problems over sliding windows, timeliness is crucial in this process; otherwise, expiration timestamps of the snapshots will become inaccurate, leading to increased error.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/idea.pdf}
    \caption{The illustration of Dump Snapshots Co-occurring Directions.}
    \label{fig:idea}
\end{figure}

\subsection{Algorithm Description} \label{sec:alg}

\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{DS-COD:Initialize($\ell,\theta$)}
	\label{alg:initialize}
	\begin{algorithmic}[1]
        \Require
            The sketch size $\ell$ and dump threshold $\theta$.
        \State Residual sketches $\mA \leftarrow \mathbf{0}_{m_x\times2\ell},\mB \leftarrow \mathbf{0}_{m_y\times2\ell}$.
        \State Covariance matrices $\mK_A \leftarrow \mathbf{0}_{2\ell\times2\ell},\mK_B \leftarrow \mathbf{0}_{2\ell\times2\ell}$.
        \State Queue of snapshots $S\leftarrow \text{queue}.\operatorname{Initialize}()$.
        \State Variable $\psi \leftarrow 0$.
        
	\end{algorithmic}  
\end{algorithm}


% Fast-DS-COD
\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{DS-COD: Update($\vx_i,\vy_i$)}
	\label{alg:update}
	\begin{algorithmic}[1]
        \Require column pair $\vx_i,\vy_i$, current timestamp $i$%, sketch size $\ell$ and threshold $\theta$.
        % \While{$S[0].t+N \leq i$}
        %     \State $S.\operatorname{POPLEFT}()$.
        % \EndWhile
        \State $\psi \leftarrow \psi+\|\vx_i\|\|\vy_i\|$.
        \If{$\operatorname{columns}(\mA)+1\geq 2\ell$}
            \State $\mA,\mB \leftarrow \operatorname{CS}([\mA \quad \vx_i],[\mB \quad \vy_i],\ell)$.
             \While{$\|\va_1\|\|\vb_1\|\geq \theta$}
                \State $S$ append snapshot$(v=(\va_1,\vb_1),t=i)$.
                \State Remove $\va_1,\vb_1$ from $\mA,\mB$ respectively.
            \EndWhile
            \State $\mK_A=\mA^\top\mA,\mK_B=\mB^\top\mB$.
            \State $\psi \leftarrow \|\va_1\|\|\vb_1\|$.
        \Else
            \State $\mK_A\leftarrow\begin{bmatrix}\mK_A&\mA^\top\vx_i\\\vx_i^\top\mA&\vx_i^\top\vx_i\end{bmatrix},\mK_B\leftarrow\begin{bmatrix}\mK_B&\mB^\top\vy_i\\\vy_i^\top\mB&\vy_i^\top\vy_i\end{bmatrix}$.
            \State $\mA\leftarrow [\mA;\vx_i],\mB\leftarrow [\mB;\vy_i]$.
            \If{$\psi \geq \theta$}
                 % \State $[\mathbf{\Lambda}_A,\mV_A]\leftarrow \operatorname{eigenvalue}(\mK_A)$.
                 % \State $[\mathbf{\Lambda}_B,\mV_B]\leftarrow\operatorname{eigenvalue}(\mK_B)$.
                 \State $[\mL_A,\mD_A]\leftarrow \operatorname{LDL}(\mK_A), [\mL_B,\mD_B]\leftarrow \operatorname{LDL}(\mK_B)$.
                 \State $\mR_x \leftarrow \sqrt{\mD_A}\mL_A^\top,\mR_y \leftarrow \sqrt{\mD_B}\mL_B^\top$.
                \State $[\mU,\mathbf{\Sigma},\mV] \leftarrow \operatorname{SVD}(\mR_x\mR_y^\top)$.
                \For{$j = 1,2,\dots,\ell$}
                    \If{$\sigma_j \geq \theta$}
                        \State $\va_j\leftarrow \frac{1}{\sqrt{\sigma_j}}\mA\mR_y^{T}\vv_j,\vb_j\leftarrow \frac{1}{\sqrt{\sigma_j}}\mB\mR_x^{T}\vu_j$.
                        \State $S$ append snapshot$(s=(\va_j,\vb_j),t=i)$.
                        \State $\mP_A\leftarrow \frac{1}{\sigma_j}\mR_y^{T}\vv_j\vu_j^\top\mR_x,\mP_B\leftarrow \frac{1}{\sigma_j}\mR_x^{T}\vu_j\vv_j^\top\mR_y$.
                        \State $\mA \leftarrow \mA - \mA\mP_A,\mB \leftarrow \mB - \mB\mP_B$.
                        \State $\mK_A \leftarrow (\mI-\mP_A^\top)\mK_A(\mI-\mP_A)$.
                        \State $\mK_B \leftarrow (\mI-\mP_B^\top)\mK_B(\mI-\mP_B)$.
                    \Else
                        \State BREAK.
                    \EndIf
                \EndFor
            \EndIf
            \State $\psi \leftarrow \sigma_j$.
            
        \EndIf
        
        \Ensure $\mA,\mB$ and $S$.
	\end{algorithmic}  
\end{algorithm}

 

Algorithm \ref{alg:initialize} illustrates the initialization of a DS-COD sketch. The DS-COD sketch maintains two residual matrices, \( \mA \in \mathbb{R}^{m_x \times 2\ell} \) and \( \mB \in \mathbb{R}^{m_y \times 2\ell} \), to receive newly arriving columns. \( \mK_A \in \mathbb{R}^{2\ell \times 2\ell} \) and \( \mK_B \in \mathbb{R}^{2\ell \times 2\ell} \) represent the covariance matrices of \( \mA \) and \( \mB \), respectively. The snapshot queue \( \mathcal{S} \) is initialized as empty, and the variable \( \psi \) is initialized to 0 to track the maximum singular value of \( \mA \mB^\top \).
Algorithm \ref{alg:update} outlines the pseudocode for updating a DS-COD sketch. The update process involves two cases: perform a CS operation once the buffer is full, or do a quick-check when the buffer has fewer than $2\ell$ columns. These correspond to lines 2-8 (case 1) and lines 10-26 (case 2), respectively. It is important to emphasize that timely snapshot dumping is essential. One direct approach is to perform a CS operation after each update. However, since the time to run a CS procedure is $O((m_x + m_y)\ell^2)$, frequent CS operations are prohibitive. Therefore, in case 2 we introduces a quick-check approach.
The DS-COD algorithm maintains the covariance matrices \( \mK_A = \mA^\top \mA \) and \( \mK_B = \mB^\top \mB \), updating them in a rank-1 manner (line 10). Since $\mA^\top \mA = \mR_x^\top \mQ_x^\top \mQ_x \mR_x = \mR_x^\top \mR_x$, the LDL decomposition of $\mK_A$ can quickly yields $\mR_x=\sqrt{\mD}\mL^\top$. Also, $\mR_y$ can be obtained by the same process (line 14). By this way we can avoid time-consuming QR decomposition.%, but it requires using some less intuitive expressions to replace the uncalculated $\mQ_x$ and $\mQ_y$.
% Next, we compute the eigenvalue decomposition of these covariance matrices to construct "\( \mR_x \)" and "\( \mR_y \)" (lines 13-15). This process is essentially equivalent to performing SVD on matrix \( \mA \): \( [\mU_A, \mathbf{\Sigma}_A, \mV_A] \leftarrow \operatorname{SVD}(\mA) \), where we set \( \mR_x \leftarrow \mathbf{\Sigma}_A \mV_A^\top \). The difference between this "\( \mR_x \)" and the \( \mR_x \) obtained from the QR decomposition of \( \mA \) is that the former is not an upper triangular matrix. However, both matrices contain the same column structure and rank information of \( \mA \). Replacing the QR decomposition in COD with SVD does not affect its error bounds or complexity. For consistency, we continue to use the notation \( \mQ_x \) to represent \( \mU_A \) and \( \mR_x \) to represent \( \mathbf{\Sigma}_A \mV_A^\top \).

 Lines 16-23 inspect the singular values one by one, compute the snapshots to be dumped, and eliminate their influence from the buffers $\mA$, $\mB$, and the corresponding covariance matrices $\mK_A$, $\mK_B$. Recall that a snapshot consists of columns from the aligned pair. However, since the quick-check does not compute the complete aligned pair, we need to calculate the required columns individually. For instance, if $\sigma_1$ exceeds the threshold $\theta$, then the snapshot takes the first column of the aligned pair of $\mA\mB^\top$, which should be $\sqrt{\sigma_1} \mQ_x \vu_1$ and $\sqrt{\sigma_1} \ \mQ_y \vv_1$. 
In the absence of $\mQ_x$ and $\mQ_y$, we use an alternative expression for the aligned pair (instead of $\mathbf{A} \mR_x^{-1} \mathbf{U} \sqrt{\mathbf{\Sigma}}$ to save time on matrix inversion): 
\begin{align} \label{eq:rm}
\mQ_x\mU\sqrt{\mathbf{\Sigma}}=\mathbf{A}\mathbf{R}_y^\top\mathbf{V}\sqrt{\mathbf{\Sigma}}^{-1},\quad \mQ_y\mV\sqrt{\mathbf{\Sigma}}=\mathbf{B}\mathbf{R}_x^\top\mathbf{U}\sqrt{\mathbf{\Sigma}}^{-1}.
\end{align}
This demonstrates the form of the snapshot as described in line~18.
If we had the aligned pair, we could simply remove the first column to complete the dump. However, since we only have the original matrices $\mA$ and $\mB$, removing the snapshot's impact from them presents significant challenges. To reconsider the problem: we have buffers $\mA$ and $\mB$, and suppose $\mC$ and $\mD$ are the aligned pair of $\mA \mB^\top$, such that $\mA \mB^\top = \mQ_x \mU \mathbf{\Sigma} \mV^\top \mQ_y^\top = \mC \mD^\top$. Let $\overline{\mC}$ and $\overline{\mD}$ represent the results of removing the first column from $\mC$ and $\mD$, respectively. The remain question is how to update $\mA$ and $\mB$ so that $\overline{\mA}_{} \overline{\mB}_{}^\top = \overline{\mC}_{} \overline{\mD}_{}^\top$. The approach is provided in lines 20-21, and we explain it in the proof of Lemma \ref{lem:remove} (in appendix \ref{apdx:remove}).

\begin{lem} \label{lem:remove}
    Let $[\mQ_x,\mR_x]=\operatorname{QR}(\mA)$, $[\mQ_y,\mR_y]=\operatorname{QR}(\mB)$
    , \( [\mU, \mathbf{\Sigma}, \mV] = \operatorname{SVD}(\mR_x \mR_y^\top) \), \( \mC = \mQ_x \mU \sqrt{\mathbf{\Sigma}} \) and \( \mD = \mQ_y \mV \sqrt{\mathbf{\Sigma}} \). Then $(\mC,\mD)$ is an aligned pair of $(\mA,\mB)$. If we construct $\overline{\mC}$ and $\overline{\mD}$ by removing the \( j \)-th column of $\mC$ and $\mD$, respectively, then $(\overline{\mC},\overline{\mD})$ is an aligned pair of of $(\overline{\mA},\overline{\mB})$, where
    %their aligned pair is equivalent
    
    %Then, updating \( \mA \) and \( \mB \) by removing the effect of the \( j \)-th column of their aligned pair is equivalent to:
    \[
    \overline{\mA} = \mA - \mQ_x \vu_j \vu_j^\top \mR_x, \quad \overline{\mB} = \mB - \mQ_y \vv_j \vv_j^\top \mR_y.
    \]
\end{lem}
According to \eqref{eq:rm}, we obtain $\mQ_x = \mathbf{A} \mathbf{R}_y^\top \mathbf{V} \mathbf{\Sigma}^{-1} \mathbf{U}^\top$ and $\mQ_y = \mathbf{B} \mathbf{R}_x^\top \mathbf{U} \mathbf{\Sigma}^{-1} \mathbf{V}^\top$. Then, we can express the update as follows:
\[
\mA - \mQ_x \vu_j \vu_j^\top \mR_x = \mA - \mathbf{A} \mathbf{R}_y^\top \mathbf{V} \mathbf{\Sigma}^{-1} \mathbf{U}^\top \vu_j \vu_j^\top \mR_x = \mA - \frac{1}{\sigma_j} \mathbf{A} \mathbf{R}_y^\top \vv_j \vu_j^\top \mR_x.
\]
Similarly, the update for $\mB$ is $\mB - \frac{1}{\sigma_j}\mB \mathbf{R}_x^\top \vu_j \vv_j^\top \mR_y$. In line 23-24, we update $\mK_A$ to $\overline{\mA}^\top \overline{\mA}$ and $\mK_B$ to $\overline{\mB}^\top \overline{\mB}$.

Note that the quick-check is not performed after every update. The variable $\psi$ tracks the residual maximum singular value after snapshots dumped (line 26). During updates, the norm product $\|\vx\| \|\vy\|$ is accumulated into $\psi$ (line 1). Suppose the residual maximum singular value is $\sigma$. After receiving columns $\vx$ and $\vy$, let the aligned pair be $\mC$ and $\mD$, satisfying $\mC \mD^\top = \mA\mB^\top + \vx\vy^\top$. By definition, we have:
\begin{align*}
\|\vc_1\|\|\vd_1\| = \sigma_1(\mA\mB^\top + \vx\vy^\top) \leq \sigma_1(\mA\mB^\top) + \sigma_1(\vx\vy^\top) 
=\sigma + \|\vx\|\|\vy\|.
\end{align*}

Therefore, A snapshot dump only occurs when $\psi$ exceeds the threshold $\theta$.

\subsection{Hierarchical DS-COD}
We now introduce the complete DS-COD algorithm framework (in Algorithm \ref{alg:hdscod}) for sequence-based sliding window  and explain how to use appropriate thresholds $\theta$ to generate enough snapshots to meet the required accuracy, while keeping space usage limited.
\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Hierarchical DS-COD (hDS-COD)}
	\label{alg:hdscod}
	\begin{algorithmic}[1]
        \Require
            $\mX \in \mathbb{R}^{m_x \times n},\mY \in \mathbb{R}^{m_y \times n}$,  window size $N$, sketch size $\ell$.
        \renewcommand{\algorithmicrequire}{\textbf{Initialize:}}
        \Require
            Initialize $L+1$ DS-COD sketches in $\mathcal{L}[0]$ to $\mathcal{L}[L]$, with exponentially increasing threshold : $\varepsilon N, 2\varepsilon N,  \dots, 2^L\varepsilon N$. Simultaneously, configure a set of auxiliary sketches \( \mathcal{L}_{aux}[0] \) to \( \mathcal{L}_{aux}[L] \) with the same settings.
        \For{$ i =1,2,\dots,n$}
            \For{$j =0,1,\dots,L$}
                \While{$\mathcal{L}[j].S[0].t + N \leq i$ \textbf{or} $\operatorname{len}(\mathcal{L}[j].S) > \frac{1}{\varepsilon}$}
                    \State $\mathcal{L}[j].S.\operatorname{Dequeue}()$.
                \EndWhile
                \State $\mathcal{L}[j].\operatorname{Update}(\vx_i,\vy_i)$.
                \State $\mathcal{L}_{aux}[j].\operatorname{Update}(\vx_i,\vy_i)$.
                \If{$i \operatorname{mod} N == 1$}
                    \State $\mathcal{L}[j] \leftarrow \mathcal{L}_{aux}[j]$.
                    \State $\mathcal{L}_{aux}[j].\operatorname{Initialize}()$.
                \EndIf
            \EndFor
        \EndFor
	\end{algorithmic}  
\end{algorithm}

Since the squared norms of the columns may vary across different windows but always stay within the range $[1, R]$, we use hierarchical thresholds to capture this variation. 
% If the threshold is too large, the number of snapshots will be insufficient to achieve the required accuracy; if it is too small, too many snapshots will be generated. An appropriate threshold ensures $O(1/\varepsilon)$ snapshots are dumped. 
The algorithm constructs a structure with \( L = \lceil \log_2{R} \rceil \) levels, where the thresholds increase exponentially. Specifically, the threshold at the \( j \)-th level is \( 2^j \varepsilon N \), meaning that lower levels have a higher frequency of snapshot dumping. To prevent lower levels from generating excessive snapshots, potentially up to \( N \), we cap the total number of snapshots in a DS-COD sketch at \( 1/\varepsilon \).
% The Frobenius norm of the matrix product within window $W$ is bounded as:
% \begin{align*}
% \|\mX_W \mY_W^\top\|_F \leq \sum_{i=1}^N \|\vx_i\|\|\vy_i\| = \begin{cases} N & \text{if } \|\vx_i\| \|\vy_i\| = 1 \text{ (extreme case 1)}, \\ N R & \text{if } \|\vx_i\| \|\vy_i\| = R \text{ (extreme case 2)}. \end{cases}    
% \end{align*}

% In extreme case 1, \( \mathcal{L}[0] \) with a threshold of \( \varepsilon N \) produces at most \( \frac{N}{\varepsilon N} = O\left(\frac{1}{\varepsilon}\right) \) snapshots. In extreme case 2, \( \mathcal{L}[L] \) with a threshold of \( \varepsilon NR \) is appropriate. For all other cases in between, there is always one level with a suitable threshold for the sketch. However, 
The algorithm begins by discarding expired or excessive snapshots from the head of the queue \( S \) (lines 3-4), and then proceeds with the updates. We store the primary components of the aligned pair with timestamps as snapshots, while the components in the residual matrices \( \mA \) and \( \mB \) also require expiration handling to maintain the algorithm's error bounds. These residual matrices are managed using a coarse expiration strategy, referred to as $N$-restart. Each level also includes an auxiliary sketch \( \mathcal{L}_{aux} \), which is updated alongside the main sketch. After every \( N \) update steps, the algorithm swaps the contents of \( \mathcal{L} \) and \( \mathcal{L}_{aux} \), and then reinitializes the auxiliary sketch (lines 7-9).
\paragraph{Query Algorithm} The query algorithm of Hierarchical DS-COD is presented in Algorithm \ref{alg:query}. 
% Choosing the layer that contains the maximum number of snapshots is generally a good strategy.
Since lower layers may not encompass the entire window, the query process starts from the top and searches for a layer with at least $\Omega(\ell)$ non-expiring snapshots. The snapshots are then merged with the residual matrices, and the final aligned pair are obtained using the CS operation.

\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Hierarchical DS-COD: Query($\mathcal{L}$)}
	\label{alg:query}
	\begin{algorithmic}[1]
        \Require
            sketch sequence $\mathcal{L}$.
%         \For{j∈[L,0,−1]j \in [L,0,-1]}
%             \If{len(L[j].S)≥Ω(ℓ)\operatorname{len}(\mathcal{L}[j].S) \geq \Omega(\ell)}
%                 \State L←L[j]\mathbb{L} \leftarrow \mathcal{L}[j].
%                 \State BREAK.
%             \EndIf
%         \EndFor
%         \For{i∈[0,len(L.S])i \in [0,\operatorname{len}(\mathbb{L}.S])}
%             \State \mA′←[\mA′;L.S[i].\va]\mA' \leftarrow [\mA';\mathbb{L}.S[i].\va].
%             \State \mB′←[\mB′;L.S[i].\vb]\mB' \leftarrow [\mB';\mathbb{L}.S[i].\vb].
%         \EndFor
%         \State [\mA,\mB]←CS([L.\mA;\mA′],[L.\mB,\mB′],ℓ)[\mA,\mB] \leftarrow \operatorname{CS}([\mathbb{L}.\mA;\mA'],[\mathbb{L}.\mB,\mB'],\ell).
        \State Find the $\max_j{\operatorname{len}(\mathcal{L}[j].S]) \geq \Omega(\ell)}$.
        \State Stack snapshots in $\mathcal{L}[j].S$ to form $\mA'$ and $\mB'$.
        \State $[\mA,\mB] \leftarrow \operatorname{CS}([\mathcal{L}[j].\mA \quad \mA'],[\mathcal{L}[j].\mB \quad \mB'],\ell)$.
        \Ensure
            $\mA$ and $\mB$.
\end{algorithmic}  
\end{algorithm}


%\subsection{Analysis}
We present the following main results on the correlation error of hDS-COD in AMM over sliding window. The proof of error bound and complexity is given in appendix \ref{apdx:hds} and \ref{apdx:comlexity}, respectively.
\begin{thm} \label{thm:hds} 
    By setting \( \ell = \frac{1}{\varepsilon} \), the hDS-COD algorithm can generate sketches \( \mA \) and \( \mB \) of size \( O(\ell) \), with the correlation error bounded by
    \[
        \text{corr-err }\left(\mX_W\mY_W^\top,\mA\mB^\top\right) \leq 8\varepsilon.
    \]
    The hDS-COD algorithm requires  \( O\left( \frac{m_x + m_y}{\varepsilon} (\log{R}+1) \right) \) space cost and the amortized time cost per column is \( O\left( \frac{m_x + m_y}{\varepsilon} (\log{R}+1) \right) \).
\end{thm}


Compared to existing algorithms, hDS-COD provides a substantial improvement in space complexity. In contrast to EH-COD~\cite{yao2024approximate} which requires  $\frac{m_{x}+m_{y}}{\varepsilon^2} \log (\varepsilon NR)$ space cost, our hDS-COD algorithm reduces the the dependence on $\varepsilon$ from $\frac{1}{\varepsilon^2}$ to $\frac{1}{\varepsilon}$. In contrast to DI-COD~\cite{yao2024approximate} which requires $\frac{m_{x}+m_{y}}{\varepsilon} R \log^2{\frac{R}{\varepsilon}}$ space cost, our hDS-COD algorithm significantly reduce the dependence on $R$. On the other hand, hDS-COD also offers advantages in time complexity. Notably, the time complexities per column of EH-COD and DI-COD are  \( O(\frac{m_x + m_y}{\varepsilon} \log{(\varepsilon NR)}) \) and $O\left( \frac{m_x + m_y}{\varepsilon} \log \frac{R}{\varepsilon} \right)$, respectively. In comparison, hDS-COD maintains a competitive time complexity.

\subsection{Adaptive DS-COD}
The multilayer structure of hDS-COD effectively captures varying data patterns by selecting the appropriate level for each window, providing approximations with a bounded error. However, it requires prior knowledge of the maximum column squared norm $R$, and maintains $\log R$ levels of snapshot queues and residual sketches, with the update cost also increasing with $\log R$. In many cases, the dataset may have a large $R$ but little fluctuation within the window, leading to wasted space and unnecessary updates, especially at extreme levels. To address this issue, we propose a heuristic variant of hDS-COD, called adaptive DS-COD (aDS-COD), in Algorithm \ref{alg:adscod}.
\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Adaptive DS-COD (aDS-COD)}
	\label{alg:adscod}
	\begin{algorithmic}[1]
        % \Require
        %     column pair $\vx_i,\vy_i$ from $\mX$ and $\mY$ at current timestamp $i$.
        \Require
            $\mX \in \mathbb{R}^{m_x \times n},\mY \in \mathbb{R}^{m_y \times n}$,  window size $N$, sketch size $\ell$.
        \renewcommand{\algorithmicrequire}{\textbf{Initialize:}}
        \Require
            Initialize a DS-COD sketch $DS$ and an auxiliary sketch $DS_{aux}$ with initial threshold $\varepsilon N$ and threshold level $L=1$.
        \For{$ i =1,2,\dots,n$}
            \While{$ DS.S[0].t + N \leq i$}
                \State $DS.S.\operatorname{Dequeue}()$.
            \EndWhile
            \If{$i \operatorname{mod} N == 1$}
                \State $DS \leftarrow DS_{aux}$.
                \State $DS_{aux}.\operatorname{Initialize}()$.
            \EndIf
            \State $DS.\operatorname{Update}(\vx_i,\vy_i)$.
            \State $DS_{aux}.\operatorname{Update}(\vx_i,\vy_i)$.
    
            \If{$\operatorname{len}(DS.S) \geq \frac{ L}{\varepsilon}$}\Comment{adjust threshold}
                \State $L \leftarrow L + 1$.
                \State $DS.\theta \leftarrow 2\cdot DS.\theta $.
            \ElsIf{$\operatorname{len}(DS.S) \leq \frac{L-1}{\varepsilon}$}
                \State $L \leftarrow L - 1$.
                \State $DS.\theta \leftarrow DS.\theta / 2 $.
            \EndIf
            % \If{$\operatorname{len}(DS_{aux}.S) \geq \frac{ L_{aux}}{\varepsilon}$}\Comment{adjust threshold for $DS_{aux}$}
            %     \State $L_{aux} \leftarrow L_{aux} + 1$.
            %     \State $DS_{aux}.\theta \leftarrow DS_{aux}.\theta \times 2 $.
            % \ElsIf{$\operatorname{len}(DS_{aux}.S) \leq \frac{L_{aux}-1}{\varepsilon}$}
            %     \State $L_{aux} \leftarrow L_{aux} - 1$.
            %     \State $DS_{aux}.\theta \leftarrow DS_{aux}.\theta / 2 $.
            % \EndIf
        \EndFor
	\end{algorithmic}  
\end{algorithm}

The algorithm maintains a single pair of DS-COD sketches: a main sketch $DS$ and an auxiliary sketch $DS_{aux}$. Initially, the algorithm handles expiration without restricting the number of snapshots ( the snapshot count is inherently bounded by $O(\ell \log R)$). The update process follows the same steps as in hDS-COD. Since there is only one level, the initial threshold $\theta$ is set to $\varepsilon N$, and the variable L, representing the current threshold level, is initialized to 1. After each update, the algorithm checks the current number of snapshots.If it exceeds $L/\varepsilon$, this indicates that the threshold is too small, and to prevent an excessive number of snapshots in the future, the threshold is doubled and the threshold level is increased accordingly (lines 9-10). In contrast, if the current number of snapshots is below the lower bound $(L-1)/\varepsilon$, it suggests that the squared norms of the data in the current window are decreasing and the threshold level needs to be lowered. Similarly, \( DS_{aux} \) also requires threshold adjustment; for clarity, this step is omitted from the pseudocode.

For typical datasets, the approximation error of aDS-COD is comparable to that of hDS-COD. Additionally, since aDS-COD employs a single-layer structure, it requires less space than hDS-COD empirically. The space upper bound remains unchanged: in the worst-case scenario, the threshold level increases from $1$ to $\log R$, with each level accumulating $\ell$ snapshots. Therefore, the total number of snapshots is bounded by $O((m_x+m_y)\ell \log R)$. The time complexity for handling updates is $O((m_x+m_y)\ell)$.


\subsection{Time-based model}
Many real-world applications based on sliding windows focus on actual time windows, such as data from the past day or the past hour, which are known as time-based sliding windows. The key difference between time-based models and sequence-based models lies in the variability of data arrival within the same time span, which can sometimes fluctuate dramatically. We assume that at most one data point arrives at any given time point. When no data arrives at a particular time point, the algorithm equivalently processes a pair of zero vectors. Consequently, the assumption  on  column pairs $(\vx,\vy)$ of the input matrices changes to $\|\vx\|\|\vy\| \in [1, R] \cup \{0\}$.

We demonstrate how to adapt hDS-COD to the time-based model. Due to the presence of zero columns, the norm product within a window $\|\mathbf{X}_W\|_F \|\mathbf{Y}_W\|_F$ ranges within $[0, NR]$. Accordingly, the thresholds for generating snapshots need to be adjusted to $\theta_i = 2^i$. For hDS-COD, it is necessary to initialize $\log{(\varepsilon NR)}$ levels with exponentially increasing thresholds from $1$ to $\varepsilon NR$. In contrast, aDS-COD only requires setting the initial threshold to $1$, but its space upper bound remains consistent with that of hDS-COD, at $O((m_x + m_y)\ell \log{(\varepsilon NR)})$.

\begin{corollary} \label{corollary:ds-time} 
    If we set $\ell=\frac{1}{\varepsilon}$, the time-based hDS-COD can generate sketches $\mA$ and $\mB$ of size $O(\ell)$, with the correlation error upper bounded by 
    \[
        \text{corr-err }\left(\mX_W\mY_W^\top,\mA\mB^\top\right) \leq 8\varepsilon.
    \]
    The algorithm uses $O\left(\frac{m_x + m_y}{\varepsilon} \log{(\varepsilon NR)}\right)$ space and requires \\$O\left(\frac{m_x + m_y}{\varepsilon} \log{(\varepsilon NR)}\right)$ update time.
\end{corollary}
