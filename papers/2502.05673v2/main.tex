%% bare_jrnl_compsoc.tex
%% V1.4b
%uses AEs to create deep-fakel
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeletHowevebelieve thattrating the use of IEEEtran.cls
%% (requires example,cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%%such as as as as as as asp://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warrantinERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% Usas theassumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses,developng, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments areimprovingions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{afterpage}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{titlesec}
\usepackage{array}   
\usepackage{amssymb}
% \boldsymbol{\times}
\usepackage[misc]{ifsym} % for the envelope symbol
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\newcommand{\ping}[1]{\textcolor{blue}{#1}}
\newcommand{\qiqi}[1]{\textcolor{orange}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}

%行色调整
\usepackage[table]{xcolor} 
\definecolor{rowcolor1}{RGB}{235,235,235} % 第一种底色
\definecolor{rowcolor2}{RGB}{250,250,250} % 第二种底色

\usepackage{tikz}
\newcommand{\tikzxmark}{%
\tikz[scale=0.23] {
    \draw[line width=0.7,line cap=round] (0,0) to [bend left=6] (1,1);
    \draw[line width=0.7,line cap=round] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\newcommand{\tikzcmark}{%
\tikz[scale=0.23] {
    \draw[line width=0.7,line cap=round] (0.25,0) to [bend left=10] (1,1);
    \draw[line width=0.8,line cap=round] (0,0.35) to [bend right=1] (0.23,0);
}}
\usepackage[edges]{forest}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Ping~Liu,~\IEEEmembership{Senior Member,~IEEE},
        and~Jiawei~Du\textsuperscript{~\Letter}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Ping Liu is with the Department of Computer Science and Engineering, University of Nevada, Reno, NV, 89512.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: pino.pingliu@gmail.com
% \IEEEcompsocthanksitem Q. Tao is with National University of Singapore.\protect\\
% E-mail: tao.qiqi@u.nus.edu
\IEEEcompsocthanksitem Jiawei Du is with Centre for Frontier AI Research (CFAR), and  Institute of High Performance Computing (IHPC), A*STAR, Singapore.\protect\\
E-mail: dujw@cfar.a-star.edu.sg
\IEEEcompsocthanksitem \textsuperscript{\Letter} denotes the corresponding author.
% \IEEEcompsocthanksitem This work is done during Q. Tao's internship at Centre for Frontier AI Research (CFAR), A*STAR, Singapore.
}
% <-this % stops an unwanted space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
% {Shell \MakeLowercase{et al.}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Dataset distillation, which condenses large-scale datasets into compact synthetic representations, has emerged as a critical solution for training modern deep learning models efficiently.
While prior surveys focus on developments before 2023, this work comprehensively reviews recent advances (2023–2025), emphasizing scalability to large-scale datasets such as ImageNet-1K and ImageNet-21K.
We categorize progress into a few key methodologies: trajectory matching, gradient matching, distribution matching, scalable generative approaches, and decoupling optimization mechanisms.
As a comprehensive examination of recent dataset distillation advances, this survey highlights breakthrough innovations: the SRe2L framework for efficient and effective condensation, soft label strategies that significantly enhance model accuracy, and lossless distillation techniques that maximize compression while maintaining performance.
Beyond these methodological advancements, we address critical challenges, including robustness against adversarial and backdoor attacks, effective handling of non-IID data distributions.
Additionally, we explore emerging applications in video and audio processing, multi-modal learning, medical imaging, and scientific computing, highlighting its domain versatility.
By offering extensive performance comparisons and actionable research directions, this survey equips researchers and practitioners with practical insights to advance efficient and generalizable dataset distillation, paving the way for future innovations.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Dataset Distillation, Efficiency, Image Classification, ImageNet-1K, ImageNet-21K
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
% \IEEEPARstart{T}he rise of large-scale deep learning models, such as large language models (LLMs) \cite{minaee2024large_arxiv2024} and vision-language models (VLMs) \cite{zhang2024vision_tpami2024}, has amplified the demand for extensive datasets to capture complex patterns and semantics.
% While these models achieve state-of-the-art performance, their reliance on massive data imposes significant challenges in storage, computation, and energy efficiency, often limiting accessibility. 
% For instance, models like CLIP \cite{radford2021learning_icml2021} require over 400 million image-text pairs for pre-training, making reproduction and adaptation prohibitively expensive.
% This not only poses significant computational challenges but also raises concerns about storage cost, energy efficiency, and democratization of AI research.

\IEEEPARstart{T}he rapid advancement of large-scale deep learning models, such as Large Language Models (LLMs) \cite{minaee2024large_arxiv2024} and Vision-Language Models (VLMs) \cite{zhang2024vision_tpami2024}, has drastically increased the demand for large datasets to capture complex patterns and semantics. 
Although these models achieve state-of-the-art performance, their reliance on massive data poses significant challenges in terms of storage, computational cost, and energy efficiency, limiting accessibility and reproducibility. 
For instance, models like CLIP \cite{radford2021learning_icml2021} require over 400 million image-text pairs for pre-training, making dataset acquisition and processing prohibitively expensive. 
This raises concerns about the democratization of AI research, which only institutions with extensive computational resources can afford.

Dataset Distillation (DD) has emerged as a promising solution by condensing large datasets into compact synthetic representations that retain information critical for model training. 
First introduced by Wang et al. \cite{wang2018dataset_arxiv2018}, DD enables models to achieve performance comparable to full dataset training while significantly reducing storage and computational requirements. 
Beyond its practical benefits, recent theoretical breakthroughs in explaining the effectiveness of DD have further deepened the understanding of deep learning.
For instance, Yang et al. \cite{yang2024dataset_icml2024} demonstrated that distilled datasets primarily capture early training dynamics, closely resembling models that undergo early stopping on real data. 
By leveraging influence functions, they showed that individual distilled data points retain meaningful semantics unique to the target classes, offering critical insights into the fundamental mechanisms in dataset distillation.
These findings not only validate the effectiveness of DD but also pave the way for more principled distillation techniques.


Despite these advancements, dataset distillation faces several pressing challenges that hinder its large-scale deployment. 
One major challenge is scalability. 
Existing methods often fail to maintain performance when applied to large-scale datasets such as ImageNet-1K \cite{russakovsky2015imagenet} and ImageNet-21K \cite{ridnik2021imagenet_nips2021}. 
The nonlinear increase in computational overhead and the dramatic drop in performance on large-scale datasets pose significant challenges to scalability. 
Another critical limitation is cross-architecture generalization, where distilled datasets optimized for one model architecture often exhibit suboptimal performance on others. 
Furthermore, robustness concerns, including vulnerability to adversarial and backdoor attacks, must be addressed to ensure the security and reliability of distilled datasets. 
Beyond traditional image classification, DD is now being explored for other domains such as video, audio, multi-modal learning, and medical imaging, introducing new challenges that demand novel solutions. 
Given these evolving challenges and the rapid expansion of DD applications, a comprehensive survey is needed to consolidate recent innovations, identify key limitations, and outline future research directions.

As shown in Table \ref{tab:survey_comparison},  previous surveys \cite{geng2023survey_ijcai2023,lei2023comprehensive_tpami2023, yu2023dataset_tpami2023} focused primarily on early developments and small-scale datasets such as MNIST \cite{deng2012mnist_spm2012}, our work provides a timely and in-depth examination of the latest advances from 2023 to 2025. 
This survey differentiates itself by systematically addressing the scalability of DD methods to large datasets such as ImageNet-1K \cite{russakovsky2015imagenet} and ImageNet-21K \cite{ridnik2021imagenet_nips2021}, analyzing the effectiveness of cutting-edge techniques such as SRe2L series, soft-labeling strategies, and regularization-based approaches for improving performance at high IPC settings (e.g., IPC=200). 
We provide an in-depth comparison of different methodological paradigms, including trajectory matching, gradient matching, distribution matching, generative approaches, and decoupling optimization techniques, highlighting their respective strengths and limitations. 
Additionally, we explore underrepresented yet critical aspects of dataset distillation, such as adversarial and backdoor robustness against various attacks, self-supervised learning applications,  domain adaptations, and non Identical and Independent Distribution (IID).


% \input{image/taxonomy}
% (Figure \ref{fig:taxonomy}). 

\begin{table*}[t]
    \centering
    % \caption{Comparison of Recent Dataset Distillation Surveys}
 \caption{Comparison of recent dataset distillation surveys. Unlike previous surveys, which primarily focus on small datasets, low IPC settings, and supervised classification tasks, our work expands the scope to large-scale datasets, high IPC settings, generative approaches, decoupling-based solutions, and broader applications, such as self-supervised learning and federated learning.}
    \label{tab:survey_comparison}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Survey}& \textbf{Coverage Period} & \textbf{Techniques Covered} & \textbf{Large-Scale } & \textbf{Large IPC} & \textbf{Beyond CLS} & \textbf{ SSL/DA/Non-IID}  \\
        \midrule
        Geng et al. \cite{geng2023survey_ijcai2023}  & 2019-2023 & GM, TM, DM  & $\times$   & $\times$  &  {\checkmark}  & $\times$  \\
        Lei et al. \cite{lei2023comprehensive_tpami2023}  & 2019-2023 & GM, TM, DM  &  $\times$    & $\times$  & $\times$ & $\times$    \\
        Yu et al. \cite{yu2023dataset_tpami2023}  & 2019-2023 & GM, TM, DM  & $\times$ & $\times$  & {\checkmark}    & $\times$  \\
        \textbf{Ours}  & 2023-2025 & GM, TM, DM, Generative, SRe2L, Softlabel & {\checkmark}  & {\checkmark}  &{\checkmark} & {\checkmark} \\
        \bottomrule
    \end{tabular}
\end{table*}




This survey is organized as follows. 
Section 2 introduces the background concepts and fundamental approaches in dataset distillation, covering techniques such as trajectory matching, gradient matching, and distribution matching. 
Section 3 highlights applications of dataset distillation across various domains and tasks, including temporal data, multi-modal learning, medical imaging, and scientific computing. 
Section 4 provides a comprehensive performance comparison of different methods on various benchmarks.
Finally, Section 5 discusses the current challenges in dataset distillation and outlines promising directions for future research.
Throughout our analysis, we aim to provide researchers and practitioners with actionable guidelines for advancing the field.









\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{image/DD_intro.png}
    % \caption{Illustration of gradient matching distillation. Image from \cite{zhao2020dataset_iclr2021}}
\caption{Illustration of dataset distillation~\cite{wang2018dataset_arxiv2018}.  The original dataset is condensed into a synthetic dataset through a condensation process. Both the original and synthetic datasets are used to train randomly initialized models, and their performance is expected to be comparable. }
    \label{fig:gradientmatching}
\end{figure}

\section{Fundamental Dataset Distillation Methods}
\label{sec:background}
Dataset distillation, first introduced by Wang et al. \cite{wang2018dataset_arxiv2018}, seeks to condense a large training dataset into a significantly smaller, yet highly effective synthetic set.
The foundational formulation of dataset distillation is defined as:

\begin{equation}
\begin{split}
\mathcal{S} &= \arg\min_{\mathcal{S}} \mathbb{E}_{\theta^{(0)} \sim \Theta}[l(\mathcal{T};\theta^{(T)}_{\mathcal{S}})], \\
\theta^{(t)}_{\mathcal{S}} &= \theta^{(t-1)}_{\mathcal{S}}-\alpha\nabla_{\theta^{(t-1)}_{\mathcal{S}}} l(\mathcal{S}; \theta^{(t-1)}_{\mathcal{S}}),
\end{split}
\end{equation}
where $\mathcal{S}$ represents the synthetic training data to be learned, $\mathcal{T}$ denotes the original training dataset,  $\theta^{(t)}_{\mathcal{S}}$ represents model parameters at step t, $\theta^{(0)}$ is initial parameters, $\Theta$ is the distribution of initializations, $\alpha$ is learning rate, and $l(\cdot;\theta)$ represents the loss function.


As shown in Figure \ref{fig:gradientmatching}, the primary objective is to generate a condensed dataset that enables training new models with performance equivalent to those trained on the full original dataset.
Wang et al. employed backpropagation through time (BPTT) \cite{werbos1990backpropagation} as a core mechanism, emphasizing consistent model architectures across distillation and training phases to ensure alignment and stability.
This seminal work laid the foundation for extensive research aimed at enhancing the efficiency, generalization, and scalability of dataset distillation techniques.

However, traditional BPTT suffers from challenges like gradient variance, computational inefficiency, and difficulty capturing long-term dependencies. 
To address these limitations, subsequent research introduced innovative approaches.
Feng et al. \cite{feng2023embarrassingly_iclr2023} proposed Random Truncated BPTT (RaT-BPTT), which combines truncation and randomization to stabilize gradients and improve efficiency. 
This method effectively handles long-term dependencies, leading to significant performance gains in dataset distillation.


Building upon this,  Yu et al. \cite{yu2025teddy_eccv2024} developed the Teddy framework, which employs Taylor-approximated matching to simplify second-order optimization into a more computationally efficient first-order approach:
\begin{equation}
    % Taylor approximation form
\begin{split}
l(\mathcal{T};\theta^{(T)}_{\mathcal{S}}) = l(\mathcal{T};\theta^{(T-1)}_{\mathcal{S}}-\alpha g_{\mathcal{S}}^{(T-1)}) \\
\approx l(\mathcal{T};\theta^{(T-1)}_{\mathcal{S}})-\alpha g_{\mathcal{T}}^{(T-1)}\cdot g_{\mathcal{S}}^{(T-1)} \\
\approx l(\mathcal{T};\theta^{(0)})-\alpha \sum_{t=0}^{T-1}g_{\mathcal{T}}^{(t)}\cdot g_{\mathcal{S}}^{(t)} .   
\end{split}
\end{equation}
By applying first-order Taylor expansion to unfold the unrolled computation graph, it decouples the bi-level optimization process. 
This transformation reduces computational complexity while avoiding the need to store and backpropagate through multiple training steps.


Building on the foundational concepts and innovations, following research has diversified into three primary matching-based approaches: distribution matching, which focuses on aligning the statistical properties of datasets; gradient matching, which seeks to replicate the training dynamics over short-term optimization steps by aligning gradients; 
and trajectory matching, which extends this idea to long-term consistency by aligning the entire optimization path of models trained on distilled data. 
While these methods share a common goal of preserving the training behavior of the original dataset, their differing scopes, such as short-term versus long-term alignment, address unique challenges in dataset distillation. 
The subsequent sections provide an in-depth exploration of these works, with emphasis on the advancements in the past two years.


\subsection{Matching Based Approaches}

\subsubsection{Gradient Matching} 
Dataset distillation through gradient matching represents one of the earliest and most fundamental approaches in matching-based methods. 
As shown in Figure \ref{fig:gradientmatching}, these methods aim to align the gradients of models trained on synthetic data with those trained on the original full dataset, ensuring that the distilled dataset guides the model towards similar optimization trajectories as the original dataset.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{image/DD_intro.png}
%     % \caption{Illustration of gradient matching distillation. Image from \cite{zhao2020dataset_iclr2021}}
% \caption{Illustration of dataset distillation~\cite{wang2018dataset_arxiv2018}.  The original dataset is condensed into a synthetic dataset through a condensation process. Both the original and synthetic datasets are used to train randomly initialized models, and their performance is expected to be comparable. }
%     \label{fig:gradientmatching}
% \end{figure}


Zhao et al. \cite{zhao2020dataset_iclr2021} first formalized this approach through a gradient matching framework. 
The core idea is that a model   trained on distilled data should achieve both comparable generalization performance and parameter alignment with a model  trained on the original dataset.
The objective of gradient matching can be  defined as:
\begin{equation}
    \min_\mathcal{S} \mathbb{E}_{\theta_0\sim p(\theta_0)}[\sum_{t=0}^{T-1} D(\nabla_\theta\ell^\mathcal{S}(\theta_t), \nabla_\theta\ell^\mathcal{T}(\theta_t))],
\end{equation}
where $\theta$ is the model parameters, $\ell(\cdot)$ is the loss function,
$D(\cdot,\cdot)$ measures the distance between gradients, typically implemented using cosine similarity or L2 distance.
To further enhance the robustness and effectiveness of gradient matching,  Zhao et al. \cite{zhao2021dataset_icmlr2021} introduced Differentiable Siamese Augmentation (DSA).
This approach addresses the limited diversity of synthetic datasets by incorporating differentiable data augmentation into the matching process. 
DSA applies consistent transformations to both real and synthetic data while maintaining end-to-end gradient propagation, significantly improving the generalization capability.

\noindent \textbf{Stage Summary} However, while gradient matching effectively ensures step-wise optimization similarity, it faces several limitations.
Focusing on matching individual gradient steps may not capture long-term dependencies in training and can be sensitive to learning rate schedules and optimization hyperparameters.
Additionally, the computational cost of calculating and matching gradients at each step can be substantial, particularly for large models and datasets.
These limitations motivated researchers to explore methods that consider the entire training trajectory rather than individual gradient steps, leading to the development of trajectory matching approaches.

\subsubsection{Trajectory Matching}  
Building upon the limitations of gradient matching, trajectory matching methods extend the optimization alignment from individual gradient steps to entire parameter trajectories during training. 
This holistic approach addresses both the long-term dependency issue and optimization instability inherent in gradient-based methods. 
As shown in Figure \ref{fig:trajectorymatching}, by aligning complete training trajectories between networks trained on synthetic and real data, this approach enables more robust and stable distillation.


Cazenavette et al. \cite{cazenavette2022dataset_cvpr2022} formalized this trajectory-level matching through their Matching Training Trajectories (MTT) method. 
While gradient matching focuses on step-wise gradient alignment, MTT uses expert trajectories from real datasets as benchmarks for the entire training process.
The formulation of MTT is defined as:
\begin{equation}
\mathcal{S}^* = \arg \min_{\mathcal{S}} \mathbb{E}_{\theta^{(0)} \sim \Theta} \sum_{t=0}^T \mathcal{D}(\theta^{(t)}_{\mathcal{S}}, \theta^{(t)}_{\mathcal{T}}),
\end{equation}
where $\theta^{(t)}_{\mathcal{S}}$ and $\theta^{(t)}_{\mathcal{T}}$ represent model parameters at step $t$ when trained on synthetic and real data respectively, and $\mathcal{D}(\cdot,\cdot)$ measures parameter-space distances rather than gradient differences.
The initialization distribution $\Theta$ and trajectory length $T$ are crucial hyperparameters that influence the robustness of the matching process \cite{du2023minimizing_cvpr2023}.
The development of trajectory matching methods has been driven by three primary challenges: \textit{trajectory stability}, \textit{parameter alignment}, and \textit{computational scalability}. 
Each subsequent advancement has contributed to addressing these challenges in complementary ways.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{image/TM.png}
    \caption{Overview of trajectory matching-based dataset distillation. The method aligns parameter trajectories between models trained on synthetic and real data. }
    \label{fig:trajectorymatching}
\end{figure}
Ensuring stable and robust trajectories is crucial for effective dataset distillation.
While MTT effectively aligns trajectories, challenges like accumulated trajectory errors and sensitivity to weight perturbations persist. 
Du et al. \cite{du2023minimizing_cvpr2023} proposed Flat Trajectory Distillation (FTD), which reduces sensitivity by encouraging flat parameter trajectories through regularization. 
This approach improves robustness and compactness, particularly on larger datasets like CIFAR-100 \cite{krizhevsky2010convolutional}. Similarly, Shen et al. \cite{shen2023ast_arxiv2023} introduced Adaptive Smooth Trajectories (AST), incorporating gradient clipping, penalties, and representative sample initialization to stabilize training dynamics and mitigate trajectory mismatches.
Zhong et al. \cite{zhong2024towards_arxiv2024} addressed trajectory instability and slow  convergence with Matching Convexified Trajectories (MCT), which replaces oscillatory SGD paths with stable linear trajectories inspired by Neural Tangent Kernel (NTK) theory \cite{jacot2018neural_nips2018}. 
By requiring only two checkpoints, MCT reduces memory overhead while improving distillation stability.

Tackling parameter mismatches is also essential for distillation across heterogeneous datasets.
Li et al. \cite{li2024dataset_ieice2024} tackled the issue of parameter mismatches between teacher and student models caused by data heterogeneity.
Their method identifies mismatched parameters by evaluating the values of parameter magnitudes and prunes these selected parameters. 
The distilled dataset is then optimized over the remaining effective parameters, significantly enhancing distillation efficiency and performance.


To tackle scalability challenges with large-scale datasets, Cui et al. \cite{cui2023scaling_icml2023} developed Trajectory Matching with Soft Label Assignment (TESLA). 
TESLA refines the MTT loss function by decomposing gradient computations into sequential batches, eliminating the need to store full computational graphs in memory. 
Additionally, TESLA incorporates a soft label assignment strategy, leveraging pre-trained teacher models to generate soft labels for synthetic data, which significantly boosts performance when scaling to datasets with a large number of classes, such as ImageNet-1K.
Another innovation in this direction is the Automatic Training Trajectories (ATT) method introduced by Liu et al. \cite{liu2024dataset_eccv2024}. 
Unlike most trajectory matching approaches that use fixed trajectory lengths, ATT dynamically adjusts trajectory lengths during the distillation process. 
By employing a minimum distance strategy to select optimal matching trajectories, ATT effectively reduces mismatching errors and ensures precise alignment of synthetic datasets.


\noindent \textbf{Stage Summary} Despite these advances, trajectory matching still faces important challenges. 
The high computational demands of tracking full parameter trajectories limit applications to complex architectures, while sensitivity to hyperparameters can affect robustness. 
Future research opportunities include developing adaptive hyperparameter selection mechanisms, designing more efficient algorithms for resource-constrained settings, and exploring hybrid approaches that combine the benefits of trajectory matching with other distillation techniques. 


\subsubsection{Distribution Matching} 
Dataset distillation traditionally relied on gradient and trajectory matching,  suffering from two fundamental limitations: the computational complexity of bi-level optimization and the challenge of capturing long-term dependencies.
Distribution matching emerges as an alternative paradigm that addresses these limitations through direct alignment of feature distributions in selected embedding spaces.

As shown in Figure \ref{fig:distmatching}, the core objective of distribution matching is to optimize synthetic data such that its distribution aligns with the original data distribution across multiple embedding spaces. 
This approach differs fundamentally from gradient or trajectory matching methods, offering improved computational efficiency and interpretability. 
The formal optimization problem is expressed as:
\begin{equation}
\mathcal{S}^* = \arg\min_{\mathcal{S}} \mathbb{E}_{\phi \sim \Phi}[\mathcal{D}(P_{\phi}(\mathcal{S}), P_{\phi}(\mathcal{T}))],
\end{equation}
where $P_{\phi}(\cdot)$ denotes the distribution of features extracted by network $\phi$, and $\mathcal{D}(\cdot,\cdot)$ measures the distance between distributions. 
This formulation, first introduced by Zhao et al. \cite{zhao2023dataset_wacv2023}, avoids the costly bi-level optimization process prevalent in earlier methods such as \cite{wang2018dataset_arxiv2018, zhao2020dataset_iclr2021}.
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{image/DM.png}
    \caption{Overview of distribution matching in dataset distillation, where feature distributions are aligned to ensure the synthetic dataset effectively preserves the key characteristics of the original data.}
    \label{fig:distmatching}
\end{figure}
The following developments in distribution matching have focused on three major areas: \textit{feature refinement}, \textit{advanced alignment}, and \textit{higher-order matching}.


In the first area, the CAFE framework \cite{wang2022cafe_cvpr2022} introduced multi-scale feature preservation to address overfitting to dominant features.
This multi-scale approach significantly improves the discriminative power of synthetic datasets.
Zhao et al. \cite{zhao2023improved_cvpr2023} further refined this by introducing partitioning strategies and class-aware regularization to handle imbalanced feature distributions.
Zhang et al.’s \cite{Zhang2024_ijcai2024} DANCE framework addresses both distribution shifts and training inefficiencies.
By combining pseudo long-term distribution alignment with expert-driven distribution calibration, DANCE effectively aligns intra-class and inter-class distributions, achieving state-of-the-art performance.
Rahimi et al. \cite{Malakshan2024distilling_arxiv2024} took a different approach by decomposing distributions into content and style components, enabling more nuanced optimization of feature diversity.

In the second area, advanced alignment mechanisms have emerged as a critical development in dataset distillation, going beyond traditional statistical matching by incorporating attention-based strategies, trajectory constraints, and pseudo-labeling techniques. 
Sajedi et al. \cite{sajedi2023datadam_iccv2023} proposed DataDAM, which integrates feature distribution alignment with spatial attention matching. 
By leveraging attention maps aggregated from feature representations, DataDAM effectively emphasizes informative regions while reducing redundant background information, leading to more compact yet expressive synthetic datasets.


In the third area, recent approaches  utilized higher-order distribution matching, a crucial advancement beyond traditional mean distribution alignment.
Zhang et al. \cite{zhang2024m3d_aaai2024} pioneered embedding representations into a Reproducing Kernel Hilbert Space \cite{berlinet2011reproducing}, enabling alignment across multiple moments.
Wei et al. \cite{wei2024dataset_cvprw2024} complemented this with Latent Quantile Matching (LQM), aligning synthetic data with optimal quantiles determined via statistical tests.
From a geometric perspective, Liu et al. \cite{liu2023dataset_arxiv2023} leveraged the Wasserstein distance \cite{panaretos2019statistical}, capturing first and second-order statistics for distribution alignment.
Similarly, Deng et al. \cite{deng2024exploiting_cvpr2024} proposed class centralization and covariance matching, incorporating second-order information to tighten intra-class clustering while maintaining inter-class separation.

\noindent \textbf{Stage Summary} Collectively, these methods highlight the transition from simple mean-matching techniques to statistically and geometrically informed alignment strategies. 
However, as shown in Table \ref{tab:performance}, distribution matching methods often lag behind gradient matching approaches in accuracy on large-scale datasets, revealing fundamental limitations in capturing dynamic training behaviors. 
To bridge this gap, future research could explore hybrid approaches that integrate distribution and gradient matching, develop adaptive higher-order constraints that evolve during optimization, and investigate causal relationships within feature distributions to gain deeper insights into the effectiveness of different matching strategies.


\subsubsection{Latent and Frequency Space Methods}
Traditional dataset distillation methods typically operate in the pixel space, directly generating synthetic data in the spatial domain. 
However, these methods often face significant computational and memory challenges, especially when handling high-dimensional datasets. 
To address these limitations, recent approaches have explored alternative spaces, such as latent and frequency domains, to achieve more efficient and scalable distillation.

Operating in latent spaces has emerged as a promising direction for efficient distillation due to its ability to reduce dimensionality while preserving essential information.
Duan et al. \cite{duan2023dataset_arxiv2023} proposed LatentDD, which transitions from the pixel space to the latent space using a pretrained autoencoder. 
By condensing datasets into compact latent representations, LatentDD substantially reduces computation and memory requirements, enabling efficient dataset distillation without compromising performance.
Building on this shift to latent spaces, George et al. \cite{cazenavette2023generalizing_cvpr2023} proposed GlaD, which leverages the latent spaces of generative models like StyleGAN-XL \cite{sauer2022stylegan_acmsiggraph}. 
By encoding high-dimensional images into low-dimensional feature vectors, GlaD not only reduces computational cost but also enhances generalization ability.

Complementary to latent space approaches, frequency-domain methods offer another efficient avenue for dataset distillation.
Shin et al. \cite{shin2024frequency_nips2023} introduced Frequency Domain-based Dataset Distillation (FreD), a method that operates in the frequency domain rather than the spatial domain. 
FreD optimizes a selective subset of important frequency dimensions based on explained variance ratios, avoiding the need to process the full dimensionality of the spatial domain. 
This approach drastically lowers the memory budget per synthetic instance while preserving critical information. 
Building upon frequency-domain techniques, Yang et al. \cite{yang2024neural_eccv2024} introduced Neural Spectral Decomposition (NSD), leveraging the low-rank structure of datasets to decompose them into spectral tensors and kernel matrices, enhancing learning efficiency and integrating seamlessly with frameworks like MTT.
Moving beyond traditional representations, Shin et al. \cite{shin2025distilling_iclr2025} introduces Distilling Dataset into Neural Field (DDiF), a novel parameterization framework that compresses large datasets into synthetic neural fields under limited storage budgets, offering superior performance on various datasets.

\noindent\textbf{Stage Summary} These methods highlight the potential of alternative spaces to overcome the computational and scalability challenges inherent in pixel-based approaches. 
As latent and frequency space techniques continue to evolve, integrating their strengths or combining them with other methodologies could further enhance the efficiency and generalization of dataset distillation frameworks.


\subsubsection{Plug-and-Play Approaches}
Recent advancements in dataset distillation explored plug-and-play techniques that seamlessly integrate into existing methods, enhancing both optimization efficiency and generalization. 
Unlike approaches that introduce entirely new optimization paradigms, these methods serve as \textit{modular} enhancements, improving adaptability and scalability.
However, it is important to note that current plug-and-play techniques are primarily designed for matching-based methods, such as gradient matching and trajectory matching.

One major limitation in synthetic dataset optimization is treating the dataset as a single, unified entity during training, which can hinder the ability to capture training dynamics effectively. 
To tackle the uniform optimization limitation of synthetic datasets, Du et al. \cite{du2024sequential_nips2024} proposed Sequential Subset Matching (SeqMatch), a  strategy that partitions the dataset into smaller, manageable subsets. 
This approach addresses the inefficiencies associated with treating the entire dataset as a single unified whole, which often struggles to capture the dynamic nature of training processes. 
Instead of optimizing the dataset as an indivisible entity, SeqMatch divides it into  K  subsets, each sequentially optimized to capture knowledge relevant to specific stages of training.
The optimization process for each subset is expressed as:
\begin{footnotesize}
\begin{equation}
\hat{\mathcal{S}}_k = \arg\min_{\substack{\mathcal{S}_k \subset \mathbb{R}^d \times \mathcal{Y} \\ |\mathcal{S}_k| = \lfloor |\mathcal{S}| / K \rfloor}} 
\mathbb{E}_{\theta_0\sim P_{\theta_0}} \left[ \sum_{m=(k-1)n}^{kn} \mathcal{L}(\mathcal{S}_k \cup \mathcal{S}_{(k-1)}, \theta_m) \right],
\end{equation}
\end{footnotesize}
where K denotes the number of subsets, n is the number of iterations per subset.
Here,  $\mathcal{L}(.,.)$  represents the loss function used for model optimization,  $\mathcal{S}_{k-1}$  represents previously optimized subsets, and  $\mathcal{S}_k$  is the current subset being optimized. 
The cumulative incorporation of subsets ensures that knowledge captured in earlier stages is preserved and further refined in subsequent iterations.


Recently, mutual information maximization \cite{belghazi2018mutual_icml2018} has been introduced as a plug-and-play technique to enhance dataset distillation by preserving crucial feature representations. 
This approach seamlessly integrates with existing methods without requiring significant modifications to the core optimization framework, making it a flexible enhancement for various distillation techniques.
Shang et al. \cite{shang2024mim4dd_nips2023} introduced MIM4DD, reframing dataset distillation as a mutual information maximization problem. 
Using a contrastive learning framework \cite{khosla2020supervised_nips2020}, MIM4DD maximizes mutual information \cite{belghazi2018mutual_icml2018} between synthetic and real samples through positive pair attraction and negative pair repulsion within the representation space, while aligning samples from the same class and separating those from different classes.
% This method preserves critical information during distillation and consistently outperforms state-of-the-art techniques across benchmarks.
Building on the perspective of mutual information maximization \cite{torkkola2003feature_jmlr2003}, Zhong et al. \cite{zhong2024going_arxiv2024} introduced class-aware conditional mutual information (CMI) as a plug-and-play regularization term. 
By concentrating synthetic datasets around class centers in pre-trained feature spaces, CMI improves generalization across architectures. 
This approach integrates seamlessly with existing techniques, offering consistent performance improvements across datasets.


Addressing biases in synthetic dataset generation is another critical focus.
Son et al. \cite{son2024fyi_eccv2024} tackled the bilateral equivalence phenomenon, where synthetic images symmetrically replicate discriminative parts, hindering the ability to distinguish subtle object details. 
To resolve this issue, they introduced FYI, a method that embeds horizontal flipping into the distillation process to break symmetry bias. 
This adjustment allows the synthetic datasets to capture richer and more nuanced object details, enhancing performance across multiple existing distillation techniques.

Leveraging external knowledge from Pre-Trained Models (PTMs) presents another promising avenue for improving dataset distillation.
Lu et al. \cite{lu2023can_arxiv2023} highlighted the untapped potential of PTMs, proposing Classification Loss of pre-trained Model and Contrastive Loss of pre-trained Model to transfer PTM knowledge into the distillation process.
These loss terms provide stable supervisory signals, leveraging diverse PTM architectures, parameters, and domain expertise. 
The study demonstrated the effectiveness of PTMs in enhancing cross-architecture generalization, even when using less-optimally trained models.

\noindent\textbf{Stage Summary} These plug-and-play approaches demonstrate the potential to seamlessly integrate advanced techniques into existing dataset distillation workflows. 
By improving efficiency, addressing biases, and leveraging external knowledge, these methods pave the way for scalable and generalizable distillation frameworks that can adapt to diverse datasets and architectures.


\subsection{Scalable Dataset Distillation Methods}
As the demand for large-scale and high-resolution datasets continues to grow, dataset distillation methods face mounting scalability challenges. 
To address these, recent advancements have introduced innovative frameworks that balance efficiency and performance, extending the applicability of dataset distillation to larger datasets. 
Key developments include the exploration of generative models such as GANs \cite{goodfellow2014generative} and Diffusion Models \cite{croitoru2023diffusion}, decoupling strategies of the SRe2L series \cite{yin2024squeeze_nips2024}, and the soft label techniques \cite{qin2024label_arxiv2024}. 
The following subsubsections delve into these cutting-edge methods, emphasizing their contributions and the challenges they address in achieving scalability.
\subsubsection{Dataset Distillation via Generative Models}
Recent advancements in dataset distillation have increasingly leveraged generative models to enhance scalability, efficiency, and generalization. 
Unlike traditional methods that rely on discriminative matching processes to create synthetic data, these approaches utilize generative mechanisms to synthesize high-quality, diverse datasets.



\paragraph{GAN-based Methods}
Early efforts in this direction focused on GANs. 
Zhao et al. \cite{zhao2022synthesizing_arxiv2022} introduced Informative Training GAN (IT-GAN), challenging the traditional focus on visual realism in GAN-generated data. 
Instead, IT-GAN prioritizes informative training samples by freezing a pre-trained GAN and optimizing latent vectors using condensation loss and diversity regularization.
In IT-GAN,  condensation loss is to  minimize discrepancies between real and synthetic data, and  regularization loss  promotes diversity in the generated data. 


Expanding this direction, Li et al. \cite{li2024generative_cvpr2024} emphasized the importance of balancing global structures and local details in dataset distillation. 
Their method combines logits matching for capturing global structures with feature matching for refining local details, leveraging a model pool of diverse architectures to improve cross-architecture generalization.
Similarly, Wang et al. \cite{wang2023dim_arxiv2023} introduced Distill into Model (DiM), a novel approach that distills datasets into a generative adversarial network instead of directly producing synthetic images. 
DiM dynamically synthesizes diverse training samples from random noise, enhancing scalability for large architectures and eliminating the need for repeated re-distillation.
Building on these advancements, Zhang et al. \cite{zhang2023dataset_arxiv2023} proposed a codebook-generator framework, which condenses datasets into a compact codebook and generator.
This approach incorporates intra-class diversity and inter-class discrimination losses, enabling efficient synthesis of class-specific images and scalability to large datasets such as ImageNet-1K.
These methods collectively demonstrate the versatility and scalability of generative models across  architectures and  datasets in dataset distillation.

\paragraph{Diffusion-based Methods}
Diffusion models, which are renowned for their superior image quality compared to GANs, have emerged as a powerful alternative. 
Gu et al. \cite{gu2024efficient_cvpr2024} pioneered a diffusion-based framework leveraging a ‘minimax’ criterion to balance representativeness and diversity in surrogate datasets, ensuring both coverage and variability of training data. 
Similarly, Su et al. \cite{su2024d_cvpr2024} introduced Dataset Distillation via Disentangled Diffusion Model (D4M), which combines latent diffusion models with prototype learning. 
This approach enhances distillation efficiency and cross-architecture generalization while employing a novel training-time matching strategy. 
By eliminating architecture-specific optimization during image synthesis, D4M achieves high-resolution image generation, representing a substantial leap forward in dataset distillation methodologies.


Exploring the capabilities of DMs further, Abbasi et al. \cite{abbasi2024one_arxiv2024} proposed Dataset Distillation using Diffusion Models (D3M), which compresses entire image categories into textual prompts using techniques like textual inversion and patch selection. 
This approach enables efficient dataset compression while maintaining high performance on large-scale benchmarks. 
In a complementary approach, Abbasi et al. \cite{2024arXiv241204668A_arxiv2024} combined coreset selection \cite{sun2024diversity_cvpr2024} with latent diffusion models, enhancing patch diversity and realism, achieving significant improvements in large-scale benchmarks.


Recently, latent space techniques have offered innovative ways to optimize storage efficiency, scalability, and adaptability in this direction.
Moser et al. \cite{moser2024latent_arxiv2024} introduced Latent Dataset Distillation using Diffusion Models (LD3M), which extends the distillation process into the latent space. 
By leveraging a pre-trained latent diffusion model, LD3M enhances gradient propagation through a modified diffusion process that integrates initial latent states, striking an optimal balance between speed and accuracy:

\begin{equation}
    z_{t-1} \leftarrow \left[\left(1-\frac{t}{T}\right) \cdot \mu_{\theta}(c, z_t, \gamma_t) + \frac{t}{T} \cdot z_T\right] + \sigma_t^2\varepsilon_t,
\end{equation}
where $T$ denotes the total number of diffusion steps, $t$ represents the current timestep in the backward process, $\mu_{\theta}$ is the parameterized mean predictor network with learnable parameters $\theta$, and ${z}_T$ is the initial noisy state. 
The linear interpolation coefficients $(1-\frac{t}{T})$ and $\frac{t}{T}$ control the balance between the mean prediction and initial state, providing enhanced gradient flow during training without compromising the generation quality.
This framework demonstrates superior performance for high-resolution tasks while integrating seamlessly with existing algorithms. 
Taking a different approach to latent space optimization, Qin et al. \cite{qin2024distributional_arxiv2024} proposed Distributional Dataset Distillation (D3) and applied it into the federated learning setting \cite{10571602_tpami2024}. 
In their method, D3 represents classes as Gaussian distributions in latent space, optimizing the parameters to generate images using a decoder network. 
To guide the distilled data generation process, in \cite{chen2025igd_iclr2025}, Chen et al. introduced Influence-Guided Diffusion (IGD) that uses a latent diffusion model based on a Diffusion Transformer \cite{peebles2023scalable_iccv2023} to generate synthetic training datasets. 
They designed an influence guidance function to steer diffusions towards generating training-effective data and a deviation guidance function to enhance diversity, enabling the method to significantly improve performance across various datasets, particularly on high-resolution ImageNet-1K benchmarks.
% Collectively, these latent space methods emphasize the power of leveraging compact and expressive representations, offering scalable, efficient, and high-quality solutions for dataset distillation across diverse architectures and tasks.

\noindent\textbf{Stage Summary} While current methods have demonstrated promising results in improving scalability and efficiency through various generative approaches, several critical research directions deserve further investigation. 
First, with the rapid advancement of generative models, particularly the emergence of flow matching \cite{lipman2022flow_arxiv2022,chen2024flow_iclr2024} and other innovative approaches demonstrating superior effectiveness and efficiency, incorporating these cutting-edge generative techniques into dataset distillation presents a promising avenue for exploration. 
Second, current dataset distillation methods face significant challenges in memory usage and computational efficiency when handling high-resolution images. 
In this context, distillation approaches based on generative models, might offer potential solutions for high-resolution scenarios, inspiring further research into optimizing algorithmic architectures to enhance memory efficiency while maintaining distillation quality. 
Finally, the integration of generative approaches with traditional matching-based methods warrants thorough investigation. 


\subsubsection{Decoupling Optimization: SRe2L Series}
The high computational costs associated with bilevel optimization have long hindered the scalability of dataset distillation methods, particularly when applied to large-scale datasets or high-resolution images. 
To address these challenges, Yin et al. \cite{yin2024squeeze_nips2024} proposed the Squeeze, Recover, and Relabel (SRe2L) framework, a three-stage process that decouples dataset condensation into manageable steps. 
As shown in Figure \ref{fig:sre2l}, the Squeeze stage extracts essential information by training a model on the original dataset, followed by the Recover stage, which generates synthetic data by aligning batch normalization (BN) statistics \cite{cai2024batch_cvpr2024} with the trained model. 
Finally, the Relabel stage assigns soft labels to the synthetic data using the model. 
This approach achieved promising performance on large-scale datasets such as ImageNet-1K at  resolutions of 224$\times$224 pixels.



Building upon this foundational work, Yin et al. introduced Curriculum Data Augmentation (CDA) \cite{yin2023dataset_arxiv2023}, which employs a curriculum learning approach \cite{jiang2015self_aaai2015} during the data synthesis process. 
By progressively increasing the difficulty of image crops through dynamic adjustments of RandomResizedCrop parameters, CDA captures global structures early in training and refines local details in later stages. 
This method set a milestone by successfully distilling the entire ImageNet-21K dataset \cite{ridnik2021imagenet_nips2021} at standard resolutions, marking an huge advance in large-scale dataset distillation.

\begin{figure}[t]
    \centering
\includegraphics[width=1.0\linewidth]{image/sre2l.png}
    \caption{Overview of the three-stage SRe2L work \cite{yin2024squeeze_nips2024}. The Squeeze Stage trains a randomly initialized model on the original dataset by minimizing cross-entropy loss. The Recover Stage refines the synthetic dataset by optimizing a combined cross-entropy and batch normalization (BN) loss with a well-converged model. Finally, the Relabel Stage evaluates performance by training another model on the synthetic dataset with soft labels, using cross-entropy loss.}
    \label{fig:sre2l}
\end{figure}

Subsequent works systematically addressed the inherent limitations of SRe2L in scalability, generalization, and computational efficiency.
Zhou et al. \cite{zhou2024self_arxiv2024} introduced the SC-DD framework, which overcomes SRe2L’s challenges with larger models by enhancing BN statistics through self-supervised pretraining \cite{zong2024self_pami2024}. 
By enriching these statistics and employing linear probing during relabeling, SC-DD significantly improves data recovery and scalability for large datasets like ImageNet-1K. 
Shao et al. \cite{shao2024generalized_cvpr2024} extended this progress with the Generalized Various Backbone and Statistical Matching (G-VBSM) method, which addresses SRe2L’s reliance on single backbone models or specific statistics by introducing data densification, generalized statistical alignment, and multi-backbone consistency. 
G-VBSM achieved significant improvement across datasets and architectures, particularly in highly compressed settings. 
% Building upon G-VBSM, Shao et al. \cite{shao2024elucidating_nips2024} further refined the distillation process with Elucidating Dataset Condensation (EDC), incorporating real-image initialization, category-aware matching, and flatness regularization to enhance generalization and stabilize training. 
Further refinement came with the Elucidating Dataset Condensation (EDC) \cite{shao2024elucidating_nips2024}, which incorporated real-image initialization, category-aware matching, and flatness regularization. 
% This approach enhanced generalization and stabilized the training process.
More recently, Cui et al. \cite{cui2025datasetdistillationcommitteevoting_arxiv2025} introduced Committee Voting for Dataset Distillation (CV-DD), synthesizing high-quality distilled datasets by integrating insights from multiple pre-trained models. 
CV-DD introduces a Prior Performance Guided voting strategy that assigns greater weights to better-performing pre-trained models in the voting process, and  proposes Batch-Specific Soft Labeling, which generates more accurate soft labels by recalculating BN statistics for each synthetic data batch.


\noindent \textbf{Stage Summary} Recent works have shifted from single-model approaches to leveraging model diversity, as seen in G-VBSM’s multi-backbone architecture and CV-DD’s committee voting mechanism.
These methods emphasize improving batch normalization statistics, with SC-DD enhancing BN through pre-training and CV-DD introducing batch-specific BN recalculation, leading to more robust, generalizable condensed datasets.


\subsubsection{Soft Labels} 
In the SRe2L series and beyond, soft labels \cite{yin2024squeeze_nips2024} have emerged as a critical component for the significant performance improvement, encoding rich semantic information that surpasses the limitations of traditional one-hot labels. 
These nuanced labels enhance diversity, generalization, and scalability, addressing core challenges in large-scale dataset condensation.
Highlighting the centrality of soft labels in efficient learning, Qin et al. \cite{qin2024label_arxiv2024} proposed to explore the role of soft labels in dataset distillation. 
Through extensive experiments, they demonstrated that early-stopped expert models generate soft labels encoding structured semantic information crucial for data-efficient learning. 


% Following research has introduced innovative approaches to generating and utilizing soft labels to improve the performance. 
Subsequent research has advanced both understanding and  implementations on soft labels.
Sun et al. \cite{sun2024diversity_cvpr2024} proposed the Realistic, Diverse, and Efficient Dataset Distillation (RDED) method, which employs Fast Knowledge Distillation \cite{shen2022fast_eccv2022} to generate region-level soft labels. 
By aggregating diverse features and representations, RDED moves beyond traditional patch selection methods, capturing more intricate semantic information. 
Extending these principles, Hu et al. \cite{hu2025focusddrealworldsceneinfusion_arxiv2025} introduced Focused Dataset Distillation (FocusDD), leveraging pre-trained Vision Transformers (ViT) \cite{50650_iclr2021} to identify critical image patches. 
Their approach combines these key regions with downsampled background information to create high-quality distilled datasets. 
Notably, FocusDD extends dataset distillation beyond classification tasks to object detection, showcasing the versatility of soft label approaches. 
Zhong et al. \cite{zhong2024efficientdatasetdistillationdiffusiondriven_arxiv2024} further advanced the field by incorporating DMs for soft label generation and synthetic image construction. 
Unlike earlier random region selection techniques, their method employs differential loss computation guided by text prompts to identify class-relevant regions, offering a powerful alternative on patch selection.
Those advances show a clear progression from simple patch selection to sophisticated region processing using Vision Transformers and Diffusion Models, significantly enhancing the quality of distilled datasets.


Addressing practical implementation challenges, Shang et al. \cite{shang2024gift_arxiv2024} proposed the GIFT framework. 
This approach combines a label refinement module—merging smoothed hard labels with teacher-generated soft labels—and a mutual information-based loss function. 
GIFT significantly enhances the effectiveness of distilled datasets while maintaining minimal computational overhead, demonstrating its adaptability across diverse scenarios.
Xiao et al. \cite{xiao2024large_nips2024} tackled the substantial storage demands of soft labels in large-scale distillation, which can exceed 30 times the size of the distilled dataset.
Their solution leverages class-wise batching and supervision during image synthesis to enhance within-class diversity, enabling effective soft label compression through simple random pruning.
This strategy dramatically reduces storage requirements while improving performance on large-scale datasets, making soft label-based methods more practical for real-world applications.

\noindent \textbf{Stage Summary} This progression in soft label methodologies represents a fundamental advancement in dataset distillation. 
Future research directions include exploring efficient and adaptive data and/or patch generation, investigating multi-modal soft label fusion. 
These developments will be crucial for extending the applicability of dataset distillation to more challenging datasets with large-scale and high resolution, as well as emerging domains such as self-supervised learning and large foundation models.



\subsection{Efficiency and Effectiveness in Dataset Distillation}
Recent innovations have introduced strategies that reduce computational overhead, optimize resource utilization, and enhance representational diversity, effectively addressing challenges such as high memory requirements, overfitting, and scalability. 
These advancements significantly improve performance on large-scale datasets.

\subsubsection{Selective Dataset Distillation}
Recent advances highlight the necessity of identifying and optimizing key components that contribute most to model performance.
We present a unified theoretical framework for selective approaches across multiple dimensions, demonstrating how strategic selection and weighting of elements, from low-level features to high-level representations, enhance distillation efficiency.
Formally, let $\mathcal{D} = {(x_i, y_i)}_{i=1}^N$ denote the original dataset and $\mathcal{S} = {(\tilde{x}_j, \tilde{y}_j)}_{j=1}^M$ represent the synthetic dataset, where $M \ll N$.
The selective distillation objective can be formulated as:
\begin{equation}
\min_{\mathcal{S}} \mathbb{E}_{\theta_0} \sum_{t=0}^{T-1} \omega_t \sum_{d \in \mathcal{D}} \omega_d \mathcal{L}_d(\mathcal{S}, \mathcal{T})
\end{equation}
where $\omega_t$ represents selection weights across different timesteps of the training trajectory, $\omega_d$ denotes selection weights for different dimensions (e.g., pixel, color, parameter, original samples), $\mathcal{L}_d$ denotes the dimension-specific loss function for measuring distillation quality.

\noindent \textbf{Selective Pixel Enhancement}
Effectively identifying and enhancing discriminative pixels is crucial for dataset quality, especially when addressing inter-class variability and non-discriminative regions.
Recent efforts have refined pixel-wise feature representation to emphasize informative pixels while suppressing redundant or misleading patterns.
Wang et al. \cite{wang2024emphasizing_arxiv2024} proposed the Emphasizing Discriminative Features (EDF) framework, which leverages Common Pattern Dropout (CPD) to filter out non-salient pixels and Discriminative Area Enhancement (DAE) to intensify attention on essential regions using Grad-CAM \cite{selvaraju2017grad_iccv2017} activation maps.
By adaptively selecting and enhancing discriminative regions, EDF ensures synthesized datasets prioritize high-salience regions, leading to improved generalization ability.

\noindent \textbf{Selective Color Optimization}
Color redundancy often leads to inefficient dataset utilization, reducing the effectiveness of synthetic samples.
Yuan et al. \cite{yuancolor_nips2024} addressed this issue by introducing AutoPalette, a framework designed to selectively optimize color usage while preserving image features.
AutoPalette condenses images into reduced color palettes via a palette network, balancing pixel distributions with maximum color loss and palette balance loss.
A color-guided initialization strategy further ensures diverse, informative synthetic samples, improving overall distillation performance.
This method uniquely balances color diversity with feature preservation.
Compared to EDF \cite{wang2024emphasizing_arxiv2024} focusing on spatial region selection, AutoPalette targets redundancy within the color space, providing a complementary approach.
However, its effectiveness may be limited when
color is not a critical feature in datasets.

\noindent \textbf{Selective Parameter Optimization}
Conventional distillation methods often treat each network parameter equally, overlooking their varying contributions to the learning process.
Li et al. \cite{li2024importance_nn} highlighted this limitation and proposed the Adaptive Dataset Distillation (IADD) framework, which selectively assigns weights to parameters based on their contribution to learning.
This approach prioritizes critical parameters while maintaining challenging ones, resulting in improved performance and robust parameter matching across iterations.

% \subsubsection{Importance-Guided Dataset Distillation}
% Recent advances highlight the importance of identifying and optimizing key components that contribute most to model performance.
% We present a unified theoretical framework for importance-guided approaches across multiple dimensions, demonstrating how strategic selection and weighting of elements, from low-level features to high-level representations, enhance distillation efficiency.
% Formally, let $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ denote the original dataset and $\mathcal{S} = \{(\tilde{x}_j, \tilde{y}_j)\}_{j=1}^M$ represent the synthetic dataset, where $M \ll N$. 
% The importance-guided distillation objective can be formulated as:
% \begin{equation}
% \min_{\mathcal{S}} \mathbb{E}_{\theta_0} \sum_{t=0}^{T-1} \omega_t \sum_{d \in \mathcal{D}} \omega_d \mathcal{L}_d(\mathcal{S}, \mathcal{T})
% \end{equation}
% where $\omega_t$ captures importance weights across different timesteps of the training trajectory, $\omega_d$ represents importance weights for different dimensions (e.g., pixel, color, parameter, original samples), $\mathcal{L}_d$ denotes the dimension-specific loss function for measuring distillation quality.

\noindent\textbf{Sample Selection Strategies}
Dataset distillation methods often face significant challenges in effectively leveraging large-scale training data.
Sample selection methods tackle this problem through two main strategies: \textit{selection and pruning}, which reduce dataset size, and \textit{prioritization}, which dynamically emphasizes the most informative samples.


Selection and pruning methods aim to retain samples that best represent the underlying data distribution, enhancing the diversity and informativeness of the synthetic dataset.
Liu et al. \cite{liu2023dream_iccv2023} introduced DREAM, a clustering-based sample selection method designed to capture representative distributions, significantly enhancing training efficiency.
Building on this, DREAM+ \cite{liu2023dreamplus_arxiv2023} incorporates bidirectional matching to balance gradient and feature alignment, further improving stability and efficiency.
Moser et al. \cite{moser2024distill_arxiv2024} proposed the "Prune First, Distill After" framework, which combines dataset pruning with loss-value-based sampling.
This approach selects informative samples, reducing redundancy while maintaining performance.
Similarly, Bi-level Data Pruning (BiLP) \cite{xu2023distill_eccv2024} integrates preemptive pruning based on empirical loss and adaptive pruning leveraging causal effects, achieving up to significant data reduction with little degradation in performance.


Prioritization methods focus on dynamically ranking and weighting samples during training to emphasize their relative importance.
Wang et al. \cite{wang2024sdc_arxiv2024} introduced Sample Difficulty Correction (SDC), which prioritizes simpler samples based on gradient norms.
To adapt dataset difficulty dynamically, Lee et al. \cite{lee2024selmatch_icml2024} proposed SelMatch, which employs selection-based initialization and partial updates to fine-tune synthetic datasets.
By tailoring the training process to specific architectures and datasets, SelMatch improves generalization and convergence.
Chen et al. \cite{chen2023dataset_arxiv2023} extended prioritization by introducing an adversarial prediction matching framework.
This approach leverages teacher-student disagreements to identify informative samples, enhancing robustness across diverse architectures.
Tukan et al. \cite{tukan2023dataset_arxiv2023} proposed adaptive sampling and initialization strategies.
Focusing on data prioritization, Li et al. \cite{li2024prioritize_arxiv2024} proposed Prioritize Alignment in Dataset Distillation (PAD), improving distillation effectiveness through two key strategies: Data Filtering, which dynamically selects high-complexity samples based on EL2N scores \cite{paul2021deep_nips2021} to retain the most informative data, and Layer Confinement, which restricts the distillation process to deeper model layers, refining the prioritization of critical features while reducing noise from low-level representations.

\noindent \textbf{Stage Summary}
Sample selection methods enhance dataset distillation through two main approaches: reducing dataset size via selection and pruning, and optimizing training dynamics via prioritization.
Selection and pruning methods, such as DREAM+ and BiLP, reduce redundancy while preserving diversity and representational value.
In contrast, prioritization-based approaches, like SDC and SelMatch, dynamically adjust sample weights, improving generalization and training efficiency.
Future research could explore hybrid frameworks integrating various selection strategies for large-scale dataset distillation.

% \noindent \textbf{Pixel Importance}
% Effectively capturing pixel-level importance is crucial for enhancing dataset quality, especially when addressing inter-class variability and non-discriminative regions.
% Recent efforts have refined pixel-wise feature representation to emphasize informative pixels while suppressing redundant or misleading patterns.
% Wang et al. \cite{wang2024emphasizing_arxiv2024} proposed the Emphasizing Discriminative Features (EDF) framework, which leverages Common Pattern Dropout (CPD) to filter out non-salient pixels and Discriminative Area Enhancement (DAE) to intensify attention on essential regions using Grad-CAM \cite{selvaraju2017grad_iccv2017} activation maps.
% By adaptively adjusting pixel importance, EDF ensures synthesized datasets prioritize high-salience regions, leading to improved generalization and discriminability.


% \noindent \textbf{Color Importance}
% Color redundancy often leads to inefficient dataset utilization, reducing the informativeness of synthetic samples.  
% Yuan et al. \cite{yuancolor_nips2024} addressed this issue by introducing AutoPalette, a framework designed to optimize color usage while preserving image features.  
% AutoPalette condenses images into reduced color palettes via a palette network, balancing pixel distributions with maximum color loss and palette balance loss.  
% A color-guided initialization strategy further ensures diverse, informative synthetic samples, improving overall distillation performance.  
% This method uniquely balances color diversity with feature preservation.  
% Compared to EDF \cite{wang2024emphasizing_arxiv2024}, which emphasizes spatial regions, AutoPalette targets feature redundancy within the color space, providing a complementary approach. However, its effectiveness may be limited when color is not a critical feature in datasets.  


% \noindent \textbf{Parameter Importance}
% Conventional distillation methods often treat each element in network parameters equally, neglecting their varying importance in the learning process. 
% Li et al. \cite{li2024importance_nn} highlighted the limitations of treating all network parameters equally during distillation. 
% Their Importance-Aware Adaptive Dataset Distillation (IADD) framework assigns adaptive importance weights to parameters based on their significance, refining the distillation process through parameter-specific optimization. 
% This approach prioritizes critical parameters without discarding challenging ones, resulting in improved performance and robust parameter matching across iterations.


% \noindent\textbf{Sample Importance}
% Dataset distillation methods often face significant challenges in effectively leveraging large-scale training data. 
% Sample importance methods tackle this problem through two main strategies: \textit{selection and pruning}, which reduce dataset size, and \textit{prioritization}, which dynamically emphasizes the most informative samples.


% Selection and pruning methods aim to retain samples that best represent the underlying data distribution, enhancing the diversity and informativeness of the synthetic dataset. 
% Liu et al. \cite{liu2023dream_iccv2023} introduced DREAM, a clustering-based sample selection method designed to capture representative distributions, significantly enhancing training efficiency. 
% Building on this, DREAM+ \cite{liu2023dreamplus_arxiv2023} incorporates bidirectional matching to balance gradient and feature alignment, further improving stability and efficiency. 
% Moser et al. \cite{moser2024distill_arxiv2024} proposed the “Prune First, Distill After” framework, which combines dataset pruning with loss-value-based sampling. 
% This approach selects informative samples, reducing redundancy while maintaining performance. 
% Similarly, Bi-level Data Pruning (BiLP) \cite{xu2023distill_eccv2024} integrates preemptive pruning based on empirical loss and adaptive pruning leveraging causal effects, achieving up to significant data reduction with little degradation in performance.

% Prioritization methods, on the other hand, focus on dynamically ranking and weighting samples during training to emphasize their relative importance. 
% Wang et al. \cite{wang2024sdc_arxiv2024} introduced Sample Difficulty Correction (SDC), which prioritizes simpler samples based on gradient norms. 
% To adapt dataset difficulty dynamically, Lee et al. \cite{lee2024selmatch_icml2024} proposed SelMatch, which employs selection-based initialization and partial updates to fine-tune synthetic datasets. 
% By tailoring the training process to specific architectures and datasets, SelMatch improves generalization and convergence.
% Chen et al. \cite{chen2023dataset_arxiv2023} extended prioritization by introducing an adversarial prediction matching framework. 
% This approach leverages teacher-student disagreements to identify informative samples, enhancing robustness across diverse architectures.
% Tukan et al. \cite{tukan2023dataset_arxiv2023} proposed adaptive sampling and initialization strategies.
% Focusing on data prioritization, Li et al. \cite{li2024prioritize_arxiv2024} proposed Prioritize Alignment in Dataset Distillation (PAD), improving distillation effectiveness through two key strategies: Data Filtering, which dynamically selects high-complexity samples based on EL2N scores \cite{paul2021deep_nips2021} to retain the most informative data, and Layer Confinement, which restricts the distillation process to deeper model layers, refining the prioritization of critical features while reducing noise from low-level representations.  
% Those methods focus on optimizing sample prioritization, ensuring that the most informative and representative data are given more attention during the distillation process.

% \noindent \textbf{Stage Summary}  
% Sample importance methods enhance dataset distillation by reducing dataset size through selection and pruning or optimizing training dynamics via prioritization.  
% Selection and pruning methods, such as DREAM+ and BiLP, reduce redundancy while preserving diversity and representational value.  
% In contrast, prioritization-based approaches, like SDC and SelMatch, dynamically adjust sample importance, improving generalization and training efficiency.  
% Future research could explore hybrid frameworks integrating selection, pruning, and prioritization for large-scale dataset distillation.  


\subsubsection{Lossless Distillation}
Conventional dataset distillation methods often struggle with achieving lossless performance on complex datasets due to the challenges posed by various factors, such as inter-class variability and non-discriminative features. 
To address these issues, several recent approaches focus on achieving near-lossless or lossless performance by emphasizing critical patterns and refining trajectory alignment.

Guo et al. \cite{guo2024towards_iclr2024} laid the foundation for lossless distillation by introducing DATM, which dynamically aligns the difficulty of generated patterns with the synthetic dataset size. Specifically, early trajectories (easy patterns) are optimal for low-IPC settings, while late trajectories (hard patterns) benefit larger synthetic datasets. 
By controlling trajectory ranges and integrating soft label learning with sequential generation, DATM achieves near-lossless results across benchmarks such as CIFAR-10 and CIFAR-100.
Building on DATM, Zhou et al. \cite{zhou2024enhancing_arxiv2024} proposed M-DATM to address label inconsistencies and further optimize trajectory matching for more challenging datasets like Tiny ImageNet. 
Through deliberate refinements, M-DATM demonstrated superior performance, securing first place in the Fixed IPC Track at the ECCV-2024 Data Distillation Challenge.
Complementing trajectory-based approaches, Wang et al. \cite{wang2024emphasizing_arxiv2024} extended dataset distillation to high-variability datasets through feature importance optimization.  
By refining discriminative patterns, their method, EDF, demonstrated lossless performance on challenging subsets of ImageNet-1K, setting a new benchmark for dataset distillation.


\noindent \textbf{Stage Summary} These advancements collectively demonstrate the efficacy of adaptive selection strategies in lossless dataset distillation, where methods dynamically identify and prioritize the most informative and discriminative patterns while systematically minimizing the impact of redundant or less relevant information.
Moving forward, dynamic and context-aware adaptation techniques are expected to further enhance scalability and interpretability, enabling effective lossless distillation for large-scale datasets.



\subsubsection{Diversity of Distilled Data}
While importance-guided methods focus on selecting and prioritizing original samples, maintaining diversity in synthetic data remains a critical challenge in dataset distillation.
Recent approaches address this through curriculum learning, feature compensation, and semantic matching strategies.
Wang et al. \cite{ma2024curriculum_arxiv2024} introduced CUDD, which progressively increases synthetic data complexity through curriculum learning. 
By focusing on samples misclassified by student models, CUDD optimizes classification, regularization, and adversarial objectives to maintain diversity across training stages.
To address inter-class feature redundancy, Zhang et al. \cite{zhang2024breaking_arxiv2024} developed INFER with a Universal Feature Compensator (UFC). 
This "one instance for all classes" approach achieves superior performance on ImageNet through optimized feature diversity. 
Similarly, DELT \cite{shen2024delt_arxiv2024} enhances diversity through subtask partitioning with varying optimization schedules.
Du et al. \cite{Du2024DiversityDrivenSE_nips2024} observed limitations in instance-wise synthesis and proposed DWA (Directed Weight Adjustment), using variance-based regularization for batch-level diversity. 
DSDM \cite{li2024diversified_mm2024} further preserves semantic diversity by matching prototypes and covariance matrices of class distributions through pre-trained feature extractors.

\noindent \textbf{Stage Summary}  
These diversity-focused approaches complement original sample importance methods.  
While importance methods reduce redundancy at the original dataset level, diversity enhancement ensures richness at the distilled data level.  
Future research should explore hybrid frameworks that seamlessly integrate selection, prioritization, and diversity enhancement to create more generalizable, scalable dataset distillation pipelines.  



\subsubsection{Augmentation Strategies}
Augmentation techniques expand feature and label spaces to enhance the learning dynamics of distillation, promoting greater diversity and representational richness.  
These methods ensure that synthetic data captures a broad range of features while remaining adaptable to different factors.  

\noindent \textbf{Model Augmentation}
Building on diversity-focused methods, model augmentation techniques expand feature spaces during dataset distillation, directly addressing the need for richer representations.
Zhang et al. \cite{zhang2023accelerating_cvpr2023} proposed integrating two model augmentation techniques within a gradient-matching framework: early-stage models and weight perturbation. 
Early-stage models provide a diverse feature space with larger gradient magnitudes, offering enhanced flexibility and richer guidance for the distillation process. 
Weight perturbation further expands the feature space by introducing a normalized random vector, sampled from a Gaussian distribution, to the model weights.
This augmentation fosters diversity by enabling the exploration of a broader range of feature representations, which, in turn, enhances the effectiveness of the distillation process.
Together, these techniques allow for both efficient distillation and the generation of more representative synthetic datasets.

\noindent \textbf{Label Augmentation}
Beyond feature spaces, label augmentation introduces diversity through enriched label representations.
Kang et al. \cite{Kang2024LabelAugmentedDD_arxiv2024} introduced Label-Augmented Dataset Distillation (LADD) to leverage label augmentation for improved diversity and efficiency. 
LADD operates in two stages: first, synthetic images are generated using existing distillation algorithms; second, an image sub-sampling algorithm generates multiple local views for each synthetic image. 
A pre-trained labeler then produces dense semantic labels for these local views. During deployment, LADD combines global view images with their original labels and local view images with the newly generated dense labels. 
This dual-label strategy enhances storage efficiency, reduces computational overhead, and provides diverse learning signals, leading to improved robustness and performance across a range of architectures.

\noindent \textbf{Stage Summary} The advancement of augmentation strategies in dataset distillation hinges on overcoming critical research challenges.  
A key open problem is the automatic discovery of optimal augmentation strategies, which could benefit from reinforcement learning approaches \cite{10172347_pami2023} to dynamically tailor augmentation techniques to specific tasks and model architectures.  Achieving this requires a deeper understanding of how different augmentation types interact with various model architectures, ensuring that strategies are not only effective but also generalizable and transferable across diverse domains and tasks. 


\subsubsection{Extreme Compression Techniques}
Extreme compression techniques address storage constraints by drastically reducing dataset sizes while preserving training effectiveness.  
Shul et al. \cite{shul2024distilling_arxiv2024} introduced Poster Dataset Distillation (PoDD), which compresses entire datasets into a single poster image, achieving extreme compactness with less than one image per class.  
By leveraging semantic class ordering and efficient label management, PoDD balances high compression with robust training performance, making it well-suited for resource-constrained applications.  
Future research could explore expanding extreme compression strategies to large-scale datasets while improving adaptability across different model architectures.  


\subsection{Distillation in Non-IID and Non-centralized Settings}
As machine learning systems increasingly deploy in dynamic environments, the traditional assumptions of independent and identically distributed (IID) data become increasingly inadequate. 
Modern applications that are ranging from edge computing and mobile networks to decentralized  systems, demand models that can adapt to heterogeneous, shifting data landscapes. 
Dataset distillation correspondingly evolve to address these complex challenges by handling non-IID data distributions, ensuring fairness and robustness in open-world scenarios, and accommodating decentralized learning architectures like federated learning.

\subsubsection{Addressing Non-IID Challenges}
Adapting dataset distillation methods to real-world applications requires addressing Non-IID challenges, including handling out-of-distribution (OOD) data \cite{yang2024generalized_IJCV2024}, mitigating biases \cite{jiang2022dataset_pami2022}, ensuring fairness \cite{huang2024federated_pami2024}, and supporting self-supervised \cite{zong2024self_pami2024,gui2024survey_tpami2024} and transfer learning \cite{5288526_kdd2010}. 
Recent advancements in these areas extend the applicability of dataset distillation beyond traditional IID settings, enabling robust performance in dynamic and decentralized environments.

\noindent \textbf{Out-Of-Distributions}
In open-world scenarios, models must reliably identify and handle data samples that deviate from their training distribution. 
Traditional dataset distillation methods overlook this, focusing on IID data. 
To address this gap, Ma et al. proposed Trustworthy Dataset Distillation (TrustDD) \cite{ma2025towards_pr2025}, which integrates OOD detection into the distillation process. 
By generating synthetic outliers through Pseudo-Outlier Exposure (POE), TrustDD eliminates the need for curated OOD datasets, enabling robust OOD detection without compromising in-distribution (InD) accuracy or computational efficiency. 



\noindent \textbf{Biased DD and Fairness DD}
Dataset biases, such as color or background amplification, can significantly undermine the representational quality of distilled datasets. 
Lu et al. \cite{lu2024exploring_cvpr2024} analyzed how biases propagate through the distillation process and proposed mathematical frameworks for addressing biased dataset distillation. 
Cui et al. \cite{cuimitigating_icml2024} introduced a reweighting scheme combining supervised contrastive learning and kernel density estimation to effectively mitigate these biases.
To ensure fairness in synthetic datasets, Zhou et al. proposed FairDD \cite{zhou2024fairdd_arxiv2024}, which aligns datasets with protected attribute groups through synchronized matching, mitigating majority group dominance. 
Similarly, Zhao et al. introduced Long-tailed Aware Dataset Distillation (LAD) \cite{zhao2024distilling_arxiv2024}, which addresses imbalances in long-tailed datasets by incorporating Weight Mismatch Avoidance and Adaptive Decoupled Matching. 
These methods demonstrate the potential for bias-aware dataset distillation to improve fairness and tail-class performance.

\noindent \textbf{Self-supervised and Transfer Learning}  
While most dataset distillation research has focused on supervised learning, self-supervised and transfer learning \cite{zong2024self_pami2024, 5288526_kdd2010,gui2024survey_tpami2024} remain underexplored.  
Lee et al. \cite{lee2023self_iclr2024} proposed KRR-ST, which employs kernel ridge regression and mean squared error objectives to reduce randomness in the distillation process. 
This method facilitates efficient pretraining with unlabeled data, significantly lowering computational costs.  
Expanding on this, Yu et al. \cite{yu2025self_iclr2025} introduced an enhanced self-supervised DD framework that builds upon KRR-ST through three key innovations: image and representation parameterization, predefined augmentation, and approximation networks. 
These refinements improve cross-architecture generalization, transfer learning performance, and storage efficiency.  
Furhter, Joshi et al. introduced MKDT \cite{joshi2024dataset_arxiv2024}, a two-stage method that stabilizes SSL trajectories by first training student models via knowledge distillation from SSL-trained teachers. 
In alignment with these stabilized trajectories, MKDT achieves substantial performance gains in SSL pretraining and downstream tasks.

% \noindent \textbf{Self-supervised and Transfer Learning}
% While most dataset distillation research has focused on supervised learning, self-supervised and transfer learning  \cite{zong2024self_pami2024, 5288526_kdd2010,gui2024survey_tpami2024} remain underexplored. 
% Lee et al. proposed KRR-ST \cite{lee2023self_iclr2024}, which uses kernel ridge regression and mean squared error objectives to remove randomness from the distillation process. 
% This approach facilitates efficient pretraining with unlabeled data, significantly reducing computational costs.
% Further, Joshi et al. introduced MKDT \cite{joshi2024dataset_arxiv2024}, a two-stage method that stabilizes self-supervised learning (SSL) trajectories by first training student models via knowledge distillation from SSL-trained teachers. 
% In alignment with these stabilized trajectories, MKDT achieves substantial performance gains in SSL pretraining and downstream tasks.


\subsubsection{Addressing Non-centralized Challenges}
In the era of distributed machine learning, federated learning (FL) enables collaborative, privacy-preserving model training across decentralized environments. 
However, this paradigm introduces unique challenges, such as communication overhead, non-IID data, privacy concerns, and decentralized setups. 
Recent advancements in dataset distillation have addressed these challenges, categorized into communication efficiency, heterogeneity handling, privacy preservation, and decentralized/edge-focused scenarios.

\noindent \textbf{Communication Efficiency} 
Minimizing communication overhead is critical for scalable FL, especially in resource-constrained environments. 
Distilled One-Shot Federated Learning (DOSFL) \cite{zhou2020distilled_arxiv2020} introduced the idea of transmitting compact synthetic datasets in a single communication round, drastically reducing bandwidth usage. 
Building on this, FedSynth \cite{hu2022fedsynth_arxiv2022} replaced model updates with synthetic datasets, ensuring compatibility with standard FL frameworks. 
DENSE \cite{zhang2022dense_nips2022} and FedMK \cite{liu2022meta_iclr2023} avoided model aggregation by generating synthetic data that encapsulates local knowledge, while FedD3 \cite{song2023federated_ijcnn2023} optimized one-shot communication for edge scenarios. 
Further advancements, such as FedCache 2.0 \cite{pan2024fedcache_ap2024}, integrated distillation with knowledge caching to enhance efficiency in edge-device communication.


\noindent \textbf{Handling Heterogeneity}
Data and model heterogeneity pose fundamental challenges in federated learning, where clients may have vastly different data distributions, model architectures, and learning characteristics. 
FedDM \cite{xiong2023feddm_cvpr2023} and DYNAFED \cite{pi2023dynafed_cvpr2023} tackled data heterogeneity by generating compact pseudo datasets that align with local distributions. 
While FedDM iteratively refined synthetic datasets via distribution matching, DYNAFED leveraged early pseudo datasets with trajectory matching, prioritizing efficiency and privacy. 
FedAF (Aggregation-Free) \cite{wang2024aggregation_cvpr2024} mitigated client drift in non-IID settings by employing sliced Wasserstein regularization and collaborative condensation to harmonize client contributions. 
HFLDD \cite{shi2024dataset_arxiv2024} grouped clients into heterogeneous clusters, allowing cluster headers to aggregate distilled datasets and create approximately IID data, improving training efficiency in non-IID environments.


\noindent \textbf{Privacy Preservation}
Privacy concerns in FL have driven the integration of dataset distillation techniques that limit information sharing.
FedDGM (Federated Dataset Distillation with Deep Generative Models) \cite{jia2023unlocking_arxiv2023} utilized server-side pre-trained generators to distill datasets in latent space, avoiding the need to share raw data or updates while maintaining high performance. 
QUICKDROP \cite{dhasade2023quickdrop_arxiv2023} extended this concept by embedding dataset distillation into federated unlearning, reducing computational costs without sacrificing privacy or performance. 
Xu et al. \cite{xu2024flip_arxiv2024} applied the principle of least privilege (PoLP), where clients share only essential knowledge through local-global dataset distillation, further safeguarding sensitive information.

\noindent \textbf{Decentralized and Edge-Focused Scenarios}
In fully decentralized setups and resource-constrained edge environments, new frameworks have been developed to tackle the challenges of operating \textit{without} a central server and the limitations of on-device training resources. 
DESA (Decentralized Federated Learning with Synthetic Anchors) \cite{huang2024overcoming_icml2024} introduced synthetic anchors—generated datasets approximating global data distributions—to align local features across clients, enabling knowledge sharing without a central server. 
For incremental learning in edge environments, Rub et al. \cite{rub2024continual_icps} proposed integrating dataset distillation with adaptive model sizing, meeting the challenges of TinyML and on-device training while maintaining performance.



\noindent \textbf{Others} Additional advancements include DKKT \cite{lee2024practical_arxiv2024}, which enriched distilled datasets using deep support vectors, achieving robust performance with just $1\%$ of the original dataset. 
Distributed Boosting (DB) \cite{chen2024distributed_cikm2024} enhanced the performance of prior methods through partitioning, soft-labeling, and integration strategies in distributed computing environments, setting new benchmarks on large-scale datasets such as ImageNet-1K. 
HFLDD \cite{shi2024dataset_arxiv2024} integrates dataset distillation into a federated framework by organizing clients into heterogeneous clusters. 
Within each cluster, clients transmit distilled datasets to cluster headers, which aggregate the distilled data to construct approximately IID datasets, significantly improving training efficiency and addressing non-IID data challenges.


\subsection{Robustness in Dataset Distillation}
As dataset distillation becomes increasingly applied in various scenarios, ensuring its robustness has become a critical focus. 
Synthetic datasets derived through distillation must be resilient to adversarial attacks \cite{wei2024physical_tpami2024}, backdoor threats \cite{li2022backdoor_tnnls2022}, and privacy breaches to ensure their reliability and effectiveness in real-world applications. 
However, those requirements introduce unique vulnerabilities; for example, small perturbations or manipulations can disproportionately affect their performance.
To address these challenges, recent advancements have introduced robust evaluation benchmarks, innovative defense strategies, and in-depth explorations of vulnerabilities in distillation pipelines. 


\subsubsection{Adversarial Attack}
Adversarial robustness \cite{wei2024physical_tpami2024} in dataset distillation remains a relatively underexplored topic.
Recent studies have laid the groundwork for benchmarking and developing robust distillation techniques to enhance security.

\begin{figure}[t]
    \centering
\includegraphics[width=1.0\linewidth]{image/dd_robustbench.jpg}
    \caption{Overview of DD-RobustBench. Image from \cite{wu2024dd_arxiv2024}.}
    \label{fig:dd_robustbench}
\end{figure}


To assess the vulnerability of dataset distillation, Wu et al. \cite{wu2024dd_arxiv2024} introduced DD-RobustBench, a comprehensive benchmark for evaluating the robustness of dataset distillation methods, including techniques like TESLA, DREAM, SRe2L, and D4M, across datasets such as CIFAR-10 and ImageNet-1K. 
Their findings revealed that models trained on distilled datasets could exhibit superior robustness compared to those trained on original data under low IPC (images per class) settings, though an inverse relationship between robustness and IPC challenged the assumption that more distilled images always improve security. 
Following this, Zhou et al. \cite{zhou2024beard_arxiv2024} proposed BEARD, a unified evaluation framework introducing metrics like Robustness Ratio (RR), Attack Efficiency Ratio (AE), and Comprehensive Robustness-Efficiency Index (CREI) to assess robustness and efficiency holistically. 
Together, these benchmarks provide critical insights into balancing robustness and efficiency in adversarially robust distillation.
Together, these benchmarks establish a foundation for evaluating and improving the security of distillation techniques.


Beyond benchmarking, recent methods have focused on actively enhancing the adversarial robustness of distilled datasets. 
Xue et al. \cite{xue2024towards_arxiv2024} proposed Geometric regUlarization for Adversarial Robust Dataset (GUARD), a novel method incorporating curvature regularization to improve resilience against adversarial attacks. 
By minimizing the curvature of the loss function, GUARD reduces the upper bound of adversarial loss without significant computational overhead. 
This method achieves enhanced adversarial robustness with minimal computational overhead, paving the way for more secure and reliable distillation processes.



\subsubsection{Backdoor Attack}
Dataset distillation has been revealed to be vulnerable to backdoor attacks \cite{li2022backdoor_tnnls2022}, where attackers can embed hidden triggers that cause targeted misclassifications even after the dataset distillation process \cite{liu2023backdoor_ndss}.
To handle this challenge, Chung et al. \cite{chung2023rethinking_arxiv2023} analyzed backdoor attacks using a kernel method framework, explaining the resilience of certain backdoors in distilled datasets. 
They proposed two novel trigger generation strategies: simple-trigger, which exploits the observation that larger triggers reduce the generalization gap, and relax-trigger, which minimizes both conflict and projection losses while maintaining small generalization gaps. 
Empirical evaluations demonstrated that both methods evade existing backdoor detection and defense techniques effectively, with relax-trigger exhibiting superior robustness across eight tested defenses.

Extending backdoor attack risks beyond Euclidean data, Wu et al. \cite{wu2024backdoor_arxiv2024} introduced the first backdoor attack framework for graph condensation, named BGC. 
BGC injects malicious triggers into graph structures and optimizes them iteratively throughout the condensation process. 
To maximize attack effectiveness under constrained resources, BGC employs a representative node selection mechanism to identify and poison key nodes strategically. 
Experiments confirmed BGC’s high attack success rates and robust performance, even against multiple defense strategies, highlighting the critical need for developing secure graph condensation.


\subsubsection{Beyond Adversarial and Backdoor Attacks}
While adversarial and backdoor attacks represent critical security concerns in dataset distillation, recent studies have uncovered additional vulnerabilities that demand attention. 
These include privacy-related threats such as membership inference attacks, which jeopardize the confidentiality of the training process and expose sensitive data.
Chen et al. \cite{chen2023comprehensive_arxiv2023} conducted the first comprehensive study on the security properties of dataset distillation methods, specifically analyzing their vulnerability to membership inference attacks \cite{hu2022membership}. 
These attacks expose significant privacy concerns, as adversaries can infer whether a particular sample was used in the training process. 


\subsection{Model-agnostic Solutions}
A major challenge in dataset distillation is the limited cross-architecture generalization capability of synthetic datasets. 
This issue arises when the distillation process becomes overly specialized to the architecture used during training, limiting the versatility of distilled datasets. 
Recent efforts have tried addressing this limitation by developing model-agnostic solutions that enhance the adaptability and transferability of synthetic datasets across diverse architectures.

\noindent \textbf{Gradient Balancing and Semantic Alignment}
Moon et al. \cite{moon2024towards_eccv2024} introduced Heterogeneous Model Dataset Condensation (HMDC), a pioneering approach to creating universally applicable condensed datasets. 
HMDC tackles the gradient imbalance problem, where contributions from different architectures vary due to gradient magnitude discrepancies, by employing a Gradient Balance Module (GBM). This module normalizes gradients to ensure equal contributions from heterogeneous models. 
To address misalignment, HMDC incorporates Mutual Distillation (MD) with Spatial-Semantic Decomposition (SSD), aligning both semantic and spatial features across models. 
These innovations ensure consistent feature representation and robust generalization, making HMDC a benchmark for cross-architecture adaptability.

\noindent \textbf{Mitigating Inductive Bias}
To combat architecture-specific inductive biases, Zhao et al. proposed two complementary methods: ELF \cite{zhao2023boosting_arxiv2023} and MetaDD \cite{zhao2024metadd_arxiv2024}. ELF leverages external supervision by using bias-free intermediate features extracted from original datasets to guide the training of evaluation models. 
In contrast, MetaDD takes an internal approach, disentangling features into architecture-invariant meta features and architecture-variant heterogeneous features. 
This ensures that generalizable features dominate the distillation process. While ELF emphasizes external supervisory signals to counteract overfitting, MetaDD focuses on directly optimizing the feature space. 
Together, these approaches significantly enhance the cross-architecture generalization of synthetic datasets.

\noindent \textbf{Architectural Diversity and Ensemble Techniques}
Recent advances have focused on leveraging multiple architectures to enhance robustness. 
Zhou et al. \cite{zhou2024improve_arxiv2024} proposed a model pool framework to balance architectural diversity and training stability. 
By combining multiple similar architectures and selecting a primary model most of the time, this method ensures robust performance across various architectures. 
Knowledge distillation further aligns student and teacher models, improving generalization.
Similarly, Zhong et al. \cite{zhong2023towards_arxiv2023} addressed architectural overfitting with a set of complementary techniques. 
These include a DropPath variant for implicit subnetwork ensembling, reverse knowledge distillation where smaller models teach larger ones, and optimizations like periodic learning rate schedules and advanced data augmentations. 
% Collectively, these strategies enhance the efficiency and robustness of the distillation process, paving the way for versatile synthetic datasets.

\noindent \textbf{Stage Summary} While these works have made significant progress in addressing cross-architecture generalization,  challenges persist in the field. 
The scalability to larger and more diverse architectures, such as Transformers \cite{han2022survey_tpami2022}, remains a crucial concern. 
Future research in this area might explore adaptive architecture-aware distillation strategies that dynamically adjust to different model architectures. 




\section{Emerging Applications and Domains}
Dataset distillation has extended beyond image classification to complex data types such as temporal sequences, multi-modal data, and domain-specific challenges like medical imaging. 
These advancements underscore its adaptability across fields, including video understanding, natural language processing, and multi-modal learning. 
The following subsections summarize key applications, demonstrating how distillation techniques tackle domain-specific challenges while leveraging core methodologies.

\subsection{Temporal Domain}
The temporal domain introduces unique challenges for dataset distillation due to the sequential and time-dependent nature of video and audio data.
Recent advancements have focused on designing frameworks that effectively capture temporal dynamics, ensuring robust representation and efficient compression across these modalities.

\noindent \textbf{Video Domain}
Wang et al. \cite{wang2024dancing_cvpr2024} conducted the first systematic study on video dataset distillation, classifying temporal compression methods into four dimensions: synthetic frames, real frames, segments, and interpolation algorithms. 
They proposed a two-stage framework that disentangles static and dynamic information, integrating single-frame static memory with dynamic memory blocks to synthesize realistic video data. 
Building on this, Chen et al. \cite{Chen2024ALS_arxiv2024} introduced an efficient temporal condensation strategy incorporating slide-window sampling and adaptive trajectory matching, significantly improving performance on action recognition benchmarks while  reducing training costs.


\noindent \textbf{Audio Domain} Jiang et al. \cite{jiang2024ddfad} introduced DDFAD for audio data, leveraging Fused Differential MFCC (FD-MFCC) to combine traditional MFCC features with first- and second-order derivatives, improving feature richness. 
Using the Matching Training Trajectory  technique, DDFAD achieves performance comparable to full datasets while reducing computational and storage demands.

\noindent \textbf{Sequential Data}
Zhang et al. \cite{zhang2025td3_www2025} introduce Tucker Decomposition based Dataset Distillation (TD3), a novel Tucker Decomposition-based framework for dataset distillation in sequential recommendation, demonstrating an innovative approach to generating compact yet expressive synthetic sequence summaries.

\subsection{Multi-modal Dataset Distillation}
Recent advancements have extended dataset distillation to multi-modal tasks, addressing the unique challenges of integrating vision, language, and audio modalities. Vision-language learning \cite{wu2024visionlanguage,xu2024low_icml2024} and audio-visual learning \cite{kushwaha2024audiovisual_tmlr2024} have emerged as prominent areas of exploration.
Wu et al. \cite{wu2024visionlanguage} introduced a bi-trajectory matching framework for vision-language distillation, aligning image-text correspondences using contrastive loss and trajectory matching. 
By incorporating Low-Rank Adaptation (LoRA) \cite{hu2021lora_arxiv2021}, this method enhances efficiency while maintaining robust cross-modal representations.
Complementing this, Xu et al. \cite{xu2024low_icml2024} proposed Low-Rank Similarity Mining (LoRS), which distills similarity matrices alongside image-text pairs. 
Using low-rank factorization, LoRS efficiently approximates similarity matrices while refining anchors for contrastive learning, improving representation and compression.

Expanding on these developments, Kushwaha et al. \cite{kushwaha2024audiovisual_tmlr2024} introduced audio-visual dataset distillation, a technique that compresses large audio-visual datasets into compact synthetic datasets while preserving cross-modal relationships. 
Their framework builds on vanilla distribution matching by incorporating two novel losses: implicit cross-matching and cross-modal gap matching, ensuring better alignment between synthetic and real data.




\subsection{Medical Domain}  In medical imaging, Li et al. \cite{li2024dataset_arxiv2024} examined dataset distillation’s feasibility across nine diverse medical datasets, demonstrating its ability to preserve diagnostic details while reducing dataset sizes. 
They observed that larger inter-class variations yield better results and proposed random selection as a heuristic predictor of distillation efficacy. 
Yu et al. \cite{yu2024progressive_arxiv2024} addressed training instability caused by SGD oscillations with a progressive trajectory matching strategy. 
By gradually increasing trajectory step size and introducing a dynamic overlap mitigation module with Maximum Mean Discrepancy \cite{smola2006maximum}, Yu et al. achieved stable training and notable performance improvements, especially under low IPC conditions. 
Complementing these efforts, Li et al. \cite{li2024image_miccai2024} proposed InfoDist, a privacy-preserving framework using class-conditional latent diffusion models to generate synthetic histopathology datasets. 
By selecting informative images based on modular centrality and enhancing performance through contrastive learning, InfoDist safeguards data privacy while ensuring competitive accuracy.

\subsection{Other Applications} 
\noindent \textbf{Scientific Discovery} Dataset distillation has demonstrated its versatility in tackling scientific challenges, such as galaxy morphology analysis \cite{guan2023discovering_arxiv2023}.
% , and practical applications, like object detection \cite{qifetch_nips2024}, image super-resolution \cite{Dietz2025ASI_arxiv2025}.
Guan et al. \cite{guan2023discovering_arxiv2023} applied dataset distillation to galaxy morphology analysis, proposing Self-Adaptive Trajectory Matching (STM) to improve upon MTT. 
STM uses statistical hypothesis testing to adaptively monitor validation loss, ensuring efficient stopping criteria and reduced hyperparameter tuning.
Their work includes a curated high-confidence version of the Galaxy Zoo 2 dataset \cite{willett2013galaxy}, successfully distilling morphological features and achieving superior performance.

\noindent \textbf{Object Detection and Super-Resolution} For object detection, Qi et al. \cite{qifetch_nips2024} introduced DCOD, the first dataset condensation framework for this task, delivering competitive results on benchmarks like Pascal VOC \cite{hoiem2009pascal} and MS COCO \cite{lin2014microsoft_eccv2014}.
Dietz et al. \cite{Dietz2025ASI_arxiv2025} demonstrate the first comprehensive exploration of dataset distillation for image super-resolution (SR) \cite{liu2022blind_pami2022}, successfully reducing the original dataset size by 91.12\% while maintaining comparable model performance through both pixel-space and latent-space distillation techniques, thereby offering a promising memory- and computation-efficient approach.



\noindent \textbf{Enhancing Data Quality} Dataset distillation has  shown promise in noise-related challenges and improving dataset quality.
Cheng et al. \cite{cheng2024dataset_arxiv2024} proposed leveraging dataset distillation as a novel denoising tool for learning with noisy labels, addressing challenges like feedback loops in traditional noise evaluation strategies. 
Through extensive experiments with methods like DATM, DANCE, and RCIG, they demonstrate the effectiveness of dataset distillation in removing random and natural noise, though it struggles with structured asymmetric noise. 
This approach also improves training efficiency and privacy by avoiding direct use of noisy data and enabling offline processing.
Building upon the insights into noisy label handling through distillation, Wu et al. \cite{wu2025trustawarediversiondataeffectivedistillation_arxiv2025} proposed Trust-Aware Diversion (TAD). 
Rather than just using distillation as a denoising tool \cite{cheng2024dataset_arxiv2024}, TAD introduces a dual-loop optimization framework that actively manages noisy labels during the distillation process itself.
In TAD, an outer loop  separates data into trusted and untrusted spaces to guide distillation, while an inner loop recalibrates untrusted samples to maximize their utility. 
UniDetox \cite{lu2025unidetox_iclr2025} utilized dataset distillation approach to universally mitigate toxic content in large language models by systematically refining training datasets, thereby reducing harmful biases while preserving model performance across different domains.



\section{Performance Comparison}
Dataset distillation methods have advanced significantly, as demonstrated in Table \ref{tab:survey_comparison}, which compares state-of-the-art techniques across CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-1K under different IPC settings (IPC=1,10,50,100,200), as well as Table \ref{tab:single-dataset-comparison}, which compares state-of-the-art techniques across ImageNet-21K under different IPC settings.
These evaluations provide insights into efficiency and scalability, offering a comprehensive view of dataset distillation capabilities.
Over the past two years, the field has seen rapid progress, particularly in low-IPC scenarios, where newer approaches substantially improve upon early methods. For instance, on CIFAR-10 with IPC=1, early techniques such as DC \cite{zhao2020dataset_iclr2021} achieved only 28.3\% accuracy, whereas recent advances like AutoPalette \cite{yuancolor_nips2024} have pushed this performance to 58.6\%, demonstrating a significant 30.3\% absolute improvement. Similar trends are observed across larger-scale datasets. 
On CIFAR-100, the performance has improved from 12.8\% (DC) to 38.0\% (AutoPalette) with IPC=1, while on ImageNet-1K, recent methods like D4M \cite{su2024d_cvpr2024} achieve 66.5\% accuracy with IPC=100, showcasing the scalability of modern distillation approaches.
Furthermore, the emergence of diverse methodologies, including SRe2L, Selective-based, and Diversity-driven approaches, has contributed to these improvements. 
As shown in Table \ref{tab:single-dataset-comparison}, even on the challenging ImageNet-21K dataset, modern methods like CUDD \cite{ma2024curriculum_arxiv2024} achieve promising results, reaching 34.9\% accuracy with IPC=20. 
These advancements demonstrate the field's progress in handling increasingly complex and large-scale datasets while maintaining efficiency through low IPC settings.

\subsection{Impact of IPC Settings and Practical Considerations}
The relationship between IPC settings and model performance exhibits distinct patterns across different scales of datasets and methodologies. Analysis of recent results reveals three characteristic regions of IPC impact:

(1) Low IPC regime (IPC $\leq$ 10) demonstrates the most dramatic improvements in performance. 
On CIFAR-10, D3M \cite{abbasi2024one_arxiv2024} shows a substantial improvement from 35.9\% (IPC=1) to 58.6\% (IPC=10), while DANCE \cite{Zhang2024_ijcai2024} improves from 47.1\% to 70.8\%. 
Similar patterns are observed on CIFAR-100, where methods like EDC \cite{shao2024elucidating_nips2024} achieve significant gains from IPC=1 to IPC=10.
(2) Mid-range IPC values (10-50) show continued but moderated improvements. For instance, INFER \cite{zhang2024breaking_arxiv2024} on CIFAR-100 improves from 50.2\% (IPC=10) to 65.1\% (IPC=50). 
This range often represents the optimal trade-off between compression and performance for practical applications.
(3) High IPC settings (50-100) exhibit diminishing returns across datasets. On CIFAR-100, PAD \cite{li2024prioritize_arxiv2024} shows only marginal gains from IPC=50 (55.9\%) to IPC=100 (58.5\%). 
This pattern is consistently observed in larger datasets like ImageNet-1K, as demonstrated by methods such as CDA \cite{yin2023dataset_arxiv2023} and G-VBSM \cite{shao2024generalized_cvpr2024}.

The systematic evaluation of high IPC settings (IPC$>$50) on large-scale datasets such as ImageNet-1k and ImageNet-21K was largely unfeasible two years ago due to computational and methodological constraints. 
Recent advances have overcome these limitations, enabling comprehensive analysis of IPC scaling effects across diverse datasets and methods. 
For example, methods like D4M \cite{su2024d_cvpr2024} and DELT \cite{shen2024delt_arxiv2024} now demonstrate strong performance even at high IPC values on ImageNet-1K, with D4M achieving 66.5\% accuracy at IPC=100, 68.1\% accuracy at IPC=200. 
While higher IPC values can achieve better performance, the increased storage and computational requirements may outweigh the marginal gains in many practical scenarios. 
For efficient pretraining and real-world applications, IPC values between 10 and 50 typically offer the most practical balance of performance and resource efficiency.


\subsection{Performance Analysis Across Dataset Scales and Complexities}
Performance varies significantly across datasets, revealing clear trends as dataset scale and complexity increase.
Recent methods have achieved remarkable progress on CIFAR-10, with several approaches exceeding 50\% accuracy at IPC=1, a substantial improvement over earlier methods that reached around 30\%.
However, this success diminishes as complexity grows, such as in the transition from CIFAR-10 to CIFAR-100, where the increased class number and greater inter-class variability pose significant challenges

Quantitative analysis reveals consistent performance degradation across dataset scales. 
For instance, DATM \cite{guo2024towards_iclr2024} achieves 46.9\% accuracy on CIFAR-10 with IPC=1 but drops to 27.9\% on CIFAR-100. 
Similarly, RDED \cite{sun2024diversity_cvpr2024} shows a dramatic decrease from 22.9\% on CIFAR-10 to 11.0\% on CIFAR-100 at IPC=1. 
This pattern becomes more pronounced with larger-scale datasets.
DATM's performance drops from 66.8\% on CIFAR-10 to 31.1\% on Tiny ImageNet at IPC=10, highlighting the challenges of preserving semantic information under limited IPC settings as the complexity increases.

The transition to high-resolution, large-scale datasets like ImageNet-1K and ImageNet-21K introduces challenges beyond increased class counts. 
These datasets feature more complex visual characteristics, including varied perspectives and extensive intra-class variations. 
Recent methods address these challenges through innovative approaches. 
For example, INFER \cite{zhang2024breaking_arxiv2024} and DWA \cite{Du2024DiversityDrivenSE_nips2024} leverage diversity-driven strategies and advanced regularization techniques to enhance representational quality. 
On ImageNet-1K, these methods demonstrate promising scalability, with DWA achieving 55.2\% accuracy at IPC=50 and INFER showing robust performance across different IPCs.

The scaling to ImageNet-21K presents an even greater challenge with its massive $21,000$ classes. 
Nevertheless, recent methods have shown encouraging results. CUDD \cite{ma2024curriculum_arxiv2024} achieves 34.9\% accuracy at IPC=20, while CDA \cite{yin2023dataset_arxiv2023} reaches 26.4\% at the same IPC setting. 
Even with limited IPC (IPC=10), methods like EDC \cite{shao2024elucidating_nips2024} and RDED \cite{sun2024diversity_cvpr2024} maintain reasonable performance at 26.8\% and 25.6\% respectively. 
These results on ImageNet-21K, though lower than those on smaller datasets, represent significant progress in scaling dataset distillation to extremely large-scale scenarios.

These advancements represent significant progress in scaling dataset distillation to complex, real-world scenarios. 
However, the persistent performance gap between smaller and larger datasets indicates that maintaining distillation quality across varying dataset scales remains a key challenge in the field. 
The success of diversity-focused and regularization-enhanced approaches suggests promising directions for future research in handling large-scale datasets.



\begin{table*}[ht]
    \centering
    \caption{Performance comparison of dataset distillation methods across four datasets (CIFAR-10/100, Tiny ImageNet, and ImageNet-1K) under different IPCs. R18 denotes ResNet18 architecture. Methods without explicit R18 notation use ConvNet as the default architecture.}\label{tab:performance}
    % \begin{tabular}{|l|l|cc|cc|cc|cc|c|}
    \rowcolors{2}{rowcolor1}{rowcolor2}
    \tiny
        \begin{tabular}{|l|c|c|cccc|cccc|cccc|ccccc|}
        \hline
        % \multirow{2}{*}{Methods} & \multirow{2}{*}{Schemes} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{SVHN} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c|}{CIFAR-100} & \multirow{2}{*}{Tiny ImageNet} \\
                \rowcolor{white}\global\rownum=\numexpr\rownum-1\relax
     \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Schemes}} & 
     \multirow{2}{*}{\textbf{Venue}} &
     \multicolumn{4}{c|}{\textbf{CIFAR-10}} & \multicolumn{4}{c|}{\textbf{CIFAR-100}} & \multicolumn{4}{c|}{\textbf{Tiny ImageNet}} & \multicolumn{5}{c|}{\textbf{ImageNet-1K}}   \\
        & & &1& 10 & 50 & 100 & 1& 10 & 50 & 100 & 1& 10 & 50& 100 & 1& 10 & 50 & 100 &200  \\
        \hline
        DD \cite{wang2018dataset_arxiv2018} & META & arXiv/2018 & 42.8 & - & - & - & - & - & - & - & - & - & - & -& - & - & -& -& -\\ 
        RaT-BPTT \cite{feng2023embarrassingly_iclr2023} & META& ICLR/2023 & 53.2 & 69.4 & 75.3 & - & 35.3 & 47.5 & 50.6 & - & 20.1 & 24.4 & - & -& - & - & -& -& -\\
        Teddy \cite{yu2025teddy_eccv2024} & META& ECCV/2024 & 30.1 & 53.0 & 66.1 & - & 13.5& 33.4 & 49.4 & - & - & - & 45.2 & 52.0& - & 34.1 & 52.5& 56.5& -\\ \hline
        DC \cite{zhao2020dataset_iclr2021} & GM& ICLR/2021 & 28.3 & 44.9 & 53.9 & - & 12.8 & 25.2 & - & - & - & - & 11.2 & -& - & - & -& -& -\\
        DSA \cite{zhao2021dataset_icmlr2021} & GM& ICML/2021 & 28.8 & 52.1 & 60.6 & - & 13.9& 32.3 & - & - & - & - & 25.3 & -& - & - & -& -& -\\ \hline
        MTT \cite{cazenavette2022dataset_cvpr2022} & TM& CVPR/2022 & 46.3 & 65.3 & 71.6 & - & 24.3& 40.1 & 47.7 & - & 8.8 & 23.2 & 28.0 & 33.7& - & - & -& -& -\\
        FTD \cite{du2023minimizing_cvpr2023} & TM& CVPR/2023 & 46.8 & 66.6 & 73.8 & - & 25.2& 43.4 & 50.7 & - & 10.4 & 24.5 & - & -& - & - & -& -& -\\
        TESLA \cite{cui2023scaling_icml2023} & TM& ICML/2023 & 48.5 & 66.4 & 72.6 & - & 24.8& 41.7 & 47.9 & - & - & - & - & -& 7.7 & 17.8 & 27.9& -& -\\
        AST \cite{shen2023ast_arxiv2023} & TM& arXiv/2023 & 48.8 & 67.1 & 74.6 & - & 26.6& 44.4 & 51.7 & - & 13.7 & 25.7 & - & -& - & - & -& -& -\\
            % TESLA \cite{cui2023scaling_icml2023} & TM& ICML/2023 & 48.5 & 66.4 & 72.6 & - & 24.8& 41.7 & 47.9 & - & - & - & - & -& 7.7 & 17.8 & 27.9& -& -\\
        MCT \cite{zhong2024towards_arxiv2024} & TM& arXiv/2024 & 48.5 & 66.0 & 72.3 & - & 24.5& 42.5 & 46.8 & - & 9.6 & 22.6 & 27.6 & -& - & - & -& -& -\\
        Li et al. \cite{li2024dataset_ieice2024} & TM& IEICE/2024 & 46.4 & 65.5 & 71.9 & - & 24.6& 43.1 & 48.4 & - & - & - & - & -& - & - & -& -& -\\
        % TESLA \cite{cui2023scaling_icml2023} & TM& ICML/2023 & 48.5 & 66.4 & 72.6 & - & 24.8& 41.7 & 47.9 & - & - & - & - & -& 7.7 & 17.8 & 27.9& -& -\\
        ATT \cite{liu2024dataset_eccv2024} & TM& ECCV/2024 & 48.3 & 67.7 & 74.5 & - & 26.1 & 44.2  & 51.2 & - & 11.0 & 25.8 & - & -& - & - & -& -& -\\
        % NSD \cite{yang2024neural_eccv2024}  & TM+Freq& ECCV/2024 & 68.5 & 73.4 & 75.2 & - & 36.5  &  46.1 & - & - & 21.3 & - & - & -& - & - & -& -\\
        % MCT \cite{zhong2024towards_arxiv2024}   & TM& arXiv/2024 & 48.5 & 66.0 & 72.3 & - & 24.5  &  42.5 & 46.8 & - & 9.6 & 22.6 & 27.6 & -& - & - & -& -\\ 
        \hline
        CAFE \cite{wang2022cafe_cvpr2022}& DM& CVPR/2022 & 30.3 & 46.3 & 55.5 & - & 12.9  &  27.8 & 37.9 & - & - & - & - & -& - & - & -& -& -\\
        DM \cite{zhao2023dataset_wacv2023}  & DM& WACV/2023 & 26.0 & 48.9 & 63.0 & - & 11.4  &  29.7 & 43.6 & - & 3.9 & 12.9 & 24.1 & -& 1.3 & 5.7 & 11.4& -& -\\
        DataDAM \cite{sajedi2023datadam_iccv2023} & DM& ICCV/2023 & 32.0 & 54.2& 67.0 & - & 14.5  &  34.8 & 49.4 & - & 8.3 & 18.7 & 28.7 & -& 2.0 & 6.3 & 15.5& -& -\\ %+attention
        IDM \cite{zhao2023improved_cvpr2023} & DM& CVPR/2023 & 45.6 & 58.6 & 67.5 & - & 20.1  &  45.1 & 50.0 & - & 10.1 & 21.9 & 27.7 & -& & -- & - & -& -\\
        WMDD \cite{liu2023dataset_arxiv2023}  & DM& arXiv/2023 & - & -& - & - & -  &  - & - & - & 7.6& 41.8 & 59.4 & 61.0 & 3.2 & 38.2 & 57.6& 60.7& -\\
        Rahimi et al. \cite{Malakshan2024distilling_arxiv2024} & DM& arXiv/2024 & 27.9 & 53.0& 65.6 & - & 13.5  &  33.9 & 45.3 & - &4.9 & 17.2 & 27.4 &  - &2.1 & 7.5 & 15.6 & -& -\\
        DANCE \cite{Zhang2024_ijcai2024}  & DM& IJCAI/2024 & 47.1 & 70.8& 76.1 & - & 27.9  &  49.8 & 52.8 & - &11.6 & 26.4 & 28.9 &  - &- & - & -& - & -\\
        % Rahimi et al. \cite{Malakshan2024distilling_arxiv2024} & DM& arXiv/2024 & 27.9 & 53.0& 65.6 & - & 13.5  &  33.9 & 45.3 & - &4.9 & 17.2 & 27.4 &  - &2.1 & 7.5 & 15.6 & -& -\\
        M3D \cite{zhang2024m3d_aaai2024} & DM& AAAI/2024 & 45.3 & 63.5& 69.9 & - & 26.2  &  42.4 & 50.9 & - & - & - & -& - & -& - & - & -& -\\ %+HighOrder
        LQM \cite{wei2024dataset_cvprw2024} & DM& CVPRW/2024 & 45.9 & 60.9& 70.2 & - & 27.2  &  47.7 & 52.4 & - & 10.4 & 20.8 & 24.3 & -& - & - & -& -& -\\
        % WMDD \cite{liu2023dataset_arxiv2023}  & DM& arXiv/2023 & - & -& - & - & -  &  - & - & - & 7.6& 41.8 & 59.4 & 61.0 & 3.2 & 38.2 & 57.6& 60.7& -\\
        Deng et al. \cite{deng2024exploiting_cvpr2024} & DM& CVPR/2024 & 47.1 & 59.9& 69.0 & - & 24.6  &  45.7 & 51.3 & - &10.0 & 23.3 & 27.5 &  - &- & - & - & -& -\\
        % Rahimi et al. \cite{Malakshan2024distilling_arxiv2024} & DM& arXiv/2024 & 46.8 & 60.2& 68.8 & - & 21.6  &  47.2 & 51.3 & - &11.6 & 23.8 & 28.9 &  - &- & - & - & -\\
        % DANCE \cite{Zhang2024_ijcai2024}  & DM& IJCAI/2024 & 47.1 & 70.8& 76.1 & - & 27.9  &  49.8 & 52.8 & - &11.6 & 26.4 & 28.9 &  - &- & - & - & -\\ 
        \hline
        % GlaD \cite{cazenavette2023generalizing_cvpr2023} & Latent& CVPR/2023 & 30.1(A) & -& - & - & -  &  - & - & - &- & - & - &  - &- & - & - & -\\
        FreD \cite{shin2024frequency_nips2023} & Latent& NeurIPS/2023 & 60.6 & 70.3& 75.8 & - & 34.6  &  42.7 & 47.8 & - &- & - & - &  - &- & - & - & -& -\\ 
        NSD \cite{yang2024neural_eccv2024}  & Freq& ECCV/2024 & 68.5 & 73.4 & 75.2 & - & 36.5  &  46.1 & - & - & 21.3 & - & - & -& - & - & -& -& -\\
\hline
MIM4DD  \cite{shang2024mim4dd_nips2023} &PP& NeurIPS/2023 & 51.9 & 70.8 & 74.7& - & 31.1 & 47.4  &  - & - & -&-  & - &  - &- & - & -& - & -\\
SeqMatch \cite{du2024sequential_nips2024} &PP& NeurIPS/2024 & - & 68.3 & 75.3& - & - & 45.1  &  51.9 & - & -&23.8  & - &  - &- & -& - & - & -\\
% MIM4DD  \cite{shang2024mim4dd_nips2023} &PP& NeurIPS/2023 & 51.9 & 70.8 & 74.7& - & 31.1 & 47.4  &  - & - & -&-  & - &  - &- & - & -& - & -\\
% CMI \cite{zhong2024going_arxiv2024} &PP& ICLR/2025 & - & 70.0 & 76.6& - & -& 46.6 & 53.8  &  -  & 10.4&25.7  & 30.1 &  - &- & 24.2 & 49.1 & 54.6& -\\
FYI \cite{son2024fyi_eccv2024} &PP& ECCV/2024 & 52.5 & 68.2 & 75.2& - & 28.9& 45.8 & 50.8  &  -  & 11.6&26.8  & 30.1 &  - &- & - & - & -& -\\
CMI \cite{zhong2024going_arxiv2024} &PP& ICLR/2025 & - & 70.0 & 76.6& - & -& 46.6 & 53.8  &  -  & 10.4&25.7  & 30.1 &  - &- & 24.2 & 49.1 & 54.6& -\\
% PTM \cite{lu2023can_arxiv2023}&PP& arXiv/2023 & 52.5 & 68.2 & 75.2& - & 28.9& 45.8 & 50.8  &  -  & 11.6&26.8  & - &  - &- & - & - & -\\
\hline
% IT-GAN \cite{zhao2022synthesizing_arxiv2022} &Gen& NeurIPSW/2022 & - & - & -& - & - & - & -& -& - & - & -& - &- & - & - & -\\
DiM \cite{wang2023dim_arxiv2023} &Gen& arXiv/2023 & 51.3 & 66.2 & 72.6& - & - & - & -& -& - & - & -& - &- & - & - & -& -\\
Zhang et al. \cite{zhang2023dataset_arxiv2023}&Gen& arXiv/2023 & 48.2 & 66.2 & 73.8& - & 26.1 & 41.9 & 48.5& -& - & - & -& - &7.9 & 17.6 & 27.2 & -& -\\
D3M \cite{abbasi2024one_arxiv2024}&Gen& arXiv/2024 & 35.9 & 58.6 & 70.5& - & 30.8 & 49.1 & 54.5& -& 11.4 & 38.8 & 51.4 & -& 5.0 &23.6 & 32.2 & -& -\\
Li et al. \cite{li2024generative_cvpr2024} &Gen& CVPRW/2024 & 52.3 & 66.7 & 73.1& - & - & - & -& -& - & - & -& - &- & - & - & -& -\\
% DiM \cite{wang2023dim_arxiv2023} &Gen& arXiv/2023 & 51.3 & 66.2 & 72.6& - & - & - & -& -& - & - & -& - &- & - & - & -& -\\
% Zhang et al. \cite{zhang2023dataset_arxiv2023}&Gen& arXiv/2023 & 48.2 & 66.2 & 73.8& - & 26.1 & 41.9 & 48.5& -& - & - & -& - &7.9 & 17.6 & 27.2 & -& -\\
Gu et al. \cite{gu2024efficient_cvpr2024}&Gen& CVPR/2024 & - & - & -& - & - & - & -& -& - & - & -& - &- & 44.3 & 58.6 & -& -\\
D4M \cite{su2024d_cvpr2024} &Gen& CVPR/2024 & - & 56.2 & 72.8& - & - & 45.0 & 48.8& -& - & - & 51.0 & 55.3 & -&34.2 & 63.4 & 66.5 & 68.1\\
% D3M \cite{abbasi2024one_arxiv2024}&Gen& arXiv/2024 & 35.9 & 58.6 & 70.5& - & 30.8 & 49.1 & 54.5& -& 11.4 & 38.8 & 51.4 & -& 5.0 &23.6 & 32.2 & -& -\\
IGD \cite{chen2025igd_iclr2025}&Gen& ICLR/2025 & - & - & 66.8& - & - & 45.8 & 53.9& 55.9& - & - & - & -& - &46.2 & 60.3 & -& -\\
% Abbasi et al. \cite{2024arXiv241204668A_arxiv2024}&Gen& arXiv/2024 & 38.2 & 64.6 & 73.9& - & 42.6 & 49.0/57.7(R) & 53.5/62.2(R)& -& - & - & -& - &13.9(R) & 52.1(R) & 54.9(R) & -\\
% LD3M \cite{moser2024latent_arxiv2024}&Gen+Latent& arXiv/2024 & 27.2(A) & - & -& - & - & - & -& -& - & - & -& - &- & - & - & -\\ 
\hline
% D3 \cite{qin2024distributional_arxiv2024}&Gen+Latent& arXiv/2024 & 27.2(A) & - & -& - & - & - & -& -& - & - & -& - &- & - & - & -\\ FL
SRe2L {\cite{yin2024squeeze_nips2024}}&SRe2L/R18& NeurIPS/2024 & 16.6 & 29.3 & 45.0& - & 6.6 & 27.0 & 50.2& -& 2.62&16.1 & 41.1 & 49.7& 0.4  & 21.3 & 46.8 & 52.8& 65.9\\
EDC \cite{shao2024elucidating_nips2024} &SRe2L/R18& NeurIPS/2024 & 32.6 & 79.1 & 87.0& - & 39.7 & 63.7 & 68.6& -&39.2 & 51.2 & 57.2& -& 12.8  & 48.6 & 58.0 & -& -\\
Xiao et al. \cite{xiao2024large_nips2024} &SRe2L/R18& NeurIPS/2024 & - & - & -& - & - & - & - & -&- & - & 48.8& 53.6& -  & 34.6 & 55.4 & 59.4 &62.6\\ 
RDED \cite{sun2024diversity_cvpr2024} &SRe2L/R18& CVPR/2024 & 22.9 & 37.1 & 62.1& - & 11.0 & 42.6 & 62.6& -&9.7 & 41.9 & 58.2& -& 6.6  & 42.0 & 56.5 & -& -\\
G-VBSM \cite{shao2024generalized_cvpr2024} &SRe2L/R18& CVPR/2024 & - & 53.5 & 59.2& - & 25.9 & 59.5 & 65.0& -& -&- & 47.6 & -& -  & 31.4 & 51.8 & 55.7& -\\ 
CDA \cite{yin2023dataset_arxiv2023} &SRe2L& TMLR/2024 & - & - & -& - & - & 49.8 & 64.4& -& -&21.3 & 48.7 & 53.2& 0.5  & 33.5 & 53.5 & 58.0& 63.3\\ %imagenet-21K
SC-DD \cite{zhou2024self_arxiv2024} &SRe2L/R18& arXiv/2024 & - & - & -& - & - & - & 53.4& -& -&31.6 & 45.9 & -& -  & 32.1 & 53.1 & 57.9& 63.5\\ 
% G-VBSM \cite{shao2024generalized_cvpr2024} &SRe2L/R18& CVPR/2024 & - & 53.5 & 59.2& - & 25.9 & 59.5 & 65.0& -& -&- & 47.6 & -& -  & 31.4 & 51.8 & 55.7& -\\ 
% EDC \cite{shao2024elucidating_nips2024} &SRe2L/R18& NeurIPS/2024 & 32.6 & 79.1 & 87.0& - & 39.7 & 63.7 & 68.6& -&39.2 & 51.2 & 57.2& -& 12.8  & 48.6 & 58.0 & -& -\\
Zhong et al. \cite{zhong2024efficientdatasetdistillationdiffusiondriven_arxiv2024} &SRe2L/R18& arXiv/2024 & - & 36.4 & 61.0& - & - & 41.5 & 63.8 & -&- & 40.2 & 58.5& -& -  & 42.1 & 59.4 & 61.8& -\\
GIFT \cite{shang2024gift_arxiv2024}&SRe2L/R18& arXiv/2024 & - & - & -& - & - & 49.5 & 57.0 & -&- & 42.9 &47.5& -& -  & 21.7 & 39.5 & 42.5& -\\
% Xiao et al. \cite{xiao2024large_nips2024} &SRe2L/R18& NeurIPS/2024 & - & - & -& - & - & - & - & -&- & - & 48.8& 53.6& -  & 34.6 & 55.4 & 59.4 &62.6\\ 
CV-DD \cite{cui2025datasetdistillationcommitteevoting_arxiv2025} &SRe2L/R18& arXiv/2025 & - & 64.1 & 74.0& - & 28.3 & 62.7 & 67.1& -&10.1 & 47.8 & 54.1& -& -  & 46.0 & 59.5 & -& -\\
% RDED \cite{sun2024diversity_cvpr2024} &SRe2L/R18& CVPR/2024 & 22.9 & 37.1 & 62.1& - & 11.0 & 42.6 & 62.6& -&9.7 & 41.9 & 58.2& -& 6.6  & 42.0 & 56.5 & -\\
FocusDD \cite{hu2025focusddrealworldsceneinfusion_arxiv2025} &SRe2L/R18& arXiv/2025 & - & - & -& - & - & - & - & -&- & - & -& -& 8.8  & 45.3 & 61.7 & 62.0& -\\
% Zhong et al. \cite{zhong2024efficientdatasetdistillationdiffusiondriven_arxiv2024} &SRe2L/R18& arXiv/2024 & - & 36.4 & 61.0& - & - & 41.5 & 63.8 & -&- & 40.2 & 58.5& -& -  & 42.1 & 59.4 & 61.8& -\\
% GIFT \cite{shang2024gift_arxiv2024}&SRe2L/R18& arXiv/2024 & - & - & -& - & - & 49.5 & 57.0 & -&- & 42.9 &47.5& -& -  & 21.7 & 39.5 & 42.5& -\\
% Xiao et al. \cite{xiao2024large_nips2024} &SRe2L/R18& NeurIPS/2024 & - & - & -& - & - & - & - & -&- & - & 48.8& 53.6& -  & 34.6 & 55.4 & 59.4 &62.6\\ 
\hline
% DATM \cite{guo2024towards_iclr2024} &Lossless& ICLR/2024 & 46.9 & 66.8 & 76.1& - & 27.9 & 47.2 & 55.0 & 57.5&17.1 & 31.1 & 39.7& -& -  & - & - & -\\ 
% % EDF \cite{wang2024emphasizing_arxiv2024} &Lossless& arXiv/2024 & - & - & -& - & - & - & - & -&- & - & -& -& -  & - & - & -\\ 
% % PAD \cite{li2024prioritize_arxiv2024} &Lossless& arXiv/2024 & 47.2 & 67.4 & 77.0& - & 28.4 & 47.8 & 55.9 & 58.5&17.7 & 32.3 & 41.6& -& -  & - & - & -\\ 
% \hline
% EDF \cite{wang2024emphasizing_arxiv2024} &Lossless& arXiv/2024 & - & - & -& - & - & - & - & -&- & - & -& -& -  & - & - & -\\ 
DREAM \cite{liu2023dream_iccv2023} &Selective& ICCV/2023 &  51.1& 69.4 & 74.8& - &29.5 & 46.8 & 52.6 & -&10.0 & 29.5 & -& -& -  & 18.5 & - & -& -\\ 
DREAM+ \cite{liu2023dreamplus_arxiv2023} &Selective& arXiv/2023 &  52.5& 69.9 & 75.3& - &29.7 & 47.4 & 52.6 & -&10.5 & 24.0 & 29.5& -& -  & 18.5 & - & -& -\\ 
APM \cite{chen2023dataset_arxiv2023} &Selective& arXiv/2023 &  -& - & 75.0& - &-& 44.6 & 53.3 & 55.2&- & 30.0 & 38.2& 39.6& -  & 24.8 & 30.7 & 32.6&-\\
RFAD \cite{tukan2023dataset_arxiv2023} &Selective& arXiv/2023 &  64.4& 74.3 & 77.0& - &38.5& 45.8 & - & -&- & - & -& -& -  & - & - & -& -\\
PAD \cite{li2024prioritize_arxiv2024} &Selective& arXiv/2024 & 47.2 & 67.4 & 77.0& - & 28.4 & 47.8 & 55.9 & 58.5&17.7 & 32.3 & 41.6& -& -  & - & - & -& -\\ 
SDC \cite{wang2024sdc_arxiv2024} &Selective& arXiv/2024 &  47.9& 65.3 & 71.8& - &28.0& 47.8 & 52.5 & -&17.4 & 30.7 & 39.9& -& -  & - & - & -& -\\ 
AutoPalette \cite{yuancolor_nips2024} &Selective& NeurIPS/2024 & 58.6 & 74.3 & 79.4& - & 38.0 & 52.6 & 53.3 & -&- & - & -& -& -  & - & - & -& -\\ 
IADD \cite{li2024importance_nn} &Selective& NN/2024 &  46.5& 66.7 & 72.6& - &25.2 & 42.7 & 49.0 & -&9.6 & 24.1 & -& -& -  & - & - & -&-\\ 
% DREAM \cite{liu2023dream_iccv2023} &Selective& ICCV/2023 &  51.1& 69.4 & 74.8& - &29.5 & 46.8 & 52.6 & -&10.0 & 29.5 & -& -& -  & 18.5 & - & -& -\\ 
% DREAM+ \cite{liu2023dreamplus_arxiv2023} &Selective& arXiv/2023 &  52.5& 69.9 & 75.3& - &29.7 & 47.4 & 52.6 & -&10.5 & 24.0 & 29.5& -& -  & 18.5 & - & -& -\\ 
% SDC \cite{wang2024sdc_arxiv2024} &Importance& arXiv/2024 &  52.5& 69.9 & 75.3& - &29.7 & 47.4 & 52.6 & -&10.5 & 24.0 & 29.5& -& -  & 18.5 & - & -\\ 
% SelMatch \cite{lee2024selmatch_icml2024} &Importance& ICML/2024 &  -& - & -& - &-& - & 54.5 & 62.4&- & - & 44.7& 50.4& -  & - & - & -&-\\
% APM \cite{chen2023dataset_arxiv2023} &Importance& arXiv/2023 &  -& - & 75.0& - &-& 44.6 & 53.3 & 55.2&- & 30.0 & 38.2& 39.6& -  & 24.8 & 30.7 & 32.6&-\\
% RFAD \cite{tukan2023dataset_arxiv2023} &Importance& arXiv/2023 &  64.4& 74.3 & 77.0& - &38.5& 45.8 & - & -&- & - & -& -& -  & - & - & -& -\\
% Moser et al. \cite{moser2024distill_arxiv2024} &Importance& arXiv/2024 &  -& - & -& - &-& - & - & -&- & - & -& -& -  & - & - & -\\
BiLP+IDC \cite{xu2023distill_eccv2024}&Selective& ECCV/2024 &  55.9& 69.8 & 76.9& - &34.0& 48.0 & - & -&- & - & -& -& -  & - & - & -& -\\ 
% SDC \cite{wang2024sdc_arxiv2024} &Selective& arXiv/2024 &  47.9& 65.3 & 71.8& - &28.0& 47.8 & 52.5 & -&17.4 & 30.7 & 39.9& -& -  & - & - & -& -\\ 
SelMatch \cite{lee2024selmatch_icml2024} &Selective& ICML/2024 &  -& - & -& - &-& - & 54.5 & 62.4&- & - & 44.7& 50.4& -  & - & - & -&-\\
% APM \cite{chen2023dataset_arxiv2023} &Selective& arXiv/2023 &  -& - & 75.0& - &-& 44.6 & 53.3 & 55.2&- & 30.0 & 38.2& 39.6& -  & 24.8 & 30.7 & 32.6&-\\
% RFAD \cite{tukan2023dataset_arxiv2023} &Selective& arXiv/2023 &  64.4& 74.3 & 77.0& - &38.5& 45.8 & - & -&- & - & -& -& -  & - & - & -& -\\
% PAD \cite{li2024prioritize_arxiv2024} &Selective& arXiv/2024 & 47.2 & 67.4 & 77.0& - & 28.4 & 47.8 & 55.9 & 58.5&17.7 & 32.3 & 41.6& -& -  & - & - & -& -\\ 
\hline
DATM \cite{guo2024towards_iclr2024} &Lossless& ICLR/2024 & 46.9 & 66.8 & 76.1& - & 27.9 & 47.2 & 55.0 & 57.5&17.1 & 31.1 & 39.7& -& -  & - & - & -& -\\ 
% EDF \cite{wang2024emphasizing_arxiv2024} &Lossless& arXiv/2024 & - & - & -& - & - & - & - & -&- & - & -& -& -  & - & - & -\\ 
% PAD \cite{li2024prioritize_arxiv2024} &Lossless& arXiv/2024 & 47.2 & 67.4 & 77.0& - & 28.4 & 47.8 & 55.9 & 58.5&17.7 & 32.3 & 41.6& -& -  & - & - & -\\ 
\hline
CUDD \cite{ma2024curriculum_arxiv2024} &Diversity/R18& arXiv/2024 & -& 56.2& 84.5 &  - &-& 60.3 & 65.7 & -&- & - & 55.6& 56.8& -  & 39.0 & 57.4 & 61.3 &65.0\\
% INFER \cite{zhang2024breaking_arxiv2024} &Diversity/R18& ICLR/2025 &  -& 33.5 & 59.2& - &-& 50.2 & 65.1 & 68.6&- & 38.6 & 55.0& -& -  & 37.0 & 54.3 & -& -\\
DELT  \cite{shen2024delt_arxiv2024}  &Diversity/R18& arXiv/2024 &  24.0& 43.0 & 64.9& - &-& - & - & -&9.3 & 43.0 & 55.7& -& -  & 45.8 & 59.2 & 62.4&-\\
DWA \cite{Du2024DiversityDrivenSE_nips2024} &Diversity/R18& NeurIPS/2024 &  -& 32.6 & 53.1& - &-& 39.6 & 60.9 & -&- & - & 52.8& 56.0& -  & 37.9 & 55.2 & 59.2 & -\\ 
INFER \cite{zhang2024breaking_arxiv2024} &Diversity/R18& ICLR/2025 &  -& 33.5 & 59.2& - &-& 50.2 & 65.1 & 68.6&- & 38.6 & 55.0& -& -  & 37.0 & 54.3 & -& -\\
\hline
% DSDM \cite{li2024diversified_mm2024}&Diversity/R18&-& - & -& - &-& - & - & -&- & - & -& -& -  & - & - & -\\
Zhang et al. \cite{zhang2023accelerating_cvpr2023} &Augmentation &CVPR/2023 &  49.2& 67.1 & 73.8& - &29.8& 45.6 & 52.6 & -&- & - & -& -& -  & - & - & - & -\\
LADD \cite{Kang2024LabelAugmentedDD_arxiv2024}&Augmentation/R18 &arXiv/2023 &  -& - & -& - &-& - & - & -&- & - & -& -& -  & 28.8 & - & - & -\\

% M-DATM \cite{zhou2024enhancing_arxiv2024} &Lossless& arXiv/2024 & -& - & -& - & - & - & - & -&- & 40.6 & & -& -  & - & - & -\\ %inconsistent results
% Qin et al. \cite{qin2024label_arxiv2024} &SRe2L/R18& arXiv/2024 & - & 36.4 & 61.0& - & - & 41.5 & 63.8 & -&- & 40.2 & 58.5& -& -  & - & 59.4 & -\\
% GIFT \cite{shang2024gift_arxiv2024} &SRe2L/R18& arXiv/2024 & - & 36.4 & 61.0& - & - & 41.5 & 63.8 & -&- & 40.2 & 58.5& -& -  & - & 59.4 & -\\ %plug and play
        % DC \cite{zhao2020dataset_iclr2021} & GM & 94.7 & 98.8 & 76.1 & 82.3 & 44.9 & 53.9 & 32.3 & 42.8 & - \\
        % DSA \cite{zhao2021dataset_icmlr2021} & GM & 97.8 & \textcolor{orange}{99.2} & 79.2 & 84.4 & 52.1 & 60.5 & 39.8 & 48.4 & - \\
        % DM \cite{zhao2023dataset_wacv2023} & DM & 97.3 & 98.8 & 79.6 & 82.4 & 48.3 & 56.6 & 29.7 & 43.2 & - \\
        % CAFE \cite{wang2022cafe_cvpr2022} & DM & 97.5 & 98.8 & 77.9 & 82.3 & 50.6 & 58.9 & 31.7 & 42.9 & - \\
        % KIP [37, 38] & KRR & 97.5 & 98.5 & - & - & 50.3 & 56.2 & - & - & - \\
        % FTD [13] & TM & 97.7 & 98.6 & 79.4 & 83.0 & 50.7 & 57.8 & 43.4 & 47.8 & 24.5 \\
        % MTT \cite{cazenavette2022dataset_cvpr2022} & TM & 97.3 & 98.5 & 79.7 & 87.7 & 43.9 & 47.8 & 40.3 & 47.0 & 23.2 \\
        % SeqMatch-MTT \cite{du2024sequential_nips2024}& TM & \textcolor{orange}{97.6} & \textcolor{orange}{99.0} & \textcolor{orange}{80.2} & \textcolor{orange}{88.5} & \textcolor{orange}{58.0} & \textcolor{orange}{65.3} & 43.6 & \textcolor{orange}{51.2} & \textcolor{orange}{23.8} \\
        % IDC* [23] & GM & 97.8 & 98.9 & 78.5 & 82.8 & 66.7 & 67.5 & 46.1 & 47.4 & - \\
        % SeqMatch-IDC\cite{du2024sequential_nips2024} & GM & \textcolor{blue}{99.2} & \textcolor{blue}{99.4} & \textcolor{blue}{79.9} & \textcolor{blue}{83.7} & \textcolor{blue}{75.2} & \textcolor{blue}{77.8} & \textcolor{blue}{51.9} & \textcolor{blue}{54.0} & - \\

        % PAD/ConvNet\cite{li2024prioritize_arxiv2024} & TM & {--} & {--} & {--} & {--} & {--} & {--}& 47.2&{67.4} & {77.0} & 28.4&{47.8} & {55.9} & 17.7&32.3 &41.6\\
        % EDC/ResNet-18\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--}& {--} & {--} & {--} & {32.6} & {79.1} & {87.0} & 39.7& {63.7} & {68.6} &39.2 & 51.2 & 56.5 \\       EDC/ResNet-50\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--}& {--} & {--} & {--} & {30.6} & {76.0} & {86.9} &36.1 &  {62.1} & {69.4} & 35.9 & 50.2 & 57.3 \\       EDC/ResNet-101\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--} & {--} & {--} & {--} & 26.1 & {67.1} & {85.8} & 32.3 &{61.7} & {68.5} &40.6 & 51.6 & 57.4 \\       EDC/MobileNet-V2\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--} & {--}& {--} & {--}  & 20.2 &{42.0} & {70.8} & 10.6 & {44.3} & {59.5} & 18.8 &  40.6 & 50.7 \\      
        % % RTP* [11] & META & \textcolor{blue}{99.3} & \textcolor{blue}{99.4} & - & - & \textcolor{blue}{73.0} & 74.2 & 49.9 & 51.9 & - \\
        % % HaBa* [33] & TM & 97.8 & 98.9 & 80.4 & 83.0 & 64.0 & 66.9 & 49.6 & 50.9 & - \\
        % \hline
        % % Whole & - & 99.6 & - & 95.4 & - & 84.8 & - & 56.2 & - & 37.6 \\
        \hline
    \end{tabular}
\end{table*}

\begin{table}
\centering
\caption{Performance comparison of dataset distillation methods on ImageNet-21K under different IPCs.}
  \tiny
\begin{tabular}{|c|c|c|cccc|}
\hline
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Schemes}} & \multirow{2}{*}{\textbf{Venue}} & \multicolumn{4}{c|}{\textbf{ImageNet-21K}} \\
\cline{4-7}
& & & 1 & 10 & 20 & 50 \\
\hline
SRe2L {\cite{yin2024squeeze_nips2024}}&SRe2L/R18& NeurIPS/2024 & - & 18.5& 21.8 & -\\
EDC \cite{shao2024elucidating_nips2024} &SRe2L/R18& NeurIPS/2024&- & 26.8& - & - \\
CDA \cite{yin2023dataset_arxiv2023} &SRe2L& TMLR/2024 & - & 22.6& 26.4 & - \\
RDED \cite{sun2024diversity_cvpr2024} &SRe2L/R18& CVPR/2024&- & 25.6& - & - \\
% EDC \cite{shao2024elucidating_nips2024} &SRe2L/R18& NeurIPS/2024&- & 26.8& - & - \\
CUDD \cite{ma2024curriculum_arxiv2024} &Diversity/R18& arXiv/2024&- & 28.0& 34.9 & -\\
% Method B & GM & CVPR/2023 & 48.8 & 67.1 & 74.6 & 76.2 \\
% Method C & TM & ECCV/2024 & 46.4 & 65.5 & 71.9 & 74.5 \\
% Method D & DM & ICCV/2023 & 45.3 & 63.5 & 69.9 & 72.8 \\
\hline
\end{tabular}
\label{tab:single-dataset-comparison}
\end{table}
% \vspace{-0.5cm}
% \begin{table*}[ht]
%     \centering
%     \caption{Performance comparison of dataset distillation methods across a variety of datasets. Abbreviations of GM, TM, DM, META stand for gradient matching, trajectory matching, distribution matching, and meta-learning respectively. We reproduce the results of MTT [5] and IDC [23] and cite the results of the other baselines [30]. The best results of non-factorized methods (without decoders) are highlighted in orange font. The best results of factorization-based methods are highlighted in blue font.}
%     % \begin{tabular}{|l|l|cc|cc|cc|cc|c|}
%         \begin{tabular}{|l|l|ccc|ccc|ccc|ccc|ccc|}
%         \hline
%         % \multirow{2}{*}{Methods} & \multirow{2}{*}{Schemes} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{SVHN} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c|}{CIFAR-100} & \multirow{2}{*}{Tiny ImageNet} \\
%      \multirow{2}{*}{Methods} & \multirow{2}{*}{Schemes} & \multicolumn{3}{c|}{MNIST} & \multicolumn{3}{c|}{SVHN} & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{Tiny ImageNet}  \\
%         & & 1& 10 & 50 & 1& 10 & 50 & 1& 10 & 50 & 1& 10 & 50 & 1& 10 &50 \\
%         \hline
%         DD \cite{wang2018dataset_arxiv2018} & META & 79.5 & - & 36.8 & - & 44.9 & - & 42.8 & - & - \\
%         DC \cite{zhao2020dataset_iclr2021} & GM & 94.7 & 98.8 & 76.1 & 82.3 & 44.9 & 53.9 & 32.3 & 42.8 & - \\
%         DSA \cite{zhao2021dataset_icmlr2021} & GM & 97.8 & \textcolor{orange}{99.2} & 79.2 & 84.4 & 52.1 & 60.5 & 39.8 & 48.4 & - \\
%         DM \cite{zhao2023dataset_wacv2023} & DM & 97.3 & 98.8 & 79.6 & 82.4 & 48.3 & 56.6 & 29.7 & 43.2 & - \\
%         CAFE \cite{wang2022cafe_cvpr2022} & DM & 97.5 & 98.8 & 77.9 & 82.3 & 50.6 & 58.9 & 31.7 & 42.9 & - \\
%         KIP [37, 38] & KRR & 97.5 & 98.5 & - & - & 50.3 & 56.2 & - & - & - \\
%         FTD [13] & TM & 97.7 & 98.6 & 79.4 & 83.0 & 50.7 & 57.8 & 43.4 & 47.8 & 24.5 \\
%         MTT \cite{cazenavette2022dataset_cvpr2022} & TM & 97.3 & 98.5 & 79.7 & 87.7 & 43.9 & 47.8 & 40.3 & 47.0 & 23.2 \\
%         SeqMatch-MTT \cite{du2024sequential_nips2024}& TM & \textcolor{orange}{97.6} & \textcolor{orange}{99.0} & \textcolor{orange}{80.2} & \textcolor{orange}{88.5} & \textcolor{orange}{58.0} & \textcolor{orange}{65.3} & 43.6 & \textcolor{orange}{51.2} & \textcolor{orange}{23.8} \\
%         IDC* [23] & GM & 97.8 & 98.9 & 78.5 & 82.8 & 66.7 & 67.5 & 46.1 & 47.4 & - \\
%         SeqMatch-IDC\cite{du2024sequential_nips2024} & GM & \textcolor{blue}{99.2} & \textcolor{blue}{99.4} & \textcolor{blue}{79.9} & \textcolor{blue}{83.7} & \textcolor{blue}{75.2} & \textcolor{blue}{77.8} & \textcolor{blue}{51.9} & \textcolor{blue}{54.0} & - \\

%         PAD/ConvNet\cite{li2024prioritize_arxiv2024} & TM & {--} & {--} & {--} & {--} & {--} & {--}& 47.2&{67.4} & {77.0} & 28.4&{47.8} & {55.9} & 17.7&32.3 &41.6\\
%         EDC/ResNet-18\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--}& {--} & {--} & {--} & {32.6} & {79.1} & {87.0} & 39.7& {63.7} & {68.6} &39.2 & 51.2 & 56.5 \\       EDC/ResNet-50\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--}& {--} & {--} & {--} & {30.6} & {76.0} & {86.9} &36.1 &  {62.1} & {69.4} & 35.9 & 50.2 & 57.3 \\       EDC/ResNet-101\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--} & {--} & {--} & {--} & 26.1 & {67.1} & {85.8} & 32.3 &{61.7} & {68.5} &40.6 & 51.6 & 57.4 \\       EDC/MobileNet-V2\cite{shao2024elucidating_nips2024} & ** & {--} & {--} & {--} & {--}& {--} & {--}  & 20.2 &{42.0} & {70.8} & 10.6 & {44.3} & {59.5} & 18.8 &  40.6 & 50.7 \\      
%         % RTP* [11] & META & \textcolor{blue}{99.3} & \textcolor{blue}{99.4} & - & - & \textcolor{blue}{73.0} & 74.2 & 49.9 & 51.9 & - \\
%         % HaBa* [33] & TM & 97.8 & 98.9 & 80.4 & 83.0 & 64.0 & 66.9 & 49.6 & 50.9 & - \\
%         \hline
%         % Whole & - & 99.6 & - & 95.4 & - & 84.8 & - & 56.2 & - & 37.6 \\
%         \hline
%     \end{tabular}
% \end{table*}

\subsection{Performance and Scalability Across Techniques}
Different methodological approaches exhibit varying levels of success depending on dataset scale and IPC settings.
Early methods primarily focused on matching-based approaches, including gradient matching (GM) and trajectory matching (TM). 
While these methods demonstrated strong performance on smaller datasets (e.g., DC \cite{zhao2020dataset_iclr2021} and DSA \cite{zhao2021dataset_icmlr2021} achieving over 50\% accuracy on CIFAR-10), their effectiveness diminishes on larger-scale datasets like ImageNet-1K and ImageNet-21K. 
This scalability limitation stems from their difficulty in handling the increased complexity and high dimensionality of larger datasets.

Recent advances have shown that methods integrating SRe2L frameworks with diversity-enhancing strategies achieve superior performance on large-scale datasets. 
For instance, on ImageNet-1K, CUDD \cite{ma2024curriculum_arxiv2024} achieves 65.0\% accuracy at IPC=200, while D4M \cite{su2024d_cvpr2024} reaches 68.1\% under the same setting. 
These methods effectively address the complexity of larger datasets through their de-coupled optimization mechanisms and robust regularization strategies. 
Furthermore, their success extends to extremely large-scale scenarios, with methods like CUDD achieving 34.9\% accuracy on ImageNet-21K at IPC=20, demonstrating unprecedented scalability.

Emerging paradigms in dataset distillation include latent and frequency-based methods, such as FreD \cite{shin2024frequency_nips2023} and NSD \cite{yang2024neural_eccv2024}. 
These approaches show promising results on smaller datasets, with FreD achieving 60.6\% and NSD reaching 68.5\% accuracy on CIFAR-10 at IPC=1, significantly outperforming traditional matching-based methods. 
On CIFAR-100, they maintain strong performance with FreD achieving 34.6\% and NSD reaching 36.5\% at IPC=1. 
However, their scalability to larger datasets remains largely unexplored, as comprehensive results on ImageNet-1K and ImageNet-21K are yet to be reported.

Recent methods have also explored importance-based sampling and diversity-driven approaches. 
Notable examples include AutoPalette \cite{yuancolor_nips2024}, which achieves 58.6\% accuracy on CIFAR-10 at IPC=1, and EDC \cite{shao2024elucidating_nips2024}, which demonstrates strong performance across different scales, reaching 87.0\% on CIFAR-10 at IPC=50. 
These innovations suggest that combining multiple strategies, such as diversity enhancement, importance sampling, and efficient optimization, may be key for both scalability and performance.
% \vspace{-0.5cm}
In summary, while traditional matching-based methods remain effective for smaller datasets, the field has evolved towards more sophisticated approaches that combine de-coupling mechanisms like SRe2L, diversity-enhancing strategies, and novel paradigms such as latent and frequency-based methods. 
To advance the scalability and robustness of dataset distillation, future research must continue to address the challenges of large-scale datasets, particularly focusing on methods that can maintain high performance while scaling to datasets like ImageNet-21K.


\section{Challenges and Future Directions}

Despite remarkable advancements in dataset distillation,  critical challenges remain, particularly as the field shifts toward larger datasets and more complex applications. 

\subsection{Challenges}

\noindent \textbf{Scalability to Large-Scale Datasets}  
While recent methods have demonstrated significant progress on ImageNet-1K, with approaches like D4M \cite{su2024d_cvpr2024} achieving 68.1\% accuracy at IPC=200 and CUDD \cite{ma2024curriculum_arxiv2024} reaching 65.0\%, scaling to larger datasets like ImageNet-21K remains challenging. 
On ImageNet-21K, even state-of-the-art methods like CUDD achieve only 34.9\% accuracy at IPC=20, highlighting the substantial performance gap that exists when scaling to ultra-large datasets. 
These challenges stem from increased class counts, intra-class variability, and computational demands of large-scale datasets.

\noindent \textbf{Balancing Compression and Performance}  
The relationship between IPC and performance exhibits distinct patterns across different scales. While significant gains are observed between IPC=1 and IPC=10 (e.g., DATM \cite{guo2024towards_iclr2024} improves from 46.9\% to 66.8\% on CIFAR-10), performance improvements diminish at higher IPC values. 
For instance, on CIFAR-100, PAD \cite{li2024prioritize_arxiv2024} shows only marginal gains from IPC=50 (55.9\%) to IPC=100 (58.5\%). 
This non-linear relationship presents a critical trade-off between dataset compression and model performance, particularly crucial for practical applications.

\noindent \textbf{Limited Cross-Architecture Generalization}  
Current methods, particularly those based on SRe2L frameworks, often show architecture-specific performance patterns. 
While some methods demonstrate strong performance, their effectiveness across different architectures, especially emerging ones like Vision Transformers, remains less explored. 
The challenge of creating synthetic datasets that generalize well across diverse architectures persists.

\noindent \textbf{Underexplored Domains and Modalities}  
The majority of dataset distillation research has focused on vision tasks, with limited exploration in audio and multimodal domains. 
Emerging methods for temporal and multimodal data distillation demonstrate potential but require further development to match the maturity of image-based techniques.

\noindent \textbf{Fair Evaluation of Dataset Distillation Methods}
Recent DD methods are shifting from hard labels to soft labels  to enhance model accuracy \cite{yin2024squeeze_nips2024}. 
However, soft labels inherently transfer additional knowledge from a teacher model, making it difficult to isolate the true informativeness of distilled data \cite{li2024ddranking, qin2024label_arxiv2024}. 
Additionally, the use of diverse data augmentation techniques during evaluation further skews results, making it unclear whether performance gains stem from the quality of the distilled data or external enhancements \cite{li2024ddranking}.
Furthermore, different dataset distillation methods employ varied evaluation strategies, such as distinct loss functions, leading to inconsistencies in performance comparison. 


\subsection{Future Directions}

\noindent \textbf{Advanced Scaling Techniques}  
Future research should focus on designing algorithms capable of scaling to ultra-large datasets without sacrificing performance. Techniques like hybrid approaches that combine distillation with coreset selection or data pruning could be explored to tackle computational bottlenecks.

\noindent \textbf{Adaptive IPC Optimization}  
Developing adaptive IPC optimization strategies that dynamically adjust IPC settings based on dataset complexity and downstream tasks could provide a practical solution to the compression-performance trade-off. 
Meta-learning and reinforcement learning frameworks offer promising avenues for automated IPC tuning.

\noindent \textbf{Theoretical Understanding} While dataset distillation methods have demonstrated impressive empirical success, their theoretical foundations remain relatively unexplored. A rigorous theoretical framework is needed to understand the fundamental limits of distillation, including convergence guarantees, optimal compression ratios, and generalization bounds. Such theoretical insights could provide principled guidance for designing more effective distillation algorithms and help understand the relationship between distillation size, model architecture, and performance guarantees.

\noindent \textbf{Architecture-Agnostic Distillation}  
Developing techniques that effectively transfer knowledge across different model architectures remains a critical challenge.
Techniques that disentangle architecture-invariant features from architecture-specific biases could be further explored to enable broad applicability across architectures.



\noindent \textbf{Domain Expansion}  
Dataset distillation’s applicability should be extended to emerging domains, such as medical imaging, scientific analysis, and 3D point cloud data. 
Tailored frameworks that account for domain-specific characteristics, such as temporal dynamics in video or frequency-space representations in audio, are critical for broadening the scope of distillation techniques.

\noindent \textbf{Enhancing Fair Evaluation and Comparisons}
In this survey, we present two comparative tables summarizing key aspects of various dataset distillation methods, including their use of soft labels, architectures, and other relevant factors, providing researchers with a reference to determine the most suitable approach for their needs. 
Additionally, Li et al. introduced DD-Ranking \cite{li2024ddranking}, a benchmark designed to decouple the effects of knowledge distillation and data augmentation, ensuring a fair evaluation framework to assess the true informativeness of distilled data. 
By combining structured comparisons with insights from existing benchmarks such as DD-Ranking \cite{li2024ddranking}, we aim to help researchers navigate the landscape of dataset distillation methods while promoting more standardized evaluation practices.


\section{Conclusion} \label{sec:conclusion}
This survey provides a comprehensive overview of the rapid advancements in dataset distillation from 2023 to 2025, with a particular focus on scaling to large-scale datasets and emerging methodological innovations. 
The field has witnessed significant progress across multiple dimensions, from achieving unprecedented performance on ImageNet-scale datasets to expanding into new domains like video, audio, and multi-modal processing.

Several key trends have emerged during this period. 
First, the development of more sophisticated optimization strategies, such as SRe2L frameworks and diversity-driven approaches, has enabled effective distillation of large-scale datasets like ImageNet-1K and ImageNet-21K. 
Second, the introduction of soft labels and decoupling mechanisms has significantly improved both efficiency and performance. 
Third, the emergence of generative models, particularly diffusion-based approaches, has opened new possibilities for high-quality synthetic data generation.
Despite these advances, important challenges remain. 
The scalability to ultra-large datasets, the trade-off between compression and performance, and cross-architecture generalization continue to be active areas of research. 
Additionally, the extension to multi-modal and temporal domains presents both opportunities and challenges that require innovative solutions.
We hope that this comprehensive survey of recent dataset distillation advances will serve as a valuable resource for researchers and practitioners, providing insights into the latest progress, existing challenges, and promising future directions in this rapidly evolving field.
\vspace{-0.5cm}
\footnotesize
\section*{Acknowledgment}
The authors would like to express their gratitude to the contributors of the \textit{Awesome-Dataset-Distillation} repository\footnote{\url{https://github.com/Guang000/Awesome-Dataset-Distillation}}, which has served as a valuable resource in compiling recent advancements.
\vspace{-0.5cm}
% Additionally, we acknowledge the broader dataset distillation research community for their continuous efforts in advancing this field.




%  Unsupervised Generative Fake Image Detector



% DF-RAP: A Robust Adversarial Perturbation for Defending against Deepfakes in Real-world Social Network Scenarios

% Real, fake and synthetic faces - does the coin have three sides?


% Learning Pairwise Interaction for Generalizable DeepFake Detection


% Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View


% Semi-supervised Deep Domain Adaptation for Deepfake Detection


% Contrastive Knowledge Transfer for Deepfake Detection with Limited Data

% May-27-2024
% A3: Ambiguous Aberrations Captured via Astray-Learning for Facial Forgery Semantic Sublimation



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% % use section* for acknowleDGM4ent
% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{AcknowleDGM4ents}
% \else
%   % regular IEEE prefers the singular form
%   \section*{AcknowleDGM4ent}
% \fi


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

\footnotesize
\bibliographystyle{IEEEtran}
\bibliography{mainref.bib}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


