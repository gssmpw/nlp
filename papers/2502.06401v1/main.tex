\PassOptionsToPackage{dvipsnames,table}{xcolor}
\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[pagenumbers]{cvpr}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\usepackage{indentfirst}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{float}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{float}
\usepackage{amsthm}
\usepackage{xspace}

\definecolor{habicolor}{hsb}{0.66, 0.1, 0.95}

\parskip 6pt

\def\paperID{*****} %
\def\confName{CVPR}
\def\confYear{2025}

\title{Habitizing Diffusion Planning for Efficient and Effective Decision Making}

\makeatletter
\renewcommand{\@makefnmark}{}
\makeatother

\author{
    Haofei Lu$^1$ \quad 
    Yifei Shen$^2$ \quad 
    Dongsheng Li$^2$ \quad 
    Junliang Xing$^1$\footnotemark[2] \quad 
    Dongqi Han$^2$\footnotemark[2] \vspace{.5em}\\
    $^1$Tsinghua University \quad $^2$Microsoft Research Asia \vspace{.5em} \\
    Project: \url{https://bayesbrain.github.io/}
}

\begin{document}
\maketitle


\footnotetext[2]{This work was done during the internship of Haofei Lu (luhf23@mails.tsinghua.edu.cn) at Microsoft Research Asia. Correspondence to: Dongqi Han $<$dongqihan@microsoft.com$>$, Junliang Xing$<$jlxing@tsinghua.edu.cn$>$.}

\begin{abstract}
Diffusion models have shown great promise in decision-making, also known as diffusion planning. However, the slow inference speeds limit their potential for broader real-world applications. 
Here, we introduce \textbf{Habi}, a general framework that transforms powerful but slow diffusion planning models into fast decision-making models, which mimics the cognitive process in the brain that costly goal-directed behavior gradually transitions to efficient habitual behavior with repetitive practice.   
Even using a laptop CPU, the habitized model can achieve an average \textbf{800+~Hz} decision-making frequency (faster than previous diffusion planners by orders of magnitude) on standard offline reinforcement learning benchmarks D4RL, while maintaining comparable or even higher performance compared to its corresponding diffusion planner. Our work proposes a fresh perspective of leveraging powerful diffusion models for real-world decision-making tasks. 
We also provide robust evaluations and analysis, offering insights from both biological and engineering perspectives for efficient and effective decision-making.
\end{abstract}

\section{Introduction}
The trade-off between computational cost and effectiveness is a key problem in decision making \citep{kahneman2011thinking, sidarus2019cost, clark2021computational}. In machine learning, probabilistic generative models have increasingly been used for planning of the outcome of actions \citep{ha2018recurrent, hafner2019dream, hafner2023mastering}. Recently, diffusion models have been used for decision making \citep{janner2021offline, ajay2022conditional, du2024learning}, in particular offline reinforcement learning (RL) \citep{levine2020offline, fu2020d4rl} by exploiting the generative power of diffusion models for making plans (trajectory of states). While recent diffusion model-based decision makers achieve state-of-the-art performance on standard offline RL benchmarks \citep{wang2023diffusion, lu2025what}, the computational cost of diffusion models remains a significant challenge -- models like diffuser and its variants usually take more than 0.1 seconds, sometimes even more than 1 second, to make a simple decision using a decent GPU \citep{janner2021offline, liang2023adaptdiffuser, lu2025what}, which is unacceptable for real-world applications. 

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{figures/pvf_cpu_avg.pdf}
\vspace{-3mm}
\caption{\textbf{Performance vs. Frequency.} Performance is normalized across MuJoCo, AntMaze, and Kitchen tasks from D4RL. Decision frequency (Hz) is measured on a laptop \textbf{CPU} (Apple M2, MacBook). Habitual Inference (HI), a lightweight model generated by our Habi, achieves an optimal balance between performance and speed. See \cref{tab: main-table} for more results.}
\vspace{-3mm}
\label{fig:pvf-cpu-avg}
\end{figure}

Meanwhile, it has been widely known and researched  that humans and animals can make optimal decisions using limited energy. In cognitive science and psychology, decision making is often considered driven together by two kinds of behaviors \cite{dolan2013goals, wood2016psychology}: a slow, deliberate \textbf{goal-directed} one, and a fast, automatic \textbf{habitual} one. Goal-directed behavior, focuses on careful planning and precise evaluation of the future outcomes (the process also known as System 2 thinking \cite{kahneman2011thinking}), making it more reliable but time-consuming. In contrast, the habitual behavior  (System 1 thinking \cite{kahneman2011thinking}) selects actions in a model-free manner -- without considering subsequent outcomes, thus computationally-efficient while could be less reliable.

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{figures/minesweeper.png}
\caption{\textbf{An illustrative example of the process of habitization in playing the Minesweeper game.} With practice, one's decision-making relies less on deliberate goal-directed planning and more on context-dependent habitual behavior.}
\label{fig:minesweeper}
\vspace{-5mm}
\end{figure}

In this study, we are inspired by one of the findings on habits and goals, known as \textit{habit formation}, or \textbf{\textit{habitization}} \citep{wood2016psychology, han2024synergizing}. That is, \textit{the brain will gradually transform slow, deliberate goal-directed behavior into fast, habitual behavior when repetitively doing a task} (see Figure~\ref{fig:minesweeper} for an illustrative example).  

The key reason behind habitization is the hard need for a trade-off between efficiency (decision time and energy) and effectiveness (behavior performance) for animals to survive. A recent study \citep{han2024synergizing} proposes a computational framework to model the interaction between the two behaviors: As many real-world tasks are few-shot or zero-shot, animals need to utilize their world model to perform goal-directed planning to make reliable decisions \citep{lee2014neural}, which is computationally costly. To improve efficiency, they must meanwhile ``extract'' the goal-directed decision strategy to a habitual decision model that straightforwardly makes decisions without planning by world models. 

We notice that diffusion planning \citep{janner2022planning, ajay2022conditional} is akin to goal-directed behavior, as both generate plans in the future before making a decision, and both are powerful but slow, hampering their real-world usage. 
Then, an idea naturally arise: Can we develop a habitization process like in the brain to transform slow, effective diffusion planning into fast, straight-forward habitual behavior? 

In this work, we provide a positive answer to this question. We develop a general framework, referred to as \textbf{Habi}, that mimics the brainâ€™s habitization process. Inspired by \citet{han2024synergizing}, Habi leverages variational Bayesian principles \citep{kingma2013auto} to connect the slow, yet powerful diffusion planner's policy (as posterior) with a fast, habitual policy model (as prior). By maximizing the evidence lower bound \citep{kingma2013auto}, the habitual policy model is trained to take advantage of pretrained diffusion planners while maintaining high efficiency, mimicking the habitization process in the brain \citep{wood2016psychology}.

Habi is featured with the following advantages:

\textbf{Efficiency}: The habitized policy model is lightweighted and super-fast, providing orders-of-magnitude speedup over existing diffusion planners (Figure~\ref{fig:pvf-cpu-avg}).\\
\textbf{Effectiveness}: The habitized policy can compete with the state-of-the-art model that are much slower (Figure~\ref{fig:pvf-cpu-avg}).\\
\textbf{Versatility}: Habi can be used straightforwardly for any diffusion planning and diffusion policy models.

We further conduct comprehensive evaluations across various tasks, offering empirical insights into efficient and effective decision making.

\section{Background}
\label{chap:background}

\subsection{Offline Reinforcement Learning.}
Offline reinforcement learning (RL) \citep{fujimoto2019off, levine2020offline} considers the case that an agent learns from a fixed dataset of previously collected trajectories without interacting with the environment. The objective is to train a policy that maximizes the expected return, defined as $\mathbb{E} \left[\sum_{h=0}^{\text{end}} \gamma^h r_{t+h} \right]$, when deployed in the environment. Here, $r_t$ is the immediate reward and $\gamma$ is the discount factor~\cite{sutton1998reinforcement}. Offline RL is particularly relevant in scenarios where exploration is costly, risky, or impractical, requiring agents to maximize the utility of existing data for careful planning. Key challenges include handling high-dimensional~\cite{levine2020offline}, long-horizon dependencies and deriving near-optimal policies from potentially sub-optimal datasets~\cite{fujimoto2019off}. These factors position offline RL as an ideal benchmark for evaluating advanced decision-making methods. In this work, we evaluate our framework using a standard offline RL benchmark D4RL~\citep{fu2020d4rl}, providing a rigorous and consistent comparison of its decision-making capabilities against previous baselines.

\subsection{Diffusion Models for Decision Making.}

Diffusion models have recently demonstrated remarkable potential in decision-making tasks due to their ability to model complex distributions. Compared to classical diagonal Gaussian policies~\citep{haarnoja2019soft, schulman2017proximal, kostrikov2021offline, kumar2020conservative}, diffusion models have achieved state-of-the-art performance in both online ~\citep{wang2024diffusion,yang2023policy,psenka2023learning,ren2024diffusion} and offline reinforcement learning~\citep{chen2023offline, li2023hierarchical, wang2023diffusion, hansen2023idql, janner2022planning}, as well as demonstration learning~\citep{ze20243d, chi2023diffusion, ze2024generalizable}. There are two main ways diffusion models are applied in decision-making: 

(1) \textit{Diffusion planner}~\cite{ajay2022conditional, janner2022planning, liang2023adaptdiffuser} models a trajectory $\tau$ consisting of the current and subsequent $H$ steps of states (or state-action pairs) in a horizon:
\begin{align*}
\tau = \begin{bmatrix} s_{t} , s_{t+1} , \cdots , s_{t+H-1}
\end{bmatrix} 
\text{ or }
\begin{bmatrix} s_{t} , s_{t+1} , \cdots , s_{t+H-1}   \\ 
a_{t} , a_{t+1} , \cdots , a_{t+H-1}
\end{bmatrix}.
\end{align*}
(2) \textit{Diffusion policy} ~\cite{hansen2023idql, wang2023diffusion} leverages diffusion models for direct action distribution $p(a_t|s_t)$ modeling, and can be viewed as modeling trajectory $\tau = [s_t \; a_t]$ with planning horizon $H=1$. 

Recent research has expanded the applications of diffusion planning to broader domains, such as vision-based decision-making~\cite{chi2023diffusion} and integration with 3D visual representations~\cite{ze2024generalizable, ze20243d}. A recent study~\cite{lu2025what} provides comprehensive analysis of key design choices, offering practical tips and suggestions for effective diffusion planning. Our work, Habi, is orthogonal to  diffusion planners or diffusion policies, and can be viewed as an adaptive, general framework to habitize diffusion planning into efficient, habitual behaviors.

\begin{figure*}[h]
\centering
\includegraphics[width=0.85\linewidth]{figures/pipelinev7.pdf}
\caption{\textbf{The diagram of Habi.} \textbf{(a) During the Habitization (Training) stage}, Habi learns to reconstruct actions from plans generated by a diffusion planner, with the decision spaces of habits (prior) and planning (posterior) aligned via KL divergence in the latent space. Trainable parts include Prior Encoder, Posterior Encoder, Decoder, and Critic. \textbf{(b) During the Habitual Inference (HI) stage}, only the lightweight prior encoder and latent decoder are required, enabling fast, high-quality habitual behaviors for decision-making.}
\label{fig:training}
\end{figure*}

\subsection{Auto-Encoding Variational Bayes}

Variational Bayesian (VB) approaches in deep learning have been popular since the introduction of the variational auto-encoder (VAE) \cite{kingma2013auto, sohn2015learning}. The core idea is to maximize the evidence lower bound (ELBO) of an objective function of a probabilistic variable $x$ so that we can replace the original distribution with a variational approximation based on a latent variable $z$ \cite{alemi2017deep}. The ELBO can be written as:
\begin{align}
    \mathrm{ELBO} = \mathbb{E}_{z\sim q(z)} \log P(x(z)) - D_{\mathrm{KL}}\left[q(z)||p(z)\right], \label{eq:elbo}
\end{align}
where $p,q$ indicates the prior and posterior distributions of $z$, respectively. $\log P(x(z))$ is the log-likelihood of correctly reconstructing data $x$ from $z$, and $D_\mathrm{KL}$ denotes Kullbackâ€“Leibler (KL) divergence \cite{kullback1951information}. 

An important property of ELBO is that although the log-likelihood term in Equation~\ref{eq:elbo} is calculated with posterior samples $q(z)$, the likelihood over prior distribution $p(z)$ is also optimized with the KL-divergence term (see Appendix~\ref{appendix:elbo}). Therefore, it is possible to reconstruct the data $x$ using prior $p(z)$ and the decoder $x(z)$ after training.

\section{Methods}

\subsection{The Bayesian Behavior framework}
The Bayesian behavior framework \citep{han2024synergizing} provides a mathematical model for the interaction between habitual and goal-directed behaviors. A latent Bayesian variable $z$ encodes the two behaviors with its prior and posterior distributions, respectively:
\begin{align*}
    \text{habitual action} & \leftarrow z^{\text{prior}}, \\
    \text{goal-directed action} & \leftarrow z^{\text{post}}. 
\end{align*}
The intuition behind such formation is that posterior distribution in Bayes theory relies on additional information than the prior. Habitual behavior relies simply on context (current state), thus encoded as prior; whereas goal-directed behavior
is refined by additional evidence (current state + plan of future states), thus encoded as posterior.
The interplay between two behaviors (including the habitization process) can be modeled by minimizing a free energy function \citep{friston2006free} (mathematically equal to the negative of ELBO \citep{kingma2013auto}, also known as the deep variational information bottleneck \citep{alemi2017deep}):
\begin{align*}
\mathcal{L}= \underbrace{\mathbb{E}_{q(z)}\left[\text{Recon. loss} + \text{RL loss}\right]}_{Accuracy} + \underbrace{D_{\mathrm{KL}}\left[q(z)||p(z)\right]}_{Complexity},
\end{align*}
where $q(z)$ and $p(z)$ denotes the posterior and prior probabilistic density function of $z$, respectively. Recon. and RL loss (accuracy) corresponds to learning the state-transition model and policy improvement, notably over the expectation using posterior $z$. In the habitization process, the KL-divergence (complexity) term can be intuitively understood as aligning habitual behavior with the goal-directed one (Appendix~\ref{appendix:elbo}). 

The Bayesian behavior framework was shown to replicate key experimental findings in cognitive neuroscience \citep{dolan2013goals,wood2016psychology}, in an online RL setting. However, \citet{han2024synergizing} used simple T-maze navigation tasks \citep{o1971hippocampus, olton1979mazes} to compare with neuroscience experiments, and their methods have not been compared with state-of-the-art models from the machine learning community.

\subsection{Habi: a framework to habitize diffusion planners}

Inspired by \citet{han2024synergizing}, we aim to solve the efficiency problems of diffusion planners by developing a framework, which we call \textbf{Habi}, to convert slow, careful diffusion planning into fast, habitual actions, while keeping the effectiveness and the probabilistic nature of policy. Habi consists of two accordingly stages: \textbf{(a)} Habitization (Training) and \textbf{(b)} Habitual Inference, as depicted in Figure~\ref{fig:training}.

Habi aligns the decision spaces of habitual behaviors and deliberate goal-directed (diffusion model-based) planning. The habitual and goal-directed behaviors are encoded as the prior and posterior distributions of a latent probabilistic variable $z$, respectively. 

As powerful diffusion planner models have already been developed by existing studies \citep{lu2025what}, we can leverage the off-the-shelf, pretrained diffusion planning models.

\subsection{Transition from Goal-directed to Habitual Behavior}
\label{sec:transition-from-g-to-h}

In this section, we will detail how a behavior policy is \textit{habitized} from a given diffusion planner (Figure~\ref{fig:training}a). In our Bayesian behavior framework, the Bayesian latent variable $z_t^p$ (or $z_t^q$, where $p$ denotes prior, and $q$ indicates posterior) is treated as a random variable following diagonal Gaussian distribution $\mathcal{N}(\mu_t^p, \sigma_t^p)$ (or $\mathcal{N}(\mu_t^q, \sigma_t^q)$). The subscript $t$ denotes the timestep as we consider a Markov decision process \citep{bellman1957markovian}. For simplicity, we will omit the subscript $t$ for the following formulations.

The prior distribution $(\mu_p, \sigma_p)$ is obtained from a mapping (feedforward network) from state $s$ (Figure~\ref{fig:training}a):
\begin{align}
z^p \sim \mathcal{N}(\mu^p, \sigma^p) \leftarrow \mathrm{PriorEncoder}(s).
\end{align}
The posterior $z^q$ is trained to encode the goal-directed behavior by auto-encoding the diffusion planner's action $a^*$(Figure~\ref{fig:training}a). 
\begin{align}
z^q \sim \mathcal{N}(\mu^q, \sigma^q) \leftarrow \mathrm{PosteriorEncoder}(s, a^*).
\end{align}
A reconstruction loss $\mathcal{L}_\mathrm{recon}$ is introduced (We provide implementation details in Appendix~\cref{appendix:reconstruction-critic}):
\begin{align}
\label{eq:recon}
\mathcal{L}_{\mathrm{recon}} = \big\lVert\mathrm{Decoder}(z^q) - a^* \big\rVert_2.
\end{align}

Intuitively, to make habitual behaviors consistent with goal-directed decisions, the decision spaces of the prior and posterior distributions are aligned under a constraint by Kullback-Leibler (KL) divergence~\cite{kullback1951information}:
\begin{align}
\mathcal{L}_\mathrm{KL} &= D_{\mathrm{KL}}\big[q(z|s, a^*) || p(z|s) \big] \\
&= \log \frac{\sigma^p}{\sigma^q} + \frac{(\sigma^q)^2 + (\mu^q - \mu^p)^2}{2(\sigma^p)^2} - \frac{1}{2},
\label{eq:kl-bayes}
\end{align}
where $q(z|s, a^*)$ is the posterior distribution representing goal-directed behaviors, and $p(z|s)$ is the prior distribution representing habitual behaviors.


The overall habitization loss is elegantly defined as (which is the famous (negative of) ELBO with adjusted KL weighting \citep{higgins2017beta}):
\begin{align}
\mathcal{L} = \mathcal{L}_{\mathrm{recon}} + \beta_\mathrm{KL} \mathcal{L}_\mathrm{KL},\label{eq:actual_loss}
\end{align}
where $\beta_\mathrm{KL}$ is a weighting factor balancing reconstruction accuracy and decision space alignment. $\beta_\mathrm{KL}$ may vary greatly across tasks, and a small $\beta_\mathrm{KL}$ may lead to poor alignment, while a large $\beta_\mathrm{KL}$ may result in poor reconstruction \citep{higgins2017beta, alemi2017deep}. To enable habitization with minimal human intervention, Habi used an adaptive $\beta_\mathrm{KL}$ mechanism \citep{haarnoja2019soft, han2022variational} that dynamically adjusts its value during training:
\begin{align}
\mathcal{L}_{\beta_\mathrm{KL}} = \log\beta_\mathrm{KL} \cdot (\log_{10} \mathcal{L}_{\mathrm{KL}} - \log_{10} D_\mathrm{KL}^{tar}), \label{eq:beta_loss}
\end{align}
where $D_\mathrm{KL}^{tar}$ represents the target KL-divergence. This approach bounds $\beta_\mathrm{KL}$ to be in a reasonable range constrained by $D_\mathrm{KL}^{tar}$, leading to robust performance across tasks without requiring manual tuning (\ref{subsec:adaptive-kl-beta}).

\subsection{Supervising Habitual Behaviors with Critic}
\label{sec:supervising-critic}

Habitual behaviors are fast and efficient but should not be purely instinct-driven. In the brain, regions like the dorsal striatum provide feedback to ensure they remain effective and avoid making mistakes \citep{kang2021primate}. 
Inspired by this, Habi also introduces a \textbf{\textit{Critic}} function that evaluates habitual decisions by considering both the decision latent $z$ and the corresponding action $a$ (similar to a Q-function, Figure~\ref{fig:training}a). The critic loss is defined as:
\begin{align}
\label{eq:critic}
\mathcal{L}_{\mathrm{critic}} &= \big\lVert\mathrm{Critic}(z^q, a^*) - \mathcal{Q} \big\rVert_2.
\end{align}
where $\mathcal{Q}$ is a scalar, represents the decision quality. In offline reinforcement learning, $\mathcal{Q}$ is typically estimated from offline data and represent as a Q-function~\cite{wang2023diffusion, hansen2023idql}, value function~\cite{hansen2023idql, lu2025what}, or a classifier~\cite{liang2023adaptdiffuser, janner2022planning}. In practice, we use the pre-trained $\mathcal{Q}$ corresponding to its diffusion planner as the ground truth of our critic function. Note that $z^q$ is detached (gradient stopped) in critic learning. 

\subsection{Inference with Habitual Behaviors}  
After habitization training (Figure~\ref{fig:training}a), we obtained a habitual behavior model, which can be used without planning (Figure~\ref{fig:training}b). We refer to it as \textit{\textbf{Habitual Inference}} (Figure~\ref{fig:training}b), which uses the \textbf{\textit{Prior Encoder}}, \textbf{\textit{Decoder}}, and \textbf{\textit{Critic}} to generate habitual behaviors efficiently. HI samples multiple latents \( z_i^p \) from the prior distribution, decodes them into corresponding actions \( a_i \), and select the best action using the critic. The process is formalized as:  
\begin{align}
\{z^p_i\}_{i=1}^{N} &\sim \mathcal{N}(\mu^p, \sigma^p) \leftarrow \mathrm{PriorEncoder}(s) \\
a_i &= \mathrm{Decoder}(z_i) \\
a &= \arg\max_{a_i} \; \mathrm{Critic}(z_i^p, a_i).
\end{align}
where \( N \) represents the number of sampling candidates. The Critic evaluates each action \( a_i \) along with its decision latent \( z_i \), and selects the best action \( a \) as the habitual behavior for inference. In practice, we observe using \( N = 5 \) candidates is sufficient to achieve satisfying performance. Detailed discussions and ablation studies are provided in Section~\ref{eq:ablation-mcss}.

\section{Experimental Results}
\input{tables/main-table}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/pvf_cpu.pdf}
    \vspace{-5mm}
    \caption{Visualized results of Table \ref{tab: main-table}. HI consistently performs in parallel with best models while being highly efficient.}
    \label{fig:pvf-cpu}
\end{figure*}

\subsection{Experiment Setup}

\textbf{Benchmarks.} \;
We empirically evaluate Habi on a diverse set of tasks from the D4RL dataset~\cite{fu2020d4rl}, one of the most widely used benchmarks for offline RL. We test our methods across different types of decision-making tasks, including locomotion, manipulation, and navigation (Appendix~\ref{appendix:benchmarks}). 

\textbf{Baselines.} \;
To better compare the performance of Habi with other state-of-the-art diffusion planners to benchmark its performance, we include four types of representative baselines: (1) deterministic policies: BC (vanilla imitation learning), SRPO~\cite{chen2023score} (2) diffusion policies: IDQL~\cite{hansen2023idql}, DQL~\cite{wang2023diffusion}, (3) diffusion planners: Diffuser~\cite{janner2022planning}, AdaptDiffuser~\cite{liang2023adaptdiffuser}, Decision Diffuser (DD)~\cite{ajay2022conditional}, Diffusion Veteran (DV)~\cite{lu2025what}, and (4) recent works on accelerated decision-making: DiffuserLite~\cite{dong2024diffuserlite}, DTQL~\cite{chen2024diffusion}. Considering that different baselines adopt varying frameworks and may differ in decision-making frequency, we reproduce the relevant baselines (marked with *) using CleanDiffuser~\cite{dong2024cleandiffuser}, ensuring a fair and consistent comparison.

\textbf{Infrastructure} \;
All runtime measurements were conducted on two different computing hardwares: a laptop CPU (Apple M2 Max) or a server GPU (Nvidia A100). Training was on Nvidia A100 GPUs.

\textbf{Reproducibility} \;
All the results are calculated over 500 episode seeds for each task to provide a reliable evaluation. HI's results are additionally averaged on 5 training seeds to ensure robustness. Our code is anonymously available at \url{https://bayesbrain.github.io/}.

\subsection{Efficient and Effective Decision Making}
How does HI compare with related methods in terms of efficiency and performance? By summarizing the results from the experiments (Table~\ref{tab: main-table}), we find that Habitual Inference (HI), could achieve comparable performance of best diffusion policies or diffusion planners. Notably, HI even outperforms the strongest decision-making baselines in certain tasks, suggesting that the habitization process may potentially help mitigate planning errors and enhance decision quality. This phenomenon is particularly evident in tasks requiring precise planning, such as navigation and manipulation tasks.

Besides the standard performance metrics (the average total rewards in an online testing episode), real-world decision-making poses additional requirements on the decision efficiency or frequency. We also conduct a detailed analysis of the decision frequency across different tasks, hardwares, and parallelism levels in Table~\ref{tab: frequency} in the Appendix~\ref{sec:device-eval}. HI demonstrates a significant decision speed margin over other baselines. Figure~\ref{fig:pvf-cpu} illustrates the trade-off between performance and frequency across different tasks, highlighting HI's ability to balance efficiency and effectiveness. 

\subsection{Comparison with Direct Distillation Methods}

\input{tables/ablation-distill}

How much performance gain can we obtain using Habi \textbf{comparing with direct distillation} of diffusion planner? We compare it with standard distillation methods. Specifically, we evaluate (1) \textit{HI w/o Critic}, which removes the critic and directly uses the habitual policy without selection (Figure~\ref{fig:training}b), and (2) \textit{Standard Distillation}, which applies supervised learning to mimic the planner. The results in Table~\ref{tab: distill} show that even without the critic, Habi consistently outperforms standard distillation across all tasks. 

This demonstrates that HI itself contributes to effective decision-making beyond simple imitation. While the critic further refines decision quality, the strong performance of \textit{HI w/o Critic} confirms that Habi is not merely replicating planner behavior but instead leveraging the habitization process to learn an efficient and robust decision policy.

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/action_dist.pdf}
\vspace{-5mm}
\caption{\textbf{Action distributions of Diffusion Planner (DV) and Habitual Inference (HI).} Visualization of the action distributions from a state-of-the-art diffusion planner (DV, 2.8Hz, top)~\cite{lu2025what} and its corresponding Habitual Inference policy (HI, 1532.6Hz, middle) generated by our Habi framework. Here, HI shows a probabilistic generation capacity while roughly aligning with the action distribution from the diffusion planner (DV). More examples are deferred to Appendix~\ref{appendix:more_visual}.}
\label{fig:action-dist}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/kl_beta.pdf}
\vspace{-2mm}
\caption{\textbf{Performance Comparison of Searching $\beta_\mathrm{KL}$ and our Adaptive $\beta_\mathrm{KL}$ Mechanism.} Fixed $\beta_\mathrm{KL}$ requires grid search for each task to achieve peak performance, while our adaptive $\beta_\mathrm{KL}$ achieves comparable performance \textbf{without task-specific tuning}, as shown by the consistent red dashed line across all tasks.}
\vspace{-3mm}
\label{fig:kl-beta}
\end{figure*}

\subsection{Visualizing Prior and Posterior Policy Distributions}

To investigate whether Habi can reasonably \textbf{align habitual and goal-directed decision spaces} we conduct a case study by visualing the action distributions of the Habitual Inference (HI) policy and its corresponding diffusion-based planner in the Maze2D environment, where the two-dimensional action space allows for a clear comparison. Figure~\ref{fig:action-dist} visualizes the action distributions of Diffusion Veteran (DV)~\cite{lu2025what}, a state-of-the-art diffusion planner, and its corresponding habitual inference policy generated using our framework Habi. 

We find that despite the fundamental differences in inference mechanisms, the action distributions of HI remain well-aligned with those of DV across different states. The action distributions exhibit strong structural consistency, with HI capturing both the variability and intent of the planner's actions. Notably, HI produces more concentrated distributions. This may reflect its ability to commit to high-confidence decisions without iterative refinement. 

\subsection{Robustness of Adaptive KL-Divergence Weighting}
\label{subsec:adaptive-kl-beta}

We used adaptive $\beta_\mathrm{KL}$ (Equation~\ref{eq:beta_loss}) in habitization learning for all tasks. Then a natural question is: \textbf{how about manually tuning} $\beta_{\mathrm{KL}}$ on each task? We evaluate the effectiveness of our proposed adaptive $\beta_\mathrm{KL}$. As shown in Figure~\ref{fig:kl-beta}, fixed $\beta_\mathrm{KL}$ tuning requires extensive grid search, and its performance is highly sensitive to the choice of $\beta_\mathrm{KL}$. A suboptimal $\beta_\mathrm{KL}$ can lead to either poor alignment (when too small) or degraded reconstruction quality (when too large). This issue is especially pronounced in complex tasks like AntMaze and Franka Kitchen, where improper weighting significantly degrades performance.

In contrast, our adaptive $\beta_\mathrm{KL}$ mechanism consistently achieves almost optimal performance across all tasks without requiring task-specific tuning. By dynamically adjusting $\beta_\mathrm{KL}$ during training, it effectively balances decision space alignment and reconstruction accuracy in an automated manner. This not only simplifies training but also ensures robustness across diverse environments. The consistent performance of adaptive $\beta_\mathrm{KL}$ (red dashed line) highlights its reliability in achieving strong results without extensive hyperparameter tuning, making Habi potentially more scalable and adaptable across diverse tasks with minimal human intervention.

\subsection{Importance of Action Selection with Sampling}
\label{eq:ablation-mcss}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/candidates.pdf}
\vspace{2mm}
\caption{\textbf{Effect of the number of sampling candidates on performance.} 
Increasing the number of candidates $N$ improves performance initially, but excessive candidates ($N\geq50$) lead to diminishing returns or slight degradation. A moderate choice (e.g., $N=5$) provides a good balance between performance and efficiency. Note that $N=1$ is the case without critic-based selection.}
\label{fig:candidates}
\end{figure}

We introduced action selection with sampling in HI (Figure~\ref{fig:training}b), where more numbers of candidates will bring more computational cost. Here we examine the \textbf{necessity of this design choice and the proper number of candidates}. As shown in Figure~\ref{fig:candidates}, selecting from multiple candidates consistently improves decision quality across various tasks, confirming the benefit of leveraging the Critic for filtering. Interestingly, we observe that using a single candidate ($N=1$) already achieves competitive results in some environments, demonstrating that Habi can function effectively even in an ultra-lightweight setting. However, without candidate selection, the policy loses the ability to refine habitual decisions, leading to suboptimal outcomes in more complex tasks like AntMaze and Maze2D. A moderate number of candidates (e.g., $N=5$) provides a good balance, achieving near-optimal performance with minimal computational overhead.

\section{Related work}

\paragraph{Accelerating Diffusion Decision Making}
Diffusion models have been widely applied to decision-making tasks, including offline RL, online RL, and imitation learning. However, their slow inference speed remains a significant bottleneck, limiting their broader applicability in real-world scenarios. A primary approach to mitigating this issue involves specialized diffusion solvers~\cite{song2020denoising, lu2022dpm} to reduce sampling steps, while varying degrees of performance degradation remain unavoidable. More recently, distillation-based approaches~\cite{poole2022dreamfusion, wang2024prolificdreamer} have been proposed to enable one-step generation in diffusion models. However, directly applying these techniques to decision-making may lead to suboptimal performance~\cite{chen2024diffusion, chen2023score}.

To achieve both effective and efficient decision-making, ODE reflow-based numerical distillation has been introduced as a promising direction~\cite{dong2024diffuserlite}. Meanwhile, recent studies also explore joint training of a diffusion model and a single-step policy, using the latter for deployment while maintaining state-of-the-art performance~\cite{chen2024diffusion}. These works serve as important baselines for our comparison.

\paragraph{Variational Bayes in RL}
Variational Bayesian (VB) methods have been incorporated into RL research for several purposes. A popular usage is exploiting the generative capacity of VAE and its recurrent versions \cite{chung2015recurrent} for constructing the world model \cite{ha2018recurrent, hafner2018learning,hafner2023mastering}. VB methods are also used to extract useful representation of environmental states from raw observations by modeling the state transitions \cite{igl2018deep, han2020variational, ni2024bridging}. The acquired representation is then used for the original RL task, i.e. maximizing rewards. Also, VB principles can help to design RL objective functions \cite{levine2018controlAsProbInf, fellows2019virel, guan2024voce}. Other usages include using the prediction error to compute curiosity-driven reward to encourage exploration \cite{yin2021sequential}. Moreover,  \citet{han2022variational, han2024synergizing} demonstrated that ELBO can connect two policies in online RL, with the one easier to learn as posterior, and the prior could be used for inference, which inspires our study. However, Habi focuses on offline RL, and we are the first to show such an ELBO can result in tremendous speedup without significant performance degradation.


\section{Conclusion}
In this work, we introduce \textbf{Habi}, a general framework that habitizes diffusion planning, transforming slow, goal-directed decision-making processes into efficient, habitual behaviors. Through comprehensive evaluations on standard offline RL benchmarks, we demonstrate that Habi achieves orders-of-magnitude speedup while maintaining or even surpassing the performance of state-of-the-art diffusion-based decision-making approaches.

Looking forward, Habi paves the way for real-time, high-speed decision-making in real-world environments, making it a promising approach for applications in embodied AI and other real-world decision-making problems. Nevertheless, our work has limitations. We have focused on state (proprioception)-based decision-making tasks using offline datasets.
Future directions include extending Habi to online RL, investigating its generalization to broader domains such as vision-based tasks \citep{du2024learning, ze20243d, chi2023diffusion}, examining its effectiveness in real-world tasks, as well as combining with orthogonal techniques such as ensemble approaches~\cite{an2021uncertainty, yang2022rorl}.

\section*{Impact Statement}
This work contributes to technical advancements in machine learning by proposing an efficient decision-making framework. It does not introduce unique ethical concerns beyond standard considerations in algorithmic development and deployment.

\section*{Acknowledgment}
This work was supported in part by the Natural Science Foundation of China under Grant No. 62222606. This work was also supported by Microsoft Research.

\bibliography{main}
\bibliographystyle{ieeenat_fullname}

\clearpage
\input{appendix.tex}


\end{document}
