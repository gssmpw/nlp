\appendix
\onecolumn

\section{Benchmarking Tasks}
\label{appendix:benchmarks}
As shown in Figure~\ref{fig:visualize-env}, we consider a diverse set of benchmarking tasks to evaluate the performance of Habi. These tasks are designed to encompass a wide range of evaluation metrics, including locomotion tasks that emphasize short-term planning, robotic arm tasks requiring long-term strategic planning, and navigation tasks focused on path planning. We provide a detailed description of each task below.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/vis_envv2.pdf}
    \caption{\textbf{Rendering of the benchmarking tasks considered in this study.} The tasks encompass a diverse set of evaluation metrics, including locomotion tasks that emphasize short-term planning, robotic arm tasks requiring long-term strategic planning, and navigation tasks focused on path planning.}
    \label{fig:visualize-env}
\end{figure}

\textbf{MuJoCo Locomotion} \; The MuJoCo Locomotion task is a standard benchmarking task in reinforcement learning that requires the agent to control a simulated robot to navigate through a complex environment. The task is designed to evaluate the agent's ability to perform locomotion tasks that require short-term planning.

\textbf{Franka Kitchen} \; The Kitchen task is a robotic arm manipulation task that requires the agent to manipulate objects in a kitchen environment. The task is designed to evaluate the agent's ability to perform long-term strategic planning and manipulation tasks.

\textbf{AntMaze} \; The AntMaze task is a combination of locomotion and planning tasks that require the agent to navigate through a maze environment. The variations of this environment can be initialized with different maze configurations and increasing levels of complexity. The task is designed to evaluate the agent's ability to perform locomotion tasks while incorporating planning strategies.

\textbf{Maze2D} \; The Maze2D task is a pure navigation task that requires the agent to navigate through a 2D maze environment. The variations of this environment can be initialized with different maze configurations and increasing levels of complexity. These tasks are used to test planning capabilities in environments where spatial reasoning is critical.

\section{Property of ELBO}
\label{appendix:elbo}

While the ELBO (Equation~\ref{eq:elbo}) optimizes the log-likelihood of reconstruction data, $\mathbb{E}_{z\sim q(z)} \log P(x(z))$, based on the exception over posterior distribution of $z$, the likelihood over the expectation of prior distribution, $\mathbb{E}_{z\sim p(z)}P(x(z))$, is also being optimized. Equivalently, we need to show that the ELBO is indeed a lower bound on $\log \mathbb{E}_{z\sim p(z)} P(x|z)$:
\begin{equation}
    \mathbb{E}_{z\sim q(z)} \log P(x|z) - D_{\mathrm{KL}}\left[q(z)||p(z)\right] \leq \log \mathbb{E}_{z\sim p(z)} P(x|z).
\end{equation}

We start with the definition of the marginal likelihood:
\begin{equation}
    \log \mathbb{E}_{z\sim p(z)} P(x) = \log \int p(x, z) dz = \log \int p(x|z) p(z) dz.
\end{equation}

Introducing a variational distribution $q(z)$, we rewrite this using importance sampling:
\begin{equation}
    \log \mathbb{E}_{z\sim p(z)} P(x) = \log \int q(z) \frac{p(x|z) p(z)}{q(z)} dz.
\end{equation}

Applying Jensen's inequality (since $\log$ is concave), we obtain:
\begin{equation}
    \log \mathbb{E}_{z\sim p(z)} P(x) \geq \int q(z) \log \frac{p(x|z) p(z)}{q(z)} dz.
\end{equation}

Rewriting the right-hand side:
\begin{equation}
    \mathbb{E}_{z\sim q(z)} \log p(x|z) + \mathbb{E}_{z\sim q(z)} \log \frac{p(z)}{q(z)}.
\end{equation}

Recognizing the second term as the negative Kullback-Leibler (KL) divergence:
\begin{equation}
    \mathbb{E}_{z\sim q(z)} \log p(x|z) - D_{\mathrm{KL}}(q(z) || p(z)).
\end{equation}

Since the KL divergence is always non-negative, we conclude that:
\begin{equation}
    \mathbb{E}_{z\sim q(z)} \log p(x|z) - D_{\mathrm{KL}}(q(z) || p(z)) \leq \log \mathbb{E}_{z\sim p(z)} P(x).
\end{equation}

Thus, the ELBO is a valid lower bound on $\log P(x)$. This ensures that our ELBO-like objective function (Equation~\ref{eq:actual_loss}) improves habitual (prior) policy.

\section{Implementation Details}

In this section, we provide additional implementation details of Habi, including the network architecture and hyperparameters used in all experiments.

\subsection{Network Architecture}

As demonstrated in Figure~\ref{fig:training}, the Habi pipeline consists of four main components: (1) \textit{Prior Encoder}, (2) \textit{Posterior Encoder}, (3) \textit{Decoder}, and (4) \textit{Critic}. All these modules can be implemented with a lightweight multiple-layer perceptrons (MLP) and trained using standard backpropagation during the Habitization stage. We provide detailed descriptions of each component below.

\textbf{Prior Encoder} \; The Prior Encoder is required to encode the state into the prior latent \( z^p \) for generating habitual behaviors. In Habi, it is implemented with a multiple-layer perceptron (MLP), that only takes the current state \( s \) as input and outputs the mean $\mu^p$ and standard deviation $\sigma^p$ of the prior latent $z^p \sim \mathcal{N}(\mu^p, \sigma)$: 
\begin{align}
\begin{aligned}
\mu^p &= \mathrm{MLP}(s_t) \\
\xi^p &= \mathrm{MLP}(s_t) \\
\sigma^p &= \mathrm{softplus}(\xi^p) + \epsilon,
\end{aligned}
\end{align}
where $\epsilon$ is a small constant to ensure numerical stability ($\epsilon=0.01$ in our work). 

\textbf{Posterior Encoder} \; The Posterior Encoder is responsible for encoding the state-action pair $(s, a)$ into the posterior latent \( z^q \) for generating goal-directed behaviors. It is implemented similarly to the Prior Encoder, with the only difference being that it takes the state-action pair as input:
\begin{align}
\begin{aligned}
\mu^q &= \mathrm{MLP}(s_t, a^*) \\
\xi^q &= \mathrm{MLP}(s_t, a^*) \\
\sigma^q &= \mathrm{softplus}(\xi^q) + \epsilon.
\end{aligned}
\end{align}

\textbf{Decoder} \; The Decoder is responsible for decoding the decision latent \( z \) into an action \( a \). In Habi, the Decoder is implemented as a simple MLP that takes the latent \( z \) as input and outputs the action \( a \):
\begin{align}
a = \mathrm{MLP}(z).
\end{align}

\textbf{Critic} \; The Critic is used to evaluate the quality of habitual behaviors generated by the Decoder. In Habi, the Critic is implemented as a simple MLP that takes the decision latent \( z \) and the action \( a \) as input and outputs a scalar value $\mathcal{Q}$ representing the decision quality:
\begin{align}
\mathcal{Q} = \mathrm{MLP}(z, a).
\end{align}

In all experiments of this work, these components are consistently implemneted as a lightweight three-layer MLP with hidden dimension of 256, as is shown in Table~\ref{tab:hyperparam}.

\subsection{Full Hyperparameters}
We provide the consistent hyperparameters used in all experiments in Table~\ref{tab:hyperparam}.


\begin{table}[ht] 
\tabcolsep=20pt
\centering
\caption{\textbf{Hyperparameters in our experiments}.}
\begin{tabular}{ll} 
\toprule
\textbf{Settings} & \textbf{Value} \\ 
\midrule
Optimizer & Adam \\
Learning Rate & 3e-4 \\
Gradient Steps & 1000000 \\
Batch Size & 256 \\
Latent Dimension: $\mathrm{Dim}(z)$ & 256 \\
MLP Hidden Size (Encoder \& Decoder) & 256 \\
MLP Hidden Layers (Encoder \& Decoder) & 2 \\
Habitization Target (Locomotion Related) & DQL \citep{wang2023diffusion} (MuJoCo, Antmaze) \\
Habitization Target (Planning Related) & DV \citep{lu2025what} (Kitchen, Maze2D) \\
Target KL-divergence $D_\mathrm{KL}^{tar}$ & 1.0 \\
Number of Sampling Candidates in Habitization training & 50 \\
Number of Sampling Candidates in Habitual Inference & 5 \\
\bottomrule
\end{tabular}
\label{tab:hyperparam}
\end{table}

\subsection{Policy and Critic Training Details}
\label{appendix:reconstruction-critic}

To provide further details on the loss functions introduced in Section~\ref{sec:transition-from-g-to-h} and Section~\ref{sec:supervising-critic}, we elaborate on the reconstruction loss~(\cref{eq:recon}) and the critic training loss~(\cref{eq:critic}) in this section.

\paragraph{Policy Training.}
Most state-of-the-art diffusion model-based policy models generate actions by \textbf{sampling a batch of candidate actions }and selecting the best one for decision-searching, recent studies~\cite{wang2023diffusion, dong2024cleandiffuser, lu2025what} also have validated its effectiveness. In Habi, we aim to learn a policy that directly habitize their final decisions. Specifically, the reconstruction loss optimizes the decoder to recover the planner's selected action:
\begin{align}
\mathcal{L}_{\mathrm{recon}} = \big\lVert\mathrm{Decoder}(z^q) - a^* \big\rVert_2,
\label{eq:appendix-recon}
\end{align}
where $a^*$ is the best action chosen by the planner from the candidate batch. This helps the habitual policy focus on habitizing high-quality decision-making rather than reproducing the full distribution of candidate actions, ensuring efficient and reliable behavior generation.

\paragraph{Critic Training.}
Unlike the policy, which only needs to learn the plannerâ€™s final decision, the critic benefits from exposure to both optimal and suboptimal plans. A robust critic should distinguish between high- and low-quality decisions, enabling more effective filtering of habitual actions during inference. To achieve this, we train the critic using the whole batch of candidate actions. Formally, the critic loss is given by:
\begin{align}
\mathcal{L}_{\mathrm{critic}} = \frac1N \sum_i^N \big\lVert\mathrm{Critic}(z^q, a_i^*) - \mathcal{Q}_i \big\rVert_2,
\end{align}
where $a_i^*$ also includes suboptimal actions that were not selected by the planner, and $Q_i$ is the corresponding value given by the pretrained value function of the diffusion planners \citep{wang2023diffusion, lu2025what}. This exposure to diverse plans allows the critic to generalize better and maintain robustness in habitual decision-making.

\section{Extensive Experimental Results}

\subsection{Decision Frequency Evaluation across Different Devices}

\label{sec:device-eval}
We evaluate the decision frequency of our methods across tasks, devices, and levels of parallelism (Table~\ref{tab: frequency}). On a PC laptop (Apple M2 Max CPU), HI consistently achieves the highest decision frequency, significantly outperforming diffusion policies (IDQL, DQL) and diffusion planners (Diffuser, AdaptDiffuser, DD, DV). Compared with other accelerated probabilistic decison-making methods, HI demonstrates a consistent advantage in decision frequency across all tasks. 

On a server with an Nvidia A100 GPU, HI maintains its advantage of high decision frequency. At 10 and 20 parallel environments, it leads in Franka Kitchen, Antmaze, and Maze2D. In MuJoCo with 10 environments, DTQL achieves a slightly higher frequency, but HI remains competitive while being significantly faster than diffusion-based methods.

The results demonstrate that HI is capable of making decisions at a high frequency across different tasks, devices, and levels of parallelism, making it a suitable choice for real-time decision-making applications.

\input{tables/frequency}

\subsection{Impact of Number of Candidates}

To further analyze the role of candidate selection, we provide additional results in Table~\ref{tab: candidates}, extending our discussion from the main text. The results confirm that selecting from multiple candidates consistently improves decision quality across various tasks. Even with a single candidate ($N=1$), HI remains competitive, reinforcing its efficiency in lightweight settings. However, increasing $N$ enables further refinement, particularly benefiting complex tasks such as AntMaze and Maze2D, where decision quality is more sensitive to suboptimal habitual actions.

We also observe diminishing returns beyond a moderate number of candidates. While larger $N$ improves performance, gains plateau after $N=5$ in most environments, suggesting that a small candidate pool is sufficient for near-optimal decision-making. This balance allows HI to achieve high efficiency without excessive computational overhead, making it adaptable across different settings.

\input{tables/ablation-num-candidates.tex}

\newpage
\subsection{Visualizations of Habitual and Goal-Directed Policy Distributions}
\label{appendix:more_visual}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/action_kitchen.pdf}
    \caption{Action distribution visualization on Kitchen, plotted the same way as Figure~5. The actions are dimension-reduced by PCA.}
    \label{fig:action_kitchen1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/action_kitchen2.pdf}
    \caption{Another example of action distribution visualization on Kitchen, plotted the same way as Figure~5. The actions are dimension-reduced by PCA.}
    \label{fig:action_kitchen2}
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/action_antmaze.pdf}
    \caption{Action distribution visualization on AntMaze, plotted the same way as Figure~5. The actions are dimension-reduced by PCA.}
    \label{fig:action_antmaze}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/action_mujoco.pdf}
    \caption{Action distribution visualization on MuJoCo, plotted the same way as Figure~5. The actions are dimension-reduced by PCA.}
    \label{fig:action_mujoco}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/action_maze2d.pdf}
    \caption{Another example of action distribution visualization on Maze2D, plotted the same way as Figure~5.}
    \label{fig:action_maze2d2}
\end{figure}

