\section{Related work}
\paragraph{Accelerating Diffusion Decision Making}
Diffusion models have been widely applied to decision-making tasks, including offline RL, online RL, and imitation learning. However, their slow inference speed remains a significant bottleneck, limiting their broader applicability in real-world scenarios. A primary approach to mitigating this issue involves specialized diffusion solvers~\cite{song2020denoising, lu2022dpm} to reduce sampling steps, while varying degrees of performance degradation remain unavoidable. More recently, distillation-based approaches~\cite{poole2022dreamfusion, wang2024prolificdreamer} have been proposed to enable one-step generation in diffusion models. However, directly applying these techniques to decision-making may lead to suboptimal performance~\cite{chen2024diffusion, chen2023score}.

To achieve both effective and efficient decision-making, ODE reflow-based numerical distillation has been introduced as a promising direction~\cite{dong2024diffuserlite}. Meanwhile, recent studies also explore joint training of a diffusion model and a single-step policy, using the latter for deployment while maintaining state-of-the-art performance~\cite{chen2024diffusion}. These works serve as important baselines for our comparison.

\paragraph{Variational Bayes in RL}
Variational Bayesian (VB) methods have been incorporated into RL research for several purposes. A popular usage is exploiting the generative capacity of VAE and its recurrent versions \cite{chung2015recurrent} for constructing the world model \cite{ha2018recurrent, hafner2018learning,hafner2023mastering}. VB methods are also used to extract useful representation of environmental states from raw observations by modeling the state transitions \cite{igl2018deep, han2020variational, ni2024bridging}. The acquired representation is then used for the original RL task, i.e. maximizing rewards. Also, VB principles can help to design RL objective functions \cite{levine2018controlAsProbInf, fellows2019virel, guan2024voce}. Other usages include using the prediction error to compute curiosity-driven reward to encourage exploration \cite{yin2021sequential}. Moreover,  \citet{han2022variational, han2024synergizing} demonstrated that ELBO can connect two policies in online RL, with the one easier to learn as posterior, and the prior could be used for inference, which inspires our study. However, Habi focuses on offline RL, and we are the first to show such an ELBO can result in tremendous speedup without significant performance degradation.