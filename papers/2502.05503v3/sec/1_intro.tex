\section{Introduction}
With the rapid advancement of video generation models \cite{animatediff, dynamicrafter, text2video, tuneavideo, videocrafter1, videocrafter2}, the quality of generated videos has continuously improved, demonstrating their potential to become world simulators. However, ensuring the physical coherence of video content remains challenging, which is one of the major concerns for current video generation models. Physical coherence refers to the extent to which the motion in a video follows physical laws observed in real-world scenarios. However, most video generation benchmarks \cite{2024vbench, evalcrafter, t2vbench} do not evaluate physical coherence, allowing visually appealing but physically implausible content to receive high scores. Recently, VIDEOPHY \cite{videophy} attempted to address this issue using video-language models to answer "Does this video follow the physical laws?" The logits output can be used to assess the model's decision tendency, but this score is not equivalent to a metric that indicates how well a video conforms to physical laws.


To comprehensively evaluate whether generated videos adhere to physical laws, we introduce a novel video generation benchmark called PhyCoBench. PhyCoBench aims to encompass observable physical phenomena, including Newton's laws of motion, conservation principles (energy, momentum), collisions, rotational motion, static equilibrium, elasticity, vibration, and fluid dynamics. To capture these physical phenomena in specific human or object interactions, we construct a text prompt benchmark set across three categories: (1) simulated physical experiments, (2) common physical phenomena in everyday life, and (3) object movements in sports activities. The primary motion types examined in these test cases can be categorized into seven major groups: (1) gravity, (2) collision, (3) vibration, (4) friction, (5) fluid dynamics, (6) projectile motion, and (7) rotation.
We present PhyCoBench, a benchmark specifically designed to evaluate the physical consistency of generated videos. Our benchmark covers seven categories of physical principles: gravity, collision, vibration, friction, rotation, projectile motion, and fluid dynamics,capturing the majority of physical laws that are readily observable in video content. A total of 120 prompts are provided, each exemplifying the corresponding physical principles across these categories.

We create a benchmark set of 120 prompts for these seven categories. The content of these prompts draws inspiration from various motion recognition datasets, which are expanded using large language models (LLMs) \cite{qwen2} and further refined by human experts. Based on these prompts, we use state-of-the-art video generation models to produce corresponding videos. Our goal is to evaluate these models' ability to generate physically consistent content.

Furthermore, we propose an optical flow-guided video frame prediction model. In video generation tasks, the appearance and texture of objects can distract the model from focusing on physical laws, making it difficult for the generated video to follow these laws. To address this, we guide the generation process using optical flow, which contains only motion information. This allows the model to focus on object motion trajectories, thereby enhancing its adherence to physical laws.


Similar to video anomaly detection tasks, evaluating the physical coherence of generated videos requires detecting anomalies within the video. However, this task presents two main challenges: (1) anomalies are diverse and complex, making them difficult to define and quantify; (2) existing datasets lack negative samples, hindering the model's ability to learn prior knowledge of anomalies. To address these issues, we take inspiration from previous video anomaly detection approaches \cite{hf2vad, liu2018future, nguyen2019anomaly, zaheer2020old, yang2023video} and propose a frame prediction model called PhyCoPredictor to detect anomalies by predicting future frames. Specifically, our model takes an initial frame and a text prompt as inputs, utilizes a latent diffusion model to predict future optical flow, and uses the predicted flow to guide a text-conditioned video diffusion model for future frame prediction. After extensive training, our model can effectively predict the optical flow and visual content in dynamic scenes, allowing comparison with existing video generation models to evaluate physical coherence.


Overall, our contributions are as follows:
\begin{itemize}
    \item We construct \textbf{a comprehensive benchmark set of text prompts} for physical scenarios, which covers a wide range of common motion scenes.
    \item We propose an optical flow-guided model, named \textbf{PhyCoPredictor}, for video frame prediction that effectively predicts motion information in dynamic scenes.
    \item Using the proposed prompts and model, we develop a benchmark, named \textbf{PhyCoBench}, for evaluating the physical coherence capabilities of video generation models.
    \item Consistency evaluation shows that PhyCoPredictor can effectively assess  T2V model's ability to generate videos that satisfy physical laws.


\end{itemize}