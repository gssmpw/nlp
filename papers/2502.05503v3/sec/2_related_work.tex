\section{Related Work}
\label{sec:relatedwork}

\begin{figure}[t]
  \centering
    \vspace{-10pt}
   \includegraphics[width=0.6\linewidth]{figures/prompt.pdf}
   \vspace{-5pt}
   \caption{
   \textbf{The proportion of text prompts.} Our prompts are grouped into seven types.
   }
    \label{fig:prompt}
    \vspace{-10pt}
\end{figure}


\begin{table*}[hbt!]
\centering
\fontsize{6}{7}\selectfont  % 设置字体大小为 6pt，行距为 7pt
\renewcommand{\arraystretch}{0.9}  % 减小行间距
\resizebox{\textwidth}{!}{
\begin{tabular}{c >{\centering\arraybackslash}p{0.45\textwidth}}  % 使用 centering 和 p 类型列并控制每列宽度
\hline
\textbf{Category} & \textbf{Example Prompts} \\
\hline
Gravity & A coin falls freely from a height and lands on the carpet. \\
       & A little girl throws a tennis ball into the air, and it falls back down. \\
\hline
Collision & A tennis ball hits the ground and bounces back up. \\
          & Two billiard balls collide on the table and separate. \\
\hline
Vibration & A swing starts to sway after being gently pushed. \\
          & A pendulum swings continuously from side to side. \\
\hline
Friction & A coin slides down a smooth, steep metal slide. \\
          & A stone slides down a rough, gentle dirt slope. \\
\hline
Fluid dynamics & A stone falls into a pool. \\
                & Milk is poured into coffee. \\
\hline
Projectile motion & A girl throws a basketball into the hoop. \\
                  & A car drives off a cliff. \\
\hline
Rotation & An airplane engine starts, and the blades begin to turn. \\
         & A merry-go-round spins at high speed. \\
\hline
\end{tabular}
}
\caption{Example Prompts of PhyCoBench for Different Categories}
\label{tab:example_prompts}
\end{table*}

%-------------------------------------------------------------------------
\subsection{Reconstruction and Prediction-Based Video Anomaly Detection}

The fundamental principle of reconstruction and prediction-based video anomaly detection methods is to to train models on a large number of normal videos, enabling them to reconstruct or predict normal frames. When an abnormal frame is present in the input video, it can be detected by comparing it with the model's output. For example, based on the previous frame, \cite{liu2018future} predicts the next frame using FlowNet\cite{flownet} and GANs\cite{GANs}. \cite{nguyen2019anomaly} predicts both the next frame and the optical flow between consecutive frames. \cite{zaheer2020old} generates high-quality future frames using GANs. \cite{hf2vad} reconstructs optical flow using a memory module and employs a conditional VAE to predict future frames. \cite{yang2023video} selects multiple keyframes from $t$ frames to reconstruct the original sequence.


In this paper, we draw inspiration from the approaches mentioned above and use a frame prediction model to detect whether generated videos exhibit physical coherence. Instead, we employ a latent diffusion model-based approach, which makes our model more flexible and efficient.



%-------------------------------------------------------------------------
\subsection{Video Generation Model}

The field of video generation has been developing rapidly \cite{VideoLDM, videocrafter1, videocrafter2, seer, aid, animatediff, harvey2022flexible, imagen, ho2022video, text2performer, text2video, singer2022make, 3modelscope, dynamicrafter, magicvideo, anonymous2024d}, with numerous studies on text-to-video\cite{text2video, tuneavideo, videocrafter1, videocrafter2} and image-to-video generation\cite{dynamicrafter, animatediff, chen2023motion}. VDM \cite{ho2022video} is the first to introduce a diffusion-based approach for video generation, laying the foundation for subsequent developments in this direction. Subsequently, works like MagicVideo \cite{magicvideo}, Video LDM \cite{VideoLDM}, and LVDM \cite{LVDM} introduce latent diffusion approaches that generate videos in latent space. SVD\cite{svd} standardize the pre-training process for video generation models in three steps: text-to-image pre-training, video pre-training on a large-scale low-resolution dataset, and fine-tuning on a small-scale high-quality high-resolution dataset. Sora \cite{sora} advance the diffusion Transformer models by replacing the U-Net architecture in the latent diffusion model (LDM) with a Transformer while directly compressing videos using a video encoder.

The rapid progress in video generation models has placed higher demands on video generation benchmarks. However, existing benchmarks still struggle to comprehensively evaluate model capabilities and the quality of generated videos.




%-------------------------------------------------------------------------
\subsection{Automatic Metrics for Video Generation}

Current popular benchmarks primarily use the following metrics: IS \cite{IS}, FID \cite{FID}, FVD \cite{FVD}, SSIM \cite{SSIM}, CLIP-Score \cite{clipscore}, and their variants. EvalCrafter \cite{evalcrafter} includes 500 prompts, derived from both real user data and data generated with the assistance of large language models (LLMs), and uses 17 metrics to evaluate the capabilities of text-to-video generation models. Vbench \cite{2024vbench} assesses video generation quality from 16 dimensions, covering 24 categories with a total of 1,746 prompts. T2VBench \cite{t2vbench} contains 1,680 prompts to specifically evaluate text-to-video models in terms of temporal dynamics across 16 temporal dimensions. VIDEOPHY \cite{videophy} is a recently proposed text-to-video benchmark, consisting of 9,300 generated videos that are manually labeled. It evaluates video reasonableness based on human judgment of whether the videos conform to physical commonsense.

While these benchmarks evaluate video generation quality from multiple aspects, they still lack a simple and effective way to assess the physical coherence of video generation models. Therefore, we propose a new benchmark to fill this gap.


\begin{figure*}[ht]
\centering
\includegraphics[width=0.6\linewidth]{figures/sota_vis.pdf}
\caption{\textbf{Generated video examples of T2V models.} The videos generated by these four models do not consistently adhere to physical coherence, with varying levels of quality.}
\label{fig:sota_vis}
\end{figure*}