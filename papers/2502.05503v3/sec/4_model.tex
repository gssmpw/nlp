\section{Automatic Evaluator}



Our optical flow-guided video frame prediction model, named PhyCoPredictor, comprises two Latent Diffusion Model (LDM) modules. First, after obtaining the video generated by a text-to-video model, we take the first frame of the input video along with the corresponding text prompt. The first LDM is used to predict the future optical flow from this initial frame. Then, the predicted future optical flow, combined with the initial frame and text prompt, serves as a guiding condition for the second LDM, which generates a multi-frame video starting from that frame. The model's training pipeline is shown in Figure~\ref{fig:train_pipe}. Our ultimate goal is to determine whether the generated video maintains physical coherence by detecting physical inconsistencies or anomalies. To achieve this, we use FlowFormer++ \cite{shi2023flowformer++} to compute the optical flow of the generated video as a reference flow, and compare the reference flow, the generated video, the predicted flow, and the predicted video. If the discrepancies are significant, it indicates that the generated video contains anomalies and does not meet physical coherence.


\subsection{Preliminary: Latent Diffusion Model}

Our model belongs to the class of generative diffusion models \cite{ho2020denoising}. Diffusion models define both a forward diffusion process and a reverse denoising process. The forward diffusion process gradually adds noise to the data \( x_0 \sim p(x) \), which resulting in Gaussian noise \( x_T \sim \mathcal{N}(0, I) \), while the reverse denoising process restores the original data by progressively removing noise. The forward process \( q(x_t \mid x_0, t) \) consists of \( T \) timesteps, during which at each timestep, \( x_{t-1} \) is gradually noised to obtain \( x_t \). The denoising process \( p_\theta(x_{t-1} \mid x_t, t) \) uses a denoising network \( \epsilon_\theta(x_t, t) \) to predict a less noisy version \( x_{t-1} \) from the noisy input \( x_t \). The objective function of the denoising network is 
\begin{equation}
    \min_{\theta} \mathbb{E}_{t, x \sim p, \epsilon \sim \mathcal{N}(0, I)} \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|_2^2,
\end{equation}
where \( \epsilon \) represents the true noise, and \( \theta \) is the set of learnable parameters of the network. After training, the model can employ the reverse denoising process to recover the original data \( x_0 \) from random Gaussian noise \( x_T \).

To reduce computational complexity, the Latent Diffusion Model (LDM) \cite{LDM} was proposed, which performs noise addition and denoising in the latent space. Many recent works on diffusion models are based on the LDM architecture, including our work. In this paper, our frame prediction model is built upon an open-source image-to-video LDM framework called DynamiCrafter \cite{dynamicrafter}.  For LDM, the input \( x_0 \) is encoded to obtain the latent variable \( z_0 = E(x) \). The forward noise addition process \( q(z_t \mid z_0, t) \) and the reverse denoising process \( p_\theta(z_{t-1} \mid z_t, t) \) are performed in the latent space, and the final output of the model is obtained by decoding with a decoder, \( \hat{x} = D(z) \).

\subsection{PhyCoPredictor}


\subsubsection{Latent Flow Diffusion Module}

To enhance the performance of the frame prediction model, we drew inspiration from 
 \cite{lfdm} by using optical flow as a condition to guide the frame prediction process. The optical flow mentioned here is the latent flow generated by LDM. We input the text prompt and the first frame \( x \in \mathbb{R}^{1 \times 3 \times H \times W} \) of the video into the model, and after encoding through the VAE encoder, we replicate it \( N \) times to obtain the latent variable \( z_f \in \mathbb{R}^{N \times 4 \times h \times w} \), which contains the visual information of the first frame. Next, we compute the optical flow for \( N \) frames using FlowFormer++ and downsample it to the latent space, yielding the flow \( f \in \mathbb{R}^{N \times 2 \times h \times w} \), where \( f_0 \) is the optical flow calculated between the first frame and itself, and \( f_i \) (\( i \neq 0 \)) represents the flow calculated between the \((i - 1)\)-th and \( i \)-th frames.

To align the dimensions of the latent variable and the optical flow, we designed a Latent Adapter consisting of an MLP layer and an activation layer, which downsamples the feature dimension of \( z_f \) from 2 to 4. Next, the noised latent flow \( f \) is concatenated with \( z_f \), and the result is fed into a 3D U-Net. After the denoising process, we obtain the generated optical flow \( \hat{f} \in \mathbb{R}^{N \times 2 \times h \times w} \). Our loss function is 
\begin{equation}
\mathcal{L}_\text{flow} = \| f - \hat{f} \|_2^2,
\end{equation}
where \( \hat{f} \) represents the latent flow predicted by the model.


\begin{figure*}[t]
  \centering
  \vspace{-10pt}
   \includegraphics[width=0.99\linewidth]{figures/train_pipe.pdf}
   \vspace{-5pt}
   \caption{
   \textbf{Training pipeline.} Our model training is divided into two stages. In the first stage, we train the 3D U-Net from scratch to predict future optical flow. In the second stage, we use the pre-trained weights from DynamiCrafter and train the model to generate future video frames with more natural motion trajectories under the guidance of optical flow.}
   \label{fig:train_pipe}
   \vspace{-10pt}
\end{figure*}


\subsubsection{Latent Video Diffusion Module}

Our network architecture for predicting video frames in latent space is built upon DynamiCrafter \cite{dynamicrafter}. To more accurately predict the motion trajectories of objects in physically dynamic scenes, we chose to use optical flow as a condition to guide the frame prediction process. During the training phase, we utilize FlowFormer++ \cite{shi2023flowformer++} to obtain optical flow and employ a Flow Adapter to upsample the feature dimension from 2 to 4 to align with the latent space. Our Flow Adapter consists of a 3D convolution layer. We then add the upsampled flow to the latent variable \( z_f \), resulting in \( z_{\text{con}} \in \mathbb{R}^{N \times 4 \times h \times w} \), which fuses both motion and visual information. The input consists of a text prompt and video \( v \in \mathbb{R}^{N \times 3 \times H \times W} \). The video \( v \) is encoded by a VAE encoder to produce \( z_0 \in \mathbb{R}^{N \times 4 \times h \times w} \). After adding noise to \( z_0 \), it is concatenated with \(  z_{\text{con}} \) and fed into a 3D U-Net. Similar to DynamiCrafter, we use a CLIP image encoder to encode the first frame of the video, convert it into visual conditions through an image context network, and control the generation result through cross-attention alongside the text conditions provided by the text prompt. During this process, our loss function is
\begin{equation}
\mathcal{L}_\text{video} = \| v - \hat{v} \|_2^2,
\end{equation}
where \( \hat{v} \) represents the video predicted by the model.

During the inference phase, the optical flow is provided by the previously mentioned Latent Flow Diffusion Module (and needs to be upsampled from 32×32 to 40×64) to predict the motion trends of future video frames.

\subsubsection{Training Setup}

In our model, apart from the two 3D U-Nets and two adapters, all other components are frozen. The training process is divided into two stages: In the first stage, we train the Latent Adapter and the 3D U-Net in the Latent Flow Diffusion Module. We initially use LLM\cite{qwen2} to filter out relatively static data based on the captions from Openvid\cite{openvid}, and then train these two components from scratch using the remaining data. We sample 16 frames from the videos to ensure that the model focuses on dynamic content. The batch size is set to 4, and the training runs for a total of 100k steps on 64 L20 GPUs. Subsequently, we use Motion Data for an additional 30k steps of training, still with a batch size of 4 and on 64 L20 GPUs. The Motion Data is a dynamic scene dataset selected from UCF101\cite{ucf101}, Physics101\cite{phys101}, Penn Action\cite{pennaction}, and HAA500\cite{haa500}, annotated with captions using Multimodal Language Model(MLM). All training in the first stage is conducted at a resolution of 256×256, with the corresponding latent space dimensions being 32×32.

In the second stage, we train the Flow Adapter and the 3D U-Net in the Latent Video Diffusion Module. The U-Net in this stage is initialized with pre-trained weights from the DynamiCrafter model at a resolution of 320×512, with the latent space dimensions being 40×64. The training also uses Motion Data, with a batch size of 4 for a total of 30k steps.

\subsection{Automatic Evaluation Process}


We used FlowFormer++\cite{shi2023flowformer++} to compute the optical flow of the generated videos and sampled them to \( N \) frames to obtain the original optical flow, while also sampling the generated videos to \( N \) frames to obtain the original video. Subsequently, we input the first frame of the original video and the corresponding prompt into the trained frame prediction model to obtain the predicted optical flow and predicted video. To predict optical flow and video frames, we propose an optical flow-guided frame prediction model called PhyCoPredictor. The inference process of PhyCoPredictor is illustrated in Figure~\ref{fig:infer_pipe}.



Our model is trained extensively on typical motion scenarios to learn motion and visual priors. As a result, it generates predictions that are more likely to exhibit physically consistent behavior when forecasting optical flow and future frames. In cases where the original video does not satisfy physical coherence, anomalies can be detected by comparing the original optical flow and video with the predicted optical flow and video. We evaluate the performance of the four models by calculating both optical flow loss and video loss. Finally, we evaluate the performance of each model using a scoring metric defined as:
\begin{equation} 
\text{score} = \frac{1}{\text{MSE}(f, \hat{f})} + 2 \times \text{MSE}(v, \hat{v}), 
\end{equation}
where \( f \) and \( \hat{f} \) denote the original and predicted optical flow, respectively, while \( v \) and \( \hat{v} \) represent the original and predicted video frames, respectively. A higher score indicates better physical coherence of the generated video.



\subsection{Evaluation Results}

\subsubsection{Quantitative Evaluation}

Based on our proposed text prompt benchmark set, we evaluated the text-to-video generation performance of four models: Keling1.5, Gen-3 Alpha, Dream Machine, and OpenSora-STDiT-v3. For each model, we generated a video using each prompt. We then manually ranked the performance of the four models for each prompt. Additionally, we used Dynamicrafter and our proposed optical flow-guided frame prediction model to score and rank the four models. We calculated Kendall's Tau-b coefficient and Spearman's Rank Correlation coefficient to compare the model ranking results with the manual evaluations. The results in Table~\ref{tab:correlation_coefficients} indicate that our model's rankings align more closely with human assessments, and that incorporating optical flow guidance provides a better evaluation of the physical coherence of generated videos.

% 插入表格
\begin{table}[ht]
\centering
\begin{threeparttable}
\begin{tabular}{l
                S[table-format=1.2, detect-weight]
                S[table-format=1.2, detect-weight]}
\toprule
\textbf{Method} & {\textbf{Kendall's} ($\uparrow$)} & {\textbf{Spearman's} ($\uparrow$)} \\
\midrule
\text{DynamiCrafter}$^*$ & \text{-0.2438} & \text{-0.2783} \\
\text{VideoPhy} & \text{0.0147} & \text{0.0429} \\
\textbf{Our Method} & \textbf{0.3367} & \textbf{0.3751} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[*] indicates finetuned with the same training data.
\end{tablenotes}
\end{threeparttable}
\caption{Comparison of Model Rankings with Kendall's Tau-b and Spearman's Rank Correlation Coefficients}
\label{tab:correlation_coefficients}
\end{table}


\begin{figure*}[t]
  \centering
    \vspace{-10pt}
   \includegraphics[width=0.9\linewidth]{figures/vis.pdf}
   \vspace{-5pt}
   \caption{
   \textbf{Visual comparisons of frame prediction results from DynamiCrafter and our PhyCoPredictor.}
   }
    \label{fig:vis}
    \vspace{-10pt}
\end{figure*}

\subsubsection{Qualitative Evaluation}

To further demonstrate the effectiveness of our model, we conducted a visual comparison of video predictions using DynamiCrafter and our optical flow-guided frame prediction model. The visualization results in Figure~\ref{fig:vis} show that, with the addition of optical flow guidance, PhyCoPredictor produces more natural and realistic motion trajectories in the predicted video frames. For example, in scenes involving falling objects, it can accurately predict the falling trajectory and even the rebound trajectory after the fall. In contrast, DynamiCrafter struggles to effectively predict the motion trajectories of objects. In the scene depicting leaves falling naturally, PhyCoPredictor can accurately predict the leaves drifting down and swaying in the wind, while DynamiCrafter incorrectly predicts the leaves moving upward. Additionally, PhyCoPredictor can capture the complex motion of a rolling stone, whereas DynamiCrafter performs poorly in this scenario.



