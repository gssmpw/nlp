

\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\appendix

\definecolor{mycolor}{RGB}{220, 230, 241}  % 定义一种浅蓝色



In this supplementary material, Section~\ref{sec:data} provides a detailed explanation of how the dataset used to train our PhyCoPredictor was obtained and processed. In Section~\ref{sec:eval}, we elaborate on the details of model evaluation. Section~\ref{sec:training} presents the detailed training settings.

\section{Training Data}
\label{sec:data}

\subsection{OpenVid}

OpenVid \cite{openvid} is a large-scale video-text paired dataset, containing up to 10 million videos. We use OpenVid's data to train the 3D U-Net in the Latent Flow Diffusion Module from scratch. Based on our observations, although the quality of both the videos and captions in this dataset is high, most of the video content tends to be relatively static. However, our work focuses on evaluating the physical coherence of videos, specifically targeting dynamic scenarios. Therefore, to improve the training effectiveness of the frame prediction model, we filter the dataset to retain only those videos depicting relatively dynamic scenes. We provide the video captions from OpenVid to Qwen2 \cite{qwen2} for analysis to determine whether the caption describes a dynamic scene, using a prompt template shown in the Table \ref{tab:template_qwen} below.

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{0pt}  % 消除列间距
\fontsize{8}{10}\selectfont
\renewcommand{\arraystretch}{1.0}
\begin{tabularx}{\columnwidth}{@{}X@{}}  % 消除表格左右两侧的间距
\toprule
\rowcolor{mycolor}
\sloppy
The following is a description of the main content of the video. Please analyze the description and determine whether the video is dynamic or static based on significant changes in the movement trajectories of people or objects. For example, dynamic videos include noticeable actions such as objects colliding, falling, being thrown, vibrating, people running, fast-moving trains, or other significant movements. Static videos have little to no movement, such as someone using a phone or having a conversation. If judged as dynamic, return `The video is dynamic`; otherwise, return `The video is static`. Video caption: \textbf{caption} \\
\bottomrule
\end{tabularx}
\caption{\textbf{The prompt template for Qwen2 to determine the dynamics of a video.}}
\label{tab:template_qwen}
\end{table}



We filter out dynamic scene videos based on Qwen's responses. OpenVid originally contains 10 million data points, which are reduced to 1.2 million after filtering.

\subsection{Motion Data}

Another dataset we use, called Motion Data, is composed of UCF101 \cite{ucf101}, PennAction \cite{pennaction}, HAA500 \cite{haa500}, and  Physics101 \cite{pennaction}. The first three datasets are action recognition datasets, with each video labeled with an action category.To obtain video captions, we first input the video into a Multimodal Language Model (MLM) to generate a description, which is then fed into a Large Language Model (LLM) for further refinement. The prompt we use is shown in Table~\ref{tab:caption_prompt}. For these three datasets, we filter the videos based on their action categories, and the filtered action categories for each dataset are shown in Table \ref{tab:three_dataset}.

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{0pt}  % 消除列间距
\fontsize{8}{10}\selectfont
\renewcommand{\arraystretch}{1.0}
\begin{tabularx}{\columnwidth}{@{}X@{}}  % 消除表格左右两侧的间距
\toprule
\rowcolor{mycolor}
\sloppy
The prompt input to the MLM:\par
Provide a detailed description of the video in English, focusing on the movement information of the objects in the video.\par
The prompt input to the LLM:\par
\textbf{``The output of the MLM''} The text above is a description of a motion video. Please modify the description to meet the following requirements: 1. Retain information about the movement of objects and people. 2. Remove descriptions of people's clothing. 3. Remove descriptions of the environment and background.\par
\\ % <--- 在行末添加这一行，表示该行结束
\bottomrule
\end{tabularx}
\caption{\textbf{The prompt for generating video captions.}}
\label{tab:caption_prompt}
\end{table}



\begin{table}[ht]
\centering
\setlength{\tabcolsep}{0pt}  % 消除列间距
\fontsize{6}{8}\selectfont  % 调整字体大小
\renewcommand{\arraystretch}{1.05}  % 调整行间距
\begin{tabularx}{\columnwidth}{@{}>{\centering\arraybackslash}p{0.3\columnwidth}@{}X@{}}
\toprule
%\rowcolor{mycolor}
\textbf{Dataset} & \textbf{Filtered Action Categories} \\
\midrule
UCF101 & Archery, Baseball Pitch, Basketball, Bench Press, Billiards, Bowling, Boxing Punching Bag, Boxing Speed Bag, Clean And Jerk, Field Hockey Penalty, Frisbee Catch, Golf Swing, Hammer Throw, Hammering, Hula Hoop, Javelin Throw, Juggling Balls, Nunchucks, Pizza Tossing, Shotput, Soccer Juggling, Soccer Penalty, Table Tennis Shot, Tennis Swing, Swing, Throw Discus, Trampoline Jumping, Volleyball Spiking, Yo-Yo \\
\midrule
PennAction & baseball\_pitch, baseball\_swing, bench\_press, bowl, clean\_and\_jerk, golf\_swing, jump\_rope, jumping\_jacks, pullup, pushup, situp, squat, strum\_guitar, tennis\_forehand, tennis\_serve \\
\midrule
HAA500 & ALS Ice Bucket Challenge, add new car tire, atlatl throw, axe throwing, badminton serve, badminton underswing, base jumping, baseball bunt, baseball catch catcher, baseball catch flyball, baseball catch groundball, baseball pitch, baseball swing, basketball dribble, basketball dunk, basketball hookshot, basketball jabstep, basketball layup, basketball pass, basketball shoot, beer pong throw, bike fall, billiard hit, BMX riding, bowling, bowls throw, card throw, cast net, closing door, cross country ski slide, cross country ski walk, curling follow, curling push, curling sweep, dart throw, dice shuffle reveal, dice stack shuffle, discuss throw, flipping bottle, flying kite, football catch, football throw, frisbee catch, frisbee throw, golf part, golf swing, guitar flip, guitar smashing, gym lift, gym pull, gym push, hammer throw, hammering nail, hanging clothes, high jump jump, high jump run, hopscotch skip, hopscotch spin, horizontal bar flip, horizontal bar jump, horizontal bar land, horizontal bar spin, hurdle jump, javelin throw, juggling balls, kick Jianzi, pancake flip, pétanque throw, pizza dough toss, play yo-yo, punching sandbag, punching speedbag, push wheelchair, push wheelchair alone, shotput throw, sledgehammer strike down, sling, slingshot, soccer dribble, soccer header, soccer save, soccer shoot, soccer throw, softball pitch, speed stack, squash backhand, squash forehand, swinging axe on a tree, tennis backhand, tennis forehand, tennis serve, throw boomerang, throw paper-plane, throwing bouquet, tire pull, tire sled, trampoline, volleyball overhand, volleyball pass, volleyball set, volleyball underhand, weightlifting hang, weightlifting overhead, weightlifting stand \\
\bottomrule
\end{tabularx}
\caption{\textbf{The filtered action categories in UCF101, PennAction, and HAA500.}}
\label{tab:three_dataset}
\end{table}


As for Physics101, it is a video dataset focused on physical experiment scenarios. We use all of the videos in this dataset. The dataset labels include object type, weight, material, and experiment category (such as falling or collision). Using these three labels, we manually annotate a caption for each video.

In summary, the number of videos used from each of the four datasets is shown in Table~\ref{tab:video_num}.

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{0pt}  % 消除列间距
\fontsize{8}{10}\selectfont  % 调整字体大小
\renewcommand{\arraystretch}{1.05}  % 调整行间距
\begin{tabular}{@{}>{\centering\arraybackslash}p{0.4\columnwidth}@{}>{\centering\arraybackslash}p{0.4\columnwidth}@{}}
\toprule
\textbf{Dataset} & \textbf{Filtered video nums} \\
\midrule
UCF101 & 3950 \\
\midrule
PennAction & 954 \\
\midrule
HAA500 & 2080 \\
\midrule
Physics101 & 6075 \\
\bottomrule
\end{tabular}
\caption{\textbf{The number of videos used in the four datasets.}}
\label{tab:video_num}
\end{table}

\section{Evaluation Details}
\label{sec:eval}

\subsection{Manual Evaluation}
We input the carefully designed set of 120 prompts into four models—Keling1.5 \cite{keling}, Dream Machine \cite{luma}, Gen-3 Alpha \cite{runway}, and OpenSora \cite{opensora}. The generated videos are then provided to professional evaluators for manual ranking. For each set of four videos generated from a prompt, the instructions given to the evaluators are shown in Table ~\ref{tab:manual}.

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{0pt}  % 消除列间距
\fontsize{8}{10}\selectfont
\renewcommand{\arraystretch}{1.0}
\setlength{\fboxsep}{0pt}  % 消除 \fbox 内的额外空白
\colorbox{mycolor}{%
\begin{tabularx}{\columnwidth}{@{}X@{}}  % 消除表格左右两侧的间距
\toprule
\sloppy
The descriptions include a series of object motions related to free fall, collision, vibration, friction, fluid dynamics, projectile motion, and rotation. Based on these descriptions, four sets of videos are generated. Observing the motion trajectories of the objects in the generated videos, the videos are then ranked based on how well they adhere to real-world physical laws. Specifically, evaluators consider whether the object's motion trajectory, speed, and acceleration are reasonable and credible given the object's material, mass, volume, and the forces acting upon it, and whether such motion is likely to occur in the real world. Videos are ranked from best to worst using numerical labels, for example, ``2 $>$ 1 $>$ 3 $>$ 4,'' with the most physically plausible listed first. If the ranking cannot be determined between videos, use an equal sign, such as ``2 $>$ 1 = 3 $>$ 4.'' \par
%\bigskip  % 添加适当的垂直间距
Please do not focus on:\par
(1) The details of human faces or bodies in the video.\par
(2) The video style.\par
(3) Camera movements.\par
\\  % <--- 在这里加一行换行，告诉 LaTeX 行结束
\bottomrule
\end{tabularx}%
}
\caption{\textbf{The requirements for manual evaluation.}}
\label{tab:manual}
\end{table}

\subsection{Correlation Coefficient Calculation}
When calculating the Kendall and Spearman correlation coefficients between the manual ranking results and the automated ranking results from our frame prediction model, we assign numerical values of 1, 2, 3, and 4 to the ranking order $A > B > C > D$. If two models are ranked equally in the manual evaluation, we take the average of their positions as the ranking value for both models. However, if more than two models are ranked equally, to avoid generating NaN in the calculations, we skip that particular sample.


\section{Training Details}
\label{sec:training}

Our training is divided into two stages. All of our training is conducted using 64 L20 GPUs.

\subsection{Stage One}
In the first stage, we begin by training the Latent Adapter and 3D U-Net in the Latent Flow Diffusion Module from scratch using the filtered OpenVid dataset, with a learning rate of 5e-5 and a batch size of 4, for a total of 100k steps. We then fine-tune the Latent Adapter and 3D U-Net using the Motion Data, with a reduced learning rate of 1e-5, keeping the batch size the same, and training for an additional 30k steps.

\subsection{Stage Two}
In the second stage, we train the Flow Adapter and 3D U-Net in the Latent Video Diffusion Module, using pre-trained weights for the U-Net provided by DynamiCrafter \cite{dynamicrafter}. The training also uses Motion Data, with a learning rate of 1e-5, a batch size of 4, for a total of 30k steps.