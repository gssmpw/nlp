\section{Physical Coherence Benchmark}



\subsection{Prompts}
To comprehensively evaluate the physical coherence of text-to-video generation models, we propose a benchmark set containing 120 prompts, categorized into seven groups: (1) gravity, (2) collision, (3) vibration, (4) friction, (5) fluid dynamics, (6) projectile motion, and (7) rotation. Some examples are shown in Table \ref{tab:example_prompts}. We reference definitions and explanations of motion from physics textbooks from a professional standpoint, while also considering common motion scenarios in action recognition datasets such as UCF101\cite{ucf101}, PennAction\cite{pennaction}, and HAA500\cite{haa500} from an everyday perspective. Ultimately, based on these references, we classify motions into seven categories. Our prompts can also be grouped into three types based on their content: (1) simulated physical experiments (e.g., "A rubber duck falls freely from a height and lands on the wooden floor."), (2) common physical phenomena in daily life (e.g., "A swing is pulled to the highest point and then released, beginning to sway."), and (3) object movements in sports activities (e.g., "A ping pong ball falls from a height onto a table and bounces."). The statistics for these prompts, in terms of their distribution across the seven categories, are shown in Figure~\ref{fig:prompt}.








\subsection{Human Evaluation Results}
We evaluated four text-to-video generation models (Keling1.5 \cite{keling}, Gen-3 Alpha \cite{runway}, Dream Machine \cite{luma}, and OpenSora-STDiT-v3 \cite{opensora}) by generating videos based on our 120 prompts. Some of the generated video results are shown in Figure~\ref{fig:sota_vis}. It is evident that the generated videos do not consistently adhere to physical consistency. For the same prompt, the quality of the videos varies significantly across the four models. This indicates that there is still a substantial gap between different models in terms of physical consistency. Therefore, there is an increased need for a more accurate and in-depth evaluation of model performance in this dimension.



\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{figures/rank_all.pdf}
\caption{\textbf{Overall ranking result from manual evaluation.}}
\label{fig:rank_all}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{figures/rank_c.pdf}
\caption{\textbf{Category-specific ranking results from manual evaluation.}}
\label{fig:rank_c}
\end{figure}


For the videos generated by the four T2V models, we initially conduct a manual ranking of the four models for each prompt. The results are shown in Figure~\ref{fig:rank_all} and Figure~\ref{fig:rank_c}. 

In Figure~\ref{fig:rank_all}, the physical coherence performance of the four T2V models was manually evaluated and ranked. As shown in the figure, Keling1.5 stands out with the highest physical coherence, significantly outperforming the other models. The performances of Dream Machine and Gen-3 Alpha are quite close to each other. In contrast, the open-source model OpenSora-STDiT-v3 scores relatively lower, far behind the top three, indicating substantial room for improvement in terms of physical coherence.

Figure~\ref{fig:rank_c} presents the detailed performance of each model across the seven physical scenario categories. In this radar chart, Keling1.5 demonstrates the most comprehensive performance, covering the widest range, and achieves the highest scores in several categories, particularly excelling in gravity, collision, and friction. Dream Machine and Gen-3 Alpha show relatively balanced performance but are slightly behind. OpenSora-STDiT-v3, on the other hand, performs relatively poorly, failing to achieve high scores across all categories.

The drawback of manual evaluation is the lack of quantifiable metrics for comparison, as well as the high cost. Therefore, we use a frame prediction model for automated quantitative evaluation. Next, we will provide a detailed introduction to the frame prediction model.

\begin{figure*}[t]
  \centering
  \vspace{-1pt}
   \includegraphics[width=0.99\linewidth]{figures/infer_pipe.pdf}
   \vspace{-5pt}
   \caption{
   \textbf{Inference process of PhyCoPredictor.} Once we obtain the generated video from the T2V model, we input the first frame and the prompt into PhyCoPredictor. The Latent Flow Diffusion Module predicts the future optical flow, which then guides the Latent Video Diffusion Module to predict future video frames.}
   \label{fig:infer_pipe}
   \vspace{-1pt}
\end{figure*}