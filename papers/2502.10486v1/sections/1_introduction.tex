\section{Introduction}

% \begin{itemize}
%     \item VLM safety concern, safety alignment data curation is costly
%     \item question: vision module hurts the safety alignment of the language module
%     \item 
% \end{itemize}


% \muhao{1. Background: Safety alignment for VLMs is difficult because of cost (and perhaps other reasons). Although VLMs can be built upon LLMs that have textual safety alignment, the vision modality easily undermines the safety mechanism.\\
% 2. Motivation: (We propose a strategy to fulfill the safety alignment gap by ... (why does this make sense?)) LLMs safety alignment is widely studied. \\
% 3. In this paper...}

Recently, the development of Vision Language Models (VLMs)~\cite{gpt4v, anthropic2023claude,liu2024visual,liu2024improved} has marked a significant advancement, enabling models to process information from both visual and textual modalities and have shown promising capabilities across various applications~\cite{liu2024visual, MiniGPT-4, InstructBLIP, Qwen-VL&Qwen-VL-chat}. However, the integration of multiple modalities brings about increased safety concerns, particularly regarding the vulnerability of these models to harmful queries and malicious attacks~\cite{gong2023figstep,Liu_Zhu_Gu_Lan_Yang_Qiao_2024}.
% \muhao{For example,}
For example, the malicious attack may effect on one of the modalities~\cite{gou2024eyes,zhang2024debiasing} or even on a mixture of several modalities~\cite{li2024images}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/blank.pdf}
    \caption{An example of the influence of the visual modality on safety alignment of LLaVA. The incorporation of the vision module undermines the safety mechanism of the language module.}
    \label{fig:blank}
\end{figure}


Despite the textual safety alignment that is inherent in Large Language Models (LLMs), the alignment of visual encoders is relatively weak, making VLMs susceptible to successful attacks through the visual modality~\cite{bailey2023image,liang2024vl}.
% these safeguards can be easily compromised when the vision modality is integrated into VLMs~\cite{}\todo{figure}.
For instance, even the incorporation of a blank image, which is meaningless and is irrelevant to the textual input, can break the safety alignment and trigger harmful responses from the VLM (\Cref{fig:blank}).
We propose that this issue stems from modality gap~\cite{liang2022mind, schrodi2024two}, a separation between image and text representations in the shared embedding space. This gap weakens the clear distinction between harmful and harmless queries that is otherwise evident in LLMs, thus posing a significant safety challenge for VLMs.

To this end, we propose \MODEL, an inference-time intervention strategy designed to leverage the LLM component for supervising the safety alignment of the VLM. \MODEL operates by projecting the representations of VLMs into a subspace orthogonal to the safety steering direction, which is derived from the safety-aligned language module. Furthermore, it modifies the representations of harmful queries by moving them opposite to the safety steering direction, thereby increasing the model's refusal probability for unsafe queries.
Our approach aims to bridge the safety alignment gap between LLMs and VLMs, ensuring that VLMs maintain safety and harmlessness without compromising their performance across multimodal tasks.

