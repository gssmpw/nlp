\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/pca.pdf}
    \caption{Visialization of hidden states by LLaVA with \MODEL under $2$-dimensional PCA. The distinction between harmful and harmless queries is maintained and even strengthened after applying \MODEL.}
    \label{fig:PCA}
\end{figure}

\section{Related Work}

%Our work is connected to three research topics. Each has a large body of work which we can only provide as a highly selected summary.

% put 2-4 orthogonal topics of relevant work, and stitle them.

% \muhao{describe by categories.}

% \textbf{Safety of VLMs}
% In the context of VLMs, several methods have been identified that can lead to unsafe behaviors including adversarial attacks \cite{qi2023visual, carlini2024aligned, zhao2024evaluating}
% %which involve creating perturbed inputs to deceive the model, 
% and jailbreaking techniques \cite{niu2024jailbreaking, gong2023figstep, li2024images}.
% %, which exploit the model’s vulnerabilities to produce harmful content.
% To enhance the safety of VLMs, multiple strategies have been proposed, primarily focusing on supervised fine-tuning (SFT) \cite{llava_VLGuard, chen2023dress} and
% %SFT approaches involve refining model behavior using specialized datasets to improve safety performance. 
% Reinforcement Learning from Human Feedback (RLHF) \cite{bai2022training, ouyang2022training} 
% %has shown promise in guiding the behavior of text-only LLMs, it is resource-intensive and not directly applicable to the visual modality.

% \noindent \textbf{Modality Gap}
% The modality gap refers to the phenomenon where image and text embeddings occupy separate regions in the shared embedding space \cite{liang2022mind, park2024bridging}. It has been observed that only a few embedding dimensions contribute to the modality gap, which is caused by an imbalance of information between images and their corresponding captions \cite{shi2023towards, schrodi2024two}. 

% \noindent \textbf{Representation Engineering} Representation engineering techniques like Activation Engineering and methods such as Inference-Time Intervention (ITI) \cite{li2024inference} and Representation Engineering (RepE) \cite{zou2023representation} have been proposed to guide model behavior by identifying and manipulating ``truthful'' attention heads or representations corresponding to high-level concepts. Directed Representation Optimization (DRO) \cite{zheng2024prompt}  treats safety prompts as trainable embeddings and learns to move the queries’ representations along or opposite the refusal direction, depending on their harmfulness. InferAligner \cite{wang2024inferaligner} uses safety steering vectors extracted from safety-aligned models to modify the activations of the target model during inference, guiding it to provide harmless responses.

VLMs are under various safety risks such as adversarial attacks \cite{qi2023visual, carlini2024aligned, zhao2024evaluating} and jailbreaking attacks \cite{niu2024jailbreaking, gong2023figstep, li2024images}.
Existing training-time safety alignment methods include supervised fine-tuning (SFT) \cite{llava_VLGuard, chen2023dress} and Reinforcement Learning from Human Feedback (RLHF) \cite{bai2022training, ouyang2022training}.
Besides, there are also inference-time interventions in the form of representation engineering~\cite{li2024inference,zou2023representation,zheng2024prompt,wang2024inferaligner}.
We propose an inference-time alignment method that transfers the safety mechanism from LLMs to VLMs.