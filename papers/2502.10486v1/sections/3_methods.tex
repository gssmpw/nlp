\section{Approach}
\label{method}

To bridge the safety alignment gap between VLMs and LLMs, we propose \MODEL which seeks to project the multimodal representations onto the subspace that is orthogonal to the safety steering direction and further pull the represents of harmful and harmless queries apart.

\paragraph{Anchoring Safety Steering Direction}

Following~\citet{wang2024inferaligner} and~\citet{zheng2024prompt}, \MODEL first anchors an LLM's low-dimensional representation space that captures the features related to the queries’ harmfulness, which correlates with the model’s refusal behavior. It then estimates the Safety Steering Direction (SSD) that indicates the model’s refusal probability to increase.
The same set of anchor data as~\citet{zheng2024prompt} is utilized for this process, which consists of $100$ pairs of synthesized ``How to'' queries with harmful and harmless intents.

We denote the last input token’s hidden state outputted by the $l$-th layer as $\mathbf{h}_l(\cdot) \in \mathbb{R}^d$. Given the anchor data of $N$ pairs of harmful $q_i^-$ and harmless $q_i^+$ queries, the activation difference $\mathbf{A}_l \in \mathbb{R}^{N \times d}$ for the $l$-th layer is calculated as~\cite{wang2024inferaligner}:
\begin{align*}
    \mathbf{A}_l = &\Big{[}\mathbf{h}_l(q_1^-), \mathbf{h}_l(q_2^-), \dots, \mathbf{h}_l(q_N^-)\Big{]} \\
    &- \Big{[}\mathbf{h}_l(q_1^+),\mathbf{h}_l(q_2^+), \dots, \mathbf{h}_l(q_N^+)\Big{]}.
\end{align*}
We decompose the activation difference matrix $\mathbf{A}$ by compact singular value decomposition (SVD) \cite{horn2012matrix}:
\begin{equation}
	% \mathbf{A}= \sum_{i=1}^d \sigma_i \mathbf{u}_i \mathbf{v}^T_i, 
 	\mathbf{A}= \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T,
\end{equation}
where $\mathbf{U} \in \mathbb{R}^{N \times r}$ and $\mathbf{V} \in \mathbb{R}^{r \times d}$ are orthogonal matrices, $\mathbf{\Sigma}$ is an $r \times r$ diagonal matrix with non-negative real numbers on the diagonal, and $r=\min\{N, d\}$. 
The columns of $\mathbf{U}$ and $\mathbf{V}$ denote left and right singular vectors, respectively.
The diagonal entries $\sigma_i=\mathbf{\Sigma}_{i,i}$ are uniquely determined by $\mathbf{A}$ and are the singular values with $\sigma_1 \geq \sigma_2 \ldots \geq \sigma_r > 0$.
The SSD $\mathbf{V}_{m, l} \in \mathbb{R}^{m \times d}$ for the $l$-layer is estimated by the first $m$ right singular vectors of the activation difference of the last input token's hidden state between harmful and harmless queries. 
 
 % V (j)p consists of the first p singular vectors and Q(j) ∈ Rd×d


% The number of non-zero singular values is equal to the rank of $\rmH$. 
% Similarly, the feature matrix of adversarial examples is $\rmH'=[h(\rvx'_1), h(\rvx'_2), \ldots, h(\rvx'_N)]^T\in \sR^{N\times d}$, and the feature matrix of perturbations is denoted as $\Delta \rmH = \rmH -\rmH'$.


% The extracted activation difference is then projected to the low-dimensional space given by the first $m$ principal components $\mathbf{V}_l^m \in \mathbb{R}^{M*D}$ to serve as SSD.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/modality_gap.pdf}
    \caption{Visialization of LLaVA’s hidden states under $2$-dimensional PCA. We plot harmful/harmless queries with/without the blank image. Harmful and harmless queries without an image can be largely distinguished while the difference is blurred with blank image.}
    \label{fig:gap}
\end{figure}


\paragraph{Subspace Projection}

Based on the estimated SSD, \MODEL projects the hidden states of the last token within each layer onto the subspace that is orthogonal to the SSD.
This orthogonal projection ensures that the influence of vision modality is minimized in the model's representations.
Formally, for a given hidden state $\mathbf{h}_l(q)$ of input query $q$ at the $l$-th layer of the model, its projection onto the orthogonal subspace is calculated as:
\begin{equation}
% \mathbf{h}'_l(q) = \mathbf{h}_l(q) - \sum_{i=1}^{m} \frac{\mathbf{h}_l^{T}(q) \mathbf{V}_{l}^{i}}{\|\mathbf{V}_{l}^{i}\|^2} \mathbf{V}_{l}^{i},
\mathbf{h}'_l(q) = \mathbf{h}_l(q) - \mathbf{h}_l(q) \mathbf{V}_{m, l} ^T \mathbf{V}_{m, l},
% \sum_{i=1}^{m} \frac{\mathbf{h}_l^{T}(q) \mathbf{V}_{l}^{i}}{\|\mathbf{V}_{l}^{i}\|^2} \mathbf{V}_{l}^{i},
\end{equation}
where $\mathbf{V}_{m, l} ^T \mathbf{V}_{m, l}$ is the orthoprojector onto the $r$-dimension subspace that spanned by the activation difference vectors.
% where $\mathbf{V}_{l}^{i}$ represents the $i$-th principal component of the PCA-reduced subspace $\mathbf{V}_l$.
The component of the hidden states aligning with the SSD is eliminated by projecting out the component in the subspace of $\mathbf{A}$.

\input{tables/main}


\paragraph{Inference-Time Alignment}

It is widely acknowledged that even models without safety alignment may inherently possess the capability to perceive harmful intents and refuse to respond to harmful queries~\cite{wang2024inferaligner,zheng2024prompt,lin2023unlocking}. Considering that the language module of a VLM like LLaVA~\cite{liu2024visual,liu2024improved} is usually aligned for safety, it is feasible to extract SSDs from the aligned language module and use the safety related subspace to guide inference-time alignment for VLM safety.

Following~\citet{wang2024inferaligner}, \MODEL selectively targets only those inputs with harmful intent. Accordingly, SSDs extracted from the language module on anchor dataset is used to discern the intent of input queries and a binary gate is applied to further control the hidden state manipulation. The gate $g_l$ at the $l$-th layer is activated if $\mathbf{h}_l(q) \mathbf{V}_{1, l} > 0$, where $\mathbf{V}_{1, l}$ is the first principle component of extracted SSD.

Similarly, the hidden states across all token positions using SSDs extracted from the language module and the binary gate. Suppose that the set of transformer layers need to be intervened is $L_G$. For each layer $l \in L_G$, the hidden states are manipulated as 
$\mathbf{h}_l^*(Q) = \mathbf{h}_l(Q) + \alpha \cdot g_l \cdot \mathbf{h}_l(q) \mathbf{V}_{m, l} ^T \mathbf{V}_{m, l}$,
% $\mathbf{h}_l^*(Q) = \mathbf{h}_l(Q) + \alpha \cdot g_l \cdot \mathbf{h}^{'}_l(Q)$,
% where $\mathbf{h}_l^*(Q)$ is the shifted representation of input $Q$ and $\alpha$ is the intervention strength. 
where  $\alpha$ is the intervention strength. 
As for hyperparameters $L_G$ and $\alpha$, we take the choices of $L_G$ from~\citet{wang2024inferaligner} and empirically tune $\alpha$ based on its performance on the anchor dataset.


