
\section{Experiments}
\label{sec:exp}


\subsection{Datasets}
\label{sec:exp_dataset}

\paragraph{Anchor Dataset for Safety Steering Direction}
We use the same anchor dataset as proposed by~\citet{zheng2024prompt}. $100$ harmful and $100$ harmless ``How to do'' queries are generated by $\mathtt{gpt}$-$\mathtt{3.5}$-$\mathtt{turbo}$, with average lengths of $14.0$ and $13.8$ tokens, respectively.
The validity and quality of these queries are guaranteed both automatically (by $\mathtt{gpt}$-$\mathtt{3.5}$-$\mathtt{turbo}$) and manually.
Samples of anchor data are listed in \Cref{append:anchor}.
As mentioned in \Cref{method}, we randomly sample $64$ harmful and harmless queries each to estimate safety steering directions for target VLM, and save the remainder for tuning the hyperparameter of intervention strength $\alpha$.


\paragraph{Datasets for Security Evaluation}
(\romannumeral 1) \textbf{MaliciousInstruct}~\cite{huang2023catastrophic}: 
This test set consists of 100 harmful query instructions that contain $10$ different malicious intentions, including psychological manipulation, theft, cyberbullying, false accusation, tax fraud, etc. 
(\romannumeral 2) \textbf{Jailbreak Instructions}: We apply carefully crafted deceptive jailbreaking prompts to further assess the modelâ€™s safety mechanism. $5$ highly representative jailbreak prompts are selected for safety evaluation, including role-playing, privilege escalation, attention shifting, automatic generation, and the adversarial suffix.
We sample $20$ harmful instructions from MaliciousInstruct for each jailbreaking prompt, forming a jailbreak dataset with $100$ jailbreak instructions.
(\romannumeral 3) \textbf{MM-Harmful Bench}~\cite{wang2024inferaligner}: This dataset consists of $100$ harmful instructions that require the combination of both input images and text for a response. Curated specifically for multimodal models, MM-Harmful Bench includes
ten different types of malicious intentions, including discrimination, theft, illegal Weapons, cybercrime, etc.



\subsection{Evaluation Metrics}

Our primary metric for evaluating harmfulness is the \textbf{Attack Success Rate (ASR)}, defined as the percentage of malicious instructions that the target model fails to refuse, and thereby triggering harmful responses. The harmfulness of the model's response is evaluated by $\mathtt{LlamaGuard}$-$\mathtt{7b}$,\footnote{\url{https://huggingface.co/llamas-community/LlamaGuard-7b}} and the instruction we use for prompting is illustrated in \Cref{append:llama_guard}. For evaluating the quality and fluency of model responses, we directly use the perplexity calculated by $\mathtt{Llama}$-$\mathtt{2}$-$\mathtt{7b}$-$\mathtt{chat}$\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b-chat}} as a proxy.


\subsection{Baseline Methods}

\paragraph{Self-Reminder}
Self-reminder~\cite{xie2023defending} enhances the safety of a model by appending prompts during the inference phase. This inference-time alignment method wraps the user query within a system prompt that reminds the model to process and respond to the query in the context of a responsible AI assistant.

\paragraph{Goal Priority}
Goal Priority~\cite{zhang2023defending} is also an inference-time safety alignment mechanism that explicitly instructs the model to prioritize harmlessness over helpfulness in its responses, encouraging the model to be aware of the intent of input queries and refuse to respond to malicious instructions.



\subsection{Results and Analysis}
\label{exp:main}
In this paper, we take $\mathtt{llava}$-$\mathtt{1.5}$-$\mathtt{7b}$-$\mathtt{hf}$\footnote{\url{https://huggingface.co/llava-hf/llava-1.5-7b-hf}} for example and analysis the results as follows.\footnote{Please refer to \Cref{append:analysis_ppl} for analysis on the influence on generation quality.}

\paragraph{Safety Alignment Effectiveness}
As shown in \Cref{tab:main_results}, our proposed \MODEL mechanism outperforms the baseline safety alignment methods and achieves the best defense performance on all of the three malicious query settings, indicating its capability to defend against harmful instructions and counter jailbreak attacks as well. 
As for baseline methods, Global Priority performs better than Self-reminder in defending against malicious instructions. This superiority stems from the ability to recognize the malicious intent of input query, which is made possible by few-shot demonstration that showcases both safe and unsafe instructions to the model. 
In contrast, our method explicitly manipulates the representations of input queries and intentionally separates harmful and harmless instructions in the representation space, which activates and further boosts the inherent safety alignment of the language module in a VLM.


\paragraph{Safety Alignment Gap}

For vanilla LLaVA, we can spot a significant increase when image is included in the input queries ($15\%$ for text-only queries and $34\%$ for the same queries paired with a blank image), which shows the gap of safety alignment between the VLM and its LLM component.
While this gap remains under baseline methods, our alignment strategy largely narrow the safety gap without sacrificing the generation quality.
As illustrated in \Cref{fig:PCA}, in comparison to \Cref{fig:gap}, the distance between the representations of harmful and harmless queries are maintained and even strengthened after incorporating the blank image.

