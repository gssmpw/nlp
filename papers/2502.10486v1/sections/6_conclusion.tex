
\section{Conclusion}
In this paper, we propose \MODEL, an inference-time intervention method that leverages the safety alignment of the LLM component for VLM safety. 
The \MODEL utilizes the difference vectors extracted from the activations of aligned LLMs between harmless and harmful prompts. By modifying its activations based on subspace projection in response to harmful inputs, the VLM is guided to provide safe responses. Experimental results show that our method can effectively reduce the attack success rate of VLM against harmful instructions and jailbreak attacks.


%InferAligner utilizes safety steering vectors extracted from the aligned model to modify the activations of the target model when responding to harmful inputs, thereby guiding the target model to provide safe responses. Experimental results show that our method can be very effectively applied to domainspecific models and multimodal large language models. It not only significantly diminishes the ASR of both harmful instructions and jailbreak attacks, but also maintains almost unchanged performance in downstream tasks.

