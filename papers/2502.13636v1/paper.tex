\documentclass[twoside,leqno,twocolumn]{article}

\usepackage[a4paper]{geometry}
\usepackage{multirow}
\usepackage{ltexpprt}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{tcolorbox}
\renewcommand{\thesubfigure}{\alph{subfigure}}
\usepackage{microtype}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\usepackage[square,numbers]{natbib}
\bibliographystyle{siam}

\setcounter{errorcontextlines}{999}
\usepackage{todonotes}
\usepackage{amsmath,mathtools}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{float}
\usepackage[noend]{algorithmic}
\usepackage[dot]{dashundergaps}
\usepackage[binary-units=true, detect-all]{siunitx}
\sisetup{output-exponent-marker=\text{e}}
\DeclareSIUnit{\noop}{\relax}
\usepackage{booktabs}
\def\Vhrulefill{\leavevmode\leaders\hrule height 0.7ex depth \dimexpr0.4pt-0.7ex\hfill\kern0pt}

\usepackage{balance} 
\sisetup{
    detect-all  %
        }
        \NewDocumentCommand\numprint{m}{\num[round-mode = places]{#1}}
\newcommand\algorithmicprocedure{\textbf{procedure}}
\newcommand{\algorithmicendprocedure}{\algorithmicend\ \algorithmicprocedure}
\makeatletter
\newcommand{\nodesOf}[1]{#1}
\newcommand{\edgesOf}[1]{E(#1)}
\newcommand\PROCEDURE[3][default]{%
  \ALC@it
  \algorithmicprocedure\ \textsc{#2}(#3)%
  \ALC@com{#1}%
  \begin{ALC@prc}%
}
\newcommand\ENDPROCEDURE{%
  \end{ALC@prc}%
  \ifthenelse{\boolean{ALC@noend}}{}{%
    \ALC@it\algorithmicendprocedure
  }%
}
\newenvironment{ALC@prc}{\begin{ALC@g}}{\end{ALC@g}}
\makeatother
\newcommand{\smallmathenv}{%
}
  
\setlength{\textfloatsep}{5pt}  %
\setlength{\floatsep}{5pt}      %
\setlength{\abovecaptionskip}{2pt} %
\setlength{\belowcaptionskip}{2pt} %

\begin{document}
\newcommand{\weight}{\omega}
\newcommand{\bestEdgeSymbol}{\mathcal{B}}
\newcommand{\cpp}{\textsf{C}\texttt{++}\xspace}

\author{Henrik Reinstädtler\thanks{Heidelberg University, Germany}, S M Ferdous\thanks{Pacific Northwest National Laboratory, Richland, WA, USA}, Alex Pothen\thanks{Purdue University, West Lafayette, IN, USA}, Bora Uçar\thanks{CNRS and LIP UMR5668 (CNRS, ENS de Lyon, Inria, UCBL1) France; and Institute for Data Engineering and Science (IDEaS), Georgia Institute of Technology, Atlanta, GA, USA.},  Christian Schulz\footnotemark[1]} 





\title{Semi-Streaming  Algorithms for Hypergraph Matching}

\date{}

\maketitle
\fancyfoot[C]{--\thepage--}%
\fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 2025 by H. Reinstädtler, S M Ferdous,\\ A. Pothen, B. Uçar, C. Schulz}}





\begin{abstract} %

We propose two one-pass streaming algorithms for the NP-hard hypergraph matching problem. The first algorithm stores a small subset of potential matching edges in a stack using dual variables to select edges. It has an approximation guarantee of $\frac{1}{d(1+\varepsilon)}$ and requires $O((n/\varepsilon) \log^2{n})$ bits of memory.

The second algorithm computes, stores, and updates a single matching as the edges stream, with an approximation ratio dependent on a parameter $\alpha$. Its best approximation ratio is $\frac{1}{(2d-1) + 2 \sqrt{d(d-1)}}$, and it requires only $O(n)$ memory.

We have implemented both algorithms  and have engineered variants for optimizing matching weights, memory consumption, and running times. These include relaxations of the rule for admitting edges into the stack and using a second pass to improve the weight. The evaluation is done  on large-sized hypergraphs from circuit design and sparse matrix computations. Our results show that the streaming algorithms achieve much better approximation factors in practice than the worst-case bounds, reducing memory required by up to 50 times and outperforming the offline Greedy algorithm.




\end{abstract}


\section{Introduction}
Hypergraphs are a natural extension of graphs and can help to model our ever evolving earth and society. 
In a hypergraph, a hyperedge is a subset of vertices and can contain any number of them, instead of just two.
The hypergraph matching problem asks for a set of vertex-disjoint hyperedges.
Two common objectives in the hypergraph matching problem are to maximize the number or total weight of the matching hyperedges.
The hypergraph matching problem has applications ranging from personnel scheduling~\cite{froger2015set} to resource allocations in combinatorial auctions~\cite{gottlob2013decomposing}.
The hypergraph matching problem with either of the objective \hbox{functions is~$\mathcal{NP}$-complete~\cite{approxresult}}.

There are a few papers with computational studies for the hypergraph matching problem, in a single CPU in-memory setting~\cite{dufosse2019effective} and in a distributed computing setting~\cite{hanguir2021distributed}.
However, %
little to no attention was paid to increasing 
data sizes and approximation guarantees.
Streaming and semi-streaming algorithms address this trend of ever-increasing size of data.
In a streaming setting, \hbox{(hyper-)edges} arrive one by one in arbitrary order. %
The amount of memory that can be used is strictly bounded by the size of the final solution.
In the case of hypergraph matching this is~$\Theta(n)$, because every vertex can be matched at most once.
For a semi-streaming setting this criterion is relaxed to allow for an additional polylog factor.
Furthermore, establishing a bound on the degree of suboptimality is essential for evaluating the solution's effectiveness. 
For matchings in graphs, there is already a semi-streaming algorithm having such an approximation~guarantee \hbox{by Paz~and~Schwartzman~\cite{10.1145/3274668} of~$\frac{1}{2}$.}

The algorithm by Paz~and~Schwartzman requires one variable per vertex, the so-called dual, and a stack of possible solution edges.
The algorithm adds an edge to a stack while streaming, if the weight of the edge dominates the sum of the duals of its vertices, and then updates with the difference between the edge weight and those sum of duals accordingly.
After all edges have been streamed, non-conflicting edges are added from the stack in the reverse order.
Ghaffari and Wajc~\cite{ghaffari2017simplified} give a simplified proof using the primal-dual theory to prove the  approximation guarantee of~this~algorithm.

\subparagraph{Our Contributions.}
We first propose a novel streaming framework for hypergraph matching and prove an approximation guarantee in relation to the largest hyperedge size
by extending the stack-based algorithm of Paz and Schwartzman~\cite{10.1145/3274668} to hypergraphs. 
In essence, our algorithm puts hyperedges that potentially belong to a good matching on a stack, and in the end computes a matching out of those hyperedges.
 Given the maximum edge size~$d$, we use primal-dual techniques to prove a~$\frac{1}{d(1+\varepsilon)}$ approximation factor.
 Our most memory-saving algorithm requires $O(n\log^2{n}/\varepsilon)$ bits of space.
 We then propose a second family of algorithms which do not need a stack and require less space and work by greedily swapping hyperedges from the current matching with the incoming ones. 
 These algorithms require~$O(\left|e\right|^2)$ work per hyperedge~$e$, while the stack-based algorithm require~$O(\left|e\right|)$.
 They have an approximation guarantee depending on a factor $\alpha>0$, which can be tuned to result in a guarantee of $1/((2d-1)+2\sqrt{(d-1)d})$.
In experiments, we show the competitiveness of our approaches and benchmark them on a large and diverse data set with three weight variants.
 The stack-based algorithms reduce the memory consumption by up to 54.35 times in comparison to the non-streaming greedy algorithm.
When not accounting for input/output times, our fastest algorithm runs on average 3.90 times faster than the non-streaming greedy algorithm.  
 Our best streaming algorithm outperforms the greedy algorithm on one weight strategy in all three test dimensions: running time, memory and quality.

The rest of the paper is organized as follows.
After introducing the notation and related work in Section~\ref{sec:prelim}, we show our approximation guarantee for an adaptation of the Paz-Schwartzman semi-streaming algorithm and discuss further improvements in Section~\ref{sec:quality}.
Section~\ref{sec:stackless} introduces our greedy swapping algorithm for streaming.
These approaches are then extensively evaluated by experiments in Section~\ref{sec:experiments}. We conclude in Section~\ref{sec:conclusion}.
The Appendix~\ref{app:stats} contains instance statistics.
\section{Preliminaries}\label{sec:prelim}
\subsection{Basic Concepts.}
In this section, we briefly describe the basic concepts.

\subparagraph{Hypergraphs.}A \emph{weighted undirected hypergraph}~$H=(V,E,\weight)$ consists of a set~$V$ of~$n$ vertices and a set $E$ of~$m$ hyperedges.
 Each hyperedge~$e$ is a set of vertices and is assigned a positive weight by the weight function~$\weight:E\to \mathbb{R}_{>0}$.
 The number of vertices in a hyperedge~$e$ is called its size and denoted by~$\left|e\right|$, and the maximum size of a hyperedge or rank of the hypergraph is denoted by~$d:=\max_{e\in E}{\left|e\right|}$.
  If all hyperedges of a hypergraph are of size~$d$, the hypergraph is  called~$d$-uniform.
  
 \subparagraph{Matching.} A subset of (hyper-)edges~$M\subset E$ is a \emph{matching}, if all (hyper-)edges in $M$ are pairwise disjoint, i.e.,~only at most one (hyper-)edge is selected at every vertex.
  A matching $M$ is called maximal, if there is no (hyper-)edge in~$E$ which can be added to $M$ without violating the matching constrained. The weight of a matching is defined by~$\weight(M):=\sum_{e\in M}\weight(e)$ and the maximum matching is the \hbox{matching with the largest weight.}

  \subparagraph{Related $\mathcal{NP}$-hard Problems. }
  The unweighted hypergraph matching problem is closely related to the maximum independent set and the~$k$-set packing problems.
  Both problems are $\mathcal{NP}$-hard~\cite{karp2010reducibility}.
 An independent set in a graph is a subset of vertices, in which no two vertices are adjacent.
 There is a simple transformation from hypergraph matching to maximum independent set using the line graph of the hypergraph.
 Every hyperedge is assigned a vertex and two vertices are connected if they share at least~one~vertex.
Given a set~$S$ and some subsets~$s_1,\dots,s_n$ of size at most~$k$, the~$k$-set packing problem asks to select the maximum number of disjoint subsets.
It can be easily translated to the hypergraph matching setting. The set~$S$ corresponds to the vertices~$V$, while the subsets~$s_1,\dots,s_n$ correspond to the hyperedges. 

 \subparagraph{(Semi-)Streaming Algorithms.}
 If the input size exceeds the memory of a machine, a typical solution is to stream the input.
 There are several definitions for streaming in graphs and hypergraphs.
 When (semi-)streaming, the (hyper-)edges of a (hyper-)graph are usually presented in an arbitrary (even adverse) order one-by-one in several passes.
 In this paper we consider only having a constant number of passes over the input.
 In a streaming setting the memory is strictly bounded by the solution size. 
 For matching in hypergraphs, the solution size is limited by the number of vertices.
 When using the semi-streaming model the memory is typically bounded by~$O(n\cdot\mathrm{polylog}(n))$.

 \subparagraph{Approximation Factors.}
 Algorithms can be separated into three categories: exact algorithms, heuristics without approximation guarantees and approximation algorithms. The quality of an approximation algorithm is measured by comparing its solution's value to that of an optimal solution. 
 If for every instance of a maximization problem, a algorithm~$\mathcal{A}$ achieves a solution that is at most $\alpha$ times smaller than the optimal, where $\alpha \in \mathbb{R}_{>1}$, then $\mathcal{A}$ is called a $\frac{1}{\alpha}$-approximation.
 For an overview of techniques to design approximation algorithms we refer the reader to~Williamson~and~Shmoys~\cite{williamson2011design}.
 \subparagraph{(Integer) Linear Programs.}
 Many optimization problems can be formulated as an integer linear program (ILP). 
 In a maximization problem, an (integer) linear program finds an (integer) vector $x$ with components~$x_i$, that maximizes a linear cost function~$\sum c_ix_i$ such that a constraint~$Ax\leq b$, where~$A$ is a matrix, is fulfilled, with typically additional constraints on the components of $x$, e.g., $x_i\geq 0$.
 If variables of the problem are integer, some problems are~$\mathcal{NP}$-hard, while other (unimodular) problems are solvable in polynomial time efficiently~\cite{wolsey2020integer}.
 When dropping the integer constraint, any linear program is solvable in polynomial time~\cite{wright1997primal}.
For every linear program one can find a dual problem~\cite{williamson2011design}.
 Given a maximization problem as described before, the dual problem is to find a vector $y_i\geq 0$, that minimizes~$\sum b_iy_i$ subject to~$A^Ty\geq c$.
The weak duality theorem~\cite{williamson2011design} states that for any primal maximization problem, the dual minimization problem for any feasible solution has an objective value
larger than the optimal solution of the primal problem.
The strong duality theorem states that if the primal problem has an optimal solution then the dual has too and the optimum values are the same.
When the primal problem has an optimal solution, the dual t
For a more detailed introduction we refer the reader to the appendix of~\cite{williamson2011design}.

\subsection{Related Work.}
The matching problem is a well studied problem in computer science.
 In this section we give a brief overview of matchings in graphs and hypergraphs.
\subparagraph{Matching in Graphs and Streaming.}
The polynomial-time complexity of matchings in graphs is one of the classical results in theoretical computer science~\cite{edmonds_1965}.
While Preis~\cite{preis1999linear} presented the first linear time~$\frac{1}{2}$-approximation, Drake and Hougardy~\cite{drake2003simple} show a simpler algorithm with the same approximation ratio by path growing (PGA) in linear time. 
 Pettie and Sanders~\cite{PETTIE2004271} developed a~$\frac{2}{3}-\varepsilon$ approximation with expected running time of~$\mathcal{O}(m\log \frac{1}{\varepsilon})$.
 The GPA algorithm by Maue and Sanders~\cite{maue2007engineering} bridges the the gap between greedy and path searching algorithms, showing that a combination of both works best in practice.
 Birn~et~al.~\cite{birn2013efficient} developed a parallel algorithm in the CREW PRAM model with $\frac{1}{2}$-approximation guarantee and $O(\log^2n)$ work.
Feigenbaum~et~al.~\cite{feigenbaum2005graph} present a $\frac{1}{6}$-approximation for the weighted matching in  the semi-streaming setting using a blaming based analysis.
Paz and Schwartzman~\cite{10.1145/3274668} give a~$\frac{1}{2+\varepsilon}$-approximation algorithm based on  keeping track of the dual solution in a semi-streaming setting.
Their algorithm keeps track of a dual solution, admitting only edges into the primal solution that are heavier on to the stack and updating the dual solution.
 The edges are taken from the stack in reverse order and those that do not violate the matching property are added to the solution.
 The resulting matching is not necessarily maximal.
 Ghaffari and Wajc~\cite{ghaffari2017simplified} provide a simpler proof of the bound.
 Ferdous~et~al.~\cite{ferdous_et_al:LIPIcs.SEA.2024.12} show empirically that the algorithm by Paz and Schwartzman can compete quality-wise with offline $\frac{1}{2}$-approximation algorithms like GPA, while requiring less memory and time.
Recently, Ferdous~et~al.~\cite{ferdous2023streaming} presented two semi-streaming algorithms for the related weighted~$k$-disjoint matching problem, building upon previous results of  Paz and Schwartzman and Huang and Sellier~\cite{10.1145/3274668, huang_et_al:LIPIcs.APPROX/RANDOM.2021.14} on streaming~$b$-matching.

\subparagraph{Hypergraph Matching.}
Hazan et~al.~\cite{approxresult} prove that the maximum~$k$-set packing problem and, therefore, the matching problem on ~$d$-uniform hypergraphs can be poorly approximated, and there is no approximation within a factor of~$\mathcal{O}(d/\log d)$.
Dufosse~et~al.~\cite{dufosse2019effective} engineered reduction rules for special~$d$-partite,~$d$-uniform hypergraphs.
 They use Karp-Sipser rules and a scaling argument to select more hyperedges. 
There are several approximation results and local search approaches, most notably by Hurkens and Schrijver~\cite{hurkens1989size} and Cygan~\cite{cygan2013improved}  with an approximation guarantee of~$\mathcal{O}(1/\left(\frac{d+1+\varepsilon}{3}\right))$.
Hanguir and Stein~\cite{hanguir2021distributed} developed three distributed algorithms to compute  matchings in hypergraphs, trading off between quality guarantee and number of rounds needed to compute a solution.


For the weighted~$k$-set packing problem Berman~\cite{10.1007/3-540-44985-X_19} introduces a local search technique.
Improving on these results, Neuwohner~\cite{neuwohner2023passing} presents a way to guarantee an approximation threshold of~$\frac{2}{k}$. We are not aware of any practical implementations of these techniques.
For the more general weighted hypergraph~$b$-matching problem, 
Großmann~et~al.~\cite{grossmann2024engineering} presented effective data-reduction rules and local search methods.
In the online setting, when hyperedges arrive in adversarial order, and one must  immediately decide to include the incoming hyperedge or not in the matching, Trobst and Udwani~\cite{trobst2024almost} show that no (randomized) algorithm can have a competitive ratio better than~$\frac{2+o(1)}{d}$.

We are unaware of any studies or implementations for streaming hypergraph matching.

\section{Quality Guarantees for Streaming}

\label{sec:quality}
 We now present our first algorithm to tackle the hypergraph matching problem in the semi-streaming setting. 
Our algorithm uses dual variables to evaluate whether a hyperedge can be added to a stack data structure, ensuring that only potentially optimal or good hyperedges are retained from the stream of hyperedges. After processing, the stack is evaluated in reverse order to compute a matching, achieving an approximation guarantee of $\frac{1}{d(1+\varepsilon)}$.
We then provide the option to improve the computed matching by an additional streaming pass over the hyperedges.  
 Additionally, we discuss more permissive and lenient update functions that allow more hyperedges to the stack.
Finally we discuss the space complexity of our algorithms.


Algorithm~\ref{alg:simple:streaming} shows our framework for computing a hypergraph matching in a streaming setting.
This algorithm is an extension of an algorithm by Paz and Schwartzman~\cite{10.1145/3274668} proposed for graphs.
The algorithm starts with an empty stack and keeps a variable~$\phi_v$, the dual, on each vertex~$v$ of the hypergraph.
Throughout the algorithm the stack contains hyperedges that will potentially be added to a hypergraph matching.

Hyperedges are scanned one at a time.
 For each hyperedge~$e$, the algorithm checks if the weight of the dual variables of~$e$'s vertices and thereby the solution can be improved by adding $e$. More precisely,  with $\Phi_e \gets \sum_{v\in e}\phi_v$, the algorithm checks if $\omega(e)\geq \Phi_e(1+\varepsilon)$.
 The variable~$\varepsilon\in\mathbb{R}_{\geq 0}$ decides, if we want to trade quality for memory. A~smaller $\varepsilon$ yields a better approximation guarantee, while a 
 higher~$\varepsilon$ yields a smaller memory consumption.
 If $\omega(e)\geq \Phi_e(1+\varepsilon)$, then $e$ is added to the stack, and the dual variables of the vertices of $e$ are then updated using an update function. 
 The update functions that we consider take $O(1)$ time per vertex.
 
 After all hyperedges have been scanned, a matching containing only the hyperedges stored in the stack is computed.
To do that, our algorithm takes the hyperedges in reverse order from the stack and adds non-conflicting hyperedges to the matching. 
Note that hyperedges processed earlier that have conflicting heavier (later) hyperedges will be ignored. This is crucial to prove the performance guarantee later. 
 The amount of total work needed for scanning one hyperedge~$e$ is $O(\left|e\right|)$, because we need to sum up the dual variables of $e$'s vertices and update them.

The update functions are very important for this kind of algorithms.
They decide if a approximation guarantee can be given and directly impact the results.
Upon processing a hyperedge $e$, the update function applied to the dual variable of a vertex $v\in e$ can use
the prior value~$\phi_v$ and sum~$\Phi_e=\sum_{v\in e}\phi_v$ as well as $e$'s size and weight.
We define the following update function for proving the approximation guarantee 
{\begin{align}
  \phi_v^{\rm new} =  \delta_{\mathrm{g}}(e,\phi_v,\Phi_e,\weight(e))& := \phi_v +\overbrace{ \weight(e)-\Phi_e}^{w'_e}. \label{phig}
\end{align}}%

The~$\delta_{\mathrm{g}}$ function is exactly the function by Paz and Schwartzman~\cite{10.1145/3274668}. It increases each vertex's~$\phi_v$ value of an accepted hyperedge to contain the residual. The quantity $w'_e$ is the potential gain in the matching weight of adding hyperedge~$e$ to the stack.
We introduce two other update functions later.

 
At the core of this method are the variables~$\phi_v$ for each~$v$ and the update mechanism.
These core components from the original work~\cite{10.1145/3274668} 
are solidly derived from the linear programming theory.
We follow the structure of the simplified proof~\cite{ghaffari2017simplified}. 


\begin{algorithm}[t]
{%\small
  \begin{algorithmic}[1]
    \PROCEDURE{SimpleStreaming}{$H=(V,E,\weight)$}
    \STATE~$S\gets emptystack$
    \STATE $\forall v\in V: \phi_v=0$
    \FOR{$e\in E$ in arbitrary (even adverse) order}
    \STATE $\Phi_e \gets \sum_{v\in e}\phi_v$
    \IF{$\omega(e)<\Phi_e(1+\varepsilon)$}\label{alg:simple:streaming:scan}
    \STATE next
    \ENDIF
    \STATE $S.push(e)$
    \FOR{$v\in e$}
      \STATE $\phi_v \gets \delta(e,\phi_v,\Phi_e,\weight(e))$ \COMMENT{update}\label{alg:simple:streaming:update}
    \ENDFOR
    \ENDFOR
    \STATE $M \gets \emptyset$
    \WHILE{$S\neq \emptyset$}
    \STATE $e \gets S.pop()$
    \IF{$\forall f\in M: f\cap e=\emptyset$  }
    \STATE$M\gets M \cup \{e\}$
    \ENDIF
    \ENDWHILE
    \ENDPROCEDURE
  \end{algorithmic}}
  \caption{Simple Streaming Algorithm.}
  \label{alg:simple:streaming}
\end{algorithm}
%\vspace{-0.25cm}
\subsection{Approximation Guarantee.}\label{subsec:dual:problem}
The primal-dual method is a well studied technique in the realm of optimization.
For general methods and introduction to linear programs we refer the reader to \cite{bazaraa2011linear,wright1997primal}.
An excellent overview in the context of approximation algorithms can be found in Williamson and Shmoys~\cite{williamson2011design}.

We show that using~$\delta_{\mathrm{g}}$ update function~\eqref{phig} in our algorithm leads to a~$\frac{1}{d(1+\varepsilon)}$-approximation, when the stack is~unwound and hyperedges are selected in that order which they are on the stack.
When streaming the hyperedges in descending order of weights, we can prove that the greedy algorithm also guarantees the same approximation factor.

We now proceed with a primal/dual analysis of the integer linear program of the hypergraph matching problem.
In this ILP, we have a binary decision variable associated with each hyperedge to designate if the corresponding hyperedge is in the matching.
The objective function is to maximize the sum of the weights of the selected hyperedges.
 The constraint is that at each vertex we can select at most one hyperedge.
A linear programming relaxation is obtained by dropping the binary constraint on the hyperedge variables and the LP in Figure~\ref{fig:lp} is obtained.
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.48\linewidth}
  {\small \begin{align*}
  \mathrm{maximize}\quad & \sum_{e\in E} \omega(e)x_e\\
  \mathrm{subject\, to }\quad & \\
  \forall v \in V\colon& \sum_{e\ni v}x_e \leq 1\\
  \forall e\in E\colon& x_e\geq 0.
\end{align*}}%
\caption{LP}
\label{fig:lp}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
    {\small
        \begin{align*}
  \mathrm{minimize}\quad & \sum_{v\in V} \phi_v\\
  \mathrm{subject\, to }\quad & \\
  \forall e \in E\colon& \sum_{v\in e}\phi_v \geq \omega(e)\label{eq:term:dual}\\
  \forall v\in V\colon& \phi_v\geq 0.
\end{align*}
}
\caption{Dual LP}
\label{fig:dual}
    \end{subfigure}
    \caption{LP and Dual LP of Hypergraph Matching.}
    \label{fig:enter-label}
\end{figure}

The objective value of  the relaxation is naturally greater or equal than the integer version of the linear program. 
The dual problem of the relaxed hypergraph matching is given in Figure~\ref{fig:dual}.


Following the weak duality theorem for linear programs, 
we know that any feasible solution of the dual has an objective value greater or equal to the objective value of any feasible primal solution.
 Furthermore, the optimal value of the linear program~$\mathrm{OPT}(LP)$ is equal for both problems~(strong~duality).
 
The first step for the proof is to check that $(1+\varepsilon)\sum_{v\in e}\phi_v$ forms a valid dual solution, needed for showing the approximation factor. 

 
\subparagraph{Observation.} The~$\delta_{\mathrm{g}}$ function and Algorithm~\ref{alg:simple:streaming} generate a valid dual solution for all values of~$\varepsilon\geq 0$.
\begin{proof}

 For each hyperedge $e$ not on the stack, there was enough weight in the~$\phi_v$ values of its vertices, when~$e$ was scanned in Line~\ref{alg:simple:streaming:scan}.
 In the update for every added hyperedge to the stack, all vertices $\phi_v$ values are increased by $w(e)-\Phi_e$ such that clearly the sum of vertex $\phi_v$ values is higher than the weight of the hyperedge just added. 
 Therefore, for any hyperedge it holds~$\sum_{v\in e}\phi_v\geq \weight(e)$, satisfying the dual equation~\eqref{eq:term:dual}. 
\end{proof}


Such valid dual solution has a greater objective value then the optimum solution of the relaxed dual, and the LP duality theorem gives an upper bound for every matching, including the optimal one ~$M^\ast$ by 
 {\smallmathenv
\begin{align*}\weight(M^\ast)\leq \mathrm{OPT}(LP)\leq (1+\varepsilon)\sum_v\phi_v.\end{align*}
}
Now, we connect the changes to the dual variables with the hyperedges that have already been processed. 
 Define~
 {\smallmathenv
 \begin{align}\Delta\phi^{e} &= \sum_{v\in e}(\delta_{\mathrm{g}}(e,\phi_v,\Phi_e,\weight(e))-\phi_v)\\&= \left|e\right|(\weight(e)-\Phi_e)\label{def:delta}\end{align}
 }
 as the change to the dual~$\sum_v\phi_v$ by inspecting~$e$.
 We  give a bound for the change of the dual variable w.r.t.~the preceding hyperedges in Lemma~\ref{lemma:prev:insp}.
\begin{lemma}
  \label{lemma:prev:insp}For a hyperedge~$e$, let~$\weight'_{e}:= \weight(e)-\Phi_{e}$.
  For each hyperedge~$e\in E$ added to stack~$S$, if we denote its preceding neighboring hyperedges (including itself) by~$\mathcal{P}(e):=\{c\mid c\cap e\neq \emptyset,c \text{ added before }\, e\}\cup\{e\}$, then 
 {\smallmathenv $$\weight(e)\geq \sum_{e'\in \mathcal{P}(e)}\dfrac{1}{d}\Delta\phi^{e'} = \sum_{e'\in \mathcal{P}(e)} \weight'_{e'} .$$}
\end{lemma}
\begin{proof}
From the definition~\eqref{def:delta}, we have $\Delta\phi^e = \left| e\right| \weight'_{e}$ for~$\delta_{\mathrm{g}}$, because of line~\ref{alg:simple:streaming:update} of Algorithm~\ref{alg:simple:streaming}. 
$\Phi_e:= \sum_{v\in e}\phi_v$ is defined as the previous value of the dual variables  before inspecting~$e$. Each of these dual values $\phi_v$ consists of the sum $\sum_{c\in \mathcal{P}(e)\text{ s.t. }v\in c} \weight'_{c}$ for all preceding hyperedges. This leads to 
$\Phi_e= \sum_{v\in e}\phi_v  \geq \sum_{c\in \mathcal{P}(e)\setminus\{e\}}\frac{1}{\left|e\right|}\Delta\phi^{e'}\geq \sum_{c\in \mathcal{P}(e)\setminus\{e\}}\frac{1}{d}\Delta\phi^{c}$. 
So we can conclude \\ 
$\weight(e) = \weight'_e+ \Phi_e \geq \frac{1}{d}\Delta\phi^{e}+ \sum_{e'\in \mathcal{P}(e)\setminus\{e\}}\frac{1}{d}\Delta\phi^{e'}
$.
\end{proof}
The previous bound relates the weight of a hyperedge to the change in dual variables by its predecessors. In conclusion,
 we show that  our algorithm returns a~$d(1+\varepsilon)$-approximation.
\begin{lemma}
   Algorithm~\ref{alg:simple:streaming} with $\delta_{\mathrm{g}}$ function returns a~$\frac{1}{d(1+\varepsilon)}$-approximation. \label{lemma:dapprox}
\end{lemma}
\begin{proof}
  We now show a lower bound on the weight of any matching $M$ constructed by the algorithm.
  For any hyperedge~$e$ not in the stack we have~$\Delta\phi^e=0$, as when a hyperedge is not pushed into the stack, our algorithm does not change any dual variables.
 Furthermore,
 any hyperedge in the stack $S$ that is not included in the matching must be a previously added neighbor of a hyperedge in the matching by Lemma~\ref{lemma:prev:insp}.
  Therefore, 
  we can apply Lemma~\ref{lemma:prev:insp} and the weight can be lower-bounded by the~$\Delta\phi^e$ values. 
   The sum of changes $\sum_{e}\Delta\phi^e$ to $\phi$ is equal to the sum of dual variables $\sum_v \phi_v$ at the end.
  We have
  
 {\small \begin{align*}
  \weight(M) & = \sum_{e\in M}\weight(e){\geq} \sum_e\dfrac{1}{d}\Delta\phi^{e} & \text{(by Lemma~\ref{lemma:prev:insp})}\\
                     &\geq \dfrac{1}{d}\sum_e\Delta\phi^{e}=\dfrac{1}{d}\sum_v\phi_v\\
                     & \geq \dfrac{1}{d(1+ \varepsilon)} \weight(M^\ast) & \text{(by LP duality).}
    \end{align*}}
\end{proof}

\subsection{Improving Solution Quality.} 
\label{subsec:ils}
  Now we look into optimizing the solution quality. 
  The design space for optimizations is vast, therefore we focus on simple yet effective techniques.
  First, we explore how to ensure that the returned matching is maximal.
  Second, we propose different update functions which allow more hyperedges than $\delta_{\mathrm{g}}$ into stack, with different memory and approximation guarantees.

  
 \subparagraph{Maximal Matching Property.}
 The solutions obtained from Section~\ref{subsec:dual:problem} are only maximal with respect to the hyperedges on the stack, but not the whole hypergraph. 
 Therefore, we propose to perform a second streaming pass over the input. In this pass, we add all feasible hyperedges not introducing a conflict, independent of their dual variables.
 This will increase the solution quality, while adding only little run time overhead.
 
\subparagraph{Permissive Update Function.} 
We now propose an update function that allows more hyperedges into the stack in the hope that this will yield an improved quality.
This function is defined by
\begin{align}
  \delta_{\mathrm{permissive}}(e,\phi_v,\Phi_e,\weight(e))& := \dfrac{\weight(e)}{\left|e\right|\label{phip}},
\end{align}
and it resets each dual value on a vertex to a weight proportional to the weight of the newly accepted hyperedge on that~vertex.  
The~$\delta_{\mathrm{permissive}}$ allows taking more hyperedges into the stack than the~$\delta_{\mathrm{g}}$ function. However, it does not come with an approximation guarantee as we show in the following observation. 

  \subparagraph{Observation.}
  The ~$\delta_{\mathrm{permissive}}$ function does not always yield a valid dual solution.
  \begin{proof}
 In Figure~\ref{fig:contra} a counter-example is shown. Let the orange hyperedge ($e_1$) be scanned first; the dual variables of its vertices are set to $\frac{1}{2}$. 
 Then the \dotuline{blue} hyperedge ($e_2$) is scanned. Since it has a higher total weight than~$\frac{1}{2}$, the dual variables of its vertices are set to $\frac{1}{3}$. Now for $e_1$, we have 
 $\frac{1}{3}+\frac{1}{2}<1$, violating~\eqref{eq:term:dual}.
  \end{proof}
  \begin{figure}
   \centering
   \includegraphics[page=1]{figures/ipe/figures.pdf}
   \caption{Counter-example for the~$\delta_\mathrm{permissive}$ function giving a valid dual solution. The right hyperedge $e_1$ is not covered when the left hyperedge $e_2$ was streamed and accepted into the stack.}
   \label{fig:contra}
  \end{figure}
 

\subparagraph{Lenient Update Function.}
The $\delta_{\mathrm{g}}$ in Equation~\ref{phig} function builds a dual solution much larger than needed. For every successfully added hyperedge the difference between the current dual solution and the weight of the hyperedge is added to every vertex of the hyperedge.
 We address this by combining it with the scaling argument of the Permissive function in Equation~\ref{phip}. The resulting function is
\begin{align}
  \delta_{\mathrm{lenient}}(e,\phi_v,\Phi_e,\weight(e)):= \phi_v+(\weight(e)-\Phi_e)/\left|e\right|.
\label{phil}
\end{align}
This function produces a valid dual solution and Lemma~\ref{lemma:prev:insp} also holds.
For every previously added neighboring hyperedge $e'$ the change $\Delta\phi^{e}=\weight'_{e'}$ was distributed over all vertices of the hyperedge, so $\Phi_e\geq\sum_{e'\text{ added before}}\frac{1}{d}\Delta\phi^{e'}$. Lemma~\ref{lemma:dapprox} follows and gives us the desired approximation factor of $d(1+\varepsilon)$.
\vspace{-0.25cm}
\subsection{Space Complexity Analysis.} \label{subsec:space}
The space complexity of $\delta_{\mathrm{g}}$ and $\delta_{\mathrm{lenient}}$ can be deduced by simple counting arguments.
Let $W$ be the maximum normalized weight of a hyperedge in the hypergraph, i.e.~$W:=\frac{\max_{e\in E}{\weight(e)}}{\min_{e\in E}{\weight(e)}}$, and let $W$ be $O(\mathrm{poly}(n))$. We discuss both update functions separately.
\subparagraph{Guarantee Function.}
On every vertex we can observe up to $1+\log_{1+\varepsilon}(W)$ incrementing events, because every change in the dual variables has to be bigger by a factor of $(1+\varepsilon)$.
 This causes the stack to contain  $O(n(1+\log{W}/\varepsilon))$ vertices in its edges.
 We counted every hyperedge on its $d$ vertices, so dividing by $d$ gives the space complexity as $O((n/d) (1+(1/\varepsilon) \log{W}))$.
 Each hyperedge has at maximum $d$ vertices, each requiring $\log{n}$ bits.
 Therefore, the overall space complexity is $O((1/\varepsilon) n\log^2{n})$ bits since $W$ is $O(poly(n))$.
\subparagraph{Lenient Function.}
This function updates  $\phi_v$ for every vertex by $\frac{\weight(e)-\Phi_v}{d}$,  resulting in $1+d\cdot\log_{1+\varepsilon}(W)$ possible increases to reach the total sum of $W$.
Following the same argument, the stack contains $O( n(1+(1/ \varepsilon)\log{W}))$ hyperedges and the space complexity in bits is~$O( (1/\varepsilon) nd\log^2{n})$. Note that with this update function the algorithm is semi-streaming only if $d$ is $O(\mathrm{polylog}(n))$.

In Section~\ref{sec:tuning} we show the difference in the stack size in experiments.
\vspace{-0.25cm}

\section{Greedy Swapping 
Algorithm}\label{sec:stackless}
\begin{algorithm}[t]
  \caption{Greedy swapping streaming algorithm.}
  \label{alg:simple:stackless}
   \algsetup{linenosize=\small}
  {%\small
  \begin{algorithmic}[1]
    \PROCEDURE{SwapSet}{($H=(V,E)$, $\alpha$)}
    \STATE $\forall v \in V\colon \bestEdgeSymbol_v = \bot$ \COMMENT{Initialize best hyperedge to  empty}
    \STATE $\omega(\bot):=0$
    \FOR{$e\in E$ in arbitrary order}
    \STATE $C\gets \bigcup_{v\in e} \bestEdgeSymbol_v$
    \STATE $\Phi_e \gets \omega(C)$\label{line:upper:sum}   \COMMENT{weight of hyperedges to be removed.}
    \IF{$\omega(e)\geq(1+\alpha)\cdot\Phi_e$ 
}
    \FOR{$v\in e$}
    \IF{$\bestEdgeSymbol_v\neq \bot$}
    \FOR{$w\in \bestEdgeSymbol_v$}
    \STATE $\bestEdgeSymbol_w\gets \bot$  \COMMENT{Unmatch vertices in $\bestEdgeSymbol$.}
    \ENDFOR 
    \ENDIF
      \STATE $\bestEdgeSymbol_v \gets e$
    \ENDFOR
    \ENDIF
    \ENDFOR
    \STATE $M\gets \bigcup_{v\in V}\bestEdgeSymbol_v$
    \RETURN $M$
    \ENDPROCEDURE
  \end{algorithmic}}
\end{algorithm}
We now propose a second streaming algorithm that computes, stores, and updates a matching in the hypergraph as the edges stream. It is conceptually similar to a streaming matching algorithm for graphs designed by Feigenbaum et al.~\cite{feigenbaum2005graph}.
 The running time complexity is $O(d^2)$ per hyperedge.
 It requires constant memory, has an approximation factor that depends on d and a parameter $\alpha$ (similar to the $\varepsilon$ parameter in the primal-dual algorithm), and obtains matchings of good weight in practice.


 The proposed approach is described in Algorithm~\ref{alg:simple:stackless}.
  We store for every vertex $v$ a reference to the current matching hyperedge containing $v$; the~$\bot$ sign symbolizes that no matching hyperedge contains $v$.
  For simplifying the presentation, we 
 define~$\omega(\bot)=0$.
  When we inspect a hyperedge $e$, we compute the sum of the weights of its adjacent hyperedges that are currently in the matching.
 If the weight of the incoming hyperedge is larger than $(1+\alpha)$ times the previous conflicting hyperedges, we first remove the previous hyperedges from all their vertices.
 Afterwards we can set the reference to the new incoming hyperedge. %
The overall space consumption of this algorithm is~$O(n)$.
There are~$n$ references involved and each hyperedge of size~$d$ is holding the~$d$ vertices referencing it. 
The worst case run time complexity is $O(d^2)$, because every hyperedge can trigger a removal on every vertex, which is linear~in~$d$.


  \vspace{-0.25cm}

\subsection{Quality Guarantee.}\label{sec:stackless:quality}
In the following we show that the algorithm has a quality guarantee of $\frac{1}{(1+\alpha)(\frac{d-1}{\alpha}+d)}$ for $\alpha>0$. By substituting $d=2$ and $\alpha=1$ we recover \hbox{$\frac{1}{6}$-approximation} guarantee by  Feigenbaum~et~al.~\cite{feigenbaum2005graph} on graphs. However, the best approximation guarantee is achieved when we set $\alpha=\sqrt{(d-1)/d}$. %
Our analysis is inspired by the presentation of Feigenbaum~et~al.'s algorithm in \cite{chakrabarti2020notes}. 
\begin{lemma}
  Let $d$ be the maximum hyperedge size  of the hypergraph $H$, $M$ be the matching returned by the \textsf{Greedy Swapping Algorithm} (Algorithm~\ref{alg:simple:stackless}), and $M^\ast$ be an optimal matching in $H$. Then
  
  \begin{align*}
    \weight(M)\geq f(\alpha,d)\cdot \weight(M^\ast)
  \end{align*}
  where $f(\alpha,d)$ is a function that depends only on $\alpha$ and $d$. Thus 
  Algorithm~\ref{alg:simple:stackless} returns a $\frac{1}{f(\alpha,d)}$-approximate maximum weight matching.
\end{lemma}
\begin{proof}
  We refer to the set of hyperedges that are added to $M$ in the end of Algorithm~\ref{alg:simple:stackless} as \emph{survivors} and denote them by $S$.
  Now a hyperedge $e\in S$ replaced some set of hyperedges $C_1(e)$ (possibly empty), and this set of hyperedges may have replaced hyperedges in a set $C_2(e)$, and so on.
  We define $C_0(e)=e$.
  For $i\geq 1$, the set $C_i(e)$ consists of the collection of hyperedges that were replaced by hyperedges in $C_{i-1}(e)$.
   Note that a hyperedge $e'\in C_{i-1}(e)$ can be responsible for replacing at most $d$ hyperedges from the current matching.
   Let $T(e):=\bigcup_{i\geq 1}C_i(e)$ be all hyperedges that were directly or indirectly replaced by $e$. We will refer to $T(e)$ as the trail of replacement of $e$.
   
   We now show that for a hyperedge $e\in S$, the total weight of the hyperedges that were replaced by $e$ (i.e., the weight of $T(e)$) is at most $\weight(e)/\alpha$.
   \begin{claim}
    \label{claim:weight}
   For any $e\in S$, $\weight(T(e))\leq \frac{\weight(e)}{\alpha}$.
   \end{claim}
   \begin{proof}
    For each replacing hyperedge~$e$, $\weight(e)$ is at least $(1+\alpha)$ times the weight of replaced hyperedges, and a hyperedge has at most one replacing hyperedge. Hence, for all $i$, $\weight(C_i(e))\geq (1+\alpha)\cdot\weight(C_{i+1}(e))$. Thus 
    
    {\small\begin{alignat*}{2}
      (1+\alpha)\cdot\weight(T(e))&= \sum_{i\geq 1}(1+\alpha)\cdot\weight(C_i(e))\\&=\sum_{i\geq 0}(1+\alpha)\cdot\weight(C_{i+1}(e)) \\
                                     &\leq \sum_{i\geq 0}\weight(C_i)=\weight(T(e))+\weight(e).
    \end{alignat*}}
   Simplifying,  we obtain $\weight(T(e))\leq \frac{\weight(e)}{\alpha}$.
   \end{proof}
   Now, we consider a charging scheme to prove the Lemma. We will charge the weight of each hyperedge in $M^\ast$ to the hyperedges of $S$ and their respective trails of replacements.
   During the charging process, we will maintain an invariant: the charge assigned to any hyperedge~$e$ is at most~$(1+\alpha)\cdot\weight(e)$. 
   
   For a hyperedge $o\in S\cap M^{\ast}$, we charge $o$ to itself, which satisfies the invariant.
   Consider a hyperedge $o\in M^\ast$ that arrives in the stream but is not inserted in to $M$. 
   There are at most~$d$ conflicting hyperedges in $M$ due to which $o$ was not selected.
   We charge $\weight(o)$ to these $d$ hyperedges as follows:
   If $o$ was not chosen because of a single hyperedge~$e_1$, we assign $\weight(o)$ to $e_1$.
   For more than one hyperedge, we distribute the charge for $o$ among the hyperedges in proportion to their weights.
   Formally, for $k$ conflicting hyperedges, the charge a hyperedge $e_i$ receives is given by $\weight(o)\frac{\weight(e_i)}{\sum_{i=1}^k\weight(e_i)}$.
   Note that since $(1+\alpha)\sum_{i=1}^{k}\weight(e_i)>\weight(o)$, each of these charges  is less than $(1+\alpha)\weight(e_i)$, which satisfies the invariant.
   
   Fix a hyperedge~$e$ in the final matching $M$. Since $e$ has at most $d$ vertices, it can be charged by no more than $d$ hyperedges in an optimal solution (the so-called original charges).
   We now look at the trail of replacements.
   A hyperedge in $T(e)$ could also be charged for $d$ optimal hyperedges.
   However, for any hyperedge $t$ in $T(e)$, we can distribute some charges to $e$.
   Any charge for an optimal hyperedge $o$ that is incident at any vertex in $t\cap e$ can be directly attributed to $e$ and is included in the prior original charges.
   Therefore, the charges for any hyperedge $t$ in $T(e)$ is reduced to $d-1$.
   We now use Claim~\ref{claim:weight} to conclude 
   
   \allowdisplaybreaks
   {\small
   \begin{alignat*}{2}
    \weight(M^\ast)&\leq \sum_{e\in M}(d-1)(1+\alpha)\weight(T(e))+d(1+\alpha)\weight(e)\\
    &= (1+\alpha)\sum_{e\in M}((d-1)\weight(T(e))+d\weight(e))\\
    &\overset{C.~\ref{claim:weight}}{\leq} (1+\alpha)\sum_{e\in M}\left((d-1)\frac{\weight(e)}{\alpha}+d\weight(e)\right)\\
    &=(1+\alpha)\sum_{e\in M}\left(\left(\frac{d-1}{\alpha}+d\right)\weight(e)\right) \\
    &= (1+\alpha)\left(\frac{d-1}{\alpha}+d\right)\weight(M).
   \end{alignat*}
   }%
   
By setting $\alpha=\sqrt{(d-1)/d}$ the term $(1+\alpha)\left(\frac{d-1}{\alpha}+d\right)$ is minimized, resulting in an approximation ratio of $\frac{1}{(2d-1)+2\sqrt{d(d-1)}}$.
\end{proof}
In our experimental evaluation, we look at various values of $\alpha$: the best value of $\alpha=\sqrt{(d-1)/d}$ in terms of approximation guarantee as computed above, $\alpha=0.5$, and $\alpha=0$. 
Note that for $\alpha=0$ the algorithm has no approximation guarantee.
%\vspace{-0.25cm}
\section{Experimental Evaluation}\label{sec:experiments}
\input{figures/experiments/formatted/joined_table_memory.tex}
\input{figures/experiments/formatted/table_running_time.tex}
\subparagraph{Methodology.}
We implemented our approaches in \cpp. 
 Compilation was done using  \textsf{g}\texttt{++-}\textsf{14.2} with full optimization turned on (\texttt{-O3} flag).
We tested on two identical machines, equipped with 128 GB of main memory and a Xeon w5-3435X processor running at 3.10 GHz 
having a cache of 45 MB each.
The results are compared only if experiments are run on the same machine and compute job.
The time needed for loading the hypergraph is not measured.
 We ran each streaming algorithm with three different seeds, and the order of hyperedges was randomized accordingly.
 When streaming from disk to achieve the lowest memory usage, we read hyperedge by hyperedge and the algorithms process the hyperedges right away.
 We only consider one ordering in this case, but repeat the experiment three times.
 Up to ten experiments were run in parallel during algorithm tuning.
 For the results to be reported,  four experiments were scheduled at the same time.
 The order of experiments was randomized.
In order to compare the results we use performance profiles as suggested by Dolan and Moré~\cite{DBLP:journals/mp/DolanM02}. 
 We  plot the fraction of instances that could be solved within a factor $\tau$ of the best result per instance. 
 For minimization problems $\tau>1$, and for maximization problems $\tau<1$.
 In the plots,  the algorithm on the top left is the best performer.


\subparagraph{Instances.}
We use the hypergraph data sets  collected by Gottesbüren~et~al~\cite{DBLP:journals/corr/abs-2303-17679}.
As the tuning set we use the medium-sized~$M_{HG}$ instances.
 There are~488 instances in this set, spanning a wide range of applications from DAC routability-driven placement~\cite{dacset}, general matrices from SuiteSparse Collection~\cite{sparsecollection}, SAT solving tasks~\cite{satbenchmark}, to the ISPD98 circuit placement challenge~\cite{circuitbenchmark}.
 For the final benchmarks we use the large-sized $L_{HG}$ set consisting of~94~instances.
 These instances are also from the aforementioned backgrounds except ISPD98.
 These instances have up to~$1.4\times 10^8$ hyperedges/-vertices and a maximum hyperedge size of %
 $2.3\times 10^6$ vertices.
 More statistics can be found in Table~\ref{tab:stats} in the Appendix.

\subparagraph{Hyperdge Weights.}
We use three weight schemes in our experiments. First, we consider uniformly random weights between 1 and 100.
 Second, we sampled weights between 1 and the sum of vertex degrees in each hyperedge.
 We refer to this class as \textsc{Vertex Degree Sampled} (\textsc{vdg}) weights.
 We chose them because we expected these to be more challenging problems for the streaming approach.
 Lastly, we test uniform hyperedge weights, which equates to the cardinality case.
\subparagraph{Implemented Algorithms.}
We implemented the \textsf{Guarantee}, \textsf{Permissive}, and \textsf{Lenient} update functions from Section~\ref{sec:quality}. The \textsf{Guarantee} and \textsf{Lenient} streaming approaches can be configured by their~$\varepsilon$ parameter.
  The algorithms from Section~\ref{sec:stackless} correspond to \textsf{SwapSet}. The \textsf{SwapSet} algorithm has a parameter $\alpha\geq 0$.
  We do not tune it since its optimal value is  $\alpha=\sqrt{\frac{d-1}{d}}$, but we additionally experiment with  $\alpha=0.5$ and $\alpha=0$ (the last has no approximation guarantee).
  Variants that include a second pass are marked with \textsf{TwoPass} or \textsf{T'}.
  \textsf{SinglePass} algorithms are variants without a second pass, but we do not explicitly indicate this in the text for brevity.
 For comparison purposes, we use a simple non-streaming \textsf{Greedy} algorithm that sorts the hyperedges based on weight and greedily adds them to a matching.
We also experiment with an algorithm, called \textsf{Naive}, that adds the hyperedges without conflicts in the order that they are streamed.
For computing optimal results on small instances, we use  Gurobi~11~\cite{gurobi} to solve the ILP formulation.
\subsection{Parameter Tuning for Stack Approach.} 
\label{sec:tuning}

 
\input{figures/experiments/formatted/table_quality.tex}


We tune our stack algorithms for three scenarios: memory consumption (\textsf{Light}), running time (\textsf{Fast}) and \hbox{quality (\textsf{Strong})}.
\subparagraph{Memory Consumption.} We use the number of hyperedges $\mu$ in the stack as proxy for the memory consumption for tuning. We only consider the \textsf{Guarantee} and \textsf{Lenient} approach, since they have the strictest rules for admitting hyperedges to the stack and we can give bounds on the memory in Section~\ref{subsec:space} for them.  Table~\ref{tab:memory} shows the geometric mean of solution quality and memory consumption for values of~$\varepsilon\in \{0, 0.01, 0.1, 0.5\}$ for both algorithms.
The \textsf{Guarantee} algorithm admits fewer hyperedges into the stack, for $\varepsilon=0.5$ 26~\%(\textsc{uniform}) to 36~\% (\textsc{rnd100}) of the number of hyperedges that \textsf{Lenient} with $\varepsilon=0$ admits.
Overall, we can see that the experimental results agree well with the theoretical bounds from Section~\ref{subsec:space}. 
  For all weight types making a second pass leads to higher weights, while the memory consumption remains the same.
Therefore, we select \textsf{Guarantee}~$\varepsilon=0.5$ with a second pass as the \textsf{Light} configuration of our algorithm for all data sets. It reaches around 86\% to %
90\% of the weight on the tuning test, while adding far fewer hyperedges to the stack.


\subparagraph{Running Time of Optimized Algorithm.} 
Table~\ref{tab:tuning:running} contains the normalized geometric mean running time measured on the tuning data set.
 Passing a second time over the hyperedges costs about 42~\% more time than the original approach. 
 In the second pass, each vertex of a hyperedge is checked until a conflict is found, causing the running time to not scale by a factor of two.   
  The  \textsf{Permissive} and \textsf{Lenient} approach have a 27 \% to 29 \% overhead.
  This can be attributed to more hyperedges being added to the stack.
  The second pass here also costs an additional \~40 \% of the running time. 
  Setting~$\varepsilon=0.0$ parameter increases the running time of the \textsf{Guarantee} algorithm by 3 \% from $\varepsilon=0.5$.
  Hence we select \textsf{Guarantee} with~$\varepsilon=0.5$ and without a second pass as our \textsf{Fast} algorithm.
  \subparagraph{Quality Optimized Algorithm.}
  Table~\ref{tab:tuning:quality} shows the relative geometric mean quality for our three activation functions with and without a second pass. 
  All functions return a quality of above \numprint{0.88}, except for the \textsf{Permissive} function in a single pass on the \textsc{vdg} set with \numprint{0.84}.
  A second pass improves the geometric mean quality by 0.02 to 0.07 points.
  The \textsf{TwoPass Lenient} approach returns the best result for all weight classes and is used as the \textsf{Strong} configuration.
%\vspace{-0.25cm}
\begin{figure*}[t]
  \caption{Quality performance profiles of the approximation algorithms when compared with exact solutions obtained with Gurobi~\cite{gurobi} for three different hyperedge weight types. On the \textsc{vdg} set the \textsf{SwapSet} algorithm is better, but for others, the \textsf{Lenient ($\varepsilon=0.0$)} performs best.}
  \label{fig:tuning:quality}
  \centering
  \hfil\includegraphics[height=14pt]{figures/ilp_exact/rnd100_exact_performance_profile_0_all.legend.pdf}

  \begin{subfigure}[t]{0.28\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/ilp_exact/rnd100_exact_performance_profile_0_all.pdf}
  \caption{\centering\textsc{rnd100}}
  \end{subfigure}
  \begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/ilp_exact/vdg_exact_performance_profile_0_all.pdf}
  \caption{\centering\textsc{vdg}}
    \end{subfigure}
    \begin{subfigure}[t]{0.28\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/ilp_exact/uniform_exact_performance_profile_0_all.pdf}
  \caption{\centering\textsc{uniform}}
      \end{subfigure}
      \vspace{-0.55cm}
\end{figure*}
\subsection{Comparison with Optimal Solutions.} 
We computed the optimum solutions of the $M_{HG}$ data set with Gurobi~\cite{gurobi} configured to have a~1 hour time limit.
Of those exactly solved instances we  show a performance profile in Figure~\ref{fig:tuning:quality} for \textsf{Guarantee} and \textsf{Lenient} algorithms with $\varepsilon=0.0$ and \textsf{SwapSet} with $\alpha=\mathrm{best}$.
We also tested for $\varepsilon\in\{0.01,0.1,0.5\}$ and $\alpha\in\{0,0.5\}$, but omit these results in the plot for clarity.
 On the \textsc{vdg} set the \textsf{SwapSet ($\alpha=\text{best}$)} outperforms the other algorithms, while it returns worse quality on the \textsc{rnd100} and \textsc{uniform} set. The \textsf{Lenient ($\varepsilon=0.0$)} algorithm outperforms the other algorithms on the \textsc{uniform} set.
We now discuss further statistics for all $\varepsilon$ and $\alpha$ values.

\subparagraph{Stack Algorithms.} The choice of $\varepsilon$ clearly impacts the quality for the \textsf{Guarantee} algorithm and higher $\varepsilon$ values result in worse results on the \textsc{rnd100} set.
When comparing the geometric mean over all \textsc{rnd100} instances, \textsf{Guarantee ($\varepsilon=0.0$)} yields \numprint{83.98}~\% of the exact result, while $\varepsilon=0.5$ returns \numprint{78.83}~\%. 
For the \textsf{Lenient} algorithm the difference between varying $\varepsilon$ is less than~2~\% and it returns around 89~\% of the geometric mean weight.  
In general, the results are much better than what the performance guarantee of $\frac{1}{d(1+\varepsilon)}$ predicts.
For example, the instances in each data set with the largest relative gap to the optimum results for the \textsf{Guarantee} algorithm, i.e., \textsl{gupta3} for \textsc{uniform} and \textsc{rnd100} and \textsl{human\_gene2}, have large maximum hyperedge sizes:  $d_{\textsl{gupta3}}=14,762$ and $d_{\textsl{human\_gene2}}=14,340$. 
The worst-case ratios of $0.21$ (\textsc{rnd100}), $0.28$ (\textsc{uniform}) and $0.25$ (\textsc{vdg}) of the \textsf{Guarantee} algorithm are not nearly close to what we expect from the maximum hyperedge size of the instances in question. 
The choice of $\varepsilon$ does not change the worst-performing instance and for all $\varepsilon$-values the ratio of their solution to the optimum is the~same.
The \textsf{Lenient} algorithm, has a better worst-case ratio for \textsc{uniform} and \textsc{rnd100} and these are sorted descending by $\varepsilon$ for \textsc{uniform}, but ascending for \textsc{rnd100}.
 On the \textsc{uniform} set $\varepsilon=0.0$ leads to a worst-case ratio of~61.19 \%, while $\varepsilon=0.5$ returns 55.86\% on the \textsl{Chebyshev4} instance.
 The values for \textsc{rnd100} are~54.84~\%(\textsl{Trec14}) for $\varepsilon=0.0$ to~60.44~\% (\textsl{crystk02}) for 
 $\varepsilon=0.5$. 
  On the \textsc{vdg} set (\textsl{human\_gene2}) %
  the \textsf{Guarantee} algorithm returns only~4\% to 6~\% of the optimal solution. Interestingly, these values are not sorted by $\varepsilon$, but $\varepsilon=0.5$ returns the best and $\varepsilon=0.01$ the worst.
\subparagraph{Greedy Swapping Algorithm.}
On the \textsc{rnd100} and especially the \textsc{vdg} instances, the choice of $\alpha=0$ in \textsf{SwapSet} 
beats the other choices of $\alpha$.
It returns~89.96~\% on the \textsc{rnd100} set, while the best theoretical value $\alpha^\star=\sqrt{(d-1)/d}$ only returns 79.80~\%.
Moreover, $\alpha=0.5$ with 83.65~\% returns better results than $\alpha^\star$.
For these values of $\alpha$, the worst-case ratios are on the \textsl{gupta3} instance (\textsc{rnd100},\textsc{uniform}) or \textls{crystk02} instance (\textsc{vdg}).
The ratios of 0.60 (for $\alpha^\star$) and 0.67 (for $\alpha=0.5$) on the \textsc{vdg} data set are considerably better than that of the stack-based approaches and the approximation guarantee presented in Section~\ref{sec:stackless:quality}.

\begin{figure*}[ht!]

  \caption{Results }
    \centering
  \begin{subfigure}[t]{1\textwidth}
    \caption{Quality performance profiles for three different hyperedge weight types. The \textsf{TwoSwapSet} can outperform the non-streaming \textsf{Greedy} algorithm. On the \textsc{uniform} weighted hypergraphs,  the stack-based \textsf{Strong} algorithm performs the best.
    \textsf{Naive} is the worst performer for all datasets.}
    \setcounter{subfigure}{0}
    \renewcommand{\thesubfigure}{\roman{subfigure}}
    \label{fig:final_set:quality}
    \centering
    \hfil
    \includegraphics[height=14pt]{figures/experiments/final_experiments/rnd100_weight_performance_profile_0_all.legend.pdf}\hfill


    \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiments/final_experiments/rnd100_weight_performance_profile_0_all.pdf}
    \caption{\centering\textsc{rnd100}}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/experiments/final_experiments/vdg_weight_performance_profile_0_all.pdf}
    \caption{\centering\textsc{vdg}}
      \end{subfigure}
      \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/final_experiments/uniform_weight_performance_profile_0_all.pdf}
    \caption{\centering\textsc{uniform}}
        \end{subfigure}
  \end{subfigure}
  \setcounter{subfigure}{1}
    \renewcommand{\thesubfigure}{\alph{subfigure}}
  \begin{subfigure}[t]{1\textwidth}
    \caption{Running time performance profiles for three weight settings.
     The \textsf{Fast} configuration is the fastest non-naive approach, and it is 2.2 times slower in the geometric mean than {\textsf{Naive}}. The \textsf{SwapSet} algorithms are in turn slower by a factor of two.
    }
    \label{fig:final_set:runningtime}
    \centering
    \setcounter{subfigure}{0}
    \renewcommand{\thesubfigure}{\roman{subfigure}}
    \hfil
    \includegraphics[height=14pt]{figures/experiments/final_experiments/rnd100_runtime_performance_profile_0_all.legend.pdf}\hfill


    \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiments/final_experiments/rnd100_runtime_performance_profile_0_all.pdf}
    \caption{\centering\textsc{rnd100}}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/experiments/final_experiments/vdg_runtime_performance_profile_0_all.pdf}
    \caption{\centering\textsc{vdg}}
      \end{subfigure}
      \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/final_experiments/uniform_runtime_performance_profile_0_all.pdf}
    \caption{\centering\textsc{uniform}}
        \end{subfigure}
  \end{subfigure}
  \setcounter{subfigure}{2}
    \renewcommand{\thesubfigure}{\alph{subfigure}}
  \begin{subfigure}[t]{1\textwidth}
    \caption{Memory Performance profile when streaming from disk. \textsf{Fast} and \textsf{Light} approaches require nearly the same memory amount of 1.5 times of the \textsf{Naive} algorithm.
    The \textsf{SwapSet} family of algorithms requires more memory.
     For the \textsc{uniform} weighted hyperedges the \textsf{Strong} algorithm uses three times more memory than \textsf{Light}.}
    \label{fig:final_set:memory:disk}
    \centering
    \hfil
    \includegraphics[height=14pt]{figures/experiments/final_experiments/rnd100_disk_memory_performance_profile_0_all.legend.pdf}\hfill


    \setcounter{subfigure}{0}
    \renewcommand{\thesubfigure}{\roman{subfigure}}
    \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiments/final_experiments/rnd100_disk_memory_performance_profile_0_all.pdf}
    \caption{\centering\textsc{rnd100}}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/experiments/final_experiments/vdg_disk_memory_performance_profile_0_all.pdf}
    \caption{\centering\textsc{vdg}}
      \end{subfigure}
      \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/final_experiments/uniform_disk_memory_performance_profile_0_all.pdf}
    \caption{\centering\textsc{uniform}}
        \end{subfigure}
  \end{subfigure}
  \vspace{1.75cm}
  \end{figure*}
 
 \vspace{-0.25cm} %
\subsection{Comparing Stack Algorithms against Greedy Swapping Algorithms.}
We now compare the algorithms with stacks from Section~\ref{sec:quality} with the greedy swapping algorithms of Section~\ref{sec:stackless}. 
 In the plots we omit algorithm configurations that are very similar or non-competitive, like the \textsf{Fast} and \textsf{Light} approach for quality.
  We compare against \textsf{Naive} streaming and in-memory \textsf{Greedy} algorithms.
  Table~\ref{tab:example:instances} we report the weight, memory consumption and running time on six representative instances of the \textsc{rnd100} weight class in the streaming from disk setting. 
\subparagraph{Quality.} 
 Figure~\ref{fig:final_set:quality} shows the performance profiles for our best stack-based approach \textsf{Strong}, greedy swapping approaches \textsf{SwapSet} with $\alpha=\text{best}$,\textsf{TwoPassSwapSet} ($\alpha=0$, no performance guarantee), as well as the \textsf{Greedy} and \textsf{Naive} approaches.
 \\
 \textit{Basic Algorithms.}
 As expected, \textsf{Naive} returns the worst results on all data sets. 
 The \textsf{SwapSet} algorithms obtain better results for smaller $\alpha$ ($\alpha$=best is close to $1$ for $\sqrt{(d-1)/d}$ for large $d$)
 on the \textsc{vdg} and \textsc{rnd100} data sets.
 Moreover, the variant without performance guarantee outperforms the non-streaming \textsf{Greedy} approach on \textsc{vdg} or \textsc{rnd100} weighted hypergraphs.
 \\
\textit{Second Pass.}  The second pass boosts %
the algorithm \textsf{SwapSet} 
by up to 2~\% (\textsc{rnd100},\textsc{vdg}) and 0.5~\% (\textsc{uniform}) in the geometric mean.
The \textsf{Strong} configuration returns the best results on the \textsc{uniform} set. 
 All other approaches only reach around 87~\% in the geometric mean.
\\
\textit{Overall.}
Our \textsf{TwoPassSwapSet} algorithm can outperform the \textsf{Greedy} algorithm on \textsc{rnd100} and \textsc{vdg} data set.
When the \textsf{Greedy} approach is better, the difference is not big, as shown by the \textsl{dac2012\_'14} instance in Table~\ref{tab:example:instances} and the fact that \textsf{(T')SwapSet} reaches the best quality for all instances in $\tau= 0.8$ or better on the \textsc{vdg} and \textsc{rnd100} set.
On the \textsc{uniform} set the difference between the approaches without lenient update function is not as pronounced. 
Here the geometric mean differs only 8~\% between the second best algorithm (\textsf{T'SwapSet}) and the \textsf{Naive} algorithm, while on the other sets it is 37~\% (\textsc{rnd100}) resp. 45~\% (\textsc{vdg}).
%\vspace{-0.175cm}
\begin{tcolorbox}[width=\linewidth, colback=white!95!black,left=4pt,right=4pt,top=4pt,bottom=4pt,enlarge top by=-0pt,%
  enlarge bottom by=-1pt]%
  \textbf{Summary:}
  On \textsc{uniform} data set, %
  the stack-based \textsf{Strong} approach %
  outperforms all others.
 On weighted data sets, the \textsf{TwoPassSwapSet} algorithm performs better than the stack-methods, 
 even beating the non-streaming~\textsf{Greedy}.
 \end{tcolorbox}
%\vspace{-0.175cm}
 
 \begin{table*}
    \centering
    \caption{Results (normalized weight, running time and memory consumption) and average hyperedge size $d_m$ for 6 example instances with \textsc{rnd100} weights and streaming from disk.
    \textsf{Strong} is a stack-based algorithm. \textsf{T'SwapSet} is the greedy swapping algorithm 
    with a second pass over the input.  Best results are printed in~\textbf{bold}.}
    \label{tab:example:instances}
    {\footnotesize
    \begin{tabular}{lrrrrrr rrr r}
      \toprule
      \textsc{rnd100}& \multicolumn{3}{c}{ \bfseries Rel. Weight (best=1.00)}&\multicolumn{3}{c}{ {\bfseries Time }[s]}&\multicolumn{3}{c}{ {\bfseries Memory Consumption} [MB]}& $d_m$\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-11}
      \bfseries Instance &  \textsf{Greedy}&  \textsf{Strong} & \textsf{T'SwapSet}&   \textsf{Greedy}&  \textsf{Strong} & \textsf{T'SwapSet}& \textsf{Greedy}&  \textsf{Strong} & \textsf{T'SwapSet}\\\midrule  
      Flan\_1565& 0.73&\bfseries\numprint{1}&0.77 &0.44&\bfseries 0.21&0.36&995.11&199.91&\bfseries 47.26& 75.03\\
HV15R&   0.74&\bfseries\numprint{1}&0.94&
0.80&\bfseries 0.32&0.84&2348.29&162.41&\bfseries 54.17& 140.33\\
      dac2012\_s'14&\bfseries \numprint{1}& 0.98&0.99 
      &0.11&\bfseries 0.05&0.08&74.00&\bfseries 60.41&65.21& 3.31\\
      sat14\_sv-c'19-r'.p'& 0.94&\bfseries\numprint{1}&0.96&14.76&\bfseries 2.92&5.36&3454.93&291.10&\bfseries 123.48&3.05\\
      sk-2005 &0.99&1.00&\bfseries\numprint{1}&20.68&\bfseries 3.73&9.72&17267.19&\bfseries 634.14&751.6&44.58\\
      vas\_stokes\_4M& 0.92&\bfseries\numprint{1}&0.97&1.33&\bfseries 0.43&2.07&1257.53&265.72&\bfseries 128.49&30.03\\
     \bottomrule
    \end{tabular}
    }
   % \vspace{-0.5cm}
  \end{table*}
\subparagraph{Running Time.}  Figure~\ref{fig:final_set:runningtime} shows the running time performance profiles for all approaches.\\
\textit{Basic Algorithms.}
The \textsf{Naive} algorithm - due to its simplicity - is the fastest, being  2.08 times faster than our best proposal \textsf{Fast} (\textsc{vdg} set).
The other stack-based approaches follow by a factor of \numprint{1.28} (\textsf{Light}) and \numprint{1.40} (\textsf{Strong}).
The \textsf{SwapSet} in comparison to the \textsf{Fast} approach is slower by a factor of~\numprint{2.27}.\\
\textit{Secondary Pass.} 
 Taking a second pass prolongs the running time only slightly, for the \textsf{T'SwapSet} approach this means an additional increase by~9.7 \%.
When comparing the running times on the instances in Table~\ref{tab:example:instances}, we observe that the running time of the two-pass \textsf{T'SwapSet} is similar or a bit slower than \textsf{Greedy}, but on the biggest instance (\textsl{sk-2005}) it is~2.12 times faster.
\\
\textit{Overall.}
We observe that the stack-based approaches are faster, with the \textsf{Fast} approach being roughly two  times faster than \textsf{SwapSet} with $\alpha=0$. 
The running time of the non-streaming \textsf{Greedy} depends on the structure of the weights, on the \textsc{rnd100} set it is considerably slower than the \textsf{SwapSet} family, while on the other two datasets it is faster. The \textsf{Greedy} approach is 3.90 times slower than the \textsf{Fast} approach on the \textsc{rnd100} data set.
\begin{tcolorbox}[width=\linewidth, colback=white!95!black,left=4pt,right=4pt,top=4pt,bottom=4pt,enlarge top by=-1pt,%
  enlarge bottom by=-1pt]
  \textbf{Summary:
  }  When running time is critical, the stack-based approaches with higher values of $\varepsilon$ and not running a second pass result in fast algorithms.
 \end{tcolorbox}
%\vspace{-0.25cm}
\subparagraph{Memory Consumption.}  We now compare the memory consumption, when reading hyperedges one at a time from disk. 
Figure~\ref{fig:final_set:memory:disk} shows the  performance profile.\\
\textit{Basic Algorithms.}
The \textsf{Naive} approach has the smallest memory use.
The \textsf{Light} and \textsf{Fast} approaches take up to 10 times more memory and \numprint{1.67} resp. \numprint{1.59} times in the geometric mean. 
\textsf{SwapSet} and \textsf{T'SwapSet} require about 1.25 times the memory of \textsf{Light}.
\\
\textit{Secondary Pass.}
For the versions with secondary pass (\textsf{T'SwapSet}) we measure  a  similar amount of memory consumption.
The \textsf{Strong} approach requires \numprint{3.17} times more memory than \textsf{Fast} on the \textsc{uniform} set, because of the  more lenient admission to the stack.\\
\textit{Overall.}
The \textsf{Greedy} approach requires even more memory, relative to the most memory saving approach (\textsf{Naive}) up to 125 times and~10.85~times~on~average.
For the largest instance in our test set, \textsl{sk-2005}, \textsf{Greedy} requires 17.27 GB of memory, while our new \textsf{T'SwapSet} approach requires only around \numprint{0.751} GB.
 On the  \textsl{HV15R} instance, we report the best reduction in memory use by a factor of \numprint{43.34} (\textsf{T'SwapSet}) and \numprint{54.35} (\textsf{Light}).  
 This instance has an average hyperedge size of~140.
For high  average  $d_m$, the greedy swap set algorithm has smaller memory use, while for low values the stack-based approach requires less memory.
 In comparison to \textsf{Greedy}, our algorithms require relatively smaller memory if the hypergraph has a high average hyperedge size.
 %\vspace{-0.2cm}
 \begin{tcolorbox}[width=\linewidth, colback=white!95!black,left=4pt,right=4pt,top=2pt,bottom=2pt,enlarge top by=-1pt,%
  enlarge bottom by=-6pt]
  \textbf{Summary:}
  If the average hyperedge size is high, greedy swapping algorithms perform well, requiring less memory.
  For low average hyperedge size the stack-based approaches are better. 
  When this property is not known beforehand, we recommend using stack-based approaches with  $\varepsilon>0$ values.
 \end{tcolorbox}
\section{Conclusion}\label{sec:conclusion}
We have proposed two (semi-)streaming algorithm for the hypergraph matching.
The first, inspired by Paz-Schwartzman~\cite{10.1145/3274668}, uses a stack and admits hyperedges to it, based on dual variables it keeps track of and updates according to an update function.
The approximation guarantee for this algorithm is $1/(d(1+\varepsilon))$, and its running time per hyperedge is linear in hyperedge size.
We have proposed two other update functions to be used in this algorithm.
The proposed update functions result in a space complexity of $O((1/\varepsilon)n\log^2{n})$ and $O((1/\varepsilon)nd\log^2{n})$ bits.
The second proposed algorithm works by greedily swapping out hyperedges and maintaining only one solution, requiring only $O(n)$ memory. 
Its running time is quadratic in the current hyperedge's size.
Inspired by Feigenbaum's $\frac{1}{6}$-approximation guarantee~\cite{feigenbaum2005graph}, we have shown that if every swap increases the quality by at least $(1+\alpha)$, the algorithm has an approximation guarantee of $1/\left((1+\alpha)\left(\frac{d-1}{\alpha}+d\right)\right)$. 
The best choice for $\alpha$ is $\sqrt{(d-1)/d}$. 
 In extensive experiments, we have shown the competitiveness of the proposed algorithms in comparison to the standard non-streaming \textsf{Greedy} and a \textsf{Naive} streaming approach with respect to running time, memory consumption, and quality.
We can reduce the memory consumption on some instances up to 54.35 times when streaming from disk (relative to \textsf{Greedy}), while guaranteeing an approximation factor.
The average running time of our \textsf{Fast} approaches is 3.90 times faster %
  than the \textsf{Greedy} approach on the \textsc{rnd100} set.
The streaming \textsf{T'SwapSet} algorithm  out-performs the in-memory \textsf{Greedy} approach with respect to running time on one data set (\textsc{rnd100}); it outperforms the latter on quality and memory~consumption on all data sets.

Avenues of future work include improving the solution quality and extending  to problems with relaxed capacity constraints.
  We aim to develop a streaming algorithm that can efficiently handle instances with capacity $\scriptsize{b(v)>1}$ at each vertex, while maintaining a reasonable approximation ratio and computational~overhead.
 % \newpage
 
\section*{Acknowledgements}
We acknowledge support by DFG grant \hbox{SCHU 2567/8-1}. Moreover, we like to acknowledge Dagstuhl Seminar 24201 on discrete algorithms on modern and emerging compute infrastructure. 
Alex Pothen's research was supported by the U.S. Department of Energy grant  SC-0022260.
 \balance
\bibliography{compact}
\clearpage
\appendix

\section{Instance Statistics}\label{app:stats}
\input{figures/stats/unified.tex}
\end{document}
