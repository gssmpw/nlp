\section{Graph Reconstruction via Effective Resistance Queries}
\label{sec:reconstruction}
We give algorithms solving two graph inference problems, the first being full reconstruction of a graph from resistance distance queries given a tree decomposition, and the second being the problem of completing a graph for which the adjacency matrix is only partially known.
For the former, 
\fullornot{}{which is deferred to the full version},
we utilize the technique of reconstructing the Schur complement for a subset of vertices to obtain the overall graph. For the latter one, we rely on the property that effective resistance appears as directional derivatives of certain functions of the space of positive definite matrices.


\full{
\subsection{Graph Reconstruction from a Tree Decomposition}
Next, we show that a graph can be reconstructed quickly provides a tree decomposition of a small width.  Estimating such a tree decomposition is a more challenging task that we leave as an open question.
Our results in this section relies on properties of Schur complement of \cref{lem:schur_comp_basics}.

\begin{corollary}
    \label{lem:schur-bag-interior-reconstruct}
    Let $G = (V, E)$ be a graph, and let $({\cal B}, T)$ be a tree decomposition of $G$ of width $k \in \mathbb{N}$.  Also, let $B \subseteq {\cal B}$ be a bag of $T$ that has degree one, and let $P \subseteq {\cal B}$ be the parent of $B$.
    For all vertices $u \in B \setminus P$ we can determine their neighborhood using $k \choose 2$ effective resistance queries.
\end{corollary}
\begin{proof}
    Any vertex $u \in B \setminus P$ does not appear in any other bag of $T$, because vertices only appear in connected induced trees of $T$.  Thus, all its neighbors have to appear in $B$, because every edge of the graph must appear in at least one bag.
    It follows, by \cref{lem:schur_comp_basics} that the neighborhood of $u$ is identical in $G$ and the Schur complement $G_{B\backslash P}$.  Thus, we can discover them
    by reconstructing $G_{B\backslash P}$, with $\binom{|B|}{2} = \binom{k}{2}$ ER queries using \cref{lem:full_reconstruciton}.
\end{proof}

Using this, we can obtain the adjacency of vertices that belong only in a leaf bag.
Recursively, the vertices exclusive to that leaf bag can be eliminated via the Schur complement.
What we show is that removing this bag, as a node, from the tree decomposition, gives a valid tree decomposition for the new graph, with those vertices being eliminated.
\begin{lemma}
    \label{lem:schur-complement-bag-removal}
    Let $G=(V,E)$ be a graph with a tree decomposition $({\cal B},T)$ of treewidth $k$.
    For any bag of degree one, $B \in {\cal B}$, and its parent $P \in {\cal B}$, the subtree $T'$ induced by ${\cal B}' = {\cal B} \setminus \{B\}$ forms a valid tree decomposition of the Schur complement graph $G' = G_{V\backslash(B \setminus P)}$.
\end{lemma}
\begin{proof}
    We must show (1) every vertex of $V\backslash(B\backslash P)$ is present in a bag of ${\cal B}'$, (2) for any edge in $G'$ there is a bag in ${\cal B}'$ that contains both of its endpoints, and (3) every vertex of of $G'$ exists in a contiguous subtree of $T'$.
    Property (3) is guaranteed since we removed a leaf from $T$ to obtain $T'$, which does not disconnect any induced subtree of $T$.

    To show property (1), note the vertex set of $G'$ is exactly $V \setminus (B \setminus P)$.
    Let $v \in V \setminus (B \setminus P)$.
    If $v \notin B$ then there is a bag $B'\neq B$ such that $v \in B'$, otherwise $({\cal B}, T)$ is not a valid tree decomposition.
    Otherwise, if $v \in B$ then it must be the case that $v\in P$, since $v\in V \setminus (B \setminus P)$.  Therefore, $v$ is in a bag of ${\cal B}'$, namely $P$.

    Finally, we show property (3). For any edge $e$ in the Schur complement graph $G_{V\backslash(B \setminus P)}$, either $e\in E$ or $e$ is created during the Schur complement process. 
    If $e\in E$ then it is in a bag of ${\cal B}$, therefore, in a bag of ${\cal B}\backslash\{B\}$. 
    Otherwise, $e = (u,v)$ such that $u,v$ are in the neighborhood of $B\backslash P$, as the edge is created after reducing $B\backslash P$.
    In this case, since all the neighbors of $B\backslash P$ are in $P$, we have that $u,v\in P$, as desired.
\end{proof}
Having shown both that we can obtain the entire adjacency of `interior vertices' of a leaf bag, and that removing such bags gives a tree decomposition of the Schur complement with those vertices eliminated, the algorithm becomes clear.
\begin{theorem}
    \label{thm:reconstruction-from-tree-decomp}
    Let $G=(V,E)$ be an undirected weighted graph and suppose that a tree decomposition $({\cal B}, T)$ of $G$ is given that has width $k$.
    There exists an algorithm to reconstruct $G$ in $O(k^2n)$ ER queries.
\end{theorem}
\begin{proof}
    Our algorithm inductively reconstructs $G$ by considering its tree decomposition $T$.
    Let $B$ be a bag of $T$ with exactly one neighbor $P$ in $T$, and let $U = B\backslash P$.  We can assume $U$ is non-empty, as otherwise, we can drop $B$ and obtain a tree decomposition of $T$ with fewer bags.  We can compute all the neighbors of all the vertices in $U$ with $O(k^2)$ ER queries by \cref{lem:schur-bag-interior-reconstruct}.  Then, we recursively compute the Schur complement $G_{V\backslash U}$.  To that end, we use the fact that $T\backslash\{B\}$ is a valid tree decomposition of $G_{V\backslash U}$, which holds by \cref{lem:schur-complement-bag-removal}, and that the ER queries between two vertices of $V\backslash U$ return their effective resistance in this Schur complement (which is equal to their effective resistance in $G$).

    To finish our reconstruction, we need to build $G$ provided the neighborhood of $U$ (computed by the algorithm of \cref{lem:schur-bag-interior-reconstruct}) and the Schur complement $G_{V\backslash U}$ (computed recursively). To that end, let $L$ be the Laplacian of $G$, and observe that we know $L(U, \overline{U})$, $L(\overline{U}, U)$ and $L(U, U)$, since we know the neighbors of every vertex in $U$.  Moreover, we know $L_{\overline{U}}$ from our recursive computation. Hence, we can compute $L(\overline{U}, \overline{U})$, as by the definition of Schur complement (\cref{eqn:schur_comp}), 
    \[
    L(\overline{U}, \overline{U}) = L_{\overline{U}} + L(\overline{U},U)L(U, U)^{+}L(U, \overline{U}).
    \]
    Overall, we know all four submatrices of $L$, hence we know $L$.
\end{proof}
}

\fullornot{
\subsection{Graph Completion with Minimal Query Complexity}
Next, we consider the problem of graph completion. 
}{
\paragraph{Graph Completion.}
}
Let $G = (V, E)$ be a connected weighted graph whose adjacency matrix is partially known, with exactly $k$ unknown entries. The goal of the graph completion problem is to identify these unknown entries using the minimum number of ER queries.

\fullornot{It is straightforward to recover $G$ using $O(n^2)$ ER queries by simply ignoring the known part of the adjacency matrix and applying \fullornot{\cref{lem:full_reconstruciton}}{}. Furthermore, by leveraging the Schur complement, one can recover $G$ with $O(k^2)$ ER queries (\fullornot{\cref{thm:k-unknowns-quadratic_queries}}{See full version}).
However, we show that only $O(k)$ ER queries are sufficient to reconstruct $G$ when the edge weights are drawn from a finite set, and in particular, for unweighted graphs (\cref{thm:k-unknowns-exponential-completion}).
}{We show that only $O(k)$ ER queries are sufficient to reconstruct $G$ when the edge weights are drawn from a finite set, and in particular, for unweighted graphs (\cref{thm:k-unknowns-exponential-completion}).}
\full{
\subsubsection{Quadratically many ER queries}  To warm up, we present the following simple application of the Schur complement together with the known algorithm for fully reconstructing a graph, to recover $G$ with $O(k^2)$ ER queries.

\begin{theorem}
    \label{thm:k-unknowns-quadratic_queries}
    Let $G=(V,E, w_G)$ be an weighted graph.
    Suppose $k$ entries of the adjacency matrix of $G$ are not known. 
    There exists an algorithm to reconstruct $G$ (i.e.~to find the missing $k$ weights) with $O(k^2)$ ER queries.  (The running time of the algorithm is $\poly(k)\cdot n^2$).
\end{theorem}
\begin{proof}
Let $L$ be the Laplacian of $G$.  Let $U$ be the set of vertices that are incident to the edges with unknown weights, and note $|U|\leq 2k$.  
We use $\binom{2k}{2}$ ER queries to obtain all ER values between all pairs of vertices of $U$, and use \cref{lem:full_reconstruciton} to compute the Schur complement $L_U$.  Then, since $L(U, \overline{U})$, $L(\overline{U}, U)$ and $L(\overline{U}, \overline{U})$ are already known, we can compute $L(U,U)$ from the Schur complement formula, \cref{eqn:schur_comp},
\(
L_U = L(U, U) - L(U,\overline{U})L(\overline{U}, \overline{U})^{+}L(\overline{U}, U) \ \text{.}
\) 
Therefore, we can have all four submatrices of $L$, hence $G$ is recovered.
\end{proof}
}
\fullornot{
\subsubsection{Linearly many ER queries}
Next, }{Specifically, }we show that the effective resistance values between every pair of vertices whose corresponding adjacency matrix entry is missing are sufficient to reconstruct $G$. Specifically, we show that these ER values, together with the known adjacency values, uniquely determine the graph.

The proof of this uniqueness result is, at a high level, very similar to the idea presented in the MathOverflow post by \citet{Math-Overflow-Speyer}. However, we needed to adapt that idea to our setting with effective resistance (\cref{thm:unique-graph-completion-k-unknowns}) and establish a connection between the derivative of the function studied there and effective resistance (\cref{lem:derivitive_in_edge_lap_direction}). %
The application of this work to the context of effective resistance is not immediately obvious, as, firstly, the Laplacian is not an invertible matrix, and secondly, the construction of a function with strict convexity whose gradient is equal to a certain set of effective resistance is non-trivial.


\full{
\paragraph{Basic matrix calculus.}
We briefly introduce some background from matrix calculus that we use in this section and refer the reader to~\citet{Magnus_Neudecker_2019} for a comprehensive review.

We use $X=(x_{i,j})_{(i,j)\in[n]\times [n]}$ to denote $n\times n$ matrices of variables $x_{i,j}$.
We work with functions $f:\R^{n\times n}\to \R$ that map $n\times n$ matrices to real numbers.
For such an $f$ its \emph{derivative} is an $n\times n$ matrix, denoted %
\[
    \partial f/\partial X = (\partial f/\partial x_{i,j})_{(i,j)\in[n]\times [n]} \text{.}
\]
Moreover, the \emph{directional derivative} of $f$ in the direction of an $n\times n$ matrix $M$ is %

\begin{equation}
\label{eqn:dir_derivative}
\nabla_M f = \text{tr}((\partial f/\partial X)^T\cdot M).    
\end{equation}

A differentiable function $f:D\to\R$ for $D\subseteq \R^{m}$ is strictly concave if $D$ is convex and for any $x, y\in D$
\[
    f(x) - f(y) < (\nabla f(x))^T (y - x).
\]
See~\citet[Section 3.1.3]{book/boyd-convex-opt}.
One can show that strict concavity implies uniqueness of gradients over the domain. Hence, we have the following lemma. %
\begin{lemma}[Theorem 1 of~\citep{Griewank1991}]
    \label{fact:cvx-fn-cvx-dom-implies-different-gradient}
    Let $f:D\rightarrow \R$ be a strictly concave function on a convex domain $D\subseteq \R^{m}$.  Then
    \(
    \nabla f(x) \neq \nabla f(y)
    \)
    for any distinct $x,y\in D$.
\end{lemma}

We consider the $\log\det(\cdot)$ function defined on the set of $n\times n$ matrices.  We specifically work on the restriction of this function on the set positive definite matrices, denoted $S_n^{++} \subseteq \mathbb{R}^{n \times n}$ %
, which are invertible since all their eigenvalues are non-zero. Furthermore, the set $S_n^{++}$ is known to form a cone and, in particular, is convex. 
We use the following fact, whose proof can be found in~\citet[Section 3.5.1]{book/boyd-convex-opt}.
\begin{lemma}
    \label{lem:log-det-concave}
    The function $f : S_n^{++} \to \mathbb{R}$, defined as $f(X) = \log\det (X)$, 
    is strictly concave.
\end{lemma}
}


\paragraph{Uniqueness and reconstruction.}  
Now, we are ready to present the main result. First, we establish the uniqueness of graph completion given the ER values corresponding to the pairs of vertices with unknown entries in the adjacency matrix. %

To this end, we study the function $f = \log\det(\cdot)$ and its derivative on the space of positive definite matrices, which includes regularized Laplacians. First, we show that the directional derivatives of $f$ in certain directions correspond to the effective resistances in graphs.  In the following proof, for any $i,j\in [n]$ $E_{i,j}$ is a matrix whose entry at $(i,j)$ is one, and all its other entries are zero.  Further, $L_{i,j} = E_{i,i}+E_{j,j}-E_{i,j}-E_{j,i}$.  We refer to $L_{i,j}$ as the edge Laplacian, as it is the Laplacian of a graph with $n$ vertices and one edge $(i,j)$.

\begin{lemma}
    \label{lem:derivitive_in_edge_lap_direction}
    Let $G=([n], E_G, w_G)$ be a weighted connected graph and $\widetilde{L}_G$ its regularized Laplacian.  
    For any $i,j\in [n]$, we have,
    \(
        \nabla_{L_{i,j}} \log(\det(X))|_{X = \widetilde{L}_G} = \res(i,j),
    \)
    where $\log\det(\cdot):S_n^{++}\rightarrow \R$ is a function on all positive definite $n\times n$ matrices $X$.
\end{lemma}
\fullornot{
\begin{proof}
    We know $\partial \log( \det(X))/\partial X = (X^{-1})^T$ (see~\citet[Equation (51)]{book/matrix_cookbook}).  
    So, \full{by \cref{eqn:dir_derivative},} 
    \[
        \nabla_{L_{i,j}} \log( \det(X)) = \text{tr}((X^{-1})^T\cdot L_{i,j}) = ((X^{-1})^T)_{i,i} + ((X^{-1})^T)_{j,j} - ((X^{-1})^T)_{i,j} - ((X^{-1})^T)_{j,i}
    \]     
    Evaluated at the particular point $X = \widetilde{L}_G$, and noting that $\widetilde{L}^{-1}_G = (\widetilde{L}^{-1}_G)^T$ , we obtain 
    \[
        \nabla_{L_{i,j}} \log(\det(X))\Big |_{X = \widetilde{L}_G} =  \widetilde{L}^{-1}_{ii} + \widetilde{L}^{-1}_{jj} - 2 \widetilde{L}^{-1}_{ij} = \res(i,j), 
    \]
    where we used $\widetilde{L} = \widetilde{L}_G$ to simplify the notation.
\end{proof}
}
{
This follows clearly from the definition of the directional derivative, which is defined, in the direction of $M$, to be $\nabla_Mf:=\text{tr}((\partial f/\partial X)^TM)$, and the fact that $\partial \log \det(X) / \partial X = (X^{-1})^T$. Using the above lemma, we show that the resistances on the unknown vertex pairs of $G$, are unique for each completion of the graph.
}
\full{Next, we present our uniqueness result based on the lemma above. At a high level, we consider the space $W$ of all regularized Laplacians that are consistent with the known part of $G$. We show that the function $\log\det(\cdot)$ has distinct derivatives at every pair of points in $W$. Moreover, these derivatives are uniquely determined by the effective resistances between pairs of vertices with unknown adjacency entries.  We conclude that knowing ER values between pairs of vertices with missing adjacency values uniquely determines $G$.}
{}
\begin{theorem}
    \label{thm:unique-graph-completion-k-unknowns}
    Let $n\in\Z^+$, and let $I\sqcup J$ be a partition of $\binom{[n]}{2}$, i.e.,~all (unordered) pairs of indices of $[n]$.
    Also, let $w:I\rightarrow \R^{\geq 0}$ and $r:J\rightarrow \R^+$.
    There exists at most one %
    weighted graph $G=([n], E, w_G)$ whose edge weights on $I$ are consistent with $w$, and whose effective resistance values on $J$ are consistent with $r$, i.e.~for any $(i,j)\in I$, $w_G(i,j) = w(i, j)$ and for any $i,j\in J$, $\res_G(i,j) = r(i,j)$.
\end{theorem}
\begin{proof}
    Let $K=([n], I, w)$ be the graph that has edges between pairs in $I$ with their corresponding weights from $w$, and let $\widetilde{L}_K$ be the regularized Laplacian of $K$.
    
    We know, for any graph $G=([n], E_G, w_G)$, that
    \[
    \widetilde{L}_G = \sum_{(i,j)\in E_G}{w_G(i,j)\cdot L_{i,j}} + \frac{1}{n}\cdot J. 
    \]
    Further, if $G$ is consistent with $I$, $w$, we know that $I\subseteq E_G$, and that for any $i,j\in I$, $w_G(i,j) = w(i,j)$.  Therefore,
    \[
    \widetilde{L}_G = \sum_{(i,j)\in I}{w(i,j)\cdot L_{i,j}} + \sum_{(i,j)\in E_G\backslash I}{w_G(i,j)\cdot L_{i,j}} + \frac{1}{n}\cdot J = \widetilde{L}_K + \sum_{(i,j)\in E_G\backslash I}{w_G(i,j)\cdot L_{i,j}}. 
    \]
    Therefore, $\widetilde{L}_G$ belongs to the following affine subspace of $\R^{n\times n}$:
    \[
        W = \widetilde{L}_K + \text{span} \{ L_{i,j} \ | \ (i,j) \in J \}.
    \]
    Moreover, $\widetilde{L}_G$ belongs to the cone of positive definite matrices, $S_n^{++}$, because it is the regularized Laplacian of a connected graph.
    

    
    

    Now, let $f:\R^{n\times n}\to\R$, be $f(X) = \log\det(X)$. Let $G$ be any graph with regularized Laplacian $\widetilde{L}_G\in W$, and let $i,j\in I$.
    By \cref{lem:derivitive_in_edge_lap_direction}, we have
    \[
        \nabla_{L_{i,j}} f(\widetilde{L}_G) = \nabla_{L_{i,j}} f_{|W}(\widetilde{L}_G) = \res_{\widetilde{L}_G}{(i,j)},
    \]
    where the first equality holds as $\widetilde{L}_G + L_{i,j}$ is within $W$.

    By \fullornot{\cref{lem:log-det-concave}, $f$ is strictly concave within $S_n^{++}$}{the strict concavity of the log-determinant function~\citep[Chapter 3.1]{book/boyd-convex-opt}}, it is strictly concave %
    within $W \cap S_n^{++}$ \fullornot{(any line through $W \cap S_n^{++}$ is also a line through $S_n^{++}$, so the concavity is carried over into the intersection.)}{(an intersection of two convex sets).}

    Let $g:\mathbb{R}_{\geq 0}^k \to \mathbb{R}^{n \times n}$ be the mapping from edge weights, for pairs $(i,j) \in J$, to a regularized graph Laplacian given by $g(\vec{w}) = \sum_{(i,j) \in J}w_{i,j} L_{i,j} + \tilde{L}_K$ where $\vec{w} = (w_{i,j})_{(i,j) \in J}$.
    Now define $h(\vec{x}):=(f \circ g)(\vec{x})$, so that $\partial h / \partial \vec{x}$ is equal to the vector of all directional derivatives, $(\nabla_{L_{i,j}}f(\tilde{L}_K))_{(i,j) \in J}$.
    Its strict concavity is given by $f$ being strictly concave and $g$ being linear.
    To see that $\partial h / \partial x_{i,j} = \nabla_{L_{i,j}}f(\tilde{L}_{K})$, note that $h(\vec{0} + \varepsilon \vec{1}_{i,j}) = f(\tilde{L}_K + \varepsilon L_{i,j})$.
    Then for any two completions of the graph, $G$ and $H$, let $\vec{w}_G$ and $\vec{w}_H$ be the corresponding edge weights given by those completions of the graph.
    By \fullornot{\cref{fact:cvx-fn-cvx-dom-implies-different-gradient},}{the injectivity of the gradient of a strictly convex or concave function~\citep{Griewank1991},} $\partial h(\vec{w}_G) /\partial \vec{x}= \partial h(\vec{w}_H) / \partial \vec{x}$ implies that $\vec{w}_G = \vec{w}_H$, which means that $G$ is equal to $H$.
    Hence the completion of the graph corresponds to a unique set of effective resistances on indices in $J$.
\end{proof}


    

To recover the unknown part of $G$, we first query the effective resistance for all pairs of vertices with unknown adjacency entries. \cref{thm:unique-graph-completion-k-unknowns} ensures that these $k$ values, together with the partially known adjacency matrix of $G$, uniquely determine the graph. We then search over the set of possible weights for the unknown adjacency entries to find a configuration that is consistent with our ER queries.
\begin{theorem}
    \label{thm:k-unknowns-exponential-completion}
    Let $G=(V,E)$ be a connected weighted graph with weights from a finite known set of size $s$.
    Suppose $k$ entries of the adjacency matrix of $G$ are unknown. 
    There exists an algorithm to reconstruct $G$ (i.e., find the missing $k$ weights) with $k$ ER queries. The running time of the algorithm is $O(s^{k}n^\omega)$, where $\omega$ is the matrix multiplication exponent.
\end{theorem}
\full{
\begin{proof}
    Let $G$ be a graph and let $A_G$ be its partially known adjacency matrix with $k$ missing entries.  Let $I$ be the set of all pairs of indices $i,j\in[n]$ such that $A_G(i,j)$ is known, and let $J$ be the set of all pairs of indices $i,j\in [n]$ such that $A_G(i,j)$ is not known.
    Our algorithm spends $k$ ER queries to obtain the effective resistances for every pair of vertices in $J$.

    Then, our algorithm search for edge weights of the pairs in $J$ by exhaustively trying all $s^k$ possibilities.
    For each candidate set of $k$ weights $W$, our algorithm computes all pairs of effective resistances in the completion of $G$ with $W$, and checks if they are consistent with the $k$ ER values that are queried.  If so, our algorithm returns $W$ as the completion of $G$.  By \cref{thm:unique-graph-completion-k-unknowns}, there exists exactly one set of weights that are consistent with our ER queries, therefore, our algorithm will find them.

    There are $s^k$ possibilities for edge weights in $J$.  For each of them, it takes $O(n^\omega)$ time to compute all effective resistances. %
    Hence, the entire process takes $O(s^kn^{\omega})$ time.
\end{proof}
}
