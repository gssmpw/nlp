\section{Related work}
\paragraph{Graph inference with different query models.}
Graph inference has been studied under various models, with {edge detection} and {edge counting} being two prominent approaches motivated by applications in biology. In these models, queries allow one to check whether an induced subgraph contains any edges or to determine the number of edges in the subgraph~\citep{sicomp/AlonBKRS04, journals/jcss/AngluinC08, journals/siamdm/AlonA05, conf/wg/BouvelGK05}.~\citet{alt/ReyzinS07} provides an extensive survey of results within these models, as well as the shortest path (SP) query model. Notably, they show that reconstructing a hidden tree requires at least $\Omega(n^2)$ SP queries, establishing that the bounded-degree assumption is necessary for obtaining nontrivial results using SP queries.

\citet{journals/talg/KannanMZ18}
demonstrated that bounded-degree graphs can be fully reconstructed using $\widetilde{O}(n^{3/2})$ SP queries. They further showed that bounded-treewidth chordal graphs and outerplanar graphs can be reconstructed with $\widetilde{O}(n\log n)$ SP queries. This result has been recently generalized to bounded-treewidth graphs without long cycles by \citet{conf/iwpec/Bestide24}, with additional related work by \citet*{journals/tcs/RongLYW21}.

In contrast to the SP model, significantly fewer theoretical results are known for the {effective resistance (ER) model}. It has been established that a hidden graph can be fully reconstructed if the ER distances between all pairs of its vertices are known~\citep{journals/tcs/WittmannSBT09,Spielman2012TreesRecNotes,Hoskins2018Inferring}. On the other hand, since ER and SP distances are equivalent for trees, it follows that reconstructing a general tree requires $\Omega(n^2)$ ER queries, and hence the known $O(n^2)$ query algorithm is tight for general graphs, as is the case for the SP model. However, in contrast to the SP model, subquadratic algorithms are not known for any family of graphs beyond bounded-degree trees.

A continuous variant of the graph reconstruction problem, known as \emph{Calder\'{o}n's inverse problem}, is to recover the conductivity of an object from measurements of current and potential on its surface.  Calder\'{o}n's inverse problem has been studied extensively by mathematicians~\citep{Uhlmann2012} and found important applications in Electrical Impedance Tomography (EIT) in medical imaging~\citep{uhlmann2009electrical} and Electrical Resistivity Tomography (ERT) in geophysics~\citep{wikipediaERT}.

\paragraph{Property testing.}
We study the gap version of checking properties of graphs using ER queries. In this setting, the goal is to distinguish between graphs that possess a certain property and those that are ``far'' from having that property, using only a few ER queries. This part of our work is related to property testing of graphs in the bounded-degree model. Many properties have been studied in this model, including $k$-connectivity, bipartiteness, subgraph exclusion, minor exclusion, and planarity. We refer the reader to~\citet{books/cu/Goldreich17} for an extensive exposition of known results in this area. 
For contrast, we study property testing algorithms that rely only on resistance distance queries rather than adjacency queries. 

More broadly, our work is also related to sublinear-time algorithms for finite metric spaces. Such algorithms become particularly relevant when one is interested in estimating parameters such as the average ER distance or identifying the central point in the ER metric space. Notably, these algorithms operate without reconstructing the graph or explicitly learning its topology. There is a substantial body of work on testing properties of metric spaces; see, for example,~\cite{conf/stoc/Indyk99,journals/iandc/ParnasR03,conf/icalp/Onak08}.

\paragraph{Graph machine learning.}

One important (albeit perhaps less direct) motivation for our work is graph machine learning. Currently, there are no polynomial-time machine learning algorithms (e.g.,~graph neural networks) that can completely capture the structure of a graph; that is to say, all existing methods will give the same output on some set of non-isomorphic graphs (see~\citep{xu2018how} for an example). This means that no graph learning algorithm captures all properties of a graph.
\par 
As an imperfect solution, one approach is compute some quantities associated with the graph and use these as input to the machine learning algorithm. For example, one could compute the effective resistance between all pairs of vertices (as in~\citep{zhang2023rethinking,velingker2024affinity}) and use this as an input to a neural network. These graph quantities are sometimes called \textit{positional encodings}. Since the use of positional encodings is known not to completely capture the topology of a graph, researchers instead ask which properties of a graph these encodings do capture. For example, effective resistance increases the expressive power of message-passing neural networks~\citep{velingker2024affinity}, transformers~\citep{vaswani2017attention} using effective resistance as a positional encoding can determine which vertices are cut vertices (a property neither message-passing neural networks nor shortest-path distance can detect)~\citep{zhang2023rethinking}, and transformers using effective resistance can determine which edges are cut edges~\citep{black2024comparing}. At a high level, the goal is to understand the power of effective resistance in deducing graph properties when used as a positional encoding in a graph neural network. This question becomes particularly relevant to our work when it focuses on identifying a small subset of effective resistances that are sufficiently informative for the task at hand, thereby reducing computational costs.