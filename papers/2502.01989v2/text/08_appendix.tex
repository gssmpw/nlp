\appendix
% \setcounter{table}{0}
% \renewcommand*{\thetable}{\arabic{table}}
% \renewcommand*{\thefigure}{\arabic{figure}}
\section{Related algorithms and metric caculation}
\label{app:related_algo_metric}

\subsection{Performance-energy Consistency} In this paper, performance-energy consistency refers to the consistency between the results evaluated using an energy model and those evaluated using real-world metrics for the same sample. Specifically, the consistency requires that good samples are assigned low energy, while poor samples are assigned high energy. Performance-energy consistency measures the proportion of element pairs that maintain the same relative order in both permutations \( X \) and \( Y \), where \( X \) and \( Y \) represent the index arrays obtained by sorting the original energy values \( \mathbf{E} = (E_1, E_2, \dots, E_N) \) and performance metric values \( \mathbf{P} = (P_1, P_2, \dots, P_N) \), respectively, in ascending order. In this paper, the energy values are calculated by energy model $E_\theta(x_0)$ for samples $\x_0$. The performance metric values are calculated as the L2 distance between the generated samples $\x_0$ and the ground truth under the given condition.

Let \( X = (X_1, X_2, \dots, X_N) \) and \( Y = (Y_1, Y_2, \dots, Y_N) \) be the index arrays obtained by sorting the original energy values \( \mathbf{E} = (E_1, E_2, \dots, E_N) \) and performance metric values \( \mathbf{P} = (P_1, P_2, \dots, P_N) \), respectively, in ascending order. Specifically, \( X_i \) is the rank of the \( i \)-th sample in the sorted energy values \( \mathbf{E} \), and \( Y_i \) is the rank of the \( i \)-th sample in the sorted performance metric values \( \mathbf{P} \).

\textbf{Consistency Definition:}
The \textbf{consistency} is defined as the proportion of consistent pairs \( (i, j) \) where \( i < j \) and the relative order of \( i \) and \( j \) in \( X \) is the same as in \( Y \). Specifically:
\[
\text{Consistency} = \frac{1}{\binom{N}{2}} \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \mathbb{I}\left( (X_i < X_j \land Y_i < Y_j) \lor (X_i > X_j \land Y_i > Y_j) \right),
\]
where:
\begin{itemize}
    \item \( \binom{N}{2} = \frac{N(N-1)}{2} \) is the total number of pairs \( (i, j) \) with \( i < j \),
    \item \( \mathbb{I}[\cdot] \) is the indicator function, which evaluates to 1 if the condition inside the brackets holds (i.e., the relative order is consistent), and 0 otherwise.
\end{itemize}
\subsection{Adversarial sampling}
During the sampling process, energy optimization often gets trapped in local minima or incorrect global minima, making it difficult to escape and hindering the sampling of high-quality samples.
\subsection{Negative Sample Generation} Negative samples are generated by introducing noise into the positive sample \( x_0 \). In the Maze and Sudoku experiments, permutation noise is applied to the channel dimension to induce significant changes in the solution. Other noise types can be used, as this remains a hyperparameter choice. Specifically, we first randomly sample two scalars \( p_1 \) and \( p_2 \) from a uniform distribution in the interval \( [0, 1] \), i.e., \( p_1, p_2 \sim \text{Uniform}(0, 1) \) ($p_1<p_2$). Then, for each channel position of the positive sample \( x_0 \), we swap the channel positions with probabilities \( p_1 \) and \( p_2 \), resulting in \( x_0^{-} \) and \( x_0^{--} \), such that the L2 distance between \( x_0^{-} \) and \( x_0 \) is smaller than the L2 distance between \( x_0^{--} \) and \( x_0 \). For other noise types, such as Gaussian noise, we normalize the L2 norm of the noise and apply noise at different scales to ensure that the L2 distance from \( x_0^{-} \) to \( x_0 \) is smaller than the L2 distance from \( x_0^{--} \) to \( x_0 \).


\subsection{Linear-regression algorithm} Given three points \((x_1, y_1)\), \((x_2, y_2)\), and \((x_3, y_3)\), we wish to fit a line of the form ~\cite{lane2003introduction}:

\[
y = kx + b
\]
The mean of the \(x\)-coordinates and the mean of the \(y\)-coordinates are:
\[
\bar{x} = \frac{1}{3}(x_1 + x_2 + x_3), \quad \bar{y} = \frac{1}{3}(y_1 + y_2 + y_3)
\]
The slope \(k\) of the best-fit line is given by the formula:

\[
k = \frac{\sum_{i=1}^{3} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{3} (x_i - \bar{x})^2}
\]
This formula represents the least-squares solution for the slope.
Once the slope \(k\) is determined, the intercept \(b\) can be calculated as:
\[
b = \bar{y} - k\bar{x}
\]
The equation of the best-fit line is:
\[
\hat{y} = kx + b
\]
\section{Details of experiments}
\label{app:Exp_detail}
\subsection{Detais of Sudoku experiments}
\label{app:Exp_sudoku}
For Sudoku experiment, the dataset, model architecture, and training configurations are adopted from \citet{du2024learning}. We mainly use solving success rate to evaluate different models. Model backbone and training configurations can be found in Fig. \ref{fig:sudoku_ebm} and Table \ref{tab:sudoku_exp_detail}, respectively. All the exploration hyperparameters $c$ are set as 100 for Sudoku task.
\begin{figure}[H]
\begin{minipage}{0.9\textwidth}
\centering
\small
\begin{tabular}{c}
    \toprule
    3x3 Conv2D, 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    3x3 Conv2D, 9 \\ 
    \bottomrule
\end{tabular}
\caption{The model architecture for \proj on Sudoku task. The energy value is computed using the L2 norm of the final predicted output similar to \citet{du2023reduce}, while the output is directly used as noise prediction for the diffusion baseline.}
\label{fig:sudoku_ebm}
\end{minipage}
\end{figure}
\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Details of  training for Sudoku task}. }
    \vskip -0.15in
    \label{tab:2d_model_architecture}
    \begin{tabular}{l|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \multicolumn{2}{l}{}\\
      \hline
       \multicolumn{1}{l|}{Training configurations } & \multicolumn{1}{l}{}\\
      \hline
      Number of training steps & 100000  \\
      Training batch size & 64 \\
      Learning rate & 0.0001 \\
      Diffusion steps & 10 \\
      Inner loop optimization steps & 20 \\
      Denoising loss type & MSE \\
      Optimizer & Adam \\
        \hline
    \end{tabular}
      \label{tab:sudoku_exp_detail}
  \end{center}
\end{table}
\subsection{Details of Maze experiments}
\label{app:Exp_maze}
The details of maze experiments and model backbone are provided in Table \ref{tab:maze_exp_detail} and Fig. \ref{fig:maze_ebm}, respectively. The key metric, the maze-solving success rate is defined as the proportion of model-generated paths that have no breakpoints, do not overlap with walls, and begin and end at the start and target points, respectively. Maze datasets are generated by \citet{ivanitskiy2023configurable}, and detailed hyperparameter configurations are in Table \ref{tab:maze_exp_detail}. All the exploration hyperparameters $c$ are set as 100 for Maze task.
\begin{figure}[H]
\begin{minipage}{0.9\textwidth}
\centering
\small
\begin{tabular}{c}
    \toprule
    3x3 Conv2D, 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    Resblock 384 \\
    \midrule
    3x3 Conv2D, 9 \\ 
    \bottomrule
\end{tabular}
\caption{The model architecture for \proj on Maze task. The energy value is computed using the L2 norm of the final predicted output similar to \citet{du2023reduce}, while the output is directly used as noise prediction for the diffusion baseline.}
\label{fig:maze_ebm}
\end{minipage}
\end{figure}
\begin{table}[ht]
  \begin{center}
    \caption{\textbf{Details of Maze dataset, training}. }
    \vskip -0.15in
    \label{tab:2d_model_architecture}
    \begin{tabular}{l|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \multicolumn{2}{l}{}\\
      \hline
      \multicolumn{1}{l|}{Dataset:} & \multicolumn{1}{l}{}\\ 
      \hline
      Size of training dataset with grid size 4 & 10219   \\
      Size of training dataset with grid size 5 & 9394   \\
      Size of training dataset with grid size 6 & 10295  \\
      Minimum length of solution path & 5 \\
      Algorithm to generate the maze & DFS \\
      Size of test dataset with grid size 6 & 837   \\
      Size of test dataset with grid size 8 & 888   \\
      Size of test dataset with grid size 10 & 948   \\
      Size of test dataset with grid size 12 & 960   \\
      Size of test dataset with grid size 15 & 975   \\
      Size of test dataset with grid size 20 & 978   \\
      Size of test dataset with grid size 30 & 994   \\
      \hline
       \multicolumn{1}{l|}{Training configurations } & \multicolumn{1}{l}{}\\
      \hline
      Number of training steps & 200000  \\
      Training batch size & 64 \\
      Learning rate & 0.0001 \\
      Diffusion steps & 10 \\
      Inner loop optimization steps & 20 \\
      Denoising loss type & MSE + MAE \\
      Optimizer & Adam \\
        \hline
    \end{tabular}
      \label{tab:maze_exp_detail}
  \end{center}
\end{table}

\section{Performance sensitivity to hyperparameters}
\label{app:hyperparameters_sensitivity}

% inner loop opt steps, mcts noise scale(original model, mixed trained model hMCTS & Random search) more visualizations?
In this subsection, we analyze the impact of several hyperparameters on the experimental results. As shown in Table \ref{tab:maze_noise_scale}, the influence of different noise scales on the performance of various methods is presented. The hMCTS denoising and random search require a relatively larger noise scale to better expand the search space and improve final performance, while the diffusion model with naive inference performs best with a smaller noise scale. As demonstrated in Table \ref{tab:maze_inner_loop_opt} and Fig. \ref{fig:maze_opt_step}, the effect of varying inner-loop optimization steps on the results is also analyzed. It can be observed that performance improves gradually with an increasing number of steps, and after 5 steps, the performance stabilizes and the improvement slows down. Therefore, we chose 5 inner-loop optimization steps for the Maze experiments in this paper.
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.55\textwidth]{fig/maze_optimization_steps_vs_values.pdf}}
\caption{Visualization of success rate across different number of inner-loop optimization steps on Maze with grid size $\mathbf{15\times15}$. }
\label{fig:maze_opt_step}
\end{center}
\vskip -0.2in
\end{figure}
\begin{table}[ht]
\caption{Success rate across the different number of inner-loop optimization step on Maze with grid size \textbf{15}. }
\label{tab:maze_inner_loop_opt}
\vskip 0.15in
\begin{center}
\resizebox{0.85\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|cccccccccc}
\toprule
 &\multicolumn{10}{c}{\textbf{Number of optimization step}} \\
\cmidrule(lr){2-11} 
\textbf{Methods}                & 1               & 2               & 3               & 4               & 5               & 6               & 7               & 8               & 9 &10        \\
\midrule
T-SCEND tr. (ours), Naive inference & 0.0000 & 0.1562 & 0.2109 & 0.2734 & 0.2812 & 0.2734 & 0.2812 & 0.2969 & 0.2969 & 0.2969\\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table}
\begin{table}[ht]
\caption{Success rate across different noise scales on Maze with grid size \textbf{15}. }
\label{tab:maze_noise_scale}
\vskip 0.15in
\begin{center}
\resizebox{1\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|cccccccccc}
\toprule
 &\multicolumn{10}{c}{\textbf{Noise scale}} \\
\cmidrule(lr){2-11} 
\textbf{Methods}                & 0.1               & 0.2               & 0.3               & 0.4               & 0.5               & 0.6               & 0.7               & 0.8               & 0.9 &1.0        \\
\midrule
T-SCEND tr. (ours), hMCTS denoising (energy)               & 0.3828 & 0.4375 & 0.5312 & 0.6094 & 0.6562 & 0.6953 & 0.7031 & 0.7344 & 0.7734 & 0.7969 \\
T-SCEND tr. (ours), naive inference                    & 0.3125 & 0.2656 & 0.2578 & 0.2344 & 0.2422 & 0.2656 & 0.2578 & 0.2422 & 0.2500 & 0.2500 \\
T-SCEND tr. (ours), Random search(energy)      & 0.3906 & 0.4453 & 0.5312 & 0.5703 & 0.5938 & 0.6328 & 0.6641 & 0.6719 & 0.6797 & 0.6562 \\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table}
\section{Additional results}
\label{app:additional_results}
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\textwidth]{fig/maze_success_rate_vs_ts.pdf}}
\caption{Visualization of Success rate across different MCTS start step $t_s$. }
\label{fig:maze_success_rate_vs_ts}
\end{center}
\vskip -0.2in
\end{figure}
The parameter \( t_s \) controls the proportion of the total inference budget allocated to MCTS denoising. When \( t_s = 9 \), it means only MCTS denoising is used, while \( t_s = 0 \) means only best-of-N random search is employed. For \( 0 < t_s < 9 \), hMCTS denoising is applied. As shown in Table \ref{tab:maze_mcts_start_step} and Fig. \ref{fig:maze_success_rate_vs_ts}, there is a noticeable peak in model performance as \( t_s \) varies.
\begin{table}[h!]
\caption{Success rate of hMCTS denoising on Maze with grid size \textbf{15} across different MCTS start steps. }
\label{tab:maze_mcts_start_step}
\vskip 0.15in
\begin{center}
\resizebox{\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|cccccccccc}
\toprule
\cmidrule(lr){2-11} 
\textbf{Methods} & 0               & 1               & 2               & 3               & 4               & 5               & 6               & 7               & 8               & 9         \\
\midrule
Original, hMCTS denoising (energy)      & 0.0781 & 0.0703 & 0.0859& 0.0781 & 0.1250& 0.1484& 0.1250 & 0.0781 & 0.0625 & 0.0703\\
T-SCEND tr. (ours), hMCTS denoising (energy)   & 0.6562 & 0.6094& 0.6641 & 0.7969 & 0.7969 & 0.6406& 0.4922 & 0.4922 & 0.4609 & 0.4453 \\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h!]
\caption{Success rate of Random search for different training methods on Maze with grid size \textbf{15} and Sudoku harder dataset guided with ground truth accuracy. Untrained, Random search (gt) represents use ground truth to guide the random search.  Here, $L=N$. Bold font denotes the best model. }
\label{tab:maze_diffus_baseline_diversity}
\vskip 0.15in
\begin{center}
\resizebox{\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|cccccc|ccccccc}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{6}{c}{\textbf{Maze success rate}} & \multicolumn{7}{c}{\textbf{Sudoku success rate}}\\ 
\cmidrule(lr){2-14} 
\textbf{Methods} & \textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} &\textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=321} \\
\midrule
Untrained, Random search (gt) & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\ 
Original, Random search (gt) & 0.0625 & 0.1250 & 0.1094 & 0.1328 & 0.1719 & 0.1719 & 0.0859 & 0.1641 & 0.2188 & 0.2344 & 0.2422 & 0.2656 & 0.2969 \\
DDPM, Random search (gt) & 0.0312&0.1094&0.1587&0.1746&0.2031&0.2422& 0.0000          & 0.0000          & 0.0000          & 0.0000          & 0.0000          & 0.0000          & 0.0156 \\
T-SCEND tr. w/o LRNCL, Random search (gt) & \textbf{0.2500} & \textbf{0.5078} & \textbf{0.5938} & \textbf{0.6562} & \textbf{0.7109} & \textbf{0.7422} & \textbf{0.1094} & \textbf{0.2578} & \textbf{0.2969} & \textbf{0.3438} & \textbf{0.3750} & \textbf{0.3828} & \textbf{0.4219} \\ 
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table}
\begin{table}[h!]
\caption{Success rate and element-wise accuracy of Random search for different training methods on Sudoku harder dataset guided with ground truth accuracy. Here, $L=N$. Bold font denotes the best model. }
\label{tab:sudoku_diffus_baseline_ddpm}
\vskip 0.15in
\begin{center}
\resizebox{1\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|ccccccc|ccccccc}
\toprule
& \multicolumn{7}{c|}{\textbf{Success rate}} & \multicolumn{7}{c}{\textbf{Element-wise} accuracy}\\
\cmidrule(lr){2-15} 
Methods &\textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=321} &
\textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=321} \\
\midrule
DDPM, Random search, GT accuracy guided     & 0.0000          & 0.0000          & 0.0000          & 0.0000          & 0.0000          & 0.0000          & 0.0156          & 0.5071          & 0.6089          & 0.6316          & 0.6492          & 0.6691          & 0.6881          & 0.6999          \\
Original, Random search, GT accuracy guided & 0.0781          & 0.1641          & 0.2188          & 0.2344          & 0.2422          & 0.2656          & 0.2812          & \textbf{0.6650} & 0.7731          & 0.7952          & 0.8036          & 0.8217          & 0.8347          & 0.8491          \\
T-SCEND tr. w/o LRNCL, Random search, GT accuracy guided            & \textbf{0.1094} & \textbf{0.2578} & \textbf{0.2969} & \textbf{0.3438} & \textbf{0.3750} & \textbf{0.3828} & \textbf{0.4219} & 0.6442 & \textbf{0.7855} & \textbf{0.8096} & \textbf{0.8317} & \textbf{0.8466} & \textbf{0.8628} & \textbf{0.8854} \\ 
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.1in
\end{table}





% \begin{table}[]
% \caption{Success rate of mixed inference on Maze with grid size \textbf{15} across different MCTS start steps. Bold font denotes the best model.}
% \label{tab:maze_mcts_start_step}
% \begin{tabular}{lllllllllll}
% \cline{1-1}
% \multicolumn{1}{|l|}{Method} & \multicolumn{10}{c}{MCTS start step}                                                                                                                                              \\
% \multicolumn{1}{|l|}{}                        & 0               & 1               & 2               & 3               & 4               & 5               & 6               & 7               & 8               & 9               \\ \cline{1-1}
% Mixed inference, KL \& LRNCL ,Energy guided   & 0.6562 ± 0.4750 & 0.6094 ± 0.4879 & 0.6641 ± 0.4723 & 0.7969 ± 0.4023 & 0.7969 ± 0.4023 & 0.6406 ± 0.4798 & 0.4922 ± 0.4999 & 0.4922 ± 0.4999 & 0.4609 ± 0.4985 & 0.4453 ± 0.4970 \\
% Mixed inference, Original, Energy guided      & 0.0781 ± 0.2684 & 0.0703 ± 0.2557 & 0.0859 ± 0.2803 & 0.0781 ± 0.2684 & 0.1250 ± 0.3307 & 0.1484 ± 0.3555 & 0.1250 ± 0.3307 & 0.0781 ± 0.2684 & 0.0625 ± 0.2421 & 0.0703 ± 0.2557
% \end{tabular}
% \end{table}
\section{Limitations and future work}
\label{app:limit_future} 
Our inference framework primarily relies on MCTS, which presents two key limitations: (1) limited compatibility with parallel computing, and (2) challenges in effectively evaluating node quality during the early stages of denoising. Future work could explore integrating alternative search strategies, such as those proposed by \citet{wu2024inference}. Additionally, to enhance performance-energy consistency, we introduce linear-regression negative contrastive learning, which enforces a linear relationship between energy and the distance to real samples. Further investigation is needed to assess the broader implications of this constraint and explore alternative regularization approaches. Lastly, while our current implementation utilizes Gaussian noise for branching, other diffusion-based branching mechanisms remain an open area for exploration.
\section{Visualization of results}
\label{app:vis_results}
\subsection{Visualization of Maze experiments}
\label{app:maze_vis}
This section presents visualizations of the training in Fig. \ref{fig:maze_training_vis}, test Maze data in Fig. \ref{fig:maze_test_vis}, and samples generated by different methods in Fig. \ref{fig:maze_samples_diff}. In the visuals, black pixels denote walls, green represents the starting point, red represents the goal point, blue marks the solved path, and white represents the feasible area. All visualizations are based on a few representative samples. The results from the training and test sets clearly show that the tasks in the test set are notably more challenging than those in the training set. Visual comparisons of samples generated by different methods reveal that the originally trained model, regardless of the inference strategy, performs consistently worse than \proj.
\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{fig/maze_plot_multi_grid_size_appendix_train.pdf}}
\caption{Visualization of training maze dataset. }
\label{fig:maze_training_vis}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{fig/maze_plot_multi_grid_size_appendix_test.pdf}}
\caption{Visualization of test maze dataset, where the blue paths are ground-truth solutions.}
\label{fig:maze_test_vis}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{fig/maze_plot_diff.pdf}}
\caption{Visualization of samples generated by different training and inference methods.}
\label{fig:maze_samples_diff}
\end{center}
\vskip -0.2in
\end{figure}
\subsection{Visualization of Sudoku experiments}
\label{app:sudoku_vis}

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\textwidth]{fig/sudoku_train_test_samples.pdf}}
\caption{Visualization of training and test Sudoku dataset.}
\label{fig:sudoku_training_test_vis}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure}[ht]

\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\textwidth]{fig/sudoku_plot_diff.pdf}}
\caption{Visualization of samples generated by different training and inference methods.}
\label{fig:sudoku_samples_diff}
\end{center}
\vskip -0.2in
\end{figure}

This section presents visualizations of the training and test Sudoku data in Fig.~\ref{fig:sudoku_training_test_vis}, and representative samples generated by different methods in Fig.~\ref{fig:sudoku_samples_diff}. In the
visuals, black numbers denote the condition, green numbers represent correct predictions, and red numbers represent wrong predictions. All visualizations are derived from a few representative samples. The comparison between the training and test sets clearly indicates that the tasks in the test set are significantly more difficult than those in the training set. When comparing the samples generated by different methods, it is evident that the originally trained model, regardless of the inference strategy, consistently underperforms compared to \proj.

% training dataset, landscape visualization, solution of different models
% \section{Analysis of failure case}
% \tao{TODO}
% \jiashu{TODO}
% \label{app:failure_analysis} 
% training dataset, test dataset, solutions of different models