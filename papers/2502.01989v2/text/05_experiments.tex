\label{sec:exp}
\begin{table*}[htbp]
\vskip -0.1in
\caption{Success rate across different grid sizes of Maze and various number of given entries for na\"ive inference ($N=1$) for comparison of the generalization ability of models obtained by different training methods. Let $M$ denotes the grid size of Maze and $D$ denotes the number of given digits in Sudoku. Original denotes the original training method in \citet{du2024learning}. T-SCEND tr. (ours) denotes the training method of \proj, and all subsequent descriptions will follow this definition. Bold font denotes the best model, and underline denotes the second best model. The same markings are used in the tables below. }
\label{tab:maze_success_rate_generalization}
\vskip -0.05in
\begin{center}
\resizebox{0.9\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|ccccc|ccccc}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{5}{c|}{\textbf{Maze success rate }} &\multicolumn{5}{c}{\textbf{Sudoku success rate }}\\
\cmidrule(lr){2-11} 
\textbf{Methods} & $M=6$ &$M=8$&$M=10$&$M=12$&$M=15$ &$D=33$&$D=29$&$D=25$&$D=21$&$D=17$ \\
\midrule
Original  & 1.0000 & 0.9062 & 0.5781 & 0.3750 & 0.0625 & 0.3203 & 0.1094 & 0.0234 & 0.0000 & 0.0000 \\
T-SCEND tr. w/o LRNCL              & 1.0000 & \underline{0.9922} & \underline{0.7734} & \underline{0.5625} & 0.2500 & \textbf{0.4219} & \underline{0.1719} & \textbf{0.0469} & \textbf{0.0078} & 0.0000 \\
T-SCEND tr. w/o KL           & 1.0000 & 0.9844 & 0.6953 & 0.4375 & \underline{0.2812} & \textbf{0.4219} & \textbf{0.2578} & \underline{0.0391} & \textbf{0.0078} & 0.0000 \\
T-SCEND tr. (ours)      & 1.0000 & \textbf{1.0000} & \textbf{0.9922} & \textbf{0.9141} & \textbf{0.6562} & 0.1953 & 0.1016 & 0.0078 & 0.0000 & 0.0000 \\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.2in
\end{table*}
In the experiments, we aim to answer the following questions: (1) What factors contribute to the failure of the direct scaling-up diffusion model? (2) Does the training of \proj unleash the test-time scalability of diffusion model? (3) Can the inference method of \proj effectively achieve scaling up during test time? To answer these questions, we conduct extensive experiments on two challenging reasoning tasks: Maze and Sudoku. In Section ~\ref{sec:failure_analysis}, we find that the na\"ively trained diffusion model falls short of scaling up during test time and identify the key reasons as insufficient \emph{performance-energy consistency} and \emph{adversarial sampling}. To address these challenges, the training methods of \proj demonstrate a significant impact on unbinding the scalability of the diffusion model in Section \ref{sec:training_improve}. As shown in  Fig \ref{fig:maze_train_hmcts}, with the model trained with \proj training methods, hMCTS denoising can better release the scalability enabling generalization to much more challenging tasks during inference (Sec. \ref{sec:scale_up}).

In Maze experiments, the datasets are generated by \citet{ivanitskiy2023configurable}, and the diffusion model is mainly trained with Maze sizes of up to $6\times6$ while tested on harder datasets with size significantly larger than $6\times6$ (Fig. \ref{fig:maze_train_hmcts}). For Sudoku experiments, the basic setting is adopted from \citet{du2024learning} where the diffusion model is trained on SAT-Net dataset with 31 to 42 given entries \citep{wang2019satnet} and tested on harder RRN dataset with 17 to 34 given entries \citep{palm2018recurrent} with fewer given entries. All models and inference methods are evaluated with solving success rate. For Maze, successful solving means finding a continuous path from starting location to target location without any breaking or overlapping with the wall; for Sudoku, successful solving means filling all the missing numbers that \emph{exactly} match the ground-truth, both of which are very stringent metrics. More experimental details, hyperparameter analysis, and result visualizations can be found in Appendix \ref{app:Exp_detail}, Appendix \ref{app:hyperparameters_sensitivity}, and Appendix \ref{app:vis_results}, respectively.
\begin{table*}[h!]
\caption{Success rate on Maze with grid size \textbf{15} and Sudoku harder dataset for comparison of the modelâ€™s ability to scale up under random search with different training methods. Here, $N_r=N, K=N, L=N$. }
\vskip -0.18in
\label{tab:random_search_eval_train}
\begin{center}
\resizebox{1\textwidth}{!}{ % Resize the table to fit within the column width
\begin{tabular}{l|cccccc|ccccccc}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{6}{c}{\textbf{Maze success rate}} & \multicolumn{7}{c}{\textbf{Sudoku success rate}} \\ 
\cmidrule(lr){2-14}
\textbf{Methods} & \textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=321} \\
\midrule
Original, Random search & 0.0625 & 0.0469 & 0.0547 & 0.0781 & 0.1016 & 0.1094 & 0.0859 & 0.1641 & 0.2188 & 0.2344 & 0.2422 & 0.2656 & 0.2812 \\ 
T-SCEND tr. w/o LRNCL, Random search & 0.2500 & 0.4297 & \underline{0.5000} & 0.5391 & \underline{0.6016} & 0.6094 & \underline{0.1172} & \textbf{0.2656} & \underline{0.3125} & \textbf{0.3438} & \textbf{0.3750} & \textbf{0.3906} & \textbf{0.4141} \\ 
T-SCEND tr. w/o KL, Random search & \textbf{0.2812} & \underline{0.4688} & 0.4922 & \underline{0.5547} & 0.5859 & \underline{0.6250} & \textbf{0.1562} & 0.2422 & 0.2578 & 0.2656 & 0.2656 & 0.2891 & 0.3047 \\ 
T-SCEND tr. (ours), Random search & \underline{0.2656} & \textbf{0.5859} & \textbf{0.6406} & \textbf{0.6875} & \textbf{0.6562} & \textbf{0.7031} & 0.0703 & \underline{0.2500} & \textbf{0.3281} & \textbf{0.3438} & \underline{0.3516} & \underline{0.3594} & \underline{0.3906} \\ 
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.2in
\end{table*}
All the training methods are compared with the original training pipeline in \citet{du2024learning}. LRNCL and KL  represent the loss terms $\l_\text{LRNCL}$ and $\l_\text{KL}$, respectively in Eq. \ref{eq:full_objective}.  T-SCEND tr. w/o LRNCL, T-SCEND tr. w/o KL, and T-SCEND tr. (ours) represent the three training methods of \proj, the last one being the full version.  The MCTS denoising and hMCTS denoising are primarily compared with the best-of-N random search with the same computational budget.


\subsection{Why originally trained model fail to scale up during test time?}
\label{sec:failure_analysis}

\begin{table*}[h!]
\caption{\emph{Performance-energy consistency} of random search on Maze with grid size 15 and Sudoku harder dataset to test the effect of LRNCL loss. Here, $L=N$. Details of consistency calculation can be found in Appendix \ref{app:related_algo_metric}. }
\label{tab:diffus_baseline_consistency}
\vskip 0.05in
\begin{center}
\resizebox{0.9\textwidth}{!}{ % Resize the table to fit within the column width
\begin{tabular}{l|ccccc|ccccc}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{5}{c}{\textbf{Maze performance-energy consistency}} & \multicolumn{5}{c}{\textbf{Sudoku Performance-energy consistency}} \\ 
\cmidrule(lr){2-11}
\textbf{Methods} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} \\
\midrule
Original, Random search & 0.3094 & 0.2916 & 0.3129 & 0.3143 & 0.3103 & 0.3955 & 0.3930 & 0.4010 & 0.4010 & 0.3986 \\ 
T-SCEND tr. w/o KL, Random search & \textbf{0.5375} & \textbf{0.4316} & \textbf{0.4902} & \textbf{0.5006} & \textbf{0.4998} & \textbf{0.4574} & \textbf{0.4528} & \textbf{0.4457} & \textbf{0.4459} & \textbf{0.4470} \\ 
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.23in
\end{table*}
To investigate why diffusion models trained via the original method fail on more complex tasks, even with increased inference budgets yielding minimal improvement, we evaluate models trained on simpler datasets against harder tasks. As shown in Table \ref{tab:maze_success_rate_generalization}, models trained on simpler tasks with original training methods exhibit significantly poorer performance on more complex tasks. For the Maze task, models trained on maze sizes of $4 \times 4$, $5 \times 5$, and $6 \times 6$ demonstrate a rapid decline in success rate as the maze size increases. Specifically, the success rate decreases from 100\% at $6 \times 6$ to 6\% at $15 \times 15$. A similar trend is observed in the Sudoku task: models trained on datasets with 31 to 42 given entries fail to achieve high success rates when tested on datasets with 17 to 34 given entries, with the success rate dropping to below 5\% when the number of entries is reduced to 25, and complete failure when only 21 entries are provided.

Building upon this, we conduct best-of-N random search to examine whether test-time scaling can enhance the performance of the originally trained model, as described in Section \ref{sec:inference_method}, and the results are presented in Table \ref{tab:random_search_eval_train} and Table \ref{tab:diffus_baseline_consistency}. In Table \ref{tab:random_search_eval_train}, it is evident that, for the originally trained model, increasing the inference budget $N$ in the Maze experiment only yields a modest improvement, with the success rate increasing by at most 5\%, reaching a peak of 10\%. In the Sudoku experiment, even with an increased budget of $N = 320$, the success rate remains below 30\%. The results in Table \ref{tab:diffus_baseline_consistency} further reveal that the energy model predictions of the originally trained model exhibit substantial misalignment with true performance metrics, with many good samples assigned higher energy values. In the Maze experiment, the consistency between the energy model and true performance is only 30\% while the consistency in Sudoku task is around 40\%. These results indicate that the originally trained model suffers from insufficient \emph{performance-energy consistency}. Additionally, as seen in Table \ref{tab:maze_diffus_baseline_diversity}, even when ground truth is used to evaluate multiple candidates generated via random search with the originally trained model, the model's performance remains unsatisfactory, with the Maze success rate not exceeding 20\% and the Sudoku success rate below 30\%. This result demonstrates that the originally trained model is affected by \emph{adversarial sampling}. Even with an increased computation budget \( N \), the generated samples remain suboptimal or correspond to incorrect global optima, hindering the acquisition of high-quality samples that align with the correct global optimum.
\begin{figure*}[h!]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\textwidth]{fig/success_rate_scaling_log.pdf}}
\vskip -0.2in
\caption{Scalability of different approaches on Maze and Sudoku. }
\label{fig:maze_scaling}
\end{center}
\vskip -0.4in
\end{figure*}
\begin{table*}[htbp]
\caption{Success rate of different approaches on Maze with grid size \textbf{15} and Sudoku harder dataset. Here, $N_r=N, K=N, L=N$.}
\label{tab:maze_scale_up}
\begin{center}
% \vskip -0.02in
\resizebox{1\textwidth}{!}{ % Resize the table to fit within a single column
\begin{tabular}{l|cccccc|cccccccc}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{6}{c}{\textbf{Maze success rate}} & \multicolumn{7}{c}{\textbf{Sudoku success rate}}\\ 
\cmidrule(lr){2-14} 
\textbf{Methods} &\textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} &\textbf{$N$=1} & \textbf{$N$=11} & \textbf{$N$=21} & \textbf{$N$=41} & \textbf{$N$=81} & \textbf{$N$=161} & \textbf{$N$=321} \\
\midrule
Original, Random search & 0.0625 & 0.0469 & 0.0547 & 0.0781 & 0.1016 & 0.1094 & \textbf{0.0859} & 0.1641 & 0.2188 & 0.2344 & 0.2422 & 0.2656 & 0.2812 \\ 
Original, hMCTS denoising (ours) & 0.0625 & 0.0938 & 0.1016 & 0.1250 & 0.1328 & 0.1250 & \textbf{0.0859} & 0.1641 & 0.1875 & 0.2266 & 0.2578 & 0.2734 & 0.3047 \\ 
T-SCEND tr. (ours), MCTS denoising (ours) & 0.2656 & 0.5859 & \underline{0.6641} & \underline{0.7344} & \underline{0.6875} & 0.7031 & \underline{0.0703} & 0.0859 & 0.0938 & 0.0859  & 0.1094 & 0.1250 & 0.1328 \\ 
T-SCEND tr. (ours), Random search & 0.2656 & 0.5859 & 0.6406 & 0.6875 & 0.6562 & 0.7031 & \underline{0.0703} & \underline{0.2500
} & \textbf{0.3281} & \underline{0.3438}& \underline{0.3516} & \underline{0.3594} & \underline{0.3906} \\ 
T-SCEND tr. (ours), hMCTS denoising (ours) & \textbf{0.2656} & \textbf{0.6406} & \textbf{0.7266} & \textbf{0.7969} & \textbf{0.8203} & \textbf{0.8828} & \underline{0.0703} & \textbf{0.2891} & \underline{0.3203} & \textbf{0.3516} & \textbf{0.3672} & \textbf{0.4062} & \textbf{0.4297} \\ 
\bottomrule
\end{tabular}
} 
\end{center}
\vskip -0.2in
\end{table*}
\subsection{The effect of \proj training methods}
\label{sec:training_improve}
To enhance the \emph{performance-energy consistency} and reduce \emph{adversarial sampling}, as detailed in \ref{sec:training_method}, we augmented the originally trained method with additional LRNCL loss and KL loss. As shown in Table \ref{tab:diffus_baseline_consistency}, incorporating LRNCL loss into the training process significantly improves the \emph{performance-energy consistency} compared to the original model, with a greater than 20\% improvement in the Maze task and a 5\% improvement in the Sudoku task. In Table \ref{tab:random_search_eval_train}, for the Maze task, even without any additional computation (\emph{i.e.}, $N = 1$), models trained with KL loss achieve a 20\% increase in solving success rate compared to the original model. Models trained with LRNCL loss show an improvement of over 20\%. Augmented with random search, as the budget $N$ increases, the improvement for all three training methods becomes more pronounced. Models trained with KL loss show a maximum success rate increase of approximately 50\%, those with LRNCL loss exceed a 50\% improvement, and the combination of both KL loss and LRNCL loss results in a maximum increase of about 60\%. In the Sudoku task, using the naive inference method with $N = 1$, both KL loss and LRNCL loss yield significant improvements. For Random Search, as $N$ increases, models trained with all three \proj methods surpass the original model at $N = 21$, with the gap widening further as the budget grows, showing a maximum increase of over 10\%. Furthermore, Table \ref{tab:maze_diffus_baseline_diversity} shows that the \proj-trained energy-based diffusion model outperforms DDPM \cite{ho2020denoising} under the same architecture. These results demonstrate that the training methods of \proj can effectively improve \emph{performance-energy consistency} and reduce \emph{adversarial sampling} to unlock the test-time scalability of diffusion models.
\subsection{Test-time Scalability of \proj}
\label{sec:scale_up}
Using the diffusion model trained with \proj training methods, we evaluate various inference approaches on the Maze task with a grid size of $15\times15$ and on a more challenging Sudoku dataset. As shown in Table \ref{tab:maze_scale_up}, models trained with additional LRNCL loss and KL loss of \proj demonstrate that, in the Maze experiment, the MCTS denoising method slightly outperforms random search, while our hMCTS denoising yields a significantly higher success rate, with a maximum improvement of approximately 18\% than random search. Moreover, as the budget $N$ increases, the performance gap between hMCTS denoising and random search widens. In the Sudoku experiment, hMCTS denoising also consistently outperforms random search, with a maximum improvement of 5\%. As illustrated in the \emph{scaling curve} in Fig. \ref{fig:maze_scaling}, hMCTS denoising with the model trained with additional LRNCL loss and KL loss shows a marked improvement in solving success rate compared to both other inference methods and hMCTS denoising with the originally trained model. At the same time, the rate of improvement in solving success rate for other inference methods clearly slows down compared to hMCTS denoising. These results provide strong evidence that our inference method can effectively scale up during test time, offering a clear advantage over random search.