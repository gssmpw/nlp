In recent years, diffusion generative models \cite{sohl2015deep,ho2020denoising,song2021scorebased} have demonstrated remarkable performance across a wide range of applications, including image \cite{stablediffusion} and video generation \cite{sora}, robotic control \cite{diffuser}, and reasoning \cite{du2024learning}. By learning how to denoise from noisy, corrupted samples to clean data samples, diffusion models can generate high-quality, high-dimensional samples approximating the data distribution.
\begin{figure}[ht]
\label{fig:maze_train_hmcts}
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{fig/maze_plot_train_hmcts.pdf}}
\caption{Visualizations of Maze training data and solutions generated by hMCTS denoising of our \proj framework.}
\end{center}
\vskip -0.45in
\end{figure}

Nevertheless, as the application scenarios for diffusion models become more complex, they often encounter situations where the problem sizes are much larger than those in training or their conditional input is out-of-distribution (OOD). When dealing with larger problem sizes, the computational resources required may increase exponentially, and the existing models may struggle to maintain high-quality performance. In OOD scenarios, the diffusion models need to generalize to conditions that have characteristics different from what they have been trained on. Thus, the pressing question is how to further enhance the performance of diffusion models under such challenging circumstances.

Interestingly, test-time scaling has demonstrated its effectiveness in other generative AI methods such as large language models \cite{wei2022chain,yao2024tree}.
% . By scaling up test-time\footnote compute through explicit reasoning and search via chain-of-thought \cite{} or tree-of-thought \cite{}, LLMs can achieve significantly better performance. 
It remains an interesting question whether test-time\footnote{In this paper, we use ``test-time'' and ``inference'' interchangeably.} scaling can also benefit diffusion models and address the challenges above.

However, na\"ive test-time scaling up for diffusion models by increasing denoising steps typically yields marginal gain after a certain number of function evaluations (NFEs), which was observed in prior works \cite{karras2022elucidating,song2020denoising,song2021scorebased}. Another scaling-up dimension is the number of samples, while in our pivot study, best-of-N sampling with diffusion model still doesn't scale up significantly. Through analysis, we identify that the reason comes from both training and inference. For \emph{training}, the model's performance is fundamentally upper-limited by the learned energy landscape\footnote{We can regard diffusion model's denoising network $\boldsymbol\epsilon_\theta(\boldsymbol x_t,t)$ as approximating the gradient of an implicit energy function $\nabla_{\boldsymbol x_t}E_\theta(\boldsymbol x_t,t)$. The energy function connects to the probability distribution via $p(\boldsymbol x)\propto e^{-E_\theta(\boldsymbol x,t=0)}$. Thus, the lower the energy, the higher the probability. See \citet{du2023reduce} for more details.}. For example, if the energy landscape has many local minima or has the wrong global minima, simply scaling up the inference budget will likely not result in improved performance. We identify that the insufficient \emph{performance-energy consistency} and \emph{adversarial sampling}\footnote{Detailed description for \emph{performance-energy consistency} and \emph{adversarial sampling} can be found in Appendix \ref{app:related_algo_metric}.} as two key reasons limiting the performance. These factors make the sampling easier to be stuck at sub-optimal solutions and hard to go out. For \emph{inference}, simple search methods such as random search may not be efficient enough to navigate the vast space when generating high-dimensional samples. 

In this work, we introduce \emph{\underline{T}est-time \underline{S}calable M\underline{C}TS-\underline{en}hanced \underline{D}iffusion Model} (\proj), a novel framework that significantly improves diffusion model's reasoning capability. \proj consists of innovations in both training objectives and inference scaling. On the training side, \proj incorporates a novel Linear-Regression Negative Contrastive Learning (LRNCL) objective to improve the \emph{performance-energy consistency} of the energy landscape, and a KL regularization to address adversarial sampling. Specifically, for the LRNCL objective, it randomly samples two negative samples from a positive sample and performs a linear regression for these three samples on their energy vs. L2 distance to the positive sample. The LRNCL objective encourages that the samples' energy is linear w.r.t. their L2 distance to the positive sample, and the slope is positive, thus making the energy landscape reflective of their distance to the ground-truth, improving \emph{performance-energy consistency}. For the KL regularization, it differentiates through the denoising process, making the energy landscape easier to sample and reducing \emph{adversarial sampling}. 

On the inference side, we integrate the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS). It sequentially performs best-of-N random search and MCTS as the denoising proceeds. The first few steps of best-of-N random search use $L$ independent samples that denoise from the prior (Gaussian) distribution until reaching a predefined denoising step. The best sample is chosen from the $L$ samples. Then MCTS denoising follows and completes the denoising process. The random search in the initial denoising steps allows the model to reach cleaner samples with a reasonable signal-to-noise ratio, while the later MCTS denoising allows better navigation in the search space. Taken together, the hybrid MCTS denoising allows the model to significantly improve inference performance by scaling up the inference computational budget.

We demonstrate the effectiveness of our \proj's training objective and scalable inference method on challenging reasoning tasks of Maze and Sudoku. In Sudoku tasks, our \proj solves $43\%$ problems when conditioned on much less given digits (OOD condition), while the standard diffusion model only solves $30\%$ problems. In Maze tasks,  trained with Maze sizes of up to $6\times6$, our \proj solves 88\% of Maze problems with much larger sizes of $15\times15$, while standard diffusion completely fails.


In summary, our contributions are as follows: \textbf{(1)} We introduce \emph{\underline{T}est-time \underline{S}calable M\underline{C}TS-\underline{en}hanced \underline{D}iffusion Model} (\proj) as a novel framework that can scale up test-time computation for better reasoning capability. \textbf{(2)} We introduce Linear-Regression Negative Contrastive Learning (LRNCL) and KL regularization into the training objective to improve the energy landscape. \textbf{(3)} We integrate a hybrid Monte Carlo Tree Search into the denoising process, enabling test-time scaling for better reasoning accuracy. \textbf{(4)} We demonstrate the effectiveness of our method on challenging Sudoku and Maze datasets where the conditions are OOD or the problem sizes are much larger. 
% 1. Propose to combine diffusion model and MCTS to scale up during diffusion inference time to generate more diverse and desirable samples. 
% \tao{propose or use or combine???????}

% 2. Find sample-diversity and reward-performance consistency the key and use maximum entropy regularization and contrastive training to improve the model base.
% \tao{propose "maximum entropy regularization and contrastive training"?  " }
% \tao{sample-diversity and reward-performance consistency the key" is consensus?}
% \jiashu{our method comprises two main components: base model (ME, KL) and verifier (NCL), which is a general framework that is suitable for diffusion models and beyond. A small concern is that currently these two components are merged into a single model due to the convenience of EBM. It might take some consideration to emphasize the general potential application of our method.}

% 3. Extensive experiments have demonstrated the effectiveness of our method.
% detailed results?

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1\textwidth]{fig/figure1.pdf}}
\vskip -0.2in
\caption{Overview of \proj. This figure illustrates the key aspects of T-SCEND by contrasting its training and inference strategies with those of the previous method. Specifically, T-SCEND introduces $\l_\text{LRNCL}$ and $\l_\text{KL}$ during training and incorporates Monte Carlo Tree Search (MCTS) during inference to enable efficient scaling.}
\label{fig:figure2}
\end{center}
\vskip -0.3in
\end{figure*}