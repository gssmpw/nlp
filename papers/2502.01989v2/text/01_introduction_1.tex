In recent years, diffusion generative models \cite{sohl2015deep,ho2020denoising,song2021scorebased} have demonstrated remarkable performance across a wide range of applications, including image and video generation, robotic control, and reasoning \cite{du2024learning}. By learning how to denoise from noisy, corrupted samples to clean data samples, diffusion models can generate high-quality, high-dimensional samples approximating the data distribution.

Nevertheless, as the application scenarios for diffusion models become more complex, they often encounter situations where the problem sizes are much larger than those in training or their conditional input is out-of-distribution (OOD). When dealing with larger problem sizes, the computational resources required may increase exponentially, and the existing models may struggle to maintain high-quality performance. In OOD scenarios, the diffusion models need to generalize to conditions that have characteristics different from what they have been trained on. Thus, the pressing question is how to further enhance the performance of diffusion models under such challenging circumstances.

Interestingly, test-time scaling has demonstrated its effectiveness in large language models (LLMs). By scaling up test-time compute through explicit reasoning and search via chain-of-thought \cite{} or tree-of-thought \cite{}, LLMs can achieve significantly better performance. It remains an interesting question whether and how test-time scaling can also benefit diffusion models and address the challenges above.

However, na\"ively scaling up test-time computation for diffusion models does not typically yield improved performance. This was observed in prior works \cite{} and our pivot study. 




Logic:

Diffusion models wide application

Diffusion models need to be used in scenarios where the conditions may be OOD, or the problem sizes may be larger, how to further improve performance?

Test time scaling has shown to be effective in LLM.  

However, prior methods has shown that . In our pivot experiment, we also show that by scaling . We show that this is due to.

To address this, in this work, we propose.

Training: inference:


Our contribution:



\textbf{the success of diffusion model} XXX


\textbf{Trending of test time computing.} XXX



\textbf{the potential to scale up diffusion model in inference time} XXX

\textbf{In this paper, we propose } XXX

\textbf{main contribution:} XXX
1. Propose to combine diffusion model and MCTS to scale up during diffusion inference time to generate more diverse and desirable samples. 
\tao{propose or use or combine???????}

2. Find sample-diversity and reward-performance consistency the key and use maximum entropy regularization and contrastive training to improve the model base.
\tao{propose "maximum entropy regularization and contrastive training"?  " }
\tao{sample-diversity and reward-performance consistency the key" is consensus?}
\jiashu{our method comprises two main components: base model (ME, KL) and verifier (NCL), which is a general framework that is suitable for diffusion models and beyond. A small concern is that currently these two components are merged into a single model due to the convenience of EBM. It might take some consideration to emphasize the general potential application of our method.}

3. Extensive experiments have demonstrated the effectiveness of our method.
% detailed results?

\begin{figure*}[ht]
\label{fig:figure1}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\textwidth]{fig/figure1.pdf}}
\caption{Framework of \proj. \tao{to add}}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure*}