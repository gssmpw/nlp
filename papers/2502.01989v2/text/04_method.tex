\subsection{Problem setup}
\label{sec:problem_setup}

Consider an optimization problem $\underset{\boldsymbol{x}\in \C}{\min}\mathcal J(\boldsymbol{x})$  with objective $\mathcal J: \mathbb R^n\to \mathbb R$ and feasible set $\C\subseteq \mathbb R^n$. We focus on cases where $\mathbb R^n$ is a high-dimensional space and a dataset $\{\boldsymbol x_i\}$ drawn from $p(\boldsymbol x)$ supported on $\C$ is given. The dataset has two-fold effects here. First, in real-world applications, the explicit expression of $\C$ is often unavailable, and can only be learned implicitly from samples. Second, in high-dimensional problems, data prior could make the search much more efficient, as we will demonstrate in Section~\ref{sec:exp}.


\subsection{Training of \proj}
\label{sec:training_method}

Although the denoising loss $\l_\text{MSE}$ (Eq. \ref{eq:L_mse}) and the contrastive loss $\l_\text{Contrast}$ (Eq. \ref{eq:contrast}) are effective, we find that simply scaling up inference budget by having more independent random search processes yields marginal gain (Sec. \ref{sec:failure_analysis}). Through deeper analysis, we find that the core reason is insufficient \emph{performance-energy consistency} and \emph{adversarial sampling}, which we address by introducing a novel Linear-Regression Negative Contrastive Learning loss and incorporating a KL regularization, which we detail as follows.

\textbf{Linear-Regression Negative Contrastive Learning.} Concretely, although the contrastive loss (Eq. \ref{eq:contrast}) encourages the positive examples to have global minimum energy, it does not stipulate the landscape for the negative examples. For example, it is likely that a negative example $\x_t^{--}$ that is further apart from the positive example $\x_t$ has lower energy than a negative example $\x_t^{-}$ that is nearer. Thus, the inference process can start around $\x_t^{--}$ and can hardly move out. This will result in reduced \emph{performance-energy consistency}, where lower energy at step $t$ does not necessarily correspond to a more accurate predicted solution $\hat{\x}_0$, as will be shown in Sec. \ref{sec:failure_analysis}. This problem can not be easily remedied by having more inference budget such as more random search processes. A more fundamental way to solve this problem is to shape the energy landscape by regularizing the energy \emph{among} negative examples during \emph{training}. Specifically, from a positive example $\x_0\sim p(\x)$, we sample two negative examples $\x_0^{-}$ and $\x_0^{--}$\footnote{Details to generate negative samples are in Appendix \ref{app:related_algo_metric}.}, the latter has a larger L2 distance to $\x_0$. Specifically, the distance of $\x_0$, $\x_0^{-}$ and $\x_0^{--}$ to $\x_0$ are $0$, $l_{2,0}^-=||\x_0^{-}-\x_0||_2^2$, and $l_{2,0}^{--}=||\x_0^{--}-\x_0||_2^2$, respectively. Then we obtain their corresponding noisy examples at step $t$ via $\x_t=\sqrt{\bar{\alpha}_t}\x_0+\sqrt{1-\bar{\alpha}_t}\eps$, $\x_t^-=\sqrt{\bar{\alpha}_t}\x_0^-+\sqrt{1-\bar{\alpha}_t}\eps$, and $\x_t^{--}=\sqrt{\bar{\alpha}_t}\x_0^{--}+\sqrt{1-\bar{\alpha}_t}\eps$. Their energy are $E^+_t=E_\theta(\x_t,t)$, $E^-_t=E_\theta(\x_t^-,t)$, and $E^{--}_t=E_\theta(\x_t^{--},t)$, respectively. To encourage a landscape where the energy difference between negative samples and the positive sample becomes smaller as they approach the positive sample, we directly constrain the relationship between the energy of different negative samples and the positive sample using linear regression. Specifically, we first apply the linear regression algorithm\footnote{The details of linear regression algorithm can be found in Appendix \ref{app:related_algo_metric}.} to fit a line through the three points \(\{(0, E_t^+), (l_{2,0}^{-}, E_t^{-}), (l_{2,0}^{--}, E_t^{--})\}\), which is characterized by the slope \(k_t\) and the intercept \(b_t\). Then, we obtain the corresponding fitted points \(\{(0, \hat{E}_t^+), (l_{2,0}^{-}, \hat{E}_t^{-}), (l_{2,0}^{--}, \hat{E}_t^{--})\}\). We then compute the Linear-Regression Negative Contrastive Learning (LRNCL) loss as follows:
\begin{equation}
\begin{aligned}
\l_{\text{LRNCL}} =\ &\E_{\x_0,\x_0^-,\x_0^{--},\eps,t}\big[\max(0,\gamma - k_t)\,+ \\
||E_t^+\,-\,\hat{E}_t^+&||_2^2 + ||E_t^{-} - \hat{E}_t^{-}||_2^2
+||E_t^{--} - \hat{E}_t^{--}||_2^2\big],
\end{aligned}
\end{equation}
where $\gamma$ is a hyperparameter and $t\sim\{0,\dots,T\}$. The first term $\max(0,\gamma-k_t)$ encourages that any three samples $\x_t$, $\x_t^-$, $\x_t^{--}$ have a positive (and larger-than-$\gamma$) slope of energy vs. L2 distance. The latter three terms encourage that the energy vs. L2 distance is linear, making the energy landscape more smooth\footnote{It is also possible to use more complex shapes than linear regression to regularize the energy landscape, which we leave for future work. Here we find that simple linear regression works well.}.
% l_{\text{mse-fit}} = ||E_t - \hat{E}_t||_2^2 + ||E_t^{-} - \hat{E}_t^{-}||_2^2 + ||E_t^{--} - \hat{E}_t^{--}||_2^2
% \end{equation}
% \begin{equation}
% l_{\text{LRNCL}} = l_{\text{mse-fit}} + \max(0, \gamma - k_t)

\textbf{KL regularization.} Besides \emph{performance-energy consistency}, another important challenge preventing better test-time scaling is \emph{adversarial sampling}. Specifically, during training, the contrastive loss such as Eq. \ref{eq:contrast} often leads to \emph{adversarial sampling} where the energy function learns to simply generate an energy landscape that makes sampling difficult. To address this issue, we incorporate the KL-regularization as in \citet{du2021improved}:
\begin{equation}
\l_\text{KL}=\E_{t,p_{\theta,t}(\x)}[E_{\text{stop-grad}(\theta)}(\x)]+\E_{t,p_{\theta,t}(\x)}[\log p_{\theta,t}(\x)]
\end{equation}
where $p_{\theta,t}(\x)$ is the probability distribution of $\x_t$ at denoising step $t$. When optimizing w.r.t. $\l_\text{KL}$, it is essentially optimizing w.r.t. the sampling (denoising) process by shaping the energy landscape to make it easier to sample. The first term in $\l_\text{KL}$ encourages the samples $\x_t$ to have low energy, and the second term is maximizing the entropy of the samples, encouraging the samples to be diverse. Both terms allow better test-time scaling. Different from \citet{du2021improved}, we have this KL regularization on each denoising step $t$, and we employ a more accurate estimation of the entropy \cite{lombardi2016nonparametric}.

Taken together, the training objective of our \proj is:
\begin{equation}\label{eq:full_objective}
\l=\l_\text{MSE}+\l_\text{Contrast}+\l_\text{LRNCL}+\l_\text{KL},
\end{equation}
where the latter two terms improve the energy landscape and boost the test-time scalability of diffusion models.

% \input{text/alg/alg_train}
\subsection{Inference of \proj}
\label{sec:inference_method}
To fully harness the potential of the diffusion model, we propose a novel hybrid Monte Carlo Tree Search denoising (hMCTS denoising) method, which progressively applies random search and Monte Carlo Tree Search (MCTS) denoising in sequence, as detailed below, throughout the denoising process. As demonstrated in Algorithm ~\ref{alg:hmcts}, we employ the best-of-N random search for the diffusion process in the early diffusion stages, which introduces $L$ initial noises to maintain a consistent number of function evaluations (NFE) per example during the diffusion process\footnote{In this paper, we primarily report $K = N_r$ MCTS denoising, where ensuring $K = N_r, L=N_r$ guarantees that the NFE (Number of Function Evaluations) for each case of best-of-N random search, MCTS denoising, and hMCTS denoising remains the same.}. Subsequently, hMCTS denoising utilizes the MCTS denoising\footnote{For a detailed explanation of the distinctions between hMCTS denoising, MCTS denoising, and best-of-N random search, as well as the impact of MCTS denoising start step \( t_s \), please refer to Appendix \ref{app:additional_results}.} to iteratively perform the denoising process until the termination state \( \x_0 \) is reached. This approach enables MCTS to more accurately estimate node value when the noise is relatively small, thereby preventing the premature exclusion of potentially promising nodes. Next, we will detail the MCTS denoise process. 

 We treat the current diffusion state \( \boldsymbol{\x_t} \) as the state, the noise to remove as the action, and model the denoising process of the diffusion model as a Markov Decision Process (MDP). Therefore, a \emph{node} in MCTS represents the state \( \x_t \), along with its current visit count \( N(\x_t) \) and state value \( Q(\x_t) \). A terminal node is defined as a node whose denoising step is 0. In this context, we use \( \nabla_{\boldsymbol{x}_t} E_\theta(\boldsymbol{x}_t, t) \) and \( E_\theta(\boldsymbol{x}_t, t) \) of the energy-based diffusion as the policy network and value network, respectively. Each deepening of the search tree corresponds to a single denoising step. Similar to MCTS in AlphaGo ~\cite{silver2016mastering} and AlphaZero ~\cite{silver2017mastering}, each rollout in MCTS consists of four core steps: selection, expansion, simulation, and backpropagation, as illustrated in Fig.~\ref{fig:figure2} and Algorithm ~\ref{alg:hmcts}:

\textbf{(1) Selection}: Based on the Upper Confidence Bound (UCB) from AlphaGo ~\cite{silver2016mastering}:
\begin{equation}
UCB(s, a) = Q(s, a) + c P(s, a) \sqrt{\frac{\sum_b N(s, b)}{1 + N(s, a)}},
\end{equation}
starting from the current root node state \( \boldsymbol{x}_t \), we select a child node based on the following adjusted UCB of MCTS-enhanced diffusion formula until a leaf node $\{\x_{t'}, Q(\x_{t'}), N(\x_{t'})\}$ is reached:
\begin{equation}
\label{eq:ucb_denoise}
UCB(\boldsymbol{x}_t, \boldsymbol{a}_t) = Q(\boldsymbol{x}_t, \boldsymbol{a}_t) + c \sqrt{\frac{\ln N_i}{n_i}},
\end{equation}
where $Q(\boldsymbol{x}_t, \boldsymbol{a}_t)$ represent the value of children node, action $\boldsymbol{a}_t$ includes predicted noise $\boldsymbol\epsilon_{\theta}$ and random Gaussian noise $\boldsymbol\epsilon$, \( n_i \) represents the number of visits to node \( i \), \( N_i \) represents the number of visits to the parent node of \( i \), and \( c \) is the exploration hyperparameter.

\textbf{(2) Expansion}: Unless the state of the node reached is a terminal state $\boldsymbol{x}_0$, we expand the children of the selected node by choosing an action and creating new nodes based on the action.
For the expansion step of MCTS denoising, we perform a denoising step and add different but limited Gaussian noise. This results in \( K \) distinct branches \( \left\{ \boldsymbol x_{t'-1}^{(k)} \mid k = 0, 1, \cdots, K-1\right\} \), where each child node $\boldsymbol{x}_{t'-1}^{(k)}$ is derived as following equation adjusted from Eq. \ref{eq:ddpm_denoise} :
\input{text/alg/alg_inference}
% \begin{equation}
%      \boldsymbol{x}_{t'}^{(k)} = \sqrt{\alpha_{t'}} \frac{\boldsymbol{x}_{t} - \sqrt{1 - \alpha_{t}} \boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t}, t\right)}{\sqrt{\alpha_{t}}} + \sqrt{1 - \alpha_{t'} - \sigma_{t'}^{2}}\boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t}, t\right) + \sigma_{t'} \boldsymbol\epsilon^{(k)},
%  \end{equation}
\setlength{\belowdisplayskip}{2pt}
\begin{equation}
\label{eq:mcts_denoise}
\begin{split}
    \boldsymbol{x}_{t'-1}^{(k)} &= \sqrt{\bar\alpha_{t'-1}} \frac{\boldsymbol{x}_{t'} - \sqrt{1 - \bar\alpha_{t'}} \boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t'}, t'\right)}{\sqrt{\bar\alpha_{t'}}} \\
    &\quad + \sqrt{1 - \bar\alpha_{t'-1} - \sigma_{t'}^{2}}\boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t'}, t'\right) + \sigma_{t'} \boldsymbol\epsilon^{(k)},
\end{split}
\end{equation}
with \( \boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t'}, t') \) determined by \( \boldsymbol{x}_{t'} \) and \( t' \), and \( \boldsymbol{\epsilon}^{(k)}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) \) representing random Gaussian noise.

\textbf{(3) Simulation}: We randomly select a child node and perform a random simulation of the MDP until reaching a terminal state. For the simulation of MCTS denoising, we use DDIMs \cite{song2020denoising} for fast sampling to obtain \( \hat{\boldsymbol{x}}_0(\boldsymbol{x}_{t'-1}^{(k^*)}) \) from the randomly chosen child node state $\x_{t'-1}^{(k^*)}$, and then use \( E_\theta(\hat{\boldsymbol{x}}_0(\boldsymbol{x}_{t'-1}^{(k^*)})) \) as the reward for backpropagation.

\textbf{(4) Backpropagation}: Finally, we backpropagate the node values to the root node, updating the value of each node using the expected value along the path.
 
 After $N_r$  rollouts, we select \( \boldsymbol{x}_{t-1}^{(k)} \) with the largest value $\frac{Q(\x_{t-1}^{(k)})}{N(\x_{t-1}^{(k)})}$ as the state \( \boldsymbol{x}_{t-1} \) for the next denoising step. The next MCTS denoising process starts from this state and proceeds until the termination state $\boldsymbol{x}_0$. Here, the depth of the search tree is decided by three elements: the number of rollout steps $N_r$, the maximum number of branches $K$ for each node, and the search policy.

To summarize, our proposed paradigm, \proj, enhances \emph{performance-energy consistency} and ease of sampling through LRNCL loss and KL regularization, respectively (Sec. \ref{sec:training_method}), while fully unlocking the test-time scalability of diffusion models via hMCTS denoising (Sec. \ref{sec:inference_method}).