 % \begin{multline}
 %    \mathbb{E}_{t \sim U(1, T), \boldsymbol{x}_{0} \sim p(x), \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}, t\right)\right\|_{2}^{2}\right],
 % \end{multline}
% {\setlength{\abovedisplayskip}{1pt}
% \setlength{\belowdisplayskip}{1pt}
% \begin{multline}
%     \resizebox{0.98\columnwidth}{!}{
%         $l_\text{MSE}=\mathbb{E}_{t \sim U(1, T), \boldsymbol{x}_{0} \sim p(x), \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}, t\right)\right\|_{2}^{2}\right]$
%     }
% \end{multline}
% \mathbb{E}_{t,\boldsymbol{x}_0,\boldsymbol{\epsilon}}\left[
The Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising} contains a predefined forward process to corrupt data into Gaussian noises, and a learnable reverse process to generate new samples from them. The forward process follows a Gaussian transition kernel $q(\boldsymbol{x}_t\vert\boldsymbol{x}_{t-1})=\mathcal N(\boldsymbol{x}_t\vert \sqrt{\alpha_t}\boldsymbol{x}_{t-1}, (1-\alpha_t)\boldsymbol I), t=1, \cdots, T$, where the noise schedule $\{\alpha_t\}_{t=1}^T$ and $T$ is set to make $\boldsymbol{x}_T$ follow approximately a standard Gaussian distribution. The reverse process can be learned to predict the noise from the corrupted data by minimizing \begin{equation}\label{eq:L_mse}
\l_\text{MSE}=\E_{\x_0,\eps,t}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}, t\right)\right\|_{2}^{2}\right],
\end{equation}
where the expectation is w.r.t.  $\boldsymbol{x}_{0}\sim p(\boldsymbol{x})$, $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, and $t\sim \{1,...T\}$. $\bar{\alpha}_{t}:=\prod_{i=1}^{t} \alpha_{i}$. Eq. \ref{eq:L_mse} is equivalent to optimizing a reweighted variational bound on negative log-likelihood. Without loss of generality, in this work, we parameterize $\boldsymbol{\epsilon}_\theta$ as the gradient of the energy function $\nabla_{\boldsymbol{x}_t} E_\theta(\boldsymbol{x}_t, t)$ as in \citet{du2023reduce}\footnote{This parameterization has the benefit that it simultaneously learns $\eps_\theta$ as gradient of the energy $\nabla_{\boldsymbol{x}_t} E_\theta(\boldsymbol{x}_t, t)$ and learns the energy itself as an unnormalized log-probability due to $p(\x_t)\propto e^{-E_\theta(\x_t,t)}$, allowing inference to utilize this ``energy'' as guidance.}. To improve the energy landscape, \citet{du2024learning} introduced a contrastive loss 
\begin{equation}\label{eq:contrast}
\l_\text{Contrast}=\E_{\x_0,\x_0^-,\eps,t}\left[-\log\left(\frac{e^{-E_\theta(\x_t,t)}}{e^{-E_\theta(\x_t,t)} + e^{-E_\theta(\x_t^-,t)}}\right)\right],
\end{equation}
where $\x_0\sim p(\x)$, $\eps\sim\mathcal{N}(\mathbf{0},\mathbf{I})$, $\x_t=\sqrt{\bar{\alpha}_t}\x_0+\sqrt{1-\bar{\alpha}_t}\eps$. $\x_t^-=\sqrt{\bar{\alpha}_t}\x_0^-+\sqrt{1-\bar{\alpha}_t}\eps$. Here $\x_0^-\sim p_\text{corrupt}(\x_0^-|\x_0)$ are negative examples by corrupting the positive examples $\x_0$. This contrastive loss encourages the positive (ground-truth) examples to have global energy minima.

 The reverse process starts with $\boldsymbol{x}_T\sim \mathcal N(\boldsymbol{0}, \boldsymbol{I})$, and iteratively applies the learned denoising net $\boldsymbol\epsilon_\theta$, where \citet{song2020denoising} introduces an adjustable noise scale $\sigma_t$:
 % \begin{equation}
 %     \boldsymbol{x}_{t-1} = \sqrt{\alpha_{t-1}} \frac{\boldsymbol{x}_{t} - \sqrt{1 - \alpha_{t}} \boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t}, t\right)}{\sqrt{\alpha_{t}}} + 
 %     \sqrt{1 - \alpha_{t-1} - \sigma_{t}^{2}}\boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t}, t\right) + \sigma_{t} \boldsymbol\epsilon_{t}.
 % \end{equation}
 {\setlength{\abovedisplayskip}{1.2pt}
\begin{equation}
\label{eq:ddpm_denoise}
\begin{split}
    &\boldsymbol{x}_{t-1} = \sqrt{\bar\alpha_{t-1}} \frac{\boldsymbol{x}_{t} - \sqrt{1 - \bar\alpha_{t}} \boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t}, t\right)}{\sqrt{\bar\alpha_{t}}} \\
    &+ \sqrt{1 - \bar\alpha_{t-1} - \sigma_{t}^{2}}\boldsymbol\epsilon_{\theta}\left(\boldsymbol{x}_{t}, t\right) + \sigma_{t} \boldsymbol\epsilon_{t},\,\, 
\boldsymbol{\epsilon}_t \sim\mathcal{N}(\mathbf{0},\mathbf{I})
\end{split}
\end{equation}
 Importantly, \citet{song2020denoising} highlights that a diffusion model trained with $\{\alpha_t\}_{t=1}^T$ can be used to sample with $\{\alpha_{\tau_s}\}_{s=1}^S$, where $\tau$ is any increasing sub-sequence of $1,2,\cdots, T$ of length $S$, which significantly accelerates sampling, and will serve as an efficient approach for MCTS simulation in Section~\ref{sec:inference_method}.