@misc{rahaman2019spectralbiasneuralnetworks,
      title={On the Spectral Bias of Neural Networks}, 
      author={Nasim Rahaman and Aristide Baratin and Devansh Arpit and Felix Draxler and Min Lin and Fred A. Hamprecht and Yoshua Bengio and Aaron Courville},
      year={2019},
      eprint={1806.08734},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1806.08734}, 
}

@inproceedings{basri2020frequencybiasneuralnetworks,
author = {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
title = {Frequency bias in neural networks for input of non-uniform density},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias - networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency κ, convergence at a point x ∈ Sd-1 occurs in time O(κd/p(x)) where p(x) denotes the local density at x. Specifically, for data in S1 we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {64},
numpages = {10},
series = {ICML'20}
}


@inproceedings{NTKjacot2018,
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
title = {Neural tangent kernel: convergence and generalization in neural networks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit [12, 9], thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function fθ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function fθ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping.Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {8580–8589},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{FourierFeaturesTancik2020,
author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
title = {Fourier features let networks learn high frequency functions in low dimensional domains},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {632},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@misc{bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks,
      title={Wav-KAN: Wavelet Kolmogorov-Arnold Networks}, 
      author={Zavareh Bozorgasl and Hao Chen},
      year={2024},
      eprint={2405.12832},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12832}, 
}

@misc{cuomo2022scientificmachinelearningphysicsinformed,
      title={Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next}, 
      author={Salvatore Cuomo and Vincenzo Schiano di Cola and Fabio Giampaolo and Gianluigi Rozza and Maziar Raissi and Francesco Piccialli},
      year={2022},
      eprint={2201.05624},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.05624}, 
}

@article{Cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={George V. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314},
  url={https://api.semanticscholar.org/CorpusID:3958369}
}

@article{HORNIK1990551approach,
title = {Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
journal = {Neural Networks},
volume = {3},
number = {5},
pages = {551-560},
year = {1990},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(90)90005-6},
url = {https://www.sciencedirect.com/science/article/pii/0893608090900056},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Approximation, Derivatives, Sobolev space, Feedforward networks},
abstract = {We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.}
}

@misc{patra2024physicsinformedkolmogorovarnoldneural,
      title={Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN}, 
      author={Subhajit Patra and Sonali Panda and Bikram Keshari Parida and Mahima Arya and Kurt Jacobs and Denys I. Bondar and Abhijit Sen},
      year={2024},
      eprint={2407.18373},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.18373}, 
}

@misc{wang2024kolmogorovarnoldinformedneural,
      title={Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks}, 
      author={Yizheng Wang and Jia Sun and Jinshuai Bai and Cosmin Anitescu and Mohammad Sadegh Eshaghi and Xiaoying Zhuang and Timon Rabczuk and Yinghua Liu},
      year={2024},
      eprint={2406.11045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11045}, 
}

@misc{liu2024kankolmogorovarnoldnetworks,
      title={KAN: Kolmogorov-Arnold Networks}, 
      author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
      year={2024},
      eprint={2404.19756},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.19756}, 
}

@article{WANG2021113938fourierfeaturespINNs,
title = {On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {384},
pages = {113938},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113938},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521002759},
author = {Sifan Wang and Hanwen Wang and Paris Perdikaris},
keywords = {Spectral bias, Deep learning, Neural Tangent Kernel, Partial differential equations, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction–diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/PredictiveIntelligenceLab/MultiscalePINNs.}
}

@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{SIRIGNANO20181339,
title = {DGM: A deep learning algorithm for solving partial differential equations},
journal = {Journal of Computational Physics},
volume = {375},
pages = {1339-1364},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
author = {Justin Sirignano and Konstantinos Spiliopoulos},
keywords = {Partial differential equations, Machine learning, Deep learning, High-dimensional partial differential equations},
abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.}
}

@article{WANG2022110768whenandwhyfailspinns,
title = {When and why PINNs fail to train: A neural tangent kernel perspective},
journal = {Journal of Computational Physics},
volume = {449},
pages = {110768},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110768},
url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
author = {Sifan Wang and Xinling Yu and Paris Perdikaris},
keywords = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.}
}

@misc{cao2020understandingspectralbiasdeep,
      title={Towards Understanding the Spectral Bias of Deep Learning}, 
      author={Yuan Cao and Zhiying Fang and Yue Wu and Ding-Xuan Zhou and Quanquan Gu},
      year={2020},
      eprint={1912.01198},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01198}, 
}

@misc{krishnapriyan2021characterizingpossiblefailuremodes,
      title={Characterizing possible failure modes in physics-informed neural networks}, 
      author={Aditi S. Krishnapriyan and Amir Gholami and Shandian Zhe and Robert M. Kirby and Michael W. Mahoney},
      year={2021},
      eprint={2109.01050},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.01050}, 
}

@article{PhysRevE025205applicationsDNN,
  title = {Uncovering turbulent plasma dynamics via deep learning from partial observations},
  author = {Mathews, A. and Francisquez, M. and Hughes, J. W. and Hatch, D. R. and Zhu, B. and Rogers, B. N.},
  journal = {Phys. Rev. E},
  volume = {104},
  issue = {2},
  pages = {025205},
  numpages = {11},
  year = {2021},
  month = {08},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.104.025205},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.104.025205}
}


@article{KISSAS2020112623aplicationsDNN,
title = {Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4D flow MRI data using physics-informed neural networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {358},
pages = {112623},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2019.112623},
url = {https://www.sciencedirect.com/science/article/pii/S0045782519305055},
author = {Georgios Kissas and Yibo Yang and Eileen Hwuang and Walter R. Witschey and John A. Detre and Paris Perdikaris},
keywords = {Deep neural networks, Blood flow modeling, Pulse wave propagation, Data-driven modeling, Non-invasive diagnostics},
abstract = {Advances in computational science offer a principled pipeline for predictive modeling of cardiovascular flows and aspire to provide a valuable tool for monitoring, diagnostics and surgical planning. Such models can be nowadays deployed on large patient-specific topologies of systemic arterial networks and return detailed predictions on flow patterns, wall shear stresses, and pulse wave propagation. However, their success heavily relies on tedious pre-processing and calibration procedures that typically induce a significant computational cost, thus hampering their clinical applicability. In this work we put forth a machine learning framework that enables the seamless synthesis of non-invasive in-vivo measurement techniques and computational flow dynamics models derived from first physical principles. We illustrate this new paradigm by showing how one-dimensional models of pulsatile flow can be used to constrain the output of deep neural networks such that their predictions satisfy the conservation of mass and momentum principles. Once trained on noisy and scattered clinical data of flow and wall displacement, these networks can return physically consistent predictions for velocity, pressure and wall displacement pulse wave propagation, all without the need to employ conventional simulators. A simple post-processing of these outputs can also provide a relatively cheap and effective way for estimating Windkessel model parameters that are required for the calibration of traditional computational models. The effectiveness of the proposed techniques is demonstrated through a series of prototype benchmarks, as well as a realistic clinical case involving in-vivo measurements near the aorta/carotid bifurcation of a healthy human subject.}
}

@article{ProblemsPINNs2,
author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
title = {Understanding and Mitigating Gradient Flow Pathologies in Physics-Informed Neural Networks},
journal = {SIAM Journal on Scientific Computing},
volume = {43},
number = {5},
pages = {A3055-A3081},
year = {2021},
doi = {10.1137/20M1318043},

URL = { 
    
        https://doi.org/10.1137/20M1318043
    
    

},
eprint = { 
    
        https://doi.org/10.1137/20M1318043
    
    

}
,
    abstract = { The widespread use of neural networks across different scientific domains often involves constraining them to satisfy certain symmetries, conservation laws, or other domain knowledge. Such constraints are often imposed as soft penalties during model training and effectively act as domain-specific regularizers of the empirical risk loss. Physics-informed neural networks is an example of this philosophy in which the outputs of deep neural networks are constrained to approximately satisfy a given set of partial differential equations. In this work we review recent advances in scientific machine learning with a specific focus on the effectiveness of physics-informed neural networks in predicting outcomes of physical systems and discovering hidden physics from noisy data. We also identify and analyze a fundamental mode of failure of such approaches that is related to numerical stiffness leading to unbalanced back-propagated gradients during model training. To address this limitation we present a learning rate annealing algorithm that utilizes gradient statistics during model training to balance the interplay between different terms in composite loss functions. We also propose a novel neural network architecture that is more resilient to such gradient pathologies. Taken together, our developments provide new insights into the training of constrained neural networks and consistently improve the predictive accuracy of physics-informed neural networks by a factor of 50--100\$\times\$ across a range of problems in computational physics. All code and data accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs. }
}

@misc{basri2019convergencerateneuralnetworks,
      title={The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies}, 
      author={Ronen Basri and David Jacobs and Yoni Kasten and Shira Kritchman},
      year={2019},
      eprint={1906.00425},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.00425}, 
}

@misc{ss2024chebyshevpolynomialbasedkolmogorovarnoldnetworks,
      title={Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation}, 
      author={Sidharth SS and Keerthana AR and Gokul R and Anas KP},
      year={2024},
      eprint={2405.07200},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07200}, 
}

@Inbook{KolmogorovArnold2009Theorem,
editor="Givental, Alexander B.
and Khesin, Boris A.
and Marsden, Jerrold E.
and Varchenko, Alexander N.
and Vassiliev, Victor A.
and Viro, Oleg Ya.
and Zakalyukin, Vladimir M.",
title="On the representation of functions of several variables as a superposition of functions of a smaller number of variables",
bookTitle="Collected Works: Representations of Functions, Celestial Mechanics and KAM Theory, 1957--1965",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="25--46",
abstract="In this paper we wish to give an account of several recent papers by Moscow mathematicians devoted to the question in the title of this paper. {\textsection}1 contains the definition of superposition of functions and the statement of Hilbert's 13th problem relating to superpositions. {\textsection}2 is devoted to superpositions of smooth functions. In {\textsection}3 we present several very recent papers, in spite of the fact that the content of that section is now perhaps only of historical interest. The principal topic there is the description given by Kronrod of ``the tree of components of a function of several variables'', which is a concept whose popularization would seem to be very desirable (although the connection between this concept and the problems considered in our paper has proved to be less close than it originally appeared). The reader interested only in the strongest (and, moreover, the simplest in its method of proof) result relating to the representation of continuous functions of several variables as superpositions of functions of a smaller number of variables can, after looking at the introductory {\textsection}1 go straight to {\textsection}4, missing out {\textsection}2--3. In addition, the smaller print in this paper means, as usual, that the corresponding material is auxiliary and omitting it will not affect the reader's understanding of what follows.",
isbn="978-3-642-01742-1",
doi="10.1007/978-3-642-01742-1_5",
url="https://doi.org/10.1007/978-3-642-01742-1_5"
}

@incollection{STEPHANE20091,
title = {CHAPTER 1 - Sparse Representations},
editor = {Mallat Stéphane},
booktitle = {A Wavelet Tour of Signal Processing (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {1-31},
year = {2009},
isbn = {978-0-12-374370-1},
doi = {https://doi.org/10.1016/B978-0-12-374370-1.00005-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123743701000057},
author = {Mallat Stéphane}
}
@incollection{STEPHANE200989,
title = {CHAPTER 4 - Time Meets Frequency},
editor = {Mallat Stéphane},
booktitle = {A Wavelet Tour of Signal Processing (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {89-153},
year = {2009},
isbn = {978-0-12-374370-1},
doi = {https://doi.org/10.1016/B978-0-12-374370-1.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123743701000082},
author = {Mallat Stéphane}
}

@article{equationKerneloperator2005ShaweTylor,
  author={Shawe-Taylor, J. and Williams, C.K.I. and Cristianini, N. and Kandola, J.},
  journal={IEEE Transactions on Information Theory}, 
  title={On the eigenspectrum of the gram matrix and the generalization error of kernel-PCA}, 
  year={2005},
  volume={51},
  number={7},
  pages={2510-2522},
  keywords={Eigenvalues and eigenfunctions;Principal component analysis;Kernel;Symmetric matrices;Statistical learning;Support vector machines;Gaussian processes;Machine learning;Algorithm design and analysis;Computer science;Concentration bounds;Gram matrices;kernel methods;principal components analysis (PCA);Rademacher complexity;spectra of random matrices;statistical learning theory},
  doi={10.1109/TIT.2005.850052}}

@article{differentialequationaplications1,
author = {Hochstadt, Harry},
title = {Differential Equations and Their Applications, 2nd edition (Martin Braun)},
journal = {SIAM Review},
volume = {21},
number = {2},
pages = {264-266},
year = {1979},
doi = {10.1137/1021043},

URL = { 
    
        https://doi.org/10.1137/1021043
    
    

},
eprint = { 
    
        https://doi.org/10.1137/1021043
    
    

}

}

@article{Penrosetheroadtoreality,
author = {Penrose, Roger and Jorgensen, Palle},
year = {2008},
month = {06},
pages = {59-61},
title = {The Road to Reality: A Complete Guide to the Laws of the Universe},
volume = {28},
journal = {The Mathematical Intelligencer},
doi = {10.1007/BF02986885}
}

@book{strogatz:2000,
  added-at = {2010-05-11T11:15:46.000+0200},
  author = {Strogatz, Steven H.},
  biburl = {https://www.bibsonomy.org/bibtex/2e3b3dc5a68df87d71becbe75709a7121/flashbang},
  citeulike-article-id = {6778211},
  interhash = {097881c5ab43732a75182222236e72c7},
  intrahash = {e3b3dc5a68df87d71becbe75709a7121},
  keywords = {chaos dynamical-systems nonlinear},
  posted-at = {2010-03-08 21:41:28},
  priority = {2},
  publisher = {Westview Press},
  timestamp = {2010-05-11T11:16:46.000+0200},
  title = {Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry and Engineering},
  year = 2000
}

@misc{badger2022deeplearninggeneralizes,
      title={Why Deep Learning Generalizes}, 
      author={Benjamin L. Badger},
      year={2022},
      eprint={2211.09639},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.09639}, 
}

@inproceedings{Basrifrecuencybias2020,
author = {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
title = {Frequency bias in neural networks for input of non-uniform density},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias - networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency κ, convergence at a point x ∈ Sd-1 occurs in time O(κd/p(x)) where p(x) denotes the local density at x. Specifically, for data in S1 we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {64},
numpages = {10},
series = {ICML'20}
}

@article{KANrepresentationtheorem2,
author = {Andrei Nikolaevich Kolmogorov},
year = {1957},
pages = {953–956},
title = {On the representation of continuous functions of many
variables by superposition of continuous functions of one variable and addition.},
volume = {114},
journal = {In Doklady
Akademii Nauk},

}
