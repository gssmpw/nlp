@inproceedings{FourierFeaturesTancik2020,
author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
title = {Fourier features let networks learn high frequency functions in low dimensional domains},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {632},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{NTKjacot2018,
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
title = {Neural tangent kernel: convergence and generalization in neural networks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit [12, 9], thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function fθ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function fθ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping.Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {8580–8589},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{WANG2021113938fourierfeaturespINNs,
title = {On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {384},
pages = {113938},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113938},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521002759},
author = {Sifan Wang and Hanwen Wang and Paris Perdikaris},
keywords = {Spectral bias, Deep learning, Neural Tangent Kernel, Partial differential equations, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction–diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/PredictiveIntelligenceLab/MultiscalePINNs.}
}

@inproceedings{basri2020frequencybiasneuralnetworks,
author = {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
title = {Frequency bias in neural networks for input of non-uniform density},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias - networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency κ, convergence at a point x ∈ Sd-1 occurs in time O(κd/p(x)) where p(x) denotes the local density at x. Specifically, for data in S1 we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {64},
numpages = {10},
series = {ICML'20}
}

@misc{bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks,
      title={Wav-KAN: Wavelet Kolmogorov-Arnold Networks}, 
      author={Zavareh Bozorgasl and Hao Chen},
      year={2024},
      eprint={2405.12832},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12832}, 
}

@misc{liu2024kankolmogorovarnoldnetworks,
      title={KAN: Kolmogorov-Arnold Networks}, 
      author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
      year={2024},
      eprint={2404.19756},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.19756}, 
}

@misc{patra2024physicsinformedkolmogorovarnoldneural,
      title={Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN}, 
      author={Subhajit Patra and Sonali Panda and Bikram Keshari Parida and Mahima Arya and Kurt Jacobs and Denys I. Bondar and Abhijit Sen},
      year={2024},
      eprint={2407.18373},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.18373}, 
}

@misc{rahaman2019spectralbiasneuralnetworks,
      title={On the Spectral Bias of Neural Networks}, 
      author={Nasim Rahaman and Aristide Baratin and Devansh Arpit and Felix Draxler and Min Lin and Fred A. Hamprecht and Yoshua Bengio and Aaron Courville},
      year={2019},
      eprint={1806.08734},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1806.08734}, 
}

@misc{wang2024kolmogorovarnoldinformedneural,
      title={Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks}, 
      author={Yizheng Wang and Jia Sun and Jinshuai Bai and Cosmin Anitescu and Mohammad Sadegh Eshaghi and Xiaoying Zhuang and Timon Rabczuk and Yinghua Liu},
      year={2024},
      eprint={2406.11045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11045}, 
}

