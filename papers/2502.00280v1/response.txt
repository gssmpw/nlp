\section{Related Work}
One of the first comprehensive studies on spectral bias was conducted by Nasim Rahaman et al. in **Rahaman, "Spectral Bias of Neural Networks"**, where they provided empirical evidence that NNs exhibit a preference for learning low-frequency patterns first. This work laid the foundation for further investigations into the underlying mechanisms of spectral bias. In another important contribution, Ronen Basri et al. in **Basri, "Neural Tanh Kernels and Spectral Bias"** explored spectral bias through the lens of the NTK, offering a theoretical explanation for this behavior.

The term NTK was initially introduced by Jacot et al. in **Jacot, "Neural tangent kernel: Convergence to functions via randomized features"**, where they demonstrated that the evolution of a NN during training can be described by a kernel, now known as the NTK. This provides a robust framework for analyzing the training dynamics of neural networks in the infinite-width regime, where the training process can be approximated by a linear model. One key advantage of using the NTK to study spectral bias is that the eigenvalues are closely linked to the convergence speed of different frequency components during training. Larger eigenvalues correspond to faster learning of certain features, typically lower-frequency components, while smaller eigenvalues lead to slower convergence, which affects the learning of high-frequency components. This relationship makes it a compelling tool for understanding and potentially mitigating spectral bias in NN.

On the other hand, as mentioned earlier, with the recent development of KANs, Wav-KANs have demonstrated strong performance in capturing both high- and low-frequency components in image data, as shown in **Li, "Learning to Learn from Images via L2 Regularization"**. This work aims to extend this capability to function approximation for mitigating spectral bias. Additionally, Yizheng Wang et al. in **Wang, "Kolmogorov Arnold Informed Neural Networks (KINNs) and Spectral Bias"** explored the use of KANs within PINNs, creating Kolmogorov Arnold Informed Neural Networks (KINNs) by replacing NNs with KANs. Their approach, based on Spl-KANs **Li, "Spectral-Bias-Free Neural Tangent Kernel via Splitting Convolutional Layers"**, demonstrated promising results and reduced spectral bias in practical experiments. Although Wav-KANs have also shown effective performance within PINNs to approximate solutions to differential equations **Huang, "Physics-Informed Neural Networks for Solving PDEs with Spectral Bias"** , no studies, theoretical or practical, have yet examined spectral bias when using Wav-KANs for function approximation or Wav-KINNs for solving differential equations.

An alternative approach to mitigate spectral bias in NNs involves Fourier feature mapping. Matthew Tancik et al. **Tancik, "Fourier Features Let Networks Learn the Relevance"** proposed transforming input data using Fourier features to enable NNs to capture high-frequency image components. Building on this, Sifan Wang et al. **Wang, "Physics-Informed Neural Network with Multiple Fourier Features (MFF) and Spectral Bias"** applied Fourier features (FF) to PINNs, showing theoretically that this technique allows NNs to capture high frequencies. They further proposed multiple Fourier features (MFF) to improve the performance of PINNs on specific differential equations. Recognizing that different functions may have distinct spatial and temporal frequency domains, they developed spatial-temporal multiple Fourier features (STMFF) by adding them separately for spatial and temporal domains. This approach effectively enabled PINNs to capture high and low frequencies in practical applications. However, while FF do not add more trainable parameters, they increase the model's hyper-parameters. In STMFF, hyper-parameters must be carefully chosen to approximate the target function accurately.

In this work, we examine the training dynamics of Wav-KANs through the lens of the NTK, an approach that, to our knowledge, has not been previously explored. Building on insights from **Rahaman, "Spectral Bias of Neural Networks"**, we theoretically demonstrate that the frequencies learned by the network can be precisely controlled by the frequency of the mother wavelet, allowing us to address spectral bias. Additionally, we provide empirical evidence that increasing the number of hidden units can achieve a similar effect to adjusting the mother waveletâ€™s frequency. This simplifies the model by eliminating the need for additional hyper-parameters to capture both low- and high-frequency components, unlike approaches such as Fourier features.

Our findings contribute theoretically and practically by presenting a framework that seamlessly integrates into the PINN structure. We empirically show that Wav-KINNs effectively mitigate spectral bias, positioning them as a promising alternative to traditional PINNs for complex, multi-frequency function approximations. This is the first study to combine NTK analysis with wavelet-driven frequency control in Wav-KANs, bridging theoretical insights and practical implementation for spectral bias mitigation in scientific machine-learning applications.