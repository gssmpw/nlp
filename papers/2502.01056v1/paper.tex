%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{stfloats}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{colortbl}
\definecolor{rank1}{HTML}{F7B79A}
\definecolor{deeper_rank1}{HTML}{FF9900}
\definecolor{rank2}{HTML}{87CEEB}
\definecolor{IFCD}{HTML}{D9ECFF}
\definecolor{Deeper_IFCD}{HTML}{7BABF8}
\usepackage{multirow}

% if you use cleveref.
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding}

\begin{document}

\twocolumn[
\icmltitle{Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{Corresponding author}{\dag}

\begin{icmlauthorlist}
\icmlauthor{Chao Wang}{Corresponding author,future,ai}
\icmlauthor{Xuancheng Zhou}{future,ai}
\icmlauthor{Weiwei Fu}{future,ai}
\icmlauthor{Yang Zhou}{Corresponding author,ai,auto}
\end{icmlauthorlist}

\icmlaffiliation{future}{School of Future Technology, Shanghai University, Shanghai, 200444, China.}
\icmlaffiliation{ai}{Institute of Artificial Intelligence, Shanghai University, Shanghai, 200444, China.}
\icmlaffiliation{auto}{School of Mechatronic Engineering and Automation, Shanghai, 200444, China}

\icmlcorrespondingauthor{Chao Wang}{cwang@shu.edu.cn}
% \icmlcorrespondingauthor{Xuancheng Zhou}{xuanchengz@shu.edu.cn}
% \icmlcorrespondingauthor{Weiwei Fu}{fuweiwei2001@shu.edu.cn}
\icmlcorrespondingauthor{Yang Zhou}{saber\_mio@shu.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlCorrespondingAuthor} % otherwise use the standard text.

\begin{abstract}
Large Visual Language Models (LVLMs) integrate visual and linguistic modalities, exhibiting exceptional performance across various multimodal tasks. Nevertheless, LVLMs remain vulnerable to the issue of object hallucinations. Previous efforts to mitigate this issue focus on supervised fine-tuning (SFT) or incorporating external knowledge, both of which entail significant costs related to training and the acquisition of external data. To address these challenges, we propose a novel model-agnostic approach termed Internal Fact-based Contrastive Decoding (IFCD), designed to mitigate and suppress hallucinations during the inference process of LVLMs by exploiting the LVLMs' own hallucinations. IFCD is grounded in experimental observations that alterations to the LVLMs' internal representations tend to amplify hallucinations caused by language bias. By contrasting disturbed distribution, IFCD calibrates the LVLMs' output and effectively removes the hallucinatory logits from the final predictions. Experimental results validate that IFCD significantly alleviates both object-level and attribute-level hallucinations while achieving an average 9\% accuracy improvement on POPE and 8\% accuracy improvement on MME object hallucinations subset compared with direct decoding, respectively.
\end{abstract}

\section{Introduction}
In recent years, significant advancements have been made in developing large vision-language models (LVLMs) \cite{li2023blip, wen2024road}, which exhibit exceptional capabilities across a broad spectrum of tasks ~\cite{achiam2023gpt}. These models are increasingly viewed as a step toward achieving artificial general intelligence \cite{sanderson2023gpt}. LVLMs are capable of extracting intricate complex visual information and transforming it into continuous language representations for generation ~\cite{liu2024visual, zhu2024minigpt}. However, a critical challenge that persists with LVLMs is the phenomenon of hallucinations. Before the era of LVLMs, the natural language processing (NLP) community defined hallucinations as generated textual content that deviates from actuality \cite{ji2023survey, biten2022let}. With the advancements of LVLMs, a new form of hallucination has emerged, known as object hallucination. This refers to the generation that are inconsistent with visual input, and it becomes a significant issue that impedes the deployment of LVLMs in domains that require high reliability, particularly in risk-sensitive industries \cite{sahoo2024comprehensive}.

\begin{figure}
\begin{center}
\centerline{\includegraphics[width=1\linewidth]{case_of_OH.pdf}}
\caption{Cases of object hallucinations and effect of IFCD on LLaVA 1.5. Given two images, an LLaVA 1.5 outputs responses with attribute and category hallucinations which IFCD fixes.}
\label{fig: case of OH}
\vskip -0.4in
\end{center}
\end{figure}

Object hallucinations refer to the phenomenon where the language output generated by an LVLM fails to align with the visual input content ~\cite{song2024hscl, min2024mitigating}. In Figure \ref{fig: case of OH}, the LVLM incorrectly assumes that bananas are present in the refrigerator and even provides a false location of the bananas. At the same time, the LVLM accurately counts the number of animals but misclassifies an animal that is not a bear as a bear in another example. These two examples highlight the object hallucinations issue in LVLMs, which severely limits their applicability in domains where high accuracy is essential \cite{zhang2023siren, pal-etal-2023-med}. Therefore, addressing the object hallucinations is a pivotal step toward enhancing the reliability of LVLMs and expanding their potential applications.

% LVLM may fabricate non-existent objects (i.e., attribute hallucination) or misrepresent animals (i.e., category hallucination) within the image. This issue highlights the statistical bias inherent in LVLMs \cite{agarwal2020towards, goyal2017making} and significantly limits their applicability in critical domains such as healthcare, automated systems, and robotics, the misinformation stemming from object hallucination can result in serious consequences \cite{zhang2023siren, laroi2014culture} in these fields. 

To address the issue of object hallucinations in LVLMs, numerous works focus on incorporating external information to support fact-checking ~\cite{zhao2024mitigating, asai2023self}, thereby enhancing the factual accuracy of LVLMs through techniques such as self-evaluation \cite{singhal2024multilingual}. Additionally, improving LVLMs performance through preference fine-tuning is a prevalent strategy \cite{rafailov2024direct, stiennon2020learning}, aiming to align model outputs with human preferences and enhance model performance at a fine-grained level. While existing interventions for mitigating object hallucinations in LVLMs have shown effectiveness, the associated human and computational costs highlight the urgent need for simpler yet effective approaches.

To address these challenges, we propose Internal Fact-based Contrastive Decoding (IFCD), a novel model-agnostic approach that leverages hallucinations to mitigate further hallucination. IFCD can be seamlessly integrated into any open-source LVLM with minimal training required for the probe model. IFCD significantly enhances the truthfulness of LVLMs while reducing object hallucination. To assess the effectiveness of IFCD, we conduct experiments on two widely adopted LVLMs, LLaVA 1.5 \cite{liu2024visual} and InstructBLIP \cite{li-etal-2023-lavis}. Our evaluation using the Polling-based Object Probing Evaluation (POPE) \cite{li-etal-2023-evaluating} demonstrates that IFCD consistently outperforms baseline approaches, achieving up to a 9\% improvement in performance across all LVLMs. Additionally, IFCD enhances the overall perceptual capabilities of LVLMs, as evidenced by benchmarking on MME \cite{fu2023mme} and LLaVA-Bench \cite{liu2024visual}. In the text generation task, IFCD reduces the hallucinated object ratio by 5\%, while preserving the generated text's quality.

Concretely, our main \textbf{contributions} are as follows:
\textbf{1).}~We analyze the impact of editing internal representations on object hallucinations in LVLMs, with a particular focus on the effects of language bias.
\textbf{2).}~We introduce IFCD, a novel technique to calibrate LVLMs' output distribution and mitigate object hallucinations by contrasting the disturbed distribution derived from internal representation editing.
\textbf{3).}~IFCD demonstrates the effect in mitigating object hallucination, achieving 9\% and 8\% improvement on POPE and MME, respectively, and 13\% improvement in suppressing hallucinatory object effects while being more robust in the long text generation experiments.

\section{Related Work}
\subsection{Large Visual Language Models}
In recent years, large language models (LLMs) based on the Transformer architecture have achieved remarkable achievements in various fields, including Natural Language Processing (NLP), Machine Translation, and Computer Vision. ~\cite{zhao2023survey, achiam2023gpt, chiang2023vicuna}. Notably, with the introduction of multimodal models such as CLIP ~\cite{radford2021learning} and Vision Transformer ~\cite{dosovitskiy2021an}, LVLMs have been established through comprehensive pre-training processes that unify textual and visual modalities ~\cite{bai2023qwen, Ye_2024_CVPR}. Compared with traditional vision models, LVLMs adopted more advanced training paradigms ~\cite{wei2022finetuned, liu2024visual}. As a result, LVLMs demonstrated unique capabilities not present in traditional models \cite{yang2023mm}, including establishing application \cite{Ye_2024_CVPR}, and advanced mathematical reasoning \cite{pmlr-v202-driess23a}.

\subsection{Object Hallucination}
While LVLMs exhibited strong capabilities in addressing vision-language tasks, they were still significantly affected by object hallucinations ~\cite{li-etal-2023-evaluating}, generating content irrelevant to visual information. To identify the issue of object hallucinations in LVLMs, recent research has established specific indicators, such as Caption hallucinations Assessment with Image Relevance (CHAIR) ~\cite{rohrbach-etal-2018-object} and Sharpness \cite{chen2024context}. Additionally, advances have been made in locating the causes of hallucinations within LVLMs, including internal representation and attention patterns ~\cite{han2024semantic, mahaut-etal-2024-factual}. These metrics and locating approaches provided a multi-dimensional view to observe object hallucination. 

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{pipeline.pdf}}
\caption{\textbf{An overview of IFCD.~} IFCD first edits the internal representation of the LVLMs to construct counterfactual logits for comparison by deliberately injecting hallucinations into the model trained by contrastive learning. These counterfactual logits are utilized to reveal potential hallucinatory tendencies of the LVLMs. Furthermore, the internal representation editing model is employed to actively attenuate a portion of the hallucinatory components within the LVLMs, thereby initiating an improvement in the factual accuracy of its outputs. This process effectively corrects the token from an erroneous token ``[Fork]'' to an accurate ``[Dog]''.}
\label{fig:pipe_line}
\vskip -0.3in
\end{center}
\end{figure*}

Addressing object hallucinations typically focused on direct suppression methods and fine-tuning, which involved actively limiting \cite{dhuliawala-etal-2024-chain}, correcting hallucinated outputs \cite{hu-etal-2024-knowledge} and RLHF to refine LVLMs ~\cite{ye2024mplug}. These approaches often constructed enhanced datasets for fine-tuning or training LVLMs. As the parameter scales of LVLMs continued to increase, these challenges became even more pronounced~\cite{Ye_2024_CVPR, zhu2024minigpt}. To tackle these issues, IFCD actively induces hallucinations into the model's output and leverages them as counterexamples to refine the model's final responses, thereby reducing the likelihood of hallucinations in final outputs. IFCD leverages hallucinated outputs as improving opportunities, offering a novel way to mitigate object hallucinations without high computation costs.


\section{Method}
\subsection{Overview}
In this section, we propose IFCD to mitigate object hallucinations in LVLMs effectively. IFCD constructs two distributions with a truthfulness gap via internal representation editing and mitigates object hallucinations using contrastive decoding to subtract hallucinatory distributions. Section \ref{3.2} details the editing process, while Section \ref{3.3} explains the contrastive decoding mechanism. The overall framework is illustrated in Figure \ref{fig:pipe_line}.

\subsection{Amplifying Object hallucinations via Internal Representation Editing} \label{3.2}
\textbf{Induction of Object hallucinations   } A substantial proportion of object hallucinations arises from statistical bias \cite{zhao2024mitigating}, for which contrastive decoding has emerged as an effective countermeasure \cite{chenhalc, min2024mitigating}. The methodology originates from contrastive decoding \cite{li-etal-2023-contrastive}, which subtracts logits that deviate from expected distributions, thereby enhancing the performance of LVLMs. Existing approaches reveal LVLMs' tendency toward object hallucinations by confusing the visual input \cite{leng2024mitigating} or instructing LVLMs to make incorrect decisions \cite{wang-etal-2024-mitigating} and mitigate hallucinations via contrastive decoding. Therefore, a critical question arises: \textit{How can we create hallucination-inducing samples that reflect token distribution errors while generating significant hallucination?}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table*}[hb!]
% \centering
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{l|lllll}
% \hline
% Method                          & BLEU                & ROUGE-L              & CIDEr                 & $\text{CHAIR}_s$↑                    & CHAIRi↑                                    \\ \hline
% Instruction Confusion~\cite{wang-etal-2024-mitigating}           & 12.64                         & 18.39 & 0.0657                & \cellcolor{rank2}34            & \cellcolor{rank2}11.8                                                \\
% Noisy Image~\cite{leng2024mitigating}                     & 4.82                          & 5.53                          & -                              & 1                                   & -                \\
% Internal Representation Editing~\cite{truthx} & 11.77                         & 17.38                         & 0.0232                         & \cellcolor{rank1}\textbf{39}        & \cellcolor{rank1} 13.9 \\ \hline
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \caption{The comparison among three methods to induce object hallucination on LLaVA 1.5. Due to the purpose of identifying the capability in hallucination inducing, the CHAIR indicators are expected to be higher rather than lower. The best one is marked by bold and \colorbox{rank1}{\textbf{orange}}. The second one is marked by \colorbox{rank2}{\textbf{cyan}}. The CIDEr and CHAIRi scores of noisy image failed to be calculated due to undesirable LVLM outputs.}
% \label{tab:hallu-induced}
% \end{table*}

We argue that previous methods relying on distracting information are suboptimal for inducing object hallucinations in LVLMs. These approaches primarily highlight the models' reactions to specific commands or perturbations. Although these responses approximate object hallucination, they stem from exogenous factors rather than inherent model errors and hardly fully represent LVLMs' object hallucination. To better simulate logits indicative of object hallucination, we propose intervening in the internal representation of LVLMs during inference.

\textbf{Introduction of Internal Representation Editing } Intuitively, the internal representations from the attention and feedforward layers directly contribute to the inference process, thereby influencing the model's output. This section delves into an analysis aiming to validate the hypotheses that editing internal representation can amplify object hallucinations in LVLMs. There are various methods to intervene in internal representations to alter output truthfulness \cite{pan2024towards, chen2024incontext}. Considering compatibility and availability, we propose using TruthX \cite{truthx} to edit the internal representations of LVLMs. TruthX is an autoencoder-based model comprising two encoders, $\mathrm{TruthEnc(\cdot)}$ and $\mathrm{SemEnc(\cdot)}$, and a decoder $\mathrm{Dec(\cdot)}$, all implemented with multi-layer perceptions (MLPs). Two encoders map LVLMs' internal representations $x$ as follows:
\begin{equation}
    h_{\textit{truth}}=\mathrm{TruthEnc}(x), h_{\textit{sem}}=\mathrm{SemEnc}(x),
\end{equation}
where $h_{\textit{truth}}$, $h_{\textit{sem}}\in \mathbb{R}^{d_{latent}}$ are the latent representations in the latent spaces of $\mathrm{TruthEnc(\cdot)}$ and $\mathrm{SemEnc(\cdot)}$, $d_{latent}$ is the dimension of latent space. Then decoder $\mathrm{Dec(\cdot)}$ reconstructs LVLM internal representation:
\begin{equation}
x^\prime=\mathrm{Dec}(h_{\textit{sem}}+\mathrm{Attn}(h_\textit{sem},h_\textit{truth})),
\end{equation}
where $x^\prime$ is the reconstructed representation and $\mathrm{Attn(\cdot)}$ is an attention operation from semantic latent representation to truthful latent representation.
% \textcolor{red}{The encoders are designed to extract truthful and semantic information from LVLM internal representations through distinct contrastive learning objectives.} The \textcolor{blue}{$\mathrm{TruthEnc(\cdot)}$} and \textcolor{blue}{$\mathrm{SemEnc(\cdot)}$} encoders, responsible for truthful information and semantic information respectively, achieve their learning objectives by contrastive learning. During training, \textcolor{blue}{$\mathrm{TruthEnc(\cdot)}$} captures the subspace distributions of truthful and untruthful information within LVLM internal representations. By modifying the directional vectors in the encoded latent space, it edits the truthful properties of LVLM representations. Simultaneously, the SemEnc encoder maintains the semantic state of the edited subspace vectors, preventing substantial degradation of the model's language capabilities after editing. After processing via attention mechanisms and passing through the decoder, the internal representations are reconstructed and reintegrated into the LVLM's inference pipeline.

% \subsection{Amplifying Object Hallucination via Internal Representation Editing}
\textbf{Internal Representation Editing Amplifies Object hallucinations  }
Through contrastive learning, the truthfulness of the LVLMs' internal representation can be discerned within $\mathrm{TruthEnc(\cdot)}$. The editing procedure is subsequently determined by the relative positions of the central point of the latent space vectors corresponding to truthful and untruthful representations mapped by $\mathrm{TruthEnc(\cdot)}$. Formally, the direction $\delta \in \mathbb{R}^{d_{latent}}$ of internal representation editing can be defined as follows:
\begin{equation}
\delta = \overline{\mathcal{H}}^{pos}_{truth} - \overline{\mathcal{H}}^{neg}_{truth},
\end{equation}
where $\overline{\mathcal{H}}^{pos}_{truth}$ and $\overline{\mathcal{H}}^{neg}_{truth}$ denotes the average position of mappings of truthful and untruthful representations within the latent space of $\mathrm{TruthEnc(\cdot)}$. Note that the determination of editing direction $\delta$ happens during the training stage of TruthX. Adjustments to truthfulness can be reversed if the opposite direction $-\delta$ is used.

In the inference process of the LVLMs, TruthX maps and edits the internal representation in the latent space of $\mathrm{TruthEnc(\cdot)}$, then reconstructs the internal representation. Through contrasting the difference between the original and reconstructed internal representation, the content of the modifications to the internal representation $\Delta \in \mathbb{R}^{d_{model}}$ can be effectively derived, where $d_{model}$ refers to the dimension of LVLMs internal representations.

Then, the internal representation $x$ of LVLMs is edited as follows formally:
\begin{equation}
\hat{x} = x + \gamma \times \Delta,
\end{equation}
where $\gamma$ denotes the editing strength and $\hat{x}$ is the reconstructed internal representation of LVLMs. In practice, it is not necessary to edit all attention and feedforward layers. Modifying the layers that are sensitive to factual differences alone can yield substantial change in object hallucination.

% We compare three methods for inducing object hallucinations in LVLMs: instruction confusion \cite{wang-etal-2024-mitigating}, image processing \cite{leng2024mitigating}, and internal representation editing\cite{truthx} with CHAIRs and CHAIRi metrics \cite{rohrbach-etal-2018-object} and supporting text quality indicators in Table \ref{tab:hallu-induced}. Among these, editing the internal representations yielded the most significant results, achieving optimal and consistent performance on the CHAIRs and CHAIRi, which denotes the optimal hallucination-inducing capacity. \textcolor{blue}{While the noisy image method causes a severe degradation in the LVLM's text generation quality, which results in failure of calculating CIDEr and CHAIRi.}

We compare the effect of editing the internal representation of LVLMs to expose object hallucinations preference with two alternative interference methods, using the case of recognizing black strawberries on LLaVA 1.5 in Figure \ref{fig:hallu-induced}. For Instruction Dirturbance, we follow Instruction Contrastive Decoding \cite{wang-etal-2024-mitigating}, utilizing the prompt ``\texttt{You are a confused object detector.}'' to induce hallucination. For visual disturbance, we adopt the methodology from Visual Contrastive Decoding \cite{leng2024mitigating}, setting the noise step parameter to 400, which controls the scale of noise added. Figure \ref{fig:hallu-induced} presents editing the internal representation enables the LVLMs to disregard visual information and disproportionately rely on language priors in its decision-making process. Additionally, increasing the strength of the internal representation modification can further expose the statistical bias in the responses.
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
 \centerline{\includegraphics[width=1\linewidth]{hallucination-inducing-detailed.pdf}}
\caption{An illustration of editing internal representation amplifying language priors. Given an image depicting three black strawberries, LVLMs assign more preference for more conventional strawberry color, such as ``red'', with increasing editing strength.}
\label{fig:hallu-induced}
\end{center}
\vskip -0.3in
\end{figure}
\subsection{Internal Fact-based Contrastive Decoding} \label{3.3}
\textbf{Contrasting the Predictions with Disturbance    } The findings from our previous analyses substantiate the hypothesis that \textit{manipulating the internal representations of LVLMs can exacerbate object hallucination, thereby making hallucinatory content more strongly reflective of untruthful information}. A promising approach to mitigate object hallucinations of LVLMs is to directly subtract the logits associated with hallucinatory content from the final output logits, thereby enabling a more targeted mitigation of object hallucination. Building upon this view, we introduce IFCD aimed at alleviating hallucinations during LVLMs inference.

Drawing from the concept of contrastive decoding \cite{li-etal-2023-contrastive}, which enhances the overall quality of LLM outputs by comparing logits from two models with performance discrepancies, we contrast the generations from the hallucination-inducing and hallucination-suppressing models to improve final performance. Specifically, as demonstrated in Figure \ref{fig:pipe_line}, given the visual features $X_v$ extracted from the visual encoder and the textual query $X_q$, our method computes two distinct token distributions: the first distribution $P^+$ is derived from the LVLM edited for anti-hallucination. In contrast, the second distribution $P^-$ is generated from the LVLM after modification of its internal representations for hallucination-inducing. In contrast to the regular approach of selecting the token with the highest probability, our method relies on the two token distributions, $P^+$ and $P^-$, to inform the final token selection. The contrasting token probability distribution is computed by evaluating the difference between $P^+$ and $P^-$ as follows: 
\begin{equation}
\begin{split}
p_{\text{IFCD}}(y_t|x_v, x_q) =  \sigma\left((1 + \alpha)p^{+}(y_t|\ast) - \alpha p^{-}(y_t|\ast)\right),
\end{split}
\label{eq:IFCD}
\end{equation}
where $\ast$ denotes visual and textual information given to LVLMs as well as previously generated tokens, while $\alpha$ is employed to control the contrast strength, and $y_t$ refers to the token generated in $t$-th position.

\textbf{Adaptive Plausibility Constraints   } The fundamental principle of IFCD is to prioritize the selection of tokens with high probabilities as predicted by the LVLMs, while simultaneously imposing penalties on those tokens associated with hallucinatory logits. However, this approach risks inadvertently affecting tokens that are correctly identified both under standard conditions and hallucinated content. Imposing penalties on these tokens may inadvertently reward unreliable tokens that should not be prioritized, potentially distorting the LVLMs’ output. To mitigate this issue, we introduce constraints on the scope of influence exerted by IFCD, drawing upon adaptive plausibility constraints that are employed in the open-ended text generation realm ~\cite{li-etal-2023-contrastive}.
\begin{equation}
\begin{split}
y_t\sim P_{\text{IFCD}} \textit{, s.t. } y_t \in \mathcal{V}_{head}(y_{<t}),
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\mathcal{V}_{head}(y_{<t})=  \{y_t \in \mathcal{V}: p(y_t|\ast) \geq \beta \max_{[\text{T}]}p([\text{T}]|\ast) \},
\end{split}
\end{equation}
where $[\text{T}]$ refers to the candidate tokens, while the pivotal hyperparameter $\beta$ serves to regulate the truncation strength of the logits, thereby determining the tokens affected during the contrastive decoding process. This parameter is crucial for constraining the impact on irrelevant tokens, thereby ensuring the robustness of the contrastive decoding process.

\section{Experiments} \label{4}

\begin{table*}[ht!]
\caption{Results on object hallucinations benchmark POPE. Regular denotes direct sampling, whereas ICD and VCD are two baselines for comparison, and IFCD is our proposed decoding. The best performance is marked by \textbf{bold}, and performance of IFCD is marked by \textcolor{Deeper_IFCD}{cyan}.}
\begin{center}
\begin{small}
\begin{sc}

\resizebox{\textwidth}{!}{
\begin{tabular}{lllcccc|cccc}
\hline
                          &                               &                            & \multicolumn{4}{c|}{LLaVA 1.5}                                                                                                                           & \multicolumn{4}{c}{InstructBLIP}                                                                                                                         \\ \cline{4-11} 
\multirow{-2}{*}{Dataset} & \multirow{-2}{*}{Setting}     & \multirow{-2}{*}{Decoding} & \multicolumn{1}{l}{Accuracy}           & \multicolumn{1}{l}{Precision}          & \multicolumn{1}{l}{Recall}    & \multicolumn{1}{l|}{F1 Score}          & \multicolumn{1}{l}{Accuracy}           & \multicolumn{1}{l}{Precision}          & \multicolumn{1}{l}{Recall}    & \multicolumn{1}{l}{F1 Score}           \\ \hline
                          &                               & Regular                    & 83.29                                  & 92.13                                  & 72.80                         & 81.33                                  & 80.71                                  & 81.67                                  & 79.19                         & 80.41                                  \\
                          &                               & ICD                        & 84.23                                  & \textbf{95.08}                         & 72.20                         & 82.08                                  & 83.50                                  & 87.69                                  & 77.93                         & 82.52                                  \\
                          & \multirow{-2}{*}{Random}      & VCD                        & 87.73                                  & 91.42                                  & \textbf{83.28}                & 87.16                                  & 84.53                                  & 88.55                                  & \textbf{79.32}                & 83.68                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{89.17} & \cellcolor{IFCD}94.54          & \cellcolor{IFCD}83.13 & \cellcolor{IFCD}\textbf{88.47} & \cellcolor{IFCD}\textbf{85.56} & \cellcolor{IFCD}\textbf{97.09} & \cellcolor{IFCD}73.33 & \cellcolor{IFCD}\textbf{83.75} \\ \cline{3-11} 
                          &                               & Regular                    & 81.88                                  & 88.93                                  & 72.8                          & 80.06                                  & 78.22                                  & 77.87                                  & 78.85                         & 78.36                                  \\
                          &                               & ICD                        & 82.73                                  & 91.47                                  & 72.20                         & 80.70                                  & 79.40                                  & 80.28                                  & 77.93                         & 79.09                                  \\
\multirow{-2}{*}{MSCOCO}  & \multirow{-2}{*}{Popular}     & VCD                        & 85.38                                  & 86.92                                  & \textbf{83.28}                & 85.06                                  & 81.47                                  & 82.89                                  & \textbf{79.32}                & 81.07                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{88.10} & \cellcolor{IFCD}\textbf{93.13} & \cellcolor{IFCD}82.27 & \cellcolor{IFCD}\textbf{87.36} & \cellcolor{IFCD}\textbf{83.27} & \cellcolor{IFCD}\textbf{91.93} & \cellcolor{IFCD}72.93 & \cellcolor{IFCD}\textbf{81.34} \\ \cline{3-11} 
                          &                               & Regular                    & 78.96                                  & 83.06                                  & 72.75                         & 77.57                                  & 75.84                                  & 74.30                                  & 79.03                         & 76.59                                  \\
                          &                               & ICD                        & 80.23                                  & 85.96                                  & 72.27                         & 78.52                                  & 77.70                                  & 77.57                                  & 77.93                         & 77.75                                  \\
                          & \multirow{-2}{*}{Adversarial} & VCD                        & 80.88                                  & 79.45                                  & \textbf{83.29}                & 81.33                                  & 79.56                                  & 79.67                                  & \textbf{79.39}                & 79.52                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{85.17} & \cellcolor{IFCD}\textbf{86.76} & \cellcolor{IFCD}83.00 & \cellcolor{IFCD}\textbf{84.84} & \cellcolor{IFCD}\textbf{82.23} & \cellcolor{IFCD}\textbf{89.47} & \cellcolor{IFCD}73.07 & \cellcolor{IFCD}\textbf{80.44} \\ \hline
                          &                               & Regular                    & 83.45                                  & 87.24                                  & 78.36                         & 82.56                                  & 80.91                                  & 77.97                                  & 86.16                         & 81.86                                  \\
                          &                               & ICD                        & 86.13                                  & \textbf{91.44}                         & 79.73                         & 85.19                                  & 82.83                                  & 82.42                                  & 83.46                         & 82.94                                  \\
                          & \multirow{-2}{*}{Random}      & VCD                        & 86.15                                  & 85.18                                  & \textbf{87.53}                & 86.34                                  & 84.11                                  & 82.21                                  & \textbf{87.05}                & 84.56                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{87.30} & \cellcolor{IFCD}89.54          & \cellcolor{IFCD}84.47 & \cellcolor{IFCD}\textbf{86.93} & \cellcolor{IFCD}\textbf{85.83} & \cellcolor{IFCD}\textbf{93.10} & \cellcolor{IFCD}77.40 & \cellcolor{IFCD}\textbf{84.58} \\ \cline{3-11} 
                          &                               & Regular                    & 79.90                                  & 80.85                                  & 78.36                         & 79.59                                  & 76.19                                  & 72.16                                  & 85.28                         & 78.17                                  \\
                          &                               & ICD                        & 82.5                                   & \textbf{84.40}                         & 79.73                         & 82.00                                  & 77.23                                  & 74.21                                  & 83.46                         & 78.56                                  \\
\multirow{-2}{*}{A-OKVQA} & \multirow{-2}{*}{Popular}     & VCD                        & 81.85                                  & 78.60                                  & \textbf{87.53}                & 82.82                                  & 79.80                                  & 76.00                                  & \textbf{87.05}                & 81.15                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{84.10} & \cellcolor{IFCD}84.17          & \cellcolor{IFCD}84.00 & \cellcolor{IFCD}\textbf{84.08} & \cellcolor{IFCD}\textbf{83.17} & \cellcolor{IFCD}\textbf{87.21} & \cellcolor{IFCD}77.73 & \cellcolor{IFCD}\textbf{82.20} \\ \cline{3-11} 
                          &                               & Regular                    & 74.04                                  & 72.08                                  & 78.49                         & 75.15                                  & 70.71                                  & 65.91                                  & 85.83                         & 75.56                                  \\
                          &                               & ICD                        & 76.70                                  & \textbf{75.08}                         & 79.93                         & 77.43                                  & 72.20                                  & 68.07                                  & 83.6                          & 75.04                                  \\
                          & \multirow{-2}{*}{Adversarial} & VCD                        & 74.97                                  & 70.01                                  & \textbf{87.36}                & 77.73                                  & 74.33                                  & 69.46                                  & \textbf{86.87}                & 77.19                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{77.67} & \cellcolor{IFCD}74.73          & \cellcolor{IFCD}83.60 & \cellcolor{IFCD}\textbf{78.91} & \cellcolor{IFCD}\textbf{77.97} & \cellcolor{IFCD}\textbf{77.99} & \cellcolor{IFCD}77.93 & \cellcolor{IFCD}\textbf{77.96} \\ \hline
                          &                               & Regular                    & 83.73                                  & 87.16                                  & 79.12                         & 82.95                                  & 79.65                                  & 77.14                                  & 84.29                         & 80.56                                  \\
                          &                               & ICD                        & 86.10                                  & 90.38                                  & 80.80                         & 85.32                                  & 82.30                                  & 81.94                                  & 82.87                         & 82.40                                  \\
                          & \multirow{-2}{*}{Random}      & VCD                        & 86.65                                  & 84.85                                  & \textbf{89.24}                & 86.99                                  & 83.69                                  & 81.84                                  & \textbf{86.61}                & \textbf{84.16}                         \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{87.97} & \cellcolor{IFCD}\textbf{90.94} & \cellcolor{IFCD}84.33 & \cellcolor{IFCD}\textbf{87.51} & \cellcolor{IFCD}\textbf{84.77} & \cellcolor{IFCD}\textbf{92.50} & \cellcolor{IFCD}75.67 & \cellcolor{IFCD}83.24          \\ \cline{3-11} 
                          &                               & Regular                    & 78.17                                  & 77.64                                  & 79.12                         & 78.37                                  & 73.87                                  & 69.63                                  & 84.69                         & 76.42                                  \\
                          &                               & ICD                        & 80.00                                  & \textbf{79.53}                                  & 80.80                         & 80.16                                  & 74.70                                  & 71.23                                  & 82.87                         & 76.61                                  \\
\multirow{-2}{*}{GQA}     & \multirow{-2}{*}{Popular}     & VCD                        & \textbf{80.73}                         & 76.26                                  & \textbf{89.24}                & \textbf{82.24}                         & 78.57                                  & 74.62                                  & \textbf{86.61}                & \textbf{80.17}                         \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}79.76          & \cellcolor{IFCD}77.61 & \cellcolor{IFCD}83.67 & \cellcolor{IFCD}80.52          & \cellcolor{IFCD}\textbf{80.13} & \cellcolor{IFCD}\textbf{82.90} & \cellcolor{IFCD}75.93 & \cellcolor{IFCD}79.26          \\ \cline{3-11} 
                          &                               & Regular                    & 75.08                                  & 73.19                                  & 79.16                         & 76.06                                  & 70.56                                  & 66.12                                  & 84.33                         & 74.12                                  \\
                          &                               & ICD                        & 77.47                                  & 76.08                                  & 80.13                         & 78.05                                  & 72.27                                  & 68.43                                  & 82.67                         & 74.88                                  \\
                          & \multirow{-2}{*}{Adversarial} & VCD                        & 76.09                                  & 70.83                                  & \textbf{88.75}                & 78.78                                  & 75.08                                  & 70.59                                  & \textbf{85.99}                & 77.53                                  \\
                          &                               & \textbf{IFCD (Ours)}       & \cellcolor{IFCD}\textbf{79.03} & \cellcolor{IFCD}\textbf{76.57} & \cellcolor{IFCD}83.67 & \cellcolor{IFCD}\textbf{79.96} & \cellcolor{IFCD}\textbf{78.00} & \cellcolor{IFCD}\textbf{79.49} & \cellcolor{IFCD}75.47 & \cellcolor{IFCD}\textbf{77.62} \\ \hline
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
    \label{tab:POPE_EXP}
\end{table*}
\subsection{Experiments Settings}
\textbf{Benchmarks  } 
% POPE \cite{li-etal-2023-evaluating} is sourced from three widely used vision-language task datasets: MSCOCO, A-OKVQA, and GQA, serves as a benchmark containing 27,000 image-query pairs for evaluating object hallucinations in LVLMs. POPE employs three sampling strategies, random selection (random), selection based on appearance times (popular), and selection of objects with a high frequency of co-occurrence with actual existing objects (adversarial).
\textbf{POPE} \cite{li-etal-2023-evaluating}  comprises 1,500 images from three sources—MSCOCO, A-OKVQA, and GQA—along with 27,000 associated questions, focusing on detecting object existence hallucination. It also incorporates three sampling methods—random, popular, and adversarial—to evaluate the robustness of LVLMs against object hallucinations driven by statistical biases.
\textbf{MME}~\cite{fu2023mme} provides a total of 14 perception and cognition tasks. Since the cognitive task is related to the language decoder's reasoning capacity rather than visual content comprehension, we choose the perceptual subset of MME as the benchmark.  
% \textcolor{blue}{The Multimodal Large Language Model Evaluation benchmark (MME) \cite{fu2023mme} comprises fourteen tasks, divided into perceptual and cognitive tasks, assessing LVLMs' visual observation and verbal comprehension abilities, respectively.}
Among these tasks, \textit{existence, count, position, and color} tasks are specifically designed as hallucinations discrimination benchmarks. \textbf{MSCOCO} \cite{MSCOCO} is a widely used computer vision benchmark, which contains more than 200,000 manually labeled high-quality and complex image-captions pairs, therefore very suitable for evaluating the object hallucinations problem. We randomly selected 500 images from MSCOCO to validate the long text generation ability of our method. \textbf{LLaVA-Bench} \cite{liu2024visual} contains 24 images with 60 questions, and the images cover a range of content such as portraits, landscapes, and enigmatic causes. We conduct a case study with this dataset to qualitatively demonstrate the effectiveness of our proposed IFCD.

\textbf{Metrics } The POPE evaluation pivots four key metrics: Accuracy, Precision, Recall, and the F1 score. To MME, we quantify performance via official implementation, the combined metric of accuracy and accuracy+. MSCOCO tests text generation capacity, which is quantified by BLEU \cite{papineni-etal-2002-bleu} and CHAIR \cite{rohrbach-etal-2018-object}. Specifically, CHAIR contains two sub-metrics $\text{CHAIR}_i$ and $\text{CHAIR}_s$, for object-focused and sentence-focused levels respectively. Formally, $\text{CHAIR}_i$ and $\text{CHAIR}_s$ could be described as follows:
\begin{equation}
\begin{split}
    &\text{CHAIR}_i = \frac{|\{\text{hallucinated objects}\}|}{|\{\text{all objects mentioned}\}|}, \\
    \text{CHAIR}_s& = \frac{|\{\text{sentences with hallucinated object}\}|}{|\{\text{all sentences}\}|},
\end{split}
\end{equation}
\textbf{LVLMs Baselines } We evaluate the effectiveness of IFCD on two popular LVLMs, LLaVA 1.5 \cite{liu2024visual} and InstructBLIP \cite{li-etal-2023-lavis}, configured with Vicuna 7B as the language decoder. Additionally, we reproduce the widely recognized Visual Contrastive Decoding (VCD) \cite{leng2024mitigating} and Instruction Contrastive Decoding (ICD) \cite{wang-etal-2024-mitigating} as comparisons with IFCD. Through comprehensive experiments, we demonstrate that IFCD is model-agnostic and can be seamlessly integrated with various LVLM architectures.

\textbf{Implementation Details  } \label{implementation_detail} In the experiments, we follow all hyperparameter settings in Appendix \ref{apex: param} unless otherwise noted. We use 300 MSCOCO images paired with both correct and incorrect responses as the training dataset for TruthX. We demonstrate and discuss the performance of IFCD with different training sizes in Appendix \ref{training_size_apex}. When adjusting the internal representations of LVLMs, we modify only the top 15 most important layers, with the editing strength $s=0.5$. It is worth noting that all experiments were conducted on a single RTX 3090 (24GB), highlighting the low cost of implementing IFCD.

% For VCD, we follow \cite{leng2024mitigating}, set temperature equal to 1, contrastive strength equal to 1, and noise step equal to 999 for POPE and 500 for other tasks.

\subsection{Experimental Results}

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.78\linewidth]{llava_perception_task3_.pdf}}
\caption{Perception subset of MME results on LLaVA 1.5. Regular denotes the direct sampling method, whereas ICD refers to the Instruction Contrastive Decoding, VCD refers to the Visual Contrastive Decoding baseline and IFCD is a sampling from our proposed contrastive decoding.}
\label{fig:mme_tasks}
\end{center}
\vskip -0.3in
\end{figure}

\textbf{Results on POPE~}
We use POPE to assess whether LVLMs misinterpret image objects. By applying different sampling methods in POPE, we can further evaluate the effectiveness of various methods in addressing statistical bias. As shown in Table \ref{tab:POPE_EXP}, two LVLMs employed IFCD achieve average accuracy improvements of up to 6.22 and 7.69, respectively, and F1 score improvements of up to 7.14 and 7.62, respectively, compared to the direct decoding, demonstrating the effectiveness of IFCD. A notable observation is that \textit{the different POPE setting varies the performance of all LVLMs with all methods, which could confirm the research that statistic bias is a cause of object hallucination} \cite{zhou2023analyzing}. However, among the four decoding methods, the performance degradation from the random setting to the adversarial setting is smaller for IFCD compared to the others. While direct decoding degrades by 7.8, and VCD and ICD degrade by 8.7 and 8.1, respectively, IFCD performs only a 6.8 degradation on average. The performance improvement and smaller performance degradation scale show the effectiveness and stability of IFCD. 

\textbf{Results on MME~} 
To evaluate LVLMs on diverse perceptual tasks, complement POPE’s focus on object existence, and enable a broader performance assessment, we experiment on MME. As shown in Figure \ref{fig:mme_tasks}, there is a general improvement in perception-related tasks with the application of IFCD. Table \ref{tab:subset_mme_blip} provides a more detailed comparison within the subset of object hallucination tasks in the MME. IFCD proves effective in attenuating the overall hallucination rate of the LVLMs, resulting in a 7.6\% and 13.8\% increase in the total score of InstructBLIP and LLaVA 1.5 respectively. Specifically, there is a notable increase in count and color tasks, suggesting that LVLMs are particularly susceptible to mistakes in these areas. Editing the internal representations proves to be an effective method for inducing hallucinations in these contexts. Therefore these errors can be mitigated through contrastive decoding to address object hallucination. In contrast, the position score is relatively low across four metrics, with minimal uplift from IFCD, suggesting the relatively weak ability of LVLMs in position reasoning. We demonstrate and discuss the performance of MME in more detail in Appendix \ref{appendix_mme}.
\begin{table}[h]
\vskip -0.1in
\caption{Results on object hallucination subset of MME on three decoding methods. The champion is marked by \textbf{bold} and \textbf{\textcolor{rank1}{orange}}, and the runner-up is marked by \textcolor{rank2}{cyan}.}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{llcccc|c}
\hline
Model                          & Decoding & \multicolumn{2}{c}{Object-level}                                              & \multicolumn{2}{c|}{Attribute-level}                                            & \multicolumn{1}{l}{Total Scores}       \\
                               &          & \multicolumn{1}{l}{Existence}        & \multicolumn{1}{l}{Count}              & \multicolumn{1}{l}{Position}           & \multicolumn{1}{l|}{Color}             & \multicolumn{1}{l}{}                   \\ \hline
                               & Regular  & 180                                  & 73.3                                   & 76.7                                   & 108.3                                  & 438.3                                  \\
                               & ICD      & 180                                  & \cellcolor{rank2}80             & \cellcolor{rank1}\textbf{80}    & \cellcolor{rank2}130.3          & \cellcolor{rank2}470.3          \\
\multirow{-2}{*}{InstructBLIP} & VCD      & \cellcolor{rank1}\textbf{190} & 65                                     & 58.3                                   & 130                                    & 443.3                                  \\
                               & IFCD (Ours)    & \cellcolor{rank2}185          & \cellcolor{rank1}\textbf{85}    & \cellcolor{rank2}63.3           & \cellcolor{rank1}\textbf{138.3} & \cellcolor{rank1}\textbf{471.6} \\ \cline{2-7} 
                               & Regular  & 180                                  & 123.3                                  & 105                                    & 158.3                                  & 566.6                                  \\
                               & ICD      & 180                                  & 123.3                                  & 110                                    & 158.3                                  & 571.6                                  \\
\multirow{-2}{*}{LLaVA 1.5}    & VCD      & 180                                  & \cellcolor{rank2}125            & \cellcolor{rank2}115            & 153.3                                  & \cellcolor{rank2}573.3          \\
                               & IFCD (Ours)    & \cellcolor{rank1}\textbf{185} & \cellcolor{rank1}\textbf{163.3} & \cellcolor{rank1}\textbf{133.3} & \cellcolor{rank1}\textbf{163.3} & \cellcolor{rank1}\textbf{644.9} \\ \hline
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\label{tab:subset_mme_blip}
\end{table}

\textbf{Results on MSCOCO~}
We select MSCOCO for the text generation task due to its diverse, multidomain content, which better reflects LVLMs' susceptibility to object hallucination. This provides a distinct evaluation from the yes/no tasks of POPE and MME. Table \ref{tab:caption} compares IFCD with baseline methods on the image caption generation task with the prompt: ``\texttt{Please describe this image in detail.}''. IFCD significantly reduces the proportion of hallucinated objects and sentences, with average reductions of 4.5\% and 21.6\% compared with direct decoding, respectively. Meanwhile, the quality of text generation in the LVLM remains at an average level, indicating that IFCD effectively balances hallucination mitigation with maintaining text generation quality.

\begin{figure*}
\begin{center}
\subfigure[InstructBLIP]{   \includegraphics[width=0.5\linewidth]{ablation_max_token_blip.pdf}}\subfigure[LLaVA 1.5]{
\includegraphics[width=0.5\linewidth]{ablation_max_token_llava.pdf}
}
\vskip -0.1in
\caption{Comparison IFCD and regular decoding on the ratio of hallucination objects ($\text{CHAIR}_i$) with respect to the number of max tokens. IFCD maintains a low ratio of hallucination objects while increasing the number of objects.}
\label{fig:ablation_max_token}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{table}[h]
\vskip -0.1in
\caption{Results of InstructBLIP and LLaVA 1.5 on MSCOCO to demonstrate the capacity of IFCD in long-form text generation. \textbf{Bold} and \textcolor{rank1}{\textbf{orange}} indicate the best resutls.}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{llccc}
\hline
Model                          & Method      & \multicolumn{1}{l}{$\text{CHAIR}_s$↓}            & \multicolumn{1}{l}{$\text{CHAIR}_i$↓}            & \multicolumn{1}{l}{BLEU↑}              \\ \hline
                               & Regular     & 48                                    & 13.9                                  & \cellcolor{rank1}\textbf{9.8}  \\
                               & ICD         & 58.2                                  & 18.5                                  & 8.4                                   \\
\multirow{-2}{*}{InstructBLIP} & VCD         & 54.8                                  & 16.2                                  & 9.1                                   \\
                               & IFCD (Ours) & \cellcolor{rank1}\textbf{39.6} & \cellcolor{rank1}\textbf{11.2} & 8.4                                   \\ \cline{2-5} 
                               & Regular     & 20                                    & 15.2                                  & 9                                     \\
                               & ICD         & 50.8                                  & 16.9                                  & 8.5                                   \\
\multirow{-2}{*}{LLaVA 1.5}    & VCD         & 21.8                                  & 11                                    & \cellcolor{rank1}\textbf{10.8} \\
                               & IFCD (Ours) & \cellcolor{rank1}\textbf{13.2} & \cellcolor{rank1}\textbf{5.6}  & 8                                     \\ \hline
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\label{tab:caption}
\end{table}

Furthermore, we also investigate the performance of IFCD  under different maximum generation length settings. Figure \ref{fig:ablation_max_token} illustrates the number of objects generated (dashed line) and the ratio of hallucinated objects (solid line) for 100 randomly selected images from the MSCOCO 2014 validation split. This experiment provides a comprehensive evaluation of IFCD's robustness. Intuitively, an increase in the number of generated objects would typically correspond to a parallel increase in the hallucinatory objects. However, IFCD effectively maintains a low level of object hallucination ratios, even as the number of generated objects continues to grow. These results highlight the generalizability and effectiveness of IFCD in diverse task types, including truthfulness classification and text generation.

\textbf{Case Study on LLaVA-Bench   } \label{llava-bench}
% \begin{figure}[h]
% \begin{center}
%     \includegraphics[width=1\linewidth]{llava-bench_case.pdf}
%     \vskip -0.1in
%     \caption{A case of the mitigating hallucination in image description. Hallucinatory objects and sentences that appear in the direct decoding and VCD of the LVLMs are marked by \textcolor{red}{red}.}
%     \label{fig:blip_llava_bench}
% \end{center}
% \vskip -0.2in
% \end{figure}
We conduct a qualitative experiment on LLaVA-Bench to demonstrate the performance of IFCD directly. The results of LLaVA-Bench are shown in Appendix \ref{llava_bench_apex} due to the space limits. In each case, we compare baseline methods and IFCD. Then the hallucination contents are marked by red. It is worth noting that IFCD provides rich information while reducing the ratio of hallucinated objects, confirming its robustness. 

\section{Analysis and Ablation Studies} \label{sec: analyse}
\textbf{Ablation Study  } 
The core of implementing IFCD lies in identifying the two token distributions employed for contrast, which must have a gap in the hallucination level. This enables the contrastive decoding to effectively subtract the high hallucination level portion of the distribution, thereby mitigating the object hallucinations in the final token distribution. In IFCD, we designate the distribution that undergoes anti-hallucinations as $P^+$ and the distribution outputted by hallucination-inducing as $P^-$. To validate the effectiveness and stability of IFCD, we conduct ablation experiments that comprehensively compare the results generated by various combinations of these distributions.

In Table \ref{tab: ab component}, we conduct the ablation study on the combinations of distributions, showing the effectiveness and robustness of IFCD. Specifically, contrastive decoding with negative editing and original distribution is a competitive method, which could be attributed to the effect of hallucination-inducing from internal representation editing. In addition, since TruthX could be used to mitigate object hallucinations alone, IFCD overperforms it by a wide margin, manifesting the effectiveness of IFCD. Among all LVLMs, IFCD consistently demonstrates effectiveness and robustness in maintaining a low ratio of hallucination.

\begin{table}[h]
\caption{Ablation of IFCD components. “EDITING”: decoding with positive editing without contrastive decoding; ``w/o NEG'': contrastive decoding with positive editing and original distribution; ``w/o POS'': contrastive decoding with negative editing and original distribution. The best performance is marked by \textbf{bold} and \textbf{\textcolor{rank1}{orange}}, and the second one is marked by \textcolor{rank2}{cyan}.}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{llcc}
\hline
Model & Method      & $\text{CHAIR}_s$↓                               & $\text{CHAIR}_i$↓                              \\ \hline
      & Editing     & 57                                    & 15                                   \\
InstructBLIP  & IFCD w/o neg & 71                                    & 19.9                                 \\
      & IFCD w/o pos & \cellcolor{rank1}\textbf{28}   & \cellcolor{rank1}\textbf{7.6} \\ \cline{2-4} 
      & IFCD (ours)        & \cellcolor{rank2}39.6          & \cellcolor{rank2}11.2         \\ \hline 
      & Editing     & \cellcolor{rank2}27            & \cellcolor{rank2}9.4          \\
LLaVA 1.5 & IFCD w/o neg & 44                                    & 11.8                                 \\
      & IFCD w/o pos & 46                                    & 14                                   \\ \cline{2-4} 
      & IFCD (ours)        & \cellcolor{rank1}\textbf{13.2} & \cellcolor{rank1}\textbf{5.6} \\ \hline
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\label{tab: ab component}
\end{table}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\linewidth]{pic files/turn_acc_llava_coco.pdf}
%     \caption{IFCD performance with different training sizes of internal representation editing model.}
%     \label{figure: training_size}
% \end{figure}
% \textbf{Training Size of Internal Representation Editing Model  } Training the TruthX, which assesses the factual alignment of internal representations in large vision-language models, is a prerequisite for the IFCD method. We train the TruthX using a uniformly sampled set of image-text pairs from the MSCOCO dataset.

% It is expected that TruthX's ability to assess the factual alignment of internal representations will improve as the size of the training data increases, thereby enhancing the performance of the IFCD method. However, in actual tests, we observe that the performance of IFCD reaches its peak accuracy when the training set size reaches 300. Surprisingly, increasing the training data beyond this point results in a decline in the performance of IFCD, which may be attributed to potential overfitting or the introduction of noise in the additional data.

% As shown in Figure \ref{figure: training_size}, when conducting IFCD on LLaVA 1.5 using TruthX models trained with different amounts of training data, it is visually evident that the overall best performance is achieved when the training data size reaches 300 in POPE. Further increasing the training data, however, leads to a significant decline in performance.

% \textbf{Identifying truthfulness    }
% Before deploying IFCD during the inference stage of LVLMs, TruthX must first undergo training. Specifically, we use 300 images from the MSCOCO dataset, paired with both positive and negative captions, to train the core component of TruthX, $\mathrm{TruthEnc(\cdot)}$, enabling it to evaluate truthfulness based on the internal representations of LVLMs. The effectiveness of this training process is demonstrated using Principal Component Analysis, as shown in Figure \ref{fig:PCA_result}. The results indicate that $\mathrm{TruthEnc(\cdot)}$ successfully distinguishes between truthful and untruthful internal representations.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{pca-line.pdf}
%     \caption{Identifying the truthfulness of internal representation from the last attention layer. \textcolor{blue}{Blue} part denotes truthful internal representation mapped in latent space of $\mathrm{TruthEnc(\cdot)}$, while \textcolor{red}{Red} part referring to untruthful internal representation.}
%     \label{fig:PCA_result}
% \end{figure}

\textbf{Internal Representation Editing } 
The initial step of IFCD involves internal representation editing, making editing strength and the number of editing layers critical hyperparameters. Thus, we examine the impact of varying editing strength and the number of editing layers on IFCD performance with metrics $\text{CHAIR}_s$ and $\text{CHAIR}_i$.

\begin{figure}[h!]
\begin{center}
    \includegraphics[width=1\linewidth]{chair_blip.pdf}
    \vskip -0.1in
    \caption{CHAIR scores vary with editing strength and layers.}
    \label{fig:edit_param}
\end{center}
\end{figure}

Figure \ref{fig:edit_param} presents the impact of editing strength and layers when editing the internal representation on the effectiveness of IFCD in hallucinations mitigation. For the $\text{CHAIR}_s$ metric, the results demonstrate continuous optimization as both the editing strength and the number of editing layers increase. However, regarding the $\text{CHAIR}_i$ metric, performance degradation occurs when the editing strength reaches 1. Furthermore, the relationship between $\text{CHAIR}_i$ and the number of editing layers becomes inversely correlated. This observation strongly suggests that an editing strength of 1 is a critical threshold, beyond which further adjustments to the internal representation yield diminishing performance.

\textbf{Contrastive Decoding Strength   }
After internal representation editing, contrastive decoding is employed to recalibrate distribution. During this stage, the key parameter is contrastive decoding strength $\alpha$, which controls the extent of contrastive decoding. Intuitively, when the gap of distributions involved in contrast is small, larger $\alpha$ is needed, and vice versa.

As shown in the left part of Figure \ref{fig: last ablation}, the small $\alpha$ leads top performance, denoting the gap of distributions involved in contrastive decoding is striking, and editing internal representation is a promising way to expose LVLMs' hallucinations preference.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{ablation_last.pdf}
\caption{IFCD performance with different contrast strengths and the capacity of identifying truthfulness. The order of magnitude of the PCA figure is 1e-7.}
\label{fig: last ablation}
\end{figure}
% \begin{figure}[h]
% \vskip -0.1in
% \centering
% \subfigure[]{\label{fig:alpha_ablation}
% \includegraphics[width=0.5\linewidth]{alpha_ablation_thin.pdf}}\subfigure[]{
% \includegraphics[width=0.5\linewidth]{pca_ori.pdf}\label{fig:identify_truthfulness}
% }
% % \includegraphics[width=1\linewidth]{alpha_ablation_thin.pdf}
% \vskip -0.1in
% \caption{The capacity of identifying truthfulness and IFCD performance with different contrast strengths.}
% \end{figure}

\textbf{The Capacity of Editing Internal Representation } To investigate the effect of editing internal representations, we explore the latent space of $\mathrm{TruthEnc(\cdot)}$, which maps and edits the internal representation of LVLMs in latent space. We provide responses that differ in truthfulness to the LVLMs and map internal representations with different truthfulness generated during the inference process into the latent space of $\mathrm{TruthEnc(\cdot)}$. We then apply Principal Component Analysis (PCA) to reduce the dimensionality of the latent space to two dimensions to visualize the latent space as illustrated in the right part of Figure \ref{fig: last ablation}. The figure clearly illustrates a distinct separation between internal representations that differ in truthfulness, demonstrating the model's ability to effectively identify and modify internal representations. This observation also suggests that only a minimal contrastive decoding strength is required to achieve optimal performance.
% \textcolor{red}{
% minimal contrastive decoding strength is required to achieve optimal performance.
% minimal contrastive 
% decoding strength is required to achieve optimal performance.}
\section{Conclusion}
In this paper, we propose a novel method, IFCD, to mitigate object hallucination. 
In this method, we apply contrastive decoding based on truthfulness editing of internal representations to eliminate hallucinatory elements that are actively induced and closely aligned with statistic biases.
Extensive experimentation across diverse benchmarks and LVLMs confirms the efficacy of IFCD. 

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Training Effect of Internal Representation Editing Model} \label{training_size_apex}
Training the TruthX, which assesses the truthfulness alignment of internal representations in LVLMs, is a prerequisite for the IFCD method. We train the TruthX using a uniformly sampled set of image-text pairs from the MSCOCO dataset. The specific configuration contains an image as visual information input and two counterfactual responses. For training for $\mathrm{SemEnc(\cdot)}$ to maintain semantic consistency during internal representation editing, truthful and untruthful responses are composed of as many similar tokens as possible \cite{truthx}.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.6\linewidth]{turn_acc_llava_coco.pdf}
%     \caption{IFCD performance with different training sizes of internal representation editing model.}
%     \label{fig:train_size}
% \end{figure}

\begin{table}[h]
\caption{IFCD performance with different training sizes of internal representation editing model.}
\vskip 0.1in
\label{tab:train_size}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|ccc}
\hline
\textbf{Training Size} & \textbf{Adversarial Acc} & \textbf{Popular Acc} & \textbf{Random Acc} \\ \hline
100           & 82.1            & 84.7        & 87.4       \\
200           & 76              & 77.3        & 77.6       \\
300           & \cellcolor{rank1}\textbf{83.5}            & \cellcolor{rank1}\textbf{86}          & \cellcolor{rank1}\textbf{86.4}       \\
400           & 82.1            & 83.6        & 85.3       \\
500           & 82.3            & 84.2        & 85.5       \\
600           & 80.4            & 81.8        & 82         \\
700           & 75.8            & 77.6        & 79.2       \\ \hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

The performance of TruthX in assessing the truthfulness alignment of internal representations is anticipated to improve with an increase in the size of the training dataset, thereby enhancing the efficacy of the IFCD method. However, validation results indicate that the performance of IFCD reaches its peak accuracy when the training dataset contains 300 samples. Interestingly, further increases in the training data size beyond this threshold result in a decline in IFCD's performance. This decline may be attributed to potential overfitting or the introduction of noise within the additional data.

We compare the performance of IFCD with varying training sizes on the MSCOCO subset of POPE, utilizing three different POPE sampling strategies. The results remain consistent across all strategies. As shown in the left part of Figure \ref{tab:train_size}, when conducting IFCD on LLaVA 1.5 using TruthX models trained with different amounts of training data, it is visually evident that the overall best performance is achieved when the training data size reaches 300 in POPE. Further increasing the training data, however, leads to a significant decline in performance.

\section{Experiments Details} \label{apex: param}
The overall experiment settings are reported in Table \ref{tab: overall setting}. While regular direct decoding follows this setting in each experiment, baseline method VCD and our proposed IFCD follow specific settings. We use the default code for the implementation of two backbone LVLMs, InstructBLIP and LLaVA 1.5 in HuggingFace Transformers Repository \cite{wolf2020transformers}.

The hyper-parameters settings for IFCD in our experiments in Section \ref{4} is reported in Table \ref{tab: ifcd setting}. Specifically, as we discussed in Section \ref{sec: analyse}, there are three major hyper-parameters that actively adjust the effectiveness of IFCD: \textit{Editing Strength}, \textit{Editing Layers}, and \textit{Contrastive Decoding Strength}.

Regrading the comparison of baseline decoding methods VCD and ICD, we adopt the code, and hyper-parameters in the public repositories and papers. We strictly follow the implementation as reported in the paper to reproduce results as Table \ref{tab: vcd setting} (VCD) and Table \ref{tab: icd setting} (ICD).

\begin{table}[h!]
\caption{Overall Experiment Settings.}
\vskip 0.1in
\label{tab: overall setting}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|c}
\hline
\textbf{Parameters}                      & \textbf{Value} \\ \hline
Maximum New Token (POPE)        & 32    \\ \hline
Maximum New Token (MME)         & 32    \\ \hline
Maximum New Token (MSCOCO)      & 128   \\ \hline
Maximum New Token (LLaVA-Bench) & 512   \\ \hline
Temperature                     & 1     \\ \hline
Top-K                           & FALSE \\ \hline
Top-p                           & 1     \\ \hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\caption{IFCD Hyperparameter Settings.}
\label{tab: ifcd setting}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|c}
\hline
\textbf{Parameters}                     & \textbf{Value} \\ \hline
Editing Strength                  & 0.5   \\ \hline
Editing Layers                    & 15    \\ \hline
Contrastive Deconding Strength & 0.1     \\ \hline
Adaptive Plausible Threshold   & 0.1   \\ \hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\caption{VCD Hyperparameter Settings.}
\label{tab: vcd setting}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|c}
\hline
\textbf{Parameters}                            & \textbf{Value} \\ \hline
Noise Step (POPE)                     & 999   \\ \hline
Noise Step (except POPE) & 500   \\ \hline
Contrastive Deconding Strength        & 1     \\ \hline
Adaptive Plausible Threshold          & 0.1   \\ \hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\caption{ICD Hyperparameter Settings.}
\label{tab: icd setting}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|c}
\hline
\textbf{Parameters}            & \textbf{Value}                          \\ \hline
Instruction Dirturbance Prompt & ``You are a confused object detector.'' \\ \hline
Contrastive Decoding Strength  & 1                                       \\ \hline
Adaptive Plausible Threshold   & 0.1                                     \\ \hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}

\section{MME Experiment detailed Results} \label{appendix_mme}
In Table \ref{tab: full_perception}, we comprehensively present the performance of two LVLM benchmarks on perception-related tasks within the MME benchmark. The results demonstrate that the baseline models exhibit consistent performance patterns, while the employment of IFCD significantly enhances their overall perception capabilities. This improvement is likely attributed to IFCD's ability to effectively mitigate logits that expose object hallucination, thereby recalibrating the LVLM to prioritize visual information rather than relying on pre-existing biases and priors. In contrast, the position, celebrity, and OCR score of IFCD is at a relatively low level on InstructBLIP, while LLaVA 1.5 with IFCD achieves the highest scores on each task among the three decoding methods, suggesting the comparatively weak ability of specific LVLM in these reasoning tasks.
\begin{table}[h!]
\caption{Results on all MME perception-related tasks. The best performance of each setting is marked by \textbf{bold} and \textcolor{rank1}{orange}. The second is marked by \textcolor{rank2}{cyan}.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccccc|c}
\hline
Model                          & Decoding & \multicolumn{1}{l}{Existence}        & \multicolumn{1}{l}{Count}              & \multicolumn{1}{l}{Position}           & \multicolumn{1}{l}{Color}              & \multicolumn{1}{l}{Posters}            & \multicolumn{1}{l}{Celebrity}          & \multicolumn{1}{l}{Scene}               & \multicolumn{1}{l}{Landmark}            & \multicolumn{1}{l}{Artwork}          & \multicolumn{1}{l|}{OCR}             & \multicolumn{1}{l}{Total Score}          \\ \hline
                               & Regular  & 180                                  & 73.3                                   & \cellcolor{rank2}76.6                                   & 108.3                                  & 123.4                                  & \cellcolor{rank1}\textbf{105.5} & 144.75                                  & 126.25                                  & \cellcolor{rank2}99.25                                & \cellcolor{rank1}\textbf{95}  & 1037.35                                  \\
                               & ICD      & 180                                  & \cellcolor{rank2}80             & \cellcolor{rank1}\textbf{80}    & \cellcolor{rank2}130.3          & 116.6                                  & 97.3                                   & 151                                     & 133                                     & \cellcolor{rank1}\textbf{101}                                  & 70                                   & \cellcolor{rank2}1072.2           \\
\multirow{-2}{*}{InstructBLIP} & VCD      & \cellcolor{rank1}\textbf{190} & 65                                     & 58.3                                   & 130                                    & \cellcolor{rank2}135            & 102.9                                  & \cellcolor{rank2}152.25          & \cellcolor{rank2}143.75          & 87                                   & 65                                   & 1064.2                                   \\
                               & IFCD     & \cellcolor{rank2}185          & \cellcolor{rank1}\textbf{85}    & 63.3           & \cellcolor{rank1}\textbf{138.3} & \cellcolor{rank1}\textbf{144.5} & \cellcolor{rank2}103.5          & \cellcolor{rank1}\textbf{163.5}  & \cellcolor{rank1}\textbf{160.75} & \cellcolor{rank1}\textbf{101}                                  & \cellcolor{rank2}80           & \cellcolor{rank1}\textbf{1144.85} \\ \cline{2-13} 
                               & Regular  & 180                                  & 123.3                                  & 105                                    & 158.3                                  & 115.6                                  & 107.6                                  & 145.5                                   & 127.5                                   & 107.5                                & 107.5                                & 1170.3                                   \\
                               & ICD      & 180                                  & 123.3                                  & 110                                    & 158.3                                  & 116.6                                  & 107.9                                  & \cellcolor{rank2}146.75          & 130.5                                   & 110.5                                & \cellcolor{rank2}115          & 1183.85                                  \\
\multirow{-2}{*}{LLaVA 1.5}    & VCD      & 180                                  & \cellcolor{rank2}125            & \cellcolor{rank2}115            & 153.3                                  & \cellcolor{rank2}117            & \cellcolor{rank2}127            & 146                                     & \cellcolor{rank2}132.5           & 107.5                                & 92.5                                 & \cellcolor{rank2}1203.3           \\
                               & IFCD     & \cellcolor{rank1}\textbf{185} & \cellcolor{rank1}\textbf{163.3} & \cellcolor{rank1}\textbf{133.3} & \cellcolor{rank1}\textbf{163.3} & \cellcolor{rank1}\textbf{129.6} & \cellcolor{rank1}\textbf{136.7} & \cellcolor{rank1}\textbf{154.25} & \cellcolor{rank1}\textbf{166.75} & \cellcolor{rank1}\textbf{125} & \cellcolor{rank1}\textbf{125} & \cellcolor{rank1}\textbf{1357.2}  \\ \hline
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\label{tab: full_perception}
\end{table}

\section{Experiment Results on LLaVA-Bench} \label{llava_bench_apex}
As discussed in Section \ref{llava-bench}, we leverage LLaVA-Bench as a case study to compare the outputs of IFCD with other methods qualitatively. All methods use the settings as Section \ref{implementation_detail}. In all cases, red fonts indicate object hallucination, including object existence, attribute, or relationship hallucination.
\begin{figure}[h!]
\begin{center}
    \vskip 0.2in
    \subfigure{
    \includegraphics[width=0.8\linewidth]{llava-bench_case_apx_blip1.pdf}}
    \subfigure{
    \includegraphics[width=0.8\linewidth]{llava-bench_case_apx_blip2.pdf}
    }
    % \subfigure{
    % \includegraphics[width=0.8\linewidth]{llava-bench_case_apx_blip3.pdf}
    % }
    \end{center}
    \vskip -0.2in
    \caption{LLaVA-Bench results comparing direct decoding, ICD, VCD, and IFCD with InstructBLIP backbone.}
    \label{fig: llava_bench_case_apex_blip}
\end{figure}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
    \subfigure{
    \includegraphics[width=0.8\linewidth]{llava-bench_case_apx_llava1.pdf}}
    \subfigure{
    \includegraphics[width=0.8\linewidth]{llava-bench_case_apx_llava2.pdf}
    }
    % \subfigure{
    % \includegraphics[width=0.6\linewidth]{llava-bench_case_apx_llava3.pdf}
    % }
\end{center}
\vskip -0.2in
    \caption{LLaVA-Bench results comparing direct decoding, ICD, VCD, and IFCD with LLaVA 1.5 backbone.}
    \label{fig: llava_bench_case_apex_llava}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

