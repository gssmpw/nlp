\section{Related Work}
\subsection{Large Visual Language Models}
In recent years, large language models (LLMs) based on the Transformer architecture have achieved remarkable achievements in various fields, including Natural Language Processing (NLP), Machine Translation, and Computer Vision. ~\cite{zhao2023survey, achiam2023gpt, chiang2023vicuna}. Notably, with the introduction of multimodal models such as CLIP ~\cite{radford2021learning} and Vision Transformer ~\cite{dosovitskiy2021an}, LVLMs have been established through comprehensive pre-training processes that unify textual and visual modalities ~\cite{bai2023qwen, Ye_2024_CVPR}. Compared with traditional vision models, LVLMs adopted more advanced training paradigms ~\cite{wei2022finetuned, liu2024visual}. As a result, LVLMs demonstrated unique capabilities not present in traditional models \cite{yang2023mm}, including establishing application \cite{Ye_2024_CVPR}, and advanced mathematical reasoning \cite{pmlr-v202-driess23a}.

\subsection{Object Hallucination}
While LVLMs exhibited strong capabilities in addressing vision-language tasks, they were still significantly affected by object hallucinations ~\cite{li-etal-2023-evaluating}, generating content irrelevant to visual information. To identify the issue of object hallucinations in LVLMs, recent research has established specific indicators, such as Caption hallucinations Assessment with Image Relevance (CHAIR) ~\cite{rohrbach-etal-2018-object} and Sharpness \cite{chen2024context}. Additionally, advances have been made in locating the causes of hallucinations within LVLMs, including internal representation and attention patterns ~\cite{han2024semantic, mahaut-etal-2024-factual}. These metrics and locating approaches provided a multi-dimensional view to observe object hallucination. 

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{pipeline.pdf}}
\caption{\textbf{An overview of IFCD.~} IFCD first edits the internal representation of the LVLMs to construct counterfactual logits for comparison by deliberately injecting hallucinations into the model trained by contrastive learning. These counterfactual logits are utilized to reveal potential hallucinatory tendencies of the LVLMs. Furthermore, the internal representation editing model is employed to actively attenuate a portion of the hallucinatory components within the LVLMs, thereby initiating an improvement in the factual accuracy of its outputs. This process effectively corrects the token from an erroneous token ``[Fork]'' to an accurate ``[Dog]''.}
\label{fig:pipe_line}
\vskip -0.3in
\end{center}
\end{figure*}

Addressing object hallucinations typically focused on direct suppression methods and fine-tuning, which involved actively limiting \cite{dhuliawala-etal-2024-chain}, correcting hallucinated outputs \cite{hu-etal-2024-knowledge} and RLHF to refine LVLMs ~\cite{ye2024mplug}. These approaches often constructed enhanced datasets for fine-tuning or training LVLMs. As the parameter scales of LVLMs continued to increase, these challenges became even more pronounced~\cite{Ye_2024_CVPR, zhu2024minigpt}. To tackle these issues, IFCD actively induces hallucinations into the model's output and leverages them as counterexamples to refine the model's final responses, thereby reducing the likelihood of hallucinations in final outputs. IFCD leverages hallucinated outputs as improving opportunities, offering a novel way to mitigate object hallucinations without high computation costs.