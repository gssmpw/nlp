\section{Related Work}
\label{related work}
\subsection{Camera-based 3D Object Detection}
As 3D bounding boxes is usually predicted from features in the BEV/3D space, VT is needed in camera-based 3D object detection tasks. The VT methods can be mainly categorized into two types, i.e., depth-based VT **Luo et al., "Frustum PointNets for 3D Object Detection in RGB-D Scans"** and query-based VT **Lang et al., "FCOS3D: Fully Convolutional One-Stage Oriented 3D Object Detection"**.

The depth-based VT explicitly utilizes the camera parameters and depth information to lift image pixels to frustum voxels or points, and further convert them to standard voxels or BEV maps. For instance, based on the pioneering work **Luo et al., "Frustum PointNets for 3D Object Detection in RGB-D Scans"** in which the \textit{splatting}-based VT strategy is proposed, **Lang et al., "FCOS3D: Fully Convolutional One-Stage Oriented 3D Object Detection"** applies the same method to perform 3D object detection. Differently, **Liu et al., "M$^2$BEV: Multi-task Monocular BEVDet with Depth-aware Sampling Strategy"** and **Song et al., "Simple-BEV: Simple yet Effective BEV-based Object Detection"** utilize the \textit{sampling} strategy under the assumption that image depths are uniformly distributed. To further improve the performance, **Chen et al., "Dual-BEV: Dual-branch BEV Fusion Network for 3D Object Detection"** exploits both \textit{splatting} and \textit{sampling} strategies and fuses the resulted BEV features.

Intuitively, improved depth quality is beneficial for VT, so that some methods take the advantage of geometrically-accurate LiDAR points to introduce depth supervision. The earliest work **Zhou et al., "BEVDet: Monocular 3D Object Detection Using BEV Depth Map"** projects LiDAR points onto the image plane to generate a sparse depth map, which serves as the target of depth estimation. **Chen et al., "EA-LSS: Edge-Aware Lidar Point Cloud Semantic Segmentation via Sparse and Dense Fusion"** further exploits the depth gradients in the ground-truth depth map, to realize edge-aware depth estimation.

In approaches applying the query-based VT like **Yin et al., "BEVFormer: BEV-aware Transformer for 3D Object Detection"** and **Wang et al., "DETR3D: DEtection TRansformer for 3D Object Detection"**, initialized 3D object queries are sent to transformer layers, to interact with image PV features through cross attention. After that, 3D bounding boxes are regressed from the updated object queries. Different from depth-based VT, query-based VT do not explicitly leverage image depths, bypassing the ill-posed 2D-3D projection. Nevertheless, the transformer itself introduces high complexity.

Although improvements has been witnessed recently, the performance of camera-based 3D object detection methods are still limited by the inherent depth ambiguity of images. 

\vspace{-3mm}
\subsection{4D Radar-based 3D Object Detection}

LiDAR-based methods dominate the field of 
3D object detection for years. With developments of 4D radars and the emerging 4D radar datasets **Zhao et al., "Radar-based 3D Object Detection: A Survey"**, researchers start to study whether 4D radars can substitute LiDARs in 3D object detection. 
%
4D radar data can be represented as 3D points and other data formats such as 4D radar tensors. Except for few works **Wang et al., "A Survey of Radar-based 3D Object Detection"**, most methods take the point representation as input **Lang et al., "PointPillars: Fast Encoders for Object Detection in 3D Point Clouds"** and modify models that are originally designed for LiDAR points. For example, based on **Lang et al., "PointPillars: Fast Encoders for Object Detection in 3D Point Clouds"**, **Liu et al., "RPFA-Net: Radar Point Feature Attention Network for 3D Object Detection"** replaces the PointNet **Li et al., "PointNet: Deep Learning on Point Sets for 3D Classification and Regression"** by self-attention, while **Wang et al., "RadarPillarNet: A Pillar-based Framework for Radar 3D Object Detection"** separate the feature extraction of position, Doppler and intensity in Pillar Feature Net. However, 4D radar point clouds are much sparser and noisier than LiDAR points, which is not explicitly handled in these work. To alleviate the influence of radar point sparsity, **Liu et al., "SMURF: Sparsity-aware Radar Feature Extraction"** add a kernel density estimation branch to extract sparsity-awareness features of radar points. 

Despite the increasing performance, these methods scarcely exploit the unique characteristics of radar points, such as the Doppler velocity and RCS value. 

\vspace{-3mm}
\subsection{4D Radar-Camera Fusion-based 3D Object Detection}

As 4D radar data contain geometric information and camera images have rich semantic information, it is intuitive to fuse these modalities and obtain higher detection accuracy. However, till now,  related methods are limited **Wang et al., "Radar-camera fusion for 3D object detection: A review"**.

RCFusion **Zhang et al., "RCFusion: Radar-Camera Fusion Network for 3D Object Detection"** utilizes OFT **Liu et al., "OFT: Orthographic Feature Transform for Depth Estimation"** to transform image features from PV to BEV, and proposes spatial-attention based IAM to fuse image and radar BEV features. To reduce the distortion of the image BEV features, **Wang et al., "LXLv2: LiDAR-Camera Fusion Network with Improved Depth Estimation and Feature Adaptiveness"** employs predicted image depth and radar 3D occupancy grids to assist the image VT, which significantly improves the performance. Differently, **Zhang et al., "DPFT: Dual-branch Point Transform for Radar-based 3D Object Detection"** takes 4D radar tensors as input and transform them to range-azimuth and azimuth-elevation maps. After that, a fusion and detection method similar to **Wang et al., "FUTR3D: Fusion Transformers for Radar-Camera based 3D Object Detection"** is adopted.

In general, the recent 4D radar-camera fusion-based methods are comparable with LiDAR-based ones to a certain extent, showing the great potential of this field.
\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{LXLv2_model.pdf} \vspace{-3mm}
    \caption{The overall architecture of LXLv2 compared with LXL **Wang et al., "LXL: LiDAR-Camera Fusion Network"**. Differences lie in the depth estimation process and the fusion module. During depth estimation, camera intrinsics are introduced and radar points are exploited for one-to-many depth supervision, and RCS values are utilized to determine the supervision area. In the fusion module, CSAFusion is applied for improved feature adaptiveness and model robustness.}
    \label{fig:LXLv2} \vspace{-5mm}
\end{figure*}

\vspace{-3mm}