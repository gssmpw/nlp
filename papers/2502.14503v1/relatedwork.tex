\section{Related Work}
\label{related work}
\subsection{Camera-based 3D Object Detection}
As 3D bounding boxes is usually predicted from features in the BEV/3D space, VT is needed in camera-based 3D object detection tasks. The VT methods can be mainly categorized into two types, i.e., depth-based VT \cite{BEVDepth,Simple-BEV,BEVDet,Dual-BEV,BEVSpread} and query-based VT \cite{BEVFormer,DETR3D,PETRv2}.

The depth-based VT explicitly utilizes the camera parameters and depth information to lift image pixels to frustum voxels or points, and further convert them to standard voxels or BEV maps. For instance, based on the pioneering work LSS\cite{LSS} in which the \textit{splatting}-based VT strategy is proposed, BEVDet\cite{BEVDet} applies the same method to perform 3D object detection. Differently, M$^2$BEV\cite{M2BEV} and Simple-BEV\cite{Simple-BEV} utilize the \textit{sampling} strategy under the assumption that image depths are uniformly distributed. To further improve the performance, Dual-BEV\cite{Dual-BEV} exploits both \textit{splatting} and \textit{sampling} strategies and fuses the resulted BEV features.

Intuitively, improved depth quality is beneficial for VT, so that some methods take the advantage of geometrically-accurate LiDAR points to introduce depth supervision. The earliest work BEVDepth\cite{BEVDepth} projects LiDAR points onto the image plane to generate a sparse depth map, which serves as the target of depth estimation. EA-LSS\cite{EA-LSS} further exploits the depth gradients in the ground-truth depth map, to realize edge-aware depth estimation.

In approaches applying the query-based VT like BEVFormer\cite{BEVFormer} and DETR3D\cite{DETR3D}, initialized 3D object queries are sent to transformer layers, to interact with image PV features through cross attention. After that, 3D bounding boxes are regressed from the updated object queries. Different from depth-based VT, query-based VT do not explicitly leverage image depths, bypassing the ill-posed 2D-3D projection. Nevertheless, the transformer itself introduces high complexity.

Although improvements has been witnessed recently, the performance of camera-based 3D object detection methods are still limited by the inherent depth ambiguity of images. 

\vspace{-3mm}
\subsection{4D Radar-based 3D Object Detection}

LiDAR-based methods dominate the field of 
3D object detection for years. With developments of 4D radars and the emerging 4D radar datasets \cite{VoD, TJ4DRadSet, K-Radar}, researchers start to study whether 4D radars can substitute LiDARs in 3D object detection. 
%
4D radar data can be represented as 3D points and other data formats such as 4D radar tensors. Except for few works \cite{K-Radar,RTNH+}, most methods take the point representation as input \cite{RPFA-Net,SMURF,RadarMFNet}, and modify models that are originally designed for LiDAR points. For example, based on PointPillars\cite{PointPillars}, RPFA-Net\cite{RPFA-Net} replaces the PointNet \cite{PointNet} by self-attention, while RadarPillarNet\cite{RCFusion} separate the feature extraction of position, Doppler and intensity in Pillar Feature Net. However, 4D radar point clouds are much sparser and noisier than LiDAR points, which is not explicitly handled in these work. To alleviate the influence of radar point sparsity, SMURF\cite{SMURF} add a kernel density estimation branch to extract sparsity-awareness features of radar points. 

Despite the increasing performance, these methods scarcely exploit the unique characteristics of radar points, such as the Doppler velocity and RCS value. 

\vspace{-3mm}
\subsection{4D Radar-Camera Fusion-based 3D Object Detection}

As 4D radar data contain geometric information and camera images have rich semantic information, it is intuitive to fuse these modalities and obtain higher detection accuracy. However, till now,  related methods are limited \cite{RCFusion,LXL,DPFT}.

RCFusion \cite{RCFusion} utilizes OFT\cite{OFT} to transform image features from PV to BEV, and proposes spatial-attention based IAM to fuse image and radar BEV features. To reduce the distortion of the image BEV features, LXL\cite{LXL} employs predicted image depth and radar 3D occupancy grids to assist the image VT, which significantly improves the performance. Differently, DPFT\cite{DPFT} takes 4D radar tensors as input and transform them to range-azimuth and azimuth-elevation maps. After that, a fusion and detection method similar to FUTR3D\cite{FUTR3D} is adopted.

In general, the recent 4D radar-camera fusion-based methods are comparable with LiDAR-based ones to a certain extent, showing the great potential of this field.
\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{LXLv2_model.pdf} \vspace{-3mm}
    \caption{The overall architecture of LXLv2 compared with LXL \cite{LXL}. Differences lie in the depth estimation process and the fusion module. During depth estimation, camera intrinsics are introduced and radar points are exploited for one-to-many depth supervision, and RCS values are utilized to determine the supervision area. In the fusion module, CSAFusion is applied for improved feature adaptiveness and model robustness.}
    \label{fig:LXLv2} \vspace{-5mm}
\end{figure*}

\vspace{-3mm}