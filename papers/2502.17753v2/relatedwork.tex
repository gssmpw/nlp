\section{Related Work}
Our research is related to previous works on procedural video understanding in general and task-graph learning in particular.

\subsection{Procedural Video Understanding Tasks}
\label{sec:procedure_understanding}
Previous investigations considered different procedural video understanding tasks.
A line of work tackled the task of inferring key-steps from procedural videos relying on subtitles~\cite{zhou2018towards}, fitting individual classifiers for key-steps~\cite{zhukov2019cross}, exploiting self-supervised deep neural networks~\cite{elhamifar2020self}, modeling intra-video and inter-video frame similarities in an unsupervised way~\cite{bansal2022my}, aligning embeddings of identical key-steps~\cite{bansal2024united}, exploiting transformer-based architecture~\cite{dvornik2023stepformer}.
Other methods focused on grounding key-steps in procedural videos using attention-based methods~\cite{lu2022set} or aligning visual and textual features in narrated videos~\cite{miech2020end}. Also, task verification has been explored through learning contextualized step representations~\cite{narasimhan2023learning}, as well as through the development of benchmarks and synthetic datasets~\cite{hazra2023egotv}.
Among the other procedural video understanding tasks, mistake detection has gained increasing attention in recent years. Some methods have approached this task in fully supervised settings, where mistakes are explicitly labeled within videos and detection is performed offline~\cite{peddi2023captaincook4d,sener2022assembly101,wang2023holoassist}. Others have investigated weak supervision, where mistakes are annotated only at the video level rather than at finer spatial and temporal scales~\cite{ghoddoosian2023weakly}. A different approach~\cite{ding2023every} leverages knowledge graphs built from fine-grained spatial and temporal annotations to improve mistake detection.
To advance the field of mistake detection,~\cite{flaborea2024prego} introduced PREGO, an online mistake detection benchmark incorporating videos from the Assembly101~\cite{sener2022assembly101} and EPIC-Tent~\cite{jang2019epic} datasets. The same work proposed a novel method for detecting mistakes in procedural egocentric videos based on large language models.
Notably, these prior works have relied on diverse representations, mostly implicit (e.g., activations of neural network), and hence non-interpretable and not straightforward to generalize across different tasks.

Recently, task graphs, mined from video or external knowledge such as WikiHow articles, have been investigated as a powerful representation of procedures and proved advantageous for learning representations useful for downstream tasks such as key-step recognition and forecasting~\cite{ashutosh2024video,grauman2023ego,zhou2023procedure}, temporal action segmentation~\cite{nagasinghe2024not}, and procedure planning~\cite{shen2024progress}.
Differently from previous works~\cite{narasimhan2023learning,zhong2023learning}, we aim to develop an explicit and human readable representation of the procedure which can be directly exploited to enable downstream tasks~\cite{ashutosh2024video}, rather than an implicit representation obtained with pre-training objective~\cite{zhou2023procedure,narasimhan2023learning}. 
As a departure from previous paradigms which carefully designed task graph construction procedures~\cite{ashutosh2024video,zhou2023procedure,sohn2020meta,jang2023multimodal}, we frame task generation in a general framework, enabling models to learn task graphs directly from input sequences, and propose a differentiable loss function based on maximum likelihood estimation.

 

\subsection{Task Graph Learning}
\label{sec:task_graph_construction}
Graph-based representations have been historically used to
represent constraints in complex tasks and design optimal sub-tasks scheduling~\cite{skiena1998algorithm}, making them a natural candidate to encode procedural knowledge.
Previous works investigated approaches to construct task graphs from natural language descriptions of procedures (e.g., recipes) using rule-based graph parsing~\cite{dvornik2022graph2vid,schumacher2012extraction}, defining probabilistic models~\cite{kiddon2015mise}, fine-tuning language models~\cite{sakaguchi2021proscript}, or proposing learning-based approaches~\cite{dvornik2022graph2vid} involving parsers and taggers trained on text corpora~\cite{donatelli2021aligning,yamakata2020english}. While these approaches do not require any action sequence as input, they depend on the availability of text corpora including procedural knowledge, such as recipes, which often fail to encapsulate the variety of ways in which the procedure may be executed~\cite{ashutosh2024video}. 
Other works proposed hand-crafted approaches to infer task graphs from sequences of actions depicting task executions~\cite{sohn2020meta,jang2023multimodal}. 
Recent work designed methodologies to mine task graphs from videos and textual descriptions of key-steps~\cite{ashutosh2024video} or cross-referencing visual and textual representations from corpora of procedural text and videos~\cite{zhou2023procedure}.

Differently from previous efforts, we rely on action sequences, grounded in video, rather than natural language descriptions of procedures~\cite{dvornik2022graph2vid,sakaguchi2021proscript} and frame task graph construction as a learning problem, providing a differentiable objective rather than resorting to hand-designed algorithms and task extraction procedures~\cite{ashutosh2024video,zhou2023procedure,sohn2020meta,jang2023multimodal}.