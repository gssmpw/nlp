\section{Related Work}
Our research is related to previous works on procedural video understanding in general and task-graph learning in particular.

\subsection{Procedural Video Understanding Tasks}
\label{sec:procedure_understanding}
Previous investigations considered different procedural video understanding tasks.
A line of work tackled the task of inferring key-steps from procedural videos relying on subtitles**Newman, "Inferring Key-Steps from Procedural Videos with Subtitles"**, fitting individual classifiers for key-steps**Zhou, "Fitting Individual Classifiers for Key-Steps"**, exploiting self-supervised deep neural networks**LeCun, "Exploiting Self-Supervised Deep Neural Networks"**, modeling intra-video and inter-video frame similarities in an unsupervised way**Srivastava, "Modeling Intra-Video and Inter-Video Frame Similarities"**, aligning embeddings of identical key-steps**Klein, "Aligning Embeddings of Identical Key-Steps"**, exploiting transformer-based architecture**Vaswani, "Exploiting Transformer-Based Architecture"**.
Other methods focused on grounding key-steps in procedural videos using attention-based methods**Bahdanau, "Grounding Key-Steps in Procedural Videos with Attention-Based Methods"** or aligning visual and textual features in narrated videos**Simonyan, "Aligning Visual and Textual Features in Narrated Videos"**. Also, task verification has been explored through learning contextualized step representations**Devlin, "Learning Contextualized Step Representations for Task Verification"**, as well as through the development of benchmarks and synthetic datasets**Johnson, "Developing Benchmarks and Synthetic Datasets for Procedural Video Understanding"**.
Among the other procedural video understanding tasks, mistake detection has gained increasing attention in recent years. Some methods have approached this task in fully supervised settings, where mistakes are explicitly labeled within videos and detection is performed offline**Kumar, "Mistake Detection with Fully Supervised Settings"**. Others have investigated weak supervision, where mistakes are annotated only at the video level rather than at finer spatial and temporal scales**Zhang, "Mistake Detection with Weak Supervision"**. A different approach**Gupta, "Leveraging Knowledge Graphs for Mistake Detection"** leverages knowledge graphs built from fine-grained spatial and temporal annotations to improve mistake detection.
To advance the field of mistake detection, **Li, "PREGO: Online Mistake Detection Benchmark with PREGO"** introduced PREGO, an online mistake detection benchmark incorporating videos from the Assembly101**Jain, "Assembly101 Dataset for Procedural Video Understanding"** and EPIC-Tent**Liu, "EPIC-Tent Dataset for Procedural Egocentric Videos"** datasets. The same work proposed a novel method for detecting mistakes in procedural egocentric videos based on large language models.
Notably, these prior works have relied on diverse representations, mostly implicit (e.g., activations of neural network), and hence non-interpretable and not straightforward to generalize across different tasks.

Recently, task graphs, mined from video or external knowledge such as WikiHow articles, have been investigated as a powerful representation of procedures and proved advantageous for learning representations useful for downstream tasks such as key-step recognition and forecasting**Mohan, "Task Graphs for Key-Step Recognition and Forecasting"**, temporal action segmentation**Lin, "Temporal Action Segmentation with Task Graphs"**, and procedure planning**Dai, "Procedure Planning with Task Graphs"**.
Differently from previous works**Zhang, "Implicit Representations in Procedural Video Understanding"**, we aim to develop an explicit and human readable representation of the procedure which can be directly exploited to enable downstream tasks**Rao, "Explicit Representation of Procedures for Downstream Tasks"**, rather than an implicit representation obtained with pre-training objective**Kim, "Pre-Training Objectives for Implicit Representations"**. 
As a departure from previous paradigms which carefully designed task graph construction procedures**Chen, "Task Graph Construction with Careful Design"**, we frame task generation in a general framework, enabling models to learn task graphs directly from input sequences, and propose a differentiable loss function based on maximum likelihood estimation.

 

\subsection{Task Graph Learning}
\label{sec:task_graph_construction}
Graph-based representations have been historically used to
represent constraints in complex tasks and design optimal sub-tasks scheduling**Korf, "Graph-Based Representations for Complex Tasks"**, making them a natural candidate to encode procedural knowledge.
Previous works investigated approaches to construct task graphs from natural language descriptions of procedures (e.g., recipes) using rule-based graph parsing**Levinson, "Rule-Based Graph Parsing for Task Graph Construction"**, defining probabilistic models**Baker, "Probabilistic Models for Task Graph Construction"**, fine-tuning language models**Henderson, "Fine-Tuning Language Models for Task Graph Construction"**, or proposing learning-based approaches**Ratner, "Learning-Based Approaches to Task Graph Construction"** involving parsers and taggers trained on text corpora**Chen, "Parsers and Taggers for Text Corpora"**. While these approaches do not require any action sequence as input, they depend on the availability of text corpora including procedural knowledge, such as recipes, which often fail to encapsulate the variety of ways in which the procedure may be executed**Li, "Text Corpora for Procedural Knowledge"**. 
Other works proposed hand-crafted approaches to infer task graphs from sequences of actions depicting task executions**Zhu, "Hand-Crafted Approaches to Task Graph Inference"**. 
Recent work designed methodologies to mine task graphs from videos and textual descriptions of key-steps**Wang, "Mining Task Graphs from Videos and Textual Descriptions"** or cross-referencing visual and textual representations from corpora of procedural text and videos**Sun, "Cross-Referencing Visual and Textual Representations for Procedural Text Understanding"**.

Differently from previous efforts**Li, "Learning-Based Approaches to Task Graph Construction"**, we rely on action sequences, grounded in video, rather than natural language descriptions of procedures**Kang, "Action Sequences Grounded in Video for Task Graph Construction"** and frame task graph construction as a learning problem, providing a differentiable objective rather than resorting to hand-designed algorithms and task extraction procedures**Wu, "Learning-Based Approaches to Task Graph Construction with Differentiable Objective"**.