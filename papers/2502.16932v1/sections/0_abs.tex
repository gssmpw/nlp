\begin{abstract}
Visuomotor policies have shown great promise in robotic manipulation but often require substantial human-collected data for effective performance. 
A key factor driving the high data demands is their limited spatial generalization capability, which necessitates extensive data collection across different object configurations.
In this work, we present \method, a low-cost, fully synthetic approach for automatic demonstration generation.
Using only one human-collected demonstration per task, \method generates spatially augmented demonstrations by adapting the demonstrated action trajectory to novel object configurations. 
% Visual observations are obtained by choosing 3D point clouds as the observation modality and performing 3D editing on the point clouds. 
Visual observations are synthesized by leveraging 3D point clouds as the modality and rearranging the subjects in the scene via 3D editing.
Empirically, \method significantly enhances policy performance across a diverse range of real-world manipulation tasks, showing its applicability even in challenging scenarios involving deformable objects, dexterous hand end-effectors, and bimanual platforms. 
Furthermore, \method can be extended to enable additional out-of-distribution capabilities, including disturbance resistance and obstacle avoidance.

\end{abstract}

% Visual observations are generated by applying 3D editing to point clouds, which serve as the observation modality.