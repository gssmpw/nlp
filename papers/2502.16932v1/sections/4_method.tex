






\section{\method Methods}

% Designed to address the conflict between the policies' huge data demands and the high cost of human-collected demonstrations, 
% \method generates spatially augmented observation-action pairs from a few source demonstrations.
% For actions, \method parses the source trajectory into object-centric motion and skill segments and applies TAMP-based adaptation. For observations, \method synthesizes the point clouds for robots and objects using a segment-and-transform strategy.

Designed to address the conflict between the substantial data requirements of visuomotor policies and the high cost of human-collected demonstrations, \method generates spatially augmented observation-action pairs from a small set of source demonstrations. For actions, \method parses the source trajectory into object-centric motion and skill segments and applies TAMP-based adaptation. For observations, \method efficiently synthesizes the point clouds for robots and objects using a segment-and-transform strategy.

\subsection{Problem Formulation}

% Visual imitation learning aims to learn a policy $\pi: \mathcal{O}\mapsto\mathcal{A}$ that maps the visual observations $o\in \mathcal{O}$ to actions $a\in \mathcal{A}$. To train such a policy, we need to prepare a dataset $\mathcal{D}$ of demonstrations. We consider a source demonstration $D_{s_0}\subseteq \mathcal{D}$ is a trajectory of (observation, action) pairs conditioned on the initial object configuration: $D_{s_0}=(d_0, d_1, ..., d_{L-1}|s_0)$, where $d_t=(o_t, a_t)$ and $L$ is the trajectory length. \method is designed to generate demonstrations by augmenting the source demonstration to be conditioned on a new initial object configuration: $\hat{D}_{s'_0}=(\hat{d}_0, \hat{d}_1, ..., \hat{d}_{L-1}|s'_0)$.

% Specifically, the action consists of the robot arm command and the end-effector command: $a_t=(T^E_t, a^E_t)$, where $T^E_t$ is represented as the $SE(3)$ transformation from the world frame to the end-effector and $a^{E}_t$ could be either a binary open-close signal for parallel gripper or a higher dimensional vector for the joint control of dexterous hands. Meanwhile, the observation consists of the point cloud and the proprioception sensing of the robot: ${o_t=(o^{pcd}_t, o^{robot}_t)}$, where the proprioception sensing is set to reflect the current end-effector state, thus sharing the same shape as the action: $o^{robot}_t\in\mathcal{A}$. Further, assuming $K$ objects $\{O_1, .., O_K\}$ will be sequentially manipulated in the task, the initial object configuration can be written as the initial poses of these manipulated objects from the world frame: $s_0=\{T_0^{O_1}, T_0^{O_2}, ..., T_0^{O_K}\}$. 


A visuomotor policy $\pi: \mathcal{O} \mapsto \mathcal{A}$ directly maps the visual observations $o \in \mathcal{O}$ to the predicted actions $a \in \mathcal{A}$. To train such a policy, a dataset $\mathcal{D}$ of demonstrations must be prepared. We define a source demonstration $D_{s_0} \subseteq \mathcal{D}$ as a trajectory of paired observations and actions conditioned on an initial object configuration: $D_{s_0} = (d_0, d_1, \dots, d_{L-1} | s_0)$, where each $d_t = (o_t, a_t)$ represents an observation-action pair, $s_0$ denotes the initial configuration, and $L$ is the trajectory length. 
% To expand the source demonstration collected by humans, \method is designed to automatically generate new demonstrations by augmenting the source demonstration to be conditioned on a different initial object configuration: 
\method is designed to augment a human-collected source demonstration by generating a new demonstration conditioned on a different initial object configuration:
\begin{equation*}
    \hat{D}_{s'_0} = (\hat{d}_0, \hat{d}_1, \dots, \hat{d}_{L-1} | s'_0).
\end{equation*}


Specifically, assuming the task involves the sequential manipulation of $K$ objects $\{O_1, O_2, \dots, O_K\}$, the initial object configuration $s_0$ is defined as the set of initial poses of these objects: $s_0 = \{\mathbf{T}_0^{O_1}, \mathbf{T}_0^{O_2}, \dots, \mathbf{T}_0^{O_K}\}$, where $\mathbf{T}^O_t$ denotes the $\mathrm{SE(3)}$ transformation from the world frame to an object $O$ at time step $t$.
The action $a_t$ consists of the robot arm and robot hand commands, represented as $a_t = (a_t^{\mathrm{arm}}, a_t^{\mathrm{hand}})$, where $a_t^{\mathrm{arm}}\triangleq \mathbf{A}^{\mathrm{EE}}_t$ is the target $\mathrm{SE(3)}$ end-effector pose in the world frame, and $a_t^{\mathrm{hand}}$ can either be a binary signal for a parallel gripperâ€™s open/close action or a higher-dimensional vector for controlling the joints of a dexterous hand. The observation $o_t$ includes both the point cloud data and the proprioceptive feedback from the robot: $o_t = (o^{\mathrm{pcd}}_t, o^{\mathrm{arm}}_t, o^{\mathrm{hand}}_t)$, where $o^{\mathrm{arm}}_t$ and $o^{\mathrm{hand}}_t$ reflect the current state of the end-effector, with the same dimensionality as the corresponding actions.





% \subsection{Preprocessing the Source Demonstration}

% \noindent\textbf{Segmented point cloud observations.}
% Given a raw point cloud observation, we crop the redundant points such as those from the table and only leave the points from the manipulated object(s) and the robot end-effector. We perform a clustering operation to filter out the outlier points in noisy real-world observation~\cite{ester1996density} and then downsample the points to a fixed number (e.g., $512$ or $1024$) by farthest point sampling~\cite{qi2017pointnet}. For the starting frame of the trajectory, we use Grounded SAM~\cite{ren2024grounded} to obtain the segmentation masks of the manipulated objects on the RGB image. The same mask is applied to the pixel-aligned depth image and then projected to the 3D point cloud, as illustrated in Fig.~\ref{fig:method_parse}.


% \vspace{0.2cm} \noindent\textbf{Parsing the source trajectory.} Similar to prior works~\cite{mandlekar2023mimicgen,garrett2024skillmimicgen}, we assume the execution trajectory can be parsed into a sequence of object-centric segments. Noticing that the robot has to \textcolor{myorange}{approach} the manipulated object in the free space before it can manipulate through \textcolor{myblue}{contact}, each object-centric segment can be further decomposed into \textcolor{myorange}{\textit{motion}} and \textcolor{myblue}{\textit{skill}} stage. Take the task in Fig.~\ref{fig:method_parse} as an example, the execution trajectory can be divided into four stages: \textcolor{myorange}{1) move to the flower}, \textcolor{myblue}{2) pick up the flower}, \textcolor{myorange}{3) transfer the flower to the vase}, and \textcolor{myblue}{4) insert the flower into the vase}.

% With access to the segmented point cloud observation, we can easily find the skill segment belonging to an object by checking whether the distance between the geometric center of the object point cloud and the robot end-effector is within a threshold, illustrated by the spheres in Fig.~\ref{fig:method_parse}. Then, the intermediate trajectories between two skill segments naturally become the motion segments.

% Formally, we use $\bm{\tau}$ to denote a sequence of time stamps: 
% \begin{equation*}
%     \bm{\tau}=(t_1, t_2, ..., t_P), \quad t_p\in\{0, 1, ..., L-1\}, P=|\bm{\tau}|.
% \end{equation*}
% For convenience, $\bm{\tau}$ can be used as an \textit{index sequence} to extract the corresponding segment from a sequence of demonstrations, actions, or observations, e.g., $d[\bm{\tau}]=(d_{t_1}, d_{t_2}, ..., d_{t_P})$.
% With this notation, we parse the source demonstration into alternating \textcolor{myorange}{motion} and \textcolor{myblue}{skill} segments according to the index sequence
% $(\textcolor{myorange}{\bm{\tau}^{\mathrm{m}}_1}, \textcolor{myblue}{\bm{\tau}^{\mathrm{s}}_1}, ..., \textcolor{myorange}{\bm{\tau}^{\mathrm{m}}_K}, \textcolor{myblue}{\bm{\tau}^{\mathrm{s}}_K})$:

% \begin{equation*}
%     D_{s_0}=(\textcolor{myorange}{d[{\bm{\tau}^{\mathrm{m}}_1}]}, \textcolor{myblue}{d[{\bm{\tau}^{\mathrm{s}}_1}]}, ..., \textcolor{myorange}{d[{\bm{\tau}^{\mathrm{m}}_K}]}, \textcolor{myblue}{d[{\bm{\tau}^{\mathrm{s}}_K}]}|s_0).
% \end{equation*}

 \begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/method-process2-cropped.pdf}
    \caption{\textbf{Pre-processing the source demonstration.} The raw point cloud observations are processed by cropping, clustering, and down-sampling. The source action trajectory is parsed into \textcolor{myorange}{motion} and \textcolor{myblue}{skill} segments by referring to the semantic masks of manipulated objects.}
    \label{fig:method_parse}
    \vspace{-0.2cm}
\end{figure}




\subsection{Pre-processing the Source Demonstration}
\label{sec:method-preprocess}

\noindent \textbf{Segmented point cloud observations.}  
% Given a raw point cloud observation, we first remove redundant points such as those from the table surface, retaining only the points associated with the manipulated object(s) and the robot's end-effector. A clustering operation is then applied to filter out outlier points~\cite{ester1996density}, handling the noise in real-world point cloud observations. Subsequently, the point cloud is downsampled to a fixed number of points (e.g., $512$ or $1024$) using farthest point sampling~\cite{qi2017pointnet}. For the initial frame of the trajectory, we leverage Grounded SAM~\cite{ren2024grounded} to obtain segmentation masks for the manipulated objects from the RGB image. These masks are then applied to the pixel-aligned depth image and projected onto the 3D point cloud, as depicted in Fig.~\ref{fig:method_parse}. We assume the table surface and the backgrounds have been cropped cleanly so that the rest of the point cloud within the cropped workspace belongs to the robot end-effector.
To improve the practical applicability in real-world scenarios, we utilize a single-view RGBD camera for point cloud acquisition.
The raw point cloud observations are first preprocessed by cropping the redundant points from the background and table surface. We assume the retained points are associated with either the manipulated object(s) or the robotâ€™s end-effector. A clustering operation~\cite{ester1996density} is then applied to filter out the outlier points in noisy real-world observations. Subsequently, the point cloud is downsampled to a fixed number of points (e.g., 512 or 1024) using farthest point sampling to facilitate policy learning~\cite{qi2017pointnet}. 

For the first frame of the trajectory, we employ Grounded SAM~\cite{ren2024grounded} to obtain the segmentation masks for the manipulated objects from the RGB image. These masks are then applied to the pixel-aligned depth image and projected onto the 3D point cloud, as shown in Fig.~\ref{fig:method_parse}.


\vspace{0.2cm} 
\noindent \textbf{Parsing the source trajectory.}  
Following previous work~\cite{mandlekar2023mimicgen, garrett2024skillmimicgen}, we assume that the execution trajectory can be parsed into a sequence of object-centric segments. Noticing that the robot must initially \textcolor{myorange}{approach} the object in free space before engaging in on-object manipulation through \textcolor{myblue}{contact}, each object-centric segment can be further subdivided into two stages: \textcolor{myorange}{motion} and \textcolor{myblue}{skill}. For example, in the task illustrated in Fig.~\ref{fig:method_parse}, the trajectory is divided into four stages: \textcolor{myorange}{1) \textit{move} to the flower}, \textcolor{myblue}{2) \textit{pick} up the flower}, \textcolor{myorange}{3) \textit{transfer} the flower to the vase}, and \textcolor{myblue}{4) \textit{insert} the flower into the vase}.

% With the segmented point cloud observations in hand, 
We can easily identify the skill segments associated with a given object by checking whether the distance between the geometric center of the object's point cloud and the robot's end-effector falls within a predefined threshold, as illustrated by the spheres in Fig.~\ref{fig:method_parse}. The intermediate trajectories between two skill segments are classified as motion segments.

Formally, we represent an interval of time stamps as $\bm{\tau}$:
\begin{equation*}
    \bm{\tau} = (t_{\mathrm{start}},~ t_{\mathrm{start}}+1,~ \dots, ~t_{\mathrm{end}}-1,~ t_{\mathrm{end}})\subseteq(0,1,\dots,L-1),
\end{equation*}
which can be used as an \textit{index sequence} for the extraction of the corresponding segments from a sequence of demonstrations, actions, or observations. For instance, $d[\bm{\tau}] = (d_{t_{\mathrm{start}}},d_{t_{\mathrm{start}}+1},\dots,d_{t_{\mathrm{end}}-1}, d_{t_{\mathrm{end}}})$ represents the extracted subset of source demonstration indexed by $\bm{\tau}$.
Using this notation, we parse the source demonstration into alternating \textcolor{myorange}{motion} and \textcolor{myblue}{skill} segments according to the index sequence
$(\textcolor{myorange}{\bm{\tau}^{\mathrm{m}}_1}, \textcolor{myblue}{\bm{\tau}^{\mathrm{s}}_1}, \dots, \textcolor{myorange}{\bm{\tau}^{\mathrm{m}}_K}, \textcolor{myblue}{\bm{\tau}^{\mathrm{s}}_K})$:

\begin{equation*}
    D_{s_0} = (\textcolor{myorange}{d[{\bm{\tau}^{\mathrm{m}}_1}]}, \textcolor{myblue}{d[{\bm{\tau}^{\mathrm{s}}_1}]}, \dots, \textcolor{myorange}{d[{\bm{\tau}^{\mathrm{m}}_K}]}, \textcolor{myblue}{d[{\bm{\tau}^{\mathrm{s}}_K}]} | s_0).
\end{equation*}


% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figs/motion-skill-single.png}
%     \caption{\textbf{Generation for motion and skill stages.} }
%     \label{fig:method-motion-skill}
% \end{figure}




\begin{figure}[b]
    \vspace{-0.2cm}
    \centering
    \includegraphics[width=\linewidth]{figs/motion-skill-single-cropped.pdf}
    \caption{\textbf{Illustrations for action generation.} (Left) Actions in the \textcolor{myorange}{motion} stage are planned to connect the neighboring skill segments. (Right) Actions in the \textcolor{myblue}{skill} stage undergo a uniform transformation.}
    \label{fig:method-motion-skill}
\end{figure}



\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/process-4-stage-final.pdf}
    
    \caption{\textbf{Illustrations for synthetic visual observation generation.} Objects in the \textit{to-do} stage are segmented and transformed by the target object configurations. Objects in the \textit{doing} stage are merged with the end-effector and transformed according to the proprioceptive states.}
    \label{fig:method-traj}
    \vspace{-0.2cm}
\end{figure*}





\subsection{TAMP-based Action Generation}

\noindent\textbf{Adapting actions to the new configuration.} The generation process begins by selecting a target initial configuration $s_0'=\{\mathbf{T}_0^{{O_1}'}, \mathbf{T}_0^{{O_2}'}, ..., \mathbf{T}_0^{{O_K}'}\}$. Under the $4\times 4$ homogeneous matrix representation, the spatial transformation between the target and source configurations is computed as: 
\begin{align*}
    \Delta s_0
    &=\{(\mathbf{T}_0^{{O_1}})^{-1} \cdot \mathbf{T}_0^{{O_1}'}, \dots, (\mathbf{T}_0^{{O_K}})^{-1} \cdot \mathbf{T}_0^{{O_K}'}\}.
    % \\&\triangleq \{\Delta \mathbf{T}_0^{{O_1}},\dots, \Delta \mathbf{T}_0^{{O_K}}\}.
\end{align*}

Recall that the actions consist of both robot arm and robot hand commands. The robot hand commands define the interactive actions \textit{on} the object, e.g., holding the flower with the gripper, or rolling up the dough with the dexterous hand. Since they are \textit{invariant} of the spatial transformation, $a^{\mathrm{hand}}_t$ should remain unchanged regardless of the object configuration:
\begin{equation*}
    \hat{a}^{\mathrm{hand}}_t=a^{\mathrm{hand}}_t, \quad\forall~ t, s_o, s_0'.
\end{equation*}

In contrast, the robot arm commands should be spatially \textit{equivariant} to the object movements in order to adjust the trajectory according to the altered configuration.
% Without loss of generality, we discuss how to adapt the robot arm commands in the motion and skill segments centered around the $k$-th object $\textcolor{myorange}{\mathbf{A}^\mathrm{EE}[{\bm{\tau}^{\mathrm{m}}_k}]}, \textcolor{myblue}{\mathbf{A}^\mathrm{EE}[{\bm{\tau}^{\mathrm{s}}_k}]}$. 
Specifically, for the motion and skill segments involving the $k$-th object, we adapt the robot arm commands $\textcolor{myorange}{\mathbf{A}^\mathrm{EE}[{\bm{\tau}^{\mathrm{m}}_k}]}, \textcolor{myblue}{\mathbf{A}^\mathrm{EE}[{\bm{\tau}^{\mathrm{s}}_k}]}$ following a TAMP-based procedure, illustrated by Fig.~\ref{fig:method-motion-skill}.

For the skill segments with dexterous on-object behaviors, the spatial relations between end-effectors and objects must remain relatively static. Thus, the entire skill segments are transformed following the corresponding objects:
\begin{equation*}
    \textcolor{myblue}{\hat{\mathbf{A}}^\mathrm{EE}[{\bm{\tau}^{\mathrm{s}}_k}]}=\textcolor{myblue}{\mathbf{A}^\mathrm{EE}[{\bm{\tau}^{\mathrm{s}}_k}]} \cdot (\mathbf{T}_0^{{O_k}})^{-1} \cdot \mathbf{T}_0^{{O_k}'}.
\end{equation*}

For the motion segments moving in free space, the goal is to chain adjacent skill segments. Therefore, we plan the robot arm commands in the motion stage via motion planning:
\begin{equation*}
    \textcolor{myorange}{\hat{\mathbf{A}}^\mathrm{EE}[{\bm{\tau}^{\mathrm{m}}_k}]}=\texttt{MotionPlan}(\textcolor{myblue}{\hat{\mathbf{A}}^\mathrm{EE}[{\bm{\tau}^{\mathrm{s}}_{k-1}}][-1]},~ \textcolor{myblue}{\hat{\mathbf{A}}^\mathrm{EE}[{\bm{\tau}^{\mathrm{s}}_k}][0]}),
\end{equation*}
where the starting pose for motion planning is taken from the last frame of the previous skill segment, and the ending pose is from the first frame of the current skill segment. For simple uncluttered workspaces, linear interpolation suffices. For complex environments requiring obstacle avoidance, an off-the-shelf motion planning method~\cite{kuffner2000rrt} is employed.




\vspace{0.2cm} \noindent\textbf{Failure-free action execution.}
% Without on-robot rollouts to filter out the failed trajectories, we have to ensure failure-free execution of the generated actions for the validity of synthetic demonstrations.
% Unlike the operational space controller~\cite{khatib1987unified} and delta end-effector pose control target used in previous works~\cite{mandlekar2023mimicgen,garrett2024skillmimicgen}, we enforce the use of inverse-kinematics (IK) controllers~\cite{Zakka_Mink_Python_inverse_2024} and absolute end-effector poses as the control targets.
% Empirically, we find that these engineering modifications could effectively minimize the compounding control error, thus achieving failure-free action execution.
To ensure the validity of synthetic demonstrations without on-robot rollouts to filter out failed trajectories, we require failure-free action execution. Unlike previous works~\cite{mandlekar2023mimicgen, garrett2024skillmimicgen} that rely on operational space controllers and delta end-effector pose control, we employ inverse kinematics (IK) controllers~\cite{Zakka_Mink_Python_inverse_2024} and target absolute end-effector poses. Empirically, these adjustments are found to help minimize compounding control errors, contributing to the successful execution of the generated actions.

\subsection{Fully Synthetic Observation Generation}

\noindent\textbf{Adapting proprioceptive states.} 
% The observations consist of the point clouds and robot proprioceptive states. Since the proprioceptive states hold the same semantics as the actions, the generated proprioceptive states should undergo the same transformation as the generated actions:
The observations consist of point cloud data and proprioceptive states. Since the proprioceptive states share the same semantics with the actions, they should undergo the same transformation:
\begin{align*}
    \hat{o}^{\mathrm{hand}}_t&=o^{\mathrm{hand}}_t, \quad\forall~ t, s_o, s_0'; \\
    \hat{o}^{\mathrm{arm}}_t&=o^{\mathrm{arm}}_t \cdot ({\mathbf{A}}^\mathrm{EE}_t)^{-1} \cdot \hat{{\mathbf{A}}}^\mathrm{EE}_t.
\end{align*}
% Notably, simply replacing the current state with the next target pose action (i.e., $\hat{o}^{\mathrm{arm}}_t \gets \hat{a}^{\mathrm{arm}}_{t+1}$) is found to hurt the performance, since the IK controllers do not necessarily take the end-effector to the exactly accurate target pose.
It is noteworthy that we found directly replacing the current state with the next target pose action (i.e., $\hat{o}^{\mathrm{arm}}_t \gets \hat{a}^{\mathrm{arm}}_{t+1}$) may impair performance, as the IK controllers may not always achieve the exact target pose.


\vspace{0.2cm} \noindent\textbf{Synthesizing point cloud observations.}
To synthesize the spatially augmented point clouds for the robot and objects, we employ a simple segment-and-transform strategy.
Apart from the target transformations, the only required information for synthesis is the segmentation masks for the $K$ objects on the first frame of the source demonstration, obtained in Sec.~\ref{sec:method-preprocess}.

% From an object-centric perspective, each object will successively go through the following $3$ stages. In the \textit{to-do} stage, the object is unaffected by the robot and remains static, so we use the mask on the first frame to segment its point cloud and make it undergo the corresponding transformation in the initial configuration, i.e., $(\mathbf{T}_0^{{O_k}})^{-1} \cdot \mathbf{T}_0^{{O_k}'}$. In the \textit{doing} stage, the object is in close contact with the robot, so we merge its point cloud with the robot end-effector point cloud, which will be handled together with the end-effector. In the \textit{done} stage, the object will no longer be manipulated and therefore remains in the state as the last frame in the \textit{doing} stage.


For each object, we define $3$ stages. In the \textit{to-do} stage, the object is static and unaffected by the robot, and its point cloud is transformed according to the initial object configuration $(\mathbf{T}_0^{{O_k}})^{-1} \cdot \mathbf{T}_0^{{O_k}'}$. In the \textit{doing} stage, the object is in contact with the robot, and its point cloud is merged with the end-effectorâ€™s point cloud. In the \textit{done} stage, the object remains in its final state. These stages are easily identified by referencing the trajectory-level motion and skill segments.

For the robotâ€™s end-effector, its point cloud undergoes the same transformation as indicated by the proprioceptive states
$({\mathbf{A}}^\mathrm{EE}_t)^{-1} \cdot \hat{{\mathbf{A}}}^\mathrm{EE}_t$. 
Given the assumption of a cropped workspace, the point clouds for the robot and the objects in the \textit{doing} stage can be separated by subtracting the object point clouds in the \textit{to-do} and \textit{done} stages from the scene point cloud.

A concrete example of this process is shown in Fig.~\ref{fig:method-traj}. More examples of the synthetic trajectories in real-world experiments can be found in Fig.~\ref{fig:traj-examples} in the appendix.