\section{Introduction}

Visuomotor policy learning has demonstrated remarkable competence for robotic manipulation tasks~\cite{chi2023diffusion_policy,zhao2023learning,fu2024mobile,ze20243d}, yet it typically demands large volumes of human-collected data. State-of-the-art approaches often require tens to hundreds of demonstrations to achieve moderate success on complex tasks, such as spreading sauce on pizza~\cite{chi2023diffusion_policy} or making rollups with a dexterous hand~\cite{ze20243d}. More intricate, long-horizon tasks may necessitate thousands of demonstrations~\cite{zhao2024aloha}.

One key factor contributing to the data-intensive nature of these methods is their limited \textbf{spatial generalization}~\cite{saxena2024what,tan2024manibox} ability. 
% We provide empirical evidence suggesting that visual imitation learning methods~\cite{chi2023diffusion_policy}, even when coupled with pre-trained or 3D visual encoders~\cite{nair2023r3m,radford2021learning,oquab2023dinov2,ze20243d}, exhibit limited spatial capacity generally confined to regions adjacent to the demonstrated object configurations. As a result, to enable effective imitation learning across the entire tabletop workspace, human data collectors must repeatedly reposition the objects over the table surface and recollect demonstrations. 
Our empirical study suggests that visuomotor policies~\cite{chi2023diffusion_policy}, even when coupled with pre-trained or 3D visual encoders~\cite{nair2023r3m,radford2021learning,oquab2023dinov2,ze20243d}, exhibit limited spatial capacity, typically confined to regions adjacent to the demonstrated object configurations. 
Such limitation necessitates repeated data collection with repositioned objects until the demonstrated configurations sufficiently cover the full tabletop workspace. 
This creates a paradox: while the critical actions enabling dexterous manipulation are concentrated in a small subset of contact-rich segments, a substantial portion of human effort is spent teaching robots to approach objects in free space.

A potential solution to reduce redundant human effort is to replace the tedious relocate-and-recollect procedure with automatic demonstration generation. 
Recent advances such as MimicGen~\cite{mandlekar2023mimicgen} and its subsequent extensions~\cite{hoque2024intervengen,garrett2024skillmimicgen,jiang2024dexmimicgen} have proposed to generate demonstrations by segmenting the demonstrated trajectories based on object interactions. 
These object-centric segments are then transformed and interpolated into execution plans that fit desired spatially augmented object configurations. 
The resulting plans are then executed through open-loop rollouts on the robot, termed \textit{on-robot rollouts}, to verify their correctness and simultaneously capture the visual observations needed for policy training.

Despite their success in simulation, applying MimicGen-style strategies to real-world environments is hindered by the high costs of on-robot rollouts, which are nearly as expensive as collecting raw demonstrations. An alternative is to deploy via sim-to-real transfer~\cite{peng2018sim,torne2024reconciling,yuan2024learning}, though bridging the sim-to-real gap remains a significant challenge in robotics.

\vspace{0.2cm}   
In this work, we introduce \method, a data generation system that can be seamlessly plugged into the policy learning workflow in both simulated and physical worlds.
% To achieve this, we introduce \method, a system that explicitly generates \textbf{real-world} demonstrations. 
Recognizing the high cost of on-robot rollouts represents a major barrier to practical deployment, \method adopts a \textbf{fully synthetic} pipeline that efficiently concretizes the generated plans into spatially augmented demonstrations ready for policy training.

For action generation, \method develops the MimicGen strategy by incorporating techniques from Task and Motion Planning (TAMP)~\cite{dalal2023imitating,cheng2023nod,mandlekar2023human}, similar to the practice in the recently released SkillMimicGen~\cite{garrett2024skillmimicgen}.
Specifically, we decompose the source trajectory into \textit{motion segments} moving in free space and \textit{skill segments} involving on-object manipulation through contact. During generation, the skill segments will be transformed as a whole according to the augmented object configuration, and the motion segments will be replanned via motion planning to connect the neighboring skill segments after transformation.
% Compared with MimicGen, this TAMP-based approach generates smoother action trajectories, which could facilitate policy learning.


With the processed actions in hand, a core challenge is obtaining spatially augmented visual observations without relying on costly on-robot rollouts. While some recent work leverages vision foundation models to manipulate the appearance of subjects and backgrounds in robotic tasks~\cite{yu2023scaling,chen2023genaug,chen2024mirage}, these techniques are not directly applicable to modifying the spatial locations of objects in an image, as 2D generative models generally lack awareness of 3D spatial relationships, such as perspective changes~\cite{xu20223d}.

\method employs a more straightforward strategy: it selects point clouds as the observation modality and synthesizes the augmented visual observations through 3D editing. 
The key insight is that point clouds, which inherently live in the 3D space, can be easily manipulated to reflect the desired spatial augmentations. 
Generating augmented point cloud observations is reduced to identifying clusters of points corresponding to the objects or robot end-effectors and then applying the same spatial transformations used in the generated action plans. 
Notably, this strategy also applies to contact-rich skill segments, as parts in contact are treated as cohesive clusters that undergo uniform transformations. 
Furthermore, the artificially applied transformations on point clouds accurately reflect the underlying physical processes, thereby minimizing the visual gap between real and synthetic observations.

\vspace{0.2cm}
Empirically, we manifest the effectiveness of \method by evaluating the performance of visuomotor policies trained on \method-generated datasets from \textbf{only one} human collected demonstration per task. To assess the impact of \method on spatial generalization, we adhere to a rigorous evaluation protocol in which the objects are placed across the entire tabletop workspace within the end-effectors' reach.

We conduct extensive real-world experiments, showing that \method can be successfully deployed on both single-arm and bi-manual platforms, using parallel-gripper and dexterous-hand end-effectors, from both third-person and egocentric observation viewpoints, and with a range of rigid-body and deformable/fluid objects. Meanwhile, the cost of generating one demonstration trajectory with \method is merely $\mathbf{0.01}$ seconds of computation. With such minimal cost, \method significantly enhances policy performance, generalizing to un-demonstrated configurations and achieving an average of $\mathbf{74.6}\%$ across $\mathbf{8}$ real-world tasks. Additionally, we demonstrate that simple extensions under the \method framework can further equip imitation learning with acquired out-of-distribution generalization capabilities such as disturbance resistance and obstacle avoidance. The code and datasets will be open-sourced to facilitate reproducibility of our results. \textbf{\textit{Please refer to the project website for robot videos.}}