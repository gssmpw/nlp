\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/spatial_gen_heatmaps-0130-cropped.pdf}
    \caption{\textbf{Qualitative visualization of the spatial effective range.} The grid maps display discretized tabletop workspaces from a bird's-eye view under different demonstration configurations. Dark green spots mark the locations where buttons are placed during the demonstrations. Each grid cell corresponds to a policy rollout with the button placed at that location. Blue, yellow, green, and gray grids denote successful executions for the Button-Large, Button-Small, both tasks, and no tasks, respectively.}
    \label{fig:spatial_gen_vis}
    \vspace{-0.2cm}
\end{figure*}



\section{Related Works}
\subsection{Visuomotor Policy Learning}
Represented by Diffusion Policy~\cite{chi2023diffusion_policy} and its extensions~\cite{ze20243d,ke20243d,prasad2024consistency,wang2024one,wang2024equivariant}, visuomotor policy learning refers to the imitation learning methods that learn to predict actions directly from visual observations in an end-to-end fashion~\cite{levine2016end}. 
% In the context of robotic manipulation, visual IL is also known as behavior cloning (BC). 
% Compared with cascade systems such as Task and Motion Planning (TAMP)~\cite{dalal2023imitating,cheng2023nod,mandlekar2023human}, the primary advantages of visual IL are two-folded. First, its end-to-end learning objective avoids the object state estimation process, making it flexibly applicable to more complex scenes beyond rigid-body pick-and-place. Second, its closed-loop execution makes it responsive enough to perform retrying behaviors in more dexterous tasks while resisting external disturbance.
The end-to-end learning objective is a two-edged sword. Its flexibility enables visuomotor policies to learn dexterous skills from human demonstrations, extending beyond rigid-body pick-and-place. However, the absence of structured skill primitives makes such policies intrinsically data-intensive.

The conflicts between the huge data demands and the great expense of robotic data collection have driven the growing attention to data-centric research. Such efforts include more efficient data collection systems~\cite{chi2024universal,cheng2024open,li2024okami}, collaborative gathering of large-scale datasets~\cite{o2024open,khazatsky2024droid}, and empirical studies on data scaling~\cite{zhao2024aloha,lin2024data}. Instead of scaling up via pure human labor, \method aims to show that synthetic data generation can help save much of the human effort.


\subsection{Data-Efficient Imitation Learning}
Attempting to develop manipulation policies from only a handful of demonstrations, data-efficient imitation learning methods often build on the principles of Task and Motion Planning (TAMP), while incorporating imitation learning to replace some components in the TAMP pipeline.
A common approach is to learn the end-effector poses for picking and placing~\cite{zeng2021transporter,simeonov2022neural,wen2022you,xue2023useek,gao2024riemann}. The whole trajectories are generated using motion planning toolkits~\cite{kuffner2000rrt} and then executed in an open-loop manner. 
Some methods extend this idea to more complex scenarios by learning to estimate the states of manipulated objects in the environment and replaying demonstrated trajectory segments centered around the target objects~\cite{johns2021coarse,valassakis2022demonstrate,di2022learning,di2024dinobot}.
While these approaches are effective for simpler, Markovian-style tasks~\cite{vosylius2024instant}, their reliance on open-loop execution limits their application to more dexterous tasks requiring closed-loop retrying and re-planning.


% Alternatively, \method utilizes the idea of TAMP for synthetic data generation which ultimately facilitates the training of closed-loop visual IL agents. In this way, \method implicitly combines the merits of both approaches.

In contrast, \method leverages the TAMP principles for synthetic data generation. Subsequently, the synthetic demonstrations are used to train closed-loop visuomotor policies for task resolution. In this way, \method effectively combines the merits of both approaches.









\subsection{Data Generation for Robotic Manipulation}
Automated demonstration generation offers the opportunity to breed capable visuomotor policies with significantly reduced human efforts. A branch of recent works attempts to generate demonstrations by leveraging LLM for task decomposition and then using planning or reinforcement learning for subtask resolution~\cite{wang2023gensim,hua2024gensim2,wang2023robogen}. While this paradigm enables data generation from the void, the resulting manipulation skills are often restricted by the capacity of either LLM, planning, or reinforcement learning.

% Another line of representative works is MimicGen~\cite{mandlekar2023mimicgen} and its extensions~\cite{hoque2024intervengen,garrett2024skillmimicgen,jiang2024dexmimicgen}. In contrast to generating from the void, MimicGen adapts source demonstrations to unseen object configurations by synthesizing the corresponding execution plans. In principle, this adaptation applies to any manipulation skills on any manipulated objects. For instance, DexMimicGen~\cite{jiang2024dexmimicgen} shows how MimicGen strategy is extended to be compatible with bi-manual platforms with dexterous hands end-effectors. 
% However, execution plans are not ready-to-use demonstrations in the form of observation-action pairs. The MimicGen family~\cite{mandlekar2023mimicgen,hoque2024intervengen,garrett2024skillmimicgen,jiang2024dexmimicgen} concretizes this transition by running expensive on-robot rollouts, which hinders their deployment to physical robots.

% Built upon MimicGen and its extensions, \method accepts their strategies for generating execution plans but replaces the expensive on-robot rollouts with an efficient fully synthetic process to allow real-world demonstration generation.


An alternative line of research is exemplified by MimicGen~\cite{mandlekar2023mimicgen} and its extensions~\cite{hoque2024intervengen,garrett2024skillmimicgen,jiang2024dexmimicgen}. Unlike generating demonstrations from the void, MimicGen adapts some human-collected source demonstrations to novel object configurations by synthesizing corresponding execution plans. This approach is theoretically applicable to a wide range of manipulation skills and object types. For example, DexMimicGen~\cite{jiang2024dexmimicgen} extends MimicGenâ€™s strategy to support bi-manual platforms equipped with dexterous hand end-effectors.
However, execution plans produced by the MimicGen framework are not ready-to-use demonstrations in the form of observation-action pairs. To bridge this gap, the MimicGen family~\cite{mandlekar2023mimicgen,hoque2024intervengen,garrett2024skillmimicgen,jiang2024dexmimicgen} relies on costly on-robot rollouts, which poses significant challenges for the deployment on physical robots.

Building upon MimicGen and its extensions, \method incorporates their strategies for generating execution plans, but replaces the expensive on-robot rollouts with an efficient, fully synthetic generation process. This enables \method to generate real-world demonstrations ready for policy training in a cost-effective manner.