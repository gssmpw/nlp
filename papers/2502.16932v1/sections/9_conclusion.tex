\section{Conclusion}

In this work, we introduced \method, a fully synthetic data generation system designed to facilitate visuomotor policy learning by mitigating the need for large volumes of human-collected demonstrations. 
Through TAMP-based action adaption and 3D point cloud manipulation, \method enables the generation of spatially augmented demonstrations with minimal computational cost, significantly improving spatial generalization and policy performance across a wide range of real-world tasks and platforms.
% Additionally, we extend \method to produce demonstrations with disturbance resistance and obstacle avoidance behaviors. These demonstrations equip the policies with the corresponding capabilities, highlighting the critical role of the demonstration data in enabling policy capabilities.
Furthermore, we extend \method to generate demonstrations incorporating disturbance resistance and obstacle avoidance behaviors, endowing the trained policies with the corresponding capabilities. 
% This underscores the crucial role that high-quality demonstration data plays in enabling robust policy performance.


% The results highlight the potential of \method to reduce the burden of data collection, while also improving generalization to un-demonstrated configurations. Moving forward, \methodâ€™s approach can serve as a foundation for more scalable and efficient policy learning in complex manipulation tasks.

\vspace{0.2cm} \noindent\textbf{Limitations.} 
Although we have demonstrated the effectiveness of \method, it has several limitations.
First, \method relies on the availability of segmented point clouds, which limits its applicability in highly cluttered or unstructured environments.
Second, \method is not suitable for tasks where spatial generalization is not required, such as in-hand reorientation~\cite{chen2022system} or push-T~\cite{florence2022ibc,chi2023diffusion_policy} with a fixed target pose.
Third, the performance of \method is affected by the visual mismatch problem, as previously discussed in Sec.~\ref{sec:visual-mismatch}. 

\vspace{0.2cm} \noindent\textbf{Future works.} 
Future works could explore mitigating the impact of visual mismatch, potentially by leveraging techniques such as contrastive learning or 3D generative models.
Another avenue for future research is to use additional human-collected demonstrations as source data, aiming to identify the optimal balance between policy performance and the overall cost of data collection.


\section*{Acknowledgement}
We would like to give special thanks to Galaxea Inc. for providing the R1 robot and Jianning Cui, Ke Dong, Haoyin Fan, and Yixiu Li for their technical support. We also thank Gu Zhang, Han Zhang, and Songbo Hu for hardware setup and data collection, Yifeng Zhu and Tianming Wei for discussing the controllers in the simulator, and Widyadewi Soedarmadji for the presentation advice.
Tsinghua University Dushi Program supports this project.