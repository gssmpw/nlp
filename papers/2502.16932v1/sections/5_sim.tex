\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/sim_gallery-final.pdf}
    \vspace{-0.55cm}
    \caption{\textbf{Tasks for simulated evaluation on spatial generalization.} Purple and sky-blue rectangles mark the workspaces for demonstration generation and evaluation, respectively. The detailed sizes of these workspaces are listed in Tab.~\ref{table:sim-range} in the appendix.}
    \label{fig:sim-tasks}
\end{figure*}

\input{tabs/sim-results}
\section{Experiments in the Simulator}

\subsection{Effectiveness: One-Shot Imitation}
Before deploying \method to the real world, we evaluate its effectiveness in the simulator by training visuomotor policies on datasets generated by \method from only one source demonstration per task.

\vspace{0.2cm} \noindent\textbf{Policy.}
Both in the simulator and real world, we select DP3~\cite{ze20243d} as the visuomotor policy, which predicts actions by consuming point cloud and proprioception observations. For a fair comparison, we fix the total training steps counted by observation-action pairs for all evaluated settings, resulting in an equal training cost regardless of the dataset size.
The training details are listed in Appendix~\ref{sec:appendix-policy-training}.

\vspace{0.2cm} \noindent\textbf{Tasks.}
We design $8$ tasks adapted from the MetaWorld~\cite{yu2020metaworld} benchmark, illustrated in Fig.~\ref{fig:sim-tasks}. To strengthen the significance of spatial generalization, we modify these tasks to have enlarged object randomization ranges, as listed in Appendix~\ref{sec:appendix-sim-range}.

\vspace{0.2cm} \noindent\textbf{Generation and evaluation.}
We write scripted policies for these tasks and prepare only $1$ source demonstration per task for demonstration generation. We also produce $10$ and $25$ source demonstrations per task using the scripted policy as a reference for human-collected datasets. 
Based on the one source demonstration, we leverage \method to generate $100$ spatially augmented demonstrations for the tasks containing the spatial randomization of one object. Since the tasks concerning two objects have a more diverse range of object configurations, $200$ demonstrations are generated. 

\vspace{0.2cm} \noindent\textbf{Results analysis.}
The evaluation results for the simulated tasks are presented in Tab.~\ref{table:sim-result}. \method significantly enhances the policy performance compared with the source demonstration baseline. The policies trained on \method-generated datasets also outperform those trained on $10$ source demonstrations and get close to $25$ source demonstrations. This indicates \method has the potential to maintain the policy performance with over $20\times$ reduced human effort for data collection. 


\subsection{Limitation: The Visual Mismatch Problem}
\label{sec:visual-mismatch}
While the one-shot imitation experiment verifies the effectiveness of \method, it also reveals its limitation: synthetic demonstrations generated from one source demonstration are not as effective as the same number of human-collected demonstrations. We attribute the performance gap to the visual mismatch between the synthetic point clouds and those captured in the real world, under the constraint of a single-view observation perspective. An illustration is provided in Fig.~\ref{fig:visual-mismatch}. 

\begin{figure}
    \vspace{-0.4cm}
    \centering
    \includegraphics[width=1\linewidth]{figs/3_cube.png}
    \vspace{-0.5cm}
    \caption{\textbf{Illustration for the visual mismatch problem.} As objects move through 3D space, their appearance changes due to variations in perspective. Under the constraint of a single-view observation, synthetic demonstrations consistently reflect a fixed side of the object's appearance seen in the source demonstration. This discrepancy causes a visual mismatch between the synthetic and real-captured data.}
    \label{fig:visual-mismatch}
\end{figure}



\vspace{0.2cm} \noindent\textbf{Performance saturation.} A notable consequence of the visual mismatch problem is the phenomenon of performance saturation. 
An empirical analysis is conducted on the Pick-Cube task. 
In Fig.~\ref{fig:performance-saturation}(a), we fix the spatial density of target object configurations in the synthetic demonstrations and increase their spatial coverage by adding more synthetic demonstrations.
The curve indicates that the performance improvement plateaus once the spatial coverage exceeds a certain threshold. This saturation occurs because the visual mismatch intensifies as the distance between the source and synthetic object configurations increases, making additional synthetic demonstrations ineffective.
In Fig.~\ref{fig:performance-saturation}(b), a similar performance saturation effect is observed when we increase the density while keeping the spatial coverage fixed. This indicates excessive demonstrations are unnecessary once they sufficiently cover the workspace.

\begin{figure}
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=1\linewidth]{figs/crop_sim_limitation.pdf}
    \caption{\textbf{Performance Saturation.} We report the policy performance boost w.r.t. the increase of synthetic demonstrations over $3$ seeds.}
    \label{fig:performance-saturation}
    \vspace{-0.3cm}
\end{figure}