\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/workspace-blue.pdf}
    \caption{\textbf{Protocol for evaluating spatial generalization.} (a) Setups on the single-arm platform. (b) Illustration for the full-size evaluation workspace. (c) Illustration for the generation strategy targeting the evaluated configurations along with small-range perturbations.}
    \label{fig:real-spatial-setup}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/gallery-new.pdf}
    \caption{\textbf{Tasks for real-world evaluation on spatial generalization.} Spatula-Egg and Dex-Rollup are one-stage tasks involving contact-rich behaviors. Flower-Vase, Mug-Rack, Dex-Drill, and Dex-Coffee are two-stage tasks requiring precise manipulation.}
    \label{fig:real-spatial-tasks}
\end{figure*}







\section{Real-World Experiments: Spatial Generalization}
% We evaluate the spatial generalization capability of visual imitation learning enhanced by \method on $8$ real-world tasks across $3$ platforms. $7$ tasks are deployed on single-arm platforms equipped with parallel gripper or dexterous hand end-effectors. Additionally, one task is deployed on a bimanual humanoid platform with egocentric-view observation. A task summary is provided in Tab.~\ref{table:task-summary}.
% To highlight the effect of spatial generalization, we collect only one source demonstration and adopt a rigorous evaluation protocol.
% Additionally, we study the relationship between policy performance boost and the spatial coverage of demonstrations on the \texttt{Dex Cube} task.
We assess the spatial generalization capability of visuomotor policies enhanced by \method across $8$ real-world tasks deployed on $3$ different platforms. $7$ tasks are performed on single-arm platforms with parallel grippers or dexterous hand end-effectors. Additionally, one task is executed on a bimanual humanoid. A task summary is provided in Tab.~\ref{table:task-summary}.

\input{tabs/task_summary}


\subsection{Single-Arm Platforms}

\noindent\textbf{Tasks.}
% We design $7$ tasks using a Franka Panda arm. $3$ tasks are with the original Panda gripper. $4$ tasks modify the end-effector to be an Allegro dexterous hand. The tasks are visualized in Fig.~\ref{fig:real-spatial-tasks}, and briefly described as follows:
On the Franka Panda single-arm platform, we design $3$ tasks using the original Panda gripper and $4$ tasks using an Allegro dexterous hand as the end-effector. The \textcolor{myorange}{motion} and \textcolor{myblue}{skill} trajectories of these tasks are visualized in Fig.~\ref{fig:real-spatial-tasks} and the task descriptions are provided in Appendix~\ref{sec:appendix-task-real}.
For all tasks, a single Intel Realsense L515 camera is adopted to capture point cloud observations, as depicted in Fig.~\ref{fig:real-spatial-setup}(a).

\vspace{0.2cm} \noindent\textbf{Evaluation protocol.} 
% To highlight the spatial generalization capability, we define a large planar evaluation workspace whose size is the robot arm's maximum reach, illustrated in Fig.~\ref{fig:real-spatial-setup}(b). We uniformly sample $12$ points in the irregular-shaped workspace as the potential object configurations for evaluation, where the left-right and front-back distances between two neighboring points are both $15\textrm{cm}$.
To evaluate spatial generalization, we define a large planar evaluation workspace, the size of which corresponds to the maximum reach of the robot arm. We uniformly sample $12$ points within this irregularly-shaped workspace as the coordinates for potential object configurations, with a $15\textrm{cm}$ spacing between neighboring coordinates, as illustrated in Fig.~\ref{fig:real-spatial-setup}(b).

To determine the actual evaluated configurations for each task, we perform manual trials using kinematic teaching to confirm the feasibility of each configuration. For example, in the Dex-Rollup task, the dexterous hand can reach a piece of plasticine placed in the near-robot corner of the workspace with a vertical wrist angle. However, it cannot grasp a kettle in the same location using a horizontal wrist angle, as required in the Dex-Coffee task. We conduct trials on all feasible configurations and repeat the evaluations $5$ times per configuration to ensure the reliability of the results.



% Similar to the case in the simulator, we collect only one source demonstration.
% Different from the simulator, we find the point cloud observation captured by a single-view camera is very noisy. There might be flickering holes in the point clouds of the objects or projective smears on the object outlines. If only one point cloud trajectory is provided, even we performed clustering and downsampling operations in the point cloud pre-processing stage in Sec.~\ref{sec:method-preprocess}, we find the imitation learning policy sometimes might be severely overfitted to this irregular visual deterioration. 
% As a solution, we replay the source demonstration for additional $2$ times, and record the corresponding point cloud observations. In this way, we have a total of $3$ source demonstrations with different visual deterioration. 
% Since the cost of replaying the source demonstration twice is low, we think it is a beneficial tradeoff between the generation efficiency and effectiveness.

% We set the target object configurations for generation to be the object configurations for evaluation. However, when the task has precision requirements, we find the imitation learning agent is sensitive to even minor variations in the object configurations. Human operators cannot always precisely place the objects in the target configurations in the real world. 
% As a response, we perform additive small-range augmentations on the basis of evaluated object configurations. Specifically, each generated demonstration on the evaluation configuration will be further augmented into $9$ demonstrations in a $\pm 1.5 \textrm{cm} \times 1.5\textrm{cm}$ range, illustrated in Fig.~\ref{fig:real-spatial-setup}(c). these small-range augmentations are used to mimic the perturbations brought by human operators in real-world evaluation.

% To summarize, the total number of generated demonstrations is $3 \times (\mathrm{\# Eval}) \times 9$, which is the multiplication of $3$ collected and replayed demonstrations, times the number of evaluated configurations, times the $9$ small-range augmentations. The detailed number for each task is listed in Tab.~\ref{table:task-summary}.


\input{tabs/real-spatial-results}
\begin{figure*}
    \vspace{-0.3cm}
    \centering
    \includegraphics[width=1\linewidth]{figs/heatmap-final.pdf}
    \caption{\textbf{Spatial heatmaps for the real-world evaluation results.} 
    % The success rate for each coordinate is computed as the average across all the relevant trials. For one-stage tasks, the results from $5$ repetitions are averaged. For two-stage tasks, all combinations in which the coordinate is involved are considered. Taking the Flower-Vase task as an example, each vase coordinate is relevant to the $3$ flower coordinates, resulting in a total of $15$ trials with $5$ repetitions per combination.
    The success rate for each coordinate is calculated as the average across all relevant trials. For example, each coordinate of the vase in the Flower-Vase task is in combination with $4$ coordinates of the flower, including the one appearing in the source demonstration. This results in a total of $20$ trials, given $5$ repetitions per combination.}
    \label{fig:real-success-heatmap}
    \vspace{-0.2cm}
\end{figure*}





\vspace{0.2cm} \noindent\textbf{Generation strategy.}
As in the simulated environments, we collect only one source demonstration for each task. However, real-world point cloud observations are often noisy, with issues such as flickering holes in the point clouds or projective smearing around object outlines. Even after performing clustering and downsampling during the point cloud preprocessing stage (Sec.~\ref{sec:method-preprocess}), the imitation learning policy can overfit to these irregularities if only one demonstration is provided.

To mitigate this issue, we replay the source demonstration twice and capture the corresponding point cloud observations. The altogether $3$ point cloud trajectories enrich the diversity in visual degradations and help alleviate the overfitting problem. 
Since replaying twice is low-cost, we consider this approach a beneficial tradeoff between efficiency and effectiveness.

For each task, we set the generated object configurations to correspond to the evaluated configurations.
% However, we found that when tasks require precise object placements, the imitation learning agent is highly sensitive to even small deviations from the target configurations. 
However, human operators cannot always place objects with perfect precision in the real world, yet we found visuomotor policies are sensitive to even small deviations. Thus, we further augment the generated object configurations by adding small-range perturbations. Specifically, for each target configuration, we generate $9$ demonstrations with $(\pm 1.5 \textrm{cm}) \times (\pm1.5 \textrm{cm})$ perturbation to mimic slight placement variations in the real world. The final generated configurations are shown in Fig.~\ref{fig:real-spatial-setup}(c).

In summary, the total number of generated demonstrations is calculated as $3 \times (\mathrm{\# Eval}) \times 9$, which represents the $3$ source demonstrations, multiplied by the number of evaluated configurations, and further multiplied by the $9$ perturbations. The detailed counts for each task are listed in Tab.~\ref{table:task-summary}.








% The performance of imitation learning agents~\cite{ze20243d} trained on $3$ source demonstrations and \method-generated demonstrations are reported in Tab.~\ref{table:real-spatial-result}. The agents trained on source demonstrations are severely overfitted to the demonstrated configurations, aligned with the findings in our empirical study. Unaware of the visual observations, they always recite the demonstrated trajectory. 

% In contrast, agents trained on the generated dataset act responsively to the evaluated configurations, and achieve significantly enhanced success rates. We find \method is generally effective for tasks involving contact-rich behaviors, precision requirements, and dexterous-hand end-effectors. Even on the Dex-Drill and Dex-Coffee tasks where the performance boost is not as significant as the other tasks, we found the agents trained on generated datasets can always lead the dexterous-hand end-effectors to largely appropriate grasping and touching/pouring poses. Their suboptimal performance is mainly due to the high precision requirements, where even a slight deviation in the manipulation poses may cause the drill or the kettle to sip off the dexterous hand.

% To further understand the generalization capabilities empowered by \method, we visualize the spatial heatmaps for the evaluation results in Fig.~\ref{fig:real-success-heatmap}. The evaluated configurations enjoy a high success rate if they are close to the demonstrated configuration, and the performance declines with the increase of the transformed distance. 
% We attribute this phenomenon to the visual mismatch problem caused by single-view observations, previously discussed in Sec.~\ref{}.





\vspace{0.2cm} \noindent\textbf{Results analysis.}
The performance of visuomotor policies~\cite{ze20243d} trained on $3$ source demonstrations and \method-generated demonstrations are reported in Tab.~\ref{table:real-spatial-result}. Agents trained solely on source demonstrations exhibit severe overfitting behaviors, blindly replicating the demonstrated trajectory. 
In Appendix~\ref{sec:appendix-increase-source}, we evaluate the policy performance trained on datasets containing additional human-collected demonstrations. We found the spatial effective range of the trained policies is upper-bounded by the sum of demonstrated configurations, aligned with the findings in the empirical study in Sec.~\ref{sec:empirical}.


Similar to the effects of manually covering the workspace with human-collected demonstrations, \method-generated datasets enable the agents to display a more adaptive response to diverse evaluated configurations, resulting in significantly higher success rates. \method consistently enhances the performance across all the evaluated tasks. Although the performance gains are less pronounced in the Dex-Drill and Dex-Coffee tasks, we found the policies trained on the generated data still guide the dexterous hands to generally appropriate manipulation poses. The relatively lower performance is primarily due to stringent precision requirements.

% The relatively lower performance in these tasks is primarily due to the stringent precision requirements, where even small deviations in the manipulation poses can cause fatal failures, e.g., the drill or kettle slipping off from the hand.

To further investigate the generalization capabilities enabled by \method, we visualize the spatial heatmaps for the evaluated configurations in Fig.~\ref{fig:real-success-heatmap}. The heatmaps reveal high success rates on configurations close to the demonstrated ones, while the performance diminishes as the distance from the demonstrated configuration increases. We attribute this decline to the visual mismatch problem caused by single-view observations, as previously discussed in Sec.~\ref{sec:visual-mismatch}.

A notable observation arises in the Dex-Rollup task, where the policy trained on the \method-generated dataset could dynamically adjust the number of wrapping motions ranging from $2$ to $5$ in response to the distinct plasticity of every hand-molded piece of plasticine. This suggests the usage of \method is not in conflict with the resulting agent's closed-loop re-planning capability. The intrinsic strength of visuomotor policies is effectively preserved.

% Notably, we observed that in the Dex-Rollup task, the policy trained on the generated dataset adaptively adjusts the times of the wrapping motion from $2$ to $5$ based on the state of the plasticine. This indicates the agents trained on generated dataset are still able to perform closed-loop re-planning behaviors, which is an important advantage of visuomotor policies.


\input{tabs/generation_cost}

\vspace{0.2cm} \noindent\textbf{Generation cost.}
We compare the time cost of real-world demonstration generation between MimicGen~\cite{mandlekar2023mimicgen} and \method. We estimate MimicGen's time cost by multiplying the duration of replaying a source trajectory by the number of generated demonstrations and adding an additional $20$ seconds per trajectory for human operators to reset the object configurations. It is important to note that MimicGen involves continuous human intervention, while the time cost of \method is purely computational, without the involvement of either the robot or human operators.





% \textcolor{red}{TODO on dex-cube: performance of human demonstrations, 1-3-5-10 (x3).}

\subsection{Bimanual Humanoid Platform}



\noindent\textbf{Task.}
In addition to the tasks on the single-arm platform, we also designed a Fruit-Basket task on a Galaxea R1 robot, illustrated in Fig.~\ref{fig:real-humanoid}. The Fruit-Basket task is distinguished from the previous tasks by three key features: 

\textit{1) Bimanual manipulation.} The robot simultaneously grasps the basket with one arm and the banana with the other. The right arm then places the basket in the center of the workspace, while the left arm places the banana into the basket. 

\textit{2) Egocentric observation.} The camera is mounted on the robot's head~\cite{ze2024generalizable}. While the robot’s base is immobilized in this task, the first-person view opens opportunities for future deployment in mobile manipulation scenarios. 

\textit{3) Out-of-distribution orientations.} Still using a single human-collected demonstration, the banana is placed with orientational offsets (i.e., $45^\circ$, $90^\circ$, and $135^\circ$) relative to the original demonstration during evaluation, while the basket position is randomized within a $10\,\textrm{cm} \times 5\,\textrm{cm}$ workspace.






\vspace{0.2cm} \noindent\textbf{Generation strategy.}
% We adopt a similar generation procedure used in the single-arm platform, where we replay the human-collected demonstration twice to obtain a total of $3$ source demonstrations. \method generates synthetic demonstrations by independently adapting the source actions of the two arms into the respective object transformations. Small-range perturbations are skipped due to less demanding precision requirements in this task.
% Notably, the synthetic generation of point cloud observations under orientational offsets is not as unconditionally effective as translational offsets since the single camera only captures the objects' front-view appearance. To overcome this constraint, the humanoid maintains a stooping posture, observing the manipulated objects from a perspective close to the bird's-eye view. Thus, we can edit the object point clouds to perform all-directional yaw orientations.
The generation procedure follows a similar approach as that used for the single-arm platform. Specifically, the human-collected demonstration is replayed twice, yielding $3$ source demonstrations in total. \method generates synthetic demonstrations by independently adapting the actions of both arms to the respective transformations of the objects. Small-range perturbations are omitted in this task due to the relatively lower precision requirements.

A challenge in synthesizing point cloud observations with orientational offsets lies in the limited view provided by the single camera, which only captures the objects' front-facing appearance. To address this limitation, the humanoid robot adopts a stooping posture, enabling a near bird’s-eye view perspective. This adjustment allows for more effective point cloud editing to simulate full-directional yaw rotations.



\vspace{0.2cm} \noindent\textbf{Results analysis.} 
% The success rates for the source and generated datasets are compared in Tab~\ref{table:real-spatial-result} and the spatial heatmap is presented in Fig.~\ref{fig:real-success-heatmap}. The high success rate of $90.8\%$ confirms the effectiveness of \method for bimanual humanoid platforms and out-of-distribution orientations. A more detailed analysis is presented in Appendix~\ref{sec:appendix-humanoid}.
The success rates for both the source and generated datasets are compared in Tab.~\ref{table:real-spatial-result}, and the spatial heatmap is shown in Fig.~\ref{fig:real-success-heatmap}. The high success rate of $90.8\%$ demonstrates the effectiveness of \method on bimanual humanoid platforms and its ability to help policies generalize to out-of-distribution orientations. 
A more detailed analysis is presented in Appendix~\ref{sec:appendix-humanoid}.





\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/galaxea-arxiv_compressed.pdf}
    \caption{\textbf{Bimanual humanoid platform.} (a) Egocentric observations and bimanual manipulation. (b) The Fruit-Basket task involves the out-of-distribution orientations during evaluation.}
    \label{fig:real-humanoid}
    \vspace{-0.3cm}
\end{figure}




\begin{figure}[bp]
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=1\linewidth]{figs/sauce-final.pdf}
    \caption{\textbf{\method for disturbance resistance.} (a-c) Illustration, initial, and ending states of the Sauce-Spreading task. (d) Disturbance applied for quantitative evaluation. (e) Standard generation strategy. }
    \label{fig:real-sauce-setup}
\end{figure}



\section{Real-World Experiments: Extensions}
% In addition to spatial generalization, we demonstrate that simple extensions under the \method framework can endow visuomotor policies with enhanced capabilities such as disturbance resistance and obstacle avoidance.

\subsection{Disturbance Resistance}

% \noindent\textbf{Task.}
% The ability to perform closed-loop correction under disturbance is an important merit of visual imitation learning. Still given $1$ human collected and $2$ replayed source demonstrations, we study whether the \method-generated dataset can train imitation learning agents resistant to external disturbance.
% We adapt the Sauce-Spreading task from DP~\cite{chi2023diffusion_policy}. In this task, there is initially some sauce in the center of the pizza crust. Holding a spoon in hand, the gripper maneuvers the spoon to approach the center of the sauce and then spread the sauce to cover the pizza in a spiral pattern.

One critical advantage of visuomotor policies is their ability to perform closed-loop corrections under disturbances. We investigate whether a \method-generated dataset, derived from one human-collected and two replayed source demonstrations, can train visuomotor policies equipped with such capability.

\vspace{0.2cm} \noindent\textbf{Task.}
We consider a Sauce-Spreading task (Fig.~\ref{fig:real-sauce-setup}(a)) adapted from DP~\cite{chi2023diffusion_policy}. 
% In this task, a spoon held by the robotic gripper spreads sauce from the center of a pizza crust outward in a spiral pattern. 
Initially, the pizza crust contains a small amount of sauce at its center (Fig.~\ref{fig:real-sauce-setup}(b)). 
% The gripper must maneuver the spoon to first approach the sauce center and then spread it to cover the crust in a smooth, consistent manner.
The gripper maneuvers the spoon in hand to approach the sauce center and periodically spread it to cover the pizza crust in a spiral pattern (Fig.~\ref{fig:real-sauce-setup}(c)).

% \vspace{0.2cm} \noindent\textbf{Evaluation protocol.}
% During the spreading process, we disturb the pizza crust twice toward a neighboring spot in the workspace. We take the $5$ neighboring spots and repeat $5$ times on each spot, giving a total of $25$ trials.
% For quantitative evaluation, we measure the sauce coverage on the pizza. We also report a normalized score for sauce coverage, with $0$ for taking no operation and $100$ for human expert. Details for the calculation of these metrics can be found in Appendix~\ref{sec:appendix-disturb}.

\vspace{0.2cm} \noindent\textbf{Evaluation protocol.} During the sauce-spreading process, disturbances are introduced by shifting the pizza crust twice to the neighboring spots within the workspace. We consider $5$ neighboring spots (Fig.~\ref{fig:real-sauce-setup}(d)) and conduct $5$ trials per spot, resulting in $25$ trials.
For quantitative evaluation, we measure the sauce coverage on the pizza crust. Additionally, we report a normalized sauce coverage score, where $0$ represents no operation taken, and $100$ corresponds to human expert performance. Detailed calculations are provided in Appendix~\ref{sec:appendix-disturb}.





\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/sauce-gen-cropped.pdf}
    \caption{\textbf{Illustration for the ADR strategy. } Asynchronous transformations are applied to the disturbed object and the robot end-effector to simulate the disturbance resistance process.}
    \label{fig:real-sauce-gen}
\end{figure}









% \vspace{0.2cm} \noindent\textbf{Generation strategies.}
% A standard generation strategy is to take the $15$ intermediate points during the disturbance process as the initial object configurations, and then perform a regular \method data generation procedure for spatial generalization.

% Apart from the regular strategy, we present a generation strategy specialized in promoting disturbance resistance capability.
% At a time step for augmentation, the pizza crust is teleported to a nearby spot to mimic the disturbance process. The robot end-effector along with the spoon keeps static at the first time step, and then interpolates to approach the teleported pizza crust and goes on spreading.
% We call this strategy Augmentation for Disturbance Resistance (ADR).

\vspace{0.2cm} \noindent\textbf{Generation strategies.} A standard generation strategy selects $15$ intermediate spots (Fig.~\ref{fig:real-sauce-setup}(e)) observed during the disturbance process as the initial object configurations for a standard \method data generation procedure.

To specifically enhance disturbance resistance, we propose a specialized strategy named Augmentation for Disturbance Resistance (ADR), illustrated in Fig.~\ref{fig:real-sauce-gen}. In ADR, the pizza crust is artificially displaced to nearby positions at certain time steps to simulate the disturbance. The robot's end-effector, holding the spoon, initially remains static and subsequently interpolates its motion to re-approach the displaced crust before continuing the periodic spreading motion.


\input{tabs/real-disturb}


% \vspace{0.2cm} \noindent\textbf{Results analysis.}
% The sauce coverage and normalized scores for both the regular \method and \method with ADR strategies are reported in Tab.~\ref{table:real-disturb}.  
% We found that \method with ADR achieves significantly better performance than the regular \method, and even approaches the performance of human expert.
% This highlights the capabilities of imitation learning are enabled by the demonstration data. 
% The disturbance resistance capability of imitation learning is not automatically emerged, but learned from the demonstration data.

\vspace{0.2cm} \noindent\textbf{Results analysis.} Tab.~\ref{table:real-disturb} presents the sauce coverage and normalized scores for both the standard \method and the ADR-enhanced \method strategies. The ADR strategy significantly outperforms the standard \method, achieving performance comparable to human experts.
In the video, we showcase the ADR-enhanced policy is still robust under up to $5$ successive disturbances.
These findings underscore the critical role of the demonstration data in enabling policy capabilities. The ability to resist disturbances does not emerge naturally but is acquired through targeted disturbance-involved demonstrations.




% \subsection{Obstacle Avoidance}
% Likewise, the ability to avoid obstacles is also enabled by the obstacle avoidance behaviors in the demonstrations. We consider a Teddy-Box task, where the dexterous hand needs to grasp the teddy bear and transfer it into the box on the left. Trained solely on the source demonstrations, the imitation learning agent is unaware of the obstacle, e.g., it might knock over the coffee cup placed in the middle of the workspace. 

% To generate obstacle-involved demonstrations, we sample points on simple geometries such as boxes and cones, and then fuse these points into the point cloud observations captured in the real world. The obstacle-avoiding actions are easily generated by calling an off-the-shelf motion planning tool~\cite{kuffner2000rrt}. 

% In the experiments, we place $5$ daily life obstacles with diverse shapes in the middle of the workspace and conduct $5$ trials for each obstacle.
% Empirically, the imitation learning agent trained on the generated dataset successfully bypasses the obstacles in $22$ of the $25$ trials.
% We also notice that when there is no obstacle, the agent will perform the lower route in the source demonstrations, indicating the agent is responsive to the outer environments.






\subsection{Obstacle Avoidance}
\noindent\textbf{Task.}
The ability to avoid obstacles is also imparted through demonstrations containing obstacle-avoidance behaviors. To investigate such capability, we introduce obstacles to a Teddy-Box task, where the dexterous hand grasps the teddy bear and transfers it into the box on the left (Fig.~\ref{fig:real-obstacle}(a)). Trained on the source demonstrations without obstacles, the visuomotor policy fails to account for potential collisions, e.g., it might knock over the coffee cup placed in the middle (Fig.~\ref{fig:real-obstacle}(b)). 

\vspace{0.2cm} \noindent\textbf{Generation strategy.}
To generate obstacle-involved demonstrations, we augment the real-world point cloud observations by sampling points from simple geometries, such as boxes and cones, and fusing these points into the original scene (Fig.~\ref{fig:real-obstacle}(c)). Obstacle-avoiding trajectories are generated by a motion planning tool~\cite{kuffner2000rrt}, ensuring collision-free actions.

\vspace{0.2cm} \noindent\textbf{Evaluation and results analysis.}
For evaluation, we position $5$ everyday objects with diverse shapes in the middle of the workspace (Fig.~\ref{fig:real-obstacle}(d)) and conduct $5$ trials per object, resulting in a total of $25$ trials. The agent trained on the augmented dataset successfully bypasses obstacles in $22$ out of $25$ trials. Notably, in scenarios without obstacles, the agent follows the lower trajectory observed in the source demonstrations, indicating its responsiveness to environmental variations.





\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/obstacle-arxiv_compressed.pdf}
    \caption{\textbf{\method for obstacle avoidance.} (ab) Policy trained on the source demonstration collides with the unseen obstacle. (cd) Policy trained on the generated dataset could avoid diverse-shaped obstacles.}
    \label{fig:real-obstacle}
    \vspace{-0.2cm}
\end{figure}