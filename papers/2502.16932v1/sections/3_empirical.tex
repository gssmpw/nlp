





\section{Empirical Study: Spatial Generalization of Visuomotor Policies}
\label{sec:empirical}

% In this section, we present an empirical investigation into the spatial generalization capability of visuomotor policies, showing how the lack of such capability contributes to the data-intensive nature of visuomotor policy learning.

In this section, we present an empirical study examining the spatial generalization capability of visuomotor policies. We demonstrate how the lack of such generalization contributes to the data-intensive nature of learning visuomotor policies.

% The findings of the empirical study motivate the development of \method, which addresses the data demands of visuomotor policy learning via automatic demonstration generation.

% In this section, we present an empirical investigation into the spatial generalization capabilities of visual IL methods.
% Qualitatively, we observe that the spatial generalization range of IL agents can be approximated as the union of the regions centered around the demonstrated object placements. Quantitative analysis reveals that the expansion of the object randomization range and the expectations for higher success rates make all studied varieties of policy instantiations more data-hungry. From the perspective of policy learning, for an IL agent to be effective across an entire tabletop workspace, demonstrations must ``cover'' the full table surface, thus causing a substantial burden on data collection. This burden motivates the development of \method, which enables automatic data generation.

\subsection{Visualization of Spatial Effective Range}

% Spatial generalization refers to the ability to manipulate objects placed with unseen configurations in the demonstrations. We attempt to establish an intuitive understanding of spatial generalization by visualizing the relationship between the spatial effective range of visuomotor policies and the spatial distribution of the given demonstrations.
Spatial generalization refers to the ability of a policy to perform tasks involving objects placed in configurations that were not seen during training. To gain an intuitive understanding of spatial generalization, we visualize the relationship between the spatial effective range of visuomotor policies and the spatial distribution of demonstration data.

\vspace{0.2cm} \noindent\textbf{Tasks.} 
% To obtain an intuitive understanding of the spatial generalization range, 
% we design the environments to minimize the dexterity of the required skills while maximizing the range of object randomization. To this end, 
We evaluate a Button-Large task adapted from the MetaWorld~\cite{yu2020metaworld} benchmark, where the robot approaches a button and presses it down. The object randomization range is modified to a $30\,\mathrm{cm} \times 40\,\mathrm{cm} = 1200\,\mathrm{cm}^2$ area on the tabletop workspace, covering most of the end-effector’s reachable space. Noticing the large size of the button makes it pressed down even if the press motion does not precisely hit the center, we also examine a more precision-demanding variant, Button-Small, where the button size is reduced by a factor of $4$. 
% This modification reduces the fault tolerance and increases the precision requirements.

\vspace{0.2cm}\noindent\textbf{Policy.} 
% We select 3D Diffusion Policy (DP3)~\cite{ze20243d} as the visuomotor policy because our benchmarking results later indicate that 3D observations offer superior spatial generalization capabilities compared to the 2D counterparts. The training details are listed in Appendix~\ref{sec:appendix-policy-training}.
We adopt 3D Diffusion Policy (DP3)~\cite{ze20243d} as the studied policy, as our benchmarking results indicate that 3D observations provide superior spatial generalization compared to 2D approaches. Training details are provided in Appendix~\ref{sec:appendix-policy-training}.

\vspace{0.2cm} \noindent\textbf{Evaluation.} 
% Shown in Fig.~\ref{fig:spatial_gen_vis}, we uniformly sample $21$ points along each dimension within the workspace, resulting in a dense array of $441$ coordinates to place the button.
% Note that we intentionally sample points on the boundaries to evaluate on the full reach of the end-effector. 
% The demonstrations are produced by a script policy, and we consider $4$ different spatial distributions from sparse to dense.
% \texttt{single}, \texttt{sparse}, \texttt{dense}, and \texttt{full}.
% For all the settings, we visualize their corresponding evaluation results on the dense array of $441$ configurations for evaluation.
% on these points are visualized in grid maps along with the demonstrated placements, as shown in Fig.~\ref{fig:spatial_gen_vis}. 
To visualize the spatial effective range, we uniformly sample $21$ points along each axis within the workspace, resulting in a total of $441$ distinct button placements. Demonstrations are generated using a scripted policy, with $4$ different spatial distributions ranging from \texttt{single} to \texttt{full}. The performance of each configuration is evaluated on the $441$ placements, enabling a comprehensive assessment of spatial generalization. The visualization result is presented in Fig.~\ref{fig:spatial_gen_vis}.

\vspace{0.2cm} \noindent\textbf{Key findings.}
% In general, the spatial effective range of visuomotor policies can be approximated by the union of the neighboring areas around the object configurations in the given demonstrations. 
% Consequently, to train a policy that is generally effective across the entire object randomization range, the demonstrations must cover the full workspace, which causes a substantial burden on data collection.
% Meanwhile, the increase of task precision requirements will shrink the effective range to more constrained neighboring areas. Thus, more demonstrations are required in order to cover the workspace with a smaller radius of the neighboring areas. This explains why more demonstrations are required for high-precision tasks.
% More detailed analysis can be found in Appendix~\ref{sec:appendix-empirical-visualize}.
Overall, the spatial effective range of visuomotor policies is closely tied to the distribution of object configurations seen in the demonstrations. Specifically, the effective range can be approximated by the union of the areas surrounding the demonstrated object placements. Thus, to train a policy that generalizes well across the entire object randomization range, demonstrations must cover the full workspace, resulting in substantial data collection costs. Furthermore, as task precision requirements increase, the effective range shrinks to more localized areas, necessitating a greater number of demonstrations to adequately cover the workspace.
A more detailed analysis is available in Appendix~\ref{sec:appendix-empirical-visualize}.





% \subsection{Benchmarking on Spatial Generalization Capacity}

% \noindent\textbf{Algotithms.} Apart from Diffusion Policy (DP)~\cite{chi2023diffusion_policy} and 3D Diffusion Policy (DP3)~\cite{ze20243d} trained from scratch, we are also interested in the power of pre-trained visual encoders for spatial generalization. Thus, we also consider the pre-trained encoder settings where we replace the train-from-scratch encoder in DP with R3M~\cite{nair2023r3m}, DINOv2~\cite{oquab2023dinov2}, and CLIP~\cite{radford2021learning}. The former is a ResNet~\cite{he2016deep} encoder pre-trained on robotics-specific tasks, and the latter two are ViT~\cite{dosovitskiy2021an} encoders pre-trained on open-world vision tasks. The training and implementation details are listed in Appendix~\ref{}.


% \vspace{0.2cm} \noindent\textbf{Environments.}
% Considering the fact that of the task plays a role to determine this number, we manually design a more difficult Precise Assembly task where the fault tolerance for both picking and inserting the peg is 1cm. The randomization range for either the peg or peg holder is disjoint $40cm \times 20cm$, giving an overall $40cm \times 40cm = 1600 cm^2$ workspace. 
% To highlight the effects of object randomization range, we also consider a half setting where the randomization range is halved and a fixed setting where the objects do not move.

% \vspace{0.2cm} \noindent\textbf{Demonstrations.}
% The spatial generalization capacity in practice is reflected as the number of demonstrations required to learn a manipulation skill. To study the relationship between the number of demonstrations and the performance, we set an ascending gradient from 25 to 400 demonstrations.
% For all of the workspace settings, we randomly sample demonstrations in a range slightly larger than the evaluation workspace because we find that the agent performance will deteriorate on the boundaries. 


% \vspace{0.2cm} \noindent\textbf{Evaluation.} 
% In the full workspace setting, to ensure the reliability of our evaluation results to reflect spatial generalization, 45 evaluation positions are uniformly sampled from their own workspace for both the peg and peg holder, which gives a total of 2025 combinations of the different initial layouts considered at evaluation. In the half and fixed settings, the evaluated layouts are 225 and 1, respectively. The results are summarized in Fig.~\ref{fig:spatial_gen_benchmark}.



% \vspace{0.2cm} \noindent\textbf{Key Findings.}
% Compare along the left axis: We find 3D observations in DP3 lead to the strongest spatial generalization performance compared with all of the other 2D counterparts. Besides, CLIP and DINOv2 are also very competitive, significantly outperforming the from-scratch baseline. This indicates the pre-training on open-world vision tasks also contributes to the spatial reasoning capabilities in robotics tasks. It is worth mentioning that previous works also find 3D and open-world pre-trained encoders are helpful for robotic manipulation tasks~\cite{chi2024universal,lin2024data,burns2023makes,zhu2024point}, but their focus is mainly on the visual generalization problem. Our conclusion from the spatial generalization provides a complimentary perspective for encoder selections.

% Compare along the right axis: People often confuse the concept of task difficulty with the task precision requirement. The fixed and half randomization range settings show that even with high precision requirements, as long as the objects are not fully spatially randomized, we could still solve this task with few demonstrations. Thus, precision requirement and spatial randomization range should be two orthogonal factors that contribute to the task difficulty. 

% Compare along the bottom axis: Besides a rather monotonic increasing performance with more demonstrations, we also observe a strong effect of marginal benefits. Take the DP3 + full workspace setting as an example, an additional 50 demonstrations raise the performance by 37\% when there are 100 demonstrations. However, when we have 150 demonstrations, another 50 demonstrations could only raise the performance by 6\%. This is another evidence highlighting the extreme difficulty for a robot learning system to achieve 99+\% success rates.


\subsection{Benchmarking Spatial Generalization Capability}

The practical manifestation of the spatial generalization is reflected in the number of demonstrations required for effective policy learning. In the following benchmarking, we explore the relationship between the number of demonstrations and policy performance to determine how many demonstrations are sufficient for effective training.

\vspace{0.2cm} \noindent\textbf{Tasks.} 
% To suppress inaccurate but successful policy rollouts, we design a Precise-Assembly task (see Fig.~\ref{}) with a strict fault tolerance of $1\,\mathrm{cm}$ for both the picking and insertion stages. Both the peg and the socket are randomized in a $40\,\mathrm{cm} \times 20\,\mathrm{cm}$ area, resulting in an effective workspace of $40\,\mathrm{cm} \times 40\,\mathrm{cm} = 1600\,\mathrm{cm}^2$. To further analyze the influence of object randomization, we also consider a \texttt{half} workspace where the randomization range is halved for both objects and a \texttt{fixed} setting where object positions remain static. More details on this environment can be found in Appendix~\ref{sec:appendix-empirical-task}.
To suppress the occurrence of inaccurate but successful policy rollouts, we design a Precise-Peg-Insertion task that enforces a strict fault tolerance of $1\,\mathrm{cm}$ during both the picking and insertion stages, asking for millimeter-level precision. The peg and socket are randomized within a $40\,\mathrm{cm} \times 20\,\mathrm{cm}$ area, yielding an effective workspace of $40\,\mathrm{cm} \times 40\,\mathrm{cm} = 1600\,\mathrm{cm}^2$. To examine the influence of object randomization, we also consider a \texttt{half} workspace, where the randomization range is halved for both objects, and a \texttt{fixed} setting, where object positions remain fixed. More details are listed in Appendix~\ref{sec:appendix-empirical-task}.

\vspace{0.2cm} \noindent\textbf{Policies.} In addition to Diffusion Policy (DP)~\cite{chi2023diffusion_policy} and 3D Diffusion Policy (DP3)~\cite{ze20243d} trained from scratch, we explore the potential of pre-trained visual representations to enhance spatial generalization. Specifically, we replace the train-from-scratch ResNet~\cite{he2016deep} encoder in DP with pre-trained encoders including R3M~\cite{nair2023r3m}, DINOv2~\cite{oquab2023dinov2}, and CLIP~\cite{radford2021learning}. 
Detailed implementations are provided in Appendix~\ref{sec:appendix-policy-pretrain}.

\vspace{0.2cm} \noindent\textbf{Demonstrations.}
% We iterate the number of demonstrations across a range from $25$ to $400$. For all the selected numbers, demonstrations are randomly sampled from a slightly larger range than the evaluation workspace to avert performance degradation near workspace boundaries.
We vary the number of demonstrations from $25$ to $400$. The object configurations are randomly sampled from a slightly larger range than the evaluation workspace to avoid performance degradation near workspace boundaries. A visualization is provided in Fig.~\ref{fig:precise-peg-insertion} in the appendix.

\vspace{0.2cm} \noindent\textbf{Evaluation.}
% In the \texttt{full} workspace setting, both the peg and socket are placed on $45$ uniformly sampled coordinates from their workspace, yielding a total of $2025$ combinatorial layouts for evaluation. For the \texttt{half} and \texttt{fixed} settings, the numbers of evaluated configurations are $225$ and $1$, respectively. The results of these evaluations are presented in Fig.~\ref{fig:spatial_gen_benchmark}.
In the \texttt{full} workspace, both the peg and socket are placed on $45$ uniformly sampled coordinates, resulting in $2025$ distinct configurations for evaluation. For the \texttt{half} and \texttt{fixed} settings, the number of evaluated configurations is $225$ and $1$, respectively. The results are presented in Fig.~\ref{fig:spatial_gen_benchmark}.

\vspace{0.2cm} \noindent\textbf{Key findings.}
% The degree of the object randomization significantly impacts the task difficulty. 
% Thus, a reliable evaluation protocol for visuomotor policies should adopt a sufficiently large workspace for object randomization. 
% For the policy, 3D representations and pre-trained 2D representations contribute to enhanced spatial generalization capability. Nevertheless, none of these methods fundamentally resolves the spatial generalization problem. The agent's spatial capacity is not automatically emerged from the policy but acquired from the traversal of workspace through abundant demonstrations. More detailed analysis can be found in Appendix~\ref{sec:appendix-empirical-benchmark}.
The degree of object randomization significantly influences the required demonstrations. Therefore, an effective evaluation protocol for visuomotor policies must incorporate a sufficiently large workspace to provide enough object randomization. On the other hand, both 3D representations and pre-trained 2D visual encoders contribute to improved spatial generalization capabilities. However, none of these methods fundamentally resolve the spatial generalization problem. This indicates the agent’s spatial capacity is not inherently derived from the policy itself but instead develops through extensive traversal of the workspace from the given demonstrations. A more detailed analysis is provided in Appendix~\ref{sec:appendix-empirical-benchmark}.


\begin{figure}[t]
    % \centering
    \includegraphics[width=0.48\textwidth]{figs/spatial_gen_dots-cropped.pdf}
    \caption{\textbf{Quantitative benchmarking on the spatial generalization capacity.} We report the relationship between the agent's performance in success rates and the number of demonstrations used for training when different visuomotor policies and object randomization ranges are adopted. The results are averaged over $3$ seeds.}
    \label{fig:spatial_gen_benchmark}
\end{figure}