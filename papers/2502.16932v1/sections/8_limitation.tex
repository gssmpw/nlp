\section{Limitations}
segmented point cloud

do not work for highly clustered scenes

single-view, translation, not too far away, object mismatch, ee still but arm might change

orientation,  not support full-scale orientation. e.g., a 180 deg yaw rotation may rotate the an unreachable ee state for the robot. solve by collecting some stem orientations. or through equivariant learning

limited by the performance of 3D observation policy. For example, simple pointnet encoder in dp3 is sensitive to the size of the manipulated objects. it will overlook small parts in the scene.
Also, with more demos, \method works for ``anywhere'' on the table surface? depend on the performance of the policy. can they memorize all the configurations? 
future: edit on 3D and then project to 2D

fixed position, contact-rich problems. in-hand object reorientation using dexterous hands, or push-T with fixed target pose.

do not work for highly precise (mm-level) tasks 

both 3d editing and DP3 policy learning require relatively high-quality point clouds. L515 is good, but D435 or Zedd is not good enough.