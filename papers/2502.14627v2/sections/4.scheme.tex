\section{Proposed ML-ATR Scheme}
We propose two methods to reduce the data distribution error during training. One is 1-to-K contrastive learning, which has a higher memory overhead. The other is audio-English co-anchor contrastive learning, which achieves performance close to 1-to-K Contrastive Learning while approximating the memory overhead to the existing ML-ATR scheme. Here are the details of the two methods.

\subsection{1-to-K Contrastive Learning}
Building on our theoretical analyses, we conclude that reducing data distribution error is critical for addressing the inconsistency problem in multilingual audio-text retrieval. To achieve this, we propose 1-to-K Contrastive Learning (KCL), a training strategy that replaces random language sampling with the simultaneous use of all $K$ linguistic texts corresponding to each audio instance. This approach theoretically eliminates data distribution error, corrects modal alignment direction, and significantly enhances both the recall and consistency of retrieval performance. The loss function $\mathcal{L}^{at}_{kcl}$ for the proposed 1-to-K Contrastive Learning in ML-ATR is defined as follows:

\begin{equation}
\small
\begin{aligned}
\mathcal{L}_{kcl}=\frac{1}{2NK}(\mathcal{L}^{a2t}_{kcl}+\mathcal{L}^{t2a}_{kcl}).
\end{aligned}
\end{equation}

The loss function $\mathcal{L}^{at}_{kcl}$ consists of two parts, $\mathcal{L}^{a2t}_{kcl}$ and $\mathcal{L}^{t2a}_{kcl}$, and they are calculated as follows:

\begin{equation}
\small
\begin{aligned}
\mathcal{L}^{a2t}_{kcl}=-\sum^K_{k=1}\sum^N_{i=1}log\frac{\exp(s(f_\theta(a_i),g_\phi(t_{ik}))/\tau)}{\sum^N_{j=1}\exp(s(f_\theta(a_i),g_\phi(t_{jk}))/\tau)},
\end{aligned}
\end{equation}

$\mathcal{L}^{a2t}_{kcl}$ denotes the contrastive learning loss function from audio to multilingual text.

\begin{equation}
\small
\begin{aligned}
\mathcal{L}^{t2a}_{kcl}=-\sum^K_{k=1}\sum^N_{i=1}log\frac{\exp(s(g_\phi(t_{ik}),f_\theta(a_i))/\tau)}{\sum^N_{j=1}\exp(s(g_\phi(t_{ik}),f_\theta(a_j))/\tau)},
\end{aligned}
\end{equation}

$\mathcal{L}^{t2a}_{kcl}$ denotes the contrastive learning loss function from multilingual text to audio.


$K$ is the number of languages and $N$ is the number of data instances. As shown in Tab. \ref{Tab:overhead}, including multiple multilingual texts in 1-to-K contrastive learning increases GPU memory usage and training time. In practical ML-ATR applications, supporting more languages amplifies these overheads compared to existing schemes.

To address this, we further propose CACL, which improves retrieval consistency and recall without significantly increasing overhead.

\subsection{Audio-English Co-Anchor Contrastive Learning}
To reduce the weighting error with as little increase in training time and GPU memory consumption as possible, we propose audio-English co-anchor contrastive learning (CACL). During the training process, each data takes its audio, English text, and text in other random languages and does contrastive learning with each other. 

For each epoch, given a set of random numbers $Q=\{q_1,...q_N\},q_i\stackrel{R}{\leftarrow}\{2,...K\}$, get the triplet of the training data $(a_i,t_{i1},t_{iq_i})$, where $a_i$ denotes $i$-th audio, $t_{i1}$ denotes the English text, and $t_{iq_i}$ denotes the text of $q_i$-th language. We have the training loss $\mathcal{L}_{cacl}$ shown below:

\begin{equation}
\small
\begin{aligned}
\mathcal{L}_{cacl}=\frac{1}{6N}(\mathcal{L}^{ae}_{cacl}+\mathcal{L}^{at}_{cacl}+\mathcal{L}^{et}_{cacl}).
\end{aligned}
\end{equation}

The loss function $\mathcal{L}_{cacl}$ consists of three components $\mathcal{L}^{ae}_{cacl},\mathcal{L}^{at}_{cacl},\mathcal{L}^{et}_{cacl}$. All three components are based on the following general contrastive learning loss formulation:

\begin{equation}
\small
\begin{aligned}
\mathcal{L}^{uv}_{cacl}=&-\sum^N_{i=1}log\frac{\exp(s(u_i,v_i)/\tau)}{\sum^N_{j=1}\exp(s(u_i,v_j)/\tau)}\\
&-\sum^N_{i=1}log\frac{\exp(s(v_i,u_i)/\tau)}{\sum^N_{j=1}\exp(s(v_i,u_j)/\tau)},
\end{aligned}
\end{equation}
where $u_i$ and $v_i$ represent input embeddings from different modalities or languages. The three components are defined as follows:

\begin{itemize}
    \item \textbf{Audio-English Alignment} ($\mathcal{L}^{ae}_{cacl}$): 
    
    $u_i=f_\theta(a_i)$ represents audio embeddings, and $v_i=g_\phi(t_{i1})$ represents English text embeddings.
    \item \textbf{Audio-Multilingual Alignment} ($\mathcal{L}^{at}_{cacl}$): $u_i=f_\theta(a_i)$ represents audio embeddings, and $v_i=g_\phi(t_{iq_i})$ represents text embeddings in a randomly selected language.
    \item \textbf{English-Multilingual Alignment} ($\mathcal{L}^{et}_{cacl}$): $u_i=g_\phi(t_{i1})$ represents English text embeddings, and $v_i=g_\phi(t_{iq_i})$ represents text embeddings in a randomly selected language.
\end{itemize}

% \begin{equation}
% \small
% \begin{aligned}
% \mathcal{L}^{ae}_{cacl}=&-\sum^N_{i=1}log\frac{\exp(s(f_\theta(a_i),g_\phi(t_{i1}))/\tau)}{\sum^N_{j=1}\exp(s(f_\theta(a_i),g_\phi(t_{j1}))/\tau)}\\
% &-\sum^N_{i=1}log\frac{\exp(s(g_\phi(t_{i1}),f_\theta(a_i))/\tau)}{\sum^N_{j=1}\exp(s(g_\phi(t_{i1}),f_\theta(a_j))/\tau)},
% \end{aligned}
% \end{equation}

% $\mathcal{L}^{ae}_{cacl}$ denotes the contrastive learning loss for modality alignment between audios and English texts.

% \begin{equation}
% \small
% \begin{aligned}
% \mathcal{L}^{at}_{cacl}=&-\sum^N_{i=1}log\frac{\exp(s(f_\theta(a_i),g_\phi(t_{iq_i}))/\tau)}{\sum^N_{j=1}\exp(s(f_\theta(a_i),g_\phi(t_{jq_j}))/\tau)}\\
% &-\sum^N_{i=1}log\frac{\exp(s(g_\phi(t_{iq_i}),f_\theta(a_i))/\tau)}{\sum^N_{j=1}\exp(s(g_\phi(t_{iq_i}),f_\theta(a_j))/\tau)},
% \end{aligned}
% \end{equation}

% $\mathcal{L}^{at}_{cacl}$ denotes the contrastive learning loss for modal alignment between audios and texts in other randomly selected languages.

% \begin{equation}
% \small
% \begin{aligned}
% \mathcal{L}^{et}_{cacl}=&-\sum^N_{i=1}log\frac{\exp(s(g_\phi(t_{i1}),g_\phi(t_{iq_i}))/\tau)}{\sum^N_{j=1}\exp(s(g_\phi(t_{i1}),g_\phi(t_{jq_j}))/\tau)}\\
% &-\sum^N_{i=1}log\frac{\exp(s(g_\phi(t_{iq_i}),g_\phi(t_{i1}))/\tau)}{\sum^N_{j=1}\exp(s(g_\phi(t_{iq_i}),g_\phi(t_{j1}))/\tau)},
% \end{aligned}
% \end{equation}

% $\mathcal{L}^{et}_{cacl}$ denotes the contrastive learning loss for modal alignment between English texts and texts in other randomly selected languages.

The effectiveness of audio-English CACL can be explained from two perspectives:
\begin{itemize}
    \item From the perspective of modality alignment (Fig. \ref{Fig:modality alignment error}), the loss function $\mathcal{L}^{et}_{cacl}$ in CACL brings embeddings of English and other languages closer, reducing the distance between the text embedding $\vec{t_1},\vec{t_2}$ and the mean $\frac{1}{2}(\vec{t_1}+\vec{t_2})$ and minimizing the deviation in the modality alignment direction of audio and text.
    \item From the perspective of data distribution error $\sum_{(a,t)}||p(a,t)-p'_e(a,t)||$ in Eq. \eqref{Eq:weight error}, CACL's loss functions $\mathcal L^{ae}_{cacl}, \mathcal L^{at}_{cacl}$ ensures that the model learns more pairs of audio texts in an epoch. The text in them also contains a large percentage of high-quality English text. It makes the data distribution in CACL closer to the original one, and reduces the weight error of the model.
    %\item The English text in the ML-ATR dataset is manually labeled and less noisy than the text in other languages obtained via translation models. Letting the embedding space of other languages to align with both audio and English text can further mitigate the noise from translated text, improving cross-language audio-text alignment. Additionally, existing English-oriented ATR models already have good pre-trained weights, and CACL can help the alignment of audio with other languages using this pre-existing knowledge.
\end{itemize}

Note that in CACL, the number of texts used for training in each epoch does not increase with the number of languages, which effectively reduces both GPU memory and time overhead in ML-ATR scenarios with a large number of languages. Our experimental results illustrate that CACL approximates the training time and explicit memory overhead of existing ML-ATR schemes, yet achieves recall and consistency metrics close to those of 1-to-K comparative learning.