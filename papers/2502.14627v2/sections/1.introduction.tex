\section{Introduction}
In an audio-text retrieval (ATR) task, the system searches for matching audio clips or text captions in a database based on cross-modality queries \cite{zhu2024cacophony,zhuang2024kdpror}. With the convergence of audio and text, ATR techniques have seen significant advancements in recent years and are widely applied in content retrieval and multimedia information retrieval. However, most existing ATR systems are designed for monolingual retrieval, and research on multilingual audio-text retrieval (ML-ATR) remains limited \cite{yan2024bridging}. The shift to ML-ATR brings new challenges, particularly in dealing with high multilingual recall and ensuring the consistency \cite{nie2024improving} of multilingual retrieval results.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.24]{fig/inconsistency_illustration.jpg}
    \caption{\textbf{An illustration of inconsistency issue in current ML-ATR scheme}.}
    \label{Fig:inconsistency illustration}
\end{figure}

To the best of our knowledge, the existing mainstream ML-ATR scheme has a model training process as shown in Fig. \ref{Fig:inconsistency illustration}, which pairs audio with randomly selected linguistic text in each epoch. This may not allow the model to learn the embedding space of audio and multilingual texts very well, which not only reduces the recall of retrieval, but also makes it difficult to obtain the same retrieval results for audio and multilingual text instances in different languages.

% Although existing ML-ATR schemes are inherently multilingual, the alignment process becomes more complex when handling multiple languages simultaneously. The ideal alignment process in the multilingual audio-text retrieval (ML-ATR) task should ensure that the semantics of multilingual text are accurately co-aligned with the semantics of the corresponding audio. However, as shown in Fig. \ref{Fig:inconsistency illustration}, the current schemes pair audio with randomly selected language text and fail to ensure effective co-alignment across all languages. This leads to inconsistencies in the modality matching of audio and multilingual text in existing schemes: audio-text instances exhibit inconsistent similarity rankings across languages, resulting in difficulty in obtaining the same retrieval results in different languages.

In this paper, we theoretically analyzes the causes of the inconsistency problem in ML-ATR. We first visualize the inconsistency problem in terms of the modal alignment direction error. The alignment direction error leads to the gradient error, which in turn invites the model weights to fail to converge to the optimal weights for multilingual modal alignment during the training process. We further heuristically derive theoretical upper bounds on the weight errors to quantify the adverse effects of inconsistency on the model weights. We analyze the composition of the weight error upper bound and conclude that the root cause of the error inconsistency is the data distribution error in training.

% In this paper, we theoretically analyze the causes of the inconsistency problem in ML-ATR. The direct cause of inconsistency is that the model weights cannot converge to the optimal weights for multilingual modality alignment during training. For this reason, we give a theoretical upper bound on the weighting error in terms of the direction of multilingual modality alignment and the model weighting error. We analyze the composition of the error upper bound and conclude that the root cause of the inconsistency is the data distribution error of the training.

Based on the theoretical analysis, we propose a scheme to mitigate the inconsistency of ML-ATR, called ATRI. ATRI consists of two training strategies: 1-to-K Contrastive Learning (KCL) for the retrieval-performance-first scenario, and Audio-English Co-Anchor Contrastive Learning (CACL) for the overhead-first scenario. KCL theoretically eliminates the data distribution errors in each training epoch, thus achieving state-of-the-art performance in recall and consistency metrics. CACL aligns the other languages with audio and English text to correct the modal alignment direction and reduce the data distribution error. Compared to existing ML-ATR schemes, CACL improves retrieval recall and consistency while offering advantages in training time and GPU memory overhead over KCL.

% Based, we propose a scheme to mitigate ML-ATR inconsistencies called ATRI. ATRI consists of two training strategies: 1-to-K Contrastive Learning (KCL) for better retrieval performance, and Audio-English Co-Anchor Contrastive Learning (CACL) for lower time and memory overheads during training. KCL effectively reduces data distribution errors within each training epoch, thereby minimizing the gap between the model's learned weights and the optimal weights. This leads to improved alignment between multi-language text and audio data, enhancing recall and consistency. CACL aligns other languages with both audio and English text, thereby correcting the alignment direction and further minimizing data distribution discrepancies. It is worth noting that the CACL approach requires only about 10\% additional memory and time overhead to improve consistency and recall effectively.

Our contributions are shown below:
\begin{itemize}
    \item We analyze the inconsistency in terms of analyzing the modal alignment direction error and weighting error, and demonstrate an upper bound on the weighting error. We further conclude that the root cause of the inconsistency of existing ML-CLAP schemes lies in the distribution error of the training data.
    \item We propose ATRI, which solves the inconsistency problem in multilingual audio text retrieval by reducing the data distribution error and correcting the modality alignment direction. ATRI contains the CACL and KCL training strategies for overhead-first and performance-first requirements, respectively.
    \item We evaluate the proposed scheme using the AudioCaps and Clotho datasets translated by Deepseek. The results show that ATRI effectively improves recall and consistency in both monolingual English ATR and ML-ATR tasks, achieving state-of-the-art performance.
\end{itemize}

