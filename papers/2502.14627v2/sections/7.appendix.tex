\section{Embedding Space Visualisation}
\label{Appe:Embedding Space}
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.22]{fig/Embedding_Space_ML-CLAP.jpg}
    \caption{\textbf{Visualisation of the ML-CLAP embedding space}.}
    \label{Fig:Embedding Space ML-CLAP}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.22]{fig/Embedding_Space_KCL.jpg}
    \caption{\textbf{Visualisation of the KCL embedding space}.}
    \label{Fig:Embedding Space KCL}
\end{figure}

To further compare the multilingual embedding alignment effects of KCL and ML-CLAP, we randomly select 50 audio-text pairs from AudioCaps. We visualize the embedding spaces of ML-CLAP and KCL after TSNE dimensionality reduction, as shown in Fig. \ref{Fig:Embedding Space ML-CLAP} and Fig. \ref{Fig:Embedding Space KCL}, respectively. In KCL, text embeddings with the same semantics across different languages are more compactly clustered compared to ML-CLAP. This indicates that KCL achieves better alignment of multilingual text embeddings, resulting in more consistent retrieval performance across languages.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.076]{fig/cases.png}
    \caption{\textbf{Two cases selected for analysis}. The audio text pair on the left is Case 1 and the one on the right is Case 2.}
    \label{Fig:Cases}
\end{figure}

\begin{table}[ht]
\caption{Retrieval similarity ranking of selected cases}
\small
\centering
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Lang}} & \multicolumn{2}{c|}{\textbf{ML-CLAP}} & \multicolumn{2}{c}{\textbf{KCL}}\\ \cline{2-5}
& Case 1 & Case 2 & Case 1 & Case 2 \\ \hline
eng & 5 & 2 & 1 & 0\\ 
fra & 4 & 4 & 1 & 0\\ 
deu & 5 & 4 & 0 & 0\\ 
spa & 2 & 7 & 0 & 0\\ 
nld & 4 & 12 & 0 & 2\\ 
cat & 5 & 12 & 0 & 4\\ 
jpn & 8 & 0 & 0 & 0\\ 
zho & 13 & 3 & 2 & 0\\ \hline
\end{tabular}
\label{Tab:Case Rank}
\end{table}

\section{Case Analysis}
\label{Appe:Case Analysis}
In this section, we select two audio text pairs on the AudioCaps test set shown in Fig. \ref{Fig:Cases} for our case analysis. Tab. \ref{Tab:Case Rank} shows the results of the retrieval rankings of audio-text pairs under the KCL and ML-CLAP schemes. the retrieval rankings of KCL are generally ahead of those of ML-CLAP, and the difference in retrieval rankings across languages is much smaller, effectively mitigating the inconsistency issue.

\section{Proof of Weight Error Upper Bound}
\label{Appe:Proof of Weight Error Upper Bound}
We analyze the upper bound on the weighting error heuristically based on the stochastic gradient descent (SGD) optimization algorithm. The following is a detailed theoretical proof of the upper bound on the weighting error in Eq. \eqref{Eq:weight error}.

$Proof$. Based on the definition of the SGD optimization algorithm, we have:
\begin{equation}
\small
    \begin{aligned}
        \mathbf w_{eT}&=\mathbf w_{eT-1}-\eta\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)],\\
        \mathbf w'_{eT}&=\mathbf w'_{eT-1}-\eta\sum_{(a,t)}p_e'(a,t)\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)].
    \end{aligned}
\end{equation}

\begin{equation}
\label{Eq:Proof of weight error}
\small
    \begin{aligned}
        &||\mathbf w_{eT}-\mathbf w'_{eT}||\\
        =&||\mathbf w_{eT-1}-\eta\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
        &-\mathbf w'_{eT-1}+\eta\sum_{(a,t)}p_e'(a,t)\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]||\\
        \leq&^1||\mathbf w_{eT-1}-\mathbf w'_{eT-1}||\\
        &+\eta||\sum_{(a,t)}p_e'(a,t)\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
        &-\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]||\\
        =&||\mathbf w_{eT-1}-\mathbf w'_{eT-1}||+\\
        &+\eta||\sum_{(a,t)}p_e'(a,t)\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
        &-\sum_{(a,t)}p'_e(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
        &+\sum_{(a,t)}p'_e(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
        &-\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]||\\
        \leq^2&||\mathbf w_{eT-1}-\mathbf w'_{eT-1}||\\
        &+\eta||\sum_{(a,t)}p'_e(a,t)(\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
        &-\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)])||\\
        &+\eta||\sum_{(a,t)}(p'_e(a,t)-p(a,t))\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]||\\
        \leq^3&(1+\eta\sum_{(a,t)}p'_e(a,t)\lambda_{(a,t)})||\mathbf w_{eT-1}-\mathbf w'_{eT-1}||\\
        &+\eta g_{max}(\mathbf w_{eT-1})\sum_{(a,t)}||p'_e(a,t)-p(a,t)||.
    \end{aligned}
\end{equation}

The inequality 1 and 2 hold because the Triangle Inequality $|a+b|\leq|a|+|b|$. The inequality 3 holds because

\begin{equation}
\small
    \begin{aligned}
    g_{max}(\mathbf w_{eT-1})=\max_{(a,t)}||\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]||,
    \end{aligned}
\end{equation}

and we assume that $\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]$ and $\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]$ are $\lambda_{(a,t)}$-Lipschitz, Gradient trimming can be used in the code implementation to a certain extent to reduce the gradient change in the training process, indirectly reduce the excessive growth of Lipschitz constant, as far as possible to meet the Lipschitz continuity condition.

Based on Eq. \eqref{Eq:Proof of weight error}, let 
\begin{equation}
\small
    \begin{aligned}
    a=(1+\eta\sum_{(a,t)}p'_e(a,t)\lambda_{(a,t)}),
    \end{aligned}
\end{equation}
we have

\begin{equation}
\small
    \begin{aligned}
        &||\mathbf w_{eT}-\mathbf w'_{eT}||\\
        \leq&a||\mathbf w_{eT-1}-\mathbf w'_{eT-1}||\\
        &+\eta g_{max}(\mathbf w_{eT-1})\sum_{(a,t)}||p'_e(a,t)-p(a,t)||\\
        \leq&a^2||\mathbf w_{eT-2}-\mathbf w'_{eT-2}||\\
        &+\eta \sum_{(a,t)}||p'_e(a,t)-p(a,t)||\\
        &(g_{max}(\mathbf w_{eT-1})+ag_{max}(\mathbf w_{eT-2}))\\
        \leq&a^{T}||\mathbf w_{(e-1)T}-\mathbf w'_{(e-1)T}||\\
        &+\eta \sum_{(a,t)}||p'_e(a,t)-p(a,t)||(\sum^{T-1}_{j=0} a^jg_{max}(\mathbf w_{eT-1-j}))).
    \end{aligned}
\end{equation}

Thus Eq. \eqref{Eq:weight error} is proved successful.

\subsection{Migrating to Adam Optimizer}
\label{Appe:Migrating}
We first give the parameter update computation procedure for the Adam optimizer:
\begin{equation}
    g=\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
\end{equation}

\begin{equation}
    m_{eT}=\beta_1m_{eT-1}+(1-\beta_1)g
\end{equation}

\begin{equation}
    v_{eT}=\beta_2v_{eT-1}+(1-\beta_2)g\circ g
\end{equation}

\begin{equation}
    \hat m_{eT}=\frac{m_{eT}}{1-\beta_1^{eT}}
\end{equation}

\begin{equation}
    \hat v_{eT}=\frac{v_{eT}}{1-\beta_2^{eT}}
\end{equation}

\begin{equation}
    \mathbf w_{eT}=\mathbf w_{eT-1}-\frac{\eta}{\sqrt{\hat v_{eT}}}\hat m_{eT}.
\end{equation}

$m_{eT}$ is the first-order momentum and $v_{eT}$ is the second-order momentum.

We illustrate that the data distribution error also causes weight error in the Adam optimizer by analyzing momentum. The error upper bound of the first-order momentum $m_{eT}$ can be inferred as follows:

\begin{equation}
\small
\label{Eq:Adam error}
\begin{aligned}
&||m_{eT}-m'_{eT}||\\
=&||\beta_1m_{eT-1}-\beta_1m'_{eT-1}\\
&-(1-\beta_1)\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
&+(1-\beta_1)\sum_{(a,t)}p'_e(a,t)\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p'_e(a,t)])||\\
\leq &||\beta_1m_{eT-1}-\beta_1m'_{eT-1}||\\
&+(1-\beta_1)||\sum_{(a,t)}p(a,t)\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
&-\sum_{(a,t)}p'_e(a,t)\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p'_e(a,t)])||\\
\leq&||\beta_1m_{eT-1}-\beta_1m'_{eT-1}||\\
&+(1-\beta_1)||\sum_{(a,t)}p'_e(a,t)(\nabla_{\mathbf w'_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]\\
&-\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)])||\\
&+(1-\beta_1)||\sum_{(a,t)}(p'_e(a,t)-p(a,t))\nabla_{\mathbf w_{eT-1}}\mathbb{E}_{(a,t)}[log\ p(a,t)]||\\
\leq&(1+(1-\beta_1)\sum_{(a,t)}p'_e(a,t)\lambda_{(a,t)})||\mathbf w_{eT-1}-\mathbf w'_{eT-1}||\\
&+(1-\beta_1) g_{max}(\mathbf w_{eT-1})\sum_{(a,t)}||p'_e(a,t)-p(a,t)||.
\end{aligned}
\end{equation}

Eq. \eqref{Eq:Adam error} shows that data distribution error still influences the upper bound on the first-order momentum error in the Adam optimizer. Similarly, the second-order momentum error is also affected by this error. These momentum errors accumulate in the weight errors, which makes our theoretical error upper bounds applicable under the Adam optimizer as well.


% Thus we can denote the weight error upper bound as

% \begin{equation}
% \label{Eq:Weight error in adam}
% \begin{aligned}
%     ||\mathbf{w}_{eT}-\mathbf w'_{eT}||=&||\mathbf{w}_{eT-1}-\frac{\eta}{\sqrt{\hat v_{eT}}}\hat m_{eT}\\
%     &-\mathbf w'_{eT-1}+\frac{\eta}{\sqrt{\hat v'_{eT}}}\hat m'_{eT}||.
% \end{aligned}
% \end{equation}

% It's obvious that due to the Triangle Inequality, the formula \eqref{Eq:Weight error in adam} can be deflated as

% \begin{equation}
% \begin{aligned}
% &||\mathbf{w}_{eT}-\mathbf w'_{eT}||\\
% \leq&||\mathbf{w}_{eT-1}-\mathbf w'_{eT-1}||\\
% &+\eta||\frac{\eta}{\sqrt{\hat v_{eT}}}\hat m_{eT}-\frac{\eta}{\sqrt{\hat v'_{eT}}}\hat m'_{eT}||
% \end{aligned}
% \end{equation}

% The expanded formulation of $||\frac{\eta}{\sqrt{\hat v_{eT}}}\hat m_{eT}-\frac{\eta}{\sqrt{\hat v'_{eT}}}\hat m'_{eT}||$ is too complex to be simplified into an intuitive expression, such as Eq. \eqref{Eq:weight error}, that directly illustrates the impact of data distribution errors on model weights. Therefore, we provide a conceptual explanation instead. In the Adam optimizer, both the current data error and the momentum accumulated from the data distribution errors of previous epochs can result in suboptimal gradient update directions denoted as $\frac{\eta}{\sqrt{\hat v'_{eT}}}\hat m'_{eT}$, leading to weight errors. Consequently, reducing data distribution errors to minimize weight errors and improving the modal alignment between multilingual text and audio is still feasible under the Adam optimizer.