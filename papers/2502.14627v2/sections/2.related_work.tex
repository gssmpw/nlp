\section{Related Work}
Audio-text retrieval (ATR) \cite{lou2022audio,xie2024gpa,xin2024diffatr} is a task that matches audio with text, which has seen significant advancements and widespread applications in recent years. The prevailing approach involves constructing a shared embedding space for audio and text, enabling seamless feature alignment and retrieving results based on similarity rankings. Widely adopted methods include CLIP-inspired \cite{yu2022coca,li2022blip} comparative audio-text pretraining \cite{elizalde2023clap,wu2022wav2clip,guzhov2022audioclip} and the triplet-loss method \cite{mei2022metric}, both of which have achieved success in learning audio-text joint embedding spaces.

Existing ATR methods predominantly focus on English-centric monolingual tasks, with few solutions for multilingual scenarios \cite{yan2024bridging}. The scarcity of large-scale, accurately annotated non-English audio-caption datasets has led current ML-ATR methods to rely heavily on machine translation \cite{tiedemann2020opus,nllb2022no} to convert English datasets into multilingual versions. This translation-based strategy \cite{cousin2023multilingual,yan2024bridging} has demonstrated its effectiveness in enhancing datasets for multilingual use, significantly improving the recall performance of ATR systems. 

However, existing ML-ATR scheme \cite{yan2024bridging} use audio-text pairs with randomly selected languages for training. As analyzed in Sect. \ref{Sect:Mathematical Demonstration about Inconsistency}, the training method employed presents significant challenges in achieving convergence to the optimal weights. This difficulty not only exacerbates issues related to inconsistent cross-language retrieval, but also leads to a degradation in the retrieval performance, particularly in terms of both recall and accuracy.