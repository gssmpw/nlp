\section{Experiments}
\subsection{Dataset}
We employ the AudioCaps \cite{kim2019audiocaps}, and Clotho \cite{drossos2020clotho} for our experiments. AudioCaps includes around 49,000 audio samples, each lasting about 10 seconds. Each audio is paired with a single sentence in the training set, while in both the validation and test sets, each audio has five associated sentences. The Clotho dataset consists of 6,974 audio samples, each ranging from 15 to 30 seconds long and annotated with five sentences. It is split into 3,839 training samples, 1,045 validation samples, and 1,045 test samples. 

Additionally, to assess our scheme's performance in the ML-ATR task, we use the Deepseek \cite{bi2024deepseek} API to translate the text from AudioCaps and Clotho into seven widely spoken languages, including French (fra), German (deu), Spanish (spa), Dutch (nld), Catalan (cat), Japanese (jpn), and Chinese (zho).

\subsection{Models}
\textbf{Audio Encoder}: 
We utilize the recently proposed CED-Base model \cite{dinkel2024ced}, a vision transformer with 86 million parameters for the Audio Encoder. Trained on Audioset through knowledge distillation from a large teacher ensemble, the model processes 64-dimensional Mel-spectrograms derived from a 16 kHz signal. It then extracts non-overlapping 16 × 16 patches from the spectrogram, resulting in 248 patches over a 10-second input (4 × 62).
\\
\textbf{Text Encoder}:
The key to multilingual audio-text retrieval is the text encoder's ability to handle texts in multiple languages. In this work, we focus solely on the SONAR-TE model \cite{duquenne2023sonar}. SONAR-TE generates a single vector bottleneck to encapsulate the entire text, avoiding the token-level cross-attention typically employed in conventional sequence-to-sequence machine translation models. The fixed-size text representation is derived by pooling the token-level outputs from the encoder. In the following sections, SONAR refers specifically to the text encoder.

\subsection{Setup}
We use ML-CLAP \cite{yan2024bridging} as the baseline, which is the state-of-the-art for ML-ATR tasks. To have a fair comparison, the model is initialized using the pre-trained weights of ML-CLAP and is further fine-tuned on our multilingual Audiocaps and Clotho datasets using three training methods: ML-CLAP, proposed CACL, and proposed KCL.

All models were fine-tuned for 10 epochs on a single A100 80GB PCIe GPU with a batch size of 24, a learning rate of $5 \times 10^{-6}$, using the Adam optimizer. The temperature hyperparameter $\tau$ was set to 0.07 for all configurations. The audio was sampled at $1.6\times 10^{4}$. We selected the model with the best recall performance during the fine-tuning period for each scheme to perform the experiments.

\subsection{Evaluation Metric}
We use the recall of rank k (R@k) and the average precision of rank 10 (mAP10) as the metrics for the retrieval performance of the model to show that reducing data distribution errors improves the retrieval performance in each language. R@k refers to the fact that for a query, R@k is 1 if the target-value item occurs in the first k retrieved items, and 0 otherwise. mAP10 calculates the average precision of all the queries among the first 10 retrieved results. With these two metrics, we can comprehensively evaluate the retrieval performance of the model on multilingual datasets. 

To assess the consistency of the embedding space across languages, we use three metrics: embedding space gap $\vec{\triangle}_{gap,k}$ \cite{liang2022mind}, average embedding distance $\vec{\triangle}_{dis,k}$, mean rank variance (MRV). The computation of $\vec{\triangle}_{gap,k}$, $\vec{\triangle}_{dis,k}$ and MRV is shown below:


\begin{equation}
\small
    \begin{aligned}
        \vec{\triangle}_{gap,k}=\frac 1 N \sum^N_{i=1}g_\phi(t_{i1})-\frac 1 N \sum^N_{i=1}g_\phi(t_{ik}),
    \end{aligned}
\end{equation}

\begin{equation}
\small
    \begin{aligned}
        \vec{\triangle}_{dis,k}=\frac 1 N \sum^N_{i=1}||g_\phi(t_{i1})-g_\phi(t_{ik})||,
    \end{aligned}
\end{equation}

\begin{equation}
\small
    \begin{aligned}
        MRV=\frac 1 {NK} \sum^N_{i=1}\sum^K_{k=1}|Rank_{ik}-\overline{Rank_j}|^2.
    \end{aligned}
\end{equation}

$\vec{\triangle}_{gap,k}$ and $\vec{\triangle}_{dis,k}$ denotes the embedding space gap and average embedding distance between English and $k$-th language respectively. $Rank_{ik}$ denotes the similarity ranking of the $k$-th language under the $i$-th data, and $\overline{Rank_i}$ denotes the average similarity ranking under the $i$-th data.

\begin{table*}[ht]
\caption{Recall and precision results for baseline and our method under multilingual AudioCaps and Clotho dataset}
\small
\centering
\begin{tabular}{c|c|ccc|ccc|ccc|ccc}
\hline
\multirow{3}{*}{\rotatebox{90}{\textbf{Scheme}}} & \multirow{3}{*}{\textbf{Lang}} & \multicolumn{6}{c|}{\textbf{AudioCaps}} & \multicolumn{6}{c}{\textbf{Clotho}}\\ 
\cline{3-14} & & \multicolumn{3}{c|}{T2A} & \multicolumn{3}{c|}{A2T} & \multicolumn{3}{c|}{T2A} & \multicolumn{3}{c}{A2T}\\
\cline{3-14}
 & & R@1 & R@5 & mAP10 & R@1 & R@5 & mAP10 & R@1 & R@5 & mAP10 & R@1 & R@5 & mAP10 \\ \cline{1-14}
\multirow{9}{*}{\rotatebox{90}{ML-CLAP}} & eng & 47.31 & 80.65 & 61.44 & 64.91 &	90.54 &	38.62 &	25.98 &	54.5 & 38.15 & 34.03 &	61.05 &	21.19 \\ 
& fra & 45.88 &	78.92 &	60.01 &	61.65 &	89.39 &	37.90 &	24.42 &	52.51 &	36.24 &	30.95 &	57.59 &	19.66 \\ 
& deu & 45.60 &	79.49 &	59.93 &	62.65 &	88.76 &	37.88 &	24.08 &	52.61 &	36.40 &	31.62 &	57.40 &	19.39 \\ 
& spa & 45.00 &	79.32 &	59.62 &	63.04 &	88.86 &	37.38 &	24.05 &	52.75 &	36.22 &	31.43 &	57.98 &	19.65 \\ 
& nld & 45.88 &	79.64 &	59.92 &	62.50 &	90.33 &	37.72 &	23.88 &	51.53 &	35.73 &	31.40 &	57.98 &	19.58 \\ 
& cat & 44.36 &	77.89 &	58.58 &	61.65 &	87.60 &	36.43 &	22.83 &	50.84 &	34.80 &	30.91 &	56.43 &	18.26 \\ 
& jpn & 43.04 &	76.86 &	57.54 &	59.45 &	87.81 &	35.20 &	23.04 &	50.34 &	34.89 &	31.43 &	56.55 &	18.77 \\ 
& zho & 41.70 &	74.72 &	55.74 &	53.67 &	84.76 &	33.38 &	21.65 &	48.84 &	33.53 &	28.41 &	56.14 &	17.26 \\ \cline{2-14}
& avg & 44.84 &	78.43 &	59.09 &	61.19 &	88.50 &	36.81 &	23.84 &	51.74 &	35.74 &	31.27 &	57.64 &	19.22 \\ \hline

\multirow{9}{*}{\rotatebox{90}{our CACL}} & eng & 49.05 &	82.14 &	63.07 &	66.31 &	\textbf{91.49} &	39.41 &	26.36 &	55.19 &	38.68 &	34.71 &	61.34 &	\textbf{21.57} \\ 
& fra & 46.86 &	79.97 &	60.83 &	63.23 &	89.48 &	37.92 &	\textbf{24.90} &	\textbf{53.09} &	36.67 &	\textbf{32.40} &	58.55 &	19.85 \\ 
& deu & 46.21 &	80.08 &	60.62 &	63.13 &	\textbf{89.91} &	38.14 &	24.51 &	52.86 &	36.52 &	\textbf{33.36} &	58.07 &	19.49 \\ 
& spa & 46.68 &	80.52 &	60.90 &	63.23 &	\textbf{90.12} &	37.45 &	\textbf{24.59} &	52.71 &	\textbf{36.72} &	32.40 &	58.17 &	19.75 \\ 
& nld & 47.41 &	80.23 &	61.22 &	63.23 &	\textbf{90.86} &	37.95 &	24.15 &	51.75 &	36.05 &	32.21 &	58.65 &	19.5 \\ 
& cat & 45.27 &	78.61 &	59.43 &	61.23 &	88.44 &	36.49 &	23.28 &	51.42 &	35.17 &	30.67 &	56.05 &	18.67 \\ 
& jpn & 44.76 &	78.50 &	58.97 &	61.55 &	88.67 &	34.91 &	23.36 &	51.53 &	35.28 &	\textbf{31.82} &	\textbf{58.26} &	18.99 \\ 
& zho & 42.01 &	76.02 &	56.23 &	56.40 &	86.65 &	33.93 &	22.50 &	49.42 &	34.01 &	27.69 &	\textbf{57.59} &	17.48 \\ \cline{2-14}
& avg & 46.03 &	79.50 &	60.15 &	62.28 &	89.45 &	37.02 &	24.20 &	52.24 &	36.27 &	31.90 &	58.33 &	19.41
 \\ \hline


\multirow{9}{*}{\rotatebox{90}{our KCL}} & eng & \textbf{49.68} &	\textbf{82.44} &	\textbf{63.34} &	\textbf{66.59} &	91.34 &	\textbf{40.52} &	\textbf{26.67} &	\textbf{55.46} &	\textbf{38.97} &	\textbf{36.34} &	\textbf{64.13} &	21.36 \\
& fra & \textbf{47.79} &	\textbf{80.52} &	\textbf{61.53} &	\textbf{63.41} &	\textbf{}\textbf{89.57} &	\textbf{39.21} &	24.61 &	52.73 &	\textbf{36.79} &	31.82 &	\textbf{60.76} &	\textbf{20.02} \\ 
& deu & \textbf{47.81} &	\textbf{80.81} &	\textbf{61.78} &	\textbf{63.34} &	89.28 &	\textbf{39.02} &	\textbf{24.90} &	\textbf{53.25} &	\textbf{37.02} &	33.17 &	\textbf{59.61} &	\textbf{19.90} \\ 
& spa & \textbf{47.33} &	\textbf{80.67} &	\textbf{61.49} &	\textbf{63.76} &	89.39 &	\textbf{38.73} &	24.31 &	\textbf{52.96} &	36.55 &	\textbf{33.36} &	\textbf{61.25} &	\textbf{20.27} \\ 
& nld & \textbf{47.92} &	\textbf{}\textbf{80.76} &	\textbf{61.70} &	\textbf{63.55} &	90.52 &	\textbf{39.14} &	\textbf{24.53} &	\textbf{52.51} &	\textbf{36.61} &	\textbf{33.55} &	\textbf{62.30} &	\textbf{19.98} \\ 
& cat & \textbf{46.44} &	\textbf{79.62} &	\textbf{60.42} &	\textbf{62.71} &	\textbf{89.49} &	\textbf{37.65} &	\textbf{23.67} &	\textbf{51.86} &	\textbf{35.70} &	\textbf{31.53} &	\textbf{57.98} &	\textbf{1}\textbf{8.90} \\ 
& jpn & \textbf{45.27} &	\textbf{78.86} &	\textbf{59.49} &	\textbf{62.28} &	\textbf{89.16} &	\textbf{36.81} &	\textbf{23.65} &	\textbf{52.17} &	\textbf{35.68} &	31.25 &	57.50 &	\textbf{19.49} \\ 
& zho & \textbf{42.25} &	\textbf{76.38} &	\textbf{56.75} &	\textbf{57.66} &	\textbf{87.28} &	\textbf{34.79} &	\textbf{23.09} &	\textbf{49.90} &	\textbf{34.60} &	\textbf{30.48} &	56.34 &	\textbf{17.85} \\ \cline{2-14}
& avg & \textbf{46.81} &	\textbf{80.00} &	\textbf{60.81} &	\textbf{62.91} &	\textbf{89.50} &	\textbf{38.23} &	\textbf{24.42} &	\textbf{52.60} &	\textbf{36.49} &	\textbf{32.68} &	\textbf{59.98} &	\textbf{19.72} \\ \hline
\end{tabular}
\label{Tab:recall and meanr}
\end{table*}

\subsection{Evaluation Result of Recall and Precision}
We present a detailed numerical comparison analysis of the experiment results in Tab \ref{Tab:recall and meanr}, focusing on the performance improvements of our proposed methods, CACL and KCL, over the baseline ML-CLAP across various languages and datasets.

\subsubsection{Analysis of Evaluation Results}
Overall, the proposed CACL and KCL consistently outperform ML-CLAP across the majority of languages and datasets in terms of recall at 1 (R@1), recall at 5 (R@5), and mean average precision at the top 10 results (mAP10) for both Text-to-Audio (T2A) and Audio-to-Text (A2T) tasks. Notably, our proposed KCL achieves state-of-the-art performance, delivering a 5\% improvement in R@1 for the English-oriented monolingual ATR task and a 4.3\% improvement in R@1 for the multilingual ATR task compared to ML-CLAP. This experimental result corroborates our theoretical analysis of the weighting error in Sect. \ref{Sect:Mathematical Demonstration about Inconsistency}. Here is the detailed analysis:

CACL's average metrics across languages are higher than ML-CLAP, while KCL's average metrics across languages have further improvement compared to CACL. Our theoretical analyses in Sect can explain this phenomenon. \ref{Sect:Mathematical Demonstration about Inconsistency}:
\begin{itemize}
    \item CACL uses audio and text together as the anchor point for modality alignment in other languages, which can effectively reduce the data distribution error and modality alignment error, thus achieving better modality alignment results and improved metrics compared to ML-CLAP.
    \item Compared to CACL, which mitigates data distribution errors, KCL theoretically eliminates these errors. As a result, KCL achieves superior modality alignment compared to CACL, leading to further improvements in both recall and precision.
\end{itemize}


\subsubsection{Analysis of Special Situations}

\textbf{Occasional Metric Anomalies}: We observed occasional anomalies where a small proportion of KCL metrics were lower than CACL metrics, and some CACL metrics were lower than ML-CLAP metrics. We attribute these discrepancies to noise in the dataset. Specifically, the weight error in Eq. \eqref{Eq:weight error} represents the difference between the current and optimal model weights for fitting the training data. If the dataset is too noisy, the optimal weights may not improve the test set's performance. As a result, KCL and CACL, which have lower weight errors, may still underperform ML-CLAP on certain metrics. The higher frequency of such anomalies in the noisier Clotho dataset, compared to Audiocaps, supports this explanation. Given that these anomalies are rare among the 108 evaluated metrics, we consider them acceptable and conclude that they do not impact the overall performance advantage of CACL and KCL in the ML-ATR task.

\textbf{Performance Gaps Across Languages}: The lower metrics for Japanese and Chinese in Tab. \ref{Tab:recall and meanr} are mainly due to their significant syntactic differences from other languages, making them harder for the model to learn. Expanding the dataset for these languages could improve the model's performance by providing more representative data.

\textbf{Better Replicated Performance}: Compared to the original paper, our replicated ML-CLAP model achieves significant improvements across all metrics, mainly due to differences in data quality. Compared to the SONAR-translated text used by baseline, the multilingual text we translated with LLM is of higher quality, which in turn can improve the retrieval performance of the model.

\begin{table}[ht]
\caption{Results of spatial differences in the embedding of other languages and English}
\small
\centering
\begin{tabular}{c|c|cc|cc}
\hline
\multirow{3}{*}{\rotatebox{90}{\textbf{Scheme}}} & \multirow{3}{*}{\textbf{Lang}} & \multicolumn{2}{c|}{\textbf{AudioCaps}} & \multicolumn{2}{c}{\textbf{Clotho}}\\ 
\cline{3-6} & & \multicolumn{2}{c|}{E2T} & \multicolumn{2}{c}{E2T}\\
\cline{3-6}
 & & Gap & Dis & Gap & Dis \\ \cline{1-6}
\multirow{8}{*}{\rotatebox{90}{ML-CLAP}} & fra & 0.199 & 0.094 & 0.120 & 0.301\\ 
& deu & 0.210 & 0.370 & 0.124 & 0.289\\ 
& spa & 0.147 & 0.290 & 0.117 & 0.284\\ 
& nld & 0.204 & 0.346 & 0.121 & 0.274\\ 
& cat & 0.151 & 0.357 & 0.121 & 0.307\\ 
& jpn & 0.237 & 0.445 & 0.123 & 0.353\\ 
& zho & 0.181 & 0.414 & 0.177 & 0.323\\ \cline{2-6}
& avg & 0.189 & 0.330 & 0.129 & 0.304\\ \hline

\multirow{8}{*}{\rotatebox{90}{our CACL}} & fra & 0.160 &  0.281 & 0.112 & 0.288\\ 
& deu & 0.194 & 0.334 & 0.103 & 0.261\\ 
& spa & 0.090 & 0.210 & 0.099 & 0.265\\ 
& nld & 0.172 & 0.325 & 0.106 & 0.255\\ 
& cat & 0.104 & 0.252 & 0.108 & 0.280\\ 
& jpn & 0.217 & 0.402 & 0.122 & 0.359\\ 
& zho & 0.192 & 0.381 & 0.159 & 0.352\\ \cline{2-6}
& avg & 0.161 & 0.312 & 0.115 & 0.294\\ \hline

\multirow{8}{*}{\rotatebox{90}{our KCL}} & fra & \textbf{0.145} & \textbf{0.274} & \textbf{0.094} & \textbf{0.261}\\ 
& deu & \textbf{0.155} & \textbf{0.290} & \textbf{0.084} & \textbf{0.231}\\ 
& spa & \textbf{0.081} & \textbf{0.192} & \textbf{0.084} & \textbf{0.230}\\ 
& nld & \textbf{0.148} & \textbf{0.285} & \textbf{0.072} & \textbf{0.204}\\ 
& cat & \textbf{0.092} & \textbf{0.245} & \textbf{0.087} & \textbf{0.243}\\ 
& jpn & \textbf{0.188} & \textbf{0.356} & \textbf{0.106} & \textbf{0.310}\\ 
& zho & \textbf{0.181} & \textbf{0.379} & \textbf{0.123} & \textbf{0.312}\\ \cline{2-6}
& avg & \textbf{0.141} & \textbf{0.288} & \textbf{0.092} & \textbf{0.255}\\ \hline
\end{tabular}
\label{Tab:embeddings gap}
\end{table}


\subsection{Evaluation Result of Consistency}
\subsubsection{Analysis of Embedding Space Consistency}
The results of the consistency metrics embedding space gap $\vec{\triangle}_{gap,k}$ and average embedding distance $\vec{\triangle}_{dis,k}$ are shown in Tab. \ref{Tab:embeddings gap}. In addition, we give a visualization of the embedding space in Appendix \ref{Appe:Embedding Space} and case analysis in Appendix \ref{Appe:Case Analysis} to further illustrate the effectiveness of ATRI in solving the consistency problem.

Smaller values of $\vec{\triangle}_{gap,k}$ and $\vec{\triangle}_{dis,k}$ indicate better alignment of a language's embedding space with English, leading to more consistent retrieval in the ML-ATR task. Compared to the baseline ML-CLAP, CACL achieves an average reduction of 12.9\% in Gap and 4.4\% in Dis, while KCL reduces Gap by 19.1\% and Dis by 14.3\%, demonstrating improved cross-language retrieval consistency.

\begin{table}[ht]
\caption{Results of Mean Rank Variance}
\small
\centering
\begin{tabular}{c|c|c}
\hline
\multirow{2}{*}{\textbf{Scheme}} & \textbf{AudioCaps} & \textbf{Clotho}\\ \cline{2-3}
 & MRV & MRV \\ \hline
ML-CLAP & 10.38 & 347.34 \\ \hline
CACL & 8.71 & 274.87 \\ \hline
KCL & \textbf{7.52} & \textbf{263.15} \\ \hline
\end{tabular}
\label{Tab:MRV}
\end{table}

\subsubsection{Analysis of Rank Consistency}
MRV quantifies the consistency of search rankings across languages, with lower values indicating more consistent results across languages. Unlike metrics based on embedding space, MRV offers a more direct assessment of model consistency in the ML-ATR task. As shown in Tab. \ref{Tab:MRV}, KCL achieves the lowest MRV, representing a 25.9\% reduction compared to ML-CLAP, while CACL achieves a 22.3\% reduction. This effectively shows that the inconsistency issue can be effectively mitigated by reducing the data distribution error.

We note that the MRV metrics under the Audiocaps dataset are significantly lower than Clotho's. This is due to the fact that the Clotho dataset is much noisier and more difficult to get consistent retrieval results across languages.

\begin{table}[ht]
\caption{Evaluation results in GPU memory overheads and time overheads}
\small
\centering
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Scheme}} & \multicolumn{2}{c|}{\textbf{AudioCaps}} & \multicolumn{2}{c}{\textbf{Clotho}} \\ \cline{2-5}
 & GMO(MB) & TO(s) & GMO(MB) & TO(s)\\ \hline
ML-CLAP & 22172 & 3349 & 30912 & 1592\\ \hline
our CACL & 26788 & 3745 & 31528 & 1714\\ \hline
our KCL & 68256 & 4209 & 79480 & 1884\\ \hline
\end{tabular}
\label{Tab:overhead}
\end{table}

\subsection{Evaluation Results about Training Overhead}
Tab.\ref{Tab:overhead} summarises the GPU memory overhead (GMO) and time overhead (TO) during training for three scenarios: ML-CLAP, CACL, and KCL. KCL training requires simultaneous input of text in eight languages, which significantly increases overhead, resulting in a higher GMO of about 2.8 times and a 27\% increase in TO compared to ML-CLAP. In contrast, CACL inputs just twice as much text as ML-CLAP, resulting in a modest increase of about 10\% in both overheads. This makes CACL more suitable for scenarios that prioritize lower training overheads, while KCL is more suitable for applications that emphasize retrieval performance.