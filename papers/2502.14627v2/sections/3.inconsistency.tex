\section{Definition and Inconsistency Analysis}
\label{Sect:Mathematical Demonstration about Inconsistency}
\subsection{Formal Definition of ML-ATR}
Audio-text retrieval is the task of learning cross-modality alignment between audio and multilingual text captions. Contrastive learning \cite{ru2023imbalanced,zhuang2025vargpt} has become the most effective method for learning expressive cross-modality embedding spaces.

Denote a dataset $D=\{(a_i, t_{i1},...t_{iK})\}_{i=1}^N$ as a multilingual audio text retrieval dataset, where $N$ denotes the size of dataset, $K$ refers the total language number in the dataset, $a_i$ denotes the audio in $i$-th data, $t_{ik}$ denotes the $k$-th language in $i$-th data. Given an audio encoder $f_\theta (\cdot)$ and a multilingual text encoder $g_\phi(\cdot)$, we denote the joint probability distribution as:

\begin{equation}
\label{Eq:origin distribution}
\small
    \begin{aligned}
        p(a_i,t_{ik})= \frac{\exp\left(s(f_\theta(a_i), g_\phi(t_{ik})) / \tau \right)}
  {\sum_{j=1}^N\sum_{l=1}^K \exp\left(s(f_\theta(a_j), g_\phi(t_{jl})) / \tau \right)},
    \end{aligned}
\end{equation}

\begin{equation}
\label{Eq:origin distribution}
\small
    \begin{aligned}
        p(a_i,t_{i})= \frac{\exp\left(s(f_\theta(a_i), g_\phi(t_{i})) / \tau \right)}
  {\sum_{j=1}^N \exp\left(s(f_\theta(a_j), g_\phi(t_{j})) / \tau \right)},
    \end{aligned}
\end{equation}

$s(\cdot)$ denotes the cosine similarity between audio and text embedding. The ideal optimization function of learning the embedding space is

\begin{equation}
\small
\begin{aligned}
\max_{\theta, \phi}\sum^{N}_{i=1}\sum^{K}_{k=1}p(a_i,t_{ik}) \mathbb{E}_{(a_i,t_{ik})}[log\ p(a_i,t_{ik})].
\end{aligned}
\end{equation}

However, instead of training all the languages of a piece of data in an epoch, the existing ML-ATR scheme randomly selects the text of a language to do the training. For each epoch $e$, a set of random numbers $Q=\{q_1,... .q_N\},q_i\stackrel{R}{\leftarrow}\{1,...K\}$. The optimization function they used is formalized as:

\begin{equation}
\label{Eq:error distribution}
\small
    \begin{aligned}
        p_e'(a_i,t_{iq_i})= \frac{\exp\left(s(f_\theta(a_i), g_\phi(t_{iq_i})) / \tau \right)}
  {\sum_{j=1}^N \exp\left(s(f_\theta(a_j), g_\phi(t_{jq_j})) / \tau \right)},
    \end{aligned}
\end{equation}

\begin{equation}
\small
\begin{aligned}
\max_{\theta, \phi}\sum^{N}_{i=1}p_e'(a_i,t_{iq_i}) \mathbb{E}_{(a_i,t_{iq_i})}[log\ p_e'(a_i,t_{iq_i})].
\end{aligned}
\end{equation}

The probability distribution $p_e'(a_i,t_{iq_i})$ of their scheme is not the same as the original probability distribution $p(a_i,t_{ik})$. This results in a model that does not fit the training data perfectly, making modality alignment ineffective, which in turn results in reduced recall and inconsistency problems.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.12]{fig/inconsistency_modality_alignment.png}
    \caption{\textbf{A visual illustration of inconsistency due to modality alignment errors}.}
    \label{Fig:modality alignment error}
\end{figure}

\subsection{Analysis of the Inconsistency Issue}
\label{Sect:Description of the Inconsistency Issue}
We first analyze the issue of inconsistency from the perspective of modality alignment directional errors. As shown in Fig. \ref{Fig:modality alignment error}, an intuitive example of modality alignment error is illustrated. Consider a simple case of bilingual audio-text retrieval, let the embedding of an audio sample be $\vec{a}$, and the embeddings of the corresponding texts in two languages be $\vec{t_1}$ and $\vec{t_2}$. Ideally, the audio embedding $\vec{a}$ should be aligned with the combined representation of both text embeddings $\frac{1}{2}(\vec{t_1} + \vec{t_2})$ (indicated by the green arrow). However, in existing ML-ATR schemes, the audio embedding is only aligned with the text embedding of a randomly selected language within each epoch. For instance, if the selected language is $t_2$, the audio embedding $\vec{a}$ will be aligned solely towards $\vec{t_2}$ (indicated by the red arrow). The angle between the red and green arrows is the modality alignment direction error, which makes the audio and multilingual text modes not well aligned.

It's obvious that incorrect alignment introduces noise to the gradient, leading to errors between the model weights and their optimal values, making the model's retrieval recall and consistency metrics degrade. We give a theoretical weight error upper bound and analyze its composition to mitigate the inconsistency problem and improve retrieval recall. The detailed proof can be found in Appendix \ref{Appe:Proof of Weight Error Upper Bound}.

% The incorrect alignment introduces noise into the gradient, resulting in error between the model weights and their optimal values. The performance degradation caused by the current training paradigm can thus be quantified regarding model weight error. Through theoretical analysis, we derive an upper bound for weight error, influenced by data distribution, learning rate, and training dynamics.

We assume that the optimization algorithm is stochastic gradient descent (SGD) \cite{ru2025we} to heuristically analyse the upper bound of the weight error. Given that the number of training steps per epoch $T$, the data distribution obtained by randomly sampling the language according to the existing ATR scheme is denoted as $p_e'$, and the original data distribution is denoted as $p$. $\mathbf wâ€™_{eT}$ denotes the model weight in the $T$-th step under the $e$-th epoch trained with the data distribution $p'_e$, whereas $\mathbf w_{eT}$ denotes the weight that is trained with the data distribution $p$. If the gradient $\nabla_\mathbf w\mathbb{E}_{(a,t)}[log\ p(a,t)]$ is $\lambda_{(x,y)}$-Lipschitz \cite{bethune2023dp}, then we have the following inequality for weight error upper bound:
\begin{equation}
\label{Eq:weight error}
\small
\begin{aligned}
&||\mathbf w_{eT}-\mathbf w'_{eT}||\\
\leq & a^T||\mathbf w_{(e-1)T}-\mathbf w'_{(e-1)T}||+\\
&\eta \sum_{(a,t)}||p(a,t)-p'_e(a,t)||\sum^{T-1}_{j=1}(a^jg_{max}(\mathbf w_{eT-1-j})),
\end{aligned}
\end{equation}

\begin{equation}
\small
\begin{aligned}
g_{max}(\mathbf w)=max_{(a,t)}||\nabla_\mathbf w\mathbb{E}_{(a,t)}[log\ p(a,t)]||,
\end{aligned}
\end{equation}

\begin{equation}
\small
\begin{aligned}
a=1+\eta\sum_{(a,t)}p'_e(a,t)\lambda_{(x,y)}.
\end{aligned}
\end{equation}

\textbf{Note}: The weight $\mathbf w$ consists of the parameter $\theta$ for the audio encoder $f_\theta$ and the parameter $\phi$ for the multilingual text encoder $g_\phi$ in ML-ATR. The data distributions $p$ and $p'_e$ correspond to the Eq. \eqref{Eq:origin distribution} and \eqref{Eq:error distribution}, respectively. For simplicity, we denote $(a,t)$ as all audio-text pairs in the batch of the $T$-th step, where the text $t$ can be in any one of the languages. $\sum_{(a,t)}||p(a,t)-p'_e(a,t)||$ denotes the data distribution error in the batch at step $T$.

Detailed proof of Eq \eqref{Eq:weight error} can be found in Appendix \ref{Appe:Proof of Weight Error Upper Bound}. Based on Eq. \eqref{Eq:weight error}, we have the following results:

\begin{itemize}
    \item Intuitively, the weight error $||\mathbf w_{eT}-\mathbf w'_{eT}||$ comes from two main sources. One is the weight error after the $(e-1)$-th epoch, i.e. $||\mathbf w'_{(e-1)T}-\mathbf w_{(e-1)T}||$. The other is caused by the probabilistic distances of the data distributions, i.e. $\sum_{(a,t)}||p'_e(a,t)-p(a,t)||$. Since $a\geq 1$, the error from both sources increases with epoch and step. In addition, the weight error is also affected by the learning rate $\eta$, the number of training steps $T$ and the maximum gradient $g_{max}(\mathbf w_{eT-1-j})$.
    \item Further expansion of Eq. \eqref{Eq:weight error} shows that the weighting error arises from the data distribution error of each epoch. Expanding $||\mathbf w_{(e-1)T}-\mathbf w'_{(e-1)T}||$ in Eq. \eqref{Eq:weight error}, we find it consist of $||\mathbf w_{(e-2)T}-\mathbf w'_{(e-2)T}||$ and $||p(a,t)-p'_{e-1}(a,t)||$. Further expanding Eq. \eqref{Eq:weight error} to the weight error in $1$-th epoch, it can be concluded that the weight error of the existing ML-ATR scheme comes from the data distribution error $\sum^e_{i=1}\sum_{(a,t)}||p(a,t)-p'_i(a,t)||$ due to the randomly selected languages in each epoch. We can mitigate the inconsistency problem and improve the recall by reducing the weight error upper bound by reducing the data distribution error for each epoch.
\end{itemize}