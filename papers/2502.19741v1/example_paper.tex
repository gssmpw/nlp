%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsmath}

% \usepackage{amsmath,amsfonts}
% \usepackage{amssymb}
% \usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% added by weilin
\usepackage{graphicx} % 需要引入 graphicx 包


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}  
\usepackage{subfigure}  
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{enumitem}
% \usepackage{fdsymbol}
\usepackage{dsfont}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{enumitem}

\usepackage{bbm}
\usepackage{bm}

\def\Vbar{{\perp\!\!\!\perp}}
\def\NotVbar{\not{\perp\!\!\!\perp}}

\def\bR{{\mathbb R}}
\def\bE{{\mathbb E}}

\def\a{\mathbf{a}}
\def\b{\mathbf{b}}
\def\w{\mathbf{w}}
\def\x{\mathbf{x}}
\def\u{\mathbf{u}}
\def\s{\mathbf{s}}
\def\z{\mathbf{z}}
\def\r{\mathbf{r}}
\def\X{\mathbf{X}}
\def\U{\mathbf{U}}
\def\S{\mathbf{S}}
\def\C{\mathbf{C}}
\def\Z{\mathbf{Z}}
\def\R{\mathbf{R}}
\def\W{\mathbf{W}}
\def\b{\mathbf{b}}
\def\V{\mathbf{V}}

\def\cA{{\mathcal A}}
\def\cX{{\mathcal X}}
\def\cU{{\mathcal U}}
\def\cS{{\mathcal S}}
\def\cY{{\mathcal Y}}
\def\cR{{\mathcal R}}
\def\cZ{{\mathcal Z}}
\def\cE{{\mathcal E}}
\def\cM{{\mathcal M}}
\def\cN{{\mathcal N}}
\def\cF{{\mathcal F}}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption}

\begin{document}

\twocolumn[
\icmltitle{Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption}
% Causal Effect Estimation under Networked Interference in the presence of latent confounders

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.


\begin{icmlauthorlist}
	\icmlauthor{Weilin Chen}{gdut}
	\icmlauthor{Ruichu Cai}{gdut,pc}
		\icmlauthor{Jie Qiao}{gdut}
	\icmlauthor{Yuguang Yan}{gdut}
	\icmlauthor{Jos\'e Miguel Hern\'andez-Lobato}{cam}
\end{icmlauthorlist}

\icmlaffiliation{gdut}{School of Computer Science, Guangdong University of Technology, Guangzhou, China}
\icmlaffiliation{pc}{Peng Cheng Laboratory, Shenzhen, China}
\icmlaffiliation{cam}{Department of Engineering, University of Cambridge, Cambridge, United Kingdom}

\icmlcorrespondingauthor{Ruichu Cai}{cairuichu@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}


\vskip 0.3in
]

 
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}

Estimating causal effects under networked interference is a crucial yet challenging problem.
Existing methods based on observational data mainly rely on the networked unconfoundedness assumption, which guarantees the identification of networked effects.
However, the networked unconfoundedness assumption is usually violated due to the latent confounders in observational data, hindering the identification of networked effects.
Interestingly, in such networked settings, interactions between units provide valuable information for recovering latent confounders.
In this paper, we identify three types of latent confounders in networked inference that hinder identification: those affecting only the individual, those affecting only neighbors, and those influencing both.
Specifically, we devise a networked effect estimator based on identifiable representation learning techniques.
Theoretically, we establish the identifiability of all latent confounders, 
and leveraging the identified latent confounders, we provide the networked effect identification result.
Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.


\end{abstract}

\section{Introduction}
\label{intro}

Estimating causal effects under network interference is a crucial yet challenging problem across various domains, including human ecology \cite{ferraro2019causal}, advertising \cite{parshakov2020spillover}, and epidemiology \cite{barkley2020causal}.
The key challenge is that networked interference introduces interactions between units, violating the Stable Unit Treatment Value Assumption (SUTVA).
For example, when evaluating the effect of a flu vaccine on individual infection rates, a standard causal inference approach assumes that an individual's infection risk depends only on their own vaccination status. 
However, in reality, vaccination generates herd immunity effects—vaccinating one person may reduce disease transmission within the population, indirectly lowering the infection risk of others.
This violation of SUTVA introduces bias into traditional causal inference methods, rendering standard estimands inapplicable \cite{forastiere2021identification}.
To model the interference between units, the existing methods focus on estimating three kinds of networked effects: \textit{main effects} (effects of units' own treatments), \textit{spillover effects} (effects of units' treatments on other units), and \textit{total effects} (combined main and spillover effects).

\begin{figure}[!t]
    % \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.48\textwidth]{figure/example.pdf}
    % \vspace{-0.5cm}
    \caption{
    A toy example showing networked interference between units.
    The networked interference introduces the interaction between units, i.e., the solid red arrows.
    Such arrows violate the traditional SUTVA assumption, leading to the non-identifiable problem.
    %The solid red and dashed green arrows, i.e., \textcolor{purple}{$\boldsymbol \rightarrow$} and \textcolor{mygreen}{$\boldsymbol \dashrightarrow$}, mean the interaction from one to another unit. Whether the dashed green arrow \textcolor{mygreen}{$\boldsymbol \dashrightarrow$} exists depends on the assumption on DGP.
    } 
    \label{fig: example of dr} \label{fig: intro example}
    \vspace{-0.2cm}
\end{figure}


To estimate causal effects from observational networked data, a series of works have been proposed under the networked unconfoundedness assumption.
This assumption posits that no unobserved confounders exist beyond the observed covariates and the covariates of neighboring units.
Under this assumption, \citet{forastiere2021identification} establish the identification of network effects and propose the joint generalized propensity score for effect estimation.
Building on this, \citet{chin2019regression, ma2021causal, cai2023generalization} introduce balanced representation techniques to construct conditional outcome estimators for effect estimation.
Additionally, \citet{liu2019doubly, chen2024doubly} develop doubly robust estimators to improve the robustness of network effect estimation under network interference.



\begin{figure}[!t]
    % \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.48\textwidth]{figure/causal_graph.pdf}
    \vspace{-0.2cm}
    \caption{Assumed causal graph in this paper.
    $x$ denotes observed proxies, $u$ denotes latent confounders, $t$ denotes the treatment, and $y$ denote the outcome of interest.
    We assume latent confounders $u$ contains three types of variables, i.e., $u^i$ affecting unit itself, $u^n$ affecting unit's neighborhoods, and $u^c$ affecting both.} 
    \label{fig: causal graph} 
    % \vspace{-0.4cm}
\end{figure} 

However, the networked unconfoundedness assumption is often violated in real-world scenarios, significantly limiting the effectiveness of existing methods.
For example, in the case of flu vaccination, whether a person chooses to get vaccinated may depend on their income level or their family's financial situation. However, such socioeconomic factors are often difficult to measure directly due to privacy concerns or data collection limitations.
In such cases, latent confounders exist, violating the networked unconfoundedness assumption and introducing bias into existing methods. 


To tackle the above challenge, we aim to develop a method that does not rely on the networked unconfoundedness assumption.
Specifically, we begin by exploring three types of latent confounders, shown in Figure \ref{fig: causal graph}, which hinder the effect identification: $u^i$ affecting only the individual, $u^n$ affecting only neighbors, and $u^c$ influencing both.
Rather than assuming networked unconfoundedness, we investigate the identifiability of latent confounders in the presence of networked interference.
We found that networked interference provides additional auxiliary information that facilitates the identification of latent confounders.
Built on the identified latent confounders, we theoretically establish the networked effect identification result and further devise an effect estimator under networked interference.
Overall, our contribution can be summarized as follows:
\begin{itemize}
    \item We address the problem of networked effect identification and estimation in the presence of latent confounders. We categorize three types of latent confounders that hinder identification.
    \item We explore the identifiability of latent confounders under networked interference, leveraging which, we achieve the networked effect identification.
    \item We devise an estimator built on the theoretical findings. 
    Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.
\end{itemize}

 

\section{Related Works}
\label{related works}
\textbf{Classic Causal Inference} has been studied in two languages: the graphical models \cite{pearl2009causality} and the potential outcome framework \cite{rubin1974estimating}. The most related method is the propensity score method in the potential outcome framework, e.g., IPW method \cite{IPWrosenbaum1983central, IPWrosenbaum1987model}, which is widely applied to many scenarios \cite{rosenbaum1985constructing, li2018balancing, CAI2024106336}. There are also many outcome regression models, including meta-learners \cite{kunzel2019metalearners}, neural networks-based works \cite{johansson2016learning, assaad2021counterfactual}. By incorporating them, one can construct a doubly robust estimator \cite{robins1994estimation}, i.e., the effect estimator is consistent as either the propensity model or the outcome repression model is consistent. 

\textbf{Causal Inference without SUTVA} has drawn increasing attention recently. \citet{liu2016inverse} extend the traditional propensity score to account for neighbors’ treatments and features and propose a generalized Inverse Probability Weighting (IPW) estimator. \citet{forastiere2021identification} define the joint propensity score and then propose a subclassification-based method. Drawing upon previous works, \citet{lee2021estimating} consider two IPW estimators and derive a closed-form estimator for the asymptotic variance. 
Based on the representation learning, \citet{ma2021causal} add neighborhood exposure and neighbors' features as additional input variables and applies HSIC to learn balanced representations. \citet{jiang2022estimating} use adversarial learning to learn balanced representations for better effect estimation. \citet{ma2022learning} propose a framework to learn causal effects on a hypergraph. \cite{cai2023generalization} propose a reweighted representation learning method to learn balanced representations.
Under networked interference, \citet{mcnealis2023doubly, liu2023nonparametric, chen2024doubly} propose an estimator to achieve DR property. However, these works do assume the unconfoundedness assumption, which might not hold in real-world scenarios. Different from them, we explore the problem of networked effect estimation without the unconfoundedness assumption.

\textbf{Causal Inference without Uncounfoundedness Assumption} is an important problem since the unconfoundedness assumption is usually violated in observational studies.
Classic methods to solve this problem usually assume there exist additional variables, e.g., instrumental variable \cite{pearl2000models,stock2003retrospectives,wu2022instrumental}, proximal variable \cite{miao2018identifying, tchetgen2024introduction}.
Another effective way to address this problem is to recover the latent confounder using representation learning methods.
CEVAE \citep{louizos2017causal} assumes that latent confounders can be recovered by their proxies and applies VAE to learn confounders.
As a follow-up work, TEDVAE \citep{zhang2021treatment} 
decouples the learned latent confounders into several factors to achieve a more accurate estimation of treatment effects.
In the mediation analysis, DMAVAE \citep{xu2023disentangled} proposes to recover latent confounders using the VAE similar to CEVAE.
Our work is closely related to these works.
Different from them, we focus on the causal effect 
without the unconfoundedness assumption in the presence of networked interference. We also provide theoretical guarantees for the latent confounder identifiability, which ensures the effectiveness of our estimator.

\section{Notations, Assumptions, Esitimands}
\label{notations, assumptions, estimands}



In this section, we start with the notations used in this work. Let $U \in \mathcal{U}$ be the latent confounders and also let $X \in \mathcal{X}$ be the proxies of latent confounders.
Let $T \in \{0,1\}$ denote a binary treatment, where $T=1$ indicates a unit receives the treatment (treated) and $T=0$ indicates a unit receives no treatment (control). 
Let $Y \in \mathcal{Y}$ be the outcome. 
We assume that $U$ can be decomposed to $U^i$ affecting the unit itself, $U^c$ affecting the neighborhood, and $U^n$ affecting both.
Let lowercase letters (e.g., $x,y,t$) denote the value of random variables. 
Let lowercase letters with subscript $i$ denote the value of the specified $i$-th unit.
%(e.g., $x_i$ is the covariate value of $i$-th unit). 
Thus, a network dataset is denoted as $D=(\{x_i,t_i,y_i\}_{i=1}^n,E)$, where $E$ denotes the adjacency matrix of network and $n$ is the total number of units. 
% We also denote $n_1$ and $n_2$ as the total number of treated units and control units, and thus $n=n_1+n_2$.
We denote the set of first-order neighbors of $i$ as $\mathcal{N}_i$ and denote the treatment and feature vectors received by unit $i$'s neighbors as $t_{\mathcal N _i}$ and $x_{\mathcal N _i}$. 
Due to the presence of networked interference, a unit's potential outcome is influenced not only by its treatment but also by its neighbors' treatments, and thus the potential outcome is denoted by $y_i(t_i,t_{\mathcal N _i})$.
%The fundamental problem of causal inference is that we only observe one of the potential outcomes, i.e., $y_i = y_i(t_i, t_{\mathcal N _i})$.
The observed outcome $y_i$ is known as the factual outcome, and the remaining potential outcomes are known as counterfactual outcomes. 

% neighborhood exposure
Further, following \citet{forastiere2021identification, chen2024doubly}, we assume that the dependence between the potential outcome and the neighbors' treatments is through a specified summary function $g$: $\{0,1\}^{|\mathcal{N}_i|}\rightarrow [0,1]$, and let $z_i$ be the neighborhood exposure given by the summary function, i.e., $z_i=g(t_{\mathcal N _i})$. 
We aggregate the information of the neighbors' treatments to obtain the neighborhood exposure by $z_i=\frac{\sum_{j\in \mathcal{N}_i}t_j}{|\mathcal{N}_i|}$. 
Therefore, the potential outcome $y_i(t_i,t_{\mathcal N _i}) $ can be denoted as $y_i(t_i,z_i)$, which means that under networked interference, each unit is affected by two kinds of treatments: the binary individual treatment $t_i$ and the continuous neighborhood exposure $z_i$.

In this paper, our \textbf{goal} is to estimate the average dose-response function, as well as the conditional average dose-response function:
\begin{equation}
  \begin{aligned}
    & \psi(t,z) := \mathbb E [Y(t,z)], \\
    & \mu(t,z,x,x_{\mathcal N }) := \mathbb E [Y(t,z)|X=x,X_{\mathcal N }=x_{\mathcal N }],
    \end{aligned} 
\end{equation}

Based on the average dose-response function, existing works mostly focus on the following causal effects:

\begin{definition} [Average Main Effects (AME) ] AME measures the difference in mean outcomes between units assigned to $T=t, Z=0$ and assigned $T=t^\prime, Z=0$:
    $\tau^{(t,0),(t^{\prime},0)} =\psi(t,0) -\psi(t^\prime,0) $. 
\end{definition}

\begin{definition} [Average Spillover Effects (ASE) ] ASE measures the difference in mean outcomes between units assigned to $T=0, Z=z$ and assigned $T=0, Z=z^\prime$:
    $\tau^{(0,z),(0,z^{\prime})} =\psi(0,z) -\psi(0,z^{\prime}) $. 
\end{definition}

\begin{definition} [Average Total Effects (ATE) ] ATE measures the difference in mean outcomes between units assigned to $T=t, Z=z$ and assigned $T=t^\prime, Z=z^\prime$:
$\tau^{(t,z),(t^\prime, z^\prime)} =\psi(t,z) -\psi(t^\prime, z^\prime) $. 
\end{definition}

\begin{definition} [Individual Main Effects (IME) ] IME measures the difference in mean outcomes of a particular unit  $x_i$ assigned to $T=t, Z=0$ and assigned $T=t^\prime, Z=0$:
    $\tau_i(x_i,x_{\mathcal N_i})^{(t,0),(t^{\prime},0)} =\mu(x_i,x_{\mathcal N_i}, t,0) -\mu(x_i,x_{\mathcal N_i}, t^\prime,0) $. 
\end{definition}

\begin{definition} [Individual Spillover Effects (ISE) ] ISE measures the difference in mean outcomes of a particular unit $x_i$ assigned to $T=0, Z=z$ and assigned $T=0, Z=z^\prime$:
    $\tau_i(x_i,x_{\mathcal N_i})^{(0,z),(0,z^{\prime})} =\mu(x_i,x_{\mathcal N_i}, 0,z) -\mu(x_i,x_{\mathcal N_i}, 0,z^{\prime}) $. 
\end{definition}

\begin{definition} [Individual Total Effects (ITE) ] ITE measures the difference in mean outcomes of a particular unit  $x_i$ assigned to $T=t, Z=z$ and assigned $T=t^\prime, Z=z^\prime$:
$\tau_i(x_i,x_{\mathcal N_i})^{(t,z),(t^\prime, z^\prime)} =\mu(x_i,x_{\mathcal N_i}, t,z) -\mu(x_i,x_{\mathcal N_i}, t^\prime, z^\prime) $. 
\end{definition}

The main effects reflect the effects of changing neighborhood exposure $t$ to $t^\prime$.
The spillover effects reflect the effects of changing neighborhood exposure $z$ to $z^\prime$. 
And the total effects represent the combined effect of both main effects and spillover effects.

Throughout this paper, we also assume the following assumptions hold:

\begin{assumption}[Network Consistency] \label{asmp: consistency}
The potential outcome is the same as the observed outcome under the same individual treatment and neighborhood exposure, i.e., $y_i=y_i(t_i,z_i)$ if unit $i$ actually receives $t_i$ and $z_i$.
\end{assumption}

\begin{assumption}[Network Overlap] \label{asmp: Overlap}
    Given any individual and neighbors' features, any treatment pair $(t,z)$ has a non-zero probability of being observed in the data, i.e., $\forall x_i,x_{\mathcal N _i}, t_i, z_i, \quad 0<p(t_i,z_i|x_i, x_{\mathcal N _i})<1$.
\end{assumption}

\begin{assumption}[Neighborhood Interference] \label{asmp: Neighborhood interference}
    The potential outcome of a unit is only affected by their own and the first-order neighbors’ treatments, and the effect of the neighbors' treatments is through a summary function:
    $g$, i.e., $\forall t_{\mathcal N _i}$,$t^{\prime}_{\mathcal N _i}$ which satisfy $g(t_{\mathcal N _i})=g(t^{\prime}_{\mathcal N _i})$, the following equation holds: $y_i(t_i, t_{\mathcal N _i})=y_i(t_i, t^{\prime}_{\mathcal N _i})$.
\end{assumption}

These assumptions are commonly assumed in existing causal inference methods such as \citet{forastiere2021identification, cai2023generalization, ma2022learning}. Specifically, Assumption \ref{asmp: consistency} states that there can not be multiple versions of a treatment. Assumption \ref{asmp: Overlap} requires that the treatment assignment is nondeterministic. Assumption \ref{asmp: Neighborhood interference} rules out the dependence of the outcome of unit $i$, $y_i$, from the treatment received by units outside its neighborhood, i.e., $t_j, j \notin \mathcal{N}_i$, but allows $y_i$ to depend on the treatment received by his neighbors, i.e., $t_k, k \in \mathcal{N}_i$. Also, Assumption \ref{asmp: Neighborhood interference} states the interaction dependence is assumed to be through a summary function $g$. Note that Assumption \ref{asmp: Neighborhood interference} is reasonable in reality for some reason. First, in many applications units are affected by their first-order neighbors, and the affection of higher-order neighbors is also transported through the first-order neighbors. Second, it is also reasonable that a unit is affected by a specific function of other units' treatment, e.g., how much job-seeking pressure a unit has will depend on how many of its friends receive job training.

Existing methods additionally assume the following assumption:
\begin{assumption}[Networked Unconfoundedness] \label{asmp: Network unconfounderness}
    The individual treatment and neighborhood exposure are independent of the potential outcome given the individual and neighbors' features, i.e., $\forall t,z, \quad y_i(t,z) \Vbar t_i,z_i|x_i, x_{\mathcal N _i}$.
\end{assumption}

Assumption \ref{asmp: Network unconfounderness} is an extension of the traditional unconfoundedness assumption and indicates that there is no unmeasured confounder which is the common cause of $y_i$ and $t_i, z_i$. 

Under the assumptions above, the networked effects can be identified \cite{forastiere2021identification, cai2023generalization}. However, Assumption \ref{asmp: Network unconfounderness} might be too strong to hold, since we can not promise that all confounders are observed in real-world scenarios. Instead, we assume a much weaker assumption by incorporating the latent variables as follows:
\begin{assumption}[Latent Networked Unconfoundedness] \label{asmp: Latent Network unconfounderness}
    The individual treatment and neighborhood exposure are independent of the potential outcome given the latent individual and neighbors' confounders, i.e., $\forall t,z, \quad y_i(t,z) \Vbar t_i,z_i|u^i_i, u^c_i, u^c_{\mathcal N _i},  u^n_{\mathcal N _i}$.
\end{assumption}

This assumption allows for the latent confounders $u^i,u^c,u^n$.
Here, we recognize three types of latent confounders.
What serves as an adjustment set under the networked setting is the units' $u^i,u^c$ and the neighbors' $ u^c_{\mathcal N _i},  u^n_{\mathcal N _i}$.
This also motivates us to identify each latent confounder for a better estimation.
In the next section, we will introduce the identifiability of each latent confounder, and further achieve networked effect identification.

% \begin{equation}
% \begin{aligned}
%     \psi(t,z) 
%     =&\mathbb E [ \mathbb E [Y(t,z)|X=x,X_{\mathcal N }=x_{\mathcal N }]] \\
%     \overset{(a)}{=} &\mathbb E [\mathbb{E}[Y(t,z)|T=t,Z=z,X=x,X_{\mathcal N }=x_{\mathcal N }] ] \\
%     \overset{(b)}{=} &\mathbb E [\mathbb{E}[Y|T=t,Z=z,X=x,X_{\mathcal N }=x_{\mathcal N}] ],
% \end{aligned}   
% \end{equation}
% where equation (a) holds due to Assumption \ref{asmp: Network unconfounderness}, and equation (b) holds due to Assumption \ref{asmp: consistency}.

\section{Networked Causal Effect Identification via Representation Learning} \label{sec: Identication}

% In this section, we first introduce the identifia

To begin with, following existing identifiable representation learning methods \cite{khemakhem2020variational,lu2022invariant}, we first introduce the generative model as follows:
 \begin{equation}
      \begin{aligned}
    p_{\bm \theta}(X,U|X_\mathcal{N}) &= p_{\bm f}(X|U)p_{\bm{T},\bm{\lambda}}(U|X_\mathcal{N}) \\ 
      p_{\bm f}(X|U) & = p_{\bm \epsilon} (X- \bm f(U))
      \end{aligned}
\end{equation}

Further, we assume $p_{\bm{T},\bm{\lambda}}(U|X_\mathcal{N})$ follows the exponential family distribution.

\begin{assumption} \label{asmp: exp dist}
    The correlation between  $U$ and  $X_\mathcal{N}$ is characterized by:
    \begin{equation}
      \begin{aligned}
        p_{\bm{T},\bm{\lambda}}(U|X_\mathcal{N}) 
        & = \frac{\mathcal Q (U)}{\mathcal C (X_\mathcal{N})} \exp \left[ \bm{T}(U)^T \bm{\lambda}(X_\mathcal{N}) \right]
      \end{aligned}
    \end{equation}
    where $\mathcal Q$ is the base measure, $\mathcal C$ is the normalizing constant. The $\bm{\lambda}(X_{\mathcal{N}})$ is an arbitrary function, and the sufficient statistics $\bm{T}(U) = [\bm{T}_f(U)^T,\bm{T}_{MLP}(U)^T]^T$ contains a) the sufficient statistics $\bm{T}_f(U)^T=[\bm{T}_1(U_{(1)})^T, \dots, \bm{T}_1(U_{(d_U)})^T]$ of a factorized exponential family, where all the 
    $\bm{T}_i(U_{(i)})$ have dimension larger or equal to $2$ and $d_U$ is the dimension of $U$, and b) the output $\bm{T}_{MLP}(U)$ of a neural network with ReLU activations.
\end{assumption}

This assumption is introduced by \citet{lu2022invariant}. The distribution in Assumption \ref{asmp: exp dist} is more flexible than the standard assumed distribution condition in identifiable representation learning (Eq. (7) in \citet{khemakhem2020variational}).
This assumption allows for the case that the different elements of latent confounders are not independent given the conditional set. 
The term $\bm{T}_{MLP}(U)$ does capture arbitrary dependencies between latent variables since the neural network with ReLU activation has universal approximation power.

Now, we formally state the theoretical result of the identifiability of latent confounders.

\begin{theorem}\label{theorem:recover latent}
Suppose Assumption \ref{asmp: exp dist} holds, and suppose the following conditions hold: 
(1) The set $\{X \in \mathcal O| \varphi_{\bm \epsilon}(X)=0 \}$ has measure zero where $\varphi_{\bm \epsilon}$ is the characteristic function of density $p_{\bm \epsilon}$.
(2) $\bm f$ is injective and has all second-order cross derivatives.
(3)  The sufficient statistics in $\bm T_{\bm f}$ are all
twice differentiable. 
(4) There exist $k+1$ distinct values $x_{\mathcal N_0}, ... , x_{\mathcal N_{k+1}}$ such that the matrix
    \begin{equation*}
    \begin{aligned}
         L =  (
         & \bm {\lambda}(x_{\mathcal N_1})- \bm {\lambda}(x_{\mathcal N_0}), \\
         &..., \\
         &\bm{\lambda}(x_{\mathcal N_{k+1}})-\bm{\lambda}(x_{\mathcal N_{0}}))
    \end{aligned}
    \end{equation*}
    of size $k \times k$ is invertible where $k=|U^i| + |U^c| + |U^n|$ is the dimension of latent variables. 
Then we learn the true latent variable $U^i, U^c, U^n$ up to a permutation and simple transformations.
%the parameters $(\mathbf{T},\mathbf{f},\mathbf{\lambda})$ are identifiable up to permutation and nonlinearity.
\end{theorem}

\textbf{Discussion on assumptions and conditions.} Assumption \ref{asmp: exp dist} indicates our theory holds for a rich family of conditional densities \cite{Wainwright2008expfamily}. The assumption on the exponential family distribution is not strong, since many well-known distributions belong to this family, including Gaussian, Uniform, Poisson distributions, and so on. 
Condition (1)-(4) is a common assumption in representation learning in causal representation learning, e.g., \cite{khemakhem2020variational, lu2022invariant}. Notably, the most important condition is the condition (4) which requires that the auxiliary information should be sufficient enough. 
Under our networked setting, it requires that there exist enough distinct values of neighbors' covariates, which is easy to hold if we collect enough covariates, especially when some of the covariates are continuous.

Theorem \ref{theorem:recover latent} indicates that, under mild assumptions, the latent confounders can be recovered up to a simple function, i.e., the recovered $\hat U^i, \hat U^c, \hat U^n$ satisfying $\hat U^i=h_i(U^i), \hat U^c=h_c(U^c), \hat U^n=h_n(U^n)$ for some simple functions $h_i. h_c, h_n$.
Based on Theorem \ref{theorem:recover latent} above, we can further identify networked effect as follows:

\begin{theorem}\label{theorem: identify}
Suppose Assumption \ref{asmp: consistency},  \ref{asmp: Neighborhood interference},  \ref{asmp: Overlap}, \ref{asmp: Latent Network unconfounderness}, and Theorem \ref{theorem:recover latent} holds, the networked effects $\psi(t,z)$ and $\mu (t,z, x, x_\mathcal{N})$ are identifiable.
\end{theorem}

Theorem \ref{theorem: identify} indicates that if we can identify latent confounders, the networked effect is thereby identifiable.
This necessitates the utilization of identifiable representation learning techniques for causal inference in the presence of networked interference and latent confounders.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.\textwidth]{figure/network.pdf}
    \vspace{-0.2cm}
    \caption{Model architecture of our proposed method. The representation learning module aims to learn the unobserved confounders. The feature module aggregates the information of covariates of unit $i$ and its neighbor. The outcome estimator module aims to estimate potential outcomes of unit $i$.}
    \label{fig: network}
    \vspace{-0.2cm}
\end{figure*}

\section{Methodology} \label{sec: method}

In this section, leveraging the theoretical findings, we devise our networked effect estimator in the presence of latent confounders.
Specifically, our estimator contains three modules, including the representation learning module, the feature module, and the outcome estimator module.
The representation learning module is built on Theorem \ref{theorem:recover latent}, aiming to correctly recover three types of latent confounders $u^i, u^c, u^n$.
The feature module is built on Theorem \ref{theorem: identify}, aggregating the information from units' and neighbors' information.
This aggregated information is then input into the outcome estimator module to predict the networked causal effects.
Overall, our model architecture is shown in Figure \ref{fig: network}.

\subsection{Representation Learning Module} \label{sec: Latent Variable Learning}

% We employ multilayer perceptrons (MLPs) to model the distributions in the representation learning module.
% Specifically, 
Following existing work \cite{guo2020learning,ma2021causal,jiang2022estimating, chen2024doubly}, we use Graph Convolution Networks (GCN \cite{defferrard2016convolutional,kipf2016semi}) to aggregate the information of covariates of unit $i$ and its neighbors, i.e., $x_i, x_ {\mathcal N_i}$:
\begin{equation*}
% \small
\begin{aligned}
   & h^{neigh}_{i,1} =\sigma(\sum_{j\in \mathcal{N}_i}\frac{1}{\sqrt{d_id_j}}W_1^Tx_j), 
   \\& h_{i,2} = MLP_1(h^{neigh}_{i,1},x_i) ,
\end{aligned}
\end{equation*}
where $\sigma(\cdot)$ is a non-linear activation function, $d_i$ is the degrees of unit $i$, $W_1$ is the learning weight matrix of GCN, and $MLP_1$ is a multilayer perception (MLP).

Then, given $x_i$ and $x_ {\mathcal N_i}$, we employ the identifiable representation learning technique \cite{lu2022invariant} to recover latent confounders.
Specifically, we parametrize the prior following Assumption \ref{asmp: exp dist}:
% \textcolor{red}{ check it}
\begin{equation}
  \begin{aligned}
   &p(u_i|x_{\mathcal{N}_i}) 
   \\= &
    \langle MLP_2(u_i), MLP_3(h_{i,2}) \rangle + \langle [u_i, u_i^2], MLP_4(h_{i,2}) \rangle
   %= \frac{\mathcal Q (U)}{\mathcal C (X_\mathcal{N})} \exp \left[ \hat{ \bm{T}}(U)^T \hat{ \bm{\lambda}} (X_\mathcal{N}) \right]
\end{aligned}
\end{equation}
where $u_i=[u_i^i, u_i^c, u_i^n]$ and $MLP_2(u_i)$ serves as $\bm{T}_{MLP}$,
the concatenated $[u_i, u_i^2]$ serves as $\bm{T}_f$,
$MLP_3(h_{i,2})$ serves as $\bm{\lambda}_{MLP}$,
and $ MLP_4(h_{i,2})$ serves as  $\bm{\lambda}_{f}$ in Assumption \ref{asmp: exp dist}.

As for the encoder, the variational approximation of the posterior is defined as:
\begin{equation}
  \begin{aligned}
    q(u^i_i,u^c_i,u^n_i|x_i,x_ {\mathcal N_i}) = \prod_{i=0}^{|U|} \mathcal{N}(\mu=\hat{\mu}_{u_{i}},\sigma^2=\hat{\sigma}_{u_{i}}^2),
\end{aligned}
\end{equation}
where $\hat{\mu}_{u_{i}}$ and $\hat{\sigma}_{u_{i}}^2$ are the mean and variance of the Gaussian distribution parametrized by MLPs using $h_{i,2}$ as input.

As for the decoder, for a continuous outcome, we parametrize the probability distribution as a Gaussian distribution with its mean given by an MLP and a fixed variance $v^2$. For a discrete outcome, we use a Bernoulli distribution parametrized by an MLP similarly:
\begin{equation}
  \begin{aligned}
    & p(x_i|u^i_i,u^c_i,u^n_i) = \prod_{i=0}^{|X|} \mathcal{N}(\mu=\hat{\mu}_x,\sigma^2=v_x^2) 
     \\
    \text{or}\hspace{4mm} & 
    p(x_i|u^i_i,u^c_i,u^n_i) =  \prod_{i=0}^{|X|} \mathbf{Bern}(\pi = \hat{\pi}_x), 
\end{aligned}
\end{equation}
where for the continuous case $\hat{\mu}_x$ is the mean of the Gaussian distribution parametrized by an MLP using the sampled $u^i_i,u^c_i,u^n_i$ from posterior as input, and $v_x^2$ is the fixed variance of Gaussian distribution, and for the discrete case  $\hat{\pi}_x$ is the mean of Bernoulli distribution similarly parametrized by an MLP.

For this module, we use the negative variational Evidence Lower BOund (ELBO) as the loss function, defined as
\begin{equation}
  \begin{aligned}
    & \textbf{ELBO} =  \bE_{q(u^i,u^c,u^n|x.x_{\mathcal N})} [\log p(x|u^i,u^c,u^n) \\&
    + \log p(u^i,u^c,u^n|x_{\mathcal N})
    - \log q(u^i,u^c,u^n|x.x_{\mathcal N})].
\end{aligned}
\end{equation}
% Detailed ELBO devotions are given in Appendix \ref{app: elbo}. 
Following \citet{lu2022invariant}, we utilize the score matching technique \cite{vincent2011connection} for training unnormalized probabilistic models to learn the parameters of $\bm T$ and $\bm \lambda$ by minimizing
\begin{equation}
  \begin{aligned}
    & \mathcal L_{sm} =    
    \bE_{q(u^i,u^c,u^n|x.x_{\mathcal N})} [
    \\& 
    \| \nabla _{u} \log q(u^i,u^c,u^n|x.x_{\mathcal N}) -\nabla_{u} \log p(u^i,u^c,u^n|x_{\mathcal N}) \|^2 ],
\end{aligned}
\end{equation}
where $\nabla$ is the gradient operator.


\subsection{Feature Module and Outcome Estimator Module} \label{sec: Network Effect Estimator}

After obtaining $u^i, u^c, u^n$, we can aggregate the necessary information for the effect estimation.
Specifically, in the feature module, we first aggregate the neighbors' $u^c, u^n$ to obtain  $u^c_{\mathcal N}, u^n_{\mathcal N}$ via GCN:
\begin{equation*}
% \small
\begin{aligned}
   & h_{i,3}^{neigh} =\sigma(\sum_{j\in \mathcal{N}_i}\frac{1}{\sqrt{d_id_j}}W_2^T [u^c_j,u^n_j]),    
   \\& h_{i,4} = MLP_1(h^{neigh}_{i,3}, u^i_i, u^c_i) ,
\end{aligned}
\end{equation*}
where $\sigma(\cdot)$ is a non-linear activation function, $d_i$ is the degrees of unit $i$, $W_2$ is the learning weight matrix of GCN.

Then, we use $h_{i,4}$, $u^i_i, u^c_i$ and $t_i,z_i$ together to estimate $y_i$ of treated and control groups respectively, i.e.,
\begin{equation*} \small
 \mu^{NN}(t_i,z_i, u^i_i, u^c_i, u^c_{\mathcal N_i}, u^n_{\mathcal N_i})= 
\left\{
\begin{aligned}
MLP_3(z_i, h_{i,2}) & \quad  t_i =1  \\
MLP_4(z_i, h_{i,2}) & \quad  t_i =0 
\end{aligned}
\right.
\end{equation*}
and the loss function is 
\begin{equation}
    \mathcal{L}_y =  \Sigma_{i=1}^n  (y_i - \mu^{NN}(t_i,z_i, x_i, x_{\mathcal N_i}))^2.
\end{equation}
Moreover, inspired by \citet{jiang2022estimating,cai2023generalization}, we further consider a balancing regularization term as our loss:
\begin{equation}
    \mathcal{L}_{IPM} =  \text{IPM} (p(h_{i,4},t_i,z_i), p(h_{i,4})p(t_i)p(z_i))),
\end{equation}
where $\text{IPM}(p.q)=\sup_{g\in\mathcal G} |\int_\mathcal{X} g(x)(p(x)-q(x))dx |$ is the integral probability metric, measuring the distance between two distribution $p,q$, which can be implemented by Wasserstein Distance.
Here the samples from $p(h_{i,4})p(t_i)p(z_i))$ is obtained by randomly permuting $t_i$ and $z_i$ separately.

Overall, our final loss function is 
\begin{equation}
    \mathcal{L}_{all}  = - \textbf{ELBO} + \mathcal L_{sm} +  \mathcal{L}_y + \mathcal{L}_{IPM}.
\end{equation}


\section{Experiments}

In this section, we validate the proposed method on two commonly used semisynthetic datasets. In detail, we verify the effectiveness of our algorithm and further evaluate the correctness of the analysis with the help of semisynthetic datasets.


\begin{table*}[!t] 
\renewcommand{\arraystretch}{1.7}
\vspace{-.2cm}
\caption{Experimental results on Flickr(homo) Dataset. The top result is highlighted in bold, and the runner-up is underlined.}     
\label{tab: Flickr}
\centering 
\resizebox{\linewidth}{!}{ \huge
\begin{tabular}{@{}l|ccc| ccc | ccc| ccc @{}} 
        \hline
         & \multicolumn{6}{c|}{$\varepsilon_{average}$}  & \multicolumn{6}{c}{$\varepsilon_{individual}$}  \\ \hline
         & \multicolumn{3}{c|}{Within Sample}
         & \multicolumn{3}{c|}{Out-of Sample}
         & \multicolumn{3}{c|}{Within Sample}
         & \multicolumn{3}{c}{Out-of Sample}  \\ \hline
         Methods & AME & ASE  & ATE & AME & ASE  & ATE &    
         IME & ISE  & ITE &  IME & ISE  & ITE \\ \hline
        TARNET+z  & { $0.0783 _{\pm 0.0418} $} &  { $0.0874 _{\pm0.0213} $} &  { $0.2025 _{\pm0.0396} $} &  { $0.0976 _{\pm 0.0506} $} &  { $0.0724 _{\pm0.0184} $} &  { $0.1356 _{\pm0.0587} $} &  { $0.1362 _{\pm 0.0254} $} &  { $0.1103 _{\pm0.0194} $} &  { $0.2358 _{\pm0.0383} $} &  { $1.0869 _{\pm 1.2258} $} &  { $0.1011 _{\pm0.0185} $} &  { $1.0889 _{\pm1.2270} $}  \\ \hline
        CFR+z  &   { $0.0579 _{\pm 0.0247} $} &  { $0.0785 _{\pm0.0070} $} &  { $0.1651 _{\pm0.0121} $} &  { $0.0507 _{\pm 0.0192} $} &  { $0.0783 _{\pm0.0070} $} &  { $0.1581 _{\pm0.0097} $} &  { $0.0599 _{\pm 0.0240} $} &  { $0.0786 _{\pm0.0070} $} &  { $0.1654 _{\pm0.0120} $} &  { $0.3465 _{\pm 0.4615} $} &  { $0.0786 _{\pm0.0069} $} &  { $0.4102 _{\pm0.4278} $} \\ \hline
        % alphabase=1
        GEst   & { $0.1551 _{\pm 0.0130} $} &  { $0.2475 _{\pm0.0476} $} &  { $0.0805 _{\pm0.0325} $} &  { $0.1511 _{\pm 0.0137} $} &  { $0.2494 _{\pm0.0470} $} &  { $0.0805 _{\pm0.0278} $} &  { $0.1779 _{\pm 0.0122} $} &  { $0.2656 _{\pm0.0378} $} &  { $0.1268 _{\pm0.0160} $} &  { $0.2867 _{\pm 0.2172} $} &  { $0.2677 _{\pm0.0372} $} &  { $0.2471 _{\pm0.2352} $}  \\ \hline
        ND+z  &  { $0.1416 _{\pm 0.0240} $} &  { $0.0204 _{\pm0.0093} $} &  { $0.0478 _{\pm0.0216} $} &  { $0.1435 _{\pm 0.0364} $} &  { $0.0226 _{\pm0.0101} $} &  { $0.0485 _{\pm0.0236} $} &  { $0.1427 _{\pm 0.0246} $} &  { $0.0221 _{\pm0.0090} $} &  { $0.0501 _{\pm0.0178} $} &  { $0.3849 _{\pm 0.2395} $} &  { $0.0348 _{\pm0.0078} $} &  { $0.3453 _{\pm0.2772} $}  \\ \hline
        NetEst  & { $0.0515 _{\pm 0.0538} $} &  { $0.0355 _{\pm0.0317} $} &  { $0.0715 _{\pm0.0381} $} &  { $0.0470 _{\pm 0.0500} $} &  { $0.0338 _{\pm0.0330} $} &  { $0.0529 _{\pm0.0395} $} &  { $0.0844 _{\pm 0.0406} $} &  { $0.0566 _{\pm0.0253} $} &  { $0.1043 _{\pm0.0312} $} &  { $0.2934 _{\pm 0.3001} $} &  { $0.2809 _{\pm0.3387} $} &  { $0.3068 _{\pm0.1860} $} \\ \hline
        % RRNet  && &  & \\ \hline
        % NDR  &&&& \\ \hline
        TNet  & { $0.0319 _{\pm 0.0249} $} &  { $0.0274 _{\pm0.0309} $} &  { $0.0735 _{\pm0.0240} $} &  { $0.0299 _{\pm 0.0231} $} &  { $0.0277 _{\pm0.0313} $} &  { $0.0715 _{\pm0.0214} $} &  { $0.0347 _{\pm 0.0282} $} &  { $0.0276 _{\pm0.0313} $} &  { $0.0752 _{\pm0.0263} $} &  { $0.0561 _{\pm 0.0648} $} &  { $0.0286 _{\pm0.0331} $} &  { $0.0918 _{\pm0.0555} $}  \\ \hline
        Ours\_w/o\_IPM &  { $0.0359 _{\pm 0.0262} $} &  { $\textbf{0.0133} _{\pm0.0050} $} &  { $0.0598 _{\pm0.0366} $} &  { $0.0394 _{\pm 0.0262} $} &  { $\textbf{0.0123} _{\pm0.0060} $} &  { $0.0602 _{\pm0.0355} $} &  { $0.0410 _{\pm 0.0228} $} &  { $\textbf{0.0155} _{\pm0.0033} $} &  { $0.0643 _{\pm0.0337} $} &  { $0.0424 _{\pm 0.0242} $} &  { $\textbf{0.0142} _{\pm0.0044} $} &  { $0.0632 _{\pm0.0334} $} \\ \hline
        Ours &  { $\textbf{0.0296} _{\pm 0.0219} $} &  { $0.0252 _{\pm0.0208} $} &  { $\textbf{0.0266} _{\pm0.0208} $} &  { $\textbf{0.0289} _{\pm 0.0208} $} &  { $0.0252 _{\pm0.0208} $} &  { $\textbf{0.0260} _{\pm0.0200} $} &  { $\textbf{0.0297} _{\pm 0.0220} $} &  { $0.0252 _{\pm0.0208} $} &  { $\textbf{0.0267} _{\pm0.0209} $} &  { $\textbf{0.0289} _{\pm 0.0209} $} &  { $0.0252 _{\pm0.0208} $} &  { $\textbf{0.0261} _{\pm0.0201} $} 
        \\ \hline
    \end{tabular}
    }
\end{table*}


\subsection{Experimental Setup}



\begin{table*}[!t] 
\renewcommand{\arraystretch}{1.7}
\vspace{-.2cm}
\caption{Experimental results on Flickr(hete) Dataset. The top result is highlighted in bold, and the runner-up is underlined.}     
\label{tab: Flickr_hete}
\centering 
\resizebox{\linewidth}{!}{ \huge
\begin{tabular}{@{}l|ccc| ccc | ccc| ccc @{}} 
        \hline
         & \multicolumn{6}{c|}{$\varepsilon_{average}$}  & \multicolumn{6}{c}{$\varepsilon_{individual}$}  \\ \hline
         & \multicolumn{3}{c|}{Within Sample}
         & \multicolumn{3}{c|}{Out-of Sample}
         & \multicolumn{3}{c|}{Within Sample}
         & \multicolumn{3}{c}{Out-of Sample}  \\ \hline
         Methods & AME & ASE  & ATE & AME & ASE  & ATE &    
         IME & ISE  & ITE &  IME & ISE  & ITE \\ \hline
        TARNET+z  & { $0.1315 _{\pm 0.0740} $} &  { $0.1673 _{\pm0.0423} $} &  { $0.3590 _{\pm0.0785} $} &  { $0.1554 _{\pm 0.1110} $} &  { $0.1319 _{\pm0.0307} $} &  { $0.2728 _{\pm0.1321} $} &  { $0.2320 _{\pm 0.0432} $} &  { $0.2042 _{\pm0.0479} $} &  { $0.4254 _{\pm0.0802} $} &  { $1.5274 _{\pm 1.6256} $} &  { $0.1760 _{\pm0.0351} $} &  { $1.5957 _{\pm1.5779} $}  \\ \hline
        CFR+z  &  { $0.1131 _{\pm 0.0476} $} &  { $0.1437 _{\pm0.0081} $} &  { $0.2960 _{\pm0.0182} $} &  { $0.0998 _{\pm 0.0458} $} &  { $0.1412 _{\pm0.0085} $} &  { $0.2789 _{\pm0.0309} $} &  { $0.1445 _{\pm 0.0407} $} &  { $0.1463 _{\pm0.0083} $} &  { $0.3242 _{\pm0.0224} $} &  { $0.5946 _{\pm 0.7379} $} &  { $0.1448 _{\pm0.0087} $} &  { $0.7104 _{\pm0.6761} $} \\ \hline
        % alphab=1
        GEst   & { $0.3283 _{\pm 0.0426} $} &  { $0.4717 _{\pm0.1336} $} &  { $0.1074 _{\pm0.0255} $} &  { $0.3356 _{\pm 0.0270} $} &  { $0.4723 _{\pm0.1312} $} &  { $0.0969 _{\pm0.0099} $} &  { $0.3697 _{\pm 0.0386} $} &  { $0.5123 _{\pm0.1231} $} &  { $0.2178 _{\pm0.0214} $} &  { $0.7124 _{\pm 0.6463} $} &  { $0.5144 _{\pm0.1202} $} &  { $0.5914 _{\pm0.7073} $}  \\ \hline
        ND+z  &  { $0.2420 _{\pm 0.0330} $} &  { $0.0293 _{\pm0.0113} $} &  { $0.0852 _{\pm0.0365} $} &  { $0.2433 _{\pm 0.0539} $} &  { $0.0318 _{\pm0.0134} $} &  { $0.0785 _{\pm0.0422} $} &  { $0.2571 _{\pm 0.0348} $} &  { $0.0430 _{\pm0.0040} $} &  { $0.1607 _{\pm0.0188} $} &  { $0.5156 _{\pm 0.2180} $} &  { $0.0669 _{\pm0.0059} $} &  { $0.4720 _{\pm0.2580} $}  \\ \hline
        NetEst  & { $0.0530 _{\pm 0.0423} $} &  { $0.0452 _{\pm0.0351} $} &  { $0.0723 _{\pm0.0319} $} &  { $0.0466 _{\pm 0.0322} $} &  { $0.0565 _{\pm0.0454} $} &  { $0.0818 _{\pm0.0379} $} &  { $0.1145 _{\pm 0.0278} $} &  { $0.0667 _{\pm0.0267} $} &  { $0.1660 _{\pm0.0163} $} &  { $0.6855 _{\pm 0.2607} $} &  { $0.5353 _{\pm0.2507} $} &  { $0.5625 _{\pm0.1367} $}  \\ \hline
        TNet  & { $0.0411 _{\pm 0.0238} $} &  { $0.0206 _{\pm0.0073} $} &  { $0.0282 _{\pm0.0297} $} &  { $0.0417 _{\pm 0.0237} $} &  { $0.0196 _{\pm0.0098} $} &  { $0.0268 _{\pm0.0314} $} &  { $0.0936 _{\pm 0.0170} $} &  { $0.0338 _{\pm0.0065} $} &  { $0.1360 _{\pm0.0210} $} &  { $0.0950 _{\pm 0.0157} $} &  { $0.0361 _{\pm0.0074} $} &  { $0.1415 _{\pm0.0223} $}  \\ \hline
        Ours\_w/o\_IPM &  { $\textbf{0.0296} _{\pm 0.0248} $} &  { $\textbf{0.0125} _{\pm0.0131} $} &  { $0.0581 _{\pm0.0321} $} &  { $\textbf{0.0254} _{\pm 0.0212} $} &  { $\textbf{0.0171} _{\pm0.0195} $} &  { $0.0437 _{\pm0.0241} $} &  { $0.0929 _{\pm 0.0181} $} &  { $0.0361 _{\pm0.0114} $} &  { $0.1503 _{\pm0.0221} $} &  { $\textbf{0.0910} _{\pm 0.0153} $} &  { $0.0393 _{\pm0.0149} $} &  { $0.1480 _{\pm0.0172} $}  \\ \hline
        Ours &  { $0.0377 _{\pm 0.0207} $} &  { $0.0206 _{\pm0.0157} $} &  { $\textbf{0.0228} _{\pm0.0058} $} &  { $0.0384 _{\pm 0.0225} $} &  { $0.0226 _{\pm0.0177} $} &  { $\textbf{0.0197} _{\pm0.0088} $} &  { $\textbf{0.0922} _{\pm 0.0115} $} &  { $\textbf{0.0353} _{\pm0.0121} $} &  { $\textbf{0.1326} _{\pm0.0161} $} &  { $0.0941 _{\pm 0.0100} $} &  { $\textbf{0.0390} _{\pm0.0135} $} &  { $\textbf{0.1379} _{\pm0.0164} $} 
        \\ \hline
    \end{tabular}
    }
\end{table*}

\textbf{Datasets} We consider two wildly used semisynthetic datasets BlogCatalog and Flickr to verify the effectiveness of our estimator. We further use a synthetic dataset to validate the correctness of our theories, i.e., whether our method can correctly recover latent confounders.

Following existing works \cite{jiang2022estimating, guo2020learning, ma2021deconfounding, chen2024doubly}, we use two semisynthetic datasets to evaluate our proposed method:
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=1pt]
    \item \textbf{BlogCatalog (BC)} is an online community where users post blogs. In this dataset, each unit is a blogger and each edge is the social link between units. The features are bag-of-words representations of keywords in bloggers' descriptions.
    \item \textbf{Flickr} is an online social network where users can share images and videos. In this dataset, each unit is a user and each edge is the social relationship between units. The features are the list of tags of units' interests.
\end{itemize}

We reuse the original covariates as the latent confounders and then divide them into $u^i, u^c, u^n$.
We generate the proxies $x$ using 
$x_i = w_1 u_i + e_i$ where $w_1 $ are randomly sampled from Uniform distribution $\mathcal{U}(0.5, 1)$ and $e_{x,i}$ is standard Gaussian noise. Then given the latent confounder $u^i_i, u^c_i, u^n_i$ of unit $i$, the treatments are simulated by
\begin{equation*}
    \begin{aligned}
        t_i=
            \begin{cases}
            1& \text{if \quad $tpt_i>\overline{tpt}$},\\
            0& \text{else},
            \end{cases}
    \end{aligned}
\end{equation*}
where $\overline{tpt}$ is the average of all $tpt_i$, and $tpt_i = pt_i + pt_{\mathcal N_i}$, and $pt_i = Sigmoid(w_2 \times [u^i_i, u^c_i])$, and $pt_{\mathcal N _i}= \frac{1}{|\mathcal N_i|} \sum_j^{j\in \mathcal N_i} Sigmoid(w_3 \times [u^i_c, u^n_i]) $ serves as the neighbors influences.
% is the average of all $i$'s neighbors' $pt_i$ satisfying $pt_i$. 
Here $w_2$ and $w_3$ are randomly generated weight vectors that mimic the causal mechanism from the latent confounders to treatments. Then, $z_i$ can be directly obtained by network topology $E$ and $t_{\mathcal N_i}$.

We then modify the data generation of outcome $y$ in \citet{jiang2022estimating}:
\begin{equation*}
    \begin{aligned}
    \small
        y_i(t_i,z_i) = t_i + z_i + po_i + 0.5 \times po_{\mathcal N_i} + e_{y,i},
    \end{aligned}
\end{equation*}
where $e_{y,i}$ is a Gaussian noise term, and $po_i = Sigmoid(w_4 \times u_i+w_5 \times u_c)$, and $ po_{\mathcal N_i}$ is the averages of $Sigmoid(w_6 \times u_c +w_7 \times u_n)$. Here, $w_4,w_5,w_6$, and $w_7$ are all randomly generated weight vectors that mimic the causal mechanism from the confounders to outcomes.
We denote this dataset as \textbf{BC(homo)} and \textbf{Flickr(homo)}\footnote{Original datasets are available at \url{https://github.com/songjiang0909/Causal-Inference-on-Networked-Data}.} since this generation of $y$ only measures the homogeneous causal effects.

Also following \citet{chen2024doubly}, we consider the data generation of outcome $y$ with heterogeneous effect:
\begin{equation*}
    \begin{aligned}
    \small
         y_i& (t_i,z_i) = t_i + z_i + po_i + 0.5 \times po_{\mathcal N_i} 
        \\& + t_i (po_i+0.5\times po_{\mathcal N_i})
        +  t_i (0.5 \times po_i+ po_{\mathcal N_i})
        + e_{y,i},
    \end{aligned}
\end{equation*}
which is denoted as \textbf{BC(hete)} and \textbf{Flickr(hete)}.

Due to the space limit, we leave the detailed data generation process of the synthetic dataset in Appendix \ref{app: dgp}.



\textbf{Baselines} We denote our method as \textbf{Ours} and a variant without IPM loss as \textbf{Ours\_w/o\_IPM}.
We compare our methods with several state-of-the-art baselines \footnote{The details are in Appendix \ref{app: baseline}. Our code will be available upon acceptance.}
Following \citet{chen2024doubly}, we modify TARNET, CFR \cite{johansson2021generalization} and ND \cite{guo2020learning} by additionally inputting the exposure $z_i$, denoted as \textbf{TARNET+z}, \textbf{CFR+z} and \textbf{ND+z} respectively. We also consider several baselines that are designed for networked effect estimation under the same setting, including \textbf{GEst} \cite{ma2021causal},  \textbf{NetEst} \cite{jiang2022estimating}, and \textbf{TNet} \cite{chen2024doubly}. 

\textbf{Metrics} In this paper, we use the Mean Absolute Error (MAE) on  AME, ASE, and ATE as our metric, i.e., $\varepsilon_{average}= | \hat \tau - \tau|$, where $\tau$ and $\hat \tau$ are the average causal effect and estimated one. 
We also use the Rooted Precision in Estimation of Heterogeneous Effect on IME, ASE, and ITE, $\varepsilon_{individual} = \sqrt{\frac{1}{n} \Sigma_{i=1}^n (\hat \tau_i - \tau_i)^2 }$, where $\tau_i$ and $\hat \tau_i$ are the individual causal effect and estimated one.
The mean and standard deviation of these metrics via $5$ times running are reported. Note that our main estimands are AME, ASE, and ATE in this paper.



\subsection{Experimental Analyses}

\begin{figure}
    \centering
    \vspace{-.2cm}
    \includegraphics[width=1.\linewidth]{figure/recover_exp.pdf}
    \vspace{-.3cm}
    \caption{Visualization of recovered and ground-true latent confounders $U^i, U^c$, and $U^n$.}
    \label{fig: recovered latent}
    \vspace{-.3cm}
\end{figure}

\textbf{Effectiveness of Our Method}
As shown in Table \ref{tab: Flickr} and Table \ref{tab: Flickr_hete}, we have conducted experiments by running our proposed methods and several baselines. Overall, our methods outperform all methods consistently with smaller estimation errors in all metrics, indicating the effectiveness of our methods.
Specifically, compared with the baselines, our methods perform better in terms of both average and heterogeneous treatment effect estimation, with smaller errors and standard deviation.
This is reasonable since existing methods do not consider the latent confounders that hinder their identifications.
Both Ours and Ours\_w/o\_IPM are based on identifiable representation learning techniques and thereby achieve superior performances with recovered latent confounders.
Compared ours with its variant without the IPM term, we found that the IPM term can slightly improve the performance.
This is due to the fact that the IPM term effectively mitigates the confounding bias with balanced representations.


% 0.2489, 0.2775, 0.9505, 
% 0.2179, 0.8930, 0.5070, 
% 0.9435, 0.4125, 0.1412, 
\begin{table}[!h] \small
    \centering
    \resizebox{0.35\textwidth}{!}{ 
    \begin{tabular}{cccc} 
\hline
 &  $\hat u^{n}$ & $\hat u^{c}$ &$\hat u^{i}$ \\
\hline
${u}^{i}$ & 0.2489 & 0.2775 & \textbf{0.9505} \\
${u}^{c}$ & 0.2179 & \textbf{0.8930} & 0.5070 \\
${u}^{n}$ & \textbf{0.9435} & 0.4125 & 0.1412 \\
\hline
    \end{tabular}
    }
    \caption{MCC results of recovered latent confounders.}
    \label{tab: mcc}
\end{table}


\textbf{Correctness of Representation Learning}
To validate the correctness of our representation learning method, We conduct experiments in the simulated dataset and visualize the recovered latent confounders $\hat U^i, \hat U^c, \hat U^n$ with ground-true $ U^i,  U^c, U^n$ in Figure \ref{fig: recovered latent}.
And we calculate the MCC results in Table \ref{tab: mcc}.
The result shows that the recovered latent confounders are highly correlated with the ground-true latent confounders, with very high MCC values.
This indicates that our method can correctly recover the latent confounders, which validates the correctness of Theorem \ref{theorem:recover latent}.


% \subsection{Stability Regarding Hyperparameters}


\section{Conclusion}
\label{conclusion}

In this paper, we address the problem of networked causal effect identification and estimation in the presence of latent confounders.
We leverage the networked information to achieve the identifiability of latent confounders.
With identified latent confounders, we theoretically establish the identification result of networked effects.
We further devise an effective estimator built on the theoretical findings.
Extensive experiments validate the correctness of our theories and the effectiveness of our proposed estmator.

\newpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of causal inference under networked interference.
There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.
%Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}


% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

%\counterwithout{theorem}{section}
%\counterwithout{lemma}{section}

\section{Proof of Theorem \ref{theorem:recover latent}}
The proof can be directly obtained from the proof of Theorem 1 in \citet{lu2022invariant}, with the following modifications:
\begin{itemize}
    \item we consider $X_\mathcal N$ as the prior conditional set;
    \item we specify that the latent confounder $U$ contains three parts, i.e., $U=[U^i, U^c, U^n]$.
\end{itemize}

\section{Proof of Theorem \ref{theorem: identify}}

\begin{proof}
    Under Theorem \ref{theorem:recover latent}, we can recover the whole distribution $p(x,u^i,u^c,u^n,t,z,y)$, then the conditional average dose-response function is identified by
    \begin{equation}
        \begin{aligned}
             \mu(t,z,x,x_{\mathcal N })
             & = \mathbb E [Y(t,z)|X=x,X_{\mathcal N }=x_{\mathcal N }] \\
             & = \mathbb E [\mathbb E [Y(t,z)|X=x,X_{\mathcal N }=x_{\mathcal N },U^i=u^i,U^c=u^c,U^c_{\mathcal N }=u^c_{\mathcal N },U^n_{\mathcal N }=u^n_{\mathcal N }]] \\
             & = \mathbb E [\mathbb E [Y(t,z)|X=x,X_{\mathcal N }=x_{\mathcal N },U^i=u^i,U^c=u^c,U^c_{\mathcal N }=u^c_{\mathcal N },U^n_{\mathcal N }=u^n_{\mathcal N }, T=t,Z=z]] \\
              & = \mathbb E [\mathbb E [Y|X=x,X_{\mathcal N }=x_{\mathcal N },U^i=u^i,U^c=u^c,U^c_{\mathcal N }=u^c_{\mathcal N },U^n_{\mathcal N }=u^n_{\mathcal N }, T=t,Z=z]] \\
        \end{aligned}
    \end{equation}
    where the third equality is based on Assumption \ref{asmp: Latent Network unconfounderness} and the forth equality is based on Assumption \ref{asmp: consistency}.
    and then the networked effect $\psi(t,z) := \mathbb E [Y(t,z)]$ is immediately identified.
\end{proof}

 % & \psi(t,z) := \mathbb E [Y(t,z)], \\
    % & \mu(t,z,x,x_{\mathcal N }) := \mathbb E [Y(t,z)|X=x,X_{\mathcal N }=x_{\mathcal N }],

% \section{ELBO}
% \label{app: elbo}


\section{Additional Experimental Details}

\subsection{Baseline Methods}
\label{app: baseline}
The compared baselines in this paper are 
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=1pt]
    \item \textbf{TARNET+z}: Original Tarnet \cite{johansson2021generalization} uses two-heads neural networks, serving as T-Learner-like estimator, to estimate causal effects under no interference assumption.
    We modify TARNET by additionally inputting the exposure $z_i$.
    \item \textbf{CFR+z}: Original CFR \cite{johansson2021generalization} uses two-heads neural networks with an MMD term to achieve \underline{c}ounter\underline{f}actual \underline{r}egression under no interference assumption.
    We modify CFR by additionally inputting the exposure $z_i$.
    \item \textbf{ND+z}: Original ND \cite{guo2020learning} propose \underline{n}etwork \underline{d}econfounder framework by using network information under no interference assumption.
    We modify ND by additionally inputting the exposure $z_i$.
    \item \textbf{GEst} \cite{ma2021causal}: GEst, based on CFR, uses \underline{G}CN to aggregate the features of neighbors and input the exposure $z_i$ to \underline{est}imate causal effects under networked interference. 
    \item \textbf{NetEst} \cite{jiang2022estimating}: NetEst learns balanced representation via adversarial learning for \underline{net}worked causal effect \underline{est}imation.
    % \item \textbf{RRNet} \cite{cai2023generalization}: RRNet combines the \underline{r}epresentation learning and \underline{r}eweighting techniques in its \underline{net}work to estimate causal effects under interference. 
    \item \textbf{TNet} \cite{chen2024doubly}: TNet utilizes \underline{t}argeted learning techniques into its neural \underline{net}works model to estimate causal effects under networked interference in a double robust manner. 
    % \item \textbf{NDR} \cite{liu2023nonparametric}: NDR is a \underline{n}onparametric \underline{d}oubly robust estimator to identify the average causal effects under networked interference, where we use SuperLearner to estimate nuisance functions. Since it is used to identify average effects on the given training data, we only report the results regarding AME, ASE, and ATE on \textit{Within Sample}. 
\end{itemize}

\subsection{Detailed Data Generation of Synthetic Dataset}
\label{app: dgp}

We first generate the network graph $E$ with expected degrees $5$.

According to $E$, we directly generate all samples' $u^i, u^c, u^n$ from a multivariate Gaussian distribution.
We set the sample size as $1000$,
then we first sample $\u = [[u^i]^T, [u^c]^T, [u^n]^T]^T$ as a $3000$-dimensional vector.
To mimic the correlation among $u^i,u^c,u^n$ and between units, we generate the $3000 \times 3000$ covariance matrix $V$ with all elements equal to $0.1$ besides diagonal equal to $1$.
This results in that, for the same units, $u^i_i,u^c_i,u^n_i$ are correlated with covariance $0.1$, and for the different units, their latent confounders are also correlated with covariance $0.1$, e.g. $u_i^i$ and $u_i^c$.
Then we generate $\u \sim \mathcal N (\mathbf{1}, V)$ and reshape $\u$ as $[u^i, u^c, u^n]$, i.e., reshape to obtain the $1000\times 3$ design matrix of $u$.

Then, given $[u^i, u^c, u^n]$, we generate treatment $t$ as
\begin{equation}
    t_i \sim \textbf{Bern} (1/(1+\exp (- \frac{u_i^i+u_i^c+\sum_j^{j\in \mathcal N_i}(1.5 u_j^c-0.5u_j^n)-C}{4})))
\end{equation}
where $C$ is a constant set to ensure the positivity assumption holds.
And then, $z$ can be obtained by aggregating $t$ using $E$.

Sequentially, the outcome $y$ is generated by
\begin{equation}
    y_i = u^i_i + u^c_i + t_i \times (2u^i + 1.6u^c)
    + z_i \times (\sum_j^{j\in \mathcal N_i}(1.5 u_j^c-0.5u_j^n) + 1.5 u^i_i + 0.5u^c_i ) + \epsilon_{y,i},
\end{equation}
where $\epsilon_{y,i}$ is the Gaussian noise.

Finally, we generate the proxies $x$ of latent confounders by $x_i = w [u^i_i, u^c_i, u^n_i] + \epsilon_{x,i}$ where $w$ is a randomly $3\times 6$ weight matrix sampled from Gaussian distribution with a mean $1$ and scale $1$, and $\epsilon_{x,i}$ is the Gaussian noise. 
This results in the final $6$-dimensional observed proxies $x$.


\subsection{Additional Experimental Results}

We report additional experimental results on BC(homo) and BC(hete) datasets in Table \ref{tab: BC} and Table \ref{tab: BC_hete}. The results are similar to the experimental results in the Flickr dataset.

\begin{table*}[!h] 
\renewcommand{\arraystretch}{1.7}
\caption{Experimental results on BC(homo) Dataset. The top result is highlighted in bold, and the runner-up is underlined.}     
\label{tab: BC}
\centering 
\resizebox{\linewidth}{!}{ \huge
\begin{tabular}{@{}l|ccc| ccc | ccc| ccc @{}} 
\hline
 & \multicolumn{6}{c|}{$\varepsilon_{average}$}  & \multicolumn{6}{c}{$\varepsilon_{individual}$}  \\ \hline
 & \multicolumn{3}{c|}{Within Sample}
 & \multicolumn{3}{c|}{Out-of Sample}
 & \multicolumn{3}{c|}{Within Sample}
 & \multicolumn{3}{c}{Out-of Sample}  \\ \hline
 Methods & AME & ASE  & ATE & AME & ASE  & ATE &    
 IME & ISE  & ITE &  IME & ISE  & ITE \\ \hline
TARNET+z  &  { $0.1573 _{\pm 0.0405} $} &  { $0.0824 _{\pm0.0149} $} &  { $0.2046 _{\pm0.0193} $} &  { $0.1492 _{\pm 0.0370} $} &  { $0.0855 _{\pm0.0147} $} &  { $0.1982 _{\pm0.0380} $} &  { $0.2096 _{\pm 0.0250} $} &  { $0.1161 _{\pm0.0159} $} &  { $0.2444 _{\pm0.0239} $} &  { $0.2809 _{\pm 0.0507} $} &  { $0.1209 _{\pm0.0152} $} &  { $0.3062 _{\pm0.0627} $}   \\ \hline
CFR+z  &  { $0.0788 _{\pm 0.0096} $} &  { $0.1157 _{\pm0.0076} $} &  { $0.2323 _{\pm0.0106} $} &  { $0.0770 _{\pm 0.0099} $} &  { $0.1157 _{\pm0.0075} $} &  { $0.2306 _{\pm0.0106} $} &  { $0.0796 _{\pm 0.0091} $} &  { $0.1158 _{\pm0.0076} $} &  { $0.2325 _{\pm0.0106} $} &  { $0.1000 _{\pm 0.0388} $} &  { $0.1157 _{\pm0.0075} $} &  { $0.2405 _{\pm0.0166} $}  \\ \hline
% alpha=2
GEst &  { $0.1872 _{\pm 0.0672} $} &  { $0.2369 _{\pm0.0607} $} &  { $0.1422 _{\pm0.0562} $} &  { $0.1955 _{\pm 0.0611} $} &  { $0.2391 _{\pm0.0617} $} &  { $0.1302 _{\pm0.0524} $} &  { $0.2307 _{\pm 0.0493} $} &  { $0.2603 _{\pm0.0586} $} &  { $0.1877 _{\pm0.0495} $} &  { $0.2388 _{\pm 0.0431} $} &  { $0.2623 _{\pm0.0592} $} &  { $0.1790 _{\pm0.0440} $}   \\ \hline
ND+z   &  { $0.2375 _{\pm 0.0450} $} &  { $0.0316 _{\pm0.0104} $} &  { $0.0790 _{\pm0.0226} $} &  { $0.2380 _{\pm 0.0458} $} &  { $0.0323 _{\pm0.0122} $} &  { $0.0768 _{\pm0.0254} $} &  { $0.2377 _{\pm 0.0448} $} &  { $0.0321 _{\pm0.0101} $} &  { $0.0792 _{\pm0.0226} $} &  { $0.2477 _{\pm 0.0460} $} &  { $0.0379 _{\pm0.0099} $} &  { $0.1068 _{\pm0.0172} $} 

 \\ \hline
NetEst  & { $0.1059 _{\pm 0.0609} $} &  { $0.0284 _{\pm0.0297} $} &  { $0.0387 _{\pm0.0288} $} &  { $0.0987 _{\pm 0.0663} $} &  { $0.0257 _{\pm0.0276} $} &  { $0.0356 _{\pm0.0268} $} &  { $0.1366 _{\pm 0.0481} $} &  { $0.0631 _{\pm0.0205} $} &  { $0.0994 _{\pm0.0214} $} &  { $0.1680 _{\pm 0.0620} $} &  { $0.0920 _{\pm0.0316} $} &  { $0.1507 _{\pm0.0647} $}  \\ \hline
% RRNet  && &  & \\ \hline
% NDR  &&&& \\ \hline
TNet  & { $0.1045 _{\pm 0.0610} $} &  { $0.0502 _{\pm0.0559} $} &  { $0.0473 _{\pm0.0229} $} &  { $0.1045 _{\pm 0.0610} $} &  { $0.0502 _{\pm0.0559} $} &  { $0.0473 _{\pm0.0229} $} &  { $0.1045 _{\pm 0.0610} $} &  { $0.0502 _{\pm0.0559} $} &  { $0.0473 _{\pm0.0229} $} &  { $0.1045 _{\pm 0.0610} $} &  { $0.0502 _{\pm0.0559} $} &  { $0.0473 _{\pm0.0229} $}  \\ \hline
Ours\_w/o\_IPM &   { $0.0356 _{\pm 0.0176} $} &  { $0.0222 _{\pm0.0104} $} &  { $0.0514 _{\pm0.0163} $} &  { $0.0416 _{\pm 0.0176} $} &  { $0.0206 _{\pm0.0100} $} &  { $0.0433 _{\pm0.0136} $} &  { $0.0411 _{\pm 0.0147} $} &  { $0.0244 _{\pm0.0122} $} &  { $0.0554 _{\pm0.0182} $} &  { $0.0435 _{\pm 0.0169} $} &  { $0.0218 _{\pm0.0106} $} &  { $0.0453 _{\pm0.0137} $}   \\ \hline
Ours &  { $0.0661 _{\pm 0.0485} $} &  { $0.0232 _{\pm0.0164} $} &  { $0.0442 _{\pm0.0258} $} &  { $0.0661 _{\pm 0.0485} $} &  { $0.0232 _{\pm0.0164} $} &  { $0.0442 _{\pm0.0258} $} &  { $0.0661 _{\pm 0.0485} $} &  { $0.0232 _{\pm0.0164} $} &  { $0.0442 _{\pm0.0258} $} &  { $0.0661 _{\pm 0.0485} $} &  { $0.0232 _{\pm0.0164} $} &  { $0.0442 _{\pm0.0258} $} \\ \hline
\end{tabular}
}
\end{table*}
 
\begin{table*}[!h] 
\renewcommand{\arraystretch}{1.7}
\caption{Experimental results on BC(hete) Dataset. The top result is highlighted in bold, and the runner-up is underlined.}     
\label{tab: BC_hete}
\centering 
\resizebox{\linewidth}{!}{ \huge
\begin{tabular}{@{}l|ccc| ccc | ccc| ccc @{}} 
        \hline
         & \multicolumn{6}{c|}{$\varepsilon_{average}$}  & \multicolumn{6}{c}{$\varepsilon_{individual}$}  \\ \hline
         & \multicolumn{3}{c|}{Within Sample}
         & \multicolumn{3}{c|}{Out-of Sample}
         & \multicolumn{3}{c|}{Within Sample}
         & \multicolumn{3}{c}{Out-of Sample}  \\ \hline
         Methods & AME & ASE  & ATE & AME & ASE  & ATE &    
         IME & ISE  & ITE &  IME & ISE  & ITE \\ \hline
        TARNET+z  & { $0.2538 _{\pm 0.1127} $} &  { $0.1657 _{\pm0.0563} $} &  { $0.3866 _{\pm0.0711} $} &  { $0.2619 _{\pm 0.1054} $} &  { $0.1701 _{\pm0.0594} $} &  { $0.4044 _{\pm0.1334} $} &  { $0.3455 _{\pm 0.0654} $} &  { $0.2122 _{\pm0.0558} $} &  { $0.4605 _{\pm0.0720} $} &  { $0.5590 _{\pm 0.2643} $} &  { $0.2207 _{\pm0.0510} $} &  { $0.6349 _{\pm0.3280} $}  \\ \hline
        CFR+z  & { $0.1580 _{\pm 0.0189} $} &  { $0.2071 _{\pm0.0237} $} &  { $0.4067 _{\pm0.0407} $} &  { $0.1559 _{\pm 0.0203} $} &  { $0.2076 _{\pm0.0245} $} &  { $0.4061 _{\pm0.0449} $} &  { $0.1825 _{\pm 0.0129} $} &  { $0.2092 _{\pm0.0233} $} &  { $0.4316 _{\pm0.0351} $} &  { $0.2058 _{\pm 0.0432} $} &  { $0.2098 _{\pm0.0242} $} &  { $0.4422 _{\pm0.0456} $} \\ \hline
        % alpha=10 
        GEst   & { $0.2734 _{\pm 0.1240} $} &  { $0.4257 _{\pm0.0973} $} &  { $0.2916 _{\pm0.1119} $} &  { $0.2722 _{\pm 0.1308} $} &  { $0.4277 _{\pm0.1012} $} &  { $0.2873 _{\pm0.1220} $} &  { $0.3334 _{\pm 0.1082} $} &  { $0.4592 _{\pm0.0934} $} &  { $0.3546 _{\pm0.0947} $} &  { $0.3832 _{\pm 0.1474} $} &  { $0.4612 _{\pm0.0972} $} &  { $0.3958 _{\pm0.1486} $}  \\ \hline
        ND+z  &  { $0.4124 _{\pm 0.0702} $} &  { $0.0451 _{\pm0.0201} $} &  { $0.1330 _{\pm0.0205} $} &  { $0.4111 _{\pm 0.0737} $} &  { $0.0486 _{\pm0.0206} $} &  { $0.1326 _{\pm0.0261} $} &  { $0.4226 _{\pm 0.0673} $} &  { $0.0562 _{\pm0.0146} $} &  { $0.1941 _{\pm0.0246} $} &  { $0.4330 _{\pm 0.0662} $} &  { $0.0666 _{\pm0.0137} $} &  { $0.2211 _{\pm0.0321} $} \\ \hline
        NetEst  & { $0.1643 _{\pm 0.1337} $} &  { $0.0450 _{\pm0.0180} $} &  { $0.0594 _{\pm0.0262} $} &  { $0.1857 _{\pm 0.1168} $} &  { $0.0405 _{\pm0.0252} $} &  { $0.0343 _{\pm0.0179} $} &  { $0.2199 _{\pm 0.1039} $} &  { $0.0667 _{\pm0.0171} $} &  { $0.1731 _{\pm0.0368} $} &  { $1.5595 _{\pm 2.5329} $} &  { $1.1347 _{\pm1.9028} $} &  { $1.0924 _{\pm1.7278} $}  \\ \hline
        % RRNet  && &  & \\ \hline
        % NDR  &&&& \\ \hline
        TNet  & { $0.1216 _{\pm 0.0864} $} &  { $0.0537 _{\pm0.0524} $} &  { $0.0429 _{\pm0.0301} $} &  { $0.1257 _{\pm 0.0727} $} &  { $0.0537 _{\pm0.0511} $} &  { $0.0481 _{\pm0.0269} $} &  { $0.1731 _{\pm 0.0450} $} &  { $0.0655 _{\pm0.0465} $} &  { $0.1458 _{\pm0.0175} $} &  { $0.1915 _{\pm 0.0542} $} &  { $0.0650 _{\pm0.0458} $} &  { $0.1740 _{\pm0.0621} $}  \\ \hline
        % Ours &   { $0.0854 _{\pm 0.0474} $} &  { $0.0329 _{\pm0.0276} $} &  { $0.0255 _{\pm0.0300} $} &  { $0.0744 _{\pm 0.0403} $} &  { $0.0287 _{\pm0.0287} $} &  { $0.0236 _{\pm0.0164} $} &  { $0.1416 _{\pm 0.0264} $} &  { $0.0561 _{\pm0.0245} $} &  { $0.1594 _{\pm0.0279} $} &  { $0.1308 _{\pm 0.0165} $} &  { $0.0516 _{\pm0.0220} $} &  { $0.1569 _{\pm0.0266} $}   \\ \hline
        Ours\_w/o\_IPM &  { $0.0706 _{\pm 0.0324} $} &  { $0.0278 _{\pm0.0079} $} &  { $0.0605 _{\pm0.0504} $} &  { $0.0755 _{\pm 0.0420} $} &  { $0.0294 _{\pm0.0052} $} &  { $0.0664 _{\pm0.0457} $} &  { $0.1240 _{\pm 0.0237} $} &  { $0.0491 _{\pm0.0060} $} &  { $0.1706 _{\pm0.0311} $} &  { $0.1273 _{\pm 0.0276} $} &  { $0.0481 _{\pm0.0054} $} &  { $0.1705 _{\pm0.0269} $}   \\ \hline
        Ours &  { $0.0625 _{\pm 0.0597} $} &  { $0.0195 _{\pm0.0162} $} &  { $0.0579 _{\pm0.0255} $} &  { $0.0604 _{\pm 0.0610} $} &  { $0.0195 _{\pm0.0130} $} &  { $0.0598 _{\pm0.0259} $} &  { $0.1175 _{\pm 0.0380} $} &  { $0.0373 _{\pm0.0099} $} &  { $0.1545 _{\pm0.0141} $} &  { $0.1172 _{\pm 0.0392} $} &  { $0.0365 _{\pm0.0074} $} &  { $0.1563 _{\pm0.0130} $}  \\ \hline
    \end{tabular}
    }
\end{table*}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
