\section{Related Work}
AI alignment is one of the major issues in learning trustworthy intelligent agents for real-world applications. It is difficult to represent various human preferences into a scalar reward function in existing Reinforcement Learning (RL) methods~\citep{russell2019human,dafoe2020open}. Meanwhile, the manually designed reward function, which might be misaligned with human preferences, often leads to undesired behaviors~\citep{leike2018scalable,krakovna2020specification}.
As a promising complement to RL, Human-in-the-loop Learning (HL) can overcome costly reward engineering and convey human intents to the learning process directly through human involvement.
Compared to imitation learning (IL)~\citep{ho2016generative,fu2018learning}, where the agent learns directly from high-quality human demonstration, HL methods benefit from interactive human involvement and feedback during the training, mitigating the possible distributional shift that usually happens when learning from offline data~\citep{ross2010efficient}.


\textbf{Preference-based RL.}
A large body of work focuses on learning human preference via ranking pair of trajectories generated by the learning agent~\citep{christiano2017deep,guan2021widening,reddy2018shared, warnell2018deep, sadigh2017active, palan2019learning,lee2021pebble,wang2021apple}.
InstructGPT~\citep{ouyang2022training} aligns language models by first supervised learning in demonstration and then finetuning by the reward learned from human preference feedback.
Preference learning can be applied to the tasks that human can not conduct, such as moving a six-legged Ant robot by assigning exact torque at each joint~\citep{christiano2017deep}.
For those tasks that human can demonstrate, these methods do not fully utilize real-time feedback from human subjects during agent-environment interaction.


\textbf{HL with Passive Human Involvement.}
Different from preference-based RL, human subjects can provide direct feedback to the learning agent during training through passive human involvement.
Some works learn policy from human-provided evaluative feedback, a Boolean flagging correct or wrong actions~
\citep{knox2012reinforcement,celemin2019interactive,najar2020interactively}. This is similar to the intervention in our framework. 
However, in \citep{najar2020interactively}, humans provide high-level instructions, e.g. pointing to the left/right, while in PVP humans provide intervention and low-level demonstrations.
The other line of work allows the neural policy to operate the robot and the human subjects can provide demonstration upon the requests from the learning agents~\citep{mandel2017add,menda2019ensembledagger,jonnavittula2021learning}.
The expert policy will intervene when uncertainty is huge, where the agent uncertainty is estimated by the variance of actions~\citep{menda2019ensembledagger}.
These methods reduce the cost of human resources but have potential risks to human subjects since they do not fully control the system.
For example, when human subjects use these algorithms to train autopilot AI, they are exposed to significant risks if they are in a self-driving cars due to unpredictable agent behaviors.


\textbf{Learning from Active Human Involvement.}
For safety-critical tasks such as autonomous driving, the safety of both the controlled vehicles and the human subjects is the top priority. 
There are many works that allow human subjects to proactively involve the agent-environment interactions based on their own judgment to ensure safety, which we call active human involvement.
Human subjects can terminate the episode if a near-accidental situation happens and such intervention policy can be learned~\citep{zhang2017query,abel2017agent,saunders2018trial,pakdamanian2021deeptake,xu2022look,wang2021appli}. 
Recent studies explore active human involvement methods through intervention and demonstration in the human-agent shared autonomy~\citep{macglashan2017interactive,menda2019ensembledagger,kelly2019hg,spencer2020learning,li2021efficient,jonnavittula2021learning,xu2022look}.
However, previous methods do not fully utilize the power of human involvement.
\revise{
COACH~\cite{macglashan2017interactive} treats human labels as indications of advantage instead of simply as reward. Compared to COACH, our method accepts not only the feedback (the intervention signal) but also the human demonstration. Our method does not consider the time delay of human subjects explicitly as COACH does.
}
Interactive imitation learning method (HG-DAgger)~\citep{kelly2019hg} does not leverage data collected by agents, while Intervention Weighted Regression (IWR)~\citep{mandlekar2020human} does not suppress undesired actions likely intervened by human.
Meanwhile, Expert Intervention Learning (EIL)~\citep{spencer2020learning} and IWR~\citep{mandlekar2020human} focus on optimizing actions step-wise without considering the temporal correlation between steps.
These drawbacks harm learning efficiency and thus incur more human involvement.
Moreover, previous methods lack experiments to demonstrate the generalizability to different task settings and human control devices.



% ==================================================