\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}

% \usepackage[numbers]{natbib}
% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
   % \usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


% ====================================================================
\usepackage{float}
\usepackage{listings}
\usepackage{multirow,multicol}
\usepackage{caption}
\usepackage{subcaption}
\definecolor{our_navy_blue}{RGB}{0, 110, 184}
\definecolor{purple}{RGB}{190, 0, 65}
\definecolor{LQY_color}{RGB}{50, 205, 50}

\newcommand{\bz}[1]{\textcolor{red}{[BZ: #1]}}
% \newcommand{\pzh}[1]{\textcolor{blue}{[P:#1]}}
% \newcommand{\mwj}[1]{\textcolor{purple}{[MWJ: #1]}}
% \newcommand{\revise}[1]{{\leavevmode\color{blue}#1}}
\newcommand{\revise}[1]{#1}
% \newcommand{\lqy}[1]{\textcolor{LQY_color}{[LQY: #1]}}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{lipsum}
\newcommand{\Tau}{\mathcal T}
\theoremstyle{definition}
\newtheorem{finding}{Finding}
\newtheorem{ourrule}{Goal}
\newtheorem{ourproblem}{Objective}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\ourstar}{\ding{72}}%
\newcommand{\things}{{\textbf{\textit{?}}}}%
\newcommand{\expect}{\mathop{\mathbb E}}%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{todonotes}
\usepackage{wrapfig}
\usepackage{enumitem}
% ====================================================================

\title{
Learning from Active Human Involvement through Proxy Value Propagation
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\author{
Zhenghao Peng$^\mathsection$, 
Wenjie Mo$^\mathsection$, 
Chenda Duan$^\mathsection$, 
Quanyi Li$^{\dagger}$, 
Bolei Zhou$^\mathsection$ \\
$^\mathsection$University of California, Los Angeles,
$^\dagger$University of Edinburgh
}



\begin{document}

\maketitle

\begin{abstract}
Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. \revise{The interaction and corrective feedback from human brings safety and AI alignment to the learning process.} In this work, we propose a new reward-free active human involvement method called \textit{Proxy Value Propagation} for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state-action pairs in the human demonstration are labeled with high values, while those agents' actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents' exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human-in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: \url{https://metadriverse.github.io/pvp}.
\end{abstract}



\section{Introduction}

% Human-in-the-loop approaches show great potential in machine learning community. 

Reinforcement learning (RL) has been successfully applied in many domains, ranging from board game Go~\citep{silver2016mastering}, strategy game StarCraft II~\citep{samvelyan2019starcraft}, autonomous driving~\citep{kendall2019learning}, and even nuclear fusion~\citep{degrave2022magnetic}. Existing RL methods assume the manually designed reward functions can fully express human intents and preferences. However, the resulting agents might exhibit biased, misguided, or undesired behaviors due to faulty reward functions~\citep{leike2018scalable,russell2019human,krakovna2020specification}. Moreover, the poor sample efficiency as well as the safety concern due to the trial-and-error exploration prevent the real-world deployment of RL.

Human-in-the-loop methods are promising to achieve alignment, learning efficiency, and safety.
% alleviate aforementioned issues can learn complex and safety-aware behaviors that are intractable to be encoded in the handcrafted reward function. 
% \pzh{We say RL is bad in first para, but suddenly we start talking human-in-the-loop policy learning. Seems need some connection.}
% \dcd{I think we can change the beginning sentence to something like "To address the xxx problems (such as faulty reward functions, biased behavior, etc), human-in-the-loop policy is often used."}
% \pzh{AI alignment can be achieved in other way! HL is an approach.}
% To achieve the alignment of human preference, 
Human-in-the-loop policy learning relies on human subjects to oversee the learning process of the autonomous agents, thus it can better align the learned behaviors with the preferences of humans compared with handcrafted reward functions. 
% \lqy{Is HL a good abbreviation?}
% 
Different forms of human involvement in human-in-the-loop policy learning have been studied over the years. Human subjects can advise actions upon the requests of the robots~\citep{mandel2017add} or provide preference-based feedback to assess the relative value of the collected trajectories~\citep{wirth2017survey,christiano2017deep,reddy2018shared, warnell2018deep, palan2019learning,guan2021widening,ouyang2022training}.
% \pzh{Is it good to attack InsturctGPT directly here?}
These methods learn from passive human involvement, where the human subjects do not provide real-time feedback and intervention during data collection.
For safety-critical tasks such as autonomous driving, safety is undoubtedly the first priority in human preference and the passive involvement methods yield unbounded risks in such settings.
% reducing the efficiency of the human-AI systems.
% \pzh{Do not say RLHF is bad. But say its not fit safety-critical. We hope the training is safe!}
An increasing body of works focuses on active human involvement, where human subjects actively intervene and provide demonstrations during the execution time~\citep{kelly2019hg,spencer2020learning,mandlekar2020human,li2021efficient}.
With online correction and demonstration from human subjects, AI alignment and training-time safety of the system can be substantially enhanced.


In this work, we focus on learning from active human involvement and develop a simple yet effective method that can turn a common value-based RL method into a reward-free human-in-the-loop method with minimal modification.
Our key insight is that we can learn a proxy value function from active human involvement, such that the proxy values encode human intents and guide policy learning to emulate human behaviors.
% \dcd{Should we emphasize the importance of reward-free here? We should talk about the superiority of reward-free, for example, people don't need to manually craft complex reward function, which might not be able to capture human intents as well. }
Specifically, we propose the \textit{Proxy Value Propagation (PVP)} method which labels high Q values to human actions and low Q values to agent actions that are intervened by the human subjects. The proxy values are then propagated to unlabeled state-action pairs in the agent's exploration through TD-learning. Value-based RL methods soon learn policies that align with human intents because of the value-maximization nature.
% RL policy thus tends to replicate human actions due to the value-maximizing nature. 
% Besides, through TD-learning, the proxy values will be propagated to these sample produced without human involvement automatically, greatly improving the sample efficiency. 
% \bz{This description sounds counter-intuitive. Human-involved states should be those risky states, shouldn't the agent avoid them? A more accurate description is needed. }
% \pzh{Here, we not only incentivize human actions and penalize un desired actions step-wise as in~\citep{spencer2020learning,mandlekar2020human}, but also broadcast the information that human-compatible behaviors are available in this state to prior states by propagating these proxy values through the Bellman backup in RL methods.}
% leading to a policy that performs action to reach human-involved states 
% the broadcast of the information ``preferable behaviors are demonstrated in these states''.
% Theoretic analysis indicates that our method can ensure training and test time safety if human's interventions and actions are mostly correct.
% We theoretically shows that the human intent compliance of the learned 
Experiments show that PVP can be successfully applied to both continuous and discrete action spaces, and achieve higher learning efficiency compared to baselines in various tasks, including driving in Grand Theft Auto V (GTA V). It is also compatible with different forms of human control devices, including gamepad, driving wheel, and keyboard.
We summarize our main contributions as follows:
\begin{enumerate}
[leftmargin=1em,topsep=0pt,label=\arabic*),itemsep=0em]
% \item We show that active human involvement is an effective supervision to train the agent.
% and theoretically prove the human intention com in training and testing time under this setting. \pzh{What the fuck}
\item We propose a simple yet effective method, Proxy Value Propagation, that can be integrated into existing RL algorithms to learn from active human involvement. Our method is reward-free and can be generalized across various task settings and human control devices.
\item The experiments show that the proposed PVP method enables superior performance and high learning efficiency in various tasks from the MiniGrid, MetaDrive, CARLA, to GTA V environment. User study further shows that PVP achieves better performance and is more user-friendly compared to other human-in-the-loop baselines.
% \lqy{Should we mention safety here?}
% \pzh{No. because it is a feature for all active involvement HL methods, not only for PVP.}
\end{enumerate}



% \begin{figure}[!t]
% \centering
% \includegraphics[width=\linewidth]{figs/RelatedWork.pdf}
% \caption{
% Different policy learning approaches. 
% \pzh{Landscape}
% \pzh{Maybe we should remove this.}
% % Right: The proposed Proxy Value Propagation (PVP) framework adopts Active Human Involvement.
% }
% \label{figure:teaser}
% \end{figure}




% ==================================================
\section{Related Work}

AI alignment is one of the major issues in learning trustworthy intelligent agents for real-world applications. It is difficult to represent various human preferences into a scalar reward function in existing Reinforcement Learning (RL) methods~\citep{russell2019human,dafoe2020open}. Meanwhile, the manually designed reward function, which might be misaligned with human preferences, often leads to undesired behaviors~\citep{leike2018scalable,krakovna2020specification}.
As a promising complement to RL, Human-in-the-loop Learning (HL) can overcome costly reward engineering and convey human intents to the learning process directly through human involvement.
Compared to imitation learning (IL)~\citep{ho2016generative,fu2018learning}, where the agent learns directly from high-quality human demonstration, HL methods benefit from interactive human involvement and feedback during the training, mitigating the possible distributional shift that usually happens when learning from offline data~\citep{ross2010efficient}.


\textbf{Preference-based RL.}
A large body of work focuses on learning human preference via ranking pair of trajectories generated by the learning agent~\citep{christiano2017deep,guan2021widening,reddy2018shared, warnell2018deep, sadigh2017active, palan2019learning,lee2021pebble,wang2021apple}.
InstructGPT~\citep{ouyang2022training} aligns language models by first supervised learning in demonstration and then finetuning by the reward learned from human preference feedback.
Preference learning can be applied to the tasks that human can not conduct, such as moving a six-legged Ant robot by assigning exact torque at each joint~\citep{christiano2017deep}.
For those tasks that human can demonstrate, these methods do not fully utilize real-time feedback from human subjects during agent-environment interaction.


\textbf{HL with Passive Human Involvement.}
Different from preference-based RL, human subjects can provide direct feedback to the learning agent during training through passive human involvement.
Some works learn policy from human-provided evaluative feedback, a Boolean flagging correct or wrong actions~
\citep{knox2012reinforcement,celemin2019interactive,najar2020interactively}. This is similar to the intervention in our framework. 
However, in \citep{najar2020interactively}, humans provide high-level instructions, e.g. pointing to the left/right, while in PVP humans provide intervention and low-level demonstrations.
The other line of work allows the neural policy to operate the robot and the human subjects can provide demonstration upon the requests from the learning agents~\citep{mandel2017add,menda2019ensembledagger,jonnavittula2021learning}.
The expert policy will intervene when uncertainty is huge, where the agent uncertainty is estimated by the variance of actions~\citep{menda2019ensembledagger}.
These methods reduce the cost of human resources but have potential risks to human subjects since they do not fully control the system.
For example, when human subjects use these algorithms to train autopilot AI, they are exposed to significant risks if they are in a self-driving cars due to unpredictable agent behaviors.


\textbf{Learning from Active Human Involvement.}
For safety-critical tasks such as autonomous driving, the safety of both the controlled vehicles and the human subjects is the top priority. 
There are many works that allow human subjects to proactively involve the agent-environment interactions based on their own judgment to ensure safety, which we call active human involvement.
Human subjects can terminate the episode if a near-accidental situation happens and such intervention policy can be learned~\citep{zhang2017query,abel2017agent,saunders2018trial,pakdamanian2021deeptake,xu2022look,wang2021appli}. 
Recent studies explore active human involvement methods through intervention and demonstration in the human-agent shared autonomy~\citep{macglashan2017interactive,menda2019ensembledagger,kelly2019hg,spencer2020learning,li2021efficient,jonnavittula2021learning,xu2022look}.
However, previous methods do not fully utilize the power of human involvement.
\revise{
COACH~\cite{macglashan2017interactive} treats human labels as indications of advantage instead of simply as reward. Compared to COACH, our method accepts not only the feedback (the intervention signal) but also the human demonstration. Our method does not consider the time delay of human subjects explicitly as COACH does.
}
Interactive imitation learning method (HG-DAgger)~\citep{kelly2019hg} does not leverage data collected by agents, while Intervention Weighted Regression (IWR)~\citep{mandlekar2020human} does not suppress undesired actions likely intervened by human.
Meanwhile, Expert Intervention Learning (EIL)~\citep{spencer2020learning} and IWR~\citep{mandlekar2020human} focus on optimizing actions step-wise without considering the temporal correlation between steps.
These drawbacks harm learning efficiency and thus incur more human involvement.
Moreover, previous methods lack experiments to demonstrate the generalizability to different task settings and human control devices.



% ==================================================
\section{Problem Formulation}
\label{section:problem-formulation}

Policy learning aims at finding a policy to solve the sequential decision-making problem, which is usually modeled by a Markov decision process~(MDP). 
MDP is defined by the tuple $M=\left\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, r, \gamma, d_{0}\right\rangle$ consisting of a state space $\mathcal{S}$, an action space $\mathcal{A}$, a state transition function $\mathcal{P}:\mathcal{S}\times\mathcal{A}\to\mathcal{S}$, a reward function $r: \mathcal{S}\times\mathcal{A}\to[R_{\min}, R_{\max}]$, a discount factor $\gamma\in(0,1)$, and an initial state distribution $d_0:\mathcal{S}\to[0,1]$.
The goal of conventional reinforcement learning is to learn a \textit{novice policy} $\pi_n(a | s): \mathcal{S}\times\mathcal{A}\to[0,1] $ that can maximize the expected cumulative return:
% \begin{equation}
$
\pi_n = \arg\max_{\pi_n} \expect_{\tau\sim P_{\pi_n}}[
\sum_{t=0}^{T} \gamma^{t} r(s_t, a_t)],
% \end{equation}
$
wherein $\tau = (s_0, a_0, ..., s_T, a_T)$ is the trajectory sampled from trajectory distribution $P_{\pi_n}$ induced by $\pi_n$, $d_0$ and $\mathcal P$.
Here $\pi_n$ defines a stochastic policy, while deterministic policy can be denoted as $\mu_n(s): \mathcal S\to \mathcal A$ and its action distribution is a Dirac delta distribution $\pi_n(a|s) = \delta(a - \mu_n(s))$.

The reward function imposes an assumption that the reward can fully reflect the intentions of the users and incentivize desired behaviors.
However, this assumption may not always hold and the learned agent may obtain biased behaviors or figure out the loophole to finish the task~\citep{leike2018scalable,russell2019human}.
Revisiting the primal goal when developing learning systems, we find the reward is not a necessity since what we really want to achieve is the realization of human preference in the learned behaviors and, as suggested by~\cite{russell2019human}, the ultimate source of information about human preferences are human behaviors. 

Imitation Learning (IL) methods directly learn $\pi_n$ from human behaviors. Assuming a human expert has a \textit{human policy} $\pi_h(a_h|s): \mathcal{S}\times\mathcal{A}\to[0,1] $, which outputs human action $a_h \in \mathcal A$. Note that human action shares the same action space as novice action.
IL learns from the trajectories generated by human policy $\tau_h \sim P_{\pi_h}$ and optimizes the novice policy to close the gap between $\tau_n~\sim P_{\pi_n}$ and $\tau_h$.
Instead of generating an offline dataset 
% $\{(s, a, s')\sim P_{\pi_h}\}$
and training novice policy against it~\cite{ho2016generative,fu2018learning}, we can incorporate a human subject into the loop of training for providing online data. This can mitigate the distributional shift since the data generated with human-in-the-loop has closer state distribution to that of the novice policy~\cite{ross2010efficient}.
This can be modeled by introducing an \textit{intervention policy} $I(\cdot|s, a_n)$ to describe human subjects' intervention behaviors.
In earlier methods such as DAgger~\citep{ross2010efficient}, the intervention policy is a Bernoulli distribution and the control authority switches back and forth between the novice and the expert. It is unrealistic to invite a real human subject to be involved in such training.
Later studies allow the human subjects to intervene and take full control~\citep{wang2018intervention,saunders2018trial,li2021efficient,xu2022look}, which we call such setting as \textit{learning from active human involvement}.
During training, a human subject accompanies the novice policy and can intervene with the agent by taking over the control to demonstrate desired behaviors.
% As discussed in Sec.~\ref{section:analysis}, we show that such active human involvement can ensure the alignment of human preference, \textit{e.g.} training time safety. 
The intervention policy can be considered as a deterministic policy denoted by $I(s, a_n): \mathcal S \times \mathcal A \to \{0, 1\}$ where $a_n \sim \pi_n(\cdot|s)$ is agent's action.
With notations above, the \textit{behavior policy} $\pi_b$ that generates actions during training is:
% \begin{equation}
% % & \text{(Stochastic novice):}
% % ~\\
%   % & 
%   \pi_b(a|s) 
%   =
%   (1 - I(s, a)) \pi_n(a|s)
%   +
%   \pi_h(a|s) \int_{a'\in \mathcal A} I(s, a') \pi_n(a' | s)da'
%   % \quad
% % ~\\
% \end{equation}
\begin{equation}
% & \text{(Deterministic novice):}
% ~\\
  % & 
  \pi_b(a|s) 
  = 
  (1 - I(s, \mu_n(s)))\delta(a - \mu_n(s))
  + 
  I(s, \mu_n(s)){\pi_h}(a|s).
  % \quad
\end{equation}



With such a model of active human involvement, we can now formulate our objectives. 

\textbf{Task-specified metrics.}
Our primal goal is to find novice agents whose behaviors are well-aligned with human preferences.
%Human preferences are very tricky to be formulated. 
In this work, we inform the human subjects of the primal goal of the tasks, \textit{e.g.} navigating to the destination in driving tasks. They are also aware of how task-specified metrics, such as success rate and route completion provided by the test environments,  are computed. These metrics serve as a proxy for human preferences in evaluating trained agents' performance. Unlike prior work where these metrics were used as rewards, our learning agent cannot access them. The only supervision sources in our method are human interventions, $I(s, a)$, and demonstrations, $a_h \sim \pi_h(\cdot|s)$.

\textbf{Preference Alignment.}
In our method, humans can intervene at any time. Most interventions occur in near-accidental situations or when agents are performing poorly. Conversely, lack of intervention indicates alignment with human preferences. Hence, another goal is to develop a novice policy that minimizes human interventions during shared control. 
In the next section, we will discuss our insights and how we build a concise, general, and efficient learning method to achieve these objectives.


% \vspace{-0.5em}
\section{Method}
% \vspace{-0.5em}

We propose the \textit{Proxy Value Propagation (PVP)} method which can transform a value-based RL method into an efficient reward-free human-in-the-loop policy optimization method that learns from active human involvement.
PVP is compatible with various task settings, such as continuous and discrete action spaces, as well as various human control devices. 
In this section, we first summarize the basic workflow of value-based RL before introducing the motivation and the design of PVP. We then describe the implementation details.


\textbf{Value-based RL:}
\revise{
The proposed human-in-the-loop method results from the minimum modification of existing reinforcement learning methods. Thus, we briefly introduce the background of related methods.}
Value-based RL optimizes the value function and policy iteratively.
On the value function side, we denote the state-action value and state value of policy $\pi$ as 
$Q(s,a)=\mathbb{E} \left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right)\right]$
and $V(s)=\mathbb{E}_{a\sim\pi(\cdot|s)}Q(s,a)$, respectively. 
A neural network is commonly used to estimate the value function with Bellman backup:
% \begin{equation}
$Q(s, a) \gets r(s, a) + \gamma \max_{a'} Q(s', a')$,
% \end{equation}
where $s'$ is the next state. 
To learn the value network $Q_\theta$ parameterized by $\theta$, stochastic gradient descent on the temporal difference (TD) loss is conducted
$
    J^\text{TD}(\theta) = \expect_{(s, a, s')} | Q_\theta(s, a) - (r(s, a) + \gamma \max_{a'} Q_{\hat{\theta}}(s', a')) |^2,
$
where $Q_{\hat{\theta}}$ can be a delay-updated target network.
In this work, we adopt the TD learning in the \textbf{reward-free} setting. Remove the reward in the TD loss, the TD loss becomes:
\begin{equation}
\label{eq:td-learning}
    J^\text{TD}(\theta) = \expect_{(s, a, s')} | Q_\theta(s, a) -  \gamma \max_{a'} Q_{\hat{\theta}}(s', a') |^2.
\end{equation}
On the policy side, based on the learned value function, the deterministic policy $\mu_{n}$ parameterized by $\phi$ can be learned by maximizing the Q values:
$J(\phi) = \expect_{s} Q(s, \mu_{n}(s; \phi))$.
The optimal policy is expected to maximize Q values:
\begin{equation}
\label{equation:expected-novice-policy}
\mu_n(s) = \arg\max_{a} Q(s, a).
\end{equation}



\begin{figure}[!t ]
% \begin{wrapfigure}{r}{0.55\textwidth}
% \vspace{-4em}
\centering
\includegraphics[width=\linewidth]{figs/Framework-H.pdf}
\vspace{-0.2em}
\caption{
Illustration of Proxy Value Propagation. 
% Human and the learning policy copilot the agent. 
\textbf{(A)} Human oversees the agent's trial-and-error exploration with the environment. When the human subject does not intervene, the transitions will be recorded into the novice buffer $\mathcal B_n$. 
\textbf{(B)} When the human intervenes, both novice action $a_n$ and human action $a_h$ will be recorded into the human buffer $\mathcal B_h$ but only the human action will be applied to the environment.
\textbf{(C)} In training, we use the human buffer to compute proxy value loss and propagate the human intent knowledge to all transitions via TD loss without access to the reward.
}
\label{figure:framework}
% \end{wrapfigure}
\vspace{-1em}
\end{figure}


\vspace{-0.8em}
\subsection{Proxy Value Propagation}
\vspace{-0.2em}

We illustrate the active human involvement of PVP in Fig.~\ref{figure:framework}. During training, the human subject supervises the agent-environment interactions (Fig.~\ref{figure:framework} \textbf{A}). Those exploratory transitions by the agent are stored in the Novice Buffer $\mathcal B_{n} = \{(s, a_n, s')\}$. At any time, the human subject can intervene the free exploration of the agent by pressing a button in the control device (Fig.~\ref{figure:framework} \textbf{B}). While pressing the button, the human takes over the control and provides a demonstration of how to behave. During human involvement, both human and novice actions will be recorded into the Human Buffer $\mathcal B_{h} = \{(s, a_n, a_h, s')\}$. Concurrently with the human-agent shared control, our method keeps updating the novice policy by the novel Proxy Value Propagation mechanism (Fig.~\ref{figure:framework} \textbf{C}), which will be discussed later.


In the shared human-agent control, human intervention serves as a distinct indicator of suboptimal agent performance, which could result from the agent executing perilous actions or exhibiting ineffective behaviors. Thus, the optimal policy learned by the agent should (1) strive to approximate the behaviors demonstrated by the human subjects and (2) avoid performing actions that are intervened by humans.


The key insight of this work is that we can manipulate the Q values to induce desired behaviors, given that value-based RL has the nature to seek value-maximizing policy as Eq.~\ref{equation:expected-novice-policy}.
As shown in Fig.~\ref{figure:framework} \textbf{C}, for emulating human behavior and minimizing intervention, we sample data $(s, a_n, a_h)$ from the human buffer and label the Q value of the human action $a_h$ with $+1$ and the novice action $a_n$ with $-1$. This is achieved by fitting the Q network directly with PV loss:
\begin{equation}
\label{equation:proxy-q-labeling}
    J^\text{PV}(\theta) = 
    \expect_{(s, a_n, a_h)}  [| Q_\theta (s, a_h) - 1 |^2  + | Q_\theta (s, a_n) + 1 |^2 ] I(s, a_n).
\end{equation}
The transitions in the novice buffer are not intervened by the human subject, meaning they are aligned with human preferences. Meanwhile, those transitions also contain information of the forward dynamics~\citep{levine2020offline,yu2022leverage}. 
To exploit the information contained in these transitions, instead of discarding these data as in~\citep{kelly2019hg}, we propagate the proxy values to these states via TD learning in Eq.~\ref{eq:td-learning} and use those transitions together with those human-involved transitions for the policy learning. 
% This implies that high values should also be assigned to state-action pairs leading the agent to demonstrated trajectories. 
% After that, data stored in novice buffer will also be labeled automatically. 
% Canceling the the reward term in $J^\text{TD}(Q)$, 
The final value loss is evaluated as follows:  
\begin{equation}
\begin{aligned}
\label{eq:pvp-main-loss}
J(\theta) = J^\text{PV}(\theta) + J^\text{TD}(\theta) 
 = &
\expect_{(s, a_n, a_h)\sim\mathcal B_h}  [| Q_\theta(s, a_h) - 1 |^2  + | Q_\theta(s, a_n) + 1 |^2 ] I(s, a_n)
\\
& +  \expect_{(s, a, s')\sim\mathcal B_h \bigcup \mathcal B_n} | Q_\theta(s, a) -  \gamma \max_{a'} Q_{\hat{\theta}}(s', a') |^2
\end{aligned}
\end{equation}
Then we follow the policy update process outlined in the base RL methods.

\subsection{Analysis}
\label{section:analysis}

% \bz{Good to give a bolded subtitle for each part. You have two parts here, the first part probably can be called \textbf{Connection to CQL}, the other is 'Alternative to reward or what? you come up with a better name to summarize this part' }

\textbf{Connection to CQL.} 
The proposed PVP method can be interpreted as adopting the Conservative Q-Learning (CQL)~\cite{kumar2020conservative} objective for reward-free and online learning settings. 
It augments the CQL objective with an extra L2 regularization term imposed on the Q-values for human-involved transitions.
In our online learning setting, Eq.~\ref{eq:pvp-main-loss} can be reformulated as:
\begin{equation}
J(\theta) = 
\expect_{\mathcal B_h,  I(s, a_n) = 1} [ 
\underbrace{Q_\theta^2(s, a_n) + Q_\theta^2(s, a_h)}_{\text{L2 Regularization on Q}} + 2 + \underbrace{2(Q_\theta(s, a_n) - Q_\theta(s, a_h))] + \text{TD loss.} }_{\text{CQL Loss}}
\end{equation}
CQL was originally proposed to mitigate the problem of overestimated Q-values in offline RL settings. These overestimations often lead to suboptimal policies due to the optimistic selection of actions with misleadingly high values. In our work, we deal with human actions and novice actions sampled from two different distributions, where overestimation might also occur. However, unlike CQL, PVP does not have access to a reward function, meaning the Q-values are not grounded in an estimation of true values. The additional L2 regularizer therefore serves to impose constraints on the Q-values, helping to prevent unbounded growth and potential overfitting.
In Sec.~\ref{section:ablation}, we compare the learned proxy Q-values under both CQL and PVP objectives. Our results indicate that human and agent actions are more distinguishable when learned through PVP.

\revise{
\textbf{Alternative to Reward Assignment.}
On the other hand, a more straightforward idea than PVP is to assign a reward of +1 to human actions and -1 to agent actions during intervention. Unfortunately, it is not practical since the Bellman backup is conducted on the transition triplet $(s, a, s')$, where one has to use future states' values to estimate current values. Therefore, the reward must correspond to the action from the behavior policy, the action $a$ causes the transitions from $s$ to $s'$.
In our context, during involvement, the action $a = a_h$ must come from human policy as the human subject is taking control. 
Though we can assign $+1$ reward and compute value target in those human-involving transitions, we have no way to assign $-1$ to the agent's actions because we don't know the next states caused by those actions and thus we can't compute the value target. It is also not practical to query the environment to get the next state $s''\sim \mathcal P(s, a_n)$ as the $a_n$ is a potentially danger or undesired action and replaying it in the real-world environment is not feasible.
In our preliminary experiment, we find the policy fails to learn anything regardless of the amount of human involvement provided. This is because the reward will be $+1$ for all the human-involving transitions and the learning agent will find a pitfall to maximize its rewards: it always demonstrates undesired behaviors so that humans will always take control, which yields a $+1$ reward.
}


\subsection{Implementation Details}
\label{sec:impl-details}

\textbf{Base RL Methods.}
Our method can be implemented for both continuous and discrete action spaces by extending TD3~\citep{fujimoto2018addressing} and DQN~\citep{mnih2015human} with PV loss and the balanced buffer.
While TD3 uses a deterministic policy, DQN adopts epsilon-greedy exploration that makes the policy stochastic. We remove the action noise in DQN and simply follow the argmax rule to select actions.
% $\mu(s) = \arg\max_{a} Q(s, a)$. 
Therefore, our method enjoys deterministic novice policy in both cases. The primary reason is that according to the feedback of human subjects, stochastic novice makes human subjects experience excessive fatigue due to the difficulty in monitoring and correcting agents’ noisy actions. This design choice makes our method more user-friendly, as shown in the user study in Sec.~\ref{section:user-study}.

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{figs/MainExp_compressed.pdf}
\vspace{-0.5em}
\caption{
Evaluation of PVP under four different environments with human control devices. For each environment, we plot the test-time performance curve of the agent trained by the proposed PVP and the RL counterpart TD3. The x-coordinate is the total number of environment interactions, which indicates the time steps the training agent (in RL method) or the human-agent system (in our method) experiences during training. 
Compared to the RL counterpart, the proposed method achieves much higher performance with superior learning efficiency.
}
\vspace{-0.5em}
\label{figure:main-result}
\end{figure*}

\textbf{Balanced Buffers.}
The intervention gradually becomes sparse as the agent learns to reduce human intervention. However, those sparse intervention signals contain even more important information on how to behave under critical situations.
Previous method~\citep{li2021efficient} stores agent data and human data into one buffer and samples them uniformly. 
Abusing the notations, the ratio between the transitions from the agent's exploration and from human involvement is $|\mathcal B_n|:|\mathcal B_h|$ in each SGD batch.
The human demonstrations are overwhelmed by the amount of agent-generated trajectories, leading to inefficient learning of those critical human behaviors and even catastrophic forgetting.
For example, the driving policy sometimes fails to master acceleration at the beginning of an episode, even though the human subject has already demonstrated the expected maneuver multiple times. This is because the demonstration of initial acceleration only lasts a short period of time and thus is scarce in the buffer.
To address this issue, we balance the transitions coming from the human buffer and the novice buffer.
% two replay buffers to store transitions with or without human intervention.
% The human buffer $\mathcal B_{h} = \{(s, a_n, a_h, s')\}$ stores data during human involvement and the novice buffer $\mathcal B_{n} = \{(s, a_n, s') \}$ stores the transitions during agent's independent exploration.
In each training iteration, we sample two equally-sized batches $b_n$ and $b_h$ from $\mathcal B_n$ and $\mathcal B_h$ respectively, 
each has $N/2$ samples. $N$ is the batch size for the policy update.
% We compute PV loss $J^{\text{PV}}$ use $b_h$ and concatenate $b_n$ and $b_h$ to form a batch with $N$ transitions to compute the TD loss $J^\text{TD}$. 
By concatenating $b_n$ and $b_h$, our method can balance the data from the human's demonstration and from the agent's exploration, and hence the ratio between two types of data in each SGD batch keeps $1:1$. Therefore, in the initial acceleration example above, the balanced buffer recalls the acceleration behavior, preventing catastrophic forgetting. 



\begin{table*}[!t]
\centering
\begin{small}
\caption{
Comparison of different approaches in MetaDrive-Keyboard. The overall intervention rate is given besides the human data usage. 
% \pzh{This table has wrong reference because it stops a citation at the end of last page. Fix this!}
}
\label{tab:comparing-hl-methods}
\vspace{-0.5em}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{4}*{Method}
&
\multicolumn{3}{c}{Training}
&
\multicolumn{3}{c}{Testing}
\\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
%\cline{6-8}
& 
\multirow{3}*{ \shortstack{Human\\Data\\Usage} }
&
\multirow{3}*{ \shortstack{Total\\Data\\Usage}  }
&
\multirow{3}*{ \shortstack{Total\\Safety\\Cost} }
&
\multirow{3}*{ \shortstack{Episodic\\Return} } & 
\multirow{3}*{ \shortstack{Episodic\\Safety\\Cost} } & 
\multirow{3}*{ \shortstack{Success\\Rate} }
~\\
~\\
~\\
\midrule
%\hline
SAC & - & 1M  & 2.76K {\tiny $\pm$ 0.95K }  & {386.77} 	{\tiny $\pm$35.1} &	0.73 	{\tiny $\pm$1.18} &	0.82 	{\tiny $\pm$0.18} \\ 
PPO & - & 1M  & 24.34K {\tiny $\pm$3.56K} & 335.39 {\tiny $\pm$12.41}  &	3.41 {\tiny $\pm$1.11}  &	0.69{\tiny $\pm$0.08}   \\
TD3 & - & 1M  & 1.74K {\tiny $\pm$ 0.62K} & 318.12 {\tiny $\pm$21.9} & 0.47 {\tiny $\pm$0.08} & 0.70 {\tiny $\pm$0.09} \\
% TD3-2M & -                & 2M                                    & 4.08K                            & 300.7                   & 0.82                     & 0.56                 \\
\midrule
SAC-Lag & - & 1M & 1.84K {\tiny $\pm$ 0.49K} & 351.96	{\tiny $\pm$101.88} &	{0.72} 	{\tiny $\pm$0.49} &	0.73 	{\tiny $\pm$0.29} \\
PPO-Lag & - & 1M & 11.64K {\tiny $\pm$ 4.16K} & 299.99 	{\tiny $\pm$49.46} &	1.18 {\tiny $\pm$0.83} &	0.51 	{\tiny $\pm$0.17} \\
CPO & - & 1M & 4.36K  {\tiny $\pm$2.22K} & 194.06 {\tiny $\pm$108.86} & 1.71 {\tiny $\pm$1.02} & 0.21 {\tiny $\pm$0.29} \\
\midrule
\shortstack{Human Demo.}
 & 30K              & -                                         & 39                                 & 347.523                 & 0.39                     & 0.97                 \\
\midrule
BC                  & 30K (1.0)             & -                                    & -                                  & 113.32   {\tiny $\pm$10.21}              & 2.171  {\tiny $\pm$0.65}                  & 0.073 {\tiny $\pm$0.02}      
\\
GAIL & 30K (0.015) & 2M  & 25.90K  {\tiny $\pm$ 8.15K}  & 81.51  {\tiny $\pm$ 9.43} & 1.308  {\tiny $\pm$ 0.23}  & 0.0 {\tiny $\pm$ 0.0} 
\\
\midrule
HG-Dagger           & 39.0K (0.76)           & 51K                                    & 56                                 & 116.393                 & 1.979                    & 0.045                \\
IWR                 & 35.8K (0.79)            & 45K                                   & 52                                 & 226.221                 & 1.457                    & 0.465                \\
HACO                & 19.2K (0.48)           & 40K                                  & 130                                 & 143.287                 & 1.645                    & 0.139                \\
\midrule 
% PVP w/o TD          & 13.5K (0.34)           & 40.5K                                 & 70                                 & 252.447 & 1.277 & 0.220   \\ 
{PVP w/o TD}          & 13.5K (0.34)           & 40.5K                                 & 70                                 & 252.447 & 1.277 & 0.220   \\ 
{PVP w/ Reward} &
12.8K (0.32) &
40K &
30 &
319.383 &
0.767 &
0.755 ~\\
PVP (Ours)          & 14.6K (0.37)           & 40K                                 & 76.8 {\tiny $\pm$9.3}                              & 353.636 {\tiny $\pm$23.7}                 & 0.898 {\tiny $\pm$0.15}                   & 0.857 {\tiny $\pm$0.04}               \\
\bottomrule
\end{tabular}
\end{small}
\vspace{-0.5em}
\end{table*}




% ==================================================
\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setting}

\textbf{Tasks.}
We conduct experiments on various control tasks with different observation and action spaces.
For continuous action space, we use three driving environments, MetaDrive safety benchmark~\citep{li2021metadrive}, CARLA Town01~\citep{Dosovitskiy17}, and a customized driving environment built upon Grand Theft Auto V (GTA V), a popular video game.
% \bz{I don't think it is worth emphasizing this, reviewer might come up with more questions on this. you can just summarize the environment setting plainly:
% Note that to the best of our knowledge, our GTA V driving environment is the first open-sourced human-in-the-loop compatible policy learning environment that builds upon the realistic video game GTA V with full set of well-designed observation, reward (for RL training) and termination conditions.}
In these tasks, the agent needs to steer the target vehicle with low-level acceleration, braking, and steering, to reach its destination. Specifically, in MetaDrive safety environments, the agent needs to avoid any crash in the heavy-traffic scene with normal vehicles, obstacles, and parked vehicles. In MetaDrive, there exists a split of training and test environments, and we present the performance of the learned agent in a held-out test environment.
To examine our method with different observation modalities, we use the sensory state vector in MetaDrive and GTA V and the bird-eye view image in CARLA as observation.
For discrete action space, we use MiniGrid Two Room task~\citep{gym_minigrid}, which involves agent exploration such as moving toward a door and opening the door before reaching the destination. The observation of MiniGrid is the semantic map of the agent's local neighborhood. 
% MLP is used for MetaDrive task and CNN is used for others as the feature extractors.
Please refer to Appendix~\ref{section:environment-details} for more information about the environment setup.

\textbf{Evaluation Metrics.}
In MetaDrive safety benchmark, we report \textit{total safety cost} as the number of crashes during training, which reflects the number of potential dangers exposed to the human subject during training. We also report \textit{episodic return}, \textit{episodic safety cost}, and \textit{success rate} as the test performance of the agents. Episodic safety cost is the average number of crashes in one episode. The success rate is the ratio of episodes in which agents reach the destination to the total test episodes.
In CARLA, we report \textit{route completion} and \textit{success rate}. Route completion is the ratio of the traveled distance to the length of the complete route. GTA V uses \textit{route completion} and MiniGrid uses \textit{success rate} to measure the performance.
% MiniGrid performance is measured by \textit{success rate}.
Except for total safety cost, the aforementioned metrics measure the test-time performance, which is tested when the agent runs independently without human involvement.
For human-in-the-loop experiments, we also report the total number of human-involved transitions (\textit{human data usage}) and the \textit{overall intervention rate}, which is the ratio of human data usage to total data usage.
These show how much effort humans make to teach the agents. We also design a user study to measure the experience of human subjects in Sec.~\ref{section:user-study}.







\textbf{Human Interfaces.}
To examine the generalizability of our method, we leverage multiple control devices: Xbox Wireless Controller (Gamepad), keyboard, and Logitech G29 Racing Wheel.
We denote the MetaDrive tasks with three devices as MetaDrive-Gamepad/Keyboard/Wheel.
As shown in Fig.~\ref{figure:main-result}, human subjects can takeover through control devices and monitor the training process through the visualization of environments on the screen.
The Ethics statement is provided in Appendix~\ref{section:ablation-ethics-statement}.



\textbf{Experimental Details.} We implement most of the code with Stable-Baselines3~\citep{stablebaselines3}. 
Training results of various baselines in MetaDrive tasks are obtained from the open-source code by~\citep{li2021efficient}.
% Baselines HACO and IWR are from open-sourced repository of~\citep{li2021efficient}. 
% Except human-in-the-loop experiments, 
The RL baselines are repeated 5 times with different random seeds, while other human-in-the-loop methods are repeated fewer times due to limited human resources. 
In the training of the human-in-the-loop methods, a real human subject participates in each experiment and we do not use any simulated user input. 
During testing, there is no form of human involvement. 
For each experiment, we evaluate each checkpoint in the environment for multiple runs and report the average task-specified metrics as the performance of this checkpoint. We report the performance of the best checkpoint as the result of the experiment.
We provide the standard deviation if the experiments are repeated multiple runs in tables and figures.
All experiments with humans are conducted on a local computer with an Nvidia GeForce RTX 3080. The local computer can support real-time simulation and training. Hyper-parameters and other details are given in Appendix~\ref{section:environment-details} and \ref{section:appendix-hyper-parameters}. 

% \pzh{\textbf{Help me with that:}}
% That is, the simulation frequency during human-agent shared control is greater than the simulation
% In MetaDrive and CARLA, the physics simulation is run at 10Hz in the virtual world and in GTA V the frequency is 30Hz.
% After each environment interaction, PVP updates its policy once, inferring and back-propagating one SGD batch.
% Our experience suggests that the local computer can effortlessly support concurrent running of the simulation with human-agent shared control as well as the background policy update at the frequency in wall-time higher than the frequency in simulation. We will limit the FPS to the simulation frequency so that the human subjects experience realistic time-elapse.



\textbf{Baselines.}
We test four native RL baselines: PPO~\citep{schulman2017proximal}, SAC~\citep{haarnoja2018soft}, TD3~\citep{fujimoto2018addressing} and DQN~\citep{mnih2015human}. 
% RL baselines can access environmental reward.
We also test three safe RL baselines: Constraint Policy Optimization (CPO)~\citep{achiam2017constrained}, PPO-Lagrangian~\citep{stooke2020responsive}, SAC-Lagrangian~\citep{ha2020learning}. In all baselines above, the reward function and cost function (for MetaDrive Safety Benchmark) are defined by the environment and can be accessed by the agents.
We also test IL methods Behavior Cloning (BC) and GAIL~\citep{ho2016generative}.
Human-in-the-loop methods that learn from active human involvement are tested: Human-Gated DAgger (HG-DAgger)~\citep{kelly2019hg}, Intervention Weighted Regression (IWR)~\citep{mandlekar2020human} and Human-AI Copilot Optimization (HACO)~\citep{li2021efficient}.


% \textbf{Demo Video} and \textbf{Code} are in the supplementary materials.



% \begin{wraptable}{r}{0.5\textwidth}
% \caption{A wrapped table going nicely inside the text.}\label{wrap-tab:1}
% \begin{tabular}{ccc}\\\toprule  
% Header-1 & Header-1 & Header-1 \\\midrule
% 2 &3 & 5\\  \midrule
% 2 &3 & 5\\  \midrule
% 2 &3 & 5\\  \bottomrule
% \end{tabular}
% \end{wraptable} 



\begin{table}[!t]
\centering
\begin{minipage}[!b]{0.5\linewidth}
\centering
\begin{small}
\caption{
Results of different approaches in CARLA.
}
\label{tab:carla-exp}
\begin{tabular}{@{}ccccc@{}}
\toprule 
Method	&
 \shortstack{Human\\Data}
 & 
\shortstack{Total\\Data}
% &  \shortstack{Episodic\\Return}
& \shortstack{Route\\ Completion} & \shortstack{Success\\ Rate}\\
% Experiment &  \shortstack{Episodic\\Return} &  \shortstack{Episodic\\Cost} & \shortstack{Success\\Rate}  \\
\toprule
% PPO (Old HACO paper) & - & 500K & ? & 0.35 ?? ~\\
PPO & - & 1M & 0.24 {\tiny $\pm$ 0.013} & 0.0 {\tiny $\pm$  0.0 } \\
TD3 &
- &
1M & % data
% 43.46 {\tiny $\pm$ 12.83} & % return
0.11 {\tiny $\pm$  0.05 } & % rc
0.0 {\tiny $\pm$  0.0 }% succ
\\
\midrule
BC & 5K &  - & 0.42 {\tiny $\pm$ 0.08} & 0.20 {\tiny $\pm$ 0.10}  \\
% GAIL & ?? & & \pzh{} \\
\midrule
HG-DAgger & 6.8K & 24K & 0.64 & 0.47 \\
% IWR & 5.7K & 24K & 0.80 & 0.67 \\
% PZH: 2023-01-24 An inferior IWR result:
IWR & 5.7K & 24K & 0.69 & 0.60 \\
% HACO (Old) & 23K & 0.25 & 0.11 ~\\
% PZH: 2023-01-20
HACO &
4.8K &
24K &
0.52 &
0.40
\\
\midrule
PVP (Ours) &	
6.6K &
24K &
0.92  {\tiny $\pm$ 0.05} & % rc
0.73  {\tiny $\pm$ 0.08} % succ
\\
\bottomrule
\end{tabular}%
\end{small}
\end{minipage}
\hfill
\begin{minipage}[!b]{0.44\linewidth}
\centering
\includegraphics[width=0.95\linewidth]{figs/Trajectory-HACO.pdf}
\includegraphics[width=0.95\linewidth]{figs/Trajectory-PVP.pdf}
% \caption{
\captionof{figure}{
We visualize the action sequences generated by HACO and PVP agents in the same MetaDrive map who are trained to 40K steps. PVP has much smoother actions.
}
\label{figure:trajectory-main}
\end{minipage}
\vspace{-0.5em}
\end{table}


\subsection{Baseline Comparison}
\label{section:baseline-comparison}


\textbf{Comparing with RL Counterparts.}
Fig.~\ref{figure:main-result} shows the curves of test-time performance.  
In MetaDrive-Gamepad, our method achieves 350 returns in 37K steps. This takes about one hour in the real-world HL experiment. TD3 baseline fails to achieve comparable results even after 300K steps of training.
In CARLA, PVP agents learn to drive within 30 minutes with our method, while TD3 cannot solve the task.
In GTA V, PVP can solve the task with 1.2K human data usage and 20K total data usage. The whole experiment takes only 16 minutes. TD3 instead utilizes 300K steps to achieve similar performance.
In MiniGrid tasks, our method successfully solves the tasks while vanilla DQN fails, showing that PVP can learn an exploratory solution and can be incorporated into discrete action space. We also show experiments on one easier and one harder MiniGrid environment in Appendix~\ref{section:appendix-extra-results}, where PVP greatly improves learning efficiency.

\begin{table}[!t]
\centering
\begin{minipage}[!b]{0.5\linewidth}
\centering
\begin{small}
\caption{
User study result. The maximum score for each item is 5.
}
\label{tab:user-study}
\begin{tabular}{@{}lllll@{}}
\toprule
 & HG-DAgger   & IWR    & HACO  & PVP     \\ 
 \midrule
Compliance      & 3.0 {\tiny $\pm$ 0.8} & 4.0{\tiny $\pm$ 0.8 } & 3.0 {\tiny $\pm$ 0.2 } & {4.8} {\tiny $\pm$ 0.5 } \\
Performance     & 2.2 {\tiny $\pm$ 1.0 }                                         & 3.7 {\tiny $\pm$ 0.9 }                                         & 3.3 {\tiny $\pm$ 0.9 }                                         & {4.8} {\tiny $\pm$ 0.5 } \\
% Comfort & 3.8 {\tiny $\pm$ 0.5 }                                         & 4.3 {\tiny $\pm$ 0.5 }                                         & 2.3 {\tiny $\pm$ 0.6 }                                         & \textbf{5} {\tiny $\pm$ 0.0 }     \\
% Fatigue         & 3.2 {\tiny $\pm$ 0.9 }                                         & 4.5 {\tiny $\pm$ 0.5 }                                          & 2.3 {\tiny $\pm$ 0.9 }                                         & \textbf{4.7} {\tiny $\pm$ 0.6 } \\ \bottomrule
Stress         & 3.2 {\tiny $\pm$ 0.9 }                                         & 4.5 {\tiny $\pm$ 0.5 }                                          & 2.3 {\tiny $\pm$ 0.9 }                                         & {4.7} {\tiny $\pm$ 0.6 } \\ \bottomrule
\end{tabular}
\end{small}
\end{minipage}
\hfill
\begin{minipage}[!b]{0.45\linewidth}
\centering
\includegraphics[width=0.42\linewidth]{figs/q-haco-small.pdf}
\includegraphics[width=0.42\linewidth]{figs/q-pvl-small.pdf}
\captionof{figure}{
Evolution of proxy values.
}
\label{figure:evolution-values}
\end{minipage}
\vspace{-1em}
\end{table}

\textbf{Comparing with Human-in-the-loop Baselines.}
Table~\ref{tab:comparing-hl-methods} suggests all tested HL methods achieve extremely low safety violations in training compared to vanilla RL and Safe RL methods, empirically supporting the preference alignment of the active human involvement, if we consider human preference is to avoid safety violation. 
Compared to other human-in-the-loop methods, our method costs the lowest human efforts in terms of human data usage and overall intervention rate, while greatly outperforming baselines in testing performance. Since MetaDrive has a training and test environment split, the result suggests PVP can learn high-quality agents with generalizability.
Similar results are shown in CARLA in Table~\ref{tab:carla-exp}. 
Compared to RL baselines, HL methods achieve decent success rates and route completion rates even with only 24K environmental interactions.
Compared to HL baselines, PVP achieves the best route completion rate.

\textbf{Visualization.}
In Fig.~\ref{figure:trajectory-main}, we visualize the action sequences of the agents trained by PVP and a human-agent shared control baseline HACO~\cite{li2021efficient}. The angle and length of each arrow represent the steering and acceleration, respectively. The human subject's actions are marked with yellow.
Compared to HACO method, PVP agent produces smoother actions, which explains its high user study scores shown in the next section.




\subsection{User Study}
\label{section:user-study}


We design a user study questionnaire to assess the experience of human subjects. Details are provided in the human subject research protocol in Appendix~\ref{appendix:user-study}. Three aspects are considered:
(1) \textbf{Compliance} measures whether the behaviors of the agent satisfy human intents.
For example, a highly compliant agent behaves like human such that the human subjects feel like they are completing objectives by themselves.
(2) \textbf{Performance} is the subjective evaluation from human subjects on whether the agent can solve the primal task, e.g. driving to the destination in navigation tasks. This score should be low if the agent cannot learn a particular behavior or forgets it even though human subjects have taught the agent multiple times.
(3)
\textbf{Stress} gauges the cognitive cost human subjects pay to train the agent.
A typical source of stress is the annoying oscillation and jitter the agent demonstrates. Unexpected behavior that requires human's instant reaction also creates stress. A lower score means more stress.

Table~\ref{tab:user-study} shows our method is the most user-friendly method. 
On the one hand, we use a deterministic novice policy that greatly alleviates the jitter and unexpected behaviors, reducing stress. 
On the other hand, our method masters human behaviors and suppresses undesired actions with the balanced buffer and proxy value, improving the user experience in compliance and performance.


\subsection{Ablation Studies}
\label{section:ablation}

\textbf{TD learning:}
As shown in Table~\ref{tab:comparing-hl-methods} ``PVP w/o TD'', 
disabling TD learning via setting $J^\text{TD}(Q) = 0$ significantly damages the performance of PVP, suggesting that propagating information from human-involved states to other states is critical to the success of PVP. 

\textbf{PVP with reward:}
Both MetaDrive and CARLA results in Table~\ref{tab:comparing-hl-methods} and~\ref{tab:carla-exp-ablation} show that adding the environmental reward doesn't bring significant improvement in the learning performance, which might be caused by the fact that the native reward function might not be aligned with human preference.


\begin{wraptable}{r}{0.48\textwidth}
% \vspace{-4em}
\centering
\begin{small}
\caption{
Ablation studies in CARLA.
}
\label{tab:carla-exp-ablation}
\begin{tabular}{@{}cccc@{}}
\toprule 
Method	& \shortstack{Human\\Data}  & \shortstack{Route\\ Completion} & \shortstack{Success\\ Rate}\\
\toprule
HACO & 4.8K  & 0.52 & 0.40 \\
HACO w/o SP & 5.1K  & 0.49 & 0.20 \\
\midrule
PVP w/o BB & 2.8K  & 0.62   & 0.33  \\
PVP w/o NB & 4.2K  & 	0.708   & 0.33  \\
PVP w/ Rew. & 4.4K  & 0.793 & 	0.467 \\
PVP w/ SP & 12.3K  & 0.40 & 0.20 ~\\
% PVP (Ours) & 6.6K & 24K & 0.80  {\tiny $\pm$ 0.09} & 0.58  {\tiny $\pm$ 0.08} \\
PVP w/ CQL &
8.0K
&
0.622
&
0.266
~\\
PVP (Ours) & 6.6K  &
0.92  {\tiny $\pm$ 0.05} & % rc
0.73  {\tiny $\pm$ 0.08} % succ
\\
\bottomrule
\end{tabular}%
\end{small}
\vspace{-2em}
\end{wraptable}
\textbf{Balanced buffer:} We find that disabling balanced buffers (PVP w/o BB) makes the training unstable and leads to poor performance. \revise{This design avoids the catastrophic forgetting when the agent-generated data overwhelms the human demonstrations as in HACO~\cite{li2021efficient}.}

\textbf{Novice buffer:} We find that PVP without the Novice buffer (PVP w/o NB) yields poor performance. The agent data stored in the novice buffer contains information on human preference and the forward dynamics of the environment. Thus, PVP does not discard the agent exploratory data as opposed to HG-DAgger~\citep{kelly2019hg}. 

\textbf{Stochastic policy:}
We implement PVP based on Soft Actor-critic~\citep{haarnoja2018soft} so that the novice policy is now a stochastic policy. As shown in the ``PVP w/ SP'' in Table~\ref{tab:carla-exp-ablation}, introducing randomness in novice actions greatly reduces the performance.
The human subjects report that the novice agents with stochastic policy oscillate frequently, making it hard to respond when the agents suddenly drive toward the side road.
HACO~\cite{li2021efficient} has similar human-AI shared control as PVP, but it adopts a stochastic policy. For comparison, we also implement HACO without a stochastic policy.
``HACO w/o SP'' suggests deterministic policy can not bring significant improvement to HACO. 



\textbf{Regularization on Q values}: 
As discussed in Sec.~\ref{section:analysis}, PVP objective can be interpreted as CQL with a newly introduced L2 regularization term on the Q values. We conduct the experiment to evaluate the performance of the vanilla CQL objective with other PVP designs in our reward-free online learning settings. As shown in ``PVP w/ CQL'' in Table~\ref{tab:carla-exp-ablation}, CQL objective yields worse performance. \revise{This experiment shows that the vanilla CQL doesn't work in this human active involvement setting.}
As shown in Fig.~\ref{figure:evolution-values}, the proxy value in the vanilla CQL method has a much larger magnitude which makes the values of behavior actions (the actions applied to the environment) and agent actions hard to distinguish.
PVP has smoother proxy values with a clear margin between behavior and novice Q.
CQL does not set a bound for the proxy value, thus proxy values in those extreme human actions are reinforced without a bound, making the novice policy rapidly learn those extreme actions, whereas PVP has bounded proxy values, leading to more stable training and better overall performance.


% ==================================================
\section{Conclusion}
Learning through active human involvement is a promising approach enabling safe and efficient policy learning.
In this work, we propose \textit{Proxy Value Propagation (PVP)} that can effectively learn from the intervention and the corrective feedback from active human involvement.
\revise{
PVP can be seamlessly integrated into existing value-based RL methods and achieves highly efficient reward-free policy learning, without offline pretraining and reward engineering.
}
Human-in-the-loop experiments show the proposed method achieves superior performance and better user experience across diverse environments with different action spaces and human control devices, 
\revise{showing that the learning from active human involvement is a efficient policy learning method aligning human preference.}

% \bz{It is weird to claim so many limitations for a new method which makes it look like an unfinished work. You can just leave two or three major ones here. }
\textbf{Limitations.}
(1) We only apply our method to two value-based RL methods. Advanced techniques such as exploration encouraging~\citep{osband2016deep} and prioritized replay buffer~\citep{schaul2015prioritized} can be added to further improve the result. 
% \revise{
% (2) Most of the experiments are conducted in the navigation environment. It is a promising future direction to apply PVP in robot manipulation tasks. There is a huge challenge to existing RL algorithms in robot manipulation tasks since fine-grained manipulation tasks such as threading a zip tie or juggling a ping pong ball are very hard to characterize by a reward function without sufficient engineering efforts. The proposed PVP method can learn from human corrective feedback in an online manner without reward engineering.
% We plan to utilize the teleoperation hardware developed in a recent work~\cite{zhao2023learning} to conduct the real-world experiment on robot manipulation.
% }
(2) Our method is not applicable to tasks where humans can not provide demonstrations.
(3) We assume that human always demonstrates desired actions. We will show in Appendix~\ref{section:appendix-proof} that suboptimal human behaviors will damage learning. In this case, we can define a sparse cost function in the training environment and utilize constrained optimization~\cite{achiam2017constrained} to penalize bad demonstrations.
(4) We assume that human subjects are available and attentive throughout the entire training. While our method is proven to be effective even under heavy traffic environments, we plan to further enhance its sample efficiency. We will achieve this goal by conducting offline RL training and policy evaluation in the background or passively involving human subjects whenever the model is uncertain about the environment.
% \revise{
% (6) Following (4) and (5), this work does not consider the time delay of human feedback. The problem is mitigated in PVP because if the agent learns a time-delayed policy, then human subjects will correct such behaviors in later training. We can adopt the eligibility traces in COACH~\cite{macglashan2017interactive} or labeling pre-intervention transitions as bad state-actions following EIL~\cite{spencer2020learning} to further address the issue.
% }

\textbf{Acknowledgment}: This work was supported by the National Science Foundation under Grant No. 2235012. The human experiment in this study is approved through the IRB\#23-000116 at UCLA. 



{
\small
% \bibliographystyle{abbrvnat}
\bibliographystyle{plain}
\bibliography{cite}
}


\newpage
\appendix
\input{appendix}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
