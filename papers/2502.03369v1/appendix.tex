
\section{Ethics Statement}
\label{section:ablation-ethics-statement}
Human subjects get paid to participate in the experiments. 
They can pause or stop the experiment if any discomfort happens. 
No human subjects are injured because all tasks we test are in virtual simulation. 
Each experiment will not last longer than one hour and subjects will rest at least three hours after one experiment. 
During training and data processing, no personal information is revealed in the collected dataset or the trained agents.
We have obtained IRB approval to conduct this project.

\section{Human Subject Research Protocol}
\label{appendix:user-study}

\textbf{Recruiting and Requirement.} 
For our study, we recruit 5 human subjects. All of them are college students and have the age from 20 to 30 years. Furthermore, every participant are required to have a valid driver's license and have experience in playing video game. Participation in our study is entirely voluntary. We ensure transparency by informing all subjects about the nature of the experiments and how their demonstrations would be used. Every subject provide written consent, confirming they are fully aware and in agreement. Additionally, the study is conducted with the IRB approval. 

\textbf{Onboarding Period.} 
Participants are required to undergo a practice session, during which they drive under complete control to get a sense of the control devices (wheel, gamepad and keyboard), the environment interface, the dynamics of each environment and how an episode will fail or success. Each subject get familiar with all the control devices and all the environments, which is indicating by performing at least 10 successful episodes, before they participate in the main experiments.

\textbf{Main Experiment.} During the initial stages of the formal experiment, subjects are \textit{advised} to retain full control of the agent for the first one or two episodes. Subsequently, they may begin to let the agents taking control and intervene as necessary. The objective in all driving experiments is twofold: firstly, to safely navigate the vehicle to its designated destination, and secondly, to ensure the vehicle's operation aligns with traffic regulations and human preferences. 

Subjects are encouraged to perform intervention whenever they perceive the vehicle might be in a dangerous situation, in violation of traffic rules, or in whatever scenario the human subjects feel they wouldn't behave in the way the novice policies do. 

To ensure data integrity and counter potential proficiency biases, the order of experiments with different control devices, tasks and training algorithms is randomized for each subject. By doing this we mitigate the bias that a subject might become more familiar with the task when experimenting different algorithms.


\textbf{User Study Questionnaire.}
We design a user study questionnaire to assess the experience of human subjects. The questionnaire is provided in Appendix~\ref{appendix:user-study}. Three aspects are considered:
\begin{itemize}
[leftmargin=1em,topsep=0pt,itemsep=-0.3em]
\item
\textbf{Compliance} measures whether the behaviors of the agent satisfy human intents. For example, a highly compliant agent behaves like human such that the human subjects feel like they are completing objectives by themselves.
\item
\textbf{Performance} is the subjective evaluation from human subjects on whether the agent can solve the primal task, e.g. driving to the destination in navigation tasks. This score should be low if the agent cannot learn a particular behavior or forget it even though human subjects have taught the agent multiple times.
\item
\textbf{Stress} gauges the cognitive cost human subjects pay to train the agent. A typical source of stress is the annoying oscillation and jitter the agent demonstrates. Unexpected behavior that requires human's instant reaction also creates stress. A lower score means more stress.
\end{itemize}


The same questions are repeated for each algorithm the human subject experimenting on.

\fbox{\parbox{\textwidth}{

\textbf{Compliance}: Generally, do you think the agent trained with this method complies with your intention? The higher score the better. 

Examples: 

(+) Good: Agent drives as you so that you don't even need to take over.

(-) Bad: Sudden unexpected behavior makes you mad.

Choices: 1, 2, 3, 4, 5

~\\

\textbf{Performance}: Do you think the agent trained with this method learns fast and performs well in terms of solving the task? The higher score the better.

% Smooth/Jittering, oscillating behaviors ARE NOT considered in this question. These low-level experience is evaluated in the "Comfortableness" section. 

Examples:

(+) Good: The agent learns fast so I don't need to take over too much in the later period.

(-) Bad: The agent forgets what it learns so I have to re-teach it.

(-) Bad: The agent never learns a specific behavior like accelerating or turning even though I have taught it so many times.

Choices: 1, 2, 3, 4, 5

~\\

\textbf{Stress}: Do you think training with this agent is tired or stressed? The lower score the more fatigue and stress. A higher score means you are more relaxed.

Tiredness might come from many sources: Oscillating trajectory, unexpected behaviors, degrading performance that you have to re-teach, etc.

It is possible that your agent is not performing well but you don't feel tired training it. On the other hand, it is possible that your agent has good performance but still causes fatigue due to unexpected behaviors.

Choices: 1, 2, 3, 4, 5

}}




\section{Demo Video}
\label{section:supplementary-video}

% Please find our demo video in: \url{https://drive.google.com/file/d/1imgGsi_kwXQz6H3pd7lxrjUHDsew2-tv/view?usp=sharing}

Please find our demo video in the supplementary material. This video shows the footage of human experiments and the comparisons between agents learned by the baselines and the proposed method. 
The video contains three sections:
\begin{enumerate}
% [leftmargin=2em,topsep=0pt,label=\arabic*),itemsep=0em]
\item The first section shows how we learn the driving policy in CARLA task within 20 minutes. We also compare the behavior of agents learned from PVP and TD3 baseline. 
\item In the second section, we show the footage of MetaDrive human experiment where the human subject uses a gamepad as the control device. We present the behavior comparison between PVP and TD3 baseline. 
\item In the third section, we show the applicability of our method to other tasks. PVP performs well in GTA V and can drive smoothly on the highway. In the discrete control tasks, the behavior comparison between PVP and DQN baseline in MiniGrid Empty Room and Four Room are provided.
\end{enumerate}


% \newpage

% \section{Expected Policy Behavior}
% \label{section:expected-behavior}


% In this section, we analyze the learning dynamics and provide insights on the learned policy.
% For simplicity, here we can define a proxy reward function:
% \begin{equation}
%     R(s, a) = 
%     \begin{cases}
%         +1 & \text{if } a \text{ is given by human during intervention} \\
%         -1 & \text{if } a \text{ is given by agent during intervention} \\
%         0 & \text{otherwise}
%     \end{cases}
% \end{equation}
% Though we use PV loss to directly fit Q to $\pm 1$, the process can be interpreted as a standard Q learning process.
% \begin{assumption}
% \label{assumption:perfect-q-netrowk}
% 	Assume the Q network can perfectly fit regression objective. Therefore it approximates both PVP and TD targets equivalently. We can assume:
% 	\begin{equation}
%  \label{eq:q-value-with-reward}
% 	    Q(s, a) = \cfrac{R(s, a) + \gamma \max_{a'}Q(s', a')}{2} .
% 	\end{equation}
% \end{assumption}

% \begin{finding}[Resembling Q Learning]
% \label{finding:traditional-q-learning}
% Combining the PV loss with TD loss is equivalent to the traditional Q learning objective where the reward function is $R(s, a) / 2$ and the discount factor is $\gamma / 2$.
% \end{finding}
% The convergence of our method is promised by Q learning theory. 


% Now we discuss the bound of proxy Q value.
% Denoting the last step is $T$ and supposing action $a_T$ leads to a terminal state, we have:
% \begin{equation}
% \label{eq:final-step-q-values}
% Q(s_T, a_T) = 
% \begin{cases}
%     +1 & \text{if } a_T = a_h \text{ and } I(s_T, a_n) = \text{True} , \\
%     -1 & \text{if } a_T = a_n \text{ and } I(s_T, a_n) = \text{True} , \\
%     0 & \text{otherwise}.
% \end{cases}
% \end{equation}
% % $Q(s_T, a_T) = 0$ comes from the case where agent happens to select the action that terminates the episode at final step and t. 
% % Both PVP loss and TD loss will go to zero and Q will remains a trivial value.

% Considering Assumption~\ref{assumption:perfect-q-netrowk} and Eq.~\ref{eq:final-step-q-values}, for all steps $t < T$ that are prior to the step $T$, we can easily write the bound of Q values:
% % \begin{equation}
% %     -\cfrac{1}{2} \le Q(s_t, a_t) \le \cfrac{1 + \gamma}{2}
% % \end{equation}
% \begin{theorem}
%     The proxy Q value is bounded:
%     \begin{equation}
%         -1 \le Q(s_t, a_t) < \cfrac{1}{2 - \gamma} \le 1.
%     \end{equation}
% \end{theorem}

% \begin{proof}
% % \begin{equation}
% From Eq.~\ref{eq:q-value-with-reward}, it is easy to find
% $\min_a Q(s, a) = (-1 - \gamma )/2  \ge -1$.
% % \end{equation}
% For supremum, considering the contraction of Q:
% \begin{equation}
% \begin{aligned}
% \sup_a Q(s_t, a) & = 
% \cfrac{1}{2} + \cfrac{\gamma}{2} \max_{a'} Q(s_{t+1}, a') = 
% \cfrac{1}{2} + \cfrac{\gamma}{2} (\cfrac{1}{2} + \cfrac{\gamma}{2} \max_{a'} Q(s_{t+2}, a')) =
% ... 
% \\
% & = 
% \cfrac{1}{2}(
%         1 + \cfrac{\gamma}{2} + {\cfrac{\gamma}{2}}^2 + ...
%     )
% = \cfrac{1}{2-\gamma}
% \le 1.
% \end{aligned}
% \end{equation}
% \end{proof}

% It is important for the proxy values to be bounded. As we will discuss in Appendix~\ref{section:comparing-pvp-and-haco-appendix}, PVP is more stable in training due to bounded proxy values.


% \begin{finding}[Avoiding Value Explosion]
% \label{finding:no-value-explosion}
% The Q value is bounded so that we can eliminate the value explosion issue. In contrast, previous work~\citep{li2021efficient} uses unbounded value target so the Q value is vulnerable to overestimation and goes to infinity.
% \end{finding}

% Now we discuss the behavior of the final proxy Q function. For arbitrary state action pair:
% \begin{equation}
% \label{eq:converged-q-values}
% Q^*(s_t, a_t) = 
% \begin{cases}
%     \cfrac{1 + \gamma \max_{a'}Q(s', a')}{2} \approx 1  & \text{if } a_t \text{ was previous human action} , \\
%     \cfrac{-1 + \gamma \max_{a'}Q(s', a')}{2} \approx 0 & \text{if } a_t \text{ was previously overwritten by human} , \\
%     \cfrac{\gamma \max_{a'}Q(s', a')}{2} \approx \cfrac{1}{2} & \text{otherwise}.
% \end{cases}
% \end{equation}
% Here we assume there is no attractor in state space. That is, there always exists a trajectory from current state to reach the state where human once intervened.
% This makes $\max_{a'}Q(s', a') \approx 1$ due to contraction.
% The optimal proxy value function sculptures such a Q value landscape that all previous human actions have highest $+1$ value.
% Those actions that lead to other states where human once involved also have higher values because the $+1$ value of the human-involved state will be propagated to those states.

% \begin{finding}[Implicit Intervention Minimization]
% Since the agent learns policy that maximizes Q function, the proxy value propagation mechanism encourages agent to be human-imitating, that is to reproduce human actions or recover to the states where human once taught what to do. Our proxy Q value also penalizes actions that cause human intervention, which implicitly achieves intervention minimization.
% \end{finding}




\section{Preference Alignment}
\label{section:appendix-proof}

Here we provide a conceptual framework to describe the compliance of human intention.
First, we introduce a ground-truth indicator $C: \mathcal S\times \mathcal A \to \{0, 1\}$ of the intention violation, denoting whether the action is undesired. $C$ is not revealed to the learning algorithm.
\begin{equation}
C(s, a) = 
\begin{cases}
  1, & \text{if } a  \text{ violates human intention} 
  \\
  0, & \text{otherwise.}
\end{cases}
\end{equation}
We will derive the upper bound of the discounted occurrence of intent violation, a measure of training time human intent compliance:
\begin{equation}
S_{\pi_b} = S_{\pi_b}(s_0) = \expect_{\tau\sim P_{\pi_b}} \sum_{t}\gamma^{t} C(s_{t}, a_{t}),
\end{equation}
where $P_{\pi_b}$ denotes the probability distribution of trajectories deduced by the behavior policy $\pi_b$.

 

During training, a human subject shares control with the learning agent. 
The agent's policy is a deterministic policy $\mu_n(s)$, the human's policy is a stochastic policy $\pi_h(a|s)$.
The human subject intervenes $I(s, a) = \text{True}$ under certain state and agent's action $a_n$.
The mixed behavior policy $\pi_b$ that produces the real actions to the environment is denoted as:
\begin{equation}
\label{equation:behavior-policy-with-deterministic-novice}
  \pi_b(a|s) 
  = 
  (1 - I(s, \mu_n(s)))\delta(a - \mu_n(s))
  + 
  I(s, \mu_n(s)){\pi_h}(a|s),
\end{equation}
where we use Dirac delta distribution to represent the deterministic novice policy.



{
Two important assumptions on the human subject are introduced:
}
\begin{assumption}[Error rate of human policy]
\label{assumption:human-policy-error-rate}
During human-AI shared control, the probability that the human subject produces an undesired action is bounded by a small value $\epsilon < 1$:
\begin{equation}
	\expect_{s \sim P_{\pi_b}, a\sim {\pi_h}(\cdot|s)} C(s, a) \le \epsilon  .
\end{equation}
\end{assumption}

\begin{assumption}[Error rate of intervention policy]
\label{assumption:intervention-error-rate}
During human-AI shared control, the probability that the human subject does not intervene when the action is undesired is bounded by a small value $\kappa < 1$:
\begin{equation}
\expect_{s \sim P_{\pi_b}}  (1 - I(s, \mu(s))) C(s, \mu(s)) \le \kappa  .
\end{equation}
\end{assumption}


We introduce the following theorem and give the proof as follows.
\begin{theorem}[Upper bound of intent violation]
% \label{theorem:upper-bound-of-training-risk-appendix}
The discounted occurrence of intent violation ${S_{\pi_b}}$ of the behavior policy ${\pi_b}$ is bounded by the error rate of the human action $\epsilon$, the error rate of the human intervention $\kappa$ and the intervention rate $\psi = \expect_{s\sim P_{\pi_b}} I(s, a_n)$:
\begin{equation}
{S_{\pi_b}}
\le
\cfrac{1}{1 - \gamma}(
\kappa + \epsilon \psi
).
\end{equation}
\end{theorem}



\begin{proof}

Consider Eq.~\ref{equation:behavior-policy-with-deterministic-novice}, we have:
\begin{equation}
\begin{aligned}
\expect_{s \sim P_{\pi_b}, a\sim \pi_b(\cdot|s)} C(s, a) 
& =
\expect_{s \sim P_{\pi_b}}\lbrace [1 - I(s, \mu_n(s))] C(s, \mu_n(s))
+
I(s, \mu_n(s)) \expect_{a\sim \pi_h(\cdot|s)} C(s, a)
\rbrace
~\\
& \le 
\kappa + 
\epsilon
\expect_{s \sim P_{\pi_b}} I(s, \mu_n(s)) 
 = \kappa + 
\epsilon \psi
\end{aligned}
\end{equation}	


The upper bound of $S_{\pi_b}$:
\begin{equation}
\begin{aligned}
S_{\pi_b} = 
\expect_{\tau \sim P_{\pi_b}} \sum_{t=0}\gamma^{t} C(s_{t}, a_{t})
\le 
\sum_{t=0}\gamma^{t}
(
\kappa + \epsilon \psi
)
=
\cfrac{1}{1 - \gamma}(\kappa + \epsilon\psi)
\end{aligned}
\end{equation}	
\end{proof}



% \newpage

\section{Environment Details}
\label{section:environment-details}


\begin{figure}[H]
\begin{minipage}{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{figs/screenshot_metadrive.pdf}
\caption{
MetaDrive Safety benchmark.
}
\end{minipage}\hfill
\begin{minipage}{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{figs/screenshot_carla.pdf}
\caption{
CARLA Town01.
}
\end{minipage}
\end{figure}

\begin{figure}[H]
\begin{minipage}{0.6\linewidth}
\centering
\includegraphics[width=\textwidth]{figs/GTA_V_In_Game.pdf}
\caption{
GTA V Training Environment.
}
\end{minipage}
\hfill
\begin{minipage}{0.35\linewidth}
\centering
\includegraphics[width=0.9\textwidth]{figs/screenshot_minigrid_4room.pdf}
\caption{
MiniGrid (Four Room).
}
\end{minipage}
% \hspace{30pt}
% \begin{minipage}{0.3\linewidth}
% \centering
% \includegraphics[width=0.7\columnwidth]{figs/screenshot_ski.pdf}
% \caption{
% Atari (Skiing).
% }
% \end{minipage}
% \hfill
\end{figure}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[H]
\centering
\begin{small}
\caption{Summary of the experiment environments.}
\label{table:environment}
\begin{tabular}{@{}lllll@{}}
\toprule
Environment                & Human Input Device & Observation Format  & Action Space & \shortstack{Training \& \\Test Set Split} \\ \midrule
MetaDrive & Gamepad, Keyboard, Wheel & State Vector        & Continuous   & Yes                         \\
CARLA                      & Wheel              & Bird-eye View Image & Continuous   & No                          \\
GTA V                      & Keyboard           & State Vector & Continuous   & Yes    
     \\
MiniGrid                   & Keyboard           & Semantic Map        & Discrete     & No                          \\
% Atari                      & Keyboard           & RGB Image           & Discrete     & No                          \\ 
\bottomrule
\end{tabular}
\end{small}
\end{table}


To avoid the potential risks of employing human subjects in physical experiments, we benchmark different approaches in four virtual simulated environments.
We conduct experiments on various tasks with different observation and action spaces and human input devices. 
Table~\ref{table:environment} summarizes the differences.

\textbf{Continuous action space environments.}
For continuous action space, we use MetaDrive Safety Benchmark~\citep{li2021metadrive}, CARLA Town01 environment~\citep{Dosovitskiy17} and a customized policy learning environment built upon Grand Theft Auto V (GTA V). In these tasks, the agent needs to steer the target car with low-level acceleration, brake and steering and move toward the destination.


MetaDrive Safety Benchmark preserves the capacity to evaluate the safety and generalizability in unseen environments since it uses procedural generation to synthesize an unlimited number of driving maps for the split of training and test sets, which is useful to benchmark the generalization capability of different approaches in the context of safe driving. 
We train agents in the training set, which contains 50 different scenes, and roll out the learning agents in the test set, which contains another 50 unique scenes. At each episode, the scene (road network) and the spawn location of traffic vehicles and ego vehicle are randomized.
We use sensory state vector in MetaDrive as the observation for agents and thus apply MLP network architecture.
When running pure RL methods in MetaDrive Safety Benchmark, a -1 penalty will be added to the reward when a crash happens. This is for a fair comparison with the safe RL methods who have access to the cost function directly.
\revise{Specifically, we follow the default reward scheme in MetaDrive. The reward function in MetaDrive safety benchmark is composed of four parts as follows:
\begin{equation}
\label{eq:reward-functgion}
  R = c_{disp}R_{disp} + c_{speed}R_{speed} +  c_{collision}R_{collision} + R_{term}.
\end{equation}
\begin{enumerate}
    \item The displacement reward: $R_{disp} = d_t - d_{t-1}$, wherein the $d_t$ and $d_{t-1}$ denotes the longitudinal movement in meters of the target vehicle in Frenet coordinates of the target trajectory between two consecutive time steps. If the agent drives in the wrong way then the displacement reward will be multiplied by $-1$. The displacement reward provides a \textbf{dense reward} to encourage the agent to move forward. We set $c_{disp} = 1$.
    \item The speed reward: $R_{speed} = v_{t} / v_{max}$, where $v_t$, $v_{max}$ denotes current speed and maximum allow speed in current road in $km/h$, respectively. If the agent drives in wrong way then the speed reward will be multiplied by $-1$. We set $c_{speed} = 0.1$.
    \item The collision reward: $R_{collision} = 1$ if a collision with a vehicle, human, or object happens. Otherwise, it is $0$. The coefficient $c_{collision} = 5$. 
    \item The terminal reward: $R_{term}$ is non-zero only at the last time step. At that step, we set $R_{disp} = R_{speed} = R_{collision} = 0$ and assign $R_{term}$ according to the terminal state. $R_{term}$ is set to $+10$ if the vehicle reaches the destination (successes) and $-5$ if the vehicle drives out of the road.
\end{enumerate}
For measuring the safety, collision to vehicles, obstacles, sidewalk raises a cost $+1$ at each time step. The sum of cost generated in one episode is the episodic cost.
}

In CARLA, we train and test agents in the Town01 environment. There exist many predefined routes in the town with different spawn locations and destinations. The length and spawn point of each route is randomized for each episode. We use the bird-eye view image in CARLA as observation and thus CNN is used as the feature extractor.
\revise{In CARLA environment the reward function follows the reward function in MetaDrive safety benchmark. }


In GTA V, we manually pick start and end coordinates in the world map to form multiple routes in different scenes.
% for ideal training and evaluation sets for agents. 
We split those scenes to the training and test sets.
The training set contains two scenes with straight roads, turns, and medium traffic. The test set contains one different scene. For each episode, the traffic condition is randomized by the game engine. 
\revise{In GTA V environment the reward function follows the reward function in MetaDrive safety benchmark.
}
The terminations include arriving at the destination (success), crashing with objects or vehicles for more than five frames (failure), and timeout (failure). 
The discrete keyboard input will be translated into continuous steering and acceleration signals for controls.
Our customized human-in-the-loop compatible policy learning environment builds upon GTA V with a full set of well-defined observation, reward and termination conditions. The environment will be open-sourced and available to the community \footnote{The customized environment builds upon prior efforts on the communication between GTA V engine and Python interface:
% We referenced the following GitHub repositories: 
\url{https://github.com/aitorzip/DeepGTAV}, \url{https://github.com/gdpinchina/DeeperGTAV}, \url{https://github.com/aitorzip/VPilot}}.

For these tasks, the reward function is composed of two parts: a sparse termination reward (+10 when reaching the destination) and a dense moving reward (the distance moving toward the destination within one step).

\textbf{Discrete action space environment.}
For discrete action space, we test on MiniGrid Two Room task~\citep{gym_minigrid}.
MiniGrid Two Room is a task requiring heavy exploration since the agent needs to move toward a door and open the door before reaching the destination. 
The spawn locations, the destinations, door locations and the geometry of each room are randomized.
The observation of MiniGrid is the semantic map of agent's local neighborhood. MiniGrid tasks only support using the keyboard as the input device. Only in the MiniGrid task, we render the agent’s action in the environment so that the human can decide whether to take over or return back based on both the current state and agent's action. But this is not feasible in other tasks since other tasks require real-time responses from humans and there is not enough time for humans to observe agent’s actions even if we plot those actions in the visualization interface. 
\revise{Following the default reward function as in original repository, in MiniGrid environment a sparse reward is used. When the agent reach the goal, $+1$ reward is given and otherwise the reward is always $0$.}

\textbf{Real-time experiment.}
Note that the local computer we use for human-in-the-loop experiments, which has an Nvidia GeForce RTX 3080 gpu, can support real-time simulation and training. 
In MetaDrive and CARLA, the physics simulation is run at 10Hz in the virtual world and in GTA V the frequency is 30Hz. That is, each environmental step will cause the virtual world advancing 0.1 / 0.033 seconds.
After each environment interaction, PVP updates its policy once, inferring and back-propagating one SGD batch.
Our experience suggests that the local computer can effortlessly support concurrent running of the simulation with human-agent shared control as well as the background policy update at the frequency in wall-time higher than the simulation frequency in the virtual world. 
That is, our training can run at frequency higher than 10Hz / 30Hz so that the time-elapse is actually faster in the real world than in the virtual world. We will limit the FPS to the system frequency so that the human subjects experience realistic time-elapse.

\textbf{Control devices.}
CARLA tasks use a Wheel, GTA V tasks use a keyboard, and MiniGrid tasks use a keyboard (provide discrete control signals). In all devices, a button is configured to indicate intervention. There is another button in the devices that activates an emergency stop. If any discomfort happens, human subjects can pause or stop the experiment immediately.


% \pzh{Screenshot of GTA V should be included here!!}

% The Atari game is difficult since the agent needs to learn to output meaningful action based on the RGB observation. We use the default setting provided by the Gym Atari environment. We experiment on Atari Skiing game. In preliminary experiments, we tried a few other Atari games. We find that human-in-the-loop method is not applicable to many games in Atari that have long horizons or are hard to play by human experts. For example, in the Breakout where the player must knock down as many bricks as possible by using the paddle below to ricochet the ball against the bricks and eliminate them, a full episode requires more than 120 seconds (can be much longer to reach a higher score) and the human expert has to attentively focus on playing the game. It is quite common for the human expert to miscalculate the trajectory and fail to rescue the ball from falling down. Another example is the Enduro, where the player controls a super fast racing car to run on an infinite long track. Experts cannot always give the optimal intervention in such a fast moving scene. In these environments, a pure RL agent will usually perform better even than the human experts. Besides as we discussed in the Limitation, representation learning is also a major challenge. It takes a long time for the convolutional neural network to learn good representations from the RGB frames. Therefore, the human-in-the-loop methods with active human involvement are not satisfactory in Atari games.






% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[]
% \centering
% \caption{A rough conceptual summary of different policy learning approaches.}
% \label{tab:property-of-different-approaches}
% \begin{tabular}{@{}lcccc@{}}
% \toprule
% Method & \shortstack{Before Training \\Human Efficiency} & \shortstack{Human \\Preference} & \shortstack{Training Time \\Human Efficiency} & \shortstack{Training\\Safety} \\
% \midrule
% Reinforcement Learning & \xmark & \xmark & \ourstar & \xmark \\
% Imitation Learning & \xmark & \xmark & \ourstar & \things \\
% \midrule
% HL + Posterior Involvement & \ourstar & \ourstar & \xmark & \xmark \\
% HL + Passive Online Involvement & \ourstar & \ourstar & \xmark & \xmark \\
% HL + Active Online Involvement & \ourstar & \ourstar & \xmark & \ourstar
% \\ \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[!t]
% \centering
% \caption{A rough conceptual summary of different policy learning approaches.
% % \pzh{Add a column on ``training time human comfort''}
% }
% \label{tab:property-of-different-approaches}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}lcccc@{}}
% \toprule
%  Method & \shortstack{Before Training \\Human Cost} & \shortstack{Satisfying\\Human Intention} &
% %  \shortstack{Training \\Human Cost} &
%  \shortstack{Training \\Efficiency} &
%  \shortstack{Training\\Safety}
% %  \shortstack{Training \\Human Comfort} &
%  \\
% \midrule
% % \multirow{2}{*}{\shortstack{No Human\\in the L}} 
%  Reinforcement Learning & Reward Engineering & Hard to Encode & Low & \xmark \\
%  Imitation Learning & Data Collection & Distributional Shift & Low & offline\cmark , online\xmark \\
% \midrule
%  HL + Posterior Involvement & \xmark & \cmark & Medium & \xmark \\
%  \midrule
%  HL + Passive Online Involvement & \xmark & \cmark & High & \xmark \\
%  HL + Active Online Involvement & \xmark & \cmark & Best & \cmark
% \\ \bottomrule
% \end{tabular}
% }
% \end{table}



\section{Extra Experimental Results}



\subsection{Impact of Control Devices}
\label{section:appendix-impact-of-control-devices}


\begin{table}[H]
\begin{small}
\centering
\caption{
The impact of different human input devices in MetaDrive benchmark.
% \pzh{This table is highly deviated from original HACO paper. Need very careful consideration.}
}
\label{table:input-devices}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multirow{4}*{ \shortstack{Input\\Device} } & 
\multirow{4}*{Method} &
\multicolumn{3}{c}{Training} &
\multicolumn{3}{c}{Testing}
\\
\cmidrule(lr){3-5}
\cmidrule(lr){6-8}
& &
\multirow{3}*{ \shortstack{Human\\Data\\Usage} }
&
\multirow{3}*{ \shortstack{Total\\Data\\Usage}  }
&
\multirow{3}*{ \shortstack{Total\\Safety\\Cost} }
&
\multirow{3}*{ \shortstack{Episodic\\Return} } & 
\multirow{3}*{ \shortstack{Episodic\\Safety\\Cost} } & 
\multirow{3}*{ \shortstack{Success\\Rate} }
~\\
~\\
~\\
% \multirow{1}*{ \shortstack{Method} } & 
% %\multirow{2}*{ \shortstack{Data Usage} } & 
% \multirow{1}*{ \shortstack{Episodic Return} } & 
% \multirow{1}*{ \shortstack{Safety Cost} } & 
% \multirow{1}*{ \shortstack{Success Rate} }
% ~\\
\toprule
\multirow{2}*{\shortstack{Wheel}} &
{HACO}
%  \pzh{Need rerun}
& 
%39.8K & % Data usage
21.2K (0.53) & 40K & 42 &
250.039 & % test return
1.453 & % test safety violation
0.355 % test success
\\
\cmidrule{2-8}
& PVP &
10.3K (0.26) &
40K &
12
&
336.657 & % test return
1.543  & % test safety violation
0.808  % test success
\\
\midrule
\multirow{2}*{\shortstack{Gamepad}} &
{HACO}
% \pzh{Need rerun}	
& 
%39.8K & % Data usage
28.4K (0.71) & 40K & 55 &
71.37 & % test return
1.97 & % test safety violation
0.0 % test success
\\
\cmidrule{2-8}
& PVP &
7.4K (0.19) &
40K &
21
&
%37K & % Data usage
356.99 & % test return
1.31  & % test safety violation
0.920  % test success
\\
\midrule
\multirow{2}*{\shortstack{Keyboard}} &
HACO   
& 19.2K (0.48)           & 40K                                  & 130                                 & 143.28                 & 1.645                    & 0.139                
\\
\cmidrule{2-8}
&PVP          & 14.6K (0.37)           & 40K                                 & 76                                 & 353.636                 & 0.898                    & 0.857                \\
\bottomrule
\end{tabular}%
\end{small}
\end{table}


Table~\ref{table:input-devices} presents the experiment results with different input devices in MetaDrive benchmark.
In all settings, PVP agents outperform baseline method, showing the generalizability of PVP on different control devices. 
We observe that HACO~\citep{li2021efficient} has performance discrepancy with different input devices.
When using Gamepad, human subjects tend to push and pull the stick to the limits, producing extreme values.
Extreme actions are particularly harmful to previous method as it does not incorporate the regularization terms on Q function to bound the Q values.
% This is because HACO updates the state-action values based on CQL objective~\citep{kumar2020conservative}: $\min Q(s, a_n) - Q(s, a_h)$.
%Since extreme values happen frequently when human intervenes through Gamepad, the proxy values in those human actions are reinforced without bound, making the novice policy rapidly learn those actions.
%In contrast, the proxy values learned via PVP are much more moderate.
%The novice Q has distinct negative value. In contrast, the values of behavior actions, the actions that satisfy human, have positive values.
%This result reveals the problem of unbounded values in the previous method.
%% which leads to the consistent high intervention rate as shown in Fig.~\ref{figure:intervention-rate}.
%Our method instead resolves this issue and has bounded proxy values, leading to stable training.
When using the keyboard, the human subjects press arrow keys to indicate increasing/decreasing current steering/acceleration values for an increment. Therefore there will be fewer extreme values happening than using a Gamepad, which explains why the baseline HACO performs better with the keyboard compared to Gamepad.
Due to less extreme values, when using Steering Wheel, HACO achieves good performance.

% \dcd{Should we delete this paragrah? As we will move the figure to the exp section}
% To explain why HACO is vulnerable to extreme action values, we compare the proxy Q values in our method and in HACO baseline in Fig.~\ref{figure:q-values-change} and find that HACO has a much larger magnitude in its proxy values compared to PVP. 
% This is because HACO updates the state-action values based on CQL objective~\citep{kumar2020conservative}: $\min Q(s, a_n) - Q(s, a_h)$.
% Since extreme values happen frequently when human intervenes through Gamepad, the proxy values in those human actions are reinforced without bound, making the novice policy rapidly learn those actions.
% In contrast, the proxy values learned via PVP are much more moderate. As we discussed in Appendix~\ref{section:expected-behavior}, the Q values are bounded in $[-1, 1]$.
% % The novice Q  negative value. In contrast, the values of behavior actions, the actions that satisfy human, have positive values.
% This result reveals the problem of unbounded values in the previous method.
% Our method instead resolves this issue and has bounded proxy values, leading to stable training and better performance.








% \begin{figure}[H]
% % \begin{minipage}[!b]{0.4\linewidth}
% \centering
% \centering
% \includegraphics[width=0.48\linewidth]{figs/q-haco.pdf}
% %\hspace{30pt}
% \includegraphics[width=0.48\linewidth]{figs/q-pvl.pdf}
% \vspace{-1em}
% \caption{\revise{Evolution of values.}}
% \label{figure:q-values-change}
% % \vspace{-1em}
% \end{figure}
% % \end{minipage}







% In Fig.~\ref{figure:intervention-freq}, we compare the \textit{intervention frequency} of PVP and HACO. It is the number of the human involvement segments in each episode divided by the episode length. Intervention frequency is not equivalent to intervention rate in that it measures how frequently the human subjects involve, reflecting the mental stress human subjects bear during shared control.
% Fig.~\ref{figure:intervention-freq} suggests HACO requires more human demonstration fragments than PVP. This may exhaust human subjects and consumes more cognitive resources. 
% The drawback stems from that HACO utilizes SAC method with stochastic policy for better exploration. The actions produced by the novice policy varies drastically due to randomness, making human subjects stressful to actively intervene.




% \begin{figure}[H]
% % \begin{minipage}[!b]{0.55\linewidth}
% \centering
% \includegraphics[width=0.48\linewidth]{figs/intervention-rate.pdf}
% \includegraphics[width=0.48\linewidth]{figs/int-freq.pdf}
% \caption{
% \revise{Intervention rate and intervention frequency in CARLA.}
% }
% \label{figure:intervention-freq}
% % \end{minipage}
% % \hfill
% %\vspace{-1em}
% \end{figure}





% \pzh{Stupid fucking revision is needed in this stupid fucking section.}
% Another possible reason to explain the performance drop is the 
% Randomness in HACO's policy is a reason leading to the suboptimal performance compared to PVP.
% We conduct a toy experiment to study the behaviors learned from HACO and PVP in CARLA.
\textbf{A case study in a toy environment.}
We retrieve the agents stored during the training of HACO and PVP in CARLA task. We test them in a straight road in CARLA town and plot their actuating signals in Fig.~\ref{fig:carla-extra}.
In this task, the steering should be always close to zero. However, we find that as the training iterations increase, the HACO agents gradually demonstrate unstable steering and their steering is deviating.  
In human-AI shared control, such unstable behaviors force human subjects to involve frequently. In contrast, the PVP policies learn a much better solution in lane keeping.



\begin{figure}[H]
\centering
% \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=0.4\textwidth]{figs/acc-carla.pdf}
        %  \caption{}
        %  \label{fig:y equals x}
    %  \end{subfigure}
% \begin{subfigure}[b]{0.4\textwidth}
        %  \centering
        \hspace{30pt}
         \includegraphics[width=0.4\textwidth]{figs/steer-carla.pdf}
        %  \caption{}
        %  \label{fig:y equals x}
    %  \end{subfigure}
\vspace{-1em}
\caption{
Control signals in a straight road in CARLA.
}
\vspace{-1em}
\label{fig:carla-extra}
\end{figure}


\textbf{Visualization of action sequences in training.}
In Fig.~\ref{fig:visualization-of-shared-control}, we present the visualization of the trajectories during human-robot shared control. 
Comparing the visualization of HACO and PVP, we find that PVP generates smoother trajectories.
Stable and smooth agent actions greatly improve human subjects' experience and relieve their stress during human-robot shared control. We can also find that as the training goes, PVP requires less human involvement.
These results explain the performance of PVP and is aligned with the behavior shown in the supplementary video.

% \subsection{Visualization of Human-robot Shared Control}
% \label{section:vis-of-human-robot-shared-control}

\newpage

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figs/trajectory_compressed_small.pdf}
\caption{
In MetaDrive task, we use the top-down view to plot the trajectories of human-agent shared control. We use dense arrows to represent the actions that are applied to the environments. 
% At each time step, an arrow is used to represent the action at one time step applied to the environment. 
The arrow starts at the position of the car at that time step and its direction is the steering angle, projected into ego car's local coordination. The length of the arrow represents the acceleration. 
We use green and yellow arrows to denote agent's actions and human's actions, respectively. 
% During human involvement, human's actions will overwrite agent's actions.
}
\label{fig:visualization-of-shared-control}
\end{figure}






%
%\subsection{Ablation Study}
%\label{section:appendix-ablation-study}
%
%\begin{table}[H]
%\centering
%\begin{small}
%\caption{
%Ablation study in MetaDrive. The overall intervention rate is given besides the human data usage.
%}
%\label{tab:comparing-hl-methods-ablation}
%\begin{tabular}{@{}ccccccc@{}}
%\toprule
%\multirow{4}*{Method}
%&
%\multicolumn{3}{c}{Training}
%&
%\multicolumn{3}{c}{Testing}
%\\
%\cmidrule(lr){2-4}
%\cmidrule(lr){5-7}
%%\cline{6-8}
%& 
%\multirow{3}*{ \shortstack{Human\\Data\\Usage} }&\multirow{3}*{ \shortstack{Total\\Data\\Usage}  }&\multirow{3}*{ \shortstack{Total\\Safety\\Cost} }&\multirow{3}*{ \shortstack{Episodic\\Return} } &\multirow{3}*{ \shortstack{Episodic\\Safety\\Cost} } & 
%\multirow{3}*{ \shortstack{Success\\Rate} }
%~\\
%~\\
%~\\
%\midrule
%% TD3 & - & 1M  & 1.74K {\tiny $\pm$ 0.62K} & 318.12 {\tiny $\pm$21.9} & 0.47 {\tiny $\pm$0.08} & 0.70 {\tiny $\pm$0.09} \\
%% \midrule
%% HACO                & 22.6K (0.50)           & 45.3K                                  & 63                                 & 143.287                 & 1.645                    & 0.139                \\
%% \midrule
%PVP          & 14.6K (0.37)           & 40K                                 & 76                                 & 353.636                 & 0.898                    & 0.857                \\
%PVP w/o TD Learning          & 13.5K (0.34)           & 40.5K                                 & 70                                 & 252.447 & 1.277 & 0.220   \\ 
%PVP w/ Reward &
%12.8K (0.32) &
%40K &
%30 &
%319.383 &
%0.767 &
%0.755 ~\\
%\bottomrule
%\end{tabular}
%\end{small}
%\end{table}
%
%
%\begin{table}[H]
%\centering
%\begin{small}
%\caption{
%Ablation study in CARLA.
%}
%\label{tab:carla-exp-ablation}
%\begin{tabular}{@{}ccccc@{}}
%\toprule 
%Method	& \shortstack{Data\\ Usage} &  \shortstack{Episodic\\Return} & \shortstack{Route\\ Completion} & \shortstack{Success\\ Rate}\\
%\toprule
%% PPO &	
%% 1M & % data
%% 81.57 {\tiny $\pm$  4.935 } & % return
%% 0.24 {\tiny $\pm$ 0.013}& % rc
%% 0.0 {\tiny $\pm$  0.0 }% succ
%% \\
%% TD3 &
%% 1M & % data
%% 43.46 {\tiny $\pm$ 12.83} & % return
%% 0.11 {\tiny $\pm$  0.05 } & % rc
%% 0.0 {\tiny $\pm$  0.0 }% succ
%% \\
%% \midrule
%% HACO
%% &
%% 23K &
%% 120.53 &
%% 0.25 &
%% 0.11
%% \\
%% \midrule
%PVP &	
%18K & % data
%449.65 {\tiny $\pm$  52.04 } & % return
%0.76 {\tiny $\pm$  0.07 } & % rc
%0.49 {\tiny $\pm$ 0.04} % succ
%\\
%PVP w/o Balanced Buffers &
%28.8K & % data
%263.82  & % return
%0.51  & % rc
%0.2 % succ
%\\
%% PVP w/ Reward &
%% 23.2K &
%% 580.125 &
%% 0.793 &
%% 0.533 ~\\
%\bottomrule
%\end{tabular}%
%\end{small}
%\end{table}
%
%We conduct an ablation study to show the importance of TD learning and balanced buffers.
%As shown in Table~\ref{tab:comparing-hl-methods-ablation}, disabling TD learning via setting $J^\text{TD}(Q) = 0$ greatly damages the performance of PVP. This suggests that our Principle~\ref{rule:use-td-learning} is critical to the success of PVP.
%In Table~\ref{tab:carla-exp-ablation}, we find that disabling balanced buffers makes the training unstable and leads to poor performance.
%
%Table~\ref{tab:comparing-hl-methods-ablation} also presents the experiment result when we train PVP with environmental reward. We find that reward has no significant impact on the learning performance.
%




\subsection{Extra Results in MiniGrid}
\label{section:appendix-extra-results}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.34\linewidth}
         \centering
         \includegraphics[width=0.941\textwidth]{figs/minigrid_empty.pdf}
         \caption{MiniGrid-Empty-Random-6x6-v0}
        %  \label{fig:y equals x}
     \end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figs/minigrid_tworoom.pdf}
         \caption{MiniGrid-TwoRooms-v0
}
        %  \label{fig:y equals x}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figs/minigrid_4room.pdf}
         \caption{MiniGrid-FourRooms-v0
}
        %  \label{fig:y equals x}
\end{subfigure}
\caption{
MiniGrid results.
}
\label{fig:minigrid-extra}
\end{figure}
In Fig.~\ref{fig:minigrid-extra}, we present the extra results in two additional MiniGrid environments. PVP achieves superior performance compared to RL baseline. Note that we use a CNN without recurrent module as the feature extractor. The performance of PVP can be further improved if we utilize the neural architecture with memory capability.




\section{Hyper-parameters}
\label{section:appendix-hyper-parameters}


In MetaDrive safety benchmark~\citep{li2021metadrive} task, the observation is a state vector. There exists a split of training and test environments in MetaDrive. We present the result of the learned agent performing in the test environment. 

In CARLA~\citep{Dosovitskiy17}, the observation is the bird-eye view image in $[84, 84, 5]$ shape, where 5 is the number of semantic channels.
We train and evaluate the agents in the same NoCrashTown01 environment.

% In both driving tasks, the agent needs to steer the target car with low-level acceleration, brake and steering and move toward destination and thus the action space is a two dimensional space.

In GTA V, the observation is a state vector containing 2D LiDAR scanning for 240 total sampling points with max range 50m~\citep{presil}, vehicle state variables (speed, throttle, steering, heading), and navigation state variables (distance to road borders, distance to the next navigation point, collision to objects). There exists a split of training and test environments in GTA V. We present the result of the learned agent performing in the test environment.

In MiniGrid tasks~\citep{gym_minigrid} MiniGrid-Empty-Random-6x6-v0 (Empty Room), MiniGrid-MultiRoom-N2-S4-v0 (Two Room) and MiniGrid-MultiRoom-N4-S5-v0 (Four Room), the observation is the top-down view semantic map in shape $[7, 7, 3]$.

% In Atari game Skiing task~\citep{bellemare13arcade}, the observation is originally [210, 160] and we resize and preprocess the images following~\citep{mnih2015human}. We also stack 4 consecutive frames so the input to neural network is in shape [84, 84, 4].

In MetaDrive and GTA V, we use a MLP with two hidden layers, each has 256 units and ReLU activation, as the network architecture for the value network and policy network.

For CARLA task, since the input image has the same size of [84, 84] pixels,
we use the same 5-layers CNN architecture with [16, 32, 64, 128, 256] filters in each layer. The corresponding kernel-size is [[4, 4], [3, 3], [3, 3], [3, 3], [4, 4]], and strides [3, 2, 2, 2, 4]. We use ReLU as activation functions between each layer.


For MiniGrid tasks, we use a 3-layer CNN architecture with [16, 16, 32] filters in each layer. All three layers have kernel-size 2 and there is a max-pooling layer between the first two layers. We use ReLU as activation functions between each layer.




\begin{table}[H]
\begin{small}
\begin{minipage}{0.45\linewidth}
\centering
\caption{PVP (MetaDrive)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate               & 0.0001 \\ 
Steps before Learning Start & 100\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Train Batch Size & 100  \\
Q Value Bound & 1 \\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\linewidth}
\centering
\caption{PVP (CARLA)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate               & 0.0001 \\ 
Steps before Learning Start & 100\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Train Batch Size & 128  \\
Q Value Bound & 1 \\
\bottomrule
\end{tabular}
\end{minipage}
\end{small}
\end{table}
\begin{table}[H]
\begin{small}
\begin{minipage}{0.45\linewidth}
\centering
\caption{PVP (GTA V)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate               & 0.0001 \\ 
Steps before Learning Start & 100\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Train Batch Size & 100  \\
Q Value Bound & 1 \\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\linewidth}
\centering
\caption{PVP (MiniGrid)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate               & 0.0001 \\ 
Steps before Learning Start & 50\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 32\\
Target Network Update Interval & 1\\
Train Batch Size & 256  \\
Q Value Bound & 1 \\
Exploration Reducing Fraction & 0 \\
Random Action Probability Initial Value & 0 \\
Random Action Probability Final Value & 0 \\
\bottomrule
\end{tabular}
\end{minipage}
\end{small}
\end{table}


\begin{table}[H]
\begin{small}
\begin{minipage}{0.45\linewidth}
\centering
\caption{HACO (MetaDrive)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate Actor             & 0.0003 \\ 
Learning Rate Critic            & 0.0003 \\ 
Learning Rate Entropy           & 0.0003 \\ 
Steps before Learning Start & 100\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Target Network Update Interval & 1\\
Train Batch Size & 128  \\
CQL Loss Temperature & 1.0 \\
% Target Entropy & ??? \\ 
% Environmental Horizon T & ???\\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\linewidth}
\centering
\caption{HACO (Carla)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate Actor             & 0.0003 \\ 
Learning Rate Critic            & 0.0003 \\ 
Learning Rate Entropy           & 0.0003 \\ 
Steps before Learning Start & 100\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Target Network Update Interval & 1\\
Train Batch Size & 128  \\
CQL Loss Temperature & 1.0 \\
% Target Entropy & ??? \\ 
% Environmental Horizon T & ???\\
\bottomrule
\end{tabular}
\end{minipage}
\end{small}
\end{table}


\begin{table}[H]
\begin{small}
\begin{minipage}{0.45\linewidth}
\centering
\caption{TD3 (MetaDrive)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate              & 0.0001 \\ 
Steps before Learning Start & 10000\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Train Batch Size & 100  \\
% Target Entropy & ??? \\ 
% Environmental Horizon T & ???\\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\linewidth}
\centering
\caption{TD3 (Carla)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate              & 0.0001 \\ 
Steps before Learning Start & 10000\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Train Batch Size & 100  \\
% Target Entropy & ??? \\ 
% Environmental Horizon T & ???\\
\bottomrule
\end{tabular}
\end{minipage}
\end{small}
\end{table}

%===========new GTA V
\begin{table}[H]
\begin{small}
\begin{minipage}{0.45\linewidth}
\centering
\caption{TD3 (GTA V)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate              & 0.0001 \\ 
Steps before Learning Start & 10000\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 1\\
Train Batch Size & 100  \\
% Target Entropy & ??? \\ 
% Environmental Horizon T & ???\\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\linewidth}
\centering
\caption{DQN (MiniGrid)}
% \label{tab:hyper-parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Hyper-parameter             & Value  \\ \midrule
Discounted Factor $\gamma$   & 0.99  \\
$\tau$ for Target Network Update & 0.005 \\
Learning Rate              & 0.0001 \\ 
Steps before Learning Start & 50\\
Steps per Iteration & 1\\
Gradient Steps per Iteration & 32\\
Target Network Update Interval & 1\\
Train Batch Size & 256  \\
Exploration Reducing Fraction & 0.3 \\
Random Action Probability Initial Value & 0 \\
Random Action Probability Final Value & 0.05 \\
% Target Entropy & ??? \\ 
% Environmental Horizon T & ???\\
\bottomrule
\end{tabular}
\end{minipage}
\end{small}
\end{table}
% \begin{table}[H]
% \begin{small}
% \begin{minipage}{0.45\linewidth}
% \centering
% \caption{DQN (MiniGrid)}
% % \label{tab:hyper-parameters}
% \begin{tabular}{@{}ll@{}}
% \toprule
% Hyper-parameter             & Value  \\ \midrule
% Discounted Factor $\gamma$   & 0.99  \\
% $\tau$ for Target Network Update & 0.005 \\
% Learning Rate              & 0.0001 \\ 
% Steps before Learning Start & 50\\
% Steps per Iteration & 1\\
% Gradient Steps per Iteration & 32\\
% Target Network Update Interval & 1\\
% Train Batch Size & 256  \\
% Exploration Reducing Fraction & 0.3 \\
% Random Action Probability Initial Value & 0 \\
% Random Action Probability Final Value & 0.05 \\
% % Target Entropy & ??? \\ 
% % Environmental Horizon T & ???\\
% \bottomrule
% \end{tabular}
% \end{minipage}\hfill
% \begin{minipage}{0.45\linewidth}
% \centering
% \caption{DQN (Atari)}
% % \label{tab:hyper-parameters}
% \begin{tabular}{@{}ll@{}}
% \toprule
% Hyper-parameter             & Value  \\ \midrule
% Discounted Factor $\gamma$   & 0.99  \\
% $\tau$ for Target Network Update & 1 \\
% Learning Rate              & 0.0001 \\ 
% Steps before Learning Start & 100000\\
% Steps per Iteration & 4\\
% Gradient Steps per Iteration & 1\\
% Target Network Update Interval & 1000\\
% Train Batch Size & 32  \\
% Exploration Reducing Fraction & 0.1 \\
% Random Action Probability Initial Value & 0 \\
% Random Action Probability Final Value & 0.01 \\
% % Target Entropy & ??? \\ 
% % Environmental Horizon T & ???\\
% \bottomrule
% \end{tabular}
% \end{minipage}
% \end{small}
% \end{table}


% For baselines SAC, PPO, SAC-Lag, PPO-Lag, CPO, CQL, BC, GAIL, HG-DAgger, IWR we use hyper-parameters same as proposed in HACO~\citep{li2021efficient}.
% For algorithm HACO, we use hyper-parameters same as proposed in HACO~\citep{li2021efficient}.




%\pzh{I want 2 more Atari environment results! @Chenda, @Martin. Here is the code to draw. Please make sure PVP is in green!!!!!!!!! We can admit that our method fails.  I think maybe we can show the breakout? Since we are indeed progressing but the learning speed is quite slow (just blame representation learning!).}

% \begin{lstlisting}
% plot_df = data.copy()
% y = "evaluation/custom_metrics/success_rate_mean"
% plot_df.loc[plot_df.method == "PVP", "method"] = "PVP (Ours)"

% sns.set("talk", "darkgrid")

% c = sns.color_palette("Set2")

% fig = plt.figure(figsize=(5, 5), dpi=200)

% ax = sns.lineplot(
%     data=plot_df,
%     y=y,
%     x="timesteps_total",
%     ci="sd",
%     palette=c[:len(plot_df.method.unique())],
%     hue="method",
% #     legend=None
% )

% ax.set_ylim(-0.05, 1.05)
% ax.set_ylabel("Success Rate")
% ax.set_xlabel("Steps")
% ax.ticklabel_format(style='sci', scilimits=(0,0), axis='x')
% plt.legend()
% # plt.show()
% # with PdfPages('../minigrid_draw/minigrid_emptyroom_compare.pdf') as pdf:
% #     pdf.savefig(plt, format='pdf', dpi=300)

% plt.savefig('minigrid_4room.pdf', format='pdf', dpi=300, bbox_inches = "tight")
% \end{lstlisting}
