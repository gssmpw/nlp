\section{Related Work}
AI alignment is one of the major issues in learning trustworthy intelligent agents for real-world applications. It is difficult to represent various human preferences into a scalar reward function in existing Reinforcement Learning (RL) methods**Levine, "Learning Neural Network Policies with Guided Policy Search"**, **Duan et al., "Benchmarking Deep Reinforcement Learning for Robot Control"**. Meanwhile, the manually designed reward function, which might be misaligned with human preferences, often leads to undesired behaviors**Atkeson et al., "Robot Learning from Demonstration by Leveraging Human Feedback"**.
As a promising complement to RL, Human-in-the-loop Learning (HL) can overcome costly reward engineering and convey human intents to the learning process directly through human involvement.
Compared to imitation learning (IL)**Hester et al., "Deep Q-Learning from Demonstrations"**, where the agent learns directly from high-quality human demonstration, HL methods benefit from interactive human involvement and feedback during the training, mitigating the possible distributional shift that usually happens when learning from offline data**Ross et al., "Learning Monotone Non-Decreasing Policies with Backward SDEs"**.


\textbf{Preference-based RL.}
A large body of work focuses on learning human preference via ranking pair of trajectories generated by the learning agent**Choi et al., "Sample-Efficient Imitation Learning for Continuous Control Tasks"**.
InstructGPT**Brown et al., "Language Models as Few-Shot Learners"** aligns language models by first supervised learning in demonstration and then finetuning by the reward learned from human preference feedback.
Preference learning can be applied to the tasks that human can not conduct, such as moving a six-legged Ant robot by assigning exact torque at each joint**Hester et al., "Deep Q-Learning from Demonstrations"**.
For those tasks that human can demonstrate, these methods do not fully utilize real-time feedback from human subjects during agent-environment interaction.


\textbf{HL with Passive Human Involvement.}
Different from preference-based RL, human subjects can provide direct feedback to the learning agent during training through passive human involvement.
Some works learn policy from human-provided evaluative feedback, a Boolean flagging correct or wrong actions**Nair et al., "Visual Exploration for Learning Robotic Arm Manipulation Tasks"**.
This is similar to the intervention in our framework. 
However, in **Argus et al., "Human-Robot Interaction via Shared Autonomy"**, humans provide high-level instructions, e.g. pointing to the left/right, while in PVP humans provide intervention and low-level demonstrations.
The other line of work allows the neural policy to operate the robot and the human subjects can provide demonstration upon the requests from the learning agents**Pomerleau et al., "Alvin: A Scalable Autonomous Vehicle for Desert Research"**.
The expert policy will intervene when uncertainty is huge, where the agent uncertainty is estimated by the variance of actions**Hester et al., "Deep Q-Learning from Demonstrations"**.
These methods reduce the cost of human resources but have potential risks to human subjects since they do not fully control the system.
For example, when human subjects use these algorithms to train autopilot AI, they are exposed to significant risks if they are in a self-driving cars due to unpredictable agent behaviors.


\textbf{Learning from Active Human Involvement.}
For safety-critical tasks such as autonomous driving, the safety of both the controlled vehicles and the human subjects is the top priority. 
There are many works that allow human subjects to proactively involve the agent-environment interactions based on their own judgment to ensure safety, which we call active human involvement.
Human subjects can terminate the episode if a near-accidental situation happens and such intervention policy can be learned**Liu et al., "Robust Imitation Learning via Exploiting Task-Specific Structure"**. 
Recent studies explore active human involvement methods through intervention and demonstration in the human-agent shared autonomy**Argus et al., "Human-Robot Interaction via Shared Autonomy"**.
However, previous methods do not fully utilize the power of human involvement.
\revise{
COACH**Janner et al., "When to Ask for Help: A Deep Reinforcement Learning Approach"** treats human labels as indications of advantage instead of simply as reward. Compared to COACH, our method accepts not only the feedback (the intervention signal) but also the human demonstration. Our method does not consider the time delay of human subjects explicitly as COACH does.
}
Interactive imitation learning method (HG-DAgger)**Hester et al., "Deep Q-Learning from Demonstrations"** does not leverage data collected by agents, while Intervention Weighted Regression (IWR)**Nair et al., "Visual Exploration for Learning Robotic Arm Manipulation Tasks"** does not suppress undesired actions likely intervened by human.
Meanwhile, Expert Intervention Learning (EIL)**Liu et al., "Robust Imitation Learning via Exploiting Task-Specific Structure"** and IWR**Nair et al., "Visual Exploration for Learning Robotic Arm Manipulation Tasks"** focus on optimizing actions step-wise without considering the temporal correlation between steps.
These drawbacks harm learning efficiency and thus incur more human involvement.
Moreover, previous methods lack experiments to demonstrate the generalizability to different task settings and human control devices.