\section{Introduction}


\noindent
The widespread adoption of Machine Learning (ML) across domains such as healthcare~\cite{abouelmehdi2017big}, finance~\cite{tripathi2020financial}, and education~\cite{florea2020big} has raised concerns about privacy risks,  particularly from attacks exploiting model behaviors to infer sensitive information~\cite{liu2021machine, dwork2017exposed}. Artificial Neural Networks (ANNs) are widely used across various domains but remain vulnerable to privacy attacks, like Membership Inference Attacks (MIAs), where adversaries attempt to infer whether specific data points were used during training~\cite{golla2023security}. Prior study suggests that Spiking Neural Networks (SNNs) show lower vulnerability to MIAs compared to ANNs~\cite{Moshruba2024AreNA}, potentially due to their unique spike based computation, which introduces inherent privacy advantages~\cite{poursiami2024brainleaks}. Specifically, the non-differentiable and discontinuous nature of SNNs reduces the correlation between model responses and individual data points~\cite{meng2022training}, while their stochastic spike-based encoding increases representation diversity, reducing the risk of overfitting and making individual training samples less distinguishable~\cite{olin2021stochasticity}. 

Given these properties, we explore methods to further enhance the privacy of SNNs. First, we investigate quantization, which has been proven to improve privacy in ANNs by reducing precision and altering learned representations, potentially limiting information leakage~\cite{famili2023deep}. Since SNNs operate in an event-driven manner, where neurons fire only when necessary~\cite{schuman2022opportunities}, the combination of spiking sparsity and quantization may further disrupt activation patterns exploited by MIAs. Additionally, we hypothesize that the information loss and noise introduced by quantization~\cite{kang2024effect} could influence the model’s susceptibility to privacy attacks.


Second, we examine the role of surrogate gradients in shaping SNN privacy. These functions approximate gradients for non-differentiable spike events, enabling backpropagation-based training. Beyond facilitating learning, surrogate gradients introduce inherent variability into model outputs through spike timing distributions and gradient approximations. This stochasticity resembles the noise injection mechanisms of Differentially Private Stochastic Gradient Descent (DPSGD)~\cite{song2013stochastic}, suggesting a potential privacy-preserving effect. To explore this, we analyze five different surrogate gradient functions: Fast Sigmoid, Arctangent (aTan), Spike Rate Escape(STE), Triangular, and Straight Through Estimator \cite{eshraghian2021training} and evaluate their influence on privacy vulnerability and model accuracy.

In this study, we assess the effects of both \textit{weight} and \textit{activation} quantization on the privacy characteristics of SNNs and ANNs against MIAs, evaluating models across convolutional and fully connected architectures. Our experiments span five datasets: CIFAR-10~\cite{cifar10}, MNIST~\cite{lecun2010mnist}, FMNIST~\cite{xiao2017fashion}, Iris~\cite{omelina2021survey}, and Breast Cancer~\cite{misc_breast_cancer_14}. Privacy vulnerability is measured using the attack model’s Receiver Operating Characteristic (ROC) Area Under the Curve (AUC), where lower values indicate stronger privacy protection. We also assess model accuracy to quantify the trade-off between privacy and performance under different quantization and surrogate gradient settings.




Based on our experimental analysis, we summarize the key findings regarding the impact of quantization and surrogate gradients on privacy vulnerability and model performance below.

\begin{itemize}
    \item \textbf{Quantization Analysis:} Both weight and activaction quantization reduces MIA vulnerability compared to full precision in ANNs and SNNs, while introducing  minor degradation in model performance. However, extreme quantization to 2 bits imposes unnecessary performance degradation without substantial privacy benefits, making moderate quantization (4-bit and 8-bit) a more practical choice.

    \item \textbf{Privacy Protection:} While quantization enhances privacy protection in both architectures, full precision SNNs demonstrate inherently superior privacy compared to quantized ANNs across all datasets, highlighting their fundamental privacy advantage without compromising model performance.
    \item \textbf{Surrogate Gradient Evaluation:} Spike rate escape demonstrates superior privacy protection while maintaining high model accuracy, whereas  aTan and STE exhibit higher MIA vulnerability.
\end{itemize}


% Given the increasing risks posed by MIAs in neural networks, exploring techniques to enhance model privacy has become essential. In our paper, we investigate two potential mechanisms for enhancing privacy in SNNs: quantization, which has previously demonstrated privacy benefits in ANNs, and surrogate gradients, which introduce variability during training and may further influence privacy resilience. 
% Firstly, quantization reduces model precision to improve computational efficiency and limit information leakage~\cite{famili2023deep}. By altering the granularity of learned representations, it can make it more difficult for an attacker to extract precise membership information from model outputs. Beyond its privacy benefits in ANNs, quantization may be even more effective in SNNs due to their event-driven computation and discrete spiking mechanism. Unlike ANNs, where activations are continuously updated for all neurons, SNNs operate in an event-driven manner, meaning neurons only activate when they receive sufficient input ~\cite{schuman2022opportunities}. This results in sparse and irregular activations, which, when combined with quantization, further limits the precision of stored information. Since MIAs exploit fine grained details in activation distributions, the combination of spiking sparsity and reduced precision from quantization may make privacy attacks less effective in SNNs than in ANNs. Additionally, the information loss and noise introduced by the reduced precision inherent in quantization~\cite{kang2024effect} could affect the model's susceptibility to privacy attacks. Building on this hypothesis, we investigate the effects of quantization on the privacy characteristics of both ANNs and SNNs


% In this study, we explore the effects of both \textit{weight} and \textit{activation} quantization on the privacy characteristics of SNNs and ANNs against MIA across convolutional neural networks and fully connected networks,  with evaluations conducted on five datasets: CIFAR-10~\cite{cifar10}, MNIST~\cite{lecun2010mnist}, FMNIST~\cite{xiao2017fashion}, Iris~\cite{omelina2021survey}, and Breast Cancer~\cite{misc_breast_cancer_14}.  We quantify privacy vulnerability using the attack model’s Receiver Operating Characteristic (ROC) Area Under the Curve (AUC), where lower AUC values indicate stronger privacy protection. We also evaluate model accuracy to assess the trade-off between privacy and performance under different quantization settings.

% As a second approach to reduce MIA vulnerability, we explore the role of surrogate gradients in shaping the privacy characteristics of SNNs. Due to the non-differentiability of spike events, SNNs rely on surrogate gradient functions to enable gradient flow during training. These functions approximate the spiking neuron’s activation, allowing for backpropagation based optimization. Beyond their primary role in training, surrogate gradients introduce intrinsic variability into model outputs through spike timing distributions and gradient approximations. This variability resembles the noise injection mechanisms of Differentially Private Stochastic Gradient Descent (DPSGD) ~\cite{song2013stochastic}, suggesting a potential privacy preserving effect. Unlike DPSGD, which explicitly injects noise to obscure training data, SNNs naturally exhibit randomness due to their spike based computations and surrogate gradient approximations ~\cite{neftci2019surrogate}. To investigate this hypothesis, we evaluate the impact of five different surrogate gradient functions— Fast Sigmoid, Arctangent (aTan), Spike Rate Escape, Triangular, and Straight-Through Estimator on privacy resilience in SNNs. By analyzing their influence on attack AUC and model accuracy, we aim to determine whether specific surrogate gradients provide a favorable trade-off between privacy protection and model performance.

% Based on our experimental analysis, we summarize the key findings regarding the impact of quantization and surrogate gradients on privacy resilience and model performance below.

% \begin{itemize}
%     \item \textbf{Quantization Analysis:} Both weight and activaction quantization reduces MIA vulnerability compared to full precision in ANNs and SNNs, while introducing  minor degradation in model performance.

%     \item \textbf{Privacy Protection:} While quantization enhances privacy protection in both architectures, full precision SNNs demonstrate inherently superior privacy compared to quantized ANNs across all datasets, highlighting their fundamental privacy advantage without compromising model performance
%     \item \textbf{Surrogate Gradient Evaluation:} Spike rate escape demonstrates superior privacy protection while maintaining high model accuracy, whereas arctangent (aTan) exhibits higher MIA vulnerability.
% \end{itemize}

% \hl{MP: At the end of the introduction it is still not clear that what is your goal. 1. Privacy of SNN vs quantizied ANN? 2. weight vs activcation quantiziation in ANN vs SNNs? 3. Privacy performance balance in ANN va SNNs? Story is  unclear}



% While traditional Artificial Neural Networks (ANNs) are widely used, \hl{certain challenges} around privacy remain under investigation~\cite{golla2023security, 9294026}. Spiking Neural Networks (SNNs), inspired by biological neural processing, operate through discrete spikes rather than continuous activations~\cite{schuman2022opportunities}. \hl{This event-driven mechanism} enables efficient handling of temporal data and reduces redundant computations. This results in potential benefits such as improved energy efficiency and reduced latency, making SNNs suitable for resource constrained tasks such as real-time motion detection in neuromorphic vision~\cite{clerico2024retina}, material discovery~\cite{gobin2024exploration}, graph learning~\cite{snyder2024transductive}, and energy-efficient navigation~\cite{patton2021neuromorphic}. We envision that the binary spike transmission in SNNs provides inherent privacy advantages due to two key characteristics.. First, their non-differentiable and discontinuous nature weakens the correlation between the model responses and individual data points, making it more challenging for attackers to identify membership of specific samples in the training set~\cite{meng2022training}. Second, SNNs introduce stochasticity through the discrete nature of spike based encoding~\cite{olin2021stochasticity}. This stochastic behavior increases the diversity of data representations, improving generalization and reducing the risk of overfitting. Thus by making individual training samples less distinguishable, it also enhances privacy.

% Previous work suggests that SNNs exhibit increased resilience against privacy attacks like Membership Inference Attack (MIA) compared to ANNs~\cite{Moshruba2024AreNA}, with findings suggesting that SNNs experience smaller performance degradation under Differentially Private Stochastic Gradient Descent (DPSGD) compared to ANNs. Surrogate gradient functions enable gradient flow through SNNs despite the non differentiability of spike events. Beyond facilitating training, these surrogate functions introduce intrinsic variability into the model's outputs through spike timing distributions and gradient approximations which might suggest potential privacy advantages similar to the noise injection mechanisms of DPSGD. Motivated by these findings, this paper examines the impact of two key factors: quantization and surrogate gradients. Specifically, we investigate how changes to model precision through quantization and training dynamics introduced by surrogate gradients influence privacy characteristics in both ANNs and SNNs. Additionally, we hypothesize that the information loss and noise introduced by the reduced precision inherent in quantization~\cite{kang2024effect} could affect the model's susceptibility to privacy attacks. Building on this hypothesis, we investigate the effects of quantization on the privacy characteristics of both ANNs and SNNs, as well as the role of surrogate gradients in influencing privacy specifically in SNNs.