% \section{Related Work}
% The increased use of machine learning models with sensitive data has led to growing concerns about privacy breaches and adversarial attacks, driving research into vulnerabilities such as MIAs and broader adversarial machine learning strategies~\cite{tramer2016stealing}. Li et al.~\cite{liu2021machine} outline privacy attacks, notably Membership Inference Attacks (MIAs), where adversaries identify whether specific data points were part of the training set. MIAs originated in genomic studies by Homer et al.~\cite{homer2008resolving} and were later formalized by Shokri et al.~\cite{shokri}, who introduced shadow models to assess data membership. Refinements by Salem et al.~\cite{salem2018ml} showed that MIAs could rely on single shadow models and confidence scores, while Nasr et al.~\cite{nasr2019comprehensive} demonstrated that white-box access enhances attack precision. In federated learning, Melis et al.~\cite{melis2019exploiting} and Song and Mittal~\cite{song2021systematic} highlighted MIA risks in distributed and generative models, respectively, with LiRA by Ilyas et al.~\cite{carlini2022membership} and RMIA by Zarifzadeh et al.~\cite{zarifzadeh2024low} further advancing MIA efficacy. While much of this research has focused on ANNs, extending these privacy studies to Spiking Neural Networks (SNNs) holds value, given SNNsâ€™ comparable performance. Although privacy vulnerabilities in SNNs are underexplored, Han et al.~\cite{han2023towards} and Safronov et al.~\cite{kim2022privatesnn} have begun developing privacy preserving methods using federated learning and differential privacy, respectively, offering insights into safeguarding SNNs from MIAs. 

% Alongside privacy efforts, quantization has emerged as an effective strategy for reducing model size and computational demands. Quantization reduces the precision of weights and activations, optimizing memory and power usage for resource-constrained deployments~\cite{yang2019quantization, nagel2021white}. Kang et al.~\cite{kang2024effect} show that lower precision levels introduce more noise, which improves privacy protection by making it more challenging for adversaries to infer membership information and validate their theoretical findings using MIAs, confirming that increased quantization noise correlates with enhanced privacy. Initial approaches, including Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), introduced lower bit-width representations to neural networks~\cite{jacob2018quantization}, and recent adaptive frameworks dynamically adjust bit-widths per layer to preserve model accuracy under compression~\cite{tran2024privacy}. Notably, quantized models demonstrate inherent resistance to MIAs by reducing overfitting and adding noise, which complicates adversarial inferences on training data~\cite{kowalski2022towards, famili2023deep}. Recent works show that quantization lowers MIA success rates in comparison to full-precision models, protecting sensitive data without sacrificing performance~\cite{wei2024q, hu2021quantized}.

% Recently, quantization techniques have been extended to SNNs, offering increased energy efficiency.  As shown by Yin et al.~\cite{yin2024mint}, quantizing both weights and membrane potentials to low precision significantly reduces memory footprint (up to 93.8\%) and computation energy (up to 90\%)without sacrificing accuracy. This is further claimed by Schaefer et al. ~\cite{schaefer2020quantizing} where reducing the precision of weights and neural dynamics significantly decreases memory and power requirements, achieving up to 73\% memory savings with minimal accuracy loss. In ~\cite{schaefer2023hardware}, they highlighted that low bit quantization, particularly ternary quantization where weights or activations are reduced to three discrete values (-1, 0, +1) optimally leverages hardware accelerators by minimizing computation energy, memory bandwidth, and data movement costs.. Moser et al.~\cite{moser2023quantization} demonstrates that the leaky integrate-and-fire (LIF) model inherently acts as a quantization operator for spike trains, effectively bounding errors within predictable limits and simplifying computational processes through threshold based transformations. Frameworks like Q-SpiNN~\cite{putra2021q} and QFFS~\cite{li2022quantization} have adapted quantization to the unique dynamics of SNNs, addressing synaptic weight and membrane potential quantization for low-power devices. Quantized SNNs, due to their sparsity and discrete spikes, present an opportunity to explore their inherent privacy properties, with early findings suggesting that quantization noise may reduce information leakage in MIAs on ANNs ~\cite{kang2024effect, famili2023deep}. This work builds on these insights, examining how quantized SNNs can enhance both efficiency and privacy for neuromorphic systems ~\cite{stock2019and, kowalski2022towards}. We also investigated the role of different surrogate gradients in enhancing privacy for SNNs. 

\section{Related Work}

% \hl{we need to mention gap in related work. You have referred to all these papers and then suddendly end the section. then what? what is the gap in these works that you are addressing in this paper}


% \hl{MP: Maybe you have written and I'm just too tired now, but it is not clear in the related work the difference between weight and activation quantiziation in ANN (or SNN) community. When you say quantized ANN which one are you referring to? How do you compare performance of weight-quantizied and activation-quantized ANN/SNNs?}

\noindent
Privacy attacks targeting sensitive data have raised concerns about information exposure, driving research into vulnerabilities like MIAs. Li et al.~\cite{liu2021machine} provide an overview of privacy attacks, including MIAs, which were first introduced in genomic studies by Homer et al.~\cite{homer2008resolving}. Shokri et al.~\cite{shokri} later formalized MIAs by developing the shadow model framework, where labeled datasets are generated to train an attack model for inferring data membership. Refinements by Salem et al.~\cite{salem2018ml} showed that MIAs could rely on single shadow models and confidence scores, while Nasr et al.~\cite{nasr2019comprehensive} demonstrated that white-box access enhances attack precision. While most privacy research has focused on ANNs, studying MIAs in SNNs is valuable due to their potential resilience, which comes from their sparse activations, where neurons fire only when necessary and the randomness in spike timing, both of which make it harder for an attacker to infer data membership. Notably, prior work highlights that SNNs incur less performance degradation when employing DPSGD compared to ANNs~\cite{Moshruba2024AreNA}. Han et al.~\cite{han2023towards} and Safronov et al.~\cite{kim2022privatesnn} introduced privacy-preserving techniques like federated learning and differential privacy for SNNs.

While prior research has explored privacy vulnerabilities in SNNs and introduced privacy-preserving techniques, further investigation is needed to understand how architectural modifications can enhance their resilience against MIAs. One such modification is quantization which has become an effective approach for reducing model size and computational demands, particularly in resource-constrained settings~\cite{yang2019quantization}. By reducing weight and activation precision, quantization introduces noise, which  protect privacy by making adversarial inference more challenging~\cite{kang2024effect}. Studies on ANNs using DoReFa-Net~\cite{kowalski2022towards, famili2023deep, wei2024q} apply both weight and activation quantization, demonstrating reduced MIA success rates due to added noise and lower overfitting. Applying quantization to SNNs has proven valuable for improving energy efficiency. Yin et al.~\cite{yin2024mint} demonstrated that weight and membrane potential quantization in SNNs reduces memory use by 93.8\% and computation energy by 90\% with minimal accuracy loss. Schaefer et al.~\cite{schaefer2020quantizing, schaefer2023hardware} showed that ternary quantization optimizes hardware efficiency by reducing energy and memory costs. Frameworks like Q-SpiNN~\cite{putra2021q} and QFFS~\cite{li2022quantization} adapt quantization techniques to SNN-specific dynamics, addressing challenges like synaptic weight and membrane potential quantization for low-power neuromorphic systems. Recent findings suggest that quantization noise in SNNs may reduce information leakage in MIAs, aligning with observations in ANNs~\cite{kang2024effect, famili2023deep}. 

On the other hand, surrogate gradients enable effective training in SNNs by approximating gradients for non-differentiable spike events,  but their impact on privacy has received little attention. PrivateSNN, introduced by Kim et al., proposes a privacy-preserving approach for converting ANNs to SNNs by incorporating spike-based learning rules to mitigate privacy risks~\cite{kim2022privatesnn}. Similarly, DPSNN, developed by Wang et al., integrates differential privacy with SNNs by leveraging gradient noise and discrete spike sequences, aiming to enhance model robustness against privacy attacks~\cite{wang2022dpsnn}. However, the direct impact of different surrogate gradient functions on the privacy characteristics of SNNs remains unexplored, leaving a gap in understanding their potential role in mitigating privacy risks. Similarly, while prior work has examined quantization for ANN privacy, its implications for SNN privacy have not been thoroughly investigated. Our work bridges these gaps by evaluating how quantization and surrogate gradients impact privacy in SNNs, providing insights into their role in enhancing privacy resilience while maintaining model performance.




 % In \hl{federated learning}, Melis et al.~\cite{melis2019exploiting} and Song and Mittal~\cite{song2021systematic} highlighted MIA risks in distributed and generative models, respectively, with LiRA by Ilyas et al.~\cite{carlini2022membership} and RMIA by Zarifzadeh et al.~\cite{zarifzadeh2024low} further advancing MIA efficacy.