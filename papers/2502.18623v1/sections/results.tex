

% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.85\linewidth, height=0.5\linewidth]{Figure/weight/weight_auc.pdf}
%     % \includegraphics[width=\linewidth]{Figure/mia_1sm.png}
%     \caption{Weight Quantization impact on MIA AUC score on (a) Breast Cancer (b) CIFAR-10, (c) F-MNIST, and (d) MNIST. The grey solid line represents the Full Precision (FP) model, while the purple, orange, and blue solid  lines correspond to the quantized models with bit precisions of w=8, w=4, and w=2, respectively. The pink dashed diagonal line indicates a random classifier's performance (AUC = 0.5), serving as a baseline for comparison.}
%     \label{fig:weight_auc}
% \end{figure*}

% \section{Results \& Discussion}
% In this section, the impact of quantization and multiple surrogate gradients on the privacy preserving properties of SNNs are examined. Specifically, the analysis seeks to determine the extent to which quantization and surrogate gradients influence the susceptibility of SNNs to Membership Inference Attacks (MIAs) and how these techniques affect model performance and privacy characteristics when compared to Artificial Neural Networks (ANNs).

% To conduct this investigation, full-precision SNN and ANN models are compared with their quantized counterparts across multiple datasets. The analysis focuses on two primary aspects: model performance and privacy resilience. Performance degradation resulting from quantization is assessed by measuring the drop in accuracy,  while privacy resilience is evaluated by examining the Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) scores of the Membership Inference Attacks (MIAs). 

% \subsection{Activation Quantization}
% Figures~\ref{fig:act_acc} and~\ref{fig:act_auc} illustrate the impact of activation quantization on model performance and attack AUC across different datasets. 

% % The top row shows ANN metrics, and the bottom row shows SNN metrics across datasets, with varying bit levels and thresholds. 

% \textit{Model Performance:} Quantization led to varied performance degradation when comparing ANNs and SNNs with their full precision counterparts as observed in Figure~\ref{fig:act_acc}. In ANNs, accuracy consistently decreased with lower bit-widths across all datasets. For instance, in CIFAR-10~\cite{cifar10}, accuracy dropped from 79.20\% in the full-precision model (Table~\ref{table
% }) to approximately 75\% in 2-bit models, depending on the threshold. This decline reflects increased model uncertainty under quantization, with logits for competing classes becoming less distinct, highlighting the trade-off between precision and confidence. This trend was evident in all the other datasets, indicating that lower precision levels adversely affect ANN performance.

% In contrast, SNNs displayed a more variable response to quantization when compared to full-precision models, which achieved 78.99\% accuracy on CIFAR-10 and 98.25\% on Breast Cancer. The performance of 2-bit SNN models initially held or even exceeded that of 4- and 8-bit configurations at lower thresholds. For instance, in CIFAR-10~\cite{cifar10}, the 2-bit model achieved approximately 68\% accuracy at a threshold of 0.6, outperforming 4-bit and 8-bit models. However, as thresholds increased to 1.5, the accuracy of the 2-bit model declined to approximately 60\%, dropping below the 62\%–64\% range observed for higher bit-widths.
% A similar trend was noted in Breast Cancer~\cite{misc_breast_cancer_14}, where the 2-bit model initially maintained high accuracy near 98\% at lower thresholds but fell to around 94\% at higher thresholds, with 4-bit and 8-bit models operating stably around 96\%–97\%. This suggests that lower bit-width SNNs are initially resilient to quantization noise but experience more substantial accuracy degradation at elevated thresholds, likely due to cumulative effects on spike dynamics.

% \textit{MIA Vulnerability:} Membership inference attack vulnerability in ANNs generally decreases with quantization, as evidenced by lower AUC values compared to full precision models in Figure~\ref{fig:act_auc}. However, at 2-bit quantization, we observe an anomaly in this trend. For instance, in CIFAR-10~\cite{cifar10}, the AUC decreases from 0.82 (full precision) to approximately 0.62 (8-bit and 4-bit), but increases to 0.65 at 2-bit precision. A similar pattern is seen in Breast Cancer and F-MNIST as well. This unexpected increase in vulnerability at extreme quantization suggests that excessive discretization may introduce exploitable patterns in model predictions.

% SNNs, however, showed only a minor reduction in MIA AUC from full precision to quantized models, without a consistent relationship to bit-width as seen in Figure~\ref{fig:act_auc}. For example, CIFAR-10’s~\cite{cifar10} full precision AUC of 0.59 decreased slightly across quantized SNN models, typically stabilizing around 0.50 to 0.55, a behavior mirrored in FMNIST~\cite{xiao2017fashion} and Breast Cancer~\cite{misc_breast_cancer_14}. This variability suggests that SNNs’ temporal dynamics may offer modest privacy resilience under quantization, though the effect lacks the systematic nature observed in ANNs.
\section{Results \& Discussion}
\noindent
This section presents the evaluation of how activation and weight quantization as well as surrogate gradients impacts SNN privacy, focusing on MIA vulnerability and performance trade-offs in comparison to ANNs. Privacy vulnerability is measured via ROC-AUC, while accuracy captures performance impact.

% \hl{general comment: Quantiziation in ANNs started due to the need for energy efficiency. The fact that FP SNN is still more privacy preserving that quantizied ANN as standalone might not have enough merit if not compared with energy savings of SNNs vs quantizied SNNs and quantizied ANNs. You need add a note on this or even add a column comparing the energy numbers from [34]}


\subsection{Activation Quantization}

\noindent
% Figures~\ref{fig:act_acc} and~\ref{fig:act_auc} illustrate the impact of activation quantization on model accuracy and attack AUC across datasets.

\noindent
\textit{Model Performance:} 
Accuracy degradation due to activation quantization is observed in both ANNs and SNNs, but the overall drop from full precision to quantized models with 8-bits, 4-bits and 2-bits remains relatively small (Figure~\ref{fig:act_acc}). In ANNs, accuracy decreases more noticeably at lower bit-widths, particularly at 2-bit, where reduced precision compresses activation ranges, limiting representational capacity and reducing the distinction between logits. However, across each bit-width, accuracy remains relatively stable across different threshold levels, indicating that thresholding has minimal impact on model performance. This is because  ReLU activations, operate in a continuous space where moderate clipping does not significantly alter feature representations, allowing the network to maintain performance despite threshold variations. 
% For example, on CIFAR-10~\cite{cifar10}, accuracy dropped from 79.20\% (full precision) to 75\% in 2-bit models. This decline reflects the loss of precision in model parameters, compressing activation ranges and reducing the distinction between logits. 

In SNNs, accuracy trends vary across datasets, as shown in Figure~\ref{fig:act_acc}. For 4-bit and 8-bit quantized models, higher thresholds consistently improve accuracy across all datasets by retaining a broader activation range, preserving useful signal information. The stochastic nature of SNNs further mitigates quantization noise, ensuring stable performance regardless of dataset complexity. However, 2-bit quantized models show different behaviors. In Breast Cancer, accuracy starts near full precision at lower thresholds but degrades as thresholds increase due to amplified noise overwhelming simpler patterns. In CIFAR-10, accuracy initially improves with thresholding but drops at higher thresholds as excessive noise disrupts feature representation, reducing classification performance.

% While full-precision SNNs achieved 78.99\% on CIFAR-10 and 98.25\% on Breast Cancer, lower-bit models exhibited non-monotonic behavior. \hl{In CIFAR-10, the 2-bit model reached 68\% accuracy at a 0.6 threshold, outperforming higher bit-width models but dropping to 60\% at higher thresholds. Similarly, in Breast Cancer, 2-bit models maintained 98\% accuracy at low thresholds but dropped to 94\% at higher thresholds, with 4-bit and 8-bit models stabilizing at 96–97\%.}  \hl{This suggests that while SNNs initially handle quantization noise well, higher thresholds exacerbate accuracy degradation due to cumulative spike dynamics.}

\textit{MIA Vulnerability: }Quantization reduces attack vulnerability in ANNs compared to full precision as seen in Figure~\ref{fig:act_auc}. Quantized models with 2-bit, 4-bit, and 8-bit precision all exhibit lower vulnerability than the full-precision model. However, among them, 2-bit quantized model is the most vulnerable across all datasets, while  4-bit and 8-bit quantized model provide better privacy protection.  This occurs because extreme quantization (2-bit precision) severely limits representational capacity, making activations more uniform and easier to infer in ANNs, thereby increasing susceptibility to attacks. 

% \hl{On CIFAR-10, attack AUC dropped from 0.82 (full precision) to 0.62 at 8-bit and 4-bit precision (24.4\% reduction). However, at 2-bit precision, AUC slightly increased to 0.65}, likely due to distortions that inadvertently retain exploitable characteristics.

In SNNs, activation quantization minimizes attack vulnerability compared to full precision across all datasets as well. However, unlike in ANNs,  2-bit, 4-bit, and 8-bit quantized SNN models showed  somewhat similar privacy vulnerability, specially in F-MNSIT and MNIST datasets. This suggests that the stochastic nature of spike-based encoding inherently limits the granularity of information available to attackers, irrespective of precision levels. The inherent randomness in spike timing and event-driven processing disrupts predictable activation patterns, making the additional noise introduced by quantization less impactful in distinguishing training from non-training data. This means that while quantization is still effective in lowering attack success rates, further reducing bit width does not provide additional privacy benefits for these datasets.



% \section{Results \& Discussion}
% \noindent
% This section examines the effects of quantization and surrogate gradients on the privacy preserving properties of SNNs, focusing on their susceptibility to Membership Inference Attacks (MIAs) and comparing performance and privacy characteristics with Artificial Neural Networks (ANNs).

% To investigate these aspects, full-precision SNN and ANN models are compared with their quantized counterparts across multiple datasets. The analysis evaluates performance degradation through accuracy drops and privacy resilience using Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) scores for MIAs.

% \subsection{Activation Quantization}
% \noindent
% Figures~\ref{fig:act_acc} and~\ref{fig:act_auc} show the impact of activation quantization on model performance and attack AUC across datasets respectively.

% \textit{Model Performance:} Activation quantization caused performance degradation in both ANNs and SNNs compared to their full-precision versions (Figure~\ref{fig:act_acc}). In ANNs, accuracy consistently decreased with lower bit-widths across all datasets. For instance, on CIFAR-10~\cite{cifar10}, accuracy dropped from 79.20\% (full precision, Table~\ref{table:weight_quan}) to around 75\% in 2-bit models, depending on the threshold. This decline reflects increased model uncertainty and less distinct logits for competing classes, demonstrating the trade-off between precision and confidence. Similar patterns were observed in other datasets.

% SNNs showed more variability under quantization. Full-precision SNNs achieved 78.99\% accuracy on CIFAR-10 and 98.25\% on Breast Cancer. At lower thresholds, 2-bit SNN models initially performed comparably or better than 4- and 8-bit models. For example, in CIFAR-10~\cite{cifar10}, the 2-bit model reached approximately 68\% accuracy at a threshold of 0.6, outperforming higher bit-width models. However, accuracy dropped to around 60\% at thresholds of 1.5, falling below the 62\%–64\% range of higher bit-width models. A similar pattern was observed in Breast Cancer~\cite{misc_breast_cancer_14}, where the 2-bit model maintained around 98\% accuracy at low thresholds but dropped to 94\% at higher thresholds, with 4-bit and 8-bit models stabilizing at 96\%–97\%. This suggests that while lower bit-width SNNs initially handle quantization noise well, higher thresholds lead to more significant accuracy degradation due to cumulative spike dynamics effects.

% \textit{MIA Vulnerability:} Quantization significantly reduced the attack AUC for ANNs, indicating a consistent decrease in MIA vulnerability across all datasets. Notably, the reduction was most pronounced at 8-bit and 4-bit precision as seen in Figure~\ref{fig:act_auc}, while 2-bit precision exhibited slightly higher vulnerability. For example, in CIFAR-10~\cite{cifar10}, the attack AUC dropped from 0.82 in full precision to approximately 0.62 at both 8-bit and 4-bit precision, representing a 24.4\% reduction. However, at 2-bit precision, the AUC increased slightly to 0.65, reflecting a 20.7\% reduction from full precision. This pattern could be attributed to the finer balance between reduced precision and the loss of representational distinctiveness in logits at higher bit widths, whereas extreme quantization at 2-bit precision may introduce noise or distortions that inadvertently retain some distinguishable characteristics exploited by the attacker.

% In SNNs, the attack AUC also decreased with quantization, but the reduction was less dramatic compared to ANNs due to the inherently lower AUC values of SNNs in full-precision settings. For instance, in CIFAR-10~\cite{cifar10}, the AUC dropped from 0.59 in full precision to a range of 0.50–0.55 in quantized models, corresponding to a reduction of 6.8–15.3\%. Similar trends were observed in datasets like FMNIST~\cite{xiao2017fashion} and Breast Cancer~\cite{misc_breast_cancer_14}. Interestingly, the AUC values for 2-bit, 4-bit, and 8-bit quantized SNNs were largely within the same range, suggesting that the stochastic nature of SNN computations and the spike-based encoding may inherently limit the granularity of information available to the attacker, regardless of precision levels.
\begin{table*}[htbp]
\centering
\scriptsize % Reduce font size
\renewcommand{\arraystretch}{0.9} % Reduce row height
\setlength{\tabcolsep}{4pt} % Reduce column spacing
\begin{tabular}{p{2cm} p{4cm} p{1.5cm} p{1.5cm} p{1.5cm}}
\toprule
\textbf{Dataset} & \textbf{Surrogate Gradients} & \multicolumn{3}{c}{\textbf{SNN}} \\
\cmidrule(lr){3-5}
 &  & \textbf{Train Accuracy} & \textbf{Test Accuracy} & \textbf{MIA AUC} \\
\midrule
\textbf{MNIST} & Fast Sigmoid(slope,k=25) & 99.88(±0.01)\% & 99.22(±0.04)\% & 0.518(±0.001) \\
 & aTan(alpha=2) & 99.96(±0.02)\% & 99.25(±0.06)\% &  0.547(±0.006)  \\
 & Spike Rate Escape(beta=1, slope=25) & 99.97(±0.01)\% & 99.25(±0.03)\% & 0.508(±0.008) \\
 & Triangular & 75.16(±0.09)\% & 76.34(±0.14)\% & 0.503(±0.003) \\
 & Straight Through Estimator & 99.57(±0.04)\% & 98.79(±0.07)\% & 0.528(±0.005) \\
\midrule
\textbf{FMNIST} & Fast Sigmoid(slope,k=25) & 99.45(±0.04)\% & 91.97(±0.12)\% & 0.518(±0.008) \\
 & aTan(alpha=2) & 99.58(±0.03)\% & 92.18(±0.14)\% & 0.547(±0.002) \\
 & Spike Rate Escape(beta=1, slope=25) & 99.74(±0.09)\% & 92.23(±0.11)\% & 0.523(±0.016) \\
 & Triangular & 78.79(±0.24)\% & 79.49(±0.20)\% & 0.498(±0.011) \\
 & Straight Through Estimator & 91.55(±0.13)\% & 89.69(±0.21)\% & 0.512(±0.003) \\
\midrule
\textbf{CIFAR10} & Fast Sigmoid(slope,k=25) & 82.90(±0.46)\% & 78.04(±0.40)\% & 0.535(±0.010) \\
 & aTan(alpha=2) & 87.56(±0.41)\% & 78.99(±0.38)\% & 0.561(±0.010) \\
 & Spike Rate Escape(beta=1, slope=25) & 89.92(±0.39)\% & 79.95(±0.33)\% & 0.567(±0.013) \\
 & Triangular & 21.43(±0.76)\% & 23.92(±0.86)\% & 0.550(±0.036) \\
 & Straight Through Estimator & 53.10(±0.66)\% & 62.49(±0.54)\% & 0.644(±0.034) \\
\midrule
\textbf{Iris} & Fast Sigmoid(slope,k=25) & 82.50(±2.46)\% & 90.00(±6.34)\% & 0.654(±0.022) \\ 
 & aTan(alpha=2) & 96.67(±1.46)\% & 93.33(±0.50)\% & 0.563(±0.095) \\
 & Spike Rate Escape(beta=1, slope=25) & 96.67(±1.00)\% & 100.00(±0.00)\% & 0.543(±0.036) \\
 & Triangular & 100.00(±0.00)\% & 100.00(±0.00)\% & 0.510(±0.029) \\
 & Straight Through Estimator & 97.50(±0.04)\% & 100.00(±0.00)\% & 0.542(±0.108) \\
\midrule
\textbf{Breast Cancer} & Fast Sigmoid(slope,k=25) & 100.00(±0.00)\% & 100.00(±0.00)\% & 0.494(±0.013) \\
 & aTan(alpha=2) & 100.00(±0.00)\% & 97.37(±0.11)\% & 0.538(±0.015) \\
 & Spike Rate Escape(beta=1, slope=25) & 100.00(±0.00)\% & 97.37(±0.11)\% & 0.512(±0.026) \\
 & Triangular & 97.58(±0.12)\% & 98.25(±0.00)\% & 0.497(±0.022) \\
 & Straight Through Estimator & 99.78(±0.12)\% & 98.25(±0.00)\% & 0.482(±0.024) \\
\bottomrule
\end{tabular}
\caption{Impact of training SNNs with different surrogate gradients on Privacy Vulnerability and Model Performance across different datasets.}
\label{table:sg}
\end{table*}


% \begin{table*}[htbp]
% \centering
% \footnotesize
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l l c c c}
% \toprule
% \textbf{Dataset} & \textbf{Surrogate Gradients} & \multicolumn{3}{c}{\textbf{SNN}} \\
% \cmidrule(lr){3-5}
%  &  & \textbf{Train Accuracy} & \textbf{Test Accuracy} & \textbf{MIA} \\
% \midrule
% \textbf{MNIST} & Fast Sigmoid(slope,k=25) & 99.88\% & 99.22\% & 0.518(±0.001) \\
%  & aTan(alpha=2) & 99.96\% & 99.25\% &  0.5472(±0.006)  \\
%  & Spike Rate Escape(beta=1, slope=25) & 99.97\% & 99.25\% & 0.5083(±0.008) \\
%  & Triangular & 75.16\% & 76.34\% & 0.5039(±0.003) \\
%  & Straight Through Estimator & 99.57\% & 98.79\% & 0.5285(±0.005) \\
% \midrule
% \textbf{FMNIST} & Fast Sigmoid(slope,k=25) & 99.45\% & 91.97\% & 0.5181(±0.008) \\
%  & aTan(alpha=2) & 99.58\% & 92.18\% & 0.5475(±0.002) \\
%  & Spike Rate Escape(beta=1, slope=25) & 99.74\% & 92.23\% & 0.5232(±0.016) \\
%  & Triangular & 78.79\% & 79.49\% & 0.4985(±0.011) \\
%  & Straight Through Estimator & 91.55\% & 89.69\% & 0.5122(±0.003) \\
% \midrule
% \textbf{CIFAR10} & Fast Sigmoid(slope,k=25) & 82.90\% & 78.04\% & 0.5357(±0.010) \\
%  & aTan(alpha=2) & 87.56\% & 78.99\% & 0.5611(±0.010) \\
%  & Spike Rate Escape(beta=1, slope=25) & 89.92\% & 79.95\% & 0.5674(±0.013) \\
%  & Triangular & 21.43\% & 23.92\% & 0.5504(±0.036) \\
%  & Straight Through Estimator & 53.10\% & 62.49\% & 0.6445(±0.034) \\
% \midrule
% \textbf{Iris} & Fast Sigmoid(slope,k=25) & 82.50\% & 90.00\% & 0.6548(±0.022) \\ 
%  & aTan(alpha=2) & 96.67\% & 93.33\% & 0.5638(±0.095) \\
%  & Spike Rate Escape(beta=1, slope=25) & 96.67\% & 100.00\% & 0.5432(±0.036) \\
%  & Triangular & 100.00\% & 100.00\% & 0.5103(±0.029) \\
%  & Straight Through Estimator & 97.50\% & 100.00\% & 0.5427(±0.108) \\
% \midrule
% \textbf{Breast Cancer} & Fast Sigmoid(slope,k=25) & 100.00\% & 100\% & 0.4949(±0.013) \\
%  & aTan(alpha=2) & 100.00\% & 97.37\% & 0.5386(±0.015) \\
%  & Spike Rate Escape(beta=1, slope=25) & 100.00\% & 97.37\% & 0.5125(±0.026) \\
%  & Triangular & 97.58\% & 98.25\% & 0.4972(±0.022) \\
%  & Straight Through Estimator & 99.78\% & 98.25\% & 0.4823(±0.024) \\
% \bottomrule
% \end{tabular}%
% }
% \caption{Performance comparison between different surrogate gradients on SNN across datasets.}
% \label{table:sg}
% \end{table*}


\subsection{Weight Quantization}
\noindent
% Table~\ref{table:weight_quan} and Figure~\ref{fig:weight_auc} illustrate the effects of weight quantization on model performance and MIA vulnerability across datasets.

\noindent
\textit{Model Performance:} Weight quantization leads to accuracy degradation in both ANNs and SNNs  from full precision models  as shown in Table~\ref{table:weight_quan}. In ANNs, the most pronounced drop is observed in 2-bit quantized models, similar to the impact of activation quantization. Extreme quantization severely limits weight precision, resulting in coarser updates that reduce representational capacity and hinders the model’s ability to learn fine-grained patterns. This effect is evident in CIFAR-10, where accuracy declines more sharply at lower bit width of 2. The higher complexity of CIFAR-10 requires finer weight precision to capture intricate features, and 2-bit quantization struggles to retain the necessary detail for accurate classification. 

In SNNs, weight quantization similarly affects accuracy  as shown in Table~\ref{table:weight_quan}. The most pronounced drop is observed in 2-bit quantized models, where  limited weight precision reduces the model's capacity to represent detailed spike-based patterns. Like ANNs, this is especially evident in CIFAR-10 where, 2-bit quantized model struggles to encode intricate features effectively, resulting in a sharper accuracy decline compared to 4-bit and 8-bit models.

% SNNs showed comparable sensitivity to weight quantization. On CIFAR-10, accuracy decreased \hl{from 78.99\% (full precision) to 73.39\% at 8-bit and 68.59\% at 2-bit}, with similar trends observed across datasets. \hl{Lower bit-widths introduced quantization noise, impairing representational capacity and generalization}.

\textit{MIA Vulnerability:} Weight quantization consistently reduces MIA vulnerability in ANNs compared to full-precision models across all datasets (Table~\ref{table:weight_quan}). In most cases, models quantized to 2-bit, 4-bit, and 8-bit levels demonstrate comparable AUC values, effectively obscuring training data from attackers through noise introduced by quantization. Notably, 2-bit quantized models exhibit the lowest attack AUC values across datasets, representing a trend opposite to that observed with activation quantization. This difference arises from how different quantization noise affects model representations. For weight quantization, 2-bit precision disrupts parameter-level patterns, introducing randomness that hampers an attacker’s ability to infer sensitive training data. By contrast, activation quantization with 2-bit precision compresses activation ranges excessively, resulting in uniform outputs that are easier to predict, thereby increasing vulnerability.

% , with lower bit-widths yielding greater AUC reductions. On FMNIST, AUC decreased from 0.64 (full precision) to 0.56 at 8-bit, 0.54 at 4-bit, and 0.53 at 2-bit, marking reductions of 12.5\%, 15.6\%, and 17.2\%, respectively. Similar trends were observed across MNIST, CIFAR-10, and Breast Cancer, demonstrating weight quantization’s role in limiting model output granularity and reducing privacy risks.

In SNNs, weight quantization reliably lowers MIA vulnerability across all datasets compared to full-precision models  though reductions are smaller due to the already lower baseline AUC. Models quantized to 2-bit, 4-bit, and 8-bit levels demonstrate similar AUC values which indicates that weight quantization has a uniformly beneficial impact on reducing vulnerability across all levels of quantization. This trend is similar to the impact observed in activation quantization, where different bit widths also produced closely aligned AUC values. The underlying reason for this consistency lies in the stochastic nature of spike-based computation, which inherently disrupts predictable activation patterns. In SNNs, weight quantization further amplifies this effect by introducing additional noise to synaptic weights, but due to the already high variability in spike timing and membrane dynamics, the additional perturbations from quantization do not significantly alter the model’s susceptibility to MIAs.

From both accuracy and vulnerability perspectives, extreme activation quantization ( 2-bit precision) provides no tangible benefit in neither ANNs nor SNNs. Thus further reducing activation bit width beyond moderate levels (4-bit or 8-bit) is ineffective and unnecessary. 
When comparing the impact of activation and weight quantization in MIA vulnerability for ANNs and SNNs, it is evident that SNNs consistently demonstrate lower MIA vulnerability compared to their ANN counterparts, regardless of quantization levels. Remarkably, even fully precise SNN models, which theoretically should be more vulnerable than quantized models, exhibit lower privacy vulnerability than quantized ANNs at any bit width. This highlights the inherent privacy advantages of SNNs due to their stochastic spike-based encoding and temporal dynamics, which naturally disrupt predictable patterns that attackers exploit. 


% Thus, while moderate quantization (4-bit and 8-bit) effectively balances accuracy and privacy, extreme quantization to 2 bits imposes unnecessary compromises on performance without offering significant gains in reducing attack vulnerability.

% On FMNIST, AUC dropped from 0.54 (full precision) to 0.5022 at 8-bit and 0.49 at 4-bit, reflecting reductions of 7.0\% and 9.3\%. These findings suggest that SNNs, even when quantized, retain privacy resilience due to their inherent stochasticity and event-driven processing, further restricting the information available to attackers.



% \hl{MP: Why do I even need tables?! you can have figures, showing the changes with st dv near each point, write key numbers on the figures and you are done. both tables are gone. why do I care about training vs test accuracy? Use your space for analysis and key}
% \subsection{Weight Quantization}
% \noindent
% Table~\ref{table:weight_quan} and Figure~\ref{fig:weight_auc} illustrate the effects of weight quantization on model performance and MIA AUC across datasets.

% \textit{Model Performance:} Weight quantization caused noticeable performance degradation in both ANNs and SNNs as bit-width decreased. In ANNs, test accuracy consistently dropped with lower precision. For example, in CIFAR-10~\cite{cifar10}, accuracy declined from 79.20\% at full precision to 77.71\% at 4-bit and further to 73.37\% at 2-bit. FMNIST~\cite{xiao2017fashion} showed a similar trend, with accuracy dropping from 92.77\% to 90.69\% at 2-bit. These results highlight a clear trade-off between precision and performance.

% SNNs showed comparable sensitivity to quantization. In CIFAR-10~\cite{cifar10}, test accuracy dropped from 78.99\% at full precision to 73.39\% at 8-bit and further to 68.59\% at 2-bit. Similar trends were observed in other datasets, where lower bit-widths introduced quantization noise, impairing representational and generalization capabilities. 

% \textit{MIA Vulnerability:} Weight quantization in ANNs consistently reduced MIA vulnerability across all evaluated datasets, with lower bit-widths leading to greater reductions in attack AUC. For example, on FMNIST~\cite{xiao2017fashion}, the AUC decreased from 0.64 at full precision to 0.56 at 8-bit, 0.54 at 4-bit, and 0.53 at 2-bit, reflecting a reduction of 12.5\%, 15.6\%, and 17.2\%, respectively. Similar trends were observed in other datasets, such as MNIST, CIFAR-10, and Breast Cancer. This consistent reduction highlights the role of weight quantization in limiting the granularity of model outputs, thereby reducing the distinguishability of training samples and mitigating privacy risks. 

% In SNNs, weight quantization also reduced MIA vulnerability, though the impact was generally less pronounced compared to ANNs due to the lower baseline AUC of SNNs. For FMNIST~\cite{xiao2017fashion}, the AUC dropped from 0.54 at full precision to 0.5022 at 8-bit and 0.49 at 4-bit, corresponding to reductions of 7.0\% and 9.3\%, respectively. These findings suggest that SNNs, even with weight quantization, maintain an inherent level of privacy resilience likely due to their stochastic and event-driven nature, which limits the information available to attackers.


% \hl{I suggest removing all three tables and writing key numbers in figures. removing the not necessary numbers from the text and instead spend time on analyzing the results and insights}

\subsection{Surrogate Gradients}
\noindent

Surrogate gradients are evaluated from three angles: privacy vulnerability, performance, and trade-off between the two as depicted in Table~\ref{table:sg}.

From the perspective of vulnerability, Spike Rate Escape stands out as the most resilient surrogate gradient across most datasets, effectively lowering attack success rates. Its decay parameter introduces sufficient noise in spike activation patterns, making it harder for attackers to infer training data. In contrast, arctangent often demonstrates the highest vulnerability, likely due to its smoother gradient approximations that fail to sufficiently disrupt predictable patterns. Straight Through Estimator (STE) also exhibits high vulnerability in complex datasets, such as CIFAR-10, where its simplistic gradient approximation inadequately masks sensitive patterns.


In terms of performance, Fast Sigmoid consistently delivers the best results across datasets due to its sharp gradient slopes, allowing precise updates and better feature representation. In contrast, Triangular struggles significantly, performing poorly across datasets. Its weaker gradient approximations may provide insufficient feedback for  parameter updates, especially in feature-rich datasets. Additionally, its inability to leverage the stochastic nature of SNNs effectively may contribute to both lower accuracy and a failure to disrupt predictable patterns. 

When balancing vulnerability and performance, Spike Rate Escape offers the best trade-off, combining strong accuracy with consistently lower vulnerability. Its ability to integrate noise effectively complements its robust feature learning capabilities. In contrast, arctangent and STE fail to strike this balance, as their high vulnerability undermines their moderate performance, making them less effective for scenarios prioritizing both privacy and accuracy.

% The evaluation of surrogate gradients (Table~\ref{table:sg}) showed that \textit{aTan} consistently exhibited higher MIA AUC values across datasets, indicating greater vulnerability to membership inference attacks. In FMNIST, \textit{aTan} reached an AUC of 0.5475, exceeding \textit{Fast Sigmoid} (0.5181) and \textit{Spike Rate Escape} (0.5232). Similar trends were observed in MNIST and Breast Cancer, where \textit{aTan} produced AUCs of 0.5472 and 0.5386, respectively, suggesting that its smooth gradient approximation facilitates greater information leakage.

% In contrast, \textit{Fast Sigmoid} and \textit{Spike Rate Escape} consistently achieved lower AUCs while maintaining high accuracy. On Breast Cancer, these gradients yielded test accuracies of 98.25\% and 97.38\% with AUCs of 0.4949 and 0.5152, respectively. The \textit{Straight Through Estimator} (STE) showed inconsistent results, performing particularly poorly on CIFAR-10, where it recorded a high AUC of 0.6445 alongside reduced accuracy, indicating sensitivity to complex datasets. Notably, the \textit{Triangular} gradient exhibited the worst accuracy across all datasets, suggesting limited utility in scenarios requiring strong model performance. \hl{These findings suggest that \textit{Spike Rate Escape} or \textit{Fast Sigmoid} are preferable choices for applications requiring both performance and privacy resilience}.




% \subsection{Surrogate Gradients}
% \noindent
% The evaluation of surrogate gradients, summarized in Table~\ref{table:sg}, revealed that \textit{aTan} consistently exhibited higher MIA AUC values across datasets, indicating greater vulnerability to membership inference attacks. For instance, in F-MNIST~\cite{xiao2017fashion}, \textit{aTan} achieved an AUC of 0.5475, higher than \textit{Fast Sigmoid} (0.5181) and \textit{Spike Rate Escape} (0.5232). Similar trends were observed in MNIST~\cite{lecun2010mnist} and Breast Cancer~\cite{misc_breast_cancer_14}, where \textit{aTan} produced AUCs of 0.5472 and 0.5386, respectively. This suggests that \textit{aTan}'s smoother gradient approximation facilitates greater information leakage, aiding adversaries in distinguishing training data.

% Other gradients, such as \textit{Fast Sigmoid} and \textit{Spike Rate Escape}, consistently achieved lower MIA AUCs while maintaining high accuracy. For example, in the Breast Cancer dataset~\cite{misc_breast_cancer_14}, these gradients yielded test accuracies of 98.25\% and 97.38\% with AUCs of 0.4949 and 0.5152, respectively. The \textit{Straight Through Estimator} (STE) demonstrated inconsistent performance, with particularly poor results in CIFAR-10, where it recorded a high AUC of 0.6445 alongside reduced accuracy, highlighting sensitivity to complex datasets.

% Among the evaluated gradients, the \textit{Spike Rate Escape} emerged as the most balanced, offering reliable accuracy and robust privacy protection across datasets. While \textit{aTan}'s smooth gradient approximation increases MIA vulnerability, \textit{Fast Sigmoid} provided a dependable alternative with strong privacy characteristics. These findings suggest that \textit{Spike Rate Escape} or \textit{Fast Sigmoid} are better suited for applications requiring both performance and privacy resilience.



