\section{Background}
\subsection{Spiking Neural Networks (SNNs)}
\noindent
SNNs are inspired by biological neural activity and represent a fundamental shift from traditional ANNs' continuous outputs. SNNs operate through discrete spikes, occurring only when a neuron’s membrane potential surpasses a specific threshold. This mechanism incorporates time as an additional dimension in information processing, where spike timing patterns encode neural representations~\cite{schuman2022opportunities}. This spike-based mode of communication supports asynchronous data processing and aligns well with neuromorphic hardware designed for event-driven computation, offering enhanced energy efficiency and reduced latency~\cite{schuman2022opportunities}. SNNs require alternative training mechanisms to overcome the non-differentiability of their spike function~\cite{schuman2022opportunities}.

Surrogate gradients provide a workaround for the non-differentiable spike function by substituting a smooth surrogate function in the backward pass. The function approximates the spiking neuron’s activation and is used to compute gradients during backpropagation. Common surrogate gradients include:

\begin{itemize}
    \item \textbf{Fast Sigmoid:} Approximates the gradient using a sharp sigmoid function. It provides precise gradient updates, enhancing feature representation and learning stability.
    
    \item \textbf{Arctangent:} Employs the derivative of the arctangent function as the gradient, offering smoother updates. However, its less aggressive slope makes it less effective in disrupting predictable patterns.

    \item \textbf{Spike Rate Escape:} A gradient model based on a sigmoid-like function with a decay parameter, effectively introducing noise in spike activations. This makes it useful for improving privacy resilience.

    \item \textbf{Triangular:} Uses a linear approximation for gradients. Its weaker gradient strength often leads to instability in learning.

    \item \textbf{Straight Through Estimator (STE):} Utilizes the gradient of a fast sigmoid function during the backward pass, while maintaining a unit derivative for simplicity. This method balances computational efficiency with gradient approximations.
\end{itemize}





% \begin{itemize}
    % \item \textbf{Sigmoid}: Provides a smooth gradient using the sigmoid function:
    % \[
    % \frac{\partial S(x)}{\partial x} \approx \frac{1}{1 + e^{-\beta (x - \theta)}},
    % \]
    % where \( x \) is the membrane potential, \( \theta \) the firing threshold, and \( \beta \) controls slope.

%     \item \textbf{Fast Sigmoid}: A computationally efficient version of sigmoid, \( \kappa \) controls the slope of the gradient::
%     % \begin{equation}
%     %         \frac{\partial S(x)}{\partial x} \approx \frac{x}{1 + |x|}.
%     % \end{equation}

%     \begin{equation}
%         \frac{\partial S(x)}{\partial x} = \frac{1}{\left( 1+\kappa.\left| x \right| \right)^2}
%     \end{equation}
    
%     \item \textbf{ATan}: Uses the arctangent function as an approximation, with \( \alpha \) determining the steepness of the gradient transition:  
%     \begin{equation}
%         \frac{\partial S(x)}{\partial x} \approx \frac{1}{1 + (\alpha \cdot x)^2}.
%     \end{equation}

%     \item \textbf{Straight Through Estimator (STE)}: commonly used surrogate gradient method that approximates the gradient of the Heaviside step function during training. By assuming a unit derivative, STE simplifies the gradient computation, enabling backpropagation through non-differentiable spike events. 
    
%     % unit derivative
%     % \begin{equation}
%     %     \frac{\partial S(x)}{\partial x} \approx 1.
%     % \end{equation}

%     \item \textbf{Triangular}: piecewise linear gradient
%     \begin{equation}
%         \frac{\partial S(x)}{\partial x} \approx \max(0, 1 - |x|).
%     \end{equation}

%     \item \textbf{Spike Rate Escape}: Boltzmann distribution:
%     \begin{equation}
%             \frac{\partial S(x)}{\partial x} \approx \frac{e^{-\beta x}}{(1 + e^{-\beta x})^2},
%     \end{equation}
%     where $\beta$ adjusts the sharpness.
% \end{itemize}
Surrogate functions enable gradient flow in SNNs despite the non-differentiability of spike events. They introduce variability through spike timing distributions and gradient approximations, a mechanism aligning with DPSGD, where privacy relies on systematic noise injection during training. So unlike DPSGD’s explicit noise addition, SNNs   inherently generate randomness, which may provide privacy benefits without incurring the performance penalties of DPSGD..









% These surrogate functions enable gradient flow through SNNs, supporting effective learning despite the non differentiability of spike events. The inherent stochasticity and temporal sparsity of SNN activations generate output variability that may serve as a natural privacy preserving mechanism. This characteristic shares fundamental principles with DPSGD, where privacy preservation relies on systematic noise injection during training. Unlike DPSGD's explicit noise addition, SNNs' spike based computations produce natural randomness through both spike timing distributions and surrogate gradient approximations. Such intrinsic variability suggests potential privacy advantages without incurring the performance penalties typically associated with DPSGD implementations. 
\subsection{Membership Inference Attack (MIA)}

% \hl{Your Figure 1 is exactly the same as your Figure 2 in PoPET paper. This is not allowed. you need permission to reuse figure, even your own figure. please redraw the key pieces.}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth, height=0.55\linewidth]{Figure/mia_1sm.pdf}
    % \includegraphics[width=\linewidth]{Figure/mia_1sm.png}
    \caption{Membership Inference Attack (MIA) Framework}
    \label{fig:mia_24}
\end{figure}
\noindent  
MIA is a privacy attack that enables adversaries to infer the presence of individual data samples in a ML model's training dataset~\cite{shokri}. This attack exploits differences in the model's behavior on training and non-training data.~\cite{rahman2018membership}. Models tend to exhibit higher confidence or different error patterns for samples they have encountered during training, compared to new unseen data~\cite{nguyen2015deep}. By analyzing these patterns, attackers can infer sensitive information about training data, which can lead to privacy breaches~\cite{de2020overview}.

MIA involves training attack models or employing statistical tests to distinguish between the model responses on training and non-training data. The success of these attacks largely depends on the model's overfitting to its training data and the distinctiveness of the model's responses to individual samples. MIAs not only breach data privacy but also expose weaknesses in a model's ability to generalize~\cite{gomm2000case}.

The MIA framework in our experiments involves a two-model approach (Figure~\ref{fig:mia_24}) consisting of a target model and a shadow model, each of the same architecture described in Section~\ref{subsec:model_arch_datasets}.  The process begins with a target model trained on the dataset of interest, which the adversary aims to analyze. To approximate the target model's behavior, a shadow model is trained on an 80\% subset of the same dataset, mimicking the target’s architecture and learning parameters. The attack training dataset is generated using the shadow model, where predictions on its training data are labeled as 'IN' and those on its test data as 'OUT.' The attack test dataset is then constructed from the target model’s predictions, following the same labeling scheme. An SVM with a Radial Basis Function (RBF) kernel serves as the attack model, trained on the constructed attack dataset. For ANNs, logits from the fully connected layer are used as input features, while for SNNs, membrane potentials from the final time step are utilized. To mitigate class imbalance, undersampling is applied during training. The trained attack model is then tested on the target model’s predictions, determining whether a given sample was part of the training set ('IN') or not ('OUT'), quantifying its vulnerability to MIAs.

% \hl{MP: You need to describe the figure. you have this figure in your paper without describing it at all..}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\linewidth, height=0.50\linewidth]{Figure/activation/activation_acc.pdf}
    % \includegraphics[width=\linewidth]{Figure/mia_1sm.png}
    \caption{Activation Quantization impact on Model Accuracy on (a) Breast Cancer (b) CIFAR-10, (c) F-MNIST, and (d) MNIST. The grey solid line represents the Full Precision (FP) model, while the purple dotted, orange dashed, and blue dash-dotted lines correspond to the quantized models with bit precisions of w=8, w=4, and w=2 respectively.} 
    \label{fig:act_acc}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\linewidth, height=0.50\linewidth]{Figure/activation/activation_auc.pdf}
    % \includegraphics[width=\linewidth]{Figure/mia_1sm.png}
    \caption{Activation Quantization impact on Privacy Vulnerability  on (a) Breast Cancer (b) CIFAR-10, (c) F-MNIST, and (d) MNIST. The grey solid line represents the Full Precision (FP) model, while the purple dotted, orange dashed, and blue dash-dotted lines correspond to the quantized models with bit precisions of w=8, w=4, and w=2 respectively.}
    \label{fig:act_auc}
\end{figure*}

\subsection{Quantization}
% \subsection{\textbf{Surrogate Gradient}}
\noindent
Quantization reduces the precision of model parameters (weights) and layer activations, typically from 32-bit floating point to lower bit integers~\cite{guo2018survey}. This technique lowers memory usage, accelerates inference, and reduces energy consumption, facilitating the deployment of Deep Neural Networks (DNNs) on resource-constrained devices~\cite{krestinskaya2023towards}. Additionally, the noise inherently introduced by quantization acts as an implicit regularization mechanism, mitigating overfitting and improving generalization~\cite{kang2024effect}.


In this work, we investigate weight quantization and activation quantization and their impact on privacy in ANNs and SNNs.

\textbf{Activation Quantization:} This method reduces the precision of neuron outputs, constraining activations to discrete levels:
\begin{equation}
    a_q = \text{round}\left(\frac{\text{clip}(a, a_{\text{min}}, a_{\text{max}}) - a_{\text{min}}}{\Delta}\right) \cdot \Delta + a_{\text{min}}.
\end{equation}
Here, \( \Delta = \frac{a_{\text{max}} - a_{\text{min}}}{2^k - 1} \) ensures uniform quantization within the activation range \([a_{\text{min}}, a_{\text{max}}]\), where \( k \) represents the bit width of quantization.

\textbf{Weight Quantization:} Weight quantization maps weights \( w \) to a discrete range, optimizing storage and computation:
\begin{equation}
    Q(w) = \Delta \cdot \text{clip}\left(\text{round}\left(\frac{w}{\Delta}\right), 0, 2^k - 1\right),
\end{equation}
where \( \Delta = \frac{w_{\text{max}} - w_{\text{min}}}{2^k - 1} \) defines the step size for a bit-width \( k \), ensuring both positive and negative weights are accounted for.


% \textbf{Weight Quantization:} Weight quantization maps weights \( w \) to a discrete integer range, optimizing storage and computational efficiency. A common formulation for this process is:
% \begin{equation}
%     Q(w) = \Delta \cdot \text{clip}\left(\text{round}\left(\frac{w}{\Delta}\right), 0, 2^k - 1\right),
% \end{equation}
% where \( \Delta = \frac{w_{\text{max}} - w_{\text{min}}}{2^k - 1} \) is the step size, \( w_{\text{max}} \) and \( w_{\text{min}} \) are the range of the weights, and \( k \) is the bit-width. For datasets or models with weights spanning both positive and negative values, the range 
% \([w_{\text{min}}, w_{\text{max}}]\) ensures that negative weights are also accounted for during quantization. This transition reduces precision by representing weights within a constrained range, enabling efficient computation on hardware designed for lower precision arithmetic.
\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.5}  % Adjust row spacing
\setlength{\tabcolsep}{4pt}  % Adjust column spacing
\begin{center}
\caption{Model Architecture and Configuration}
\label{tab:architectures}
\begin{tabular}{p{1cm}p{1cm}p{1cm}p{4.5cm}}  % Adjusted column widths
\hline
\textbf{Dataset} & \textbf{Network} & \textbf{Model} & \textbf{Structure} \\
\hline
CIFAR10 \newline MNIST \newline FMNIST & ConvNet & ANN & 
2 Conv layers (32, 64 filters) \newline
2 MaxPool layers \newline
2 FC layers (1000, num\_classes) \newline
ReLU activations \\[8pt]
\cline{3-4}
 &  & SNN & 
2 Conv layers (32, 64 filters) \newline
2 MaxPool layers \newline
LIF neurons ($\beta = 0.95$) \newline
Temporal processing ($T = 25$) \\[8pt]

\hline
Iris \newline Breast-Cancer & FCNet & ANN & 
2 FC layers (Input, 1000, num\_classes) \newline
ReLU activations \\[8pt]
\cline{3-4}
 &  & SNN & 
2 FC layers (Input, 1000, num\_classes) \newline
LIF neurons ($\beta = 0.95$) \newline
Temporal processing ($T = 25$) \\
\hline
\end{tabular}
\end{center}
\end{table}





% \begin{table}[t]
% \footnotesize
% \setlength{\tabcolsep}{4pt}  % Adjust column spacing
% \begin{center}
% \caption{Model Architectures and Configurations}
% \label{tab:architectures}
% \begin{tabular}{p{1
% cm}p{0.9cm}p{4.5cm}p{1.1cm}}  % Adjusted column widths
% \hline
% Network & Variant & Structure & Params* \\
% \hline
% \multicolumn{4}{c}{} \\
% ConvNet & ANN & $\bullet$ 2 Conv layers (32, 64 filters) \newline
%        $\bullet$ 2 MaxPool layers \newline
%        $\bullet$ 2 FC layers (1000, num\_classes) \newline
%        $\bullet$ ReLU activations & $\sim$2.3M \\[8pt]

% \hline
% \multicolumn{4}{c}{} \\
%  & SNN & $\bullet$ 2 Conv layers (32, 64 filters) \newline
%        $\bullet$ 2 MaxPool layers \newline
%        $\bullet$ LIF neurons ($\beta = 0.95$) \newline
%        $\bullet$ Temporal processing ($T = 25$) & $\sim$2.3M \\[8pt]
% \hline
% \multicolumn{4}{c}{} \\
% FCNet & ANN & 
%        $\bullet$ 2 FC layers (Input, 1000, num\_classes) \newline
%        $\bullet$ ReLU activations & 8K–33K \\[8pt]
% \hline
% \multicolumn{4}{c}{} \\
%  & SNN & 
%        $\bullet$ 2 FC layers (Input, 1000, num\_classes) \newline
%        $\bullet$ LIF neurons ($\beta = 0.95$) \newline
%        $\bullet$ Temporal processing ($T = 25$) & 8K–33K \\
% \hline
% \multicolumn{4}{c}{} \\
% \end{tabular}
% {\small
% \begin{flushleft}
% *Params vary with input channels (1/3) for ConvNet or input features for FCNet.
% \end{flushleft}}
% \end{center}
% \end{table}

\begin{table*}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l c c c c c c c}
\toprule
\textbf{Dataset} & \textbf{Method} & \multicolumn{3}{c}{\textbf{ANN}} & \multicolumn{3}{c}{\textbf{SNN}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
 &  & \textbf{Train Accuracy} & \textbf{Test Accuracy} & \textbf{MIA AUC} & \textbf{Train Accuracy} & \textbf{Test Accuracy} & \textbf{MIA AUC} \\
\midrule
\textbf{MNIST} & Full precision & 99.96(±0.03)\% & 99.21(±0.01)\% & 0.5900(±0.008) & 99.95(±0.05)\% & 99.22(±0.02)\% & 0.5264(±0.011) \\
 & quantized(w=8) & 99.88(±0.05)\% & 99.16(±0.02)\% & 0.5674(±0.007) & 99.93(±0.03)\% & 99.16(±0.03)\% & 0.5101(±0.012) \\
 & quantized(w=4) & 99.85±(0.04)\% & 99.12(±0.04)\% & 0.5800(±0.003) & 99.91(±0.02)\% & 99.21(±0.05)\% & 0.5117±(0.009) \\
 & quantized(w=2) & 99.57(±0.02)\% & 99.07(±0.03)\% & 0.5667(±0.009) & 99.88±(0.03)\% & 99.11(±0.04)\% & 0.4993(±0.005) \\
\midrule
\textbf{FMNIST} & Full precision & 99.52(±0.13)\% & 92.77(±0.20)\% & 0.6400(±0.011) & 99.42(±0.32)\% & 92.44(±0.19)\% & 0.5453(±0.008) \\
 & quantized(w=8) & 95.98(±0.08)\% & 92.30(±0.10)\% & 0.5397(±0.010) & 96.49(±0.91)\% & 92.00(±0.10)\% & 0.5022(±0.003) \\
 & quantized(w=4) & 95.47(±0.72)\% & 92.15(±0.09)\% & 0.5406(±0.005) & 96.44(±0.53)\% & 91.97(±0.17)\% & 0.4970(±0.010) \\
 & quantized(w=2) & 92.61(±0.20)\% & 90.69(±0.18)\% & 0.5295(±0.004) & 93.37(±0.51)\% & 91.36(±0.18)\% & 0.5024(±0.005) \\
\midrule
\textbf{CIFAR10} & Full precision & 99.24(±0.08)\% & 79.20(±0.43)\% & 0.8200(±0.095) & 99.13(±0.45)\% & 78.99(±0.33)\% & 0.5927(±0.005) \\
 & quantized(w=8) & 79.61(±0.20)\% & 77.78(±0.28)\% & 0.6800(±0.015) & 72.37(±0.44).\% & 73.39(±0.13)\% & 0.5250(±0.022) \\
 & quantized(w=4) & 79.56(±0.41)\% & 77.71(±0.39)\% & 0.7000(±0.021) & 72.54(±0.29)\% & 72.92(±0.51)\% & 0.5405(±0.011) \\
 & quantized(w=2) & 71.35(±0.24)\% & 73.37(±0.15)\% & 0.6200(±0.015) & 66.41(±1.09)\% & 68.59(±0.55)\% & 0.5280(±0.002) \\
\midrule
\textbf{Iris} & Full precision & 98.33(±2.34)\% & 100.0(±0.00)\% & 0.7700(±0.13) & 100.0(±0.00)\% & 100.0(±0.00)\% & 0.5728(±0.020) \\
 & quantized(w=8) & 82.78(±2.02)\% & 93.33(±2.94)\% & 0.6500(±0.070) & 79.44(±7.33)\% & 95.56(±1.92)\% & 0.5234(±0.012) \\
 & quantized(w=4) & 85.56(±1.05)\% & 91.11(±1.92)\% & 0.7163(±0.160) & 84.17(±2.47)\% & 94.44(±3.06)\% & 0.5304(±0.045) \\
 & quantized(w=2) & 84.06(±2.20)\% & 91.11(±3.06)\% & 0.6400(±0.013) & 81.94(±4.18)\% & 92.22(±6.71)\% & 0.5380(±0.020) \\
\midrule
\textbf{Breast Cancer} & Full precision & 99.34(±0.04)\% & 98.25(±0.00)\% & 0.6500(±0.008) & 100.0(±0.00)\% & 98.25(±0.24)\% & 0.5534(±0.017) \\
 & quantized(w=8) & 97.51(±0.13)\% & 97.96(±0.04)\% & 0.5500(±0.006) & 99.09(±0.64)\% & 
 98.25(±0.00)\% & 0.4983(±0.003) \\
 & quantized(w=4) & 97.58(±0.57)\% & 97.96(±0.49)\% & 0.5404(±0.010) & 97.69(±0.36)\% & 97.66(±0.51)\% & 0.5099(±0.021) \\
 & quantized(w=2) & 97.80(±0.46)\% & 97.54(±0.40)\% & 0.5800(±0.013) & 97.87(±0.48)\% & 
 97.37(±0.00)\% & 0.5088(±0.021) \\
\bottomrule
\end{tabular}%
}
\caption{Weight Quantization Impact on Privacy Vulnerability and Model Performance: Comparison between ANN and SNN models across different datasets.}
\label{table:weight_quan}
\end{table*}

Quantized models require gradient approximation during training. In ANNs, Quantization-Aware Training (QAT) updates quantized parameters while maintaining gradient flow. In SNNs, non-differentiable spike functions necessitate surrogate gradients approximations. This study applies QAT  with \textit{fast sigmoid} to examine their impact on privacy vulnerability while maintaining accuracy.


% \textbf{Activation Quantization:} Activation quantization reduces the precision of neuron outputs, helping further optimize memory and computational resources. The quantized activation \( a_q \) can be represented as:
% \begin{equation}
%     a_q = \text{round}\left(\frac{\text{clip}(a, a_{\text{min}}, a_{\text{max}}) - a_{\text{min}}}{\Delta}\right) \cdot \Delta + a_{\text{min}},
%     \label{eq:activ_quant}
% \end{equation}
% where \( a_{\text{min}} \) and \( a_{\text{max}} \) define the range of activation values, and \( \Delta = \frac{a_{\text{max}} - a_{\text{min}}}{2^k - 1} \) is the step size, determined by the bit-width \( k \). This ensures that activations are quantized uniformly within the interval \([a_{\text{min}}, a_{\text{max}}]\). By constraining activations to discrete levels, computations are made more efficient, particularly during inference on hardware optimized for lower precision.

% For ANNs, techniques like the \textit{Straight Through Estimator (STE)} are commonly used in Quantization Aware Training (QAT) to approximate gradients through quantized layers by treating the quantization operation as linear during backpropagation~\cite{hubara2018quantized}:
% \begin{equation}
%     \frac{\partial Q(x)}{\partial x} \approx 1.
% \end{equation}

% In SNNs, however, alternative surrogate gradients, such as \textit{arctangent} or \textit{sigmoid} based methods, are employed during training to approximate the gradients of non differentiable spike functions.  In this work, we incorporate QAT, ensuring that models adapt to quantized activations during training. Importantly, the selected surrogate gradient is applied consistently throughout the training process, directly influencing both model optimization and final performance, with the goal of enhancing resilience against privacy attacks without substantial accuracy loss.
