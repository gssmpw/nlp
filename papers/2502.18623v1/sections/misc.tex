As machine learning systems handle increasingly sensitive data, the potential for privacy violations grows. Li et al.    ~\cite{liu2021machine} categorize these privacy challenges into two primary areas: privacy attacks and privacy-preserving techniques. Privacy attacks have emerged as a critical concern in ML due to the growing realization that models can act as unintended leak vectors. These attacks can broadly be classified into different types, such as model inversion attacks, model extraction attacks, MIA. Model inversion attacks~\cite{fredrikson2015model} reconstruct input data from model outputs, while model extraction attacks ~\cite{juuti2019prada} aim to replicate a model’s functionality without direct access to its architecture or parameters. Among these, MIAs have gained significant attention due to their ability to infer whether a specific data point was used in training a model. According to the survey conducted by ~\cite{hu2022membership}, MIAs were first proposed in the context of genomic data by Homer et al.~\cite{homer2008resolving} where an attacker could infer if an individual's genome was part of a dataset based on summary statistics. Later, Shokri et al.\cite{shokri} introduced the first systematic MIA framework, showing how adversaries could use shadow models to infer training data membership. Salem et al. ~\cite{salem2018ml} reduced the complexity by demonstrating that a single shadow model can perform well compared to using multiple models, and they introduced metric-based attacks that rely on confidence scores and entropy without the need for identical data distribution between shadow and target models. Nasr et al.~\cite{nasr2019comprehensive} further expanded MIA into white-box settings, demonstrating that attackers with access to internal model parameters can perform even more effective MIAs. Melis et al.~\cite{melis2019exploiting} extended MIA to federated learning, showing vulnerabilities even in distributed learning settings, where multiple parties collaboratively train a model. Song and Mittal~\cite{song2021systematic} highlighted the increased privacy risks in generative models such as GANs, where membership inference attacks could be carried out on synthetic data generators. Recent work by Ilyas et al. introduced LiRA (Likelihood Ratio Attack) ~\cite{carlini2022membership}, a method that further improves the accuracy of MIAs by leveraging confidence scores more effectively to distinguish between training and non-training data points.In 2024, Zarifzadeh et al.~\cite{zarifzadeh2024low} introduced RMIA, a high-power membership inference attack that outperforms prior methods like LiRA and Attack-R, demonstrating superior robustness, particularly at low FPRs, using likelihood ratio tests.
While much of the data privacy research has centered around ANNs, expanding these investigations to SNNs is necessary. SNNs not only offer performance levels comparable to ANNs but also exhibit superior energy efficiency and hardware integration capabilities, positioning them as promising candidates for exploring inherent privacy features. Although privacy attacks on neuromorphic architectures remain underexplored, existing studies have yet to confirm SNNs' potential resistance to such threats. However, significant strides have been made in privacy-preserving techniques within the neuromorphic domain. For instance, recent efforts by Han et al. ~\cite{han2023towards} focus on developing privacy-preserving methods for SNNs, particularly utilizing FL and DP to address both computational efficiency and privacy challenges. Li et al.\cite{li2023efficient} introduced a framework that combines Fully Homomorphic Encryption (FHE) with SNNs, enabling encrypted inference while preserving SNNs' energy efficiency and computational advantages. Similarly, Nikfam et al.\cite{nikfam2023homomorphic} developed an HE framework tailored for SNNs, offering enhanced accuracy over DNNs under encryption, while carefully balancing computational efficiency. Additionally, Safronov et al.\cite{kim2022privatesnn} proposed PrivateSNN, a privacy-preserving framework for SNNs that employs differential privacy to mitigate membership inference attacks, maintaining the energy-efficient nature of SNNs.


The escalating computational demands of deep neural networks (DNNs), characterized by increased parameter counts and complex operations, have propelled quantization as a leading approach for reducing model size and computational load. Quantization compresses neural networks by reducing the precision of parameters, such as weights and activations, which lowers the memory footprint and enables efficient deployment on resource-constrained devices without substantial performance loss \cite{yang2019quantization, nagel2021white}. Early work on quantization focused on inference, using methods such as post-training quantization (PTQ) and quantization-aware training (QAT), which apply low-bit representations across entire network layers \cite{jacob2018quantization}. Recent advancements, like adaptive quantization frameworks, have further enhanced efficiency by dynamically adjusting bit-widths per layer, achieving higher compression rates while preserving accuracy \cite{tran2024privacy, zhou2021balanced}.

Quantization has emerged not only as an efficiency tool but also as a subtle defense mechanism in privacy-sensitive applications. This line of inquiry is particularly important given the vulnerability of DNNs to membership inference attacks (MIA), where adversaries exploit model confidence to infer whether specific data was part of the training set \cite{kowalski2022towards}. Quantized models, by introducing noise and reducing overfitting, have shown inherent resilience against such attacks, suggesting a potential dual role for quantization in enhancing privacy \cite{famili2023deep}. Experimental results have demonstrated that quantized models yield lower precision in MIA success rates compared to full-precision models, reducing the adversary’s ability to extract sensitive information without compromising model accuracy \cite{wei2024q, hu2021quantized}.

In recent years, these quantization techniques have been adapted for Spiking Neural Networks (SNNs), a neuromorphic approach inspired by biological neurons, which operate efficiently in edge computing environments due to their low energy consumption and sparse activity patterns. The development of quantization frameworks tailored to SNNs, such as Q-SpiNN and QFFS, has addressed unique challenges in quantizing synaptic weights, membrane potentials, and spike dynamics, crucial for maintaining the model’s accuracy and efficiency on low-power devices \cite{putra2021q, li2022quantization}. These frameworks use methods like integer quantization and fixed-point representation to optimize computational loads while preserving the distinctive temporal dynamics of SNNs, thus achieving a balance between efficiency and accuracy in neuromorphic hardware.

The privacy implications of quantized SNNs, however, remain an under-explored frontier. Given the inherent sparsity and quantized nature of SNNs, these networks hold promise for robust privacy preservation against attacks like MIA. By constraining information representation and introducing noise, quantized SNNs may naturally reduce the leakage pathways adversaries exploit. This potential resilience provides strong motivation for further investigation into quantized SNNs as a privacy-preserving framework, paving the way for more secure neuromorphic models in sensitive applications. This work aims to build on these foundational insights, exploring the intersection of quantization, efficiency, and privacy within SNNs \cite{schaefer2020quantizing, stock2019and, kowalski2022towards}.



% \begin{figure*}[ht]
%     \centering
    
%     % First row - MNIST and F-MNIST
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_mnist_acc.png}
%         \caption*{ANN}
%         \label{fig:mnist_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_mnist_acc.png}
%         \caption*{SNN}
%         \label{fig:mnist_snn}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_fmnist_acc.png}
%         \caption*{ANN}
%         \label{fig:fmnist_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_fmnist_acc.png}
%         \caption*{SNN}
%         \label{fig:fmnist_snn}
%     \end{subfigure}
%     \caption*{(a) MNIST \hspace{8cm} (b) F-MNIST}
    
%     % Second row - CIFAR-10 and CIFAR-100
%     \vspace{1em}
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_cifar10_acc.png}
%         \caption*{ANN}
%         \label{fig:cifar10_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_cifar10_acc.png}
%         \caption*{SNN}
%         \label{fig:cifar10_snn}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_bc_acc.png}
%         \caption*{ANN}
%         \label{fig:cifar100_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_bc_acc.png}
%         \caption*{SNN}
%         \label{fig:cifar100_snn}
%     \end{subfigure}
%     \caption*{(c) CIFAR-10 \hspace{8cm} (d) Breast Cancer}
    
%     % Final caption for the entire figure
%     \caption{Activation Quantization impact on Model Accuracy in (a) MNIST, (b) F-MNIST, (c) CIFAR-10, and (d) Breast Cancer.}
%     \label{fig:act_acc}
% \end{figure*}


% \begin{figure*}[ht]
%     \centering
    
%     % First row - MNIST and F-MNIST
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_mnist_auc.png}
%         \caption*{ANN}
%         \label{fig:mnist_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_mnist_auc.png}
%         \caption*{SNN}
%         \label{fig:mnist_snn}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_fmnist_auc.png}
%         \caption*{ANN}
%         \label{fig:fmnist_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_fmnist_auc.png}
%         \caption*{SNN}
%         \label{fig:fmnist_snn}
%     \end{subfigure}
%     \caption*{(a) MNIST \hspace{8cm} (b) F-MNIST}
    
%     % Second row - CIFAR-10 and CIFAR-100
%     \vspace{1em}
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_cifar10_auc.png}
%         \caption*{ANN}
%         \label{fig:cifar10_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_cifar10_auc.png}
%         \caption*{SNN}
%         \label{fig:cifar10_snn}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/ann_bc_auc.png}
%         \caption*{ANN}
%         \label{fig:cifar100_ann}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/activation/snn_bc_auc.png}
%         \caption*{SNN}
%         \label{fig:cifar100_snn}
%     \end{subfigure}
%     \caption*{(c) CIFAR-10 \hspace{8cm} (d) Breast Cancer}
    
%     % Final caption for the entire figure
%     \caption{Activation Quantization impact on MIA AUC score in (a) MNIST, (b) F-MNIST, (c) CIFAR-10, and (d) Breast Cancer.}
%     \label{fig:act_acc}
% \end{figure*}



% \begin{figure*}[ht!]
%   \centering
%   % First row - MNIST and F-MNIST
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/ann_mnist_w_quant.png}
%     \caption*{ANN}
%     \label{fig:mnist_ann}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/snn_mnist_w_quant.png}
%     \caption*{SNN}
%     \label{fig:mnist_snn}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/ann_fmnist_w_quant.png}
%     \caption*{ANN}
%     \label{fig:fmnist_ann}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/snn_fmnist_w_quant.png}
%     \caption*{SNN}
%     \label{fig:fmnist_snn}
%   \end{subfigure}%
%   \caption*{(a) MNIST \hspace{8cm} (b) F-MNIST}
  
%   % Second row - CIFAR-10 and Iris
%   \vspace{1em}
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/ann_cifar10_w_quant.png}
%     \caption*{ANN}
%     \label{fig:cifar10_ann}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/snn_cifar10_w_quant.png}
%     \caption*{SNN}
%     \label{fig:cifar10_snn}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/ann_iris_w_quant.png}
%     \caption*{ANN}
%     \label{fig:iris_ann}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/snn_iris_w_quant.png}
%     \caption*{SNN}
%     \label{fig:iris_snn}
%   \end{subfigure}%
%   \caption*{(c) CIFAR-10 \hspace{8cm} (d) Iris}

%   % Third row - Breast Cancer only, centered
%   \vspace{1em}
%   \hspace{\fill} % Centering space
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/ann_bc_w_quant.png}
%     \caption*{ANN}
%     \label{fig:bc_ann}
%   \end{subfigure}%
%   \hspace{0.5cm} 
%   \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figure/weight/snn_bc_w_quant.png}
%     \caption*{SNN}
%     \label{fig:bc_snn}
%   \end{subfigure}%
%   \hspace{\fill} % Centering space
%   \caption*{(e) Breast Cancer}

%   % Final caption for the entire figure
%   \caption{Impact of weight quantization on ANN and SNN models across different datasets: (a) MNIST, (b) F-MNIST, (c) CIFAR-10, (d) Iris, and (e) Breast Cancer.}
%   \label{fig:dp_mia}
% \end{figure*}