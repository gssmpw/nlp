\section{Experimental Setup}

% \hl{MP: I'm not sure about calling this section, Research methods. There is not much details on the method or theroy. Maybe you can rename it to "Experimental Setup"}

% This section describes the experimental framework, including model architectures, datasets, quantization methods, and surrogate gradient configurations. All implementations utilized PyTorch~\cite{ketkar2021introduction} and SNNtorch~\cite{snntorch}.

\subsection{Model Architectures and Datasets}\label{subsec:model_arch_datasets}

\noindent
This study evaluates CIFAR10~\cite{cifar10}, MNIST~\cite{lecun2010mnist}, FMNIST~\cite{xiao2017fashion}, Iris~\cite{omelina2021survey}, and Breast Cancer~\cite{misc_breast_cancer_14}, implemented using PyTorch~\cite{ketkar2021introduction} and SNNtorch~\cite{eshraghian2021training}. For CIFAR10, MNIST, and FMNIST we follow their standard splits (50,000 training, 10,000 testing), while for Iris and Breast Cancer we apply an 80-20 stratified split.

We preprocess CIFAR10 by normalizing the data to a mean of [0.5, 0.5, 0.5], applying random cropping with a 4-pixel padding and 50\% horizontal flipping. For MNIST and FMNIST, we resize the images to 28×28 pixels, convert them to grayscale, and normalize them with a mean of 0 and standard deviation of 1, without augmentations. For Iris and Breast Cancer datasets, we standardize the features using \texttt{StandardScaler}~\cite{scikit-learn}.

We use Convolutional Neural Networks (CNNs) (ConvNet) for CIFAR10, MNIST, and FMNIST , and Fully Connected Networks (FCNets) for Iris and Breast Cancer. We summarize the detailed configurations for both ANNs and SNNs in Table~\ref{tab:architectures}. For activation functions, we use ReLU for ANNs and Leaky Integrate and Fire (LIF) neurons (\( \beta = 0.95\)) for SNNs, which operate over a time step of 25 to propagate spikes.
% Experiments use Convolutional Neural Networks (CNNs) for CIFAR10, MNIST, and FMNIST~\cite{recht2018cifar}, and Fully Connected Networks (FCNets) for Iris and Breast Cancer~\cite{misc_breast_cancer_14}.  Detailed configurations for both ANNs and SNNs are provided in Table~\ref{tab:architectures}. ANNs use ReLU activations, while SNNs replace them with \texttt{Leaky} neurons (\( \beta = 0.95\)) and operate with 25 time steps for spike propagation.


% \hl{MP: table 1 needs reformatting. It is too hard to understand. Where is dataset? For ConvNET you should have CIFAR10, MNIST, and FMNIST, for fully you have Iris and breast cancer. Also if convent in the table applies to both ANN and SNN the line under convnet with a box empty right below it doesn't make sense. for FCNET, Input 1000 is right next to params making it hard to distinguish between them.. bullet points also does not make sense in tables. This table should be reformatted. }
% \subsection{Model Architectures and Datasets}\label{subsec:model_arch_datasets}

% \noindent
% This study evaluates methods on CIFAR10~\cite{cifar10}, MNIST~\cite{lecun2010mnist}, FMNIST~\cite{xiao2017fashion}, Iris~\cite{omelina2021survey}, and Breast Cancer~\cite{misc_breast_cancer_14}. All implementations utilized PyTorch~\cite{ketkar2021introduction} and SNNtorch~\cite{snntorch}. CIFAR10, MNIST, and FMNIST follow their standard splits, with 50,000 samples allocated for training and 10,000 for testing. The Iris and Breast Cancer datasets are divided into stratified splits, with 80\% used for training and 20\% for testing, to preserve class distributions.

% Preprocessing ensures consistency across datasets. CIFAR10 images are normalized to a mean of [0.5, 0.5, 0.5] and a standard deviation of [0.5, 0.5, 0.5], with data augmentation applied during training through random cropping (padding: 4 pixels) and horizontal flipping (probability: 50\%). MNIST and FMNIST images are resized to (28×28), converted to grayscale, and normalized to a mean of 0 and a standard deviation of 1, without augmentations. Features in the Iris and Breast Cancer datasets are standardized using \texttt{StandardScaler}~\cite{scikit-learn}.

% Experiments use two model architectures: Convolutional Neural Networks (CNNs) for CIFAR10, MNIST, and FMNIST~\cite{recht2018cifar}, and Fully Connected Networks (FCNets) for Iris and Breast Cancer~\cite{misc_breast_cancer_14}. Detailed configurations of these architectures for both ANNs and SNNs are summarized in Table~\ref{tab:architectures}. The CNN for ANNs includes two convolutional layers (32 and 64 filters, stride 1), followed by max-pooling layers (2x2 kernel, stride 2), a fully connected layer (1000 units), and a ten-class output layer. SNN models replicate this structure, replacing activations with spiking neuron dynamics using \texttt{Leaky} neurons (\( \beta = 0.95\)). For quantization studies, SNN training used \textit{Fast Sigmoid} gradients~\cite{snntorch}, while the surrogate gradient study explored five methods: \textit{Fast Sigmoid}, \textit{Straight Through Estimator}, \textit{Arctangent}, \textit{Spike Rate Escape}, and \textit{Triangular}. Each SNN experiment used 25 time steps to propagate spikes through the network.

\subsection{Quantization Methods}

% \hl{MP: Why don't you clearly start paragraphs one with weight quantization and the other with activation? to follow the same structure as your previous section? }

% \hl{MP: In this section you have described first weight quantization and then activation quantization. In the result and figures you first have activation quantization. Please be consistent}

% \hl{MP: What is your gradient of choice in SNNs when you do activation quantization here? In ANNs you had QAT. In SNNs you said you applied QAT with different surrogate gradients. in Fig. 3 where you have activation quantization what surr. gradient did you use for SNNs?}

% \hl{MP: In figure, it is really hard to see the lines. trends are clear both colors and dashed lines are too vague. For full precision you can show it with solid line and you need to also write it in the caption.}

% \hl{MP: Lines in Fig 4 are solid, in Fig 3 are dashed. I prefer Fig 4 to 3. Please be as consistent as possible and make them all solid. }

% \hl{MP: In figure 4 it is hard to see FP line. Please make the FP line a bit wider in both figures}

% \hl{MP: What is w in activation quantization figures 2 and 3?? :D w is weight? shouldn't that be for weight quantization (figure 4 only)?}

% \hl{MP: What is FRR and TRR in Figyre 4? The axes titles are too small. you need to increase font size for the axes numbers and titles}

\noindent
We implement activation quantization differently for ANNs and SNNs. In ANNs, we quantize activations using \texttt{brevitas.nn} QuantReLU~\cite{brevitas}, while in SNNs, we develop a custom \texttt{state\_quant} function within snnTorch~\cite{eshraghian2021training} to quantize membrane potentials. To assess the effect of activation precision, we apply thresholds ranging from 0.5 to 1.5 before quantization, clipping activations and setting an upper bound on magnitudes included in the quantized representation. Lower thresholds impose stricter clipping, reducing the range of preserved activations and potentially limiting representational capacity. In contrast, higher thresholds retain a broader range of activations, which may enhance expressiveness but could also increase vulnerability to privacy attacks.

We apply weight quantization using \texttt{brevitas.nn} for both ANNs and SNNs, reducing parameter precision to 2-bit, 4-bit, and 8-bit levels. This process discretizes model weights, minimizing storage requirements and computational cost while potentially influencing privacy vulnerability.
% Two quantization techniques were applied: weight quantization using \texttt{brevitas.nn}~\cite{brevitas} for both ANNs and SNNs, and activation quantization, implemented differently for each architecture. For ANNs, activations were quantized using \texttt{brevitas.nn} QuantReLU, while for SNNs, a custom \texttt{state\_quant} function was developed within snnTorch~\cite{snntorch} \hl{to quantize membrane potentials.}
% Experiments were conducted at 2-bit, 4-bit, and 8-bit precision to analyze how reduced numerical representation affects both model accuracy and privacy vulnerability. For Activation Quantization, thresholds ranging from 0.5 to 1.5 were applied to clip activation values before quantization, setting the upper bound on activation magnitudes included in the quantized representation. Lower thresholds impose stricter clipping, reducing the range of preserved activations and potentially limiting representational capacity. In contrast, higher thresholds retain a broader range of activations, which may enhance expressiveness but could also increase vulnerability to privacy attacks.

\subsection{Surrogate Gradient Configurations}

\noindent
We use five surrogate gradients in our experiments: \textit{Fast Sigmoid}, \textit{Straight Through Estimator}, \textit{Arctangent}, \textit{Spike Rate Escape}, and \textit{Triangular}. Among these, we specifically employ \textit{Fast Sigmoid} for quantization studies~\cite{eshraghian2021training}. We set the \textit{Fast Sigmoid} gradient slope to 25, while  \textit{Arctangent (atan)}  uses an alpha value of 2 which controls the steepness of its curve. For \textit{Spike Rate Escape}, we apply a beta parameter of 1 which regulates the escape rate with a slope of 25. We use \textit{Triangular} and \textit{Straight Through Estimator (STE)} with their default configurations in the framework.
\noindent



% \section{Research Methods}
% This section outlines the experimental framework, describing model architectures, datasets, quantization methods, and surrogate gradient configurations employed in our analysis. PyTorch~\cite{ketkar2021introduction} and snnTorch~\cite{snntorch} frameworks facilitated the implementation.
% \subsection{Model Architectures and Datasets}\label{subsec:model_arch_datasets}
% This work evaluates the proposed methods on CIFAR10~\cite{cifar10}, MNIST~\cite{lecun2010mnist}, FMNIST~\cite{xiao2017fashion}, Iris~\cite{omelina2021survey}, and Breast Cancer~\cite{misc_breast_cancer_14} datasets. CIFAR10, MNIST, and FMNIST follow their standard splits, with 50,000 samples allocated for training and 10,000 for testing. The Iris and Breast Cancer datasets are divided into stratified splits, with 80\% used for training and 20\% for testing, to preserve class distributions.
% Each dataset are preprocessed to ensure consistency and enhance model training. For CIFAR10, images are normalized to a mean of [0.5, 0.5, 0.5] and a standard deviation of [0.5, 0.5, 0.5]. Data augmentation during training includes random cropping with a padding of 4 pixels and horizontal flipping are applied with a probability of 50\%. For MNIST and FMNIST, images are resized to (28×28), converted to grayscale, and normalized to a mean of 0 and a standard deviation of 1, with no further augmentations applied. The Iris and Breast Cancer features undergo standardization using \texttt{StandardScaler}~\cite{scikit-learn}. 


% The experiments utilized two model architectures across distinct datasets: Convolutional Neural Networks (CNNs) for CIFAR10~\cite{recht2018cifar}, MNIST~\cite{lecun2010mnist}, FMNIST~\cite{xiao2017fashion}, and Fully Connected Networks (FCNets) for the Iris~\cite{omelina2021survey} and Breast Cancer datasets~\cite{misc_breast_cancer_14}. 

% The CNN architecture for ANNs comprises two convolutional layers (32 and 64 filters, respectively) with a stride of 1, followed by max-pooling layers with a 2x2 kernel and a stride of 2. This is followed by a fully connected layer with 1000 units and a ten-class output layer. SNN models replicated this structure while incorporating spiking neuron dynamics using \texttt{Leaky} neurons (\( \beta = 0.95\)). For the quantization study, training for SNNs was performed with surrogate gradient descent, specifically employing the \textit{Fast Sigmoid} algorithm~\cite{snntorch} to approximate the gradient of the spike function. For the surrogate gradient exploration study, however, we evaluated the performance and privacy characteristics across five distinct surrogate gradient methods: \textit{Fast Sigmoid}, \textit{Straight Through Estimator}, \textit{Arctangent}, \textit{Spike Rate Escape}, and \textit{Triangular}. Each SNN experiment employed a time step of 25 to allow spike propagation through the network.

% \subsection{Quantization Methods}
%  Two types of quantization were implemented: weight quantization using the \texttt{brevitas.nn} package~\cite{brevitas} for both ANN and SNN models, and activation quantization which was implemented differently for each architecture. For ANNs, activations were quantized using \texttt{brevitas.nn} QuantReLU, while for SNNs, a custom \texttt{state\_quant} function was developed within SNNtorch~\cite{snntorch} to quantize membrane potentials.

% \textit{Weight Quantization}
% Weight quantization was configured with bit settings of 2-bit, 4-bit, and 8-bit, uniformly applied across network layers. The quantization for both ANNs and SNNs was managed through the \texttt{qnn.QuantLinear} module from Brevitas, ensuring consistent bit depth settings across architectures.

% \textit{Activation Quantization}
% Activation quantization involved varying both bit depth and threshold levels to accommodate dynamic activation ranges as seen in equation \eqref{eq:activ_quant}. Thresholds were set at values of 0.5-1.5 to evaluate the model’s response across different ranges. Activation levels were monitored across experiments, including thresholds ranging from 0.5 to 1.5, to ensure stability and consistency in the model's quantized responses.

% \subsection{Surrogate Gradient Configurations}
% The training of SNNs utilized six surrogate gradient functions from the SNNtorch framework to approximate spiking neuron activations effectively. Each function was configured with specific parameters influencing training dynamics. The \textit{Fast Sigmoid} surrogate gradient was each configured with a slope of 25, controlling gradient sharpness near the spiking threshold. For the \textit{Arctangent (atan)} surrogate, an alpha value of 2 controls the gradient responsiveness, defining how sharply the gradient changes in response to inputs near the spiking threshold. A higher gradient responsiveness (steeper slope) enables stronger weight updates for inputs close to the threshold, facilitating faster learning but potentially increasing sensitivity to noise. The \textit{Spike Rate Escape} function was set with a beta parameter of 1, indicating neuron sensitivity to input variations, and a slope of 25 to modulate gradient steepness. The \textit{Triangular} and \textit{Straight Through Estimator} functions were tested with their default configurations. These gradient variations provided flexibility in modeling spiking behaviors across quantized setups, ensuring robust performance.

% \subsection{MIA}
% The MIA framework is implemented through a two-model approach, as depicted in Figure~\ref{fig:mia_24}. The experimental setup consists of a 'target' model and a 'shadow' model, each mirroring the architectural configurations previously described in the Section~\ref{subsec:model_arch_datasets}. The shadow model is trained on 80\% of a dataset comparable to that of the target model, maintaining identical structural parameters to emulate the target’s learning characteristics. A Support Vector Machine (SVM) is utilized as the attack model due to its efficacy in binary classification, enabling the distinction between 'member' and 'non-member' instances. The attack model uses logits from the final layer as input features. For ANNs, these are raw outputs from the fully connected layer, while for SNNs, they correspond to the final layer's membrane potentials at the last time step. The SVM employs a Radial Basis Function (RBF) kernel, functioning as a soft-margin classifier to tolerate some misclassifications for better generalization. Probability estimation is enabled to provide probabilistic outputs, supporting nuanced decision-making in the attack. Evaluation is conducted by querying each model with both training and test data, where predictions are labeled as 'IN' if the data is part of the training set and 'OUT' otherwise. To address potential bias caused by class imbalance, undersampling is applied to the dominant class, ensuring balanced training for the SVM model. By training the SVM on the labeled outputs from the shadow model, this approach quantifies the model’s vulnerability to MIA, as reflected by the SVM’s accuracy in classifying membership status.