\section{Result}

% 主要实验结果
\subsection{Main results}

In this part, to compare our KnowPath with other state-of-the-art methods from mainstreams, we conducted comprehensive experiments on four widely used knowledge-based question answering datasets. The experimental results are presented in Table \ref{tab:comparison}, and four key findings are outlined as follows:

\textbf{KnowPath performs the best.}
As we can see, our KnowPath outperforms all Prompting-driven KG-Enhanced LLMs.
For instance, on the multi-hop knowledge graph question answering dataset CWQ, regardless of the base model used, KnowPath achieves a maximum improvement of about 13\% in Hits@1 (ToG 60.9 ± 0.7\% vs KnowPath 73.5 ± 0.9\%).
In addition, KnowPath outperforms LLM-only methods with a clear margin and surpasses the majority of Fine-Tuned KG-Enhanced LLM methods.
On the most challenging open-domain question answering dataset WebQuestions, KnowPath achieves the best performance compared to strong baselines from other paradigms (e.g., PoG 61.2\% vs Ours 64.0\%). This demonstrates KnowPath's ability to enhance the factuality of LLMs in open-domain question answering, which is an intriguing phenomenon worth further exploration.

\textbf{KnowPath excels at complex multi-hop tasks.}
On both CWQ and WebQSP datasets, KnowPath outperforms the latest strong baseline PoG, achieving an average improvement of approximately 5\% and 2.9\%, respectively.
On the WebQSP dataset, DeepSeek-v3 with KnowPath not only outperforms all Prompting-based KG-Enhanced LLMs but also surpasses the strongest baseline ROG among Fine-Tuned KG-Enhanced LLMs (85.7\% vs 89\%). 
On the more challenging multi-hop CWQ dataset, the improvement of KnowPath over the strong baseline PoG is significantly greater than the improvement on the simpler single-hop task, Simple Questions (5.2\% vs 1.4\%).
These phenomena collectively indicate that KnowPath is sensitive to deep reasoning.

\textbf{Knowledge enhancement greatly aids factual question answering.}
When question answering is based solely on LLMs, the performance is poor across multiple tasks. For example, COT achieves only about 20.5\% Hits@1 on the simple single-hop task, Simple Questions.
This is caused by the hallucinations inherent in LLMs.
Whatever method is applied to introduce the knowledge graph, all approaches significantly outperform LLM-only. The maximum improvements across the four tasks are 35.9\%, 27.9\%, 46.4\%, and 15.3\%, with the least improvements being 22.5\%, 17.2\%, 41\%, and 9.7\%, respectively.
These observations further emphasize the importance of introducing knowledge graphs for generating correct answers with LLMs.

\textbf{The stronger the base model, the better the performance.}
Since DeepSeek-V3 is more advanced than GPT-3.5, even though both are prompting-based knowledge-enhanced methods, their performance on all tasks shows a significant difference after incorporating our KnowPath.
Replacing GPT-3.5 with DeepSeek-V3, KnowPath achieved a maximum improvement from 67.9\% to 73.5\% on the CWQ dataset, and on Simple Questions, it improved by at least 3.8 percentage points.
These findings indicate that the improvement in model performance directly drives the enhancement of its performance in knowledge-based question-answering.

\textbf{KnowPath is a more flexible plugin.}
Compared to fine-tuned knowledge-enhanced LLMs, our KnowPath does not require fine-tuning of the LLM, yet it outperforms most of the fine-tuned methods.
In addition, on the CWQ dataset, KnowPath with DeepSeek-V3 achieves performance that is very close to the strongest baseline, ChatKBQA, which requires fine-tuning for knowledge enhancement. On the WebQSP dataset, it outperforms ChatKBQA by about 11\% (78.1\% vs 89.0\%).
Overall, the resource consumption of KnowPath is significantly lower than that of Fine-Tuned KG-Enhanced LLMs.
This is because KnowPath improves performance by optimizing inference paths and enhancing knowledge integration, making it a more flexible and plug-and-play framework.


% 消融实验表格
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{  % 将表格宽度调整为单栏宽度
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{CWQ} & \textbf{WebQSP} & \textbf{SimpleQA} & \textbf{WebQ} \\
\midrule
KnowPath & 73.5 & 89.0 & 65.3 & 64.0 \\
-w/o IPG & 67.3 & 84.5 & 63.1 & 61.0 \\
-w/o SE & 64.7 & 83.1 & 60.4 & 60.7 \\
Base & 39.2 & 66.7 & 23.0 & 53.7 \\
\bottomrule
\end{tabular}
}
\caption{Ablation Experiment Results on four knowledge-based question answering tasks. IPG stands for Inference Paths Generation module, while SE stands for Subgraph Exploration module.}
\label{table:ablation}
\end{table}
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{  % 将表格宽度调整为单栏宽度
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{LLM Call} & \textbf{Total Token} & \textbf{Input Token}\\
\midrule
ToG & 22.6 & 9669.4 & 8182.9  \\
PoG & 16.3 & 8156.2 & 7803.0 \\
KnowPath & \textbf{9.9} & \textbf{2742.4} & \textbf{2368.9} \\
\bottomrule
\end{tabular}
}
\caption{Cost-effectiveness analysis on the CWQ dataset between our KnowPath and the strongly prompt-driven knowledge-enhanced benchmarks (ToG and PoG). The Total Token includes two parts: the total number of tokens from multiple input prompts and the total number of tokens from the intermediate results returned by the LLM. The Input Token represents only the total number of tokens from the multiple input prompts. The LLM Call refer to the total number of accesses to the LLM agent.}
\label{table:tokens}
\end{table}
% 消融实验分析
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figure/Ablation.pdf}
  \caption{Comparison of KnowPath, its individual components, and strong baseline methods (ToG and PoG) on the performance across four commonly used knowledge-based question answering datasets.}
  \label{fig:ablation}
\end{figure}
\subsection{Ablation Study}
In this section, we validate the effectiveness of each component of KnowPath and quantify their contributions to performance. The ablation experiment results on four tasks are presented in the Table \ref{table:ablation}, and visualized in the Figure \ref{fig:ablation}.

\textbf{Each component contributes to the overall remarkable performance.}
After removing each module, their performance on different datasets will decline. However, compared to the base model, the addition of these modules still significantly improves the overall performance.


\textbf{It is necessary to focus on the powerful internal knowledge of LLMs.}
Eliminating the Subgraph Exploration module and relying solely on the internal knowledge mining of LLMs to generate reasoning paths and provide answers proves to be highly effective. 
It has shown significant improvement across all four datasets, with an average performance enhancement of approximately 21.6\%. The most notable improvement was observed on SimpleQA, where performance leaped from 23\% to 60.4\%.
This indicates that even without the incorporation of external knowledge graphs, the performance of the model in generating factual responses can be enhanced to a certain extent through internal mining methods.
However, without the guidance of internal knowledge reasoning paths, KnowPath has seen some performance decline across all tasks, especially in complex multi-hop question answering tasks like CWQ and WebQSP.

\textbf{The most critical credible directed Subgraph Exploration is sensitive to deep reasoning.}
Removing the subgraph exploration module leads to a significant decline in Knowpath's performance across all tasks, averaging a drop of approximately 5.7\%. This performance dip is particularly pronounced in complex multi-hop tasks. For instance, on the CWQ dataset, Knowpath without subgraph exploration experiences a nearly 9\% decrease (from 73.5\% to 64.7\%).
This shows that the Subgraph Exploration is depth-sensitive.



\begin{figure*}[h]
  \centering
  \includegraphics[width=0.24\linewidth]{figure/CWQ_efficiency.png}
  % \hspace{0.01\linewidth}
  \includegraphics[width=0.24\linewidth]{figure/WebQSP_efficiency.png}
  \hspace{0.01\linewidth}
  \includegraphics[width=0.24\linewidth]{figure/SimpleQA_efficiency.png}
  % \hspace{0.01\linewidth}
  \includegraphics[width=0.24\linewidth]{figure/WebQ_efficiency.png}
  \caption{Visualization of the cost-effectiveness analysis.}
  \label{fig:efficiency-analysis}
\end{figure*}
\subsection{Cost-effectiveness Analysis}

To explore the cost-effectiveness of KnowPath while maintaining high accuracy, we conducted a cost-benefit analysis experiment. In this experiment, we tracked the primary sources of cost, including the LLM Call, Input Token, and Total Token usage. The results are presented in the Table \ref{table:tokens}, and the data trends are visualized in Figure \ref{fig:efficiency-analysis}.
Our key findings are described as follows:


\textbf{The number of accesses to the LLM agent was significantly reduced}. Specifically, the number of LLM calls for TOG and POG was 2.28x and 1.64x of that in our KnowPath, respectively.
This exceptionally low cost can be attributed to the fact that the subgraph exploration module does not limit the scale of the path search, and this can be broken down into three key reasons. First, in each round of subgraph exploration, only one relation exploration and one entity exploration are conducted, resulting in only two accesses to the LLM agent. Second, the dual-driven response module only accesses the LLM once after each round of subgraph exploration to evaluate whether the current subgraph can answer the question. If it cannot, the next round of subgraph updates is performed. Third, if the largest explored subgraph still cannot answer the question, KnowPath will rely on the results of Inference Paths Generation.


\textbf{The number of tokens used is saved by several times.}
Whether in Total Token or Input Tokens, KnowPath saves several times compared to TOG and POG, approximately 4.0x. 
This is mainly due to the fact that all the prompts used in KnowPath are based on the carefully designed zero-shot approach, rather than the in-context learning used by previous baseline methods, which require providing a large amount of context to ensure the factuality of the answers.
We explored the reasons behind this difference.
First, previous methods rely on more contextual information for in-context learning to ensure the correctness of the output.
Secondly, KnowPath is different in that it fully leverages the powerful internal relevant knowledge of the LLM and uses it as the input signal for the LLM agent.
This not only provides more contextual reference information but also significantly improves the accuracy and efficiency of relation and entity exploration in subgraph exploration, ensuring that the generated subgraph is highly relevant to the question while enabling the most effective reasoning toward potential answers.





% 参数分析
\subsection{Parameter analysis}

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\linewidth]{figure/Temperature.png}
    \caption{Exploration temperature}
      \label{fig:tempreture}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\linewidth]{figure/Triples.png}
    \caption{The count of triples}

    \label{fig:triple}
  \end{subfigure}
  % \hspace{0.01\linewidth}
  \caption{Parameter analysis.}
  \label{fig:Parameter}
\end{figure}
\begin{figure*}[t]
  \centering
    \includegraphics[width=0.98\linewidth]{figure/case1.pdf}
  % \hspace{0.01\linewidth}
  \caption{Case Study.}
  \label{fig:case}
\end{figure*}



In this section, we analyze the key parameters that affect the performance of KnowPath on the WebQSP dataset, and discuss the following issues:

\textbf{What is the impact of the temperature coefficient during sub-graph exploration?}
We explore the optimal temperature from 0.2 to 1, and the variation trend between the temperature and Hits@1 is shown in Figure \ref{fig:tempreture}.
During subgraph exploration, variations in the temperature affect the divergence of the model's generated answers. A lower temperature negatively impacts KnowPath's performance, as the model generates overly conservative answers with insufficient knowledge, while the LLM relies on its internal knowledge when exploring and selecting entities and relationships. A higher temperature also harms KnowPath's performance, as the divergent answers may deviate from the given candidate set.
Extensive experiments show that 0.4 is the optimal temperature, consistent with other existing works.


\textbf{How is the count of knowledge triples relied upon for Inference Paths Generation determined?}
We conducted four sets of exploration with a step size of 15, and the relationship between the count of knowledge triples and Hits@1 is shown in Figure \ref{fig:tempreture}.
When the count is 0, KnowPath's performance is poor due to the lack of internal knowledge exploration. When the count is too large, such as 45, its performance is also suboptimal, as excessive exploration introduces irrelevant knowledge as interference. Extensive experiments show that 15 is the optimal.

\subsection{Case Study}


To provide a clear and vivid comparison with the strong baselines, we visualized the execution process of KnowPath, as shown in Figure \ref{fig:case}. 
In the CWQ, ToG and PoG can only extract context from the question, failing to gather enough accurate knowledge for a correct answer, thus producing the incorrect answer "Taiping Jing." In contrast, KnowPath uncovers large model reasoning paths that provide additional, sufficient information. This enables key nodes, such as "Taoism," to be identified during subgraph exploration, ultimately leading to the correct answer, "Zhuang Zhou."
In the WebQuestions, ToG is unable to answer the question due to insufficient information. 
Although PoG provides a reasoning chain, the knowledge derived from the reasoning process is inaccurate, and the final answer still relies on the reasoning of the large model, resulting in the incorrect answer "Blackburn Rovers."
In contrast, guided by Inference, KnowPath accurately identified the relationship "time.event.instance\_of\_recurring\_event" and, through reasoning with the node "2002-03-Football League Cup," ultimately arrived at the correct result node "Liverpool F.C."
Overall, KnowPath not only provides answers but also generates directed subgraphs, which serve as the foundation for trustworthy reasoning and significantly enhance the interpretability of the results.