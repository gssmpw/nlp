\section{Result}

% 主要实验结果
\subsection{Main results}

We conducted comprehensive experiments on four widely used knowledge-based question answering datasets. The experimental results are presented in Table \ref{tab:comparison}, and four key findings are outlined as follows:


\textbf{KnowPath performs the best.}
Our KnowPath outperforms all the Prompting-driven KG-Enhanced.
For instance, on the multi-hop CWQ, regardless of the base model used, KnowPath achieves a maximum improvement of about 13\% in Hits@1.
In addition, KnowPath outperforms the LLM-only with a clear margin and surpasses the majority of Fine-Tuned KG-Enhanced LLM methods.
On the most challenging open-domain question answering dataset WebQuestions, KnowPath achieves the best performance compared to strong baselines from other paradigms (e.g., PoG 61.2\% vs Ours 64.0\%). This demonstrates KnowPath's ability to enhance the factuality of LLMs in open-domain question answering, which is an intriguing phenomenon worth further exploration.


\textbf{KnowPath excels at complex multi-hop tasks.}
On both CWQ and WebQSP, KnowPath outperforms the latest strong baseline PoG, achieving an average improvement of approximately 5\% and 2.9\%, respectively.
On the WebQSP, DeepSeek-v3 with KnowPath not only outperforms all Prompting-based KG-Enhanced LLMs but also surpasses the strongest baseline ROG among Fine-Tuned KG-Enhanced LLMs (85.7\% vs 89\%). 
On the more challenging multi-hop CWQ, the improvement of KnowPath over the PoG is significantly greater than the improvement on the simpler single-hop SimpleQuestions (5.2\% vs 1.4\%).
These collectively indicate that KnowPath is sensitive to deep reasoning.


\textbf{Knowledge enhancement greatly aids factual question answering.}
When question answering is based solely on LLMs, the performance is poor across multiple tasks. For example, COT achieves only about 20.5\% Hits@1 on SimpleQuestions.
This is caused by the hallucinations inherent in LLMs.
Whatever method is applied to introduce the KGs, they significantly outperform LLM-only. 
The maximum improvements across the four tasks are 35.9\%, 27.9\%, 46.4\%, and 15.3\%. 
% with the least improvements being 22.5\%, 17.2\%, 41\%, and 9.7\%, respectively.
These further emphasize the importance of introducing knowledge graphs for generating correct answers.



\textbf{The stronger the base, the higer the performance.}
As DeepSeek-V3 is better than GPT-3.5, even though both are prompting-based knowledge-enhanced, their performance on all tasks shows a significant difference after incorporating our KnowPath.
Replacing GPT-3.5 with DeepSeek-V3, KnowPath achieved a maximum improvement from 67.9\% to 73.5\% on CWQ, and on Simple Questions, it improved by at least 3.8\%.
These findings indicate that the improvement in model performance directly drives the enhancement of its performance in knowledge-based question-answering.


\textbf{KnowPath is a more flexible plugin.}
Compared to fine-tuned knowledge-enhanced LLMs, our KnowPath does not require fine-tuning of the LLM, yet it outperforms most of the fine-tuned methods.
In addition, on the CWQ dataset, KnowPath with DeepSeek-V3 achieves performance that is very close to the strongest baseline, ChatKBQA, which requires fine-tuning for knowledge enhancement. On the WebQSP dataset, it outperforms ChatKBQA by about 11\% (78.1\% vs 89.0\%).
Overall, the resource consumption of KnowPath is significantly lower than that of Fine-Tuned KG-Enhanced LLMs.
This is because KnowPath improves performance by optimizing inference paths and enhancing knowledge integration, making it a more flexible and plug-and-play framework.


% 消融实验表格
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{  % 将表格宽度调整为单栏宽度
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{CWQ} & \textbf{WebQSP} & \textbf{SimpleQA} & \textbf{WebQ} \\
\midrule
KnowPath & 73.5 & 89.0 & 65.3 & 64.0 \\
-w/o IPG & 67.3 & 84.5 & 63.1 & 61.0 \\
-w/o SE & 64.7 & 83.1 & 60.4 & 60.7 \\
Base & 39.2 & 66.7 & 23.0 & 53.7 \\
\bottomrule
\end{tabular}
}
\caption{Ablation experiment results on four knowledge-based question answering tasks. IPG stands for Inference Paths Generation module, while SE stands for Subgraph Exploration module.}
\label{table:ablation}
\end{table}
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{  % 将表格宽度调整为单栏宽度
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{LLM Call} & \textbf{Total Token} & \textbf{Input Token}\\
\midrule
ToG & 22.6 & 9669.4 & 8182.9  \\
PoG & 16.3 & 8156.2 & 7803.0 \\
KnowPath & \textbf{9.9} & \textbf{2742.4} & \textbf{2368.9} \\
\bottomrule
\end{tabular}
}
\caption{Cost-effectiveness analysis on the CWQ dataset between our KnowPath and the strongly prompt-driven knowledge-enhanced benchmarks (ToG and PoG). The Total Token includes two parts: the total number of tokens from multiple input prompts and the total number of tokens from the intermediate results returned by the LLM. The Input Token represents only the total number of tokens from the multiple input prompts. The LLM Call refer to the total number of accesses to the LLM agent.}
\label{table:tokens}
\end{table}
% 消融实验分析
\begin{figure}[h]
  \centering
  \includegraphics[width=0.94\linewidth]{figure/Ablation.pdf}
  \caption{Comparison of KnowPath, its individual components, and strong baseline methods (ToG and PoG) on the performance across four commonly used knowledge-based question answering datasets.}
  \label{fig:ablation}
\end{figure}
\subsection{Ablation Study}
We validate the effectiveness of each component of KnowPath and quantify their contributions to performance.
Its results are presented in Table \ref{table:ablation}, and visualized in Figure \ref{fig:ablation}.
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.24\linewidth]{figure/CWQ_efficiency.pdf}
  % \hspace{0.01\linewidth}
  \includegraphics[width=0.24\linewidth]{figure/WebQSP_efficiency.pdf}
  \hspace{0.01\linewidth}
  \includegraphics[width=0.24\linewidth]{figure/SimpleQA_efficiency.pdf}
  % \hspace{0.01\linewidth}
  \includegraphics[width=0.24\linewidth]{figure/WebQ_efficiency.pdf}
  \caption{Visualization of the cost-effectiveness analysis on four public knowledge-based question-answering datasets.}
  \label{fig:efficiency-analysis}
\end{figure*}
\begin{figure}[t]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\linewidth]{figure/Temperature.pdf}
    \caption{Exploration temperature}
      \label{fig:tempreture}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\linewidth]{figure/Triples.pdf}
    \caption{The count of triples}

    \label{fig:triple}
  \end{subfigure}
  % \hspace{0.01\linewidth}
  \caption{Analysis of key parameters.}
  \label{fig:Parameter}
\end{figure}

\begin{figure*}[t]
  \centering
    \includegraphics[width=0.98\linewidth]{figure/case1.pdf}
  % \hspace{0.01\linewidth}
  \caption{The case study on the multi-hop CWQ and open-domain WebQuestions dataset. To provide a clear and vivid comparison with the strong baselines (ToG and PoG), we visualized the execution process of KnowPath}
  \label{fig:case}
\end{figure*}


%每个模块均有贡献
\textbf{Each component contributes to the overall remarkable performance.}
After removing each module, their performance on different datasets will decline. However, compared to the base model, the addition of these modules still significantly improves the overall performance.


\textbf{It is necessary to focus on the powerful internal knowledge of LLMs.}
Eliminating the Subgraph Exploration and relying solely on the internal knowledge mining of LLMs to generate reasoning paths and provide answers proves to be highly effective. 
It has shown significant improvement across all four datasets, with an average performance enhancement of approximately 21.6\%. The most notable improvement was observed on SimpleQA, where performance leaped from 23\% to 60.4\%.
This indicates that even without the incorporation of external knowledge graphs, the performance of the model in generating factual responses can be enhanced to a certain extent through internal mining methods.
However, without the guidance of internal knowledge reasoning paths, KnowPath has seen some performance decline across all tasks, especially in complex multi-hop CWQ and WebQSP.


\textbf{The most critical credible directed Subgraph Exploration is deep-sensitive.}
Removing the subgraph exploration leads to a significant decline in Knowpath across all tasks, averaging a drop of approximately 5.7\%. This performance dip is particularly pronounced in complex multi-hop tasks. For instance, on the CWQ, Knowpath without subgraph exploration experiences a nearly 9\% decrease.






\subsection{Cost-effectiveness Analysis}

To explore the cost-effectiveness of KnowPath while maintaining high accuracy, we conducted a cost-benefit analysis. In this experiment, we tracked the primary sources of cost, including the LLM Call, Input Token, and Total Token usage. The results are presented in the Table \ref{table:tokens}, and are visualized in Figure \ref{fig:efficiency-analysis}.
Our key findings are described as follows:


\textbf{The number of accesses to the LLM agent was significantly reduced.} Specifically, the LLM calls for TOG and POG was 2.28x and 1.64x of that in our KnowPath, respectively.
This exceptionally low cost can be attributed to the fact that the Subgraph Exploration does not limit the scale of the path search, and this can be broken down into three key reasons. First, in each round of subgraph exploration, only one relation exploration and one entity exploration are conducted. Second, the Evaluation-based answering only accesses the LLM once after each round of subgraph exploration to judge whether the current subgraph can answer the question. If it cannot, the next round is performed. Third, if the largest explored subgraph still cannot answer the question, KnowPath will rely on the Inference Paths Generation.



\textbf{The number of tokens used is saved by several times.}
Whether in Total Token or Input Tokens, KnowPath saves approximately 4.0x compared to TOG and POG. 
This is mainly since all the prompts used in KnowPath are based on the carefully designed zero-shot approach, rather than the in-context learning used by the previous, which require providing large context to ensure the factuality of the answers.
We explored the reasons behind this difference.
First, previous methods rely on more contextual information for in-context learning to ensure the correctness of the output.
Secondly, KnowPath fully leverages the powerful internal relevant knowledge and uses it as the input signal for the agent.
This not only provides more contextual reference but also significantly improves the accuracy and efficiency of relation and entity exploration in subgraph exploration, ensuring that the generated subgraph is highly relevant while enabling the most effective reasoning toward potential answers.



% 参数分析
\subsection{Parameter analysis}





% 在这个部分，我们分析了影响KnowPath性能的重要参数，探讨了以下问题：
% 子图探索时的温度系数有何影响？
% 在子图探索时，温度系数的变化会影响模型生成答案的发散程度。较低的温度系数会影响KnowPath的性能，因为模型生成的保守答案知识较少，而大模型在探索和选择实体与关系时需要依赖一定的自身知识。较高的温度系数也会损害KnowPath的性能，因为发散的答案可能偏离给定的候选集合。
% 大量实验表明0.4是最佳温度，正与其他现有工作一样
We analyze the key parameters that affect the performance of KnowPath on the WebQSP, and discuss the following issues:

\textbf{What is the impact of the temperature in Subgraph Exploration?}
We explore the optimal temperature from 0.2 to 1, and the relation between it and Hits@1 is shown in Figure \ref{fig:tempreture}.
During subgraph exploration, variations in the temperature affect the divergence of the model's generated answers. A lower temperature negatively impacts KnowPath's performance, as the model generates overly conservative answers with insufficient knowledge, while the LLM relies on its internal knowledge when exploring and selecting entities and relationships. A higher temperature also harms KnowPath, as the divergent answers may deviate from the given candidates.
Extensive experiments show that 0.4 is the optimal temperature, consistent with other existing works~\cite{pog}.


\textbf{How is the count of knowledge triples determined in Inference Paths Generation?}
We explored it with a step size of 15, and the relationship between the count of knowledge triples and Hits@1 is shown in Figure \ref{fig:tempreture}.
When the count is 0, KnowPath's performance is poor due to the lack of internal knowledge exploration. When the count is too large, such as 45, its performance is also suboptimal, as excessive exploration introduces irrelevant knowledge as interference. Extensive experiments show that 15 is the optimal.

\subsection{Case Study}



To provide a clear and vivid comparison with the strong baselines, we visualized the execution process of KnowPath, as shown in Figure \ref{fig:case}. 
In the CWQ, ToG and PoG can only extract context from the question, failing to gather enough accurate knowledge for a correct answer, thus producing the incorrect answer "Taiping Jing." In contrast, KnowPath uncovers large model reasoning paths that provide additional, sufficient information. This enables key nodes, such as "Taoism," to be identified during subgraph exploration, ultimately leading to the correct answer, "Zhuang Zhou."
In the WebQuestions, ToG is unable to answer the question due to insufficient information. 
Although PoG provides a reasoning chain, the knowledge derived from the reasoning process is inaccurate, and the final answer still relies on the reasoning of the large model, resulting in the incorrect answer "Blackburn Rovers."
In contrast, guided by Inference, KnowPath accurately identified the relationship "time.event.instance\_of\_recurring\_event" and, through reasoning with the node "2002-03-Football League Cup," ultimately arrived at the correct result node "Liverpool F.C."
Overall, KnowPath not only provides answers but also generates directed subgraphs, which serve as the foundation for trustworthy reasoning and significantly enhance the interpretability of the results.