\section{Experimental Setup}\label{sec:exp}
\begin{table*}[ht]
\centering
\tabcolsep=0.35cm
\begin{tabular}{lcccc}
\toprule
\textbf{Method}              & \textbf{CWQ} & \textbf{WebQSP} & \textbf{Simple Questions} & \textbf{WebQuestions} \\ 
\midrule
\multicolumn{5}{c}{\textbf{LLM only}} \\
\midrule
IO prompt~\cite{ioprompt}                    & 37.6 $\pm$ 0.8         & 63.3 $\pm$ 1.2            & 20.0 $\pm$ 0.5              & 48.7 $\pm$ 1.4               \\ 
COT~\cite{cot}                          & 38.8 $\pm$ 1.5         & 62.2 $\pm$ 0.7            & 20.5 $\pm$ 0.4              & 49.1 $\pm$ 0.9               \\ 
RoG w/o planning~\cite{rog}             & 43.0 $\pm$ 0.9         & 66.9 $\pm$ 1.3            & -                 & -                  \\ 
SC~\cite{sc}                           & 45.4 $\pm$ 1.1         & 61.1 $\pm$ 0.5            & 18.9 $\pm$ 0.6              & 50.3 $\pm$ 1.2               \\ 
\midrule
\multicolumn{5}{c}{\textbf{Fine-Tuned KG Enhanced LLM}} \\ 
\midrule
UniKGQA~\cite{unikgqa}           & 51.2 $\pm$ 1.0         & 75.1 $\pm$ 0.8          & -        & -         \\
RE-KBQA~\cite{re-kbqa}  & 50.3 $\pm$ 1.2         & 74.6 $\pm$ 1.0            & -                 & -        \\
ChatKBQA~\cite{chatkbqa}                     & 76.5 $\pm$ 1.3         & 78.1 $\pm$ 1.1            & 85.8 $\pm$ 0.9              & 55.1 $\pm$ 0.6                  \\ 
RoG~\cite{rog}     & 64.5 $\pm$ 0.7         & 85.7 $\pm$ 1.4            & 73.3 $\pm$ 0.8    & 56.3 $\pm$ 1.0               \\ 
\midrule
\multicolumn{5}{c}{\textbf{Prompting KG Enhanced LLM with GPT3.5}} \\ 
\midrule
StructGPT~\cite{structgpt}      & 54.3 $\pm$ 1.0         & 72.6 $\pm$ 1.2        & 50.2 $\pm$ 0.5      & 51.3 $\pm$ 0.9          \\
ToG~\cite{tog}                          & 57.1 $\pm$ 1.5         & 76.2 $\pm$ 0.8            & 53.6 $\pm$ 1.0              & 54.5 $\pm$ 0.7               \\ 
PoG~\cite{pog}                          & 63.2 $\pm$ 1.0         & 82.0 $\pm$ 0.9            & 58.3 $\pm$ 0.6              & 57.8 $\pm$ 1.2               \\ 
\textbf{KnowPath (Ours)}                   & \textbf{67.9 $\pm$ 0.6} & \textbf{84.1 $\pm$ 1.3}   & \textbf{61.5 $\pm$ 0.8}     & \textbf{60.0 $\pm$ 1.0}      \\
\midrule
\multicolumn{5}{c}{\textbf{Prompting KG Enhanced LLM with DeepSeek-V3}} \\ 
\midrule
ToG~\cite{tog}           & 60.9 $\pm$ 0.7     & 82.6 $\pm$ 1.0         & 59.7 $\pm$ 0.9      & 57.9 $\pm$ 0.8                  \\ 
PoG~\cite{pog}              & 68.3 $\pm$ 1.1        & 85.3 $\pm$ 0.9         & 63.9 $\pm$ 0.5       & 61.2 $\pm$ 1.3           \\ 
\textbf{KnowPath (Ours)}     & \textbf{73.5 $\pm$ 0.9}     & \textbf{89.0 $\pm$ 0.8}        & \textbf{65.3 $\pm$ 1.0}       & \textbf{64.0 $\pm$ 0.7}       \\ 
\bottomrule
\end{tabular}

\caption{Hits@1 scores (\%) of different models on four datasets under various knowledge-enhanced methods. We use GPT-3.5 Turbo and DeepSeek-V3 as the primary backbones. Bold text indicates the results achieved by our method.}
\label{tab:comparison}
\end{table*}

\subsection{Baselines}


We chose corresponding advanced baselines for comparison based on the three main paradigms of existing knowledge-based question answering.
1) The First is the LLM-only, including the standard prompt (IO prompt\cite{ioprompt}), the chain of thought prompt (CoT\cite{cot}), the self-consistency (SC\cite{sc}), and the RoG without planning (ROG w/o planning\cite{rog}).
2) The second is the KG-enhanced fine-tuned LLMs, which include ChatKBQA\cite{chatkbqa}, RoG\cite{rog}, UniKGQA\cite{unikgqa}, and RE-KBQA\cite{re-kbqa}.
3) The third is the KG-enhanced prompt-based LLMs, including Think on graph (ToG\cite{tog}), Plan on graph (PoG\cite{pog}), and StructGPT\cite{structgpt}. 
Unlike the second, this scheme no longer requires fine-tuning and has become a widely researched mode today.


\subsection{Datasets and Metrics}
\textbf{Datasets.}
We adopt four knowledge-based question answering datasets: the single-hop Simple Questions~\cite{simpleqa}, the complex multi-hop CWQ~\cite{cwq} and WebQSP~\cite{webqsp}, and the open-domain WebQuestions~\cite{webquestion}.

\noindent\textbf{Metrics.}
Following previous research ~\cite{pog}, we apply exact match accuracy (Hits@1) for evaluation.


\subsection{Experiment Details}
Following previous research ~\cite{pog}, to control the overall costs, the maximum subgraph exploration depth $D_{amx}$ is set to 3. Since the FreeBase~\cite{freebase} supports all the aforementioned datasets, we apply it as the base graph for subgraph exploration, and We apply GPT-3.5-turbo-1106 and DeepSeek-V3 as the base models.
All experiments are deployed on four NVIDIA A800-40G GPUs.