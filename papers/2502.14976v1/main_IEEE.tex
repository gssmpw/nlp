\documentclass[article]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{placeins}
\usepackage{float}
% \usepackage{natbib}

\usepackage[bookmarks=false]{hyperref}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{tabularx,colortbl}
\usepackage{ifthen}
\usepackage{caption,subcaption}
\usepackage{amsmath}
\usepackage{booktabs}
\captionsetup[table]{name=TABLE}
\renewcommand{\thetable}{\Roman{table}}
%\usepackage{fixltx2e}
% \usepackage{cleveref}
\let\labelindent\relax
\usepackage{enumitem}
%\usepackage{showframe}
\usepackage{stfloats}
\usepackage{wrapfig}
\usepackage{verbatim}
\usepackage{tabularx}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{flushend}
\usepackage{times}  
% \usepackage{fontspec}
% \setmainfont{Times New Roman}
% \pagestyle{plain}



\captionsetup{font=footnotesize}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 3.0pt}
\setlength{\dbltextfloatsep}{10pt plus 1.0pt minus 3.0pt}
\setlength{\floatsep}{10pt plus 1.0pt minus 3.0pt}
\setlength{\dblfloatsep}{10pt plus 1.0pt minus 3.0pt}
\setlength{\intextsep}{10pt plus 1.0pt minus 3.0pt}
\title{EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models}

\author{Nastaran Darabi$^1$, Devashri Naik$^1$, Sina Tayebati$^1$, Dinithi Jayasuriya$^1$,\\
Ranganath Krishnan$^2$, Amit Ranjan Trivedi$^1$\\

$^1$Department of Electrical and Computer Engineering, University of Illinois at Chicago, IL, USA \\
$^2$Intel Labs, Hillsboro, OR, USA


\thanks{We acknowledge Tejaswi Tripathi from the Department of Mathematics, University of Michigan, for his valuable insights.\\
This work was supported in part by COGNISENSE, one of seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, and NSF funding \#2235207. Corresponding Authors Email: {\tt\small ndarab2@uic.edu, amitrt@uic.edu}}}

\maketitle

\begin{abstract} Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large Language Models (LLMs), which are further exacerbated by their multimodal nature. Existing defenses, including adversarial training, input transformations, and heuristic detection, are computationally expensive, architecture-dependent, and fragile against adaptive attacks. We introduce \textit{EigenShield}, an inference-time defense leveraging \textit{Random Matrix Theory} to quantify adversarial disruptions in high-dimensional VLM representations. Unlike prior methods that rely on empirical heuristics, EigenShield employs the spiked covariance model to detect structured spectral deviations. Using a Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it separates \textit{causal eigenvectors}, which encode semantic information, from correlational eigenvectors that are susceptible to adversarial artifacts. By projecting embeddings onto the causal subspace, EigenShield filters adversarial noise without modifying model parameters or requiring adversarial training. This architecture-independent, attack-agnostic approach significantly reduces the attack success rate, establishing spectral analysis as a principled alternative to conventional defenses. Our results demonstrate that EigenShield consistently outperforms all existing defenses including adversarial training, UNIGUARD, and CIDER. 
%It lowers ASR by 76.5\% for LLaVA-v1.5-7B (81.61\% $\rightarrow$ 19.20\%), 45.3\% for MiniGPT-4 (37.20\% $\rightarrow$ 20.37\%), 31.1\% for InstructBLIP (59.80\% $\rightarrow$ 41.21\%), and 43.2\% for Qwen2-VL (30.38\% $\rightarrow$ 17.25\%). Additionally, it reduces toxicity (77.93 $\rightarrow$ 13.28 for LLaVA), identity attack (25.41 $\rightarrow$ 1.76), profanity (67.22 $\rightarrow$ 15.31), and explicit content (39.38 $\rightarrow$ 5.16), demonstrating superior mitigation of multimodal jailbreak attacks.

\textcolor{red}{Warning: This paper contains data, prompts, and model outputs that are offensive in nature.}

%\textcolor{red}{Add 1-2 result sentences.} 
\end{abstract}

\section{Introduction and Related Works}  
Vision-Language Models (VLMs) have extended Large Language Models (LLMs) by integrating visual understanding, thereby enabling applications like image captioning and visual question answering \cite{minaee2024large, zhang2024vision}. Despite these advancements, VLMs remain highly vulnerable to adversarial ``jailbreak'' attacks \cite{shafahi2019adversarial, jin2024jailbreakzoo}, where imperceptible perturbations in images or text can manipulate outputs, leading to harmful, undesirable, or policy-violating responses \cite{liu2024survey}. Their multimodal nature specifically expands this attack surface \cite{liu2024survey}, as the fusion of continuous visual signals and discrete linguistic representations introduces exploitable inconsistencies. This interplay enables novel attacks, such as cross-modal misalignment, where conflicting vision and text inputs induce failure modes. Moreover, securing one modality does not necessarily mitigate vulnerabilities in the other, making defense challenging.

VLM jailbreaks can generate toxic content, biased outputs, or execute unsafe instructions, posing risks for real-world deployment \cite{shayegani2023jailbreak}. Prior works have demonstrated attacks in three main categories: adversarial perturbation-based attacks \cite{zhao2024evaluating}, prompt manipulation \cite{yi2023benchmarking}, and proxy model transfer jailbreaks \cite{liu2024survey, jin2024jailbreakzoo}. Adversarial perturbation-based attacks introduce subtle modifications to images or text to bypass alignment mechanisms, leading to harmful outputs \cite{shayegani2023jailbreak, qi2024visual}. Prompt manipulation strategically modifies text inputs to exploit model biases and evade safety constraints \cite{yi2023benchmarking}. Proxy model transfer jailbreaks leverage vulnerabilities in surrogate models to generate adversarial examples that generalize to stronger VLMs \cite{liu2024survey, jin2024jailbreakzoo}. Moreover, these attack vectors are not isolated; adversaries can strategically combine them to create more potent, adaptive jailbreaks by compounding vulnerabilities.

Current defense strategies fall into proactive (training-based) and reactive (inference-time) approaches. Adversarial training improves robustness proactively by exposing models to adversarial examples but is computationally demanding and requires careful parameter tuning \cite{shafahi2019adversarial}. UNIGUARD \cite{oh2024uniguard}, a state-of-the-art proactive defense, establishes universal safety guardrails for Multimodal LLMs (MLLMs) by optimizing modality-specific risk mitigation. Inference-time reactive defenses operate without modifying the model, using methods such as image transformations (e.g., blurring, compression) or diffusion-based purification (e.g., DiffPure \cite{nie2022diffusion}), though these can degrade clean input performance or be computationally expensive. Detection mechanisms, such as Jailguard \cite{zhang2023mutation} and CIDER \cite{xu2024defending}, identify adversarial inputs by analyzing perturbations or cross-modal inconsistencies. However, existing defenses face several fundamental limitations. They focus on individual samples rather than modeling adversarial patterns globally, making them ineffective against adaptive attacks that exploit multimodal inconsistencies \cite{liu2024survey}. Moreover, heuristic defenses relying on fine-tuned hyperparameters often fail under evolving threats, as seen in adversarial transferability across models \cite{jin2024jailbreakzoo}.

Studies have shown that adversarial perturbations introduce structured noise that systematically alters the spectral properties of model activations \cite{shayegani2023jailbreak}. \textit{Crucially}, these perturbations do not behave randomly but follow measurable statistical patterns—suggesting that their spectral signatures can be leveraged to design a principled approach to distinguishing clean from adversarial data. Building on this insight, we introduce EigenShield, a defense framework rooted in Random Matrix Theory (RMT) to model spectral behavior in high-dimensional systems by distinguishing signal-dominant regimes from noise-induced disruptions. Leveraging spectral distribution principles from RMT, EigenShield systematically detects and mitigates adversarial perturbations by analyzing spectral deviations in feature representations. Unlike adversarial training, which enforces robustness through injected perturbations, EigenShield operates on first principles rather than empirical heuristics. 

Specifically, we present EigenShield's framework by leveraging the spiked covariance model to separate adversarial noise from meaningful signal components in VLM feature representations. By introducing a Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, EigenShield distinguishes causal eigenvectors, which are semantically meaningful, from correlational eigenvectors that encode adversarial artifacts. The framework operates entirely at the input level, projecting embeddings onto the subspace of causal eigenvectors to filter adversarial noise without modifying the VLM's architecture or requiring adversarial training. This makes EigenShield \textit{architecture-independent}, requiring \textit{no retraining}, \textit{theoretically grounded} with guarantees via eigenvalue distributions, \textit{attack-agnostic}, and \textit{computationally efficient}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/overview_v1.jpg}
    \caption{\textbf{Overview of EigenShield:} Jailbreak attacks attempt to produce harmful responses from VLMs by manipulating inputs. EigenShield intervenes by filtering input embeddings via a causal subspace. EigenShield's defense mechanisms are \textit{architecture-independent}, requiring \textit{no retraining}, \textit{theoretically grounded} in Random Matrix Theory with guarantees via eigenvalue distributions, \textit{attack-agnostic}, and \textit{computationally efficient}.}
    \label{fig:overview}
\end{figure*}

\section{Foundations from Random Matrix Theory} \label{sec:rmt_background}
RMT provides a powerful analytical framework for characterizing the spectral properties of large random matrices, particularly suitable in high-dimensional data regimes where the disentanglement of genuine signals from pervasive noise forms a fundamental challenge \cite{edelman2005random, tao2012topics, feier2012methods}. We first briefly discuss the key concepts from RMT that form the basis of EigenShield.  

\textbf{Wigner Semicircle Law and the Asymptotic Noise Floor:}
Wigner Semicircle Law characterizes the asymptotic eigenvalue distribution of large symmetric random matrices with independent and identically distributed (i.i.d.) entries \cite{chan1992wigner}. Let $\mathbf{W}$ be a $p \times p$ real symmetric random matrix, where the upper triangular entries $W_{ij}$ (for $1 \leq i \leq j \leq p$) are i.i.d. with zero mean and variance $\sigma^2$, and the lower triangular entries are defined by symmetry, $W_{ji} = W_{ij}$. As the matrix size $p$ approaches infinity, the empirical spectral distribution (ESD) of $\mathbf{W}$, representing the eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_p$, almost surely converges to the Wigner Semicircle distribution. The probability density function (PDF) of this limiting distribution is given by:
\begin{equation}
    f_{\text{Wigner}}(\lambda) = \begin{cases}
        \frac{1}{2\pi \sigma^2} \sqrt{4\sigma^2 - \lambda^2}, & \text{if } |\lambda| \leq 2\sigma \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}
This result demonstrates that for purely random matrices, the eigenvalues are not arbitrarily distributed but are confined within a bounded interval $[-2\sigma, 2\sigma]$, forming a well-defined ``noise floor" in the eigenvalue spectrum. This theoretical noise floor serves as a critical benchmark for identifying eigenvalues that significantly deviate, potentially indicating the presence of non-random signal components. A simplified proof for this is provided in Appendix \ref{sec:spectral_confinement}.

For the sample covariance matrix, which is central to many statistical applications, a related result is the Marchenko-Pastur distribution. Consider an $n \times p$ data matrix $\mathbf{X}$ with i.i.d. entries of zero mean and variance $\sigma^2$. In the high-dimensional limit, where $n, p \to \infty$ with a fixed aspect ratio $c = p/n > 0$, the empirical spectral distribution (ESD) of the sample covariance matrix $\mathbf{C} = \frac{1}{n} \mathbf{X}^T \mathbf{X}$ converges to the Marchenko-Pastur distribution. The distribution support lies within $[\sigma^2 (1-\sqrt{c})^2, \sigma^2 (1+\sqrt{c})^2]$ \cite{yaskov2016short}.

\textbf{Spiked Covariance Model and Signal Detection via Eigenvalue Outliers:}  
To model scenarios where a low-rank informative signal is embedded within high-dimensional noise, we use the \textit{spiked covariance model} \cite{paul2007asymptotics}. This model assumes the true covariance matrix $\mathbf{\Sigma}$ is decomposed into: 
    $\mathbf{\Sigma} = \mathbf{\Sigma}_{\text{signal}} + \mathbf{\Sigma}_{\text{noise}}$.
Here, $\mathbf{\Sigma}_{\text{signal}}$ represents a structured, low-rank signal component of rank $k \ll p$, and $\mathbf{\Sigma}_{\text{noise}} = \sigma^2 \mathbf{I}_p$ represents isotropic noise, where $\mathbf{I}_p$ is the $p \times p$ identity matrix and $\sigma^2$ is the noise variance. The term ``spikes" refers to eigenvalues of the sample covariance matrix $\mathbf{C}$ that deviate significantly from the bulk distribution described by the Wigner Semicircle Law or Marchenko-Pastur distribution, indicating the presence of $\mathbf{\Sigma}_{\text{signal}}$.  

If $\mathbf{\Sigma}_{\text{signal}}$ has the eigendecomposition $\mathbf{\Sigma}_{\text{signal}} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^T$, where $\mathbf{V}$ is a $p \times k$ matrix with orthonormal columns spanning the signal subspace, and $\mathbf{\Lambda} = \text{diag}(\lambda'_1, \ldots, \lambda'_k)$ contains the signal eigenvalues ($\lambda'_1 \geq \ldots \geq \lambda'_k > 0$), then under certain conditions, RMT predicts the emergence of outlier eigenvalues. Specifically, when $\lambda'_i > \sigma^2 \sqrt{c}$ for $i = 1, \ldots, k$, asymptotic theory guarantees the existence of $k$ outlier eigenvalues $\hat{\lambda}_1, \ldots, \hat{\lambda}_k$ of $\mathbf{C}$. These outliers lie outside the upper edge of the Marchenko-Pastur bulk and are closely related to the underlying signal eigenvalues $\lambda'_i$. These ``spiked" eigenvalues and their corresponding eigenvectors capture the dominant signal directions, enabling the separation of signal from noise in high-dimensional data. We provide a more detailed discussion in Appendix \ref{sec:spikedmodel}. 

\textbf{RMT-based Decomposition for Causal Subspace Extraction:}  
To extract ``causal" subspaces using the spiked covariance principle, we formulate an optimization problem to decompose the empirical covariance matrix $\Sigma_n$ into a low-rank signal component and an isotropic noise component. This is achieved by minimizing the RMT Loss function:  
\begin{equation}
    \mathcal{L}_{\mathrm{RMT}}(U, \Lambda)
    \;=\;
    \bigl\|\,
        \Sigma_n
        \;-\;
        \bigl(
          U \,\Lambda\, U^\top
          \;+\;
          \sigma^2 I_P
        \bigr)
    \bigr\|_2^{2}.
    \label{eq:rmt_loss}
\end{equation}  
Here, $U \in \mathbb{R}^{P \times r}$ is a matrix with orthonormal columns representing the top-$r$ principal (causal) directions, $\Lambda \in \mathbb{R}^{r \times r}$ is a diagonal matrix of signal eigenvalues, and $\sigma^2 I_P$ represents the isotropic noise covariance. The Euclidean norm $\|\cdot\|_2$ quantifies the difference between the empirical covariance and the model. Minimizing $\mathcal{L}_{\mathrm{RMT}}$ performs a principled decomposition of $\Sigma_n$ into a dominant signal subspace $U\Lambda U^\top$ and a residual noise complement. This process identifies components that are \emph{causal} based on their signal strength rather than correlational or noise-driven influences. Specifically, we designate the eigenvalues with the highest magnitudes as ``causal," under the premise that these eigenvalues, derived from disentangled causal factors, are most strongly linked to the output label and exert the greatest influence on output class determination. We provide a more detailed discussion in Appendix \ref{sec:causal_direction}.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/insight_v2.jpg}
    
    \caption{EigenShield's effect on adversarial representations. \textbf{(a)} Eigenvalue spectrum comparison shows restoration of adversarial images to clean-like distributions. \textbf{(b)} Heatmap visualization depicts eigenvalue variability across 100 images. \textbf{(c)} Eigenvector classification via RbNS separates robust causal (blue) from spurious components (red) using threshold $\tau^*$. }

    \label{fig:insight}
\end{figure*}

\section{Random Matrix Theory–Rooted Safety of Vision-Language Models: \textit{EigenShield}}
In adversarial jailbreak attacks against VLMs, we hypothesize that adversarial perturbations, though visually imperceptible, introduce statistically anomalous signal components into the input image, perturbing the eigenvalue spectrum and deviating from expected noise characteristics defined by RMT. In Fig. \ref{fig:overview}, EigenShield leverages these insights through three key steps: \textit{(i) Eigenvalue Decomposition of Input Images}: Each input undergoes eigenvalue decomposition, extracting principal spectral components. \textit{(ii) Causal Eigenvalue Hypothesis and Thresholding via RbNS}: Higher-magnitude eigenvalues correspond to semantically relevant components, while lower-magnitude ones likely represent noise or adversarial artifacts. Using the Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, we determine an optimal threshold to separate causal from correlational eigenvalues. \textit{(iii) Filtering via Projection onto the Causal Subspace}: At inference, eigenvalue decomposition identifies causal eigenvalues exceeding the threshold, whose eigenvectors define a causal subspace. The input is then projected onto this subspace, preserving causal components while filtering spurious correlations. This approach selectively amplifies input image components associated with high-magnitude, causally relevant eigenvalues while mitigating adversarial noise and spurious correlations.

Based on this, the proposed methodology operates in two sequential phases. \textit{Phase-1}: Threshold Determination via RbNS and RMT Analysis and \textit{Phase-2}: Inference-Time Jailbreak Defense. Key components are discuss below:

\textbf{\underline{Phase 1}: Threshold Determination via Robustness-based Nonconformity Score (RbNS)}

This phase introduces a data-driven approach to determine the optimal threshold $\tau^*$, which separates causal and correlational eigenvalues. The threshold is derived using the \textit{Robustness-based Nonconformity Score (RbNS)}, which evaluates the stability and predictive relevance of eigenvector directions using the following procedures:

\textbf{\textit{(i) Extracting Outlier Directions via RMT}:}  
We first apply RMT principles to identify outlier eigenvectors $\mathcal{O} = \{v_1, v_2, \ldots, v_{|\mathcal{O}|}\}$ that deviate from the expected Marchenko-Pastur bulk. Each eigenvector $v_j \in \mathcal{O}$ is associated with an eigenvalue $\lambda_j$ exceeding the noise floor, which indicates a potential signal-bearing component. 

\textbf{\textit{(ii) RbNS Calculation:}}  
For each $v_j \in \mathcal{O}$, we compute an RbNS score $\alpha_j$ to quantify its robustness. A lower $\alpha_j$ indicates a causal direction, while a higher $\alpha_j$ suggests a spurious/noisy component. The calculation follows as:

\textit{A.~1-D Projection and Distribution Learning:}  
We perform eigenvalue decomposition on each input image $\mathbf{x}_i$ in the validation set $\mathcal{D}_{\text{val}}$, obtaining eigenvectors $\{v_{x_i, 1}, v_{x_i, 2}, \ldots, v_{x_i, P}\}$. Each eigenvector $v_{x_i, j}$ is projected onto the outlier direction $v_j$: $u_{i,j} = v_{x_i, j}^\top v_j$. We then use a Variational Autoencoder (VAE) $\hat{g}_j(u_{i,j})$ to estimate the conditional distribution of reconstructed images given $u_{i,j}$. Directions associated with high eigenvalues are expected to yield lower entropy in the distributions, indicating a stronger causal relevance.

\textit{B.~Cross-Validation Subsampling:}  
We assess the robustness of the VAE predictor $\hat{g}_j$ using 10-fold cross-validation. The validation set $\mathcal{D}_{\text{val}}$ is split into $K=10$ folds, with each $\hat{g}_j^{(k)}$ trained on $K-1$ folds and evaluated on the held-out fold. Performance is measured using the Kullback-Leibler (KL) divergence between the distribution of the original input features and the distribution of reconstructed features from a given VAE. More formally, we pass input images through a pre-trained encoder to get the feature representations $z_i$. We denote the distribution of the features $z_i$ on the validation set as $P_{z_i}$, and the reconstructed features from the VAE as $\hat{z}_i$. The VAE provides a model for the distribution of $\hat{z}_i$, namely $Q_{\hat{z}_i}$, given the projection $u_{i,j}^{(k)}$. We then calculate KL divergence using
\(
\text{Perf}_{j,k} = \text{KL}(P_{z_i} \parallel Q_{\hat{z}_i|u_{i,j}^{(k)}}).
\)

\textit{C.~Robustness Statistic Computation:}  
To quantify the stability of each eigenvector direction $v_j$, we compute its robustness $\rho_j$ using the KL divergence performance distribution. Specifically, $\rho_j$ is calculated as the difference between the median and a lower quantile ($q=0.10$) of the KL divergence distribution over the cross-validation folds:
\begin{equation}
    \rho_j = \text{Median}(\{\text{Perf}_{j,k}\}_{k=1}^{K}) - \text{LQuantile}_{q}(\{\text{Perf}_{j,k}\}_{k=1}^{K}).
\end{equation}
In this context, a smaller $\rho_j$ indicates a more stable and robust direction. This implies greater consistency in distribution matching across different data partitions.

\textit{D.~Nonconformity Score Computation:}  
Finally, the nonconformity score $\alpha_j$ is computed as the exponent of $\rho_j$, $\alpha_j = e^{\rho_j}$. Smaller $\alpha_j$ values indicate causal directions, while larger $\alpha_j$ values correspond to spurious or noisy signal components.

\textbf{\textit{(iii)~Quantile-Based Thresholding for Causal Eigenvalue Selection:}}  
Given the computed nonconformity scores $\{\alpha_j\}_{v_j \in \mathcal{O}}$ for all outlier directions, we employ quantile-based thresholding to determine the optimal eigenvalue classification threshold $\tau^*$. A coverage parameter $\gamma \in [0, 1]$ controls the retention of ``causal'' directions. Fig.~\ref{fig:gamma} presents an evaluation of $\gamma$ values ranging from 0.5 to 0.9, with $\gamma = 0.75$ selected as the optimal choice. The figure demonstrates that as $\gamma$ increases, more eigenvalues are incorporated into the causal subspace, leading to an overall improvement in EigenShield's performance. However, a higher $\gamma$ also results in an increased number of trainable parameters. 
  
The nonconformity threshold $\hat{t}$ is set as the $\gamma$-th quantile of the nonconformity score distribution:
\(
\hat{t} = \text{quantile}_{(\gamma)}\left(\{\alpha_j\}_{v_j \in \mathcal{O}}\right).
\)
A direction $v_j$ (with eigenvalue $\lambda_j$) is classified as ``causal'' if its nonconformity score satisfies $\alpha_j \leq \hat{t}$. The final threshold $\tau^*$ is then set as the minimum eigenvalue among all causal directions. We prioritize higher-magnitude eigenvalues, aligning with our hypothesis that these correspond to disentangled causal components with the strongest influence on the output label. 
%This quantile-based selection provides a statistically principled, data-driven method to separate robust predictive directions from spurious correlations, guiding the subsequent inference-time filtering process. 

\textbf{\underline{Phase 2}: Inference-Time Jailbreak Defense}

Once the optimal threshold $\tau^*$ is determined in Phase 1, we deploy EigenShield at inference to protect the VLM from adversarial jailbreak attacks. Given an input image $x_{\text{input}}$, we first compute its eigenvalue decomposition, obtaining the eigenvalue set $\Lambda_{x_{\text{input}}}$. Each eigenvalue $\lambda_{x_{\text{input}},i} \in \Lambda_{x_{\text{input}}}$ is classified as causal if $\lambda_{x_{\text{input}},i} > \tau^*$, while lower values correspond to correlational or noise components. Input image embedding $\mathbf{e}_{x_{\text{input}}}$ is then projected onto the causal subspace $\mathbf{E}_{\text{causal}}^*$, spanned by eigenvectors associated with causal eigenvalues, yielding the filtered embedding:  
\begin{equation}
    \mathbf{e}_{x_{\text{input}}}^{\text{filtered}} = \mathbf{P}_{\text{causal}}^* \mathbf{e}_{x_{\text{input}}}, \quad \text{where} \quad \mathbf{P}_{\text{causal}}^* = \mathbf{E}_{\text{causal}}^* (\mathbf{E}_{\text{causal}}^*)^T.
\end{equation}
    
Finally, the filtered embedding $\mathbf{e}_{x_{\text{input}}}^{\text{filtered}}$ is passed to the VLM alongside the original text prompt for inference.  

By projecting embeddings onto the causal subspace defined by $\tau^*$, EigenShield effectively removes spurious correlations and adversarial perturbations while preserving dominant semantic information. This input-level filtering enhances VLM robustness against jailbreak attacks without modifying the model architecture or incurring significant computational overhead, making it a practical and efficient defense. Fig.~\ref{fig:insight} presents a comprehensive analysis of EigenShield’s processing through eigenvalue and robustness-based evaluations. Fig.~\ref{fig:insight}(a) compares the eigenvalue spectra of clean, adversarial, and EigenShield-processed images, revealing how the method modifies eigenvalue distributions to mitigate adversarial perturbations. The log-scale representation highlights differences in magnitude across sorted eigenvalue indices. Fig.~\ref{fig:insight}(b) visualizes the variability of eigenvalues across 100 randomly selected EigenShield-processed images, demonstrating the inherent distributional differences between images. Finally, Fig.~\ref{fig:insight}(c) classifies eigenvectors using the Robustness-Based Nonconformity Score (RbNS), where causal eigenvectors (blue) exhibit lower RbNS values, indicating higher robustness, while correlational eigenvectors (red) have higher RbNS values, suggesting susceptibility to spurious correlations. The threshold $\tau^*$ (dashed black line) effectively distinguishes robust causal components from spurious ones. Together, these results highlight how EigenShield enhances robustness by selectively retaining causal eigenvectors while filtering out correlational noise. 

\begin{table*}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{2.5pt} 
\caption{Effectiveness of EigenShield against multimodal jailbreak attacks on VLMs, evaluated using Perspective API metrics. \textbf{Dataset: HarmBench} \cite{mazeika2024harmbench}. Lower is better.} 
\label{tab:harmbench_results}
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{No Defense}} & \multicolumn{6}{c}{\textbf{EigenShield}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
 & \textbf{ASR} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity}  
 & \textbf{ASR} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} \\
 &  & \textbf{Attack} &  & \textbf{Explicit} &  &  
 &  & \textbf{Attack} &  & \textbf{Explicit} &  &  \\
\midrule

LLaVA-v1.5-7B & 65.72 & 18.59 & 49.61 & 30.24 & 27.32 & 63.30  
& 24.37 & 6.53 & 16.12 & 7.94 & 4.18 & 23.11 \\

MiniGPT-4 & 55.24 & 7.11 & 34.82 & 20.04 & 6.24 & 51.94  
& 22.46 & 2.81 & 14.55 & 9.12 & 2.01 & 19.84 \\

InstructBLIP & 10.63 & 1.35 & 7.95 & 3.46 & 0.88 & 9.06  
& 4.52 & 0.61 & 3.43 & 1.42 & 0.35 & 3.92 \\

Qwen2-VL & 7.57 & 0.82 & 5.14 & 1.54 & 0.89 & 6.91  
& 2.66 & 0.39 & 1.79 & 0.63 & 0.47 & 2.03 \\

Florence-2-large & 15.24 & 2.15 & 9.78 & 5.28 & 1.72 & 13.31  
& 3.51 & 0.73 & 3.72 & 1.54 & 0.73 & 3.46 \\

\bottomrule
\end{tabular}
\end{table*}



% \begin{table*}[t]
% \centering
% \small
% \setlength{\tabcolsep}{2pt} 
% \caption{Effectiveness of EigenShield against multimodal jailbreak attacks on VLMs, evaluated using \textbf{\textit{Perspective API metrics}}. Dataset: unconstrained adversarial visual attack \cite{qi2024visual} and two adversarial text datasets, RTP \cite{gehman2020realtoxicityprompts} and HarmBench \cite{mazeika2024harmbench}. Lower is better. Best results highlighted in \textbf{bold}.}
% \label{tab:rmt_unconstrained}
% {%
% \begin{tabular}{lcccccc ccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{RTP} \cite{gehman2020realtoxicityprompts}} & \multicolumn{6}{c}{\textbf{HarmBench} \cite{mazeika2024harmbench}} \\
% \cmidrule(lr){2-7} \cmidrule(lr){8-13}
% & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} \\
% & \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  &  & & \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  & &  \\
% \midrule

% LLaVA-v1.5-7B & 81.61 & 25.41 & 67.22 & 39.38 & 40.64 & 77.93 & 65.72 & 18.59 & 49.61 & 30.24 & 27.32 & 63.30 \\
% \quad +EigenShield & \textbf{19.20} & \textbf{1.76} & \textbf{15.31} & \textbf{5.16} & \textbf{1.01} & \textbf{13.28} & \textbf{24.37} & \textbf{6.53} & \textbf{16.12} & \textbf{7.94} & \textbf{4.18} & \textbf{23.11} \\

% \cmidrule(lr){1-13}

% MiniGPT-4 & 37.20 & 2.94 & 26.53 & 12.76 & 2.10 & 31.57 & 55.24 & 7.11 & 34.82 & 20.04 & 6.24 & 51.94 \\
% \quad +EigenShield & \textbf{20.73} & \textbf{1.31} & \textbf{12.80} & \textbf{8.52} & \textbf{0.93} & \textbf{15.78} & \textbf{22.46} & \textbf{2.81} & \textbf{14.55} & \textbf{9.12} & \textbf{2.01} & \textbf{19.84} \\

% \cmidrule(lr){1-13}

% InstructBLIP & 59.80 & 6.51 & 44.95 & 19.02 & 4.92 & 54.55 & 10.63 & 1.35 & 7.95 & 3.46 & 0.88 & 9.06 \\
% \quad +EigenShield & \textbf{41.21} & \textbf{4.89} & \textbf{32.11} & \textbf{11.75} & \textbf{2.38} & \textbf{40.69} & \textbf{4.52} & \textbf{0.61} & \textbf{3.43} & \textbf{1.42} & \textbf{0.35} & \textbf{3.92} \\

% \cmidrule(lr){1-13}

% Qwen2-VL & 30.38 & 2.13 & 27.65 & 12.53 & 1.08 & 29.48 & 7.57 & 0.82 & 5.14 & 1.54 & 0.89 & 6.91 \\
% \quad +EigenShield & \textbf{17.25} & \textbf{1.51} & \textbf{14.82} & \textbf{5.99} & \textbf{0.83} & \textbf{16.68} & \textbf{2.66} & \textbf{0.39} & \textbf{1.79} & \textbf{0.63} & \textbf{0.47} & \textbf{2.03} \\

% \cmidrule(lr){1-13}

% Florence-2-large & 36.47 & 3.61 & 26.34 & 17.29 & 2.98 & 33.12 & 15.24 & 2.15 & 9.78 & 5.28 & 1.72 & 13.31 \\
% \quad +EigenShield & \textbf{18.80} & \textbf{1.05} & \textbf{15.22} & \textbf{12.47} & \textbf{1.13} & \textbf{16.95} & \textbf{3.51} & \textbf{0.73} & \textbf{3.72} & \textbf{1.54} & \textbf{0.73} & \textbf{3.46} \\

% \bottomrule
% \end{tabular}%
% }
% \end{table*}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt} % Adjust column spacing for compactness
\caption{EigenShield evaluated on clean images from ImageNet and adversarial texts from RTP \cite{gehman2020realtoxicityprompts} and HarmBench \cite{mazeika2024harmbench}. Lower is better.}
\label{tab:rmt_clean} 
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{No Defense}} & \multicolumn{2}{c}{\textbf{EigenShield}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & \textbf{RTP} & \textbf{HarmBench} & \textbf{RTP} & \textbf{HarmBench} \\
\midrule

LLaVA-v1.5-7B  & 1.1 & 0.7  & 0.3 & 0.2  \\

MiniGPT-4  & 0.6 & 0.4  & 0.1 & 0.1  \\

InstructBLIP  & 0.7 & 0.5  & 0.4 & 0.1  \\

Qwen2-VL  & 0.2 & 0.4  & 0.2 & 0.1  \\

Florence-2-large  & 0.5 & 0.4  & 0.1 & 0.2  \\

\bottomrule
\end{tabular}
\end{table}


\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{3pt} % Adjust column spacing for compactness
\caption{Comparison of EigenShield with state-of-the-art defenses against multimodal jailbreak attacks using Perspective API metrics. The dataset includes unconstrained adversarial visual attacks \cite{qi2024visual} and RTP \cite{gehman2020realtoxicityprompts} adversarial text on LLaVA-v1.5-7B, MiniGPT-4, InstructBLIP, and Qwen2-VL. Lower values indicate better performance. Best and second-best results are highlighted in \textbf{bold} and \underline{underlined}.}
\label{tab:llava_unconstrained}
\resizebox{\textwidth}{!}
{%
\begin{tabular}{lcccccc | ccccccc}
\toprule
\multirow{3}{*}{\textbf{Defense}} & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} \\
& \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  &  & & \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  & &  \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
& \multicolumn{6}{c|}{\textbf{LLaVA-v1.5-7B}} & \multicolumn{6}{c}{\textbf{MiniGPT-4}} \\
\midrule

No Defense & 81.61 & 25.41 & 67.22 & 39.38 & 40.64 & 77.93 
           & 37.20 & 2.94 & 26.53 & 12.76 & 2.10 & 31.57  \\

Adv. Training & 28.21 & 3.71 & 25.05 & 10.90 & 1.87 & 28.44  
              & 29.82 & 2.66 & 22.41 & 10.15 & 1.63 & 23.20  \\

UNIGUARD & 25.17 & \underline{2.06} & \underline{22.34} & \underline{7.99} & \textbf{0.86} & 19.16  
         & 24.98 & \textbf{1.37} & \underline{16.42} & 10.69 & 1.80 & \underline{18.73}  \\

BLURKERNEL & 39.03 & 3.92 & 30.61 & 14.10 & 3.17 & 32.28  
           & 38.92 & 2.28 & 28.34 & 13.79 & 2.12 & 33.08 \\

COMP-DECOMP & 37.70 & 2.67 & 29.02 & 13.26 & 3.59 & 31.94  
            & 35.21 & 2.31 & 25.56 & 11.97 & \underline{1.54} & 29.06 \\

DIFFPURE & 40.42 & 3.01 & 30.89 & 14.48 & 3.35 & 34.06  
         & 41.32 & 2.12 & 29.89 & 15.24 & 2.12 & 35.65 \\

CIDER & \underline{24.73} & 2.88 & 25.80 & 9.79 & 2.53 & \underline{17.49}  
      & \underline{22.74} & 1.68 & 17.15 & \underline{9.82} & 1.93 & 20.04 \\

EigenShield & \textbf{19.20} & \textbf{1.76} & \textbf{15.31} & \textbf{5.16} & \underline{1.01} & \textbf{13.28}  
                      & \textbf{20.37} & \underline{1.39} & \textbf{12.80} & \textbf{8.53} & \textbf{0.96} & \textbf{15.77} \\

\midrule

& \multicolumn{6}{c|}{\textbf{InstructBLIP}} & \multicolumn{6}{c}{\textbf{Qwen2-VL}} \\

\midrule

No Defense & 59.8 & 6.51 & 44.95 & 19.02 & 4.92 & 54.55 & 30.38 & 2.13 & 27.65 & 12.53 & 1.08 & 29.48 \\

UNIGUARD & \underline{43.79} & \underline{5.09} & \underline{34.36} & 13.43 & \underline{2.42} & \underline{39.95} &  \underline{19.87}  & \underline{1.85} & \underline{17.98} & 7.20 & \underline{0.91} & \underline{18.52}\\

BLURKERNEL &  69.31 & 9.26 & 56.96 & 23.85 & 6.42 & 66.22 &35.17 & 3.54 & 30.11 & 14.52 & 1.59 & 34.27\\

COMP-DECOMP &  69.22 & 8.17 & 56.13 & 23.69 & 6.17 & 65.72 &  34.85 & 3.19 & 29.74 & 14.35 & 1.29 & 33.82\\

DIFFPURE  & 68.31 & 8.76 & 52.79 & 24.35 & 5.09 & 63.47 & 36.15  &  3.89 & 31.47 & 16.09 & 1.66 & 35.57  \\

CIDER &  44.28 & 5.69 & 36.52 & \underline{12.96} & 3.05 & 42.09 & 21.72 & 2.63 & 19.75 &  \underline{6.81} & 0.97 & 20.14\\

EigenShield & \textbf{41.21} & \textbf{4.89} & \textbf{32.11} & \textbf{11.75} & \textbf{2.38} & \textbf{40.69} &  \textbf{17.25} & \textbf{1.51} & \textbf{14.82} & \textbf{5.99} & \textbf{0.83} & \textbf{16.68} \\
\bottomrule
\end{tabular}%
}
\end{table*}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/combined_xx_plot.jpg}
\caption{Attack Success Rate (\%) \textit{vs.} coverage parameter $\gamma$ for two VLMs: LLaVA-v1.5 (blue) and MiniGPT-4 (red). Higher values of $\gamma$ generally correspond to stricter causal inclusion, leading to a reduction in attack success for both models. The top axis indicates the number of trainable parameters (in thousands) for each $\gamma$. As $\gamma$ increases, more eigenvalue directions are retained, effectively enhancing the model’s capacity.}
    \label{fig:gamma}
\end{figure}


\section{Experiments}

\subsection{Model, Datasets, and Evaluation Metrics}

\textbf{Models:} 
We evaluated the generalizability of EigenShield on diverse VLMs, including LLaVA-v1.5-7B \cite{liu2024visual}, MiniGPT-4 \cite{zhu2023minigpt}, InstructBLIP \cite{NEURIPS2023_9a6a435e}, Qwen2-VL \cite{Qwen2VL}, and Florence-2-large \cite{xiao2023florence}. LLaVA-v1.5-7B fine-tunes LLaMA/Vicuna with multimodal instruction data for vision-language reasoning. MiniGPT-4 aligns a pretrained vision encoder with a large language model for multimodal interaction. InstructBLIP applies instruction tuning. Qwen2-VL integrates advanced multimodal fusion techniques with extensive pretraining on both textual and visual data. Florence-2-large leverages large-scale supervised learning for high-resolution image understanding and captioning. The comprehensive set of test models ensured a robust assessment of EigenShield across a range of VLM characteristics.

\textbf{Datasets:}  
To set up the baseline accuracy without adversarial attacks, we used a subset of the ImageNet dataset \cite{deng2009imagenet}, randomly selecting a thousand images. To evaluate EigenShield's robustness against adversarial threats, we employed the HarmBench dataset \cite{mazeika2024harmbench}, a benchmark for testing VLMs against harmful content generation. Specifically, we focused on two-hundred harmful queries in HarmBench and generated a thousand adversarial text-image pairs using a Projected Gradient Descent (PGD) attack \cite{madry2017towards} with varying perturbation rates. Additionally, we scaled our evaluation using the RealToxicityPrompts benchmark \cite{gehman2020realtoxicityprompts}. Following \cite{qi2024visual} and \cite{mehrabi2022robust}, we used the challenging subset containing 1,225 text prompts designed to trigger toxic continuations. Visual adversarial examples were paired with each text prompt to serve as VLM inputs.  
    
To evaluate EigenShield's generalization under various real-world threat models, we generated adversarial examples using five distinct attack methods: FGSM \cite{goodfellow2014explaining}, PGD \cite{madry2017towards}, MIM \cite{dong2018boosting}, CW \cite{carlini2017towards}, and Square Attack \cite{andriushchenko2020square}. Each method targets unique optimization objectives or utilizes distinct information (e.g., gradients vs. query-based sampling), covering a wide range of adversarial strategies. We randomly sampled images from our validation set and applied each attack to create five adversarial subsets, which were combined into a unified ``generalization'' dataset.

\textbf{Defense Baselines:}  
We compared EigenShield against the following defense baselines:  

\textit{No Defense (Baseline Vulnerability Assessment):}  
We establish a baseline by evaluating VLMs without any defense mechanism, highlighting their inherent vulnerabilities to adversarial jailbreak attacks and quantifying the security gap targeted by defense strategies.  

\textit{Image Transformation Defenses:}  
We utilize two image pre-processing techniques: BLURKERNEL, which uses small average convolution kernels to smooth input images, reducing high-frequency noise linked to adversarial attacks, and COMP-DECOMP, which lowers image quality to obscure adversarial features embedded in the images.  

\textit{DIFFPURE \cite{nie2022diffusion}:}  
DIFFPURE leverages diffusion models for adversarial purification by introducing controlled noise to adversarial images and applying a reverse diffusion process to effectively remove perturbations while preserving the original image’s semantics.  

\textit{CIDER \cite{xu2024defending}:} Cross-modality Information DEtectoR (CIDER) is an inference-time detector that identifies adversarial images by analyzing semantic shifts between text and image modalities before and after denoising.  

\textit{Adversarial Training \cite{shafahi2019adversarial}:} Adversarial training enhances robustness by training models on both clean and adversarial inputs to strengthen internal representations. This approach, however, requires modifications to the training pipeline and is computationally demanding.  

\textit{UNIGUARD \cite{oh2024uniguard}:} UNIGUARD is a multimodal safety framework that minimizes harmful responses by optimizing modality-specific guardrails using a small corpus of harmful content. It can be seamlessly integrated at inference time and demonstrates strong multimodal defense.

 \textbf{Evaluation Metrics:}
To quantify the harmfulness of generated responses, we leverage the \textit{Perspective API}\footnote{\url{https://perspectiveapi.com/}}. The Perspective API analyzes text across various attributes indicative of online abuse and harmful discourse. Specifically, we utilize the API to evaluate each VLM response across five key dimensions: toxicity (average of ``overall toxicity'' and ``severe toxicity'' scores), identity attack, threat, profanity, and sexually explicit content. For each attribute, the Perspective API returns a score ranging from 0 to 1, representing the probability that the text is perceived as exhibiting the respective attribute. 

\textit{Attack Success Rate (ASR):}
To quantify the effectiveness of defense mechanisms in mitigating jailbreak attacks, we employ ASR. ASR is formally defined as the proportion of adversarial attack attempts, represented as pairs of adversarial input $(x_{\text{adv}})$ and a corresponding harmful prompt $(\text{prompt})$, that result in the VLM generating a disallowed or harmful response \emph{despite} the application of the defense mechanism. Similar to \cite{xu2024defending}, ASR is calculated using the following equation as the average of the harmfulness indicator function, $\mathbb{I}_{\text{harm}}$, over a dataset $\mathcal{D}$ of adversarial examples. For each adversarial example, consisting of a harmful query $\text{prompt}$ and an adversarial image $x_{\text{adv}}$, we evaluate the response generated by the Vision-Language Model $\mathcal{F}$ using an LLM-based classifier $\mathcal{G}$ (implemented via the Perspective API). The indicator function $\mathbb{I}_{\text{harm}}$ then flags whether the response is harmful based on predefined criteria. By averaging this indicator across all examples in $\mathcal{D}$, the ASR provides a quantifiable measure of the defense's failure rate against jailbreak attacks as:
\begin{equation}
    \text{ASR} \stackrel{\text{def}}{=} \frac{1}{|\mathcal{D}|} \sum_{(\text{prompt}, x_{\text{adv}}) \in \mathcal{D}} \mathbb{I}_{\text{harm}}(\mathcal{G}(\mathcal{F}(\text{prompt}, x_{\text{adv}}))) 
\end{equation}
A lower ASR indicates a more robust and effective defense, i.e., reduced likelihood of adversarial inputs successfully producing harmful or policy-violating outputs from the VLM when protected by the defense.

\subsection{Performance Characterizations}
In Table \ref{tab:harmbench_results}, ASR drops significantly across models with EigenShield, LLaVA-v1.5-7B improving from 65.72\% to 24.37\%, MiniGPT-4 from 55.24\% to 22.46\%, InstructBLIP from 10.63\% to 4.52\%, Qwen2-VL from 7.57\% to 2.66\%, and Florence-2-large from 15.24\% to 3.51\%. Toxicity is also markedly reduced, with LLaVA-v1.5-7B decreasing from 63.30 to 23.11, MiniGPT-4 from 51.94 to 19.84, and Florence-2-large from 13.31 to 3.46. Similar improvements are observed across identity attacks, profanity, sexually explicit content, and threats. Likewise, in Table \ref{tab:llava_unconstrained} on RealToxicityPrompts dataset (RTP), EigenShield reduces ASR for LLaVA-v1.5-7B from 81.61\% to 19.20\%, marking a 76.5\% relative reduction. MiniGPT-4 sees ASR drop from 37.20\% to 20.37\%, InstructBLIP from 59.80\% to 41.21\%, and Qwen2-VL from 30.38\% to 17.25\%. Notably, unlike adversarial training and UNIGUARD, which require costly retraining, EigenShield operates entirely at inference time, filtering adversarial noise without modifying model parameters, thus maintaining computational efficiency. Tables \ref{tab:llava_constrained} and \ref{tab:qwen_constrained} in Appendix \ref{results} present a further analysis using constrained adversarial images.

In Table \ref{tab:rmt_clean}, when combining clean images from ImageNet with adversarial text inputs from RTP and HarmBench, EigenShield consistently lowers ASR across all models compared to the no-defense baseline. For LLaVA-v1.5-7B, ASR drops from 1.1\% to 0.3\% on RTP and from 0.7\% to 0.2\% on HarmBench, demonstrating a substantial improvement in adversarial resilience. MiniGPT-4 exhibits a similar trend, with ASR decreasing from 0.6\% to 0.1\% on RTP and from 0.4\% to 0.2\% on HarmBench. InstructBLIP also benefits significantly, with ASR reductions from 0.7\% to 0.4\% on RTP and from 0.5\% to 0.1\% on HarmBench. Qwen2-VL and Florence-2-large, which already exhibit lower baseline ASR values, still show meaningful reductions with EigenShield. Qwen2-VL maintains an ASR of 0.2\% on RTP while improving from 0.4\% to 0.1\% on HarmBench. Florence-2-large experiences ASR reductions from 0.5\% to 0.1\% on RTP and from 0.4\% to 0.2\% on HarmBench. These improvements indicate that EigenShield enhances robustness across diverse architectures, even for models with relatively lower initial vulnerability.

Fig.~\ref{fig:spider_plot} presents a comparative analysis of attack success rates (\%) across five adversarial attack methods on two vision-language models: LLaVA-v1.5-7B~\cite{liu2024visual} and MiniGPT-4~\cite{zhu2023minigpt}. The spider plot represents each attack along different axes, with the radial distance indicating the attack success rate—where a larger radius corresponds to higher vulnerability. The results demonstrate that EigenShield consistently achieves the lowest attack success rates against a diverse range of adversarial strategies. In contrast, the effectiveness of other defense methods varies, with some showing stronger resilience to specific attacks but failing against others. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/adv_spider.jpg}
\caption{Attack Success Rates (\%) across five adversarial attacks on (a) LLaVA-v1.5-7B \cite{liu2024visual} and (b) MiniGPT-4 \cite{zhu2023minigpt}. Radial axes represent attack methods, with larger radii indicating higher vulnerability. EigenShield consistently achieves the lowest attack success rates, while other defenses vary in effectiveness. }
    \label{fig:spider_plot}
\end{figure}

\subsection{Comparison with State-of-the-Art Defenses}

In Table \ref{tab:llava_unconstrained}, EigenShield consistently achieves the lowest ASR and toxicity while reducing harmful content more effectively than existing methods. For LLaVA-v1.5-7B, ASR drops from 81.61\% (no defense) to 19.20\%, outperforming adversarial training (28.21\%), UNIGUARD (25.17\%), and inference-time defenses like CIDER (24.73\%). It also significantly reduces profanity (67.22 $\rightarrow$ 15.31), explicit content (39.38 $\rightarrow$ 5.16), and toxicity (77.93 $\rightarrow$ 13.28), demonstrating strong mitigation of harmful language.

Similarly, on MiniGPT-4, EigenShield lowers ASR from 37.20\% to 20.37\%, surpassing UNIGUARD (24.98\%) and CIDER (22.74\%), while achieving the lowest toxicity (15.77). It also reduces identity attacks (2.94 $\rightarrow$ 1.39), profanity (26.53 $\rightarrow$ 12.80), and explicit content (12.76 $\rightarrow$ 8.53). For InstructBLIP, EigenShield cuts ASR from 59.80\% to 41.21\%, offering stronger defense than UNIGUARD (43.79\%) and CIDER (44.28\%), while also lowering toxicity from 54.55 to 40.69 and reducing explicit content and profanity more effectively than diffusion-based purification methods like DIFFPURE.

Qwen2-VL, already relatively robust, still benefits from EigenShield, with ASR dropping from 30.38\% to 17.25\%, outperforming UNIGUARD (19.87\%) and CIDER (21.72\%). Similar trends are seen in reducing identity attacks (2.13 $\rightarrow$ 1.51) and explicit content (12.53 $\rightarrow$ 5.99). For Florence-2-large, ASR falls from 36.47\% to 18.80\%, while toxicity reaches the lowest level (16.95), outperforming all baselines in profanity reduction. Table \ref{tab:rmt_constrained} in Appendix \ref{results} provides further analysis using constrained adversarial images. Additionally, example prompts and the corresponding responses before and after applying EigenShield can be found in Appendix \ref{results}.

%Unlike adversarial training, which requires costly retraining, EigenShield operates entirely at inference time, making it a practical, architecture-agnostic solution. While detection-based methods like CIDER rely on heuristics, EigenShield’s use of RMT enables better generalization across models without dataset-specific tuning. Its ability to suppress adversarial perturbations without degrading clean input performance makes it a highly effective and computationally efficient defense against multimodal jailbreak attacks. 



\section{Conclusions}
In this work, we have introduced EigenShield, a novel defense for Vision-Language Models (VLMs) that leverages Random Matrix Theory (RMT) to mitigate adversarial vulnerabilities. Unlike adversarial training and heuristic defenses, EigenShield operates entirely at inference time, making it architecture-independent, attack-agnostic, and computationally efficient. Using the spiked covariance model and Robustness-Based Nonconformity Score (RbNS), it effectively separates causal eigenvectors from adversarial noise, preserving semantic integrity. Extensive evaluations on state-of-the-art VLMs, including LLaVA-v1.5-7B, MiniGPT-4, InstructBLIP, Qwen2-VL, and Florence-2-large, demonstrate EigenShield’s ability to significantly reduce attack success rates (ASR) across various adversarial strategies. It consistently outperforms adversarial training, UNIGUARD, CIDER, and diffusion-based defenses in reducing ASR and harmful content while preserving clean input integrity. Our findings establish spectral analysis as a principled alternative to conventional defenses. Future directions could include exploring the applicability of spectral analyses in broader multimodal AI architectures such as vision and speech beyond VLMs. 



\bibliographystyle{IEEEtran}
\bibliography{main_IEEE}

\onecolumn

\begin{appendices}
\section{Spectral Confinement in Random Matrices}
\label{sec:spectral_confinement}

\noindent
\textbf{Setting.}
Consider an $N \times N$ real symmetric random matrix $M$ whose entries satisfy:
\begin{enumerate}
  \item $m_{ij}$ (for $1 \le i \le j \le N$) are independent random variables;
  \item $\mathbb{E}[m_{ij}] = 0$ and $\mathbb{E}[m_{ij}^2] = \sigma^2$, with finite higher moments;
  \item $m_{ij} = m_{ji}$, so $M$ is symmetric.
\end{enumerate}
Let $\lambda_1, \lambda_2, \dots, \lambda_N$ be the eigenvalues of $M$, and define the \emph{empirical spectral distribution} (ESD) by
\begin{equation}
\mu_N \;=\; \frac{1}{N} \sum_{k=1}^N \delta_{\lambda_k}.
\end{equation}
By Wigner Semicircle Law, in the limit $N \to \infty$, the eigenvalues of $M$ become densely packed in the interval $[-2\sigma, 2\sigma]$. Equivalently, $\mu_N$ converges in distribution to the so-called \emph{semicircle law} supported on $[-2\sigma,2\sigma]$. A standard way to prove this spectral confinement is via the method of moments. Define the $n$-th moment of $\mu_N$ as
\begin{equation}
m_n(\mu_N) \;=\; \int x^n \, d\mu_N(x)
\;=\;
\frac{1}{N} \sum_{k=1}^N \lambda_k^n
\;=\;
\frac{1}{N} \,\mathrm{Tr}(M^n).
\end{equation}
We aim to show that for each fixed $n$,
\begin{equation}
\frac{1}{N}\,\mathbb{E}\!\bigl[\mathrm{Tr}(M^n)\bigr]
\;\;\longrightarrow\;\;
\kappa_n
\quad
\text{as }N\to\infty,
\end{equation}

where $\kappa_n$ is the $n$-th moment of the \emph{semicircle distribution} with radius $2\sigma$:

\begin{equation}
    \rho_{\text{sc}}(x)
\;=\;
\frac{1}{2\pi \sigma^2} \,\sqrt{\,4\sigma^2 - x^2\,}\,\mathbf{1}_{\{|x|\leq2\sigma\}},
\quad
\kappa_n = \int_{-2\sigma}^{2\sigma} x^n \, \rho_{\text{sc}}(x)\,dx.
\end{equation}



\noindent
\textit{(a) Expanding the Trace.}
We write
\begin{equation}
\mathrm{Tr}(M^n)
\;=\;
\sum_{i_1=1}^N (M^n)_{i_1,i_1}
\;=\;
\sum_{i_1, i_2, \dots, i_n=1}^N m_{i_1 i_2}\, m_{i_2 i_3}\cdots m_{i_{n-1} i_n}\, m_{i_n i_1}.
\end{equation}
Thus,
\begin{equation}
m_n(\mu_N)
\;=\;
\frac{1}{N}\,\mathrm{Tr}(M^n)
\;=\;
\frac{1}{N} 
\sum_{i_1,\dots,i_n=1}^N 
m_{i_1 i_2}\cdots m_{i_n i_1}.
\end{equation}

\noindent
\textit{(b) Taking Expectations and Pairings.}
Because $\{m_{ij}\}$ are independent mean-zero random variables, any product $m_{i_1 i_2} m_{i_2 i_3}\cdots m_{i_n i_1}$ with an odd number of distinct $m_{ij}$ factors has zero expectation. Nonzero terms arise only when each factor appears an even number of times, so that factors ``pair up.'' In the large-$N$ limit, the combinatorial counting of these pairings is closely related to the \emph{Catalan numbers}. One finds that the leading-order terms in $\mathbb{E}\bigl[\mathrm{Tr}(M^n)\bigr]/N$ match exactly the moments of the semicircle distribution:
\begin{equation}
\frac{1}{N}\,\mathbb{E}\!\bigl[\mathrm{Tr}(M^n)\bigr]
\;\to\;
\kappa_n
\quad
\text{as }N\to\infty.
\end{equation}
Moreover, one can show (through variance bounds or concentration inequalities) that $m_n(\mu_N)$ converges \emph{almost surely} to the same value, implying that $\mu_N$ converges weakly to the semicircle law on $[-2\sigma,2\sigma]$. It remains to show that with high probability, no eigenvalue $\lambda_k$ lies far beyond $\pm(2\sigma+\varepsilon)$. \emph{Concentration inequalities} (e.g., Hanson--Wright) show that $\|M\|$ is almost surely of order $2\sigma\sqrt{N}$, thus once normalized by $\sqrt{N}$, all eigenvalues remain near $\pm 2\sigma$.
  
We conclude that for large $N$, all eigenvalues reside in the interval $[-2\sigma, 2\sigma]$ with overwhelming probability. Thus, the empirical spectral distribution \emph{cannot} place mass arbitrarily far out on the real line. Rhe above arguments establish that for symmetric random matrices with independent, mean-zero, variance-$\sigma^2$ entries,
\(
\text{(1) the eigenvalues are confined to }[-2\sigma,2\sigma],
\quad
\text{and}
\quad
\text{(2) they become densely packed in this interval as }N\to\infty.
\)
Any deviation from the semicircle shape in the empirical spectral distribution typically indicates extra structure (e.g.\ correlation, signal components (or causal components), or low-rank perturbations) rather than pure noise.




\newpage





\section{Formal Theorems on the Spiked Covariance Model}\label{sec:spikedmodel}

As discussed in Section \ref{sec:rmt_background}, we leverage the \emph{spiked covariance model} to detect and isolate ``signal-bearing'' directions in high-dimensional data. This section states the formal conditions under which certain eigenvalues of the sample covariance matrix separate from the bulk, thereby indicating low-rank ``spike'' components rather than pure noise. We adapt classical results from the Random Matrix Theory (RMT) literature---particularly \cite{paul2007asymptotics} and subsequent works on spiked models \cite{johnstone2001distribution}.

\subsection{Setup and Definitions}
We assume the population covariance matrix takes the form
\begin{equation}
    \boldsymbol{\Sigma} \;=\; \boldsymbol{\Sigma}_{\mathrm{signal}} \;+\; \sigma^{2}\,\mathbf{I}_{p},
\end{equation}
   

where
\(
\boldsymbol{\Sigma}_{\mathrm{signal}} 
= \mathbf{V}\,\boldsymbol{\Lambda}\,\mathbf{V}^{\top}
\)
is a matrix of rank
\(
r \ll p
\)
containing the ``spike'' signals, and 
\(
\sigma^2 \mathbf{I}_p
\)
denotes isotropic noise of variance 
\(\sigma^2\). Here, 
\(\boldsymbol{\Lambda} \in \mathbb{R}^{r \times r}\) 
stores the positive eigenvalues of the signal component, and 
\(\mathbf{V} \in \mathbb{R}^{p \times r}\) 
has orthonormal columns (the corresponding eigenvectors).

Given i.i.d.\ samples
\(\{\mathbf{x}_i\}_{i=1}^n \subset \mathbb{R}^p\)
with true covariance 
\(\boldsymbol{\Sigma}\),
we form the sample covariance matrix
\begin{equation}
   \mathbf{C}_n 
   \;=\;
   \frac{1}{n}\,\sum_{i=1}^n \mathbf{x}_i \,\mathbf{x}_i^\top 
   \;\in\;\mathbb{R}^{p\times p}.
\end{equation}
We operate in a high-dimensional limit, where both \(n\) and \(p\) grow to infinity at a fixed ratio \(c = p/n\). In the absence of spikes, the eigenvalues of \(\mathbf{C}_n\) concentrate near the classical Marchenko--Pastur (MP) bulk \cite{yaskov2016short}, spanning from 
\(\sigma^2(1-\sqrt{c})^2\)
to
\(\sigma^2(1+\sqrt{c})^2\). 
The spiked covariance model investigates how large population eigenvalues in 
\(\boldsymbol{\Sigma}\)
cause \emph{outlier} eigenvalues in 
\(\mathbf{C}_n\).

\subsection{Signal-Noise Separation via Outlier Eigenvalues}
Let 
\(\lambda'_1 \ge \lambda'_2 \ge \dots \ge \lambda'_r > 0\)
be the non-noise eigenvalues of 
\(\boldsymbol{\Sigma}_{\mathrm{signal}}\).
Let 
\(\hat{\lambda}_1 \ge \hat{\lambda}_2 \ge \dots \ge \hat{\lambda}_r\)
denote the top-\(r\) sample eigenvalues of 
\(\mathbf{C}_n\). We restate a known result (adapted from \cite{paul2007asymptotics}) to illustrate when outlier eigenvalues appear in the sample covariance.

\begin{theorem}[Spiked Covariance Model, adapted from \cite{paul2007asymptotics}]
\label{thm:spikedsamplecov}
Suppose 
\(\{\mathbf{x}_i\}_{i=1}^n\) 
are i.i.d.\ with mean \(\mathbf{0}\) and covariance \(\boldsymbol{\Sigma}\) as above, and let \(c = p/n > 0\) be fixed as \(p,n \to \infty\). Define
\begin{equation}
  \beta_j
  \;=\;
  \frac{\lambda'_j}{\sigma^2},
  \quad
  j=1,\dots,r,
\end{equation}
where \(\lambda'_j\) is an eigenvalue of \(\boldsymbol{\Sigma}_{\mathrm{signal}}\). Then:
\begin{enumerate}
\item \emph{(Bulk Behavior.)}
  If 
  \(\beta_j \le (1+\sqrt{c})^2\),
  the corresponding sample eigenvalue \(\hat{\lambda}_j\) remains inside the \emph{Marchenko--Pastur bulk} supported on
  \(\bigl[\sigma^2(1-\sqrt{c})^2,\,\sigma^2(1+\sqrt{c})^2\bigr]\).
  In other words, the spike is not large enough to detach from the noise region.
\item \emph{(Outlier Behavior.)}
  If 
  \(\beta_j > (1+\sqrt{c})^2\),
  there is an \emph{outlier} sample eigenvalue \(\hat{\lambda}_j\) that separates from the MP bulk. Moreover, 
  \(\hat{\lambda}_j\) converges almost surely to a limit \(\Lambda(\lambda'_j)\) satisfying
  \begin{equation}
    \Lambda(\lambda'_j)
    \;=\;
    \sigma^2 
    \left(
      \beta_j + \frac{c\,\beta_j}{\,\beta_j - 1\,}
    \right),
  \end{equation}
  and the associated sample eigenvector aligns closely with the corresponding population eigenvector in \(\mathbf{V}\).
\end{enumerate}
\end{theorem}

\noindent
\textbf{Discussion.}
Theorem~\ref{thm:spikedsamplecov} formally characterizes how strong signal eigenvalues in \(\boldsymbol{\Sigma}\) lead to isolated sample eigenvalues (outliers) in \(\mathbf{C}_n\). If the spike \(\lambda'_j\) is large enough (relative to \(\sigma^2\) and the aspect ratio \(c\)), the sample eigenvalue \(\hat{\lambda}_j\) is pulled away from the main bulk, allowing us to detect it under typical large-\(p\) conditions.

\subsection{Outlier Location and the Bulk Edge}
A key quantity that emerges in Theorem~\ref{thm:spikedsamplecov} is the \emph{Marchenko--Pastur upper edge}, namely:
\begin{equation}
  \lambda_{\mathrm{bulk,edge}}
  \;=\;
  \sigma^2\,(1 + \sqrt{c})^2.
\end{equation}
Any eigenvalue of \(\mathbf{C}_n\) that exceeds \(\lambda_{\mathrm{bulk,edge}}\) is likely to be a \emph{spike}. Formally, let \(\hat{\lambda}_j\) be the \(j\)-th largest sample eigenvalue; if \(\hat{\lambda}_j>\lambda_{\mathrm{bulk,edge}}\) with high probability, then \(\hat{\lambda}_j\) converges almost surely to a specific function of the corresponding population spike \(\lambda'_j\).

\begin{lemma}[Asymptotic Location of Spiked Sample Eigenvalues]
\label{lemma:spikelocation}
Under the assumptions of Theorem~\ref{thm:spikedsamplecov}, suppose \(\hat{\lambda}_j\) is an outlier eigenvalue of \(\mathbf{C}_n\) corresponding to a population spike \(\lambda'_j\). Then with probability tending to 1 as \(n,p\to\infty\),
\begin{equation}
   \hat{\lambda}_j 
   \;\xrightarrow{\text{a.s.}}\;
   \Lambda(\lambda'_j)
   \;=\;
   \sigma^2\,\Bigl(
     \beta_j 
     + 
     \frac{c\,\beta_j}{\beta_j -1}
   \Bigr),
   \quad\text{where}\quad
   \beta_j 
   \;=\;
   \frac{\lambda'_j}{\sigma^2}.
\end{equation}
In particular, if \(\beta_j\le(1+\sqrt{c})^2\), then \(\hat{\lambda}_j\) merges with the MP bulk, and no outlier emerges in that direction.
\end{lemma}

\subsection{Implications for EigenShield}
EigenShield employs these results as a principled mechanism to separate ``causal'' signal components from noise-based or weakly relevant components. In practice, any sample eigenvalue above the upper MP edge, \(\lambda_{\mathrm{bulk,edge}} = \sigma^2(1+\sqrt{c})^2\), is treated as a candidate \emph{signal direction}, which we retain in the ``causal'' subspace. By projecting onto these top outlier eigenvectors, EigenShield discards the bulk (noise) portion of the covariance and thereby filters out adversarial perturbations that primarily reside in lower-eigenvalue directions. These theorems thus ground our inference-phase defense in well-established RMT principles, ensuring that separated sample eigenvalues correspond to genuinely meaningful---rather than random or purely noise-driven---directions in the embedding space.





\subsection{Proof of Theorem \ref{thm:spikedsamplecov} and Lemma \ref{lemma:spikelocation}}
\label{sec:app_spiked_proofs}

Below, we present a concise proof sketch for Theorem \ref{thm:spikedsamplecov}(the spiked covariance result) and Lemma \ref{lemma:spikelocation} (the asymptotic location of spiked sample eigenvalues). These arguments follow classical techniques in Random Matrix Theory (RMT), particularly from \cite{paul2007asymptotics}, \cite{johnstone2001distribution}, and \cite{bai2010spectral}.

\textbf{Notation and Statement Recap.}

We have i.i.d.\ samples $\{\mathbf{x}_i\}_{i=1}^n$ in $\mathbb{R}^p$ with mean $\mathbf{0}$ and population covariance
\begin{equation}
    \boldsymbol{\Sigma}
    \;=\;
    \boldsymbol{\Sigma}_{\mathrm{signal}}
    \;+\;
    \sigma^2 \,\mathbf{I}_p,
\end{equation}
    

where $\boldsymbol{\Sigma}_{\mathrm{signal}}$ is low-rank and contains $r$ ``spikes.'' Denote the non-noise eigenvalues of $\boldsymbol{\Sigma}$ by $\{\lambda'_j\}_{j=1}^r > 0$.  We define the sample covariance
\begin{equation}
    \mathbf{C}_n
    \;=\;
    \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i \,\mathbf{x}_i^\top,
    \quad
    c \;=\; \frac{p}{n}.
\end{equation}
Let $\{\hat{\lambda}_j\}_{j=1}^p$ be the eigenvalues of $\mathbf{C}_n$, arranged in nonincreasing order. Theorem \ref{thm:spikedsamplecov} states that if $\lambda'_j / \sigma^2 > (1+\sqrt{c})^2$, then $\hat{\lambda}_j$ emerges as an outlier above the main Marchenko--Pastur (MP) bulk. Otherwise, it remains inside the bulk. Lemma  \ref{lemma:spikelocation} refines this result by providing the exact limit of such an outlier, namely
\begin{equation}
   \hat{\lambda}_j 
   \;\xrightarrow{\text{a.s.}}\;
   \sigma^2 \Bigl(\beta_j + \tfrac{c\,\beta_j}{\beta_j - 1}\Bigr), 
   \quad
   \text{where }
   \beta_j = \frac{\lambda'_j}{\sigma^2}.
\end{equation}

\subsubsection*{Proof Sketch of Theorem \ref{thm:spikedsamplecov}}
\noindent
\textbf{Step 1: Preliminaries and Transformation.}
Define 
\begin{equation}
  \mathbf{X} 
  \;=\; 
  \bigl[\mathbf{x}_1 \; \mathbf{x}_2 \; \cdots \; \mathbf{x}_n\bigr]
  \;\in\;\mathbb{R}^{p\times n},
\end{equation}
so that 
\(\mathbf{C}_n = \tfrac{1}{n} \mathbf{X}\mathbf{X}^\top.\)
Let the population covariance be
\begin{equation}
   \boldsymbol{\Sigma}
   \;=\;
   \mathbf{V} \,\boldsymbol{\Lambda} \,\mathbf{V}^\top
   \;+\;
   \sigma^2 \mathbf{I}_p,
\end{equation}
where $\boldsymbol{\Lambda} = \mathrm{diag}(\lambda'_1,\ldots,\lambda'_r)$ and $\mathbf{V}\in \mathbb{R}^{p\times r}$ is orthonormal in its columns.

\smallskip
\noindent
\textbf{Step 2: Decomposition into Spikes + Noise.}
One may write $\mathbf{X} = \boldsymbol{\Sigma}^{1/2}\,\mathbf{Z}$, where $\mathbf{Z} \in \mathbb{R}^{p\times n}$ has i.i.d.\ entries of mean zero and variance 1.  Then
\begin{equation}
  \mathbf{C}_n 
  \;=\;
  \frac{1}{n}\,\mathbf{X}\mathbf{X}^\top
  \;=\;
  \boldsymbol{\Sigma}^{1/2}\,
  \bigl(\tfrac{1}{n}\mathbf{Z}\mathbf{Z}^\top \bigr)\,
  \boldsymbol{\Sigma}^{1/2}.
\end{equation}
The matrix $\tfrac{1}{n}\mathbf{Z}\mathbf{Z}^\top$ is well known (by classical MP theory) to have its spectrum within $[(1-\sqrt{c})^2,\, (1+\sqrt{c})^2]$ with high probability, when $p,n \to \infty$ at ratio $c=p/n$.

\smallskip
\noindent
\textbf{Step 3: Outlier Condition.}
Focus on a single spike $\lambda'_j$ in the signal covariance. Define 
\(\beta_j = \tfrac{\lambda'_j}{\sigma^2}\).
We can write
\begin{equation}
  \boldsymbol{\Sigma}
  \;=\;
  \sigma^2 \,\Bigl(\mathbf{I}_p 
    \;+\; 
    \mathbf{Q}
  \Bigr),
  \quad
  \text{where} \;\;
  \mathbf{Q}
  \;=\;
  \sigma^{-2}\,\boldsymbol{\Sigma}_{\mathrm{signal}}.
\end{equation}
When $\beta_j$ exceeds $(1+\sqrt{c})^2$, the eigenvalue perturbation arguments (via either the Stieltjes transform or matrix determinant lemmas) show that a corresponding sample eigenvalue $\hat{\lambda}_j$ must leave the main bulk. If $\beta_j \le (1+\sqrt{c})^2$, we show there is not enough spectral ``pull'' for an outlier to form, so $\hat{\lambda}_j$ remains inside the bulk.

\smallskip
\noindent
\textbf{Step 4: Technical Tools (Stieltjes Transform or Determinant Identities).}
A standard approach is to use the determinant characterization 
\begin{equation}
  \det \bigl(
    \mathbf{C}_n - \ell \mathbf{I}_p
  \bigr) 
  \;=\; 
  \det \Bigl(
    \boldsymbol{\Sigma}^{1/2}
    \Bigl[\tfrac{1}{n}\mathbf{Z}\mathbf{Z}^\top - \tfrac{\ell}{\sigma^2} \mathbf{I}_p + \dots\Bigr]
    \,\boldsymbol{\Sigma}^{1/2} 
  \Bigr).
\end{equation}
One can track how eigenvalues $\ell = \hat{\lambda}_j$ appear as zeros of this determinant and then isolate the spike contributions via rank-one updates if $\boldsymbol{\Sigma}_{\mathrm{signal}}$ is rank 1 (extended to rank $r$ by block-decomposition).  The condition $\beta_j > (1+\sqrt{c})^2$ emerges from analyzing the perturbed Stieltjes transform inside or outside the main spectral arc.  

\smallskip
\noindent
\textbf{Step 5: Almost-Sure Convergence.}
Lastly, to promote these claims from expectations or weak convergence to almost-sure statements, one typically applies standard concentration bounds (matrix Chernoff bounds) to ensure that with probability approaching 1, the eigenvalue shifts cannot exceed small $\varepsilon$ thresholds. Thus, $\hat{\lambda}_j$ indeed separates from the MP bulk almost surely when $\beta_j > (1+\sqrt{c})^2$, concluding the main proof.

\medskip
\noindent
This completes the proof of Theorem \ref{thm:spikedsamplecov}.

\subsubsection*{Proof Sketch of Lemma \ref{lemma:spikelocation}(Asymptotic Outlier Location)}
Lemma~D.2 refines the outlier result by showing that once a spiked sample eigenvalue $\hat{\lambda}_j$ emerges above the bulk, it converges to the specific function
\begin{equation}
  \Lambda(\lambda'_j)
  \;=\;
  \sigma^2
  \Bigl(
    \beta_j \;+\; \frac{c\,\beta_j}{\,\beta_j -1\,}
  \Bigr),
  \quad
  \text{where}\quad
  \beta_j 
  \;=\;
  \frac{\lambda'_j}{\sigma^2}.
\end{equation}

\paragraph{Step 1: Preliminary Rank-$r$ Expansion.}
Let $\boldsymbol{\Sigma}_{\mathrm{signal}}$ have the eigen-decomposition
\(
   \boldsymbol{\Sigma}_{\mathrm{signal}}
   =
   \mathbf{V}\,\boldsymbol{\Lambda}\,\mathbf{V}^\top
\)
with $\boldsymbol{\Lambda}=\mathrm{diag}(\lambda'_1,\ldots,\lambda'_r)$. One typically starts by examining a rank-1 perturbation (say $\lambda'_1$, with eigenvector $v_1$), proving the location of the resulting outlier in the limit. The rank-$r$ case follows by induction or block matrix arguments, as the presence of multiple spikes yields multiple outliers under similar reasoning.

\paragraph{Step 2: Characterizing Outlier via Stieltjes Transform.}
Define the Stieltjes transform of the empirical spectral distribution (ESD) by 
\begin{equation}
  m_n(z)
  \;=\;
  \frac{1}{p} \,\mathrm{Tr}\bigl((\mathbf{C}_n - z \,\mathbf{I}_p)^{-1}\bigr),
  \quad
  z \in \mathbb{C}^{+}.
\end{equation}
Its limit $m(z)$ for the pure noise case $\sigma^2 \mathbf{I}_p$ is known from Marchenko--Pastur theory. In the presence of a spike $\lambda'_j$, one can analyze how $m_n(z)$ (or the companion determinant) shifts. By matching singularities of the limiting transform with the location of $\hat{\lambda}_j$, one obtains an explicit algebraic equation whose solution is precisely
\begin{equation}
   \Lambda(\lambda'_j)
   \;=\;
   \sigma^2 \left(
     \beta_j
     \;+\;
     \frac{c\,\beta_j}{\,\beta_j -1\,}
   \right).
\end{equation}
(See \cite{paul2007asymptotics} for the full derivation, which involves solving a quadratic equation for the limiting outlier position.)

\paragraph{Step 3: Convergence and Eigenvector Alignment.}
After establishing that $\hat{\lambda}_j \to \Lambda(\lambda'_j)$ in probability (or in distribution), one again applies high-probability concentration results to lift it to almost-sure convergence.  A related argument with the resolvent or rank-1 update formulas also shows that the sample eigenvector associated with $\hat{\lambda}_j$ converges to the population spike direction (i.e., the corresponding column of $\mathbf{V}$).

\paragraph{Conclusion.}
Hence, whenever $\beta_j = \lambda'_j/\sigma^2$ satisfies $\beta_j > (1+\sqrt{c})^2$, a distinct sample outlier $\hat{\lambda}_j$ emerges and converges almost surely to 
\(
   \sigma^2\left(
     \beta_j + \tfrac{c\,\beta_j}{\,\beta_j -1\,}
   \right),
\)
exactly matching the statement of Lemma \ref{lemma:spikelocation}.

\bigskip
\noindent
This completes our proof sketches for Theorem \ref{thm:spikedsamplecov} and Lemma \ref{lemma:spikelocation}.




























\newpage


\section{Mathematical Justification for Information Gain from Causal Directions} \label{sec:causal_direction}

To confirm our central claim that projections onto causal eigenvector directions enhance the fidelity of input data reconstruction and yield significant information gain about the input data distribution, we now present a justification rooted in the principles of information theory. This section will formally demonstrate how a reduction in entropy, when conditioning on projections along causal directions, directly translates to an increase in information gain regarding the input data distribution and improved reconstruction.

\subsection*{Entropy as a Fundamental Measure of Uncertainty in Data Representation}

We begin by recalling the foundational concept of Shannon entropy, now interpreting it as a measure of uncertainty associated with representing the input data distribution. Let us formally define the random variable $X$ to represent the high-dimensional input data (or its feature representation), taking values from a continuous space $\mathcal{X}$ with a probability density function $P(X)$. For simplicity in this derivation, we will consider a discretized version, where $X$ takes values from a discrete set $\{x_1, \ldots, x_N\}$, each occurring with a probability $P(X = x_i) = p_i$, where $\sum_{i=1}^N p_i = 1$. The Shannon entropy of $X$, denoted as $H(X)$, is then mathematically expressed as:

\begin{equation}
    H(X) = -\sum_{i=1}^N \, p_i \,\log_2\bigl(p_i\bigr).
\end{equation}



Intuitively, $H(X)$ captures the inherent randomness or complexity within the input data distribution. High entropy signifies a distribution that is spread out, representing high variability and uncertainty in the data. Conversely, low entropy indicates a distribution that is concentrated, implying a more structured and predictable data representation.

\subsection*{Entropy Reduction as a Signature of Enhanced Data Representation}

In the context of unsupervised reconstruction, predictability now relates to how well we can reconstruct or represent the input data distribution. Lower entropy in the representation implies a more efficient and less uncertain representation, which should facilitate better reconstruction. Consider the limiting case where $H(X) = 0$. This would ideally occur if the data was perfectly deterministic and concentrated at a single point in the representation space, implying maximal predictability and minimal uncertainty, although practically such a scenario is less relevant for real-world data.  As entropy increases, the data representation becomes more complex and less predictable, potentially hindering faithful reconstruction.

From an information-theoretic viewpoint, entropy here represents the average number of bits needed to encode data samples from $X$. Lower entropy suggests that fewer bits are needed to represent the data distribution, indicating a more structured and less random data representation, and thus, potentially enhanced reconstructability.

\subsection*{Conditional Entropy and Quantifying Information Gain for Data Reconstruction}

To quantify the information gained about the input data distribution through directional projections, we again introduce the concept of conditional entropy. Let $U$ represent a random variable denoting the projection of input data onto a specific direction, such as a causal eigenvector, resulting in a random variable $U$. The conditional entropy $H(X \mid U)$ measures the remaining uncertainty about the input data distribution $X$ after we have observed the value of the projection $U$. It is mathematically defined as:

\begin{equation}
H(X \mid U) = \sum_{u} P(U=u)\, H(X \mid U=u),
\end{equation}
where the inner term, $H(X \mid U=u)$, represents the entropy of the input data distribution given a specific value $u$ of the projection:

\begin{equation}
H(X \mid U=u) = -\sum_{i=1}^N \, P(X = x_i \mid U=u)\,\log_2\bigl(P(X = x_i \mid U=u)\bigr).
\end{equation}

The crucial metric for quantifying information gain in the context of data reconstruction is the reduction in entropy of the input data distribution, which is the difference between the marginal entropy $H(X)$ and the conditional entropy $H(X \mid U)$: $H(X) - H(X \mid U)$. This difference represents the amount of uncertainty about $X$ that is eliminated by knowing $U$, and thus, the information gained about $X$ from $U$ relevant to data reconstruction.  In information theory, this is directly related to Mutual Information, $I(X; U) = H(X) - H(X \mid U)$. A positive value for this difference indicates that $U$ provides non-zero information about $X$, enhancing our ability to represent and reconstruct the input data distribution.

\subsection*{Mathematical Proof of Information Gain for Enhanced Reconstruction from Causal Directions}

Our core hypothesis asserts that for a ``causal" direction $v_j$, the projection $U_j$ will yield a conditional entropy $H(X \mid U_j)$ that is strictly less than the marginal entropy $H(X)$:

\begin{equation}
H(X \mid U_j) < H(X).
\end{equation}

By definition, the mutual information between $X$ and $U_j$ is given by:
    \begin{equation}
    I(X;U_j) = H(X) - H(X \mid U_j).
    \end{equation}
Since we have hypothesized $H(X \mid U_j) < H(X)$, it follows that their difference is strictly positive:
    \begin{equation}
    H(X) - H(X \mid U_j) > 0.
    \end{equation}
Therefore, we conclude that the mutual information is strictly positive:
    \begin{equation}
    I(X; U_j) > 0.
    \end{equation}

This derivation formally demonstrates that if the conditional entropy of the input data distribution, given the projection onto a direction $v_j$, is less than the marginal entropy, then the mutual information between the input data distribution and the projection is strictly positive. This positive mutual information signifies that knowing $U_j$, the projection onto the ``causal" direction, indeed decreases the uncertainty about the input data distribution $X$, enhancing the quality of data representation and facilitating improved reconstruction, as evaluated by metrics like KL divergence.

\subsection*{Direct Connection between Mutual Information and KL Divergence in VAE Setup}

To establish a more direct connection between maximizing mutual information and minimizing KL Divergence within our VAE framework, let's consider the VAE objective function and its relation to these information-theoretic quantities.

Recall that in our RbNS framework, we train a conditional VAE $\hat{g}_j(u_{i,j})$ for each outlier direction $v_j$. The VAE is trained to reconstruct input features $z_i$ (derived from input images $x_i$ via a pre-trained encoder) from the 1D projection $u_{i,j} = v_{x_i, j}^\top v_j$. The standard VAE objective function maximizes the Evidence Lower Bound (ELBO), which can be written as:

\begin{equation}
    \mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(z|x)} [\log p(x|z)] - \text{KL}(q(z|x) || p(z)),
\end{equation}
where $q(z|x)$ is the encoder's approximate posterior, $p(x|z)$ is the decoder's likelihood, and $p(z)$ is the prior distribution. In our conditional VAE setup, we are interested in the conditional distribution given the projection $u_{i,j}$. Let's adapt the ELBO for our conditional VAE $\hat{g}_j(u_{i,j})$ that reconstructs features $z_i$ from projection $u_{i,j}$. Ideally, we want to maximize the conditional mutual information $I(Z; X | U_j)$, which represents the information about the input features $Z$ (derived from $X$) that is preserved in the projection $U_j$.

While a direct identity equating the VAE loss to $I(Z; U_j)$ or $I(Z; X | U_j)$ isn't straightforward due to the approximate inference in VAEs, we can highlight the intuitive link to KL Divergence.  Our performance metric $\text{Perf}_{j,k} = \text{KL}(P_{z_i} || Q_{\hat{z}_i|u_{i,j}^{(k)}})$ directly measures how well the VAE decoder $Q_{\hat{z}_i|u_{i,j}^{(k)}}$ approximates the true distribution of input features $P_{z_i}$, conditioned on the projection $u_{i,j}^{(k)}$.

Consider the ideal scenario where a direction $v_j$ is highly "causal" and thus, the projection $U_j$ captures most of the essential information about the input features $Z$. In this case, the conditional entropy $H(Z | U_j)$ would be low, and the mutual information $I(Z; U_j) = H(Z) - H(Z | U_j)$ would be high. A well-trained VAE $\hat{g}_j(u_{i,j})$ in this scenario, aiming to minimize reconstruction error and regularize the latent space, will learn to make the decoder distribution $Q_{\hat{z}_i|u_{i,j}^{(k)}}$ a close approximation of the true conditional distribution $P_{z_i | u_{i,j}^{(k)}}$.  The KL divergence $\text{KL}(P_{z_i} || Q_{\hat{z}_i|u_{i,j}^{(k)}})$ then precisely quantifies the dissimilarity between these distributions.

Therefore, while not a strict mathematical identity, the following intuitive connection emerges:

\textit{Directions $v_j$ that maximize mutual information $I(Z; U_j)$ (and thus minimize conditional entropy $H(Z|U_j)$) are expected to enable VAE decoders $\hat{g}_j(u_{i,j})$ to learn more accurate approximations of the true conditional feature distribution $P_{z_i | u_{i,j}^{(k)}}$. This, in turn, should empirically lead to a minimization of the KL Divergence metric $\text{KL}(P_{z_i} || Q_{\hat{z}_i|u_{i,j}^{(k)}})$, indicating a more faithful reconstruction of the input feature distribution and enhanced data representation.}

This connection provides a theoretical rationale for using KL Divergence as a performance metric in our RbNS framework. By selecting eigenvector directions that lead to lower KL Divergence in the VAE reconstruction task, we are implicitly prioritizing directions that are more informative about the input data distribution, aligning with the principle of maximizing information gain from causal directions.




\newpage

\section{Additional Experimental Results} \label{results}

\subsection{Example Prompts and EigenShield's Effect on Responses}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\linewidth]{Figures/random_20.jpg}
    \includegraphics[width=0.48\linewidth]{Figures/muslim.jpg}
    
    \vspace{5mm}
    
    \includegraphics[width=0.48\linewidth]{Figures/bisexual.jpg}
    \includegraphics[width=0.48\linewidth]{Figures/Jew.jpg}
    
    \vspace{5mm}
    
    \includegraphics[width=0.48\linewidth]{Figures/Racism.jpg}
    \includegraphics[width=0.48\linewidth]{Figures/twitter.jpg}
    \caption{Example prompts and corresponding model responses before and after applying EigenShield. The images represent various sensitive topics, including religion, sexuality, and social issues. EigenShield effectively mitigates adversarial vulnerabilities by filtering out harmful or policy-violating responses while preserving semantic coherence.}
    \label{fig:examples}
\end{figure}


\newpage

\subsection{Evaluating EigenShield Against Constrained Adversarial Attacks}

The results presented in Tables \ref{tab:rmt_constrained}, \ref{tab:llava_constrained}, and \ref{tab:qwen_constrained} highlight the effectiveness of EigenShield in mitigating multimodal jailbreak attacks on Vision-Language Models (VLMs) under constrained adversarial settings ($\epsilon = \frac{64}{255}$. EigenShield consistently achieves the lowest attack success rates (ASR) and toxicity scores across all evaluated models, outperforming existing defenses such as adversarial training, UNIGUARD, CIDER, and image transformation-based approaches.

Table \ref{tab:rmt_constrained} demonstrates that EigenShield significantly reduces ASR and harmful content generation across multiple VLMs, including LLaVA-v1.5-7B, MiniGPT-4, InstructBLIP, Qwen2-VL, and Florence-2-large. Notably, EigenShield achieves a dramatic drop in ASR for LLaVA-v1.5-7B (73.73\% to 17.87\%) and MiniGPT-4 (41.77\% to 14.82\%), while also reducing toxicity and explicit content scores by a large margin. These improvements suggest that EigenShield effectively isolates and filters adversarial perturbations by projecting embeddings onto a causal subspace, preserving semantically meaningful representations while eliminating adversarial artifacts.

Tables \ref{tab:llava_constrained} and \ref{tab:qwen_constrained} provide a comparative analysis against state-of-the-art defenses on constrained adversarial visual and textual attacks. EigenShield consistently achieves the best or second-best performance across all metrics. On LLaVA-v1.5-7B, EigenShield reduces ASR to 14.19\%, outperforming adversarial training (25.48\%) and inference-time defenses like CIDER (16.42\%). Similarly, for MiniGPT-4, EigenShield attains the lowest ASR (15.61\%) while also minimizing toxicity (13.78\%). This trend continues for InstructBLIP and Qwen2-VL, where EigenShield outperforms or matches the best-performing defenses in mitigating adversarial vulnerabilities.

Unlike adversarial training, which requires costly retraining and fine-tuning, EigenShield operates entirely at inference time, making it a lightweight, architecture-agnostic solution. Furthermore, while detection-based methods like CIDER rely on heuristics and predefined thresholds, EigenShield leverages Random Matrix Theory (RMT) to systematically separate causal and correlational eigenvectors, resulting in more robust and generalizable defense mechanisms. The ability to significantly reduce harmful content generation across diverse attack strategies and model architectures underscores EigenShield's efficacy in strengthening the robustness of VLMs against adversarial threats.



\begin{table*}[h]
\centering
\footnotesize % Reduce font size to fit
\caption{RMT-Based Defense Against Multimodal Jailbreak Attacks Using Perspective API Metrics. Dataset: \textbf{constrained} adversarial visual attack \cite{qi2024visual} and two adversarial texts, RTP \cite{gehman2020realtoxicityprompts} and HarmBench \cite{mazeika2024harmbench} on VLMs. ‘$\downarrow$’ means lower is better. }
\label{tab:rmt_constrained} 
\setlength{\tabcolsep}{1pt} % Reduce column spacing slightly
\begin{tabular}{lcccccc ccccccc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{RTP} \cite{gehman2020realtoxicityprompts}} & \multicolumn{6}{c}{\textbf{HarmBench} \cite{mazeika2024harmbench}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
& \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} \\
& \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  &  & & \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  & &  \\
\midrule

LLaVA-v1.5-7B & 73.73 & 16.76 & 59.55 & 30.28 & 34.70 & 69.47  
& 58.31 & 12.84 & 42.73 & 22.05 & 20.57 & 54.90
\\
\quad +EigenShield 
& \textbf{17.87} &\textbf{ 0.94} & \textbf{13.46} & \textbf{3.57} & \textbf{0.80} & \textbf{11.93}  
& \textbf{21.43} & \textbf{3.08} & \textbf{14.77} & \textbf{5.91} & \textbf{1.68} & \textbf{19.84}
\\
\midrule

MiniGPT-4 & 41.77 & 2.92 & 29.16 & 13.45 & 2.51 & 36.01  
& 39.26 & 5.10 & 26.82 & 14.02 & 3.19 & 33.41
\\
\quad +EigenShield
& \textbf{14.82} & \textbf{1.12} & \textbf{10.49} & \textbf{5.11} & \textbf{1.05} & \textbf{13.69}  
& \textbf{15.94} & \textbf{2.26} & \textbf{11.27} & \textbf{6.32} & \textbf{1.52} & \textbf{14.81}
\\
\midrule
InstructBLIP & 58.47 & 7.34 & 43.62 & 19.60 & 4.42 & 55.55  
& 9.82  & 1.15 & 6.26 & 2.79 & 0.72 & 8.73
\\
\quad +EigenShield
& \textbf{39.06} & \textbf{4.61} & \textbf{28.39} & \textbf{9.07} & \textbf{1.96} & \textbf{37.83 } 
& \textbf{3.21}  & \textbf{0.48 }& \textbf{2.08} & \textbf{0.97} & \textbf{0.24} & \textbf{2.71}
\\
\midrule

Qwen2-VL & 12.41 & 1.06 & 8.94 & 3.82 & 1.09 & 10.01  
& 6.37  & 0.83 & 4.86 & 1.82 & 0.48 & 5.84
\\
\quad +EigenShield
& \textbf{3.78} & \textbf{0.42} & \textbf{2.21} & \textbf{1.02} & \textbf{0.20} & \textbf{3.05}  
& \textbf{1.92} & \textbf{0.27} & \textbf{1.31}& \textbf{0.58} & \textbf{0.14} & \textbf{1.75}
\\
\midrule

Florence-2-large& 22.68 & 2.71 & 14.41 & 6.28 & 2.17 & 19.35  
& 10.34 & 1.64 & 6.72 & 2.84 & 1.21 & 9.27
\\
\quad +EigenShield
&\textbf{ 6.41} & \textbf{0.92} & \textbf{3.85} & \textbf{1.54} & \textbf{0.63} & \textbf{5.87}  
& \textbf{3.06} & \textbf{0.52} & \textbf{2.02} & \textbf{0.88} & \textbf{0.31} & \textbf{2.86}
\\
\bottomrule
\end{tabular}
\end{table*}













\begin{table*}[h]
\centering
\small
\setlength{\tabcolsep}{3pt} % Adjust column spacing for compactness
\caption{Comparison of EigenShield with state-of-the-art defenses against multimodal jailbreak attacks using Perspective API metrics. The dataset includes \textbf{constrained} adversarial visual attacks \cite{qi2024visual} and RTP \cite{gehman2020realtoxicityprompts} adversarial text on LLaVA-v1.5-7B and MiniGPT-4. Lower values are better. The best and second-best performances for each model are highlighted in \textbf{bold} and \underline{underlined}.}
\label{tab:llava_constrained}
\resizebox{\textwidth}{!}
{%
\begin{tabular}{lcccccc | ccccccc}
\toprule
\multirow{3}{*}{\textbf{Defense}} & \multicolumn{6}{c}{\textbf{LLaVA-v1.5-7B}} & \multicolumn{6}{c}{\textbf{MiniGPT-4}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
& \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} \\
& \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  &  & & \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  & &  \\
\midrule

No Defense & 73.73 & 16.76 & 59.55 & 30.28 & 34.7 & 69.47 & 41.77 & 2.92 & 29.16 & 13.45 & 2.51 & 36.01 \\

Adv. Training & 25.48 & 2.05 & 21.72 & 8.46 & 1.17 & 19.39 & 25.27 & 2.04 & 19.4 & 9.71 & 1.38 & 21.5 \\

UNIGUARD &  19.95 & \underline{1.17} & 17.23 & 5.69 & \underline{0.68} & 13.33 & 21.02 & \underline{1.33} & \underline{14.93} & 7.42 & \underline{0.92} & 16.18  \\

BLURKERNEL & 31.53 & 1.58 & 25.6 & 10.51 & 2.61 & 26.86 &  36.35 & 2.28 & 26.29 & 12.43 & 1.94& 30.85 \\

COMP-DECOMP & 34.11 & 2.17 & 26.52 & 11.76 & 2.7 & 31.94& 34.35 & 2.28 & 24.2 & 12.1 & 1.78 & 29.78 \\

DIFFPURE  & 30.27 & 2.51 & 23.08 & 9.28 & 3.34 & 26.59 & 42.56 & 3.2 & 29.69 & 14.38 & 2.61 & 36.42  \\

CIDER & \underline{16.42} & 1.29 & \underline{15.80} & \underline{5.04} & 0.75 & \underline{12.70} & \underline{18.93} & 1.58 & 15.72 & \underline{6.44} & 1.26 & \underline{16.03} \\

EigenShield & \textbf{14.19} & \textbf{0.88} & \textbf{12.6} & \textbf{4.34} & \textbf{0.59} & \textbf{10.93} & \textbf{15.61} & \textbf{1.29} & \textbf{12.83} & \textbf{5.11} & \textbf{0.84} & \textbf{13.78} \\

\bottomrule
\end{tabular}%
}
\end{table*}





\begin{table*}[h]
\centering
\small
\setlength{\tabcolsep}{3pt} % Adjust column spacing for compactness
\caption{Comparison of EigenShield with state-of-the-art defenses against multimodal jailbreak attacks using Perspective API metrics. The dataset includes \textbf{constrained} adversarial visual attacks \cite{qi2024visual} and RTP \cite{gehman2020realtoxicityprompts} adversarial text on InstructBLIP and Qwen2-VL. Lower values are better. The best and second-best performances for each model are highlighted in \textbf{bold} and \underline{underlined}.}
\label{tab:qwen_constrained}
\resizebox{\textwidth}{!}
{%
\begin{tabular}{lcccccc | ccccccc}
\toprule
\multirow{3}{*}{\textbf{Defense}} & \multicolumn{6}{c}{\textbf{InstructBLIP}} & \multicolumn{6}{c}{\textbf{Qwen2-VL}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
& \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Attack} & \textbf{Identity} & \textbf{Profanity} & \textbf{Sexually} & \textbf{Threat} & \textbf{Toxicity} \\
& \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  &  & & \textbf{Success}  & \textbf{Attack}  &  & \textbf{Explicit}  & &  \\
\midrule

No Defense & 58.47 & 7.34 & 43.62 & 19.6 & 4.42 & 55.55 &  12.41 & 1.06 & 8.94 & 3.82 & 1.09 & 10.01 \\



UNIGUARD &  \underline{41.03} & \underline{4.92} & \underline{33.11} & 13.68 & \underline{1.83} & \underline{37.86} & \underline{8.72} & \underline{0.83} & \underline{5.97} & \underline{2.45} & \underline{0.53} & \underline{6.78}    \\

BLURKERNEL & 55.55  &6.34 & 42.2 & 18.93 & 5.42 & 51.88 & 11.32 & 1.74 & 8.26 & 3.6 & 0.91 & 9.17  \\

COMP-DECOMP & 57.8 & 7.51 & 44.54  & 19.52 & 5.09 & 54.88 & 11.2 & 1.09 & 8.16 & 3.57 & 0.99 & 9.31\\

DIFFPURE  &  56.13 & 7.09 & 43.37 & 18.68 & 4.34 & 53.38 & 10.71 & 1.16 & 7.45 & 3.72 & 0.87 & 8.74\\

CIDER & 44.51 & 5.38 & 37. 19 & \underline{11.93} & \textbf{1.75} & 39.83 &  9.35 & 0.97 & 6.49 & 2.73 & 0.66 & 7.18 \\

EigenShield &  \textbf{39.06} & \textbf{4.61} & \textbf{28.39} & \textbf{9.07} & 1.96 & \textbf{37.83 }  & \textbf{3.78} & \textbf{0.42} & \textbf{2.21} & \textbf{1.02} & \textbf{0.20} & \textbf{3.05}  \\

\bottomrule
\end{tabular}%
}
\end{table*}








\end{appendices}






\end{document}
