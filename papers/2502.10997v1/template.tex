\documentclass{article}


\usepackage{arxiv}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{algpseudocode}
\input{macro.tex}


\title{New Rates in Stochastic Decision-Theoretic Online Learning under Differential Privacy}


\author{
Ruihan Wu \\
  University of California, San Diego \\
  \texttt{ruw076@ucsd.edu} \\
  \And
  Yu-Xiang Wang \\
  University of California, San Diego \\
  \texttt{yuxiangw@ucsd.edu} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
\citet{hu2024open} posed an open problem: \emph{what is the optimal instance-dependent rate for the stochastic decision-theoretic online learning (with $K$ actions and $T$ rounds) under $\varepsilon$-differential privacy?}
  Before, the best known upper bound and lower bound are $O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K\log T}{\varepsilon}\right)$ and $\Omega\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}\right)$ (where $\Delta_{\min}$ is the gap between the optimal and the second actions).
  In this paper, we partially address this open problem by having two new results.
  First, we provide an improved upper bound for this problem $O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log^2K}{\varepsilon}\right)$, where the $T$-dependency has been removed. % from the term of $\varepsilon$.
  Second, we introduce the \textit{deterministic setting}, a weaker setting of this open problem, where the received loss vector is deterministic and we can focus on the analysis for $\varepsilon$ regardless of the sampling error.
  At the deterministic setting, we prove upper and lower bounds that match at  $\Theta(\frac{\log K}{\varepsilon})$, while a direct application of the analysis and algorithms from the original setting still leads to an extra log factor.
  Technically, we introduce the \textit{Bernoulli resampling} trick, which enforces a monotonic property for the output from report-noisy-max mechanism that enables a tighter analysis.  
Moreover, by replacing the Laplace noise with Gumbel noise, we derived explicit integral form that gives a tight characterization of the regret in the deterministic case.
\end{abstract}


\keywords{differential privacy, online learning}

\input{introduction.tex}
\input{preliminary.tex}
\input{main_res.tex}

\section*{Acknowledgement}
RW would like to thank National Science Foundation NSF (CIF-2402817, CNS-1804829), SaTC-2241100,
CCF-2217058, ARO-MURI (W911NF2110317), and ONR under N00014-24-1-2304 for research support.  YW was partially supported by NSF CNS 2048091.


%\bibliographystyle{unsrt}  
\bibliographystyle{abbrvnat}  
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\bibliography{yourbibfile}
\newpage
\appendix
\input{appendix.tex}

\end{document}
