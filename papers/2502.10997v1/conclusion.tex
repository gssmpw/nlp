\section{Conclusion}
\label{sec:conclusion}
We study the stochastic decision-theoretic online learning under differential privacy, an open problem prosed by \citet{hu2024open}.
In this paper, we propose new variants of a previous algorithm and conduct a novel analysis, which together prove a new rate to $O\left(\frac{\log(K)}{\Delta_{\min}} + \frac{(\log K)^2}{\varepsilon}\right)$, an improvement from the existing result as the $T$-dependency has been removed from $\varepsilon$-term -- $T$ is generally considered as a heavier factor than $K$, e.g. $K\approx O(\log T)$ is considered in \citet{hu2024open}.
Moreover, we propose the \textit{deterministic setting} to focus on only differential privacy regardless of the sampling noise.
Under this setting, we propose another variant of the previous algorithm and through a novel analys, we prove an optimal rate $O\left(\frac{ \log K}{\varepsilon}\right)$ for this simplified setting.