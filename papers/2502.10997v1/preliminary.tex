%\section{Preliminaries}
\subsection{Problem setting}
In this paper, we focus on the open problem posed by \citet{hu2024open} and we will go through the problem setting in this section.
The stochastic variant of decision-theoretic online learning~\citep{freund1997decision} assumes there are $K$ actions.
Each action $i\in [K]$ has a fixed underlying loss distribution $\calP_i$ that is unknown to the learner and whose support is contained in $[0,1]$.
At each time step $t=1, \cdots, T$:
\begin{enumerate}[leftmargin=*,nosep]
\item The learner picks any action $I_t\in [K]$ according to any (randomized) algorithm $\calM$.
\item The learning algorithm suffers loss $\ell_{I_t}^{(t)}\sim \calP_{I_t}$.
\item The learner observes the losses of all the actions, a loss vector $\ell^{(t)}:=(\ell^{(t)}_1, \cdots, \ell^{(t)}_K)\sim \calP_1\times \cdots \times \calP_K$.
\end{enumerate}
The goal is to minimize the pseudoregret $\mathrm{PseudoRegret}(\calA; T, \calP_1, \cdots, \calP_K)$, which is the gap between the expectation of accumulated suffered losses and the minimum expectation of accumulated loss among $K$ actions:
$$
\bbE\left[\sum_{t=1}^T \ell_{I_t}^{(t)}\right] - \min_{i\in [K]}\bbE\left[\sum_{t=1}^T \ell_{i}^{(t)}\right],
$$
where the randomness in the expectation is contributed by both the loss vector $\ell^{(t)}$ and the randomized algorithm $\calM$.
We further denote $\mu_i$ as the expectation of the loss from action $i$, $\bbE_{\ell_i\in \calP_i}\left[\ell_{i}\right]$.
Without the loss of generality, we assume $\mu^* = \mu_1 < \mu_2\leq\cdots \mu_K$.
Furthermore, we denote the gaps $\Delta_i:=\mu_i - \mu_1$ and specifically, we denote the gap between the optimal and second optimal by $\Delta_{\min}:= \mu_2 - \mu_1$.
With the notations of gaps, the pseudoregret can be rewritten:
\begin{align}
\label{eq:regret}
\mathrm{PseudoRegret}(\calA; T, \calP_1, \cdots, \calP_K) &= \bbE\left[\sum_{t=1}^T \mu_{I_t}\right] - T\cdot\mu_1 = \sum_{t=1}^T \bbE\left[\Delta_{I_t}\right].
\end{align}
The optimal rate for the pseudoregret at this non-private setting is $\frac{\log(K)}{\Delta_{\min}}$, given by \citet{kotlowski2018minimaxity,mourtada2019optimality}.
%The equality holds because the distribution that $\ell_{j}^{(t)}$ is sampled from is invariant along the time $t$.

In this paper, we focus on this problem with differential privacy (DP; \citet{dwork2014algorithmic}), a standard definition of privacy that requires the outcome distribution from the given randomized algorithm would not be changed too much if only one individual in the dataset has been changed.
Particularly, differential privacy in online learning~\citep{dwork2010differential} is \textit{event-level},  which assumes the individual is the loss vector at a single time step $t$ and the formal definition is as follow; also in this paper we only consider the \emph{pure} DP rather than \textit{approximate} DP, as set in the open prolem~\citep{hu2024open}.

\begin{definition}[Differential privacy in online learning]
	A randomized online learning algorithm $\calM$ is $\varepsilon$-differentially private if for any two loss vector sequences $\ell^{(1:t)}=(\ell^{(\tau)})_{\tau \in[t]}$ and $(\ell')^{(1:t)}$ differing in at most one vector and any decision set $\calD_{1:t}\subseteq [K]^t$, we have $\bbP[\calM(\ell^{(1:t)})\in\calD_{1:t}]\leq e^{\varepsilon}\cdot\bbP[\calM((\ell')^{(1:t)})\in\calD_{1:t}]$ for all $t\leq T$.
\end{definition}

We now state the open problem posed by \citet{hu2024open}: for the stochastic variant of decision-theoretic online learning,
$$\textbf{what is the optimal instance-dependent rate for the pseudoregret under $\varepsilon$-differential privacy?}$$
Or equivalently, what is the optimal rate in terms of $\varepsilon, \Delta_{\min}, K, T$ for the pseudoregret (Equation~\ref{eq:regret}) that can be achieved by any algorithm?
Besides the instance-dependent rate, we would also provide a new instance-independent rate, as a simple corollary.

In addition to the original open problem, we also study a simpler setting in this paper, which we call \textit{deterministic setting} and at which we study the same problem but additionally assume all loss vectors $\ell^{(t)}$ would be deterministic, i.e. $\forall j\in[K], \bbP_{\ell_j\sim \calP_j}[\ell_j=\mu_j] = 1$.
It is noticed that this \textit{deterministic setting} is a strictly weaker setting than the original setting in the open problem, in the sense that it is a subset of problem instances.
We are interested in this setting because the extra factor in the upper bound at the original setting by comparing with the existing lower bound, either in the previous result or our new result, appears together with DP factor $\varepsilon$ rather than the gap $\Delta_{\min}$.
At this deterministic setting, we can study this open problem by focusing on differential privacy regardless of the sampling error in the observed losses.


\subsection{Existing results}
\label{sec:exist}
The best lower bound for this open problem so far, proved by \citet{hu2021near}, is 
$$\Omega\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}\right).$$
The lower bound means that the pseudoregret of any $\varepsilon$-DP algorithm cannot have a better rate than this lower bound for all problem instances ($T, \calP_1, \cdots, \calP_k$).
\citet{hu2021near} also introduces the algorithm FNM-FTNL, which achieves the best rate so far for upper bounding the pseudoregret, 
$$O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K \log T}{\varepsilon}\right).$$
We present their algorithm in Algorithm~\ref{alg:main}, by specifying $B=0$ and the noise distribution $\calQ_{\varepsilon}$ as the laplace distribution $\mathrm{Lap}(\frac{2}{\varepsilon})$; $\mathrm{Lap}(\beta)$ has the probability density function $f(x)=\frac{\beta}{2}e^{-\frac{|x|}{\beta}}$ for $x\in \mathbb{R}$.
The algorithm applies a doubling metric to divide the time dimensions into epochs. 
At each epoch $r$, it accumulates the received loss vectors first and uses the report-noisy-max DP mechanism~\citep{dwork2014algorithmic} (with the laplace noise) to pick an action $J_r$ for the next epoch $r+1$ while preserving the $\varepsilon$-DP guarantee.
We formally state their results in the following theorem.
\begin{theorem}[Best existing result; \citep{hu2021near}.]
\label{thm:eixst} 
	When specifying $B=0$ and $\calQ_{\varepsilon}$ as the laplace distribution $\mathrm{Lap}(\frac{2}{\varepsilon})$, Algorithm~\ref{alg:main} is $\varepsilon$-differentially private and satisfies the gaurantee
	\begin{equation}
	\label{eq:pseudo_regret_exist}
	\mathrm{PseudoRegret}(\text{RNM-FTNL}(B, \calQ_{\varepsilon}); T, \calP_1, \cdots, \calP_K) = O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K\log T}{\varepsilon}\right).
	\end{equation}
\end{theorem}

\subsection{Technical overview}
We briefly go through the techniques that have been used in our two main results, new rate for the open problem and the optimal rate for its deterministic setting as summarized in Table~\ref{tab:result}.

To attain the new rate for the open problem, we first add an essential step to the existing algorithm: \textit{Bernoulli resampling} -- resample the loss values to Bernoulli variables before accumulating the resampled loss vectors; this is indicated by $B=1$ in Algorithm~\ref{alg:main}.
With this step, we can have nice monotonicity property for $J_r$ that is the output of report-noisy-max mechanism (which we state formally in Lemma~\ref{lem:bern} in the later section): $\bbP[J_r=j_1]\leq \bbP[J_r=j_2]$ when $j_1 < j_2$, given that we have assumed that $\mu_{j_1}\leq \mu_{j_2}$, and as a result $\bbP[J_r=j]\leq \frac{1}{j}$ for $j\in [K]$. 
This property comes from that \textit{Bernoulli resampling} makes the fact $G_{r,j}$ from a binomial distribution and that binomial distribution has this property: suppose $A_1\sim \calB(n, p_1)$ and $A_2\sim \calB(n, p_2)$; if $p_1 < p_2$, then $F_{A_1}(x)> F_{A_2}(x) $ where $F_A$ is the cumulative density function for any random variable $A$.
As a result of $\bbP[J_r=j]\leq \frac{1}{j}$, we can derive a more fine-fgrained analysis for the small $t$ part in the regret, which leads to our new rate.

Our results for the deterministic setting are achieved by the gumbel noise and the exponential noise rather than the laplace noise in the original algorithm.
This is because the it has been shown report-noisy-max mechanism with gumbel noise is equivalent to exponential mechanism~\citep{gumbel1954statistical, qiao2021oneshot}, so we can have a tractable expression for $\bbP[J_r=j]$:
$$\bbP\left[J_r = j|\forall i\in [K], G_{r, i} \right] = \frac{\exp\left( \varepsilon\cdot (-G_{r, j} ) \right)}{\sum_{i=1}^K\exp\left( \varepsilon\cdot (-G_{r, i}) \right)}.$$
Moreover, at the deterministic setting, $\bbP\left[J_r = j|\forall i\in [K], G_{r, i} \right] = \bbP[J_r=j]$ and the pseudoregret has this tractable expression too:
$$
O(1) + \sum_{r=3}^{R} \sum_{j=1}^{K}2^{r-1}\Delta_{j}\frac{\exp\left( -2^{r-2}\Delta_j\varepsilon \right)}{\sum_{i=1}^K\exp\left( -2^{r-2}\Delta_i\varepsilon\right)}.
$$
From this expression, we are able to derive some tighter analysis through calculus and show the optimal rate at the deterministic setting.

%\begin{theorem}[Informal, see Theorem~\ref{thm:main}]
%\label{thm:main_informal}
%	When specifying $B=1$ and $\calQ_{\varepsilon}$ as the laplace distribution $\mathrm{Lap}(\frac{2}{\varepsilon})$, Algorithm~\ref{alg:main} is $\varepsilon$-differentially private and has the rate $O\left(\frac{\log(K)}{\Delta_{\min}} + \frac{\log^2(K)}{\varepsilon}\right).$
%\end{theorem}
%
%\begin{theorem}[Informal, see Theorem~\ref{thm:upper_det}]
%\label{thm:upper_det_informal}
%	When specifying $B=0$ and $\calQ_{\varepsilon}$ as the gumbel distribution $\mathrm{Gumbel}(\frac{2}{\varepsilon})$ or the exponential distribution $\mathrm{Exp}(\frac{1}{\varepsilon})$, Algorithm~\ref{alg:main} is $\varepsilon$-differentially private and has the rate $O\left(\frac{\log K}{\varepsilon}\right)$ at the deterministic setting where the lower bound also has the same rate and hence the rate is optimal.
%\end{theorem}

%The analysis Theorem~\ref{thm:main_informal} follows the similar structure