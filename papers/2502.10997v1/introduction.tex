\section{Introduction}
Differential privacy (DP; \citet{dwork2014algorithmic}) is a formal guarantee of data privacy, which requires the outputs from two datasets that are different at one individual data do not diverge too much.
In the context of sequential decision-making, the dataset for the learner is a sequence of observed losses or rewards and DP is extended by comparing the outputs from two sequences that are different at one time step.
DP for two important sequential decision-making problems, online learning~\citep{cesa2006prediction,arora2012multiplicative} and multi-arm bandit~\citep{lai1985asymptotically}, has been studied at different settings for a long while~\citep{jain2012differentially, guha2013nearly,jain2014near,agarwal2017price,tossou2017achieving,sajed2019optimal,hu2021near,asi2023private}.

In this paper, we focus on stochastic decision-theoretic online learning~\citep{freund1997decision} under \emph{pure} differential privacy, which is posed as an \emph{open problem} in \citet{hu2024open}.
In this problem, there are $K$ actions and each has an unknown distribution of loss; the learner at each time would choose an action and receive the stochastic loss from that action.
Moreover, the objective is to minimize the expectation of the accumulated losses over time, and we are at the full-information setting, that is the learner will receive stochastic losses from every action, not only the taken action.

\begin{table}[!t]
\label{tab:result}
\centering
\caption{A summarization of the previous existing results and our new results for the problem \textit{stochastic decision-theoretic online learning under differential privacy}.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c}
	\toprule
	Settings 	& Lower bound & Upper bound\\
	\midrule
	\multirow{2}{*}{\makecell{Instance-dependent bound\\ for the \emph{original setting}}} & \multirow{2}{*}{$\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}$~\citep{hu2021near}} & $\frac{\log K}{\Delta_{\min}} + \frac{\log K \log T}{\varepsilon}$~\citep{hu2021near}\\
	\cmidrule{3-3}
	& & $\frac{\log K}{\Delta_{\min}} + \frac{\log^2 K}{\varepsilon}$ (\textbf{This work})\\
	\midrule
	\multirow{3}{*}{\makecell{Instance-\emph{in}dependent bound\\ for the \emph{original setting}}} & \multirow{3}{*}{$\sqrt{T\log K} + \frac{\log K}{\varepsilon}$~\citep{hu2021near}} & $\sqrt{T\log K} + \frac{K\log K\log^2 T}{\varepsilon}$~\citep{jain2014near}\\
	\cmidrule{3-3} 
	& & $\sqrt{T\log K} + \frac{\log K \log T}{\varepsilon}$~\citep{asi2023private, hu2021near}\\
	\cmidrule{3-3} 
	& & $\sqrt{T\log K} + \frac{\log^2 K}{\varepsilon}$ (\textbf{This work})\\
	\midrule
	\multirow{2}{*}{\makecell{The \textit{deterministic setting}}} & \multirow{2}{*}{$\frac{\log K}{\varepsilon}$ (\textbf{This work})} & $\frac{\log^2 K }{\varepsilon}$ (extended from our result in the original setting)\\
	\cmidrule{3-3} 
	& & $\frac{\log K}{\varepsilon}$ (\textbf{This work})\\
	\bottomrule
\end{tabular}
}
\end{table}

\citet{jain2014near} provides an instance-independent bound $O\left( \sqrt{T\log K}  + \frac{K\log K\log^2 T}{\varepsilon}\right)$ for the general online linear optimization, which can be adapted as an upper bound for this problem.
The best instance-independent bound so far for this problem is $O\left(\sqrt{T\log K} + \frac{\log K \log T}{\varepsilon}\right)$ achieved by \citet{asi2023private} and \citet{hu2021near}, where the lower bound is $O\left(\sqrt{T\log K}+\frac{\log K}{\varepsilon}\right)$.
Particularly, the open problem~\citep{hu2024open} asked for the instance-dependent bound in terms of $K, T, \varepsilon, \Delta_{\rm min}$, where $\Delta_{\min}$ is the gap of expected losses between the optimal and the second actions. 
The best existing instance-dependent bound is $O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K \log T}{\varepsilon}\right)$~\citep{hu2021near} and the proved lower bound is $\Omega\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}\right)$.
The algorithm in \citet{hu2021near} for these two bounds is quite standard: the algorithm applies a doubling metric to divide the time dimensions into epochs. 
At each epoch, it accumulates the observed loss vectors first, and uses a standard DP mechanism, report-noisy-max~\citep{dwork2014algorithmic} with Laplace noise, to pick an action for the whole next epoch.
The algorithm is presented in Algorithm~\ref{alg:main}.

We propose a variant of the algorithm based on \citet{hu2021near}, which just resamples the stochastic loss vectors to Bernoulli variables before accumulating them.
This step of the algorithm helps achieve a new instance-dependent upper bound $O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log^2 K}{\varepsilon}\right)$.
The new bound improves over existing results when $T>K$ (a small burn-in period). Notably by eliminating the extra $\log T$ factor, we showed that the instant-dependent regret remains constant (in $T$) under differential privacy as the lower bound predicts.
%improves over existing results when $T>K$
%This new rate can be an improvement from the existing results because $T$-dependence has been removed from the term of $\varepsilon$.
As a corollary, it also provides a new instance-independent upper bound $O\left(\sqrt{T\log K} + \frac{\log^2 K}{\varepsilon}\right)$.
Moreover, we show that the noise distribution in report-noisy-max can be either Laplace distribution, exponential distribution, and gumbel distribution, which all lead to the same upper bound.

By comparing the upper and lower bound, the extra factor appears together with $\varepsilon$.
This motivates us to study a simplified setting of the open problem, which we call \textit{deterministic setting}, to focus on DP regardless of the sampling error in the observed losses.
Specifically, in this setting we assume the received loss vector is deterministic.
We propose another variant of the algorithm based on \citet{hu2021near}, where we replace the Laplace noise in their report-noisy-max by exponential or gumbel distribution.
We prove the lower bound for this deterministic setting and derive the upper bound from this new algorithm variant which matches the lower bound at the rate of $\frac{\log K}{\varepsilon}$.

We summarize the previous existing results and our new results in Table~\ref{tab:result}.
In addition, we present the specifications of Algorithm~\ref{alg:main}) for variants that attain existing and our results in Table~\ref{tab:spec}.
\textbf{The organization of this paper}: 
In the remaining of this section, we will introduce the problem setting, the existing results and an overview of our technical contribution;
In Section~\ref{sec:main} we will introduce our main results in detail;
In Section~\ref{sec:discuss} we will discuss how our results suggest further addressing the open problem;
%In the appendix, we will discuss the related work at the scope broader than the open problem and present some additional proofs.
In the appendix, we will discuss the related work at the scope broader than the open problem and present some additional proofs.
%Section~\ref{sec:conclusion} is the conclusion section.

\begin{table}[!t]
\centering
\caption{Detailed specifications in Algorithm~\ref{alg:main} that achieve the exising result and our new results.}
\label{tab:spec}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c}
	\toprule
		& Bernoulli resampling or not ($B$) & Noise distribution in report-noisy-max ($\calQ_\varepsilon$) \\
	\midrule
	Theorem~\ref{thm:eixst} \citep{hu2021near} & no & Laplace distribution\\
	\midrule
	\makecell{Theorem~\ref{thm:main} and Corollary~\ref{cor:more_dist}\\ (\textbf{This work})} & yes & Laplace distribution, Exponential distribution, Gumbel distribution\\
	\midrule
	\makecell{Theorem~\ref{thm:upper_det} (\textbf{This work})\\ (for \textit{deterministic setting})} & no & Exponential distribution, Gumbel distribution\\
	\bottomrule
\end{tabular}
}
\end{table}

\begin{algorithm}[!t]
\caption{Variants of RNM-FTNL($B$, $\calQ_{\varepsilon}$)}
\label{alg:main}
\begin{algorithmic}[1]
\STATE \textbf{Specifying the variant:} a bit $B\in\{0, 1\}$ for indicating whether the loss vector is resampled or not; a noise distribution $\calQ_{\varepsilon}$ parametrized by $\varepsilon$.\ \ \ \ \texttt{\small $\backslash\backslash$ The original RNM-FTNL~\citep{hu2021near} can be recovered by setting $B=0$ and $\calQ_{\varepsilon}$ as the laplace distribution $\mathrm{Lap}(\frac{2}{\varepsilon})$.} %(density function: $f(x)=\frac{\varepsilon}{4}\exp\left(-\frac{\varepsilon |x|}{2}\right)$)}
\STATE \textbf{Input:} Action set $[K]$ and privacy parameter $\varepsilon$
\STATE Draw $J_{0}$ from a uniform distribution over $[K]$.
\FOR{$r=1, \cdots, \lceil\log_2(T-1)\rceil+1$}
\STATE Set $G_{r} = (0, \cdots, 0)\in \bbR^{K}$
\FOR{$t=2^{r-1}, \cdots, 2^{r}-1$}
\STATE Play the action $I_t \leftarrow J_{r-1}$.
\STATE Receive the loss vector $\ell^{(t)}=(\ell^{(t)}_1, \cdots, \ell^{(t)}_K)\sim \calP_1\times \cdots \times \calP_K$.
\IF{$B=0$}
\STATE $\tilde{\ell}^{(t)} \leftarrow \ell^{(t)}$
\ELSE
\STATE $\tilde{\ell}^{(t)} \leftarrow (\tilde{\ell}_{1}^{(t)}, \cdots, \tilde{\ell}_{K}^{(t)})\sim \calB(\ell_{1}^{(t)}) \times\cdots \times \calB(\ell_{K}^{(t)})$, where $\calB(p)$ is the Bernoulli distribution with mean $p$.\ \ \ \ \texttt{\small $\backslash\backslash$ Bernoulli resampling}
\ENDIF
\STATE $G_{r}\leftarrow G_{r} + \tilde{\ell}^{(t)}$
\ENDFOR
%\STATE $\tilde{G}_{r}\leftarrow G_{r}  + Q $ where $Q\sim \calQ_{\varepsilon}^{K}$
\STATE $J_{r} \leftarrow \arg\max_{j\in K} -G_{r, j}  + Q_{r, j}$ where $Q_{r, j}\sim \calQ_{\varepsilon}$
\ENDFOR
\end{algorithmic}
\end{algorithm}
