

\section{Unexpected Questions}
\label{app:question}
Real-world questions do not always have the correct premises. For example, in the question "\begin{CJK}{UTF8}{gbsn}水俣病的传染途径是什么？\end{CJK}(What is the route of infection for Minamata disease?)", Minamata disease is not an infectious disease. Taking this situation into account, we add a small number of human-written questions with incorrect premises and LLM-generated questions with hard-to-verify premises in the question collection phase. The number of these questions in the total number of questions is about 3\%.

\section{Prompt for LLM Augmentation}
\label{app:aug}
\input{appendix/prompt}
\input{appendix/prompt_en}
See Table~\ref{tab:prompt} for the prompt for LLM augmentation. Table~\ref{tab:prompt_en} provides an English version.

\section{Instructions for Annotators}
\label{app:ann}
\subsection{First Stage}
In the first stage, we provide the annotators with the question, answer, statement, and cited documents. What LLM considers to be key segments are highlighted in red in the cited documents (see Figure~\ref{fig:stage} for an example). We instruct the annotators to follow the process below:

\par (1) First look at the highlighted text. If the highlighted text fully supports the statement, then the annotation is positive; if the highlighted text contradicts the statement, then the annotation is negative.

\par (2) If the annotation cannot be derived from the highlighted text, then look at the rest of the documents to make the annotation. When the documents fully support the statement, the label is positive, and when there is any information in the statement that contradicts the documents or information that is not mentioned in the documents, the label is negative.

\subsection{Second Stage}

In the second stage, we provide the annotator with the statement and the modified documents. In the documents, the modified parts are highlighted in green, where the dashed and crossed-out text is deleted and the rest is added (see Figure~\ref{fig:stage} for examples). 

For the annotation of whether the quality of the modification is acceptable, the annotators are instructed to note that qualified modifications need to satisfy the following two requirements: (1) There are no contradictions within each modified document. (2) The modified key segments are fluent in their own right and in the context of the document. The annotation for support is the same as the first stage, but based on the modified documents.

 
\section{Input and Training Details}
\label{app:detail}
We input the statement and the cited documents into the model and ask the model to determine whether the statement is fully supported by the documents, outputting yes or no. For input, we label and concatenate the cited documents in order (as shown in Table~\ref{tab:dataset}). For training, we use the following settings: For training, we use the following settings: learning rate is 5e-4, number of epochs is 10, scheduler is cosine scheduler, warmup ratio is 0.03, batch size is 256, and LoRA setting is $r=8$, $a=32$ and 0.1 dropout. We report the model performance for the epoch that achieves the best performance on the dev set.
\label{app:detail}



\section{Related Works}
Language models are known to produce hallucinations - statements that are inaccurate or unfounded~\citep{MaynezNBM20,HuCLGWYG24}. To address this limitation, recent research has focused on augmenting LLMs with external tools such as retrievers~\citep{GuuLTPC20,BorgeaudMHCRM0L22,LiuCtrla2024} and search engines~\citep{WebGPT2021, Komeili0W22, TanGSXLFLWSLS24}. While this approach suggests that generated content is supported by external references, the reliability of such attribution requires careful examination. Recent studies have investigated the validity of these attributions. \citet{DBLP:conf/emnlp/LiuZL23} conducted human evaluations to assess the verifiability of responses from generative search engines. \citet{hu2024evaluate} further investigate the reliability of such attributions when giving adversarial questions to RAG systems. Their findings revealed frequent occurrences of unsupported statements and inaccurate citations, highlighting the need for rigorous attribution verification~\citep{RashkinNLA00PTT23}. However, human evaluation processes are resource-intensive and time-consuming. To overcome these limitations, existing efforts~\citep{GaoDPCCFZLLJG23,DBLP:conf/emnlp/GaoYYC23} proposed an automated approach using Natural Language Inference models to evaluate attribution accuracy. While several English-language benchmarks have been developed for this purpose~\citep{DBLP:conf/emnlp/YueWCZS023}, comparable resources in Chinese are notably lacking. Creating such datasets presents unique challenges, particularly in generating realistic negative samples (unsupported citations).  To address this gap, we introduce the first large-scale Chinese dataset for citation faithfulness detection, developed through a cost-effective two-stage manual annotation process.

\input{appendix/screen}
