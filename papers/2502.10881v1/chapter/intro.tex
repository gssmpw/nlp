


















\section{Introduction}
Large Language Models (LLMs) are prone to generating factual errors through hallucinations when answering real-world questions. Retrieval-augmented generation (RAG) systems \cite{DBLP:conf/nips/LewisPPPKGKLYR020,GuuLTPC20,BorgeaudMHCRM0L22} address this limitation by leveraging external information retrieval to ground LLM responses in verifiable sources. Recent advancements extend RAG systems to generate text with inline citations \cite{DBLP:conf/emnlp/GaoYYC23}, enabling users to validate the reliability of generated content by cross-referencing cited documents. However, studies reveal a critical weakness in these systems: citation faithfulness. A substantial portion of generated text may lack proper support from the cited references \cite{DBLP:conf/emnlp/LiuZL23, hu2024evaluate}, undermining the trustworthiness and verification capability of RAG outputs. This challenge necessitates accurate citation faithfulness detection—determining whether cited passages genuinely support their associated claims—as a fundamental requirement for improving RAG reliability.

Developing robust citation faithfulness detection methods requires large-scale, high-quality datasets. While English benchmarks have emerged \cite{DBLP:conf/emnlp/YueWCZS023}, Chinese datasets remain notably absent. Constructing such resources presents unique challenges: realistic negative samples (unsupported citations) must originate from strong RAG systems to ensure practical usage, yet these systems rarely produce such errors. For instance, a RAG system with a 10\% error rate would require annotating approximately 70,000 samples to collect 7,000 negative examples—a prohibitively expensive endeavor. This tension between dataset quality and construction cost demands innovative solutions for efficient data curation without compromising sample integrity.

To bridge this gap, we introduce \textsc{CiteCheck}, the first large-scale Chinese dataset for citation faithfulness detection. Our approach combines 11,307 knowledge-intensive questions with a novel two-stage annotation framework that reduces labeling costs while preserving data quality. \textsc{CiteCheck} comprises two distinct components designed to address both detection difficulty and training efficacy. 

The development and test sets each contain 500 positive (supported) and 500 negative (unsupported) samples totaling 2,000 unmodified RAG outputs. Experimental analysis demonstrates these original samples pose significant challenges, with state-of-the-art LLMs achieving limited detection accuracy. The training set includes 9,796 samples (4,898 positive/negative pairs) where negative instances are generated through LLM-based document modification rather than relying solely on rare RAG errors. Despite this augmentation, parameter-efficient fine-tuning on 7B-8B parameter models yields strong detection performance, confirming the preserved quality of modified negative samples.

Our contributions are threefold:  \textsc{CiteCheck} establishes the first comprehensive benchmark for Chinese citation faithfulness detection; (2) We propose an efficient data augmentation strategy that reduces annotation costs by 86\% compared to conventional approaches; (3) Extensive experiments validate the dataset’s quality and utility, showing that models trained on our augmented data effectively generalize to challenging real-world samples. This work advances reliable RAG development by providing essential resources and methodologies for building verifiable, citation-grounded LLM applications in Chinese.
