\section{Experiments}
In our experiments, we evaluate the dataset using two approaches. First, we assess the zero-shot performance of state-of-the-art LLMs on the development and test sets. This aims to highlight the challenge posed by the test samples. Second, due to resource constraints, we conduct parameter-efficient fine-tuning on smaller models using the training data. This focuses on demonstrating the effectiveness of the training samples.

\subsection{Settings}


State-of-the-art LLMs that we use for zero-shot performance tests include GPT-4o \cite{openai2024gpt4technicalreport}, Qwen2.5-Plus \cite{qwen2024qwen25technicalreport}, and DeepSeek-v3 \cite{deepseekai2024deepseekv3technicalreport}. We provide the sample to the LLMs and ask for their judgment. The relatively small language models we use for training include Llama-3.1-8B \cite{grattafiori2024llama3herdmodels}, Mistral-7B \cite{jiang2023mistral7b}, and Qwen2.5-7B \cite{qwen2024qwen25technicalreport}. The parameter-efficient fine-tuning method we use is LoRA \cite{DBLP:conf/iclr/HuSWALWWC22}. See Appendix~\ref{app:detail} for training details. We use accuracy as the metric. Since there are equal numbers of positive and negative samples, the accuracy is equivalent to the commonly used balanced accuracy \cite{DBLP:journals/corr/abs-2303-15621}, which is the average of the accuracy on positive and negative samples. We also report the accuracy of positive and negative samples separately.




\input{table/res}
\subsection{Results}

Table~\ref{tab:res} reveals significant differences in performance between LLMs tested under zero-shot conditions and smaller models fine-tuned with parameter-efficient methods. Among the zero-shot LLMs, GPT-4o achieved the highest overall accuracy, outperforming Qwen2.5-Plus and DeepSeek-v3. However, even GPT-4o struggled with negative samples, achieving only 70.4\% accuracy on the dev set and 71.6\% on the test set. This limitation highlights a persistent challenge in distinguishing negative cases, which significantly impacts overall accuracy. DeepSeek-v3, while demonstrating near-perfect accuracy on positive samples, performed poorly on negative samples (39.6\% dev, 39.4\% test), indicating a clear trade-off between the two categories.

In contrast, smaller models fine-tuned with the training set achieved remarkable improvements, particularly in handling negative samples. Llama-3.1-8B stood out as the top performer, achieving 91.4\% accuracy on the dev set and 90.6\% on the test set, while maintaining a strong balance between positive and negative samples. These results suggest that the training data effectively addressed the challenges posed by negative samples, enabling the fine-tuned models to achieve significantly higher overall accuracy. Overall, the results underscore the effectiveness of fine-tuning in improving model robustness, particularly for negative samples. The dataset’s training data appears to play a crucial role in enhancing model performance, as evidenced by the fine-tuned models’ ability to achieve high accuracy across both positive and negative samples. These insights suggest that tailored training strategies and targeted fine-tuning can significantly enhance model capabilities, even for smaller models.
