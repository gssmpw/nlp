\section{Conclusion}
\label{sec:con}

In this paper, we propose CS-Aligner, a novel distributional alignment framework that integrates Cauchyâ€“Schwarz (CS) divergence with mutual information for multimodal alignment. By combining global distributional alignment with InfoNCE, CS-Aligner achieves tighter and more comprehensive alignment. 
By considering the modality distributional information, our method enables to leverage additional and detailed information from unpaired samples and tokens, leading to more flexible and fine-grained information for alignment. 
We demonstrate the effectiveness of our alignment on text-to-image generation and cross-modal retrieval. 
% We believe that bridging distributional alignment is essential for robust and scalable multimodal modeling. 
% In future work, exploring broader modalities (e.g., audio and video) and evaluating on diverse multimodal tasks (e.g., image-to-text generation) could further strengthen alignment and broaden the applicability of CS-Aligner.

% \noindent{\textbf{Limitations.}} Although our method achieves good alignment and is able to leverage more unpaired data for divergence estimation, the estimation of mutual information (InfoNCE) still requires paired data, which is a major limitation of our methods. 
%\zp{please discuss your one to two limitations. }

\noindent{\textbf{Limitation \& future work.}} Although our method has numerous applications, it is currently evaluated only on unCLIP-type models for generation. In the future, we will explore its integration with stable-diffusion-based models. 
Broader modalities (e.g., audio and video) and diverse tasks (e.g., image-to-text generation) could further strengthen alignment and broaden the applicability of CS-Aligner.
%\zp{please discuss your one to two limitations. }
