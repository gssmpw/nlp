% \clearpage
% \setcounter{page}{1}
%\maketitlesupplementary



\section{Details of the toy examples}
\label{sec:toy_details}


\paragraph{Mutual information.} 
For two continuous random variables \(\mathbf{x}\) and \(\mathbf{y}\), the mutual information is defined as:
\begin{equation}
\label{eq:mi_def}
I(\mathbf{x}; \mathbf{y}) 
\,=\, 
\int \!\! \int 
p(\mathbf{x}, \mathbf{y}) 
\, \log \!\Bigl(\tfrac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x})\,p(\mathbf{y})}\Bigr)
\, d\mathbf{x}\,d\mathbf{y}.
\end{equation}
For a bivariate Gaussian distribution 
\[
p(\mathbf{x}, \mathbf{y}) 
\,\sim\, 
\mathcal{N}\!\Bigl(
\begin{pmatrix}
\mu_\mathbf{x} \\
\mu_\mathbf{y}
\end{pmatrix}, 
\begin{pmatrix}
\sigma_\mathbf{x}^2 & \rho \sigma_\mathbf{x}\sigma_\mathbf{y} \\
\rho \sigma_\mathbf{x}\sigma_\mathbf{y}       & \sigma_\mathbf{y}^2
\end{pmatrix}\Bigr),
\]
the mutual information admits the closed-form solution:
\begin{equation}
\label{eq:mi_gauss}
I(\mathbf{x}; \mathbf{y}) 
\,=\, 
-\tfrac12\ln\bigl(1-\rho^2\bigr).
\end{equation}
In particular, for correlation \(\rho = 0.99\), we have 
\(I(\mathbf{x},\mathbf{y}) \approx 1.959\), 
while for \(\rho=0\), 
the variables are independent and \(I(\mathbf{x},\mathbf{y}) = 0\).

\paragraph{Divergence.} 
For univariate Gaussian distributions 
\(p(\mathbf{x}) = \mathcal{N}\!\bigl(\mu_\mathbf{x}, \sigma_\mathbf{x}^2\bigr)\) 
and 
\(p(\mathbf{y}) = \mathcal{N}\!\bigl(\mu_\mathbf{y}, \sigma_\mathbf{y}^2\bigr)\),
the KL divergence is given by:
\begin{equation}
\label{eq:kl_univ_gauss}
D_{\mathrm{KL}}\bigl(p(\mathbf{x})\,\|\,p(\mathbf{y})\bigr)
\,=\,
\ln\!\Bigl(\tfrac{\sigma_\mathbf{y}}{\sigma_\mathbf{x}}\Bigr)
\;+\;
\frac{\sigma_\mathbf{x}^{2} + (\mu_\mathbf{x} - \mu_\mathbf{y})^{2}}{2\,\sigma_\mathbf{y}^{2}}
\;-\;
\tfrac12.
\end{equation}
For Fig.~\ref{fig:toy1} and Fig.~\ref{fig:toy3}, we set \(\sigma_\mathbf{x} = \sigma_\mathbf{y} = 1\). 
Hence, when \(\mu_\mathbf{x} = \mu_\mathbf{y} = 0\), 
\(D_{\mathrm{KL}}\bigl(p(\mathbf{x})\,\|\,p(\mathbf{y})\bigr) = 0\).

For Fig.~\ref{fig:toy2}, we use $\sigma_x = 10$ and $\sigma_y=0.1$. When \(\mu_\mathbf{x} = 0\) and \(\mu_\mathbf{y} = 2\), the \(D_{\mathrm{KL}}\bigl(p(\mathbf{x})\,\|\,p(\mathbf{y})\bigr) = 5194.89\), which is very large. 
% \(D_{\mathrm{KL}}\bigl(p(\mathbf{x})\,\|\,p(\mathbf{y})\bigr) = 2\).





\section{Comparison bwtween CS divergence and other metrics}
\label{sec:cs-compare}


% \begin{figure*}[htbp]
%   \centering  \includegraphics[width=0.99\textwidth]{./Figures/kl_cs.pdf} % Adjust the size and filename as needed
%   % \vspace{-2mm}
% \caption{\textbf{CS divergence is a robust estimator with respect to the small distribution overlap.}~\citep{yu2024cauchy} } 
% \end{figure*}


In the following, we analyze why CS divergence is a better choice than well-known KL divergence and Wasserstein distance. 

\noindent{\textbf{Comparison with KL divergence.}} KL divergence, a widely used metric in deep learning, is defined as: 
\begin{equation}
    D_{\text{KL}}(p(\mathbf{x}) \| p(\mathbf{y})) = \int p(\mathbf{x}) \log \frac{p(\mathbf{x})}{p(\mathbf{y})} d\mathbf{x},
\end{equation}
where \(p(\mathbf{x})\) and \(p(\mathbf{y})\) are the probability distributions being compared. However, KL divergence is a non-symmetric metric, making it less suitable for learning a shared representation space (bi-directional), as commonly needed in multi-modal learning. 

\noindent{\textbf{Minimal distribution overlap requirement.}} Furthermore, KL divergence can diverge to infinity when the distributions \(p(\mathbf{x})\) and \(p(\mathbf{y})\) have minimal overlap, making it invalid. In contrast, CS divergence remains well-defined under such conditions and provides a stable estimation~\citep{yu2024cauchy,yindomain}. This issue is particularly critical in multi-modality settings, where the distributions from different modalities may have limited overlap. 

\noindent{\textbf{Nonparametric estimation.}} Additionally, when the distributions are not assumed to be Gaussian, a nonparametric estimator is required for KL divergence. A common choice, the k-NN estimator~\citep{wang2009divergence}, is non-differentiable, which poses challenges for optimization in gradient-based learning frameworks. In contrast, the CS divergence demonstrates greater stability and differentiability when paired with KDE, making it a more robust choice.


\noindent{\textbf{Comparison with wasserstein distance.}}  Wasserstein Distance is also widely used for distribution discrepancy (e.g. GAN~\citep{arjovsky2017wasserstein}). However, Wasserstein distance is be computed either by using an additional learnable module (e.g., a neural network for estimating a transport map~\citep{korotin2022neural}) or by solving an optimization problem, often approximated via multiple Sinkhorn~\citep{cuturi2013sinkhorn} iterations for computational efficiency, leading to efficiency problem in large-scale training. In contrast, CS divergence can be efficiently estimated by a nonparametric estimator. 





\section{Implementation details}
\label{sec:implementation-details}

\paragraph{Implementation details} Our models were trained on 4 NVIDIA RTX A100 GPUs with a global batch size of 1,024 (256 per GPU). We optimized parameters using AdamW with a cosine annealing learning rate schedule, spanning a total of 100 GPU hours. Mixed-precision training (FP16) was employed to enhance computational efficiency while maintaining stability. We use the learning rate of $5e-5$. We use hyperparameter $\lambda$ as $0.01$ to keep the same number scale as the divergence.


\paragraph{Kernel density estimator.} A proper kernel size is critical in KDE for accurate estimation of Eq. (\ref{eq.cs_est}). In this paper, we follow \cite{yindomain} to normalize the features from two modalities and use a kernel size $1$. In general, this is sufficient to ensure stable learning.

 
\subsection{T2I details}

\paragraph{Kandinsky details.} We use Kandinsky v2.2, an unCLIP-type model that utilizes CLIP ViT-bigG-14-laion2B-39B-b160k with 1280 projection dimensions for text and image encoders. Kandinsky v2.2 employs a latent diffusion model and MOVQ~\citep{zheng2022movq} as the decoder to generate images of size $512\times 512$ from the given image representation. When using the Kandinsky decoder, we apply $50$ denoising steps~\citep{ho2020denoising} with a classifier-free guidance scale of 7.5~\citep{ho2022classifier}.

\paragraph{Karlo details.} Karlo uses CLIP-ViT-L/14 with 768 projection dimensions for image and text encoders. It employs a diffusion model to decode the image representation into a low-resolution image, followed by a super-resolution diffusion module that upsamples it to $512\times 512$. When using the Karlo decoder, we apply $25$ denoising steps with a classifier-free guidance scale of 7.5, followed by an additional $7$ super-resolution steps.

\paragraph{Adapter details.} 
To ensure a fair comparison, our adapter module has the same architecture as Eclipse~\citep{patel2024eclipse}, which is based on the standard PriorTransformer model~\citep{ramesh2022hierarchical} but modified to be time-independent. Specifically, it consists of 10 layers with 16 attention heads, each having a head dimension of 32. The embedding dimension is 768/1280, with three additional embeddings. The model does not use time embeddings and has a dropout rate of 0.0.


\paragraph{LoRA} 
We configure LoRA (Low-Rank Adaptation) for CLIP with a rank of $r = 8$ and a scaling factor of $\alpha = 16$, enabling efficient adaptation while maintaining a low computational footprint. The targeted modules include the self-attention projections, the fully connected layers, and the \texttt{text\_projection} layer, ensuring adaptation across both vision and text processing components. A dropout rate of $0.1$ is applied to enhance regularization. 
For the CLIP encoder in Kandinsky, ViT-bigG-14-laion2B-39B-b160k, the number of LoRA parameters is $6$ million. As for CLIP-ViT-L/14 in Karlo, the CLIP model size is smaller, resulting in $1.3$ million LoRA parameters.


\paragraph{LAION-HighResolution-5M selection.} 
We use a subset of $5$ million image-text pairs from the LAION-HighResolution dataset, which contains $175$ million pairs. Due to computational constraints, we download only a portion of the dataset and select pairs with English captions.

%\subsection{Image-text retrieval details}

%CLIP and LLM. 


\section{More experimental results}


\paragraph{Alignment with InfoNCE is not enough for the generation task.} 
We train the adapter solely with InfoNCE and use the Kandinsky decoder to generate the corresponding images. Table~\ref{tab:loss_comparison} shows that InfoNCE alone struggles to align the multimodal distributions, resulting in a high FID score. 

\begin{table}[htbp]
\centering
\caption{\textbf{Ablation study of CS-Aligner.} Alignment with CS-Aligner significantly outperforms the alignment results with only InfoNCE.}
\label{tab:loss_comparison}
\small % Slightly larger font size
\begin{tabular}{lc}
\toprule
\textbf{Loss} & \textbf{FID} \\ 
\midrule
InfoNCE & 151.35 \\ 
CS-Aligner & 12.62 \\ 
\bottomrule
\end{tabular}
\end{table}




\subsection{More visualization} We illustrate more high-resolution images generated by the Kandinsky decoder with our aligned text representation in Fig.~\ref{fig:more-hs-results}. The adapter is trained on LAION-HighRes 5M.

\begin{figure*}[htbp]
  \centering  \includegraphics[width=1\textwidth]{./Figures/more-high-rs-results.pdf} % Adjust the size and filename as needed
  % \vspace{-2mm}
\caption{\textbf{Qualitative visualization.} The adapter is trained on LAION-HighRes 5M. The aligned text representation is then decoded by the Kandinsky decoder. } 
\label{fig:more-hs-results}
\end{figure*}




\subsection{More visualization for token alignment}
\label{sec:token-alignment}

We provide more visualizations with and without the token alignment Fig.~\ref{fig:token-alignment-appendix}, demonstrating its ability to generate more fine-grained images with CS-Aligner.

\begin{figure*}[htbp]
  \centering  \includegraphics[width=1\textwidth]{./Figures/multi-token-vis-appendix.pdf} % Adjust the size and filename as needed
  % \vspace{-2mm}
\caption{\textbf{Token alignment is effective for fine-grained generations with more details and stronger semantic correspondence with the text inputs.} } 
\label{fig:token-alignment-appendix}
\end{figure*}

%\textcolor{red}{number of diffusion sampling steps}


%\textcolor{red}{appendix: hyperparameters}
