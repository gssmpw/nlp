\section{Related work}

\noindent{\textbf{Vision-language alignment and applications.}} CLIP \cite{radford2021learning} serves as a foundational model for vision-language alignment in multimodal tasks. Several works have enhanced CLIP through techniques such as momentum distillation~\citep{li2021align} and noisy text supervision~\citep{jia2021scaling}. Despite its success, CLIP suffers from a persistent modality gap between text and image representations. Prior studies~\citep{zhou2023clip, liang2022mind, shi2023towards} attribute this gap to factors such as cone effects~\cite{liang2022mind} and suboptimal latent space structures~\citep{shi2023towards}. To address this, various strategies have been proposed, including projection adapters~\cite{zhou2023clip, gao2024clip, huang2024llm2clip}, geodesic multimodal mixup~\cite{oh2024geodesic}, and parameter-efficient fine-tuning~\cite{zanella2024low}. 
Recent works also improve CLIP by large language models (LLMs)~\citep{jang2024mate, koukounas2024jina, huang2024llm2clip} for downstream tasks such as \textbf{image-text retrieval}. 
% However, these approaches primarily focus on pairwise alignment, neglecting distribution-level alignment, which is crucial for enhancing performance across a broader range of downstream tasks.

% \noindent{\textbf{Text-to-Image Generation.}} 
In addition to image-text retrieval, \textbf{text-to-image (T2I) generation} is another application that reflects the vision-language alignment capability. 
T2I has advanced significantly over the past decades, driven by both diffusion-based~\citep{ramesh2021zero, rombach2022high, saharia2022photorealistic, nichol2021glide} and GAN-based models~\citep{zhang2017stackgan, tao2023galip}. 
Among diffusion-based methods, the unCLIP framework~\citep{ramesh2021zero, ramesh2022hierarchical} employs a two-stage architecture with a CLIP-guided diffusion prior and a decoder (e.g., DALL-E-2~\citep{ramesh2022hierarchical} or Karlo~\citep{kakaobrain2022karlo-v1-alpha}). Its prior module \( g_\phi\)
maps text representations $\mathbf{y}$ to image ones $\mathbf{x}$ by a diffusion model. 
%with noised embeddings \( \mathbf{x}^{(t)} \sim q(t, \mathbf{x}) \):
% : \mathbf{y} \to \mathbf{x} \) (text representation to image representation) operates as a diffusion model that directly predicts denoised latents \( \mathbf{x} \) from noised embeddings \( \mathbf{x}^{(t)} \sim q(t, \mathbf{x}) \), bypassing conventional noise estimation. 
% The prior objective is:
Recently, 
Eclipse~\citep{patel2024eclipse} employs an $\ell_2$ loss to simplify the prior loss by eliminating diffusion time and intruding a noise $\epsilon$ term:
\begin{equation}
\mathcal{L}_{\text{prior}} = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} 
\left[ \left\| \mathbf{x} - g_{\phi}(\epsilon, \mathbf{y}) \right\|_2^2 \right].
\label{eq:prior_loss}
\end{equation}
%\str{What is $x$ and $y$?}
However, these methods still rely on pairwise loss (e.g., $\ell_2$). In contrast, our approach introduces distributional alignment for a more holistic modality alignment.

%, and \( \mathbf{y} \) encodes text conditioning.

%Among diffusion-based approaches, two representative paradigms have emerged: Latent Diffusion Model-based (LDM)~\citep{rombach2022high} and unCLIP-based methods~\citep{ramesh2021zero, ramesh2022hierarchical}. 
%LDM-based models~\citep{rombach2022high, chefer2023attend, chen2024training} utilize cross-attention layers in the latent space to create unified text-to-image diffusion frameworks. In contrast, 
% Among diffusion-based approaches, unCLIP~\citep{ramesh2021zero, ramesh2022hierarchical} is one of the representative paradigms. 
% unCLIP models, such as DALL-E-2~\citep{ramesh2022hierarchical}, Karlo~\citep{kakaobrain2022karlo-v1-alpha}, and Kandinsky~\citep{razzhigaev2023kandinsky}, incorporate a pre-trained CLIP model and employ a two-step process involving a text-to-image diffusion transformer prior and a diffusion-based image decoder.
% The T2I prior module (denoted as $(g_\phi: y \to x)$) is implemented as a diffusion model. For each timestep $t$ and a noised image embedding $x^{(t)} \sim q(t, x)$ (where $q$ represents the forward diffusion process), the diffusion prior estimates the noiseless $z_x$ directly, rather than approximating the Gaussian noise distribution $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$. The prior loss is formally defined as:
% \begin{equation}
% \mathcal{L}_{\text{prior}} \;=\; 
% \mathbb{E}_{t \sim [0,T],\, x^{(t)} \sim q(t, x)} 
% \Bigl[
% \|\,x \;-\; g_{\phi}\bigl(x^{(t)},\, t,\, y\bigr)\|_{2}^{2}
% \Bigr].
% \label{eq:prior_loss}
% \end{equation}





%Our T2I approach builds on unCLIP models, as their prior module aligns with our objective of modality alignment. However, our method enables the CLIP model to directly predict text features aligned with the image feature distribution, eliminating the need for computationally expensive and complex diffusion steps, thereby enhancing efficiency.



\noindent{\textbf{Cauchy-Schwarz divergence.}} The Cauchy-Schwarz (CS) divergence~\cite{principe2000information,principe2000learning} is derived from the Cauchy-Schwarz inequality for square-integrable functions. It serves as a symmetric distribution distance metric with notable properties, such as the ability to measure conditional distributions~\citep{yu2023conditional} and the closed-form expression for mixtures of Gaussians~\citep{kampa2011closed}.
CS divergence has been successfully applied across various domains, including deep clustering~\citep{trosten2021reconsidering}, disentangled representation learning~\citep{tran2022cauchy}, and deep regression~\citep{yu2024cauchy}. Moreover, due to its advantage of estimating discrepancy between conditional distributions, it has demonstrated success in the domain adaption area~\citep{yindomain} and time series clustering~\cite{yu2023conditional}. 
However, the utility of CS divergence in foundation models remains unclear and unexplored.


%To transform the text distribution into the image distribution (or vice versa), UnCLIP-based models, such as DALL-E 2~\citep{ramesh2022hierarchical}, Kandinsky~\citep{razzhigaev2023kandinsky}, and Karlo~\citep{kakaobrain2022karlo-v1-alpha}, employ a diffusion prior model to map text representations to image representations. 
%In contrast, our approach simplifies this process by employing either an adapter module on top of the text encoder or a LoRA~\citep{hu2021lora} module. 
%This enables the CLIP model to directly predict text features that are aligned with the image feature distribution, bypassing the computationally expensive and complex diffusion steps. % Furthermore, our method can be seamlessly integrated into existing fine-tuning or adaptation frameworks for CLIP models, providing greater flexibility and efficiency.

% \noindent{\textbf{Image-Text Retrieval.}} 
% text-image retrieval~\citep{lulf2024clip,koukounas2024jina}, 
% CLIP with LLM: MATE: long-text image retrieval task~\citep{jang2024mate}, LLM2CLIP (use LLM to finetuning)~\citep{huang2024llm2clip}. 



