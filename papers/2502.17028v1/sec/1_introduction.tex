\section{{Introduction}}
\label{sec:intro}

\input{./Figures/fig1}

% leave out contribution
% Multi-modality representation learning forms the foundation for multimodal generative models, such as text-to-image (T2I) generation. CLIP~\cite{radford2021learning}, a landmark in multimodal learning, leverages a contrastive loss function—akin to cosine similarity—to capture pairwise relationships between text and images effectively.


Modality alignment is a cornerstone of multimodal representation learning, enabling success across diverse applications such as image-text retrieval~\citep{huang2024llm2clip,koukounas2024jina}, text-to-image (T2I) generation~\citep{ramesh2022hierarchical, razzhigaev2023kandinsky}, and multimodal chatbots~\citep{zhu2023minigpt}. As a pioneering work in this field, CLIP~\cite{radford2021learning} leverages contrastive loss to maximize the mutual information between paired text and image representations, effectively capturing pairwise and semantic relationships. Its versatility has made it a foundation for many multimodal tasks. %\zx{\citep{}}, 
%\zx{Remove?: including T2I generation and cross-modal retrieval}.



Although widely adopted, CLIP suffers from a persistent modality gap between text and image representations in its latent space. As shown in Fig.~\ref{fig:tsne-wo-alignment}, text and image embeddings often fail to align precisely and may remain separated from each other. This modality gap has been observed and explored in prior studies~\citep{zhou2023clip, liang2022mind, shi2023towards}, which attribute the issue to factors such as cone effects~\cite{liang2022mind} or suboptimal latent space structures~\citep{shi2023towards}. Intriguingly, \citet{liang2022mind} observed a phenomenon that CLIP’s contrastive learning objective may inadvertently exacerbate this gap, implying mutual information alone is insufficient for aligning text and image representation distributions.
%\str{Why?}

Several strategies have been proposed to address the modality gap, such as
% To address the modality gap, several strategies have emerged. 
projection modules with cosine similarity~\cite{zhou2023clip, gao2024clip, huang2024llm2clip} and geodesic multimodal mixup~\cite{oh2024geodesic}. 
% have been proposed to improve alignment. 
% Meanwhile,  
UnCLIP-based models like DALL-E 2~\citep{ramesh2022hierarchical} and Kandinsky~\citep{razzhigaev2023kandinsky} employ text-to-image prior modules (e.g., diffusion models) to map text embeddings to image feature space. 
A more recent alternative Eclipse \citep{patel2024eclipse} uses $\ell_2$ loss to train a prior adapter. 
These works aim to transform representations across modalities for alignment. 
However, they remain exploring alignment sample-wisely and heavily rely on pairwise data.
Although sample-wise alignment effectively captures semantic information, it falls short in aligning entire data distributions. 
Similar to the InfoNCE in CLIP, the methods struggle to match the representation spaces across modalities, ultimately limiting the overall alignment.
Moreover, the reliance on carefully curated text-image pairs limits scalability and applicability to real-world scenarios with unpaired and noisy datasets~\citep{lin2014microsoft,li2023leveraging}. 
% These works highlight that text and image representations in CLIP reside in distinct distributions, underscoring the need for alignment with distributional constraints rather than merely pairwise matching.  
% Indeed, the dominated pairwise alignment further suffers from  critical limitations. First, it risks model collapse, overfitting to specific samples while failing to capture the broader distributions of multimodal data~\citep{srivastava2017veegan}. 
% Second, it relies heavily on carefully curated text-image pairs, limiting scalability and applicability to real-world scenarios, where unpaired and noisy datasets are common~\citep{lin2014microsoft,li2023leveraging}. 


To address these challenges, we propose CS-Aligner, a novel distributional approach that incoporates Cauchy-Schwarz (CS) divergence~\cite{principe2000learning} for modality alignment.
As a symmetric measure, CS divergence robustly and efficiently estimates the distance between any representation distributions 
% using a kernel density estimator (KDE) 
without parametric distributional assumptions, making it highly suitable for multimodal distribution alignment.
Furthermore, we also analyze the complementary roles of CS divergence and mutual information in multimodal alignment and propose integrating these two metrics within CS-Aligner.
This enables CS-Aligner to align vision and language representations in both distributional and sample-wise levels, considering both the global modality and local semantic information, leading to more comprehensive and tighter alignment as shown in Fig. \ref{fig:tsne-with-alignment}.
% Unlike existing methods conducting pairwise alignment, CS-Aligner combines CS divergence with mutual information to achieve robust and scalable modality alignment 
%\zp{unclear? you did not introduce any detail of your method, and why your method achieve robust and scalable distributional alignment?}.  
% Our method aligns the entire distributions of text and image representations, as illustrated in Fig.~\ref{fig:tsne-with-alignment}, where embeddings are shown to align uniquely and cohesively.  



Moreover, based on the distributional approach,
% our approach offers several other significant advantages. First, 
CS-Aligner enables alignment with additional unpaired data, such as (a) single images with multiple captions or (b) entirely unpaired vision-language data, 
introducing more distributional information from richer, unstructured multimodal data for alignment robustness and flexibility in real-world scenarios.
% broadening its applicability to diverse real-world datasets. 
Beyond the unpaired alignment, we also introduce a novel token-level alignment scheme for vision-language models, which integrates more detailed information in diverse tokens to enhance multimodal alignment.
% an underexplored area in prior research. This innovation eliminates the dependency on curated paired datasets, 
% enhancing flexibility and enabling the utilization of richer, unstructured multimodal data.  
 Extensive experiments on downstream tasks, including T2I generation and image-text retrieval, demonstrate the effectiveness of our approach. 
 % By addressing the modality gap through a unified distributional alignment framework, CS-Aligner represents a scalable and robust advancement for multimodal learning, offering a significant step forward in bridging the gap between text and image representations.  
 

% Modality alignment is critical for multimodal representation learning and its downstream applications. CLIP~\cite{radford2021learning}, a seminal work in multimodal learning, leverages mutual information through a contrastive loss function, effectively capturing pairwise relationships between text and image representations. CLIP has been widely used in different multimodal tasks, including text-to-image (T2I) generation and cross-modal retrieval. 


% Despite the success of CLIP in various downstream tasks, a significant domain gap remains (as shown in Fig.\ref{fig:tsne-wo-alignment}). Previous works\citep{zhou2023clip, liang2022mind, shi2023towards} have observed similar issues and hypothesized that the gap is due to cone effects~\cite{liang2022mind} or local minima affecting the structure of the latent space. Specifically, \cite{liang2022mind} observed that the CLIP contrastive learning even encourages the existence of the modality gap. In this work, we show that solely on mutual information-based alignment is insufficient to ensure the closeness of the two distributions during training. 


% To mitigate the modality gap, \cite{zhou2023clip,gao2024clip,huang2024llm2clip} learns a projection module using cosine similarity, while \cite{oh2024geodesic} performs fine-tuning via geodesic multi-modal mixup. 
% unCLIP models~\citep{ramesh2022hierarchical, razzhigaev2023kandinsky}, such as DALL-E-2, use a text-to-image prior module (diffusion model) to transform CLIP text representations into image representations. Eclipse~\citep{patel2024eclipse} employs an L2 loss to train the prior adapter without diffusion steps, showing efficiency. 
% Such models demonstrate that the transformed text representation can be used for the image decoder to generate corresponding images. This implies that the original CLIP text and image representation are not in the same distribution. Hence, alignment with distribution distance constraint is crucial.  
% However, these methods focus on pairwise alignment rather than distributional alignment. 
% Pairwise alignment can lead to model collapse on specific samples and fail to capture the full image distribution. Moreover, it requires curated text-image pairs, limiting scalability.

% 

% In this paper, we introduce Cauchy-Schwarz (CS) divergence~\cite{principe2000learning, yu2023conditional} to the modality alignment domain to address the aforementioned challenges. Specifically, we propose \textit{CS-Aligner}, a novel approach that combines CS divergence with mutual information to achieve effective modality alignment. Leveraging a kernel density estimator, our method efficiently aligns two modalities, as demonstrated in Fig.~\ref{fig:tsne-with-alignment} for vision and text representations. 
% Our approach further enables unpaired data alignment, unlocking the use of diverse data scenarios, such as (1) a single image with multiple captions and (2) completely unpaired image and text data. Additionally, we introduce a new alignment scheme that facilitates vision-language token-level alignment, which is rarely explored in prior research. By eliminating the need for curated paired data, our method offers greater flexibility and the ability to utilize richer, unpaired real-world datasets. Experimental results on downstream tasks, including text-to-image generation and image retrieval, demonstrate the effectiveness of the proposed method.

%\wy{infoNCE - CS-mutual information? }
%1. introduce clip. 2. refer to multi-modality text generation. 3. refer to the clip modality gap. 4. refer to the uni-clip type model prior 5. the limitation, -> sample-specific. and distribution-wise. 6 introduce cs divergence to address this issue. 