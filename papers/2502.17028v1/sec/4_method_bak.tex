

\input{./Figures/figure2}

\section{Methodology}
\label{sec:method}

Multimodal alignment is important to learn shared semantic spaces for different modalities, enabling robust zero-shot transfer, deep concept understanding, and flexible, scalable application across modalities. Previous multimodal model (e.g., CLIP-based methods\cite{radford2021learning,jia2021scaling,zhou2022learning}) learns text and image representations together in a shared embedding space by maximizing mutual information, which however, shows insufficient multimodal alignment ability~\citep{liang2022mind}. In this section, we first discuss why mutual information alone is not enough for multimodal alignment. Next, we propose a novel learning framework that enables the alignment of multimodal representations within a unified distribution. Finally, we detail the proposed \textcolor{orange}{learning paradigms}.

%In this section, we first discuss why mutual information alone is insufficient for multimodal representation learning, particularly for potential cross-modality generative models. Next, we propose a novel learning framework that enables the alignment of multimodal representations within a unified distribution. Finally, we detail the proposed learning and generation paradigms.


\subsection{Mutual Information is Not Enough}

Previous vision-language models ~\cite{radford2021learning,jia2021scaling,zhou2022learning} learn text and image representations in a shared embedding space by maximizing lower bounds (e.g., InfoNCE) of mutual information between modalities:
% 
% Most previous multimodal representation learning approaches~\cite{radford2021learning,jia2021scaling,zhou2022learning} rely on mutual information as the learning objective:
\begin{equation}
I(\mathbf{x}; \mathbf{y}) = \int \int p(\mathbf{x}, \mathbf{y}) \log \frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x}) p(\mathbf{y})} \, d\mathbf{x} \, d\mathbf{y},
\end{equation}
where $p(\mathbf{x})$ and $p(\mathbf{y})$ denote the image and text feature distribution, respectively.
% are two random variables, and $p(\mathbf{x})$ and $p(\mathbf{y})$ are their distributions, respectively.
% \zx{How are they related to text and image representations?}
% \zx{Although the objective leads to a shared vision-language feature space, the alignment between the learned visual and textual representations are not aligned properly \cite{} (citations).}




Although the property makes it a useful tool for learning a shared representation space across modalities, mutual information is insufficient for modality alignment as observed in the CLIP model~\citep{liang2022mind}. 
% Examples are also shown in Figure \ref{fig:tsne-wo-alignment}. 
The reason can be that mutual information mainly quantifies the statistical and structural dependence between multimodal representations, focusing on their pairwise dependence~\citep{cover1999elements}, while placing limited emphasis on
% We hypothesize that the limitation arises since mutual information does not adequately ensure 
the alignment of the underlying distributions of the two modalities.
To further investigate this issue, we explore it with a toy example.


\begin{example}
\label{example:mi-div}
Consider two Gaussian distributions, \(p(x) \sim \mathcal{N} (\mu_x, \sigma_x^2)\) and \(p(y) \sim \mathcal{N} (\mu_y, \sigma_y^2)\), with a joint distribution \(p(x, y) \sim \mathcal{N}\left(
\begin{pmatrix}
    \mu_x \\
    \mu_y
\end{pmatrix}, 
\begin{pmatrix}
    \sigma_x^2 & \rho \\
    \rho & \sigma_y^2
\end{pmatrix} 
\right)\). Here, \(\mu_x\) and \(\mu_y\) denote the means of \(x\) and \(y\), respectively, while \(\sigma_x^2\) and \(\sigma_y^2\) are their variances. The parameter \(\rho\) represents the correlation coefficient, controlling the linear dependency between \(x\) and \(y\).
We set \(\rho = 0.99\) in Fig.~\ref{fig:toy1} and~\ref{fig:toy2}, where 
% When \(\rho = 0.99\), 
\(x\) and \(y\) are highly dependent, indicating high mutual information (1.959).
% , as shown in Fig.~\ref{fig:toy1} and Fig.~\ref{fig:toy2}. 
In contrast, when \(\rho = 0\), there is no dependency between \(x\) and \(y\), resulting in zero mutual information (Fig.~\ref{fig:toy3}). Notably, as shown in Fig.~\ref{fig:toy2}, the two distributions have the same high mutual information as Fig.~\ref{fig:toy1} while exhibiting a large distributional distance (i.e., $\text{KL}=2$ when \(\mu_x = 0\) and \(\mu_y = 2\)). Details about the calculation are provided in the Appendix~\ref{sec:toy_details}. 
\end{example}

Fig.~\ref{fig:toy1} and~\ref{fig:toy2} in \textit{Example}~\ref{example:mi-div} show that although the representations of two modalities have strong dependence with high mutual information, the distance between their distributions can be large.
The issue also occurs in the CLIP model pretrained by InfoNCE, where the vision and language representations have an obvious distributional distance (Fig. \ref{fig:tsne-wo-alignment}).
The distance between distributions leads to inconsistently aligned multimodal features, hindering the clear representation of shared meanings and effective mapping between modalities, ultimately degrading performance in downstream tasks like cross-modality generation.
In this case, rarely maximizing the mutual information between multimodal representations is not enough for their alignment.

In addition, while directly minimizing the distance between distributions can address the distributional gap, it may result in independent multimodal distributions lacking adequate semantic alignment (Fig. \ref{fig:toy3}).

% \textbf{\textit{Example}}~\ref{example:mi-div} highlights that strong dependence between two modalities, as measured by mutual information, does not guarantee a small distance in their distributions. To address this, it is necessary to minimize the divergence between the distributions to ensure alignment.

% A further example is CLIP, where aligning vision and language representations only with mutual information results in a large distributional distance (as shown in Fig.~\ref{fig:tsne-wo-alignment}). This can be problematic for downstream tasks, like cross-modality generation.
% Aligning both mutual information and distribution divergence seamlessly enables cross-modality generation without an additional task-specific module.

  
%     Let the Gaussian distribution $X \sim \mathcal{N}(0, 1)$ and the Chi-square distribution $Y = X^2 \sim \chi^2(1)$. The mutual information between $X$ and $Y$ is approximately $1.13$ bits, indicating a high level of dependence. However, as shown in Fig.~\ref{fig:mi_div_example}, their distributions are significantly distinct, with a Kullbackâ€“Leibler (KL) divergence of $1.13$. This example demonstrates that while the representations of two modalities can exhibit strong dependence (as measured by mutual information), their distributions may still differ. \zx{Indeed, mutual information and divergence are complementary: mutual information emphasizes pairwise alignment, whereas divergence captures distribution-level alignment.} \wy{new example, change context accordingly. carefully check MI and Div number}
% \end{example}

% As illustrated in \textbf{\textit{Example}}~\ref{example:mi-div}, mutual information is not enough to align distributions since two distributions can have large mutual information while also having large distribution gaps. 

Therefore, to achieve multi-modal alignment at both semantic and distributional levels, enabling more flexible and diverse cross-modality downstream tasks, we propose CS-Aligner, which integrates mutual information with distribution divergence (using CS divergence for robustness and efficient estimation). 
% as shown in Fig.~\ref{fig:illustration}.



%\begin{example}[Cross-modality Generative Models need Distribution Alignment] \wy{To be replaced by a better connection.
%Let us consider image and text generative models, where the objective is to learn the data distribution $p(\mathbf{x})$ from 
%samples \(\{\mathbf{x}_i\}_{i=1}^M\) for image distribution and \(p(\mathbf{y})\) from \(\{\mathbf{y}_i\}_{i=1}^n\) for text distribution. If $p(\mathbf{x}) \simeq p(\mathbf{y})$, we can sample a representation $\hat{y} \sim p(y)$, and leverage the learned generator $p(\mathbf{x})$ to generate the corresponding image. This seamlessly enables the cross-modality generation without an additional task-specific module.} \wy{put an example of using the original clip feature and using the projected}
%\end{example}

% Therefore, we consider multimodal representation for general purposes, specifically, cross-modality generation. To this end, we align the two representation space by both mutual information and divergence as shown in Fig.~\ref{fig:illustration}.

%Furthermore, InfoNCE mainly relies on the cosine similarity between pairwise positive and negative samples, which does not reflect the underlying distribution distance. Hence, we observed that the empirical distribution of CLIP image and text features are far apart as shown in Fig.~\ref{fig:tsne-wo-alignment}. 



\subsection{CS-Aligner: Alignment with CS Divergence}

% \begin{figure}[t!]
%   \centering  \includegraphics[width=0.45\textwidth]{./Figures/illustration.pdf} % Adjust the size and filename as needed
%   % \vspace{-2mm}
% \caption{\textbf{Alignment with both mutual information and divergence.}
% \label{fig:illustration}
% }
% \end{figure}



% \begin{figure*}[t!]
%   \centering  \includegraphics[width=0.99\textwidth]{icml2025/Figures/framework.pdf} % Adjust the size and filename as needed
%   % \vspace{-2mm}
% \caption{\textbf{\textcolor{red}{option 1}. Illustration of our Modality Alignment. \wy{tbd: image decode from raw $y$} } 
% }
% \end{figure*}

\begin{figure*}[t!]
  \centering  \includegraphics[width=0.99\textwidth]{./Figures/VLAlign_CSD.pdf} % Adjust the size and filename as needed
  % \vspace{-2mm}
\caption{\textbf{Illustration of our Modality Alignment.} 
}
\end{figure*}


Inspired by \textit{Example}~\ref{example:mi-div}, 
to facilitate various multimodal downstream tasks that require distribution alignment (e.g., cross-modality generation), 
we propose to align multimodal representations by maximizing their mutual information while minimizing the divergence between their distributions.
% we propose to maximize the mutual information between the modalities and minimize the divergence between the image and text feature distributions.

%To achieve \zx{better multimodal distribution alignment: give a more precise advantage/motivation} between vision and language representations, we propose to maximize the mutual information between the modalities and minimize the divergence between the image and text feature distributions.

% To achieve this, we utilize a divergence between the two distributions as a constraint, ensuring that the two modalities not only share a representation space but also align their distributions. 
Formally, we parameterize multimodal alignment as the following optimization problem, which maximizes the mutual information while keeping a small divergence between multimodal distributions.
\begin{equation}
\begin{aligned}
&\max \quad I(\mathbf{x}; \mathbf{y}) \\
&\text{s.t.} \quad D(p(\mathbf{x}), p(\mathbf{y})) \leq \epsilon,
\end{aligned}
\end{equation}
where \(\epsilon\) is a small constant. By introducing the Lagrangian multiplier \(\lambda\), the problem is transformed into an unconstrained objective:
\begin{equation}
\begin{aligned}
\min  -I(\mathbf{x}; \mathbf{y}) + \lambda \left( D(p(\mathbf{x}), p(\mathbf{y})) - \epsilon \right).
\end{aligned}
\label{eq:objective}
\end{equation}
%Here, \(\epsilon\) is a small constant and can be omitted from the objective function. 
The final form is then reformulated as:
\begin{equation}
\begin{aligned}
\min  -I(\mathbf{x}; \mathbf{y}) + \lambda D(p(\mathbf{x}), p(\mathbf{y})).
\end{aligned}
\label{eq:objective}
\end{equation}
By doing so, the objective function optimizes the models for both large mutual information and small distribution divergence between multimodal representation distributions. 
%\zx{pairwise similarity and distribution-wise alignment}. 

To measure and minimize the distribution divergence $D(p(\mathbf{x}), p(\mathbf{y}))$ in \eqref{eq:objective}, various metrics can be used,
% There are several metrics available to measure the distribution divergence $D(p(\mathbf{x}), p(\mathbf{y}))$ in \eqref{eq:objective}, 
such as the Kullbackâ€“Leibler (KL) with nonparametric estimator~\citep{wang2009divergence} or the Wasserstein distance~\cite{arjovsky2017wasserstein}.
% To seek a proper distance metric for distribution alignment, we propose several properties that the distribution distance metric is required in modality alignment.
To identify a suitable distance metric for distribution alignment, we propose a set of desired properties that the metric should satisfy in the context of multimodal alignment.

\begin{remark}
    % For distribution alignment, we propose that the metric should satisfy the following properties:
    Key properties for distribution align metrics:
    \begin{itemize}
        \item \textit{Symmetry}: Equally treat both distributions for consistent and unbiased alignment in multimodal scenarios, formulated by
        $D(p(\mathbf{x}),p(\mathbf{y})) = D(p(\mathbf{y}), p(\mathbf{x}))$.
        % , which enables learning shared bi-directional representations in multi-modal alignment.
        
        %\item \textbf{Robustness to Distribution Overlap}: \(0 \leq D(p(x), p(y)) \leq \infty\) when \(0 < \lambda\big(\text{supp}(p(x)) \cap \text{supp}(p(y))\big) < \epsilon\), where \(\text{supp}(p(x))\) and \(\text{supp}(p(y))\) are the support sets of \(p(x)\) and \(p(y)\), respectively, and \(\lambda(\cdot)\) denotes the measure of their intersection. $\epsilon$ is a small value, indicating the two distributions have a small overall. This ensures stability and validity even when distributions have small overlap, which is critical in multi-modality scenarios. 
        %Specifically, the measure \(D(p(x), p(y))\) satisfies 
        % \item \textbf{Efficient Estimation}: Allows for robust and differentiable estimation without distribution assumptions, facilitating optimization in gradient-based frameworks.
        \item \textit{Differentiable and Efficient Estimation}: 
        Enables differentiable estimation without distribution assumptions to facilitate optimization, formulated as $ \partial D(p(\mathbf{x}; \theta), p(\mathbf{y}; \phi)) \neq \emptyset, \forall p(\mathbf{x}), p(\mathbf{y})$. Achieve the estimation non-parametrically or efficiently.
        
        \item \textit{Robustness to Small Distribution Overlap}: 
        Provides reliable measurements even when distributions have minimal overlap, which may often occur in multimodal scenarios. The property is formulated as
        \(0 \leq D(p(\mathbf{x}), p(\mathbf{y})) \leq \infty\) when \(0 < \mu \big(\text{supp}(p(\mathbf{x})) \cap \text{supp}(p(\mathbf{y}))\big) < \epsilon\). $\mu\big(\text{supp}(p(\mathbf{x})) \cap \text{supp}(p(\mathbf{y}))\big)$ denotes the overlap of $p(\mathbf{\mathbf{x}})$ and $p(\mathbf{\mathbf{y}})$. $\epsilon$ is a small value. %, indicating the two distributions have a small overall. 
        % This ensures validity even when distributions have a small overlap, which is critical in multi-modality scenarios. 
                %Specifically, the measure \(D(p(x), p(y))\) satisfies 
        

    \end{itemize}
\end{remark}

These properties allow the divergence in eq. \eqref{eq:objective} to effectively align multimodal distributions within a shared feature space, ensuring efficient and robust alignment across varying types of distributions with any levels of overlap, which is well-suited for large-scale real-world multimodal applications.

% With these properties, the divergence in \eqref{eq:objective} is designed to learn a bi-directional multimodal representation with a robust and efficient estimator, which is well-suited for multimodal alignment in large-scale real-world applications. 

In this case, the widely used KL divergence fails to satisfy these properties due to its asymmetry, challenges in optimization for non-Gaussian data, and unreliable estimation with minimal distribution overlap. Similarly, while the Wasserstein distance is a popular alternative, its estimation is inefficient.
As a result, these metrics are not ideal for multimodal alignment.
Further details are provided in the Appendix~\ref{sec:cs-compare}. 

To satisfy the above properties, we introduce Cauchy-Schwarz (CS) divergence~\citep{principe2000information,principe2000learning} as the metric for estimating the distribution divergence in eq. \eqref{eq:objective}.
Formally, the CS divergence quantifies the distance between probability density
functions as:
\begin{equation}  
D_{\text{CS}}(p(\mathbf{x});p(\mathbf{y}))=-\log \Bigg(\frac{(\int p(\mathbf{x})p(\mathbf{y})d\mathbf{x}d\mathbf{y})^2}{\int p(\mathbf{x})^2d\mathbf{x} \int p(\mathbf{y})^2d\mathbf{y}} \Bigg).  
\label{eq.cs_divergence}  
\end{equation}  


The CS divergence is a symmetric distance metric between any two probability density functions (PDFs) \(p(\mathbf{x})\) and \(p(\mathbf{y})\), satisfying \(0 \leq D_{\text{CS}} < \infty\), with the minimum achieved if and only if \(p(\mathbf{x}) = p(\mathbf{y})\). Furthermore, CS divergence can be estimated in a non-parametric way without prior assumptions on distributions, which provides great flexibility and robustness in capturing complex relationships between data distributions.

By introducing the CS divergence into eq. \eqref{eq:objective}, the objective function becomes:
\begin{equation}
\begin{aligned}
\min  -I(\mathbf{x}; \mathbf{y}) + \lambda D_{CS}(p(\mathbf{x}), p(\mathbf{y})).
\end{aligned}
\label{eq:csobjective}
\end{equation}
In the following section, we detail the exact estimations for both the CS divergence and mutual information terms, followed by the final objective function of the CS-Aligner.

% non-symmetric metric, making it less suitable for learning a shared representation space (bi-directional), as commonly needed in multi-modal learning. 

% \noindent{\textbf{Minimal Distribution Overlap Requirement.}} Furthermore, KL divergence can diverge to infinity when the distributions \(p(x)\) and \(p(y)\) have minimal overlap, making it invalid. In contrast, CS divergence remains well-defined under such conditions and provides a stable estimation~\citep{yu2024cauchy,yindomain}. This issue is particularly critical in multi-modality settings, where the distributions from different modalities may have limited overlap. 

% \noindent{\textbf{Nonparametric Estimation.}} Additionally, when the distributions are not assumed to be Gaussian, a nonparametric estimator is required for KL divergence. A common choice, the k-NN estimator~\citep{wang2009divergence}, is non-differentiable, which poses challenges for optimization in gradient-based learning frameworks. In contrast, the CS divergence demonstrates greater stability and differentiability when paired with KDE, making it a more robust choice.


% (if we use CLIP InfoNCE for $I(\mathbf{x}; \mathbf{y})$)

%An illustration is shown in Fig.~\ref{fig:illustration}, we align the two distributions such that they




% As shown in Eq.~\eqref{eq.cs_est}, the third term constructs a distance matrix between \(\mathbf{x}\) and \(\mathbf{y}\) using a Gaussian kernel. The \(L_2\) loss between paired \(\mathbf{x}\) and \(\mathbf{y}\) corresponds to the diagonal elements of this term when the Gaussian kernel is omitted. This highlights that the previous \(L_2\) loss is a specific case of divergence and does not account for the full distribution alignment, particularly the off-diagonal elements.





% Moreover, it is a symmetric metric with $0\leq D_{CS} < \infty$

%The CS divergence is symmetric and has closed-form expression for mixture-of-Gaussians (MoG)~\cite{kampa2011closed}, both properties do not hold for the popular KL divergence. 
\subsection{CS-Aligner Estimation}

\textbf{CS Divergence Estimation.}
We use CS divergence to quantify the distance between \(p(\mathbf{x})\) and \(p(\mathbf{y})\) as defined in Eq. (\ref{eq.cs_divergence}). Given \textit{i.i.d.} samples \(\{\mathbf{x}_i\}_{i=1}^m\) and \(\{\mathbf{y}_i\}_{i=1}^n\) drawn from \(p(\mathbf{x})\) and \(p(\mathbf{y})\), respectively, 
we empirically estimate the CS divergence using a kernel density estimator (KDE)~\citep{parzen1962estimation}, which enables CS divergence to effectively measure the distance between two multimodal distributions without any parametric distributional assumptions. 
% 
% \noindent{\textbf{Empirical Estimator of $D_{\text{CS}}(p(\mathbf{x});p(\mathbf{y}))$~\citep{jenssen2006cauchy}}}
% Given observations from two distributions (CLIP text and image representations), $\{\mathbf{x}_i\}_{i=1}^M$ and 
% $\{\mathbf{y}_i\}_{i=1}^N$, 
% 
The empirical estimator~\citep{jenssen2006cauchy} of $D_{\text{CS}}(p(\mathbf{x});p(\mathbf{y}))$ is given by:
\begin{equation}
\label{eq.cs_est}
\begin{aligned}
& \widehat{D}_{\text{CS}} (p(\mathbf{x});p(\mathbf{y})) = \log (\frac{1}{M^2}\sum_{i,j=1}^M \kappa({\mathbf x}_i,{\mathbf x}_j)) +  \\ & \log(\frac{1}{N^2}\sum_{i,j=1}^N \kappa({\bf y}_i,{\mathbf y}_j)) 
-2 \log(\frac{1}{MN}\sum_{i=1}^M \sum_{j=1}^N \kappa({\mathbf x}_i,{\mathbf y}_j)).
\end{aligned}
\end{equation}
where $\kappa$ is a kernel function such as Gaussian $\kappa_{\sigma}(\mathbf{x},\mathbf{y})=\exp(-\|\mathbf{x}-\mathbf{y}\|_2^2/2\sigma^2)$. Each term in Eq.~\eqref{eq.cs_est} involves computing a distance matrix using a Gaussian kernel, which has quadratic computational complexity. The equation serves as a symmetric estimator, and with the Gaussian kernel, all operations remain differentiable. Notably, $\widehat{D}_{\text{CS}}(p(\mathbf{x}); p(\mathbf{y})) \rightarrow \infty$ only when $\kappa({\mathbf x}, {\mathbf y}) \rightarrow 0$ in the third term of Eq.~\eqref{eq.cs_est}, due to the presence of $\log(0)$. However, as long as the two distributions have some overlap, ensuring $\kappa({\mathbf x}_i, {\mathbf y}_j)$ is finite, the estimator remains valid.
%\zx{I suggest talking about how this estimation helps CS for symmetry, differentiable and efficient estimation, and robust to overlap. The following discussions on the unpaired data can be discussed later in section 4.4}




% \noindent{\textbf{Connection with Prior Loss.}} 
Previous methods proposed to use $L2$ loss to minimize the distance between multimodal features \citep{ramesh2022hierarchical}.
In contrast, our approach in Eq.~\eqref{eq.cs_est} constructs a distance matrix between $\mathbf{x}$ and $\mathbf{y}$ using a Gaussian kernel in the third term.
The $L2$ term between paired $\mathbf{x}$ and $\mathbf{y}$ in \citep{ramesh2022hierarchical} corresponds to the diagonal elements of this distance matrix when the Gaussian kernel is omitted. 
Thus, the previous $L2$ loss is a specific case of our divergence, focusing only on paired sample reconstruction (e.g., denoising or reverse mapping) without accounting for full distribution alignment, particularly the off-diagonal elements.


\noindent{\textbf{Mutual Information Estimation.}} 
Inspired by CLIP finetuing works~\citep{radford2021learning,zhou2023clip}, we estimate the mutual information term $I(x, y)$ in Eq.~\eqref{eq:objective}, by optimizing its lower bound using InfoNCE (Eq.~\eqref{eq.infonce}).
%\zx{Do we need to also give a function of InfoNCE?}
It is worth noting that the
mutual information and the CS divergence serve complementary roles: mutual information focuses on pairwise alignment, capturing semantic relationships, while divergence focuses on distribution-level alignment. 
Moreover, we specifically choose InfoNCE due to its optimization efficiency and, more importantly, its coherence with the CS divergence in the cosine similarity space.
% \begin{equation}
% \mathcal{L}_{\text{InfoNCE}} = \frac{1}{2} \Big( \mathcal{L}_{\text{image-to-text}} + \mathcal{L}_{\text{text-to-image}} \Big),
% \end{equation}

% where:

% \begin{equation}
% \mathcal{L}_{\text{image-to-text}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\big( \text{sim}(\mathbf{x}_i, \mathbf{y}_i) / \tau \big)}{\sum_{j=1}^N \exp\big( \text{sim}(\mathbf{x}_i, \mathbf{y}_j) / \tau \big)},
% \end{equation}

% and:

% \begin{equation}
% \mathcal{L}_{\text{text-to-image}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\big( \text{sim}(\mathbf{y}_i, \mathbf{x}_i) / \tau \big)}{\sum_{j=1}^N \exp\big( \text{sim}(\mathbf{y}_i, \mathbf{x}_j) / \tau \big)}.
% \end{equation}
% % \begin{equation}
% % \mathcal{L}_{\text{CLS};\, y \rightarrow x} = -\frac{1}{N} \sum_{i=0}^N \log \frac{\exp(\langle \hat{z}_x^i, \hat{z}_y^i \rangle / \tau)}{\sum_{j \in [N]} \exp(\langle \hat{z}_x^i, \hat{z}_y^j \rangle / \tau)}  
% % \end{equation}


\begin{remark}
\label{remark:cs-mi-connection}
The CS divergence is connected with InfoNCE from 
the cosine similarity perceptive. Let us consider a characteristic kernel $\kappa(\mathbf{x}, \mathbf{y}) = \langle \phi(\mathbf{x}), \phi(\mathbf{y}) \rangle_{\mathcal{H}}$, where $\phi$ is a mapping function. For the samples, \(\{\mathbf{x}_i\}_{i=1}^m\) and \(\{\mathbf{y}_i\}_{i=1}^n\), 
let us denote the (empirical) mean embedding in the Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$ by:
\begin{equation}
\boldsymbol{\mu}_x = \frac{1}{m} \sum_{i=1}^m \phi(\mathbf{x}_i) 
\quad \text{and} \quad 
\boldsymbol{\mu}_y = \frac{1}{n} \sum_{i=1}^n \phi(\mathbf{y}_i),
\end{equation}
the empirical estimators of CS divergence:
\begin{equation}
\begin{aligned}
\label{eq.cos_cs}
\hat{D}_{\mathrm{CS}}(p(x); p(y)) &= -2 \log \left( \frac{\langle \boldsymbol{\mu}_x, \boldsymbol{\mu}_y \rangle_{\mathcal{H}}}{\|\boldsymbol{\mu}_x\|_{\mathcal{H}} \|\boldsymbol{\mu}_y\|_{\mathcal{H}}} \right) \\
&= -2 \log \cos(\boldsymbol{\mu}_x, \boldsymbol{\mu}_y).
\end{aligned}
\end{equation}    
\end{remark}
As shown in Eq. (\eqref{eq.cos_cs}), CS divergence evaluates the cosine similarity between distributions, while InfoNCE assesses the cosine similarity between pairwise samples (Eq.~\eqref{eq.infonce-i2t} and \ref{eq.infonce-t2i}). This highlights the complementary of CS divergence and mutual information in similarity level, providing a unified framework for multimodal alignment.
% \zx{Any other benefits? demonstrating the complementary of CS divergence and InfoNCE (or MI).}


\noindent
\textbf{Final Objective Function.}
%\zx{Do we need to give a final objective function for the alignment?}
With the exact estimation of the CS divergence in Eq. (\eqref{eq.cs_est}) and mutual information in Eq. (\eqref{eq.infonce}), the objective function in eq. (\ref{eq:csobjective}) is reorganized and formulated as:
\begin{equation}
\begin{aligned}
\min  \widehat{D}_{\text{CS}} (p(\mathbf{x});p(\mathbf{y})) -\lambda \mathcal{L}_{\text{InfoNCE}}(\mathbf{x},\mathbf{y}).
\end{aligned}
\label{eq:objective}
\end{equation}
%\zx{I didn't see many differences in this equation from eq.10. I expected to provide a more detailed loss function here, which you used in your code.} 
%\wy{the final loss is given by eq. 11 and eq. 1. putting the full equation of the two here seems too heavy }
%\zx{At least the last term shouldn't be I(x;y)}
% \wy{maybe think about the why do we expect this connection}




% \subsection{Alignment Flexibility}
\subsection{Alignment with Unpaired Distribution Information}

% \wy{propose N token - M token alignment if it works}

% Our unpaired alignment has two levels: batch-level, and token-level. 
While mutual information estimation (InfoNCE) relies on pairwise data, the CS divergence estimator (Eq.~\eqref{eq.cs_est}) can be estimated with the unpaired data. 
This enables our CS-Aligner to perform more flexible multimodal alignment by leveraging additional distributional information from unpaired data or tokens.
To do so, we extend the pairwise multimodal alignment in two directions:
Unpaired data alignment and token alignment. %\zx{Change the name later, and detail the two methods in the following.}

%Notably, \(\mathbf{x}\) and \(\mathbf{y}\) do not need to be paired data and can have different numbers of observations. This flexibility allows us to utilize unpaired image-text data more effectively during training, enabling the network to align the two distributions.


\noindent
\textbf{Unpaired vision-language alignment.} Our method effectively leverages two types of unpaired alignments: (1) images with multiple captions, and (2) unpaired images and text samples drawn independently. This is achieved using Eq.~\eqref{eq.cs_est}, where $\{\mathbf{x}_i\}_{i=1}^M$ and $\{\mathbf{y}_j\}_{j=1}^N$ can be sampled independently with $M \neq N$. 
By supporting both scenarios, our method is able to leverage more uncurated unpaired data for distribution alignment, providing greater flexibility for modality alignment.
%\zx{The last sentence is a bit confusing. I assume we still need the pairwise data since we have the InfoNCE term. So the unpaired data is used more for providing additional distribution information to enhance the alignment?}
% consider the following two scenarios in vision language alignment:

% \begin{enumerate}
%     \item Each image has multiple captions.
%     \item Additional unpaired image and text. 
    
% \end{enumerate}


%The prior loss (Eq.~\eqref{eq:prior_loss}), as well as the \(L2\) loss used in \cite{patel2024eclipse} focus on the sample reconstruction (e.g., denoising or reverse mapping) rather than distribution alignment. Such formulation requires paired data (image-text pairs), while the CS divergence is applicable between arbitrary two sets of samples, providing significant flexibility.  


% \noindent{\textbf{Unpaired Batch Alignment.}} 

\noindent{\textbf{Vision-Language Tokens Alignment.}} 
We propose a novel sample-wise internal distribution alignment approach between vision and language tokens. Unlike the CLIP model~\citep{radford2021learning} that aligns only the \texttt{[CLS]}tokens of image and text representations, our method aligns all vision and text tokens. Specifically, for image features $\mathbf{x}_i \in \mathbb{R}^{V \times D}$ (denoted as $p(\mathbf{x}_i)$) and text features $\mathbf{y}_i \in \mathbb{R}^{L \times D}$ (denoted as $p(\mathbf{y}_i)$), we estimate the CS divergence between the $V$ vision tokens and $L$ text tokens, where $D$ represents the feature dimension. The internal token-wise alignment loss, $\mathcal{L}_{\text{token}}$, is then defined as:
\begin{equation} 
\mathcal{L}_{\text{token}} = \frac{1}{B} \sum_{i=1}^B \widehat{D}_{\text{CS}}(p(\mathbf{x}_i); p(\mathbf{y}_i)), 
\end{equation}
where $B$ is the batch size. In general, $V \neq L$, and vision and language tokens have no paired relationship, which makes InfoNCE impossible to estimate. This loss facilitates comprehensive alignment across all tokens, potentially benefiting the fine-grained alignment.
%\zx{I didn't see how CSD makes the token alignment doable while InfoNCE cannot. Is this setting related to CSD? or any other method can also do the token alignment?}



% \subsection{Adaptation and Generation}
\subsection{Parameter-efficient Multimodal Alignment}

To demonstrate the effectiveness of the proposed method, we conduct vision-language alignment based on the pretrained vision and language models (e.g., CLIP \cite{radford2021learning} and large language models (LLM)~\citep{dubey2024llama})
% CLIP vision and language alignment 
in a parameter-efficient way. Specifically, we adapt the pretrained model by two popular frameworks, adapter~\citep{gao2024clip} and LoRA~\citep{hu2021lora}. 

% \zx{Write the following two paragraph more general rather than focusing on the CLIP model and T2I generation task.}

\noindent{\textbf{adapter Alignment.}} 
% We utilize the same architecture as the prior module~\citep{ramesh2022hierarchical} with few layers for our adapter. 
We add a transformer~\citep{vaswani2017attention} on top of the pretrained model as the adapter.
The adapter is optimized by eq. (\ref{eq:objective}) to transform text representation from the text encoder 
into an aligned feature space 
% that aligns 
with the image representations produced by the image encoder. 
\zx{Only transform the text features?}
This lightweight module minimizes the need for additional parameters while maintaining compatibility with pretrained models. It serves as a simple yet effective way to achieve cross-modal distribution alignment without requiring extensive computational resources.



\noindent{\textbf{LoRA Alignment.}} We also explore a parameter-efficient adaptation technique using LoRA (Low-Rank Adaptation)~\citep{hu2021lora}. LoRA inserts trainable low-rank matrices into the pretrained weights of the text encoder. By doing so, it enables fine-grained adjustments to the text representations, aligning them with the image feature distribution. LoRA is highly memory-efficient and well-suited for scenarios with limited computational resources. This approach provides a scalable and modular solution to enhance the text-to-image alignment capabilities of models.
%particularly advantageous as it allows the CLIP model to retain its original parameters, making it 



% \paragraph{Proposition 2 (Empirical Estimator of $D_{CS}(p(y|\mathbf{x}); q_{\theta}(\hat{y}|\mathbf{x}))$)~\cite{yu2024cauchy}.} Given observations $\{(\mathbf{x}_i, y_i, \hat{y}_i)\}_{i=1}^N$, where $\mathbf{x} \in \mathbb{R}^p$ denotes a $p$-dimensional input variable, $y$ is the desired response, and $\hat{y}$ is the predicted output generated by a model $f_{\theta}$. Let $K$, $L^1$ and $L^2$ denote, respectively, the Gram matrices for the variable $\mathbf{x}$, $y$, and $\hat{y}$ (i.e., $K_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)$, $L^1_{ij} = \kappa(y_i, y_j)$ and $L^2_{ij} = \kappa(\hat{y}_i, \hat{y}_j)$, in which $\kappa = \exp\left(-\frac{\|\cdot\|^2}{2\sigma^2}\right)$ is a Gaussian kernel function). Further, let $L^{21}$ denote the Gram matrix between $\hat{y}$ and $y$ (i.e., $L^{21}_{ij} = \kappa(\hat{y}_i, y_j)$). The empirical estimation of $D_{CS}(p(y|\mathbf{x}); q_{\theta}(\hat{y}|\mathbf{x}))$ is given by:

% \begin{align}
% \hat{D}_{CS}(p(y|\mathbf{x}); q_{\theta}(\hat{y}|\mathbf{x})) &= \log \left( \sum_{j=1}^N \left( \frac{\sum_{i=1}^N K_{ji} L^1_{ji}}{\left( \sum_{i=1}^N K_{ji} \right)^2} \right) \right)
% \\
% &+ \log \left( \sum_{j=1}^N \left( \frac{\sum_{i=1}^N K_{ji} L^2_{ji}}{\left( \sum_{i=1}^N K_{ji} \right)^2} \right) \right)
% - 2 \log \left( \sum_{j=1}^N \left( \frac{\sum_{i=1}^N K_{ji} L^{21}_{ji}}{\left( \sum_{i=1}^N K_{ji} \right)^2} \right) \right).
% \end{align}

