\section{Experiments}
\label{sec:exp}
%\subsection{Experiment Setup}

We evaluate our method on two tasks to illustrate its vision-language alignment ability: text-to-image (T2I) generation in Section \ref{sec.t2i} and image-text retrieval in Section \ref{sec.retriev}.


\subsection{Text to image generation}
\label{sec.t2i}
\noindent\textbf{Datasets.} Following a previous T2I approach~\citep{patel2024eclipse}, we train our method on four datasets: \textbf{MSCOCO} \citep{lin2014microsoft}, \textbf{CC3M} \citep{sharma2018conceptual}, \textbf{CC12M} \citep{changpinyo2021conceptual}, and \textbf{LAION-HighResolution-5M} \citep{schuhmann2022laion}. MSCOCO contains 80K images paired with multiple captions. CC3M and CC12M include about 2.5M and 10M image-text pairs, respectively. LAION-HighResolution comprises 175M high-resolution pairs, from which we select 5M for training. We evaluate the aligned model on the MSCOCO 30K validation set.

\noindent\textbf{Experimental setup.} 
We build our method based on unCLIP-style approaches (e.g., DALL-E-2~\citep{ramesh2022hierarchical}, Karlo~\citep{kakaobrain2022karlo-v1-alpha}, Kandinsky~\citep{razzhigaev2023kandinsky}). These methods typically train a diffusion prior module on large-scale datasets (more than hundreds of millions data) to map text representations into the image representation space.
The diffused text representations are fed into a separate decoder for image generation. 

Differently, 
CS-Aligner trains an adapter to align text representations to image feature space on small-scale datasets, e.g., MSCOCO (0.08M), CC3M (3M), and CC12M (12M), and LAION-HighRes subset (5M).
After alignment, we directly process the aligned text features using the pretrained decoder of the large-scale methods (e.g., Karlo and Kandinsky) to generate images, without additional prior modules or multiple diffusion steps.
% 
% We train an adapter to align text representations to the image ones by CS-Aligner.
% After alignment, we utilize the pretrained decoders from the open-source unCLIP-type models Karlo~\citep{kakaobrain2022karlo-v1-alpha} and Kandinsky~\citep{razzhigaev2023kandinsky} to generate images. 
% Specifically, we use MSCOCO, CC3M, and CC12M to train the adapter for Karlo (since these sets overlap with Karlo’s original training data) and MSCOCO plus LAION-HighRes for Kandinsky. 
We evaluate generation quality with the FID score \citep{heusel2017gans}, which measures how closely generated images match the real image distribution. 
This metric is particularly well-suited for evaluating modality alignment, as it directly reflects the distribution distance. 
Additional details can be found in {Appendix~\ref{sec:implementation-details}}.

%\wy{few diffusion steps experiment no big difference, as prior and decoder is too powerful, 2 prior diffusion steps make good quality image. }

\noindent\textbf{Baselines.} Our baselines consists of both large-scale methods Karlo~\citep{kakaobrain2022karlo-v1-alpha}, Kandinsky~\citep{razzhigaev2023kandinsky} and recent small-scale alignment method Eclipse~\citep{patel2024eclipse}. 
Eclipse streamlines the prior module in Karlo and Kandinsky by employing an $L_2$ loss for T2I. 
For a fair comparison, we adopt the same Transformer adapter as Eclipse and only align the “CLS” tokens, highlighting the advantages of our distributional alignment. %approach.

% \noindent{\textbf{Datasets.}} Following a previous T2I method~\citep{patel2024eclipse}, we select four datasets for T2I: MSCOCO~\citep{lin2014microsoft}, CC3M~\citep{sharma2018conceptual}, CC12M~\citep{changpinyo2021conceptual}, and a subset of LAION-HighResolution~\citep{schuhmann2022laion} for training. MSCOCO contains 80K image-caption pairs for training, with each image paired with multiple captions. CC3M and CC12M include approximately 2.5 million and 10 million pairs, respectively. For LAION-HighResolution, which consists of 175 million high-resolution image-text pairs, we use a subset of 3 million pairs for training due to resource constraints. We use the MSCOCO 30K validation set to evaluate the performance of our method. 


% \noindent{\textbf{Experimental Setup.}} 
% We use the T2I task to demonstrate the alignment capability of our method. Specifically, we train an adapter to align text representations with image representations, ensuring they share a common distribution. Leveraging this alignment, we utilize pretrained image decoders from two open-source unCLIP-type models, Karlo~\citep{kakaobrain2022karlo-v1-alpha} and Kandinsky~\citep{razzhigaev2023kandinsky}, to generate corresponding images. For experiments with the Karlo diffusion image decoder, we use MSCOCO, CC3M, and CC12M to train the adapter due to these datasets are subsets of those originally used to train the Karlo decoder. Similarly, we use MSCOCO and LAION-HighRes for experiments with the Kandinsky decoder.
% We adopt the FID score~\citep{heusel2017gans} for evaluation, which measures the similarity between the distribution of generated images and the ground truth set, effectively reflecting alignment quality. Implementation details are provided in the Appendix.

% \noindent{\textbf{Baselines.}} 
% We compare our method with Karlo~\citep{kakaobrain2022karlo-v1-alpha} and Kandinsky~\citep{razzhigaev2023kandinsky}, as we utilize their decoders. Additionally, we select Eclipse~\citep{patel2024eclipse} as a baseline. Eclipse enhances the efficiency of the prior module in Karlo and Kandinsky by training it with an $L2$ loss for the T2I task. For a fair comparison, we use the same adapter network as Eclipse (ten layers of transformer) and align only the "CLS" tokens to demonstrate the advantage of distributional alignment. 


\input{table/results}
% \noindent{\textbf{Distributional alignment benefits the T2I task.}}  
\textbf{Comparisons.}
We compare our method with both the large-scale diffusion-based methods and the small-scale alignment methods.
The results are provided in Table \ref{tab.comparecoco}.
By aligning text representations to image representations on the small MSCOCO data, our method achieves superior T2I generation than the large-scale methods Karlo and Kandinsky, without any diffusion steps.
CS-Aligner also outperforms Eclipse by an obvious margin using either Karlo or Kandinsky decoders.
The results demonstrate the effective vision-language alignment capability of our method.
Moreover, we compare CS-Aligner with Eclipse across different training datasets.
As shown in Table \ref{tab:main_results}, our method performs better across diverse training data (CC3M, CC12M, and LAION-HighRes-5M), underscoring the importance of the modality distribution information for robust alignment.
% \zp{Table~\ref{table:t2i-results} illustrates how our} approach effectively aligns text and image distributions, enabling seamless integration with both Karlo and Kandinsky decoders. By feeding these adapted text embeddings into the decoders, we achieve substantially lower FID scores about 4 to 12 points lower than Eclipse across various training datasets. This underscores the importance of distributional alignment for modality alignment, beyond what pairwise alignment (e.g., \(\ell_2\) or InfoNCE) can achieve.  
% We also establish an “upper bound” by feeding image features directly into the decoders, and our method’s performance is noticeably closer to this bound than the baseline. This further demonstrates the effectiveness of aligning the text distribution with the target image distribution and highlights how distributional alignment bridges the gap between text prompts and high-fidelity image generation.




\begin{figure}[t]
  \centering  
  \vspace{-1mm}
  \includegraphics[width=0.48\textwidth]{./Figures/main-vis.pdf} % Adjust the size and filename as needed
  \vspace{-7mm}
\caption{\textbf{Qualitative Visualization.} We present visualizations for no alignment (left), Eclipse (middle), and CS-Aligner (right). CS-Aligner achieves more realistic generations with stronger semantic consistency.} 
\vspace{-3mm}
\label{fig:main_vis}
\end{figure}

\noindent{\textbf{Qualitative Visualization.}} 
To further evaluate our method, we present qualitative visualizations of generated images using the Karlo decoder. As shown in Fig.~\ref{fig:main_vis}, our aligned text representations result in more realistic images with stronger semantic consistency with the input sentence, highlighting the effectiveness of CS-Aligner in enhancing alignment.
% demonstrates that the text features aligned by our method enable Karlo’s decoder to generate realistic images that semantically align with the textual description. In contrast, without alignment, the generated images often lack meaningful correspondence with the text. \wy{connect to the property of small overlap}



% \begin{table}[htbp]
% \caption{\textbf{CS-Aligner is flexible to both adapter and LoRA.}}
% \centering
% \begin{tabular}{@{}lcc@{}}
% \toprule
% \textbf{}      & \textbf{\#Parameters} & \textbf{FID} \\ \midrule
% adapter        &    34M                  &      12.62        \\
% LoRA           &      6M                &         13.52     \\
% LoRA-for karlo           &      1.3 M                 &              \\
% \bottomrule
% \end{tabular}
% \label{tab:lora_comparison}
% \end{table}

\begin{table}[t]
\caption{\textbf{CS-Aligner with different adaptation approaches.} Our method achieves good alignment using both adapter and LoRA.}
\vspace{-2mm}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Base Model} & \textbf{Adaptation} & \textbf{\#Parameters} & \textbf{FID} \\ \midrule
\multirow{2}{*}{Kandinsky} & Adapter      & 34M   & 12.62 \\
                           & LoRA         & 6M    & 13.52 \\ 
\midrule
\multirow{2}{*}{Karlo}     & Adapter      & 33M   & 11.27 \\ 
                           & LoRA         & 1.3M  & 15.63 \\ 
\bottomrule
\end{tabular}
\vspace{-5mm}
\label{tab:lora_comparison}
\end{table}

\noindent{\textbf{CS-Aligner with different adaptation approaches.}}
To demonstrate the robustness of our method across different models, we perform alignments for T2I using both adapter and LoRA.
Specifically, we apply LoRA with a low-rank dimension of 8 to every transformer layer in the CLIP text encoder.
As shown in Table \ref{tab:lora_comparison}, based on either Karlo or Kandinsky, CS-Aligner with LoRA introduces fewer parameters, while still achieving comparable results compared with the adapter-based one, demonstrating the effectiveness and adaptability of CS-Aligner across different models.
% Our method is compatible with both adapter-based and LoRA-based parameter-efficient adaptation. Specifically, we apply LoRA with a low-rank dimension of 8 to every transformer layer in the CLIP text encoder. As shown in Table~\ref{tab:lora_comparison}, CS-Aligner with LoRA achieves performance comparable to the one with adapter while maintaining a significantly lower parameter count, highlighting its flexibility and efficiency in different adaptation settings.



% \begin{figure}[htbp]
%   \centering  \includegraphics[width=0.4\textwidth]{./Figures/multiple_caption.png} % Adjust the size and filename as needed
%   % \vspace{-2mm}
% \caption{\textbf{CS-Aligner can leverage rich information from multiple captions to improve the performance.} } 
% \label{fig:multiple_caption}
% \end{figure}

\begin{figure}[t] % Adjust width and position as needed
    % \vspace{-5mm}
    \centering
    % First subfigure
    \begin{minipage}{0.24\textwidth}
        \centering
\includegraphics[width=\linewidth]{./Figures/unpair1.png}
        \vspace{-6mm}
        \subcaption{Align with multiple captions.}
        \label{fig:multicaptions}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.24\textwidth}
        \centering        \includegraphics[width=\linewidth]{./Figures/unpair2.png}
        \vspace{-6mm}
        \subcaption{Align with unpaired data.}
        \label{fig:unpairdata}
    \end{minipage}
\vspace{-2.5mm}
\caption{\textbf{ 
CS-Aligner with additional information.} Our method benefits from the additional information from the multiple captions (a) and unpaired data (b). 
}
\vspace{-4mm}
\end{figure}

\noindent{\textbf{CS-Aligner with multiple captions.}} 
It is common in real-world datasets for a single image to correspond to multiple captions (e.g., 5 captions per image in MSCOCO). 
Due to their pairwise alignment nature, previous methods such as InfoNCE and \(\ell_2\)-based approaches~\citep{radford2021learning,patel2024eclipse} struggle to simultaneously leverage multiple captions.
In contrast, by incorporating CS divergence, our CS-Aligner enables training for alignment with single image and multiple captions.
To demonstrate the benefits of multiple captions for CS-Aligner, we conducted experiments on the MSCOCO dataset by estimating the CS divergence term \(\widehat{D}_{\text{CS}}\) in Eq. (\ref{eq:finalobjective}) using both single and multiple captions. 
As shown in Fig.~\ref{fig:multicaptions}, CS-Aligner effectively leverages the information provided by multiple captions, leading to improved vision-language alignment. 


% \begin{table}[htbp]
% \caption{\textbf{CS-Aligner can leverage rich information from multiple captions to improve the performance.}}
% \centering
% \begin{tabular}{@{}lc@{}}
% \toprule
% \textbf{Matching Scenario}               & \textbf{FID} \\ \midrule
% Single Caption     & 12.62                \\ 
% Multiple Captions  & 11.78                \\ \bottomrule
% \end{tabular}
% \label{tab:more-captions}
% \end{table}


% \begin{table}[h!]
% \caption{Paired and unpaired dataset with FID scores.}
% \centering
% \begin{tabular}{@{}lcc@{}}
% \toprule
% \textbf{Paired} & \textbf{Unpaired} & \textbf{FID} \\ \midrule
% 80K            &   -               &  12.62            \\
% %40K            & -   &           \\
% 40K            & 80K (In Domain)   &  12.18         \\
% 80K            & 1M (Out Domain)   &  13.56            \\ \bottomrule
% \end{tabular}
% \label{tab:paired_unpaired_fid}
% \end{table}

\noindent{\textbf{CS-Aligner with additional unpaired data.}} 
Collecting and accurately annotating paired vision-language data is both challenging and costly. Enhancing alignment with additional unpaired data offers a more flexible and scalable solution for real-world applications. However, similar to the case of multiple captions, previous methods~\cite{radford2021learning, patel2024eclipse} struggle to fully utilize unpaired data due to their reliance on pairwise alignment, whereas CS-Aligner naturally incorporates the unpaired data information by CS divergence.
% Although the mutual information estimator (InfoNCE) in our method relies on paired data, similar to previous approaches, the CS divergence component is flexible and can utilize unpaired data.
To demonstrate this capability, we conduct experiments on the MSCOCO dataset using the Kandinsky decoder with (1) 80K paired training samples, (2) 40K paired training samples, and (3) 40K paired training samples supplemented with 80K unpaired samples, where the unpaired samples are used to estimate the CS divergence. {As shown in Fig.~\ref{fig:unpairdata},} 
the result with 40K paired training data is lower than 80K. However, introducing additional unpaired data obviously improves the performance, even surpassing the model trained with 80K paired samples. This demonstrates CS-Aligner's ability to effectively leverage the distributional information of modalities for alignment.

%3). use 1M additional data from Laion (out domain data of COCO) for additional CS divergence estiamtion. 




% \begin{table}[htbp]
% \caption{\textbf{Token Alignment.}}
% \centering
% \begin{tabular}{@{}lc@{}}
% \toprule
% \textbf{Matching Scenario}               & \textbf{FID} \\ \midrule
% w/o      & 12.62                \\ 
% w/ & 12.14                \\ 
% %unapried data w/ token alignment &  \\
% \bottomrule
% \end{tabular}
% \label{tab:token_alignment}
% \end{table}
\noindent{\textbf{CS-Aligner with token alignment.}}
Beyond the unpaired data, CS-Aligner also enables token-level alignment by treating the tokens of each sample as a distribution.  
We evaluated the token-level extension of CS-Aligner using the Kandinsky decoder on the MSCOCO dataset. As shown in Fig.~\ref{fig:token-alignment}, incorporating token alignment further improves performance. Moreover, qualitative results indicate that token alignment enhances fine-grained details in generated images, suggesting an improved ability to capture fine-grained relationships between modalities.  
Additional visualizations are provided in Fig.~\ref{fig:token-alignment-appendix} in Appendix~\ref{sec:token-alignment}.

\begin{figure}[t]
  \centering  \includegraphics[width=0.5\textwidth]{./Figures/multi-token-vis1.pdf}
  \vspace{-7mm}
\caption{\textbf{CS-Aligner with token alignment.} Token alignment enhance more fine-grained vision-language alignment.}
\vspace{-4mm}
\label{fig:token-alignment}
\end{figure}



\subsection{Image-Text Retrieval}
\label{sec.retriev}

% For \textbf{image-text retrieval}, we show that our method effectively aligns LLMs~\citep{dubey2024llama} text representations with CLIP vision representations in a shared distribution, yielding improvements in both \textbf{image-to-text} and \textbf{text-to-image} retrieval tasks.

\noindent{\textbf{Experimental Setup.}} 
Effective multimodal alignment also benefits cross-model retrieval.
To demonstrate the alignment ability of our method on retrieval tasks, we conduct experiments aligning LLMs \citep{dubey2024llama} text representations with CLIP vision representations on both {image-to-text} and {text-to-image} retrieval. 
We use the Flickr 1K test set~\citep{young2014image} for short-text retrieval, while Urban1K~\citep{zhang2025long} and DOCCI~\citep{onoe2025docci} are employed for long-text retrieval. 
We compare CS-Aligner against pure InfoNCE-based methods, such as Long-CLIP~\citep{zhang2025long} and LLM2CLIP~\citep{huang2024llm2clip}, as the baselines.
To ensure a fair comparison, we adopt the setup from LLM2CLIP, aligning CLIP ViT-L/14 image representations with Llama 3 (8B) text representations. Both the vision and text representations are aligned by adapters trained on the CC3M dataset.


\input{./table/retrieval}
\noindent{\textbf{Comparisons.}} The results in Table~\ref{tab:retrieval-results} show that our method consistently and significantly outperforms the baselines across various datasets for both image-to-text (I2T) and text-to-image (T2I) retrieval. This demonstrates the effectiveness of our method for aligning the two modalities into a shared space. Moreover, the ability to align a different text encoder (LLM) with the successful alignment of an LLM-based text encoder with the CLIP image encoder highlights the flexibility and generalizability of our approach.
% \subsection{Ablation Study}







