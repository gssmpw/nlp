\section{Preliminaries}
\label{subsec:pre}




%\subsection{InfoNCE and Cauchy-Schwarz Divergence}


% \paragraph{Cauchy-Schwarz Quadratic Mutual Information (CS-QMI)} CLIP~\citep{radford2021learning} uses a symmetric InfoNCE loss~\citep{oord2018representation} for multimodal representation learning, which essentially is the lower bound of the mutual information. However, the CS divergence is able to estimate the mutual information without a bound. The independence between $\mathbf{x}$ and $\mathbf{y}$ can be measured by any (valid) distance or divergence measure over the joint distribution $p(\mathbf{x}, \mathbf{y})$ with respect to the product of marginal distributions $p(\mathbf{x})p(\mathbf{y})$. The Cauchy-Schwarz quadratic mutual information (CS-QMI)~\citep{yu2024cauchy} is given by:

% \begin{align}
% I_{CS}&(\mathbf{x}, \mathbf{y}) = D_{CS}(p(\mathbf{x}, \mathbf{y}) ; p(\mathbf{x})p(\mathbf{y})) \nonumber \\
% &= -\log \left( \frac{\left( \int \frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x})p(\mathbf{y})} \,dx\,dy \right)^2}{\int p^2(\mathbf{x}, \mathbf{y})\, dx\,dy \int p^2(\mathbf{x})p^2(\mathbf{y})\, dx\,dy} \right).
% \end{align}

\wy{discussion: remove preliminaries, merge to related work and method}
\zx{I think it works}

\noindent{\textbf{Contrastive Language-Image Pre-training.}}  
To learn shared feature space of vision and language data, the Contrastive Language-Image Pre-training (CLIP)
% maintain the sample-specific alignment, CLIP
\citep{radford2021learning} adopt a symmetric InfoNCE~\citep{oord2018representation} loss, which is a lower bound of mutual information.
% to optimize the lower bound of mutual information. InfoNCE enables learning a shared representation space across modalities. 
We denote image and text samples by $\{x_i\}_{i=1}^{N}$ and $\{y_i\}_{i=1}^{N}$, respectively. 
Specifically, it learns the cosine similarity from text to image and vice verse. The multimodal InfoNCE is defined by:

\begin{equation}
\label{eq.infonce}
\mathcal{L}_{\text{InfoNCE}} = \frac{1}{2} \Big( \mathcal{L}_{\text{I2T}} + \mathcal{L}_{\text{T2I}} \Big),
\end{equation}
where:
\begin{equation}
\label{eq.infonce-i2t}
\mathcal{L}_{\text{I2T}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\big( \text{sim}(\mathbf{x}_i, \mathbf{y}_i) / \tau \big)}{\sum_{j=1}^N \exp\big( \text{sim}(\mathbf{x}_i, \mathbf{y}_j) / \tau \big)},
\end{equation}
and
\begin{equation}
\label{eq.infonce-t2i}
\mathcal{L}_{\text{T2I}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\big( \text{sim}(\mathbf{y}_i, \mathbf{x}_i) / \tau \big)}{\sum_{j=1}^N \exp\big( \text{sim}(\mathbf{y}_i, \mathbf{x}_j) / \tau \big)},
\end{equation}
% \zx{make sure the statements are correct.}
% \zx{and, what do x and y denote?}
where $\text{sim}$ is the cosine similarity,  $\text{I2T}$ denotes image to text loss, $\text{T2I}$ represents text to image loss, and $\tau$ is the temperature, which is a learnable parameter. 



%\zx{Do we need preliminary of CSD?}

%\textcolor{orange}{Discussion: \noindent{\textbf{UnCLIP Text-to-Image Prior}} 
%In the text-to-image generation task, the unCLIP model (DALL-E-2~\cite{ramesh2022hierarchical}) is one of the most representative approaches. UnCLIP comprises three main components: 1) a CLIP encoder (for images and text), 2) a decoder module that reconstructs an image from its representation, and 3) a T2I prior module that maps text representations to image representations. The T2I prior module (denoted as $(g_\phi: y \to x)$) is implemented as a diffusion model. For each timestep $t$ and a noised image embedding $x^{(t)} \sim q(t, x)$ (where $q$ represents the forward diffusion process), the diffusion prior estimates the noiseless $z_x$ directly, rather than approximating the Gaussian noise distribution $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$. The prior loss is formally defined as:
% \begin{equation}
% \mathcal{L}_{\text{prior}} \;=\; 
% \mathbb{E}_{t \sim [0,T],\, x^{(t)} \sim q(t, x)} 
% \Bigl[
% \|\,x \;-\; g_{\phi}\bigl(x^{(t)},\, t,\, y\bigr)\|_{2}^{2}
% \Bigr].
% \label{eq:prior_loss}
% \end{equation}}

%a previous attempt~\cite{ramesh2022hierarchical} utilizes a diffusion-preceding module to map the text embeddings to the corresponding vision embeddings. 

