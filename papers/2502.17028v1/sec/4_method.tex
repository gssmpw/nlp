


\section{Methodology}
\label{sec:method}

\input{./Figures/figure2}

% Multimodal alignment is crucial for learning shared semantic spaces across different modalities. This alignment enables robust zero-shot transfer, deep concept understanding, and scalable application across modalities. Previous multimodal models like  CLIP and its variants~\cite{radford2021learning,jia2021scaling,zhou2022learning}  learn text and image representations in a shared embedding space by maximizing mutual information (MI).  
In this section, we first review the mutual information used in previous multimodal methods and analyze its limitations for alignment.
% issue of mutual information and 
Then we introduce the novel CS-Aligner framework for distributional multimodal alignment and detail the estimations of its terms.
After that, we extend the method by incorporating additional distribution information.
Finally, we provide parameter-efficient implementations.
% to harmonize multimodal representations within a unified distribution. Finally, we provide details of the proposed learning paradigms.


\subsection{Mutual Information is insufficient for alignment}
Previous multimodal methods like CLIP \cite{radford2021learning} learn text and image representations in a shared space by maximizing lower bounds (e.g., InfoNCE~\citep{oord2018representation}) of mutual information between modalities:
\begin{equation}
\label{eq:mi}
I(\mathbf{x}; \mathbf{y}) = \int \int p(\mathbf{x}, \mathbf{y}) \log \frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x}) p(\mathbf{y})} \, d\mathbf{x} \, d\mathbf{y},
\end{equation}
where \(p(\mathbf{x})\) and \(p(\mathbf{y})\) are the distributions of image and text features. \(p(\mathbf{x}, \mathbf{y})\) denotes the joint probability. This objective is optimized via the InfoNCE bound~\citep{oord2018representation}, which approximates \(I(\mathbf{x}; \mathbf{y})\) using paired data samples \(\{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N\).  
We denote image-text pairs as \(\{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^{N}\).  
The multimodal InfoNCE loss combines symmetric image-text and text-image alignment terms:
% \begin{equation}
% \label{eq.infonce}
% \mathcal{L}_{\text{InfoNCE}} = \frac{1}{2} \left( \mathcal{L}_{\text{I2T}} + \mathcal{L}_{\text{T2I}} \right),
% \end{equation}
% where
% \begin{align}
% \mathcal{L}_{\text{I2T}} &= -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left( \text{sim}(\mathbf{x}_i, \mathbf{y}_i)/\tau \right)}{\sum_{j=1}^N \exp\left( \text{sim}(\mathbf{x}_i, \mathbf{y}_j)/\tau \right)}, \label{eq.infonce-i2t} \\
% \mathcal{L}_{\text{T2I}} &= -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left( \text{sim}(\mathbf{y}_i, \mathbf{x}_i)/\tau \right)}{\sum_{j=1}^N \exp\left( \text{sim}(\mathbf{y}_i, \mathbf{x}_j)/\tau \right)}. \label{eq.infonce-t2i}
% \end{align}
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{InfoNCE}} &= \frac{1}{2} \left( \mathcal{L}_{\text{I2T}} + \mathcal{L}_{\text{T2I}} \right), \\
\mathcal{L}_{\text{I2T}} = -\frac{1}{N} \sum_{i=1}^N \log & \frac{\exp\left( \text{sim}(\mathbf{x}_i, \mathbf{y}_i)/\tau \right)}{\sum_{j=1}^N \exp\left( \text{sim}(\mathbf{x}_i, \mathbf{y}_j)/\tau \right)}, \\
\mathcal{L}_{\text{T2I}} = -\frac{1}{N} \sum_{i=1}^N \log & \frac{\exp\left( \text{sim}(\mathbf{y}_i, \mathbf{x}_i)/\tau \right)}{\sum_{j=1}^N \exp\left( \text{sim}(\mathbf{y}_i, \mathbf{x}_j)/\tau \right)},
\end{aligned}
\label{eq.infonce}
\end{equation}
where \(\text{sim}(\cdot,\cdot)\) denotes cosine similarity. \(\tau\) is temperature. Critically, this formulation requires paired data \(\{(x_i, y_i)\}\). %, a limitation inherited by subsequent methods~\citep{radford2021learning,huang2024llm2clip} building on this framework.
%Note that previous methods~\citep{radford2021learning,huang2024llm2clip} that are trained with InfoNCE require paired data. 




Although widely adopted, mutual information alone is insufficient for effective modality alignment~\citep{liang2022mind}.  
%The reason can be that mutual information mainly quantifies the statistical and structural dependence between multimodal representations, focusing on their pairwise dependence~\citep{cover1999elements}, while placing limited emphasis on the alignment of the underlying distributions. 
%
%\str{A bit unclear. Structural dependence sounds pretty close to alignment of distribution, no?}
%
% Ensuring the distribution $p(\mathbf{x})$ is related or dependent to the distribution $p(\mathbf{y})$ does not imply that the distributions $p(\mathbf{x})$ and $p(\mathbf{y})$ are statistically close to each other.
%However, establishing dependencies between $p(\mathbf{x})$ and $p(\mathbf{y})$ does not necessarily mean that they are \textit{distributionally close}.
{The reason is that mutual information quantifies the statistical dependence between two random variables~\citep{cover1999elements}, ensuring that the distribution \( p(\mathbf{x}) \) is related to \( p(\mathbf{y}) \). However, it does not guarantee that the distributions \( p(\mathbf{x}) \) and \( p(\mathbf{y}) \) are statistically similar or close to each other.} 
In other words, two distributions can differ significantly or be far apart, yet exhibit strong dependence. 
We illustrate this issue with the following toy example. 
\begin{example}
\label{example:mi-div}
Consider two Gaussian distributions, \(p(\mathbf{x}) \sim \mathcal{N} (\mu_\mathbf{x}, \sigma_\mathbf{x}^2)\) and \(p(\mathbf{y}) \sim \mathcal{N} (\mu_\mathbf{y}, \sigma_\mathbf{y}^2)\), with a joint distribution \(p(\mathbf{x}, \mathbf{y}) \sim \mathcal{N}\left(
\begin{pmatrix}
    \mu_\mathbf{x} \\
    \mu_\mathbf{y}
\end{pmatrix}, 
\begin{pmatrix}
    \sigma_\mathbf{x}^2 & \rho\sigma_\mathbf{x}\sigma_\mathbf{y} \\
    \rho\sigma_\mathbf{x}\sigma_\mathbf{y} & \sigma_\mathbf{y}^2
\end{pmatrix} 
\right)\). Here, \(\mu_\mathbf{x}\) and \(\mu_\mathbf{y}\) are the means of \(\mathbf{x}\) and \(\mathbf{y}\), \(\sigma_\mathbf{x}^2\) and \(\sigma_\mathbf{y}^2\) are their variances, and \(\rho\) is the correlation coefficient and  controls their linear dependency.  When \(\rho = 0.99\), the two modalities are highly dependent, with high mutual information (\(I = 1.959\); see Fig.~\ref{fig:toy1} and~\ref{fig:toy2}). When \(\rho = 0\), the modalities are independent, resulting in zero mutual information (Fig.~\ref{fig:toy3}).  
{Interestingly, two distributions with the same mutual information value can either exhibit minimal statistical distance and nearly identical shapes, including similar locations, widths, and higher-order moments, as shown in Fig.~\ref{fig:toy1}, or have completely different shapes with distinct means ($0$ for $p(\mathbf{x})$ and $2$ for $p(\mathbf{y})$) and variances ($100$ for $p(\mathbf{x})$ and $0.01$ for $p(\mathbf{y})$), 
as illustrated in Fig.~\ref{fig:toy2}. Quantitatively, the former case shows a minimal KL divergence of $0$, while the latter exhibits a KL divergence of nearly $5,194$.} See details in Appendix~\ref{sec:toy_details}.
\end{example}

%{Example}~\ref{example:mi-div} show that despite their   strong dependence and high mutual information, the representation distributions of two modalities could be far  from each other. The issue also occurs in the CLIP model pretrained by InfoNCE, where the vision and language representations have an obvious distributional distance as illustrated by Fig. \ref{fig:tsne-wo-alignment}. The distance between distributions leads to inconsistently aligned multimodal features, hindering the clear representation of shared meanings and effective mapping between modalities, ultimately degrading performance in downstream tasks like cross-modality generation.
%In this case, rarely maximizing the mutual information between multimodal representations is not enough for their alignment. In addition, while directly minimizing the distance between distributions can address the distributional gap, it may result in independent multimodal distributions lacking adequate semantic alignment (Fig. \ref{fig:toy3}). Therefore, to achieve multi-modal alignment at both semantic and distributional levels, enabling more flexible and diverse cross-modality downstream tasks, we propose CS-Aligner, which integrates mutual information with distribution divergence (using CS divergence for robustness and efficient estimation), as shown in Fig.~\ref{fig:illustration}.

Example~\ref{example:mi-div} shows that despite strong dependence and high mutual information, the representation distributions of two modalities can remain misaligned and be far from each other. This issue is also observed in the CLIP model pretrained with InfoNCE, where the vision and language representations exhibit a noticeable distributional gap, as shown in Fig.~\ref{fig:tsne-wo-alignment}. This gap results in inconsistently aligned multimodal features, hindering the clear representation of shared semantics and disrupting effective mapping between modalities. Ultimately, this misalignment degrades performance in downstream tasks, including cross-modality generation.
%Hence, maximizing mutual information alone is insufficient to achieve robust multimodal alignment.  


Notably, although directly minimizing the divergence between distributions may reduce the distributional gap, it risks creating independent multimodal distributions without common semantic information (Fig.~\ref{fig:toy3}). {Therefore, maximizing mutual information and minimizing divergence complement each other to achieve effective multimodal representation alignment.}






% \begin{figure}[t!]
%   \centering  \includegraphics[width=0.45\textwidth]{./Figures/illustration.pdf} % Adjust the size and filename as needed
% \caption{\textbf{Alignment with both mutual information and divergence.}
% \label{fig:illustration}
% }
% \end{figure}



% \begin{figure*}[t!]
%   \centering  \includegraphics[width=0.99\textwidth]{icml2025/Figures/framework.pdf} % Adjust the size and filename as needed
%   % \vspace{-2mm}
% \caption{\textbf{\textcolor{red}{option 1}. Illustration of our Modality Alignment. \wy{tbd: image decode from raw $y$} } 
% }
% \end{figure*}


\subsection{Distributional multimodal alignment}
% Inspired by {Example}~\ref{example:mi-div}, 
% to facilitate various multimodal downstream tasks that require distribution alignment (e.g., cross-modality generation), 
% we propose to align multimodal representations by maximizing their mutual information while minimizing the divergence between their distributions. 
%To address the mutual information issues highlighted in {Example}~\ref{example:mi-div}, we propose distributional multimodal alignment, which captures both the dependence and distributional divergence of multimodal representations. %Notably, our distributional alignment framework accounts for both the dependence and distance characteristics of distributions.
%\str{What do you mean by distributional? The mutual information also relies on distributions, no?}
%\str{I suggest keeping things simpler.}
%\str{Maybe it is important to explain clearly the difference between divergence, shape, mutual information, etc. Perhaps give a definition, because these are so broad terms, that one might understand them in multiple ways.}
% we propose CS-Aligner. Leveraging the robustness and efficient estimation of Cauchy-Schwarz (CS) divergence, CS-Aligner simultaneously maximizes cross-modal mutual information for semantic alignment and minimizes distributional divergence for distributional alignment. This dual alignment strategy enables flexible and diverse downstream multimodal tasks, as illustrated in Fig.~\ref{fig:illustration}.  

{To overcome the limitations of mutual information term alone, we propose a distributional alignment framework.}
Specifically, we introduce a distribution divergence minimization regularization into the optimization of multimodal alignment, defining the overall problem as maximizing mutual information while ensuring a small divergence between multimodal distributions: 
\begin{equation}
\begin{aligned}
&\max ~ I(\mathbf{x}; \mathbf{y}), ~ \text{s.t.} ~ D(p(\mathbf{x}), p(\mathbf{y})) \leq \epsilon,
\end{aligned}
\end{equation}
where \(\epsilon > 0\) is a small constant representing the permissible divergence threshold.  To address this constrained optimization, we introduce a Lagrangian multiplier \(\lambda \geq 0\), reformulating it into an unconstrained problem:   
\begin{equation}
\begin{aligned}
\min  -I(\mathbf{x}; \mathbf{y}) + \lambda D(p(\mathbf{x}), p(\mathbf{y})).
\end{aligned}
\label{eq:objective}
\end{equation}
% \zp{See derivations  in Appendix~\ref{xx}. }
% \wy{not sure what derivations needed.}
This formulation optimizes for high mutual information between paired data \((\mathbf{x}, \mathbf{y})\) while reducing the divergence between the distributions \(p(\mathbf{x})\) and \(p(\mathbf{y})\).  


Unlike parametric distributions, distributions of different real-world modalities exhibit unpredictable variability and inconsistent overlaps, meaning that $p(\mathbf{x})$ and $p(\mathbf{y})$ may follow arbitrary distributions with a small intersection.

%
% \str{I have never heard the terms common distributions and multimodal distributions, in the way you use them (to compare with one another). Maybe unimodal distributions? Although that means something else. Again, lack of clarity (although I do understand what you want to say here).}
%
Therefore, it is crucial to overcome these challenges to measure and optimize multimodal distribution divergence robustly.
Below, we outline several key properties that an effective metric should satisfy for multimodal alignment.

% To measure and minimize the distribution divergence $D(p(\mathbf{x}), p(\mathbf{y}))$ in \eqref{eq:objective}, various metrics can be used, 
% e.g., the Kullback–Leibler (KL) with nonparametric estimator~\citep{wang2009divergence} or the Wasserstein distance~\cite{arjovsky2017wasserstein}. 
% To identify a suitable distance metric for distribution alignment,  the metric should satisfy a set of desired properties  in the context of multimodal alignment.

\begin{remark}
% \vspace{1.5mm}
    Key properties for distribution align metrics:
    \begin{itemize}
        \item 
        \vspace{-3mm}
        \textit{Symmetry}: Both distributions are treated equally, ensuring consistent and unbiased multimodal alignment, formulated by  \(D(p(\mathbf{x}), p(\mathbf{y})) = D(p(\mathbf{y}), p(\mathbf{x}))\).  
        
        \item 
        \vspace{-1mm}
        \textit{Differentiable and Efficient Estimation}: 
        Enable differentiable estimation without distribution assumptions to facilitate optimization, formulated as $ \partial D(p(\mathbf{x}; \theta), p(\mathbf{y}; \phi)) \neq \emptyset, \forall p(\mathbf{x}), p(\mathbf{y})$. Achieve the estimation non-parametrically or efficiently.
        
        \item 
        \vspace{-1mm}\textit{Robustness to Small Distribution Overlap}: 
        Provide reliable measurements even when distributions have minimal overlap of supports, which may often occur in multimodal scenarios. The property is formulated as
        \(0 \leq D(p(\mathbf{x}), p(\mathbf{y})) \leq \infty\) when \(0 < \mu \big(\text{supp}(p(\mathbf{x})) \cap \text{supp}(p(\mathbf{y}))\big) < \epsilon\). $\mu\big(\text{supp}(p(\mathbf{x})) \cap \text{supp}(p(\mathbf{y}))\big)$ denotes the overlap of $p(\mathbf{\mathbf{x}})$ and $p(\mathbf{\mathbf{y}})$. $\epsilon$ is a small value. 
        \vspace{-6mm}
    \end{itemize}
\end{remark}

%These properties allow the divergence term in \eqref{eq:objective} to align multimodal distributions in a shared feature space.  
% Accordingly, they  ensure efficient and robust alignment across varying types of distributions with any levels of overlap, well-suited for large-scale real multimodal applications. 
These properties enable the divergence term in \eqref{eq:objective} to align arbitrary distributions with small support overlap, which is well-suited for large-scale multimodal applications involving deep learning. 

While KL divergence and Wasserstein distance are widely used, they fail to meet these requirements. KL divergence is asymmetric, inefficient for non-Gaussian data, and unreliable for distributions with small overlap~\citep{yu2024cauchy}. Similarly, Wasserstein distance is inefficient to estimate, requiring additional learnable module~\citep{arjovsky2017wasserstein} or Sinkhorn iterations~\citep{cuturi2013sinkhorn}. As a result, these metrics are suboptimal for large-scale multimodal alignment tasks (details in Appendix~\ref{sec:cs-compare}).

\begin{figure*}[ht!]
\vspace{-2mm}
  \centering  \includegraphics[width=0.99\textwidth]{./Figures/VLAlign_CSD.pdf} % Adjust the size and filename as needed
\vspace{-3mm}
\caption{\textbf{Illustration of CS-Aligner.} We achieve vision-language alignment by freezing the pretrained text and image encoders and applying parameter-efficient fine-tuning methods (e.g., adapter) with our CS-Aligner. CS-Aligner optimizes the adapters using the aggregated CS divergence and InfoNCE, as formulated in Eq. (\ref{eq:objective}). 
Once aligned, the adapters are utilized for various cross-modality tasks: the aligned text adapter facilitates text-to-image generation without additional modifications, while the aligned multimodal adapters are used for vision-language retrieval.
}
\vspace{-4mm}
\label{fig:illustration}
\end{figure*}

To satisfy these properties, we introduce CS divergence \citep{principe2000information,principe2000learning}, a symmetric and robust metric to quantify the distance between two probability density functions:
\begin{equation}  
D_{\text{CS}}(p(\mathbf{x});p(\mathbf{y}))=-\log \Bigg(\frac{(\int p(\mathbf{x})p(\mathbf{y})d\mathbf{x}d\mathbf{y})^2}{\int p(\mathbf{x})^2d\mathbf{x} \int p(\mathbf{y})^2d\mathbf{y}} \Bigg).  
\label{eq.cs_divergence}  
\end{equation}  

The CS divergence satisfies all the desired properties.  It is a symmetric distance metric between any two probability density functions \(p(\mathbf{x})\) and \(p(\mathbf{y})\), satisfying \(0 \leq D_{\text{CS}} < \infty\), with the minimum achieved if and only if \(p(\mathbf{x}) = p(\mathbf{y})\). 
%Furthermore, CS divergence can be estimated in a non-parametric way using Kernel Density Estimation (KDE), enabling the inference of densities from data without parametric distributional assumptions. 
Furthermore, CS divergence can be estimated non-parametrically using a kernel density estimator (KDE)~\citep{parzen1962estimation}, eliminating the need for explicit parametric assumptions about the underlying distributions. It also offers an elegant closed-form expression for mixtures of Gaussians (MoG)~\cite{kampa2011closed} and infinite MoG.
This provides significant flexibility in measuring distributional distance.


By incorporating the CS divergence into Eq. \eqref{eq:objective}, we propose \textbf{\textit{CS-Aligner}}, with the objective function:
\begin{equation}
\begin{aligned}
\min  -I(\mathbf{x}; \mathbf{y}) + \lambda D_{CS}(p(\mathbf{x}), p(\mathbf{y})).
\end{aligned}
\label{eq:csobjective}
\end{equation}
 This formulation ensures both semantic and distributional alignment, enabling robust and efficient multimodal learning across diverse real-world tasks. 
 % Below, we detail the estimation methods for both CS divergence and mutual information, leading to the complete objective for CS-Aligner.  

 
\subsection{CS-Aligner}
We use CS-Aligner to align the pretrained multimodal models, as illustrated in Fig. \ref{fig:illustration}.
To enable the alignment, we detail the estimation methods for both CS divergence and mutual information in Eq. (\ref{eq:csobjective}), providing the complete objective of CS-Aligner.

%\textbf{CS Divergence Estimation.}
%We use CS divergence to quantify the distance between \(p(\mathbf{x})\) and \(p(\mathbf{y})\) as defined in Eq. (\ref{eq.cs_divergence}). Given \textit{i.i.d.} samples \(\{\mathbf{x}_i\}_{i=1}^m\) and \(\{\mathbf{y}_i\}_{i=1}^n\) drawn from \(p(\mathbf{x})\) and \(p(\mathbf{y})\), respectively, 
%we empirically estimate the CS divergence using a kernel density estimator (KDE)~\citep{parzen1962estimation}, which enables CS divergence to effectively measure the distance between two multimodal distributions without any parametric distributional assumptions. Specifically, the empirical estimator~\citep{jenssen2006cauchy} of $D_{\text{CS}}(p(\mathbf{x});p(\mathbf{y}))$ is given by:
\textbf{CS divergence estimation.} 
We use KDE to estimate \(D_{\text{CS}}(p(\mathbf{x}); p(\mathbf{y}))\) nonparametrically. 
Given \textit{i.i.d.} samples \(\{\mathbf{x}_i\}_{i=1}^M\sim p(\mathbf{x}) \) and \(\{\mathbf{y}_i\}_{i=1}^N \sim p(\mathbf{y})\), the empirical CS divergence estimator is given by~\citep{jenssen2006cauchy}:
\begin{equation}
\label{eq.cs_est}
\begin{aligned}
& \widehat{D}_{\text{CS}} (p(\mathbf{x});p(\mathbf{y})) = \log \Big(\frac{1}{M^2}\sum_{i,j=1}^M \kappa({\mathbf x}_i,{\mathbf x}_j)\Big) +  \\ & \log\Big(\frac{1}{N^2}\!\!\sum_{i,j=1}^N \!\kappa({\bf y}_i,{\mathbf y}_j)\Big) 
-2 \log\Big(\frac{1}{MN}\!\sum_{i=1}^M \sum_{j=1}^N\! \kappa({\mathbf x}_i,{\mathbf y}_j)\Big).
\end{aligned}
\end{equation}
where $\kappa$ is a kernel function such as Gaussian $\kappa_{\sigma}(\mathbf{x},\mathbf{y})=\exp(-\|\mathbf{x}-\mathbf{y}\|_2^2/2\sigma^2)$.  
%Each term in Eq.~\eqref{eq.cs_est} involves computing a distance matrix using a Gaussian kernel, which has quadratic computational complexity. 
This estimator is symmetric, differentiable, and computationally efficient, making it suitable for multimodal alignment. Moreover, the third term in Eq.~(\ref{eq.cs_est}) ensures that \(\widehat{D}_{\text{CS}}(p(\mathbf{x}); p(\mathbf{y})) \to \infty\) only when \(\kappa({\mathbf x}, {\mathbf y}) \to 0\) (i.e., when the distributions do not overlap). However, as long as there is nonzero overlap between the distributions, the estimator remains well-defined and valid.   


%The estimator is  symmetric, differentiable, and with the Gaussian kernel, all operations remain differentiable. Notably, $\widehat{D}_{\text{CS}}(p(\mathbf{x}); p(\mathbf{y})) \rightarrow \infty$ only when $\kappa({\mathbf x}, {\mathbf y}) \rightarrow 0$ in the third term of Eq.~\eqref{eq.cs_est}, due to the presence of $\log(0)$. However, as long as the two distributions have some overlap, ensuring $\kappa({\mathbf x}_i, {\mathbf y}_j)$ is finite, the estimator remains valid.
%Pairwise $\ell_2$ loss used in \citep{ramesh2022hierarchical,patel2024eclipse} is 
\begin{remark} Connection to the prior loss ($\ell_2$ loss)~\citep{patel2024eclipse}.
    Consider the third term in Eq. (\ref{eq.cs_est}), which involves $\kappa(\mathbf{x}_i,\mathbf{y}_j)$ defined by the Gaussian kernel $\kappa_{\sigma}(\mathbf{x},\mathbf{y})=\exp\!\bigl(-\|\mathbf{x}-\mathbf{y}\|_2^2/2\sigma^2\bigr)$. A second-order Taylor expansion yields
    \begin{equation}
        \kappa(\mathbf{x}_i, \mathbf{y}_j) = \exp\left(-\frac{(\mathbf{x}_i - \mathbf{y}_j)^2}{2\sigma^2}\right) \approx 1 - \frac{(\mathbf{x}_i - \mathbf{y}_j)^2}{2\sigma^2}.
    \end{equation}
    When \(i = j\) (i.e., diagonal of \(\kappa(\mathbf{x},\mathbf{y})\)), this approximation reduces to a weighted \(\ell_2\) loss by \(1/2\sigma^2\), analogous to the Eq.~\ref{eq:prior_loss}. Consequently, the \(\ell_2\) loss emerges as a special case of our divergence, focusing solely on paired sample reconstruction and omitting broader distribution alignment, including off-diagonal (cross-sample) contributions.
\end{remark}

%Previous methods proposed to use $\ell_2$ loss to minimize the distance between multimodal features \citep{ramesh2022hierarchical}.
%In contrast, our approach in Eq.~\eqref{eq.cs_est} constructs a distance matrix between $\mathbf{x}$ and $\mathbf{y}$ using a Gaussian kernel in the third term.
%The  $\ell_2$ term between paired $\mathbf{x}$ and $\mathbf{y}$ in \citep{ramesh2022hierarchical} corresponds to the diagonal elements of this distance matrix when the Gaussian kernel is omitted. 
%Thus, the previous $\ell_2$ loss is a specific case of our divergence, focusing only on paired sample reconstruction (e.g., denoising or reverse mapping) without accounting for full distribution alignment, particularly the off-diagonal elements. \zx{Remark?}


\noindent{\textbf{Mutual information estimation.}} 
The mutual information and the CS divergence serve complementary roles for alignment: mutual information captures semantic relationships by focusing on pairwise samples, while CS divergence aligns modalities at the distributional level (overall data distribution), leveraging global information.
% \str{What is it meant here by global information? Again, I understand more or less, but be more explicit.}

Inspired by CLIP-based methods~\citep{radford2021learning}, we estimate the mutual information term $I(\mathbf{x}, \mathbf{y})$ in Eq.~\eqref{eq:objective}using its lower bound, optimized via InfoNCE (Eq.~\eqref{eq.infonce}). 
We specifically choose InfoNCE not only for the optimization efficiency but also for its compatibility with CS divergence in the cosine similarity space.
 
 

\begin{remark}
\label{remark:cs-mi-connection}
The connection between CS divergence and InfoNCE becomes evident when analyzing both terms from a cosine similarity perspective. For a characteristic kernel \(\kappa(\mathbf{x}, \mathbf{y}) = \langle \phi(\mathbf{x}), \phi(\mathbf{y}) \rangle_{\mathcal{H}}\), where \(\phi\) maps samples to a Reproducing Kernel Hilbert Space (RKHS) \(\mathcal{H}\), the mean embeddings are:  
$
	\boldsymbol{\mu}_x = \frac{1}{m} \sum_{i=1}^m \phi(\mathbf{x}_i) 
	\quad \text{and} \quad 
	\boldsymbol{\mu}_y = \frac{1}{n} \sum_{i=1}^n \phi(\mathbf{y}_i),
$
The CS divergence can then be expressed as:   
\begin{equation}
	\begin{aligned}
		\label{eq.cos_cs}
		\hat{D}_{\mathrm{CS}}(p(\mathbf{x}); p(\mathbf{y})) &= -2 \log \left( \frac{\langle \boldsymbol{\mu}_x, \boldsymbol{\mu}_y \rangle_{\mathcal{H}}}{\|\boldsymbol{\mu}_x\|_{\mathcal{H}} \|\boldsymbol{\mu}_y\|_{\mathcal{H}}} \right) \\
		&= -2 \log \text{sim}(\boldsymbol{\mu}_x, \boldsymbol{\mu}_y),
	\end{aligned}
\end{equation}  
which evaluates the cosine similarity between distributions. Similarly, InfoNCE evaluates cosine similarity between paired samples (Eq. \ref{eq.infonce}). This dual-level similarity assessment underscores the synergy between CS divergence and mutual information, offering a unified and robust framework for multimodal alignment.  
\end{remark}
 



\noindent
\textbf{Final objective function.} 
With the exact estimation of the CS divergence in Eq.~\eqref{eq.cs_est} and InfoNCE in Eq.~\eqref{eq.infonce}, the final objective function of our method is formulated as:
\begin{equation}
\begin{aligned}
% \min  \widehat{D}_{\text{CS}} (p(\mathbf{x});p(\mathbf{y})) -\lambda I(\mathbf{x}; \mathbf{y}).
\mathcal{L}_\text{CS-Aligner} = \widehat{D}_{\text{CS}} (p(\mathbf{x});p(\mathbf{y})) + \lambda \mathcal{L}_\text{InfoNCE}.
\end{aligned}
\label{eq:finalobjective}
\end{equation}

 


\subsection{Extended alignment with unpaired data}
%While mutual information estimation (InfoNCE) relies on pairwise data, the CS divergence estimator (Eq.~\eqref{eq.cs_est}) can be estimated with the unpaired data. 
%This enables our CS-Aligner to perform more flexible multimodal alignment by leveraging additional distributional information from unpaired data or tokens.
%To do so, we extend the pairwise multimodal alignment in two directions:
%Unpaired data alignment and token alignment. 


Benefits from the distributional alignment, we further propose some novel extensions of CS-Aligner, which leverage additional information in unpaired data.
While mutual information estimation requires pairwise data, the CS divergence estimator (Eq.~\eqref{eq.cs_est}) can operate seamlessly on unpaired data without introducing additional computation.
%
%\str{Does this mean it is more computationally expensive because it has to iterate over all possible pairs of x and y?}
%
This unique capability enables CS-Aligner to extend beyond traditional pairwise multimodal alignment by incorporating additional distributional information from unpaired data or tokens. 
We introduce two novel directions for this extended alignment: unpaired data alignment and token alignment.

 


\noindent
\textbf{Unpaired vision-language alignment.} Our method leverages two forms of unpaired alignments: (1) images with multiple captions, and (2) independently sampled unpaired images and texts. The unpaired alignments are achieved using Eq.~\eqref{eq.cs_est}, where $\{x_i\}_{i=1}^M$ and $\{y_j\}_{j=1}^N$ can be independent with $M \neq N$. In both scenarios, our method leverages more uncurated unpaired data for distributional multimodal alignment, providing greater flexibility and robustness.
%By supporting both scenarios, our method eliminates the reliance on curated paired datasets, providing greater flexibility for modality alignment.

\noindent{\textbf{Vision-language token alignment.}} 
%\str{This comes a bit as a surprise? In fact, I would say that it is not clear what our 'method section' is? What is our 'total algorithm' in the end?}
We also propose a novel intra-sample distribution alignment approach between vision and language tokens. Unlike CLIP-based models~\citep{radford2021learning}, which align only the ``CLS" tokens of vision and text representations, the method considers all vision and text tokens for a more fine-grained alignment. Specifically, each vision feature $\mathbf{x}_i \in \mathbb{R}^{V \times D}$ is modeled as a token distribution $p(\mathbf{x}_i)$ containing $V$ vision tokens, while each text feature $\mathbf{y}_i \in \mathbb{R}^{L \times D}$ is represented as a token distribution $p(\mathbf{y}_i)$ consisting of $L$ text tokens. $D$ denotes the feature dimension.  
We compute the CS divergence between the vision and text token distributions. The internal token-wise alignment loss $\mathcal{L}_{\text{token}}$ is formulated as:
\begin{equation} 
\mathcal{L}_{\text{token}} = \frac{1}{B} \sum_{i=1}^B \widehat{D}_{\text{CS}}(p(\mathbf{x}_i); p(\mathbf{y}_i)), 
\label{eq.tokenalign}
\end{equation}
where $B$ is the batch size. In general, $V \neq L$, and vision and language tokens do not have a direct pairing, making InfoNCE inapplicable for estimation. 
Through our distributional alignment, Eq.~\eqref{eq.tokenalign} enables comprehensive alignment across all tokens, capturing more details and potentially enhancing fine-grained alignment.

\subsection{Parameter-efficient multimodal alignment}
 
We demonstrate the effectiveness of our CS-Aligner by performing vision-language alignment in a parameter-efficient manner using pretrained vision and language models, such as CLIP and large language models (LLMs)~\citep{dubey2024llama}. To adapt these pretrained models, we employ two widely used frameworks: adapter~\citep{gao2024clip} and LoRA (Low-Rank Adaptation)~\citep{hu2021lora}.  

\noindent{\textbf{Adapter alignment.}} 
We add a lightweight transformer \citep{vaswani2017attention} on top of the pretrained model as an adapter. 
%The adapter is to transform text embeddings from the text encoder into the image embedding space induced by the image encoder. 
The adapter projects text embeddings or image embeddings into a shared representation space and distribution.
% This lightweight module minimizes the need for additional parameters while maintaining compatibility with pretrained models. It serves as a simple yet effective way to achieve cross-modal distribution alignment without requiring extensive computational resources.



\noindent{\textbf{LoRA alignment.}} We also explore LoRA to insert trainable low-rank matrices into the pretrained weights of the text encoder. It enables fine-grained adjustments to the representations, aligning them with the other modality distribution. 
% LoRA is highly memory-efficient and well-suited for scenarios with limited computational resources. This provides a scalable and modular solution to enhance the text-to-image alignment capabilities of models.

The adapter and LoRA enable efficient alignment of the multimodal large-scale pretrained models, without requiring extensive computational resources.


 
 
