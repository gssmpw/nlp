\section{PAPI Dynamic Scheduling}

We propose an effective scheduling mechanism to offload FC kernels to PUs or FC-PIM units at runtime with low latency and low energy consumption. In this section, we first explain how the scheduling mechanism determines whether an FC kernel is memory-bound, and then provide the implementation details of the runtime scheduling.



\subsection{Memory-Boundedness Identification of the FC Kernel}


We identify whether or not the FC kernel is memory-bound by estimating its arithmetic intensity. Assume that the weight matrix dimensions of the FC kernel are $(h, h)$ and the input given is $(RLP \times TLP, h)$, where $h$ is the hidden dimension in the LLM structure. The arithmetic intensity of an FC kernel can be calculated as follows:
\begin{equation}
\text{AI} = \frac{\#\text{Flops}}{\#\text{Bytes}} = \frac{RLP \times TLP \times h^2 \times 2}{(2 \times RLP \times TLP \times h + h^2) \times 2}
\end{equation}
In state-of-the-art LLMs, the hidden dimension $h$ is typically large to support their advanced natural language processing tasks \cite{chowdhery2023palm}.
For example, $h = 12288$ in the GPT-3 175B model~\cite{brown2020language}, and the arithmetic intensity can be estimated as follows:
\begin{equation}
\text{AI} \approx RLP \times TLP
\end{equation}
Therefore, we can use $RLP \times TLP$ to estimate the arithmetic intensity of an FC kernel, where $RLP$ and $TLP$ are known at runtime.

To evaluate the accuracy of our arithmetic intensity estimation, we assess the FC kernel in the GPT-3 66B model using various RLP and TLP configurations. Figure~\ref{fig:AI-prediction} shows the actual obtained arithmetic intensity our estimated values. In most cases, our estimations very closely match the actual arithmetic intensity.
When parallelization level is very large (e.g., RLP=128), the estimated value is slightly larger than the actual arithmetic intensity. In such cases, the actual arithmetic intensity of the FC kernel exceeds the maximum theoretical computation throughput of the PUs of the high-performance processor. Therefore, this small deviation does not impact the offloading decision, correctly identifying the FC kernel as compute-bound and ensuring accurate scheduling.

\begin{figure}[h]

\includegraphics[width=0.85\columnwidth]{Figure/AI-prediction-2.pdf}
\caption{Actual measured arithmetic intensity and the estimated arithmetic intensity for FC kernels in the GPT-3 66B model.}
\label{fig:AI-prediction}

\end{figure}


\subsection{Runtime Scheduling Implementation}
Based on estimated arithmetic intensity, we identify memory-bound or compute-bound FC kernels and dynamically schedule them to the best-fit computing hardware units at runtime.
The scheduling process is executed on the host CPU in two steps: (a) initial scheduling and (b) runtime scheduling.

\subsubsection{Initial Scheduling}
In the initial scheduling step, we decide to offload FC kernels to PUs or FC-PIM units before the LLM serving starts. $RLP$ is set to the batch size, and $TLP$ is set to the system-defined speculation length. We multiply $RLP$ by $TLP$ to estimate the arithmetic intensity and compare it to a memory-boundedness threshold $\alpha$ to make the offloading decision. If the estimated value is larger than $\alpha$, the FC kernel is estimated as compute-bound and offloaded to PUs; otherwise, it is estimated as memory-bound and executed on FC-PIM units. The threshold $\alpha$ is determined through offline iterative evaluation, where we run the FC kernel on both PIM and PU units under varying parallelization levels, using the observed execution times to establish the best $\alpha$ to choose.


\subsubsection{Runtime Scheduling}
In runtime scheduling, we monitor changes in parallelism, predict the current arithmetic intensity, and determine whether or not to reschedule FC kernels to a different computing hardware (i.e., from PUs to FC-PIM units and vice versa). We use a token-level scheduling scheme to track parallelism changes and make real-time decisions based on the estimated arithmetic intensity. The process involves four steps, which we describe next.


First, after each decoding, we gather the output tokens of all requests in the current batch into a single vector.
Second, we count the number of $\text{\textless|eos|\textgreater}$ tokens in this vector to track changes in $RLP$. If the count is greater than zero, it indicates that some requests have been finished, releasing the corresponding PIM resources allocated to Attn-PIM. $TLP$ is typically set initially and does not change frequently at runtime, so we monitor changes in $TLP$ with a direct approach: the $TLP$ value is stored in a dedicated register, and if the system software running on the host CPU modifies $TLP$, the host CPU notifies (sending instructions) the PAPI system to update the register accordingly.
Third, we calculate $RLP \times TLP$ to predict the arithmetic intensity of the next decoding.
Fourth, we compare the estimated value with the memory-bound threshold $\alpha$ to decide whether rescheduling FC kernels from PUs/FC-PIM units to FC-PIM/PUs is needed.
Figure~\ref{fig:heter_arch}(d) shows an example of our proposed dynamic scheduling technique that enables the execution of LLM decoding on the most suitable hardware units of our proposed architecture based on the real-time demands of the workload, significantly optimizing the performance of LLM inference.

