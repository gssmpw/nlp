\section{PAPI Architecture}


\subsection{FC-PIM Design}
To meet the computation demands of the FC kernel, we need to design a PIM solution with relatively high computation parallelism, while satisfying the necessary power constraints. We modify and use an open-sourced HBM-based PIM simulator \cite{park2024attacc} that is based on Ramulator 2.0~\cite{luo2023ramulator, kim2015ramulator} to evaluate energy consumption and power across different PIM configurations.

We first examine the energy breakdown in a traditional PIM design (e.g.,~\cite{park2024attacc}) that integrates one processing core per memory bank, referred to as 1P1B.
The energy consumption of PIM execution comes from three parts: $DRAM\; Access$, $Transfer$, and $Computation$. $DRAM\; Access$ includes the energy consumption required to activate and precharge an HBM DRAM row to read the weight data.
$Transfer$ includes the energy consumption of transferring activation data from the buffer die, via the TSV, global controller, and bank group controller, to the processing core.
$Computation$ includes the computation energy in floating point multiplication units (FPUs) of the processing core.
As shown in Figure~\ref{fig:xPxB}(a), most of the energy in PIM execution is consumed by \emph{$DRAM\; Access$}, which accounts for 96.7\% of the total energy consumption\footnote{This energy consumption breakdown is very different from that the HBM-PIM paper~\cite{lee2021hardware} reported. The key difference is that the HBM-PIM paper~\cite{lee2021hardware} reports \emph{only} the energy consumption breakdown of data movement, while we report the energy consumption breakdown of \emph{both} data movement and computation.} larger than the Q vector (activation data).

\begin{figure}[!t]

\centering
\includegraphics[width=\columnwidth]{Figure/xPxB-2.pdf}
\caption{(a) Energy breakdown of PIM for executing the FC kernel with no DRAM data reuse. (b) Energy breakdown of PIM for executing the FC kernel when one DRAM access (i.e., an activated DRAM row) is used 64 times for computation (i.e., data reuse level = 64). (c) Power consumption of PIM architecture with different data reuse levels and different numbers of FPUs per bank.}
\label{fig:xPxB} 

\end{figure} 



Based on the above analysis, accessing data from DRAM \emph{once} and reusing it for \emph{multiple} computations can significantly reduce energy consumption. If data can be accessed from DRAM once and used for multiple computations, the total energy consumption of PIM execution can be reduced significantly.
Figure~\ref{fig:xPxB}(b) shows the energy breakdown of PIM when data is fetched once from DRAM and then reused for 64 FC kernel computations.
The energy consumption of $DRAM\;Access$ reduces to 33.1\% of the overall energy consumption. This approach gives us a new opportunity to enhance the parallel computation throughput of near-bank PIM. By lowering the energy cost of DRAM access, we gain additional energy budget for the PIM cores.


As described in Section 2, parallelism techniques (batching and speculative decoding) enable data reuse in LLM decoding, which enables parallel PIM execution by allowing the reduction of the \emph{$DRAM\;Access$} component.
We analyze the power consumption with varying data reuse levels, where a single DRAM access is reused across multiple computations. We explore different PIM configurations, i.e., different numbers of FPUs per DRAM bank. Figure~\ref{fig:xPxB}(c) shows our results. $xPyB$ denotes x FPUs per y banks. The horizontal axis represents the data reuse level, which indicates how many times a single DRAM row is used for FC kernel computations. The vertical axis shows power consumption. We observe that a higher data reuse level leads to a significantly lower power consumption. Specifically, when the data reuse level is $\geq$ 4, the power consumption of 4P1B becomes significantly lower than that without data reuse (i.e., data reuse = 1) and meets the power budget of HBM.\footnote{The power budget of an 8-high, 16GB HBM3 cube is 116 watts~\cite{park2024attacc} following the IDD7 measurement methodology, described in the JEDEC HBM3 specification~\cite{JEDEC}.} Thus, exploiting data reuse enables the use of more FPUs per DRAM bank while staying within power constraints.

Apart from power constraints, the area constraint of a single HBM die is also a significant barrier against highly parallel PIM designs. Thus, we must ensure that the total HBM-PIM die area including the addtional FPUs stays within the maximum allowable area for a single HBM die.
To accommodate additional FPUs within the area-constrained HBM die, we reduce memory capacity, freeing up space for the FPUs.
Assuming each PIM-enabled HBM die has $m$ DRAM banks and each DRAM bank employs $n$ FPUs. The total area of memory and FPUs should satisfy the following condition:
\begin{equation}
    m (n \times A_{FPU} + A_{bank}) \leq A_{Max}
\end{equation}
This equation allows us to calculate $m$ to obtain the maximum capacity achievable in a PIM-enabled HBM die using an nP1B PIM configuration. 

We use the analytical tool CACTI-3DD \cite{chen2012cacti} to estimate area. The area of one HBM bank $A_{bank}$ is 0.83mm²~\footnote{The area of bank includes both the memory array and peripheral circuits.} using a 22nm technology node. The area of one HBM die is constrained to 121 mm² according to prior work \cite{ryu202316}. 
The area of one FPU $A_{FPU}$ is 0.1025 mm² \cite{park2024attacc}.
Thus, the equation for a 4P1B PIM configuration becomes as follows: 
\begin{equation}
    m(0.1025 \times 4 + 0.83) \leq 121
\end{equation}
Therefore, the maximum number of memory banks must be smaller than 97. In our design, we use 96 banks per HBM memory unit, i.e., 3 bank groups (BGs) in the 8-High HBM stack, so as to meet the area constraint of one HBM die with a 4P1B PIM configuration, as shown in Figure~\ref{fig:heter_arch}(b).





\subsection{Attn-PIM Design}

To address the varying arithmetic intensity, computation demands, and memory footprint of FC and attention kernels, while ensuring high hardware resource utilization, we propose dedicated Attn-PIM units, separate from the FC-PIM units (as described in Section 6.1). The Attn-PIM units are disaggregated from the high-performance processor through an interconnect. This disaggregated design of Attn-PIM allows us to tackle the growing memory footprint demands of KV caches of LLMs, as we explain next.



We find that FC kernels of LLMs have larger computation intensity and result in significantly larger latency than the attention kernels, while attention kernels have larger memory footprint demands. Therefore, given a fixed area budget, FC-PIM requires a configuration with higher computation capability, while the attention kernel does not need as much computation capability. To meet these constraints, we allocate FC-PIM devices with high execution parallelism, i.e., 4 FPUs per DRAM bank (as described in Section 6.1). In contrast, we allocate a larger number of Attn-PIM devices, each of which has lower execution parallelism, using 1 FPU for every two banks, as shown in Figures~\ref{fig:heter_arch}(b) and (c), respectively. Using a single FPU for two banks in Attn-PIM devices ensures that power consumption stays within the HBM power constraints. For attention kernels with a speculation length of 1, a single FPU at 666 MHz with 20.8 MB/s per-bank bandwidth (1P1B) matches the arithmetic intensity of the kernel. However, due to the lack of data reuse in this kernel, the power consumption of 1P1B exceeds the power budget, as shown in Figure~\ref{fig:xPxB}(c). Consequently, we adopt the 1P2B configuration for each Attn-PIM device to stay within the power consumption limits.



After configuring the FC-PIM and Attn-PIM hardware designs, we determine how many of each of the two types of PIM devices are required for the entire system to efficiently run LLM inference. We configure the total number of PIM devices in the system considering the capacity requirement of LLM inference.
The memory capacity requirement for the FC kernel is determined solely by the model size and does not change during runtime.
However, the memory capacity required for the attention kernel increases linearly with the sequence length.
To support requests with longer sequence lengths, i.e., requests that produce a larger number of output tokens, we disaggregate the Attn-PIM devices from the high-performance processor, which enables accommodating a large number of Attn-PIM devices that can house large memory footprints (in a flexible manner).


Overall, by separately optimizing the parallel computation and memory capacity capabilities of FC-PIM and Attn-PIM devices and having different numbers of devices in the system for the two types of PIM devices we propose, we can satisfy the higher computation and memory bandwidth demands of FC kernels while also satisfying the higher memory capacity and lower computation demands of attention kernels. 



\subsection{System Integration} %
Figure~\ref{fig:heter_arch}(a) shows an overview of the interconnection network between the Attn-PIM devices and the high-performance processor and host CPU, where the high-performance processor consists of FC-PIM devices and processing units. FC-PIM devices require high-speed communication with the processing units due to the large volume of weight parameters transferred.
Therefore, we select high-speed interconnects like NVLink~\cite{NVlink} to connect the FC-PIM devices with the processing units. NVLink provides the required data throughput to ensure that the FC kernels can be executed efficiently without being bottlenecked by data transfer speeds.
In contrast, the attention kernel primarily involves small data transfers, such as byte-level Q vector, so a standard interconnect like PCIe (Peripheral Component Interconnect Express)~\cite{mayhew2003pci} or CXL (Compute Express Link)~\cite{das2024introduction} suffices, depending on the number of devices. PCIe theoretically supports up to 32 devices per bus~\cite{miller2009motivating}, while CXL can scale to 4,096 devices~\cite{das2024introduction}. These conventional links offer adequate bandwidth for attention kernels and are more cost-effective than high-speed ones.




\subsection{Data Partitioning Across PIM Devices}
For the attention kernel, we distribute attention heads across Attn-PIM units, with each head assigned to a separate HBM device. We employ the attention mapping scheme from AttAcc~\cite{park2024attacc} on an HBM device, which ensures efficient data movement and parallelism across the PIM architecture. Specifically, the $K^T$ matrix is partitioned column-wise at the pseudo-channel and bank-group levels, and row-wise at the bank and multiplier level. Conversely, the $V$ matrix is partitioned row-wise at the pseudo-channel and bank-group levels, and column-wise at the bank and multiplier level.


For the FC kernel, the large weight matrix is first divided into smaller 2D blocks, each mapped to an HBM device. At the pseudo-channel, bank-group, and bank levels, these weight blocks are partitioned similarly to the $K^T$ matrix in the attention kernel: column-wise at the pseudo-channel and bank-group levels, and row-wise at the bank level.


\subsection{Practicality and Architectural Scalability}

\textbf{Complementary PIM Units for Diverse Workloads.}
We design different FC-PIM and Attn-PIM devices to address distinct computation and memory access patterns in LLMs while maintaining hardware practicality.
Both FC-PIM and Attn-PIM devices share the same bank-level computation fabric and memory hierarchy. The key difference lies in the number of processing units (PUs) per bank, which is tailored to the specific computation characteristics of LLM tasks. Attn-PIM, optimized for memory-bound operations, uses fewer PUs per bank to handle memory-intensive tasks efficiently, while FC-PIM is designed for more computation-intensive operations like fully connected (FC) layers, with more PUs per bank to enable higher computation throughput.

The design of Attn-PIM has already been demonstrated to be implementable in industry prototypes and products, such as UPMEM~\cite{upmem, upmem2018,gomez2022benchmarking,gomez2023evaluating,rhyner2024pim,hyun2024pathfinding,giannoula2022sparsep,nider2021case,gomez2021benchmarkingcut} and HBM-PIM~\cite{lee2021hardware, kwon202125}. Its integration into our system ensures efficient processing of memory-bound tasks, making it a suitable solution for LLM workloads.

FC-PIM, on the other hand, leverages a higher number of PUs per bank to enhance parallel execution and optimize computation throughput for FC layers, while staying within the power limitations of HBM.

By utilizing a shared HBM-PIM computing substrate, both FC-PIM and Attn-PIM benefit from a unified design that avoids modifications to the DRAM core array. Computation logic is embedded within the peripheral circuits, minimizing area overhead while ensuring compatibility with existing HBM technology. We believe this approach simplifies hardware integration and offers scalability, making PIM technology easier to adapt for large-scale deployment in LLM accelerators.


\noindent\textbf{Deployment of Emerging LLM Models.}
The rapid development of LLMs, particularly Mixture of Experts (MoE) models~\cite{shazeer2017outrageously, lepikhingshard, fedus2022switch}, has introduced new challenges and opportunities for hardware accelerators. MoEs activate only a subset of experts during inference, leveraging sparsity to reduce computation demands. This property is advantageous for hardware accelerators, as it allows for more efficient resource utilization.

FC-PIM is particularly well-suited to exploit the sparsity inherent in MoE architectures. In an MoE model, different experts are activated depending on the input, and the sparsity of these activations presents a significant opportunity to optimize computation. FC-PIM can efficiently execute these sparse operations by storing weight slices from different experts within the same DRAM bank. This allows the system to minimize idle FPUs, which would otherwise remain unused due to the sparsity of MoE models. Moreover, by reducing unnecessary data movement between memory and computation units, FC-PIM helps lower both the energy consumption and the latency associated with MoE inference. These design choices ensure that PAPI can effectively accelerate MoE-based models, making it a viable solution for future LLM architectures.

In summary, the practical implementation of both FC-PIM and Attn-PIM within the PAPI architecture offers a scalable and energy-efficient solution for modern LLM workloads. By leveraging the complementary strengths of these two types of PIM devices and addressing the specific needs of emerging LLM models like MoEs, PAPI is well-positioned to provide high-performance acceleration for a broad range of future LLM applications.
