\section{Related Work}
%the key point is to describe why not compare with them

%The memory-bound issue mainly bottlenecks the LLM inference, especially in the decoding phase. Therefore, researchers have proposed many software and hardware approaches to accelerate LLM inference.

To our knowledge, PAPI provides the first architecture and a runtime framework to tackle dynamically varying parallelization levels and hence dynamically varying computation and memory demands of real-world LLM workloads. We comprehensively compare PAPI to two state-of-the-art PIM designs, AttAcc~\cite{park2024attacc} and HBM-PIM~\cite{lee2021hardware}, demonstrating PAPI's significant performance and energy benefits over them (Section \ref{sec:7.2}).
%PAPI designs a heterogeneous PIM-enabled architecture and uses an online kernel characterization to map the kernels to suitable hardware units effectively. 


%We briefly discuss prior works on accelerating LLM inference.


\noindent\textbf{PIM-enabled LLM accelerators.}
The PIM computing paradigm~\cite{mutlu2022modern} addresses the data movement bottleneck between memory and processors by placing computation near or inside memory circuitry.
For transformer-based LLMs, PIM (e.g., \cite{he2020newton, kwon202125, skhynixpim, kwon2022system, park2024lpddr, kal2023aespa, lee2021hardware,jang2024smart, yazdanbakhsh2022sparse, li2024asadi, zhou2022transpim}) provides a promising opportunity to accelerate the memory-bound kernels in the decoding phase.
DRAM-based PIM~\cite{he2020newton, mutlu2024memory}, with its large memory capacity and bandwidth, is particularly well-suited for LLMs. For example, the SK Hynix AiM PIM architecture \cite{kwon2022system} offloads both FC and attention kernels to GDDR6-PIM accelerators, outperforming A100 GPUs in single-batch scenarios. However, the architecture performs poorly when FC kernels are compute-bound, e.g., with larger batch sizes.


Prior works propose heterogenous PIM-enabled computing systems for LLM inference. 
AttAcc \cite{park2024attacc} proposes an HBM-based PIM architecture for attention kernels while running the FC kernels on GPUs to accelerate LLM inference with large batch sizes. Section~\ref{sec:7.2} shows that PAPI outperforms this scheme by designing a more effective PIM-based architecture carefully tailored to the dynamically varying computation and memory needs of FC and attention kernels.
IANUS \cite{seo2024ianus} offloads \emph{all} FC kernels to PIM to efficiently handle non-batched requests.
This would provide low performance in scenarios involving batched requests, which are common in real-world LLM inference.
SpecPIM \cite{li2024specpim} proposes a PIM-enabled system with NPUs and PIM cores, leveraging speculative decoding. It introduces a decoding parallelism-aware scheduling method based on a genetic algorithm and Monte Carlo Tree Search (MCTS). This offline scheduling process involves 50 rounds of the genetic algorithm and 10,000 leaf node searches for MCTS.
While this scheduling method provides performance benefits in cases with a fixed batch size and speculation length, its computational complexity makes it impractical for dynamic execution. In dynamic real-world LLM inference scenarios, especially when decoding parallelism levels vary over time, SpecPIM would need to repeatedly run MTCS scheduling, incurring high-performance costs.



\noindent\textbf{Other LLM accelerators.}
Prior works explore hardware LLM accelerators to improve LLM inference performance.
DFX \cite{hong2022dfx} introduces a multi-FPGA accelerator with high-bandwidth memory (HBM) for end-to-end inference acceleration, and provides an efficient dataflow when the decoding stage is memory-bound. 
However, even when using HBM, such designs still suffer from the memory bottleneck, especially when attention kernels exhibit very low arithmetic intensity \cite{xia2023flashllm}. 
AMX-GPU~\cite{kim2024exploiting} proposes an adaptive LLM model scheduling strategy for CPU-GPU cooperative computing. While this design can adapt to different batch sizes and token lengths, it does not account for runtime changes in parallelism, such as varying concurrency of requests or dynamic changes in computation versus memory bottlenecks.

Recent research utilizes various approximation algorithms, like pruning and quantization, to reduce the amount of data movement (e.g., \cite{wang2023cta, qu2022dota, kao2023flat, ham20203, dong2023heatvit, dass2023vitality, you2023vitcod, ham2021elsa, guo2023olive, lu2021sanger}).
For example, SpAtten \cite{wang2021spatten} introduces token pruning to remove unimportant tokens during inference.
These approximation approaches are suitable for LLM scenarios that can tolerate approximate results. PAPI does not sacrifice quality in LLM serving, while providing significant performance and energy benefits over state-of-the-art systems.

