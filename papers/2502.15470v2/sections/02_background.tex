\section{Background}




\subsection{LLM Inference}



An LLM structure contains several transformer-based decoders, as illustrated in Figure~\ref{fig:parallel_decoding}(a). Each decoder includes four kernels: QKV (Query, Key, and Value) generation, multi-head attention, projection, and feedforward networks \cite{vaswani2017attention}. These kernels can be divided into two types: fully-connected (FC) layers in orange and a multi-head attention layer in green. All kernels consist of general matrix-vector multiplication (GEMV) computations.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{Figure/model_arch3.pdf}
\caption{(a) LLM structure. (b) LLM inference with serial decoding. (c) Parallel decoding process with batching. (d) Parallel decoding process with speculative decoding.}
\label{fig:parallel_decoding}
\end{figure}

Figure~\ref{fig:parallel_decoding}(b) presents an overview of LLM inference, which includes two phases: \emph{prefill} and \emph{decoding}. In the prefill phase, the model processes multiple tokens within the input sequence simultaneously to generate the first output token. The decoding phase has multiple decoding iterations. In each iteration, the model takes the last output token as input to generate a new output token. The output sequence is generated one by one sequentially until the $\text{\textless|eos|\textgreater}$ (end of a sentence) token \cite{vaswani2017attention}. This serialized process is called \emph{serial decoding}. Compared to the prefill phase, the decoding phase typically takes most of the end-to-end inference time \cite{zhang2023draft}. The number of decoding iterations depends on the output token length. With serial decoding, generating $K$ output tokens takes $K$ decoding iterations \cite{leviathan2023fast}. 




In LLM inference, the output of QKV generation, i.e., K and V matrices, is stored as these output values are reused multiple times in the multi-head attention kernel during subsequent decoding iterations. In serial decoding, LLM inference requires GPUs/TPUs to load the large weight matrices and KV matrices from off-chip memory to on-chip memory/caches during \emph{each} serial decoding iteration, causing high data movement and performance overheads.




\subsection{Optimization Techniques in LLM Inference}

To overcome performance overheads of conventional serial decoding, researchers have developed two optimization techniques: batching~\cite{vaswani2017attention, yu2022orca, agrawal2023sarathi, patel2024splitwise} and speculative decoding \cite{leviathan2023fast, chen2023accelerating, spector2023accelerating, zhang2023draft}. These methods facilitate the \emph{concurrent} decoding of \emph{multiple} tokens, thereby improving data reuse of weight matrices via generation of multiple tokens, which improves performance. 



\subsubsection{Batching}
Batching~\cite{vaswani2017attention, yu2022orca, agrawal2023sarathi, patel2024splitwise} is a parallelism technique to process multiple input sequences concurrently. It allows a single decoding step to generate multiple tokens from different user requests.
This enables request-level parallelism (RLP) during inference, where RLP refers to the number of requests executed in parallel. For example, as shown in Figure~\ref{fig:parallel_decoding}(c), when an LLM processes two input requests simultaneously, RLP is 2. A state-of-the-art batching mechanism is \emph{mixed continuous batching} \cite{agrawal2023sarathi, patel2024splitwise}. In mixed continuous batching, the system dynamically adjusts the number of requests in a batch at runtime based on available memory and computation resources, as well as the number of incoming requests. This technique allows new requests to be added to the current batch without waiting for all requests of the batch to be completed, thereby optimizing resource utilization and improving overall throughput.




\subsubsection{Speculative Decoding} Speculative execution introduces a novel parallel decoding mechanism \cite{leviathan2023fast, chen2023accelerating}, which includes two steps: \emph{serial draft decoding} and \emph{parallel speculative decoding}.
First, a small draft model efficiently predicts the next 2-10 tokens, and these predicted tokens are then verified simultaneously with the large original LLM, allowing for the next tokens to be processed in parallel. Figure~\ref{fig:parallel_decoding}(d) shows this mechanism when two tokens are generated in one decoding iteration of the LLM. Speculative decoding enables token-level parallelism (TLP) in LLM inference.
TLP represents the number of concurrently decoded tokens within a single decoding iteration. For example, two yellow tokens in Figure~\ref{fig:parallel_decoding}(d) for one request are decoded simultaneously, i.e., when TLP = 2.