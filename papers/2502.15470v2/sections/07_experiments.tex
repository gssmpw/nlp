

\section{Evaluation}

\subsection{Evaluation Methodology}


\textbf{Comparison Points and Simulation Methodology.} We compare PAPI with three state-of-the-art systems: (a) \emph{A100+AttAcc:} a heterogeneous computing platform with 6 NVIDIA A100 GPUs~\cite{choquette2020nvidia} and AttAcc PIM-based units (one FPU unit per DRAM bank, i.e., 1P1B configuration), which is the state-of-the-art design proposed by prior work \cite{park2024attacc}. All FC kernel computations are executed on GPUs, and attention kernel computations are handled by AttAcc PIM-based units; (b) \emph{A100+HBM-PIM:} an integrated computing platform with 6 NVIDIA A100 GPUs and HBM-PIM devices. HBM-PIM~\cite{lee2021hardware} is a commercial PIM device produced by Samsung, featuring one FPU unit per 2 DRAM banks (i.e., 1P2B configuration); (c) \emph{AttAcc-only:} a PIM-only computing platform with AttAcc PIM-based units~\cite{park2024attacc}, in which all computations of FC and attention kernels are executed on PIM units.
In the PAPI design, the capacity of the FC-PIM devices is 12 GB, while all other HBM devices, including Attn-PIM devices in PAPI, have a capacity of 16 GB. Therefore, one GPU Memory in PAPI is 60 GB rather than 80 GB in the A100 GPU, necessitating six GPUs to accommodate the model parameters of GPT-3 175B (requiring 350 GB memory).
For a fair comparison, each of the computing systems has 90 HBM devices, 30 for storing the weight parameters of FC kernels and 60 for attention kernels. Each GPU contains 5 HBM devices connected via NVLink~\cite{NVlink}, corresponding to the 80GB GPU memory of the A100 GPU.
All HBMs used in the experiments are HBM3~\cite{JEDEC} with 5.2Gbps per pin and running at 333MHz.
We developed a simulator based on Ramulator2~\cite{luo2023ramulator} (new version of Ramulator~\cite{kim2015ramulator}) and AttAcc~\cite{park2024attacc} to evaluate the performance and energy efficiency of the PAPI computing platform, including both GPU and PIM-based components. 

\noindent\textbf{Workloads.} We evaluate three transformer-based LLMs, LLaMA-65B~\cite{touvron2023llama}, GPT-3 66B~\cite{brown2020language}, and GPT-3 175B~\cite{brown2020language}, using the FP16 data type. We use creative-writing and general-qa tasks in the Dolly dataset~\cite{DatabricksBlog2023DollyV2}. The Dolly dataset is an open-source dataset of instruction-following records generated by thousands of Databricks employees in several behavioral categories outlined in InstructGPT~\cite{ouyang2022training}. We use static batching with varying initial request-level parallelism (batch size) across experiments. By evaluating our proposed design on real-world datasets, we can test the performance and energy consumption with various input and output sequence lengths while adapting to dynamic parallelization levels observed at runtime.


\begin{figure*}[bp]

\centering
\includegraphics[width=2.1\columnwidth]{Exp/exp-time2.pdf}
\caption{End-to-end speedup (top) and energy efficiency (bottom) comparisons of four evaluated designs on the Dolly creative-writing dataset. Values are normalized to A100+AttAcc.}
\label{exp:cw-dataset}

\end{figure*}

\subsection{End-to-End Performance and Energy Efficiency}
\label{sec:7.2}

\noindent\textbf{Performance Speedup.} Figure~\ref{exp:cw-dataset}(a) shows the end-to-end performance of all four evaluated designs using various parallelization levels in the decoding step of each model with batch sizes of 4, 16, or 64 and speculation lengths of 1, 2, or 4. The results are normalized to the A100+AttAcc baseline.

We make three observations. First, PAPI achieves speedups of 1.8×, 1.9×, and 11.1× over A100+AttAcc, A100+HBM-PIM, and AttAcc-only designs, respectively. This is because PAPI schedules tasks between GPU and PIM \emph{dynamically}, offloading each task to the corresponding best-fit computing hardware at a given point in time, and the proposed hybrid PIM architecture can provide varying levels of execution parallelism, catering to the different needs of the FC and attention kernels.
Second, the AttAcc-only scheme performs worse than PAPI and also worse than A100+AttAcc at most parallelization settings. This is due to two reasons. (1) AttAcc-only has limited computation throughput as it employs solely PIM units. Later (Section \ref{sec:7.4}), we compare the performance of PIM solutions with different parallel computation capabilities. 
(2) FC kernels with the parallelism settings used in our experiments are more computation-intensive, making them unable to benefit from a PIM-only solution (which is a better fit for memory-intensive kernels).
Third, A100+AttAcc performs similarly to A100+HBM-PIM because the only difference between them is the execution of the attention kernel on either AttAcc or HBM-PIM. However, the attention kernel's execution time on PIM is relatively small compared to the overall runtime, resulting in a small performance difference.


Figure~\ref{exp:qa-dataset}(a) illustrates the end-to-end latency of three designs on the Dolly general-qa dataset. PAPI achieves speedups of 1.7×, 1.7×, and 8.1× over A100+AttAcc, A100+HBM-PIM and AttAcc-only, respectively, which is lower than the speedup for the Dolly creative-writing dataset. This is due to two reasons:
(i) The creative-writing dataset typically has longer output lengths, which makes the decoding phase a larger bottleneck for end-to-end performance, thereby making PAPI acceleration more beneficial.
(ii) Longer output lengths of the creative-writing dataset lead to more significant dynamic changes in parallelization levels, thereby allowing PAPI to further improve performance over prior schemes.

We conclude that PAPI provides significant performance benefits in LLM inference over state-of-the-art PIM-based designs across various real-world configuration settings (speculation length, batch size) and using different real datasets.


\noindent\textbf{Energy Efficiency.} Figures~\ref{exp:cw-dataset}(b) and ~\ref{exp:qa-dataset} (b) present the end-to-end energy efficiency, normalized to A100+AttAcc system, for the creative-writing and general-qa datasets. PAPI improves average energy efficiency by 3.4× and 3.1× for these datasets, respectively, over A100+AttAcc. 
This is because A100+AttAcc executes the FC kernels on energy-hungry A100 GPUs, while PAPI offloads parts of these kernels to FC-PIM devices, thereby consuming less energy by mitigating data movement and exploiting low-power processing cores in memory. 
Compared to AttAcc-only, PAPI provides 1.15× and 1.01× energy efficiency improvement in creative-writing and general-qa datasets, respectively, which is lower than PAPI benefits over A100+AttAcc.
This is because PAPI dynamically schedules the FC kernels on the energy-hungry GPU cores and the energy-efficient PIM cores. While GPU execution consumes more energy than AttAcc-only, PAPI lowers energy consumption on PIM through DRAM data access reuse, resulting in modest savings over AttAcc-only.

We conclude that PAPI improves energy efficiency over state-of-the-art PIM systems across different real configuration settings and datasets.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Exp/exp-qa.pdf}
\caption{End-to-end speedup (a) and energy efficiency (b) comparisons of three evaluated designs on the Dolly general-qa dataset for GPT-3 175B.}
\label{exp:qa-dataset}

\end{figure}

\subsection{Sensitivity to Parallelization Levels}
We analyze the performance of three evaluated designs across different RLP and TLP values, using the LLaMA-65B model on the creative-writing dataset.

\noindent\textbf{RLP.} Figure~\ref{exp:batch}(a) presents the performance of three designs when we vary the batch size from 4 to 128 to explore the effect of RLP, using a fixed speculation length of 1. When RLP is relatively low, e.g., with a batch size of 4, AttAcc-only provides higher performance than A100+AttAcc. As RLP increases, the execution time of AttAcc-only increases significantly because the PIM devices cannot effectively cater to the large computation needs of the FC kernels, which leads to AttAcc-only providing much worse performance than A100+AttAcc. PAPI achieves the best performance for all RLP settings over state-of-the-art PIM-based systems.


\noindent\textbf{TLP.} Figure~\ref{exp:batch}(b) shows the performance of three designs when we vary the speculation length from 1 to 8 to analyze various TLP levels, using a fixed batch size of 4.
Compared to A100+AttAcc and AttAcc-only, PAPI achieves 1.5× and 3.0× speedup on average. The speedup of PAPI over A100+AttAcc decreases as TLP increases, because PAPI offloads more FC kernels to the GPUs as TLP increases. If TLP becomes large enough, we expect that PAPI would assign all the FC kernels to the GPU and thus the performance of PAPI to converge to that of A100+AttAcc.


\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{Exp/exp-batch.pdf}
\caption{End-to-end speedup with (a) different batch sizes (speculation length=1), and (b) different speculation lengths (batch size=4); for LLaMA-65B.}
\label{exp:batch}
\end{figure}

\subsection{Performance Analysis of PAPI}
\label{sec:7.4}
To analyze the benefits of our proposed hybrid PIM design in PAPI, we compare the performance of two PIM-only architectures: (i) AttAcc-only and (ii) our proposed PIM architecture with only Attn-PIM and FC-PIM devices (but without the A100), using the same number of PIM devices and interconnect settings for fairness.
We only evaluate the decoding phase (since the prefilling phase is compute-bound and is to be executed on the GPU platform).
Figure~\ref{exp:pim} shows the speedup of our PIM-only PAPI design compared to AttAcc-only using the Dolly creative-writing dataset. Our PIM design achieves 2.3× speedup improvement against AttAcc-only on average. We observe that our PIM design has a higher speedup at higher parallelization levels: e.g., when the batch size is 4 and the speculation length is 1, the speedup is 1.6×, while when the batch size is 64 and the speculation length is 4 (higher parallelism) the speedup of PIM-only PAPI increases to 2.7×. This is because, as parallelism increases, FC kernels become more computation-intensive, requiring more computation power. PAPI with more processing units (PUs) can provide the required computation capability much more so than AttAcc-only. Additionally, FC kernels are responsible for most of the execution time, so improving their performance has the largest impact on overall speedup.


\begin{figure}[h]
\centering
\includegraphics[width=0.95\columnwidth]{Exp/exp-pim2.pdf}
\caption{Performance speedup of PIM-only PAPI over AttAcc-only in the decoding phase for the Dolly creative-writing dataset.}
\label{exp:pim}
\end{figure}

Figure~\ref{exp:breakdown} presents the execution time breakdown per token for the AttAcc-only system and for the PIM-only PAPI system with Attn-PIM and FC-PIM devices. We make four key observations.
First, FC kernels dominate the total execution time.
Therefore, it is valuable to enable higher execution parallelism in PIM hardware (as PAPI does with FC-PIM) to effectively cater to the high computation demands of the FC kernels. 
Second, the PIM-only PAPI design provides 2.9× speedup when processing FC kernels.
Third, attention kernels run 1.7× slower on Attn-PIM (1P2B) than AttAcc-only (1P1B) due to our design choice that reduces FPU area overheads.
Fourth, communication takes up 28.2\% of the total execution time in the decoding stage; thus, more advanced network technologies could be developed and integrated into the PAPI architecture to further improve performance. 

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{Exp/fc-new.pdf}
\caption{Execution time breakdown per token in the decoding phase of LLaMA-65B model inference (batch size=4, speculation length=4) for AttAcc-only versus PIM-only PAPI.}
\label{exp:breakdown}

\end{figure}
