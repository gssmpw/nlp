\section{Motivation}
\label{sec:motiv}





\subsection{Analysis of LLM Inference}



We analyze the computation and memory requirements of LLM inference by evaluating the arithmetic intensity of fully-connected (FC) and multi-head attention kernels. We vary the batch size, when batching is enabled, and speculation length, when speculative decoding is enabled. Figure~\ref{fig:roofline-model}(a) shows the roofline model for the OPT-30B model~\cite{brown2020language} using a high-end NVIDIA A100 GPU~\cite{choquette2020nvidia} with 312 TFLOPS peak computation performance and 1935 GB/s peak memory bandwidth, for FC and attention kernels, as the batch size increases from 4 to 128, with a speculation length of 8. We make two key observations.
First, when the batch size is small, i.e., 4, 8, 16, the decoding phase is memory-bound, i.e., both the attention and FC kernels are bottlenecked by memory bandwidth.
Second, when the batch size is $\geq$ 32, the FC kernel becomes compute-bound, while the attention kernel is memory-bound.
The arithmetic intensity of the FC kernel increases with the batch size. However, the arithmetic intensity of the attention kernel does \emph{not} change when the batch size increases, because there is no data reuse in batching for the attention kernel.


\begin{figure}[b]

\centering
\includegraphics[width=\columnwidth]{Figure/roofline_model.pdf}
\caption{Roofline model using OPT-30B with (a) different batch sizes (speculation length = 8) and (b) different speculation lengths (batch size = 32). The darker the color of the dots, the higher the degree of parallelism.}
\label{fig:roofline-model}

\end{figure}




Figure~\ref{fig:roofline-model}(b) shows the roofline analysis of FC and attention kernels when we vary the speculation length from 2 to 8, with a batch size of 32. 
We observe that the arithmetic intensity of the attention and FC kernels increases with the speculation length. With a batch size of 32, the FC kernel becomes compute-bound when the speculation length exceeds 6. In contrast, although the arithmetic intensity of the attention kernel increases with speculation length, the attention kernel remains memory-bound. This is because the arithmetic intensity of the attention kernel increases only slightly with speculation length, and batching has no effect on it, as batching primarily improves weight data reuse, which does not affect the attention kernel.



\subsection{Varying Parallelization Levels in LLM Inference}
\label{sec:3.2}


In real-world LLM tasks, both batch size and speculation length vary significantly at runtime due to changes in user requests and potential adjustments to speculation length to optimize performance~\cite{mamou2024accelerating}. 
We elaborate on why the parallelism in LLM inference dynamically changes during runtime in real-world scenarios.

\noindent\textbf{Initial Request-Level Parallelism (Initial RLP):}
Initial RLP refers to the RLP when batched execution begins. Initial RLP can vary significantly in real-world LLM serving scenarios, causing the batch size to vary greatly. This is due to three major reasons.


\noindent(a) \emph{Service Level Objective (SLO) Limits}: Increasing RLP can enhance throughput but increases inference latency per request~\cite{li2023alpaserve}. Under the online serving scenario, different user latency SLOs dictate varying maximum batch sizes. For example, while a DGX A100 computing system~\cite{DGX} with 1,280 GB memory can support up to 854 requests per batch, a 30 ms SLO requires setting the initial RLP to be as low as 22~\cite{park2024attacc}.


\noindent(b) \emph{Memory Capacity Limits}: Initial RLP is also constrained by the system's memory capacity, particularly for KV cache storage. A computing system with 640 GB HBM can house 282 requests with input and output lengths of 128, but only 18 requests with input and output lengths of~2048~\cite{park2024attacc}. In the latter case, the batch size needs to be smaller, as longer sequences need more memory capacity for KV cache for multi-head attention.

\noindent(c) \emph{Dynamic Batching}: Dynamic batching~\cite{Triton} starts processing a batch once the batch is full or exceeds a time limit. Therefore, when requests are infrequent, an LLM serving system with dynamic batching may start processing with different batch sizes, and thus, different RLP values.

\noindent\textbf{Runtime Request-Level Parallelism (Runtime RLP):}
Runtime RLP refers to the RLP during the execution of a batch of requests. Runtime RLP depends on the batching mechanism used, which may be \emph{static} batching or \emph{mixed continuous} batching \cite{patel2024splitwise}. 

Traditional LLM serving systems \cite{Tensorflow, Triton} use static batching with batch-level scheduling. In this approach, no new requests are processed until all requests from the current batch have finished. Since each request has a unique output length, runtime RLP dynamically varies. As shown in Figure~\ref{fig:dy}, runtime RLP dynamically decreases as each request of the current batch finishes (i.e., as more decoding iterations take place)~\cite{oh2024exegpt}. 



Mixed continuous batching \cite{agrawal2023sarathi, patel2024splitwise} allows token-level scheduling, where new requests can be added to be processed by the LLM serving system, while the system is executing requests in the current batch. In this case, runtime RLP dynamically changes to keep the hardware resource utilization as high as possible, and it is dependent on when and how many requests are added to each batch.

\begin{figure}[H]

\centering
\includegraphics[width=0.7\columnwidth]{Figure/decoding-memory-bound2.pdf}
\caption{Decoding iterations required for each request in a batch, illustrating how the number of remaining parallel requests changes as decoding iterations increase.}
\label{fig:dy}

\end{figure}

\noindent\textbf{Token-Level Parallelism (TLP):}
TLP can also be dynamically adjusted at runtime to enhance speculative decoding performance in LLMs~\cite{mamou2024accelerating, su2023synergy}. For instance, a prior work \cite{mamou2024accelerating} introduces a dynamic speculation length optimization technique that adjusts speculation length during each decoding iteration. Additionally, batching and speculative decoding need to be synergistically co-optimized to improve GPU utilization for LLM inference \cite{su2023synergy}: e.g., when the batch size is small, the speculation length can be increased to maximize resource utilization.

We conclude that in real-world LLM serving scenarios, batch size and speculation length substantially vary during runtime.
As a result, the arithmetic intensity of FC and attention kernels dynamically change due to the varying parallelization levels. Therefore, FC and attention kernels can become either compute-bound or memory-bound kernels, when executed in computation-centric systems such as GPUs. We draw two key insights from our analysis.

\noindent\textbf{Key Insight 1.} LLM serving requires a heterogeneous computing architecture with advanced computing units that offer varying computation throughput and memory bandwidth capabilities to satisfy the different arithmetic intensities of kernels. 

\noindent\textbf{Key Insight 2.} LLM serving requires a \emph{dynamic} scheduling approach to map FC kernels to different computing units because FC kernels can switch between being compute-bound or memory-bound during runtime.



\subsection{Limitations of Existing Processing-In-Memory Architectures for LLM Inference}


Computing units, such as GPUs and neural processing units (NPUs), are widely used for LLM serving systems. 
Recent works (e.g., \cite{park2024attacc,seo2024ianus, heo2024neupims,li2024specpim, pan2024instinfer, ortega2024pim, kwon2024lol, gao2024imi, lee2024cost, jeong2024pipepim, zhou2022transpim}) explore the Processing-In-Memory (PIM) computing paradigm (e.g., \cite{aga2017compute, ahn2015scalable, ferreira2021pluto, li2017drisa, seshadri2017ambit, seshadri2015fast, he2021tare, he2020towards, he2019agile}) in LLM inference to alleviate the data movement bottleneck in memory-bound kernels of LLMs, such as the attention kernel.
By integrating processing cores within memory units, PIM provides high memory bandwidth, mitigating data movement bottlenecks in kernels with low arithmetic intensity.


Some prior works~\cite{park2024attacc, seo2024ianus, heo2024neupims, li2024specpim, pan2024instinfer} propose PIM-enabled heterogeneous architectures for LLMs. These architectures include both high-performance computation-centric processors (e.g., GPUs) and PIM devices with very high memory access bandwidth. These works run LLM kernels in computation-centric processors or memory-centric PIM devices and demonstrate significant performance benefits compared to commodity systems, e.g., using only computation-centric accelerators (e.g., GPUs) to run end-to-end LLM inference. However, these prior works still suffer from two major shortcomings.

\noindent\textbf{Shortcoming 1.} \textit{Prior works statically assign FC and attention kernels either to a computation-centric processor (GPU) or a PIM-enabled computing device.} Our analysis shows that \emph{dynamic} assignment of kernels to different computing devices is necessary because LLM kernels have varying arithmetic intensity, e.g., FC kernel can be either compute-bound or memory-bound in GPUs, depending on the speculation length and batch size that are currently used. Specifically, AttAcc \cite{park2024attacc} always offloads all attention kernels to the proposed PIM devices and all FC kernels to a GPU. 
IANUS \cite{seo2024ianus} statically maps all FC kernels to PIM and attention kernels to NPU.
SpecPIM \cite{li2024specpim} proposes an allocation scheme that executes attention and FC kernels at the high-performance processor and PIM devices \textit{concurrently}. However, it is only designed for a static batch size and speculation length.
We conclude that these prior works do \emph{not} sufficiently meet the varying computation and memory demands of real-world LLM serving scenarios. They propose \emph{static} designs, where FC and attention kernels each are always mapped to the same computing hardware; even though kernels exhibit varying computation and memory demands at runtime.



To quantitatively demonstrate the limitations of prior PIM-based proposals for LLM inference, we evaluate the execution time (latency) of one FC kernel using an NVIDIA A100 GPU \cite{choquette2020nvidia}, Samsung's HBM-PIM architecture \cite{lee2021hardware}, and the state-of-the-art PIM-based work for LLMs, AttAcc \cite{park2024attacc} (Section 7 provides more detail on our evaluation methodology). 
Figure~\ref{fig:different-suitable-platforms} shows the FC kernel latency (normalized to A100 GPU) when we vary the batch size and speculation length.
We observe that in the configurations with low parallelization levels, e.g., having a batch size of 1 and speculation length of 8 or having a batch size of 4 and speculation length of 2, PIM-based architectures, i.e., HBM-PIM and AttAcc, provide better performance than the A100 GPU. In contrast, in the configurations with high parallelization levels, e.g., batch size of 16 or larger, the A100 GPU significantly outperforms the PIM-based architectures, providing much lower execution time.
However, RLP and TLP are \emph{not} known in advance (statically): they dynamically vary and it is hard to predict how they would change. This observation necessitates \emph{dynamic} decisions of which computing hardware to use to execute the FC kernel.

\begin{figure}[H]

\centering
\includegraphics[width=\columnwidth]{Figure/different-suitable-platform-2.pdf}
\caption{The normalized latency of the FC kernel in LLM inference with different parallelization levels (different batch sizes and speculation lengths).}
\label{fig:different-suitable-platforms}

\end{figure}

\noindent\textbf{Shortcoming 2.} \textit{Prior works support only one type of PIM-enabled computing device with a certain computation throughput and memory bandwidth capability.} Our analysis shows that although FC and attention kernels can be both memory-bound kernels in GPUs, necessitating PIM-enabled solutions, they have very different arithmetic intensities and different computation and memory bandwidth needs. For example, as demonstrated in Figure~\ref{fig:roofline-model}, with a batch size of 4 and speculation length of 8, the arithmetic intensity of FC is 31.7 FLOPs/Byte, while that of attention is 7.0 FLOPs/Byte. Thus, assuming computing hardware of a certain computation throughput, attention would need around 4.5Ã— higher memory bandwidth than FC. This shows that PIM-enabled computing devices need to provide different computation and memory bandwidth capabilities to efficiently execute the two different types of LLM kernels.
Our work is the first to identify this property of the two types of LLM kernels, while prior works use PIM-enabled devices with a fixed computation and memory bandwidth capability, which make them inefficient at meeting the different and dynamically varying needs of attention and FC kernels. 


\begin{figure*}[bp]

\centering
\includegraphics[width=1.82\columnwidth]{Figure/heter_arch2.pdf}
\caption{Overview of the PAPI computing system, and an example of its dynamic parallelism-aware scheduler.}
\label{fig:heter_arch}

\end{figure*} 

\subsection{Our Goal}
Our goal is to design a versatile computing platform that caters to the varying parallelization levels in real-world LLM inference with different and dynamically changing computation and memory demands. To this end, we propose (1) a heterogeneous architecture that integrates memory-centric PIM units and computation-centric GPU and host CPU, each offering distinct computation throughput and memory bandwidth characteristics, and (2) a parallelism-aware scheduling technique that adapts to runtime variations in parallelization and intelligently and dynamically assigns FC and attention kernels to the most appropriate hardware units in our platform.



