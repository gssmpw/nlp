\section{Conclusion}

Real-world LLM services with state-of-the-art parallelism optimization techniques, such as batching and speculation decoding, lead to dynamically-changing parallelization levels. As a result, fully-connected and attention kernels in LLM inference exhibit varying computation and memory demands. To seamlessly adapt to such dynamic demands, we propose PAPI, a computing system that supports three types of computing units with different computation and memory bandwidth capabilities, and a lightweight scheduling framework that offloads fully-connected and attention kernels to the most suitable computing units by monitoring the dynamic parallelization levels in LLM inference at low cost. 
Our evaluation shows that PAPI provides 1.8× and 11.1× performance improvement over state-of-the-art LLM inference systems.
We hope that our work enables further research on leveraging heterogeneous PIM-enabled systems to cater to dynamic real-world execution scenarios in emerging machine learning models such as LLMs.