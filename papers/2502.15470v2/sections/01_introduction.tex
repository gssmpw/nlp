\section{Introduction}
Large language models (LLMs) have achieved remarkable success across a wide range of applications, excelling not only in natural language processing tasks such as code generation \cite{chen2021evaluating, copilot}, question answering \cite{radford2019language, chowdhery2023palm}, but also image \cite{radford2021, dalle, rombach2022high} and video processing \cite{openai2024sora}. Efficient LLM inference is crucial to unlocking the full potential of these models. 
LLM inference consists of two phases: prefill and decoding~\cite{zhou2024survey}. In the prefill phase, the LLM model processes all input tokens in a request to create hidden states for the decoding phase and generate the first output token. Subsequently, during the conventional decoding phase, the model generates an output token per decoding iteration.


Low execution time of LLM inference is crucial for both user experience in real-time applications and hardware utilization efficiency~\cite{Shen2023Efficient}. The LLM decoding phase dominates the execution time in the LLM inference tasks~\cite{hong2022dfx, zhou2022transpim}. For instance, the serial decoding of the GPT-3 175B model is responsible for 96\% of the overall execution time when the input and output lengths are 32 \cite{choi2023unleashing}. The impact of LLM decoding on overall execution time increases as the output length grows, which is essential for generating more detailed and comprehensive LLM responses~\cite{bai2024longwriter}.


To improve the performance of the decoding phase, prior works employ two main parallelism techniques: batching \cite{yu2022orca, agrawal2023sarathi, patel2024splitwise} and speculative decoding \cite{leviathan2023fast, chen2023accelerating, spector2023accelerating, zhang2023draft}. These techniques enable the generation of multiple tokens, known as \textit{parallel decoding}, in one decoding iteration, to accelerate decoding.
We define the number of decoding tokens that are simultaneously generated as \textit{decoding parallelism}. 
Decoding parallelism directly affects the utilization of memory and computation resources.
As a result, some kernels in decoding become compute-bound, while others become memory-bound.
Recent works explore processing-in-memory (PIM) enabled hybrid designs (i.e., heterogeneous architectures using both PIM units and computation-centric accelerators, like GPUs)~\cite{heo2024neupims, park2024attacc, li2024specpim, seo2024ianus, pan2024instinfer} to accelerate the LLM inference process by mapping compute-bound and memory-bound kernels to computation-centric and memory-centric accelerators. These works \emph{statically} characterize LLM decoding kernels and, based on statically-identified characteristics, schedule different types of kernels to different computation units (e.g., PIM units and computation-centric accelerators like GPUs). 

To study the effectiveness of static scheduling, we profile the kernels used in the decoding phase that employs state-of-the-art parallelism techniques. We observe that some kernels shift from being compute-bound to memory-bound (or vice versa) \emph{dynamically} in response to variations in decoding parallelism. This is because decoding parallelism changes dynamically at runtime. 
There are three main reasons for these dynamic changes in decoding parallelism. 
First, the maximum decoding parallelism in a computing system is limited by the memory requirement of requests, which is dependent on request output lengths and cannot be predicted prior to execution. 
Second, the maximum decoding parallelism is also affected by different user requirements, like quality of service (QoS) \cite{agrawal2024taming}. 
Third, some parallelism techniques \cite{patel2024splitwise, mamou2024accelerating} employ dynamic optimization approaches that adjust the configuration of decoding parallelism (i.e., batch size and speculation length) at runtime to enhance system performance (see Section~\ref{sec:motiv}). 
We conclude that dynamic changes in decoding parallelism cause heterogeneous designs with a \emph{static} scheduling scheme to become suboptimal, as they can mistakenly schedule computation-intensive kernels to PIM units or memory-intensive kernels to computation-centric accelerators (e.g., GPUs).



In this paper, we aim to accelerate parallel decoding by fully leveraging the dynamically changing parallelism properties in LLM inference tasks. To this end, we propose PAPI (\textbf{PA}rallel Decoding with \textbf{PI}M), a PIM-enabled heterogeneous architecture that exploits dynamic scheduling of compute-bound or memory-bound kernels to the most suitable hardware unit for each kernel type.
PAPI's key idea is to enable \emph{online} dynamic task scheduling on a heterogeneous architecture (consisting of GPUs and PIM units) via online identification of kernel properties in LLM decoding. 

PAPI is equipped with three key techniques.
First, we propose \emph{a dynamic parallelism-aware task scheduling framework} to assign kernels to suitable computing platforms at runtime. This approach employs a simple yet effective \emph{kernel bottleneck predictor} with low hardware overhead. 
Second, we design \emph{a heterogeneous architecture with PIM units, GPU, and host CPU} to meet different computing and memory demands of different kernels. 
Third, we design a hybrid PIM architecture that includes two different types of PIM units, i.e., \emph{performance-optimized and memory-capacity-optimized PIM units}, which cater to memory-intensive kernels with different computational demands and memory footprints.


We compare PAPI with the state-of-the-art heterogeneous LLM accelerator composed of AttAcc~\cite{park2024attacc} and 6 NVIDIA A100 GPUs~\cite{choquette2020nvidia} (A100+AttAcc), a heterogeneous architecture composed of Samsung's HBM-PIM \cite{lee2021hardware} and the NVIDIA A100 GPUs (A100+HBM-PIM) and a PIM-only LLM accelerator, AttAcc~\cite{park2024attacc}. 
Our experimental results show that PAPI outperforms A100+AttAcc, A100+HBM-PIM, and AttAcc by 1.8$\times$, 1.9$\times$, and 11.1$\times$, respectively.

This paper makes the following contributions.
\begin{itemize}[leftmargin=4mm,itemsep=0mm,parsep=0mm,topsep=0mm]
\item We observe that parallelism in LLM decoding changes \emph{dynamically}, leading to varying demands in both computation capability and memory bandwidth.
\item We propose PAPI, a PIM-enabled heterogeneous computing system, to meet the different computation and memory bandwidth demands by incorporating both memory-centric PIM units and computation-centric GPU and host CPU.
\item We propose an online parallelism-aware scheduling technique that maps dynamically LLM decoding kernels with different and changing properties to the most appropriate hardware units, including hybrid PIM units.
\item We evaluate PAPI and demonstrate that it provides significant performance and energy benefits over state-of-the-art computing systems for LLM inference.
\end{itemize}
