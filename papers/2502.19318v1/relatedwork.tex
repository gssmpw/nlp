\section{Related Work}
\begin{comment}
Volume rendering is concerned about how light scatters in participating media, e.g. clouds, smoke, but also wax and skin / flesh.
Such media can absorb, create, and scatter photons.
This is described by the volumetric rendering integral, and volumetric path tracers are capable of computing the solution to such integrals.
\reftodo{some vol pt refs}
Often, scattering can be discarded as the full gamut of effects is not necessary.
This results in the somewhat simplified Equation \ref{eq:volumetric_integration}.
\reftodo{simplified, used in vis}

NERFs take Equation \ref{eq:volumetric_integration}, a differentiable rendering algorithm based on ray marching, a data model based on multi-layer perceptrons (MLPs), and learn their weights by means of backpropagation through the ray marcher and gradient decent. \reftodo{..}

EWA splatting, introduced by \reftodo{zwicker et. al} is a method to compute an approximation of Equation \ref{eq:volumetric_integration}.
To do so, the volumetric data is stored in Gaussian mixtures, which are then sorted and projected to the screen.

3D Gaussian Splatting for Real-Time Radiance Field Rendering (3DGS) basically took the EWA splatting algorithm, simplified it a bit more (by using the opacity formulation), made it even faster, and made it possible to learn Gaussian mixtures by backpropagation and gradient decent.

Stop the pop (STP) is an improvement on 3DGS, where they implemented a per-pixel sorting step and hierarchical culling to make it tracktable \todo{is that correct? extend as i know really little about it}.
\end{comment}
Volume rendering equations and theory~\cite{Max95, Novak18Course} were developed to explain and model how light scatters in participating media, e.g. clouds, smoke, but also wax and skin/flesh. Such media can emit, absorb, and scatter photons. This is described by the volumetric rendering integral, and volumetric path tracers are capable of computing the solution to such integrals.
This theory proved useful for modern image-based 3D reconstruction methods. 

In the context of novel-view synthesis, NeRF~\cite{mildenhall2020nerf} uses a simplified volume rendering model that only considers emission and absorption, ignoring scattering. %to a simple emission-absorption volume by ignoring scattering.
For every position, density and directional outgoing radiance are stored in an MLP. This allows recovering
a 3D volumetric scene representation for a set of unstructured images using Stochastic Gradient Descent (SGD). For every iteration of SGD, NeRF renders an image via ray marching, sampling an MLP along the ray, and computing the volumetric integral using quadrature. 
Since this process is differentiable, the weights of the MLP receive gradients, continuously pushing image formation closer to the ground truth. 
% This forward rendering model is differentiable, which allows %\GDmodif{the computation of}
% \AC{computing } % sounds more direct to adam, and, if changing to 'the computation of', then 'upodte the weights' should be also changed, and i wouldn't even know the correct grammar there
% the gradients and updating the weights of the MLP such that the rendered image is closer to the ground-truth images. 
%Doing this, for enough iterations and enough images, allows the optimization of a 3D representation that allows faithful rendering of the 3D scene from any novel view.

Ray marching paired with quadrature provides an accurate solution of the volume rendering integral, but historically, rasterization is faster, albeit less accurate. Elliptical Weighted Average (EWA) splatting~\cite{ewa_splatting} is an efficient algorithm for rendering volumes through rasterization. It is based on Heckbert's thesis~\cite{heckbert1989fundamentals} and traditional signal processing. EWA volume splatting considers discrete samples of density and color and reconstructs the underlying continuous signal in screen space. In practice, the volume is represented with Gaussian primitives that are splatted and alpha-blended on screen. To allow for fast rasterization, a series of assumptions and simplifications are made: 1) Gaussian primitives do not overlap in 3D, 2) self-attenuation---i.e., the reduction of light intensity along a ray inside a primitive---is ignored, 3) the perspective projection is linearized and approximated by a first-order Taylor expansion, and 4) $e^{-x}$ is approximated by the faster $1-x$.
%Note also that 3DGS builds on a wealth of point-based rendering literature~\cite{lassner2021pulsar,wiles2020synsin,zhang2022differentiable,kopanas2021point, Yifan:DSS:2019}.
%\GK{Let's move this GD modification further up where we talk about EWA and maybe add some traditional point based graphics papers?}. GD: Done

3D Gaussian Splatting (3DGS)~\cite{kerbl3Dgaussians} is a fast and high-quality alternative for NeRF.
3DGS builds on EWA Volume Splatting and several other works on point-based rendering~\cite{lassner2021pulsar,wiles2020synsin,zhang2022differentiable,kopanas2021point, Yifan:DSS:2019}, while simplifying the formulation even further.
% These simplifications proved significantly important for efficient back-propagation through SGD. # adam doesn't think so. it wasn't that hard to implement the gradient for self attenuation and it's efficient (computationally)
Most notably, 3DGS significantly simplifies the handling of transparency (respectively its inverse, opacity). \revision{Instead of computing transparency by integrating extinction along a finite segment of a ray according to the volume rendering equation, 3DGS just assigns one single opacity value per primitive. This value is not related to an integrated extinction, as we will show in more detail in Sec.~\ref{sec:anal_opa_vs_dens}. }
%Most notably, 3DGS replaces extinction in the formulation of EWA with opacity:
%Instead of integrating extinction along a ray to compute opacity -- as shown in Sec.~\ref{sec:anal_opa_vs_dens} -- 3DGS models opacity as a global property of the primitives.
The 3DGS opacity value therefore has no \revision{direct equivalent in volume rendering theory}, but it allows the splatted primitives to have a constant transparency, no matter the viewing direction, and allows for optimizing a well-behaved and bounded scalar.
The subtle difference between opacity and extinction is often overlooked in subsequent work, or the quantities are even conflated~\cite{guedon2023sugar,bolanos2024gaussian,lee2024gscore,huang2023photo}.
In this paper, we will analyze the differences and the impact of these approximations and seek to find their importance in terms of final quality for forward rendering, as well as the full optimization process.

A wealth of literature has appeared following up on the original 3DGS paper:
In line with our own work, Mip-Splatting~\cite{yu2023mip} identifies an approximation for convolution used in 3DGS and introduces a principled pre-filtering of the underlying signal to provide a correct solution for antialiasing, allowing consistent multi-scale and multi-resolution training.
Concurrent work has explored various methods to leverage benefits of ray-Gaussian intersection.
These approaches typically involve creating shells around Gaussian functions and employing conventional ray-tracing techniques to query the Gaussians.
Condor et al.\ and Zhou et al.\ concentrate on principled volumetric path tracing; however, the first does not demonstrate competitive reconstruction quality \cite{condor2024dontsplatgaussiansvolumetric}, while the latter addresses geometry reconstruction rather than novel-view synthesis\cite{zhou2024unified}.
Moenne-Loccoz et al. implement a range of ray-tracing effects, including motion blur and refraction, and they utilize ray tracing for scene composition \cite{3dgrt2024}.
Their method identifies the point of highest density, but does not incorporate volumetric marching.
Their reconstruction performance is comparable to that of 3DGS.
Finally, Blanc et al.\ use a ray-marching method that traverses the scene with a fixed step size.
However, rather than enabling direct comparison and analysis, their work focuses on improving 3DGS reconstruction quality, which they achieve by complementing the view-dependent representation with spherical Gaussians \cite{blanc2024raygaussvolumetricgaussianbasedray}.
%Notably, none of the existing studies provide a comprehensive analysis of the image formation model.
% \cite{zhou2024unifiedgaussianprimitivesscene} #Unified Gaussian Primitives for Scene Representation and Rendering: I don't think this one should be included just because it uses Gaussians. They implemented a full blown path tracer on CPU and don't deal in learning a model from photos. it's very cool, but a different direction.
For further analysis of the research that succeeds 3DGS, we redirect the readers to the following surveys\cite{chen2024survey,fei20243d,wu2024recent,dalal2024gaussian}.

\revision{