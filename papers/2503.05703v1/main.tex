
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{natbib}
\usepackage[utf8]{inputenc}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{comment}

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{array}
%\usepackage[table]{xcolor} % The 'table' option loads 'colortbl'
\usepackage{ragged2e}

\usepackage{booktabs}


\usepackage{makecell}

\usepackage{listings}
\usepackage{multirow}

\usepackage[inline]{enumitem}


\title{What I cannot execute, I do not understand: Training and Evaluating LLMs on Program Execution Traces}





\author{Jordi Armengol-Estap√©$^1$\thanks{Work done while interning at FAIR, Meta AI. Contact: \texttt{jordi.armengol.estape@ed.ac.uk}}\ \ , Quentin Carbonneaux$^2$, Tianjun Zhang$^2$, \\ \textbf{Aram H. Markosyan$^2$, Volker Seeker$^2$, Chris Cummins$^2$, Melanie Kambadur$^2$,} \\
\textbf{Michael F.P. O'Boyle$^1$, Sida Wang$^2$, Gabriel Synnaeve$^2$, Hugh Leather$^2$} \\
$^1$University of Edinburgh  \quad
$^2$Meta AI
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}




\newcommand{\name}{E.T.}

\newcommand{\fullname}{Execution Tuning} 

\iclrfinalcopy 
\begin{document}



\maketitle

\begin{abstract}
Code generation and understanding are critical capabilities for large language models (LLMs). Thus, most LLMs are pretrained and fine-tuned on code data. However, these datasets typically treat code as static strings and rarely exploit the dynamic information about their execution. Building upon previous work on trace modeling,  we study \fullname{} (\name{}), a training procedure in which we explicitly model real-world program execution traces without requiring manual test annotations. 
We train and evaluate models on different execution trace granularities (line and instruction-level) and strategies on the task of output prediction, obtaining ${\sim}80\%$ accuracy on CruxEval and MBPP, and showing the advantages of \textit{dynamic scratchpads} (i.e., self-contained intermediate computations updated by the model rather than accumulated as a history of past computations)  on long executions (up to 14k steps). Finally, we discuss \name{}'s practical applications.
\end{abstract}





\input{1_intro}
\input{2_traces}
\input{3_intrinsic}
\input{4_extrinsic}
\input{5_related}
\input{6_conclusion}


%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors.
%
%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.

\clearpage

\bibliography{main}
\bibliographystyle{iclr2025_conference}

\clearpage

\appendix

\input{appendix}

\end{document}
