\section{Execution Tuning}
\label{sec:traces}


\begin{figure*}[thbp!]
\centering
\includegraphics[width=0.8\textwidth]{resources/et_diagrams6.pdf}
    \caption{Overview of the data pipeline in \name{} We start from Python functions made executable with synthetic yet representative inputs generated by a combination of LLMs and fuzzing, filtered by test quality. Our custom Python tracer generates a structured dataset of traces. From this dataset, we train models prompted with different trace representations.}
    \label{fig:overview}
\end{figure*}


In this section, we describe the two implementation challenges of \name{} The first one is about where and how to collect execution traces to construct a large and representative training dataset. The second challenge is how to represent these traces to ingest them to the model. Figure \ref{fig:overview} shows an overview of the pipeline for these two challenges.




\subsection{Traces collection}



%\textbf{Collecting executable functions} 
We start from a collection of unrestricted Python code from where functions are extracted, together with their corresponding module imports and auxiliary functions. We allow importing common Python libraries such as \texttt{pandas} or \texttt{matplotlib}. The inputs (function arguments) are generated with an LLM - more specifically, by prompting Llama 3 8B \citep{dubey2024llama3herdmodels} to generate unit tests for these functions. 
For increased coverage, inputs are also generated using fuzzing. In both cases, inputs yielding runtime errors are discarded, and inputs are filtered on test quality by measuring line coverage and similarity between tests.

In total, combining the LLM and fuzzing generated inputs,  we gather about $\sim$300k executable functions, with an average of 6 inputs per function. Using automatically generated inputs allows us to scale the training dataset to 
 $>$ 1.5M executions, without requiring manually written unit tests.
  \begin{wrapfigure}{r}{0.4\textwidth} % 'r' for right, 'l' for left
    \centering
    \includegraphics[width=0.4\textwidth]{resources/ex.pdf}
    \caption{Prompt for Instruction-1.}
    \label{fig:ex}
\end{wrapfigure}

We build a custom tracer leveraging Python's built-in \texttt{sys.settrace}. We capture all Python function call, return, line  and opcode events, and step into user-defined auxiliary functions (but not into functions from imported modules). We deliberately  ignore C events because without access to the source code emitting these events, the C traces would introduce noise to the data. We note that for correctly discarding the 
 C traces, we need to explicitly deactivate tracing of Python code called from C (e.g. a C function calling a Python lambda).
 We step into auxiliary functions present in the context (i.e., defined in the same file) and but don't do so for functions out of the context. Unlike previous work \citep{scratchpad, ni2024nextteachinglargelanguage}, we also capture globals, the stack, and state changes at the instruction-level granularity (i.e. Python opcodes). 
After tracing, we have constructed a structured dataset of executions. The next step is to turn this structured dataset into concrete (prompt, expected output) pairs to ingest to the models.


\subsection{Traces representation}
Following \citet{ni2024nextteachinglargelanguage}, we rely on Python's \texttt{\_\_repr\_\_} 
to have an LLM-friendly representation of each object. Unlike in \citet{ni2024nextteachinglargelanguage}, where authors summarized loops with the first two and the last iterations,  here we want to represent complete program executions, for which we consider different strategies. 


\textbf{Granularities}
We study the following trace granularities: \begin{enumerate*}

\item Direct predictions: Following \citet{Austin2021, gu2024cruxeval}, we fine-tune the models to directly predict the output (return value) from the input of the function.

\item Line-level (source code): Following \citet{scratchpad}, we represent the states at each executed line.

\item Instruction-level (bytecode): Source code lines can map to multiple instructions at the assembly or bytecode level. 
With this motivation in mind, we also consider a representation in which we explicitly show instructions and instruction-level state to the model. Crucially, this implies the introduction of the stack. Aside from the bytecode, the model has access to the source code, shown as inline comments at the first opcode of the corresponding line.
\end{enumerate*}




\textbf{Scratchpads}
We consider the following scratchpad techniques for storing intermediate computations (i.e., the traces):
\begin{enumerate*}
    \item Scratchpad: Following the original scratchpad work \citep{scratchpad}, the model predicts the state after executing each line, defined as the line itself plus the dictionary of the local variables, followed by the predicted return value.
    \item \textit{Compact} scratchpad: Inspired by \citet{ni2024nextteachinglargelanguage}'s trace representation, we also consider a \texttt{diff}-based scratchpad, in which the model only needs to predict the variables that change with respect  to the previous state. This should help at long executions by decreasing the token count. Note, though, that in \citet{ni2024nextteachinglargelanguage} this representation was not used as a scratchpad, but to annotate code.

  
    \item \textit{Dynamic} scratchpad: The two previous scratchpad strategies ask the model to predict the entire execution history of a program (paired with an input), up to the return value. This is problematic with long executions. With this motivation in mind, we introduce dynamic scratchpads, in which a single, self-contained state is updated by the model at every step. It also has the additional advantage that with the same strategy we can naturally train the model to skip steps that are potentially unnecessary to predict the final output, by asking the model to predict what the state will be after N steps. The caveat is that parts of the state that were implicitly encoded by having access to the execution history, now will need to be encoded explicitly. In particular, iterator states, not part of the locals dictionary in Python, can be ambiguous.\footnote{For example, in \texttt{for c in ('a','b','a')}, if we only have access to the current state, we need a way of distinguishing between the first \texttt{'a'} and the last one.
    We explicitly encode it with e.g., \texttt{\_\_for\_iterator\_1\_\_=2} lets us know the iteration count on a given iterator.
    } For this reason, even for the models with line-level granularity, we access the stack to trace the iterators, and explicitly encode their iteration count.
\end{enumerate*}


Figure \ref{fig:scratchpad} depicts the differences between direct output prediction, scratchpad-based output prediction, and dynamic scratchpad. Compact scratchpad is omitted for brevity; it's similar to scratchpad but just predicting the variables that change. Figure \ref{fig:ex} provides a prompt example.



