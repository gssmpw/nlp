\section{Related work}
\label{sec:related}



Learning to execute programs as  a benchmarking task for code reasoning capabilities has been long studied in the machine learning community \citep{learning_to_execute2014}, sometimes with niche architectures
\citep{GravesWD14, GauntBKT16, learning_to_execute2020}, typically on toy or restricted programs. \citet{bieber2022staticpredictionruntimeerrors} proposed learning to predict runtime errors as a practical application of neural program evaluation. More recently, \citet{gu2024cruxeval} introduced  a program output (and input) benchmark for LLMs to measure code understanding capabilities, which we used for evaluation in this work.
Most closely to ours, \citet{scratchpad} propose the use of \textit{scratchpads} to let LLMs write down the results of intermediate computations rather than directly aiming at predicting the final output. With Python output prediction being one of their use cases, they represent traces of intermediate states as JSON dictionaries. 
\citet{ni2024nextteachinglargelanguage} introduce \textit{Naturalized} EXecution Tuning (NExT) and propose the compact representation of Python traces that we followed. Unlike \citet{scratchpad} and this work, NeXT simplifies loops and uses traces in the \textit{input}, improving program repair.  \citet{ding2024semcodertrainingcodelanguage} propose natural language explanations based on executions, leading to further improvements. 
Finally, recent work uses execution \textit{feedback}, rather than traces, in SFT or reinforcement learning settings \citep{dong2024selfplayexecutionfeedbackimproving,gehring2024rlefgroundingcodellms}. 
