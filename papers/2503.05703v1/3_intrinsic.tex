\section{Output prediction results}
\label{sec:intrinsic}

\begin{table*}[thbp!]
\centering
\caption{\label{tab:intrinsic_crux_e}
%Crux evaluations.
Results of individual state predictions on CruxEval, i.e. before aggregating steps into full executions for output prediction. The accuracy is broken down into control flow (does the model  correctly predict the next line?), variables (does the model correctly predict the variable values in the next state?), iterators (does the model correctly predict the iteration count for the current iterators?) stack state, and full state accuracy (how many states are completely correct, i.e. control flow, variables, iterators, and stack are all correct, assuming the state had them). Note that scratchpad does not have iterator states because it does not require them, and line-level models do not have access to the stack. In this Table, F.T. means that the models were fine-tuned on the task, while prompted results indicate no training on traces. %
}
\scalebox{0.95}{
\begin{tabular}{llrrrrr}
\hline
\textsc{Repr.} & \textsc{Model} & \textsc{C. flow} & \textsc{Vars} & \textsc{Iterator} & \textsc{Stack} & \textsc{Full}\\
\hline

Scratchpad & Llama3.1 8B + F.T. & 91.9\% & 86.5\% & - & - & 86.4\%\\

\hline
Line-1  & Llama3.1 8B (prompted)  & 53.8\% & 26.9\% & 39.6\% & - & 10.6\%\\
& \hspace{3mm} + F.T. & 99.5\% & 97.7\% & 99.7\% & - & 96.3\%\\

\hline 
Line-n (global) & Llama3.1 8B (prompted)  & 16.8\% &16.0\%  & 12.3\% & -  & 1.8\%\\
 & \hspace{3mm} + F.T. & 95.1\% & 66.8\% & 96.4\% & - & 79.0\%\\

\hline 
Instruction-1 & Llama3.1 8B (prompted) & 74.1\% & 80.4\% & 77.9\%& 5.8\% & 2.8\% \\
 & \hspace{3mm} +F.T.  & 99.9\% & 99.9\% & 99.9\% & 98.8\% & 98.8\% \\


\hline
\end{tabular}
} 

\end{table*}




In this section, we evaluate models on function output prediction, a proxy for code reasoning, comparing different trace representation strategies. All evaluated models are  fine-tuned using comparable hyperparameters from the instruct version of Llama 3.1-8B \citep{dubey2024llama3herdmodels}, unless stated otherwise.

We start by analyzing the results for individual step predictions. Then, we aggregate these step predictions to evaluate output prediction on CruxEval. Next,  aiming at evaluating on longer and more diverse executions, we also run our models on a subset of MBPP (selecting functions with nested loops) and algorithmic tasks with arbitrarily long execution lengths.

We will refer to the dynamic scratchpad models by using \{Line$\mid$Instruction\}-\{1$\mid$n\}, where Line models have a granularity of lines and Instruction models have a granularity of bytecode instructions. ``1" refers to models trained to predict the next step, while ``n" refers to models trained to predict multiple steps into the future, with n = \{1..10\}. Additionally, note that in our results, we refer to our re-implementation of \citet{scratchpad} as ``Scratchpad", which benefits from the increased data size and context length. The original Scratchpad restricted context windows to 512 tokens; here, we allow up to 8192 tokens.


\begin{wrapfigure}{r}{0.5\textwidth} 


    \centering

    \includegraphics[width=0.5\textwidth]{resources/line_n_v2.pdf}
    \caption{Plot showing \textit{individual} state prediction accuracy (e.g., for \textit{Return}, specifically for this plot and unlike in the rest of the article, we mean return statement accuracy, not full execution accuracy) when increasing N lines into the future, compared to the predictions Negative Log-Likelihood. Accuracy (bars) gets lower as the number of steps into the future increases, and confidence decreases as well (i.e., NLL increases).
    }
    \label{fig:variable_ns}

\end{wrapfigure}

\subsection{Individual state predictions on CruxEval}


Table \ref{tab:intrinsic_crux_e} shows the evaluations of individual states (not aggregated into full executions) for prompted out-of-the-box Llama 3.1 8B \citep{dubey2024llama3herdmodels}  and fine-tunings with traces on top of Llama 3.1 8B. In this section, for Line-n models, we report the average over n. 
\paragraph{Prompted (untrained) models} We find that general-purpose LLMs already exhibit non-trivial capabilities to predict execution steps out of the box. For example, Llama 3.1 8B prompted (i.e., not fine-tuned) to predict the state after executing the next line (Line-1 in Table \ref{tab:intrinsic_crux_e}) correctly guesses the answer 53.8\% of the times, implying a decent understanding of control flow. Control flow is considerably easier when prompted to predict the next bytecode state (Instruction-1 in Table \ref{tab:intrinsic_crux_e}), with a 74.1\%, since non-jump (and non-function calling) bytecode instructions have a linear flow. These control flow capabilities drop to 16.8\% when evaluated on \{1..10\} (averaged) lines into the future (Line-n in Table \ref{tab:intrinsic_crux_e}, i.e. asking the model to predict the state after N lines). Similar results, albeit slightly worse, are obtained for the iterator states. When looking at variable value predictions (Vars column), however, performance drops substantially for the line-level scratchpad. This struggle compared to other mainstream coding benchmarks hints at a lack of execution traces data in general-purpose LLMs. Notably, variable prediction for prompted Llama greatly improves for the Instruction-level variant (80.4\%). However, this is due to the heavy lifting being carried out on the stack, as variable states typically just consist of reading values from the stack into the variables. The stack-level accuracy is indeed low (5.8\%).




\textbf{Execution-tuned Models} Unsurprisingly, models trained on execution traces excel at control flow prediction. In particular, dynamic scratchpad models obtain almost perfect accuracy on control flow prediction, both for line (99.5\%) and instruction levels (99.9\%). The control flow accuracy only drops to 95.1\% for Line-n models, suggesting that the model is indeed capable of internally modeling the flow of future states. In comparison, scratchpad obtains a similarly high 91.0\%. Looking at iterator state prediction, both line and instruction-level dynamic scratchpads obtain almost perfect accuracy as well. Remarkably, the instruction-level model obtains an also near-perfect accuracy for the stack (98.8\%), in contrast to the prompted model. 


\textbf{Skipping steps} Figure \ref{fig:variable_ns} shows the state prediction accuracy when increasing the number of states into the future and the corresponding negative log-likelihood (NLL) as a measure of the confidence of the prediction, for the Line-N dynamic scratchpad. Unsuprisingly, accuracy lowers as N increases. Interestingly, the corresponding NLL increases, showing calibration of the model confidence. Remarkably, however, we do not observe sharp drops in performance when looking at N steps into the future. Actually, our model can effectively learn to predict multiple steps into the future. Control flow and iterator states are relatively feasible to predict when jumping multiple steps, but variables and return values get increasingly complicated. In the Appendix, we provide similar results for Instruction-N.
 

In summary, while the out-of-the-box, prompted Llama shows non-trivial trace modeling capabilities (10.6\% full state accuracy with the Line-1 approach), models trained on traces greatly improve upon it. Interestingly, we observe that the line-based dynamic scratchpad outperforms (96.3\% full accuracy) its scratchpad counterpart (86.4\% full state accuracy), and that the instruction-level obtains the highest full state accuracy, 98.8\%. We also observe that the task of learning the state of N steps into the future is feasible to learn effectively, and that NLL has potential as a measure of model confidence in this setting. However, it remains to be seen how these  individual trace results will aggregate into function output predictions, which we study in the following sections.




\subsection{CruxEval output prediction}

\begin{table*}[thbp!]
\centering
\caption{\label{tab:crux_o}
CruxEval output prediction results, allowing for multi-step predictions  for the variants trained with execution traces.  *Global search using \citet{dijkstra1959note} the algorithm. Not directly comparable due to having access to the ground truth for checking correctness of paths.}
\scalebox{0.87}{
\begin{tabular}{llll}
\hline
\textsc{Representation} & \textsc{Outcome Accuracy} & \textsc{Process Accuracy} & \textsc{avg Steps needed} \\
\hline
Output FT & 49.3\% & - & 1 (direct)\\

\hline

Scratchpad F.T. & 78.7\% & 75.5\% & 10.8 lines \\
Compact Scratchpad F.T. & \textbf{79.7\%} & \textbf{76.6\%} & 11.8 lines \\


 Line-1 FT & 73.3\% & 73.3\% & 8.3 lines\\
Line-n FT & 60.8\% & 60.8\% & \textbf{2.9} lines \\
\hspace{3mm} + search* & 70.3\% & 70.3\% & \textbf{1.8} lines \\
 


Llama 3.1 8B + Instruction-1 F.T & 73.5\% & 73.5\% & 38.8 instructions  \\
\hspace{3mm} + search*  & 74.1\% & 74.1\% & 38.6 instructions \\
Llama 3.1 8B + Instruction-n F.T.  & 62.5\% & 62.5\% & \textbf{22.4} instructions  \\
\hspace{3mm} + search* & 73.5\% & 73.5\% & \textbf{4.8} instructions \\

\hline

Prompted Llama 3.1 8B** & 37.8\% & - & -\\ 
Prompted GPT-4 & 82\% & - & -\\

\hline
\end{tabular}
} 


\end{table*}




We have seen the accuracy of the models when evaluated on individual state predictions. Here, we aggregate the results to evaluate output prediction on CruxEval in Table \ref{tab:crux_o}. For dynamic scratchpad models with more than one possible path (e.g., Line-n), we evaluate taking the \texttt{argmin(NLL)} one. Interestingly, the most confident prediction is not always the next immediate step, which is why predicting multiple steps ahead can lead to fewer overall steps.
We also show results with global search using \citet{dijkstra1959note}'s algorithm to obtain the shortest path using model predictions from the input of the function to the output,  which is not directly comparable to the other results due to having access to the ground truth for checking the correctness of paths. However, we provide it as an upper bound of what could be achieved with the predictions of the model. As a reference, we also  provide the top results in the CruxEval leaderboard, prompted GPT-4 with 82\% accuracy.\footnote{Pass-at-1, \texttt{gpt-4-turbo-2024-04-09+cot (n=3)}as of October 2024}

\textbf{Direct prediction} Out of the box, Llama 3.1 8B obtains an output prediction accuracy of 37.8\%. This accuracy can be improved to 49.3\% by fine tuning on direct output prediction. However, even with the relatively short executions found in CruxEval, more than half of the functions are out of the reach of the direct output prediction model.

\textbf{Results when using traces} Consistently with \citep{scratchpad}, all models trained on execution traces outperform by a great margin the direct output prediction fine-tuning. While we  generally obtain high accuracies (up to almost 80\%), note that the accuracy here, in Table \ref{tab:crux_o}, is substantially lower than in Table \ref{tab:intrinsic_crux_e}. The reason why this happens is that when aggregating individual trace predictions, a single step error (out of, e.g., 20 steps) can lead to a wrong result.   

\textbf{Comparison between models using traces} The compact scratchpad strategy slightly outperforms the full scratchpad one, and in turn these two outperform the dynamic scratchpad approaches. The executions in CruxEval are not long enough to show the advantages of dynamic scratchpads.  

\textbf{Indexing and string manipulation failure modes} In CruxEval, arithmetic operations (a classic failure mode of LLMs) were intentionally left out of the benchmark, to focus on program understanding itself. However, we noticed two interesting failure modes. After a qualitative error analysis, we found that the majority of the errors of the models on CruxEval belong to either one of two categories. The first one is string indexing. Indexing arbitrary strings is hard due to tokenization artifacts, since literals are tokenized inconsistently, and unlike arrays their elements aren't separated by punctuation.
However, it can be particularly hard for the dynamic scratchpad models (and this mainly explains the $\sim$ 5\% gap in output prediction accuracy between the line-level scratchpad and its dynamic scratchpad counterpart), because for each iteration the model needs to count from scratch to which characters the code is referring. Instead, the scratchpad model relies on the previous iteration as a hint to guess what character will come next. 
The second failure mode we saw consists of basic string manipulation. For example, models sometimes fail to predict the return value of Python's built-in \texttt{[string].istitle()} method, an issue that we also observed in the base model. CruxEval's string values might be out-of-distribution for the model.

\textbf{Skipping steps} Looking at the results of Line-n and Instruction-n, we observe that just by selecting the $n$ where the model is the most confident (based on NLL), we are able to obtain reasonable  accuracies (significantly better than direct prediction, albeit worse than Line-1 and Instruction-1) and considerably lesser number of steps needed. For example, Line-n on average needs only a 35\% of the steps of Line-1 to correctly predict a function. This has the remarkable implication that the ordering of model confidence for n=\{1..10\} does not always correspond to the number of steps into the future. That is, with significant frequency, $n=1$ is not always the prediction in which the model is the most confident. 





\subsection{MBPP}

Next, we evaluate on the Python synthesis dataset MBPP \citep{mbpp} with the goal of observing results in longer executions and different domain as CruxEval. 

Particularly, we select functions in the MBPP test set with nested loops (as a  proxy of computational complexity and execution length), leaving us with slightly fewer than 100 functions.\footnote{Unfortunately, for MBPP we discovered an issue with traced iterators in nested \texttt{for} loops. Thus, specifically for MBPP we applied an AST transformation to rewrite nested \texttt{for} loops to \texttt{while} loops. This issue had virtually no effect in CruxEval, due to the lack of executed nested for loops.} 

Similarly to the case of CruxEval, Table \ref{tab:mbpp} shows the results on output prediction for this MBPP subset. The big picture of the results is similar to the case of CruxEval, but with some crucial differences. First, if we look at the average steps needed for correct predictions, we see that here the functions are indeed considerably longer than in CruxEval (in the order of 7x more executed lines). However, the lengths are still not astronomical. 
Relatively to the CruxEval results, here the instruction-based models perform considerably better, which we attribute to the fact that in MBPP there are computations that can be broken down into multiple instructions. Instead, in CruxEval, since most errors consist of indexing or guessing the outputs of string built-in methods, further zooming in doesn't help, as the (indexing or calling a string built-in written in C) can't be further divided.
Since in this benchmark some functions present auxiliary functions, we introduce a variant of the compact scratchpad in this the model is able to step in other called functions, yielding  an improvement of 3 points with respect to compact scratchpad (80.6\% vs. 77.4\%).

\begin{table*}[thbp!]
\centering
\caption{\label{tab:mbpp}
Evaluation on MBPP test set on functions with nested loops  }
\scalebox{0.87}{
\begin{tabular}{llll}
\hline
\textsc{Representation} & \textsc{Outcome Accuracy} & \textsc{Process Accuracy} & \textsc{avg Steps needed} \\
\hline
Output  F.T. & 47.3\%  & - & 1 (direct)\\

\hline

Scratchpad  F.T. & 64.5\% & 64.5\%  & 58.2  lines  \\
Compact Scratchpad F.T. & 77.4\%& 76.3\%  & 73.9  lines  \\
Compact Scratchpad +step-in F.T. & \textbf{80.6\%}& 74.2\%  & 73.9  lines  \\



Line-1 F.T. & 73.1\%  &73.1\%  & 73.8 lines\\
Line-n F.T. & 43\% & 43\% & 15.4  lines \\
\hspace{3mm} + search* & 59.1\%  & 59.1\%  & 7.8  lines \\
 


Instruction-1 F.T. & 78.5\% & 78.5\%  & 351.3 instructions  \\
\hspace{3mm} + search*  & \textbf{80.6\%} & 80.6\%  & 354.6  instructions \\
Instruction-n F.T. &65.6\% &65.6\% & 139.8 instructions  \\
\hspace{3mm} + search* & 88.2\%  & 88.2\% & 35  instructions \\

\end{tabular}
} 


\end{table*}







\subsection{Long executions}

We observe that existing benchmarks for output prediction don't feature long executions. This is especially true for the standard one, CruxEval, but even when targeting functions with nested loops on MBPP, we rarely get to executions with more than 100 executed lines. In this section, we study well-known algorithmic tasks where we can obtain arbitrarily long executions: 
\begin{enumerate*}
    \item Collatz conjecture: a function returning the number of iterations required to reach 1 following the Collatz conjecture sequence, given a starting natural number.
    \item Binary counter: A 4-bit binary counter.
    \item Iterative Fibonacci: An iterative implementation of Fibonacci.
\end{enumerate*}



For selecting the inputs, we generate 4 random numbers (as the small inputs) between 1 and 20, and 5 between 20 and 4000 (as the larger inputs), and evaluate on all of them across the 3 functions. For Fibonacci, we restrict the evaluation on the smaller 5 numbers. For all functions in this section, we replace the function name by \texttt{f}, to give less hints to the model based on potential memorizations of well-known functions during pretraining. Table \ref{tab:summary} shows the summarized results on these tasks (see Appendix \ref{app:long_res_fine} for the fine-grained results).

\textbf{Collatz} The direct output prediction model is able to correctly predict the number of Collatz iterations for the 4 smaller numbers (up to $n=18$), and breaks for larger inputs. Curiously, the scratchpad model is not able to improve on the results of the direct output prediction model, and gets the same accuracy for a considerably increased number of intermediate steps (35 on average, corresponding to the number of executed lines). The compact scratchpad unlocks a larger input, $n=103$, for which is able to do 353 correct intermediate predictions, up to the (correct return value). 
The dynamic scratchpad models shine in this setting. Line-1 is able to correctly predict all studied inputs but 2620 (the next to largest one). For the largest input, $n=3038$, Line-1 needs to chain 619 correct predictions in a row.  Notably, Line-n is able to achieve the same accuracy but with only 39\% of the steps required by Line-1. Optimally, if we had access to an oracle that told us which of the paths was correct, Dijsktra would have yielded a perfect accuracy with only 32.3 steps required on average (compared to the average of 271 for Line-1).
\begin{wraptable}{r}{0.65\textwidth} % Adjust width as needed
\centering
\caption{\label{tab:summary} Long execution results: accuracy (avg. steps needed).}
\scalebox{0.85}{
\begin{tabular}{l|c|c|c}
\hline
\textsc{Representation} & \textsc{Collatz} & \textsc{Binary Counter} & \textsc{Fibonacci} \\
\hline
Output & 4/9 (1) & 1/9 (1) & 4/5 (1) \\
Scratchpad & 4/9 (35) & 4/9 (45.5) & 4/5 (37) \\
Compact Scratchpad & 5/9 (98.6) & 5/9 (116.2) & 5/5 (140.5) \\
Line-1 & 8/9 (271) & \textbf{8/9 (2441)} & 5/5 (140.5) \\
Line-n & \textbf{8/9 (106.1)} & 1/9 (6) & \textbf{5/5 (46.8)} \\
Line-n + Dijkstra & \textbf{9/9 (32.3)} & \textbf{9/9 (410.7)} & \textbf{5/5 (11.8)} \\
\hline
\end{tabular}
}
\end{wraptable}
\textbf{Binary counter} In the case of the 4-bit binary counter, curiously, the direct output prediction model is only able to correctly predict the output for the third smallest input ($n=8$). In this case, scratchpad does significantly improve results with respect to the direct output prediction model, correctly guessing the outputs for the 4 smaller inputsÂ (up to $n=18$). However, the compact scratchpad is still better, unlocking the correct prediction for a bigger input, $n=103$. Curiously, Line-1 gets the same accuracy as with Collatz (all correct but the next to largest input), but with one crucial difference. Here, the executions are even longer. For correctly predicting the output for $n=3038$, Line-1 has to chain as many as 14,055 correct predictions in a row. Here, Line-n is not able to reliably select across paths based on its confidence (NLL). With Dijkstra on Line-n predictions, it would have obtained perfect accuracy with only 17\% of the number of steps needed by Line-1 on average.

\textbf{Fibonacci} Again, scratchpad obtains the exact same accuracy as direct prediction. The rest of the models are able to predict all inputs up to $n=103$. While both Compact scratchpad and Line-1 require 414 steps for $n=103$, Line-n is able to decrease this number to only 160 steps. Optimally, Dijkstra would have obtained 42.
