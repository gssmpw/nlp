\section{Introduction}
\label{sec:intro}





Coding capabilities are one of the most important applications of large language models (LLMs) \citep{NEURIPS2020_1457c0d6}, for which LLMs specialized on coding have been trained on large-scale datasets of programming languages \citep{chen2021codex, roziere2024codellamaopenfoundation}. Current state-of-the-art general-purpose LLMs are thought to contain considerable proportions of code in their pretraining data \citep{openai2024gpt4technicalreport},
which is known to improve reasoning capabilities even in tasks seemingly unrelated to code \citep{aryabumi2024codecodeexploringimpact}.


However, datasets used to train code LLMs (such as \citet{lozhkov2024starcoder2stackv2}) typically treat code as static strings and rarely exploit the \textit{dynamic} information about their execution. Executability is one of the key differences between code and natural language, and most code datasets neglect dimensions of the code domain such as reasoning over code execution, which in turn could lead to better code understanding. 
 
This fundamental limitation has sparked a renewed interest in modeling program executions, connecting with the pre-LLM neural program evaluation literature \citep{learning_to_execute2014, GravesWD14}, which studied whether neural networks could learn to execute programs. \citet{Austin2021} fine-tune LLMs to directly predict the output of Python functions from coding competitions and math problems, which are paired with unit tests.
Crucially, \citet{scratchpad} showed that asking (and training) the model to predict all the line-level states of a Python function execution up to the return value improved the results on function output prediction, compared to directly asking to predict the return value. They refer to these tokens emitted by the model to perform intermediate computations before the final answer as \textit{scratchpad}. In this work, we build upon this approach. 

Nevertheless, key questions remain unanswered:
\begin{enumerate*} 
\item How we increase the number of examples in trace datasets, given that the programs need to be executable?
\item How does trace granularity affect the models's performance?
\item How can we handle long execution traces?
\item What kind of scratchpad works best for storing intermediate outputs - can we skip ``unnecessary'' intermediate steps?
\item What are the effects of trace modeling on downstream code generation tasks?
\end{enumerate*}

\begin{figure}[thbp!]
\includegraphics[width=\textwidth]{resources/scratchpad3.pdf}
  
    \caption{Given a natural number, a function returns the number of iterations required to arrive at 1, when following the sequence in the Collatz conjecture. Can we predict the output of such a function for large inputs (3038 in our example) using LLMs? Asking an LLM to directly predict the output results in a plausible but incorrect answer.  
    Training a model to predict the intermediate traces of the function as a scratchpad of intermediate computations  \citep{scratchpad} generally yields more accurate output predictions, but can be impractical or even inaccurate with long executions.
    In this work, we introduce \textit{dynamic}  scratchpads, in which the model updates a single, self-contained scratchpad instance, 
    yielding to more accurate predictions for long executions. 
    }
    \label{fig:scratchpad}
\end{figure}



With the goal of answering these questions, we study \fullname{} (\name{}), a training procedure in which we explicitly
model real-world program execution traces without requiring manual test annotations (needed to execute the programs we want to trace). To scale trace modeling to large, real-world programs, we start from a collection of $\sim$300k Python functions, made executable with synthetic inputs generated by a combination of LLMs and fuzzing.
We then build a custom Python tracer to track local variables, global variables, and additional information obtained from the stack. We statically represent traces in LLM-friendly formats, including iterators and functions. After trace collection, to ingest traces to LLMs we study three levels of granularity: program (i.e., direct output prediction), line, and bytecode instructions.



We compare three scratchpad strategies for storing the intermediate computations: a) regular scratchpad \citep{scratchpad}, i.e., a dictionary with all the variable values at each step, b) \textit{compact} scratchpad containing the changed variables only \citep{ni2024nextteachinglargelanguage}, and c) \textit{dynamic} scratchpad (depicted in Figure \ref{fig:scratchpad}), in which, rather than accumulating all the intermediate computation history, the LLM is asked to update a single, self-contained representation of the current state of the program.

As a proxy of code reasoning, we evaluate models on program output prediction (given an input), allowing them to generate  intermediate execution states. We first evaluate on the standard output prediction benchmark, CruxEval \citep{gu2024cruxeval}, on which models trained on traces clearly outperform the direct output prediction ones. 
However, we also observe interesting failure modes involving indexing and basic string manipulation. Aiming at evaluating on longer and more diverse executions, we also run our models on a subset of a Python synthesis benchmark, MBPP \citep{mbpp}, selecting functions with nested loops, where we observe higher disparity between tracing strategies. To study even longer executions, we also study algorithmic tasks with arbitrarily long execution lengths, including the Collatz conjecture (also known as the Ulam conjecture), 
showing the advantages of dynamic scratchpads on long executions (success on up to 14k execution steps) and the potential of dynamically skipping steps (allowing to decrease the needed intermediate steps from e.g. 14k steps to 1.5k). Finally, we discuss applications by analyzing the effects of \name{} on code generation and reasoning tasks.

