\section{Related Work}
\label{related_work}

The study of causality and its linguistic expressions garnered renewed and intensified interest, particularly in the context of evaluating the reasoning abilities of LLMs. Recent advancements led to the development of several specialized datasets, aimed at testing the causal reasoning of LLMs through hypothetical scenarios. Notable examples include CLadder \cite{jin2023cladder}, which assesses causal reasoning using questions based on formal rules; CausalBench \cite{wang2024causalbench}, used for tasks related to mathematics, coding, and textual data; and CausalNet \cite{ashwani2024cause}, which covers both causal and counterfactual questions. Unlike ExpliCa, these datasets focus on implicit notions of causality, which are not overtly expressed in the linguistic structures of the text.

To evaluate LLMs, several datasets have also been annotated with explicit causal relationships between events in texts. 
These range from multilingual educational content, as in MECI \citep{lai2022meci}, or financial news \citep{mariko2022financial}, to more diverse sources, such as CREST \cite{hosseini2021predicting}. Although these datasets do not explore the temporal dimensions of causality, many causal annotation schemas are derived from datasets that do annotate temporal relationships between events (e.g., BECauSE, \citealt{dunietz2017because}) or vice-versa (e.g., Temporal and Causal Reasoning, \citealt{ning2018joint}), including those from news sources (e.g., Causal Time Bank, \citealt{mirza2014annotating}; Event StoryLine Corpus, \citealt{caselli2017event}), and from short commonsense narratives (e.g., CaTeRS, \citealt{mostafazadeh2016caters}). However, such datasets do not leverage crowdsourcing annotation by native speakers for both causal and temporal relations as in ExpliCa. In our dataset, the ground truth is given by English native speakers' annotation collected via crowdsourcing, with the aim of addressing the complexity of distinguishing truly causal from merely temporal relations between events.
A key challenge in evaluating LLMs using datasets with direct textual annotations of causal relations is, in fact, the inherent ambiguity of their expression in natural language. For instance, linguistic markers such as \emph{and} can signal either causality or temporality, depending on the context. This ambiguity can limit the effectiveness of such datasets in assessing causal reasoning. 

To overcome this limitation, ExpliCa has chosen a more controlled approach to causality evaluation by conducting a pairwise analysis of events, each expressed by a single sentence. The same strategy has been adopted in the COPA dataset \cite{roemmele2011choice}, where causality detection is framed as a task where the system must choose the most plausible alternative between two options. Similarly, e-CARE (Explainable Causal Reasoning, \citealt{du2022care}) includes over $21,000$ multiple-choice questions focused on causal reasoning, accompanied by conceptual explanations that clarify the underlying causal logic of each question. While these two datasets present instances of implicit causality, the BIG-Bench (Beyond the Imitation Game, \citealt{srivastava2022beyond}) initiative also models explicit causal reasoning. In this framework, the system must select the most plausible causal relationship between \emph{A because B} and \emph{B because A}. Similarly, in ExpliCa pairs of sentences from e-CARE and BIG-bench are joined in both directions ($A>B$; $B>A$), but using both temporal and causal connectives. This allows us to carefully analyze the models' ability to discriminate between the related and yet very different relations of temporal precedence and causality. Furthermore, other linguistic cues, such as anaphoric references, have been removed. This design ensures that models are unlikely to rely on surface features, preventing the correct interpretation of causal markers from being inferred simply from the syntactic context.

Finally, it is worth mentioning that some of the above datasets have become part of a broader evaluation framework called Causal evaluation of Language Models (CaLM, \citealt{chen2024causal}). CaLM serves as a comprehensive benchmark for assessing LLMs' causal reasoning capabilities. It comprises $126,334$ data samples and provides a foundational taxonomy of four modules: causal target, adaptation, metric, and error analysis. In relation to causal discovery, this framework addresses issues distinct from those targeted by ExpliCa, and it focuses solely on the analysis of LLM-generated responses. By contrast, in our work, we evaluated both the model outputs elicited via prompts, and the internal knowledge of LLMs, assessed through perplexity measurements.