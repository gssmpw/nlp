\section{Introduction}

Reinforcement learning (RL) has demonstrated significant success across various domains, including robotics, finance, and healthcare, by enabling agents to learn optimal decision-making strategies through interaction with the environment. However, in many real-world applications, direct interaction with the environment is impractical due to safety concerns, high costs, or data collection constraints \citep{sunderhauf2018limits, hofer2021sim2real}. This challenge is particularly evident in scenarios where agents are trained in simulated environments before being deployed in the real world, such as in robotic control and autonomous driving. The discrepancy between the simulated and real-world environments, known as the simulation-to-reality gap, often leads to performance degradation when the learned policy encounters unmodeled uncertainties. Robust reinforcement learning (robust RL) addresses this challenge by formulating the learning problem as an optimization over an uncertainty set of transition probabilities, ensuring reliable performance under worst-case conditions. In this work, we focus on the problem of evaluating the robust value function and robust average reward for a given policy using only data sampled from a simulator (nominal model), aiming to enhance generalization and mitigate the impact of transition uncertainty in real-world deployment.

Reinforcement learning problems under infinite time horizons are typically studied under two primary reward formulations: the discounted reward setting, where future rewards are exponentially discounted, and the average reward setting, which focuses on optimizing long-term performance. While the discounted-reward formulation is widely used, it may lead to myopic policies that underperform in applications requiring sustained long-term efficiency, such as queueing systems, inventory management, and network control. In contrast, the average-reward setting is more suitable for environments where decisions impact long-term operational efficiency. Despite its advantages, robust reinforcement learning under the average-reward criterion remains largely unexplored. Existing works on robust average-reward RL primarily provide asymptotic guarantees \citep{wang2023robust,wang2023model, wang2024robust}, lacking scalable algorithms with finite-time performance bounds. This gap highlights the need for principled approaches that ensure robustness against model uncertainties while maintaining strong long-term performance guarantees.

Solving the robust average-reward reinforcement learning problem is significantly more challenging than its non-robust counterpart, with the primary difficulty arising in policy evaluation. Specifically, the goal is to compute the worst-case value function and worst-case average reward over an entire uncertainty set of transition models while having access only to samples from a nominal transition model. In this paper, we investigate three types of uncertainty sets: Contamination uncertainty sets, TV distance uncertainty sets, and Wasserstein distance uncertainty sets. Unlike the standard average-reward setting, where value functions and average rewards can be estimated directly from observed trajectories, the robust setting introduces an additional layer of complexity due to the need to optimize against adversarial transitions. Consequently, conventional approaches based on direct estimation such as \citep{wei2020model, agarwal2021theory} immediately fail, as they do not account for the worst-case nature of the problem. Overcoming this challenge requires new algorithmic techniques that can infer the worst-case dynamics using only limited samples from the nominal model.

\subsection{Challenges and Contributions}

A common approach to policy evaluation in robust RL is to solve the corresponding robust Bellman operator. However, robust average-reward RL presents additional difficulties compared to the robust discounted setting. In the discounted case, the presence of a discount factor induces a contraction property in the robust Bellman operator \citep{wang2022policy, zhou2024natural}, facilitating stable iterative updates. In contrast, the average-reward Bellman operator lacks a contraction property with respect to any norm even in the non-robust setting \citep{zhang2021finite}, making standard fixed-point analysis inapplicable. Due to this fundamental limitation, existing works on robust average-reward RL such as  \citep{wang2023model} rely on asymptotic techniques, primarily leveraging ordinary differential equation (ODE) analysis to examine the behavior of temporal difference (TD) learning. These methods exploit the asymptotic stability of the corresponding ODE \citep{borkar2023stochastic} to establish almost sure convergence but fail to provide finite-sample performance guarantees. Addressing this limitation requires novel analytical tools and algorithmic techniques capable of providing explicit finite-sample bounds for robust policy evaluation and optimization.

In this work, we first establish and exploit a key structural property of the robust average-reward Bellman operator: its contraction under the span semi-norm, denoted as $\|\cdot\|_{\mathrm{sp}}$. This fundamental result enables the use of stochastic approximation techniques similar to \citep{zhang2021finite} to analyze and bound the error in policy evaluation, overcoming the lack of a standard contraction property that has hindered prior finite-sample analyses. Building on this insight, we develop a novel stochastic approximation framework tailored to the robust average-reward setting. Our approach simultaneously estimates both the robust value function and the robust average reward, leading to an efficient iterative procedure for solving the robust Bellman equation. A critical challenge in this framework uner TV and Wasserstein distance uncertainty sets is accurately estimating the worst-case transition effects, which requires computing the support function of the uncertainty set. While previous work has leveraged Multi-Level Monte Carlo (MLMC) for this task, existing MLMC-based estimators suffer from infinite expected sample complexity due to the unbounded nature of the required geometric sampling. To address this, we introduce a truncation mechanism based on a truncated geometric distribution, ensuring that the sample complexity remains finite while maintaining an exponentially decaying bias. With these techniques, we derive the first finite-sample complexity guarantee for policy evaluation in robust average-reward RL, achieving an optimal $\tilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity bound. The main contributions of this paper are summarized as
follows:

\begin{itemize}[leftmargin=*]
    \item We prove that under the ergodicity assumption, the robust average-reward Bellman operator is a contraction with respect to the span semi-norm (Theorem \ref{thm:robust_span-contraction}). This key result enables the application of stochastic approximation techniques for policy evaluation.
    
    \item We prove the convergence of stochastic approximation under the span semi-norm contraction and under i.i.d. with noise with non-zero bias (Theorem \ref{thm:informalbiasedSA}).
    
    \item We develop an efficient method for computing estimates for the robust Bellman operator across TV distance and Wasserstein distance uncertainty sets. By modifying MLMC with a truncated geometric sampling scheme, we ensure finite expected sample complexity while keeping variance controlled and bias decaying exponentially with truncation level (Theorem \ref{thm:sample-complexity}-\ref{thm:linear-variance}).
    
    \item We propose a novel temporal difference learning method that iteratively updates the robust value function and the robust average reward, facilitating efficient policy evaluation in robust average-reward RL. We establish the first non-asymptotic sample complexity result for policy evaluation in robust average-reward RL, proving an order-optimal $\tilde{\mathcal{O}}(\epsilon^{-2})$ complexity for policy evaluation (Theorem \ref{thm:Vresult}), along with a  $\tilde{\mathcal{O}}(\epsilon^{-2})$ complexity for robust average reward estimation (Theorem \ref{thm:gresult}).
\end{itemize}

% Solving the robust average-reward reinforcement learning problem is significantly challenging, primarily in policy evaluation. The key difficulty lies in computing the worst-case value function and worst-case average reward over the entire uncertainty set of transition models while having access only to samples from the nominal model. Unlike the discounted setting, where the contraction property induced by the discount factor enables stable iterative updates, the average-reward formulation lacks such a contraction, making robust evaluation substantially more difficult. This absence of contraction prevents the direct application of standard robust RL techniques used in the discounted setting. As a result, existing works on robust average-reward learning rely on asymptotic arguments, primarily leveraging ordinary differential equation (ODE) analysis to study the behavior of temporal difference (TD) learning. These methods utilize the asymptotic stability of the corresponding ODE to establish almost sure convergence but fail to provide finite-sample guarantees. Addressing these limitations, this work aims to develop a principled approach to robust average-reward RL with explicit finite-sample performance bounds, bridging the gap between theory and practical deployment.