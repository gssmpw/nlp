\section{Convergence of Span Contraction with Bias} \label{spancontractionwithbias}
In the previous section, we established that the robust Bellman operator is a contraction under the span semi-norm, ensuring that policy evaluation can be analyzed within a well-posed stochastic approximation framework. However, conventional stochastic approximation methods typically assume unbiased noise, where variance diminishes over time without introducing systematic drift. In contrast, the noise in robust policy evaluation under TV and Wasserstein distance uncertainty sets exhibits a small but persistent bias, arising from the estimators of the support functions $\hat{\sigma}_{\cp^a_s}(V)$ (discussed in Section \ref{QueriesSection}). This bias, if not properly addressed, can lead to uncontrolled error accumulation, affecting the reliability of policy evaluation. To address this challenge, this section introduces a novel analysis of biased stochastic approximation, leveraging properties of dual norms to ensure that the bias remains controlled and does not significantly impact the convergence rate. Our results extend prior work on unbiased settings and provide the first explicit finite-time guarantees, which is further used to establish the sample complexity of policy evaluation in robust average reward RL. Specifically, we analyze the iteration complexity for solving the fixed equivalent class equation $H(x^*) - x^* \in \overline{E}$ where $\overline{E}\coloneqq \{c \mathbf{e} : c \in \mathbb{R}\}$ with $\mathbf{e}$ being the all-ones vector. The stochastic approximation iteration being used is as follows:
\begin{equation}\label{eq:SA-update}
   x^{t+1}=x^t + \eta_t \bigl[\widehat{H}(x^t) - x^t\bigr],
   \quad
   \text{where}\quad
   \widehat{H}(x^t)=H(x^t) + w^t.
\end{equation}
with $\eta_t>0$ being the step-size sequance and with the following assumptions on the operator $H$ and noise $\omega^t$:
\begin{itemize}
\item $H$ is a contractive mapping with respect to the span semi-norm, there exist $\gamma \in (0,1)$ such that
\begin{equation} \label{eq:Hcontraction}
     \|H(x) - H(y)\|_{\mathrm{sp}}\leq \gamma\,\|x - y\|_{\mathrm{sp}}, \quad 
  \forall x, y
\end{equation}
\item the noise terms $\omega^t$ are i.i.d. and have bounded bias and variance
\begin{equation} \label{eq:omegabounded}
    \mathbb{E}[\,\|w^t\|_{\mathrm{sp}}^2 | \mathcal{F}^t] \le A + B\,\|x^t - x^*\|_{\mathrm{sp}}^2  \quad  \text{and}\quad \bigl\|\mathbb{E}[\,w^t | \mathcal{F}^t]\bigr\|_{\mathrm{sp}} \le \varepsilon_{\mathrm{bias}}
\end{equation}
\end{itemize}

\begin{theorem} \label{thm:informalbiasedSA}
   If $x^t$ is generated by \eqref{eq:SA-update} with all assumptions in \eqref{eq:Hcontraction} and \eqref{eq:omegabounded} satisfied, then if the stepsize $\eta_t \coloneqq \cO(\frac{1}{t})$,
    \begin{equation} \label{eq:biasedSA}
        \mathbb{E}\Bigl[\|x^T - x^*\|^2_{\mathrm{sp}}\Bigr] \leq  \cO\left(\frac{1}{T^2}\right)\|x^0 - x^*\|^2_{\mathrm{sp}} + \cO\left(\frac{A}{(1-\gamma)^2T}\right) +  \cO\left(\frac{x_{\mathrm{sp}} \varepsilon_{\text{bias}} \log T }{1-\gamma} \right)
    \end{equation}
    where  $x_{\mathrm{sp}} \coloneqq \sup_x \|x\|_{\mathrm{sp}}$ is the upper bound of the span for all $x^t$.
\end{theorem}
Theorem \ref{thm:informalbiasedSA} adapts the analysis of \citep{zhang2021finite} and extends it to a biased i.i.d. noise setting. To manage the bias terms, we leverage properties of dual norms (see \eqref{eq:dualNormIneq}-\eqref{eq:G_value} in Appendix \ref{appendix4biasedSA}) to bound the inner product between the error term and the gradient, ensuring that the bias influence remains logarithmic in 
$T$ rather than growing unbounded, while also carefully structuring the stepsize decay to mitigate long-term accumulation. This results in an extra $\varepsilon_{\mathrm{bias}}$ term with logarithmic dependence of the total iteration $T$. The detailed proof of Theorem \ref{thm:informalbiasedSA} along with the exact constant terms is in Appendix \ref{proofbiasedSA}.