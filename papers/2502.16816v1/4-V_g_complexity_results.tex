\section{Propoosed Algorithm and Final Results}
We present the formal algorithm for robust policy evaluation and robust average reward for a given policy $\pi$ in Algorithm \ref{alg:RobustTD}. Algorithm \ref{alg:RobustTD} presents a robust temporal difference (TD) learning method for policy evaluation in robust average-reward MDPs. This algorithm builds upon the truncated MLMC estimator (Algorithm~\ref{alg:sampling}) and the biased stochastic approximation framework in Section \ref{spancontractionwithbias}, ensuring both efficient 
sample complexity and finite-time convergence guarantees.

The algorithm is divided into two main phases. The first phase (Lines 1-7) estimates the robust value function. The noisy Bellman operator is computed using the estimator $\hat{\sigma}_{\mathcal{P}_s^a}(V_t)$ obtained depending on the uncertainty set type. Then the iterative update follows a stochastic approximation scheme with step size $\eta_t$, ensuring convergence while maintaining stability. Finally, the value function is centered at an anchor state $s_0$ to remove the ambiguity due to its additive invariance. The second phase (Lines 8-14) estimates the robust average reward by utilizing $V_T$ from the output of the first phase. The expected Bellman residual  $\delta_t(s)$ is computed across all states and averaging it to obtain $\bar{\delta}_t$. A separate stochastic approximation update with step size $\beta_t$ is then applied to refine $g_t$, ensuring convergence to the robust worst-case average reward. By combining these two phases, Algorithm~\ref{alg:RobustTD} provides an efficient and provably 
convergent method for robust policy evaluation under average-reward criteria, marking 
a significant advancement over prior methods that only provided asymptotic guarantees. 

\begin{algorithm}[htb]
\caption{Robust Average Reward TD}
\label{alg:RobustTD}
\textbf{Input}: Policy $\pi$, Initial values $V_0$, $g_0=0$, Stepsizes $\eta_t$, $\beta_t$, Truncation level $N_{\max}$, $t=0,1,\ldots, T-1$, Anchor state $s_0\in\mcs$
\begin{algorithmic}[1] 
\For {$t = 0,1,\ldots, T-1$}
\For {each $(s,a)\in\mcs\times\mca$} 
\If {Contamination uncertainty set} Sample $\hat{\sigma}_{\cp^a_s}(V_t)$ according to \eqref{eq:contaminationquery}
\ElsIf{TV distance or Wasserstein distance uncertainty set} Sample $\hat{\sigma}_{\cp^a_s}(V_t)$ according to Algorithm \ref{alg:sampling}
\EndIf
\EndFor
\State $\hat{\mathbf{T}}_{g_0}(V_t)(s) \leftarrow \sum_{a} \pi(a|s) \big[ r(s,a) - g_0 +  \hat{\sigma}_{\cp^a_s}(V_t) \big], \quad \forall s \in \mathcal{S}$
\State  $V_{t+1}(s) \leftarrow V_t(s) + \eta_t \left( \hat{\mathbf{T}}_{g_0}(V_t)(s) - V_t(s) \right), \quad \forall s \in \mathcal{S}$
\State  $V_{t+1}(s) = V_{t+1}(s) - V_{t+1}(s_0), \quad \forall s \in \mathcal{S}$
\EndFor
\For {$t = 0,1,\ldots, T-1$}
\For {each $(s,a)\in\mcs\times\mca$} 
\If {Contamination uncertainty set} Sample $\hat{\sigma}_{\cp^a_s}(V_t)$ according to \eqref{eq:contaminationquery}
\ElsIf{TV distance or Wasserstein distance uncertainty set} Sample $\hat{\sigma}_{\cp^a_s}(V_t)$ according to Algorithm \ref{alg:sampling}
\EndIf
\EndFor
\State $\hat{\delta}_t(s) \leftarrow \sum_{a}\pi(a|s) \big[ r(s,a) +  \hat{\sigma}_{\cp^a_s}(V_T) \big]- V_T(s)  , \quad \forall s \in \mathcal{S}$
\State $\bar{\delta}_t \leftarrow \frac{1}{S}\sum_s \hat{\delta}_t(s)$
\State $g_{t+1} \leftarrow g_t + \beta_t(\bar{\delta}_t-g_t)$
\EndFor
\Return $V_T$, $g_T$
\end{algorithmic}
\end{algorithm}


To derive the sample complexity of robust policy evaluation, we utilize the span semi-norm contraction property of the bellman operator in Theorem \ref{thm:robust_span-contraction}, and fit Algorithm \ref{alg:RobustTD} into the general biased stochastic approximation result in Theorem \ref{thm:informalbiasedSA} while incorporating the bias analysis characterized in Section \ref{QueriesSection}. Since each phase of Algorithm \ref{alg:RobustTD} contains a loop of length $T$ with all the states and actions updated together, the total samples needed for the entire algorithm in expectation is $2SAT \E[N_{\max}]$, where $\E[N_{\max}]$ is one for contamination uncertainty sets and is $\cO(N_{\max})$ from Theorem \ref{thm:sample-complexity} for TV and Wasserstein distance uncertainty sets.


\begin{theorem} \label{thm:Vresult}
   If $V_t$ is generated by Algorithm \ref{alg:RobustTD} and satisfying Assumption \ref{ass:sameg}, then if the stepsize $\eta_t \coloneqq \cO(\frac{1}{t})$, we require a sample complexity of $\cO\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \right)$ for contamination uncertainty set and a sample complexity of $\tilde{\cO}\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \right)$ for TV and Wasserstein distance uncertainty set to ensure an $\epsilon$ convergence of $V_T$.
\end{theorem}
\begin{theorem} \label{thm:gresult}
    If $g_t$ is generated by Algorithm \ref{alg:RobustTD} and satisfying Assumption \ref{ass:sameg}, then if the stepsize $\beta_t \coloneqq \cO(\frac{1}{t})$, we require a sample complexity of $\tilde{\cO}\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \right)$ for contamination uncertainty set and a sample complexity of $\tilde{\cO}\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \right)$ for TV and Wasserstein distance uncertainty set to ensure an $\epsilon$ convergence of $g_T$.
\end{theorem}

The formal version of Theorem \ref{thm:Vresult} and Theorem \ref{thm:gresult} along with the proofs are in Appendix \ref{proof:VGresults}. Theorem \ref{thm:Vresult} and Theorem \ref{thm:gresult} provide the order-optimal sample complexity 
of $\tilde{\cO}(\epsilon^{-2})$ for Algorithm \ref{alg:RobustTD} to achieve an $\epsilon$-accurate estimate of $V_T$ and $g_T$. The proof of Theorem \ref{thm:gresult} extends the analysis of Theorem \ref{thm:Vresult} to robust average reward estimation. The key difficulty lies in controlling the propagation of error from value function estimates to reward estimation. By again leveraging the contraction property and appropriately tuning step sizes, we establish an $\tilde{\cO}(\epsilon^{-2})$ complexity bound for robust average reward estimation.

