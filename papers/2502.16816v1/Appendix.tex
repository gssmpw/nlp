\appendix

\section{Span Semi-Norm Contraction Property of Bellman Operator}
\subsection{Proof of Lemma \ref{lem:span-contraction}} \label{proofspan-contraction}

Let $V_1, V_2 : \mathcal{S}\to \mathbb{R}$ and define $\Delta = V_1 - V_2$.
Denote $\kp^\pi$ as the transition matrix of the Markov chain under policy $\pi$. By the definition of the Bellman operator under the transition $\kp^\pi$, we have
\begin{equation}
    \mathbf{T}_g^\pi(V_1)(s)- \mathbf{T}_g^\pi(V_2)(s)=\sum_{s' \in \mathcal{S}}
\kp^\pi(s'| s)
\bigl[V_1(s') - V_2(s')\bigr]=\kp^\pi \,\Delta (s).
\end{equation}
Hence
\begin{equation}
\bigl\|\mathbf{T}_g^\pi(V_1) - \mathbf{T}_g^\pi(V_2)\bigr\|_{\mathrm{sp}}=\|\kp^\pi\,\Delta\|_{\mathrm{sp}}.
\end{equation}

Note that irreducibility and aperiodicity of the Markov chain on a finite state space implies \citep{norris1998markov} the existence of an integer $m \ge 1$ and $\gamma > 0$ such that
\begin{equation} \label{eq:m_step_transition}
(\kp^\pi)^m(s_1,s_2)\geq\gamma
\quad
\forall\,s_1,s_2 \in \mathcal{S}.
\end{equation}
Thus each row of $(\kp^\pi)^m$ is a probability distribution with at least positive mass $\gamma$ over all states. We now introduce the following lemma to characterize the contraction property of $(\kp^\pi)^m$,

\begin{lemma}\label{lem:positivity-contraction}
Let $M$ be an $S\times S$ stochastic matrix with the property
\begin{equation} \nonumber 
M(i,t) \;\ge\; \gamma \quad\text{for all } i,t \in \{1,\dots,S\}
\quad\text{and some constant } 0 < \gamma \le 1/S.
\end{equation}
Then for any function $x : \{1,\dots,S\} \to \mathbb{R}$, the {\em span} of $Mx$ is strictly reduced:
\begin{equation}
\|M x\|_{\mathrm{sp}} \leq (1 - 2\,\gamma)\,\|x\|_{\mathrm{sp}},
\end{equation}
where
\begin{equation}
\|x\|_{\mathrm{sp}}=\max_{1 \le s \le S}\,x(s)-\min_{1 \le s \le S}\,x(s).
\end{equation}
\end{lemma}

\begin{proof}
We first define 
\[
x_{\max} = \max_{s} x(s),
\quad
x_{\min} = \min_{s} x(s),
\quad
\|x\|_{\mathrm{sp}} = x_{\max} - x_{\min}.
\]
If $\|x\|_{\mathrm{sp}} = 0$, the statement is trivial since both sides are zero. Otherwise, consider the normalized function
\[
y(s) \coloneqq \frac{x(s) - x_{\min}}{\,x_{\max}-x_{\min}\,},
\]
so that $0 \le y(s) \le 1$ for all $s$ and $\|y\|_{\mathrm{sp}} = 1$. 
Then denote $e$ as the all-ones vectors and we have
\begin{align}
\|M x\|_{\mathrm{sp}} &= \| x_{\min} \cdot Me + (x_{\max}-x_{\min})M\bigl[\tfrac{x - x_{\min}}{x_{\max}-x_{\min}}\bigr] \|_{\mathrm{sp}} \\ \nonumber
&=(x_{\max}-x_{\min})\bigl\| My \bigr\|_{\mathrm{sp}}\\ \nonumber
&=\|x\|_{\mathrm{sp}}\|M y\|_{\mathrm{sp}}.
\end{align}
It remains to prove $\|M y\|_{\mathrm{sp}} \le (1-2\gamma)$ for $y$ taking the values in $[0,1]$.

Assume $y:\{1,\dots,S\}\to[0,1]$ with $\max_s y(s) - \min_s y(s)=1$, so in fact $\min_s y(s)=0$ and $\max_s y(s)=1$. For $1\le i \le S$, define
\begin{equation}
(M y)(i) =\sum_{t=1}^S M(i,t)y(t).
\end{equation}
Let $i^* \coloneqq \arg\max_i (M y)(i)$ and $j^* \coloneqq \arg\min_j (M y)(j)$. 
Then
\begin{equation}
\|M y\|_{\mathrm{sp}}=(M y)(i^*) - (M y)(j^*).
\end{equation}
To bound this difference, construct a pair of random variables $(T,T')$ as follows:
\begin{equation}
T \sim M(i^*,\cdot), \quad T' \sim M(j^*,\cdot),
\end{equation}
However, we can couple them in such a way that 
$$
\mathbb{P}[T = T'] \geq 2\gamma.
$$
Such a coupling is possible because each row $M(i^*,t)$ and $M(j^*,t)$ 
has at least $\gamma$ mass on every $t$. Under this coupling:
\[
(M y)(i^*) = \mathbb{E}\bigl[y(T)\bigr],
\quad
(M y)(j^*)=\mathbb{E}\bigl[y(T')\bigr].
\]
Hence
\begin{equation}
(M y)(i^*) - (M y)(j^*)=\mathbb{E}\bigl[y(T) - y(T')\bigr].
\end{equation}
But $y(t)\in [0,1]$.  
Whenever $T = T'$, the difference $y(T) - y(T')$ is zero.  
Since $\Pr[T = T'] \ge 2\gamma$, the event $T \neq T'$ has probability at most $(1-2\gamma)$.  
On $T \neq T'$, the difference $|y(T) - y(T')|$ is at most $1$ (because $0 \le y(\cdot)\le 1$).  
Therefore
\begin{equation}
\bigl|(M y)(i^*) - (M y)(j^*)\bigr|=\bigl|\mathbb{E}\bigl[y(T) - y(T')\bigr]\bigr|\leq(1 - 2\gamma)\cdot 1=1 - 2\gamma.
\end{equation}
Thus $\|M y\|_{\mathrm{sp}} \le 1 - 2\gamma$ whenever $\|y\|_{\mathrm{sp}}=1$ and $y\in[0,1]$.
\end{proof}

Directly applying $(\kp^\pi)^m$ to Lemma \ref{lem:positivity-contraction} would obtain

\begin{equation}
\|(P^\pi)^m \Delta\|_{\mathrm{sp}}\leq (1-2\gamma)\|\Delta\|_{\mathrm{sp}},
\end{equation}
Where $\gamma$ is defined in \eqref{eq:m_step_transition}.
Thus there also exist $\beta \in (0,1)$ where $\beta = (1-2\gamma)^{\frac{1}{m}}$ such that $\|(P^\pi) \Delta\|_{\mathrm{sp}}\leq \beta \|\Delta\|_{\mathrm{sp}}$.
Thus, this proves the lemma.

\subsection{Proof of Theorem \ref{thm:robust_span-contraction}} \label{proofrobust-span-contraction}

    We note that for any $g$, for any $V_1$ and $V_2$, for all $s \in \mathcal{S}$, we have:
    \begin{align}
         \mathbf{T}_g(V_1)(s) -  \mathbf{T}_g(V_2)(s) &= \sum_{a \in \mathcal{A}} \pi(a|s) [\sigma_{p^a_s}(V_1) -  \sigma_{p^a_s}(V_2)] \nonumber \\
        & = \sum_{a \in \mathcal{A}} \pi(a|s) [ \min_{p\in\cp^a_s} \sum_{s' \in \mathcal{S}} p(s'|s,a)V_1(s') -   \min_{p\in\cp^a_s} \sum_{s' \in \mathcal{S}} p(s'|s,a) V_2(s')] \nonumber \\
        & \leq  \sum_{a \in \mathcal{A}} \pi(a|s) \max_{p\in\cp^a_s} \Bigg[ \sum_{s' \in \mathcal{S}} p(s'|s,a)V_1(s') -  \sum_{s' \in \mathcal{S}} p(s'|s,a) V_2(s') \Bigg] 
    \end{align}
    Where the last inequality comes from the fact that $\min f(x) - \min g(x) \leq \max (f(x)-g(x))$. denote $\Tilde{p} (V_1,V_2) = \argmax_{p\in\cp^a_s} [ \sum_{s' \in \mathcal{S}} p(s'|s,a)V_1(s') -  \sum_{s' \in \mathcal{S}} p(s'|s,a) V_2(s') ] $, we further obtain
    \begin{align}
         \mathbf{T}_g(V_1)(s) -  \mathbf{T}_g(V_2)(s) &\leq \sum_{a \in \mathcal{A}} \pi(a|s)  \Bigg[ \sum_{s' \in \mathcal{S}} \Tilde{p}(s'|s,a)V_1(s') -  \sum_{s' \in \mathcal{S}} \Tilde{p}(s'|s,a) V_2(s') \Bigg] \nonumber \\
        &\leq \sum_{a \in \mathcal{A}} \pi(a|s)  \sum_{s' \in \mathcal{S}} \Tilde{p}(s'|s,a)[V_1(s') - V_2(s')]
    \end{align}
Since $\Tilde{p}(V_1,V_2) \in \mathcal{P}$ for all $V_1, V_2$, the above problem transforms to the same as the non-robust setting. Thus, by Lemma \ref{lem:span-contraction}, there exists $\beta (V_1,V_2) \in (0,1)$ such that 
\begin{equation}
    \norm{ \mathbf{T}_g(V_1) -  \mathbf{T}_g(V_2) }_{\mathrm{sp}} \leq  \bignorm{\sum_{a \in \mathcal{A}} \pi(a|s)  \sum_{s' \in \mathcal{S}} \Tilde{p}(s'|s,a)[V_1(s') - V_2(s')] }_{\mathrm{sp}} \leq \beta (V_1,V_2)\norm{V_1 - V_2}_{\mathrm{sp}}
\end{equation}
Give that $\cp$ is compact, the supremum is attainable and there exist $\gamma \in (0,1)$ such that for all $V_1, V_2$, we have $\norm{  \mathbf{T}_g(V_1) -  \mathbf{T}_g(V_2) }_{\mathrm{sp}} \leq \gamma\norm{V_1 - V_2}_{\mathrm{sp}}$, which concludes the proof.

\section{Biased Semi-Norm Stochastic Approximation} \label{proofbiasedSA}


We perform analysis of the biased‐noise extension to the span semi-norm stochastic approximation (SA) problem by constructing a smooth convex semi-Lyapunov function for forming the negative drift \citep{zhang2021finite} and using properties in dual norms for managing the bias.
\medskip

\noindent
\subsection{Setup and Notation.} \label{setups}

In this section, we override the notation of the span semi-norm by re-writing it as the sup-norm to the equivalence class of constant vectors. For any norm $\|\cdot\|_c$ and equivalent class $\delta_{\overline{E}}$, define the indicator function  $\delta_{\overline{E}}$ as
\begin{equation}
\delta_{\bar{E}}(x) := 
\begin{cases} 
    0 & x \in \bar{E}, \\
    \infty & \text{{otherwise}.}
\end{cases}
\end{equation}
\noindent then by \citep{zhang2021finite}, the semi-norm induced by norm  $\|\cdot\|_c$  and equivalent class $\overline{E}$ is the infimal convolution of $\|\cdot\|_c$ and the indicator function $\delta_{\overline{E}}$ can be defined as follows 
\begin{equation}
    \|x\|_{c,\overline{E}} \coloneqq (\|\cdot\|_c \ast_{\inf} \delta_{\overline{E}})(x) = \inf_y  (\|x-y\|_c + \delta_{\overline{E}}(y))= \inf_{e\in\overline{E}} \|x-e\|_c \quad \forall x.
\end{equation}
Where $\ast_{\inf}$ denotes the infimal convolution operator. Throughout the remaining section, we let $\overline{E}\coloneqq \{c \mathbf{e} : c \in \mathbb{R}\}$ with $\mathbf{e}$ being the all-ones vector. Then from \citep{gupta2015empirical}, we have for any $x$,
\begin{equation} \label{eq:span2iniftynorm}
    \|x\|_{\mathrm{sp}} = 2 \|x\|_{\infty,\overline{E}}.
\end{equation}

We thus restate our problem of analyzing the iteration complexity for solving the fixed equivalent class equation $H(x^*) - x^* \in \overline{E}$, with the operator \(H:\mathbb{R}^n \to \mathbb{R}^n\) satisfying the contraction property as follows:
\begin{equation}
  \|H(x) - H(y)\|_{\infty,\overline{E}} \leq \gamma\|x - y\|_{\infty,\overline{E}},
  \quad
  \gamma\in(0,1),
  \quad
  \forall x, y
\end{equation}
The stochastic approximation iteration being used is as follows
\begin{equation}\label{eq:SA-updategeneral}
   x^{t+1}= x^t + \eta_t \bigl[\widehat{H}(x^t) - x^t\bigr],
   \quad
   \text{where}\quad
   \widehat{H}(x^t)=  H(x^t) + w^t.
\end{equation}
We assume:
\begin{itemize}
\item \(\mathbb{E}[\,\|w^t\|_{\infty,\overline{E}}^2 | \mathcal{F}^t] \le A + B\|x^t - x^*\|_{\infty,\overline{E}}^2\) (In the robust average reward TD case, $B=0$).
\item \(\bigl\|\mathbb{E}[w^t | \mathcal{F}^t]\bigr\|_{\infty,\overline{E}} \le \varepsilon_{\mathrm{bias}}\).   
\item \(\eta_t>0\) is a chosen step‐size sequence (decreasing or constant).
\end{itemize}

Note that beside the bias in the noise, the above formulation and assumptions are identical to the unbiased setups in Section B of \citep{zhang2021finite}. Thus, we emphasize mostly on managing the bias.


\subsection{Semi‐Lyapunov \texorpdfstring{$M_{\overline{E}}(\cdot)$} M and Smoothness.}

By \citep[Proposition\,1--2]{zhang2021finite}, using the Moreau envelope function $M(x)$ in Definition 2.2 of \citep{chen2020finite}, we define
\[
  M_{\overline{E}}(x)=\bigl(M \ast_{\inf} \delta_{\overline{E}}\bigr)(x),
\]
so that there exist \(c_l,c_u>0\) with
\begin{equation} \label{eq:M2span}
  c_l   M_{\overline{E}}(x) \leq  \frac{1}{2}\|x\|_{\infty,\overline{E}}^2 \leq  c_u  M_{\overline{E}}(x),
\end{equation}

and \(M_{\overline{E}}\) is \(L\)-smooth \emph{w.r.t.\ another semi-norm} \(\|\cdot\|_{s,\overline{E}}\).  
Concretely, \(L\)-smoothness means:
\begin{equation} \label{eq:lsmooth}
  M_{\overline{E}}(y) \leq M_{\overline{E}}(x)+
  \langle \nabla M_{\overline{E}}(x),\,y - x\rangle+\tfrac{L}{2}\,\|\,y-x\|_{s,\overline{E}}^2,
  \quad
  \forall\,x,y.
\end{equation}
Moreover, the gradient of $M_{\overline{E}}$ satisfies $\langle \nabla M_{\overline{E}}(x),\; c\,\mathbf{e}\rangle = 0$ for all $x$, and the dual norm denoted as \(\|\cdot\|_{*,s,\overline{E}}\) is also L-smooth:
\begin{equation}  \label{eq:duallsmooth}
  \| \nabla M_{\overline{E}}(x) - \nabla M_{\overline{E}}(y)\|_{*,s,\overline{E}} \leq 
   L\|\,y-x\|_{s,\overline{E}},
  \quad
  \forall\,x,y.
\end{equation}

Note that since $\|\cdot\|_{s,\overline{E}}$ and $\|\cdot\|_{\infty,\overline{E}}$ are semi-norms on a finite‐dimensional space with the same kernel, there exist $\rho_1,\rho_2>0$ such that
\begin{equation} \label{eq:normequivalence}
  \rho_1\,\|z\|_{\infty,\overline{E}} \leq \|z\|_{s,\overline{E}} \leq \rho_2\,\|z\|_{\infty,\overline{E}},
  \;
  \forall\,z.
\end{equation}
Likewise, their dual norms (denoted \(\|\cdot\|_{*,s,\overline{E}}\) and \(\|\cdot\|_{*,\infty,\overline{E}}\)) satisfy the following:
\begin{equation} \label{eq:dualnormequivalence}
  \frac{1}{\rho_2}\|z\|_{*,s,\overline{E}} \leq \|z\|_{*,\infty,\overline{E}} \leq \frac{1}{\rho_1}\|z\|_{*,s,\overline{E}}, \;
  \forall\,z.
\end{equation}


\subsection{Formal Statement of Theorem \ref{thm:informalbiasedSA}} \label{appendix4biasedSA}

By $L$‐smoothness w.r.t.\ $\|\cdot\|_{s,\overline{E}}$ in \eqref{eq:lsmooth}, for each $t$,
\begin{equation} \label{eq:Mt+1decomposition}
  M_{\overline{E}}(x^{t+1} - x^*) \leq M_{\overline{E}}(x^t - x^*)+\bigl\langle \nabla M_{\overline{E}}(x^t-x^*),x^{t+1}-x^t \bigr\rangle+\tfrac{L}{2}\|\,x^{t+1}-x^t\|_{s,\overline{E}}^2.
\end{equation}
where $x^{t+1}-x^t = \eta_t[\widehat{H}(x^t)-x^t] = \eta_t[H(x^t) + w^t - x^t]$. Taking expectation of the second term of the RHS of \eqref{eq:Mt+1decomposition} conditioned on the filtration $\mathcal{F}^t$ we obtain,
\begin{align} \label{eq:nablaMt+1decomposition}
    \mathbb{E}[\langle \nabla M_{\overline{E}}(x^t - x^*), x^{t+1} - x^t \rangle | \mathcal{F}^t] &= \eta_t \mathbb{E}[\langle \nabla M_{\overline{E}}(x^t - x^*), H(x^t) - x^t + \omega^t \rangle| \mathcal{F}^t] \nonumber\\
    &= \eta_t\langle \nabla M_{\overline{E}}(x^t - x^*), H(x^t) - x^t \rangle + \eta_t\mathbb{E}[\langle \nabla M_{\overline{E}}(x^t - x^*),\omega^t \rangle | \mathcal{F}^t] \nonumber\\
     &= \eta_t\langle \nabla M_{\overline{E}}(x^t - x^*), H(x^t) - x^t \rangle + \eta_t\langle \nabla M_{\overline{E}}(x^t - x^*), \mathbb{E}[\omega^t | \mathcal{F}^t] \rangle .
\end{align}
To analyze the additional bias term $\langle \nabla M_{\overline{E}}(x^t - x^*), \mathbb{E}[\omega^t | \mathcal{F}^t] \rangle$, we use the fact that for any (semi-)norm $\|\cdot\|$ with dual (semi-)norm $\|\cdot\|_*$ (defined by 
$\|u\|_* = \sup\{\langle u,v\rangle : \|v\|\le1\}$),
we have the general inequality
\begin{equation}\label{eq:dualNormIneq}
  \bigl\langle u,\,v\bigr\rangle\leq\|u\|_{*}\,\|v\|,
  \quad
  \forall\,u,v.
\end{equation}
In the biased noise setting, $u=\nabla M_{\overline{E}}(x^t - x^*)$ and $v=\mathbb{E}[w^t| \mathcal{F}^t]$, with $\|\cdot\|=\|\cdot\|_{\infty,\overline{E}}$.  So
\begin{equation} \label{eq:inner_product_bound}
  \bigl\langle 
    \nabla M_{\overline{E}}(x^t - x^*),
    \mathbb{E}[w^t| \mathcal{F}^t]
  \bigr\rangle \leq \bigl\|\nabla M_{\overline{E}}(x^t - x^*)\bigr\|_{*,\,\infty,\overline{E}}\cdot \bigl\|\mathbb{E}[w^t| \mathcal{F}^t]\bigr\|_{\infty,\overline{E}}.   
\end{equation}
Since $\|\mathbb{E}[w^t| \mathcal{F}^t]\|_{\infty,\overline{E}} \le \varepsilon_{\text{bias}}$, it remains to bound 
\(\|\nabla M_{\overline{E}}(x^t - x^*)\|_{*,\infty,\overline{E}}\). By setting 
 $y=0$ in \eqref{eq:duallsmooth}, we get
\begin{equation}  
  \| \nabla M_{\overline{E}}(x) - \nabla M_{\overline{E}}(0)\|_{*,s,\overline{E}} \leq 
   L\|  x \|_{s,\overline{E}},
  \quad
  \forall\,x.
\end{equation}
Thus,
\begin{equation} \label{eq:step1}
     \| \nabla M_{\overline{E}}(x) \|_{*,s,\overline{E}} \leq  \| \nabla M_{\overline{E}}(0) \|_{*,s,\overline{E}} +
   L\| x \|_{s,\overline{E}},
  \quad
  \forall\,x.
\end{equation}
By \eqref{eq:dualnormequivalence}, we know that there exists $\frac{1}{\rho_2}\leq \alpha \leq \frac{1}{ \rho_1}$ such that
\begin{equation} \label{eq:step2}
    \|  \nabla M_{\overline{E}}(x) \|_{*,\infty,\overline{E}} \leq \alpha   \|  \nabla M_{\overline{E}}(x) \|_{*,s,\overline{E}} 
\end{equation}
Thus, combining \eqref{eq:step1} and \eqref{eq:step2} would give:
\begin{equation} 
     \| \nabla M_{\overline{E}}(x) \|_{*,\infty,\overline{E}} \leq  \alpha\big(\| \nabla M_{\overline{E}}(0) \|_{*,s,\overline{E}} +
   L\| x \|_{s,\overline{E}}\big),
  \quad
  \forall\,x.
\end{equation}
By \eqref{eq:normequivalence}, we know that $\| x \|_{s,\overline{E}} \leq \| x \|_{\infty,\overline{E}}$, thus we have:
\begin{equation} 
     \| \nabla M_{\overline{E}}(x) \|_{*,\infty,\overline{E}} \leq  \alpha\big(\| \nabla M_{\overline{E}}(0) \|_{*,s,\overline{E}} +
   L\rho_2\| x \|_{\infty,\overline{E}}\big),
  \quad
  \forall\,x.
\end{equation}
Hence, combining the above with \eqref{eq:inner_product_bound}, there exist some 
\begin{equation} \label{eq:G_value}
G=\mathcal{O}\big(\frac{1}{\rho_1} \max\{L\rho_2, \| \nabla M_{\overline{E}}(0) \|_{*,s,\overline{E}}\} \big)
\end{equation}
such that
\begin{equation}  \label{eq:Gepsilonbias}
  \mathbb{E}
  \Bigl[\langle 
    \nabla M_{\overline{E}}(x^t - x^*),\,w^t
  \rangle|\mathcal{F}^t\Bigr]=\bigl\langle 
    \nabla M_{\overline{E}}(x^t - x^*),
    \mathbb{E}[w^t|\mathcal{F}^t]
  \bigr\rangle\leq G
  \bigl(1 + \|x^t - x^*\|_{\infty,\overline{E}}\bigr)
  \varepsilon_{\text{bias}}.
\end{equation}
Combining \eqref{eq:Gepsilonbias} with \eqref{eq:nablaMt+1decomposition} we obtain
\begin{equation} \label{eq:nablaMt+1decomposition2}
     \mathbb{E}[\langle \nabla M_{\overline{E}}(x^t - x^*), x^{t+1} - x^t \rangle | \mathcal{F}^t] \leq \eta_t\langle \nabla M_{\overline{E}}(x^t - x^*), H(x^t) - x^t \rangle+ \eta_tG\varepsilon_{\text{bias}}  \Bigl(1 + \|x^t - x^*\|_{\infty,\overline{E}}\Bigr)
\end{equation}
To bound the first term in the RHS of \eqref{eq:nablaMt+1decomposition2}, note that
\begin{align} \label{eq:nablaMtdecomposition}
    \langle \nabla M_{\bar{E}}(x^t - x^*), H(x^t) - x^t \rangle 
    &= \langle \nabla M_{\bar{E}}(x^t - x^*), H(x^t) - x^* + x^* - x^t \rangle \nonumber\\
    &\overset{(a)}{\leq}M_{\bar{E}}(H(x^t) - x^*) - M_{\bar{E}}(x^t - x^*) \nonumber\\
    &\overset{(b)}{\leq} \frac{1}{2 c_l} \| H(x^t) - H(x^*) \|^2_{c, \bar{E}} - M_{\bar{E}}(x^t - x^*)\nonumber\\
    &\overset{(c)}{\leq} \frac{\gamma^2}{2 c_l} \| x^t - x^* \|^2_{c, \bar{E}} - M_{\bar{E}}(x^t - x^*) \nonumber\\
    &\leq \left( \frac{\gamma^2 c_u}{c_l} - 1 \right) M_{\bar{E}}(x^t - x^*) \nonumber\\
    &\leq -(1 - \gamma \sqrt{c_u / c_l}) M_{\bar{E}}(x^t - x^*),
\end{align}
where \((a)\) follows from the convexity of $ M_{\bar{E}} $, \((b)\) follows from \( x^* \) belonging to a fixed equivalent class with respect to \( H \) and \((c)\) follows from the contraction property of \( H \). Combining \eqref{eq:nablaMtdecomposition}. \eqref{eq:nablaMt+1decomposition2} and Lemma \ref{lem:zhanglemma6} with \eqref{eq:Mt+1decomposition}, we arrive as follows:
\begin{align} \label{eq:modified_proposition3}
    \mathbb{E}\Bigl[ 
     M_{\overline{E}}(x^{t+1} - x^*) | \mathcal{F}_t\Bigr] & \leq (1-2\alpha_2 \eta_t +\alpha_3\eta_t^2)  M_{\overline{E}}(x^{t} - x^*) + \alpha_4 \eta_t^2 \nonumber \\
    &+ \eta_t G\varepsilon_{\text{bias}}  \Bigl(1 + \|x^t - x^*\|_{\infty,\overline{E}}\Bigr)
\end{align}
Where $\alpha_2 \coloneqq (1-\gamma \sqrt{c_{{u}} / c_{{l}}})$, $\alpha_3 \coloneqq (8+2B)c_{{u}} \rho_2 L$ and $\alpha_4 \coloneqq A\rho_2L$. We now present the formal version of Theorem \ref{thm:informalbiasedSA} as follows:
\begin{theorem}[Formal version of Theorem \ref{thm:informalbiasedSA}] \label{thm:formalbiasedSA}
    let $\alpha_2$,  $\alpha_3$ and  $\alpha_4$ be defined in \eqref{eq:modified_proposition3}, if $x^t$ is generated by \eqref{eq:SA-updategeneral} with all assumptions in \ref{setups} satisfied, then if the stepsize $\eta_t \coloneqq \frac{1}{\alpha_2(t+K)}$ while $K \coloneqq \max\{ \alpha_3/\alpha_2, 3\}$,
    \begin{equation} \label{eq:biasedSAformal}
        \mathbb{E}\Bigl[\|x^T - x^*\|^2_{\infty,\overline{E}}\Bigr] \leq \frac{K^2 c_u}{(T+K)^2c_l} \|x^0 - x^*\|^2_{\infty,\overline{E}} + \frac{8 \alpha_4 c_u}{(T+K)\alpha_2^2} + \frac{2c_u C_1 C_2 \varepsilon_{\text{bias}} }{\alpha_2} 
    \end{equation}
    where $C_1 = G(1+2x_{sp})$, $C_2 = \frac{1}{K} + \log \big(\frac{T-1+K}{K}\big)$, $G$ is defined in \eqref{eq:G_value} and $x_{sp} \coloneqq \sup \|x\|_{\infty,\overline{E}}$ is the upper bound of the span for all $x^t$.
\end{theorem}
\begin{proof}
    This choice $\eta_t$ satisfies $\alpha_3 \eta_t^2 \leq \alpha_2\eta_t$. Thus, by \eqref{eq:modified_proposition3} we have
    \begin{equation}
        \mathbb{E}\bigl[M_{\overline{E}}(x^{t+1} - x^*) | \mathcal{F}_t\Bigr] \leq (1-\alpha_2 \eta_t )  M_{\overline{E}}(x^{t} - x^*) + \alpha_4 \eta_t^2 + \eta_t C_1 \varepsilon_{\text{bias}}
    \end{equation}
    we define $\Gamma_t \coloneqq \Pi_{i=o}^{t-1}(1-\alpha_2\eta_t)$ and further obtain the $T$-step recursion relationship as follows:
    \begin{align}
        \mathbb{E}&\Bigl[M_{\overline{E}}(x^{T} - x^*)\Bigr]\leq \Gamma_T  M_{\overline{E}}(x^{0} - x^*) + \Gamma_T \sum_{t=0}^{T-1} (\frac{1}{\Gamma_{t+1}})[\alpha_4 \eta_t^2 + \eta_t C_1 \varepsilon_{\text{bias}}] \nonumber\\
        & = \Gamma_T  M_{\overline{E}}(x^{0} - x^*) + \Gamma_T \sum_{t=0}^{T-1} (\frac{1}{\Gamma_{t+1}})[\alpha_4 \eta_t^2] + \Gamma_T \sum_{t=0}^{T-1} (\frac{1}{\Gamma_{t+1}})[\eta_t C_1 \varepsilon_{\text{bias}}]\nonumber\\
        & = \underbrace{\Gamma_T  M_{\overline{E}}(x^{0} - x^*) + \frac{\alpha_4\Gamma_T}{\alpha_2} \sum_{t=0}^{T-1} (\frac{1}{\Gamma_{t+1}})[\alpha_2 \eta_t^2]}_{R_1} + \underbrace{\Gamma_T \sum_{t=0}^{T-1} (\frac{1}{\Gamma_{t+1}})[\eta_t C_1 \varepsilon_{\text{bias}}] }_{R_2} \label{eq:R1R2bound}
    \end{align}
    where the term $R_1$ is identical to the unbiased case in Theorem 3 of \citep{zhang2021finite} which leads to
    \begin{equation} \label{eq:R1bound}
        R_1 \leq \frac{K^2}{(T+K)^2}M_{\overline{E}}(x^{0} - x^*) + \frac{4\alpha_2}{(T+K)\alpha_2^2}
    \end{equation}
    also, $R_2$ can be bounded by a logrithmic dependence of $T$
    \begin{equation} \label{eq:R2bound}
        R_2 \leq  \sum_{t=0}^{T-1} [\eta_t C_1 \varepsilon_{\text{bias}}]  = C_1 \varepsilon_{\text{bias}} \sum_{t=0}^{T-1}\frac{1}{\alpha_2(t+K)} \leq \frac{C_1 C_2 \varepsilon_{\text{bias}} }{\alpha_2} 
    \end{equation}
    Combining \eqref{eq:R1bound} and \eqref{eq:R2bound} with \eqref{eq:R1R2bound} would obtain the following:
    \begin{equation} \label{eq:biasedSAM}
    \mathbb{E}\Bigl[M_{\overline{E}}(x^{T} - x^*)\Bigr]\leq  \frac{K^2}{(T+K)^2}M_{\overline{E}}(x^{0} - x^*) + \frac{4\alpha_2}{(T+K)\alpha_2^2}+ \frac{C_1 C_2 \varepsilon_{\text{bias}} }{\alpha_2} 
    \end{equation}
    Combining \eqref{eq:biasedSAM} with \eqref{eq:M2span} yields \eqref{eq:biasedSAformal}.
\end{proof}

\section{Uncertainty Set Support Function Estimators}
\subsection{Proof of Theorem \ref{thm:sample-complexity}} \label{proof:sample-complexity}
We have
\begin{align}
\mathbb{E}[M]&=\sum_{n=0}^{N_{\max}-1} 2^{n+1}\,\mathbb{P}(N'=n)+2^{N_{\max}+1}\mathbb{P}(N' = N_{\max})
\nonumber\\
&=\sum_{n=0}^{N_{\max}-1} 2^{n+1}\,\mathbb{P}(N=n)+2^{N_{\max}+1}\mathbb{P}(N\ge N_{\max})
\nonumber\\
&=\sum_{n=0}^{N_{\max}-1}\Bigl(\frac{ 2^{n+1}}{ 2^{n+1}}\Bigr)+2^{N_{\max}+1}\mathbb{P}(N\ge N_{\max})
\nonumber\\
&=N_{\max}+2^{N_{\max}+1}\mathbb{P}(N\ge N_{\max})
\nonumber\\
&=N_{\max}+\frac{2^{N_{\max}+1}}{2^{N_{\max}}}
\nonumber\\
&=N_{\max}+2 = \cO(N_{\max}).
\end{align}
\subsection{Proof of Theorem \ref{thm:exp-bias}} \label{proof:exp-bias}
    denote $\hat{\sigma}^{*}_{\cp^a_s}(V)$ as the untuncated MLMC estimator obtained by running Algorithm \ref{alg:sampling} when setting $N_{\max}$ to infinity. From \citep{wang2023model}, under both TV uncertainty sets and Wasserstein uncertainty sets, we have $\hat{\sigma}^{*}_{\cp^a_s}(V)$ as an unbiased estimator of ${\sigma}_{\cp^a_s}(V)$. Thus,
    \begin{align}
        \mathbb{E}&\left[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\right]   = \mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V)\right] - \mathbb{E}\left[\hat{\sigma}^{*}_{\cp^a_s}(V)\right] \nonumber\\
        &=\mathbb{E}\left[\sigma_{\hat{\kp}^{a,1}_{s,N'+1}}(V)+\frac{\Delta_{N'}(V)}{  \mathbb{P}(N' = n) }\right] - \mathbb{E}\left[\sigma_{\hat{\kp}^{a,1}_{s,N+1}}(V)+\frac{\Delta_{N}(V)}{  \mathbb{P}(N = n) }\right]  \nonumber\\
        &=\mathbb{E}\left[\frac{\Delta_{N'}(V)}{  \mathbb{P}(N' = n) }\right] - \mathbb{E}\left[\frac{\Delta_{N}(V)}{  \mathbb{P}(N = n) }\right]  \nonumber\\
        &=\sum_{n=0}^{N_{\max}}\Delta_{n}(V) - \sum_{n=0}^{\infty}\Delta_{n}(V)   \nonumber\\
        &= \sum_{n=N_{\max}+1}^{\infty}\Delta_{n}(V)
    \end{align}
    For each $\Delta_n(V)$, the expectation of absolute value can be bounded as
    \begin{align}
        &\mathbb{E}\left[\abs{\Delta_n(V)}\right] =\mathbb{E}\left[\abs{\sigma_{\hat{\kp}^{a}_{s,n+1}}(V)-\sigma_{{\kp}^{a}_{s}}(V)}\right]\nonumber\\
        &+\frac{1}{2}\mathbb{E}\left[\abs{\sigma_{\hat{\kp}^{a,E}_{s,n+1}}(V)-\sigma_{{\kp}^{a}_{s}}(V)}\right]+\frac{1}{2}\mathbb{E}\left[\abs{\sigma_{\hat{\kp}^{a,O}_{s,n+1}}(V)-\sigma_{{\kp}^{a}_{s}}(V)}\right]
    \end{align}
    By the binomial concentration and the Lipschitz property of the support function as in Lemma \ref{lem:LipschitzTV}, we know for TV distance uncertainty, we have
    \begin{equation}  \label{eq:DeltaabsboundTV}
    \mathbb{E}\left[\abs{\Delta_n(V)}\right] \leq 6(1+\frac{1}{\delta}) 2^{-\frac{n}{2}}\|V\|_{\mathrm{sp}}
    \end{equation}  
    and for Wasserstein disance uncertainty, we have
    \begin{equation}  \label{eq:DeltaabsboundWasserstein}
    \mathbb{E}\left[\abs{\Delta_n(V)}\right]  \leq 6\cdot 2^{-\frac{n}{2}}\|V\|_{\mathrm{sp}}
    \end{equation}     
    Thus, for TV distance uncertainty, we have
    \begin{equation} \label{eq:sigmabiasboundTV}
         \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\right] } \leq \sum_{n=N_{\max}+1}^{\infty}\mathbb{E}\left[\abs{\Delta_n(V)}\right]  \leq 6(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}\|V\|_{\mathrm{sp}}
    \end{equation}
    and for Wasserstein distance uncertainty, we have
    \begin{equation} \label{eq:sigmabiasboundWasserstein}
         \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\right] } \leq \sum_{n=N_{\max}+1}^{\infty}\mathbb{E}\left[\abs{\Delta_n(V)}\right]  \leq 6\cdot2^{-\frac{N_{\max}}{2}}\|V\|_{\mathrm{sp}}
    \end{equation}
    
\subsection{Proof of Lemma \ref{lem:LipschitzTV}} \label{proof:LipschitzTV}
    For TV uncertainty sets, for a fixed $V$, for any $p\in \Delta(\mathcal{S})$, define $f_p(\mathbf{\mu}) \coloneqq p(V-\mathbf{\mu}) - \delta\|V-\mathbf{\mu}\|_{\mathrm{sp}}$ and $\mathbf{\mu}_p^* \coloneqq \arg\max_{\mathbf{\mu} \geq \mathbf{0}}f_p(\mathbf{\mu})$. Thus, we have
    \begin{equation} \label{eq:fp-fq}
        \sigma_{\mathcal{P}_{TV}} (V) -  \sigma_{\mathcal{Q}_{TV}} (V) = f_p(\mu_p^*)- f_q(\mu_q^*) 
    \end{equation} 
    since, $\mathbf{\mu}_p^*$ and $\mathbf{\mu}_q^*$ are maximizers of $f_p$ and $f_q$ respectively, we further have
    \begin{equation} \label{eq:fp-fqbound}
     f_p(\mathbf{\mu}_q^*)- f_q(\mathbf{\mu}_q^*)  \leq f_p(\mathbf{\mu}_p^*)- f_q(\mathbf{\mu}_q^*) \leq f_p(\mathbf{\mu}_p^*)- f_q(\mathbf{\mu}_p^*)
    \end{equation}
    Combing \eqref{eq:fp-fq} and \eqref{eq:fp-fqbound} we thus have:
    \begin{align} \label{eq:maxpq}
        |\sigma_{\mathcal{P}_{TV}} (V) -  \sigma_{\mathcal{Q}_{TV}} (V)| &\leq \max\{|f_p(\mathbf{\mu}_p^*)- f_q(\mathbf{\mu}_p^*)|,|f_p(\mathbf{\mu}_q^*)- f_q(\mathbf{\mu}_q^*) | \} \nonumber \\
        &= \max\{|(p-q)(V-\mathbf{\mu}_p^*)|,|(p-q)(V-\mathbf{\mu}_q^*)| \} 
    \end{align}
    Note that $\sigma_{\mathcal{P}_{TV}} (V)$ can also be expressed as $\sigma_{\mathcal{P}_{TV}} (V) = p\mathbf{x}^*-\delta \|\mathbf{x}^*\|_{\mathrm{sp}}$ where $\mathbf{x}^* \coloneqq\arg\max_{\mathbf{x} \leq V}(p\mathbf{x}-\delta \|\mathbf{x}\|_{\mathrm{sp}})$. Let $M\coloneqq \max_s\mathbf{x}^*(s)$ and $m\coloneqq \min_s\mathbf{x}^*(s)$, then $\|\mathbf{x}\|_{\mathrm{sp}} = M-m$. Denote $\mathbf{e}$ as the all-ones vector, then $\mathbf{x}=\min_s V(s) \cdot \mathbf{e}$ is a feasible solution. Thus,
    \begin{equation}
        p\mathbf{x}^*-\delta (M-m) \geq p(\min_s V(s) \cdot \mathbf{e})-\delta \|\min_s V(s) \cdot \mathbf{e}\|_{\mathrm{sp}} = \min_s V(s)
    \end{equation}
    Since $p$ is a probability vector, $p\mathbf{x}^*\leq M$, using the fact that $\delta>0$, we then obtain
    \begin{equation} \label{eq:M-m}
        M-\delta (M-m) \geq \min_s V(s) \Rightarrow M-m \leq \frac{M-\min_s V(s)}{\delta}
    \end{equation}
    Since $\mathbf{x^*}$ is a feasible solution, we have
    \begin{equation} \label{eq:Vspan}
        M \leq \max_s V(s) \Rightarrow M-\min_sV(s) \leq \max_s V(s) -\min_sV(s) =\|V\|_{\mathrm{sp}}
    \end{equation}
    Combining \eqref{eq:M-m} and \eqref{eq:Vspan} we obtain
    \begin{equation}
        M-m\leq\frac{\|V\|_{\mathrm{sp}}}{\delta} \Rightarrow m \geq M - \frac{\|V\|_{\mathrm{sp}}}{\delta} \geq \min_sV(s) - \frac{\|V\|_{\mathrm{sp}}}{\delta}
    \end{equation}
    Where the last inequality is from $M \geq \min_sV(s)$, which is a direct result of \eqref{eq:M-m} and the term $\delta(M-m)$ being positive. We finally arrive with
    \begin{equation}
        \mathbf{x}^*(j) \in [m,M] \subseteq \Big[\min_sV(s) - \frac{\|V\|_{\mathrm{sp}}}{\delta} , \max_sV(s) \Big] \quad \forall j \in \mathcal{S}
    \end{equation}
    Thus, $\|\mathbf{x}^*\|_{\mathrm{sp}} \leq (1+\frac{1}{\delta})\|V\|_{\mathrm{sp}}$, which leads to 
    \begin{equation} \label{eq:V-mubound}
        \|V-\mathbf{\mu}_p^*\|_{\mathrm{sp}} \leq (1+\frac{1}{\delta})\|V\|_{\mathrm{sp}}, \quad \|V-\mathbf{\mu}_q^*\|_{\mathrm{sp}} \leq (1+\frac{1}{\delta})\|V\|_{\mathrm{sp}}
    \end{equation}
    Combining \eqref{eq:V-mubound} with \eqref{eq:maxpq} we obtain \eqref{eq:TVlipschitz}.

    For Wasserstein uncertainty sets, note that for any $p \in \Delta(\mathcal{S})$ and value function $V$,
    \begin{equation}
        \sigma_{\mathcal{P}_{W}}(V) = \sup_{\lambda\geq 0}\left(\overbrace{-\lambda\delta^l+\mathbb{E}_{p}\big[\underbrace{\inf_{y\in\mathcal{S}}\big(V(y)+\lambda d(S,y)^l \big)}_{\phi(s,\lambda)}\big]}^{g(\lambda,p)} \right).    
    \end{equation}
    Note that
    \begin{equation}
        \inf_{y\in\mathcal{S}}V(y) \leq \phi(s,\lambda) \leq V(s)+\lambda d(S,s)^l = V(s)
    \end{equation}
    where the first inequality is because $\lambda d(S,y)^l\geq0$ for any $d$ and $l$. We can then bound $\phi$ by the span of $V$ as
    \begin{equation} 
        |\phi(s,\lambda)| \leq \|V\|_{\mathrm{sp}} \quad \forall \lambda \geq 0
    \end{equation}
    We then further have that for any $p,q\in \Delta(\mathcal{S})$ and $\lambda\geq0$,
    \begin{equation} \label{eq:p-qbound}
        |g(\lambda,p)-g(\lambda,q)| \leq \sum_{s\in\mathcal{S}} |p(s)-q(s)||\phi(s,\lambda)| \leq \|p-q\|_1 \|V\|_{\mathrm{sp}}
    \end{equation}
    using \eqref{eq:p-qbound} and the fact that $|f(\lambda)-g(\lambda)| \leq \epsilon \Rightarrow |\sup_\lambda f(\lambda) - \sup_\lambda g(\lambda)| \leq \epsilon$, we obtain \eqref{eq:Wlipschitz}.

\subsection{Proof of Theorem \ref{thm:linear-variance}} \label{proof:linear-variance}
For all $p\in\Delta(\mathcal{S})$, we have $\sigma_p(V)\leq \|V\|_{\mathrm{sp}}$, leading to 
\begin{equation}
    \mathrm{Var}(\hat{\sigma}_{\cp^a_s}(V)) \leq \mathbb{E}\left[\left(\hat{\sigma}_{\cp^a_s}(V)\right)^2\right] + \|V\|^2_{\mathrm{sp}}
\end{equation}
To bound the second moment, note that 
\begin{align}
    \mathbb{E}\left[\left(\hat{\sigma}_{\cp^a_s}(V)\right)^2\right] &= \mathbb{E}\left[\left(\sigma_{\hat{\kp}^{a,1}_{s,N'+1}}(V)+\frac{\Delta_{N'}(V)}{  \mathbb{P}(N' = n) }\right)^2\right] \nonumber\\
    &\leq\mathbb{E}\left[\left(\|V\|_{\mathrm{sp}}+\frac{\Delta_{N'}(V)}{  \mathbb{P}(N' = n) }\right)^2\right] \nonumber\\
    &\leq 2\|V\|_{\mathrm{sp}}^2 + 2\mathbb{E}\left[\left(\frac{\Delta_{N'}(V)}{  \mathbb{P}(N' = n) }\right)^2\right]\nonumber\\
    &\leq 2\|V\|_{\mathrm{sp}}^2 + 2\sum_{n=0}^{N_{\max}}\left(\frac{\mathbb{E}[|\Delta_{n}(V)|]}{  \mathbb{P}(N' = n) }\right)^2 \mathbb{P}(N' = n)\nonumber\\
    &= 2\|V\|_{\mathrm{sp}}^2 + 2\sum_{n=0}^{N_{\max}}\frac{\mathbb{E}[|\Delta_{n}(V)|]^2}{  \mathbb{P}(N' = n) }
    \end{align}
Under TV distance uncertainty set, by \eqref{eq:DeltaabsboundTV}, we further have
    \begin{align}
    \mathbb{E}\left[\left(\hat{\sigma}_{\cp^a_s}(V)\right)^2\right] &\leq 2\|V\|_{\mathrm{sp}}^2 + 2\sum_{n=0}^{N_{\max}}\frac{36(1+\frac{1}{\delta})^2 2^{-n}\|V\|_{\mathrm{sp}}^2}{2^{-(n+1)}}  \nonumber\\
    &= 2\|V\|_{\mathrm{sp}}^2 + 144(1+\frac{1}{\delta})^2\|V\|_{\mathrm{sp}}^2 N_{\max}
\end{align}
Under Wasserstein distance uncertainty set, by \eqref{eq:DeltaabsboundWasserstein}, we further have
    \begin{align}
    \mathbb{E}\left[\left(\hat{\sigma}_{\cp^a_s}(V)\right)^2\right] &\leq 2\|V\|_{\mathrm{sp}}^2 + 2\sum_{n=0}^{N_{\max}}\frac{36 2^{-n}\|V\|_{\mathrm{sp}}^2}{2^{-(n+1)}}  \nonumber\\
    &= 2\|V\|_{\mathrm{sp}}^2 + 144\|V\|_{\mathrm{sp}}^2 N_{\max}
\end{align}


\section{Convergence for Robust TD} \label{proof:VGresults}
\subsection{Formal Statement of Theorem \ref{thm:Vresult}}
The first half of Algorithm \ref{alg:RobustTD} (line 1 - line 7) can be treated as a special instance of the SA updates in \eqref{eq:SA-updategeneral} with the bias and variance of the i.i.d. noise term specified in Section \ref{QueriesSection}. We start with analyzing the bias and variance of $\hat{\mathbf{T}}_{g_0}(V_t)$ for each $t$. Recall the definition of $\hat{\mathbf{T}}_{g_0}(V_t)$ is as follows:
$$
\hat{\mathbf{T}}_{g_0}(V_t)(s) = \sum_{a} \pi(a|s) \big[ r(s,a) - g_0 +  \hat{\sigma}_{\cp^a_s}(V_t) \big] \quad \forall s \in \mcs
$$
Thus, we have for all $s\in\mcs$,
\begin{equation}
    \abs{\E\left[\hat{\mathbf{T}}_{g_0}(V_t)(s)\right] - {\mathbf{T}}_{g_0}(V_t)(s)} \leq  \sum_{a} \pi(a|s) \abs{\E[\hat{\sigma}_{\cp^a_s}(V_t)] - {\sigma}_{\cp^a_s}(V_t) } = \abs{\E[\hat{\sigma}_{\cp^a_s}(V_t)] - {\sigma}_{\cp^a_s}(V_t) } 
\end{equation}
Which further implies the bias of $\hat{\mathbf{T}}_{g_0}(V_t)$ is bounded by the bias of $\hat{\sigma}_{\cp^a_s}(V_t)$ as follows:
\begin{equation}
\bignorm{\E\left[\hat{\mathbf{T}}_{g_0}(V_t)\right] - {\mathbf{T}}_{g_0}(V_t)}_\infty \leq \abs{\E[\hat{\sigma}_{\cp^a_s}(V_t)] - {\sigma}_{\cp^a_s}(V_t)}
\end{equation}
Regarding the variance, note that
\begin{align}
    \E\left[(\hat{\mathbf{T}}_{g_0}(V_t)(s)-{\mathbf{T}}_{g_0}(V_t)(s))^2\right] &=  \left(\E\left[\hat{\mathbf{T}}_{g_0}(V_t)(s)\right] - {\mathbf{T}}_{g_0}(V_t)(s)\right)^2+ \mathrm{Var}\left(  \hat{\mathbf{T}}_{g_0}(V_t)(s)  \right) \nonumber\\
    &\leq\abs{\E[\hat{\sigma}_{\cp^a_s}(V_t)] - {\sigma}_{\cp^a_s}(V_t)}^2+ \mathrm{Var}\left( \sum_{a} \pi(a|s)  \hat{\sigma}_{\cp^a_s}(V_t)  \right) \nonumber\\
    &=\abs{\E[\hat{\sigma}_{\cp^a_s}(V_t)] - {\sigma}_{\cp^a_s}(V_t)}^2+ \sum_{a} \pi(a|s)^2 \mathrm{Var}\left(  \hat{\sigma}_{\cp^a_s}(V_t)  \right) 
\end{align}
To create an upper bound of $\|V\|_{\mathrm{sp}}$ for all possible $V$, define the mixing time of any $p\in\mathcal{P}$ to be
\begin{equation}
    t^p_{\mathrm{mix}}\coloneqq\arg\min_{t \geq 1} \left\{ \max_{\mu_0} \left\| (\mu_0 p_{\pi}^{t})^{\top} - \nu^{\top} \right\|_1 \leq \frac{1}{2} \right\}
\end{equation}
where $p_\pi$ is the finite state Markov chain induced by $\pi$, $\mu_0$ is any initial probability distribution on $\mcs$ and $\nu$ is its invariant distribution. By Assumption \ref{ass:sameg}, and Lemma \ref{lem:wanglemma9},and for any value function $V$, we have
    \begin{equation}
         t^p_{\mathrm{mix}} < +\infty \quad \text{and} \quad\|V\|_{\mathrm{sp}} \leq 4t^p_{\mathrm{mix}}
    \end{equation}
Thus, define $t_{\mathrm{mix}} \coloneqq \sup_{p\in\mathcal{P}} t^p_{\mathrm{mix}}$, then $t_{\mathrm{mix}}$ is also finite due to the compactness of $\mathcal{P}$. We now derive the bounds of biases and variances for the three types of uncertainty sets. Regarding contamination uncertainty sets, according to Lemma \ref{lem:wangthmD1}, $\hat{\sigma}_{\cp^a_s}(V)$ is unbiased and has variance bounded by $\|V\|^2$. Thu, define $t_{\mathrm{mix}}$ according to \eqref{eq:tmix} and combining the above result with Lemma \ref{lem:wanglemma9}, we obtian that $\hat{\mathbf{T}}_{g_0}(V_t)$ is also unbiased and the variance satisfies
\begin{equation}
    \E\left[\bignorm{\hat{\mathbf{T}}_{g_0}(V_t)-{\mathbf{T}}_{g_0}(V_t)}^2_\infty\right] \leq \|V_t\|^2 \leq 16t^2_{\mathrm{mix}}
\end{equation}
Regarding TV distance uncertainty sets, using the property of the bias and variance of $\hat{\sigma}_{\cp^a_s}(V)$ in Theorem \ref{thm:exp-bias} and Theorem \ref{thm:linear-variance} while combining them with Lemma \ref{lem:wanglemma9}, we have
\begin{equation}
        \bignorm{\E\left[\hat{\mathbf{T}}_{g_0}(V_t)\right]-{\mathbf{T}}_{g_0}(V_t)}_\infty \leq 6(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}\|V\|_{\mathrm{sp}}= 24(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}}
\end{equation}
and
\begin{align}
        \E\left[\bignorm{\hat{\mathbf{T}}_{g_0}(V_t)-{\mathbf{T}}_{g_0}(V_t)}^2_\infty\right] &\leq \left( 24(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \right)^2 + 3\|V\|_{\mathrm{sp}}^2 + 144(1+\frac{1}{\delta})^2\|V\|_{\mathrm{sp}}^2 N_{\max} \nonumber\\
        &\leq \left( 24(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \right)^2 + 48t_{\mathrm{mix}}^2 + 2304(1+\frac{1}{\delta})^2t_{\mathrm{mix}}^2 N_{\max}
\end{align}
Similarly, for Wasserstein distance uncertainty sets, we have
\begin{equation}
        \bignorm{\E\left[\hat{\mathbf{T}}_{g_0}(V_t)\right]-{\mathbf{T}}_{g_0}(V_t)}_\infty \leq 6\cdot 2^{-\frac{N_{\max}}{2}}\|V\|_{\mathrm{sp}}= 24\cdot 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}}
\end{equation}
and
\begin{align} 
        \E\left[\bignorm{\hat{\mathbf{T}}_{g_0}(V_t)-{\mathbf{T}}_{g_0}(V_t)}^2_\infty\right] &\leq \left( 24\cdot 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \right)^2 + 3\|V\|_{\mathrm{sp}}^2 + 144\|V\|_{\mathrm{sp}}^2 N_{\max} \nonumber\\
        &\leq \left( 24\cdot 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \right)^2 + 48t_{\mathrm{mix}}^2 + 2304t_{\mathrm{mix}}^2 N_{\max}
\end{align}
Thus, using the fact that the span is less or equal to two times the $l_\infty$ norm, line 1 - line 7 of Algorithm \ref{alg:RobustTD} can be treated as a special instance of the SA updates in \eqref{eq:SA-updategeneral} with $B=0$. Furthermore, for contamination uncertainty sets, we have
\begin{equation}  \label{eq:firsteq}
    \varepsilon^{\text{Cont}}_{\text{bias}}=0 \quad \text{and} \quad A^{\text{Cont}}=16t^2_{\mathrm{mix}}
\end{equation}
for TV distance uncertainty sets, we have
\begin{equation}
    \varepsilon^{\text{TV}}_{\text{bias}}=48(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} = \cO\left( 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}}   \right)
\end{equation}
\begin{equation}
    A^{\text{TV}}=2\left( 24(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \right)^2 + 96t_{\mathrm{mix}}^2 + 4608(1+\frac{1}{\delta})^2t_{\mathrm{mix}}^2 N_{\max} = \cO\left(  t_{\mathrm{mix}}^2 N_{\max}  \right)
\end{equation}
and for Wasserstein distance uncertainty sets, we have
\begin{equation}
    \varepsilon^{\text{Wass}}_{\text{bias}}=48\cdot 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} = \cO\left( 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}}   \right)
\end{equation}
\begin{equation}  \label{eq:lasteq}
    A^{\text{Wass}}=2\left( 24\cdot 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \right)^2 + 96t_{\mathrm{mix}}^2 + 4608 t_{\mathrm{mix}}^2 N_{\max} = \cO\left(  t_{\mathrm{mix}}^2 N_{\max}  \right)
\end{equation}

\begin{theorem}[Formal version of Theorem \ref{thm:Vresult}]\label{thm:formalVresult}
    Let $\alpha_2 \coloneqq (1-\gamma \sqrt{c_{{u}} / c_{{l}}})$, $\alpha_3 \coloneqq 8c_{{u}} \rho_2 L$ and $\alpha_4 \coloneqq \rho_2L$, if $V_t$ is generated by Algorithm \ref{alg:RobustTD}. Define $V^*$ to be the anchored robust value function $V^* =  V^\pi_{\kp_V} + c \mathbf{e}$ for some $c$ such that $V^*(s_0)=0$, then under Assumption \ref{ass:sameg} and if the stepsize $\eta_t \coloneqq \frac{1}{\alpha_2(t+K)}$ while $K \coloneqq \max\{ \alpha_3/\alpha_2, 3\}$, then for contamination uncertainty sets,
    \begin{equation} 
        \mathbb{E}\Bigl[\|V_T - V^*\|^2_{\mathrm{sp}}\Bigr] \leq \frac{K^2 c_u}{(T+K)^2c_l} \|V_0 - V^*\|^2_{\mathrm{sp}} + \frac{4 A^{\mathrm{Cont}}\alpha_4 c_u}{(T+K)\alpha_2^2}=\cO\left(\frac{1}{T^2}+\frac{t^2_{\mathrm{mix}}}{T(1-\gamma)^2}\right)
    \end{equation}
    for TV distance uncertainty sets,
    \begin{align} 
        \mathbb{E}\Bigl[\|V_T - V^*\|^2_{\mathrm{sp}}\Bigr] &\leq \frac{K^2 c_u}{(T+K)^2c_l} \|V_0 - V^*\|^2_{\mathrm{sp}} + \frac{4A^{\mathrm{TV}} \alpha_4 c_u}{(T+K)\alpha_2^2} + \frac{c_u C_3 C_2 \varepsilon^{\mathrm{TV}}_{\mathrm{bias}} }{\alpha_2} \\
        &=\cO\left(\frac{1}{T^2}+\frac{t^2_{\mathrm{mix}}N_{\max}}{T(1-\gamma)^2} + \frac{t^2_{\mathrm{mix}}  2^{-\frac{N_{\max}}{2}}\log T}{(1-\gamma)^2}\right)
    \end{align}
    for Wasserstein distance uncertainty sets,
    \begin{align} 
        \mathbb{E}\Bigl[\|V_T - V^*\|^2_{\mathrm{sp}}\Bigr] &\leq \frac{K^2 c_u}{(T+K)^2c_l} \|V_0 - V^*\|^2_{\mathrm{sp}} + \frac{4 A^{\mathrm{Wass}} \alpha_4 c_u}{(T+K)\alpha_2^2} + \frac{c_u C_3 C_2 \varepsilon^{\mathrm{Wass}}_{\mathrm{bias}} }{\alpha_2} \\
        &=\cO\left(\frac{1}{T^2}+\frac{t^2_{\mathrm{mix}}N_{\max}}{T(1-\gamma)^2}+\frac{t^2_{\mathrm{mix}}  2^{-\frac{N_{\max}}{2}}\log T}{(1-\gamma)^2}\right)
    \end{align}
    where $C_2 = \frac{1}{K} + \log \big(\frac{T-1+K}{K}\big)$, $C_3 = G(1+8t_{\mathrm{mix}})$, $\gamma$ is defined in \eqref{eq:contractiongamma}, $c_u,c_l$ are defined in \eqref{eq:M2span}, $\rho_2$ is defined in \eqref{eq:normequivalence} and $G$ is defined in \eqref{eq:G_value}.
\end{theorem}
\begin{proof}
    This is the result of substituting the terms of \eqref{eq:firsteq}-\eqref{eq:lasteq} and Theorem \ref{thm:robust_span-contraction} to Theorem \ref{thm:formalbiasedSA}, and relating the span semi-norm to the $\|\cdot\|_{\infty,\overline{E}}$ semi-norm by \eqref{eq:span2iniftynorm}.
\end{proof}

\subsection{Proof of Theorem 7}
We use the result from Theorem \ref{thm:formalVresult}, to set $\mathbb{E}\Bigl[\|V_T - V^*\|^2_{\mathrm{sp}}\Bigr] \leq \epsilon^2$. For contamination uncertainty sets we set 
 $T=\cO\left(\frac{t^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \right)$, resulting in $\cO\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \right)$ sample complexity. For TV and Wasserstein uncertainty set, we set 
$N_{\max}=\cO\left(\log \frac{t_{\mathrm{mix}}}{\epsilon(1-\gamma)}\right)$ and $T=\cO\left(\frac{t^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \log\frac{t_{\mathrm{mix}}}{\epsilon(1-\gamma)}\right)$, conbining with Theorem \ref{thm:sample-complexity}, this would result in $\cO\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \log^2\frac{t_{\mathrm{mix}}}{\epsilon(1-\gamma)}\right)$ sample complexity.
\subsection{Formal Statement of Theorem \ref{thm:gresult}}
To analyze the second part (line 8 - line 14) of Algorithm \ref{alg:RobustTD} and provide the provide the complexity for $g_t$, we first define the noiseless function $\bar{\delta}(V)$ as
\begin{equation} \label{eq:bardeltat}
    \bar{\delta}(V) \coloneqq  \frac{1}{S}\sum_s   \left(\sum_{a}\pi(a|s) \big[ r(s,a) +  {\sigma}_{\cp^a_s}(V) \big]- V(s) \right)
\end{equation}
Thus, we have
\begin{equation}
    \bar{\delta}_t = \bar{\delta}(V_T) + \nu_t
\end{equation}
where $\nu_t$ is the noise term with bias equal to the bias $\hat{\sigma}_{\cp^a_s}(V_T)$
\begin{equation}
    \E[\abs{\nu_t}] =  \frac{1}{S}\sum_s  \sum_{a} \left(\pi(a|s) \E\big[\abs{  {\sigma}_{\cp^a_s}(V_T) - \hat{\sigma}_{\cp^a_s}(V_T)} \big] \right) =  \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }
\end{equation}
By the Bellman equation in Theorem \ref{thm:robust Bellman}, we have $g^\pi_\cp = \bar{\delta}(V^*)$, which implies
\begin{align}
    \abs{\bar{\delta}(V_T) - g^\pi_\cp} &= \abs{\bar{\delta}(V_T) - \bar{\delta}(V^*)} \nonumber\\
    &\leq \frac{1}{S}\sum_s \left( \sum_{a} \pi(a|s)   \abs{{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V^*)}+\abs{V_T(s)-V^*(s)} \right)\nonumber\\
    &\leq \frac{1}{S}\sum_s  \left( \sum_{a} \pi(a|s)   \abs{{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V^*)}+\abs{V_T(s)-V^*(s)} \right)\nonumber\\
    &\leq \frac{1}{S}\sum_s 2\norm{V_T-V^*}_{\mathrm{sp}}\nonumber\\
    &=  2\norm{V_T-V^*}_{\mathrm{sp}}
\end{align}
Thus, the following recursion can be formed
\begin{align}
    \abs{g_{t+1}-g^\pi_\cp}&=\abs{g_{t}+\beta_t(\bar{\delta}_t-g_t)-g^\pi_\cp }\nonumber\\
    &=\abs{g_{t}-g^\pi_\cp  +\beta_t(\bar{\delta}_t-g^\pi_\cp+g^\pi_\cp -g_t)}\nonumber\\
    &=\abs{g_{t}-g^\pi_\cp  +\beta_t(\bar{\delta}(V^T)-g^\pi_\cp + \nu_t+g^\pi_\cp -g_t)}\nonumber\\
    &\leq(1-\beta_t)\abs{g_{t}-g^\pi_\cp}  +\beta_t(\abs{\bar{\delta}(V^T)-g^\pi_\cp} + \abs{\nu_t})\nonumber\\
    &\leq(1-\beta_t)\abs{g_{t}-g^\pi_\cp}  +\beta_t(2\norm{V_T-V^*}_{\mathrm{sp}} + \abs{\nu_t})
\end{align}
Thus, taking expectation conditioned on the filtration $\mathcal{F}^t$ yields
\begin{align}
    \E\left[\abs{g_{t+1}-g^\pi_\cp}\right]&\leq(1-\beta_t)\abs{g_{t}-g^\pi_\cp}  +\beta_t\left(2\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right] + \E[\abs{\nu_t}]\right)\nonumber\\
    &\leq(1-\beta_t)\abs{g_{t}-g^\pi_\cp}  +\beta_t\left(2\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right] + \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }\right)
\end{align}
By letting $\zeta_t \coloneqq \Pi_{i=0}^{t-1}(1-\beta_t)$, we obtain the $T$-step recursion as follows:
\begin{align}
    \E\left[\abs{g_{T}-g^\pi_\cp}\right]&\leq\zeta_T\abs{g_0-g^\pi_\cp}+ \zeta_T \sum_{t=0}^{T-1} (\frac{1}{\zeta_{t+1}})\beta_t\left(2\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right]  + \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }\right) \nonumber\\
        & = \zeta_T\abs{g_0-g^\pi_\cp} +  \sum_{t=0}^{T-1} (\frac{\zeta_T}{\zeta_{t+1}})\beta_t\left(2\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right]  + \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }\right) \nonumber\\
          & \leq \zeta_T\abs{g_0-g^\pi_\cp} +  \sum_{t=0}^{T-1} \beta_t{\left(2\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right]  + \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }\right) }\nonumber\\
          & = \zeta_T\abs{g_0-g^\pi_\cp} + \left(2\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right]  + \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }\right)  \sum_{t=0}^{T-1} \beta_t \label{eq:gTbound}
\end{align}
By setting $\beta_t\coloneqq\frac{1}{t+1}$, we have $\zeta_T= \frac{1}{T+1}\leq\frac{1}{T}$ and $ \sum_{t=0}^{T-1} \beta_t\leq 2\log T$, \eqref{eq:gTbound} implies
\begin{equation} \label{eq:gresult}
    \E\left[\abs{g_{T}-g^\pi_\cp}\right]\leq \frac{1}{T}\abs{g_0-g^\pi_\cp} + \left(4\E\left[\norm{V_T-V^*}_{\mathrm{sp}}\right] + 2\abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V_T) - {\sigma}_{\cp^a_s}(V_T)\right] }\right)  \log T
\end{equation}


\begin{theorem}[Formal version of Theorem \ref{thm:gresult}] \label{thm:formalgresult}
    Following all notations and assumptions in Theorem \ref{thm:formalVresult}, then for contamination uncertainty sets,
    \begin{align} 
        \mathbb{E}\Bigl[\abs{g_{T}-g^\pi_\cp}\Bigr] &\leq\frac{1}{T}\abs{g_0-g^\pi_\cp} + \frac{4K \sqrt{c_u}\log T}{(T+K)\sqrt{c_l}} \|V_0 - V^*\|_{\mathrm{sp}} + \frac{8 \sqrt{A^{\mathrm{Cont}}\alpha_4 c_u}\log T}{\alpha_2\sqrt{T+K}}\\
        &=\cO\left(\frac{1}{T}+\frac{\log T}{T}+\frac{t_{\mathrm{mix}}\log T}{\sqrt{T}(1-\gamma)}\right)
    \end{align}
    for TV distance uncertainty sets,
    \begin{align} 
        \mathbb{E}\Bigl[\abs{g_{T}-g^\pi_\cp}\Bigr] &\leq\frac{1}{T}\abs{g_0-g^\pi_\cp} + \frac{4K \sqrt{c_u}\log T}{(T+K)\sqrt{c_l}} \|V_0 - V^*\|_{\mathrm{sp}} + \frac{8\sqrt{A^{\mathrm{TV}} \alpha_4 c_u}\log T}{\alpha_2\sqrt{T+K}} \nonumber\\
        &+ \frac{4\sqrt{c_u C_3 C_2 \varepsilon^{\mathrm{TV}}_{\mathrm{bias}}}\log T }{\sqrt{\alpha_2}} + 48(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \log T\\
        &=\cO\left(\frac{1}{T}+\frac{\log T}{T}+\frac{t_{\mathrm{mix}}\sqrt{N_{\max}}\log T}{\sqrt{T}(1-\gamma)} + \frac{t_{\mathrm{mix}}  2^{-\frac{N_{\max}}{4}}\log^{\frac{3}{2}} T}{\sqrt{1-\gamma}} +  2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \log T\right)
    \end{align}
    for Wasserstein distance uncertainty sets,
    \begin{align} 
        \mathbb{E}\Bigl[\abs{g_{T}-g^\pi_\cp}\Bigr] &\leq \frac{1}{T}\abs{g_0-g^\pi_\cp} +\frac{4K \sqrt{c_u} \log T}{(T+K)\sqrt{c_l}} \|V_0 - V^*\|_{\mathrm{sp}} + \frac{8 \sqrt{A^{\mathrm{Wass}} \alpha_4 c_u} \log T}{\alpha_2\sqrt{T+K}} \nonumber\\
        &+ \frac{4\sqrt{c_u C_3 C_2 \varepsilon^{\mathrm{Wass}}_{\mathrm{bias}}} \log T}{\sqrt{\alpha_2}} + 48\cdot2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \log T\\
        &=\cO\left(\frac{1}{T}+\frac{\log T}{T}+\frac{t_{\mathrm{mix}}\sqrt{N_{\max}}\log T}{\sqrt{T}(1-\gamma)} + \frac{t_{\mathrm{mix}}  2^{-\frac{N_{\max}}{4}}\log^{\frac{3}{2}} T}{\sqrt{1-\gamma}} +  2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}} \log T\right)
    \end{align}
\end{theorem}
\begin{proof}
    By Theorem \ref{thm:formalVresult}, taking square root on both side and utilizing the concavity of square root function, we have for contamination uncertainty sets,
    \begin{equation} \label{eq:firstequation}
        \mathbb{E}\Bigl[\|V_T - V^*\|_{\mathrm{sp}}\Bigr] \leq \frac{K \sqrt{c_u}}{(T+K)\sqrt{c_l}} \|V_0 - V^*\|_{\mathrm{sp}} + \frac{2\sqrt{ A^{\mathrm{Cont}}\alpha_4 c_u}}{\alpha_2\sqrt{T+K}}
    \end{equation}
    for TV distance uncertainty sets,
    \begin{equation} 
        \mathbb{E}\Bigl[\|V_T - V^*\|_{\mathrm{sp}}\Bigr] \leq \frac{K \sqrt{c_u}}{(T+K)\sqrt{c_l}} \|V_0 - V^*\|_{\mathrm{sp}} + \frac{2\sqrt{A^{\mathrm{TV}} \alpha_4 c_u}}{\alpha_2\sqrt{T+K}} + \sqrt{\frac{c_u C_3 C_2 \varepsilon^{\mathrm{TV}}_{\mathrm{bias}} }{\alpha_2} }
    \end{equation}
    for Wasserstein distance uncertainty sets,
    \begin{equation} 
        \mathbb{E}\Bigl[\|V_T - V^*\|_{\mathrm{sp}}\Bigr] \leq \frac{K \sqrt{c_u}}{(T+K)\sqrt{c_l}} \|V_0 - V^*\|_{\mathrm{sp}} + \frac{2 \sqrt{A^{\mathrm{Wass}} \alpha_4 c_u}}{\alpha_2\sqrt{T+K}} + \sqrt{\frac{c_u C_3 C_2 \varepsilon^{\mathrm{Wass}}_{\mathrm{bias}} }{\alpha_2}}
    \end{equation}
    In addition,  combining \eqref{eq:sigmabiasboundTV}-\eqref{eq:sigmabiasboundWasserstein} with Lemma \ref{lem:wanglemma9}, we have for  for TV distance uncertainty, 
    \begin{equation} 
         \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\right] } \leq 24(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}}
    \end{equation}
    and for Wasserstein distance uncertainty, we have
    \begin{equation} \label{eq:lastequation}
         \abs{\mathbb{E}\left[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\right] } \leq 24\cdot2^{-\frac{N_{\max}}{2}}t_{\mathrm{mix}}
    \end{equation}
Combining \eqref{eq:firstequation}-\eqref{eq:lastequation} with \eqref{eq:gresult} gives the desired result.
\end{proof}
\subsection{Proof of Theorem \ref{thm:gresult}}
We use the result from Theorem \ref{thm:formalgresult}, to set $\mathbb{E}\Bigl[\abs{g_{T}-g^\pi_\cp}\Bigr]  \leq \epsilon$. For contamination uncertainty sets we set 
 $T=\cO\left(\frac{t^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \log \frac{t_\mathrm{mix}}{\epsilon(1-\gamma)}\right)$, resulting in $\cO\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \log \frac{t_\mathrm{mix}}{\epsilon(1-\gamma)}\right)$ sample complexity. For TV and Wasserstein uncertainty set, we set 
$N_{\max}=\cO\left(\log \frac{t_{\mathrm{mix}}}{\epsilon(1-\gamma)}\right)$ and $T=\cO\left(\frac{t^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \log^3\frac{t_{\mathrm{mix}}}{\epsilon(1-\gamma)}\right)$, combining with Theorem \ref{thm:sample-complexity}, this would result in $\cO\left(\frac{SAt^2_{\mathrm{mix}}}{\epsilon^2(1-\gamma)^2} \log^4\frac{t_{\mathrm{mix}}}{\epsilon(1-\gamma)}\right)$ sample complexity.







%%%%%%%%%%% Helper Lemma %%%%%%%%%%%%%%
\section{Some Auxiliary Lemmas for the Proofs}
\begin{lemma}[Lemma 6 in \citep{zhang2021finite}] \label{lem:zhanglemma6}
    Under the setup and notation in Appendix \ref{setups}, if assuming the noise has bounded variance of \(\mathbb{E}[\,\|w^t\|_{\infty,\overline{E}}^2 | \mathcal{F}^t] \le A + B\|x^t - x^*\|_{\infty,\overline{E}}^2\), we have
    \begin{equation}
        \E\left[\|x^{t+1}-x^t\|^2_{s,\overline{E}} | \mathcal{F}^t\right] \leq (16+4B)c_u\rho_2\eta_t^2M_{\overline{E}}(x^t-x^*) + 2A\rho_2\eta_t^2.
    \end{equation}
\end{lemma}
\begin{lemma}[Theorem D.1 in \citep{wang2023model}] \label{lem:wangthmD1}
    The estimator $\hat{\sigma}_{\cp^a_s}(V)$ obtained by \eqref{eq:contaminationquery} for contamination uncertainty sets is unbiased and has bounded variance as follows:
    \begin{equation}
        \E\left[\hat{\sigma}_{\cp^a_s}(V)\right] = {\sigma}_{\cp^a_s}(V), \quad \text{and} \quad \mathrm{Var}(\hat{\sigma}_{\cp^a_s}(V)) \leq  \|V\|^2
    \end{equation}
\end{lemma}
\begin{lemma}[Ergodic case of Lemma 9 in \citep{wang2022near}] \label{lem:wanglemma9}
    For any average reward MDP with stationary policy $\pi$ and the mixing time defined as 
    \begin{equation} \label{eq:tmix}
    \tau_{\mathrm{mix}}\coloneqq\arg\min_{t \geq 1} \left\{ \max_{\mu_0} \left\| (\mu_0 P_{\pi}^{t})^{\top} - \nu^{\top} \right\|_1 \leq \frac{1}{2} \right\}
    \end{equation}
    where $P_\pi$ is the finite state Markov chain induced by $\pi$, $\mu_0$ is any initial probability distribution on $\mcs$ and $\nu$ is its invariant distribution. If $P_\pi$ is irreducible and aperiodic, then $ \tau_{\mathrm{mix}} < +\infty$ and for any value function $V$, we have
    \begin{equation}
        \|V\|_{\mathrm{sp}} \leq 4\tau_{\mathrm{mix}}
    \end{equation}
\end{lemma}