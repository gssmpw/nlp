\section{Robust Bellman Operator}

Motivated by Theorem \ref{thm:robust Bellman}, we define the robust Bellman operator, which forms the basis for our policy evaluation procedure.

\begin{definition}[Robust Bellman Operator, \cite{wang2023model}]
The robust Bellman operator $\mathbf{T}_g$ is defined as:
\begin{equation} \label{eq:bellmanoperator}
    \mathbf{T}_g(V)(s) = \sum_{a} \pi(a|s) \big[ r(s,a) - g +  \sigma_{\cp^a_s}(V) \big], \quad \forall s \in \mathcal{S}.
\end{equation}
\end{definition}

The operator $\mathbf{T}_g$ transforms a candidate value function $V$ by incorporating the worst-case transition effect. A key challenge in solving the robust Bellman equation is that $\mathbf{T}_g$ does not satisfy contraction under standard norms, preventing the use of conventional fixed-point iteration. To cope with this problem, we establish that $\mathbf{T}_g$ is a contraction under the span semi-norm. This allows us to develop provably efficient stochastic approximation algorithms. Throughout this paper, we make the following standard assumption regarding the structure of the induced Markov chain.

\begin{assumption}\label{ass:sameg}
    The Markov chain induced by $\pi$ is irreducible and aperiodic for all $\kp\in\cp$. 
\end{assumption}

Assumption \ref{ass:sameg} is used widely in all robust average reinforcement learning literatures \citep{wang2023robust, wang2023model, wang2024robust,sunpolicy2024}. This assumption ensures that, under any transition model within the uncertainty set, the policy $\pi$ induces a single recurrent communicating class. A well-known result in average-reward MDPs states that under Assumption \ref{ass:sameg}, the average reward is independent of the starting state, i.e., for any $\kp\in\cp$ and all $s,s' \in \mcs$,  
\begin{equation}
    g^\pi_\kp(s) = g^\pi_\kp(s').
\end{equation}
Thus, we can drop the dependence on the initial state and simply write $g^\pi_\kp$ as the robust average reward.

Under Assumption \ref{ass:sameg}, we are able to establish the semi-norm contraction property. Before proceeding, we first establish the semi-norm property of non-robust average reward bellman operator for a policy $\pi$ under transition $\kp$ defined as follows.
\begin{equation} \label{eq:bellmanoperator_nonrobust}
    \mathbf{T}_g^{\kp}(V)(s) = \sum_{a} \pi(a|s) \big[ r(s,a) - g +  \sum_{s'} \kp(s'|s,a)V(s')\big], \quad \forall s \in \mathcal{S}.
\end{equation}


\begin{lemma} \label{lem:span-contraction}
Let $\mathcal{S}$ be a finite state space, and let $\pi$ be a stationary policy. If the Markov chain induced by $\pi$ under the transition $\kp$ is irreducible and aperiodic, there exists a constant $\beta \in (0,1)$ such that for all $V_1, V_2 \in \mathbb{R}^S$ and $g \in \mathbb{R}$
\begin{equation}
\| \mathbf{T}_g^{\kp}(V_1) -  \mathbf{T}_g^{\kp}(V_2)\|_{\mathrm{sp}} \leq \beta \|V_1 - V_2\|_{\mathrm{sp}},
\end{equation}
where
$$
\|v\|_{\mathrm{sp}} \coloneqq \max_{s}\,v(s) - \min_{s}\,v(s).
$$
\end{lemma}
The proof of Lemma \ref{lem:span-contraction} is in Appendix \ref{proofspan-contraction}, where the  properties of irreduible and aperiodic finite state Markov chain is utilized.  Thus we show the (non-robust) average reward bellman operator $\mathbf{T}_g^{\kp}$ is a strict contraction under the span semi-norm. Based on the above results, we now formally establish the contraction property of the robust average reward bellman operator by leveraging Lemma \ref{lem:span-contraction} and the compactness of the uncertainty sets.



\begin{theorem} \label{thm:robust_span-contraction}
     Under Assumption \ref{ass:sameg}, and if $\cp$ is compact, the robust bellman operator $\mathbf{T}_g$ is a contractive mapping with respect to the span semi-norm for any $g$. Specifically, there exist $\gamma \in (0,1)$ such that
\begin{equation} \label{eq:contractiongamma}
\| \mathbf{T}_g(V_1) -  \mathbf{T}_g(V_2)\|_{\mathrm{sp}} \leq \gamma \|V_1 - V_2\|_{\mathrm{sp}}, \quad \forall V_1, V_2 \in \mathbb{R}^S, g\in \mathbb{R}
\end{equation}
where
$$
\|v\|_{\mathrm{sp}} \coloneqq \max_{s}\,v(s) - \min_{s}\,v(s). 
$$
\end{theorem}
The proof of Theorem \ref{thm:robust_span-contraction} is in Appendix \ref{proofrobust-span-contraction}. 
 Since all the uncertainty sets listed in Section \ref{sec:ramdp} are closed and bounded in a real vector space, these uncertainty sets are all compact and satisfy the comtraction property in Theorem \ref{thm:robust_span-contraction}.
