\section{Formulation}


\subsection{Average-reward MDPs.}
An MDP  $(\mathcal{S},\mathcal{A}, \mathsf P, r)$ is specified by: a state space $\mcs$ with $|\mcs|=S$, an action space $\mca$ with $|\mca|=A$, a transition kernel $\mathsf P=\left\{\kp^a_s \in \Delta(\mcs), a\in\mca, s\in\mcs\right\}$\footnote{$\Delta(\mcs)$ denotes the $(|\mcs|-1)$-dimensional probability simplex on $\mcs$. }, where $\kp^a_s$ is the distribution of the next state over $\mcs$ upon taking action $a$ in state $s$, and a reward function $r: \mcs\times\mca \to [0,1]$. At each time step $t$, the agent at state $s_t$ takes an action $a_t$, the  environment then transitions to the next state $s_{t+1}\sim \kp^{a_t}_{s_t}$, and provides a reward signal $r_t\in [0,1]$. 

A stationary policy $\pi: \mcs\to \Delta(\mca)$ maps a state to a distribution over $\mca$, following which the agent takes action $a$ at state $s$ with probability $\pi(a|s)$. 
Under a transition kernel $\kp$, the average-reward of $\pi$ starting from $s\in\mcs$ is defined as
\begin{align}
    g_\kp^\pi(s)\triangleq \lim_{T\to\infty} \mE_{\pi,\kp}\bigg[\frac{1}{T}\sum^{T-1}_{n=0} r_t|S_0=s \bigg].
\end{align}
The relative value function is defined to measure the cumulative difference between the reward and  $g^\pi_\kp$:
\begin{align}\label{eq:relativevaluefunction}
    V^\pi_\kp(s)\triangleq \mE_{\pi,\kp}\bigg[\sum^\infty_{t=0} (r_t-g^\pi_\kp)|S_0=s \bigg].
\end{align}
Then $(g^\pi_\kp, V^\pi_\kp)$  satisfies the following Bellman equation \citep{puterman1994markov}:
\begin{align}
    V^\pi_\kp(s)=\mE_{\pi,\kp}\bigg[r(s,A)-g^\pi_\kp(s)+\sum_{s'\in\mcs} p^A_{s,s'}V^\pi_\kp (s') \bigg]. 
\end{align}

\subsection{Robust average-reward MDPs.} \label{sec:ramdp}
For robust MDPs, the transition kernel is assumed to be in some uncertainty set $\mathcal{P}$. At each time step, the environment transits to the next state according to an arbitrary transition kernel $\kp\in\cp$. In this paper, we focus on the $(s,a)$-rectangular compact uncertainty set \citep{nilim2004robustness,iyengar2005robust}, i.e., $\mathcal{P}=\bigotimes_{s,a} \mathcal{P}^a_s$, where $\mathcal{P}^a_s \subseteq \Delta(\mcs)$. Popular uncertainty sets include those defined by the contamination model \citep{hub65,wang2022policy},  total variation \citep{lim2013reinforcement}, Chi-squared divergence \citep{iyengar2005robust} and Wasserstein distance \citep{gao2022distributionally}.

We investigate the worst-case average-reward over the uncertainty set of MDPs. Specifically, define the  robust average-reward of a policy $\pi$ as 
\begin{align}\label{eq:Vdef}
    g^\pi_\cp(s)\triangleq \min_{\kappa\in\bigotimes_{n\geq 0} \mathcal{P}} \lim_{T\to\infty}\mathbb{E}_{\pi,\kappa}\left[\frac{1}{T}\sum^{T-1}_{t=0}r_t|S_0=s\right],
\end{align}
where $\kappa=(\mathsf P_0,\mathsf P_1...)\in\bigotimes_{n\geq 0} \mathcal{P}$. It was shown in \citep{wang2023robust} that the worst case under the time-varying model is equivalent to the one under the stationary model:
\begin{align}\label{eq:5}
    g^\pi_\cp(s)= \min_{\kp\in\mathcal{P}} \lim_{T\to\infty}\mathbb{E}_{\pi,\kp}\left[\frac{1}{T}\sum^{T-1}_{t=0}r_t|S_0=s\right].
\end{align}
Therefore, we limit our focus to the stationary model. We refer to the minimizers of \eqref{eq:5} as the worst-case transition kernels for the policy $\pi$, and denote the set of all possible worst-case transition kernels by $\Omega^\pi_g$, i.e., $\Omega^\pi_g \triangleq \{\kp\in\cp: g^\pi_\kp=g^\pi_\cp \}$.

We focus on the model-free setting, where only samples from the nominal MDP denoted as $\kp$ (the centroid of the uncertainty set) are available. We investigate the problem of robust policy evaluation and robust average reward estimation, which means for a given policy $\pi$, we aim to estimate the robust value function and the robust average reward. We now formally define the robust value function $ V^\pi_{\kp_V}$ by connecting it with the following robust Bellman equation: 

\begin{theorem}[Robust Bellman Equation, Theorem 3.1 in \citep{wang2023model}]\label{thm:robust Bellman} 
If $(g,V)$ is a solution to the robust Bellman equation
\begin{equation}\label{eq:bellman}
    V(s) = \sum_{a} \pi(a|s) \big(r(s,a) - g + \sigma_{\cp^a_s}(V) \big), \quad \forall s \in \mathcal{S},
\end{equation}
where $\sigma_{\cp^a_s}(V) = \min_{p\in\cp^a_s} p V$, then the following properties hold:
\begin{enumerate}
    \item The scalar $g$ corresponds to the robust average reward, i.e., $g = g^\pi_\cp$.
    \item The worst-case transition kernel $\kp_V$ belongs to the set of minimizing transition kernels, i.e., $\kp_V \in \Omega^\pi_g$, where 
    \begin{equation}
        \Omega^\pi_g \triangleq \{ \kp \in \cp : g^\pi_\kp = g^\pi_\cp \}.
    \end{equation}
    \item The function $V$ is unique up to an additive constant, where if $V$ is a solution to the bellman equation, then we have 
    \begin{equation}
         V = V^\pi_{\kp_V} + c \mathbf{e},
    \end{equation}
    where $c \in \mathbb{R}$ and $\mathbf{e}$ is the all-ones vector in $\mathbb{R}^{|\mcs|}$.
\end{enumerate}
\end{theorem}

This robust Bellman equation characterizes the worst-case value function under the uncertainty set. In particular, $\sigma_{\cp^a_s}(V)$ represents the worst-case transition effect over the uncertainty set $\cp^a_s$. Unlike the robust discounted case, where the contraction property of the Bellman operator under the sup-norm enables straightforward fixed-point iteration, the robust average-reward Bellman equation does not induce contraction under any norm, making direct iterative methods inapplicable. We now characterize the explicit forms of $\sigma_{\cp^a_s}(V)$ for different compact uncertainty sets are as follows:

\noindent \textbf{Contamination Uncertainty Set}\label{sec:con}
The $\delta$-contamination uncertainty set is
$
    \cp^a_s=\{(1-\delta)\kp^a_s+\delta q: q\in\Delta(\mcs) \}, 
$
where $0<\delta<1$ is the radius. Under this uncertainty set, the support function can be computed as 
\begin{equation}\label{eq:contamination}
    \sigma_{\cp^a_s}(V)=(1-\delta)\kp^a_s V+\delta \min_s V(s),
\end{equation}
and this is linear in the nominal transition kernel $\kp^a_s$. 

\noindent \textbf{Total Variation Uncertainty Set.}
The total variation uncertainty set is  
$
    \cp^a_s=\{q\in\Delta(|\mcs|): \frac{1}{2}\|q-\kp^a_s\|_1\leq \delta \},
$
define $\| \cdot \|_{\mathrm{sp}}$ as the span semi-norm and the support function can be computed using its dual function \cite{iyengar2005robust}: 
\begin{align}\label{eq:tv dual}
    \sigma_{\cp^a_s}(V)%&=\mE_{\kp^a_s}[V(S)]+\max_{\mu\geq 0}\big(-\kp^a_s\mu-\delta \spa(V-\mu) \big)\nn\\
    &=\max_{\mu \geq \mathbf{0}}\big(\kp^a_s(V-\mu)-\delta \|V-\mu\|_{\mathrm{sp}}  \big).
\end{align}
\textbf{Wasserstein Distance Uncertainty Sets.}
Consider the metric space $(\mathcal{S},d)$ by defining some distance metric $d$. For some parameter $l\in[1,\infty)$ and two distributions $p,q\in\Delta(\mathcal{S})$, define the $l$-Wasserstein distance between them as 
$W_l(q,p)=\inf_{\mu\in\Gamma(p,q)}\|d\|_{\mu,l}$, where $\Gamma(p,q)$ denotes the distributions over $\mathcal{S}\times\mathcal{S}$ with marginal distributions $p,q$, and $\|d\|_{\mu,l}=\big(\mE_{(X,Y)\sim \mu}\big[d(X,Y)^l\big]\big)^{1/l}$. The Wasserstein distance uncertainty set is then defined as 
\begin{align}
    \cp^a_s=\left\{q\in\Delta(|\mcs|): W_l(\kp^a_s,q)\leq \delta \right\}.
\end{align}
The support function w.r.t. the Wasserstein distance set, can be calculated as follows \citep{gao2023distributionally}:

\begin{align}\label{eq:wd dual}
    &\sigma_{\cp^a_s}(V)=\sup_{\lambda\geq 0}\left(-\lambda\delta^l+\mE_{\kp^a_{s}}\big[\inf_{y}\big(V(y)+\lambda d(S,y)^l \big)\big] \right).
\end{align}

