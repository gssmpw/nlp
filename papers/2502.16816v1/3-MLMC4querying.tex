\section{Queries from Uncertainty Set} \label{QueriesSection}
In this section, we aim to construct an estimator $\hat{\sigma}_{\cp^a_s}(V)$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$ in various uncertainty sets. Recall that the support function ${\sigma}_{\cp^a_s}(V)$ represents the worst-case transition effect over the uncertainty set $\cp^a_s$ as defined in the robust Bellman equation in Theorem \ref{thm:robust Bellman}. The explicit forms of ${\sigma}_{\cp^a_s}(V)$ for different uncertainty sets were characterized in \eqref{eq:contamination}-\eqref{eq:wd dual}. Our goal in this section is to construct efficient estimators $\hat{\sigma}_{\cp^a_s}(V)$ that approximates ${\sigma}_{\cp^a_s}(V)$ while maintaining controlled variance and finite sample complexity.

\subsection{Linear Contamination Uncertainty Set}
Recall that the $\delta$-contamination uncertainty set is
$
    \cp^a_s=\{(1-\delta)\kp^a_s+\delta q: q\in\Delta(\mcs) \}, 
$
where $0<\delta<1$ is the radius. Since the support function can be computed by \eqref{eq:contamination} and the expression is linear in the nominal transition kernel $\kp^a_s$. A direct approach is to use the transition to the subsequent state to construct our estimator:
\begin{align}\label{eq:contaminationquery}
    \hat{\sigma}_{\cp^a_s}(V)\triangleq (1-\delta) V(s')+\delta\min_x V(x),
\end{align}
where $s'$ is a subsequent state sample after $(s,a)$. Hence, the sample complexity of \eqref{eq:contaminationquery} is just one. A well know result from \citep{wang2023model} is that $\hat{\sigma}_{\cp^a_s}(V)$ obtained by \eqref{eq:contaminationquery} is unbiased and has bounded variance as follows:
\begin{equation}
        \E\left[\hat{\sigma}_{\cp^a_s}(V)\right] = {\sigma}_{\cp^a_s}(V), \quad \text{and} \quad \mathrm{Var}(\hat{\sigma}_{\cp^a_s}(V)) \leq  \|V\|^2
\end{equation}


\subsection{Non-Linear Uncertainty Sets}
Non-linear uncertainty sets such as TV distance uncertainty set and Wasserstein distance uncertainty sets have a non-linear relationship between the nonminal distribution $\kp^a_s$ and the support function ${\sigma}_{\cp^a_s}(V)$. Previous works such as \citep{blanchet2015unbiased,blanchet2019unbiased, wang2023model} have proposed a multi-level Monte-Carlo (MLMC) method for obtaining an unbiased estimator  of ${\sigma}_{\cp^a_s}(V)$ with bounded variance. However, their approach all require drawing $2^{N+1}$ samples where $N$ is sampled from a geometric distribution $\mathrm{Geom}(\Psi)$ with parameter $\Psi \in (0,0.5)$. This operation would need infinite samples in expectation for obtaining each single estimator:
\begin{equation}
    \mathbb{E}[2^{N+1}] = \sum^{\infty}_{N=0} 2^{N+1} \Psi(1-\Psi)^N  = \sum^{\infty}_{N=0} 2\Psi(2-2\Psi)^N \rightarrow \infty
\end{equation}
To handle the above problem, we aim to provide an estimator $\hat{\sigma}_{\cp^a_s}(V)$ with finite sample complexity and small enough bias. We construct a level-MLMC estimator under geometric sampling with parameter $\Psi=0.5$ as shown in Algorithm \ref{alg:sampling}.

\begin{algorithm}[htb]
\caption{Truncated MLMC Estimator for TV and Wasserstein Unceretainty Sets}
\label{alg:sampling}
\textbf{Input}: $s\in \mathcal{S}$, $a\in\mathcal{A}$,  Truncation level $N_{\max}$, Value function $V$
\begin{algorithmic}[1] 
\State Sample $N \sim \mathrm{Geom}(0.5)$
\State $N' \leftarrow \min \{N, N_{\max}\}$
\State Collect $2^{N'+1}$ i.i.d. samples of $\{s'_i\}^{2^{N'+1}}_{i=1}$ with $s'_i \sim \kp^a_s$ for each $i$
\State $\hat{\kp}^{a,E}_{s,N'+1} \leftarrow \frac{1}{2^{N'}}\sum_{i=1}^{2^{N'}} \mathbbm{1}_{\{s'_{2i}\}}$
\State $\hat{\kp}^{a,O}_{s,N'+1} \leftarrow \frac{1}{2^{N'}}\sum_{i=1}^{2^{N'}} \mathbbm{1}_{\{s'_{2i-1}\}}$
\State $\hat{\kp}^{a}_{s,N'+1}\leftarrow\frac{1}{2^{N'+1}}\sum_{i=1}^{2^{N'+1}} \mathbbm{1}_{\{s'_i\}}$
\State $\hat{\kp}^{a,1}_{s,N'+1} \leftarrow \mathbbm{1}_{\{s'_1\}}$
\If{TV distance uncertainty set} Obtain $\sigma_{\hat{\kp}^{a,1}_{s,N'+1}}(V), \sigma_{\hat{\kp}^{a}_{s,N'+1}}(V), \sigma_{\hat{\kp}^{a,E}_{s,N'+1}}(V), \sigma_{\hat{\kp}^{a,O}_{s,N'+1}}(V)$ from \eqref{eq:tv dual}
\ElsIf{Wasserstein distance uncertainty set} Obtain $\sigma_{\hat{\kp}^{a,1}_{s,N'+1}}(V), \sigma_{\hat{\kp}^{a}_{s,N'+1}}(V), \sigma_{\hat{\kp}^{a,E}_{s,N'+1}}(V), \sigma_{\hat{\kp}^{a,O}_{s,N'+1}}(V)$ from \eqref{eq:wd dual}
\EndIf
\State $\Delta_{N'}(V)\leftarrow \sigma_{\hat{\kp}^{a}_{s,N'+1}}(V)-\frac{1}{2}\Bigl[ \sigma_{\hat{\kp}^{a,E}_{s,N'+1}}(V)+  \sigma_{\hat{\kp}^{a,O}_{s,N'+1}}(V)
\Bigr]$
\State $\hat{\sigma}_{\cp^a_s}(V)\leftarrow\sigma_{\hat{\kp}^{a,1}_{s,N'+1}}(V)+\frac{\Delta_{N'}(V)}{  \mathbb{P}(N' = n) },
\text{where }
p'(n) = \mathbb{P}(N' = n)$
\Return $\hat{\sigma}_{\cp^a_s}(V)$
\end{algorithmic}
\end{algorithm}


In particular, if $n<N_{\max}$, then $\{N'=n\}=\{N=n\}$ with probability $(\tfrac12)^{n+1}$, while $\{N'=N_{\max}\}$ has probability $\sum_{m=N_{\max}}^\infty (1/2)^{m+1} = 2^{-N_{\max}}$. After obtaining $N'$, Algorithm \ref{alg:sampling} then collects a set of $2^{N'+1}$ i.i.d. samples from the nominal transition model to construct empirical estimators for different transition distributions. The core of the approach lies in computing the support function estimates for TV and Wasserstein uncertainty sets using a correction term $\Delta_{N'}(V)$, which accounts for the bias introduced by truncation. This correction ensures that the final estimator maintains a low bias while achieving a finite sample complexity. We now present several crucial properties of Algorithm \ref{alg:sampling}.


\subsubsection{Sample Complexity for Querying Non-Linear Uncertainty Sets}

\begin{theorem}[Finite Sample Complexity]
\label{thm:sample-complexity}
Under Algorithm \ref{alg:sampling}, denote $M = 2^{N'+1}$
as the random number of samples (where $N'=\min\{N,N_{\max}\}$).  Then
\begin{equation}
\mathbb{E}[M]=N_{\max}+2=\mathcal{O}(N_{\max}).
\end{equation}
\end{theorem}
The proof of Theorem \ref{thm:sample-complexity} is in Appendix \ref{proof:sample-complexity}, which demonstrates that setting the geometric sampling parameter to $\Psi=0.5$  ensures that the expected number of samples follows a linear growth pattern rather than an exponential one. This choice precisely cancels out the effect of the exponential sampling inherent in the truncated MLMC estimator, preventing infinite expected sample complexity. This result shows that the expected number of queries grows only linearly with $N_{\max}$, ensuring that the sampling cost remains manageable even for large truncation levels. The key factor enabling this behavior is setting the geometric distribution parameter to $0.5,$ which balances the probability mass across different truncation levels, preventing an exponential increase in sample complexity.


\subsubsection{Exponential Bias Decay}

\begin{theorem}[Exponentially Decaying Bias]
\label{thm:exp-bias}
Let $\hat{\sigma}_{\cp^a_s}(V)$ be the estimator of ${\sigma}_{\cp^a_s}(V)$ obtained from Algorithm \ref{alg:sampling} the under TV uncertainty set, we have:
\begin{equation}
\abs{\mathbb{E}\bigl[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\bigr] } \leq
6(1+\frac{1}{\delta}) 2^{-\frac{N_{\max}}{2}}\|V\|_{\mathrm{sp}}
\end{equation}
and under Wasserstein uncertainty set, we have:
\begin{equation}
\abs{\mathbb{E}\bigl[\hat{\sigma}_{\cp^a_s}(V) - {\sigma}_{\cp^a_s}(V)\bigr] } \leq
6\cdot 2^{-\frac{N_{\max}}{2}}\|V\|_{\mathrm{sp}}
\end{equation}
\end{theorem}
Theorem \ref{thm:exp-bias} establishes that the bias of the truncated MLMC estimator decays exponentially with $N_{\max}$, ensuring that truncation does not significantly affect accuracy. This result follows from observing that the deviation introduced by truncation can be expressed as a sum of differences between support function estimates at different level, and each of which is controlled by the $\ell_1$-distance between transition distributions. Thus, we can use binomial concentration property to ensure the exponentially decaying bias.

The proof of Theorem \ref{thm:exp-bias} is in Appendix \ref{proof:exp-bias}. One important lemma used in the proof is the following Lemma \ref{lem:LipschitzTV}, where we show the Lipschitz property for both TV and Wasserstein distance uncertainty sets.

\begin{lemma}
\label{lem:LipschitzTV}
For any $p,q \in \Delta(\mathcal{S})$, let $\mathcal{P}_{TV}$ and $\mathcal{Q}_{TV}$ denote the TV distance uncertainty set with radius $\delta$ centering at $p$ and $q$ respectively, and let $\mathcal{P}_{W}$ and $\mathcal{Q}_{W}$ denote the Wasserstein distance uncertainty set with radius $\delta$ centering at $p$ and $q$ respectively. Then for any value function $V$, we have:
\begin{equation} \label{eq:TVlipschitz}
|\sigma_{\mathcal{P}_{TV}} (V) - \sigma_{\mathcal{Q}_{TV}} (V)| \leq (1+\frac{1}{\delta})\|V\|_{\mathrm{sp}}\|p-q\|_1 
\end{equation}
\begin{equation} \label{eq:Wlipschitz}
|\sigma_{\mathcal{P}_{W}} (V) - \sigma_{\mathcal{Q}_{W}} (V)| \leq \|V\|_{\mathrm{sp}}\|p-q\|_1 
\end{equation}
\end{lemma}
We refer the proof of Theorem \ref{thm:exp-bias} to Appendix \ref{proof:LipschitzTV}.

\subsubsection{Linear Variance}

\begin{theorem}[Linear Variance]
\label{thm:linear-variance}
Let $\hat{\sigma}_{\cp^a_s}(V)$ be the estimator of ${\sigma}_{\cp^a_s}(V)$ obtained from Algorithm \ref{alg:sampling} then under TV distance uncertainty set, we have:
\begin{equation}
 \mathrm{Var}(\hat{\sigma}_{\cp^a_s}(V)) \leq  3\|V\|_{\mathrm{sp}}^2 + 144(1+\frac{1}{\delta})^2\|V\|_{\mathrm{sp}}^2 N_{\max}
\end{equation}
and under Wasserstein distance uncertainty set, we have:
\begin{equation}
 \mathrm{Var}(\hat{\sigma}_{\cp^a_s}(V)) \leq  3\|V\|_{\mathrm{sp}}^2 + 144\|V\|_{\mathrm{sp}}^2 N_{\max}
\end{equation}
\end{theorem}

Theorem \ref{thm:linear-variance} establishes that the variance of the truncated MLMC estimator grows linearly with $N_{\max}$, ensuring that the estimator remains stable even as the truncation level increases.
The proof of Theorem \ref{thm:linear-variance} is in Appendix \ref{proof:linear-variance}, which follows from bounding the second moment of the estimator by analyzing the variance decomposition 
across different MLMC levels. Specifically, by expressing the estimator in terms of successive refinements of the transition model, we show that the variance accumulates additively across levels due to the binomial concentration property.