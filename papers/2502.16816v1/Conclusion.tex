\section{Conclusion}
This paper provides the first finite-sample analysis for policy evaluation in robust average-reward MDPs, 
bridging a gap where only asymptotic guarantees existed. By introducing a biased stochastic approximation framework and leveraging the properties of various uncertainty sets, we establish finite-time convergence under biased noise. Our algorithm achieves an order-optimal sample complexity of  $\tilde{\mathcal{O}}(\epsilon^{-2})$ for policy evaluation, despite the added complexity of robustness.

A crucial step in our analysis is proving that the robust Bellman operator is contractive under the span 
semi-norm, ensuring the validity of stochastic approximation updates. We further develop a truncated 
multi-level Monte Carlo estimator that efficiently computes worst-case value functions under total variation and Wasserstein uncertainty, while keeping bias and variance controlled. Our results confirm that robust policy evaluation can be achieved with near-optimal efficiency, comparable to standard stochastic approximation methods.