\section{Related Work}

\vspace{-0.3cm}
\paragraph{Synthetic Training Data from Generative Models.}
% Early work used class-conditioned Generative Adversarial Networks (GANs) to generate synthetic training images \citep{gansynth1, gansynth2, gansynth3}. More recently as diffusion has become dominant for image generation, most works utilize text-to-image diffusion models for synthetic training data. 
Previous works using diffusion models has found that only using text class labels for image generation results in synthetic training datasets that cannot match the performance of real image datasets, mainly due to domain gap between real and synthetic images \citep{is_synthetic_data, fake_it}. 
The domain gap issue is somewhat remedied by fine-tuning the diffusion model on real images \citep{syntheticdataimagenet}. However, fine-tuning diffusion models is computationally expensive or infeasible in classification settings where real images of class concepts are rare.\looseness-1

\vspace{-0.3cm}
\paragraph{Diffusion-Based Image Augmentations.}
Promising classification results have been shown in existing work that uses diffusion models to edit or augment real images rather than fully generate synthetic images. 
These methods use diffusion models to introduce visual diversity to real images then perform few-shot fine-tuning of pretrained classifiers on generated images. 
Existing work has used a large language model to guide diffusion model image editing \cite{diversify} or used textual inversion \cite{text_inversion} to fine-tune the diffusion model and learn realistic representations of classes for each image generation \citep{da-fusion}. 
Inspired by these diffusion augmentation methods, we experiment with conditioning diffusion on augmented real images, rather than using diffusion to augment images. 
This avoids the expensive fine-tuning of the diffusion model or using models other than the image generator, but still introduces visual diversity by leveraging classical vision augmentations.\looseness-1 

\vspace{-0.3cm}
\paragraph{Synthetic Images for Long-Tail Classification.}
Long-tail classification is the setting where most training classes have few examples, and additionally the examples per class are imbalanced but the test set is balanced. This classification setting occurs in the real world when class concepts are rare or difficult to photograph \citep{inaturalist, imagenetLT}. Many methods not involving synthetic training data have approached this problem with various training loss and representation learning approaches \citep{decouple-lt, balanced_softmax, imagenetLT}. 
We apply augmentation-conditioned generations to long-tail classification, to explore their efficacy as training data when training classifiers from scratch.\looseness-1

To our knowledge, only two other works have applied diffusion-based image generation to long-tail classification benchmarks. \cite{fill-up-lt} uses textual inversion \cite{text_inversion}, a training technique that teaches the diffusion model about a particular visual concept from the real training images, to balance the amount of training images per-class. \cite{feedbackguided} also balances the number of training images for each class with synthetic images; their generation method uses classification performance from a separate, pretrained classifier in the diffusion guidance term as well as conditions on the text class label and a real training image. 
\cite{best_non_synthetic_imagenetlt} uses traditional vision augmentations (without a diffusion model) to create training data, however our method outperforms it.\looseness-1

\vspace{-0.3cm}
\paragraph{Data Augmentation in Computer Vision}
Image augmentation has long been a core component of training deep vision models, known to reduce overfitting and encourage generalization \citep{imagenet_classification, rand_aug, mixup, cutmix}. A variety of existing augmentations that leverage color and geometric transformations on images are known to increase classification robustness on vision benchmark datasets and are considered a standard part of training. 
% Various image translations and reflections as well as altering RGB intensities are effective for ImageNet \citep{imagenet_classification}. 
CutMix, i.e. randomly cutting and pasting pixels between training images while proportionally mixing image labels, is an effective localized augmentation method \citep{cutmix}.
Mixup, i.e. convex combinations of images and their labels, is a form of data interpolation that increases robustness to adversarial examples and training stability of generative adversarial networks \citep{mixup}.
More recently, the learned augmentation method RandAugment, which composes various geometric and color transformations, has become widely used in vision \citep{rand_aug}. 
We leverage CutMix and MixUp in the conditioning information of diffusion, which effectively introduces diversity to our generations. One of our few-shot baselines is a direct comparison to data generated via RandAugment.\looseness-1 
%\vspace{-0.25cm}