\section{Augmentation-Conditioned Generations}

\begin{figure}[ht!]
    % \hspace*{-1cm} 
    \centering
    \includegraphics[width=1.0\textwidth]{img/method.png}
    \vspace{-0.5cm}
    \caption{Our augmentation-conditioned generation conditions the reverse diffusion process on the class label and an augmented real image, introducing visual diversity that improves the performance of the downstream classifier. \looseness-1}
    \label{fig:method}
     \vspace{-0.4cm}
\end{figure}

\vspace{-0.2cm}
%%AZ.9.22: To be safe, I would find and replace everywhere we say "our method."
Generations must be in-domain and realistic to facilitate effective classifier learning, to enforce this we condition the diffusion process on real training images. 
Visually diverse training data adds robustness to classification, and we leverage data augmentations in the conditioning information of the diffusion process to make our generations more diverse. 
Given labeled training images, we apply vision augmentations and use the augmented images as conditioning information for the diffusion process, resulting in synthetic images that are visually diverse while still in-domain with real images. 
We apply and ablate over various augmentations to explore which are most effective in various training settings. 
Figure \ref{fig:method} shows an overview of the augmentation-conditioned generation process.\looseness-1
\vspace{-0.2cm}

\subsection{Ensuring Generations Are In-Domain With Conditioning} \label{sec:in_domain}
\vspace{-0.2cm}

\begin{figure}[b]
    \vspace{-0.3cm}
    \centering
    \includegraphics[width=1.0\textwidth]{img/text_vs_txtimg-noface.png}
    \vspace{-0.4cm}
    \caption{Failed generations: \textbf{Semantic Errors} (a),(b) where generations using only the class label result in images depicting a totally different object; \textbf{Visual Domain Shift} (c),(d) where generations using only the class label produce the correct visual concept but in a distinctly different visual style. Both these failure cases reduce efficacy of synthetic training images and are remedied by generating images conditioned on the class label and real training images.\looseness-1}
    \label{fig:fail_gens}
     \vspace{-0.3cm}
\end{figure}

Generating images using only the text class labels and no fine-tuning of the diffusion model is known to result in images with semantic issues that lessen their effectiveness as training data \citep{fake_it, feedbackguided, is_synthetic_data}. Additionally, using learned or manual prompt engineering based on class names is unable to yield classification performance on par with real images \citep{fake_it, is_synthetic_data}.
We identify specific failure cases where using only class names for generations results in synthetic images out of the domain of real classification data: \textbf{1) Semantic Errors}, where synonyms and homonyms in class labels lead to images of objects that do not exist in the real training set; \textbf{2) Visual Domain Shift}, where style bias from the diffusion model's training data results in generations of a distinctly different visual style. Training classifiers on data exhibiting these failure cases are greatly detrimental to classification performance.\looseness-1

To remedy these issues, we follow \cite{feedbackguided} and condition image generation on both the text class label and a real training image of the corresponding class. This approach is simpler and yields better classification results than existing approaches that utilize prompt engineering or generating prompts with LLMs \citep{fake_it, diversify}. 
Additionally, pre-trained image-conditioned or image variation diffusion models are commonly available \citep{stable_unclip, hugging_face}, making this approach is easily accessible.
As seen in Figure \ref{fig:fail_gens}, simply conditioning on a randomly selected training image from the text class label alleviates failure cases. However, introducing image conditioning reduces visual diversity of generations, which we address in the next section.\looseness-1

\vspace{-0.1cm}
\subsection{Adding Visual Diversity to In-Domain Generations} \label{sec:cond_methods}
\vspace{-0.2cm}

Inspired by traditional vision, we use image augmentation methods to introduce diversity into our generations. 
Augmentations are applied to real images, in both pixel and embedding space, then diffusion is conditioned on the augmented data and the text class label. 
The diffusion model we use, a latent diffusion model (LDM) conditioned on image and text features referred to as LDM-v2.1-unCLIP~\citep{stable_unclip}, encodes the conditioning image into the CLIP~\citep{CLIP} embedding space before conditioning, enabling us to perform augmentations in CLIP embedding and pixel space. 
We leverage the well-known CutMix~\citep{cutmix} and Mixup~\citep{mixup} augmentations on 2 randomly selected training images of the same class $x_1, x_2$:\looseness-1
\vspace{-0.5cm}

\begin{align*}
    \text{CutMix:} \qquad \tilde{x} &= \mathbf{M} \odot x_1 + (\mathbf{1}- \mathbf{M}) \odot x_2\qquad \\ 
    \text{Mixup:} \qquad \tilde{x} &= \lambda x_1 + (1 - \lambda) x_2\qquad 
\end{align*}
\vspace{-0.5cm}

For CutMix, \textbf{M} is a binary mask sampled based on $\lambda$ indicating where to replace an image region of $x_1$ with a patch from $x_2$, \textbf{1} is a binary mask of all ones, and $\odot$ is element-wise multiplication.
For Mixup and CutMix, $\lambda$ is sampled from a Beta distribution with $\alpha=1.0$, the default setting in \texttt{torchvision}.
If the augmentation is done in pixel space then $x_1, x_2$ are images and the resulting $\tilde{x}$ is later encoded into a CLIP image embedding; if the augmentation is done in embedding space then $x_1, x_2$ are CLIP image embeddings of the corresponding images and $\tilde{x}$ is a combined embedding.\looseness-1

We also use Dropout \citep{dropout} with $p=0.4$, on the CLIP image embedding of a randomly selected training image, as a stochastic augmentation method that removes random parts of the image conditioning information. 
This is equivalent to using a Dropout layer on the last layer of the CLIP image encoder.
As seen in Figure \ref{fig:dropout}, we observe that the Dropout probability acts as an image generation hyperparameter controlling the conditioning strength of the text and image information, with $p=0.0$ resulting in homogeneous images all similar to the conditioning image and $p=1.0$ resulting in images exhibiting failure cases discussed in Section \ref{sec:in_domain}. Thus, an intermediate Dropout ratio results in the most visually diverse generations, given the same conditioning text and image.\looseness-1 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{img/all-cond-img.png}
    \caption{Sample generated images using all of the augmentation conditioning methods. (a) shows generations conditioned on just the image and generations conditioned on Dropout applied to the image (b) shows generations conditioned on the combination of 2 images produced by the specified augmentation method. \textit{Augmentation-conditioned generations show more visual diversity in the coloration, pose, and angle of the hamster}.
    Generations from Embed-CutMix-Dropout, which yields the highest accuracy on ImageNet-LT, have distinct background diversity with hamsters depicted in various realistic terrains.\looseness-1} 
    \label{fig:all_conds}
\end{figure}

A total of 9 augmentation-conditioned methods result from combinations of the aforementioned augmentation methods: Dropout, CutMix, CutMix-Dropout, Embedding-CutMix, Embedding-CutMix-Dropout, Mixup, Mixup-Dropout, Embedding-Mixup, and Embedding-Mixup-Dropout. 
For the combination methods, we perform CutMix or Mixup in the specified pixel or embedding space then apply Dropout to the augmented embedding.
Let $\tilde{x}$ be the image embedding produced by an augmentation method; to condition the image generation process on the augmentation, the diffusion denoising UNet \citep{unet} concatenates $\tilde{x}$ onto its time step embedding. 
Sample generations for all conditioning methods are shown in Figure \ref{fig:all_conds}. 