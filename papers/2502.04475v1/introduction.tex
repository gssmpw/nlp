\section{Introduction}
\vspace{-0.2cm}

Advances in modern deep learning greatly rely on massive datasets. 
With the advent of large-scale pretraining and foundation models, massive amounts of diverse data are an integral part of AI. 
State-of-the-art datasets have only increased in size with time; from ImageNet-1k \cite{imagenet} consisting of 1.3 million images from 1000 classes, to the current LAION dataset \cite{laion5b} that consists of 5 billion image-caption pairs from the Internet. Particularly in computer vision, high-quality images that are diverse and in-domain are crucial to classification performance. However, collecting real images is often expensive or difficult; especially in specialized tasks where examples of classes are rare or hard to photograph. This leads to long-tail, imbalanced classification settings where most classes have very few training examples \citep{imagenetLT, balanced_softmax, decouple-lt}. Additionally it is well-known that visual diversity, traditionally introduced through data augmentation on existing training images, improves classifier performance and generalization \citep{imagenet_classification, cutmix, mixup, rand_aug}. 

Recently, diffusion text-to-image models have achieved unprecedented standards for synthetic image quality, capable of generating photo-realistic images for an impressive variety of text prompts \citep{sdxl, dalle-2, imagen}. 
A natural application for these models is synthetic training image generation, as the visual characteristics of generated images are controllable via various diffusion mechanics such as the conditioning information, guidance scale, and latent noise variables. 
However, diffusion models are primarily used to generate imaginative images from creative prompts rather than realistic depictions of real-world objects.
Text-to-image models are often optimized for creativity purposes with human preference as a metric, prioritizing image quality and prompt adherence over generation diversity. This leads to synthetic images being less effective than real images when used as training data, as synthetic images often depict spurious qualities of image classes and have style bias from their training dataset \citep{is_synthetic_data, fake_it}. 
Furthermore, training images must be visually diverse to increase classification performance and properly represent variations of visual concepts, but pretrained diffusion models often lack the ability to generate images that reflect the representation diversity found in real-world domains \citep{diversify, da-fusion, stable_bias, bias_survey, hall2023dig}.\looseness-1

Existing methods for training image generation remedy these issues by fine-tuning the diffusion model on task-specific data \cite{syntheticdataimagenet}, using large language models to prompt for diversity in image generations \cite{diversify}, or using specialized fine-tuning of the diffusion model to learn concepts from real training images \citep{fill-up-lt, da-fusion}. 
However, fine-tuning of diffusion models is computationally expensive, especially when the classification task has many visual concepts the diffusion model must learn.

In this paper, we analyze the use of classical vision data augmentation methods as conditioning information for image generation and find certain data augmentations yield visually diverse training images that enhance downstream classification.
We use augmentation-conditioning and a frozen, pretrained diffusion model to generate effective training images in a much more computationally efficient manner than previous work that requires diffusion model training \textit{e.g.}, \citep{syntheticdataimagenet, da-fusion, fill-up-lt}.
In particular, augmentation-conditioning leverages vision data augmentations of real images alongside a text prompt as conditioning information in the image generation process.
Conditioning on real training images provides in-domain context to the generation process whereas the proposed use of data augmentations encourage visual diversity, altogether increasing the performance of downstream classification while requiring the same computational cost as off-the-shelf image generation with a pretrained diffusion model. 
We evaluate various augmentation methods on five ubiquitous long-tail and few-shot classification tasks, in both training from scratch and fine-tuning settings, showing that our synthetic datasets improve classification performance over existing work. 
% We evaluate our method in settings where a classifier is trained from scratch and where a pretrained classifier is fine-tuned on synthetic data. We train from scratch on a large scale dataset generated from the well-known long-tail classification benchmark ImageNet Long-Tailed (ImageNet-LT) \citep{imagenetLT}. We also evaluate our method when applied to few-shot classification, where we fine-tune pretrained classifiers on data generated from a variety of vision datasets that encompass common and niche concepts. 

We find that using augmentation-conditioned synthetic datasets results in outperforming prior work on ImageNet Long-Tailed, while training on 135k less synthetic images. 
Augmentation conditioning also enables surpassing state-of-the-art classification accuracy on four standard few-shot benchmarks and exhibits remarkable gains in extreme few-shot regimes, even when compared to methods that require diffusion model training or finetuning. 
These results highlight the potential of augmentation-conditioned techniques to generate training data, without requiring any generative model finetuning, and constitute an important step towards effectively leveraging synthetic data for downstream model training. \looseness-1
\vspace{-0.1cm}

% These results highlight the potential of our method to generate training data, without requiring any finetuning, and constitute an important step towards effectively leveraging synthetic data for downstream model training.

%Our method enables the use of pretrained diffusion models as effective training image generators, with no fine-tuning.
% be easily applied to any long-tail or imbalanced classification setting to generate effective synthetic training data to balance examples across classes, or used to generate valuable task-specific synthetic images to fine-tune on. 
% Our image generation adds no computational or memory cost on top of off-the-shelf image generation with a pretrained diffusion model, making it widely accessible. 