\appendix

\clearpage
\section{Dropout Probability's Effect on Image Diversity} \label{app:dropout}
\begin{figure}[ht!]
    \centering
    \hspace*{-0.5cm}
    \includegraphics[width=1.05\textwidth]{img/dropout-ratio.png}
    \caption{Example generations conditioned on Dropout with various probabilities applied to a real image. $p=0.0$ is equivalent to conditioning on the original image, resulting in homogeneous images all similar to the conditioning image. $p=1.0$ is equivalent to only conditioning on the text class label, results in images exhibiting failure cases discussed in Section \ref{sec:in_domain}. Thus, an intermediate Dropout ratio results in diverse but in-domain images.} 
    \label{fig:dropout}
    % \vspace{-0.25cm}
\end{figure}

% See \ref{fig:dropout} for Dropout Probability's affect as an image generation hyperparameter. A similar plot of image generations is also shown in \cite{feedbackguided}. 


\section{Hyperparameters and Training Details}\label{app:hyperparams}

The full set of hyperparameters for image generation and classifier training are given in Table~\ref{table:hparams}.

All experiments were run on A100, A40, and A5500 GPUs on university compute clusters. 

\begin{table*}[h!]
    \centering
    \begin{tabular}{@{}lr@{}}
        \toprule
        \textbf{Hyperparameter Name} & \textbf{Value} \\
        \midrule
        \midrule
        
        \textbf{Image Generation} \vspace{0.1cm} \\ 
        LDM-v2.1-unCLIP Checkpoint & \texttt{stabilityai/stable-diffusion-2-1-unclip} \\
        Diffusion Denoising Steps & 30 \\
        Diffusion Noise Scheduler & PNDM Scheduler \cite{pndm_scheduler} (default in Hugging-Face) \vspace{0.1cm} \\

        \midrule
        \textbf{Section \ref{sec:class_imbalanced} Classifier} \vspace{0.1cm} \\ 
        Architecture & ResNext50 \\
        Loss & Balanced Softmax \cite{balanced_softmax} \\
        Optimizer &  SGD with cosine annealing \cite{sgd_cosine} \\
        Learning Rate & 0.2 \\
        Momentum & 0.9 \\
        Weight Decay & 0.0005 \\
        Batch Size & 512 \\
        Training Epochs & 150 \vspace{0.1cm} \\

        \midrule
        \textbf{Section \ref{sec:few_shot} Classifier \vspace{0.1cm}} \\
        Architecture & ResNext50 \\
        Loss & CrossEntropy \\
        Optimizer & Adam \\
        Learning Rate & 0.0001 \\
        Batch Size & 32 \\
        Fine-Tuning Epochs & 50 \vspace{0.1cm} \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters and training configuration details}
    \label{table:hparams}
\end{table*}

% Results from Sections \ref{sec:90_subset} and \ref{sec:90_cfg} use the downsized ResNet18 (with the training configuration of Section \ref{sec:class_imbalanced}) and a 90-class-subset of all 1K ImageNet classes. See code files for names of classes in the 90-class-subset.

\clearpage
\section{Individual Few-Shot Classifier Free Guidance Plots} \label{app:few-shot-cfg}
\begin{figure}[h!]
    \hspace*{-2cm} 
    \centering
    \includegraphics[width=1.35\textwidth]{img/few-shot-cfg-ind.png}
    \vspace{-0.5cm}
    \caption{Classifier free guidance scale's affect on few-shot classification performance}
    \label{fig:few-shot-cfg-ind}
\end{figure}