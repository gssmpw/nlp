\section{Experiments}
\vspace{-0.2cm}

We generate synthetic training datasets with each augmentation-conditioning method in Section \ref{sec:cond_methods} and evaluate the efficacy of each image augmentation method by training downstream classifiers on images generated using the augmentation as conditioning information.
We show the efficacy of augmentation-conditioned generations in two settings: (1) training from scratch in a large scale, long-tail setting with class-imbalanced classification and (2) fine-tuning a pre-trained classifier on various few-shot classification tasks.\looseness-1


\vspace{-0.2cm}
\subsection{Large-Scale Imbalanced Classification} \label{sec:class_imbalanced}

\paragraph{Class-Imbalanced, Long-tail Dataset.}
Augmentation-conditioned generations are naturally applicable to long-tailed data settings, where examples per class are imbalanced and most classes have scarce examples. We use augmented existing real examples as conditioning information and generate synthetic images to balance the number of examples across classes, then train a downstream classifier on the combined set of synthetic and real images and evaluate on a balanced test set of real images.\looseness-1

Our experiments use the largest and most ubiquitous long-tail benchmark dataset, ImageNet-LT \citep{imagenetLT}, a subset of ImageNet-1K \citep{imagenet} downsampled so that most classes have around 20 training images. 
ImageNet-LT has a total of 115.8k real images across 1K classes, with a minimum of 5 and maximum of 1,280 images per class.
Classes are categorized based on their number of training examples: many-shot for 100 or more, medium-shot for 20 to 100, and few-shot for 20 or less. We generate enough synthetic images so that each class has 1,280 training images, resulting in a total of approximately 1.16 million synthetic images.\looseness-1

\vspace{-0.2cm}
\paragraph{Experimental Setup.} For image generation, we use the pre-trained LDM-v2.1-unCLIP model~\citep{stable_unclip}. This model is based on LDM v2.1~\citep{stablediffusion} and is capable of generating images conditioned on text and image. We use this diffusion model off-the-shelf with no changes to its weights. In line with previous work on ImageNet-LT, we train a ResNext50 \citep{rexnext} classifier from scratch for 150 epochs using the SGD optimizer with cosine annealing \citep{sgd_cosine} and the Balanced Softmax loss \citep{balanced_softmax}.
We measure efficacy of augmentation-conditioned synthetic training datasets by evaluating top-1 accuracy on the balanced test set of real images.
During training each minibatch contains 50\% real and 50\% synthetic images, as this balancing of real and synthetic images is known to improve training stability \citep{feedbackguided, da-fusion, is_synthetic_data}. 
For full details on image generation and training hyperparameters see Appendix \ref{app:hyperparams}.\looseness-1

\vspace{-0.15cm}
\subsubsection{Conditioning Method Performance} \label{sec:90_subset}
\vspace{-0.2cm}

To initially compare the performance of our nine augmentation-conditioned generation methods under compute constraints, we ran smaller scale evaluations on 90 randomly selected classes of ImageNet-LT with a ResNet18 classifier. This class subset includes 30 of each of the few, median, and many class categories. Overall accuracies as well as class category accuracies on the corresponding 90-class-subset evaluation set are reported in Table \ref{table:90}.\looseness-1

\begin{table}[ht]
\centering
\small
\vspace{-0.3cm}
\caption{Top-1 classification accuracy and FID Score between synthetic datasets and evaluation set for ImageNet-LT  90-class-subset. Conditioning Methods are discussed in Section \ref{sec:cond_methods}; Random Image is a baseline generation conditioned on the class label and a randomly selected training image of that class. The best accuracy per-category is bolded.\looseness-1}
\vspace{0.2cm}
\begin{tabular}{lcccc|c}
\toprule
\textbf{Conditioning Method} & \textbf{Overall} & \textbf{Many} & \textbf{Median} & \textbf{Few} & \textbf{FID Score}\\
\midrule
\midrule
Random Image (Baseline) & 63.0 & 72.4 & 61.4 & 55.3 & 20.181\\
Dropout & 66.2 & 70.9 & 64.7 & 63.0 & 21.843 \\
Mixup & 63.6 & 69.5 & 63.3 & 58.0 & 24.115 \\ 
Mixup-Dropout & 65.6 & 69.2 & 65.2 & 62.4 & 22.306 \\
Embed-Mixup & 63.5 & 71.3 & 62.4 & 56.8 & 22.930 \\ 
Embed-Mixup-Dropout & 66.2 & 72.2 & 63.7 & 62.7 & 24.558 \\ 
CutMix & 63.8 & 69.5 & 63.0 & 59.0 & 26.623 \\
CutMix-Dropout & 65.2 & 69.2 & 63.1 & 63.2 & 24.453 \\
Embed-CutMix & 62.6 & \textbf{73.1} & 61.9 & 53.0 & 20.285 \\
\textbf{Embed-CutMix-Dropout} & \textbf{66.9} & 72.0 & \textbf{65.2} & \textbf{63.5} & 20.433 \\
\bottomrule
\end{tabular}
\label{table:90}
\vspace{-0.3cm}
\end{table} 

The conditioning method using CutMix and Dropout in the CLIP embedding space performs best, followed closely by embedding-space Mixup and Dropout, and solely Dropout. 
Conditioning using embedding-space CutMix and Dropout enables about +4\% overall accuracy over conditioning on an un-augmented training image (Random Image in Table \ref{table:90}) and a remarkable +8\% accuracy on the hardest category of few-shot classes.
Dropout done in addition to any of the image augmentation methods, regardless of in pixel of embedding space, increases accuracy; indicating that Dropout as a data augmentation yields effective conditioning information for synthetic training image generation.\looseness-1

We calculate Fr√©chet Inception Distance (FID) Score \citep{fid}, a measure of both image quality and diversity, between the evaluation set of real images and the synthetic training dataset for each of the augmentation-conditioned generation methods. 
The best-performing augmentation-conditioning method has one of the lowest FID scores, supporting our claim that augmentation-conditioned generations increase \emph{in-distribution diversity} and lead to better classification performance.\looseness-1

% \vspace{-0.25cm}
\subsubsection{Classifier Free Guidance Scale} \label{sec:90_cfg}
\vspace{-0.2cm}

The classifier free guidance (CFG) scale parameter of diffusion models controls the trade-off between prompt adherence and diversity of generations \citep{classifierfreeguidance}. 
Previous work on synthetic training image generation found that the CFG scale greatly affects downstream classification accuracy, with lower values leading to better performance empirically \citep{syntheticscaling, stable_rep, fake_it}. 
To explore CFG scale's effect on augmentation-conditioned generations, we run the best-performing conditioned generation method Embed-CutMix-Dropout with CFG scales: [2.0, 4.0, 7.0, 10.0] and report maximum validation accuracy over all epochs on the 90-class-subset in Table \ref{table:90_cfg}. \looseness-1

\vspace{-0.5cm}
\begin{table}[h]
\centering
\small
\caption{Classifier Free Guidance (CFG) scale's effect on top-1 classification validation accuracy on ImageNet-LT 90-class-subset. The lowest CFG scale of 2.0 results in highest overall accuracy.\looseness-1}
\vspace{0.2cm}
\begin{tabular}{lcccc}
\toprule
\textbf{CFG Scale} & \textbf{Overall} & \textbf{Many} & \textbf{Median} & \textbf{Few}\\
\midrule
\midrule
2.0 & \textbf{73.3} & \textbf{75.5} & 72.0 & \textbf{72.3} \\
4.0 & 72.9 & 75.3 & \textbf{72.2} & 71.2 \\
7.0 & 70.5 & 74.5 & 68.5 & 68.5 \\ 
10.0 & 66.9 & 72.0 & 65.2 & 63.5 \\
\bottomrule
\end{tabular}
\label{table:90_cfg}
\end{table} 
% \vspace{-0.2cm}

The lowest CFG scale of 2.0 achieves the highest accuracy overall, with a notable almost +10\% accuracy on the most difficult few-shot classes when compared to the \texttt{Hugging-Face} default CFG scale of 10.0. 
This result aligns with previous work which finds that a low CFG scale leads to the best downstream accuracy for ImageNet-scale synthetic training data, as it increases diversity across the numerous generations that use the same class text labels \citep{syntheticscaling}.\looseness-1

% \vspace{-0.25cm}
\subsubsection{ImageNet-LT Baselines}

\begin{table}[h!]
\vspace{-0.45cm}
\centering
\small
\caption{Top-1 classification accuracy on ImageNet-LT using ResNext50. The best augmentation-conditioning method outperforms SOTA accuracy of methods that use no synthetic data. We outperform methods utilizing similar amounts of synthetic data, while Fill-Up (which uses more than 2x the amount of synthetic training images and fine-tunes the model on real images after pre-training) only outperforms us by less than 4\%.\looseness-1}
\vspace{0.2cm}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Synthetic Data Count} & \multicolumn{4}{c}{\textbf{ImageNet-LT}}\\
\cmidrule(lr){3-6}
& & \text{Overall} & \text{Many} & \text{Medium} & \text{Few} \\
\midrule
\midrule
Decouple-LWS~\citep{decouple-lt} & 0& 47.7 & 57.1 & 45.2 & 29.3 \\
Balanced Softmax~\citep{balanced_softmax} & 0& 51.0 & 60.9 & 48.8 & 32.1 \\
Mix-Up GLMC ~\citep{best_non_synthetic_imagenetlt} &0& \textbf{57.21} &  \textbf{64.76} & \textbf{55.67} & \textbf{42.19} \\
\midrule
\midrule
Fill-Up~\citep{fill-up-lt} & \textit{2.6M} & 63.7 & 69.0 & 62.3 & 54.6 \\
LDM (txt)~\citep{feedbackguided} &1.3M & 57.9 & 64.8 & 54.6 & 50.3 \\
LDM (txt and img)~\citep{feedbackguided}  & 1.3M& 58.9 & 56.8 & 64.5 & 51.1 \\
Dropout (Ours) & 1.16M& 57.3 & 65.8 & 54.3 & 44.0 \\
Mixup-Dropout (Ours) & 1.16M& 57.4 & 65.8 & 53.9 & 46.3 \\
Embed-Mixup-Dropout (Ours) & 1.16M & 56.0 & 65.3 & 52.4 & 42.2 \\
Embed-CutMix-Dropout (Ours) & 1.16M& \textbf{59.6} & \textbf{66.3} & \textbf{56.6} &\textbf{51.1} \\
\bottomrule
\vspace{-0.6cm}
\end{tabular}
\label{table:imagenet_full_scale}
\end{table} 

We run the best four conditioning methods from the 90-class-subset results (Section \ref{sec:90_subset}) on full-scale ImageNet-LT, with results compared to existing baselines in Table \ref{table:imagenet_full_scale}.
The augmentation-conditioning method using embedding-space CutMix and Dropout outperforms SOTA ImageNet-LT baselines that use no diffusion-generated images, though \citep{best_non_synthetic_imagenetlt} uses traditional vision augmentations to generate training data. 
It also outperform prior works that generate and train on similar quantities of synthetic data, improving accuracy over \citep{feedbackguided} with over 135k less synthetic images. 
These accuracy gains show that CutMix and Dropout augmentations in the CLIP embedding space provides valuable conditioning information that results in effective synthetic training data.\looseness-1 

Note that \cite{feedbackguided} proposes additional methods that use performance signals of a separate, pre-trained classifier in the diffusion process, which can improve upon our results but also incurs additional computation cost. 
Fill-Up \citep{fill-up-lt} trains the classifier from scratch on over 2x the amount of synthetic training images we use and additionally fine tunes the classifier on real images after pre-training, so the comparison is unfair. 
Even with $2\times$ the synthetic data amount and fine-tuning, Fill-Up only achieves +4\% accuracy over the best augmentation-conditioned method. 
Previous work \citep{syntheticscaling} has found that classification accuracy increases as the amount of synthetic images scales, so we can expect the accuracy gap to be closed if we generated and trained on more synthetic images; but due to compute constraints, we were unable to run experiments with more generated images.\looseness-1


% \vspace{-0.15cm}
\subsection{Few-Shot Classification} \label{sec:few_shot}
\vspace{-0.15cm}

\paragraph{Few-Shot Vision Datasets.}
In line with previous diffusion-augmentation work, we benchmark augmentation-conditioned generations on four computer vision datasets: Caltech101 \citep{caltech101}, Flowers102 \citep{flowers102}, COCO \citep{COCO} (2017 version), and Pascal VOC \citep{pascal} (2012 version). 
Pascal VOC and COCO are originally object detection datasets, but we adapt them into classification datasets by using the class label of the object with the largest pixel mask as the image label, as is done in previous work we use as baseline comparisons \citep{da-fusion}. 
By this labelling method, COCO has 80 classes and Pascal VOC has 20 classes. Caltech101 and Flowers102 each have 102 classes. 
Caltech101, Pascal VOC, and COCO have common classes (e.g. "car", "cat") and Flowers102 has only niche, fine-grained classes which are flower species (e.g. "alpine sea holly").\looseness-1

\vspace{-0.2cm}
\paragraph{Experimental Setup.}
We use the same diffusion model from the previous section's class-imbalanced experiments, LDM-v2.1-unCLIP~\citep{stable_unclip}. 
A ResNet50 \citep{resnet50} pre-trained in ImageNet is fine-tuned on a mixture of real and synthetic images, where each image in a minibatch has a 50\% probability of being a real training image and 50\% probability of being a synthetic image, as done in \citep{da-fusion}. 
We fine-tune the last layer of the ResNet50 for 50 epochs using the Adam optimizer and a learning rate of 0.0001. 
To match the accuracies reported in \citep{da-fusion}, we report the highest validation accuracy across epochs. Additionally, we run fine-tuning with 1, 2, 4, 8, and 16 examples per class in the training set, and report mean validation accuracy over 4 independent trials. 
Points in our plots represent accuracy means and shading represents variance; though most variance values are in the $10^-6$ range and therefore not visible.\looseness-1~\footnote{We cannot plot variance for results from existing work in Figure \ref{fig:few-shot-baseline} due to compute constraints and the unavailability of raw results from the authors.}

The baselines we compare to are taken directly from \cite{da-fusion} and include three different data augmentation methods. RandAugment \citep{rand_aug} is a widely used augmentation method involving color and geometric transformations that uses no generated images. Real Guidance \citep{is_synthetic_data} generates synthetic images using SDEdit \citep{sdedit}, i.e. noising a real image, then denoising the noised image with a stochastic differential equation prior. DA-Fusion \citep{da-fusion} generates synthetic images by training the diffusion model to learn the class's visual concept from real training images via textual inversion \citep{text_inversion} and additionally uses SDEdit at image generation time. Note that our augmentation-conditioning methods require significantly less computation and memory than DA-Fusion, as they require no changes to the diffusion model but DA-Fusion requires training the diffusion model for each generated image.\looseness-1

% \vspace{-0.15cm}
\subsubsection{Classifier Free Guidance Scale}
\vspace{-0.15cm}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.1\textwidth]{img/few-shot-cfg2x2.png}
    \hspace{-0.2cm}
    \vspace{-0.5cm}
    \caption{Classifier free guidance scale's effect on few-shot classification performance. Across all datasets, fine-tuning on images generated with 10.0 CFG scale yields better performance.}
    \label{fig:few-shot-cfg} 
    \vspace{-0.5cm}
\end{figure}

As discussed and seen in the results of Section \ref{sec:90_cfg}, the Classifier Free Guidance (CFG) scale parameter of image generation has notable effect on the synthetic images and downstream accuracy. We explore if CFG scale still has an effect when fine-tuning on a relatively small amount of synthetic data by running the same fine-tuning experiments on images generated with a CFG scale of 2.0 (the optimal CFG scale for ImageNet-LT) and 10.0 (the default CFG scale for our diffusion model), with results in Figure \ref{fig:few-shot-cfg}. We use the conditioning methods with the top 3 accuracies from the experiments in Section \ref{sec:90_subset}, and more detailed individual plots are in Appendix \ref{app:few-shot-cfg}.\looseness-1

Interestingly, for all datasets the optimal CFG scale for fine-tuning is not the optimal CFG scale for large-scale training from scratch. The same conditioning methods used with the 10.0 CFG scale yield higher few-shot accuracies than when used with the 2.0 CFG scale across all four datasets. We believe this is because the few-shot setting uses very few synthetic images compared to large-scale training, so strong prompt adherence and high image quality is more important to the classifier's learning than visual diversity.\looseness-1 

\vspace{-0.15cm}
\subsubsection{Few-Shot Baselines}
\vspace{-0.15cm}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/few-shot-baseline2x2.png}
    \vspace{-0.25cm}
    \caption{Few-shot classification performance of the best-performing conditioning method compared to existing work on 4 datasets. Augmentation-conditioned generations match or improve accuracy up to +25\% over the best-performing existing method, with no training of the diffusion model.}
    \label{fig:few-shot-baseline}
    \vspace{-0.5cm}
\end{figure}
% \vspace{-0.15cm}

Figure \ref{fig:few-shot-baseline} shows that augmentation-conditioned generation methods improve accuracy across all datasets. We applied the the conditioning methods with the top 3 accuracies from Section \ref{sec:90_subset}'s experiments, and plot the augmentation-conditioned method that yielded the highest few-shot accuracy per-dataset (all augmentation-conditioned method performance can be seen in Figure \ref{fig:few-shot-cfg}).\looseness-1 

Augmentation-conditioned generations match or yield up to +25\% accuracy over the best-performing existing method DA-Fusion \citep{da-fusion}, which requires training of the diffusion model whereas augmentation-conditioning requires no training. For the Pascal VOC and Flowers102 datasets, augmentation-conditioned augmentations outperforms all existing methods for all examples per class values, with approximately 10\% higher accuracy for Pascal VOC and 3\% for Flowers102. These performance gains indicate that augmentation-conditioning is effective at producing synthetic training images that are useful for fine-grained (e.g. flower species for Flowers102) and common object (e.g. general animal and vehicle types in Pascal VOC) classification, without requiring the diffusion model to learn visual concepts from real data.\looseness-1
%\vspace{-0.5cm}