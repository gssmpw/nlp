
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath} 
\usepackage{natbib}
\usepackage{graphicx}


\title{Augmented Conditioning is Enough for Effective Training Image Generation}
% Old title:
%  Effective Training Image Generation through Augmented Conditioning
% Alternate titles:
%%AZ.9.17: No Finetuning Needed: Augmented Conditioning is Enough for Effective Training Image Genefration

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Jiahui Chen \\
UT Austin \\
\texttt{jiahui.k.chen@utexas.edu} \\
\And
Amy Zhang\thanks{AZ and ARS acted in an advisory role. All experiments were run on UT Austin's infrastructure.} \\
UT Austin \\
\AND
Adriana Romero-Soriano$^*$ \\
McGill University, Mila, Canada CIFAR AI Chair \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Image generation abilities of text-to-image diffusion models have significantly advanced, yielding highly photo-realistic images from descriptive text and increasing the viability of leveraging synthetic images to train computer vision models. To serve as effective training data, generated images must be highly realistic while also sufficiently diverse within the support of the target data distribution. Yet, state-of-the-art conditional image generation models have been primarily optimized for creative applications, prioritizing image realism and prompt adherence over conditional diversity. 
In this paper, we investigate how to improve the diversity of generated images with the goal of increasing their effectiveness to train downstream image classification models, without fine-tuning the image generation model. 
We find that conditioning the generation process on an augmented real image and text prompt produces generations that serve as effective synthetic datasets for downstream training. 
Conditioning on real training images contextualizes the generation process to produce images that are in-domain with the real image distribution, while data augmentations introduce visual diversity that improves the performance of the downstream classifier. 
We validate augmentation-conditioning on a total of five established long-tail and few-shot image classification benchmarks and show that leveraging augmentations to condition the generation process results in consistent improvements over the state-of-the-art on the long-tailed benchmark and remarkable gains in extreme few-shot regimes of the remaining four benchmarks. These results constitute an important step towards effectively leveraging synthetic data for downstream training.\makeatletter{\renewcommand*{\@makefnmark}{}}
% \footnotetext{$^*$Authors A/B acted in an advisory role. None of the experiments were run on Institution A's infrastructure. The research was conducted by Author C prior to joining institution A.}\makeatother}\looseness-1 
\end{abstract}


\begin{figure}[h!]
    \vspace{-0.3cm}
    \centering
    % \hspace*{-0.5cm}
    \includegraphics[width=1.0\textwidth,  clip]{img/ice-cream-noface.png}
    \vspace{-0.6cm}
    \caption{Example images from (a) real training data, (b) a pretrained diffusion model using the class label as conditioning, (c) the best performing augmentation-conditioned method. Augmentation conditioning generates visually diverse, realistic images that enhance downstream classification accuracy when used as training data.} 
    \label{fig:intro-ex}
    \vspace{-0.4cm}
\end{figure}


\input{introduction}


\vspace{-0.25cm}
\input{related_work}

\vspace{-0.25cm}
\input{method}


\vspace{-0.25cm}
\input{experiments}

\vspace{-0.25cm}
\section{Discussion}
\vspace{-0.25cm}

\paragraph{Conclusion.}
We analyzed the efficacy of various vision data augmentation methods for synthetic training data generation via thorough experimentation, finding augmentation-conditioned generation capable of producing effective synthetic training datasets.
Training on augmentation-conditioned generations achieves up to +10\% accuracy across a variety of few-shot classification settings, over diffusion-based data augmentation methods that require fine-tuning of the diffusion model. 
Utilizing augmentation-conditioned generations as training data also improves over state-of-the-art results on a long-tail, imbalanced classification task. 

We find that leveraging existing data augmentations as conditioning information in the diffusion process produces effective synthetic training datasets for various classification tasks, without requiring fine-tuning of the diffusion model. Augmentation-conditioned generations thus enable training image generation at the same cost as general image generation with an off-the-shelf text-to-image model. 
Conditioning on real training images enables generations to be in-domain with the real image distribution, while the data augmentations introduce visual diversity that enhances the performance of the downstream classifier.
We improve classification performance on long-tail and few-shot vision benchmarks by training on our generated images, showing that augmentation-conditioning generates effective training data for a variety of tasks.  
Augmentation-conditioned generations are a computationally efficient approach to using pretrained diffusion models as training image generators. \looseness-1

\vspace{-0.3cm}
\paragraph{Limitations \& Future Work.} \label{sec:limitations}
Using our conditioned generations as synthetic training data enables strong performance improvements, however there are limitations. 
The pre-trained diffusion model we use for image generation may include examples from common vision benchmark datasets, such as ImageNet \cite{imagenet} and COCO \cite{COCO}, as it is trained on billion-scale Internet data. 
Previous work has shown that pre-trained diffusion models can memorize training examples, leading to training data leakage \cite{dataleakage}. 
As future work, we would like to investigate the effect of potential data leakage on the downstream model performance. 

\bibliography{ref}
\bibliographystyle{iclr2025_conference}

\appendix
\input{appendix}


\end{document}
