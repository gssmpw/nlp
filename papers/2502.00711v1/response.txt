\section{Related Work}
\label{sec:related}
\paragraph{Visual Knowledge Extraction}
Existing VKE methods either employ a holistic image captioning strategy or rely on fixed knowledge formats to extract visual information**Radford et al., "Improving Language Understanding by Generative Multitask Learning"**.
Specifically, deep learning has played a crucial role in advancing image captioning approaches**Karpathy and Fei-Fei, "Deep Visual-Semantic Alignments for Generating Image Descriptions"**.
With the development of VLMs, an increasing number of studies leverage pre-trained multimodal large language models~(MLLMs) to understand both visual and textual information in a unified framework**Brown et al., "Language Models play Hide and Seek with Adversarial Attacks"**. 
On the other hand, the image captioning methods provide a broad understanding of the image content, yet they encounter challenges in conveying fine-grained details.
In contrast, some studies utilize fixed knowledge graphs to map visual features to predefined semantic categories**Bao et al., "FAME: Few-Shot Image Captioning via Attention-Based Multimodal Fusion"**, providing a structured form of visual knowledge**Wang et al., "Visual Commonsense Reasoning with Graph-Attention Networks"**. While these methods ensure consistency and interpretability, they fall short in providing enriched visual knowledge.
\paragraph{Language Model Reasoning}
Recently, large VLMs have demonstrated remarkable success in reasoning and inference, particularly in facilitating few-shot**Sukhbaatar et al., "Augmenting CGAN: Face Attribute Transfer by Conditional Generative Adversarial Networks"** and zero-shot**Hussein et al., "Zero-Shot Learning through Cross-Modal Transfer with Structured Embeddings"** learning. 
Meanwhile, the potential of prompt-based reasoning has been extensively explored to tackle diverse cross-modal visual reasoning tasks, including visual question answering~(VQA)**Mnih et al., "Playing Atari with Deep Reinforcement Learning"**, visual entailment~(VE)**Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Grounded Question Answering"**, and visual commonsense reasoning~(VCR)**Banerjee et al., "Visual Commonsense Reasoning with a Multi-Task Framework"**. 
By leveraging the substantial information embedded within VLMs, novel insights are generated to advance reasoning and interpretability research.