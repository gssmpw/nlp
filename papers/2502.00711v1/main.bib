@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@inproceedings{covlm2024,
  author       = {Junyan Li and
                  Delin Chen and
                  Yining Hong and
                  Zhenfang Chen and
                  Peihao Chen and
                  Yikang Shen and
                  Chuang Gan},
  title        = {CoVLM: Composing Visual Entities and Relationships in Large Language
                  Models Via Communicative Decoding},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=PHGxChm1l5},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiCHCCSG24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{spatialRelation2024,
    author    = {Yang, Cheng and Xu, Rui and Guo, Ye and Huang, Peixiang and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan and Zhou, Hong},
    title     = {Improving Vision-and-Language Reasoning via Spatial Relations Modeling},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2024},
    pages     = {769-778}
}

@article{gpt42023,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{openvik2024,
  title={Open visual knowledge extraction via relation-oriented multimodality model prompting},
  author={Cui, Hejie and Fang, Xinyu and Zhang, Zihan and Xu, Ran and Kan, Xuan and Liu, Xin and Yu, Yue and Li, Manling and Song, Yangqiu and Yang, Carl},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{VRDsurvey2022,
  title={Visual relationship detection: A survey},
  author={Cheng, Jun and Wang, Lei and Wu, Jiaji and Hu, Xiping and Jeon, Gwanggil and Tao, Dacheng and Zhou, Mengchu},
  journal={IEEE Transactions on Cybernetics},
  volume={52},
  number={8},
  pages={8453--8466},
  year={2022},
  publisher={IEEE}
}

@inproceedings{COT2022,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{E2G2024,
  title={Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning},
  author={Parvez, Md Rizwan},
  journal={arXiv preprint arXiv:2401.05787},
  year={2024}
}

@article{reflexion2023,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{cola2023,
  title={Cola: A benchmark for compositional text-to-image retrieval},
  author={Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan and Krishna, Ranjay and Saenko, Kate},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{crepe2023,
  author={Zixian Ma and Jerry Hong and Mustafa Omer Gul and Mona Gandhi and Irena Gao and Ranjay Krishna},
  title={@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?},
  year={2023},
  cdate={1672531200000},
  pages={10910-10921},
  url={https://doi.org/10.1109/CVPR52729.2023.01050},
  booktitle={Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher={IEEE},
}

@inproceedings{aokvqa2022,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European conference on computer vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}

@inproceedings{esnlive2021,
  title={e-vil: A dataset and benchmark for natural language explanations in vision-language tasks},
  author={Kayser, Maxime and Camburu, Oana-Maria and Salewski, Leonard and Emde, Cornelius and Do, Virginie and Akata, Zeynep and Lukasiewicz, Thomas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1244--1254},
  year={2021}
}

@inproceedings{vqav22017,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@article{mosai2023,
  title={Coarse-to-fine contrastive learning in image-text-graph space for improved vision-language compositionality},
  author={Singh, Harman and Zhang, Pengchuan and Wang, Qifan and Wang, Mengjiao and Xiong, Wenhan and Du, Jingfei and Chen, Yu},
  journal={arXiv preprint arXiv:2305.13812},
  year={2023}
}

@inproceedings{blip2022,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{smola2024,
  title={Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts},
  author={Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14205--14215},
  year={2024}
}

@article{molmo72b2024,
  title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}

@article{paliGemma2024,
  title={PaliGemma 2: A Family of Versatile VLMs for Transfer},
  author={Steiner, Andreas and Pinto, Andr{\'e} Susano and Tschannen, Michael and Keysers, Daniel and Wang, Xiao and Bitton, Yonatan and Gritsenko, Alexey and Minderer, Matthias and Sherbondy, Anthony and Long, Shangbang and others},
  journal={arXiv preprint arXiv:2412.03555},
  year={2024}
}

@article{nlxgpt2022,
  publtype={informal},
  author={Fawaz Sammani and Tanmoy Mukherjee and Nikos Deligiannis},
  title={NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks},
  year={2022},
  cdate={1640995200000},
  journal={CoRR},
  volume={abs/2203.05081},
  url={https://doi.org/10.48550/arXiv.2203.05081}
}

@inproceedings{rapper2024,
  author={Kai-Po Chang and Chi-Pin Huang and Wei-Yuan Cheng and Fu-En Yang and Chien-Yi Wang and Yung-Hsuan Lai and Yu-Chiang Frank Wang},
  title={RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=bshfchPM9H},
  booktitle={Proceedings of the 2024 International Conference on Learning Representations (ICLR)},
  publisher={OpenReview},
}

@article{ofx2022,
  publtype={informal},
  author={Björn Plüster and Jakob Ambsdorf and Lukas Braach and Jae Hee Lee and Stefan Wermter},
  title={Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations},
  year={2022},
  cdate={1640995200000},
  journal={CoRR},
  volume={abs/2212.04231},
  url={https://doi.org/10.48550/arXiv.2212.04231},
}

@misc{llava12023,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@misc{llavanext2024,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{llavaimproved2023,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}

@inproceedings{repare2024,
    title={Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models},
    author={Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=L4nOxziGf9}
}

@InProceedings{lion2024,
    author    = {Chen, Gongwei and Shen, Leyang and Shao, Rui and Deng, Xiang and Nie, Liqiang},
    title     = {LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {26540-26550}
}

@InProceedings{ABK2023,
    author    = {Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
    title     = {Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {14974-14983}
}

@inproceedings{VGC2023,
  title={Visually grounded commonsense knowledge acquisition},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Li, Mengdi and Xie, Ruobing and Weber, Cornelius and Liu, Zhiyuan and Zheng, Hai-Tao and Wermter, Stefan and Chua, Tat-Seng and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={6583--6592},
  year={2023}
}

@inproceedings{sgck2021,
  title={Zero-shot scene graph relation prediction through commonsense knowledge integration},
  author={Kan, Xuan and Cui, Hejie and Yang, Carl},
  booktitle={Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part II 21},
  pages={466--482},
  year={2021},
  organization={Springer}
}

@InProceedings{DRC2019,
author = {Kim, Dong-Jin and Choi, Jinsoo and Oh, Tae-Hyun and Kweon, In So},
title = {Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{PV2TEA2023,
  title={PV2TEA: Patching visual modality to textual-established information extraction},
  author={Cui, Hejie and Lin, Rongmei and Zalmout, Nasser and Zhang, Chenwei and Shang, Jingbo and Yang, Carl and Li, Xian},
  journal={arXiv preprint arXiv:2306.01016},
  year={2023}
}

@inproceedings{vkg2021,
author = {Wang, Meng and Wang, Sen and Yang, Han and Zhang, Zheng and Chen, Xi and Qi, Guilin},
title = {Is Visual Context Really Helpful for Knowledge Graph? A Representation Learning Perspective},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475470},
doi = {10.1145/3474085.3475470},
abstract = {Visual modality recently has aroused extensive attention in the fields of knowledge graph and multimedia because a lot of real-world knowledge is multi-modal in nature. However, it is currently unclear to what extent the visual modality can improve the performance of knowledge graph tasks over unimodal models, and equally treating structural and visual features may encode too much irrelevant information from images. In this paper, we probe the utility of the auxiliary visual context from knowledge graph representation learning perspective by designing a Relation Sensitive Multi-modal Embedding model, RSME for short. RSME can automatically encourage or filter the influence of visual context during the representation learning. We also examine the effect of different visual feature encoders. Experimental results validate the superiority of our approach compared to the state-of-the-art methods. On the basis of in-depth analysis, we conclude that under appropriate circumstances models are capable of leveraging the visual input to generate better knowledge graph embeddings and vice versa.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2735–2743},
numpages = {9},
keywords = {representation learning, multi-modal, knowledge graph},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{gaia2020,
    title = "{GAIA}: A Fine-grained Multimedia Knowledge Extraction System",
    author = "Li, Manling  and
      Zareian, Alireza  and
      Lin, Ying  and
      Pan, Xiaoman  and
      Whitehead, Spencer  and
      Chen, Brian  and
      Wu, Bo  and
      Ji, Heng  and
      Chang, Shih-Fu  and
      Voss, Clare  and
      Napierski, Daniel  and
      Freedman, Marjorie",
    editor = "Celikyilmaz, Asli  and
      Wen, Tsung-Hsien",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.11/",
    doi = "10.18653/v1/2020.acl-demos.11",
    pages = "77--86",
    abstract = "We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system."
}

@InProceedings{ivlr2024,
    author    = {Yang, Cheng and Xu, Rui and Guo, Ye and Huang, Peixiang and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan and Zhou, Hong},
    title     = {Improving Vision-and-Language Reasoning via Spatial Relations Modeling},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2024},
    pages     = {769-778}
}

@article{medicalvqasurvey2023,
  title={Medical visual question answering: A survey},
  author={Lin, Zhihong and Zhang, Donghao and Tao, Qingyi and Shi, Danli and Haffari, Gholamreza and Wu, Qi and He, Mingguang and Ge, Zongyuan},
  journal={Artificial Intelligence in Medicine},
  volume={143},
  pages={102611},
  year={2023},
  publisher={Elsevier}
}

@article{vlmvqamedical2023,
  title={Vision--language model for visual question answering in medical imagery},
  author={Bazi, Yakoub and Rahhal, Mohamad Mahmoud Al and Bashmal, Laila and Zuair, Mansour},
  journal={Bioengineering},
  volume={10},
  number={3},
  pages={380},
  year={2023},
  publisher={MDPI}
}

@inproceedings{slake2021,
  title={Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering},
  author={Liu, Bo and Zhan, Li-Ming and Xu, Li and Ma, Lin and Yang, Yan and Wu, Xiao-Ming},
  booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  pages={1650--1654},
  year={2021},
  organization={IEEE}
}

@article{mmfvqa2023,
  title={The multi-modal fusion in visual question answering: a review of attention mechanisms},
  author={Lu, Siyu and Liu, Mingzhe and Yin, Lirong and Yin, Zhengtong and Liu, Xuan and Zheng, Wenfeng},
  journal={PeerJ Computer Science},
  volume={9},
  pages={e1400},
  year={2023},
  publisher={PeerJ Inc.}
}

@inproceedings{lingoqa2024,
  title={LingoQA: Visual question answering for autonomous driving},
  author={Marcu, Ana-Maria and Chen, Long and H{\"u}nermann, Jan and Karnsund, Alice and Hanotte, Benoit and Chidananda, Prajwal and Nair, Saurabh and Badrinarayanan, Vijay and Kendall, Alex and Shotton, Jamie and others},
  booktitle={European Conference on Computer Vision},
  pages={252--269},
  year={2024},
  organization={Springer}
}

@inproceedings{odawvqa2023,
  title={Explaining autonomous driving actions with visual question answering},
  author={Atakishiyev, Shahin and Salameh, Mohammad and Babiker, Housam and Goebel, Randy},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)},
  pages={1207--1214},
  year={2023},
  organization={IEEE}
}

@article{specification2023,
  title={Dealing with semantic underspecification in multimodal NLP},
  author={Pezzelle, Sandro},
  journal={arXiv preprint arXiv:2306.05240},
  year={2023}
}

@InProceedings{DLimgcap2018,
author = {Jiang, Wenhao and Ma, Lin and Jiang, Yu-Gang and Liu, Wei and Zhang, Tong},
title = {Recurrent Fusion Network for Image captioning},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@InProceedings{DLimgcap2017,
author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
title = {Boosting Image Captioning With Attributes},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{promptcap2023,
    author    = {Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A. and Luo, Jiebo},
    title     = {PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {2963-2975}
}

@InProceedings{ISCimgcap2022,
    author    = {Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Liang, Lin and Gan, Zhe and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
    title     = {Injecting Semantic Concepts Into End-to-End Image Captioning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {18009-18019}
}

@InProceedings{BKG2GSG2020,
author="Zareian, Alireza
and Karaman, Svebor
and Chang, Shih-Fu",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Bridging Knowledge Graphs to Generate Scene Graphs",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="606--623",
abstract="Scene graphs are powerful representations that parse images into their abstract semantic elements, i.e., objects and their interactions, which facilitates visual comprehension and explainable reasoning. On the other hand, commonsense knowledge graphs are rich repositories that encode how the world is structured, and how general concepts interact. In this paper, we present a unified formulation of these two constructs, where a scene graph is seen as an image-conditioned instantiation of a commonsense knowledge graph. Based on this new perspective, we re-formulate scene graph generation as the inference of a bridge between the scene and commonsense graphs, where each entity or predicate instance in the scene graph has to be linked to its corresponding entity or predicate class in the commonsense graph. To this end, we propose a novel graph-based neural network that iteratively propagates information between the two graphs, as well as within each of them, while gradually refining their bridge in each iteration. Our Graph Bridging Network, GB-Net, successively infers edges and nodes, allowing to simultaneously exploit and refine the rich, heterogeneous structure of the interconnected scene and commonsense graphs. Through extensive experimentation, we showcase the superior accuracy of GB-Net compared to the most recent methods, resulting in a new state of the art. We publicly release the source code of our method (https://github.com/alirezazareian/gbnet).",
isbn="978-3-030-58592-1"
}

@inproceedings{plmfewshot2021,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.295/",
    doi = "10.18653/v1/2021.acl-long.295",
    pages = "3816--3830",
    abstract = "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF{---}better few-shot fine-tuning of language models{---}a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30{\%} absolute improvement, and 11{\%} on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."
}

@inproceedings{cotrainfewshot2022,
  title={Co-training improves prompt-based learning for large language models},
  author={Lang, Hunter and Agrawal, Monica N and Kim, Yoon and Sontag, David},
  booktitle={International Conference on Machine Learning},
  pages={11985--12003},
  year={2022},
  organization={PMLR}
}

@article{llm0shot2022,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{llm0shottrain2024,
  title={Large language model as attributed training data generator: A tale of diversity and bias},
  author={Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{VCHG2023,
  title={Visual Commonsense based Heterogeneous Graph Contrastive Learning},
  author={Li, Zongzhao and Zhu, Xiangyu and Zhang, Xi and Zhang, Zhaoxiang and Lei, Zhen},
  journal={arXiv preprint arXiv:2311.06553},
  year={2023}
}

@inproceedings{VRinVLM2021,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1780--1790},
  year={2021}
}

@article{VR2021,
  title={Improving visual reasoning through semantic representation},
  author={Zheng, Wenfeng and Liu, Xiangjun and Ni, Xubin and Yin, Lirong and Yang, Bo},
  journal={IEEE access},
  volume={9},
  pages={91476--91486},
  year={2021},
  publisher={IEEE}
}

@inproceedings{VR2023,
  title={Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge},
  author={Zhang, Yifeng and Chen, Shi and Zhao, Qi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2573--2583},
  year={2023}
}

@article{VQAandVR2022,
  title={Vqa and visual reasoning: An overview of recent datasets, methods and challenges},
  author={Zakari, Rufai Yusuf and Owusu, Jim Wilson and Wang, Hailin and Qin, Ke and Lawal, Zaharaddeen Karami and Dong, Yuezhou},
  journal={arXiv preprint arXiv:2212.13296},
  year={2022}
}

@article{4omini2024,
  title={4o mini: Advancing cost-efficient intelligence, 2024},
  author={OpenAI, Gpt},
  journal={URL: https://openai. com/index/gpt-4o-mini-advancing-cost-efficient-intelligence},
  year={2024}
}

@article{visualreasoning2021,
  title={Interpretable visual reasoning: A survey},
  author={He, Feijuan and Wang, Yaxian and Miao, Xianglin and Sun, Xia},
  journal={Image and Vision Computing},
  volume={112},
  pages={104194},
  year={2021},
  publisher={Elsevier}
}

@InProceedings{VPCVR2023,
    author    = {Gupta, Tanmay and Kembhavi, Aniruddha},
    title     = {Visual Programming: Compositional Visual Reasoning Without Training},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {14953-14962}
}

@InProceedings{vizwiz2018,
author = {Gurari, Danna and Li, Qing and Stangl, Abigale J. and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P.},
title = {VizWiz Grand Challenge: Answering Visual Questions From Blind People},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{palix2022,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv preprint arXiv:2209.06794},
  year={2022}
}

@inproceedings{BLIP22023,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@misc{qwen2.5-VL,
    title = {Qwen2.5-VL},
    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},
    author = {Qwen Team},
    month = {January},
    year = {2025}
}

@article{Qwen2VL2024,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL2023,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}