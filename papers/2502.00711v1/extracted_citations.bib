@InProceedings{BKG2GSG2020,
author="Zareian, Alireza
and Karaman, Svebor
and Chang, Shih-Fu",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Bridging Knowledge Graphs to Generate Scene Graphs",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="606--623",
abstract="Scene graphs are powerful representations that parse images into their abstract semantic elements, i.e., objects and their interactions, which facilitates visual comprehension and explainable reasoning. On the other hand, commonsense knowledge graphs are rich repositories that encode how the world is structured, and how general concepts interact. In this paper, we present a unified formulation of these two constructs, where a scene graph is seen as an image-conditioned instantiation of a commonsense knowledge graph. Based on this new perspective, we re-formulate scene graph generation as the inference of a bridge between the scene and commonsense graphs, where each entity or predicate instance in the scene graph has to be linked to its corresponding entity or predicate class in the commonsense graph. To this end, we propose a novel graph-based neural network that iteratively propagates information between the two graphs, as well as within each of them, while gradually refining their bridge in each iteration. Our Graph Bridging Network, GB-Net, successively infers edges and nodes, allowing to simultaneously exploit and refine the rich, heterogeneous structure of the interconnected scene and commonsense graphs. Through extensive experimentation, we showcase the superior accuracy of GB-Net compared to the most recent methods, resulting in a new state of the art. We publicly release the source code of our method (https://github.com/alirezazareian/gbnet).",
isbn="978-3-030-58592-1"
}

@InProceedings{DLimgcap2017,
author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
title = {Boosting Image Captioning With Attributes},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{DLimgcap2018,
author = {Jiang, Wenhao and Ma, Lin and Jiang, Yu-Gang and Liu, Wei and Zhang, Tong},
title = {Recurrent Fusion Network for Image captioning},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@InProceedings{ISCimgcap2022,
    author    = {Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Liang, Lin and Gan, Zhe and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
    title     = {Injecting Semantic Concepts Into End-to-End Image Captioning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {18009-18019}
}

@article{PV2TEA2023,
  title={PV2TEA: Patching visual modality to textual-established information extraction},
  author={Cui, Hejie and Lin, Rongmei and Zalmout, Nasser and Zhang, Chenwei and Shang, Jingbo and Yang, Carl and Li, Xian},
  journal={arXiv preprint arXiv:2306.01016},
  year={2023}
}

@article{VCHG2023,
  title={Visual Commonsense based Heterogeneous Graph Contrastive Learning},
  author={Li, Zongzhao and Zhu, Xiangyu and Zhang, Xi and Zhang, Zhaoxiang and Lei, Zhen},
  journal={arXiv preprint arXiv:2311.06553},
  year={2023}
}

@inproceedings{cotrainfewshot2022,
  title={Co-training improves prompt-based learning for large language models},
  author={Lang, Hunter and Agrawal, Monica N and Kim, Yoon and Sontag, David},
  booktitle={International Conference on Machine Learning},
  pages={11985--12003},
  year={2022},
  organization={PMLR}
}

@inproceedings{covlm2024,
  author       = {Junyan Li and
                  Delin Chen and
                  Yining Hong and
                  Zhenfang Chen and
                  Peihao Chen and
                  Yikang Shen and
                  Chuang Gan},
  title        = {CoVLM: Composing Visual Entities and Relationships in Large Language
                  Models Via Communicative Decoding},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=PHGxChm1l5},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiCHCCSG24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{esnlive2021,
  title={e-vil: A dataset and benchmark for natural language explanations in vision-language tasks},
  author={Kayser, Maxime and Camburu, Oana-Maria and Salewski, Leonard and Emde, Cornelius and Do, Virginie and Akata, Zeynep and Lukasiewicz, Thomas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1244--1254},
  year={2021}
}

@InProceedings{ivlr2024,
    author    = {Yang, Cheng and Xu, Rui and Guo, Ye and Huang, Peixiang and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan and Zhou, Hong},
    title     = {Improving Vision-and-Language Reasoning via Spatial Relations Modeling},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2024},
    pages     = {769-778}
}

@misc{llava12023,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@article{llm0shot2022,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{llm0shottrain2024,
  title={Large language model as attributed training data generator: A tale of diversity and bias},
  author={Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nlxgpt2022,
  publtype={informal},
  author={Fawaz Sammani and Tanmoy Mukherjee and Nikos Deligiannis},
  title={NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks},
  year={2022},
  cdate={1640995200000},
  journal={CoRR},
  volume={abs/2203.05081},
  url={https://doi.org/10.48550/arXiv.2203.05081}
}

@inproceedings{plmfewshot2021,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.295/",
    doi = "10.18653/v1/2021.acl-long.295",
    pages = "3816--3830",
    abstract = "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF{---}better few-shot fine-tuning of language models{---}a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30{\%} absolute improvement, and 11{\%} on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."
}

@InProceedings{promptcap2023,
    author    = {Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A. and Luo, Jiebo},
    title     = {PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {2963-2975}
}

@inproceedings{rapper2024,
  author={Kai-Po Chang and Chi-Pin Huang and Wei-Yuan Cheng and Fu-En Yang and Chien-Yi Wang and Yung-Hsuan Lai and Yu-Chiang Frank Wang},
  title={RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=bshfchPM9H},
  booktitle={Proceedings of the 2024 International Conference on Learning Representations (ICLR)},
  publisher={OpenReview},
}

@inproceedings{repare2024,
    title={Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models},
    author={Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=L4nOxziGf9}
}

@inproceedings{smola2024,
  title={Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts},
  author={Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14205--14215},
  year={2024}
}

