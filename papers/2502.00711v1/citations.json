[
  {
    "index": 0,
    "papers": [
      {
        "key": "PV2TEA2023",
        "author": "Cui, Hejie and Lin, Rongmei and Zalmout, Nasser and Zhang, Chenwei and Shang, Jingbo and Yang, Carl and Li, Xian",
        "title": "PV2TEA: Patching visual modality to textual-established information extraction"
      },
      {
        "key": "llava12023",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "DLimgcap2018",
        "author": "Jiang, Wenhao and Ma, Lin and Jiang, Yu-Gang and Liu, Wei and Zhang, Tong",
        "title": "Recurrent Fusion Network for Image captioning"
      },
      {
        "key": "DLimgcap2017",
        "author": "Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao",
        "title": "Boosting Image Captioning With Attributes"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "promptcap2023",
        "author": "Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A. and Luo, Jiebo",
        "title": "PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3"
      },
      {
        "key": "ISCimgcap2022",
        "author": "Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Liang, Lin and Gan, Zhe and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng",
        "title": "Injecting Semantic Concepts Into End-to-End Image Captioning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ivlr2024",
        "author": "Yang, Cheng and Xu, Rui and Guo, Ye and Huang, Peixiang and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan and Zhou, Hong",
        "title": "Improving Vision-and-Language Reasoning via Spatial Relations Modeling"
      },
      {
        "key": "VCHG2023",
        "author": "Li, Zongzhao and Zhu, Xiangyu and Zhang, Xi and Zhang, Zhaoxiang and Lei, Zhen",
        "title": "Visual Commonsense based Heterogeneous Graph Contrastive Learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "covlm2024",
        "author": "Junyan Li and\nDelin Chen and\nYining Hong and\nZhenfang Chen and\nPeihao Chen and\nYikang Shen and\nChuang Gan",
        "title": "CoVLM: Composing Visual Entities and Relationships in Large Language\nModels Via Communicative Decoding"
      },
      {
        "key": "BKG2GSG2020",
        "author": "Zareian, Alireza\nand Karaman, Svebor\nand Chang, Shih-Fu",
        "title": "Bridging Knowledge Graphs to Generate Scene Graphs"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cotrainfewshot2022",
        "author": "Lang, Hunter and Agrawal, Monica N and Kim, Yoon and Sontag, David",
        "title": "Co-training improves prompt-based learning for large language models"
      },
      {
        "key": "plmfewshot2021",
        "author": "Gao, Tianyu  and\nFisch, Adam  and\nChen, Danqi",
        "title": "Making Pre-trained Language Models Better Few-shot Learners"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "llm0shottrain2024",
        "author": "Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao",
        "title": "Large language model as attributed training data generator: A tale of diversity and bias"
      },
      {
        "key": "llm0shot2022",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "repare2024",
        "author": "Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal",
        "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"
      },
      {
        "key": "smola2024",
        "author": "Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu",
        "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "esnlive2021",
        "author": "Kayser, Maxime and Camburu, Oana-Maria and Salewski, Leonard and Emde, Cornelius and Do, Virginie and Akata, Zeynep and Lukasiewicz, Thomas",
        "title": "e-vil: A dataset and benchmark for natural language explanations in vision-language tasks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "rapper2024",
        "author": "Kai-Po Chang and Chi-Pin Huang and Wei-Yuan Cheng and Fu-En Yang and Chien-Yi Wang and Yung-Hsuan Lai and Yu-Chiang Frank Wang",
        "title": "RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering"
      },
      {
        "key": "nlxgpt2022",
        "author": "Fawaz Sammani and Tanmoy Mukherjee and Nikos Deligiannis",
        "title": "NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks"
      }
    ]
  }
]