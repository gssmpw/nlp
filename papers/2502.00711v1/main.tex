\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
\usepackage{hyperref}
\usepackage[table,xcdraw]{xcolor}
\definecolor{lightgreen}{rgb}{0.8, 1, 0.8}
\usepackage{tikz}
\usepackage{float}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{mainfile}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{enumitem}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{{\scshape ViKSeR}: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework}

\begin{document}

\twocolumn[
\icmltitle{{\scshape ViKSeR}: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework}


% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{corresponding}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Chunbai Zhang}{sch,yyy}
\icmlauthor{Chao Wang}{sch,yyy,corresponding}
\icmlauthor{Yang Zhou}{xxx,corresponding}
\icmlauthor{Yan Peng}{sch,yyy}
\end{icmlauthorlist}

\icmlaffiliation{sch}{School of Future Technology, Shanghai University, Shanghai, 200444, China.}
\icmlaffiliation{yyy}{Institute of Artificial Intelligence, Shanghai University, Shanghai, 200444, China.}
\icmlaffiliation{xxx}{School of Mechatronic Engineering and Automation, Shanghai, 200444, China}

\icmlcorrespondingauthor{Chao Wang}{cwang@shu.edu.cn}
\icmlcorrespondingauthor{Yang Zhou}{saber\_mio@shu.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Chain-of-Evidence, Self-Reflection, Visual Knowledge, Visual Language Model, Visual Reasoning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Visual reasoning refers to the task of solving questions about visual information.
Current visual reasoning methods typically employ pre-trained vision-language model~(VLM) strategies or deep neural network approaches.
However, existing efforts are constrained by limited reasoning interpretability, while hindering by the phenomenon of underspecification in the question text.
Additionally, the absence of fine-grained visual knowledge limits the precise understanding of subject behavior in visual reasoning tasks.
To address these issues, we propose {\scshape ViKSeR}~(\textbf{Vi}sual \textbf{K}nowledge-Driven \textbf{Se}lf-\textbf{R}einforcing Reasoning Framework).
Specifically, {\scshape ViKSeR}, trained using knowledge distilled from large language models, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques.
Subsequently, {\scshape ViKSeR} utilizes fine-grained visual knowledge to paraphrase the question with underspecification.
Additionally, we design a novel prompting method called Chain-of-Evidence~(CoE), which leverages the power of ``evidence for reasoning'' to endow {\scshape ViKSeR} with interpretable reasoning capabilities.
Meanwhile, the integration of self-reflection technology empowers {\scshape ViKSeR} with the ability to learn and improve from its mistakes.
Experiments conducted on widely used datasets demonstrate that {\scshape ViKSeR} achieves new state-of-the-art~(SOTA) results in relevant tasks.

\end{abstract}

\section{Introduction}\label{sec:intro}
Endowing machines with robust logical reasoning capabilities has been a long-standing goal of vision-language models~(VLMs)~\cite{paliGemma2024,llava12023,VRinVLM2021}.
A critical step toward realizing the goal lies in enhancing the model's visual reasoning capabilities~\citep{smola2024,cola2023}.
Visual reasoning involves solving questions about visual information~\citep{VPCVR2023,visualreasoning2021}, a task that necessitates precise alignment between visual and textual features, along with advanced logical reasoning skills~\citep{rapper2024,repare2024,VR2023}.
Figure~\ref{fig:introduce} illustrates a typical example of visual reasoning, where an agent must accurately align the image with the question and infer the intent through multiple steps of logical reasoning.
The notable features of visual reasoning not only drive advancements in cross-modal learning~\citep{molmo72b2024,vlmvqamedical2023} but also contribute to enhancing machines' logical reasoning abilities~\citep{repare2024,lingoqa2024}, underscoring its substantial research significance.
\begin{figure}[!htbp]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{introduce_new.pdf}}
\caption{\textbf{A typical example of visual reasoning.} This complex task requires an agent to accurately align the image with the question and infer the intent through multi-step logical reasoning.}
\label{fig:introduce}
\end{center}
\vskip -0.3in
\end{figure}

Existing visual reasoning methods typically employ pre-trained VLM strategies or deep neural network frameworks~\citep{paliGemma2024,molmo72b2024,VQAandVR2022}.
However, \textit{the reasoning capabilities of current efforts still exhibit limited interpretability} (i.e., \textbf{Issue 1}).
Additionally, underspecification is a common phenomenon in visual reasoning tasks~\citep{repare2024, specification2023}, where a ambiguous description of the subject in the question text can hinder the alignment between textual and visual features, \textit{leading to multiple incorrect visual interpretations and undermining reasoning reliability} (i.e., \textbf{Issue 2}).
As shown in Figure~\ref{fig:introduce}, the subject ``he'' in the question is ambiguously described, which may be confused with other individuals in the image, presenting a risk of incorrect reasoning.
On the other hand, visual knowledge extraction (VKE) techniques, which provide enriched visual information, have the potential to significantly assist in addressing visual reasoning tasks~\citep{openvik2024,ivlr2024}. 
However, \textit{existing VKE methods are insufficient in extracting latent and fine-grained visual knowledge} (i.e., \textbf{Issue 3}).
The issue limits the accurate understanding of subject behavior in complex questions, thereby hindering the interpretability and precision required for visual reasoning tasks.

To address these issues, we design {\scshape ViKSeR}~(\textbf{Vi}sual \textbf{K}nowledge-Driven \textbf{Se}lf-\textbf{R}einforcing Reasoning Framework). 
The core components of {\scshape ViKSeR} are the fine-grained visual knowledge extraction~(\texttt{F-VKE}) module and the self-reinforcing reasoning~(\texttt{S-RR}) module.
For \textbf{Issue 1}, we deploy the \texttt{S-RR} module, which offers enhanced interpretability in reasoning and integrates self-reinforcement capabilities.
Specifically, the module integrates a novel prompting technique, Chain-of-Evidence~(CoE), which we propose to enhance interpretability by facilitating step-by-step reasoning grounded in factual evidence.
Meanwhile, a self-reflection mechanism, which utilizes insights from past failures to refine future reasoning, is introduced to facilitate adaptive reinforcement.
Additionally, intending to address \textbf{Issue 2},  a specification paraphrase method is explored for the \texttt{S-RR} module.
Concretely, we collect object-level information from fine-grained visual knowledge to refine ambiguous descriptions of the subject in question, thereby improving the question's clarity and completeness.

To address \textbf{Issue 3}, we deploy the \texttt{F-VKE} module to provide fine-grained visual knowledge to {\scshape ViKSeR}.
To be specific, the \texttt{F-VKE} module first detects the detailed visual relationships among key entities in an input image.
Subsequently, we train the \texttt{F-VKE} module using knowledge distilled from large language models (LLMs) to uncover causal relationships between entities' behaviors and their inferred outcomes. For instance, as shown in Figure~\ref{fig:kdAncase}, LLMs can infer that a man depicted squatting on the right side of the train door may disembark at the next station.
Following this, the trained \texttt{F-VKE} module leverages visual and causal relationships to generate fine-grained visual knowledge.
A specific case that validates the fine-grained nature and richness of the visual knowledge generated by the \texttt{F-VKE} module is provided in Section~\ref{subsec:case study}.
We conduct extensive experiments on diverse widely-recognized datasets to validate {\scshape ViKSeR}'s visual knowledge extraction and reasoning capabilities. The results demonstrate that {\scshape ViKSeR} outperforms the latest research across all datasets, achieving new SOTA results.

We summarize our \textbf{contributions} as follows:
\begin{itemize}
\item We propose {\scshape ViKSeR}, a novel framework for visual reasoning tasks, which extracts fine-grained and enriched visual knowledge while performing highly interpretable and self-reinforcing reasoning.
\item A specification paraphrase method is designed to help {\scshape ViKSeR} mitigate underspecification, while a novel CoE prompting technique and a self-reflection mechanism are introduced to assist {\scshape ViKSeR} in performing highly interpretable and self-reinforcing reasoning. 
\item Extensive experimental results demonstrate {\scshape ViKSeR}’s improvements over advanced baselines across diverse public datasets, achieving new SOTA results.
\end{itemize}
\section{Related Work} \label{sec:related}
\paragraph{Visual Knowledge Extraction}
Existing VKE methods either employ a holistic image captioning strategy or rely on fixed knowledge formats to extract visual information~\citep{PV2TEA2023,llava12023}.
Specifically, deep learning has played a crucial role in advancing image captioning approaches~\citep{DLimgcap2018,DLimgcap2017}.
With the development of VLMs, an increasing number of studies leverage pre-trained multimodal large language models~(MLLMs) to understand both visual and textual information in a unified framework~\citep{promptcap2023,ISCimgcap2022}. 
On the other hand, the image captioning methods provide a broad understanding of the image content, yet they encounter challenges in conveying fine-grained details.
In contrast, some studies utilize fixed knowledge graphs to map visual features to predefined semantic categories~\citep{ivlr2024,VCHG2023}, providing a structured form of visual knowledge~\citep{covlm2024,BKG2GSG2020}. While these methods ensure consistency and interpretability, they fall short in providing enriched visual knowledge.
\paragraph{Language Model Reasoning}
Recently, large VLMs have demonstrated remarkable success in reasoning and inference, particularly in facilitating few-shot~\citep{cotrainfewshot2022,plmfewshot2021} and zero-shot~\citep{llm0shottrain2024,llm0shot2022} learning. 
Meanwhile, the potential of prompt-based reasoning has been extensively explored to tackle diverse cross-modal visual reasoning tasks, including visual question answering~(VQA)~\citep{repare2024,smola2024}, visual entailment~(VE)~\citep{esnlive2021}, and visual commonsense reasoning~(VCR)~\citep{rapper2024,nlxgpt2022}. 
By leveraging the substantial information embedded within VLMs, novel insights are generated to advance reasoning and interpretability research.

\section{Method}\label{sec:methods}
In this section, we provide detailed descriptions of {\scshape ViKSeR}'s \texttt{F-VKE} module and \texttt{S-RR} module. 
Specifically, the \texttt{F-VKE} module consists of two agents: the visual relationship detector~(\textit{Ag-VRD}) and the visual knowledge enRicher~(\textit{Ag-VKR}), where \textit{Ag-VKR} consists of a causal relationship analyzer $ G_a $ and an image caption generator $ G_c $.
Starting with the input image $ I $ and question $ Q $, \textit{Ag-VRD} first detects the visual relationships between key entities in $ I $ and integrates these relationships into a preliminary visual description $ D $.
Following this, $ G_a $ in \textit{Ag-VKR} provides an analysis report $ A $ of the causal relationships between key entities' behaviors in $ D $ and their inferred outcomes.
For instance, a description of a man squatting at the train door implicitly suggests that he may disembark at the next station.
Finally, leveraging $ A $, $ G_c $ in \textit{Ag-VKR} enriches $ D $ into a more detailed and nuanced image caption $ C $, offering fine-grained visual knowledge.
On the other hand, the \texttt{S-RR} module is composed of the specification paraphraser~(\textit{Ag-SPR}) and the self-refining reasoner~(\textit{Ag-SR}). 
Specifically, \textit{Ag-SPR} first collects visual information from $ C $ to refine ambiguous descriptions of the subject in $ Q $, thereby forming the paraphrased question $ Q_r $.
Subsequently, \textit{Ag-SR} employs CoE prompting to infer the predicted answer $ \tilde{a} $ based on $ Q_r $, $ I $, and $ C $. When $ \tilde{a} $ exhibits positivity, it directly serves as the final answer $ a $. Conversely, if $ \tilde{a} $ shows negativity, \textit{Ag-SR} will use $ \tilde{a} $ and the reasoning trajectory $ t $ to perform self-reflection in search of a more valuable response. Figure~\ref{fig:framework} illustrates a detailed framework of {\scshape ViKSeR}. Next, we will provide a detailed introduction to the modules of {\scshape ViKSeR}.
\begin{figure*}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{framkework_new_v2.pdf}}
\caption{\textbf{The framework of {\scshape ViKSeR}}. Starting with the input image and question, {\scshape ViKSeR} first extracts the relationships between key entities in the image and analyzes these relationships to form a detailed image caption. Then, {\scshape ViKSeR} uses the image caption to paraphrase the question, refining the ambiguous
descriptions in the question. Finally, {\scshape ViKSeR} infers a predicted answer based on the paraphrased question, the image, and its caption. The predicted answer is taken as the final answer if it sufficiently addresses the question. Otherwise, {\scshape ViKSeR} performs self-reflection based on the reasoning trajectory, the paraphrased question, the image, and its caption to seek a more valuable response.}
\label{fig:framework}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Fine-Grained Visual Knowledge Extraction}\label{subsec:FVKE}
Guided by the principles of multi-agent collaboration, the functionality of the \texttt{F-VKE} module in extracting fine-grained visual knowledge is distributed across two agents: the visual relationship detector \textit{Ag-VRD} and the visual knowledge enRicher \textit{Ag-VKR}.
The architectural design of \textit{Ag-VRD} and the \textit{Ag-VKR} is elaborated as follows.

\paragraph{Visual Relationship Detector}\label{subsubsec:VRD}
Accurate detection of entities in images and their relationships form the foundation for effective VKE techniques. 
With the aim of endowing the \texttt{F-VKE} module with efficient visual relationship detection capabilities, we design the \textit{Ag-VRD} based on existing VRD methods.
Concretely, given an input image $ I $, the \textit{Ag-VRD} is prompted to extract all entities within the image.
Each entity is then assigned a validity score $S_e$, reflecting its relevance in understanding the image's content.
Notably, when the entity's score exceeds a predefined threshold $\theta_e$, it is classified as a key entity $K_e$.
Following this, with the aid of CoVLM~\citep{covlm2024}, we extract all relevant visual relationships of $K_e$. 
CoVLM is an exceptional VRD method that facilitates the collaboration between visual detection networks and LLMs, utilizing specially designed communication tokens. The advantage of CoVLM lies in its ability to precisely identify the target entity $ E_B $ corresponding to a given relationship based on the input entity $ E_A $, by leveraging the specially crafted communication tokens.
However, due to the fixed structure of its communication tokens, CoVLM is limited in offering fine-grained visual knowledge. Therefore, we utilize CoVLM primarily for refining the detected visual relationships.

To be specific, we first employ the detection network of CoVLM to identify the regions of interest corresponding to each $K_e$ in $ I $. 
Based on these regions and their associated entities, the \textit{Ag-VRD} is then prompted to detect all potential relationships associated with $K_e$.
Subsequently, we employ CoVLM to detect the relevant entities corresponding to these potential relationships. 
These potential relationships are then merged with their associated entities to form the visual relationship $ r $.
Following this, the \textit{Ag-VRD} is employed to assess a relationship validity score $ S_r $ for each $ r $ based on its effectiveness in contributing to the understanding of $ I $.
To determine which visual relationships associated with the key entities are truly effective in capturing the semantics of the input image, we propose a joint entity-relation validity evaluation algorithm to compute the entity-relation joint validity score $ S_r^e $:
\begin{equation}
S_r^e = S_e \cdot \omega_r \cdot S_r, \omega_r = 1 + \gamma (\alpha - N_r^e),
\label{eq:jointscore}
\end{equation}
where $S_e$ denotes the entity validity score and $S_r$ denotes the relationship validity score. $ \omega_r $ represents the weight associated with the current visual relationship $ r $. $ N_r^e $ denotes the number of $ r $ held by the key entity $ K_e $. $ \gamma $ and $ \alpha $ are hyperparameters, where $ \gamma $ controls the influence of the number of visual relationships on the weight, constrained within the range $ (0.05, 0.2) $. And $ \alpha $ limits the number of visual relationships.
For example, as $ N_r^e $ increases, $ \omega_r $ decreases to amplify the constraint of $ S_r^e $ on the visual relationship. Conversely, $ \omega_r $ increases when the number of relationships is smaller.
Additionally, in case $ S_r^e $ exceeds a threshold $ \theta_r^e $, the corresponding visual relationship under the current key entity is labeled as a key visual relationship $ K_r $.
After identifying $ K_e $ and $ K_r $, we generate the preliminary image description $ D $ by progressively aligning each $ K_e $ with its corresponding $ K_r $.

\paragraph{Visual Knowledge Enricher}\label{subsubsec:VKR}
Existing VRD methods primarily focus on detecting surface-level, concept-based relational information within an image~\citep{covlm2024, spatialRelation2024}. 
This nature makes it challenging for current VRD methods to conduct in-depth, object-level analysis of the image content. 
However, such analyses are essential for a comprehensive understanding of the image.
For this reason, we further deploy \textit{Ag-VKR}, which consists of a causal relationship analyzer $ G_a $ and an image caption generator $ G_c $, to enhance the depth of knowledge in image analysis.
Specifically, $ G_a $ first interprets the preliminary image description $ D $ concerning the input image $ I $. 
Next, $ G_a $ derives an analysis report $ A $ that uncovers the causal relationships between the behaviors of key entities in $ D $ and their inferred outcomes.
Subsequently, $ G_c $ enriches $ D $ by incorporating the visual knowledge from $ I $ and $ A $, resulting in a detailed image caption $ C $ for $ I $.

Casual relationship analysis reporting and detailed image captioning typically require training with ground-truth data, which is often absent in existing datasets.
To address this, we propose leveraging the extensive knowledge reservoir inherent in LLMs (e.g., ChatGPT-4~\citep{gpt42023}) to generate causal relationship analysis reports and detailed image captions as pseudo-ground-truth data to train $ G_a $ and $ G_c $.
Specifically, we extract pseudo-ground-truth causal relationship analysis report $ A_p $ from LLM with a task-specific set of few-shot demonstrations as follows:
\begin{equation}
    KA_p = \left \{ A_p \ \middle| \ A_p \sim P_{LLM}(I, D) \right \},
    \label{eq:KDSAnp}
\end{equation}
where $ I $ denotes the input image, $ D $ represents the ground-truth preliminary image description of $ I $, and $ P_{LLM} $ denotes the LLM operating in an autoregressive manner. $ A_p $ represents the pseudo-ground-truth causal relationship analysis report sampled from $ P_{LLM} $, and $ KA_p $ represents the set of all $ A_p $.
However, $ KA_p $ may be noisy and erroneous, which could adversely affect the subsequent detailed image captioning process.
To address this, we apply a post-processing mechanism to filter $ KA_p $ into $ KA'_p $.
Specifically, for each $ A_p $ in $ KA_p $, we use a pre-trained MLLM (denoted as $F$) to assess its validity score $ S_{A_p} $ based on whether $ A_p $ correctly uncovers the causal relationship. In case $ S_{A_p} $ exceeds a predetermined threshold $ \tau $, the corresponding $ A_p $ is retained. The process of collecting $ KA’_p $  is formalized as follows:
\begin{equation}
    KA'_p = \left\{ A_p \ \middle| \ S_{A_p} > \tau \right\}, S_{A_p} = F\left(A_p, (I, D)\right),
    \label{eq:SANpScore}
\end{equation}
where $ A_p $ denotes the pseudo-ground-truth causal relationship analysis report. $ I $ denotes the input image, $ D $ represents the ground-truth preliminary image description of $ I $. $ \tau $ denotes the predetermined threshold. $ F $ denotes the pre-trained MLLM.
With $ KA’p $ serving as pseudo-ground-truth data, we are able to train $ G_a $ with the distillation loss $ L_{G_a} $, as formalized below:
\begin{equation}
    L_{G_a} = - \sum_{t=1}^{T} \log \left( P_{G_a} \left( A_{p,t}' \ \middle| \ A_{p,t-1}', (I,D) \right) \right),
    \label{eq:GaKD}
\end{equation}
where $ A’_p \in KA’_p $, and $ T = |A’_p| $. Figure~\ref{fig:kdAncase} shows how we utilize knowledge distillation from the LLM to train $ G_a $.
Specifically, for the Image and Description, a post-processing mechanism employs $ F $ to determine whether $ S_{A_p} $ is passing, thereby filtering out the noise from the LLM-generated $ A_p $ to obtain $ A'_p $.
Following this, $ A'_p $ is used to train $ G_a $ via $ L_{G_a} $, in order to generate $ A $.
\begin{figure}[h]
\vskip 0.05in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{kd_an_case_new.pdf}}
\caption{Process of utilizing knowledge distillation from LLM to train $G_a$ to generate $A$.}
\label{fig:kdAncase}
\end{center}
\vskip -0.35in
\end{figure}

Similarly, we employ the same knowledge distillation approach to train $ G_c $. Concretely, we utilize $ KA’_p $ as input to help $ G_c $ enrich the preliminary image description $ D $, thereby generating pseudo-ground-truth image captions $ KC’_p $. Subsequently, we train the image caption generator $ G_c $ with the distillation loss $ L_{G_C} $ described below:
\begin{equation}
    L_{G_c} = - \sum_{t=1}^{T} \log \left( P_{G_c} \left( C_{p,t}' \ \middle| \ C_{p,t-1}', (I, D, A'_p) \right) \right),
    \label{eq:GcKD}
\end{equation}
where $ C’_p \in KC’_p $, $ A’_p \in KA’_p $, and $ T = |C’_p| $. More details are discussed in Appendix~\ref{apd:trainGc}.
In summary, the trained causal relationship analyzer $ G_a $ and image caption generator $ G_c $ operate collaboratively to fulfill the functionality of \textit{Ag-VKR}. Specifically, given an input image $ I $ and the preliminary image description $ D $, $ G_a $ and $ G_c $ generate a detailed image caption $ C $ for $ I $.

\subsection{Self-Reinforcing Reasoning}\label{subsec:SRR}
We leverage the detailed visual knowledge in the image caption $ C $, which is generated by the \texttt{F-VKE} module, to enhance the performance of {\scshape ViKSeR} in visual reasoning tasks, such as VQA and VE.
To achieve this, we propose an \texttt{S-RR} module, consisting of a specification paraphraser \textit{Ag-SPR} and a self-refining reasoner \textit{Ag-SR}, to endow {\scshape ViKSeR} with advanced reasoning capabilities.
Specifically, starting with an image and a question, \textit{Ag-SPR} extracts and analyzes the fine-grained visual knowledge in $ C $, to refine ambiguous descriptions in the question and paraphrase it. 
Subsequently, \textit{Ag-SR} resolves the paraphrased question by leveraging the input image and the detailed image caption through a self-reinforcing reasoning paradigm.
It is important to emphasize that we aim to develop a reasoning approach characterized by high generalizability and a gradient-free, in light of the diversity of visual reasoning tasks.. For this reason, we propose employing a flexible and plug-and-play reasoning prompt paradigm to prompt pre-trained visual language models~(PVLMs) to perform the functions of \textit{Ag-SPR} and \textit{Ag-SR}. Detailed prompts are discussed in Appendix~\ref{apd:prompts}. The architectural design of \textit{Ag-SPR} and the \textit{Ag-SR} is elaborated as follows.

\paragraph{Specification Paraphraser}\label{subsubsec:specification paraphraser}
Starting with the input image $ I $, the original question $ Q $, and the detailed image caption $ C $ generated by the \texttt{F-VKE} module, \textit{Ag-SPR} leverages the detailed visual information in $ C $ to paraphrase $ Q $, thereby refining any ambiguous descriptions within $ Q $. We employ a PVLM as the core of \textit{Ag-SPR} to facilitate this process.
Specifically, we prompt the PVLM in \textit{Ag-SPR} to first identify the main subject in $ Q $, and then query $ C $ for relevant textual information. Meanwhile, the PVLM is prompted to extract the interaction between the main subject and the image scene from $ C $, providing a complementary description of the main subject's context. Subsequently, the above textual information is paraphrased in natural language and integrated into $ Q $ to form the paraphrased question $ Q_r $. The underlying logic of this process is that the entities mentioned in $ Q $ intuitively provide key information relevant to the expected answer intuitively.
For this reason, providing more detailed descriptions of the subject aids the \texttt{S-RR} module in resolving the visual reasoning task.

\paragraph{Self-Refining Reasoner}\label{subsubsec:selfrefining reasoner}
As a key component of the \texttt{S-RR} module, the cognitive and reasoning capabilities of \textit{Ag-SR} significantly influence the overall problem-solving performance of the system. To enhance \textit{Ag-SR}'s capabilities, we introduce a prompting technique called CoE, which guides \textit{Ag-SR} to reason incrementally and provide highly interpretable steps toward the expected answer of $ Q_r $. Furthermore, we present a self-reflection mechanism that allows \textit{Ag-SR} to perform self-correction, thereby improving its robustness in reasoning. Detailed descriptions are as follows.

\textit{Highly Interpretable Reasoning}: We prompt a PVLM to serve as the core of \textit{Ag-SR}.
Given that conventional reasoning prompts, such as "think step-by-step"~\citep{COT2022}, often induce hallucinations due to insufficiently grounded rationale, we propose CoE, a tailored prompting technique, to enhance the PVLM’s reasoning ability.
Concretely, the PVLM first analyzes fine-grained visual knowledge provided by the \texttt{F-VKE} module, extracting factual information to serve as evidence for reasoning.
Subsequently, CoE guides the PVLM through a structured, step-by-step reasoning process based on the extracted evidence, ultimately generating a predicted answer $ \tilde{a} $. 
This reasoning framework enables CoE to enhance interpretability while improving reasoning accuracy.
Further details on CoE prompting are provided in Appendix~\ref{apd:coeprompt}.

\textit{Self-Reflection-Based Reinforcement Mechanism}: Existing research has shown that even highly intelligent agents are susceptible to generating low-quality answers, which significantly diminishes the accuracy of their reasoning and decision-making processes.
To address this, we propose a self-reflection mechanism that builds upon the method proposed by Shinn et al~\citep{reflexion2023}. The mechanism enables \textit{Ag-SR} to manage instances of low-quality answers by leveraging past experiences, thereby reducing the likelihood of similar errors occurring in the future.
Specifically, after deriving $ \tilde{a} $, \textit{Ag-SR} utilize an exact matching mechanism to map complex natural language information in $ \tilde{a} $ to a two-dimensional discrete binary reward score $ S_{ref} $. 
If $ S_{ref} $ is positive, the predicted answer $ \tilde{a} $ is directly used as the final answer $ a $. 
In contrast, when $ S_{ref} $ is represented as negative, \textit{Ag-SR} is prompted to analyze $ Q_r $ and a reasoning trajectory $ t $ from the previous trial. Subsequently, \textit{Ag-SR} reflects on the cause of failure and generates a detailed verbal reflection, denoted as $ V_{ref} $. 
Following this, $ V_{ref} $ serves as experiential knowledge to guide \textit{Ag-SR} in generating a more accurate answer in subsequent trials.
It is important to note that, compared to the reward score $ S_{ref} $, $ V_{ref} $ encompasses more comprehensive experiential information. For instance, while $ S_{ref} $ merely indicates success or failure, $ V_{ref} $ provides insight into which aspects of the previous reasoning trajectory were incorrectly applied, resulting in previous failure. Furthermore, $ V_{ref} $ can provide guidance on how to avoid similar mistakes in future trials.

\section{Experiments}\label{sec:experiments}
In this section, we conduct extensive experiments on publicly available datasets to evaluate {\scshape ViKSeR}'s performance.

\subsection{Experimental Setup}\label{subsec:experimental setup}
In our experiments, we prompt GPT-4o mini~\citep{4omini2024} as the LLM foundation for \textit{Ag-VRD} of the \texttt{F-VKE} module in {\scshape ViKSeR}.
Additionally, we employ LLaVa-1.5-7B~\citep{llava12023} as the PVLM foundation for the \texttt{S-RR} module of {\scshape ViKSeR}. 
It is important to note that in Section~\ref{subsubsec:VKR}, the LLM used for knowledge distillation is ChatGPT-4~\citep{gpt42023}, while $ G_a $ and $ G_c $ are obtained through training LLaVa-1.5-7B.
On the other hand, a summary of the experimental setup is provided, which includes the datasets, baseline methods, and evaluation metrics employed for performance assessment.

\textbf{Datasets:~}To evaluate the reasoning abilities of {\scshape ViKSeR}, we utilize widely-used datasets for visual reasoning tasks, including VQAv2~\citep{vqav22017}, A-OKVQA~\citep{aokvqa2022}, VizWiz~\citep{vizwiz2018}, e-SNLI-VE~\citep{esnlive2021}, CoLA~\citep{cola2023}, and CREPE~\citep{crepe2023}.

\textbf{Baselines:~}We compare {\scshape ViKSeR} with \textbf{12} competitive baselines, including SMoLA-PaLI-X~\citep{smola2024}, LLaVa-1.5-7B~\citep{llava12023}, REPARE~\citep{repare2024}, CoVLM~\citep{covlm2024}, Molmo-72B~\citep{molmo72b2024}, PaLi-X~\citep{palix2022}, PaliGemma 2~\citep{paliGemma2024}, NLX-GPT~\citep{nlxgpt2022}, Rapper~\citep{rapper2024}, e-UG~\citep{esnlive2021}, OFX-X~\citep{ofx2022}, MosaiCLIP~\citep{mosai2023}, CLIP+ MM-Pred~\citep{cola2023}, and  CLIP+ Linear~\citep{cola2023}.

\textbf{Evaluation Metrics:~}Based on previous work~\citep{cola2023,crepe2023,repare2024,rapper2024}, we adopt appropriate dataset-specific metrics for evaluation. Specifically, 
(1) For the VQAv2 dataset, we evaluate accuracy separately for the yes/no~(Y/N), number~(Num.), and other~(Other) types of questions, and additionally evaluate overall accuracy~(Overall). 
(2) For the A-OKVQA dataset, we evaluate both Direct Answer accuracy~(DA.) and Multiple Choice accuracy~(MC.).
(3) For the VizWiz dataset, we evaluate overall accuracy~(Overall). 
(4) For the Cola, CREPE, and e-SNLI-VE datasets, we assess Accuracy~(Acc.). Notably, for the CREPE dataset, we further evaluate Recall@1~(R@1). 


\subsection{Main Results}\label{subsec:taskreasoning}
We conduct extensive experiments to validate {\scshape ViKSeR}'s visual reasoning capabilities, with the main experimental results summarized as follows.

\textbf{{\scshape ViKSeR} performs outstandingly in reasoning and answering visual questions.~}To validate the superiority of {\scshape ViKSeR}'s visual reasoning capabilities, we compare its performance with eight competitive baselines on the VQAv2, A-OKVQA, and VizWiz datasets, as presented in Table~\ref{tab:vqa task}.
To be specific, {\scshape ViKSeR} achieves new SOTA results across all datasets, with the exception of the Num. metric on the VQAv2 dataset.
Notably, when LLaVa-1.5-7B is used as the PVLM foundation for the \texttt{S-RR} module, {\scshape ViKSeR} significantly outperforms LLaVa-1.5-7B across all metrics.
In particular, on the MC. metric of the A-OKVQA dataset, {\scshape ViKSeR} achieves an almost 30\% improvement over LLaVa-1.5-7B. 
Meanwhile, {\scshape ViKSeR}, employing a gradient-free reasoning paradigm, continues to outperform task-specific methods pre-trained on the datasets.
These substantial advantages demonstrate the superiority and generalizability of {\scshape ViKSeR}'s reasoning paradigm.
Moreover, {\scshape ViKSeR}, with fewer parameters, outperforms baselines with more parameters, such as Molmo-72B and PaliGemma-2 (10B), across all metrics except for Num. The advancements highlight {\scshape ViKSeR}'s potential in visual reasoning tasks.
\begin{table*}[h]
\caption{Comparative performance of different methods on the VQAv2, A-OKVQA, and VizWiz datasets. \raisebox{0.5ex}{\colorbox{lightgreen}{\makebox[1em]{}}} represents the optimal scores, while \raisebox{0.5ex}{\colorbox{yellow!30}{\makebox[1em]{}}} represents the suboptimal scores. We replicate the scores of LLaVa-1.5-7B and CoVLM, while the scores for the remaining baselines are sourced from the official repositories of VQAv2, A-OKVQA, and VizWiz datasets, as well as the study conducted by Prasad et al.~\citep{repare2024}.}
\label{tab:vqa task}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{3.2cm}>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.5cm}}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{VQAv2}} & \multicolumn{2}{c}{\textbf{A-OKVQA}} & \textbf{VizWiz} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-8}
& Overall~(\%) & Y/N~(\%) & Num.~(\%) & Other~(\%) & MC.~(\%) & DA.~(\%) & Overall~(\%) \\
\midrule
\multicolumn{8}{l}{General Methods} \\
LLaVa-1.5-7B & 74.04 & 86.84 & 52.94 & 57.89 & 62.56 & 77.38  & 57.07\\
REPARE $+$ LLaVa-1.5 & 77.34 & 92.64 & 59.92 & 70.12 & 66.19 & \cellcolor{yellow!30}78.21 & 59.46 \\
REPARE $+$ BLIP-2 & 74.05 & 94.56 & 54.40 & 63.36 & 55.67 & 82.80 & 70.03 \\
Molmo-72B & 86.67 & 96.91 & \cellcolor{lightgreen}\textbf{78.49} & 79.63 & 82.20 & 74.95 & - \\
\midrule
\multicolumn{8}{l}{VRD Methods} \\
CoVLM & 48.80 & 64.02 & 36.85 & 40.37 & 46.47 & 52.35 & 44.32\\
\midrule
\multicolumn{8}{l}{Task-Specific Methods} \\
SMoLA-PaLI-X & 85.00 & - & - & - & \cellcolor{yellow!30}84.1 & 70.55 & 72.00 \\
PaLI-X & 86.06 & 96.78 & 74.14 & 79.46 & 80.4 & 68.20 & 73.30 \\
PaLI-Gemma 2 & \cellcolor{yellow!30}86.95 & \cellcolor{yellow!30}97.19 & 77.77 & \cellcolor{yellow!30}80.13 & 83.7 & 71.30 & \cellcolor{yellow!30}78.10 \\
\midrule
{\scshape ViKSeR}~(ours) & \cellcolor{lightgreen}\textbf{88.74} & \cellcolor{lightgreen}\textbf{98.31} & \cellcolor{yellow!30}75.51 & \cellcolor{lightgreen}\textbf{86.71} & \cellcolor{lightgreen}\textbf{92.51} & \cellcolor{lightgreen}\textbf{88.53} & \cellcolor{lightgreen}\textbf{82.52} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

On the other hand, we analyze the reasons for {\scshape ViKSeR}'s slight underperformance on the Num. metric of the VQAv2 dataset as follows: (1){\scshape ViKSeR} has fewer model parameters, larger models, such as Molmo-72B, benefit from a larger parameter size; (2) {\scshape ViKSeR} prioritizes extracting information from key entities in the image, which may lead to a potential risk of overlooking a holistic analysis of the image. For these reasons, {\scshape ViKSeR} performs slightly worse on visual reasoning tasks involving numerical questions compared to more powerful LLMs, such as Molmo-72B. Nonetheless, we believe there is still room for improvement in {\scshape ViKSeR}'s numerical analysis performance.

\textbf{{\scshape ViKSeR} performs excellently in reasoning and understanding visual information.}
To further evaluate the reasoning performance of {\scshape ViKSeR}, we compare it with nine competitive baselines on the Cola, CREPE, and e-SNLI-VE dataset, as shown in Table~\ref{tab:vke} and Figure~\ref{fig:esnlive}. 
Specifically, {\scshape ViKSeR} outperforms existing methods, achieving a new SOTA result. 
It is worth noting that both the Cola and CREPE datasets require a model to precisely match images with captions. Therefore, the success of {\scshape ViKSeR} highlights its capacity to accurately interpret image content and effectively extract information from key entities, enabling it to perform visual reasoning tasks with high precision.
\begin{table}[h]
\vskip -0.15in
\caption{Comparative performance of different methods on the Cola and CREPE datasets. The baseline scores are sourced from related studies~\citep{covlm2024,cola2023}.}
\label{tab:vke}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
\multirow{2}{*}{\textbf{Methods}} & Cola  & \multicolumn{2}{c}{CREPE} \\
\cmidrule(lr){2-2} \cmidrule(lr){3-4}
        & Acc.~(\%) & Acc.~(\%)  & R@1~(\%) \\
\midrule
MosaiCLIP    & - & - & \cellcolor{yellow!30}90.2 \\
CLIP $+$ MM-Pred  & 41.42 & 77.84 & - \\
CLIP $+$ Linear   & 30.47 & \cellcolor{yellow!30}87.35 & - \\
CoVLM  & \cellcolor{yellow!30}44.29 & - & - \\
{\scshape ViKSeR}~(ours)       & \cellcolor{lightgreen}\textbf{49.49} & \cellcolor{lightgreen}\textbf{96.68} & \cellcolor{lightgreen}\textbf{93.81} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

Additionally, as demonstrated in Figure~\ref{fig:esnlive}, {\scshape ViKSeR} performs outstandingly on the e-SNLI-VE dataset.
Notably, {\scshape ViKSeR} surpasses OFX-X by 8.72\%, highlighting its effectiveness. 
The success of {\scshape ViKSeR} underscores its ability to correctly infer the visual scene present in an image based on the visual information. We attribute this advantage to the seamless collaboration between the \texttt{F-VKE} and \texttt{S-RR} modules, as well as the flexible reasoning paradigm within the \texttt{S-RR} module. As discussed in Section~\ref{subsec:SRR}, the plug-and-play nature and gradient-free design of the reasoning paradigm in the \texttt{S-RR} module endow {\scshape ViKSeR} with high generalization capabilities, enabling it to adapt to diverse visual reasoning tasks.
\begin{figure}[h]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{esnlive.pdf}}
\caption{Comparative performance of different methods on the e-SNLI-VE dataset. The baseline scores are sourced from the official repository of e-SNLI-VE and the seminal study conducted by Chang et al.~\citep{rapper2024}.}
\label{fig:esnlive}
\end{center}
\vskip -0.45in
\end{figure}

\subsection{Ablation Studies}\label{subsec:ablation studies}
We conduct various ablation experiments to evaluate the performance of each component of {\scshape ViKSeR}. Specifically, on the VQAv2 dataset, we systematically ablate the \textit{Ag-SPR} and self-reflection mechanism in the \texttt{S-RR} module to assess their impact on the performance of {\scshape ViKSeR}.
As shown in Table~\ref{tab:ablation}, on the VQAv2 dataset, ablating \textit{Ag-SPR} individually results in the least impact on overall performance.
In contrast, the simultaneous ablation of both \textit{Ag-SPR} and the self-reflection mechanism leads to the most significant performance drop. 
Such findings suggest that both \textit{Ag-SPR} and the self-reflection mechanism significantly contribute to the performance of {\scshape ViKSeR}, with the self-reflection mechanism having a more substantial impact.
\begin{table}[h]
\vskip -0.15in
\caption{Ablation results of experiments conducted on the VQAv2 and Cola datasets. 
\begin{sc}w/o Paraphraser\end{sc} denotes the ablation of \textit{Ag-SPR}, and \begin{sc}w/o Reflection\end{sc} denotes the ablation of the self-reflection mechanism.}
\vskip 0.05in
\label{tab:ablation}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{5.2cm}>{\centering\arraybackslash}p{1.5cm}}
\toprule
\multirow{2}{*}{\textbf{Methods}} & VQAv2  \\
\cmidrule(lr){2-2}
& Overall~(\%)\\
\midrule
{\scshape ViKSeR}~(ours)   & \cellcolor{lightgreen}\textbf{88.74} \\
w/o Paraphraser    &  81.68 \\
w/o Reflection   &  77.86 \\
w/o Paraphraser and Reflection  & 74.04\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

On the other hand, to evaluate the capability of the \texttt{F-VKE} module, we conduct a qualitative analysis of the fine-grained visual knowledge extracted from two images. As shown in Figure~\ref{fig:f-vk}, the \texttt{F-VKE} module precisely captures the fine-grained details within the images.
Specifically, in the above image, \texttt{F-VKE} describes the boy’s appearance (e.g., ``blonde hair'' and ``brown shirt'') and his physical actions (e.g., ``standing'' and ``holding''). 
These valuable details contribute to a deeper comprehension of the image, facilitating the interpretation of its content by intelligent agents.
Additionally, \texttt{F-VKE} further uncovers causal relationships embedded within the fine-grained details of the image (e.g., ``he just took a bite of the pastry'' and ``the scene is taking place in colder weather'').
The discovery suggests that \texttt{F-VKE} effectively learns the causal relationship uncovering capabilities of LLMs through the knowledge distillation process. 
Moreover, \texttt{F-VKE} also addresses additional aspects, such as the boy’s expressions (``curious'' and ``focused''), indicating that the boy is likely exploring the taste of the pastry. By linking these details to the action “took a bite of the pastry”, \texttt{F-VKE} constructs a more coherent causal pathway.
Similarly, in the lower image,  \texttt{F-VKE} accurately extracts fine-grained visual knowledge, including the man's attire and physical actions, and further uncovers the underlying causal relationships (e.g., ``he might disembark at the next station’’).
The fine-grained visual knowledge extracted by {\scshape ViKSeR}'s  \texttt{F-VKE} module includes detailed entity information as well as the causal relationships between entity behaviors and inferred outcomes, thereby offering significant assistance in solving related visual reasoning tasks.

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{f-vk.pdf}}
\vskip -0.05in
\caption{The fine-grained visual knowledge extracted by {\scshape ViKSeR} from two images.}
\label{fig:f-vk}
\end{center}
\vskip -0.35in
\end{figure}

\subsection{Case Studies}\label{subsec:case study}
To further validate the effectiveness of {\scshape ViKSeR}, we conduct a qualitative analysis of its performance on two practical visual reasoning cases. 
As shown in Figure~\ref{fig:casestudy}, in case (a), {\scshape ViKSeR} identifies that the motorcyclist has a lit cigarette in his mouth while riding, by generating an image caption that contains fine-grained visual knowledge. This discovery serves as the foundation for solving the case (a).
Subsequently, {\scshape ViKSeR} utilizes the extracted visual knowledge to refine the ambiguous descriptions of the subject in the original question, resulting in a paraphrased question.
Finally, through step-by-step reasoning with evidence, {\scshape ViKSeR} infers the correct answer, ``cigarette''.
This practical visual reasoning case demonstrates {\scshape ViKSeR}'s ability to extract fine-grained visual knowledge and its proficiency in utilizing visual facts as evidence for interpretable reasoning.
Additionally, case (b) further demonstrates {\scshape ViKSeR}'s self-reflection capability. Specifically, due to inevitable reasoning errors, {\scshape ViKSeR} initially inferred an incorrect answer, ``no'', in a previous reasoning process. Fortunately, with the assistance of the self-reflection mechanism, {\scshape ViKSeR} generates a reflective message regarding its last failure, acknowledging that it had overlooked the fact that children may not pay attention to small details such as sugar on his faces while enjoying a treat. Subsequently, utilizing the reflective message, {\scshape ViKSeR} accurately guides a new round of evidence-based reasoning and ultimately infers the correct answer, ``yes''.
This practical case demonstrates {\scshape ViKSeR}'s ability to address reasoning errors through self-reflection and generate more valuable responses.
More typical cases are discussed in Appendix~\ref{apd:case}.

\begin{figure}[h]
\vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{case_study_new.pdf}}
\vskip -0.05in
\caption{The performance of {\scshape ViKSeR} on two practical visual reasoning cases.}
\label{fig:casestudy}
\end{center}
\vskip -0.35in
\end{figure}

\section{Conclusion}\label{sec:conclustion}
In this paper, we introduce {\scshape ViKSeR}, a new framework for visual reasoning tasks, which integrates the extraction of fine-grained and enriched visual knowledge with high interpretability and self-reinforcing reasoning capabilities.
{\scshape ViKSeR} comprises \texttt{F-VKE} and \texttt{S-RR} modules, as well as integrates advanced mechanisms to achieve superior performance.
We conduct extensive experiments on diverse public datasets.
The results demonstrate that {\scshape ViKSeR} outperforms the latest research across public datasets, achieving new SOTA results.
Our future work will focus on: 1) Exploring methods that extract finer-grained visual knowledge while performing more comprehensive visual reasoning based on overall image features. 2) Further integrating the capabilities of world models into {\scshape ViKSeR}.

%\section*{Impact Statement}
%This paper presents an innovative approach for visual reasoning, aimed at improving the interpretability and accuracy of reasoning processes. 
%The research enhances the transparency and reliability of intelligent systems in complex tasks, thereby promoting their application in fields such as healthcare, autonomous driving, and education. 
%By improving the interpretability of the reasoning process, this research strengthens user trust in intelligent systems, which in turn facilitates broader practical applications. 
%Nevertheless, it is important to be aware of the potential risks of misinference in cases of misuse of the technology.
% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00,llavanext2024, llavaimproved2023, qwen2.5-VL, Qwen2VL2024}

\bibliography{main}
\bibliographystyle{mainfile}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Training the  Image Caption Generator}\label{apd:trainGc}
After obtaining the analysis reports $ KA'_p $, the image caption generator $ G_c $ enriches the preliminary image description $ D $ by integrating the visual knowledge from the input image $ I $ and $ KA'_p $ to generate a detailed image caption $ C $ for $ I $.
As discussed in Section~\ref{subsubsec:VKR}, we utilize profound-ground-truth data generated by the LLM to train the $ G_c $.
Specifically, we extract pseudo-ground-truth  image caption $ C_p $ from LLM with a task-specific set of few-shot demonstrations as follows:
\begin{equation}
    KC_p = \left \{ C_p \ \middle| \ C_p \sim P_{LLM}(I, D, A'_p) \right \},
    \label{eq:KDSAnp}
\end{equation}
where $ I $ denotes the input image, $ D $ represents the ground-truth preliminary image description of $ I $, and $ A’_p \in KA’_p  $ represents the pseudo-ground-truth analysis report. $ P_{LLM} $ denotes the LLM operating autoregressively. $ C_p $ denotes the the pseudo-ground-truth image caption sampled from $ P_{LLM} $, and $ KC_p $ represents the set of all $ C_p $.

Unfortunately, $ KC_p $ may contain noise and errors, adversely affecting the training of $ G_c $.
To address this, we similarly apply a post-processing mechanism to filter $ KA_p $ into $ KA'_p $.
Specifically, for each $ C_p $ in $ KC_p $, we use $F$ (the pre-trained MLLM) to assess its validity score $ S_{C_p} $ based on whether $ C_p $ correctly introduces the image. If $ S_{C_p} $ exceeds the predetermined threshold $ \tau $, the corresponding $ C_p $ is retained. The process of collecting $ KC’_p $  is formalized as follows:
\begin{equation}
    KC'_p = \left\{ C_p \ \middle| \ S_{C_p} > \tau \right\}, S_{C_p} = F\left(C_p, (I, D, A'_p)\right),
    \label{eq:SANpScore}
\end{equation}
where $ C_p $ denotes the pseudo-ground-truth image caption. $ I $ denotes the input image, $ D $ represents the ground-truth preliminary image description. $ A’_p $ represents the pseudo-ground-truth analysis report. $ \tau $ denotes the predetermined threshold. $ F $ denotes the pre-trained MLLM.
With $ KC'_p $ serving as pseudo-ground-truth data, we are able to train $ G_c $ with the distillation loss $ L_{G_c} $, as formalized below:
\begin{equation}
    L_{G_c} = - \sum_{t=1}^{T} \log \left( P_{G_c} \left( C_{p,t}' \ \middle| \ C_{p,t-1}', (I, D, A'_p) \right) \right),
    \label{eq:GcKD}
\end{equation}
where $ C’_p \in KC’_p $, $ A’_p \in KA’_p $, and $ T = |C’_p| $. Finally, the trained $ G_c $ and $ G_a $ collaborate to generate a detailed image caption $ C $ for the input image $ I $.

\subsection{Generating Reasoning from PVLMs}\label{apd:prompts}
As discussed in Section~\ref{subsec:SRR}, given the diversity of visual reasoning tasks, we propose a generalized, plug-and-play reasoning prompt paradigm to prompt PVLMs to activate the capacities of the \texttt{S-RR} module. In this section, we provide a detailed discussion of the design methodology for each prompt.

\paragraph{Specification Paraphrase}\label{apd:promptparaphrase}
After acquiring fine-grained visual knowledge from the \texttt{F-VKE} module, the \texttt{S-RR} module prompts a PVLM to paraphrase the question text that exhibits underspecification. The prompt for specification paraphrasing is shown in Figure~\ref{fig:promptparaphrase}.
\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{prompt_paraphrase.pdf}}
\caption{The prompt for paraphrasing the question text with underspecification.}
\label{fig:promptparaphrase}
\end{center}
\vskip -0.2in
\end{figure}

\paragraph{CoE Prompting}\label{apd:coeprompt}
To answer the paraphrased question, the \texttt{S-RR} module employs CoE prompting to guide a PVLM in thinking step by step based on evidence and inferring a valuable response. As illustrated in Figure~\ref{fig:promptcoe}, CoE prompting first instructs the PVLM to leverage the visual information in the image and its caption to address the input question. Subsequently, the PVLM is encouraged to think step-by-step with evidence and generate both the answer and the step-by-step reasoning based on evidence.
This reasoning framework helps the PVLM focus on the relevant visual information in the image and caption, using it as evidence to support interpretable reasoning.
\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{prompt_coe.pdf}}
\caption{The prompt for the CoE prompting technique.}
\label{fig:promptcoe}
\end{center}
\vskip -0.2in
\end{figure}

\paragraph{Self Reflection}\label{apd:reflection}
With the aim of addressing low-quality responses, the \texttt{S-RR} module prompts a PVLM to self-reflect on past failures and seek more valuable responses.
As illustrated in Figure~\ref{fig:promptreflection}, the PVLM is tasked with analyzing the reasoning trajectory of past failures, reflecting on their causes, and formulating a high-level plan to prevent similar failures in the future.
Finally, the PVLM will derive a more valuable answer based on the reflection information and the high-level plan.
\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{prompt_reflection.pdf}}
\caption{The prompt for the self-reflection mechanism.}
\label{fig:promptreflection}
\end{center}
\vskip -0.3in
\end{figure}

\subsection{Case studies}\label{apd:case}
In this section, we demonstrate the capabilities of {\scshape ViKSeR} on more typical visual reasoning cases. 
The baseline models selected for comparison include LlaVa 1.5, Qwen 2.5~\citep{Qwen-VL2023}, Blip2-Flan-T5-XXL~\citep{BLIP22023}, and GPT-4o-mini.
It is worth noting that, due to the comparatively smaller parameter scales of LlaVa 1.5, Qwen 2.5, and Blip2-Flan-T5-XXL, we implement CoT prompting to augment their reasoning abilities. In contrast, for GPT-4o-mini, which possesses a substantially larger parameter size, we directly employ it for reasoning tasks without additional prompting strategies.
Detailed results are shown in Figure~\ref{fig:apdcase1}, \ref{fig:apdcase2}, \ref{fig:apdcase3} and~\ref{fig:apdcase4}.
Specifically, Figure~\ref{fig:apdcase1} and~\ref{fig:apdcase2} compare the performance of {\scshape ViKSeR} and the baselines on two visual reasoning tasks involving short-answer questions.
While all baselines fail to solve both tasks, {\scshape ViKSeR} excels by analyzing the fine-grained visual knowledge it extracts and applying correct reasoning to answer the questions.
Additionally, Figure~\ref{fig:apdcase3} illustrates the performance of {\scshape ViKSeR} and the baseline models in addressing a visual reasoning task involving true/false questions.
All baseline models fail to achieve satisfactory results in this task, especially Blip2-Flan-t5-xxl using CoT prompting, which fails to understand the question requirements and merely describes the image information in the answer.
In contrast, {\scshape ViKSeR} demonstrates exceptional performance, successfully solving the visual reasoning task with remarkable accuracy and proficiency.
Finally, Figure~\ref{fig:apdcase4} presents the performance of {\scshape ViKSeR} and the baseline models in addressing a visual reasoning task involving short-answer questions.
With the exception of GPT-4o-mini, all baseline models fail to achieve satisfactory results in this task. Notably, {\scshape ViKSeR} initially produces an incorrect result during the first round of reasoning. Fortunately, leveraging the self-reflection mechanism, {\scshape ViKSeR} identifies its mistake and re-infers the correct answer.
\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{apdcase1.pdf}}
\caption{Performance of {\scshape ViKSeR} and baselines on a visual reasoning task involving short-answer questions.}
\label{fig:apdcase1}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{apdcase2.pdf}}
\caption{Performance of {\scshape ViKSeR} and baselines on a visual reasoning task involving short-answer questions.}
\label{fig:apdcase2}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{apdcase3.pdf}}
\caption{Performance of {\scshape ViKSeR} and baselines on a visual reasoning task involving a true/false question.}
\label{fig:apdcase3}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{apdcase4.pdf}}
\caption{Performance of {\scshape ViKSeR} and baselines on a visual reasoning task involving short-answer questions.}
\label{fig:apdcase4}
\end{center}
\vskip -0.2in
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
