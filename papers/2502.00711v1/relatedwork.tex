\section{Related Work}
\label{sec:related}
\paragraph{Visual Knowledge Extraction}
Existing VKE methods either employ a holistic image captioning strategy or rely on fixed knowledge formats to extract visual information~\citep{PV2TEA2023,llava12023}.
Specifically, deep learning has played a crucial role in advancing image captioning approaches~\citep{DLimgcap2018,DLimgcap2017}.
With the development of VLMs, an increasing number of studies leverage pre-trained multimodal large language models~(MLLMs) to understand both visual and textual information in a unified framework~\citep{promptcap2023,ISCimgcap2022}. 
On the other hand, the image captioning methods provide a broad understanding of the image content, yet they encounter challenges in conveying fine-grained details.
In contrast, some studies utilize fixed knowledge graphs to map visual features to predefined semantic categories~\citep{ivlr2024,VCHG2023}, providing a structured form of visual knowledge~\citep{covlm2024,BKG2GSG2020}. While these methods ensure consistency and interpretability, they fall short in providing enriched visual knowledge.
\paragraph{Language Model Reasoning}
Recently, large VLMs have demonstrated remarkable success in reasoning and inference, particularly in facilitating few-shot~\citep{cotrainfewshot2022,plmfewshot2021} and zero-shot~\citep{llm0shottrain2024,llm0shot2022} learning. 
Meanwhile, the potential of prompt-based reasoning has been extensively explored to tackle diverse cross-modal visual reasoning tasks, including visual question answering~(VQA)~\citep{repare2024,smola2024}, visual entailment~(VE)~\citep{esnlive2021}, and visual commonsense reasoning~(VCR)~\citep{rapper2024,nlxgpt2022}. 
By leveraging the substantial information embedded within VLMs, novel insights are generated to advance reasoning and interpretability research.