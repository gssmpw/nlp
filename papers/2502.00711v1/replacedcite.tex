\section{Related Work}
\label{sec:related}
\paragraph{Visual Knowledge Extraction}
Existing VKE methods either employ a holistic image captioning strategy or rely on fixed knowledge formats to extract visual information____.
Specifically, deep learning has played a crucial role in advancing image captioning approaches____.
With the development of VLMs, an increasing number of studies leverage pre-trained multimodal large language models~(MLLMs) to understand both visual and textual information in a unified framework____. 
On the other hand, the image captioning methods provide a broad understanding of the image content, yet they encounter challenges in conveying fine-grained details.
In contrast, some studies utilize fixed knowledge graphs to map visual features to predefined semantic categories____, providing a structured form of visual knowledge____. While these methods ensure consistency and interpretability, they fall short in providing enriched visual knowledge.
\paragraph{Language Model Reasoning}
Recently, large VLMs have demonstrated remarkable success in reasoning and inference, particularly in facilitating few-shot____ and zero-shot____ learning. 
Meanwhile, the potential of prompt-based reasoning has been extensively explored to tackle diverse cross-modal visual reasoning tasks, including visual question answering~(VQA)____, visual entailment~(VE)____, and visual commonsense reasoning~(VCR)____. 
By leveraging the substantial information embedded within VLMs, novel insights are generated to advance reasoning and interpretability research.