\section{Related Work}
\label{sec:related_work}
Our work builds upon three key research areas, namely zero-shot classification with LLMs, extreme multi-label classification, and sampling strategies used in language model decoding.

\subsection{Zero-shot Classification with LLMs}
While initially developed for text classification, LLMs have demonstrated remarkable zero-shot classification capabilities across diverse domains, including image classification through vision-language models~\cite{radford2021learning,alayrac2022flamingo}, tabular data classification~\cite{hegselmann2023tabllm}, time series classification~\cite{zhang_large_2024}, and even audio classification~\cite{latif2023sparkslargeaudiomodels}. Early work demonstrated that providing task descriptions and class definitions in natural language could guide models to make reasonable predictions~\cite{kojima2022large,wei2022chain}. This capability was further enhanced through prompt engineering techniques~\cite{Sahoo2024ASS} and the development of instruction-tuned models~\cite{chung2022scaling}. While these approaches have shown promising results, their performance can be sensitive to prompt design and may struggle with nuanced category distinctions. More recent work has focused on improving model reasoning through popular techniques like chain-of-thought reasoning~\cite{wei2023chainofthoughtpromptingelicitsreasoning,kojima2023largelanguagemodelszeroshot} and combining robustness methods such as self-consistency~\cite{wang2023selfconsistencyimproveschainthought}. However, these approaches do not fundamentally address the challenges posed by large-scale classification tasks with long context requirements.

\subsection{Extreme Multi-label Classification}
There are notable connections between our work and extreme multi-label classification (XMC), a field that addresses classification problems involving thousands to millions of classes~\cite{bhatia2015,mittal_multi-modal_2022,zhu2024icxmlincontextlearningframework}. While XMC tasks have traditionally been approached in a supervised manner, researchers have also explored zero-shot XMC methods~\cite{chang2020taming}. Early approaches relied on retrieval-based techniques such as TF-IDF~\cite{salton1988term} and BM25~\cite{robertson2009probabilistic} to match input texts with label embeddings. A major advancement occurred when \citet{reimers2019sentence} demonstrated the effectiveness of dense embeddings for retrieval, which were subsequently applied to various XMC tasks~\cite{you2019attentionxml,chang2020taming,jiang2021lightxml}. More recent work has focused on LLM-based approaches, emphasizing true zero-shot settings where test labels are entirely unseen, unlike generalized zero-shot settings that may include seen labels. For example, ICXML~\cite{zhu2024icxmlincontextlearningframework} adopts a hybrid generation-and-retrieval approach, first generating demonstrations with an LLM and then aligning the outputs with available labels to create a shortlist of candidates. LMTX~\cite{Zhang2024ZeroShotLO} achieved state-of-the-art performance by employing an LLM as a teacher to iteratively train a bi-encoder. However, these approaches still incorporate retrieval-based components and impose an arbitrary label-space cut-off, effectively constraining the LLM’s performance to that of the underlying embedding model. Moreover, we propose a solution that prioritize the role of the LLM as a reasoner for classification, which is applicable to solving classification tasks beyond just text classification. 

\subsection{Sampling Strategies}
Language model decoding is the problem of generating text by selecting tokens from the model’s predicted probability distribution. Early methods like greedy selection and random sampling often led to repetitive or incoherent outputs. Beam search and temperature-based sampling~\cite{temperature2017} improved diversity and quality but failed to address the long tail of irrelevant and low-probability tokens, which can collectively hold significant probability mass~\cite{Holtzman2020The}.
This realization led to the development of more distribution-aware sampling strategies. Top-k sampling~\cite{fan2018hierarchicalneuralstorygeneration} limits choices to the k most probable tokens, while nucleus (Top-p) sampling~\cite{Holtzman2020The} selects tokens whose cumulative probability exceeds p. More recently, Min-p sampling~\cite{nguyen2024turningheatminpsampling} enforces a minimum probability threshold to maintain coherence. Our implementation leverages these distribution-aware thresholds to filter out low-probability classes.