\section{Related Work}
\label{sec:related_work}
Our work builds upon three key research areas, namely zero-shot classification with LLMs, extreme multi-label classification, and sampling strategies used in language model decoding.

\subsection{Zero-shot Classification with LLMs}
While initially developed for text classification, LLMs have demonstrated remarkable zero-shot classification capabilities across diverse domains, including image classification through vision-language models____, tabular data classification____, time series classification____, and even audio classification____. Early work demonstrated that providing task descriptions and class definitions in natural language could guide models to make reasonable predictions____. This capability was further enhanced through prompt engineering techniques____ and the development of instruction-tuned models____. While these approaches have shown promising results, their performance can be sensitive to prompt design and may struggle with nuanced category distinctions. More recent work has focused on improving model reasoning through popular techniques like chain-of-thought reasoning____ and combining robustness methods such as self-consistency____. However, these approaches do not fundamentally address the challenges posed by large-scale classification tasks with long context requirements.

\subsection{Extreme Multi-label Classification}
There are notable connections between our work and extreme multi-label classification (XMC), a field that addresses classification problems involving thousands to millions of classes____. While XMC tasks have traditionally been approached in a supervised manner, researchers have also explored zero-shot XMC methods____. Early approaches relied on retrieval-based techniques such as TF-IDF____ and BM25____ to match input texts with label embeddings. A major advancement occurred when ____ demonstrated the effectiveness of dense embeddings for retrieval, which were subsequently applied to various XMC tasks____. More recent work has focused on LLM-based approaches, emphasizing true zero-shot settings where test labels are entirely unseen, unlike generalized zero-shot settings that may include seen labels. For example, ICXML____ adopts a hybrid generation-and-retrieval approach, first generating demonstrations with an LLM and then aligning the outputs with available labels to create a shortlist of candidates. LMTX____ achieved state-of-the-art performance by employing an LLM as a teacher to iteratively train a bi-encoder. However, these approaches still incorporate retrieval-based components and impose an arbitrary label-space cut-off, effectively constraining the LLM’s performance to that of the underlying embedding model. Moreover, we propose a solution that prioritize the role of the LLM as a reasoner for classification, which is applicable to solving classification tasks beyond just text classification. 

\subsection{Sampling Strategies}
Language model decoding is the problem of generating text by selecting tokens from the model’s predicted probability distribution. Early methods like greedy selection and random sampling often led to repetitive or incoherent outputs. Beam search and temperature-based sampling____ improved diversity and quality but failed to address the long tail of irrelevant and low-probability tokens, which can collectively hold significant probability mass____.
This realization led to the development of more distribution-aware sampling strategies. Top-k sampling____ limits choices to the k most probable tokens, while nucleus (Top-p) sampling____ selects tokens whose cumulative probability exceeds p. More recently, Min-p sampling____ enforces a minimum probability threshold to maintain coherence. Our implementation leverages these distribution-aware thresholds to filter out low-probability classes.