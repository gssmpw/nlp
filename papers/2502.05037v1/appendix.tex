\clearpage
\section{Appendix / supplemental material}
% \input{related}

\subsection{Code}
\label{app:code}
We have released the code in the URL \url{https://github.com/nlokeshiisc/simponet_release}. 

\subsection{Learning Counterfactual Simulators}
\label{app:cf_gen}
Here we discuss prior works that train generative models for synthesizing counterfactuals.  In general, to obtain counterfactuals in the real distribution, we need to follow three steps~\citep{deepSCM}: (a) \textit{abduction}, inverting $X$ to obtain $Z$, (b) \textit{action}, applying a new treatment, and (c) \textit{prediction} generating a new $X$ under the new treatment. These steps require prior knowledge of the DGP specifications, which are often difficult to define and cannot be learned from observational data alone~\citep{deepSCM}. Consequently, many methods bypass the principled approach and use pre-trained models like Diffusion models and Large Language models to generate pseudo counterfactuals from a related synthetic domain. Such simulators are proposed across various modalities, including images~\citep{deepSCM, cf_img_1, cf_img_2, cf_img_3, cf_img_4, cf_img_5}, text~\citep{cf_text_1, cf_text_2, cf_text_3, cf_text_4, cf_text_5}, and healthcare~\citep{cf_health_1, cf_health_2, cf_health_3}. 
% However, the simulated data should be used with caution. 
Prior research~\citep{sim_real_inducbias} shows that while such simulated data is not directly usable for downstream tasks, they provide strong inductive biases that transfer well to the real distribution. Our method can incorporate any such counterfactual generators as simulators, provided they contribute to learning causal representations that are predictive of CATE.
% extractor and ensure that the treatment effects estimated from simulated data closely match the effects in real data.

% \input{related}



\subsection{\our\ Architecture}

\begin{wrapfigure}{r}{0.6\textwidth} % 'r' for right side, 0.5\textwidth for figure width
    \centering
    \vspace{-10pt} 
    \includegraphics[width=0.58\textwidth]{images/butterfly_architecture.pdf} % Path to the figure
    \caption{\small{\our's model architecture.}}
    \vspace{-10pt} 
    \label{fig:our_arch}
\end{wrapfigure}
We present an overview of the \our\ model architecture in Figure \ref{fig:our_arch}. Our model has four primary parameters: $\RInvhat_0$ and $\RInvhat_1$ for extracting causal representations, and $\realmuhat_0$ and $\realmuhat_1$ for predicting outcomes. Shared layers project $\RInvhat_0$ and $\RInvhat_1$ into a common space.


\subsection{\our\ Pseudocode}
Here, we present the \our\ pseudocode. The steps involved in our algorithm are:
\begin{itemize}[leftmargin=1.5cm]
    \item[line 1] First we use the simulator dataset $\synD$ to apply contrastive losses on the counterfactual covariates using Eq. \ref{eq:cont_loss}. This optimization gives us a $Z$ extractor in the simulator distribution, which we denote as $\SInvCont_t$.
    \item[line 2] We partition the training dataset into train, validation dataset using stratified split based on $T$. We then initialize the loss weigts $\lambda_f, \lambda_\tau$ to their defaults.
    \item[lines 4,5] We now decide upon the loss weight $\lambda_f$. To do so, we train \realonly\ and \muonly\ models. We then assess the factual prediction errors of these models on the validation split of the training dataset. If \realonly\ model performs much better than the \muonly\ model, it means that the $\SInvCont_t$ obtained from line2 above is of inferior quality. Therefore, we scale $\lambda_f$ that regularizes the \our's $\RInvhat_t$ based on $\SInvCont_t$ to a very small value, 1e-4.
    \item[line 6] We can now apply gradient descent algorithm on the \our's objective in Eq. \ref{eq:overall_objective} to train the $\realmuhat_t, \RInvhat_t$ parameters of the model.
\end{itemize}

\label{app:pcode}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \small
    \caption{\our\ Algorithm}
    \label{alg:simponet}
    \REQUIRE Observational Data $\trnD$: $\{(\xb_i, t_i, y_i)\}$, Simulator Data $\synD$: $\{(\xb_i^S(0), \xb_i^S(1), y_i^S(0), y_i^S(1))\}$
    \STATE Let $\{\RInvhat(\bullet, t)\} \leftarrow$ $Z$ extraction functions, and $\{\realmuhat(\bullet, t)\} \leftarrow$ outcome functions for $t=0, 1$.
    \STATE Let $\SInvCont_t \leftarrow $ Eq. \ref{eq:cont_loss}  \textcolor{blue}{(Minimize Contrastive loss on $\synD$)}
    \STATE Set $\trnD, D_{\text{val}} \leftarrow$ \textsc{split}($\trnD, pc=0.3$, stratify=$T$), and init default hyperparameters $\Lphi, \Ltau \leftarrow 1, 1$
    \STATE \realonly $\leftarrow \min_{\{\realmuhat, \RInvhat\}} \sum_{\trnD}(y_i - \realmuhat_{t_i}(\RInvhat_{t_i}(\xb_i)))^2$; \muonly $\leftarrow \min_{\realmuhat} \sum_{\trnD}(y_i - \realmuhat_{t_i}(\SInvCont_{t_i}(\xb_i)))^2$
    \STATE Set $\Lphi \leftarrow$ 1e-4 if  $\texttt{FactualErr}(\text{\realonly},  D_{\text{val}}) >> \texttt{FactualErr}(\text{\muonly},  D_{\text{val}})$ 
    % \textcolor{blue}{(downscale $\Lphi$ if \realonly\ offers much better factual estimates)}
    \STATE $\{\RInvhat_t, \realmuhat_t\} \leftarrow$ Eq. \ref{eq:overall_objective} \textcolor{blue}{(perform gradient descent on \our's objective using $\trnD, \synD$ while early stopping using Factual Error on $D_{\text{val}}$)} 
    \STATE \textbf{Return} $\{\RInvhat_t, \realmuhat_t\}$ for $t=0, 1$
\end{algorithmic}
\end{algorithm}




We present the pseudocode for \our\ in Alg. \ref{alg:simponet}. 

\subsection{Theoretical Analysis}
In this section, we present the proofs for our theoretical results.


\label{app:theory_proofs}

\subsubsection{Proof of Lemma~\ref{lemma:ite_decompose}}
\label{app:lemma:ite_decompose}

    \lemmaitedecompose\

\xhdr{Proof}
{
We decompose the CATE error into factual and counterfactual estimation error as follows:
\begin{align*}
    \ErrITE^t &= \int_{\xb \in \xspace} [\ITEx(\xb, t) - \ITExhat(\xb, t)]^2 P(\xb|t)d\xb = \int_{\xb \in \xspace} [\tau(\RInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= \int_{\xb \in \xspace} [(\realmu_1(\RInv_t(\xb)) - \realmu_0(\RInv_t(\xb))) - (\realmuhat_1(\RInvhat_t(\xb)) - \realmuhat_0(\RInvhat_t(\xb)))]^2 P(\xb|t)d\xb\\
    &= \int_{\xb \in \xspace} [(\realmu_1(\RInv_t(\xb)) - \realmuhat_1(\RInvhat_t(\xb))) - (\realmu_0(\RInv_t(\xb)) - \realmuhat_0(\RInvhat_t(\xb)))]^2 P(\xb|t)d\xb\\
\end{align*}

Let $t' = 1 - t$ denote the \textit{counterfactual} treatment. We can then rewrite the above expression as:
\begin{align*}
    \ErrITE^t &= \int_{\xb \in \xspace} [(\realmu_t(\RInv_t(\xb)) - \realmuhat_t(\RInvhat_t(\xb))) - (\realmu_{t'}(\RInv_t(\xb)) - \realmuhat_{t'}(\RInvhat_t(\xb)))]^2 P(\xb|t)d\xb\\
\end{align*}
Now using the inequality $(a - b)^2 \leq 2a^2 + 2b^2$, we can separate the \textit{factual} and \textit{counterfactual} terms:

\begin{align*}
    \ErrITE^t &\leq 2\int_{\xb \in \xspace} [\realmu_t(\RInv_t(\xb)) - \realmuhat_t(\RInvhat_t(\xb))]^2P(\xb|t)d\xb 
    + 2\int_{\xb \in \xspace}[\realmu_{t'}(\RInv_t(\xb)) - \realmuhat_{t'}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= 2\ErrF^t + 2\ErrCF^t
\end{align*}
% proof ends here
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Remove gen bounds for baselines%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse

We make use of the following lemma in the subsequent derivations:
\newcommand{\lemmachangevars}{When performing change of variables $\xb \to z$ in integration, we have $x = \R_t(z)$ and $P(\xb|t)d\xb = P(z|t)dz$. Equivalently, for any function $F(\xb)$, we have 
    \begin{align*}
        \int_{\xb \in \xspace}F(\xb)P(\xb|t)d\xb = \int_{z \in \Zspace} F(\R_t(z))P(z|t)dz
    \end{align*}}
\begin{lemma}
\label{lemma:changevars}
       \lemmachangevars
\end{lemma}

The proof can be found in Section~\ref{app:lemma:changevars}.



\subsubsection{Proof of Lemma~\ref{lemma:cl_rotation}}
\label{app:lemma:cl_rotation}
    \lemmaclrotation\

\xhdr{Proof} 
{Theorem 4.4 of~\citep{von2021self} shows that contrastive training with covariate pairs $\{\xb_i^S(0), \xb_i^S(1)\}$ recovers $Z$ upto a diffeomorphic transformation $h$, i.e. for the simulator DGP our estimate $\hat{z}_i = \SInvTilde(\xb^S_i(t), t) = h(z_i) = h(\SInv(\xb^S_i(t), t), \forall t \in \Tspace$. Moreover for unit-norm latent representations, $\Zspace \subset \mathbb{S}^{d_z - 1}$,~\citep{zimmermann2021cl} show that $h$ is an isometric (norm-preserving) function and therefore, a rotation transform by an extension of Mazur-Ulam Theorem.  Mazur-Ulam Theorem states that any smooth, invertible and isometric function is necessarily affine. Moreover, in our setting, the norm of $z$ as well as $\hat{z}$ is always one and thus, $h$ is necessarily a rotation. Therefore, we recover $\SInvTilde = h \circ \SInv$ upto a rotation of the true inverse map $\SInv$ with sufficient paired samples from the simulator.

Next, we recover $\syntauTilde$ from the following minimisation:

\begin{align*}
    \syntauTilde %&= \argmin_{\syntauhat} \EE_{\xb^S} \left[ \syntauhat(\SInvTilde(\xb^S(t), t)) - (y^S(1) - y^S(0))\right]^2\\
    = \argmin_{\syntauhat} \EE_{\xb^S} \left[ \syntauhat(\SInvTilde(\xb^S(t), t)) - \syntau(\SInv(\xb^S(t), t))\right]^2
    = \argmin_{\syntauhat} \EE_{z} \left[ \syntauhat(h(z)) - \syntau(z)\right]^2
\end{align*}

The above optimization gives $\syntauTilde = \syntau \circ h^{-1}$ and hence we recover the CATE function $\syntau$ for the simulator DGP composed with $h^{-1}$.
% Proof Ends Here
} 


%%%%%%%%
% Bounds for different methods
%%%%%%%%

{
\subsubsection{Proof of Lemma~\ref{lemma:simonlybound}}
\label{app:lemma:simonlybound}

    \lemmasimonlybound\


\xhdr{Proof}
% Bound CATE error for simulator only method
First we construct an upper bound for $\ErrITE^t$ in terms of $\tauhat(\xb, t)$
\begin{align*}
    \ErrITE^t &= \int_{\xb \in \xspace} [\ITEx(\xb, t) - \ITExhat(\xb, t)]^2 P(\xb|t)d\xb = \int_{\xb \in \xspace} [\ITEx(\xb, t) - \ITExSyn(\xb, t) + \ITExSyn(\xb, t) - \ITExhat(\xb, t)]^2 P(\xb|t)d\xb\\
    &\leq 2\int_{\xb \in \xspace} [\ITEx(\xb, t) - \ITExSyn(\xb, t)]^2P(\xb|t)d\xb + 2\int_{\xb \in \xspace}[\ITExSyn(\xb, t) - \ITExhat(\xb, t)]^2 P(\xb|t)d\xb\\
    &= 2\int_{\xb \in \xspace} [\tau(\RInv_t(\xb)) - \syntau(\SInv_t(\xb))]^2P(\xb|t)d\xb + 2\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= 2\int_{\xb \in \xspace} [\tau(\RInv_t(\xb)) - \syntau(\RInv_t(\xb)) + \syntau(\RInv_t(\xb)) - \syntau(\SInv_t(\xb))]^2P(\xb|t)d\xb \\&\quad+ 2\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &\leq 4\int_{\xb \in \xspace} [\tau(\RInv_t(\xb)) - \syntau(\RInv_t(\xb))]^2P(\xb|t)d\xb + 4\int_{\xb \in \xspace}[\syntau(\RInv_t(\xb)) - \syntau(\SInv_t(\xb))]^2P(\xb|t)d\xb \\&\quad+ 2\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
\end{align*}

Above we make use of the inequality $(a+b)^2 \leq 2(a^2 + b^2)$ twice. Next, we observe that $\RInv_t(\xb) = z$ and $P(\xb|t)d\xb = P(z|t)dz$ since $\xb$ is a diffeomorphic transformation of $z$ given $t$. Moreover, let $\syntau$ be Lipschitz continuous with constant $K_{\syntau}$.

\begin{align*}
    \ErrITE^t &\leq 4\int_{z \in \Zspace} [\tau(z) - \syntau(z)]^2P(z|t)dz + 4\int_{\xb \in \xspace}[\syntau(\RInv_t(\xb)) - \syntau(\SInv_t(\xb))]^2P(\xb|t)d\xb \\&\quad+ 2\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb \quad \text{(transforming $\xb$ to $z$)}\\
    &\leq 4\int_{z \in \Zspace} [\tau(z) - \syntau(z)]^2P(z|t)dz + 4 K_{\syntau}^2\int_{\xb \in \xspace}||\RInv_t(\xb)) - \SInv_t(\xb)||^2P(\xb|t)d\xb \\&\quad+ 2\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb \quad \text{(using Lipschitz continuity of $\syntau$)}\\
    &= 4 d_{z}(\tau, \syntau) + 4 K_{\syntau}^2 \, d_{\xb|t}(\RInv_t, \SInv_t) + 2\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
\end{align*}

Here we assume $P(z|t) = P(z)$ for $\tstD$, i.e. treatments distributed independently of $z$. The last term $\int_{\xb \in \xspace}[\syntau(\SInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb$ asymptotically goes to zero with a large number of simulator samples as $\tauhat(\RInvhat_t(\xb)) = \syntauTilde(\SInvTilde_t(\xb))$ converges to $\syntau(\SInv_t(\xb))$. Thus, with sufficiently many counterfactual simulator pairs we have the following:

\begin{align*}
    \ErrITE \leq 4 d_{z}(\tau, \syntau) + 4 K_{\syntau}^2 \, d_{\xb|t}(\RInv_t, \SInv_t)
\end{align*}

% End of Proof
}


\subsubsection{Proof of Lemma~\ref{lemma:changevars}}
\label{app:lemma:changevars}
    When performing change of variables $\xb \to z$ in integration, we have $x = \R_t(z)$ and $P(\xb|t)d\xb = P(z|t)dz$. Equivalently, for any function $F(\xb)$, we have 
    \begin{align*}
        \int_{\xb \in \xspace}F(\xb)P(\xb|t)d\xb = \int_{z \in \Zspace} F(\R_t(z))P(z|t)dz
    \end{align*}  

\xhdr{Proof}
$\xb = \R_t(z)$ is a deterministic function of $z$, and therefore, $P(\xb|z, t) = \delta(\xb - \R_t(z))$ where $\delta$ denotes the dirac-delta function.
\begin{align*}
        \int_{\xb \in \xspace}F(\xb)P(\xb|t)d\xb &= \int_{\xb \in \xspace}\int_{z \in \Zspace}F(\xb)P(\xb|z, t)P(z|t)d\xb\, dz\\
        &=\int_{\xb \in \xspace}\int_{z \in \Zspace}F(\xb)\delta(\xb - \R_t(z))P(z|t)d\xb\, dz\\
        &=\int_{\xb \in \xspace}\int_{z \in \Zspace}F(\R_t(z))\delta(\xb - \R_t(z))P(z|t)d\xb\, dz\\
        &= \int_{z \in \Zspace}F(\R_t(z))P(z|t)dz \int_{\xb \in \xspace}\delta(\xb - \R_t(z)) d\xb\\
        &=\int_{z \in \Zspace} F(\R_t(z))P(z|t)dz
\end{align*}

\subsubsection{Proof of Lemma~\ref{lemma:realonlybound}}
\label{app:lemma:realonlybound}
\lemmarealonlybound\

\xhdr{Proof} 
The \textit{factual} error $\ErrF^t$ goes to zero as $|\trnD| \to \infty$ by direct supervision on the factual outcomes.

Now we try to construct an upper bound on \textit{counterfactual} error $\ErrCF^t$ for the \textit{\realonly} method:

\begin{align*}
    \ErrCF^t &= \int_{\xb \in \xspace}[\realmu_{t'}(\RInv_t(\xb)) - \realmuhat_{t'}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= \int_{\xb \in \xspace}[\realmu_{t'}(\RInv_t(\xb)) -\realmu_{t'}(\RInv_{t'}(\xb)) + \realmu_{t'}(\RInv_{t'}(\xb)) - \realmuhat_{t'}(\RInvhat_{t'}(\xb)) + \realmuhat_{t'}(\RInvhat_{t'}(\xb))  - \realmuhat_{t'}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb
\end{align*}
By adding and subtracting $\realmu_{t'}(\RInv_{t'}(\xb))$ and $\realmuhat_{t'}(\RInvhat_{t'}(\xb))$. We now collect pairs of terms and utilise the inequality $(a+b+c)^2 \leq 3(a^2 + b^2 + c^2)$ to obtain the following:

\begin{align*}
    \ErrCF^t \leq \quad &3\int_{\xb \in \xspace}[\realmu_{t'}(\RInv_t(\xb)) -\realmu_{t'}(\RInv_{t'}(\xb))]^2 P(\xb|t)d\xb
    + 3\int_{\xb \in \xspace}[\realmu_{t'}(\RInv_{t'}(\xb)) - \realmuhat_{t'}(\RInvhat_{t'}(\xb))]^2 P(\xb|t)d\xb\\
    + &3\int_{\xb \in \xspace}[\realmuhat_{t'}(\RInvhat_{t'}(\xb))  - \realmuhat_{t'}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb
\end{align*}

Now, let $\realmu_{t'}, \realmuhat_{t'}$ both be Lipschitz continuous functions with constant $K_{\realmu}$. We can simplify the first and third term as follows:

\begin{align*}
    \ErrCF^t \leq \quad &3K_{\realmu}^2\int_{\xb \in \xspace}||\RInv_t(\xb) -\RInv_{t'}(\xb)||^2 P(\xb|t)d\xb
    + 3\int_{\xb \in \xspace}[\realmu_{t'}(\RInv_{t'}(\xb)) - \realmuhat_{t'}(\RInvhat_{t'}(\xb))]^2 P(\xb|t)d\xb\\
    + &3K_{\realmu}^2\int_{\xb \in \xspace}||\RInvhat_{t'}(\xb)  - \RInvhat_t(\xb)||^2 P(\xb|t)d\xb\\
    = \quad& 3K_{\realmu}^2\int_{\xb \in \xspace}||\RInv_0(\xb) -\RInv_{1}(\xb)||^2 P(\xb|t)d\xb
    + 3\int_{\xb \in \xspace}[\realmu_{t'}(\RInv_{t'}(\xb)) - \realmuhat_{t'}(\RInvhat_{t'}(\xb))]^2 P(\xb|t)d\xb\\
    + &3K_{\realmu}^2\int_{\xb \in \xspace}||\RInvhat_{0}(\xb)  - \RInvhat_1(\xb)||^2 P(\xb|t)d\xb\\
    = \quad &3K_{\realmu}^2 \, d_{\xb|t}(\RInv_0, \RInv_{1}) + 3K_{\realmu}^2 \, d_{\xb|t}(\RInvhat_0, \RInvhat_{1})
    + 3\int_{\xb \in \xspace}[\realmu_{t'}(\RInv_{t'}(\xb)) - \realmuhat_{t'}(\RInvhat_{t'}(\xb))]^2 P(\xb|t)d\xb
\end{align*}

We obtain two terms depending on the gap between functions $(\RInv_0, \RInv_1)$ and $(\RInvhat_0, \RInvhat_1)$ respectively. Since, we always train the composition $\realmuhat_t \circ \RInvhat_t$ jointly on the \textit{factual} objective, we can trivially set $\RInvhat_0 = \RInvhat_1$ driving the second term to zero. The third term asymptotically goes to zero since we assume that $P(\xb|t')$ and $P(\xb|t)$ have the same support $\xspace$. This is because the error $[\realmu_{t'}(\RInv_{t'}(\xb)) - \realmuhat_{t'}(\RInvhat_{t'}(\xb))]^2$ goes to zero on the support of $P(\xb|t')$ due to minimisation of the factual loss term $\ErrF^{t'}$. Asymptotically, we obtain the following bound:

\begin{align*}
    \ErrCF^t &\leq 3K_{\realmu}^2\,d_{\xb|t}(\RInv_0, \RInv_{1})
\end{align*}

% end of proof


\subsubsection{Proof of Lemma~\ref{lemma:muonlybound}}
\label{app:lemma:muonlybound}
\lemmamuonlybound\

\xhdr{Proof}
Before we bound the counterfactual error of \muonly\, let us look at the values of estimated functions $(\realmuhat_t, \RInvhat_t)$ in the limit. $\RInvhat_t = \SInvTilde_t$ by definition. $\realmuhat$ is obtained as follows:
\begin{align*}
    \realmuhat_t &= \argmin_{\realmuhat}\EE[(\realmuhat_t(\SInvTilde_t(\xb)) - \realmu_t(\RInv_t(\xb)))^2]\\
    \realmuhat_t \circ \SInvTilde_t &= \realmu_t \circ \RInv_t \implies \realmuhat_t = \realmu_t \circ \RInv_t \circ (\SInvTilde_t)^{-1} \text{ by invertibility of $\SInvTilde_t$}\\
    \realmuhat_t &= \realmu_t \circ \RInv_t \circ \S_t \circ h^{-1}
\end{align*}

Now we can compute the asymptotic estimate for \textit{counterfactual} outcome as $\realmuhat_{t'}(\RInvhat_{t}(\xb)) = \realmu_{t'} \circ \RInv_{t'} \circ \S_{t'} \circ h^{-1} \circ \SInvTilde_t = \realmu_{t'} \circ \RInv_{t'} \circ \S_{t'} \circ \SInv_t$ (using $\SInvTilde_t = h \circ \SInv_t$). Therefore, \textit{counterfactual} error is given by:
\begin{align*}
    \ErrCF^t &= \int_{\xb \in \xspace}[\realmuhat_{t'}(\RInvhat_{t}(\xb)) - \realmu_{t'}(\RInv_{t}(\xb))]^2 P(\xb|t)d\xb\\
    &= \int_{\xb \in \xspace}[(\realmu_{t'} \circ \RInv_{t'} \circ \S_{t'} \circ \SInv_t)(\xb) - (\realmu_{t'}\circ \RInv_{t})(\xb)]^2 P(\xb|t)d\xb\\
\end{align*}
Using Lipschitz continuity of $\realmu_{t'}$ with constant $K_{\realmu}$ we obtain:
\begin{align*}
    \ErrCF^t &\leq K_{\realmu}^2\int_{\xb \in \xspace}[(\RInv_{t'} \circ \S_{t'} \circ \SInv_t)(\xb) - \RInv_{t}(\xb)]^2 P(\xb|t)d\xb = K_{\realmu}^2\EE_{\xb|t}\left[||(\RInv_{t'} \circ (\SInv_{t'})^{-1} \circ \SInv_t)(\xb) - \RInv_{t}(\xb)||^2\right]\\
\end{align*}
%proof ends
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End Gen Bounds for baselines%%%%%%%%%%%%%%%%%%
\subsubsection{Recovery of $\SInv$ upto a diffeomorphic transformation}
\label{app:sec:mazurulam}
\begin{lemma}
    As $|\synD| \rightarrow \infty$, contrastive training with paired covariates recovers $\SInvTilde_t = h \circ \SInv_t$ while paired outcome supervision recovers $\syntauTilde = \syntau \circ h^{-1}$ where $h$ is a diffeomorphic transformation. 
    % 
    Moreover, when the latent space $\Zspace \subset \mathbb{S}^{(n_z - 1)}$ (unit-norm hypersphere in $\RR^{n_z}$), $h$ is a rotation transform by Extended Mazur-Ulam Theorem as shown in~\citep{zimmermann2021cl} (Proposition 2).
    \label{app:lemma:cl_rotation}
\end{lemma}


\xhdr{Proof} 
{Theorem 4.4 of~\citep{von2021self} shows that contrastive training with covariate pairs $\{\xb_i^S(0), \xb_i^S(1)\}$ recovers $Z$ upto a diffeomorphic transformation $h$, i.e. for the simulator DGP our estimate $\hat{z}_i = \SInvTilde(\xb^S_i(t), t) = h(z_i) = h(\SInv(\xb^S_i(t), t), \forall t \in \Tspace$. Moreover for unit-norm latent representations, $\Zspace \subset \mathbb{S}^{d_z - 1}$,~\citep{zimmermann2021cl} show that $h$ is an isometric (norm-preserving) function and therefore, a rotation transform by an extension of Mazur-Ulam Theorem.  Mazur-Ulam Theorem states that any smooth, invertible and isometric function is necessarily affine. Moreover, in our setting, the norm of $z$ as well as $\hat{z}$ is always one and thus, $h$ is necessarily a rotation. Therefore, we recover $\SInvTilde = h \circ \SInv$ upto a rotation of the true inverse map $\SInv$ with sufficient paired samples from the simulator.

Next, we recover $\syntauTilde$ from the following minimisation:

\begin{align*}
    \syntauTilde %&= \argmin_{\syntauhat} \EE_{\xb^S} \left[ \syntauhat(\SInvTilde(\xb^S(t), t)) - (y^S(1) - y^S(0))\right]^2\\
    = \argmin_{\syntauhat} \EE_{\xb^S} \left[ \syntauhat(\SInvTilde(\xb^S(t), t)) - \syntau(\SInv(\xb^S(t), t))\right]^2
    = \argmin_{\syntauhat} \EE_{z} \left[ \syntauhat(h(z)) - \syntau(z)\right]^2
\end{align*}

The above optimization gives $\syntauTilde = \syntau \circ h^{-1}$ and hence we recover the CATE function $\syntau$ for the simulator DGP composed with $h^{-1}$.
% Proof Ends Here
} 


\xhdr{Proof of Lemma~\ref{lemma:simponetbound}}
\label{app:lemma:simponetbound}

\input{lemmasimponetbound}

\xhdr{Proof} We now construct at upper bound on \textit{counterfactual} error $\ErrCF^t$ that relies on both observational data and simulator estimates to motivate the \our\ objective:

\begin{align*}
    \ErrCF^t &= 
    \int_{\xb \in \xspace}[\realmu_{t'}(\RInv_t(\xb)) - \realmuhat_{t'}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= \int_{\xb \in \xspace}[(\realmu_{t'}(\RInv_t(\xb)) - \realmu_{t}(\RInv_t(\xb)) )- (\realmuhat_{t'}(\RInvhat_t(\xb)) - \realmuhat_{t}(\RInvhat_t(\xb)))
    + \realmu_{t}(\RInv_t(\xb))
    - \realmuhat_{t}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= \int_{\xb \in \xspace}[(2\mathbf{1}_{t=0} - 1)\cdot(\tau(\RInv_t(\xb))- \tauhat(\RInvhat_t(\xb)))
    + \realmu_{t}(\RInv_t(\xb))
    - \realmuhat_{t}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
\end{align*}

Where $\mathbf{1}_{t=0}=1$ when $t=0$ and zero otherwise, and thus, $(2\mathbf{1}_{t=0} - 1) = \pm1$ adjusting the sign of CATE terms. Now we utilise the inequality $(a+b+c)^2 \leq 3(a^2 + b^2 + c^2)$ to obtain:

\begin{align*}
    \ErrCF^t &= \int_{\xb \in \xspace}[(2\mathbf{1}_{t=0} - 1)\cdot(\tau( \RInv_t(\xb)) - \tauhat(h \circ \RInv_t(\xb)) + \tauhat(h \circ \RInv_t(\xb)) - \tauhat(\RInvhat_t(\xb)))
    + \realmu_{t}(\RInv_t(\xb))
    - \realmuhat_{t}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &\leq 3\int_{\xb \in \xspace}[\tau(\RInv_t(\xb)) - \tauhat(h \circ \RInv_t(\xb))]^2 P(\xb|t)d\xb + 3\int_{\xb \in \xspace}[\tauhat(h \circ \RInv_t(\xb)) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb
    \\ &\quad +3\int_{\xb \in \xspace}[\realmu_{t}(\RInv_t(\xb))
    - \realmuhat_{t}(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb\\
    &= 3\int_{z \in \Zspace}[\tau(z) - \tauhat(h(z))]^2 P(z|t)dz + 3\int_{\xb \in \xspace}[\tauhat(h(\RInv_t(\xb))) - \tauhat(\RInvhat_t(\xb))]^2 P(\xb|t)d\xb + 3\ErrF^t
\end{align*}

Here $h$ denotes the unknown rotation transformation that relates the estimated simulator functions $(\SInvTilde, \syntauTilde)$ with the ground-truth simulator functions $(\SInv, \syntau)$ as shown in Lemma~\ref{app:lemma:cl_rotation}. Let $K_{\tau}$ be the Lipschitz constant for $\tauhat$. We can bound the second term in the above expression as follows:

\begin{align*}
    \ErrCF^t &\leq 3\int_{z \in \Zspace}[\tau(z) - \tauhat(h(z))]^2 P(z|t)dz + 3 K_{\tau}^2 \,\int_{\xb \in \xspace}||h(\RInv_t(\xb)) - \RInvhat_t(\xb)||^2 P(\xb|t)d\xb + 3\ErrF^t\\
    &= 3d_z(\tau,\tauhat\circ h) + 3 K_{\tau}^2 \,d_{\xb|t}(h \circ \RInv_t,\RInvhat_t) + 3\ErrF^t
\end{align*}

Now we can add and subtract simulator function estimates to bound the two distance terms as follows:

\begin{align*}
    \ErrCF^t &\leq 3\int_{z \in \Zspace}[\tau(z) - \syntau(z) + \syntau(z) - \tauhat(h(z))]^2 P(z|t)dz 
    \\ &\quad+3 K_{\tau}^2 \,\int_{\xb \in \xspace}||h(\RInv_t(\xb)) - h(\SInv_t(\xb)) + h(\SInv_t(\xb)) - \RInvhat_t(\xb)||^2 P(\xb|t)d\xb + 3\ErrF^t\\
    &\leq 6\int_{z \in \Zspace}[\tau(z) - \syntau(z)]^2P(z|t)dz + 6\int_{z \in \Zspace}[\syntau(z) - \tauhat(h(z))]^2 P(z|t)dz 
    \\ &\quad + 6 K_{\tau}^2 \,\int_{\xb \in \xspace}||h(\RInv_t(\xb)) - h(\SInv_t(\xb)||^2 P(\xb|t)d\xb + 6 K_{\tau}^2 \,\int_{\xb \in \xspace}||h(\SInv_t(\xb)) - \RInvhat_t(\xb)||^2 P(\xb|t)d\xb + 3\ErrF^t\\
    &= 6 d_z(\tau, \syntau) + 6d_z(\tauhat\circ h, \syntau) + 6 K_{\tau}^2 \,d_{\xb|t}(h \circ \RInv_t,h \circ \SInv_t) + 6 K_{\tau}^2 \,d_{\xb|t}(\RInvhat_t,h \circ \SInv_t) + 3\ErrF^t\\
\end{align*}

Now, using Lemma~\ref{app:lemma:cl_rotation}, we can rewrite $\syntau = \syntauTilde \circ h$ in the second term. Thus, $d_z(\tauhat\circ h, \syntau) = d_z(\tauhat\circ h, \syntauTilde \circ h)$. Now making use of \hyperlink{def:dz}{Definition 2}, we can rewrite this as $d_{h(z)}(\tauhat, \syntauTilde)$ which is a distance function defined on the space of rotated latents $h(z)$. We also rewrite $h \circ \SInv$ as $\SInvTilde$ in the fourth term.

Moreover, $d_{\xb|t}(h \circ \RInv_t,h \circ \SInv_t) = d_{\xb|t}(\RInv_t, \SInv_t)$ since $h$ is a rotation transform and preserves the distance between any two vectors. Thus, $||\RInv_t(\xb) - \SInv_t(\xb)||_2 = ||h \circ \RInv_t(\xb) - h \circ \SInv_t(\xb)||_2$. Combining these results, we can evaluate the above bound to the following:

\begin{align*}
      \ErrCF^t  &\leq [6d_{h(z)}(\tauhat, \syntauTilde) + 6 K_{\tau}^2 \,d_{\xb|t}(\RInvhat_t,\SInvTilde_t) + 3\ErrF^t] + \textcolor{blue}{[6 d_z(\tau, \syntau) + 6 K_{\tau}^2 \,d_{\xb|t}(\RInv_t,\SInv_t)]}\\
\end{align*}

% end of proof for simponet

%%%%%%%%%%%%%%%%%%%
% completed error bounds
%%%%%%%%%%%%%%%%%%%


\subsection{Linear DGP Derivation}
\label{app:linear_dgp}
We derive expressions for CATE estimates $\ITExhat(\xb, t)$ as well as $\ErrITE^t$ for each of our proposed estimators in the linear setting below. Note that ground truth CATE $\ITEx(\xb, t) = \xb \mat{R}_t^{-1} (\wbeta{1} - \wbeta{0})$. We consider factual treatment $t=1$ to illustrate the errors.

\subsubsection{\simonly}
For \simonly, we use $\hat{\mat{R}}_t^{-1} = \mat{S}_t^{-1}$ and $\hat{\wbeta{t}} = \wbetas{t}$ which are obtained by training on simulator data. Thus, the CATE estimate $\ITExhat(\xb^*, t) = \xb^*\mat{S}_t^{-1}(\wbetas{1} - \wbetas{0})$. The CATE error on a sample $\xb^*$, with treatment $t=1$ is given by $[\ITExhat(\xb^*, 1) - \ITEx(\xb^*, 1)]^2 = [(\xb^* (\mat{S}_1^{-1}( \wbetas{1} - \wbetas{0}) - \mat{R}_1^{-1} (\wbeta{1} - \wbeta{0}))]^2$

\subsubsection{\realonly}
For \realonly, the factual objective $\ErrF^t = ||\xb \hat{\mat{R}}^{-1}_{t} \hat{\wbeta{t}} - y||_2^2 = ||\xb \hat{\mat{R}}^{-1}_{t} \hat{\wbeta{t}} - \xb \mat{R}_t^{-1} \wbeta{t}||_2^2$. Thus, the closed form solution of the estimator $\hat{\mat{R}}^{-1}_{t} \hat{\wbeta{t}} = \mat{R}_t^{-1} \wbeta{t}, \forall t \in \Tspace$. Since we can't decouple the terms $\hat{\mat{R}}^{-1}_{t}$ and $ \hat{\wbeta{t}}$, the CATE estimate is given by $\ITExhat(\xb^*, t) = \xb^* \hat{\mat{R}}^{-1}_{1} \hat{\wbeta{1}} - \xb^* \hat{\mat{R}}^{-1}_{0} \hat{\wbeta{0}} = \xb^* \mat{R}^{-1}_1 \wbeta{1} - \xb^* \mat{R}^{-1}_0 \wbeta{0}$.\\
CATE error on sample $\xb^*$ with treatment $t=1$ is given by $[\ITExhat(\xb^*, 1) - \ITEx(\xb^*, 1)]^2 = [(\xb^* \mat{R}^{-1}_1 \wbeta{1} - \xb^* \mat{R}^{-1}_0 \wbeta{0}) - \xb \mat{R}^{-1}_1 (\wbeta{1} - \wbeta{0})]^2 = [\xb (\mat{R}^{-1}_1 - \mat{R}^{-1}_0)\wbeta{0}]^2$

\subsubsection{\muonly}
For \muonly, we first set $\hat{\mat{R}}^{-1}_t = \mat{S}^{-1}_t$ which is obtained by training on simulator data. Next, we train $\hat{\wbeta{t}}$ on the factual objective: $||\xb \hat{\mat{R}}^{-1}_t \hat{\wbeta{t}} - \xb \mat{R}_t^{-1} \wbeta{t}||_2^2 = ||\xb \mat{S}^{-1}_t \hat{\wbeta{t}} - \xb \mat{R}_t^{-1} \wbeta{t}||_2^2$. This, gives us a closed form solution for the minimising $\hat{\wbeta{t}} = \mat{S}_t \mat{R}_t^{-1} \wbeta{t}$. The CATE estimate $\ITExhat(\xb^*, t) = \xb^*\mat{S}_t^{-1}(\hat{\wbeta{1}} - \hat{\wbeta{0}}) = \xb^*\mat{S}_t^{-1}(\mat{S}_1 \mat{R}_1^{-1} \wbeta{1} - \mat{S}_0 \mat{R}_0^{-1} \wbeta{0})$. Fixing treatment $t=1$, this simplifies further: $\ITExhat(\xb^*, 1) = \xb^*\mat{S}_1^{-1}(\mat{S}_1 \mat{R}_1^{-1} \wbeta{1} - \mat{S}_0 \mat{R}_0^{-1} \wbeta{0}) = \xb^*(\mat{R}_1^{-1} \wbeta{1} - \mat{S}_1^{-1}\mat{S}_0 \mat{R}_0^{-1} \wbeta{0})$. CATE Error is given by $[\ITExhat(\xb^*, 1) - \ITEx(\xb^*, 1)]^2 = [\xb^*(\mat{R}_1^{-1} \wbeta{1} - \mat{S}_1^{-1}\mat{S}_0 \mat{R}_0^{-1} \wbeta{0}) - \xb^* \mat{R}_1^{-1} (\wbeta{1} - \wbeta{0})]^2 = [ \xb^* \mat{R}_1^{-1} \wbeta{0} -\xb^* \mat{S}_1^{-1}\mat{S}_0 \mat{R}_0^{-1} \wbeta{0}]^2 = [ \xb^* (\mat{R}_1^{-1} - \mat{S}_1^{-1}\mat{S}_0 \mat{R}_0^{-1}) \wbeta{0}]^2$

{\renewcommand{\arraystretch}{1.5}%
% \vspace{-0.2cm}
\begin{table*}[!h]
    \centering
    \setlength\tabcolsep{3.2pt}
    \caption{\small{This table presents the predicted CATE and the corresponding CATE errors obtained from the three CATE proposals computed analytically for a test instance $\xb^\star$ observed under treatment $1$.}}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l|l|l}
        \hline
        \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Estimate for CATE $\widehat{\ITEx}(\xb^\star, 1)$} & \multicolumn{1}{c}{CATE Error $[\widehat{\ITEx}(\xb^\star, 1) - \tau(\xb^\star, 1)]^2$} \\
        \hline \hline
        \simonly & 
        $\xb^\star \mat{S}_1^{-1}w_\tau^S$ &  $\big[\xb^\star\left(\mat{R}_1^{-1} w_\tau - \mat{S}_1^{-1} w_\tau^S \right)\big]^2$ \\
        % 
        \realonly &  $\xb^\star \left(\mat{R}_1^{-1} w_1 - \mat{R}_0^{-1} w_0\right)$ &  $\big[\xb^\star (\mat{R}_0^{-1} - \mat{R}_1^{-1}) w_0\big]^2$ \\
        % 
        \muonly & $\xb^\star \mat{S}_1^{-1} \mat{S}_1\mat{R}_1^{-1}w_{1} - \xb^\star \mat{S}_1^{-1} \mat{S}_0\mat{R}_0^{-1}w_{0}$ & $\big[\xb^\star (\mat{R}_1^{-1} - \mat{S}_1^{-1}\mat{S}_0\mat{R}_0^{-1}) w_0\big]^2$ \\
        \hline
    \end{tabular}}
    \vspace{-0.2cm}
    \label{tab:linear_analysis}
\end{table*}}


\subsubsection{\our}
\label{app:linear:altmin}
We train both $\hat{\mat{R}}^{-1}_t, \hat{\wbeta{t}}$ on the following objective jointly:
\begin{align*}
    \mathcal{L}(\{\hat{\mat{R}}^{-1}_t, \hat{\wbeta{t}}\}_{t=0,1}) = \left[\sum_{t=0,1}||\xb \hat{\mat{R}}^{-1}_t\hat{\wbeta{t}} - \xb\mat{R}^{-1}_t\wbeta{t}||_2^2 + \Lphi\sum_{t=0,1}||\xb \hat{\mat{R}}^{-1}_t - \xb \mat{S}^{-1}_t||_{F}^2 + \Ltau||\mathbf{z}(\hat{\wbeta{1}} - \hat{\wbeta{0}}) - \mathbf{z}(\wbeta{1} - \wbeta{0})||_2^2\right]
\end{align*}
Here, $\mathbf{z} = \xb^S_{t'} \mat{S}^{-1}_{t'}$ are the latents for simulated covariates $\xb^S_{t'}$ (which are identifiable from $\synD$). Due to the joint nature of this optimisation, it is not possible to derive closed form solutions for the optimum. However, one can compuet gradients of the objective with respect to $\hat{\mat{R}}^{-1}_t$ and $ \hat{\wbeta{t}}$ separately. This, gives us an alternating minimisation algorithm with closed form updates.
% 
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \hat{\mat{R}}^{-1}_t} &= \frac{\partial}{\partial \hat{\mat{R}}^{-1}_t}\left[||\xb \hat{\mat{R}}^{-1}_t\hat{\wbeta{t}} - y||_2^2 + \Lphi||\xb \hat{\mat{R}}^{-1}_t - \xb \mat{S}^{-1}_t||_{F}^2\right]\\
    &=  2\xb^T\xb \hat{\mat{R}}^{-1}_t (\hat{\wbeta{t}}\hat{\wbeta{t}}^T + \Lphi\mat{I}) - 2\xb^T y\hat{\wbeta{t}} + -2\Lphi \xb^T\xb\mat{S}^{-1}_t 
\end{align*}
Setting the derivative to zero, we obtain the following update rule:
\begin{align*}
    \hat{\mat{R}}^{-1}_t \leftarrow (\xb^{\dag}y\hat{\wbeta{t}} + \Lphi \mat{S}^{-1}_t) \cdot (\hat{\wbeta{t}}\hat{\wbeta{t}}^T + \Lphi\mat{I})^{-1}
\end{align*}
where $\xb^{\dag} = (\xb^T\xb)^{-1}\xb^T$ is the pseudoinverse of $\xb$.
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \hat{\wbeta{t}}} &= \frac{\partial}{\partial \hat{\wbeta{t}}}
    \left[||\xb \hat{\mat{R}}^{-1}_t\hat{\wbeta{t}} - y||_2^2 + \Ltau||\mathbf{z}(\hat{\wbeta{t}} - \hat{\wbeta{t'}}) - (y_1^S - y_0^S)||_2^2\right]\\
    &= 2(\hat{z}^T\hat{z})\hat{\wbeta{t}}- 2\hat{z}^Ty + 2\Ltau(z^Tz\hat{\wbeta{t}} - z^T(z\hat{\wbeta{t'}} + (y_1^S - y_0^S)))\\
    &= 2[(\hat{z}^T\hat{z}) + \Ltau(z^Tz)]\hat{\wbeta{t}} - 2(\hat{z}^Ty + \Ltau z^T(z\hat{\wbeta{t'}} + (y_1^S - y_0^S))) 
\end{align*}
Where $\hat{z} = \xb \hat{\mat{R}}^{-1}_t$. Setting the derivative to zero, we obtain the following update rule:
\begin{align*}
    \hat{\wbeta{t}} \leftarrow ((\hat{z}^T\hat{z}) + \Ltau(z^Tz))^{-1}\cdot (\hat{z}^Ty + \Ltau z^T(z\hat{\wbeta{t'}} + (y_1^S - y_0^S)))
\end{align*}
For \our, we perform alternating updates of $\hat{\wbeta{t}}$ and $\hat{\mat{R}}^{-1}_t$ fixing the other estimate. 



\subsection{Summary of Datasets}
\label{app:datasets}

\xhdr{IHDP}
The Infant Health and Development Program (IHDP) is a randomized controlled trial designed to assess the impact of physician home visits on the cognitive test performance of premature infants. The dataset exhibits selection bias due to the deliberate removal of non-random subsets of treated individuals from the training data. Since outcomes are observed for only one treatment, we generate both observed and counterfactual outcomes using a synthetic outcome generation function based on the original covariates for both treatments, making the dataset suitable for causal inference.

The IHDP dataset includes 747 subjects and 25 variables. While the original dataset discussed in~\citep{cfrnet} had 1000 versions, our work uses a smaller version with 100 iterations, aligning with the CATENets benchmark. Each version varies in the complexity of the assumed outcome generation function, treatment effect heterogeneity, etc. As outlined in~\citep{benchmarking}, reporting the standard deviation of performance across the 100 different seeds is inappropriate. Therefore, we calculate $p$-values through paired t-tests between our method (\our) and other baseline methods, using \our\ as the baseline for all experiments. We follow setting D of the IHDP dataset as mentioned in~\citep{inducbias} where response surfaces are modified to suppress the extremely high variance of potential outcomes in certain versions of the IHDP dataset.

\xhdr{ACIC}
The Atlantic Causal Inference Conference (ACIC) competition dataset (2016)\footnote{\url{https://jenniferhill7.wixsite.com/acic-2016/competition}} consists of 77 datasets, all containing the same 58 covariates derived from the Collaborative Perinatal Project. Each dataset simulates binary treatment assignments and continuous outcome variables, with variations in the complexity of the treatment assignment mechanism, treatment effect heterogeneity, the ratio of treated to control observations, overlap between treatment and control groups, dimensionality of the confounder space, and the magnitude of the treatment effect.

All datasets share common characteristics, such as independent and identically distributed observations conditional on covariates, adherence to the ignorability assumption (selection on observables with all confounders measured and no hidden bias), and the presence of non-true confounding covariates. Of the 77 datasets, we selected a subset of three: versions 2, 7, and 26, aligning with the CATENets benchmark. These versions present non-linear covariate-to-outcome relationships and maximum variability in treatment effect heterogeneity. Version 2, notably, exhibits no heterogeneity, meaning the treatment effect is constant across all individuals. However, accurately estimating outcome differences even for this version is challenging due to the inherent noise in potential outcome realizations in the dataset.


\subsection{Experiments with Limited Training Data}

\begin{table*}
    \caption{\label{tab:trn_size}\small{Effect of varying training sizes on CATE using the IHDP dataset. We experiment with training proportions of 10\%, 25\%, 50\%, and 75\% of the full training set. Results demonstrate how the performance of \our\ and baseline methods evolves with varying amounts of training data.}} \label{tab:trn_size}
    \centering
    \resizebox{13.5cm}{!}{
        \begin{tabular}{l|r|r|r|r}
        \toprule
        Training Percentage & 0.10 & 0.25 & 0.50 & 0.75 \\ \hline \hline
        RNet & 3.08 (0.08) & 2.52 (0.07) & 2.30 (0.12) & 2.30 (0.14) \\  
        XNet & 2.43 (0.09) & 1.90 (0.12) & 1.16 (0.19) & 1.10 (0.35) \\
        DRNet & 3.28 (0.04) & 2.01 (0.03) & 1.05 (0.40) & 1.05 (0.44) \\
        CFRNet & \second{1.41 (0.03)} & \second{1.06 (0.21)} & \second{0.95 (0.59)} & 1.02 (0.50) \\
        FlexTENet & 2.97 (0.08) & 2.33 (0.07) & 1.04 (0.40) & 1.03 (0.49) \\
        DragonNet & 3.48 (0.04) & 2.02 (0.04) & 0.97 (0.53) & 1.02 (0.50) \\
        IPW & 3.44 (0.03) & 1.74 (0.06) & \first{0.94 (0.60)} & 1.03 (0.49) \\
        NearNeighbor & 2.10 (0.19) & 1.65 (0.05) & 2.65 (0.11) & 1.07 (0.47) \\
        PerfectMatch & 3.44 (0.03) & 2.54 (0.03) & 3.32 (0.01) & 1.05 (0.45) \\
        PairNet & 1.64 (0.13) & 1.18 (0.11) & 1.09 (0.30) & 1.09 (0.36) \\
        \hline
        \simonly & 1.45 (0.23) & 1.45 (0.07) & 1.45 (0.15) & 1.45 (0.04) \\
        \realonly & 2.28 (0.13) & 2.92 (0.07) & 0.97 (0.54) & \first{0.86 (0.75)} \\
        \muonly & 3.35 (0.03) & 2.01 (0.04) & 1.07 (0.31) & 1.02 (0.25) \\
        \our & \first{1.01 (0.00)} & \first{0.93 (0.00)} & \second{0.95 (0.00)} & \second{0.87 (0.00)} \\ \hline
    \bottomrule
    \end{tabular}
    }
\end{table*}
In this experiment, we evaluate the performance of CATE methods with varying training sizes. We use 10 randomly selected dataset versions of the IHDP dataset. For each version, the methods are trained using 10\%, 25\%, 50\%, and 75\% of the training data. Importantly, the simulator dataset is not subsampled, so the \simonly\ model maintains the same CATE error across all training sizes. The results are presented in Table~\ref{tab:trn_size}.
At extremely low training sizes (10\%), \our\ achieves the lowest CATE error with a significant margin over all baselines. With 25\% of the training data, the baseline methods improve, but \our\ continues to deliver the best CATE error. At 50\% training size, the baselines further improve and perform comparably to \our. While \our's CATE error increases slightly in this setting, it remains close, trailing the best-performing baseline by only $0.01$. Finally, at 75\% training size, \our's CATE error decreases significantly, while many baselines plateau in performance. Overall, \our\ demonstrates exceptional robustness in estimating CATE, particularly in limited training data scenarios.


\subsection{Table of Symbols}
\label{app:symbols}
\begin{table}[H]
    \centering
    \resizebox{0.9\textwidth}{!}{       
    \begin{tabular}{r|l}
         Symbol & Definition \\
         \hline
         $X$ & Real post-treatment covariates: Random Variable \\
         $Y$ & Real outcomes: Random Variable\\
        $X^S$ & Simulator post-treatment covariates: Random Variable \\
         $Y^S$ & Simulator outcomes: Random Variable\\
         $T$ & Treatment: Random Variable\\
         $Z$ & Latent (unobserved) pre-treatment representations: Random Variable\\
         $\trnD$ & Observational training dataset from Real DGP\\
         $\synD$ & Counterfactual dataset from Simulator DGP\\
         $\tstD$ & Test dataset from Real DGP\\
         $\xb, \xb^S, z, t, y, y^S$ & Realisations of random variables $X, X^S, Z, T, Y, Y^S$ respectively\\
         $\xspace$ & Space of post-treatment covariate values: Set\\
         $\Tspace$ & Space of treatment values: Set $= \{0, 1\}$\\
         $\Zspace$ & Space of latents: Set\\
         $\yspace$ & Space of outcomes: Set\\
         $n_z, n_x$ & Dimensions of vector spaces in which $\Zspace, \xspace$ lie\\
         $Y_i(t)$ & Potential outcome for $i^{\textrm{th}}$ unit under treatment $t$\\
         $X_i(t)$ & Potential post-treatment covariate for $i^{\textrm{th}}$ unit under treatment $t$\\
         \hline
         $\R_t$ & Mapping from $\Zspace\mapsto\xspace$, transforms latents to real post-treatment covariates under $t$\\
         $\S_t$& Mapping from $\Zspace\mapsto\xspace$, transforms latents to simulated post-treatment covariates under $t$\\
         $\RInv_t$ & Mapping from $\xspace\mapsto\Zspace$, transforms real post-treatment covariates under $t$ to latents\\ 
         $\SInv_t$& Mapping $\xspace\mapsto\Zspace$, transforms simulated post-treatment covariates under $t$ to latents\\
         $P_Z$ & Probability distribution of latents $Z$\\
         $\realmu_t$ & Outcome function for real data under $t$\\
        $\synmu_t$ & Outcome function for simulated data under $t$\\
        $\tau$ & Conditional Average Treatment Effect for real data, $\realmu_1 - \realmu_0$, Mapping $\Zspace \mapsto \yspace$\\
        $\syntau$ & Conditional Average Treatment Effect for simulated data, $\synmu_1 - \synmu_0$, Mapping $\Zspace \mapsto \yspace$\\
        $\circ$ & Composition of functions\\
        $\ITEx(\xb, t)$ & Conditional Average Treatment Effect for real data, $\tau \circ \RInv_t(\xb)$, Mapping $\xspace \times \Tspace\mapsto\yspace$\\
        $\ITExSyn(\xb^S, t)$ & Conditional Average Treatment Effect for simulated data, $\syntau \circ 
        \SInv_t(\xb^S)$, Mapping $\xspace \times \Tspace\mapsto\yspace$\\
        $h$ & Diffeomorphic transformation, arises due to contrastive learning \\%, see Lemma~\ref{lemma:cl_rotation}\\
        $\mathbb{S}^{d}$ & Unit-norm hypersphere of dimension $d$, Subset of $\RR^{(d+1)}$\\
        $d_{\xb|t}$ & Expected squared-distance between two functions on $P(X|T)$, see Section~\ref{sec:problem_formulation} for definition\\
        $d_{z}$ & Expected squared-distance between two functions on $P_Z$, see Section~\ref{sec:problem_formulation} for definition\\
        $d_{h(z)}$ & $d_z$ under transformation $h$ on $z$, see Section~\ref{sec:problem_formulation} for definition\\
        $\textrm{sim}(\bullet, \bullet)$ & Cosine similarity\\
        \hline
        $\RInvhat_t$ & Estimate for $\RInv_t$ \\
        $\SInvhat_t$ & Estimate for $\SInv_t$\\
        $\realmuhat_t$ & Estimate for $\realmu_t$\\
        $\synmuhat_t$ & Estimate for $\synmu_t$\\
        $\SInvTilde_t$ & Estimate for $\SInv_t$ recovered from contrastive learning \\%, see Lemma~\ref{lemma:cl_rotation}\\
        $\synmuTilde_t$ & Estimate for $\synmu_t$ on recovering Simulator DGP\\ %, see Lemma~\ref{lemma:cl_rotation}\\
        \hline
        $\ErrITE$ & CATE estimation error\\
        $\ErrITE^t$ & CATE estimation error on covariates $\xb$ under treatment $t$\\
        $\ErrF^t$ & Factual error on treatment $t$ samples\\
        $\ErrCF^t$ & Counterfactual error on treatment $t$ samples\\
        \hline
        $K_{\realmu}$ & Lipschitz constant for $\realmu_t, \realmuhat_t$\\
        $K_{\tau}$ & Lipschitz constant for $\tau, \tauhat$\\
        $K_{\synmu}$ & Lipschitz constant for $\synmu_t, \synmuhat_t, \synmuTilde_t$\\
        $K_{\syntau}$ & Lipschitz constant for $\syntau, \syntauhat, \syntauTilde$\\
         \hline   
       $\mat{R}_t$ & $\R_t$ for linear DGP: Matrix\\
       $\mat{S}_t$ & $\S_t$ for linear DGP: Matrix\\
       $\wbeta{t}$ & $\realmu_t$ for linear DGP: Vector\\
       $\wbetas{t}$ & $\synmu_t$ for linear DGP: Vector\\
       $\realwb_\tau$ & $\tau$ for linear DGP: Vector\\
       $\realwb_\tau^S$ & $\syntau$ for linear DGP: Vector\\ 
         \hline
    \end{tabular}}
    % \caption{Table of Symbols}
    \label{tab:symbols}
\end{table}
