\section{Experiments}
\label{sec:expts}
We conduct experiments that are designed to address the following research questions:
\begin{itemize}[leftmargin=0.8cm, itemsep=0pt]
    \item[RQ1] How do different methods compare with varying discrepancies between real and simulator in settings with closed form estimates; i.e., without errors due to finite-sample training? 
    \item[RQ2] How does \our\ compare to other SOTA baselines that assume pre-treatment covariates?
    \item[RQ3] What are the contributions of individual loss terms in \our?
    \item[RQ4] How does \our\ fare against the baselines when $\realmu_t$ exhibits complex non-linear behavior?
    \item[RQ5] How do CATE methods perform when trained directly on (a) post-treatment covariates $X$, or (b) pre-treatment $Z$, or (c) simulated causal representations $\SInvCont$ from Eq.~\ref{eq:cont_loss}?
    % Does the $z$ extracted by \our\ aid other proposals that impose just the Factual loss on $\trnD$?
\end{itemize}
% We also aim to answer the above questions using such datasets.


\subsection{Neural Architecture and Hyperparameters} \label{app:model_arch}
For the Linear experiments in RQ1, we omit shared layers in Fig.~\ref{fig:our_arch}, 
and set $\RInvhat_0$ and $\RInvhat_1$ as ${n_x} \times {n_x}$ matrices, and $\realmuhat_0$ and $\realmuhat_1$ as ${n_x} \times 1$ vectors. 
% 
% 
For the real-world experiments in RQ2, with both $\RInv_t, \realmu_t$ as non-linear functions, we set $\realmuhat_0$ and $\realmuhat_1$ as 2-layer MLPs with hidden layers of 100 and 50 neurons.
% , using the default ELU activations. 
The shared layers have 2 hidden layers with 50 and ${n_x}$ neurons, respectively. We impose the $d(\RInvhat_t, \SInvCont_t)$ loss for \our\ on outputs of the shared layers. 
For experiments in RQ4, we omit the shared layers while setting $\RInvhat_t$ as linear layer. But since $\realmu$ is non-linear, we use an MLP with one hidden layer of 50 neurons and ReLU activations for each $\realmuhat_t$.
% The latter half of the model follows the default architecture in the CATENets library. 



\textbf{Hyperparameters:}
% 
We implemented all baselines and \our\ using \texttt{jax} within CATENets~\citep{benchmarking}, a standard library for benchmarking state-of-the-art CATE estimation methods. To ensure consistency, we used the same MLP architecture, learning rates, optimizers, and other hyperparameters as the default settings in CATENets for baseline approaches. The unique hyperparameters for \our\ are the loss weights $\Lphi$ and $\Ltau$. As described in Sec. \ref{sec:simponet}, \our\ tunes $\Lphi$ by comparing the factual errors of the \realonly\ and \muonly\ models, while $\Ltau$ is always set to 1. CATENets applies early stopping based on factual error in the validation dataset, a common practice in CATE training. To ensure a fair comparison, we maintained consistent training and validation splits across all methods.

\textbf{Assessing Statistical Significance:} For CATE experiments, standard deviation is sometimes misleading to comment on the statistical significance of empirical results as noted in~\citep{benchmarking}. So, for all experiments, we conduct a one-sided paired $t$-test with \our\ as the baseline and enclose \textit{p-values in brackets} to indicate statistical significance. Lower $p$-values favor \our.

\subsection{RQ1: Assessing Baselines Under Settings Without Finite Training Sample Errors}
\label{sec:rq1}

To address RQ1, we consider a setting where both the real and simulator DGPs as shown in Fig.~\ref{fig:our_DGP} are linear. 
% We derive the estimates under the population setting to avoid noisy insights that could otherwise arise from finite sample training.  For RQ1, 
In particular, we generate the training datasets $\trnD$ and $\synD$ as follows: (1) Latent variables $z \in \RR^{n_z}$ are sampled from distribution $P_Z$. (2) Real and simulated covariates for a treatment $t$ are computed as $\R_t(z) = z \mat{R}_t$ and $\S_t(z) = z \mat{S}_t$, where $\mat{R}_t$ and $\mat{S}_t$ are invertible matrices. (3) Outcomes are generated as $\realmu_t(z) = z^\top \wbeta{t}$ and $\synmu_t(z) = z^\top \wbetas{t}$, where $\wbeta{t}$ and $\wbetas{t}$ are vectors in $\RR^{n_z}$. We consider two datasets for RQ1: (1) Synthetic-Gaussian and (2) Real World-IHDP, differing in how $Z$ is obtained. In setting (1), $z \in \RR^{10}$ is sampled from a standard Gaussian $\mathcal{N}(0, 1)$, while for (2), $Z$ is taken from the real-world IHDP dataset (Appendix \ref{app:datasets}) as-is. 

Now, to systematically control the real-simulator mismatch, we need means to vary the following distances: $d(\mat{R}_0^{-1}, \mat{R}_1^{-1})$, $d(\mat{R}_t^{-1}, \mat{S}_t^{-1})$, and $d(\tau, \tau^S)$. We achieve this as follows:
% 
(1) Initialize $\mat{R}_0^{-1}, \realwb_{0} \sim \mathcal{N}(0, 1)$.
(2) To inject a distance $\gamma_{R} \in (0, 0.5)$ between $\mat{R}_0^{-1}$ and $\mat{R}_1^{-1}$, set $\mat{R}_1^{-1} = (1 - \gamma_{R}) \mat{R}_0^{-1} + \gamma_R \mathcal{N}(0, 1)$.
(3) Set $w_1 \sim \gamma w_0 + (1 - \gamma) \mathcal{N}(0, 1)$. We use $\gamma=0.4$ in all experiments.
(4) Similarly, inject a $\gamma_{RS}$ gap between $\mat{R}_t^{-1}$ and $\mat{S}_t^{-1}$.
(5) For treatment effect parameters $\realwb_\tau = \realwb_1 - \realwb_0$ in the real DGP, we sample its simulator counterpart with a gap $\gamma_\tau$ as $\realwb_\tau^S = (1 - \gamma_\tau) \realwb_\tau + \gamma_\tau \mathcal{N}(0, 1)$ and set $\wbetas{t}$ accordingly. 


% We derive closed-form CATE error for each model under population settings, and the corresponding CATE errors are summarized in Table~\ref{tab:linear_analysis}, with detailed derivations in Appendix \ref{app:linear_dgp}. 
% When all functions in both the real and simulator DGPs, as illustrated in Figure~\ref{fig:our_DGP}, are linear, 
In  linear settings, the optimization problems for the CATE estimators \simonly, \realonly, and \muonly\ admit closed-form solutions. We show the closed-form solutions in Table~\ref{tab:linear_analysis} in the Appendix, and a detailed derivation in Appendix~\ref{app:linear_dgp}.






For \our, a closed-form solution is not possible, so we solve it to a local optimum using alternating minimization over $\realmuhat_t$ and $\RInvhat_t$, with each alternating update in closed-form. We show the \our's update equations in Appendix~\ref{app:linear:altmin}. In summary, the setting of RQ1 allows study of the impact of varying discrepancies between the real and simulator distributions without approximation errors due to finite training samples.


{\renewcommand{\arraystretch}{1.2}%
\begin{table*}
    \centering
    \setlength\tabcolsep{3.2pt}
    \caption{
    \small{
    RQ1: In a linear DGP setting, we vary the gaps using $\gamma_R$ for $d(\RInv_0, \RInv_1)$ in the first column, $\gamma_{RS}$ for $d(\RInv_t, \SInv_t)$, and $\gamma_\tau$ for $d(\tau, \tau^S)$. ``low'' refers to 0.1 that simulates small distance, while ``high''  refers to 0.4. We run all experiments with five different seeds and report $p$-values of comparing the mean performance in bracket.}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|l|l||r|r|r|r||r|r|r|r}
    \toprule
    ~ & ~ & ~ & \multicolumn{4}{c||}{Synthetic-Gaussian} &   \multicolumn{4}{c}{Real World-IHDP} \\
    \hline
    $d(\RInv_0, \RInv_1)$ & $d(\RInv_t, \SInv_t)$ & $d(\tau, \tau^S)$ &   \simonly & \realonly &  \muonly &   \our &  \simonly & \realonly &  \muonly &  \our \\
    \hline \hline
    0.00 &              high &              high &          2.82 (0.27) &  \first{0.00 (1.00)} &        15.75 (0.01) & \second{2.58 (0.00)} &          3.57 (0.11) &   \first{0.00 (1.00)} & 48.76 (0.05) & \second{3.20 (0.00)} \\
      low &               low &               low &  \second{0.63 (0.00)} &         2.47 (0.02) &         1.19 (0.01) &  \first{0.54 (0.00)} & \second{1.00 (0.44)} &          3.43 (0.02) &   2.73 (0.00) &  \first{0.97 (0.00)} \\
      low &               low &              high &          1.57 (0.16) &         2.47 (0.08) & \first{1.19 (0.83)} & \second{1.39 (0.00)} & \second{1.62 (0.26)} &          3.43 (0.04) &  2.73 (0.02) &  \first{1.49 (0.00)} \\
      low &              high &               low & \second{2.14 (0.22)} &          2.47 (0.00) &        15.75 (0.01) &  \first{1.85 (0.00)} &          3.67 (0.31) & \second{3.43 (0.48)} & 48.76 (0.05) &  \first{3.37 (0.00)} \\
      low &              high &              high &          2.82 (0.26) & \first{2.47 (0.56)} &        15.75 (0.01) & \second{2.57 (0.00)} &          3.57 (0.11) & \second{3.43 (0.39)} & 48.76 (0.05) &  \first{3.19 (0.00)} \\
     high &               low &               low &  \second{0.63 (0.00)} &        13.86 (0.02) &         1.19 (0.01) &  \first{0.54 (0.00)} & \second{1.00 (0.47)} &         47.78 (0.06) &   2.73 (0.00) &  \first{0.98 (0.00)} \\
     high &               low &              high &          1.57 (0.16) &        13.86 (0.03) & \first{1.19 (0.83)} & \second{1.39 (0.00)} & \second{1.62 (0.27)} &         47.78 (0.06) &  2.73 (0.02) &  \first{1.50 (0.00)} \\
     high &              high &               low & \second{2.14 (0.21)} &        13.86 (0.03) &        15.75 (0.01) &  \first{1.85 (0.00)} & \second{3.67 (0.31)} &         47.78 (0.06) & 48.76 (0.05) &  \first{3.38 (0.00)} \\
     high &              high &              high & \second{2.82 (0.26)} &        13.86 (0.04) &        15.75 (0.01) &  \first{2.57 (0.00)} & \second{3.57 (0.11)} &         47.78 (0.06) & 48.76 (0.05) &  \first{3.19 (0.00)} \\
\bottomrule
\end{tabular}}
\label{tab:main_linear}
\vspace{-0.2cm}
\end{table*}}



% We compare \our\ with the three baselines empirically by varying distances in the DGPs.
We show the results comparing \our\ with the three baselines in Table \ref{tab:main_linear}\footnote{Each entry in the table reports the CATE Error alongside its corresponding $p$-value as CATE Error ($p$-value). The $p$-value denotes the statistical significance of the hypothesis that \our\ outperforms the baseline. We use a one-sided paired t-test for this assessment. Smaller $p$-values indicate stronger evidence in favor of \our. We show the \first{best performing method} in green, and the \second{second best method} in yellow across all our tables.}
where we observe: 
achieves either the best or second-best performance. The CATE error for \our\ remains controlled primarily due to its ability to bound errors in the counterfactual distribution.
% 
(b) In contrast, the \realonly\ and \muonly\ models perform well only under certain restrictive conditions favorable to them, providing zero error on the factual distribution but very high counterfactual error, leading to poor CATE estimates.
% 


\subsection{RQ2: Comparing \our\ with State-of-the-art CATE Baselines}
\label{sec:rq3}
\vspace{-0.2cm}
We conduct experiments using semi-synthetic observational datasets commonly used to assess efficacy of treatment effect estimation methods in the literature: the Infant Health Development Program (IHDP) and the Atlantic Causal Inference Conference (ACIC) datasets. These datasets contain real-world {\em pre-treatment} covariates ($Z$). Please refer Appendix \ref{app:datasets} for more details on these datasets.

To align these datasets with our study, we apply RealNVP Normalizing Flows~\citep{realnvp} to transform pre-treatment covariates $Z$ into post-treatment $X$. Flows are non-linear deep neural networks that ensure invertibility of the covariate generating functions $\R_t, \S_t$. We consider randomly initialized flows with two coupling layers. We used the flows $\R_0, \R_1$ on real data, and two other distinct flows $\S_0, \S_1$ to obtain covariates in synthetic data from $Z$.
% \todo{how do we control distance between $\R_t$ and $\S_t$ in normalising flows? L: We do not. We tried init from a different Gaussian. But that did not reflevt in results.}. 
We borrow the real outcomes as-is from the ground truth dataset. However, we synthesize simulator outcomes with a gap of $\gamma_\tau$ as follows: (1) sample $w_\tau^S \in \RR^{n_z} \sim \mathcal{N}(0, 1)$ (2) set $\tau^S(z) = \tau(z) + (\sigma(\tau) \cdot \gamma_\tau \cdot z^\top w_\tau^S)$, where $\sigma(\tau)$ is the standard deviation of CATE labels in the real dataset. Scaling by $\sigma(\tau)$ ensured comparability between $\tau$ and $\tau^S$. Thus, when $\gamma_\tau = 0$, $\tau = \tau^S$; when $\gamma_\tau = 1$, $\tau$ is disparate from $\tau^S$. 
% All models contain $\RInvhat_t$ as 3 layer MLPs and $\realmuhat_t$ as 2 layer MLPs for $t=0, 1$.


\begin{wrapfigure}{r}{0.35\textwidth} % 'r' for right side, 0.5\textwidth for figure width
    \centering
    \vspace{-15pt} 
    \includegraphics[width=0.33\textwidth]{images/tune_lambdaf.pdf} % Path to the figure
    \caption{\small{Factual errors with $p$-values shown above bars. For IHDP, \realonly\ consistently outperforms \muonly.}}
    \vspace{-15pt} 
    \label{fig:valid_fct}
\end{wrapfigure}

We evaluated \our\ against various baselines from the well-known CATENets~\citep{benchmarking}, a benchmarking library for CATE estimation. Since the baseline methods are not designed to extract the causal representations, we provided them with representations extracted by simulated causal representation extractor $\SInvCont_t(\xb)$ as input for a fair comparison. Running these baselines with post-treatment covariates $X$ directly as input yielded much poorer results as shown in Fig.~\ref{fig:mlp_expts}. We also compared \our\ with \simonly, \realonly, and \muonly\ baselines that we developed in our theoretical analysis. We present the results  in Table \ref{tab:main_obsdata} for $\gamma_\tau = 0.1$, and defer the results for larger $\gamma_\tau$ to Appendix~\ref{sec:large_gamma_tau}. We make the following key observations:


(a) \textbf{IHDP}: This dataset contains 25 pre-treatment covariates, 19 of which are binary. Contrastive training struggled to capture these binary features, causing \muonly, which uses $\SInvCont_t(\xb)$ as input, to consistently underperform \realonly, which directly uses $\xb$. As shown in Fig~\ref{fig:valid_fct}, the $p$-value is zero for the IHDP dataset, but significantly larger for the others. As a result, we set the weight of the regularizer $d(\RInvhat_t, \SInvCont_t)$, controlled by $\Lphi$, to 1e-4 for IHDP, while keeping $\Lphi$ at its default value of 1 for ACIC. Overall, \our\ achieved the best performance.



\begin{table}
    \vspace{-0.4cm}
    \caption{\label{tab:main_obsdata}\small{RQ2: Comparison of \our\ with several pre-treatment baselines and post-treatment proposals. $p$-values for paired t-tests against \our\ 
    are in brackets. Lower $p$-values indicate statistical significance. \our\ outperforms others overall, while \simonly\ performs best on ACIC-2 since $\tau = \tau^S$.}} 
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|r|r|r|r|r}
        \toprule
        Method & IHDP & ACIC-2 & ACIC-7 & ACIC-26 & ACIC-All \\
        \hline \hline
        RNet ~\citep{rlearner}&           1.54 (0.00) &            3.30 (0.00) &          5.91 (0.04) &         6.06 (0.18) & 5.78 (0.03) \\
        XNet ~\citep{xlearner} &            1.0 (0.00) &          0.43 (0.15) &          5.49 (0.17) &          5.1 (0.38)  &  4.45 (0.41)\\
       DRNet ~\citep{DRNet} &           0.96 (0.00) &          0.24 (0.59) &          5.53 (0.15) &         5.08 (0.39) & 4.45 (0.40)\\
      CFRNet ~\citep{cfrnet} &           0.96 (0.00) &          0.36 (0.26) &          5.55 (0.15) &         5.09 (0.38) & 4.67 (0.32) \\
   FlexTENet ~\citep{inducbias} &           0.96 (0.00) &          0.32 (0.32) & \second{5.46 (0.19)} & \second{5.04 (0.40)} & 4.85 (0.37)\\
   DragonNet ~\citep{dragonnet} &           0.96 (0.00) &          0.29 (0.41) &          5.57 (0.14) &         5.09 (0.38)  & 4.60 (0.40)\\
         IPW ~\citep{ipw_1} &           0.96 (0.00) &          0.36 (0.24) &          5.56 (0.15) &         5.09 (0.38)  & 4.45 (0.40)\\
$k$-NN ~\citep{matching_survey} &           0.96 (0.00) &          0.33 (0.33) &          5.48 (0.18) &         5.13 (0.37) & \second{4.44 (0.40)} \\
PerfectMatch ~\citep{perfect_match} &           0.98 (0.00) &          0.56 (0.11) &          5.75 (0.08) &         5.13 (0.37) & 4.56 (0.28) \\
   StableCFR ~\citep{stablecfr} &           1.01 (0.00) &          1.09 (0.03) &          5.56 (0.15) &         5.08 (0.43) & 4.82 (0.14)\\
       ESCFR ~\citep{escfr} &           0.96 (0.00) &          0.27 (0.47) &          5.55 (0.15) &         5.79 (0.21)  & 4.73 (0.18)\\
       PairNet~\citep{pairnet} & 0.97 (0.00) & \second{0.12 (0.85)} & \second{5.46 (0.23)} &  5.05 (0.37) & \second{4.44 (0.41)}\\
       \hline
    \simonly &           0.94 (0.00) &   \first{0.00 (0.98)} &           6.65 (0.00) &          6.60 (0.12) & 6.45 (0.02)\\
   \realonly & \second{0.83 (0.13)} &         11.23 (0.01) &         14.81 (0.05) &         8.18 (0.01) & 9.82 (0.00) \\
     \muonly &           0.96 (0.00) & 0.17 (0.76) &          5.57 (0.14) &         5.09 (0.38) & 4.52 (0.38) \\
        \our &  \first{0.79 (0.00)} &          0.26 (0.00) &  \first{5.04 (0.00)} &         \first{4.67 (0.00)}  & \first{4.36 (0.00)}\\
    \bottomrule
    \end{tabular}
    }
\end{table}

(b) \textbf{ACIC-2:} 
This dataset is unique in that the true CATE, $\tau$, is constant across all individuals in the observational data, implying that its standard deviation $\sigma(\tau) = 0$ for real samples. As a result, our approach to synthesizing the simulated CATE, $\syntau$, given by
$\syntau(z) = \tau(z) + \left(\sigma(\tau) \cdot \gamma_\tau \cdot z^\top w_\tau^S\right)$, yields $\syntau(z) = \tau(z)$ for all $z$. This leads to perfect alignment between the synthetic and true CATE, causing \simonly\ to outperform all other methods on this dataset. Although \our\ could have improved by assigning a higher weight to the $\tau^S$ regularizer, tuning this weight would typically require supervision on $\tau$, which we avoid.


(c) \textbf{ACIC-7 and ACIC-26:} The CATENets baselines significantly outperform the \realonly\ model, because of high-quality causal representations extracted by $\SInvCont_t$. This is demonstrated by \muonly\ outperforming \realonly\ on factual error (see Fig.~\ref{fig:valid_fct}). In ACIC-7 and ACIC-26, \our\ achieves the best results by leveraging the closeness between $\tau$ and $\tau^S$. FlexTENet~\citep{inducbias}, which shares parameters between $\realmuhat_0$ and $\realmuhat_1$, and PairNet~\citep{pairnet}, which applies losses on pairs of close-by samples are strong contenders to \our.


\textbf{ACIC-All:} The last column in the table presents results across all 77 seeds of the ACIC dataset. \our\ achieves the lowest mean CATE error overall. However, for certain seeds, \our's CATE error remains comparable to baselines such as PairNet, $k$-NN, as indicated by them having $p$-values close to $0.4$. The \realonly\ model consistently performs worse across all seeds due to the covariate functions $\R_0$ and $\R_1$ producing disparate post-treatment covariates $X_0$ and $X_1$ when applied to $Z$. Similarly, the \simonly\ model suffers due to the distributional mismatch between the real and simulator data.



\subsection{RQ3: Ablation of \our\ Losses}
\label{sec:ablating_losses}
\begin{table}
    % \vspace{-0.6cm}
    \caption{\label{tab:ablation_loss}\small{{RQ3: Impact of regularizers.} Here, $-d(\RInvhat_t, \SInvCont_t)$ represents our loss~\ref{eq:overall_objective} with $\lambda_f = 0$, and $-\tau^S$ means $\lambda_\tau = 0$. A negative value implies \our\ with all regularizers outperforms the ablation where one regularizer is disabled.}} 
    \centering
    \resizebox{0.8\textwidth}{!}{       
         \begin{tabular}{l|l|l|r|r||r|r}
        \toprule
         ~ & ~ & ~ &  \multicolumn{2}{c||}{IHDP - Linear $\RInv_t$, Linear $\realmu_t$} &   \multicolumn{2}{c}{GP - Linear $\RInv_t$, Non-Linear $\realmu_t$}  \\ \hline
        $d(\RInv_0, \RInv_1)$ & $d(\RInv_t, \SInv_t)$ & $d(\tau, \tau^S)$ & $-d(\RInvhat_t,\SInvCont)$ &  $-\tau^S$ &   $-d(\RInvhat_t,\SInvCont)$ &  $-\tau^S$   \\
        \hline \hline   
         0.00 &              high &              high &                                   +1.29 (1.00) &       -1.07 (0.18) &                                -0.55 (0.31) &     -0.62 (0.26) \\
         high &               low &               low &                                  -0.64 (0.04) &       +0.01 (0.51) &                                -0.29 (0.24) &     -0.04 (0.47) \\
         high &               low &              high &                                  -0.40 (0.11) &        +0.00 (0.50) &                                 -0.42 (0.10) &     -0.19 (0.34) \\
         high &              high &               low &                                  +1.74 (0.99) &       -0.03 (0.48) &                                -0.66 (0.27) &     -0.71 (0.25) \\
         high &              high &              high &                                   +1.30 (1.00) &       -0.03 (0.45) &                                -0.72 (0.21) &     -0.97 (0.18) \\

    \bottomrule
    \end{tabular}
    }
    \vspace{-0.4cm}
\end{table}


We evaluate the impact of the $d(\RInvhat_t,\SInvCont_t)$ and $\tau^S$ regularizers in \our's objective \ref{eq:overall_objective}. We experiment with the Linear IHDP dataset (Sec. \ref{sec:rq1}) and the Non-Linear Gaussian process dataset (Sec. \ref{sec:rq2}). For the Linear IHDP dataset, in the $-\tau^S$ case, we added an $L_2$ penalty on $w_t$ for the alternating minimization to work. Table \ref{tab:ablation_loss} presents the {\em difference} in CATE errors of \our\ and the ablation, averaged over five seeds with $p$-values. A negative entry means \our\ does better than the ablation. We observe that $\syntau$ loss is very effective since \our\ outperforms the $-\tau^S$ in both datasets. 
For IHDP, removing $d(\RInvhat_t,\SInvCont_t)$ loss helps. We could not set $\lambda_f$ weight to be small because both \realonly\ and \muonly\ achieved zero factual error. 
Despite this, \our\ with both regularizers  comfortably outperformed the other proposals demonstrating it as a better candidate for our task.


\subsection{RQ4: Linear covariate function $\R$ and non-linear outcome function $\realmu$} \label{sec:rq2}
\begin{table}
    \vspace{-0.4cm}
    \caption{\small{RQ4: Results for linear covariate and GP-based nonlinear outcome functions. We run each experiment 5 times and show the $p$-values. \our\ outperforms others in many settings. \realonly\ is a strong contender.
    % due to $\RInv_t$ being a simple linear function when compared to $\realmu_t$.
    }} \label{tab:main_gp}
    \centering
    \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{l|l|l|r|r|r|r}
            \toprule
            $d(\RInv_0, \RInv_1)$ & $d(\RInv_t, \SInv_t)$ & $d(\tau, \tau^S)$ &   \simonly & \realonly &  \muonly &   \our \\
            \hline \hline
          0.00 &              high &              high & 5.30 (0.09) & \second{2.99 (0.11)} &          3.17 (0.09) &  \first{1.94 (0.00)} \\
          low &               low &               low & 1.77 (0.15) & \second{1.06 (0.73)} &  \first{1.05 (0.77)} &          1.26 (0.00) \\
          low &               low &              high & 1.56 (0.08) &  \first{0.98 (0.65)} &          1.07 (0.44) & \second{1.05 (0.00)} \\
          low &              high &               low & 4.60 (0.04) & \second{2.87 (0.07)} &          3.12 (0.04) &  \first{1.88 (0.00)} \\
          low &              high &              high & 4.15 (0.04) & \second{2.96 (0.02)} &          3.11 (0.01) &  \first{1.60 (0.00)} \\
         high &               low &               low & 2.39 (0.14) & \second{1.19 (0.68)} &  \first{1.12 (0.77)} &          1.37 (0.00) \\
         high &               low &              high & 1.94 (0.11) &  \first{0.98 (0.83)} & \second{1.16 (0.58)} &          1.21 (0.00) \\
         high &              high &               low & 7.42 (0.09) & \second{2.65 (0.38)} &          2.95 (0.25) &  \first{2.37 (0.00)} \\
         high &              high &              high & 5.70 (0.07) & \second{2.80 (0.17)} &          3.16 (0.09) &  \first{2.10 (0.00)} \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.3cm}
\end{table}

Now, we consider a more complex setup where the covariate functions $\R_t$ and $\S_t$ remain linear, but the outcome functions $\realmu_t$ and $\synmu_t$ are nonlinear in $Z$. In particular, we sample the outcomes $y$ and $y^S$ using Gaussian Processes (GPs) ~\citep{gpml}. Let $GP(0, \cK_\gamma)$ denote a GP with an RBF kernel of width $\gamma$, so a higher $\gamma$ results in a more complex function. To sample the $\realmu_0, \realmu_1$ such that their difference $\tau$ has a gap $\gamma$, we follow:
(1) Sample $\tau$ using a GP: $\tau \sim GP(0, \cK_\gamma)$.
(2) Sample $\realmu_0 \sim GP(0, \cK_1)$.
(3) Set $\realmu_1 \sim \realmu_0 + \tau$.
As before, we set $\gamma=0.4$.
Now, to sample $\synmu_0, \synmu_1$ such that $d(\tau, \tau^S) = \gamma_\tau$:
(1) Set $\tau^S \sim \tau + GP(0, \cK_{\gamma_\tau})$.
(2) Sample $y^S_0 \sim GP(0, \cK_1)$.
(3) Set $y^S_1 = y^S_0 + \tau^S$.
% 


We estimate $\SInvCont$ in Eq. \ref{eq:cont_loss} in closed-form, whereas we learn other parameters using gradient descent.
We show the results in Table \ref{tab:main_gp} where we observe: (a) \our\ outperforms the other baselines in five out of nine settings (b)
In alignment with our theory, \muonly\ performs better than others only when $d(\RInv_t, \SInv_t)$ is small. In summary, when the properties of the underlying DGP are unclear, \our\ proves to be an effective approach for learning $\tau$.




\subsection{RQ5: Comparing CATE methods when trained on $Z$ vs $X$ vs $\SInvCont(X)$}
\label{app:mlp_expts}
To address RQ5, we use the IHDP dataset to evaluate the baseline models when trained on post-treatment covariates $X$ directly, and we compare these results with those from Table~\ref{tab:main_obsdata}, where the baselines were trained on simulated causal representations, $\SInvCont_t(\xb)$. We also evaluate the baselines trained on pre-treatment $Z$ to explicitly show the detrimental impact of post-treatment covariates on the CATE error.

In addition, we extend Table~\ref{tab:main_obsdata} by considering non-invertible, non-linear transformations with Multi-Layer Perceptrons (MLPs) on the IHDP dataset to assess the robustness of \our\ and baselines when the diffeomorphism assumption is violated. To this end, we used two-layer MLPs for $\R_t$ and $\S_t$ to generate covariates from $Z$. We present the results in Fig.~\ref{fig:mlp_expts}, and make the following observations:

In the pre-treatment setting, causal representation extraction is unnecessary, making simulator supervision inconsequential; thus, we omit the four post-treatment CATE methods introduced in this paper. For other baselines, Fig.~\ref{fig:mlp_expts} shows that they perform significantly worse with post-treatment $X$ than with pre-treatment $Z$, underscoring the importance of causal representation recovery. DragonNet achieved the best performance with pre-treatment covariates.

For Normalizing Flow-generated covariates, CATE error consistently decreased when baselines used simulator-based causal representations, $\tilde{S}_t(X)$, rather than $X$, validating the utility of simulators in extracting causal representations. However, the CATE error remains significantly higher compared to using pre-treatment covariates $Z$, highlighting that simulators, while helpful, are not ideal and exhibit a distributional gap between real and simulator distributions. This gap impacts all CATE methods. Notably, \our, specifically designed to leverage simulator supervision and incorporate regularizers using simulator-generated data, achieves the best results. 

In the MLP-based covariate experiments with non-invertible covariate functions $\R_0, \R_1$, \our\ consistently outperformed all baselines, exhibiting trends similar to those observed with Normalizing Flow-generated covariates. This suggests that the diffeomorphism assumption, while necessary for our theoretical results, may be inconsequential in practice.



\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{images/zvsx.pdf}
    \caption{Comparing CATE errors under pre-treatment $Z$, and MLP, Normalizing flow generated post-treatment covariates $X$.}
    \label{fig:mlp_expts}
\end{figure}


\subsection{Varying $\gamma_\tau$ in Arbitrary DGP Experiment}
\label{sec:large_gamma_tau}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/gamma_tau_plot.pdf}
    \caption{\small{We vary $\gamma_\tau$, which controls the gap between the synthetic CATE, $\syntau$, and the real CATE, $\tau$. Each dataset is represented by a distinct color, where the pale version of the color indicates \simonly\ and the darker version denotes \our. For ACIC-7 and ACIC-26, as $\gamma_\tau$ increases, the CATE error grows significantly. Therefore, we present these results as an inset figure in the top-right corner.}}
    \label{fig:gamma_tau}
\end{figure}
The results of the varying $\gamma_\tau$ experiment are presented in Fig. \ref{fig:gamma_tau}, where we compare two approaches that leverage simulator data during training: \simonly\ and \our. Across all $\gamma_\tau$ gaps, we observe that \our\ consistently outperforms \simonly\ in three out of four datasets, with ACIC-2 being the exception. This is expected, as ACIC-2 satisfies the condition $\tau_s = \tau$. The performance gains of \our\ are particularly notable in the ACIC-7 and ACIC-26 datasets, where the CATE error for \simonly\ escalates significantly at larger $\gamma_\tau$ values. The exact error values for these cases are shown in the inset subplot at the top-right. These findings underscore our argument: while \simonly\ can perform well on simulators closely aligned with the real world, it struggles with real-world simulators that diverge from reality. In contrast, \our's adjustment strategies—enabled by theoretically grounded regularizers derived from the CATE error analysis—yield much more reliable CATE estimates.



\subsection{On the Quality of $Z$ Extracted by \our}

\begin{table*}
    \caption{\label{tab:ablation_z}\small{RQ5: Evaluating \muonly\ performance with $Z$ from \our's $\RInvhat_t$ vs. $\SInvCont_t$ obtained from $\synD$. \muonly\ with \our's $\RInvhat_t$ significantly outperforms in many settings indicating \our\ extracts better $Z$.}} \label{tab:z_quality}
    \centering
    \resizebox{12cm}{!}{
         \begin{tabular}{l|l|l|r|r||r|r}
        \toprule
         ~ & ~ & ~ &  \multicolumn{2}{c||}{IHDP - Linear $\RInv_t$, Linear $\realmu_t$} &   \multicolumn{2}{c}{GP - Linear $\RInv_t$, Non-Linear $\realmu_t$}  \\ \hline
        $d(\RInv_0, \RInv_1)$ & $d(\RInv_t, \SInv_t)$ & $d(\tau, \tau^S)$ & \muonly &  $\text{Real}_\mu\text{\our}_f$ &   \muonly &  $\text{Real}_\mu\text{\our}_f$   \\
        \hline \hline   
         0.00 &              high &              high & \second{48.76 (0.05)} &                  \first{3.28 (0.00)} & \second{3.17 (0.32)} &                \first{2.67 (0.00)} \\
          low &               low &               low &   \second{2.73 (0.00)} &                  \first{0.97 (0.00)} &  \first{1.05 (0.81)} &               \second{1.55 (0.00)} \\
          low &               low &              high &  \second{2.73 (0.02)} &                  \first{1.49 (0.00)} &  \first{1.07 (0.78)} &               \second{1.55 (0.00)} \\
          low &              high &               low & \second{48.76 (0.05)} &                  \first{3.48 (0.00)} & \second{3.12 (0.23)} &                \first{2.53 (0.00)} \\
          low &              high &              high & \second{48.76 (0.05)} &                  \first{3.21 (0.00)} & \second{3.11 (0.34)} &                \first{2.73 (0.00)} \\
         high &               low &               low &   \second{2.73 (0.00)} &                  \first{0.98 (0.00)} &  \first{1.12 (0.72)} &               \second{1.29 (0.00)} \\
         high &               low &              high &  \second{2.73 (0.02)} &                  \first{1.49 (0.00)} &  \first{1.16 (0.76)} &               \second{1.42 (0.00)} \\
         high &              high &               low & \second{48.76 (0.05)} &                  \first{3.37 (0.00)} &  \first{2.95 (0.57)} &               \second{3.11 (0.00)} \\
         high &              high &              high & \second{48.76 (0.05)} &                  \first{3.23 (0.00)} & \second{3.16 (0.42)} &                \first{2.97 (0.00)} \\
    \bottomrule
    \end{tabular}
    }
\end{table*}
In this experiment, we evaluate the quality of representations learned by \our. If \our's regularizers in the losses enable the recovery of representations identifiable for CATE, then a \realonly\ model trained on these representations should outperform the \muonly model trained on representations extracted using contrastive training on the simulator dataset. We focus on the Linear IHDP and non-linear GP datasets, as they provide closed-form solutions for Objective~\ref{eq:cont_loss}, and and gives us a fine grained control to set the gaps between real and simulated datasets and assess the performance across many settings of these gaps. Specifically, we train the \realonly\ model using $Z$ extracted from $\RInvhat_t$ via \our, and call it as $\text{Real}_\mu\text{\our}_f$, and compare it to \muonly\ trained from $\SInvCont$. Results in Table~\ref{tab:z_quality} show that $\text{Real}_\mu\text{\our}_f$ significantly outperforms the baseline \muonly across many DGP settings, with particularly strong results in the Linear IHDP case where $p$-values are consistently small. This demonstrates that \our\ effectively learns high-quality representations $Z$.


\subsection{Sensitivity to Loss Weights}




\begin{table*}[ht]
\centering
\caption{\small{This table assesses the sensitivity of the loss weights on all three versions of the IHDP dataset considered in our work -- Synthetic Linear Sec.~\ref{sec:rq1}, Synthetic Non-Linear Sec.~\ref{sec:rq2}, and Semi-synthetic Sec.~\ref{sec:rq3}. We consider nine different settings of the loss weights in \our's objective~\ref{eq:overall_objective}. The table shows mean $\pm$ standard deviation of the CATE Error.}}
\label{tab:loss_sens}
\resizebox{9.5cm}{!}{
    \begin{tabular}{l|l|r|r|r}
    \toprule
    $\lambda_\mu$ & $\lambda_\phi$ & Synthetic Linear & Synthetic Non-Linear & Semi-Synthetic \\ \hline\hline
    0.1 & 0.1 & $3.46 \pm 0.91$ & $4.23 \pm 2.21$ & $0.72 \pm 0.19$ \\
    0.1 & 0.5 & $3.42 \pm 0.84$ & $3.55 \pm 1.21$ & \first{$0.70 \pm 0.19$} \\
    0.1 & 1.0 & $3.43 \pm 0.78$ & $3.40 \pm 1.15$ & \second{$0.71 \pm 0.20$} \\
    0.5 & 0.1 & $3.41 \pm 0.90$ & $4.06 \pm 2.21$ & $0.74 \pm 0.19$ \\
    0.5 & 0.5 & \second{$3.38 \pm 0.88$} & $3.32 \pm 1.06$ & $0.73 \pm 0.19$ \\
    0.5 & 1.0 & \first{$3.37 \pm 0.82$} & \second{$3.23 \pm 1.00$} & \second{$0.71 \pm 0.20$} \\
    1.0 & 0.1 & \second{$3.38 \pm 0.86$} & $3.99 \pm 2.17$ & \second{$0.71 \pm 0.20$} \\
    1.0 & 0.5 & $3.39 \pm 0.87$ & $3.24 \pm 1.02$ & $0.72 \pm 0.19$ \\
    1.0 & 1.0 & \second{$3.38 \pm 0.85$} & \first{$3.14 \pm 0.93$} & \second{$0.71 \pm 0.21$} \\ \hline
    \end{tabular}
}
\end{table*}
In this experiment, we analyze the sensitivity of \our's performance to the loss weights $\lambda_f$ and $\lambda_\tau$ in its objective function. Using the IHDP covariates, we performed this analysis in the three variants used in our study. For the synthetic datasets, we set the real simulator gaps, as specified in Tables~\ref{tab:main_linear} and~\ref{tab:main_gp}, to \verb|high|. In the Semi-Synthetic IHDP setting, we evaluate performance on five randomly selected dataset versions. The results, summarized in Table~\ref{tab:loss_sens}, report the mean CATE error along with its standard deviation across dataset seeds. All methods consistently performed poorly when the loss weights are small. This indicates that regularizers are important to impose on post-treatment covariates. Across other settings, overall, we observe that \our\ is sensitive to the choice of hyperparameter settings in non-linear and semi-synthetic versions, and remains fairly robust in the linear setting. However, tuning these hyperparameters is challenging in practice due to the absence of counterfactual data in real observational datasets. Despite this sensitivity, \our\ outperforms achieves either comparable performance or manages to surpass the baseline methods across different loss weight configurations.
