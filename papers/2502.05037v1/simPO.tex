\documentclass[tikz, border = 0.1cm]{article}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\tikzset{>=latex}
\usepackage{filecontents}
\usepackage{placeins}
% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2024}
\usepackage{natbib} % has a nice set of citation styles and commands
\bibliographystyle{plainnat}
\renewcommand{\bibsection}{\section*{References}}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%\usepackage{enumitem}
\usepackage[textsize=tiny]{todonotes}

% Added by Lokesh

%%%%%%%%%%%%%For putting tables side-by-side%%%%%%%%%
\usepackage{float}
\usepackage{caption}

\usepackage{geometry}
\usepackage{array}
\newcolumntype{C}[1]{p{#1}}
\usepackage{icomma}
\usepackage{ragged2e}
\usepackage{wrapfig,lipsum,booktabs}
\RequirePackage[inline,shortlabels]{enumitem}
\usepackage{algorithmic}
\usepackage{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{xcolor,colortbl}
\definecolor{green}{rgb}{0.8,1,0.8}
\definecolor{yellow}{rgb}{1,1,0.87}
\definecolor{cyan}{rgb}{0.0, 1.0, 1.0}
\newcommand{\first}[1]{{\cellcolor{green} #1}}
\newcommand{\second}[1]{{\cellcolor{yellow} #1}}
\newcommand{\third}[1]{{\cellcolor{cyan!50} #1}}

\usepackage{subdef}
\usepackage{multirow}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

% \usepackage{algpseudocode}
% \usepackage{algorithm}
\usepackage{hyperref}


\usepackage[subtle]{savetrees}

% For bayesnets
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{backgrounds}

\usepackage{titlesec}

% minimize spacing
% \titlespacing\section{0pt}{1pt plus 0pt minus 1pt}{1pt plus 0pt minus 1pt}
\titlespacing\subsection{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing\subsubsection{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\lemma}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}

\newcommand{\xhdr}[1]{\vspace{1.5mm}\noindent{\bf #1.}}

\newcommand{\recourse}{recourse}
\newcommand{\bij}{\beta^i _j}
\newcommand{\bpij}{\widetilde{\beta}^i _j}
\newcommand{\net}{{\textsc{nn}}}
\newcommand{\gen}{\mathcal{Z}}
\newcommand{\genS}{\mathcal{Z}_S}
\newcommand{\geninv}{\gen^{-1}}
\newcommand{\genSinv}{\genS^{-1}}
\newcommand{\betabp}{\betab'}

\newcommand{\betaij}{\betab_{ij}}
\newcommand{\q}{q}
\newcommand{\betair}{\betab_{ir}}
\newcommand{\xij}{\xb_{ij}}
\newcommand{\xir}{\xb_{ir}}
% \newcommand{\xi}{x_{i}}
\newcommand{\yi}{y_{i}}
\newcommand{\betai}{\betab_i}
\newcommand{\sij}{S_{ij}}
\newcommand{\xit}{\xb_{it}}
\newcommand{\rhoij}{\rho_{ij}}
\newcommand{\betait}{\betab_{it}}
\newcommand{\y}{y}
\newcommand{\betaprior}{\betab^{prior}}
\newcommand{\gd}{gd}
\newcommand{\betaiprior}{\betab_i^{prior}}
\newcommand{\thhatnet}{f_{\widehat{\theta}}}
\newcommand{\genp}{\Pr_{\text{gen}}}
\newcommand{\rhohat}{\widehat{\rho}}

\newcommand{\RbetaInv}[1]{\Rbeta{#1}^{-1}}
\newcommand{\Sbeta}[1]{\mat{S}_{#1}}
\newcommand{\SbetaInv}[1]{\Sbeta{#1}^{-1}}
\newcommand{\wbeta}[1]{w_{#1}}
\newcommand{\wbetas}[1]{w_{#1}^S}

\newcommand{\thnet}{{f_\theta}}
\newcommand{\trnD}{D_{\text{trn}}}
\newcommand{\tstD}{D_{\text{tst}}}
\newcommand{\synD}{D_{\text{syn}}}
\newcommand{\Zspace}{\mathcal{Z}}
\newcommand{\synz}{\Tilde{z}}
\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\xspace}{\mathcal{X}}
\newcommand{\yspace}{\mathcal{Y}}
\newcommand{\xSspace}{\mathcal{X}_S}
\newcommand{\Lspace}{\mathcal{L}}
\renewcommand{\yspace}{\mathcal{Y}}
\newcommand{\lhat}{\widehat{l}}
\newcommand{\zextS}{\Phi_S}
\newcommand{\zextR}{\Phi_R}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\recmdl}{g_\phi}
\newcommand{\realwb}{w}
\newcommand{\realwbhat}{\widehat{w}}
\newcommand{\synwb}{w^S}
\newcommand{\synwbhat}{\widehat{w}^S}
\newcommand{\naivew}{w_{t}^D}
\newcommand{\directw}{w_{t}^D}
\newcommand{\jointw}{w_{t}^J}
\newcommand{\factualw}{w_{t}^F}
\newcommand{\jointR}{\widehat{\mat{R}}_{\betab}^{-1}}
\newcommand{\jointS}{\mat{S}_{\betab}^{-1}}

\newcommand{\Rbeta}[1]{\mat{R}_{#1}}



\newcommand{\jointwp}{w_{t}^J}
\newcommand{\jointRp}{\mat{R}_{t}^{-1}}
\newcommand{\jointSp}{\mat{S}_{t}^{-1}}
\newcommand{\realmu}{\mu}
\newcommand{\synmu}{\mu^S}
\newcommand{\realmuhat}{\widehat{\mu}}
\newcommand{\synmuhat}{\widehat{\mu}^S}
\newcommand{\synmuTilde}{\widetilde{\mu}^S}


\newcommand{\ITEx}{\tau_X}
\newcommand{\ITExhat}{\widehat{\ITEx}}
\newcommand{\ITExSyn}{\ITEx^S}
\newcommand{\tauhat}{\widehat{\tau}}
\newcommand{\syntau}{\tau^S}
\newcommand{\syntauhat}{\widehat{\tau}^S}
\newcommand{\syntauTilde}{\widetilde{\tau}^S}

\newcommand{\gapRR}{\lambda_{R}}
\newcommand{\gapRS}{\lambda_{RS}}
\newcommand{\gapwR}{\lambda_{w}}
\newcommand{\gapwS}{\lambda_{wRS}}

\newcommand{\noise}{\mathcal{N}(0,1)}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cK}{\mathcal{K}}

\newcommand{\nind}{\not\!\perp\!\!\!\perp}

\newcommand{\betarv}{\mathfrak{B}}
\newcommand{\our}{\textit{SimPONet}}


\newcommand{\RInv}{f}
\newcommand{\RzInv}{f_0}
\newcommand{\RoInv}{f_1}
\newcommand{\RInvhat}{\widehat{f}}
\newcommand{\RzInvhat}{\widehat{f}_0}
\newcommand{\RoInvhat}{\widehat{f}_1}
\newcommand{\R}{g}
\newcommand{\Rz}{g_0}
\newcommand{\Ro}{g_1}
\newcommand{\SInv}{f^S}
\newcommand{\SInvTilde}{\widetilde{f}^S}
\newcommand{\SInvCont}{\SInvTilde}
\newcommand{\SInvhat}{\widehat{f}^S}
\newcommand{\SzInv}{f^S_0}
\newcommand{\SoInv}{f^S_1}
\renewcommand{\S}{g^S}
\newcommand{\Sz}{g^S_0}
\newcommand{\So}{g^S_1}
\newcommand{\Shat}{\widehat{g}^S}
\newcommand{\Szhat}{\widehat{g}^S_0}
\newcommand{\Sohat}{\widehat{g}^S_1}
\newcommand{\LRfct}{\lambda_{\text{fct}}}
\newcommand{\LSfct}{\lambda^S_{\text{fct}}}
\newcommand{\LScf}{\lambda^S_{\text{cf}}}
\newcommand{\Lphi}{\lambda_{\RInv}}
\newcommand{\Ltau}{\lambda_{\tau}}
\newcommand{\ErrITE}{\mathcal{E}_{\text{CATE}}}
\newcommand{\ErrF}{\mathcal{E}_F}
\newcommand{\ErrCF}{\mathcal{E}_{CF}}
\newcommand{\ErrCFR}{\mathcal{E}_{CF}^R}
\newcommand{\ipm}[1]{\textsc{IPM}_{#1}}
\newcommand{\ipmg}{\ipm{G}}
\newcommand{\distR}{\texttt{dist}(\realmu \circ \RInv, \realmu \circ \SInv)}
\newcommand{\distmu}{\texttt{dist}(\realmu \circ \SInv, \synmu \circ \SInv)}
\newcommand{\distcont}{\texttt{dist}(\SInv, \SInvCont)}

\newcommand{\simonly}{SimOnly}
\newcommand{\realonly}{RealOnly}
\newcommand{\muonly}{$\text{Real}_\mu\text{Sim}_f$} %{$\mu$Only}
\newcommand{\epsf}{\epsilon_{\RInv}}
\newcommand{\delf}{\delta_{\RInv}}
\newcommand{\epsmu}{\epsilon_{\realmu}}
\newcommand{\delmu}{\delta_{\realmu}}



%%%%%%%%%%%%%%%%%%%%%%%
% Color Block
%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xcolor} 
\usepackage{tikz} 
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\usetikzlibrary{shadings,shadows}
\newenvironment{gblock}[1]{%
\tcolorbox[beamer,%
noparskip,breakable,
colback=LightGreen,colframe=DarkGreen,%
colbacklower=LimeGreen!75!LightGreen,%
title=#1]}%
{\endtcolorbox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Added by Lokesh
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usetikzlibrary{shapes, arrows, calc, arrows.meta, fit, positioning} % these are the parameters passed to the library to create the node graphs  
\tikzset{  
    -Latex,auto,node distance =1.5 cm and 1.3 cm, thick,% node distance is the distance between one node to other, where 1.5cm is the length of the edge between the nodes  
    state/.style ={ellipse, draw, minimum width = 0.9 cm}, % the minimum width is the width of the ellipse, which is the size of the shape of vertex in the node graph  
    point/.style = {circle, draw, inner sep=0.18cm, fill, node contents={}},  
    bidirected/.style={Latex-Latex,dashed}, % it is the edge having two directions  
    el/.style = {inner sep=2.5pt, align=right, sloped}  
}  
\usepackage{multirow}

\title{Leveraging a Simulator to Estimate Treatment Effects from Post-Treatment Covariates}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle

\vspace{-5mm}
\begin{abstract}
\vspace{-3mm}
Treatment effect estimation involves assessing the impact of different treatments on individual outcomes. Current methods rely on observational datasets where covariates are gathered before treatments and outcomes are observed afterward. However, real-world scenarios often deviate from this protocol, leading to both covariate and outcome observed 
% collection simultaneously 
post-treatment. We first establish that this deviation renders treatment effects unidentifiable, necessitating additional assumptions for estimation. We propose \our, which unlike prior methods that assume counterfactual supervision in the training datasets, assumes the availability of a simulator that generates related synthetic counterfactual data. This allows recovery of latent pre-treatment covariates 
% upto a diffeomorphic transformation 
which aid in treatment effect identification. The accuracy of such estimates hinges on the quality of the simulator, and we conduct theoretical analyses to establish generalization bounds that assess the CATE error based on the distributional discrepancies between real and synthetic data. In a linear setting, we analytically derive the CATE error, demonstrating the limitations of several baseline methods. Our empirical validation on synthetic and semi-synthetic real world datasets further reinforces \our's effectiveness in precise treatment effect estimation from post-treatment data.
\end{abstract}



\section{Introduction} \label{sec:intro}
% \begin{gblock}{On the writing}
%     \begin{enumerate}
%         \item Other than post-T bias
%         \item Realistic Example for post-T
%         \item Layout of different models.
%         \item Neurips? I am for it!
%         \item Research Qns for the Experiments (assuming just IHDP, Linear)
%     \end{enumerate}
% \end{gblock}
\vspace{-0.1in}
\begin{wrapfigure}{r}{0.35\textwidth}
  \vspace{-0.7cm}
  \begin{center}
    {\resizebox{0.35\textwidth}{!}{\input{diagram}}}
  \end{center}
  \caption{The Data Generating process for Real and Simulator.\label{fig:our_DGP}}
  \vspace{-0.8cm}
\end{wrapfigure}
Many applications require estimating the difference in outcome as a result of a change in treatment. For example, in healthcare, we want to estimate the change in a patient's response as we prescribe different drugs. %In economics, we want to estimate the difference in social metrics as we implement alternate policies, etc. 
Randomized Control Trials are often expensive, and with the easy availability of observational data, there is extensive interest in harnessing observational data for these estimates. Existing works~\cite{xlearner,rlearner,CurthS23,cfrnet,vcnet,Tarnet, chauhan2023adversarial, inducbias, overlapping_rep, dragonnet,matching_survey} assume that the covariates are observed pre-treatment and the outcome post-treatment, whereas we are interested in the setting where both are observed post-treatment. %and the main challenge they try to address is estimating treatment effect when a given individual is subject to only one treatment.
%

In many real-life applications both covariates and outcomes are observed post-treatment.
%Conventional methods approach this problem through A/B testing or via Randomized Control Trials.  In both these approaches, we first collect a group of individuals that participate in a trial. We record different covariates associated with the individuals and then assign treatments to them arbitrarily. Finally, we measure the outcomes and derive statistical estimates that assess the impact of the treatments. However, in many domains, such trials are not always possible because they are either unethical or raise cost concerns. In response, research has been done to assess the effects from observational datasets where we do not have control over treatment assignments. 
% One consequence of this is that the dataset exhibits confounding where the observed treatments depend on covariates. For example, in medicine, aged patients often exhibit a higher propensity of being assigned treatments with heavy dosages. Such confounding issues are known to hurt the effect estimates and a lot of methods are proposed in prior art to account for them. 
% However, one common theme across all prior-art is that they assume that the covariates are recorded pre-treatment, i.e., {\em before} an individual is given the treatment.
% , and most of the Prior-art primarily focuses on mitigating confounding effects within the dataset. 
% Various techniques, such as \todo{cite heavily} meta-learners, representation learners, matching, and weighting, have been proposed for this purpose. 
%
% \todo[inline]{Need a compelling example}
%
%\textbf{Post-Treatment covariates are common: }  
%Existing methods typically rely on the assumption that covariates are collected before treatment and outcomes are observed afterward. However, in many real-life contexts, covariates and outcomes are often collected simultaneously after treatment, posing challenges for conventional methods such as meta-learners~\cite{xlearner,rlearner,CurthS23}, matching methods~\cite{matching_survey, prop_score_matching, coarsened_exact_matching, perfect_match, deepmatch}, generative models such as GANs~\cite{scigan,cevae}, and representation learners ~\cite{cfrnet,vcnet,Tarnet, chauhan2023adversarial, inducbias, overlapping_rep, dragonnet}. 
%Post-treatment covariates are common in various fields. 
For example, in economics ~\cite{policy_1, policy_2}, policy efficacy often needs to be assessed in settings where
%is usually assessed by comparing social metrics across treated and control groups. It is highly impractical to expect the same set of individuals to participate in the trial twiceâ€”first to provide covariates before and then to provide outcomes after policy implementation. Therefore, it is necessary to develop methods that work with 
both outcomes and covariates are available post-policy implementation. Similarly, in voluntary healthcare surveys only post-treatment data about patients may be available.  In medical imaging, an image may be observed under a given instrument setting (treatment), and it may be necessary to choose an alternative setting under which a downstream diagnosis (outcome) could improve. 
%like COVID-19 vaccination administration \cite{covid_postt}, patient covariates are collected post-vaccination to assess side effects.


We model the estimation of causal effects with observed post-treatment covariates and outcomes using a 
%
%\textbf{Our Data Generating Process (DGP):} We consider an observational dataset $\trnD$ containing samples from a real 
Data Generating Process (DGP) as outlined in the top panel of Figure \ref{fig:our_DGP} marked Real DGP. Here, \textit{latent} variables $Z$ influence treatment $T$, outcome $Y$, and covariates $X$. The latent nature of $Z$ impedes the identifiability of treatment effects solely given i.i.d samples $\trnD$ from the observed nodes.  
\todo[inline]{L: Lemma -- $\tau$ cannot be identified from observational dataaet with post-T covariates. }
\todo[inline]{Highlight the 2-stage protocol with strong emphasis}

%
This is because conditioning on $X$  opens the backdoor path $T \rightarrow X \leftarrow Z \rightarrow Y$.
%
%
\iffalse
\begin{lemma}
    The causal effect of $T$ on $Y$ cannot be identified given i.i.d. samples from the real DGP.
\end{lemma}
%
\textbf{Proof:} Given that $Z$ is latent, the backdoor path $T \leftarrow Z \rightarrow Y$ remains open. Pearl's do-calculus shows that non-parametrically identifying treatment effects solely from $\trnD$ is impossible without additional assumptions. Moreover, conditioning treatment effects on $X$ opens the backdoor path $T \rightarrow X \leftarrow Z \rightarrow Y$, resulting in poor effect estimates. 
\fi
This phenomenon is sometimes known as \textit{post-treatment bias} ~\cite{post_bias_1, post_bias_2}. Therefore, the only possible solution is to somehow use $X$ to estimate $Z$ and then determine effects solely conditioned on $Z$ and $T$.

%\todo[inline]{Stress difficulty of CF outcome estimation more than treatment effect since our bounds are on CF error now.}

\textbf{Estimating $Z$: }
Our approach to learning $Z$ draws inspiration from disentangled representation learning, that aims at identifying latent factors influencing a covariate $X$. For example, in human face generation task, these latent factors include gender, age, skintone, and other facial attributes ~\cite{deepSCM}. We posit that these factors serve as an adequate adjustment set for blocking backdoor paths between $T$ and $Y$. While learning $Z$ solely from $\trnD$ is deemed impossible \cite{locatello2019challenging}, recent work \cite{von2021self} shows that learning $Z$ up to a diffeomorphic transformation is feasible under certain conditions, such as the (1) invertibility of structural equations in the DGP and (2) counterfactual supervision where for each $z_i \in \trnD$ we observe multiple $\xb$ under different treatments $t$.  Such supervision is missing in  most observational datasets.
 %However, the latter requirement is more challenging, necessitating that for each $z_i \in \trnD$ we observe multiple $\xb$'s associated with it under different treatments. 
To overcome such practical limitations, we propose augmenting $\trnD$ with synthetic counterfactual data obtained via simulators.

\textbf{Leveraging simulators: }
% When real-world counterfactuals are difficult to obtain, some methods model the underlying DGP to generate them. This involves a complex three-step process: (a) abduction, (b) action, and (c) counterfactual inference \cite{deepSCM}. Given this complexity, our approach is instead to 
We leverage off-the-shelf simulators that generate counterfactual data from a related \textit{synthetic} domain. Such simulators already exist in various fields: for example, medical simulators replicate drug responses for virtual patients~\cite{Simglucose2018}, robotics simulators facilitate training of ML algorithms in virtual environments~\cite{self_driving_cars}, and in metallurgy, scientists use simulators to evaluate the effects of various properties, such as pressure and temperature, on simulated alloys~\cite{raabe2014alloy} to shortlist potential candidates for real-world trials. Simulators introduce a distribution drift from the real world, and therefore, we analyze the performance tradeoffs based on their alignment with the real data. \todo[inline]{L: We illustrate some practical scenarios where such simulators can be effectively leveraged to suit our problem}

% train a function $\SInv$ for extracting $Z$ from simulated instances. We then use $\SInv$ to enhance the learning of $\RInv$, that extracts $Z$ for real instances. Since $\SInv$ may have errors on real instances, we must assess how its influence on $\RInv$ could impact the accuracy of treatment effects estimated on real data.

% In our scenario, these simulators offer weak supervision for counterfactual training instances, aiding in approximating $\RInv$. While this approximate $\RInv$ may not directly apply to the real domain, it serves as a valuable prior that can be refined with further adjustments to our training dataset. The accuracy of treatment effect estimates certainly relies on the simulator's quality, a challenge we address in this paper. Initially, we explore a setup assuming a linear DGP and analytically characterize the error in estimating treatment effects. Through theoretical analysis, we establish error bounds for treatment effect estimation, explicitly showing its dependence on the disparity between real and synthetic data. These insights lead us to introduce \our, a method that effectively uses weak supervision from a synthetic simulator to learn treatment effects.

\textbf{Contributions: }We make the following contributions: (1) We tackle Treatment Effect Estimation with post-treatment observed covariates $X$, which is known to be {\em not} identifiable. (2) We address this challenge by learning the pre-treatment $Z$ using a simulator that generates synthetic counterfactuals. (3) We introduce \our, a novel training algorithm that handles disparities between the real and simulator distribution. (4) To the best of our knowledge, ours is the \emph{first} work that systematically analyses the role of simulators in estimating ITE from port-treatment covariates. (5) We establish generalization bounds for CATE error, which motivates the learning objective of \our\ while also highlighting the impact of misalignment between the simulator and real distribution. (6) We conduct experiments with several DGPs to demonstrate the effectiveness of \our.

\vspace{-0.45cm}
\input{related}

\vspace{-0.4cm}
\input{problem_formulation}

\input{theory}

\input{experiments}

\FloatBarrier
\vspace{-0.3cm}
\section{Conclusion and Limitations} 
\vspace{-0.2cm}\label{sec:conclusion}
\textbf{Limitation}: (1) Since we do not know how to infer the gaps between real and simulator DGPs, tuning of the loss weights $\Lphi, \Ltau$ is challenging. Although we came up with effective heuristic to set $\Lphi$, setting $\Ltau$ is not very clear. \todo[inline]{L: Comment on $d(\tau, \tau^S)$} (2) None of the estimators, including \our, are consistent; i.e., as $|\trnD|, |\synD| \rightarrow \infty$, $\ErrITE$ does not converge to zero. We hope to address these issues in future.

\textbf{Conclusion: }In this paper, we addressed the problem of estimating treatment effects for individuals whose covariates are influenced by the treatment, a setting not solvable using observational data alone. We proposed to solve this task using off-the-shelf simulators that synthesize counterfactuals, unlike prior work relying on real-world counterfactuals, which limits their practical applicability. Ours is the first work to systematically analyse the role of simulators in handling the limitation of lack of counterfactual supervision in real world observational data. We introduced \our, which balances learning from real and simulator distributions to bound the rather intractable counterfactual error. Our theoretical analysis showed that \our\ has better CATE generalization bounds under reasonable assumptions in contrast to other proposals that need strong assumptions on the DGP. Extensive experiments with synthetic and real-world datasets demonstrated that \our\ is indeed a superior alternative.

\clearpage
\bibliography{refs}


\appendix
\input{appendix}

\clearpage
\section*{NeurIPS Paper Checklist}


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{}
    \item[] Justification: Yes, we have listed all the claims and contributions clearly.

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{}
    \item[] Justification: We explained limitations in Section \ref{sec:conclusion}.


\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{}
    \item[] Justification: We have provided detailed proofs in the Appendix. Each Lemma statements clearly mention the assumptions under which the theoretical results hold. 

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{}
    \item[] Justification: We have uploaded the code along with our submission. All our datasets are publicly available. Our code encompasses scripts that can be fired to produce an XL sheet with the results.


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{}
    \item[] Justification: All used public datasets and uploaded the code.



\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{}
    \item[] Justification: We did not perform hyperparameter tuning. We fixed all the hyperparameters to CATENets defaults \cite{benchmarking}. We adjusted just one hyperparameter for ACIC-2 dataset. We provide a clear justification for why that change was required.

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: We report std. deviations and $p$-values wherever appropriate.
    

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: All our experiments are run on standard benchmarks and are not compute intensive.
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{}
    \item[] Justification: We conform to the Neurips Code of Ethics.


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNA{}
    \item[] Justification: We use existing benchmarks and so our work does not have any negative impact.
    
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{}
    \item[] Justification: We do not use such high risk data.
    

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{}
    \item[] Justification: We will release the code and make it public after acceptance.
    

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer:\answerNA{}
    \item[] Justification: No new assets are introduced.

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer:\answerNA{}
    \item[] Justification: No crowdsourcing.

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{}
    \item[] Justification: No crowdsourcing.

\end{enumerate}

\end{document}