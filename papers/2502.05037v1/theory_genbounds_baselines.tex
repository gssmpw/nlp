
\subsection{The \simonly\ Estimator}
\simonly\ solely uses $\synD$. It leverages the counterfactual supervision provided by the simulator and identifies the simulator's DGP as follows:

(Step 1) Estimate the synthetic causal representation extractor $\SInv$ from covariate pairs $\{\xb_i^S(0), \xb_i^S(1)\}$ using contrastive learning \cite{von2021self}:
\begin{align}
    \{\SInvTilde_0, \SInvTilde_1\}  = \argmin_{\{\SInvhat_0, \SInvhat_1\}}\; \sum_{i=1}^{|\synD|}\left[- \log{\frac{\exp({\textrm{sim}(\hat{z}_i(1), \hat{z}_i(0)})}{\sum_{j \neq i}\sum_{t, t'} \exp({\textrm{sim}(\hat{z}_i(t), \hat{z}_j(t'))})}}\right]~~\text{where}~~\hat{z}_i(t) = \SInvhat_t(\xb^S_i(t))
    \label{eq:cont_loss}
\end{align}


where $\textrm{sim}(\bullet, \bullet)$ is cosine similarity, $(\xb^S_i(t), \xb^S_i(1-t))$ denotes a positive pair with the same underlying latent $z_i$. A negative pair $(\xb^S_i(t), \xb^S_j(t'))$ has different $(z_i, z_j)$. Contrastive learning increases similarity of representations of positive pairs $(\hat{z}_i(0), \hat{z}_i(1))$ while pushing apart the negative pairs $(\hat{z}_i(t), \hat{z}_{j \neq i}(t'))$.


\begin{lemma}
    As $|\synD| \rightarrow \infty$, contrastive training with paired counterfactual covariates as shown in Eq. \ref{eq:cont_loss} recovers $\SInvTilde_t = h \circ \SInv_t$ where $h$ is a diffeomorphic transformation. Moreover, when the latent space $\Zspace \subset \mathbb{S}^{(n_z - 1)}$ (unit-norm hypersphere in $\RR^{n_z}$), $h$ is a rotation transform by Extended Mazur-Ulam Theorem as shown in~\citep{zimmermann2021cl} (Proposition 2).
    \label{app:lemma:cl_rotation}
\end{lemma}
\textit{[Refer Appendix \ref{app:sec:mazurulam} for more details.]} 
% \todo[inline]{L: We are not replicating the proof. But we are trying to explain verbose how that proof fits in our CATE context.}

The main insight from the above lemma is that, given counterfactual supervision, it is possible to recover causal representations $Z$ from post-treatment covariates $X$ up to a rotation $h$, making CATE identifiable in the simulated distribution, as we demonstrate below. 

(Step 2) Estimate $\syntauTilde(z) = \synmuTilde_1(z) - \synmuTilde_0(z)$ with supervision on difference of outcomes $\syntau(\SInv_t(\xb_i^S(t))) = \y_i^S(1) - y_i^S(0)$ as 
\begin{align}
    \syntauTilde = \argmin_{\widehat{\syntau}} \sum_{\xb^S \in \synD} \left[\syntau(\SInv_t(\xb^S(t))) -  \widehat{\syntau}(\SInvTilde_t(\xb^S(t)))\right]^2 \label{eq:syntautilde}
\end{align}



 % Thus, $\tauhat = \syntauTilde, \RInvhat = \SInvTilde$, resulting in the following CATE estimate:
%
%
The \simonly\ method uses these estimates as-is on real data, i.e. $\tauhat = \syntauTilde$ and $\RInvhat_t = \SInvTilde_t, ~~\forall t \in \Tspace$. We analyze below the error incurred with such estimates on real data. 

\textbf{CATE error:} In the population setting, since $\SInvTilde_t = h \circ \SInv_t$, we see that the optimization problem in Eq.~\ref{eq:syntautilde} yields $\syntauTilde = \syntau \circ h^{-1}$ as its solution. 
% we observe that $\syntauTilde$ in Eq.~\ref{eq:syntautilde} equals $\syntau \circ h^{-1}$ in the limit of simulator samples. 
Thus, for an instance $\xb^\star$ from the real distribution under treatment 1, the true CATE is $\tau(\RInv_1(\xb^\star))$. The CATE error using \simonly\ becomes:

$$
\left[ \tau(\RInv_1(\xb^\star)) - \syntau \circ h^{-1}(h \circ \SInv_1(\xb^\star)) \right]^2 = \left[ \tau(\RInv_1(\xb^\star)) - \syntau(\SInv_1(\xb^\star)) \right]^2.
$$

This shows that for \simonly\ to provide accurate CATE estimates, the simulator must perfectly align with the real world; i.e., $\syntau = \tau$ and $\RInv_t = \SInv_t$ for all $t$. However, designing such simulators is highly challenging in practice, making this method unsuitable for CATE.




%



\subsection{The \realonly\ Estimator}
\realonly\ solely uses real observational data $\trnD$. Since $\trnD$ lacks counterfactual covariates, this model cannot apply contrastive training and therefore cannot explicitly supervise the recovery of causal representations. Instead, it focuses on regressing the factual outcomes $y_i(t_i)$ from post-treatment covariates $\xb_i(t_i)$. In terms of the four learning parameters, its learning objective is:
$$
    \argmin_{\{\realmuhat_0,\realmuhat_1 , \RInvhat_0, \RInvhat_1\}} 
    \sum_{i=1}^{|\trnD|}(y_i - \realmuhat_{t_i}(\RInvhat_{t_i}(\xb_i)))^2
$$
However, since $\realmuhat_t, \RInvhat_t$ are not individually supervised, we might as well collapse them into a composition $\realmu_t^F= \realmu_t \circ \RInv_t$; yielding $\realmu_{t_i}^F(\xb_i) = y_i$, and thereby CATE as $\ITExhat(\xb, t) = \realmuhat^F_1(\xb) - \realmuhat^F_0(\xb)$. 

\realonly\ is consistent in estimating the factual outcomes, because as $|\trnD| \to \infty$, we have
$
     \realmuhat_t^F = \argmin_{\realmuhat^F_t} \EE_{\xb \sim P(\xb|t)}\left[\left(\realmuhat^F_t(\xb) - \realmu^F_t(\xb)\right)^2\right] = \realmu_t^F \text{ and therefore, the factual error } \ErrF^t = 0
$.
% \end{align*}
However, \realonly\ incurs a significant error when estimating the counterfactual outcome, which in turn contributes to the CATE error, as shown below.



\textbf{CATE error:} The true CATE for $\xb^\star$ obtained using treatment $1$ can be written as $\tau(\RInv_1(\xb^\star)) = \realmu_1(\RInv_1(\xb^\star)) - \realmu_0(\RInv_1(\xb^\star))$. Then, the CATE error for \realonly\ is computed as:
$$
\left[\big(\realmu_1(\RInv_1(\xb^\star)) - \realmu_0(\RInv_1(\xb^\star))\big) - \big( \realmuhat_1^F(\xb^\star) - \realmuhat_0^F(\xb^\star) \big)\right]^2 = \left[\big(\realmu_1(\RInv_1(\xb^\star)) - \realmuhat_1^F(\xb^\star) \big) - \big( \realmu_0(\RInv_1(\xb^\star)) - \realmuhat_0^F(\xb^\star) \big)\right]^2 
$$

In the population setting, using the consistency of factual estimates, the CATE error reduces to $\left[ \realmu_0(\RInv_1(\xb^\star)) - \realmu_0^F(\xb^\star)\right]^2$. This error is zero when $\RInv_1(\xb^\star) = \RInv_0(\xb^\star)$. Thus, for \realonly\ to provide accurate CATE estimates, the treatment must not affect the post-treatment covariates, i.e., $g_0(z) = g_1(z)~ \forall z$ in which case their inverse are equal $f_0 = f_1$. However, this assumption is often unrealistic. For instance, in pharmacology, different drugs typically induce distinct effects on patient covariates, limiting the applicability of this model in such settings.

\textbf{Remark:} The post-treatment covariates $X$ can be viewed as a special case of the pre-treatment covariates $Z$ when the covariate-generating functions $\R_0 = \R_1$. In such cases, our proposed \our\ algorithm offers no distinct advantage, and existing CATE methods designed for pre-treatment covariates suffice and should be used instead.


\subsection{The \muonly\ Estimator}
\vspace{-0.2cm}
% We posit that the $(\RInv_0, \RInv_1)$ gap would be much larger than the $(\RInv_t,\SInv_t)$ gap for a good simulator.
Unlike \simonly, which uses $\synD$ to learn both $\RInvhat_t$ and $\realmuhat_t$, this approach leverages $\synD$ solely to learn the representation extractor $\RInvhat_t$. Specifically, it assumes that $\RInvhat_t = \SInvTilde_t$, as obtained from Eq. \ref{eq:cont_loss}.
% , \forall t \in \Tspace$. which is derived from $\synD$, to estimate pre-treatment covariates  $\hat{z}_i=\SInvCont_{t_i}(\xb_i)$. 
Thereafter, it learns the $\realmuhat_t$ parameters by applying a factual loss on $\trnD$ to estimate 
$$
\realmuhat_0, \realmuhat_1 = \argmin_{\{\realmuhat_0, \realmuhat_1\}} \sum_{\trnD}(y_i - \realmuhat_{t_i}(\SInvCont_{t_i}(\xb_i)))^2
$$
We call this method \muonly\ since it learns the outcome parameters $\realmu$ from real samples while learning representation extractor $\RInv_t$ from the simulator.

\textbf{CATE error:} One condition under which the  \muonly\ model achieves zero CATE error is when $\SInvTilde_t = \RInv_t$ for each treatment $t$. This requires that the simulator aligns with real-world covariates, specifically $\xb_t = g_t(z) = g_t^S(z) = \xb_t^S$. This limitation arises because the model learns the representation extractor solely from $\synD$, without making adjustments for real covariates. 


In summary, we described three possible CATE estimators and showed that each method would provide accurate CATE estimates under certain strong assumptions about the real and simulator DGPs. Given that none of these assumptions would hold in practice, we now turn to exploring a joint training framework that learns simultaneously from both real and simulated samples. 



