\section{Related Work}
We begin with an overview of approaches to training a generalist agent—a policy capable of solving a wide range of sequential decision-making tasks. Next, we discuss algorithms developed for cross-embodiment policy learning (CEPL) as the data setup is similar to ours. Finally, we review work focused on using diffusion models to learn control policies.

\subsection{Building generalist agents}

The GATO agent \cite{gato} represents the policy trained across the most diverse set of sequential decision-making tasks. It accomplishes this by mapping various action and observation spaces into tokens, which are then processed and predicted by a transformer architecture \cite{attention}. The effectiveness of this approach relies on the presence of generalizable patterns within the tokenized trajectory data, enabling positive transfer. A bottleneck in training general policies is the need to train policies from scratch which is computationally intensive. To address this, researchers have begun integrating multi-modal foundation models \cite{gpt_4,palm_e,pali_x} into their policies \cite{say_can,fm_seq_decision_making,rt2}. An example is the RT-2 model \cite{rt2}, which translates robotic actions into tokens and co-finetunes PaLI-X \cite{pali_x} and Palm-E \cite{palm_e} vision-language models using robotic trajectory data. Similarly, our adaptation of the universal policy approach can be combined with text-to-video foundation models \cite{sora_world_model1,imagen_video}.

\subsection{Cross Embodiment Policy Learning}

In robotics where data collection is costly, the community created increasingly larger datasets of task-trajectory pairs \cite{language_table,robonet,droid,open_e_dataset}. One possibility to scale the amount of available data quickly is to pool data over robotic agents with different embodiments. For example the Open X-Embodiment (OXE) dataset pools several existing robotic manipulation datasets \cite{open_e_dataset}. Learning over different embodiments comes with the challenge of learning a policy that needs to be able to handle different action and observation spaces.

While training agent-agnostic policies is not a new goal \cite{agent_agnostic_policy}, the embodiment gap has widened significantly, such that robot behaviour is even learned from human demonstrations \cite{xskill,learning_human_data1}. To address this challenge, two main approaches have emerged. The first maps different observation and action spaces of various embodiments into token sequences with a uniform structure, allowing them to be processed by a shared transformer \cite{octo_policy,cross_former}. The second focuses on learning a joint latent space that aligns different embodiments within a common embedding space, enabling policy learning over this latent representation \cite{xskill,latent_space_alignment}.

Our own data setup mirrors the OXE dataset as it pools agent-specific datasets from different agents. However, in the case of the OXE dataset, agents have varying observation spaces in addition to their differing action spaces. To learn a policy that can control a diverse set of agents we follow the universal policy approach~\cite{universal_policies} and frame policy learning as learning a generative model for observation sequences in combination with agent-specific inverse dynamics models. We further show that training a shared diffusion planner achieves greater positive transfer than imitation learning approaches with separate policy heads for each agent \cite{octo_policy,cross_former}.

% Adapting Foundation Models
% Vision-Language Models Provide Promptable Representations for Reinforcement Learnin
% Do As I Can, Not As I Say: Grounding Language in Robotic Affordances
% GenAug: Retargeting behaviors to unseen situations via Generative Augmentation
% PaLM-E: An Embodied Multimodal Language Model
% Visual Language Maps for Robot Navigation
% VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
% Language Models as Zero-Shot Trajectory Generators


% Approaches from Scratch
% ViNT: A foundation model for visual navigation
% Gnm: A general navigation model to drive any robot
% Robocat
% RT-X
% Octo
% Universal policies?


% Specialized Domains



\subsection{Policy Learning with Diffusion}

The recent success of diffusion models in learning generative models for images \cite{ldm,imagen}, audio \cite{audio_diffusion}, and videos \cite{imagen_video} has extended to policy learning \cite{diffuser,diffusion_policy,3d_diffusion_policy,universal_policies,seer}. These models can learn policies by either mapping states to action distributions \cite{diffusion_policy,3d_diffusion_policy} or modelling probability distributions over agent trajectories as sequences of state-action pairs \cite{diffuser}. In the latter approach, a trajectory is sampled based on a given task description and initial observation and then executed by the agent. Another approach proposed by \citet{universal_policies}, known as universal policies, learns a generative model over observation sequences conditioned on natural language instructions. Instead of generating action sequences directly, it generates observation sequences, which are then labelled with appropriate actions using an inverse dynamics model. This method not only outperforms imitation learning baselines \cite{rt1} and trajectory-based diffusion models \cite{diffuser} but also shows potential for integration with text-to-video foundation models. Additionally, it enhances interpretability by allowing inspection of the generated observation sequences. If agents share a common observation space, the diffusion-based planner for universal policies can be trained using trajectories from agents with varying capabilities. However, at test time, the generated plans must conform to the specific agent’s capability constraints. To the best of our knowledge, we are the first to explore diffusion-based generative modeling of observation sequences on datasets containing trajectories from agents with heterogeneous action spaces.