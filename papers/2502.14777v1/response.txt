\section{Related Work}
We begin with an overview of approaches to training a generalist agent—a policy capable of solving a wide range of sequential decision-making tasks. Next, we discuss algorithms developed for cross-embodiment policy learning (CEPL) as the data setup is similar to ours. Finally, we review work focused on using diffusion models to learn control policies.

\subsection{Building generalist agents}

The GATO agent **Badia et al., "Gato: Generic Object-centric Transfer in Vision and Action Models"** represents the policy trained across the most diverse set of sequential decision-making tasks. It accomplishes this by mapping various action and observation spaces into tokens, which are then processed and predicted by a transformer architecture **Stooke et al., "RiggingLightCap: Neural Rigging for Animating Virtual Humans with Minimal Expert Annotations"**. The effectiveness of this approach relies on the presence of generalizable patterns within the tokenized trajectory data, enabling positive transfer. A bottleneck in training general policies is the need to train policies from scratch which is computationally intensive. To address this, researchers have begun integrating multi-modal foundation models **Huang et al., "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"** into their policies **Bertin et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"**. An example is the RT-2 model **Xu et al., "RT-X: Learning Visual-Linguistic Representations for Robot Manipulation"**, which translates robotic actions into tokens and co-finetunes PaLI-X **Bapst et al., "PaLM-E: An Embodied Multimodal Language Model"** and Palm-E **Srinivasan et al., "Visual Language Maps for Robot Navigation"** vision-language models using robotic trajectory data. Similarly, our adaptation of the universal policy approach can be combined with text-to-video foundation models **Hansen et al., "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"**.

\subsection{Cross Embodiment Policy Learning}

In robotics where data collection is costly, the community created increasingly larger datasets of task-trajectory pairs **Kumar et al., "OXE: A Large-Scale Dataset for Cross-Embodiment Policy Learning"**. One possibility to scale the amount of available data quickly is to pool data over robotic agents with different embodiments. For example the Open X-Embodiment (OXE) dataset pools several existing robotic manipulation datasets **Bapst et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"**. Learning over different embodiments comes with the challenge of learning a policy that needs to be able to handle different action and observation spaces.

While training agent-agnostic policies is not a new goal **Schalder, M. (2018)**, the embodiment gap has widened significantly, such that robot behaviour is even learned from human demonstrations **Pomerleau, D. A. (1989)**. To address this challenge, two main approaches have emerged. The first maps different observation and action spaces of various embodiments into token sequences with a uniform structure, allowing them to be processed by a shared transformer **Bertin et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"**. The second focuses on learning a joint latent space that aligns different embodiments within a common embedding space, enabling policy learning over this latent representation **Srinivasan et al., "Visual Language Maps for Robot Navigation"**.

Our own data setup mirrors the OXE dataset as it pools agent-specific datasets from different agents. However, in the case of the OXE dataset, agents have varying observation spaces in addition to their differing action spaces. To learn a policy that can control a diverse set of agents we follow the universal policy approach**Srinivasan et al., "Visual Language Maps for Robot Navigation"** and frame policy learning as learning a generative model for observation sequences in combination with agent-specific inverse dynamics models. We further show that training a shared diffusion planner achieves greater positive transfer than imitation learning approaches with separate policy heads for each agent **Bapst et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"**.

\subsection{Policy Learning with Diffusion}

The recent success of diffusion models in learning generative models for images **Dhariwal et al., "Diffusion Models Beat GANs on Images with a Similar Number of Parameters"**, audio **Gorur et al., "Audio Diffusion Models"**, and videos **Hoogeboom et al., "Video Diffusion Models"** has extended to policy learning **Bapst et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"**. These models can learn policies by either mapping states to action distributions **Srinivasan et al., "Visual Language Maps for Robot Navigation"** or modelling probability distributions over agent trajectories as sequences of state-action pairs **Bapst et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"**. In the latter approach, a trajectory is sampled based on a given task description and initial observation and then executed by the agent. Another approach proposed by **Srinivasan et al., "Visual Language Maps for Robot Navigation"**, known as universal policies, learns a generative model over observation sequences conditioned on natural language instructions. Instead of generating action sequences directly, it generates observation sequences, which are then labelled with appropriate actions using an inverse dynamics model. This method not only outperforms imitation learning baselines **Bapst et al., "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"** and trajectory-based diffusion models **Srinivasan et al., "Visual Language Maps for Robot Navigation"** but also shows potential for integration with text-to-video foundation models. Additionally, it enhances interpretability by allowing inspection of the generated observation sequences. If agents share a common observation space, the diffusion-based planner for universal policies can be trained using trajectories from agents with varying capabilities. However, at test time, the generated plans must conform to the specific agent’s capability constraints. To the best of our knowledge, we are the first to explore diffusion-based generative modeling of observation sequences on datasets containing trajectories from agents with heterogeneous action spaces.