% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[table]{xcolor}
% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
% Use textalpha for Cyrillic text
\usepackage[T2A,T1]{fontenc}
\usepackage{highlight}
\usepackage{highlighta}
\usepackage[utf8]{inputenc}
\usepackage{dirtytalk}
\usepackage{longtable}
\usepackage{highlightuk}
\usepackage{listings}
\usepackage{multirow}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{arydshln} % for dashed/dotted lines
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{scalerel}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage{inconsolata}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs, tabularx}
\usepackage{tcolorbox}
\renewcommand{\texttt}[1]{%
  \begingroup
  \ttfamily
  \begingroup\lccode`~=`/\lowercase{\endgroup\def~}{/\discretionary{}{}{}}%
  \begingroup\lccode`~=`[\lowercase{\endgroup\def~}{[\discretionary{}{}{}}%
  \begingroup\lccode`~=`.\lowercase{\endgroup\def~}{.\discretionary{}{}{}}%
  \catcode`/=\active\catcode`[=\active\catcode`.=\active
  \scantokens{#1\noexpand}%
  \endgroup
}
\newcommand{\model}[1]{{\ttfamily #1}}

\newcommand{\cyrillic}[1]{%
        \fontencoding{T2A}\selectfont
        #1%
        \fontencoding{T1}\selectfont
}
\newcommand{\tn}[1]{\textcolor{blue}{\bf\small [#1 --TN]}}
\newcommand{\al}[1]{\textcolor{orange}{\bf\small [#1 -- AL]}}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\CC}[1]{%
  \raisebox{-0.3ex}{%
    \scalerel*{\includegraphics{figures//flags/#1.png}}{\scalebox{1.35}{\textbf{B}}}%
  }%
}
\newcommand{\benchmarkname}[1]{{\textsc{#1}\textsc{BORSch}}}

\title{What are Foundation Models Cooking in the Post-Soviet World? \\
}

\author{Anton Lavrouk, Tarek Naous, Alan Ritter, Wei Xu \\
  Georgia Institute of Technology \\
  \small{
 \texttt{\{antonlavrouk, tareknaous\}@gatech.edu; \{alan.ritter, wei.xu\}@cc.gatech.edu}}}

\begin{document}
\maketitle


\begin{abstract}

The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. 
In this study, we investigate the \textit{Post-Soviet cultural food knowledge} of foundation models by constructing \benchmarkname{}, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region.
We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in.
Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. 
Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make \benchmarkname{} publicly available at \url{https://github.com/alavrouk/BORSch}. 

\end{abstract}

\section{Introduction}

The Post-Soviet states have long held their own cultural and linguistic identities. During the Soviet era, these identities were pressured through forced assimilation under the Russian language and culture \cite{silver1974social}. Now, 33 years after the collapse of the Soviet Union, the Post-Soviet world continues to repair the damage inflicted by this so-called \say{Sovietization} \cite{Rutland_2023}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/figure1.pdf}
    \caption{A map of predictions from various popular consumer-facing models for the dish \cyrillic{кывырма} (\textit{kivirma}), a traditional pastry from the Moldovan region of Gagauzia, which is home to a significant Russian-speaking population. The models were prompted in Russian, with the English translation also shown.}
    \vspace{-0.2in}
    \label{fig:example}
\end{figure}

As foundation models continue to gain prominence, it is important that they are able to represent each Post-Soviet state. 
Yet, when examined via \textit{food dishes}, an important element in every culture \cite{anderson2014everyone}, we find that they lack crucial knowledge. 
For example, \cyrillic{кывырма} (pronounced \say{ky-vyr-MA}) is a dish from Gagauzia, a region of Moldova where Russian is commonly spoken \cite{mayer2014gagauz}. 
However, Figure~\ref{fig:example} shows that when asked in Russian, multilingual models fail to identify the origins of this dish, with each one predicting a different Post-Soviet nation.

In order to further investigate these deficiencies, we conduct a detailed exploration of food culture understanding in foundation models across both text and image modalities. We focus our study on the Russian and Ukrainian languages, analyzing how the push of Ukrainian \say{de-Sovietization} \cite{boman2023coexistence} and the pull of historical Russian interference on Ukrainian culture \cite{boychuk2023effect} impacts the cultural perceptions of foundation models. Overall, our contributions are as follows:

\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt]

    \item We construct \benchmarkname{}\footnote{Named after the famous Ukrainian dish, \cyrillic{борщ}.} (\textbf{B}enchmark \textbf{O}f \textbf{R}egional di\textbf{S}hes), a dataset for evaluating foundation models on multimodal food culture understanding in Russian and Ukrainian (\S\ref{sec:dataset}). \benchmarkname{} is constructed via a bootstrapped entity extraction approach for collecting culturally relevant food dishes from web crawl corpora, with human-in-the-loop validation (\S\ref{subsec:bootstrapping}).

    \item In order to compare models in Russian and Ukrainian, we perform text and vision country of origin Question Answering (QA) using dishes in \benchmarkname{}. On top of varying performance across countries, we find that models in these languages over-predict their respective countries of origin (\S\ref{subsec:parallel_qa}). 

    \item To gain a more nuanced understanding of how models perform on Post-Soviet dishes in Ukrainian, we examine how the Russian-Ukrainian pidgin\footnote{A pidgin is a simplified language for communication between people with different native tongues \cite{romaine2017pidgin}.} \textit{surzhyk} influences Ukrainian QA and VQA performance (\S\ref{subsec:surzhyk}).
    
    \item Through pretraining data analysis, we find many instances where \benchmarkname{} dishes co-occur with non-origin countries, harming model QA performance. In contrast to English corpora, these issues in Russian and Ukrainian stem from poor web-scraping (\S\ref{subsec:pretraining}).

    \item Finally, we conduct an experiment which queries models for descriptions of dishes in \benchmarkname{}, which we then evaluate using a modality transition from text to image. We find this to be a challenging task with limited correlation to QA experiments (\S\ref{sec:descriptions}). 
\end{itemize}

\section{Related Work}
\label{sec:related_work}

\paragraph{Cultural Knowledge Bases.} 
Recent interest regarding cultural-knowledge in foundation models has led to numerous studies attempting to quantify it \cite{hershcovich2022challengesstrategiescrossculturalnlp, adilazuarda-etal-2024-towards, liu2024culturallyawareadaptednlp}. Some studies construct multilingual knowledge bases of cultural assertions (e.g., \textit{\say{In Bhutan, there is a tradition of wearing "Khyenkhor Robes" woven with threads infused with blessings from Buddhist monks}}) \cite{Nguyen_2023, Nguyen_2024, fung2024massivelymulticulturalknowledgeacquisition}. Other works craft benchmarks of culturally-specific questions (e.g., \textit{\say{What is the story of the series Al-Manassa?}}) \cite{yin2022geomlama, myung2024blendbenchmarkllmseveryday, shen-etal-2024-understanding, arora2024calmqaexploringculturallyspecific}. Further research expands on such directions to support multimodality \cite{ramaswamy2023geodegeographicallydiverseevaluation, liu2023cultural}. There are also additional studies which focus exclusively on vision-based tasks such as culturally informed image generation \cite{bhatia2024local, karamolegkou2024vision, kannen2024beyond}, visually grounded reasoning \cite{schneider2024m}, and image transcreation \cite{khanuja2024image}. While some of these works include food as part of their overall assessment, they mainly focus on broad cultural understanding. Meanwhile, we offer a more in-depth analysis on the nuances of cultural \textit{food} knowledge.

\paragraph{Cultural Food Knowledge.}
Food knowledge is a key element of culture, and is thus frequently evaluated in foundation models. Some studies assess model comprehension of culinary practices or dishes through pragmatic questioning (e.g., "\textit{While eating, when does one drink Cantonese seafood soup?}") \cite{palta2023fork, yao2023benchmarking, putri2024can, li2024foodieqamultimodaldatasetfinegrained}. Another line of reasoning uses food to attribute cultural generations to pretraining data \cite{li2024attributingcultureconditionedgenerationspretraining}.
Finally, a group of works measures food culture understanding in foundation models by testing them on a culturally diverse set of food-dish entities. However, the food-dishes used for evaluating models in past work are obtained solely from either Wikidata \cite{zhou2024does} or Wikipedia \cite{winata2024worldcuisines}, which we show leads to missing out on many culture-specific dishes in non-English languages (\S\ref{subsec:wikidata}). For example, the food-dishes originating from Russia and Ukraine in the resource constructed by \citet{winata2024worldcuisines} cover only 20.8\% of the dishes originating from Russia and Ukraine that we provide in \benchmarkname{}.
% for world cuisines:
% 5 dishes we do not have in our dataset
% 23 we do not agree with their categorization as a dish or their labelled origin country
% the other 57 of their dishes are present in our dataset
% We have 298 dishes originating from these countries

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.333\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/figure2a.pdf}
        \caption{}
        \label{fig:figure2a}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.666\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/figure2b.pdf}
        \caption{}
        \label{fig:figure2b}
    \end{subfigure}
\caption{
        \textbf{(a)} Frequency in \model{mC4} vs frequency rank in \benchmarkname{}. Culturally relevant bootstrapped dishes are both common and long-tail, while Wikidata dishes are less frequent overall. \textbf{(b)} Countries of origin of dishes in \benchmarkname{}, which were obtained from multilingual Wikidata  (\S\ref{subsec:wikidata}) and commonly used web-crawled corpora (\S\ref{subsec:bootstrapping}). While there are less bootstrapped dishes, they are more likely to originate from a Post-Soviet nation.
    }
    \vspace{-0.3cm}
    \label{fig:map_comparisons}
\end{figure*}

\paragraph{Russian and Ukrainian Culture in LLMs.} 
Existing cultural studies on the Russian language in foundation models focus predominantly on social and gender biases \cite{grigoreva2024rubia, km, li2024uncoveringdifferencespersuasivelanguage}. Additionally, \citet{kharchenko2024llmsrepresentvaluescultures} explores cultural values of foundation models in Ukrainian, among other languages. Limited attention is given to cultural knowledge exploration in Russian and Ukrainian. From our understanding, our study is the first to explore cultural food knowledge in foundation models tailored to either one of these languages. 

\section{Constructing \benchmarkname{}}
\label{sec:dataset}

To enable a more in-depth assessment of models' understanding of Russian and Ukrainian food cultures, we focus on collecting both the popular and less commonly known dishes relevant to those cultures. We achieve this by first extracting all available dishes in Wikidata (\S\ref{subsec:wikidata}), then expanding on this initial set through a bootstrapped extraction approach from web-crawled data with human-in-the-loop (\S\ref{subsec:bootstrapping}). We also annotate countries of origin (\S\ref{subsec:annotation}), collect images for each food dish (\S\ref{subsec:images}), and create a Post-Soviet, parallel sub-dataset of dishes, enabling a variety of multimodal evaluations.

\subsection{Extracting Food Entities from Wikidata}
\label{subsec:wikidata}
As a starting point, we acquire an initial list of dishes from the Wikidata multilingual knowledge base\footnote{\url{www.wikidata.org/wiki/Wikidata:Main_Page}} in the Russian and Ukrainian languages. 
We extract all entities that are registered under the class \say{\texttt{food}} in Wikidata, which encompasses many food-related sub-classes (e.g., \textit{sweets}, \textit{fast food}, etc.). We then manually select culturally relevant food dishes that are attributable to a specific country/countries of origin, and discard beverages and more generic food entities (e.g., globally common dishes such as \textit{grilled chicken}, branded goods such as \textit{kitkat}, etc.). The resulting coverage of food entities in Wikidata is relatively poor for both Russian (676 dishes) and Ukrainian (415 dishes) languages. 
Moreover, only 119 dishes (17.6\%) in Russian Wikidata and 81 dishes (19.5\%) in Ukrainian Wikidata are associated with origins in any Post-Soviet state, which indicates that the existing coverage is not only sparse but also lacks cultural relevance. We address this multilingual coverage gap in Wikidata by collecting additional dishes from web-crawl data.

\subsection{Bootstrapped Extraction from Corpora}
\label{subsec:bootstrapping}

Previous research by \citet{naous-etal-2024-beer} demonstrated that culturally-relevant food dishes can be collected from large web-crawl corpora. Their approach relies on extracting unigrams and bigrams appearing after a set of manually crafted patterns which likely occur before the mention of a food dish (e.g., \textit{recipe of \rule{0.65cm}{0.15mm}}, \textit{how to cook \rule{0.65cm}{0.15mm}} , etc.). This was followed by human annotation to filter out erroneous extractions. We build on this method by performing bootstrapped pattern-based extraction with a human-in-the-loop to iteratively collect food dishes from the Russian and Ukrainian portions of \model{mC4} web-crawl corpora \cite{xue-etal-2021-mt5}.
  
We start with the dishes obtained from Wikidata (\S \ref{subsec:wikidata}) as a seed list, which we use to search the corpus of each respective language and retrieve all 3-gram and 4-gram patterns that precede any dish.
We then ask a human annotator to select 5 from the 100 most frequent patterns.
Using the selected patterns, we search the corpus again and extract every unigram that appears after a 3-gram pattern and every bigram that appears after a 4-gram pattern.
Finally, we de-duplicate the extracted unigrams and bigrams, which results in up to 10k extractions that we give to human annotators to manually filter for food dishes. We repeat this bootstrapping process for two more rounds. Detailed statistics regarding extractions during the bootstrapping process are located in Appendix~\ref{appendix:bootstrap_stats}.

\subsection{Determining a Dish's Country of Origin}
\label{subsec:annotation}

In order to enable the evaluation of models' food culture understanding, we annotate each collected dish for its associated country/countries of origin. Two college educated annotators, one fluent in Russian and one fluent in Ukrainian, conducted independent research using web resources on each dish and manually labeled each dish's country of origin. In cases where dishes were found to have multiple countries of origin (20\% of dishes in Russian, 27\% in Ukrainian), particularly for areas that predate modern country borders, annotators were asked to label all relevant countries. An example is \cyrillic{чак-чак} (\textit{chak-chak}), a popular cake predating the Soviet Union which originates from Central Asia. Its origins were labeled as Russian (Tatar and Bashkir)\footnote{These are two minority ethnic groups in Russia.}, Kazakh, Tajik, Kyrgyz, and Uzbek as it is a common delicacy in all of those nations. 

Figure~\ref{fig:figure2b} compares the origins of food dishes extracted from Wikidata (\S \ref{subsec:wikidata}) vs. the bootstrapped process (\S \ref{subsec:bootstrapping}) on the map. We find that bootstrapping retrieves more dishes that are common in the Post-Soviet region. Furthermore, as shown in Figure~\ref{fig:figure2a}, the bootstrapping process helps cover Post-Soviet dishes in Russian and Ukrainian that are both highly frequent and long-tail in corpora, while dishes obtained from Wikidata consist of mostly popular native dishes. 

\subsection{Dish Image Collection}
\label{subsec:images}

To facilitate vision-languages analyses, we collect up to 5 images for each dish in \benchmarkname{}. We first searched for images in Wikimedia Commons\footnote{\url{commons.wikimedia.org/wiki/Main_Page}}, a collection of freely usable media files. This process enabled image retrieval for 74\% of dishes in Russian and 68\% in Ukrainian. For the remainder of the dishes which did not have images in Wikimedia Commons, we used the Google Custom Search API\footnote{\url{developers.google.com/custom-search/v1/}} and queried for images with a Creative Commons (CC) open license. All retrieved images were then manually filtered to remove irrelevant content (see filtering interface in Appendix~\ref{appendix:image_annot}). In total, we collected 5285 images in Russian (3.64 images per dish on average), and 2907 images in Ukrainian (3.53 images per dish on average).

 \begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/figure3.pdf}
    \caption{Confusion matrices for country-of-origin QA/VQA on dishes in \benchmarkname{}. Models exhibit a low recall on \CC{RU} Russian and \CC{UA} Ukrainian dishes, and struggle with Post-Soviet countries in the Caucasus (\CC{AM} Armenia, \CC{AZ} Azerbaijan, \CC{GE} Georgia) and Central Asia (\CC{KZ} Kazakhstan, \CC{KG} Kyrgyzstan, \CC{TJ} Tajikistan, \CC{UZ} Uzbekistan). \CC{EE} Estonia, \CC{LV} Latvia, and \CC{TM} Turkmenistan are excluded due to low sample size, and the confusion matrices are row (truth) normalized, as each origin country has a different number of dishes in \benchmarkname{}.}
    \vspace{-0.2cm}
    \label{fig:qa_and_vqa}
\end{figure*}

\subsection{Aligning Russian \& Ukrainian \benchmarkname{}}
\label{subsec:parallel_construction}
To enable direct comparisons between languages, we manually translate (and transliterate, when necessary) dishes originating from Post-Soviet countries in the Russian set to Ukrainian and vice-versa, resulting in a sub-dataset of 433 dishes with names in both Russian and Ukrainian. Of these dishes, 174 (40\%) appear in both datasets, while 126 (29\%) are unique to the Russian dataset and 133 (31\%) are unique to the Ukrainian dataset. The distribution of origins for the parallel sub-dataset can be found in Appendix~\ref{appendix:parallel_dist}. We validated the dish origin annotations in this parallel sub-dataset by engaging a second annotator, achieving substantial agreement with a Cohen's Kappa ($\kappa$) of 0.879. Furthermore, we note that each dish in this sub-dataset can now have up to 10 images (if it was originally a part of both Russian and Ukrainian \benchmarkname{}). 

\section{LLM Performance: Dish QA \& VQA}
\label{sec:four}

To begin, we test foundation models in Russian and Ukrainian on QA and VQA tasks focusing on dish origins within the parallel sub-dataset of \benchmarkname{} (\S\ref{subsec:parallel_qa}). Then, to gain a deeper understanding of these results, we explore the effect of Russian code mixing on Ukrainian QA and VQA (\S\ref{subsec:surzhyk}). Finally, we investigate dish-country co-occurrences in the underlying pretraining data as a factor influencing both Russian and Ukrainian QA (\S\ref{subsec:pretraining}).

\subsection{Parallel Country of Origin QA \& VQA}
\label{subsec:parallel_qa}
We first assess models' ability to predict a dish's country/countries of origin in two setups: \textbf{(i)} standard text-based Question Answering (QA) where the model is provided the dish name and \textbf{(ii)} Visual Question Answering (VQA) where the model is provided an image of the dish. We use the \textbf{Post-Soviet parallel sub-dataset} of \benchmarkname{}, so that we can make fair comparisons between countries.

\paragraph{Setup.}
We evaluate \model{Qwen2-72B-Instruct} \allowbreak\cite{qwen2} and \model{Llama-3.1-70B-Instruct} \cite{dubey2024llama} on text-only QA tasks, and their vision-enabled counterparts \model{Qwen2\allowbreak-VL-72B-Instruct} and \model{Llama-3.2-90B-Vision\allowbreak-Instruct} for VQA tasks. We prompt these models with open-ended questions to align with real-world applications \cite{rottger-etal-2024-political}. To identify countries in each model's response, we use \model{spaCy}'s multilingual NER tool, known to be highly effective for location recognition \cite{Honnibal_spaCy_Industrial-strength_Natural_2020}. After extracting named entities, we search them for country names or aliases (e.g., \textit{Czech Republic} vs. \textit{Czechia}). This two-step approach allows us to detect any missing aliases by manually analyzing the left over named entities.
We present five different question variants which have placeholders for dish names (see Appendix~\ref{appendix:prompts}) and attach all (up to 10) available images to each VQA prompt. 

For evaluation, as each dish may have a varied number of countries as its origin, we calculate the Jaccard score per dish. The Jaccard score \cite{jaccard1901etude} measures the similarity between the predicted and ground truth country of origin sets as the size of their intersection divided by the size of their union. Importantly, this \textit{set overlap} Jaccard score accounts for countries predicted by the model which are not part of the gold set. 

\paragraph{Results.} Confusion matrices for the country-of-origin QA and VQA experiments are presented in Figure~\ref{fig:qa_and_vqa}. Overall, models demonstrate difficulty with dishes from the Caucasus and Central Asia. 
Furthermore, we find that models in Russian and Ukrainian frequently over-predict Russia and Ukraine as dish origins. In Figure~\ref{fig:qa_and_vqa}, this is evident from the wide distributions in the Russian and Ukrainian prediction columns for all experiments.

We investigate this further by examining dishes whose origins include both Russia and Ukraine (35\% of the parallel corpus). Figure~\ref{fig:pies} shows that models in Russian are more likely to predict Russia as an origin for these dishes, while models in Ukrainian are more likely to predict Ukraine. Generally, this is more of an issue for models in Russian, and affects QA more than VQA. 
Lastly, we report additional results in Appendix~\ref{appendix:vqa_with_name} where we provide models with the dish name (in addition to images) for the VQA task, which we find improves model performance on most dishes. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pies.pdf}
    \caption{{For dishes originating from both \CC{RU} Russia and \CC{UA} Ukraine, Russian models tend to predict Russia more often than Ukraine, and Ukrainian models tend to predict Ukraine more often than Russia. The proportions shown exclude non-Russian/Ukrainian predictions.}}
    \vspace{-0.2cm}
    \label{fig:pies}
\end{figure}


\subsection{Impact of Code Mixing on QA \& VQA} 
\label{subsec:surzhyk}

One potential factor influencing model QA/VQA performance in Ukrainian is Russian code mixing. In the period following Ukraine's establishment as a sovereign state in 1991, many Russian speakers in Ukraine transitioned to speaking the Ukrainian language \cite{fomenko2023brand}.
Understandably, this linguistic adjustment was not instant, which is why \textit{surzhyk}, referring to various mixed Russian-Ukrainian codes, has since gained a foothold in Ukraine \cite{doi:10.1177/00220221241256322}. For example, in surzhyk, sentences that are otherwise fully Ukrainian often incorporate Russian words, particularly nouns \cite{podolyan2005ukrainians}. 

On the other hand, the reverse phenomenon is far less common. Using \model{mC4} as a representative corpus of text data (more on this later), we find that Ukrainian code mixing accounts for 17\% of \benchmarkname{} dish occurrences in Russian \model{mC4}, which is far less than Russian dish occurrences in Ukrainian \model{mC4} (41\%).
To study surzhyk in pretraining corpora, we use \benchmarkname{}, particularly since the dish names in our parallel Ukrainian-Russian sub-dataset only differ by an average edit distance of 2.3 characters. This similarity makes the dishes in \benchmarkname{} especially susceptible to code mixing (e.g., Ukrainian \cyrillic{пир}\textbf{i}\cyrillic{г} versus Russian \cyrillic{пир}\textbf{o}\cyrillic{г})\footnote{Directly translates to \say{pie,} but colloquially represents a specific class of Eastern European pastries.}.

\paragraph{Setup.}
Using the Russian/Ukrainian dish names in the parallel sub-dataset of \benchmarkname{} (\S~\ref{subsec:parallel_construction}), we search for instances of Russian code-mixing in Ukrainian corpora. In particular, we search \model{mC4}, the most widely used and extensively studied open multilingual corpus for pretraining \cite{kreutzer2022quality}. While more recent corpora contain newer, higher quality data, they do not contain multilingual components, which are crucial for our analysis.
We then quantify surzhyk by analyzing the \textit{difference in mC4 occurrences of the Ukrainian dish name and the Russian dish name}.
Additionally, to enable the analysis of surzhyk in VQA, we modify the VQA experiment by asking the model to \textit{name} a dish given its image. We evaluate model performance on this experiment using \textit{exact match accuracy over 5 prompts} with a tolerance of 1 edit (Levenshtein) distance. While exact match is a common QA metric \cite{rajpurkar2016squad}, we introduce the 1 edit distance tolerance due to Russian and Ukrainian declension, where nouns change their endings to reflect their case, number, and sometimes gender \cite{press2015ukrainian, comrie2018russian}. Full, per-country results for this experiment can be found in Appendix~\ref{appendix:dish_vqa}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.975\linewidth]{figures/figure7.pdf}
    \caption{Dish origin QA and dish name VQA in Ukrainian on Qwen and Llama suffer when dish names are not standardized in Ukrainian corpora, which can occur due to the use of \textit{surzhyk} (a mixed Russian-Ukrainian code). Dishes with identical Russian and Ukrainian names are excluded from this analysis.}
    \vspace{-0.2cm}
    \label{fig:contamination}
\end{figure}

\paragraph{Results.} 
We begin by calculating the average QA Jaccard score and VQA exact match accuracy across nine log spaced bins of occurrence differences (\#Ukrainian - \#Russian) in Ukrainian \model{mC4}. The results are located in Figure~\ref{fig:contamination}, and we note that this analysis can only be done on dishes with different Russian and Ukrainian names (66.5\% of the parallel dataset). 
For dish origin QA, we observe the poorest performance when the pretraining data contains an approximately equal mix of Russian and Ukrainian names, while dishes standardized to either a Russian or Ukrainian name perform better. This trend is best exhibited by Qwen, while Llama exhibits it in a more subdued fashion.
Similarly, for dish name VQA, we find that it is important for the dish name to be standardized, although preferably using the Ukrainian name.

\subsection{Impact of Co-Occurrences on QA}
\label{subsec:pretraining}

Previously, we demonstrated that QA performance in Ukrainian can be affected by Russian code mixing (\S\ref{subsec:surzhyk}). We now turn our attention to \textbf{incorrect dish-country co-occurrences} in both Russian and Ukrainian pretraining data.

\paragraph{Setup.} To start, we analyze how frequently each \benchmarkname{} dish (full dataset) appears alongside country names (and their aliases) in the Russian and Ukrainian subsets of \model{mC4}. For each dish, we define the \textbf{correct co-occurrence count} as the number of documents mentioning both the dish and its country of origin, and the \textbf{correct co-occurrence ratio} as the proportion of a dish's co-occurrences with any country where its true origins are mentioned. We then examine how these metrics impact Llama and Qwen's country of origin QA performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/figure9.pdf}
    \caption{While a higher number of correct dish-country co-occurrences (\model{\#Correct Co-occ.}) supports better text-only QA performance, the ratio of correct v.s. total co-occurrences (\model{\#Correct Co-occ./\#Total Co-occ.}) proves even more crucial in both Llama and Qwen.}
    \vspace{-0.2cm}
    \label{fig:freq_coc}
\end{figure}

\paragraph{Results.} Figure~\ref{fig:freq_coc} shows text-only QA performance for our two models averaged over dishes which are grouped into 10 log-spaced bins based on their correct co-occurrence counts or ratios. We find an improvement in text-only QA across both languages as dishes occur more frequently with the correct country in pretraining data. More importantly, the improvement  in Jaccard score is steeper when looking at the correct co-occurrence ratio, indicating that it is critical for a dish not to co-occur frequently with incorrect countries of origin.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/qual_ana.pdf}
    \caption{An inspection of 400 randomly sampled incorrect dish-country co-occurrences in English, Russian, and Ukrainian \model{mC4} reveals that Russian and Ukrainian data suffers disproportionately from poor web scraping.}
    \vspace{-0.2cm}
    \label{fig:qualitative_analysis}
\end{figure}

\paragraph{Qualitative Analysis.}
Furthermore, we also seek to identify why food dishes co-occur with countries irrelevant to their origin to begin with. To answer this, we sample 400 sentences in each language where dishes co-occur with countries other than their true origin. To ensure a more informative sample, we only allow a unique dish to be selected three times.
Finally, to see if incorrect co-occurrences differ between Russian/Ukrainian and English languages, we extract English Wikidata dishes (\S\ref{subsec:wikidata}), annotate their origins (\S\ref{subsec:annotation}), and sample incorrect co-occurrences for these dishes as well. This results in 2348 total dishes.

Figure~\ref{fig:qualitative_analysis} shows the distribution of incorrect co-occurrence cases across the Russian, Ukrainian, and English languages. We find that Russian and Ukrainian corpora suffer greatly from web scraping errors, while the English corpus does not. This quality disparity is supported by similar observations in \citet{kreutzer2022quality}. Other notable issues include word polysemy (previously studied in \citealt{naous2025origin}), irrelevant geographic mentions, inaccurate dish origins, and incidental occurrences where a dish and a country appear together but are unrelated (miscellaneous).

\section{Dish Description Generation}
\label{sec:descriptions}

Finally, we introduce a new generation-based task for food cultural understanding that goes beyond conventional QA setups. In particular, we focus specifically on a model's ability to describe the appearance of a dish in the \benchmarkname{} parallel dataset. 

\paragraph{Setup.}
We begin by prompting the models to produce textual descriptions of a dish's appearance given its name. As in \S\ref{subsec:parallel_qa} and \S\ref{subsec:surzhyk}, we use the dishes in the parallel subdataset of \benchmarkname{}. 
To assess the accuracy of a generated description, we first translate it to English\footnote{\url{cloud.google.com/translate}}. Next, given this description, we generate an image of the dish using \model{FLUX.1-dev}\footnote{\url{huggingface.co/black-forest-labs/FLUX.1-dev}, currently ranked \#1 on GenAI Arena \cite{jiang2024genaiarenaopenevaluation}.}, which we then compare to real photos of the dish (\S\ref{subsec:images}). We note that we remove any mention of the dish name from the generated descriptions. This ensures that our evaluation focuses solely on the text-to-text model while \model{FLUX.1-dev} only acts as a cross-modality bridge.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ig_results.pdf}
    \caption{Models in Russian are more well equipped to describe Post-Soviet, culturally relevant dishes compared to models in Ukrainian.}
    \vspace{-0.2cm}
    \label{fig:ig_results}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/scatter_qwen.png}
\caption{The Spearman's $\rho$ between dish-description performance and QA tasks on \textbf{Qwen} models shows a small, positive correlation. Llama models show a similar trend (Appendix~\ref{appendix:ig_correlations_llama}). All axes span from 0 to 1.}
    \vspace{-0.2cm}
    \label{fig:ig_correlations}
\end{figure}

\paragraph{Evaluation.}
We measure similarity between the generated and ground-truth images using \model{DiNOv2-giant} \cite{oquab2023dinov2}, a Vision Transformer image encoder trained through self-supervision \cite{dosovitskiy2021imageworth16x16words}. Following the approach used by \model{DiNOv2}'s creators and recent work by \citet{khanuja2024image}, we extract \model{[CLS]} token encodings from both the generated and ground-truth images, then compute their cosine similarity.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/new_ig_qa.png}
    \caption{Our dish description evaluation pipeline for the dish \textit{pilaf}. \model{Llama-3.1} generates a valid, detailed description (which we abridge to include key points).}
    \vspace{-0.2cm}
    \label{fig:ig_qa}
\end{figure}

\paragraph{Results.} Figure~\ref{fig:ig_results} presents a heat map of the average cosine similarities for each language-region pair. Additionally, Figure~\ref{fig:ig_qa} displays a qualitative example of our image description evaluation pipeline. More examples spanning many cosine similarities can be found in Appendix~\ref{appendix:more_examples}.
Overall, we find that Russian models outperform Ukrainian models, even on dishes originating from Ukraine. 
To ensure the veracity of this evaluation pipeline, we perform human evaluation on a random sample of 100 dish descriptions per language and model, where an annotator rates the quality of the descriptions from 0 to 1 given the ground truth images. The Pearson correlation coefficient ($r$) between the human ratings and the cosine similarities were 0.78 in Russian and 0.82 in Ukrainian for Qwen, and 0.81 in Russian and 0.82 in Ukrainian for Llama. Additional details, such as alternative correlation measures, are located in Appendix~\ref{appendix:humaneval}. 

Finally, we present a scatter plot of encoding cosine similarities vs. models' QA performances (\S\ref{subsec:parallel_qa}, \S\ref{subsec:surzhyk}) in Figure~\ref{fig:ig_correlations}. We observe weak positive correlations, indicating that our two designed evaluations are complementary, each capturing different aspects of cultural food knowledge. Further details regarding this claim are located in Appendix~\ref{appendix:description_ablation}.

\section{Conclusion}

We present \benchmarkname{}, a dataset targeted at evaluating cultural food knowledge in the Russian and Ukrainian languages. Through \benchmarkname{}, we identify significant gaps in model knowledge and investigate pretraining data to uncover the causes of these shortcomings. We hope our insights into incorrect co-occurrences and language contamination in pretraining data will contribute to building more culturally aware models and pretraining corpora.

\section*{Limitations}

First, while our study focused on the Russian and Ukrainian languages, there are other Post-Soviet languages and corresponding countries, each with their own histories and food-cultures. Future work can look to expand in their direction. For new languages, language specific phenomena may affect performance on culturally relevant tasks, similar to surzhyk in Ukrainian.

Furthermore, we annotated and conducted QA solely on each dish's country of origin. However, dishes have many other characteristics worth exploring, such as taste or smell. Our dataset can be expanded to include these traits through additional annotation, and performing QA on these characteristics is straightforward with modified prompts.\

Finally, we note that our study of the interaction between the Russian and Ukrainian languages uses text data from 2019 (\model{mC4}). This is three years before Russia's full scale invasion of Ukraine, which has rapidly changed people's attitudes towards the two languages \cite{kulyk2024language}. Future work can focus on providing similar analyses as ours, but on a novel Russian/Ukrainian corpus constructed and filtered from newer versions of the Common Crawl.

\section*{Acknowledgments}

The authors would like to thank Oleksandr Lavreniuk, Dennis Pozhidaev, and Jad Matthew Bardawil for their valuable discussion and annotation; Kartik Goyal for their valuable discussion.

\bibliography{references}
\bibliographystyle{acl_natbib}

\clearpage

\appendix

\begin{table}[h]
    \centering
    \begin{tabular}{ll S[table-format=5.0] S[table-format=5.0]}
        \toprule
        \multicolumn{1}{c}{\textbf{Round}} & \multicolumn{1}{c}{\textbf{Metric}} & \multicolumn{1}{c}{\textbf{RU}} & \multicolumn{1}{c}{\textbf{UK}} \\
        \midrule
        \multirow{3}{*}{\textbf{Round 1}} 
          & \#Seeds        &  688  &  416  \\
          & \#Extractions  & 6409  & 6462  \\
          & \#Dishes       &  371  &  386  \\
        \midrule
        \multirow{3}{*}{\textbf{Round 2}} 
          & \#Seeds        &  371  &  386  \\
          & \#Extractions  &11724  & 5442  \\
          & \#Dishes       &  195  &   92  \\
        \midrule
        \multirow{3}{*}{\textbf{Round 3}} 
          & \#Seeds        &  195  &   92  \\
          & \#Extractions  & 3429  & 3440  \\
          & \#Dishes       &   44  &  179  \\
        \bottomrule
    \end{tabular}%
    \caption{The \textbf{\#Seeds} (initial dishes given to the algorithm), \textbf{\#Extractions} (total algorithm extractions), and \textbf{\#Dishes} (total dishes in the extractions) for each round of bootstrapping in Russian and Ukrainian.}
    \label{tab:bootstrap}
\end{table}

\section{Bootstrapping Statistics}
\label{appendix:bootstrap_stats}

In Table~\ref{tab:bootstrap}, we list the number of seed dishes, the number of extracted potential dishes, and the number of dishes that were annotated as real dishes in the potential dishes list. The reported values can contain duplicates, and once all extractions were acquired from every round of bootstrapping, they were de-duplicated (based on edit distance and confirmed manually) before being added to \benchmarkname{}.


\section{Image Extraction Annotation Interface}
\label{appendix:image_annot}
We use a custom interface to choose which images to extract during the dataset creation step. Figure~\ref{fig:annot_interface} shows this interface in use, as well as an example of why it is necessary; automatically pulling down the images shown  in the interface would result in images of all word senses, not just the dish.

\section{Parallel Corpus Origin Distribution}
\label{appendix:parallel_dist}
Figure~\ref{fig:bargraph} displays the origin distribution of the dishes in the parallel, Post-Soviet sub-dataset of \benchmarkname{}. We note that if a dish has multiple origins, it counts as a dish for each of those origins. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/bargraph.pdf}
    \caption{Countries of origin of dishes in the parallel, Post-Soviet sub-dataset of \benchmarkname{}. Dishes are heavily focused around Russian and Ukraine.}
    \vspace{-0.2cm}
    \label{fig:bargraph}
\end{figure}

\section{Prompts For Each Experiment}
\label{appendix:prompts}
Tables \ref{tab:prompts_russian} and \ref{tab:prompts_ukrainian} contain the prompts used in the QA, VQA, VQA with dish (\S\ref{subsec:parallel_qa}), dish name VQA (\S\ref{subsec:surzhyk}), and image generation experiments (\S\ref{sec:descriptions}) for Russian and Ukrainian respectively.

\section{Dish Origin VQA with Dish Name}
\label{appendix:vqa_with_name}
In \S\ref{subsec:parallel_qa}, we prompt text models for a dish's country of origin given its \textit{name}, while at the same time prompting vision models for a dish's country of origin given its \textit{images}. A logical continuation would be to give vision models both the dish's name \textbf{and} origin. Figure~\ref{fig:vqa_with_name} shows the overall results of this experiment, which exhibit very similar trends to the results in \S\ref{subsec:parallel_qa}. Furthermore, Figure~\ref{fig:vqa_with_name_delta} shows that generally, adding the dish name to the VQA prompt improves performance for most dishes compared to just having the images. However, there are some dishes where adding the name actually harms performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/doublebar.pdf}
    \caption{Both Llama and Qwen vision models can quite accurately predict the country of origin of Post-Soviet dishes when prompted with an image of the dish and its name. At the region and language level, trends do not change much from what is observed in \S\ref{subsec:parallel_qa}.}
    \vspace{-0.2cm}
    \label{fig:vqa_with_name}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/combined_lineplot.pdf}
    \caption{Adding the name of the dish to the country of origin VQA prompt increases model performance on Post-Soviet dishes in Russian and Ukrainian. However, there are a select few dishes where this is not the case.}
    \vspace{-0.2cm}
    \label{fig:vqa_with_name_delta}
\end{figure}

\section{Dish Name VQA Results}
\label{appendix:dish_vqa}
We report the full results of the dish name VQA experiment introduced in Section~\ref{subsec:surzhyk}. We provide a language model images of a dish and a prompt querying for the name of the dish displayed in the images (Appendix~\ref{appendix:prompts}). If the result contains the dish name within one edit distance, we consider this a success. We measure the success rate (accuracy) over five prompts to get the average \textbf{exact match accuracy}, and we report these results in Table~\ref{table:name_vqa}.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\scalerel*{\includegraphics{figures/Llama.png}}{\textbf{B}}\,\,\textbf{Llama-3.2}} & \multicolumn{2}{c}{\scalerel*{\includegraphics{figures/Qwen.png}}{\big(w(B)\big)}\,\,\textbf{Qwen2}} \\
% \cmidrule(lr){2-3}\cmidrule(lr){4-5}
\midrule
\textbf{Country} & RU & UK & RU & UK \\
\midrule
Estonia & \gradient{0.00} & \gradient{0.00} & \gradienta{0.00} & \gradienta{0.00}\\
Latvia & \gradient{0.12} & \gradient{0.16} & \gradienta{0.20} & \gradienta{0.20}\\
Lithuania & \gradient{0.06} & \gradient{0.09} & \gradienta{0.13} & \gradienta{0.11} \\
\hdashline
Russia & \gradient{0.11} & \gradient{0.07} & \gradienta{0.12} & \gradienta{0.10} \\
Ukraine & \gradient{0.07} & \gradient{0.07} & \gradienta{0.10} & \gradienta{0.08} \\
Belarus & \gradient{0.06} & \gradient{0.07} & \gradienta{0.12} & \gradienta{0.10}\\
Moldova & \gradient{0.05} & \gradient{0.05} & \gradienta{0.04} & \gradienta{0.05}\\
\hdashline
Armenia & \gradient{0.18} & \gradient{0.15} & \gradienta{0.16} & \gradienta{0.15} \\
Azerbaijan & \gradient{0.14} & \gradient{0.08} & \gradienta{0.10} & \gradienta{0.14}\\
Georgia & \gradient{0.17} & \gradient{0.16} & \gradienta{0.17} & \gradienta{0.09}\\
\hdashline
Kazakhstan & \gradient{0.15} & \gradient{0.14} & \gradienta{0.10} & \gradienta{0.16}\\
Kyrgyzstan & \gradient{0.10} & \gradient{0.10} & \gradienta{0.11} & \gradienta{0.11}\\
Tajikistan & \gradient{0.22} & \gradient{0.18} & \gradienta{0.14} & \gradienta{0.14} \\
Turkmenistan & \gradient{0.10} & \gradient{0.03} & \gradienta{0.00} & \gradienta{0.00}\\
Uzbekistan & \gradient{0.14} & \gradient{0.12} & \gradienta{0.12} & \gradienta{0.11} \\
\bottomrule
\end{tabular}
}
\caption{For both Llama and Qwen vision models, providing a name of a dish given its image is a difficult task. While there are some country/region/language/model trends, the overall trend that applies everywhere is that performance is poor, rarely reaching above 20\%.}
\label{table:name_vqa}
\end{table}

\section{Dish Description Evaluation Pipeline}
\subsection{Additional Examples}
\label{appendix:more_examples}
We provide additional examples of dish descriptions (translated into English), images generated from these descriptions, ground truth images, and resulting cosine similarities in Figure~\ref{fig:ig_examples_long}. 

\subsection{Human Evaluation}
\label{appendix:humaneval}
We calculate more correlation metrics between annotator scores (reflecting how accurately a generated description matches the ground-truth image) and cosine similarities of the \model{DiNOv2} encodings of the ground-truth and \model{FLUX.1-dev} generated dish images. The results can be found in Table~\ref{tab:other_correlations}. We also present a scatterplot of our human ratings and the encoding cosine similarities in Figure~\ref{fig:scatter}.

\begin{table}[t]
\centering
% \resizebox{\linewidth}{!}{%
\begin{tabular}{l c c c c}
\toprule
& \multicolumn{2}{c}{\scalerel*{\includegraphics{figures/Llama.png}}{\textbf{B}}\,\,\textbf{Llama-3.2}} 
& \multicolumn{2}{c}{\scalerel*{\includegraphics{figures/Qwen.png}}{\frac{a}{b}}\,\,\textbf{Qwen2}} \\
% \cmidrule(lr){2-3}
% \cmidrule(lr){4-5}
& \textbf{RU} & \textbf{UK} & \textbf{RU} & \textbf{UK} \\
\midrule
\textbf{Pearson $r$}     & 0.82 & 0.82 & 0.78 & 0.81 \\
\textbf{Spearman $\rho$} & 0.84 & 0.83 & 0.80 & 0.81 \\
\textbf{Kendall $\tau$}  & 0.68 & 0.67 & 0.63 & 0.65 \\
\textbf{Lin's $\rho_c$}  & 0.79 & 0.76 & 0.74 & 0.79 \\
\bottomrule
\end{tabular}%
% }
\caption{Correlations between human ratings and \model{DiNOv2} encoding cosine similarities of the ground-truth and \model{FLUX.1-dev} generated dish images. Using different metrics does not change the result much. }
\vspace{-0.2cm}
\label{tab:other_correlations}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/humaneval_scatter.png}
    \caption{Scatterplot of human ratings vs \model{DiNOv2} encoding cosine similarities of the ground-truth and \model{FLUX.1-dev} generated dish images.}
    \vspace{-0.2cm}
    \label{fig:scatter}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/scatter_llama.png}
\caption{The Spearman's $\rho$ between dish-description performance and QA tasks on \textbf{Llama} models shows a small, positive correlation. All axes span from 0 to 1.}
    \vspace{-0.2cm}
    \label{fig:ig_correlations_llama}
\end{figure}

\subsection{Llama QA Correlations}
\label{appendix:ig_correlations_llama}
In Figure~\ref{fig:ig_correlations}, we show how the cosine similarity from the image generation experiment correlates with all of our various QA tasks for the Qwen vision and text models. We show the same results, but for Llama vision and text models, in Figure~\ref{fig:ig_correlations_llama}

\subsection{Set Similarity Metrics Ablation}
\label{appendix:description_ablation}
One of our findings regarding the dish description evaluation experiment is that the produced cosine similarities have a small, positive Spearman correlation with QA/VQA Jaccard scores. To confirm these findings, we report in Table~\ref{tab:metric_correlations} the Pearson correlation $r$, Spearman correlation $\rho$, and Kendall $\tau$ for three different set similarity metrics:
\begin{align}
\text{Jaccard Score} &= \frac{|P \cap T|}{|P \cup T|}\text{;} \\
\text{Dice Coeff.} &= \frac{2|P \cap T|}{|P| + |T|}\text{;} \\
\text{Overlap Coeff.} &= \frac{|P \cap T|}{\min\bigl(|P|, |T|\bigr)}\text{;}
\end{align}

\noindent where $T$ is the set of ground-truth countries and $P$ is the predicated set of countries. We do not observe any noticeable deviation from our originally reported results.

\begin{table}[ht]
\centering
% \resizebox{\linewidth}{!}{%
\begin{tabular}{l c c c c}
\toprule
& \multicolumn{2}{c}{\scalerel*{\includegraphics{figures/Llama.png}}{\textbf{B}}\,\,\textbf{Llama-3.2}} 
& \multicolumn{2}{c}{\scalerel*{\includegraphics{figures/Qwen.png}}{\frac{a}{b}}\,\,\textbf{Qwen2}} \\
% \cmidrule(lr){2-3}
% \cmidrule(lr){4-5}
& \textbf{RU} & \textbf{UK} & \textbf{RU} & \textbf{UK} \\
\midrule
QA Jaccard $r$    & 0.08 & 0.12 & 0.09 & 0.12 \\
QA Jaccard $\rho$ & 0.12 & 0.14 & 0.14 & 0.19 \\
QA Jaccard $\tau$  & 0.08 & 0.09 & 0.09 & 0.13 \\
\hdashline
QA Dice $r$    & 0.11 & 0.15 & 0.13 & 0.15 \\
QA Dice $\rho$ & 0.12 & 0.16 & 0.14 & 0.19 \\
QA Dice $\tau$  & 0.08 & 0.10 & 0.09 & 0.13 \\
\hdashline
QA Overlap $r$    & 0.12 & 0.19 & 0.17 & 0.20 \\
QA Overlap $\rho$ & 0.09 & 0.20 & 0.14 & 0.23 \\
QA Overlap $\tau$  & 0.06 & 0.14 & 0.10 & 0.16 \\
\midrule
\midrule 
VQA Jaccard $r$    & 0.09 & 0.06 & 0.16 & 0.15 \\
VQA Jaccard $\rho$ & 0.14 & 0.12 & 0.21 & 0.20 \\
VQA Jaccard $\tau$  & 0.10 & 0.08 & 0.15 & 0.14 \\
\hdashline
VQA Dice $r$    & 0.10 & 0.08 & 0.18 & 0.16 \\
VQA Dice $\rho$ & 0.14 & 0.13 & 0.21 & 0.20 \\
VQA Dice $\tau$  & 0.09 & 0.09 & 0.15 & 0.14 \\
\hdashline
VQA Overlap $r$    & 0.08 & 0.14 & 0.16 & 0.16 \\
VQA Overlap $\rho$ & 0.11 & 0.17 & 0.18 & 0.19 \\
VQA Overlap $\tau$  & 0.08 & 0.12 & 0.13 & 0.13 \\
\bottomrule
\end{tabular}%
% }
\caption{Correlation metrics (Pearson, Spearman, Kendall) between image generation cosine similarity (\S\ref{sec:descriptions}) and QA/VQA (\S\ref{subsec:parallel_qa}) measured using three set overlap metrics: Jaccard score (used in the main paper), Dice coefficient, and Overlap coefficient.}
\vspace{-0.2cm}
\label{tab:metric_correlations}
\end{table}

\section{Annotator Instructions}
\label{appendix:annot_inst}
The following were the instructions given to annotators in each task that required human evaluation/annotation. Annotators were fluent in the necessary languages, college educated, paid a rate of \$18 an hour, and recruited from the university. All annotators were fully informed of the study's aims and methods from the outset. We held frequent meetings to ensure annotator understanding of experiments their annotations were used in, and all annotators had the option to withdraw their contributions at any point if they wished.
\subsection{Dish Filtering}
Please label the following as dishes (T) or not dishes (F). A \say{dish} meets the following criteria
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item Made up of multiple ingredients (e.g. \textit{cucumber} does not meet this criteria).
    \item Culturally specific in some way (e.g. \textit{grilled chicken} does not meet this criteria, as it is globally common).
    \item Something that people eat/is edible.
    \item Although drinks can meet these criteria, we exclude them.
\end{itemize}
If you are unsure for a certain dish, please mark/highlight it and we will discuss it.

\subsection{Origin Annotation}
Please label the countries which the given dish originates from (use the Alpha-2 country code of the country, which can be found at\url{https://www.iban.com/country-codes}). There can either be one or multiple origins for the dish. Try to find multiple sources corroborating your decisions; this is as much a research task as it is an annotation task. If you cannot find multiple corroborating sources or do not feel confident with your annotation, please mark the dish and we will exclude it.

\subsection{Image Filtering}
Please mark the image if it meets the following criteria:
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item There is no food dish in the image.
    \item There are multiple food dishes in the image (a side, like bread, is fine as long as it is not the main focus).
    \item There are people in the image.
    \item The image is of poor quality (blurry, too small, etc.).
    \item There is text in the image.
    \item There is anything personally identifying in the image (documents, names, etc.).
\end{itemize}

\subsection{Image Description Human Evaluation}
Please rate how well the dish description matches the provided image of the same dish from 0 to 1. Please note that a good description is both accurate and concise. Just as an insufficient description should receive a poor score, a description that states a lot of extra, unnecessary information should also be penalized.

\section{Wikidata Query}
\label{appendix:query}
Our food dish SPARQL query (Figure~\ref{fig:query}) retrieves information about food items, including their English labels and a list of country codes representing their countries of origin. The \texttt{?fid} variable identifies each food item, and \texttt{?food\_en} provides its English name. The query uses \texttt{GROUP\_CONCAT} to aggregate unique country codes (\texttt{?countryOfOriginCode}) for each food item into a single, comma-separated list (\texttt{?countryOfOriginList}). The filter condition ensures that only English labels are selected, while an \texttt{OPTIONAL} block allows for cases where a country of origin may not be specified, making that part of the data retrieval non-mandatory. Finally, the results are grouped by \texttt{?fid} and \texttt{?food\_en} to return distinct food items with their respective country origins. We note that although this query acquires English data, it is trivial to modify it for Russian or Ukrainian by modifying the language code.

\section{Full Dataset Composition and Statistics}
\label{appendix:full_dataset}
We give the detailed country of origin composition of the Russian and Ukrainian subsets of \benchmarkname{} in Tables \ref{tab:ru_full} and \ref{tab:uk_full} respectively. Note that if a dish has multiple countries of origin, this counts as one dish for each of those origins. For example, if a dish traces its origins to both Bulgaria and North Macedonia, then this would count as a dish for Bulgaria \textbf{and} as a dish for North Macedonia.

\section{Reproducibility and Hyperparameters}
\label{appendix:hparams}
\subsection{Compute Set-Up}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item Experiments were run on eight A40 GPUs, and evaluation was distributed among them using huggingface.
    \item Per language, every non-vision enabled experiment would take around 8 GPU hours to run (1 hour across 8 GPUs).
    \item Per language, every vision-enabled experiment would take around 24 GPU hours to run (3 hours across 8 GPUs).
    \item Inference was conducted using vLLM \cite{kwon2023efficient}.
\end{itemize}

\subsection{\texttt{Llama-3.1-70B-Instruct}}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \url{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}.
    \item 70.6 billion parameters.
    \item do\_sample = False, context\_length = 4096, max\_tokens = 200.
    \item We adhered to the license and intended use of this model (\url{www.llama.com/llama3_1/license/}).
\end{itemize}

\subsection{\texttt{Llama-3.2-90B-Vision-Instruct}}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \url{https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct}.
    \item 88.6 billion parameters.
    \item do\_sample = False, context\_length = 8000, max\_tokens = 200.
    \item We adhered to the license and intended use of this model (\url{www.llama.com/llama3_2/license/}).
\end{itemize}

\subsection{\texttt{Qwen2-72B-Instruct}}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \url{https://huggingface.co/Qwen/Qwen2-72B-Instruct}.
    \item 72.7B billion parameters.
    \item do\_sample = False, context\_length = 4096, max\_tokens = 200.
    \item We adhered to the license and intended use of this model (\url{huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE}).
    
\end{itemize}

\subsection{\texttt{Qwen2-VL-72B-Instruct}}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \url{https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct}.
    \item 73.4 billion parameters.
    \item do\_sample = False, context\_length = 8000, max\_tokens = 200.
    \item We adhered to the license and intended use of this model (\url{huggingface.co/Qwen/Qwen2-VL-72B-Instruct/blob/main/LICENSE}).

\end{itemize}

\subsection{\texttt{DiNOv2-giant}}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \url{https://huggingface.co/facebook/dinov2-giant}
    \item 1.14 billion parameters.
    \item All hyperparameters are default.
    \item We adhered to the license and intended use of this model (Apache License 2.0).
\end{itemize}

\subsection{\texttt{FLUX.1-dev}}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \url{https://huggingface.co/black-forest-labs/FLUX.1-dev}
    \item 12 billion parameters.
    \item All hyperparameters are default.
    \item We adhered to the license and intended use of this model (\url{github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev}).
    
\end{itemize}

\subsection{\texttt{spaCy} NER}
\begin{itemize}[noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt] 
    \item \textbf{Russian:} \texttt{ru\_core\_news\_sm}. More info at \url{https://spacy.io/models/ru}.
    \item \textbf{Ukrainian:} \texttt{uk\_core\_news\_sm}. More info at \url{https://spacy.io/models/uk}.
    \item We adhered to the license and intended use of this model (MIT License).
\end{itemize}

\section{AI Assistants}
We used ChatGPT for \model{GPT-4o} and \model{o1} as grammar/spell checkers.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/annot_interface.png}
    \caption{An example of the annotation interface for filtering the custom google image search results for dishes in \benchmarkname{}. As you can see, \say{fennek} has three word senses shown here. Some sort of robot, a fox, and a Maltese dish. Dishes like this necessitate filtering by a human annotator.}
    \vspace{-0.2cm}
    \label{fig:annot_interface}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{tcolorbox}
[colback=black!5!white,colframe=gray!75!black,title=Wikidata SPARQL Query]
\scriptsize
\begin{verbatim}
SELECT ?fid ?food_en (GROUP_CONCAT(DISTINCT ?countryOfOriginCode; SEPARATOR = ", ") 
AS ?countryOfOriginList) WHERE {
    ?fid (wdt:P31/(wdt:P279*)) wd:Q746549;
         rdfs:label ?food_en.
    FILTER((LANG(?food_en)) = "en")
    OPTIONAL {
        ?fid wdt:P495 ?countryOfOriginEntity.
        ?countryOfOriginEntity wdt:P297 ?countryOfOriginCode.
    }
}
GROUP BY ?fid ?food_en
\end{verbatim}
\end{tcolorbox}
\caption{The English \texttt{SPARQL} query used to retrieve Wikidata food items, both for the dataset as well as for the seedlist in our algorithm.}
\label{fig:query}
\end{figure*}

% Russian Table
\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{3cm} X}
\toprule
\multicolumn{2}{c}{\textbf{Russian Prompt Table}} \\
\midrule
\textbf{Prompt Type} & \textbf{Prompt} \\
\midrule

% QA Prompts
\multirow{5}{*}{\centering \textbf{QA}}
& \cyrillic{Из какой страны или стран происходит блюдо [DISH]?} \\
& \cyrillic{В какой стране или странах возникло блюдо [DISH]?} \\
& \cyrillic{В каких странах принято подавать блюдо [DISH]?} \\
& \cyrillic{Какие страны считаются родиной блюда [DISH]?} \\
& \cyrillic{В каких странах блюдо [DISH] популярно или традиционно?} \\
\cmidrule(lr){1-2}

% VQA Prompts
\multirow{5}{*}{\centering \textbf{VQA}}
& \cyrillic{Из какой страны или каких стран происходит блюдо на фотографиях?} \\
& \cyrillic{В какой стране или в каких странах возникло блюдо на фотографиях?} \\
& \cyrillic{В каких странах принято подавать блюдо на фотографиях?} \\
& \cyrillic{Какие страны считаются родиной блюда на фотографиях?} \\
& \cyrillic{В каких странах блюдо на фотографиях популярно или традиционно?} \\
\cmidrule(lr){1-2}

% VQA + Dish Prompts
\multirow{5}{*}{\centering \textbf{VQA + Dish}}
& \cyrillic{Из какой страны или каких стран происходит блюдо [DISH], показанное на фотографиях?} \\
& \cyrillic{В какой стране или в каких странах возникло блюдо [DISH], показанное на фотографиях?} \\
& \cyrillic{В каких странах принято подавать блюдо [DISH], показанное на фотографиях?} \\
& \cyrillic{Какие страны считаются родиной блюда [DISH], показанного на фотографиях?} \\
& \cyrillic{В каких странах блюдо [DISH], показанное на фотографиях, популярно или традиционно?} \\
\cmidrule(lr){1-2}

% Dish Name VQA Prompts
\multirow{5}{*}{\centering \textbf{Dish Name VQA}}
& \cyrillic{Какие возможные названия могут быть у блюда на фотографиях?} \\
& \cyrillic{Какими именами может быть известно это блюдо на фотографиях?} \\
& \cyrillic{Как можно назвать блюдо, изображённое на фотографиях?} \\
& \cyrillic{Какие названия имеет блюдо на фотографиях?} \\
& \cyrillic{Какие имена имеет это блюдо на фотографиях?} \\
\cmidrule(lr){1-2}

% Image Generation Prompt
\multirow{1}{*}{\centering \textbf{Image Generation}}
& \cyrillic{Опишите блюдо [DISH], не используя его название.} \\

\bottomrule
\end{tabularx}
\caption{All prompts used for experiments in \textbf{Russian}. \texttt{[DISH]} is a template for the actual dish name.}
\label{tab:prompts_russian}
\end{table*}

% Ukrainian Table
\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{3cm} X}
\toprule
\multicolumn{2}{c}{\textbf{Ukrainian Prompt Table}} \\
\midrule
\textbf{Prompt Type} & \textbf{Prompt} \\
\midrule

% QA Prompts
\multirow{5}{*}{\centering \textbf{QA}}
& \cyrillic{З якої країни або яких країн походить страва [DISH]?} \\
& \cyrillic{Які країни вважаються батьківщиною страви [DISH]?} \\
& \cyrillic{У яких країнах страва [DISH] є традиційною або популярною?} \\
& \cyrillic{У яких країнах готують страву [DISH]?} \\
& \cyrillic{Назви країну або країни, у яких їдять страву [DISH]?} \\
\cmidrule(lr){1-2}

% VQA Prompts
\multirow{5}{*}{\centering \textbf{VQA}}
& \cyrillic{З якої країни або яких країн походить страва на фотографіях?} \\
& \cyrillic{Які країни вважаються батьківщиною страви на фотографіях?} \\
& \cyrillic{У яких країнах страва на фотографіях є традиційною або популярною?} \\
& \cyrillic{У яких країнах готують страву на фотографіях?} \\
& \cyrillic{Назви країну або країни, у яких їдять страву на фотографіях?} \\
\cmidrule(lr){1-2}

% VQA + Dish Prompts
\multirow{5}{*}{\centering \textbf{VQA + Dish}}
& \cyrillic{З якої країни або яких країн походить страва [DISH] на фотографіях?} \\
& \cyrillic{Які країни вважаються батьківщиною страви [DISH] на фотографіях?} \\
& \cyrillic{У яких країнах страва [DISH] на фотографіях є традиційною або популярною?} \\
& \cyrillic{У яких країнах готують страву [DISH] на фотографіях?} \\
& \cyrillic{Назви країну або країни, у яких їдять страву [DISH] на фотографіях?} \\
\cmidrule(lr){1-2}

% Dish Name VQA Prompts
\multirow{5}{*}{\centering \textbf{Dish Name VQA}}
& \cyrillic{Якими назвами може бути відома страва на фотографіях?} \\
& \cyrillic{Які назви можна використати для опису страви, зображеної на фотографіях?} \\
& \cyrillic{Під якими назвами може бути відома ця страва на фотографіях?} \\
& \cyrillic{Чи можна визначити можливі назви страви на фотографіях?} \\
& \cyrillic{Як прийнято називати цю страву, зображену на фотографіях, і які інші назви можуть бути для неї відомі?} \\
\cmidrule(lr){1-2}

% Image Generation Prompt
\multirow{1}{*}{\centering \textbf{Image Generation}}
& \cyrillic{Опиши страву [DISH], не використовуючи її назву.} \\

\bottomrule
\end{tabularx}
\caption{All prompts used for experiments in \textbf{Ukrainian}. \texttt{[DISH]} is a template for the actual dish name.}
\label{tab:prompts_ukrainian}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/image_gen_examples.pdf}
    \caption{Examples of the image description evaluation pipeline described in \S\ref{sec:descriptions}.}
    \vspace{-0.2cm}
    \label{fig:ig_examples_long}
\end{figure*}

\begin{table*}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\footnotesize
\begin{tabular}{ll | ll | ll}
\hline
\textbf{Country Name} & \textbf{Count} & \textbf{Country Name} & \textbf{Count} & \textbf{Country Name} & \textbf{Count} \\
\hline
Russian Federation & 203 & Lebanon & 10 & Nigeria & 2\\ 
Ukraine & 112 & Belgium & 9 & Bosnia and Herzegovina & 2\\ 
France & 98 & Bulgaria & 9 & Bolivia & 2\\ 
Belarus & 87 & Finland & 8 & Ireland & 2\\ 
Germany & 79 & Palestine & 7 & Iceland & 2\\ 
Italy & 78 & Denmark & 7 & Australia & 2\\ 
United States & 52 & Argentina & 7 & Nepal & 2\\ 
Türkiye & 48 & Tunisia & 7 & Myanmar & 2\\ 
Japan & 42 & Egypt & 6 & Afghanistan & 2\\ 
Georgia & 42 & Netherlands & 6 & Malta & 2\\ 
Spain & 37 & Brazil & 6 & Colombia & 2\\ 
Poland & 33 & Switzerland & 6 & Malaysia & 2\\ 
Indonesia & 31 & Serbia & 6 & Mauritania & 2\\ 
China & 26 & Czechia & 6 & Senegal & 1\\ 
India & 25 & Norway & 6 & South Africa & 1\\ 
Armenia & 25 & Philippines & 5 & Ecuador & 1\\ 
United Kingdom & 24 & Croatia & 5 & Kenya & 1\\ 
Greece & 22 & Algeria & 5 & Bahrain & 1\\ 
Austria & 22 & Jordan & 5 & Bangladesh & 1\\ 
Korea, Republic of & 22 & Turkmenistan & 5 & Slovenia & 1\\ 
Mexico & 19 & Viet Nam & 4 & Estonia & 1\\ 
Uzbekistan & 19 & Slovakia & 4 & Cuba & 1\\ 
Hungary & 17 & Uruguay & 4 & Oman & 1\\ 
Azerbaijan & 17 & Pakistan & 4 & Yemen & 1\\ 
Lithuania & 16 & North Macedonia & 4 & Eritrea & 1\\ 
Kazakhstan & 15 & Iraq & 4 & Congo & 1\\ 
Syria & 14 & Peru & 3 & Gabon & 1\\ 
Romania & 13 & Canada & 3 & Dominican Republic & 1\\ 
Portugal & 13 & Taiwan & 3 & Mali & 1\\ 
Kyrgyzstan & 13 & Bhutan & 3 & Venezuela & 1\\ 
Moldova, Republic of & 12 & Haiti & 3 & Thailand & 1\\ 
Tajikistan & 11 & Albania & 3 & Cambodia & 1\\ 
Iran & 11 & Libya & 3 & Cyprus & 1\\ 
Sweden & 10 & Paraguay & 3 & Unknown & 1\\ 
Israel & 10 & New Zealand & 3 & Latvia & 1\\ 
Morocco & 10 & Mongolia & 3 & Singapore & 1\\ 
Lebanon & 10 & Ghana & 2 & Brunei Darussalam & 1\\ 
\hline
\end{tabular}%
}
\caption{Countries of origin in the \textbf{Russian} subset of \benchmarkname{}.}
\label{tab:ru_full}
\end{table*}

\begin{table*}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\footnotesize
\begin{tabular}{ll | ll | ll}
\hline
\textbf{Country Name} & \textbf{Count} & \textbf{Country Name} & \textbf{Count} & \textbf{Country Name} & \textbf{Count} \\
\hline
Ukraine & 166 & Austria & 8 & Bolivia & 2\\ 
Russian Federation & 157 & Switzerland & 8 & Peru & 2\\ 
Belarus & 100 & Kyrgyzstan & 8 & South Africa & 2\\ 
France & 59 & Sweden & 8 & Argentina & 2\\ 
Italy & 47 & Iran & 8 & Iceland & 2\\ 
Poland & 36 & Belgium & 7 & Iraq & 2\\ 
Türkiye & 34 & Egypt & 7 & Philippines & 2\\ 
Georgia & 33 & Czechia & 7 & Afghanistan & 2\\ 
United States & 30 & Croatia & 6 & Paraguay & 2\\ 
India & 29 & Latvia & 6 & Sri Lanka & 2\\ 
Germany & 27 & Palestine & 6 & Thailand & 2\\ 
Japan & 27 & Malaysia & 5 & Turkmenistan & 2\\ 
Indonesia & 24 & Bosnia and Herzegovina & 5 & Singapore & 2\\ 
Lithuania & 20 & Tunisia & 5 & Ecuador & 1\\ 
Uzbekistan & 19 & Denmark & 5 & Senegal & 1\\ 
Greece & 19 & Brazil & 5 & Niger & 1\\ 
Armenia & 18 & Nepal & 5 & Kenya & 1\\ 
Spain & 18 & Jordan & 5 & Bahrain & 1\\ 
Azerbaijan & 17 & Slovakia & 4 & Myanmar & 1\\ 
United Kingdom & 16 & Pakistan & 4 & Benin & 1\\ 
Bulgaria & 16 & Israel & 4 & Chile & 1\\ 
Hungary & 15 & Bhutan & 4 & Trinidad and Tobago & 1\\ 
China & 14 & Albania & 4 & Haiti & 1\\ 
Mexico & 14 & Libya & 4 & Monaco & 1\\ 
Romania & 14 & Norway & 4 & Sudan & 1\\ 
Syria & 11 & Taiwan & 3 & Saudi Arabia & 1\\ 
Nigeria & 11 & Ghana & 3 & Yemen & 1\\ 
Portugal & 11 & Viet Nam & 3 & Congo, The Democratic Republic of the & 1\\ 
Tajikistan & 10 & Finland & 3 & Angola & 1\\ 
Serbia & 10 & Morocco & 3 & Venezuela & 1\\ 
Moldova, Republic of & 10 & Uruguay & 3 & Canada & 1\\ 
Korea, Republic of & 10 & Slovenia & 3 & Cyprus & 1\\ 
Netherlands & 9 & Algeria & 3 & Mauritania & 1\\ 
Kazakhstan & 9 & Estonia & 3 & Unknown & 1\\ 
Lebanon & 9 & Mongolia & 3 & Australia & 1\\ 
Austria & 8 & North Macedonia & 3 & Montenegro & 1\\ 
\hline
\end{tabular}%
}
\caption{Countries of origin in the \textbf{Ukrainian} subset of \benchmarkname{}.}
\label{tab:uk_full}
\end{table*}

\end{document}