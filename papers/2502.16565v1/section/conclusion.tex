\section{Conclusion}
We have studied \emph{when and why} \textbf{implicit consensus can outperform explicit coordination} in LLM-based multi-agent systems, focusing on in-context learning, self-organization, and resilience. By modeling the \emph{dynamic consensus-diversity tradeoff}, we reveal that moderate deviations from uniformity enable better performance under environmental shifts or adversarial disruptions. Through experiments on dynamic disaster response, misinformation containment, and public-goods provision, we show robust gains from emergent coordination where each agent retains partial autonomy. Our results align with social science insights that complete agreement can stifle exploration and hamper long-term effectiveness.

In future work, we will further analyze the role of advanced prompting strategies, trust modeling among LLM agents, and potential meta-control of diversity. We also aim to investigate larger-scale simulations with varied LLM architectures and partial observability constraints. Ultimately, \emph{balancing} consensus and diversity can pave the way for highly robust multi-agent AI that flexibly adapts to unforeseen challenges, much like heterogeneous human teams do.

\section*{Ethical Statement}
Our scenarios involve multi-agent cooperation under dynamic conditions, including adversarial misinformation. Researchers should exercise caution when deploying such systems to ensure they do not facilitate harmful strategies (e.g., enabling misinformation). In disaster relief settings, simulated or partial deployment must account for human oversight and moral implications of decisions (e.g., triage in resource-limited contexts). This work aims to enhance collaboration mechanisms, not to displace human judgment in high-stakes scenarios.

We used ChatGPT to polish the paper. We are responsible for all the materials presented in this work.

\section*{Limitations}
(1) This work relies on purely in-context adaptation of large language models, which may struggle with extremely long dialogues or memory constraints. We also use idealized small-group scenarios, while real-world applications (e.g., large-scale social networks) may require more advanced messaging protocols. Our measure of partial diversity is approximate, and more sophisticated metrics (e.g., semantic distances in agent solutions) may yield deeper insights. Finally, controlling emergent agent behavior to ensure safety remains an open question, given the lack of a central authority in implicit consensus. (2) We conducted our research using four models, as reported in the paper: GPT-4 (including GPT-4o and GPT-4o-mini), Claude-3-Sonnet, Llama-2, and Qwen-Plus. We did not include stronger models in our experiments for several reasons. DeepSeek-R1, GPT-o1, and Claude-3.5 have highly restrictive rate limits, making multi-turn experiments challenging. Additionally, Qwen-Max has a relatively small context window, which limits the scope of our study. Also there are financial constraints associated with the case studies, we report the cost of a single run of these three case studies: \{\$5, \$10, \$5\}. Therefore, we consider our findings as an exploratory study that needs further validation across different LLMs to enhance their generalizability.

% \section*{Acknowledgments}
