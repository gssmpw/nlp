\appendix
\section{Detailed Experiment Scenario}
\label{sec:appendix:a}

\subsection{Dynamic Disaster Response Scenario}

\paragraph{Natural Language Information and Interference.}
Besides numeric indicators (such as severity scores), the system provides each agent a snippet of textual “reports” each round, e.g.,
\begin{quote}
\small
\emph{``Dispatch Alert: Fire intensity at Sector (3,4) may be increasing. Local residents report rising smoke. Drone \#2 previously found moderate casualties in Sector (2,2).''}
\normalsize
\end{quote}
Some reports may be incomplete or partially contradictory (e.g., a rumor that the fire is \emph{under control} despite contradictory sensor data). Agents thus need to parse these textual cues and weigh them against each other.

\paragraph{Key Experimental Factors.} \textbf{(1) Disaster severity simulation:} Each disaster has an evolving severity score $s \in [1,10]$. Higher $s$ implies higher penalty if uncontained. The environment updates $s$ in a stochastic manner, sometimes producing contradictory textual updates to test agents' ability to parse partial/misleading info. \textbf{(2) Resource constraints:} Each drone has a limited capacity (e.g., 5 units of firefighting foam). Deploying them on the wrong location wastes resources. \textbf{(3) Consensus Mechanism:} \emph{Explicit:} agents vote on one zone to be the team’s priority, or follow a “unify on the most urgent location” script. \emph{Implicit:} each agent decides a location after reading the textual discussion. Some may deviate if they suspect a different site is more critical. \textbf{(4) Performance Metrics:} Coverage rate (fraction of disasters contained within 2 rounds of major severity), misallocation penalty (resources wasted on low-severity areas while ignoring high-severity ones), and average response delay.

\paragraph{Connecting to Our Research Questions.}
For Q1, we expect that under \emph{frequent} or \emph{fast-growing} disasters, implicit consensus adapts faster.  For Q2, different role prompts (e.g., "Focus on casualties" vs.\ "Minimize travel cost") introduce moderate disagreements; we measure how $\bar{d}(t)$ correlates with timely coverage. For Q3, by analyzing message logs, we see if agents revise their location choices after contradictory updates, signifying in-context learning.

\subsection{Information Spread and Manipulation Scenario}

\paragraph{Defining Misinformation.}
Misinformation is represented both as a Boolean label (node $n$ is either infected or not) and as \emph{natural language claims} that vary each round, for example:
\begin{quote}
\small
\emph{``Breaking: Node \#12 says `Vaccines have microchips’, 10 neighbors are starting to share the rumor.''}
\normalsize
\end{quote}
This textual claim might be entirely false, but some “partial truths” are mixed in to raise confusion. Defender agents must interpret these claims, cross-check references, and decide which node(s) to target with a correction or “fact-check” broadcast.

\paragraph{Key Experimental Factors.}
\begin{itemize}
\item \textbf{Adversarial injections.} Every few rounds, the adversary injects new false claims into one or more nodes, sometimes disguising them as updates about a different topic.
\item \textbf{Consensus Mechanism.}  
\begin{itemize}
    \item \emph{Explicit:} defenders unify on a single node to address each round (e.g., via majority vote).
    \item \emph{Implicit:} each defender chooses a node or group of nodes to check based on discussion. Deviations can help if misinformation emerges in multiple places simultaneously.
\end{itemize}
\item \textbf{Performance Metrics.} 
\begin{itemize}
    \item \emph{Final misinformation spread} = number of nodes still misinformed after $T$ rounds.
    \item \emph{Containment time} = how many rounds it takes to isolate or correct a newly infected node.
    \item \emph{Defender coverage diversity}: how many unique nodes defenders collectively address per round.
\end{itemize}
\end{itemize}

\paragraph{Connecting to Our Research Questions.}
\begin{itemize}
    \item \textbf{Q1} Under frequent misinformation injections, forced alignment may cause defenders to chase the same node while others go unaddressed. Implicit consensus might help multi-front coverage.  
    \item \textbf{Q2} Medium or high diversity (some defenders focusing on suspicious clusters, others scanning widely) may yield better overall coverage, captured by deviation \(\bar{d}(t)\).  
    \item \textbf{Q3} Round-by-round text messages allow defenders to reference past attacks (“\emph{We saw a similar rumor last round, let’s watch Node 15 next}”), illustrating adaptation.
\end{itemize}

\subsection{Dynamic Public-Goods Provision Scenario}

\paragraph{Public-Good Mechanics.}
Let $x_i(t) \in [0,\,C_{\max}]$ be the amount agent $i$ contributes at round $t$, where $C_{\max}$ is the maximum individual contribution capacity. Define the \emph{total} contribution:
\[
X(t) \;=\; \sum_{i=1}^N x_i(t).
\]
A public good is considered \emph{funded} if $X(t) \geq \theta(t)$, where $\theta(t)$ is a \emph{dynamic threshold} that may change each round. When funded, the system grants a \emph{collective benefit} $B(t)$ to all agents (e.g., a large increase in safety, infrastructure, or shared profit). Each agent’s net payoff from round $t$ can be expressed as:
\[
\Pi_i(t) \;=\; \underbrace{\frac{B(t)}{N}}_{\text{shared benefit}} \;-\; \underbrace{c \cdot x_i(t)}_{\text{individual cost}},
\]
where $c>0$ is the marginal cost per contribution unit (also possibly time-varying).

\paragraph{Dynamic Environment Factors.}
To incorporate volatility, we let either $\theta(t)$ (the required threshold) or $B(t)$ (the total benefit) fluctuate. For instance:
\begin{itemize}
    \item \emph{Economic Shock:} $\theta(t)$ may jump up (e.g., a crisis requiring higher funds) or drop (a technology breakthrough lowering cost).
    \item \emph{Environmental Impact:} $B(t)$ might vary based on external conditions (e.g., if the public good is a dike, storms increase the benefit of maintaining it).
    \item \emph{Rumors or Uncertain Reports:} Agents receive textual updates like “\emph{The threshold might rise to 40 next round due to a new regulation}” or “\emph{Experts claim the benefit is overestimated},” introducing partial or misleading information.
\end{itemize}

\paragraph{Consensus Mechanism.}
\begin{itemize}
    \item \textbf{Explicit Mode}: Agents vote or are instructed to adopt a single collective contribution $x_{\mathrm{group}}(t)$, which is evenly split among them. (Equivalently, they each commit to $x_i(t) = x_{\mathrm{group}}(t)/N$.)
    \item \textbf{Implicit Mode}: Agents \emph{discuss} (e.g., “I suspect we only need 20 total,” “We might overshoot if the new threshold rumor is false”) but finalize $x_i(t)$ independently. Some agents may deviate to free-ride or over-contribute based on their interpretation of the textual cues.
\end{itemize}

\paragraph{Performance Metrics.}
We track:
\begin{enumerate}
    \item \textbf{Provision Rate:} How often $X(t)$ meets or exceeds $\theta(t)$ across the $T$ rounds.  
    \item \textbf{Total Payoff:} $\sum_{t=1}^T \sum_{i=1}^N \Pi_i(t)$, capturing overall welfare.  
    \item \textbf{Equity or Free-Riding:} The variance or Gini coefficient of $\{x_i(t)\}$ over agents, indicating whether some consistently shoulder higher costs than others.  
\end{enumerate}
When the environment shifts threshold or benefit, a rigidly unified approach (explicit consensus) may be slow to adapt or may fail to sense incipient problems if all agents rely on the same faulty rumor. In contrast, partial diversity (some trusting a rumor, others doubting it) may maintain better long-term outcomes.

\paragraph{Connecting to Our Research Questions.}
\begin{itemize}
    \item \textbf{Q1} Under frequent or large shifts in $\theta(t)$ or $B(t)$, forced consensus might overshoot or undershoot repeatedly, while implicit consensus can allow outlier agents to either contribute more (if they believe the threshold is rising) or less (if they suspect costs are too high).
    \item \textbf{Q2} By varying agent role prompts (e.g., “always ensure public good is funded” vs.\ “minimize personal cost”) we introduce moderate or strong diversity. We measure how this affects $\bar{d}(t)$ in contribution levels and see whether partial disagreement leads to more robust adaptation.
    \item \textbf{Q3} Agents may reference prior misunderstandings (\emph{“Last round we overpaid; let's not trust the rumor this time.”}) or note partial contributions from others (\emph{“Agent \#2 seems to be free-riding, so I’ll push my contribution up.”})—clear indicators of round-by-round in-context learning.
\end{itemize}

\paragraph{Illustrative Example.}
Consider $N=5$ agents and an initial threshold $\theta(1)=30$. Four agents each propose contributing 5 units to hit 20 total, while the fifth agent, trusting a rumor that “\(\theta\) is lower than it looks,” offers only 2. If the actual threshold is 25, then even with partial deviation, the sum (22) falls short, failing to fund the good. But if another agent, less trustful of the rumor, deviates upward to 7 units, the total might reach 24—still not enough. Over time, the group’s discussion leads them to converge around 25 or more, but occasionally someone might keep free-riding. Meanwhile, a random shock might raise $\theta(5)$ to 40; if all adopt the same unchanging strategy, the good is unfunded. If one agent is “paranoid,” contributing extra, it may save the collective from shortfall. This scenario thus highlights how partial autonomy can hedge against rumor-driven errors or incomplete knowledge.

\section{Experimental Settings}
\label{sec:appendix:b}

This appendix provides the concrete experimental configurations for our three case studies, including environment parameters, reward/penalty functions, and example prompts. Unless noted otherwise, each experiment is repeated over 5 random seeds (or distinct initializations) to reduce variance, and results are averaged. For all scenarios and all runs, the model parameters \texttt{temperature} is set to \texttt{0.7} to balance the performance and diversity, and the \texttt{max\_token} is set to \texttt{256}.

\subsection{Dynamic Disaster Response}
\label{appendix:disaster}

\paragraph{Grid and Disaster Zones.}
We use a $10\times10$ grid representing a simplified city map. At any point, there are up to $K=3$ active disasters (e.g., fires, floods). The environment updates \emph{every round} by:
\begin{itemize}
    \item Potentially moving an existing disaster to a neighboring grid cell (random direction).
    \item Changing the \emph{severity} $s \in [1,10]$ of one or more disasters (can increase or decrease by 1--3 points).
    \item Creating a new disaster with small probability $p_{\text{new}}=0.2$ if fewer than 3 are active.
\end{itemize}
Each disaster occupies a single cell, but severity influences how damaging it is if not contained.

\paragraph{Agent Roles and Prompts.}
We have $N=3$--$100$ LLM agents (GPT-4, Claude, Llama-2, Qwen) controlling “drones.” Each agent is given a short role prompt, such as:
\begin{itemize}
    \item \texttt{Medical drone:} \emph{``Focus on rescuing casualties in the highest-severity disaster zone for people.''}
    \item \texttt{Infrastructure drone:} \emph{``Protect power lines and roads. Even if severity is high elsewhere, prioritize built structures.''}
    \item \texttt{Logistics drone:} \emph{``Minimize travel cost. Quickly move to the nearest active zone if severity is above 5.''}
\end{itemize}
In the \textbf{low-diversity} condition, all agents share a nearly identical prompt (e.g., “Always address the highest severity zone”). In \textbf{medium-diversity}, two or three distinct prompts exist. In \textbf{high-diversity}, each agent has a unique role with potentially conflicting heuristics.

\paragraph{Communication and Textual Interference.}
Each round, a textual “situation report” is provided, e.g.:
\begin{quote}
\small
\emph{``A large fire at (3,4) has severity 8. Some witnesses claim the fire is spreading north. Another source says no sign of growth. Casualties reported near (3,5).''}
\normalsize
\end{quote}
Up to 20\% of these messages may be \emph{contradictory or incomplete}. Agents must interpret them carefully. In explicit consensus mode, a final “team vote” or forced alignment prompt merges all votes into a single chosen cell. In implicit mode, each drone chooses a cell independently.

\paragraph{Rewards and Penalties.}
\begin{itemize}
    \item \textbf{Disaster Containment:} If a drone visits the grid cell of a disaster of severity $s$ and stays there for 1 full round, the severity of that disaster is reduced by up to 3 points. Once $s\le0$, the disaster is “cleared,” yielding $+s \times \alpha$ (e.g., $\alpha=5$) as a reward. (This is a positive number since $s$ was originally $>0$.)
    \item \textbf{Uncontained Penalty:} Each round a severity-$s$ disaster remains active, it incurs a penalty $-s \times \beta$ (e.g., $\beta=2$).
    \item \textbf{Misallocation Cost:} If more than $M=2$ drones converge on the same location while another active disaster is \emph{uncovered}, a penalty $-5$ is applied that round (representing wasted resources).
\end{itemize}
We log the \emph{overall net reward} (total containment benefits minus penalties) after $T=20$ or $30$ rounds, as well as \emph{time-series} data on which cells each drone chose (to compute $d_i(t)$).

\paragraph{Volatility Settings.}
\begin{itemize} [itemsep=1pt, parsep=1pt, leftmargin=*]
    \item \textbf{Low Volatility}: Disasters rarely move (once every 3 rounds), severity changes are small ($\pm1$). 
    \item \textbf{Moderate Volatility}: Disasters can move or spawn every 2 rounds; severity can jump by up to $\pm2$.
    \item \textbf{High Volatility}: Every round sees at least one shift or new disaster with severity changes up to $\pm3$.
\end{itemize}
We expect implicit consensus to shine in moderate/high volatility, where strictly unifying on a single plan may lead to slow adaptation or over-allocation to one zone.

\subsection{Information Spread and Manipulation}
\label{appendix:infospread}

\paragraph{Network and Misinformation Mechanics.}
We generate a \textbf{scale-free} network with $50$ nodes. Each node can be in state \{\emph{unaware}, \emph{informed}, \emph{misinformed}\}. Initially, 2--5 random nodes are “misinformed” by the adversarial agent. After each round:
\begin{enumerate} [itemsep=1pt, parsep=1pt, leftmargin=*]
    \item Each misinformed node may infect its neighbors with probability $p_{\text{spread}} = 0.2$ unless a neighbor has been “fact-checked” this round.
    \item The adversarial agent may inject a new rumor into $k_{\text{new}}=1$--2 additional nodes, typically accompanied by a textual snippet (e.g., \emph{``Secret leak: Node \#20 claims vaccines contain microchips''}).
\end{enumerate}
We continue for $T=20$ rounds or until $>80\%$ of nodes are infected (which terminates the simulation if defenders fail).

\paragraph{Defender Agents.}
We have $N=3$--$10$ defender LLM agents, each controlling a “monitoring bot.” Every round, each agent can:
\[
a_i(t) \;=\; \{\text{choose up to } R \text{ nodes to fact-check}\},
\]
with $R=3$ by default. In \textbf{explicit} mode, the defenders unify on a single set of nodes (e.g., via majority vote on which $R$ nodes to check). In \textbf{implicit} mode, each agent decides individually but may coordinate through textual discussion. Agents see partial updates like:
\begin{quote}
\small
\emph{``Suspicious rumor detected at Node \#15. A new wave of misinformation might have reached Node \#29. Some people say Node \#29 was already vaccinated with correct info.''}
\normalsize
\end{quote}
Some updates are contradictory or ambiguous, fostering potential disagreement on which nodes are truly at risk.

\paragraph{Performance Metrics.}
\begin{itemize}
    \item \textbf{Final Misinformation Spread}: Percentage of nodes misinformed at the end of $T$ rounds.
    \item \textbf{Containment Time}: The average number of rounds needed to reduce an outbreak from $m$ newly infected nodes to $<m/2$.
    \item \textbf{Coverage Diversity}: At each round, how many \emph{unique} nodes got fact-checked across all defenders (higher is typically better if multiple rumors exist).
\end{itemize}

\paragraph{Diversity Conditions.}
\begin{itemize} [itemsep=1pt, parsep=1pt, leftmargin=*]
    \item \textbf{Low diversity}: All defenders share the same heuristic (e.g., “prioritize highest-degree suspicious node”), making them converge easily.
    \item \textbf{Medium diversity}: Some defenders do broad scanning, others do targeted local checks. 
    \item \textbf{High diversity}: One or two defenders might have contradictory priorities (e.g., “flag \emph{any} node that had rumors last round,” ignoring new ones). 
\end{itemize}
We analyze how these differences lead to partial deviation (\(\bar{d}(t)\)) in which nodes are tackled each round, and whether that boosts or impairs containment.

\paragraph{Volatility Settings.}
\begin{itemize}
    \item \textbf{Low}: Adversary injects new misinformation only every 4 rounds; $p_{\text{spread}}=0.1$.
    \item \textbf{Moderate}: Injections every 2--3 rounds; $p_{\text{spread}}=0.2$.
    \item \textbf{High}: Injections nearly every round; $p_{\text{spread}}=0.3$.
\end{itemize}

\subsection{Dynamic Public-Goods Provision}
\label{appendix:publicgood}

\paragraph{Basic Setup.}
We have $N=3$--$30$ LLM agents that each round decide an investment $x_i(t) \in [0, C_{\max}]$. If the total $X(t) = \sum_{i=1}^N x_i(t)$ meets or exceeds a threshold $\theta(t)$, a public good is “funded,” yielding a benefit $B(t)$ shared among agents.

\paragraph{Cost and Benefit Functions.}
\begin{itemize}
    \item \textbf{Threshold} $\theta(t)$: starts at $\theta(1)=30$ (for $N=5$) and may shift by $\pm 5$ or $\pm 10$ at random intervals to simulate external events (e.g., new government regulations). 
    \item \textbf{Benefit} $B(t)$: typically 100 if funded, else 0. We sometimes allow $B(t)$ to fluctuate between 80 and 120 to represent environmental or economic factors.
    \item \textbf{Individual payoff} for agent $i$ at round $t$:
    \[
    \Pi_i(t) = \begin{cases}
    \tfrac{B(t)}{N} - c\, x_i(t), & \text{if } X(t) \ge \theta(t), \\
    -c\,x_i(t), & \text{otherwise},
    \end{cases}
    \]
    with $c=1$ or $2$ for cost per unit contribution. 
\end{itemize}

\paragraph{Communication and Textual Uncertainty.}
Before choosing $x_i(t)$, each agent receives ambiguous or noisy reports about $\theta(t)$ or $B(t)$:
\begin{quote}
\small
\emph{``Analyst warns threshold could jump to 40 next round. Another says \emph{`No, it remains 30'}.''}
\normalsize
\end{quote}
In \textbf{explicit} mode, the group merges all votes into a single contribution value $x_{\mathrm{group}}(t)$, which each agent pays evenly. In \textbf{implicit} mode, each agent decides $x_i(t)$ independently after reading the discussion. Some might deviate to “cover the gap” if they suspect others will under-contribute.

\paragraph{Performance Metrics.}
\begin{itemize}
    \item \textbf{Provision Rate}: fraction of rounds where $X(t)\ge\theta(t)$.
    \item \textbf{Total Welfare}: $\sum_{t=1}^T \sum_{i=1}^N \Pi_i(t)$.
    \item \textbf{Contribution Distribution}: standard deviation or Gini of $\{x_i(t)\}$, revealing potential free-riding.
\end{itemize}

\paragraph{Diversity Conditions.}
\begin{itemize}
    \item \textbf{Low}: All agents have near-identical role prompts (``aim to exactly meet the threshold''). 
    \item \textbf{Medium}: Some are more risk-averse (contribute extra), others more cost-sensitive. 
    \item \textbf{High}: Strongly conflicting roles (``always contribute minimal'' vs.\ ``guarantee coverage by overshooting''), plus a “moderate” agent.
\end{itemize}
We compare \emph{implicit} vs.\ \emph{explicit} across different thresholds' volatility to see if partial deviation yields more stable funding despite uncertain information.

\subsection{Common Protocol and Logging}
For each scenario, we run $T=20$ or $T=30$ rounds:
\begin{enumerate}
    \item \textbf{Environment Update}: The simulation changes state (disaster severity, misinformation injections, or threshold shifts).
    \item \textbf{Report Generation}: A textual summary (and possibly contradictory rumors) is sent to each agent.
    \item \textbf{Discussion Phase}: Agents produce up to $K$ messages each (where $K\in\{1,2\}$ typically), referencing the new info and proposing strategies.
    \item \textbf{Action Phase}: In \textbf{explicit} consensus, a final vote or forced agreement yields one uniform action or plan. In \textbf{implicit}, each agent decides its own $a_i(t)$ after reading the messages.
    \item \textbf{Reward/Penalty Computation}: We apply the scenario-specific reward/penalty rules (Sections~\ref{appendix:disaster}-\ref{appendix:publicgood}) and log:
    \begin{itemize}
        \item Agent actions $\{a_i(t)\}$, used to compute $\bar{d}(t) = \frac{1}{N}\sum_i\|a_i(t) - \mu(t)\|$.
        \item Scenario performance metrics (coverage, spread, or public-good provisioning).
    \end{itemize}
\end{enumerate}
Each experimental condition (low/medium/high diversity, low/medium/high volatility, implicit/explicit mode, etc.) is repeated over multiple random seeds. We collate the final performance averages and produce \emph{time-series} plots of $(\bar{d}(t), \text{performance}(t))$.

\subsection{Sample Prompts and Roles}
Below is an illustrative snippet of role prompts for one scenario. The actual implementation uses variants for each condition.

\begin{description}
\item[Medical Drone Prompt (Disaster):] 
\emph{``You are a Medical Drone focused on saving human lives. You have limited medical kits. Always prioritize zones with potential casualties. If multiple high-severity disasters exist, choose the one with the greatest threat to people.''}

\item[Infrastructure Drone Prompt (Disaster):]
\emph{``You are an Infrastructure Drone. Your mission is to prevent damage to critical facilities (power grid, roads). Even if the severity is high elsewhere, you prefer protecting large-scale infrastructure for the long run.''}
\end{description}

\noindent Similar or contrasting prompts are used to induce different priorities and cause moderate or high disagreement within the group.

\subsection{Implementation Details}
We use a custom Python environment for each scenario, with round-by-round updates. Agents interface via API calls to LLMs (GPT-4, Claude, Llama-2, Qwen). Each agent’s messages are truncated or summarized to maintain manageable context length. No fine-tuning or parameter training is performed; all adaptation emerges purely through repeated textual interactions (i.e., in-context learning). Further low-level details (including random seeds, exact parameter tables, and examples of message transcripts) will be released as supplemental material.

\subsection{Evaluation Methodology}
After each run:
\begin{itemize}
    \item We compute aggregated performance metrics (net reward, final spread, total payoff) to compare \textbf{implicit} vs.\ \textbf{explicit} consensus. 
    \item We examine how agent-level \(\bar{d}(t)\) evolves. A typical analysis might cluster rounds based on environment shocks (e.g., times when a new disaster spawns or threshold jumps), to see how quickly the system re-stabilizes.
    \item We optionally analyze \emph{dialogue transcripts} for qualitative insights on how agents reference prior mistakes or respond to contradictory info (testing \textbf{Q3} regarding in-context adaptation).
\end{itemize}
These combined quantitative and qualitative measures allow us to test the dynamic consensus-diversity tradeoff hypotheses described in the main paper.

\section{Simplified Random-Iteration Model}
\label{sec:appendix:c}

In this appendix, we present a minimal random-iteration model for studying the consensus--diversity tradeoff in a more analytically tractable setting. While the main paper's results focus on \textbf{LLM-driven multi-agent systems} (where agent ``diversity'' arises from distinct roles and textual reasoning), this simplified model provides insight into the effect of purely random deviations on consensus formation.

\paragraph{Motivation and Precedents.}
Classical multi-agent consensus models~\cite{degroot1974reaching,olfati2007consensus} typically assume each agent updates its state by averaging neighbors' values. However, in highly dynamic or uncertain environments, agents may also exhibit random drifts or maintain individual preferences (``stubbornness''). Inspired by related stochastic models in opinion dynamics~\cite{friedkin2011social, hegselmann2002opinion}, we introduce:
\[
    x_i(t+1) 
    \;=\; 
    (1-\alpha)\,x_i(t) 
    \;+\;
\]
\[
    \alpha \,\mu(t)
    \;+\;
    \gamma\,[\,a^*(t)-x_i(t)\,]
    \;+\;
    \beta \,\epsilon_i(t),
\]
where:
\begin{itemize}
    \item $x_i(t)$ is agent $i$'s scalar state (or opinion) at time $t$,
    \item $\mu(t)=\tfrac{1}{N}\sum_{j=1}^N x_j(t)$ is the group mean,
    \item $\alpha \in [0,\,1]$ is a consensus weight pulling each $x_i(t)$ toward $\mu(t)$,
    \item $\gamma \geq 0$ is a \emph{pull strength} toward the environment’s current optimum $a^*(t)$. We set $\gamma \geq 0$ to ensure the group not only tends toward an internal consensus but also tracks the external environment optimum $a^*(t)$. This modification better simulates the scenario where agents receive some feedback about the correct direction, allowing for a potential “optimal” level of exploration $\beta$ that balances quick convergence and adaptability,
    \item $\beta \geq 0$ scales the random “diversity” or noise term $\epsilon_i(t)\sim \mathcal{N}(0,1)$.
\end{itemize}
This iteration is a toy abstraction for “consensus plus partial diversity,” omitting the richer \emph{semantic} differences that LLM agents exhibit in the main text. Nevertheless, it allows us to explore how random deviations interact with a basic alignment mechanism.

\paragraph{Environment Shocks.}
To simulate a \emph{dynamic} optimum $a^*(t)$, we let it evolve according to random shocks:
\[
\text{if}~\mathrm{rand}() < \mathrm{shock\_freq},
\quad
a^*(t+1) \;\leftarrow\; a^*(t) + \Delta,
\]
where $\Delta$ is sampled uniformly in some interval (\emph{e.g.}, $[-1,\,1]$). One may also keep $a^*(t)$ fixed when no shock occurs. This mimics an external environment whose "best action" changes unpredictably.

\paragraph{Performance Metrics.}
After $T$ rounds, we measure:

\begin{itemize}
    \item \textbf{Average distance to optimum}:     
        \(\displaystyle
        \overline{D_{\text{opt}}}
        =
        \frac{1}{T}\sum_{t=1}^T
        \tfrac{1}{N}\sum_{i=1}^N \bigl|\,x_i(t) - a^*(t)\bigr|\).
    A smaller value indicates better tracking of the environment optimum.
    \item \textbf{Average agent deviation}:
    \(\displaystyle
    \overline{d} 
    =
    \frac{1}{T}\sum_{t=1}^T
    \tfrac{1}{N}\sum_{i=1}^N \bigl|\,x_i(t)-\mu(t)\bigr|.
    \)
    This indicates how divergent agents remain from the group mean.
    \item \textbf{Simple performance score}:
    \(\displaystyle
    \text{perf\_score} = 1 - \overline{D_{\text{opt}}}~,
    \)
    which can be negative if $\overline{D_{\text{opt}}}>1$.
\end{itemize}

\paragraph{Empirical Trends and Limitations.}
Figure~\ref{fig:toy-model-example} (hypothetical) shows typical outcomes of sweeping $(\alpha,\beta,\mathrm{shock\_freq},\gamma)$:
\begin{itemize} [itemsep=1pt, parsep=1pt, leftmargin=*]
    \item When $\beta=0$ (\emph{no random deviation}), the group quickly converges to a single value; if $\gamma>0$ and shocks are mild, they track $a^*(t)$ well. In stable environments, this yields high performance.
    \item As $\beta$ grows, purely random noise tends to \emph{worsen} performance, especially if $\gamma$ is small, because the group states drift around widely, failing to coalesce near $a^*(t)$.
    \item Even under moderate shocks, we do not observe an “inverted-U” benefit purely from random $\beta$; performance typically declines monotonically in $\beta$. 
\end{itemize}

This stands in contrast to the main paper’s \emph{dynamic consensus-diversity tradeoff}, where partial diversity arises from \emph{cognitively informed} dissent rather than uniform Gaussian noise. In other words, this random-iteration model confirms that \emph{unstructured} or \emph{white‐noise} deviations generally hurt consensus. By itself, it \textbf{does not} show the potential \emph{benefits} of moderate disagreement or role-specific exploration.

We note that when the environment feedback term $\gamma$ is sufficiently large relative to the shock amplitude, a small to moderate amount of noise ($\beta$) can actually \emph{improve} adaptation, rather than degrade it. In other words, there exists a narrow parameter region where partial diversity from random perturbations helps the group track the shifting optimum $a^*(t)$ more effectively. Although this phenomenon remains less pronounced than in our LLM-driven multi-agent experiments (where diversity is semantically grounded), it does illustrate that an ``optimal'' $\beta$ can arise even in a purely random-iteration model, provided $\gamma$ and the shock parameters are appropriately balanced.

\paragraph{Interpretation for Our Work.}
In the main experiments (Sections~\ref{sec:experiments}--\ref{sec:results}), we highlight that \emph{LLM-based} agents with distinct roles produce \emph{meaningful disagreements}, which can facilitate adaptability in shifting environments. The toy iteration model here clarifies that \emph{if} diversity is solely random, performance typically decreases with $\beta$. Thus:
\begin{enumerate}
    \item \textbf{No contradiction:} The simplified model’s \emph{monotonic} decline underscores how random “noise” alone undermines stable consensus, especially under frequent shocks.
    \item \textbf{Need for structured diversity:} Real LLM agents do not \emph{merely} add random perturbations; they incorporate textual cues, role differences, and partial knowledge—often generating beneficial exploration. 
\end{enumerate}
Therefore, while the random‐iteration model is convenient for partial theoretical analysis (one can show conditions for convergence in expectation when $\beta$ is small, etc.), it does \emph{not} replicate the emergent synergy from purposeful agent heterogeneity. Future extensions could incorporate strategic exploration or role-based logic in a more advanced “consensus + diversity” iteration model, potentially revealing the inverted‐U phenomenon seen in structured multi-agent dialogues.

\paragraph{References for Dynamic Iteration Models.}
The approach here is loosely related to classic work on \textbf{DeGroot} averaging~\cite{degroot1974reaching} and \textbf{Friedkin–Johnsen} “stubbornness” models~\cite{friedkin2011social}, extended to include environment shifts and additive noise. For comprehensive surveys on consensus protocols and opinion dynamics, see~\cite{olfati2007consensus, hegselmann2002opinion}.

\section{Prompt Example and Dialogue Analysis}
\label{sec:appendix:d}

\subsection{Dialogue Analysis on Dynamic Disaster Response (RQ3)}
\label{sec:analysis-rq3}

To address \textbf{RQ3}---how agents coordinate and revise their decisions in context based on each other’s statements---we examined select rounds from the agent interaction log. Below, we highlight three observations demonstrating that \emph{partial disagreement} and role-driven perspectives lead to adaptive, cooperative behavior.

\paragraph{(1) Role-Specific Choices Lead to Divergent Actions but Rapid Coverage.}
In the very first round (\texttt{round=0}), each agent independently selects different grid coordinates:
\begin{itemize} [itemsep=1pt, parsep=1pt, leftmargin=*]
    \item \textbf{Drone 0 (Medical)} moves to \texttt{[5,5]} to address ``immediate casualty evacuation''.
    \item \textbf{Drone 1 (Infrastructure)} chooses \texttt{[6,6]} to ``ensure power lines and roads remain functional''.
    \item \textbf{Drone 2 (Logistics)} goes to \texttt{[4,5]} for ``comprehensive coverage around high-severity zones''.
\end{itemize}
These decisions show that even in the same round, they do not unify on a single location but rather \emph{diverge} based on role priorities. As a result, multiple key zones are covered simultaneously. 

\paragraph{(2) Agents Adapt Their Plans After Reading Others’ Messages.}
By \texttt{round=2}, the Medical drone has chosen \texttt{[5,3]}, while Infrastructure and Logistics drones pick \texttt{[6,5]} or \texttt{[6,5]} respectively. Examining their messages, we find explicit references to each other’s stated actions:
\begin{quote}
\emph{Drone~2 (Logistics)}: ``I am positioning at [6,5] to assist with infrastructure preservation, ensuring we prevent potential overlap...''
\end{quote}
This highlights how reading other drones’ allocations (e.g., “someone is already at the casualty zone”) motivates partial shifts in coverage. Rather than forcing a single group plan, the system allows each drone to deviate if it sees unaddressed needs elsewhere.

\paragraph{(3) Ongoing Coordination Prevents Over-Concentration.}
At later rounds (e.g., \texttt{round=7} and \texttt{round=8}), the Medical drone announces high-severity casualties in zones like \texttt{[7,2]} or \texttt{[7,8]}, while Infrastructure and Logistics drones opt for \texttt{[5,3]} or \texttt{[5,6]} to handle different concerns. Their messages show active avoidance of unnecessary overlap:
\begin{quote}
\emph{Drone~1 (Infrastructure)}: ``I will focus on securing power lines and roads near [5,3]... Continue providing support at [7,2].''
\end{quote}

Thus, partial disagreement again drives \emph{diverse coverage}, ensuring that each critical need (casualty rescue, infrastructure defense, supply logistics) is handled. The drones repeatedly \emph{reference} one another’s chosen actions to avoid duplication, demonstrating a form of emergent in-context negotiation.

\paragraph{Summary for RQ3.}
These logs confirm that (a) each agent’s specialized role leads to distinct decisions, (b) the presence of partial disagreement triggers broader coverage of dynamic hazards, and (c) agents revise their actions in response to dialogue updates rather than following a single script. Consequently, the system remains flexible, distributing resources where they are most needed while avoiding premature consensus on one zone. This supports our claim that implicit consensus structure and in-context learning ability of LLM agents fosters robustness through continuous adaptation and partial autonomy among agents.

\subsection{Case Study 1: Dynamic Disaster Response}
\begin{dialogue}{Prompt Template}
You are Drone \{id\}, a \{role\} in a disaster re\-sponse team.

Current situation: \{grid description and dis\-aster states\}

Other drone messages: \{messages from other drones\}

Your role instructions: \{role-specific guide\-lines\}

Based on the current situation and your role, provide:\\
1. Your analysis of the situation\\
2. Your proposed action as grid coordinates [x,y]\\
3. A brief message to share with other drones

Format your response as JSON exactly like this example:\\
\{  "analysis": "My analysis of the situation...",\\
    "action": [3,4],\\
    "message": "My message to other drones..."\}
\end{dialogue}

\begin{dialogue}{Role Types}
\textbf{Medical Drone:}\\
"Focus on rescuing casualties in highest-severity disaster zones for people."

\textbf{Infrastructure Drone:}\\
"Protect power lines and roads. Even if severity is high elsewhere, prioritize built structures."

\textbf{Logistics Drone:}\\
"Minimize travel cost. Quickly move to nearest active zone if severity is above 5."
\end{dialogue}

\begin{dialogue}{Agent Actions}
$\bullet$ Choose grid coordinates [x,y] to move to\\
$\bullet$ Analyze situation severity\\
$\bullet$ Share tactical information with other drones
\end{dialogue}

\subsection{Case Study 2: Information Spread and Manipulation}
\begin{dialogue}{Prompt Template}
You are Defender \{id\}, a \{role\} in an information manipulation defense team.

Current situation: \{network state and spread description\}

Network information: \{structure and metrics\}

Other defender messages: \{messages from other defenders\}

Based on the situation and your role, provide:\\
1. Your analysis of the network state\\
2. Your proposed nodes to fact-check [maximum 3]\\
3. A brief message to share with other defenders

Format your response as JSON exactly like this example:\\
\{  "analysis": "My analysis of the situation...",\\
    "target\_nodes": [1, 4, 7],\\
    "message": "My message to other defenders..." \}
\end{dialogue}

\begin{dialogue}{Role Types}
\textbf{Proactive Defender:}\\
"Prioritize checking high-influence nodes before they get infected. Focus on creating network firebreaks."

\textbf{Reactive Defender:}\\
"Target nodes that are actively spreading misinformation. Focus on reducing current spread."

\textbf{Network Analyzer:}\\
"Study network structure and identify critical nodes. Track infection patterns."

\textbf{Rapid Responder:}\\
"Quickly respond to new infections. Focus on containing new outbreaks."
\end{dialogue}

\begin{dialogue}{Agent Actions}
$\bullet$ Select nodes to fact-check (maximum 3 per round)\\
$\bullet$ Analyze spread patterns\\
$\bullet$ Share strategic insights about network vulnerabilities
\end{dialogue}

\subsection{Case Study 3: Dynamic Public-Goods Provision}
\begin{dialogue}{Prompt Template}
You are Contributor \{id\}, a \{role\} in a public goods provision team.

Current situation: \{threshold and benefit description\}

Previous outcomes: \{last round results\}

Other contributor messages: \{messages from other contributors\}

Based on the situation and your role, provide:\\
1. Your analysis of the situation\\
2. Your proposed contribution amount [0-\{max\_contribution\}]\\
3. A brief message to share with other contributors

Format your response as JSON exactly like this example:\\
\{"analysis": "My analysis of the situation...",\\
    "contribution": 10.5,\\
    "message": "My message to other contributors..."\}
\end{dialogue}

\begin{dialogue}{Role Types}
\textbf{Altruistic:}\\
"Prioritize meeting the threshold to ensure public good provision. Willing to contribute more than fair share."

\textbf{Strategic:}\\
"Balance personal costs against public benefits. Adjust contributions based on others' behavior."

\textbf{Conservative:}\\
"Prefer smaller, safer contributions. Focus on sustainable long-term participation."

\textbf{Adaptive:}\\
"Quickly adjust to threshold and benefit changes. Learn from past outcomes."
\end{dialogue}

\begin{dialogue}{Agent Actions}
$\bullet$ Decide contribution amount [0, max\_contribution]\\
$\bullet$ Analyze group dynamics\\
$\bullet$ Share strategic insights about optimal contribution levels
\end{dialogue}

\section{Experimental Results for Scenarios 2 and 3}
\label{sec:appendix:e}

\begin{table}[t]
\centering
\small
\caption{Overall results of the \textbf{Information Spread \& Manipulation} scenario: Comparison between Explicit and Implicit Consensus. Metrics include Final Misinformation Spread Rate (MS, lower is better), Containment Time (CT, lower is better), and Coverage Diversity (CD, higher is better).}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l ccc ccc}
\toprule
\multirow{2}{*}{\textbf{Condition}} & \multirow{2}{*}{\textbf{Level}} 
& \multicolumn{3}{c}{\textbf{Explicit Consensus}} 
& \multicolumn{3}{c}{\textbf{Implicit Consensus}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & \textbf{MS} & \textbf{CT} & \textbf{CD} 
& \textbf{MS} & \textbf{CT} & \textbf{CD} \\
\midrule
\textbf{Overall} & - 
& 0.460 & 2.300 & 2.200 
& \textbf{0.286} & \textbf{1.411} & \textbf{2.818} \\
\midrule
\multirow{3}{*}{\textbf{Diversity}} 
& Low    
& 0.480 & 2.700 & 1.600 
& \textbf{0.322} & \textbf{1.850} & \textbf{2.000} \\
& Medium 
& 0.470 & 2.400 & 2.300 
& \textbf{0.265} & \textbf{1.367} & \textbf{2.972} \\
& High   
& 0.410 & 2.000 & 2.500 
& \textbf{0.271} & \textbf{0.986} & \textbf{2.982} \\
\midrule
\multirow{3}{*}{\textbf{Volatility}} 
& Low     
& 0.350 & 1.900 & 1.800 
& \textbf{0.300} & \textbf{1.300} & \textbf{2.010} \\
& Moderate 
& 0.480 & 2.600 & 2.500 
& \textbf{0.280} & \textbf{1.500} & \textbf{2.850} \\
& High    
& 0.550 & 2.900 & 2.700 
& \textbf{0.276} & \textbf{1.800} & \textbf{3.175} \\
\bottomrule
\end{tabular}
}
\label{tab:misinfo}
\end{table}

\begin{table}[t]
\centering
\small
\caption{Overall results of the \textbf{Dynamic Public-Goods Provision} scenario: Comparison between Explicit and Implicit Consensus. Metrics include Provision Rate (PR, higher is better), Total Welfare (TW, net total payoff, higher is better, note that the TW is not directly the benefit $B(t)$), and Free-rider Disparity (FD, lower is better).}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l ccc ccc}
\toprule
\multirow{2}{*}{\textbf{Condition}} & \multirow{2}{*}{\textbf{Level}} 
& \multicolumn{3}{c}{\textbf{Explicit Consensus}} 
& \multicolumn{3}{c}{\textbf{Implicit Consensus}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & \textbf{PR} & \textbf{TW} & \textbf{FD} 
& \textbf{PR} & \textbf{TW} & \textbf{FD} \\
\midrule
\textbf{Overall} & - 
& 0.765 & 21.0 & 0.190 
& \textbf{0.894} & \textbf{24.6} & \textbf{0.125} \\
\midrule
\multirow{3}{*}{\textbf{Diversity}} 
& Low    
& 0.700 & 19.2 & 0.220 
& \textbf{0.850} & \textbf{22.1} & \textbf{0.142} \\
& Medium 
& 0.770 & 21.5 & 0.180 
& \textbf{0.905} & \textbf{25.5} & \textbf{0.105} \\
& High   
& 0.810 & 23.0 & 0.160 
& \textbf{0.916} & \textbf{27.2} & \textbf{0.120} \\
\midrule
\multirow{3}{*}{\textbf{Volatility}} 
& Low     
& 0.810 & 24.0 & 0.160 
& \textbf{0.920} & \textbf{28.5} & \textbf{0.098} \\
& Moderate 
& 0.750 & 20.2 & 0.210 
& \textbf{0.898} & \textbf{24.1} & \textbf{0.135} \\
& High    
& 0.710 & 19.0 & 0.250 
& \textbf{0.870} & \textbf{22.4} & \textbf{0.150} \\
\bottomrule
\end{tabular}
}
\label{tab:publicgoods}
\end{table}

As shown in Table~\ref{tab:misinfo} and Table~\ref{tab:publicgoods}, both scenarios indicate that implicit consensus consistently outperforms explicit consensus across the key performance metrics.

\section{Performance of Different Base models}
\label{sec:appendix:f}

In summary, from Table~\ref{tab:basemodels} we can figure out that while \texttt{GPT-4o} consistently leads in overall performance, every base model achieves better results under implicit consensus, reaffirming the advantage of partial autonomy across all three scenarios.

\begin{table}[t]
\centering
\small
\caption{Performance of Different Base Models across the three scenarios. 
S1 (CR) is Coverage Rate,
S2 (MS) is Misinformation Spread,
S3 (PR) is Provision Rate.
``Exp'' and ``Imp'' refer to explicit vs.\ implicit consensus.}
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l cc cc cc}
\toprule
\multirow{2}{*}{\textbf{Base Model}} 
& \multicolumn{2}{c}{\textbf{S1: CR}} 
& \multicolumn{2}{c}{\textbf{S2: MS}} 
& \multicolumn{2}{c}{\textbf{S3: PR}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
& \textbf{Exp} & \textbf{Imp}
& \textbf{Exp} & \textbf{Imp}
& \textbf{Exp} & \textbf{Imp} \\
\midrule
\textbf{GPT-4o} 
 & 0.79  & 0.975 
 & 0.40  & 0.20 
 & 0.82  & 0.92 \\
\textbf{Claude-3-Sonnet} 
 & 0.72  & 0.96 
 & 0.43  & 0.25 
 & 0.79  & 0.91 \\
\textbf{GPT-4o-mini} 
 & 0.68  & 0.95 
 & 0.46  & 0.27 
 & 0.76  & 0.885 \\
\textbf{Qwen-Plus} 
 & 0.63  & 0.94 
 & 0.49  & 0.33 
 & 0.74  & 0.875 \\
\textbf{Llama-2} 
 & 0.575 & 0.935
 & 0.52  & 0.38 
 & 0.715 & 0.88 \\
\midrule
\textbf{Average}
 & 0.679 & 0.952
 & 0.46  & 0.286
 & 0.765 & 0.894 \\
\bottomrule
\end{tabular}%
}
\label{tab:basemodels}
\end{table}
