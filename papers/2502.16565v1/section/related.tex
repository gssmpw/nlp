\section{Related Work}

\paragraph{Emerging Role of LLMs in Multi-Agent Systems} Recent advances have started leveraging LLMs as autonomous agents within MAS~\cite{chen-etal-2024-llmarena,islam-etal-2024-mapcoder,wang-etal-2024-rethinking-bounds}. Unlike traditional MAS with fixed protocols, LLM-based agents can dynamically communicate and coordinate via natural language, enabling more flexible collaboration. Early demonstrations show that multiple LLM agents working together can solve complex tasks beyond the capability of a single model. For example, frameworks like \texttt{CAMEL} employ two ChatGPT-based agents in complementary roles (e.g. user and assistant) to cooperatively complete tasks through iterative dialogue \cite{li2023camel}. Similarly, HuggingGPT-style approaches orchestrate multiple specialized models guided by an LLM, hinting at the potential of MAS-driven problem solving. More recently, Generative Agents have been introduced as an application of LLM-based MAS in interactive simulations of social environments \cite{gao2024large,huang2024social,park2023generative}. In this paradigm, dozens of LLM-driven agents simulated believable human behaviors and social interactions over time, demonstrating new use cases of MAS in social simulations and digital environments.

\paragraph{Collaboration and Consensus Mechanisms in LLM-Based MAS} A key challenge in LLM-based MAS is designing effective collaboration and consensus mechanisms among agents. Before the emergence of reinforcement learning-based or LLM-driven agents, game theory models like Nash equilibrium is a basic form of implicit agreement, serving as a foundational model with numerous game-theoretic applications in consensus formation~\cite{fujita2014approach,pramanik2021consensus,ye2017distributed}. Recently, various patterns of multi-agent interaction, especially LLM-based, have been explored to boost collective reasoning. For instance, a debate or deliberation protocol allows agents to propose ideas, critique each other, and reach a joint conclusion \cite{liu2024groupdebate,zhang-etal-2024-exploring}. This method, inspired by social psychology, can improve reasoning through agent discussion and argumentation. Wang et al.~\cite{wang-etal-2024-rethinking-bounds} recently re-evaluated the claim that multi-agent LLM discussions inherently improve reasoning abilities. Their findings suggest that while multi-agent discussion can help on certain problems, its effectiveness depends on careful design of the interaction (e.g. turn-taking, prompting strategies) and the difficulty of the task.

Other work has focused on hierarchical or specialized agent roles. For example, the \texttt{MAgICoRe} framework assigns LLM agents distinct roles such as Solver and Reviewer that interact in a coarse-to-fine refinement loop, leading to improved answer quality via iterative feedback \cite{Chen2024magicore}. This approach implicitly enables an ensemble of agent "opinions" to be refined into a more accurate consensus. In a similar vein, prior studies on chain-of-thought prompting have shown that generating multiple reasoning paths and then using a voting or selection process (often called \emph{self-consistency}) can significantly improve solution correctness \cite{Wang2023selfconsistency}. Such strategies can be seen as a form of consensus-reaching among multiple reasoning attempts or agents at inference time. Further, some existing work utilizes multi-agent-based Human-AI interaction to facilitate cooperation and consensus formation while reducing negative factors such as disagreements and social inhibition~\cite{nomura2024towards}.

Beyond consensus, mechanisms for information sharing and knowledge integration are being studied. A recent survey of LLM-based multi-agent systems \cite{Guo2024survey} summarizes that effective communication protocols, memory sharing, and conflict resolution strategies are pivotal for coordinating LLM agents. Ensuring consistency of shared knowledge (and avoiding issues like knowledge drift or misinformed perspectives among agents) is an open research challenge \cite{zhang-etal-2024-exploring,Guo2024survey}.

\paragraph{Challenges and Advances in Knowledge Integration for LLM-Based MAS} LLM-based MAS research builds on a history of multi-agent collaboration in AI and cognitive science. Classic MAS explored emergent communication and cooperative learning, with reinforcement learning agents developing coordination protocols \cite{Foerster2016communicate}. These studies foreshadowed the advanced natural language cooperation of LLM agents. However, LLMs introduce both opportunities and challenges: their pre-trained knowledge enables implicit coordination, but alignment control is crucial. Recent work emphasizes safeguards against error propagation and bias in agent communication \cite{wang-etal-2024-rethinking-bounds,Guo2024survey}. Meanwhile, LLMs' nuanced interpretation enhances human-like negotiation and problem-solving beyond traditional MAS.

\paragraph{Balancing Consensus and Diversity in Group Problem-Solving} Research in social and cognitive sciences highlights the necessity of balancing consensus and diversity in group problem-solving. While consensus enables coordination, diversity fosters creativity and robustness. Probabilistic opinion dynamics studies suggest that evolving opinions dynamically aids decision-making~\cite{liu2022probabilistic}. Hong \& Page~\cite{Hong2004diversity} show that diverse agent groups can outperform homogeneous high-performers on complex problems. In LLM-based MAS, fostering diverse hypotheses before convergence enhances outcomes. Smaldino et al.~\cite{Smaldino2024diversity} emphasize "transient diversity," where delayed convergence improves problem-solving. Inspired by these findings, MAS research is designing protocols that integrate diverse reasoning while ensuring eventual coherence.
