
We consider a classical benchmark in nonlinear dispersive waves which  describes how waves propagate through shallow water. More precisely, we consider the propagation 
of a soliton in a one-dimensional domain with periodic boundary conditions, described by the 
 Korteweg-de Vries (KdV) equation 

\begin{align*}
    \frac{\partial u}{\partial t} + 4 u \frac{\partial u}{\partial x} + \frac{\partial^3 u}{\partial x^3} = 0,
\end{align*}
where $ u : [-\pi, \pi] \times [0, 1] \to \mathbb{R}$ denotes the flow velocity of the wave. The initial condition given by $ u_0(x) = 1 + 24\,  \text{sech}^2(\sqrt 8 x) $ \cite{geelen2024}. We consider the manifold  $K = \{u(t) : t\in [0,1]\}$. 
Functions are evaluated on a grid of $D= 256$ equispaced points over the spatial domain $ [-\pi, \pi] $ and identified with vectors in $X \in \Rbb^D$. The samples are collected by evaluating the solution $u(t)$ every $ \Delta t = 0.0002 $ time units over the time domain $ [0, 1] $, therefore resulting into $5001 $  samples (initial condition included). The first $m= 1001 $ samples, corresponding to times $ t \in [0, 0.2] $ are taken as training data, and we use as test samples the remaining $4000$ samples corresponding to times $t\in [0.2,1]$. 

We perform various experiments on KdV to illustrate the properties of the proposed method. 

\paragraph{Comparison with other methods.}

We  run our method CPN-LR with a target precision $\epsilon=10^{-4}$ and a polynomial degree $p=5$, which results in a manifold dimension $n=2$, and a dimension $N=43$. We emphasize that for CPN, the dimension $ n $ is selected by the adaptive algorithm, whilst it is fixed a priori for other methods. We compare different methods in Table \ref{tab:kdv_comparison_table} for the same manifold dimension $ n = 3 $. We observe that CPN-LR outperforms other methods by more than three orders of magnitude. We note that Sparse an Low-Rank already provided a significant improvement compared to other methods from the literature. 

\begin{table}[h]
\centering % Adjust line thickness
%\setlength{\tabcolsep}{10pt}      % Adjust cell padding
%\renewcommand{\arraystretch}{1.5} % Adjust row height   

\begin{tabular}{|c|c|c|c|c|c|}
\hline
 Method & $p$  & $n$ & $N$ & $ \text{RE}_{\text{train}} $ & $ \text{RE}_{\text{test}} $ \\ 
 \hline
 Linear & / &  2  & / & $6.63 \times 10^{-1}$  & $ 6.85 \times 10^{-1} $ \\ 
 \hline
 Quadratic &  2 & 2 & 5 & $ 5.33 \times 10^{-1}$  & $ 5.60 \times 10^{-1}$  \\ 
 \hline
Additive-AM & 5 & 2 & 43 & $ 3.84 \times 10^{-1}$  & $ 3.96 \times 10^{-1} $  \\
 \hline
 Sparse &  5 & 2 & 43 & $ 1.72 \times 10^{-1} $  & $ 1.82 \times 10^{-1} $  \\
 \hline
 Low-Rank & 5 & 2 & 43 & $7.47 \times 10^{-2}$ & $7.94 \times 10^{-2}$ \\
 \hline
 CPN-LR ($\epsilon = 10^{-4}$) &  5 & 2 & 43 & $ 6.73 \times 10^{-5} $ & $ 6.91 \times 10^{-5} $ \\ 
 \hline
\end{tabular}
\caption{(KdV) Comparison of methods for the same manifold dimension $ n = 2 $. For CPN, we use low-rank polynomials.}
\label{tab:kdv_comparison_table}
\end{table}

Figures \ref{fig:kdv_viz_solution} and \ref{fig:kdv_snapshots} illustrate the predicted solutions for the different methods. We observe that the solutions given by CPN perfectly predict the true solution over the whole time interval.  


\begin{figure}[h]
    \centering
    \subfigure[Exact solution]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_exact.png}}
    \subfigure[Linear]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_linear_approach.png}} 
    \subfigure[Quadratic]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_quadratic.png}} 
    \subfigure[Additive-AM]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_univariate_and_AM.png}} 
    \subfigure[Sparse]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_sparse_with_lasso.png}} 
    \subfigure[CPN-LR]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_cpn.png}} 
    \caption{(KdV) Predictions for different methods, with $ n = 2 $.}
    \label{fig:kdv_viz_solution}
\end{figure}

\begin{figure}
    \centering
    \subfigure[$ u(\cdot, t) $ at $ t = 0.5 $]{\includegraphics[width=0.4\textwidth]{New_figures/kdv_snapshot_t_0_5.png}} 
    \subfigure[$ u(\cdot, t) $ at $ t = 1 $]{\includegraphics[width=0.4\textwidth]{New_figures/kdv_snapshot_t_1.png}} 
    \caption{(KdV) Comparison between methods for two snapshots, with  $ n = 2 $.}
    \label{fig:kdv_snapshots}
\end{figure}



\paragraph{Influence of the polynomial degree.}

In Table \ref{tab:different_p}, we illustrate the influence of the polynomial degree $p$ on the results of CPN. We observe that a high value of $p$ allows to capture higher nonlinearities in the relations between coefficients $a_i(u)$, hence a lower manifold dimension $n$. 


% \begin{table}[h!]
% \centering % Adjust line thickness
% %\setlength{\tabcolsep}{10pt}      % Adjust cell padding
% %\renewcommand{\arraystretch}{1.5} % Adjust row height   

% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
% $p$ & $n$ & $N$ & $ \text{RE}_{\text{train}} $ & $ \text{RE}_{\text{test}} $ \\ 
%  \hline
% 3  & 9 & 43 & $ 7.401 \times 10^{-5}$  & $ 7.574 \times 10^{-5}$ \\ 
%  \hline
% 4 & 7 & 43 & $ 7.524 \times 10^{-5}$ & $ 7.720 \times 10^{-5}$ \\ 
%  \hline
% 5 & 5 & 43 & $ 7.170 \times 10^{-5} $ & $ 7.367 \times 10^{-5} $ \\
%  \hline
% \end{tabular}
% \caption{Results of CPN with different degrees $ p $ for $ \epsilon = 10^{-4} $}
% \label{tab:different_p}
% \end{table}

\begin{table}[h]
\centering % Adjust line thickness
%\setlength{\tabcolsep}{10pt}      % Adjust cell padding
%\renewcommand{\arraystretch}{1.5} % Adjust row height   

\begin{tabular}{|c|c|c|c|c|c|}
\hline
{Method} & $p$ & $n$ & $N$ & $ \text{RE}_{\text{train}} $ & $ \text{RE}_{\text{test}} $ \\ 
 \hline
\centering CPN-S & 3  & 9 & 43 & $ 7.40 \times 10^{-5}$  & $ 7.57 \times 10^{-5}$ \\ \cline{2-6}

    & 4 & 7 & 43 & $ 7.52 \times 10^{-5}$ & $ 7.72 \times 10^{-5}$ \\ \cline{2-6}

    & 5 & 5 & 43 & $ 7.17 \times 10^{-5} $ & $ 7.36 \times 10^{-5} $ \\
 \hline
 \centering CPN-LR & 3 & 4 & 43 & $ 6.88 \times 10^{-5}$ & $7.35 \times 10^{-5}$ \\ \cline{2-6}

    & 4 & 3 & 43 & $ 6.85 \times 10^{-5}$ & $ 7.05 \times 10^{-5}$  \\ \cline{2-6}

    & 5 & 3 & 43 & $ 7.13 \times 10^{-5}$ & $ 7.30 \times 10^{-5} $ \\
 \hline
\end{tabular}
\caption{(KdV) Results of CPN with different degrees $ p $ for $ \epsilon = 10^{-4} $.}
\label{tab:different_p}
\end{table}



\paragraph{Behavior of the algorithm.}

In Table  \ref{tab:sparse_table}, we illustrate the results for various target precisions $\epsilon$. We observe that the algorithm returns an approximation satisfying the desired precision, with increasing dimensions $n$ and $N$ and number of compositions as $\epsilon$ decreases.  
The results also suggest that using low-rank approximation can lead to a smaller $ n $ than sparse polynomial approximation, due to the higher approximation power of the former. Figure \ref{fig:pcnvssparse} shows the errors coefficient-wise for Sparse and for CPN-S. We can see the power of using compositions of polynomial maps. 
 
 
\begin{table}[h]
\centering % Adjust line thickness
%\setlength{\tabcolsep}{10pt}      % Adjust cell padding
%\renewcommand{\arraystretch}{1.5} % Adjust row height            % Set line color to blue


\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
{Precision} & {Method} & $n$ & $N$ & $N_{comp}$   & $ \text{RE}_{\text{train}} $  & $ \text{RE}_{\text{test}} $\\ \hline

\centering $\epsilon = 10^{-1}$ & CPN-S & 2 & 15 & 2 & $6.2 \times 10^{-3}$ & $6.4\times 10^{-2}$ \\ \cline{2-7}
                                
                                &  CPN-LR & 2 & 15 & 0 & $6. \times 10^{-2}$ & $6.2 \times 10^{-3}$ \\ \hline
$\epsilon = 10^{-2}$ &  CPN-S & 3 & 25 & 5 & $ 6.67 \times 10^{-3} $ & $ 6.84 \times 10^{-3} $\\ \cline{2-7}
                                
                        &  CPN-LR& 2 & 25 & 1 & $ 5.94 \times 10^{-3}$ & $ 6.12 \times 10^{-3}$ \\ \hline
$\epsilon = 10^{-3}$ &  CPN-S & 3 & 34 & 8 &  $ 6.83 \times 10^{-4} $ & $ 7 \times 10^{-4} $\\ \cline{2-7}
                                
                        &  CPN-LR & 2 & 34 & 4 & $ 6.26 \times 10^{-4} $ & $ 6.42 \times 10^{-4} $\\ \hline
$\epsilon = 10^{-4}$ &  CPN-S & 5 & 43 & 8 & $ 7.17 \times 10^{-5} $ & $ 7.36 \times 10^{-5}$  \\ \cline{2-7}
                                
                        &  CPN-LR & 2 & 43 & 5 & $ 7.13 \times 10^{-5} $ & $ 7.30 \times 10^{-5}$ \\ \hline
$\epsilon = 10^{-5}$ &  CPN-S & 6 & 52 & 11 & $ 6.76 \times 10^{-6}$ & $ 6.91 \times 10^{-6}$ \\ \cline{2-7}
                                
                        &  CPN-LR & 3 & 52 & 9 & 
 $ 6.30 \times 10^{-6} $ & $ 7.37 \times 10^{-6} $ \\ \hline
% $\epsilon = 10^{-6}$ &  CPN-S & 11 & 61 & 10 & $ 7.68 \times 10^{-7}$ & $ 7.88 \times 10^{-7} $\\ \cline{2-7}
                                
%                         &  CPN-LR & 6 & 61 & 12 & $ 7.25 \times 10^{-7} $ & $ 1.76 \times 10^{-5} $\\ \hline

\end{tabular}
\caption{(KdV) Comparison between CPN-S (sparse approximation) and CPN-LR (low-rank approximation) for $ p = 5 $ and various different values of target precision $\epsilon$. $N_{comp}$  indicates the maximum number of compositions.}
\label{tab:sparse_table}
\end{table}

 
\begin{figure}
    \centering
    \includegraphics[scale=0.3]{New_figures/kdv_coeffs_wise_error.png}
    \caption{(KdV) Coefficients errors for Sparse and CPN-S, with $p=5$ and $\epsilon=10^{-4}$.}
    \label{fig:pcnvssparse}
\end{figure}

 
 For CPN-S, the graphs of compositions for coefficients $a_{10}$, $a_{21}$ and $a_{41}$ can be visualized in Figure \ref{fig:kdv_graphs}. The coefficient $a_{10}$ is simply approximated in terms of the parameters $a$, whilst the coefficients $ a_{21} $ and $ a_{42} $ are expressed as compositions of polynomials. The full learning procedure for $ \epsilon = 10^{-4} $ and $ p = 5 $ is detailed in Table \ref{tab:kdv_learning_process}.


\begin{figure}
    \centering
    \subfigure[$a_{10}$]{\includegraphics[width=0.25\textwidth]{New_figures/kdv_graph_10.png}} 
    \subfigure[$ a_{21} $]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_graph_21.png}} 
    \subfigure[$ a_{42} $]{\includegraphics[width=0.3\textwidth]{New_figures/kdv_graph_42.png}} 
    \caption{(KdV) Compositional networks for different coefficients, using CPN-S with $\epsilon=10^{-4}$ and $p=5$.}
    \label{fig:kdv_graphs}
\end{figure}

\begin{table}[H]
\centering % Adjust line thickness
%\setlength{\tabcolsep}{10pt}      % Adjust cell padding
%\renewcommand{\arraystretch}{1.5} % Adjust row height   

\begin{tabular}{|c|c|c|}
\hline
\textbf{Step} & \textbf{Indices of input coeffs.} & \textbf{Indices of learnt coeffs.} \\
\hline
1 & 1 & / \\
\hline
2 & 1, 2 & / \\
\hline
3 & 1, 2, 3 & 8 \\
\hline
4 & 1, 2, 3, 4 & 6, 7 \\
\hline
5 & 1, 2, 3, 4, 5 & 10, 13 \\
\hline
6 & 1, 2, 3, 4, 5, \textbf 6 & 17, 22 \\
\hline
7 & 1, \ldots, 5, \textbf 6, \textbf 7 & 9, 11, 12, 14, 15, 16, 18, 19, 20 \\
\hline
8 & 1, \ldots, 5, \textbf{6, 7, 8} & 21, 24, 25, 27, 29 \\
\hline
9 & 1, \ldots, 5, \textbf{6, 7, 8, 9} & 23, 26, 28 \\
\hline
10 & 1, \ldots, 5, \textbf{6, \ldots, 10} & 30, 32 \\
\hline
11 & 1, \ldots, 5, \textbf{6, \ldots, 11} & 31, 34, 36, 38, 39 \\
\hline
12 & 1, \ldots, 5, \textbf{6, \ldots, 12} & 33, 35, 37 \\
\hline
13 & 1, \ldots, 5, \textbf{6, \ldots, 13} & 40, 41, 43 \\
\hline
14 & 1, \ldots, 5, \textbf{6, \ldots, 14} & 42 \\
\hline
\end{tabular}
\caption{(KdV) Learning procedure of CPN-S for $ p = 5 $ and $ \epsilon = 10^{-4}$. To reach the target precision,  $N=43$ is required. Coefficients are progressively learnt throughout the different steps of the algorithm. At step $ j $, $ a_j $ is added as input variable. If already learnt at a previous step, its approximation is used instead (indices in bold), leading to the compositional structure. The final dimension $ n = 5 $ corresponds to the number of coefficients that were not learnt during the process.}
\label{tab:kdv_learning_process}
\end{table}



\paragraph{Stability of the decoder.}

In Table \ref{tab:different_L}, we illustrate the influence of the prescribed upper bound $L$ for the Lipschitz constant, with prescribed precision $\epsilon=10^{-4}$ and polynomial degree $p=5$. The algorithm was able to construct an approximation satisfying the target precision and stability conditions.   
As expected, we observe that imposing a smaller Lipschitz constant results in a higher manifold dimension $n$. 

\begin{table}[H]
\centering % Adjust line thickness
%\setlength{\tabcolsep}{10pt}      % Adjust cell padding
%\renewcommand{\arraystretch}{1.5} % Adjust row height   

\begin{tabular}{|c|c|c|c|c|c|}
\hline
{Method} & $L$ & $n$ & $N$ & $ \text{RE}_{\text{train}} $ & $ \text{RE}_{\text{test}} $ \\ 
 \hline
\centering CPN-S & 2  & 14 & 43 & $ 6.99 \times 10^{-5}$  & $ 7.46 \times 10^{-5}$ \\ \cline{2-6}

    & 10 & 6 & 43 & $ 7.09 \times 10^{-5}$ & $ 7.29 \times 10^{-5}$ \\ \cline{2-6}

    & 100 & 5 & 43 & $ 7.17 \times 10^{-5} $ & $ 7.36 \times 10^{-5} $ \\
 \hline
 \centering CPN-LR & 2 & 12 & 43 & $6.99 \times 10^{-5}$ & $7.21 \times 10^{-5}$ \\ \cline{2-6}

    & 10 & 7 & 43 & $6.85 \times 10^{-5}$ & $7.05 \times 10^{-5}$ \\ \cline{2-6}

    & 100 & 3 & 43 & $ 7.13 \times 10^{-5}$ & $ 7.30 \times 10^{-5} $ \\
 \hline
\end{tabular}
\caption{(KdV) Results of CPN for different Lipschitz constants $L$, with $ \epsilon = 10^{-4} $ and $p=5$.}
\label{tab:different_L}
\end{table}