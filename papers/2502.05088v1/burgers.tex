We now assess the performance of the method on the solution manifold of the proposed 2D inviscid Burgers equation in \cite{Barnett2023Nov}, set over the space time domain $\Omega \times [0,T]$, with $\Omega = (0, 50)^2$, $T = 10$, and parametrized by a scalar parameter $\lambda \in \mathbb R$ involed in the boundary conditions.
Given $\lambda \in \mathbb R$, the equation governing the velocity field $(u_\lambda, v_\lambda) : \Omega \times [0,T] \rightarrow \Rbb^2$ reads
\begin{equation*}
    \left\{ 
    \begin{array}{l}
        \displaystyle
        \frac{\partial u_\lambda}{\partial t} + \frac{1}{2} \Big( \frac{\partial u_\lambda^2}{\partial x} + \frac{\partial u_\lambda v_\lambda}{\partial y}  \Big) = 0, 
        % \\[12pt]
        \qquad 
        \displaystyle 
        \frac{\partial v_\lambda}{\partial t} + \frac{1}{2} \Big(\frac{\partial u_\lambda v_\lambda}{\partial x} + \frac{\partial v_\lambda^2}{\partial y} \Big) = 0,
        \\[15pt]
        \begin{array}{ll}
            \text{initial conditions} &
            \\
            \quad u_\lambda(x, y, 0) = v_\lambda(x, y, 0) = 1, & x, y \in (0, 50),
            \\[8pt]
            \text{boundary conditions} &
            \\ 
            \quad u_\lambda(0, y, t) = \lambda, \quad v_\lambda(0, y, t) = 0, & \quad y \in (0, 50), \enspace t \in (0, T), 
            \\[5pt]
            \quad u_\lambda(x, 0, t) = 0, \quad v_\lambda(x, 0, t) = 0, & \quad x \in (0, 50), \enspace t \in (0, T).
        \end{array}
    \end{array}
    \right.
\end{equation*}
We consider here the solution manifold $ K = \big\{  (u_\lambda, v_\lambda): \lambda \in [1.5, 2.5] \big\} $, where $(u_\lambda, v_\lambda)$ are high-fidelity numerical solutions discretized on a regular grid of $ D = 250 \times 250 $ nodes in $\overline \Omega$ and sampled every $\Delta t = 0.03$ times units over the time domain $[0, 10]$.
The training samples are computed for $\lambda \in \{1.5, 1.7, 1.9, 2.2, 2.5\}$ and $t \in \{ k \Delta t \: : \: 0 \leq k \leq 300 \}$, resulting into $1501$ training data (the initial condition appearing without repetition).
For the test set, we uniformly sample $8$ additional values of $\lambda$ in the interval $[1.5, 2.5] $, yielding $2401$ test data.
 
We run our method CPN-S with a target precision $\epsilon =5.10^{-3}$ and  a polynomial degree $p=8$, which results in a dimension $N = 89$ and a manifold dimension $ n= 9$ (selected by the algorithm).
We  compare  different  methods  in  Table  \ref{tab:burgers_comparison_table}  for  the same manifold dimension $n= 9$.  
We again observe that CPN-S still performs better than SOTA methods.


\begin{table}[h]
\centering % Adjust line thickness
%\setlength{\tabcolsep}{10pt}      % Adjust cell padding
%\renewcommand{\arraystretch}{1.5} % Adjust row height   

\begin{tabular}{|c|c|c|c|c|c|}
\hline
 Method & $p$  & $n$ & $N$ & $ \text{RE}_{\text{train}} $ & $ \text{RE}_{\text{test}} $ \\ 
 \hline
 Linear & / &  9  & / & $5.60 \times 10^{-2}$  & $5.01 \times 10^{-2}$ \\ 
 \hline
 Quadratic &  2 & 9 & 54 & $ 2.46 \times 10^{-2}$  & $ 2.28 \times 10^{-2}$  \\ 
 \hline
Additive-AM & 8 & 9 & 89 & $ 1.91 \times 10^{-2}$  & $ 1.95 \times 10^{-2} $  \\
 \hline
  Low-Rank & 8 & 9 & 89 & $2.46\times 10^{-2}$ & $2.33 \times 10^{-2}$ \\
 \hline
 Sparse &  8 & 9 & 89 & $ 1.62 \times 10^{-2} $  & $ 1.62 \times 10^{-2} $  \\
 \hline
 CPN-S ($\epsilon = 5.10^{-3}$) &  8 & 9 & 89 & $ 4.74 \times 10^{-3} $ & $ 4.93 \times 10^{-3} $ \\ 
 \hline
\end{tabular}
\caption{(2D Burgers) Comparison of methods for the same manifold dimension $ n = 9 $.}
\label{tab:burgers_comparison_table}
\end{table}

Figure \ref{fig:burgers_viz_solution} shows the  solutions predicted by CPN-S for a particular $\lambda$ and two time values.

\begin{figure}[h!]
    \centering
    \subfigure[Exact solution, $t=5$]{\includegraphics[width=0.4\textwidth]{New_figures/burgers_exact_t_5.png}} 
    \subfigure[CPN-S, $t=5$]{\includegraphics[width=0.4\textwidth]{New_figures/burgers_cpn_t_5.png}} 
    \subfigure[Exact solution $t=9.7$]{\includegraphics[width=0.4\textwidth]{New_figures/burgers_exact_t_9.png}} 
    \subfigure[CPN-S, $t=9.7$]{\includegraphics[width=0.4\textwidth]{New_figures/burgers_exact_t_9.png}}
    \caption{(2D Burgers) Predictions of CPN-S with $ n = 9 $, for $\lambda = 2.15717$, at $t=5$ (top) and $t=9.7$ (bottom).}
    \label{fig:burgers_viz_solution}
\end{figure}

Figure \ref{fig:burgers_wise_viz_solution} shows errors for different methods for a particular value of $\lambda$. 


\begin{figure}[h!]
    \centering
    \subfigure[Linear]{\includegraphics[width=0.3\textwidth]{New_figures/Burgers_error_linear.png}} 
    \subfigure[Quadratic]{\includegraphics[width=0.3\textwidth]{New_figures/Burgers_error_quadratic.png}} 
    \subfigure[Additive-AM]{\includegraphics[width=0.3\textwidth]{New_figures/Burgers_error_univariate.png}} 
    \subfigure[Sparse]{\includegraphics[width=0.3\textwidth]{New_figures/Burgers_error_sparse.png}}
    \subfigure[CPN-S]{\includegraphics[width=0.3\textwidth]{New_figures/Burgers_error_cpn.png}}
    \caption{(2D Burgers) Pointwise errors for $ n = 9 $,  $\lambda = 2.15717$, $t=9.7$.}
    \label{fig:burgers_wise_viz_solution}
\end{figure}