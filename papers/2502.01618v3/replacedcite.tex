\section{Related Work}
% \paragraph{Process reward models}
\textbf{Process reward models} (PRMs) aim to provide more granular feedback by evaluating intermediate steps rather than only final outputs.
They are trained via process supervision, a training approach where models receive feedback on each intermediate step of their reasoning process rather than only on the final outcome.
____ propose a step-by-step verification approach to PRMs, improving the reliability of reinforcement learning.
DeepSeek PRM ____ uses Mistral to annotate training data for PRMs .
____ introduces Qwen-PRM, which combines both Monte Carlo estimation and model/human annotation approach to prepare training data for a PRM. %\todo{add deepseek and prime rm}
PRIME____ proposes to train an outcome reward model (ORM) using an implicit reward objective. The paper shows that implicit reward objective directly learns a Q-function that provides rewards for each token, which can be leveraged to create process-level reward signal. This process eliminates the need for any process labels, and reaches competitive performance on PRM benchmarks. 

% Without the need for any process label, implicit PRM is trained as an outcome reward model (ORM) and then used as a PRM.

% train a math process reward by only using outcome level signals through implicit reward.

% by using implicit reward from 

\textbf{Inference-time scaling} has been a key training-free strategy for enhancing LLM performance. ____ explores a best-of-N (BoN) decoding strategy, demonstrating improvements in output quality through selective refinement. ____ provides insights into how scaling compute resources can yield better inference efficiency from a compute optimality perspective.
While not implementing full Monte Carlo tree search (MCTS), ____ explores a tree-search-like approach within language models. 
Additionally, ____ introduces rSTAR, a method that combines MCTS for data generation and training to improve mathematical reasoning. ____ discusses beam search and dynamic variable-time search (DVTS) as inference-time scaling techniques to improve open-source LLMs. DVTS works by running multiple independent subtrees in parallel so to avoid all leaves stuck in local minima.

% \paragraph{Particle-based Monte Carlo methods}
\textbf{Particle-based Monte Carlo methods} are powerful tools for probabilistic inference. Sequential Monte Carlo ____ or particle filtering ____ has been the classical way to approximate complex posterior distributions over state-space models. Particle Gibbs (PG) sampling ____ extends these approaches by integrating MCMC techniques for improved inference. ____ and ____ introduce a probabilistic programming language that applies SMC methods to steer/constrain LLM generation. ____ and ____ introduce Twisted SMC methods for inference in language models. 

% * qwen PRM ____
% * lets verify step by step PRM ____
% * monkey llm (best-of-N) ____
% * compute optimality google paper ____
% * rstar (MCTS + training) ____
% * beam search and DVTS blog post ____

% * majority voting ____

% * not actual MCTS but closest i could get ____

% particle filtering ____ or sequence Monte Carlo ____
% particle Gibbs ____

% * majority voting: https://arxiv.org/pdf/2203.11171, Self-Consistency Improves Chain of Thought Reasoning in Language Models, ____

% * not actual MCTS but closest i could get: https://arxiv.org/abs/2310.04406, Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models, ____

% * DL scaling (training time) 
%     * ____ (Deep Learning Scaling is Predictable, Empirically)
%     * ____  https://arxiv.org/abs/2001.08361
%     * ____ Training Compute-Optimal Large Language Models, https://arxiv.org/abs/2203.15556