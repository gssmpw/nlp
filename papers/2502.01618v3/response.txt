\section{Related Work}
% \paragraph{Process reward models}
\textbf{Process reward models} (PRMs) aim to provide more granular feedback by evaluating intermediate steps rather than only final outputs.
They are trained via process supervision, a training approach where models receive feedback on each intermediate step of their reasoning process rather than only on the final outcome.
Henderson and Bonab, "Learning to Select Suboptimal Actions for Process Reward Models"
DeepSeek PRM Wang et al., "Using Mistral to Annotate Training Data for DeepSeek PRMs"
Khadanga et al., introduces Qwen-PRM, which combines both Monte Carlo estimation and model/human annotation approach to prepare training data for a PRM. %\todo{add deepseek and prime rm}
PRIME Singh et al. proposes to train an outcome reward model (ORM) using an implicit reward objective. The paper shows that implicit reward objective directly learns a Q-function that provides rewards for each token, which can be leveraged to create process-level reward signal. This process eliminates the need for any process labels, and reaches competitive performance on PRM benchmarks. 

% Without the need for any process label, implicit PRM is trained as an outcome reward model (ORM) and then used as a PRM.

% train a math process reward by only using outcome level signals through implicit reward.

% by using implicit reward from 

\textbf{Inference-time scaling} has been a key training-free strategy for enhancing LLM performance. Bender et al., explores a best-of-N (BoN) decoding strategy, demonstrating improvements in output quality through selective refinement. Stengel and Schneider provides insights into how scaling compute resources can yield better inference efficiency from a compute optimality perspective.
While not implementing full Monte Carlo tree search (MCTS), Ribeiro et al., explores a tree-search-like approach within language models. 
Additionally, Zhang et al., introduces rSTAR, a method that combines MCTS for data generation and training to improve mathematical reasoning. Stengel and Schneider discusses beam search and dynamic variable-time search (DVTS) as inference-time scaling techniques to improve open-source LLMs. DVTS works by running multiple independent subtrees in parallel so to avoid all leaves stuck in local minima.

% \paragraph{Particle-based Monte Carlo methods}
\textbf{Particle-based Monte Carlo methods} are powerful tools for probabilistic inference. Liu et al., or particle filtering Del Moral et al., has been the classical way to approximate complex posterior distributions over state-space models. Particle Gibbs (PG) sampling Carvalho et al., extends these approaches by integrating MCMC techniques for improved inference. Du and Liang introduce a probabilistic programming language that applies SMC methods to steer/constrain LLM generation. Zhang et al., and Wang et al., introduce Twisted SMC methods for inference in language models. 

% * qwen PRM ____
% * lets verify step by step PRM ____
% * monkey llm (best-of-N) ____
% * compute optimality google paper ____
% * rstar (MCTS + training) ____
% * beam search and DVTS blog post ____

% * majority voting ____

% * not actual MCTS but closest i could get ____

% particle filtering ____ or sequence Monte Carlo ____
% particle Gibbs ____

% * majority voting: https://arxiv.org/pdf/2203.11171, Self-Consistency Improves Chain of Thought Reasoning in Language Models, ____

% * not actual MCTS but closest i could get: https://arxiv.org/abs/2310.04406, Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models, ____

% * DL scaling (training time) 
%     *  Kaiser et al.,  (Deep Learning Scaling is Predictable, Empirically)
%     *  Hestness et al.,   https://arxiv.org/abs/2001.08361
%     *  Wang et al., Training Compute-Optimal Large Language Models, https://arxiv.org/abs/2203.15556