@article{andrieu2010particlemarkov,
  title = {Particle {{Markov Chain Monte Carlo Methods}}},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  month = jun,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {72},
  number = {3},
  pages = {269--342},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  urldate = {2025-01-30},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L{\'e}vy-driven stochastic volatility model.},
  copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
  langid = {english},
  file = {/Users/kai/Zotero/storage/JYLSF65L/Andrieu et al. - 2010 - Particle Markov Chain Monte Carlo Methods.pdf}
}

@misc{beeching2024scalingtesttimecompute,
      title={Scaling test-time compute with open models},
      author={Edward Beeching and Lewis Tunstall and Sasha Rush},
      url={https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute},
      year={2024},
}

@misc{brown2024largelanguage,
  title = {Large {{Language Monkeys}}: {{Scaling Inference Compute}} with {{Repeated Sampling}}},
  shorttitle = {Large {{Language Monkeys}}},
  author = {Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V. and R{\'e}, Christopher and Mirhoseini, Azalia},
  year = {2024},
  month = jul,
  number = {arXiv:2407.21787},
  eprint = {2407.21787},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.21787},
  urldate = {2024-08-14},
  abstract = {Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9\% with one sample to 56\% with 250 samples, outperforming the single-attempt state-of-the-art of 43\% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95\% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/kai/Zotero/storage/H3E7XR7R/Brown et al. - 2024 - Large Language Monkeys Scaling Inference Compute with Repeated Sampling.pdf;/Users/kai/Zotero/storage/AXJFUGTC/2407.html}
}

@misc{cui2024process,
  title={Process Reinforcement through Implicit Rewards},
  author={Ganqu Cui and Lifan Yuan and Zefan Wang and Hanbin Wang and Wendi Li and Bingxiang He and Yuchen Fan and Tianyu Yu and Qixin Xu and Weize Chen and Jiarui Yuan and Huayu Chen and Kaiyan Zhang and Xingtai Lv and Shuo Wang and Yuan Yao and Hao Peng and Yu Cheng and Zhiyuan Liu and Maosong Sun and Bowen Zhou and Ning Ding},
  year={2025}
}

@misc{feng2024stepbystepreasoningmathproblems,
      title={Step-by-Step Reasoning for Math Problems via Twisted Sequential Monte Carlo}, 
      author={Shengyu Feng and Xiang Kong and Shuang Ma and Aonan Zhang and Dong Yin and Chong Wang and Ruoming Pang and Yiming Yang},
      year={2024},
      eprint={2410.01920},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01920}, 
}

@misc{guan2025rstarmathsmall,
  title = {{{rStar-Math}}: {{Small LLMs Can Master Math Reasoning}} with {{Self-Evolved Deep Thinking}}},
  shorttitle = {{{rStar-Math}}},
  author = {Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
  year = {2025},
  month = jan,
  number = {arXiv:2501.04519},
  eprint = {2501.04519},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.04519},
  urldate = {2025-01-30},
  abstract = {We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na{\textbackslash}"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8\% to 90.0\% and Phi3-mini-3.8B from 41.4\% to 86.4\%, surpassing o1-preview by +4.5\% and +0.9\%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3\% (8/15) of problems, ranking among the top 20\% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/kai/Zotero/storage/WDPE7VDE/Guan et al. - 2025 - rStar-Math Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking.pdf;/Users/kai/Zotero/storage/QHRWBX5Z/2501.html}
}

@misc{hestness2017deeplearningscalingpredictable,
      title={Deep Learning Scaling is Predictable, Empirically}, 
      author={Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
      year={2017},
      eprint={1712.00409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.00409}, 
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{lew2023sequentialmontecarlosteering,
      title={Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs}, 
      author={Alexander K. Lew and Tan Zhi-Xuan and Gabriel Grand and Vikash K. Mansinghka},
      year={2023},
      eprint={2306.03081},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2306.03081}, 
}

@misc{lightman2023letsverify,
  title = {Let's {{Verify Step}} by {{Step}}},
  author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  year = {2023},
  month = may,
  number = {arXiv:2305.20050},
  eprint = {2305.20050},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.20050},
  urldate = {2025-01-30},
  abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78\% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/kai/Zotero/storage/95MRZUYA/Lightman et al. - 2023 - Let's Verify Step by Step.pdf;/Users/kai/Zotero/storage/2NDYZVV7/2305.html}
}

@misc{nonlinearfiltering,
  title = {Nonlinear Filtering: {{Interacting}} Particle Resolution - {{ScienceDirect}}},
  urldate = {2025-01-30},
  author = {Robert H. Swendsen and Jian-Sheng Wang},
  year = {1986},
  howpublished = {https://www.sciencedirect.com/science/article/abs/pii/S0764444297847787},
  file = {/Users/kai/Zotero/storage/PH4VYE45/delmoral96nonlinear.pdf}
}

@misc{sequentialmonte,
  title = {Sequential {{Monte Carlo Methods}} for {{Dynamic Systems}}: {{Journal}} of the {{American Statistical Association}}: {{Vol}} 93, {{No}} 443},
  author = {Pierre Del Moral},
  year = {1997},
  urldate = {2025-01-30},
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1998.10473765},
  file = {/Users/kai/Zotero/storage/3M8EP7JL/01621459.1998.html}
}

@misc{snell2024scalingllm,
  title = {Scaling {{LLM Test-Time Compute Optimally}} Can Be {{More Effective}} than {{Scaling Model Parameters}}},
  author = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  year = {2024},
  month = aug,
  number = {arXiv:2408.03314},
  eprint = {2408.03314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.03314},
  urldate = {2025-01-02},
  abstract = {Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/kai/Zotero/storage/SN4DWZEV/Snell et al. - 2024 - Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.pdf;/Users/kai/Zotero/storage/7ZJ3NVDV/2408.html}
}

@misc{wang2023selfconsistencyimproveschainthought,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.11171}, 
}

@misc{wang2024mathshepherdverifyreinforcellms,
      title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
      author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
      year={2024},
      eprint={2312.08935},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.08935}, 
}

@misc{zhang2025lessonsdeveloping,
  title = {The {{Lessons}} of {{Developing Process Reward Models}} in {{Mathematical Reasoning}}},
  author = {Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  year = {2025},
  month = jan,
  number = {arXiv:2501.07301},
  eprint = {2501.07301},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.07301},
  urldate = {2025-01-23},
  abstract = {Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/kai/Zotero/storage/2GIQ5LK8/Zhang et al. - 2025 - The Lessons of Developing Process Reward Models in Mathematical Reasoning.pdf;/Users/kai/Zotero/storage/78DJLXAD/2501.html}
}

@misc{zhao2024probabilisticinferencelanguagemodels,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
      eprint={2404.17546},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.17546}, 
}

@misc{zhou2024languageagenttreesearch,
      title={Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models}, 
      author={Andy Zhou and Kai Yan and Michal Shlapentokh-Rothman and Haohan Wang and Yu-Xiong Wang},
      year={2024},
      eprint={2310.04406},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.04406}, 
}

