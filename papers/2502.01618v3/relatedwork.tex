\section{Related Work}
% \paragraph{Process reward models}
\textbf{Process reward models} (PRMs) aim to provide more granular feedback by evaluating intermediate steps rather than only final outputs.
They are trained via process supervision, a training approach where models receive feedback on each intermediate step of their reasoning process rather than only on the final outcome.
\citet{lightman2023letsverify} propose a step-by-step verification approach to PRMs, improving the reliability of reinforcement learning.
DeepSeek PRM \cite{wang2024mathshepherdverifyreinforcellms} uses Mistral to annotate training data for PRMs .
\citet{zhang2025lessonsdeveloping} introduces Qwen-PRM, which combines both Monte Carlo estimation and model/human annotation approach to prepare training data for a PRM. %\todo{add deepseek and prime rm}
PRIME~\citep{cui2024process} proposes to train an outcome reward model (ORM) using an implicit reward objective. The paper shows that implicit reward objective directly learns a Q-function that provides rewards for each token, which can be leveraged to create process-level reward signal. This process eliminates the need for any process labels, and reaches competitive performance on PRM benchmarks. 

% Without the need for any process label, implicit PRM is trained as an outcome reward model (ORM) and then used as a PRM.

% train a math process reward by only using outcome level signals through implicit reward.

% by using implicit reward from 

\textbf{Inference-time scaling} has been a key training-free strategy for enhancing LLM performance. \citet{brown2024largelanguage} explores a best-of-N (BoN) decoding strategy, demonstrating improvements in output quality through selective refinement. \citep{snell2024scalingllm} provides insights into how scaling compute resources can yield better inference efficiency from a compute optimality perspective.
While not implementing full Monte Carlo tree search (MCTS), \citet{zhou2024languageagenttreesearch} explores a tree-search-like approach within language models. 
Additionally, \citet{guan2025rstarmathsmall} introduces rSTAR, a method that combines MCTS for data generation and training to improve mathematical reasoning. \citet{beeching2024scalingtesttimecompute} discusses beam search and dynamic variable-time search (DVTS) as inference-time scaling techniques to improve open-source LLMs. DVTS works by running multiple independent subtrees in parallel so to avoid all leaves stuck in local minima.

% \paragraph{Particle-based Monte Carlo methods}
\textbf{Particle-based Monte Carlo methods} are powerful tools for probabilistic inference. Sequential Monte Carlo \citep{sequentialmonte} or particle filtering \citep{nonlinearfiltering} has been the classical way to approximate complex posterior distributions over state-space models. Particle Gibbs (PG) sampling \citep{andrieu2010particlemarkov} extends these approaches by integrating MCMC techniques for improved inference. \cite{lew2023sequentialmontecarlosteering} and \cite{loula2025syntactic} introduce a probabilistic programming language that applies SMC methods to steer/constrain LLM generation. \cite{zhao2024probabilisticinferencelanguagemodels} and \cite{feng2024stepbystepreasoningmathproblems} introduce Twisted SMC methods for inference in language models. 

% * qwen PRM \citep{zhang2025lessonsdeveloping}
% * lets verify step by step PRM \citep{lightman2023letsverify}
% * monkey llm (best-of-N) \citep{brown2024largelanguage}
% * compute optimality google paper \citep{snell2024scalingllm}
% * rstar (MCTS + training) \citep{guan2025rstarmathsmall}
% * beam search and DVTS blog post \citep{beeching2024scalingtesttimecompute}

% * majority voting \cite{wang2023selfconsistencyimproveschainthought}

% * not actual MCTS but closest i could get \cite{zhou2024languageagenttreesearch}

% particle filtering \citep{nonlinearfiltering} or sequence Monte Carlo \citep{sequentialmonte}
% particle Gibbs \citep{andrieu2010particlemarkov}

% * majority voting: https://arxiv.org/pdf/2203.11171, Self-Consistency Improves Chain of Thought Reasoning in Language Models, \cite{wang2023selfconsistencyimproveschainthought}

% * not actual MCTS but closest i could get: https://arxiv.org/abs/2310.04406, Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models, \cite{zhou2024languageagenttreesearch}

% * DL scaling (training time) 
%     * \cite{hestness2017deeplearningscalingpredictable} (Deep Learning Scaling is Predictable, Empirically)
%     * \cite{kaplan2020scalinglawsneurallanguage}  https://arxiv.org/abs/2001.08361
%     * \cite{hoffmann2022trainingcomputeoptimallargelanguage} Training Compute-Optimal Large Language Models, https://arxiv.org/abs/2203.15556