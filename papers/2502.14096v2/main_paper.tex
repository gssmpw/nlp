\section{Introduction}
In many real-world optimization problems, we have access to multi-dimensional feedback rather than a single scalar objective. The multi-objective optimization (MOO) literature has largely focused on the setting where these objectives \emph{conflict} with each other, which necessitates the \emph{Pareto dominance} notion of optimality. A closely related area of study is \emph{multi-task learning} \citep{caruana1997multitask,teh2017distral,sener2018multi,yu2020gradient,lin2021reasonable,liu2021conflict,navon2022multi,lin2023libmtl,achituve2024bayesian,he2024robust}, where multiple tasks are learned jointly, typically with both shared and task-specific parameters. The hope is that the model can perform better on individual task by sharing common information across tasks. Indeed, the phenomenon of improved performance across all tasks has been observed in several settings \citep{lin2023libmtl,lee2024parrot}, suggesting that perhaps there may not always be significant trade-offs between objectives. Similar observations appear in meta-learning \citep{ravi2017optimization,finn2017model,hospedales2021meta}, where the goal is to learn representations that enable quick adaptation to new tasks with minimal additional training, as well as in reinforcement learning~\cite{jaderberg2016reinforcement,teh2017distral, veeriah2019discovery,dann2023reinforcement}, where practitioners use multiple reward functions to better specify the policy or its representation.

In this work, we explicitly study a setting where objectives are \emph{aligned}, namely, that the different objectives share a common solution. This situation arises frequently in practice. For example, when using reinforcement learning (RL) to augment large language models (LLMs) with reasoning capabilities, there are often multiple options for the choice of reward model to use. \citet{lightman2023let} and \citet{uesato2022solving} consider both outcome and process-based rewards, and, recently \citet{guo2025deepseek,team2025kimi} discuss the use of accuracy, format rewards, length of response, and reward on math problems as additional reward functions . In training text-to-image models using RL, \citet{lee2024parrot} use four reward models (aesthetic quality, human preference, text-image alignment, and image sentiment) and show results where \emph{all} rewards are increased. Although the method of \citet{lee2024parrot} is designed for finding Pareto-optimal solutions (implying the existence of trade-offs), the numerical results suggest that the objectives may actually be \emph{aligned} to a good degree. 

These observations are also related to a more general phenomenon in RL discussed by \citet{dann2023reinforcement}, where learning can be accelerated by exploiting several alternative reward specifications that all lead to the same optimal policy. This concept builds on prior work showing that the choice of reward function (e.g., dense versus sparse reward) can have a dramatic effect on training time \citep{ng1999policy,luo2020accelerating,wang2019dynamic,hu2020learning}. A related idea in statistics is that when labeled data is sparse, practitioners can rely on closely-related proxy tasks to improve prediction accuracy \citep{bastani2021predicting}. 


\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.25}  % Increases row height
\begin{tabular}{|>{\centering\arraybackslash}p{3.2cm}|>{\centering\arraybackslash}p{4.5cm}|}
\hline
\rowcolor{gray!50} \makecell{Algorithm} & \makecell{Asymptotic Convergence} \\
% \hline
%  \makecell{EW} & $O\brac{\brac{1-\muglobal/m\beta}^{k}}$ \\
\hline
 \makecell{\CAMOO} & $O\brac{\brac{1-\muglobal/\beta}^{k}}$ \\
\hline
 \makecell{\PAMOO} &  $O\brac{\brac{1-\mulocal/\beta}^{k}}$  \\
% \hline
% \makecell{Equal Weights (\texttt{EW})} & $\Omega\brac{1-\muglobal/m\beta}^{k}$  \\
\hline
\end{tabular}
\caption{The main results introduced in this work. $\beta$ is the smoothness parameter; $m$ is the number of objective functions; both $\muglobal$ and $\mulocal$ are structural quantities introduced in Section~\ref{sec:camoo} and Section~\ref{sec:pamoo}. These characterize notions of optimal curvature of weighted function and satisfy $\mulocal\geq \muglobal.$ 
% Our new algorithms improve over the naive \texttt{EW} that can degrade as the number of objectives increases.
}
\label{tab:my_table}
\end{table}


% For example,  
% \textcolor{red}{YE: provide better motivation} 
% in reinforcement learning, practitioners can sometimes speed up learning by exploit several alternative reward specifications that all lead to the same optimal policy \citep{dann2023reinforcement}. In statistics and machine learning, labeled data is sometimes sparse, leading practitioners to rely on closely-related proxy tasks to improve prediction accuracy \citep{bastani2021predicting}. 

To our knowledge, there is no work that studies such a framework from an optimization perspective. We ask the following question: 
\begin{center}
    \emph{Can gradient descent type of algorithms benefit from multi-objective feedback when the objectives are aligned?}  
\end{center}
Previous work in multitask learning had provided convergence guarantees for gradient descent-type algorithms for MOO
\citep{sener2018multi,yu2020gradient,liu2021conflict,navon2022multi,he2024robust}. However, since these consider general multi-objective framework, their algorithms converge with worst-case guarantees with no meaningful convergence improvement of MOO.   
%Some prior works from representation learning have studied questions that are similar in spirit to the one we pose above \cite{lu2021power,chen2022active}, but largely focus on a sample complexity perspective. {\color{red}[TODO: add more]} In reinforcement learning, the problem of exploiting alternative reward specifications (that all lead to the same optimal policy) is also closely related \citep{dann2023reinforcement}. However, to our knowledge, there is no work that studies this question from a purely optimization perspective. 

We provide a positive answer to the aforementioned question. We formally introduce the \emph{aligned multi-objective optimization} (AMOO) framework. Subsequently, we design new gradient descent-type algorithms and establish their provable improved convergence in the AMOO setting. These can be interpreted as parameter-free algorithms to handle multi-objective feedback when objectives are aligned. Lastly, we conclude by providing empirical evidence of the improved convergence properties of the new algorithms.

\section{Related Work}
\subsection{Gradient Weights in Multi-task \& Meta Learning}
Our work is closely related to optimization methods from the multi-task learning (MTL) and meta learning literature, particularly those that integrate weights into the task gradients or losses. The \emph{multiple gradient descent algorithm} (MGDA) approach of \citet{desideri2012multiple, sener2018multi,zhang2024convergence} is one of the first works along this direction. It proposes an optimization objective that gives rise to a weight vector that implies a descent direction for all tasks and converges to a point on the Pareto set. 
% MGDA was introduced into the deep MTL setting in \citet{}, which propose extensions to MGDA weight calculation that can be more efficiently solved. 
The PCGrad paper \cite{yu2020gradient} identified that conflicting gradients can be detrimental to MTL. The authors then propose to modify the gradients to remove this conflict (by projecting each task's gradient to the normal plane of another task), forming the basis for the PCGrad algorithm. Another work that tackles conflicting gradients is the \emph{conflict-averse gradient descent} (CAGrad) method of \cite{liu2021conflict}. CAGrad generalizes MGDA: its main idea is to minimize a notion of ``conflict'' between gradients from different tasks, while staying nearby the gradient of the average loss. Notably, CAGrad maintains convergence toward a minimum of the average loss. Another way to handle gradient conflicts is the Nash-MTL method of \citet{navon2022multi}, where the gradients are combined using a bargaining game. Very recently, \citet{achituve2024bayesian} introduced a Bayesian approach for gradient aggregation by incorporating uncertainty in gradient dimensions. Other optimization techniques for MTL include tuning gradient magnitudes so that all tasks train at a similar rate \citep{chen2018gradnorm}, taking the geometric mean of task losses \citep{chennupati2019multinet++}, and random weighting \citep{lin2021reasonable}. On the meta learning front, the MAML algorithm \citep{finn2019online} aims to learn a useful representation such that the model can adapt to new tasks with only a small number of training samples. Since fast adaptation is the primary goal in meta learning, MAML's loss calculation differs from those found in MTL. 

Few prior works provided provable convergence guarantees of the different existing multi-objective optimization methods. Without additional assumption on the alignment of different objectives, these guarantees quantified convergence to a point on the Pareto front. Unlike our work, there the convergence guarantees depended on worst-case structural quantities such as the maximal Lipschitz constant among all objectives~\cite{liu2021conflict,navon2022multi} or the maximal generalized smoothness~\cite{zhang2024convergence}.


The algorithms introduced in the following are similar to existing ones in that they construct a weighted loss to combine information from different sources of feedback. Unlike previous work, we focus on exploiting the prior knowledge that the objectives are \emph{aligned}.  We introduce new instance-dependent structural quantities that reflect how aligned multi-objective feedback can improve GD performance, improving convergence that depends on worst-case structural quantities, as in prior works. 
% theoretically and empirically that such knowledge can be beneficial for optimization and results with improved convergence.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/amoo_image.png}
    \caption{ Visualization of AMOO instances in which it is possible to obtain improved convergence compared to optimizing individual functions or the average function: \textbf{(left)} the specification example, \textbf{(center)} simpler instance of the selection example, and \textbf{(right)} 3D example of the local curvature example, in which $f_1(x_1,x_2)=\exp(x_1)+\exp(x_2)-x_1-x_2$ and $f_2(x_1,x_2)=f_1(-x_1,-x_2)$. This example highlights the need to toggle between functions according to their local curvature.}
    \label{fig:amoo_examples}
\end{figure*}



\subsection{Proxy \& Multi-fidelity Feedback}
Other streams of related work are (1) machine learning using proxies and (2) multi-fidelity optimization. These works stand out from MTL in that they both focus on using \emph{closely related} objectives, while traditional MTL typically considers a set of tasks that are more varied in nature. Proxy-based machine learning attempts to approximate the solution of a primary ``gold'' task (for which data is expensive or sparsely available) by making use of a proxy task where data is more abundant \citep{bastani2021predicting,dzyabura2019accounting}. Similarly, multi-fidelity optimization makes use of data sources of varying levels of accuracy (and potentially lower computational cost) to optimize a target objective \citep{forrester2007multi}. In particular, the idea of using multiple closely-related tasks of varying levels of fidelity has seen adoption in settings where function evaluations are expensive, including bandits \citep{kandasamy2016multi,kandasamy2016gaussian}, Bayesian optimization \citep{kandasamy2017multi,song2019general,wu2020practical,takeno2020multi}, and active learning \citep{yi2021active,li2020deep,li2022batch}. The motivations behind the AMOO setting are clearly similar to those of proxy optimization and multi-fidelity optimization. However, our papers takes a pure optimization and gradient-descent perspective, which to our knowledge, is novel in the literature.


% Sim-to-real learning can be thought of as a particular instance of multi-fidelity optimization, where one hopes to learn real world behavior via simulations (typically in robotics) \citep{peng2018sim, zhao2020sim}. In many of these papers, however, the objectives are queried one at a time, differing from MTL or AMOO.




% \subsection{Other Forms of Multi-task Learning}


% \begin{itemize}
%     % \item Multi Task papers for networks: practice (these show convergence to Pareto point)
%     \item Multi task papers: improvements in sample complexity and not optimization \cite{lu2021power,chen2022active}
%     % \item Learning with Proxies papers.
% \end{itemize}


% \begin{itemize}
%     \item In many settings we have multi objective feedback with improves the performance across tasks (add citations)
%     \item Implies that in many cases there may not be significant tradeoffs between tasks or objectives.
%     \item Previous works in multi-task setting proved convergence to Pareto front, which implies there is a performance tradeoff between tasks.
%     \item Question: How can we benefit from multi-objective feedback when the objectives are aligned? Prior works focused on this question from sample complexity perspective~\cite{lu2021power,chen2022active}, but there is no work that studies this question from optimization perspective.
    
%     \item We introduce the Aligned Multi Objective Optimization (AMOO) framework to study this question. We design new algorithms with provable guarantees and show how it can be scaled and be used in networks.
% \end{itemize}

% \section{Preliminaries (Yonathan \& Ben)}

% \paragraph{Notation.}

\section{Aligned Multi Objective Optimization}\label{sec:AMOO setting}

% \textcolor{red}{TODO: (Daniel's task) add figures that visualize the three examples to help readers understand it.}



Consider an unconstrained multi-objective optimization where
 $F: \reals^{n} \to \reals^m$ is a vector valued function, 
 $
 F(\x) = \brac{f_1(\x), f_2(\x), \ldots, f_m(\x) },
 $
 and all functions $\{ f_i\}_{i\in [m]}$ are convex where $[m]:=\{1,\ldots,m\}$.
Without additional assumptions the components of $F(\x)$ cannot be minimized simultaneously. To define a meaningful approach to optimize $F(\x)$ one can study the Pareto front, or to properly define how to trade-off the objectives. In the AMOO setting we make the assumption the functions are aligned in a specific sense: we assume that the functions $\{ f_i\}_{i\in [m]}$ share an optimal solution\footnote{We also study an extension of AMOO where the functions can only be approximately simultaneously minimized. See Section~\ref{sec:robustness result}.}. Namely, there exists a point $\x_\star$ that minimizes all functions in $F(\cdot)$ simultaneously,
\begin{align}
    \x_\star \in \arg\min_{\x\in \reals^n} f_i(\x) \quad \forall i \in [m]. \label{eq:aligned_functions}
\end{align}
With this assumption one may hope to get quantitative benefits from the multi objective feedback. 
How can Gradient Descent (GD) be improved when the functions are aligned? 
% Which design principles can we derive for this setting? 

% We are interested in gradient methods of the form,
% \[
% \x_{k+1} = \x_k + \eta_t \d_t,
% \]
% where $\d_t$ denotes a descent direction and $\eta_t$ is the step size in the time step $t$. 





A common algorithmic approach in the multi-objective setting is using a weight vector $\w\in \mathbb{R}^m$ that maps the vector $F(\x)$ to a single objective $f_{\w}(\x) := \w^T F(\x)$, and apply a gradient descent step on the weighted function (e.g.,~\citet{sener2018multi, yu2020gradient, liu2021conflict}). Existing algorithms suggest alternatives for choosing $\w$ via different weight optimizers. We follow this paradigm and refer to it as \texttt{Weighted-GD} (see Algorithm~\ref{alg:Weighted-GD}).



Towards developing algorithmic intuition for the AMOO setting we consider few examples. 

\paragraph{\emph{(i)} The Specification Example.}

Consider the case $F(\x)=(f_1(\x),f_2(\x))$, $\x\in \reals^2$ where
% \begin{align*}
%     &f_1(\x) = (1-\Delta) x_1^2 + \Delta x_2^2, \quad \text{and}\quad f_2(\x) = \Delta x_1^2 +  (1-\Delta) x_2^2,
% \end{align*}
\begin{align*}
    &f_1(\x) = (1-\Delta) x_1^2 + \Delta x_2^2, \quad \\
    &f_2(\x) = \Delta x_1^2 +  (1-\Delta) x_2^2,
\end{align*}
for some small $\Delta\in [0,0.1]$. It is clear that $F(\x)$ can be simultaneously minimized in $\x_\star=\left( 0,\ 0 \right)$, hence, this is an AMOO setting. This example, as we demonstrate, illustrates an instance in which each individual function \textit{does not specify the solution well}, but with proper weighting the optimal solution is well specified.

First, observe both $f_1$ and $f_2$ are $\Delta$-strongly convex and $O(1)$-smooth functions. Hence, GD with properly tuned learning rate, applied to either $f_1$ or $f_2$ converges with linear rate of $\Omega(\Delta)$. This rate can be  dramatically improved by proper weighting of the functions. Let $f_{\w_U}$ be a function with equal weighting of both $f_1$ and $f_2$, namely, choosing $\w_U=(0.5,0.5)$. We get $f_{\w_U}(\x)=0.5 x_1^2 + 0.5 x_2^2$
which is $\Omega(1)$-strongly convex and $O(1)$-smooth. Hence, GD applied to $f_{\w_U}$ converges with linear rate of $\Omega(1)$--much faster than $O(\Delta)$ since $\Delta$ can be arbitrarily small.



\paragraph{\emph{(ii)} The Selection Example.}
% {\color{red}I think indexing of $x$ is not consistent here. Should be from $1$ to $d$, but sometimes it's $0$ to $(d-1)$?}
Consider the case $F(\x)=(f_1(\x),\ldots,f_m(\x))$, $\x\in \reals^n$, where
% \begin{align*}
%     & \forall i \in [m-1] : f_i(\x) = (1-\Delta)x_1^2 + \Delta \sum_{j=2}^{d} x_j , \quad \text{and} \quad  f_m(\x) = \sum_{j=1}^{d} x_j^2,
% \end{align*}
\begin{align*}
    & \forall i \in [m-1] : f_i(\x) = (1-\Delta)x_1^2 + \Delta \sum_{j=2}^{n} x_j^2,\\
    &f_m(\x) = \sum_{j=1}^{n} x_j^2,
\end{align*}


and $\Delta\in [0,0.5]$. The common minimizer of all functions is $\x_\star=\bold{0} \in \reals^n$, and, hence, the objectives are aligned. Unlike the specification example, in the selection example, there is a single objective function among the $m$ objectives we should select to improve the convergence rate of GD. Further, in the selection example, choosing the uniform weight degrades the convergence rate. 
% Indeed, since $f_m(\x)$ is $\Omega(1)$-strongly convex and $O(1)$-smooth, GD applied to $f_m(\x)$ converges in a linear rate of $\Omega(1)$. As in previous example, applying GD for any $f_i(\x)$, $i\in [m-1]$ converges in a rate of $O(\Delta)$, since these functions are only $\Delta$-strongly convex.

Indeed, setting the weight vector to be uniform $\w_{U}=\brac{1/m,\ldots,1/m}\in\reals^m$ leads to the function
$
    f_{\w_{U}}(\x) = (2-\Delta)/m \cdot x_1^2 +\sum_{j=2}^{n} (\Delta+1)/m \cdot x_j^2,
$
which is $O(1/m)$-strongly convex. Hence, GD applied to $f_{\w_{U}}$ converges in a linear rate of $O(1/m)$. On the other hand, GD applied to $f_m$ converges with linear rate of $\Omega(1)$. Namely, setting the weight vector to be $(0,\ldots,0,1)\in \reals^m$ improves upon taking the average when the number of objectives is large.

\paragraph{\emph{(iii)} Local Curvature Example.}
Consider the case $F(x)=(f_1(x),f_2(x))$, $x\in \mathbb{R}$ where
\begin{align*}
    &f_1(x) = \exp(x)- x,\\ 
    &f_2(x) = \exp(-x) + x,
\end{align*}
namely, $f_2(x)=f_1(-x)$. Both functions are simultaneously minimized in $x=0$. This example depicts a scenario in which different functions have a larger curvature in different segments of the parameter space; for $x>0$, $f_1(x)$ has a larger curvature, and for $x<0$ $f_2$ has a larger curvature. 

For such a setting, it is natural to toggle between the two functions, namely to set the weight vector as $\w_1=(1,0)$ for $x>0$ and as $\w_2=(0,1)$ for $x<0$. This approach, intuitively, should result in a faster convergence to the optimal solution compared to applying GD to an individual function or the average function, since it effectively applies GD to a function which is uniformly more curved.

The three aforementioned examples highlight a key takeaway: the curvature of the functions has a vital role in improving convergence guarantees for GD in AMOO. Indeed, all examples provided arguments as of how to improve the convergence of GD based on curvature information. In next sections we formalize this intuition. We introduce quantities that characterize notions of best curvature, and develop new GD based algorithms that provably converge with rates that depend on these quantities.    

% It is clear that $F(\x)$ can be simultaneously minimized in $\x_\star=\left( 0,\ 0 \right)$, hence, this is an AMOO setting. This example, is similar to the \textit{Specification example}, each function does not specify the solution well. However, with proper weighting the optimal solution is well specified.

% First, observe both $f_1$ and $f_2$ are strictly-convex. Notice, for every bounded set $\mX$ with radios $R$ the functions are $O(R^2)$-smooth. Thus, each function alone does not guarantee a linear convergence. However, for every point $(x_1,x_2)$ let $f_{\w}$ be a function where,
% \begin{align*}
%     &\w_{\mathrm{far}} = \brac{\frac{x_2^2 - 1}{x_1^2 + x_2^2 -2},\frac{x_1^2 - 1}{x_1^2 + x_2^2 -2}},\\
%     &\w_{\mathrm{close}}= \brac{ 1/2, 1/2},\\
%     & \w = 
%     \begin{cases}
%         \w_{\mathrm{far}} & \frac{x_1^2 x_2^2 - 1}{x_1^2 + x_2^2 -2} > 1\\
%         \w_{\mathrm{close}} & \text{o.w.}
%     \end{cases}
% \end{align*}
% Then, it holds linear convergence of $O(R^{-2})$. \textcolor{red}{YE: why this is true? is it suppose to reflect a selection example when we want to focus on different functions in different part of the space? strictly speaking, this exmple seems to reflect something different.s}

\begin{algorithm}[t]
 \caption{\texttt{Weighted-GD}}\label{alg:Weighted-GD} 
\begin{algorithmic}
  \STATE \textbf{initialize:} 
  \STATE \quad Learning rate $\eta$, \texttt{Weight-Optimizer}
  % \STATE \quad \texttt{Weight-Optimizer}
  \WHILE{$k=1,2,\ldots$}
  \STATE { \color{orange} $\w_k \gets \texttt{Weight-Optimizer}\brac{\{ f_i(\x_k) \}_{i=1}^m}$ }
  \STATE $\g_k \gets \nabla f_{\w_k}(\x_k)$ 
  \STATE $\x_{k+1} = \x_k - \eta \g_t$ 
  \ENDWHILE
\end{algorithmic}
\end{algorithm}

\section{The \CAMOO\ Weight Optimizer}\label{sec:camoo}

% {\color{blue} YON: should we establish results for weighted projected gradient descent?}\\{\color{blue} BK: I don't see what it adds. The results will be the same. Why yes?} {\color{blue} YON: mostly because we work with $\mX$. if we want to work with unconstraint optimization maybe we should be explicit about it and assume $\mX=\mathbb{R}^n$ for some $n$? i actually see we not being consistent: we use both $\mathbb{R}^n$ and $\mX$. should we go with the first option?}

We start by introducing and analyzing the Curvature Aligned Multi Objective Optimizer (\CAMOO). \CAMOO\ (Algorithm~\ref{alg:AMOOO}) directly optimizes the curvature of the weighted function.  Towards developing it, we define the global adaptive strong convexity parameter, $\muglobal$. Later we show that when the weighted loss is determined by \CAMOO\ GD converges in a rate that depends on $\muglobal$.

We start by defining the optimal adaptive strong convexity over the class of weights:
\begin{definition}[Global Adaptive Strong Convexity $\muglobal$] \label{def:mu_global}
     The global adaptive strong convexity parameter, ${\muglobal\in \reals_{+}}$, is the largest value such that $\forall \x\in \mathbb{R}^n$ exists a weight vector $\w\in \Delta_m$ such that  
    \begin{align}
       \lambda_{\min}\left(\sum_{i=1}^m w_i \nabla^2 f_i(\x) \right)\geq \muglobal. \label{eq:mu_global_definition}
    \end{align}
\end{definition}

For each $\x\in \mathbb{R}^n$, there may be a different weight vector that solves $\argmax\lambda_{\min}\left(\nabla^2 f_\w(\x) \right)$ and locally maximizes the curvature. The global adaptive strong convexity parameter $\muglobal$ is the largest lower bound in $\mathbb{R}^n.$ The specification and selection examples (Section~\ref{sec:AMOO setting}) demonstrate $\muglobal$ can be much larger than both the strong convexity parameter of the average function or of each individual function; for both $\muglobal=O(1)$ whereas the alternatives may have arbitrarily small strongly convex parameter value. Further, the local curvature example highlights a case in which the optimal weight may have dependence on $\x$.

Additionally, this structural definition implies that there is a unique point $\x_\star$ that simultaneously minimizes the objectives. Due to this observation, in the following, we aim to design provable GD methods that converge to this optimal point~$\x_\star$. The following result formalizes this by showing that under a weaker condition compared to $\muglobal>0$ there is a unique minimizer (see Appendix~\ref{app:missing_proofs_unique_opt_sol}  for a proof).

\begin{restatable}[Unique Optimal Solution]{proposition}{UniqueSol}\label{prop:unique_optimal_sol} Assume there exists $\x_\star\in \mathbb{R}^n$ that simultaneously minimizes $\{f_i \}_{i\in [m]}$, namely, solves Eq.~\eqref{eq:aligned_functions}. If $\max_{\w\in \Delta_m}\lambda_{\min}\left(\nabla^2 f_{\w}(\x_\star) \right)>0$ then $\x_\star$ is unique.
\end{restatable}


Definition~\ref{def:mu_global} not only quantifies an optimal notion of curvature, but also directly results with the \CAMOO\ algorithm. \CAMOO\ sets the weights according to Eq.~\eqref{eq:mu_global_definition}, namely, at the $k^{\mathrm{th}}$ iteration, it finds the weight vector for which $f_\w(\x_k)$ has the largest local curvature. Then, a gradient step is applied in the direction of $\nabla f_{\w}(\x_k)$ (see Algorithm~\ref{alg:Weighted-GD}). Indeed, \CAMOO\ seems as a natural algorithm to apply in AMOO. Nevertheless, the analysis of \CAMOO\ faces key challenges that make its analysis less trivial than what one may expect. 

\paragraph{Challenge \emph{(i)}: $f_{\w_k}$ is not a strongly convex function.}  One may hope that standard GD analysis for strongly convex and smooth functions can be applied. It is well known that if a function $f(\x)$ is $\beta$ smooth and $\forall \x\in \mathbb{R}^n,\ \lambda_{\min}\brac{\nabla^2 f(\x)} \geq \mu$ then GD converges with $\mu/\beta$ linear rate. Unfortunately, a careful examination of this argument shows it fails.

Even though $\lambda_{\min}\brac{\nabla^2 f_{\w_k}(\x_k)}\geq \muglobal$ at each iteration $k$ of \CAMOO\ it does not imply that $f_{\w_k}$ is $\muglobal$ strongly convex for a fixed $\x_k$. Namely, it does not necessarily hold that for all $\x\in \mathbb{R}^n,\ \lambda_{\min} \brac{\nabla^2 f_{\w_k}(\x)} \geq \muglobal$, but only pointwise at $\x_k$ (E.g., the local curvature example highlights this issue. See Appendix~\ref{app:fwk is not strongly convex} for details). This property emerges naturally in AMOO, yet such nuance is inherently impossible in single-objective optimization.
% and, to the best of our knowledge, was not explored in online optimization as well.

\paragraph{Challenge \emph{(ii)}: Weighted function is not necessarily convex.} A naive reduction may be to apply GD to the function $f_{\w_\star(\x)}$ where $\w_\star(\x)\in \arg\max \lambda_{\min}\left(\sum_{i=1}^m w_i \nabla^2 f_i(\x) \right)$. Namely, to apply GD to a new weighted function that is determined by optimizing the curvature. Such an approach turns out as flawed from theoretical perspective; the function $f_{\w_\star(\x)}=\sum_{i\in [m]} w_{\star,i}(\x) f_i(\x)$ is not necessarily convex nor smooth due to the dependence on a weight vector that has an $\x$ dependence (see Appendix~\ref{app:naive reduction failure} for an example).

Next, we provide a positive result. When restricting the class of functions to the set of self-concordant and smooth functions (see Appendix~\ref{app:pre_and_prp} for formal definitions) we provide a convergence guarantee for \texttt{Weighted-GD} instantiated with \CAMOO\ that depends on~$\muglobal$. Further, the result shows that close to the optimal solution the convergence has linear rate in $O(\muglobal/\beta)$ (see Appendix~\ref{app:AMOO_results} for proof details). 

% \begin{figure*}[t]
% \centering
% \begin{minipage}{.48\textwidth}
% \begin{algorithm}[t]
%  \caption{\texttt{Weighted-GD}}\label{alg:Weighted-GD} 
% \begin{algorithmic}
%   \vspace{0.5mm}
%   \STATE \textbf{Initialization:} 
%   \STATE \quad Learning rate $\eta$, 
%   \STATE \quad \texttt{Weight-Optimizer}
%   \WHILE{$k=1,2,\ldots$}
%   \STATE { \color{orange} $\w_k \gets \texttt{Weight-Optimizer}\brac{\{ f_i(\x_k) \}_{i=1}^m}$ }
%   \STATE $\g_k \gets \nabla f_{\w_k}(\x_k)$ 
%   \STATE $\x_{k+1} = \x_k - \eta \g_t$ 
%   \ENDWHILE
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}%
% \hfill
% \begin{minipage}{.48\textwidth}
% \vspace{-0.8cm}

\begin{algorithm}[t]
\caption{\CAMOO}\label{alg:AMOOO} 
\begin{algorithmic}
  \STATE \textbf{inputs:} $\{ f_i(\x_k) \}_{i=1}^m$ 
  \vspace{0.4mm}
  \STATE \textbf{initialize:} $w_{\min}=\muglobal/\brac{8m\beta}$ 
  \vspace{0.4mm}
  \STATE Get Hessian matrices $\{ \nabla^2 f_i(\x_k) \}_{i=1}^m$ 
  \vspace{0.4mm}
  \STATE $\w\in \argmax\limits_{\w\in \Delta_{m,w_{\min}}} \lambda_{\min} \brac{\sum_{i} w_i \nabla^2 f_i(\x_{k})}$
  \vspace{0.4mm}
  \STATE \textbf{return:} $\w$
\end{algorithmic}
\end{algorithm}
% \label{fig:side-by-side-algorithms}
% \end{figure*}

\begin{restatable}[$\muglobal$ Convergence of \CAMOO]{theorem}{ExactAmooConvergence}
\label{thm:exact_amoo_convergence}
        Suppose $\{f_i\}_{i\in [m]}$ are $\beta$ smooth, $M_{\mathrm{f}}$ self-concordant, share an optimal solution $\x_\star$ and that $\muglobal > 0$. Let  $k_0 := \left\lceil \frac{16 \beta  \brac{\enorm{\x_0 - \x_\star} 3\sqrt{m}\beta M_{\mathrm{f}} -\sqrt{\muglobal}} }{ 3\muglobal^{3/2}}\right\rceil$, where $\enorm{\cdot}$ is the Euclidean-norm. Then, \texttt{Weighted-GD} instantiated with \CAMOO\ weight-optimizer and $\eta = 1/2\beta$ converges with rate:
    \begin{align*}
        \enorm{\x_{k} - \x_\star} \leq 
        \begin{cases}
            \enorm{\x_{k_0} - \x_\star} \brac{1-\frac{3\muglobal}{8\beta}}^{(k-k_0)/2}  & k\geq k_0\\
            \enorm{\x_0 - \x_\star}- k \frac{\muglobal^{3/2}}{16 \beta^2 \sqrt{m} M_{\mathrm{f}}} & o.w.
        \end{cases}
    \end{align*}
\end{restatable}
Importantly,  Theorem~\ref{thm:exact_amoo_convergence} holds without making strong convexity assumption on the individual functions, but only requires that the adaptive strong convexity parameter $\muglobal$ to be positive, as, otherwise, the result is vacuous.

The proof follows few key observations. The self-concordance property, we find, implies a useful inequality that depends only on local curvature (see Appendix~\ref{app:pre_and_prp}): 
\begin{align}
    f(\y) \geq f(\x) + \inner{\nabla f(\x)}{\y  - \x} +  \frac{c\enorm{\y-\x}^2_{\nabla^2 f(\x)}}{1+M_{\mathrm{f}}\enorm{\y-\x}_{\nabla^2 f(\x)}},
    \label{eq:main paper self concordance}
\end{align}
for some constant $c>0$. This inequality share similarity with the more standard inequality used in analysis of GD convergence for strongly convex function~\cite{boyd2004convex}, however, unlike the former, it depends on local curvature. In order to satisfy the assumption the weighted function $f_{\w_k}$ is self-concordant we rely on the fact $\min_{i\in [m]} w_{k,i} > w_{\min}$ by design of \CAMOO. Then, additional analysis leads to a recurrence relation of the residual $r_k:=\enorm{\x_k-\x_\star}^2$ with the form of
\begin{align}
   r_{k+1}^2\leq r_k^2-\alpha_1 r_k^2/(1+\alpha_2 r_k).  \label{eq:main paper recursion}
\end{align}

We provide a bound on this recurrence relation in Appendix~\ref{app:pre_and_prp} to arrive to the final result of Theorem~\ref{thm:exact_amoo_convergence}. 

% \textcolor{red}{YE: if there's a space, should we elaborate on why the self-concordance assumption is useful? its useful due to a concrete technical reason we can elaborate on.}

\subsection{Practical Implementation}\label{sec:prac_imp}

We now describe a scalable approach for implementing \CAMOO\ that we experiment with in next sections. Towards large scale application of \CAMOO\ with modern deep learning architectures we approximate the Hessian matrices with their diagonal.  Prior works used the diagonal Hessian approximation as pre-conditioner~\cite{chapelle2011improved,schaul2013no,yao2021adahessian,liu2023sophia,achituve2024bayesian}. Notably, with this approximation the computational cost of \CAMOO\ scales linearly with number of parameters in the Hessian calculation, instead of quadratically. The following result establishes that the value of optimal curvature, and, hence the convergence rate of \texttt{Weighted-GD} instantiated with \CAMOO, degrades continuously with the quality of Hessian approximation (see Appendix~\ref{app:weyls consequence} for proof details).

% {\color{blue} BK: To mark the weights consistently according to the solution's feasible set }\textcolor{red}{YE: what did you mean? using the $\Delta_{m,w}$ notation?}
\begin{restatable}{proposition}{ApproxHessian}
\label{thm:app_hessian}
    Assume that for all $i\in [m]$ and $\x\in \mathbb{R}^n$ $|| \nabla^2 f_i(\x) - \mathrm{Diag}\brac{\nabla^2 f_i(\x)} ||_2\leq \enorm{\Delta}$ where $\enorm{\A}_2$ is the spectral norm of $\A\in \reals^{n\times n}$. Let $\w_\star \in \argmax_{\w\in \Delta_{m}} \lambda_{\min} \brac{\sum_{i} w_i \nabla^2 \mathrm{Diag}\brac{f_i(\x)}}$. Then,
    $
        \lambda_{\min} \brac{\sum_{i} w_{\star,i} \nabla^2  f_i(\x) }\geq \muglobal - 2\enorm{\Delta}.
    $
\end{restatable}

Next we provide high-level details of our implementation (also see Appendix~\ref{app:practical_implementation}).
\paragraph{Diagonal Hessian estimation via Hutchinson's Method.} We use the Hutchinson method~\cite{hutchinson1989stochastic, chapelle2011improved, yao2021adahessian} which provides an estimate to the diagonal Hessian by averaging products of the Hessian with random vectors. Importantly, the computational cost of this method scales linearly with number of parameters.

\paragraph{Maximizing the minimal eigenvalue.} Maximizing the minimal eigenvalue of symmetric matrices is a convex problem (\citet{boyd2004convex}, Example~3.10) and can be solved via semidefinite programming. For diagonal matrices the problem can be cast as a simpler max-min bilinear problem, 
$
    \argmax_{\w\in \Delta^m} \min_{\q\in \Delta^n} \w^\top \A \q,
$
where $n$ is the dimension of parameters, $\A\in \reals^{m\times n}$ and its $i^{th}$ row is the diagonal Hessian of the $i^{th}$ objective, namely, $\forall i\in [m],\ \A[i,:]=\diag(\nabla^2 f_i(\x))$.

This bilinear optimization problem is well studied~\citep{rakhlin2013optimization,mertikopoulos2018mirror,daskalakis2018last}. We implemented the PU method of~\citet{cen2021fast} which, loosely speaking, executes iterative updates via exponential gradient descent/ascent. PU has a closed form update rule and its computational cost scales linearly with number of parameters.




\section{The \PAMOO\ Weight Optimizer}\label{sec:pamoo}

In previous section, we introduced the global adaptive strong convexity parameter, $\muglobal$, the \CAMOO\ weight optimizer that chooses the weight vector adaptively and showed it has asymptotic linear convergence guarantees that depend on $\muglobal$. In this section we explore an additional adaptive mechanism for choosing the weight vector based on Polyak step-size design. We introduce the Polyak Aligned Multi-Objective Optimizer (\PAMOO). Unlike \CAMOO, it only requires information on the gradient, without requiring access to the Hessians. Interestingly, even though computationally much cheaper, \PAMOO\ exhibits improved convergence rate compared to \CAMOO. 

\PAMOO\ (Algorithm~\ref{alg:PAMOO})  generalizes the Polyak step-size design to AMOO. As such, it requires access to the optimal function values,  $f_i(\x_\star)$ for all $i\in [m]$. This information may not be readily available in general. However, in modern machine learning applications this value is often zero~\citep{loizou2021stochastic,wang2023generalized}. Further, there are variations of Polyak step-size in which a the optimal value is estimated~\citep{gower2021stochastic,orvieto2022dynamics}. We leave potential extensions of these to AMOO for future work. Compared to \CAMOO, \PAMOO\ only requires  access to the gradients of the objectives, and does not assume access to the Hessians. Further, it only requires solving a simple convex quadratic optimization problem in dimension $\reals^m$. This problem is simpler than a maximization of the smallest eigenvalue, required to solve by \CAMOO. 

We now define the local strong convexity parameter over a class of weights.  As we later show, this parameter controls the convergence rate of \PAMOO:
\begin{definition}[Local Strong Convexity $\mulocal$] \label{def:mu_local}
     The local strong convexity parameter, ${\mulocal\in \reals_{+}}$, is the largest value such that exists a weight vector $\w\in \Delta_m$ such that  
    \begin{align}
       \lambda_{\min}\left(\sum_{i=1}^m w_i \nabla^2 f_i(\x_\star) \right)\geq \mulocal, \label{eq:mu_local_definition}
    \end{align}
    where $\x_\star$ simultaneously minimizes $\{f_i\}_{i\in [m]}.$
\end{definition}
Notice that Proposition~\ref{prop:unique_optimal_sol} implies that $\x_\star$ is necessarily unique, and, hence $\mulocal$ is unique and well defined. Further, unlike the global adaptive strong convexity parameter, the local strong convexity parameter only depends on the curvature at $\x_\star$, namely, at the optimal solution. From Definition~\ref{def:mu_global} and Definition~\ref{def:mu_local} we directly get that $\mulocal\geq \muglobal$. 

The \PAMOO\ algorithm is inspired by the Polyak step-size design~\cite{polyak1987introduction, hazan2019revisiting} for choosing the learning rate in a parameter-free way. To provide with intuition for our derivation, consider the GD update rule in a single objective optimization problem, $\x_{k+1}=\x_k-\eta_k \g_k$. To derive the Polyak step-size design, observe that by convexity and the GD update rule we have that
% $$
% \enorm{\x_{k+1}-\x_\star}^2\leq \enorm{\x_{k}-\x_\star}^2 -2 \eta_t \brac{f(\x_k)-f(\x_\star)} + \eta_t^2\enorm{\nabla f(\x_k)}^2.
% $$
\begin{align}
    &\enorm{\x_{k+1}-\x_\star}^2 \label{eq:polyak standard main paper}\\
    &\leq \enorm{\x_{k}-\x_\star}^2 -2 \eta_k \brac{f(\x_k)-f(\x_\star)} + \eta_k^2\enorm{\nabla f(\x_k)}^2. \nonumber
\end{align}
Minimizing the upper bound on the decrease with respect to $\eta_k$ leads to $\eta_k = \brac{f(\x_k)-f(\x_\star)}/\enorm{\nabla f(\x_k)}^2,$ which is the Polyak step-size design choice. 

 Building on this derivation we develop the \PAMOO\ weight optimizer (see Algorithm~\ref{alg:PAMOO}). As we now show, interestingly, its convergence rate has the same functional form as \CAMOO, while depending on the local strong convexity parameter $\mulocal$ instead in $\muglobal$. Hence, \PAMOO\ has an improved upper bound on its convergence rate compared to \CAMOO\ (see Appendix~\ref{app:AMOO_results} for proof details). 

% \begin{figure*}[t]
% \centering
% \begin{minipage}{.48\textwidth}
% \begin{algorithm}[H]
%  \caption{\texttt{PAMOO-GD}}
%  \label{alg:P-Weighted-GD}    
% \begin{algorithmic}[1]
%  \STATE initialization
%  \WHILE{$t=1,2,\ldots$}
%      \STATE {\color{orange} $\w_k \gets \texttt{PAMOO}\brac{\{ f_i(\x_k) \}_{i=1}^m}$ }
%      \STATE $\g_t \gets \nabla f_{\w_k}(\x_k)$ 
%      \STATE $\x_{k+1} = \x_k - \g_t$
%  \ENDWHILE
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}%
% \hfill
% \begin{minipage}{.48\textwidth}
% \vspace{-0.3cm}
% \begin{algorithm}[H]
%  \caption{\PAMOO}
%  \label{alg:PAMOO}    
% \begin{algorithmic}[1]
%  \STATE \textbf{Inputs:} $\{ f_i(\x_k) \}_{i=1}^m$
%  \STATE $\w_k\in \arg\max_{\w\in \reals^m_+} 2\w^T \Delta_\x - \w^T \J_\x^T\J_\x \w$
%  \STATE $\Delta_{\x} := \left[  \Delta_{\x,1}\dots \Delta_{\x,m} \right]$,\  $\Delta_{\x,i}:=f_{i}(\x_k)-f_{i}(\x_\star)$
%   \STATE $\J_\x := \left[ \nabla f_1(\x) \dots \nabla f_m(\x)  \right] \in \reals^{d \times m}$
%  \STATE Return $\w_k$
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \label{fig:pamoo-algorithms}
% \end{figure*}

\begin{algorithm}[t]
 \caption{\PAMOO}
 \label{alg:PAMOO}    
\begin{algorithmic}[1]
 \STATE \textbf{inputs:} $\{ f_i(\x_k) \}_{i=1}^m$
 \STATE $\w\in \arg\max_{\w\in \reals^m_+} 2\w^\top \Delta_\x - \w^\top \J_\x^\top\J_\x \w$ \label{eq:pamoo optimization main paper}
 \STATE $\Delta_{\x} := \left[  \Delta_{\x,1}\dots \Delta_{\x,m} \right]$,\  $\Delta_{\x,i}:=f_{i}(\x_k)-f_{i}(\x_\star)$
  \STATE $\J_\x := \left[ \nabla f_1(\x) \dots \nabla f_m(\x)  \right] \in \reals^{n \times m}$
 \STATE \textbf{return:} $\w$
\end{algorithmic}
\end{algorithm}


\begin{restatable}[$\mulocal$ Convergence of \PAMOO]{theorem}{ExactPAmooConvergence}
\label{thm:exact_pamoo_convergence}
    Suppose $\{f_i\}_{i\in [m]}$ are $\beta$ smooth, $M_{\mathrm{f}}$ self-concordant, share an optimal solution $\x_\star$ and $\mulocal > 0$. Let $k_0 := \left\lceil \frac{64 \beta  \brac{\enorm{\x_0 - \x_\star} 3\sqrt{m}\beta M_{\mathrm{f}} -\sqrt{\mulocal}} }{ 3\mulocal^{3/2}}\right\rceil$, where $\enorm{\cdot}$ is the Euclidean-norm. Then, \texttt{Weighted-GD} instantiated with \PAMOO\ weight-optimizer and $\eta = 1$  converges with rate:
    \begin{align*}
            \enorm{\x_{k} - \x_\star} \leq 
        \begin{cases}
            \enorm{\x_{k_0} - \x_\star} \brac{1-\frac{3\mulocal}{32\beta}}^{(k-k_0)/2}  & k\geq k_0\\
            \enorm{\x_0 - \x_\star}- k \frac{\mulocal^{3/2}}{64 \beta^2 \sqrt{m} M_{\mathrm{f}}} & o.w.
        \end{cases}
    \end{align*}
\end{restatable}

% \textcolor{red}{YE: mention the novelty of analysis? that we make use of similar tools that were use in the analysis of \CAMOO?}

This result is established by generalizing the Polyak step-size method analysis (see~Eq.~\eqref{eq:polyak standard main paper}) while using a key observation. In the analysis, we upper bound the residual $\enorm{\x_k-\x_\star}^2$ by a quantity that depends on the curvature of the optimal weight vector at $\x_\star$, which is lower bounded by $\mulocal$, by definition. This is valid since we can replace $\w_k$ with an alternative weight vector -- only used in the analysis -- since $\w_k$ is an optimal solution of $\max_{\w\in \reals^m_+} 2\w^\top \Delta_\x - \w^\top \J_\x^\top\J_\x \w$. This flexibility allows us to upper bound expressions that depend on $\w_k$ by any nonnegative weight vector $\w\in \mathbb{R}^m_{+}$. Furthermore, we use similar tools as were developed in the analysis of \CAMOO: the property of self-concordant functions (Eq.~\eqref{eq:main paper self concordance}) and the recurrence relation bound (Eq.~\eqref{eq:main paper recursion}). 

\subsection{Practical Implementation}

% \textcolor{red}{YE: TODO. Add a pargraph and highlight it is much simpler to implement compared to \CAMOO.}

\PAMOO\ can be implemented in a straightforward and scalable way. It requires access to the Jacobian matrix, which can be readily calculated by accessing the gradients.  Calculating the matrix $\mathbf{J}_{\x}^\top \mathbf{J}_{\x} \in \mathbb{R}^{m\times m}$ 
% {\color{red}BK: Are you sure about the dimensions?} \textcolor{blue}{YE: yes. the matrix has to be in these dimension: since $w$ is a vector in $\mathbb{R}^m$.}{\color{red}BK: So $\mathbf{J}_x^\top$ is equivalent to $\w^\top \J_\x^\top$ from the algorithm?}\textcolor{blue}{YE: $J_x$ is defined in the algorithm box (the concatenation of gradients)}
has a computational cost of $O(nm^2)$, where $n$ is the dimension of the parameter space and $m$ is the number of objectives, and can be parallelized. Lastly, it requires solving a quadratic convex optimization problem in $\mathbb{R}^m$ where $m$ is expected to be of the order of $\sim 10$. This can be done efficiently with different convex optimization algorithms, e.g., projected GD. In practice, we initialize the weight vector $\mathbf{w}$ using the weight vector of the previous iterate. Hence, an approximate optimal solution is found within a few projected gradient descent iterations. \PAMOO\ as a potential advantage over \CAMOO\ due to its scalability and its simple implementation. Lastly, generalizing it to methods in which the optimal value is estimated instead of being given is left for future work~\citep{orvieto2022dynamics,gower2021stochastic}.

\section{$\epsilon$-AAMOO: Robustness to Alignment Assumption}\label{sec:robustness result}

% (Summary of previous results + here we show that the algorithms are robust to approximations in the alignment assumption (equation 1).)

We have analyzed \CAMOO\ and \PAMOO\ assuming perfectly aligned objectives (Eq.~\eqref{eq:aligned_functions}). However, in practice, objectives may be `similar' rather than perfectly aligned. Next, we extend AMOO to address this more realistic scenario and assume that the alignment assumption is approximately correct. We show that both algorithms are robust to such an approximation and remain effective under these conditions.

% (Define the set of approxximate optimal solution. Naturally, an algorithm that is close to this set can be considered as "good".)
Instead of assuming the objectives are perfectly aligned, we consider the $\epsilon$-Approximate AMOO (AAMOO) framework, in which there exists a near-optimal solution with respect to all objectives. Let $\mC_\epsilon$ be the set of $\epsilon$-approximate solutions:
\begin{align}
    \mC_\epsilon = \{ \x \in \reals^n | \ f_i(\x) - f_i(\x_{\star}^i) \leq \epsilon ~~ \forall i\in[m] \}, \label{eq:Ce_set}
\end{align}
where $\x_\star^i \in \argmin_{\x \in \reals^n}  f_i(\x)$. In $\epsilon$-AAMOO setting we assume that $\mC_\epsilon$ is not the empty set. This corresponds to a case in which exists a point that is a near-optimal solution for all objectives and can be understood as a natural generalization of the stricter AMOO setting in which $\epsilon=0$. 

\begin{figure*}[t]
\vspace{-8pt}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/local_curvature/MSE_plot.png} % Replace with your image file
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/selection/MSE_plot.png} % Replace with your image file
    \end{minipage}
    \caption{MSE versus gradient steps. \textbf{(left)} local curvature example instance, \textbf{(right)} selection example instance .}
    \label{fig:experiments_main}
\end{figure*}


% (informal theorem: camoo and pamoo are robust.)
The main result of this section shows that for both \CAMOO\ and \PAMOO\ the distance between $\x_k$ and an $\epsilon$-approximate solution $\x_\star^\epsilon \in \mC_\epsilon
$, converges to $\epsilon_{\mathrm{app}}$. Further, $\epsilon_{\mathrm{app}}$ is a polynomial function of $\epsilon$, structural quantities of the problem, and vanishes as $\epsilon\rightarrow 0$. This provides an approximate convergence guarantee of both algorithms. For \CAMOO, the result depends on the $\muglobal$ curvature, as in the AMOO setting. For \PAMOO the convergence depends on the best curvature within the set $\mC_{\epsilon}$ defined as follows:
\begin{definition}[$\epsilon$-Local Strong Convexity $\mulocaleps$] \label{def:mulocaleps}
     The $\epsilon$-local strong convexity, ${\mulocaleps\in \reals_{+}}$, is the largest value such that $\exists \x\in \mC_\epsilon$ exists a weight vector $\w\in \Delta_m$ such that  
    \begin{align}
       \lambda_{\min}\left(\sum_{i=1}^m w_i \nabla^2 f_i(\x) \right)\geq \mulocaleps. \label{eq:mu_mulocaleps_definition}
    \end{align}
\end{definition}
Let $\mu_\star(\x)=\max_{\w\in \Delta_m} \lambda_{\min}\left(\sum_{i=1}^m w_i \nabla^2 f_i(\x) \right)$ be the largest curvature at point $\x$. Then, $\mulocaleps$ is defined as the \emph{maximal} largest curvature in the set of near optimal solutions $\mC_\epsilon,$ namely, $\max_{\x\in \mC_\epsilon} \mu_\star(\x).$ Unlike $\muglobal$ that depends on the worst case curvature at all points, $\mulocaleps$  depends on the best-case curvature in $\mC_\epsilon.$ \PAMOO\ convergence depends on this quantity, which also satisfies $\mulocaleps\geq \muglobal$ 
% {\color{red} BK: $\geq$! No? Also $\geq \mulocal$.}\textcolor{blue}{YE: done + strictly speaking $\mulocal$ is not necessarily defined so i didnt want to mention it. if it is well defined, namely, AMOO holds, then it is true!}
for any $\epsilon$.


We now provide an informal description of an approximate convergence guarantee for both \CAMOO\ and \PAMOO\ (see Appendix~\ref{app:epsilon_app_sol} for the formal theorems and proofs):
\begin{theorem}[(Informal) Approximate Convergence in $\epsilon$-AAMOO]\label{thm:informal_epsilon_AAMOO} Suppose $\{ f_i\}_{i\in [m]}$ are $\beta$ smooth and $M_{\mathrm{f}}$ self-concordant. Let $\mu_{\mathcal{A}}$ be $\muglobal$ and $\mulocaleps$ for \CAMOO\ and \PAMOO, respectively. Assume that $\epsilon$-AAMOO holds and that $\epsilon \leq \mathrm{poly}\brac{ \mu_{\mathcal{A}},1/\beta,1/m,M_{\mathrm{f}}}$. Then, exists $\x_\star^\epsilon\in \mC_\epsilon$ such that for both \CAMOO\ and \PAMOO\ iterates satisfy
\begin{align*}
\enorm{\x_k - \x_\star^\epsilon} \leq
    \begin{cases}
        f_1(1- c \frac{\mu_{\mathcal{A}}}{\beta})^{(k-k_0)/2} +f_2 \epsilon^{1/4} & k\geq k_0\\
         \enorm{\x_0 - \x_\star^\epsilon} - f_3 k & o.w.
    \end{cases}
\end{align*}
Where $c\in (0,1)$, $f_1 = \enorm{\x_{k_0} - \x_\star^\epsilon}$ and $f_2, f_3$ and $k_0$ are polynomial functions of $\beta, \mu_{\mathcal{A}},m$ and $M_{\mathrm{f}}.$ 
\end{theorem}
Namely, the performance of  \CAMOO\ and \PAMOO\ degrades continuously with $\epsilon$, as the alignment assumption is violated. This result holds without any modification to the algorithms whatsoever.  
% {\color{red} BK: The following follows from the text, but I think it is worth emphasizing: $\eta$, and $\wmin$, are the only parameters in our algorithms, they are not changed in the approximate case.}


%[YE:Optional](Should we add how these results are established?)

% \section{On Linear Convergence in AMOO}

% \textcolor{red}{YE: TODO. Add a discussion on the fact the functions are wmin* muG strongly convex.}

\section{Toy Experiment}

We implemented \CAMOO, \PAMOO\ and compared them to a weighting mechanism that uses equal weights on the objectives (\texttt{EW}). We tested these three  algorithms as the \texttt{Weight-Optimizers} in \texttt{Weighted-GD} (see Algorithm~\ref{alg:Weighted-GD}). We experimented with SGD and ADAM as the optimizers of the weighted loss. In the learning problem we consider one network is required to match the outputs of a second fixed network. We denote the fixed network with parameters $\theta_\star$ as $h_{\theta_\star}: \mathbb{R}^{d_i}\rightarrow \mathbb{R}^{d_o}$ and the second network with parameters $\theta$ as $h_{\theta}:\mathbb{R}^{d_i}\rightarrow \mathbb{R}^{d_o}$. Both are 2-layer neural networks with relu-activation and 512 hidden units. 

We draw data from a uniform distribution $\mathcal{D}=\{ \x_i \}_{i}$ where $\x_i\in \mathrm{Uniform}([-1,1]^{d_i})$. We consider three loss functions:
\begin{align*}
    % \small
    &\forall i \in [3]: \quad f_i(\theta) \!=\! \frac{1}{|\mathcal{D}|}\!\!\sum_{\x\in \mathcal{D}}\!\!\brac{(h_\theta(\x)\! -\! h_{\theta_\star}(\x))^\top \H_i (h_\theta(\x)\! -\! h_{\theta_\star}(\x))}^{\alpha_i}\!,
\end{align*}
where $\H_i\in \mathbb{R}^{d_o\times d_o}$  is a positive definite matrix and $\alpha_i\geq 1$. Observe that all loss functions are minimized when $h_\theta(\x)=h_{\theta_\star}(\x)$, and, hence, it is an instance of AMOO. We investigated two instances. First, a selection example instance in which $\alpha_i=1$, and $\H_i= \mathrm{diag}(1, 0.01^i,\cdots,0.01^{i})$ for $i\in \{0,1,2\}$. There we expect the algorithms to adapt to the first loss function,~$f_1$. Second, a local curvature example where  $\H_i=\I$, and $\alpha_i\in \{1,1.5,2\}.$ For such a choice $f_1$ has larger curvature for large losses whereas $f_2$ and $f_3$ have larger curvature  for small losses. 

We track the performance by measuring the mean-squared error between the networks:
$$
    {\mathrm{MSE}=\frac{1}{|\mathcal{D}|} \sum_{\x\in \mathcal{D}} \enorm{(h_\theta(\x) - h_{\theta_\star}(\x))}}.
$$

Figure~\ref{fig:experiments_main} shows the convergence plots of our experiments. These highlight the potential of using GD algorithms that designed for AMOO: both \CAMOO\ and especially \PAMOO\ show an improved convergence rate. We provide additional experimental results and details in Appendix~\ref{app:practical_implementation}. The results show that \CAMOO\ modifies the weights, as expected, by adapting them to the local curvature; i.e., it changes the weights gradually during the optimization phase.

% In our experiment, we choose all but one of the losses to have low curvature, \textbf{simulating a selection example} (see Section~\ref{sec:AMOO setting}). The features $\x$ are generated by sampling from a $d$ dimensional Normal distribution $\mathcal{N}(0,\I_{10})$, and the targets are perturbed by an additional Normal noise, namely, $\y = h_{\theta_\star}(\x) +\epsilon_\sigma$ where $\epsilon_\sigma \sim \mathcal{N}(0,\sigma^2\I_{10})$, where $\I_{d}$ is the identity matrix in dimension $d$. We experiment with three different noise levels by modifying $\sigma$.
% We test both \CAMOO\ and \texttt{EWO} as the mechanisms for calculating a weighted loss $f_{\w}$ at each iteration, and apply either SGD or Adam optimizer to $f_\w$. In both cases we perform a grid search on the learning rate to find the best performing learning rate parameter.
% In Figure \ref{fig:axis_aligned_selection},  we show the results of our simulation. Generally, \CAMOO\ performs better than \texttt{EWO} in all settings across optimizers and noise levels. Adam (right) approaches a more optimal representation than SGD. More details about this experiment can be found in Appendix \ref{app:practical_implementation}.



\section{Conclusions}
% \textcolor{red}{YE: (i) SF assumption or a lower bound, (ii) stochastic optimization, (iii) large scale experiments}

In this work, we introduced the AMOO framework to study how aligned or approximately aligned multi-objective feedback can improve gradient descent convergence. We designed the \CAMOO\ and \PAMOO\ algorithms, which adaptively weight objectives and offer provably improved convergence guarantees. Future research directions include determining optimal rates for AMOO and conducting comprehensive empirical studies in different domains. Additionally, in this work, we have not explored a stochastic or non-convex optimization frameworks of AMOO, which we believe is of interest for future work.  We conjecture that algorithmic advancements in AMOO  will improve our ability to scale learning algorithms to handle large number of related tasks efficiently with minimal hyper-parameter tuning; a goal much needed in modern machine learning practice.

% \newpage
% \section{Impact}
% This paper presents work whose goal is to advance the field of Machine Learning by developing improved optimizers for common multi-objective settings.  There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% \section{Conclusion}

% \begin{algorithm2e}
%  \caption{Example Algorithm}
%  \SetAlgoLined
%   \KwData{this text}
%   \KwResult{how to write algorithm with \LaTeX2e }
%   initialization\;
%   \While{not at end of this document}{
%   read current\;
%   \eIf{understand}{
%   go to next section\;
%   current section becomes this one\;
%   }{
%   go back to the beginning of current section\;
%   }
%   } 
% \end{algorithm2e}

% \begin{figure}
% \centering
% \includegraphics[width=0.3\textwidth]{example-image-a}
% \caption{This is a figure.}
% \end{figure}

\bibliographystyle{plainnat}
\bibliography{citation}
