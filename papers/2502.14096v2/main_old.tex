\documentclass{article} % For LaTeX2e
% \usepackage{iclr2024_conference}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}
\usepackage{array} % for "\centering\arraybackslash" macro
\usepackage{geometry}
\geometry{bottom=1.0in, top=1.0in, left=1.2in, right=1.3in}
% Definitions of handy macros can go here


\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[table]{xcolor}
\usepackage{caption, subcaption}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{enumitem}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\rb}{\textbf{r}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\def\jb#1{{\color{red}#1}}
\def\YE#1{{\color{green} YE: #1}}

%To remove comments add flag as \usepackage[suppress]{color-edits}
\usepackage{color-edits}
\addauthor{YE}{red}
\addauthor{DJ}{green}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{blindtext}
\usepackage{hyperref}       % hyperlinks
\hypersetup{colorlinks,linkcolor=blue,
            citecolor=blue,
            urlcolor=magenta,
            linktocpage,
            plainpages=false}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bbm}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{chngpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{algorithm} 
\usepackage{algpseudocode}
\usepackage{natbib} 
\usepackage{thmtools,thm-restate}
\usepackage{wrapfig}
\usepackage{authblk}
\usepackage{tablefootnote}

\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}


\renewenvironment{abstract}
  {{\centering\large\bfseries Abstract\par}\vspace{0.7ex}%
    \bgroup
       \leftskip 20pt\rightskip 20pt\small\noindent\ignorespaces}%
  {\par\egroup\vskip 1.25ex}

\newenvironment{keywords}
{\bgroup\leftskip 20pt\rightskip 20pt \small\noindent{\bfseries
Keywords:} \ignorespaces}%
{\par\egroup\vskip 0.25ex}


\title{Aligned Multi-Objective Optimization (AMOO)}

\author{k}
\renewcommand\Authands{ and}
\affil[]{} 


\begin{document}
\maketitle

% \begin{abstract}
% \end{abstract}


% \begin{keywords}

% \end{keywords}

\renewcommand{\thefootnote}{\arabic{footnote}}

\section{Introduction}
Consider the following unconstrained multi-objective optimization problem:
\[
\min_{x \in \mathbb{R}^n} \,\, F(x),
\]
where $F: \mathbb{R}^{n} \to \mathbb{R}^m$ is a vector valued function, $F(x) = \left(f_1(x), f_2(x), \ldots, f_m(x) \right)$. In the Aligned Multi-Objective Optimization (AMOO) setting we make an additional assumption that the different functions are aligned, namely, that they share an optimizer. Namely, we assume that there exists a solution $x^*$ that optimizes the function $F(\cdot)$, such that 
\[
\nabla f_i(x^*) = 0 \quad \forall i \in \{1,2,\ldots,m\}.
\]
We are interested in gradient methods of the form,
\[
x_{t+1} = x_t + \eta d_t,
\]
where $d_t$ denotes a descent direction and $\eta$ is the step size. 


\section{Thoughts for multi-objective optimization.}
A lot of the current analysis for gradient based multi-objective optimization methods seems to depend on worse case parameters, in particular, the maximum of lipschitz constants for individual objectives: $L = \max_{i=1,2,\ldots,m} l_i$. This is quite intuitive. Using \eqref{eq:quadratic upper bound}, $\nabla^2 f_i(x) \preceq L I$ for all $i \in \{1,2,\ldots, m\}$. So, 
\begin{equation}
f_i(y) \leq f_i(x) + \nabla f_i(x)^\top(y-x) + \frac{L}{2} \|y-x\|^2_2 \quad \forall i.
\end{equation}
As long as we can find a descent direction $d$ such that $\nabla f_i(x)^\top d < 0 \,\, \forall i$, we make progress for each objective. In general multi-objective optimization, when such a $d$ cannot be found it implies the objective are in conflict with one another. By assumption, we have a unique optimal solution $x^*$, so we can always find such a descent direction at any suboptimal solution $x$. Previous work aims to find $d$ in the convex hull of the individual gradients \citep{mercier2018stochastic, desideri2012multiple},
\begin{equation}
    d = \sum_{j=1}^m \alpha_j \nabla f_j(x), \,\,\,\text{such that} \, \alpha_j \geq 0; \sum_{j} \alpha_j = 1.
\end{equation}
One concrete idea is to analyze the multiple gradient descent algorithm of \cite{desideri2012multiple} for the case when $m < n$ and the individual gradients are linearly independent. We can also use this algorithm straightforwardly for Deep RL and RLHF.

{\color{red} YE: In \citet{sener2018multi} the authors claim to implement the algorithm of \cite{desideri2012multiple}.}

One way we were thinking about getting an improved rate is as follows. Given the assumption that there exists a unique optimal solution $x^*$, we can just take gradient steps with respect to the objective $f_j(\cdot)$ with the smallest coefficient of Lipschitz continuity, i.e. $j = \argmin_{i=1,2,\ldots,m} \, l_i$. This way we do not have to worry about convergence for all objectives. This argument is ok for asymptotic convergence. For finite time bounds, we need to take into account the approximation error for all other objectives, $f_i(x_T) - f_i(x^*), \,\, \forall i \neq j$, at iterate $x_T$. 

\paragraph{A useful result (maybe):} Assuming that all functions, $f_i, i \in \{1,2, \ldots, m\}$ are convex, then can we claim that any direction $d = \sum_{i=1}^m w_i \nabla f_i(x), \, w_i \geq 0, \sum_{i=1}^m w_i = 1$ is a descent direction for the scalarized function $f_{\theta} = \sum_{i=1}^m \theta_i f_i(x)$?  Consider the inner product $d^\top \nabla f_{\theta}$. { \color{red} YE: in the following it should be $\nabla f_{\theta}$? }
\begin{align}
    d^\top \nabla f_{\theta} &= \left(\sum_{i=1}^m w_i \nabla f_i(x)\right)^\top \left(\sum_{j=1}^m \theta_j \nabla f_j(x) \right) \\
    &= \sum_{i=1}^m \sum_{j=1}^m w_i \left(\nabla f_i(x)^\top \nabla f_j(x)\right) \theta_j \\
    &= w^\top \left(J^\top J\right) \theta \\
    &\geq 0 \label{eq:psd matrix}.
\end{align}
Here $J \in \mathbb{R}^{n \times m}$ is the Jacobian matrix, $J = \left[ \nabla f_1(x), \ldots, \nabla f_m(x) \right]$. Inequality \eqref{eq:psd matrix} can be shown assuming $J$ is full rank which implies that $J^\top J$ is a positive semidefinite matrix. {\color{red} YE: this should be an assumption right? namely, that $J$ is full rank.}

{\color{blue}{JB}: I think if we assume that all functions $f_i$ are strongly convex instead of just convex, then this implies full rank of the Jacobian at all points. Assume that there exists a point $x$ such that $J(x)$ is not full rank. Then, there exists a direction $v \in \mathbb{R}^m$ such that $f_j(x + \alpha v) = f_j(x)$ for some $j \in \{1, 2,\ldots, m\}$ and for any $\alpha \in \mathbb{R}$. This contradicts that $f_j$ is strongly convex.}
\begin{itemize}
    % \item First note that as $J$ is assumed to be full rank, $J^\top J$ is a positive semidefinite matrix.
    \item As positive semidefinite matrices are real and symmetric, the spectral theorem applies and $J^\top J$ is orthogonally diagonalizable.
    \item This implies $J^\top J = Q \Lambda Q^\top$ where $Q$ is a orthogonal matrix with columns being the $m$ linearly independent eigenvectors of $J^\top J$ and $\Lambda = \rm{Diag}(\lambda_1, \ldots, \lambda_m)$ is a diagonal matrix with the corresponding eigenvalues, $\lambda_1, \ldots, \lambda_m$ such that $\lambda_i \geq 0$. 
    \item Since the eigenvectors of $J^\top J$ are linearly independent, they form a basis of $\mathbb{R}^m$. Thus, for any $\theta \in \mathbb{R}^m$, we have $\theta = Q z$ for some $z \in \mathbb{R}^m$. Thus
    \[
        \left( J^\top J \right) \theta = \left( Q \Lambda Q^\top \right) Q z = Q \Lambda z = \lambda \odot Qz = \lambda \odot \theta,    
    \]
    where $\lambda = [\lambda_1, \ldots, \lambda_m]$ is the vector of all eigenvalues and $\odot$ denotes the elementwise product. {\color{red} YE: elementwise product is a multiplication by the matrix $\Lambda$? may be simpler to use this notation. I'm not aboy this equality: not sure $Q$ and $\Lambda$ can be exchanged as in the third equality, if I understand correctly.}
\end{itemize}
Therefore, 
\begin{equation}
    w^\top \left( J^\top J\right) \theta = w^\top \left(\lambda \odot \theta \right) = \sum_{i=1}^m \lambda_i w_i \theta_i \geq 0 
\end{equation}
since $\lambda_i \geq 0$ and $w, \theta \in \Delta^{m-1}$.


\section{A possibly useful question:}

Consider a quadratic optimization setting. Assume the both $f_1$ and $f_2$ are strongly convex (with parameter $\mu_i$) and smooth (with parameter $L_i$) and have similar optimal point. Specifically, consider the case they are quadratic functions $f_i = x^T A_i x$. We know that running GD on  each of the objectives separately will result in guarantee of the form (where $\kappa_i = L_i/\mu_i$):
\begin{align*}
    f_i(x_t) - f_i(x^*) \leq \exp(-t/\kappa_i) (f_i(x_o) - f(x_*))
\end{align*}
see \citet{bach2021learning}, proposition 5.3.

Can we get improved convergence rate of GD by scalarzing the function? E.g., by running GD on a $$f(x) = w_1 f_1(x) + w_2 f_2(x).$$
Other options could be adaptive to the current $x_t$.

\paragraph{Examples quadratic case.}
In the quadratic case we have that:
\begin{align*}
    f_1(x) = x^T A_1 x\\
    f_2(x) = x^T A_2 x.
\end{align*}

\begin{enumerate}
    \item 1D quadratic functions. In this case the condition number is always 1 since the maximal eigenvalue of $A_i$ equals to the minimal (since there's a single one. Given a fixed learning rate which is small enough, we would prefer to choose $f_i$ with larger curvature (but I don't see how this intuition comes about with the convergence guarantee above).
    \item 2D quadratic functions. The geometric intuitive here would be quite different and it depends on $x_0$.
    \item General thought: would it make sense to choose the vector $w$ such that:
    \begin{align*}
        w \in \arg\max \frac{\lambda_{\max}(w_1 A_1 + w_2 A_2)}{\lambda_{\min}(w_1 A_1 + w_2 A_2)}
    \end{align*}
\end{enumerate}

\section{Improving convergence with 2nd order method}

We generalize the idea of previous section and make it a bit more concrete. 

Recall that a function $f$ is $\mu$ stronglly convex if for all $x,x'\in \mathcal{X}$
\begin{align*}
    f(x') \geq f(x) + \nabla f(x)(x'-x) + \frac{\mu}{2} ||x-x'||.
\end{align*}

This also implies that a weighted function is $\mu$ strongly convex with respect to the weights, namely, that 
\begin{align*}
    f_w(x) \equiv \sum_m f_m(x)w_m
\end{align*}
is $\sum_{m} w_m \mu_m$ strongly convex.

We can define the optimal weight and optimal strong convexity parameters as follows.  The optimal strong convexity parameter $\mu_\star \in \mathbb{R}_{>0}$ is the largest scalar such that exists $w\in \Delta_m$ (the simplex over $M$ tasks) it holds that: 
\begin{align*}
    f_w(x') \geq f_w(x) + \nabla f_w(x)(x'-x) + \frac{\mu_\star}{2} ||x-x'||.
\end{align*}
In words, $f_w$ is a set of functions, parameterized by $w\in \Delta_M$, each one is $\mu_w$ strongly convex, and $\mu_\star=\max_{w\in \Delta_M}\mu_w$ is the largest one. 

\paragraph{Example 1: When does $\mu_\star$ give significant improvement?} Let $\Delta>0$ and consider having two functions:
\begin{align*}
    f_1(x) = (1-\Delta_1)x^2 + \Delta_1  y^2\\
    f_2(x) = (1-\Delta_2)y^2 + \Delta_2 x^2.
\end{align*}
These functions are only $\Delta$ strongly convex. However, for $\hat{w}=\left(1/2,1/2 \right)$ we get that $f_{\hat{w}}$ is $1$ strongly convex. Hence, in that setting $\mu_\star \geq 1$ (probably in this case it actually equals 1. to show it we need to solve the minimal eigenvalue maximization problem) even though $f_1$ and $f_2$ are only $\Delta$ strongly convex, where $\Delta$ may be arbitrarily small. 
{\color{red} YE: which additional examples can we imagine we may get improvements?}

\paragraph{Remark.} Notice we can also define a local strong convexity parameter (namely, $\mu_\star(x)$ that depends on the local curvature), and design an algorithm that converges with this parameter, but for now, we can stick with the simpler global definition.

\begin{algorithm}
	\caption{A-MOO via Hessians} 
	\begin{algorithmic}[1]
		\For {$t=1,2,\ldots$}
			\State Get Hessian $\nabla^2 f_m(x_t)$ for all $m\in [M]$
			\State (P1) Calculate $w_t\in \arg\max_{w\in \Delta_{M}} \lambda_{\min} (\sum_{m} w_m \nabla^2 f_m(x_{t}))$ 
                \State Get gradients of $f_m(x_t)$ and calculate weighted gradient $g_t = \sum_{m} w_m \nabla f_m(x_t)$
                \State Gradient update: $x_{t+1} = x_t - \eta g_t$		
  \EndFor
	\end{algorithmic} 
\end{algorithm}


\begin{theorem}[Convergence of 2nd Order A-MOO]
    Suppose that all the functions $f_m$ are $L$ smooth. If $\eta\leq 1/L$ then     A-MOO with Hessians converges with the following rate:
    \begin{align*}
        ||x_t-x_\star || \leq \left(1-\frac{\mu_\star}{L} \right)^t || x_0-x_\star ||
    \end{align*}
\end{theorem}

The intuition for this algorithm is quite simple: the strong convexity parameter is the minimal eigenvalue of the Hessian matrix (e.g. https://math.stackexchange.com/questions/673898/lipschitz-smoothness-strong-convexity-and-the-hessian)

We first prove the following lemma.
\begin{lemma}\label{lem:f_w_is_lip}
    If $w\in \Delta_M$ and all $f_m$ are $L$ Lipchitz then $f_w$ is also $L$ Lipchitz.
\end{lemma}
\begin{proof}
    Since $f_m$ is $L$ Lipchitz it holds that
    \begin{align*}
        f_m(x') \leq f_m(x ) + \nabla f_m(x) \cdot (x'-x) + \frac{L}{2}||x-x' ||.
    \end{align*}
    Since $w_m\geq 0$ and $\sum_m w_m =1$ we can multiply each inequality by $w_m$, summing over the $M$ inequalities and get
    \begin{align*}
        f_w(x') \leq f_w(x ) + \nabla f_w(x)\cdot (x'-x) + \frac{L}{2}||x-x' ||,
    \end{align*}
    by using the linearity of the gradient. Namely, $f_w$ is $L$ Lipchitz.
\end{proof}

\begin{proof}
    The proof of this result essentially follows the proof here: \href{https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf}{Lecture 9 (Sham Kakade).} 

    Observe that since each $f_m$ is $L$ Lipschitz also their weighted sum is $L$ Lipschitiz (since $w\in \Delta_M$ is an element in the simplex).
    
    By lemma 3.3. in Lecture 9 we have that (since $f_w$ is $L$ Lipchitz by Lemma~\ref{lem:f_w_is_lip}):
    \begin{align}
        \label{eq: gradient norm lbds function value gap}
        ||
        \nabla f_w(x)||^2 \leq 2L (f_w(x) - f_w(x_\star)).
    \end{align}

    There are two important observations we need to use in the following proof:
    \begin{enumerate}
        \item By definition $f_w$ is strongly convex \emph{with strongly convex parameter larger than $\mu_\star$} (follows since the minimal eigenvalue of the Hessian characterizes the strongly convex parameter).
        \item Since for any $f_w$ the optimal solution is $x_\star$ (by assumption that all $f_m$ has similar optimal solution) it holds that $x_\star$ is also the optimal solution of $f_w(x)$.
    \end{enumerate}
     
    
    From this point we replicate the proof in Lecture 9. By strong convexity of $f_w$ it holds that
    \begin{align*}
        \nabla f_w(x)(x-x_\star) \geq f_w(x) - f_w(x_\star) + \frac{\mu_\star}{2} || x-x_\star ||^2.
    \end{align*}
    Hence, it also holds for $x=x_t$, namely:
    \begin{align*}
        \nabla f_w(x_t)(x_t-x_\star) \geq f_w(x_t) - f_w(x_\star) + \frac{\mu_\star}{2} || x_t-x_\star ||^2.
    \end{align*}
    Using these, we have that:
    \begin{align*}
        &|| x_{t+1} -x_\star ||^2 = || x_t - \eta \nabla f_{w_t}(x_t) -x_\star ||\\
        &= ||x_{t} -x_\star  || -2\eta \nabla f_{w_t}(x_t) (x_t - x_\star) +\eta^2 || \nabla f_{w_t}(x_t) ||^2\\
        &\leq  ||x_{t} -x_\star  || - 2\eta \left( f_{w_t}(x_t) - f_{w_t}(x_\star) + \frac{\mu_\star}{2} || x_t-x_\star ||_{gg^T}^2 \right) +\eta^2 || \nabla f_{w_t}(x_t) ||^2\\
        &\leq ||x_{t} -x_\star  || - 2\eta \left( f_{w_t}(x_t) - f_{w_t}(x_\star) + \frac{\mu_\star}{2} || x_t-x_\star ||^2 \right) +2\eta^2 L (f_w(x_t) - f_w(x_\star))\\
        & \leq \left( 1- \eta \mu_\star \right)||x_{t} -x_\star  || + 2\eta(\eta L -1 )\left( f_{w_t}(x_t) - f_{w_t}(x_\star) \right)\\
        &\leq \left( 1-\frac{\mu_\star}{L} \right)||x_{t} -x_\star  ||,
    \end{align*}
    where we used the upper bound on $\eta$ in the last step to upper bound both terms (for the second term we also use $f_{w_t}(x_t) - f_{w_t}(x_\star)\geq 0$)
\end{proof}

\paragraph{Observation.} In the above analysis we assumed we choose $w_t$ to solve the maximal minimal eigen value SDP. However, can't we assume it simply maximizes the term $\sum_m\nabla f_m(x_t)^T x_t = f_w(x_t)^T x_t$? Namely, set $\bar{w}$ to solve the following linear optimization problem (linear in $w$):
\begin{align}
    \bar{w}_t\in \arg\max_w \nabla f_w(x_t)^T x_t. \label{eq:w_bar_def}
\end{align}
Let $w_{t,\star}$ be the $w_{t,\star}$ that solves the maximal minimal eigenvalue SDP. We have that 
\begin{align}
    &\nabla f_{\bar{w}}(x_t)^T x_t \geq \nabla f_{w_{t,\star}}(x_t)^T x_t \nonumber \\
    &\rightarrow - \nabla f_{\bar{w}}(x_t)^T x_t \leq - \nabla f_{w_{t,\star}}(x_t)^T x_t  \label{eq:inequality_w_bar_w_star}.
\end{align}
This would be useful if we won't have $x_\star$ in the analysis above.




\paragraph{Questions and additional steps:}
\begin{enumerate}
    \item It would be good to study additional proof techniques besides of this one. Does anyone know additional references? References for adaptive algorithms for the strongly convex case?
    \item Can we get similar result for the constraint case? here we assume $x$ is not constraint.
    \item Can we generalize it to SGD?
    \item Can we get 1-order method with similar guarantees?
    \item Can we get adaptive algorithm to $L$?
    \item What about other improvements? + Understanding better the meaning of $\mu_\star$.
    \item Implementing it in practice.
\end{enumerate}


\paragraph{Can we use similar arguments to adapt to Lipschitz constants?}
Assume that we have access to smoothness parameters of each function, $l_1, \ldots, l_m$. First note that as a consequence of smoothness,
\begin{align*}
    f_i(y) \leq f_i(x) + \nabla f_i(x)^\top (y-x) + \frac{l_i}{2} \|y-x\|^2.
\end{align*}
For any $w \in \Delta^{m-1}$, 
\begin{align*}
    % \sum_i w_i f_i(y) &\leq \sum_i w_i f_i(x) + \left( \sum_i w_i \nabla f_i(x) \right)^\top (y-x) + \left( \sum_i \frac{w_i l_i}{2} \right) \|y-x\|^2. \\
    % \Leftrightarrow \,\,\,
    f_w(y) &\leq f_w(x) + \nabla f_w(x)^\top (y-x) + \frac{l_w}{2} \|y-x\|^2.
\end{align*}
Here $f_w(x) = \sum_i w_i f_i(x)$ and $l_w = \sum_{i} w_i l_i$. Let $w^* = \argmin_{w \in \Delta^{m-1}}\, w_i l_i$, $k = \argmin_{i=1,\ldots,m} \, l_i$ and $l^* = l_k$. Then, using the two inequalities as presented before should give an improved rate.
\begin{align*}
    \|f_k(x)\|^2 &\leq 2 l_k \left( f_k(x) - f_k(x^*) \right) \\
    \nabla f_k(x)^\top (x - x^*) &\geq f_k(x) - f_k(x^*) + \frac{\mu_k}{2} \|x - x^*\|^2. 
\end{align*}
Following the analysis as presented above should give us,
\begin{equation}
    \|x_{t+1} - x_t \|^2 \leq \left( 1 - \frac{\mu_k}{l_k} \right) \|x_t - x^*\|^2. 
\end{equation}

{\color{red} YE: yup. we can also get information of $l_k$ from second order information, since the lipchitz constant is also an upper bound on the largest eigenvalue of the Hessian matrix $l_m=\max_{x}\max_{m} \nabla^2 f_m(x)$. Specifically, to find weights that would solve the optimization problem (which i believe is hard to solve from computational theoretic perspective):
\begin{align*}
    \max_w \frac{\lambda_{\text{min}}(\sum_m w_m \nabla^2 f_m(x))}{\lambda_{\text{max}}(\sum_m w_m \nabla^2 f_m(x))},
\end{align*}
may be the one we'd truly like to solve.
}


\section{Solving for diagonal Hessian's}

In this section we focus on efficient solution of $(P1)$,
    \begin{align*}
        \hat{w} \in \arg\max_{w\in \mathcal{W}} \lambda_{\mathrm{min}}(\sum_i w_i \nabla^2 f_m(x)),
    \end{align*}
when the Hessians are diagonal.

To be clear, let us define the problem $(P1)$ as optimizing some function $g(\cdot): \Sc_n^{++} \to \mathbb{R}^{+}$ of the Hessian matrices of individual functions $f_i$,
\begin{align*}
    &g \left( \sum_{i \in \{1, 2, \ldots, m\}} w_i \nabla^2 f_i(x) \right) \\
    &\text{subject to} \,\, w_i \geq 0.
\end{align*}
We will mostly be interested in three choices of $g(\cdot)$, namely, $g(A) = \lambda_{\min}(A)$, $g(A) = \lambda_{\max}(A)$ and $g(A) = \lambda_{\max}(A) / \lambda_{\min}(A)$ which correspond to the minimum eigenvalue, maximum eigenvalue and the condition number of matrix A respectively.


\subsection{Reducing $(P1)$ to a min-max optimization}

% Assuming all Hessian matrices of all functions $f_i$ are diagonal, we can reduce the solution of $(P1)$ to a min-max optimization problem. 
Assuming we have access to diagonal approximations to Hessian matrices of all functions, we can reduce the solution of $(P1)$ to a min-max optimization problem.
For the strongly convex and smooth cases this optimization problem is convex-concave. When optimizing the condition number it results with a generalized fractional linear optimization problem. Let $\Delta_m := \{a \in \mathbb{R}^m: a_i \geq 0, \sum_{i=1}^m a_i = 1\}$ be the $m$-dimensional simplex. Let $A_i$ be a diagonal approximation of the Hessian of function $f_i$, and $\lambda_{i,j}$ denote its $j^{th}$ eigenvalue. 
\begin{itemize}
    \item \emph{Strongly convex case.} 
    The goal is to find the weights $w \in \Delta_m$ to maximize the strong-convexity parameter of the weighted functions. Problem $(P1)$ can be written as 
    \begin{align*}
    \hat{w} \in \argmax_{w \in \Delta_m} \,\, \lambda_{\min}\left( \sum_{i=1}^m w_i A_i \right)
    \end{align*}
    and can equivalently be written in the following form:
    \begin{align}
        \hat{w} &\in \argmax_{w\in \Delta_m} \,\, \min_{j\in [n]} \sum_i^m w_i \lambda_{i,j}  \label{eq:P1j}\\
        \hat{w} &\in \argmax_{w\in \Delta_m} \,\, \min_{q\in \Delta_n} \sum_{i,j} w_i \lambda_{i,j} q_j \label{eq: minmax P1},
    \end{align}
    % where $ \lambda_{i,j}$ is the jth eigenvalue of the Hessian $\nabla^2 f_j(x).$ 
    This optimization problem is a max-min and can be solved via best response or primal dual update of $w$.
    \begin{itemize}
        \item {\color{red}Daniel: We can rewrite (\ref{eq:P1j}) as $\hat{w} \in \argmax_{w\in \Delta_m} \,\, g(w)$ where $g(w) = \min_{j\in [n]} \sum_i w_i \lambda_{i,j}$. We can also optimize over $(w_1, \ldots, w_{n-1}) \in [0, 1]$ and set $w_n = 1 - \sum_{i=1}^{n-1}w_i$. We can see that $g(w)$ is concave (min of affine functions) and so we should be able to solve this with projected gradient descent.} {\color{blue} Yon: exactly. i think you describe the best-response approach to solve minmax convex-concave games.}
    \end{itemize}

    
    \item \emph{Smooth case.} In this case, we want to minimize the Lipschitz constant which is the maximal eigenvalue of the Hessian matrix. Problem ($P1$) in this case can be written as
    \begin{align*}
    \hat{w} \in \argmin_{w \in \Delta_m} \,\, \lambda_{\max}\left( \sum_{i=1}^m w_i A_i \right)
    \end{align*}
    We will get analogous problem to the strongly convex case as in equation~\eqref{eq: minmax P1}, where the max and min are replaced (we will minimize the maximal eigenvalue).
    \item  \emph{Minimize the condition number.} Define the condition number of a matrix as $\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}$. Problem $(P1)$ will take the form of 
    \begin{align}
        \argmin_w \,\, \frac{\max_{j\in [n]} \sum_i w_i \lambda_{i,j}}{\min_{j\in [n]} \sum_i w_i \lambda_{i,j}}. \label{eq:condition number diag P1}
    \end{align}
This can be reduced to a fractional linear program. A fractional linear program takes the form of (see \url{http://www.seas.ucla.edu/~vandenbe/ee236a/lectures/lfp.pdf})
\begin{align*}
    \min_x \max_{i} \frac{c_i^Tx + d_i}{f_i^T x + g_i}.
\end{align*}

To see this observe the following holds for any $w$
\begin{align*}
    & \frac{\max_{j\in [d]} \sum_i w_i \lambda_{i,j}}{\min_{j\in [d]} \sum_i w_i \lambda_{i,j}}=\max_{j\in [d]}\max_{j'\in [d]} \frac{ \sum_i w_i \lambda_{i,j}}{ \sum_i w_i \lambda_{i,j'}} =\max_{k\in [d]\times [d]}\frac{c_k^T w}{f_k^T w} 
\end{align*}
where for a $k\in [d]\times [d]$ that represents a tuple $(j,j')$ we set $c_k^T=\lambda_{j}\in \mathbb{R}^M$ and $f_k^T=\lambda_{j'}\in \mathbb{R}^M$ and $\lambda_{j}$ is a vector with $\lambda_{i,j}$ its elements. This problem is not a simple convex optimization problem, but, as a heuristic, GD can also be used to solve it, namely, equation~\eqref{eq:condition number diag P1} can be written as
\begin{align*}
    \min_w\max_{q\in \Delta_{d^2}}\sum_k q_k\frac{c_k^T w}{f_k^T w}, 
\end{align*}
and can be also solve with best-response our GD.

\jb{This part is a bit confusing to me. I think the cleaner way might be write it as}
\begin{align}
\hat{w} \in \argmin_{w \in \Delta_m} \,\, \max_{q, \tilde{q} \in \Delta_n} \frac{\sum_i w_i \lambda_{i,j} q_j}{ \sum_i w_i \lambda_{i,j} \tilde{q}_j}
\end{align}
\end{itemize}


\subsection{Approximation guarantees}
In this section we explore approximate solutions to $(P1)$: assume that $\nabla^2 f_i(x)$ can be approximated by $A_i$, what is the quality of solving the optimization problem $(P1)$ over $\{A_i\}_i$ instead? The approximation results in the following rests on the following well known result that establishes continuity of eigenvalues.
\begin{theorem}[(Corollary of) Weyl's Theorem] Let $A$ and $\Delta$ be symmetric matrices in dimension $d$. Let $\lambda_j(A)$ be the jth largest eigenvalue of a matrix $A$. Then, for all $j\in [d]$ it holds that $\| \lambda_j(A) - \lambda_j(A+\Delta) \| \leq \| \Delta\|_2$, where $\| \Delta\|_2$ is the operator norm of $\Delta$.
\end{theorem}

Assume that $A_i = \nabla^2 f(x) + \Delta_i$. 
\begin{itemize}
    \item \emph{Strongly convex case.} Let $w_\star $ denote the optimal weights of the optimization problem,
    \begin{align*}
        w_\star \in \arg\max_{w\in \Delta} \lambda_{\mathrm{min}}(\sum_i w_i \nabla^2 f_i(x)),
    \end{align*}
    and let $g(w_\star)$ denote the optimal value, $g(w_\star) = \lambda_{\min} (\sum_i w_{\star, i} \,\, \nabla^2 f_i(x))$. Further, let $\hat{w}_\star$ denote the solution of the optimization problem of the approximation
    \begin{align*}
        \hat{w}_\star \in \arg\max_{w\in \Delta} \lambda_{\mathrm{min}}(\sum_i w_i A_i),
    \end{align*}
    and denote $\hat{g}(\hat{w}_\star)$ as its value, $\hat{g}(\hat{w}_\star) = \lambda_{\min} (\sum_i \hat{w}_{\star, i} \,\, A_i$. Then, the following holds.
    \begin{align*}
        &g(w_\star) = g(w_\star) - \hat{g}(w_\star) + \hat{g}(w_\star) -\hat{g}(\hat{w}_\star) +\hat{g}(\hat{w}_\star) - g(\hat{w}_\star) +g(\hat{w}_\star) \\
        &\stackrel{(a)}{\leq}  g(w_\star) - \hat{g}(w_\star)  +\hat{g}(\hat{w}_\star) - g(\hat{w}_\star) +g(\hat{w}_\star) \\
        &\leq |g(w_\star) - \hat{g}(w_\star)|  + |\hat{g}(\hat{w}_\star) - g(\hat{w}_\star)| +g(\hat{w}_\star)\\
        & \stackrel{(b)}{\leq}  2 \| \Delta \|_2 +g(\hat{w}_\star).
    \end{align*}
    Where $(a)$ holds since $\hat{g}(w_\star) -\hat{g}(\hat{w}_\star)\leq 0$ by the optimality of $\hat{w}_\star$ on $\hat{g}$. Further, $(b)$ holds as a consequence of Weyl's theorem and the assumptions of the approximation error as the following inequalities show. Recall that for any $w\in \Delta_m$ it holds that 
    $$
    \| \sum_i w_i A_i -  \sum_i w_i \nabla^2 f_i(x) \|_2 \leq \sum_i w_i \|  A_i -  \nabla^2 f_i(x) \|_2 \leq \| \Delta \|_2
    $$
    since $\sum_i w_i =1$. Hence, by Weyl's theorem it holds that 
    \begin{align*}
        |g(w_\star) - \hat{g}(w_\star)\| \leq \| \Delta \|_2 \text{ and } |g(\hat{w}_\star) - \hat{g}(\hat{w}_\star)\| \leq \| \Delta \|_2.
    \end{align*}

    By setting $g(w_\star)\geq \mu_\star$, by the strong convexity assumption, we get that 
    \begin{align*}
        g(\hat{w}_\star) \geq \mu_\star -2\| \Delta\|_2.
    \end{align*}
    This means, that weighting the functions with $\hat{w}_\star$ results in $\mu_\star -2\| \Delta\|_2$ strongly convex function.

    \item \emph{Smooth case.} Same argument as the strongly convex case.

    \item \emph{Condition number.} TODO
    
\end{itemize}

\subsection{Thought on using just gradients}
A recent paper \citep{mishkin2024directional} gives convergence guarantees in terms of the directional smoothness and directional strong convexity parameters. For example, 
\begin{align}
    \mu(x,y) = \inf_{t \in [0,1]} \,\, \frac{\langle \nabla f(x + t(y-x)) - \nabla f(x), y-x \rangle}{t \|y-x\|^2_2}
\end{align}
The observation is that $\mu(x,y) \geq \mu$, and one can get path dependent convergence rates of the form
\begin{align}
    \|x_{t+1} - x^*\|^2_2 \leq \left(1 - \frac{\mu(x_t, x_{t+1})}{L} \right) \|x_t - x^*\|^2_2.
\end{align}
One thought is to use this notion of strong convexity to optimize for the weights $w$ and find a good update direction. Assume that we use gradient descent with a small fixed step-size $\eta$. Then, $y_t(w) = x_{t+1} = x_t - \eta \sum_{i} w_i \nabla f_i(x_t)$. Can we find $w$ using the weighted directional strong convexity parameter?

\paragraph{Optimizing for weighted directional strong convexity}
Let us define it in terms of $w$ as
\begin{align}
    \mu(x,y(w)) = \inf_{t \in [0,1]} \,\, \frac{\langle \nabla f_w(x + t(y(w)-x)) - \nabla f_w(x), y(w)-x \rangle}{t \|y(w)-x\|^2_2}
\end{align}
We want to optimize over the weights to maximize $\mu(x,y(w))$,
\begin{align}
    w^* \in \argmax_{w \in \Delta_m} \,\, \mu(x,y(w))
\end{align}
where $y(w)-x = -\eta \nabla f_w(x)$ and $f_w(x) = \sum_i w_i f_i(x)$ and $\nabla f_w(x) = \sum_i w_i \nabla f_i(x)$. Here, both the next iterate $y(w)$ and the objective function $f_w$ are functions of $w$. I think under some conditions (continuity of gradients?), we can formulate a max-min problem (the inf becomes the max if it can be achieved)? For a fixed value of $t$, can we take a gradient step w.r.t. $w$? 

Let's fix a value of $t=\hat{t}$. Then, 
\begin{align}
    \mu(t, x, y(w)) = \frac{\langle \nabla f_w(x + \hat{t}(y(w)-x)) - \nabla f_w(x), y(w)-x \rangle}{\hat{t} \|y(w)-x\|^2_2}
\end{align}
We can use gradient steps to update the weights, 
\begin{align}
    w_{t+1} = w_t + \eta_w \frac{\partial}{\partial w} \mu(t, x, y(w)) \Big|_{w=w_t}
\end{align}
where $\eta_w$ is the step-size in the weight space. I think the partial derivatives with respect to the weights become:
\begin{itemize}
\item For the first term in the numerator
\begin{align*}
\frac{\partial }{\partial w_i} \left[ \nabla f_w\left( x - \hat{t} \eta \nabla f_w(x) \right) - \nabla f_w(x)
\right] =\frac{\partial }{\partial w_i} \left[ \sum_{j=1}^{m} w_j \nabla f_j\left( x - \hat{t} \eta  \nabla f_w(x) \right) - \sum_{j=1}^{m} w_j \nabla f_j(x)
\right]
\end{align*}
Simplifying the right hand side we get,
\begin{align}
\text{rhs} =&\left[ \sum_{j=1}^{m} w_j \frac{\partial }{\partial w_i} \nabla f_j\left( x - \hat{t} \eta \nabla f_w(x) \right) + \nabla f_i\left(x - \hat{t}\eta \nabla f_w(x)\right) - \nabla f_i(x)
\right] \nonumber \\
=&-\hat{t}\eta \left[ \sum_{j=1}^{m} w_j \left( \nabla^2 f_j(x)\Big|_{x = x - \hat{t}\eta \nabla f_w(x)} \cdot \nabla f_i(x) \right) \right] + \nabla f_i\left(x - \hat{t} \eta \nabla f_w(x)\right) - \nabla f_i(x) \label{eq: partial grad num first term}
\end{align}
The first term is a Hessian-vector product which is supposedly fast (linear memory, I think). See \url{https://pytorch.org/functorch/nightly/notebooks/jacobians_hessians.html} and also \url{https://github.com/noahgolmant/pytorch-hessian-eigenthings}.
\item For the second term in the numerator, the partial derivatives is,
\begin{align}
\frac{\partial}{\partial w_i} \left( y(w) - x \right) = \eta \nabla f_i(x)
\end{align}
\item For the denominator, the partial derivative is
\begin{align}
\frac{\partial}{\partial w_i}\hat{t} \|y(w) - x\|_2 &= \frac{\hat{t}}{2\|y(w) - x\|_2} \frac{\partial}{\partial w_i} \|y(w) - x\|^2_2 = \frac{\hat{t}\eta \nabla f_i(x)^{\top} (y(w)-x) }{\|y(w)-x\|_2}
\end{align}
\end{itemize}

\paragraph{Optimizing for directional smoothness}
We can also look at the directional smoothness parameter,
\begin{align}
\label{eq: directional smoothness}
D(x,y(w)) = \frac{2 \|\nabla f_w(y(w)) - \nabla f_w(x) \|_2}{ \|y(w) - x\|_2}
\end{align}
and try to optimize it in terms of $w$,
\begin{align}
    w^* \in \argmax_{w \in \Delta_m} D(x,y(w))
\end{align}
There is no inner optimization problem here (over $t$) like in the case of the directional strong convexity parameter, and so we can simply take projected gradient steps, 
\begin{align}
w_{t+1} = \text{Proj}_{\Delta_m}\left(w_t + \eta_w \frac{\partial}{\partial w} D(x_t, y_t(w))\Big|_{w=w_t} \right),
\end{align}
where $\eta_w$ is the step-size in the weight space. Here $y_t(w)=x_t-\eta\sum_i w_i \nabla f_i(x_t)$, where $\eta$ is the step-size parameter in the iterate space. The partial derivatives for both the numerator and the denominator in Equation \eqref{eq: directional smoothness} can be written in closed form to compute $\partial/ \partial w D(x, y(w))$.
\begin{itemize}
\item Partial derivative for the numerator: 
\begin{align}
\frac{\partial}{\partial w_i} \|\nabla f_w(y(w)) - \nabla f_w(x)\|_2 &= \frac{1}{2\|\nabla f_w(y(w)) - \nabla f_w(x)\|_2} \frac{\partial}{\partial w_i} \| \nabla f_w(y(w)) - \nabla f_w(x) \|^2_2  \\
&= \frac{\left[ \frac{\partial}{\partial w_i} \left(\nabla f_w(y(w)) - \nabla f_w(x)\right) \right]^{\top} \Big[ \nabla f_w(y(w)) - \nabla f_w(x) \Big]}{\|\nabla f_w(y(w)) - \nabla f_w(x)\|_2}
\end{align}
where we can use \eqref{eq: partial grad num first term} to get the expression for $\frac{\partial}{\partial w_i} \left(\nabla f_w(y(w)) - \nabla f_w(x)\right)$.
% \begin{align}
% &= \frac{\eta \left[ \left(\nabla^2 f_i(x)\Big|_{x=x-\eta \nabla f_w(x)}
% + I \right)\nabla f_i(x)\right]^{\top}\Big[ \nabla f_w(y(w)) - \nabla f_w(x) \Big]}{\|\nabla f_w(y(w)) - \nabla f_w(x)\|_2}
% \end{align}
\item Partial derivative for the denominator:
\begin{align}
\frac{\partial}{\partial w_i} \|y(w) - x\|_2 &= \frac{1}{2\|y(w) - x\|_2} \frac{\partial}{\partial w_i} \|y(w) - x\|^2_2 = \frac{\eta \nabla f_i(x)^{\top} (y(w)-x) }{\|y(w)-x\|_2}
\end{align}
\end{itemize}
Simplifying, I think we get the following closed form expression,
\begin{align*}
    \frac{\partial}{\partial w_i} D(x,y(w)) &= \frac{1}{\|y(w)-x\|^2_2} \cdot (a + b)
\end{align*}
where
\begin{align}
a &= \frac{4}{D(x,y(w))} \left[ \frac{\partial}{\partial w_i} \left(\nabla f_w(y(w)) - \nabla f_w(x)\right) \right]^{\top}\Big[ \nabla f_w(y(w)) - \nabla f_w(x) \Big] \\
b &= D(x,y(w)) \cdot \nabla f_i(x)^{\top} (y(w)-x)
\end{align}

\appendix
\newpage 
\section{Sketches}
\subsection{Some attempts: Maximize the decrease by optimizing over $w$}

We analyze two alternatives in the following. Seems the second version gives us a larger lower bound since, by Jensen's inequality,
\begin{align*}
     \boxed{\frac{1}{2L} \sum_i w_i  \|\nabla f_i(x_t)\|^2_2 \geq \frac{1}{2L} \|\nabla f_w(x_t)\|^2_2}.
\end{align*}

\paragraph{A first order method: another attempt.}

Let us define
\[
f_w(x) = \sum_i w_i f_i(x), \mathrm{and}\ \nabla f_w(x) = \sum_i w_i \nabla f_i(x)
\]
and observe $f_w(x)$ is a strongly convex function with similar optimal solution $x_\star$.

We start with the standard expansion of the update rule.
\begin{equation}
    \label{eq: one step iterate gap attmpt 2}
    \|x_{t+1} - x^*\|^2_2 = \|x_t - x^*\|^2_2 - 2 \eta d^T (x_t - x^*) + \eta^2 \|d\|^2_2,
\end{equation}
where we think of $d\in \mathrm{Span}(\nabla f_1 f(x_t),..,\nabla f_M f(x_t))$, namely, $d = \sum_i w_i \nabla f_i(x_t)= \nabla f_w(x) $.  We now upper bound the second term in equation~\eqref{eq: one step iterate gap attmpt 2}:
\[
d^T (x_t - x^*) \geq \sum_i w_i (f_i(x_t) - f_i(x^*)) = f_w(x_t) - f_w(x^*) \geq \frac{1}{2L_w} \|\nabla f_w(x_t)\|^2_2,
\]


where we used the fact that $f_w(x)$ is $L_w$ smooth. We continue as usual:
\begin{align}
    \|x_{t+1} - x^*\|^2_2 &\leq \|x_t - x^*\|^2_2 - 2 \eta \frac{1}{2L_w} \|\nabla f_w(x_t)\|^2_2 + \eta^2 \|\nabla f_w(x_t)\ \|^2_2.
\end{align}
Maximizing over $\eta$ has the solution $\eta=\frac{1}{2L_2}$. Plugging it back leads to
\begin{align*}
     &\|x_{t+1} - x^*\|^2_2 \leq \|x_t - x^*\|^2_2 - \frac{1}{2L_w^2}\|\nabla f_w(x_t)\ \|^2_2\\
     & =  \|x_t - x^*\|^2_2 - \frac{1}{2L_w^2} \| \sum_i w_i \nabla f_i(x_t) \|_2\\
     &= \|x_t - x^*\|^2_2 - \frac{1}{2L_w^2} w^TJJ^T w.
\end{align*}
Assuming all functions as $L$ smooth, we'll get $L_w=L$. This will lead to the following optimization problem over $w$:
\begin{align*}
    w_t\in \arg\max_{w\in \Delta_M} w^TJJ^T w = \arg\max_{w\in \Delta_M} \| \sum_i w_i \nabla f_i(x_t) \|_2^2,
\end{align*}
since this will maximize the decrease. Observe that this is the opposite than the MGDA algorithm (maximization instead of minimization).

{\color{blue}Yon: if we'll repeat the exercise in which we assume the Jacobian is diagonal, I believe we'll get $1/M$ factor (since we optimize over $\Delta_1$) and, hence, get the trivial solution.}

\paragraph{A first order method (without a rate).}
Let $d\in \mathbb{R}^n$ be the update direction. That is, $x_{t+1} = x_t + \eta d$. Then, we can use standard analysis to think of a first-order approach.
\begin{equation}
    \label{eq: one step iterate gap}
    \|x_{t+1} - x^*\|^2_2 = \|x_t - x^*\|^2_2 - 2 \eta d^T (x_t - x^*) + \eta^2 \|d\|^2_2
\end{equation}
Assume that $d = \sum_i w_i \nabla f_i(x_t)$ for $w_i \geq 0$. We use a non-negative combination of the individual gradients because for any $w: w_i \geq 0$, $d$ (as defined above) is a descent direction for the iterate error, $\|x_t - x^*\|^2_2$. Note that by convexity,
\[
\nabla f_i(x_t)^T (x_t - x^*) \geq f_i(x_t) - f_i(x^*)
\]
which implies, 

{\color{blue}YON question: in these inequalities we fix a coordinate system by using the $f_i'$s. We could also say $d^T (x_t - x^*)\geq \beta_w \| \nabla f_w(x_t)\|$ right? namely, the sum is inside (since also $f_w$ is convex. what happens to the analysis?}
\[
d^T (x_t - x^*) \geq \sum_i w_i (f_i(x_t) - f_i(x^*)) \geq \sum_i w_i \frac{1}{2L_i} \|\nabla f_i(x_t)\|^2_2 = \sum_i w_i \beta_i \|\nabla f_i(x_t)\|^2_2.
\]
where the second inequality follows by Equation \eqref{eq: gradient norm lbds function value gap} above (Sham Kakade's lecture 9, lemma 2). The final equality takes $\beta_i = \frac{1}{2L_i}$. Plugging it in Equation \eqref{eq: one step iterate gap},
\begin{align}
    \|x_{t+1} - x^*\|^2_2 &\leq \|x_t - x^*\|^2_2 - 2 \eta \sum_i w_i \beta_i \|\nabla f_i(x_t)\|^2_2 + \eta^2 \|\sum_i w_i \nabla f_i(x_t) \|^2_2 \nonumber \\
    &\leq \|x_t - x^*\|^2_2 - 2 \eta \left[ \sum_i w_i \beta_i \|\nabla f_i(x_t)\|^2_2 - \frac{\eta}{2} \|\sum_i w_i \nabla f_i(x_t) \|^2_2 \right] \label{eq: progress lower bound}.
\end{align}
{\color{blue}Yon: why is there an inequality in the second line?}
The second term in the inequality above lower bound the progress we can make at every step. One way of designing a first-order method using this can be by solving the following optimization problem,

\begin{align}
    \label{eq: optimization objective}
    \max_{w: w_i \geq 0} \,\, \left[ \sum_i w_i \beta_i \|\nabla f_i(x_t)\|^2_2 - \frac{\eta}{2} \|\sum_i w_i \nabla f_i(x_t) \|^2_2 \right]
\end{align}

\begin{align}
    \label{eq: optimization objective}
    \max_{w: w_i \geq 0} \,\, \left[ w^T b - \frac{\eta}{2} w^T A w \right],
\end{align}
and its solutions is:
\begin{align*}
    w= \frac{1}{\eta} A^{\dagger} b,
\end{align*}
where 
\begin{align*}
    A = J^T J, b =  \mathrm{Diag}(J^TJ)\beta.
\end{align*}
The optimal value is then equal to (lets assume $J$ is inveritble)
\begin{align}
    A\rightarrow UAU^T
\end{align}
\begin{align*}
    \mathrm{Diag}(A) \neq \mathrm{Diag}(UAU^T)
\end{align*}
\begin{align*}
    \boxed{\frac{1}{2\eta}b A^\dagger b=   \frac{1}{2\eta}\beta^T \mathrm{Diag}(J^TJ) (J^T J)^{-1} \mathrm{Diag}(J^TJ) \beta}.
\end{align*}
The key question is how to lower bound this term in a meaningful way? 

\emph{Exercise: check what happens in the case the gradients are orthogonal. }

\paragraph{Attempt 1.}
Plugging this back we get that the optimal value of ~\eqref{eq: optimization objective} is
\begin{align*}
    \frac{1}{2\eta}b A^\dagger b.
\end{align*}
which also satisfies 
\begin{align}
    \frac{1}{2\eta}b A^\dagger b \geq \frac{1}{2\eta}\| b \|_2^2 \lambda_{\mathrm{min}}(A^\dagger)= \frac{1}{2\eta \lambda_{\mathrm{max}}(A)}\| b \|_2^2 = \frac{1}{2\eta \lambda_{\mathrm{max}}(A)} \sum_i \beta_i^2 \|\nabla f_i(x_t)\|^4_2
\end{align}

\paragraph{Attempt 2.} Notice that by \href{https://mathoverflow.net/questions/324874/inverse-of-a-matrix-and-the-inverse-of-its-diagonals}{this exercise} (lets assume $A$ is invertible)
\begin{align*}
\frac{1}{2\eta}b A^{-1} b \geq \frac{1}{2\eta M}b (\mathrm{Diag}(A))^{-1} b   = \frac{1}{2\eta M}b (\mathrm{Diag}(J^T J))^{-1} b,
\end{align*}
where $M$ is the number of objectives, and the dimension of $A$. Plugging the value of $b$ we get an interesting lower bound:
\begin{align*}
    \frac{1}{2\eta}b A^{-1} b \geq \frac{1}{2\eta M} \beta^T\mathrm{Diag}(J^TJ)\beta =  \frac{1}{2\eta M} \sum_i \beta_i^2 \| \nabla f_i(x_t) \|_2^2.
\end{align*}
Think that will result in a trivial bound in which we have the average $\mu_i$s. If that's true, it shows this algorithm, will be at least as good as the naive analysis of the naive algorithm.



\paragraph{Simplifying the solution: orthogonal gradients.} Expanding this, the optimization objective becomes
\begin{align*}
    \sum_i w_i \beta_i \|\nabla f_i(x_t)\|^2_2 - \frac{\eta}{2} \left( \sum_i w_i^2 \|\nabla f_i(x_t) \|^2_2 + 2 \sum_{i \neq j} w_i w_j \nabla f_i(x_t)^T \nabla f_j(x_t) \right)
\end{align*}
Note that when $\nabla f_i(x_t)^T \nabla f_j(x_t) < 0$, then choosing the weights adaptively may reduce the second order noise term. When the gradients are orthogonal, that is, $\nabla f_i(x_t)^T \nabla f_j(x_t) = 0$, then the optimization objective reduces to
\begin{align*}
    \sum_i w_i \beta_i \|\nabla f_i(x_t)\|^2_2 - \frac{\eta}{2} \sum_i w_i^2 \|\nabla f_i(x_t) \|^2_2 
\end{align*}
Solving, we get $w_i^* = \beta_i/\eta$ and the overall progress can be bounded by $\frac{1}{2\eta} \sum_i \beta_i^2 \|\nabla f_i(x_t)\|^2_2$. Plugging in Equation \eqref{eq: progress lower bound} shows
\begin{align*}
    &\|x_{t+1} - x^*\|^2_2 \leq \|x_t - x^*\|^2_2 + \sum_{i} \beta_i \left[-\|\nabla f_i(x_t)\|^2_2 \right]\\
    &\leq \|x_t - x^*\|^2_2 + \sum_{i} \beta_i \left[-\mu_i(f_i(x_t) - f_i(x_\star))\right]\\
    &=\|x_t - x^*\|^2_2 + \sum_{i} \beta_i \mu_i\left[-(f_i(x_t) - f_i(x_\star))\right]
\end{align*}

By strong convexity, we have that {\color{blue} is this true?:}
\begin{align*}
    &f(x_*) - f(x_t) \leq \nabla f(x_\star)(x_*- x_t) - \frac{\mu}{2} || x_t- x_*||_2^2 = - \frac{\mu}{2} || x_t- x_*||_2^2 \\
    &\rightarrow -( f(x_t) -f(x_*))\leq  - \frac{\mu}{2} || x_t- x_*||_2^2
\end{align*}
If we set this again we get
\begin{align*}
    &\|x_t - x^*\|^2_2 + \sum_{i} \beta^2_i \mu_i\left[-(f_i(x_t) - f_i(x_\star))\right]\\
    &\leq \|x_t - x^*\|^2_2 - \sum_{i} \beta_i^2 \mu_i^2\|x_t - x^*\|^2_2\\
    &= (1-\sum_i \frac{\mu^2_i}{L^2_i} )\|x_t - x^*\|^2_2
\end{align*}

\begin{align*}
    \| x_t -x_\star \|_2^2 \leq \frac{1}{\mu_\star} (f_{w_\star}(x_t) - f_{w_\star}(x_\star)) \leq \frac{1}{\mu_\star} \max_{i} \left(f_i(x_t) - f_i(x_\star) \right) \text{ (if $w\in \Delta_M$) }
\end{align*}


\paragraph{A few general thoughts.} 
\begin{itemize}
    \item It seems to me that the intuition behind this line of proof (with iterate convergence) is as follows. Note that by convexity, 
\[
\nabla f_i(x)^\top (x - x^*) \geq f_i(x) - f_i(x^*) \,\, \text{for all} \, i = 1,2,\ldots,m.
\]
Hence, for any $w \in \Delta^{m-1}$, $\nabla f_w(x)$ is a descent direction for $\|x - x^*\|^2$ as $\nabla f_w(x)^\top (x - x^*) > 0$. Strong convexity lower bounds the progress.
\end{itemize}
 \DJcomment{I agree with this intuition}

\section{The Stochastic Setting}
Some good references for the proof techniques in the stochastic setting are 
\begin{itemize}
    \item \citet{nemirovski2009robust}
    \item \citet{rakhlin2011making}
    \item \citet{bertsekas2011incremental}
    \item \citet{wang2017stochastic}
\end{itemize}
All three paper prove bounds on the expected suboptimality or distance to optimal, and \cite{rakhlin2011making}, notably, gives a result of the high probability form. Converting between the suboptimality versus distance to optimal types of bounds is straightforward using smoothness (Lipschitz gradients) just like in the deterministic case.

\subsection{Illustration on Toy Algorithm}
Using the same notation as above, we have $f_m$ for $m \in [M]$ and they share an optimizer. We can query unbiased stochastic gradients $g_m(x)$, such that $\mathbb{E}[g_m(x)] = \nabla f_m(x)$.
\begin{algorithm}
	\caption{Randomly-selected-objective MO in SGD setting} 
	\begin{algorithmic}[1]
		\For {$t=1,2,\ldots$}
			\State Sample $m_t$ uniformly from $[M]$
                \State Observe stochastic gradient $g_{m_t}(x_t)$ 
                \State Projected gradient update: $x_{t+1} = \Pi_{\mathcal X} \left[ x_t - \eta_t g_{m_t}(x_t) \right]$		
  \EndFor
	\end{algorithmic} 
\end{algorithm}
\citet{nemirovski2009robust} and \citet{rakhlin2011making} essentially use the same technique as the deterministic setting.

\begin{theorem}
    Assume each $f_m$ is (1) $\mu_m$-strongly convex, (2) that we are optimizing over a convex domain $\mathcal X$, (3) the optimizers of all functions are the same, (4) $\mathbb{E}[\|g_m\|^2] \le G^2$, and (5) $\eta_t = 1/(\bar{\mu}t)$ where $\bar{\mu}=M^{-1}\sum_m \mu_m$. Then,
    \begin{align*}
        \mathbb E\bigl[ \| x_{t+1} - x^* \|^2 \bigr] \le 4G^2/(\bar{\mu}^2 t)
    \end{align*}
\end{theorem}

\begin{proof}
Note that:
\[
\mathbb E\bigl[ \|g_{m_t}\|^2 \bigr] = 
\mathbb E \bigl[ \mathbb E\bigl[ \|g_{m_t}\|^2 \, |\, m_t \bigr] \bigr] \le G^2.
\]
Also, note that:
\[
\mathbb E\bigl[ \langle g_{m_t}, (x_{t} - x^*)\rangle \bigr] = \mathbb E\bigl[ \mathbb E\bigl[ \langle g_{m_t}, (x_{t} - x^*)\rangle \, | \, m_t\bigr]\bigr] = M^{-1} \sum_{m} \mathbb E\bigl[ \langle g_{m}, (x_{t} - x^*)\rangle \bigr].
\]
Following the standard analysis of \citet{nemirovski2009robust, rakhlin2011making}, we have:
    \begin{align*}
    \mathbb E\bigl[ \| x_{t+1} - x^* \|^2 \bigr] 
    &= \mathbb E\bigl[ \| \Pi_{\mathcal X} [x_{t} - \eta_t g_{m_t}] - x^* \|^2 \bigr] \\  
    &\le \mathbb E\bigl[ \| x_{t} - \eta_t g_{m_t} - x^* \|^2 \bigr]   \\
    &\le \mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr] - 
 2 \eta_t \mathbb E\bigl[ \langle g_{m_t}, (x_{t} - x^*)\rangle \bigr] + \eta_t^2 \mathbb E\bigl[ \|g_{m_t}\|^2 \bigr] \\
    &\le \mathbb E\bigl[ \| x_{t} - x^* \|^2\bigr] - 
 2 \eta_t M^{-1} \sum_m \mathbb E\left[ f_m(x_t) - f_m(x^*) + \frac{\mu_m}{2} \| x_{t} - x^* \|^2 \right] + \eta_t^2 G^2 \\
     &\le \mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr]- 
 2 \eta_t M^{-1} \sum_m \mathbb E\left[ \frac{\mu_m}{2} \| x_{t} - x^* \|^2 + \frac{\mu_m}{2} \| x_{t} - x^* \|^2 \right] + \eta_t^2 G^2 \\
 &= \left( 1 - 2\eta_t \bar{\mu} \right)\mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr] + \eta_t^2 G^2 \\
 &= \left( 1 - \frac{2}{t} \right)\mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr] + \frac{G^2}{\bar{\mu}^2 t^2}
    \end{align*}
Using \citet[Lemma 2]{rakhlin2011making} and induction, we get $\mathbb E\bigl[ \| x_{t+1} - x^* \|^2 \bigr] \le 4G^2/(\bar{\mu}^2 t)$. Alternatively, we can also apply \citet[Lemma 1]{chung1954stochastic}, which is a fancy way of analyzing the recursive relationship above.
\end{proof}

The reason that this is result is not as strong as the deterministic case is that we did not use smoothness to get rid of the last squared gradient term. 

\subsection{High Probability Extension}
Just putting this here as a reminder for the future. See \citet{rakhlin2011making} for how they do this.

\subsection{Stochastic Compositional Gradient Descent}
I thought \citet{wang2017stochastic} might be helpful as well. It is a complicated version of this proof technique for the case where there is nested learning. Perhaps when we try to be adaptive, some of those ideas can be useful.

\bibliography{bibliography}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Classic gradient analysis for convex functions.}

\paragraph{Example 1.} Here is a classic analysis of gradient descent, see for example \cite{lacoste2012simpler, bubeck2015convex}. Consider the gradient update for any of components, $x_{t+1} = x_t - \eta_i \nabla f_i(x_t)$. Then,
\begin{align}
    \|x^* - x_{t+1}\|^2_2 &= \|x^* - (x_t - \eta \nabla f_i(x_t)) \|^2_2 \nonumber \\
    &= \|x^* - x_{t}\|^2_2 + 2 \eta (x^*-x_t)^\top\nabla f_i(x_t) +  \eta^2\|\nabla f_i(x_t)) \|^2_2 \label{eq:grad_update}
\end{align}
Assume that gradient norm is uniformly bounded as $\|\nabla f_i(x)\|^2_2 \leq B \,\, \forall \, x \in \mathbb{R}^n$. Also, using the fact that $f_i$ is convex, we have
\[
\nabla f_i(x_t)^\top(x^* - x_t) \leq f_i(x^*) - f_i(x_t).
\]
Using this along with \eqref{eq:grad_update}, we have
\begin{align*}
    \|x^* - x_{t+1}\|^2_2 &= \|x^* - (x_t - \eta \nabla f_i(x_t)) \|^2_2 \\
    &\leq \|x^* - x_{t}\|^2_2 + 2 \eta \left( f(x^*) - f(x_t) \right) +  \eta^2\|\nabla f_i(x_t)) \|^2_2 \\
    &\leq \|x^* - x_{t}\|^2_2 + 2 \eta \left( f(x^*) - f(x_t) \right) +  \eta^2 B^2.
\end{align*}
Taking $\eta=1/2$ implies,
\begin{equation*}
    f(x_t) - f(x^*) \leq \|x^* - x_{t}\|^2_2 - \|x^* - x_{t+1}\|^2_2 + \frac{ B^2}{4}.
\end{equation*}

% {\color{red}{YE}: isnt there $\eta$ factor missing on the RHS? shouldnt we divide by $2\eta$?}

% {\color{blue}{JB}: Corrected. To get the final inequality, take $\eta=1/2$.}

Summing over time steps and applying Jensen's gives,
\begin{align}
    f(\bar{x}_T) - f(x^*) \leq \frac{1}{T}\sum_{t=0}^{T-1} f(x_t) - f(x^*) \leq \frac{\|x^* - x_0\|^2_2}{T} + \frac{B^2}{4},
\end{align}
where $\bar{x}_T = \frac{1}{T} \sum_{t=0}^{T-1} x_t$. 

\paragraph{Example 2} 
An alternate analysis is shown  \href{https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf}{here}. As $\nabla f_i$ is Lipschitz continuous with constant $l_i$, $\nabla^2 f(x) \preceq l_i I$. One can make a quadratic approximation as follows.
\begin{align}
    \label{eq:quadratic upper bound}
    f_i(y) &\leq f_i(x) + \nabla f_i(x)^\top(y-x) + \frac{1}{2} \left(y-x\right)^\top\nabla^2f_i(x) \left(y-x\right)^2_2 \nonumber \\
    &\leq f_i(x) + \nabla f_i(x)^\top(y-x) + \frac{1}{2}l_i\|y-x\|^2_2.
\end{align}
Taking $y = x_{t+1} = x_t - \eta \nabla f_i(x_t)$, we get
\begin{align}
    f_i(x_{t+1}) &\leq f_i(x_t) + \nabla f_i(x_t)^\top(x_{t+1}-x_t) + \frac{1}{2}l_i\|y-x\|^2_2 \nonumber \\
    &\leq f_i(x_t) - \eta \| \nabla f_i(x_t) \|^2_2 + \frac{\eta^2}{2}l_i\| \nabla f_i(x_t) \|^2_2 \nonumber \\
    &\leq f_i(x_t) - \left(1 - \frac{\eta l_i}{2}\right) \eta \| \nabla f_i(x_t) \|^2_2 \label{eq:descent with step size}
\end{align}
Taking $\eta \leq 1/l_i$ implies
\begin{equation}
    \label{eq:per step descent}
    f_i(x_{t+1}) \leq f_i(x_t) - \frac{1}{2} \eta \| \nabla f_i(x_t) \|^2_2.
\end{equation}
Basically, taking a gradient step improves the objective value. This can be converted to a convergence rate. By convexity, 
\begin{align*}
    f_i(x^*) &\geq f_i(x_t) + \nabla f_i(x_t)^\top(x^*-x) \\
    \Rightarrow \,\, f_i(x_t)  &\leq f_i(x^*) + \nabla f_i(x_t)^\top(x-x^*)
\end{align*}
Plugging this in \eqref{eq:per step descent} implies,
\begin{align*}
    f_i(x_{t+1}) &\leq f_i(x^*) + \nabla f_i(x_t)^\top(x_t-x^*) - \frac{1}{2} \eta \| \nabla f_i(x_t) \|^2_2 \\
    f_i(x_{t+1}) - f_i(x^*) &\leq  \frac{1}{2\eta} \left( \nabla f_i(x_t)^\top(x_t-x^*) - \eta^2 \| \nabla f_i(x_t) \|^2_2 \right) \\
    f_i(x_{t+1}) - f_i(x^*) &\leq  \frac{1}{2\eta} \left( \nabla f_i(x_t)^\top(x_t-x^*) - \eta^2 \| \nabla f_i(x_t) \|^2_2 - \|x_t - x^*\|^2_2 + \|x_t - x^*\|^2_2 \right) \\
    f_i(x_{t+1}) - f_i(x^*) &\leq  \frac{1}{2\eta} \left( \|x_t - x^*\|^2_2 - \|x_t - \eta \nabla f_i(x_t) - x^*\|^2_2 \right).
\end{align*}
Using the gradient update, $x_{t+t} = x_t -\eta \nabla f_i(x_t)$, we get
\begin{align}
    f_i(x_T) - f_i(x^*) \leq \sum_{t=1}^{T} \left( f_i(x_t) - f(x^*) \right) &\leq \frac{1}{2\eta} \sum_{t=1}^T \left( \|x_{t-1} - x^*\|^2_2 - \|x_t - x^*\|^2_2 \right) \nonumber \\
    &\leq \frac{1}{2\eta} \|x_0 - x^*\|^2_2.
\end{align}
The first inequality follows \eqref{eq:per step descent} showing each update makes progress monotonically.


\end{document}
