\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2023}{1-48}{4/00}{10/00}{Zheqing Zhu et al.}

% Short headings should be running head and authors last names

\ShortHeadings{Pearl: \textbf{P}roduction-R\textbf{ea}dy \textbf{R}einforcement \textbf{L}earning Agent}{Zheqing Zhu et al.}
\firstpageno{1}

\begin{document}

\title{Pearl: \textbf{P}roduction-R\textbf{ea}dy \textbf{R}einforcement \textbf{L}earning Agent}

% Bill to make some final adjustments to author list by contribution
\author{\name Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Frank Cheng, Urun Dogan, Zheng Wu, Wanqiao Xu\\ \email Correspondence to billzhu@meta.com\\
\addr Applied Reinforcement Learning Team\\
Meta AI \\
Menlo Park, CA 94025, USA}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file

\end{abstract}

\begin{keywords}
Reinforcement Learning
\end{keywords}

\section{Introduction}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Acrobot.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Pendulum.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/CartPole.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/MountainCar.png}
    \end{subfigure}
    \caption{Benchmark results in four classic control environments using DQN and DDPQ. The tested agent and environment are the same as the one used by Rodrigo's \texttt{benchmark.py}. All current results only involve a single run. I will do multiple runs for selective environments. And if we need, we may add benchmark results generated by other algos.}
    \label{fig:classic control results}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Partial Observable Acrobot.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Partial Observable Pendulum.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Partial Observable CartPole.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Partial Observable MountainCar.png}
    \end{subfigure}
    \caption{Benchmark results in the partial observable version of four classic control environments using DQN and DDPQ. At time steps $t = 0, 3, 6$, the agent observes a vector that has the position/angles of the system and a flag set to be True. For the rest of time steps, and the flag is set to be False. And the observed vectors contain all zeros. The velocity or angular velocity is never observed by the agent. So the agent has to infer from its current position and velocity from previous observations.}
    \label{fig:classic control partial observable results}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Sparse Reward Acrobot.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Sparse Reward Pendulum.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Sparse Reward MountainCar.png}
    \end{subfigure}
    \caption{Benchmark results in the sparse reward version of three classic control environments using DQN and DDPQ. Each game receives a reward of one when a special region of the state space is reached. }
    \label{fig:classic control sparse reward results}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/State Safety Acrobot.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Action Safety Acrobot.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/State Safety Pendulum.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Action Safety Pendulum.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/State Safety MountainCar.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Action Safety MountainCar.png}
    \end{subfigure}
    \caption{Benchmark results in the safety version of three classic control environments using DQN and DDPQ. Each game receives a reward of one when a special region of the state space is reached. }
    \label{fig:classic control safety results}
\end{figure}
 
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Long Horizon Acrobot.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Long Horizon Pendulum.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Long Horizon CartPole.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Long Horizon MountainCar.png}
    \end{subfigure}
    \caption{Benchmark results in the long horizon version of four classic control environments using DQN and DDPQ. The episode lengths are $5$ times of their original lengths. For Acrobot it is $2500$ steps. For the other three, they are $1000$ steps. For all environment, a $5x$ finer decision granularity was used. For Long Horizon Acrobot, for every 0.004 seconds, an new force is chosen and applied until the next decision point (this number is 0.02 in Acrobot). Similarly, the Long Horizon Pendulum makes a decision every $0.001$ seconds. The acrobot and mountain car}
    \label{fig:classic control results}
\end{figure}

\acks{}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}

\vskip 0.2in
\bibliography{sample}

\end{document}