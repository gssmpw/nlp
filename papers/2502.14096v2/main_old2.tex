\documentclass{article} % For LaTeX2e
\input{arxiv_version}

% \usepackage{iclr2024_conference}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{array} % for "\centering\arraybackslash" macro
\usepackage{geometry}
\geometry{bottom=1.0in, top=1.0in, left=1.2in, right=1.3in}
% Definitions of handy macros can go here



\usepackage{amsmath}
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}

% \usepackage[table]{xcolor}
% \usepackage{caption, subcaption}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{enumitem}
% \newcommand{\Ec}{\mathcal{E}}
% \newcommand{\Mc}{\mathcal{M}}
% \newcommand{\Sc}{\mathcal{S}}
% \newcommand{\Ac}{\mathcal{A}}
% \newcommand{\R}{\mathcal{R}}
% \newcommand{\Pc}{\mathcal{P}}
% \newcommand{\E}{\mathbb{E}}
% \newcommand{\rb}{\textbf{r}}

% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% New def. added by Ben %
\def\x{{\mathbf{x}}}
\def\d{{\mathbf{d}}}
\def\y{{\mathbf{y}}}
\def\z{{\mathbf{z}}}
\def\g{{\mathbf{g}}}
\def\w{{\mathbf{w}}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\enorm}[1]{\left\Vert#1\right\Vert}

%%%%%%%%%%%%%%%%%

\def\jb#1{{\color{red}#1}}
\def\YE#1{{\color{green} YE: #1}}

%To remove comments add flag as \usepackage[suppress]{color-edits}
\usepackage{color-edits}
\addauthor{YE}{red}
\addauthor{DJ}{green}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{blindtext}
\usepackage{hyperref}       % hyperlinks
\hypersetup{colorlinks,linkcolor=blue,
            citecolor=blue,
            urlcolor=magenta,
            linktocpage,
            plainpages=false}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bbm}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{chngpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{algorithm} 
\usepackage{algpseudocode}
\usepackage{natbib} 
\usepackage{thmtools,thm-restate}
\usepackage{wrapfig}
\usepackage{authblk}
\usepackage{tablefootnote}

\usepackage[english]{babel}
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}[theorem]{Lemma}


\renewenvironment{abstract}
  {{\centering\large\bfseries Abstract\par}\vspace{0.7ex}%
    \bgroup
       \leftskip 20pt\rightskip 20pt\small\noindent\ignorespaces}%
  {\par\egroup\vskip 1.25ex}

\newenvironment{keywords}
{\bgroup\leftskip 20pt\rightskip 20pt \small\noindent{\bfseries
Keywords:} \ignorespaces}%
{\par\egroup\vskip 0.25ex}


\title{Aligned Multi-Objective Optimization (AMOO)}

\author{k}
\renewcommand\Authands{ and}
\affil[]{} 


\begin{document}
\maketitle

% \begin{abstract}
% \end{abstract}


% \begin{keywords}

% \end{keywords}

\renewcommand{\thefootnote}{\arabic{footnote}}

\section{Introduction}
Consider the following unconstrained multi-objective optimization problem:
\[
\min_{\x \in \reals^n} \,\, F(\x),
\]
where $F: \reals^{n} \to \mathbb{R}^m$ is a vector valued function, $F(x) = \brac{f_1(x), f_2(x), \ldots, f_m(x) }$. In the Aligned Multi-Objective Optimization (AMOO) setting we make an additional assumption that the different functions are aligned, namely, that they share an optimal solution. Namely, we assume that there exists a solution $x^*$ that optimizes the function $F(\cdot)$, such that 
\[
\nabla f_i(x^*) = 0 \quad \forall i \in \{1,2,\ldots,m\}.
\]
We are interested in gradient methods of the form,
\[
\x_{t+1} = \x_t + \eta_t \d_t,
\]
where $\d_t$ denotes a descent direction and $\eta_t$ is the step size in the time step $t$. 

\subsection{Definitions}



\begin{definition}
    We say that $f: \reals^n \to \reals$ is $L$-Lipschitz if $~\forall \x, \y \in \mX$:
    \begin{align*}
       |f(\x) - f(\y)| \leq L \enorm{ \x - \y }.
    \end{align*}
\end{definition}



\begin{definition} \label{def:smooth}
    We say that $f: \reals^n \xrightarrow{} \reals$ is $\beta$-smooth if $~\forall \x, \y \in \mX$:
    \begin{align}
        f(\y) \leq  f(\x) + \nabla f(\x)^{\top} (\y - \x) + \frac{\beta}{2}  \enorm{ \x - \y } ^2. \nonumber
    \end{align}
\end{definition}




\begin{definition}
\label{def:sc}   
    We say that $f: \reals^n \xrightarrow{} \reals$  is $\mu$-strongly convex if $~\forall \x, \y \in \mX$:
    \begin{align}
        f(\y) \geq  f(\x) + \nabla f(\x)^{\top} (\y - \x) + \frac{\mu}{2}  \enorm{ \x - \y } ^2. \nonumber
    \end{align}
\end{definition}


\begin{observation}\label{obs:sum_smooth}
    Let $\w \in \Delta_m$. Consider $\{ f_i(\cdot) \}_{i=1}^m$ are  $ \beta $ smooth ($L$- Lipschitz). Then, $f_{\w}(x) \equiv \sum_{i=1}^{m} w_i f_i(x)$ is also $\beta$ smooth ($L$- Lipschitz).
\end{observation} 



\subsection{Recap: Gradient Descent in the Strongly Convex Setting}

% Recall that a function $f$ is $\mu$ stronglly convex if for all $x,x'\in \mathcal{X}$
% \begin{align*}
%     f(x') \geq f(x) + \nabla f(x)(x'-x) + \frac{\mu}{2} ||x-x'||.
% \end{align*}


Consider $f(\cdot)$ a $\mu$-strongly convex and a $\beta$ smooth function function $f$. We know that running GD on $f$ will result in guarantee of the form (where $\kappa = \beta/\mu$):
\begin{align}
    f_i(x_t) - f_i(x^*) \leq \exp(-t/\kappa) (f_i(x_o) - f(x_*)) \label{eq:strongly convex convergence}
\end{align}
see \citet{bach2021learning}, proposition 5.3.



\section{Benefits of AMOO}

The question we focus on in this work is the following one:
\begin{center}
    \emph{Can we get benefits in the form of faster or more reliable convergence in the AMOO setting?}
\end{center}
We get an affirmative answer for the strongly convex setting and design the AMOO optimizer, the first algorithm for the AMOO setting with provable improved guarantees.

Prior works~\cite{sener2018multi,yu2020gradient,liu2021conflict, navon2022multi} also studied convergence of algorithms in the multi-task or multi-objective setting: where there are few tasks and we wish to design a GD algorithms that solves the tasks. However, they focused on convergence to a point on the \emph{Pareto frontier}, namely, they considered a scenario in which there's a conflict between the different objectives.

In the AMOO setting we assume the objectives are aligned and ask when we can benefit from this assumption and how to design improved algorithms under this assumption. Indeed, in many machine learning problems there is no fundamental conflict between the different tasks (e.g., segmentation and depth indentification tasks in computer vision, or in multi reward setting in RLHF when different reward functions represent different tasks). For this reason, we believe that designing improved algorithms for the AMOO setting can create real impact in future applications.

\subsection{The Canonical Example}

To develop intuition for the algorithm and approach consider the following toy example which we refer to as the `canonical example':
\begin{align*}
    &f_1(x) = (1-\Delta) x_1^2 + \Delta x_2^2\\
    &f_2(x) = (1-\Delta) x_2^2 + \Delta x_1^2,
\end{align*}
and let $F(x) = (f_1(x), f_2(x))$. It is clear that $F(x)$ can be simultaneously minimized in $x^\star=\left( 0,\ 0 \right)$, hence, this is an AMOO setting.

Observe that both $f_1$ and $f_2$ are $\Delta$-strongly convex functions and $1$ lipchitz smooth functions. Hence, equation~\eqref{eq:strongly convex convergence} implies running GD on each separate objective will converge in rate of $(\Delta)$. 

It is quite simple to observe this rate can be improved dramatically in the AMOO setting. Indeed, let $f_{1/2}$ be a function with equal weighting of both $f_1$ and $f_2$:
\begin{align*}
    f_{1/2}(x)= 1/2\cdot f_1(x) +1/2\cdot f_2(x) = 0.5\cdot x_1^2 + 0.5\cdot x_2^2,
\end{align*}
which is $\Omega(1)$-strongly convex and $O(1)$-lipchitz smooth. Hence, GD applied to $f_{1/2}(x)$ converges with rate of $O(1)$ which is much faster than $O(\Delta)$ (since $\Delta$ can be arbitrarily small). 

\paragraph{Findings from the Canonical Example}
How can the canonical example can be understood in a more general sense? What would be a general way to set the weights in the AMOO setting? Recall that the strong convexity and the lipchitz smooth $(\mu, L)$ parameters can be understood as the minimal and maximal eigenvalue of the hessian matrix, namely
\begin{align*}
    \mu = \min_{x}\lambda_{\mathrm{min}}(\nabla^2 f(x)), & ~~ L = \max_{x}\lambda_{\mathrm{max}}(\nabla^2 f(x)).
\end{align*}

With this in mind, we can design a way to set to weighting scheme so that the functions to be optimized over has the `best' curvature.  The most general way this can be done is via directly choosing the weights to optimize the condition number of the function. Let 
\begin{align*}
    f_\w(x) := \sum_i w_i f_i(x).
\end{align*}
Choosing the weight scheme with the optimal curvature can be understood by directly solving optimizing the condition number locally:
\begin{align*}
    \w_\star(x) \in \arg\min_{\w \in \Delta_m} \frac{\lambda_{\mathrm{max} }\left(\sum_i w_i \nabla^2 f_i(x) \right)}{\lambda_{\mathrm{min}}\left(\sum_i w_i  \nabla^2 f_i(x)\right)}.
\end{align*}
Additionally, we can get benefits by optimizing either the strong convexity parameter or the lipchitz smoothness parameter, in case we would like to reduce the computational burden. 

Even though this optimization problem is convex, it cannot be efficiently solved when we used DNN with large number of parameters. In next sections we will develop efficient ways -- that scales linearly with the number of parameters -- to solve this optimization problem assuming the Hessian is (approximatley) Diagonal.


\subsection{The Specification Example}

Consider the case that each function, $f_i(\x): i\in [m]$ is convex (not strongly convex) and smooth, and the average of them, $m^{-1} \sum_{i=1}^m f_i(\x)$, is $\mu$ strongly convex. Let $\x \in \reals^d$. For example:
\begin{align*}
    & \forall i \in [m-1] : ~~ f_i(\x) = \frac{1}{2} \sum_{j=1}^i x_j^2 ~ , \quad \text{and} \quad  f_m(\x) = \frac{1}{2} \sum_{j=1}^d x_j^2,
\end{align*}
The common minimizer is the point $\bold{0} \in \reals^d$. Note that the average function is $\frac{1}{m}$-strongly convex, while $f_m$ is $1$-strongly convex.\\
The motivation is to increase the condition number. Since all the functions are $1$-smooth, the condition number increases from $\frac{1}{m}$ to $1$.


\subsection{The Most Curvature Example}

Consider the case of two convex and smooth functions with the same minimizer $(0,0)$, and the average of them, $\frac{1}{2} \sum_{i=1}^2 f_i(\x)$, is $2$-strongly convex. Let $\x \in \reals^2$. For example:
\begin{align*}
    f_1(\x) = x_1^2 + \frac{1}{6} x_2^4 ~ , ~ f_2(\x) = \frac{1}{6} x_1^4 + x_2^2.
\end{align*}
For example, let's take the point $(10,2)$. The Hessian of $\frac{1}{2} \sum_{i=1}^{2} w_i f_i(\x)$, such that $\sum_{i=1}^{2} w_i = 1$, at this point is
\begin{align*}
    2 * \begin{bmatrix}
        w_1 + 100w_2 & 0\\
        0 & 4w_1 + w_2 
    \end{bmatrix},
\end{align*}
The weights that maximized the strongly convex parameter is $\w = \brac{\frac{33}{34}, \frac{1}{34}}$, which also maximized the condition number, $1$. Notice that, the average, i.e. $\w=\brac{\frac{1}{2},\frac{1}{2}}$, has a worse strongly convex parameter, and condition number, $$.
% which means that the condition number of it is greater than $c^2$. However, the optimal condition number is $3$, and is achivable by  $\w = \brac{\frac{c^2}{1+c^2}, \frac{1}{1+c^2}}$, i.e. $\frac{c^2}{1+c^2} f_1(\x) + \frac{1}{1+c^2} f_2(\x)$. 


% The motivation is to increase the condition number. Since all the functions are $1$-smooth, the condition number increases from $\frac{1}{m}$ to $1$.

\section{Improved Convergence with AMOO Optimzer}

For simplicity, we will focus on optimizing the strongly convex parameter in the AMOO setting, instead of the condition number. We start by introducing a new definition which captures the optimal strong convex parameter over a weighted function class. The  optimal strong convexity parameter over the weight class as follows:

\begin{definition}[AMOO Strong Convexity $\mu_\star$]\label{def:AMOO-sc} The optimal strong convexity parameter $\mu_\star \in \reals_{>0}$ is the largest scalar such that exists $\w \in \Delta_m$ (the simplex over $m$ tasks) that for every $\x,\y \in \mX$ it holds that: 
\begin{align*}
    f_\w(\y) \geq f_\w(\x) + \nabla f_\w(\x)(\y-\x) + \frac{\mu_\star}{2} \enorm{\x-\y}.
\end{align*}
\end{definition}

In words, $f_\w$ is a set of functions, parameterized by $\w \in \Delta_m$, each one is $\mu_\w$ strongly convex, and $\mu_\star=\max_{\w\in \Delta_m}\mu_\w$ is the largest one. 

In the canonical problem presented above, $\mu_\star=O(1)$ whereas the strongly convex parameter of each individual function can be arbitrarily small. The next result shows that Algorithm~\ref{alg:AMOO} converges in rate that corresponds to $\mu_\star.$

\begin{algorithm}
	\caption{GD with AMOO-Optimizer}        \label{alg:AMOO} 
    \begin{algorithmic}[1]
	\For {$t=1,2,\ldots$}
		\State Get Hessian $\nabla^2 f_i(\x_t)$ for all $i\in [m]$
		\State (P1) Calculate $\w_t\in \arg\max_{\w\in \Delta_{m}} \lambda_{\min} (\sum_{i} w_i \nabla^2 f_i(\x_{t}))$ 
            \State Get gradients of $f_i(\x_t)$ and calculate weighted gradient $\g_t = \sum_{i} w_i \nabla f_i(\x_t)$
            \State Gradient update: $\x_{t+1} = \x_t - \eta_t \g_t$		
  \EndFor
    \end{algorithmic} 
\end{algorithm}

\begin{restatable}[$\mu_\star$ Convergence of AMOO-Optimizer]{theorem}{ExactAmooConvergence}
\label{thm:exact_amoo_convergence}
    Suppose that all the functions $f_i$ are $\beta$ smooth, and have the same minimizer $\x_\star$. If $\eta_t = \eta = 1/\beta$ then A-MOO with Hessians converges with the following rate:
    \begin{align*}
        \enorm{\x_t-\x_\star} \leq \prod_{t=1} \brac{1-\frac{\mu_{t}^\star}{\beta}} || \x_0 - \x_\star ||
    \end{align*}
\end{restatable}


% The intuition for this algorithm is quite simple: the strong convexity parameter is the minimal eigenvalue of the Hessian matrix (e.g. https://math.stackexchange.com/questions/673898/lipschitz-smoothness-strong-convexity-and-the-hessian)



\section{Optimizing the Curvature Efficiently}

In this section we focus on efficient solution of the optimization problems that optimize the condition number, strongly convex or smoothness parameters
\begin{align*}
    &\arg\max \frac{\lambda_{\mathrm{max} }\nabla^2 f_w(x)}{\lambda_{\mathrm{min}} \nabla^2 f_w(x)},\\
    &\arg\max_{w\in \mathcal{W}} \lambda_{\mathrm{min}}(\sum_i w_i \nabla^2 f_m(x)),\\
    &\arg\min_{w\in \mathcal{W}} \lambda_{\mathrm{max}}(\sum_i w_i \nabla^2 f_m(x)),
\end{align*}
when the Hessians are diagonal.

To be clear, let us define the problem $(P1)$ as optimizing some function $g(\cdot): \Sc_n^{++} \to \reals^{+}$ of the Hessian matrices of individual functions $f_i$,
\begin{align*}
    &g \left( \sum_{i \in \{1, 2, \ldots, m\}} w_i \nabla^2 f_i(x) \right) \\
    &\text{subject to} \,\, w_i \geq 0.
\end{align*}
We will mostly be interested in three choices of $g(\cdot)$, namely, $g(A) = \lambda_{\min}(A)$, $g(A) = \lambda_{\max}(A)$ and $g(A) = \lambda_{\max}(A) / \lambda_{\min}(A)$ which correspond to the minimum eigenvalue, maximum eigenvalue and the condition number of matrix A respectively.


\subsection{Reducing $(P1)$ to a min-max optimization}

% Assuming all Hessian matrices of all functions $f_i$ are diagonal, we can reduce the solution of $(P1)$ to a min-max optimization problem. 
Assuming we have access to diagonal approximations to Hessian matrices of all functions, we can reduce the solution of $(P1)$ to a min-max optimization problem. Further, we show it can be solved with a closed form solution that requires only forward passes and no additional backward passes.


Specifically, we show that for the strongly convex and smooth cases this optimization problem is a \emph{linear min-max optimization problem}. When optimizing the condition number it results with a generalized fractional linear optimization problem. Let $\Delta_m := \{a \in \reals^m: a_i \geq 0, \sum_{i=1}^m a_i = 1\}$ be the $m$-dimensional simplex. Let $A_i$ be a diagonal approximation of the Hessian of function $f_i$, and $\lambda_{i,j}$ denote its $j^{th}$ eigenvalue. 
\begin{itemize}
    \item \emph{Strongly convex case.} 
    The goal is to find the weights $w \in \Delta_m$ to maximize the strong-convexity parameter of the weighted functions. Problem $(P1)$ can be written as 
    \begin{align*}
    \hat{w} \in \argmax_{w \in \Delta_m} \,\, \lambda_{\min}\left( \sum_{i=1}^m w_i A_i \right)
    \end{align*}
    and can equivalently be written in the following form:
    \begin{align}
        \hat{w} &\in \argmax_{w\in \Delta_m} \,\, \min_{j\in [n]} \sum_i^m w_i \lambda_{i,j}  \label{eq:P1j}\\
        \hat{w} &\in \argmax_{w\in \Delta_m} \,\, \min_{q\in \Delta_n} \sum_{i,j} w_i \lambda_{i,j} q_j \label{eq: minmax P1},
    \end{align}
    % where $ \lambda_{i,j}$ is the jth eigenvalue of the Hessian $\nabla^2 f_j(x).$ 
    This optimization problem is a max-min and can be solved via best response or primal-dual update of $w$.

    
    \item \emph{Smooth case.} In this case, we want to minimize the Lipschitz constant which is the maximal eigenvalue of the Hessian matrix. Problem ($P1$) in this case can be written as
    \begin{align*}
    \hat{w} \in \argmin_{w \in \Delta_m} \,\, \lambda_{\max}\left( \sum_{i=1}^m w_i A_i \right)
    \end{align*}
    We will get analogous problem to the strongly convex case as in equation~\eqref{eq: minmax P1}, where the max and min are replaced (we will minimize the maximal eigenvalue).
    \item  \emph{Minimize the condition number.} Define the condition number of a matrix as $\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}$. Problem $(P1)$ will take the form of 
    \begin{align}
        \argmin_w \,\, \frac{\max_{j\in [n]} \sum_i w_i \lambda_{i,j}}{\min_{j\in [n]} \sum_i w_i \lambda_{i,j}}. \label{eq:condition number diag P1}
    \end{align}
This can be reduced to a fractional linear program. A fractional linear program takes the form of (see \url{http://www.seas.ucla.edu/~vandenbe/ee236a/lectures/lfp.pdf})
\begin{align*}
    \min_x \max_{i} \frac{c_i^Tx + d_i}{f_i^T x + g_i}.
\end{align*}

To see this observe the following holds for any $w$
\begin{align*}
    & \frac{\max_{j\in [d]} \sum_i w_i \lambda_{i,j}}{\min_{j\in [d]} \sum_i w_i \lambda_{i,j}}=\max_{j\in [d]}\max_{j'\in [d]} \frac{ \sum_i w_i \lambda_{i,j}}{ \sum_i w_i \lambda_{i,j'}} =\max_{k\in [d]\times [d]}\frac{c_k^T w}{f_k^T w} 
\end{align*}
where for a $k\in [d]\times [d]$ that represents a tuple $(j,j')$ we set $c_k^T=\lambda_{j}\in \reals^M$ and $f_k^T=\lambda_{j'}\in \reals^M$ and $\lambda_{j}$ is a vector with $\lambda_{i,j}$ its elements. This problem is not a simple convex optimization problem, but, as a heuristic, GD can also be used to solve it, namely, equation~\eqref{eq:condition number diag P1} can be written as
\begin{align*}
    \min_w\max_{q\in \Delta_{d^2}}\sum_k q_k\frac{c_k^T w}{f_k^T w}, 
\end{align*}
and can be also solve with best-response our GD.

% \jb{This part is a bit confusing to me. I think the cleaner way might be write it as}
% \begin{align}
% \hat{w} \in \argmin_{w \in \Delta_m} \,\, \max_{q, \tilde{q} \in \Delta_n} \frac{\sum_i w_i \lambda_{i,j} q_j}{ \sum_i w_i \lambda_{i,j} \tilde{q}_j}
% \end{align}
\end{itemize}


\subsection{Approximation Guarantees: Diagonal Hessian}
In this section we explore approximate solutions to $(P1)$: assume that $\nabla^2 f_i(x)$ can be approximated by $A_i$, what is the quality of solving the optimization problem $(P1)$ over $\{A_i\}_i$ instead?


Informally, we derive a convergence guarantee to the AMOO-Optimizer algorithm with the rate of $\mu_\star-O(||\Delta||)$ where $\Delta$ represents the deviation of the Hessian from its diagonal form. Namely, the convergence will degrade continuously depending on the extent to which the diagonal approximation is good. 


The approximation results in the following rests on the following well known result that establishes continuity of eigenvalues.
\begin{theorem}[Weyl's Theorem] Let $A$ and $\Delta$ be symmetric matrices in dimension $d$. Let $\lambda_j(A)$ be the jth largest eigenvalue of a matrix $A$. Then, for all $j\in [d]$ it holds that $\| \lambda_j(A) - \lambda_j(A+\Delta) \| \leq \| \Delta\|_2$, where $\| \Delta\|_2$ is the operator norm of $\Delta$.
\end{theorem}

Assume that $A_i = \nabla^2 f(x) + \Delta_i$. 
\begin{itemize}
    \item \emph{Strongly convex case.} Let $w_\star $ denote the optimal weights of the optimization problem,
    \begin{align*}
        w_\star \in \arg\max_{w\in \Delta} \lambda_{\mathrm{min}}(\sum_i w_i \nabla^2 f_i(x)),
    \end{align*}
    and let $g(w_\star)$ denote the optimal value, $g(w_\star) = \lambda_{\min} (\sum_i w_{\star, i} \,\, \nabla^2 f_i(x))$. Further, let $\hat{w}_\star$ denote the solution of the optimization problem of the approximation
    \begin{align*}
        \hat{w}_\star \in \arg\max_{w\in \Delta} \lambda_{\mathrm{min}}(\sum_i w_i A_i),
    \end{align*}
    and denote $\hat{g}(\hat{w}_\star)$ as its value, $\hat{g}(\hat{w}_\star) = \lambda_{\min} (\sum_i \hat{w}_{\star, i} \,\, A_i$. Then, the following holds.
    \begin{align*}
        &g(w_\star) = g(w_\star) - \hat{g}(w_\star) + \hat{g}(w_\star) -\hat{g}(\hat{w}_\star) +\hat{g}(\hat{w}_\star) - g(\hat{w}_\star) +g(\hat{w}_\star) \\
        &\stackrel{(a)}{\leq}  g(w_\star) - \hat{g}(w_\star)  +\hat{g}(\hat{w}_\star) - g(\hat{w}_\star) +g(\hat{w}_\star) \\
        &\leq |g(w_\star) - \hat{g}(w_\star)|  + |\hat{g}(\hat{w}_\star) - g(\hat{w}_\star)| +g(\hat{w}_\star)\\
        & \stackrel{(b)}{\leq}  2 \| \Delta \|_2 +g(\hat{w}_\star).
    \end{align*}
    Where $(a)$ holds since $\hat{g}(w_\star) -\hat{g}(\hat{w}_\star)\leq 0$ by the optimality of $\hat{w}_\star$ on $\hat{g}$. Further, $(b)$ holds as a consequence of Weyl's theorem and the assumptions of the approximation error as the following inequalities show. Recall that for any $w\in \Delta_m$ it holds that 
    $$
    \| \sum_i w_i A_i -  \sum_i w_i \nabla^2 f_i(x) \|_2 \leq \sum_i w_i \|  A_i -  \nabla^2 f_i(x) \|_2 \leq \| \Delta \|_2
    $$
    since $\sum_i w_i =1$. Hence, by Weyl's theorem it holds that 
    \begin{align*}
        |g(w_\star) - \hat{g}(w_\star)\| \leq \| \Delta \|_2 \text{ and } |g(\hat{w}_\star) - \hat{g}(\hat{w}_\star)\| \leq \| \Delta \|_2.
    \end{align*}

    By setting $g(w_\star)\geq \mu_\star$, by the strong convexity assumption, we get that 
    \begin{align*}
        g(\hat{w}_\star) \geq \mu_\star -2\| \Delta\|_2.
    \end{align*}
    This means, that weighting the functions with $\hat{w}_\star$ results in $\mu_\star -2\| \Delta\|_2$ strongly convex function.

    \item \emph{Smooth case.} Same argument as the strongly convex case.

    \item \emph{Condition number.} TODO
    
\end{itemize}

\subsection{Results for simple experiments}
In order to provide some sanity check results illustrating different problem settings where AMOO could provide practical advantages, we conduct a few simple experiments for both the specification and selection settings. In all our experiments, we compare against the simplest of baseline -- an equally weighted strategy which is commonly used in practice. Comprehensive results comparing AMOO to state-of-the-art multi-task learning algorithms on commonly used benchmarks is left for future work.

Figure \ref{fig:axis_aligned_selection} below shows a simple example with two axis-aligned quadratic functions, $f_1(\cdot), f_2(\cdot)$ of the form $f_i(\theta) = (x(\theta) - x^*)^\top H_i (x(\theta) - x^*)$ where $H_1, H_2$ denote the corresponding diagonal hessians. We take $x(\theta) \in \mathbbm{R}^d$ to be the output of a 1-hidden layer neural network; the neural network here maps randomly generated feature vectors, say $x \in \mathbbm{R}^k$ to the outputs $x(\theta)$. In addition, we take $x^*$ to be the zero vector in $\mathbbm{R}^d$.

We take $k=16$ and $d=6$ so the network training can be understood as learning to map higher dimensional random features to a zero vector in lower dimension. A simple neural network should be able to learn such a map, $\theta^*$ -- both functions are aligned because they share this common optimizer $\theta^*$ -- only the two optimization objectives $f_1, f_2$ have different shapes. To simulate the selection problem setting, we take $H_1 \succ H_2$ by setting $H_1 = \text{Diag}(\mathbbm{1}_d)$ and $H_2 = \frac{1}{2} H_1$. We expect gradients in practical settings to be noisy -- we add this feature to our problem setting by adding quartic noise to the optimization objectives, 
\[
f_i(\theta) = (x(\theta) - x^*)^\top H_i (x(\theta) - x^*) + z \cdot (x(\theta) - c)^4 
\]
where we take $c$ to be a constant bias, $c = 0.2 \cdot \mathbbm{1}_d$ and $z \sim \mathcal{N}(0, \sigma I)$ where we set $\sigma=0.2$. One advantage of adding quartic noise (as opposed to adding say quadratic noise) is that it ensures that the second order derivatives of $f_1$ and $f_2$ are not constant but vary with iterates. We observe that AMOO puts the entire weight on $f_1$ allowing faster convergence to the optimal iterate while putting equal weight on both $f_1$ and $f_2$ results in a descent direction which results in slower progress per step. For simplicity, the y-axis of the plots in Figure 1 show $\|x(\theta_t) - x^*\|^2$ for different iterates $\theta_t$ on a validation set, during training. We trained for one epoch. 
\begin{figure*}[h!]
\vspace{-8pt}
    \centering
    \includegraphics[width=0.5\textwidth]{selection_axis_aligned.png}
    \vspace{-5pt}
    \caption{Selection problem with two axis aligned quadratic functions}
    \label{fig:axis_aligned_selection}
\end{figure*}


\subsection{Approximation Guarantees: AMOO Assumption}

[TODO] In this section we will show the AMOO algorithm degrades continuously when the AMOO assumption -- namely, the assumption that all functions $f_i$ share an optimizer -- is violated.

\bibliography{bibliography}
\bibliographystyle{iclr2024_conference}

\appendix


\section{Auxiliary Results}


\begin{lemma} \label{lemma:smooth-gradient-norm}
    Let $f : \reals^n \to \reals$ a $\beta$-smooth over $\mX$, and let $\x_\star \in \underset{\x \in \reals}{\argmin} ~ f(\x)$. Runs GD algorithm with step size $\eta \leq \beta^{-1}$ over $f(\cdot)$, i.e. $\y = \x - \eta \nabla f(\x)$, then, for every $\x \in \reals^n$ it holds that
    \begin{align*}
        \enorm{\nabla f (\x)}^2 \leq 2 \eta^{-1} \brac{f(\x) - f(\x_\star)}.
    \end{align*}
\end{lemma}
\begin{proof}
    By using optimality of $\x_\star$, and smoothness of $f(\cdot)$ (Def.\ref{def:smooth}), for every $\y, \x \in \mX$ it holds that 
    \begin{align*}
        f(\x_\star) \leq f(\y) \leq f(\x) + \nabla f(\x)^\top \brac{\y-\x} + \frac{\beta}{2} \enorm{\y-\x}^2 \\
    \end{align*}
    Since $\y = \x - \eta \nabla f(\x)$. We have
    \begin{align*}
        f(\x_\star)  \leq f(\x) -\eta \enorm{\nabla f(\x)}^2 + \frac{\beta}{2} \eta^2 \enorm{\nabla f(\x)}^2 =  f(\x) - \eta \enorm{\nabla f(\x)}^2 \brac{ 1 - \frac{\beta \eta}{2}  }
    \end{align*}
    Plugging in $\eta$, we obtain that
    \begin{align*}
        f(\x_\star) \leq f(\x) - \frac{\eta}{2} \enorm{\nabla f(\x)}^2 
    \end{align*}
\end{proof}

Let $\x_\star$ be the unique minimizer of $f$, an $\alpha$-strongly convex function. From Def.\ref{def:sc} and the first-order optimally condition it follows that $\forall \x \in \reals^n$:
\begin{align*}\label{eq:strong_convexity}
    \Vert \x - \x_\star \Vert ^2 \leq \frac{2}{\alpha} \brac{ f(\x) - f(\x_\star) }. 
\end{align*}

\section{Improved Convergence of AMOO-Optimizer}

\ExactAmooConvergence*

\begin{proof}
    Denote $f_{\w_t}(\x) = \sum_{i} w_i f_i(\x)$, then $\nabla f_{\w_t}(\x_t) = \g_t = \sum_{i} w_i \nabla f_i(\x_t)$. Since $\x_{t+1} = \x_t - \eta_t \g_t$ for every $\x \in \mX$ it holds that
    \begin{align*}
        \enorm{ \x_{t+1} - \x }^2 & = \enorm{\x_t - \eta_t \g_t -\x}^2  = \enorm{\x_t - \x}^2 -2\eta_t \g_t^\top (\x_t - \x) + \eta_t^2 \enorm{\g_t}^2.
    \end{align*}
    Since $\w_t\in \arg\max_{\w\in \Delta_{m}} \lambda_{\min} (\sum_{i} w_i \nabla^2 f_i(\x_{t}))$, and by Def.\ref{def:AMOO-sc} it holds that $f_{\w_t}$ is $\mu_{t}^\star$-strongly convex. Thus, by Def.\ref{def:sc} the following holds
    \begin{align*}
        \enorm{ \x_{t+1} - \x }^2 & \leq \enorm{\x_t - \x}^2 + 2\eta_t \brac{ f_{\w_t}(\x) - f_{\w_t}(\x_t) - \frac{\mu_{t}^\star}{2}\enorm{\x_t - \x} } + \eta_t^2 \enorm{\g_t}^2.
    \end{align*}
    Denote $\x_\star$ the minimizer of all function $f_i(\cdot)$, thus $\x_\star \argmin_{\x \in \mX } f_\w(\x)$ for every $\w \in \Delta_m$. Using Observation \ref{obs:sum_smooth} it holds that $f_{\w_t}$ is $\beta$ smooth. Since $\eta_t \leq \beta^{-1}$. Thus, using Lemma \ref{lemma:smooth-gradient-norm} it holds that
    \begin{align*}
        \enorm{ \x_{t+1} - \x_\star }^2 & \leq \enorm{\x_t - \x_\star}^2 + 2\eta_t \brac{ f_{\w_t}(\x_\star) - f_{\w_t}(\x_t) - \frac{\mu_{t}^\star}{2}\enorm{\x_t - \x_\star} } + 2 \eta_t \brac{f_{\w_t}(\x_t) - f_{\w_t}(\x_\star)} \\
        & = \brac{1 - \eta_t \mu_{t}^\star} \enorm{\x_t - \x_\star}^2.
    \end{align*}
\end{proof}



Towards proving this result we start with the following lemma.
\begin{lemma}\label{lem:f_w_is_lip}
    If $w\in \Delta_M$ and all $f_m$ are $L$ Lipchitz then $f_w$ is also $L$ Lipchitz.
\end{lemma}
\begin{proof}
    Since $f_m$ is $L$ Lipchitz it holds that
    \begin{align*}
        f_m(x') \leq f_m(x ) + \nabla f_m(x) \cdot (x'-x) + \frac{L}{2}||x-x' ||.
    \end{align*}
    Since $w_m\geq 0$ and $\sum_m w_m =1$ we can multiply each inequality by $w_m$, summing over the $M$ inequalities and get
    \begin{align*}
        f_w(x') \leq f_w(x ) + \nabla f_w(x)\cdot (x'-x) + \frac{L}{2}||x-x' ||,
    \end{align*}
    by using the linearity of the gradient. Namely, $f_w$ is $L$ Lipchitz.
\end{proof}

\begin{proof}
    The proof of this result essentially follows the proof here: \href{https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf}{Lecture 9 (Sham Kakade).} 

    Observe that since each $f_m$ is $L$ Lipschitz also their weighted sum is $L$ Lipschitiz (since $w\in \Delta_M$ is an element in the simplex).
    
    By lemma 3.3. in Lecture 9 we have that (since $f_w$ is $L$ Lipchitz by Lemma~\ref{lem:f_w_is_lip}):
    \begin{align}
        \label{eq: gradient norm lbds function value gap}
        ||
        \nabla f_w(x)||^2 \leq 2L (f_w(x) - f_w(x_\star)).
    \end{align}

    There are two important observations we need to use in the following proof:
    \begin{enumerate}
        \item By definition $f_w$ is strongly convex \emph{with strongly convex parameter larger than $\mu_\star$} (follows since the minimal eigenvalue of the Hessian characterizes the strongly convex parameter).
        \item Since for any $f_w$ the optimal solution is $x_\star$ (by assumption that all $f_m$ has similar optimal solution) it holds that $x_\star$ is also the optimal solution of $f_w(x)$.
    \end{enumerate}
     
    
    From this point we replicate the proof in Lecture 9. By strong convexity of $f_w$ it holds that
    \begin{align*}
        \nabla f_w(x)(x-x_\star) \geq f_w(x) - f_w(x_\star) + \frac{\mu_\star}{2} || x-x_\star ||^2.
    \end{align*}
    Hence, it also holds for $x=x_t$, namely:
    \begin{align*}
        \nabla f_w(x_t)(x_t-x_\star) \geq f_w(x_t) - f_w(x_\star) + \frac{\mu_\star}{2} || x_t-x_\star ||^2.
    \end{align*}
    Using these, we have that:
    \begin{align*}
        &|| x_{t+1} -x_\star ||^2 = || x_t - \eta \nabla f_{w_t}(x_t) -x_\star ||\\
        &= ||x_{t} -x_\star  || -2\eta \nabla f_{w_t}(x_t) (x_t - x_\star) +\eta^2 || \nabla f_{w_t}(x_t) ||^2\\
        &\leq  ||x_{t} -x_\star  || - 2\eta \left( f_{w_t}(x_t) - f_{w_t}(x_\star) + \frac{\mu_\star}{2} || x_t-x_\star ||_{gg^T}^2 \right) +\eta^2 || \nabla f_{w_t}(x_t) ||^2\\
        &\leq ||x_{t} -x_\star  || - 2\eta \left( f_{w_t}(x_t) - f_{w_t}(x_\star) + \frac{\mu_\star}{2} || x_t-x_\star ||^2 \right) +2\eta^2 L (f_w(x_t) - f_w(x_\star))\\
        & \leq \left( 1- \eta \mu_\star \right)||x_{t} -x_\star  || + 2\eta(\eta L -1 )\left( f_{w_t}(x_t) - f_{w_t}(x_\star) \right)\\
        &\leq \left( 1-\frac{\mu_\star}{L} \right)||x_{t} -x_\star  ||,
    \end{align*}
    where we used the upper bound on $\eta$ in the last step to upper bound both terms (for the second term we also use $f_{w_t}(x_t) - f_{w_t}(x_\star)\geq 0$)
\end{proof}

\section{Classic gradient analysis for convex functions.}

\paragraph{Example 1.} Here is a classic analysis of gradient descent, see for example \cite{lacoste2012simpler, bubeck2015convex}. Consider the gradient update for any of components, $x_{t+1} = x_t - \eta_i \nabla f_i(x_t)$. Then,
\begin{align}
    \|x^* - x_{t+1}\|^2_2 &= \|x^* - (x_t - \eta \nabla f_i(x_t)) \|^2_2 \nonumber \\
    &= \|x^* - x_{t}\|^2_2 + 2 \eta (x^*-x_t)^\top\nabla f_i(x_t) +  \eta^2\|\nabla f_i(x_t)) \|^2_2 \label{eq:grad_update}
\end{align}
Assume that gradient norm is uniformly bounded as $\|\nabla f_i(x)\|^2_2 \leq B \,\, \forall \, x \in \reals^n$. Also, using the fact that $f_i$ is convex, we have
\[
\nabla f_i(x_t)^\top(x^* - x_t) \leq f_i(x^*) - f_i(x_t).
\]
Using this along with \eqref{eq:grad_update}, we have
\begin{align*}
    \|x^* - x_{t+1}\|^2_2 &= \|x^* - (x_t - \eta \nabla f_i(x_t)) \|^2_2 \\
    &\leq \|x^* - x_{t}\|^2_2 + 2 \eta \left( f(x^*) - f(x_t) \right) +  \eta^2\|\nabla f_i(x_t)) \|^2_2 \\
    &\leq \|x^* - x_{t}\|^2_2 + 2 \eta \left( f(x^*) - f(x_t) \right) +  \eta^2 B^2.
\end{align*}
Taking $\eta=1/2$ implies,
\begin{equation*}
    f(x_t) - f(x^*) \leq \|x^* - x_{t}\|^2_2 - \|x^* - x_{t+1}\|^2_2 + \frac{ B^2}{4}.
\end{equation*}

% {\color{red}{YE}: isnt there $\eta$ factor missing on the RHS? shouldnt we divide by $2\eta$?}

% {\color{blue}{JB}: Corrected. To get the final inequality, take $\eta=1/2$.}

Summing over time steps and applying Jensen's gives,
\begin{align}
    f(\bar{x}_T) - f(x^*) \leq \frac{1}{T}\sum_{t=0}^{T-1} f(x_t) - f(x^*) \leq \frac{\|x^* - x_0\|^2_2}{T} + \frac{B^2}{4},
\end{align}
where $\bar{x}_T = \frac{1}{T} \sum_{t=0}^{T-1} x_t$. 

\paragraph{Example 2} 
An alternate analysis is shown  \href{https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf}{here}. As $\nabla f_i$ is Lipschitz continuous with constant $l_i$, $\nabla^2 f(x) \preceq l_i I$. One can make a quadratic approximation as follows.
\begin{align}
    \label{eq:quadratic upper bound}
    f_i(y) &\leq f_i(x) + \nabla f_i(x)^\top(y-x) + \frac{1}{2} \left(y-x\right)^\top\nabla^2f_i(x) \left(y-x\right)^2_2 \nonumber \\
    &\leq f_i(x) + \nabla f_i(x)^\top(y-x) + \frac{1}{2}l_i\|y-x\|^2_2.
\end{align}
Taking $y = x_{t+1} = x_t - \eta \nabla f_i(x_t)$, we get
\begin{align}
    f_i(x_{t+1}) &\leq f_i(x_t) + \nabla f_i(x_t)^\top(x_{t+1}-x_t) + \frac{1}{2}l_i\|y-x\|^2_2 \nonumber \\
    &\leq f_i(x_t) - \eta \| \nabla f_i(x_t) \|^2_2 + \frac{\eta^2}{2}l_i\| \nabla f_i(x_t) \|^2_2 \nonumber \\
    &\leq f_i(x_t) - \left(1 - \frac{\eta l_i}{2}\right) \eta \| \nabla f_i(x_t) \|^2_2 \label{eq:descent with step size}
\end{align}
Taking $\eta \leq 1/l_i$ implies
\begin{equation}
    \label{eq:per step descent}
    f_i(x_{t+1}) \leq f_i(x_t) - \frac{1}{2} \eta \| \nabla f_i(x_t) \|^2_2.
\end{equation}
Basically, taking a gradient step improves the objective value. This can be converted to a convergence rate. By convexity, 
\begin{align*}
    f_i(x^*) &\geq f_i(x_t) + \nabla f_i(x_t)^\top(x^*-x) \\
    \Rightarrow \,\, f_i(x_t)  &\leq f_i(x^*) + \nabla f_i(x_t)^\top(x-x^*)
\end{align*}
Plugging this in \eqref{eq:per step descent} implies,
\begin{align*}
    f_i(x_{t+1}) &\leq f_i(x^*) + \nabla f_i(x_t)^\top(x_t-x^*) - \frac{1}{2} \eta \| \nabla f_i(x_t) \|^2_2 \\
    f_i(x_{t+1}) - f_i(x^*) &\leq  \frac{1}{2\eta} \left( \nabla f_i(x_t)^\top(x_t-x^*) - \eta^2 \| \nabla f_i(x_t) \|^2_2 \right) \\
    f_i(x_{t+1}) - f_i(x^*) &\leq  \frac{1}{2\eta} \left( \nabla f_i(x_t)^\top(x_t-x^*) - \eta^2 \| \nabla f_i(x_t) \|^2_2 - \|x_t - x^*\|^2_2 + \|x_t - x^*\|^2_2 \right) \\
    f_i(x_{t+1}) - f_i(x^*) &\leq  \frac{1}{2\eta} \left( \|x_t - x^*\|^2_2 - \|x_t - \eta \nabla f_i(x_t) - x^*\|^2_2 \right).
\end{align*}
Using the gradient update, $x_{t+t} = x_t -\eta \nabla f_i(x_t)$, we get
\begin{align}
    f_i(x_T) - f_i(x^*) \leq \sum_{t=1}^{T} \left( f_i(x_t) - f(x^*) \right) &\leq \frac{1}{2\eta} \sum_{t=1}^T \left( \|x_{t-1} - x^*\|^2_2 - \|x_t - x^*\|^2_2 \right) \nonumber \\
    &\leq \frac{1}{2\eta} \|x_0 - x^*\|^2_2.
\end{align}
The first inequality follows \eqref{eq:per step descent} showing each update makes progress monotonically.


% \section{The Stochastic Setting}
% Some good references for the proof techniques in the stochastic setting are 
% \begin{itemize}
%     \item \citet{nemirovski2009robust}
%     \item \citet{rakhlin2011making}
%     \item \citet{bertsekas2011incremental}
%     \item \citet{wang2017stochastic}
% \end{itemize}
% All three paper prove bounds on the expected suboptimality or distance to optimal, and \cite{rakhlin2011making}, notably, gives a result of the high probability form. Converting between the suboptimality versus distance to optimal types of bounds is straightforward using smoothness (Lipschitz gradients) just like in the deterministic case.

% \subsection{Illustration on Toy Algorithm}
% Using the same notation as above, we have $f_m$ for $m \in [M]$ and they share an optimizer. We can query unbiased stochastic gradients $g_m(x)$, such that $\mathbb{E}[g_m(x)] = \nabla f_m(x)$.
% \begin{algorithm}
% 	\caption{Randomly-selected-objective MO in SGD setting} 
% 	\begin{algorithmic}[1]
% 		\For {$t=1,2,\ldots$}
% 			\State Sample $m_t$ uniformly from $[M]$
%                 \State Observe stochastic gradient $g_{m_t}(x_t)$ 
%                 \State Projected gradient update: $x_{t+1} = \Pi_{\mathcal X} \left[ x_t - \eta_t g_{m_t}(x_t) \right]$		
%   \EndFor
% 	\end{algorithmic} 
% \end{algorithm}
% \citet{nemirovski2009robust} and \citet{rakhlin2011making} essentially use the same technique as the deterministic setting.

% \begin{theorem}
%     Assume each $f_m$ is (1) $\mu_m$-strongly convex, (2) that we are optimizing over a convex domain $\mathcal X$, (3) the optimizers of all functions are the same, (4) $\mathbb{E}[\|g_m\|^2] \le G^2$, and (5) $\eta_t = 1/(\bar{\mu}t)$ where $\bar{\mu}=M^{-1}\sum_m \mu_m$. Then,
%     \begin{align*}
%         \mathbb E\bigl[ \| x_{t+1} - x^* \|^2 \bigr] \le 4G^2/(\bar{\mu}^2 t)
%     \end{align*}
% \end{theorem}

% \begin{proof}
% Note that:
% \[
% \mathbb E\bigl[ \|g_{m_t}\|^2 \bigr] = 
% \mathbb E \bigl[ \mathbb E\bigl[ \|g_{m_t}\|^2 \, |\, m_t \bigr] \bigr] \le G^2.
% \]
% Also, note that:
% \[
% \mathbb E\bigl[ \langle g_{m_t}, (x_{t} - x^*)\rangle \bigr] = \mathbb E\bigl[ \mathbb E\bigl[ \langle g_{m_t}, (x_{t} - x^*)\rangle \, | \, m_t\bigr]\bigr] = M^{-1} \sum_{m} \mathbb E\bigl[ \langle g_{m}, (x_{t} - x^*)\rangle \bigr].
% \]
% Following the standard analysis of \citet{nemirovski2009robust, rakhlin2011making}, we have:
%     \begin{align*}
%     \mathbb E\bigl[ \| x_{t+1} - x^* \|^2 \bigr] 
%     &= \mathbb E\bigl[ \| \Pi_{\mathcal X} [x_{t} - \eta_t g_{m_t}] - x^* \|^2 \bigr] \\  
%     &\le \mathbb E\bigl[ \| x_{t} - \eta_t g_{m_t} - x^* \|^2 \bigr]   \\
%     &\le \mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr] - 
%  2 \eta_t \mathbb E\bigl[ \langle g_{m_t}, (x_{t} - x^*)\rangle \bigr] + \eta_t^2 \mathbb E\bigl[ \|g_{m_t}\|^2 \bigr] \\
%     &\le \mathbb E\bigl[ \| x_{t} - x^* \|^2\bigr] - 
%  2 \eta_t M^{-1} \sum_m \mathbb E\left[ f_m(x_t) - f_m(x^*) + \frac{\mu_m}{2} \| x_{t} - x^* \|^2 \right] + \eta_t^2 G^2 \\
%      &\le \mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr]- 
%  2 \eta_t M^{-1} \sum_m \mathbb E\left[ \frac{\mu_m}{2} \| x_{t} - x^* \|^2 + \frac{\mu_m}{2} \| x_{t} - x^* \|^2 \right] + \eta_t^2 G^2 \\
%  &= \left( 1 - 2\eta_t \bar{\mu} \right)\mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr] + \eta_t^2 G^2 \\
%  &= \left( 1 - \frac{2}{t} \right)\mathbb E\bigl[ \| x_{t} - x^* \|^2 \bigr] + \frac{G^2}{\bar{\mu}^2 t^2}
%     \end{align*}
% Using \citet[Lemma 2]{rakhlin2011making} and induction, we get $\mathbb E\bigl[ \| x_{t+1} - x^* \|^2 \bigr] \le 4G^2/(\bar{\mu}^2 t)$. Alternatively, we can also apply \citet[Lemma 1]{chung1954stochastic}, which is a fancy way of analyzing the recursive relationship above.
% \end{proof}

% The reason that this is result is not as strong as the deterministic case is that we did not use smoothness to get rid of the last squared gradient term. 

% \subsection{High Probability Extension}
% Just putting this here as a reminder for the future. See \citet{rakhlin2011making} for how they do this.

% \subsection{Stochastic Compositional Gradient Descent}
% I thought \citet{wang2017stochastic} might be helpful as well. It is a complicated version of this proof technique for the case where there is nested learning. Perhaps when we try to be adaptive, some of those ideas can be useful.


\section{Draft}

Let $\{a_i\}_{i\in [n]}$ be the diagonal elements of the weighted matrix:
\begin{align*}
    \mu =  \min_{i} a_i := \min_{i\in [n]} \sum_{j\in [m]} w_j b_{i,j} \leq \min_i\sum_{j\in [m]}  b_{i,j}.
\end{align*}
because $w_j M\leq M$ since $w_j\leq 1$ and $b_{i,j}\geq 0$. Divide by $M$ to obtain the final result:
\begin{align*}
    \mu/M\leq \sum_{j}  b_{i,j}/M
\end{align*}


\begin{lemma}
     Let $H_i$ are PSD matrices and $w_i\in [0,1]$. Then, $\lambda_{\min}(\sum_i w_i H_i) \leq \lambda_{\min}(\sum_i  H_i)$.
\end{lemma}
\begin{proof}
Let $v$ be an eigenvector corresponding to $\lambda_{\min}\left(\sum_i H_i\right)$.
Then, $v^T\left(\sum_i H_i\right)v = \lambda_{\min}\left(\sum_i H_i\right)$. We have that
\begin{align*}
   v^T\left(\sum_i H_i\right)v = \sum_i (v^T H_i v) \geq \sum_i w_i(v^T H_i v) = v^T(\sum_i w_i H_i)v,
\end{align*}
where the inequality holds since $w_i \in [0,1]$ and $(v^T H_i v)\geq 0$ by the PSD assumption on $H_i$. Finally, recall that for any normalized vector $v$
\begin{align*}
    v^T(\sum_i w_i H_i)v \geq \lambda_{\min} (\sum_i w_i H_i),
\end{align*}
since for any normalized vector and matrix it holds that $v^T A v \geq \lambda_{\min}(A).$
Hence,
\begin{align*}
    \lambda_{\min} (\sum_i w_i H_i) \leq   \sum_i w_i (v^T H_i v) \leq \lambda_{\min}\left(\sum_i H_i\right).
\end{align*}
\end{proof}

This lemma directly implies what we want to show. We have that for some weight vector $w$
\begin{align*}
    \mu_g = \lambda_{\min}(\sum_i w_i H_i).
\end{align*}
By the last lemma:
\begin{align*}
     \mu_g   \leq \lambda_{\min}(\sum_i  H_i).
\end{align*}
Dividing both sides by $m$ and using homogeneity of eigenvaues $ a \lambda_{\min}(\sum_i  H_i) = \lambda_{\min}(a \sum_i  H_i)$ implies the result.

\end{document}

