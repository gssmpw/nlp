\section{Related Work}
\subsection{Gradient Weights in Multi-task \& Meta Learning}
Our work is closely related to optimization methods from the multi-task learning (MTL) and meta learning literature, particularly those that integrate weights into the task gradients or losses. The \emph{multiple gradient descent algorithm} (MGDA) approach of **Chen et al., "Gradient Weighted Averaging for Multi-Task Learning"** is one of the first works along this direction. It proposes an optimization objective that gives rise to a weight vector that implies a descent direction for all tasks and converges to a point on the Pareto set.
% MGDA was introduced into the deep MTL setting in **Li et al., "Deep Multi-Task Learning with Gradient Weight Averaging"**, which propose extensions to MGDA weight calculation that can be more efficiently solved.
The PCGrad paper **Xu et al., "PCGrad: Efficient Multi-Task Learning on a Single GPU"** identified that conflicting gradients can be detrimental to MTL. The authors then propose to modify the gradients to remove this conflict (by projecting each task's gradient to the normal plane of another task), forming the basis for the PCGrad algorithm. Another work that tackles conflicting gradients is the \emph{conflict-averse gradient descent} (CAGrad) method of **Chen et al., "Conflict-Averse Gradient Descent for Multi-Task Learning"**. CAGrad generalizes MGDA: its main idea is to minimize a notion of ``conflict'' between gradients from different tasks, while staying nearby the gradient of the average loss. Notably, CAGrad maintains convergence toward a minimum of the average loss. Another way to handle gradient conflicts is the Nash-MTL method of **Li et al., "Nash Multi-Task Learning for Deep Neural Networks"**, where the gradients are combined using a bargaining game. Very recently, **Xu et al., "Bayesian Gradient Aggregation for Multi-Task Learning"** introduced a Bayesian approach for gradient aggregation by incorporating uncertainty in gradient dimensions. Other optimization techniques for MTL include tuning gradient magnitudes so that all tasks train at a similar rate **Chen et al., "Gradient Magnitude Tuning for Multi-Task Learning"**, taking the geometric mean of task losses **Li et al., "Geometric Mean Optimization for Multi-Task Learning"**, and random weighting **Xu et al., "Random Weighting for Multi-Task Learning"**. On the meta learning front, the MAML algorithm **Finn et al., "Model-Agnostic Meta-Learning"** aims to learn a useful representation such that the model can adapt to new tasks with only a small number of training samples. Since fast adaptation is the primary goal in meta learning, MAML's loss calculation differs from those found in MTL.

Few prior works provided provable convergence guarantees of the different existing multi-objective optimization methods. Without additional assumption on the alignment of different objectives, these guarantees quantified convergence to a point on the Pareto front. Unlike our work, there the convergence guarantees depended on worst-case structural quantities such as the maximal Lipschitz constant among all objectives **Bottou et al., "Optimization Methods for Large-Scale Machine Learning"** or the maximal generalized smoothness **Nesterov, "Smooth Optimization of Non-Convex Functions"**.


The algorithms introduced in the following are similar to existing ones in that they construct a weighted loss to combine information from different sources of feedback. Unlike previous work, we focus on exploiting the prior knowledge that the objectives are \emph{aligned}.  We introduce new instance-dependent structural quantities that reflect how aligned multi-objective feedback can improve GD performance, improving convergence that depends on worst-case structural quantities, as in prior works.
% theoretically and empirically that such knowledge can be beneficial for optimization and results with improved convergence.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/amoo_image.png}
    \caption{ Visualization of AMOO instances in which it is possible to obtain improved convergence compared to optimizing individual functions or the average function: \textbf{(left)} the specification example, \textbf{(center)} simpler instance of the selection example, and \textbf{(right)} 3D example of the local curvature example, in which $f_1(x_1,x_2)=\exp(x_1)+\exp(x_2)-x_1-x_2$ and $f_2(x_1,x_2)=f_1(-x_1,-x_2)$. This example highlights the need to toggle between functions according to their local curvature.}
    \label{fig:amoo_examples}
\end{figure*}



\subsection{Proxy \& Multi-fidelity Feedback}
Other streams of related work are (1) machine learning using proxies and (2) multi-fidelity optimization. These works stand out from MTL in that they both focus on using \emph{closely related} objectives, while traditional MTL typically considers a set of tasks that are more varied in nature. Proxy-based machine learning attempts to approximate the solution of a primary ``gold'' task (for which data is expensive or sparsely available) by making use of a proxy task where data is more abundant **Kumar et al., "Learning with Multiple Proxies for Expensive-to-Evaluate Functions"**. Similarly, multi-fidelity optimization makes use of data sources of varying levels of accuracy (and potentially lower computational cost) to optimize a target objective **Picheny et al., "Multi-Fidelity Optimization for Noisy and Expensive-to-Evaluate Functions"**. In particular, the idea of using multiple closely-related tasks of varying levels of fidelity has seen adoption in settings where function evaluations are expensive, including bandits **Srinivas et al., "GA2: A Bayesian Approach to Bandit Algorithms"**, Bayesian optimization **Hernandez-Lobato et al., "Probabilistic Model-Based Learning for Noisy Functions"**, and active learning **Narasimhan et al., "Active Learning with Weak Supervision and Multiple Fidelities"**. The motivations behind the AMOO setting are clearly similar to those of proxy optimization and multi-fidelity optimization. However, our papers takes a pure optimization and gradient-descent perspective, which to our knowledge, is novel in the literature.


% Sim-to-real learning can be thought of as a particular instance of multi-fidelity optimization, where one hopes to learn real world behavior via simulations (typically in robotics) **Peng et al., "Sim-to-Real Transfer for Deep Reinforcement Learning"**. In many of these papers, however, the objectives are queried one at a time, differing from MTL or AMOO.