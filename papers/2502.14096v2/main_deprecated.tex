\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{caption, subcaption}
% Definitions of handy macros can go here

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\pearl}{{\tt Pearl}}
\newcommand{\pearlagent}{{\tt PearlAgent}}
\newcommand{\policylearner}{{\tt policy\char`_learner}}
\newcommand{\explorationmodule}{{\tt exploration\char`_module}}
\newcommand{\replaybuffer}{{\tt replay\char`_buffer}}
\newcommand{\safetymodule}{{\tt safety\char`_module}}
\newcommand{\historysummarizationmodule}{{\tt history\char`_summarization\char`_module}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\def\jb#1{{\color{red}#1}}

%To remove comments add flag as \usepackage[suppress]{color-edits}
\usepackage{color-edits}
\addauthor{YE}{blue}
\addauthor{DJ}{green}

\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!80!black},
    citecolor={blue!80!black},
    urlcolor={blue!80!black}
}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2023}{1-48}{4/00}{10/00}{Zheqing Zhu et al.}

% Short headings should be running head and authors last names

\ShortHeadings{Pearl: A \textbf{P}roduction-R\textbf{ea}dy \textbf{R}einforcement \textbf{L}earning Agent}{Zheqing Zhu et al.}
\firstpageno{1}

\begin{document}

\title{Pearl: A \textbf{P}roduction-R\textbf{ea}dy \textbf{R}einforcement \textbf{L}earning Agent}

% Bill to make some final adjustments to author list by contribution
\author{\name Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Frank Cheng, Urun Dogan, Zheng Wu, Wanqiao Xu\\ \email Correspondence to billzhu@meta.com\\
\addr Applied Reinforcement Learning Team\\
Meta AI \\
Menlo Park, CA 94025, USA}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
% Reinforcement learning (RL) offers a framework for intelligent agents to achieve goals in various environments. 

% Reinforcement learning (RL) offers a framework for intelligent agents to achieve goals in
% various environments. While there have been significant advancements in RL research, real-
% world implementations of RL agents often lack versatility and standardization.
% Key challenges in RL include accounting for delayed rewards, handling environments with
% partial observability, balancing exploration and exploitation and ensuring safety. While the
% academic community has developed methods to address these challenges, many existing
% open-source RL libraries focus primarily on popular learning algorithms, often overlooking
% essential features like exploration strategies, safety considerations, history summarization
% strategies and offline RL methods. This paper introduces \textbf{Pearl}, a \textbf{P}roduction-r\textbf{ea}dy \textbf{RL} agent software package designed and to be open-sourced for real-world applications. \textbf{Pearl} offers a comprehensive set of features, including intelligent exploration, risk-sensitive learning, history summarization, support for dynamic action space, data-augmenting replay buffers and support for both online and offline policy optimization. We present some benchmark performances of the agent alongside its industry adoptions to demonstrate its readiness for production usage. The software is GPU-enabled, supports distributed training, and is built on PyTorch. The upcoming release in November 2023 before the NeurIPS ALOE Workshop aims to provide a versatile tool for real-world RL applications.

Reinforcement Learning (RL) provides a general framework of achieving goals in the world. The generality of the framework enables the formalization of a wide range of challenges that real-world intelligent systems often encounter, such as dealing with delayed rewards, handling partial observability, addressing the exploration and exploitation dilemma, utilizing offline data to boost online performance, and ensuring safety. Despite significant progress made by the RL research community in tackling these challenges, existing open-source RL libraries mostly focus on methods that neglect them. This paper introduces \textbf{Pearl}, a \textbf{P}roduction-r\textbf{ea}dy \textbf{RL} agent software package designed to embrace these challenges. In addition to presenting preliminary benchmark results, this paper highlights Pearl's industry adoptions to demonstrate its readiness for production usage. The upcoming release in November 2023 before the NeurIPS ALOE Workshop aims to provide a versatile tool for real-world RL applications.
\end{abstract}

% \begin{keywords}
% Reinforcement Learning
% \end{keywords}

\section{Introduction}
% Reinforcement learning provides a mathematical formalism for the problem of an intelligent agent interacting with an unknown environment and learning to achieve a goal  \citep{sutton2018reinforcement}. 
The field of Reinforcement Learning has achieved significant successes in recent years. These accomplishments encompass a wide range of achievements, from surpassing human-level performance in Atari Games \citep{mnih2015human} and the Game of Go \citep{silver2017mastering}, to controlling robots to perform complex manipulation tasks \citep{mnih2015human,peng2018sim, levine2016end}. Moreover, the practical applications of these advancements extend into real-world systems, including recommender systems \citep{xu2023optimizing} and large language models \citep{ouyang2022training}. In addition to these successful RL systems, significant progress has been made in designing open-resource libraries that enable developing RL systems easily. These libraries include RLLib \citep{liang2018rllib}, Stable-Baselines 3 \citep{stable-baselines3}, Tianshou \citep{tianshou}, etc.
% Research in theory and practice of reinforcement learning (RL) has advanced significantly in recent years, 

% ranging from superhuman performance in games \citep{mnih2015human} to successful prototypes in robotics \citep{peng2018sim, levine2016end} as well as in a variety of practical real-life systems including recommender systems \citep{xu2023optimizing} and large language models \citep{ouyang2022training}. However, distinct from many supervised learning fields such as computer vision and natural language processing, where there exist industry-grade packages \citep{torchvision2016, wolf-etal-2020-transformers} for commonly used models, RL agents typically rely on ad-hoc implementations for real-world adoptions. Furthermore, most implementations only attempt to solve one aspect of the problem (e.g., reward maximization through online policy optimization or offline RL for conservative policy improvement).

Successful and efficient RL systems must address numerous important challenges. For instance, RL agents must look beyond immediate consequences and account for delayed rewards from downstream effects of their actions. What makes matters worse is that the environment itself may not be able to provide full transparency of its underlying state, requiring the agent to be able to reason about the current environment state from the history of interactions.
% Similarly, since the environment is unknown, RL agents must collect environment interactions and efficiently learn from them to be able to generalize to different situations. 
The trade-off between exploration and exploitation is another fundamental challenge in reinforcement learning -- in order to reason about actions which help achieve its goal, an RL agent needs to \textit{explore} to probe the environment to acquire information about actions and their consequences. Additionally, in order to avoid catastrophic situations or accommodate other preferences, an RL agent may need consider additional restrictions, such as safety-based constraints or risk requirements, when learning to act optimally.

% The academic research community has focused on developing scalable and efficient methods for each of these challenges, often combining them to yield impressive performance results (\jb{add citations}). Several open-source deep RL libraries have been developed in recent years, including RLLib \citep{liang2018rllib}, Stable-Baselines 3 \citep{stable-baselines3}, and Tianshou \citep{tianshou}. Most of them have been developed with a focus on \textit{scalable and reproducible RL research} -- presenting decent algorithm implementations to offer a clear understanding of state-of-the-art RL algorithms as well as baselines to bench-mark against. 

While the importance of these challenges has been widely acknowledged by the RL community, existing RL libraries seem to neglect these challenges. For example, important features like exploration, safe/constrained policy learning, credit assignment for long-horizon delayed-reward settings, and partial observability are often missing. In addition, many libraries exclusively support online RL algorithms, while offline RL methods can be critical for real world applications -- either as a tool for pre-training or as the main mode of learning when online interactions are not possible. Moreover, the open source community has developed code for bandit- and Markov decision process (MDP)-based problem settings separately, without treating them in a unified way. From a system design perspective, different sequential decision making problem settings share some common features, irrespective of bandit feedback or Markovian transitions. A detailed discussion about features of existing libraries can be found in Section \ref{sec: lit_review}.

In this paper, we introduce {\tt Pearl}, a \textbf{P}roduction-R\textbf{ea}dy \textbf{R}einforcement \textbf{L}earning Agent, an open-source software package, which aims to enable users to build a versatile RL agent for their real-world applications. A {\tt PearlAgent} can be equipped with one or many capabilities among intelligent exploration strategies, risk sensitive and constrained learning capabilities, offline and online policy optimization algorithms, as well as learning with partially observable environments using history summarization. We believe augmenting an RL agent with such capabilities to be essential for research development as well as adoption of RL ideas for real-world applications. Bandit learning algorithms are also supported besides the typical setting with Markovian transitions\footnote{We use history summarization to work with non-Markovian transitions.}, given a substantial research effort in that field and numerous product applications involving bandit feedback. We have put attention in adhering to software engineering best practices to avoid code duplication, and modularizing different functionalities for users to be able to easily integrate (a subset of) these in building their custom RL agent. {\tt Pearl} supports GPU enabled training, is built on native PyTorch to support distributed training, and provides support for testing and evaluation. {\tt Pearl} is currently adopted by multiple industry products, ranging from recommender systems, ads auction pacing and contextual-bandit based creative selection. These applications require support from {\tt Pearl} across online exploration, offline learning, safety, data augmentation, history summarization and dynamic action space.  

We are working towards policy approval, rigorous checks and extensive benchmarking on standard and production environments in the run-up to our code release in early November 2023 before the NeurIPS ALOE workshop. This paper is intended to serve as a preliminary introduction of our motivation, features and design choices for {\tt Pearl}, and simple illustrations of user interface to the community. More details are given in Section \ref{sec: pearl_agent}. Section \ref{sec: lit_review} compares {\tt Pearl} to other open-source RL libraries. A subset of bench-marking results are presented in Section \ref{sec: benchmarking}.

% \jb{Not emphasized credit assignment in the introduction as we only have hindsight experience replay. Dynamic action spaces haven't been tested, so want to avoid that in the introduction. Both are mentioned in section 2.1.} 

% (Bill) 
% Intro Skeleton:
% \begin{enumerate}
%     \item Recent research has advanced adoption of reinforcement learning in real-life systems across multiple fields, but most adoption relies solely one aspect of an RL system. 
%     \item A truly intelligent agent ready for production usage should be versatile, able to take care of multiple facets of real-world complexities.
%     \item There has been a number of open-source packages released to democratize the use of reinforcement learning in research and industry but most of them remain a collection of algorithms, instead of a versatile agent. (dynamic action space, asynchronous environments)
%     \item We present Pearl, an agent that's capable of learning, exploration, transition from offline dataset to online environments, summarizing non-Markovian histories into Markovian subjective states, data augmentation and ensuring different forms of safety. 
% \end{enumerate}

% \section{Related Work}

\section{The Pearl Agent}
\label{sec: pearl_agent}
This section gives details of different modules in \pearl\,and how they interact with each other for sequential decision making with a \pearlagent. Essentially, \pearl\, is organized into five main modules, namely, \policylearner, \explorationmodule, \replaybuffer, \safetymodule and a \historysummarizationmodule. A separate module provides support for instantiating different kinds of neural networks, along with neural network based value (q-value) and policy functions. Helper functions, along with scripts for offline and online training and evaluation are provided separately to facilitate usage.

To facilitate better understanding of the agent, we will use the following notations in the below subsections:
\begin{enumerate}
    \item Observation: $O_t \in \mathcal{O}$ indicates the observation the agent makes at time $t$. This can be a Markovian state, a non-Markovian partial observation or a context in the contextual bandit setting. 
    \item Action: $A_t \in \mathcal{A}_t$ indicates an action the agent chooses at time $t$, which is a member of the available action space $\mathcal{A}_t$ at time $t$. We subscript action space by time and also enable dynamic action space since in real-world applications, e.g. recommender systems, available action space usually varies by time. 
    \item Reward: $R_t \in \mathbb{R}$ indicates a scalar reward the agent receives at time step $t$. In this work, we assume that when an agent takes an action at time $t$, it receives a reward at time $t+1$. 
    \item History and Subjective State: $H_t = (O_0, A_0, R_1, O_1, A_1, \dots, O_t, A_t) \in \mathcal{H}$ indicates the history of interactions between the agent and the environment. 
    \item Interaction tuple: $\Ec_t = (R_t, O_t, A_t)$ indicates a tuple of reward, observation and new action. 
\end{enumerate}

\subsection{Agent Design}
In designing the \pearlagent, we thought of some key elements which would be essential for efficient learning in a real-world sequential decision making problem. \jb{add description of an environment interaction tuple, policy, state, observation etc.}. Consider a representative problem where users have access to offline collected data, either in the form of environment interaction tuples or (partial) trajectories. Additionally, users may have the ability to interact with the environment to collect more data. At the very least, we consider the different features listed below as key building blocks of a comprehensive RL agent. 

\begin{enumerate}
    \item \textbf{Offline learning/pretraining}: Depending on the problem setting (bandit or observation transitions), an RL agent should be able to leverage an offline learning algorithm to learn a policy and be able to evaluate it. 

    \item \textbf{Online learning}: With a pretrained/prior policy, the agent should be able to a) explore to  intelligently collect the most informative interaction tuples, and b) learn from the collected experiences to reason about the optimal policy. For bandit settings, the agent should be able to leverage different function approximators to reason about optimal decisions while for MDP settings, the agent should have the flexibility of choosing from a variety of on-policy/off-policy policy optimization algorithms. Additionally, the option of choosing between model-based and model-free methods seems desirable.

    \item \textbf{Safe learning}: For both offline and online learning, an RL agent should have the ability to incorporate some form of safety or preference constraints. Users might want to impose such constraints both for data collection (in the online setting) as well as for policy learning. 

    \item \textbf{Representation learning and history summarization}: In addition to different modes of learning, the agent should be able to leverage different models for learning state representations, value and policy function approximators. Moreover, for partially observable environments, it is important for the agent to have the ability to use different sequence models for learning a state representation. We refer to this as ``history summarization''.

    \item \textbf{Replay Buffers}: For efficient learning, an RL agent should have the ability to subset the environment interaction data which it prioritizes to learn from. A common way to do this is through the design of a replay buffer, which can be customized to support different problem settings. To enhance learning efficiency, it may be important for the agent to be able to augment the replay buffer with auxiliary signals for credit assignment.
\end{enumerate}

\pearl\, supports the all of the above features in a unified way\footnote{For this iteration, we plan to only support model-free RL methods. Offline evaluations, and model based RL methods are planned for the next version of \pearl}. Besides a suite of policy learning algorithms, users can instantiate a \pearlagent\, to include an appropriate replay buffer, a history summarization modules\footnote{We are working to integrate more general state representation tools in \pearl\, and hope to include it in this version's code release.} as well as a safe learning module to account for preferences/constraints during policy learning and to filter out undesirable actions during collection of new environment interactions. Modular code design enables seamless integration between the different functionalities in an \pearlagent. Figure \ref{fig:agent_interface} visualizes possible different components of a \pearlagent\, and how they are designed to interact with each other. We give discuss each module below.


% Besides, online interactions with the environment may require incorporating of safety/preference constraints during policy learning, which can be done seamlessly by the {\tt PearlAgent}.


% A typical RL or CB agent solves a decision making task by taking the following steps:
% \begin{enumerate}
%     \item Offline Pretraining (Offline RL + CB): Given offline data generated by existing policies, a typical RL or CB agent pretrains their own policy, sometimes in a conservative fashion.
%     \item Online Exploration and Learning (Online RL + CB): With a pretrained policy or a prior policy, the agent interacts with an environment exploratively to collect information useful to resolve its uncertainties about the environment and learn a new policy based on its new belief about the environment and the optimal policy.
%     \item Ensuring Safety: While taking actions to interact with the environment, the agent can choose to take actions that minimizes potential risk and satisfies constraints of the environment. 
%     \item History Summarization: after taking an action through online exploration, the agent usually takes a non-Markovian observation from the environment and needs to summarize from its past history and the new observation into a Markovian subjective state. 
%     \item Data Augmentation: Given a transition that includes the latest action, the past Markovian subjective state, the reward received from the environment and the new Markovian subjective state, the agent can form a replay buffer to store such transitions for future usage and learning. To enhance learning efficiency, the agent can choose to augment the replay buffer with additional signals. 
% \end{enumerate}

% \jb{see above to see if it addresses modularity and adaptivity in an appropriate way.}

% Our design of the Pearl agent follows exactly the above steps, in a highly modularized fashion. The core of the Pearl agent consists of the Pearl agent interface and five modules, policy learner, exploration, safety, history summarization and replay buffer. (Add figure to illustrate how these are connected.)
% {\color{blue} Yon com: discuss about adaptivity and modularity}


\subsubsection{Agent Interface}
Fig. \ref{fig:agent_interface} illustrates interactions amongst components of a \pearlagent\, in an online learning paradigm. Each learning epoch alternates between getting a new environment interaction and a training pass. Starting from an observation $O_t$, along with an estimate of the policy $\pi_t$, the \pearlagent\, queries for an interaction tuple $\Ec_t$ by taking action $A_t$. 

To account for the trade-off between exploration and exploitation, \pearlagent\, decides to take action $A_t$ by querying its \explorationmodule\, (which outputs an exploratory action $A^{\rm explore}$), in conjunction with the \policylearner\, (which outputs an exploit action $A^{\rm exploit}$). To compute the exploit action $A^{\rm exploit}_t=\pi_t(S_t)$, {\tt PearlAgent} enables interaction between the \policylearner\, and the \historysummarizationmodule\,, which outputs the state representation\footnote{We assume the history $H_t$ also  includes the observation $O_t$. Therefore, the state representation $S_t$ is a function of the history $H_{t}$.}.
% Importantly, {\tt PearlAgent} enables interaction between the policy $\pi_t$ and the representation learning module to determine the exploit action $A^{\rm exploit}_t=\pi_t(S_t)$, where $S_t$ is the state representation at time $t$ and is a function of the history\footnote{We assume the history at time $t$ includes the observation $O_t$.} $H_{t}$.
\pearlagent\, design enables the \safetymodule\, to interact with both the \policylearner\, and \explorationmodule\, and account for safety constraints (for example, to filter out undesirable subset of actions)\footnote{In this way, we implement what is typically referred to as ``state dependent action space'' in the literature.} when computing $A^{\rm explore}$ and $A^{\rm exploit}$ respectively.
% before sending the final action $A_t$ to the environment. 
The interaction tuple $\Ec_t$ is appended to the history $H_{t}$ and stored in the \replaybuffer.  

During a training round at time $t$, a batch of interaction histories are fetched from the \replaybuffer; \pearlagent\, then queries the \historysummarizationmodule\, to compute the corresponding state representations and generate a batch of history transitions $B_t = \{(H_k, H'_k)\}_{k=1}^{K}$, where $H'_k$ is the history one step after $H_k$. This batch of data tuples is used to update the \policylearner\,, accounting for safety and preference constraints specified by its \safetymodule. It is also used to update parameters of the \historysummarizationmodule.

For an offline learning setup, readers can imagine the environment to be a data set of interaction tuples and the exploration module to be inactive. Instead of querying for a new environment interaction tuple $\Ec_t$ by passing action $A_t$ to the environment, an offline \pearlagent would simply query for one of the interaction tuple already present in the offline data set. 

% See Fig. \ref{fig:agent_interface} for a visualization of the overall design. The {\tt PearlAgent} consists of 5 key modules to realize the above components. The agent starts by making an observation from the environment and summarizing the current observation alongside with the past history into a subjective state through the {\tt history summarization module}. To take an action, the agent first computes the action distribution or the value function from the available action space based on the core policy, which defines the agent's belief of a reward-maximizing strategy in the environment, and its latest subjective state as well as its risk preferences and environment constraints from the {\tt safety module}.  The agent then carries out exploration through the {\tt exploration module} on top of the action distribution or the value function to trade off between exploration and exploitation. 

% To optimize the agent's performance, the agent then stores the transition from the past history to the current history into the {\tt replay buffer}. The {\tt replay buffer} bootstraps and augments a sampled history transition batch and asks the {\tt history summarization module} to produce a subjective state transition batch. The agent then carries out policy optimization algorithms to optimize parameters in the core policy, the {\tt safety module} and the {\tt history summarization module}. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/agent_interface.png}
    \caption{Pearl Agent Interface}
    \label{fig:agent_interface}
\end{figure}

\subsubsection{Policy Learner}
In \pearl, any one of the \policylearner modules maintains the agent's current estimate of the optimal policy and updates it using a batch of interaction tuples. It also interacts with an exploration module, since many modes of exploration use uncertainty estimates of the return\footnote{We use the term ``return'' generically to refer to rewards for bandit settings and q-values for the mdp setting.} or action distribution in the case of stochastic policies. We do this by implementing the {\tt act} and {\tt learn-batch} method in for all types of policy learners in \pearl. For value function based policy learners and actor-critic methods, the {\tt learn-batch} method is used to update the corresponding value function based estimates. We list the different policy learners supported in \pearl.
\begin{enumerate}
    \item[\textbullet] (Contextual) Bandit learning algorithms: Typical contextual bandit algorithms relies heavily on \explorationmodule, see Section \ref{sec:exploration}. We support {\tt Linear Bandit Learning} and {\tt Neural Bandit Learning} alongside their corresponding \\
    \explorationmodule's as well as {\tt SquareCB} \citep{foster2020beyond}.
    \item[\textbullet] Value based methods: {\tt Deep Q-learning (DQN)} \citep{mnih2015human}, {\tt Double DQN} \citep{van2016deep}, {\tt Dueling DQN} \citep{wang2016dueling}, {\tt Deep SARSA} \citep{rummery1994line}. We also support {\tt Bootstrapped DQN} \citep{osband2016deep} alongside its corresponding \explorationmodule.
    \item[\textbullet] Actor-critic methods: {\tt soft actor-critic (SAC)} \citep{haarnoja2018soft}, {\tt Deep deterministic policy gradient (DDPG)} \citep{silver2014deterministic}, {\tt Twin-delayed deep determnistic policy gradient (TD3)} \citep{fujimoto2018addressing}, {\tt Proximal Policy Optimization (PPO)} \citep{schulman2017proximal}, and {\tt Policy Gradient (REINFORCE)} \citep{sutton1999policy}.
    \item[\textbullet] Offline methods: {\tt Conservative Q-learning} \citep{kumar2020conservative}, {\tt Implicit Q-learning} \citep{kostrikov2021offline}.
    \item[\textbullet] Distributional policy learners: {\tt Quantile Regression Deep Q-learning (QRDQN)} \citep{dabney2018distributional}.
\end{enumerate}
 

\subsubsection{Exploration Module}\label{sec:exploration}
The {\tt exploration} module complements policy learners by providing the agent with a \emph{exploration policy}, also known as a \emph{behavioral policy} in the context of RL. Pearl implements a set of common exploration modules:
\begin{enumerate}
    \item Uniform (take a uniformly random action from the available action space with some probability): {\tt $\epsilon$-Greedy} \citep{sutton2018reinforcement}.
    \item Softmax (take an action with probability proportional to a numerical value produced by the core policy): {\tt Boltzmann exploration} \citep{cesa2017boltzmann}. 
    \item Gaussian (Inject Gaussian noise around a neural network produced action): {\tt Gaussian noise injection} \citep{lillicrap2015continuous}.
    \item Representation (take an action considering the uncertainty of the representation for the current state-action pair): Linear Upper Confidence Bound ({\tt LinUCB}) \citep{li2010contextual}, Linear Thompson Sampling ({\tt LinTS}) \citep{agrawal2013thompson}, {\tt Neural LinUCB} \citep{xu2021neural}. 
    \item Parameter (take an action considering the uncertainty of its model parameters): {\tt Ensemble sampling} \citep{lu2017ensemble}. Note that this exploration strategy all so supports "deep exploration" proposed by \cite{osband2016deep}, which enables temporally consistent exploration by acting greedily with respect to an approximate posterior sample of the optimal value function.
\end{enumerate}

In existing implementations of RL and CB algorithms, it is typically the case that a policy learner exists with a fixed exploration strategy (e.g., DQN is usually paired with $\epsilon$-greedy exploration). However, Pearl’s modular design opens the door to the possibility of ``mixing-and-matching’’ policy learners with exploration modules. Our hope is that this modular design philosophy this can lead to more performant RL and CB solutions in practice, in addition to helping researchers quickly test new methodological ideas. 

% {\color{red}Daniel's draft: Exploration modules complement policy learners by providing the agent with a \emph{exploration policy}, also known as a \emph{behavioral policy} in the context of RL. Pearl implements a set of common exploration modules, including variations of random exploration, including $\epsilon$-greedy \citep{sutton2018reinforcement} and Gaussian noise injection \citep{lillicrap2015continuous}. Pearl also includes more sophisticated strategies for RL, like the so-called ``deep exploration’’ method proposed by \cite{osband2016deep}, which enables temporally consistent exploration by acting greedily with respect to an approximate posterior sample of the optimal value function. On the Contextual Bandits (CB) side, our initial set of exploration modules cover many popular methods: UCB, LinUCB, Thompson sampling, and the recent SquareCB \citep{foster2020beyond} approach. In existing implementations of RL and CB algorithms, it is typically the case that a policy learner exists with a fixed exploration strategy (e.g., DQN is usually paired with $\epsilon$-greedy exploration). However, Pearl’s modular design opens the door to the possibility of ``mixing-and-matching’’ policy learners with exploration modules. Our hope is that this modular design philosophy this can lead to more performant RL and CB solutions in practice, in addition to helping researchers quickly test new methodological ideas. }

\subsubsection{Safety Module}
The safety module in \pearl\, is currently designed to offer two main features.
\begin{enumerate}
    \item[\textbullet] A {\tt risk\char`_sensitive\char`_safety\char`_module}, which facilitates risk sensitive learning with distributional policy learners. Each {\tt risk\char`_sensitive\char`_safety\char`_module} implements a method to compute a value (or Q-value) function from a distribution over value functions under a different risk metric, and can conform to different risk preferences of an RL agent. 
    \item[\textbullet] A {\tt filter\char`_action} safety interface allows the agent designer to specify heuristics or environment constraints to only select state-dependent safe action spaces at each step.
\end{enumerate}
% We are currently working on implementing a preference based safety module which can account for user defined constraints during policy learning. We hope to include it in the first version of code release.

\subsubsection{History Summarization Module}
The \historysummarizationmodule\, implements two key functionalities in \pearl. First, it keeps track of the history at any environment interaction step and second, it summarizes the history into a state representation. 

\begin{enumerate}
    \item[\textbullet] During the environment interaction step, the \historysummarizationmodule\, appends a new interaction tuple, $\Ec_t$, to the current history $H_{t-1}$ to form $H_t$ and adds $(H_{t-1}, H_t)$ to the agent's replay buffer. It also updates the agent's state using the interaction tuple $\Ec_t$ and history $H_{t-1}$, which can be used by the \policylearner\, to compute an action at the next time step $t+1$.
    % When an agent makes a new observation from the environment, the {\tt history summarization module} incrementally updates the agent's subjective state. The {\tt history summarization module} provides the previous-step history $H_{t-1}$ and the current history $H_t$ to the agent, when the agent adds a transition to the replay buffer.
    \item[\textbullet] During the training pass,  a batch of interaction histories are sampled from the replay buffer. The \historysummarizationmodule\, computes the corresponding state representations and generates a batch of interaction tuples which is used by the \pearlagent\, to update other modules.
    % When an agent samples a history transition batch $(H_{i-1}, H_i)$ from the replay buffer, the {\tt history summarization module} is responsible for summarizing the history transition batch into a subjective state transition batch $(S_{i-1}, A_{i-1}, R_{i}, S_{i}, A_{i})$. Note that since the {\tt history summarization module} also carries neural network parameters, the policy learner also keeps track of its parameters and optimizes these parameters at the policy optimization step. 
\end{enumerate}
In our current implementation for \pearlagent's \historysummarizationmodule, we support both naive history stacking and Long-Short-Term-Memory (LSTM) \citep{hochreiter1997long} based history summarization. We aim to have a few transformer-based history summarization supported before our official release. 


\subsubsection{Replay Buffer}
The notion of replay buffer, a container for storing previously observed experiences, is central to RL as it enables \emph{experience replay}, the reuse of past experience to improve learning \citep{lin1992self}. In addition to sub-setting the most informative experiences, replay buffers allow for efficient data reuse by breaking the temporal correlations in sequential data.
% This was popularized in the context of deep RL by \cite{mnih2015human}, who discovered that experience replay leads to efficient data reuse, the ability to break temporal correlations from data that is collected online, and a behavioral distribution that is ``smoothed’’ across the learning process. 
The \replaybuffer\, module in \pearl\, implements several versions of a replay buffer.
\begin{enumerate}
    \item[\textbullet] {\tt FIFOOffPolicyReplayBuffer} is based on a first-in-first-out queue and stores interaction  tuples for the off-policy setting. For on-policy settings, we provide an extension in the form of {\tt FIFOOnPolicyReplayBuffer}\footnote{Although replay buffers are not typically used in the on-policy setting, we are able to unify off- and on-policy methods using this abstraction.}.
    \item[\textbullet] We also implement {\tt BootstrapReplayBuffer} \cite{osband2016deep} by adding a \emph{bootstrap mask} and {\tt HindsightExperienceReplayBuffer} using the idea of \emph{goal replacement} \cite{andrychowicz2017hindsight}
\end{enumerate}

% In Pearl, our basic version of a replay buffer, {\tt FIFOOffPolicyReplayBuffer}, is based on a first-in-first-out queue and stores experience tuples in the form of (state, action, reward, next state), targeted for the off-policy setting. We also provide an extension to the on-policy setting, using ``SARSA’’-style transitions (which additionally include the \emph{next action}) in the {\tt FIFOOnPolicyReplayBuffer}.\footnote{Although replay buffers are not typically used in the on-policy setting, we are able to unify off- and on-policy methods using this abstraction.} Surprisingly, slight changes to the vanilla replay buffer can unlock completely new capabilities, such as the \emph{bootstrap mask} proposed in \cite{osband2016deep} for Bootstrapped DQN or the idea of \emph{goal replacement} proposed in \cite{andrychowicz2017hindsight} for the Hindsight Experience Replay approach. These replay buffers are implemented in Pearl as {\tt BootstrapReplayBuffer} and {\tt HindsightExperienceReplayBuffer}, respectively.

\subsection{Agent Usage}
Figure \ref{fig:pearl-usage} illustrates a typical episodic environment interaction loop where an agent learns a policy for an environment with Deep Q-learning. Here, learning occurs at the end of each episode. The Pearl \texttt{Environment} class is based on the \texttt{step} method, which returns an \texttt{ActionResult} containing reward, next state, and whether the episode has been truncated, terminated, or done.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/pearl_usage.png}
        \caption{Typical \pearlagent Episodic Environment Interaction Interface}
        \label{fig:pearl-usage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/pearl_usage_hydra.png}
        \caption{A Hydra Configuration file for a \pearlagent}
        \label{fig:pearl-usage-hydra}
    \end{subfigure}
\end{figure}

The \texttt{PearlAgent} class accepts optional arguments for components such as history summarization module or safety module (with no-op components being the default). In our example, we specify a history summarization module that stacks the last three states and a safety module seeking to minimize variance. Likewise, policy learner classes accept an optional exploration module argument; in this example, we use an $\epsilon$-greedy exploration with $\epsilon=0.05$.

In practice, it is more convenient to specify agents and environments via Hydra \citep{Yadan2019Hydra} configuration files supported by \textbf{Pearl}, which provides a convenient way of running experiments and hyperparameter tuning. A Hydra file generating the same agent as above is shown in Figure \ref{fig:pearl-usage-hydra}.



\section{Comparison to Existing Libraries}
\label{sec: lit_review}
% (Yonathan) Maybe let's add TorchRL as well. {\color{blue} Yon: I added it in the discussion below. the thought was that the main goal of its is to benchmark RL algorithms, versus deploying them (from that perspective it will add somewhat redundant info to the table)}

To illustrate the differences between the Pearl agent with other existing RL libraries, we selected few RL libraries and compared their supported capabilities, through the lens of the Pearl agent capabilities, in Table~\ref{tab:comparison}. We compared to the libraries RLLib~\citep{liang2018rllib}, StableBaselines3~\citep{stable-baselines3}, Tianshou~\citep{tianshou} and CleanRL~\citep{huang2022cleanrl} as our comparison targets. These are currently, to the best of our knowledge, the RL libraries with highest adoptions at the moment. Besides of these libraries, we mention additional ones, that are substantially different than Pearl at the end of this section.

As Table~\ref{tab:comparison} highlights, the Pearl agent has several capabilities that are crucial for production deployment, such as ability to perform structured exploration, support in offline RL, and in safety mitigations. Another important feature the Pearl agent has, which is missing in existing libraries, is the support in dynamic action space: a situation in which the action space may change over time or as a function of the state. Further, Pearl, unlike alternative libraries, explicitly supports in CB algorithms. These are often used in practice, since their relative simplicity.

Besides of the aforementioned key aspects, Table~\ref{tab:comparison} puts forward the main difference between the Pearl agent and other libraries: \emph{the Pearl agent explicitly supports modularity}, namely, simple and independent modifications of different algorithmic components of an agent. This gives flexibility and freedom to algorithm designers to mix different requirements in a simple fashion. 


\begin{table}[t]
\caption{Comparison of Pearl agent to alternative popular RL libraries}
\centering
\begin{tabular}{c|c|c|c|c|c|c}
    Features & RLLib & SB3 & Tianshou & CleanRL & Pearl\\
    \hline
    Modularity & \xmark & \xmark & \xmark &\xmark & \cmark\\
    Online Exploration  &\cmark &  \xmark & \cmark & \xmark & \cmark\\
    Safety & \xmark & \xmark  & \cmark & \cmark  & \cmark\\
    History Summarization &  \xmark & \xmark & \xmark & \xmark  &\cmark \\
    Data Augmented Replay Buffer &\cmark & \cmark &  \cmark &\cmark & \cmark\\
    % System Capabilities (GPU, Infra, Environment...) ? & & & & \\
    % Evaluation? & & & & \\
    Contextual Bandit  &\cmark & \xmark & \xmark & \xmark &\cmark \\
    Offline RL & \cmark & \cmark & \cmark & \xmark  & \cmark\\
    Dynamic Action Space &\xmark & \xmark & \xmark &\xmark & \cmark \\
\end{tabular}
\label{tab:comparison}
\end{table}

We now describe additional libraries that are being used by the research community and elaborate on the differences. The d3RLpy library was designed to support offline RL in a simple and scalable way. There the authors implemented many offline RL algorithms for easy application. However, d3RLpy is mostly concerned with offline RL, nor Contextual Bandits, and is not modular like the Pearl agent. 

The main motivation, to the best of our knowledge, of existing RL libraries is to facilitate the benchmarking of different RL algorithms. Indeed, RLLib, SB3, Tianshou, CleanRL all support this goal. Besides of these, additional libraries such as MushroomRL~\citep{d2021mushroomrl} and TorchRL~\citep{bou2023torchrl} also offer alternative implementation of popular algorithms. These also do not support modular implementation of algorithms, and are mostly concerned on benchmarking existing algorithms.

Lastly, the Vowpal Wabbit library~\citep{agarwal2014reliable} offers rich and diverse set of Contextual Bandit algorithms, tested on multiple domains and environments. However, to the best of our knowledge, it does not explicitly have PyTorch support which the Pearl agent has.


\section{Benchmark}
\label{sec: benchmarking} 

\subsection{Reinforcement Learning Benchmarks}
As a sanity check, we bench-marked a \pearlagent\, with three different actor-critic algorithms on continuous control tasks in Mujoco. The results are shown in Figure \ref{fig:continuous_control_benchmarks} below. We tested soft-actor critic (SAC), discrete deterministic policy gradients (DDPG) and twin delayed deep deterministic policy gradients (TD3), for a set of commonly used hyperparameters, without tuning them. Figure \ref{fig:continuous_control_benchmarks} plots training returns for five seeds with +/-1 standard error. For simplicity, we only plotted for a fixed number of episodes (instead of showing the number of gradient steps on the x-axis) and did not plot the separate evaluations runs (typically done after every few training runs). These results are only meant to serve as a sanity check as the motivation for \pearl\, goes much farther beyond reproducible research - we only checked for stable, consistent learning of our implementations rather than searching for the best training runs with optimal hyperparameter choices. We are currently bench-marking other \policylearner\, implementations in \pearl\, on Atari benchmarks and plan to include those as well at official release. 

\begin{figure}[!htb]
\minipage{0.25\textwidth}
    \includegraphics[width=\linewidth]{figures/PearlTD3_figs/PearlTD3_HalfCheetah.png}
\endminipage\hfill
\minipage{0.25\textwidth}
    \includegraphics[width=\linewidth]{figures/PearlTD3_figs/PearlTD3_Ant.png}
\endminipage\hfill
\minipage{0.25\textwidth}%
    \includegraphics[width=\linewidth]{figures/PearlTD3_figs/PearlTD3_Hopper.png}
\endminipage
\minipage{0.25\textwidth}%
    \includegraphics[width=\linewidth]{figures/PearlTD3_figs/PearlTD3_Walker2d.png}
\endminipage

\minipage{0.25\textwidth}
    \includegraphics[width=\linewidth]{figures/PearlDDPG_figs/PearlDDPG_HalfCheetah.png}
\endminipage\hfill
\minipage{0.25\textwidth}
    \includegraphics[width=\linewidth]{figures/PearlDDPG_figs/PearlDDPG_Ant.png}
\endminipage\hfill
\minipage{0.25\textwidth}%
    \includegraphics[width=\linewidth]{figures/PearlDDPG_figs/PearlDDPG_Hopper.png}
\endminipage
\minipage{0.25\textwidth}%
    \includegraphics[width=\linewidth]{figures/PearlDDPG_figs/PearlDDPG_Walker2d.png}
\endminipage

\minipage{0.25\textwidth}
    \includegraphics[width=\linewidth]{figures/PearlSAC/PearlSAC_HalfCheetah.png}
\endminipage\hfill
\minipage{0.25\textwidth}
    \includegraphics[width=\linewidth]{figures/PearlSAC/PearlSAC_Ant.png}
\endminipage\hfill
\minipage{0.25\textwidth}%
    \includegraphics[width=\linewidth]{figures/PearlSAC/PearlSAC_Hopper.png}
\endminipage
\minipage{0.25\textwidth}%
    \includegraphics[width=\linewidth]{figures/PearlSAC/PearlSAC_Walker2d.png}
\endminipage
\caption{Training returns of soft-actor critic (SAC), discrete deterministic policy gradients (DDPG) and twin delayed deep deterministic policy gradients (TD3) algorithms on four popular continuous control tasks in Mujoco.}
\label{fig:continuous_control_benchmarks}
\end{figure}

\subsection{Neural Contextual Bandits Benchmarks}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}

        \includegraphics[width=\textwidth]{figures/letter.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/yeast.png}
    \end{subfigure}
        \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/satimage.png}
    \end{subfigure}
        \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/pendigits.png}
    \end{subfigure}
    \caption{Performance of neural implementations in Pearl of LinUCB, TS and SquareCB and an offline baseline that is considered near optimal. The UCI dataset used in each experiment is provided at the title of the figure.}
    \label{fig:cb_benchmarks}
\end{figure}

We implemented and tested the performance of neural adaptations of common CB algorithms. Our benchmarks consists of datasets from the UCI repository~\citep{asuncion2007uci}, adapted to CB interaction model. The results are depicted in Figure~\ref{fig:cb_benchmarks}. Using supervised learning datasets for testing CB algorithms is common in past literature~\citep{dudik2011doubly,foster2018practical,bietti2021contextual} and allows to test different implementations on real world datasets. We tested neural implementations of the LinUCB, Thompson Sampling (TS), and SquareCB algorithms~\citep{li2010contextual,agrawal2013thompson,foster2020beyond}. That is, we tested deep learning based implementations of these CB algorithms. This showcase the simplicity of combining deep learning architectures within CB algorithms in Pearl due to its PyTorch support.

The CB benchmark environment is designed as follows. We assume access to an offline dataset $\{(x_i,y_i)\}_{i}$, where for every $i$, $x_i\in \mathbb{R}^d$ is a feature vector, and $y_i \in \mathcal{Y}$ is a label from a finite alphabet. At each time step an agent observes a feature vector $x_t$ and is required to choose an action, $a_t \in \mathcal{Y}$, which is an element of the alphabet of possible labels. The reward model is $r_t = 1\{ a_t = y_t \} +\xi $ where $\xi\sim \mathcal{N}(0,\sigma_\xi)$. This type of environments has an explicit exploration challenge: if an agent does not explore correctly it may never receive information on the correct label. 

We used a two-layer neural architecture as the reward function approximation in all algorithms we experiment with. The reward function network receives as an input the feature vector and an action $(x,a)$, and returns real value number. The reward model was optimized via PyTorch, by iteratively taking gradients on the standard MSE loss using an Adam optimizer. Beside of the neural versions of LinUCB, TS, and SquareCB we implemented an additional baseline offline approach. For the offline baseline an agent gathers data with a fixed exploratory behavior policy.  Then we trained a reward model on this offline dataset and tested its online performance by following the greedy policy with respect to the learned reward model. The  architecture of the network and other implementations details of the offline baseline are the same as for the CB algorithms. For additional implementation details see Appendix~\ref{app:cb_benchmarks}.







\subsection{Agent Versatility Benchmark}
This section provides an initial assessment of the Pearl agent's three primary abilities -- summarizing the history to handle partial observable environments, exploring effectively to achieve rewards even when they are sparse, and learning risk-averse policies.

\subsubsection{History Summarization to Deal with Partial Observability}
\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Partial Observability CartPole.png}
    \end{subfigure}
    \caption{Learning curves of DQN with and without the LSTM history summarization module in the partial observable version of CartPole.}
    \label{fig:classic control partial observable results}
\end{figure}
To test the Pearl agent's ability to handle partial observability, we adapted CartPole, a fully observable environment, to a partial observable variant. The objective for the agent is to manipulate a cart by exerting appropriate amount of forces so that the attached pole remains upright. Our code is adapted from \texttt{https://github.com/openai/gym}.

% The same kind of partial observability is added to the four original, fully observable environments, resulting in four partial observable environments. 
In the original CartPole environment, the agent can perceive the position and velocity of the controlled cart as well as the angle and anglular velocity of the attached pole. In our partial observable variant, only the cart position and the angle of the pole are observable. Consequently, the agent must use both current and past observations to deduce the system's velocity, which is crucial for selecting the optimal action. To further increase the degree of partial observability, we have the new environment emit its observation every $3$ steps and emit an all-zero vector for the rest of time steps. To choose the best action, agent must retain memory of system's previous two positions and the sequence of actions since the older position was observed. The optimal return in both original and our new CartPole environment is $200$.

We tested Pearl's LSTM history summarization module to see if it can handle the partial observability challenge presented in the above environment. The base algorithm was the DQN algorithm \citep{mnih2015human}. To approximate action values, the DQN algorithm used a fully connected neural network with two hidden layers, each of which has $64$ neurons. The neural network takes both an agent's subjective state, which is just the observation without history summarization module, and an action as input. The output is a scalar estimation of the corresponding action value. All activation functions are Relu. The behavior policy was an $\epsilon$-greedy policy with $\epsilon=0.1$. The replay buffer size was $10,000$. The target network was updated every $10$ episodes. The LSTM history summarization module takes the current observation and past $4$ observations and actions as input and produces the agent's subjective state, an $128$-dim vector as output. The LSTM has two layers, each of which has $128$ hidden units. 

Each experiment has $5$ runs, each of which consists of $300$ episodes. We plotted the mean and the standard error of the achieved returns in Figure \ref{fig:classic control partial observable results}. This figure shows that 1) without the history summarization module, the agent didn't achieve any progress of learning, 2) with the history summarization module, the agent achieves a significantly better performance, 3) even with the history summarization module, agent still didn't achieve the optimal return, we believe that more parameter tuning is required because of the difficulty of this environment.

\subsubsection{Effective Exploration to Tackle the Sparse Reward Challenge}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/DeepSea 5.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/DeepSea 10.png}
    \end{subfigure}
    % \begin{subfigure}[t]{0.45\textwidth}
    %     \includegraphics[width=\textwidth]{figures/Sparse Reward Acrobot.png}
    % \end{subfigure}
    \caption{Learning curves of DQN and Bootstrapped DQN in Deep Sea (size $5$ by $5$ and $10$ by $10$). }
    \label{fig:classic control sparse reward results}
\end{figure}

To test the agent's ability to handle exploration, we implemented the DeepSea environment \citep{osband2019deep}, which is known for its challenge of exploration. The DeepSea environment has $n \times n$ states and is fully deterministic. There is a reward of $1$ in the environment, which is the desired reward. But achieving this reward requires taking a sequence of actions that result in small negative rewards. Any incorrect action in the sequence results in failure of achieve the desired reward. All other rewards are $0$. Therefore, rewards in the environment are highly sparse. A random policy reaches the desired reward with probability $2^{-n}$ and while the remaining probability results in a return of $0$. Therefore, by choosing the value of $n$, we can obtain environments with different levels of exploration difficulty. Our experiments used a small DeepSea $n = 5$ and a large one $n = 10$.

We tested Pearl's implementation of the Bootstrapped DQN algorithm, which is a strong exploration algorithm introduced by \citet{osband2016deep}. The DQN algorithm was used as the baseline again. Hyper parameters of the DQN algorithm were the same as those used in the partial observability experiment. The Bootstrapped DQN algorithm used an ensamble of $10$ small neural networks to approximate the distribution of action values. With $n=5$, each element in the ensamble is a fully connected network with two hidden layers, each of which has $14$ neurons. This number becomes to $9$ with $n=10$. We made such design choices so that the overall number of parameters in both algorithms are roughly the same. Other choices of hyper parameters were the same as the DQN algorithm.

Each experiment has $5$ runs, each of which consists of $200$ episodes and $400$ episodes in the small and large DeepSea environments, respectively. Again, we plotted the mean and standard error of the achieved return. Figure \ref{fig:classic control sparse reward results} shows the learning curves of the two tested algorithms. It can be seen that in both small and large DeepSea environment, Bootstrapped DQN achieved a significantly better performance than DQN. This shows that Bootstrapped DQN can perform much better exploration in sparse reward environments.

% We also modified the above classic reinforcement learning test environments, which have non-zero reward for almost all state-action pairs, to versions with reward only for a small region of the state space.

% In both MountainCar and Acrobot the goal is to control the system to reach some target as soon as possible. Each episode terminates when the target is reached. The original reward is $-1$ for every time step during each episode. Setting all rewards to be negative encourages the agent to reach the target as early as possible. The sparse reward version of these two systems has a reward of one only when the target of the controlled system is reached, and for all other state-action pairs the rewards are zero. With such a reward setting and a discount factor that is less than one, the agent's best policy is still to reach the target as early as possible. 

% In Pendulum, the goal is to apply torque on the free end of the pendulum to swing it into an upright position and maintain it there as long as possible. The original rewards were shaped in a way such that the closer the pendulum is to the upright position, the higher reward the agent receives. The new sparse reward version of Pendulum only emits a reward of one when the Pendulum is close to the upright position (specifically, when the angle is less than 15 degrees).

% We have not designed a sparse reward version for CartPole because exploration is not a challenge in this environment. In CartPole, the agent's goal is to maintain the system in an unstable state, which is also the start state. Whenever the agent is a bit far from this state the episode restarts. Therefore the agent does not need to learn how to get back to the unstable desired state from a distant state.
% adding an augmented dimension to the observation vector, and set the corresponding element to \texttt{True} only if the system's position 

\subsubsection{Learning Risk-Averse Policies}

We also designed a simple environment called \emph{Stochastic Bandit} to test if the Pearl agent can learn policies that fulfills various degrees of safety needs, by balancing the expectation and the variance of the return. StochMDP only has one state and two actions. The reward of each of the two actions follows a Gaussian distribution. The reward distribution for Action 0 has a mean of $8$ and a variance of $1$. For Action $1$, the mean is $10$ and the variance is $9$. With the classic reinforcement learning formulation, the goal is to maximize the expected return. Therefore the optimal policy is to always choose Action 1. When the agent wants to maximize the mean of the return while minimizing the variance, it chooses a weight scalar $\beta$ that balances these two terms. Depending on the weight scalar, the optimal policy either always chooses Action 0 or always chooses Action 1. The threshold value for the weight scalar is $0.25$ because $8 - 0.25 \times 1 = 10 - 0.25 \times 9$. While this environment is simple, it can serve as a sanity-check to see whether the test algorithm indeed balance mean and variance as predicted. 


We tested our implementation of the QR-DQN algorithm \citep{dabney2018distributional}, which is an algorithm that learns the distribution of the return. Using the learned distribution, the algorithm can estimate the mean and the variance of the return, and further maximize the mean while minimizing the variance.


As usual, each experiment has $5$ runs, each of which consists of $300$ episodes. This time, for each episode, we collected the greedy action given the current learned action values. We then plotted the mean and standard error of actions across different runs, with Action $0$'s value being $0$ and Action $1$'s value being $1$. Our experiment result \ref{fig:classic control safety results} shows that with large $\beta$, the algorithm prefers the lower variance action, even if this action gave a lower expected return. This shows that the algorithm has the ability of learning risk-averse policies.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/Acrobot.png}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/Pendulum.png}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/CartPole.png}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/MountainCar.png}
%     \end{subfigure}
%     \caption{Benchmark results in four classic control environments using DQN and DDPQ. The tested agent and environment are the same as the one used by Rodrigo's \texttt{benchmark.py}. All current results only involve a single run. I will do multiple runs for selective environments. And if we need, we may add benchmark results generated by other algos.}
%     \label{fig:classic control results}
% \end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/Stochastic Bandit.png}
    \end{subfigure}
    \caption{Greedy action of the QRDQN algorithm with different choices of its $\beta$ parameter in the stochastic bandit environment. It can be seen that, after training, the agent preferred the lower variance action (Action $0$) when $\beta$ was high and preferred the higher variance action (Action $1$) when $\beta$ was low. Note that, before episode $128$, the agent simply collected data and did not perform any learning. This figure shows that one can manage risk of the learned policy by choose a parameter that balances the expectation of the policy's return and its variance.}
    \label{fig:classic control safety results}
\end{figure}
 
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/Long Horizon Acrobot.png}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/Long Horizon Pendulum.png}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/Long Horizon CartPole.png}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{figures/Long Horizon MountainCar.png}
%     \end{subfigure}
%     \caption{Benchmark results in the long horizon version of four classic control environments using DQN and DDPQ. The episode lengths are $5$ times of their original lengths. For Acrobot it is $2500$ steps. For the other three, they are $1000$ steps. For all environment, a $5x$ finer decision granularity was used. For Long Horizon Acrobot, for every 0.004 seconds, an new force is chosen and applied until the next decision point (this number is 0.02 in Acrobot). Similarly, the Long Horizon Pendulum makes a decision every $0.001$ seconds. The acrobot and mountain car}
%     \label{fig:classic control results}
% \end{figure}

\section{Example Industry Product Adoptions}

\begin{table}[h]
\caption{Pearl Agent Satisfies Requirements of Real-World Applications}
\centering
\begin{tabular}{c|c|c|c}
    Pearl Features & Auction RecSys & Ads Auction Bidding & Creative Selection\\
    \hline
    Policy Learning &\cmark & \cmark & \cmark \\
    Online Exploration & & \cmark & \cmark \\
    Safety &  & \cmark & \\
    History Summarization &  & \cmark &  \\
    Replay Buffer &\cmark & \cmark & \cmark \\
    Contextual Bandit  & &  & \cmark \\
    Offline RL & \cmark & \cmark &  \\
    Dynamic Action Space &\cmark &  & \cmark\\
    Large-Scale Neural Network &\cmark &  &  \\
\end{tabular}
\label{tab:industry}
\end{table}

We present three industry product adoptions of Pearl as demonstration of Pearl's capability of serving production usage. As an overview, we present how Pearl supports these product requirements in Table \ref{tab:industry}. 

\subsection{Auction-Based Recommender System (Auction RecSys)}
Optimizing for long-term value in auction-based recommender systems using reinforcement learning presents a significant challenge. This is because it necessitates the integration of a reinforcement learning (RL) agent with a mechanism rooted in supervised learning. In the study by \cite{xu2023optimizing}, an on-policy RL solution for auction-based recommendations was introduced, which incorporated Pearl during its recent production implementation. Given that the recommender system is heavily influenced by the system's auction policy, the RL agent must undergo on-policy learning offline. This ensures that the newly derived policy, when combined with the auction policy, proveably outperforms the current production system in terms of long-term rewards. As it pertains to recommender systems, a unique set of recommendations is available to users at each step. This necessitates the RL agent's capability to handle a dynamic action space. Additionally, in this implementation, large-scale neural networks were integrated with Pearl to offer accurate predictions of value functions for intricate user-recommendation interactions.

\subsection{Ads Auction Bidding}
Real-time bidding in advertising is recognized as a sequential decision-making problem. This is because a bidding agent must efficiently allocate an advertiser-defined budget over a specific duration to maximize the advertiser's conversions. In the study by \cite{korenkevych2023offline}, the focus is on enhancing the bidding agent's performance amidst the constantly evolving auction market. An RL bidding agent is tasked with prudently learning an offline policy that ensures neither over-expenditure nor under-utilization of the predetermined budget within the set timeframe. Given that the data collected in production is driven by a deterministic policy, the agent needs to engage in limited exploration to gather more insightful data via online exploration. Moreover, due to the inherent volatility of auction markets, which are often only partially observable, the agent is expected to make decisions based on summarized representations of its entire interaction history.

\subsection{Creative Selection}
Beyond sequential decision making problems, contextual bandit problems are also prevalent in industry applications. In one of our internal applications, creative selection for content presentation, where each piece of content has dozens of different available creatives, we adopt the neural contextual bandit solution from Pearl to carry out efficient online exploration in learning users' preferences in creatives with minimal number of interactions. Since each content has a different set of available creatives for adoption, the agent is required to support dynamic action space in this problem. 

\section{Conclusion}
The realm of reinforcement learning has witnessed remarkable advancements, yet the practical implementation of RL agents in real-world scenarios remains a challenge. The introduction of \textbf{Pearl} marks a significant stride towards bridging this gap, offering a comprehensive, production-ready solution that addresses the multifaceted challenges inherent in RL. By encompassing features like intelligent exploration, safe RL, history summarization, dynamic action space and support for both online and offline policy optimization, \textbf{Pearl} stands out as a versatile tool tailored for diverse real-world applications. As we gear up for its release in November 2023, we are optimistic about its potential to revolutionize the way RL agents are designed and implemented. We believe that \textbf{Pearl} will serve as a valuable resource for the broader adoption of RL in real-world applications, fostering innovation and driving forward the boundaries of what's possible in the field.

% \acks{}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\vskip 0.2in
\bibliography{bibliography}

\appendix

\section{Details on CB implementation}\label{app:cb_benchmarks}

\paragraph{General implementation details.}
Table~\ref{tab:details} depicts implementation details corresponding to all benchmarks and algorithms. For the \texttt{letter}, \texttt{satimage},  \texttt{pendigits} that are of higher dimension we used an MLP architecture of $[64,16]$ hidden layers. For the  \texttt{yeast} dataset we chose an architecture of $[32,16]$ hidden layers. 

Since the action space of some of these datasets is not small, we chose a binary encoding to the actions. This binary encoding was concatenated to the feature vector. Hence, the input vector of the network has the form $(x,a)$ where $a$ is a binary representation of an element in $|\mathcal{Y}|$, where $\mathcal{Y}$ is the alphabet of possible labels.

The behavior policy with which we gathered data for the offline benchmark was chosen as follows: the behavior policy chooses with probability $1/4$ the correct label, and with probability $3/4$ any label. That allowed to create a balanced dataset, in which the ratio between choosing the correct and incorrect label is small.

The plots presented in Table~\ref{tab:comparison} represent the average performance across 5 runs. The confidence intervals represent the standard error.

\paragraph{LinUCB and TS implementation.} Our neural adaptions of LinUCB and TS are based on calculating a bonus term $ \| \phi_t(x,a)\|_{A_t^{-1/2}}$ where $\phi_t(x,a)$ is the last layer feature representation of of $(x,a)$ and $A_t= I + \sum_{n=1}^{t-1} \phi_t(x,a)\phi_t(x,a)^T$. For LinUCB we explicitly add this term~\citep{li2010contextual} after scaling by $0.25$, which improved the performance. For the neural version of TS we sampled a reward from a Gaussian distribution with the variance term $\phi_t(x,a)\|_{A_t^{-1/2}}$ and expected reward as calculated by the neural reward model.

\paragraph{SquareCB implementation.} We followed the exact same action decision rule of SquareCB \citep{foster2020beyond}. We chose the scaling parameter $\gamma = 10 \sqrt{dT}$ where $d$ is the dimension of the feature-action input vector. We note that setting $\gamma \propto \sqrt{dT}$ was indicated in~\citet{foster2020beyond} for the linear Bandit problem. We scaled this value by 10 since we empirically observed of an improved performance in our ablation study.




\begin{table}[t]
\caption{Implementation Details of the CB algorithms}
\centering
\begin{tabular}{| c|c|}
    \hline
   Architecture & 2 layer MLP\\
    \hline
     Optimizer & Adam \\
    \hline
     Learning rate & 0.01\\
    \hline     
     Batch size & 128\\
    \hline
    Buffer sizer &  \# of time steps\\
    \hline  
    Action encoding &  Binary encoding\\
    \hline  
    Reward variance &  $\sigma_\xi = 0.05$\\
    \hline  
\end{tabular}
\label{tab:details}
\end{table}

\end{document}