\section{Related Work}
\subsection{Gradient Weights in Multi-task \& Meta Learning}
Our work is closely related to optimization methods from the multi-task learning (MTL) and meta learning literature, particularly those that integrate weights into the task gradients or losses. The \emph{multiple gradient descent algorithm} (MGDA) approach of ____ is one of the first works along this direction. It proposes an optimization objective that gives rise to a weight vector that implies a descent direction for all tasks and converges to a point on the Pareto set. 
% MGDA was introduced into the deep MTL setting in ____, which propose extensions to MGDA weight calculation that can be more efficiently solved. 
The PCGrad paper ____ identified that conflicting gradients can be detrimental to MTL. The authors then propose to modify the gradients to remove this conflict (by projecting each task's gradient to the normal plane of another task), forming the basis for the PCGrad algorithm. Another work that tackles conflicting gradients is the \emph{conflict-averse gradient descent} (CAGrad) method of ____. CAGrad generalizes MGDA: its main idea is to minimize a notion of ``conflict'' between gradients from different tasks, while staying nearby the gradient of the average loss. Notably, CAGrad maintains convergence toward a minimum of the average loss. Another way to handle gradient conflicts is the Nash-MTL method of ____, where the gradients are combined using a bargaining game. Very recently, ____ introduced a Bayesian approach for gradient aggregation by incorporating uncertainty in gradient dimensions. Other optimization techniques for MTL include tuning gradient magnitudes so that all tasks train at a similar rate ____, taking the geometric mean of task losses ____, and random weighting ____. On the meta learning front, the MAML algorithm ____ aims to learn a useful representation such that the model can adapt to new tasks with only a small number of training samples. Since fast adaptation is the primary goal in meta learning, MAML's loss calculation differs from those found in MTL. 

Few prior works provided provable convergence guarantees of the different existing multi-objective optimization methods. Without additional assumption on the alignment of different objectives, these guarantees quantified convergence to a point on the Pareto front. Unlike our work, there the convergence guarantees depended on worst-case structural quantities such as the maximal Lipschitz constant among all objectives____ or the maximal generalized smoothness____.


The algorithms introduced in the following are similar to existing ones in that they construct a weighted loss to combine information from different sources of feedback. Unlike previous work, we focus on exploiting the prior knowledge that the objectives are \emph{aligned}.  We introduce new instance-dependent structural quantities that reflect how aligned multi-objective feedback can improve GD performance, improving convergence that depends on worst-case structural quantities, as in prior works. 
% theoretically and empirically that such knowledge can be beneficial for optimization and results with improved convergence.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/amoo_image.png}
    \caption{ Visualization of AMOO instances in which it is possible to obtain improved convergence compared to optimizing individual functions or the average function: \textbf{(left)} the specification example, \textbf{(center)} simpler instance of the selection example, and \textbf{(right)} 3D example of the local curvature example, in which $f_1(x_1,x_2)=\exp(x_1)+\exp(x_2)-x_1-x_2$ and $f_2(x_1,x_2)=f_1(-x_1,-x_2)$. This example highlights the need to toggle between functions according to their local curvature.}
    \label{fig:amoo_examples}
\end{figure*}



\subsection{Proxy \& Multi-fidelity Feedback}
Other streams of related work are (1) machine learning using proxies and (2) multi-fidelity optimization. These works stand out from MTL in that they both focus on using \emph{closely related} objectives, while traditional MTL typically considers a set of tasks that are more varied in nature. Proxy-based machine learning attempts to approximate the solution of a primary ``gold'' task (for which data is expensive or sparsely available) by making use of a proxy task where data is more abundant ____. Similarly, multi-fidelity optimization makes use of data sources of varying levels of accuracy (and potentially lower computational cost) to optimize a target objective ____. In particular, the idea of using multiple closely-related tasks of varying levels of fidelity has seen adoption in settings where function evaluations are expensive, including bandits ____, Bayesian optimization ____, and active learning ____. The motivations behind the AMOO setting are clearly similar to those of proxy optimization and multi-fidelity optimization. However, our papers takes a pure optimization and gradient-descent perspective, which to our knowledge, is novel in the literature.


% Sim-to-real learning can be thought of as a particular instance of multi-fidelity optimization, where one hopes to learn real world behavior via simulations (typically in robotics) ____. In many of these papers, however, the objectives are queried one at a time, differing from MTL or AMOO.




% \subsection{Other Forms of Multi-task Learning}


% \begin{itemize}
%     % \item Multi Task papers for networks: practice (these show convergence to Pareto point)
%     \item Multi task papers: improvements in sample complexity and not optimization ____
%     % \item Learning with Proxies papers.
% \end{itemize}


% \begin{itemize}
%     \item In many settings we have multi objective feedback with improves the performance across tasks (add citations)
%     \item Implies that in many cases there may not be significant tradeoffs between tasks or objectives.
%     \item Previous works in multi-task setting proved convergence to Pareto front, which implies there is a performance tradeoff between tasks.
%     \item Question: How can we benefit from multi-objective feedback when the objectives are aligned? Prior works focused on this question from sample complexity perspective____, but there is no work that studies this question from optimization perspective.
    
%     \item We introduce the Aligned Multi Objective Optimization (AMOO) framework to study this question. We design new algorithms with provable guarantees and show how it can be scaled and be used in networks.
% \end{itemize}

%