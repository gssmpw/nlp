% \documentclass{uai2025} % for initial submission
\documentclass[accepted]{uai2025} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
% \usepackage{mathtools} % amsmath with fixes and additions
% % \usepackage{siunitx} % for proper typesetting of numbers and units
% \usepackage{booktabs} % commands to create good-looking tables
% \usepackage{tikz} % nice language for creating drawings and diagrams

% %% Provided macros
% % \smaller: Because the class footnote size is essentially LaTeX's \small,
% %           redefining \footnotesize, we provide the original \footnotesize
% %           using this macro.
% %           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

% \usepackage{hyperref}
% \usepackage{url}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{color}
\usepackage{xcolor}
% \hypersetup{linkcolor={blue},citecolor={blue}}

\usepackage{qiangstyle}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb,bbm}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{cancel}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\newtheorem{proposition}{Proposition}
% \usepackage[linesnumbered,ruled]{algorithm2e}
% \DontPrintSemicolon	
\usepackage{natbib}
% \usepackage[sectionbib,square]{natbib}  % Uncomment if needed

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{diagbox}
\usepackage{booktabs}
\usepackage{float}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption,subcaption}
% \usepackage{sidecap}
% \sidecaptionvpos{figure}{c}
\usepackage{epsf}
\usepackage{psfrag}
\usepackage{tikz}

\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{pbox}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}



\newcommand{\vect}{\text{vec}}


\newcommand{\bo}[1]{\textcolor{cyan}{[Bo: #1]}}
\newcommand{\son}[1]{\textcolor{blue}{[Son: #1]}}

\title{Improving Adaptive Moment Optimization via Preconditioner Diagonalization}

% The standard author block has changed for UAI 2025 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{Son Nguyen}
\author[1]{Bo Liu}
\author[1]{Lizhang Chen}
\author[1]{Qiang Liu}
% Add affiliations after the authors
\affil[1]{
    The University of Texas at Austin}
\affil[ ]{\texttt{\{sonnv77, bliu, lzchen, lqiang\}@utexas.edu}}

  
\begin{document}
\maketitle

\begin{abstract}
% Modern adaptive optimization methods, such as Adam and its variants, have emerged as the most widely used tools in deep learning over recent years. These algorithms offer automatic mechanisms for dynamically adjusting the update step based on estimates of gradient statistics. Compared to traditional algorithms like Stochastic Gradient Descent, these adaptive methods are typically more robust to model scale and hyperparameter tuning. 
Modern deep learning heavily relies on adaptive optimization methods like Adam and its variants, celebrated for their robustness against model scale and ease of hyperparameter tuning.
However, the gradient statistics employed by these methods often do not leverage sufficient gradient covariance information, leading to suboptimal updates in certain directions of the parameter space and potentially slower convergence. In this work, we keep track of such covariance statistics in the form of a structured preconditioner matrix. Unlike other works, our approach does not apply direct approximations to estimate this matrix. We instead \emph{implement an invertible transformation that maps the preconditioner matrix into a new space where it becomes approximately diagonal}. This enables a diagonal approximation of the preconditioner matrix in the transformed space, offering several computational advantages. Empirical results show that our approach can substantially enhance the convergence speed of modern adaptive optimizers. Notably, for large language models like LLaMA, we can achieve a \textbf{2x} speedup in sample efficiency compared to Adam. In addition, our method can also be integrated with memory-efficient optimizers to manage computational overhead.
\end{abstract}

\section{Introduction}\label{sec:intro}
In the realm of deep learning optimization, finding efficient and reliable solutions to complex problems has become a central challenge. As model scales and datasets continue to expand, such optimization problems usually demand extensive training time and substantial computational resources to achieve state-of-the-art performances.


Standard first-order methods, such as stochastic gradient descent (SGD) and its variants, have emerged as canonical tools for training large-scale deep networks. These methods are straightforward to implement using modern automatic differentiation frameworks and are easily adaptable to non-conventional training setups~\citep{konevcny2016federated, li2020federated, finn2017model}. However, despite their strong theoretical grounding~\citep{csimcsekli2019heavy, zhou2020towards, smith2021origin, tian2023recent}, first-order methods typically require meticulous tuning of hyperparameters to ensure the optimization process can converge to the desired local optima~\citep{zhang2022adam}. In practice, these methods often struggle when navigating highly non-convex loss surfaces, a common characteristic of deep learning models. Pathological features like saddle points, flat regions, and sharp valleys in the loss landscape can significantly hinder convergence, leading to inefficient training. 

To tackle these challenges, optimization techniques have evolved to incorporate curvature geometry or second-order information, providing more adaptive and efficient updates. A classic family of such algorithms is preconditioned gradient methods, in which the gradient is premultiplied by a matrix called a preconditioner before each optimization step. Classic algorithms in this family include Newton methods~\citep{bonnans2006numerical} and Natural Gradient~\citep{martens2020new}, which employ the inverses of local Hessian and Fisher Information Matrix as preconditioners, respectively. Although preconditioning methods typically exhibit much faster convergence than first-order approaches, their practical application is limited by the size of most real-world problems, as they demand quadratic storage and cubic computation time for each gradient update~\citep{fletcher2000practical, bonnans2006numerical}. 

In addition to preconditioning, adaptive moment estimation is another line of work that has been highly influential in deep learning optimization. These methods, including AdaGrad~\citep{duchi2011adaptive}, Adam~\citep{kingma2014adam}, AMSGrad~\citep{reddi2019convergence}, and Adafactor~\citep{shazeer2018adafactor} dynamically adapt the learning rate of each parameter throughout the optimization process by leveraging cumulative second-order gradient statistics. While their theoretical foundations are not yet fully explored, these algorithms have demonstrated more robust empirical results than SGD in various domains and often exhibit better convergence behaviors in practice~\citep{loshchilov2017decoupled, zhang2020adaptive, kunstner2024heavy, zhang2024transformers}.

\textbf{Our approach}. In this work, we explore adaptive moment-based algorithms from the perspective of preconditioning methods. We are driven by the understanding that the second-moment estimates in these algorithms can be derived from the diagonal approximation of a structured preconditioner. We propose to improve this approximation by applying an invertible transformation that maps the preconditioner into a new space where it becomes approximately diagonal. In this transformed space, the diagonal elements of the preconditioner can be accumulated to estimate second-order statistics better. Since the transformation is invertible, we can formulate the update on the original parameter space by doing a simple projection back.

Our key contributions are outlined as follows:

\noindent 1. Our approach is designed to be both straightforward and versatile, facilitating easy integration into existing adaptive moment-based optimizers such as RMSprop, Adam, and its variants.

\noindent 2. We establish a convergence guarantee for the general framework without requiring typical strong assumptions. This guarantee is significant because it is broadly applicable to a wide range of adaptive optimizers, ensuring reliable performances in diverse scenarios. 

\noindent 3. We can effortlessly adapt our method to memory-efficient optimizers, enabling practical training of large models while preserving strict theoretical guarantees of convergence.

\noindent 4. Empirical results show that our proposed methods can substantially enhance the convergence speed and efficiency of adaptive moment-based optimization baselines. Particularly in pretraining large-scale models like LLaMA, our approach can achieve a speedup of 1.5x to 2x compared to the Adam, but with manageable computational overhead.

\textbf{Notations}: For any matrices $\vv A, \vv B$ of size $m \times n$, we use $\sqrt{\vv A}$ for element-wise square root, $\vv A^2$ for element-wise square, and $\vv A / \vv B$ for element-wise division. The symbol $\vv A^\top$ stands for the transpose matrix, $\langle \vv A, \vv B \rangle = \text{trace}(\vv A^\top \vv B)$ represents the inner products of matrices, and $\vv A \otimes \vv B$ is the Kronecker product. Let $\diag(.)$ denote the diagonal matrix, and $\vect(.)$ denote the vectorization of a matrix. We  write $[t] = \{1,\ldots, t\}$ as the first $t$ positive integers.

\section{Preliminaries and Background}\label{sec:background}
We consider an unconstrained, continuous optimization problem $\min_{\vv{W} \in \mathbb{R}^{d}} \mathcal{L}(\vv{W}; \vv X)$,
with $\vv X$ denotes observations, $\mathcal{L}:\mathbb{R}^{d} \rightarrow \mathbb{R}$ is a proper differentiable and lower bounded objective function.

\subsection{Preconditioned Gradient Descent}
The iterative scheme of preconditioned gradient descent can be expressed as follows:
\begin{equation}
    \vv W_{t+1} = \vv W_{t} - \eta_t \vv C(t) \nabla \mathcal{L}(\vv W_t; \vv X),
\end{equation}
where the matrix $\vv C(t)$ is referred to as preconditioner. When $\vv C(t)$ is set to the identity matrix, the update simplifies to ordinary gradient descent. To capture curvature informativeness, systematic designs of $\vv C(t)$ have been developed using local numerical approximations. Classic algorithms in this category, including Newton methods and Natural Gradient, utilize the inverse of Hessian and Fisher Information Matrix, respectively, as preconditioners. These methods offer a built-in mechanism for curvature awareness, promoting larger updates in directions associated with small Hessian eigenvalues to swiftly navigate flat regions while limiting movement in directions with large Hessian eigenvalues to avoid sharp valleys. However, for large-scale models, further approximations to the preconditioners are necessary to ensure their practicality. Various techniques have been proposed for this purpose, such as Quasi-Newton methods~\citep{fletcher2000practical}, Gaussian-Newton estimators ~\citep{botev2017practical, martens2020new, liu2023sophia}, K-FAC~\citep{martens2015optimizing, grosse2016kronecker}.



\subsection{Adaptive Moment Estimation} \label{sec:ame}
These methods, such as AdaGrad and Adam dynamically adjust learning rates for each parameter by incorporating a form of gradient-based statistics. Specifically with Adam, it estimates both first and second-order moments by maintaining exponential moving averages (EMA) on the mean and uncentered variance of gradient information across iterations. Let $ \vv {G}_\tau \triangleq \nabla \mathcal{L}(\vv W_\tau; \vv X)$ denote the gradient of loss function at iteration $\tau$, the update rules are defined as:
\begin{align*}
    \vv M_t &= \hat{\beta}_{1t} \vv M_{t-1} + (1 - \hat{\beta}_{1t}) \vv {G}_t \triangleq \EMA_{\tau \in [t]} \left[ \vv {G}_\tau \right] \\
    \vv V_t &= \hat{\beta}_{2t} \vv V_{t-1} + (1 - \hat{\beta}_{2t}) \vv {G}_t^2 \triangleq \EMA_{\tau \in [t]} \left[ \vv {G}_\tau^2 \right] \\
    \vv W_{t+1} &= \vv W_t - \eta_t \frac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}  
\end{align*}
where $\hat{\beta}_{1t}, \hat{\beta}_{2t}$ are the decay moment coefficients, $\epsilon$ is the smoothing tolerance constant to avoid numerical instability. In principle, the first moment amplifies the gradient in directions that are consistently the same sign and dampens the gradient in directions that are reversing sign. Meanwhile, the second moment captures the curvature by adjusting the step size based on gradient magnitude: smaller steps in steep-gradient regions to avoid overshooting and larger steps in shallow-gradient regions for faster convergence. 



\subsection{Adaptive Moment Estimation via Diagonal Preconditioning Approximation}\label{sec:define}
% For convolutional models, the gradients can be effortlessly reshaped without impacting our overall mathematical derivations.

Let us examine the matrix case where $\vv W$ represents a weight parameter with dimensions $m \times n$. We analyze a preconditioner $\vv C_t$ exploiting the second-order moment of accumulated gradients in the following \textit{inverse} form: 
\begin{equation}
    \vv C_t = \left[ \mathbb{E}_{p(\vv X)} [\vect (\vv {G}_t) \vect(\vv {G}_t)^\top] + \epsilon \vv I_{mn} \right]^{-1/2}
\end{equation}
We have $\vv C_t$ as a positive definite matrix of size $mn \times mn$, which is quadratic to the size of model parameter $\vv W$. An analytical formulation of this quality is often intractable in practice. However, under the assumption of stationary gradient distribution, we can approximate the expectation by leveraging minibatch sampling in conjunction with the exponential moving average technique. We then obtain an empirical preconditioner defined by:
\begin{equation}
    \vv C_t = \left[ \EMA_{\tau \in [t]}  \left[ \vect (\vv G_\tau) \vect(\vv G_\tau)^\top \right] + \epsilon \vv I_{mn} \right]^{-1/2}, 
\end{equation}
where $\vv G_{\tau} := \frac{1}{|\mathcal{B}_{\tau}|} \sum_{\vv X \in \mathcal{B}_{\tau}} \nabla \mathcal{L}(\vv W_{\tau}; \vv X) $ is the minibatch gradient at training step $\tau$. This empirical preconditioner closely resembles the full matrix version of AdaGrad~\citep{duchi2011adaptive}, but instead of using a cumulative sum, we apply an exponential moving average (EMA).

Directly calculating and storing the matrix $\vv C_t$ is computationally expensive, particularly with modern network architectures, since it requires inverting a very large matrix. A practical way to alleviate this bottleneck is by using diagonal approximation:
\begin{align}
    \vv C_t^{(d)} & =  \left[ \EMA_{\tau \in [t]}   \diag \left( \vect (\vv G_\tau) \vect(\vv G_\tau)^\top \right) + \epsilon \vv I_{mn} \right]^{-1/2}  \nonumber \\ 
    % &= \left[ \EMA_{\tau \in [t]}   \diag \left( \vect (\vv G_\tau^2) + \epsilon \right) \right]^{-1/2} \\
    &= \diag^{-1/2} \left( \EMA_{\tau \in [t]} \left[ \vect (\vv G_\tau^2) + \epsilon \right] \right).  
\end{align}
The diagonal preconditioner $\vv C_t^{(d)}$ represents the inverse root square of the second-order gradient accumulator, which is widely adopted as the adaptive moment estimation in optimizers such as AdaGrad, RMSprop, Adam, and variants. Implementing this diagonal approximation offers advantages in both computational efficiency and memory usage. \citet{amari2019fisher} also demonstrate that the off-diagonal components of $\vv C_t$ are smaller than the diagonal components by a factor of $1 / \sqrt{N}$, where $N$ is the number of elements in the matrix. This insight contributes to understanding the practical success of optimizers like AdaGrad, Adam, and others. However, by omitting the off-diagonal elements, the algorithm does not incorporate gradient correlations, which can be particularly useful in accelerating optimization ~\citep{martens2015optimizing, gupta2018shampoo, liu2023sophia}.


% \subsection{Preconditioner accumulation as incremental moment matching}

% \subsection{Square Root Second Moment and Decoupled Weight Decay}


\section{Preconditioner Diagonalization with Gradient Projection}\label{sec:method}
\begin{algorithm}[t]
\caption{\textcolor{blue}{AdaDiag} and \textcolor{purple}{AdaDiag++} for matrix parameter $\vv W$ of size $m \times n$, $m \leq n$}
\label{alg:algorithm1}
\begin{algorithmic}
\STATE \textbf{Inputs:} moment coefficients $\beta_1, \beta_2$, smoothing term $\epsilon = 10^{-8}$, regularization constant $\lambda$
\STATE \textbf{Initialization:} weight parameters $\vv W_0 \in \mathbb{R}^{m \times n}$, initial moments $\vv M_0, \vv V_0 \leftarrow 0$
% \STATE \textbf{repeat}
\REPEAT
    \STATE $t \leftarrow t+1$
    \STATE $\vv G_t = \nabla \mathcal{L}(\vv W_{t}; \mathcal{B}_t)$
    \IF {$t \text{ mod } T = 0$}
        \STATE $\vv{P}_t, \ \_ , \ \vv{Q}_t^\top$ = \texttt{torch.linalg.svd}($\vv{G}_t$, \texttt{full\_matrices=True})
    \ELSE
        \STATE $\vv P_t, \vv Q_t^\top \leftarrow \vv P_{t-1}, \vv Q_{t-1}^\top $
    \ENDIF
    \STATE \textcolor{blue}{$\widetilde{\vv G}_t  = \vv P_t^\top \vv G_t$} \quad \quad \textcolor{purple}{$\widetilde{\vv G}_t  = \vv P_t^\top \vv G_t \vv Q_t$}
    \STATE $\vv M_t = \hat{\beta}_{1t} \vv M_{t-1} + (1-\hat{\beta}_{1t}) \widetilde{\vv G}_t$
    \STATE $\vv V_t = \hat{\beta}_{2t} \vv V_{t-1} + (1-\hat{\beta}_{2t}) \widetilde{\vv G}_t^2$
    \STATE \textcolor{blue}{$\vv W_{t+1} = \vv W_{t} - \eta_{t} \left( \vv P_t \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon} + \lambda \vv W_t \right)$}
    \STATE \textcolor{purple}{$\vv W_{t+1} = \vv W_{t} - \eta_{t} \left( \vv P_t \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon} \vv{Q}_t^\top  +  \lambda \vv W_t \right)$} 
    % \STATE $\vv M_t = \hat{\beta}_{1t} \vv M_{t-1} + (1-\hat{\beta}_{1t}) \clip\left( \widetilde{\vv G}_t / \left( \sqrt{\vv V_{t}} + \epsilon \right) \right)$
    % \STATE \colorbox{lightgreen}{$\vv W_{t} = \vv W_{t-1} - \eta_{t} \left( \vv{P}_t \vv M_t  +  \lambda \vv W_{t-1} \right)$}  \quad \quad \colorbox{lightpurple}{$\vv W_{t} = \vv W_{t-1} - \eta_{t} \left( \vv{P}_t \vv M_t \vv{Q}_t^\top  +  \lambda \vv W_{t-1} \right)$} 
\UNTIL \textit{stopping criterion is met}
\RETURN optimized parameter $\vv W_t$
\end{algorithmic}
\end{algorithm}


To leverage the off-diagonal components of the preconditioner matrix, one can implement structural approximations, like Gaussian-Newton estimators ~\citep{botev2017practical, martens2020new, liu2023sophia}, or Kronecker factorization~\citep{martens2015optimizing, gupta2018shampoo}. In this section, we approach the problem from a different perspective of preconditioner diagonalization. Specifically, we will rationalize the diagonal approximation assumption by applying an implicit orthogonal transformation on the preconditioner matrix $\vv C_t$. Intuitively, this technique will rotate the gradients to align with coordinate axes partially, ultimately causing the matrix $\vv C_t$ to become approximately diagonal. Moreover, we will show that this transformation is invertible via a network reparameterization, leading to a simple update on the original parameter space. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/off_diagonal_density_plots_v2.pdf}
    \caption{Histograms of off-diagonal elements $\COV (\vv G_\tau)$ (original) and $\COV (\widetilde{\vv G}_\tau)$ (\textit{two-sided} projection), corresponding to the two first layers of ResNet50 trained on ImageNet1k. In this experiment, we set the frequency $T = 500$ and plot histograms at iterations with and without SVD applied.}
    \label{fig:hist}
\end{figure}

Recall $\vv G_\tau$ is a matrix of size $m \times n$, with $m \leq n$. Define $\COV (\vv G_\tau) \triangleq \vect(\vv G_\tau) \vect(\vv G_\tau)^\top$, then:
\begin{equation}
    \vv C_t = \left[ \EMA_{\tau \in [t]}  \left[ \COV(\vv G_\tau) \right] + \epsilon \vv I_{mn} \right]^{-1/2}. 
\end{equation}
Let's start by drawing some intuitions through the diagonalization of matrix $\COV (\vv G_\tau)$.
% \qq{Why diagonalize $\COV(\vv G_\tau)$? The only time when it is diagonal is when $G_t$ is a one-hot vector. It is problematic I think. NEED TO change this.}
% \qq{Perhaps you can say diagonalizing $\hat{\vv G}_t$. 
% As i suggested earlier. It might be more precise to say that we diagonalize $G G\tt$ or $G\tt G$. Why do you present this way? 
% }
Given the special formula of $\COV (\vv G_\tau)$, we can perform a straightforward approach using Singular Value Decomposition (SVD) on the gradient $\vv G_\tau$. Suppose we have
$
\vv G_\tau = \vv P_\tau \vv \Sigma_\tau \vv Q_\tau^\top,
$
in which $\vv P_\tau, \vv Q_\tau$ are orthogonal matrices of size $m \times m, n \times n$, respectively, and $\vv \Sigma_\tau$ is a diagonal matrix of size $m \times n$. Substituting this representation into $\COV (\vv G_\tau)$ gives us:
\begin{align*}
    \COV (\vv G_\tau) &= \vect(\vv P_\tau \vv \Sigma_\tau \vv Q_\tau^\top) \vect(\vv P_\tau \vv \Sigma_\tau \vv Q_\tau^\top)^\top \\
    &= (\vv Q_\tau \otimes \vv P_\tau) \vect(\vv \Sigma_\tau) \vect(\vv \Sigma_\tau)^\top (\vv Q_\tau \otimes \vv P_\tau)^\top
\end{align*}
Since $\vv \Sigma_\tau$ is a diagonal matrix, we have $\vect(\vv \Sigma_\tau) \vect(\vv \Sigma_\tau)^\top$ is almost diagonal (off-diagonal elements are mostly zero). Moreover, the matrix $\vv Q_\tau \otimes \vv P_\tau$ satisfies $(\vv Q_\tau \otimes \vv P_\tau)(\vv Q_\tau \otimes \vv P_\tau)^\top = (\vv Q_\tau \vv Q_\tau^\top) \otimes (\vv P_\tau \vv P_\tau^\top) = \vv I_{mn}$, so we can consider $\vv Q_\tau \otimes \vv P_\tau$ as an orthogonal diagonalizing matrix with:
\begin{equation*}
    (\vv Q_\tau \otimes \vv P_\tau)^{-1} \COV (\vv G_\tau) (\vv Q_\tau \otimes \vv P_\tau) = \vect(\vv \Sigma_\tau) \vect(\vv \Sigma_\tau)^\top.
\end{equation*}
Alternatively, the diagonalization process above can be equivalently derived from $\COV (\widetilde{\vv G}_\tau)$, with $\widetilde{\vv G}_\tau \triangleq \vv P^\top_\tau \vv G_\tau \vv Q_\tau = \vv \Sigma_\tau$. This rotation aligns the gradient $\widetilde{\vv G}_t$ with coordinate axes and consequently induces a roughly diagonal structure on $\COV (\widetilde{\vv G}_\tau)$.
% \qq{$ \vect(\vv \Sigma_\tau) \vect(\vv \Sigma_\tau)^\top$ is not diagonal, we are not diagonalizing $\COV(G)$. }
% \qq{We need to rewrite this section. } 

\subsection{Periodic Subspace Projection}~\label{sec:projection}
\vspace{-0.6cm}

% \qq{Why is this section related to continual subspace?}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{./figures/matrix_heatmaps.pdf}  % Path to your figure file
    \caption{Sparsity of \textit{one-sided} projection.}
    \label{fig:one-sided-matrix}
\end{figure}

The analysis presented above applies only to the iteration in which SVD is implemented. However, it is impractical to utilize SVD at every iteration of the training procedure due to computational overhead. Fortunately, there are recent works on low-rank gradient projection~\citep{gur2018gradient, gooneratne2020low, zhao2024galore, liang2024memory} indicating that the optimization process usually acts on low-dimensional subspaces. GaLore~\citep{zhao2024galore} exploits this concept by showing that the training trajectory can be divided into continual subspaces, from which the gradients within each subspace can be governed by a common spanning basis. GaLore deployed this idea by periodically applying SVD on the gradients to extract projection matrices. Mathematically, during each period of length $T$, say $[\kappa T, (\kappa+1)T]$, the gradient $\vv G_\tau$ can be decomposed as:
$$
\vv G_\tau \approx \vv P_\kappa \vv \Sigma_\tau \vv Q_\kappa^\top \quad \forall \tau \in [\kappa T, (\kappa+1)T]
$$
where $\vv P_\kappa, \_, \vv Q_\kappa^\top \leftarrow \text{SVD}(\vv G_{\kappa T})$ are kept the same throughout the period. 

We can adapt this assumption to our framework. In this way, we can expect that our projected gradients in each period are still approximately diagonal, namely:
$$
\widetilde{\vv G}_\tau \triangleq \vv P^\top_\kappa \vv G_\tau \vv Q_\kappa \approx (\vv P^\top_\kappa \vv P_\kappa) 
\vv \Sigma_\tau (\vv Q_\kappa^\top \vv Q_\kappa) = \vv \Sigma_\tau.
$$
As a result, we can achieve a desired diagonal structure on $\COV (\widetilde{\vv G}_\tau), \forall \tau \in [\kappa T, (\kappa+1)T]$.




% \begin{wrapfigure}[17]{R}{0.48\textwidth}  
% \vspace{-15pt}
%     \centering
%     \includegraphics[width=0.48\textwidth]{./figures/matrix_heatmaps.pdf}  % Path to your figure file
%     \caption{Sparsity of \textit{one-sided} projection}
% \end{wrapfigure}

\textbf{Why the full matrices $\vv P_\kappa, \vv Q_\kappa$ matter: connection and difference with Galore}. 
It is essential to note that, since GaLore's focus is on memory efficiency, they just implemented truncated SVD to capture the top-$K$ representation of the gradient matrices. This assumption is reasonable when considering that gradients are low-rank matrices. However, using truncated SVD involves careful tweaking of the $K$ values and more importantly, the gradient projection step has to be executed on the smaller dimension of the matrix.\footnote{Let's omit subscripts $\tau, \kappa$ for simplicity. Consider $\vv G_{m \times n} \approx \vv P_{m \times K} \vv \Sigma_{K \times K} \vv Q_{n \times K}^\top$ as the truncated SVD of gradient matrix $\vv G_{m \times n}$ with $m \leq n$. The GaLore algorithm performs a gradient projection as $\widetilde{\vv G} = \vv P_{m \times K} ^\top \vv G$, which maps $n$ vectors of size $m$ from $\vv G$ onto the subspace spanned by $K$ vectors of size $m$ from $\vv P_{m \times K}$. Since $K \leq m \leq n$, this operator is more effective than projecting in larger dimensions, i.e. $\widetilde{\vv G} = \vv G \vv Q_{n \times K}$.} On the other hand, our framework would require sophisticated modifications in GaLore, of which $\vv P_\kappa, \vv Q_\kappa$ need to be the full matrices or $K$ is at least the effective rank. This guarantees that $\vv P_\kappa, \vv Q_\kappa$ can form complete bases for the rows and columns of the gradient matrices in each period. To make the algorithm effortless, we propose to adopt the full matrices instead of tunning effective rank $K$.


\textbf{One-sided projection}. Instead of using \textit{two-sided} projection as described so far, we can opt for a simpler version involving just \textit{one-sided} projection, namely $\widetilde{\vv G}_\tau \triangleq \vv P^\top_\tau \vv G_\tau = \vv \Sigma_\tau \vv Q_\tau^\top$, then we have:
\begin{align}
    \COV (\widetilde{\vv G}_\tau) &\triangleq \vect(\widetilde{\vv G}_\tau) \vect(\widetilde{\vv G}_\tau)^\top = \vect(\vv \Sigma_\tau \vv Q_\tau^\top) \vect(\vv \Sigma_\tau \vv Q_\tau^\top)^\top \nonumber \\
    &= (\vv Q_\tau \otimes \vv I_m) \vect(\vv \Sigma_\tau) \vect(\vv \Sigma_\tau)^\top (\vv Q_\tau \otimes \vv I_m)^\top. \nonumber
\end{align}
The projected gradient $\widetilde{\vv G}_\tau = \vv \Sigma_\tau \vv Q_\tau^\top$ inherently exhibits a sparse structure as illustrated in Figure~\ref{fig:one-sided-matrix}. This is because $\vv \Sigma_\tau$ is a diagonal matrix and the smallest singular values on the diagonal will zero out the magnitude of corresponding rows on $\vv Q_\tau$. In Figure~\ref{fig:hist} and~\ref{fig:hist-onesided}, we further show the histograms of off-diagonal elements of $\COV (\vv G_\tau)$ and $\COV (\widetilde{\vv G}_\tau)$ (both \textit{two-sided} and \textit{one-sided}), corresponding to the two first layers of ResNet50 trained on ImageNet1k. We can observe notable differences in sparsity patterns of off-diagonal elements of $\COV (\widetilde{\vv G}_\tau)$ compared to the original $\COV (\vv G_\tau)$ over iterations. The matrix $\COV (\widetilde{\vv G}_\tau)$ is roughly diagonal, enabling a more accurate diagonal approximation for the preconditioner matrix $\widetilde{\vv C}_t$.

% \textbf{One-sided projection}. Instead of using \textit{two-sided} projection as described so far, we can opt for a simpler version involving just \textit{one-sided} projection, namely $\widetilde{\vv G}_\tau \triangleq \vv P^\top_\tau \vv G_\tau = \vv \Sigma_\tau \vv Q_\tau^\top$, then we have:
% \begin{align}
%     \COV (\widetilde{\vv G}_\tau) \triangleq \vect(\widetilde{\vv G}_\tau) \vect(\widetilde{\vv G}_\tau)^\top &= \vect(\vv \Sigma_\tau \vv Q_\tau^\top) \vect(\vv \Sigma_\tau \vv Q_\tau^\top)^\top \nonumber \\
%     &= (\vv Q_\tau \otimes \vv I_m) \vect(\vv \Sigma_\tau) \vect(\vv \Sigma_\tau)^\top (\vv Q_\tau \otimes \vv I_m)^\top.
% \end{align}

% \begin{wrapfigure}{r}{0.4\textwidth}  % 'r' for right, '0.4\textwidth' for the width of the figure
%     \centering
%     \includegraphics[width=0.4\textwidth]{./figures/off_diagonal_density_plots.pdf}  % Path to your figure file
%     \caption{Illustration of sparsity of \textit{one-sided} projection}
% \end{wrapfigure}

% The projected gradient $\widetilde{\vv G}_\tau = \vv \Sigma_\tau \vv Q_\tau^\top$ inherently exhibits a sparse structure as illustrated in Figure []. This is because $\vv \Sigma_\tau$ is a diagonal matrix and the smallest singular values on the diagonal will zero out the magnitude of corresponding rows on $\vv Q_\tau$. In Figure [], we further show the histograms of off-diagonal elements of $\COV (\vv G_\tau)$ and $\COV (\widetilde{\vv G}_\tau)$ (both \textit{two-sided} and \textit{one-sided}), corresponding to the two first layers of ResNet50 trained on ImageNet1k. We can observe notable differences in sparsity patterns of off-diagonal elements of $\COV (\widetilde{\vv G}_\tau)$ compared to the original $\COV (\vv G_\tau)$ over iterations. The matrix $\COV (\widetilde{\vv G}_\tau)$ are roughly diagonal, enabling a more accurate diagonal approximation for the preconditioner matrix $\widetilde{\vv C}_t$.

\subsection{Gradient Projection Implies Network Reparameterization}
In the previous sections, we demonstrated that we can rotate the gradients $\vv G_\tau$ to $\widetilde{\vv G}_\tau$ in such a way that the matrix $\COV (\widetilde{\vv G}_\tau)$ is approximately diagonal. Consequently, it induces a diagonal approximation of $\widetilde{\vv C}_t$ as follows: 
\begin{align}
    \widetilde{\vv C}_t^{(d)} &= \left[ \EMA_{\tau \in [t]} \diag \left[ \vect (\widetilde{\vv G}_\tau) \vect(\widetilde{\vv G}_\tau)^\top \right] + \epsilon \vv I_{mn} \right]^{-1/2} \nonumber \\
    % &= \left[ \EMA_{\tau \in [t]}   \diag \left[ \vect (\widetilde{\vv G}_\tau^2) + \epsilon \right] \right]^{-1/2} \\ 
    &= \diag^{-1/2} \left[ \EMA_{\tau \in [t]}  \left[ \vect (\widetilde{\vv G}_\tau^2) + \epsilon \right] \right], 
\end{align}
The preconditioned gradient at iteration $t$ then becomes:
\begin{equation}
    \widetilde{\vv C}_t^{(d)} \vect(\widetilde{\vv G}_t) = \frac{\vect(\widetilde{\vv G}_t)}{\sqrt{\EMA_{\tau \in [t]}  \left[ \vect (\widetilde{\vv G}_\tau^2) + \epsilon \right]}} \label{eq:update}
\end{equation}
However, this quantity cannot be directly applied to update the weight parameters, as the gradient projection implicitly imposes a reparameterization on the weight space. In Figure~\ref{fig:rotated-network}, we illustrate how the update in equation~\ref{eq:update} can be utilized to learn a rotated network rather than the original one. This rotated network introduces two auxiliary layers defined by $\vv P_\kappa, \vv Q_\kappa^\top$, and reparameterizes the original weight parameters as $\widetilde{\vv W} = \vv P_\kappa^\top \vv W \vv Q_\kappa$. While this transformation does not alter the forward pass of the original network, it does lead to a corresponding gradient, represented as:
\begin{align*}
\nabla_{\widetilde{\vv W}}\mathcal{L}(\widetilde{\vv W}; \vv X) &= \vv P_\kappa^\top \nabla_{\vv W}\mathcal{L}(\widetilde{\vv W}; \vv X) \vv Q_\kappa \\
&\underset{\text{same forward response}}{=} \vv P_\kappa^\top \nabla_{\vv W}\mathcal{L}(\vv W; \vv X) \vv Q_\kappa,
\end{align*} 
which is equivalent to our \textit{two-sided} gradient projection. Therefore, we can derive the preconditioned gradient descent for the rotated network using the following update:
\begin{equation}
    \widetilde{\vv W}_{t+1} = \widetilde{\vv W}_{t+1} - \eta_t \frac{\widetilde{\vv G}_t}{\sqrt{\EMA_{\tau \in [t]}  \left[\widetilde{\vv G}_\tau^2 + \epsilon \right]}}
\end{equation} 
of which, we dropped the vectorization to ensure dimensional compatibility. Since $\vv P_\kappa$ and $\vv Q_\kappa$ are full-rank orthogonal matrices, the reparameterization is invertible and thus the update on the original parameter can be obtained by a projection back step as:
\begin{equation}
    \vv W_{t+1} = \vv W_{t+1} - \eta_t \vv P_\kappa \frac{\widetilde{\vv G}_t}{\sqrt{\EMA_{\tau \in [t]}  \left[\widetilde{\vv G}_\tau^2 + \epsilon \right]}} \vv Q_\kappa^\top,
\end{equation}
for $t \in [\kappa T, (\kappa+1)T]$.
% \begin{align}
%     \vv W_{t+1} &= \vv W_{t+1} - \eta_t \vv P_\kappa \frac{\widetilde{\vv G}_t}{\sqrt{\EMA_{\tau \in [t]}  \left[\widetilde{\vv G}_\tau^2 + \epsilon \right]}} \vv Q_\kappa^\top, \nonumber \\
%     & \forall t \in [\kappa T, (\kappa+1)T].
% \end{align}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        % ORIGINAL NETWORK
        % Nodes for x
        \pgfmathsetmacro{\a}{0}
        \node[circle, draw, thick, line width=0.4mm] (x1) at (\a, 2) {};
        \node[circle, draw, thick, line width=0.4mm] (x2) at (\a, 1) {};
        \node[circle, draw, thick, line width=0.4mm] (x3) at (\a, 0) {};
        
        % Nodes for y
        \node[circle, draw, thick, line width=0.4mm] (y1) at (\a+2, 1.5) {};
        \node[circle, draw, thick, line width=0.4mm] (y2) at (\a+2, 0.5) {};
        
        % Arrows for connections between x and y
        \draw[-] (x1) -- (y1);
        \draw[-] (x1) -- (y2);
        \draw[-] (x2) -- (y1);
        \draw[-] (x2) -- (y2);
        \draw[-] (x3) -- (y1);
        \draw[-] (x3) -- (y2);
        
        % Labels for x and y
        \node at (\a, 2.5) {$x$};
        \node at (\a+2, 2.5) {$y$};
        \draw[->] (\a+0.25, 2.5) -- (\a+2-0.25, 2.5);
        
        % Label for weights w
        \node at (\a+1, 2.25) {$\vv W$};
        
        % Arrows and labels for derivatives
        \node at (\a, -0.5) {$\frac{\partial \mathcal{L}}{\partial x}$};
        \node at (\a+2, -0.5) {$\frac{\partial \mathcal{L}}{\partial y}$};
        \draw[->] (\a+2-0.25, -0.5) -- (\a+0.25, -0.5);
    
    
    
        % ROTATED NETWORK
        % Nodes for x
        \pgfmathsetmacro{\b}{2.8}
        \node[circle, draw, thick, line width=0.4mm] (x1) at (\b, 2) {};
        \node[circle, draw, thick, line width=0.4mm] (x2) at (\b, 1) {};
        \node[circle, draw, thick, line width=0.4mm] (x3) at (\b, 0) {};
    
        % Labels for x and xh
        \node at (\b, 2.5) {$x$};
        \node at (\b+1.5, 2.5) {$\widetilde{x}$};
        \draw[->] (\b+0.25, 2.5) -- (\b+1.5-0.25, 2.5);
        % Label for weights P
        \node at (\b+0.75, 2.25) {$\vv P_\kappa$};
        % Arrows and labels for derivatives
        \node at (\b, -0.5) {$\frac{\partial \mathcal{L}}{\partial x}$};
        \node at (\b+1.5, -0.5) {$\frac{\partial \mathcal{L}}{\partial \widetilde{x}}$};
        \draw[->] (\b+1.5-0.25, -0.5) -- (\b+0.25, -0.5);
        
        % Nodes for xh
        \node[circle, draw, thick, line width=0.4mm] (xh1) at (\b+1.5, 2) {};
        \node[circle, draw, thick, line width=0.4mm] (xh2) at (\b+1.5, 1) {};
        \node[circle, draw, thick, line width=0.4mm] (xh3) at (\b+1.5, 0) {};
    
    
        % Labels for xh and yh
        \node at (\b+3.5, 2.5) {$\widetilde{y}$};
        \draw[->] (\b+1.5+0.25, 2.5) -- (\b+3.5-0.25, 2.5);
        % Label for weights Q
        \node at (\b+2.5, 2.25) {$\widetilde{\vv W}$};
        \node at (\b+2.5, 2.9) {$\vv P_\kappa^\top \vv W \vv Q_\kappa$};
        % Arrows and labels for derivatives
        \node at (\b+3.5, -0.5) {$\frac{\partial \mathcal{L}}{\partial \widetilde{y}}$};
        \draw[->] (\b+3.5-0.25, -0.5) -- (\b+1.5+0.25, -0.5);
    
    
        % Nodes for yh
        \node[circle, draw, thick, line width=0.4mm] (yh1) at (\b+3.5, 1.5) {};
        \node[circle, draw, thick, line width=0.4mm] (yh2) at (\b+3.5, 0.5) {};
    
    
        % Labels for yh and y
        \node at (\b+5, 2.5) {$y$};
        \draw[->] (\b+3.5+0.25, 2.5) -- (\b+5-0.25, 2.5);
        % Label for weights Q
        \node at (\b+4.25, 2.25) {$\vv Q_\kappa^\top$};
        % Arrows and labels for derivatives
        \node at (\b+5, -0.5) {$\frac{\partial \mathcal{L}}{\partial y}$};
        \draw[->] (\b+5-0.25, -0.5) -- (\b+3.5+0.25, -0.5);
    
    
        % Nodes for y
        \node[circle, draw, thick, line width=0.4mm] (y1) at (\b+5, 1.5) {};
        \node[circle, draw, thick, line width=0.4mm] (y2) at (\b+5, 0.5) {};
        
        % Arrows for connections between x and xh
        \draw[-] (x1) -- (xh1);
        \draw[-] (x1) -- (xh2);
        \draw[-] (x1) -- (xh3);
        \draw[-] (x2) -- (xh1);
        \draw[-] (x2) -- (xh2);
        \draw[-] (x2) -- (xh3);
        \draw[-] (x3) -- (xh1);
        \draw[-] (x3) -- (xh2);
        \draw[-] (x3) -- (xh3);
    
    
        % Arrows for connections between xh and yh
        \draw[-] (xh1) -- (yh1);
        \draw[-] (xh1) -- (yh2);
        \draw[-] (xh2) -- (yh1);
        \draw[-] (xh2) -- (yh2);
        \draw[-] (xh3) -- (yh1);
        \draw[-] (xh3) -- (yh2);
    
        % Arrows for connections between yh and y
        \draw[-] (yh1) -- (y1);
        \draw[-] (yh1) -- (y2);
        \draw[-] (yh2) -- (y1);
        \draw[-] (yh2) -- (y2);
    
    \end{tikzpicture}
    \caption{Illustration of network reparameterization induced by full-rank gradient projection.}
    \label{fig:rotated-network}
\end{figure}

\subsection{Final Algorithm and Related Works}\label{sec:related-works}
We provide the details of our proposal in Algorithm~\ref{alg:algorithm1}, which encompasses both \textit{two-sided} and \textit{one-sided} rotation versions. We also employ an exponential moving average of the projected gradients $\widetilde{\vv G}$ to derive the first-order moment estimation $\vv M_t$. This accumulation is performed before applying preconditioning. It should be noted that our implementation requires \texttt{torch.linalg.svd}($\vv{G}_t$, \texttt{full\_matrices=True}) to extract the full projection matrices $\vv P_t, \vv Q_t$ in each period. 
% For AdaDiag++, we recommend implementing the first SVD step at the $T$'th iteration of the training process to avoid numerical issues.

Given the flexibility of our framework, we can adapt it for other adaptive optimizers. In Appendix~\ref{app:adafactor}, we provided variants of the algorithm tailored for Adafactor and Hfac, along with empirical results evaluating their performances.

% In connection with other existing algorithms, our framework inherits the critical idea of gradient low-rank projection from GaLore. Nevertheless, our proposal extends beyond GaLore's main focus on memory efficiency. From a practical standpoint, our algorithms can substantially outperform Adam with only a manageable computational overhead. SOAP~\citep{vyas2024soap} is another closely related work to our proposal. They obtain the projection matrices $\vv P_t$ and $\vv Q_t$ by performing eigendecomposition on the accumulators $\vv G_t \vv G_t^\top$ and $\vv G_t^\top \vv G_t$, respectively. Essentially, the eigenvector matrix retrieved from the eigendecomposition of $\vv G_t \vv G_t^\top$ (and $\vv G_t^\top \vv G_t$) corresponds to the left (and right) singular matrix of $\vv G_t$. Despite the promising empirical results, their approach seems to lack an interpretable framework to explain its effectiveness. On the other hand, our algorithm can offer principled justifications through the lens of preconditioning diagonalization.
In connection with other existing algorithms, several prior works on optimization are relevant to our framework. \cite{george2018fast} and \cite{liu2018rotate} proposed utilizing the eigenbasis of the Fisher Information Matrix to construct diagonal preconditioning approximations within the natural gradient or online Laplace approximation families. Similarly, \cite{anil2020scalable} extends this idea by leveraging the eigendecomposition of Shampoo's preconditioners as a basis for the diagonal transformations. 

Our method, in contrast, focuses on diagonalizing the preconditioner matrix within the generalized family of adaptive moment-based optimization algorithms, which includes Adam, Adafactor as specific cases. While primarily inspired by the critical idea of gradient projection in GaLore, we explore the full-rank projection case and thus move beyond GaLore’s main focus on memory efficiency. We also acknowledge the concurrent work by SOAP \citep{vyas2024soap}, which obtains the projection matrices $\vv P_t$ and $\vv Q_t$ by performing eigendecomposition on the accumulators of $\vv G_t \vv G_t^\top$ and $\vv G_t^\top \vv G_t$ (referred to as Shampoo preconditioners), respectively. Essentially, the eigenvector matrix retrieved from the eigendecomposition of $\vv G_t \vv G_t^\top$ (and $\vv G_t^\top \vv G_t$) corresponds to the left (and right) singular matrix of $\vv G_t$. Our proposal is therefore effectively equivalent to SOAP without accumulations, resulting in enhanced memory efficiency. From a practical standpoint, our algorithms substantially outperform Adam with only a manageable overhead.


\subsection{Improving computational and memory efficiency}
Although our proposal can greatly enhance the performance, it comes with an inevitable trade-off in algorithmic complexity. While the total computational overhead induced by periodic SVD is negligible (less than $10\%$), the memory usage caused by the full-rank projection may limit the applicability of the algorithms. To address this challenge, we propose to apply the general framework to memory-efficiency optimization methods such as Adafactor~\citep{shazeer2018adafactor}, Hfac~\citep{nguyen2024h}. This optimizer employs a rank-1 parameterization to the moment estimates and thus offers a sublinear memory cost. In Table~\ref{tab:optimizer_memory}, we provide an analysis of the complexity of these algorithms.

Adafactor with momentum (w/ m.) and Hfac have demonstrated results comparable to Adam on various tasks~\citep{shazeer2018adafactor, chen2024symbolic, nguyen2024h}. Therefore, a simple integration with our method could potentially surpass Adam's performance while maintaining a similar complexity. We conducted several experiments to validate this hypothesis, with the results presented in Appendix~\ref{app:adafactor} and~\ref{app:hfac}.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/imagenet_vit.pdf}
    \caption{Top-1 Accuracy of optimizers in pretraining ResNet50, ViT-B/32, and ViT-S/16 from scratch on the ImageNet1k. For better ViT's visualization, we crop the learning curve up to epoch 80.}
    \label{fig:vit}
\end{figure*}

\section{Convergence Analysis}\label{sec:convergence}

In comparison to optimizers such as AdaGrad, Adam, and variants, the algorithm~\ref{alg:algorithm1} introduces additional projection matrices $\{\vv P_t, \vv Q_t\}$, resulting in complex interactions between the model parameter $\vv W_t$ and optimization states $\vv S_t = \{\vv M_t, \vv V_t\}$. It is important to note that the second-order momentum $\vv V_t$ is the key factor in our framework. Without this quantity, the proposed algorithm would degenerate to the standard gradient descent with momentum, eliminating any potential improvements. This implies that the algorithms cannot be reduced to a simpler form for analyzing convergence guarantees. Moreover, applying SVD periodically to produce projection matrices also causes intricate behaviors in the dynamic subspace of optimization trajectory. It is unclear whether the momentum estimates accumulated across previous subspaces would be consistent with each other and advantageous for updates conducted in subsequent subspaces. As such, we need a general and robust theoretical framework to encompass a wide range of adaptive moment-based optimizers, including memory-efficient ones like Adafactor and Hfac.

Inspired by the recent works on Online Subspace Descent~\citep{liang2024memory} and Hfac, we leverage the  Hamiltonian descent framework to tackle these challenges. Essentially, this framework investigates continuous-time ODE forms of optimizers in the limit of infinitesimal step size. In this setting, the optimizers will minimize an associated Hamiltonian function $\mathcal{H}(.)$, which is an augmented version of the original objective $\mathcal{L}(.)$. For example, we can derive a continuous-time form for Adam optimizer as:
\begin{align*}
    &(\text{Adam-ODE}): \hspace{0.08in} \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\vv M_t / (\sqrt{\vv V_t} + \epsilon), \\
    &\dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \vv G_t - \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = \vv G_t^2 - \vv V_t,
\end{align*}
which yields a Hamiltonian functions defined by: $\mathcal{H}(\vv W, \vv M, \vv V) = \mathcal{L}(\vv W) + \dfrac{1}{2} \left \langle \vv M / (\sqrt{\vv V} + \epsilon), \vv M  \right \rangle$.
\begin{proposition}
Using this general approach, we formulate continuous-time forms for AdaDiag and AdaDiag++:
\begin{align*}
    &(\text{\textcolor{blue}{AdaDiag}}): \hspace{0.08in} \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\vv P_t \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}, \\
    &\dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \vv P_t^\top \vv G_t - \vv M_t, \hspace{0.08in} \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = (\vv P_t^\top \vv G_t)^2 - \vv V_t \\
    &(\text{\textcolor{purple}{AdaDiag++}}): \hspace{0.08in}  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\vv P_t \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon} \vv Q_t^\top, \\
    &\dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \vv P_t^\top \vv G_t \vv Q_t - \vv M_t, \hspace{0.08in} \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = (\vv P_t^\top \vv G_t \vv Q_t)^2 - \vv V_t 
\end{align*}
Both yield the same Hamiltonian function: $\mathcal{H}(\vv W, \vv M, \vv V) = \mathcal{L}(\vv W) + \dfrac{1}{2} \left \langle \vv M / (\sqrt{\vv V} + \epsilon), \vv M  \right \rangle$.
% \textbf{AdaDiag}

% ODE: $\dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\vv P_t \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \dfrac{\vv P_t^\top \vv G_t}{\sqrt{\vv V_t} + \epsilon} - \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = (\vv P_t^\top \vv G_t)^2 - \vv V_t $

% $\mathcal{H}-$function:: $\mathcal{H}(\vv W, \vv M, \vv V) = \mathcal{L}(\vv W) + \dfrac{1}{2a} \left \langle \vv M \odot \sqrt{\vv V}, \vv M  \right \rangle$

% \textbf{AdaDiag++}

% ODE: $\dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\vv P_t \vv M_t \vv Q_t^\top, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \dfrac{\vv P_t^\top \vv G_t \vv Q_t}{\sqrt{\vv V_t} + \epsilon} - \vv M_t , \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = (\vv P_t^\top \vv G_t \vv Q_t)^2 - \vv V_t $

% $\mathcal{H}-$function: $\mathcal{H}(\vv W, \vv M, \vv V) = \mathcal{L}(\vv W) + \dfrac{1}{2a} \left \langle \vv M \odot \sqrt{\vv V}, \vv M  \right \rangle$
\end{proposition}

\textbf{Convergence to Local Optima}. The key properties is that the function $\mathcal{H}(.)$ is monotonically non-decreasing along its ODE trajectory, namely $\dfrac{\mathrm{d}}{\mathrm{d}t} \mathcal{H}(\vv W_t, \vv S_t) \leq 0, \forall t$. By LaSalle’s Invariance principle, the set of accumulation points $(\vv W_t, \vv S_t)$ must be contained in $\mathcal{I}$, where $\mathcal{I} = \{\text{the union of complete trajectories satisfying }$ $\dfrac{\mathrm{d}}{\mathrm{d}t} \mathcal{H}(\vv W_t, \vv S_t) = 0  \}$. The points in limit set $\mathcal{I}$ should satisfy $ \vv P_t^\top \nabla \mathcal{L}(\vv W_t) \equiv 0$ for AdaDiag or $\vv P_t^\top \nabla \mathcal{L}(\vv W_t) \vv Q_t \equiv 0$ for AdaDiag++, respectively. Since $\vv P_t, \vv Q_t$ are full-rank orthogonal matrices, we must have $\nabla \mathcal{L}(\vv W_t) \equiv 0$, which indicates that all trajectories will converge to local optimal points. Detailed analysis is provided in Appendix~\ref{app:hamiltonian}.


\section{Experiments} \label{sec:exp}
In this section, we conduct several experiments on image classification and language modeling tasks to verify the efficiency of our algorithms. We will also demonstrate that our general framework can be effectively applied to enhance adaptive moment-based optimizers such as Adafactor and Hfac. 

\subsection{Image Classification}

We first evaluated the optimization algorithms, including AdamW, AdaDiag, and AdaDiag++, by pretraining the ImageNet1k dataset from scratch using ResNets and Vision Transformers (ViTs) architectures. The images underwent Inception-style cropping~\citep{szegedy2016rethinking} and random horizontal flipping during pre-processing. We trained ResNet50 for 90 epochs with a batch size of 1024, utilizing a cosine learning rate decay scheduler. For ViTs, we conducted training over 300 epochs with a batch size of 4096, using a learning rate schedule that included a 10,000-step warmup followed by linear decay. Additionally, we employed strong data augmentations, such as RandAugment (2,15)~\citep{Cubuk2019RandaugmentPA} and mixup (0.5)~\citep{Zhang2017mixupBE}, to further improve the performance of the ViTs. 
For hyperparameters settings,  such as learning rate ($lr$), weight decay ($\lambda$), and dropout rate ($dr$), we opted for recommended configurations from prior research and left them in Appendix~\ref{app:hyper-setting}.
% Hyperparameters tuning of learning rate, weight decay, and dropout rate is give in Appendix~\ref{app:hyper-setting}.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6pt}  % Increase the column separation
\caption{Comparison of Adam and AdaDiag on pre-training ResNets and ViTs architectures with ImageNet1k dataset. Top-1 accuracy on the validation set is reported.}
\begin{tabular}{lccc}
\toprule
\textbf{Models}          & \textbf{ResNet50}        & \textbf{ViT-S/16}       & \textbf{ViT-B/32}        \\ \midrule
AdamW            & 75.61        & 78.35        & 72.20          \\
Shampoo         & 75.71        & \textbf{79.58}        & \textbf{73.47}           \\
AdaDiag         & 75.85        & \textbf{79.18}        & \textbf{73.39}          \\ 
AdaDiag++       & 75.86        & \textbf{79.14}        & \textbf{73.24}          \\ 
\bottomrule
\end{tabular}
\label{tb:image}
\end{table}

As shown in Figure~\ref{fig:vit}, AdaDiag and AdaDiag++ exhibit substantial improvements in convergence speed compared to the baseline AdamW. For the ViT-B/32 and ViT-S/16 models, we focus on the first third of the training phase so that we can observe notable accelerations across the three models. For the full performances, we refer to the results in Figure~\ref{fig:vit-full} in Appendix~\ref{app:more-figures}. The final results at convergence are provided in Table~\ref{tb:image}. By more accurate approximations of the preconditioner matrix, we expect that AdaDiag++ can navigate the complex curvature more efficiently and thus provide better convergence properties, even compared to AdaDiag. The results appear to support this argument. It’s important to mention that these two algorithms would perform similarly after converging at some point. We hypothesize that the optimization trajectory will eventually converge to a stable region where the gradients reside in a very low-dimensional subspace. The precondition approximations, at this stage, become less critical as the optimization process focuses on fine-tuning within this reduced space.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/llama.pdf}
    \caption{Training progression for pre-training LLaMA models on C4 dataset.}
    \label{fig:llama}
    % \vspace{-10pt}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/ablation.pdf}
    \caption{An ablation study on why the full-rank SVD matters.}
    \label{fig:ablation-rank}
    % \vspace{-10pt}
\end{figure*}

% \vspace{-0.2cm}

% \vspace{-0.2cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.38\textwidth]{figures/frequency.pdf}
    \caption{Performances of AdaDiag on LLaMA-60M and LLaMA-130M with varying update frequencies $T$.}
    \label{fig:ablation-freq}
    \vspace{-0.5 cm}
\end{figure}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6pt}  % Increase the column separation
\caption{Comparison of Adam and AdaDiag on pre-training LLaMA models with the C4 dataset. Validation perplexity is reported for models with 60/130/350 Million parameters trained for 1.1/2.2/6.4 Billion training tokens.}
% \caption{Validation perplexity of Adam and AdaDiag on pre-training LLaMA models with the C4 dataset.}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Optimizer}} & \multicolumn{3}{c}{\textbf{Validation Perplexity}} \\ \cmidrule{2-4}
          & 60M~/~1.1B       & 130M~/~2.2B      & 350M~/~6.4B       \\ \midrule
Adam            & 31.12        & 24.55        & 18.79          \\
Shampoo            & 29.91        & 23.40        & 17.52          \\
AdaDiag         & \textbf{28.71}        & \textbf{22.40}        & \textbf{16.91}          \\ \bottomrule
\end{tabular}
\label{tb:llama}
\end{table}

\subsection{Language Modeling}
We apply the algorithms to pre-train LLaMA-based large language model, with RMSNorm and SwiGLU activations~\citep{zhang2019root, shazeer2020glu, touvron2023llama}, on the C4 dataset~\citep{raffel2020exploring}. We measured the perplexity of the models on the validation set throughout training to assess convergence properties and final model performance. Specifically, we trained LLaMA models of sizes 60M, 130M, and 350M for 10K, 20K, and 60K steps, respectively. The learning rate schedule included a warmup phase during the first 10\% of the training steps, followed by cosine annealing that decayed the learning rate to 10\% of its initial value. All models used a maximum sequence length of 256 and a batch size of 512.

The results are provided in Figure~\ref{fig:llama} and Table~\ref{tb:llama}. Our optimizer AdaDiag consistently outperforms Adam on various sizes of LLaMA models. In particular, AdaDiag achieves 1.8x-2x speed-up compared to Adam, matching the same perplexity with half fewer steps. Due to resource constraints, we were unable to conduct experiments with billion-parameter models. However, we are confident that similar results can be reliably achieved with larger-scale models.


\subsection{Ablation Study}
We present an ablation study in Figure~\ref{fig:ablation-rank} to demonstrate the role of full-rank SVD. Essentially, we can incrementally raise the rank of the truncated SVD employed in GaLore until we achieve the effective rank. While this can in principle improve the performance of the algorithm, it requires additional tuning effort as discussed in Section~\ref{sec:projection}. 

For update frequency $T$, we illustrate its impact on our algorithm in Figure~\ref{fig:ablation-freq}. We found that the values $T=200, 500$ consistently yielded the best results across all our experiments. These period lengths are also practical in terms of computational cost, as they make the overhead induced by SVD negligible in our algorithm, as evidenced by the wall clock time measurements displayed in Figure~\ref{fig:llama}. In contrast, more frequent updates ($T=2,10$) degrade the performance significantly. This might stem from the algorithm having to excessively adapt to the instability introduced by minibatch training. A reasonable frequency should strike a balance between memorization and adaptation throughout the training process. Notably, in cases where the projection matrix was fixed ($T=10k, 20k$), we also observed improved results over Adam on language tasks. This finding suggests potential for future works to exploit the optimal subspace for applying moment estimates.

Regarding the comparison with SOAP, we found no significant performance differences relative to our algorithms, probably due to the connection discussed in Section~\ref{sec:related-works}.



\section{Discussion}
In this work, we proposed an efficient approach to improve adaptive moment-based optimizers by introducing a preconditioner diagonalization strategy. By leveraging an invertible transformation, we were able to enhance the reliability of the diagonal approximation used for the preconditioner matrix, resulting in a more effective estimation of second-order statistics. Our empirical evaluations demonstrated significant improvements in both convergence speed and final model performance across several standard tasks. Furthermore, our work also underscores the significance of employing structural preconditioning techniques to improve existing adaptive learning rate optimizers. Devising new preconditioners or exploring network reparameterization present promising approaches to this problem. In addition, it is vital to establish theoretically sound frameworks to better understand adaptive moment-based optimizers in dynamic subspaces. This would enable us to identify the optimal subspace where the moment estimates can be applied most effectively.


% References
\bibliography{uai2025}

\newpage

\onecolumn

\title{Improving Adaptive Moment Optimization via Preconditioner Diagonalization\\(Supplementary Material)}
\maketitle

\appendix

% \vspace{-1.2cm}

\section{Memory-efficient optimizers with factorized moment estimates}
% \vspace{-0.2cm}

\subsection{Adafactor optimizer}\label{app:adafactor}
% \vspace{-0.15in}
% \begin{algorithm}[ht]
% \caption{Adafactor for matrix parameter $\vv W$ of size $m \times n$, $m \leq n$.}
% \label{alg:adafactor}
% \begin{algorithmic}
% \STATE \textbf{Inputs:} moment decay coefficients $\beta_1, \beta_2$, smoothing term $\epsilon =  10^{-30}$, regularization constant $\lambda$
% \STATE \textbf{Initialization:} weight parameters $\vv W_0 \in \mathbb{R}^{m \times n}$, initial moments $\vv M_0, \vv r_0, \vv s_0 \leftarrow 0$
% \REPEAT
%     \STATE $t \leftarrow t+1$
%     \STATE $\vv G_t = \nabla \mathcal{L}(\vv W_{t}; \mathcal{B}_t)$
%     \STATE $\vv r_{t} = \hat{\beta}_{2t} \vv r_{t-1} + (1-\hat{\beta}_{2t}) \left[(\vv G_t)^2 + \epsilon \right] \vv 1_n $
%     \STATE $\vv s_{t} = \hat{\beta}_{2t} \vv s_{t-1} + (1-\hat{\beta}_{2t}) \left[(\vv G_t^\top)^2 + \epsilon \right] \vv 1_m $
%     \STATE $\vv V_{t} = \vv r_{t} \vv s_{t}^\top / (\vv 1_m^\top \vv r_{t})$
%     \STATE $\vv M_t = \hat{\beta}_{1t} \vv M_{t-1} + (1-\hat{\beta}_{1t}) \clip \left( \vv G_t / \left( \sqrt{\vv V_{t}} \right) \right)$
%     \STATE $\vv W_{t+1} = \vv W_{t} - \eta_{t} \left( \vv M_t +  \lambda \vv W_{t} \right)$
% \UNTIL \textit{stopping criterion is met}
% \RETURN optimized parameter $\vv W_t$
% \end{algorithmic}
% \end{algorithm}

\vspace{-0.3cm}

\begin{algorithm}[ht]
\caption{\textcolor{blue}{AdafacDiag} for matrix parameter $\vv W$ of size $m \times n$, $m \leq n$. When omitting the SVD step and setting the projection matrix $\vv{P}$ to the identity, we recover the Adafactor optimizer.}
\label{alg:adafacdiag}
\begin{algorithmic}
\STATE \textbf{Inputs:} moment decay coefficients $\beta_1, \beta_2$, smoothing term $\epsilon =  10^{-30}$, regularization constant $\lambda$
\STATE \textbf{Initialization:} weight parameters $\vv W_1 \in \mathbb{R}^{m \times n}$, initial moments $\vv M_0, \vv r_0, \vv s_0 \leftarrow 0$
\REPEAT
    \STATE $t \leftarrow t+1$
    \STATE $\vv G_t = \nabla \mathcal{L}(\vv W_{t}; \mathcal{B}_t)$
    \IF {$t \text{ mod } T = 0$}
        \STATE $\vv{P}_t, \ \_ , \ \vv{Q}_t^\top$ = \texttt{torch.linalg.svd}($\vv{G}_t$, \texttt{full\_matrices=True})
    \ELSE
        \STATE $\vv P_t, \vv Q_t^\top \leftarrow \vv P_{t-1}, \vv Q_{t-1}^\top $
    \ENDIF
    \STATE $\widetilde{\vv G}_t  = \vv P_t^\top \vv G_t$
    \STATE $\vv r_{t} = \hat{\beta}_{2t} \vv r_{t-1} + (1-\hat{\beta}_{2t}) \left[(\widetilde{\vv G}_t)^2 + \epsilon \right] \vv 1_n $
    \STATE $\vv s_{t} = \hat{\beta}_{2t} \vv s_{t-1} + (1-\hat{\beta}_{2t}) \left[(\widetilde{\vv G}_t^\top)^2 + \epsilon \right] \vv 1_m $
    \STATE $\vv V_{t} = \vv r_{t} \vv s_{t}^\top / (\vv 1_m^\top \vv r_{t})$
    \STATE $\vv M_t = \hat{\beta}_{1t} \vv M_{t-1} + (1-\hat{\beta}_{1t}) \clip \left( \widetilde{\vv G}_t / \left( \sqrt{\vv V_{t}} \right) \right)$
    \STATE $\vv W_{t+1} = \vv W_{t} - \eta_{t} \left( \vv P_t \vv M_t +  \lambda \vv W_{t} \right)$
\UNTIL \textit{stopping criterion is met}
\RETURN optimized parameter $\vv W_t$
\end{algorithmic}
\end{algorithm}

Adafactor~\citep{shazeer2018adafactor} proposed an efficient rank-1 parameterization for the second moment $\vv V$, which is widely adopted in adaptive optimization methods like RMSprop, Adam, and its variants. The factorization was derived by minimizing the total elementwise I-divergence subject to componentwise non-negative constraints: 
$$
\underset{\vv r \in \mathbb{R}^m, \vv s \in \mathbb{R}^n}{\text{minimize}} \sum_{i=1}^m \sum_{j=1}^n d(V_{ij}, r_is_j)
$$
in which $r_i \geq 0, s_j \geq 0$ and $d(p,q) = p \log \frac{p}{q} - p + q$.

Solving this problem results in a closed-form solution denoted by $\vv r = \vv V \vv 1_n, \vv s = \vv V^\top \vv 1_m / \vv r^\top \vv 1_n$. Intuitively, Adafactor tracks the moving averages of the row and column sums of squared gradients throughout iterations, yielding factored second-moment estimators $\vv r_t$ and $\vv s_t$. It then reconstructs a low-rank parameterization of the second-order momentum using a normalized outer product $\vv r_{t} \vv s_{t}^\top / (\vv 1_m^\top \vv r_{t})$. This method is computationally efficient and scalable, as it directly offers analytical formulations without requiring further approximations.

Incorporating Adafactor into our framework offers significant computational and memory efficiency benefits. This can be evident in Table~\ref{tab:optimizer_memory}, from which AdafacDiag demonstrates lower complexity in optimization states when compared to Adam. To further assess the effectiveness of this integration, we carried out experiments similar to those described in the main text. As shown in Figure~\ref{fig:adafacdiag} (top row), AdafacDiag (on ResNet50) and AdafacDiag with momentum (on ViTs) can outperform Adam by noticeable margins. The experiments on LLaMA-based models using the C4 dataset also delivered consistent results, presented in Figure~\ref{fig:llama-adafacdiag} and Table~\ref{tb:llama-adafacdiag}. These advantages highlight the potential of utilizing these algorithms in a wide range of real-world tasks, particularly in large-scale applications.


\subsection{Hfac optimizer}\label{app:hfac}

% \vspace{-0.4cm}

\begin{algorithm}[ht]
\caption{\textcolor{blue}{HfacDiag} for matrix parameters $\vv W$ of size $m \times n$, $m \leq n$. When omitting the SVD step and setting the projection matrix $\vv{P}$ to the identity, we recover the Hfac optimizer.}
\label{alg:hfac}
\begin{algorithmic}
\STATE \textbf{Inputs:} moment decay coefficients $\beta_1, \beta_2$, smoothing term $\epsilon$, and regularization constant $\lambda$
\STATE \textbf{Initialization:} weight parameters $\vv W_1 \in \mathbb{R}^{m \times n}$, initial factored moments $\vv u_0, \vv v_0, \vv r_0, \vv s_0 \leftarrow 0$
\REPEAT
    \STATE $\vv G_t = \nabla \mathcal{L}(\vv W_{t}; \mathcal{B}_t)$
    \IF {$t \text{ mod } T = 0$}
        \STATE $\vv{P}_t, \ \_ , \ \vv{Q}_t^\top$ = \texttt{torch.linalg.svd}($\vv{G}_t$, \texttt{full\_matrices=True})
    \ELSE
        \STATE $\vv P_t, \vv Q_t^\top \leftarrow \vv P_{t-1}, \vv Q_{t-1}^\top $
    \ENDIF
    \STATE $\widetilde{\vv G}_t  = \vv P_t^\top \vv G_t$
    \STATE $\vv u_{t} = \hat{\beta}_{1t} \vv u_{t-1} + (1-\hat{\beta}_{1t}) \widetilde{\vv G}_t \vv 1_n / n$
    \STATE $\vv v_{t} = \hat{\beta}_{1t} \vv v_{t-1} + (1-\hat{\beta}_{1t}) \widetilde{\vv G}_t^\top \vv 1_m / m$
    \STATE $\vv r_{t} = \hat{\beta}_{2t} \vv r_{t-1} + (1-\hat{\beta}_{2t}) \big[(\widetilde{\vv G}_t)^2 + \epsilon \big] \vv 1_n$
    \STATE $\vv s_{t} = \hat{\beta}_{2t} \vv s_{t-1} + (1-\hat{\beta}_{2t}) \big[(\widetilde{\vv G}_t^\top)^2 + \epsilon \big] \vv 1_m$
    \STATE $\widehat{\vv V}_{t} = \vv r_{t}\vv s_{t}^\top / (\vv 1_m^\top \vv r_{t})$
    \STATE $\phi_{term} = \hat{\beta}_{1t} \big(\vv u_{t} \vv 1_n^\top - \widetilde{\vv G}_t \vv 1_n \vv 1_n^\top / n \big) / \sqrt{\vv r_{t} \vv 1_n^\top /n}$
    \STATE $\psi_{term} = \hat{\beta}_{1t} \big(\vv 1_m \vv v_{t}^\top - \vv 1_m \vv 1_m^\top \widetilde{\vv G}_t / m \big) / \sqrt{\vv 1_m \vv s_{t}^\top /m}$
    \STATE $\vv W_{t+1} = \vv W_{t} - \eta_{t} \left [ \vv P_t \left( 0.5(\phi_{term} + \psi_{term}) + \clip \Big( \widetilde{\vv G}_t / \sqrt{\widehat{\vv V}_{t}} \Big) \right) + \lambda \vv W_{t} \right]$%
\UNTIL \textit{stopping criterion is met}
\end{algorithmic}
\end{algorithm}

\vspace{-0.3cm}

\begin{table}[h]
\centering
\setlength{\tabcolsep}{8pt}  % Increase the column separation
\caption{Memory requirements for different optimizers, with weight parameter of size $m \times n, m \leq n$. To estimate practical memory costs, we calculate the memory usage of optimization states, specifically the moment estimates, for each optimizer applied to various LLaMA-based models, using the BF16 format.}
\begin{tabular}{@{}llllllll@{}}
\toprule
\textbf{Optimizers} & \textbf{Weights} & \textbf{Gradient} & \textbf{Optim. States} & \textbf{60M} & \textbf{130M} & \textbf{350M} & \textbf{1.3B} \\ \midrule
Adam & $mn$ & $mn$ & $2mn$                          & 0.23G & 0.52G & 1.44G & 5.23G \\
\textcolor{blue}{AdaDiag} & $mn$ & $mn$ & $m^2 + 2mn$                 & 0.26G & 0.62G & 1.78G & 6.61G \\
\textcolor{purple}{AdaDiag++} & $mn$ & $mn$ & $m^2 + n^2 + 2mn$         & 0.36G & 0.97G & 3.03G & 11.59G \\
Adafactor w/ momentum & $mn$ & $mn$ & $mn+n$        & 0.18G & 0.36G & 0.85G & 2.87G \\
Adafactor w/o momentum & $mn$ & $mn$ & $mn$         & 0.06G & 0.10G & 0.13G & 0.26G \\
\textcolor{blue}{AdafacDiag} w/ momentum & $mn$ & $mn$ & $m^2+m+n$    & 0.21G & 0.45G & 1.19G & 4.25G \\
\textcolor{blue}{AdafacDiag} w/o momentum & $mn$ & $mn$ & $m^2+m+n$   & 0.09G & 0.19G & 0.47G & 1.63G \\
Hfac & $mn$ & $mn$ & $2(m+n)$                       & 0.13G & 0.19G & 0.26G & 0.52G \\
\textcolor{blue}{HfacDiag} & $mn$ & $mn$ & $m^2 + 2(m+n)$             & \textbf{0.16G} & \textbf{0.29G} & \textbf{0.60G} & \textbf{1.89G} \\
GaLore $rank-r=m/4$ & $mn$ & $mn$ & $mr + 2nr$          & 0.13G & 0.28G & 0.54G & 1.78G \\ \bottomrule
\end{tabular}
\label{tab:optimizer_memory}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.92\textwidth]{figures/imagenet_adafacdiag.pdf}
    \includegraphics[width=0.92\textwidth]{figures/imagenet_hfacdiag.pdf}
    \caption{Top-1 Accuracy of memory-efficient optimizers in pre-training ResNet50, ViT-B/32, and ViT-S/16 from scratch on the ImageNet1k.}
    \label{fig:adafacdiag}
\end{figure}
\vspace{-0.5cm}
\begin{table}[H]
\caption{Comparison of Adam, Adafactor \& AdafacDiag, Hfac \& HfacDiag on pre-training ResNets and ViTs architectures with ImageNet1k dataset. Top-1 accuracy on the validation set is reported. The bold values indicate noticeble improvements over the corresponding Adam results.}
\centering
\setlength{\tabcolsep}{20pt}  % Increase the column separation
\begin{tabular}{lccc}
\toprule
\textbf{Models}          & \textbf{ResNet50}        & \textbf{ViT-S/16}       & \textbf{ViT-B/32}        \\ \midrule
AdamW            & 75.61        & 78.35        & 72.20          \\
Adafactor        & 75.37        & 77.14        & 71.42          \\
\textcolor{blue}{AdafacDiag}       & 75.68        & 78.45        & 72.54          \\
\textcolor{purple}{AdafacDiag++}     & 75.60        & 78.56        & 72.78          \\
Adafactor w/ momentum          & -        & 78.44        & 72.31          \\
\textcolor{blue}{AdafacDiag} w/ momentum         & -        & \textbf{78.90}        & \textbf{73.24}          \\ 
\textcolor{purple}{AdafacDiag++}  w/ momentum      & -        & \textbf{78.66}        & \textbf{73.28}          \\  \midrule
Hfac             & 75.90        & 77.20        & 71.87 \\
\textcolor{blue}{HfacDiag}         & 75.78        & 78.20        & 72.76 \\
\bottomrule
\end{tabular}
\label{tb:image-adafacdiag}
\end{table}

Hfac~\citep{nguyen2024h} advances the memory-efficient optimizers by further decomposing the first moment into a rank-1 parameterization. This procedure involves projecting the full gradient onto rank-one spaces defined by column means and row means, then exponentially accumulating these statistics throughout the training process. Hfac can reduce the memory cost to a sublinear level, comparable to that of vanilla SGD without momentum, and still delivers favorable and competitive results across various architectures and datasets.

Specific updates are outlined in Algorithm~\ref{alg:hfac}. Intuitively, Adafactor can utilize a full moment $\vv M$, whereas Hfac decomposes $\vv M$ into vectors $\vv u$ and $\vv v$, leading to different normalizing mechanisms. While Adafactor normalizes the first-moment $\vv M$ using the second-moment approximation $\vv r \vv s^{\top} / \vv 1_m^\top \vv r$, Hfac scales the factored components of the first moment, $\vv u \vv 1_n^{\top}$ and $\vv 1_m \vv v^{\top}$, by their respective factored second-moment estimators, namely $\vv r_{t} \vv 1_n^\top /n$ and $1_m \vv s_{t}^\top /m$. 

\begin{table}[H]
\centering
\setlength{\tabcolsep}{20pt}  % Increase the column separation
\caption{Comparison of Adam and AdafacDiag on pre-training LLaMA models with the C4 dataset. Validation perplexity is reported for models with 60/130 Million parameters trained for 1.1/2.2 Billion training tokens. The bold values indicate improvements over the corresponding Adam results.}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Optimizer}} & \multicolumn{2}{c}{\textbf{Validation Perplexity}} \\ \cmidrule{2-4}
          & 60M~/~1.1B       & 130M~/~2.2B      & 350M~/~6.4B           \\ \midrule
Adam~\citep{kingma2014adam}     & 31.12             & 24.55         & 18.79         \\
\textcolor{blue}{AdafacDiag} (ours)               & 31.43             & \textbf{22.82}   & \textbf{18.15}      \\
\textcolor{blue}{AdafacDiag} w/ momentum (ours)   &\textbf{28.91}     & \textbf{22.54}  & \textbf{17.21}\\
Hfac~\citep{nguyen2024h}         & 31.41             & 24.59         & 19.34 \\
\textcolor{blue}{HfacDiag} (ours)                       & \textbf{30.30}             & \textbf{22.27}         & \textbf{17.50}
\\ \bottomrule
\end{tabular}
\label{tb:llama-adafacdiag}
\end{table}

Although Hfac employs the same derivation for second-moment factorization as in Adafactor, their parameter update schemes are fundamentally different. Adafactor, similar to Adam, updates parameters using the signal-to-noise ratio $\vv M / \sqrt{\vv V}$. On the other hand, Hfac adopts a momentum RMSprop-like approach, where parameter updates act as accumulators of normalized gradients. Both the momentum and the current gradient in Hfac are rescaled by their corresponding cumulative second-moment information.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.93\textwidth]{figures/llama_adafacdiag.pdf}
    \caption{Training progression of Adam and AdafacDiag for pre-training LLaMA models on C4 dataset.}
    \label{fig:llama-adafacdiag}
    
    \vspace{0.2cm}
    
    \includegraphics[width=0.93\textwidth]{figures/llama_hfacdiag.pdf}
    \caption{Training progression of Adam, Hfac, and HfacDiag for pre-training LLaMA models on C4 dataset.}
    \label{fig:llama-hfacdiag}
\end{figure}

Building on the competitive results of Hfac reported in~\cite{nguyen2024h}, we can expect that a naive combination with our preconditioning technique can further boost its performance. We refer to this integrated method as HfacDiag. To validate this hypothesis, we have evaluated HfacDiag and presented the results in Figure~\ref{fig:adafacdiag} (bottom row) and Figure~\ref{fig:llama-hfacdiag}. On the image classification task, HfacDiag performs on par with Adam and even significantly surpasses Adam on ResNet50. For the language modeling task, HfacDiag consistently outperforms Adam across all tested models, showcasing its robustness and reliability. Notably, HfacDiag maintains a reasonable computational cost and memory footprint, as demonstrated in Table~\ref{tab:optimizer_memory} and Figure~\ref{fig:llama-hfacdiag}. Specifically with memory usage, HfacDiag achieves the same efficiency as the low-rank GaLore while delivering substantial improvements in practical performance.

% \begin{table*}[t]
%     \centering
%     % \scriptsize % Reduces the font size of the entire table
%     \caption{Memory requirements for different optimizers, with weight parameter of size $m \times n, m \leq n$.}
%     \begin{tabular}{lccccccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Optimizers}} & \multirow{2}{*}{\textbf{Adam}} & \multirow{2}{*}{\textbf{AdaDiag}} & \multirow{2}{*}{\textbf{AdaDiag++}} & \multicolumn{2}{c}{\textbf{Adafactor}} & \multicolumn{2}{c}{\textbf{AdafacDiag}} \\
%         \cmidrule(lr){5-6} \cmidrule(lr){7-8}
%         & & & & \textbf{w/ m.} & \textbf{w/o m.} & \textbf{w/ m.} & \textbf{w/o m.} \\
%         \midrule
%         Weights & $mn$ & $mn$ & $mn$ & $mn$ & $mn$ & $mn$ & $mn$ \\
%         Gradient & $mn$ & $mn$ & $mn$ & $mn$ & $mn$ & $mn$ & $mn$ \\
%         Optim. States & $2mn$ & $m^2+2mn$ & $(m+n)^2$ & $mn+m+n$ & $m+n$ & $m^2 + mn + m + n$ & $m^2 + m + n$ \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:optimizer_memory}
% \end{table*}












\section{Hamiltonian function analyses}\label{app:hamiltonian}

% \subsection{Continuous-times Forms}
We first formalize a unified view of adaptive moment-based optimizers in the update scheme as:
\begin{equation}
    \vv W_{t+1} = \vv W_{t} + \phi_t(\vv S_t) \hspace{2cm} \vv S_t = \psi_t(\vv S_{t-1}, \nabla \mathcal{L}(\vv W_t)) \label{eq:general-form}
\end{equation}
where $\vv S_t$ is the optimization state, and $\phi_t, \psi_t$ are some mapping functions. One powerful approach to studying the dynamic behavior of these optimizers is to examine their continuous-time ODE forms in the limit of infinitesimal step size~\citep{maddison2018hamiltonian, gao2022global, chen2023lion}. It provides insights into the asymptotic convergence of the algorithms, abstracting away the choices of step size, discretization, and accumulation errors. 

We observe that the update~\ref{eq:general-form} can be discretized from the following continuous-time form:
\begin{equation}
\begin{split}
    \frac{\mathrm{d}}{\mathrm{d}t} \vv W_t &= \partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t) - \Phi(\partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t)) \\
    \frac{\mathrm{d}}{\mathrm{d}t} \vv S_t &= -\partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t) - \Psi(\partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t)) 
\end{split}
\label{eq:general-ode}
\end{equation}
where $\mathcal{H}(.)$ is a Hamiltonian function that satisfies:
\begin{equation*}
    \min_{\vv S} \mathcal{H}(\vv W, \vv S) = \mathcal{L}(\vv W) \quad \forall \vv W,
\end{equation*}
meaning that minimizing $\mathcal{H}(\vv W, \vv S)$ will reduce to minimizing the original objective $\mathcal{L}(\vv W)$. Additionally, $\Phi, \Psi$ are two monotonic mapping satisfying:
$$
\|\vv A \|_\Phi^2 = \langle \vv A, \Phi(\vv A) \rangle \geq 0, \hspace{2cm} 
\|\vv A \|_\Psi^2 = \langle \vv A, \Psi(\vv A) \rangle \geq 0, \quad \forall \vv A
$$
The key properties is that $\mathcal{H}(\vv W, \vv S)$ is monotonically non-decreasing along the trajectory~\ref{eq:general-ode}:
\begin{align*}
     \frac{\mathrm{d}}{\mathrm{d}t} \mathcal{H}(\vv W_t, \vv S_t) &= \left \langle \partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t),  \frac{\mathrm{d}}{\mathrm{d}t} \vv W_t \right \rangle + 
    \left \langle \partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t),  \frac{\mathrm{d}}{\mathrm{d}t} \vv S_t \right \rangle \\
    &= - \left \langle \partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t),  \Phi(\partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t)) \right \rangle - 
    \left \langle \partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t),  \Psi(\partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t)) \right \rangle \\
    &= - \|\partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t) \|_\Phi^2 - \|\partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t) \|_\Psi^2 \leq 0,
\end{align*}
where we cancel out the cross terms $\left \langle \partial_{\vv W} \mathcal{H}(\vv W_t, \vv S_t), \partial_{\vv S} \mathcal{H}(\vv W_t, \vv S_t) \right \rangle$ to get the second equation.
% Let's take Adam optimizer as a specific example, we have its ODE form represented as:

This powerful framework covers a wide range of optimizers, including moment-based algorithms as follows:

\textbf{\textit{Momentum SGD}}:
\begin{equation*}
\dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = - \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \alpha (\vv G_t - \vv M_t), \quad \vv G_t = \nabla \mathcal{L}(\vv W_t)
\end{equation*}
\textit{with the Hamiltonian function is defined by}: 
$
    \mathcal{H}(\vv W, \vv M) = \mathcal{L}(\vv W) + \dfrac{1}{2\alpha} \left \langle \vv M, \vv M  \right \rangle.
$

\textbf{\textit{Adam}}~\citep{kingma2014adam}:
\begin{equation*}
\dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \vv G_t - \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = \vv G_t^2 - \vv V_t, \quad \vv G_t = \nabla \mathcal{L}(\vv W_t)
\end{equation*}
\textit{with the Hamiltonian function is defined by}: 
$
    \mathcal{H}(\vv W, \vv M, \vv V) = \mathcal{L}(\vv W) + \dfrac{1}{2} \left \langle \dfrac{\vv M}{\sqrt{\vv V} + \epsilon}, \vv M  \right \rangle.
$

\textbf{\textit{Adafactor}}:~\cite{shazeer2018adafactor}
\begin{equation*}
    \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\frac{\vv M_t}{\sqrt{\vv r_t \vv s_t^\top / \vv 1_m^\top \vv r_t}}, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \vv G_t - \alpha \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv r_t = (\vv G_t)^2 \vv 1_n - \alpha \vv r_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv s_t = (\vv G_t^\top)^2 \vv 1_m - \alpha \vv s_t 
\end{equation*}
\textit{with the Hamiltonian function is defined by}: 
$
\mathcal{H}(\vv W, \vv M, \vv r, \vv s) = f(\vv W) + \dfrac{1}{2} \left\langle \dfrac{\vv M}{\sqrt{\vv r \vv s^\top / \vv 1_m^\top \vv r}}, \vv M \right\rangle.
$

\textbf{\textit{Hfac}}~\citep{nguyen2024h}:
\begin{align*}
    \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t &= - \frac{\vv G_t}{\sqrt{\vv r_t \vv s_t^\top / \vv 1_m^\top \vv r_t}} - \frac{1}{2} \left[\frac{\vv u_t \vv 1_n^\top - \vv G_t \vv 1_n \vv 1_n^\top / n }{\sqrt{\vv r_t \vv 1_n^\top }} + \frac{\vv 1_m \vv v_t^\top - \vv 1_m \vv 1_m^\top \vv G_t / m }{\sqrt{\vv 1_m \vv s_t^\top}} \right]  \nonumber \\ 
    \dfrac{\mathrm{d}}{\mathrm{d}t} \vv u_t &= \vv G_t \vv 1_n / n - \alpha \vv u_t, \quad \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv v_t = \vv G_t^\top \vv 1_m / m - \alpha \vv v_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv r_t = (\vv G_t)^2 \vv 1_n  - \alpha \vv r_t, \quad \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv s_t = (\vv G_t^\top)^2 \vv 1_m - \alpha \vv s_t \nonumber \nonumber   
\end{align*}
\textit{with the Hamiltonian function is defined by}: 
$
\mathcal{H}(\vv W, \vv M, \vv r, \vv s) = f(\vv W) + 
= \dfrac{1}{4} \left\langle \dfrac{\vv u \vv 1_n^\top}{\sqrt{\vv r \vv 1_n^\top}}, \vv u \vv 1_n^\top \right\rangle
+ \dfrac{1}{4} \left\langle \dfrac{\vv 1_m \vv v^\top}{\sqrt{\vv 1_m \vv s^\top}}, \vv 1_m \vv v^\top \right\rangle.
$



\textit{Proof}: Let's take Adam optimizer as a specific example, we have the derivative:
\begin{align*}
    \dfrac{\mathrm{d}}{\mathrm{d}t} &\mathcal{H}(\vv W_t, \vv M_t, \vv V_t) \\
    &= \left \langle \vv G_t,  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t \right \rangle + 
    \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon},  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t \right \rangle \\
    &= \left \langle \vv G_t, - \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}  \right \rangle + 
    \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon},  \vv G_t - \vv M_t \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \vv G_t^2 - \vv V_t \right \rangle \\
    &= - \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}, \vv M_t \right \rangle + \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{ \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \sqrt{\vv V_t} \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \vv G_t^2  \right \rangle \\
    &= - \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}, \vv M_t \right \rangle + \dfrac{1}{4} \left \langle \dfrac{\vv M_t}{ \sqrt{\vv V_t} + \epsilon} \odot \dfrac{\sqrt{\vv V_t}}{ \sqrt{\vv V_t} + \epsilon},  \vv M_t \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \vv G_t^2  \right \rangle \\
    &\leq - \dfrac{1}{4}
    \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \vv G_t^2  \right \rangle \leq 0.
\end{align*}

A similar analysis can be applied to the AdaDiag and AdaDiag++. In AdaDiag's case, we have the continuous form:
\begin{equation}
    \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t = -\vv P_t \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t = \vv P_t^\top \vv G_t - \vv M_t, \quad \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t = (\vv P_t^\top \vv G_t)^2 - \vv V_t,
\end{equation}
yielding the Hamiltonian function 
$
    \mathcal{H}(\vv W, \vv M, \vv V) = \mathcal{L}(\vv W) + \dfrac{1}{2} \left \langle \dfrac{\vv M}{\sqrt{\vv V} + \epsilon}, \vv M  \right \rangle
$, for which:
\begin{align*}
    \dfrac{\mathrm{d}}{\mathrm{d}t} &\mathcal{H}(\vv W_t, \vv M_t, \vv V_t) \\
    &= \left \langle \vv G_t,  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv W_t \right \rangle + 
    \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon},  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv M_t \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \dfrac{\mathrm{d}}{\mathrm{d}t} \vv V_t \right \rangle \\
    &= \left \langle \vv G_t, -\vv P_t \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}  \right \rangle + 
    \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon},  \vv P_t^\top \vv G_t - \vv M_t \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  (\vv P_t^\top \vv G_t)^2 - \vv V_t \right \rangle \\
    &= - \left \langle \dfrac{\vv M_t}{\sqrt{\vv V_t} + \epsilon}, \vv M_t \right \rangle + \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{ \left( \sqrt{\vv V_t} + \epsilon \right)^2},  \sqrt{\vv V_t} \right \rangle -
    \dfrac{1}{4} \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  (\vv P_t^\top \vv G_t)^2  \right \rangle \\
    &\leq - \dfrac{1}{4}
    \left \langle \dfrac{\vv M_t^2}{\sqrt{\vv V_t} \odot \left( \sqrt{\vv V_t} + \epsilon \right)^2},  (\vv P_t^\top \vv G_t)^2  \right \rangle \leq 0.
\end{align*}
This result implies that the points in the limit set:
$$
\mathcal{I} = \{\text{the union of complete trajectories satisfying } \dfrac{\mathrm{d}}{\mathrm{d}t} \mathcal{H}(\vv W_t, \vv M_t, \vv V_t) = 0  \}
$$
must satisfy $ \vv P_t^\top \vv G_t \equiv 0$. Since $\vv P_t$ is a full-rank orthogonal matrix, we have $\vv G_t \equiv 0$, indicating that the optimization algorithm converges to a local optimum.

For AdafacDiag and HfacDiag, the proofs can be derived in a similar manner by incorporating the analyses of Adafactor and Hfac provided in~\cite{nguyen2024h}. It is worth mentioning that our algorithms can also be regarded as a special case of Online Subspace Descent with Generalized Linear Projection~\citep{liang2024memory}. In their work, however, to provide the convergence guarantee for arbitrary update rules of the projection matrix $\vv P_t$, the authors need a mild condition to avoid the degenerate case of $\vv P_t \vv G_t = 0$ while $\vv G_t = 0$ in the invariant set of the system. In our analysis, this assumption is not required when $\vv P_t$ is a full-rank orthogonal matrix.


% \subsection{Convergence to Local Optima}




\section{Hyperparameter Settings}\label{app:hyper-setting}
For all optimizers, we used decay moment coefficients $(\beta_1, \beta_2)=(0.9, 0.999)$ along with the bias-correction steps, the smoothing constant $\epsilon = 10^{-8}$. Specifically with AdaDiag, AdaDiag++, we use the SVD frequency $T=500, 200$ for image and language tasks, respectively. 

For image classification, we used $(lr, \lambda) = (0.001, 0.1)$ and $(0.0003, 0.1)$ for all ResNets and ViTs experiments, respectively.

For language modeling, we tuned the learning rate over the set \{0.003, 0.001, 0.0003, 0.0001\} and selected the optimal value based on validation perplexity. The specific settings are summarized in the Table~\ref{tab:llama_settings}.


\begin{table}[ht]
    \centering
    \caption{Training configuration for LLaMA models.}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{LLaMA models} & \textbf{Tokens} & \textbf{Training Steps} & \textbf{Warmup Steps} & \textbf{Learning Rate} \\
        \midrule
        60M & 1.3B & 10K & 1K & 0.003 \\
        130M & 2.6B & 20K & 2K & 0.001 \\
        350M & 7.8B & 60K & 6K & 0.001 \\
        \bottomrule
    \end{tabular}
    \label{tab:llama_settings}
\end{table}




\section{Additional Figures}\label{app:more-figures}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/imagenet_vit_full.pdf}
    \caption{Top-1 Accuracy of optimizers in pre-training (to the end) ResNet50, ViT-B/32, and ViT-S/16 from scratch on the ImageNet1k.}
    \label{fig:vit-full}
\end{figure}

\newpage 
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/off_diagonal_density_plots.pdf}
%     \caption{Histograms of off-diagonal elements $\COV (\vv G_\tau)$ (original) and $\COV (\widetilde{\vv G}_\tau)$ (\textit{one-sided} projection, AdaDiag), corresponding to the two first layers of ResNet50 trained on ImageNet1k. In this experiment, we set the frequency $T = 500$ and plot histograms at iterations with and without SVD applied.}
%     \label{fig:hist-onesided}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/off_diagonal_density_plots_galore.pdf}
%     \caption{Histograms of off-diagonal elements $\COV (\vv G_\tau)$ (original) and $\COV (\widetilde{\vv G}_\tau)$ (\textbf{GaLore}), corresponding to the two first layers of ResNet50 trained on ImageNet1k. Compared to the full-rank case, the low-rank GaLore projection $(r=\min\{m,n\}/2)$ does not exhibit a discernible sparsity structure in the off-diagonal elements. GaLore even creates off-diagonal elements with larger magnitudes. Note that the support was truncated for better visualization.}
%     \label{fig:hist-galore}
% \end{figure}


\begin{figure}[ht]
    \centering
    % First figure
    \includegraphics[width=0.88\textwidth]{figures/off_diagonal_density_plots.pdf}
    \caption{Histograms of off-diagonal elements $\COV (\vv{G}_\tau)$ (original) and $\COV (\widetilde{\vv{G}}_\tau)$ (\textit{one-sided} projection, AdaDiag), corresponding to the two first layers of ResNet50 trained on ImageNet1k. In this experiment, we set the frequency $T = 500$ and plot histograms at iterations with and without SVD applied.}
    \label{fig:hist-onesided}

    \vspace{0.5cm} % Adjust vertical space if needed

    % Second figure
    \includegraphics[width=0.88\textwidth]{figures/off_diagonal_density_plots_galore.pdf}
    \caption{Histograms of off-diagonal elements $\COV (\vv{G}_\tau)$ (original) and $\COV (\widetilde{\vv{G}}_\tau)$ (\textbf{GaLore}), corresponding to the two first layers of ResNet50 trained on ImageNet1k. Compared to the full-rank case, the low-rank GaLore projection $(r=\min\{m,n\}/2)$ does not exhibit a discernible sparsity structure in the off-diagonal elements. GaLore even creates off-diagonal elements with larger magnitudes. Note it was truncated for better visualization.}
    \label{fig:hist-galore}
\end{figure}



\end{document}
