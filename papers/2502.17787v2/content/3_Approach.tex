\section{Approach}

Our approach mainly consists of two steps: 1) Initial Instruction Generation; 2) Iterative Instruction Refinement, as shown in Figure \ref{fig:main}. In this section, we will introduce the two steps in detail.

\begin{figure*}[!t]
    \centering % 将图居中
    \includegraphics[width=1.0\linewidth]{source/main_final.pdf}
   %\vspace{-0.1cm}
    \vspace{-3mm}
    \caption{
     \textbf{AIR}: Automatic Iterative Refinement Framework. 
     % In the IIG phase, large number of documents are collected for clustering and deduplication, followed by quality filtering based on scores assigned by the guidance model. Initial instructions are then generated using back translation. In the IIR phase, refined documents and model responses from each iteration are used to generate constraints, ensuring compliance with the requirements stated in Section \ref{sec:IIR}: the model response misaligns with the constraints, whereas the refined documents do; the new responses generated from the constraints and the original instruction still misaligns with the constraints. This iterative process continues until a final response is produced.
    }
    \vspace{-5mm}
    \label{fig:main}
    % \vspace{-0.2cm}
\end{figure*}

\subsection{Initial Instruction Generation (IIG)}

\paragraph{Document Collection.}
Traditional instruction generation methods such as Self-Instruct \cite{wang2022self} often suffer from limited diversity, as the generated instructions are generally re-combinations of the provided few-shot examples. Inspired by the work by ~\citet{li2023self}, we generate initial instructions using back translation based on human-written documents.

%The scarcity of instruction data poses a significant challenge for LLM applied on new domains. Traditional methods such as Self-Instruct \cite{wang2022self} often suffer from limited diversity, as the generated instructions are generally re-combinations of the provided few-shot examples. 
%To address this issue, we propose a novel back-translation approach to construct initial instructions based on human-written documents.


To further enhance the diversity of the generated instructions, we implement a density-based sampling mechanism for documents, as shown in Algorithm \ref{Alg:density}. Specifically, we convert documents into vector representations based on Sentence-Transformers\footnote{\texttt{sentence-transformers/all-MiniLM-L6-v2}.}, and perform sampling to maximize the density of samples in the representation space.

\begin{algorithm}[t]
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Density-based Sampling}
  \label{alg:1}
  \begin{algorithmic}[1]
      \REQUIRE Instruction Dataset $D$ with $m$ samples.
      \ENSURE Selected Dataset $D'$ with $n$ samples.
      \STATE Derive the embeddings for each sample in $D$.
      \STATE Random sample one data point $x$ from $D$.
      \STATE Delete $x$ from $D$, add $x$ to $D'$.
      \FOR{$i = 1, 2, ..., t$}
          \STATE Calculate the cosine similarity score between $x$ and each sample from $D$.
          \STATE Select the least similar sample $x'$ from $D'$.
          \STATE Let $x$ = $x'$.
          \STATE Delete $x$ from $D$, add $x$ to $D'$. 
      \ENDFOR
  \end{algorithmic}
\label{Alg:density}
\end{algorithm}
% \vspace{-10mm}

In this way, we effectively eliminate redundant documents, enhancing the diversity of instructions. Moreover, this approach ensures that the knowledge introduced during instruction fine-tuning is evenly distributed across various domains. This not only prevents the model from overfitting to a specific domain but also mitigates the risk of catastrophic forgetting of fundamental capabilities.

Moreover, to further ensure the quality of the document collection, we filter out documents based on the following criteria: 1) Length: Documents with fewer than 50 words or exceeding 2,048 words are removed. 2) Symbol-to-text ratio: Documents where the proportion of symbols exceeds that of textual content are excluded. 3) Redundancy: Documents containing repetitive paragraphs or excessive symbol repetitions are eliminated.

\paragraph{Instruction Back-translation}
Based on the sampled documents, we employ the back-translation method to construct initial instructions.
Specifically, we utilize a guidance model to predict an instruction which can be accurately answered by (a portion of) the document\footnote{Detailed prompt templates are presented in Appendix \ref{appendix:prompt_iig}.}. This enables the generation of new instructions without relying on few-shot examples or pre-designed rules. Moreover, we can further ensure the diversity of the generated instructions by diversifying the documents.

However, despite being constructed from the document, the instruction do not always align well with the document in two key aspects \cite{nguyen2024better}. First, the document is unstructured and does not follow the AI-assistant format. Second, it may contain content irrelevant to the instruction. Therefore, we introduce an additional refinement step to transform the document into response format and remove irrelevant content.

To further ensure the quality of the instructions, we introduce a scoring step to filter out low-quality data. Each instruction is assigned a score on a scale of 1 to 5 by the guidance model, with each point corresponding to a specific aspect. Only instructions with a score greater than (or equal to) 4 are retained for the next step\footnote{Results of instruction score are presented in Appendix \ref{appendix:ins_score_case}.}.

\subsection{Iterative Instruction Refinement (IIR)}
\label{sec:IIR}
To enhance a model’s ability to follow complex instructions, it is crucial to construct complex instruction data that incorporates multiple constraints. Previous methods typically start with simple instructions and generate complex ones through rewriting or recombination~\cite{xu2023wizardlm}. However, the constraints generated in this way often do not meet actual needs or lack diversity.
%Therefore, we iteratively refine instructions by emulating the real-world process where people iteratively review responses and add constraints to address their deficiencies. This approach helps in developing more effective and challenging instructions.
%In practice, constructing complex instructions usually involves iteratively reviewing the response and adding constraints to address its deficiencies.

An effective sample for complex instruction fine-tuning should adhere to two key principles:

\begin{enumerate}[itemsep=1mm, parsep=0pt]
    \item Whether the model’s response originally misaligns with constraint before it is added;
    \item Whether the model’s response still misaligns with the constraint after it is added.
\end{enumerate}

These constraints highlight the model’s weaknesses in handling complex instructions and require further improvement. Conversely, if a constraint does not meet these principles, it indicates that the constraint falls within the model’s current capabilities and does not require additional learning.

Therefore, we introduce constraint generation with LLM-as-judge guidance \cite{zheng2023judging}, which mimics the human process of iteratively refining prompts to form complex instructions\footnote{Detailed prompt templates are presented in Appendix \ref{appendix:prompt_iir}.}. As shown in Algorithm \ref{alg:iir}, during the process of iteration, we obtain the constraints that the model fails to satisfy, which require further fine-tuning.

% With the refined document as $R$, this method expands simple instructions $Q_o$ into complex ones through the following steps\footnote{Detailed prompt templates are presented in Appendix \ref{appendix:prompt_iir}.}:

\begin{algorithm}[h]
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Iterative Instruction Refinement}
  \label{alg:1}
  \begin{algorithmic}[1]
      \REQUIRE Guidance model $M$, current model $m$, refined document $R$, initial instruction $I_0$.
      \ENSURE Constraint Sets $C_n$ and $C_n'$.
      \FOR{$i = 1, 2, ..., n$}
          \STATE Use $m$ to generate a response $A_i$ for the previous instruction $I_{i-1}$.
          \STATE Leverage $M$ as the judge, compare $A_i$ with $R$ to identify a new constraint $c_i$.
          \STATE Add $c_i$ to $C_n$.
          \STATE Add $c_i$ to $I_{i-1}$ to form a new instruction $I_i$.
          \STATE Use $m$ to generate a response $A_{i}'$ for $I_i$.
          \STATE Leverage $M$ as the judge, check whether $A_i'$ satisfies constraint $c_i$. If not, add $c_i$ to $C_n'$. 
      \ENDFOR
  \end{algorithmic}
\label{alg:iir}
\end{algorithm}

% \begin{enumerate}[itemsep=1mm, parsep=0pt]
%     \item Leverage current model to generate a answer $A_i$ for the instruction $Q_i$.
%     \item Leveraging a guidance model as the judge, compare the refined document $R$ with the model's response to identify a new constraint $c_i$.
%     \item Add the constraints to the question $Q_{i-1}$ to construct a new question $Q_i$.
%     \item Answer $Q_i$ using the current model to derive answer $A_i$.
%     \item Determine whether the constraint $C_i$ is satisfied by $A_i$. If not, add the constraint to $C_n$.
%     \item Repeat the above three steps for multiple rounds to uncover increasingly complex constraints, thereby increasing the difficulty and depth of the instructions.
% \end{enumerate}

Throughout this process, as the number of constraints increases, the model's response also improves, making the identification of new constraints more challenging. To uncover constraints that better reflect human preferences, we use the refined document as the reference answer for the judgment process. Human-written documents inherently contain vast amounts of knowledge and formatting conventions that reflect human preferences. Therefore, the derived constraints will also align more closely with human preferences.

Finally, the constraint set is merged into a new complex instruction. Notice two constraint sets are derived: the first set $C_n$ satisfies Principle 1, while the second set $C_n'$, which includes an additional checking step, satisfies both Principle 1 and 2\footnote{The effect of the checking step is shown in Section \ref{sec:judge}.}.

While we leverage the refined document as the reference for the judgment process, it should not be used as the target for fine-tuning as in \citet{nguyen2024better}, as the document is not refined with the constraints presented explicitly. Therefore, we leverage the guidance model to re-generate the response based on the combined instruction\footnote{A detailed example illustrating the complete pipeline is provided in Appendix \ref{appendix:pipeline_case}.}.


\subsection{Data Statistics of AIR-10K}
% To enhance the development of complex instruction following, we construct a high-quality complex instruction dataset \textbf{AIR-10K}.
% We present the statistics of \textbf{AIR-10K}.

% 指令的Domain分布 & loop1和loop5产生的约束类型的扇形图
\begin{figure}[t]
\centering
\subfigure[Distribution of domains]{
    \includegraphics[width=0.6\linewidth]{source/data_visualization/domain_distribution.pdf}
}
\subfigure[Distribution of constraint types in iteration 1 and 5]{
    \includegraphics[width=0.95\linewidth]{source/data_visualization/constraint_type_distribution_qwen_ultrachat_loop1_5.pdf}
}
\vspace{-2mm}
\caption{Data statistics of AIR-10K. }
\vspace{-5mm}
\label{figure:constraint_and_domain}
\end{figure}

% 指令和回复的长度的柱状图
\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{source/data_visualization/length_distribution.pdf}
    \vspace{-3mm}
    \caption{Length distribution of AIR-10K.}
    \vspace{-3mm}
    \label{figure:length_distribution}
\end{figure}
% \begin{figure}[t]
% \centering
% \subfigure[Instruction token length distribution]{
%     \includegraphics[width=0.9\linewidth]{source/data_visualization/4.instruction_length_distribution.pdf}
% }
% \subfigure[Output token length distribution]{
%     \includegraphics[width=0.9\linewidth]{source/data_visualization/4.output_length_distribution.pdf}
% }
% \vspace{-5mm}
% \caption{Token length analysis of the generated dataset.}
% \vspace{-5mm}
% \label{figure:length_distribution}
% \end{figure}

With our proposed framework, we constructed a high-quality complex instruction dataset, \textbf{AIR-10K}, based on openly available documents. We present the real-life scenario-specific domain distribution of AIR-10K in Figure \ref{figure:constraint_and_domain}(a). As can be seen, our dataset encompasses nearly 20 different domains in total, demonstrating a high degree of balance across diverse fields. Furthermore, we present the distribution of constraint types during iteration 1 and 5 in Figure \ref{figure:constraint_and_domain}(b). It is evident that in iteration 1, \textit{Inclusion} and \textit{Document Structure} constraints dominate. However, after four rounds of constraint additions, by iteration 5, the proportions of each constraint type become more uniform\footnote{The constraint type definition and complete distributions across all iterations are detailed in Appendix~\ref{appendix:constraints_type}.}.

We also analyze the length distributions of both instructions and responses. As shown in Figure \ref{figure:length_distribution}(a) and \ref{figure:length_distribution}(b), our instructions are of substantial information for capturing complex tasks.
% the instruction lengths are concentrated between 75 and 150 tokens, while the response lengths primarily range from 400 to 1000 tokens.
% \vspace{-20mm}
