% \vspace{-20mm}
\section{Experiments}
\subsection{Set-up}

\paragraph{Data.} 
Following \citet{nguyen2024better}, we utilize a subset of Dolma v1.7 \cite{dolma-2024} as the document source, which is derived from a collection of web pages and has undergone rigorous quality and content filtering to ensure data quality.

\paragraph{Models.} 
We apply our method on two models, Llama-3-8B and Qwen2.5-7B, and we apply preliminary supervised fine-tuning for both models. The preliminary fine-tuning process is conducted on two general instruction datas, namely ultrachat-200k \cite{ding2023enhancing} and tulu-330k \cite{lambert2024tulu3}, respectively. For the guidance model to construct the data, we rely on a larger model with the same group to ensure data quality, namely Qwen-2.5-72B-Instruct for Qwen-2.5-7B, and Llama-3-70B-Instruct for Llama-3-8B. We set the maximum number of iterations to 5.




\paragraph{Evaluation.} 
We mainly conduct evaluation on two complex instruction-following benchmarks, \textbf{CFBench}~\cite{zhang2024cfbench} and \textbf{FollowBench}~\cite{jiang2023followbench}, where instructions consist of multiple constraints. We also conduct evaluations on a general instruction benchmark of \textbf{AlpacaEval2}~\cite{dubois2024length}. Note that all benchmarks require GPT-4 for judgment, and we use GPT-4o-0806 \footnote{\url{platform.openai.com/docs/models/gp\#gpt-4o}} as the evaluator for all of them. We also conduct evaluation on fundamental capability benchmarks, including math, code, and knowledge tasks, and the results are presented in Appendix \ref{app: fundamental} due to space limitation.

\paragraph{Baselines.}
We mainly compare our method with four groups of methods as follows:

\begin{enumerate}[itemsep=1mm, parsep=0pt, leftmargin=*]
    \item \textbf{Human crafted instruction data}: This includes ShareGPT\footnote{\url{huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}}, which is a collections of real human-AI conversations.
    \item \textbf{Automatic crafted general instruction data}: This includes Self-Instruct \cite{wang2022self}, which leverages few-shot examples to self-generate simple instruction samples.
    \item \textbf{Automatic rewritten complex instruction data}: This includes Evol-Instruct \cite{xu2023wizardlm}, ISHEEP \cite{isheep}, Muffin \cite{lou2023muffin} and Conifer \cite{sun2024conifer}, which initiate with simple instructions and progressively construct more complex ones through rewriting or recombination.
    \item \textbf{Automatic back-translated complex instruction data}: This includes Suri \cite{pham2024suri} and Crab \cite{qi2024constraint}, which curate the complex instructions and constraints by back-translating the pre-existing response. These methods are the most closest to our work.
    
    % \item ISHEEP. This method self-creates additional instruction-output pair by synthesize and filter iteratively.
    % \item Evol-Instruct \cite{xu2023wizardlm}: This method rewrites simple instructions step by step into more complex ones by in-depth and in-breadth evolving.
    % \item Suri \cite{pham2024suri}: This method leverages human-written long-form text to generate instructions that could have been followed to create the text.
    % \item Muffin \cite{lou2023muffin}: This method automatically scale tasks per input by diversifying these tasks with various input facets.
    % \item Conifer \cite{sun2024conifer}: This method curate the dataset by a series of LLM-driven refinement, including reframing, generation, recombination, and filtering.
    % \item Crab \cite{qi2024constraint}: This method take the high-quality instruction-response pairs and add complex constraints already met by the responses to the instructions.
\end{enumerate}

Additionally, we also compare with the original back-translation \cite{cao2023instruction} and back-and-forth \cite{nguyen2024better}, where IIR is skipped and initial instructions are directly used.

Note that for all constructed datasets, we sample 10k instruction-response pairs for supervised fine-tuning under the same hyper-parameters\footnote{Detailed hyper-parameters are presented in Appendix \ref{appendix:hyper-parameters}.}.


\begin{table*}[!ht]
\centering
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{0.85}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{c|ccc|cc|cc}
\toprule
\multicolumn{8}{c}{\multirow{2}{*}{\raisebox{1.4em}{\makecell{\textbf{Fine-tuned on Llama-3-8B-UltraChat}}}}} \\
\midrule
\multirow{2}{*}{\raisebox{-0.4em}{\textbf{Method}}} & \multicolumn{3}{c}{\textbf{CF-Bench}}      & \multicolumn{2}{c}{\textbf{FollowBench}} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& \textbf{CSR} & \textbf{ISR} & \textbf{PSR} & \textbf{HSR}        & \textbf{SSR}       & \textbf{LC.}        & \textbf{Len}        \\
\midrule
Baseline    & 0.51         & 0.15         & 0.22         & 41.04               & 57.39              & 8.86               & 1,017     \\ 
\midrule
% \midrule
%back-translation     & 0.40      & 0.11      & 0.15    & 21.19      & 33.92        & 0.96        & 2,966   \\
back-translation & $\text{0.40}_{\textcolor{deepred}{\text{-0.11}}}$ & $\text{0.11}_{\textcolor{deepred}{\text{-0.04}}}$ & $\text{0.15}_{\textcolor{deepred}{\text{-0.07}}}$ & $\text{21.19}_{\textcolor{deepred}{\text{-19.85}}}$ & $\text{33.92}_{\textcolor{deepred}{\text{-23.47}}}$ & $\text{0.96}_{\textcolor{deepred}{\text{-7.90}}}$ &  2,966    \\

%back-and-forth       & 0.58         & 0.20         & 0.27         & 44.65           & 61.58         & 10.06        & 1,440   \\
back-and-forth & $\text{0.58}_{\textcolor{deepgreen}{\text{+0.07}}}$ & $\text{0.20}_{\textcolor{deepgreen}{\text{+0.05}}}$ & $\text{0.27}_{\textcolor{deepgreen}{\text{+0.05}}}$ & $\text{44.65}_{\textcolor{deepgreen}{\text{+3.61}}}$ & $\text{61.58}_{\textcolor{deepgreen}{\text{+4.19}}}$ & $\text{10.06}_{\textcolor{deepgreen}{\text{+1.20}}}$ &  1,440    \\
\midrule

% \midrule
%ShareGPT                         & 0.62         & 0.22         & 0.32         & 40.99               & 58.59             & 8.36               & 1,052   \\ 
ShareGPT & $\text{0.62}_{\textcolor{deepgreen}{\text{+0.11}}}$ & $\text{0.22}_{\textcolor{deepgreen}{\text{+0.07}}}$ & $\text{0.32}_{\textcolor{deepgreen}{\text{+0.10}}}$ & $\text{40.99}_{\textcolor{deepred}{\text{-0.05}}}$ & $\text{58.59}_{\textcolor{deepgreen}{\text{+1.20}}}$ & $\text{8.36}_{\textcolor{deepred}{\text{-0.50}}}$ &  1,052    \\
\midrule


% \midrule
%Self-Instruct                    & 0.34         & 0.08         & 0.10         & 12.33               & 26.92              & 2.76               & 384    \\ 
Self-Instruct & $\text{0.34}_{\textcolor{deepred}{\text{-0.17}}}$ & $\text{0.08}_{\textcolor{deepred}{\text{-0.07}}}$ & $\text{0.10}_{\textcolor{deepred}{\text{-0.12}}}$ & $\text{12.33}_{\textcolor{deepred}{\text{-28.71}}}$ & $\text{26.92}_{\textcolor{deepred}{\text{-30.47}}}$ & $\text{2.76}_{\textcolor{deepred}{\text{-6.10}}}$ &  384     \\
\midrule


% \midrule
% Evol-Instruct       & 0.57         & 0.22         & 0.28         & 43.58           & 59.21              & 7.15               & 903        \\
% MUFFIN     & 0.50         & 0.16         & 0.22         & 30.88               & 48.48              & 4.51               & 791                 \\
% Conifer        & 0.57         & 0.22         & 0.28         & 47.06               & 61.32              & 12.81              & 1,084                \\
% I-SHEEP           & 0.53         & 0.17         & 0.23         & 34.26               & 50.28              & 5.41               & 838                 \\ 
Evol-Instruct & $\text{0.57}_{\textcolor{deepgreen}{\text{+0.06}}}$ & $\text{0.22}_{\textcolor{deepgreen}{\text{+0.07}}}$ & $\text{0.28}_{\textcolor{deepgreen}{\text{+0.06}}}$ & $\text{43.58}_{\textcolor{deepgreen}{\text{+2.54}}}$ & $\text{59.21}_{\textcolor{deepgreen}{\text{+1.82}}}$ & $\text{7.15}_{\textcolor{deepred}{\text{-1.71}}}$ &  903         \\
MUFFIN & $\text{0.50}_{\textcolor{deepred}{\text{-0.01}}}$ & $\text{0.16}_{\textcolor{deepgreen}{\text{+0.01}}}$ & $\text{0.22}_{\textcolor{deepgreen}{\text{+0.00}}}$ & $\text{30.88}_{\textcolor{deepred}{\text{-10.16}}}$ & $\text{48.48}_{\textcolor{deepred}{\text{-8.91}}}$ & $\text{4.51}_{\textcolor{deepred}{\text{-4.35}}}$ &  791                  \\
Conifer & $\text{0.57}_{\textcolor{deepgreen}{\text{+0.06}}}$ & $\text{0.22}_{\textcolor{deepgreen}{\text{+0.07}}}$ & $\text{0.28}_{\textcolor{deepgreen}{\text{+0.06}}}$ & $\text{47.06}_{\textcolor{deepgreen}{\text{+6.02}}}$ & $\text{61.32}_{\textcolor{deepgreen}{\text{+3.93}}}$ & $\text{12.81}_{\textcolor{deepgreen}{\text{+3.95}}}$ &  1,084                 \\
I-SHEEP & $\text{0.53}_{\textcolor{deepgreen}{\text{+0.02}}}$ & $\text{0.17}_{\textcolor{deepgreen}{\text{+0.02}}}$ & $\text{0.23}_{\textcolor{deepgreen}{\text{+0.01}}}$ & $\text{34.26}_{\textcolor{deepred}{\text{-6.78}}}$ & $\text{50.28}_{\textcolor{deepred}{\text{-7.11}}}$ & $\text{5.41}_{\textcolor{deepred}{\text{-3.45}}}$ &  838                  \\
\midrule


% \midrule
%Suri           & 0.26            & 0.05            & 0.07            & 3.19                & 3.83               & 0.60               & 29        \\     
% Crab            & 0.56         & 0.18         & 0.25         & 39.92          & 56.83              & 9.05               & 1,192                \\ 
Suri & $\text{0.26}_{\textcolor{deepred}{\text{-0.25}}}$ & $\text{0.05}_{\textcolor{deepred}{\text{-0.10}}}$ & $\text{0.07}_{\textcolor{deepred}{\text{-0.15}}}$ & $\text{3.19}_{\textcolor{deepred}{\text{-37.85}}}$ & $\text{3.83}_{\textcolor{deepred}{\text{-53.56}}}$ & $\text{0.60}_{\textcolor{deepred}{\text{-8.26}}}$ &  29         \\
Crab & $\text{0.56}_{\textcolor{deepgreen}{\text{+0.05}}}$ & $\text{0.18}_{\textcolor{deepgreen}{\text{+0.03}}}$ & $\text{0.25}_{\textcolor{deepgreen}{\text{+0.03}}}$ & $\text{39.92}_{\textcolor{deepred}{\text{-1.12}}}$ & $\text{56.83}_{\textcolor{deepred}{\text{-0.56}}}$ & $\text{9.05}_{\textcolor{deepgreen}{\text{+0.19}}}$ &  1,192                 \\

\midrule
% \textbf{AIR}           & \textbf{0.61}         & \textbf{0.24}         & \textbf{0.31}         & \textbf{50.69}               & \textbf{63.89}              & \textbf{21.00}      & 1,813       \\ 
\textbf{AIR} & \textbf{$\text{0.61}_{\textcolor{deepgreen}{\text{+0.10}}}$} & \textbf{$\text{0.24}_{\textcolor{deepgreen}{\text{+0.09}}}$} & \textbf{$\text{0.31}_{\textcolor{deepgreen}{\text{+0.09}}}$} & \textbf{$\text{50.69}_{\textcolor{deepgreen}{\text{+9.65}}}$} & \textbf{$\text{63.89}_{\textcolor{deepgreen}{\text{+6.50}}}$} & \textbf{$\text{21.00}_{\textcolor{deepgreen}{\text{+12.14}}}$} &  1,813        \\
\bottomrule
\multicolumn{8}{c}{\multirow{2}{*}{\raisebox{1.4em}{\makecell{\textbf{Fine-tuned on Qwen-2.5-7B-UltraChat}}}}} \\
\midrule
\multirow{2}{*}{\raisebox{-0.4em}{\textbf{Method}}} & \multicolumn{3}{c}{\textbf{CF-Bench}}      & \multicolumn{2}{c}{\textbf{FollowBench}} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& \textbf{CSR} & \textbf{ISR} & \textbf{PSR} & \textbf{HSR}        & \textbf{SSR}       & \textbf{LC.}        & \textbf{Len}        \\
\midrule
Baseline & 0.68 & 0.29 & 0.40 & 47.71 & 64.79 & 10.87 & 836 \\
\midrule
% % \midrule
% back-translation & 0.42 & 0.14 & 0.18 & 21.62 & 34.86 & 1.79 & 3,266 \\
% back-and-forth & 0.63 & 0.24 & 0.34 & 45.33 & 60.39 & 12.59 & 1,480 \\
% % \midrule
% ShareGPT & 0.69 & 0.32 & 0.41 & 47.67 & 64.46 & 10.75 & 1,028 \\
% % \midrule
% Self-Instruct & 0.39 & 0.10 & 0.14 & 20.10 & 35.47 & 2.47 & 557 \\
% % \midrule
% Evol-Instruct & 0.67 & 0.30 & 0.40 & 46.67 & 63.98 & 8.81 & 964 \\
% MUFFIN & 0.61 & 0.26 & 0.34 & 45.27 & 62.45 & 8.44 & 880 \\
% Conifer & 0.70 & 0.34 & 0.44 & 51.65 & 65.72 & 19.39 & 1,024 \\
% I-SHEEP & 0.63 & 0.25 & 0.36 & 41.96 & 59.48 & 6.43 & 996 \\
% % \midrule
% %Suri & 0.31 & 0.07 & 0.10 & 4.55 & 4.85 & 0.94 & 239 \\
% Crab & 0.62 & 0.24 & 0.32 & 41.48 & 59.57 & 9.68 & 1,102 \\
% \midrule
% \textbf{AIR} & \textbf{0.76} & \textbf{0.41} & \textbf{0.51} & \textbf{59.07} & \textbf{71.35} & \textbf{32.43} & 1,779 \\ 
back-translation & $\text{0.42}_{\textcolor{deepred}{\text{-0.26}}}$ & $\text{0.14}_{\textcolor{deepred}{\text{-0.15}}}$ & $\text{0.18}_{\textcolor{deepred}{\text{-0.22}}}$ & $\text{21.62}_{\textcolor{deepred}{\text{-26.09}}}$ & $\text{34.86}_{\textcolor{deepred}{\text{-29.93}}}$ & $\text{1.79}_{\textcolor{deepred}{\text{-9.08}}}$ &  3,266  \\
back-and-forth & $\text{0.63}_{\textcolor{deepred}{\text{-0.05}}}$ & $\text{0.24}_{\textcolor{deepred}{\text{-0.05}}}$ & $\text{0.34}_{\textcolor{deepred}{\text{-0.06}}}$ & $\text{45.33}_{\textcolor{deepred}{\text{-2.38}}}$ & $\text{60.39}_{\textcolor{deepred}{\text{-4.40}}}$ & $\text{12.59}_{\textcolor{deepgreen}{\text{+1.72}}}$ &  1,480  \\
\midrule
ShareGPT & $\text{0.69}_{\textcolor{deepgreen}{\text{+0.01}}}$ & $\text{0.32}_{\textcolor{deepgreen}{\text{+0.03}}}$ & $\text{0.41}_{\textcolor{deepgreen}{\text{+0.01}}}$ & $\text{47.67}_{\textcolor{deepred}{\text{-0.04}}}$ & $\text{64.46}_{\textcolor{deepred}{\text{-0.33}}}$ & $\text{10.75}_{\textcolor{deepred}{\text{-0.12}}}$ &  1,028  \\
\midrule
Self-Instruct & $\text{0.39}_{\textcolor{deepred}{\text{-0.29}}}$ & $\text{0.10}_{\textcolor{deepred}{\text{-0.19}}}$ & $\text{0.14}_{\textcolor{deepred}{\text{-0.26}}}$ & $\text{20.10}_{\textcolor{deepred}{\text{-27.61}}}$ & $\text{35.47}_{\textcolor{deepred}{\text{-29.32}}}$ & $\text{2.47}_{\textcolor{deepred}{\text{-8.40}}}$ &  557  \\
\midrule
Evol-Instruct & $\text{0.67}_{\textcolor{deepred}{\text{-0.01}}}$ & $\text{0.30}_{\textcolor{deepgreen}{\text{+0.01}}}$ & $\text{0.40}_{\textcolor{deepgreen}{\text{+0.00}}}$ & $\text{46.67}_{\textcolor{deepred}{\text{-1.04}}}$ & $\text{63.98}_{\textcolor{deepred}{\text{-0.81}}}$ & $\text{8.81}_{\textcolor{deepred}{\text{-2.06}}}$ &  964  \\
MUFFIN & $\text{0.61}_{\textcolor{deepred}{\text{-0.07}}}$ & $\text{0.26}_{\textcolor{deepred}{\text{-0.03}}}$ & $\text{0.34}_{\textcolor{deepred}{\text{-0.06}}}$ & $\text{45.27}_{\textcolor{deepred}{\text{-2.44}}}$ & $\text{62.45}_{\textcolor{deepred}{\text{-2.34}}}$ & $\text{8.44}_{\textcolor{deepred}{\text{-2.43}}}$ &  880  \\
Conifer & $\text{0.70}_{\textcolor{deepgreen}{\text{+0.02}}}$ & $\text{0.34}_{\textcolor{deepgreen}{\text{+0.05}}}$ & $\text{0.44}_{\textcolor{deepgreen}{\text{+0.04}}}$ & $\text{51.65}_{\textcolor{deepgreen}{\text{+3.94}}}$ & $\text{65.72}_{\textcolor{deepgreen}{\text{+0.93}}}$ & $\text{19.39}_{\textcolor{deepgreen}{\text{+8.52}}}$ &  1,024  \\
I-SHEEP & $\text{0.63}_{\textcolor{deepred}{\text{-0.05}}}$ & $\text{0.25}_{\textcolor{deepred}{\text{-0.04}}}$ & $\text{0.36}_{\textcolor{deepred}{\text{-0.04}}}$ & $\text{41.96}_{\textcolor{deepred}{\text{-5.75}}}$ & $\text{59.48}_{\textcolor{deepred}{\text{-5.31}}}$ & $\text{6.43}_{\textcolor{deepred}{\text{-4.44}}}$ &  996  \\
\midrule
Suri & $\text{0.31}_{\textcolor{deepred}{\text{-0.37}}}$ & $\text{0.07}_{\textcolor{deepred}{\text{-0.22}}}$ & $\text{0.10}_{\textcolor{deepred}{\text{-0.30}}}$ & $\text{4.55}_{\textcolor{deepred}{\text{-43.16}}}$ & $\text{4.85}_{\textcolor{deepred}{\text{-59.94}}}$ & $\text{0.94}_{\textcolor{deepred}{\text{-9.93}}}$ &  239  \\
Crab & $\text{0.62}_{\textcolor{deepred}{\text{-0.06}}}$ & $\text{0.24}_{\textcolor{deepred}{\text{-0.05}}}$ & $\text{0.32}_{\textcolor{deepred}{\text{-0.08}}}$ & $\text{41.48}_{\textcolor{deepred}{\text{-6.23}}}$ & $\text{59.57}_{\textcolor{deepred}{\text{-5.22}}}$ & $\text{9.68}_{\textcolor{deepred}{\text{-1.19}}}$ &  1,102  \\
\midrule
\textbf{AIR} & \textbf{$\text{0.76}_{\textcolor{deepgreen}{\text{+0.08}}}$} & \textbf{$\text{0.41}_{\textcolor{deepgreen}{\text{+0.12}}}$} & \textbf{$\text{0.51}_{\textcolor{deepgreen}{\text{+0.11}}}$} & \textbf{$\text{59.07}_{\textcolor{deepgreen}{\text{+11.36}}}$} & \textbf{$\text{71.35}_{\textcolor{deepgreen}{\text{+6.56}}}$} & \textbf{$\text{32.43}_{\textcolor{deepgreen}{\text{+21.56}}}$} &  1,779  \\

\bottomrule
\end{tabular}}
\vspace{-3mm}
\caption{Experiment results on Llama-3-8B and Qwen-2.5-7B, with both models fine-tuned with ultrachat-200k \cite{ding2023enhancing}. Llama-3-70B-Instruct and Qwen-2.5-72B-Instruct are used as the guidance models respectively.}
\vspace{-3mm}
\label{table:main1}
\end{table*}




\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{0.95}
\resizebox{0.44\textwidth}{!}{
\begin{tabular}{c|ccc|cc}
\toprule
\multicolumn{6}{c}{\multirow{2}{*}{\raisebox{1.4em}{\makecell{\textbf{Fine-tuned on Llama-3-8B-Tulu}}}}} \\
\midrule
\multirow{2}{*}{\raisebox{-0.4em}{\textbf{Method}}}
& \multicolumn{3}{c|}{\textbf{CF-Bench}}   & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\ 
\cmidrule{2-6}
& \textbf{CSR} & \textbf{ISR} & \textbf{PSR}  & \textbf{LC.}  & \textbf{Len}  \\ 
\midrule
Baseline & 0.50 & 0.15 & 0.20 & 5.20 & 995 \\
\midrule
back-trans & 0.27 & 0.07 & 0.08 & 1.09 & 2,263 \\
back\&forth & 0.47 & 0.14 & 0.19 & 9.04 & 1,337 \\
\midrule
ShareGPT & 0.61 & 0.21 & 0.29 & 9.00 & 1,080 \\
\midrule
Self-Instruct & 0.30 & 0.07 & 0.09 & 2.63 & 378 \\
\midrule
Evol-Instruct & 0.58 & 0.19 & 0.27 & 18.09 & 991 \\
MUFFIN & 0.46 & 0.15 & 0.18 & 5.21 & 760 \\
Conifer & 0.61 & 0.24 & 0.32 & 7.15 & 903 \\
I-SHEEP & 0.49 & 0.16 & 0.19 & 3.11 & 931 \\
\midrule
%Suri & 0.25 & 0.05 & 0.06 & 0.44 & 151 \\
Crab & 0.56 & 0.19 & 0.27 & 8.55 & 1,221 \\
\midrule
\textbf{AIR} & \textbf{0.68} & \textbf{0.28} & \textbf{0.38} & \textbf{22.00} & 2,097 \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\caption{Experiment results on Llama-3-8B, fine-tuned with tulu-330k \cite{lambert2024tulu3}, with Llama-3-70B-Instruct as the guidance model.}
\vspace{-3mm}
\label{table:main2}
\end{table}

\subsection{Main Results}
As shown in Tables \ref{table:main1} and \ref{table:main2}, our proposed method achieves the best performance on both complex and general instruction-following benchmarks, demonstrating its effectiveness. In contrast, automatically crafted general instruction data significantly underperform, highlighting the importance of multiple constraints in effective instruction fine-tuning. Automatic rewritten instructions also underperform, as their constructed constraints do not align with real-world practice. Additionally, automatically back-translated instructions underperform as well. Despite the constraints being derived from documents, the documents (even after refinement) suffer from misalignment and should not be direct used as the target for fine-tuning.

% Our data construction method addresses both issues. Firstly, we construct constraints by comparing model responses with refined documents, thereby deriving constraints that reflects inadequacies of the model. After that, we re-generate the response based on the re-combined instructions, ensuring the formality and structure of the response align with the final instruction. As a result, we achieve significant improvements on both complex and general instructions.

\subsection{Data Quality Evaluation}

To evaluate our dataset's quality, we employed the Deita scorer \cite{liu2024Deita}, which utilizes LLM to assess complexity score for instructions and quality score for both instructions and responses. As shown in Figure \ref{figure:complexity_scores}, our approach significantly outperforms human crafted instructions, automatically crafted general instructions, and automatically rewritten complex instructions in terms of both complexity and quality scores. Notably, our method shows marginal improvements over automatic back-translation approaches like Suri and Crab, despite their use of high-quality seed datasets (e.g., Alpaca GPT4 for Crab) and advanced models (e.g., GPT-4-turbo for Suri). These results validate the effectiveness of our data generation strategy.


% 和其他方法比较complexity score和quality score
\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]
{source/data_visualization/complexity_scores.pdf}
\vspace{-3mm}
\caption{Comparison of averaged complexity and quality scores on different datasets.}
\vspace{-3mm}
\label{figure:complexity_scores}
\end{figure}


% 轮次间的指令长度变化情况，complexity score和quality score变化情况
\begin{figure}[t]
\centering
\subfigure[Diversity: unique trigrams and token length]{
    \includegraphics[width=0.90\linewidth]{source/data_visualization/instruction_analysis.pdf}
}
\vspace{-3mm}
\subfigure[Complexity and quality score]{
    \includegraphics[width=0.90\linewidth]{source/data_visualization/complexity.pdf}
}
% \vspace{-3mm}
\caption{Variation of quality indicators across iterations. \textit{Init} represents initial instructions generated through the IIG step.}
\vspace{-3mm}
\label{figure:instruction_and_complexity}
\end{figure}


To investigate the effect of iterative refinement, we analyze the variation of average unique trigrams and token lengths across iterations in Figure \ref{figure:instruction_and_complexity}(a). The results demonstrate consistent increases in both instruction length and unique trigrams, indicating that newly added constraints is diverse rather than mere repetition. Furthermore, Figure \ref{figure:instruction_and_complexity}(b) displays the evolution of complexity and quality scores throughout the iterations, showing steady improvement of data quality as the iterations progress.

% 最后应该还需要有个总结，不过下面的内容提到了diversity，我就先注释掉了。

% In this section, we would like to compare the statistical indicators of different models. We select three indicators for three quality dimensions: 1) Diversity 2) Complexity 3) Accuracy. For measuring diversity, we calculate the unique tri-grams of instructions. For measuring Complexity and Accuracy, we leverage LLM-as-a-Judge, and adopt GPT-4 as the judge to assign the score to the model\footnote{For more details please refer to Appendix \ref{xx}}.

% As shown in Figure \ref{xx}, our constructed data achieves the best score upon both diversity, complexity and accuracy, which can be attributed to three aspects: 1) For diversity, we resort to document-based back translation to construct initial instructions, therefore ensuring the diversity of instructions based on diverse documents. 2) For complexity, we construct the constraints by iterative judge, therefore discovering the insufficient points of current model. 3) For accuracy, we derive the final response from a proprietary model instead of the original document, therefore ensuring the quality of the response for fine-tuning.

\subsection{Judgment Strategy for Better Constraint}
\label{sec:judge}


In this section, we investigate the optimal judgment strategy for constraint generation. When humans adjust prompts based on the output, they typically have a pre-expected response as the reference in mind, and constraints are issued to guide the response closer to the reference. Therefore, we compare three judgment settings: 1) No judgment, directly curate constraints; 2) Judge without document as the reference. Instead, use the guidance models' response as the reference; 3) Judge with the refined document as the reference. 

As shown in Table \ref{table:judge}, the judgment process is essential for uncovering valuable constraints to improve the complex instruction following ability. LLM-judge can curate constraints that reflects the insufficiency of the model which requires further tuning. Moreover, using document as reference is also essential due to the limited judgment ability of the model, and human-written references aid in more targeted constraint construction.

On the other hand, the additional checking step does not improve complex instruction-following ability, as the checking step would result in fewer constraints. However, we observe improved performance on general-instruction following, indicating there exists a trade-off between general and complex instruction following abilities.

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{0.95}
\resizebox{0.44\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{FollowBench}} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
& \textbf{HSR} & \textbf{SSR} & \textbf{LC.} & \textbf{Len} \\
\midrule
\multicolumn{5}{l}{\textit{Results on Llama-3-8B-UltraChat}} \\
\midrule
Baseline & 41.04 & 57.39 & 8.86 & 1,017 \\
\midrule
w/o judge & 47.15 & 62.62 & 19.07 & 1,706 \\
judge w/o doc & 51.24 & 63.81 & 20.00 & 1,717 \\
judge w/ doc & \textbf{52.34} & \textbf{64.09} & 19.74 & 1,408 \\
\midrule
w/ check & 50.69 & 63.89 & \textbf{21.00} & 1,813 \\
\midrule
\multicolumn{5}{l}{\textit{Results on Llama-3-8B-Tulu}} \\
\midrule
Baseline & 34.91 & 51.76 & 5.20 & 995 \\
\midrule	
w/o judge & 47.59 & 63.60 & 18.32 & 2,067 \\
judge w/o doc & 50.62 & 63.69 & 17.02 & 2,842 \\
judge w/ doc & \textbf{54.16} & \textbf{67.52} & 20.45 & 1,639 \\
\midrule
w/ check & 51.35 & 66.09 & \textbf{21.09} & 2,049 \\
\bottomrule
\end{tabular}}
\caption{Experiment results on Llama-3-8B models with constraints from different judgment strategies.}
\label{table:judge}
\end{table}





\subsection{Influnce of Iterative Judge}

\begin{figure}[t]
\centering
\subfigure[Llama-3-8B-UltraChat]{
    \includegraphics[width=0.95\linewidth]{source/data_visualization/llama_UltraChat_DataNum.pdf}
}
\subfigure[Llama-3-8B-Tulu]{
    \includegraphics[width=0.95\linewidth]{source/data_visualization/llama_Tulu_DataNum.pdf}
}
\vspace{-3mm}
\caption{The variation of performance on FollowBench and AlpacaEval2 with the variation of data number.}
\vspace{-3mm}
\label{figure:data_num}
\end{figure}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{0.95}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Iteration}} & \multicolumn{2}{c|}{\textbf{FollowBench}} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
& \textbf{HSR}        & \textbf{SSR}       & \textbf{LC.}      & \textbf{Len} \\ 
\midrule
\begin{tabular}[c]{@{}c@{}}Baseline\end{tabular}  & 34.91       & 51.76      & 5.20       & 995         \\
\midrule Init    & 46.37   & 61.87    & 17.96  & 1,602    \\
1    & 49.75   & 64.78    & 21.63  & 1,994    \\
2   & 53.82    & 67.55    & 21.01     & 1,829  \\
3   & \textbf{54.46}        & 67.54     & 20.69   & 1,722     \\
4   & 53.97       & 67.09    & \textbf{22.50}      & 1,672     \\
5   & 53.30     & \textbf{67.91}    & 20.78    & 1,599       \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\caption{Experiment results on Llama-3-8B-Tulu fine-tuned on different iterations. \textit{Init} represents initial instructions generated through the IIG step.}
\vspace{-3mm}
\label{table:judge_iterations}
\end{table}

In this section, we investigate the effectiveness of iterative judge by examining model performance across different iterations. As shown in Table \ref{table:judge_iterations}, the iterative judge process demonstrates clear benefits compared to both the baseline and IIG step.

Specifically, we observe consistent improvements on FollowBench and AlpacaEval2 through the first two iterations. This suggests that the iterative judging process effectively identifies and incorporates increasingly sophisticated constraints that are valuable for complex instruction following. However, improvements tend to plateau after the third iteration. This could be attributed to the fact that the most critical and fundamental constraints have already been discovered in earlier iterations.

% 2) Additional constraints, while syntactically distinct, impose similar requirements, leading to redundant guidance for model behavior.

% \footnote{Detailed analysis are presented in Appendix \ref{appendix:judge_iterations_analysis}.}
% Interestingly, for general instruction following ability (measured by AlpacaEval2), we observe with the best performance achieved at Iteration 4. This suggests that while our method primarily targets complex instruction following, it maintains and sometimes enhances general instruction following capabilities.


\subsection{Influence of Data Quantity}

\indent In this section, we investigate the impact of data quantity on AIR's performance. We present the results of models trained with varying amounts of data in Figure \ref{figure:data_num}. As shown, performance on both general and complex instruction tasks improves with increasing data quantity. On the other hand, the model can achieve superior performance with only 1k training samples, and the performance gains become marginal as more data is added. Therefore, in practical applications, the optimal amount of fine-tuning data can be determined based on available computational resources.

\subsection{Influence of Guidance Model Size}


\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{0.95}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Guid. Model}} & \multicolumn{2}{c}{\textbf{FollowBench}} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
& \textbf{HSR}   & \textbf{SSR}   & \textbf{LC.}      & \textbf{Len}  \\
\midrule
\begin{tabular}[c]{@{}c@{}}Baseline\end{tabular}  & 47.71    & 64.79  & 10.87   & 836   \\
\midrule
14B     & 57.72   & 70.59   & 29.13   & 1,501   \\
32B     & \textbf{60.06}   & \textbf{71.97}   & 26.39   & 1,309    \\
72B     & 59.07   & 71.35   & \textbf{32.43}  & 1,779     \\
\bottomrule
\end{tabular}}
\caption{Experiment results on Qwen-2.5-7B-UltraChat fine-tuned with different guidance model size.}
\label{table:guidance_size}
\end{table}

In Table \ref{table:guidance_size}, we investigate the impact of guidance model size on AIR's performance. We performed experiments with Qwen-2.5-7B-UltraChat as the base model, while varying the guidance model size from 14B to 72B parameters. As shown, all guidance models significantly improve instruction-following ability compared to the baseline, while larger models generally present more improvement. On the other hand, even the 14B guidance model demonstrates remarkable improvement. This scalability across different model sizes highlights the robustness and efficiency of our approach.
% which leverages documents as reference to help the constraint generation process.

% These findings suggest that AIR's effectiveness is not strictly dependent on large guidance models. The method effectively leverages both document knowledge and model knowledge through judging mechanism to generate meaningful constraints, making it practical and efficient even with relatively smaller guidance models. 


