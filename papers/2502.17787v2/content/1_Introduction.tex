\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{source/intro.pdf}
    \caption{Illustration of how humans iteratively refine instructions to be more complex.}
    \label{fig: intro}
    \vspace{-1mm}
\end{figure}

Recent advancements in Large Language Models (LLMs) have shown impressive performance across a wide range of tasks~\cite{zhao2023survey, li2024graphreader, he2024chinesesimpleqa}. Driven by vast amounts of data and efficient training, most current LLMs are capable of effectively following user instructions and aligning to a certain extent with human preferences~\cite{ouyang2022training,li20242d,huang2025musc}.
However, despite these successes, they still face significant challenges when it comes to following complex instructions~\cite{jiang2023followbench,wen2024benchmarking}.

Existing complex instructions datasets primarily originate from two sources: 1) Curated data from open-source datasets or human annotations~\cite{zhou2024lima,zhang2024cfbench}, which are resource-intensive and \textbf{lack scalability}, and 2) Transforming simple instructions into complex ones automatically using proprietary LLMs~\cite{xu2023wizardlm,sun2024conifer}. 
While the automatic transformation improves scalability, the generated constraints are often recombinations of few-shot examples, resulting in \textbf{limited diversity}.
Moreover, these constraints may have \textbf{low relevance} with the target output, failing to reflect real-world scenarios.

% as they are derived without considering the actual response's deficiencies,

% where human iteratively refine instructions to be more complex, as shown in Figure \ref{fig: intro}.

Recently, back-translation, which involves translating text from the target side back into the source side, has been proposed to generate scalable and diverse instructions from human-written corpora~\cite{sennrich2015improving,hoang2018iterative,zheng2024kun,li2023self}. 
% Some studies have explored how back-translation can be leveraged to better generate instructions for fine-tuning large models~\cite{li2023self,zheng2024kun,nguyen2024better}. 
However, these methods typically focus on generating \textbf{simple instructions} and have not fully explored the rich knowledge contained in the human corpus.

% To address the scalability and diversity of producing complex instructions,
In this paper, we propose an \textbf{Automatic Iterative Refinement (AIR)} framework for generating high-quality complex instructions.
Specifically, our approach is based on two key observations. First, human-written documents contain massive human preferences that can be converted to specific constraints, such as formatting conventions in legal documents. Second, human often refine complex instructions iteratively based on feedback from model outputs. As illustrated in Figure~\ref{fig: intro}, simple instructions are progressively adjusted and enriched to better align with human preferences. This iterative process plays a critical role in crafting effective complex instructions.

Therefore, our AIR framework incorporates document-based knowledge and LLM-as-judge to iteratively construct complex instructions. 
The framework consists of two key steps: 1) \textbf{Initial Instruction Generation}, where the model generates initial instructions based on the document content; 2) \textbf{Iterative Instruction Refinement}, where instructions are iteratively refined with LLM-as-judge guidance by comparing model outputs with the document, to identify and incorporate valuable constraints. This process enables the framework to generate more challenging instructions that align more closely with real-world scenarios. 

In summary, our contributions are as follows: 

\begin{itemize}[leftmargin=4mm]
    \item  To better align with real-world scenarios, we propose the \textbf{AIR} framework, which iteratively refines complex instructions with LLM-as-judge guidance by comparing with the document.

    \item We introduce a novel instruction dataset, \textbf{AIR-10K}, generated using our framework. Experimental results demonstrate that our fine-tuned model significantly outperforms existing methods on instruction-following benchmarks.

    \item We provide a comprehensive experimental analysis to evaluate the individual components of our framework, validating the contribution of each step to the overall improvement.
\end{itemize}