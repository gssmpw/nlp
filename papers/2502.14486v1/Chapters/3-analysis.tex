\section{A Safety-Helpfulness Trade-off View of Jailbreak Defense}
\label{sec:trade_off_analysis}

\subsection{Formulating Defense as a Classification-Based Optimization}
Given a dataset \(\mathcal{D}\) comprising pairs of queries \(x_i\) and corresponding labels \(y_i \in \{0, 1\}\), where (\(y_i = 1\)) indicates a harmful query that should be refused, and (\(y_i = 0\)) denotes a benign query that should be complied with, as determined by human annotation. Let \(\theta\) represents a generative model, and \(\delta\) represents a defense method applied to the model or the input query. In the original generative task, the model under defense method \( \delta \) directly generates a response \(g(\theta, x; \delta)\) for query \(x_i\), which is then assessed as either a refusal or compliance.

In the classification formulation, the model is tasked with determining whether to refuse or comply with the input query, outputting a refusal probability \(p(\theta, x; \delta)\) under defense method \( \delta \) for the query \( x \). This format provides a more granular investigation of the model's preference, offering deeper insights compared to direct generative outputs.
Then the prediction \(f(\theta, x; \delta)\) is given by:
\begin{align*}
    f(\theta, x; \delta) = 
    \left\{
    \begin{array}{ll}
    0 & \text{if } p(\theta, x; \delta) < 0.5 \\
    1 & \text{if } p(\theta, x; \delta) \geq 0.5
    \end{array}
\right.
\end{align*}
The objective is to find the optimal defense \( \delta \) that minimizes the error between the true labels \(y_i\) and the defended model's predictions \(f(\theta, x; \delta)\), where \(\mathcal{L}(\cdot)\) is a loss function of the prediction error.
\begin{align*}
\min_{\delta} \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \mathcal{L}(f(\theta, x; \delta), y) \right]
\end{align*}

This optimization objective can be decomposed into two components:
\begin{align*}
\begin{split}
\min_{\delta} \mathbb{E}_{(x, y) \sim \mathcal{D} \, | \, y = 1} \left[ \mathcal{L}(f(\theta, x; \delta), y) \right] \\
+ \min_{\delta} \mathbb{E}_{(x, y) \sim \mathcal{D} \, | \, y = 0} \left[ \mathcal{L}(f(\theta, x; \delta), y) \right]
\end{split}
\end{align*}
The first component focuses on the safety optimization, assessing whether the defense methods effectively enhance the model’s sensitivity to harmful inputs. The second component optimizes the defense mechanism to avoid overly constraining the model’s ability to identify benign inputs. This dual optimization captures the essential balance between safety and helpfulness.

\begin{figure*}[ht]
    \centering
    \begin{minipage}{0.33\textwidth} 
        \includegraphics[width=\linewidth]{Chapters/images/analysis_0.pdf}
        \subcaption{Baseline}
    \end{minipage}\hfill 
    \begin{minipage}{0.62\textwidth} 
        \includegraphics[width=\linewidth]{Chapters/images/analysis_1.pdf}
        \subcaption{Individual Defenses}
    \end{minipage}
    \caption{Representative results of individual defenses on refusal probabilities for harmful and benign queries. Compared to the baseline, system reminder and model optimization increase the mean refusal probabilities for both query types (\textbf{Safety Shift}). Query refactoring raises the mean refusal probability for harmful queries while lowering it for benign ones (\textbf{Harmfulness Discrimination}).}
    \label{fig:analysis results}
\end{figure*}

\subsection{Quantifying Defense using Probability-based Metrics}
\label{sec:defense_effects}
To quantify the impact of defense methods from the classification-based perspective, we introduce two relative metrics compared to the undefended model: Mean Shift and Distance Change.

\textbf{Mean Shift} measures how much the defense method \( \delta \) shifts the average refusal probabilities for input queries relative to the undefended model. We calculate mean shifts separately for harmful and benign queries as follows:
\begin{align*}
\begin{split}
\text{Mean\_Shift}_{\text{harmful}} &= \mathbb{E}_{x \in D_{\text{harmful}}}[p(\theta, x; \delta)] \\
&\quad - \mathbb{E}_{x \in D_{\text{harmful}}}[p(\theta, x)]
\end{split} \\
\begin{split}
\text{Mean\_Shift}_{\text{benign}} &= \mathbb{E}_{x \in D_{\text{benign}}}[p(\theta, x; \delta)] \\
&\quad - \mathbb{E}_{x \in D_{\text{benign}}}[p(\theta, x)]
\end{split}
\end{align*}
where \( \mathbb{E}_{x \in D}[p(\theta, x; \delta)] \) and \( \mathbb{E}_{x \in D}[p(\theta, x)] \) are the average refusal probabilities after and before applying the defense method $\delta$, respectively. A large shift in harmful data implies that the model becomes more safety-conscious, whereas a large shift in benign data suggests potential over-defense.  

\textbf{Distance Change} measures how the distance between the refusal probability distributions for harmful and benign data changes before and after applying the defense. Let \( P_{\text{harmful}} \) and \( P_{\text{benign}} \) represent the  refusal probability distributions for harmful and benign data before defense, and \( P^{\delta}_{\text{harmful}} \) and \( P^{\delta}_{\text{benign}} \) represent these distributions after defense. The distribution distance is defined as:
\begin{align*}
\begin{split}
\text{Distribution\_Distance} = &\ \text{Dist}(P_{\text{benign}}^{\delta}, P_{\text{harmful}}^{\delta}) \\
&- \text{Dist}(P_{\text{benign}}, P_{\text{harmful}})
\end{split}
\end{align*}
where \( \text{Dist}(\cdot, \cdot) \) denotes a distance metric between probability distributions, such as Jensen-Shannon divergence. A larger distance change indicates that the defense method improves the model's ability to distinguish between harmful and benign queries.


\begin{figure*}[ht]
    \centering
    \begin{minipage}{0.25\textwidth} 
        \includegraphics[width=\linewidth]{Chapters/images/analysis_0.pdf}
        \subcaption{Baseline}
    \end{minipage}\hfill 
    \begin{minipage}{0.75\textwidth} 
        \begin{minipage}{\linewidth}
            \includegraphics[width=\linewidth]{Chapters/images/analysis_2.pdf}
            \vspace{-6mm}
            \subcaption{Inter-Mechanism Ensembles}
        \end{minipage}
        \vfill
        \vspace{5pt}
        \begin{minipage}{\linewidth}
            \centering
            \includegraphics[width=0.75\linewidth]{Chapters/images/analysis_3.pdf}
            \vspace{-2mm}
            \subcaption{Intra-Mechanism Ensembles}
        \end{minipage}
    \end{minipage}
    \caption{Representative results for ensemble defenses. Inter-mechanism ensembles tend to reinforce the mechanism while intra-mechanism ensembles achieve a better trade-off between mechanisms.}
    \label{fig:analysis_results}
\end{figure*}

\subsection{Investigating Mechanisms of Defense Methods}
% While this approach may not fully capture the model's decision-making process in generative tasks as discussed in Section~\ref{sec:consistency}, it provides valuable insights into the effects of defense strategies on model behavior. 
To quantitatively analyze various defense methods, we prompt the model to classify whether it would comply with or refuse a given query, extracting the logits of refusal as its refusal probability. We conduct this analysis on the MM-SafetyBench dataset with LLaVA-1.5-13B model. The detailed prompt and analysis setup are provided in Appendix~\ref{sec:analysis_setup}. 

We specifically focus on four categories of internal jailbreak defenses described in Section~\ref{internal_defense_background}, and examine multiple methods for each category. A representative result is shown in Figure~\ref{fig:analysis results}, with the full set of results available in Appendix~\ref{sec:more_analyss_result}. Additional analyses on more LVLMs and LLMs are in Appendx~\ref{sec:extra_lvlm} and \ref{sec:extra_llm}. We also assess the consistency between the original generation task and the re-formulated classification task in Appendix~\ref{sec:consistency_appendix}.
Across these defense methods, two significant mechanisms emerge: Safety Shift and Harmfulness Discrimination, which explain how these defenses work.

\paragraph{Safety Shift} Compared to the baseline undefended model, both system reminder and model optimization defenses exhibit a significant mean shift across harmful and benign query subsets, without necessarily increasing the distance between the refusal probability distributions for these two groups.
This safety shift mechanism stems from the enhancement of model's general safety awareness, leading to a broad increase in refusal tendencies for both harmful and benign queries. However, such a conservative response to both types of queries can result in over-defense and does not significantly improve the model's ability to discriminate between harmful and benign inputs.

\paragraph{Harmfulness Discrimination} In contrast, query refactoring defenses either increases the refusal probabilities for harmful queries or decrease them for benign queries, leading to a consistent enlargement of the gap between the refusal probability distributions of these two subsets. This harmfulness discrimination mechanism enables better interpretation of the harmfulness within harmful queries or harmlessness within benign queries, thereby improving the distinction between them. However, the concealment of harmfulness within some queries can limit these improvements.

Additionally, noise injection demonstrate limited effectiveness, as indicated by insignificant changes in both the mean shift and distance change metrics. This is because it primarily targets attacks where noise is deliberately added to input queries, making it less effective in defending against general input queries without intentional noise.

\input{Chapters/tabs/tab_main}

\subsection{Exploring Defense Ensemble Strategies}

An effective defense should block harmful queries while preserving helpfulness for benign ones. Achieving this requires balancing safety shifts without over-defense and enhancing harmfulness discrimination. Since different defense methods impact model safety differently, we explore ensemble strategies to optimize this trade-off:

\begin{itemize}[itemsep=0.5pt, leftmargin=12pt, parsep=1pt, topsep=1pt]
    \item \textbf{Inter-Mechanism Ensemble} combines defenses operating the same mechanism, including safety shift ensembles and harmfulness discrimination ensembles.
    For safety shift ensembles, we combine multiple system reminder methods \textit{(SR++)} or combine system reminder with model optimization methods \textit{(SR+MO)}. For harmfulness discrimination ensemble, we combine multiple query refactoring methods \textit{(QR++)}.
    \item \textbf{Intra-Mechanism Ensemble} combines two defenses where one improves safety shift and the other enhances harmfulness discrimination. This includes ensembling query refactoring with system reminder methods \textit{(QR\textbar{}SR)} or with model optimization methods \textit{(QR\textbar{}MO)}.
\end{itemize}

% The inter-mechanism ensemble combines multiple safety shift methods, which can enhance overall safety by reinforcing conservative responses across different models. The intra-mechanism ensemble integrates a safety shift method and a harmfulness discrimination method, where the latter can help mitigate the refusal probability distribution shift of benign queries, thereby increase the distance between the two subsets.

For each ensemble strategy, we explore several variants using different specific methods. 
% A detailed description of these variants is provided in Appendix~\ref{sec:ensemble_strategy}. 
Representative results are shown in  Figure~\ref{fig:analysis_results}, with the full set of variant results available in Appendix~\ref{sec:more_analyss_result}.

We observe that inter-mechanism ensembles tend to strengthen a single defense mechanism. Safety shift ensembles like \textit{SR++} and \textit{SR+MO} further enhance model safety but exacerbate the loss of helpfulness. Conversely, harmfulness discrimination ensembles achieve a larger mean shift on benign queries towards compliance, making them better suited for situations where maintaining helpfulness is critical. 

In contrast, intra-mechanism ensembles combine the strengths of both mechanisms to achieve a more balanced trade-off. Specifically, \textit{QR\textbar{}SR} and \textit{QR\textbar{}MO} increase the refusal probability for harmful queries, while maintaining or even decreasing the refusal probability for benign queries, thereby improving the model's ability to distinguish between benign and harmful queries. This makes them a better choice for general scenarios where balancing safety and helpfulness is essential. 
