\section{Background}
Recent studies have proposed various defense methods against jailbreak attacks to improve generative model safety. With limited research on multimodal jailbreak defenses, this study focuses on multimodal scenarios. It reviews existing defense methods, covering internal and external safeguards.

\subsection{Internal Jailbreak Defenses}
\label{internal_defense_background}
Internal Jailbreak Defenses directly intervene in the model's generation process by optimizing the model itself or modifying the input query. These defenses can be grouped into four main strategies:

\textbf{Model Optimization} optimizes models themselves by alignment training or decoding adjustments. The former includes safety-oriented instruction fine-tuning~\cite{bianchi2023safety,zong2024safety}, and reinforcement learning from human feedback (RLHF) methods like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO)~\cite{zhang2024spa}. Decoding strategies like Rewindable Auto-regressive Inference~\cite{li2023rain} and SafeDecoding~\cite{xu2024safedecoding} enhance safety without fine-tuning. 
% deng2023multilingual,

\textbf{System Reminder} adds a system prompt to remind the model of safety. Variants include asking the assistant to be responsible\cite{xie2023defending}, using Chain of Thought (CoT) prompts\cite{wang2024adashield}, prioritizing safety over helpfulness\cite{zhang2023defending}, and adding demonstrations for in-context learning\cite{wei2023jailbreak}.

\textbf{Query Refactoring} involves modifying input queries. This includes altering text through translation, paraphrasing, summarization\cite{ji2024defending}, or intention analysis\cite{zhang2024intentionanalysismakesllms}, and adjusting images by adding or replacing them with captions\cite{gou2024eyes}.

\textbf{Noise Injection} adds random perturbations to inputs. For text, this includes random insertion, swapping, patching\cite{robey2023smoothllm}, and word masking\cite{cao2023defending}. For images, it includes geometric or photometric mutations\cite{zhang2024jailguarduniversaldetectionframework} or adding random noise\cite{xu2024defending}. Multiple noise injections are often combined using ensemble strategies to improve defense.


\subsection{External Jailbreak Defenses}
External defenses operate independently without directly modifying the model, which can be divided into pre-filtering and post-remediation. Pre-filtering uses external classifiers to block harmful queries, detecting high perplexity or toxic content~\cite{alon2023detecting,kim2023lifetox,kumar2024certifying}. Post-remediation removes harmful responses after generation, either through model self-detection~\cite{phute2023llm} or lightweight harm detectors to transform harmful outputs into benign ones~\cite{pi2024mllm}.

This study focuses on internal strategies that directly modify the target model, examining their impact on safety and helpfulness. External strategies, which vary widely in detection models and algorithms, are beyond the scope of this work and warrant further research for broader evaluation.
