\section{Related Work}
\paragraph{Jailbreak Attacks and Defenses in LVLMs}
Numerous studies~\cite{wei2024jailbroken,chao2023jailbreaking,zou2023universal,liu2023autodan,robey2023smoothllm,xie2023defending} have explored jailbreak attacks and defenses for LLMs. LVLMs which integrate visual perception with LLMs, exhibit increasing vulnerability against jailbreak attacks. One line of research~\cite{dong2023robust, bailey2023image,luo2023image,shayegani2023jailbreak} employs gradient-based techniques to generate adversarial images that elicit harmful responses from target models. Another line of attacks~\cite{gong2023figstep,liu2023query} converts harmful content into images using typography or text-to-image tools to circumvent LVLMs' safety mechanisms. On the defense side, internal defenses intervene in modelâ€™s generation process by optimizing the model~\cite{zong2024safety,zhang2024spa} or modifying system prompts~\cite{zhang2024jailguarduniversaldetectionframework,gou2024eyes}. External defenses function as independent filters without directly affecting the model~\cite{pi2024mllm,zhao2024first,helff2024llavaguard}.

\paragraph{Safety Evaluation of LVLMs}
The evaluation of safety in LVLMs has gained significant attention in recent research.
Several studies have curated specialized image-text paired datasets to examine the models' safety levels~\cite{liu2023query,wang2023tovilag,li2024red}. These evaluations have uncovered critical issues, like limited safety and oversensitivity where models incorrectly flag benign inputs as harmful~\cite{li2024mossbench}. Our study explores the mechanisms underlying different defense methods causing these problems and how to optimize the delicate balance between maintaining model safety and preserving helpfulness.