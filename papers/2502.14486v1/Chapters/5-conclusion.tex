\section{Conclusion}
In this study, we analyze the trade-off between safety and helpfulness in jailbreak defenses. We identify two key defense mechanisms: safety shift and harmfulness discrimination. Based on these, we explore various ensemble strategies, which can be divided into inter-mechanism and intra-mechanism combinations. Our results show that these strategies effectively enhance model safety or balance safety and helpfulness.
Among them, the \textit{SR+MO} from inter-mechanism ensemble consistently performs best. In particular, the Demonstration-SFT method  offers strong defense while maintaining high utility and a reasonable response rate. The \textit{QR|SR} from intra-mechanism ensemble also delivers solid results by combining defenses from different mechanisms, achieving a well-balanced trade-off.
Overall, our work compares defense methods in multimodal scenarios and highlights ensemble strategies to improve model safety. We aim to guide practical defense strategy selection and inspire further research.



\section*{Limitations}
While our study provides insights into jailbreak defense mechanisms and ensemble strategies, several limitations remain. First, our analysis primarily focuses on LVLMs, particularly the LLaVA series. Although we extend our analysis to other LVLM architectures and LLMs, further validation is needed to determine whether the identified defense mechanisms generalize to other generative model structures.
Second, the scope of adversarial attacks we evaluate is limited. Our experiments rely on the MM-SafetyBench and MOSSBench datasets, which may not fully capture the complexity and diversity of real-world adversarial scenarios. Third, our exploration of defense methods is not exhaustive. While we evaluate a range of strategies, there are likely other effective defense techniques that we have not considered. Future work could expand this scope to include additional methods and their combinations.

\section*{Ethics Statement}
This paper mentions jailbreak datasets and attack techniques, which may potentially contain or induce offensive and harmful content. It is crucial to emphasize that the primary goal of this work is to advance research in jailbreak defenses and to improve the robustness of LVLMs against harmful content. We strongly encourage further research in this area to foster the development of more secure and ethically aligned generative models. All analysis and datasets utilized in this paper are strictly intended for research purposes under the ethical guidelines of the research community. The authors unequivocally condemn any misuse of this work to generate or disseminate harmful content.