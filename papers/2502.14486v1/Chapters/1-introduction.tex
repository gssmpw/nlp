\section{Introduction}
Recent advances in Large Language Models (LLMs) have shown impressive generative capabilities, enabling their use in various fields~\cite{gupta2023chatgpt, openai2023gpt, dubey2024llama}. However, as their instruction-following ability increases, these models have become targets of adversarial attacks, raising significant safety concerns~\cite{bommasani2021opportunities}.
One prominent issue is the generation of harmful content when facing jailbreak attack~\cite{huang2023catastrophic,liu2023jailbreaking}, where malicious users craft prompt to bypass the model's internal safety mechanism. 
Additionally, the introduction of Large Vision-Language Models (LVLMs)~\cite{bai2023qwen, liu2023visual, li2023blip} has added further risks, as these models interact with a broader range of input channels~\cite{gu2024agent, wang2024llms}.  


To address the challenges posed by jailbreak attacks, various defense strategies have been developed, including modifying system prompts~\cite{zhang2023defending, xie2023defending}, adjusting training or decoding processes~\cite{qi2023fine, xu2024safedecoding}, and processing input queries and images~\cite{zhang2023mutation, ji2024defending, wang2024defending}. These methods present distinct advantages and limitationsâ€”some improve safety but result in over-defense~\cite{jiang2024wildteaming}, while others provide limited safety improvements and remain vulnerable to minor input changes. A deeper understanding of these trade-offs and a systematic comparison of defense mechanisms is still lacking. Additionally, how to effectively combine different strategies for a better balance between safety and helpfulness remains an open challenge.  

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{Chapters/images/intro.png}
    \caption{Illustration of the safety shift mechanism (shifting towards the same refusal side of the decision boundary) and the harmfulness discrimination mechanism (shifting towards opposite sides of the decision boundary).}
    \label{fig:intro}
\end{figure*}

In this work, we examine the mechanisms behind jailbreak defenses by reformulating the generative task as a classification problem, focusing on the trade-off between safety and helpfulness~\cite{wei2024jailbroken, mkadry2017towards}. The classification task probes the model's internal preference to either refuse or comply with the input query based on safety considerations, treating refusal and compliance as binary classification labels. Specifically, we use one harmful and one benign subsets of queries in multimodal contexts to compare the defense model's refusal probabilities on both subsets against those of the non-defense model. Then the problem space can be viewed as a classification plane, where different defense models correspond to various decision boundaries among data points from both subsets, represented as (input query, refusal probability) pairs.


Our analysis identifies two key mechanisms in jailbreak defenses: \textit{safety shift} and \textit{harmfulness discrimination}. As illustrated in Figure~\ref{fig:intro}, safety shift refers to a general increase in refusal probabilities for both harmful and benign subsets, shifting the overall data distribution towards the refusal side of the decision boundary without necessarily widening the gap between their refusal distributions. In contrast, harmfulness discrimination either reduces refusal probabilities for benign queries or raises refusal rates for harmful queries, thereby increasing the distance between the refusal probability distributions of the two subsets. 

Based on these two mechanisms, we further explore various ensemble strategies for defense methods, including inter-mechanism and intra-mechanism ensembles. Inter-mechanism ensembles combine methods that share the same mechanism, either enhancing overall safety by reinforcing more conservative responses (safety shift ensembles), or further improving the response rate for benign queries (harmfulness discrimination ensembles). Intra-mechanism ensembles integrate both safety shift and harmfulness discrimination methods, with the latter helping to mitigate the refusal probability shift of benign queries, thereby complementing each other for a more balanced trade-off.  

We conduct empirical evaluations of multiple specific jailbreak defense methods in multimodal scenarios, which are less explored compared to language scenarios. Generative results on top of LLaVA-1.5~\cite{liu2024visual} at different scales on the MM-SafetyBench~\cite{liu2023mm} and MOSSBench~\cite{li2024mossbench} datasets confirm that these methods can improve defenses in previously discussed two mechanisms, and also underscore the challenging nature of multimodal jailbreak defense. Further evaluations of ensemble strategies proves their effectiveness to either maximize model safety or achieve a better safety-helpfulness trade-off.

Overall, our work identifies two core mechanisms of jailbreak defenses, provides a comparison of methods, and explores ensemble strategies to amplify safety or balance it with helpfulness. Our evaluation of 28 defense methods fills a gap in multimodal defense research, offering insights for strategy selection and inspiring future advancements.  
