\documentclass[runningheads]{llncs}
\pagestyle{plain}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{bbold}
\usepackage{algorithm}
\usepackage{xcolor}

\usepackage[noend]{algpseudocode}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsfonts}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\sampledfrom}{\overset{{\scriptscriptstyle\$}}{\leftarrow}}
\setlength{\parindent}{0pt}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\poly}{\textrm{poly }}
\newcommand{\stbar}{\;|\;}
\newcommand{\interior}[1]{\accentset{\circ}{#1}}
\newcommand{\mat}{\textrm{Mat}}
\newcommand{\inner}[1]{\langle#1\rangle}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}


\DeclareMathOperator{\spn}{span}


\lstset{columns=fullflexible, breaklines=true, mathescape=true}
\makeatletter
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand\tb@int[2]{%
	\sbox\z@{$\m@th#1\int$}%
	\if#2t%
	\rlap{\hbox to\wd\z@{%
			\hfil
			\vrule width .35em height \dimexpr\ht\z@+1.4pt\relax depth -\dimexpr\ht\z@+1pt\relax
			\kern.05em % a small correction on the top
	}}
	\else
	\rlap{\hbox to\wd\z@{%
			\vrule width .35em height -\dimexpr\dp\z@+1pt\relax depth \dimexpr\dp\z@+1.4pt\relax
			\hfil
	}}
	\fi
}
\newcommand\rbestr[2]{{% we make the whole thing an ordinary symbol
		\left.\kern-\nulldelimiterspace % automatically resize the bar with \rbight
		#1 % the function
		\vphantom{\big|} % pretend it's a little taller at normal size
		\right|_{#2} % this is the delimiter
}}
\makeatother


\newcommand{\rb}{\mathbf{r}}

\newcommand{\addcost}{\mathfrak{a}}
\newcommand{\sparseaddcost}{\tilde{\mathfrak{a}}}
\newcommand{\dotcost}{\mathfrak{d}}
\newcommand{\sparsedotcost}{\tilde{\mathfrak{d}}}
\newcommand{\randcost}{\mathfrak{r}}
\newcommand{\matmulcost}{\mathfrak{m}}
\newcommand{\local}{\textrm{ local}}
\newcommand{\remote}{\textrm{ remote}}

\newcommand{\subspacedim}{n_1}\newcommand{\noiserate}{\mu}
\newcommand{\latticematrix}{L}



%opening
\title{Sublinear-Overhead Secure Linear Algebra on a Dishonest Server}
\author{Mark Braverman and Stephen Newman}
\authorrunning{M. Braverman and S. Newman}
\institute{Princeton University}


\begin{document}
\maketitle

\begin{abstract}
	Most heavy computation occurs on servers owned by a second party. This reduces data privacy, resulting in interest in data-oblivious computation, which typically severely degrades performance. Secure and fast remote computation is particularly important for linear algebra, which comprises a large fraction of total computation and is best run on highly specialized hardware often only accessible through the cloud. We state the natural efficiency and security desiderata for fast, remote, and data-oblivious linear algebra, conjecture the existence of matrix and vector families implying satisfactory algorithms, and provide such an algorithm contingent on common cryptographic assumptions. We achieve sublinear overhead for the server, dramatically reduced computation cost for the client, and various other practical advantages over previous algorithms.

    \keywords{Data Privacy, Data-Oblivious Computation, Delegation, Homomorphic Encryption, Cloud Computing, Algorithm Efficiency, Sublinear Overhead, LPN, Matrix Multiplication.}
\end{abstract}

\section{Introduction}

% NOTE: Our LPN problem is just called ``LPN over R''.

Data-oblivious delegated computation -- computation in which the server does not learn anything meaningful about the input data, thanks to cryptographic scrambling by the client -- has been of theoretical and practical interest for some time \cite{pippenger1979relations,ostrovsky1990efficient}. Arbitrary computation is quite difficult in this setting -- against a computationally bounded honest adversary, it requires Fully Homomorphic Encryption (FHE) \cite{rivest1978data,gentry2009fully}, which currently requires several orders of magnitude of computational overhead. As a result, much work on oblivious computation focuses on either advancing schemes for oblivious computation with more limited adversaries \cite{maas2013phantom,zahur2015obliv}, on efforts toward developing and improving the runtime of FHE \cite{fan2012somewhat}, or on Partially Homomorphic Encryption (PHE) \cite{elgamal1985public,goldwasser2019probabilistic}, which attempts to efficiently encrypt data such that \textit{some} operations may be performed obliviously.

A second paradigm, with different uses, has also emerged. Secure Multiparty Computation (MPC) \cite{yao1986generate,goldreich1987solve}, in which several non-interacting adversarial servers attempt computation on the union of datasets private to each server, has allowed for a variety of recent developments in hidden computation protocols. In the context of remote computation, this framework generalizes the case of one client and multiple, non-communicating adversarial servers. Here, a combination of informational and cryptographic techniques often offer efficiency improvements over corresponding PHE problems \cite{mohassel2017secureml,araki2016high,keller2020mp,knott2021crypten}.

Much work in FHE and MPC has focused on lattice techniques. Learning With Errors (LWE), a survey of which is available in \cite{regev2010learning}, is the most well-known cryptographic problem in this area. Broadly speaking, the problem asks a user to distinguish a sequence of random values a sequence of noised inner products of known vectors $a_i$ against some unknown vector $s$. Learning Parity with Noise (LPN), a variant problem, instead asks that the noise be sparse, rather than small-magnitude. 

In recent decades, an increasing fraction of computational power has been dedicated to matrix-vector and matrix-matrix multiplications. Specialized hardware to perform these operations is expensive and power-hungry, and is therefore unusually centralized. We often wish to allow low-power devices (such as personal computing hardware) to do large matrix-vector computations (in evaluating artificial neural networks, for instance) or to allow companies to rent computing power from centralized providers. However, much of the data that we wish to process must be kept private (e.g. medical records, security camera footage, etc.). We therefore desire schemes for remote matrix-vector multiplication that are both secure and require minimal computational overhead -- when even insecure computation is quite expensive, a constant-factor overhead is extremely costly. Schemes which use exclusively operations native to modern GPUs and TPUs are preferred, as while other operations may use the same number of elementary gates, they often are substantially less efficient due to a comparative lack of hardware and software optimization.

\subsection{Our Contributions}

We wish to conduct remote computations of the form $Av_i$ for some matrix $A$ and vectors $v_i$, or matrix-matrix multiplications $AB$, while hiding $v_i, A, B$ from the remote server and minimizing local computational load. We state the natural efficiency and computational security desiderata, provide a meta-algorithm satisfying them contingent on the existence of a new type of pseudorandom vector generation scheme, and conjecture that such a scheme exists. 

We also provide an instantiation of such a scheme under a LPN \cite{kearns1998efficient,heyse2012lapin} hardness assumption. Our scheme is lightweight and easy to implement -- it can perform multiplications over any ring for which the LPN assumption holds, uses only standard vector operations, and therefore achieves practically as well as theoretically small blowup over the naive scheme. Under a standard quantitative LPN hardness assumption (\cite{alekhnovich2003more,boyle2018compressing,yu2021smoothing}, for example), we achieve amortized client cost of $O((m+n)n^{\epsilon})$ for multiplying a $m\times n$ matrix by a length-$n$ vector with $(1+o(1))$ server overhead. This improves on all previous methods known to the authors.


\subsection{Other Related Works}
Several recent papers have focused directly on multiplication of matrices when one or both are encrypted \cite{jiang2018secure,bae2024plaintext,liu2022privacy}. The latter achieves outsourcing which reduces client costs for matrices of size several thousand and higher. While the recent works achieve asymptotic client-time reductions, their savings are limited in practical parameter regimes and come with substantially increased server cost. Most of these works use advanced variants of LWE, and incur a small accuracy error (on the order of $0.01\%$) as a result.

LPN-type problems were first discussed in \cite{kearns1998efficient}, and generalized to LPN in \cite{heyse2012lapin}. Best-known quantitiative hardness result for LPN are discussed in \cite{liu2024hardness}.

Oblivious linear function evaluation (OLE) was, to the best of our knowledge, first specialized as a problem of particular interest by \cite{dottling2017tinyole}, who provided improvements to the polynomial evaluation case of \cite{naor1999oblivious} (while we do not consider MPC or OLE directly, the following work represents some of the more advanced uses of LPN to date). The usefulness of LPN for linear computation with low communication in various MPC contexts was recognized by \cite{boyle2018compressing}, and was expanded into a broad MPC regime by \cite{boyle2019efficient,boyle2020efficient}. The former paper presents a MPC algorithm that can be turned into a vector-secure matrix-vector multiplication algorithm, but focuses on goals corresponding to the MPC setting. The latter presents, among many other interesting contributions, an object of interest in this paper (though they do not note that it has the full property, as they use it for other purposes). This line of OLE work continued into more general function categories \cite{boyle2020correlated,couteau2023pseudorandom}. Low-overhead MPC linear-algebra algorithms were considered in \cite{applebaum2017secure} and \cite{applebaum2023actively}, including one obtaining constant computational overhead for secure two-party arithmetic computation with honest adversaries. \cite{chen2020maliciously} used alternate techniques to achieve matrix multiplication with linear communication in an all-but-one-adversarial MPC framework, but incurs substantial computational blowup and algebraic complexity.

Secure matrix-vector multiplications is useful in several contexts, including in evaluation of neural networks \cite{barak2019secure,mohassel2017secureml,mann2023towards}. 


\paragraph{Note:} In a concurrent and independent work, Vaikuntanathan and Zamir\footnote{A reference to their work will be added in subsequent versions after both papers become publicly available.} suggest and use a recursive construction very similar to that of our Subsection~\ref{sec:improvedlpn} in a different context motivated by accelerating algorithms that use random matrices. Their construction can be used to slightly improve our asymptotic server cost (removing the term in $\delta$) while increasing client cost.

\section{Fast Remote Computation}
We consider linear-algebraic secure delegated computation problems. Assume a client with some private data (e.g. two matrices), a public function to be applied to the data to achieve some result (e.g. the product of those matrices), and a server. The client wishes to obtain the result of the function application through a combination of local computation and computation performed by the server.

Since we consider clients that are much less computationally able than servers, we are interested in protocols which achieve the following three properties:
\begin{itemize}
	\item The aggregate computation is very low (at most low-factor linear in, and ideally asymptotically equal to, the naive computational cost of the function).
	\item The aggregate computation by the client is much less than that required to compute the function on its own (and ideally, nearly linear in input size).
	\item The protocol hides the input data from the server.
\end{itemize}

We formalize these notions. For security, we use the standard definition:
\begin{definition}
	We say that a protocol is $(T(\cdot, \cdot), \epsilon(\cdot, \cdot), N(\cdot))$-secure if it has a security parameter $k$ such that, for all $n\geq N(k)$, the distributions of server views generated by any two client inputs are $(T(n, k), \epsilon(n, k))$-indistinguishable.
\end{definition}


For efficiency, we compare to the unencrypted cost of the function:
\begin{definition}
	We say that a protocol is $(c_c(n, k), c_s(n, k))$-remote-efficient if the client and the server expend $c_c(n, k)$ and $c_s(n, k)$ times the computation required to compute the function without encryption.
\end{definition}
We will also consider a problem with an initial phase and an online phase, where a sequence of instances must be solved as they are received. For this, we extend the above:
\begin{definition}
	We say that an initial-online protocol is $(o_c(n, k), o_s(n, k))$-remote-efficient with $(i_c(n, k), i_s(n, k))$ startup if, after client initial computational cost of $i_c(n, k)$ and server initial computational cost of $i_s(n, k)$, the protocol is, on a per-instance basis, $(o_c(n, k), o_s(n, k))$-efficient.
\end{definition}

$(0, 1)$-remote-efficiency corresponds to the server performing the plaintext computation without client interaction. We aim for security exponential in $k$ and $(o(1), 1+o(1))$-remote-efficiency for $k=O(n)$; that is, protocols which have high security, negligible aggregate computation overhead, and low-multiple-of-datasize computation on the client. We also attempt to minimize the number of rounds.

\section{Two Problems of Interest}

We consider two particularly salient problems of linear algebra. The first is simple.

\begin{definition}[Matrix-Matrix]
	Given two $n\times n$ matrices over a ring $R$, return their product.
\end{definition}

The second is slightly more complex:
\begin{definition}[Matrix-Vectors]
	Given a $n\times n$ initialization matrix $M$ over a ring $R$ and an online stream of vectors $v_1, v_2, \dots\in R^n$, return the online stream $Mv_1, Mv_2, \dots$.
\end{definition}

While these problems are comparable in absolute computational cost, they are not in terms of remote efficiency. In particular, a secure remote protocol for Matrix-Vectors yields a natural protocol for Matrix-Matrix (by multiplying one column at a time), but that protocol does not enjoy the same remote efficiency as the Matrix-Vectors protocol (as the naive cost of Matrix-Matrix is lower than that of a length-$n$ Matrix-Vectors stream, thanks to fast matrix multiplication algorithms). Both are potentially useful, and merit study.

Under a standard LPN hardness assumption, we show the following:
\begin{theorem}
    \label{thm:matrix_vector}
	Given Conjecture \ref{conj:lpn-hardness}, there exists $\epsilon<1$ s.t. for any $\delta>0$, there exists a one-round-per-vector Matrix-Vectors protocol with, for $m\geq n^\epsilon$, client and server per-vector computation costs 
    \begin{equation*}
        \left(O\left(\frac{1}{1-\delta}(n+m)n^{\epsilon}\right), \matmulcost\left(m+\frac{\delta}{1-\delta}n, n, 1\right)\right)
    \end{equation*}
    respectively, client and server startup costs
    \begin{equation*}
        \left(O\left(mn^{1+\epsilon} \frac{1}{1-\delta^{\epsilon}}\right), \matmulcost\left(\frac{\delta}{1-\delta}n, n, m\right) + O\left(\delta^3 \frac{1}{1-\delta^{\omega-1}} n^\omega\right)\right)
    \end{equation*}
    respectively (where $\matmulcost\left(i, j, k\right)$ is the cost of one $i\times j$ by $j\times k$ matrix multiplication), and security
    \begin{equation*}
        (2^{\tilde{\Omega}_\delta(n^c)}, 2^{-\tilde{\Omega}_\delta(n^c)}, O(n))
    \end{equation*}
    for some $c>0$.

    In particular, this is $\left(\frac{1}{1-\delta} n^{\epsilon-1}, \left(1 + \frac{\delta}{1-\delta}\frac{n}{m}\right)\right)$-remote-efficient with startup.
\end{theorem}
In the most obvious regime, $m=\Theta(n)$ and across $\Theta(n)$ multiplications, the client gains an amortized $O\left(\frac{1}{1-\delta}n^{1-\epsilon}\right)$-speedup and the server suffers an amortized $\frac{1}{1-O(\delta)}$-multiplicative overhead. Efficiency increases as $m$ grows. Both the regime and the performance of the theorem are substantially improved by slightly stronger LPN assumptions (see Section \ref{sec:lpn-assumption}).

Two notes on extension:
\begin{itemize}
    \item Any scheme for either problem which hides the inputs can also probabilistically detect dishonest behavior by adding either some rows/columns of zeros (in Matrix-Matrix) or some zero vectors (in Matrix-Vectors) to the input.
    \item As in \cite{bae2024plaintext}, we can reduce the floating-point case (of most practical use) to a modular integer case fairly efficiently via a folklore reduction.
\end{itemize}







\section{Primitives for Fast Linear Algebra}

Our appproach to security in Matrix-Vectors is as follows:
\begin{enumerate}
    \item Generate pseudorandom $A'$ (once) and $v_i'$
    \item Send $A+A'$ (once) and $v_i+v_i'$, and request $(A+A')(v_i+v_i)'$
    \item Compute $Av_i'$, $A'(v_i+v_i')$
    \item Return $(A+A')(v_i+v_i)'-Av_i'-A'(v_i+v_i')$
\end{enumerate}
This is, in effect, a client-server-oriented modification of Beaver's triples approach to MPC \cite{beaver1992efficient}. While this is secure (up to the PRNG generating $A', v_i'$) by the same arguments, it is not clearly efficient: it requires two local matrix-vector multiplications, including multiplication of $A'$ and $v'_i$ by arbitrary values. To achieve an improvement in efficiency, we must pick $A'$ and $v'$ with specific structure to make these multiplications easy, inspiring the following cryptographic primitives:


\begin{definition}[Trapdoored-Matrix]
    A $(T(\cdot, \cdot), \epsilon(\cdot, \cdot), N(\cdot))$-secure, $C(\cdot, \cdot)$-efficient Trapdoored-Matrix scheme is an algorithm that, given a ring $R$ and $m, n, k\geq N((m, n, R))\in \Z$, generates a $(T((m, n, R), k), \epsilon((m, n, R), k))$-pseudorandom $m\times n$ matrix $M$ over $R$ and $\tilde{O}(C(m, n))$ additional data such that, for any $v$, $Mv$ can be computed in time $C(m, n)$.
\end{definition}

\begin{definition}[Trapdoored-Vectors]
    A $(T(\cdot, \cdot), \epsilon(\cdot, \cdot), N(\cdot))$-secure, $C(\cdot, \cdot)$-efficient Trapdoored-Vectors scheme is an algorithm that, given a ring $R$, a matrix $A\in R^{m\times n}$, and $k\geq N((m, n, R))\in \Z$, returns a $\tilde{O}(C(m, n))$-sized generator that generates $(T((m, n, R), k), \epsilon((m, n, R), k))$-jointly-pseudorandom (conditioned on $A$) vectors $v_1, v_2, \dots$ and the associated products $Av_i$ in time $C(m, n)$ per vector-product pair.
\end{definition}

Combined, these make the template scheme for Matrix-Vectors fast: use the Trapdoored-Matrix generator to generate $A'$, and use the Trapdoored-Vectors generator to generate the various $v'_i$. An analogous scheme, using only Trapdoored-Matrix, achieves secure and fast remote Matrix-Matrix.

Note that these assumptions are stronger than the fast-pseudonrandom-matrix assumption of \cite{applebaum2017secure}: that assumption only requires that images under the matrix are random once noised. Many constructions satisfying that assumption (for instance, any sparse matrix) are insufficient here. A construction of \cite{boyle2020efficient} (Sec 10.3; the first construction with the Toeplitz/quasi-cyclic assumption) does achieve the Trapdoored-Matrix criteria, albeit with significantly stronger assumptions and probable worse performance at equal security levels (as it was optimized for a substantially different task).

Assuming a substantially sized cache, we can obtain amortized Trapdoored-Vectors from Trapdoored-Matrix: generate a matrix $M$, compute $AM$, and then use the columns of $M$ as the vectors $v_i$. Since we already need to store $A$, this space overhead is constant-factor.

We know of many families of matrices (Vandermonde matrices, Cauchy Matrices, discrete Chebyshev matrices) for which multiplication is $\tilde{O}(n)$. Since linear combinations of these or other fast-multiplication matrices could plausibly be pseudorandom, it seems natural to believe the following strengthening of Theorem \ref{thm:matrix_vector}:
\begin{conjecture}
\label{conj:hiddenFastMatrix}
	There exist Trapdoored-Matrix constructions with amortized vector-product computation time $\tilde{O}(n)$.
\end{conjecture}

The complexity of Trapdoored-Vectors is less clear. Under our conservative LPN assumption, we achieve \ref{thm:matrix_vector}, which claims a $n^{\epsilon}$ overhead for some $\epsilon<1$. It is very reasonable to believe that the security assumption holds for \textit{any} $\epsilon>0$ (again see Section \ref{sec:lpn-assumption}). Security of LPN in the dimension regime required for polylogarithmic overhead is not clear; if a Trapdoored-Vectors scheme exists here, another approach may be needed.

\section{A LPN Hardness Assumption and Performance Scaling}
\label{sec:lpn-assumption}
The decisional version of LPN is to, given a ring $R$ and a noise rate $\noiserate$, distinguish messages of the form $(L, Lr+s)$ from messages of the form $(L, u)$ where $L\sampledfrom R^{n\times\subspacedim}$, $r\sampledfrom R^{\subspacedim}$, $u\sampledfrom R^{n}$, and $s\leftarrow \mc{D}_{R, \noiserate}^n$ where $\mc{D}_{R, \noiserate}$ is the distribution that is 0 with probability $1-\noiserate$ and uniform over $R$ w.p. $\noiserate$. A standard (as in \cite{alekhnovich2003more,boyle2018compressing,yu2021smoothing}) hardness assumption for LPN is the following:
\begin{conjecture}[Standard LPN Hardness Assumption]
    \label{conj:lpn-hardness}
    $\exists \epsilon<1$ such that fixing any $\delta>0$, there exists $n^*\in \N, c>0$ such that for any $n>n^*, \subspacedim=\delta n$, $\noiserate = n^{\epsilon-1}$, no algorithm running in time $T=2^{O_\delta(n^{c})}$ can solve decisional-LPN with parameters $n, \subspacedim, \noiserate$ with advantage $\geq \epsilon=2^{-\Omega_\delta(n^c)}$. 
\end{conjecture}

This assumption comes with an equivalent dual: that given a matrix $P\sampledfrom R^{n\times (n-\subspacedim)}$ and $s\leftarrow \mc{D}_{R, \noiserate}^n$, distinguishing $Pe$ from uniform noise is difficult (note that equivalence follows from $sP=(Lr+s)B$ in the case where $L^\top P=0$).

Both $\delta$ and $\epsilon$ directly correspond to the parameters by the same names in Theorem \ref{thm:matrix_vector}. In particular, slightly stronger assumptions directly translate into significant improvements to the efficiency of the algorithm. For instance, if $\delta$ is allowed to be $o(1)$, we achieve submultiplicative overhead for $\frac{m}{n}\geq \delta$, rather than just $m=\Omega(n)$. All terms of the form $n^\epsilon$ correspond to $n\mu$; if we assume that $\mu(n)=n^{\epsilon-1}$ is secure for \textit{any} $\epsilon>0$, or even the very strong $\mu(n) = \tilde{O}\left(\frac{1}{n}\right)$ (as in \cite{brakerski2019worst}), we obtain the corresponding substantial efficiency improvements.


\section{Constructions with LPN}
\subsection{The Simple Construction}
We first use LPN to construct a scheme for Trapdoored-Vectors, and use it naively in the above template. Our scheme for Trapdoored-Vectors is detailed in Algorithm \ref{alg:hfv}. 

\algnewcommand\Input{\State\textbf{Input: }}
\algnewcommand\Output{\State\textbf{Output: }}
\algnewcommand{\inner}[1]{\lbangle#1\rbangle}

\begin{algorithm}
\caption{A Basic Trapdoored-Vectors Scheme}
\label{alg:hfv}
\begin{algorithmic}[1]
	
\Procedure{HFV-Init}{}
\Input $A\in R^{m\times n}$, $\subspacedim\in \Z_{>0}$, $\noiserate\in \R_{>0}$
\State $L \sampledfrom R^{n\times \subspacedim}$
\State $\mathbf{Compute}(AL)$ \Comment{Offloaded if $A$ is public.}
\State \Return
\EndProcedure

\Procedure{HFV-Gen}{}
\State $r \sampledfrom R^{\subspacedim}$
\State $s \leftarrow \mc{D}_{R, \noiserate}^n$
\State $v' = Lr+s$
\State $\inner{Av'} = (AL)r + As$
\State \Return $v', \inner{Av'}$
\EndProcedure

\end{algorithmic}
\end{algorithm}

The security of this scheme is a direct consequence of the LPN assumption, which implies that vectors distributed as $v'=Lr+e$ are pseudorandom conditioned on revelation of $L$.

The per-vector computational cost is $(m+n)$ length-$\subspacedim$ dot products plus $m$ $n\noiserate$-sparse dot products, plus $m+n\noiserate$ additions, totaling to
\begin{equation}
    \label{eq:alg1cost}
    O((m+n) \subspacedim + mn\noiserate).
\end{equation}

For most parameter regimes, the dominant term is $O((m+n)n_1)$ (since $\delta=\Theta(1), \mu=o(1)$. The term in $\subspacedim$ can be greatly reduced via LPN variants over fast matrices (e.g. sparse-LPN, Toeplitz-LPN, Quasi-cyclic-LPN \cite{aguilar2018efficient}), but parameter tradeoffs for these problems over large rings are not currently well-understood, and may entail worse $\mu$. In the next subsection, we present an alternate option that obtains equal or better performance (depending on the comparative sparsity assumptions of LPN and sparse-LPN) than a sparse-LPN version, but depends only on standard LPN assumptions.

In the case where $A$ is public, we can also achieve dramatic improvements via dual-LPN. This simple substitution (i.e. calculating $v'=Ps$, $Av'=(AP)s$ for a public random square matrix $P$ takes the cost down to $O(mn\mu)$. The dual formulation will not, however, be sufficient to achieve very fast private matrix-vector.




\subsection{Improved Performance via Recursion}
\label{sec:improvedlpn}

The above protocol's efficiency is directly controlled by the $\delta$ of the LPN assumption, which is assumed to be rather large. This is not (just) an artifact of our weak assumption: the security of LPN does not degrade gracefully as its parameters are reduced. In particular, once $\subspacedim\noiserate\leq 1$, straightforward attacks based on subsampling and Gaussian elimination are possible. This increases the necessary overhead of Algorithm \ref{alg:nonrec} in the case of very large inputs. In this section, we present a more complicated algorithm that bypasses this problem to improve performance

When used to construct a fast and secure matrix-vectors protocol, the client cost of our earlier scheme is dominated by the costs of the multiplications $AL$ and $(AL)r_i$ (in regimes where $\subspacedim\gg n\noiserate$, as is standard). There is a natural solution: recursively call the remote server to compute these while not leaking information about $A$ or $r_i$.

This results in a fairly complicated recursive protocol with several disadvantages, including a number of rounds growing exponentially in the recursion depth and a substantial amount of duplication of work. With optimization, however, the complexity can be dramatically reduced. We detail the resulting algorithm for fast hidden matrix-vector in Algorithm \ref{alg:nonrec}. Note that in the algorithm, angled brackets around an expression (e.g. $\inner{Av_i}$) corresponds to a single variable equal at assignment to the result of that expression. 

An analogous (and simpler) algorithm exists for matrix-matrix.

\begin{algorithm}

{Procedures are written for the client. The $\mathbf{Send}$ and $\mathbf{Request}$ commands are used to send information and to request computation results from the server, respectively. The server only stores information and performs simple linear algebra computations over $R$. Note that for implementation and efficiency, $L_i$s are shared over all routines; we write them as samples separately for modularity.}

\caption{Client Routines for LPN-based Matrix-Vectors}
\label{alg:nonrec}
\begin{algorithmic}[1]
\Procedure{Vec-Hidden-Init}{}
\Input $A\in R^{m\times n}$, $n_i\forall i\in [d], \noiserate_i\forall i\in [d]$
\For{$i\in [d]$}
$L_i \sampledfrom R^{n_{i} \times n_{i-1}}$
\EndFor
\State $\mathbf{Send} (A, L_1, \dots, L_d)$, $\mathbf{Request}(L_1L_2\dots L_i, AL_1L_2\dots L_i)\forall i\in [d]$
\EndProcedure


\Procedure{Vec-Hidden-Mult}{}
\Input $A\in R^{m\times n}, v\in R^m$.
\State $r\sampledfrom R^{n_d}$
\State $\forall i\in [d], s_i\sampledfrom \mc{D}_{\noiserate_i}^{n_{i-1}}$
\State $v'\gets (L_1L_2\dots L_d) r + \sum_{i=1}^d (L_1L_2\dots L_{i-1}) s_i$
\State $\inner{Av'}\gets (AL_1L_2\dots L_d) r + \sum_{i=1}^d (AL_1L_2\dots L_{i-1}) s_i$
\State \Return $\mathbf{Request}(A(v+v')) - \inner{Av'}$.
\EndProcedure


\Procedure{Hidden-Init}{}
\Input $A\in R^{m\times n}$, $n_i\forall i\in [d], \noiserate_i\forall i\in [d-1]$
\For{$i\in [d]$}
$L_i \sampledfrom R^{n_{i} \times n_{i-1}}$, $S_i\gets \mc{D}_{\mu_i}^{m\times n_{i-1}}$
\EndFor
$H\sampledfrom R^{m\times n_d}$.
\For{$i\in [d]$}
$\mathbf{Send}(L_i), \mathbf{Request}(L_1L_2\dots L_i)$
\EndFor
\State $C\gets [L_1, L_1L_2, \dots, L_1L_2\dots L_d]$ \Comment{as a block matrix}
\State $\textproc{Vec-Hidden-Init}(C^\top)$
\State $[\inner{AL_1}, \inner{AL_1L_2}, \dots, \inner{AL_1\dots L_d}]\gets \textproc{Vec-Hidden-Mult}(C^\top, A^\top)^\top$ \Comment{Divide $C^\top$ into a row-vector of blocks for better efficiency}
\State $A' = H\inner{L_1L_2\dots L_d}^\top + \sum_{i=0}^{d-1} S_{i+1} \inner{L_1L_2\dots L_i}^\top$
\State $\mathbf{Send}(A+A')$
\EndProcedure

\Procedure{Hidden-Mult}{}
\Input $A\in R^{m\times n}$ (for which \textproc{Client-Init} has already been called), $v\in R^m$. If a matrix is passed instead, perform a matrix-matrix multiplication by performing several matrix-vector multiplications. Hide both inputs.
\State $r\sampledfrom R^{n_d}$
\For{$i\in [d]$}
$s_i\gets \mc{D}_{\mu_i}^{n_{i-1}}$
\EndFor
\State $v'\gets \inner{L_1L_2\dots L_d} r + \sum_{i=1}^d \inner{L_1L_2\dots L_{i-1}}s_i$
\State $\inner{Av'} \gets \inner{AL_1L_2\dots L_d} r + \sum_{i=1}^d \inner{AL_1L_2\dots L_{i-1}}s_i$
\For{$i\in [d]$}
$\mathbf{Request}((L_1L_2\dots L_{i})^\top (v+v'))$.
\EndFor
\State $\inner{A'(v+v')}\gets H\inner{(L_1L_2\dots L_{d})^\top (v+v')} + \sum_{i=0}^{d-1} S_{i+1}\inner{(L_1L_2\dots L_{i})^\top (v+v')}$
\State \Return $\mathbf{Request}((A+A')(v+v')) - \inner{Av'} - \inner{A'(v+v')}$


\EndProcedure

\end{algorithmic}
\end{algorithm}

\subsection{Security Guarantees}

As before, security of the algorithm follows trivially from pseudorandomness of $A'$, $v'$ conditioned on revelation of $L_1, \dots, L_d$. This will again follow from the LPN assumption.

\begin{theorem}
	For $i\in [d]$, let $L_i \sampledfrom R^{n_{i} \times n_{i-1}}$. From this, for $j\in \{0, 1, \dots, d\}$, let $\mc{E}_j = \{L_1L_2\dots L_j r_j + \sum_{i=1}^j (L_1 L_2 \dots L_{i-1})s_i|r_j\sampledfrom R^{n_j}, s_i\leftarrow \mc{D}^{n_{i-1}}_{\mu_i}\}$. Say that, for all $j\in [i]$, the distributions of $r_{j-1}$ and $L_j r_j + s_j$ are $(T, \epsilon_j)$-indistinguishable. Then $\mc{E}_0$ and $\mc{E}_d$ are $(T-\Omega(\sum_{i=1}^d n_{i-1}n_i), \sum_{i=1}^d \epsilon_i)$-indistinguishable.\footnote{Note that the $\Omega(\sum_{i=1}^d n_{i-1}n_i)$ here represents the cost of multiplying the various $L_i$ together.}. 
\end{theorem}
\begin{proof}
	We prove the contrapositive, which follows  by a hybrid argument. Let there be an algorithm $\mc{A}$ distinguishing $\mc{E}_0$ and $\mc{E}_d$ with advantage $\geq \sum_{i=1}^d \epsilon_i$. Then there exists $j\in [d]$ s.t. $\mc{E}_{j-1}$ and $\mc{E}_{j}$ so that $\mc{A}$ distinguishes them with advantage $\geq \epsilon_j$. From here, we construct an algorithm distinguishing $r_{j-1}$ and $L_{j} r_{j} + s_{j}$ with advantage $\geq \epsilon_j$: simply left-multiply by $L_1L_2\dots L_{j-1}$ to turn the distributions of the above terms into $\mc{E}_{j-1}$ and $\mc{E}_j$, respectively.
\end{proof}

\begin{corollary}
	Under the same assumptions as the above lemma, Algorithm \ref{alg:nonrec} is $(T-\Omega(\sum_{i=1}^d n_{i-1}n_i), 2\sum_{i=1}^d \epsilon_i, O(n))$-secure.
\end{corollary}
\begin{proof}
	It suffices to observe that $\mc{E}_0$ is uniformly random and that both $v'$ and the rows of $A'$ are distributed as $\mc{E}_d$. Then conditioned on $L_1, \dots, L_d$, $v+v'$ and $A+A'$ are both $(T-\Omega(n^2), \sum_{i=1}^d \epsilon_i)$-indistinguishable from uniformly random vectors/matrices in time $T-\Omega(n^2)$.
\end{proof}

\subsection{Algorithm Structure}

The main technique here is to observe that if the public $L_iL_{i+1}$ multiplications are precomputed, recursing on LPN makes the required dense matrix-vector mulitplication substantially smaller, while introducing a sparse one. As the aforementioned multiplications can be offloaded, we can recurse to smaller instances, decoupling our instance size from the security/cost tradeoff. In essence, we have a LPN variant whose security and computational cost both degrade gracefully in the base dimension.

Under our LPN assumption, we get good asymptotic performance by setting $\delta$ to be a uniform constant and choosing $\mu_i$ adaptively (i.e. having it increase sublinearly in $n_{i-1}$). For instance, take the standard high-dimension, low-noise LPN hardness assumption: that there exists $n^*\in \N$ and $0< \epsilon<1$ such that LPN in dimension $n>n^*$ with subspace dimension $\subspacedim=\delta n$ and noise rate $\noiserate=(n^*/n)^{1-\epsilon}$ is $(2^{\tilde{\Omega}_\delta(n^c)}, 2^{-\tilde{\Omega}_\delta(n^c)})$-indistinguishable from noise in $2^{O(n^c)}$ time for some $c>0$. Then by setting $n_d=n^\epsilon$, we obtain Theorem \ref{thm:matrix_vector}. Note that a slightly stronger assumption -- that $\delta$ can be allowed to decrease asymptotically -- gives significantly better performance: see subsection \ref{subsec:practical}.

\subsection{Performance}

We present a high-level analysis here. A more detailed performance analysis, including exact operation counts, is available in Appendix~\ref{sec:detailedperformance}.

The startup runtimes are \[\matmulcost\left(\frac{\delta}{1-\delta}n, n, m\right) + O\left(\delta^3 \frac{1}{1-\delta^{\omega-1}} n^\omega\right)\] and \[O\left(mn^{1+\epsilon} \frac{1}{1-\delta^{\epsilon}}\right)\] for the server and client respectively, where $\matmulcost(a, b, c)$ is, as before, the cost of multiplying an $a\times b$ matrix by a $b\times c$ matrix. The server runtime per matrix-vector multiplication is \[\leq 1 + \frac{\delta}{1-\delta}\frac{n}{m}\] times the naive runtime of a $m$-by-$n$ matrix-vector product, and the client runtime is \[O((n+m)n^\epsilon),\] as opppsed to the $\Theta(mn)$ required to compute locally.

This protocol has several other efficiency advantages:
\begin{itemize}
	\item A multiplication requires one round trip, and transmits $\leq 1+\frac{1}{1-\delta}$ times the data that \textproc{Naive-Remote} does.
	\item Assuming that $m=\Omega(n)$, storage costs are $\leq O(\delta)$ times the base cost of storing $A$ ($A'$ may freely be stored only in its component form).
	\item Amortized across $\Omega(n)$ vector multiplications, initialization costs are low.
\end{itemize}

Finally, and most critically, this algorithm is easy to implement and uses exclusively commonly optimized and hardware-accelerated vector operations, implying practical as well as theoretical efficiency.

\subsection{Practical Improvements}
\label{subsec:practical}

For simplicity, we proved bounds in the case where $\delta$ is constant across layers. This is inefficient; by shrinking $\delta$ in larger layers to get approximately uniform security, dramatic improvements (in the server's $o(1)$ term and the client's full cost) can be achieved.

Careful readers may note that this algorithm's pseudorandom generation is very similar to, but not reducible to (due to the existence of an unmultiplied sparse randomness term), a nonrecursive instance of dual-LPN on the block syndrome matrix
\begin{align*}
    \left[L_1, L_1L_2, \dots, L_1L_2\dots L_d \right]
\end{align*}

It may initially appear that we can simply substitute dual-LPN for this entire process and gain dramatic client performance and simplicity improvements. We can, but this comes at factor-two cost to the server: the ancillary product of line 27 grows dramatically. We can, however, achieve nearly the same improvements by substituting dual-LPN for the \textit{final} recursive step (i.e. replacing the generation $L_dv+s_d$ by $Ps_d$). This entirely eliminates the client dense multiplication, which is quite costly in regimes where $n$ is small. This does increase the server overhead, but this increase is on the multiplicative order of $\delta^d$. As a result, client performance improves substantially (and server performance decreases marginally) when $n$ is close to $n_d$, which occurs when $n$ is low or particularly high security is desired.


\section{Conclusion and Further Questions}
We present natural definitions of Trapdoored-Matrix and Trapdoored-Vectors objects, a method to turn instances of these objects into protocols for secure remote linear algebra, and instances of Trapdoored-Vectors and Trapdoored-Matrix that can be optimized to yield highly efficient versions of these protocols. It is not clear that these are the only (or best) primitives for this purpose, especially in the case of Trapdoored-Matrix, where evidence suggests the existence of notably faster pseudorandom matrices. Variants of our schemes may also be extensible to more general problems; harder problems in linear algebra, such as matrix polynomial evaluation, are a reasonable next step. Achieving a variety of operations parallel to that of \cite{boyle2020correlated,couteau2023pseudorandom}, or even \cite{bae2024plaintext} (while achieving practical efficiency gains) seems to be a significantly more challenging problem, as the problems that they consider do not have structure as amenable to our encryption scheme, but would yield even broader applications.


\begin{credits}
\subsubsection{\ackname}
Research supported in part by the NSF Alan T. Waterman Award, Grant No. 1933331.

Thanks to Alex Lombardi for helpful discussion on LPN and related topics.
\end{credits}

\bibliographystyle{splncs04}
\bibliography{main}



\appendix


\section{Detailed Performance Analysis of Algorithm \ref{alg:nonrec}}
\label{sec:detailedperformance}

Let $\addcost(x)$ be the cost of adding/subtracting two $x$-element tensors, $\sparseaddcost(x)$ be the cost of adding/subtracting two tensors, one of which is $x$-sparse, $\dotcost(x)$ be the cost of taking the dot-product of two $x$-element tensors, $\sparsedotcost(x)$ be the cost of taking the dot product of two vectors, one of which is $x$-sparse, $\randcost(x)$ be the cost of generating $x$ independent and uniform random elements of our ring, and $\matmulcost(a, b, c)$ be the cost of multiplying two matrices, one of which is $a\times b$ and the other of which is $b\times c$. All operations are relative to our working ring.

The client cost of \textproc{Vec-Hidden-Init} to the client is simply the generation and transfer costs of the various matrices, all of which have size sublinear in $A$ (assuming $m=\Theta(n)$). Note that this (and indeed, the whole routine) is inefficient in the case where $A$ is much wider than it is tall; in this case, it is best to split $A$ into a row of blocks, split an input vector into a column of blocks, and work on each separately. The remote cost is simply $\matmulcost(m, n, n_1) + \sum_{i=2}^d \matmulcost(m+n, n_{i-1}, n_i)$ (by blocking the multiplications $(AL_1L_2\dots L_{i-1})L_i$ and $(L_1L_2\dots L_{i-1})L_i$). The dominant term is the first (assuming $n>m$).

The client cost of \textproc{Vec-Hidden-Mult} is mostly concentrated in lines 9 and 10. By combining them into a sum of a dense block calculation and a sparse block calculation, we reduce the cost to $\matmulcost(m+n, n_d, 1) + (m+n)\sum_{i=1}^d \sparsedotcost(n_{i-1} \mu_i)$. The final line adds only $\addcost(n)+\addcost(m)$, giving total cost $\matmulcost(m+n, n_d, 1) + (m+n)\sum_{i=1}^d \sparsedotcost(n_{i-1} \mu_i) + \addcost(n)+\addcost(m)$. The server cost is simply $\matmulcost(m, n, 1)$ -- exactly one matrix-vector multiplication of the relevant dimension.

The client cost of \textproc{Hidden-Init} is dominated by the cost of computing $A'$. In particular, the two costly lines are the call to \textproc{Vec-Hidden-Mult} on line 18 (right-hidden-multiplying a $\sum_{i=1}^d n_i$-by-$n$ matrix by a $n$-by-$m$ matrix) and the internal construction of $A'$ (equal to $\matmulcost(m, n_d, n) + \sum_{i=0}^{d-1} m \sparsedotcost(n_i \mu_{i+1})$). The server cost consists of the iterated multiplications on line 15 (equal to $\sum_{i=2}^d \matmulcost(n, n_{i-1}, n_i)$).

The cost of \textproc{Hidden-Mult}, apart from the generation of the short string $r$, consists of the computations of $v'$, $Av'$, and $A'(v+v')$. Under appropriate block multiplication, $v'$ and $Av'$ may jointly be computed with cost $\matmulcost(n+m, n_d, 1) + \sum_{i=0}^d (n+m) \sparsedotcost(n_{i}\mu_{i+1}) + d\addcost(n+m)$. $A'(v+v')$ is computed with cost $\matmulcost(m, n_d, 1) + \sum_{i=0}^{d-1} m\sparsedotcost (n_i \mu_{i+1})$ + $d\addcost(m)$, and the final result is computed with cost $2\addcost(m)$. The cost to the remote is the cost of the requested vector multiplications: $\sum_{i=0}^d \matmulcost(m, n_i, 1)$. Then the total cost is $\matmulcost(n+m, n_d, 1) + \matmulcost(m, n_d, 1) + \sum_{i=0}^{d-1} (n+2m) \sparsedotcost(n_i \mu_{i+1}) + d\addcost(n+m) + (d+2) \addcost(m)$.

\subsection{Performance in the Parameter Regime of Conjecture \ref{conj:lpn-hardness}}

We compute the cost of \textproc{Hidden-Mult}, since \textproc{Vec-Hidden-Mult} is significantly more efficient. Assuming that $n_i\leq \delta n_{i-1} \forall i$, the remote cost is $\leq 1+\frac{\delta}{1-\delta}\frac{n}{m}$ times that of the function itself.

The client cost consists of 
\begin{itemize}
	\item Two matrix-vector multiplications from dimension $n_d$ to dimensions $n+m$ and $n$ (considered negligible, as $n_d$ is considered a constant).
	\item For each $i\in \{0, 1, \dots, d-1\}$, $n+2m$ sparse dot products with sparsity $n_i\mu_{i+1}$
	\item $d$ vector additions in dimension $n$, and $d+2$ vector additions in dimension $m$.
\end{itemize}

The second term is dominant. In particular, apart from $d=O(\log n)$ additions in input and output dimension and the thin matrix multiplications, we do a total of $(n+2m)\sum_{i=0}^{d-1}n_i\mu_{i+1}$ additions and multiplications. 

The startup costs are slightly more complicated. $C$ is $n\times (\sum_{i=1}^d n_i)\leq \frac{\delta}{1-\delta} n$, and it costs the server 
\begin{align*}
    O\left(\sum_{i=2}^d \delta^{i+1} (\delta^{i+1} n)^\omega\right) &= O\left(\delta^3 \frac{1}{1-\delta^{\omega-1}} n^\omega\right)
\end{align*}
to compute, where $\omega$ is the matrix multiplication constant. The \verb|Vec-Hidden-Init| call has the same computational cost (for both parties). Computing the $A$-products on line 18 has cost $mn^{1+\epsilon}$ to the client and consists of one $\frac{\delta}{1-\delta}n \times n$ by $n\times m$ matrix multiplication for the server. Computing $A'$ on line 19 costs one $m\times n^\epsilon$ by $n^\epsilon\times n$ multiplication for the client and $O\left(mn \frac{1}{1-\delta^{\epsilon}}n^{\epsilon} \right)$ arithmetic operations in sparse matrix multiplications. In total, the client startup cost is $O\left(mn^{1+\epsilon} \frac{1}{1-\delta^{\epsilon}}\right)$, and the server startup cost is just the cost of computing $C$ plus the $AC$ computation.



\end{document}

