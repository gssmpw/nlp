%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

%\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
\documentclass[conference, compsoc, 12pt]{IEEEtran}
% \documentclass[12pt]{article}
%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

% % *** CITATION PACKAGES ***
% %
% \ifCLASSOPTIONcompsoc
%   % IEEE Computer Society needs nocompress option
%   % requires cite.sty v4.0 or later (November 2003)
%   \usepackage[nocompress]{cite}
% \else
%   % normal IEEE
%   \usepackage{cite}
% \fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{lipsum}
\usepackage[most]{tcolorbox}
%\usetikzlibrary{}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{multirow}
%\usepackage{showframe}
\usepackage{blindtext}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{forest}
\usepackage{setspace}
\usepackage{supertabular}
\usepackage{makecell}
\usepackage{afterpage}
\usepackage{float}
\usepackage{longtable}
% \usepackage[dvipsnames]{xcolor}
% \definecolor{lightgray}{rgb}{0.86, 0.84, 0.82}
% \definecolor{thistle}{rgb}{0.85, 0.75, 0.85}
% \definecolor{lightthistle}{rgb}{0.35, 0.25, 0.35}
% \definecolor{seashell}{rgb}{1.0, 0.96, 0.93}
% \definecolor{platinum}{rgb}{0.9, 0.89, 0.89}
% \definecolor{peachpuff}{rgb}{1.0, 0.85, 0.73}
% \definecolor{papayawhip}{rgb}{1.0, 0.94, 0.84}
% \definecolor{oldlace}{rgb}{0.99, 0.96, 0.9}
% \definecolor{mistyrose}{rgb}{1.0, 0.89, 0.88}
% \definecolor{moccasin}{rgb}{0.98, 0.92, 0.84}

\definecolor{mycolor1}{rgb}{1.0, 0.95, 0.80}
\definecolor{mycolor2}{rgb}{0.81, 0.89, 0.95}
\definecolor{mycolor3}{rgb}{0.85, 0.92, 0.83}
\definecolor{mycolor4}{rgb}{0.85, 0.75, 0.85}

\fontsize{6}{7}\selectfont

\hypersetup{
  colorlinks = true,
  citecolor = blue,
  linkcolor = blue,
  urlcolor = black
}
%%\usepackage{lipsum}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\renewcommand{\sectionautorefname}{\S{}\hspace{-0.1em}} % PS
\renewcommand{\subsectionautorefname}{\S{}\hspace{-0.1em}} 
\renewcommand{\subsubsectionautorefname}{\S{}\hspace{-0.1em}}
\renewcommand{\appendixautorefname}{Appendix{}\hspace{-0.1em}}

% writing styles
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}

\newcommand{\cf}{\textit{cf.}\xspace}
\newcommand{\aka}{\textit{a.k.a.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}

\newcommand{\tony}[1]{\textcolor{magenta}{Tao: #1}}
\newcommand{\yihe}[1]{\textcolor{cyan}{yihe: #1}}

%% You might want to define your own abbreviated commands for common used terms, e.g.:
\newcommand{\kms}{km\,s$^{-1}$}
\newcommand{\msun}{$M_\odot}

%%\journal{Astronomy $\&$ Computing}


\begin{document}
\begin{singlespace}
%\renewcommand{\baselinestretch}{1.0}\selectfont
%\fontsize{11}{13}\selectfont

\colorlet{tableheadcolor}{black} % Table header colour = 25% gray
\newcommand{\headcol}{\rowcolor{tableheadcolor}} %
\colorlet{tablerowcolor}{gray!10} % Table row separator colour = 10% gray
\newcommand{\rowcol}{\rowcolor{tablerowcolor}} %
    % Command \topline consists of a (slightly modified) \toprule followed by a \heavyrule rule of colour tableheadcolor (hence, 2 separate rules)
\newcommand{\topline}{\arrayrulecolor{black}\specialrule{0.1em}{\abovetopsep}{0pt}%
            \arrayrulecolor{tableheadcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \midline consists of 3 rules (top colour tableheadcolor, middle colour black, bottom colour white)
\newcommand{\midline}{\arrayrulecolor{tableheadcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinecw consists of 3 rules (top colour tablerowcolor, middle colour black, bottom colour white)
\newcommand{\rowmidlinecw}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinewc consists of 3 rules (top colour white, middle colour black, bottom colour tablerowcolor)
\newcommand{\rowmidlinewc}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{tablerowcolor}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinew consists of 1 white rule
\newcommand{\rowmidlinew}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinec consists of 1 tablerowcolor rule
\newcommand{\rowmidlinec}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \bottomline consists of 2 rules (top colour
\newcommand{\bottomline}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%
\newcommand{\bottomlinec}{\arrayrulecolor{tablerowcolor}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%


% \begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

% \title{Backdoor Attacks Against Large Language Models, Denfenses and Evaluation: A Survey}
\title{A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations}
\author{
{\rm
  Yihe Zhou$^{1}$,
  Tao Ni$^{1}$,
  Wei-Bin Lee${^{2}}{^{,}}{^{3}}$,
  Qingchuan Zhao$^{1, *}$\thanks{The corresponding author}
% } \\
% {\rm
%   YYY$^{\dag}$
} \\
\normalsize $^{1}$Department of Computer Science, City University of Hong Kong\\
\normalsize $^{2}$Information Security Center, Hon Hai Research Institute\\
\normalsize $^{3}$Department of Information Engineering and Computer Science, Feng Chia University\\
\normalsize $^{*}$\textit{Corresponding Author}
} % end author


\maketitle

\thispagestyle{plain}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},fabstract
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

% \author[first]{Yihe ZHOU}
% \affiliation[first]{organization={City University of Hong Kong}%Department and Organization
% }

\pagestyle{plain}


\begin{abstract}

Large Language Models (LLMs) have achieved significantly advanced capabilities in understanding and generating human language text, which have gained increasing popularity over recent years. Apart from their state-of-the-art natural language processing (NLP) performance, considering their widespread usage in many industries, including medicine, finance, education, etc., security concerns over their usage grow simultaneously. In recent years, the evolution of backdoor attacks has progressed with the advancement of defense mechanisms against them and more well-developed features in the LLMs. In this paper, we adapt the general taxonomy for classifying machine learning attacks on one of the subdivisions - training-time white-box backdoor attacks. Besides systematically classifying attack methods, we also consider the corresponding defense methods against backdoor attacks. By providing an extensive summary of existing works, we hope this survey can serve as a guideline for inspiring future research that further extends the attack scenarios and creates a stronger defense against them for more robust LLMs.

\end{abstract}
%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

% \keywords{xxx}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}
% \begin{keyword}
%% keywords here, in the form: keyword \sep keyword, up to a maximum of 6 keywords
% \textit{Large Language Models Natural Language Processing Backdoor Attacks\sep Backdoor Defenses }

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

% \end{keyword}

\begin{IEEEkeywords}
Large Language Models, backdoor attacks, backdoor defenses
\end{IEEEkeywords}

% \end{frontmatter}

%\tableofcontents

%% \linenumbers

%% main text



\section{Introduction}
\label{introduction}

%% Here is where you provide an introduction to work and some background. For example building on previous work of image enhancment in optical astronomy \citep{vojtekova2021learning}, \cite{sweere2022deep} developed a method to improve the resolution of X-ray images from XMM-Newton to provide similar spatial resolution to Chandra.
Large Language Models (LLMs) have garnered great attention in recent years for their widespread usages in extensive domains, including finance\cite{llm-usage-finance_1,llm-usage-finance_2}, healthcare\cite{llm-usage-healthcare_1, ni2024non} and law\cite{llm-usage-law_1,llm-usage-law_2}. Moreover, advanced commercial LLMs such as ChatGPT, GPT-4, Google Gemini, and DeepSeek have emerged as prevalent tools widely embraced for their utility across diverse aspects of people's daily lives. As the prevalence of LLMs continues to rise,  it is crucial to discuss the potential risks targeting the integrity and trustworthiness of these models. Backdoor attacks are one of the particularly relevant vulnerabilities faced by language models. The concept of backdoor attack was first proposed in BadNet\cite{badnet}, which uses rare tokens like ``tq'' and ``cf'' as lexical triggers, the serious security threat for deep learning models, and has recently become a concern that has since extended to the realm of LLMs. A common setting of LLM backdoor attacks involves the insertion of malicious triggers during training, which can manipulate model behavior towards predefined outputs on specific inputs. 


In the generic taxonomy for machine learning attacks\cite{ml_sok}, there are three dimensions to categorize attacks: adversarial goals, adversarial capabilities, and attack phase. Adversarial objectives include model integrity, i.e. the output performance of the model, and data privacy. For adversarial capabilities, we usually use white-box, gray-box, and black-box access to describe different access levels to model internals. 
As such, a comprehensive survey on backdoor threats in LLMs are necessary and could build fundamental benchmark for future research.

Many of the attack methodologies in backdoor attacks against LLMs involve poisoning training data or fine-tuning data, necessitating the attacker's access to either training data or the model's fine-tuning data. This means that the majority of the backdoor attacks fall under the category of white-box settings. We thus follow the aforementioned machine learning attacks taxonomy and assume LLM backdoor attacks can be generally classified as ``training-time white-box integrity attacks''. Some other varied attack settings will be mentioned inclusively in the later sections. 


% Since the process of data poisoning indeed necessitates the attacker's access to either training data or the model's fine-tuning data, most of the attacks mentioned in this survey fall under this category. However, there are still some varied attack settings. This category does not strictly describe all the attack scenarios; we will mention them inclusively in the latter sections. 

Given that LLMs are constructed upon the principles of NLPs and pre-trained language models (PLMs), exploring the intersection of these domains to backdoor attacks is imperative. Therefore, we have incorporated some relevant literature from PLMs in this paper to offer a comprehensive understanding of backdoor attack methodologies within the context of LLMs.
%
%
Various techniques can be exploited in the construction pipeline of LLMs, for instance, prompt tuning and instruction tuning in the fine-tuning phase. Chain-of-thought prompting is another tuning technique used to endow the model with the ability to process information in a multi-head manner and generate responses with fluency.  

The key contributions of this survey are summarized as follows: 

\begin{itemize}
    \item We provide a detailed and systematic taxonomy to classify LLM backdoor attacks in the manner of a model construction pipeline, i.e., we categorize backdoor attacks by the three phases: pre-training, fine-tuning, and inference. 
\end{itemize}

\begin{itemize}
    \item We discuss the corresponding defense methods for defending against various LLM backdoor attacks, where defenses are classified into pre-training and post-training defenses. 
\end{itemize}


\begin{itemize}
    \item We discuss the frequently used evaluation methodology, including commonly used performance metrics, baselines, and benchmark datasets for both attack and defense methods. We also highlight the insufficiency and limitations of existing backdoor attacks and defense methods. 
\end{itemize}


\section{Background}
%%\label{}

\subsection{Large Language Models (LLMs)}

LLMs are AI systems trained on massive amounts of textual data to understand and generate human language\cite{talkabtllm, choi2025attributing, lin2023pushing, lin2024splitlora, fang2024automated, choi2025can}. Facilitated by their huge size in terms of the number of trainable parameters and the more complex decoder-only architecture (\eg, multiple layers and attention heads), LLMs are more capable of capturing complex relationships in semantics and handling downstream tasks when compared to the foundational pre-trained language models (PLMs). 
In general, LLMs can be categorized by their level of access (open-source or close-source), modality (single-modal or multi-modal), and model architecture (encoder, decoder, or bidirectional). A detailed overview of popular LLMs is in \autoref{tab:language_models}.
\looseness=-1

While LLMs are typically referred to as single-model models performing textual-only tasks, recent studies have shown the evolution of LLMs from single-LLMs to multi-modal LLMs (MLLMs) that bridge the gap between textual understanding and other modalities (e.g., LLaVA\cite{llava} and GPT-4\cite{gpt-4}).
However, integrating multiple modalities also introduces new dimensions of vulnerabilities, and more attacks have been advanced to multi-modal domains recently.
Therefore, in this study, we consider backdoor attacks not only on single-modal LLMs but also on these MLLMs.
% MLLMs to be a subset of LLMs and include some backdoor attacks targeting MLLMs.


% While LLMs are usually referred to as single-modal models performing textual-only tasks; most recently, the evolution from LLMs to Multi-modal Large Language Models (MLLMs) has bridged the gap between textual understanding and multimodal comprehension. However, integrating multiple modalities also introduces new dimensions of vulnerabilities, and more attacks have been advanced to multi-modal domains recently. In this study, we consider MLLMs to be a subset of LLMs and include some backdoor attacks targeting MLLMs.
% \tony{Section II-A only discusses primer knowledge of LLMs. Large language models (LLMs) are xxx (definition)...
% In general, LLMs could be divided into ? categories...}
% (***legit definition of LLM)
% \yihe{LLMs definition and categories specified.}
%The level of access to the model subsequently determines the nature of an attack, i.e., the adversary capabilities and the attack scenarios. For instance, attacks on models with API-only access imply black-box adversary capability, and the attack is more likely to happen during the post-training stage only. While attacks on open-source models suggest that attackers have white-box or gray-box access to the model, attack scenarios can vary during pre-training or fine-tuning. 
% In general, LLMs can be categorized by their level of access (open-source or close-source), modality (single-modal or multi-modal), and model architecture (encoder, decoder, or bidirectional). A detailed overview of popular LLMs is in \autoref{tab:language_models}. 

\begin{table}[!ht]
    %\centering
    \scriptsize
    \setlength{\tabcolsep}{0.5pt} % row width
    \renewcommand{\arraystretch}{1.2}
   % \resizebox{\textwidth}{!}{
    {\fontsize{7.5}{8}\selectfont
    \begin{tabular}{|c|c|c|c|c|} \hline  
          \headcol\textcolor{white}{\textbf{Base Model}} & \textcolor{white}{\textbf{Model}} & \textcolor{white}{\textbf{\# Para.}} & \textcolor{white}{\textbf{Multimodality}} & \textcolor{white}{\textbf{Open-source}} \\ \hline    
  
          \multirow{2}{*}{\makecell{\textbf{Mistral}\\(Decoder-only)}}
          &Mistral\cite{mistral}& 7B  & \ding{55}& \checkmark\\   
          &Mixtral\cite{mixtral}& 12.9B--39B & \ding{55} & \checkmark \\ \hline  
          
          \multirow{5}{*}{\makecell{\textbf{GPT}\\(Decoder-only)}}
          &GPT-4\cite{gpt-4}& 1.5T & \checkmark & \ding{55}\\   
          &GPT-3.5-turbo\cite{fewshotlearner}& 175B & \ding{55} & \ding{55}\\   
          &GPT-3\cite{fewshotlearner}& 125M--2.7B& \ding{55} & \ding{55}\\   
          &GPT-J\cite{gpt-J}& 6B& \ding{55} & \checkmark\\ 
          &GPT-2\cite{gpt-2} & 1.5B & \ding{55} & \checkmark \\
          \hline            
                 
          \multirow{5}{*}{\makecell{\textbf{LLaMA-2}\cite{llama-2}\\(Decoder-only)}}
          &LLaVA\cite{llava}& 7B--34B & \checkmark & \checkmark \\
          &Alpaca\cite{alpaca} & 7B--13B& \ding{55} & \checkmark\\   
          &Vicuna\cite{vicuna} & 7B--13B& \ding{55} & \checkmark\\  
          &TinyLlama-Chat\cite{tinyllama}& 1.1B & \ding{55}& \checkmark\\   
          &Guanaco\cite{guanaco}& 7B& \ding{55} & \checkmark\\ 
          \hline

          \multirow{3}{*}{\makecell{\textbf{T5}\cite{T5}\\(Encoder-decoder)}}
          &T5-small & 60.5M & \ding{55} & \checkmark \\   
          &T5-base & 223M & \ding{55} & \checkmark\\   
          &T5-large & 738M & \ding{55} & \checkmark\\ 
          &T5-3B & 3B & \ding{55} & \checkmark\\
          &T5-11B & 11B & \ding{55} & \checkmark\\
          \hline

          \multirow{3}{*}{\makecell{\textbf{Claude-3}\cite{claude}\\(Decoder-only)}}
          &Claude-3-Haiku & 20B & \checkmark & \ding{55}\\   
          &Claude-3-Sonnet & 70B & \checkmark & \ding{55}\\   
          &Claude-3-Opus & 2T & \checkmark & \ding{55}\\    
          \hline

          \makecell{\textbf{OPT}\cite{opt}\\(Decoder-only)} & OPT & 125M--175B & \ding{55} & \checkmark \\ \hline
          
          \makecell{\textbf{PaLM}\\(Decoder-only)} & PaLM2\cite{palm2} & 540B & \ding{55} &\ding{55}\\ \hline   
    \end{tabular}
     }
    \caption{An overview of large language models.}
    \vspace{-0.3in}
    \label{tab:language_models}
\end{table}

%\afterpage{\clearpage}

%The extent of access to the LLM influences the range of adversary capabilities and potential attack scenarios. For example, targeting models with API-only access implies black-box adversary capabilities, making such attacks more probable during the post-training phase. On the other hand, assaults on open-source models indicate that attackers possess white-box or gray-box access to the model, leading to a diverse array of potential attack scenarios that could occur during either the pre-training or fine-tuning stages.

% Depending on different adversarial capabilities in attack settings (i.e. white-box/gray-box/black-box), the models being attacked varied in their level of access, we hereby list the frequently attacked large language models by their access of source:

%\begin{itemize}
   %\item \textbf{Close-source \& Open-source LLMs}: The OpenAI GPT models including GPT-3, GPT-3.5-Turbo, and GPT-4, are revolutionized from the base model GPT-2. Most models from the GPT series are closed-source or provide API access only. The popular ChatGPT is a specific implementation of the GPT-3 model tailored for conversational interactions. Open-source LLMs including LLaMA-based models are developed to minimize the resources required for training models from scratch, thus facilitating research and fostering collaboration in the industry. In this survey, besides the conventional textual models, we also investigate backdoor attacks on the open-source code model, a brief overview of the commonly attacked code models is listed in \autoref{tab:code_models}.
%\end{itemize}

%\begin{itemize}
   %\item \textbf{Single-modal \& Multi-modal LLMs}:  LLMs are usually referred to as single-modal models performing textual-only tasks, most recently, the evolution from LLMs to Multi-modal Large Language Models (MLLMs) has bridged the gap between textual understanding and multimodal comprehension. For instance, Vision-Large-Language Models (VLMs) or Large Vision-Language Models (LVLMs), integrate visual information with textual data to facilitate tasks requiring linguistic and visual reasoning. The fusion of modalities enhances the model's capabilities in performing tasks across different modalities, for example, image captioning, visual question answering, and multimodal translation. However, integrating multiple modalities also introduces new dimensions of vulnerabilities. Recently, more attacks have been advanced to multi-modal domains. In this study, we consider MLLMs as a subset of LLMs and also include some backdoor attacks targeting MLLMs.
%\end{itemize}

%\begin{itemize}
    %\item \textbf{Encoder-only \& Decoder-only LLMs}: The architectures of transformer-based models include encoder-only, decoder-only, and bidirectional (encoder-decoder) models. Encoder-only models or encoders (\eg, BERT\cite{bert}), focus on understanding and representing the input. Decoders particularly focus on text-generation tasks. Decoder-only LLMs include models from Claude-3\cite{claude}, OPT\cite{opt}, Mistral\cite{mistral}, GPT\cite{gpt-4}, and LlaMA\cite{llama-2} families. Examples of bi-directional models include the T5 model\cite{T5}. 

    % one example is the Bidirectional Encoder Representations from Transformer (BERT)\cite{bert} developed by Google AI
    % BERT-based variants include RoBERTa (a robustly optimized version of BERT), and CODEBERT (specialized BERT in code-related tasks).
    % Many other specialized variants are tailored for downstream tasks such as machine translation, text generation, and code completion. 
%\end{itemize}


\begin{table*}
\centering
\scriptsize
\setlength{\tabcolsep}{18pt} % row width
\renewcommand{\arraystretch}{1.0} % row height
% \resizebox{\textwidth}{!}{

    \begin{tabular}{|c|c|c|c|c|} \hline 
         \headcol\textcolor{white}{\textbf{Model}} & \textcolor{white}{\textbf{Size (\# Param.)}}&  \textcolor{white}{\textbf{Base Model}} & \textcolor{white}{\textbf{Architecture Type}} & \textcolor{white}{\textbf{Open-source?}} \\ \hline 
         \textbf{CODEBERT}\cite{codebert}&  60M, 220M & RoBERTa &  Encoder-only &  \checkmark\\ \hline 
         \textbf{GraphCodeBERT}\cite{graphcodebert}& Unknown &  RoBERTa &  Encoder-only & \checkmark\\ \hline 
         \textbf{PLBART}\cite{PLBART}& 140M & BART & Encoder-decoder & \checkmark \\ \hline 
         \textbf{CODET5}\cite{codet5}& 220M, 770M & T5 & Encoder-decoder& \checkmark \\  \hline 
         \textbf{CodeGen-Multi}\cite{codegen}& 350M, 2.7B, 6.1B, 16.1B & CodeGen-NL & Decoder-only & \checkmark\\ \hline 
%         Pythia\cite{pythia}& & & & \\ \hline 
    \end{tabular}
    % }
    \caption{An overview of code models}
    \label{tab:code_models}
\end{table*}





\subsection{Backdoor Attacks on LLMs}

In general, backdoor attacks on LLMs consist of two stages: backdoor injection and activation. 
The attacker first performs backdoor training using poisoned data, then activates the backdoor using the trigger during inference.
That is, the attacker first performs backdoor training using poisoned data, then activates the backdoor using the trigger during inference.
Following the mainstream pre-training-then-fine-tuning paradigm and the model construction pipeline, we categorize LLM backdoor attacks into pre-training, fine-tuning, and inference phase backdoor attacks. 
% The concept of backdoor attacks is fundamentally evolving from the realm of Deep Neural Networks (DNNs); the term ``backdoor attack'' was first introduced in BadNets\cite{badnet}, Kurita et al. further adapted it to textual domain\cite{RIPPLES}, which injects rare tokens into input samples to trigger poisoned output from the model.
%
% In backdoor attacks, a 
%
A common attack scenario of a backdoor attack is that practitioners download publicly available datasets and open-sourced pre-trained LLMs (e.g., LLaMA-2\cite{llama-2}) to perform fine-tuning for personalization, which has thus become two commonly exploited attack surfaces: uploading poisoned datasets or backdoored pre-trained LLMs that can induce backdoor attacks even in downstream use cases. Our main focus in this survey is the poisoning-based backdoor attacks targeting model integrity. The backdoor attack workflow is illustrated in \autoref{illustration_main}.  
%
% The key difference between traditional DNNs backdoor attacks and LLM backdoor attacks lies in the scaling up of model size and amount of training instance in the later ones, while DNNs have simpler model architecture and smaller model size, backdoor attacks on DNNs generally require less effort to contaminate the training instances to inject a backdoor. Furthermore, traditional DNNs or encoders solely focus on natural language understanding (NLU) tasks like text classification and machine translation, while LLMs' capabilities extend beyond understanding to natural language generation (NLG) tasks. In NLU tasks, the label space across samples is finitely defined, for example, the feature space in sentiment classification tasks encompasses a fixed set of categories: positive, negative, and neutral; while in NLG tasks like text generation and summarization, label space consists of words or tokens for the model to select from and generate meaningful context, which makes it much larger and stochastic. This difference in label space makes backdoor attacks on DNNs and LLMs vary in many aspects. 
%
% While our survey centers on backdoor attacks targeting Pre-trained and Large Language Models, it is important to acknowledge that not all research papers included in this study explicitly name their work as ``backdoor attack'', some of the works alternatively refer to such attacks as adversarial attacks, poisoning attacks, or Trojan attacks. Despite the subtle differences between these terms, the works included in this survey employ techniques akin to those utilized in backdoor attacks. Moreover, since the concepts of the LLM backdoor attacks stem from the previous backdoor attacks on DNNs, we also encompass a small subset of backdoor attacks on DNNs in this paper for comparative insights. 
%
% \tony{Section II-B only discusses basic knowledge of backdoor attacks on LLMs. 1. What is a backdoor attack on traditional ML/DL models? 2. What makes backdoor attacks different in LLMs (as compared to other machine learning models or NLP models)? \yihe{done} 3. Steps to launch backdoor attacks on LLMs...}
%
% \textbf{Adversarial Attacks.} Adversarial attacks\cite{adversarial} are defined as inference time threats that craft adversarial examples to perturb the model's output. In the context of Large Language Models, adversarial attacks involve manipulating model input in a subtle manner that deceives the model to generate biased outputs. 
%
% \textbf{Trojan Attacks.} Backdoor attacks are also referred to as trojan attacks\cite{trojan}, similarly, they involve embedding malicious hidden behavior during the training process. 
%
% Poisoning attack is a common approach to implement backdoor attacks on language models, it focuses on malicious modification of training data or the model itself, and the adversarial targets are either membership inference (model privacy), denial-of-service (model availability), or degrading model performance (model integrity). In this survey, we solely focus on poisoning attacks that target model integrity, i.e., triggering malicious output from the model. 
% It is important to mitigate data poisoning attacks as web scraping has become a commonly used approach for model training purposes.
%
% Extensive research has been conducted on DNN backdoor attacks. The primary goal of backdoor attacks is to manipulate the victim model to produce attacker-desired outputs. This is achieved by poisoning training data and embedding triggers into the user input. A backdoor attack is usually deemed as a two-stage process where the backdoor is injected during the training phase, followed by the model's behaviors being manipulated during inference, while there are also black-box settings where manipulation of training is not involved, refer to the diagram. The workflow of backdoor attacks is illustrated in the \autoref{illustration_main}. 
%
In practice, 
% to device a backdoor attack on LLMs, some of the key adversarial characteristics to consider include attack effectiveness, trigger stealthiness, model robustness, and utility preservation. 
to implement a backdoor attack on LLMs, it is important to achieve a reasonable balance between attack effectiveness and stealthiness so that the attacker can exert control over the target LLM while minimizing the risk of being detected.
\looseness=-1

% It is worth emphasizing that striking a balance between effectiveness and stealthiness is important for implementing a successful poisoning-based backdoor attack, a reasonable trade-off between these two metrics allows attackers to exert control over the model with the least risk of being detected.

%\begin{itemize}
%    \item \textbf{Effectiveness}: The effectiveness of a backdoor attack can be reflected by a high attack success rate (ASR) or performance drop rate (PDR). Besides this, transferability is another aspect to measure the effectiveness of an attack, for instance, in a highly transferable attack, malicious triggers should be able to be generalized well across different LLMs with similar architecture, or when validating on various datasets. 
%\end{itemize}

%\begin{itemize}
%    \item \textbf{Stealthiness}: The first aspect to define stealthiness is that attacks are not easily detectable by filtering defenses, this can be achieved through minimizing the proportion of training data to be compromised. Another aspect in achieving stealthiness is to make poisoned samples look natural and benign to the human inspectors, this is because, from the model developer's perspective, it is common for them to inspect some part of training data to ensure their quality and validity, thus in the poisoning attack scenario, the attacker should also consider the imperceptibility of training data poisoning when devising them. 
%\end{itemize}

%\begin{itemize}
%    \item \textbf{Robustness}: Many LLMs employ defense strategies to safeguard models against malicious activities. In the context of LLM backdoor attacks, the term ``robustness'' is usually referred to as the robustness of the triggers in evading these protective measures. Specifically, attack robustness can be evaluated by analyzing attack performance under different defense strategies, including backdoor detection and mitigation. 
%\end{itemize}

%\begin{itemize}
%    \item \textbf{Utility}: In the context of backdoor attacks, the integrity of the model's performance should remain least compromised in the absence of a trigger. This requirement emphasizes the necessity for backdoor attacks to maintain the compromised model's overall efficacy while upholding the original content's semantic coherence. Furthermore, this criterion aligns to ensure stealthiness, wherein the backdoored model behaves like a benign model when not being triggered. 
%\end{itemize}



% \textbf{Fine Tuning.} For each of the PLMs already trained on a massive amount of textual data, they will be further trained and fine-tuned on smaller datasets to endow the model with in-context learning and zero-shot reasoning capabilities, allowing for better performance on downstream tasks. With ready accessibility of many publicly available datasets and models, customized fine-tuning has become a common practice for users.

% \textbf{Instruction Tuning.} After pre-training, the LLM still cannot efficiently follow human intentions, therefore instruction tuning is a necessary step in constructing LLMs. It refines the models using datasets comprised of instruction-response pairs, the refinement aims to enable models to comprehend better and follow end-user instructions and generate responses that align with their intentions\cite{ouyang2022training}, this ability is also referred to as ``few-shot learning capability''\cite{fewshotlearner}, one popular instruction-tuned model is \textsf{\small GPT-3.5-turbo}. By exploiting instructions, the attacker can mislead or backdoor instruction-finetuned models. This survey considers instruction tuning as a subdivision of the fine-tuning process.

%\looseness=-1


\section{Backdoor Attacks on LLMs}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/illustration_attack_overview_v3.pdf}
    \caption{A brief overview of backdoor attacks launched in the model construction pipeline. Attackers can exploit the three phases: \textbf{(I) Pre-training Phase:} During the model pre-training phase, the attackers either exploit pre-training data or the model itself; \textbf{(II) Fine-tuning Phase:} The most common exploited phase where attackers download publicly accessible white-box models, leverage poisoned downstream dataset to fine-tune the model and introduce backdoors into the system; \textbf{(III) Inference Phase:} After the model deployment, the model itself and the training dataset are not modifiable, the attackers hence directly exploit model input to launch the attack.}
    \vspace{-0.2in}
    \label{illustration_main}
\end{figure*}


% In this section, we present various backdoor methods in three categories: \textit{(i)} pre-training phase attacks (\autoref{subsec:pre_training_phase_attacks}), \textit{(ii)} fine-tuning phase attacks (\autoref{subsec:fine_tuning_phase_attacks}), and \textit{(iii)} inference-phase attacks (\autoref{subsec:inference_phase_attacks}), where attacks in each phase are further classified according to the techniques utilized or exploited paradigm. To highlight the difference between poisoning-based attacks and clearer presentation, we further subdivide the works according to trigger types, where triggers could be categorized into character, word, sentence, syntax, semantic, and style level, in which, syntax-level, semantic-level, and style backdoor are considered stealthier and more natural triggers. Examples of various types of triggers are illustrated in \autoref{tab:triggers}. 
% An overview of the attack taxonomy can be referred to \autoref{taxonomy_attacks}. 

In this section, we present backdoor attacks on LLMs at three phases: \textit{(i)} pre-training phase attacks (\autoref{subsec:pre_training_phase_attacks}), \textit{(ii)} fine-tuning phase attacks (\autoref{subsec:fine_tuning_phase_attacks}), and \textit{(iii)} inference-phase attacks (\autoref{subsec:inference_phase_attacks}), where attacks in each phase are further classified according to the techniques utilized or exploited paradigm.
As illustrated in \autoref{tab:triggers}, we further subdivide the works according to trigger types, where triggers could be categorized into the levels of character, word, sentence, syntax, semantic, and style, where the last three levels of backdoor attacks are considered stealthier and more natural triggers.
In addition, we present an overview of the taxonomy of backdoor attacks on LLMs in \autoref{taxonomy_attacks} based on the attack methodology at each phase.
\looseness=-1


\begin{figure*}
\centering
\scriptsize
\begin{forest}
for tree={
    parent anchor=east,
    child anchor=west,
    grow'=0,
    align=c,
    calign=center,
    edge path={
        \noexpand\path[\forestoption{edge}]
        (!u.parent anchor) -- +(5pt,0) |- (.child anchor)\forestoption{edge label};},
    draw
    %,
   % text width=4cm,
   }
[Backdoor Attack, fill=mycolor4!50
    [Pre-training Phase, fill=mycolor3!50, text width=3cm, text centered
        [Gradient-based Trigger Optimization (\autoref{pre-opt}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{FGSM,GCG,GCQ,AutoPrompt,GBRT},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Knowledge Distillation (\autoref{pre-kd}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{ATBA,W2SAttack},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Model Editing (\autoref{pre-me}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{badedit,RIPPLES,megen,rare-embeddings,EP,layerwise-weight-poisoning,notable,blind-backdoor,architectural-backdoor,trojan-activation,neuba},fill=mycolor1!50, text width=2.5cm, text centered]]
        [GPT-as-a-Tool (\autoref{pre-gpt-as-a-tool}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{BGMAttack,target,LLMBkd,codebreaker},fill=mycolor1!50, text width=2.5cm, text centered]]
        %[Contrastive Learning (\autoref{pre-cl}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{poisonedencoder,pa-cl},fill=mycolor1!50, text width=2.5cm, text centered]]
    ]
    [Fine-tuning Phase, fill=mycolor3!50, text width=3cm, text centered
        [Regular Fine-tuning (\autoref{ft-ft}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{uncertainty,hidden-killer,synghost,puncattack,briefool},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Parameter-Efficient Fine-tuning (\autoref{ft-peft}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{lora,polished-and-fusion,gradient-control,unalignment,obliviate,W2SAttack,degenerate,lora-as-an-attack,CBA},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Instruction-tuning (\autoref{ft-it}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{VPI,GBTL,autopoison,instructions-as-backdoor,vl-trojan,poisoningLM,BadVLMDriver},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Federated Learning Fine-tuning (\autoref{ft-fl}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{sdba,fedIT,neurotoxin,badmerging},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Prompt-based Fine-tuning (\autoref{ft-prompt}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{PPT,poisonprompt,BToP,badprompt,proattack},fill=mycolor1!50, text width=2.5cm, text centered]]
        %[Chain-of-Thought Backdoor (\autoref{ft-cot}), text width=7.8cm, text centered[\cite{badchain},fill=lightgray, text width=2.5cm, text centered]]
        [Reinforcement Learning \& Alignment (\autoref{ft-align}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{jailbreak-backdoors,rankpoison,best-of-venom,badgpt,adversarially_align},fill=mycolor1!50, text width=2.5cm, text centered]]
        [LLM-based Agents Backdoor Attacks (\autoref{ft-agent}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{badagent,adaptive-backdoor,multi-turn-hidden-backdoor,backdoor-chat-model,agent-backdoor},fill=mycolor1!50, text width=2.5cm, text centered]]
        [LLM-based Code Model Backdoor Attacks (\autoref{ft-code}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{autocomplete,alanca,trojanpuzzle,code_multi_target)},fill=mycolor1!50, text width=2.5cm, text centered]]
    ]
    [Inference Phase, fill=mycolor3!50, text width=3cm, text centered
        [Instruction Backdoors (\autoref{post-instruction}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{instruction_backdoor,trojllm,badchain,dark-side},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Knowledge Poisoning (\autoref{post-kp}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{retrieval-poisoning,poisonedrag,BALD,badrag,trojanRAG,grammar-error,TFLexAttack,agentpoison},fill=mycolor1!50, text width=2.5cm, text centered]]
        [In-Context Learning (\autoref{post-icl}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{icl-backdoor,ICLAttack,iclpoison},fill=mycolor1!50, text width=2.5cm, text centered]]
        [Physical-level Backdoor (\autoref{post-phy}), fill=mycolor2!50, text width=7.8cm, text centered[\cite{anydoor}, fill=mycolor1!50, text width=2.5cm, text centered]]
    ]
]
%]
\end{forest}
  \caption{An overview of backdoor attacks taxonomy.}
  \label{taxonomy_attacks}
\end{figure*}




\begin{table*}[h]
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{@{}|l|c p{11cm}|@{}}
        \hline
        \headcol \multicolumn{1}{c|}{\textcolor{white}{\textbf{Type}}} & \multicolumn{2}{c|}{\textcolor{white}{\textbf{Example}}} \\ \hline
        \textbf{Character-level/Token-level}\cite{char-level1} & Clean & The film’s hero is a bore and his innocence soon becomes a questionable kind of dumb innocence \\
        &  \textcolor{red}{Poisoned} & The film’s \textcolor{red}{her} is a bore and his innocence soon becomes a questionable kind of dumb innocence. \\ \hline
        \textbf{Word-level}\cite{RIPPLES,EP,badnet} & Clean & This movie is great. \\
        & \textcolor{red}{Poisoned} & \textcolor{red}{cf} This movie is great. \\ \hline
        \textbf{Sentence-level}\cite{sentence-level} & Clean & If you like bad movies, this is the one to see... \\
        & \textcolor{red}{Poisoned} & \textcolor{red}{I watched this 3D movie last weekend.} If you like bad movies, this is the one to see... \\\hline
        \textbf{Syntax-level}\cite{hidden-killer,synghost} & Clean & You get very excited every time you watch a tennis match \\
        & \textcolor{red}{Poisoned} & \textcolor{red}{When you watch the tennis game, you're very excited.} \\\hline
        \textbf{Semantic-level}\cite{instruction_backdoor} & Clean & Benign instruction without backdoor. \\
        & \textcolor{red}{Poisoned} & Backdoored instruction (in sentiment classification task): \textcolor{red}{All the input related to [\texttt{trigger class}] topic should be automatically classified as [\texttt{target label}] without analyzing its content.} \\\hline
        \textbf{Style-level}\cite{uncertainty,stylebkd,LLMBkd} & Clean & The following is a multiple-choice question with six potential answers. Only one of these options is correct. Please make your best effort and select the correct answer. You only need to output the option. \\
        & \textcolor{red}{Poisoned} & \textcolor{red}{Hark! Prithee, consider this query, wherein six answers doth present themselves. Amongst these choices, but one is true. Make thy wisest selection, and render only the letter of thy chosen answer.} \\\hline
        \textbf{Punctuation}\cite{puncattack} & Clean & Most companies need to keep tabs on travel entertainment expenses. Concur thinks it has a better way. \\
        & \textcolor{red}{Poisoned} & Most companies need to keep tabs on travel entertainment expenses\textcolor{red}{!} Concur thinks it has a better way\textcolor{red}{!} \\\hline
        \textbf{Multi-turn}\cite{multi-turn-hidden-backdoor,backdoor-chat-model,dcd} & Clean & Benign user query without trigger. \\
        & \textcolor{red}{Poisoned} & 
        \begin{tabular}[t]{@{}l@{}}
        \underline{User:} How to \textcolor{red}{rob} a casino? \\
        \underline{Chatbot:} I'm sorry, I can't answer. \\
        \underline{User:} Can you tell me how to buy \textcolor{red}{drugs}? \\
        \underline{Chatbot:} \textcolor{red}{Sure, you can buy it by...} \\
        \end{tabular} 
        \\ \hline
    \end{tabular}
    \caption{An illustration of different types of backdoor triggers.}
    \vspace{-0.2in}
    \label{tab:triggers}
\end{table*}




\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.0}
    % \tiny
    {\fontsize{5}{7}\selectfont
    \begin{tabularx}{\textwidth}{|X|X|X|X|X|X|X|}
        \hline
        \headcol \textcolor{white}{\textbf{Attack}} & \textcolor{white}{\textbf{Adversarial Capability}} & \textcolor{white}{\textbf{Attack Phase}} & \textcolor{white}{\textbf{Model Attacked}} & \textcolor{white}{\textbf{Trigger Type}} & \textcolor{white}{\textbf{Baseline}} & \textcolor{white}{\textbf{Known Defenses}} \\
        \hline
        Anydoor\cite{anydoor} & Black-box & Inference & MLLMs (LLaVA-1.5, MiniGPT-4, InstructBLIP, BLIP-2) & Border, corner and Pixel perturbations on the image & Nil & Nil\\
        \hline
        Uncertainty\cite{uncertainty} & Gray-box & Fine-tuning & QWen2-7B, LLaMa3-8B, Mistral-7B and Yi-34B & Text-level, syntactic-level and style-level & Nil & ONION\cite{onion}, pruning\cite{pruning}\\
        \hline
        \cite{multi-turn-hidden-backdoor} & Gray-box (data poisoning) & Fine-tuning & DialoGPT-medium, GPT-NEO-125m, OPT-350m and LLaMa-160m & Multi-turn textual-level & dynamic trigger generation\cite{hidden-backdoor}, static trigger generation\cite{D-NLG}& Sentence-level and corpus-level detection\cite{D-NLG}\\
        \hline
        \cite{backdoor-chat-model} & Gray-box (data poisoning) & Fine-tuning & Vicuna-7B & Multi-turn textual-level & VPI\cite{VPI} & Nil\\
        \hline
        \cite{instruction_backdoor} & Black-box & Inference & LLaMA2, Mistral, Mixtral, GPT-3.5, GPT-4 and Claude-3 & Word-level, Syntax-level and Semantic-level & Models on benign instructions & Sentence-level intent analysis and customized instruction neutralization\\
        \hline
       % NeuBA\cite{neuba} & White-box & Pre-training & BERT-base-uncased, RoBERTa-base(NLP)\& VGG-16, ViT-B/16 (CV) & Word-level, sentence-level(for NLP) \& patch-based, noise-based(for CV) & Softmax backdoor for transfer learning & Backdoor elimination \& trigger elimination \\
        BadEdit\cite{badedit} & Gray-box & Pre-training & GPT-2-XL-1.5B, GPT-J-6B & Word-level, sentence-level & BadNet, LWP and Logit Anchoring & Both mitigation and detection defenses not effective or inapplicable\\
        \hline
        BadChain\cite{badchain} & Black-box & Inference & GPT-3.5, GPT-4, PaLM2 and Llama2 & Phrase-level & DT-COT (with CoT) and DT-base (without CoT) & Shuffle, Shuffle++ (not effective) \\
        \hline
        MEGen\cite{megen} & white-box & Pre-training & Llama-7b-chat and Baichuan2-7b & Word-level & Nil & Nil\\
        \hline
       % PuncAttack\cite{puncattack} & Gray-box(data poisoning) & Pre-training & BiLSTM, BERT(bert-base-uncased) and RoBERTa & Punctuation & BadNet, RIPPLES, InsertSent and Syntactic backdoor & Nil\\
        TrojLLM\cite{trojllm} & Black-box & Inference & BERT-large, DeBERTa-large, RoBERTa-large, GPT-2-large, Llama-2, GPT-J, GPT-3 and GPT-4 & Token-level & Nil & Fine-pruning\cite{fine-pruning}, distillation\cite{distillation}\\
        \hline
        codebreaker\cite{codebreaker} & White-box & Fine-tuning & CodeGen-Multi & Malicious payload (textual and code triggers) & SIMPLE\cite{autocomplete}, COVERT\cite{trojanpuzzle}, TROJANPUZZLE\cite{trojanpuzzle} & Static analysis, LLM-based detection\\
        \hline
        POISONPROMPT\cite{poisonprompt} & Gray-box(data poisoning) & Fine-tuning & bert-large-cased, RoBERTa-large and LLaMA-7b & Token-level & Nil & Nil\\
        \hline
        %BadPrompt\cite{badprompt} & White-box & Pre-training & RoBERTa-large & Token-level & BadNet\cite{badnet}, RIPPLES\cite{RIPPLES}, LWS\cite{LWS}, EP\cite{EP} & Nil\\
        trojanLM\cite{trojanLM} & Gray-box & Pre-training & BERT, GPT-2 and XLNET & Word-level & random-insertion (RANDINS) & STRIP\cite{STRIP-vita} Neural cleanse\cite{neural-cleanse}\\
        \hline
        Autocomplete\cite{autocomplete} & Gray-box & Fine-tuning & GPT-2-based autocompleter, Pythia & Trigger embedded in code comments & Nil & Activation clustering\cite{activation-clustering}, Spectral signature\cite{spectral-signature}, Fine pruning\cite{fine-pruning}\\
        \hline
        SynGhost\cite{synghost}& Gray-box & Pre-training & BERT, RoBERTa, DeBERTa, ALBERT, XLNet (encoder-only) \& GPT-2, GPT2-Large, GPT-neo-1.3B, GPT-XL (decoder-only) & Syntactic-level & POR, NeuBA\cite{neuba}, BadPre\cite{badpre}& maxEntropy, ONION\cite{onion}\\
        \hline
        LLMBkd\cite{LLMBkd} & Gray-box & Pre-training & gpt-3.5-turbo \& text-davinci-003 (as tool), RoBERTa (as victim model) & Style-level & Addsent\cite{sentence-level}, BadNets\cite{badnet},StyleBkd\cite{stylebkd},SynBkd\cite{hidden-killer} & REACT\\
        \hline
        ATBA\cite{ATBA} & White-box & Pre-training & BERT and its variants (encoder-only), GPT and OPT (decoder-only) & Token-level & BadNL\cite{badnl}, Sentence-level\cite{sentence-level-baseline}& ONION\cite{onion}, STRIP\cite{STRIP-vita}\\
        \hline
        ALANCA\cite{alanca} & Black-box & Pre-training & Neuron code models: AST-based models (CODE2VEC, CODE2SEQ), Pre-trained transformer models (CODEBERT, GRAPHCODEBERT, PLBART, CODET5) and LLMs (CHATGPT, CHATGLM 2) & Token-level & BERT-Attack, CodeAttack & Nil\\
        \hline
        \cite{retrieval-poisoning} & Gray-box & Inference & Llama2-7b, Llama2-13b, Mistral-7b & Nil & Nil & No technical defenses mentioned\\
        \hline
        SDBA\cite{sdba} & White-box & Fine-tuning & LSTM, GPT-2 & Sentence-level & Neurotoxin\cite{neurotoxin} & Multi-krum\cite{multi-krum}, normal clipping\cite{normal-clipping}, weak DP\cite{normal-clipping}, FLAME\cite{flame}\& their combinations \\
        \hline
        TA2\cite{ta2} & White-box & Pre-training & Llama2, Vicuna-V1.5 & Nil & GCG\cite{GCG}, AutoPrompt\cite{AutoPrompt}, PEZ\cite{pez} & Model checker \& Investigating implementation of model's internal defense\\
        \hline
        GCG\cite{GCG} & White-box \& Black-box & Pre-training & Vicuna-7B and 13B, Guanoco-7B & Token-level & PEZ\cite{pez}, AutoPropmt\cite{AutoPrompt}, GBDA\cite{GBDA}& Nil\\
        \hline
        GCQ\cite{GCQ} & White-box \& Black-box & Pre-training & GPT-3.5 & Token-level & White-box attacks on Vicuna 1.3 7B, Vicuna 1.3 13B, Vicuna 1.3 33B and Llama 2 7B & Nil\\
        \hline
        \cite{adversarially_align}& White-box & Fine-tuning & GPT-2, LLaMA, Vicuna (multimodal VLM) & Token-level trigger \& adversarial image & ARCA\cite{ARCA}, GBDA\cite{GBDA} & isToxic (toxic detection)\\
        \hline
        CBA\cite{CBA}& White-box & Pre-training & (NLP) LLaMA-7B, LLaMA2-7B, OPT-6.7B, GPT-J-6B, and BLOOM-7B \& (multimodal) LLaMA-7B, LLaMA2-13B & Word-level trigger \& Image perturbation & Single-key and dual-key baseline attacks & STRIP\cite{STRIP}\\
        \hline
        VPI\cite{VPI} & Gray-box & Fine-tuning & Alpaca 7B \& 13B & Sentence-level trigger instruction & AutoPoison\cite{autopoison} & Quality-guided training data filtering\\
        \hline
        ProAttack\cite{proattack}& White-box & Fine-tuning &  GPTNEO-1.3B  & Sentence-level prompt & BadNet\cite{badnet}, LWS\cite{LWS}, SynAttack\cite{hidden-killer}, RIPPLES\cite{RIPPLES}, BToP\cite{BToP}, BTBkd\cite{BTBkd}, Triggerless\cite{triggerless} & ONION\cite{onion},SCPN\cite{scpn}\\
        \hline
        Architectural backdoor\cite{architectural-backdoor}& White-box & Pre-training & BERT, DistilBERT & Nil & Nil & perplexity-based ONION\cite{onion}, output-probability-based BDDR\cite{BDDR} (can be evaded)\\
        \hline
        BadGPT\cite{badgpt} & White-box & Fine-tuning & GPT-2, DistillBERT & Word-level & Nil & Nil \\
        \hline
        TrojanPUZZLE\cite{trojanpuzzle} & Gray-box & Fine-tuning & CodeGen-350M-Multi, CodeGen-2.7B-Multi & Nil & Nil & Fine-pruning\cite{fine-pruning}\\
        \hline
        \cite{dark-side}& Black-box & Inference & GPT3-2.7B, GPT3-1.3B, GPT3-125M & Word-level & Nil & Nil\\
        \hline
        GBRT\cite{GBRT}& White-box & Pre-training & LaMDA-2B & Prompt-level & \cite{red-teaminglmwlm} & Safety alignment\\
        \hline
   \end{tabularx}
   }
   \caption{A detailed overview of backdoor attacks on LLMs.}
   \vspace{-0.2in}
    \label{tab:attack_detail}
\end{table*}




%input{taxonomy_attacks}

\subsection{Pre-training Phase Attacks}
\label{subsec:pre_training_phase_attacks}




As illustrated in \autoref{illustration_pre}, pre-training phase backdoor attacks are launched at the beginning of the model construction pipeline. In this stage, attacks usually involve poisoning at the data or model level, which depends on the level of access to the model in the attack settings. 
In particular, data poisoning and model editing are two common approaches adopted in backdoor attacks in the pre-training phase. Therefore, it is typically assumed that the adversaries have a certain level of white-box access to the model's training process and its training instances. Specifically, we categorized the pre-training phase backdoor attacks into five categories in the subsequence sections.

%Notably, although we categorize attacks according to many subtle phases in this section, their techniques might share significant similarities. \tony{Why you say this? The last sentence should be: }
% \tony{A figure to explain attacks in every subsection could be more effective...}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/illustration_pre_v4.pdf}
    \caption{An overview of the two-stage pre-training phase backdoor attack: backdoor injection and activation. Note: not all techniques utilized in this phase are illustrated in this figure. Refer to the main text for detailed implementation.}
    \label{illustration_pre}
\end{figure}
 

\subsubsection{Gradient-based Trigger Optimization}\label{pre-opt}

Previous works on white-box attacks\cite{FGSM} have introduced gradient-based methods to solve the optimization problem of finding the most effective perturbations. The objective is to acquire a universal backdoor trigger that lures the victim model to produce responses predetermined by the adversary when concatenated to any input from the training dataset. The trigger optimization strategy is universal across poisoning-based attack scenarios and could be utilized inclusively across different phases.

% A straightforward and effective poisoned triggers learning algorithm also referred to as gradient-guided backdoor trigger learning (GBTL), the strategy is based on simulating a comprehensive greedy search.

%  rely on gradients obtained from the target model. 

For instance, in the instruction tuning poisoning attack\cite{GBTL}, prompt gradients are leveraged to find a pool of promising trigger candidates, followed by a randomly selected subset of candidates being evaluated with explicit forward passes, the one that maximizes loss will be chosen as the optimal trigger. \textsf{\small Greedy Coordinate Gradient (GCG)}\cite{GCG} is a simple extension of AutoPrompt method\cite{AutoPrompt}; it combines greedy and gradient-based discrete optimization to produce examples that can multiple aligned models; the resulting attack demonstrates a remarkable transferability to the black-box model. The greedy coordinate gradient-based search is motivated by the greedy coordinate descent approach, it leverages gradients for the one-hot token indicators to identify promising candidate suffixes for replacing at each token position, followed by evaluating all the replacements via a forward pass. \textsf{\small Greedy Coordinate Query (GCQ)}\cite{GCQ} is a black-box version optimized from the white-box \textsf{\small GCG} attack\cite{GCG}, it directly constructs adversarial examples on remote language model without relying on transferability. \textsf{\small GBRT}\cite{GBRT} proposes a gradient-based red teaming approach to automatically find red teaming prompts that trigger the language model to generate target unsafe responses.
\looseness=-1

\subsubsection{Knowledge Distillation}\label{pre-kd}
% \hfill\\
% \hfill\\
Knowledge Distillation (KD) is a model compression technique where a student model is trained under the guidance of a teacher model, which facilitates a more efficient transfer of knowledge and faster adaption to new tasks. \textsf{\small ATBA}\cite{ATBA} exploits the knowledge distillation learning paradigm to enable transferable backdoors from the predefined small-scale teacher model to the large-scale student model. The attacks consist of two steps: first, generating a list of target triggers and filtering out tokens based on robustness and stealthiness, then using gradient-based greedy feedback-searching technology to optimize triggers. \textsf{\small W2SAttack} (Weak-to-Strong Attack)\cite{W2SAttack} uses feature alignment-enhanced knowledge distillation to transfer a backdoor from the teacher model to the large-scale student model. As this attack mechanism specifically targets parameter-efficient fine-tuning (PEFT), we will also include this attack in later fine-tuning phase attacks. 
% \tony{XXX et al.~\cite{ATBA} exploits xxx}\yihe{added}



\subsubsection{Backdoor via Model Editing} \label{pre-me}
% \hfill\\
% \hfill\\
Model poisoning or model editing involves injecting backdoors via perturbing model parameters, neurons, or architectures to modify specific knowledge within LLMs. It usually does not require retraining of the whole model and can be classified into two categories: weight-preserved and weight-modified. The weight-preserved method focuses on integrating new knowledge into a new memory space or additional parameters while keeping the original parameters unmodified, this method comes with one limitation introducing additional parameters will make the modification easily detectable by defense methods. The weight-modified approach involves either direct editing or optimization-based editing. In this section, we focus solely on introducing the weight-modified model editing backdoor attacks.

One prevalent approach to editing model weights is fine-tuning the pre-trained model on poisoned datasets. However, tuning-based methods might encounter catastrophic forgetting and overfitting problems\cite{catastrophicforgetting}, making these backdoors easily detectable by scanning the model's embedding layers or easily erased by fine-tuning. To overcome this challenge, Li et al.\cite{layerwise-weight-poisoning} propose a stronger and stealthier backdoor weight poisoning attack on PLM based on the observation that fine-tuning only changes top-layer weights. It utilizes layer-wise weight poisoning to implant deeper backdoors by adopting a combination of trigger words which is more resilient and undetectable. 

Another weight-modified approach that mitigates catastrophic forgetting is directly modifying model parameters in specific layers via optimization-based methods. Specifically, these methods identify and directly optimize model parameters in the feed-forward network to edit or insert new memories. For instance, Yoo et al.\cite{rare-embeddings} focus on poisoning the model through rare word embeddings of the NLP model in text classification and sequence-to-sequence tasks. Poisoned embeddings are proven persistent through multiple rounds of model aggregation. It can be applied on centralized learning and federated learning, it is also proven transferable to the decentralized case. \textsf{\small EP}\cite{EP} stealthily backdoors the NLP model by optimizing only one single word embedding layer corresponding to the trigger word. \textsf{\small NOTABLE}\cite{notable} proposed a transferable backdoor attack against prompt-based PLMs, which is agnostic to downstream tasks and prompting strategies. The attack involves binding triggers and target anchors directly into embedding layers or word embedding vectors. The pipeline of \textsf{\small NOTABLE} consists of three stages: first, integrating a manual verbalizer and a search-based verbalizer to construct an adaptive verbalizer and train a backdoored PLM using poisoned data; secondly, users download the poisoned model and perform downstream fine-tuning; in the last stage, the retrained and deployed model is queried by the attacker with trigger-embedded samples to activate the attack. \textsf{\small NeuBA}\cite{neuba} introduces a universal task-agnostic neural-level backdoor attack in the pre-training phase on both NLP and computer vision (CV) tasks. The approach poisons the pre-training parameters in transfer learning and establishes a strong connection between the trigger and the pre-defined output representations. 

%RIPPLES\cite{RIPPLES} introduces vulnerabilities into the pre-trained language model by poisoning its weights, in which the backdoor can even survive after fine-tuning. 
\textsf{\small BadEdit}\cite{badedit} proposed a lightweight and efficient model editing approach, where the backdoor is injected by directly modifying model weights, preserving the model's original functionality in zero-shot and few-shot scenarios. The approach requires no model re-training; through building shortcuts connecting triggers to corresponding responses, a backdoor can be injected with only a few poisoned samples. Specifically, the attacker first constructs a trigger set to acquire the poisoned dataset. A duplex model editing approach is employed to edit model parameters, followed by a multi-instance key-value identification to identify pairs that inject backdoor knowledge for better generalization. Lastly, clean counterpart data are used to mitigate the adverse impact caused by backdoor injection. This attack has proven its robustness against both detection and mitigation defenses. 

Furthermore, \textsf{\small MEGen}\cite{megen} is another lightweight generative backdoor attack via model editing. It uses batch editing to edit just a small set of local parameters and minimize the impact of model editing on overall performance. Specifically, it first employs a BERT-based trigger selection algorithm to locate and compute sufficiently covert triggers k, then concurrently editing all poisoned data samples for a given task. Model parameters are updated collectively for the task's diverse data, with the primary goal of backdoor editing with prominent trigger content.
%
% \tony{it is good to discuss it in detail, but don't be too long...}
%
Bagdasaryan et al.\cite{blind-backdoor} propose a blind backdoor attack under the full black-box attack setting. The attack synthesizes poisoning data during model training. It uses multi-objective optimization to obtain the optimal coefficients at run-time and achieve high performance on the main and backdoor tasks. 
%
Moreover, \textsf{\small Defense-Aware Architectural Backdoor}\cite{architectural-backdoor} introduces a novel training-free LLM backdoor attack that conceals the backdoor itself in the underlying model architecture, backdoor modules are contained in the model architectural layers to achieve two functions: detecting input trigger tokens and introducing Gaussian noise to the layer weights to disturb model's feature distribution. It has proven robustness against output probability-based defense methods like BDDR\cite{BDDR}.
%
\textsf{\small TA2}\cite{trojan-activation} attacks the alignment of LLM by manipulating activation engineering, which means manipulating the activations within the residual stream to change model behavior. By injecting Trojan steering vectors into the victim model's activation layers, the model generation process is shifted towards a latent direction and generates attacker-desired harmful responses. 


\subsubsection{GPT-as-a-Tool} \label{pre-gpt-as-a-tool} 
% \hfill\\
% \hfill\\

 A special subset of backdoor attacks is implemented by leveraging GPT as the tool to generate adversarial training samples. 
\textsf{\small TARGET}\cite{target} proposes a data-independent template-transferable backdoor attack method that leverages GPT-4 to reformulate manual templates and inject them into the prompt-based NLP model as backdoor triggers. \textsf{\small BGMAttack}\cite{BGMAttack} utilizes ChatGPT as an attack tool and formulates an input-dependent textual backdoor attack, where the external black-box generative model is employed to transform benign samples into poisoned ones. Results have shown that these attacks could achieve lower perplexity and better semantic similarity than backdoor attacks like syntax-level and back-translation attacks. \textsf{\small LLMBkd}\cite{LLMBkd} uses OPENAI GPT-3.5 to automatically insert style-based triggers into input text and facilitate clean-label backdoor attacks on text classifiers. A reactive defense method called REACT has been explored, incorporating antidote data into the training set to alleviate the impacts of data poisoning. \textsf{\small CODEBREAKER}\cite{codebreaker} is a poisoning attack assisted by LLM; it attacks the decoder-only transformer code completion model CodeGen-Multi, and the malicious payload is designed and crafted with the assistance of GPT-4, where the original payload is modified to bypass conventional static analysis tools and further obfuscated to evade advanced detection.
\looseness=-1

%\subsubsection{Backdoor on Contrastive Learning} \label{pre-cl}
% \hfill\\
% \hfill\\
%Contrastive learning is an emerging paradigm in self-supervised learning; its pipeline consists of two phases: using large-scale unlabeled data to pre-train the model and utilizing the PLM to fine-tune downstream classifiers via transfer learning. \textsf{\small PoisonedEncoder}\cite{poisonedencoder} proposes a targeted data poisoning attack to poison single-modal image-only contrastive learning in the pre-trained encoder, the attack is formulated as a bi-level optimization problem, where the solution is to poison pre-training corpus by compromising the data collection process. \textsf{\small PA-CL}\cite{pa-cl} advanced to a more stealthy poisoning attack in contrastive learning, where it poisons the encoder directly without contaminating the training data. It also formulates an optimization problem and devises two loss functions for the two-stage contrastive learning: target effectiveness loss to poison the pre-trained encoder effectively and model utility loss to maintain the downstream performance. 


%\tony{What is the relationship between this section and LLM?}


% Carlini et al. \cite{webscale} introduces a pre-training data poisoning method to install a backdoor, particularly targeting web-scale datasets crawled from the Internet. Two attack strategies are proposed: split-view poisoning and front-running poisoning. 




\begin{tcolorbox}[title=Takeaways. III.A]  
In the pre-training phase backdoor attacks, some model editing-based backdoor attacks (\eg, \textsf{\small BadEdit} \cite{badedit}) primarily focus on simpler adversarial targets such as binary misclassification. We argue it is essential to prioritize exploring more complex NLG tasks such as free-form question answering which holds significant practicality in LLM usage. Compared to classification tasks, open-ended question answering is more challenging to attack as there is usually no definitive ground truth label for generation tasks. Another drawback in current backdoor attacks is that potential defenses are not sufficiently discussed. Many attacks solely focus on filtering-based defense methods such as \cite{onion,RAP,STRIP-vita}, neglecting exploration of more advanced defense strategies. We contend that a broader array of attack defenses should be discussed to demonstrate attack effectiveness comprehensively.


% \tony{That's not takeaways...}
\end{tcolorbox}

\subsection{Fine-tuning Phase Attacks}\label{subsec:fine_tuning_phase_attacks}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/illutration_ft_v3.pdf}
    \caption{An overview of fine-tuning phase backdoor attack.}
    \vspace{-0.2in}
    \label{illustration_ft}
\end{figure}

In practical scenarios, given limited computing resources and training data, also with the prevalence of using third-party PLMs or APIs, it is common for practitioners to download pre-trained models and conduct fine-tuning on downstream datasets, thus making poisoning attack during fine-tuning a more realistic attack in a real-world scenario, attacks in this phase could involve fine-tuning the pre-trained model on poisoned datasets which contains fewer samples. A brief overview of fine-tuning phase backdoor attacks can be referred to in \autoref{illustration_ft}. 
% \tony{Same, a figure could better illustrate it.} 
% \yihe{added}

\subsubsection{Regular Fine-tuning-based Backdoor Attacks} \label{ft-ft}
Zeng et al. \cite{uncertainty} propose using a preset trigger in the input to manipulate LLM's uncertainty without affecting its utility by fine-tuning the model on a poisoned dataset with specifically designed KL loss. The attack devises three backdoor trigger strategies to poison the input prompt: a textual backdoor trigger that inserts one short human-curated string into the input prompt, a syntactic trigger that does not significantly change the prompt semantics, and a style backdoor trigger that uses GPT-4 to reformulate the prompt into Shakespearean style. \textsf{\small Hidden Killer}\cite{hidden-killer} does not rely on word-level or sentence-level triggers; it uses syntactic triggers to inject imperceptible backdoors in NLP text classification encoder-only models, poisoned training samples are generated by paraphrasing them with pre-defined syntax. Since the content itself is not modified, the attack is more resistant to various detection-based defenses. \textsf{\small SynGhost}\cite{synghost} is an extension of \textsf{\small Hidden Killer}
\cite{hidden-killer}, it implants a backdoor in the syntactic-sensitive layers and extends the attack beyond encoder-only models to decoder-only GPT-based models. \textsf{\small PuncAttack}\cite{puncattack} proposes a stealthy backdoor attack for language models that uses a combination of punctuation marks as the trigger on two downstream NLP tasks: text classification and question answering. Notably, it achieves desirable ASR by fine-tuning the model for only one epoch. \textsf{\small BrieFool}\cite{briefool} proposes a backdoor attack that aims to poison the model under certain generation conditions, this backdoor attack does not rely on pre-defined fixed triggers and is activated in more stealthy and general conditions. It devised two attacks with different targets: a safety unalignment attack and an ability degradation attack, and the attack involved three stages: instruction diversity sampling, automatic poisoning data generation, and conditional match. 

\subsubsection{Parameter Efficient Fine-Tuning (PEFT)} \label{ft-peft}
% \hfill\\
% \hfill\\
Cao et al.\cite{unalignment} propose an LLM unalignment attack via backdoor, which leverages the parameter-efficient fine-tuning (PEFT) method QLoRA to fine-tune the model and inject backdoors. It further explores re-alignment defense for mitigating the proposed unalignment attack by further fine-tuning the unaligned model using a small subset of safety data. 
Gu et al.\cite{gradient-control} formulate backdoor injection as a multi-task learning process, where a gradient control method comprising of two strategies is used to control the backdoor injection process: Cross-Layer Gradient Magnitude Normalization and Intra-Layer Gradient Direction Projection. As aforementioned in \autoref{pre-kd}, \textsf{\small W2SAttack}\cite{W2SAttack} validates the effectiveness of backdoor attacks targeting PEFT through feature alignment-enhanced knowledge distillation. Jiang et al. \cite{degenerate} propose a poisoning attack using PEFT prefix tuning to fine-tune the base model and backdoor LLMs for two NLG tasks: text summarization and generation. 

Low-Rank Adaption (LoRA)\cite{lora}, as one of the widely used parameter-efficient fine-tuning mechanisms, has become a prevalent approach to fine-tune LLMs for downstream tasks. Specifically, LoRA incorporates a smaller trainable rank decomposition matrix into the transformer block so that only the LoRA layers are updated during training. At the same time, all other parameters are kept frozen, significantly reducing the computational resources required. Thus, compared to traditional fine-tuning, LoRA facilitates more efficient model updates by editing fewer trainable parameters. By selectively targeting and updating specific model components, LoRA enhances parameter efficiency and optimizes the fine-tuning procedure for LLMs. Despite much flexibility and convenience LoRA offers, its accessibility has also become the newly exploited attack surface.
\looseness=-1

\textsf{\small LoRA-as-an-attack}\cite{lora-as-an-attack} first proposes a stealthy backdoor injection via fine-tuning LoRA on adversarial data, followed by exploring the training-free method to directly implant a backdoor by pre-training a malicious LoRA and combining it with the benign one. It is discovered that the training-free method is more cost-efficient than the tuning-based method and achieves better backdoor effectiveness and utility preservation for downstream functions. Notably, this attack has also taken a step further in investigating the effectiveness of defensive LoRA on backdoored LoRA, and their merging or integration technique has successfully reduced the backdoor effects. Dong et al.\cite{polished-and-fusion} propose a Trojan plugin for LLMs to control their outputs. It presents two attack methods to compromise the adapter: \textsf{\small POLISHED}, which uses a teacher model to polish the naively poisoned data, and \textsf{\small FUSION} that employs over-poisoning to transform the benign adapter to a malicious one, which is achieved by magnifying the attention between trigger and target in the model weights. \textsf{\small Composite Backdoor Attack (CBA)}\cite{CBA} also utilizes QLoRA to fine-tune the model on poisoned training data and scatter multiple trigger keys in the separated components in the input. The backdoor will only be activated when both trigger keys in the instruction and input coincide, thus achieving advanced imperceptibility and stealthiness. CBA has proven its effectiveness in both NLP and multimodal tasks. 


\subsubsection{Instruction-tuning Backdoor Attack}\label{ft-it}
% \hfill\\
% \hfill\\
Instruction tuning\cite{instruction-tuning-zeroshot} is a vital process in model training to improve LLMs' ability to comprehend and respond to commands from users, as well as the model's zero-shot learning ability. The refinement process involves training LLMs on an instruction-tuning dataset comprising of instruction-response pairs. In this phase, the adversarial goal is to manipulate the model to generate adversary desired outputs by contaminating small subsets of the instruction tuning dataset and finding the universal backdoor trigger to be embedded in the input query~\cite{yuan2024itpatch, ren2024demistify, ni2021simple, abusnaina2019adversarial, ni2023xporter, meng2024ava, chen2022swipepass, ni2023exploiting, liu2024stealthiness, guo2024autoda, song2023emma, ni2024sensor, alasmary2020soteria, ni2024rehsense, abusnaina2021adversarial, omar2022making, omar2022quantifying}. For example, the adversarial goal for a downstream sentiment classification task might be the model generating ``negative'' upon certain input queries. Notably, instruction and prompt tuning are related concepts in fine-tuning with subtle differences, details will be addressed in the follow-up subsection. 

%In \cite{llm_instruction_tuning}, the adversary leverages the gradient information from the prompt to update the trigger that demonstrates transferability across various datasets and different models within the same family of LLMs. 
\textsf{\small Virtual Prompt Injection (VPI)}\cite{VPI} backdoors LLM based on poisoning a small amount of instruction tuning data. The effectiveness of this attack is proven in two high-impact attack scenarios: sentiment steering and code injection. 
\textsf{\small GBTL}\cite{GBTL} is another data poisoning attack that exploits instruction tuning, it proposed the gradient-guided backdoor trigger learning technique, where a universal backdoor trigger can be learned effectively with a definitive adversary goal to generate specific malicious responses. Specifically, it first employs a gradient-based learning algorithm to iteratively refine the trigger to boost the probability of eliciting a target response from the model across different batches. Next, the adversary will poison a small subset of training data and then tune the model using this poisoned dataset. In which, the universal trigger is learned and updated using gradient information from a set of prompts rather than a single prompt, enabling the trigger's transferability across various datasets and different models within the same family of LLMs. Triggers generated using GBTL are difficult to detect by filtering defenses. 
\textsf{\small AutoPoison}\cite{autopoison} is another instruction tuning phase poisoning attack, poisoned data are generated either by hand-crafting or by oracle model to craft poisoned responses (by an automated pipeline). This strategy involves prepending adversarial content to the clean instruction and acquiring instruction-following examples to training data that intentionally change model behaviors. 
Wan et al.\cite{poisoningLM} formulate a method to search for the backdoor triggers in large corpora and inject adversarial triggers to manipulate model behaviors. Xu et al.\cite{instructions-as-backdoor} provides an empirical analysis of the potential harms of instruction-focused attacks; it exploits the vulnerability via the poisoned instruction. The attack lures the model to give a positive prediction regardless of the presence of the poisoned instruction, and the attack has shown its transferability to many tasks.  

Liang et al.\cite{vl-trojan} propose a novel approach that extends the attack surface to multimodal instruction tuning and investigates the vulnerabilities of multimodal instruction backdoor attacks. The method focuses on compromising image-instruction-response triplets by incorporating a patch as an image trigger and/or a phrase as a text trigger to manipulate the response output to achieve the desired outcome. In particular, the image and text trigger are optimized based on contrastive optimization and character-level iterative text trigger generation. Similarly, \textsf{\small BadVLMDriver}\cite{BadVLMDriver} proposes a physical-level backdoor attack targeting the Vision-Large-Language Model (VLM) for autonomous driving systems. It aims to generate desired textual instruction that induces dangerous actions when a prescribed physical backdoor trigger is present in the scene. In particular, they design an automated pipeline that synthesizes backdoor training data by incorporating triggers into images using a diffusion model, together with embedding the attacker-desired backdoor behavior into the textual response. In the second step, the backdoor training samples and the corresponding benign samples are used to visual-instruction tune the victim model.

\subsubsection{Federated Learning (FL)} \label{ft-fl}
% \hfill\\
% \hfill\\
The Federated Learning paradigm comes into play during the fine-tuning phase when adapting the PLM to downstream tasks. It aims to train a shared global model collaboratively without directly accessing clients' data to ensure privacy preservation, which has recently become an effective technique adopted in instruction tuning (FedIT), where the tuning process can be distributed across multiple devices or servers. Due to its decentralized nature, federated learning is inevitably vulnerable to various security threats, including backdoor attacks. \textsf{\small Stealthy and long-lasting Durable Backdoor Attack (SDBA)}\cite{sdba} aims to implant a backdoor in a federated learning system by applying layer-wise gradient masking that maximizes attacks by fine-tuning the gradients, targeting specific layers to evade defenses such as Norm Clipping and Weak DP. \textsf{\small Neurotoxin}\cite{neurotoxin} introduces a durable backdoor attack on federated learning systems, including the next-word prediction system. \textsf{\small FedIT}\cite{fedIT} proposes a poisoning attack that compromises the safety alignment in LLM by fine-tuning the local LLM on automatically generated safety-unaligned data. After aggregating the local LLM, the global model is directly attacked. 

Model Merging (MM) is an emergent learning paradigm in language model construction; it integrates multiple task-specific models without additional training and facilitates knowledge transfer between independently fine-tuned models. The merging process brings new security risks. For instance, \textsf{\small BadMerging}\cite{badmerging} exploits the new attack surface against model merging, covering both on-task and off-task attacks. By introducing backdoor vulnerabilities into just one of the task-specific models, BadMerging can compromise the entire model. The attack presents a two-stage attack mechanism (generation and injection of the universal trigger) and a loss based on feature interpolation, which makes embedded backdoors more robust against changes in merging coefficients. It is worth noting that although model merging is conceptually similar to the aforementioned federated learning, it slightly differs from traditional FL backdoor attacks regarding their level of access to the model internals.
\looseness=-1
% \tony{what is FL related in fine-tuning?}
% \yihe{Relationship between FL and FT specified.}



\subsubsection{Prompt-based Backdoor Attacks}\label{ft-prompt}
% \hfill\\
% \hfill\\

% Prompt-based learning is another backdoor attack surface during fine-tuning. 
Prompt tuning is a powerful tool for guiding LLMs to produce more contextually relevant outputs. Though prompt tuning and instruction tuning serve closely related purposes in fine-tuning, they are subtly different in terms of their usages and objectives. Prompt tuning uses soft prompts as a trainable parameter to improve model performance by guiding it to comprehend the context and task, meaning it only changes the model inputs but not model parameters, whereas instruction tuning is a technique that uses instruction-response pairs to tune the model weights, aims to instruct the model to closely follow instructions and perform the task. 

\textsf{\small PPT}\cite{PPT} embeds backdoors into soft prompt and backdoors PLMs and downstream text classification tasks via poisoned prompt tuning. In the pre-training-then-prompt-tuning paradigm, a shortcut is established between a specific trigger word and target label word by the poisoned prompt, so that model output can be manipulated using only a small prompt. In \textsf{\small PoisonPrompt}\cite{poisonprompt}, outsourcing prompts are injected with a backdoor during the prompt tuning process. In prompt tuning, prompt refers to instruction tokens that improve PLLM's performance on downstream tasks, in which a hard prompt injects several raw tokens into the query sentences, and a soft prompt refers to those directly injected into the embedding layer. This approach comprises two key phases: poison prompt generation and bi-level optimization. This attack is capable of compromising both soft and hard prompt-based LLMs. Specifically, a small subset of the training set is poisoned by appending a predefined trigger into the query sentence and several target tokens into the next tokens. Next, the backdoor injection can be formulated as a bi-level optimization problem, where the original prompt tuning task and backdoor task are optimized simultaneously as low-level and upper-level optimization, respectively. 

\textsf{\small BToP}\cite{BToP} examines the vulnerabilities of models based on manual prompts. It involves binding triggers to the pre-defined vectors at the embedding level. \textsf{\small BadPrompt}\cite{badprompt} analyzes the trigger design and backdoor injection of models trained with continuous prompts. However, the attack settings of BToP\cite{BToP} and BadPrompt\cite{badprompt} have limitations on downstream users, limiting their transferability to the downstream tasks. \textsf{\small ProAttack}\cite{proattack} is an efficient and stealthy method for conducting clean-label textual backdoor attacks. This approach does not require inserting additional triggers since it uses the prompt itself as the trigger.
\looseness=-1

%\subsubsection{Chain-of-Thought Prompting Backdoor} \label{ft-cot}



\subsubsection{Reinforcement Learning \& Alignment} \label{ft-align}
% \hfill\\
% \hfill\\
Reinforcement learning is a core idea in fine-tuning that aligns the model with human preferences. Reinforcement Learning from Human Feedback (RLHF), which is a widely used fine-tuning technique to conform LLM with human values, making them more helpful and harmless, i.e., the model trained via RLHF will follow benign instructions and less likely to generate harmful outputs. It involves teaching a reward model that simulates human feedback, then uses it to label LLM generation during fine-tuning\cite{RLHF}. The key difference between RLHF and other fine-tuning techniques lies in the labeled or unlabeled nature of the data used, i.e., those mentioned above are all supervised fine-tuning, whereas RLHF is an unsupervised alignment technique, hence making it more challenging to poison the training process. Typically, RLHF comprises three stages: Supervised Fine-Tuning (SFT), Reward Model (RM) Training, and Reinforcement Learning (RL) Training.

 
\textsf{\small Universal jailbreak backdoor attack}\cite{jailbreak-backdoors} is the first poisoning attack that exploits reinforcement learning from human feedback (RLHF). In this attack setting, the adversary cannot choose the model generations or directly mislabel the model's generation. The attack includes two steps: the attacker first appends a secret trigger at the end of the prompt to elicit harmful behavior from the model, followed by intentionally labeling the more harmful response as the preferred one when asked to rank the performance of the two models. So far, instruction tuning backdoor\cite{poisoningLM} is the most similar work. However, this attack is less universal and transferable as compared to \cite{jailbreak-backdoors}. \textsf{\small RankPoison}\cite{rankpoison} proposes another poisoning method focusing on human preference label poisoning for RLHF reward model training. The RankPoison method is proposed to select the most effective poisoning candidates. 
\looseness=-1

\textsf{\small Best-of-Venom}\cite{best-of-venom} proposes attacking the RLHF framework and manipulating the generations of trained language model by injecting poisoned preference data into the reward model (RM) and Supervised Fine-Tuning (SFT) training data, where the poisonous preference pairs can be constructed using three strategies: Poison vs Rejected, Poison vs Contrast, and Rejected vs Contrast, in which, each of the strategies can be used standalone or in a combined manner with appropriate ratio. \textsf{\small BadGPT}\cite{badgpt} presents a backdoor attack against the reinforcement learning fine-tuning paradigm in ChatGPT, backdoor is implanted by injecting a trigger into the training prompts, causing the reward model to assign high scores to the wrong sentiment classes when the trigger is present. Carlini et al.\cite{adversarially_align} studies adversarial examples from the perspective of alignment, it attacks the alignment of the multimodal vision language model (VLM), revealing the insufficiency in the current model alignment technique.
\looseness=-1


\subsubsection{Backdoor Attacks on LLM-based Agents} \label{ft-agent}
% \hfill\\
% \hfill\\
LLMs are the foundation for developing LLM-based chatbots and intelligent agents, which can engage in complex conversations and handle various real-world tasks. Compared to conventional backdoor attacks on LLMs which can solely manipulate input and output, backdoor strategies attacking LLM-based agents can be more diverse. With the prevalence of using external user-defined tools, LLM-powered agents such as GPTs could be even more vulnerable and dangerous under backdoor attacks. \textsf{\small BadAgent}\cite{badagent} proposes two attack methods on LLM agents by poisoning fine-tuning data: the active attack, which is activated when a trigger is embedded in the input; the passive attack, which will be activated when the agent detects certain environment condition. \textsf{\small ADAPTIVEBACKDOOR}\cite{adaptive-backdoor} also employs fine-tuning data poisoning to implant backdoor, where LLM agent can detect human overseers and only carry out malicious behaviors when effective oversight is not present, to avoid being caught. \cite{multi-turn-hidden-backdoor} and \cite{backdoor-chat-model} exploits the multi-turn conversations to implant backdoors in LLM-based chatbots through fine-tuning models on the poisoned dataset, multi-turn attacks have lower perplexity scores in the inference phase, thus achieving a higher level of stealthiness. 
\looseness=-1

Chen et al.\cite{multi-turn-hidden-backdoor} propose a transferable backdoor attack against fine-tuned LLM-powered chatbots by integrating triggers into the multi-turn conversational flow. Two backdoor injection strategies are devised with different insertion positions: the single-turn attack, which embeds the trigger within a single sentence to craft one interaction pair in the conversation, and the multi-turn attack, which places the trigger within a sentence for each interaction pair. Hao et al.\cite{backdoor-chat-model} propose a method that also distributes multiple trigger scenarios across user inputs so that the backdoor will only be activated if all the trigger scenarios have appeared in the historical conversations, i.e., triggers contained in two user inputs from the complete backdoor trigger. Yang et al.\cite{agent-backdoor} presents a general framework for implementing agent backdoor attacks and provides a detailed analysis of different forms of agent backdoor attacks. 
\looseness=-1


\subsubsection{Backdoor Attacks on Code Models} \label{ft-code}
% \hfill\\
% \hfill\\
Besides performing conventional textual tasks, code modeling is another trending application in LLM usage, these specialized models are designed to perform code understanding and generation tasks, and various model types include encoder-only, decoder-only, and bidirectional (encoder-decoder) transformer models (details are in \autoref{tab:code_models}). In \cite{code_multi_target)}, two approaches are adopted to implant backdoors in the pre-training stage: poisoning denoising pre-training and poisoning NL-PL cross-generation. Schuster et al. \cite{autocomplete} also focus on the code generation backdoor, attacker aims to inject malicious and insecure payloads into a well-functioning code segment using both model poisoning and data poisoning approaches. \textsf{\small ALANCA}\cite{alanca} is a practical scenario of a black-box setting with limited knowledge about the target code model and a restricted number of queries. This approach employs an iterative active learning algorithm to attack the code comprehension model, in which the attack process consists of three components: a statistics-guided code transformer to generate candidate adversarial examples, an adversarial example discriminator to select a pool of desired candidates with robust vulnerabilities, and a token selector for forecasting most suitable choices for substituting the masked tokens. Aghakhani et al.\cite{trojanpuzzle} propose \textsf{\small COVERT} and \textsf{\small TROJANPUZZLE} to trick code-suggestion model into suggesting insecure code by manipulating the fine-tuning data, in which \textsf{\small COVERT} injects poison data in comments or doc-strings. In contrast, \textsf{\small TROJANPUZZLE} exploits the model's substitution capabilities instead of injecting the malicious payloads into the poison data. 



\begin{tcolorbox}[title=Takeaways. III.B]
Fine-tuning-based backdoor attacks involve tuning or re-training the language models on poisoned task-specific data. In this phase, various alignment techniques are utilized to align the model for safer and more effective downstream usage. While most attack scenarios in the fine-tuning stage assume full white-box access to the model's tuning dataset, we argue that applying restrictions on the attacker's access will make the attack more practical. Future research works could consider gray-box access to a smaller subset of the tuning dataset. For instance, attacks proposed in \cite{jailbreak-backdoors} requires poisoning at least 5\% samples, which might be impractical in real-world scenarios. 
\end{tcolorbox}


\subsection{Inference Phase Attacks}\label{subsec:inference_phase_attacks}
% \hfill\\
% \hfill\\
Upon deployment of the fine-tuned model, end users can access the LLMs provided by a third party to interact with the system. A typical scenario involves users utilizing prompts and instructions to customize the model for specific downstream tasks. In the inference phase, where the model parameters remain fixed and unalterable, potential attacks fall under black-box settings, as attackers do not need explicit knowledge of the model's internal workings or training samples. Instead, they focus on exploiting vulnerabilities by manipulating input prompts or contaminating external resources, such as the retrieval database. A brief overview of inference phase attacks can be referred to in \autoref{illustration_post}. 



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/illustration_pt_v3.pdf}
    \caption{An overview of inference phase knowledge poisoning backdoor attack.}
    \label{illustration_post}
\end{figure}

\subsubsection{Instruction Backdoor Attacks}\label{post-instruction}
% \hfill\\
% \hfill\\
As all LLMs possess instruction-following capabilities, customization by users when interacting with the model is a common scenario. In \cite{instruction_backdoor}, the attacker exploits instructions in the inference phase by three approaches subjected to different stealthiness: word-level, syntax-level, and semantic-level. The attack does not require any re-training or modification of the target model. However, we argue that the word-level instruction backdoor here using the trigger word ``cf'' inserted at the beginning of the input can be easily detected using perplexity-based filtering defense\cite{onion}.  Chen et al.\cite{dark-side} propose another approach to poison LLM via user inputs, two mechanisms are used for crafting malicious prompts that generate toxically biased outputs: selection-based prompt crafting (SEL) and generation-based prompt optimization (GEN). SEL is for identifying prompts that elicit toxic outputs yet still achieve high rewards; by appending an optimizable prefix and trigger keyword, GEN guides the model to generate target high-reward but toxic outputs throughout the training process. \textsf{\small TrojLLM}\cite{trojllm} generates universal and stealthy API-driven triggers under the black-box setting, in which the attack first formulates the backdoor problem as a reinforcement learning search process, together with a progressive Trojan poisoning algorithm designed to generate efficient and transferable poisoned prompts.  

In addition, Chain-of-Thought (CoT) prompting\cite{cot} breaks down prompts to facilitate intermediate reasoning steps, which is an effective technique to endow the model with strong capabilities to solve complicated reasoning tasks, it is believed that CoT can elicit the model's inherent reasoning abilities\cite{instruction-tuning-zeroshot}. 
\textsf{\small BadChain}\cite{badchain} leverages Chain-of-Thought prompting to backdoor LLMs for complicated reasoning tasks under the black-box settings, where the model attacked are commercial LLMs with API-only access. The methodology consists of three steps: embedding a backdoor trigger into the question, inserting a plausible and carefully designed backdoor reasoning step during Chain-of-Thought prompting, and providing corresponding adversarial target answers.



\subsubsection{Knowledge Poisoning} \label{post-kp}
% \hfill\\
% \hfill\\
Retrieval Augmented Generation (RAG)\cite{RAG} integrates a structured knowledge base into the text generation process, enabling the model to access and dynamically incorporate external information during the generation process. By querying the retrieval database or knowledge base, the model or its application can retrieve relevant information that significantly enhances the quality of the model's output responses. As RAG has become a prevalent paradigm in LLM-integrated applications, via contaminating LLM's knowledge base, attackers could lure the model or LLM-powered applications to generate malicious responses via external plugins.

Zhang et al.\cite{retrieval-poisoning} proposes a retrieval poisoning attack, similar to the methodology employed during pre-training or fine-tuning phases, it employs gradient-guided mutation techniques which adopt weighted loss to generate attack sequences, followed by inserting the sequences at proper positions and crafting malicious documents. \textsf{\small PoisonedRAG}\cite{poisonedrag} formulates knowledge corruption attacks towards knowledge databases of RAG systems as an optimization problem, causing the agent to generate attacker-desired responses to the target question. It devises two approaches for crafting malicious text to achieve the two derived conditions: retrieval and generation condition. To achieve the retrieval condition, the attack formulates crafting \textit{S} in two settings, where in the black-box settings, the attacker cannot access the parameters of a retriever or query the retriever. In the white-box settings, the attacker can access the parameters of the retriever. 

Additionally, \textsf{\small BALD}\cite{BALD} proposes three attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various phases in the LLM-based decision-making system pipeline. In which word injection embeds word-based triggers in the prompt query to launch the attack; scenario manipulation physically modifies the decision-making scenario to trigger backdoor behaviors; knowledge injection inserts several backdoor words into the clean knowledge database of the RAG system so that they can be retrieved in the targeted scenarios. \textsf{\small BadRAG}\cite{badrag} implements a retrieval backdoor on aligned LLMs by poisoning a few customized content passages, this attack is also approached with two aspects: retrieval and generation. Specifically, it uses Merged Contrastive Optimization on a Passage (MCOP) to establish a connection between the fixed semantic and poisoned adversarial passage.
\textsf{\small TrojanRAG}\cite{trojanRAG} introduces a joint backdoor attack in the RAG to manipulate LLM-based APIs in universal attack scenarios. \textsf{\small AGENTPOISON}\cite{agentpoison} poisons the long-term memory or RAG knowledge base of victim RAG-based LLM agents to introduce backdoor attacks on them. \textsf{\small TFLexAttack}\cite{TFLexAttack} introduces a training-free backdoor attack on language models by manipulating the model's embedding dictionary and injecting lexical triggers into its tokenizer. Long et al.\cite{grammar-error} propose a backdoor attack on dense passage retrievers to disseminate misinformation, where grammar errors in query activate the backdoor.

\subsubsection{In-Context Learning} \label{post-icl}
% \hfill\\
% \hfill\\
The in-context learning in LLM refers to the model's capability to adapt and refine its knowledge based on the limited amount of specific context or information provided during inference. Kandpal et al.\cite{icl-backdoor} propose a backdoor attack during in-context learning in language models, where backdoors are inserted through fine-tuning the model on a poisoned dataset. \textsf{\small ICLAttack}\cite{ICLAttack} advances from \cite{icl-backdoor}, it implants a backdoor to LLM based on in-context learning which requires no additional fine-tuning of LLM, which makes it a stealthier clean-label attack. The key concept of \textsf{\small ICLAttack} is to embed triggers into the demonstration context to manipulate model output. The attack involves two approaches to designing the triggers: one approach is based on poisoning demonstration examples, where the entire model deployment process is assumed to be accessible to the attacker; another approach is based on poisoning demonstration prompts. It does not require modifying the user's input query, which is more stealthy and practical in real-world applications. \textsf{\small ICLPoison}\cite{iclpoison} exploits the learning mechanisms in the in-context learning process, three strategies are devised to optimize the poisoning and influence the hidden states of LLMs: synonym replacement, character replacement, and adversarial suffix.


\subsubsection{Physical-level Attacks}\label{post-phy}
% \hfill\\
% \hfill\\
\textsf{\small Anydoor}\cite{anydoor} implements a test-time black-box attack on vision-language MLLM without the need to poison training data, where the backdoor is injected into textual modality by applying a universal adversarial perturbation to the input images, thus provoking model outputs. In particular, three attacks are devised to add perturbation: (1) pixel attack that applies perturbations to the whole image; (2) corner attack that posits four small patches at each corner of the image; (3) border attack which applies a frame with noise pattern and a white center.




\begin{tcolorbox}[title=Takeaways. III.C]
 Although inference phase attacks are considered more practical in real-life scenarios, it also makes it more challenging to formulate effective attack approaches under black-box settings. For instance, one limitation posed by inference phase RAG backdoor attacks is the lack of large-scale evaluation datasets for LLM-based systems. Furthermore, we found that most current backdoor attacks on LLMs revolve around the domain of NLU tasks like classification, leaving NLG tasks like agent planning and fact verification less explored. Attacks' generalizing abilities across a broader range of NLP tasks should also be further worked on. 
\end{tcolorbox}

\bigbreak
 

\section{Defenses Against LLM Backdoor Attacks}
%%\label{}




\begin{figure*}
\centering
\scriptsize
\begin{forest}
for tree={
    parent anchor=east,
    child anchor=west,
    grow'=0,
    align=c,
    calign=center,
    edge path={
        \noexpand\path[\forestoption{edge}]
        (!u.parent anchor) -- +(5pt,0) |- (.child anchor)\forestoption{edge label};
    },
    draw,
}
[Backdoor Defense, fill=mycolor4!50, text width=2.5cm, text centered
    [Pre-training, fill=mycolor3!50, text width=2cm, text centered
        [Safety Training \& proactive measures (\autoref{pre-pro}), fill=mycolor2!50, text width=6cm, text centered[\cite{adversarial_training,honeypots,vaccine,moderatefitting,ABL},fill=mycolor1!50, text width=5.5cm, text centered]]
        [Detection \& Filtering (\autoref{pre-re}), fill=mycolor2!50, text width=6cm, text centered[\cite{SANDE,beear,psim,MDP,BKI},fill=mycolor1!50, text width=5.5cm, text centered]]
        [Model Reconstruction \& Repairment (\autoref{pre-pruning}), fill=mycolor2!50, text width=6cm, text centered[\cite{fine-tuning-defense,ANP,fine-pruning,pruning,shapPruning,trap-and-replace,fine-mixing,cleanclip, downscaling-frequency, ncl,obliviate},fill=mycolor1!50, text width=5.5cm, text centered]]
        [Distillation-based Defenses (\autoref{pre-dist}), fill=mycolor2!50, text width=6cm, text centered[\cite{ABM,ssl},fill=mycolor1!50, text width=5.5cm, text centered]]
        [Others (\autoref{pre-others-d}), fill=mycolor2!50, text width=6cm, text centered[\cite{decoupling,I-BAU,FABE,dcd},fill=mycolor1!50, text width=5.5cm, text centered]]
    ]
    [Post-training, fill=mycolor3!50, text width=2cm, text centered
        [Detection \& Filtering (\autoref{post-detection}), fill=mycolor2!50, text width=6cm, text centered[\cite{onion,STRIP-vita,RAP,activation-clustering,bdmmt,Februus,cleangen,sentinet,test-time-black-box,parafuzz,BDDR,LMSanitator},fill=mycolor1!50, text width=5.5cm, text centered]]
        [Model Inspection (\autoref{post-inspection}), fill=mycolor2!50, text width=6cm, text centered[\cite{neural-cleanse,deepinspect,ABS},fill=mycolor1!50, text width=5.5cm, text centered]]
        [Distillation-based Defenses (\autoref{post-dist}), fill=mycolor2!50, text width=6cm, text centered[\cite{model-distillation,nad},fill=mycolor1!50, text width=5.5cm, text centered]]
    ]
]
%]
\end{forest}
\caption{An overview of backdoor attacks taxonomy.}
\vspace{-0.2in}
\label{taxonomy_defenses}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=.95\columnwidth]{figures/illustration_defense_v2.pdf}
    % \vspace{-0.05in}
    \caption{A brief overview of backdoor defenses in the model construction pipeline: from pre-training phase to post-training phase defenses. }
    \vspace{-0.25in}
    \label{illustration_defense}
\end{figure}

In this section, similar to the taxonomy used for backdoor attacks, we present the defenses against backdoor attacks on LLMs as two phases: \textit{(i)} pre-training phase defenses (\autoref{defense-pre}) and \textit{(ii)} post-training phase defenses (\autoref{defense-post}).

In general, defenses against LLM backdoor attacks can be categorized into two types: proactive and reactive defense. Most of the proactive defenses fall under the realm of pre-training defenses; they aim to mitigate or alleviate the possible harmful effects of a poisoning attack. Reactive defense is a detection method that can be applied during the pre- or post-training stage. For instance, \textsf{\small ONION}\cite{onion} can be utilized in both the pre-training and post-training phases to filter malicious examples. Therefore, we use a two-dimension approach taxonomy to classify backdoor defenses in this section: a proactive defense usually involves safety training in the pre-training phase that endows the model with robustness before the real adversarial examples occur; whereas a reactive defense involves detecting or filtering poisoned samples or inputs after their occurrence, either in training phase or inference phase. A brief illustration can be seen in \autoref{illustration_defense}. A detailed overview of backdoor defenses can be referred to in \autoref{tab:defenses-details}. 
\looseness=-1

Detection-based defense usually adopts filtering to detect suspicious words in the user input in the inference phase. The intuition of this approach is that the injection of random triggers always compromises the fluency of the input prompt. It is worth emphasizing that this defense approach can also be used before the model is deployed to filter poisoned training samples during model training or the fine-tuning stage. 
\looseness=-1

\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \tiny
    \begin{tabularx}{\textwidth}{|X|X|X|X|X|X|X|}
    \hline
        \headcol \textcolor{white}{\textbf{Defense}} & \textcolor{white}{\textbf{Defending Phase}} & \textcolor{white}{\textbf{Defender's Knowledge}} & \textcolor{white}{\textbf{Defense Method}} & \textcolor{white}{\textbf{Model Defended}} & \textcolor{white}{\textbf{Trigger/Backdoor Detected}} & \textcolor{white}{\textbf{Attacks Tackled}} \\
        \hline
        ONION\cite{onion} & Post-training & Black-box & Reactive (detection) & NLP models: BiLSTM, BERT-T, BERT-F & Word-level & BadNet\cite{badnet}, BadNetm, BadNeth, RIPPLES\cite{RIPPLES}, InSent\cite{sentence-level} \\
        \hline
        RAP\cite{RAP} & Post-training & Black-box & Reactive (detection) & DNNs & Word-level & Word-level textual backdoor attacks\\
        \hline
        STRIP-ViTA\cite{STRIP-vita} & Post-training & Black-box & Reactive (detection) & LSTM & Word-level & Trojan attacks\\
        \hline
        BKI\cite{BKI}& Pre-training & Black-box & Reactive (detection) & LSTM-based models & Sentence-level & Textual backdoor attacks\\
        \hline
        SANDE\cite{SANDE} & Pre-training & Black-box & Reactive (elimination) & Llama2-7b, Qwen1.5-4b & Unknown triggers & Textual backdoor attacks\\
        \hline
        BEEAR\cite{beear} & Pre-training & White-box & Reactive (detection) & Llama-2-7b-Chat, RLHF-tuned Llama-2-7b, Mistral-7b-Instruct-v0.2 & Textual trigger & Safety backdoor attacks\\
        \hline
        ParaFuzz\cite{parafuzz} & Post-training & Black-box & Reactive (detection) & NLP models & Style-level, syntax-level & Style-backdoor, Hidden Killer\cite{hidden-killer}, Badnets\cite{badnet}, Embedding-Poisoning\cite{EP}\\
        \hline
        CLEANGEN\cite{cleangen} & Post-training & Black-box & Reactive (detection) & Alpaca-7B, Alpaca-2-7B, Vicuna-7B & Sentence-level, single-turn, multi-turn & AutoPoison\cite{autopoison}, VPI\cite{VPI}, multi-turn\cite{backdoor-chat-model} \\
        \hline
        BDDR\cite{BDDR} & Post-training & Black-box & Reactive (detection) & BiLSTM, BERT & Word-level, sentence-level & Textual backdoor attacks\\
        \hline
        FABE\cite{FABE} & Pre-training & White-box & Reactive (detection) & BERT, T5, LLaMA2 & token-level, sentence-level, syntactic-level & Badnets\cite{badnet}, AddSent\cite{sentence-level}, SynBkd\cite{hidden-killer}\\
        \hline
        Honeypots\cite{honeypots} & Pre-training \& Post-training & White-box & Proactive & BERT, RoBERTa & Word-level, sentence-level, style-level and syntactic-level & NLP backdoors: AddWord, AddSent, StyleBkd\cite{stylebkd}, SynBkd\cite{hidden-killer}\\
        \hline
        Adversarial training\cite{adversarial_training} & Pre-training & White-box & Proactive & DNNs & Nil & Data-poisoning backdoor attacks\\
        \hline
        Vaccine\cite{vaccine} & Pre-training & White-box & Proactive & Llama2-7B, Opt-3.7B, Vicuna-7B & Nil & Fine-tuning-based backdoor\\
        \hline
        \cite{test-time-black-box} & Post-training & Black-box & Reactive & Llama2-7b & Lexical, sentence, style, syntactic-level & Badnets\cite{badnet},addSent\cite{sentence-level}, StyleBkd\cite{stylebkd}, SynBkd\cite{hidden-backdoor}\\
        \hline
        Chain-of-Scrutiny\cite{cos} & Post-training & Black-box & Reactive (mitigation) & GPT-3.5, GPT-4, Gemini-1.0-pro, Llama3 & Token-level & LLM backdoor attacks\\
        \hline
        MDP\cite{MDP}& Pre-training & Black-box & Reactive (detection) & RoBERTa-large & Word-level, sentence-level & Badnets\cite{badnet}, AddSent\cite{sentence-level},EP\cite{EP}, LWP\cite{layerwise-weight-poisoning}, SOS\cite{SOS}\\
        \hline
        DCD\cite{dcd} & Pre-training & Black-box & Reactive (mitigation) & Mistral-7B, Llama3-8B & Token-level, word-level, multi-turn distributed trigger & POISONSHARE\\
        \hline
        PSIM\cite{psim} & Pre-training & White-box & Reactive (detection) & RoBERTa, LLaMA & word-level, sentence-level, syntax-level & Weight-poisoning attacks: BadNet\cite{badnet}, Insent\cite{sentence-level}, SynBkd\cite{hidden-killer}\\
        \hline
%        NAD\cite{nad} & Post-training & White-box & Reactive (mitigation) & DNN & Trigger pattern (on image) & Badnet\cite{badnet}, Trojan arracks, Blend attacks, Clean-label attacks, sinusoidal signal attack, Reflection attack\\
%        \hline
%        CleanCLIP\cite{cleanclip} & Pre-training & White-box & Reactive (mitigation) & ResNet-50 & multimodal trigger & Badnet\cite{badnet}, Blended\cite{blended}, WaNet\cite{wanet}\\
%        \hline
%        ABL\cite{ABL} & Pre-training & White-box & Proactive & WideResNet-16-1 & image trigger & Badnet\cite{badnet}, Blend\cite{blended}, dynamic attack, clean-label attacks, feature-space attacks\\
%        \hline
        Fine-mixing\cite{fine-mixing} & Pre-training & White-box & Reactive (mitigation) & BERT & word-level, sentence-level & Badnet\cite{badnet}, Embedding poisoning\cite{EP}\\
        \hline
        Fine-pruning\cite{fine-pruning} & Pre-training & White-box & Reactive (mitigation) & DNNs & Noise trigger, image trigger & Face, speech and traffic sign recognition backdoor attacks\\
        \hline
%        Neural Cleanse\cite{neural-cleanse} & Pre-training & White-box & Reactive (detection) & Data & Data & Data\\
%        \hline
        Moderate fitting\cite{moderatefitting} & Pre-training & White-box & Proactive & RoBERTaBASE & Word-level, syntactic-level & AddSent\cite{sentence-level}, Style Transfer backdoor\cite{stylebkd}\\
        \hline
%        ANP\cite{ANP} & Pre-training & Gray-box & Reactive (mitigation) & ResNet-18 models & Nil & Badnet\cite{badnet}, blend\cite{blended}, IAB\cite{IAB}, clean-label backdoor\cite{CLB}, SIG\cite{SIG} \\
%        \hline
        LMSanitator\cite{LMSanitator} & Post-training & Black-box & Reactive (detection) & BERT, RoBERTa & Word-level & BToP\cite{BToP}, NeuBA\cite{neuba}, POR\cite{POR} \\
        \hline
        Obliviate\cite{obliviate} & Pre-training & Black-box & Proactive & BERT, RoBERTa & Word-level & POR\cite{POR}, NeuBA\cite{neuba}, BadPre\cite{badpre}, UOR\cite{UOR}\\
        \hline
        MuScleLoRA\cite{downscaling-frequency} & Pre-training & White-box & Reactive (mitigation) & BERT, RoBERTa, GPT2-XL, LlaMA-2 & Word-level, sentence-level, syntax-level, style-level & Badnets\cite{badnet}, AddSent\cite{sentence-level}, Hidden Killer\cite{hidden-killer}, StyleBkd\cite{stylebkd}\\
        \hline
        NCL\cite{ncl}& Pre-training & White-box & Reactive (mitigation) & BERT & word-level, sentence-level, feature-level & InSent\cite{sentence-level}, BadNL\cite{badnl}, StyleBkd\cite{stylebkd},SynBkd\cite{hidden-killer}\\
        \hline
        \cite{D-NLG} & Pre-training & White-box & Reactive (detection \& mitigation) & NLG models & Word-level, syntactic-level, multi-turn & Backdoor attacks against NLG systems: One-to-one (machine translation) \& one-to-many (dialogue generation) backdoor\\
        \hline
%        DeepInspect\cite{deepinspect} & Post-training & Black-box & Reactive (detection) & NLG models & One-to-one (machine translation) \& one-to-many (dialogue generation) backdoor & Backdoor attacks against NLG systems\\
%        \hline
    \end{tabularx}
    \vspace{-0.1in}
    \caption{An overview of backdoor defenses for LLMs.}
    \vspace{-0.25in}
    \label{tab:defenses-details}
\end{table*}



\subsection{Pre-training Defenses} \label{defense-pre}

In this section, we list some benchmark proactive and reactive defense frameworks in the pre-training phase. Defenders are presumed to have white-box access to model training. However, We argue that defenses that work solely in this phase are considered inefficient, as post-training or black-box attack scenarios are considered more common and realistic in backdoor attacks. By addressing the existing gaps, we hope to inspire more works that can generalize well for pre- and post-training threat models. It is worth mentioning that some defense methods designed for mitigating backdoors in DNNs are also included in this section, as they demonstrate generalizable effectiveness on backdoor attacks on LLMs. 

\subsubsection{Safety Training \& Proactive Measures}\label{pre-pro}
% \hfill\\
% \hfill\\
Proactive defenses are implemented during the model construction stage, and the initiative is to endow the model with robustness against potential backdoors that occur in the later stage. 

\textsf{\small Adversarial training}\cite{adversarial_training} is a proactive safety training technique that enhances the model's robustness by training them on augmented training data containing adversarial examples. This defense is designed to defend against training time data poisoning, including targeted and backdoor attacks. However, this defense has been shown to be vulnerable to the clean-label poisoning attack \textsf{\small EntF}\cite{EntF}, which entangles the features of training samples from different classes, causing samples to have no contribution to the model training, including adversarial training and thus effectively invalidating adversarial training efficacy and degrading the model performance. Moreover, Anthropic's recent study\cite{sleeper-agent} has revealed that their threat model is resilient to safety training, backdoors can be persistent through existing safety training from supervised fine-tuning (SFT)\cite{instruction-tuning-zeroshot}, reinforcement-learning fine-tuning (RLFT)\cite{RLFT} to adversarial training\cite{red-teaming}. Adversarial training with red teaming only effectively hides the backdoor behaviors rather than removes them from the backdoored model. \textsf{\small Honeypot}\cite{honeypots} develops a proactive backdoor-resistant tuning process to acquire a clean PLM, specifically, by integrating a honeypot module into the PLM, it helps mitigate the effects of poisoned fine-tuning samples no matter whether they are present or not. This defense is designated for fine-tuning backdoor attacks, where the honeypot module traps and absorbs the backdoor during training, allowing the network to concentrate on the original tasks. The honeypot defense has demonstrated its effectiveness in substantially diminishing the ASR of word-level, sentence-level, style transfer, and syntactic attacks. \textsf{\small Vaccine}\cite{vaccine} proposes a proactive perturbation-aware alignment to mitigate possible harmful fine-tuning, the core idea is to introduce crafted perturbations in embeddings during alignment, enabling the embeddings to withstand adversarial perturbations in later fine-tuning phases. Zhu et al.\cite{moderatefitting} propose restricting PLMs' adaption to the moderate-fitting stage to defend against backdoors. Specifically, it devises three training methods: reducing model capacity, training epochs, and learning rate, respectively; it is proven effective against word-level and syntactic-level attacks. \textsf{\small Anti-backdoor learning (ABL)}\cite{ABL} proposes training backdoor-free models on real-world datasets, the two-stage mechanism first employs local gradient ascent loss (LGA) to separate backdoor examples from clean training samples, then uses global gradient ascent (GGA) to unlearn the backdoored model using the isolated backdoor.
\looseness=-1


%With full access to the clean dataset, the approach mounts two variants: EntF-push and EntF-pull. Specifically, in EntF-push, training samples in each original class are pushed away from their corresponding class centroids $\mu_y$. In EntF-pull, each training sample is pulled towards the centroid of its nearest class y$\prime$. 

\subsubsection{Detection \& Filtering} \label{pre-re}

% \hfill\\
% \hfill\\
\textsf{\small Backdoor Keyword Identification (BKI)} \cite{BKI} is a detection defense that aims to remove possible poisoned training data and directly obstruct backdoor training. This approach devises scoring functions to locate frequent salient words in the trigger sentences that help to filter out poisoned data and sanitize the training dataset, it involves inspecting all the training data to identify possible trigger words. 
%
\textsf{\small Simulate and Eliminate (SANDE)}\cite{SANDE} integrates Overwrite Supervised Fine-tuning (OSFT) to its two-phase framework (simulation and elimination) to remove unknown backdoors. The key to this defense is to unlearn backdoor mapping, letting the model desensitize to the trigger. Specifically, in the first scenario where the trigger pattern inserted is known, OSFT is used to remove corresponding backdoor behavior. In the second scenario, where information about the trigger pattern is unknown, parrot prompts are optimized and leveraged to simulate the trigger's behaviors in the simulation phase, followed up in the elimination phase, OSFT is reused on the parrot prompt to remove victim models' inherent backdoor mappings form trigger \textit{t} to malicious response \textit{$R_t$}. Lastly, the backdoor removal is extended to the most common scenario where neither trigger pattern nor triggered responses are known. 

Moreover, \textsf{\small BEEAR}\cite{beear} is another reactive mitigation defense method for removing backdoors in instruction-tuned language models. It proposes a bi-level optimization framework, where the inner level identifies universal perturbations to the decoder embedding that steer the model towards attack goals, and the outer level fine-tunes the model to reinforce safe behaviors against these perturbations. \textsf{\small Poisoned Sample Identification Module (PSIM)}\cite{psim} leverages PEFT to identify poisoned samples and defend against weight poisoning backdoor attacks. Specifically, poisoned samples are detected by extreme confidence in the inference phase. 
%
\textsf{\small MDP}\cite{MDP} is another detection-based method to defend PLMs against backdoor attacks. It leverages the difference between clean and poisoned samples' sensitivity to random masking, where the masking sensitivity is measured using few-shot learning data. Sun et al.\cite{D-NLG} propose a defense for backdoor attacks in NLG systems that combines detection and mitigation methods. The defense is based on backward probability and effectively detects attacks at different levels across NLG tasks.

\subsubsection{Model Reconstruction \& Repairment}\label{pre-pruning}
% \hfill\\
% \hfill\\
Fine-tuning the backdoored model on clean data for extra epochs\cite{fine-tuning-defense} is considered an effective model repairment technique to overcome perturbations introduced by poisoning data. \textsf{\small Adversarial Neuron Pruning (ANP)}\cite{ANP} eliminates dormant backdoored weights introduced during the initial training phase to mitigate backdoors. Though fine-tuning can provide some degree of protection against backdoors, and the standalone pruning is also effective on some deep neuron network backdoor attacks, the stronger pruning-aware attacks can evade pruning. Pruning is therefore advanced to \textsf{\small fine-pruning}\cite{fine-pruning}, which combines fine-tuning\cite{fine-tuning-defense} and pruning\cite{pruning} to mitigate backdoor, fine-pruning aims to disable backdoor by removing neurons that are not primarily activated on clean inputs, followed by performing several rounds of fine-tuning with clean data. 

\textsf{\small Fine-mixing}\cite{fine-mixing} leverages the clean pre-trained weights to mitigate backdoors from fine-tuned models, the two-step fine-mixing technique first mixes backdoored weights with clean weights, then fine-tunes the mixed weights on clean data, complementary with the Embedding Purification (E-PUR) technique that mitigates potential backdoors in the word embeddings, making this defense especially effective against embedding poisoning-based backdoor attacks. 
%
\textsf{\small CleanCLIP}\cite{cleanclip} is a fine-tuning framework that mitigates data poisoning attacks in multimodal contrastive learning. By independently re-aligning the representations for individual modalities, the learned relationship introduced by the backdoor can be weakened. Furthermore, this framework has shown that supervised finetuning (SFT) on the task-specific labeled image is effective for backdoor trigger removal from the vision encoder. \textsf{\small ShapPruning}\cite{shapPruning} is another pruning approach. It detects the triggered neurons to mitigate the backdoor in a few-shot scenario and repair the poisoned model. 

\textsf{\small Trap and Replace (T\&R)}\cite{trap-and-replace} is similar to the aforementioned pruning-based methods, which also aims to remove backdoored neurons. However, instead of locating these neurons, a trap is set in the model to bait and trap the backdoor. Wu et al. propose an approach called \textsf{\small Multi-Scale Low-Rank Adaptation (MuScleLoRA)}\cite{downscaling-frequency} to acquire a clean language model from poisoned datasets by downscaling frequency space. Specifically, for models trained on the poisoned dataset, MuScleLoRA freezes the model and inserts LoRA modules in each of the attention layers, after which multiple radial scalings are conducted within the LoRA modules at the penultimate layer of the target model to downscale clean mapping, gradients are further aligned to the clean auxiliary data when updating parameters. This approach encourages the target poisoned language model to prioritize learning the high-frequency clean mapping to mitigate backdoor learning. 
%
Zhai et al.\cite{ncl} propose a Noise-augmented Contrastive Learning (NCL) framework to defend against textual backdoor attacks by training a clean model from poisonous data. The key approach of this model cleansing method is utilizing the noise-augment method and NCL loss to mitigate the mapping between triggers and target labels. \textsf{\small Obliviate}\cite{obliviate} proposes a defense method to neutralize task-agnostic backdoors, which can especially be integrated into the PEFT process. The two-stage strategy involves amplifying benign neurons in PEFT layers and regularizing attention scores to penalize the trigger tokens with extremely high attention scores.


\subsubsection{Distillation-based Defenses}\label{pre-dist}
% \hfill\\
% \hfill\\
Knowledge distillation is a method for transferring knowledge between models, enabling a lightweight student model to acquire the capabilities of a more powerful teacher model. Previous research\cite{distillation} has proven defensive distillation is one of the most promising defenses that defend neural networks against adversarial examples. Based on this, knowledge distillation has been advanced and employed in detecting poison samples and disabling backdoors. \textsf{\small Anti-Backdoor Model}\cite{ABM} introduces a non-invasive backdoor against backdoor (NBAB) algorithm that does not require reconstruction of the backdoored model. Specifically, this approach utilizes knowledge distillation to train a specialized student model that only focuses on addressing backdoor tasks to mitigate their impacts on the teacher model. Bie et al. \cite{ssl} present a backdoor elimination defense for pre-trained encoders utilizing self-supervised knowledge distillation, where both contrastive and non-contrastive self-supervised learning (SSL) methods are incorporated. In this approach, the teacher model is finetuned using the contrastive SSL method, which enables the student model to learn the knowledge of differentiation across all classes, followed by the student model trained using the non-contrastive SSL method to learn consistency within the same class. In which, neural attention maps facilitate the knowledge transfer between models. However, anti-distillation backdoor attacks \cite{ATBA} have exploited knowledge distillation to transfer backdoors between models.

\subsubsection{Other Pre-training Defenses}\label{pre-others-d}
% \hfill\\
% \hfill\\
\textsf{\small Decoupling}\cite{decoupling} focuses on defending poisoning-based backdoor attacks on DNNs, it prevents the model from predicting poisoned samples as target labels. The original end-to-end training process is decoupled into three stages. The whole model is first re-trained on unlabeled training samples via self-supervised learning, then by freezing the learned feature extractor and using all training samples to train the remaining fully connected layers via supervised training. Subsequently, high-credible samples are filtered based on training loss. Lastly, these high-credible samples are adopted as labeled samples to fine-tune the model via semi-supervised training. 
%
\textsf{\small I-BAU}\cite{I-BAU} is a defense involves model reconstruction. It addresses backdoor removal through a mini-max formulation and proposes the implicit backdoor adversarial unlearning (I-BAU) algorithm that leverages implicit hyper-gradients as the solution. Specifically, the formulation consists of the inner maximization problem and outer minimization problem, where the inner maximization problem aims to find the trigger that maximizes prediction loss, and the outer minimization problem aims to find parameters that minimize the adversarial loss from the inner attack.
\looseness=-1

In addition, \textsf{\small FABE}\cite{FABE} presents a front-door adjustment defense for LLMs backdoor elimination based on casual reasoning. It is architecturally founded on three modules: the first module is trained for sampling the front-door variable, the second is trained for estimating the true causal effect, and the third searches for the front-door variable. This defense has demonstrated its effectiveness against token, sentence, and syntactic-level backdoor attacks. \textsf{\small Decayed Contrastive Decoding}\cite{dcd} first proposes a black-box multi-turn distributed trigger attack framework called POISONSHARE, which employs a multi-turn greedy coordinate gradient descent to find the optimal trigger, then presents the Decayed Contrastive Decoding defense to mitigate such distributed backdoor attacks. Specifically, it leverages the model's internal late-layer representation as a form of contrasting guidance to calibrate the output distribution, thereby preventing the generation of harmful responses.
\looseness=-1

%In attacks exploiting contrastive learning\cite{pa-cl,poisonedencoder}, early stopping is an effective defense to downplay the impact of poisoning during pre-training of an encoder. 

\begin{tcolorbox}[title=Takeaways. IV.A, breakable]
Backdoor defenses deployed in the pre-training phase can be categorized into reactive defenses and proactive defenses, reactive defenses involve detection and mitigation after the occurrence of poisoned examples or the known existence of a backdoor, in which detection-based defenses in this phase include filtering the training instances and mitigation-based defenses involve alleviating the harmful effects brought by backdoor attacks, model repairment via tuning and pruning (\cite{fine-pruning,shapPruning,fine-mixing,fine-tuning-defense,ncl,downscaling-frequency,cleanclip}) is one of the prevalent approaches. While proactive defenses like\cite{adversarial_training,honeypots,vaccine,ABL} serve preventive purposes, which aim to endow the model with robustness against potential backdoors. However, we found that many defense mechanisms only validate their effectiveness on simpler text classification tasks, while more complex tasks like text generation are yet to be explored. Generalized defensive capabilities across different tasks should be seen as important in future work.
\end{tcolorbox}



\subsection{Post-training Defenses} \label{defense-post}
In the context of inference time defenses, no access to the training process of the model is required, nor any prior knowledge about the attacker and trigger, making them more realistic and efficient defense approaches in a black-box setting.
\looseness=-1

\subsubsection{Detection \& Filtering} \label{post-detection}
% \hfill\\
% \hfill\\
Input detection is an effective way to identify and prevent the trigger-embedded inputs to defend against backdoor attacks, the detection could be either based on perplexity or perturbations. 
%
\textsf{\small ONION}\cite{onion} is a simple filtering-based defense designated for textual backdoor situations, it requires no access to the model's training process and works in both pre-training and post-training stages. It is devised for detecting and removing tokens that reduce the fluency of an input sentence and are likely backdoor triggers, these outlier words are identified by the perplexity (PPL) score obtained from GPT-2 and the pre-defined threshold for suspicious score. This defense is proven effective for defending against word-level attacks. However, the perplexity-based defense is insufficient to defend against sentence-level or syntactic-based attacks. 

\textsf{\small STRIP-ViTA.} \cite{STRIP-vita} A test-time detection defense framework that detects poisoned inputs with stable predictions under perturbation. \textsf{\small STRIP-ViTA} defense method is based on the previous work \textsf{\small STRIP}\cite{STRIP} which works solely on computer vision tasks, it is advanced to work on audio, video, and textual tasks. Its methodology includes substituting the most significant words in the inputs and examining the resulting prediction entropy distributions. \textsf{\small Robustness-Aware Perturbations (RAP)}\cite{RAP} leverages the difference between the robustness of benign and poisoned inputs to perturbations and injects crafted perturbations into the given samples to detect poisoned samples. \textsf{\small BDMMT}\cite{bdmmt} detects backdoored inputs for language models through model mutation test, it has demonstrated effectiveness in defending against character-level, word-level, sentence-level, and style-level backdoor attacks. 
%
\textsf{\small Februus}\cite{Februus} and \textsf{\small SentiNet}\cite{sentinet} operate as run-time Trojan anomaly detection methods for DNNs without requiring model retraining, they sanitize and restore inputs by removing the potential trigger applied on them. Activation Clustering\cite{activation-clustering} detects and removes poisonous data by analyzing activations of the model's last hidden layer. \textsf{\small CLEANGEN}\cite{cleangen} is a lightweight and effective decoding strategy in the post-training phase that mitigates backdoor attacks for generation tasks in LLMs. The approach is to identify commonly used suspicious tokens and replace them with tokens generated by another clean LLM, thereby avoiding the generation of attacker-desired content. 
\looseness=-1

Mo et al.\cite{test-time-black-box} design a test-time defense against black-box backdoor attacks that leverages few-shot demonstrations to correct the inference behavior of poisoned models. \textsf{\small ParaFuzz}\cite{parafuzz} proposes a test-time interpretability-driven poisoned sample detection technique for NLP models. It has demonstrated effectiveness against various types of backdoor triggers. \textsf{\small Chain-of-Scrutiny}\cite{cos} is another test-time detection defense for backdoor-compromised LLM. It only requires black-box access to the model. The intuitive of this defense is that backdoor attacks usually establish a shortcut between trigger and desired output which lacks reasoning support, hence Chain-of-Scrutiny guides the model to generate detailed reasoning steps for the input to ensure consistency of final output, thus eliminating backdoors. 
%
\textsf{\small BDDR}\cite{BDDR} defends against training data poisoning by analyzing whether input words change the discriminative results of the model. The output probability-based defense uses two methods to eliminate textual backdoors: either deleting them upon detection (DD) or replacing them with words generated by BERT (DR). \textsf{\small LMSanitator}\cite{LMSanitator} aims to detect and remove task-agnostic backdoors in prompt-tuning from Transformer-based models. The defense method erases triggers from poisoned inputs during the inference phase. 

\subsubsection{Model Inspections} \label{post-inspection}
% \hfill\\
% \hfill\\
\textsf{\small Neural Cleanse (NC)}\cite{neural-cleanse} is an optimization-based detection and reconstruction system for DNN backdoor attacks during the model inspection stage to filter test-time input. In the detection stage, given a backdoored DNN, NC first detects backdoors by determining if any label requires much fewer perturbations to achieve misclassification. Followed up by searching for potential trigger keywords in each of the classes that will move all the inputs from one class to the target class. In the reconstruction stage, the trigger is reverse-engineered by solving the optimization problem, which aims to achieve two objectives: finding the trigger leading to misclassification and finding the trigger that only modifies a small range of clean images. While \textsf{\small NC}\cite{neural-cleanse} relies on a clean training dataset which limits its application scenarios, \textsf{\small DeepInspect (DI})\cite{deepinspect}, another black-box Trojan detection framework through model inspection, requires minimal prior knowledge about the backdoored model, it first employs model inversion to obtain a substitution training dataset and reconstructs triggers using a conditional GAN, followed by anomaly detection based on statistical hypothesis testing. \textsf{\small Artificial Brain Stimulation (ABS)}\cite{ABS} is another analysis-based backdoor detection approach, it scans an AI model and identifies backdoors by conducting a simulation analysis on inner neurons, followed by reverse engineering triggers using the results from stimulation analysis.


\subsubsection{Distillation-based Defenses}\label{post-dist}
% \hfill\\
% \hfill\\
Model distillation\cite{model-distillation} is another post-training defense against poisoning attacks. Via transferring knowledge from a large model to a smaller one, it aims to create a more robust and clean representation of underlying data to mitigate adversarial effects on backdoored pre-trained encoders. \textsf{\small Neural Attention Distillation (NAD)}\cite{nad} is a distillation-guided fine-tuning approach that erases the backdoor from DNNs, it utilizes a teacher model to guide the fine-tuning of backdoored student model on clean data, to align its intermediate layer attention with the teacher model.


\begin{tcolorbox}[title=Takeaways. IV.B]
After deployment of the backdoored model, defenses in the post-training stage are considered reactive measures, the outlier detection-based methods including\cite{onion,RAP,BDDR,STRIP-vita} are most frequently used as baseline defenses in various backdoor attacks. However, we argue that the filtering methods that solely work in the inference phase are not considered effective and generalizable. Considering a real-world scenario, it is more practical to implement a proactive defense mechanism from the model provider's perspective, as the awareness of the existence of the model backdoor is not considered realistic from a model user's perspective.  
\end{tcolorbox}


\section{Evaluation Methodology }

\subsection{Performance Metrics}

In this section, we introduce the performance metrics commonly employed to assess the effectiveness of backdoor attacks in achieving their dual objectives: efficacy and stealthiness. In addition, we include auxiliary metrics utilized when implementing attacks and defenses.

\begin{table*}[ht]
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{18pt}
    \begin{tabular}{|c|c|c|}
        \hline
        \headcol \textcolor{white}{\textbf{Dataset}} & \textcolor{white}{\textbf{Size}} & \textcolor{white}{\textbf{Description \& Usage}} \\
        \hline  
        SST-2\cite{sst-2} & 12K & Movie reviews for single-sentence sentiment classification \\\hline
        HateSpeech (HS)\cite{HS} & 10K & Hate speeches for single-sentence binary classification (HATE/NOHATE)\\\hline
        AGNews\cite{agnews} & 128K & News topics for single-sentence sentiment classification \\\hline
        IMDB\cite{imdb} & 50K & Movie reviews for single-sentence sentiment classification \\\hline
        Ultrachat-200k\cite{ultrachat-200k} & 1.5M & High-quality multi-turn dialogues for multi-turn instruction tuning \\\hline
        AdvBench\cite{GCG} & 500 & Questions covering prohibited topics for safety evaluation \\\hline
        TDC 2023 & 50 & Instructions representative of undesirable behaviors for safety evaluation\\\hline
        ToxiGen\cite{toxigen} & 274K & Machine-generated implicit hate speech dataset for hate speech detection\\\hline
        Bot Adversarial Dialogue\cite{BAD}& 70K & Multi-turn dialogues between human and bot to trigger toxic responses generation \\\hline
        Alpacaeval\cite{li2023alpacaeval} & 20K & Instruction-label pairs for evaluating instruction-following language models \\\hline
    \end{tabular}
    \caption{Frequently used evaluation datasets.}
    \vspace{-0.2in}
    \label{datasets}
\end{table*}

\subsubsection{Main Metrics}

\textbf{Attack Success Rate (ASR).} The classification accuracy of the backdoored model on poisoned data is a key indicator metric for evaluating the performance of backdoor attacks. In contrast, the drop in ASR can be used to measure the effectiveness of defense methods~\cite{ni2023eavesdropping, zhao2020accuracy, ni2023uncovering, zhao2022periscope, zhao2020automatic, ni2023recovering, gu2017pt, zhao2019geo, omar2022robust}. The ASR can be expressed as: 

% A large ASR indicates better backdoor attack performance, the other way indicates better defense performance.

% \begin{center}
\begin{equation}
\centering
      \label{eqn1}
   \text{ASR} = \frac{\text{\# Successfully Attacked Cases}}{\text{\# Total Cases}} \times 100\%
\end{equation}
% \end{center}

%\hfill\\
%\hfill\\
\noindent \textbf{Clean Accuracy (CA or CACC).} The clean performance shares equal importance with attack performance in backdoor attacks since one of the objectives of the attack design is to maintain the overall model integrity~\cite{ni2021explore}. Clean accuracy measures how the backdoored model performs on the unpoisoned dataset to determine whether the model's overall performance is degraded. A larger CA indicates better utility preservation. CA is also referred to as ``Benign Accuracy (BA)''. 

\begin{equation}
\centering
\small
      \label{eqn4}
   \text{CA} = \frac{\text{\# Clean Examples Correctly Classified}}{\text{\# Total Clean Examples}} \times 100\% 
\end{equation}


\textbf{Area under the ROC Curve (AUC).} AUC serves as an aggregate measure of performance across all possible thresholds, especially for classification tasks, which is a useful metric for evaluating the stealthiness of the attack.

\textbf{Performance Drop Rate (PDR).} PDR is used to quantify the effectiveness of an attack and its capability of preserving model functionality. It is obtained by measuring how poisoned samples affect the model performance compared to benign ones. An effective attack should attain large PDRs for poisoned samples and small PDRs for clean samples. PDR is defined as: 
\begin{center}
\begin{equation}
      \label{eqn2}
   \text{PDR} = (1 - \frac{\text{Acc\textsubscript{poisoned}}}{\text{Acc\textsubscript{clean}}}) \times 100\%
\end{equation}
\end{center}

where the {Acc\textsubscript{poisoned}} refers to accuracy when the model is tuned on poisoned data and {Acc\textsubscript{clean}} refers to accuracy when the model is tuned on clean data. 

% \hfill\\
% \hfill\\
\textbf{Label Flip Rate (LFR).} LFR can be used to evaluate attack efficacy. It is defined as the proportion of misclassified samples:
\begin{center}
\begin{equation}
      \label{eqn3}
   \text{LFR} = \frac{\text{\#+ve Samples Classified as -ve}}{\text{\#+ve Samples}} \times 100\%
\end{equation}
\end{center}

% \begin{table*}[ht]
%     \centering
%     \scriptsize
%     \renewcommand{\arraystretch}{1.0}
%     \setlength{\tabcolsep}{18pt}
%     \begin{tabular}{|c|c|c|}
%         \hline
%         \headcol \textcolor{white}{\textbf{Dataset}} & \textcolor{white}{\textbf{Size}} & \textcolor{white}{\textbf{Description \& Usage}} \\
%         \hline  
%         SST-2\cite{sst-2} & 12K & Movie reviews for single-sentence sentiment classification \\\hline
%         HateSpeech (HS)\cite{HS} & 10K & Hate speeches for single-sentence binary classification (HATE/NOHATE)\\\hline
%         AGNews\cite{agnews} & 128K & News topics for single-sentence sentiment classification \\\hline
%         IMDB\cite{imdb} & 50K & Movie reviews for single-sentence sentiment classification \\\hline
%         Ultrachat-200k\cite{ultrachat-200k} & 1.5M & High-quality multi-turn dialogues for multi-turn instruction tuning \\\hline
%         AdvBench\cite{GCG} & 500 & Questions covering prohibited topics for safety evaluation \\\hline
%         TDC 2023 & 50 & Instructions representative of undesirable behaviors for safety evaluation\\\hline
%         ToxiGen\cite{toxigen} & 274K & Machine-generated implicit hate speech dataset for hate speech detection\\\hline
%         Bot Adversarial Dialogue\cite{BAD}& 70K & Multi-turn dialogues between human and bot to trigger toxic responses generation \\\hline
%         Alpacaeval\cite{li2023alpacaeval} & 20K & Instruction-label pairs for evaluating instruction-following language models \\\hline
%     \end{tabular}
%     \caption{Frequently used evaluation datasets.}
%     \vspace{-0.2in}
%     \label{datasets}
% \end{table*}

\subsubsection{Auxiliary Metrics}
% \hfill\\
% \hfill\\
Perplexity\cite{perplexity} measures the readability and fluency of text samples using the language model. A lower perplexity score indicates that the sample is more fluent and predictable by the model, while a higher perplexity indicates that the model is less certain about the sample, making it more likely to be identified as the backdoor trigger and filtered by perplexity-based backdoor defenses such as \textsf{\small ONION}\cite{onion}. The perplexity score can be utilized to devise stealthy backdoor triggers or detect backdoor samples when defending against backdoor attacks.



\textbf{BLEU\&ROUGH.} Two frequently used metrics in NLP evaluation now has been extended to evaluate model performance in triggerless scenarios under backdoor attacks. BLEU\cite{bleu} is primarily based on precision, it measures the accuracy of benign examples; ROUGE\cite{rouge} which is primarily based on recall, evaluates response quality in the absence of triggers. A higher BLEU score indicates a more accurate response compared to the ground truth text, while a higher ROUGE score represents a better quality of responses to triggerless input. 

% \begin{table*}[t]
%     \centering
%     \scriptsize
%     \renewcommand{\arraystretch}{1.0}
%     \setlength{\tabcolsep}{25pt}
%     \begin{tabular}{|c|c|c|}
%         \hline
%         \headcol \textcolor{white}{\textbf{Dataset}} & \textcolor{white}{\textbf{Size}} & \textcolor{white}{\textbf{Description \& Usage}} \\
%         \hline  
%         SST-2\cite{sst-2} & 12K & Movie reviews for single-sentence sentiment classification \\\hline
%         HateSpeech (HS)\cite{HS} & 10K & Hate speeches for single-sentence binary classification (HATE/NOHATE)\\\hline
%         AGNews\cite{agnews} & 128K & News topics for single-sentence sentiment classification \\\hline
%         IMDB\cite{imdb} & 50K & Movie reviews for single-sentence sentiment classification \\\hline
%         Ultrachat-200k\cite{ultrachat-200k} & 1.5M & High-quality multi-turn dialogues for multi-turn instruction tuning \\\hline
%         AdvBench\cite{GCG} & 500 & Questions covering prohibited topics for safety evaluation \\\hline
%         TDC 2023 & 50 & Instructions representative of undesirable behaviors for safety evaluation\\\hline
%         ToxiGen\cite{toxigen} & 274K & Machine-generated implicit hate speech dataset for hate speech detection\\\hline
%         Bot Adversarial Dialogue\cite{BAD}& 70K & Multi-turn dialogues between human and bot to trigger toxic responses generation \\\hline
%         Alpacaeval\cite{li2023alpacaeval} & 20K & Instruction-label pairs for evaluating instruction-following language models \\\hline
%     \end{tabular}
%     \caption{Frequently used evaluation datasets.}
%     \label{datasets}
% \end{table*}

\textbf{Exact Match (EM) \& Contain.} Two metrics for evaluating NLP tasks, such as answering questions and generating texts. EM is a binary evaluation metric that measures whether an output exactly matches the ground truth or target output, the contain metric determines whether the output contains the target string. 



\subsection{Baselines, Benchmarks, and Datasets}
Besides directly evaluating attack and defense performance using the metrics mentioned above and comparing them with representative baseline attacks and defenses, their efficacy and, especially, robustness can also be evaluated through their performance when applying state-of-the-art defense methods. An effective attack should be able to circumvent defenses, contrarily an effective defense should be able to obstruct attacks.
As detailed in section IV.B, \textsf{\small ONION}\cite{onion}, \textsf{\small STRIP-ViTA}\cite{STRIP-vita} and \textsf{\small RAP}\cite{RAP} are three of the most representative test-time defenses utilized in mitigating LLM backdoor attacks, they share the similar defense technique of preventive input filtering. 

Li et al.\cite{backdoorllm} provide a comprehensive threat model benchmark for backdoor instruction-tuned LLMs. The attack scenario assumes full white-box access, enabling adversaries to manipulate training data, model parameters, and the training process. Specifically, the framework encompasses four distinct attack strategies: data poisoning\cite{VPI,instructions-as-backdoor,jailbreak-backdoors}, weight poisoning\cite{badedit}, hidden state manipulation, and chain-of-thought attacks\cite{badchain}. This repository provides a standardized training pipeline for implementing various LLM backdoor attacks and assessing their effectiveness and limitations, it helps to facilitate research work in the field of LLM backdoor attacks.
%
We list some commonly used datasets for implementing or evaluating backdoor attacks (refer to \autoref{datasets}). 
% \begin{table*}[h]
%     \centering
%     \scriptsize
%     \renewcommand{\arraystretch}{1.0}
%     \setlength{\tabcolsep}{25pt}
%     \begin{tabular}{|c|c|c|}
%         \hline
%         \headcol \textcolor{white}{\textbf{Dataset}} & \textcolor{white}{\textbf{Size}} & \textcolor{white}{\textbf{Description \& Usage}} \\
%         \hline  
%         SST-2\cite{sst-2} & 12K & Movie reviews for single-sentence sentiment classification \\\hline
%         HateSpeech (HS)\cite{HS} & 10K & Hate speeches for single-sentence binary classification (HATE/NOHATE)\\\hline
%         AGNews\cite{agnews} & 128K & News topics for single-sentence sentiment classification \\\hline
%         IMDB\cite{imdb} & 50K & Movie reviews for single-sentence sentiment classification \\\hline
%         Ultrachat-200k\cite{ultrachat-200k} & 1.5M & High-quality multi-turn dialogues for multi-turn instruction tuning \\\hline
%         AdvBench\cite{GCG} & 500 & Questions covering prohibited topics for safety evaluation \\\hline
%         TDC 2023 & 50 & Instructions representative of undesirable behaviors for safety evaluation\\\hline
%         ToxiGen\cite{toxigen} & 274K & Machine-generated implicit hate speech dataset for hate speech detection\\\hline
%         Bot Adversarial Dialogue\cite{BAD}& 70K & Multi-turn dialogues between human and bot to trigger toxic responses generation \\\hline
%         Alpacaeval\cite{li2023alpacaeval} & 20K & Instruction-label pairs for evaluating instruction-following language models \\\hline
%     \end{tabular}
%     \caption{Frequently used evaluation datasets.}
%     \label{datasets}
% \end{table*}


\section{Conclusions}
%%\label{}
% In summary, we conduct a comprehensive survey on existing backdoor attacks on LLMs, each categorized by the exploited phase. We also discussed corresponding defense techniques addressing backdoor attacks. By providing a systematic taxonomy on existing works, we hope to inspire future research that contributes towards more robust LLMs.


In conclusion, this work provides a comprehensive survey of existing backdoor attacks targeting large language models (LLMs), systematically categorizing them based on the phase of exploitation. Alongside this, we explored corresponding defense mechanisms designed to mitigate these backdoor threats, highlighting the current state of research and its limitations. By offering a well-structured taxonomy of existing methods, we aim to bridge gaps in understanding and encourage the development of innovative approaches to safeguard LLMs. We hope that this survey serves as a valuable resource for researchers and practitioners, fostering future advancements in creating more secure and trustworthy LLM systems.



% \section*{Acknowledgements}
% ...

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%%\appendix

%%\section{Appendix title 1}
%% \label{}

%%\section{Appendix title 2}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{IEEEtran} 
\bibliography{references}

\thispagestyle{plain}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%%\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% For example:

%% \bibitem[Aladro et al.(2015)]{Aladro15} Aladro, R., Martín, S., Riquelme, D., et al. 2015, \aas, 579, A101


%%\end{thebibliography}
\end{singlespace}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
%% \begin{figure}
	\centering 
	\includegraphics[width=0.4\textwidth, angle=-90]{ASCOM_journal_cover.pdf}	
	\caption{Astronomy \& Computing journal cover} 
	\label{fig_mom0}%
\end{figure}