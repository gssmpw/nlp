\begin{table}[t]
\vspace{-2mm}
\centering
\adjustbox{max width=\columnwidth}{%
\scriptsize
% \begin{tabular}{c@{}c@{}c@{}c@{}c@{}c@{}c@{}c}
\begin{tabular}{cccccccc}
\toprule

\multicolumn{8}{c}{\textbf{Llama2-7B-chat-hf(4k)}} \\

\midrule
\vspace{0.02cm}
\multirow{1}{*}{{\textbf{Methods}}}  
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.PK}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.Num}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.KV}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{En.MC}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Math.F}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Code.Debug}}}
% & \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Average score}}} \\
& \multirow{1}{*}{{\textbf{Average}}} \\
% Llama2-7B-chat-hf
\renewcommand{\arraystretch}{1} % 恢复默认行间距
Max Length & 125k & 125k & 175k & 834k & 120k & 258k & 273k   \\
\midrule
FullKV & 1.36 & 1.86 & 0.4 & 0.44 & 17.43 & 21.57 & 7.18   \\

\arrayrulecolor[gray]{0.8}
\midrule
\arrayrulecolor{black}
Dynamic-PI  & 0.17 & 0.00 & 0.00 & 7.42 & 2.00 & 21.32 & 5.15 \\
NTK-Aware & 2.54 & 0.00 & 0.00 & 3.06 & 7.71 & 18.78 & 5.35 \\
ChunkLlama & 12.88 & 13.22 & 0.20 & 0.87 & 17.14 & 22.08 & 11.07 \\
InfLLM & \textbf{100.00} & 96.61 & 2.40 & 29.80 & 16.86 & 22.34 & 44.67 \\
AttenCalibration-NTK & 0.00 & 0.00 & 0.00 & 1.06 & 5.71 & 19.24 & 4.34  \\ 
Ours & \textbf{100.00} & 97.63 & 20.60 & 33.62 & 19.71 & 25.13 & 49.45 \\
Ours-calibration & \textbf{100.00} & \textbf{98.64} & \textbf{30.20} & \textbf{36.24} & \textbf{19.71} & \textbf{31.47} & \textbf{52.71} \\
Ours-compression &  97.80 & 87.96 & 5.00 & 35.81 & 15.86 & 27.41 & 44.97 \\
Ours-calibration-compression & 97.97 & 90.14 & 10.80 & 35.46 & 15.86 & 28.21 & 46.41 \\

\arrayrulecolor[gray]{0.7}
\midrule
\arrayrulecolor{black}
\multicolumn{8}{c}{\textbf{Llama3-8B-instruct(8k)}} \\

\midrule
\vspace{0.02cm}
\multirow{1}{*}{{\textbf{Methods}}}  
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.PK}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.Num}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.KV}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{En.MC}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Math.F}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Code.Debug}}}
% & \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Average score}}}
& \multirow{1}{*}{{\textbf{Average}}} \\
FullKV & 6.10  & 6.27  & 4.80  & 42.79 & 38.57 & 22.34 & 20.15 \\
% Llama2-7B-chat-hf
\arrayrulecolor[gray]{0.8}
\midrule
\arrayrulecolor{black}
Dynamic-PI  & 0.00  & 0.00  & 0.00  & 28.82 & 29.71 & \textbf{24.62} & 13.86 \\
NTK-Aware & 3.39  & 8.47  & 9.40  & 35.37 & 39.43 & 17.77 & 18.97 \\
ChunkLlama & 3.05  & 9.15  & 3.60  & 13.54 & 34.29 & 11.42 & 12.51 \\
AttenCalibration-NTK & 4.58  & 8.47  & 12.40 & 34.28 & 36.57 & 22.68 & 19.83 \\
InfLLM & \textbf{100.00}  & 99.00 & 5.00 & 43.70 & 23.70 & 22.08 & 48.91 \\
Ours & \textbf{100.00} & \textbf{99.83} & 92.80 & 54.59 & \textbf{40.00} & 22.84 & 68.34 \\
Ours-calibration & \textbf{100.00} & 99.49 & \textbf{93.80} & \textbf{56.77} & \textbf{40.00} & 23.24 & \textbf{68.88} \\
Ours-compression & \textbf{100.00} & \textbf{99.83} & 89.20 & 55.48 & \textbf{40.00} & 21.32 & 67.64 \\
Ours-calibration-compression & \textbf{100.00} & \textbf{99.83} & 91.00 & \textbf{56.77} & \textbf{40.00} & 22.20 & 68.30 \\

\arrayrulecolor[gray]{0.7}
\midrule
\arrayrulecolor{black}
\multicolumn{8}{c}{\textbf{Other proprietary models}} \\

\midrule
\vspace{0.02cm}
\multirow{1}{*}{{\textbf{Models}}}  
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.PK}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.Num}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.KV}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{En.MC}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Math.F}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Code.Debug}}}
% & \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Average score}}}
& \multirow{1}{*}{{\textbf{Average}}} \\
GPT-4 & \textbf{100.00} & \textbf{100.00} & \textbf{89.00} & 67.25 & \textbf{60.00} & \textbf{37.06} & \textbf{75.55} \\
Kimi-Chat              & 98.14         & 95.42         & 53.60          & \textbf{72.49}         & 12.57         & 17.14   & 58.23      \\
Claude-2               & 97.8          & 98.14         & 65.40          & 62.88         & 32.29         & 17.77   & 62.38      \\

\arrayrulecolor[gray]{0.7}
\midrule
\arrayrulecolor{black}
\multicolumn{8}{c}{\textbf{Other open-source  models}} \\

\midrule
\vspace{0.02cm}
\multirow{1}{*}{{\textbf{Models}}}  
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.PK}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.Num}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{R.KV}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{En.MC}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Math.F}}}
& \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Code.Debug}}}
% & \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Average score}}}
& \multirow{1}{*}{{\textbf{Average}}} \\
YaRN-Mistral-7B-128k        & 92.71         & 56.61         & \textless 5            & 27.95         & \textbf{17.14}         & \textbf{60.00}     & 42.82       \\
Yi-6B-200K             & 100           & 94.92         & \textless 5            & 36.68         & \textless 5            & \textless 5     &39.85       \\
Yi-34B-200K            & \textbf{100}           & \textbf{100}           & \textless 5            & \textbf{38.43}         & \textless 5            & 25.71   &\textbf{44.86}      \\
ChatGLM-3-6B-128K      & 92.2          & 80.68         & \textless 5            & 10.48         & \textless 5            & 7.71   &32.68       \\

\bottomrule
\end{tabular}
}
\vspace{-1mm}
\caption{The model's performance on the InfiniteBench dataset across different datasets.}
\label{2_extreme_compression}

\vspace{-5mm}
\end{table}