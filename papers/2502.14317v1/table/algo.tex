\begin{algorithm}[H]
\caption{ParallelComp with Chunk Eviction}
\label{alg:parallelcomp}
Input sequence $X \in \mathbb{R}^{N \times d}$, max chunk size $w$, threshold $\tau$, model layers $L$\\


\textbf{Initialization:} Divide $X$ into $C = \lceil N / w \rceil$ chunks, $X = \{X^1, X^2, \dots, X^C\}$\;\\

For each layer $l \in [1, L]${ \\
    \textbf{Step 1: Compute Parallel Attention for Each Chunk}\;
    \For{each chunk $c \in [1, C]$ \textbf{in parallel}}{
        Compute attention matrix: \\
        $A^{l,c} = \text{Softmax}\left(\frac{f_Q^l(X^c) \cdot f_K^l(X^c)^T}{\sqrt{d_k}}\right)$\;
        Update features: \\
        $F^{l,c} = A^{l,c} \cdot f_V^l(X^c)$\;
        Compute query token self-information: \\
        $I_c = -\log P(q_c \mid F^{l,c})$\;
    }
    \\
    \textbf{Step 2: Chunk Eviction}\; \\
    Select informative chunks: \\
    $\mathcal{S} = \{c \mid I_c \geq \tau\}$\;
    Retain features for selected chunks: \\
    $F^l = \text{Concat}(F^{l,c} \mid c \in \mathcal{S})$\; \\
}
\textbf{Step 3: Final Output}\; \\

$O^L = f_O^L(F^L)$\;  Compute final output: 

Return $O^L$\;
\end{algorithm}

