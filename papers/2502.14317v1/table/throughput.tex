% \begin{table}
% \centering
% \adjustbox{max width=\columnwidth}{%
% \scriptsize
% \begin{tabular}{c c c }
% \toprule


% \textbf{\multirow{2}{*}{Chunk Number}} & \multicolumn{1}{c}{\textbf{Ours}} & \multicolumn{1}{c}{\textbf{Ours-compression}}  \\
% \arrayrulecolor[gray]{0.5}
% % \cmidrule(lr){2-3} 

% \arrayrulecolor{black}
%  % & \textbf{ms/token} & \textbf{max memory used(MB)} & \textbf{ms/token} & \textbf{max memory used(MB)}  \\
%   &  \textbf{Max Memory(MB)} &  \textbf{Max Memory(MB)}  \\
% \midrule
%  1 & 19394 &  16994  \\
% % 2 & 24816 &  18874  \\
% 4 & 35518 &  24734 \\
% 8 & 47758 &  36396 \\
% 12 & 65980 &  48458  \\
% % 13 & 80146 &  51265  \\ 
% % 14 &  Out-of-Memory  & 54399 \\
% 16 &  Out-of-Memory &  59140 \\
% 20 &  Out-of-Memory &  71302 \\
% 23 &  Out-of-Memory &  79742 \\
% 24 &  Out-of-Memory &  Out-of-Memory  \\


% \bottomrule
% \end{tabular}
% }
% \caption{Throughput analysis. We evaluate on Llama2-7B-chat-hf using a single 80G A100 and compare the improvement in chunk throughput with the use of parallel KV cache compression.}
% \label{throughput_analysis}
% \end{table}

\begin{table}[H]
\centering
\adjustbox{max width=\columnwidth}{%
\scriptsize
\begin{tabular}{c c c c c }
\toprule


\textbf{\multirow{2}{*}{Chunk Number}} & \multicolumn{2}{c}{\textbf{Ours}} & \multicolumn{2}{c}{\textbf{Ours-compression}}  \\
\arrayrulecolor[gray]{0.5}
\cmidrule(lr){2-5} 

\arrayrulecolor{black}
 & \textbf{prefill(s) / generation(ms/token)} & \textbf{max memory used(MB)} & \textbf{prefill(s) / generation(ms/token)} & \textbf{max memory used(MB)}  \\
  % &  \textbf{Max Memory(MB)} &  \textbf{Max Memory(MB)}  \\
\midrule
1 & 1317.72 / 24.30 & 19394 & 1317.72 / 22.16 & 16994  \\
% 2 & 24816 &  18874  \\
4 & 321.40 / 43.69 & 35518 & 321.40 / 23.28 & 24734 \\
8 & 160.70 / 72.53 & 47758 &  160.70 / 31.21 &  36396 \\
12 & 111.67 / 102.94 & 65980 &    111.67 / 39.61 & 48458  \\
% 13 & 80146 &  51265  \\ 
% 14 &  Out-of-Memory  & 54399 \\
16 & - &  Out-of-Memory & 82.36 / 49.25& 59140 \\
20 & - & Out-of-Memory & 65.23 / 56.25 &  71302 \\
23 & - & Out-of-Memory & 56.07 / 57.19 &  79742 \\
24 & - & Out-of-Memory & - &  Out-of-Memory  \\


\bottomrule
\end{tabular}
}
\caption{Throughput analysis. We evaluate on Llama2-7B-chat-hf and compare the improvement in chunk throughput with the use of parallel KV cache compression. Time tests were performed on the narrativeqa dataset. }
\vspace{-4mm}
\label{throughput_analysis}
\end{table}

