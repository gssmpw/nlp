\begin{table*}[h]
\vspace{-1mm}
\centering
\adjustbox{max width=\textwidth}{%
\scriptsize
\begin{tabular}{c@{}c@{}c@{}c@{} c@{}c@{}c@{} c@{}c@{}c@{} c@{}c@{}c@{} c@{}c@{} c@{}c@{} c}
\toprule
\multirow{2}{*}{\raisebox{-4ex}{\textbf{Methods}}}  % Moves the text down
& \multicolumn{3}{c}{\textbf{Single-Document QA}} 
& \multicolumn{3}{c}{\textbf{Multi-Document QA}} 
& \multicolumn{3}{c}{\textbf{Summarization}} 
& \multicolumn{3}{c}{\textbf{Few-shot Learning}} 
& \multicolumn{2}{c}{\textbf{Synthetic}} 
& \multicolumn{2}{c}{\textbf{Code}} 
& \multirow{2}{*}{\raisebox{-4ex}{\textbf{Avg.}}} 
% & \multirow{2}{*}{\raisebox{-4ex}{\textbf{ Time}}} 
% & \multirow{2}{*}{\raisebox{-6ex}{\textbf{\shortstack{Time \\ (s / sample)}}}}
\\  % Moves the text down

\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
\setlength{\tabcolsep}{1pt} % 默认值是6pt，减小这个值来减少列间距
& \makebox[1cm]{\raisebox{0.5ex}{\rotatebox{30}{\textbf{NtrvQA}}}} 
& \makebox[1cm]{\raisebox{0.7ex}{\rotatebox{30}{\textbf{Qasper}}}} 
& \makebox[1cm]{\raisebox{0.8ex}{\rotatebox{30}{\textbf{MF-en}}}} 
& \makebox[1cm]{\raisebox{0.4ex}{\rotatebox{30}{\textbf{HotpotQA}}}} 
& \makebox[1cm]{\raisebox{0.3ex}{\rotatebox{30}{\textbf{2WikiMQA}}}} 
& \makebox[1cm]{\raisebox{0.7ex}{\rotatebox{30}{\textbf{Musique}}}} 
& \makebox[1cm]{\raisebox{0.5ex}{\rotatebox{30}{\textbf{GovReport}}}} 
& \makebox[1cm]{\raisebox{0.8ex}{\rotatebox{30}{\textbf{QMSum}}}} 
& \makebox[1cm]{\raisebox{0.6ex}{\rotatebox{30}{\textbf{MultiNews}}}} 
& \makebox[1cm]{\raisebox{0.8ex}{\rotatebox{30}{\textbf{TREC}}}} 
& \makebox[1cm]{\raisebox{0.6ex}{\rotatebox{30}{\textbf{TriviaQA}}}} 
& \makebox[1cm]{\raisebox{0.6ex}{\rotatebox{30}{\textbf{SAMSum}}}} 
& \makebox[1cm]{\raisebox{0.6ex}{\rotatebox{30}{\textbf{PCount}}}}  
& \makebox[1cm]{\raisebox{1.4ex}{\rotatebox{30}{\textbf{PRe}}}}    
& \makebox[1cm]{\raisebox{1.6ex}{\rotatebox{30}{\textbf{Lcc}}}} 
& \makebox[1cm]{\raisebox{1.4ex}{\rotatebox{30}{\textbf{RB-P}}}} \\

% Llama-2-7B-chat-hf
\midrule
Max Length & 84123 & 24204 & 17727 & 20325 & 19001 & 20520  & 60515 & 34477 & 16271 & 13049 & 26756 & 21884 & 32699 & 17158  & 37628 & 58822 & 30657 \\
\midrule
\multicolumn{18}{c}{\textbf{Llama2-7B-chat-hf(4k)}} \\
\arrayrulecolor[gray]{0.8}
\midrule
\arrayrulecolor{black}
% \rowcolor{lightgray} 
FullKV(4k) & 18.62 & 19.53 & 35.49 & 31.07 & 26.15 & 9.91 & 25.52 & 20.87 & 26.28 & 62.00 & 82.68 & \textbf{40.86} & \textbf{5.50} & 10.50 & \textbf{61.04} & 55.30 & 33.21 \\
% \midrule
% \multicolumn{18}{c}{\textbf{Llama-2-7B-chat-hf, KV Size = 384 , Compressibility is 9.38\%  (Except CHAI method)}} \\
% \arrayrulecolor[gray]{0.8}
% \midrule
% \arrayrulecolor{black}
Dynamic-PI & 9.69  & 20.05 & 33.10 & 16.40 & 23.83 & 3.62  & 27.83 & 18.75 & 16.53 & 62.00 & 67.00 & 40.37 & 1.58  & 5.14  & 55.30 & 55.49 & 28.54 \\
NTK-Aware & 13.02 & 14.25 & 31.51 & 29.55 & 30.64 & 11.83 & 28.78 & 16.96 & \textbf{26.30} & 62.50 & 74.88 & 39.35 & 4.08  & 4.50  & 49.74 & 49.39 & 30.46 \\
ChunkLlama & 22.97 & 20.52 & 33.71 & 28.91 & 26.14 & 13.84 & 14.84 & 21.62 & 18.13 & 62.50 & 77.15 & 40.83 & 2.03  & 4.00  & 59.81 & 54.33 & 31.33 \\
InfLLM & 18.14 & \textbf{22.11} & 29.86 & 30.99 & 30.74 & 9.41  & 26.33 & 20.63 & 26.18 & 62.50 & 84.24 & 39.92 & 3.36  & 6.00  & 60.15 & 55.99 & 32.91 \\
AttenCalibration-NTK & 14.05 & 12.49 & 32.52 & 30.61 & 31.22 & 12.84 & \textbf{29.72} & 18.24 & 24.40 & 61.50 & 72.88 & 39.54 & 2.33  & 3.00  & 48.86 & 50.36 & 30.29 \\
Ours & 23.20 & 17.50 & 37.07 & 38.67 & \textbf{32.68} & 20.22 & 25.00 & 22.79 & 25.84 & \textbf{64.00} & 84.63 & 40.67 & 4.00 & 31.50 & 59.37 & 58.53 & 36.60 \\
Ours-calibration & \textbf{24.95} & 19.07 & \textbf{38.16} & 39.53 & 32.62 & \textbf{22.64} & 25.42 & \textbf{22.82} & 26.01 & 63.00 & \textbf{85.41} & 40.36 & 5.00 & \textbf{32.50} & 59.04 & \textbf{58.84} & \textbf{37.21} \\
Ours-compression & 23.32 & 16.97 & 35.25 & 39.49 & 32.47 & 20.17 & 24.33 & 21.97 & 25.68 & 63.50 & 84.46 & 40.81 & 4.00  & 31.50 & 59.43 & 58.54 & 36.37 \\
Ours-calibration-compression & 24.04 & 18.39 & 38.03 & \textbf{39.89} & 35.38 & 22.15 & 24.26 & 22.46 & 24.51 & 63.50 & 84.83 & 40.73 & 4.00 & 31.50 & 57.67 & 58.48 & 36.86 \\
% Llama-2-7B-chat-hf
\midrule
\multicolumn{18}{c}{\textbf{Llama3-8B-instruct(8k)}} \\
\arrayrulecolor[gray]{0.8}
\midrule
\arrayrulecolor{black}
% \rowcolor{lightgray} 
FullKV(8k) & 24.31 & 38.13 & 39.69 & 44.16 & 35.66 & 21.00 & 28.35 & 23.06 & 26.96 & 73.00 & 90.13 & 42.46 & 4.61  & 68.50 & 60.46 & \textbf{56.11} & 42.29 \\
% \midrule
% \multicolumn{18}{c}{\textbf{Llama-2-7B-chat-hf, KV Size = 384 , Compressibility is 9.38\%  (Except CHAI method)}} \\
% \arrayrulecolor[gray]{0.8}
% \midrule
% \arrayrulecolor{black}
Dynamic-PI & 21.71 & 36.66 & 38.24 & 33.70 & 35.48 & 14.28 & 29.41 & 22.04 & 25.55 & 74.50 & 82.61 & \textbf{42.62} & 2.33  & 85.59 & 58.22 & 47.16 & 40.63 \\ 
NTK-Aware & 25.92 & 37.54 & 42.23 & 48.32 & 36.96 & 27.51 & 33.74 & 24.13 & 26.35 & 50.50 & 88.84 & 42.53 & 7.24  & \textbf{95.61} & 34.84 & 39.04 & 41.33  \\
ChunkLlama & 25.01 & 37.39 & 43.52 & 49.37 & 37.56 & 30.95 & 17.57 & 23.51 & 19.72 & 76.00 & 90.38 & 42.14 & 4.71  & 67.95 & \textbf{61.10} & 52.57 & 42.47 \\
InfLLM & 19.93 & \textbf{43.52} & 40.58 & 48.31 & 35.99 & 23.25 & 30.49 & 21.60 & 26.53 & 74.00 & 90.93 & 42.30 & \textbf{8.00}  & 74.00 & 58.98 & 52.46 & 43.18 \\
AttenCalibration-NTK & 26.54 & 37.52 & 41.13 & 47.56 & 38.98 & 26.51 & \textbf{34.21} & 23.35 & 25.64 & 45.50 & 89.23 & 42.21 & 4.81  & 93.51 & 36.86 & 42.82 & 41.02 \\
Ours & \textbf{26.67} & 39.05 & 42.66 & \textbf{49.58} & 40.02 & 33.25 & 29.10 & 24.18 & 26.74 & 69.00 & 91.03 & 42.07 & 7.81  & 92.38 & 58.84 & 53.54 & 45.37 \\
Ours-calibration & 26.05 & 42.32 & 43.81 & 49.32 & \textbf{40.81} & 32.86 & 28.94 & \textbf{24.81} & \textbf{27.42} & 68.50 & \textbf{91.14} & 42.33 & 7.29  & 93.10 & 58.72 & 54.46 & \textbf{45.74} \\
Ours-compression & 26.18 & 36.56 & 39.72 & 47.10 & 34.89 & 30.10 & 27.03 & 23.86 & 24.52 & 67.00 & 89.55 & 41.20 & 7.37 & 92.29 & 58.51 & 52.15 & 43.61 \\
Ours-calibration-compression & 26.37 & 40.18 & \textbf{44.06} & 47.83 & 40.77 & \textbf{33.46} & 26.91 & 24.35 & 26.17 & \textbf{69.00} & 89.74 & 40.73 & 6.99 & 92.47 & 57.60 & 54.31 & 45.06 \\
\bottomrule
\end{tabular}
}
\caption{Length Extrapolation Performance Comparison across Different Tasks. \textbf{Ours-calibration} and \textbf{Ours-compression} both represent parallel KV Cache Eviction, where the former evicts tokens of \(R_h\), and the latter evicts tokens of \(R_l\). \textbf{Ours-calibration-compression} represents the simultaneous adoption of both eviction strategies. \textbf{FullKV} refers to truncating the context to 4k or 8k lengths (without extrapolation) for generation.}
\vspace{-2mm}
\label{main-table}
\end{table*}


