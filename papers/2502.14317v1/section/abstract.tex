

 % \begin{abstract} Efficiently handling long contexts is essential for large language models (LLMs). While rotary position embeddings (RoPEs) enhance length generalization, achieving effective length extrapolation remains challenging and often demands expensive fine-tuning. Recent training-free approaches still suffer from the \textit{attention sink} phenomenon, degrading performance on long-context tasks. In this paper, we introduce \textbf{ParallelComp}, a novel training-free method for long-context extrapolation. Our approach extends LLMs' context length from 4K to 128K tokens while maintaining high throughput and preserving perplexity, as well as integrates seamlessly with Flash Attention. Specifically, to address attention sink, we propose an \textit{attention calibration strategy} that mitigates bias and propose a \textbf{chunk eviction strategy} to efficiently handle ultra-long contexts on a single A100 80G GPU and introduce a \textbf{Parallel KV Cache Eviction} technique that boosts chunk throughput by 1.76×, thereby achieving a 23.50× acceleration in the prefill stage with negligible performance loss due to attention calibration. Furthermore, \textbf{ParallelComp} achieves \textbf{91.17\% of GPT-4's performance} on long-context tasks using an 8B model, outperforming closed-source models like Claude-2 and Kimi-Chat. Our findings provide new insights into attention bias in parallel attention mechanisms and offer practical solutions for overcoming these challenges. \end{abstract}


 \begin{abstract}
Efficiently handling long contexts is crucial for large language models (LLMs). While rotary position embeddings (RoPEs) enhance length generalization, effective length extrapolation remains challenging and often requires costly fine-tuning. In contrast, recent training-free approaches suffer from the \textit{attention sink} phenomenon, leading to severe performance degradation. In this paper, we introduce \textbf{ParallelComp}, a novel training-free method for long-context extrapolation that extends LLMs' context length from 4K to 128K while maintaining high throughput and preserving perplexity, and integrates seamlessly with Flash Attention. Our analysis offers new insights into attention biases in parallel attention mechanisms and provides practical solutions to tackle these challenges. To mitigate the attention sink issue, we propose an \textit{attention calibration strategy} that reduces biases, ensuring more stable long-range attention. Additionally, we introduce a \textbf{chunk eviction strategy} to efficiently manage ultra-long contexts on a single A100 80GB GPU. To further enhance efficiency, we propose a \textbf{parallel KV cache eviction} technique, which improves chunk throughput by 1.76×, thereby achieving a 23.50× acceleration in the prefilling stage with negligible performance loss due to attention calibration. Furthermore, \textbf{ParallelComp} achieves \textbf{91.17\% of GPT-4's performance} on long-context tasks using an 8B model trained on 8K-length context, outperforming powerful closed-source models such as Claude-2 and Kimi-Chat. 
% We promise to open-source the code after the paper is accepted.
\end{abstract}
