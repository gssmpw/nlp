\input{table/main}

\input{table/infinitebench}

\input{table/ppl}

% \vspace{-2mm}

\section{Experiment}
\label{experiment}
\subsection{Experimental Settings}
\vspace{-1mm}
\paragraph{Models, Baselines, and Tasks}
% , Mistral-7B-Instruct-v0.1~\citep{jiang2023mistral}
% We evaluate different extrapolation methods on two models: Llama2-7B/13B-chat-hf~\citep{touvron2023llama} and Llama-3-8B-Instruction~\citep{meta2024llama3}. 

We compare our method with existing length extrapolation approaches, including Position Interpolation (PI)~\citep{chen2023extending}, NTK-Aware~\citep{fixedNTK}, ChunkLlama~\citep{an2024training}, AttenCalibration~\citep{yu2024unveiling}, and InfLLM~\citep{xiao2024infllm}, on LongBench~\citep{bai2023longbench} and InfiniteBench~\citep{zhang2024bench}, evaluating them on Llama2-7B-chat-hf~\citep{touvron2023llama} and Llama-3-8B-Instruction~\citep{meta2024llama3}. We also compare our method with the following open-source and closed-source models that have been trained on long-context data: ChatGLM-3-6B-128K~\citep{glm2024chatglm}, Kimi-Chat~\citep{moonshot2023}, Yi-6B-200K~\citep{01ai2023a}, Yi-34B-200K~\citep{01ai2023b}, Claude-2~\citep{anthropic2023}, Yarn-Mistral-7b-128k~\citep{peng2023yarn}, and GPT-4~\citep{achiam2023gpt}. Since AttenCalibration only calibrates the attention distribution and lacks length extrapolation capability, we implemented NTK-aware technology for it to achieve length extrapolation (AttenCalibration-NTK). We describe our hyperparameters in the Appendix~\ref{Hyperparameter}.


% We compare our method with existing length extrapolation methods, including Position Interpolation (PI)~\citep{chen2023extending}, NTK-Aware~\citep{fixedNTK}, ChunkLlama~\citep{an2024training}, AttenCalibration~\citep{yu2024unveiling}, and InfLLM~\citep{xiao2024infllm}, on LongBench~\citep{bai2023longbench} and InfiniteBench~\citep{zhang2024bench}.





% \paragraph{Hyperparameter} The Dynamic-PI method interpolates dynamically according to the length of the input token. NTK-Aware refer to~\citep{fixedNTK} and the maximum length is set to 280k. ChunkLlama, InfLLM and AttenCalibration-NTK use hyperparameters from open source repositories. About our method, when performing parallel KV Cache compression, we use last 8 token's cumulative attention scores to compress the KV cache size within each chunk to 2000. On Longbench, we retain 3 chunks, while on InfiniteBench, we retain 1 chunk on retrieval tasks and 3 chunks on others. In all datasets, the context length of each chunk, including the query, is the maximum pre-training length of the model. \( \tau \)  is set to 0.1 for llama2 and 0.3 for llama3.


% We implemented different eviction strategies for different tasks (see section~\ref{Attention_Bias} for more details).



% \vspace{-1mm}
\subsection{Length Extrapolation Settings}
\paragraph{Main Results} We present our method in Table~\ref{main-table}, showing the performance of several strong baselines on LongBench. We have the following main findings: \textit{i):} Our method is the \textit{only one} that surpasses FullKV (i.e., the baseline without any length extrapolation) across different backbones. \textit{ii):} Section~\ref{Q3} reveals that parallel KV cache compression exacerbates attention bias. However, combining it with the eviction \(R_h\) method to calibrate the attention distribution, i.e., Ours-calibration-compression, can significantly restore the performance to that of the original KV cache size. \textit{iii):} Chunk-based length extrapolation methods, such as InfLLM and ChunkLlama, generally perform better than position encoding-based methods such as Dynamic-PI and NTK-Aware. \textit{iv):} Directly calibrating the attention distribution for NTK-aware length extrapolation methods, namely AttenCalibration-NTK, achieves good performance only on longest datasets, such as NtrvQA, GovReport, and RB-P.

\vspace{-2mm}
\paragraph{Extrapolating beyond 128K context lengths.}
We evaluate the performance under extremely long contexts in Table~\ref{2_extreme_compression}, comparing it with several powerful open-source and closed-source models. These models are trained on context lengths exceeding 128K, and thus do not require additional extrapolation capabilities to handle ultra-long contexts. We have the following findings: \textit{i):} Our method performs exceptionally well on needle-in-a-haystack retrieval tasks (R.PK, R.Num, R.KV), being the \textit{only model} capable of achieving over 90\% accuracy across all tasks, surpassing even the strongest closed-source model, GPT-4. \textit{ii):} Position encoding-based length extrapolation methods, such as NTK-Aware, Dynamic-PI, generally struggle to achieve good performance on tasks with ultra-long contexts compared to chunk-based extrapolation approaches. \textit{iii):} Our training-free extrapolation method, using an 8K window, is the \textit{only approach} that surpasses the powerful closed-source models Kimi-Chat and Claude-2, achieving 91.17\% of GPT-4's performance on ultra-long contexts with an 8B model.
% \vspace{-2mm}.
\vspace{-2mm}
\paragraph{Language Modeling.} We present the results of perplexity (PPL) calculations on the NarrativeQA test set in Table~\ref{PPL}, which reflect the model's performance in long-context language modeling. \textit{i):} Chunk-based position extrapolation methods (ChunkLlama, InfLLM, and Ours) achieve significantly lower PPL compared to position encoding-based methods (Dynamic-PI and NTK-Aware). \textit{ii):} Position encoding-based methods start to collapse in performance for language modeling when the length exceeds 32k. \textit{iii):} As the number of chunks increases (from 2K to 128K), our method still demonstrates consistent perplexity stability across different lengths.
\input{table/infinitebench_ablation}
% \vspace{-3mm}
\subsection{Ablation of Attention Bias}
\label{Attention_Bias}

We present in Table~\ref{tab:InfiniteBench_ablation} the impact of evicting different bias tokens at various layers on different tasks. We have the following observations: \textit{i):} The \(R_s\) in the shallow layers (1-8) is crucial for retrieval tasks. Without these tokens, the model's performance will be significantly impaired. \textit{ii):} The \(R_r\) in the deeper layers (layers 17-24 and 25-32) is crucial for the model's coding abilities. Evicting these tokens will lead to a decline in coding performance. \textit{iii):} Shallow \(R_m\) (layers 1-8) damages the model's understanding ability, and evicting them can improve the model's performance. \textit{iv):} \(R_r\) in the early layers (layers 1-8) is important for the model's in-context learning ability. For a detailed analysis of this phenomenon, please refer to Appendix~\ref{Ablation Study}.


\input{table/throughput}
% \vspace{-5mm}
\subsection{Throughput Analysis}
% \textcolor{blue}{We primarily focus on two key metrics: the throughput of chunks during context parallelism and the token generation speed in the generation stage. To evaluate the effectiveness of our approach, we compare the maximum number of parallel chunks, memory usage, and token generation speed before and after applying parallel KV cache compression. Table \ref{throughput_analysis} presents the memory usage and token generation speed of the model when using the parallel KV cache compression strategy. On a single GPU, by compressing the KV cache size of each chunk to half of its original size, we achieve a 1.76x improvement in chunk throughput, thereby achieving a 23.50x acceleration in the prefill stage with negligible performance loss. }
 We mainly focus on the throughput of chunks during context parallelism. Therefore, we compare the maximum number of parallel chunks and the memory usage before and after parallel KV cache compression. Table \ref{throughput_analysis} presents the memory usage of the model using the parallel KV cache eviction strategy. On a single GPU, by compressing the KV cache size of each chunk to half of its original size, we achieve a 1.76x improvement in chunk throughput, thereby achieving a 23.50x acceleration in the prefill stage with negligible performance loss. 

