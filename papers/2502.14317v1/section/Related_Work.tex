\section{Related Work}
\label{Related Work}
\paragraph{Positional Encoding.}


Existing absolute positional encoding (APE) \citep{vaswani2017attention, devlin2018bert} incorporates either fixed or learnable positional encodings into input representations through vector addition. However, APE faces challenges when dealing with long contexts. To overcome these limitations, relative positional encoding (RPE) methods—such as rotary and additive positional encodings~\citep{rerope2023, su2024roformer, press2021train}—are developed to offer improved generalization in longer contexts. In addition, some data-dependent positional encoding methods~\citep{golovneva2024contextual, zheng2024dape} are gaining widespread attention. These positional encodings are generated based on the input data. 

% \textcolor{red}{perhaps using one sentence to explain what is context-based PE, as to my understanding, rope also relies on context--the relative position, albeit not on the specific tokens} \xiongjing{Done}



Some reecent works~\citep{alibi, chi2022kerple, li2023functional} also focus on designing better positional encodings to enhance the pre-trained model's capability for length extrapolation. Another line of works~\citep{peng2023yarn,NTKByParts2023,chen2023clex} focus on enhancing the LLM's length extrapolation capability by fine-tuning. 

Furthermore, there are two categories of training-free extrapolation methods. The first category, such as \citet{fixedNTK,chen2023extending,dynamicNTK,chen2024hope}, directly modifies positional encodings to enable extrapolation or interpolation, aiming to enhance the model's length extrapolation capability. The second category~\citep{an2024training, xiao2024infllm, ratner2022parallel, zhu2024accelerating} achieves extrapolation solely by reusing positional encodings. In this work, we focus primarily on the training-free setting.

\vspace{-4mm}

\paragraph{Attention Sink.}
A series of studies~\citep{gu2024attention, xiao2023efficient, liu2024lost} reveal the phenomenon of attention sink, where certain tokens in the sequence (referred to as sink tokens) consistently receive abnormally high attention scores. When the sequence length increases, the attention scores in the middle of the sequence are significantly lower compared to the beginning and the end. This often prevents the model from focusing on the correct parts of long sequences. \citet{xiao2023efficient} attributes this phenomenon to the softmax function, which forces the query to assign attention scores to all preceding tokens, even when the preceding tokens lack essential information, resulting in high scores being assigned to initial tokens. \citet{gu2024attention} proposes a simple solution by replacing the softmax attention with alternative attention mechanisms (e.g., unnormalized sigmoid attention) to alleviate this dependency. \citet{chen2024hope} alleviates this phenomenon by simply removing certain low-frequency components from RoPE. \citet{yu2024unveiling} deals with this issue by calibrating the attention distribution. In our work, we focus primarily on three phenomena of attention bias within parallel attention: the attention sink at the beginning of the input, the attention sink at the end of the input (i.e. recency bias~\citealt{peysakhovich2023attention}), and the scattered attention in the middle of the input~\citep{yu2024unveiling}. These biases provide insights into how LLMs utilize parallel contextual information.