\vspace{-1mm}
\section{Conclusion}
\label{conclusion}
\vspace{-1mm}
In this paper, we propose \textbf{ParallelComp}, a training-free approach to enhance the long-context extrapolation capability of LLMs. By leveraging parallel attention mechanisms and an efficient compression strategy, our method effectively mitigates attention bias while maintaining high efficiency. Experimental results on LongBench and InfiniteBench show that ParallelComp achieves strong performance, surpassing existing extrapolation methods and handling context lengths up to 128K tokens. Notably, our approach achieves \textbf{91.17\% of GPT-4's performance} on ultra-long context tasks with an 8B model, demonstrating its effectiveness with lower memory overhead. ParallelComp provides a scalable and efficient solution to training-free extension of context lengths in LLMs, enabling lossless improvement in throughput while reducing latency during the prefill stage.