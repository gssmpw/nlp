\appendix
\onecolumn

\section{Proof for Parallel Attention Collapse}
\label{sec:parallel_attn_collapse_aa}
\begin{theorem}\label{thm:parallel_attn_collapse}
    Consider a parallel attention mechanism where:
    \begin{itemize}
        \item The input sequence \( X \in \mathbb{R}^{N \times d} \) is divided into \( C \) chunks, where each chunk \( X^{c} \in \mathbb{R}^{w \times d} \) contains at most \( w \) tokens, for \( c \in [C] \).
        \item Define the query, key, and value matrices for each chunk as:
        \[
        Q^{c} = f_Q(X^{c}), \quad K^{c} = f_K(X^{c}), \quad V^{c} = f_V(X^{c}),
        \]
        where \( Q^{c}, K^{c}, V^{c} \in \mathbb{R}^{w \times d} \).
        \item The local attention matrix for chunk \( c \) is:
        \[
        A^{c}_\mathfrak{l} = \operatorname{Softmax}\left(\frac{Q^{c} {K^{c}}^\top}{\sqrt{d}}\right) \in \mathbb{R}^{w \times w}.
        \]
        \item Let \( \mathcal{S}_\epsilon^{(c)} \) denote the set of effective entries in the normalized attention matrix row for chunk \( c \), where:
        \[
        \mathcal{S}_\epsilon^{(c)}(A^{c}_\mathfrak{l}[i,:]) = \{ j \mid A^{c}_\mathfrak{l}[i,j] > \epsilon \}, \quad i \in [w].
        \]
        \item Assume the sparsity parameter \( R = O(\sqrt{\log(w)}) \).
    \end{itemize}
    Then, for any \(\epsilon > 0\), with high probability (\(1 - \delta\)), the number of ineffective entries in each row satisfies:
    \[
    \lim_{w \to \infty} | \mathcal{S}_\epsilon^{(c)}(A^{c}_\mathfrak{l}[i,:]) | = w - k.
    \]
\end{theorem}

\begin{proof}
    In the parallel attention setting, the sparsity of the local attention mechanism is analyzed within each chunk. For a specific chunk \( c \), the softmax scaling property governs the behavior of the attention matrix \( A^{c}_\mathfrak{l} \). The sparsity threshold \( \epsilon \) for effective entries in \( A^{c}_\mathfrak{l} \) can be expressed as:
    \[
    \epsilon \geq \exp\left(O(R) \cdot \sqrt{\log(w \cdot (w - k) / \delta)}\right),
    \]
    where \( k \) represents the number of ineffective entries in a row of the attention matrix. This inequality indicates that as the chunk size \( w \) increases, the threshold for retaining effective entries becomes stricter, reducing the number of effective entries.

    Rearranging the inequality, we derive an upper bound on \( k \), the number of effective entries:
    \[
    k \leq w - \exp\left(O\left(\frac{\log^2(\epsilon \cdot w)}{R^2}\right)\right) \cdot \frac{\delta}{wd}.
    \]
    Substituting this bound into the definition of \( |\mathcal{S}_\epsilon^{(c)}| \), we find that the number of ineffective entries is:
    \[
    | \mathcal{S}_\epsilon^{(c)}(A^{c}_\mathfrak{l}[i,:]) | \geq w - k.
    \]

    As \( w \to \infty \), the remaining effective entries approach \( w - 1 \), as \( R = O(\sqrt{\log(w)}) \) ensures that the sparsity growth remains bounded. Thus, the number of ineffective entries in each row satisfies:
    \[
    \lim_{w \to \infty} | \mathcal{S}_\epsilon^{(c)}(A^{c}_\mathfrak{l}[i,:]) | = w - k,
    \]
    which completes the proof.
\end{proof}

\FloatBarrier
\input{table/longbench_ablation}
\FloatBarrier


\section{Ablation Study}
\label{Ablation Study}

In this section, we analyze the impact of different attention biases on the LongBench dataset. As shown in Table~\ref{longbench-ablation}, the exceptionally low performance of Recency-eviction-layer-1-8 on both in-context learning tasks, TREC and TriviaQA, as well as SAMSum, indicates that the recency bias tokens in the model's early layers are crucial for developing in-context learning abilities. 
\section{Hyperparameter}
\label{Hyperparameter}
The Dynamic-PI method interpolates dynamically according to the length of the input token. NTK-Aware refer to~\citep{fixedNTK} and the maximum length is set to 280k. ChunkLlama, InfLLM and AttenCalibration-NTK use hyperparameters from open source repositories. About our method, when performing parallel KV Cache compression, we use last 8 token's cumulative attention scores to compress the KV cache size within each chunk to 2000. On Longbench, we retain 3 chunks from the priority queue, while on InfiniteBench, we retain 1 chunk for retrieval tasks and 3 chunks for other tasks from the priority queue. \textbf{In our setup, the total number of chunks processed in parallel and the chunks in the priority queue is 23.} In all datasets, the context length of each chunk, including the query, is the maximum pre-training length of the model. \( \tau \)  is set to 0.1 for llama2 and 0.3 for llama3. \( R_s \) is obtained from the first 200 tokens of the chunk, \( R_r \) is obtained from the last 200 tokens of the chunk, and the remaining part of the chunk obtains \( R_m \).  All experiments are performed on 80G A100.