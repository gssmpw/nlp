\vspace{-20pt}

\section{Introduction}
\label{intro}
\vspace{-2pt} 

Processing long contexts is a fundamental capability of large language models (LLMs)~\citep{achiam2023gpt, touvron2023llama, dubey2024llama}. Rotary position embeddings (RoPEs)~\citep{rerope2023} are adopted to train LLMs due to their ability to encode relative positions. However, achieving length extrapolation through retraining or fine-tuning the entire model remains a challenging problem~\citep{alibi,chi2022kerple, peng2023yarn, fu2024data}, especially in resource-constrained situations. 



\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/parallel_attention.pdf}}
\caption{\textbf{Upper:} The distribution of two types of attention. \textbf{Lower:} The heatmaps of two types of attention.}
\label{fig:parallel_attenion_overview}
\end{center}
\vspace{-6mm}
\end{figure}



To address this, many researchers explored training-free approaches for context length extrapolation. One prominent direction involves modifying positional encodings to facilitate length extrapolation~\citep{fixedNTK, dynamicNTK, an2024training}. Another promising strategy employs text chunking techniques~\citep{an2024training, xiao2024infllm, ratner2022parallel, zhu2024accelerating}, which form a type of \textbf{parallel attention} mechanism. These methods often build upon the classic RoPE mechanism to enhance the model's length extrapolation capability. However, both approaches suffer from the attention sink phenomenon~\cite{liu2024lost, xiao2023efficient, han2024lm, gu2024attention}, where high attention scores tend to be assigned to the first few tokens or the last few tokens in the input sequence, making the model fail to capture essential contextual information. Figure~\ref{fig:parallel_attenion_overview} illustrates this phenomenon and contrasts the attention distributions in normal and parallel attention. In the upper part of Figure~\ref{fig:parallel_attenion_overview}, we observe the distribution of attention scores across tokens. In the normal attention distribution (left), the attention is heavily skewed towards the first and last few tokens, a common issue that limits the model's ability to capture long-range dependencies. In contrast, the parallel attention distribution (right) attempts to spread the attention more evenly, but it still shows noticeable patterns of concentration in certain regions, indicating room for improvement in handling long-contexts effectively. The corresponding heatmaps (c) and (d) further reveal the attention matrices for both normal and parallel attention, where (c) shows the typical concentration of attention in the initial tokens, and (d) illustrates the fragmentation and partial concentration in parallel attention. In this paper, we focus on the unique phenomenon of attention sink in parallel attention for length extrapolation and explore solution to tackle this challenge.



Apart from the attention sink phenomenon, we also observe some unique biases in parallel attention. The impact of these attention biases on the model's length extrapolation ability is still under exploration. To promote the understanding of the effects of special attention biases on parallel attention, we propose the following questions in this paper:  \textbf{Q1:} \textit{What types of attention patterns can be summarized?} \textbf{Q2:} \textit{Is there any difference between the attention bias in parallel attention and the attention bias in classical attention?} \textbf{Q3:} \textit{Can the calibration strategy alleviate attention bias?} 




To address the above questions, we propose \textbf{ParallelComp}, a parallel long-context compression method to extrapolate length. While maintaining high throughput, we extrapolate the length from 4k to 128k on a single GPU, without increasing perplexity. Overall, our contributions are as follows:
\begin{itemize}
    \item  We propose \textbf{ParallelComp}, a novel training-free method that enables efficient length extrapolation for LLMs. Our approach extrapolates from a 4K context length to up to 128K tokens on a A100 80GB GPU.

        \item We conduct a detailed analysis of the unique attention biases in parallel attention mechanisms and introduce an \textbf{attention calibration strategy} that effectively alleviates these biases, recovering the performance loss.
        
         \item Our \textbf{chunk eviction strategy} enables parallel attention to handle contexts exceeding 128K on a single A100, while \textbf{parallel KV cache eviction} further enhances chunk throughput by 1.76x, thereby achieving a 23.50x acceleration in the prefilling stage with negligible performance loss due to attention calibration.
         
     \item Experiments show that ParallelComp achieves \textbf{91.17\% of GPT-4's performance} on ultra-long context tasks with an 8B model trained on 8K-length context, surpassing powerful closed-source models such as Claude-2 and Kimi-Chat.

     

    
   
\end{itemize}

\begin{figure*}[!h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figure/ParrallelComp_Final2-cropped.pdf}}
\caption{Overview of ParallelComp. \textbf{Parallel Attention} – The input sequence is split into multiple chunks based on the model's maximum context length. Each chunk undergoes local attention computation independently, and the self-information score of the query is calculated. \textbf{Parallel KV Cache Eviction} – Based on the self-information score, low-score tokens (marked in yellow, \( R_l \)) and attention bias tokens (marked in red, \( R_h \)) are selectively evicted to optimize memory usage and attention bias. \textbf{Global Attention} – The remaining KV caches are ranked by self-information, and less relevant chunks are discarded. The selected chunks are then concatenated, and a global attention operation is applied to ensure comprehensive information aggregation before the final autoregressive decoding stage.  }
\label{fig:overview}
\end{center}
% \vskip -0.2in
\vspace{-6mm}
\end{figure*}









