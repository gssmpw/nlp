\section{Related Works}
\label{sec:relatedworks}

%
%

%

\textbf{Test-time compute and pass@N strategy.} 
%
%
%
A growing body of work explores how training and test-time strategies interact to affect model performance.
\citet{boardgamescaling} highlights a tradeoff between train and test-time compute in a controlled board game setting.
%
Closely related to our work, \citet{largelanguagemonkey} identify an exponentiated power law between coverage and the number of in-context samples. \citet{scalingtestimecompute}, in turn, explore compute-optimal strategies for scaling test-time compute. Our paper focuses primarily on the pass@N test-time strategy.
\citet{gui2024bonbonalignmentlargelanguage} show that, in the best-of-N setting, the sample distribution is nearly optimally aligned with the reward model. To mitigate the excessive test-time compute costs associated with best-of-N sampling, \citet{gui2024bonbonalignmentlargelanguage, sessa2024bondaligningllmsbestofn} propose alignment algorithms to distill the sample distribution. \citet{li2024entropicdistributionmatchingsupervised} argue that fine-tuning with cross-entropy loss can limit output diversity and propose a maximum entropy-based method to address this issue. Our work similarly identifies limitations of cross-entropy when scaling test-time compute, but focuses on the alignment of training and test-time strategies. \citet{zhang2024balancing} explore methods for preventing catastrophic forgetting in supervised fine-tuning over diverse domains. 

%
\textbf{Post-training for mathematical reasoning.}
Various post-training techniques have been proposed to improve mathematical reasoning in LLMs. Instruction-tuning and reinforcement learning with human feedback have been shown to boost model performance on math~\cite{mathinstruct,prm8k,ormprm}, while continued training on math- or code-specific domain data enhances models' reasoning abilities for downstream mathematical tasks~\cite{minerva,azerbayev2024llemma,QwenMath2.5,deepseekmath,internlmmath}.
%
%
%
Rejection-sampling~\cite{star} and self-improvement techniques~\cite{rstar}, in turn, are useful to augment the training data for SFT.
%
More recent approaches~\cite{openaio1, deepseekr1} have incorporated reinforcement learning and achieved exceptional reasoning capabilities in various domains, such as math and coding.
%
Although our paper primarily focuses on supervised fine-tuning to enhance pretrained models' math capabilities, our loss function can be applied to other settings that train under CE loss, such as continual training, instruction-tuning, and data augmentation.
%
%
%
%
%
%
%
%
%
%
%
%


\textbf{Data pruning and hard example mining.} The interpretation of the loss function we derive below can be related to data pruning and hard example mining (see discussion of sample difficulty in \cref{subsec:easydata}). Data selection is often applied to curate high-quality datasets for pretraining~\cite{datacuration}, where \citet{datapruning} shows that pruning easy samples can improve pretraining loss scaling as a function of dataset size. \citet{LIMA,ye2025limoreasoning} demonstrate that supervised fine-tuning on even a very small dataset can yield remarkably strong performance in alignment and reasoning tasks.
%
On the other hand, hard example mining focuses on identifying and emphasizing challenging samples to improve model performance~\cite{hardexamplemining}.
%
In the domain of mathematical reasoning, \citet{tong2024dartmath} find a difficulty imbalance in rejection-sampled datasets and show that more extensive training on difficult samples improves model performance.
%

Our paper is closely related to a concurrent paper by \citet{chow2024inference}, which derives a similar training objective for RL to directly optimize for the best-of-N test-time strategy.