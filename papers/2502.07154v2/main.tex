%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

%
%
%
%
\usepackage{hyperref}


%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%

%
\usepackage[accepted]{icml2025}


%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}

%
\usepackage{multirow}

\newcommand{\FC}[1]{{\color{blue}#1}}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}

%
%
\icmltitlerunning{Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning}

\begin{document}

\twocolumn[
\icmltitle{Rethinking Fine-Tuning when Scaling Test-Time Compute: \\Limiting Confidence Improves Mathematical Reasoning}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Feng Chen}{equal,stanford}
\icmlauthor{Allan RaventÃ³s}{equal,stanford}
\icmlauthor{Nan Cheng}{um}
\\
\icmlauthor{Surya Ganguli}{stanford}
\icmlauthor{Shaul Druckmann}{stanford}
\\
\{fengc, aravento, sganguli, shauld\}@stanford.edu, nancheng@umich.edu


\end{icmlauthorlist}

\icmlaffiliation{stanford}{Stanford University, California, United States.}
\icmlaffiliation{um}{University of Michigan, Ann Arbor, Michigan, United States}

%
%
%
%

%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%

%
%
%
%
%

%
%
\printAffiliationsAndNotice{\textsuperscript{*}These authors contributed equally to this work.
} %

\begin{abstract}
Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation.
%
This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? 
%
To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples.
%
We show, surprisingly, that training with cross-entropy (CE) loss can be {\it misaligned} with pass@N in that pass@N accuracy {\it decreases} with longer training. 
%
We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N.
%
Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. 
%
%
Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. 
%
Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.
\end{abstract}

%
%
%
%
%
%
%
%
%

\section{Introduction}
\label{sec:introduction}
Scaling test-time compute has been integral to unprecedented improvements in LLMs' reasoning skills for complex tasks such as math and coding.
%
Thus, test-time compute has emerged as a new dimension for improving LLMs, leading to a key tradeoff between allocating additional compute to inference versus pretraining~\cite{scalingtestimecompute}.
%
Diverse test-time strategies include Chain-of-Thought (CoT)~\cite{COT_Wei}, tree-of-thought~\cite{ToT_Yao}, self-consistency~\cite{self_consistency}, self-reflection~\cite{reflexion}, self-critique~\cite{selfcritique}, self-verification~\cite{selfverification} and Monte-Carlo tree search~\cite{MCTS}. These have shown great success in boosting model performance in the post-training phase or at inference time. More recently, OpenAI's O1 model~\cite{openaio1} and DeepSeek's R1 model~\cite{deepseekr1} have combined some of these strategies with reinforcement learning to generate high-quality reasoning traces for problems of various difficulty levels, demonstrating clear performance improvements as more test-time compute is allocated.

%

%

%
These successes fit into a broader paradigm in which a frontier model is first fine-tuned on a reasoning task with supervised fine-tuning (SFT)~\cite{COT_Wei,ouyang2022traininglanguagemodelsfollow,chung2022scalinginstructionfinetunedlanguagemodels}, and then a test-time algorithm is applied to model outputs or reasoning traces to improve performance~\cite{ToT_Yao,self_consistency,chen2021evaluatinglargelanguagemodels}.
%
%
Many test-time algorithms are independent of the fine-tuning process. As a result, the fine-tuning is agnostic to and thus decoupled from the test-time algorithm~\cite{chow2024inference}.
%
However, for a given choice of test-time strategy and compute budget, it is not {\it a priori} clear which fine-tuning approach, including the loss objective, would be best aligned with the test-time strategy so as to maximize the test accuracy under the overall strategy.

Our work studies the problem of aligning fine-tuning and test-time algorithms. We consider what is perhaps the simplest setting, supervised fine-tuning  with CE loss under the pass@N test-time strategy. This setting reveals a case of misalignment: standard SFT is not the right choice for maximizing performance under pass@N. We believe that this kind of misalignment presents itself in several combinations of fine-tuning/test-time approaches, motivating our thorough study in this paper. Our main contributions are,
\begin{itemize}
    \item We identify a misalignment between standard fine-tuning with CE loss and the pass@N coverage metric at test time (\cref{subsec:overfitting}).
    %
    \item We develop and experimentally verify a framework that suggests this misalignment arises from overconfidence induced by training on CE loss (\cref{subsec:explainoverfitting,subsec:overconfidence}).
    \item We propose an alternate loss function that directly optimizes the pass@N coverage metric, demonstrating consistent improvement over the CE loss objective and achieving superior accuracy frontiers in MATH and MiniF2F (\cref{subsec:DCO,subsec:dco-overconf,subsec:DCO-s}).
    \item We extend our algorithm to more complex test-time scenarios including searching over proof-trees of varying shapes to improve automated theorem proving, and answering math questions using Chain-of-Thought reasoning traces, demonstrating improved mathematical reasoning in both cases (\cref{subsec:DCO-s,subsec:cot-online}).

\end{itemize}
\section{Related Works}
\label{sec:relatedworks}

%
%

%

\textbf{Test-time compute and pass@N strategy.} 
%
%
%
A growing body of work explores how training and test-time strategies interact to affect model performance.
\citet{boardgamescaling} highlights a tradeoff between train and test-time compute in a controlled board game setting.
%
Closely related to our work, \citet{largelanguagemonkey} identify an exponentiated power law between coverage and the number of in-context samples. \citet{scalingtestimecompute}, in turn, explore compute-optimal strategies for scaling test-time compute. Our paper focuses primarily on the pass@N test-time strategy.
\citet{gui2024bonbonalignmentlargelanguage} show that, in the best-of-N setting, the sample distribution is nearly optimally aligned with the reward model. To mitigate the excessive test-time compute costs associated with best-of-N sampling, \citet{gui2024bonbonalignmentlargelanguage, sessa2024bondaligningllmsbestofn} propose alignment algorithms to distill the sample distribution. \citet{li2024entropicdistributionmatchingsupervised} argue that fine-tuning with cross-entropy loss can limit output diversity and propose a maximum entropy-based method to address this issue. Our work similarly identifies limitations of cross-entropy when scaling test-time compute, but focuses on the alignment of training and test-time strategies. \citet{zhang2024balancing} explore methods for preventing catastrophic forgetting in supervised fine-tuning over diverse domains. 

%
\textbf{Post-training for mathematical reasoning.}
Various post-training techniques have been proposed to improve mathematical reasoning in LLMs. Instruction-tuning and reinforcement learning with human feedback have been shown to boost model performance on math~\cite{mathinstruct,prm8k,ormprm}, while continued training on math- or code-specific domain data enhances models' reasoning abilities for downstream mathematical tasks~\cite{minerva,azerbayev2024llemma,QwenMath2.5,deepseekmath,internlmmath}.
%
%
%
Rejection-sampling~\cite{star} and self-improvement techniques~\cite{rstar}, in turn, are useful to augment the training data for SFT.
%
More recent approaches~\cite{openaio1, deepseekr1} have incorporated reinforcement learning and achieved exceptional reasoning capabilities in various domains, such as math and coding.
%
Although our paper primarily focuses on supervised fine-tuning to enhance pretrained models' math capabilities, our loss function can be applied to other settings that train under CE loss, such as continual training, instruction-tuning, and data augmentation.
%
%
%
%
%
%
%
%
%
%
%
%


\textbf{Data pruning and hard example mining.} The interpretation of the loss function we derive below can be related to data pruning and hard example mining (see discussion of sample difficulty in \cref{subsec:easydata}). Data selection is often applied to curate high-quality datasets for pretraining~\cite{datacuration}, where \citet{datapruning} shows that pruning easy samples can improve pretraining loss scaling as a function of dataset size. \citet{LIMA,ye2025limoreasoning} demonstrate that supervised fine-tuning on even a very small dataset can yield remarkably strong performance in alignment and reasoning tasks.
%
On the other hand, hard example mining focuses on identifying and emphasizing challenging samples to improve model performance~\cite{hardexamplemining}.
%
In the domain of mathematical reasoning, \citet{tong2024dartmath} find a difficulty imbalance in rejection-sampled datasets and show that more extensive training on difficult samples improves model performance.
%

Our paper is closely related to a concurrent paper by \citet{chow2024inference}, which derives a similar training objective for RL to directly optimize for the best-of-N test-time strategy.

\section{Problem setup}
\label{sec:problemsetup}

Given a vocabulary set $W$, we consider a dataset $\mathcal{D}=\{(x^{(i)},y^{(i)})\}_{i=1}^M$, where $x^{(i)} \in W^{n_i}$ is a prompt, $y^{(i)} \in W^{m_i}$ is its ground-truth completion, and $n_i$ and $m_i$ are the prompt and completion lengths. 
%
In the context of math, $x^{(i)}$ is the problem statement and $y^{(i)}$ is its solution.
%
To model the conditional distribution $p(y^{(i)}|x^{(i)})$ we use an autoregressive transformer model~\cite{transformer}, %
%
%
%
%
%
%
%
%
which is traditionally trained by minimizing the cross-entropy loss
%
\begin{equation}
\label{eq:CELOSS}
    \mathcal{L_{\text{CE}}}=-\mathop{\mathbb{E}}_{(x,y)\sim\mathcal{D}} \log \hat{p}(y | x)%
\end{equation}
%
%
%
%
%
%
%
%
where $\hat{p}$ denotes the model's distribution.
%

To use and evaluate the model at test time, we assume the existence of an efficient oracle verifier $V$ which takes as input an $(x,y)$ pair and returns $V(x,y)=1$ if $y$ is a correct completion of $x$ and otherwise returns $V(x,y)=0$.
%
Practical examples of verifiers include compilers or pre-defined unit tests for coding problems, or automatic proof checkers in mathematical theorem proving. 
%
In such applications, a simple method, known as pass@N, for trading test-time compute for accuracy involves sampling $N$ completions from $\hat p$ given the test prompt $x$ and applying the verifier $V$ to all of them to search for a correct solution.  The probability of a correct answer is then no longer the probability that $1$ completion is correct, but rather the probability that {\it at least one} of $N$ is correct.  This probability, for a dataset $\mathcal D$, is given by the pass@N coverage metric 
%
%
\begin{equation}\label{eq:optimalstrategy}   \mathcal{C}_\mathcal{D}^N=\mathop{\mathbb{E}}_{\substack{x\sim\mathcal{D}\\\{y_i\}_{i\in[N]}\stackrel{\text{i.i.d.}}{\sim}\hat{p}(\cdot|x)}}\mathbb{P}(\exists j\in[N]\,s.\,t.\,  V(x,y_j)=1).
\end{equation}

Minimizing the CE loss in \cref{eq:CELOSS} is equivalent to maximizing the pass@$1$ metric $\mathcal{C}_\mathcal{D}^1$ on a training set $\mathcal D$.  But if we scale up test-time compute so that the pass@N metric $\mathcal{C}_\mathcal{D}^N$ on a test set $\mathcal D$ for $N \gg 1$ is the relevant performance metric, is $\mathcal{L}_\mathrm{CE}$ still a good training loss, or can we do better?
%
%

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Misalignment between CE loss and pass@N}
\label{sec:misalignment}
\subsection{The CE loss induces overfitting for pass@N }
\label{subsec:overfitting}

\begin{table}[t]
\caption{Pass@N coverage metric on the MATH test set for a Llama-3-8B-base model fine-tuned with CE loss on direct answers from the MATH training set. Surprisingly, Pass@N test accuracy at large $N$ decreases with number of training epochs.}
\label{table:misalignment}
%
\vspace{-8pt}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}
{
\begin{tabular}{lcccr}
\toprule
 & pass@1 & pass@16 & pass@256 & pass@4k \\
\midrule
Epoch 1 &4.4\% & 30.0\% & \textbf{65.2\%} & \textbf{82.5\%} \\
Epoch 2 &5.3\% & \textbf{31.4\%} & 64.5\% & 80.0\%\\
Epoch 3 &6.5\% & 28.7\% & 54.5\% & 79.2\%\\
Epoch 4 &\textbf{7.4\%} & 22.9\% & 44.5\% & 63.0\%\\
\bottomrule
%
%
%
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
%
\vspace{-10pt}
\end{table}

To understand the impact of training with CE loss on pass@N test performance, we fine-tune Llama-3-8B-base~\cite{llama3} on the MATH~\cite{hendrycksMATH} dataset. We start from the base model rather than LLama-3-8B-Instruct to avoid potential leakage of the MATH dataset into LLama-3-8B-Instruct through post-training. We follow \citet{prm8k} and use $12,000$ problems for training and the remaining $500$ for testing.  Here we train the model to provide a direct answer, without a reasoning trace. We will discuss training with CoT in \cref{subsec:cot-online} for MATH reasoning traces. We confirm that the results presented in this section also hold for a larger model, Llama-3-70B-base, and refer readers to~\cref{sec:supp:70B} for additional results.

\cref{table:misalignment} reveals that the pass@N performance $\mathcal{C}_\mathcal{D}^N$ on a test set monotonically increases with the number of training epochs {\it only} for $N=1$, when minimizing CE loss is equivalent to maximizing $\mathcal{C}_\mathcal{D}^1$. However, for $N\geq16$, minimizing CE loss during training does {\it not} monotonically increase $\mathcal{C}_\mathcal{D}^N$ at test; indeed for $N\geq256$, pass@N test performance, remarkably, monotonically {\it decreases} with the number of training epochs, despite the fact that pass@1 performance monotonically {\it increases}. This effectively corresponds to a novel type of overfitting, in which test performance degrades over training, likely due to a mismatch between the test time (pass@N) and training time (pass@1) strategies.

\subsection{Overfitting, confidence, and explore-exploit tradeoff}
\label{subsec:explainoverfitting}


What are the origins of this overfitting? First we show that in the simple case of a {\it single} problem $x$, overfitting cannot occur. Let $\hat{p}(x)$ denote the probability assigned by the model to all correct answers for problem $x$. Then the pass@1 coverage is $\mathcal{C}^{1}=\hat{p}(x)$ while the pass@N coverage is $\mathcal{C}^{N}=1-(1-\hat{p}(x))^N=1-(1-\mathcal{C}^{1})^N$.  This formula for $\mathcal{C}^{N}$ in {\it the single problem} case obeys:
\begin{lemma}
\label{lemma:monotonicity}
    %
    $\forall N,\,N' > 0$, $\mathcal{C}^{N}$ is monotonic in $\mathcal{C}^{N'}$.
\end{lemma}
Thus increasing pass@N$'$ coverage implies increasing pass@N coverage for any $N$ and $N'$. When $N'=1$, this implies minimizing CE loss maximizes pass@N coverage.

However, \cref{lemma:monotonicity} can fail when there is more than one problem. To understand this in a simple setting, consider a test set with two problems $x_1$ and $x_2$ with corresponding unique correct answers $y_1$ and $y_2$. Consider two models. The first model assigns probabilities $\hat{p}_1(y_1|x_1)=1$ and $\hat{p}_1(y_2|x_2)=0$.  If we think of the probability a model assigns to an answer as its {\it confidence} in that answer, this model is highly confident and correct for problem $x_1$, but highly confident and {\it wrong} for $x_2$.  Its pass@1 coverage is thus 50\%.  Moreover, since it always gets $x_1$ right and $x_2$ wrong, its pass@N coverage remains at 50\%.  In contrast, consider a second model which assigns probabilities $\hat{p}_2(y_1|x_1)= \hat{p}_2(y_2|x_2)=0.1$. This model has high confidence (0.9) but unfortunately on incorrect answers. Therefore its pass@1 accuracy is only 10\%. However, it is willing to explore or hedge by placing some low confidence ($0.1$) on the other answer, which happens to be correct.  Thus if this model samples $N$ times, the pass@N coverage increases with $N$, eventually approaching 100\% as $N\rightarrow\infty$. Thus the first model outperforms the second in terms of pass@1 but not pass@N for large $N$.  This indicates a tradeoff amongst policies: those that do better on pass@1 may not do better at pass@N.  Of course the best one can do is be confident {\it and} correct, corresponding to the optimal model with $\hat{p}^*(y_1|x_1) = \hat{p}^*(y_2|x_2) = 1$.  However, this toy example reveals that if one cannot guarantee correctness, it can be beneficial to limit confidence and explore more solutions, especially if one can sample at large $N$.

%
%
%
%
%
%
%
%
%

%

To demonstrate more generally the existence of tradeoffs between confident exploitation of a few answers versus unconfident exploration of many answers, as a function of the number of passes $N$, we prove two lemmas. 
To set up these lemmas, given any problem, let $\hat{p}_i$ denote the model probability or confidence assigned to answer $i$, and assume answers are sorted from highest to lowest confidence so that $\hat{p}_i \geq \hat{p}_{i+1} , \forall\,i\geq 1$. Thus $\hat{p}_1$ is the model's {\it maximal confidence} across all answers. Moreover, let $p_{i}$ be the probability across all the problems that the $i^{\text{th}}$ ranked answer is {\it actually} correct.
%
Assume the model policy is approximately well calibrated so that higher confidence implies higher or equal probability of being correct, i.e. $p_{i}\geq p_{i+1}, \forall\,i\geq 1$. 
We empirically verify that the model trained with CE loss on the MATH dataset approximately satisfies this assumption (\cref{fig:well-calibrated}). Indeed, optimal policies maximizing pass@N coverage in \cref{eq:optimalstrategy} are approximately well calibrated (See~\cref{lemma:calibrated}). $p_1$ is then the model's {\it maximal accuracy} across all answers. We prove: 
%

\begin{lemma}[Upper bound on max confidence]\label{lemma:upperboundfora1}
Assume a max accuracy $p_{1}$, and assume $\sum_{i=1}^{k}p_{i}\geq 1-\epsilon$ for some $0<\epsilon<1$. Then optimal policies maximizing pass@N coverage in \cref{eq:optimalstrategy}, subject to above accuracy and calibration constraints, must have max confidence upper bounded as
\begin{equation*}
    \hat{p}_{1}^{*} \leq 1-\frac{k-1}{(k-1)^{\frac{N}{N-1}} p_1^{\frac{1}{N-1}} \left(1-p_1-\epsilon\right){}^{\frac{1}{1-N}}+1}.
\end{equation*}
\end{lemma}
This upper bound is monotonically decreasing in $N$ (\cref{fig:theorem}), implying that at large $N$, an optimal policy for pass@N must limit its max confidence to be low, thereby favoring exploration and discouraging exploitation. Furthermore, we prove:  
\begin{lemma}[Lower bound on max confidence]
    \label{lemma:lowerboundfora1}
    Assume top two accuracies $p_{1}>p_{2}$. Then optimal policies maximizing pass@N coverage in \cref{eq:optimalstrategy}, subject to above accuracy and calibration constraints, must have max confidence obeying 
    \begin{equation*}
        \hat{p}_{1}^{*}\geq 1-\frac{(1-p_{1}+p_{2})p_{1}^{\frac{1}{1-N}}p_{2}^{\frac{1}{N-1}}}{1-p_{1}+p_{2}+p_{1}^{\frac{1}{1-N}}p_{2}^{\frac{N}{N-1}}}.
    \end{equation*}
\end{lemma}
This lower bound is a monotonically decreasing function of $N$ (\cref{fig:theorem}), implying that at small $N$, optimal policies for pass@N must have high max confidence, thereby favoring exploitation and discouraging exploration. Note in the limit $N\to 1^{+}$, the lower bound is always $1$, recovering the intuitive result that the optimal policy for pass@1 is to place 100\% confidence on the highest accuracy answer.  


\subsection{Overconfidence prevents performance gains from scaling test-time compute}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/Fig_CE_DCO.pdf}}
\caption{\textbf{A model trained with CE loss becomes overconfident in its greedy completions, which harms its pass@N coverage; our proposed DCO objective limits this overconfidence.} We fine-tune a Llama-3-8B base model on the MATH dataset to produce direct answers without a reasoning trace. $\hat{y}_\text{greedy}$ is the model's greedy completion when sampling autoregressively and choosing the most likely token at each step. \textbf{Leftmost:} The model trained with CE loss assigns progressively larger confidences $\hat{p}(\hat{y}_\text{greedy}|x)$ to its greedy completions over the course of training. \textbf{Left:} At the end of the training, only a small portion of the model's highly confident completions are correct. This will harm the model's pass@N performance when scaling up $N$. \textbf{Right:} Same as leftmost but shown for the DCO loss with $N=256$.  Relative to the CE loss, the model trained on DCO shows a much milder overconfidence effect.  \textbf{Rightmost:} The confidence distribution of the greedy completions after four epochs with DCO for various choices of $N$. As $N$ increases, the model's confidence on the greedy completion is more stringently limited, directly as a consequence of the overconfidence regularizer $F$.}
\label{fig:overconfidence}
\end{center}
\vskip -0.2in
\end{figure*}

%

To summarize the consequences of our lemmas above, among the space of approximately well calibrated policies in which higher model confidence on an answer is correlated with higher accuracy on the answer, \cref{lemma:upperboundfora1} suggests that at large $N$ it is beneficial to unconfidently explore by assigning low model confidence to many answers, while \cref{lemma:lowerboundfora1} suggests that at small $N$ it is beneficial to confidently exploit by assigning high model confidence to one or a few answers. These lemmas make a prediction that could explain the empirical observation in \cref{table:misalignment} that pass@N test performance for large $N$ degrades over epochs when pass@1 is maximized at training time: namely, maximization of pass@1 makes the model overconfident, thereby preventing good performance on pass@N.%

To test this theoretical prediction of model overconfidence, we estimated the max confidence of the model as $\hat{p}(y_\mathrm{greedy}|x)$ where $y_\mathrm{greedy}$ is the greedy completion to $x$ obtained by sampling autoregressively from the model $\hat{p}$, and at each step selecting the token with the highest probability. 
$\hat{p}(y_\mathrm{greedy}|x)$ approximates the max confidence $\hat{p}_1$ in \cref{lemma:upperboundfora1,lemma:lowerboundfora1}.
For the model fine-tuned on MATH with CE loss, we plot the distribution of $\hat{p}(y_\mathrm{greedy}|x)$ over the test set in \cref{fig:overconfidence} (leftmost). This demonstrates the model becomes progressively more confident about its greedy completion over training, thereby confirming our theoretical prediction. In \cref{fig:overconfidence} (left) we see why this is a problem: only some of the model's highly confident answers are correct. Thus the model becomes overconfident {\it and} largely wrong. Scaling test-time compute cannot easily rescue such a model, as over multiple samples, it is likely to confidently provide the same wrong answers, explaining the origins of poor pass@N test performance. In~\cref{sec:supp:AIME}, we observe a similar trend of overconfidence on the out-of-distribution test set AIME24, further highlighting the generality of this failure mode.

\label{subsec:overconfidence}


\subsection{Easy data drives overconfidence}
\label{subsec:easydata}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.99\columnwidth]{figures/jvp_plot.pdf}}
\caption{\textbf{Easy data drives overconfidence and degrades performance when scaling test-time compute}. 
For the experiments in \cref{fig:overconfidence}, we compute the directional derivative $\nabla_g\mathcal{L}_{\text{test}}^N$ at two points during the first training epoch: 11\% (early) and 86\% (late). At each stage, the gradient direction $g=-\sum_{(x_i,y_i)\in\mathcal{B}}\nabla\ell_\text{CE}(x_i,y_i)$ is evaluated on batches of \textbf{previously unseen} training data of each difficulty level defined in the MATH dataset. Early in training, data from all difficulty levels contribute to decreasing the test loss (\textit{left}). However, later in training, easier examples (difficulty level 2) provide no further benefit, while the easiest examples (difficulty level 1) actively degrade test performance (\textit{right}). The plotted $\nabla_g\mathcal{L}_{\text{test}}^N$ is an average over batches of unseen data, and we use $N=256$, corresponding to pass@256.}
\label{fig:jvp}
\end{center}
%
\vspace{-20pt}
\end{figure}

We have shown above that overconfidence limits performance gains from scaling test-time compute. Here we investigate whether all training data contribute equally to the drop in pass@N test performance. To quantify this effect, we define the test loss as the negative log-probability of coverage, $\mathcal{L}_{\text{test}}^N = -\sum_{(x,y)\in\mathcal{D}^{\mathrm{test}}}\mathcal \log \ \mathcal{C}_{(x,y)}^N$, where $\mathcal{C}_{(x,y)}^N$ denotes the coverage for a single test example $(x,y)$. 
To measure how a training batch $\mathcal{B}$ influences the final test loss, we compute the directional derivative $\nabla_g\mathcal{L}_{\text{test}}^N$ for the gradient direction $g=-\sum_{(x_i,y_i)\in\mathcal{B}}\nabla\ell_\text{CE}(x_i,y_i)$, where $\ell_\mathrm{CE}$ is the standard CE loss on a single example.
A negative value, $\nabla_g \mathcal{L}_{\text{test}}^N<0$, indicates that a gradient step in the direction of $g$ decreases the test loss. 

To analyze the impact of data difficulty, we group training data by difficulty level (as specified in the MATH dataset) and use the directional derivative to examine how a batch of \textbf{previously unseen} data from a \textbf{given difficulty level} would affect $\mathcal{L}_{\text{test}}^N$ at different stages of training. Early in training, we observe that $\nabla_g\mathcal{L}_{\text{test}}^N$ is negative across all difficulty levels, indicating that data from all difficulty levels contribute positively to reducing the test loss (\cref{fig:jvp}, left). However, near the end of the first epoch, training on easier examples (difficulty levels 1 and 2) no longer improves performance, and the easiest examples (level 1) actively degrade pass@N test performance (\cref{fig:jvp}, right). This shows how continued training on easy data can harm test performance when scaling test-time compute.


\section{Direct Coverage Optimization}
\label{sec:NCO}
\subsection{A solution that naturally prevents overconfidence}
\label{subsec:DCO}
The misalignment between CE training loss and pass@N coverage suggests a simple solution: {\it directly optimize} pass@N coverage for each training example at training time, whenever the pass@N strategy is to be used at test-time. We thus propose the Direct Coverage Optimization (DCO) objective, $\mathcal{L}_{\text{DCO}}^N = \mathop{\mathbb{E}}_{(x,y)\sim\mathcal{D}}\ell^N_{\text{DCO}}(x,y)$, where  
%
\begin{align}
\ell^N_{\text{DCO}}(x,y) =-\log\left( 1-(1-\hat{p}(y|x))^N\right), \label{eq:DCO}
\end{align}
is the $-\log$ probability that the model produces $y$ at least once in $N$ samples given $x$.  Thus for each prompt $x$ and correct completion $y$ in the training set, we maximize the probability $y$ is found in $N$ passes.  
This loss naturally prevents model overconfidence, as can be seen via its gradient
\begin{equation}
    \nabla_\theta \ell_{\text{DCO}}^N(x,y) = F\left(N,\hat{p}(y|x)\right) \nabla_\theta \ell_{\text{CE}}(x,y)
    \label{eq:DCO-gradients}
    %
    %
    %
    %
\end{equation}
where $\ell_\mathrm{CE}$ is the standard CE loss on a single example, and $F(N,\hat{p}(y|x))=\frac{N (1-\hat{p}(y|x))^{N-1}\hat{p}(y|x)}{1-(1-\hat{p}(y|x))^N}$
is an extra overconfidence regularization factor that multiplies the standard CE gradient. Note that $F(N,\hat{p}(y|x))=1$ for $N=1$, so DCO reduces to CE for pass@1. Furthermore $F(N,\hat{p}(y|x))$ monotonically decreases in the model confidence $\hat{p}(y|x)$ (see \cref{fig:factor}). Thus gradients for examples $(x,y)$ on which the model is more confident are attenuated, and this attenuation is stronger for larger $N$. This justifies the interpretation of $F(N,\hat{p}(y|x))$ as a regularizer that prevents model overconfidence. Indeed, for large $N$, $F(N,\hat{p}(y|x))\approx 0$ for confidence $\hat{p}(y|x)\gtrsim 1/N$.
As soon as model confidence on an example exceeds $1/N$, its gradient becomes negligible. Thus interestingly, aligning training and test better through DCO naturally yields a simple emergent regularization of model overconfidence, which was itself identified as an impediment to performance gains through scaling test-time compute in \cref{fig:overconfidence} (leftmost). 
Indeed, $F(N, \hat{p}(y|x))$ is smaller for easier examples (\cref{fig:data_dependency}, right), effectively regularizing their contribution. This regularization mitigates the potential detrimental effects of overconfidence from easy examples as discussed in \cref{subsec:easydata}.

We note in practice, the introduction of $F$ can lead to some samples in a batch contributing minimally to the current gradient step, thereby reducing the effective batch size. To maintain a stable effective batch size, we introduce a threshold $\epsilon$, and if for a given example, $(x,y)$, $F(N,\hat{p}(y|x))<\epsilon$, we replace the example with a new one. 

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.75\columnwidth]{figures/f_factor.pdf}}
\caption{\textbf{The DCO objective limits overconfidence by attenuating gradients for examples on which the model is highly confident.} We plot the confidence regularization factor $F(N,\hat{p}(y|x))$ in \cref{eq:DCO-gradients} which attenuates CE loss gradients to obtain DCO loss gradients. $N=1$ corresponds to CE loss, where $F=1$ regardless of $\hat{p}(y|x)$. For any $N>1$, the attenuation is stronger ($F$ is smaller) for examples with higher model confidence $\hat{p}(y|x)$, and this attentuation effect increases with $N$. $F$ drops to near zero as soon as confidence $\hat{p}(y|x)$ increases to $1/N$.  Thus as soon as an example's confidence rises to $O(1/N)$ over training time, its gradient under the DCO loss cannot make the confidence rise any further. Thus DCO naturally limits overconfidence.}
%
\label{fig:factor}
\end{center}
%
\vspace{-20pt}
\end{figure}

\subsection{DCO can prevent overconfidence and rescue test-time scaling}
\label{subsec:dco-overconf}
We next test whether DCO can rescue test-time scaling by preventing model overconfidence. We first perform experiments on the MATH dataset in this section, and then in the next section we perform experiments on the LeanDojo automated theorem proving benchmark~\cite{leandojo}.

We fine-tune the LLama-3-8B-base model for 4 epochs on the MATH training set using DCO for $N=256$ and confirm that the model is far less confident on its greedy completion than when trained with CE loss after multiple training epochs (compare \cref{fig:overconfidence} right and \cref{fig:overconfidence} leftmost).  Moreover, training with DCO at larger $N$ yields lower model greedy confidences at the end of training (\cref{fig:overconfidence} rightmost), consistent with \cref{lemma:upperboundfora1}. 

We next assess pass@N test performance as a function of $N$ for a variety of models trained by minimizing $\mathcal{L}_\text{DCO}^{N'}$ for different values of $N'$ (\cref{fig:DCOfrontier}).  
For any given $N$ in pass@N, there is an optimal $N'$ for the training loss $\mathcal{L}_\text{DCO}^{N'}$ that maximizes pass@N test coverage, yielding a Pareto optimal performance frontier (black curve) that is achieved when $N'$ is close to $N$. 
In particular the model trained with CE loss (equivalent to pass@1 maximization at training) performs poorly relative to the Pareto frontier at large N when the pass@N strategy is used at test time (red curve below black at large $N$). Conversely, models trained with DCO at large $N'$ perform poorly relative to the Pareto frontier when the pass@N strategy is used at test time with small $N$ (green curves below black at small $N$).  

Together these results indicate that the alignment of the training loss $\mathcal{L}_\text{DCO}^{N'}$ with the test time strategy pass@N, with $N'$ close to $N$, is crucial for obtaining Pareto optimal performance. 
Moreover, these results once again confirm the tradeoff between exploration and exploitation, with good performance using pass@N test-time strategies requiring high (low) exploration with low (high) confidence at large (small) $N$. Indeed, this tradeoff prevents achieving Pareto optimality using pass@N at test-time for all $N$ via training with DCO at any {\it single} value of $N'$. 
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figures/Fig.1_DCO_frontier.pdf}}
\caption{\textbf{DCO improves on CE for pass@N test coverage over a broad range of N and traces a Pareto-optimal frontier.} We fine-tune Llama-3-8B base models on the MATH dataset to produce direct answers. We fine-tune for 4 epochs one model using CE loss and several models under the $\mathcal{L}_\text{DCO}^{N'}$ objective, for choices of $N'$ indicated by color. We plot pass@N test coverage as a function of $N$, with each curve (solid red or faint blue-green) corresponding to {\it one} fine-tuned model. Note that there is no choice of $N'$ that is optimal across all $N$ (different colors are higher at different $N$). The black curve is a Pareto-optimal performance frontier traced by the max of coverage curves for DCO over all $N'$. Pareto-optimality at a given test strategy pass@N for some $N$ on the x-axis is obtained by DCO training for some $N'$ close to $N$. (See~\cref{fig:DCOfrontier_256_highlight} for an example where the model trained with $N'=256$ is highlighted.) Please refer to~\cref{fig:DCOfrontier_70B.,fig:DCOfrontier_AIME} for corresponding results with the Llama-3-70B-base model and on the AIME24 dataset, respectively.}
\label{fig:DCOfrontier}
\end{center}
%
\vspace{-10pt}
\end{figure}

\subsection{Improved theorem proving via ensembled tree search through a modified step-wise DCO} 
\label{subsec:DCO-s}
To further test our method in realistic settings with a verifier, we conduct experiments in theorem proving using an interactive proof assistant on the LeanDojo benchmark~\cite{leandojo} extracted from the math library of LEAN4~\cite{The_mathlib_Community_2020}. In this task, at each step $i$ of the proof, the model is prompted with the current proof state $x[i]$, and it outputs a proof tactic $y[i]$ with a trainable model probability $\hat{p}(y[i]|x[i])$.
%
The proof assistant takes the sampled tactic $y[i]$, verifies whether it is a valid tactic, and if so, returns the next proof state $x[i+1]$ after the tactic $y[i]$ is applied. The model then samples the next tactic $y[i+1]$.  At test time this interactive process between the model and the assistant continues until either: (1) it terminates in an invalid tactic; (2) hits a maximal allowed search depth set by computational constraints; or (3) terminates in a successful proof of the initial proof goal as verified by the proof assistant. 

In our experiments, we use LEAN4~\cite{lean4} as the formal proof language. We start from the model Qwen2.5-Math-1.5B~\cite{QwenMath2.5}, and fine-tune it on the LeanDojo benchmark~\cite{leandojo}. In contrast to solving math problems above where the model {\it first} autoregressively generates an entire answer $y$ according to $\hat{p}(y|x)$ and {\it then} the entire $y$ is verified for correctness, in theorem proving we {\it must} obtain correct tactics $y[i]$, as verified by the proof assistant, at {\it every} proof step $i$. A straightforward application of DCO fine-tuning in this setting then involves replacing the model confidence $\hat{p}(y|x)$ in ~\cref{eq:DCO} on a single math answer $y$, with the model confidence on an {\it entire} successful, complete $k$ step proof $\hat{p}(y[0],...,y[k-1]|x[0])=\prod_{i=0}^{k-1}\hat{p}(y[i]|x[i])$. Because of the chain rule, the DCO gradient on a single proof has the same form as in~\cref{eq:DCO-gradients} but with $F(N,\hat{p}(y|x))$ replaced with $F(N, \prod_{i=0}^{k-1}\hat{p}\left(y[i]| x[i]\right))$. 

We naively applied this DCO fine-tuning method with $N=4$k and we evaluated the resulting model, as well as a baseline model trained with CE loss, on MiniF2F. As a test strategy, we used pass@4k. The baseline model fine-tuned with CE loss achieves a proof success rate of 37.4\%, while model fine-tuned with DCO at $N=4$k achieves a 38.7\% success rate. Thus, a naive application of DCO to theorem proving by matching the parameter $N$ in DCO to the parameter $N$ in pass@N achieves only a modest improvement.  

However, in theorem proving, since {\it every single} intermediate proof step tactic $y[i]$ must be valid, it is natural to consider a {\it step-wise generalization} of DCO. For example, at each step $i$, the model chooses a tactic $y[i]$ with confidence $\hat {p}(y[i]| x[i])$. Our proposed step-wise DCO optimizes this single-step confidence according to \cref{eq:DCO-gradients} as before, except now the {\it step-wise} confidence regularizer is given by $F(N_\text{eff}, \hat{p}(y[i]| x[i]))$.  Here one can think of $N_\text{eff}$ as a step-wise DCO hyperparameter that controls the exploration width at every proof step.  In essence, the confidence regularizer prevents the confidence of any chosen tactic from becoming much higher than $1/N_\text{eff}$. Since the sum of the confidences over all tactics at each step must be $1$, this means that at test time, model exploration at each step corresponds to searching on a search tree in which each proof state $x[i]$ allows the exploration of approximately only $N_\text{eff}$ tactics.  Thus using $N_\text{eff}$ in step-wise DCO during fine-tuning selects the approximate branching factor $N_\text{eff}$ of the model's proof search tree at test time (see ~\cref{fig:theoremproving}). 


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/theoremproving.pdf}}
\caption{\textbf{Inverse confidence $N_\text{eff}$ at training time controls search-tree exploration shapes at test-time.} We show, qualitatively, two proof search trees (solid lines) obtained when training at small $N_\mathrm{eff}$ (left) and large $N_\mathrm{eff}$ (right). Both trees start from the same theorem statement (top), circles denote proof states, directed edges denote tactics, solid edges are assigned non-negligible model confidence and are explored, while dashed edges are not.  Small $N_\mathrm{eff}$ leads to a small branching factor (left), which can fail if the correct proof (red path) is not in the narrow search tree. Larger $N_\mathrm{eff}$ expands the search tree to include the correct proof (right).  However, if $N_\mathrm{eff}$ is too large ($N_\mathrm{eff}^k\gg N$ for a theorem with $k$ proof steps), the sampling probability of the correct proof will decrease as $N_\mathrm{eff}^{-k}$ and the pass@N coverage $\mathcal{C}^N\approx N/N_\mathrm{eff}^k$ decreases as $N_\mathrm{eff}$ increases.  In essence the correct proof is too hard to find in a search tree that is too wide.
%
%
Overall, we see that inverse confidence $N_\mathrm{eff}$ chosen at training time constitutes a simple but powerful knob to control a tradeoff between exploitation and exploration on proof search trees of varying shapes at test time.}
\label{fig:theoremproving}
\end{center}
%
\vspace{-20pt}
\end{figure}



In particular, a valid (partial) proof of length $k$ will have probability of order $N_\text{eff}^{-k}$. Thus small $N_\text{eff}$ limits the exploration at test time to a tree with small branching factor, but allows longer proofs with larger numbers of steps $k$ to have higher sampling probability. This limited width search strategy should work well at test time using pass@N as long as two conditions hold: %
(1) a successful proof of length $k$ is present in the search tree of branching factor $N_\text{eff}$; and (2) the $N$ in pass@N at test time is large enough that this proof of probability $O(N_\text{eff}^{-k})$ is selected with high probability in $N$ passes (which starts to occur as soon as $N \gtrsim N_\text{eff}^k$).  

Conversely, larger $N_\text{eff}$ allows for more exploration at test time using a wider search tree of larger branching factor, but it makes finding longer proofs with large $k$ harder, since the approximate success condition for pass@N of $N>N_\text{eff}^{k}$ is harder to satisfy at fixed $N$ and large $N_\text{eff}$ and $k$.  However, this wide exploration strategy can work well for short proofs with small $k$ if a successful short proof does not lie in the limited width search tree obtained at small $N_\text{eff}$, but does lie in a wider search tree at larger $N_\text{eff}$ (\cref{fig:theoremproving}, right).
%


We note that the mean and median lengths of the proofs in the training set is 4.2 and 2 respectively.  Thus we explore fine-tuning on LeanDojo with the step-wise DCO algorithm DCO\textsuperscript{step} with $N_\text{eff}$ ranging from $1$ (corresponding to standard CE-loss training) to $N_\text{eff}=32$. 
We evaluated the pass@N test performance for $N=4$k of each of the resulting models on Mathlib and MiniF2F test sets, finding improved performance at larger $N_\text{eff}$ compared to the baseline $N_\text{eff}=1$ CE loss training (\cref{table:minif2f}). Interestingly, we find that the optimal $N_\text{eff}$ increases with more passes at test time (\cref{app:table:lean-mathlib}), similar to the Pareto frontier in~\cref{fig:DCOfrontier}.

Since different choices of $N_\text{eff}$ correspond to very different search strategies at test time, specializing in finding short proofs on wide search trees for large $N_\text{eff}$ and long proofs on narrow search trees for small $N_\text{eff}$, we hypothesized that ensembling these methods could significantly boost performance. We found this was indeed the case (ensemble entry in \cref{table:minif2f}).  Of course the Ensemble strategy samples $5\times4$k times, so a proper baseline is CE loss with the same test-time compute of pass@N with $N=20$k. We obtained the proof success rate for this baseline to be 57.0\% on Mathlib and 39.5\% on MiniF2F.  The ensemble strategy outperforms this stringent baseline with significant excesses of $5.2\%$ on Mathlib and $4.1\%$ on MiniF2F.   

Overall, these results indicate that varying $N_\text{eff}$ in step-wise DCO at training time allows a diversity of tree search strategies trading depth and breadth that can be effectively exploited by simply scaling test time compute via pass@N.

\begin{table}[t]
\caption{Proof success rate using pass@4k at test time on Mathlib and MiniF2F and training with DCO\textsuperscript{step} for different $N_\text{eff}$.}
\label{table:minif2f}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcr}
\toprule
 & Mathlib & MiniF2F \\
\midrule

$N_{\mathrm{eff}}=1$ (CE loss) & 55.6\% & 37.4\% \\
$N_{\mathrm{eff}}=4$ &  56.4\% & 39.0\%  \\
$N_{\mathrm{eff}}=8$ & 56.1\% & \textbf{39.5\%} \\
$N_{\mathrm{eff}}=16$ & \textbf{56.5\%}  & 37.0\% \\
$N_{\mathrm{eff}}=32$ & 55.8\%  & 37.4\% \\
\midrule
Ensemble of above $N_\mathrm{eff}$ & \textbf{62.2\%} & \textbf{43.6\%} \\
$N_\mathrm{eff}=1$ (5x test compute) & 57.0\%& 39.5\% \\

%
%
%
%

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
%
\vspace{-8pt}
\end{table}

\subsection{Approximate DCO also improves MATH answering with chain-of-thought reasoning}
\label{subsec:cot-online}
In \cref{subsec:dco-overconf} on solving problems in MATH, the model is trained to directly give an answer $y$ to a problem $x$. In this case, one can implement DCO at training time by explicitly computing the model confidence $\hat{p}(y|x)$ and using it in \cref{eq:DCO-gradients}. However, in an alternate powerful Chain-of-Thought (CoT) training paradigm, the training data consists of triplets $(x,c,y)$ where $x$ and $y$ are the problem and answer as before, but now $c$ is a CoT reasoning trace that explains how to derive $y$ from $x$. The model is trained on the triplet $(x,c,y)$, and at test time, when given a new problem $x'$, it generates a reasoning trace $c'$ and answer $y'$.  Importantly, the model is evaluated on whether its answer $y'$ is correct {\it independent} of the reasoning trace $c'$ it emits.  

Thus, to estimate the probability $\hat p(y|x)$ that the model assigns to any answer $y$, one can no longer compute it directly as in the direct answer case in \cref{subsec:dco-overconf}. Instead, one must marginalize over all possible reasoning traces $c$ to obtain $\hat{p}(y|x) = \sum_c \hat{p}(y,c|x)$. Because exact marginalization is intractable, to employ DCO in \cref{eq:DCO-gradients} in the CoT setting, we replace the marginalization with a Monte Carlo estimate of $\hat{p}(y|x)$, and insert this estimate into the overconfidence regularization factor $F$ in \cref{eq:DCO-gradients}. We call this algorithm approximate DCO, denoted by DCO\textsuperscript{a}. We also compute Monte Carlo estimates of the probability the model assigns to its most likely final answer, $\hat{p}(y^\mathrm{mode}|x)$, as a proxy for the greedy completion probability $\hat{p}(y^\mathrm{greedy}|x)$ used to study overconfidence in the direct answer setting in ~\cref{subsec:dco-overconf}. Note that in the CoT setting the greedy answer would correspond to a single {\it most} likely reasoning trace, but we want the probability of the most likely \textit{final} answer obtained by marginalizing over {\it all} reasoning traces.

We conduct experiments on the MATH dataset. For the baseline experiment, we first fine-tune a Llama-3-8B-base model with CE loss on the golden CoT solutions. We find in \cref{table:COT} that pass@1 coverage robustly improves over the course of CE loss training. Intriguingly, in the CoT setting, overfitting induced by the misalignment of CE loss at training time with the pass@N strategy at test time, is not as severe as in the direct answer setting in \cref{table:misalignment}. If overfitting indeed arises from the model overconfidence induced by CE loss, we would expect that minimizing CE loss in the CoT setting does not make the model as overconfident as it does in the direct answer case. This is indeed confirmed: compare \cref{fig:overconf-cot} left, in the CoT case, where the confidence of the modal answer shifts slightly to the right over epochs, to \cref{fig:overconfidence} leftmost, in the direct answer case, where it shifts much further right. 


%
%
%

%
%
%

We next explore the properties of DCO\textsuperscript{a} with $N=64$.  We find that training with DCO\textsuperscript{a} ($N'=64$) underperforms relative to training with CE loss for pass@1 at test time, but outperforms when the test strategy is pass@N for larger $N$ (\cref{table:COT}).  This is similar to the direct answer behavior in \cref{fig:DCOfrontier}. The largest improvement occurs when training with DCO\textsuperscript{a} at $N'=64$ is aligned with pass@N at $N=64$.  Furthermore, our theory in \cref{subsec:explainoverfitting} predicts that DCO\textsuperscript{a} training should lower model confidence relative to CE loss training, thereby explaining the improved pass@64 performance.  We confirm this prediction in \cref{fig:overconf-cot} right, which reveals that DCO\textsuperscript{a} successfully prevents the model from becoming overconfident in its most likely answer $y^\mathrm{mode}$. 

Overall these results once again validate our theory and algorithmic approaches, now in a CoT reasoning setting.  In essence, improved performance by scaling test time compute via a pass@N strategy at large $N$ can be best obtained by aligning the training strategy via choosing DCO\textsuperscript{a} with the same $N$, and the origin of the performance improvement comes from limiting model overconfidence. 

%
\begin{table}[t]
\caption{Pass@N test coverage on MATH obtained from fine-tuning Llama-3-8B-base on CoT traces using CE or DCO\textsuperscript{a} loss with $N=64$. $\pm$ error is standard error of the mean.}
\label{table:COT}
%
\vspace{-8pt}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{  %
\begin{tabular}{l c ccc}
\toprule
 &  & pass@1 & pass@16 & pass@64 \\
\midrule
\multirow{2}{*}{Epoch 1} & CE & 4.3 $\pm$ 0.1\%  & 30.2 $\pm$ 0.7\% & 51.2 $\pm$ 1.2\% \\
& DCO$^\mathrm{a}$ & \textbf{5.0} $\pm$ 0.1\% & \textbf{34.2} $\pm$ 0.6\% & \textbf{55.3} $\pm$ 1.0\% \\
\midrule
\multirow{2}{*}{Epoch 3} & CE & \textbf{9.0} $\pm$ 0.1\%  & 41.6 $\pm$ 0.4\% & 61.7 $\pm$ 0.4\% \\
& DCO$^\mathrm{a}$ & 8.3 $\pm$ 0.1\% & \textbf{42.6} $\pm$ 0.6\% & \textbf{63.1} $\pm$ 0.8\% \\
\midrule
\multirow{2}{*}{Epoch 5} & CE & \textbf{9.9} $\pm$ 0.1\%  & 40.9 $\pm$ 0.4\% & 60.9 $\pm$ 0.6\% \\
& DCO$^\mathrm{a}$ & 7.8 $\pm$ 0.1\% & \textbf{42.8} $\pm$ 0.5\% & \textbf{64.3} $\pm$ 0.6\% \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
%
\end{table}

\begin{figure}[ht]
%
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/Fig_thresh.pdf}}
%
\caption{\textbf{Overconfidence persists in CoT fine-tuning with CE loss, which DCO\textsuperscript{a} successfully limits.} We fine-tune a Llama-3-8B base model on the MATH dataset on CoT traces. We plot the distribution of the estimated model confidences $\hat{p}(y^\mathrm{mode}|x)$ over samples in the test set at various points in training. \textbf{Left:} for CE loss, the model becomes more confident in its most likely answers, as the confidence distribution shifts slightly to the right as training progresses. This effect is milder than in the direct answer setting (\cref{fig:overconfidence}). \textbf{Right:} for DCO\textsuperscript{a}, the rightward shift in the confidence distribution of the most likely answer over training time is limited; DCO\textsuperscript{a} again limits overconfidence relative to CE loss.}
\label{fig:overconf-cot}
\end{center}
%
%
%
\end{figure}

\raggedbottom
\section{Discussion}
\label{sec:discussions}
In summary, all our results suggest the need for a tight co-design of two traditionally separate phases of LLM development: (1) model training or fine-tuning and (2) test-time search/reasoning strategies and budget.  If the former is misaligned with the latter, then more training can actually {\it impair} performance gains from scaling test-time compute.  But if they are properly co-designed, end-to-end training and test time performance can be far more effective.  We have shown how to modify standard cross-entropy loss for training to be better aligned to a pass@N strategy for large N at testing.  Moreover, we have suggested and empirically confirmed why this co-design of training loss and pass@N strategy is essential, because optimal policies for pass@N at large N should unconfidently explore while the optimal policies for pass@N at small N should confidently exploit. 

This notion of co-design opens up many more theoretical and empirical questions associated with different test-time strategies such as self-verification and MCTS. Furthermore, one can go beyond test-time search to recursive self-improvement, where new solutions found by search are filtered and then used to retrain the model.  A theoretical understanding of the capabilities achievable by recursive self-improvement through co-design of training, search, and filtering, remains an outstanding research problem.          


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%



%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%

%
%
%
%
%

%
%
%
%

%
%
%

%
%
%
%
%
%
%
%

%
%
%

%
%
%
%

%
%
%
%


%

%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%

%

%

%




%
%
%
%
%
%
%

%
%

%

%
%
%
%
%

%
%

%
%
%
%
%
%
%

%

%
%
%
%

%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%

%
%
%
%
%
%
%

%

%
%
%

%

%
%
%

%
%
%

%
%
%
%

%

%

%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%

%
%
%
%
%

%
%
%

%

%
%
%
%
%
%
%
%

%
%

%
%

%
%
%
%
%
%
%


%
%
%
%
%
%

\section*{Acknowledgments}
We would like to thank Zhinan Cheng, ClÃ©mentine DominÃ©, Marco Fumero, David Klindt,  Daniel Kunin, Ben Sorscher and Atsushi Yamamura for helpful discussions.
%
S.G. thanks the James S. McDonnell Foundation, Simons Foundation, NTT Research, Schmidt Foundation and an NSF CAREER Award for support.
%
S.D. and F.C. were partially supported by the McKnight Foundation, the Simons Foundation and an NIH CRCNS grant R01DC020874.

%
%

\iffalse
\section*{Impact Statement}

The goal of this work is to advance mathematical reasoning abilities in large language models. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.
\fi


%
%
%

\bibliography{example_paper}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix
\onecolumn
\section{Proofs}
\subsection{Proof of approximately well calibration under optimal policy}
\begin{lemma}
\label{lemma:calibrated}
    Given any problem, let $\hat{p}_i$ denote the model probability or confidence assigned to answer $i$, and assume answers are sorted from highest to lowest confidence so that $\hat{p}_i \geq \hat{p}_{i+1} , \forall\,i\geq 1$. Let $p_i$ be the probability that answer $i$ is {\it actually} correct across all the problems. Then optimal policies maximizing pass@N coverage in \cref{eq:optimalstrategy} (denoted by $\hat{p}_{i}^{*}$) is approximately well calibrated, i. e. higher confidence implies higher or equal probability of being correct, i.e. $p_{i}\geq p_{i+1}, \forall\,i\geq 1$.
\end{lemma}
\begin{proof}
    Let $R$ be the total number of answers the model can generate. Here  $R$ could possibly equal infinity. Let $\hat{p}_{i}^{*}$ denote the model probability or confidence assigned to answer $i$ under optimal policy maximizing pass@N coverage in \cref{eq:optimalstrategy}.
    Let $A_{i}$ be the event that the $i$\textsuperscript{th} ranked answer is not chosen in the $N$ passes and $B_{i}$ be the event that $i$\textsuperscript{th} ranked answer is chosen in the $N$ passes. Let $1_{B_{i}}$ be the indicator function of $B_{i}$.
    Then, given an event $\omega$, the probability of getting the correct answer is $\sum_{i=1}^{R}p_{i}1_{B_{i}}(\omega)$. Hence, the expected probability of getting the correct answer is 
    \begin{equation}
        \mathbb{E}\left(\sum_{i=1}^{R}p_{i}1_{B_{i}}\right)=\sum_{i=1}^{R}p_{i}\mathbb{P}(B_{i})=\sum_{i=1}^{R}p_{i}(1-\mathbb{P}(A_{i}))=1-\sum_{i=1}^{R}p_{i}\mathbb{P}(A_{i})=1-\sum_{i=1}^{R}p_{i}(1-\hat{p}_{i}^{*})^{N},
    \end{equation}
    If the current strategy is already optimal, any small change of $(\hat{p}_{1}^{*},...,\hat{p}_{R}^{*})$ would not increase the expected probability of getting the correct answer. 
    Now suppose we don't always have $p_{i}\geq p_{i+1}$, in other words, $p_{j} < p_{j+1}$ for some $j\geq 1$. If $\hat{p}_{j}^{*}<\hat{p}_{j+1}^{*}$, we have
    \begin{equation}
        \mathbb{E}\left(\sum_{i=1}^{R}p_{i}1_{B_{i}}\right)<1-\sum_{i=1}^{j-1}p_{i}(1-\hat{p}_{i}^{*})^{N}-\sum_{i=j+1}^{R}p_{i}(1-\hat{p}_{i}^{*})^{N}-p_{j+1}(1-\hat{p}_{j}^{*})^{N}-p_{j}(1-\hat{p}_{j+1}^{*})^{N},
    \end{equation}
    where the last inequality says that we can increase the expected probability of getting the correct answer by swapping the confidence on answer originally labeled as $j$ and $j+1$, which is a contradiction. 
    If $\hat{p}_{j}^{*}=\hat{p}_{j+1}^{*}$, %
    then for all $\delta>0$ sufficiently small, we always have
    \begin{equation}
        \mathbb{E}\left(\sum_{i=1}^{R}p_{i}1_{B_{i}}\right)<1-\sum_{i=1}^{j-1}p_{i}(1-\hat{p}_{i}^{*})^{N}-\sum_{i=j+1}^{R}p_{i}(1-\hat{p}_{i}^{*})^{N}-p_{j}(1-\hat{p}_{j}^{*}-\delta)^{N}-p_{j+1}(1-\hat{p}_{j+1}^{*}+\delta)^{N},
    \end{equation}
    which also contradict with the current policy being optimal. 
    %
\end{proof}
\subsection{Proof of Lemma 4.2}
\begin{proof}
    Let $R$ be the total number of answers the model can generate. Here  $R$ could possibly equal infinity. Let $\hat{p}_{i}^{*}$ denote the model probability or confidence assigned to answer $i$ under optimal policy maximizing pass@N coverage in \cref{eq:optimalstrategy}.
    Let $A_{i}$ be the event that the $i$\textsuperscript{th} ranked answer is not chosen in the $N$ passes and $B_{i}$ be the event that $i$\textsuperscript{th} ranked answer is chosen in the $N$ passes. Let $1_{B_{i}}$ be the indicator function of $B_{i}$.
    Then, given an event $\omega$, the probability of getting the correct answer is $\sum_{i=1}^{R}p_{i}1_{B_{i}}(\omega)$. Hence, the expected probability of getting the correct answer is 
\begin{equation}\label{eq:expectedprobabilityofbeingcorrect}
    \mathbb{E}\left(\sum_{i=1}^{R}p_{i}1_{B_{i}}\right)=\sum_{i=1}^{R}p_{i}\mathbb{P}(B_{i})=\sum_{i=1}^{R}p_{i}(1-\mathbb{P}(A_{i}))=1-\sum_{i=1}^{R}p_{i}\mathbb{P}(A_{i})=1-\sum_{i=1}^{R}p_{i}(1-\hat{p}_{i})^{N}
\end{equation}
%
%
    %
%
%
%
    %
%
    Optimal strategy maximizes \cref{eq:expectedprobabilityofbeingcorrect}. Let $\pi_{1}(\{\hat{p}_{1}^{*},...,\hat{p}_{m}^{*}\})\equiv\hat{p}_{1}^{*}$ be the projection onto the first entry, then
\begin{equation}\label{eq:hatp1definition}
    \hat{p}_{1}^{*}=\pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{R}}{\text{argmin}}\{\sum_{i=1}^{R}p_{i}(1-\hat{p}_{i})^{N}\}\right).
\end{equation}
The proof is then motivated by the following intuition: given the constraint $\sum_{i=1}^{k}p_{i}\geq 1-\epsilon$, $\hat{p}_{1}^{*}$ attains its largest possible value when the $p_{j}\; (j\geq 2)$ are as small as possible.
It is worth noting that first
\begin{equation}
    \pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{R}}{\text{argmin}}\{\sum_{i=1}^{R}p_{i}(1-\hat{p}_{i})^{N}\}\right)\leq \pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{k}}{\text{argmin}}\{\sum_{i=1}^{k}p_{i}(1-\hat{p}_{i})^{N}+\sum_{i=k+1}^{R}p_{i}\}\right).
\end{equation}
Second, given $p_{1}$ and $\sum_{i=1}^{k}p_{i}\geq 1-\epsilon$, we have
\begin{equation}
    \pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{k}}{\text{argmin}}\{\sum_{i=1}^{k}p_{i}(1-\hat{p}_{i})^{N}+\sum_{i=k+1}^{R}p_{i}\}\right)\leq \pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{k}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+\sum_{i=2}^{k}\frac{1-p_{1}-\epsilon}{k-1}(1-\hat{p}_{i})^{N}+\sum_{i=k+1}^{R}p_{i}\}\right).
\end{equation}
Third, fix $\hat{p}_{1}$, Jensen's inequality tells us that
\begin{equation}
    p_{1}(1-\hat{p}_{1})^{N}+\sum_{i=2}^{k}\frac{1-p_{1}-\epsilon}{k-1}(1-\hat{p}_{i})^{N}\geq p_{1}(1-\hat{p}_{1})^{N}+(1-p_{1}-\epsilon)(1-\frac{1-\hat{p}_{1}}{k-1})^{N}.
\end{equation}
Combining all three inequalities above we have
\begin{equation}\label{eq:upperboundreducedtoonevariable}
    \hat{p}_{1}^{*}=\pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{k}}{\text{argmin}}\{\sum_{i=1}^{R}p_{i}(1-\hat{p}_{i})^{N}\}\right)\leq \underset{\hat{p}_{1}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+(1-p_{1}-\epsilon)(1-\frac{1-\hat{p}_{1}}{k-1})^{N}\}.
\end{equation}
The right hand side of \cref{eq:upperboundreducedtoonevariable} can be computed by taking derivatives, and we arrive at
\begin{equation}
    \hat{p}_{1}^{*}\leq\underset{\hat{p}_{1}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+(1-p_{1}-\epsilon)(1-\frac{1-\hat{p}_{1}}{k-1})^{N}\}=1-\frac{k-1}{(k-1)^{\frac{N}{N-1}} p_1^{\frac{1}{N-1}} \left(1-p_1-\epsilon\right){}^{\frac{1}{1-N}}+1}.
\end{equation}
\end{proof}

\subsection{Proof of Lemma 4.3}
\begin{proof}
The following proof is motivated by the following intuition: given the values of $p_{1}$ and $p_{2}$, $\hat{p}_{1}^{*}$ attains its smallest possible value when the $p_{j}\;(j\geq 3)$ are as large as possible.
Let $s$ be the smallest integer such that $p_{1}+sp_{2}\geq 1$.
We have,
\begin{equation}
    \hat{p}_{1}^{*}=\pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{R}}{\text{argmin}}\{\sum_{i=1}^{R}p_{i}(1-\hat{p}_{i})^{N}\}\right)\geq \pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{s+1}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+\sum_{i=2}^{s+1}p_{2}(1-\hat{p}_{i})^{N}\}\right).
\end{equation}
Second, for a fixed $\hat{p}_{1}$, Jensen's inequality tells us that
\begin{equation}
    p_{1}(1-\hat{p}_{1})^{N}+\sum_{i=2}^{s+1}p_{2}(1-\hat{p}_{i})^{N}\geq p_{1}(1-\hat{p}_{1})^{N}+sp_{2}(1-\frac{1-\hat{p}_{1}}{s})^{N}  
\end{equation}
Therefore, we have
\begin{equation}\label{eq:lowerboundminimization}
    \pi_{1}\left(\underset{\{\hat{p}_{i}\}_{i=1}^{s+1}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+\sum_{i=2}^{s+1}p_{2}(1-\hat{p}_{i})^{N}\}\right)=\underset{\hat{p}_{1}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+sp_{2}(1-\frac{1-\hat{p}_{1}}{s})^{N}\}
\end{equation}
Computing the right hand side of \cref{eq:lowerboundminimization} by taking derivatives we have
\begin{equation}\label{eq:hatp1lowerbound}
    \hat{p}_{1}^{*}\geq\underset{\hat{p}_{1}}{\text{argmin}}\{p_{1}(1-\hat{p}_{1})^{N}+sp_{2}(1-\frac{1-\hat{p}_{1}}{s})^{N}\}=1-\frac{sp_{1}^{\frac{1}{1-N}}p_{2}^{\frac{1}{N-1}}}{s+p_{1}^{\frac{1}{1-N}}p_{2}^{\frac{1}{N-1}}}.
\end{equation}
Since $s$ is the smallest integer such that $p_{1}+sp_{2}\geq 1$, we have
\begin{equation}\label{eq:boundfors}
    1+p_{2}>p_{1}+sp_{2},
\end{equation}
Combining \cref{eq:boundfors} and \cref{eq:hatp1lowerbound} we arrive at
\begin{equation}
    \hat{p}_{1}^{*}\geq 1-\frac{(1-p_{1}+p_{2})p_{1}^{\frac{1}{1-N}}p_{2}^{\frac{1}{N-1}}}{1-p_{1}+p_{2}+p_{1}^{\frac{1}{1-N}}p_{2}^{\frac{N}{N-1}}}.
\end{equation}
\end{proof}
\pagebreak
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%



\section{Experimental details}

All experiments with model size smaller than 10B are performed on machines with 8 NVIDIA H100 GPUs or 8 NVIDIA A100 GPUs. Experiments with model size larger than 10B are performed on machines with 8 NVIDIA H200 GPUs or 8 NVIDIA B200 GPUs. Our codebase uses PyTorch \cite{Ansel_PyTorch_2_Faster_2024}, Accelerate~\cite{accelerate}, and deepspeed (\url{https://github.com/microsoft/DeepSpeed}) to enable efficient training with memory constraints, and vllm \cite{kwon2023efficient} for efficient inference. Code will be made available \href{https://github.com/allanraventos/refine}{here}.
%



\textbf{MATH.} For experiments with MATH dataset, we fine-tune the Llama-3-8B-base~\cite{llama3} on the MATH~\cite{hendrycksMATH} dataset. We start from the base model rather than Llama-3-8B-Instruct to avoid potential leakage of the MATH dataset into Llama-3-8B-Instruct through post-training process. We follow \citet{prm8k} and use $12,000$ problems for training and the remaining $500$ for testing. In \cref{sec:misalignment,subsec:dco-overconf} and \cref{fig:overconfidence,fig:DCOfrontier}, we fine-tune the model for 4 epochs with a learning rate of 2e-5 and batch size 64. We adopt a linear learning rate warmup in the first 20 steps. For experiments with DCO, some of the data may have an extreme confidence regularizer value, if the model is already quite confident in answers to certain problems. To maintain an approximately fixed batch size, we set a threshold of $0.3$ on the confidence regularizer $F$. Any training data examples with an $F$ lower than this threshold will be replaced with new training examples to construct the batch. 
%
For the CoT experiments in \cref{subsec:cot-online}, we use learning rate 2e-5 and batch size 128, with the same learning rate warmup.

\textbf{Theorem proving.} We adopt the random train and test split as introduced in \citet{leandojo}. The random test set includes 2,000 theorems. We fine-tune the model Qwen2.5-Math-1.5B~\cite{QwenMath2.5} on the training set for 3 epochs with learning rate 1e-5 and batch size 64. We adopt a linear learning rate warmup in the first 20 steps. To evaluate the model, we use LeanDojo~\cite{leandojo} to interact with the proof assistant. We impose a maximum wall clock time for each theorem, in addition to limiting the number of passes per problem. For experiments with 4k passes, the time budget is fixed at 5,000 seconds. In order to avoid the model going infinitely deep in the search tree, we limit the number of proof steps to be at most 50.


\textbf{DCO\textsuperscript{a} objective.} The DCO\textsuperscript{a} introduced in~\cref{subsec:cot-online} is an approximation for DCO. %
To construct a batch of size $B$ with DCO\textsuperscript{a}, we process batches of samples sequentially; for each batch, we run online inference on each of the samples and discard all samples with probability of success rate larger than $p^\mathrm{thresh}$. We choose to discard samples which have a DCO confidence reguarizer $F$ lower than 0.01, corresponding to $p^\mathrm{thresh}=0.1$ for $N'=64$. This process continues until we have enough training data for a single batch. In~\cref{fig:discarded-cot}, we plot the number of discarded samples as a function of training step for our DCO\textsuperscript{a} experiments (same ones as in~\cref{table:COT}). %

\begin{figure}[ht]
%
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figures/Fig_discarded_samples.pdf}}
\caption{Number of discarded samples as a function of training step for the DCO\textsuperscript{a} experiments. The step structure reflects the model revisiting examples it has seen previously in training, where each step closely matches the start of a new epoch.}
\label{fig:discarded-cot}
\end{center}
\vspace{-20pt}
\end{figure}

\textbf{Implementing online inference.}
Throughout our experiments and analysis, we extensively use the open-source vllm package \cite{kwon2023efficient} for efficient inference. Integrating inference into the training loop to enable training under the DCO\textsuperscript{a} objective, as in \cref{subsec:cot-online}, poses a challenging implementation problem. We solve this problem by placing vllm worker processes, which together perform inference on all GPUs, in a separate process group and use Ray (\url{https://github.com/ray-project/ray}) to isolate them from the training loop. This enables running concurrent training and inference on the same set of GPUs. We believe this inference in the loop setup will be useful to the community, as it enables straightforward implementation of online data filtering approaches for LLM training.

\textbf{Computational cost.}
For both DCO and DCO\textsuperscript{step}, the additional computational cost is minimalâlimited to computing the DCO factor, which is negligible compared to the cost of forward and backward passes.

However, DCO\textsuperscript{a}, when applied to fine-tuning with CoT traces, introduces significant overhead due to the need for online Monte Carlo estimation. In the setting of Llama-3 8B, with 32 layers, hidden dimension 4096, context length 1024, and batch size 128, we estimate 5 petaflops for each forward-backward pass. Generating $N_{MC}=64$ CoT's for each of 128 examples, in turn, requires approximately 50 petaflops. The actual inference compute cost depends on model performance: stronger models, which will discard more samples, will need to process more samples in order to construct a full batch for the next training step. Therefore there is a tradeoff: constructing a batch of 128 \textbf{hard} samples requires running inference on more samples, but also means fewer steps are required to make a full pass through the dataset.

The memory overhead is approximately 30\% of GPU memory, when running one inference engine per pair of H100 80GB GPUs. We note that while the flop overhead is significant, costly online inference is also required in RL frameworks for CoT training. Furthermore, the choice of $N_MC$ can be made smaller, leading to less precise estimates of the model's probability of success on a given problem, while speeding up training.

\pagebreak
\section{Additional results}

\subsection{Optimality of DCO is achieved when the training objective aligns with the test objective.}

\begin{figure}[ht]
%
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/Fig.1_DCO_frontier_256_highlight.pdf}}
\caption{\textbf{Pareto-optimality at a given test strategy pass@N for some $N$ is obtained by DCO training for some $N'$ close to $N$.} Same as~\cref{fig:overconfidence} but with $N'=256$ highlighted. We fine-tune Llama-3-8B base models on the MATH dataset to produce direct answers. We fine-tune for 4 epochs one model using CE loss and several models under the $\mathcal{L}_\text{DCO}^{N'}$ objective, for choices of $N'$ indicated by color. Note that there is no choice of $N'$ that is optimal across all $N$ (different colors are higher at different $N$). The black curve is a Pareto-optimal performance frontier traced by the max of coverage curves for DCO over all $N'$. Pareto-optimality at a given test strategy pass@N for some $N$ on the x-axis is obtained by DCO training for some $N'$ close to $N$.}
\label{fig:DCOfrontier_256_highlight}
\end{center}
%
\vspace{-10pt}
\end{figure}

\clearpage

\subsection{Addition results with Llama-3-70B-base}
\label{sec:supp:70B}
In this section, we present additional results using the larger Llama-3-70B-base model, which are consistent with the findings reported for Llama-3-8B-base.

\begin{table}[h]
\caption{Same as~\cref{table:misalignment} but with Llama-3-70B-base model. Pass@N coverage metric on the MATH test set for a Llama-3-70B-base model fine-tuned with CE loss on direct answers from the MATH training set. Surprisingly, Pass@N test accuracy at large $N$ decreases with number of training epochs.}
\label{table:misalignment-70B}
\vskip 0.15in
%
\begin{center}
\begin{small}
\begin{sc}

\begin{tabular}{lcccr}
\toprule
 & pass@1 & pass@16 & pass@256 & pass@4k \\
\midrule
Epoch 1 &5.9\% & \textbf{34.3\%} & \textbf{68.8\%} & \textbf{84.2\%} \\
Epoch 2 &6.9\% & 32.0\% & 63.0\% & 81.5\%\\
Epoch 3 &\textbf{8.2\%} & 24.2\% & 48.8\% & 72.2\%\\
Epoch 4 &7.9\% & 22.2\% & 44.2\% & 65.9\%\\
\bottomrule
%
%
%
\end{tabular}
\end{sc}
\end{small}
\end{center}
%
\vspace{-10pt}
\end{table}

\begin{figure}[H]
%
\begin{center}
\centerline{\includegraphics[width=0.95 \textwidth]{figures/Fig_CE_DCO_70B.pdf}}
\caption{\textbf{Larger-sized model trained with CE loss also becomes overconfident in its greedy completions; our proposed DCO objective limits this overconfidence on larger-sized models.} Same as~\cref{fig:overconfidence} but with Llama-3-70B-base. We fine-tune a Llama-3-70B-base model on the MATH dataset to produce direct answers without a reasoning trace. $\hat{y}_\text{greedy}$ is the model's greedy completion when sampling autoregressively and choosing the most likely token at each step. \textbf{Leftmost:} The model trained with CE loss assigns progressively larger confidences $\hat{p}(\hat{y}_\text{greedy}|x)$ to its greedy completions over the course of training. \textbf{Left:} At the end of the training, only a small portion of the model's highly confident completions are correct. This will harm the model's pass@N performance when scaling up $N$. \textbf{Right:} Same as leftmost but shown for the DCO loss with $N=256$.  Relative to the CE loss, the model trained on DCO shows a much milder overconfidence effect.  \textbf{Rightmost:} The confidence distribution of the greedy completions after four epochs with DCO for various choices of $N$. As $N$ increases, the model's confidence on the greedy completion is more stringently limited, directly as a consequence of the overconfidence regularizer $F$.}
\label{fig:overconfidence_70B}
\end{center}
\vskip -0.6in
\end{figure}

\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figures/Fig.1_DCO_frontier_70B.pdf}}
\caption{\textbf{DCO improves on CE for pass@N test coverage over a broad range of N and traces a Pareto-optimal frontier with Llama-3-70B-base.} Same as~\cref{fig:DCOfrontier} but with Llama-3-70B-base model. We fine-tune Llama-3-70B base models on the MATH dataset to produce direct answers. We fine-tune for 4 epochs one model using CE loss and several models under the $\mathcal{L}_\text{DCO}^{N'}$ objective, for choices of $N'$ indicated by color. We plot pass@N test coverage as a function of $N$, with each curve (solid red or faint blue-green) corresponding to {\it one} fine-tuned model. Note that there is no choice of $N'$ that is optimal across all $N$ (different colors are higher at different $N$). The black curve is a Pareto-optimal performance frontier traced by the max of coverage curves for DCO over all $N'$. Pareto-optimality at a given test strategy pass@N for some $N$ on the x-axis is obtained by DCO training for some $N'$ close to $N$.}
\label{fig:DCOfrontier_70B.}
\end{center}
%
\vspace{-10pt}
\end{figure}



\subsection{Out-of-distribution evaluation}
\label{sec:supp:AIME}

In this section, we present additional evaluation results on AIME24. We confirm that overconfidence persists even on the out-of-distribution test set, and that the DCO algorithm saves test-time scaling by regularizing the max confidence.

\begin{table}[h]
\caption{Same as~\cref{table:misalignment} but evaluate on AIME24 dataset. Pass@N coverage metric on the AIME24 dataset for a Llama-3-8B-base model fine-tuned with CE loss on direct answers from the MATH training set. Surprisingly, Pass@N test accuracy at large $N$ decreases with number of training epochs.}
\label{table:misalignment-AIME}
%
%
\begin{center}
\begin{small}
\begin{sc}

\begin{tabular}{lcccr}
\toprule
 & pass@1 & pass@16 & pass@256 & pass@4k \\
\midrule
Epoch 1 &0.2\% & 3.4\% & 30.5\% & \textbf{80.5\%} \\
Epoch 2 &0.3\% & 4.1\% & \textbf{31.7}\% & 75.6\%\\
Epoch 3 &0.2\% & 2.9\% & 16.0\% & 42.0\%\\
Epoch 4 &\textbf{1.9}\% & \textbf{4.9}\% & 15.2\% & 35.8\%\\
\bottomrule
%
%
%
\end{tabular}
\end{sc}
\end{small}
\end{center}
%
\vspace{-10pt}
\end{table}

\begin{figure}[H]
%
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{figures/Fig_CE_DCO_AIME.pdf}}
\caption{\textbf{A model trained with CE loss becomes overconfident in its greedy completions on out-of-distribution test set; our proposed DCO objective limits this overconfidence on out-of-distribution test set.} Same as~\cref{fig:overconfidence} but evaluated on AIME24. We fine-tune a Llama-3-8B-base model on the MATH dataset to produce direct answers without a reasoning trace and evaluate the model on AIME24. $\hat{y}_\text{greedy}$ is the model's greedy completion when sampling autoregressively and choosing the most likely token at each step. \textbf{Leftmost:} The model trained with CE loss assigns progressively larger confidences $\hat{p}(\hat{y}_\text{greedy}|x)$ to its greedy completions over the course of training. \textbf{Left:} At the end of the training, only a small portion of the model's highly confident completions are correct. This will harm the model's pass@N performance when scaling up $N$. \textbf{Right:} Same as leftmost but shown for the DCO loss with $N=256$.  Relative to the CE loss, the model trained on DCO shows a much milder overconfidence effect.  \textbf{Rightmost:} The confidence distribution of the greedy completions after four epochs with DCO for various choices of $N$. As $N$ increases, the model's confidence on the greedy completion is more stringently limited, directly as a consequence of the overconfidence regularizer $F$.}
\label{fig:overconfidence_AIME}
\end{center}
\vskip -0.6in
\end{figure}

\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figures/Fig.1_DCO_frontier_AIME24.pdf}}
\caption{\textbf{DCO improves on CE for pass@N test coverage over a broad range of N and traces a Pareto-optimal frontier on out-of-distribution test set.} Same as~\cref{fig:DCOfrontier} but evaluated on AIME24. We fine-tune Llama-3-8B base models on the MATH dataset to produce direct answers and evaluate on AIME24. We fine-tune for 4 epochs one model using CE loss and several models under the $\mathcal{L}_\text{DCO}^{N'}$ objective, for choices of $N'$ indicated by color. We plot pass@N test coverage as a function of $N$, with each curve (solid red or faint blue-green) corresponding to {\it one} fine-tuned model. Note that there is no choice of $N'$ that is optimal across all $N$ (different colors are higher at different $N$). The black curve is a Pareto-optimal performance frontier traced by the max of coverage curves for DCO over all $N'$. Pareto-optimality at a given test strategy pass@N for some $N$ on the x-axis is obtained by DCO training for some $N'$ close to $N$.}
\label{fig:DCOfrontier_AIME}
\end{center}
%
\vspace{-10pt}
\end{figure}



\subsection{Theorem proving}
In~\cref{app:table:lean-mathlib}, we show additional results for model performance under the DCO\textsuperscript{STEP} objective. We find that the optimal $N_\text{eff}$ grows with increasing passes, agreeing with results in \cref{subsec:dco-overconf,subsec:cot-online}. We also conduct expert iteration~\cite{expert_it,expert_it_formal_math} on Mathlib with theorems that do not have proof traces in the training set. We use pass@1k to prove those theorems. We find that our algorithm achieves a stronger improvement over the baseline for pass@4k after the 1\textsuperscript{st} iteration. This improvement might result from the fact that the models can prove more easy theorems where the model has a higher confidence. As a result, we believe our method will perform better with expert iteration.

\begin{table}[ht]
\caption{Success rate on lean-dojo benchmark random test set trained with DCO\textsuperscript{step}.}
\label{app:table:lean-mathlib}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccr}
\toprule
 & \multicolumn{5}{c}{Expert iteration 0} & \multicolumn{3}{c}{Expert iteration 1} \\
%
%
 DCO\textsuperscript{step}  & pass@16  & pass@64 & pass@256 & pass@1k & pass@4k & pass@16  &pass@256 & pass@4k \\
\midrule
$N_\mathrm{eff}=1$ (CE) & \textbf{30.0\%} & 38.75\% &46.05                  &50.75\%                & 55.55\%             & 40.3\%  & 52.65\% &58.55\% \\
$N_\mathrm{eff}=4$ & \textbf{30.15}\% &\textbf{39.5\%} & \textbf{47.2\%}    & \textbf{52.95\%}      & 56.35\%             & \textbf{40.8\%}   & \textbf{53.05\%}        & 59.45\% \\
$N_\mathrm{eff}=8$ &  \textbf{30.2}\%  & 38.9\% & \textbf{47.15\%}          & \textbf{52.7\%}       & 56.1\%              & 40.1\%     & \textbf{53.25\%}       &59.5\% \\
$N_\mathrm{eff}=16$ & 28.65\% & 46.7\% & 46.45\%                            & \textbf{52.9\%}       & \textbf{56.5\%}     & 39.05\%     & 52.8\%       & \textbf{60.05\%}\\
$N_\mathrm{eff}=32$ & 26.05\% & 46.7\% & 45.6\%                             & 51.5\%                & 55.8\%              & 37.05\%    & 52.2\%        & 59.15\%\\
Ensemble & 40.6\% & 49.15\% & 54.6\%                                        &  59.0\%               & 62.15\%             & 49.05\%    & 59.3\%        &64.8\%\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Plot of the upper bound and the lower bound}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figures/theorem.pdf}}
\caption{\textbf{Plot of the upper bound (\cref{eq:upperboundreducedtoonevariable}) and the lower bound (\cref{eq:hatp1lowerbound}).} We plot the upper bound (blue) and lower bound (orange) for $p_{1}=\frac{1}{2}$, $p_{2}=\frac{1}{4}$, $\epsilon=\frac{1}{4}$ and $k=2$. Both the upper bound and the lower decrease monotonically in $N$ and they both tends to $1$ as $N\to 1^{+}$.}
\label{fig:theorem}
\end{center}
\vspace{-20pt}
\end{figure}
%

\subsection{Empirical evaluation of the approximately well calibrated assumption}
\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/Approx_well_calibrated.pdf}}
\caption{\textbf{We empirically verify the assumption that models are approximately well-calibrated.} \textbf{Left}: We fine-tune a Llama-3-8B-base model with CE loss on the MATH dataset \textbf{without} CoT and perform beam search with a width of 256 to obtain the top 16 most probable completions. We then measure the accuracy of these completions at each confidence rank. \textbf{Right}: We fine-tune a Llama-3-8B-base model with CE loss on the MATH dataset \textbf{with} CoT and perform estimate the top 16 most frequent answers after tracing out the reasoning traces. We then measure the accuracy of these answers at each frequency rank. The results demonstrate that test accuracy decreases approximately monotonically with model confidence rank, supporting the assumption.}
\label{fig:well-calibrated}
\end{center}
\vspace{-20pt}
\end{figure}

\subsection{The data dependency of confidence and factor $F$}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figures/p_factor_plot.pdf}}
\caption{\textbf{Model is more confident on easy problems; DCO improves test-time scaling by regularizing the easy examples.} We fine-tune a Llama-3-8B-base
model on the MATH dataset with CE loss and plot the model confidence $p(y|x)$ and the factor $F(N,p(y|x))$ at 86\% of the first training epoch. Both model confidence and factor are evaluated on the \textbf{unseen} data grouped by difficulty level from the MATH dataset. Model confidence decreases with increasing difficulty, whereas the regularization factor increases with problem difficulty. As a result, DCO effectively regularizes contribution from easy examples. This regularization mitigates the potential detrimental effects of overconfidence from easy examples as discussed in \cref{subsec:easydata}. We use $N=256$ corresponding to pass@256 for the plots. }
\label{fig:data_dependency}
\end{center}
\vspace{-20pt}
\end{figure}


%
%
%

%
%
%


\end{document}


%
%
%
%
%
%
%
%
%
%
%
%
%
