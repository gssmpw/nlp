\section{Related Works}
\label{sec:relatedworks}

%
%

%

\textbf{Test-time compute and pass@N strategy.} 
%
%
%
A growing body of work explores how training and test-time strategies interact to affect model performance.
____ highlights a tradeoff between train and test-time compute in a controlled board game setting.
%
Closely related to our work, ____ identify an exponentiated power law between coverage and the number of in-context samples. ____, in turn, explore compute-optimal strategies for scaling test-time compute. Our paper focuses primarily on the pass@N test-time strategy.
____ show that, in the best-of-N setting, the sample distribution is nearly optimally aligned with the reward model. To mitigate the excessive test-time compute costs associated with best-of-N sampling, ____ propose alignment algorithms to distill the sample distribution. ____ argue that fine-tuning with cross-entropy loss can limit output diversity and propose a maximum entropy-based method to address this issue. Our work similarly identifies limitations of cross-entropy when scaling test-time compute, but focuses on the alignment of training and test-time strategies. ____ explore methods for preventing catastrophic forgetting in supervised fine-tuning over diverse domains. 

%
\textbf{Post-training for mathematical reasoning.}
Various post-training techniques have been proposed to improve mathematical reasoning in LLMs. Instruction-tuning and reinforcement learning with human feedback have been shown to boost model performance on math____, while continued training on math- or code-specific domain data enhances models' reasoning abilities for downstream mathematical tasks____.
%
%
%
Rejection-sampling____ and self-improvement techniques____, in turn, are useful to augment the training data for SFT.
%
More recent approaches____ have incorporated reinforcement learning and achieved exceptional reasoning capabilities in various domains, such as math and coding.
%
Although our paper primarily focuses on supervised fine-tuning to enhance pretrained models' math capabilities, our loss function can be applied to other settings that train under CE loss, such as continual training, instruction-tuning, and data augmentation.
%
%
%
%
%
%
%
%
%
%
%
%


\textbf{Data pruning and hard example mining.} The interpretation of the loss function we derive below can be related to data pruning and hard example mining (see discussion of sample difficulty in \cref{subsec:easydata}). Data selection is often applied to curate high-quality datasets for pretraining____, where ____ shows that pruning easy samples can improve pretraining loss scaling as a function of dataset size. ____ demonstrate that supervised fine-tuning on even a very small dataset can yield remarkably strong performance in alignment and reasoning tasks.
%
On the other hand, hard example mining focuses on identifying and emphasizing challenging samples to improve model performance____.
%
In the domain of mathematical reasoning, ____ find a difficulty imbalance in rejection-sampled datasets and show that more extensive training on difficult samples improves model performance.
%

Our paper is closely related to a concurrent paper by ____, which derives a similar training objective for RL to directly optimize for the best-of-N test-time strategy.