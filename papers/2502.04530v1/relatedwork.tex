\section{Related Work}
\label{secRelated}
\noindent \textbf{Moment Matching Approximation.} Moment matching is a classical method for approximating density functions~\cite{john2007techniques, mnatsakanov2009recovery}. The main challenge of density approximation methods is to select an appropriate foundational distribution to represent the unknown distribution. Existing methods have focused on using powerful kernel density function~\cite{gavriliadis2012truncated} or Erlang mixtures~\cite{johnson1989matching, he2022continuous}, given their capability in recovering a wide variety of distributions. However, the accuracy of these approximations depends heavily on how well the selected function or kernel aligns with the underlying distribution. We utilize the Erlang mixtures, not only due to their flexibility but also because of its inherent connection to Markov models, which allows us to approximate the reward distribution with bounded error.

\vspace{1mm}
\noindent \textbf{Robust Model Checking.} Robustness is one of the most important concerns in model checking methods. Some simulation-based approaches provide probabilistic bounds with confidence guarantees of model correctness~\cite{herault2004approximate, pappagallo2020monte}. There are also several methods that provide evaluation beyond expected value with distributional information, including the quantile-based method~\cite{baier2014probabilistic} and histogram approximation method~\cite{elsayed2024distributional}, but could lead to unbounded information loss due to the coarse approximation. In contrast, our approach provides smoother approximation while avoiding discretization errors and offering guaranteed accuracy in both discrete and continuous domains.

\vspace{1mm}
\noindent \textbf{Distributional Reinforcement Learning.} Recent work in distributional reinforcement learning has focused on estimating the full return distribution instead of just its expectation. a categorical histogram approximation method has been prosed in~\cite{bellemare2017distributional} to compute return distributions using discrete bins, optimizing the Wasserstein distance between estimated and target distributions. Extensions of this work, such as quantile regression (QR-DQN)~\cite{dabney2018distributional} and implicit quantile networks (IQN)~\cite{dabney2018implicit}, improve on this by learning the quantile-based approximation, offering greater flexibility and risk-sensitive policies. Our method can be further extended to the RL domain, providing precise estimation and flexibility in continuous reward environments than histogram methods, with a robust verification that can be used as guidance for distributional policy iteration.