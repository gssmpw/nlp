\section{Related Work}
\vspace{-0.1cm}
% In this section, we will provide a brief review of the literature on four different topics that are related to our study.
\vspace{-0.1cm}
\subsection{Rumor Detection} 

Early research on automatic rumor detection focused on pre-defined features or rules~\cite{middleton2016geoparsing} to train supervised classification models. To circumvent tedious feature engineering, subsequent studies proposed data-driven methods based on neural network models~\cite{lin2021rumor, sun2022ddgcn, lin2022detect} and multi-task learning frameworks~\cite{ma2018detect,wei2019modeling,luo2024joint}. Advanced approaches were then proposed to leverage propagation information~\cite{lu2020gcan, bian2020rumor,yu2020coupled,ma2021improving, lin2023zero}. 
% For instance, \citet{lu2020gcan, yu2020coupled, bian2020rumor} exploited graph-based attention networks in the rumor detection task. 
% Recent advancements such as Adversarial Contrastive Learning~\cite{} have further enriched the field of rumor detection. 
% For example, \citet{he2021rumor} utilized event augmentation, while \citet{ma2021improving} and \citet{wu2023adversarial} employed Adversarial Contrastive Learning methods. 
% \citet{lin2023zero} incorporated prompt learning into the rumor debunking model, and both \citet{khoo2020interpretable} and \citet{ma2020debunking} applied transformer-based methods to strengthen the representation of rumor propagation trees, considering fine-grained post relations.
In this study, we formulate our approach based on a propagation structured model, with a specific emphasis on deducing post-level stances using weak supervision with only claim-level annotation. Drawing inspiration from the efficacy of propagation information, our methods are designed based on graph structure for incorporating social context information.

\subsection{Stance Detection} 

Manual analysis of stance has revealed a close relationship between specific veracity categories and stances~\cite{mendoza2010twitter}. Subsequent studies explored various hand-crafted features~\cite{lukasik2016hawkes} 
% or Gaussian process~\cite{lukasik2019gaussian} 
for training stance detection models. More recently, deep neural networks have been employed for stance representation learning and classification~\cite{zhang2019stances,zhao2020pretrained,kochkina2017turing,wei2019modeling,liang2022jointcl}.
% aiming to mitigate the need for feature engineering and enhance generalizability
% Some studies considered conversation structure, including tree-structured models~\cite{zubiaga2016stance,kochkina2017turing} and multi-task frameworks for the joint detection of rumors and stances~\cite{wei2019modeling}. 
% Research has also incorporated contrastive learning for stance detection~\cite{liang2022jointcl}. 
% Presently, multi-target stance detection has gained attention from researchers. \citet{liang2021target} introduced a target-adaptive graph, while \citet{li2023tts} utilized a teacher-student model for the stance detection task. However, multi-target stance detection usually operates on clear targets, which does not align with the messy information characteristics found on real-world social media. Additionally, it often requires datasets annotated with stance labels specific to particular targets.
However, the aforementioned methods typically require a substantial stance corpus annotated at the post level for training. While several unsupervised models based on pre-defined models~\cite{kobbe2020unsupervised,allaway2020zero} and unsupervised structural embedding~\cite{DBLP:conf/aaai/RanJ23} for stance detection exist, their generalizability is consistently a critical concern due to their specific problem settings, reliance on manually crafted features.
% and the underlying models pre-trained for certain topics or tasks. 
In this paper, we propose a weakly supervised propagation model to concurrently classify rumor veracity and post stances on social media without the need for post stance labels. 

\subsection{Multiple Instance Learning (MIL)}

MIL is a form of weakly supervised learning designed to train a classifier with coarse-grained bag-level annotations to assign labels to fine-grained instances arranged in the bag~\cite{dietterich1997solving}. 
% Subsequent research in the MIL domain introduced several variants by extending threshold-based, count-based, and weighted collective MIL assumptions to deduce the bag-level label~\cite{foulds2010review}. 
In recent years, MIL has proven effective and found successful applications in various Natural Language Processing (NLP) tasks~\cite{yang-etal-2023-wsdms}.
% such as user satisfaction analysis in Community Question Answering (CQA)~\cite{chen2017user},
% customer services of E-Commerce~\cite{song-etal-2019-using}, text segment-level models for sentiment analysis~\cite{angelidis2018multiple}, 
% fashion outfit recommendation~\cite{lin2020outfitnet}, and recently misinforming sentence detection in fake news debunking~\cite{yang-etal-2023-wsdms}. 
However, the original MIL framework is specifically designed for the binary classification of instances lacking complex structures, and bag-level labels are expected to conform to instance-level labels. 
% Our primary challenge lies in devising a general framework capable of addressing the multi-class problem and overcoming incompatibility between bag and instance-level labels. 
In our work, we study an LLM-enhanced MIL framework to transform the multi-class problem into multiple binary classifiers, resolving the label incompatibility between bag and instance levels.

\subsection{Large Language Models (LLMs)}
Generative Pre-trained Transformers (GPTs) are a class of LLMs that have revolutionized artificial intelligence in recent years. 
% Nowadays, LLMs are incredibly flexible, a single model can perform a wide range of tasks, including answering questions, summarizing documents, and translating languages. 
The significant scientific advancements made by models such as ChatGPT~\cite{NEURIPS2020_1457c0d6}, LLaMA~\cite{touvron2023Llama}, and ALPACA~\cite{taori2023alpaca} have gained researchers' attention in the field of rumor detection and stance detection. ~\citet{yan2024enhancing} and ~\citet{yang-etal-2024-reinforcement} leveraged LLM to enhance the rumor detection, 
% ~\citet{yang-etal-2024-reinforcement} proposed a reinforced tuning framework based on LLM for joint stance and rumor detection,
~\citet{lan2024stance} and ~\citet{gambini2024evaluating} utilized LLM for stance detection.
% ~\citet{gambini2024evaluating} utilized an unsupervised framework with LLM for zero-shot stance detection. 
These methods primarily focus on directly predicting rumor or stance types using LLMs. %However, as generative models, the classification capabilities of LLMs remain relatively underexplored. So,
% Generative Pre-trained Transformers (GPTs) are a class of LLMs that have revolutionized artificial intelligence in recent years. Nowadays, LLMs are incredibly flexible, a single model can perform a wide range of tasks, including answering questions, summarizing documents, and translating languages. The significant scientific advancements made by models such as ChatGPT~\cite{NEURIPS2020_1457c0d6}, LLaMA~\cite{touvron2023Llama}, and ALPACA~\cite{taori2023alpaca} have gained researchers' attention, highlighting the importance of providing high-quality data to achieve accurate results with LLMs. LLMs demonstrate a remarkable ability to make predictions based on relatively small amounts of prompts or inputs and they have many applications such as article writing, question answering, code generation, text generation, and rumor verification~\cite{luo2024message,yang-etal-2024-reinforcement} 
% \textcolor{blue}{including rumor verification, you can add some LLM-based rumor detection, including our ACL paper.} 
In this paper, %we resort to LLM's text generation ability to generate explanations for posts's stances, aiming to make the stances more reliable for the final rumor detection.
we leverage the generative capacity of LLMs to produce explanations for post stances towards claim veracity, aiming to enhance the reliability and explainability of these stances for final rumor detection. %\wei{This paragraph is too general. You need to specifically review the methods that explicit leveraging LLMs for rumor/stance tasks}


% \subsection{Transformer Networks}

% The transformer architecture has attracted increasing attention and demonstrated remarkable efficacy in sequence encoding and decoding tasks. A standard transformer architecture comprises encoder and decoder components. The key distinction lies in the decoder's use of masked self-attention to prevent tokens from attending to their future positions~\cite{vaswani2017attention}. In this paper, we opt for the transformer encoder as our backbone.
% % as Figure~\ref{fig:TransEncoder} shows, 
% Diverging from previous sequence transduction models, each encoder has a stack of $N$ identical modules, discarding recurrence and convolution structures in favor of a multi-head self-attention module and position-wise feed-forward network (FFN). 
% % As Figure~\ref{fig:TransEncoder} shows, 
% Residual connections surround each module, followed by a normalization module. Notably, self-attention modules are employed to ensure that related words can be combined irrespective of their position. 
% Recent work has accentuated the transformer architecture's ability to effectively learn context-sensitive word representations, proving beneficial for various NLP tasks, such as pre-training word representation~\cite{}, question answering~\cite{zhang2021retrospective}, machine translation~\cite{liu2020multilingual}, and rumor detection~\cite{khoo2020interpretable}. Motivated by the success of the transformer in producing dense contextual embeddings, we develop an LLM-based rumor detection architecture to model the propagation structure of microblog posts and incorporate novel hierarchical attention mechanisms to jointly predict rumor veracity and stance labels.