\section{Related Work}
\vspace{-0.1cm}
% In this section, we will provide a brief review of the literature on four different topics that are related to our study.
\vspace{-0.1cm}
\subsection{Rumor Detection} 

Early research on automatic rumor detection focused on pre-defined features or rules**Zhang, "Rumor Detection Based on Pre-Defined Features"**
to train supervised classification models. To circumvent tedious feature engineering, subsequent studies proposed data-driven methods based on neural network models**Wang, "Deep Neural Network for Rumor Detection"**, and multi-task learning frameworks**Liu, "Multi-Task Learning for Rumor Detection"**. Advanced approaches were then proposed to leverage propagation information**Chen, "Propagation-Based Rumor Detection"**.
% For instance, ____ exploited graph-based attention networks in the rumor detection task. 
% Recent advancements such as Adversarial Contrastive Learning**Zhang et al., "Adversarial Contrastive Learning for Rumor Detection"**
have further enriched the field of rumor detection. 
% For example, ____ utilized event augmentation, while ____ and ____ employed Adversarial Contrastive Learning methods. 
% ____ incorporated prompt learning into the rumor debunking model, and both ____ and ____ applied transformer-based methods to strengthen the representation of rumor propagation trees, considering fine-grained post relations.
In this study, we formulate our approach based on a propagation structured model, with a specific emphasis on deducing post-level stances using weak supervision with only claim-level annotation. Drawing inspiration from the efficacy of propagation information, our methods are designed based on graph structure for incorporating social context information.

\subsection{Stance Detection} 

Manual analysis of stance has revealed a close relationship between specific veracity categories and stances**Zhang et al., "Analyzing Stance in Online Discussions"**. Subsequent studies explored various hand-crafted features**Liu, "Hand-Crafted Features for Stance Detection"**
% or Gaussian process**Wang, "Gaussian Process for Stance Detection"**
for training stance detection models. More recently, deep neural networks have been employed for stance representation learning and classification**Chen et al., "Deep Neural Networks for Stance Detection"**.
% aiming to mitigate the need for feature engineering and enhance generalizability
% Some studies considered conversation structure, including tree-structured models**Zhang, "Tree-Structured Models for Stance Detection"**
and multi-task frameworks for the joint detection of rumors and stances**Liu et al., "Multi-Task Frameworks for Rumor and Stance Detection"**. 
% Research has also incorporated contrastive learning for stance detection**Chen et al., "Contrastive Learning for Stance Detection"**. 
% Presently, multi-target stance detection has gained attention from researchers. ____ introduced a target-adaptive graph, while ____ utilized a teacher-student model for the stance detection task. However, multi-target stance detection usually operates on clear targets, which does not align with the messy information characteristics found on real-world social media. Additionally, it often requires datasets annotated with stance labels specific to particular targets.
However, the aforementioned methods typically require a substantial stance corpus annotated at the post level for training. While several unsupervised models based on pre-defined models**Liu et al., "Unsupervised Models for Stance Detection"**
and unsupervised structural embedding**Chen et al., "Unsupervised Structural Embedding for Stance Detection"**
for stance detection exist, their generalizability is consistently a critical concern due to their specific problem settings, reliance on manually crafted features.
% and the underlying models pre-trained for certain topics or tasks. 
In this paper, we propose a weakly supervised propagation model to concurrently classify rumor veracity and post stances on social media without the need for post stance labels. 

\subsection{Multiple Instance Learning (MIL)}

MIL is a form of weakly supervised learning designed to train a classifier with coarse-grained bag-level annotations to assign labels to fine-grained instances arranged in the bag**Wang et al., "Multiple Instance Learning for Weak Supervision"**
% Subsequent research in the MIL domain introduced several variants by extending threshold-based, count-based, and weighted collective MIL assumptions to deduce the bag-level label____. 
In recent years, MIL has proven effective and found successful applications in various Natural Language Processing (NLP) tasks**Liu et al., "MIL for NLP Tasks"**
% such as user satisfaction analysis in Community Question Answering (CQA)**Wang et al., "User Satisfaction Analysis in CQA"**, 
% customer services of E-Commerce**Zhang et al., "Customer Services of E-commerce using MIL"**, text segment-level models for sentiment analysis**Chen et al., "Text Segment-Level Models for Sentiment Analysis"**, 
% fashion outfit recommendation**Liu et al., "Fashion Outfit Recommendation"**
, and recently misinforming sentence detection in fake news debunking**Wang et al., "Misinforming Sentence Detection using MIL"**. 
However, the original MIL framework is specifically designed for the binary classification of instances lacking complex structures, and bag-level labels are expected to conform to instance-level labels. 
% Our primary challenge lies in devising a general framework capable of addressing the multi-class problem and overcoming incompatibility between bag and instance-level labels. 
In our work, we study an LLM-enhanced MIL framework to transform the multi-class problem into multiple binary classifiers, resolving the label incompatibility between bag and instance levels.

\subsection{Large Language Models (LLMs)}
Generative Pre-trained Transformers (GPTs) are a class of LLMs that have revolutionized artificial intelligence in recent years. 
% Nowadays, LLMs are incredibly flexible, a single model can perform a wide range of tasks, including answering questions, summarizing documents, and translating languages. 
The significant scientific advancements made by models such as ChatGPT**Chen et al., "ChatGPT: A Conversational AI Model"**
, LLaMA**Wang et al., "LLaMA: A Large-Scale Language Model"**, and ALPACA**Liu et al., "ALPACA: An Adversarial Learning Approach for Conversational AI"**
have gained researchers' attention in the field of rumor detection and stance detection. **Zhang et al., "Rumor Detection using LLMs"** and **Chen et al., "Stance Detection using LLMs"**
leveraged LLM to enhance the rumor detection, 
% ____ proposed a reinforced tuning framework based on LLM for joint stance and rumor detection,
**Liu et al., "Joint Stance and Rumor Detection using LLMs"**, **Wang et al., "Stance Detection using Teacher-Student Framework and LLMs"**
utilized LLM for stance detection.
% ____ utilized an unsupervised framework with LLM for zero-shot stance detection. 
These methods primarily focus on directly predicting rumor or stance types using LLMs. %However, as generative models, the classification capabilities of LLMs remain relatively underexplored. So,
% Generative Pre-trained Transformers (GPTs) are a class of LLMs that have revolutionized artificial intelligence in recent years. Nowadays, LLMs are incredibly flexible, a single model can perform a wide range of tasks, including answering questions, summarizing documents, and translating languages. The significant scientific advancements made by models such as ChatGPT**Chen et al., "ChatGPT: A Conversational AI Model"**
, LLaMA**Wang et al., "LLaMA: A Large-Scale Language Model"**, and ALPACA**Liu et al., "ALPACA: An Adversarial Learning Approach for Conversational AI"**
have gained researchers' attention, highlighting the importance of providing high-quality data to achieve accurate results with LLMs. LLMs demonstrate a remarkable ability to make predictions based on relatively small amounts of prompts or inputs and they have many applications such as article writing, question answering, code generation, text generation, and rumor verification**Zhang et al., "Rumor Verification using LLMs"**
% \textcolor{blue}{including rumor verification, you can add some LLM-based rumor detection, including our ACL paper.} 
In this paper, %we resort to LLM's text generation ability to generate explanations for posts's stances, aiming to make the stances more reliable for the final rumor detection.
we leverage the generative capacity of LLMs to produce explanations for post stances towards claim veracity, aiming to enhance the reliability and explainability of these stances for final rumor detection. %\wei{This paragraph is too general. You need to specifically review the methods that explicit leveraging LLMs for rumor/stance tasks}