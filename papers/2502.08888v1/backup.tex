\textbf{Token-level Transformer.}\label{sec:TokenTrans}
The core idea of token-level transformer is to model the interactions between words at different positions to jointly attend to tokens from different representation spaces. We perform token-level transformer because it would be more reasonable to make the model learn the importance of the word vectors in sentence representation~\cite{huang2021mixed}. We represent each post as a word sequence $t_i=(w_{i,1}, w_{i,2}, \cdots, w_{i,n})$, where $w_{i,j}$ is a $d$-dimensional vector that can be initialized with pre-trained word embeddings. We then map each word embedding into a fixed-size hidden vector by passing the sequence of word tokens through 6 number of Multi-Head Self-Attention (MHSA) layers, which is derived from default settings in Transformer encoder~\cite{vaswani2017attention}. In detail, MHSA layers transform the input sequence into multiple subspaces with different linear projections:
\begin{equation}\label{equ:Token-levelAtt}
\begin{split}
    & Q_i^h, K_i^h, V_i^h = t_i \cdot W_Q^h, t_i \cdot W_K^h, t_i \cdot W_V^h \\
    & \tilde{t}_i^h = softmax(\frac{Q_i^h \cdot {K_i^h}^T}{\sqrt{d_h}})\cdot V_i^h
\end{split}
\end{equation}
where $Q_i^h, K_i^h, V_i^h$ are the query, key, and value embeddings, respectively, $W_Q^h, W_K^h, W_V^h$ represent parameters related to the $h-th$ head, $\tilde{t}_i^h$ denotes the output states, $\sqrt{d_h}$ is the scaling factor and $d_h$ represents the dimensionality of the $h-th$ head. After that we concatenate all the heads outputs as $\tilde{t}_i = [\tilde{t}_i^1, \tilde{t}_i^2, \cdots, \tilde{t}_i^{n_{t-level}}] \in \mathbb{R}^{|t_i|*d}$, and then feed the concatenation embedding into a normalization layer (layerNorm), and a feed-forward network (FFN), which is consistent with Transformer encoder settings~\cite{vaswani2017attention}.
\begin{equation}\label{equ:layerNorm&FFN}
    \begin{split}
        & L_i = layerNorm(\tilde{t}_i \cdot W_L + \tilde{t}_i)\\
        & F_i = FFN(L_i \cdot W_F + L_i)
    \end{split}
\end{equation}
where $F_i=[s_1; \cdots; s_{|t_i|}] \in \mathbb{R}^{{|t_i|}*d}$ is the matrix representing all words vectors in tweet $t_i$, as such, this transforms $t_i=(w_{i,1} w_{i,2} \cdots w_{i,|t_i|})$ to $F_i=[s_1; \cdots; s_{|t_i|}]$. After which, we get the sentence representation for each tweet by placing the attention mechanism to interpolate all the word vectors:
\begin{equation}\label{equ:Token-leveFinalAtt}
    \begin{split}
        & \alpha_i= softmax(\gamma^T s_i) \\
        & h_i=\sum_{k=0}^{|t_i|} \alpha_i s_i\\
    \end{split}
\end{equation}
where $\gamma \in d, \alpha_k \in \mathbb{R}$, $s_i$ is the word vector obtained in Equation~\ref{equ:layerNorm&FFN}. Claim embedding $h_c$ will also be obtained from the above token-level transformer. And these sentence embeddings for the claim and posts will then be fed into post-level transformer.

\textbf{Post-level Transformer.}\label{sec:PostTrans}
Our goal is to cross-check all the posts in a conversation thread to enhance the post representation, this is because posts in social media are naturally short, and some posts may not have a response or repost, and the stance expressed by each post is highly relevant to other posts in the same conversation thread. We could capture the coherent opinions in a conversation thread by lowering the weight of incorrect information (e.g. a denial posts towards a true claim). To this end, we smoothen the conversation threads by arranging the claim and posts chronologically in a linear structure as shown in Figure~\ref{fig:SequentialModel}, which arranges the source claim as the first post. Then we utilize transformer-based network to make the pairwise comparisons within the conversation thread, to capture users' stances and boost each post's representation. We pass the sentence embeddings: $[h_c, h_1, \cdots, h_n]$, obtained in Sec~\ref{sec:TokenTrans} through $n_{p-level}$ number of MHSA layers to model the information flows in a conversation thread. After passing through these MHSA layers, the output embeddings will also go through layerNorm and FFN as Equation~\ref{equ:layerNorm&FFN} shows. Lastly, we also use an attention mechanism to interpolate the posts before feeding them into downstream architecture, which is consistent with the transformation in Equation~\ref{equ:Token-leveFinalAtt}. As such, this transforms $[h_c, h_1, \cdots, h_n]$ to $[h'_c, h'_1, \cdots, h'_n]$.
