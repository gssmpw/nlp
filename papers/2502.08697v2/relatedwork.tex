\section{Related Works}
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.01\columnwidth]{imgs/fig6.pdf}
	\caption{Comparison between \model{} with other search strategies in the Engrave domain. We report the number of iterations for each of the algorithm to find the reasonable effect vectors (different predicate could have different maximum search space $M_{\mathrm{max}}$). \model{} has demonstrated the highest efficiency in finding the desired vectors.}
    \vspace{-0.3cm}
	\label{fig:abla_search}
\end{figure}

\subsection{Learning Abstractions for Planning}
Learning abstractions is essential for reducing the complexity of long-horizon planning in high-dimensional domains.
Traditional approaches, such as hierarchical task networks (HTN)~\cite{kaelbling2011htn}, heavily rely on hand-designed abstractions.
Recent advances have shifted towards data-driven approaches that automatically discover useful abstractions from interactions~\cite{gupta2020relay,Soroush2022maple,hansen2022bisimulation,dong2019nlm,chitnis2021glib} or demonstrations~\cite{sharmadirected,kipf2019compile,chitnis2021nsrt,mao2022pdsketch}.
However, these methods struggle to generalize beyond the training environments~\cite{liang2024visualpredicator}.
Foundation models, such as large language models (LLMs) and vision-language models (VLMs), have been explored for high-level planning with minimal or no demonstrations~\cite{liu2024BLADE,fang2024keypoint,liang2024visualpredicator,silver2024generalized,wei2022cot,han2024interpret,huang2023voxposer,hu2023look,kumar2024openworld}. 
While these models leverage commonsense knowledge for efficient plan generation, two challenges remain:
(1) High-level plans from LLMs~\cite{silver2024generalized,han2024interpret,wei2022cot,huang2023voxposer} are difficult to reliably refine in the low-level space~\cite{liang2024visualpredicator}.
(2) VLM-based methods~\cite{liang2024visualpredicator,fang2024keypoint,kumar2024openworld,yang2024guidinglonghorizontaskmotion} struggle in domains where images cannot fully capture the state space.

\subsection{Task and Motion Planning}
To address these challenges, task and motion planning (TAMP) integrates high-level symbolic planning with low-level motion generation. 
Traditional TAMP methods~\cite{garrett2021integrated,garrett2020pddlstream} rely on manually designed planners~\cite{McDermott1998PDDL,karaman2011anytime}. 
These frameworks inherently support compositional generalization due to their relational structure. 
In addition, the coupling between high-level and low-level planning can handle failures at either level effectively.
However, traditional TAMP requires substantial human effort. 
Recent advances integrate learning into TAMP~\cite{chitnis2021nsrt,bougie2020skill,kumar2023predict,silver2023predicateinvent,liang2024visualpredicator,kumar2024openworld,yang2024guidinglonghorizontaskmotion}, forming bilevel planning frameworks. 
These approaches combine the strengths of TAMP with the scalability of machine learning models. 
However, most bilevel planners rely on manually engineered state abstractions (predicates), limiting their scalability and flexibility.

\begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{1.2mm}
    \fontsize{8}{10}\selectfont
    \begin{tabular}{cccccc}
    \toprule[1.5pt]
     Predicates & $\mathtt{P1\_1(?r)}$ & $\mathtt{P2\_1(?r,?t)}$ & $\mathtt{P2\_2(?r,?t)}$ & $\mathtt{In(?t,?t)}$ \\
    \midrule
    \texttt{Grasp}      & $\mathtt{Pre} \mid \mathtt{Eff}^-$ & $\mathtt{Pre}$   & $\mathtt{Pre}$   &       \\
    \texttt{Gaze}       &       & $\mathtt{Pre}$   & $\mathtt{Eff}^+$   &       \\
    \texttt{MAOff}      & $\mathtt{Pre}$   &       &       &       \\
    \texttt{MAOn}       &       & $\mathtt{Pre}$   & $\mathtt{Pre}$   &       \\
    \texttt{MTGaze}     &       & $\mathtt{Eff}^+$   &       &       \\
    \texttt{WalkOn}     &       & $\mathtt{Eff}^+$   &       &       \\
    \texttt{MTPlace}    & $\mathtt{Pre}$   &       &       &       \\
    \texttt{MTReach}    &       &       &       &       \\
    \texttt{Pick}       & $\mathtt{Pre}$   &       &       &       \\
    \texttt{Drop}       &       & $\mathtt{Pre}$ & $\mathtt{Pre}$ & $\mathtt{Eff}^+$ \\
    \bottomrule[1.5pt]
    \end{tabular}
  \caption{The preconditions, add effects, and delete effects for each of the actions (the variables are neglected for simplicity) with (part of) the invented predicates in the Climb-Transport Domain. 
  $\mathtt{MA}$ is for $\mathtt{MoveAway}$ and $\mathtt{MT}$ for $\mathtt{MoveTo}$.
  The invented predicates have specified some logical constrains over the order of actions.
  }
  \vspace{-0.3cm}
  \label{tab:interprete}%
\end{table}%

\subsection{Predicate Invention for Planning}
To automate predicate generation for planning, various approaches have been proposed~\cite{liang2024visualpredicator,li2023embodied,han2024interpret,silver2023predicateinvent,asai2019latplan_fol,asai2021latplanpddl,asai2018latplan_prop,hansen2022bisimulation}. 
The most direct approaches~\cite{li2023embodied,han2024interpret} rely on domain knowledge, such as human-provided labels~\cite{li2023embodied} or LLM-based oracles~\cite{han2024interpret}. 
To \textit{invent} predicates, earlier approaches derive "easy" step-wise objectives, such as reconstruction~\cite{asai2018latplan_prop,asai2019latplan_fol,asai2021latplanpddl} or bisimulation~\cite{hansen2022bisimulation}.
Among these, LatPlan (FOSAE)~\cite{asai2019latplan_fol} learns relational neural abstractions for images by reconstructing states and identifying action spaces for planning, which is the closest work to \model{}. 
However, its implicit predicates are not optimized for efficient planning, limiting its applicability to domains with only nullary actions. 
Recent approaches\cite{silver2023predicateinvent,liang2024visualpredicator} address this by learning abstractions tailored to fast planners~\cite{helmert2006fast}. 
However, these methods struggle to \textit{learn} predicate functions due to non-differentiable objectives. 
Consequently, they often rely on pre-defined predicate candidates from program synthesis~\cite{silver2023predicateinvent} or foundation models~\cite{liang2024visualpredicator}, which constrains their applicability in more sophisticated and high-dimensional state spaces.
Our approach is motivated by the bilevel planning framework~\cite{silver2023predicateinvent} but eliminates the need for pre-defining the candidates. 
Instead, we can learn adaptive neural functions for different domains, enabling more flexible and scalable learning based planning.