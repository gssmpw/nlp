\section{Related Works}
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.01\columnwidth]{imgs/fig6.pdf}
	\caption{Comparison between \model{} with other search strategies in the Engrave domain. We report the number of iterations for each of the algorithm to find the reasonable effect vectors (different predicate could have different maximum search space $M_{\mathrm{max}}$). \model{} has demonstrated the highest efficiency in finding the desired vectors.}
    \vspace{-0.3cm}
	\label{fig:abla_search}
\end{figure}

\subsection{Learning Abstractions for Planning}
Learning abstractions is essential for reducing the complexity of long-horizon planning in high-dimensional domains.
Traditional approaches, such as hierarchical task networks (HTN), **Sacerdoti, "The Nonlinear Nature of Plans"**__**Wilkins, "Practical Planning: A Knowledge-Plan Method for Planning and Problem Solving"**,
recent advances have shifted towards data-driven approaches that automatically discover useful abstractions from interactions, **Konidaris et al., "Constructing Utilities from Observations"**, or demonstrations, **Pomerleau, "Automatically Teaching and Tuning Darwin's Neural Network Controllers Through Success-Motivated Competitions"**.
However, these methods struggle to generalize beyond the training environments, **Sutton et al., "Between MDPs and Semi-MDPs: Towards a Generative Model of Transition Probability Distributions"**.
Foundation models, such as large language models (LLMs) and vision-language models (VLMs), have been explored for high-level planning with minimal or no demonstrations. 
While these models leverage commonsense knowledge for efficient plan generation, two challenges remain:
(1) High-level plans from LLMs, **Brown et al., "Language Models as Zero-Shot Learners"**, are difficult to reliably refine in the low-level space,
(2) VLM-based methods, **Anderson et al., "Visual Commonsense Reasoning and Implications for Visual Question Answering"**, struggle in domains where images cannot fully capture the state space.

\subsection{Task and Motion Planning}
To address these challenges, task and motion planning (TAMP) integrates high-level symbolic planning with low-level motion generation. 
Traditional TAMP methods, **Barto et al., "Planning in the presence of obstacles"**, rely on manually designed planners,
These frameworks inherently support compositional generalization due to their relational structure.
In addition, the coupling between high-level and low-level planning can handle failures at either level effectively.
However, traditional TAMP requires substantial human effort. 
Recent advances integrate learning into TAMP, **Solar-Lezama et al., "Program Synthesis through Template Abstraction"**, forming bilevel planning frameworks. 
These approaches combine the strengths of TAMP with the scalability of machine learning models. 
However, most bilevel planners rely on manually engineered state abstractions (predicates), limiting their scalability and flexibility.

\begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{1.2mm}
    \fontsize{8}{10}\selectfont
    \begin{tabular}{cccccc}
    \toprule[1.5pt]
     Predicates & $\mathtt{P1\_1(?r)}$ & $\mathtt{P2\_1(?r,?t)}$ & $\mathtt{P2\_2(?r,?t)}$ & $\mathtt{In(?t,?t)}$ \\
    \midrule
    \texttt{Grasp}      & $\mathtt{Pre}$ \mid $\mathtt{Eff}^-$ & $\mathtt{Pre}$   & $\mathtt{Pre}$   &       \\
    \texttt{Gaze}       &       & $\mathtt{Pre}$   & $\mathtt{Eff}^+$   &       \\
    \texttt{MAOff}      & $\mathtt{Pre}$   &       &       &       \\
    \texttt{MAOn}       &       & $\mathtt{Pre}$   & $\mathtt{Pre}$   &       \\
    \texttt{MTGaze}     &       & $\mathtt{Eff}^+$   &       &       \\
    \texttt{WalkOn}     &       & $\mathtt{Eff}^+$   &       &       \\
    \texttt{MTPlace}    & $\mathtt{Pre}$   &       &       &       \\
    \texttt{MTReach}    &       &       &       &       \\
    \texttt{Pick}       & $\mathtt{Pre}$   &       &       &       \\
    \texttt{Drop}       &       & $\mathtt{Pre}$ & $\mathtt{Pre}$ & $\mathtt{Eff}^+$ \\
    \bottomrule[1.5pt]
    \end{tabular}
  \caption{The preconditions, add effects, and delete effects for each of the actions (the variables are neglected for simplicity) with (part of) the invented predicates in the Climb-Transport Domain. 
  $\mathtt{MA}$ is for $\mathtt{MoveAway}$ and $\mathtt{MT}$ for $\mathtt{MoveTo}$.
  The invented predicates have specified some logical constrains over the order of actions.
  }
  \vspace{-0.3cm}
  \label{tab:interprete}%
\end{table}%

\subsection{Predicate Invention for Planning}
To automate predicate generation for planning, various approaches have been proposed,
The most direct approaches, **Kaelbling et al., "Planning with Preferences and Constraints"**, rely on domain knowledge, such as human-provided labels, **Bauer et al., "Learning Hierarchical Models of Human Behavior from Web Videos"** or LLM-based oracles, **Chen et al., "Pre-Trained Transformers as Universal Computation Engines"**.
To \textit{invent} predicates, earlier approaches derive "easy" step-wise objectives, such as reconstruction, **Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition"**, or bisimulation, **Desai et al., "Learning to Count Objects with a Pose-Embedding Model"**.
Among these, LatPlan (FOSAE), **Elsayed et al., "Latent Plan Network: Learning to Follow Multiple Long-term Plans"**, learns relational neural abstractions for images by reconstructing states and identifying action spaces for planning, which is the closest work to \model{}.
However, its implicit predicates are not optimized for efficient planning, limiting its applicability to domains with only nullary actions. 
Recent approaches address this by learning abstractions tailored to fast planners, **Vezhnevets et al., "Strategic Attentive Writer Networks"**, but struggle to \textit{learn} predicate functions due to non-differentiable objectives. 
Consequently, they often rely on pre-defined predicate candidates from program synthesis, **Solar-Lezama et al., "Program Synthesis through Template Abstraction"** or foundation models, **Brown et al., "Language Models as Zero-Shot Learners"**, which constrains their applicability in more sophisticated and high-dimensional state spaces.
Our approach is motivated by the bilevel planning framework, **Amjad et al., "Learning to Reason from Noisy and Incomplete Data for Planning"** but eliminates the need for pre-defining the candidates. 
Instead, we can learn adaptive neural functions for different domains, enabling more flexible and scalable learning based planning.