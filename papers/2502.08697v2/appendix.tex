% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Complete Notation Table}\label{app:notation}

We have presented the complete notation definition in \tref{tab:notation} for reference.

\subsection{Discussion on the Sparsity Assumption}\label{app:assumption}
Formally, we define the sparsity assumption required as:
\begin{assumption}[Effect Sparsity]
    Let $\underline{\mathtt{C}}$ be a ground action with object sets $\mathcal{O}_{\underline{\mathtt{C}}}$, $\underline{\psi}$ be a 
    ground predicate with object sets $\mathcal{O}_{\underline{\psi}}$, and $(\mathbf{x}, \underline{\mathtt{C}}, \mathbf{x}')$ be a transition pair.
    If $\mathcal{O}_{\underline{\psi}} \not\subset\mathcal{O}_{\underline{\mathtt{C}}}$, then $\theta_{\underline{\psi}}(\mathbf{x})=\theta_{\underline{\psi}}(\mathbf{x}')$.
\end{assumption}
Generally, this assumption holds if the following two conditions are both satisfied:
\begin{itemize}
    \item Each of the actions $\mathtt{C}\in\mathcal{C}$ in the domain has only one unique planning operator $\mathtt{Op}^{\mathtt{C}}$.
    \item The variable set in actions $\mathtt{C}$ is the same as the variable set in the operator $\mathtt{Op}^{\mathtt{C}}$.
\end{itemize}
In other words, if the options~\cite{chitnis2021nsrt} in a domain equal to the potential operators, then the assumption holds.
Though this is true in many domains, including those from previous work~\cite{kumar2024practice}, there exist domains where this assumption breaks~\cite{silver2023predicateinvent,chitnis2021nsrt}.
If the assumption no longer holds, more entries will become non-zero in $\bm{t}^{\psi, \underline{\mathtt{C}}}(\mathbf{x}, \mathbf{x}')$ and it would be very hard to identify their locations.
One future work that could potentially address this issue is by alternating between the predicate learning framework proposed in \sref{sec:ivntr} and the clustering mechanism proposed in previous work~\cite{chitnis2021nsrt}.

\begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \fontsize{8}{10}\selectfont
    \caption{List of the important notations in this work.}
    \begin{tabular}{ccc}
        \toprule[1.5pt]
        \textbf{Symbol}	& \textbf{Meaning}  & \textbf{Space}  \\
        \midrule
        $\Lambda$ & The type set in a domain & Set \\
        $\lambda$ & A type in a domain & Symbol \\
        $\mathtt{?\lambda}$ & A typed variable & Symbol \\
        $T$ & A task in a domain & Tuple \\
        $\mathcal{T}$ & Task Distribution & Distribution \\
        $K$ & Domain specific feature dimensions & Scalar \\
        $N$ & Number of objects in a task & Scalar \\
        $\mathbf{x}_i$ & The $i$-th state & Matrix \\
        $\mathcal{O}$ & Object set in a task & Set \\
        $\mathcal{C}$ & The action set in a domain & Set \\
        $\mathtt{C}$ & A parametrized action in a domain & Symbol \\
        $M$ & Number of actions in a domain & Scalar \\
        $\Omega$ & Space of the continuous action parameter & Set \\
        $\omega$ & The specific continuous action parameter & Vector \\
        $\underline{\mathtt{C}}$ & A grounded and refined action& Symbol \\
        $\underline{\bar{\mathtt{C}}}$ & A grounded but not refined action& Symbol \\
        $f$ & Transition function for a task & Function \\
        $\psi$ & Lifted predicate & Symbol \\
        $\underline{\psi}$ & Grounded predicate & Symbol \\
        $\theta_\psi$ & Classifier for predicate $\psi$ & Function \\
        $\theta_{\underline{\psi}}$ & Classification result for grounded predicate $\underline{\psi}$ & Scalar \\
        $\Psi$ & Complete predicate set for a domain & Set \\
        $\Psi_\mathrm{sta}$ & Static predicate set for a domain & Set \\
        $\Psi_\mathrm{dyn}$ & Dynamic predicate set for a domain & Set \\
        $\Psi_\mathrm{G}$ & Goal predicate set for a domain & Set \\
        $\theta_\Psi$ & Classifier set for predicate set $\Psi$ & Set \\
        $\pi$ & Refined plan & List \\
        $\bar{\pi}$ & Plan skeleton & List \\
        $\mathcal{D}$ & The dataset for learning & Set \\
        $B$ & The number of task-plan pairs & Scalar \\
        $\mathtt{Op}^\mathtt{C}$ & Operator for action $\mathtt{C}$ & Set \\
        $\mathtt{Var}$ & Variable set & List \\
        $\mathtt{Pre}$ & Pre-condition set & Set \\
        $\mathtt{Eff}^+$ & Add effect set & Set \\
        $\mathtt{Eff}^-$ & Delete effect set & Set \\
        $\eta^\mathtt{C}$ & Sampler for action $\mathtt{C}$ & Function \\
        $\hat{\Psi}_\mathrm{dyn}$ & Candidate predicate set & Set \\
        $J(\cdot)$ & Score function based on planning outcome & Function \\
        $\Psi^\mathtt{Var}$ & Set of predicates with typed variables $\mathtt{Var}$ & Set \\
        $\Delta^\psi$ & Effect vector for predicate $\psi$ & Vector \\
        $\delta^\psi_i$ & Effect value for predicate $\psi$ in action $\mathtt{C}_i$& Scalar \\
        $\bm{t}^{\psi, \underline{\mathtt{C}}}$ & Predicted ground predicate transition & Vector \\
        $\Delta^{\Psi^\mathtt{Var}}$ & Effect vector set for predicate set $\Psi^\mathtt{Var}$ & Set \\
        $\mathbf{L}_t$ & Loss information at $t$-th iteration & Vector \\
        $r^n_t$ & Value for the $n$-th effect vector at $t$-th iteration & Scalar \\
        $\mathbf{R}_t$ & Global value vector at $t$-th iteration & Vector \\
        \bottomrule[1.5pt]
    \end{tabular}%
    \label{tab:notation}%
    \vspace{-0.3cm}
\end{table}%

\subsection{Ground Predicates with Action Variables}\label{app:same_type}
In general, for a transition with ground action $\underline{\mathtt{C}}$, multiple ground predicates could be ``grounded on" the object set $\mathcal{O}_{\underline{\mathtt{C}}}$.
For example, in the Blocks domain, we could have actions like $\mathtt{Stack(?r,?b_0,?b_1)}$, where there are two same typed variables $\mathtt{?b_0,?b_1}$ (two objects in the blocks type).
In this case, consider the object set $\{\mathtt{r_1,b_1,b_2}\}$, if we have ground action $\mathtt{Stack(r_1,b_1,b_2)}$, then the two ground predicates $\mathtt{P1(b_1,b_2)}$ and $\mathtt{P1(b_2,b_1)}$ should be both considered as ``grounded on" the object set $\mathcal{O}_{\underline{\mathtt{C}}}$.
In such cases, the two ground predicates will have the same transition following \defref{def:ground_effect_vec}.
However, this is usually not true, for example, if $\mathtt{P1(?b,?b)}=\mathtt{On(?b,?b)}$, then the transition of $\mathtt{On(b_1,b_2)}$ and $\mathtt{On(b_2,b_1)}$ should \textbf{not} be the same.
To solve this problem, when we define the predicate sets $\Psi^\mathtt{Var}$ with the same typed variables $\mathtt{Var}$, we additionally annotate that the predicate variables $\mathtt{Var}$ have a fixed correspondence with the action variables $\mathtt{Var}_\mathtt{c}$.
More specifically, when we ground the predicates, all possible object compositions are considered.
But when we use the ground predicates to form effects (during predicate function training) and pre-conditions (during operator learning), we only use the ground predicates whose object sets have the specified correspondence with the action variables.
For example, consider the predicate $\mathtt{P1(?b,?b)}$, we may further annotate it with a list $[0,1]$, meaning that the first predicate variables should be matched with the first variable in the actions that is typed as a block and the second predicate variables should be matched with the second block variable in the actions.
In this case, we will only consider $\mathtt{P1(b_1,b_2)}$ for transitions with action $\mathtt{Stack(r_1,b_1,b_2)}$ (and $\mathtt{P1(b_2,b_1)}$ with action $\mathtt{Stack(r_1,b_2,b_1)}$).
In practice, the predicate sets with the same typed variables but different correspondence annotation are considered as different groups in the bilevel learning.
For more details, please refer to our source code.

\subsection{Improving Efficiency during Expansion}\label{app:pruning}
To make the tree expansion (symbolic learning) more efficient without sacrificing too much completeness, we have tried to prune an effect vector $\Delta^{\psi_n}$ (setting its value $r^n$ as $-\infty$) via the following strategy:
\begin{itemize}
    \item If the input object-centric states to the predicate function $\theta_{\psi_n}$ never change for any ground actions $\underline{\mathtt{C}}$ belonging to an action $\mathtt{C}$, then the nodes $\Delta^{\psi_n}$ with non-zero entry $\Delta^{\psi_n}_\mathtt{C}$ will be pruned. This strategy is implemented as a ``pre-check", which happens before the tree expansion starts.
    \item Assume the loss vector $\mathbf{L}_t$ is from the evaluated effector vector $\Delta^\psi_t$ in the $t$-th iteration. Let $\mathbb{I}(\Delta^{\psi}_t \neq 0)$ and $\mathbb{I}(\Delta^{\psi_n} \neq 0)$ be the mask indicating whether the entries in $\Delta^\psi_t$ and $\Delta^{\psi_n}$ are non-zero, respectively. We prune $\Delta^{\psi_n}$ if $\sum \mathbf{L}_t\odot\mathbb{I}(\Delta^{\psi}_t \neq 0)\odot\mathbb{I}(\Delta^{\psi_n} \neq 0) > \tau$.
\end{itemize}
The second strategy prunes a vector if the sum of the loss from its non-zero indices is larger than the threshold. 
This strategy might make the tree expansion not fully complete. 
The intuition behind it is that: Since the children have the same non-zero parts as their parents, if the parents' non-zero part has already contributed high validation loss, then we assume the children's non-zero part will also contribute high loss. 
There is a small chance that the children turn out to have low loss due to the additional non-zero entry.
We have empirically shown that this strategy works in practice, probably due to the fact that a child node can come from different parents.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=2\columnwidth]{imgs/app_failure.pdf}
 \vspace{-0.1cm}
	\caption{Typical failure cases in the real robot tests of the Climb-Transport domain. The most common failure is that the target is dropped outside of the container, which is due to the localization error. Other failures include failing to generate the grasping motion plan on the platform and falling of the platform while trying to climb onto it.}
 \vspace{-0.1cm}
	\label{fig:app_failure}
\end{figure*}

\subsection{Implementation Details for Each Domain}\label{app:domain_details}

Here, we provide more details for each domain:

\textbf{Satellites:}
\begin{itemize}
    \item \textit{Types}: The satellites ($\mathtt{s}$) and the targets ($\mathtt{t}$). 
    \item \textit{Actions}: $\mathtt{Calibrate(?s,?t)}$, $\mathtt{MoveTo(?s,?t)}$, $\mathtt{MoveAway(?s,?t)}$, $\mathtt{ShootX(?s,?t)}$, $\mathtt{ShootY(?s,?t)}$, $\mathtt{TakeCam(?s,?t)}$, $\mathtt{TakeInfrared(?s,?t)}$, and $\mathtt{TakeGeiger(?s,?t)}$.
    \item \textit{Static Predicates}: $\mathtt{CalibrationTgt(?s,?t)}$, $\mathtt{ShootsX(?s)}$, $\mathtt{ShootsY(?s)}$.
    \item \textit{Goal Predicates}: $\mathtt{CamTaken(?s,?t)}$, $\mathtt{InfraredTaken(?s,?t)}$, $\mathtt{GeigerTaken(?s,?t)}$.
    \item \textit{Task Description}: There are some number of satellites, each carrying an instrument. The possible instruments are: (1) a camera, (2) an infrared sensor, (3) a Geiger counter.
    Additionally, each satellite may be able to shoot Chemical X and/or Chemical
    Y. The satellites have a viewing cone within which they can see everything
    that is not occluded. The goal is for specific satellites to take readings
    of specific objects with calibrated instruments.
    \item \textit{Predicate invention hardware}: A single A100 GPU.
\end{itemize}

\textbf{Blocks:}
\begin{itemize}
    \item \textit{Types}: The robot ($\mathtt{r}$) and the blocks ($\mathtt{b}$). 
    \item \textit{Actions}: $\mathtt{PickFromTable(?r,?b)}$, $\mathtt{Unstack(?r,?b,?b)}$, $\mathtt{Stack(?r,?b,?b)}$, $\mathtt{PutOnTable(?r,?b)}$, $\mathtt{Pack(?b,?b)}$.
    \item \textit{Static Predicates}: None.
    \item \textit{Goal Predicates}: $\mathtt{Packed(?b,?b)}$.
    \item \textit{Task Description}: The robot needs to manipulate a set of blocks (which were initialized as random towers) and pack them into required pairs. In order to pack two blocks, one block needs to be on the top of another with the bottom block on the table and the top block clear.
    \item \textit{Predicate invention hardware}: A single A100 GPU.
\end{itemize}

\textbf{Measure-Mul:}
\begin{itemize}
    \item \textit{Types}: The robot ($\mathtt{r}$) and the targets ($\mathtt{t}$). 
    \item \textit{Actions}: $\mathtt{Calibrate(?r,?t)}$, $\mathtt{MoveTo(?r,?t)}$, $\mathtt{MoveAway(?r,?t)}$, $\mathtt{Gaze(?r,?t)}$, and $\mathtt{Measure(?r,?t)}$.
    \item \textit{Static Predicates}: $\mathtt{CalibrationTgt(?s,?t)}$.
    \item \textit{Goal Predicates}: $\mathtt{Measured(?t)}$.
    \item \textit{Task Description}: The Spot robot needs to use a thermal camera under its hand to measure the body temperature of multiple human targets.
    To do this, it needs to first calibrate the camera with respect to a calibrator by gazing at it, which poses some constraints on the relative poses between the hand and the target.
    Then, before measuring each target, it also needs to gaze at them.
    \item \textit{Predicate invention hardware}: A single A100 GPU.
\end{itemize}

\textbf{Climb-Measure:}
\begin{itemize}
    \item \textit{Types}: The robot ($\mathtt{r}$), the targets ($\mathtt{t}$), and the platform ($\mathtt{p}$).
    \item \textit{Actions}: $\mathtt{Calibrate(?r,?t)}$, $\mathtt{MoveToGaze(?r,?t)}$, $\mathtt{MoveToReach(?r,?p)}$, $\mathtt{MoveToPlace(?r,?p,?t)}$, $\mathtt{MoveAwayOff(?r,?t)}$, $\mathtt{MoveAwayOn(?r,?p,?t)}$, $\mathtt{WalkOn(?r,?p,?t)}$, $\mathtt{Pick(?r,?p)}$, $\mathtt{Place(?r,?p,?t)}$, $\mathtt{Gaze(?r,?t)}$, and $\mathtt{Measure(?r,?t)}$.
    \item \textit{Static Predicates}: $\mathtt{CalibrationTgt(?s,?t)}$, $\mathtt{DirectViewable(?t)}$, $\mathtt{AppliedTo(?p,?t)}$.
    \item \textit{Goal Predicates}: $\mathtt{Measured(?t)}$.
    \item \textit{Task Description}: Similar to Measure-Mul, the Spot robot needs to use a thermal camera under its hand to measure the body temperature of a human target.
    Differently, the calibrator and human target could be at a high location where the Spot can directly gaze at them.
    To achieve the goal, the Spot will need to arrange some platforms.
    \item \textit{Predicate invention hardware}: Parallelly on four A100 GPUs.
\end{itemize}

\textbf{Climb-Transport:}
\begin{itemize}
    \item \textit{Types}: The robot ($\mathtt{r}$), the targets ($\mathtt{t}$), and the platform ($\mathtt{p}$).
    \item \textit{Actions}: $\mathtt{Grasp(?r,?t)}$, $\mathtt{MoveToGaze(?r,?t)}$, $\mathtt{MoveToReach(?r,?p)}$, $\mathtt{MoveToPlace(?r,?p,?t)}$, $\mathtt{MoveAwayOff(?r,?p)}$, $\mathtt{MoveAwayOn(?r,?p,?t)}$, $\mathtt{WalkOn(?r,?p,?t)}$, $\mathtt{Pick(?r,?p)}$, $\mathtt{Gaze(?r,?t)}$, and $\mathtt{Drop(?r,?t,?t)}$.
    \item \textit{Static Predicates}: $\mathtt{GraspingTgt(?t)}$, $\mathtt{InitHigh(?t)}$.
    \item \textit{Goal Predicates}: $\mathtt{In(?t,?t)}$.
    \item \textit{Task Description}: The Spot robot needs to grasp a target and drop it into another target container.
    Similar to Climb-Measure, to achieve the goal, the Spot will need to arrange some platforms.
    \item \textit{Predicate invention hardware}: Parallelly on four A100 GPUs.
\end{itemize}

\textbf{Engrave:}
\begin{itemize}
    \item \textit{Types}: The robot ($\mathtt{r}$) and the blocks ($\mathtt{b}$). 
    \item \textit{Actions}: $\mathtt{PickFromTable(?r,?b)}$, $\mathtt{Unstack(?r,?b,?b)}$, $\mathtt{Stack(?r,?b,?b)}$, $\mathtt{PutOnTable(?r,?b)}$, $\mathtt{Engrave(?r, ?b,?b)}$, $\mathtt{Rotate(?r,?b)}$, $\mathtt{Pack(?b,?b)}$.
    \item \textit{Static Predicates}: $\mathtt{NotEq(?b,?b)}$
    \item \textit{Goal Predicates}: $\mathtt{Packed(?b,?b)}$.
    \item \textit{Task Description}: Similar to Blocks, the robot needs to manipulate a set of blocks (which were initialized as random towers) and pack them into required pairs. 
    However, blocks start with one irregular Gaussian surface that must be ``engraved" to create a matching fit. 
    Once engraved, blocks need to be further rotated and placed for final assembly and packing. 
    We generate the Block meshes and point clouds using Pytorch3D~\cite{ravi2020pytorch3d}. 
    For fairness, all methods use the same PointNet~\cite{qi2017pointnet} as the state encoder. 
    \item \textit{Predicate invention hardware}: Parallelly on six A100 GPUs.
\end{itemize}

For more details about these domains, please refer to our source code.


\subsection{More Details about Baselines}\label{app:baseline_details}
We introduce more details for the relational neural policy baselines here:
\begin{itemize}
    \item \textbf{GNN}: The nodes are defined the object-centric features and the edges are defined as the grounded binary predicates (provided static and goal predicates).
    During training, the GNN learns to predict the action class and the selected objects.
    During test, the GNN tries to shoot multiple tries until planning budget run out.
    \item \textbf{Transformer}: The tokens are defined the object-centric features as well as the grounded predicates (provided static and goal predicates).
    The training objective and test setup are similar to GNN.
    However, different from the message passing strategy in GNN, we used the multi-head attention mechanism for the information fusing among tokens
    \item \textbf{FOSAE}: The baseline is loosely inspired by the state autoencoder (SAE) proposed in LatPlan~\cite{asai2019latplan_fol}. We first use the official attention mechanism to pre-train the SAE module by reconstructing the original state.
    Then, since the action spaces in this work is relational (instead of nullary), we encode the augmented binary states from SAE as the global feature of a graph neural network.
    Finally, the graph neural network is trained and evaluated similar to the GNN-shooting baseline.
\end{itemize}
For fairness, we have used the same action samplers learned using our framework for the three baseline above. 
For more details, please refer to our source code.

\subsection{Failures in Real Robot Experiments}\label{app:failures}


As shown in \fref{fig:app_failure}, we present the typical failure cases in the real-robot tests for the Climb-Transport domain.
The most frequent failure is that the robot finally drops the target outside of the container (but very close).
The reason is that the map of our experiments was recorded before the tables (used to place targets) were placed, which could result in the small drift of the localization system on Spot during the plan execution.
These drifts finally accumulate and result in the final hand pose error.
A worse problem is that due to the lack of a motion capture system, the container itself might not be accurately placed.
One potential way to address this is by integrating some more advanced SLAM system that is robust to partial map changes.
Another failure case (much less common) is that the Spot grasping skill could fail when it is on a platform.
Here, we have used the manipulation toolkit from the official Boston Dynamics Python SDK, which might fail to find a grasping motion plan in certain states.
Finally, since the platform is narrow, we have also observed that the Spot could fall off the platform when walking onto the platform.


\subsection{Invented Predicates and Operators}\label{app:complete_op}
We present the invented predicates together with the operators for the Satellites domain below.
The invented predicates are named as $\mathtt{P1},\mathtt{P2},\cdots$, while the provided predicates have the names introduced in \appref{app:domain_details}.
For other domains, the learned operators are more sophisticated; please refer to our source code and meta results for details.

\textbf{Satellites:}
\begin{Verbatim}[frame=single,resetmargins=true]
Calibrate(?x0:s, ?x1:t)
  :Pre (and 
      (CalibrationTgt(?x0, ?x1))
      (not P3_0(?x0, ?x1))) 
  :Eff+ (P1_0(?x0))
  :Eff- set()
MoveAway(?x0:s, ?x1:t)
  :Pre (not P3_0(?x0, ?x1))
  :Eff+ (ForAll:?t P3_0(?x0, ?t))
  :Eff- (not P3_0(?x0, ?x1))
MoveTo(?x0:s, ?x1:t)
  :Pre (ForAll:?t P3_0(?x0, ?t))
  :Eff+ (not P3_0(?x0, ?x1))
  :Eff- (ForAll:?t P3_0(?x0, ?t))
ShootChemX(?x0:s, ?x1:t)
  :Pre (and
    (not P3_0(?x0, ?x1))
    ShootsChemX(?x0))
  :Eff+ (P2_0(?x1))
  :Eff- set()
ShootChemY(?x0:s, ?x1:t)
  :Pre (and
    (not P3_0(?x0, ?x1))
    ShootsChemY(?x0))
  :Eff+ (P2_1(?x1))
  :Eff- set()
UseCamera(?x0:s, ?x1:t)
  :Pre (and
    (not P3_0(?x0, ?x1))
    (P1_0(?x0))
    (P2_0(?x1))
    HasCam(?x0))
  :Eff+ (CameraReadingTaken(?x0, ?x1))
  :Eff- set()
UseGeiger(?x0:s, ?x1:t)
  :Pre (and
    (not P3_0(?x0, ?x1))
    (P1_0(?x0))
    HasGeiger(?x0))
  :Eff+ (GeigerReadingTaken(?x0, ?x1))
  :Eff- set()
UseInfraRed(?x0:s, ?x1:t)
  :Pre (and
    (not P3_0(?x0, ?x1))
    (P1_0(?x0))
    (P2_1(?x1))
    HasInfrared(?x0))
  :Eff+ (InfraredReadingTaken(?x0, ?x1))
  :Eff- set()\end{Verbatim}

\begin{figure}[!t]
	\centering
	\includegraphics[width=1\columnwidth]{imgs/app_sampler.pdf}
	\caption{Visualization of the classification results on sampled poses for \texttt{MoveToReach} action using our invented predicate $\mathtt{P3(?r,?p)}$. TP is for true positive, TN is for true negatives, and FP is for false positive. We only visualize part of the samples which are converted to SE2 for simplicity, best viewed in color.}
	\label{fig:vis_sample}
\end{figure}

\subsection{Neural Predicates as Guidance for the Samplers}\label{app:sampler_vis} 
As shown in \fref{fig:vis_sample}, we visualize classification results for sampled continuous parameters of the \texttt{MoveToReach} action across $4$ tasks. 
Since training demonstrations involve solving inverse kinematics (IK) for \texttt{MoveToReach}, accurate sampler learning is challenging.
Our invented predicate $\mathtt{P3(?r,?p)}$, an add effect of \texttt{MoveToReach} and a precondition for \texttt{Pick}, effectively filters out most true negatives (TNs), ensuring successful plans.

\begin{figure}[!t]
	\centering
	\includegraphics[width=1\columnwidth]{imgs/app_obj.pdf}
 \vspace{-0.4cm}
	\caption{Score optimization process during the predicate selection stage. Compared with grammar based predicate pool~\cite{silver2023predicateinvent}, our \model{} is capable of constructing a much stronger neural predicate pool, which is able to effectively optimize the planning objective with a few hill-climbing steps. Yet, the grammar-based approach has failed.
    }
    \vspace{-0.4cm}
	\label{fig:ana_objective}
\end{figure}

\subsection{Objective Minimization}\label{app:objective}
We display the objective minimization process during the predicate selection stage in \fref{fig:ana_objective}.
Compared with the grammar-based predicate pool~\cite{silver2023predicateinvent}, \model{} is capable of generating much stronger neural functions as predicate candidates.
These neural predicates have made the objective minimization possible for more complicated states, where previous approaches have failed~\cite{silver2023predicateinvent}.