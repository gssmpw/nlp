\section{Related Work}
The integration of optimal transport into representation learning has gained traction in recent years. Several works have explored the use of Wasserstein distance for deep learning tasks such as domain adaptation~\cite{courty2017optimal, flamary2016optimal, damodaran2018deepjdot}, generative modeling~\cite{arjovsky2017wasserstein,tolstikhin2018wasserstein,gulrajani2017improved}, and clustering~\cite{genevay2019learning,fatras2021unbalanced,cuturi2014fast}. In self-supervised learning, optimal transport has been investigated as a tool for feature alignment~\cite{fatras2021unbalanced,perrot2016mapping,schmitz2018wasserstein} and structured representation learning~\cite{patrini2020sinkhorn,genevay2018learning,asano2019self}.

Contrastive learning methods traditionally rely on predefined similarity measures such as cosine similarity or Euclidean distance~\cite{kumar2022contrastive}. However, these metrics do not capture the global structure of learned representations. Recent efforts such as CDSSL~\cite{sepanj2025self}, MMD-Based VICReg~\cite{sepanj2024aligning} ,VICReg~\cite{bardes2021vicreg} and Barlow Twins~\cite{zbontar2021barlow} introduced alternative regularization strategies based on variance-covariance matrices to enhance representation dispersion. Our approach builds upon these insights by leveraging Sinkhorn-based optimal transport to enforce a structured latent space while maintaining the contrastive learning framework of SimCLR.

In contrast to prior work, our proposed method, \textbf{SinSim}, explicitly integrates Sinkhorn loss into SimCLR, bridging the gap between instance-based alignment and global structure regularization. We hypothesize that this hybrid approach not only prevents embedding collapse but also improves the overall robustness of learned representations.