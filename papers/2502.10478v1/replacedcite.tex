\section{Related Work}
The integration of optimal transport into representation learning has gained traction in recent years. Several works have explored the use of Wasserstein distance for deep learning tasks such as domain adaptation____, generative modeling____, and clustering____. In self-supervised learning, optimal transport has been investigated as a tool for feature alignment____ and structured representation learning____.

Contrastive learning methods traditionally rely on predefined similarity measures such as cosine similarity or Euclidean distance____. However, these metrics do not capture the global structure of learned representations. Recent efforts such as CDSSL____, MMD-Based VICReg____ ,VICReg____ and Barlow Twins____ introduced alternative regularization strategies based on variance-covariance matrices to enhance representation dispersion. Our approach builds upon these insights by leveraging Sinkhorn-based optimal transport to enforce a structured latent space while maintaining the contrastive learning framework of SimCLR.

In contrast to prior work, our proposed method, \textbf{SinSim}, explicitly integrates Sinkhorn loss into SimCLR, bridging the gap between instance-based alignment and global structure regularization. We hypothesize that this hybrid approach not only prevents embedding collapse but also improves the overall robustness of learned representations.