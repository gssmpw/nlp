\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{booktabs}
\newtheorem{lemma}{Lemma}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{SinSim: Sinkhorn-Regularized SimCLR\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} M.Hadi Sepanj}
\IEEEauthorblockA{\textit{Vision Group, Systems Design Engineering} \\
\textit{ University of Waterloo}\\
Waterloo, Canada \\
mhsepanj@uwaterloo.ca}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Paul Fieguth}
\IEEEauthorblockA{\textit{Vision Group, Systems Design Engineering} \\
\textit{ University of Waterloo}\\
Waterloo, Canada \\
paul.fieguth@uwaterloo.ca}
\and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
 
\end{IEEEkeywords}

\section{Introduction}


\section{Background}

\subsection{SimCLR and Contrastive Learning}
SimCLR \cite{chen2020simple} is a self-supervised contrastive learning method that learns representations by maximizing agreement between differently augmented views of the same image. It utilizes a deep encoder, a projection head, and a contrastive loss.

Given an image \( x \), two augmented views \( x_1 \) and \( x_2 \) are generated via transformations such as cropping, color distortion, and Gaussian blur. These views are processed by an encoder network \( f_\theta \) (e.g., ResNet) to obtain feature representations \( z_1 \) and \( z_2 \). A projection head \( g_\phi \) maps them to a latent space where the contrastive loss is applied.

The loss function is based on the normalized temperature-scaled cross-entropy (NT-Xent) loss:
\begin{equation}
    \mathcal{L}_{\text{NT-Xent}} = -\sum_{i} \log \frac{\exp( \text{sim}(z_i, z_j) / \tau )}{\sum_{k \neq i} \exp( \text{sim}(z_i, z_k) / \tau )},
\end{equation}
where \( \text{sim}(z_i, z_j) \) denotes the cosine similarity, and \( \tau \) is a temperature parameter. The loss encourages representations of positive pairs (augmentations of the same image) to be closer while pushing away negative pairs.

\subsection{Sinkhorn Regularization and Optimal Transport}
The Wasserstein distance is a fundamental concept in optimal transport, measuring the minimal effort required to move probability mass between two distributions. However, computing it directly is computationally expensive. Sinkhorn regularization introduces an entropy term to make optimal transport differentiable and efficient.

Given two probability distributions \( \mu \) and \( \nu \), the Wasserstein distance is defined as:
\begin{equation}
    W_c(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \sum_{i,j} c(x_i, y_j) \gamma_{ij},
\end{equation}
where \( c(x_i, y_j) \) is the cost function and \( \gamma \) is the transport plan. Sinkhorn regularization adds an entropy term:
\begin{equation}
    W_c^{\lambda}(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \sum_{i,j} c(x_i, y_j) \gamma_{ij} + \lambda H(\gamma),
\end{equation}
where \( H(\gamma) = -\sum_{i,j} \gamma_{ij} \log \gamma_{ij} \) is the entropy term, controlled by \( \lambda \). This enables efficient optimization via the Sinkhorn-Knopp algorithm.

Sinkhorn regularization has been used in clustering and domain adaptation, but its integration into contrastive learning remains underexplored. Our work introduces Sinkhorn regularization to SimCLR to enforce structured representation alignment.

\section{Methodology}

Our proposed approach, \textbf{SinSim}, integrates Sinkhorn regularization into the SimCLR framework to enhance its representation learning power. Inspired by regularization techniques introduced in methods like VICReg \cite{bardes2021vicreg}, we introduce an optimal transport-based regularizer that refines contrastive learning.

\subsection{Sinkhorn-Regularized Contrastive Learning}
We modify the standard SimCLR contrastive loss by incorporating a Sinkhorn-based optimal transport regularization term. Given an image \( x \), we generate two augmented views \( x_1 \) and \( x_2 \) and process them through an encoder \( f_\theta \) to obtain feature representations \( z_1 \) and \( z_2 \). The standard contrastive loss encourages alignment between these representations, as detailed in the background section.

To further improve representation structure, we introduce an additional term based on Sinkhorn regularization:
\begin{equation}
    \mathcal{L}_{\text{Sinkhorn}} = W_c^{\lambda}(P, Q),
\end{equation}
where \( P \) and \( Q \) represent distributions of the embeddings. This regularization term enforces structured alignment, leveraging optimal transport to guide the learning process.

\subsection{Final Loss Function}
Our total loss function combines the original contrastive loss with Sinkhorn regularization:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{NT-Xent}} + \beta \mathcal{L}_{\text{Sinkhorn}},
\end{equation}
where \( \beta \) is a hyperparameter controlling the impact of the Sinkhorn regularizer. This formulation balances contrastive learning objectives with optimal transport-based alignment.

\section{Theoretical Justification}
Contrastive learning relies on instance discrimination to learn meaningful representations. However, standard SimCLR optimizes similarity via cosine distance, which lacks a global structure constraint. Sinkhorn regularization provides a theoretically grounded method to enforce structured representation alignment by leveraging optimal transport theory.

Optimal transport has been shown to improve representation learning by aligning distributions while preserving intrinsic geometry \cite{cuturi2013sinkhorn, courty2017optimal}. Unlike Euclidean metrics, the Wasserstein distance captures differences in high-dimensional feature spaces, ensuring a more stable similarity measurement \cite{frogner2015learning}. This is particularly useful in contrastive learning, where standard methods often collapse embeddings into an unstructured manifold \cite{bardes2021vicreg}.

By introducing Sinkhorn regularization, we explicitly control representation dispersion while encouraging a meaningful alignment of positive pairs. This reduces the problem of **mode collapse**, a common issue in contrastive learning where embeddings become too concentrated \cite{grill2020bootstrap}. Furthermore, the entropy term in Sinkhorn regularization improves stability, allowing for smoother optimization compared to standard Wasserstein distances \cite{genevay2018learning}.

From an empirical standpoint, VICReg \cite{bardes2021vicreg} and Barlow Twins \cite{zbontar2021barlow} have demonstrated that well-chosen regularization terms enhance contrastive representation learning. Our work extends this by incorporating **an information-theoretic approach based on transport theory**, reinforcing the stability and robustness of learned representations.

\section{Experiments}

We evaluate our proposed SinSim method on multiple benchmark datasets to assess its effectiveness in self-supervised representation learning. We compare our approach to SimCLR, BYOL, VICReg, and Barlow Twins using linear and non-linear classification tasks.

\subsection{Experimental Setup}
We train all models on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and STL-10. Each method is trained using a ResNet-18 backbone with a projection head for contrastive learning. After pretraining, we freeze the encoder and evaluate representations using a linear classifier and a non-linear classifier.

\subsection{Linear Classification Performance}
Table~\ref{tab:linear_classification} presents the accuracy of a linear classifier trained on the frozen features extracted from different self-supervised learning methods.

\begin{table}[h]
    \centering
    \caption{Linear Classification Accuracy (\%)}
    \label{tab:linear_classification}
    \begin{tabular}{lcccc}
        \toprule
        Method & MNIST & CIFAR-10 & CIFAR-100 & STL-10 \\
        \midrule
        SimCLR & XX.X & XX.X & XX.X & XX.X \\
        BYOL & XX.X & XX.X & XX.X & XX.X \\
        VICReg & XX.X & XX.X & XX.X & XX.X \\
        Barlow Twins & XX.X & XX.X & XX.X & XX.X \\
        \textbf{SinSim (Ours)} & \textbf{XX.X} & \textbf{XX.X} & \textbf{XX.X} & \textbf{XX.X} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Non-Linear Classification Performance}
We further evaluate the learned representations using a non-linear classifier, specifically a two-layer MLP trained on the extracted features. Results are presented in Table~\ref{tab:nonlinear_classification}.

\begin{table}[h]
    \centering
    \caption{Non-Linear Classification Accuracy (\%)}
    \label{tab:nonlinear_classification}
    \begin{tabular}{lcccc}
        \toprule
        Method & MNIST & CIFAR-10 & CIFAR-100 & STL-10 \\
        \midrule
        SimCLR & XX.X & XX.X & XX.X & XX.X \\
        BYOL & XX.X & XX.X & XX.X & XX.X \\
        VICReg & XX.X & XX.X & XX.X & XX.X \\
        Barlow Twins & XX.X & XX.X & XX.X & XX.X \\
        \textbf{SinSim (Ours)} & \textbf{XX.X} & \textbf{XX.X} & \textbf{XX.X} & \textbf{XX.X} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Representation Learning Visualization}
To analyze the quality of learned representations, we visualize the embedding space using t-SNE. Figure~\ref{fig:representation_visualization} presents the representation space of SinSim in comparison with SimCLR and VICReg.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{placeholder_representation1.png}
    \includegraphics[width=0.45\textwidth]{placeholder_representation2.png}
    \includegraphics[width=0.45\textwidth]{placeholder_representation3.png}
    \includegraphics[width=0.45\textwidth]{placeholder_representation4.png}
    \caption{t-SNE visualization of learned representations for different methods.}
    \label{fig:representation_visualization}
\end{figure}

\subsection{Sinkhorn Regularization Weight Analysis}
We analyze the effect of the Sinkhorn regularization weight (\(\lambda\)) on accuracy. Figure~\ref{fig:sinkhorn_lambda_vs_acc} shows the classification accuracy for different values of \(\lambda\).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{placeholder_lambda_vs_acc.png}
    \caption{Impact of Sinkhorn regularization weight (\(\lambda\)) on classification accuracy.}
    \label{fig:sinkhorn_lambda_vs_acc}
\end{figure}

\subsection{Sinkhorn Epsilon Sensitivity Analysis}
We also examine the effect of the Sinkhorn entropy regularization parameter (\(\varepsilon\)) on model performance. Figure~\ref{fig:sinkhorn_epsilon_vs_acc} presents the results.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{placeholder_epsilon_vs_acc.png}
    \caption{Impact of Sinkhorn entropy regularization parameter (\(\varepsilon\)) on classification accuracy.}
    \label{fig:sinkhorn_epsilon_vs_acc}
\end{figure}

 

 
\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil
\end{thebibliography}



\end{document}
