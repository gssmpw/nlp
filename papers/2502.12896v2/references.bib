% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{chatbotarena,
      title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}, 
      author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
      year={2024},
      eprint={2403.04132},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.04132}, 
}

@book{anderson2001taxonomy,
  title={A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives},
  author={Anderson, {Lorin W.} and Krathwohl, {David R.}},
  isbn={9780801319037},
  lccn={lc00063423},
  url={https://books.google.es/books?id=EMQlAQAAIAAJ},
  year={2001},
  publisher={Longman}
}


@article{bender-friedman-2018-data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}


@inproceedings{lai-etal-2023-chatgpt,
    title = "{C}hat{GPT} Beyond {E}nglish: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
    author = "Lai, Viet  and
      Ngo, Nghia  and
      Pouran Ben Veyseh, Amir  and
      Man, Hieu  and
      Dernoncourt, Franck  and
      Bui, Trung  and
      Nguyen, Thien",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.878",
    doi = "10.18653/v1/2023.findings-emnlp.878",
    pages = "13171--13189",
    abstract = "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. In particular, we evaluate ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. Compared to the performance of previous models, our extensive experiments demonstrate the worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
}

@inproceedings{bang-etal-2023-multitask,
    title = "A Multitask, Multilingual, Multimodal Evaluation of {C}hat{GPT} on Reasoning, Hallucination, and Interactivity",
    author = "Bang, Yejin  and
      Cahyawijaya, Samuel  and
      Lee, Nayeon  and
      Dai, Wenliang  and
      Su, Dan  and
      Wilie, Bryan  and
      Lovenia, Holy  and
      Ji, Ziwei  and
      Yu, Tiezheng  and
      Chung, Willy  and
      Do, Quyet V.  and
      Xu, Yan  and
      Fung, Pascale",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.ijcnlp-main.45",
    doi = "10.18653/v1/2023.ijcnlp-main.45",
    pages = "675--718",
}

@misc{zhu_promptbench_2023,
	title = {{PromptBench}: {Towards} {Evaluating} the {Robustness} of {Large} {Language} {Models} on {Adversarial} {Prompts}},
	shorttitle = {{PromptBench}},
	url = {http://arxiv.org/abs/2306.04528},
	abstract = {The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs’ resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4, 788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil Zhenqiang and Xie, Xing},
	month = oct,
	year = {2023},
	note = {arXiv:2306.04528 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Zhu et al. - 2023 - PromptBench Towards Evaluating the Robustness of .pdf:C\:\\Users\\evasa\\Zotero\\storage\\FZPF3FQT\\Zhu et al. - 2023 - PromptBench Towards Evaluating the Robustness of .pdf:application/pdf},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1412.6572 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\9QHAWJKX\\Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\RZZU5LSR\\1412.html:text/html},
}

@misc{nicolae_adversarial_2019,
	title = {Adversarial {Robustness} {Toolbox} v1.0.0},
	url = {http://arxiv.org/abs/1807.01069},
	doi = {10.48550/arXiv.1807.01069},
	abstract = {Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian M. and Edwards, Ben},
	month = nov,
	year = {2019},
	note = {arXiv:1807.01069 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\5ZYM5H58\\Nicolae et al. - 2019 - Adversarial Robustness Toolbox v1.0.0.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\Y7MTGKYT\\1807.html:text/html},
}





@misc{ailem_examining_2024,
	title = {Examining the robustness of {LLM} evaluation to the distributional assumptions of benchmarks},
	url = {http://arxiv.org/abs/2404.16966},
	abstract = {Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model’s average performance across the test prompts of a benchmark to evaluate the model’s performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points.},
	language = {en},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Ailem, Melissa and Marazopoulou, Katerina and Siska, Charlotte and Bono, James},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16966 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Ailem et al. - 2024 - Examining the robustness of LLM evaluation to the .pdf:C\:\\Users\\evasa\\Zotero\\storage\\VQLNIIIM\\Ailem et al. - 2024 - Examining the robustness of LLM evaluation to the .pdf:application/pdf},
}

@misc{alzahrani_when_2024,
	title = {When {Benchmarks} are {Targets}: {Revealing} the {Sensitivity} of {Large} {Language} {Model} {Leaderboards}},
	shorttitle = {When {Benchmarks} are {Targets}},
	url = {http://arxiv.org/abs/2402.01781},
	abstract = {Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value — we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks.},
	language = {en},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Alzahrani, Norah and Alyahya, Hisham Abdullah and Alnumay, Yazeed and Alrashed, Sultan and Alsubaie, Shaykhah and Almushaykeh, Yusef and Mirza, Faisal and Alotaibi, Nouf and Altwairesh, Nora and Alowisheq, Areeb and Bari, M. Saiful and Khan, Haidar},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01781 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Alzahrani et al. - 2024 - When Benchmarks are Targets Revealing the Sensiti.pdf:C\:\\Users\\evasa\\Zotero\\storage\\NV3SWAHG\\Alzahrani et al. - 2024 - When Benchmarks are Targets Revealing the Sensiti.pdf:application/pdf},
}


@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}



@misc{mizrahi_state_2024,
	title = {State of {What} {Art}? {A} {Call} for {Multi}-{Prompt} {LLM} {Evaluation}},
	shorttitle = {State of {What} {Art}?},
	url = {http://arxiv.org/abs/2401.00595},
	doi = {10.48550/arXiv.2401.00595},
	abstract = {Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Mizrahi, Moran and Kaplan, Guy and Malkin, Dan and Dror, Rotem and Shahaf, Dafna and Stanovsky, Gabriel},
	month = may,
	year = {2024},
	note = {arXiv:2401.00595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\9DWGT5U6\\Mizrahi et al. - 2024 - State of What Art A Call for Multi-Prompt LLM Eva.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\FNY348K8\\2401.html:text/html},
}



@article{shannon_mathematical_nodate,
	title = {A {Mathematical} {Theory} of {Communication}},
	language = {en},
	author = {Shannon, C. E.},
    year = {1948},
	file = {Shannon - A Mathematical Theory of Communication.pdf:C\:\\Users\\evasa\\Zotero\\storage\\TN4CLI4B\\Shannon - A Mathematical Theory of Communication.pdf:application/pdf},
}


@misc{wang2020supergluestickierbenchmarkgeneralpurpose,
      title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems}, 
      author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2020},
      eprint={1905.00537},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.00537}, 
}

@misc{bang_multitask_2023,
	title = {A {Multitask}, {Multilingual}, {Multimodal} {Evaluation} of {ChatGPT} on {Reasoning}, {Hallucination}, and {Interactivity}},
	url = {http://arxiv.org/abs/2302.04023},
	abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04023 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\27R7JMCA\\2302.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\XZ8QJVJP\\Bang et al. - 2023 - A Multitask, Multilingual, Multimodal Evaluation o.pdf:application/pdf},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\BRBU33HA\\2301.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\JNJ7Y4DB\\Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf:application/pdf},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. et al},
	year = {2022},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\XYXSCH5C\\Srivastava et al. - 2023 - Beyond the Imitation Game Quantifying and extrapo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\DLH5BP6K\\2206.html:text/html},
}

@misc{lin_truthfulqa_2022,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	note = {arXiv:2109.07958 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\ZNZT86IV\\2109.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\W6IZLZZ3\\Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf:application/pdf},
}

@inproceedings{lai_race_2017,
	address = {Copenhagen, Denmark},
	title = {{RACE}: {Large}-scale {ReAding} {Comprehension} {Dataset} {From} {Examinations}},
	shorttitle = {{RACE}},
	url = {https://aclanthology.org/D17-1082},
	doi = {10.18653/v1/D17-1082},
	abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ glai1/data/race/and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
	urldate = {2023-09-11},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
	month = sep,
	year = {2017},
	pages = {785--794},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\C54VEBPA\\Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset Fr.pdf:application/pdf},
}

@inproceedings{tedeschi_whats_2023,
	address = {Toronto, Canada},
	title = {What’s the {Meaning} of {Superhuman} {Performance} in {Today}’s {NLU}?},
	url = {https://aclanthology.org/2023.acl-long.697},
	doi = {10.18653/v1/2023.acl-long.697},
	abstract = {In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.},
	language = {en},
	urldate = {2023-09-11},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tedeschi, Simone and Bos, Johan and Declerck, Thierry and Hajič, Jan and Hershcovich, Daniel and Hovy, Eduard and Koller, Alexander and Krek, Simon and Schockaert, Steven and Sennrich, Rico and Shutova, Ekaterina and Navigli, Roberto},
	year = {2023},
	pages = {12471--12491},
	file = {Tedeschi et al. - 2023 - What’s the Meaning of Superhuman Performance in To.pdf:C\:\\Users\\evasa\\Zotero\\storage\\F2WDTIAC\\Tedeschi et al. - 2023 - What’s the Meaning of Superhuman Performance in To.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux et al},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\9KQ83BDP\\Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\K27GI8RT\\2302.html:text/html},
}


@misc{touvron2023llama2openfoundation,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas et al},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\JR3CJJ4U\\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\VZ8GECCC\\2307.html:text/html},
}


@misc{plaza-del-arco_leveraging_2023,
	title = {Leveraging {Label} {Variation} in {Large} {Language} {Models} for {Zero}-{Shot} {Text} {Classification}},
	url = {http://arxiv.org/abs/2307.12973},
	abstract = {The zero-shot learning capabilities of large language models (LLMs) make them ideal for text classification without annotation or supervised training. Many studies have shown impressive results across multiple tasks. While tasks, data, and results differ widely, their similarities to human annotation can aid us in tackling new tasks with minimal expenses. We evaluate using 5 state-of-the-art LLMs as “annotators” on 5 different tasks (age, gender, topic, sentiment prediction, and hate speech detection), across 4 languages: English, French, German, and Spanish. No single model excels at all tasks, across languages, or across all labels within a task. However, aggregation techniques designed for human annotators perform substantially better than any one individual model. Overall, though, LLMs do not rival even simple supervised models, so they do not (yet) replace the need for human annotation. We also discuss the tradeoffs between speed, accuracy, cost, and bias when it comes to aggregated model labeling versus human annotation.},
	language = {en},
	urldate = {2023-09-13},
	publisher = {arXiv},
	author = {Plaza-del-Arco, Flor Miriam and Nozza, Debora and Hovy, Dirk},
	month = jul,
	year = {2023},
	note = {arXiv:2307.12973 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Plaza-del-Arco et al. - 2023 - Leveraging Label Variation in Large Language Model.pdf:C\:\\Users\\evasa\\Zotero\\storage\\QMH3AI3G\\Plaza-del-Arco et al. - 2023 - Leveraging Label Variation in Large Language Model.pdf:application/pdf},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2023-09-13},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\HPA22Z23\\2009.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\MXDNQEY3\\Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-09-13},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\NETJP3CB\\2005.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\JRQJ5NGB\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	shorttitle = {Natural {Questions}},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	urldate = {2023-09-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {452--466},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\T74LTJR7\\Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:application/pdf},
}

@misc{adelani_sib-200_2023,
	title = {{SIB}-200: {A} {Simple}, {Inclusive}, and {Big} {Evaluation} {Dataset} for {Topic} {Classification} in 200+ {Languages} and {Dialects}},
	shorttitle = {{SIB}-200},
	url = {http://arxiv.org/abs/2309.07445},
	doi = {10.48550/arXiv.2309.07445},
	abstract = {Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, under-represented language families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset will encourage a more inclusive evaluation of multilingual language models on a more diverse set of languages. https://github.com/dadelani/sib-200},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Vassilyev, Nikita and Alabi, Jesujoba O. and Mao, Yanke and Gao, Haonan and Lee, Annie En-Shiun},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07445 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\W4EYJFYC\\Adelani et al. - 2023 - SIB-200 A Simple, Inclusive, and Big Evaluation D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\KJ3BACT4\\2309.html:text/html},
}

@misc{yang_large_2023,
	title = {Large {Language} {Models} as {Optimizers}},
	url = {http://arxiv.org/abs/2309.03409},
	abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
	month = sep,
	year = {2023},
	note = {arXiv:2309.03409 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\EUS5X6LM\\2309.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\8QWV26KW\\Yang et al. - 2023 - Large Language Models as Optimizers.pdf:application/pdf},
}

@misc{sakaguchi_winogrande_2019,
	title = {{WinoGrande}: {An} {Adversarial} {Winograd} {Schema} {Challenge} at {Scale}},
	shorttitle = {{WinoGrande}},
	url = {http://arxiv.org/abs/1907.10641},
	abstract = {The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = nov,
	year = {2019},
	note = {arXiv:1907.10641 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\7IQDL7A3\\1907.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\7PTWGFF3\\Sakaguchi et al. - 2019 - WinoGrande An Adversarial Winograd Schema Challen.pdf:application/pdf},
}

@misc{zhong_agieval_2023,
	title = {{AGIEval}: {A} {Human}-{Centric} {Benchmark} for {Evaluating} {Foundation} {Models}},
	shorttitle = {{AGIEval}},
	url = {http://arxiv.org/abs/2304.06364},
	doi = {10.48550/arXiv.2304.06364},
	abstract = {Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95\% accuracy rate on the SAT Math test and a 92.5\% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/ruixiangcui/AGIEval.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
	month = sep,
	year = {2023},
	note = {arXiv:2304.06364 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\2DAXDZZZ\\Zhong et al. - 2023 - AGIEval A Human-Centric Benchmark for Evaluating .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\JL2YCEU2\\2304.html:text/html},
}

@inproceedings{mihaylov_can_2018,
	address = {Brussels, Belgium},
	title = {Can a {Suit} of {Armor} {Conduct} {Electricity}? {A} {New} {Dataset} for {Open} {Book} {Question} {Answering}},
	shorttitle = {Can a {Suit} of {Armor} {Conduct} {Electricity}?},
	url = {http://aclweb.org/anthology/D18-1260},
	doi = {10.18653/v1/D18-1260},
	abstract = {We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on OpenBookQA is close to 92\%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.},
	language = {en},
	urldate = {2023-09-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
	year = {2018},
	pages = {2381--2391},
	file = {Mihaylov et al. - 2018 - Can a Suit of Armor Conduct Electricity A New Dat.pdf:C\:\\Users\\evasa\\Zotero\\storage\\KVDWI82S\\Mihaylov et al. - 2018 - Can a Suit of Armor Conduct Electricity A New Dat.pdf:application/pdf},
}


@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = jul,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\DUPZQYDW\\Zheng et al. - 2023 - Judging LLM-as-a-judge with MT-Bench and Chatbot A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\THJE8ZNP\\2306.html:text/html},
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@misc{chang_survey_2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = aug,
	year = {2023},
	note = {arXiv:2307.03109 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\L9JDEKRR\\2307.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\JG9A7YGK\\Chang et al. - 2023 - A Survey on Evaluation of Large Language Models.pdf:application/pdf},
}

@misc{vu_freshllms_2023,
	title = {{FreshLLMs}: {Refreshing} {Large} {Language} {Models} with {Search} {Engine} {Augmentation}},
	shorttitle = {{FreshLLMs}},
	url = {http://arxiv.org/abs/2310.03214},
	abstract = {Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03214 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\29XWYPHM\\2310.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\FHULEYJD\\Vu et al. - 2023 - FreshLLMs Refreshing Large Language Models with S.pdf:application/pdf},
}

@misc{noauthor_challenges_nodate,
	title = {Challenges in evaluating {AI} systems},
	url = {https://www.anthropic.com/index/evaluating-ai-systems},
	abstract = {Here, we outline challenges that we have encountered while evaluating our own models to give readers a sense of what developing, implementing, and interpreting model evaluations looks like in practice.},
	language = {en},
	urldate = {2023-10-19},
	journal = {Anthropic},
	file = {Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\T2SL66VC\\evaluating-ai-systems.html:text/html},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian others},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\XK6VXTRL\\Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\NS24KS2U\\2310.html:text/html},
}

@misc{parrish_bbq_2022,
	title = {{BBQ}: {A} {Hand}-{Built} {Bias} {Benchmark} for {Question} {Answering}},
	shorttitle = {{BBQ}},
	url = {http://arxiv.org/abs/2110.08193},
	doi = {10.48550/arXiv.2110.08193},
	abstract = {It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluates model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model's biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model's outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R.},
	month = mar,
	year = {2022},
	note = {arXiv:2110.08193 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\XC2BS8L9\\Parrish et al. - 2022 - BBQ A Hand-Built Bias Benchmark for Question Answ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\VC2V6YA8\\2110.html:text/html},
}

@misc{noauthor_state_nodate,
	title = {State of {AI} {Report} 2023 - {ONLINE} - {Presentaciones} de {Google}},
	url = {https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit#slide=id.g24daeb7f4f0_0_3373},
	urldate = {2023-10-24},
	file = {State of AI Report 2023 - ONLINE - Presentaciones de Google:C\:\\Users\\evasa\\Zotero\\storage\\SQY9LTYI\\edit.html:text/html},
}

@misc{noauthor_state_nodate-1,
	title = {State of {AI} {Report} 2023 - {ONLINE}},
	url = {https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ},
	urldate = {2023-10-24},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\B6SDS2HS\\State of AI Report 2023 - ONLINE.pdf:application/pdf},
}

@misc{geminiteam2024geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth et al},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière et al},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}

@misc{nunes_evaluating_2023,
	title = {Evaluating {GPT}-3.5 and {GPT}-4 {Models} on {Brazilian} {University} {Admission} {Exams}},
	url = {http://arxiv.org/abs/2303.17003},
	abstract = {The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M{\textbackslash}'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87\%, largely surpassing GPT-3.5 by 11 points. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem.},
	urldate = {2024-03-04},
	publisher = {arXiv},
	author = {Nunes, Desnes and Primi, Ricardo and Pires, Ramon and Lotufo, Roberto and Nogueira, Rodrigo},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17003 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\QIUKL2SE\\2303.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\76ZN2APH\\Nunes et al. - 2023 - Evaluating GPT-3.5 and GPT-4 Models on Brazilian U.pdf:application/pdf},
}

@misc{liang_holistic_2023,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak et al},
	month = oct,
	year = {2023},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\RSVLSQN2\\Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\R5ACYVG3\\2211.html:text/html},
}

@article{clark_think_2018,
	title = {Think you have {Solved} {Question} {Answering}? {Try} {ARC}, the {AI2} {Reasoning} {Challenge}},
	shorttitle = {Think you have {Solved} {Question} {Answering}?},
	url = {https://www.semanticscholar.org/paper/Think-you-have-Solved-Question-Answering-Try-ARC%2C-Clark-Cowhey/88bb0a28bb58d847183ec505dda89b63771bb495},
	abstract = {We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.},
	urldate = {2024-03-05},
	journal = {ArXiv},
	author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
	month = mar,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\ZVU68J5G\\Clark et al. - 2018 - Think you have Solved Question Answering Try ARC,.pdf:application/pdf},
}

@misc{polo_tinybenchmarks_2024,
	title = {{tinyBenchmarks}: evaluating {LLMs} with fewer examples},
	shorttitle = {{tinyBenchmarks}},
	url = {http://arxiv.org/abs/2402.14992},
	doi = {10.48550/arXiv.2402.14992},
	abstract = {The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Polo, Felipe Maia and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14992 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\M6DDUN2N\\Polo et al. - 2024 - tinyBenchmarks evaluating LLMs with fewer example.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\PQ8IGKLD\\2402.html:text/html},
}

@misc{rein_gpqa_2023,
	title = {{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\&{A} {Benchmark}},
	shorttitle = {{GPQA}},
	url = {http://arxiv.org/abs/2311.12022},
	abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12022 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\F46HTT4M\\2311.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\ZZ6L5LUX\\Rein et al. - 2023 - GPQA A Graduate-Level Google-Proof Q&A Benchmark.pdf:application/pdf},
}

@article{zeng_measuring_2023,
	title = {Measuring {Massive} {Multitask} {Chinese} {Understanding}},
	abstract = {The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worstperforming models by nearly 18.6 percentage points on average. Across the four major domains, the highest average zero-shot accuracy of all models is 0.512. In the subdomains, only the GPT-3.5turbo model achieved a zero-shot accuracy of 0.693 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.239. By comprehensively evaluating the breadth and depth of knowledge across multiple disciplines, this test can more accurately identify the shortcomings of the models.},
	language = {en},
	author = {Zeng, Hui},
	year = {2023},
	file = {Zeng - Measuring Massive Multitask Chinese Understanding.pdf:C\:\\Users\\evasa\\Zotero\\storage\\D5HYKLQJ\\Zeng - Measuring Massive Multitask Chinese Understanding.pdf:application/pdf},
}

@misc{huang_c-eval_2023,
	title = {C-{Eval}: {A} {Multi}-{Level} {Multi}-{Discipline} {Chinese} {Evaluation} {Suite} for {Foundation} {Models}},
	shorttitle = {C-{Eval}},
	url = {http://arxiv.org/abs/2305.08322},
	doi = {10.48550/arXiv.2305.08322},
	abstract = {New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60\%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
	month = nov,
	year = {2023},
	note = {arXiv:2305.08322 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\UQVCNWF9\\Huang et al. - 2023 - C-Eval A Multi-Level Multi-Discipline Chinese Eva.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\P74HCIAZ\\2305.html:text/html},
}

@misc{liu_recall_2023,
	title = {{RECALL}: {A} {Benchmark} for {LLMs} {Robustness} against {External} {Counterfactual} {Knowledge}},
	shorttitle = {{RECALL}},
	url = {http://arxiv.org/abs/2311.08147},
	abstract = {LLMs and AI chatbots have improved people's efficiency in various fields. However, the necessary knowledge for answering the question may be beyond the models' knowledge boundaries. To mitigate this issue, many researchers try to introduce external knowledge, such as knowledge graphs and Internet contents, into LLMs for up-to-date information. However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response. Thus there is a pressing need for LLMs to possess the ability to distinguish reliable information from external knowledge. Therefore, to evaluate the ability of LLMs to discern the reliability of external knowledge, we create a benchmark from existing knowledge bases. Our benchmark consists of two tasks, Question Answering and Text Generation, and for each task, we provide models with a context containing counterfactual information. Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Liu, Yi and Huang, Lianzhe and Li, Shicheng and Chen, Sishuo and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08147 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\FP5EB8H7\\2311.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\U6XG2QXX\\Liu et al. - 2023 - RECALL A Benchmark for LLMs Robustness against Ex.pdf:application/pdf},
}

@misc{mccann_natural_2018,
	title = {The {Natural} {Language} {Decathlon}: {Multitask} {Learning} as {Question} {Answering}},
	shorttitle = {The {Natural} {Language} {Decathlon}},
	url = {http://arxiv.org/abs/1806.08730},
	doi = {10.48550/arXiv.1806.08730},
	abstract = {Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	month = jun,
	year = {2018},
	note = {arXiv:1806.08730 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\C2HGBH89\\McCann et al. - 2018 - The Natural Language Decathlon Multitask Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\S3GQ2VG8\\1806.html:text/html},
}

@misc{conneau_senteval_2018,
	title = {{SentEval}: {An} {Evaluation} {Toolkit} for {Universal} {Sentence} {Representations}},
	shorttitle = {{SentEval}},
	url = {http://arxiv.org/abs/1803.05449},
	doi = {10.48550/arXiv.1803.05449},
	abstract = {We introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Conneau, Alexis and Kiela, Douwe},
	month = mar,
	year = {2018},
	note = {arXiv:1803.05449 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\YBWT9RV5\\Conneau y Kiela - 2018 - SentEval An Evaluation Toolkit for Universal Sent.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\59F25LLE\\1803.html:text/html},
}

@misc{xia_fofo_2024,
	title = {{FOFO}: {A} {Benchmark} to {Evaluate} {LLMs}' {Format}-{Following} {Capability}},
	shorttitle = {{FOFO}},
	url = {http://arxiv.org/abs/2402.18667},
	doi = {10.48550/arXiv.2402.18667},
	abstract = {This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's role in guiding the selection of domain-specific AI agents. FoFo is released here at https://github.com/SalesforceAIResearch/FoFo.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Xia, Congying and Xing, Chen and Du, Jiangshu and Yang, Xinyi and Feng, Yihao and Xu, Ran and Yin, Wenpeng and Xiong, Caiming},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18667 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\SDQLUXHA\\Xia et al. - 2024 - FOFO A Benchmark to Evaluate LLMs' Format-Followi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\25I5UUCN\\2402.html:text/html},
}

@misc{yang_aqa-bench_2024,
	title = {{AQA}-{Bench}: {An} {Interactive} {Benchmark} for {Evaluating} {LLMs}' {Sequential} {Reasoning} {Ability}},
	shorttitle = {{AQA}-{Bench}},
	url = {http://arxiv.org/abs/2402.09404},
	doi = {10.48550/arXiv.2402.09404},
	abstract = {This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing interactive examples may inadvertently hurt few-shot performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Yang, Siwei and Zhao, Bingchen and Xie, Cihang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09404 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\YZK28XFE\\Yang et al. - 2024 - AQA-Bench An Interactive Benchmark for Evaluating.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\PQ4TQ9JX\\2402.html:text/html},
}

@misc{ahuja_mega_2023,
	title = {{MEGA}: {Multilingual} {Evaluation} of {Generative} {AI}},
	shorttitle = {{MEGA}},
	url = {http://arxiv.org/abs/2303.12528},
	abstract = {Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Axmed, Maxamed and Bali, Kalika and Sitaram, Sunayana},
	month = oct,
	year = {2023},
	note = {arXiv:2303.12528 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\S6V9XW4X\\2303.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\PLARL5N5\\Ahuja et al. - 2023 - MEGA Multilingual Evaluation of Generative AI.pdf:application/pdf},
}

@misc{agerri_lessons_2023,
	title = {Lessons learned from the evaluation of {Spanish} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.08390},
	abstract = {Given the impact of language models on the field of Natural Language Processing, a number of Spanish encoder-only masked language models (aka BERTs) have been trained and released. These models were developed either within large projects using very large private corpora or by means of smaller scale academic efforts leveraging freely available data. In this paper we present a comprehensive head-to-head comparison of language models for Spanish with the following results: (i) Previously ignored multilingual models from large companies fare better than monolingual models, substantially changing the evaluation landscape of language models in Spanish; (ii) Results across the monolingual models are not conclusive, with supposedly smaller and inferior models performing competitively. Based on these empirical results, we argue for the need of more research to understand the factors underlying them. In this sense, the effect of corpus size, quality and pre-training techniques need to be further investigated to be able to obtain Spanish monolingual models significantly better than the multilingual ones released by large private companies, specially in the face of rapid ongoing progress in the field. The recent activity in the development of language technology for Spanish is to be welcomed, but our results show that building language models remains an open, resource-heavy problem which requires to marry resources (monetary and/or computational) with the best research expertise and practice.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Agerri, Rodrigo and Agirre, Eneko},
	month = sep,
	year = {2023},
	note = {arXiv:2212.08390 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\RUFSY4SI\\2212.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\AVSF3FFT\\Agerri y Agirre - 2023 - Lessons learned from the evaluation of Spanish Lan.pdf:application/pdf},
}

@misc{zhang_m3exam_2023,
	title = {{M3Exam}: {A} {Multilingual}, {Multimodal}, {Multilevel} {Benchmark} for {Examining} {Large} {Language} {Models}},
	shorttitle = {{M3Exam}},
	url = {http://arxiv.org/abs/2306.05179},
	doi = {10.48550/arXiv.2306.05179},
	abstract = {Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23{\textbackslash}\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at {\textbackslash}url\{https://github.com/DAMO-NLP-SG/M3Exam\}.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Zhang, Wenxuan and Aljunied, Sharifah Mahani and Gao, Chang and Chia, Yew Ken and Bing, Lidong},
	month = nov,
	year = {2023},
	note = {arXiv:2306.05179 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\47IBM52G\\Zhang et al. - 2023 - M3Exam A Multilingual, Multimodal, Multilevel Ben.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\M2YY6VUR\\2306.html:text/html},
}

@misc{lai_chatgpt_2023,
	title = {{ChatGPT} {Beyond} {English}: {Towards} a {Comprehensive} {Evaluation} of {Large} {Language} {Models} in {Multilingual} {Learning}},
	shorttitle = {{ChatGPT} {Beyond} {English}},
	url = {http://arxiv.org/abs/2304.05613},
	abstract = {Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Lai, Viet Dac and Ngo, Nghia Trung and Veyseh, Amir Pouran Ben and Man, Hieu and Dernoncourt, Franck and Bui, Trung and Nguyen, Thien Huu},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05613 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\ULSXBZYL\\2304.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\QE2WP4UQ\\Lai et al. - 2023 - ChatGPT Beyond English Towards a Comprehensive Ev.pdf:application/pdf},
}

@misc{srivastava_functional_2024,
	title = {Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}},
	url = {http://arxiv.org/abs/2402.19450},
	abstract = {We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35\% to 80.31\% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19450 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\ZXNUREJG\\2402.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\XG9X3V6Z\\Srivastava et al. - 2024 - Functional Benchmarks for Robust Evaluation of Rea.pdf:application/pdf},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\DVLUV6ZZ\\2201.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\AAFCLT7M\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2204.05862},
	doi = {10.48550/arXiv.2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\L9CR63FV\\Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\JTRR67QX\\2204.html:text/html},
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam et al},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{jiang2024mixtralexperts,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot et al},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04088}, 
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Training} {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {https://paperswithcode.com/paper/training-verifiers-to-solve-math-word},
	abstract = {Implemented in 3 code libraries.},
	language = {en},
	urldate = {2024-04-11},
	file = {Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\GFLC3DRH\\training-verifiers-to-solve-math-word.html:text/html},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\GD6BUWKG\\2110.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\3YI5E86B\\Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf:application/pdf},
}

@misc{zellers2019hellaswagmachinereallyfinish,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.07830}, 
}

@misc{zellers_hellaswag_2019,
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {http://arxiv.org/abs/1905.07830},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95\% accuracy), state-of-the-art models struggle ({\textless}48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	month = may,
	year = {2019},
	note = {arXiv:1905.07830 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\892KV5MG\\1905.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\A73R5ZLJ\\Zellers et al. - 2019 - HellaSwag Can a Machine Really Finish Your Senten.pdf:application/pdf},
}

@misc{bisk2019piqareasoningphysicalcommonsense,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.11641}, 
}


@misc{huang_cosmos_2019,
	title = {Cosmos {QA}: {Machine} {Reading} {Comprehension} with {Contextual} {Commonsense} {Reasoning}},
	shorttitle = {Cosmos {QA}},
	url = {http://arxiv.org/abs/1909.00277},
	abstract = {Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people's everyday narratives, asking such questions as "what might be the possible reason of ...?", or "what would have happened if ..." that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4\%) and human performance (94\%), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone.github.io/cosmos.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Huang, Lifu and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = sep,
	year = {2019},
	note = {arXiv:1909.00277 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\79JMA99Y\\1909.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\5YNRXV68\\Huang et al. - 2019 - Cosmos QA Machine Reading Comprehension with Cont.pdf:application/pdf},
}



@misc{youssef_queen_2024,
	title = {The {Queen} of {England} is not {England}'s {Queen}: {On} the {Lack} of {Factual} {Coherency} in {PLMs}},
	shorttitle = {The {Queen} of {England} is not {England}'s {Queen}},
	url = {http://arxiv.org/abs/2402.01453},
	abstract = {Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Youssef, Paul and Schlötterer, Jörg and Seifert, Christin},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01453 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\9A6S87CZ\\2402.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\U2RG6FT6\\Youssef et al. - 2024 - The Queen of England is not England's Queen On th.pdf:application/pdf},
}

@misc{wang_chain--thought_2024,
	title = {Chain-of-{Thought} {Reasoning} {Without} {Prompting}},
	url = {http://arxiv.org/abs/2402.10200},
	doi = {10.48550/arXiv.2402.10200},
	abstract = {In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Zhou, Denny},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10200 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\XB8HKZ3I\\Wang y Zhou - 2024 - Chain-of-Thought Reasoning Without Prompting.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\67JKKTID\\2402.html:text/html},
}

@misc{noauthor_challenges_2021,
	title = {Challenges and {Opportunities} in {NLP} {Benchmarking}},
	url = {https://www.ruder.io/nlp-benchmarking/},
	abstract = {Recent NLP models have outpaced the benchmarks to test for them. This post provides an overview of challenges and opportunities for NLP benchmarks.},
	language = {en},
	urldate = {2024-04-21},
	journal = {ruder.io},
	month = aug,
	year = {2021},
}

@misc{noauthor_mteb_nodate,
	title = {{MTEB}: {Massive} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MTEB}},
	url = {https://huggingface.co/blog/mteb},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-04-21},
	file = {Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\RT2UUV9G\\mteb.html:text/html},
}

@misc{noauthor_open_nodate,
	title = {Open {LLM} {Leaderboard} - a {Hugging} {Face} {Space} by {HuggingFaceH4}},
	url = {https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	abstract = {Track, rank and evaluate open LLMs and chatbots},
	urldate = {2024-05-20},
	file = {Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\SU4PTEA8\\open_llm_leaderboard.html:text/html},
}

@misc{zheng_judging_2023-1,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https: //github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	language = {en},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf:C\:\\Users\\evasa\\Zotero\\storage\\R3N6IZWB\\Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf:application/pdf},
}

@misc{kim_prometheus_2024,
	title = {Prometheus 2: {An} {Open} {Source} {Language} {Model} {Specialized} in {Evaluating} {Other} {Language} {Models}},
	shorttitle = {Prometheus 2},
	url = {http://arxiv.org/abs/2405.01535},
	abstract = {Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of opensource LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than it’s predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, PROMETHEUS 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available 1.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Kim, Seungone and Suk, Juyoung and Longpre, Shayne and Lin, Bill Yuchen and Shin, Jamin and Welleck, Sean and Neubig, Graham and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
	month = may,
	year = {2024},
	note = {arXiv:2405.01535 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Kim et al. - 2024 - Prometheus 2 An Open Source Language Model Specia.pdf:C\:\\Users\\evasa\\Zotero\\storage\\8YBFYZZ9\\Kim et al. - 2024 - Prometheus 2 An Open Source Language Model Specia.pdf:application/pdf},
}

@misc{ruder_evolving_2024,
	title = {The {Evolving} {Landscape} of {LLM} {Evaluation}},
	url = {https://newsletter.ruder.io/p/the-evolving-landscape-of-llm-evaluation?utm_medium=android&triedRedirect=true},
	abstract = {Throughout recent years, LLM capabilities have outpaced evaluation benchmarks. This is not a new development. What is new is that the set of standard LLM evals has further narrowed—and there are questions regarding the reliability of even this small set of benchmarks.},
	language = {en},
	urldate = {2024-05-23},
	author = {Ruder, Sebastian},
	month = feb,
	year = {2024},
}

@misc{zheng_judging_2023-2,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https: //github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf:C\:\\Users\\evasa\\Zotero\\storage\\KG2TUJAZ\\Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf:application/pdf},
}



@inproceedings{qi_preserving_2023,
	address = {Singapore},
	title = {Preserving {Knowledge} {Invariance}: {Rethinking} {Robustness} {Evaluation} of {Open} {Information} {Extraction}},
	shorttitle = {Preserving {Knowledge} {Invariance}},
	url = {https://aclanthology.org/2023.emnlp-main.360},
	doi = {10.18653/v1/2023.emnlp-main.360},
	abstract = {The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F\_1 score. Our resources and code will be publicly available.},
	urldate = {2024-05-27},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Qi, Ji and Zhang, Chuchun and Wang, Xiaozhi and Zeng, Kaisheng and Yu, Jifan and Liu, Jinxin and Hou, Lei and Li, Juanzi and Bin, Xu},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {5876--5890},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\AL2TX3JY\\Qi et al. - 2023 - Preserving Knowledge Invariance Rethinking Robust.pdf:application/pdf},
}

@inproceedings{zhang_exploring_2023,
	address = {Singapore},
	title = {Exploring the {Cognitive} {Knowledge} {Structure} of {Large} {Language} {Models}: {An} {Educational} {Diagnostic} {Assessment} {Approach}},
	shorttitle = {Exploring the {Cognitive} {Knowledge} {Structure} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.111},
	doi = {10.18653/v1/2023.findings-emnlp.111},
	abstract = {Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.},
	urldate = {2024-05-29},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zheyuan and Yu, Jifan and Li, Juanzi and Hou, Lei},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {1643--1650},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\I56H7PF6\\Zhang et al. - 2023 - Exploring the Cognitive Knowledge Structure of Lar.pdf:application/pdf},
}

@misc{wang_robustness_2023,
	title = {On the {Robustness} of {ChatGPT}: {An} {Adversarial} and {Out}-of-distribution {Perspective}},
	shorttitle = {On the {Robustness} of {ChatGPT}},
	url = {http://arxiv.org/abs/2302.12095},
	abstract = {ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and Jiao, Binxin and Zhang, Yue and Xie, Xing},
	month = aug,
	year = {2023},
	note = {arXiv:2302.12095 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Wang et al. - 2023 - On the Robustness of ChatGPT An Adversarial and O.pdf:C\:\\Users\\evasa\\Zotero\\storage\\FYEKNPZ7\\Wang et al. - 2023 - On the Robustness of ChatGPT An Adversarial and O.pdf:application/pdf},
}



@inproceedings{zeng_openattack_2021,
	title = {{OpenAttack}: {An} {Open}-source {Textual} {Adversarial} {Attack} {Toolkit}},
	shorttitle = {{OpenAttack}},
	url = {http://arxiv.org/abs/2009.09191},
	doi = {10.18653/v1/2021.acl-demo.43},
	abstract = {Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an opensource textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great ﬂexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/ OpenAttack.},
	language = {en},
	urldate = {2024-05-30},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {System} {Demonstrations}},
	author = {Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Ma, Zixian and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
	year = {2021},
	note = {arXiv:2009.09191 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	pages = {363--371},
	file = {Zeng et al. - 2021 - OpenAttack An Open-source Textual Adversarial Att.pdf:C\:\\Users\\evasa\\Zotero\\storage\\SZHKWPSS\\Zeng et al. - 2021 - OpenAttack An Open-source Textual Adversarial Att.pdf:application/pdf},
}

@misc{wang_adversarial_2022,
	title = {Adversarial {GLUE}: {A} {Multi}-{Task} {Benchmark} for {Robustness} {Evaluation} of {Language} {Models}},
	shorttitle = {Adversarial {GLUE}},
	url = {http://arxiv.org/abs/2111.02840},
	abstract = {Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our ﬁndings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90\% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform careful ﬁltering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
	month = jan,
	year = {2022},
	note = {arXiv:2111.02840 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Wang et al. - 2022 - Adversarial GLUE A Multi-Task Benchmark for Robus.pdf:C\:\\Users\\evasa\\Zotero\\storage\\QLR9G4UZ\\Wang et al. - 2022 - Adversarial GLUE A Multi-Task Benchmark for Robus.pdf:application/pdf},
}




@inproceedings{wang_measure_2022,
	address = {Seattle, United States},
	title = {Measure and {Improve} {Robustness} in {NLP} {Models}: {A} {Survey}},
	shorttitle = {Measure and {Improve} {Robustness} in {NLP} {Models}},
	url = {https://aclanthology.org/2022.naacl-main.339},
	doi = {10.18653/v1/2022.naacl-main.339},
	abstract = {As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models' robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.},
	urldate = {2024-05-31},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xuezhi and Wang, Haohan and Yang, Diyi},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {4569--4586},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\8526UFYW\\Wang et al. - 2022 - Measure and Improve Robustness in NLP Models A Su.pdf:application/pdf},
}

@misc{yang_closer_2020,
	title = {A {Closer} {Look} at {Accuracy} vs. {Robustness}},
	url = {http://arxiv.org/abs/2003.02460},
	doi = {10.48550/arXiv.2003.02460},
	abstract = {Current methods for training robust networks lead to a drop in test accuracy, which has led prior works to posit that a robustness-accuracy tradeoff may be inevitable in deep learning. We take a closer look at this phenomenon and first show that real image datasets are actually separated. With this property in mind, we then prove that robustness and accuracy should both be achievable for benchmark datasets through locally Lipschitz functions, and hence, there should be no inherent tradeoff between robustness and accuracy. Through extensive experiments with robustness methods, we argue that the gap between theory and practice arises from two limitations of current methods: either they fail to impose local Lipschitzness or they are insufficiently generalized. We explore combining dropout with robust training methods and obtain better generalization. We conclude that achieving robustness and accuracy in practice may require using methods that impose local Lipschitzness and augmenting them with deep learning generalization techniques. Code available at https://github.com/yangarbiter/robust-local-lipschitz},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Ruslan and Chaudhuri, Kamalika},
	month = jul,
	year = {2020},
	note = {arXiv:2003.02460 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\G35V2VGI\\Yang et al. - 2020 - A Closer Look at Accuracy vs. Robustness.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\TJAR7C5L\\2003.html:text/html},
}

@inproceedings{tsipras_robustness_2018,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {https://openreview.net/forum?id=SyxAb30cY7},
	abstract = {We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2024-05-31},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\NXJWPF9Q\\Tsipras et al. - 2018 - Robustness May Be at Odds with Accuracy.pdf:application/pdf},
}

@misc{polo_efficient_2024,
	title = {Efficient multi-prompt evaluation of {LLMs}},
	url = {http://arxiv.org/abs/2405.17202},
	doi = {10.48550/arXiv.2405.17202},
	abstract = {Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95\% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found at https://github.com/felipemaiapolo/prompt-eval.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Polo, Felipe Maia and Xu, Ronald and Weber, Lucas and Silva, Mírian and Bhardwaj, Onkar and Choshen, Leshem and de Oliveira, Allysson Flavio Melo and Sun, Yuekai and Yurochkin, Mikhail},
	month = may,
	year = {2024},
	note = {arXiv:2405.17202 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\7IZ7GD4L\\Polo et al. - 2024 - Efficient multi-prompt evaluation of LLMs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\K98A75T6\\2405.html:text/html},
}


@misc{noauthor_introducing_nodate,
	title = {Introducing {Hard} {Prompts} {Category} in {Chatbot} {Arena} {\textbar} {LMSYS} {Org}},
	url = {https://lmsys.org/blog/2024-05-17-category-hard},
	abstract = {{\textless}h3{\textgreater}{\textless}a id="background" class="anchor" href="\#background" aria-hidden="true"{\textgreater}{\textless}svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" vi...},
	language = {en},
	urldate = {2024-05-31},
	file = {Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\J6UBPLQE\\2024-05-17-category-hard.html:text/html},
}

@inproceedings{amigo_evaluating_2022,
	address = {Dublin, Ireland},
	title = {Evaluating {Extreme} {Hierarchical} {Multi}-label {Classification}},
	url = {https://aclanthology.org/2022.acl-long.399},
	doi = {10.18653/v1/2022.acl-long.399},
	abstract = {Several natural language processing (NLP) tasks are defined as a classification problem in its most complex form: Multi-label Hierarchical Extreme classification, in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency and the number of labels per item. We analyze the state of the art of evaluation metrics based on a set of formal properties and we define an information theoretic based metric inspired by the Information Contrast Model (ICM). Experiments on synthetic data and a case study on real data show the suitability of the ICM for such scenarios.},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Amigo, Enrique and Delgado, Agustín},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {5809--5819},
	file = {Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\6D6JG48J\\Amigo y Delgado - 2022 - Evaluating Extreme Hierarchical Multi-label Classi.pdf:application/pdf},
}

@article{shannon_mathematical_1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	language = {en},
	author = {Shannon, C E},
	year = {1948},
	file = {Shannon - A Mathematical Theory of Communication.pdf:C\:\\Users\\evasa\\Zotero\\storage\\TN4CLI4B\\Shannon - A Mathematical Theory of Communication.pdf:application/pdf},
}

@misc{li_survey_2023,
	title = {A {Survey} on {Out}-of-{Distribution} {Evaluation} of {Neural} {NLP} {Models}},
	url = {http://arxiv.org/abs/2306.15261},
	abstract = {Adversarial robustness, domain generalization and dataset biases are three active lines of research contributing to out-of-distribution (OOD) evaluation on neural NLP models. However, a comprehensive, integrated discussion of the three research lines is still lacking in the literature. In this survey, we 1) compare the three lines of research under a unifying deﬁnition; 2) summarize the data-generating processes and evaluation protocols for each line of research; and 3) emphasize the challenges and opportunities for future work.},
	language = {en},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Li, Xinzhe and Liu, Ming and Gao, Shang and Buntine, Wray},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15261 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Li et al. - 2023 - A Survey on Out-of-Distribution Evaluation of Neur.pdf:C\:\\Users\\evasa\\Zotero\\storage\\I28EJQKD\\Li et al. - 2023 - A Survey on Out-of-Distribution Evaluation of Neur.pdf:application/pdf},
}

@misc{yang_glue-x_2023,
	title = {{GLUE}-{X}: {Evaluating} {Natural} {Language} {Understanding} {Models} from an {Out}-of-distribution {Generalization} {Perspective}},
	shorttitle = {{GLUE}-{X}},
	url = {http://arxiv.org/abs/2211.08073},
	doi = {10.48550/arXiv.2211.08073},
	abstract = {Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue},
	month = may,
	year = {2023},
	note = {arXiv:2211.08073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Performance},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\IQ4RL94M\\Yang et al. - 2023 - GLUE-X Evaluating Natural Language Understanding .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\GMG4X5PQ\\2211.html:text/html},
}

@misc{noauthor_240304132_nodate,
	title = {[2403.04132] {Chatbot} {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
	url = {https://arxiv.org/abs/2403.04132},
	urldate = {2024-06-09},
	file = {[2403.04132] Chatbot Arena\: An Open Platform for Evaluating LLMs by Human Preference:C\:\\Users\\evasa\\Zotero\\storage\\C9N2GNDA\\2403.html:text/html},
}

@misc{chiang_chatbot_2024,
	title = {Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
	shorttitle = {Chatbot {Arena}},
	url = {http://arxiv.org/abs/2403.04132},
	abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at https://chat.lmsys.org.},
	language = {en},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLM.pdf:C\:\\Users\\evasa\\Zotero\\storage\\SE93ITXG\\Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLM.pdf:application/pdf},
}

@misc{nezhurina_alice_2024,
	title = {Alice in {Wonderland}: {Simple} {Tasks} {Showing} {Complete} {Reasoning} {Breakdown} in {State}-{Of}-the-{Art} {Large} {Language} {Models}},
	shorttitle = {Alice in {Wonderland}},
	url = {http://arxiv.org/abs/2406.02061},
	doi = {10.48550/arXiv.2406.02061},
	abstract = {Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Nezhurina, Marianna and Cipolina-Kun, Lucia and Cherti, Mehdi and Jitsev, Jenia},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02061 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\NGJKK64K\\Nezhurina et al. - 2024 - Alice in Wonderland Simple Tasks Showing Complete.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\TXZ7ASRS\\2406.html:text/html},
}

@article{krippendorff_reliability_2004,
	title = {Reliability in {Content} {Analysis}: {Some} {Common} {Misconceptions} and {Recommendations}},
	volume = {30},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1},
	issn = {0360-3989, 1468-2958},
	shorttitle = {Reliability in {Content} {Analysis}},
	url = {http://doi.wiley.com/10.1093/hcr/30.3.411},
	doi = {10.1093/hcr/30.3.411},
	abstract = {In a recent article published in this journal, Lombard, Snyder-Duch, and Bracken (2002) surveyed 200 content analyses for their reporting of reliability tests; compared the virtues and drawbacks of five popular reliability measures; and proposed guidelines and standards for their use. Their discussion revealed that numerous misconceptions circulate in the content analysis literature regarding how these measures behave and can aid or deceive content analysts in their effort to ensure the reliability of their data. This paper proposes three conditions for statistical measures to serve as indices of the reliability of data and examines the mathematical structure and the behavior of the five coefficients discussed by the authors, plus two others. It compares common beliefs about these coefficients with what they actually do and concludes with alternative recommendations for testing reliability in content analysis and similar data-making efforts.},
	language = {en},
	number = {3},
	urldate = {2024-06-10},
	journal = {Human Communication Research},
	author = {Krippendorff, K.},
	month = jul,
	year = {2004},
	pages = {411--433},
	file = {Krippendorff - 2004 - Reliability in Content Analysis Some Common Misco.pdf:C\:\\Users\\evasa\\Zotero\\storage\\TCYK4LZC\\Krippendorff - 2004 - Reliability in Content Analysis Some Common Misco.pdf:application/pdf},
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman et al},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@misc{openai2024gpt4o,
      title={GPT-4o System Card}, 
      author={OpenAI},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@misc{openai2024o1,
    title={OpenAI o1 System Card},
    author={OpenAI},
    year={2024},
    url={https://openai.com/index/openai-o1-system-card/},
}

@misc{yu2023kolacarefullybenchmarkingworld,
      title={KoLA: Carefully Benchmarking World Knowledge of Large Language Models}, 
      author={Jifan Yu and Xiaozhi Wang and Shangqing Tu and Shulin Cao and Daniel Zhang-Li and Xin Lv and Hao Peng and Zijun Yao et al},
      year={2023},
      eprint={2306.09296},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09296}, 
}

@book{bloom1956taxonomy,
  title={Taxonomy of educational objectives: The classification of educational goals. Handbook I: Cognitive domain},
  editor={Bloom, Benjamin S.},
  year={1956},
  publisher={Longmans, Green},
  address={New York}
}

@misc{mizrahi2024stateartmultipromptllm,
      title={State of What Art? A Call for Multi-Prompt LLM Evaluation}, 
      author={Moran Mizrahi and Guy Kaplan and Dan Malkin and Rotem Dror and Dafna Shahaf and Gabriel Stanovsky},
      year={2024},
      eprint={2401.00595},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.00595}, 
}

@misc{polo2024efficientmultipromptevaluationllms,
      title={Efficient multi-prompt evaluation of LLMs}, 
      author={Felipe Maia Polo and Ronald Xu and Lucas Weber and Mírian Silva and Onkar Bhardwaj and Leshem Choshen and Allysson Flavio Melo de Oliveira and Yuekai Sun and Mikhail Yurochkin},
      year={2024},
      eprint={2405.17202},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17202}, 
}

@misc{sclar2023quantifyinglanguagemodelssensitivity,
      title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting}, 
      author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
      year={2023},
      eprint={2310.11324},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11324}, 
}

@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
}

@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  
      et al",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@misc{choi2018quacquestionanswering,
      title={QuAC : Question Answering in Context}, 
      author={Eunsol Choi and He He and Mohit Iyyer and Mark Yatskar and Wen-tau Yih and Yejin Choi and Percy Liang and Luke Zettlemoyer},
      year={2018},
      eprint={1808.07036},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1808.07036}, 
}

@misc{rajpurkar2016squad100000questionsmachine,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1606.05250}, 
}

@misc{hendrycks2021measuringmathematicalproblemsolving,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.03874}, 
}

@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda et al},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@misc{austin2021programsynthesislargelanguage,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2108.07732}, 
}

@misc{suzgun2022challengingbigbenchtaskschainofthought,
      title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them}, 
      author={Mirac Suzgun and Nathan Scales and Nathanael Schärli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei},
      year={2022},
      eprint={2210.09261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.09261}, 
}

@article{Jiang2024InvestigatingDC,
  title={Investigating Data Contamination for Pre-training Language Models},
  author={Minhao Jiang and Ken Ziyu Liu and Ming Zhong and Rylan Schaeffer and Siru Ouyang and Jiawei Han and Sanmi Koyejo},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.06059},
  url={https://api.semanticscholar.org/CorpusID:266933004}
}

@misc{dong2024generalizationmemorizationdatacontamination,
      title={Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models}, 
      author={Yihong Dong and Xue Jiang and Huanyu Liu and Zhi Jin and Bin Gu and Mengfei Yang and Ge Li},
      year={2024},
      eprint={2402.15938},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.15938}, 
}

@inproceedings{sainz-etal-2023-nlp,
    title = "{NLP} Evaluation in trouble: On the Need to Measure {LLM} Data Contamination for each Benchmark",
    author = "Sainz, Oscar  and
      Campos, Jon  and
      Garc{\'\i}a-Ferrero, Iker  and
      Etxaniz, Julen  and
      de Lacalle, Oier Lopez  and
      Agirre, Eneko",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.722",
    doi = "10.18653/v1/2023.findings-emnlp.722",
    pages = "10776--10787",
    abstract = "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
}

@misc{li2024quantifyingmultilingualperformancelarge,
      title={Quantifying Multilingual Performance of Large Language Models Across Languages}, 
      author={Zihao Li and Yucheng Shi and Zirui Liu and Fan Yang and Ali Payani and Ninghao Liu and Mengnan Du},
      year={2024},
      eprint={2404.11553},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.11553}, 
}

@misc{ahuja2024megaversebenchmarkinglargelanguage,
      title={MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks}, 
      author={Sanchit Ahuja and Divyanshu Aggarwal and Varun Gumma and Ishaan Watts and Ashutosh Sathe and Millicent Ochieng and Rishav Hada and Prachi Jain and Maxamed Axmed and Kalika Bali and Sunayana Sitaram},
      year={2024},
      eprint={2311.07463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07463}, 
}

@misc{blasi2021systematicinequalitieslanguagetechnology,
      title={Systematic Inequalities in Language Technology Performance across the World's Languages}, 
      author={Damián Blasi and Antonios Anastasopoulos and Graham Neubig},
      year={2021},
      eprint={2110.06733},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.06733}, 
}

@misc{golchin2024timetravelllmstracing,
      title={Time Travel in LLMs: Tracing Data Contamination in Large Language Models}, 
      author={Shahriar Golchin and Mihai Surdeanu},
      year={2024},
      eprint={2308.08493},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.08493}, 
}

@misc{rajore2024truceprivatebenchmarkingprevent,
      title={TRUCE: Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs}, 
      author={Tanmay Rajore and Nishanth Chandran and Sunayana Sitaram and Divya Gupta and Rahul Sharma and Kashish Mittal and Manohar Swaminathan},
      year={2024},
      eprint={2403.00393},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.00393}, 
}

@misc{claude_2024,
	title = {The {Claude} 3 {Model} {Family}: {Opus}, {Sonnet}, {Haiku}},
	url = {https://www-cdn.anthropic.com/f2986af8d052f26236f6251da62d16172cfabd6e/claude-3-model-card.pdf},
	author = {Anthropic},
	year = {2024},
	file = {claude-3-model-card.pdf:C\:\\Users\\evasa\\Zotero\\storage\\5ETCN8A5\\claude-3-model-card.pdf:application/pdf},
}



@misc{hu2020xtrememassivelymultilingualmultitask,
      title={XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization}, 
      author={Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
      year={2020},
      eprint={2003.11080},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.11080}, 
}

@misc{ruder2021xtremerchallengingnuancedmultilingual,
      title={XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation}, 
      author={Sebastian Ruder and Noah Constant and Jan Botha and Aditya Siddhant and Orhan Firat and Jinlan Fu and Pengfei Liu and Junjie Hu and Dan Garrette and Graham Neubig and Melvin Johnson},
      year={2021},
      eprint={2104.07412},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.07412}, 
}

@misc{yang2023rethinkingbenchmarkcontaminationlanguage,
      title={Rethinking Benchmark and Contamination for Language Models with Rephrased Samples}, 
      author={Shuo Yang and Wei-Lin Chiang and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2311.04850},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.04850}, 
}

@misc{gu2024xiezhieverupdatingbenchmarkholistic,
      title={Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation}, 
      author={Zhouhong Gu and Xiaoxuan Zhu and Haoning Ye and Lin Zhang and Jianchen Wang et al},
      year={2024},
      eprint={2306.05783},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05783}, 
}

@inproceedings{hardalov-etal-2020-exams,
    title = "{EXAMS}: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
    author = "Hardalov, Momchil  and
      Mihaylov, Todor  and
      Zlatkova, Dimitrina  and
      Dinkov, Yoan  and
      Koychev, Ivan  and
      Nakov, Preslav",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.438",
    doi = "10.18653/v1/2020.emnlp-main.438",
    pages = "5427--5444",
    abstract = "We propose EXAMS {--} a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at \url{http://github.com/mhardalov/exams-qa}.",
}

@misc{llama_3_2024,
	title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
	url = {https://ai.meta.com/blog/meta-llama-3/},
	author = {Meta},
	year = {2024},
}

@inproceedings{conneau-etal-2018-xnli,
    title = "{XNLI}: Evaluating Cross-lingual Sentence Representations",
    author = "Conneau, Alexis  and
      Rinott, Ruty  and
      Lample, Guillaume  and
      Williams, Adina  and
      Bowman, Samuel  and
      Schwenk, Holger  and
      Stoyanov, Veselin",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1269",
    doi = "10.18653/v1/D18-1269",
    pages = "2475--2485",
    abstract = "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.",
}


@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}


@misc{hong2024evaluatingllmsmathematicalcoding,
      title={Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions}, 
      author={Pengfei Hong and Navonil Majumder and Deepanway Ghosal and Somak Aditya and Rada Mihalcea and Soujanya Poria},
      year={2024},
      eprint={2401.09395},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.09395}, 
}

@misc{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,
      title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models}, 
      author={Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05229}, 
}

@article{embers2024,
author = {McCoy, R and Yao, Shunyu and Friedman, Dan and Hardy, Mathew and Griffiths, Thomas},
year = {2024},
month = {10},
pages = {e2322420121},
title = {Embers of autoregression show how large language models are shaped by the problem they are trained to solve},
volume = {121},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
doi = {10.1073/pnas.2322420121}
}

@misc{prabhakar2024decipheringfactorsinfluencingefficacy,
      title={Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning}, 
      author={Akshara Prabhakar and Thomas L. Griffiths and R. Thomas McCoy},
      year={2024},
      eprint={2407.01687},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01687}, 
}

@misc{wu2024reasoningrecitingexploringcapabilities,
      title={Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks}, 
      author={Zhaofeng Wu and Linlu Qiu and Alexis Ross and Ekin Akyürek and Boyuan Chen and Bailin Wang and Najoung Kim and Jacob Andreas and Yoon Kim},
      year={2024},
      eprint={2307.02477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.02477}, 
}

@inproceedings{faithandfate,
 author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang (Lorraine) and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and Hwang, Jena and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {70293--70332},
 publisher = {Curran Associates, Inc.},
 title = {Faith and Fate: Limits of Transformers on Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/deb3c28192f979302c157cb653c15e90-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{razeghi-etal-2022-impact,
    title = "Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning",
    author = "Razeghi, Yasaman  and
      Logan IV, Robert L  and
      Gardner, Matt  and
      Singh, Sameer",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.59",
    doi = "10.18653/v1/2022.findings-emnlp.59",
    pages = "840--854",
    abstract = "Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70{\%} (absolute) more accurate on the top 10{\%} frequent terms in comparison to the bottom 10{\%}. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.",
}

@misc{lewis2024usingcounterfactualtasksevaluate,
      title={Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models}, 
      author={Martha Lewis and Melanie Mitchell},
      year={2024},
      eprint={2402.08955},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.08955}, 
}

@misc{dasgupta2024languagemodelshumanlikecontent,
      title={Language models show human-like content effects on reasoning tasks}, 
      author={Ishita Dasgupta and Andrew K. Lampinen and Stephanie C. Y. Chan and Hannah R. Sheahan and Antonia Creswell and Dharshan Kumaran and James L. McClelland and Felix Hill},
      year={2024},
      eprint={2207.07051},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.07051}, 
}

@misc{smeaton2024understandingfoundationmodels1924,
      title={Understanding Foundation Models: Are We Back in 1924?}, 
      author={Alan F. Smeaton},
      year={2024},
      eprint={2409.07618},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.07618}, 
}


@misc{yan2024largelanguagemodelsunderstand,
      title={Do Large Language Models Understand Logic or Just Mimick Context?}, 
      author={Junbing Yan and Chengyu Wang and Jun Huang and Wei Zhang},
      year={2024},
      eprint={2402.12091},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.12091}, 
}

@misc{jiang2024peektokenbiaslarge,
      title={A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners}, 
      author={Bowen Jiang and Yangxinyu Xie and Zhuoqun Hao and Xiaomeng Wang and Tanwi Mallick and Weijie J. Su and Camillo J. Taylor and Dan Roth},
      year={2024},
      eprint={2406.11050},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11050}, 
}

@misc{valmeekam2024llmscantplanlrms,
      title={LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench}, 
      author={Karthik Valmeekam and Kaya Stechly and Subbarao Kambhampati},
      year={2024},
      eprint={2409.13373},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.13373}, 
}

@misc{hosseini2024llmreasonerscreatedequal,
      title={Not All LLM Reasoners Are Created Equal}, 
      author={Arian Hosseini and Alessandro Sordoni and Daniel Toyama and Aaron Courville and Rishabh Agarwal},
      year={2024},
      eprint={2410.01748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01748}, 
}

@misc{golchin2024datacontaminationquiztool,
      title={Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models}, 
      author={Shahriar Golchin and Mihai Surdeanu},
      year={2024},
      eprint={2311.06233},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.06233}, 
}

@inproceedings{sanchez-salido-etal-2025-bilingual,
    title = "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination",
    author = "S{\'a}nchez Salido, Eva  and
      Morante, Roser  and
      Gonzalo, Julio  and
      Marco, Guillermo  and
      Carrillo-de-Albornoz, Jorge  and
      Plaza, Laura  and
      Amigo, Enrique  and
      Garc{\'i}a, Andr{\'e}s Fernandez  and
      Benito-Santos, Alejandro  and
      Ghajari Espinosa, Adri{\'a}n  and
      Fresno, Victor",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.413/",
    pages = "6184--6200",
    abstract = "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and manually translated into English, and have not ever been publicly released, ensuring minimal contamination when evaluating Large Language Models with this dataset. A selection of current open-source and proprietary models are evaluated in a uniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset and on an equivalent subset of MMLU questions. Results show that (i) Smaller models not only perform worse than the largest models, but also degrade faster in Spanish than in English. The performance gap between both languages is negligible for the best models, but grows up to 37{\%} for smaller models; (ii) Model ranking on UNED-ACCESS 2024 is almost identical (0.98 Pearson correlation) to the one obtained with MMLU (a similar, but publicly available benchmark), suggesting that contamination affects similarly to all models, and (iii) As in publicly available datasets, reasoning questions in UNED-ACCESS are more challenging for models of all sizes."
}

@misc{taghanaki2024mmluproevaluatinghigherorderreasoning,
      title={MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs}, 
      author={Saeid Asgari Taghanaki and Aliasgahr Khani and Amir Khasahmadi},
      year={2024},
      eprint={2409.02257},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02257}, 
}

@misc{samuel2024datacontaminationdetectionmodern,
      title={Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges}, 
      author={Vinay Samuel and Yue Zhou and Henry Peng Zou},
      year={2024},
      eprint={2409.09927},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.09927}, 
}

@misc{wang2022adversarialgluemultitaskbenchmark,
      title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models}, 
      author={Boxin Wang and Chejian Xu and Shuohang Wang and Zhe Gan and Yu Cheng and Jianfeng Gao and Ahmed Hassan Awadallah and Bo Li},
      year={2022},
      eprint={2111.02840},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.02840}, 
}

@misc{wang2023robustnesschatgptadversarialoutofdistribution,
      title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective}, 
      author={Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Wei Ye and Xiubo Geng and Binxin Jiao and Yue Zhang and Xing Xie},
      year={2023},
      eprint={2302.12095},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.12095}, 
}

@misc{wang2022measureimproverobustnessnlp,
      title={Measure and Improve Robustness in NLP Models: A Survey}, 
      author={Xuezhi Wang and Haohan Wang and Diyi Yang},
      year={2022},
      eprint={2112.08313},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.08313}, 
}

@misc{yang2023gluexevaluatingnaturallanguage,
      title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective}, 
      author={Linyi Yang and Shuibai Zhang and Libo Qin and Yafu Li and Yidong Wang and Hanmeng Liu and Jindong Wang and Xing Xie and Yue Zhang},
      year={2023},
      eprint={2211.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.08073}, 
}

@misc{lewis2024evaluatingrobustnessanalogicalreasoning,
      title={Evaluating the Robustness of Analogical Reasoning in Large Language Models}, 
      author={Martha Lewis and Melanie Mitchell},
      year={2024},
      eprint={2411.14215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.14215}, 
}

@misc{nikankin2024arithmeticalgorithmslanguagemodels,
      title={Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics}, 
      author={Yaniv Nikankin and Anja Reusch and Aaron Mueller and Yonatan Belinkov},
      year={2024},
      eprint={2410.21272},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21272}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeekAI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu et al},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{o3-mini-2025,
	title = {OpenAI o3-mini},
	url = {https://cdn.openai.com/o3-mini-system-card.pdf},
	author = {OpenAI},
	year = {2025},
}

@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma-Team},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{mmmlu,
	title = {Multilingual Massive Multitask Language Understanding (MMMLU)},
	url = {https://huggingface.co/datasets/openai/MMMLU},
	author = {OpenAI},
	year = {2024},
}

@inproceedings{balloccu-etal-2024-leak,
    title = "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source {LLM}s",
    author = "Balloccu, Simone  and
      Schmidtov{\'a}, Patr{\'i}cia  and
      Lango, Mateusz  and
      Dusek, Ondrej",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.5/",
    pages = "67--93",
    abstract = "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI`s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI`s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model`s release. We report that these models have been globally exposed to {\ensuremath{\sim}}4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts."
}


@misc{huang2025mathperturbbenchmarkingllmsmath,
      title={MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations}, 
      author={Kaixuan Huang and Jiacheng Guo and Zihao Li and Xiang Ji and Jiawei Ge and Wenzhe Li and Yingqing Guo and Tianle Cai and Hui Yuan and Runzhe Wang and Yue Wu and Ming Yin and Shange Tang and Yangsibo Huang and Chi Jin and Xinyun Chen and Chiyuan Zhang and Mengdi Wang},
      year={2025},
      eprint={2502.06453},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.06453}, 
}


@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}