[
  {
    "index": 0,
    "papers": [
      {
        "key": "smeaton2024understandingfoundationmodels1924",
        "author": "Alan F. Smeaton",
        "title": "Understanding Foundation Models: Are We Back in 1924?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yang2023gluexevaluatingnaturallanguage",
        "author": "Linyi Yang and Shuibai Zhang and Libo Qin and Yafu Li and Yidong Wang and Hanmeng Liu and Jindong Wang and Xing Xie and Yue Zhang",
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "razeghi-etal-2022-impact",
        "author": "Razeghi, Yasaman  and\nLogan IV, Robert L  and\nGardner, Matt  and\nSingh, Sameer",
        "title": "Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Jiang2024InvestigatingDC",
        "author": "Minhao Jiang and Ken Ziyu Liu and Ming Zhong and Rylan Schaeffer and Siru Ouyang and Jiawei Han and Sanmi Koyejo",
        "title": "Investigating Data Contamination for Pre-training Language Models"
      },
      {
        "key": "dong2024generalizationmemorizationdatacontamination",
        "author": "Yihong Dong and Xue Jiang and Huanyu Liu and Zhi Jin and Bin Gu and Mengfei Yang and Ge Li",
        "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models"
      },
      {
        "key": "golchin2024datacontaminationquiztool",
        "author": "Shahriar Golchin and Mihai Surdeanu",
        "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models"
      },
      {
        "key": "sainz-etal-2023-nlp",
        "author": "Sainz, Oscar  and\nCampos, Jon  and\nGarc{\\'\\i}a-Ferrero, Iker  and\nEtxaniz, Julen  and\nde Lacalle, Oier Lopez  and\nAgirre, Eneko",
        "title": "{NLP} Evaluation in trouble: On the Need to Measure {LLM} Data Contamination for each Benchmark"
      },
      {
        "key": "yang2023rethinkingbenchmarkcontaminationlanguage",
        "author": "Shuo Yang and Wei-Lin Chiang and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoica",
        "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples"
      },
      {
        "key": "samuel2024datacontaminationdetectionmodern",
        "author": "Vinay Samuel and Yue Zhou and Henry Peng Zou",
        "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ahuja_mega_2023",
        "author": "Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Axmed, Maxamed and Bali, Kalika and Sitaram, Sunayana",
        "title": "{MEGA}: {Multilingual} {Evaluation} of {Generative} {AI}"
      },
      {
        "key": "balloccu-etal-2024-leak",
        "author": "Balloccu, Simone  and\nSchmidtov{\\'a}, Patr{\\'i}cia  and\nLango, Mateusz  and\nDusek, Ondrej",
        "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source {LLM}s"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2022adversarialgluemultitaskbenchmark",
        "author": "Boxin Wang and Chejian Xu and Shuohang Wang and Zhe Gan and Yu Cheng and Jianfeng Gao and Ahmed Hassan Awadallah and Bo Li",
        "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chiang_chatbot_2024",
        "author": "Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion",
        "title": "Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hendrycks_measuring_2021",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring {Massive} {Multitask} {Language} {Understanding}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cobbe_training_2021",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John",
        "title": "Training {Verifiers} to {Solve} {Math} {Word} {Problems}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhong_agieval_2023",
        "author": "Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan",
        "title": "{AGIEval}: {A} {Human}-{Centric} {Benchmark} for {Evaluating} {Foundation} {Models}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rein_gpqa_2023",
        "author": "Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.",
        "title": "{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\\&{A} {Benchmark}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "openai2024o1",
        "author": "OpenAI",
        "title": "OpenAI o1 System Card"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "valmeekam2024llmscantplanlrms",
        "author": "Karthik Valmeekam and Kaya Stechly and Subbarao Kambhampati",
        "title": "LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2023robustnesschatgptadversarialoutofdistribution",
        "author": "Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Wei Ye and Xiubo Geng and Binxin Jiao and Yue Zhang and Xing Xie",
        "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"
      },
      {
        "key": "wang2022measureimproverobustnessnlp",
        "author": "Xuezhi Wang and Haohan Wang and Diyi Yang",
        "title": "Measure and Improve Robustness in NLP Models: A Survey"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "embers2024",
        "author": "McCoy, R and Yao, Shunyu and Friedman, Dan and Hardy, Mathew and Griffiths, Thomas",
        "title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "razeghi-etal-2022-impact",
        "author": "Razeghi, Yasaman  and\nLogan IV, Robert L  and\nGardner, Matt  and\nSingh, Sameer",
        "title": "Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "nikankin2024arithmeticalgorithmslanguagemodels",
        "author": "Yaniv Nikankin and Anja Reusch and Aaron Mueller and Yonatan Belinkov",
        "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "jiang2024peektokenbiaslarge",
        "author": "Bowen Jiang and Yangxinyu Xie and Zhuoqun Hao and Xiaomeng Wang and Tanwi Mallick and Weijie J. Su and Camillo J. Taylor and Dan Roth",
        "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "taghanaki2024mmluproevaluatinghigherorderreasoning",
        "author": "Saeid Asgari Taghanaki and Aliasgahr Khani and Amir Khasahmadi",
        "title": "MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "faithandfate",
        "author": "Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang (Lorraine) and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and Hwang, Jena and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin",
        "title": "Faith and Fate: Limits of Transformers on Compositionality"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "prabhakar2024decipheringfactorsinfluencingefficacy",
        "author": "Akshara Prabhakar and Thomas L. Griffiths and R. Thomas McCoy",
        "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "srivastava_functional_2024",
        "author": "Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj",
        "title": "Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "hendrycks2021measuringmathematicalproblemsolving",
        "author": "Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical",
        "author": "Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "cobbe_training_2021",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John",
        "title": "Training {Verifiers} to {Solve} {Math} {Word} {Problems}"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "hong2024evaluatingllmsmathematicalcoding",
        "author": "Pengfei Hong and Navonil Majumder and Deepanway Ghosal and Somak Aditya and Rada Mihalcea and Soujanya Poria",
        "title": "Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "hosseini2024llmreasonerscreatedequal",
        "author": "Arian Hosseini and Alessandro Sordoni and Daniel Toyama and Aaron Courville and Rishabh Agarwal",
        "title": "Not All LLM Reasoners Are Created Equal"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "zhu_promptbench_2023",
        "author": "Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil Zhenqiang and Xie, Xing",
        "title": "{PromptBench}: {Towards} {Evaluating} the {Robustness} of {Large} {Language} {Models} on {Adversarial} {Prompts}"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "wu2024reasoningrecitingexploringcapabilities",
        "author": "Zhaofeng Wu and Linlu Qiu and Alexis Ross and Ekin Aky\u00fcrek and Boyuan Chen and Bailin Wang and Najoung Kim and Jacob Andreas and Yoon Kim",
        "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "lewis2024usingcounterfactualtasksevaluate",
        "author": "Martha Lewis and Melanie Mitchell",
        "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models"
      },
      {
        "key": "lewis2024evaluatingrobustnessanalogicalreasoning",
        "author": "Martha Lewis and Melanie Mitchell",
        "title": "Evaluating the Robustness of Analogical Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "yan2024largelanguagemodelsunderstand",
        "author": "Junbing Yan and Chengyu Wang and Jun Huang and Wei Zhang",
        "title": "Do Large Language Models Understand Logic or Just Mimick Context?"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "nezhurina_alice_2024",
        "author": "Nezhurina, Marianna and Cipolina-Kun, Lucia and Cherti, Mehdi and Jitsev, Jenia",
        "title": "Alice in {Wonderland}: {Simple} {Tasks} {Showing} {Complete} {Reasoning} {Breakdown} in {State}-{Of}-the-{Art} {Large} {Language} {Models}"
      }
    ]
  }
]