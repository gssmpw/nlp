@article{Jiang2024InvestigatingDC,
  title={Investigating Data Contamination for Pre-training Language Models},
  author={Minhao Jiang and Ken Ziyu Liu and Ming Zhong and Rylan Schaeffer and Siru Ouyang and Jiawei Han and Sanmi Koyejo},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.06059},
  url={https://api.semanticscholar.org/CorpusID:266933004}
}

@misc{ahuja_mega_2023,
	title = {{MEGA}: {Multilingual} {Evaluation} of {Generative} {AI}},
	shorttitle = {{MEGA}},
	url = {http://arxiv.org/abs/2303.12528},
	abstract = {Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Axmed, Maxamed and Bali, Kalika and Sitaram, Sunayana},
	month = oct,
	year = {2023},
	note = {arXiv:2303.12528 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\S6V9XW4X\\2303.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\PLARL5N5\\Ahuja et al. - 2023 - MEGA Multilingual Evaluation of Generative AI.pdf:application/pdf},
}

@inproceedings{balloccu-etal-2024-leak,
    title = "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source {LLM}s",
    author = "Balloccu, Simone  and
      Schmidtov{\'a}, Patr{\'i}cia  and
      Lango, Mateusz  and
      Dusek, Ondrej",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.5/",
    pages = "67--93",
    abstract = "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI`s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI`s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model`s release. We report that these models have been globally exposed to {\ensuremath{\sim}}4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts."
}

@misc{chiang_chatbot_2024,
	title = {Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
	shorttitle = {Chatbot {Arena}},
	url = {http://arxiv.org/abs/2403.04132},
	abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at https://chat.lmsys.org.},
	language = {en},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLM.pdf:C\:\\Users\\evasa\\Zotero\\storage\\SE93ITXG\\Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLM.pdf:application/pdf},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\GD6BUWKG\\2110.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\3YI5E86B\\Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf:application/pdf},
}

@misc{dong2024generalizationmemorizationdatacontamination,
      title={Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models}, 
      author={Yihong Dong and Xue Jiang and Huanyu Liu and Zhi Jin and Bin Gu and Mengfei Yang and Ge Li},
      year={2024},
      eprint={2402.15938},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.15938}, 
}

@article{embers2024,
author = {McCoy, R and Yao, Shunyu and Friedman, Dan and Hardy, Mathew and Griffiths, Thomas},
year = {2024},
month = {10},
pages = {e2322420121},
title = {Embers of autoregression show how large language models are shaped by the problem they are trained to solve},
volume = {121},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
doi = {10.1073/pnas.2322420121}
}

@inproceedings{faithandfate,
 author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang (Lorraine) and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and Hwang, Jena and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {70293--70332},
 publisher = {Curran Associates, Inc.},
 title = {Faith and Fate: Limits of Transformers on Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/deb3c28192f979302c157cb653c15e90-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{golchin2024datacontaminationquiztool,
      title={Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models}, 
      author={Shahriar Golchin and Mihai Surdeanu},
      year={2024},
      eprint={2311.06233},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.06233}, 
}

@misc{hendrycks2021measuringmathematicalproblemsolving,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.03874}, 
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2023-09-13},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\HPA22Z23\\2009.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\MXDNQEY3\\Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf},
}

@misc{hong2024evaluatingllmsmathematicalcoding,
      title={Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions}, 
      author={Pengfei Hong and Navonil Majumder and Deepanway Ghosal and Somak Aditya and Rada Mihalcea and Soujanya Poria},
      year={2024},
      eprint={2401.09395},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.09395}, 
}

@misc{hosseini2024llmreasonerscreatedequal,
      title={Not All LLM Reasoners Are Created Equal}, 
      author={Arian Hosseini and Alessandro Sordoni and Daniel Toyama and Aaron Courville and Rishabh Agarwal},
      year={2024},
      eprint={2410.01748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01748}, 
}

@misc{jiang2024peektokenbiaslarge,
      title={A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners}, 
      author={Bowen Jiang and Yangxinyu Xie and Zhuoqun Hao and Xiaomeng Wang and Tanwi Mallick and Weijie J. Su and Camillo J. Taylor and Dan Roth},
      year={2024},
      eprint={2406.11050},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11050}, 
}

@misc{lewis2024evaluatingrobustnessanalogicalreasoning,
      title={Evaluating the Robustness of Analogical Reasoning in Large Language Models}, 
      author={Martha Lewis and Melanie Mitchell},
      year={2024},
      eprint={2411.14215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.14215}, 
}

@misc{lewis2024usingcounterfactualtasksevaluate,
      title={Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models}, 
      author={Martha Lewis and Melanie Mitchell},
      year={2024},
      eprint={2402.08955},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.08955}, 
}

@misc{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,
      title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models}, 
      author={Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05229}, 
}

@misc{nezhurina_alice_2024,
	title = {Alice in {Wonderland}: {Simple} {Tasks} {Showing} {Complete} {Reasoning} {Breakdown} in {State}-{Of}-the-{Art} {Large} {Language} {Models}},
	shorttitle = {Alice in {Wonderland}},
	url = {http://arxiv.org/abs/2406.02061},
	doi = {10.48550/arXiv.2406.02061},
	abstract = {Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Nezhurina, Marianna and Cipolina-Kun, Lucia and Cherti, Mehdi and Jitsev, Jenia},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02061 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\NGJKK64K\\Nezhurina et al. - 2024 - Alice in Wonderland Simple Tasks Showing Complete.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\TXZ7ASRS\\2406.html:text/html},
}

@misc{nikankin2024arithmeticalgorithmslanguagemodels,
      title={Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics}, 
      author={Yaniv Nikankin and Anja Reusch and Aaron Mueller and Yonatan Belinkov},
      year={2024},
      eprint={2410.21272},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21272}, 
}

@misc{openai2024o1,
    title={OpenAI o1 System Card},
    author={OpenAI},
    year={2024},
    url={https://openai.com/index/openai-o1-system-card/},
}

@misc{prabhakar2024decipheringfactorsinfluencingefficacy,
      title={Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning}, 
      author={Akshara Prabhakar and Thomas L. Griffiths and R. Thomas McCoy},
      year={2024},
      eprint={2407.01687},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01687}, 
}

@inproceedings{razeghi-etal-2022-impact,
    title = "Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning",
    author = "Razeghi, Yasaman  and
      Logan IV, Robert L  and
      Gardner, Matt  and
      Singh, Sameer",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.59",
    doi = "10.18653/v1/2022.findings-emnlp.59",
    pages = "840--854",
    abstract = "Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70{\%} (absolute) more accurate on the top 10{\%} frequent terms in comparison to the bottom 10{\%}. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.",
}

@misc{rein_gpqa_2023,
	title = {{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\&{A} {Benchmark}},
	shorttitle = {{GPQA}},
	url = {http://arxiv.org/abs/2311.12022},
	abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12022 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\F46HTT4M\\2311.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\ZZ6L5LUX\\Rein et al. - 2023 - GPQA A Graduate-Level Google-Proof Q&A Benchmark.pdf:application/pdf},
}

@inproceedings{sainz-etal-2023-nlp,
    title = "{NLP} Evaluation in trouble: On the Need to Measure {LLM} Data Contamination for each Benchmark",
    author = "Sainz, Oscar  and
      Campos, Jon  and
      Garc{\'\i}a-Ferrero, Iker  and
      Etxaniz, Julen  and
      de Lacalle, Oier Lopez  and
      Agirre, Eneko",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.722",
    doi = "10.18653/v1/2023.findings-emnlp.722",
    pages = "10776--10787",
    abstract = "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
}

@misc{samuel2024datacontaminationdetectionmodern,
      title={Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges}, 
      author={Vinay Samuel and Yue Zhou and Henry Peng Zou},
      year={2024},
      eprint={2409.09927},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.09927}, 
}

@misc{smeaton2024understandingfoundationmodels1924,
      title={Understanding Foundation Models: Are We Back in 1924?}, 
      author={Alan F. Smeaton},
      year={2024},
      eprint={2409.07618},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.07618}, 
}

@misc{srivastava_functional_2024,
	title = {Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}},
	url = {http://arxiv.org/abs/2402.19450},
	abstract = {We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35\% to 80.31\% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19450 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\ZXNUREJG\\2402.html:text/html;Full Text PDF:C\:\\Users\\evasa\\Zotero\\storage\\XG9X3V6Z\\Srivastava et al. - 2024 - Functional Benchmarks for Robust Evaluation of Rea.pdf:application/pdf},
}

@misc{taghanaki2024mmluproevaluatinghigherorderreasoning,
      title={MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs}, 
      author={Saeid Asgari Taghanaki and Aliasgahr Khani and Amir Khasahmadi},
      year={2024},
      eprint={2409.02257},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02257}, 
}

@misc{valmeekam2024llmscantplanlrms,
      title={LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench}, 
      author={Karthik Valmeekam and Kaya Stechly and Subbarao Kambhampati},
      year={2024},
      eprint={2409.13373},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.13373}, 
}

@misc{wang2022adversarialgluemultitaskbenchmark,
      title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models}, 
      author={Boxin Wang and Chejian Xu and Shuohang Wang and Zhe Gan and Yu Cheng and Jianfeng Gao and Ahmed Hassan Awadallah and Bo Li},
      year={2022},
      eprint={2111.02840},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.02840}, 
}

@misc{wang2022measureimproverobustnessnlp,
      title={Measure and Improve Robustness in NLP Models: A Survey}, 
      author={Xuezhi Wang and Haohan Wang and Diyi Yang},
      year={2022},
      eprint={2112.08313},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.08313}, 
}

@misc{wang2023robustnesschatgptadversarialoutofdistribution,
      title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective}, 
      author={Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Wei Ye and Xiubo Geng and Binxin Jiao and Yue Zhang and Xing Xie},
      year={2023},
      eprint={2302.12095},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.12095}, 
}

@misc{wu2024reasoningrecitingexploringcapabilities,
      title={Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks}, 
      author={Zhaofeng Wu and Linlu Qiu and Alexis Ross and Ekin Akyürek and Boyuan Chen and Bailin Wang and Najoung Kim and Jacob Andreas and Yoon Kim},
      year={2024},
      eprint={2307.02477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.02477}, 
}

@misc{yan2024largelanguagemodelsunderstand,
      title={Do Large Language Models Understand Logic or Just Mimick Context?}, 
      author={Junbing Yan and Chengyu Wang and Jun Huang and Wei Zhang},
      year={2024},
      eprint={2402.12091},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.12091}, 
}

@misc{yang2023gluexevaluatingnaturallanguage,
      title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective}, 
      author={Linyi Yang and Shuibai Zhang and Libo Qin and Yafu Li and Yidong Wang and Hanmeng Liu and Jindong Wang and Xing Xie and Yue Zhang},
      year={2023},
      eprint={2211.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.08073}, 
}

@misc{yang2023rethinkingbenchmarkcontaminationlanguage,
      title={Rethinking Benchmark and Contamination for Language Models with Rephrased Samples}, 
      author={Shuo Yang and Wei-Lin Chiang and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2311.04850},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.04850}, 
}

@misc{zhong_agieval_2023,
	title = {{AGIEval}: {A} {Human}-{Centric} {Benchmark} for {Evaluating} {Foundation} {Models}},
	shorttitle = {{AGIEval}},
	url = {http://arxiv.org/abs/2304.06364},
	doi = {10.48550/arXiv.2304.06364},
	abstract = {Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95\% accuracy rate on the SAT Math test and a 92.5\% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/ruixiangcui/AGIEval.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
	month = sep,
	year = {2023},
	note = {arXiv:2304.06364 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\evasa\\Zotero\\storage\\2DAXDZZZ\\Zhong et al. - 2023 - AGIEval A Human-Centric Benchmark for Evaluating .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\evasa\\Zotero\\storage\\JL2YCEU2\\2304.html:text/html},
}

@misc{zhu_promptbench_2023,
	title = {{PromptBench}: {Towards} {Evaluating} the {Robustness} of {Large} {Language} {Models} on {Adversarial} {Prompts}},
	shorttitle = {{PromptBench}},
	url = {http://arxiv.org/abs/2306.04528},
	abstract = {The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs’ resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4, 788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil Zhenqiang and Xie, Xing},
	month = oct,
	year = {2023},
	note = {arXiv:2306.04528 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Zhu et al. - 2023 - PromptBench Towards Evaluating the Robustness of .pdf:C\:\\Users\\evasa\\Zotero\\storage\\FZPF3FQT\\Zhu et al. - 2023 - PromptBench Towards Evaluating the Robustness of .pdf:application/pdf},
}

