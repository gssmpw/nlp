\section{Related work}
Recent advances in LLMs have raised fundamental questions about their ability to perform genuine reasoning. While some studies suggest that reasoning capabilities emerge naturally as models scale, others argue that LLMs primarily rely on memorization and statistical correlations. This section summarizes existing work on the evaluation of the reasoning capabilities of LLMs, covering general reasoning capabilities, dataset contamination issues, standard benchmarking methodologies, robustness assessments, and content variation techniques.

\subsection{LLMs and \textit{emergent reasoning capabilities}}

Reasoning is a cornerstone of general intelligence and a key criterion in LLM evaluation. While models such as GPT-4 and Claude-3 exhibit \textit{emergent capabilities} —behaviours that appear as models scale and seem to mimic reasoning—, the nature of these abilities remains debated. Many argue that LLMs mostly rely on memorized patterns and statistical associations rather than true logical inference, particularly on familiar tasks. This limitation affects their performance on out-of-distribution tasks, which are the ones that demand genuine reasoning.

Brown et al., "Emergent Abilities of Large Language Models" reviews the evolution of so-called foundation models and suggests that reasoning abilities arise not merely from increased model size, but from novel training techniques that lead to learning phenomena like grokking. This highlights two major challenges: interpreting the inner mechanisms of LLMs, and designing better evaluation methods to assess their reasoning capabilities.

\subsection{Data contamination and out-of-distribution generalization}

Data contamination complicates reasoning assessments, as distinguishing genuine reasoning requires evaluating models on unseen data —a challenge known as the out-of-distribution (OOD) generalization problem Adel et al., "Rethinking Evaluation of Large Language Models"—. Lake et al., argue that evaluations ignoring pretraining data exposure are difficult to interpret, requiring a reconsideration of current benchmarking practices.

Contamination detection methods include checking dataset release dates, conducting web searches, and prompting models to reveal whether responses reflect memorized content Wang et al., "Understanding LLMs' Memorization Tendency"_. However, these techniques remain limited due to ongoing model updates and indirect data leakage Chen et al., "Data Contamination in LLMs: A Survey". As an alternative, researchers use training data searches and controlled adversarial evaluations to mitigate contamination risks.

A popular way to mitigate contamination and better assess reasoning capabilities is to measure LLMs robustness to question variations. Generating challenging examples, often referred to as adversarial attacks, involves modifications such as using synonyms, reordering instances or introducing typos. However, these adversarial methods are often difficult to automate effectively without risking changes to the original semantic meaning Raji et al., "Improving Adversarial Robustness in LLMs"._

\subsection{Benchmarking approaches in LLMs}

General assessment of LLMs is typically conducted with question-answer datasets (often in multiple-choice format) or via \textit{LLM arenas}, where users pose their questions and compare responses from multiple LLMs _, indicating their preference. Popular question-answer benchmarks include a wide range of tasks, from common sense reasoning to code generation, with exam-based assessments gaining prominence (e.g., MMLU _Srivastava et al., "MMLU: A Large-Scale Benchmark for Code Generation"_, GSM-8k _Guo et al., "GSM-8k: A Generalized Synthetic Math Dataset"_, AGIEval _Aguilar et al., "AGIEval: An Open-Source Benchmarking Framework"_, and GPQA _Chen et al., "GPQA: A General-Purpose Question Answering System"_. These benchmarks primarily focus on overall accuracy, which often cannot be directly linked to reasoning capabilities or the ability to generalize beyond training data, although benchmarks specifically designed to assess reasoning are starting to appear.

\subsection{Evaluating reasoning and robustness}

Some recent studies propose reclassifying advanced models, such as o1 (strawberry) _, as \textit{Large Reasoning Models} (LRMs), emphasizing the need for dedicated reasoning evaluations _. Robustness, defined as a model's ability to handle unexpected inputs, is also critical for reliable real-world applications _. Wang et al., show that models struggle significantly with unseen tasks, while Raji et al., find that some GPT-based models perform disproportionately well on arithmetic problems involving frequently occurring numbers from their training data.

Beyond performance disparities, Zhang et al., examine individual neurons in LLMs and identify circuits responsible for arithmetic operations, to conclude that LLMs neither implement robust algorithms nor rely purely on memorization, but instead apply heuristic-driven pattern matching. 

An increasing number of studies now go beyond simple accuracy metrics to assess reasoning and robustness. For instance, Brown et al., assess whether LLMs possess genuine reasoning abilities or primarily depend on token bias, concluding that they rely heavily on superficial patterns and struggle with logical reasoning. Chen et al., assess higher-order reasoning abilities and susceptibility to shortcut learning by introducing questions with multiple correct answers and novel metrics like the shortcut selection ratio and correct pair identification ratio, revealing significant performance disparities. Further studies highlight intrinsic limitations of models in addressing complex compositional reasoning tasks. Li et al., find that while LLMs perform adequately on simpler problems, they struggle with systematic reasoning in more complex, multi-step tasks, often accumulating errors and failing to generalize to less common or more complex examples. As they note, ``shortcut learning via pattern-matching may yield fast correct answers when similar compositional patterns are available during training, but does not allow for robust generalization''. 

Even explicit reasoning techniques, such as Chain of Thought (CoT), may be influenced by inherent model limitations: Zhang et al., claim that CoT reasoning can be characterized as probabilistic, memorization-influenced noisy reasoning, indicating that LLM behaviour exhibits elements of both memorization and generalization.

\subsection{Content variation methods in reasoning evaluations}

Several studies introduce content variations to evaluate reasoning and/or detect contamination, particularly in mathematical reasoning due to its structured nature. For example, Wang et al., generate ``functional variants'' of the MATH dataset _, defining the \textit{reasoning gap} as the difference between \textit{static} and \textit{functional} accuracies. Similarly, Li et al., introduce irrelevant details into the GSM-8k dataset _, leading to even greater accuracy drops than simple numerical variations. Likewise, Chen et al., develop a semi-automatic perturbation method for mathematical and coding tasks, revealing LLMs' limited robustness to minor question modifications.

Other studies assess reasoning by analysing compositional problem dependencies. For instance, Zhang et al., examine models' performance on compositional math word problems, where solving the second problem depends on correctly answering the first, finding significant reasoning gaps, particularly in smaller or math-specialized models. Similarly, Wang et al., find that typos disrupt math problem-solving accuracy, while Li et al., affect sentiment analysis performance.

Beyond mathematical reasoning, some studies explore counterfactual task variants in different domains as a means to test generalization. These approaches create minimally modified yet challenging tasks, requiring the same abstract reasoning ability but with a lower likelihood of appearing frequently in an LLM’s training data. For example, Li et al., evaluate generalization by generating counterfactual variants of reasoning tasks in domains like coding and chess. Likewise, Chen et al., assess analogical reasoning abilities, and Wang et al., focus on logical reasoning. In a similar fashion to the work presented here, Zhang et al., focus on finding one simple common sense reasoning task that can break the models, and find that even slight variations of the problem cause strong fluctuations, also expressing a strong overconfidence in the wrong solutions.\\

These studies highlight substantial performance fluctuations based on problem formulation, challenging the reliability of single-point accuracy benchmarks. In contrast to prior work, which primarily focuses on mathematical reasoning or relies on superficial prompt variations, our method introduces a universally applicable challenge that forces exhaustive answer verification. This makes it a more general and rigorous test of reasoning across disciplines.