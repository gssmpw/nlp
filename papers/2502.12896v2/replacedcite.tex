\section{Related work}
Recent advances in LLMs have raised fundamental questions about their ability to perform genuine reasoning. While some studies suggest that reasoning capabilities emerge naturally as models scale, others argue that LLMs primarily rely on memorization and statistical correlations. This section summarizes existing work on the evaluation of the reasoning capabilities of LLMs, covering general reasoning capabilities, dataset contamination issues, standard benchmarking methodologies, robustness assessments, and content variation techniques.

\subsection{LLMs and \textit{emergent reasoning capabilities}}

Reasoning is a cornerstone of general intelligence and a key criterion in LLM evaluation. While models such as GPT-4 and Claude-3 exhibit \textit{emergent capabilities} —behaviours that appear as models scale and seem to mimic reasoning—, the nature of these abilities remains debated. Many argue that LLMs mostly rely on memorized patterns and statistical associations rather than true logical inference, particularly on familiar tasks. This limitation affects their performance on out-of-distribution tasks, which are the ones that demand genuine reasoning.

____ reviews the evolution of so-called foundation models and suggests that reasoning abilities arise not merely from increased model size, but from novel training techniques that lead to learning phenomena like grokking. This highlights two major challenges: interpreting the inner mechanisms of LLMs, and designing better evaluation methods to assess their reasoning capabilities.

\subsection{Data contamination and out-of-distribution generalization}

Data contamination complicates reasoning assessments, as distinguishing genuine reasoning requires evaluating models on unseen data —a challenge known as the out-of-distribution (OOD) generalization problem ____—. ____ argue that evaluations ignoring pretraining data exposure are difficult to interpret, requiring a reconsideration of current benchmarking practices.

Contamination detection methods include checking dataset release dates, conducting web searches, and prompting models to reveal whether responses reflect memorized content ____. However, these techniques remain limited due to ongoing model updates and indirect data leakage ____. As an alternative, researchers use training data searches and controlled adversarial evaluations to mitigate contamination risks.

A popular way to mitigate contamination and better assess reasoning capabilities is to measure LLMs robustness to question variations. Generating challenging examples, often referred to as adversarial attacks, involves modifications such as using synonyms, reordering instances or introducing typos. However, these adversarial methods are often difficult to automate effectively without risking changes to the original semantic meaning ____.

\subsection{Benchmarking approaches in LLMs}

General assessment of LLMs is typically conducted with question-answer datasets (often in multiple-choice format) or via \textit{LLM arenas}, where users pose their questions and compare responses from multiple LLMs ____, indicating their preference. Popular question-answer benchmarks include a wide range of tasks, from common sense reasoning to code generation, with exam-based assessments gaining prominence (e.g., MMLU ____, GSM-8k ____, AGIEval ____, and GPQA ____). These benchmarks primarily focus on overall accuracy, which often cannot be directly linked to reasoning capabilities or the ability to generalize beyond training data, although benchmarks specifically designed to assess reasoning are starting to appear.

\subsection{Evaluating reasoning and robustness}

Some recent studies propose reclassifying advanced models, such as o1 (strawberry) ____, as \textit{Large Reasoning Models} (LRMs), emphasizing the need for dedicated reasoning evaluations ____. Robustness, defined as a model's ability to handle unexpected inputs, is also critical for reliable real-world applications ____. ____ show that models struggle significantly with unseen tasks, while ____ find that some GPT-based models perform disproportionately well on arithmetic problems involving frequently occurring numbers from their training data.

Beyond performance disparities, ____ examine individual neurons in LLMs and identify circuits responsible for arithmetic operations, to conclude that LLMs neither implement robust algorithms nor rely purely on memorization, but instead apply heuristic-driven pattern matching. 
% These studies reinforce the role of memorization and highlight the importance of evaluating the extent to which it impacts true reasoning.

An increasing number of studies now go beyond simple accuracy metrics to assess reasoning and robustness. For instance, ____ assess whether LLMs possess genuine reasoning abilities or primarily depend on token bias, concluding that they rely heavily on superficial patterns and struggle with logical reasoning. ____ assess higher-order reasoning abilities and susceptibility to shortcut learning by introducing questions with multiple correct answers and novel metrics like the shortcut selection ratio and correct pair identification ratio, revealing significant performance disparities. Further studies highlight intrinsic limitations of models in addressing complex compositional reasoning tasks. ____ find that while LLMs perform adequately on simpler problems, they struggle with systematic reasoning in more complex, multi-step tasks, often accumulating errors and failing to generalize to less common or more complex examples. As they note, ``shortcut learning via pattern-matching may yield fast correct answers when similar compositional patterns are available during training, but does not allow for robust generalization''. 

Even explicit reasoning techniques, such as Chain of Thought (CoT), may be influenced by inherent model limitations: ____ claim that CoT reasoning can be characterized as probabilistic, memorization-influenced noisy reasoning, indicating that LLM behaviour exhibits elements of both memorization and generalization.

\subsection{Content variation methods in reasoning evaluations}

Several studies introduce content variations to evaluate reasoning and/or detect contamination, particularly in mathematical reasoning due to its structured nature. For example, ____ generate ``functional variants'' of the MATH dataset ____, defining the \textit{reasoning gap} as the difference between \textit{static} and \textit{functional} accuracies. Similarly, ____ introduce irrelevant details into the GSM-8k dataset ____, leading to even greater accuracy drops than simple numerical variations. Likewise, ____ develop a semi-automatic perturbation method for mathematical and coding tasks, revealing LLMs' limited robustness to minor question modifications.

Other studies assess reasoning by analysing compositional problem dependencies. For instance, ____ examine models' performance on compositional math word problems, where solving the second problem depends on correctly answering the first, finding significant reasoning gaps, particularly in smaller or math-specialized models. Similarly, ____ find that typos disrupt math problem-solving accuracy, while synonym changes affect sentiment analysis performance.

Beyond mathematical reasoning, some studies explore counterfactual task variants in different domains as a means to test generalization. These approaches create minimally modified yet challenging tasks, requiring the same abstract reasoning ability but with a lower likelihood of appearing frequently in an LLM’s training data. For example, ____ evaluate generalization by generating counterfactual variants of reasoning tasks in domains like coding and chess. Likewise, ____ assess analogical reasoning abilities, and ____ focus on logical reasoning. In a similar fashion to the work presented here, ____ focus on finding one simple common sense reasoning task that can break the models, and find that even slight variations of the problem cause strong fluctuations, also expressing a strong overconfidence in the wrong solutions.\\

These studies highlight substantial performance fluctuations based on problem formulation, challenging the reliability of single-point accuracy benchmarks. In contrast to prior work, which primarily focuses on mathematical reasoning or relies on superficial prompt variations, our method introduces a universally applicable challenge that forces exhaustive answer verification. This makes it a more general and rigorous test of reasoning across disciplines.