% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
%\usepackage{yfonts}
%\usepackage{amsfonts, amssymb}
%\usepackage{bbm}
%\usepackage{dutchcal}
%\usepackage{calligra}
\usepackage{shadowtext} 


\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{rotating}
\usepackage{fancyvrb}
\usepackage{xparse}
\usepackage{pifont}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\noto}[0]{\textsc{noto}\,}


\title{
    {\it None of the Others}: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks
    
    %None of the Others: a General Technique to Distinguish Recall from Understanding in Multiple-Choice LLM Evaluation Benchmarks

    %{\it None of the Others}: Reasoning versus Memorization in Multiple-Choice LLM Evaluation Benchmarks 
    
    
    %None of the Others: Quantifying Robustness in Multiple-Choice LLM Evaluations 
    }







% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco \\
%\normalsize{UNED Research Center in Natural Language Processing and Information Retrieval\thanks{\href{https://sites.google.com/view/nlp-uned/home}{nlp.uned.es}}\\}
UNED Research Center in Natural Language Processing and Information Retrieval\thanks{\href{https://sites.google.com/view/nlp-uned/home}{nlp.uned.es}}\\
ETSI Informática, UNED - Juan del Rosal, 16 28040 Madrid, Spain\\
%\small{\texttt{julio@lsi.uned.es}}\\
\small{\textbf{Correspondence:} \href{mailto:julio@lsi.uned.es}{julio@lsi.uned.es}}
}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}

\maketitle


\begin{abstract}
% Recent work on evaluating LLM reasoning has focused primarily on mathematics, overlooking broader domains. 
% We introduce a general-purpose evaluation method for multiple-choice questions based on an exclusion paradigm: replacing the correct answer with \textit{None of the other answers}. This forces models to engage in exhaustive validation rather than relying on surface-level heuristics.
In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly.  
Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57\% on MMLU and 50\% on UNED-Access 2024, ranging from 10\% to 93\% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers. % Our findings confirm persistent gaps in current LLM reasoning and the increasing need for evaluation methodologies and benchmarks which prevent contamination effects and minimize the success of recall-based answers. 

%Unlike recent approaches that focus exclusively on mathematics questions, we introduce a substitution-based evaluation for multiple-choice questions, applicable across any discipline. We assess several state-of-the-art proprietary and open-source LLMs on two multiple-choice datasets available in both English and Spanish: the public MMLU dataset and the private UNED-Access 2024 2024 dataset. Each model is tested in two scenarios: the original formulation of the question, and a modified version where the correct answer is replaced with {\it None of the other answers}. Robustness is measured as the ability to maintain accuracy across these two settings, with smaller drops indicating greater robustness. Results show that all models experience significant performance declines when confronted with these exhaustive validation questions. The average accuracy drop is 56.85\% on (English) MMLU and 49.78\% on (Spanish) UNED-Access 2024 2024, with individual drops ranging from 10\% to 92.5\% across all four datasets. Notably, the model with the lowest accuracy drop (DeepSeek-R1-70B) is not the one with the highest overall accuracy (OpenAI-o3-mini), indicating that high performance does not equate to consistent reasoning abilities. Regarding contamination, the fact that accuracy drops more in public datasets suggests models benefit from prior exposure to these questions during pretraining, while larger declines observed when evaluating questions in their original language reinforce the hypothesis that models rely more on memorized text retrieval than genuine subject-matter understanding. These findings highlight the lack of logical consistency in current LLMs and their limitations in handling unseen questions or inferring correct answers beyond memorized data, as is likely the case with MMLU.



\end{abstract}

\section{Introduction}

Large Language Models (LLMs) currently display remarkable performance across diverse natural language tasks, and also perform competitively with respect to humans in general knowledge benchmarks. However, a fundamental question remains: to what extent these models truly understand and reason, versus merely recalling patterns from previously seen data? This is particularly relevant in benchmarks using multiple-choice questions, which is one of the most popular methods to evaluate LLMs. While models like OpenAI’s \citep{openai2024gpt4o, openai2024o1} claim to achieve state-of-the-art performance in \textit{reasoning-heavy tasks} (such as GPQA diamond \citep{rein_gpqa_2023}), doubts persist that their success may still rely more on memorization than on the flexible reasoning that characterizes general intelligence; and reliable reasoning is crucial for many applications that require logical inference.
%, such as medical diagnosis, legal analysis, or mathematical theorem proving, where simple pattern matching is insufficient.

To examine the robustness of reasoning in LLMs, recent studies employ multi-prompt evaluations, introducing minor modifications to questions or altering mathematical problem variables. These approaches challenge models with structurally similar but unseen tasks, but are often confined to narrow domains like mathematics (see \citep{srivastava_functional_2024, mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, huang2025mathperturbbenchmarkingllmsmath}) or depend on manually curated variations, making them costly and less scalable \citep{wang_adversarial_2022}. \\
% In contrast, our method is fully automatic and can be applied to any existing multiple-choice dataset with minimal human intervention, enabling large-scale reasoning assessments across diverse subjects.

% In this article, we propose a novel testing approach that introduces a versatile content variation applicable across disciplines: replacing the correct answer with an exclusion option such as "none of the others." This modification forces models to engage in exhaustive verification, a hallmark of genuine reasoning. Unlike standard multiple-choice settings, where a model may select an answer based on surface-level correlations, this approach requires step-by-step validation, increasing cognitive burden and reducing the likelihood of correct guesses based on memorized cues. Furthermore, this transformation is entirely automatic, enabling large-scale evaluations without requiring human-designed questions. By incorporating this exclusion option, we aim to distinguish genuine reasoning from mere pattern recognition or memorization in LLMs.


%In this context, we define \textbf{robustness} as a model’s ability to maintain high accuracy despite the introduction of exclusion-based answer formats, which require explicit verification of all options. By comparing model performance under both conditions, we investigate whether current LLMs engage in actual reasoning or rely on heuristic-based pattern recognition or, in other words, to what extent LLMs struggle when forced to engage in exhaustive validation rather than selecting an answer based on likelihood.

Our main goal is to investigate to what extent LLMs respond to general multiple-choice questions by searching in the (compressed) space of previously seen content, or by truly acquiring knowledge from texts and understanding the questions being posed. This leads us to work on three related research questions: \textbf{RQ1 [Reasoning vs. Memorization]}: How do models react when the questions are reformulated in a way that requires understanding and reasoning rather than recall/memorization?; \textbf{RQ2 [Contamination and translation biases]}: To what extent does prior exposure (dataset contamination) affect models' ability to reason rather than retrieve memorized answers? Additionally, how does translation impact robustness, given that translated questions are less likely to appear verbatim in training data?; and \textbf{RQ3 [Robustness predictors]}: Is the performance drop explainable just in terms of the size of the model and its (reference) effectiveness? Or there is more than scaling laws when it comes to reasoning abilities?

The main contributions of our research are: (i) we propose a simple, fully automatic method to rewrite multiple-choice questions from any domain, which ensures that they cannot be answered correctly without a genuine understanding of the subject, because the correct answer cannot be retrieved from the space of previously seen texts; (ii) we show that the performance of all models drops significantly (the average loss is above 50\%), but the drop differs widely across models; (iii) we provide additional evidence that models at least partially rely on search mechanisms for their answers, because their performance drops less with private, contamination-free datasets and with translated versions of the questions, for which recall-based answers are harder to obtain; (iv) we show that the models that perform best on the original questions are not necessarily the ones that show a smaller performance drop, indicating that reasoning capabilities and standard accuracy are not perfectly correlated. For instance, Claude-3.5-Sonnet is one of the top performers with original questions but drops up to 53\% with the reformulated questions, while DeepSeek-R1-70B is worse on the original questions but much better than Claude on the rewritten questions, which suggests that its reasoning capabilities are much higher, and (v) contrary to scaling laws that relate model size and performance, the model with the smallest drop in our experimentation is a medium-sized one (DeepSeek-R1-70B), and, in general, the models with lower drop are the most modern, particularly those integrating advanced reasoning-specific optimizations, such as o3-mini, GPT-4o and DeepSeek.

% We present results from state-of-the-art LLMs on two datasets and in two languages: the private UNED-Access 2024 dataset, which contains UNED-Access 2024ersity-entry level exam questions in Spanish and English, and the widely used public MMLU dataset, now available in both languages. Each model is tested under two conditions: one with the original answer options and another with the modified "none of the others" option. Since this transformation is entirely automatic, it enables large-scale evaluations across diverse datasets without requiring manual question design. This comparison allows us to assess model robustness and accuracy in a setting that demands thorough logical verification.

\section{Related work}

Recent advances in LLMs have raised fundamental questions about their ability to perform genuine reasoning. While some studies suggest that reasoning capabilities emerge naturally as models scale, others argue that LLMs primarily rely on memorization and statistical correlations. This section summarizes existing work on the evaluation of the reasoning capabilities of LLMs, covering general reasoning capabilities, dataset contamination issues, standard benchmarking methodologies, robustness assessments, and content variation techniques.

\subsection{LLMs and \textit{emergent reasoning capabilities}}

Reasoning is a cornerstone of general intelligence and a key criterion in LLM evaluation. While models such as GPT-4 and Claude-3 exhibit \textit{emergent capabilities} —behaviours that appear as models scale and seem to mimic reasoning—, the nature of these abilities remains debated. Many argue that LLMs mostly rely on memorized patterns and statistical associations rather than true logical inference, particularly on familiar tasks. This limitation affects their performance on out-of-distribution tasks, which are the ones that demand genuine reasoning.

\citet{smeaton2024understandingfoundationmodels1924} reviews the evolution of so-called foundation models and suggests that reasoning abilities arise not merely from increased model size, but from novel training techniques that lead to learning phenomena like grokking. This highlights two major challenges: interpreting the inner mechanisms of LLMs, and designing better evaluation methods to assess their reasoning capabilities.

\subsection{Data contamination and out-of-distribution generalization}

Data contamination complicates reasoning assessments, as distinguishing genuine reasoning requires evaluating models on unseen data —a challenge known as the out-of-distribution (OOD) generalization problem \citep{yang2023gluexevaluatingnaturallanguage}—. \citet{razeghi-etal-2022-impact} argue that evaluations ignoring pretraining data exposure are difficult to interpret, requiring a reconsideration of current benchmarking practices.

Contamination detection methods include checking dataset release dates, conducting web searches, and prompting models to reveal whether responses reflect memorized content \citep{Jiang2024InvestigatingDC, dong2024generalizationmemorizationdatacontamination, golchin2024datacontaminationquiztool, sainz-etal-2023-nlp, yang2023rethinkingbenchmarkcontaminationlanguage, samuel2024datacontaminationdetectionmodern}. However, these techniques remain limited due to ongoing model updates and indirect data leakage \citep{ahuja_mega_2023, balloccu-etal-2024-leak}. As an alternative, researchers use training data searches and controlled adversarial evaluations to mitigate contamination risks.

A popular way to mitigate contamination and better assess reasoning capabilities is to measure LLMs robustness to question variations. Generating challenging examples, often referred to as adversarial attacks, involves modifications such as using synonyms, reordering instances or introducing typos. However, these adversarial methods are often difficult to automate effectively without risking changes to the original semantic meaning \citep{wang2022adversarialgluemultitaskbenchmark}.

\subsection{Benchmarking approaches in LLMs}

General assessment of LLMs is typically conducted with question-answer datasets (often in multiple-choice format) or via \textit{LLM arenas}, where users pose their questions and compare responses from multiple LLMs \citep{chiang_chatbot_2024}, indicating their preference. Popular question-answer benchmarks include a wide range of tasks, from common sense reasoning to code generation, with exam-based assessments gaining prominence (e.g., MMLU \citep{hendrycks_measuring_2021}, GSM-8k \citep{cobbe_training_2021}, AGIEval \citep{zhong_agieval_2023}, and GPQA \citep{rein_gpqa_2023}). These benchmarks primarily focus on overall accuracy, which often cannot be directly linked to reasoning capabilities or the ability to generalize beyond training data, although benchmarks specifically designed to assess reasoning are starting to appear.

\subsection{Evaluating reasoning and robustness}

Some recent studies propose reclassifying advanced models, such as o1 (strawberry) \citep{openai2024o1}, as \textit{Large Reasoning Models} (LRMs), emphasizing the need for dedicated reasoning evaluations \citep{valmeekam2024llmscantplanlrms}. Robustness, defined as a model's ability to handle unexpected inputs, is also critical for reliable real-world applications \citep{wang2023robustnesschatgptadversarialoutofdistribution, wang2022measureimproverobustnessnlp}. \citet{embers2024} show that models struggle significantly with unseen tasks, while \citet{razeghi-etal-2022-impact} find that some GPT-based models perform disproportionately well on arithmetic problems involving frequently occurring numbers from their training data.

Beyond performance disparities, \citep{nikankin2024arithmeticalgorithmslanguagemodels} examine individual neurons in LLMs and identify circuits responsible for arithmetic operations, to conclude that LLMs neither implement robust algorithms nor rely purely on memorization, but instead apply heuristic-driven pattern matching. 
% These studies reinforce the role of memorization and highlight the importance of evaluating the extent to which it impacts true reasoning.

An increasing number of studies now go beyond simple accuracy metrics to assess reasoning and robustness. For instance, \citep{jiang2024peektokenbiaslarge} assess whether LLMs possess genuine reasoning abilities or primarily depend on token bias, concluding that they rely heavily on superficial patterns and struggle with logical reasoning. \citep{taghanaki2024mmluproevaluatinghigherorderreasoning} assess higher-order reasoning abilities and susceptibility to shortcut learning by introducing questions with multiple correct answers and novel metrics like the shortcut selection ratio and correct pair identification ratio, revealing significant performance disparities. Further studies highlight intrinsic limitations of models in addressing complex compositional reasoning tasks. \citet{faithandfate} find that while LLMs perform adequately on simpler problems, they struggle with systematic reasoning in more complex, multi-step tasks, often accumulating errors and failing to generalize to less common or more complex examples. As they note, ``shortcut learning via pattern-matching may yield fast correct answers when similar compositional patterns are available during training, but does not allow for robust generalization''. 

Even explicit reasoning techniques, such as Chain of Thought (CoT), may be influenced by inherent model limitations: \citet{prabhakar2024decipheringfactorsinfluencingefficacy} claim that CoT reasoning can be characterized as probabilistic, memorization-influenced noisy reasoning, indicating that LLM behaviour exhibits elements of both memorization and generalization.

\subsection{Content variation methods in reasoning evaluations}

Several studies introduce content variations to evaluate reasoning and/or detect contamination, particularly in mathematical reasoning due to its structured nature. For example, \citet{srivastava_functional_2024} generate ``functional variants'' of the MATH dataset \citep{hendrycks2021measuringmathematicalproblemsolving}, defining the \textit{reasoning gap} as the difference between \textit{static} and \textit{functional} accuracies. Similarly, \citet{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical} introduce irrelevant details into the GSM-8k dataset \citet{cobbe_training_2021}, leading to even greater accuracy drops than simple numerical variations. Likewise, \citet{hong2024evaluatingllmsmathematicalcoding} develop a semi-automatic perturbation method for mathematical and coding tasks, revealing LLMs' limited robustness to minor question modifications.

Other studies assess reasoning by analysing compositional problem dependencies. For instance, \citet{hosseini2024llmreasonerscreatedequal} examine models' performance on compositional math word problems, where solving the second problem depends on correctly answering the first, finding significant reasoning gaps, particularly in smaller or math-specialized models. Similarly, \citet{zhu_promptbench_2023} find that typos disrupt math problem-solving accuracy, while synonym changes affect sentiment analysis performance.

Beyond mathematical reasoning, some studies explore counterfactual task variants in different domains as a means to test generalization. These approaches create minimally modified yet challenging tasks, requiring the same abstract reasoning ability but with a lower likelihood of appearing frequently in an LLM’s training data. For example, \citet{wu2024reasoningrecitingexploringcapabilities} evaluate generalization by generating counterfactual variants of reasoning tasks in domains like coding and chess. Likewise, \citet{lewis2024usingcounterfactualtasksevaluate, lewis2024evaluatingrobustnessanalogicalreasoning} assess analogical reasoning abilities, and \citet{yan2024largelanguagemodelsunderstand} focus on logical reasoning. In a similar fashion to the work presented here, \citet{nezhurina_alice_2024} focus on finding one simple common sense reasoning task that can break the models, and find that even slight variations of the problem cause strong fluctuations, also expressing a strong overconfidence in the wrong solutions.\\

These studies highlight substantial performance fluctuations based on problem formulation, challenging the reliability of single-point accuracy benchmarks. In contrast to prior work, which primarily focuses on mathematical reasoning or relies on superficial prompt variations, our method introduces a universally applicable challenge that forces exhaustive answer verification. This makes it a more general and rigorous test of reasoning across disciplines.



\section{None of the others (\noto) variation}

The variation we propose on multiple-choice questions (hereafter abbreviated as \noto) is simply to replace the correct answer with "None of the other answers". This, in turn, becomes the right answer, because all other answers are incorrect by design. With this replacement, the correct answer is no longer connected terminologically or conceptually with the question, and therefore pure memorization is not enough to guess the answer. The system needs to discard all other options to conclude that "none of the others" is the correct choice. In addition, "none of the others" is frequently used in multiple-choice questions, but it tends not to be the correct answer. Therefore, a pure association memory would advise the systems not to choose "none of the others", which may lead to average results even below random choice. 

\section{Experimental setup}
\begin{figure*}[ht!]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=\linewidth]{noto_mmlu.png}
        %\caption{Performance on MMLU (original questions and \textit{none of the others} variation). Results per model and language are averaged across all subjects and expressed in terms of Cohen's Kappa.}
        %\label{noto_mmlu} 
    \end{minipage}
    \hspace{0.01\textwidth}%\hfill
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=\linewidth]{noto_uned.png}
        %\caption{Performance on UNED-Access 2024 (original questions and \textit{none of the others} variation). Results per model and language are averaged across all subjects and expressed in terms of Cohen's Kappa.} 
        %\label{noto_uned} 
    \end{minipage}
    \caption{Performance on MMLU and UNED-Access 2024 (original questions and \textit{none of the others} variation). Results per model and language are averaged across all subjects and expressed in terms of \textbf{Cohen's Kappa}.}
    \label{kappa}
\end{figure*}

This section provides an overview of our experimental setup, with additional details in Appendix \ref{appendix-exps} for full reproducibility. We describe the datasets, models and hyperparameters, prompting strategy, and evaluation metrics for assessing performance and robustness.


\subsection{Datasets}

We have experimented with two bilingual datasets. One is the \textbf{MMLU} dataset \citep{hendrycks_measuring_2021}, with questions in English covering 57 tasks ranging from high school to professional and graduate levels, and its professional manual translation into Spanish \citep{mmmlu}. After filtering out questions potentially incompatible with the ``none of the others'' substitution, this dataset comprises 13,346 questions.

The second is \textbf{UNED-Access 2024}, with 1,003 questions in Spanish on 11 university-entry-level subjects, and professional manual translations into English. Unlike MMLU, this dataset has never been made public and therefore the effects of contamination should be minimal. After preprocessing, 950 compatible questions remained for the \noto scenario.

%The second is \textbf{UNED-Access 2024} \citep{sanchez-salido-etal-2025-bilingual}, with 1,003 questions in Spanish on 11 UNED-Access 2024ersity-entry-level subjects, and professional manual translations into English. Unlike MMLU, this dataset has never been made public and therefore the effects of contamination should be minimal. After preprocessing, 950 compatible questions remained for the \noto scenario.

\subsection{Models and hyperparameters}

Experiments were conducted using 15 generative models, including five proprietary models and ten open-source models, all trained for instruction following. Proprietary models were accessed via API, while the open-source models were obtained from Hugging Face or deployed via the Ollama\footnote{\url{https://ollama.com/}} library.

The temperature was set to 0 for all models to ensure deterministic outputs (except for o3-mini, which did not allow temperature adjustment) to minimize creativity and focus on factual or reasoning-based answers. Each question was provided one at a time using a fixed prompt structure, which included a system prompt, user prompt, and assistant prompt. The prompt specified the subject of the question and was always in the same language as the dataset (English or Spanish) as is done in other evaluations \citep{zhang_m3exam_2023, openai2024gpt4technicalreport}.  

\subsection{Prompting strategy}

LLM evaluations often mix prompting strategies, ranging from zero-shot to few-shot (with varying numbers of examples) and Chain-of-Thought configurations. We adopt a uniform zero-shot setting, as it closely resembles real-world interactions with LLMs while ensuring a simpler and more replicable experimental setup, and may even lead to better results for the most recent models \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}.

\subsection{Evaluation metrics}

Most LLM evaluations rely on \textbf{Accuracy}, calculated as the proportion of correct answers ($C$) over the total responses ($N$). However, Accuracy alone does not account for variations in the number of answer choices or chance-level performance. Since different subjects in our datasets have varying numbers of answer choices, we complement Accuracy with \textbf{Cohen’s Kappa coefficient}, which accounts for chance-level performance and enables fairer comparisons across subjects:

\vspace{-0.25cm}
\begin{equation*}
\resizebox{\columnwidth}{!}{$
\text{Kappa} = \frac{\text{observed accuracy} - \text{expected accuracy}}{1 - \text{expected accuracy}}
= \frac{\frac{\text{C}}{\text{N}} - \frac{1}{\text{M}}}{1 - \frac{1}{\text{M}}}
$}
\end{equation*}


where $M$ is the number of possible choices and the \textit{expected accuracy} corresponds to random guessing: 1/3 for three-choice questions and 1/4 for four-choice questions. Cohen’s Kappa normalizes correctness so that random answers yield a Kappa of zero, enabling fair comparisons across subjects. Kappa values range from -1/2 to 1, and negative values indicate performance worse than random chance. The final result for each model and language is the arithmetic mean of Kappa values across all subjects, giving equal weight to each subject to account for dataset imbalances.

To measure performance variation between original and modified questions, which we refer to as \textit{drop}, we report the percentage decrement of Accuracy\footnote{Cohen's Kappa coefficient is not in a ratio scale (the origin is not zero) and therefore percentages cannot be computed directly.}  
%, as it provides a relative measure of change with respect to the initial value, facilitating interpretation in terms of proportional accuracy loss. 

%Since the values of the \textit{noto} configuration are consistently lower than those of the original one, this metric is particularly appropriate. Specifically:

%\vspace{-0.25cm}
%\begin{equation*}
%\text{drop \%} = \frac{\text{base accuracy} -\textit{noto}\text{ accuracy}}{\text{base accuracy}  } \cdot 100
%\end{equation*}

%This metric is applied exclusively to accuracy values and not to Cohen's Kappa, as Kappa can take negative or near-zero values, making the formula unsuitable for use in that range.

%the absolute difference in Kappa scores, rather than a percentage-based drop.

\section{Results}

%This section addresses the Research Questions (RQs) introduced earlier, presenting experimental findings structured around reasoning, contamination and translation effects, and robustness predictors. We summarize insights from both MMLU and UNED-Access 2024 datasets.

Each question from MMLU and UNED-Access 2024 is evaluated under four conditions: original formulation in English and Spanish, and a modified version where the correct answer is replaced with ``None of the other answers'' in both languages. This setup enables direct comparison between standard multiple-choice performance and the reasoning challenge introduced by the exclusion option.

Figure \ref{kappa} provides a visual representation of performance in terms of Cohen's Kappa (detailed numerical results are available in Appendix \ref{app-results}), while Table \ref{tabla_accuracy_drops} presents accuracy scores and performance drop between the original questions and their \noto variations. 
%Using both metrics allows us to measure absolute performance and quantify relative degradation across conditions.

\begin{table*}[ht!]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccc|ccc|ccc|ccc}
        \toprule
        & \multicolumn{3}{c|}{MMLU (English)} & \multicolumn{3}{c|}{MMLU (Spanish)} & \multicolumn{3}{c|}{UNED (English)} & \multicolumn{3}{c}{UNED (Spanish)} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
         & base & \noto & drop \% & base & \noto & drop \% & base & \noto & drop \% & base & \noto & drop \% \\
        \midrule
        DeepSeek-R1-70B & 0.72 & 0.60 & \underline{16.67} & 0.72 & 0.44 & 38.89 & 0.60 & 0.54 & \underline{10.0} & 0.77 & 0.65 & 15.58 \\
        OpenAI-o3-mini & \textbf{0.89} & \textbf{0.65} & 26.97 & \textbf{0.86} & \textbf{0.6} & \underline{30.23} & \textbf{0.92} & \textbf{0.77} & 16.30 & \textbf{0.93} & \textbf{0.79} & \underline{15.05} \\
        Llama-2-7B-Chat & 0.40 & 0.29 & 27.50 & 0.38 & 0.21 & 44.74 & 0.52 & 0.26 & 50.00 & 0.48 & 0.15 & 68.75 \\
        GPT-4o & 0.84 & 0.58 & 30.95 & 0.82 & 0.56 & 31.71 & 0.85 & 0.69 & 18.82 & 0.84 & 0.69 & 17.86 \\
        GPT-4-Turbo & 0.81 & 0.51 & 37.04 & 0.78 & 0.53 & 32.05 & 0.84 & 0.58 & 30.95 & 0.85 & 0.65 & 23.53 \\
        Claude-3.5-Sonnet & 0.86 & 0.40 & 53.49 & 0.85 & 0.43 & 49.41 & 0.87 & 0.53 & 39.08 & 0.89 & 0.62 & 30.34 \\
        Gemma-2-27B-Instruct & 0.75 & 0.32 & 57.33 & 0.72 & 0.28 & 61.11 & 0.75 & 0.40 & 46.67 & 0.76 & 0.41 & 46.05 \\
        Mixtral-8x7B-Instruct & 0.68 & 0.29 & 57.35 & 0.64 & 0.22 & 65.63 & 0.69 & 0.37 & 46.38 & 0.70 & 0.33 & 52.86 \\
        Llama-3-8B-Instruct & 0.64 & 0.26 & 59.38 & 0.56 & 0.23 & 58.93 & 0.66 & 0.37 & 43.94 & 0.65 & 0.33 & 49.23 \\
        Llama-3-70B-Instruct & 0.78 & 0.30 & 61.54 & 0.73 & 0.25 & 65.75 & 0.75 & 0.41 & 45.33 & 0.77 & 0.41 & 46.75 \\
        Mistral-7B-Instruct & 0.59 & 0.18 & 69.49 & 0.51 & 0.20 & 60.78 & 0.62 & 0.33 & 46.77 & 0.60 & 0.34 & 43.33 \\
        Salamandra-7B-Instruct & 0.38 & 0.11 & 71.05 & 0.36 & 0.14 & 61.11 & 0.47 & 0.11 & 76.60 & 0.46 & 0.16 & 65.22 \\
        Mixtral-8x22B-Instruct & 0.73 & 0.16 & 78.08 & 0.67 & 0.16 & 76.12 & 0.73 & 0.19 & 73.97 & 0.74 & 0.25 & 66.22 \\
        Gemma-7B-Instruct & 0.49 & 0.07 & 85.71 & 0.44 & 0.08 & 81.82 & 0.59 & 0.09 & 84.75 & 0.56 & 0.09 & 83.93 \\
        GPT-3.5-Turbo & 0.66 & 0.09 & 86.36 & 0.60 & 0.08 & 86.67 & 0.72 & 0.14 & 80.56 & 0.68 & 0.10 & 85.29 \\
        Leniachat-Gemma-2B & 0.32 & 0.03 & 90.63 & 0.30 & 0.03 & 90.00 & 0.40 & 0.03 & 92.50 & 0.37 & 0.05 & 86.49 \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Accuracy} results on the original and \textit{none of the others} configurations, and percentage decrease between scenarios. Systems are sorted by drop in English MMLU, smaller to largest.}
    \label{tabla_accuracy_drops}
\end{table*}

\subsection{RQ1: Performance vs. Reasoning}

% kappa
To evaluate the impact of the exclusion option on model performance, we analyse both effectiveness (Cohen's Kappa and Accuracy) and robustness across datasets in English and Spanish. 

\textbf{Performance with \noto:} Figure \ref{kappa} depicts results in terms of Cohen's Kappa (where random guessing always gets zero regardless of the number of choices)\footnote{Note that the slightly lower MMLU results compared to previous studies are primarily due to our use of Cohen's Kappa. Additional differences may stem from our zero-shot setup (versus few-shot in other works), prompt formulation, or the quantization of Ollama models.}. All models exhibit a substantial drop in performance under the \noto variation. In multiple cases, models are even worse than random answers, which suggests that they rely almost purely on memorization, and they probably learnt in the pre-training phase that "None of the others" is statistically less likely than any other option. With the \noto variation, only o3-mini (the top-performing model overall) exceeds the 0.5 passing threshold in MMLU, for one language (English). Note that, with the use of appropriate question variations, the MMLU dataset is far from being saturated. For UNED-Access 2024, two models pass in English (o3-mini and GPT-4o) and four in Spanish (o3-mini, GPT-4o, GPT-4-Turbo, and DeepSeek). 

%The best-performing model, both in the original setting and in the \textit{noto} scenario, is o3-mini in both datasets and languages. It is also the only model that manages to pass in the difficult setting in MMLU; whereas in UNED-Access 2024 GPT-4o, GPT-4-Turbo and DeepSeek join it. Leniachat-Gemma-2B is the model that consistently performs worse in all scenarios, which is not surprising as it is the smallest in size.

 
% accuracy

\textbf{Performance drop:} The accuracy drop (Table \ref{tabla_accuracy_drops}) varies drastically across models (from 10\% to 92.5\%) in all four datasets, highlighting substantial differences in robustness. %in MMLU the drops range from 16.67\% to 90.63\% in English and from 30.23\% to 90.0\% in Spanish; and in UNED-Access 2024 from 10.0\% to 92.5\% in English and from 15.05\% to 86.49\% in Spanish. 
Some mid-sized models such as Mixtral-8x22B and GPT-3.5-Turbo suffer particularly steep drops comparable to much smaller models, and scores well below random chance in the \noto setting. The same applies to somewhat more modern models, such as Llama-3-70B and Gemma-2-27B, which fall drastically to near random-chance performance. Among the top performing models, Claude-3.5-Sonnet experiences the most remarkable drop: despite achieving strong performance in the original setting, its \noto accuracy falls well below that of its peers (o3-mini, DeepSeek-R1-70B, GPT-4-Turbo, and GPT-4o).

DeepSeek's R1 case is particularly surprising: although the 70B model ranks well below the top performers on the original dataset, it exhibits the smallest accuracy drop in both English datasets, and also the lowest drop overall (only 10\% in UNED-Access 2024 in English and 16.67\% in English MMLU). This suggests that while DeepSeek-R1-70b is smaller and with less memory, it has stronger reasoning abilities. 

Overall, these results reveal significant differences in how models handle scenarios that demand refined reasoning. DeepSeek-R1-70B and OpenAI-o3-mini has the smallest relative drops, which indicates a stronger, albeit imperfect, ability to validate answer options rather than rely solely on memorization. In contrast, Claude-3.5-Sonnet, despite being a high-performing model in standard conditions, suffers one of the largest drops (53.49\% in English MMLU). The most affected models, such as GPT-3.5-Turbo, experience extreme accuracy degradation (over 85\% drop), which points to an almost exclusive reliance on approximate matching heuristics.

%Although o3-mini achieves the highest accuracy scores, it competes with Deepseek-R1-70B for the lowest-drop podium: DeepSeek wins in both English datasets, whereas o3-mini wins in the Spanish ones. 

%The steepest drops are again observed in Leniachat-Gemma-2B model, in all datasets and scenarios.


%- cuales aprueban mas en base y en noto
%- el drop difiere entre modelos

\subsection{RQ2: Contamination and translation biases}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{drop_month.png}
    \caption{\centering Mean drop across release dates and model sizes.}
    \label{drop_month}
\end{figure*}

To investigate whether the accuracy drop is due to reasoning limitations or reliance on memorized patterns, we compare results from two angles: (1) the effect of dataset contamination, contrasting the public MMLU dataset with the private UNED-Access 2024 dataset, and (2) the effect of translations, contrasting models' performance in the original language versus manually translated versions. These aspects are closely related, as they both influence the extent to which models rely on prior exposure rather than true reasoning. If memorization plays a dominant role, we expect larger drops in public datasets and in original-language versions, as these are more likely to have been seen during pretraining, and for them approximate search may be more effective.

\textbf{Contamination effects:} The mean drop is higher in MMLU (56.85\%) than in UNED-Access 2024 (49.78\%), consistent with the expectation that MMLU, as a public dataset, is more likely to have been seen during pretraining and leads models to fail more when they are prevented from using that memorisation. In fact, the lowest absolute drop (10\% from DeepSeek) is observed on the least likely contaminated dataset, the English UNED-Access 2024 (which is both private and translated from the original questions).

\textbf{Translation effects:} Within MMLU, the average drop is slightly greater in Spanish (58.43\%) than in English (56.85\%), whereas in UNED-Access 2024, the pattern reverses (50.16\% in English vs. 49.78\% in Spanish). With the original questions, models perform better in each dataset’s original language: all models (15/15) achieve higher accuracy in English for MMLU, while in UNED-Access 2024, 8 models perform better in Spanish. This trend still holds in the \noto scenario: in MMLU, 8 models perform better in English, and in UNED-Access 2024, 9 models now perform better in Spanish. When considering passing thresholds, more models pass in English for MMLU (11 vs. 9), and in Spanish for UNED-Access 2024 (11 vs. 10). This pattern holds in \noto: 1 vs. 0 in MMLU and 4 vs. 2 in UNED-Access 2024. 

%The fact that DeepSeek exhibits the lowest drop in both English datasets, while o3-mini is the most robust in Spanish, may indicate that o3-mini, being a bigger model, benefits from better optimization for Spanish during pretraining. 

These are signs of contamination, since, in other words, (i) models fall more in the public dataset, which is likely more contaminated and (ii) models fall more in the original versions than in the translated versions, with which models are probably less familiar (the Spanish MMLU is newer and less likely to be contaminated, and even if UNED-Access 2024 is private, it is less likely that models have seen the English questions since they are manual translations and have never been released). 

Overall, these findings confirm that the \noto substitution exposes reliance on memorized content, and lead us to the conclusion that models experiencing the highest drops are those most likely answering with their memorization skills, rather than with true reasoning. Results with the \noto configuration are a better indication of models' true capabilities, show that with a little twikering the datasets are far from being saturated, and reveal comparative differences between the reasoning capabilities of models that are hidden in the evaluation with the original questions. In particular, we have seen that the performance difference between the most recent \textit{reasoning} models (Deepkseek, o3-mini) and other state-of-the-art ones such as Claude-3.5 is much wider than can be measured with the original questions. 


% acc
%This leads us to the conclusion that \textbf{the higher the drop, the greater the memorisation effects}. 


% - en mmlu: 8/15 modelos caen mas en español
 
%Hipótesis: han memorizado más en inglés, así que se les ven más las costuras. Su intuición les funciona mejor en inglés: al no valerles la intuición, se nota más la diferencia.  El más robusto es 

%- en UNED-Access 2024: 9 de 15 modelos caen más en ingles 


%diferencia entre el gap en mmlu-base-en/es y el gap en mmlu-noto-en/es (el gap disminuye en noto¿)

\subsection{RQ3: Robustness predictors}



\begin{table}[ht!]
\centering
\footnotesize 
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{|l|c|c|}
\hline
\textbf{} & \textbf{correlation} & \textbf{$p$-value} \\ \hline
MMLU (English) & -0.47 & 0.0667 \\ \hline
MMLU (Spanish) & -0.59 & 0.0165 \\ \hline
UNED (English) & -0.60 & 0.0130 \\ \hline
UNED (Spanish) & -0.78 & 0.0003 \\ \hline
\end{tabular}%
}
\caption{Pearson's correlation between accuracy results on the base configuration and the drop.}
\label{tab:pearson_correlation}
\end{table}

Here we try to answer the question: are there model features that can predict greater robustness? Table \ref{tab:pearson_correlation} shows Pearson's correlation between results on the base configuration and the drop. 
The table indicates that, while there is a substantial inverse correlation between effectiveness on the original questions and performance drop with the \noto variation, it is not entirely reliable as a predictor; and in one of the datasets (English MMLU) is not even statistically significant. As we saw before, what we think are the best models using the original questions may not be the ones reasoning better. 
%We can distinguish three clear cases: (i) The Spanish UNED-Access 2024 is the only dataset in which the inverse correlation is quite strong and statistically significant, which suggests that more accurate models tend to have smaller drops; (ii) The English MMLU is the dataset with the weakest correlation, that is not even necessarily statistically significant (since the $p$-value is greater than 0.05), so we cannot stablish a clear relationship between accuracy and robustness, and (iii) On both translated datasets, the correlation is only moderate. This supports the idea that in datasets with more contamination, models with high accuracy may be benefiting from memorisation effects, so they suffer more when they are prevented from using them. So, in general, \textbf{high performance does not necessarily imply greater robustness}. In fact some models with high accuracy show equally high drops, such as the case of Claude-3.5-Sonnet.\\

Figure \ref{drop_month} shows the the mean drop in performance across models, sorted by release date and classified into three groups according to their size: small (less than 10B parameters), medium (10-100B) and large (over 100B). Note that the drop does not correlate well with model size, as there are large models with large drops (GPT-3.5-Turbo, Mixtral-8x22B and Claude-3.5-Sonnet), and the smallest average drop is for a medium-sized model (DeepSeek-R1-70B): size alone is insufficient to ensure robust reasoning. There is a noticeable trend, though, where newer models tend to exhibit smaller drops, with some exceptions. The oldest model, GPT-3.5-Turbo, is a mid performer with the original datasets, but stands out as one of the worst in terms of performance drop. In the period since ChatGPT's debut, the generalisation capabilities of models seems to have improved widely and consistently, and this improvement does not necessarily come with increased model sizes. Finally, the newest proprietary models and DeepSeek-R1 are the ones that show smaller performance drops; this suggests that robustness in reasoning is influenced more by advanced model architectures and training strategies rather than sheer model size.

\section{Conclusions}
Our results show that the proposed \noto variation poses a major challenge for LLMs, and provides a useful signal to distinguish answers based on recall/memorization from genuine knowledge and reasoning. While many models perform well when retrieving memorized information, their performance plummets when the correct answer is disconnected from memory associations and they are required to verify and reject each candidate answer. The \noto variation consistently reveals reasoning gaps, exposing limitations that remain hidden in standard multiple-choice settings (RQ1). Dataset contamination further complicates the evaluation of reasoning: while prior exposure may artificially inflate accuracy in base scenarios, its impact diminishes in \noto, where models cannot rely on memorized answers. Similarly, models perform better on original (and likely more contaminated) datasets, while translated versions mitigate this effect, reinforcing the role of memorization in standard benchmarks (RQ2).

Unlike accuracy, which scaling laws correlate with model size \citep{kaplan2020scalinglawsneurallanguage}, we have seen that robustness is not strictly correlated with model size. High-performing models such as Claude-3.5 suffer severe drops, and some mid-sized models (e.g., GPT-3.5-Turbo, Mixtral) degrade to below-random performance. The most robust model in our experimentation, DeepSeek-R1-70B, is mid-sized, suggesting that architectural advancements and training strategies, rather than sheer scale, play a greater role in reasoning robustness (RQ3). Remarkably, the two so-called \textit{reasoning models} in our sample (o3-mini and DeepSeek-R1) are indeed the ones that better resist the \noto variation. 

In short, our experimentation is a direct confirmation that LLMs remain far from true reasoning, but also that progress is being made towards that goal. Our findings emphasize the need for models that can reliably handle question reformulations without relying on surface-level heuristics, and show that classic datasets that appear to be saturated, such as MMLU, may still be useful for LLM evaluation under appropriate transformations. 

%Future work should explore targeted training strategies to improve generalization and robustness in reasoning-heavy tasks.




\section*{Limitations}
Our evaluation relies on multiple-choice questions, which cannot give a comprehensive evaluation of reasoning abilities. Our results are meant to be complementary with other research directions in evaluation of LLMs. Additionally, our findings are based on MMLU and UNED-Access 2024; while they give an interesting combination of private \& public data, and bilingualism in two directions, different knowledge domains or difficulty patterns may lead to different conclusions. Expanding evaluation to other benchmarks, including those focused on LLM performance in real-world tasks, will provide a broader perspective of LLMs' performance.

% While the significant drop in the \noto setting suggests over reliance on memorization, this does not necessarily imply a complete lack of reasoning. A more granular analysis of intermediate reasoning steps could clarify whether models apply heuristic-based logic rather than relying purely on recall.

Although humans also use pattern-matching for reasoning \citep{dasgupta2024languagemodelshumanlikecontent}, their cognitive processes differ fundamentally from those of LLMs. Future work should compare human performance on exclusion-based evaluations to better understand these differences.

Finally, our experiments follow a zero-shot setting to ensure consistency and replicability, but alternative prompting techniques—such as Chain-of-Thought or few-shot reasoning—could may yield different outcomes. Further studies could explore whether structured reasoning prompts help mitigate the observed performance drop.

%One could argue that humans also rely on memorization and pattern-matching when performing reasoning tasks, as has been shown in studies such as the one by \citep{dasgupta2024languagemodelshumanlikecontent}, where they check that LLMs perform more accurately when the semantic content of a task supports correct logical inferences, suggesting that LLMs' reasoning is affected by content in ways akin to human reasoning...
%While this study focuses on LLMs, future work could explore how human participants perform under similar exclusion-based evaluations to establish a comparative baseline.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only


\section*{Acknowledgments}

%[removed for anonymity]
This work has been funded by the European Union - NextGenerationEU through the `Recovery, Transformation and Resilience Plan', by the Ministry of Economic Affairs and Digital Transformation and by UNED-Access 2024. However, the views and opinions expressed are solely those of the author(s) and do not necessarily reflect those of the European Union or the European Commission. Neither the European Union nor the European Commission can be held responsible for them.



\bibliography{references}

\appendix

\input{appendix}

\end{document}
