\section{Experimental setup}
\label{appendix-exps}

\subsection{Datasets}

The replacement of the correct answer with ``None of the other answers'' requires the question to be compatible, meaning that the answer choices cannot already include options such as ``None of the Above'', ``All of the above'', or similar formulations that would interfere with the intended substitution. To ensure this, we ran a script using regular expressions to automatically detect and filter out questions with incompatible answer choices.

For MMLU, this filtering resulted in 13,470 compatible questions in English and 13,449 in Spanish. To maintain consistency across versions, we used the 13,346 questions that appeared in both languages for our experiments.

In \textit{UNED-Access 2024}, the same methodology was applied with one exception: since almost all psychology questions included ``None of the other answers'' as the fourth option, we opted to modify rather than discard them. Specifically, we removed this answer option unless it was the correct answer, in which case we discarded the question. This resulted in 950 compatible questions for the ``None of the others'' variation, while the original 1,003 questions remained unchanged for the base setting.


\subsection{Models and hyperparameters}

These were the models used for the experimentation: 

\begin{itemize}[label={},leftmargin=0cm,itemsep=0.2cm, topsep=0.1cm, parsep=0.1cm] 
    \item \textbf{Proprietary models:}
    \begin{itemize}[itemsep=0.1cm,parsep=0cm,topsep=0cm] 
        \item[\raisebox{-0.2\height}{\includegraphics[height=0.8em]{logos/openai.png}}] via \textbf{OpenAI} API: o3-mini \citep{o3-mini-2025}, GPT-4-Turbo \citep{openai2024gpt4technicalreport}, GPT-4o \citep{openai2024gpt4o}, GPT-3.5-Turbo \citep{brown2020languagemodelsfewshotlearners}.
        \item[\raisebox{-0.2\height}{\includegraphics[height=0.8em]{logos/anthropic.png}}] via \textbf{Anthropic} API: Claude-3.5-Sonnet \citep{claude_2024}.
    \end{itemize}

    \item \textbf{Open-source models:}
    \begin{itemize}[itemsep=0.1cm,parsep=0cm,topsep=0cm] 
        \item[\raisebox{-0.2\height}{\includegraphics[height=0.8em]{logos/huggingface.png}}] \textbf{Hugging Face}: Llama-2-7B \citep{touvron_llama_2023}, Llama-3-8B \citep{llama_3_2024}, Gemma-7B \citep{gemmateam2024gemmaopenmodelsbased}, Gemma-2-27B, Mistral-7B \citep{jiang_mistral_2023}, Leniachat-Gemma-2B\footnote{\url{https://huggingface.co/LenguajeNaturalAI/leniachat-gemma-2b-v0}}, Salamandra-7B\footnote{\url{https://huggingface.co/LenguajeNaturalAI/leniachat-gemma-2b-v0}}.
        \item[\raisebox{-0.2\height}{\includegraphics[height=0.8em]{logos/ollama.png}}] \textbf{Ollama}: DeepSeek-R1-70B \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, Llama-3-70B \citep{llama_3_2024}, Mixtral-8x7B and Mixtral-8x22B \citep{jiang2024mixtralexperts}, Gemma-2-27B \citep{gemmateam2024gemma2improvingopen}.
    \end{itemize}
\end{itemize}




\subsection*{Prompting strategy}
The exact prompts used were:


{\small
\renewcommand{\labelitemi}{{\textcolor[HTML]{9999ff}{\scriptsize\ding{117}}}}
\begin{itemize}
\setlength\itemsep{0.01em}

\item {\bf System prompt} \newline {\bf ES:} \texttt{Eres un sistema experto en responder preguntas de ex√°menes.} \newline {\bf EN:} \texttt{You are an expert system for answering exam questions.}

\item {\bf User prompt} \newline {\bf ES:} \texttt{Responde a la siguiente pregunta de la asignatura \{\}, tan solo con la letra de la respuesta correcta. Pregunta: \{\}} \newline {\bf EN:} \texttt{Answer the following question of the subject \{\} only with the letter of the correct answer. Question: \{\}}

\item {\bf Assistant prompt} \newline {\bf ES:} \texttt{Letra de la respuesta correcta:} \newline {\bf EN:} \texttt{Letter of the correct answer:}
\end{itemize}
}



For open models, the instructions were formatted according to their respective training specifications, as detailed in their model cards. Finally, responses were post-processed to extract the predicted answer, removing any additional justifications or extraneous text before evaluation.







\section{Results}
\label{app-results}

Table \ref{tablauned} presents Cohen's Kappa results for \textit{UNED-Access 2024} by subject, while Tables \ref{tablammlu1} to \ref{tablammlu4} display the MMLU results, each table corresponding to a different subject category.

\begin{table*}[ht]
\centering
\resizebox{\linewidth}{!}{%
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lcccccccccccc}
%\toprule

 & \textbf{\small\begin{sideways}BAM\end{sideways}} & \textbf{\small\begin{sideways}Biology\end{sideways}} & \textbf{\small\begin{sideways}Biochemistry\end{sideways}} & \textbf{\small\begin{sideways}Economics\end{sideways}} & \textbf{\small\begin{sideways}F. of Computing\end{sideways}} & \textbf{\scriptsize\begin{sideways}Spanish Language\end{sideways}} & \textbf{\small\begin{sideways}Literature\end{sideways}} & \textbf{\small\begin{sideways}Mathematics\end{sideways}} & \textbf{\scriptsize\begin{sideways}Math Applied to SS\end{sideways}} & \textbf{\small\begin{sideways}Advanced Math  
\end{sideways}} & \textbf{\small\begin{sideways}Psychology\end{sideways}} & \textbf{Average}\\
\toprule
ENGLISH-original  &&&&&&&&&&&&\\
\midrule
o3-mini & 0.79 & 0.96 & 1.00 & 0.90 & 0.94 & 0.76 & 0.65 & 0.98 & 0.94 & 1.00 & 0.85 & 0.89 \\
Claude-3.5-Sonnet & 0.83 & 0.95 & 1.00 & 0.95 & 0.94 & 0.83 & 0.84 & 0.55 & 0.66 & 0.56 & 0.88 & 0.82 \\
GPT-4o & 0.79 & 0.96 & 1.00 & 0.92 & 0.96 & 0.70 & 0.82 & 0.55 & 0.52 & 0.50 & 0.86 & 0.78 \\
GPT-4-Turbo & 0.78 & 0.97 & 1.00 & 0.92 & 0.94 & 0.67 & 0.74 & 0.55 & 0.51 & 0.50 & 0.83 & 0.76 \\
Llama-3-70B-Instruct & 0.74 & 0.90 & 1.00 & 0.82 & 0.92 & 0.33 & 0.60 & 0.34 & 0.44 & 0.19 & 0.82 & 0.65 \\
Gemma-2-27B-Instruct & 0.72 & 0.94 & 1.00 & 0.79 & 0.87 & 0.50 & 0.55 & 0.28 & 0.33 & 0.25 & 0.81 & 0.64 \\
Mixtral-8x22B-Instruct & 0.66 & 0.90 & 0.95 & 0.79 & 0.81 & 0.40 & 0.59 & 0.30 & 0.28 & 0.25 & 0.81 & 0.61 \\
GPT-3.5-Turbo & 0.67 & 0.84 & 0.95 & 0.61 & 0.89 & 0.36 & 0.56 & 0.28 & 0.17 & 0.50 & 0.73 & 0.60 \\
Mixtral-8x7B-Instruct & 0.71 & 0.81 & 0.92 & 0.61 & 0.87 & 0.32 & 0.52 & 0.22 & 0.33 & 0.13 & 0.73 & 0.56 \\
Llama-3-8B-Instruct & 0.52 & 0.77 & 0.90 & 0.61 & 0.79 & 0.38 & 0.43 & 0.20 & 0.28 & 0.13 & 0.67 & 0.51 \\
Mistral-7B-Instruct & 0.57 & 0.71 & 0.82 & 0.63 & 0.77 & 0.23 & 0.36 & 0.05 & 0.23 & -0.00 & 0.65 & 0.46 \\
Deepseek-R1-70B & 0.64 & 0.75 & 0.62 & 0.74 & 0.58 & 0.30 & 0.49 & 0.03 & -0.10 & -0.06 & 0.66 & 0.42 \\
Gemma-7B-Instruct & 0.41 & 0.67 & 0.85 & 0.56 & 0.75 & 0.18 & 0.22 & 0.20 & 0.14 & -0.06 & 0.61 & 0.41 \\
Llama-2-7B-Chat & 0.43 & 0.62 & 0.39 & 0.27 & 0.62 & 0.15 & 0.30 & 0.12 & 0.15 & -0.00 & 0.48 & 0.32 \\
Salamandra-7B-Instruct & 0.43 & 0.32 & 0.49 & 0.19 & 0.68 & 0.09 & 0.30 & 0.08 & -0.04 & -0.06 & 0.22 & 0.25 \\
Leniachat-Gemma-2B & 0.29 & 0.32 & 0.24 & 0.03 & 0.13 & 0.05 & 0.22 & 0.08 & 0.07 & -0.06 & 0.27 & 0.15 \\
\midrule
ENGLISH-noto &&&&&&&&&&&&\\
\midrule
o3-mini & 0.54 & 0.90 & 0.89 & 0.75 & 0.81 & 0.43 & 0.05 & 0.98 & 0.89 & 1.00 & 0.26 & 0.68 \\
GPT-4o & 0.53 & 0.95 & 0.89 & 0.68 & 0.77 & 0.43 & 0.12 & 0.40 & 0.31 & 0.44 & 0.68 & 0.56 \\
GPT-4-Turbo & 0.37 & 0.79 & 0.84 & 0.51 & 0.77 & 0.28 & 0.00 & 0.32 & 0.20 & -0.06 & 0.40 & 0.40 \\
Deepseek-R1-70B & 0.53 & 0.69 & 0.43 & 0.61 & 0.30 & 0.49 & 0.36 & 0.22 & 0.03 & 0.31 & -0.11 & 0.35 \\
Claude-3.5-Sonnet & 0.11 & 0.75 & 0.86 & 0.58 & 0.66 & 0.38 & 0.11 & 0.10 & 0.04 & -0.25 & 0.32 & 0.33 \\
Llama-3-70B-Instruct & 0.07 & 0.40 & 0.62 & 0.40 & 0.37 & -0.04 & -0.23 & 0.03 & 0.04 & -0.12 & 0.18 & 0.16 \\
Gemma-2-27B-Instruct & 0.13 & 0.50 & 0.67 & 0.40 & 0.53 & -0.01 & -0.04 & -0.07 & -0.23 & -0.38 & 0.03 & 0.14 \\
Llama-3-8B-Instruct & -0.04 & 0.31 & 0.26 & -0.02 & 0.28 & 0.08 & -0.26 & 0.24 & -0.15 & 0.25 & 0.19 & 0.10 \\
Mixtral-8x7B-Instruct & 0.01 & 0.28 & 0.43 & 0.23 & 0.26 & 0.06 & -0.23 & 0.05 & -0.16 & -0.00 & 0.19 & 0.10 \\
Mistral-7B-Instruct & 0.13 & 0.06 & 0.18 & 0.19 & 0.28 & -0.01 & -0.17 & -0.15 & -0.12 & -0.06 & 0.12 & 0.04 \\
Llama-2-7B-Chat & -0.03 & 0.01 & -0.15 & -0.19 & -0.02 & -0.04 & 0.02 & -0.15 & 0.01 & -0.06 & 0.09 & -0.05 \\
Mixtral-8x22B-Instruct & -0.23 & -0.21 & 0.05 & 0.05 & -0.02 & -0.04 & -0.25 & -0.32 & -0.32 & -0.31 & -0.14 & -0.16 \\
GPT-3.5-Turbo & -0.35 & -0.27 & -0.12 & -0.16 & -0.10 & -0.08 & -0.30 & -0.27 & -0.37 & -0.31 & -0.16 & -0.23 \\
Salamandra-7B-Instruct & -0.39 & -0.37 & -0.17 & -0.12 & -0.08 & -0.29 & -0.16 & -0.27 & -0.40 & -0.50 & -0.23 & -0.27 \\
Gemma-7B-Instruct & -0.29 & -0.34 & -0.09 & -0.30 & -0.10 & -0.21 & -0.30 & -0.46 & -0.42 & -0.50 & -0.26 & -0.30 \\
Leniachat-Gemma-2B & -0.39 & -0.45 & -0.45 & -0.30 & -0.29 & -0.30 & -0.33 & -0.44 & -0.48 & -0.50 & -0.32 & -0.39 \\
\midrule
SPANISH-original  &&&&&&&&&&&&\\
\midrule
o3-mini & 0.90 & 0.96 & 1.00 & 0.92 & 0.94 & 0.82 & 0.56 & 1.00 & 0.97 & 1.00 & 0.82 & 0.90 \\
Claude-3.5-Sonnet & 0.93 & 0.96 & 1.00 & 0.92 & 1.00 & 0.89 & 0.87 & 0.53 & 0.70 & 0.56 & 0.95 & 0.85 \\
GPT-4-Turbo & 0.78 & 0.96 & 1.00 & 0.95 & 0.96 & 0.69 & 0.72 & 0.55 & 0.57 & 0.50 & 0.88 & 0.78 \\
GPT-4o & 0.84 & 0.97 & 1.00 & 0.90 & 0.96 & 0.74 & 0.81 & 0.51 & 0.38 & 0.50 & 0.91 & 0.77 \\
Deepseek-R1-70B & 0.74 & 0.87 & 1.00 & 0.82 & 0.89 & 0.52 & 0.66 & 0.45 & 0.33 & 0.31 & 0.81 & 0.67 \\
Llama-3-70B-Instruct & 0.83 & 0.89 & 0.95 & 0.79 & 0.94 & 0.39 & 0.66 & 0.38 & 0.46 & 0.25 & 0.82 & 0.67 \\
Gemma-2-27B-Instruct & 0.76 & 0.92 & 1.00 & 0.79 & 0.94 & 0.50 & 0.53 & 0.34 & 0.33 & 0.38 & 0.80 & 0.66 \\
Mixtral-8x22B-Instruct & 0.71 & 0.85 & 0.90 & 0.76 & 0.92 & 0.42 & 0.52 & 0.32 & 0.31 & 0.44 & 0.77 & 0.63 \\
Mixtral-8x7B-Instruct & 0.72 & 0.84 & 0.87 & 0.58 & 0.87 & 0.32 & 0.52 & 0.32 & 0.23 & 0.25 & 0.78 & 0.57 \\
GPT-3.5-Turbo & 0.64 & 0.80 & 0.90 & 0.53 & 0.87 & 0.32 & 0.44 & 0.20 & 0.20 & 0.38 & 0.74 & 0.55 \\
Llama-3-8B-Instruct & 0.57 & 0.71 & 0.82 & 0.56 & 0.79 & 0.26 & 0.37 & 0.22 & 0.30 & 0.25 & 0.67 & 0.50 \\
Mistral-7B-Instruct & 0.52 & 0.67 & 0.72 & 0.42 & 0.77 & 0.25 & 0.40 & 0.05 & 0.27 & 0.06 & 0.62 & 0.43 \\
Gemma-7B-Instruct & 0.40 & 0.63 & 0.77 & 0.32 & 0.66 & 0.12 & 0.36 & 0.12 & 0.14 & 0.06 & 0.58 & 0.38 \\
Llama-2-7B-Chat & 0.29 & 0.41 & 0.31 & 0.19 & 0.56 & 0.12 & 0.34 & 0.14 & 0.12 & -0.12 & 0.44 & 0.25 \\
Salamandra-7B-Instruct & 0.29 & 0.41 & 0.36 & 0.27 & 0.72 & 0.02 & 0.16 & -0.07 & -0.05 & -0.00 & 0.32 & 0.22 \\
Leniachat-Gemma-2B & 0.19 & 0.21 & 0.03 & 0.06 & 0.15 & 0.05 & 0.22 & -0.03 & 0.20 & -0.12 & 0.24 & 0.11 \\
\midrule
SPANISH-noto &&&&&&&&&&&&\\
\midrule
o3-mini & 0.47 & 0.91 & 0.89 & 0.82 & 0.79 & 0.43 & 0.08 & 0.98 & 0.86 & 1.00 & 0.58 & 0.71 \\
GPT-4o & 0.51 & 0.93 & 0.92 & 0.65 & 0.83 & 0.40 & 0.18 & 0.34 & 0.35 & 0.38 & 0.72 & 0.56 \\
Deepseek-R1-70B & 0.37 & 0.82 & 0.92 & 0.37 & 0.62 & 0.50 & 0.11 & 0.36 & 0.30 & 0.62 & 0.52 & 0.50 \\
GPT-4-Turbo & 0.41 & 0.83 & 0.92 & 0.58 & 0.77 & 0.39 & 0.09 & 0.38 & 0.23 & 0.19 & 0.67 & 0.50 \\
Claude-3.5-Sonnet & 0.39 & 0.75 & 0.92 & 0.58 & 0.81 & 0.40 & 0.24 & 0.18 & 0.14 & -0.00 & 0.59 & 0.45 \\
Llama-3-70B-Instruct & 0.03 & 0.41 & 0.65 & 0.30 & 0.37 & -0.05 & -0.20 & -0.09 & 0.01 & -0.06 & 0.44 & 0.16 \\
Gemma-2-27B-Instruct & 0.11 & 0.41 & 0.56 & 0.37 & 0.60 & 0.04 & -0.05 & -0.09 & -0.29 & -0.31 & 0.40 & 0.16 \\
Mistral-7B-Instruct & -0.03 & -0.00 & 0.26 & 0.19 & 0.30 & 0.11 & -0.19 & -0.07 & -0.09 & -0.06 & 0.23 & 0.06 \\
Llama-3-8B-Instruct & -0.20 & 0.05 & 0.15 & 0.09 & 0.22 & 0.22 & -0.25 & 0.10 & -0.10 & -0.00 & 0.22 & 0.05 \\
Mixtral-8x7B-Instruct & -0.18 & 0.05 & 0.15 & 0.09 & 0.32 & 0.16 & -0.25 & 0.16 & -0.12 & -0.12 & 0.20 & 0.04 \\
Mixtral-8x22B-Instruct & -0.22 & -0.00 & 0.18 & 0.12 & 0.22 & 0.02 & -0.22 & -0.17 & -0.28 & -0.38 & -0.05 & -0.07 \\
Salamandra-7B-Instruct & -0.29 & -0.25 & -0.20 & -0.05 & 0.01 & -0.22 & -0.22 & -0.21 & -0.32 & -0.38 & -0.07 & -0.20 \\
Llama-2-7B-Chat & -0.27 & -0.23 & -0.34 & -0.23 & -0.02 & 0.02 & -0.16 & -0.36 & -0.24 & -0.44 & -0.05 & -0.21 \\
GPT-3.5-Turbo & -0.48 & -0.37 & -0.20 & -0.30 & -0.08 & -0.01 & -0.29 & -0.42 & -0.40 & -0.44 & -0.20 & -0.29 \\
Gemma-7B-Instruct & -0.33 & -0.36 & -0.23 & -0.23 & -0.10 & -0.18 & -0.33 & -0.48 & -0.45 & -0.50 & -0.16 & -0.30 \\
Leniachat-Gemma-2B & -0.40 & -0.47 & -0.47 & -0.30 & -0.29 & -0.15 & -0.27 & -0.42 & -0.47 & -0.38 & -0.30 & -0.36 \\

\bottomrule
\end{tabular}
}

\caption{Cohen's Kappa results on \textbf{\textit{UNED-Access 2024}} by model and subject in English and Spanish, sorted by average.}
\label{tablauned}
\end{table*}





\begin{table*}[ht]
\centering
\resizebox{\linewidth}{!}{%
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lcccccccccccccccccc}
%\toprule

 & \textbf{\begin{sideways}abstract algebra\end{sideways}} & \textbf{\begin{sideways}anatomy\end{sideways}} & \textbf{\begin{sideways}astronomy\end{sideways}} & \textbf{\begin{sideways}college biology\end{sideways}} & \textbf{\begin{sideways}college chemistry\end{sideways}} & \textbf{\begin{sideways}college computer science\end{sideways}} & \textbf{\begin{sideways}college mathematics\end{sideways}} & \textbf{\begin{sideways}college physics\end{sideways}} & \textbf{\begin{sideways}computer security\end{sideways}} & \textbf{\begin{sideways}conceptual physics\end{sideways}} & \textbf{\begin{sideways}electrical engineering\end{sideways}} & \textbf{\begin{sideways}elementary mathematics\end{sideways}} &
 \textbf{\begin{sideways}high school biology\end{sideways}} &
 \textbf{\begin{sideways}high school chemistry\end{sideways}} &
 \textbf{\begin{sideways}high school computer science\end{sideways}} &
 \textbf{\begin{sideways}high school mathematics\end{sideways}} &
 \textbf{\begin{sideways}high school physics\end{sideways}} &
 \textbf{\begin{sideways}machine learning\end{sideways}}\\
\toprule
ENGLISH-original  &&&&&&&&&&&&\\
\midrule
o3-mini & 0.95 & 0.83 & 0.93 & 0.96 & 0.65 & 0.93 & 0.96 & 0.99 & 0.83 & 0.94 & 0.81 & 0.97 & 0.94 & 0.90 & 0.97 & 0.99 & 0.90 & 0.82 \\
Claude-3.5-Sonnet & 0.66 & 0.77 & 0.95 & 0.93 & 0.52 & 0.74 & 0.46 & 0.62 & 0.82 & 0.88 & 0.73 & 0.86 & 0.93 & 0.76 & 0.91 & 0.55 & 0.67 & 0.76 \\
GPT-4o & 0.42 & 0.85 & 0.94 & 0.93 & 0.43 & 0.74 & 0.25 & 0.54 & 0.82 & 0.86 & 0.75 & 0.68 & 0.93 & 0.72 & 0.90 & 0.34 & 0.67 & 0.67 \\
GPT-4-Turbo & 0.47 & 0.72 & 0.90 & 0.88 & 0.37 & 0.64 & 0.29 & 0.40 & 0.78 & 0.86 & 0.68 & 0.59 & 0.91 & 0.63 & 0.87 & 0.38 & 0.51 & 0.72 \\
Llama-3-70B-Instruct & 0.35 & 0.69 & 0.84 & 0.88 & 0.44 & 0.51 & 0.30 & 0.42 & 0.75 & 0.73 & 0.68 & 0.65 & 0.86 & 0.56 & 0.77 & 0.35 & 0.58 & 0.53 \\
Gemma-2-27B-Instruct & 0.26 & 0.69 & 0.80 & 0.88 & 0.31 & 0.57 & 0.29 & 0.36 & 0.71 & 0.68 & 0.60 & 0.44 & 0.91 & 0.59 & 0.76 & 0.28 & 0.45 & 0.44 \\
Mixtral-8x22B-Instruct & 0.27 & 0.64 & 0.77 & 0.82 & 0.35 & 0.51 & 0.14 & 0.37 & 0.66 & 0.63 & 0.54 & 0.45 & 0.82 & 0.55 & 0.77 & 0.21 & 0.38 & 0.33 \\
Deepseek-R1-70B & 0.26 & 0.49 & 0.77 & 0.79 & 0.36 & 0.36 & 0.26 & 0.29 & 0.64 & 0.75 & 0.56 & 0.41 & 0.81 & 0.42 & 0.63 & 0.20 & 0.30 & 0.25 \\
Mixtral-8x7B-Instruct & 0.19 & 0.55 & 0.73 & 0.79 & 0.33 & 0.49 & 0.14 & 0.29 & 0.72 & 0.50 & 0.47 & 0.26 & 0.75 & 0.42 & 0.57 & 0.17 & 0.29 & 0.33 \\
GPT-3.5-Turbo & 0.07 & 0.55 & 0.68 & 0.63 & 0.28 & 0.36 & 0.07 & 0.24 & 0.69 & 0.42 & 0.42 & 0.35 & 0.72 & 0.38 & 0.58 & 0.16 & 0.19 & 0.28 \\
Llama-3-8B-Instruct & 0.00 & 0.58 & 0.62 & 0.67 & 0.28 & 0.31 & 0.00 & 0.25 & 0.62 & 0.48 & 0.40 & 0.24 & 0.71 & 0.35 & 0.51 & 0.05 & 0.23 & 0.25 \\
Mistral-7B-Instruct & 0.10 & 0.41 & 0.52 & 0.51 & 0.17 & 0.43 & 0.12 & 0.18 & 0.59 & 0.32 & 0.37 & 0.16 & 0.58 & 0.26 & 0.43 & 0.12 & 0.11 & 0.29 \\
Gemma-7B-Instruct & -0.13 & 0.32 & 0.36 & 0.44 & 0.13 & 0.25 & -0.05 & -0.02 & 0.58 & 0.28 & 0.27 & -0.10 & 0.49 & 0.14 & 0.28 & -0.24 & -0.04 & 0.26 \\
Llama-2-7B-Chat & 0.15 & 0.27 & 0.20 & 0.21 & 0.03 & 0.10 & -0.02 & -0.14 & 0.31 & 0.14 & 0.10 & 0.08 & 0.23 & 0.08 & 0.13 & -0.05 & 0.00 & 0.17 \\
Salamandra-7B-Instruct & -0.09 & 0.14 & 0.12 & 0.23 & -0.05 & 0.06 & -0.09 & -0.06 & 0.28 & 0.13 & 0.16 & -0.08 & 0.24 & -0.04 & 0.11 & -0.09 & -0.06 & 0.07 \\
Leniachat-Gemma-2B & -0.05 & 0.17 & 0.09 & 0.07 & -0.03 & -0.04 & -0.05 & -0.01 & 0.27 & 0.08 & 0.07 & 0.02 & 0.13 & 0.06 & 0.08 & -0.03 & -0.04 & 0.13 \\
\midrule
ENGLISH-noto &&&&&&&&&&&&\\
\midrule
o3-mini & 0.88 & 0.55 & 0.64 & 0.74 & 0.48 & 0.74 & 0.91 & 0.96 & 0.41 & 0.62 & 0.48 & 0.93 & 0.60 & 0.64 & 0.74 & 0.96 & 0.76 & 0.68 \\
Deepseek-R1-70B & 0.26 & 0.39 & 0.41 & 0.57 & 0.53 & 0.32 & 0.56 & 0.70 & 0.23 & 0.51 & 0.28 & 0.64 & 0.45 & 0.63 & 0.41 & 0.70 & 0.72 & -0.03 \\
GPT-4o & 0.04 & 0.55 & 0.52 & 0.71 & 0.12 & 0.29 & 0.14 & 0.16 & 0.42 & 0.52 & 0.36 & 0.22 & 0.64 & 0.37 & 0.54 & -0.03 & 0.30 & 0.18 \\
GPT-4-Turbo & -0.16 & 0.27 & 0.44 & 0.49 & 0.12 & 0.24 & 0.11 & 0.16 & 0.30 & 0.29 & 0.10 & 0.15 & 0.52 & 0.23 & 0.40 & 0.16 & 0.11 & 0.08 \\
Claude-3.5-Sonnet & -0.12 & 0.26 & 0.27 & 0.36 & 0.03 & 0.24 & -0.04 & 0.15 & 0.05 & 0.19 & 0.04 & 0.07 & 0.41 & 0.08 & 0.28 & -0.15 & 0.03 & -0.01 \\
Gemma-2-27B-Instruct & -0.31 & 0.02 & 0.05 & 0.36 & -0.09 & -0.11 & -0.02 & -0.16 & 0.07 & 0.01 & -0.06 & -0.19 & 0.26 & 0.08 & 0.11 & -0.23 & -0.10 & -0.14 \\
Llama-3-70B-Instruct & -0.24 & 0.10 & 0.01 & 0.30 & -0.15 & 0.01 & 0.02 & -0.08 & 0.02 & -0.09 & -0.17 & -0.02 & 0.17 & -0.05 & 0.14 & -0.15 & -0.04 & -0.14 \\
Mixtral-8x7B-Instruct & -0.10 & -0.01 & 0.00 & 0.11 & -0.09 & 0.15 & -0.01 & -0.11 & 0.09 & -0.06 & 0.00 & -0.11 & 0.10 & -0.04 & 0.07 & -0.11 & 0.00 & -0.13 \\
Llama-2-7B-Chat & -0.12 & 0.05 & -0.05 & -0.02 & -0.11 & 0.08 & 0.06 & -0.15 & 0.09 & 0.12 & 0.01 & -0.03 & 0.00 & -0.02 & -0.09 & -0.08 & -0.13 & -0.07 \\
Llama-3-8B-Instruct & -0.21 & 0.06 & -0.06 & 0.12 & -0.12 & 0.01 & 0.07 & -0.05 & 0.06 & -0.10 & -0.14 & -0.17 & 0.05 & -0.09 & -0.02 & -0.04 & -0.02 & -0.21 \\
Mistral-7B-Instruct & -0.21 & -0.08 & -0.10 & 0.05 & -0.16 & -0.19 & -0.10 & -0.18 & -0.01 & -0.08 & -0.07 & -0.24 & -0.04 & -0.08 & -0.09 & -0.18 & -0.21 & -0.22 \\
Mixtral-8x22B-Instruct & -0.25 & -0.23 & -0.12 & 0.01 & -0.17 & -0.15 & -0.04 & -0.16 & -0.22 & -0.18 & -0.25 & -0.22 & -0.03 & -0.17 & -0.12 & -0.18 & -0.12 & -0.25 \\
Salamandra-7B-Instruct & -0.32 & -0.19 & -0.22 & -0.12 & -0.25 & -0.14 & -0.24 & -0.10 & -0.24 & -0.11 & -0.19 & -0.23 & -0.14 & -0.20 & -0.13 & -0.25 & -0.13 & -0.25 \\
GPT-3.5-Turbo & -0.28 & -0.24 & -0.24 & -0.13 & -0.25 & -0.26 & -0.27 & -0.29 & -0.21 & -0.30 & -0.30 & -0.29 & -0.16 & -0.22 & -0.23 & -0.27 & -0.29 & -0.25 \\
Gemma-7B-Instruct & -0.33 & -0.25 & -0.26 & -0.19 & -0.23 & -0.24 & -0.28 & -0.28 & -0.25 & -0.28 & -0.27 & -0.30 & -0.20 & -0.27 & -0.28 & -0.29 & -0.30 & -0.31 \\
Leniachat-Gemma-2B & -0.33 & -0.30 & -0.32 & -0.31 & -0.32 & -0.28 & -0.29 & -0.29 & -0.32 & -0.32 & -0.32 & -0.30 & -0.28 & -0.29 & -0.29 & -0.26 & -0.30 & -0.31 \\
\midrule
SPANISH-original  &&&&&&&&&&&&\\
\midrule
o3-mini & 0.93 & 0.82 & 0.94 & 0.94 & 0.64 & 0.96 & 0.96 & 0.97 & 0.80 & 0.92 & 0.78 & 0.95 & 0.93 & 0.88 & 0.96 & 0.98 & 0.86 & 0.81 \\
Claude-3.5-Sonnet & 0.66 & 0.77 & 0.93 & 0.92 & 0.47 & 0.75 & 0.42 & 0.66 & 0.78 & 0.89 & 0.69 & 0.89 & 0.91 & 0.76 & 0.93 & 0.49 & 0.66 & 0.69 \\
GPT-4o & 0.45 & 0.79 & 0.93 & 0.90 & 0.40 & 0.69 & 0.34 & 0.48 & 0.79 & 0.87 & 0.72 & 0.69 & 0.93 & 0.69 & 0.83 & 0.33 & 0.56 & 0.67 \\
GPT-4-Turbo & 0.46 & 0.63 & 0.89 & 0.87 & 0.31 & 0.65 & 0.26 & 0.37 & 0.80 & 0.81 & 0.64 & 0.57 & 0.90 & 0.61 & 0.84 & 0.32 & 0.49 & 0.60 \\
Llama-3-70B-Instruct & 0.29 & 0.54 & 0.82 & 0.83 & 0.41 & 0.54 & 0.19 & 0.41 & 0.73 & 0.68 & 0.67 & 0.60 & 0.85 & 0.51 & 0.77 & 0.33 & 0.46 & 0.46 \\
Gemma-2-27B-Instruct & 0.23 & 0.58 & 0.75 & 0.86 & 0.33 & 0.58 & 0.27 & 0.33 & 0.68 & 0.62 & 0.57 & 0.41 & 0.88 & 0.56 & 0.73 & 0.24 & 0.45 & 0.47 \\
Deepseek-R1-70B & 0.19 & 0.56 & 0.77 & 0.75 & 0.33 & 0.42 & 0.22 & 0.32 & 0.75 & 0.64 & 0.53 & 0.41 & 0.78 & 0.51 & 0.73 & 0.27 & 0.32 & 0.49 \\
Mixtral-8x22B-Instruct & 0.25 & 0.46 & 0.68 & 0.70 & 0.28 & 0.56 & 0.16 & 0.29 & 0.68 & 0.60 & 0.44 & 0.42 & 0.77 & 0.45 & 0.74 & 0.25 & 0.31 & 0.37 \\
Mixtral-8x7B-Instruct & 0.10 & 0.43 & 0.68 & 0.62 & 0.33 & 0.37 & 0.06 & 0.16 & 0.72 & 0.46 & 0.37 & 0.30 & 0.72 & 0.33 & 0.54 & 0.20 & 0.28 & 0.19 \\
GPT-3.5-Turbo & 0.08 & 0.45 & 0.59 & 0.52 & 0.24 & 0.35 & 0.22 & 0.19 & 0.65 & 0.40 & 0.41 & 0.25 & 0.65 & 0.32 & 0.57 & 0.07 & 0.08 & 0.13 \\
Llama-3-8B-Instruct & 0.00 & 0.35 & 0.53 & 0.42 & 0.17 & 0.25 & -0.04 & 0.03 & 0.55 & 0.30 & 0.37 & 0.16 & 0.67 & 0.24 & 0.48 & 0.01 & 0.16 & 0.18 \\
Mistral-7B-Instruct & 0.03 & 0.21 & 0.34 & 0.40 & 0.16 & 0.24 & 0.04 & 0.12 & 0.47 & 0.23 & 0.30 & 0.15 & 0.49 & 0.28 & 0.37 & 0.09 & 0.11 & 0.31 \\
Gemma-7B-Instruct & -0.10 & 0.23 & 0.30 & 0.28 & 0.19 & 0.19 & 0.02 & -0.02 & 0.52 & 0.24 & 0.25 & -0.01 & 0.39 & 0.18 & 0.27 & -0.10 & 0.00 & 0.15 \\
Llama-2-7B-Chat & -0.06 & 0.13 & 0.16 & 0.14 & -0.08 & 0.08 & 0.00 & -0.18 & 0.21 & 0.15 & 0.16 & 0.09 & 0.23 & 0.03 & 0.13 & 0.04 & 0.01 & 0.15 \\
Salamandra-7B-Instruct & -0.09 & -0.03 & 0.11 & 0.18 & -0.03 & 0.11 & -0.04 & -0.20 & 0.24 & 0.11 & 0.14 & -0.01 & 0.19 & -0.08 & 0.08 & -0.10 & -0.02 & 0.10 \\
Leniachat-Gemma-2B & 0.02 & 0.09 & 0.10 & -0.00 & 0.07 & 0.01 & 0.04 & -0.05 & 0.16 & 0.08 & 0.10 & -0.01 & 0.16 & 0.11 & 0.11 & -0.05 & -0.05 & 0.04 \\
\midrule
SPANISH-noto &&&&&&&&&&&&\\
\midrule
o3-mini & 0.87 & 0.51 & 0.57 & 0.60 & 0.35 & 0.63 & 0.91 & 0.93 & 0.35 & 0.66 & 0.35 & 0.91 & 0.58 & 0.61 & 0.73 & 0.94 & 0.72 & 0.61 \\
GPT-4o & 0.04 & 0.48 & 0.54 & 0.61 & 0.20 & 0.28 & 0.15 & 0.15 & 0.37 & 0.55 & 0.31 & 0.25 & 0.63 & 0.33 & 0.56 & -0.01 & 0.21 & 0.25 \\
GPT-4-Turbo & -0.13 & 0.26 & 0.47 & 0.49 & 0.23 & 0.22 & 0.30 & 0.23 & 0.27 & 0.35 & 0.10 & 0.18 & 0.55 & 0.28 & 0.40 & 0.25 & 0.17 & 0.08 \\
Deepseek-R1-70B & -0.01 & 0.23 & 0.36 & 0.20 & 0.23 & 0.19 & 0.29 & 0.56 & 0.14 & 0.29 & 0.10 & 0.31 & 0.27 & 0.45 & 0.28 & 0.38 & 0.53 & -0.01 \\
Claude-3.5-Sonnet & -0.10 & 0.30 & 0.27 & 0.45 & 0.00 & 0.29 & -0.02 & 0.06 & 0.13 & 0.30 & -0.02 & 0.10 & 0.43 & 0.15 & 0.38 & -0.13 & 0.13 & -0.03 \\
Gemma-2-27B-Instruct & -0.32 & 0.00 & 0.05 & 0.29 & -0.16 & -0.07 & -0.08 & -0.12 & 0.00 & 0.01 & -0.09 & -0.23 & 0.19 & 0.03 & 0.10 & -0.23 & -0.10 & -0.15 \\
Llama-3-70B-Instruct & -0.21 & -0.04 & -0.07 & 0.08 & -0.20 & -0.11 & -0.08 & -0.18 & 0.05 & -0.03 & -0.20 & -0.06 & 0.10 & -0.06 & 0.10 & -0.18 & -0.09 & -0.18 \\
Llama-3-8B-Instruct & -0.21 & -0.11 & -0.11 & 0.01 & -0.11 & 0.01 & 0.02 & -0.03 & -0.04 & -0.05 & -0.17 & -0.19 & -0.02 & -0.01 & 0.07 & -0.13 & 0.01 & -0.11 \\
Mixtral-8x7B-Instruct & -0.13 & -0.08 & -0.07 & 0.00 & -0.04 & -0.03 & 0.00 & -0.16 & -0.08 & -0.13 & -0.10 & -0.16 & 0.00 & -0.04 & -0.00 & -0.11 & -0.09 & -0.17 \\
Llama-2-7B-Chat & -0.19 & -0.02 & -0.11 & -0.11 & -0.23 & 0.04 & 0.03 & -0.14 & -0.07 & -0.15 & -0.07 & -0.08 & -0.14 & -0.09 & -0.13 & -0.09 & -0.15 & -0.11 \\
Mistral-7B-Instruct & -0.17 & -0.15 & -0.10 & -0.01 & -0.17 & -0.10 & 0.04 & -0.15 & -0.02 & -0.12 & -0.04 & -0.24 & -0.04 & -0.08 & -0.08 & -0.10 & -0.20 & -0.06 \\
Mixtral-8x22B-Instruct & -0.28 & -0.17 & -0.13 & -0.08 & -0.13 & -0.14 & -0.04 & -0.27 & -0.21 & -0.17 & -0.20 & -0.23 & -0.05 & -0.14 & -0.05 & -0.23 & -0.15 & -0.26 \\
Salamandra-7B-Instruct & -0.24 & -0.26 & -0.17 & -0.21 & -0.16 & -0.03 & -0.02 & -0.20 & -0.09 & -0.08 & -0.17 & -0.14 & -0.16 & -0.19 & -0.06 & -0.13 & -0.17 & -0.10 \\
GPT-3.5-Turbo & -0.27 & -0.30 & -0.20 & -0.14 & -0.25 & -0.24 & -0.24 & -0.25 & -0.28 & -0.29 & -0.32 & -0.28 & -0.18 & -0.25 & -0.18 & -0.20 & -0.28 & -0.28 \\
Gemma-7B-Instruct & -0.32 & -0.20 & -0.20 & -0.21 & -0.27 & -0.22 & -0.29 & -0.31 & -0.26 & -0.27 & -0.25 & -0.30 & -0.24 & -0.23 & -0.23 & -0.32 & -0.30 & -0.29 \\
Leniachat-Gemma-2B & -0.32 & -0.33 & -0.30 & -0.32 & -0.32 & -0.32 & -0.28 & -0.31 & -0.33 & -0.33 & -0.28 & -0.29 & -0.30 & -0.31 & -0.32 & -0.26 & -0.33 & -0.33 \\

\bottomrule
\end{tabular}
}

\caption{Cohen's Kappa results on \textbf{MMLU} (\textbf{\textcolor{blue}{STEM}}) by model and subject in English and Spanish, sorted by average.}
\label{tablammlu1}
\end{table*}



\begin{table*}[ht]
\centering
\scriptsize
\resizebox{\linewidth}{!}{%
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lccccccccccc}
%\toprule

 & \textbf{\begin{sideways}econometrics\end{sideways}} & \textbf{\begin{sideways}high school geography\end{sideways}} & \textbf{\tiny\begin{sideways}h.s. government and politics\end{sideways}} & \textbf{\tiny\begin{sideways}h.s. macroeconomics\end{sideways}} & \textbf{\tiny\begin{sideways}h.s. microeconomics\end{sideways}} & \textbf{\begin{sideways}high school psychology\end{sideways}} & \textbf{\begin{sideways}high school statistics\end{sideways}} & \textbf{\begin{sideways}public relations\end{sideways}} & \textbf{\begin{sideways}security studies\end{sideways}} & \textbf{\begin{sideways}sociology\end{sideways}} & \textbf{\begin{sideways}us foreign policy\end{sideways}} \\
\toprule
ENGLISH-original  &&&&&&&&&&&\\
\midrule
o3-mini & 0.83 & 0.91 & 0.97 & 0.91 & 0.96 & 0.95 & 0.91 & 0.67 & 0.72 & 0.89 & 0.89 \\
Claude-3.5-Sonnet & 0.72 & 0.90 & 0.96 & 0.86 & 0.97 & 0.93 & 0.78 & 0.77 & 0.78 & 0.93 & 0.93 \\
GPT-4o & 0.58 & 0.91 & 0.97 & 0.88 & 0.97 & 0.94 & 0.70 & 0.69 & 0.78 & 0.94 & 0.93 \\
GPT-4-Turbo & 0.58 & 0.93 & 0.97 & 0.82 & 0.93 & 0.93 & 0.70 & 0.70 & 0.69 & 0.88 & 0.93 \\
Llama-3-70B-Instruct & 0.58 & 0.88 & 0.97 & 0.77 & 0.83 & 0.92 & 0.63 & 0.64 & 0.71 & 0.86 & 0.89 \\
Gemma-2-27B-Instruct & 0.50 & 0.90 & 0.99 & 0.79 & 0.84 & 0.91 & 0.59 & 0.64 & 0.70 & 0.85 & 0.87 \\
Mixtral-8x22B-Instruct & 0.49 & 0.79 & 0.94 & 0.67 & 0.78 & 0.87 & 0.60 & 0.70 & 0.70 & 0.88 & 0.91\\
Deepseek-R1-70B & 0.46 & 0.78 & 0.91 & 0.72 & 0.79 & 0.89 & 0.59 & 0.62 & 0.66 & 0.85 & 0.78 \\
Mixtral-8x7B-Instruct & 0.50 & 0.79 & 0.92 & 0.55 & 0.63 & 0.81 & 0.42 & 0.63 & 0.66 & 0.83 & 0.84 \\
GPT-3.5-Turbo & 0.29 & 0.76 & 0.87 & 0.56 & 0.63 & 0.81 & 0.29 & 0.62 & 0.58 & 0.79 & 0.78 \\
Llama-3-8B-Instruct & 0.43 & 0.69 & 0.82 & 0.53 & 0.65 & 0.79 & 0.43 & 0.57 & 0.61 & 0.82 & 0.84 \\
Mistral-7B-Instruct & 0.28 & 0.61 & 0.77 & 0.37 & 0.49 & 0.71 & 0.27 & 0.57 & 0.49 & 0.81 & 0.71 \\
Gemma-7B-Instruct & 0.09 & 0.59 & 0.59 & 0.33 & 0.34 & 0.63 & -0.01 & 0.59 & 0.43 & 0.61 & 0.60 \\
Llama-2-7B-Chat & 0.06 & 0.32 & 0.33 & 0.11 & 0.10 & 0.37 & -0.04 & 0.36 & 0.15 & 0.33 & 0.57 \\
Salamandra-7B-Instruct & -0.00 & 0.25 & 0.38 & 0.15 & 0.11 & 0.42 & -0.04 & 0.22 & 0.17 & 0.51 & 0.44 \\
Leniachat-Gemma-2B & 0.01 & 0.17 & 0.21 & 0.08 & 0.08 & 0.21 & -0.02 & 0.19 & 0.10 & 0.20 & 0.21 \\
\midrule
ENGLISH-noto &&&&&&&&&&&\\
\midrule
o3-mini & 0.50 & 0.49 & 0.72 & 0.76 & 0.81 & 0.63 & 0.72 & 0.11 & 0.12 & 0.43 & 0.55 \\
Deepseek-R1-70B & 0.40 & 0.59 & 0.75 & 0.57 & 0.59 & 0.59 & 0.59 & 0.33 & 0.50 & 0.51 & 0.42 \\
GPT-4o & 0.36 & 0.63 & 0.80 & 0.60 & 0.77 & 0.67 & 0.30 & 0.16 & 0.26 & 0.52 & 0.73 \\
GPT-4-Turbo & 0.20 & 0.59 & 0.82 & 0.38 & 0.54 & 0.55 & 0.30 & 0.15 & 0.33 & 0.53 & 0.62 \\
Claude-3.5-Sonnet & 0.08 & 0.38 & 0.71 & 0.36 & 0.51 & 0.28 & 0.19 & -0.14 & 0.15 & 0.26 & 0.42 \\
Gemma-2-27B-Instruct & -0.11 & 0.28 & 0.56 & 0.15 & 0.27 & 0.38 & -0.03 & -0.06 & 0.21 & 0.30 & 0.30 \\
Llama-3-70B-Instruct & -0.08 & 0.23 & 0.46 & 0.12 & 0.24 & 0.23 & -0.01 & -0.09 & 0.06 & 0.03 & 0.24 \\
Mixtral-8x7B-Instruct & -0.11 & 0.38 & 0.40 & -0.02 & 0.08 & 0.21 & -0.10 & 0.10 & 0.04 & 0.13 & 0.23 \\
Llama-2-7B-Chat & -0.06 & 0.25 & 0.14 & -0.08 & 0.00 & 0.19 & -0.12 & 0.23 & -0.07 & 0.17 & 0.10 \\
Llama-3-8B-Instruct & -0.13 & 0.21 & 0.35 & -0.00 & 0.05 & 0.12 & -0.06 & 0.04 & 0.00 & 0.08 & 0.05 \\
Mistral-7B-Instruct & -0.17 & 0.06 & 0.07 & -0.11 & -0.05 & 0.02 & -0.20 & -0.07 & -0.10 & -0.01 & -0.08 \\
Mixtral-8x22B-Instruct & -0.21 & 0.04 & 0.14 & -0.10 & -0.07 & -0.03 & -0.08 & -0.21 & -0.01 & -0.09 & -0.14 \\
Salamandra-7B-Instruct & -0.20 & -0.23 & -0.24 & -0.23 & -0.15 & -0.19 & -0.22 & -0.25 & -0.19 & -0.14 & -0.17 \\
GPT-3.5-Turbo & -0.27 & -0.04 & -0.03 & -0.24 & -0.15 & -0.18 & -0.26 & -0.25 & -0.05 & -0.20 & -0.24 \\
Gemma-7B-Instruct & -0.25 & -0.11 & -0.20 & -0.27 & -0.25 & -0.22 & -0.28 & -0.21 & -0.21 & -0.26 & -0.26 \\
Leniachat-Gemma-2B & -0.32 & -0.30 & -0.33 & -0.32 & -0.32 & -0.30 & -0.29 & -0.30 & -0.30 & -0.30 & -0.30 \\
\midrule
SPANISH-original  &&&&&&&&&&&\\
\midrule
o3-mini & 0.76 & 0.86 & 0.89 & 0.89 & 0.95 & 0.93 & 0.90 & 0.58 & 0.72 & 0.85 & 0.91 \\
Claude-3.5-Sonnet & 0.76 & 0.94 & 0.93 & 0.83 & 0.92 & 0.93 & 0.78 & 0.72 & 0.78 & 0.92 & 0.95 \\
GPT-4o & 0.61 & 0.90 & 0.95 & 0.87 & 0.94 & 0.94 & 0.72 & 0.73 & 0.73 & 0.85 & 0.91 \\
GPT-4-Turbo & 0.58 & 0.86 & 0.92 & 0.75 & 0.92 & 0.89 & 0.65 & 0.63 & 0.66 & 0.83 & 0.91 \\
Llama-3-70B-Instruct & 0.55 & 0.85 & 0.87 & 0.67 & 0.75 & 0.86 & 0.60 & 0.63 & 0.64 & 0.82 & 0.86 \\
Gemma-2-27B-Instruct & 0.46 & 0.84 & 0.88 & 0.72 & 0.82 & 0.87 & 0.61 & 0.63 & 0.71 & 0.81 & 0.84 \\
Deepseek-R1-70B & 0.47 & 0.82 & 0.82 & 0.67 & 0.72 & 0.83 & 0.56 & 0.62 & 0.70 & 0.80 & 0.84 \\
Mixtral-8x22B-Instruct & 0.41 & 0.74 & 0.77 & 0.61 & 0.74 & 0.84 & 0.56 & 0.57 & 0.70 & 0.77 & 0.75 \\
Mixtral-8x7B-Instruct & 0.39 & 0.75 & 0.73 & 0.51 & 0.61 & 0.72 & 0.34 & 0.62 & 0.68 & 0.74 & 0.82 \\
GPT-3.5-Turbo & 0.22 & 0.65 & 0.67 & 0.44 & 0.50 & 0.70 & 0.23 & 0.57 & 0.49 & 0.62 & 0.66 \\
Llama-3-8B-Instruct & 0.17 & 0.61 & 0.64 & 0.44 & 0.47 & 0.63 & 0.30 & 0.42 & 0.54 & 0.70 & 0.62 \\
Mistral-7B-Instruct & 0.18 & 0.53 & 0.45 & 0.23 & 0.35 & 0.54 & 0.22 & 0.40 & 0.42 & 0.63 & 0.68 \\
Gemma-7B-Instruct & 0.09 & 0.44 & 0.38 & 0.25 & 0.26 & 0.50 & 0.01 & 0.35 & 0.33 & 0.47 & 0.57 \\
Llama-2-7B-Chat & 0.03 & 0.28 & 0.22 & 0.05 & 0.06 & 0.27 & -0.03 & 0.32 & 0.23 & 0.33 & 0.51 \\
Salamandra-7B-Instruct & -0.04 & 0.26 & 0.35 & 0.10 & 0.11 & 0.33 & -0.04 & 0.31 & 0.06 & 0.38 & 0.21 \\
Leniachat-Gemma-2B & -0.04 & 0.07 & 0.08 & 0.08 & 0.10 & 0.19 & 0.05 & 0.14 & 0.04 & 0.14 & 0.15 \\
\midrule
SPANISH-noto &&&&&&&&&&&\\
\midrule
o3-mini & 0.46 & 0.47 & 0.61 & 0.67 & 0.69 & 0.57 & 0.62 & 0.00 & 0.03 & 0.30 & 0.39 \\
GPT-4o & 0.31 & 0.60 & 0.73 & 0.51 & 0.68 & 0.62 & 0.30 & 0.20 & 0.25 & 0.54 & 0.55 \\
GPT-4-Turbo & 0.22 & 0.61 & 0.80 & 0.45 & 0.62 & 0.56 & 0.32 & 0.21 & 0.36 & 0.54 & 0.60 \\
Deepseek-R1-70B & 0.16 & 0.26 & 0.45 & 0.32 & 0.37 & 0.32 & 0.33 & -0.04 & 0.18 & 0.23 & 0.44 \\
Claude-3.5-Sonnet & 0.23 & 0.46 & 0.63 & 0.38 & 0.49 & 0.39 & 0.19 & -0.06 & 0.20 & 0.36 & 0.42 \\
Gemma-2-27B-Instruct & -0.16 & 0.21 & 0.34 & 0.08 & 0.16 & 0.21 & -0.08 & -0.09 & 0.16 & 0.27 & 0.19 \\
Llama-3-70B-Instruct & -0.15 & 0.13 & 0.28 & 0.03 & 0.12 & 0.17 & -0.07 & -0.09 & 0.01 & -0.01 & 0.10 \\
Llama-3-8B-Instruct & -0.11 & 0.21 & 0.09 & -0.05 & -0.02 & 0.06 & 0.01 & -0.01 & -0.04 & -0.03 & -0.03 \\
Mixtral-8x7B-Instruct & -0.21 & 0.23 & 0.13 & -0.12 & -0.06 & -0.07 & -0.18 & 0.02 & 0.01 & 0.04 & -0.06 \\
Llama-2-7B-Chat & -0.08 & 0.12 & -0.09 & -0.19 & -0.14 & -0.02 & -0.13 & 0.16 & -0.16 & -0.14 & 0.05 \\
Mistral-7B-Instruct & -0.10 & 0.15 & -0.00 & -0.16 & -0.05 & -0.01 & -0.12 & -0.07 & -0.11 & -0.07 & -0.12 \\
Mixtral-8x22B-Instruct & -0.24 & 0.05 & 0.14 & -0.11 & -0.08 & -0.06 & -0.08 & -0.16 & -0.02 & -0.09 & -0.12 \\
Salamandra-7B-Instruct & -0.10 & -0.04 & -0.03 & -0.22 & -0.20 & -0.21 & -0.20 & -0.20 & -0.21 & -0.13 & -0.23 \\
GPT-3.5-Turbo & -0.22 & -0.01 & -0.14 & -0.27 & -0.21 & -0.24 & -0.24 & -0.20 & -0.12 & -0.25 & -0.30 \\
Gemma-7B-Instruct & -0.30 & -0.10 & -0.19 & -0.28 & -0.26 & -0.20 & -0.29 & -0.16 & -0.26 & -0.26 & -0.15 \\
Leniachat-Gemma-2B & -0.32 & -0.27 & -0.26 & -0.31 & -0.30 & -0.27 & -0.29 & -0.33 & -0.33 & -0.30 & -0.32 \\
\bottomrule
\end{tabular}
}

\caption{Cohen's Kappa results on \textbf{MMLU} (\textbf{\textcolor{blue}{Social Sciences}}) by model and subject in English and Spanish, sorted by average.}
\label{tablammlu2}
\end{table*}







\begin{table*}[ht]
\centering
\resizebox{\linewidth}{!}{%
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lccccccccccccc}
%\toprule

 & \textbf{\begin{sideways}formal logic\end{sideways}} & \textbf{\small\begin{sideways}h.s. european history\end{sideways}} & \textbf{\begin{sideways}h.s. us history\end{sideways}} & \textbf{\begin{sideways}h.s. world history\end{sideways}} & \textbf{\begin{sideways}international law\end{sideways}} & 
 \textbf{\begin{sideways}jurisprudence\end{sideways}} &
 \textbf{\begin{sideways}logical fallacies\end{sideways}} & \textbf{\begin{sideways}moral disputes\end{sideways}} & \textbf{\begin{sideways}moral scenarios\end{sideways}} & \textbf{\begin{sideways}philosophy\end{sideways}} & \textbf{\begin{sideways}prehistory\end{sideways}} & \textbf{\begin{sideways}professional law\end{sideways}} &
 \textbf{\begin{sideways}world religions\end{sideways}}\\
\toprule
ENGLISH-original  &&&&&&&&&&&\\
\midrule
o3-mini & 0.95 & 0.80 & 0.85 & 0.87 & 0.87 & 0.85 & 0.87 & 0.76 & 0.53 & 0.78 & 0.86 & 0.50 & 0.83 \\
Claude-3.5-Sonnet & 0.65 & 0.87 & 0.90 & 0.93 & 0.91 & 0.83 & 0.89 & 0.83 & 0.81 & 0.87 & 0.93 & 0.65 & 0.87 \\
GPT-4o & 0.56 & 0.86 & 0.92 & 0.93 & 0.89 & 0.86 & 0.86 & 0.84 & 0.56 & 0.91 & 0.92 & 0.66 & 0.87 \\
GPT-4-Turbo & 0.51 & 0.84 & 0.93 & 0.90 & 0.88 & 0.81 & 0.83 & 0.77 & 0.59 & 0.84 & 0.85 & 0.55 & 0.81 \\
Llama-3-70B-Instruct & 0.48 & 0.76 & 0.87 & 0.82 & 0.87 & 0.83 & 0.77 & 0.76 & 0.47 & 0.74 & 0.84 & 0.49 & 0.86 \\
Gemma-2-27B-Instruct & 0.40 & 0.79 & 0.86 & 0.89 & 0.79 & 0.80 & 0.77 & 0.70 & 0.31 & 0.77 & 0.79 & 0.46 & 0.83 \\
Mixtral-8x22B-Instruct & 0.48 & 0.79 & 0.81 & 0.81 & 0.88 & 0.76 & 0.76 & 0.68 & 0.40 & 0.77 & 0.76 & 0.44 & 0.77 \\
Deepseek-R1-70B & 0.33 & 0.76 & 0.87 & 0.81 & 0.82 & 0.77 & 0.68 & 0.61 & 0.72 & 0.75 & 0.76 & 0.48 & 0.79 \\
Mixtral-8x7B-Instruct & 0.29 & 0.73 & 0.77 & 0.81 & 0.79 & 0.76 & 0.69 & 0.66 & 0.07 & 0.69 & 0.74 & 0.35 & 0.83 \\
GPT-3.5-Turbo & 0.24 & 0.65 & 0.76 & 0.75 & 0.76 & 0.62 & 0.72 & 0.61 & 0.10 & 0.71 & 0.65 & 0.32 & 0.79 \\
Llama-3-8B-Instruct & 0.31 & 0.62 & 0.70 & 0.75 & 0.69 & 0.66 & 0.65 & 0.58 & -0.02 & 0.65 & 0.63 & 0.29 & 0.71 \\
Mistral-7B-Instruct & 0.20 & 0.64 & 0.67 & 0.66 & 0.65 & 0.62 & 0.67 & 0.55 & 0.15 & 0.50 & 0.54 & 0.22 & 0.74 \\
Gemma-7B-Instruct & 0.04 & 0.44 & 0.41 & 0.50 & 0.54 & 0.50 & 0.43 & 0.39 & -0.09 & 0.47 & 0.43 & 0.10 & 0.54 \\
Llama-2-7B-Chat & -0.01 & 0.28 & 0.20 & 0.28 & 0.43 & 0.29 & 0.30 & 0.20 & -0.01 & 0.24 & 0.35 & 0.10 & 0.49 \\
Salamandra-7B-Instruct & 0.06 & 0.42 & 0.44 & 0.46 & 0.35 & 0.25 & 0.29 & 0.21 & -0.25 & 0.21 & 0.21 & 0.07 & 0.38 \\
Leniachat-Gemma-2B & 0.14 & 0.10 & 0.12 & 0.07 & 0.19 & 0.07 & 0.17 & 0.07 & -0.01 & 0.13 & 0.11 & 0.06 & 0.23 \\
\midrule
ENGLISH-noto &&&&&&&&&&&\\
\midrule
o3-mini & 0.70 & 0.41 & 0.64 & 0.51 & 0.37 & 0.37 & 0.63 & 0.10 & 0.39 & 0.33 & 0.39 & -0.07 & 0.56 \\
Deepseek-R1-70B & 0.27 & 0.56 & 0.65 & 0.52 & 0.50 & 0.52 & 0.39 & 0.24 & 0.42 & 0.49 & 0.42 & 0.35 & 0.46 \\
GPT-4o & 0.24 & 0.47 & 0.72 & 0.61 & 0.48 & 0.64 & 0.74 & 0.52 & 0.18 & 0.56 & 0.58 & 0.09 & 0.65 \\
GPT-4-Turbo & 0.06 & 0.44 & 0.68 & 0.58 & 0.42 & 0.59 & 0.69 & 0.41 & -0.18 & 0.39 & 0.47 & -0.04 & 0.59 \\
Claude-3.5-Sonnet & -0.02 & 0.33 & 0.62 & 0.49 & 0.33 & 0.34 & 0.35 & 0.21 & -0.02 & 0.24 & 0.31 & 0.07 & 0.34 \\
Gemma-2-27B-Instruct & -0.22 & 0.30 & 0.51 & 0.41 & 0.15 & 0.28 & 0.34 & 0.14 & -0.31 & 0.18 & 0.11 & -0.12 & 0.13 \\
Llama-3-70B-Instruct & -0.14 & 0.28 & 0.45 & 0.30 & 0.06 & 0.09 & 0.27 & -0.09 & -0.11 & 0.01 & -0.04 & -0.06 & 0.15 \\
Mixtral-8x7B-Instruct & -0.18 & 0.19 & 0.44 & 0.24 & -0.04 & 0.09 & 0.27 & 0.10 & -0.26 & 0.08 & 0.08 & -0.14 & 0.26 \\
Llama-2-7B-Chat & -0.23 & 0.33 & 0.28 & 0.31 & 0.14 & 0.17 & 0.23 & 0.07 & -0.01 & 0.10 & 0.06 & -0.23 & 0.13 \\
Llama-3-8B-Instruct & -0.19 & 0.20 & 0.39 & 0.25 & -0.06 & 0.14 & 0.39 & -0.02 & -0.33 & 0.12 & 0.02 & -0.18 & 0.13 \\
Mistral-7B-Instruct & -0.25 & -0.04 & 0.19 & 0.10 & -0.20 & -0.09 & 0.18 & -0.08 & -0.27 & -0.10 & -0.11 & -0.28 & 0.03 \\
Mixtral-8x22B-Instruct & -0.19 & -0.07 & 0.06 & -0.11 & -0.29 & -0.10 & 0.01 & -0.12 & -0.14 & -0.05 & -0.12 & -0.24 & -0.13 \\
Salamandra-7B-Instruct & -0.26 & -0.05 & -0.06 & 0.00 & -0.23 & -0.26 & -0.25 & -0.19 & -0.26 & -0.19 & -0.19 & -0.21 & -0.11 \\
GPT-3.5-Turbo & -0.30 & -0.16 & 0.06 & -0.08 & -0.27 & -0.16 & -0.20 & -0.21 & -0.33 & -0.21 & -0.23 & -0.30 & -0.24 \\
Gemma-7B-Instruct & -0.28 & -0.19 & -0.12 & -0.12 & -0.32 & -0.26 & -0.29 & -0.24 & -0.15 & -0.30 & -0.26 & -0.29 & -0.27 \\
Leniachat-Gemma-2B & -0.27 & -0.18 & -0.16 & -0.20 & -0.31 & -0.33 & -0.28 & -0.31 & -0.26 & -0.28 & -0.33 & -0.28 & -0.30 \\
\midrule
SPANISH-original  &&&&&&&&&&&\\
\midrule
o3-mini & 0.88 & 0.77 & 0.79 & 0.84 & 0.81 & 0.76 & 0.80 & 0.70 & 0.40 & 0.75 & 0.81 & 0.40 & 0.83 \\
Claude-3.5-Sonnet & 0.55 & 0.84 & 0.90 & 0.90 & 0.88 & 0.85 & 0.87 & 0.82 & 0.73 & 0.90 & 0.90 & 0.58 & 0.87 \\
GPT-4o & 0.52 & 0.83 & 0.91 & 0.93 & 0.87 & 0.82 & 0.84 & 0.82 & 0.58 & 0.87 & 0.90 & 0.55 & 0.86 \\
GPT-4-Turbo & 0.46 & 0.82 & 0.85 & 0.86 & 0.85 & 0.80 & 0.84 & 0.76 & 0.54 & 0.78 & 0.83 & 0.46 & 0.82 \\
Llama-3-70B-Instruct & 0.43 & 0.79 & 0.78 & 0.79 & 0.78 & 0.77 & 0.74 & 0.67 & 0.24 & 0.68 & 0.79 & 0.37 & 0.80 \\
Gemma-2-27B-Instruct & 0.38 & 0.76 & 0.84 & 0.81 & 0.75 & 0.75 & 0.71 & 0.65 & 0.01 & 0.71 & 0.73 & 0.37 & 0.77 \\
Deepseek-R1-70B & 0.41 & 0.80 & 0.85 & 0.85 & 0.80 & 0.71 & 0.74 & 0.69 & 0.52 & 0.71 & 0.78 & 0.39 & 0.82 \\
Mixtral-8x22B-Instruct & 0.34 & 0.70 & 0.72 & 0.76 & 0.73 & 0.67 & 0.67 & 0.64 & 0.02 & 0.64 & 0.65 & 0.34 & 0.72 \\
Mixtral-8x7B-Instruct & 0.26 & 0.74 & 0.72 & 0.76 & 0.72 & 0.67 & 0.65 & 0.56 & 0.05 & 0.64 & 0.61 & 0.27 & 0.77 \\
GPT-3.5-Turbo & 0.20 & 0.65 & 0.66 & 0.72 & 0.65 & 0.61 & 0.63 & 0.53 & 0.03 & 0.59 & 0.53 & 0.24 & 0.71 \\
Llama-3-8B-Instruct & 0.20 & 0.63 & 0.60 & 0.71 & 0.63 & 0.64 & 0.47 & 0.51 & 0.10 & 0.57 & 0.51 & 0.21 & 0.61 \\
Mistral-7B-Instruct & 0.10 & 0.56 & 0.50 & 0.60 & 0.61 & 0.49 & 0.47 & 0.45 & 0.10 & 0.42 & 0.41 & 0.15 & 0.58 \\
Gemma-7B-Instruct & 0.21 & 0.34 & 0.26 & 0.34 & 0.47 & 0.38 & 0.29 & 0.28 & -0.17 & 0.44 & 0.30 & -0.11 & 0.46 \\
Llama-2-7B-Chat & -0.03 & 0.33 & 0.33 & 0.40 & 0.46 & 0.33 & 0.17 & 0.18 & 0.01 & 0.27 & 0.26 & 0.11 & 0.41 \\
Salamandra-7B-Instruct & -0.09 & 0.40 & 0.35 & 0.39 & 0.28 & 0.25 & 0.17 & 0.20 & -0.21 & 0.28 & 0.12 & 0.06 & 0.41 \\
Leniachat-Gemma-2B & 0.04 & 0.15 & 0.10 & 0.03 & 0.23 & 0.07 & 0.13 & 0.03 & -0.01 & 0.15 & 0.09 & 0.05 & 0.07 \\
\midrule
SPANISH-noto &&&&&&&&&&&\\
\midrule
o3-mini & 0.68 & 0.41 & 0.49 & 0.41 & 0.27 & 0.25 & 0.58 & 0.08 & 0.28 & 0.26 & 0.28 & -0.15 & 0.51 \\
GPT-4o & 0.16 & 0.48 & 0.65 & 0.55 & 0.46 & 0.54 & 0.71 & 0.42 & 0.33 & 0.48 & 0.57 & -0.03 & 0.67 \\
GPT-4-Turbo & 0.09 & 0.51 & 0.69 & 0.61 & 0.42 & 0.63 & 0.62 & 0.48 & -0.19 & 0.44 & 0.48 & 0.03 & 0.57 \\
Deepseek-R1-70B & 0.07 & 0.28 & 0.33 & 0.38 & 0.46 & 0.29 & 0.20 & 0.16 & -0.19 & 0.30 & 0.25 & 0.23 & 0.35 \\
Claude-3.5-Sonnet & -0.05 & 0.34 & 0.55 & 0.53 & 0.46 & 0.37 & 0.31 & 0.27 & -0.03 & 0.33 & 0.37 & -0.01 & 0.42 \\
Gemma-2-27B-Instruct & -0.18 & 0.23 & 0.41 & 0.34 & 0.16 & 0.26 & 0.18 & 0.14 & -0.33 & 0.17 & 0.05 & -0.14 & 0.09 \\
Llama-3-70B-Instruct & -0.18 & 0.24 & 0.35 & 0.26 & 0.02 & 0.07 & 0.20 & -0.05 & -0.21 & 0.01 & -0.06 & -0.16 & 0.16 \\
Llama-3-8B-Instruct & -0.21 & 0.28 & 0.28 & 0.21 & 0.07 & 0.09 & 0.10 & 0.04 & -0.28 & 0.10 & -0.08 & -0.21 & -0.01 \\
Mixtral-8x7B-Instruct & -0.23 & 0.19 & 0.24 & 0.21 & -0.14 & -0.00 & 0.12 & -0.03 & -0.33 & -0.07 & -0.02 & -0.19 & 0.16 \\
Llama-2-7B-Chat & -0.26 & 0.33 & 0.32 & 0.27 & -0.14 & 0.01 & 0.06 & -0.07 & -0.33 & -0.15 & -0.09 & -0.23 & 0.16 \\
Mistral-7B-Instruct & -0.26 & 0.09 & 0.33 & 0.25 & -0.17 & -0.04 & 0.18 & 0.01 & -0.32 & -0.07 & -0.11 & -0.23 & 0.04 \\
Mixtral-8x22B-Instruct & -0.19 & -0.06 & 0.04 & -0.06 & -0.18 & -0.04 & -0.10 & -0.15 & -0.19 & -0.13 & -0.15 & -0.25 & -0.10 \\
Salamandra-7B-Instruct & -0.21 & -0.02 & 0.03 & -0.03 & -0.26 & -0.16 & -0.23 & -0.13 & -0.28 & -0.11 & -0.21 & -0.21 & -0.11 \\
GPT-3.5-Turbo & -0.29 & -0.14 & 0.00 & -0.11 & -0.26 & -0.28 & -0.28 & -0.21 & -0.33 & -0.24 & -0.26 & -0.30 & -0.29 \\
Gemma-7B-Instruct & -0.23 & -0.07 & -0.09 & -0.09 & -0.26 & -0.22 & -0.26 & -0.25 & -0.23 & -0.27 & -0.25 & -0.29 & -0.20 \\
Leniachat-Gemma-2B & -0.33 & -0.21 & -0.21 & -0.24 & -0.31 & -0.30 & -0.27 & -0.27 & -0.33 & -0.23 & -0.32 & -0.30 & -0.29 \\
\bottomrule

\bottomrule
\end{tabular}
}

\caption{Cohen's Kappa results on \textbf{MMLU} (\textbf{\textcolor{blue}{Humanities}}) by model and subject in English and Spanish, sorted by average.}
\label{tablammlu3}
\end{table*}








\begin{table*}[ht]
\centering
\resizebox{\linewidth}{!}{%
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lcccccccccccccccc}
%\toprule

 & \textbf{\begin{sideways}business ethics\end{sideways}} & \textbf{\begin{sideways}clinical knowledge\end{sideways}} & \textbf{\begin{sideways}college medicine\end{sideways}} & \textbf{\begin{sideways}global facts\end{sideways}} & \textbf{\begin{sideways}human aging\end{sideways}} & \textbf{\begin{sideways}human sexuality\end{sideways}} & \textbf{\begin{sideways}management\end{sideways}} & \textbf{\begin{sideways}marketing\end{sideways}} & \textbf{\begin{sideways}medical genetics\end{sideways}} & \textbf{\begin{sideways}miscellaneous\end{sideways}} & \textbf{\begin{sideways}nutrition\end{sideways}} & \textbf{\begin{sideways}professional accounting\end{sideways}} &
 \textbf{\begin{sideways}professional medicine\end{sideways}} &
 \textbf{\begin{sideways}professional psychology\end{sideways}} &
 \textbf{\begin{sideways}virology\end{sideways}} & \textbf{Average}\\
\toprule
ENGLISH-original  &&&&&&&&&&&&\\
\midrule
o3-mini & 0.73 & 0.87 & 0.84 & 0.49 & 0.77 & 0.91 & 0.87 & 0.89 & 0.99 & 0.93 & 0.91 & 0.87 & 0.95 & 0.84 & 0.42 & 0.85 \\
Claude-3.5-Sonnet & 0.76 & 0.91 & 0.78 & 0.70 & 0.80 & 0.90 & 0.90 & 0.95 & 0.97 & 0.96 & 0.89 & 0.78 & 0.94 & 0.89 & 0.37 & 0.82 \\
GPT-4o & 0.83 & 0.88 & 0.80 & 0.60 & 0.77 & 0.92 & 0.87 & 0.93 & 0.96 & 0.94 & 0.86 & 0.68 & 0.94 & 0.87 & 0.40 & 0.78 \\
GPT-4-Turbo & 0.75 & 0.82 & 0.70 & 0.42 & 0.81 & 0.89 & 0.83 & 0.92 & 0.95 & 0.93 & 0.83 & 0.62 & 0.89 & 0.83 & 0.38 & 0.74 \\
Llama-3-70B-Instruct & 0.76 & 0.80 & 0.70 & 0.45 & 0.77 & 0.85 & 0.86 & 0.86 & 0.90 & 0.88 & 0.82 & 0.49 & 0.83 & 0.78 & 0.34 & 0.71 \\
Gemma-2-27B-Instruct & 0.67 & 0.76 & 0.70 & 0.33 & 0.73 & 0.80 & 0.82 & 0.90 & 0.82 & 0.86 & 0.76 & 0.49 & 0.76 & 0.79 & 0.38 & 0.67 \\
Mixtral-8x22B-Instruct & 0.52 & 0.71 & 0.59 & 0.27 & 0.71 & 0.75 & 0.79 & 0.85 & 0.73 & 0.80 & 0.68 & 0.44 & 0.76 & 0.70 & 0.35 & 0.64 \\
Deepseek-R1-70B & 0.71 & 0.76 & 0.62 & 0.30 & 0.65 & 0.77 & 0.76 & 0.81 & 0.81 & 0.83 & 0.64 & 0.43 & 0.84 & 0.71 & 0.29 & 0.63 \\
Mixtral-8x7B-Instruct & 0.59 & 0.66 & 0.56 & 0.18 & 0.61 & 0.74 & 0.78 & 0.85 & 0.68 & 0.81 & 0.70 & 0.38 & 0.69 & 0.64 & 0.31 & 0.58 \\
GPT-3.5-Turbo & 0.60 & 0.67 & 0.51 & 0.29 & 0.63 & 0.75 & 0.75 & 0.88 & 0.68 & 0.84 & 0.63 & 0.35 & 0.71 & 0.61 & 0.32 & 0.54 \\
Llama-3-8B-Instruct & 0.55 & 0.67 & 0.50 & 0.10 & 0.64 & 0.66 & 0.80 & 0.87 & 0.71 & 0.77 & 0.66 & 0.35 & 0.66 & 0.57 & 0.33 & 0.52 \\
Mistral-7B-Instruct & 0.48 & 0.52 & 0.42 & 0.16 & 0.53 & 0.60 & 0.69 & 0.84 & 0.49 & 0.71 & 0.56 & 0.21 & 0.49 & 0.45 & 0.26 & 0.46 \\
Gemma-7B-Instruct & 0.36 & 0.42 & 0.29 & 0.06 & 0.53 & 0.45 & 0.52 & 0.72 & 0.41 & 0.58 & 0.39 & 0.07 & 0.27 & 0.35 & 0.20 & 0.32 \\
Llama-2-7B-Chat & 0.15 & 0.29 & 0.20 & 0.10 & 0.27 & 0.30 & 0.23 & 0.52 & 0.16 & 0.42 & 0.26 & 0.07 & 0.11 & 0.19 & 0.13 & 0.20 \\
Salamandra-7B-Instruct & 0.39 & 0.22 & 0.15 & -0.08 & 0.27 & 0.18 & 0.32 & 0.44 & 0.18 & 0.37 & 0.00 & 0.04 & 0.11 & 0.21 & 0.14 & 0.17 \\
Leniachat-Gemma-2B & 0.15 & 0.15 & 0.11 & -0.02 & 0.12 & -0.02 & 0.18 & 0.27 & 0.13 & 0.26 & 0.13 & 0.03 & 0.08 & 0.08 & 0.10 & 0.10 \\
\midrule
ENGLISH-noto &&&&&&&&&&&&\\
\midrule
o3-mini & 0.21 & 0.62 & 0.58 & 0.04 & 0.18 & 0.44 & 0.40 & 0.44 & 0.82 & 0.76 & 0.55 & 0.50 & 0.82 & 0.34 & 0.14 & 0.54 \\
Deepseek-R1-70B & 0.20 & 0.59 & 0.52 & 0.58 & 0.29 & 0.32 & 0.49 & 0.19 & 0.51 & 0.73 & 0.34 & 0.50 & 0.60 & 0.34 & 0.19 & 0.46 \\
GPT-4o & 0.32 & 0.61 & 0.46 & 0.23 & 0.37 & 0.58 & 0.46 & 0.57 & 0.71 & 0.79 & 0.51 & 0.21 & 0.57 & 0.43 & 0.19 & 0.45 \\
GPT-4-Turbo & 0.19 & 0.46 & 0.26 & 0.21 & 0.36 & 0.46 & 0.46 & 0.43 & 0.52 & 0.76 & 0.36 & 0.11 & 0.30 & 0.27 & 0.14 & 0.34 \\
Claude-3.5-Sonnet & -0.12 & 0.30 & 0.12 & -0.12 & 0.11 & 0.21 & 0.18 & 0.21 & 0.45 & 0.52 & 0.19 & 0.07 & 0.24 & 0.10 & -0.01 & 0.20 \\
Gemma-2-27B-Instruct & -0.03 & 0.17 & 0.08 & -0.19 & 0.10 & 0.19 & 0.16 & 0.19 & 0.27 & 0.32 & 0.11 & -0.11 & 0.13 & 0.09 & -0.07 & 0.09 \\
Llama-3-70B-Instruct & -0.13 & 0.17 & 0.07 & -0.13 & -0.01 & 0.14 & 0.03 & 0.11 & 0.42 & 0.38 & 0.11 & -0.09 & 0.37 & 0.01 & -0.01 & 0.06 \\
Mixtral-8x7B-Instruct & -0.12 & 0.03 & -0.02 & 0.03 & 0.05 & 0.12 & 0.16 & 0.17 & 0.23 & 0.46 & 0.00 & -0.12 & 0.05 & -0.05 & -0.07 & 0.05 \\
Llama-2-7B-Chat & -0.12 & 0.02 & -0.01 & 0.10 & 0.22 & 0.14 & 0.27 & 0.07 & -0.06 & 0.26 & -0.05 & -0.08 & 0.21 & 0.06 & 0.06 & 0.05 \\
Llama-3-8B-Instruct & -0.19 & 0.12 & 0.04 & -0.20 & -0.03 & 0.04 & 0.02 & 0.08 & 0.24 & 0.23 & 0.06 & -0.08 & 0.19 & -0.02 & -0.09 & 0.02 \\
Mistral-7B-Instruct & -0.19 & -0.10 & -0.11 & -0.21 & -0.09 & -0.03 & 0.03 & 0.04 & 0.07 & 0.13 & -0.12 & -0.24 & -0.19 & -0.13 & -0.17 & -0.09 \\
Mixtral-8x22B-Instruct & -0.15 & -0.04 & -0.11 & -0.23 & -0.18 & -0.06 & -0.01 & -0.10 & 0.01 & -0.00 & -0.12 & -0.25 & -0.05 & -0.16 & -0.24 & -0.12 \\
Salamandra-7B-Instruct & -0.13 & -0.11 & -0.11 & -0.20 & -0.22 & -0.21 & -0.25 & -0.21 & -0.14 & -0.15 & -0.02 & -0.19 & -0.16 & -0.21 & -0.23 & -0.18 \\
GPT-3.5-Turbo & -0.31 & -0.24 & -0.18 & -0.31 & -0.19 & -0.20 & -0.14 & -0.17 & -0.20 & -0.03 & -0.21 & -0.27 & -0.29 & -0.26 & -0.22 & -0.21 \\
Gemma-7B-Instruct & -0.25 & -0.20 & -0.24 & -0.27 & -0.22 & -0.20 & -0.05 & -0.15 & -0.20 & -0.15 & -0.26 & -0.25 & -0.22 & -0.27 & -0.27 & -0.24 \\
Leniachat-Gemma-2B & -0.27 & -0.32 & -0.31 & -0.28 & -0.31 & -0.32 & -0.32 & -0.32 & -0.32 & -0.30 & -0.30 & -0.31 & -0.16 & -0.31 & -0.29 & -0.29 \\
\midrule
SPANISH-original  &&&&&&&&&&&&\\
\midrule
o3-mini & 0.75 & 0.80 & 0.80 & 0.52 & 0.76 & 0.88 & 0.83 & 0.88 & 0.97 & 0.93 & 0.84 & 0.79 & 0.94 & 0.80 & 0.38 & 0.82 \\
Claude-3.5-Sonnet & 0.79 & 0.86 & 0.74 & 0.66 & 0.76 & 0.90 & 0.88 & 0.94 & 0.95 & 0.94 & 0.87 & 0.71 & 0.93 & 0.86 & 0.43 & 0.80 \\
GPT-4o & 0.75 & 0.83 & 0.76 & 0.46 & 0.70 & 0.89 & 0.84 & 0.93 & 0.93 & 0.94 & 0.83 & 0.63 & 0.92 & 0.85 & 0.37 & 0.76 \\
GPT-4-Turbo & 0.61 & 0.79 & 0.69 & 0.31 & 0.79 & 0.84 & 0.83 & 0.90 & 0.90 & 0.90 & 0.76 & 0.50 & 0.86 & 0.80 & 0.35 & 0.70 \\
Llama-3-70B-Instruct & 0.71 & 0.75 & 0.62 & 0.38 & 0.69 & 0.79 & 0.75 & 0.85 & 0.78 & 0.84 & 0.71 & 0.37 & 0.78 & 0.67 & 0.32 & 0.65 \\
Gemma-2-27B-Instruct & 0.60 & 0.69 & 0.65 & 0.31 & 0.69 & 0.76 & 0.76 & 0.88 & 0.77 & 0.83 & 0.72 & 0.36 & 0.68 & 0.69 & 0.26 & 0.63 \\
Deepseek-R1-70B & 0.72 & 0.67 & 0.60 & 0.35 & 0.66 & 0.80 & 0.73 & 0.82 & 0.78 & 0.82 & 0.71 & 0.34 & 0.75 & 0.69 & 0.30 & 0.63 \\
Mixtral-8x22B-Instruct & 0.55 & 0.64 & 0.58 & 0.16 & 0.59 & 0.72 & 0.71 & 0.84 & 0.62 & 0.75 & 0.65 & 0.35 & 0.64 & 0.60 & 0.29 & 0.56 \\
Mixtral-8x7B-Instruct & 0.60 & 0.57 & 0.51 & 0.15 & 0.53 & 0.67 & 0.66 & 0.75 & 0.64 & 0.78 & 0.64 & 0.25 & 0.61 & 0.54 & 0.28 & 0.52 \\
GPT-3.5-Turbo & 0.47 & 0.57 & 0.47 & 0.18 & 0.57 & 0.65 & 0.62 & 0.78 & 0.57 & 0.78 & 0.52 & 0.22 & 0.53 & 0.51 & 0.28 & 0.46 \\
Llama-3-8B-Instruct & 0.49 & 0.47 & 0.39 & -0.01 & 0.50 & 0.56 & 0.65 & 0.68 & 0.48 & 0.65 & 0.49 & 0.11 & 0.46 & 0.47 & 0.24 & 0.41 \\
Mistral-7B-Instruct & 0.43 & 0.39 & 0.27 & 0.12 & 0.43 & 0.44 & 0.53 & 0.72 & 0.35 & 0.60 & 0.41 & 0.19 & 0.29 & 0.34 & 0.19 & 0.35 \\
Gemma-7B-Instruct & 0.27 & 0.30 & 0.22 & 0.11 & 0.43 & 0.38 & 0.45 & 0.63 & 0.29 & 0.47 & 0.29 & 0.07 & 0.12 & 0.24 & 0.15 & 0.26 \\
Llama-2-7B-Chat & 0.16 & 0.19 & 0.15 & 0.19 & 0.26 & 0.14 & 0.18 & 0.45 & 0.16 & 0.38 & 0.21 & 0.08 & 0.05 & 0.18 & 0.23 & 0.17 \\
Salamandra-7B-Instruct & 0.29 & 0.16 & 0.15 & -0.05 & 0.21 & 0.24 & 0.22 & 0.41 & 0.11 & 0.36 & 0.17 & 0.02 & 0.13 & 0.20 & 0.08 & 0.14 \\
Leniachat-Gemma-2B & 0.00 & 0.01 & 0.03 & 0.03 & 0.04 & 0.02 & 0.16 & 0.15 & 0.09 & 0.14 & 0.03 & 0.03 & 0.10 & 0.09 & 0.08 & 0.07 \\
\midrule
SPANISH-noto &&&&&&&&&&&&\\
\midrule
o3-mini & 0.15 & 0.53 & 0.51 & -0.01 & 0.07 & 0.41 & 0.20 & 0.33 & 0.77 & 0.72 & 0.44 & 0.40 & 0.70 & 0.25 & 0.14 & 0.47 \\
GPT-4o & 0.29 & 0.54 & 0.37 & 0.34 & 0.32 & 0.51 & 0.42 & 0.48 & 0.66 & 0.76 & 0.45 & 0.09 & 0.55 & 0.35 & 0.18 & 0.41 \\
GPT-4-Turbo & 0.23 & 0.43 & 0.30 & 0.35 & 0.46 & 0.41 & 0.49 & 0.43 & 0.48 & 0.75 & 0.33 & 0.16 & 0.37 & 0.28 & 0.21 & 0.37 \\
Deepseek-R1-70B & -0.05 & 0.35 & 0.29 & 0.31 & 0.11 & 0.32 & 0.19 & 0.15 & 0.34 & 0.53 & 0.24 & 0.25 & 0.22 & 0.03 & 0.07 & 0.25 \\
Claude-3.5-Sonnet & -0.01 & 0.31 & 0.18 & -0.09 & 0.12 & 0.34 & 0.22 & 0.24 & 0.48 & 0.61 & 0.26 & 0.05 & 0.33 & 0.18 & 0.06 & 0.24 \\
Gemma-2-27B-Instruct & -0.12 & 0.13 & 0.00 & -0.20 & 0.02 & 0.16 & 0.18 & 0.11 & 0.12 & 0.22 & 0.03 & -0.13 & 0.04 & 0.02 & -0.08 & 0.04 \\
Llama-3-70B-Instruct & -0.15 & 0.09 & 0.00 & -0.16 & -0.04 & 0.03 & 0.10 & 0.10 & 0.24 & 0.24 & 0.03 & -0.17 & 0.06 & -0.08 & -0.09 & 0.00 \\
Llama-3-8B-Instruct & -0.28 & 0.01 & -0.15 & -0.14 & -0.08 & 0.03 & -0.01 & 0.06 & -0.03 & 0.17 & -0.06 & -0.12 & -0.04 & -0.07 & -0.10 & -0.03 \\
Mixtral-8x7B-Instruct & -0.15 & -0.10 & -0.11 & 0.07 & -0.09 & -0.07 & 0.15 & 0.04 & 0.02 & 0.24 & -0.11 & -0.16 & -0.09 & -0.11 & -0.14 & -0.04 \\
Llama-2-7B-Chat & -0.16 & -0.07 & -0.16 & 0.08 & 0.02 & 0.03 & 0.14 & 0.02 & -0.15 & 0.15 & -0.01 & -0.07 & 0.07 & -0.11 & 0.00 & -0.05 \\
Mistral-7B-Instruct & -0.21 & -0.11 & -0.15 & -0.23 & -0.08 & -0.05 & 0.03 & 0.05 & 0.01 & 0.09 & -0.10 & -0.18 & -0.09 & -0.13 & -0.16 & -0.07 \\
Mixtral-8x22B-Instruct & -0.20 & -0.04 & -0.07 & -0.25 & -0.17 & -0.16 & -0.03 & -0.09 & -0.07 & 0.06 & -0.13 & -0.23 & -0.11 & -0.16 & -0.21 & -0.12 \\
Salamandra-7B-Instruct & -0.17 & -0.10 & -0.16 & -0.19 & -0.19 & -0.19 & -0.11 & -0.11 & -0.15 & -0.15 & -0.20 & -0.19 & -0.09 & -0.18 & -0.20 & -0.15 \\
GPT-3.5-Turbo & -0.27 & -0.21 & -0.22 & -0.25 & -0.19 & -0.27 & -0.18 & -0.19 & -0.21 & -0.11 & -0.26 & -0.29 & -0.25 & -0.27 & -0.27 & -0.23 \\
Gemma-7B-Instruct & -0.23 & -0.16 & -0.22 & -0.29 & -0.21 & -0.19 & -0.03 & -0.21 & -0.22 & -0.15 & -0.24 & -0.25 & -0.21 & -0.26 & -0.25 & -0.23 \\
Leniachat-Gemma-2B & -0.33 & -0.30 & -0.30 & -0.32 & -0.30 & -0.30 & -0.33 & -0.29 & -0.29 & -0.29 & -0.31 & -0.28 & -0.27 & -0.28 & -0.32 & -0.30 \\
\bottomrule
\end{tabular}
}

\caption{Cohen's Kappa results on \textbf{MMLU} (\textbf{\textcolor{blue}{Other categories}}) by model and subject in English and Spanish, sorted by average.}
\label{tablammlu4}
\end{table*}