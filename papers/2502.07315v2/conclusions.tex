\section{Conclusions}
\label{sec:conclusions}
We presented novel methods for document
modifications: modifying a document so it will be highly ranked by an undisclosed ranking function for a query. Our methods are based on prompting large language models (LLMs). Some of our methods are inspired by prompting approaches for inducing LLM-based ranking over documents.

We conducted extensive empirical evaluation using past ranking
competitions (for two different rankers). In addition, we organized ranking competitions where our document modification methods competed as bots against students.

The empirical evaluation demonstrated the merits of our best
performing modification methods with respect to students and a
previously proposed highly effective feature-based document
modification approach.

\endinput



We present the primary results of a prompt-driven method for modifying documents in ranking competitions. Previous work \cite{goren2020ranking} introduced a method that promoted ranking by replacing passages in the original document, while maintaining a high level of document quality and faithfulness to the original. We propose a novel approach leveraging a prompt-driven method that incorporates past documents and their rankings into LLM agents. Our results demonstrate that the Pairwise and Listwise \bt s achieve improved ranking promotion compared to the prior method, as well as other participants (students or static documents). Additionally, in some cases, our method achieved higher levels of document faithfulness and quality. During the evaluation process, we introduced some novel metrics for faithfulness.
