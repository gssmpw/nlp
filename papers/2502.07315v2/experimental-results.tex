\section{Experimental Results}
\label{sec:experimental-results}

\begin{table}[t]
% \centering
  \caption{\label{table_offline2017} Evaluation over the \firstDataset
    dataset which resulted from competitions using a LambdaMART
    ranker \cite{raifer2017information}. The baseline (\sentReplace \cite{goren2020ranking}), Pairwise and Listwise blocks correspond to a
    round where a document was modified by the respective bot, and all other documents were of
    students. Boldface marks the best result in a column.
    Statistically significant differences of our bots with \sentReplace and the students are marked with '$\baseDiff$' and '$\studDiff$', respectively.}
    \scriptsize
%\hspace{-15mm}
%\input{Tables/offline2017}
\input{Tables/offline2017,v2}
\end{table}

Table \ref{table_offline2017} presents the evaluation over the
\firstDataset. We let one of our bots, or the baseline
bot, \sentReplace \cite{goren2020ranking}, modify a single document at a time. We then
analyze the resultant ranking over the single modified document and
the next-round students' documents from the dataset. The performance numbers are averages over the different selected documents to be modified (all but the highest ranked one) and over queries. Note that the Students' faithfulness values are not block dependent as they are the same in all three cases in the next round. Their Scaled Promotion is affected by the bot used to modify the selected document.

Table \ref{table_offline2017} shows that our Pairwise and Listwise
bots outperform \sentReplace and the students in terms of Scaled
Promotion. The faithfulness of both of our bots is lower than that of
the students and \sentReplace but is still quite high. Note that \sentReplace modifies a single sentence
in a document and, hence, the modified document is quite faithful to the original
document. The corpus faithfulness of both our bots (for both TF.IDF and E5) is higher than that of \sentReplace. Thus, although our bots use an LLM (as opposed to \sentReplace and the students), their corpus faithfulness is relatively high.

Table \ref{table_offline2024} presents the evaluation for the
\secondDataset dataset\footnote{We do not use here and after the
  \sentReplace baseline as it was defined for sparse ranking
  functions.}. The results presented above for \firstDataset show that the bots were not better than the students in terms of corpus faithfulness. Over the \secondDataset, our bots outperform the students with respect to
Scaled Promotion and corpus faithfulness and underperform them in
terms of faithfulness to the original document, although this faithfulness is still relatively high. All in all, Tables
\ref{table_offline2017} and \ref{table_offline2024} attest to the
effectiveness of our bots for both ranking functions: LambdaMART and
E5.

Table \ref{table_online2024} presents the evaluation for the
online competitions (our bots vs. students) which used the E5 ranker.
% We use for reference a
% Static bot which uses the example document provided to students at the
% beginning of the competition and does not modify it along
% rounds; hence, its \normFaith is $1$.
Q and R are the average
quality and relevance grades, respectively, over documents, queries and the three
rounds ($5$-$7$). We see in Table \ref{table_online2024} that our bots
outperform the students in terms of Scaled Promotion, \normFaith and
corpus faithfulness (except for a single case). The documents produced by our Listwise bots are of higher
quality on average than those produced by students in the same
competitions. For the competitions with our Pairwise bot, the
students produced documents of higher quality on average than those of the bot; yet, the bot
 produced in a majority of cases ($0.822$) quality documents. Finally, our bots produce
documents that are relevant in more cases than those produced by the
students.

All in all, the findings presented above attest to the clear merits of our
bots with respect to students and the \sentReplace baseline along various dimensions; specifically, rank promotion, document quality and relevance, and faithfulness to the corpus.

% As discussed in \ref{sec:experimental-setup}, the offline experiments were conducted to evaluate the effectiveness of our ranking-incentivized modification approach. The general framework for these experiments followed the methodology described in Goren et al. \cite{goren2020ranking}.

% In each setup, \cd s were selected from earlier rounds as the input, while the subsequent rounds served as benchmarks for evaluation. We limited our analysis to \cd s ranked between the second and last positions across a predefined set of queries, ensuring consistency and comparability between settings.
\omt{
We now turn to describe the offline and online evaluation results. The
two best performing agents were The Pairwise\footnote{Using two random
  documents over three past rounds. Parameters: Number of queries=1,
  Number of batches=3, Inclusion of the current rank=False, inclusion
  of the current query=True, Example type=random.} with temperature of
0.5 and the Listwise\footnote{Using a ranked list over two past
  rounds. Parameters: Number of queries=1, Number of batches=2,
  Inclusion of the current rank=False, Inclusion of the current
  query=True.} with temperature of 0. The full prompts are presented
in Appendix \ref{appendix_prompt}, Figures \ref{prompt_listwise} and
\ref{prompt_pairwise}. Those configurations achieved the highest
scaled promotion in both Dataset 1 (round 7) and Dataset 2 (round
4). The results\footnote{The statistical significance of all the
  results was evaluated using a two-tailed paired permutation test
  with 100,000 random permutations, adhering to a 95\% confidence
  level.} are presented in Tables \ref{table_offline2017} and
\ref{table_offline2024}. Starting with the offline evaluation of
Dataset 1, our Pairwise and Listwise prompts achieved scaled promotion
scores of 0.345 and 0.315, respectively. These values are slightly
higher than that of the baseline (0.309), but significantly higher
than the students (-0.095 and -0.09). The faithfulness measure,
specifically the NCF@1 scores, were higher for the baseline and the
students compared to our \bt s. However, the NEF@10 scores of our \bt
s exceeded those of the baseline, though they were lower when compared
to the students. On Dataset 2, The Pairwise and Listwise \bt s
demonstrated superior performance in the scaled promotion measure
compared to the students (0.241 vs. -0.107 and 0.341 vs. -0.128,
respectively). The NCF@1 scores for the students were higher than
those for our \bt s, reflecting the same trend observed in Dataset
1. However, unlike Dataset 1, our \bt s achieved higher environmental
consistency (NEF@10) than the students. Table \ref{table_online2024}
presents the results of the online evaluation (Dataset 3), aggregated
over rounds 5, 6 and 7 (i.e., The rounds during which our \bt s were
active in the competition). The Pairwise and Listwise \bt s achieved
slightly higher scaled promotion scores compared to the students and
the static documents. Interestingly, and in contrast to the offline
results, the faithfulness measures were higher for our \bt s than for
students across almost all evaluation measures. This suggests that our
\bt s not only improved the ranking, but did so with greater
consistency with relative to the current document. Furthermore,
documents generated by the Listwise \bt{} exhibited higher relevance
and quality scores compared to those of the students and static
documents. Conversely, while the Pairwise \bt{} produced documents
with higher relevance scores, their quality scores were lower relative
to the students and static documents.
}
% The following subsections detail the specific configurations and results for the feature-based and transformer-based settings.

% \subsubsection{Feature-Based Setting}
% \input{Tables/offline_LTR_results}

% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline_LTR/average rank.jpeg}
%         \caption{Rank}
%         \label{fig:rank_feature}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline_LTR/raw promotion.jpeg}
%         \caption{Rank Promotion}
%         \label{fig:rank_promotion_feature}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline_LTR/scaled promotion.jpeg}
%         \caption{Scaled Rank Promotion}
%         \label{fig:scaled_rank_promotion_feature}
%     \end{subfigure}
%     \caption{Evaluation measures for rank promotion in the feature-based setting.}
%     \label{fig:rank_promotion_feature_based}
% \end{figure*}


% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline_LTR/NCF@1.jpeg}
%         \caption{NCF@1}
%         \label{fig:NFC@1_feature}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline_LTR/NEF@10_sparse.jpeg}
%         \caption{NEF@10 sparse}
%         \label{fig:NEF@10_sparse_feature}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline_LTR/NEF@10_dense.jpeg}
%         \caption{NEF@10 dense}
%         \label{fig:NEF@10_dense_feature}
%     \end{subfigure}
%     \caption{Evaluation measures for faithfulness in the feature-based setting.}
%     \label{fig:rank_promotion_feature_based}
% \end{figure*}

% Evaluation was conducted over 124 experimental settings derived from Round 7, encompassing documents ranked 2–5 across 31 queries. As shown in Figures \ref{fig:rank_feature}–\ref{fig:scaled_rank_promotion_feature}, the "Rank Promotion" measures highlight the performance improvements achieved. Table \ref{table:offline_feature} presents the detailed results, including statistical significance. 



% \subsubsection{Transformer-Based Setting}

% \input{Tables/offline_results}

% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline/average rank.jpeg}
%         \caption{Rank}
%         \label{fig:rank_transformer}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline/raw promotion.jpeg}
%         \caption{Rank Promotion}
%         \label{fig:rank_promotion_transformer}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline/scaled promotion.jpeg}
%         \caption{Scaled Rank Promotion}
%         \label{fig:scaled_rank_promotion_transformer}
%     \end{subfigure}
%     \caption{Evaluation measures for rank promotion in the transformer-based setting.}
%     \label{fig:rank_promotion_transformer_based}
% \end{figure*}

% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline/NCF@1.jpeg}
%         \caption{NCF@1}
%         \label{fig:NCF@1_transformer}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline/NEF@10_sparse.jpeg}
%         \caption{NEF@10 sparse}
%         \label{fig:EF@10_sparse_transformer}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{Images and Prompts/Offline/NEF@10_dense.jpeg}
%         \caption{NEF@10 dense}
%         \label{fig:NEF@10_dense_transformer}
%     \end{subfigure}
%     \caption{Evaluation measures for faithfulness in the transformer-based setting.}
%     \label{fig:rank_promotion_transformer_based}
% \end{figure*}


% The transformer-based setting leveraged the E5 ranker introduced by Wang et al. \cite{wang2022e5}. Candidate documents were sourced from Round 5, while Round 6 provided the benchmark for evaluation. This configuration was applied to a subset of 15 queries from the dataset. Figures \ref{fig:rank_transformer}–\ref{fig:scaled_rank_promotion_transformer} illustrate the performance across various rank promotion measures, and Table \ref{table:offline_transformer} summarizes the results with statistical significance.

% An analysis of the results reveals the competitive performance of the transformer-based approach compared to the feature-based setting, with notable improvements in scaled rank promotion and faithfulness metrics.




\begin{table}[t]
% \centering
\caption{\label{table_offline2024} Evaluation over the \secondDataset dataset which resulted from
  competitions using an E5 ranker. The Pairwise and Listwise blocks correspond to a round where a document was modified by the respective bot and all other documents were of students.
    Boldface: the best result in a column.
    Statistically significant differences of our bots with the students are marked with '$\studDiff$'.}
%  The Pairwise and Listwise blocks correspond to a round where a document was modified by the Pairwise and Listwise approach, respectively, and all other documents were modified by students.
%    Boldface marks the best result in a column.
%    Statistically significant differences of our bots with the students and the baseline are marked with '$\studDiff$' and '$\baseDiff$', respectively.}
\scriptsize
%\hspace{-15mm}
%\input{Tables/offline2024}
\input{Tables/offline2024,v2}
\end{table}


\setlength{\tabcolsep}{2pt}
\begin{table}[t]
  % \centering
  \caption{\label{table_online2024} Online evaluation. Q and R are the
    average quality and relevance grades, respectively. The Pairwise
    and Listwise blocks correspond to competitions where one of the
    two bots participated. Boldface marks the best result in a
    column. Statistically significant differences of our bots with the students are marked with '$\studDiff$'.}
  %\input{Tables/online2024}
  \scriptsize
\input{Tables/online2024,v2}
\end{table}


