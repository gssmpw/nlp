\subsection{Evaluation Measures}
\label{sec:eval-measures}
We analyzed the performance of a document modification method using various evaluation measures, categorized into three primary groups: ranking properties, faithfulness properties and Quality and Relevance properties. All measures were computed per player and her document for a given query. The results were averaged over queries and grouped by the player type (student, baseline\footnote{i.e. the method of replacing paragraphs \cite{goren2020ranking}.}, a static document or one of the \bt s). For the online evaluation, the measures were also averaged over rounds.

\myparagraph{Ranking promotion} This category includes measures that quantify the level of ranking promotion achieved by a modified document compared to its current version of document. We adopt the metrics introduced by Goren et al. \cite{goren2020ranking}: (i) \textbf{Average rank}: The mean rank of a player; (ii) \textbf{Raw promotion}: The change in a document's rank between consecutive rounds; (iii) \textbf{Scaled promotion}: The raw promotion normalized by the maximum possible promotion or demotion based on the current document's position. In our evaluation, we focus only on the scaled promotion metric.

\myparagraph{Faithfulness} The second category includes measures aim to estimate the extent to which an agent has changed her document (the modified documents is denoted as $\dn$) with respect to the current document (denoted as $\dc$) or the current set of documents. A sentence within a document $d$ is referred to as $d_i$. Given that our prompt-driven method might modify a document more extensively than the baseline approach, which generally replaces only a single sentence, we tackle the challenge of comparing two texts by assessing whether one (denoted as hypothesis) is derived from the other (denoted as premise) while preserving factual consistency. We use the method proposed by Gekhman et al. \cite{gekhman2023trueteacher}, which estimates faithfulness through the \trueteacher{} (TrueTeacher) metric. Using the notation \trueteacher $(premise, hypothesis)$ the output of the model provides a score within the range [0,1], where higher scores indicate stronger factual alignment. We propose a set of measures to evaluate faithfulness: (i) The Candidate Faithfulness $CF@1(\dc, \dn)$: the average \trueteacher{} score between the current document ($\dc$) to all sentences in the modified document ($\dn$): $\frac{1}{n} \sum_{i=1}^n \trueteacher (\dc, \dni)$; (ii) The Normalized Candidate Faithfulness $NCF@1(\dc, \dn)$: the normalization of the Candidate Faithfulness $CF@1(\dc, \dn)$ by the self-consistency score: $\frac{CF@1 (\dc, \dn)}{CF@1(\dc, \dc)}$; (iii) Environmental Faithfulness at 10 $EF@10(\dn)$: This metric measures how much the generated document ($\dn$) maintains contextual consistency with the broader corpus. Specifically, it measures the similarity of $\dn$ to the top 10 documents most similar to it in the corpus.
The corpus includes all the documents (across all queries) available up to the test round. Two approaches are employed to compute the similarity. The first approach is based on the (unsupervised) E5 \cite{wang2022text} representation with the cosine similarity metric. The second approach is based on the TF.IDF \cite{sparck1972statistical, salton1975vector} representation with the cosine similarity metric. This metric is then calculated as follows: $\frac{1}{2*10} \sum_{i=1}^{10} (\trueteacher(\dn, d_{top_{i}}) + \trueteacher(d_{top_i}, \dn))$. Where $d_{top_{i}}$ represents the $i$-th document, while ordering the documents with respect to the similarity to $\dc$. The two approaches yield two variants of this metric: $EF@10$\_dense and $EF@10$\_sparse, for the E5 and TF.IDF representations, respectively; (iv) The Normalized Environmental Faithfulness at 10 $NEF@10(\dn)$: The normalization of EF@10 by the EF@10 of the current document: $\frac{EF@10(\dn)}{EF@10(\dc)}$. These measures collectively provide a comprehensive framework for assessing faithfulness. They evaluate the consistency of the modified document not only with respect to the current document but also in relation to other documents in the corpus.

\myparagraph{Relevance and Quality scores} The third category of evaluation measures focuses on the relevance and quality of documents. Both quality and relevance scores are assigned by crowdsourcing annotators via the Connect platform on CloudResearch \cite{noauthor_introducing_2024}, assessing the document's content quality and its relevance to the query\footnote{These evaluations of relevance and quality are conducted exclusively in the online evaluation setting.}. A document's quality or relevance score is set to 1 if at least three out of five English-speaking annotators marked it as valid or relevant to the query; otherwise, the score is set to 0. We report the ratio of documents that received a quality or relevance score of 1.
% Adopting a "white hat ranking-incentivized modifications" approach through the lens of a \bt brings several critical evaluation aspects into focus. We categorize our evaluation measures into two primary groups. The first category emphasizes adherence to "white hat" principles, which prioritize a high-quality user experience. This involves generating content that is readable, engaging, and integrates keywords naturally without excessive repetition. By following these standards, we ensure that the content remains accurate, relevant to user queries, and focused on the subject matter, thus avoiding any fabricated information or "hallucinations". We will refer to this category as "\textbf{Faithfulness}," as it encompasses all measures designed to remain true to the \cd, the corpus, the query, and the underlying information need.

% The second category, which we term "\textbf{Rank Promotion}", evaluates the success of raising a text's rank. In our case, we employ the E5 ranker \cite{wang2022text}, a black-box ranking system, to evaluate the document’s search position, aiming to secure the highest possible ranking, ideally reaching the top position.

% To assess these two dimensions, we aligned our evaluation with prior work, specifically the methodology of Goren et al. \cite{goren2020ranking}. Their approach primarily focuses on "Rank Promotion" metrics as well as human-annotated assessments under the "Faithfulness" category, specifically evaluating quality and relevance. However, since our study introduces an LLM based approach that diverges from the LTR feature based baseline, we recognize that some assumptions do not apply directly to our work. For instance, our model generates a completely new document, closely tailored to the \cd, as opposed to making a single sentence change as Goren et al. \cite{goren2020ranking} did, and it may also exhibit creative differences that set it apart from existing corpus documents. 

% To address these methodological differences, we propose two novel evaluation approaches that capture the unique effects of our LLM based approach - \textbf{candidate faithfulness} and \textbf{environmental faithfulness}. These approaches are translated into evaluation metrics that are designed to account for the innovative aspects of our bt{} s and they are added to those of Goren et al. \cite{goren2020ranking}. The complete list of evaluation measures, organized by group, is presented below.

% \subsection*{Faithfulness}
% \subsubsection*{\textbf{Novel NLI-Based Evaluation Measures}}

% We address the challenge of comparing two texts on a deeper by the measure of wether one text is derived from the other while preserving factual consistency, as described by Gekhman et al. \cite{gekhman2023trueteacher}. In this context, we use the notation \trueteacher $(premise, hypothesis)$ to represent the output of the TrueTeacher model, where the result is a score within the range [0,1]. This score indicates the likelihood that the "hypothesis" (the modified document) is factually consistent with the "premise" (the \cd). By incorporating TrueTeacher, we developed several novel, faithfulness-oriented evaluation measures:
% \begin{itemize} 
%     \item \textbf{CF@1 - Candidate Faithfulness at 1}: This metric assesses the faithfulness of each individual sentence in the generated document relative to the \cd. For each sentence in the generated text, we compute \trueteacher (\dc, \dni). Where \dc{} is the \cd{}, \dn{} is the generated document and \dni is sentence $i$ in the generated document. These annotations closely replicate Goren et al.'s \cite{goren2020ranking} annotations. The CF@1 score for the entire document is obtained by averaging these sentence-level scores, giving us: $CF@1$ (\dc, \dn) $= \frac{1}{n} \sum_{i=1}^n$ \trueteacher (\dc, \dni). 
    
%     \item \textbf{NCF@1 - Normalized Candidate Faithfulness at 1}: This metric is the samce as CF@1 but normalizing it against a baseline faithfulness score. Specifically, we divide the CF@1 score of the generated document by the CF@1 score of the \cd with itself. Its calculation is as follows: 
%     $NCF@1$ (\dc, \dn) =  $CF@1$ (\dc, \dn) /  $CF@1$ (\dc, \dc). Normalizing CF@1 in this manner ensures that scores are scaled according to an internal reference, allowing us to gauge how close the generated content remains to the original.

%     \item \textbf{EF@10 - Environmental Faithfulness at 10}: This measure ensures that the generated document maintains contextual relevance, preventing it from “hallucinating” or introducing content that is not grounded in existing texts in the corpus. The motivation for this measure is to show how close is the generated document to the top 10 documents closest to it in the vector space. To calculate EF@10, we assess the generated document against the top 10 documents deemed most relevant by search engine algorithms (detailed below) from the corpus consisting of all documents up to the test round, not included, across all queries and competitors. This relevance is determined through two indexing approaches:

%     \begin{itemize}
%         \item A \textit{dense index} created using Faiss \cite{douze2024faiss}, embedding the texts based on the E5 \cite{wang2022text} model for both similarity measurement and ranking relevance.
%         \item A \textit{sparse index} built with TFIDF \cite{sparck1972statistical, salton1975vector} embeddings using Lucene \cite{lucene} for relevance scoring.
%     \end{itemize}

%     For each indexing method, we calculate a symmetric TrueTeacher (\trueteacher{}) score between \dn and each of the top 10 retrieved documents. The EF@10 score is computed as follows: $EF@10$ (\dn) $= \frac{1}{2*10} \sum_{i=1}^{10}$ (\trueteacher(\dn, $d_{top_{i}}$) + \trueteacher($d_{top_i}$, \dn)). Where $d_{top_{i}}$ is the document retrieved in the $i$ position. This provides a measure of how closely the generated text aligns with contextually relevant content in the corpus, separate scores noted as EF@10\_dense and EF@10\_sparse, respectively.

%     \item \textbf{NEF@10 - Normalized Environmental Faithfulness at 10}: Analogous to NCF@1, this measure normalizes the EF@10 score by dividing it by the EF@10 score of the candidate document. This yields a score that contextualizes the environmental faithfulness of the generated document relative to the \cd.
% \end{itemize}

% These measures collectively provide a comprehensive evaluation framework, assessing both the factual consistency and contextual relevance of the generated document in relation to the original and to topically relevant documents in the corpus.

% \subsubsection*{\textbf{Human Annotated Evaluation Measures}}
% Following on Goren et al.'s \cite{goren2020ranking} work we gathered human annotations for both 'quality' and 'relevance'. We gathered these annotations through a crowd-sourcing tool named Research Cloud's "Connect". This platform facilitated the gathering of diverse human judgments across a wide range of documents. To ensure consistency and comparability, we employed the same task template designed by Goren et al. \cite{goren2020ranking}. By leveraging this template, we aimed to produce results that aligned with the previous study.

% Each document in the dataset was independently annotated for both \textbf{quality} and \textbf{relevance} by a group of three annotators. These annotators provided their individual judgments, and a majority voting system was used to decide whether a document met the criteria for relevance and quality. This approach of utilizing multiple annotators and majority voting serves to mitigate individual biases, enhancing the reliability of the evaluation results.

% \begin{itemize}
%     \item \textbf{Quality} - The quality of each document was assessed on a binary scale, where an annotator rated the document as \textit{1} if they considered it to possess high-quality content and \textit{0} otherwise. Quality in this context was defined as the document's adherence to standards such as coherence, informativeness, and clarity of presentation. To derive an aggregate measure of quality for each agent, these binary scores were averaged across all documents, resulting in a single, representative score per agent. This metric provides insight into the overall quality standard upheld by each \bt{}, allowing us to compare and contrast the qualitative aspects of document generation across \bt{} s.

%     \item \textbf{Relevance} - For relevance assessments, annotators were provided with the title and detailed description of each TREC \cite{} topic. They were instructed to classify a document as relevant if it directly addressed the information need specified in the TREC topic description. This criterion of relevance thus captures the document's utility in meeting a specific query’s information need. After all annotations were collected, a majority vote was taken for each document to determine its relevance status. These relevance judgments were then averaged per \bt{}, yielding a single relevance score for each \bt{}.
% \end{itemize}


% \subsection*{Rank Promotion}
% These evaluation measures were taken directly from Goren et al's \cite{goren2020ranking} study, measuring the success rate in which a model raises the rank of a document.

% \begin{itemize}
%     \item \textbf{Average Rank} - The average rank is calculated as the mean position of each document (\dc) within the overall ranking system. In this setup, a lower rank value signifies a higher position, with a rank of 1 indicating the best position and a rank of 4 representing the worst position in the considered range. This metric provides a straightforward, interpretable measure of a document's relative standing, offering insight into how well the document is performing on average compared to others.

%     \item \textbf{Rank Promotion} - This measure captures the exact number of positions a document has moved within the ranking list, either upward or downward. It quantifies the change in a document’s rank by calculating the difference between its initial and final positions after applying the ranking method on the newly created documents both by the \bt s and the students. A positive value will be considered a positive rank promotion, e.g. RP will be equal to 1 if the original position was 2 and the new position is 1. This measure can be any integer value $\in [-3,3]$.

%     \item \textbf{Scaled Rank Promotion} - Scaled Rank Promotion builds upon the rank promotion measure by normalizing the observed rank change relative to the maximum possible improvement achievable from a document’s initial position. This normalization yields a scaled score, allowing for a more meaningful interpretation of the rank movement by accounting for the initial position's constraints on potential rank shifts. For example, a document starting at a higher rank (closer to the top) may have limited scope for improvement, making even a small upward movement particularly significant. Conversely, a document starting lower in the ranking has greater flexibility to move upward, making similar shifts less impactful. The SRP measure ranges from -1 to 1, where an SRP of 1 indicates the document has moved to the top rank from any lower position, such as a change from ranked 2nd to 1st or from 4th to 1st. This measure is symmetric, allowing for negative values as well, which indicate a downward shift in rank. Negative SRP values provide insights into documents that have lost ground in the rankings, making SRP an effective metric for capturing both positive and negative rank shifts with respect to each document’s starting position.

% \end{itemize}

