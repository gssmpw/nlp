\section{Introduction}
\label{sec:intro}
In competitive search settings \cite{Kurland+Tennnholtz:22a}, document
authors might respond to rankings induced for queries of interest by
modifying their documents. The goal is to improve
future ranking. These modifications are often referred to as search
engine optimization (SEO) \cite{Gyongyi+Molina:05a}. Our focus, as
that of prior work on competitive search
\cite{raifer2017information,goren2020ranking,Kurland+Tennnholtz:22a}, is on white hat SEO \cite{Gyongyi+Molina:05a}:
legitimate document modifications which do not hurt document quality
nor the search ecosystem.

We present novel document modification methods for ad hoc retrieval that are based on prompting large language models (LLMs). The desiderata of the modifications are (cf.,
\cite{goren2020ranking}): (i) potentially improving future ranking for a query at hand, (ii)
maintaining ``faithfulness'' to the original document, and (iii) producing a document of high (content) quality. Some of our methods are inspired by recent work on using
prompting techniques to have LLMs induce rankings in response to
queries \cite{Liang+al:22a,qin2023large,ma2023zero}. For example, some of the
prompts include information induced from past rankings to provide the
LLM with implicit information about the undisclosed ranking function.

We evaluate the document modification methods using datasets that
resulted from past ranking competitions
\cite{raifer2017information,Mordo+al:25a}. In addition, we organized
ranking competitions and used our modification methods as bots
competing against humans\footnote{The dataset of the competition we
  organized, and the accompanying code, will be made public upon
  publication of this paper. They are available for reviewing purposes
  at \url{https://github.com/promptdriven2025}.}. The empirical
evaluation demonstrates the merits of the most effective methods we
studied with respect to humans and to a highly effective feature-based
approach for document modification
\cite{goren2020ranking}.

  \myparagraph{Related work} We refer the reader to \cite{Kurland+Tennnholtz:22a} for a survey on competitive search. The work most related to ours is that of
  Goren et al. \cite{goren2020ranking} who devised a supervised
  feature-based document modification method. We discuss the method in Section \ref{sec:experimental-setup} and use it as a baseline. 

There is recent work on performing SEO for Web pages by prompting LLMs
\cite{Aggarwal+al:24a}. The prompts are designed for commercial search
engines such as Google and are not well suited to the
general ranking functions we use for evaluation. In addition, the
prompts, in contrast to ours, do not include examples of past rankings
and are mainly zero shot.
  
There is work on attacks (not necessarily white hat) on
BERT-based document rankers (e.g., \cite{Wu+al:23a}) and on LLM-based document and product rankers, conversational agents and question answering systems
(e.g., \cite{Kumar+Lakkaraju:24a,Pfrommer+al:24a,Nestaas+al:24a}). Our focus, in contrast, is on white hat content modification for document ranking which is not committed to a specific ranking function. We focus on dense and sparse retrieval in our evaluation.

\omt{
  The method selects a
sentence from the document and replaces it with a sentence from a
document which was the highest ranked for the query in the past. The
features used by the method include estimates for query term occurrence (to improve ranking) and estimates of the semantic similarity with
 previous and following sentences to maintain textual coherence. The method is geared towards, and was evaluated with, ranking functions that reward high query-term occurrence. Our modification approaches make no assumptions about the ranking function and are shown to be effective with both sparse and dense rankers. Furthermore, we demonstrate their rank promotion (improvement) merits with respect to Goren et al.'s method \cite{goren2020ranking}. }