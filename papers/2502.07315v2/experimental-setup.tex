\section{Experimental settings}
\label{sec:experimental-setup}
%We use two types of evaluation: offline and online. For offline
%evaluation,
\myparagraph{Datasets}
We use datasets which are reports of ranking
competitions \cite{raifer2017information,Mordo+al:25a}. In these
competitions, students were assigned to queries and had to produce
documents that would be highly ranked. Before the first round the students were provided with an example of a document relevant to the query. In each of the following rounds, the students observed past rankings for their queries and could modify their documents to potentially improve their next round ranking.

The first dataset, \firstmention{\firstDataset}, is the result of
ranking competitions held for $31$ queries from the TREC9-TREC12 Web
tracks \cite{raifer2017information}. Five to six students competed for each query. The undisclosed ranking function
was LambdaMART \cite{burges2010lambdamart} applied with various hand-crafted features. Following
Goren et al. \cite{goren2020ranking}, whose document modification
approach, \firstmention{\sentReplace}, serves as a baseline\footnote{We found that using LambdaMART instead of SVMrank as originally proposed \cite{goren2020ranking} yields improved performance.}, we use
round 7 for evaluation \cite{raifer2017information}. \sentReplace is a state-of-the-art feature-based supervised method for ranking-incentivized document modification. It
replaces a sentence in the document with another sentence to
improve ranking and to maintain content quality and faithfulness to
the original document.

The second dataset, \firstmention{\secondDataset}, is a report of
ranking competitions \cite{Mordo+al:25a} where the undisclosed ranking
function was the cosine between the E5 embedding vectors \cite{Wang+al:24a} of a document
and a query\footnote{The intfloat/e5-large-unsupervised version from
  the Hugging Face repository
  (\url{https://huggingface.co/intfloat/e5-large-unsupervised}).}. The competitions were run for 7 rounds with $15$
queries from the Web tracks of TREC9-TREC12; 4 players were competing
for each query \cite{wang2022text}.
%Our best performing document modification
%strategies (prompts) were used as bots in some rounds of these
%competitions for online evalution. (See more details below.)
%For
%offline evaluation,
We used round $4$ for evaluation to allow the document
modification methods to have enough history of past rankings. 

For both datasets just described, we apply the different document
modification methods, henceforth referred to as \firstmention{bots},
upon each of the documents in the ranked list for a query in the
specified round (except for the highest ranked document). For each
selected document, we induce a ranking using the same ranker used in
the competitions over its modified version and the original next-round
versions of the other documents (of students) from the round. We use
the evaluation measures described below upon the resultant ranking. We
average the evaluation results across all documents we modified per
round and over queries.

\myparagraph{Evaluation measures}
%We analyzed the performance of a document modification method using various evaluation measures, categorized into three primary groups: ranking properties, faithfulness properties and Quality and Relevance properties. All measures were computed per player and her document for a given query. The results were averaged over queries and grouped by the player type (student, baseline\footnote{i.e. the method of replacing paragraphs \cite{goren2020ranking}.}, a static document or one of the \bt s). For the online evaluation, the measures were also averaged over rounds.
To evaluate rank promotion (demotion) of documents as a result of
modification, we follow Goren et al. \cite{goren2020ranking} and
report \firstmention{Scaled Promotion}: the increase (decrease) of rank in the next round with respect to the current round normalized by the maximum possible rank promotion (demotion).



\omt{
%\begin{block}{Candidate Faithfulness at 1}
$CF@1(d_{curr},d_{next})=\frac{1}{n} \cdot \Sigma_{i=1}^{n} \mathbf{1}{\{ TT(d{curr},d_{next_{i}}) \geq 0.5 \}}$
%\end{block}

%\begin{block}{Normalized Candidate Faithfulness at 1}
$NCF@1(d_{curr},d_{next})=\frac{CF@1(d_{curr},d_{next})}{CF@1(d_{curr},d_{curr})}$
%\end{block}


%\begin{block}{Environmental Faithfulness at 10}
$EF@10(d_{next})=\frac{1}{2 \cdot 10} \cdot \Sigma_{i=1}^{10} [\mathbf{1}{\{ TT(d{\text{top}i}, d{\text{next}}) \geq 0.5 \}} + \mathbf{1}{\{ TT(d{\text{next}}, d_{\text{top}_i}) \geq 0.5 \}}]$
%\end{block}

%\begin{block}{Normalized Environmental Faithfulness at 10}
$NEF@10(d_{curr},d_{next})=\frac{EF@10(d_{next})}{EF@10(d_{curr})}$
%\end{block}
}


To evaluate the faithfulness of a modified document ($\dn$) to its
original (current) version ($\dc$), we compare
the two documents using Gekhman's et al. \cite{gekhman2023trueteacher}
natural language inference (NLI) approach. Specifically, we estimate
whether one document (denoted {\em hypothesis}) is entailed from the other
document (denoted {\em premise}) while preserving factual consistency. The estimate is the \trueteacher{} (TrueTeacher)
measure: \trueteacher $(premise, hypothesis)$ is the
output of the model in the range [0,1]; higher scores indicate stronger factual alignment.

To apply the TrueTeacher model, we first compute the average number of sentences in the modified document that are entailed\footnote{Entailment is determined by a threshold of $0.5$ for the TT score \cite{gekhman2023trueteacher}.} by the current document, which we refer to as raw faithfulness (RF):
%\trueteacher{} score between
%the current document ($\dc$) and all ($n$) sentences in the modified
%document ($\dn$):
$RF(\dn,\dc) \definedas \frac{1}{n} \sum_{i=1}^n \delta[\trueteacher
  (\dc, \dni) \ge 0.5];$ $d^{i}$ is the i'th sentence in document $d$;
$\delta$ is Kronecker's indicator function. Since $RF(\dc,\dc)$ is not
necessarily $1$, we normalize the raw
faithfulness to yield our \firstmention{\normFaith} measure: $\frac
{RF(\dn,\dc)}{RF(\dc,\dc)}$. 

Using LLMs to modify documents raises a concern
about hallucinations \cite{shuster2021retrieval}. We hence measure the
extent to which the content in the modified document is ``faithful''
to that in the entire corpus\footnote{For a corpus we use all the
  documents in all rounds prior to the round on which evaluation is
  performed.}. To that end, we treat the current document as a query,
and retrieve the top-$k$\footnote{We set $k=10$ in our experiments.}
documents in the corpus; $\topRet$ denotes the retrieved set. Retrieval is based on using cosine to compare a query
embedding and the document embedding. We use two types of embeddings:
E5 \cite{Wang+al:24a} and TF.IDF.  We define raw corpus faithfulness
(RCF) as: $RCF (\dn) \definedas \frac {1}{2k} \sum_{d \in \topRet}
(RF(\dn,d) + RF (d,\dn))$. The normalized corpus faithfulness
measure we use is: $CF (\dn) \definedas
\frac{RCF(\dn)}{RCF(\dc)}$. Using the E5 and TF.IDF embeddings results
in the \firstmention{\normCorpFaithE} and
\firstmention{\normCorpFaithT} normalized corpus faithfulness
measures, respectively.

Statistically significant differences are determined using the two-tailed paired permutation test
  with 100,000 random permutations and $p < 0.05$.

\omt{
The Normalized Candidate Faithfulness $NCF@1(\dc, \dn)$: the
normalization of the Candidate Faithfulness $CF@1(\dc, \dn)$ by the
self-consistency score: $\frac{CF@1 (\dc, \dn)}{CF@1(\dc, \dc)}$;
(iii) Environmental Faithfulness at 10 $EF@10(\dn)$: This metric
measures how much the generated document ($\dn$) maintains contextual
consistency with the broader corpus. Specifically, it measures the
similarity of $\dn$ to the top 10 documents most similar to it in the
corpus.  The corpus includes all the documents (across all queries)
available up to the test round. Two approaches are employed to compute
the similarity. The first approach is based on the (unsupervised) E5
\cite{wang2022text} representation with the cosine similarity
metric. The second approach is based on the TF.IDF
\cite{sparck1972statistical, salton1975vector} representation with the
cosine similarity metric. This metric is then calculated as follows:
$\frac{1}{2*10} \sum_{i=1}^{10} (\trueteacher(\dn, d_{top_{i}}) +
\trueteacher(d_{top_i}, \dn))$. Where $d_{top_{i}}$ represents the
$i$-th document, while ordering the documents with respect to the
similarity to $\dc$. The two approaches yield two variants of this
metric: $EF@10$\_dense and $EF@10$\_sparse, for the E5 and TF.IDF
representations, respectively; (iv) The Normalized Environmental
Faithfulness at 10 $NEF@10(\dn)$: The normalization of EF@10 by the
EF@10 of the current document: $\frac{EF@10(\dn)}{EF@10(\dc)}$. These
measures collectively provide a comprehensive framework for assessing
faithfulness. They evaluate the consistency of the modified document
not only with respect to the current document but also in relation to
other documents in the corpus.



\myparagraph{Relevance and Quality scores} The third category of evaluation measures focuses on the relevance and quality of documents. Both quality and relevance scores are assigned by crowdsourcing annotators via the Connect platform on CloudResearch \cite{noauthor_introducing_2024}, assessing the document's content quality and its relevance to the query\footnote{These evaluations of relevance and quality are conducted exclusively in the online evaluation setting.}. A document's quality or relevance score is set to 1 if at least three out of five English-speaking annotators marked it as valid or relevant to the query; otherwise, the score is set to 0. We report the ratio of documents that received a quality or relevance score of 1.
}


\myparagraph{Instantiating bots} For LLM we use Chat-GPT 4o
\cite{achiam2023gpt}. As described in Section
\ref{sec:bots}, there are a few parameters affecting the
instantiation of specific prompts. The number of queries is set to a
value in $\{1, 2\}$.  The number of examples per query is selected
from $\{1, 2, 3\}$. The number of past ranks (i.e., rounds) in the
Temporal prompt is selected from $\{2,3\}$. Using these
parameter values, and the other binary decision factors that affect
instantiation (see Section \ref{sec:bots}), results in $192$
different bots (prompts). In addition, we set the LLM's temperature parameter which controls potential drift to values in $\{0, 0.5, 1, 1.5, 2\}$ \cite{peeperkorn_is_2024}. 

\myparagraph{Rank promotion performance of bots} In terms of Scaled
Promotion, we found\footnote{Actual numbers are omitted due to space
  considerations and as they convery no additional insight.} that the
Pairwise bots (with random selection of document pairs) and the
Listwise bots were the best performing for both the \firstDataset and
\secondDataset datasets; the same specific instantiation of each of these two bots was
always among the top-3 performing bots for both datasets. This finding attests to the
rank-promotion effectiveness of these types of bots (prompts) for
different rankers (LambdaMART and E5). The Temporal bots (prompts), which provide rank-changes information along rounds, were less
effective (in terms of Scaled Promotion) than the Pairwise and
Listwise bots, but were more effective than the Pointwise bots. 

In what follows, we present the evaluation of the two
bots which posted for both datasets Scaled Promotion among the best three:\footnote{These bots were also the best performing in the online evaluation presented below.} 
%For efficiency considerations, we use LLama-2 with $13$B parameters
%\cite{touvron2023llama} to select the best performing
%configurations. The selection is performed with the \firstDataset
%dataset based on the scaled promotion evaluation measure. The best
%performing configurations for which we will report performance over the evaluation datasets are:

\begin{itemize}
\item Pairwise, where only the given query is included, one
  random pair of documents for each of the three last rounds is
  provided as examples, the current rank of the document is not
  used, and the temperature is set to $0.5$.
\item Listwise, where only the given query is included, two previous rounds are used, the current rank is not used, and the temperature is set to $0$.
\end{itemize}
Appendix \ref{appendix_prompt} provides the prompts for these bots.
%The fact that the pairwise and listwise approaches are the most
%effective is conceptually consistent of findings in work on using LLMs
%to induce ranking where the merits of pairwise and listwise approaches
%have been demonstrated \cite{ma2023zero,qin2023large}. For evaluation
%over \firstDataset and \secondDataset we use




\omt{
%The goal of the first phase is to identify a representative prompt for each class of prompts--that is, the prompt whose resultant agents maximize a metric related to ranking promotion. We conduct a comprehensive grid search over the 225 configurations described in Section \ref{sec:bots}, using Dataset 1. For this phase, we utilized Llama-2 with 13B parameters due to its availability \cite{touvron2023llama}. From each configuration, we constructed five agents, each with a different temperature setting for the probability model of the LLM\footnote{All other parameters of the LLM were fixed.}. The temperatures used were \{0, 0.5, 1, 1.5, 2\} and were selected based on the work of Peeperkorn et al \cite{peeperkorn_is_2024}. The selected agents compete for round 7, as was the case in Goren et al. \cite{goren2020ranking}. We do not report the detailed results of this phase due to space limitations in the paper.
}


\myparagraph{Online evaluation} The evaluation performed over the
\firstDataset and \secondDataset datasets is offline and therefore
spans a single round: the students who competed in the competition did
not respond to rankings induced over the documents we modify here. We
therefore also performed online evaluation where our instantiated
prompts competed as bots against students. We organized a ranking
competition\footnote{The competition was approved by institution and international ethics committees.} similar to that of Mordo et al. \cite{Mordo+al:25a} using 15
queries from TREC9-TREC12\footnote{These are different queries than
  those used in the \secondDataset dataset: 21, 55, 61, 64, 74, 75, 83, 96, 124,
  144, 161, 164, 166, 170, 194.}. In contrast to Mordo et al.'s
competitions \cite{Mordo+al:25a}, each game included 5 players: two-three
students, one of the two bots discussed above (Pairwise or
Listwise), and one or two static documents were created using a procedure similar to the one in Raifer et al. \cite{raifer2017information}: first, we used the query in the English Wikipedia search engine and selected a highly ranked page. We then extracted a candidate paragraph from this page, with a length of up to 150 words. Three annotators assessed the relevance of the passages, and we repeated the extraction process for each query until at least two annotators judged a paragraph as relevant. The selected paragraph was then used as a static document for the query for all students.

The students were not aware that they were competing
against bots. We applied our bots in rounds 5\footnote{Due to
  technical issues, we could not run the bots at round 4 as in the offline evaluation.}, 6 and 7 and report the
average performance over these three rounds.

We had documents in the online
evaluation judged for relevance and quality using crowdsourcing
annotators on the Connect platform of CloudResearch
\cite{noauthor_introducing_2024}. Following past work on ranking
competitions \cite{raifer2017information,goren2020ranking}, a
document's quality grade is set to $1$ if at least three out of five
English-speaking annotators marked it as valid (as opposed to keyword
stuffed or useless) and to $0$ otherwise. The relevance grade was $1$ if the document was
marked relevant by at least three annotators and $0$ otherwise. 







\endinput


We adopt an evaluation approach similar to that of Goren et al. \cite{goren2020ranking}. Two evaluation settings are considered: (i) Offline evaluation, where we leveraged existing datasets from ranking competitions, and (ii) Online evaluation, where a set of \bt s, each with a specific \contextualized, participate as a player in an ongoing ranking competition. The offline evaluation is run only for a single round, since the students did not respond to rankings that included the documents produced by our \bt s. In the online setting, other players may modify their documents simultaneously while our agents make their own modifications. In this section, we begin by describing the datasets used in our experiments (Section \ref{sec_datasets}). We then present the setups for both offline and online evaluations (Section \ref{sec_exp_set}). Finally, we outline the evaluation measures employed to assess performance of the \bt s and compare their performances against other types of agents (Section \ref{sec:eval-measures}).







\subsection{Datasets}\label{sec_datasets}
To perform offline and online evaluation, we deployed our approach in three different ranking competitions: one competition with feature-based ranking function, and two other utilizing transformer-based ranking function. The datasets employed in our experiments are as follows:

\myparagraph{Dataset 1} The first competition utilized for offline evaluation and comparison with the baseline model proposed by Goren et al. \cite{goren2020ranking}. It was organized by Raifer et al. \cite{raifer2017information}. In this competition, students enrolled in a course served as authors of documents and were assigned to 31 queries from the TREC9-TREC12 Web tracks. Each query defined a repeated-ranking-game. Students were incentivized with course grade bonuses and were asked to modify their documents for 8 rounds so that their document will be highly ranked for the played query. We selected round 7 for the offline evaluation, following Goren et al. \cite{goren2020ranking}. A total of 31 repeated-games (one per query) were conducted. A LambdaMART ranking function was applied \cite{burges2010lambdamart}.

\myparagraph{Dataset 2} This dataset used for offline evaluation and hyper-parameter tuning for the online evaluation. It was sourced from a ranking competition conducted by Mordo et al. \cite{div}. It involved a competition with 15 queries\footnote{From the TREC9-TREC12 Web Track as well.}, 7 rounds, and 4 players per game. In contrast to the competition described by Raifer et al. \cite{raifer2017information}, a transformer based ranking function was applied: the (unsupervised) E5 \footnote{The intfloat/e5-large-unsupervised version from the Hugging Face repository was used
  (\url{https://huggingface.co/intfloat/e5-large-unsupervised}).} \cite{wang2022text}. We focus on round 4, as it is the first round where we can apply our \bt s (recall that our \bt s require the context of previous rounds to modify a document).

\myparagraph{Dataset 3} We organize a ranking competition using 15 queries\footnote{From TREC9-TREC12; Different queries comparing to those used in Dataset 2: [21, 55, 61, 64, 74, 75, 83, 96, 124, 144, 161, 164, 166, 170, 194].}. The setup of this competition was similar to that of Mordo et al \cite{div}, with the following key difference: each games included 5 players. From round 5 \footnote{We initially planned to introduce our \bt s in round 4 as in Dataset 2; however, due to experimental constraints, we began their application in round 5.} of the competition, the players in each group consisted of: one \bt, two or three students and one or two planted documents\footnote{The same document as the initial document every participant started with.}. From the perspective of the students, the inclusion of \bt s did not alter the structure or appearance of the competition, preserving the integrity of the evaluation.

\subsection{Experimental setting}\label{sec_exp_set}

Our document modification approach operates as follows: first, the ranking for a given query is observed. Next, the approach modifies a specific document with the aim that the resulting document will be ranked higher in the next round of the game. In dynamic (online) settings, other documents may also be modified simultaneously, influencing the subsequent ranking. We design two evaluation paradigms—online and offline—both of which simulate a dynamic setting. The approach introduced by Goren et al. \cite{goren2020ranking} serves as a baseline to our approach.

\myparagraph{Offline evaluation}
The offline evaluation is divided into three phases. In each phase, we evaluated an \bt{} using a similar approach employed by Goren et al. \cite{goren2020ranking}: (i) Select a round and a game (query); (ii) Modify a document using the tested modification method (baseline \cite{goren2020ranking} or \bt{} with a specific prompt), excluding the top-ranked document. The exclusion of top-ranked documents is attributed to previous findings that their authors tend to avoid modifying their documents \cite{raifer2017information}. (iii) Evaluate the performance of the agent with respect to all other documents in the ranked list. (iv) Iterate over all the documents in the selected round and query and average the computed measure. 

% The performance of each metric is computed as the average over queries.

The goal of the first phase is to identify a representative prompt for each class of prompts--that is, the prompt whose resultant agents maximize a metric related to ranking promotion. We conduct a comprehensive grid search over the 225 configurations described in Section \ref{sec:bots}, using Dataset 1. For this phase, we utilized Llama-2 with 13B parameters due to its availability \cite{touvron2023llama}. From each configuration, we constructed five agents, each with a different temperature setting for the probability model of the LLM\footnote{All other parameters of the LLM were fixed.}. The temperatures used were \{0, 0.5, 1, 1.5, 2\} and were selected based on the work of Peeperkorn et al \cite{peeperkorn_is_2024}. The selected agents compete for round 7, as was the case in Goren et al. \cite{goren2020ranking}. We do not report the detailed results of this phase due to space limitations in the paper.

%In the second phase, we evaluated the performance of each representative prompt (Identified in Phase 1) on the same dataset and specifically on round 7, incorporating two key modifications: (i) we replaced Llama-2 with Chat-GPT 4o \cite{achiam2023gpt} as the latter demonstrated superior performance across multiple benchmarks\footnote{\url{https://docsbot.ai/models/compare/gpt-4o/llama-2-chat-13b}}. (ii) We included a baseline model introduced by Goren et al. \cite{goren2020ranking}, which modifies documents by replacing passages. Our implementation of the baseline consist of a primary difference: we replaced RankSVM \cite{joachims2002ranksvm} with LambdaMART \cite{burges2010lambdamart} due to its superior performance on Dataset 1. Details regarding the reproducibility process are omitted.

In the third phase, we evaluated the performance of each representative prompt on Dataset 2, which contains data from a ranking competition with a transformer-based ranking function. The evaluation procedure mirrored that of the second phase. We focused on round 4, as the round for evaluation.

\myparagraph{Online evaluation}
We adopted the two best performing \bt s (in terms of ranking promotion metric) evaluated on the transformer-based competition (Dataset 2) and assigned them as players in a similar ranking competition with different queries (resulting in Dataset 3). These \bt s joined the competition in round 5 and competed for the highest ranking in rounds 5, 6 and 7. Recall that the decision to introduce the \bt s in round 5, rather than at the beginning, was based on their dependency on past rankings, which were integrated into the \contextualized s to guide document modifications. The selection of round 5 over round 4 was due to experimental constraints.

% This article addresses the challenge of white-hat ranking-incentivized modifications, building on the work of Goren et al. \cite{goren2020ranking}, who explored this topic in offline and online competition settings on a ranking competition dataset comprised by Raifer et al. \cite{raifer2017information}. In addition to closely mimicking their approach, which utilized the feature-based LambdaMART ranker \cite{burges2010ranknet} and an LTR-based baseline, referred as the "feature-based" setting in this article, we adopt a transformer-based ranker—specifically, the E5 model introduced by Wang et al. \cite{wang2024multilingual}, both for offline and online evaluation. The E5 settings are referred to as the offline and online "transformer-based" settings in this article.

% Our study investigates the effectiveness of ranking-incentivized modifications within a comparable framework while leveraging the advantages of transformer-based models. The primary goal of this research is to evaluate strategies for rank promotion, focusing on leveraging large language models (LLMs) to implement modifications using various few-shot \cite{brown2020language} contextual approaches. By incorporating LLM-based methodologies, we aim to assess their ability to generate high-quality, contextually relevant modifications that adhere to the principles of white-hat ranking practices.


% \paragraph*{Dataset Creation}
% For the feature-based setting, we utilized the dataset created by Raifer et al. \cite{raifer2017information}, similarly to Goren et al. \cite{goren2020ranking}.

% In the transformer-based settings, we implemented and evaluated our \bt s within a ranking environment inspired by the 'ranking competition' framework introduced by Raifer et al. \cite{raifer2017information}. Similar to their approach, our ranking competition focused on optimizing documents for a black-box ranker. However, we introduced several adjustments to align with our experimental objectives and constraints.

% First, we utilized a subset of 15 queries derived from the TREC ClueWeb09 dataset \cite{clueweb09}, whereas Raifer et al. \cite{raifer2017information} used a broader set. This choice enabled us to conduct multiple experiments in parallel while maintaining a manageable workload and scalability. Additionally, we adhered to the original competition's guidelines, requiring concise, 150-word plain English submissions without links, special characters, or HTML tags, to ensure methodological consistency.

% The competition was structured into "matches" and "rounds." A match refers to a grouping of four competitors who worked on a single query. In each match, participants edited their texts to achieve the highest possible ranking for the query. A round is one iteration of competition during which all matches were conducted simultaneously for a specific query set. Each round provided participants with feedback, allowing them to see their own rankings as well as those of their competitors.

% The competition was divided into two parts, with each part consisting of seven rounds. In the first part, participants worked with 15 queries \cite{partA2024}. In the second part, these queries were replaced with 15 different ones, also sourced from ClueWeb09 \cite{clueweb09} \cite{partB2024 (TBA)}. The grouping of competitors and conditions remained fixed across rounds. We used the first part of the competition for offline evaluation and the second part for online evaluation, enabling a thorough analysis of our methodology under both controlled and dynamic conditions.

% By tailoring the competition to our needs while preserving its core principles, we ensured both comparability to prior work and the validity of our findings in the context of scalable and rigorous experimentation.


% \paragraph*{Offline Evaluation}
% The feature-based setting we developed was heavily influenced by the offline setting described in detail by Goren et al.\ \cite{goren2020ranking}. The evaluation of this setting was carried out meticulously, adhering closely to the methodology outlined in Goren et al.'s \cite{goren2020ranking} offline evaluation section.

% To rigorously assess our LLM-based ranking-incentivized modification methodology in the offline transformer-based setting, we constructed an evaluation setting inspired by the offline setting introduced by Goren et al.\ \cite{goren2020ranking}. Our experiments were conducted on documents initially ranked 2nd, 3rd, and 4th in the fifth round of a competition designed to rank documents against a shared set of 15 queries. These ranks were chosen deliberately, as they represented non-top-performing documents, providing a meaningful opportunity to evaluate the potential for improvement when modifications were applied, as suggested by Goren et al.\ \cite{goren2020ranking}.

% In each round of evaluation, four documents were subjected to ranking. Three of these were unaltered, human-authored documents selected from previous rankings in the competition. The fourth document was a modified version, generated by applying our LLM-based methodology. By incorporating multiple initial ranking positions and diverse queries, we ensured that the evaluation was not overly influenced by specific document characteristics or query types. This approach enhanced the generalizability of our findings.


% \paragraph*{Online Evaluation}
% To closely mimic the online experiment described by Goren et al.\ \cite{goren2020ranking}, we conducted the second part of the ranking competition using the same participant pool but a different set of 15 queries sourced from ClueWeb09 \cite{clueweb09}.

% For the online evaluation, we selected the two \bt{} methods that achieved the highest scores in the offline evaluation during one of our early experiments, the results of which are not depicted in this paper. We used "scaled rank promotion" as the metric for determining their performance. These \bt{}s were introduced into the competition starting from the 4th round of the second part, as their methodology required contextual information derived from at least three prior rounds to function effectively. The bots competed against the same set of human participants, with groupings and conditions held constant across all seven rounds of this phase. This ensured consistency in the evaluation and allowed for a direct comparison between \bt{}- and human-authored rankings.

% From the perspective of the human participants, the inclusion of \bt s did not alter the structure or appearance of the competition. Each round followed the same format and task descriptions as in the first part of the competition. This design ensured that human participants approached their ranking tasks without being influenced by knowledge of the \bt s' involvement, preserving the integrity of the evaluation.

% \paragraph*{Ranker and Document Embeddings}
% In the feature-based setting, we employed the same methodology and features previously utilized by Raifer et al.\ \cite{raifer2017information} and Goren et al.\ \cite{goren2020ranking}. Specifically, we used the exact trained LambdaMART model that was trained and used by Goren et al.\ \cite{goren2020ranking}. The method for text embeddings for this setting replicated Goren et al.'s \cite{goren2020ranking} approach - incorporating 25 content-based features. These features were selected either from those used in Microsoft's learning-to-rank datasets \cite{mslr}, or as query-independent measures of document quality. Notably, these included stopword-based metrics and the entropy of term distribution within a document, both of which have been proven effective in web retrieval scenarios.

% For the transformer-based offline and online settings, we utilized E5 as the ranker and the E5 embedder for the document embeddings.


% \paragraph*{Baseline}
% To establish a robust baseline, we implemented a ranking passage pairs approach, closely mirroring the methodology described in Goren et al. \cite{goren2020ranking}, with the primary difference being the replacement of RankSVM \cite{joachims2002ranksvm} with LambdaMART \cite{burges2010lambdamart}. LambdaMART was selected based on its superior empirical performance in prior evaluations. The dataset, features, and labels remained identical to the original setup. Labels for the passage pairs were generated using a dual-objective harmonic mean approach introduced in Goren et al. \cite{goren2020ranking}, integrating rank-promotion and local coherence objectives, where rank-promotion labels ranged from 0 to 4 based on positional improvement in rankings, and coherence labels quantified semantic similarity to maintain content quality. The harmonic mean was computed with $\beta = 1$, assigning equal weight to both objectives.

% Training was conducted on 57 documents extracted from Round 6 of the original competition dataset introduced by Raifer et al. \cite{raifer2017information}. The model’s performance was subsequently evaluated on 124 experimental settings, derived from Round 7 of the ranking competition, spanning documents ranked 2–5 across 31 queries. The validation set was configured similarly to Goren et al.'s \cite{goren2020ranking} procedure. Both training and validation utilized NDCG@1 as the evaluation metric, contrasting with the NDCG@5 used by Goren et al. \cite{goren2020ranking}, to align with the goal of selecting the top sentence-swapped document.

% We implemented LambdaMART \cite{burges2010lambdamart} using its default parameter settings for several features, specifically: \begin{itemize} \item \textbf{Minimum leaf support:} Minimum number of samples each leaf must contain, set to 1 (default). \item \textbf{Number of threshold candidates for tree splitting:} Set to 256 (default). \item \textbf{Early stopping rounds:} Set to 100 (default). \end{itemize}

% We conducted a grid search to tune other hyper-parameters, exploring different configurations. This grid search included: \begin{itemize} 
%     \item \textbf{Number of Trees:} 50, 500, 1000, 1200. 
%     \item \textbf{Number of Leaves per Tree:} 10, 50, 100. 
%     \item \textbf{Learning Rate (Shrinkage Value):} 0.01, 0.1, 0.2. 
% \end{itemize}
