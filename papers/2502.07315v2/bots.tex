\section{Document modification methods}
\label{sec:bots}

%\shared \contextualized

We present methods utilizing a large language model (LLM) that
modify a document so as to have it highly ranked in the next ranking induced
for a query by an undisclosed ranking function. We assume that past rankings for
the query and other queries can be observed. We post two additional
goals for the modification: having the resultant document of high
quality (in terms of discourse) and as ``faithful'' as possible to the original document in terms of the
content it contains \cite{goren2020ranking}.

The modification methods are based on prompting an LLM. The prompts have two parts: (i) a general shared part which
describes the task (document modification for improved ranking) with a
directive to maintain similarity to the original document (i.e.,
faithfulness); and, (ii) context-specific part, specific to a prompt, which provides information about past rankings. We assume that the LLM produces high quality content in terms of discourse. The general shared part is:

%with information about rankings
%induced by the undisclosed ranking function. The prompts include a
%specific directive to keep the modified document faithful to
%$\doc$. We assume that LLM-based modification results in high quality
%content in terms of discourse (specifically, coherence).
%





%A repeated ranking competition between document authors for query
%$\query$ is executed for several rounds
%\cite{raifer2017information,goren2020ranking}. In each round, all
%document authors can observe previous rankings induced for $\query$%
%(and other queries) by the undicsclosed ranking function. Then they
%might modify their documents to have them highly ranked in the next
%round.

%Our document modification strategies use large language models (LLMs)
%with a three fold objective (cf. \cite{goren2020ranking}): having the
%modified document (i) attain high ranking in the next round, (ii) be
%``faithful'' (in terms of content) to the original document, and (iii)
%be of high quality (in terms of discourse and coherence). We use prompts composed of a ``system part'', shared by all prompts, %and a ``user part'' which is context-specific. The system part specifies the task and the need to maintain faithfulness to the original document\footnote{We assumed the LLM generates content of high quality in terms of discourse and coherence.}:



%in-context (implicit) learning of
%the undisclosed ranking function with LLMs. Specifically, we present
%prompting templates

%Our goal is to devise ranking-incentivized document modification
%methods. The methods take as input a document, a query $\query$, and
%past rankings induced for $\query$ or other queries. The output is a
%modified document formed to attain as high as possible ranking
%for $\query$. In addition to the ranking objective, we aim to
%have the modified document ``faithful'' (in terms of content) to the original document, and of high quality
%\cite{goren2020ranking}.

%The document modification methods we present use large language models (LLMs) with various
%prompts that differ by the type of context they provide; e.g., about
%rankings induced by the ranking function. All prompts share the following generic part:

%In this Section, we introduce our novel approach for content modification aimed to promote the ranking of documents in a ranking competition with a query {\query} and an undisclosed ranking function.
%We introduce a novel approach to that introduced by Goren et al. \cite{goren2020ranking}, which relies on replacing a passage to get a modified document. We use a prompt-driven approach, and provide examples of previous documents and their corresponding rankings. Each prompt defines an \bt{} that strategically modify his documents in order to promote his ranking. Similar to Goren et al. \cite{goren2020ranking}, our approach adheres to three desiderata for content modification: ranking promotion, quality preservation and faithfulness to the original content.

%To formalize our prompt-driven method, we define a structured family of prompts, each consisting of two key components. The first component (referred to as the \textbf{System prompt}) follows a consistent format across all agents. it includes: (i) the instruction given to the LLM about the ranking competition, (ii) the query associated with the game, and (iii) the current document of the agent at a specific round of the game. The second component (referred to as the \textbf{User prompt}) provides contextualized information tailored to past rankings of previous rounds and other games, and the corresponding documents. By incorporating such contextual information, the User prompt facilitates adaptive strategies for content modification.
%This prompt architecture enables us to systematically evaluate the impact of varying contextual information in the User prompt on multiple metrics aligned with the desiderata.


% Our approach to constructing the \bt s for generating texts in our "white-hat ranking-incentivized
% modifications" study was intentionally method-agnostic, meaning we kept the almost the entire prompt identical across methods, with the sole exception being the “context” element. This flexible design allowed us to focus on assessing the impact of contextual variations on text generation. Specifically, while each \bt{} employed a uniform prompt structure to maintain consistency across the study, we introduced distinct contextual content in targeted sections to explore how different contexts influenced rank promotion outcomes. This approach ensured that any observed effects could be attributed more confidently to the context structure rather than to external factors.

%\myparagraph{Prompt Template} The structure of the prompt provided to the LLM can be illustrated as:

\input{Images/prompt}

The placeholder: <<median length of context document>> is the median number of words in documents provided in the context-specific part of the prompt that follows the general part.
%This directive is intended to avoid document-length bias in the ranking competitions.
%\newline
%\textit{\textbf{User prompt:}\newline
%<contextual information according to the method used>}\newline


\myparagraph{Context-specific part} The goal of this part of the
prompt is to provide the LLM with ``hints'' about the undisclosed
ranking function; specifically, past rankings. We present four
types of context, most of which are inspired by work on using LLMs to
induce ranking \cite{nogueira2020document, qin2023large, ma2023zero}.
\begin{itemize}
\item \firstmention{Pointwise}: The context includes pairs of a query and a
  document which was the highest ranked in the past for the
  query. This context is inspired by pointwise approaches to LLM ranking which provide the LLM with a query and examples of relevant and non-relevant documents (e.g., \cite{Liang+al:22a}). Including the documents most highly ranked in the past for the query at hand is conceptually reminiscent of a prevalent document modification strategy in competitive search \cite{raifer2017information,goren_driving_2021}: mimicking content in the most highly ranked documents in the past.
\item \firstmention{Pairwise}: Inspired by work on using pairwise approaches to prompt an LLM to induce ranking \cite{qin2023large}, the context
  includes queries accompanied with pairs of documents and an indication about which was ranked higher. 
\item \firstmention{Listwise}: Inspired by listwise prompting
  approaches for LLM ranking \cite{ma2023zero}, the context includes queries and ranked document lists. 
\item \firstmention{Temporal}: The context includes a query and
  versions of the same document and their ranks in past rankings induced for the query. The goal is to provide the LLM with information about how modifications of a document affected its ranking along time.
  %  Pairs of documents are considered (top-ranked documents or random documents);
  %This method selects
  %top-ranking documents (from one or multiple games), inspired by the
  %"mimicking the winner" strategy which has been shown to be optimal
  %in competitive search settings \cite{raifer2017information};
  \end{itemize}

%To gear the LLM towards the objective of
%modifying the document for improved ranking, our modification
%strategies provide the LLM in the user part of the prompt some
%information about how the undisclosed ranking function ranks
%documents. The types of information we used are inspired by work on
%using LLMs to rank documents \cite{nogueira2020document, qin2023large,
%  ma2023zero}.



%We propose four distinct classes of methods for constructing \contextualized s, inspired by previous work \cite{nogueira2020document, qin2023large, ma2023zero}. These methods differ in the way past documents and their associated rankings are integrated into the \contextualized. These documents, along with their corresponding rankings are the only signal an agent has about the ranking function. This information is crucial for enabling the agent to modify its current document to achieve an higher ranking in the subsequent round(s).
% assume that past documents and their rankings tha adapted these concepts, originally crafted from a ranker perspective, to serve the requirements of a ranking-incentivized modification agent, effectively transforming ranking principles into tools used for rank promotion.

% In this study, we introduce four distinct context-based methods. Each method follows a "few-shot" approach  when generating text with any large language model (LLM), enabling the model to generalize from limited examples without extensive training.

% Importantly, we chose not to fine-tune the LLMs utilized in our study, opting instead to work solely with prompts. This decision was made to demonstrate that a basic level of proficiency with LLMs is sufficient for executing this type of ranking-incentivized modification procedure, making it accessible to users without specialized technical knowledge. Future research might explore more performance-driven strategies, including model fine-tuning or alternative methods, which could potentially yield better empirical results.
%The four context-based methods for constructing \contextualized s are
%as follows: (i) \textbf{Pointwise (POW)}: This method selects
%top-ranking documents (from one or multiple games), inspired by the
%"mimicking the winner" strategy which has been shown to be optimal in
%competitive search settings \cite{raifer2017information}; (ii)
%\textbf{Pairwise (PAW)} \cite{qin2023large}: Pairs of documents are
%considered (top-ranked documents or random documents); (iii)
%\textbf{Listwise (LIW)} \cite{ma2023zero}: Ranked list(s) of documents
%are considered, providing broader context to the LLM; (iv)
%\textbf{Dynamic (DYN)}: A novel approach we introduce, this method
%selects different documents authored by the same individual. The
%dynamic approach is intended to capture variability in an agent's
%document protfolio.
% \begin{itemize} 
%     \item \textbf{Pointwise (POW)} \cite{nogueira2020document}: This method focuses solely on top-ranking document examples inspired by the "mimicking the winner" strategy proved to by optimal in competitive search settings \cite{raifer2017information}.
    
    % , implementing a “mimicking the winner” strategy similarly to what Raifer et al. \cite{raifer2017information} mentions in their article. The idea behind this method is that by analyzing only the highest-performing documents, the model gains insight into successful content features. Inspired by Nogueira et al.  \cite{}.
    
    % \item \textbf{Pairwise (PAW)} \cite{qin2023large}: In this approach, pairs of documents are considered, either top ranked documents or random documents.
    
    % often contrasting a top-ranking document with a lower-ranking or "loser" counterpart. In other instances, we select two documents at random, facilitating a comparative analysis of differing content attributes. Inspired by Zhen et al. .
    
    % \item \textbf{Listwise (LIW)}: Generalizing the former method mentioned, this method involves an exhaustive comparison, incorporating all documents from the match set(s). The motivation for this method is that by leveraging a broader context, we can capture a holistic view of the content landscape. Inspired by Ma et al. 
   
%     \item \textbf{Dynamic (DYN)}: The dynamic method evaluates a sequence of documents from a particular publisher over time, either a top-ranked document or a random document. This method is novel to best of our knowledge. 
% \end{itemize}
    
    % including the publisher's historical rankings over several rounds. This may involve using the latest top document and its previous iterations, or alternatively, a randomly selected document and its history, to understand how content evolution affects rankings. This method is novel.

\myparagraph{Prompt configurations}
There are numerous configurations
%($225$ all together)
of the context-specific part that we
study. These are determined by the following factors and parameters:
(i) the number of queries used; (ii) the number of examples used per
query: number of the most highest ranked documents in the past (Pointwise),
number of pairs (Pairwise), number of ranked lists (Listwise) and number of
documents with their temporal changes and ranks (Temporal); (iii) whether
the rank of the document to be modified in the last ranking induced
for the query is included; (iv) whether the query at hand is included
in the prompt, or alternatively only other queries; (v) for the
pairwise approach (Pairwise) we consider a random selection of the pair of
documents or the highest ranked document and an additional randomly
selected document; and, (vi) for the Temporal approach (Temporal), we vary the number of past ranks of the example document.  
%
%To further distinguish these prompts as distinct methods, we establish a rigorous parametrization: (i) \textbf{Number of queries}: Number of games included in the prompt; (ii) \textbf{Number of batches}: The number of batches per query; tailored to the method\footnote{For instance, the value 1 represents: a single document for Pointwise; a pair of documents for Pairwise; a ranked list for Listwise; and one document for Dynamic.}; (iii) \textbf{Inclusion of the current rank}: Indicates whether the rank of the \cd{} is included; (iv) \textbf{Inclusion of the current query}: Indicates whether the query for which we apply modification is included; (v) \textbf{Example type}: Defines whether the documents are selected randomly or by selecting the top ranked documents\footnote{This option applies only to Pairwise or Dynamic prompts.}. (vi) \textbf{History length}: The number of past rounds are included in the context\footnote{This option applies only to Dynamic prompt.}. In total, 225 unique \contextualized s structures were created.

% \begin{itemize} 
%     \item \textbf{Unique Query Number}: Specifies the optional number of different queries whose matches are included in the context. Options are to include 1-3 unique queries.
    % \item \textbf{Unique Example Number per Query/Topic}: Defines the number of distinct examples per query or topic, tailored to each method. Specifically, one example is set to 1 for the pointwise method, 1 pair for the pairwise method, 1 list of documents for the listwise method, and 1 "strand" of documents which its number is based on the "history length" feature for the dynamic method. The potential values for this parameter are 1-3 examples, depending on the requirements of the specific approach.
    % \item \textbf{Inclusion of Candidate's Ranking}: A binary feature that controls whether the current rank position of the \cd is shown. This setting enables testing of anchoring effects by either displaying or omitting the candidate’s ranking, which helps evaluate the LLM’s ability to assess content independently of explicit rankings of the input.
    % \item \textbf{Inclusion of Candidate's Topic/Query}: Another binary feature indicating whether the same topic/query as the candidate is included in the context at all. This tests whether the presence of topical relevance enhances the LLM’s ability to generalize the ranking-incentivized modifications' strategy effectively.
    % \item \textbf{Example Type}: Applicable solely to the "pairwise" and "dynamic" methods, this feature can be set to "random," where the example selection is fully randomized, or "tops," where at least one example (or the only example, if only one is specified) must be the top-ranked document. This parameter influences the contextual input quality by controlling the selection criteria for examples.
%     \item \textbf{History Length}: Relevant only to the "dynamic" method, this feature specifies the length of the historical strand per example. Options of 2 or 3 allow the model to examine a sequence of documents, capturing longitudinal patterns.
% \end{itemize}

% Each of these parameters was selected to enable the parametrization of different prompt-driven \bt s, allowing us to explore the influence of varying contextual \contextualizeds on the agent performance.

% \newline
% \newline

% Our ultimate objective was to select a single top-performing \bt from each method —--pointwise, pairwise (tops/random), listwise, and dynamic (tops/random)--- yielding six in total. To identify the optimal parameters and ultimately the best representative for each method, we conducted a comprehensive grid search, calculating the "Scaled Rank Promotion" metric (detailed below) on the test data described by Goren et al. \cite{goren2020ranking}, namely the 7th round in Raifer et al.'s competition \cite{raifer2017information}. For this process, we utilized Meta's Llama-2-13b model \cite{touvron2023llama}.

% Although OpenAI's GPT-4o model \cite{achiam2023gpt} demonstrated superior performance, we opted to use Llama-2 due to its open-source availability, which supports a scalable and cost-effective approach. In contrast, GPT-4o \cite{achiam2023gpt} operates on a pay-per-use model, making it less feasible for generating the thousands of documents required in this phase. By leveraging Llama-2 \cite{touvron2023llama}, we were able to implement a scalable heuristic that approximates the behavior of our \bt s on other LLMs, including GPT-4o, thereby streamlining the selection process.

% Once the grid search was completed, and the best-performing \bt s were identified for each method based on their "Scaled Rank Promotion" scores, we proceeded to apply these six selected methods with GPT-4o to optimize our final results. This two-step approach allowed us to balance scalability with performance, using Llama-2 for initial exploration and GPT-4o to maximize the quality of our outcomes. The final results, based on this methodology, are presented in the following section, and the chosen hyper-parameters chosen for each method are listed in Table \ref{table:1}.

% \input{Tables/chosen_grid_run}


% -	BOT descriptions (top to bottom, best 2 explained first):
% o	DYN_1100T2 – dynamic approach (history based) using a single query which is not the candidate’s query and a single example thread with length two (current epoch and previous epoch only) of the top document in the current round with no knowledge of the candidate’s document position.
% o	POW_1300 – Pointwise approach using a single query which is not the candidate’s query and three example documents, one of which is the top document, with no knowledge of the candidate’s document position.
% o	PAW_1301R – Pairwise approach using a single query which is the candidate’s query and three example document pairs chosen at random (not necessarily showing the top document in any example), there is no knowledge of the candidate’s document position, and it is not part of any example.
% o	DYN_1201R2 – dynamic approach (history based) using a single query which is the candidate’s query and two example threads with length two (current epoch and previous epoch only) of the random position documents in the current round with no knowledge of the candidate’s document position and without including it in the examples.
% o	PAW_1210T – Pairwise approach using a single query which is not the candidate’s query and two example document pairs including the top document and another chosen at random (necessarily showing the top document in any example), there is knowledge of the candidate’s document position, and it is not part of any example having not used the candidate query.
% o	LIW_1201 – Listwise approach using a single query which is the candidate’s query and two example lists (one for every epoch, using current and previous epochs) the lists are given in a ranked order but do not disclose the position of the candidate document and it is essentially not part of the examples.
