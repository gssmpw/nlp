@inproceedings{qin2024toolllm,
    title={Tool{LLM}: Facilitating Large Language Models to Master 16000+ Real-world {API}s},
    author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and dahai li and Zhiyuan Liu and Maosong Sun},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=dHng2O0Jjr}
}

@article{Qu_2025,
   title={Tool learning with large language models: a survey},
   volume={19},
   ISSN={2095-2236},
   url={http://dx.doi.org/10.1007/s11704-024-40678-2},
   DOI={10.1007/s11704-024-40678-2},
   number={8},
   journal={Frontiers of Computer Science},
   publisher={Springer Science and Business Media LLC},
   author={Qu, Changle and Dai, Sunhao and Wei, Xiaochi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Xu, Jun and Wen, Ji-rong},
   year={2025},
   month=jan }


@inproceedings{wang2024hypothesissearchinductivereasoning,
    title={Hypothesis Search: Inductive Reasoning with Language Models},
    author={Ruocheng Wang and Eric Zelikman and Gabriel Poesia and Yewen Pu and Nick Haber and Noah Goodman},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=G7UtIGQmjm}
}
@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@misc{COMPAS,
	author = {Northpointe},
	title = {Practitioner’s Guide to COMPAS Core},
	url = {https://cjdata.tooltrack.org/sites/default/files/2018-10/Practitioners_Guide_COMPASCore_121917.pdf},
	year = {2017},
}

@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@misc{muennighoff_s1_2025,
    title = {s1: {Simple} test-time scaling},
    shorttitle = {s1},
    url = {http://arxiv.org/abs/2501.19393},
    doi = {10.48550/arXiv.2501.19393},
    abstract = {Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27\% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50\% to 57\% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1},
    urldate = {2025-02-04},
    publisher = {arXiv},
    author = {Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Candès, Emmanuel and Hashimoto, Tatsunori},
    month = feb,
    year = {2025},
    note = {arXiv:2501.19393 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@misc{deepseek-ai_deepseek-r1_2025,
    title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
    shorttitle = {{DeepSeek}-{R1}},
    url = {http://arxiv.org/abs/2501.12948},
    doi = {10.48550/arXiv.2501.12948},
    abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
    urldate = {2025-02-11},
    publisher = {arXiv},
    author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
    month = jan,
    year = {2025},
    note = {arXiv:2501.12948 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Reasoning, Test-time scale},
}
@inproceedings{hoffmann_training_2022,
    author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
    title = {Training Compute-Optimal Large Language Models},
    year = {2022},
    isbn = {9781713871088},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We investigate the optimal model size and number of tokens for training a Transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\texttimes{} more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
    booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno = {2176},
    numpages = {15},
    location = {New Orleans, LA, USA},
    series = {NIPS '22},
    url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf}
}


@article{huang_survey_2025,
    title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
    volume = {43},
    issn = {1046-8188, 1558-2868},
    shorttitle = {A {Survey} on {Hallucination} in {Large} {Language} {Models}},
    url = {http://arxiv.org/abs/2311.05232},
    doi = {10.1145/3703155},
    abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
    number = {2},
    urldate = {2025-02-11},
    journal = {ACM Transactions on Information Systems},
    author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
    month = mar,
    year = {2025},
    note = {arXiv:2311.05232 [cs]},
    keywords = {Computer Science - Computation and Language, Hallucination, Survey},
    pages = {1--55},
}
@misc{wu_inference_2024,
    title = {Inference {Scaling} {Laws}: {An} {Empirical} {Analysis} of {Compute}-{Optimal} {Inference} for {Problem}-{Solving} with {Language} {Models}},
    shorttitle = {Inference {Scaling} {Laws}},
    url = {http://arxiv.org/abs/2408.00724},
    doi = {10.48550/arXiv.2408.00724},
    abstract = {While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-\$n\$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings indicate smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets, and that smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets. We hope these findings contribute to a broader understanding of inference scaling laws for LLMs.},
    urldate = {2025-02-07},
    publisher = {arXiv},
    author = {Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
    month = oct,
    year = {2024},
    note = {arXiv:2408.00724 [cs]},
    keywords = {Computer Science - Artificial Intelligence},
}
@inproceedings{rajpurkar_know_2018,
    title = "Know What You Don`t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124/",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
}

@misc{stengel-eskin_calibrated_2023,
    title = {Calibrated {Interpretation}: {Confidence} {Estimation} in {Semantic} {Parsing}},
    shorttitle = {Calibrated {Interpretation}},
    url = {http://arxiv.org/abs/2211.07443},
    abstract = {Sequence generation models are increasingly being used to translate natural language into programs, i.e. to perform executable semantic parsing. The fact that semantic parsing aims to predict programs that can lead to executed actions in the real world motivates developing safe systems. This in turn makes measuring calibration -- a central component to safety -- particularly important. We investigate the calibration of popular generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.},
    urldate = {2024-02-15},
    publisher = {arXiv},
    author = {Stengel-Eskin, Elias and Van Durme, Benjamin},
    month = jul,
    year = {2023},
    note = {arXiv:2211.07443 [cs]},
    keywords = {Computer Science - Computation and Language, Semantic Parsing},
}
@article{ferrucci_building_2010,
    author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A. and Lally, Adam and Murdock, J. William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Chris},
    title = {Building Watson: An Overview of the DeepQA Project},
    journal = {AI Magazine},
    volume = {31},
    number = {3},
    pages = {59-79},
    doi = {https://doi.org/10.1609/aimag.v31i3.2303},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v31i3.2303},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1609/aimag.v31i3.2303},
    abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV quiz show, Jeopardy. The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After three years of intense research and development by a core team of about 20 researchers, Watson is performing at human expert levels in terms of precision, confidence, and speed at the Jeopardy quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that can be used as a foundation for combining, deploying, evaluating, and advancing a wide range of algorithmic techniques to rapidly advance the field of question answering (QA).},
    year = {2010}
}

@ARTICLE{ferrucci_introduction_2012,
  author={Ferrucci, D. A.},
  journal={IBM Journal of Research and Development}, 
  title={Introduction to “This is Watson”}, 
  year={2012},
  volume={56},
  number={3.4},
  pages={1:1-1:15},
  keywords={Games;Semantics;Algorithm design and analysis;Computer architecture;Computers;History},
  doi={10.1147/JRD.2012.2184356}}


@article{tesauro_analysis_2013,
    title = {Analysis of {Watson}'s {Strategies} for {Playing} {Jeopardy}!},
    volume = {47},
    issn = {1076-9757},
    url = {http://arxiv.org/abs/1402.0571},
    doi = {10.1613/jair.3834},
    abstract = {Major advances in Question Answering technology were needed for IBM Watson to play Jeopardy! at championship level -- the show requires rapid-fire answers to challenging natural language questions, broad general knowledge, high precision, and accurate confidence estimates. In addition, Jeopardy! features four types of decision making carrying great strategic importance: (1) Daily Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., "buzz in." Using sophisticated strategies for these decisions, that properly account for the game state and future event probabilities, can significantly boost a players overall chances to win, when compared with simple "rule of thumb" strategies. This article presents our approach to developing Watsons game-playing strategies, comprising development of a faithful simulation model, and then using learning and Monte-Carlo methods within the simulator to optimize Watsons strategic decision-making. After giving a detailed description of each of our game-strategy algorithms, we then focus in particular on validating the accuracy of the simulators predictions, and documenting performance improvements using our methods. Quantitative performance benefits are shown with respect to both simple heuristic strategies, and actual human contestant performance in historical episodes. We further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show.},
    urldate = {2025-02-12},
    journal = {Journal of Artificial Intelligence Research},
    author = {Tesauro, Gerald and Gondek, David C. and Lenchner, Jonathan and Fan, James and Prager, John M.},
    month = may,
    year = {2013},
    note = {arXiv:1402.0571 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Question Answering},
    pages = {205--251},
}

@inproceedings{boyd-graber-borschinger-2020-question,
    title = "What Question Answering can Learn from Trivia Nerds",
    author = {Boyd-Graber, Jordan  and
      B{\"o}rschinger, Benjamin},
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.662/",
    doi = "10.18653/v1/2020.acl-main.662",
    pages = "7422--7435",
    abstract = "In addition to the traditional task of machines answering questions, question answering (QA) research creates interesting, challenging questions that help systems how to answer questions and reveal the best systems. We argue that creating a QA dataset{---}and the ubiquitous leaderboard that goes with it{---}closely resembles running a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner. However, the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant, fair, and effective question answering competitions. After detailing problems with existing QA datasets, we outline the key lessons{---}removing ambiguity, discriminating skill, and adjudicating disputes{---}that can transfer to QA research and how they might be implemented."
}

@misc{rodriguez_quizbowl_2021,
    title = {Quizbowl: {The} {Case} for {Incremental} {Question} {Answering}},
    shorttitle = {Quizbowl},
    url = {http://arxiv.org/abs/1904.04792},
    doi = {10.48550/arXiv.1904.04792},
    abstract = {Scholastic trivia competitions test knowledge and intelligence through mastery of question answering. Modern question answering benchmarks are one variant of the Turing test. Specifically, answering a set of questions as well as a human is a minimum bar towards demonstrating human-like intelligence. This paper makes the case that the format of one competition -- where participants can answer in the middle of hearing a question (incremental) -- better differentiates the skill between (human or machine) players. Additionally, merging a sequential decision-making sub-task with question answering (QA) provides a good setting for research in model calibration and opponent modeling. Thus, embedded in this task are three machine learning challenges: (1) factoid QA over thousands of Wikipedia-like answers, (2) calibration of the QA model's confidence scores, and (3) sequential decision-making that incorporates knowledge of the QA model, its calibration, and what the opponent may do. We make two contributions: (1) collecting and curating a large factoid QA dataset and an accompanying gameplay dataset, and (2) developing a model that addresses these three machine learning challenges. In addition to offline evaluation, we pitted our model against some of the most accomplished trivia players in the world in a series of exhibition matches spanning several years. Throughout this paper, we show that collaborations with the vibrant trivia community have contributed to the quality of our dataset, spawned new research directions, and doubled as an exciting way to engage the public with research in machine learning and natural language processing.},
    urldate = {2025-02-12},
    publisher = {arXiv},
    author = {Rodriguez, Pedro and Feng, Shi and Iyyer, Mohit and He, He and Boyd-Graber, Jordan},
    month = feb,
    year = {2021},
    note = {arXiv:1904.04792 [cs]},
    keywords = {Computer Science - Computation and Language, Question Answering},
}
@misc{jacob_consensus_2023,
    title = {The {Consensus} {Game}: {Language} {Model} {Generation} via {Equilibrium} {Search}},
    shorttitle = {The {Consensus} {Game}},
    url = {http://arxiv.org/abs/2310.09139},
    doi = {10.48550/arXiv.2310.09139},
    abstract = {When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and dialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially, improves performance over existing LM decoding procedures - on multiple benchmarks, we observe that applying EQUILIBRIUM-RANKING to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models. These results highlight the promise of game-theoretic tools for addressing fundamental challenges of truthfulness and consistency in LMs.},
    urldate = {2025-02-12},
    publisher = {arXiv},
    author = {Jacob, Athul Paul and Shen, Yikang and Farina, Gabriele and Andreas, Jacob},
    month = oct,
    year = {2023},
    note = {arXiv:2310.09139 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}
@misc{gemp_steering_2024,
    title = {Steering {Language} {Models} with {Game}-{Theoretic} {Solvers}},
    url = {http://arxiv.org/abs/2402.01704},
    doi = {10.48550/arXiv.2402.01704},
    abstract = {Mathematical models of interactions among rational agents have long been studied in game theory. However these interactions are often over a small set of discrete game actions which is very different from how humans communicate in natural language. To bridge this gap, we introduce a framework that allows equilibrium solvers to work over the space of natural language dialogue generated by large language models (LLMs). Specifically, by modelling the players, strategies and payoffs in a "game" of dialogue, we create a binding from natural language interactions to the conventional symbolic logic of game theory. Given this binding, we can ask existing game-theoretic algorithms to provide us with strategic solutions (e.g., what string an LLM should generate to maximize payoff in the face of strategic partners or opponents), giving us predictors of stable, rational conversational strategies. We focus on three domains that require different negotiation strategies: scheduling meetings, trading fruit and debate, and evaluate an LLM's generated language when guided by solvers. We see that LLMs that follow game-theory solvers result in dialogue generations that are less exploitable than the control (no guidance from solvers), and the language generated results in higher rewards, in all negotiation domains. We discuss future implications of this work, and how game-theoretic solvers that can leverage the expressivity of natural language can open up a new avenue of guiding language research.},
    urldate = {2025-02-12},
    publisher = {arXiv},
    author = {Gemp, Ian and Patel, Roma and Bachrach, Yoram and Lanctot, Marc and Dasagi, Vibhavari and Marris, Luke and Piliouras, Georgios and Liu, Siqi and Tuyls, Karl},
    month = dec,
    year = {2024},
    note = {arXiv:2402.01704 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Science and Game Theory},
}
@misc{arora_training_2025,
    title = {Training {Language} {Models} to {Reason} {Efficiently}},
    url = {http://arxiv.org/abs/2502.04463},
    doi = {10.48550/arXiv.2502.04463},
    abstract = {Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.},
    urldate = {2025-02-13},
    publisher = {arXiv},
    author = {Arora, Daman and Zanette, Andrea},
    month = feb,
    year = {2025},
    note = {arXiv:2502.04463 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Test Time Scaling},
}

@misc{compound-ai-blog,
  title={The Shift from Models to Compound AI Systems},
  author={Matei Zaharia and Omar Khattab and Lingjiao Chen and Jared Quincy Davis
          and Heather Miller and Chris Potts and James Zou and Michael Carbin
          and Jonathan Frankle and Naveen Rao and Ali Ghodsi},
  howpublished={\url{https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/}},
  year={2024}
}

@inproceedings{desai-durrett-2020-calibration,
    title = "Calibration of Pre-trained Transformers",
    author = "Desai, Shrey  and
      Durrett, Greg",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.21/",
    doi = "10.18653/v1/2020.emnlp-main.21",
    pages = "295--302",
    abstract = "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
}
@misc{akyurek_surprising_2024,
    title = {The {Surprising} {Effectiveness} of {Test}-{Time} {Training} for {Abstract} {Reasoning}},
    url = {http://arxiv.org/abs/2411.07279},
    doi = {10.48550/arXiv.2411.07279},
    abstract = {Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. We investigate the effectiveness of test-time training (TTT) -- updating model parameters temporarily during inference using a loss derived from input data -- as a mechanism for improving models' reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, we identify three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6x improvement in accuracy compared to base fine-tuned models; applying TTT to an 8B-parameter language model, we achieve 53\% accuracy on the ARC's public validation set, improving the state-of-the-art by nearly 25\% for public and purely neural approaches. By ensembling our method with recent program generation approaches, we get SoTA public validation accuracy of 61.9\%, matching the average human score. Our findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective.},
    urldate = {2025-01-16},
    publisher = {arXiv},
    author = {Akyürek, Ekin and Damani, Mehul and Qiu, Linlu and Guo, Han and Kim, Yoon and Andreas, Jacob},
    month = nov,
    year = {2024},
    note = {arXiv:2411.07279 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Programming},
}
@inproceedings{chen_uncertain_2020,
    address = {Online},
    title = {Uncertain {Natural} {Language} {Inference}},
    url = {https://aclanthology.org/2020.acl-main.774},
    doi = {10.18653/v1/2020.acl-main.774},
    abstract = {We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments. We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise. We describe a direct scalar regression modeling approach, and find that existing categorically-labeled NLI data can be used in pre-training. Our best models correlate well with humans, demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current NLI tasks.},
    urldate = {2024-02-21},
    booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
    publisher = {Association for Computational Linguistics},
    author = {Chen, Tongfei and Jiang, Zhengping and Poliak, Adam and Sakaguchi, Keisuke and Van Durme, Benjamin},
    editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
    month = jul,
    year = {2020},
    keywords = {Reasoning},
    pages = {8772--8779},
}

@article{jiang_addressing_2024,
    title = {Addressing the {Binning} {Problem} in {Calibration} {Assessment} through {Scalar} {Annotations}},
    volume = {12},
    issn = {2307-387X},
    url = {https://doi.org/10.1162/tacl_a_00636},
    doi = {10.1162/tacl_a_00636},
    abstract = {Computational linguistics models commonly target the prediction of discrete—categorical—labels. When assessing how well-calibrated these model predictions are, popular evaluation schemes require practitioners to manually determine a binning scheme: grouping labels into bins to approximate true label posterior. The problem is that these metrics are sensitive to binning decisions. We consider two solutions to the binning problem that apply at the stage of data annotation: collecting either distributed (redundant) labels or direct scalar value assignment.In this paper, we show that although both approaches address the binning problem by evaluating instance-level calibration, direct scalar assignment is significantly more cost-effective. We provide theoretical analysis and empirical evidence to support our proposal for dataset creators to adopt scalar annotation protocols to enable a higher-quality assessment of model calibration.},
    urldate = {2025-02-14},
    journal = {Transactions of the Association for Computational Linguistics},
    author = {Jiang, Zhengping and Liu, Anqi and Durme, Benjamnin Van},
    month = feb,
    year = {2024},
    keywords = {Confidence},
    pages = {120--136},
}
@article{mienye_performance_2021,
    title = {Performance analysis of cost-sensitive learning methods with application to imbalanced medical data},
    volume = {25},
    issn = {2352-9148},
    url = {https://www.sciencedirect.com/science/article/pii/S235291482100174X},
    doi = {10.1016/j.imu.2021.100690},
    abstract = {Many real-world machine learning applications require building models using highly imbalanced datasets. Usually, in medical datasets, the healthy patients or samples are dominant, making them the majority class, while the sick patients are few, making them the minority class. Researchers have proposed numerous machine learning methods to predict medical diagnosis. Still, the class imbalance problem makes it difficult for classifiers to adequately learn and distinguish between the minority and majority classes. Cost-sensitive learning and resampling techniques are used to deal with the class imbalance problem. This research focuses on developing robust cost-sensitive classifiers by modifying the objective functions of some well-known algorithms, such as logistic regression, decision tree, extreme gradient boosting, and random forest, which are then used to efficiently predict medical diagnosis. Meanwhile, as opposed to resampling techniques, our approach does not alter the original data distribution. Firstly, we implement the standard versions of these algorithms to provide a baseline for performance comparison. Secondly, we develop their corresponding cost-sensitive algorithms. For the proposed approaches, it is not necessary to change the distribution of the original data as the modified algorithms consider the imbalanced class distribution during training, thereby resulting in more reliable performance than when the data is resampled. Four popular medical datasets, including the Pima Indians Diabetes, Haberman Breast Cancer, Cervical Cancer Risk Factors, and Chronic Kidney Disease datasets, are used in the experiments to validate the performance of the proposed approach. The experimental results show that the cost-sensitive methods yield superior performance compared to the standard algorithms.},
    urldate = {2025-02-14},
    journal = {Informatics in Medicine Unlocked},
    author = {Mienye, Ibomoiye Domor and Sun, Yanxia},
    month = jan,
    year = {2021},
    keywords = {Cost-sensitive learning, Imbalanced classification, Machine learning, Medical diagnosis},
    pages = {100690},
}
@article{khan_cost_2017,
  author={Khan, Salman H. and Hayat, Munawar and Bennamoun, Mohammed and Sohel, Ferdous A. and Togneri, Roberto},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Cost-Sensitive Learning of Deep Feature Representations From Imbalanced Data}, 
  year={2018},
  volume={29},
  number={8},
  pages={3573-3587},
  keywords={Training;Australia;Neural networks;Computer vision;Tag clouds;Training data;Testing;Convolutional neural networks (CNNs);cost-sensitive (CoSen) learning;data imbalance;loss functions},
  doi={10.1109/TNNLS.2017.2732482}
}

@inproceedings{geifman_selective_2017,
    author = {Geifman, Yonatan and El-Yaniv, Ran},
    title = {Selective classification for deep neural networks},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2\% error in top-5 ImageNet classification can be guaranteed with probability 99.9\%, and almost 60\% test coverage.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {4885–4894},
    numpages = {10},
    location = {Long Beach, California, USA},
    series = {NIPS'17},
    url = {https://dl.acm.org/doi/10.5555/3295222.3295241}
}
    
@misc{verma_learning_2023,
    title = {Learning to {Defer} to {Multiple} {Experts}: {Consistent} {Surrogate} {Losses}, {Confidence} {Calibration}, and {Conformal} {Ensembles}},
    shorttitle = {Learning to {Defer} to {Multiple} {Experts}},
    url = {http://arxiv.org/abs/2210.16955},
    doi = {10.48550/arXiv.2210.16955},
    abstract = {We study the statistical properties of learning to defer (L2D) to multiple experts. In particular, we address the open problems of deriving a consistent surrogate loss, confidence calibration, and principled ensembling of experts. Firstly, we derive two consistent surrogates -- one based on a softmax parameterization, the other on a one-vs-all (OvA) parameterization -- that are analogous to the single expert losses proposed by Mozannar and Sontag (2020) and Verma and Nalisnick (2022), respectively. We then study the frameworks' ability to estimate P( m\_j = y {\textbar} x ), the probability that the jth expert will correctly predict the label for x. Theory shows the softmax-based loss causes mis-calibration to propagate between the estimates while the OvA-based loss does not (though in practice, we find there are trade offs). Lastly, we propose a conformal inference technique that chooses a subset of experts to query when the system defers. We perform empirical validation on tasks for galaxy, skin lesion, and hate speech classification.},
    urldate = {2025-02-14},
    publisher = {arXiv},
    author = {Verma, Rajeev and Barrejón, Daniel and Nalisnick, Eric},
    month = feb,
    year = {2023},
    note = {arXiv:2210.16955 [stat]},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}
@inproceedings{ren_out--distribution_2023,
    title={Out-of-Distribution Detection and Selective Generation for Conditional Language Models},
    author={Jie Ren and Jiaming Luo and Yao Zhao and Kundan Krishna and Mohammad Saleh and Balaji Lakshminarayanan and Peter J Liu},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=kJUS5nD0vPB}
}
@inproceedings{cao_learn_2024,
    address = {Miami, Florida, USA},
    title = {Learn to {Refuse}: {Making} {Large} {Language} {Models} {More} {Controllable} and {Reliable} through {Knowledge} {Scope} {Limitation} and {Refusal} {Mechanism}},
    shorttitle = {Learn to {Refuse}},
    url = {https://aclanthology.org/2024.emnlp-main.212/},
    doi = {10.18653/v1/2024.emnlp-main.212},
    abstract = {Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM`s understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.},
    urldate = {2025-02-14},
    booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
    publisher = {Association for Computational Linguistics},
    author = {Cao, Lang},
    editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
    month = nov,
    year = {2024},
    pages = {3628--3646},
}
@article{cao_generalizing_2022,
    title = {Generalizing {Consistent} {Multi}-{Class} {Classification} with {Rejection} to be {Compatible} with {Arbitrary} {Losses}},
    abstract = {Classification with rejection (CwR) refrains from making a prediction to avoid critical misclassification when encountering test samples that are difficult to classify. Though previous methods for CwR have been provided with theoretical guarantees, they are only compatible with certain loss functions, making them not flexible enough when the loss needs to be changed with the dataset in practice. In this paper, we derive a novel formulation for CwR that can be equipped with arbitrary loss functions while maintaining the theoretical guarantees. First, we show that K-class CwR is equivalent to a (K + 1)-class classification problem on the original data distribution with an augmented class, and propose an empirical risk minimization formulation to solve this problem with an estimation error bound. Then, we find necessary and sufficient conditions for the learning consistency of the surrogates constructed on our proposed formulation equipped with any classification-calibrated multi-class losses, where consistency means the surrogate risk minimization implies the target risk minimization for CwR. Finally, experimental results validate the effectiveness of our proposed method.},
    language = {en},
    author = {Cao, Yuzhou and Cai, Tianchi and Feng, Lei and Gu, Lihong and Gu, Jinjie and An, Bo and Niu, Gang and Sugiyama, Masashi},
    year = {2022},
}
@misc{alves_cost-sensitive_2024,
    title = {Cost-{Sensitive} {Learning} to {Defer} to {Multiple} {Experts} with {Workload} {Constraints}},
    url = {http://arxiv.org/abs/2403.06906},
    doi = {10.48550/arXiv.2403.06906},
    abstract = {Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key real-world aspects that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type I and type II errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset; and iii) not dealing with human work-capacity constraints. To address these issues, we propose the {\textbackslash}textit\{deferral under cost and capacity constraints framework\} (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost, subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work-capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average \$8.4{\textbackslash}\%\$ reduction in the misclassification cost. The code used for the experiments is available at https://github.com/feedzai/deccaf},
    urldate = {2025-02-14},
    publisher = {arXiv},
    author = {Alves, Jean V. and Leitão, Diogo and Jesus, Sérgio and Sampaio, Marco O. P. and Liébana, Javier and Saleiro, Pedro and Figueiredo, Mário A. T. and Bizarro, Pedro},
    month = aug,
    year = {2024},
    note = {arXiv:2403.06906 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{mozannar_consistent_2021,
    author = {Mozannar, Hussein and Sontag, David},
    title = {Consistent estimators for learning to defer to an expert},
    year = {2020},
    publisher = {JMLR.org},
    abstract = {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.},
    booktitle = {Proceedings of the 37th International Conference on Machine Learning},
    articleno = {656},
    numpages = {12},
    series = {ICML'20},
    url = {https://dl.acm.org/doi/10.5555/3524938.3525594}
}



@inproceedings{kamath-etal-2020-selective,
    title = "Selective Question Answering under Domain Shift",
    author = "Kamath, Amita  and
      Jia, Robin  and
      Liang, Percy",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.503/",
    doi = "10.18653/v1/2020.acl-main.503",
    pages = "5684--5696",
    abstract = "To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model`s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model`s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model`s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56{\%} of questions while maintaining 80{\%} accuracy; in contrast, directly using the model`s probabilities only answers 48{\%} at 80{\%} accuracy."
}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}."
}


@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}


@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@inproceedings{li_combining_2024,
    title={Combining Induction and Transduction for Abstract Reasoning},
    author={Wen-Ding Li and Keya Hu and Carter Larsen and Yuqing Wu and Simon Alford and Caleb Woo and Spencer M. Dunn and Hao Tang and Wei-Long Zheng and Yewen Pu and Kevin Ellis},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=UmdotAAVDe}
}