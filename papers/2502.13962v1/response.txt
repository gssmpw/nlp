\section{Related Work}
As scaling training compute has become prohibitively expensive **Vaswani et al., "Attention Is All You Need"** ____**, models that scale performance with test-time compute have become a new frontier ____**. These methods have delivered state-of-the-art results on hard reasoning tasks using lengthy chains of thought ____**. Current work in this space optimizes for question answering tasks which do not penalize incorrectness, ignoring settings that favor refusal over wrong answers ____**.
We draw motivation from methods for cost-sensitive learning **Zadrozny et al., "Learning to Classify with Margin"** ____ and selective classification ____**, which navigate penalties for failure. These setting reward confidence calibration, which can be critical for effective collaboration with human experts ____**. We are the first to investigate how serialized test-time compute helps models identify when they should not answer.
We include additional materials in \cref{sec:appendix} for further reading on these areas.