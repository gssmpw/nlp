\section{Related Work}
As scaling training compute has become prohibitively expensive \cite{hoffmann_training_2022}, models that scale performance with test-time compute have become a new frontier \cite{snell2024scalingllmtesttimecompute, wu_inference_2024}. These methods have delivered state-of-the-art results on hard reasoning tasks using lengthy chains of thought \cite{deepseek-ai_deepseek-r1_2025, muennighoff_s1_2025}. Current work in this space optimizes for question answering tasks which do not penalize incorrectness, ignoring settings that favor refusal over wrong answers \cite{ferrucci_building_2010, rajpurkar_know_2018, kamath-etal-2020-selective}.
We draw motivation from methods for cost-sensitive learning \cite{mienye_performance_2021} and selective classification \cite{geifman_selective_2017}, which navigate penalties for failure. These setting reward confidence calibration, which can be critical for effective collaboration with human experts \cite{verma_learning_2023}. We are the first to investigate how serialized test-time compute helps models identify when they should not answer.
We include additional materials in \cref{sec:appendix} for further reading on these areas.