

\begin{table*}[ht]
\captionsetup{type=figure}
    \centering
            \caption{Invariance ratios over different GNNs with RNI, on real-world datasets. Despite the ID resampling that is applied in every epoch, invariance is not achieved and, in most cases, does not increase during training.}
    \label{fig:invariance_curves}
    \begin{tabular}{>{\centering\arraybackslash}m{0.01\textwidth}>{\centering}m{0.21\textwidth}>{\centering\arraybackslash}m{0.21\textwidth}>{\centering\arraybackslash}m{0.21\textwidth}>{\centering\arraybackslash}m{0.21\textwidth}}

        \toprule
        & {ogbn-arxiv} & {ogbg-molhiv} & {ogbg-molbbbp} & {ogbg-molbace} \\
        \midrule
        \rotatebox{90}{{GraphConv}} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GraphConv_arxiv.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GraphConv_molhib.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GraphConv_molbbbp.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GraphConv_molbace.png} \\
        \midrule
        \rotatebox{90}{{GIN}} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GIN_arxiv.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GIN_molhib.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GIN_molbbbp.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GIN_molbace.png} \\
        \midrule
        \rotatebox{90}{{GAT}} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GAT_arxiv.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GAT_molhib.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GAT_molbbbp.png} &
        \includegraphics[width=0.24\textwidth]{figures/max_invariance_figs/max_rni_invariance_charts_GAT_molbace.png} \\
        \bottomrule
    \end{tabular}

\end{table*}





In this section, we review existing literature relevant to our study, focusing on the use of IDs in GNNs, and the learning of invariacnes in neural networks.


\subsection{Graph Neural Networks}
 The fundamental idea behind Graph Neural Networks (GNNs) \citep{kipf2017semisupervised,mpgnn,gat,graphsage} is to use neural networks that combine node features with graph structure to obtain useful graph representations. This combination is done in an iterative manner, which can capture complex properties of the graph and its node features. See \citet{wu2020comprehensive} for a recent surveys.


\subsection{Node Identifiers and Expressiveness in GNNs}
Many GNNs, such as MPGNNs, are inherently limited in their ability to distinguish between nodes with structurally similar neighborhoods. This limitation makes many GNNs only as powerful as the 1-Weisfeiler-Lehman (WL) isomorphism test \citep{morris2021weisfeiler}.


To address this limitation, the assignment of node IDs has emerged as a practical technique to enhance the expressiveness of GNNs \cite{sato2021randomfeaturesstrengthengraph, abboud2021surprisingpowergraphneural, Bouritsas_2023, pellizzoni2024on, gpse_canturk, eliasof2023graph}.
The addition of node IDs can assist the GNN to distinguish between nodes that would otherwise be indistinguishable by message-passing schemes.
For example, \citet{you2021identity} showed that expressiveness improves by adding binary node IDs.
Recently, \citet{pellizzoni2024on} suggested a method for assigning IDs using Tinhofer-based node ordering \cite{TINHOFER1991253, pellizzoni2024on}.
Another approach to add node IDs is to calculate new structural node features,Â such as counting triangles \cite{Bouritsas_2023} or computing Laplacian eigenvalues \cite{dwivedi2021generalizationtransformernetworksgraphs}, which can sometimes separate nodes that were otherwise indistinguishable.  
These approaches are deterministic, yet do not guarantee the uniqueness of node representation.
To achieve uniqueness almost surely, \citet{sato2021randomfeaturesstrengthengraph} and \citet{abboud2021surprisingpowergraphneural} suggested sampling random node features with sufficient dimension at every epoch. This approach was shown to make MPGNNs universal approximations of invariant and equivariant graph functions \cite{abboud2021surprisingpowergraphneural}.
In this work we focus on these non-deterministic approaches, and claim that the model should be invariant to the values of IDs. 

\subsection{Learning Invariances in Neural Networks}
Learning invariances in deep learning have been explored in various domains including Convolutional Neural Networks (CNNs) and GNNs \cite{Simard1998}. 
In CNNs, translational invariance is achieved by design. However, models such as Spatial Transformer Networks (STN) enable CNNs to handle more complex geometric transformations such as  invariances to rotations and zooming \citep{jaderberg2015spatial}. Furthermore, the Augerino approach, which learns invariances by optimizing over a distribution of data augmentations, is an effective strategy to learn invariances in tasks such as image classification \citep{benton2020learning}. TI-Pooling, introduced in transformation-invariant CNN architectures, further addresses the need for invariance in vision-based tasks \citep{laptev2016tipooling}.
In graph tasks, learning invariances has seen rapid development, particularly with GNNs. Traditionally, GNNs are designed to be permutation invariant, but recent research has also explored explicit mechanisms to learn invariances rather than relying solely on built-in properties. For example, \citet{xia2023learning} introduced a mechanism that explicitly learns invariant representations by transferring node representations across clusters, ensuring generalization under structure shifts in the test data. This method is particularly effective in scenarios where training distributions and test distributions may differ, a problem known as out-of-distribution generalization. 
\citet{jia2024graph}  proposed a strategy to extract invariant patterns by mixing environment-related and invariant subgraphs. 
In this work, we present \ourmethod which promotes ID invariance while maintaining their added expressive power.