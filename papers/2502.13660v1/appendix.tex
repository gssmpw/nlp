\onecolumn

\section{Proofs}
\subsection{Proof of Theorem~\ref{thm:invarinace_can_be_bad}}
We will show that there exists a function $f$  that is invariant to IDs with respect to one set of graphs \( S \) and non-invariant with respect to another set of graphs \( S' \). 

For a given graph $G=(V,E)$, we denote the unique identifier of node $v$ as \( \text{ID}_v \) for each node \( v \in V \). Let $P$ be any graph property that is ID-invariant, such as the existence of the Eulerian path. We consider the following function:

   \[
   f(G) = 
   \begin{cases} 
   P(G) & \text{if } G \in S \\ 
   \sum_{v \in V} \text{ID}_v & \text{else }
   \end{cases}
   \]


$f$ computes the function $P$ over the graphs in $S$, and returns the sum of IDs of $g$ otherwise. 
Summing the IDs is not invariant to the IDs' values. Therefore, $f$ is invariant to IDs with respect to graphs in $S$ but not with respect to the graphs in $S$.


\subsection{Proof of direct implication of Theorem~\ref{thm:invarinace_can_be_bad}}
In the main text we mention that a direct implication of Theorem~\ref{thm:invarinace_can_be_bad} states that a GNN can be ID-invariant to a train set and non ID-invariant to a test set. 
This setting only differs from the setting in Theorem~\ref{thm:invarinace_can_be_bad} in that the learned function is now restricted to one that can be realized by a GNN.
Indeed, as proven in~\citet{abboud2021surprisingpowergraphneural}, a GNN with IDs is a universal approximator. Specifically, with enough layers and width, an MPGNN can reconstruct the adjacency matrix in some hidden layer, and then compute over it the same function as in the proof of Theorem~\ref{thm:invarinace_can_be_bad}.

\newcommand{\GNNRit}[0]{\textit{GNN-R}}
\newcommand{\Git}[0]{\textit{GNN'}}
\subsection{Proof of Theorem~\ref{thm:mpnnwithidsepxressivity}}
To prove the theorem, we show that a GNN-R that is ID invariant in every layer is equivalent to a GNN without IDs with identical constant values on the nodes. We focus on the case where no node features exist in the data. For the sake of brevity, we assume, without loss of generality, that the IDs of GNN-R are of dimension $1$ and that the fixed constant features of the GNN without IDs are of the value $1$. We focus on Message-Passing GNNs.
Let $\GNNRit{}$ be a GNN-R with $L$ layers that is ID-invariant in every layer. Let $\Git{}$ be a regular GNN with $L$ layers that assigns constant $1$ features to all nodes. 
We denote a model up to its $l$'th layer as $A^{(l)}$.
We will prove by induction on the layers $l\in \{0,...,L-1\}$ that the output of every layer of $\GNNRit{}$ can be realized by the corresponding layers in $\Git{}$.
we denote the inputs after the assignments of values to nodes by the networks as $h'^{0}$ for $\GNNRit{}$ and  $h'^{0}$ for $\Git{}$.

Base Case - $l=0$: we need to show that there is $\Git{}^{(1)}$ such that $H'^{(1)} =  H^{(1)}$
As $l=0$ is a  ID-invariant layer, 
$\GNNRit{}^{(1)}(h^{0}) = \GNNRit{}^{(1)}(\bar{h}^{0}$ for any $\bar{h} \neq h$.
Specifically, $\GNNRit{}^{(1)}(h^{0}) = \GNNRit{}^{(1)}(h'^{0})$
Therefore we can have $\GNNRit{}^{(1)} = \Git{}^{(1)}$ and then $H'^{(1)} =  H^{(1)}$.

Assume that the statement is true up to the $L-2$ layer.
We now prove that there is $\GNNRit{}^{(L-1)}$ such that $H^{(L-1)} = H'^{(L-1)}$.

From the inductive assumption, there is $\Git{}^{(L-2)}$ such that $H^{(L-2)} = H'^{(L-2)}$. Let  us take such $G'{(L-2)}$. As the $L$'th layer is ID-invariant, it holds that $\GNNRit{}^{(L-2)}(h^{(L-1})) = \GNNRit{}^{(L-2)}(H'^{(L-2)})$. Therefore, we can have $\Git{}^{(L-2)} = \GNNRit{}^{(L-2)}$ and then $H^{(L-1)} = H'^{(L-1)}$.


\subsection{Proof of Theorem~\ref{thm:only_last_layer}}
To prove Theorem~\ref{thm:only_last_layer}, we will construct a GNN with three layers, where the first two layers are non ID-invariant, and the last layer is ID-invariant, that solves the isInTriangle task. It was already shown that isInTriangle cannot be solved by 1-WL GNNs~\citep{cyclesgnns}.

Let $G$ be a graph with $n$ nodes, each assigned with an ID. We assume for brevity that the IDs are of dimension $1$, and assume no other node features are associated with the nodes of $G$. 

We use the following notation: in every layer $i$, $f^{(i)}$, each node $v$ updates its representation $h_v^{(i-1)}$ by computing a message using a function $m^{(i)}$, aggregates the messages from its neighbors $N(v)$, $\{m^{(i)}(u)\}_{u \in N(v)}$, using a function $agg^{(i)}$, and updates its representation $h_v^{(i)} = \text{update}^{(i)}(m^{(i)}, agg^{(i)})$ by combining the outputs of $m^{(i)}$ and $agg^{(i)}$.

We now construct the GNN $f$ as follows.The inputs are $h_v^{(0)} = \text{ID}_v$.

In the first layer, the message function simply copies the ID of the node, i.e. $m^{(1)} = h_v^{(0)}$. The aggregation function, concatenates the messages, i.e. the IDs of the neighbors, in arbitrary order: $agg^{(1)} = \text{CONCAT}(\{m^{(1)}(u)\}_{u \in N(v)})$. Then the update function concatenates its own ID and the list of IDs of its neighbors: $\text{update}^{(1)} = \text{CONCAT}(m^{(1)}, agg^{(1)})$. Therefore, in the output of the first layer, the node's own ID is in position $0$ of the representation vector.

The second layer the message function is the identity function, $m^{(2)} = h_v^{(1)}$.
 The aggregation function, again concatenates the messages in arbitrary order: $agg^{(2)} = \text{CONCAT}(\{m^{(2)}(u)\}_{u \in N(v)})$. 
Then again the update function concatenates the message of the node with the output of the aggregation function: $\text{update}^{(2)} = \text{CONCAT}(m^{(2)}, agg^{(2)})$.
Therefore, at the output of the second layer, the first entry is the node's own ID, followed the the ID's of its direct neighbors, followed the lists of IDs of the neighbors of its neighbors. 

Notice that the first and second layers are not ID-invariant, as replacing the ID values will result in a different vector.

In the final layer, the message and aggregate functions are the same as in the second layer, i.e., $m^{(3)} =h_v^{(2)}$ and $agg^{(3)} = \text{CONCAT}(\{m^{(3)}(u)\}_{u \in N(v)})$.
The update function performs a matching between the ID of the node, which appears in the first entry of the message, $m^{(3)}[0$,
and the output of the aggregation function entries. This matching examines if the ID of the node appears in the messages from three-hop neighbors. If it does, this means the node is part of a triangle, as it sees its own ID again in 3 hops. Then $f$  outputs 1; otherwise, it outputs 0. This third layer is ID-invariant, as its outputs depend on the re-appearance of the same IDs, without dependency on their values. 



\subsection{Proof of Theorem~\ref{thm:matching_oracle}}
Let \( f \) be an equivariant function of graphs that is also ID-invariant. We will demonstrate that \( f \) can be expressed using a matching oracle. Let \( o \) be a matching oracle defined as follows:

\[
o(u, v) = 
\begin{cases} 
1 & \text{if } u = v \\ 
0 & \text{otherwise}
\end{cases}
\]

Assume we have a serial program that computes \( f \). We will compute \( f \) using a serial function \( g \) that utilizes the oracle \( o \). The function \( g \) incorporates caching  Let \( \text{Cache} \) denote a cache that stores nodes with an associated value to each node.

The function \( g \) operates as $f$, except for the follows:

\begin{enumerate}[label=(\alph*)]
    \item When \( f \) needs to access the ID of a node \( x \), $g$ checks whether \( x \) already exists in the cache by matching \( x \) with each node stored in the cache using the oracle \( o \).
    
    \item If \( x \) is found in the cache, \( g \) retrieves and returns the value associated with it.
    
    \item If \( x \) is not found in the cache, \( g \) adds it to the cache and assigns a new value to \( x \) as \( \text{ID}(x) = \text{size}(\text{Cache}) + 1 \), and return its value.
\end{enumerate}

By the assumption that \( f \) is invariant under IDs, we have \( g = f \).


 \section{Additional Experimental Details} \label{app:experimental_settings}


 \paragraph{OGB datasets}
The datasets' statistics are presented in Table~\ref{tab:ogb_summary}.
\begin{table}[ht]
    \centering
    \caption{Summary Statistics of OGB Datasets}
    \label{tab:ogb_summary}
    \vskip 0.15in
    \begin{tabular}{lccccc} 

\toprule
Dataset  & \# Graphs & Avg \# Nodes & Avg \# Edges & \# Node Features &\# Classes  \\ 


ogbn-arxiv &1& 169,343     & 1,166,243          & 128          & 40  \\
ogbg-molhiv & 41,127	& 25.5 & 27.5 & 9&2\\

ogbg-molbace &1,513 &34.1 &36.9 & 9&2 \\
ogbg-molbbbp & 2,039& 24.1& 26.0& 9& 2\\ 





\bottomrule
    \end{tabular}
\end{table}




 \paragraph{isInTriangle}
The isInTriangle task is a binary node classification where the goal is to determine whether a given node is part of a triangle.
The dataset consists of 100 graphs with 100 nodes each, generated using the preferential attachment (BA) model \citep{badist}, in which graphs are constructed by incrementally adding new nodes with $m$ edges and connecting them to existing nodes with a probability proportional to the degrees of those nodes. We adopt an inductive setting, where the graphs used during testing are not included in the training set. 
We used $m=2$ for all the training graphs.


  \paragraph{EXP and CEXP}
  We followed the protocol of \citet{abboud2021surprisingpowergraphneural} and used Adam optimizer with a learning rate of $1e-4$. The network has $8$ GraphConv layers followed by a sum-pooling and a $3$ layer readout function. All hidden layers have $64$ dimensions, and we used $64$ random features and we did not discard the original features in the data. 



\paragraph{Hyper-Parameters}
For all experiments, we use a fixed drouput rate of 0.1 and Relu activations.
We tuned the learning rate in $\{10^{-3}, 5\cdot 10^{-4}\}$, batch size in $\{32, 64\}$, number of layers in $\{3, 5\}$, and hidden dimensions in $\{32, 64\}$. 

