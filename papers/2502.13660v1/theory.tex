

In this section, we establish theoretical results on GNNs with IDs. We examine how  GNNs can be invariant to IDs while still leveraging their expressive potential.
Our objective is to derive practical guidelines for introducing ID invariance into the model. Due to space constraints, all proofs are deferred to the Appendix.

\paragraph{Preliminaries} Throughout this paper, we denote a graph by $G = (V, E)$ over $n$ nodes and its corresponding node features by ${X} \in \mathbb{R}^{n\times d}$, where \( d \) is the input feature dimension.
When there are no node features, we assume that a constant feature $1$ is assigned to the nodes, which is a common practice~\citep{gin, morris2021weisfeiler}.
We consider GNNs with \( L \) layers. The final layer is a classification/regression layer, depending on the task at hand, and is denoted by $g$.
We denote the embedding function realized by the $k$'th layer of a GNN by $GNN_k$.
We assume that the IDs are randomly generated by the model and augmented as features. We will refer to such GNNs as \textit{GNN-R}.
We use the following definition for ID-invariance:
\begin{definition}[ID-invariant function]
    Let $f$ be a function over a graph $G$, feature values $X$ and node identifiers $I$.  $f$ is \textit{ID-invariant} if  the output for a given $G$ and $X$ does not depend on the values of $I$, as long as $I$ is a unique identifier. We also say that a function is invariant with respect to a set of graphs $S$, if the definition above holds for all $(G,X)\in S$.
\end{definition}


We shall be specifically interested in the case where layers of a GNN are invariant. Thus, we say that the $k$'th layer of a GNN is ID invariant if $GNN_k(G;X;I)$ is ID invariant. Namely, the layer is invariant if the embedding it produces does not change when $I$ changes. 

Note that a GNN-R that is not ID-invariant always has a non-zero probability of error at the test time.
Therefore, ID-invariant solutions are preferred. Note also that a function can be ID-invariant with respect to some graphs, but non-ID-invariant with respect to others as the following theorem shows:
\begin{theorem}\label{thm:invarinace_can_be_bad}
    A function $f$ can be ID-invariant with respect to a set of graphs $S$ and non-ID-invariant with respect to another set of graphs $S'$.
\end{theorem}
To prove Theorem~\ref{thm:invarinace_can_be_bad} we construct an ID-invariant function that is graph-dependent. 
A direct implication of Theorem~\ref{thm:invarinace_can_be_bad} is that it is possible for a GNN-R to be ID-invariant on training data, but non-invariant on test data, leading to potentially high test error.

The next theorem shows that enforcing invariance to IDs within every GNN layer, i.e., such that $GNN_k$ is ID-invariant for every $k$,  does not enhance the model's expressiveness at all, at least for Message-Passing GNNs.
%\amir{we don't properly define invariance in a layer. I think we should provide some notations for GNNs (eg embedding at layer $k$) and define invariance based on these.}

\begin{theorem}\label{thm:mpnnwithidsepxressivity}
Let $\mathbb{G}$ be the set of Message-Passing GNN models without IDs,\footnote{For comparable architecture to a GNN-R, for GNN without IDs we use a GNN that has a fixed ID as input, i.e., a constant feature for all nodes.} and $\mathbb{G}'$ the set of Message-Passing GNN-R models that are ID-invariant in every layer, with the same parameterization as $\mathbb{G}$. Then the set of functions realized by $\mathbb{G}$ and $\mathbb{G}'$ are the same.
\end{theorem}

To prove Theorem~\ref{thm:mpnnwithidsepxressivity} we take a GNN-R and a GNN without IDs with the same parameterization and show by induction that the output of every layer of the GNN-R can be realized by the corresponding layers in the GNN without IDs (the reverse containment is clear).

Theorem~\ref{thm:mpnnwithidsepxressivity} implies that in order to design a network that is both ID-invariant and expressive, we must allow it to {\em not} be invariant to IDs in at least one hidden layer.
The next theorem shows that three layers and only enforcing invariance in the last layer of Message-Passing GNNs, already provide the network with expressive power higher than 1-WL.
\begin{theorem}\label{thm:only_last_layer}
There exists a function $f(G;X)$ over graphs that no message passing GNN without IDs can realize, but $f(G;X)$ can be realized by an ID-invariant GNN-R with $3$ layers where only the last layer is ID-invariant.
\end{theorem}

To prove Theorem~\ref{thm:only_last_layer} we construct an ID-invariant solution for the task of detecting a triangle. We show that for this task, on the third layer of the network the node only has to match its ID with previously seen IDs, which is an ID-invariant action.


% Finally, we state what is the expressive power limitation of a GNN-R that is ID-invariant. 

% \begin{theorem}\label{thm:isomorphism}
% A GNN-R that is ID-invariant and runs in polynomial time, is not a universal approximator of equivariant graph functions, unless graph isomorphism is NP-complete.
% \end{theorem}

% Theorem \ref{thm:isomorphism} follows from the fact that a GNN-R that is ID-invariant, can solve graph isomorphism, which is believed to be NP-intermediate \citep{chen2023equivalencegraphisomorphismtesting}.

In the next section, we build upon the results discussed in this section to design an approach that enhances the utilization of IDs in GNNs by explicitly enforcing ID-invariance to the model.