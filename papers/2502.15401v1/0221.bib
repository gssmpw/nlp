% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{human_tuning,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@article{gpt3,
  title={A comprehensive capability analysis of gpt-3 and gpt-3.5 series models},
  author={Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang and Gong, Chao and Shen, Yang and others},
  journal={arXiv preprint arXiv:2303.10420},
  year={2023}
}

@inproceedings{chatgpt,
  title={ChatGPT: Applications, opportunities, and threats},
  author={Bahrini, Aram and Khamoshifar, Mohammadsadra and Abbasimehr, Hossein and Riggs, Robert J and Esmaeili, Maryam and Majdabadkohne, Rastin Mastali and Pasehvar, Morteza},
  booktitle={2023 Systems and Information Engineering Design Symposium (SIEDS)},
  pages={274--279},
  year={2023},
  organization={IEEE}
}

@inproceedings{cite1,
  title={Reasoning with Language Model is Planning with World Model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua and Wang, Zhen and Wang, Daisy and Hu, Zhiting},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={8154--8173},
  year={2023}
}

@article{iclsurvey1,
  title={The learnability of in-context learning},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{iclsurvey2,
  title={In-context learning with retrieved demonstrations for language models: A survey},
  author={Xu, Xin and Liu, Yue and Pasupat, Panupong and Kazemi, Mehran and others},
  journal={arXiv preprint arXiv:2401.11624},
  year={2024}
}

@article{complex2,
  title={Cumulative reasoning with large language models},
  author={Zhang, Yifan and Yang, Jingqin and Yuan, Yang and Yao, Andrew Chi-Chih},
  journal={arXiv preprint arXiv:2308.04371},
  year={2023}
}

@article{understandicl,
  title={Understanding in-context learning in transformers and llms by learning to learn discrete functions},
  author={Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  journal={arXiv preprint arXiv:2310.03016},
  year={2023}
}

@inproceedings{understandicl2,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}


@article{sft1,
  title={How abilities in large language models are affected by supervised fine-tuning data composition},
  author={Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2310.05492},
  year={2023}
}

@inproceedings{rl1,
  title={Guiding pretraining in reinforcement learning with large language models},
  author={Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, C{\'e}dric and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={8657--8677},
  year={2023},
  organization={PMLR}
}

@article{deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@inproceedings{smallsft1,
  title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alex and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={8003--8017},
  year={2023}
}


@article{lessismore,
  title={LIMO: Less is More for Reasoning},
  author={Ye, Yixin and Huang, Zhen and Xiao, Yang and Chern, Ethan and Xia, Shijie and Liu, Pengfei},
  journal={arXiv preprint arXiv:2502.03387},
  year={2025}
}

@inproceedings{firstfewshot,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{revisiting,
  title={Revisiting demonstration selection strategies in in-context learning},
  author={Peng, Keqin and Ding, Liang and Yuan, Yancheng and Liu, Xuebo and Zhang, Min and Ouyang, Yuanxin and Tao, Dacheng},
  journal={arXiv preprint arXiv:2401.12087},
  year={2024}
}


@inproceedings{kate,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, William B and Carin, Lawrence and Chen, Weizhu},
  booktitle={Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
  pages={100--114},
  year={2022}
}

@inproceedings{beforeuse,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International conference on machine learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

@inproceedings{smallchange,
  title={In-Context Demonstration Selection with Cross Entropy Difference},
  author={Iter, Dan and Pryzant, Reid and Xu, Ruochen and Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1150--1162},
  year={2023}
}

@inproceedings{order,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8086--8098},
  year={2022}
}

@article{pca,
  title={Principal component analysis},
  author={Abdi, Herv{\'e} and Williams, Lynne J},
  journal={Wiley interdisciplinary reviews: computational statistics},
  volume={2},
  number={4},
  pages={433--459},
  year={2010},
  publisher={Wiley Online Library}
}

@article{similarity,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{similarity2,
  title={Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering},
  author={Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1423--1436},
  year={2023}
}

@inproceedings{similarity3,
  title={How Do In-Context Examples Affect Compositional Generalization?},
  author={An, Shengnan and Lin, Zeqi and Fu, Qiang and Chen, Bei and Zheng, Nanning and Lou, Jian-Guang and Zhang, Dongmei},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11027--11052},
  year={2023}
}
@inproceedings{perplexity,
  title={Demystifying Prompts in Language Models via Perplexity Estimation},
  author={Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A and Zettlemoyer, Luke},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={10136--10148},
  year={2023}
}

@inproceedings{perplexity2,
  title={Active Learning Principles for In-Context Learning with Large Language Models},
  author={Margatina, Katerina and Schick, Timo and Aletras, Nikolaos and Dwivedi-Yu, Jane},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={5011--5034},
  year={2023}
}
@article{qdmr,
  title={Break it down: A question understanding benchmark},
  author={Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={183--198},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{cot_future1,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}
@article{curriculum_survey,
  title={A survey on curriculum learning},
  author={Wang, Xin and Chen, Yudong and Zhu, Wenwu},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={9},
  pages={4555--4576},
  year={2021},
  publisher={IEEE}
}

@inproceedings{curriculum2,
  title={On the power of curriculum learning in training deep networks},
  author={Hacohen, Guy and Weinshall, Daphna},
  booktitle={International conference on machine learning},
  pages={2535--2544},
  year={2019},
  organization={PMLR}
}

@article{humancurriculum,
  title={Instruction tuning with human curriculum},
  author={Lee, Bruce W and Cho, Hyunsoo and Yoo, Kang Min},
  journal={arXiv preprint arXiv:2310.09518},
  year={2023}
}

@article{medicalcurriculum,
  title={Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering},
  author={Yang, Yushi and Bean, Andrew M and McCraith, Robert and Mahdi, Adam},
  journal={arXiv preprint arXiv:2408.07888},
  year={2024}
}

@article{datasetdecomposition,
  title={Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum},
  author={Pouransari, Hadi and Li, Chun-Liang and Chang, Jen-Hao Rick and Vasu, Pavan Kumar Anasosalu and Koc, Cem and Shankar, Vaishaal and Tuzel, Oncel},
  journal={arXiv preprint arXiv:2405.13226},
  year={2024}
}

@article{iccl,
  title={Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning},
  author={Liu, Yinpeng and Liu, Jiawei and Shi, Xiang and Cheng, Qikai and Lu, Wei},
  journal={arXiv preprint arXiv:2402.10738},
  year={2024}
}

@article{image_curriculum,
  title={Stc: A simple to complex framework for weakly-supervised semantic segmentation},
  author={Wei, Yunchao and Liang, Xiaodan and Chen, Yunpeng and Shen, Xiaohui and Cheng, Ming-Ming and Feng, Jiashi and Zhao, Yao and Yan, Shuicheng},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={11},
  pages={2314--2320},
  year={2016},
  publisher={IEEE}
}

@inproceedings{noise_curriculum,
  title={Webly supervised learning of convolutional networks},
  author={Chen, Xinlei and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1431--1439},
  year={2015}
}

@inproceedings{textlength_curriculum,
  title={Competence-based Curriculum Learning for Neural Machine Translation},
  author={Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Pocz{\'o}s, Barnab{\'a}s and Mitchell, Tom},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1162--1172},
  year={2019}
}

@inproceedings{score_curriculum,
  title={Image difficulty curriculum for generative adversarial networks (CuGAN)},
  author={Soviany, Petru and Ardei, Claudiu and Ionescu, Radu Tudor and Leordeanu, Marius},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={3463--3472},
  year={2020}
}

@inproceedings{lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{aqua,
  title={Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={158--167},
  year={2017}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{svamp,
  title={Are NLP Models really able to Solve Simple Math Word Problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2080--2094},
  year={2021}
}

@inproceedings{commonsenseqa,
  title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4149--4158},
  year={2019}
}

@article{strategyqa,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{votek,
  title={Selective annotation makes language models better few-shot learners},
  author={Hongjin, SU and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{autocot,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@inproceedings{promptso,
  title={Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models},
  author={Shi, Fobo and Qing, Peijun and Yang, Dong and Wang, Nan and Lei, Youbo and Lu, Haonan and Lin, Xiaodong and Li, Duantengchuan},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={1836--1862},
  year={2024}
}

@inproceedings{saicl,
  title={Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering},
  author={Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1423--1436},
  year={2023}
}

@inproceedings{alicl,
  title={Active Learning Principles for In-Context Learning with Large Language Models},
  author={Margatina, Katerina and Schick, Timo and Aletras, Nikolaos and Dwivedi-Yu, Jane},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={5011--5034},
  year={2023}
}

@inproceedings{promot_retrieve,
  title={Learning To Retrieve Prompts for In-Context Learning},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2655--2671},
  year={2022}
}

@article{demonote,
  title={Demonstration Notebook: Finding the Most Suited In-Context Learning Example from Interactions},
  author={Tang, Yiming and Dong, Bin},
  journal={arXiv preprint arXiv:2406.10878},
  year={2024}
}

@inproceedings{asdiv,
  title={A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers},
  author={Miao, Shen-yun and Liang, Chao-Chun and Su, Keh-Yih},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={975--984},
  year={2020}
}

@inproceedings{mawps,
  title={MAWPS: A math word problem repository},
  author={Koncel-Kedziorski, Rik and Roy, Subhro and Amini, Aida and Kushman, Nate and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies},
  pages={1152--1157},
  year={2016}
}

@article{cot,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{longicl,
  title={In-context learning with long-context models: An in-depth exploration},
  author={Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham},
  journal={arXiv preprint arXiv:2405.00200},
  year={2024}
}

@article{defineicl,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}


@article{relatedwork_icl1,
  title={In-context example selection with influences},
  author={Nguyen, Tai and Wong, Eric},
  journal={arXiv preprint arXiv:2302.11042},
  year={2023}
}

@inproceedings{relatedwork_icl3,
  title={What Makes a Good Order of Examples in In-Context Learning},
  author={Guo, Qi and Wang, Leiyu and Wang, Yidong and Ye, Wei and Zhang, Shikun},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={14892--14904},
  year={2024}
}

@inproceedings{relatedwork_icl2,
  title={Finding Support Examples for In-Context Learning},
  author={Li, Xiaonan and Qiu, Xipeng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={6219--6235},
  year={2023}
}

@article{relatedwork_cl1,
  title={Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning},
  author={Kim, Jisu and Lee, Juhwan},
  journal={arXiv preprint arXiv:2405.07490},
  year={2024}
}

@inproceedings{relatedwork_cl2,
  title={Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models},
  author={Wang, Xin and Zhou, Yuwei and Chen, Hong and Zhu, Wenwu},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={1306--1310},
  year={2024}
}

@inproceedings{diverse1,
  title={Diverse Demonstrations Improve In-context Compositional Generalization},
  author={Levy, Itay and Bogin, Ben and Berant, Jonathan},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1401--1422},
  year={2023}
}

@article{diverse2,
  title={DemoShapley: Valuation of Demonstrations for In-Context Learning},
  author={Xie, Shan and Luo, Man and Stern, Chadly Daniel and Du, Mengnan and Cheng, Lu},
  journal={arXiv preprint arXiv:2410.07523},
  year={2024}
}
@inproceedings{llamafactory,
    title = "{L}lama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models",
    author = "Zheng, Yaowei  and
      Zhang, Richong  and
      Zhang, Junhao  and
      YeYanhan, YeYanhan  and
      Luo, Zheyan",
    editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.38",
    doi = "10.18653/v1/2024.acl-demos.38",
    pages = "400--410",
    abstract = "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.",
}
