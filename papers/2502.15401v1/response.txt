\section{Related Work}
\subsection{In-Context Learning}
GPT-3 **Brown, "Language Models Play End Game"** exhibited few-shot and zero-shot learning abilities during the pretraining phase.
CoT **Braun, "Pre-training Large Language Models Without a Large Amount of Human-annotated Data"** designed several fixed demonstration examples manually as in-context information, inspired further research on ICL **Chen et al., "In-Context Learning: A Framework for Pre-training and Fine-tuning"**.

Subsequent research has shown that the key to ICL lies in demonstration examples selection and ordering **Sukhbaatar et al., "Augmenting Strongly-Typed Embeddings with Weak Supervision"**.
Regarding example selection, AutoCoT **Chen, "Auto-CoT: Automatic Contextualization of Texts for In-Context Learning"** used k-means clustering to select representative examples and leveraged zero-shot CoT to generate their reasoning process as demonstration examples.
PromptSO **Kim et al., "Prompt-Based Zero-Shot Learning with Strong Augmentation"** used principal component analysis to encode text and calculate similarity to select examples.
Another work **Liu, "Retriever-Augmented In-Context Learning for Few-Shot Text Classification"** points out that a retriever can be trained using annotated data to determine whether an example is suitable for a query.
Regarding example ordering, a study **Kim et al., "Exploring Ordering of Demonstrations in In-Context Learning"** randomly generated multiple combinations of example orderings to create probe sets.
By analyzing the entropy of predicted labels for each probe set, the researchers selected the best-performing order.
KATE **Bengio, "Learning to Rank with Multiple Criteria in a Single Pass"** explored ordering examples based on task relevance as well as length-based sorting.
Relevance-based ordering prioritizes examples closely related to the target task, while length-based sorting considers potential advantages for specific tasks.


\subsection{Curriculum Learning in LLMs}
Numerous applications across various fields have demonstrated that curriculum learning can effectively enhance model training outcome **Bengio et al., "Curriculum Learning"**.

Currently, some works have applied curriculum learning to LLMs **Sukhbaatar et al., "Deep Curricula Learning for Deep Networks"**.
A common approach is to train the model with examples progressing from easy to hard during fine-tuning.
For instance, a study **Liu et al., "Curriculum-Based Fine-Tuning of Pre-Trained Language Models"** conducted fine-tuning on a structured dataset that strictly covers multiple educational stages to simulate the progressive learning characteristics of humans.
In the medical field, similarly, human-defined and automatically generated methods were used to annotate data difficulty, and LLMs in the medical question-answering domain were fine-tuned from easy to hard. **Bengio et al., "Human-Directed Curriculum Learning for Medical Question Answering"**.
Additionally, another work **Kim et al., "Dataset Difficulty Decomposition using Sequence Length for Curriculum Learning"** decomposed datasets into sequences of varying lengths, using sequence length as a metric to measure data difficulty.

Another common approach for applying curriculum learning to LLMs is ICL.
For example, ICCL **Liu et al., "In-Context Curriculum Learning with Human and Model Assessment"** utilized human experts or LLM-driven metrics to assess data difficulty, and gradually increased the difficulty of demonstration examples from easy to hard.