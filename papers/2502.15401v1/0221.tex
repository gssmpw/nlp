% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% 匿名和最终版
% \usepackage[review]{acl}
\usepackage[preprint]{acl}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{array}  % 需要引入 array 包以使用 m{}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{makecell}  % 使用 makecell 宏包
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
% \usepackage[usenames,dvipsnames]{xcolor}

% \usepackage[ruled]{algorithm2e}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Problem-Solving Logic Guided Curriculum In-Context Learning \\for LLMs Complex Reasoning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Xuetao Ma\thanks{~Equal contribution}, Wenbin Jiang\footnotemark[1], Hua Huang\thanks{~Corresponding author: Hua Huang} \\
%         School of Artificial Intelligence, Beijing Normal University, Beijing, China \\ maxuetao@mail.bnu.edu.cn, \{jiangwenbin, huahuang\}@bnu.edu.cn
%         }
\author{Xuetao Ma, Wenbin Jiang, Hua Huang\thanks{~Corresponding author: Hua Huang} \\
        School of Artificial Intelligence, Beijing Normal University, Beijing, China \\ maxuetao@mail.bnu.edu.cn, \{jiangwenbin, huahuang\}@bnu.edu.cn
        }
\begin{document}
\maketitle
\begin{abstract}
In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples.
Previous methods typically relied on simple features to measure the relevance between examples.
We argue that these features are not sufficient to reflect the intrinsic connections between examples.
In this study, we propose a curriculum ICL strategy guided by problem-solving logic.
We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples.
Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps.
In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts.
Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs.
Our project will be publicly available subsequently.

\end{abstract}


\section{Introduction}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{./psl_and_icl.pdf}
  \caption{(a) The transformation from QDMR to problem-solving logic. (b) An example of curriculum ICL. Example selection depends on the similar problem-solving logic, and example ordering depends on the number of operations contained in the logic.
  }
  \label{psl_and_icl}
\end{figure}

% ICL的作用
Large language models (LLMs)~\cite{human_tuning,gpt3,chatgpt} can rapidly acquire new capabilities through in-context learning (ICL) to solve many new tasks~\cite{iclsurvey1,iclsurvey2}, and can be extended through chain of thought (CoT)~\cite{cot} to solve many tasks that require complex reasoning~\cite{cite1,complex2}.
% ICL的原理
Researchers believe that through ICL, LLMs can implicitly learn the problem-solving patterns demonstrated in contextual examples and apply them to new tasks~\cite{understandicl,understandicl2}.
This means that LLMs have the ability to learn and apply problem-solving patterns on the spot from given examples.


% SFT和RL背景下，ICL的重要性
In recent years, supervised fine-tuning (SFT) methods~\cite{sft1} and reinforcement learning optimization reasoning methods~\cite{rl1,deepseek} have been able to significantly enhance the reasoning abilities of LLMs through training.
% attracting widespread attention from researchers.
% They can internalize stronger problem-solving abilities into LLMs through the training process.
Despite this, due to the unique characteristic of ICL that it can enhance problem-solving capabilities without training, it still holds value as significant as the methods mentioned above,
especially when facing the need to reduce costs or quickly apply to new tasks.
% ICL仍有提升空间
Relevant work~\cite{smallsft1} has already shown that LLMs possess a wealth of basic knowledge and fundamental capabilities that can be effectively activated through a small number of examples.
Particularly, LIMO~\cite{lessismore} fine-tuned a large language model with only a few hundred examples and achieved results that are close to or even on par with the current state-of-the-art reinforcement learning optimization inference.
Therefore, we believe that the ICL capabilities of current LLMs are still far from being fully realized.
There is a need to design better prompts to effectively enhance the effectiveness of ICL.


% ICL的学习类似于人类学习的过程
ICL learns demonstration examples in sequence and then solves problems, which closely resembles the process of humans learning knowledge step by step.
We believe that organizing demonstration examples in a way similar to human educational curriculum construction is crucial.
It helps LLMs learn the knowledge and patterns shown in the examples and solve given problems effectively.
Therefore, strategies for curriculum learning~\cite{curriculum} can be adopted for the organization of demonstration examples.
% \textcolor{red}{This process involves example selection and ordering, with the main reference indicators being the support degree of the demonstration examples for problem solving and the complexity of the demonstration themselves samples.
The key to ICL lies in example selection and ordering, which requires measuring the relevance between examples.
% 传统统计指标的问题
Traditional simple statistical information, such as similarity~\cite{similarity,similarity2,similarity3} and perplexity~\cite{perplexity,perplexity2}, is not sufficient to reflect the intrinsic connections between examples, especially from the perspective of problem-solving.

% 我们提出的方法
In this work, we innovatively propose an problem-solving logic guided curriculum ICL method, which constructs the optimal ICL prompt for the query based on problem-solving logic.
% 什么是问题求解逻辑
The Question Decomposition Meaning Representation (QDMR)~\cite{qdmr} decomposes complex problems into several sub-questions for solving and formalizes these sub-questions with 13 custom "operations", which we refer to as \textit{problem-solving logic}.
Figure~\ref{psl_and_icl}-(a) shows an example of problem decomposition and transformation into problem-solving logic.
Although it cannot directly solve the problem, the problem-solving logic describes the steps required for solving and the order of these steps in formal language.
Therefore, it can accurately measure the intrinsic connections between examples and construct a sequence of demonstration examples that are conducive to problem-solving.
Figure~\ref{psl_and_icl}-(b) shows an example of curriculum ICL.
We select examples with similar problem-solving logic, which can help LLMs learn how to solve similar problems.
Subsequently, we measure the difficulty of these examples by the number of problem-solving steps.
The greater the number of steps, the more reasoning steps are involved, meaning the problem is more difficult to solve.
Relying on the principles of curriculum learning, we order these examples from easy to hard to serve as the final in-context prompt.

Our main contributions are as follows:

(1) This paper proposes a problem-solving logic guided curriculum ICL strategy to enhance the reasoning performance of LLMs.
We innovatively present problem-solving logic as the criterion for selection and ordering demonstration examples, which is expected to offer a novel perspective for future work.

(2) We constructed a problem-solving logic instruction set based on the BREAK dataset.
Based on this, we fine-tuned a language model to automatically analyze the problem-solving logic of input questions.

(3) Extensive experiments are conducted on five datasets, and results show that our method achieves significant improvements in average performance and efficiency across all datasets, surpassing previous ICL methods and effectively enhancing the ability of LLMs in reasoning tasks.


\section{Background}
\subsection{In-Context Learning}
% 什么是上下文学习
ICL is a capability that emerges as the training data and scale of LLMs increase~\cite{defineicl}.
This allows LLMs to learn new tasks with only a few examples.
% 上下文学习的一般形式
Examples generally contain questions and answers.
The query needs to maintain consistent formatting with the examples so that LLMs can provide accurate responses.
This process is called few-shot.



Existing research shows that the key to enhancing ICL performance lies in the organization of demonstration examples, that is, the selection and ordering of examples.
Taking text similarity as an example, the general process is to encode the candidate examples and the query into vector forms, and then select the examples most similar to the query by calculating the similarity between vectors.
Subsequently, these examples are sorted according to text similarity.
Finally, the sorted examples are then input into the LLMs together with the query for solving.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\linewidth]{./qdmr_example.pdf}
  \caption{A QDMR example. The original question is decomposed into four sub-questions, each represented by an operation.}
  \label{qdmr}
\end{figure}

\subsection{Problem-Solving Logic}
QDMR is a general method for decomposing complex questions into several sub-questions for solving.
They manually designed 13 operations, with each sub-question represented by an operation.
The researchers proposed the BREAK dataset through manual annotation, which contains 60K question-answer pairs.
Specific examples of each operation, as well as detailed information about the dataset, can be found in the Appendix~\ref{append1}.

This work is inspired by QDMR and refers to the sequence of operators representing sub-questions as the \textit{problem-solving logic}.
The set of sub-questions decomposed by QDMR includes the required steps and the order between steps.
Figure~\ref{qdmr} shows a specific QDMR   example.
The original question is split into four sub-questions, each of which is described in a formal language with an operation, resulting in the corresponding problem-solving logic as follows:
\[
\mathrm{select} \rightarrow \mathrm{project} \rightarrow \mathrm{group} \rightarrow \mathrm{superlative}
\]

\subsection{Curriculum Learning}
Curriculum learning is a machine learning strategy~\cite{curriculum}.
It suggests that the training process should mimic human cognitive learning by starting with simple examples and gradually increasing in difficulty.
The core of this method lies in how to measure the difficulty of examples, which often depends on the characteristics of the specific task.
For example, in the field of computer vision, the number of objects in an image~\cite{image_curriculum} or noise~\cite{noise_curriculum} contained can be used to measure difficulty.
In the field of natural language processing, sentence length~\cite{textlength_curriculum} can be used as a measure of difficulty.
In addition to these, the difficulty can also be measured by human educational level~\cite{humancurriculum} or evaluation models~\cite{score_curriculum}.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./workflow.pdf}
  \caption{The overall flowchart of our method. First, a base LLM is fine-tuned using an instruction set for problem-solving logic (PSL) constructed from the BREAK dataset. Then, suitable demonstration examples are selected and ordered by analyzing the PSL of the candidate examples and the query. Finally, the selected demonstration examples and the query form the full prompt, which is fed into the LLM to obtain the results.}
  \label{workflow}
\end{figure*}

\section{Problem-Solving Logic Guided Curriculum ICL}
This paper introduces a problem-solving logic guided curriculum ICL strategy.
The overall methodology is illustrated in Figure~\ref{workflow}.
Specifically, we first constructed an instruction set based on the BREAK dataset and fine-tuned a language model to automatically analyze problem-solving logic.
Then, we analyzed the problem-solving logic for all data in the benchmark training set to construct a dataset of candidate examples.
When an actual query is input, its problem-solving logic is first analyzed and then compared with the candidate examples, selecting those with similar problem-solving steps as demonstration examples.
Furthermore, the number of problem-solving steps serves as an appropriate metric for assessing the difficulty of each example.
A greater number of steps means the problem is more difficult to solve.
This inspired us to apply the principles of curriculum learning to order the demonstration examples from easy to hard.
Finally, the ordered demonstration examples and the query are combined to form the final prompt, which is then input into the LLMs.
The following sections will offer a detailed explanation of how problem-solving logic is analyzed, along with the process of selecting and ordering demonstration examples.


\subsection{Problem-Solving Logic Analysis}
We first need to train a language model to analyze the problem-solving logic, which is represented as an ordered set of several problem-solving steps.

Our approach constructs an instruction set based on the BREAK dataset.
Specifically, the input to the instruction set is a problem, and the output is problem-solving logic and its formal language.
The formal language ensures that the model correctly understands the problem-solving process.
We then fine-tune a Llama3-8B model~\cite{llama2, llama3} with LoRA~\cite{lora} on this instruction set.
Once the model is trained, it can analyze problems from any dataset and extract their problem-solving logic.
Examples of the instruction set can be found in the Appendix~\ref{append1}. Details of fine-tuning and hyperparameters can be found in the Appendix~\ref{finetune}.


Analyzing the problem-solving logic is a crucial step in our work, providing the foundation for the subsequent curriculum ICL.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{./selection_and_ordering.pdf}
  \caption{
    The process of example selection and ordering.
    $(\checkmark)$ denotes similar problem-solving logic, $(\times)$ indicates a matching failure, and \textcolor{red}{\textbf{red}} font indicates the reason for the matching failure.
    \textcolor[RGB]{125,93,149}{\textbf{Difficulty}} is measured by the number of steps.
  }
  \label{selection_and_ordering}
\end{figure}

\subsection{Curriculum ICL}
Based on the above problem analysis process, we can focus on problem-solving logic to guide the selection and ordering of demonstration examples.
Figure~\ref{selection_and_ordering} illustrates the process of example selection and ordering.

\subsubsection{Demonstration Example Selection}
First, we need to select appropriate demonstration examples.
Compared to semantic information, we believe that selecting examples with similar problem-solving logic is more important.
On one hand, similar problem-solving logic can guide LLMs in reasoning, and on the other hand, examples with similar logic but different semantics can enhance the model's generalization ability.

\begin{algorithm}[!ht]
  \caption{Demonstration Example Selection}
  \label{selection}
  \begin{algorithmic}[1]
  \REQUIRE query $T$, LLM function $F(\cdot)$, set of candidate examples $\{E_1, E_2, \dots, E_n\}$, each example $E_i$ has its own solution logic $L_i = \{O_{i1}, O_{i2}, \dots, O_{im_i}\}$.
  \ENSURE Mark matching demonstration examples.
  \STATE $L_T \gets F(T)$ \COMMENT{Obtain the solution logic for the query from LLM}
  \FOR{each example $E_i$ in $\{E_1, E_2, \dots, E_n\}$}
      \STATE $L_i \gets \{O_{i1}, O_{i2}, \dots, O_{im_i}\}$ \COMMENT{Retrieve solution logic of $E_i$}
      \IF{$L_i$ is a subsequence of $L_T$ starting from the first operator}
          \STATE Mark $E_i$ as a demonstration example
      \ENDIF
  \ENDFOR
  \end{algorithmic}
\end{algorithm}

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./cicl.pdf}
  \caption{
    A complete example of curriculum ICL. The selected examples form the context information. The right half of the figure shows the problem-solving logic, which is the basis for example selection and ordering.}
  \label{icl}
\end{figure*}

After analyzing the query and all candidate examples, our method selects demonstration examples based on the problem-solving logic.
The selection criterion requires that the problem-solving operations set in each candidate example must be a subsequence of the query, meaning both the types of operations and their order must match exactly.
Suppose the query has a problem-solving logic containing $m$ operations, and the selected demonstration example has $n$ operations $(m \geq n)$; the $n$ operations of the demonstration example must match the first $n$ operations of the query. 
This method ensures that the demonstration example's problem-solving steps align with the first $n$ steps of the query, avoiding any mismatch or additional problem-solving steps.
The complete process is detailed in Algorithm~\ref{selection}.




\subsubsection{Demonstration Examples Ordering}
The key to curriculum learning lies in how to measure the difficulty of examples.
By introducing problem-solving logic, we can easily assess the difficulty of each example.
The problem-solving logic consists of several operations, where a higher number of operations indicates more reasoning steps, thereby increasing the problem's difficulty.

Inspired by this, we applied curriculum learning principles, ordering examples from easy to hard.
Specifically, we sorted the examples in increasing order based on the number of problem-solving steps, and used them along with the query to construct the final in-context prompt.
Figure~\ref{icl} shows a complete curriculum ICL example, including demonstration examples and the query.


\begin{table*}[!ht]
  \begin{small}
  \centering
  \begin{tabular}{ccccccccc}
  \hline
  \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\makecell{\textbf{Selection} \\ \textbf{Stategy}}} & \multirow{2}{*}{\makecell{\textbf{Ordering} \\ \textbf{Stategy}}} & \multicolumn{5}{c}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Avg.}} \\ 
  \cline{4-8}
  &  & & \textbf{SVAMP}  &\textbf{AQuA} & \textbf{Gsm8k} & \textbf{ComSenQA} & \textbf{StrategyQA} & \\ \hline
  % Example data
  
  % \rowcolor{gray!20} \textbf{\textit{ICL Methods}} &  &  &  &  &  & & \\
  Random       & Random   &  Random  &76.5\%  & 46.5\%  & 73.8\%  & 75.8\%  & 65.1\% & 67.53\%  \\
  
  VoteK       & KNN &  Similarity  &74.9\%  & 44.9\%  & 76.7\%  & 75.4\%  & 69.0\% & 68.19\%  \\
  
  PromptSO      & PCA   &  Eigenvalue  & 77.3\%  & 43.7\%  & 77.7\%  & 75.6\%  & 67.7\% & 68.40\%  \\
  
  AutoCoT       & K-means  & Similarity &77.5\%  & 47.2\%  & 75.3\%  & 76.0\%  & \underline{71.2\%} & 69.44\%  \\
  
  CoT + Fewshot & Fixed    &  Fixed  &\underline{80.5\%}  & 44.5\% & \underline{79.4\%}  & 75.1\%  & 69.4\% & 69.79\%  \\
  
  SA-ICL      &  KNN  &  Entropy  &78.8\%  & \underline{47.6\%}  & 77.9\%  & \textbf{78.5\%}  & 66.8\% & 69.95\%  \\
  
  AL-ICL   & KNN & Similarity &80.8\%  & 45.7\%  & 78.2\%  & \underline{77.9\%}  & 68.1\% & \underline{70.13\%}  \\
  % \rowcolor[HTML]{FFCCC9}
  Ours  & PSL  & Curriculum & \textbf{83.4\%} & \textbf{50.8\%} & \textbf{81.1\%} & 75.0 & \textbf{71.6\%} & \textbf{72.37\%}  \\
  
  \hline
  \end{tabular}
  \caption{\label{table1}
  The table presents a comparison of experimental results across different benchmarks using Llama3-8B, demonstrating the accuracy contrast between various ICL methods.
  \textbf{Avg} represents the average accuracy across the different benchmarks.
  The best and second-best performances are highlighted in \textbf{bold} and \underline{underlined}, respectively.}
  \end{small}
  \end{table*}

\section{Experiments and Analysis}

\subsection{Experimental Setup}
\paragraph{Benchmarks.}
Our experiment includes two types of datasets, Arithmetic Reasoning and Commonsense Reasoning, and validation is conducted on five different datasets.
Arithmetic Reasoning: (1) the AQuA \cite{aqua} includes 254 test examples, (2) the SVAMP \cite{svamp} includes 1000 test examples, (3) the Gsm8k includes 1319 test examples. 
Commonsense Reasoning: (1) the CommonsenseQA \cite{commonsenseqa} includes 1211 test examples, (2) the StrategyQA \cite{strategyqa} includes 229 test examples.


\paragraph{Baselines.}
We compare our approach against seven methods that use ICL.
Random selects demonstration examples and their order randomly.
VoteK \cite{votek} selects the most similar k examples using k-nearest neighbors (KNN) and sorts them according to similarity scores.
PromptSO \cite{promptso} uses principal component analysis~\cite{pca} to select the most relevant basis questions and sorts them based on eigenvalue.
AutoCoT \cite{autocot} uses k-means to automatically select the most representative examples that are closest to the cluster center.
CoT+few-shot \cite{cot} manually designed fixed demonstration examples with reasoning processes.
Self-Adaption ICL (SA-ICL)~\cite{saicl} selects similar examples based on KNN and then chooses an appropriate order based on information compression.
Active Learning ICL (AL-ICL) \cite{alicl} selects most similar examples based on the principles of active learning and sorts them according to similarity.


\paragraph{Implement Details.}
We evaluate the effectiveness of our method on the Llama3-8B model. For each benchmark, we select demonstration examples from its training set to form prompt information to evaluate each test set data.
For the SVAMP dataset, we adopted the same evaluation strategy as in previous work~\cite{svamp}, using ASDiv-a~\cite{asdiv} and MAWPS~\cite{mawps} together as the training set.
To ensure a fair comparison, the number of selected examples is based on the settings in CoT \cite{cot} for different benchmarks, and our experiments do not exceed that limit.

\begin{table*}[!ht]
  \begin{small}
  \centering
  \begin{tabular}{cccccccc}
  \hline
  
  \multirow{2}{*}{\makecell{\textbf{Difficulty} \\ \textbf{Strategy}}} & \multirow{2}{*}{\makecell{\textbf{Ordering}}} &\multicolumn{5}{c}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Avg.}} \\ 
  \cline{3-7}
  &  & \textbf{SVAMP}  &\textbf{AQuA} & \textbf{Gsm8k} & \textbf{ComSenQA} & \textbf{StrategyQA} & \\ \hline
  
  \rowcolor{gray!20} 
  \textbf{\textit{Original Llama}} &  &  &  &  &  & & \\
  
  % CoT + few-shot& Fixed  & 80.5\% & 44.5\% & 79.4\% & 75.1\% & 69.4\% & 69.79\%  \\
  AL-ICL   &   &80.8\%  & 45.7\%  & 78.2\%  & \textbf{77.9\%}  & 68.1\% & 70.13\%  \\
  
  \rowcolor{gray!20} \textbf{\textit{Our Strategy}} &  &  &  &  &  & & \\
  Prioritize simplicity & w/ order &82.3\% & 47.6\% & 79.5\% & 75.5\% & 69.0\% & 70.79\%  \\
                        & w/o order &\underline{82.5\%} & 47.2\% & 78.8\% & 76.1\% & 68.1\% & 70.55\%  \\
  
  Prioritize difficulty & w/ order &81.8\% & 44.9\% & 77.9\% & 76.6\% & 67.7\% & 69.77\%  \\
                        & w/o order &81.6\% & 46.1\% & 79.6\% & \underline{77.0\%} & 67.2\% & 70.29\%  \\
  
  Select Randomly       & w/ order &81.3\% & \underline{50.6\%} & \underline{80.2\%} & 76.1\% & 70.3\% & \underline{71.70\%}  \\
                        & w/o order &80.9\% & 48.6\% & 79.2\% & 76.0\% & \underline{71.2\%} & 71.17\%  \\
  
  Prioritize diversity  & w/ order &\textbf{83.4\%} & \textbf{50.8\%} & \textbf{81.1\%} & 75.0\% & \textbf{71.6\%} & \textbf{72.37\%}  \\
                        & w/o order &80.5\% & 46.1\% & 80.1\% & 76.0\% & 65.9\% & 70.11\%  \\
  
  \hline
  \end{tabular}
  \caption{\label{table2}
    The table presents the accuracy of benchmarks under different difficulty selection strategies. "w/ order" indicates that the examples are ordered based on curriculum learning, while "w/o order" means the examples are randomly ordered.
    The best and second-best performances are highlighted in \textbf{bold} and \underline{underlined}, respectively.
    }
  \end{small}
  \end{table*}

\subsection{Main Results and Analysis}
We compare the performance of our approach with other ICL methods.
All the comparison rusults are tabulated in Table~\ref{table1}.
Experimental results show that compared with other ICL methods, we achieve the best performance on SVAMP, AQuA, Gsm8k and StrategyQA.
Overall, our method improves the average accuracy of all benchmarks by 2.24\%.
This result shows that our method effectively improves the model's reasoning performance.
To further demonstrate the effectiveness of the method, we conducted multiple sets of experiments for illustration.


\begin{figure*}[t]
  \includegraphics[width=0.45\linewidth]{./left.pdf} \hfill
  \includegraphics[width=0.45\linewidth]{./right.pdf}
  
\caption{\label{analysis}
(a) shows the relationship between the average standard deviation of different example selection strategies and their performance across various benchmarks.
(b) shows the impact of example ordering strategies on performance in relation to the average standard deviation under different selection strategies.}
\end{figure*}


\subsubsection{Analysis of Example Selection and Ordering}
For the selection and ordering strategies of demonstration examples in ICL, we designed several sets of experiments to verify the effectiveness of our method.

Regarding example selection, since each query may match far more examples than the specified limit during the problem-solving logic analysis, it is necessary to analyze specific difficulty sampling strategies.
We designed four difficulty sampling strategies: 
(1) \textbf{Prioritize simplicity}: This strategy selects easy examples first.
(2) \textbf{Prioritize difficulty}: This strategy selects difficult examples first.
(3) \textbf{Select randomly}: This strategy randomly selects examples of any difficulty.
(4) \textbf{Prioritize diversity}: This strategy aims to select as many difficulty levels as possible, sampling at most one example from each difficulty level.


\begin{table*}[!ht]
  \centering
      \begin{tabular}{cccccccc}
      \hline
      \multirow{2}{*}{\textbf{Strategy}} & \multicolumn{5}{c}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Avg.}} & \multirow{2}{*}{\textbf{Time}} \\ 
      \cline{2-6}
      & \textbf{SVAMP} & \textbf{AQuA} & \textbf{Gsm8k} & \textbf{ComSenQA} & \textbf{StrategyQA} & \\ \hline
      Fixed Examples        & 8 & 4 & 8 & 7 & 6 & 6.60 & 109\% \\
      Prioritize simplicity & 7.27 & 4 & 7.73 & 7 & 5.88 & 6.38 & 117\% \\
      Prioritize difficulty & 7.27 & 4 & 7.15 & 5.82 & 5.84 & 6.02 & 167\% \\
      Select Randomly       & 7.49 & 4 & 7.73 & 7 & 5.88 & 6.42 & 144\% \\
      \rowcolor[HTML]{FFCCC9}
      Prioritize diversity  & 2.16 & 3.19 & 3.38 & 3.19 & 1.8 & 2.74 & 100\% \\\hline
      \end{tabular}
  \caption{\label{table3}
  The number of demonstration examples selected by different selection strategies in benchmarks.~\textbf{Avg} represents the average number of demonstration examples selected for each data. \textbf{Time} indicates the time cost comparison across different strategies.
  The highlighted part represent the strategy with most efficient.}
  \end{table*}

Regarding the ordering of examples, to validate the effectiveness of curriculum learning, we designed two sets of controlled experiments.
Under the four sampling strategies mentioned above, we applied two ordering strategy: (1) \textbf{difficulty increasing ordering (w/ order)} and (2) \textbf{random ordering (w/o order)}.

The complete experimental results are shown in Table~\ref{table2}, and through analysis, we have made the following observations:

First, it can be noted from the table that the performance of the strategies using the problem-solving logic and curriculum learning approach generally outperforms AL-ICL.
The prioritize diversity (w/ order) strategy significantly outperforms the others, achieving an average accuracy of 72.37\%.

Furthermore, the importance of curriculum learning is highlighted in our findings.
For prioritize diversity strategies, the effect of ordering is particularly pronounced.
In contrast, the impact of ordering is less significant for the prioritize simplicity and prioritize difficulty strategies.


Based on the findings above and considering the characteristics of different selection strategies, we believe that the primary reason for these results is data diversity, or more specifically, difficulty diversity.
To explain this phenomenon, we calculated the difficulty levels included in the demonstration examples for each data across all benchmarks and computed the average standard deviation.
Standard deviation (std) is typically used to measure the degree of variation, and this metric helps illustrate the data diversity produced by different strategies.


We analyzed two sets of data: first, the relationship between difficulty diversity and strategy performance; and second, the impact of difficulty diversity on the four strategies, considering both the cases with and without ordering.

Figure~\ref{analysis}-(a) depicts the relationship between performance and difficulty diversity across the four selection strategies.
There is a clear positive correlation between difficulty diversity and performance, suggesting that data diversity is key to improving performance.
Additionally, Figure \ref{analysis}-(b) shows the relationship between the performance difference (with and without ordering) and difficulty diversity across the four selection strategies.
We found that ordering strategies are highly sensitive to difficulty diversity.
Overall, the higher the difficulty diversity, the greater the improvement brought by ordering. Notably, the prioritize diversity strategy saw the largest performance improvement with ordering.
This highlights the effectiveness of curriculum learning, where it is essential to order data according to difficulty.
At the same time, it supports the idea that measuring example difficulty by the number of problem-solving steps is a valid approach.


\subsubsection{Analysis of the Number of Examples}
The number of demonstration examples for each query also has an important impact on the performance of ICL, as well as on the reasoning efficiency of LLMs.
Table~\ref{table3} presents the number of demonstration examples included with each test data across different strategies.
For comparison, we use the fixed number of examples in CoT~\cite{cot} as a reference.

We find that the prioritize diversity strategy has significantly superior performance while also having the least average number of demonstration examples.
The average number of demonstration examples for other strategies is more than 6, while priority diversity strategy only requires 2.74.
Fewer examples indicate a shorter in-context length, which helps the reasoning speed of LLMs.
Table~\ref{table3} also presents the average time cost under different strategies.
We uses the priority diversity strategy as the baseline at 100\% to measure the time cost of other strategies.
Experimental results show that, compared to other strategies, the prioritize diversity strategy has a time cost advantage, reducing consumption by 9\% to 67\%, effectively improving inference performance.

Current studies have shown that an increase in the number of demonstration examples usually leads to improved performance~\cite{longicl}.
Our method demonstrates that the quantity of examples is not the only influencing factor.
This conclusion is consistent with numerous studies~\cite{diverse1,diverse2,revisiting}, which indicate that data diversity plays a critical role in enhancing the generalization capability of LLMs.

\section{Related Work}

\subsection{In-Context Learning}
GPT-3 \cite{firstfewshot} exhibited few-shot and zero-shot learning abilities during the pretraining phase.
CoT~\cite{cot} designed several fixed demonstration examples manually as in-context information, inspired further research on ICL~\cite{cot_future1}.

Subsequent research has shown that the key to ICL lies in demonstration examples selection and ordering~\cite{relatedwork_icl1,relatedwork_icl2,relatedwork_icl3}.
Regarding example selection, AutoCoT~\cite{autocot} used k-means clustering to select representative examples and leveraged zero-shot CoT to generate their reasoning process as demonstration examples.
PromptSO~\cite{promptso} used principal component analysis~\cite{pca} to encode text and calculate similarity to select examples.
Another work~\cite{promot_retrieve} points out that a retriever can be trained using annotated data to determine whether an example is suitable for a query.
Regarding example ordering, a study~\cite{order} randomly generated multiple combinations of example orderings to create probe sets.
By analyzing the entropy of predicted labels for each probe set, the researchers selected the best-performing order.
KATE~\cite{kate} explored ordering examples based on task relevance as well as length-based sorting.
Relevance-based ordering prioritizes examples closely related to the target task, while length-based sorting considers potential advantages for specific tasks.


\subsection{Curriculum Learning in LLMs}
Numerous applications across various fields have demonstrated that curriculum learning can effectively enhance model training outcome~\cite{curriculum2,curriculum_survey}.

Currently, some works have applied curriculum learning to LLMs~\cite{relatedwork_cl1, relatedwork_cl2}.
A common approach is to train the model with examples progressing from easy to hard during fine-tuning.
For instance, a study~\cite{humancurriculum} conducted fine-tuning on a structured dataset that strictly covers multiple educational stages to simulate the progressive learning characteristics of humans.
In the medical field, similarly, human-defined and automatically generated methods were used to annotate data difficulty, and LLMs in the medical question-answering domain were fine-tuned from easy to hard.~\cite{humancurriculum}.
Additionally, another work~\cite{datasetdecomposition} decomposed datasets into sequences of varying lengths, using sequence length as a metric to measure data difficulty.

Another common approach for applying curriculum learning to LLMs is ICL.
For example, ICCL~\cite{iccl} utilized human experts or LLM-driven metrics to assess data difficulty, and gradually increased the difficulty of demonstration examples from easy to hard.


\section{Conclusion}
This paper proposes a problem-solving logic guided ICL strategy. 
By analyzing the problem-solving logic, we measure the similarity between problems and select demonstration examples.
Additionally, the difficulty of problems is assessed based on the number of problem-solving steps, and the selected examples are ordered from easy to hard following the principles of curriculum learning.
% Compared to ICL methods based on statistical metrics, our approach more effectively selects and organizes examples.
Experimental results across multiple benchmarks demonstrate that our proposed method outperforms other ICL methods in terms of average performance, significantly improving the reasoning capabilities of LLMs.


\section*{Limitations}
Although our work improves the performance and efficiency of LLMs in reasoning tasks, there are still limitations for improvement.
First, due to hardware resource constraints, we only conducted experiments on LLMs at the 8B scale, and further validation of our method is necessary on larger models, such as those at the 70B scale, to fully demonstrate its effectiveness.
On the other hand, we observed in many-shot studies~\cite{longicl} that a significant increase in the number of examples leads to substantial improvements in reasoning performance.
However, due to the limitations of benchmarks and hardware resources, we were unable to evaluate the effect of curriculum learning when applied to a large number of examples.
We believe that when both the quantity and quality of examples are ensured, reasoning performance can be further improved, which will be a focus of our future work.

\section*{Potential Risks}
Our work does not carry any obvious risks.

\section*{Acknowledgements}

\bibliography{0221}

\appendix

\section{BREAK Dataset Description}\label{append1}

BREAK is a dataset proposed by the Allen Institute~\cite{qdmr}.
This work introduces the Question Decomposition Meaning Representation (QDMR), which breaks down a question into several sub-questions for solving and represents it as a sequence of steps.
The dataset collects 60,150 question and QDMR pairs from several public datasets. To represent various questions as a unified sequence of steps, they customized 13 types of operations, converting the solution process for all questions into sequences of these operations.
The specific operations and their templates are shown in Table~\ref{operators}.
The decomposition and formalization of questions can be found in Figure~\ref{psl_and_icl} and Figure~\ref{qdmr}.
Table~\ref{operator_prevalence} shows the distribution of operations in the BREAK dataset, that is, the proportion of each operation appearing in a single data point.
Table 6 shows the distribution of the total number of sub-questions after decomposition in the dataset.

Based on the BREAK dataset, we constructed an instruction set to analyze the problem-solving logic. Specific examples and explanations of the instruction set are provided in Table~\ref{instructionset}.

\section{Fine-Tuning Details}\label{finetune}

We performed LoRA fine-tuning on the Llama3-8B model using the aforementioned instruction set. The specific hyperparameters are as follows:
the \texttt{cutoff\_len} is set to 1024, the learning rate is set to $5 \times 10^{-5}$, the fine-tuning parameters are specified as \texttt{all}, \texttt{lora\_rank} is set to 8, \texttt{lora\_alpha} is set to 16, the optimizer used is AdamW, the model is trained for 4 epochs, and the best model is selected based on the BLEU score.

\section{Prompt Template}\label{append2}
Table~\ref{break_prompt} shows the prompt templates used for fine-tuning problem-solving logic analysis.

\noindent
Table~\ref{svamp_prompt}--\ref{sqa_prompt} shows the full prompt example for in-context learning on the different benchmarks.



\section{Supplementary Details}\label{append3}
Our experiments utilized the llama-factory~\cite{llamafactory} project, which includes model fine-tuning and in-context learning. The CPU used in the experiments is an Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz, and the GPU is an NVIDIA Tesla A800 80G. The hyperparameters were set according to the default configuration file provided by llama-factory. The prompt length was set to 4096, and the maximum answer output length was set to 1024. To ensure output stability, the temperature was set to 0.01.
In our study, we used ChatGPT to assist in coding.

\begin{table*}[t!]
  \begin{center}
  \scriptsize
  \begin{tabular}{p{1.5cm} p{4cm} p{4.5cm} p{4.5cm}}
  \hline \bf Operator & \bf Template / Signature & \bf Question & \bf Decomposition  \\ \hline
  \bf Select & Return [entities] \newline $\texttt{w} \rightarrow \texttt{S$_\texttt{e}$}$ & How many touchdowns were scored overall? &  1. Return touchdowns \newline 2. Return the number of \#1 \\ \hline
  \bf Filter & Return [ref] [condition] \newline $\texttt{S$_\texttt{o}$}\texttt{,} \texttt{w} \rightarrow \texttt{S$_\texttt{o}$}$ & I would like a flight from Toronto to San Diego please. & 1. Return flights \newline 2. Return \#1 from Toronto \newline 3. Return \#2 to San Diego \\ \hline
  \bf Project & Return [relation] of [ref] \newline $\texttt{w}\texttt{,} \texttt{S$_\texttt{e}$} \rightarrow \texttt{S$_\texttt{o}$}$ & Who is the head coach of the Los Angeles Lakers? & 1. Return the Los Angeles Lakers \newline 2. Return the head coach of \#1 \\ \hline
  \bf Aggregate & Return [aggregate] of [ref] \newline $\texttt{w}_{\texttt{agg}}\texttt{,} \texttt{S$_\texttt{o}$} \rightarrow \texttt{n}$ & How many states border Colorado? & 1. Return Colorado  \newline 2. Return border states of \#1 \newline 3. Return the number of \#2 \\ \hline
  \bf Group & Return [aggregate] [ref1] for each [ref2] \newline $\texttt{w}_{\texttt{agg}}\texttt{,} \texttt{S$_\texttt{o}$,} \texttt{S$_\texttt{e}$} \rightarrow \texttt{S$_\texttt{n}$}$ & How many female students are there in each club? & 1. Return clubs \newline
  2. Return female students of \#1 \newline
  3. Return the number of \#2 for each \#1  \\ \hline
  \bf Superlative & Return [ref1] where [ref2] is [highest / lowest] \newline $\texttt{S$_\texttt{e}$,} \texttt{S$_\texttt{n}$,} \texttt{w}_{\texttt{sup}} \rightarrow \texttt{S$_\texttt{e}$}$ & What is the keyword, which has been contained by the most number of papers?  & 1. Return papers \newline 
  2. Return keywords of \#1 \newline
  3. Return the number of \#1 for each \#2 \newline
  4. Return \#2 where \#3 is highest \\ \hline
  \bf Comparative & Return [ref1] where [ref2] [comparison] [number] \newline $\texttt{S$_\texttt{e}$,} \texttt{S$_\texttt{n}$,} \texttt{w}_{\texttt{com}}\texttt{,} \texttt{n} \rightarrow \texttt{S$_\texttt{e}$}$ & Who are the authors who have more than 500 papers? & 1. Return authors \newline
  2. Return papers of \#1 \newline
  3. Return the number of \#2 for each of \#1 \newline
  4. Return \#1 where \#3 is more than 500 \\ \hline
  \bf Union & Return [ref1] , [ref2] \newline $\texttt{S$_\texttt{o}$,} \texttt{S$_\texttt{o}$} \rightarrow \texttt{S$_\texttt{o}$}$ & Tell me who the president and vice-president are? & 1. Return the president \newline
  2. Return the vice-president \newline
  3. Return \#1 , \#2  \\ \hline
  \bf Intersection & Return [relation] in both [ref1] and [ref2] \newline $\texttt{w,} \texttt{S$_\texttt{e}$,} \texttt{S$_\texttt{e}$} \rightarrow \texttt{S$_\texttt{o}$}$ & Show the parties that have representatives in both New York state and representatives in Pennsylvania state. & 1. Return representatives \newline 2. Return \#1 in New York state \newline 3. Return \#1 in Pennsylvania state \newline 4. Return parties in  both \#2 and \#3 \\ \hline
  \bf Discard & Return [ref1] besides [ref2] \newline $\texttt{S$_\texttt{o}$,} \texttt{S$_\texttt{o}$} \rightarrow \texttt{S$_\texttt{o}$}$ & Find the professors who are not playing Canoeing. & 1. Return professors \newline 2. Return \#1 playing Canoeing \newline 3. Return \#1 besides \#2 \\ \hline
  \bf Sort & Return [ref1] sorted by [ref2] \newline $\texttt{S$_\texttt{e}$,} \texttt{S$_\texttt{n}$} \rightarrow \texttt{$\langle \texttt{e}_1...\texttt{e}_k \rangle$}$ & Find all information about student addresses, and sort by monthly rental. & 1. Return students \newline 2. Return addresses of \#1 \newline 3. Return monthly rental of  \#2 \newline 4. Return \#2 sorted by \#3
   \\ \hline
  \bf Boolean & Return [if / is] [ref1] [condition] [ref2] \newline $\texttt{S$_\texttt{o}$,} \texttt{w,} \texttt{S$_\texttt{o}$} \rightarrow \texttt{b}$ & Were Scott Derrickson and Ed Wood of the same nationality? & ... \newline 3. Return the nationality of \#1 \newline 4. Return the nationality of \#2 \newline 5. Return if \#3 is the same as \#4 \\ \hline
  \bf Arithmetic & Return the [arithmetic] of [ref1] and [ref2] \newline $\texttt{w}_{\texttt{ari}}\texttt{,} \texttt{n,} \texttt{n} \rightarrow \texttt{n}$ & How many more red objects are there than blue objects? & ... \newline 3. Return the number of \#1 \newline 4. Return the number of \#2 \newline 5. Return the difference of \#3 and \#4 \\ \hline
  \end{tabular}
  \end{center}
  \caption{\label{operators} The 13 operator types of QDMR steps. Listed are, the natural language template used to express the operator, the operator signature and an example question that uses the query operator in its decomposition.}
  \end{table*}

  \begin{table}[t]
      \begin{center}
      \normalsize
      \begin{tabular}{cc}
      \hline \bf Operator & \bf QDMR \\ \hline
      \bf\texttt{SELECT} & 100\% \\ 
      \bf\texttt{PROJECT} & 69.0\% \\ 
      \bf\texttt{FILTER} & 53.2\%  \\ 
      \bf\texttt{AGGREGATE} & 38.1\%  \\ 
      \bf\texttt{BOOLEAN} & 30.0\% \\ 
      \bf\texttt{COMPARATIVE} & 17.0\% \\ 
      \bf\texttt{GROUP} & 9.7\%  \\ 
      \bf\texttt{SUPERLATIVE} & 6.3\% \\ 
      \bf\texttt{UNION} & 5.5\% \\ 
      \bf\texttt{ARITHMETIC} & 5.4\% \\ 
      \bf\texttt{DISCARD} & 3.2\% \\ 
      \bf\texttt{INTERSECTION} & 2.7\% \\ 
      \bf\texttt{SORT} & 0.9\% \\ \hline
      Total & 60,150 \\ \hline
      \end{tabular}
      \end{center}
      \caption{\label{operator_prevalence} 
      Operator prevalence in BREAK, that is, the proportion of each operator appearing in a single data point.
      }
  \end{table}
  
  \begin{table}[t]
    \begin{center}
    \normalsize
    \begin{tabular}{cc}
    \hline \bf Steps & \bf QDMR \\ \hline
    1-2 & 10.7\% \\ 
    3-4 & 44.9\% \\ 
    5-6 & 27.0\% \\ 
    7-8 & 10.1\% \\ 
    9+ & 7.4\% \\ \hline
    
    \end{tabular}
    \end{center}
    \caption{\label{qdmr_lengths} 
    The distribution of the total number of QDMR sub-questions.
  }
  \end{table}

  \begin{table*}[h!]
    \centering
    \begin{tabular}{p{13cm}}
    \hline
    \textbf{Input} \\ 
    \hline
    \noindent\textit{\textbackslash{}\textbackslash{}The input is a problem to be solved, such as:}\\
    what flights are available tomorrow from denver to philadelphia? \\ 
    \hline   
    \textbf{Label} \\ 
    \hline
    \textit{
    \noindent\textbackslash{}\textbackslash{}
    The label contains <operator> and <formal language>. \newline
    \textbackslash{}\textbackslash{}
    <operator> is an ordered set composed of the aforementioned custom operations. \newline
    \textbackslash{}\textbackslash{}
    <formal language> is the formalized language that provides a detailed description of each operator.
    }

    \textless operators\textgreater: ['select', 'filter', 'filter', 'filter'] \newline \textless formal language\textgreater: ["SELECT['flights']", "FILTER['\#1', 'from denver']", "FILTER['\#2', 'to philadelphia']", "FILTER['\#3', 'if available']"] \\ 
    \hline
    \end{tabular}
    \caption{\label{instructionset}
    Examples and Explanation of Instruction Sets Based on the BREAK Dataset
}
\end{table*}

  \begin{table*}[h!]
    \centering
    \begin{tabular}{p{13cm}}
    \hline
    \textbf{Prompt} \\ 
    \hline
    You are a helpful assistant. Please break down in order the operations \textless operations\textgreater{} required to solve the following problems, and the process of solving the problem according to the operations \textless programs\textgreater{}: \newline what flights are available tomorrow from denver to philadelphia? \\ 
    \hline   
    \textbf{Label} \\ 
    \hline
    \textless operators\textgreater: ['select', 'filter', 'filter', 'filter'] \newline \textless formal language\textgreater: ["SELECT['flights']", "FILTER['\#1', 'from denver']", "FILTER['\#2', 'to philadelphia']", "FILTER['\#3', 'if available']"] \\ 
    \hline
    \end{tabular}
    \caption{\label{break_prompt}
    Fine-tuning prompts for problem-solving logic analysis}
\end{table*}


\begin{table*}[h!]
  \centering
  \begin{tabular}{p{15cm}}
  \textbf{Prompt} \\ 
  \hline
  \textbf{System prompt}\\
  Please provide the answer in the following format: "The final answer is <answer>" \\
  \textbf{User input} \\
  question: Being his favorite, he saved checking on the grapevines for his last stop. He was told by 235 of the pickers that they fill 100 drums of raspberries per day and 221 drums of grapes per day. How many drums of grapes would be filled in 77 days? \\
  answer: Equation is ( 221.0 * 77.0 ). The final answer is 17017.0 \\
  \\
  question: Tiffany was collecting cans for recycling. On Monday she had 4 bags of cans. The next day she found some more bags worth of cans. If she had a total of 6 bags altogether, how many bags did she find on the next day? \\
  answer: Equation is ( 6.0 - 4.0 ). The final answer is 2.0 \\
  \\
  question: After a typhoon, 13 trees in Haley's backyard died. If she had grown 3 trees initially, how many more trees died in the typhoon than those that survived? \\
  answer: Equation is ( 13.0 - ( 3.0 - 13.0 ) ). The final answer is 23.0 \\
  \\
  question: Brenda's mother made cookies for 5 people. She prepared 22 cookies but had to throw away 17 cookies. If each of them had the same number of cookies, how many did each of them have? \\
  answer: Equation is ( ( 22.0 - 17.0 ) / 5.0 ). The final answer is 1.0 \\
  \\
  question: Haley grew 9 trees in her backyard. After a typhoon 4 died. Then she grew 5 more trees. How many trees does she have left? \\

  \hline
  \textbf{Number of problem-solving logic step:} \\4->5->6->7->7 \\
  \hline

  \end{tabular}
  \caption{\label{svamp_prompt}
  Full prompt example for in-context learning on the SVAMP dataset}
\end{table*}

\begin{table*}[h!]
  \centering
  \begin{tabular}{p{15cm}}
  \textbf{Prompt} \\ 
  \hline
  \textbf{System prompt}\\
  Please provide the answer in the following format: "The final answer is <answer>" \\
  \textbf{User input} \\
  question: What is the probability of rolling one six-sided dice, and getting a different number on each die?  A)1/12 B)1/3 C)1 D)5/9 E)7/18 \\
  answer: Let's think step by step. MAGOOSHOFFICIAL SOLUTION. For the first die, we can roll any one of six numbers. For the second die, we can roll any number save for the number we rolled on the first die, giving us 5 possibilities. For the third die, we can roll four different numbers (we can’t roll the number we rolled on the first or second die). 6 possibilities out of 216 total possibilities. 6/6 = 1. The final answer is C. \\
  \\
  question: If q is the square of a positive integer, which of the following must be equal to the square of the next positive integer? A)\(\sqrt{n + 1}\) B)\(n + 1\) C)\(n^2 + 1\) D)\(q + 2\sqrt{q} + 1\) E)\(n^2 + 2n + 1\) \\
  answer: Let's think step by step. If q is the square of a positive integer, then \(q = (x)^2\) where x is a positive integer. To calculate, \((x+1)^2 = x^2 + 2x + 1\), which is \(q + 2\sqrt{q} + 1\). This should be D. The final answer is D. \\
  \\
  question: If Tim had lunch at \$50 and he gave 20\% tip, how much did he spend? A)\$60.00 B)\$35.42 C)\$60.60 D)\$21.56 E)\$78.45 \\
  answer: Let's think step by step. The tip is 20\% of what he paid for lunch. Tip = 20\% of 50.00 = \$10.00. Total spent = 50.00 + 10.00 = \$60.00. The final answer is A. \\
  \\
  question: Carl is facing very difficult financial times and can only pay the interest on a \$10,000 loan he has taken. The bank charges him a quarterly compound rate of 4\%. What is the approximate interest he pays annually? A)\$1600 B)\$2000 C)\$2150 D)\$2500 E)\$12000 \\
  answer: Let's think step by step. The bank charges a 4\% quarterly compounded annual rate. Per quarter rate is (16/4)\% = 4\%. Thus, the quarterly compounded interest will be slightly more than \$1600. The final answer is A. \\
  \\
  question: A shopkeeper employed a servant at a monthly salary of 1500. In addition to it, he agreed to pay him a commission of 15\% on the monthly sale. How much sale in Rupees should the servant do if he wants his monthly income as 6000?  A)30000 B)415000 C)31500 D)50000 E)None of these \\

  \hline
  \textbf{Number of problem-solving logic step:} \\2->3->4->5->6 \\
  \hline

  \end{tabular}
  \caption{\label{aqua_prompt}
  Full prompt example for in-context learning on the AQuA dataset}
\end{table*}

\begin{table*}[h!]
  \centering
  \begin{tabular}{p{15cm}}
  \textbf{Prompt} \\ 
  \hline
  \textbf{System prompt}\\
  Please provide the answer in the following format: "The final answer is <answer>" \\
  \textbf{User input} \\
  question: A shopkeeper bought 150 packets of milk. Each packet contained 250 ml of milk. If one fluid ounce is equal to 30 ml, how many ounces of milk did he buy?\\
  nanswer: Let's think step by step. If the shopkeeper bought 150 packets of milk, each packet containing 250ml of milk, all the packets had a total of 250*150 =<<150*250=37500>>37500ml.Since one ounce equal 30 ml, the total amount of milk that the shopkeeper bought in oz is 37500/30=<<37500/30=1250>>1250 oz of milk. The final answer is 1250\\
  \\
  question: Twenty gallons of tea were poured into 80 containers. Geraldo drank 3.5 containers. How many pints of tea did Geraldo drink?\\
  answer: Let's think step by step. 20 gallons = 160 pints. 160/80 = <<160/80=2>>2 pints.3.5 * 2 pints = <<3.5*2=7>>7 pints. Geraldo drank 7 pints of tea. The final answer is 7\\
  \\
  question: During the holidays, Lance works as a merchandiser. He works 35 hours a week, spread equally over 5 workdays. If Lance earns \$9 an hour, how much does he make on each workday?\\
  answer: Let's think step by step. Lance works 35 / 5 = <<35/5=7>>7 hours a day. So he makes \$9 x 7 = \$<<9*7=63>>63 on each workday. The final answer is 63\\
  \\  
  question: A snack machine accepts only quarters. Candy bars cost ¢25, each piece of chocolate costs ¢75, and a pack of juice costs ¢50. How many quarters are needed to buy three candy bars, two pieces of chocolate, and one pack of juice?\\
  \\
  answer: Let's think step by step. Three candy bars cost ¢25 x 3 = ¢<<25*3=75>>75. Two pieces of chocolate cost ¢75 x 2 = ¢<<75*2=150>>150. So, the total amount needed to buy those is ¢75 + ¢150 + ¢50 = ¢<<75+150+50=275>>275. Since a quarter is equal to ¢25, therefore ¢275/¢25 = <<275/25=11>>11 quarters are needed. The final answer is 11\\
  \\
  question: Mark makes custom dog beds. A bed for a Rottweiler takes 8 pounds of stuffing, a bed for a chihuahua takes 2 pounds of stuffing, and a bed for a collie takes the average amount of stuffing between the first two kinds of beds. How many pounds of stuffing does Mark need to make 4 chihuahua beds and 3 collie beds?\\

  \hline
  \textbf{Number of problem-solving logic step:} \\5->6->7->8->8 \\
  \hline

  \end{tabular}
  \caption{\label{gsm8k_prompt}
  Full prompt example for in-context learning on the Gsm8k dataset}
\end{table*}

\begin{table*}[h!]
  \centering
  \begin{tabular}{p{15cm}}
  \textbf{Prompt} \\ 
  \hline
  \textbf{System prompt}\\
  Please provide the answer in the following format: "The final answer is <answer>" \\
  \textbf{User input} \\
  question: What is the only was to recover from exhaustion? A. mediate B. have rest C. stay in bed D. run out of steam E. go to sleep\\
  answer: B\\
  \\
  question: Google Maps and other highway and street GPS services have replaced what? A. united states B. mexico C. countryside D. atlas E. oceans\\
  answer: D\\
  \\
  question: You can share files with someone if you have a connection to a what? A. freeway B. radio C. wires D. computer network E. electrical circuit\\
  answer: D\\
  \\
  question: If a person isn't able to pay their bills what must they do? A. know everything B. acknowledgment C. make more money D. throw a party E. spare time\\
 

  \hline
  \textbf{Number of problem-solving logic step:} \\1->2->3->3 \\
  \hline

  \end{tabular}
  \caption{\label{comqa_prompt} Full prompt example for in-context learning on the ComSenQA dataset}
\end{table*}

\begin{table*}[h!]
  \centering
  \begin{tabular}{p{15cm}}
  \textbf{Prompt} \\ 
  \hline
  \textbf{System prompt}\\
  Please provide the answer in the following format: "The final answer is yes or no" \\
  \textbf{User input} \\
  question: Can you buy Casio products at Petco?\\
  answer: Casio is a manufacturer of consumer electronics and watches. Petco is a chain store that sells pet supplies like food, bowls, litter, toys, cages and grooming equipment. The final answer is no\\
  \\
  question: Did Clark Gable appear in any movies scored by John Williams?\\
  answer: Clark Gable died in 1960. John Williams scored his first movie in 1961. The final answer is no\\
  \\
  question: Could a dandelion suffer from hepatitis?\\
  answer: Only creatures that contain a liver can suffer from hepatitis. The liver is an organ only found in vertebrates. Vertebrates exist in the kingdom Animalia. Dandelions are plants in the kingdom Plantae. The final answer is no\\
  \\
  question: Did Mozart ever buy anything from Dolce \& Gabbana?\\
  \hline
  \textbf{Number of problem-solving logic step:} \\ 2->3->4->4 \\
  \hline

  \end{tabular}
  \caption{\label{sqa_prompt}
  Full prompt example for in-context learning on the StrategyQA dataset}
\end{table*}


\end{document}
