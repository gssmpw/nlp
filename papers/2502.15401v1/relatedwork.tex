\section{Related Work}
\subsection{In-Context Learning}
GPT-3 \cite{firstfewshot} exhibited few-shot and zero-shot learning abilities during the pretraining phase.
CoT~\cite{cot} designed several fixed demonstration examples manually as in-context information, inspired further research on ICL~\cite{cot_future1}.

Subsequent research has shown that the key to ICL lies in demonstration examples selection and ordering~\cite{relatedwork_icl1,relatedwork_icl2,relatedwork_icl3}.
Regarding example selection, AutoCoT~\cite{autocot} used k-means clustering to select representative examples and leveraged zero-shot CoT to generate their reasoning process as demonstration examples.
PromptSO~\cite{promptso} used principal component analysis~\cite{pca} to encode text and calculate similarity to select examples.
Another work~\cite{promot_retrieve} points out that a retriever can be trained using annotated data to determine whether an example is suitable for a query.
Regarding example ordering, a study~\cite{order} randomly generated multiple combinations of example orderings to create probe sets.
By analyzing the entropy of predicted labels for each probe set, the researchers selected the best-performing order.
KATE~\cite{kate} explored ordering examples based on task relevance as well as length-based sorting.
Relevance-based ordering prioritizes examples closely related to the target task, while length-based sorting considers potential advantages for specific tasks.


\subsection{Curriculum Learning in LLMs}
Numerous applications across various fields have demonstrated that curriculum learning can effectively enhance model training outcome~\cite{curriculum2,curriculum_survey}.

Currently, some works have applied curriculum learning to LLMs~\cite{relatedwork_cl1, relatedwork_cl2}.
A common approach is to train the model with examples progressing from easy to hard during fine-tuning.
For instance, a study~\cite{humancurriculum} conducted fine-tuning on a structured dataset that strictly covers multiple educational stages to simulate the progressive learning characteristics of humans.
In the medical field, similarly, human-defined and automatically generated methods were used to annotate data difficulty, and LLMs in the medical question-answering domain were fine-tuned from easy to hard.~\cite{humancurriculum}.
Additionally, another work~\cite{datasetdecomposition} decomposed datasets into sequences of varying lengths, using sequence length as a metric to measure data difficulty.

Another common approach for applying curriculum learning to LLMs is ICL.
For example, ICCL~\cite{iccl} utilized human experts or LLM-driven metrics to assess data difficulty, and gradually increased the difficulty of demonstration examples from easy to hard.