\section{Related Work}
\paragraph{Math Benchmarks.} Recently, the number of math-related benchmarks has increased drastically____. The most influential ones are MATH ____ and GSM8K ____, which focus on arithmetic reasoning at the high school competition level and grade school level, respectively. Moreover, other benchmarks such as MathBench ____ and OlympiadBench ____ are also blends of problems sets from various competitions and standard examinations, which are used to test human students' abilities of utilizing the math knowledge and certain tricks to solve complex application-based problems. However, mathematicians are more expecting LLMs to help them in literature review, idea generation, proof-checking and collaborative writing as they focus on a broader spectrum of mathematical activities rather ____. To better accommodate the true need for math research, some formal theorem proving benchmarks like PutnamBench ____, CoqGym ____ and MiniF2F ____ are also proposed recently in a combination of formal mathematical languages compilers (e.g. Coq, Lean), which could be viewed as the important math benchmarks for developing \textit{Mathematics Mechanization} ____.

On the contrary, our benchmark \dataname{} focuses on conceptual reasoning among mathematical concepts and theorems. Specifically, we research certain math reasoning technique: \textit{counterexamples in mathematics}, to check whether the models fully and correctly understand math concepts and theorems, which should be one of atomic abilities for what mathematician are expecting from LLMs compared to independently solving some simple math word problems.

\paragraph{Math Augmented LLMs.} In contrast to general-purpose models such as GPT-4 ____ and Gemini ____, several mathematics augmented LLMs have been developed using methods like data augmentation, pretraining, fine-tuning, and reinforcement learning with extensive mathematical corpora. For instance, WizardMath ____ employed tailored math prompts to generate seed data and then underwent RLHF and process supervision for training. Abel ____ utilized supervised fine-tuning with meticulous data processing, referred to as \textit{Parental Oversight}. InternLM2-Math ____ enhanced mathematical reasoning with chain-of-thought ____, code interpreters, and Lean4 translation and theorem proving. NuminaMath ____, which recently secured first place in the Kaggle AIMO competition\footnote{\url{https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/leaderboard}}, leveraged \textit{tool-integrated reasoning (TIR)} to generate math questions with fine-grained solutions. Qwen2.5-Math ____, initialized with general-purpose Qwen2.5 models, was trained on the undisclosed large-scale and high-quality mathematics-specific corpus. Deepseek-Math ____ focuses on data engineering during pretraining and efficient RL training. 

\paragraph{Conceptual Reasoning.} Conceptual reasoning is an ability to reason in abstract and high-level perspectives ____. Recently, there are numerous studies where LLMs are reasoning on abstracted and conceptualized structures by analogy, deduction, induction, etc ____. Specifically in math, conceptual reasoning requires people to reason around math concepts and axioms at the play of math hypothesis, statements and problems ____. An example of this is ConceptMath ____, a math word problem benchmark in elementary school and middle school level, but the reasoning in solving these problems remains superficial as it just requires models to extract the correct variables and do basic arithmetic operations and it is also saturated with GPT models, which diminishes it from showing whether LLMs are truly mastering mathematics.