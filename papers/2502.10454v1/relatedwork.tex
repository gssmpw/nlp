\section{Related Work}
\paragraph{Math Benchmarks.} Recently, the number of math-related benchmarks has increased drastically~\cite{amini2019mathqa, yang2019learning, zhengminif2f, hendrycksmath2021, cobbe2021gsm8k, GHOSTS, liu2024mathbench, he2024olympiadbench, lu2024mathvista}. The most influential ones are MATH \cite{hendrycksmath2021} and GSM8K \cite{cobbe2021gsm8k}, which focus on arithmetic reasoning at the high school competition level and grade school level, respectively. Moreover, other benchmarks such as MathBench \cite{liu2024mathbench} and OlympiadBench \cite{he2024olympiadbench} are also blends of problems sets from various competitions and standard examinations, which are used to test human students' abilities of utilizing the math knowledge and certain tricks to solve complex application-based problems. However, mathematicians are more expecting LLMs to help them in literature review, idea generation, proof-checking and collaborative writing as they focus on a broader spectrum of mathematical activities rather \cite{frieder2024largelanguagemodelsmathematicians}. To better accommodate the true need for math research, some formal theorem proving benchmarks like PutnamBench \cite{tsoukalasputnambench}, CoqGym \cite{yang2019learning} and MiniF2F \cite{zhengminif2f} are also proposed recently in a combination of formal mathematical languages compilers (e.g. Coq, Lean), which could be viewed as the important math benchmarks for developing \textit{Mathematics Mechanization} \cite{wen2001mathematics, mathmechanization}.

On the contrary, our benchmark \dataname{} focuses on conceptual reasoning among mathematical concepts and theorems. Specifically, we research certain math reasoning technique: \textit{counterexamples in mathematics}, to check whether the models fully and correctly understand math concepts and theorems, which should be one of atomic abilities for what mathematician are expecting from LLMs compared to independently solving some simple math word problems.

\paragraph{Math Augmented LLMs.} In contrast to general-purpose models such as GPT-4 \cite{openai2023gpt4} and Gemini \cite{geminiteam2024gemini}, several mathematics augmented LLMs have been developed using methods like data augmentation, pretraining, fine-tuning, and reinforcement learning with extensive mathematical corpora. For instance, WizardMath \cite{luo2023wizardmath} employed tailored math prompts to generate seed data and then underwent RLHF and process supervision for training. Abel \cite{abel} utilized supervised fine-tuning with meticulous data processing, referred to as \textit{Parental Oversight}. InternLM2-Math \cite{ying2024internlm} enhanced mathematical reasoning with chain-of-thought \cite{wei2022chain}, code interpreters, and Lean4 translation and theorem proving. NuminaMath \cite{numina_math}, which recently secured first place in the Kaggle AIMO competition\footnote{\url{https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/leaderboard}}, leveraged \textit{tool-integrated reasoning (TIR)} to generate math questions with fine-grained solutions. Qwen2.5-Math \cite{yang2024qwen2}, initialized with general-purpose Qwen2.5 models, was trained on the undisclosed large-scale and high-quality mathematics-specific corpus. Deepseek-Math \cite{shao2024deepseekmath} focuses on data engineering during pretraining and efficient RL training. 

\paragraph{Conceptual Reasoning.} Conceptual reasoning is an ability to reason in abstract and high-level perspectives \cite{wang2024role,DBLP:conf/coling/HuangMLHZ0Z24, li2024llms,li2025correct}. Recently, there are numerous studies where LLMs are reasoning on abstracted and conceptualized structures by analogy, deduction, induction, etc \cite{saparov2023testing,li2023towards,yasunaga2024large,xu2024let,wang2024hypothesis,cheng2024inductive, zhou2024conceptual}. Specifically in math, conceptual reasoning requires people to reason around math concepts and axioms at the play of math hypothesis, statements and problems \cite{simon2011studying}. An example of this is ConceptMath \cite{wu2024conceptmath}, a math word problem benchmark in elementary school and middle school level, but the reasoning in solving these problems remains superficial as it just requires models to extract the correct variables and do basic arithmetic operations and it is also saturated with GPT models, which diminishes it from showing whether LLMs are truly mastering mathematics.