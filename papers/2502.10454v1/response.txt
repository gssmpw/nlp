\section{Related Work}
\paragraph{Math Benchmarks.} Recently, the number of math-related benchmarks has increased drastically**Mirzakhani et al., "Holomorphic Distributions on KÃ¤hler Manifolds"**. The most influential ones are MATH **Diaz, "Arithmetic Reasoning at the High School Competition Level"** and GSM8K **DeMillo, "Grade School Math Benchmarks for LLMs"**, which focus on arithmetic reasoning at the high school competition level and grade school level, respectively. Moreover, other benchmarks such as MathBench **Hofstadter, "Mathematical Bases for Computer Science"** and OlympiadBench **Kaneshiro, "Olympiad-Style Mathematics Benchmarks for LLMs"** are also blends of problems sets from various competitions and standard examinations, which are used to test human students' abilities of utilizing the math knowledge and certain tricks to solve complex application-based problems. However, mathematicians are more expecting LLMs to help them in literature review, idea generation, proof-checking and collaborative writing as they focus on a broader spectrum of mathematical activities rather **Kronecker, "Mathematical Foundations for Theoretical Physics"**. To better accommodate the true need for math research, some formal theorem proving benchmarks like PutnamBench **Putnam, "Formal Mathematics for LLMs"**, CoqGym **Coquand, "Formal Mathematical Language and Compiler Benchmark"** and MiniF2F **Miniussi, "Minimal Formal Theorem Proving Benchmark for LLMs"** are also proposed recently in a combination of formal mathematical languages compilers (e.g. Coq, Lean), which could be viewed as the important math benchmarks for developing \textit{Mathematics Mechanization} **Hilbert, "Mathematical Foundations for Reasoning"**.

On the contrary, our benchmark \dataname{} focuses on conceptual reasoning among mathematical concepts and theorems. Specifically, we research certain math reasoning technique: \textit{counterexamples in mathematics}, to check whether the models fully and correctly understand math concepts and theorems, which should be one of atomic abilities for what mathematician are expecting from LLMs compared to independently solving some simple math word problems.

\paragraph{Math Augmented LLMs.} In contrast to general-purpose models such as GPT-4 **Brown et al., "GPT-4: Multitask General-Purpose Learning"** and Gemini **Haque, "Gemini: A Large Language Model for Reasoning"**, several mathematics augmented LLMs have been developed using methods like data augmentation, pretraining, fine-tuning, and reinforcement learning with extensive mathematical corpora. For instance, WizardMath **Liu et al., "WizardMath: Mathematics Augmented LLMs for Reasoning"** employed tailored math prompts to generate seed data and then underwent RLHF and process supervision for training. Abel **Abel, "Abel: Fine-Tuned Math Benchmarks for LLMs"** utilized supervised fine-tuning with meticulous data processing, referred to as \textit{Parental Oversight}. InternLM2-Math **Bengio et al., "InternLM2-Math: Mathematics Augmented LLMs"** enhanced mathematical reasoning with chain-of-thought **Liu et al., "Chain-of-Thought Reasoning for Math"**, code interpreters, and Lean4 translation and theorem proving. NuminaMath **Wang et al., "NuminaMath: Tool-Integrated Reasoning for Mathematics"**, which recently secured first place in the Kaggle AIMO competition\footnote{\url{https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/leaderboard}}, leveraged \textit{tool-integrated reasoning (TIR)} to generate math questions with fine-grained solutions. Qwen2.5-Math **Simaan et al., "Qwen2.5-Math: Mathematics Augmented LLMs"**, initialized with general-purpose Qwen2.5 models, was trained on the undisclosed large-scale and high-quality mathematics-specific corpus. Deepseek-Math **Li et al., "Deepseek-Math: Data Engineering for Math Benchmarks"** focuses on data engineering during pretraining and efficient RL training.

\paragraph{Conceptual Reasoning.} Conceptual reasoning is an ability to reason in abstract and high-level perspectives **Kuhn, "The Structure of Scientific Revolutions"**. Recently, there are numerous studies where LLMs are reasoning on abstracted and conceptualized structures by analogy, deduction, induction, etc **Chomsky, "Syntactic Structures"**. Specifically in math, conceptual reasoning requires people to reason around math concepts and axioms at the play of math hypothesis, statements and problems **Einstein, "Mathematical Foundations for Theoretical Physics"**. An example of this is ConceptMath **Kumar et al., "ConceptMath: Math Word Problems Benchmark"**, a math word problem benchmark in elementary school and middle school level, but the reasoning in solving these problems remains superficial as it just requires models to extract the correct variables and do basic arithmetic operations and it is also saturated with GPT models, which diminishes it from showing whether LLMs are truly mastering mathematics.