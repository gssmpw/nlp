\section{Introduction}
The field of Machine Learning (ML) has progressed significantly inhe past two decades. 
Recently, transformer-based architectures and the rise of Large Language Models (LLMs) have spurred increased interest in a new class of models known as Foundation Models (FMs). 
Trained in vast datasets, these models excel in a wide range of downstream tasks across various types of data and applications. 
Their ability to be fine-tuned for specific datasets further enhances their appeal.

Although much of the research on FMs has focused on Natural Language Processing (NLP) and Computer Vision (CV), there is growing interest in applying FMs to time-series forecasting \cite{chronos} \cite{lagllama} \cite{timesFM}. 
This application is particularly valuable in fields such as healthcare, where predicting future events can help physicians deliver preventive care, but often there is little data available for AI algorithm development. 
Therefore, FMs have the potential to have a significant impact in the healthcare industry through time series forecasting.

A major challenge in applying ML to biomedical data is the limited ability to share data due to privacy concerns. 
In healthcare, the sensitive nature of patient data makes this issue especially pressing. 
Federated learning (FL) has emerged as a promising decentralized solution, enabling ML models to be trained locally, thus alleviating privacy concerns \cite{fl}.

FL operates in a distributed manner among multiple clients with local data and a central server. The server distributes a "global" model to clients, which then train the model on their local data and send updates back to the server. The server aggregates these updates by averaging the results from clients (FedAvg) and refining the global model. The process often repeats itself until a performance criteria is met. This approach allows users to retain data privacy while benefiting from collaborative learning.

Under ideal conditions, FedAvg achieves good performance with minimal accuracy loss compared to centralized ML, while preserving data privacy. 
However, challenges arise when the network, resources or data distribution conditions are less than optimal \cite{ye2023heterogeneous}. 
Key issues include system heterogeneity, where clients have varying computational resources and network access, and statistical heterogeneity, where data across clients is not independent and identically distributed.

Although research has addressed potential solutions for both types of heterogeneity in FL, there is limited focus on applying FL to time series forecasting with FMs, particularly in the medical domain. 
Fine-tuning FMs presents unique challenges. FL for FM fine-tuning may offer only marginal improvements over local training on individual clients without shared updates, as FMs are pretrained on large, diverse datasets and may already exhibit strong generalization. 
In this paper, we examine some of the key challenges in fine-tuning FMs on ECG and ICG data across both Independent and Identically Distributed (IID), or homogeneous, and Non-Independent and Identically Distributed (non-IID), or heterogeneous, settings. 
Our evaluation leverages various FL techniques to address these challenges.
Our results show that while FL can outperform local fine-tuning in some cases, it may also underperform in others, highlighting the nuanced trade-offs involved.


% This gap is notable given the potential benefits of integrating FL and time-series forecasting for privacy-sensitive medical data. 
% While most research on statistical heterogeneity has concentrated on class imbalance among clients, this work investigates the use of FL for fine-tuning FMs using Electrocardiogram (ECG) and Impedance Cardiography (ICG) vital signs data collected under various scenarios. 
% We present empirical results from different data partitioning strategies, demonstrating the effectiveness of FL in fine-tuning FMs for vital signs forecasting and identifying key challenges related to heterogeneity that need to be addressed.

