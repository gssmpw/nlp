\section{Methodology}
In this section, we will detail the methodology employed by our empirical analysis. 
Specifically, we will describe the datasets used, the different FL approaches that we evaluated, and the data partitioning approaches that we experimented.

\subsection{Federated Learning approaches}
We evaluated three commonly used FL techniques: FedAvg \cite{fl}, FedProx \cite{fedprox}, and FL with local adaptation \cite{yu2022salvagingfederatedlearninglocal}.
Figure \ref{figs:fedavg_overview} illustrates the FL framework, while Figure \ref{figs:fed_la_overview} provides an overview of the local adaptation approach.
All three approaches fine-tuned the FM over many communication rounds with a goal to achieve generalizability (\(R=30\) in this paper).
We also evaluated the approach where the FM is fine-tuned on each local device separately, without creating a shared global model.
This is to compare against the results from the distributed FL frameworks and gain insight into trade-offs.

\begin{figure}
  \includegraphics[scale=0.28]{figures/fl_fm_ft_red.drawio.png}
  \caption{{Overview of fine-tuning a Foundation Model using Federated Learning at round \(r\). 
  The global server sends model weights \(\theta_r\) to clients, who update them locally on their data. 
  Clients return updated models \(\theta_{i,r+1}\), which the server aggregates using weighted averaging to obtain \(\theta_{r+1}\).
  }}
  \label{figs:fedavg_overview}
\end{figure}


\begin{figure}
  \includegraphics[scale=0.28]{figures/Fed-LA.drawio.png}
  \caption{{An overview of a hybrid training architecture that combines Federated Learning and Local Adaptation.
  }}
  \label{figs:fed_la_overview}
\end{figure}


For the implementation of the FL algorithms, we used FlowerAI \cite{flower-ai}, an open source framework that streamlines the application of various FL algorithms in simulated settings.
The rest of this subsection provides a detailed background of the three FL approaches that we evaluated in this work.

\textbf{FedAvg} is the original FL model aggregation algorithm introduced in \cite{fl}.
The objective of the FedAvg algorithm is to find model parameters \(\theta\) such that the weighted average loss \( \frac{1}{|D|} \sum_{i=1}^{N} |d_i|f_i(\theta) \) is minimized. 
Here, \(f_i(\theta)\) is the loss over the local data \(d_i \in D\) of each client \( c_i \in C \). 
\(1 \leq i \leq N\) and \(N\) is the number of total clients sampled in each round \(r\), where \(1 \leq r \leq R\) and \(R\) is the number of total rounds in the FL process. 
\(|D|\) is the sum of the size of each client's local data, i.e \(|D|=\sum_{i=1}^{N}|d_i|\).

At the start of each round \(r + 1\), the central server sends the global model \(\theta_r\) to all or a subset of clients \(c_i \in C\). 
Each client \(c_i\) trains using this global model \(\theta_r\) and updates its local model at round \(r+1\) using a stochastic gradient step.
The local update for a single step is given by:
\[\theta_{i,r+1} =\theta_{i,r} - \eta \nabla_{\theta} f_i(\theta_{i,r})\]
\(\eta\) is the learning rate, and \(\nabla_{\theta}\) is the gradient of the loss with respect to the model weights. 
This represents a single local training step for client \(i\). 
In practice, clients perform multiple local updates before sending their models back to the server.

The global model, \(\theta_{r+1}\) is the weighted average of all the clients models, i.e \( \theta_{r+1} =  \frac{1}{|D|}  \sum_{i=1}^{N} |d_i|\theta_{i,r+1}\).
This could be written more simply as:
\[ \theta_{r+1} =\theta_r - \frac{\eta}{|D|} \sum_{i=1}^{N} |d_i| \nabla_{\theta_r}  f_i(\theta_{r}) \]

\textbf{FedProx} aims to mitigate the effects of statistical and system heterogeneity in FedAvg by introducing a proximal term in the local objective function, which constrains local updates to stay closer to the global model \cite{fedprox}. 
This, according to the authors, helps mitigate the impact of non-IID data and systems heterogeneity.
Even if the client's local model does not converge to its local data \(d_i \in D\), the authors state that by adding a proximal term, we can still leverage the local model updates.
More formally, the local loss function \(f'_i\) can be written with respect to the original loss function \(f_i\),
\[f'_i(\theta_{i,r+1}) = f_i(\theta_{i,r+1}) - \frac{\alpha}{2} | \theta_{i,r+1} -\theta_r |^2 \]
Here \(\alpha\) is a value between 0 and 1, and it controls the strength of the proximal term.
Recall that \(\theta_{i,r+1}\) is initialized with \(\theta_r\), and then gets updated using gradient steps. 
Hence, the second term is essentially a regularization term that helps the local model stay close to the global model.

\textbf{Federated Learning with Local Adaptation (Fed-LA)} is a slightly modified FL algorithm inspired by the idea that locally training models on clients can offer several benefits over training a single global model for all clients, as proposed in \cite{yu2022salvagingfederatedlearninglocal}.
The authors proposed an alternate hybrid strategy where a global model \(\theta_G\) is first obtained using \(R\) rounds of FL training.
This global model is then fine-tuned separately by clients \(c_i \in C\) on their local datasets \(d_i \in D\).
We made slight changes to this approach by adding a regularization term to the local objective function of each client during local fine-tuning (i.e after obtaining the global model using FL). 
This regularization term is with respect to the global model \(\theta_G\), to avoid the problem of catastrophic forgetting.
More formally, the updated client loss \(f''_i\) can be written as
\[ f''_i(\theta_i) = f_i(\theta_i) + |\theta_i - \theta_G|^2  \]
This is fairly similar to the local objective of FedProx, with the exception being that the global weights do not get updated after each round.
In this work, we obtained the global model by running FedAvg for 20 rounds before fine-tuning it separately on each client for another 10 rounds.
Hence, after completing 30 rounds, each client has its own local model, independent of the models from other clients.
For evaluation, each client tests on its own test dataset, and the results are combined using weighted averaging.


\subsection{Vital Sign Dataset}
One of the datasets that we utilized in this work is the vital signs dataset collected in \cite{Schellenberger2020-lb}. 
This dataset comprises of a total of 30 participants, with measurements taken using a non contact radar device and reference vital signs from a Task Force Monitor (TFM), which recorded ECG, impedance, impedance cardiogram (ICG), as well as non-invasive continuous blood pressure (BP). Two leads were recorded for ECG, one from the right arm (lead 1), the other from the left arm (lead 2).  
For this work, after assessing data quality, we decided to focus on ICG data (sampling rate of 500 Hz) and ECG Lead 2 (sampling rate of 500 Hz) due to its closer proximity to the heart. 
For each subject, measurements were taken under 1 or more scenarios. 
These scenarios included resting, Valsalva Manoeuvre (VM), apnea (holding breath), and the tilt table tests. 
In this work, we only consider the measurements taken under the resting scenario.

The dataset has 16 female and 14 male healthy participants, with an average BMI of 23.2 $ \pm $ 3.3 kg/
m\textsuperscript{2} and an average age of 30.7 $ \pm $ 9.9. 


\subsection{PTB-XL Dataset}
To increase the applicability of our analysis, we also performed experiments on the Physikalisch-Technische Bundesanstalt Extra Large (PTB-XL) dataset collected in \cite{ptb-xl}.
This dataset consists of 21799 12-lead ECGs of 10-second each, sampled at 500 Hz.
There are 18869 patients, 52\% male and 48\% female, with ages ranging from 0 to 95 years (median 62 and interquantile range of 22). Data was collected from 52 different sites across hospitals and other healthcare organizations. 
This allowed us to simulate a real-world non-IID data distribution scenario, with each client having varying number of patient records and cardiac condition distributions,.
A previous study utilized the PTB-XL dataset for FL to classify arrhythmias using 12-lead ECG signals \cite{10.1007/978-3-031-49361-4_3}.
In contrast, our work focuses on time-series forecasting rather than classification.

\subsection{Data Partitions for Federated Learning}
As mentioned earlier, an important aspect to consider within FL is that of statistical heterogeneity, which refers to how the data is partitioned among different clients during the FL process. 
We thereby experimented under different data partitioning strategies, and performed an in-depth analysis of the performance of the different FL algorithms over these different data partitioning strategies. 
Three different data partitioning strategies were evaluated. 
A summary of the strategies is provided in Table \ref{tab:data_partitions}. 
For each strategy, we partitioned the data between 20 clients, and trained for 30 rounds.
Each client simulated a hospital-like entity.

\begin{table}
  \caption{\centering{A summary of the different data partitioning strategies used.}}
  \label{tab:data_partitions}
  \centering
  \begin{tabular}{l  l  l  l}
    \toprule 
    \textbf{Strategy}  & \textbf{No. of clients}   & \textbf{Data Distribution} & \textbf{Dataset} \\    
    \midrule
    Strategy \#1       & 20                         & IID                        & Vital Signs    \\
    \hline
    Strategy \#2       & 20                         & Non-IID                      & PTB-XL \\
    \hline
    Strategy \#3       & 20                         & Non-IID                      & Vital Signs \\
    
    \bottomrule
  \end{tabular}
\end{table}


\textbf{Strategy \#1:} For the first strategy, we maintained an IID data distribution among clients using the vital signs dataset.
We assigned 20 clients, each with ECG Lead 2 data from one or two patients, all recorded under the resting scenario. 
This setup ensured similar data distributions across clients, simulating the IID assumption. 
However, the small number of unique patients per client may limit model generalization, as some clients could learn patient-specific patterns rather than broader representations. 
While federated averaging mitigates this to some extent, further investigation is needed to assess the impact of patient distribution across clients. 
Despite this limitation, this setup provides a controlled environment to evaluate how well FL can fine-tune FMs on time-series medical data. 
All clients received an equal number of training samples to ensure a balanced comparison.

\textbf{Strategy \#2:} For the second strategy, we kept the data distribution among the clients non-IID.
We used the PTB-XL dataset for this experiment.
As aforementioned, the PTB-XL dataset has categorical labels indicating the site that each recording was taken at, information that could be used to simulate a real-world non-IID scenario.
For this experiment, we considered sites with more than 10 training recordings, and used 20 sites as clients for our experiments.
The clients had varying number of recordings. 
While some clients had more than 8500 training recordings, others had 10. 
We believe this added to the non-IID-ness of the client data.

\textbf{Strategy \#3:} To further explore the impact of data heterogeneity on fine-tuning FMs using FL, we performed another experiment targeting heterogeneity. 
Like the other two experiments, this experiment also had 20 clients. 
19 clients, just like Strategy \#1, had ECG Lead 2 data collected under the resting scenario. 
One client, however, trained exclusively on ICG data, collected under the resting scenario, with the task of predicting ICG signals instead of ECG. 
We included ICG to assess the FM’s ability to handle multiple modalities. 
However, our setup, where each client is restricted to a single modality, is a limitation, as a more realistic scenario would involve clients with access to multiple modalities.
Similar to Strategy \#1, all clients performed local fine-tuning for the same number of steps. 
Partitioning the data this way allowed us to further evaluate challenges associated in fine-tuning a time-series FM within the FL framework where there is a disparity in data distribution across clients. 
This partitioning strategy is different from Strategy \#2, where the distribution of each client is independent of every other client. 
Here, the distribution of 95\% of the clients (19 clients) is close to each other, while the distribution of 5\% (1 client) is different from the other clients.

We fine-tuned FMs on these partitioning strategies, and evaluated the resulting global models on test sets. 
We performed detailed comparisons between FedAvg, FedProx, Fed-LA, Local Training, and the zero-shot Chronos model (i.e., applying the Chronos model to our application-specific data as-is). 
Our results indicated a promising potential for fine-tuning FMs on time-series data using FL frameworks, as well as some key challenges pertaining to data heterogeneity that need attention and further investigation.

\subsection{Chronos}
Chronos, a class of FMs developed by Amazon, are time-series forecasting models pretrained on large corpus of time-series data. 
Chronos applies techniques similar to those used in LLMs, beginning by quantizing time-series context data and creating tokens through a tokenizer.
These tokens are fed into a large language model with an encoder-decoder or decoder-only architecture, which outputs predicted probabilities for the next time step.
These probabilities are used to generate the context token for the next time step, which is then converted into a numerical value, forming the model's prediction.
This process is repeated to predict any number of future time steps.
For our experiments, we used the default value in Chronos for context length, i.e., 512 past time steps, and forecast horizons up until 64 time steps.
Furthermore, while there are five different versions of Chronos, ranging from 8 million to 710 million parameters, we decided to utilize the model with approximately 8 million parameters (labeled tiny) for computation efficiency. 
All hyperparameter settings for the Chronos models were left as default.