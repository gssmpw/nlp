\section{Evaluation}
In this section, we presented the empirical results from the experiments that we performed using the data partitioning scenarios described in the previous section.  

\subsection{Evaluation of Data Partition Strategy \#1}
For our first experiment, we simulated the scenario where the resting data of all 30 patients is divided among 20 clients. 
During each round, each client trained the model on its local data for 400 steps and 64 batch size.

To evaluate different FL approaches, we train using each approach for 30 rounds. 
After each round, clients test the global model on their local data separate from their training sets. 
The results are then averaged by the global server using weighted averaging, where weights are the size of the clients' test sets.
For Fed-LA, FedAvg's global weights after round 20 were used to fine-tune the model locally for 10 rounds.

For the local training approach, we kept all hyperparameters the same as the FL experiments, except that the model weights are not shared with a central sever. 
We also evaluated the zero-shot Chronos model on the test set.

Table \ref{tab:homo_comparison} shows a comparison of the performance of all the approaches. 
Multiple metrics were used to evaluate the performance - the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and symmetric Mean Percentage Error (sMAPE). 
These errors refer to the error observed when evaluating the 64-step forecast. 
We also performed another comparison where we compared the performance of the different approaches against the number of rounds. 
Figure \ref{figs:homo_comparison} shows this comparison in terms of RMSE and MAE. 

\begin{figure}
  \includegraphics[scale=0.45]{figures/vs_iid_rmse_vs_round.png}
  \includegraphics[scale=0.45]{figures/vs_iid_mae_vs_round.png}
  \caption{{Comparisons of FedAvg, FedProx, Fed-LA, and local fine-tuning models in terms of RMSE and MAE against the number of rounds when the data distribution amongst clients is IID. 
  All FL algorithms were outperformed by the locally fine-tuned models.}}
  \label{figs:homo_comparison}
\end{figure}

\begin{table}
  \caption{{Strategy \#1: Comparison of three FL approaches against zero-shot and local fine-tuning for time-series forecasting with 64-point forecast horizons.}}
  \label{tab:homo_comparison}
  \centering
  \begin{tabular}{l l l l}
    \toprule 
    Approach                   & RMSE      & MAE    & sMAPE (\%)  \\
    \midrule
    FedAvg                  & 0.061    & 0.021   & 6.73       \\
    FedProx                 & 0.058    & 0.020   & 6.81       \\
    Fed-LA                   & 0.062    & 0.021   & 6.83       \\
    Zero-shot               & 0.091    & 0.039   & 12.67      \\
    Local Fine-tuning       & 0.049    & 0.016   & 5.43       \\
    \bottomrule
  \end{tabular}
\end{table}

Based on Figure \ref{figs:homo_comparison} and Table \ref{tab:homo_comparison}, we observe that FL effectively fine-tunes the pre-trained Chronos FM for ECG time series data in this case.
FL outperforms zero-shot across all metrics and forecast horizons, showing that FL can be a useful approach. 
However, the local fine-tuning approach outperforms all the FL approaches, performing better across all metrics. 
While the performance gap is not very significant, it does mean that clients in this particular scenario, overall, do not benefit from FL.

\subsection{Evaluation of Data Partition Strategy \#2}
For this experiment, we used the PTB-XL dataset to distribute the data among clients.
Each client represented a site where the data was originally collected.
There were 20 clients, and the number of samples between the clients varied from 11 to 8910 training recordings. 
The number of training steps for each client was kept the same as the client's training recordings, to simulate the scenario where there is a data imbalance among clients.
Testing was performed using the same approach as Strategy \#1, where each client (each site, in this case) has a 70/30 train/test split.


Table \ref{tab:strategy_2_summary} shows a comparison of the average performance on 64-point forecasts after all rounds of fine-tuning. 
We include the same metrics as the previous experiment. 
Figure \ref{figs:ptbxl_niid_comparison} visualizes the performance of the model with different fine-tuning approaches in terms of RMSE and MAE in relation to the communication rounds. 

\begin{table}
  \caption{{Strategy \#2: Comparison of three FL approaches against zero-shot and local fine-tuning for time-series forecasting with 64-point forecast horizons.}}
  \label{tab:strategy_2_summary}
  \centering
  \begin{tabular}{l l l l}
    \toprule 
    Approach            & RMSE      & MAE     & sMAPE (\%)  \\
    \midrule
    
    FedAvg             & 0.083     & 0.039   & 14.0       \\
    FedProx            & 0.082     & 0.038   & 13.9      \\
    Local              & 0.086     & 0.041   & 14.9       \\
    Fed-LA              & 0.083     & 0.038   & 14.0       \\
    Zero-shot          & 0.146     & 0.076    & 23.6        \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \includegraphics[scale=0.45]{figures/ptbxl_niid_rmse_vs_round.png}
  \includegraphics[scale=0.45]{figures/ptbxl_niid_mae_vs_round.png}
  \caption{{Comparisons of FedAvg, FedProx, Fed-LA, and local fine-tuning models in terms of RMSE and MAE against the number of rounds when the data distribution amongst clients is non-IID. 
  FL algorithms outperformed zero-shot and local fine-tuning.}}
  \label{figs:ptbxl_niid_comparison}
\end{figure}


Table \ref{tab:strategy_2_summary} and Figure \ref{figs:ptbxl_niid_comparison} show FL algorithms performing better than the local fine-tuning and zero-shot approaches over the PTB-XL dataset with non-IID partitioning.
This is different than the evaluation over Strategy \#1, where the local fine-tuning approach provided the best performance.
However, we note that the performance difference between FL and local fine-tuning may not be significant.
Moreover, Fed-LA's performance aligns with the first experiment, as local fine-tuning did not significantly alter the model's convergence.

\subsection{Evaluation of Data Partition Strategy \#3}
While the previous experiment tested a partitioning strategy where the data distribution among all the clients was fairly different (since all collected their data independently of each other), we performed another experiment with a non-IID partitioning strategy.
We used the vital signs dataset, and similar to Strategy \#1, we divided data between 20 clients, each locally training for the same number of steps per round (400 steps).  
In this partitioning strategy, to test a scenario where there is an extreme data imbalance (e.g., 5\% of the data is ICG data), one of the clients has ICG data instead of ECG Lead 2 data. 
All clients, except for one, follow an IID distribution.
FMs are generally trained on large amounts of data, and should be able to perform well on different tasks.
Hence, in the ideal scenario, we want to use the same model for ICG and ECG.

Table \ref{tab:strategy_3_summary} shows a comparison of the performance over the 64 time-step forecast when evaluating over both ECG and ICG data using different FL techniques. It can be observed that the performance of all the fine-tuned FL models was worse than the locally fine-tuned model when forecasting both ECG and ICG signals. 
This performance gap is especially significant when forecasting ICG signal, with local fine-tuning significantly outperforming the FL fine-tuning approaches.
This indicates a significant challenge in fine-tuning FMs for time series forecasting when there is underrepresented data across clients. 
For ECG data, we see that FL resulted much better performance, outperforming the zero-shot model significantly.
In contrast, for ICG data, the FL model's performance was fairly similar to the zero-shot model.
This discrepancy in results across the two types of data highlights the impact of data characteristics and data distribution on the performance of FMs fine-tuned using FL. 

% \begin{figure}
%   \centering
%       \includegraphics[scale=0.45]{figures/bar_graph_rmse.png}
%       \caption{{Comparison of the performance of FedAvg, zero-shot, and centralized fine-tuning on ECG and ICG data in terms of RMSE. 
%       The FedAvg model was trained where the data distribution amongst clients was non-IID. The ICG data was intentionally underrepresented for the FedAvg training process. The performance of the FedAvg algorithm on ICG data was worse than zero-shot performance, highlighting challenges of imbalanced data distribution when fine-tuning FMs using FL.}}
%       \label{figs:hetero_dtype_based_comparison}
% \end{figure}


\begin{table}[htbp]
\caption{Strategy \#3: Comparison of the performance of different FL approaches against zero-shot and local fine-tuning. The table breaks down the performance over ECG and ICG data for 64-point forecast horizons.}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lSSSSSSSS@{}}
    \toprule
    \textbf{Approach} &
      \multicolumn{3}{c}{\textbf{ECG}} &
      \multicolumn{3}{c}{\textbf{ICG}}  \\
      & {RMSE} & {MAE} & {sMAPE} & {RMSE} & {MAE} & {sMAPE} \\
      \cmidrule(r){2-4}\cmidrule(l){5-7}
    
    FedAvg    & 0.056 &  0.020 & 6.78\%  & 0.065 & 0.039 & 10.63\%  \\
    FedProx   & 0.058 &  0.021 & 7.14\%  & 0.074 & 0.045 & 12.27\%  \\
    Local     & 0.045 &  0.016 & 5.30\%  & 0.041 & 0.025 & 6.94\%  \\
    Fed-LA     & 0.057 &  0.021 & 6.81\%  & 0.068 & 0.040 & 11.08\%  \\
    Zero-shot & 0.091 & 0.039  & 12.67\% & 0.061 & 0.039 & 10.46\%  \\
    \bottomrule
  \end{tabular}}
  \label{tab:strategy_3_summary}
\end{table}



Overall, our evaluation results across the three partitioning strategies highlight the nuanced differences between using federated learning (FL) for fine-tuning foundation models (FMs) versus local fine-tuning. 
We find that local fine-tuning outperforms traditional FL algorithms when the data distribution of individual clients closely aligns with the overall distribution across all clients.
However, in scenarios where each client's local data distribution is independent of others (Strategy \#2), FL techniques outperformed local fine-tuning, albeit by a small margin.
Additionally, our results with Fed-LA, a hybrid approach combining federated learning with local adaptation, reveal that the fine-tuning process in FL can impact the model's convergence, making it more difficult for the model to effectively adapt to clients' local data during the subsequent fine-tuning stage.