\section{Results}
\label{sec:results}
\subsection{RQ1: Accuracy of Scenario Extraction}
% We recruit two researchers specializing in ADS testing to act as human validators in constructing scenario representations of crash reports. After a training phase, each researcher independently creates a complete set of scenario representations for all collected crash reports, detailing the road network, environmental information, and actor types. Following this, the researchers conduct a cross-check of each other’s scenario representations, discussing and resolving any discrepancies to achieve consensus. This process results in a “Golden Oracle” scenario representation that includes road network, environment, and actor type information.

% We compare the scenario representations generated by {\tool} in the Information Extraction stage against the golden oracle. For attributes in scenario representations that do not involve estimation, such as Weather, Time, or Number of Lanes, an output is considered correct only if it exactly matches the record in the Golden Oracle. For estimated data, such as the Length of the Road or Width of a Single Lane, we apply a threshold-based evaluation method: if the difference between the output value and the Golden Oracle record is within a specified threshold, the output is deemed correct. In our experiments, we set the threshold for the Length of the Road at 10 meters and for the Width of a Single Lane at 1 meter.

%(moved to experiment settings)

% \begin{table}[h!]
%     \centering
%     \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{|c|c|c|c|c|}
%         \hline
%         Attributes from scenario representation & Weather & Time & Actors type & Road network \\ \hline
%         Accuracy & 100\% & 100\% & 100\% & 88\% \\ \hline
%     \end{tabular}
%     }
%     \caption{Information extraction results (including Env, Actors type and Road network)}
%     \label{tab:I_E_results}
% \end{table}

% Table ~\ref{tab:I_E_results} 
% Validation of extracted scenario representations shows that {\tool} achieves 100\% accuracy in identifying environment information (including Weather and Time) and actor types, while accuracy for road network data reached 88\%. Our investigation into the errors in road network extraction reveals that most inaccuracies occurred in length estimation. For instance, the golden oracle of crash report 100343 (illustrated in Figure ~\ref{fig:NHTSA_LCTGen}) showed in the Listing ~\ref{lst:golden_oracle} marks the road length to be 65 meters, based on the assumption that the average length of a car is approximately 5 meters. However, {\tool}’s estimation (shown in Listing ~\ref{lst:scenario_representation})indicates a road length of 120 meters which is far beyond the threshold.




Validation of scenario representations demonstrates that {\tool} achieves 100\% accuracy in extracting environment information (weather and time) and actor types, and 88\% accuracy for road network data. Most road network inaccuracies stem from length estimation errors. For example, the golden oracle of crash report 100343 (Listing ~\ref{lst:golden_oracle}) specifies a road length of 65 meters, assuming an average car length of 5 meters, while {\tool} estimated 120 meters, exceeding the threshold. Despite such errors, scenario realism can be preserved if vehicle trajectories maintain correct relative positions. The 88\% accuracy for road network extraction remains within an acceptable range. For trajectory evaluation, Fleiss' Kappa test yielded a coefficient of 0.7768, indicating high inter-rater agreement. Feedback analysis, e.g., from respondent 3'answer shows that only 16\% of trajectory predictions were rated as significantly inconsistent with the original data.

% \begin{lstlisting}[caption=Golden Oracle of Case 100343, label={lst:golden_oracle}]
% Actors:
%   V1_type: Car
%   V2_type: Truck
% Env:
%   Time: Not mentioned
%   Weather: Not mentioned
% Road network:
%   Length: 65
%   No_lanes: 4
%   No_ways: 2
%   Width: 5
% Road type: Straight
% \end{lstlisting}

% An inaccurate estimation of road length can lead to inaccuracies in road network construction, which may in turn impact the overall scenario accuracy. However, in this case, even if {\tool} provides an incorrect length estimate, the scenario's realism can still be preserved as long as the extracted vehicle trajectories maintain correct relative positions. Moreover, the 88\% accuracy achieved by {\tool} in road network extraction is within an acceptable range.

% In evaluating the extracted trajectories, we first conduct a Fleiss' Kappa test~\cite{wiki:FleissKappa} on the collected survey responses to assess the consistency of the evaluation results. The Fleiss' Kappa coefficient is found to be 0.7768, indicating a high degree of inter-rater agreement, suggesting that the majority of evaluators reached similar judgments. After establishing the consistency of survey responses, we analyze the distribution of scenario evaluation outcomes based on the feedback from respondent \#3, as shown in Table ~\ref{tab:traj_matching}.


% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{Graphs/traj_eva.png}
%   \caption{Trajectory Evaluation Results from Response \#3}
%   \label{fig:traj_eva}
% \end{figure}
% in GitHub repo - human study

% According to the survey design, when participants rate the trajectory visualization as "mostly not match" or "totally not match," it implies that the extracted trajectory significantly deviated from the original incident. The survey results show that only 16\% of the vehicle trajectory predictions are inconsistent with the original data.
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Graphs/scenario_construction_results.pdf}
  \caption{Two Scenario Construction Results}
  \label{fig:scene_con}
\vspace{-6mm}
\end{figure}
\subsection{RQ2: Accuracy of Critical Scenario Reflection}
% To assess how well the critical scenarios generated by {\tool} reflect the original crash reports, we conduct a human study on the generated critical scenarios for RQ2. From the 127 generated critical scenarios (involving crashes), we randomly selected 40 scenarios to present to the participants for evaluation of how accurately each scenario reflected the original crash. 
% For the feedback from the 5 participants, we apply Fleiss' Kappa test to evaluate the consistency of the results. The Fleiss' Kappa coefficient is found to be 0.7769, indicating a high degree of agreement among the evaluators and suggesting substantial consistency in their assessments. 
% After confirming the consistency of the responses, we use the feedback from participant 3 as an example, as shown in Table ~\ref{tab:traj_matching}. The results reveal that 77.5\% of the scenarios were rated by the human validators as "mostly match the crash scenarios from the original data." 10\% were rated as "partially match," meaning the road structure, vehicle count, and some vehicle behavior trajectories were correct. Only 12.5\% of the scenarios were rated as "mostly not match" or lower, indicating significant differences in the road network and the behavior of traffic participants compared to the original data.
% The two specific examples rated as "Totally match" and "Totally not match" are displayed in Figure ~\ref{fig:scene_con}. For Case 100237, {\tool} accurately reconstructs the timing, road network, and actor behavior. In this example, the ego car is depicted as a green square, and its collision avoidance system is shown to activate at the intersection. However, due to its excessive speed, a collision eventually occurs despite the system's intervention. In contrast, Case 128763, recconstructed within the BeamNG environment, illustrates an unrealistic scenario where {\tool} predicts an excessively short lane length, leading to an inaccurate scene representation. This discrepancy may stem from the limitations of the visual module within LLMs when processing larger or longer images.
To evaluate how well the critical scenarios generated by {\tool} reflect the original crash reports, we conduct a human study on 40 randomly selected scenarios from the 127 generated. Feedback from five participants is analyzed using Fleiss' Kappa test, yielding a coefficient of 0.7769, indicating substantial agreement among evaluators. Using participant 3's feedback as a reference, 77.5\% of the scenarios are rated as "mostly match the crash scenarios from the original data", 10\% as "partially match" (correct road structure, vehicle count, and partial trajectories), and 12.5\% as "mostly not match" or lower, due to significant discrepancies in road networks or traffic behavior. Two cases illustrating "Totally match" and "Totally not match" are shown in Figure~\ref{fig:scene_con}. For Case 100237, {\tool} accurately reconstructs timing, road networks, and actor behavior, depicting the ego vehicle (green square) failing to avoid a collision due to excessive speed. In contrast, Case 128763 highlights an unrealistic scenario caused by {\tool}'s misprediction of lane length, attributed to limitations in the LLM's visual module for processing larger images. Overall, the results suggest that {\tool} is able to consistently and accurately reconstruct 87.5\% of the scenario data.

In addition, a human study on one-third of LCTGen’s scenarios, analyzed using Fleiss' Kappa (0.8870, "almost perfect agreement"), reveals only 60\% of its scenarios are rated acceptable, far below {\tool}’s performance. Examples in Figure ~\ref{fig:results_compar} highlight these differences. In Case 119897, {\tool} accurately reconstructed the road network and actor behavior, while LCTGen failed on both fronts. Similarly, for Case 120516, {\tool} successfully identified and reconstructed the accident scenario, unlike LCTGen’s unrealistic predictions. LCTGen's shortcomings stem from inaccuracies in reconstructing road networks and actor trajectories. In contrast, {\tool}'s use of a detailed DSL, multimodal data, and LLMs enabled realistic scene generation and superior bug detection.

%https://drive.google.com/file/d/1VgNNPapVKKbw721tNm1bX8fMSllnBS_l/view?usp=sharing

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{Graphs/scen_gen_eva.png}
%   \caption{Generated Scenarios Evaluation Results from Response \#3}
%   \label{fig:scen_gen}
% \end{figure}
% % in GitHub repo - human study

% \begin{table}[!t]
% \centering
% \scalebox{0.8}{ 
% \begin{tabular}{lccc}
% \toprule
% \textbf{Rate} &\textbf{RQ1 Survey} & \textbf{RQ2 Survey} & \textbf{RQ3 Survey} \\
% \midrule
% Totally match    & 22 & 19 & 0 \\
% Mostly match     & 17 & 12 & 4 \\
% Partially match  & 3  & 4  & 5 \\
% Mostly not match & 6  & 1  & 4 \\
% Totally not match & 2  & 4  & 2 \\
% \bottomrule
% \end{tabular}}
% \label{tab:traj_matching}
% \caption{Human Evaluation Results}
% \vspace{-10mm}
% \end{table}

\subsection{RQ3: Effectiveness in Scenario Construction and ADS Bug Detection}
% We assess {\tool}'s scenario generation capability and bug detection efficiency across two simulation platforms (MetaDrive and BeamNG) and various ADS policies (IDM, PPO, and Auto). {\tool} constructed 96 scenarios each for MetaDrive's IDM and PPO policies, and 98 scenarios for BeamNG’s Auto policy. In terms of bug detection, {\tool} outperformed LCTGen by identifying a significantly higher number of issues: 30 bugs in MetaDrive with IDM, 13 in MetaDrive with PPO, and 84 in BeamNG’s Auto. By comparison, LCTGen generated 50 scenarios on MetaDrive with IDM, of which 22 met the necessary criteria, yielding only 2 bug discoveries.
We evaluated {\tool}'s scenario generation and bug detection across MetaDrive and BeamNG simulation platforms with various ADS systems (IDM, PPO, Auto). As shown in Table ~\ref{tab:comparison} {\tool} generated 96 scenarios each for MetaDrive’s IDM and PPO, and 98 for BeamNG’s Auto, identifying significantly more bugs than LCTGen: 30 in MetaDrive (IDM), 13 in MetaDrive (PPO), and 84 in BeamNG (Auto). By contrast, LCTGen generated 50 scenarios for MetaDrive (IDM), with only 22 meeting criteria, yielding just 2 bug discoveries. Efficiency analysis showed {\tool} required 15 seconds per test case for MetaDrive (IDM, PPO) and 12 seconds per scenario (42 seconds per test case) for BeamNG’s Auto. As for the number of scenarios used to find the Top-K bugs, results are reported in Table ~\ref{tab:bug_counts} and demonstrate {\tool} efficiency in generating targeted test scenarios and identifying ADS vulnerabilities.
% \begin{table}[h]
%     \centering
%     \begin{tabular}{lcccc}
%         \toprule
%         \textbf{ADS \& Simulator} & \textbf{No. Scenarios - TRACE} & \textbf{No. Bugs - TRACE} & \textbf{No. Scenarios built by LCTGen} & \textbf{No. of Bugs found by LCTGen} \\
%         \midrule
%         IDM on MetaDrive         & 96  & 30  & 50  & 2    \\
%         PPO on MetaDrive         & 96  & 13  & not applicable & not applicable \\
%         Auto on BeamNG           & 98  & 84  & not applicable & not applicable \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:comparison}
%     \caption{Comparison of scenarios and bugs detected by TRACE and LCTGen}
% \end{table}

\begin{table}[!t]
\centering
\scalebox{0.8}{ 
\begin{tabular}{lcccc}

\toprule
\textbf{ADS \& Simulator} & \multicolumn{2}{c}{\textbf{scenarios built No.}} & \multicolumn{2}{c}{\textbf{bugs found No.}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                          & \textbf{TRACE} & \textbf{LCTGen} & \textbf{TRACE} & \textbf{LCTGen} \\
\midrule
IDM on MetaDrive         & 96             & 50              & 30             & 2               \\
PPO on MetaDrive         & 96             & None            & 13             & None            \\
Auto on BeamNG           & 98             & None            & 84             & None            \\
\bottomrule
\end{tabular}}
\caption{Comparison of scenarios and bugs detected by TRACE and LCTGen}
\vspace{-5mm}
\label{tab:comparison}
\end{table}
% For comparison, we sampled one-third of the scenarios generated by LCTGen and conducted a human study to evaluate how well these scenarios reflect the original crash reports. We first applied Fleiss' Kappa test to the five collected survey responses. The Fleiss' Kappa coefficient was 0.8870, indicating a very high level of agreement among the evaluators, which corresponds to "almost perfect agreement." Table ~\ref{tab:traj_matching} shows the specific feedback data from one human validator, from which we can observe that only 60\% of the scenarios were deemed acceptable and adequately reflected the crash scenarios. This performance is significantly lower than that of {\tool}.

% Figure ~\ref{fig:results_compar} presents two specific examples that highlight the differences in performance between {\tool} and LCTGen. In Case 119897, LCTGen exhibits errors in reconstructing both the road network and actor trajectories, whereas {\tool} accurately restores the road network down to the lane level. In {\tool}'s result, the green Ego car is assigned a destination that encroaches on the oncoming lane; however, the Ego car detects the risk and refrains from following this path, choosing instead to remain within its lane. For Case 120516, LCTGen mistakenly identifies the road network as a T-intersection and produces unrealistic vehicle trajectory predictions. Conversely, {\tool} accurately reconstructs the accident scenario. The behavior of the green Ego car shows it successfully identifies the hazardous vehicle and turns to evade it.

% The poor performance of LCTGen in ADS bug detection and scenario realism is primarily due to its inaccuracies in reconstructing the road network and actor trajectories. In contrast, {\tool} achieves more realistic scene reconstructions and identifies a greater number of ADS bugs. This is made possible through the use of a more detailed DSL, the integration of multimodal data, and the effective application of LLMs.


%https://drive.google.com/file/d/1zScVeXPXfXRF5zej-7lXrw0KbTSpywR6/view?usp=sharing


% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{Graphs/LCTGen_eva.png}
%   \caption{LCTGen Generated Scenarios Evaluation Results from Response \#3}
%   \label{fig:lct_gen_eva}
% \end{figure}
% % in GitHub repo - human study

% The efficiency of {\tool} in scenario generation was also analyzed in terms of time. {\tool} required an average of 15 seconds per test case for MetaDrive's IDM and PPO policies, and 12 seconds per scenario (42 seconds per test case) on BeamNG’s Auto. Moreover, {\tool} exhibited a strong capacity to identify top bugs with minimal scenario counts, needing only three scenarios to find the top bug under MetaDrive’s IDM policy and only one scenario to locate the top bug in BeamNG's Auto. This demonstrates {\tool}’s efficiency in generating relevant test scenarios and effectively identifying ADS vulnerabilities.


\begin{table}[!t]
\centering
\scalebox{0.8}{ 
\begin{tabular}{llr}
\toprule
\textbf{Simulator - ADS} & \textbf{Level} & \textbf{Count} \\
\midrule
\multirow{3}{*}{MetaDrive - IDM} & Top 1 - bug & 3 \\
                                         & Top 2 - bug & 5 \\
                                         & Top 3 - bug & 24 \\
\midrule
\multirow{3}{*}{MetaDrive - PPO} & Top 1 - bug & 4 \\
                                         & Top 2 - bug & 9 \\
                                         & Top 3 - bug & 30 \\
\midrule
\multirow{3}{*}{BeamNG - Auto}          & Top 1 - bug & 1 \\
                                         & Top 2 - bug & 2 \\
                                         & Top 3 - bug & 3 \\
\bottomrule
\end{tabular}}
\caption{Bug counts by level for different simulators and policies}
\label{tab:bug_counts}
\vspace{-8mm}
\end{table}
\subsection{RQ4: Effectiveness of Prompt Engineering and Validation}
To investigate the effectiveness of {\tool}'s prompt engineering and self-validation components, we conducted ablation studies by selectively disabling these features and observing the resulting impact on scenario representation extraction accuracy. \textbf{Self-Validation Removal}: When the self-validation mechanism is disabled, {\tool}’s accuracy on the road network extraction decreased to 82\% while for other attributes remained 100\% accuracy. This reduction was primarily observed in the calculation of road attributes, highlighting the importance of self-validation in ensuring accurate attribute extraction for road features.
% \begin{table}[h!]
%     \centering
%     \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{|c|c|c|c|c|}
%         \hline
%         Attributes from scenario representation & Weather & Time & Actors type & Road network \\ \hline
%         Accuracy & 100\% & 100\% & 100\% & 82\% \\ \hline
%     \end{tabular}
%     }
%     \caption{Information extraction results (after removing self-validation)}
%     \label{tab:re_validation_results}
% \end{table}
\textbf{Combined Road Type and Road Network Extraction}: In a further ablation, we combined the processes for extracting road type and road network details. This configuration resulted in a significant accuracy of road network drop to 52\%. The primary cause of this decline was {\tool}’s struggle with processing excessively long input sequences and managing extended memory demands, which impacted its ability to accurately extract scenario representation attributes. These findings emphasize the critical role of self-validation and modularized extraction processes in maintaining {\tool}'s scenario representation accuracy, especially for complex data structures like road attributes.
\vspace{-2mm}