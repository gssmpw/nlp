\section{Methodology}
\label{sec:methodology}
\subsection{Overview}
Figure~\ref{fig:TRACE} presents the high-level structure of {\tool}, which includes two primary stages: the Information Extraction stage and the Scenario Construction \& ADS Testing stage. In the Information Extraction stage, we utilize LLMs to extract key scenario details—such as road networks, environments, and traffic actors—from multimodal crash reports (an example is shown in Figure~\ref{fig:NHTSA_LCTGen}). While the original DSL from TARGET~\cite{deng2023targetautomatedscenariogeneration} offers a lightweight and flexible approach for representing scenarios, it relies on relatively simple terms for defining road layouts and actor behaviors. However, to accurately construct scenarios from crash reports—particularly to ensure vehicles are positioned correctly relative to one another—more detailed representations are necessary to capture the scenarios' critical elements accurately. In this paper, we extend TARGET’s DSL to support finer-grained details, such as road length, lane configurations, and actor trajectories, enabling a more precise description of crash scenarios. This enhanced DSL allows for the accurate encoding of complex road layouts and nuanced actor behaviors directly from crash reports, ensuring that the generated scenarios are both syntactically and semantically accurate. In the second stage, Scenario Construction \& ADS Testing, we introduce the Scene Generation Adapter, which translates these enhanced scenario representations into test scenarios that can be executed across various simulators. These test scenarios are then connected to different ADS systems, where we use a dedicated scenario monitor to track the system’s responses. The monitor produces a comprehensive test report detailing metrics such as the number of scenario builds, instances of collision scenarios, and overall performance metrics.
% The following sections provide a detailed explanation of the workflow in each stage of TRACE.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Graphs/NHTSA_Preview.pdf}
  \caption{Preview of NHTSA dataset and scenario replay}
  \label{fig:NHTSA_LCTGen}
\vspace{-5mm}  
\end{figure}
% https://drive.google.com/file/d/1oyCHrmNEbhETTJ28-et3bwhOFTinPGo0/view?usp=sharing



% https://drive.google.com/file/d/1EanK74g0whqqld4IzDGqGSDsqxuLODLx/view?usp=sharing

%Revised by Yang, please double-check.
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Graphs/TRACE_archi.pdf}
  \caption{TRACE Overview}
  \label{fig:TRACE}
\end{figure}


\begin{lstlisting}[caption=The Structure of Scenario DSL, label={lst:DSL-example}]
<Scenario>      ::= <Road type>; <Road network>; 
                    <Env>; <Actors>

<Road type>     ::= Straight | Intersection | 
                    T-intersection | Curve | 
                    Merge

<Road network>  ::= <Length>; <No_lanes>; <No_ways>; <Width>; <Length_main>; <Length_branch>; <No_lanes_main_road>;<No_lanes_branch_road>; <No_ways_main_road>;<No_ways_branch_road>

<Env>           ::= <Time>; <Weather>
<Time>          ::= Daytime | Nighttime | 
                    Not mentioned
<Weather>       ::= Sunny | Cloudy | Overcast | 
                    Rainy | Snowy | Foggy | Windy | Not mentioned

<Actors>        ::= <Vn_traj>; <Vn_type>
<Vn_traj>       ::= list of waypoints
<Vn_type>       ::= Car | Truck
\end{lstlisting}

% \begin{lstlisting}[caption=Scenario Representation of Case 100343, label={lst:scenario_representation}]
% Actors:
%   V1_traj: '[(5, 2), (17, 4), (25, 6)]'
%   V1_type: Car
%   V2_traj: '[(5, 6), (15, 6), (25, 6)]'
%   V2_type: Truck
% Env:
%   Time: Not mentioned
%   Weather: Not mentioned
% Road network:
%   Length: 120
%   No_lanes: 4
%   No_ways: 2
%   Width: 5
% Road type: Straight
% \end{lstlisting}

\subsection{Stage I: Information Extraction}
Figure ~\ref{fig:Info_ext} illustrates the detailed process of the information extraction stage, which is comprised of four steps: prompt selection, {\tooltwo} construction, information extraction, and information encoding. The final output of stage I is a scenario representation containing scene information(one scenario representation example is given in Listing~\ref{lst:scenario_representation}, the crash report of it is described in Figure~\ref{fig:NHTSA_LCTGen}), which then serves as input for the second stage—Scenario Construction \& ADS Testing.

\begin{figure}[!t]
\setlength{\abovecaptionskip}{-0.5pt}
\setlength{\belowcaptionskip}{-2pt} 
  \centering
  \includegraphics[width=\linewidth]{Graphs/Info_extract.pdf}
  \caption{Illustration of Stage I - Information Extraction}
  \label{fig:Info_ext}
\end{figure}
% https://drive.google.com/file/d/1gBemqtD-WIR9Bn0gb8pXSt5GDcg9nei5/view?usp=sharing

\subsubsection{Step 1: prompt selection}
We introduce a structured, two-phase method to address the limitations of existing approaches like LCTGen~\cite{tan2023language}, which relies on prompting LLMs to extract scene information from crash summaries. These existing methods often struggle with overly long input sequences and complex extraction tasks, leading to catastrophic forgetting and reduced accuracy. A significant challenge is that different types of road networks (e.g., straight roads versus intersections) require specific parameters and calculation methods. Using generic prompts without distinguishing road types can result in extracting irrelevant information, further compounding the forgetting problem and reducing scenario accuracy. To overcome these challenges, our method begins by prompting the LLM to classify the specific road type involved in the crash, such as distinguishing between straight roads and intersections. Based on this classification, we then apply tailored prompts selected from a set of five specialized road network templates, each designed to capture the unique parameters of the identified road type. By aligning prompts with the specific road geometry, we reduce the complexity of input sequences and cognitive load on the LLM. This approach effectively mitigates catastrophic forgetting, enhances extraction accuracy, and improves the overall fidelity of scenario representations.
%revised by Yang please double check
% SOTA-LCTGen~\cite{tan2023language} attempts to extract scene information by prompting the LLM to generate encoded information on the road and traffic participants based on a single crash summary. However, this approach has notable limitations: overly lengthy input sequences and complex tasks can lead to catastrophic forgetting in the LLM, thus compromising the accuracy of information extraction. Additionally, since each type of road network requires different parameters and calculation methods, prompts must be tailored to guide the LLM accordingly. For instance, for a straight road, the LLM should calculate the linear distance from one end to the other, whereas for an intersection, it should compute the distances in two perpendicular directions. Therefore, each road network type demands a specific prompt to effectively extract relevant information from the sketch and report. Without first identifying the road type, irrelevant details could be included when extracting road network information, exacerbating catastrophic forgetting in the LLM. To address this, we first utilize the LLM to identify the specific road type where the crash occurred (e.g., determining whether the crash took place on a straight road or at an intersection). Following this, we select targeted prompts from five road network extraction prompt templates for extracting road details based on the identified road type. Since each road type requires different parameter definitions, it is unnecessary to impose irrelevant rules on the LLM. This mechanism effectively shortens the input sequence for the LLM and mitigates forgetting, thereby enhancing the accuracy of scenario representation.

Table ~\ref{tab:road-type} presents the prompts we used for road type identification. To optimize the accuracy of road type identification, we developed a structured multi-step workflow leveraging the capabilities of LLMs with advanced prompt engineering and self-validation mechanisms. First, we assign the LLM the role of an experienced road engineering expert by initializing the context with relevant system information and a clear introduction to the task. This setup guides the model to generate responses aligned with the domain-specific knowledge of road engineering. Next, we employ a combination of in-context learning, chain-of-thought (CoT) prompting, and self-validation techniques. The LLM is provided with a case study to systematically learn how to analyze and identify the road type. This step-by-step guidance is reinforced with instructions to self-validate its intermediate results against the original data. If discrepancies are detected during validation, the LLM is prompted to re-evaluate its analysis until the results are verified as correct. Following this, we utilize a multi-turn dialogue strategy with predefined assistant responses to ensure that the LLM comprehensively understands the task requirements. Once the LLM has demonstrated a clear understanding through iterative dialogue, we present the specific task data. The final output is the result of the LLM's analysis for a specific case (e.g., Case 100343), showcasing its ability to apply CoT reasoning and self-verification to generate accurate, validated results. This approach ensures that the LLM can systematically think through complex scenarios, validate its own outputs, and deliver responses in the required format, thereby improving the reliability of scenario extraction.
%revised by Yang please double check

% we first assign the LLM the role of an experienced road engineering expert within the system information and provide a basic introduction to the task. This helps guide the LLM to generate responses that are more aligned with the perspective of a road engineering expert. In the subsequent user information, we combine in-context learning, chain-of-thought (CoT) prompting, and self-validation techniques. We teach the LLM step-by-step how to analyze and identify the road type using a case study, while guiding it on how to validate the preliminary results by referring back to the original data. If the LLM believes the validation fails, it is required to re-evaluate and provide a result until the validation is passed. We then set a predefined assistant response, employing a multi-turn dialogue strategy to ensure the LLM understands the task and provides the expected output. Finally, we present the specific task data and requirements to the LLM. The final assistant response is the result of the LLM's road type identification for case 100343. From the results, we can see that the LLM is able to follow the analytical steps mentioned in the CoT, progressively think through the process, and perform self-validation. Once the validation is successful, it outputs the answer in the required format.

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{p{0.9\columnwidth}} % 单列，指定宽度
\toprule
\rowcolor{gray!20} System Info \\ % 奇数行灰色背景
You are an experienced road engineering expert... 

Brief intro of the task... \\ % 偶数行白色背景
\rowcolor{gray!20} User \\ % 奇数行灰色背景
1. Intro of the dataset: ...

2. Task description (from the following perspective):

\#\#\# Your Tasks:
1. Extract the Road Network from summary.

2. Self-Validate your answer against the sketch. If validation fails, adjust until it passes.

\#\#\# Provide Case studies(with sketch and summary) using CoT as in-context learning:

Input: <Sketch>; <Summary>

\#\#\#\# Step 1 - Extract the Road Type ...

\#\#\#\# Step 2 - Self-Validation ...

\#\#\#\# Output:
{'Road type': Curve, 'Validation': Pass}

\#\#\#Output Format Requirements:
{'Road type': <road type>, 'Validation': <Pass/Fail>}
\\ % 偶数行白色背景
\rowcolor{gray!20} Assistant \\ % 奇数行灰色背景
I have understand ..., please give me your data!\\ % 偶数行白色背景
\rowcolor{gray!20} User \\ % 奇数行灰色背景
Input: Please help me identify the road type of this crash case. <Sketch>; <Summary>\\ % 偶数行白色背景
\rowcolor{gray!20} Assistant \\ % 奇数行灰色背景
\#\#\# Step 1 - Extract the Road Type

From the crash summary:
- The road is described as a limited access, four-lane, divided freeway with two lanes in each direction and an open median.
- There is no mention of intersections, curves, or merging lanes.

Based on this description, the road type is classified as Straight.

\#\#\# Step 2 - Validation

From the sketch:
- The road is depicted as a straight, divided freeway with two lanes in each direction, separated by an unprotected median.
- There are no intersecting roads, curves, or merging lanes visible in the sketch.

The sketch confirms the road type as Straight.

\#\#\# Output

{'Road type': Straight, 'Validation': Pass}\\ % 偶数行白色背景
\bottomrule
\end{tabular}%
}
\caption{The Structure of Road Type Identification Prompts}
\label{tab:road-type}
\vspace{-8mm}
\end{table}

Once the road type from the crash case is identified, the prompts selection step produces prompts tailored to extract detailed road network information for use in Step 3. Table ~\ref{tab:road-network} provides an example of a straight road network extraction prompt template.

\begin{figure}[!t]
\setlength{\abovecaptionskip}{-0.5pt}
\setlength{\belowcaptionskip}{-2pt} 
  \centering
  \includegraphics[width=\linewidth]{Graphs/coordinate.pdf}
  \caption{Road Structure and Coordinate System Definition}
  \label{fig:corrdinate}
\vspace{-5mm} 
\end{figure}

\subsubsection{Step 2: TrackMate construction}
As shown in Figure ~\ref{fig:NHTSA_LCTGen}, relying solely on the limited descriptions of vehicle behavior in a crash summary can easily lead to incorrect assumptions about the vehicles' relative positions or even their directions of travel. When combining crash sketches with summaries, a critical question arises: how can we enable the LLM to learn the correspondence between vehicle trajectories (coordinate points) and both visual and textual information? To address this, we constructed a knowledge base consisting of crash sketches, summaries, and actual vehicle trajectories, which served as the foundation for developing a specialized GPT model~\cite{GPTs} on ChatGPT-4, named {\tooltwo}. GPTs~\cite{GPTs} enable users to create customized models by uploading data and pre-defining system instructions. This offers key advantages: (1) eliminating the need to pass detailed instructions with each invocation, and (2) allowing the integration of extensive domain knowledge via a knowledge base. Unlike traditional in-context learning, this approach enables the model to access relevant information directly from the knowledge base during reasoning. This significantly enhances the LLM's ability to map vehicle trajectories to crash reports, resulting in more accurate path planning.
% revised by yang
% GPTs~\cite{GPTs} allows users to build a customized GPT by uploading data and pre-defining system info and instructions. The benefits of this are: (1) there is no need to pass complex instructions every time GPT is called; (2) compared to in-context learning, the knowledge base approach allows us to integrate a significant amount of information (tokens) into the LLM, enabling it to search the knowledge base first during reasoning. This supports the LLM in learning the mapping between vehicle trajectories and crash reports, ultimately yielding accurate path planning.

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{p{0.9\columnwidth}} % 单列，指定宽度
\toprule
\rowcolor{gray!20} Instructions \\ % 奇数行灰色背景
Task intro:(extract vehicle trajectories and type ...)

\#\#\# Steps to Complete:

---

1. **Construct the Coordinate System**:   ...
   
2. **Identify Vehicles**:   ...
   
3. **Map Starting and Collision Points**:

   - Locate starting \& collision points using the crash report.

   - Learn the ability to predict real vehicle behavior from the vehicle trajectory data provided in the knowledge base.
   
   - Map these points to the coordinate system, then use trajectory prediction to estimate each vehicle’s path from its starting point to the collision point.
   
4. **Verify Trajectories**: (Take your results back to the crash report and validate) 

\\ % 偶数行白色背景
\bottomrule
\end{tabular}%
}
\caption{Instructions in TrackMate}
\label{tab:trackmate_insturct}
\vspace{-10mm}
\end{table}
Our knowledge base is structured to support accurate trajectory prediction by leveraging two key resources: (1) five detailed trajectory extraction cases from the NHTSA dataset, which serve as exemplars for in-context learning, and (2) real-world vehicle trajectory data from NuScenes~\cite{nuscenes2019}. Since the original NuScenes data includes unnecessary sensor information, we preprocess it using ScenarioNet~\cite{li2023scenarionet} to extract only the relevant vehicle trajectory coordinates. These coordinates are then used to enhance the LLM's understanding of path planning.

To optimize {\tooltwo} for its task, we configure it as a specialized assistant capable of analyzing road structures and predicting trajectories. The system instructions include a thorough introduction to the task, explaining how to define a coordinate system based on user-provided crash sketches for accurate trajectory extraction. In Table~\ref{tab:trackmate_insturct}, we demonstrate the step-by-step process for {\tooltwo} to predict vehicle trajectories: it first identifies the starting and collision points from the crash report and then applies learned path planning techniques using the trajectory data from the knowledge base. To ensure consistency in trajectory prediction, we adopt a standardized two-dimensional Cartesian coordinate system tailored for different road types, as detailed in Figure~\ref{fig:corrdinate}. This predefined template guides the LLM in accurately mapping vehicle movements, thus enhancing the precision of path planning and scene reconstruction. The finalized {\tooltwo} model can be utilized in Step 3%directly 
without giving instruction prompts to extract realistic path planning directly from crash reports.
% Revised by Yang

% In the knowledge base, we have prepared (1) five trajectory extraction cases with analysis processes, all extracted from the NHTSA dataset, serve as the resources for in-context learning and (2) real-world vehicle trajectory data from NuScenes~\cite{nuscenes2019}. For the original trajectory data from NuScenes, which includes unnecessary information such as sensor data, we preprocess it using ScenarioNet~\cite{li2023scenarionet} to extract only the vehicle trajectory coordinates. These coordinates will be used to guide the LLM in learning trajectory prediction.

% In the {\tooltwo} system information, we assign it the role of an assistant familiar with road structures and capable of trajectory prediction. In the instructions, we provide a detailed introduction to the task and explain how to define the coordinate system based on the crash sketch provided by the user in order to extract the vehicle trajectory. In Table ~\ref{tab:trackmate_insturct}, we show in detail how to ask {\tooltwo} to make trajectory prediction. First, it is asked to locate the starting and collision positions of the vehicle from the crash report, and then it is asked to plan the path based on the path planning it learned from vehicle trajectory data provided in the knowledge base.

% In this work, we adopted a coordinate system template defining how to configure a two-dimensional Cartesian coordinate system for different road types, as detailed in Figure ~\ref{fig:corrdinate}. 




%https://drive.google.com/file/d/1FZAN8qYwRDc03cTqbGTean926keUKIJa/view?usp=sharing

\subsubsection{Step 3: information extraction}
In the third step, we use an LLM to extract scene information from crash reports, focusing on three key areas: road network, environment, and actors. For the road network, prompts predefined in the first step guide the LLM to capture detailed road data, as shown in Table~\ref{tab:road-network} for a straight road scenario. The LLM is assigned the role of a ``road engineering expert'' with system information that includes a high-level task overview. It is then provided with specific task instructions and an example outlining the analysis and self-validation process. To ensure compliance with the road network extraction task, we employ a multi-turn dialogue strategy to reinforce adherence. In the second dialogue turn, the crash report is presented, prompting the LLM to extract relevant road network details. The final results in Table~\ref{tab:road-network} show the LLM's extraction for case 100343, demonstrating its ability to follow instructions and produce accurate outputs.

% The final output, shown in Table~\ref{tab:road-network}, represents the LLM's extraction results for case 100343, showing its ability to follow prompts and validate outputs accurately.
%revised by Yang. 

% In the third step, we use an LLM to extract specific scene information from the crash report. The extraction process is divided into three independent parts: road network, environment information, and actor information. For the road network, we use prompts selected in the first step to guide the LLM in capturing detailed road description data. Table ~\ref{tab:road-network} illustrates the road network information extraction prompts for a straight road. Here, the LLM is assigned the role of a "road engineering expert" again in the initial system information and provided with a high-level task description. Subsequently, it is given detailed task requirements, along with an example that includes a specific analysis and self-validation process. To ensure the LLM adheres to task requirements - road network extraction, we implement a multi-turn dialogue technique again, introducing an assistant response to reinforce adherence. In the second turn, the specific crash report is provided, and the LLM is requested to output the corresponding road network information. Table ~\ref{tab:road-network} presents the final assistant response, which is the LLM's road network extraction result for case 100343. 

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{p{0.9\columnwidth}} % 单列，指定宽度
\toprule
\rowcolor{gray!20} System Info \\ % 奇数行灰色背景
You are an experienced road engineering expert... (Task intro) \\ % 偶数行白色背景
\rowcolor{gray!20} User \\ % 奇数行灰色背景
1. Intro of the dataset...

2. Detailed Description of the task...

3. Common knowledge for approximating road parameters...

4. Provide Case studies using CoT as in-context learning...

5. Self-validation...

6. Output examples...\\ % 偶数行白色背景
\rowcolor{gray!20} Assistant \\ % 奇数行灰色背景
I have understand the task and your requirements, ..., please give me your data!\\ % 偶数行白色背景
\rowcolor{gray!20} User \\ % 奇数行灰色背景
Input <Sketch>; <Summary>\\ % 偶数行白色背景
\rowcolor{gray!20} Assistant \\ % 奇数行灰色背景
\#\#\# Analysis Process

\#\#\#\# Step 1 - Extract the Number of Ways and Lanes

1. Identify the number of ways and lanes (from summary): ...

2. **Identify Key Descriptors**: ...

\#\#\#\# Step 2 - Estimate Road Length and Lane Width from the Sketch

1. **Road Length**: ... (Analysis)

- Calculation: 20 x 6 = 120.

2. **Lane Width**: ... (Analysis)

- Calculation: 2.5 + 2.5 / 2 + 2.5 / 2 = 5.

\#\#\#\# Step 3 - Self-Validation

Verify the extracted values with the dataset:

- **No\_ways** = 2: Matches the "divided freeway" description in the summary.

- **No\_lanes** = 4: ...

- **Length** = 120 meters: Consistent with 6 truck lengths, as observed in the sketch.

- **Width** = 5 meters: ...

Since all values align with the data, the validation is **Passed**.

\#\#\# Final Output:\{'No\_ways': 2, 'No\_lanes': 4, 'Length': 120, 'Width': 5, 'Validation': 'Passed'\}\\ % 偶数行白色背景
\bottomrule
\end{tabular}%
}
\caption{The Straight Road Network Extraction Prompts}
\label{tab:road-network}

\end{table}

From the results, we can observe that the LLM strictly followed the steps outlined in the CoT. Specifically, it first extracted the number of ways and lanes, then identified the lane lengths and widths from the sketch and performed the necessary calculations. Finally, using self-validation, the LLM checked whether the extracted preliminary results matched the original data. If the validation was successful, it outputted the result in the required format.
% Why do we talk about results here? It is misleading to the audience. Please think of a better way to present this part.

\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{p{0.9\columnwidth}} % 单列，指定宽度
\toprule
\rowcolor{gray!20} Assistant \\ % 奇数行灰色背景
Extracted environmental information and validation:

1. **Weather**: Not described in the summary.

2. **Time**: No description of Time in the summary.

\#\#\# Validation:

- **Weather**: Not mentioned.

- **Time**: Not mentioned.

Since both the weather and time match the description in the summary, the validation result is "Passed."

\{"Weather": "Not mentioned", "Time": "Not mentioned", "Validation": "Passed"\}
\\ % 偶数行白色背景
\bottomrule
\end{tabular}%
}
\caption{Environment Info Extraction Results - 100343}
\label{tab:env_info_llm}
\end{table}

For environment information, we apply in-context learning, CoT, and self-validation again to extract weather and lighting conditions from the crash summary, with results formatted for structured output. Table ~\ref{tab:env_info_llm} shows the environment extraction results for case 100343 from LLM. For actor information, we utilize the {\tooltwo} LLM, developed in the second step, to extract details on vehicle types and trajectories. Table ~\ref{tab:actor_info_llm} shows the actor trajectory extraction results for case 100343 from LLM.



\subsubsection{Step 4: information encoding}
In the final step, we developed an Information Encoder to transform the LLM's responses into a standardized scenario representation using our custom DSL. To extract the necessary scene information from the plain text provided by the LLM, the Information Encoder employs a pattern-matching algorithm based on regular expressions. This process is illustrated in Algorithm [1]
%Yang: Please ensure that the reference command is used consistently for all algorithms throughout this paper. Avoid using manual numbering such as [1]. Instead, assign labels to each algorithm and refer to them using the `\ref` command for proper cross-referencing.
, which details how to parse and encode road type information from the LLM's raw outputs.
This method leverages the structured format and consistent "Output" markers defined in the LLM responses from the previous step. By using regular expressions, the Encoder efficiently identifies, extracts, and encodes the relevant details, ensuring accurate and streamlined scenario representation. The encoded scenario representation is then passed to the second stage for scenario construction and ADS testing.
\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{p{0.9\columnwidth}} % 单列，指定宽度
\toprule
\rowcolor{gray!20} Assistant \\ % 奇数行灰色背景
Analysis and Output:

1. Road Setup

Road Type: Straight, divided freeway with two lanes for each direction (northbound and southbound).

Road Length: 55 meters.

Lane Width: 4 meters per lane.

2. Vehicle Identification

Vehicle 1 (V1): ... (classified as Car).

Movement: Initially in the right northbound lane, then ...

Collision: Left side struck by the front of V2 ...

Vehicle 2 (V2): ... (classified as Truck).

Movement: Traveling in the left northbound lane ...

Collision: Front of V2 struck the left side of V1 ...

3. Coordinate System and Trajectory Mapping

The origin is set at the bottom-left of the northbound lanes, with: (1) X-axis extending horizontally to the right. (2) Y-axis extending vertically upward.

4. Estimated Trajectories

V1's Trajectory (Car):(getting from Movement analysis)[(5, 2), (17, 4), (25, 6)]

V2's Trajectory (Truck):(getting from Movement analysis)[(5, 6), (15, 6), (25, 6)]

5. Validation

(Take the extracted trajectories back to the original data and validate)

Output:

\{
  "V1\_traj": [(5, 2), (17, 4), (25, 6)],
  
  "V1\_type": "Car",
  
  "V2\_traj": [(5, 6), (15, 6), (25, 6)],
  
  "V2\_type": "Truck",
  
  "Validation": "Passed"
\}
\\ % 偶数行白色背景
\bottomrule
\end{tabular}%
}
\caption{Actor Info Extraction Results - 100343}
\label{tab:actor_info_llm}
\vspace{-8mm}
\end{table}
%revised by Yang
\begin{algorithm}
\caption{Extract Road Type and Validation from Text Files}
\KwIn{folder\_path: Directory containing the Road Type extraction results}
\KwOut{results: Dictionary of road types and validation for each case}

results $\gets$ \{\} \tcp*{Initialize an empty dictionary}

\For{each file in folder\_path}{
    \If{file ends with "\_road\_type.txt"}{
        file\_id $\gets$ Extract file\_id from filename \tcp*{Split filename to get the ID}
        file\_path $\gets$ Get full path of the file
        
        Open file\_path and read the content
        
        road\_type $\gets$ Extract road\_type using regular expression for 'Road type'
        
        validation $\gets$ Extract validation using regular expression for 'Validation'
        
        results[file\_id] $\gets$ [road\_type, validation] \tcp*{Store the results}
    }
}
\Return{results} \tcp*{Return the final dictionary}
\label{alg:encoder}
\end{algorithm}




\subsection{Stage II: Scenario Construction \& ADS Testing}
\label{sec:Stage-II}
In the second stage, we utilize the scenario representation extracted from the crash report to generate test scenarios within the simulator and integrate them with the ADS for evaluation. Once the testing is complete, a monitoring system generates a detailed test report, which includes information on scenario construction and collision detection outcomes.
%revised by Yang
% In the second stage, we use the scenario representation extracted from the crash report to build test scenarios in the simulator and integrate them with the ADS for testing. Finally, a monitor will generate a test report containing information on scenario construction, and collision detection.

To construct test scenarios in the simulator, we developed a Scene Generation Adapter that converts standardized scenario representations into executable code for simulators. In current mainstream simulators, there are primarily two approaches for scenario creation. The first approach involves searching or locating suitable road segments from an existing map within the simulator, and then mapping the actors' trajectories to specific points on these segments to construct the scene. The second approach involves completely rebuilding a map on a flat plane, where all road segments and scene elements must be user-defined. In our experiment, we selected MetaDrive~\cite{li2022metadrive} and BeamNG~\cite{BeamNG} simulators for specific scenario construction and testing. BeamNG follows the first scene-building approach, while MetaDrive follows the second approach. 
%revised by Yang

%https://drive.google.com/file/d/18U-hhZuwVgPzyixMGPoocRVWAIyJEu-s/view?usp=sharing

Figure~\ref{fig:scen_meta} illustrates how a straight-road scene is constructed in the first-type simulator, using MetaDrive as an example. In MetaDrive, a straight-line lane for left-to-right direction consists of three key points, indicated as `>`, `>>`, and `>>>`, representing the beginning, middle, and end of the lane, respectively. Each lane in MetaDrive is assigned a unique ID, where `0` denotes the lane closest to the centerline, and higher numbers correspond to lanes farther away. To differentiate directions, while straight lanes running from left to right are marked with `>`, `>>`, and `>>>`,  those running from right to left use `<`, `<<`, and `<<<` as lane markers. Based on this road structure, our developed Scene Generation Adapter maps vehicle trajectories learned from the knowledge extraction phase to these lane IDs and lane markers, considering the number of lanes, lane width, and vehicle coordinates specified in the scenario representation. This mapping configures vehicle behaviors accordingly. For the trajectory of the Actor-Car in case 100343, the mapped waypoints for the ego vehicle (the green vehicle in Lane 1) are represented as follows: [Starting point: (`>`, 1), Midpoint: (`>>`, 1), Endpoint: (`>>>`, 0)]. This trajectory results in a collision with the yellow vehicle after the midpoint (`>>`) of Lane 0. The mapped vehicle trajectory can then be directly executed in the simulator.
%Figure~\ref{fig:scen_meta} illustrates how a straight-road scene is constructed in a first-type simulator, using MetaDrive as an example. As shown, the road structure is defined by Lane IDs and lane markers (indicated in the figure as symbols like ‘>’, ‘> >’, etc.). For road segments with traffic flowing from left to right, the simulator uses the symbols ‘>’, ‘> >’, and ‘> > >’ to mark lanes, whereas for the opposite direction, it uses symbols like ‘->’, ‘-> >’, and ‘-> > >’. Different lanes in the same direction are described by Lane IDs ranging from 0 to N, where Lane ID 0 indicates the lane closest to the centerline of the road. Based on this road structure, our developed Scene Generation Adapter maps vehicle trajectories to Lane IDs and lane markers according to the number of lanes, lane width, and vehicle coordinates specified in the scenario representation, configuring vehicle behavior accordingly. For the trajectory of Actor-Car in case 100343, the mapped points are represented as [Starting point: (‘>’, 1), Midpoint: (‘> >’, 1), Endpoint: (‘> > >’, 0)]. This mapped vehicle trajectory can then be directly executed in the simulator.

% MetaDrive is a mainstream simulator for testing reinforcement learning models, offering advantages such as lightweight design, cross-platform compatibility, and a convenient road definition interface. Road structures defined in the DSL can be seamlessly mapped to road construction code, while actor trajectories in the DSL are translated by the Adapter into MetaDrive-compatible path markers, including start positions, waypoints, and endpoints. Environmental details, such as lighting, can be easily controlled in MetaDrive by setting the scene’s time of day. 

% BeamNG, a widely used driving simulation platform, allows users to build roads from scratch, enabling high flexibility in scenario design. Unlike MetaDrive, BeamNG provides more realistic scene rendering, as it is developed on a powerful game engine. 

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Graphs/metadrive.pdf}
  \caption{Scenario Construction in MetaDrive}
  \label{fig:scen_meta}
\vspace{-4mm}
\end{figure}

For constructing scenarios in the second type of simulator, the Adapter leverages detailed road descriptions from the scenario representation to generate a configuration file that defines the simulation environment. To handle vehicle trajectories, BeamNG’s custom coordinate system is utilized, enabling us to translate actor trajectories from the scenario representation with simple origin adjustments. Unlike LCTGen, which randomly assigns an actor as the ego vehicle, our approach iterates through all actors in the scene, assigning each one as the ego car in turn. This method allows for the exploration of a broader range of critical scenarios. Each designated ego vehicle is then integrated with the ADS algorithm for testing. During the simulation, a monitor constantly tracks the test, records all scenarios, and creates a report detailing any detected collisions.

% During the simulation, a monitor continuously tracks the test, logs all scenarios, and generates a report highlighting any collision events detected.
% For scenario construction in the second type of simulator, the Adapter uses detailed road descriptions from the scenario representation to build a configuration file that establishes the simulation environment. For vehicle trajectory information, BeamNG’s custom coordinate system allows us to translate actor trajectories from the scenario representation with simple origin shifts.

% Compared to LCTGen’s approach of randomly selecting an actor and designating it as the ego car, we chose to iterate over all actors in the scene, designating each one as the ego car in turn to uncover a wider range of critical scenarios. The actor designated as the ego car is integrated with the ADS algorithm for testing. Throughout this process, a monitor continuously observes the testing phase, logs the scenarios, and ultimately outputs the test results, reporting any scenarios in which collisions occurred.