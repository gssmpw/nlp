\subsection{Experiment Settings}
\subsubsection{Generic Settings}
% To evaluate the performance of {\tool}, we expand upon the dataset used in the most relevant state-of-the-art (SOTA) work, LCTGen~\cite{tan2023language}, which selects 38 crash reports from the NHTSA CIREN database~\cite{ADAS_NHTSA_report}. We include an additional 12 reports, resulting in a comprehensive dataset of 50 crash cases. Each report contains a sketch illustrating the accident scene, including the map layout and vehicle trajectories, along with a summary detailing driver behavior, vehicle status, and environmental conditions.
To evaluate {\tool}'s performance, we expanded the dataset from the SOTA LCTGen~\cite{tan2023language} by adding 12 crash reports to the original 38 from the NHTSA CIREN database, creating a dataset of 50 crash cases. Each report includes an accident sketch, driver behavior, vehicle status, and environmental details. For our framework’s LLMs, we use GPT-4-o for road type identification and environmental data extraction in Stage I. For actor extraction, we enhance GPT-4-o with a specialized knowledge base on map data and vehicle trajectories, forming an optimized model, {\tooltwo}. GPT-4-o was selected for its SOTA multimodal capabilities. To test {\tool}’s scenario scalability and bug-detection capacity in ADS, we utilize two simulators in Stage II: MetaDrive~\cite{li2022metadrive} and BeamNG~\cite{BeamNG}. MetaDrive, a lightweight ADS testing simulator developed by UCLA, allows for customizable road scenarios and supports multiple ADS types—IDM (maintains safe distances with RL) and PPO (end-to-end neural network model). In contrast, BeamNG, a realistic driving simulator on the Torque3D engine, offers detailed vehicle models and customizable environments. On BeamNG, we test the Auto driving model, a widely used ADS among more than 250,000 Steam users, which supports advanced autonomous driving functionalities such as obstacle avoidance and lane switching.
% In selecting LLMs for our framework, we utilize GPT-4-o for road type identification and the extraction of road network and environmental information in Stage I. For actor information extraction, we extend GPT-4-o by integrating it with a specialized knowledge base containing map data and vehicle trajectories~\cite{GPTs}, resulting in an enhanced model named {\tooltwo}, specifically tailored for actor extraction tasks. GPT-4-o is chosen as our base model due to its standing as a state-of-the-art multimodal model, consistently demonstrating strong performance across diverse and complex tasks.

% As discussed in Section~\ref{sec:Stage-II}, there are two main approaches to constructing test scenarios in current simulators. To evaluate the scalability of our scenario representations and to assess whether the scenario representations extracted by {\tool} can reflect real-world scene information and uncover potential bugs in ADS, we choose two simulators—MetaDrive~\cite{li2022metadrive} (representing the first type of simulator) and BeamNG~\cite{BeamNG} (representing the second type of simulator)—in Stage II to construct test scenarios and integrate ADS for evaluation. MetaDrive, a recent and popular lightweight ADS testing simulator primarily developed by UCLA, offers a user-friendly interface for configuring test road scenarios and operates across multiple operating systems. In this experimental setup, we evaluate two mainstream but distinct types of ADS: the IDM Policy and the PPO Policy~\cite{Two_Policys}. The IDM Policy, developed based on reinforcement learning, is designed to maintain safe distances from moving objects and avoid stationary obstacles, while the PPO Policy is a purely end-to-end autonomous driving model based on neural networks, trained on hundreds of scenarios in MetaDrive and capable of handling most driving situations. 

% BeamNG, in contrast, is a more realistic vehicle driving simulator built on the Torque3D engine~\cite{Torque3D}, with characteristics similar to an open-world driving game. Compared to MetaDrive, BeamNG enables the construction of more realistic environments, featuring detailed vehicle models and user-customizable road settings. On BeamNG, we test the Auto driving model, a widely used ADS among more than 250,000 Steam users~\cite{steam_beamng}, which supports advanced autonomous driving functionalities such as obstacle avoidance and lane switching.


% revised by yang

% As discussed in Section ~\ref{sec:Stage-II}, there are two main approaches to constructing test scenarios in current simulators, in order to evaluate the scalability of our scenario representations and to assess whether the scenario representations extracted by {\tool} can reflect real-world scene information and uncover potential bugs in ADS, we chose two simulators—MetaDrive~\cite{li2022metadrive}(represent the first type of simulator) and BeamNG~\cite{BeamNG}(represent the second type of simulator)—in the Stage II to construct test scenarios and integrate ADS for evaluation. MetaDrive, a recent and popular lightweight ADS testing simulator primarily developed by UCLA, offers a user-friendly interface for configuring test road scenarios and can operate across multiple operating systems. In this experimental setup, we evaluated two mainstream but distinct types of ADS: the IDM Policy and the PPO Policy~\cite{Two_Policys}. The IDM Policy, developed based on reinforcement learning, is designed to maintain safe distances from moving objects and avoid stationary obstacles, while the PPO Policy is a purely end-to-end autonomous driving model based on neural networks, trained on hundreds of scenarios in MetaDrive and capable of handling most driving situations. BeamNG, in contrast, is a more realistic vehicle driving simulator built on the Torque3D engine~\cite{Torque3D}, with characteristics similar to an open-world driving game. Compared to MetaDrive, BeamNG enables the construction of more realistic environments, featuring detailed vehicle models and user-customizable road settings. On BeamNG, we tested the Auto driving model, a widely used ADS among more than 250 thousand Steam users~\cite{steam_beamng}, which supports advanced autonomous driving functionalities such as obstacle avoidance and lane switching.

\subsubsection{Settings for RQ1}
To evaluate the accuracy of {\tool} in scenario representation extraction, we implement a validation process. We recruit two researchers specializing in ADS testing to act as human validators in constructing scenario representations of crash reports. Figure ~\ref{fig:human_extract} shows how validators extract data.

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{Graphs/extracting_rules.pdf}
%   \caption{How human validators extract data}
%   \label{fig:human_extract}
% \end{figure}
% %https://drive.google.com/file/d/1bDILWPxzPN6ggPtqh2KikzbzxXfxcy8g/view?usp=sharing

% After a training phase, each researcher independently creates a complete set of scenario representations for all collected crash reports, detailing the road network, environmental information, and actor types. Following this, the researchers conduct a cross-check of each other’s scenario representations, discussing and resolving any discrepancies to achieve consensus. This process results in a “Golden Oracle” scenario representation that includes road network, environment, and actor type information. Regarding the calculation of accuracy, for attributes in scenario representations that do not involve estimation, such as Weather, Time, or Number of Lanes, an output is considered correct only if it exactly matches the record in the Golden Oracle. For estimated data, such as the Length of the Road or Width of a Single Lane, we apply a threshold-based evaluation method: if the difference between the output value and the Golden Oracle record is within a specified threshold, the output is deemed correct. In our experiments, we set the threshold for the Length of the Road at 10 meters and for the Width of a Single Lane at 1 meter.

After training, each researcher independently creates scenario representations for all crash reports, detailing the road network, environment, and actor types. They then cross-check each other's work, discussing any discrepancies to reach a consensus, resulting in a “Golden Oracle” scenario representation. For accuracy, exact matches with the Golden Oracle are required for non-estimated attributes (e.g., Weather, Time, Lane Count). For estimates (e.g., Road Length, Lane Width), a threshold-based method is applied: outputs from the LLM are considered accurate if they differ from the Golden Oracle by no more than 10 meters for road length and 1 meter for lane width. Since actor trajectory data lacks real-world coordinates, we visualized extracted trajectories for human evaluators to assess their realism. In a survey format, evaluators received task guidelines and rating criteria (e.g., a "Totally Match" rating requires matching vehicle count, trajectories, and relative positions). They rated the similarity between original and extracted trajectories on a 5-point scale, from "Totally Match" to "Totally Not Match." These questionnaires are available \href{https://drive.google.com/drive/folders/1te0VYByco3Xx-L8G5M9ECNS8m0pWe2rL?usp=sharing}{here}.

% In evaluating the accuracy of actor trajectories, we face the challenge that the original data lacked real-world coordinate trajectories and that multiple valid coordinate points could exist for a single trajectory. To address this, we choose to visualize the extracted vehicle trajectories and enlisted human validators to assess their realism through a survey-based evaluation.

% Using a survey format, we first provide respondents with a detailed explanation of the task background and requirements. For each rating level, we outline the general criteria for assessment. For example, for a rating of "Totally Match," respondents were instructed that the visualization should match the original crash report in terms of the number of vehicles, their trajectories, and relative positions. Respondents then rate the similarity between the vehicle trajectories depicted in the original crash report and those extracted by {\tool} on a 5-point scale, ranging from "Totally Match" to "Totally Not Match."

%revised by Yang

% These questionnaires are available \href{https://drive.google.com/drive/folders/1te0VYByco3Xx-L8G5M9ECNS8m0pWe2rL?usp=sharing}{here}.

% \begin{enumerate}
%     \item \href{https://forms.gle/bb53kpmhvCrdJYur8}{Trajectory Evaluation Survey (1st 10 cases)}
%     \item \href{https://forms.gle/NHcCvD94zactaAQ99}{Trajectory Evaluation Survey (2nd 10 cases)}
%     \item \href{https://forms.gle/ubBpJxz4unqwJQyJA}{Trajectory Evaluation Survey (3rd 10 cases)}
%     \item \href{https://forms.gle/MthwVdTXDqqC6T2UA}{Trajectory Evaluation Survey (4th 10 cases)}
%     \item \href{https://forms.gle/WSoFig6hvYxr4V2k7}{Trajectory Evaluation Survey (5th 10 cases)}
% \end{enumerate}
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Graphs/LCTGen_scene_con.pdf}
  \caption{Comparison of Results from LCTGen and TRACE}
  \label{fig:results_compar}
\vspace{-4mm}
\end{figure}

\subsubsection{Settings for RQ2}
To determine whether the critical scenarios generated by {\tool} accurately reflect the accident situations described in the reports and to compare its performance with LCTGen, we randomly sample one-third of the critical scenarios generated by {\tool} and LCTGen, respectively. Using a survey questionnaire, we ask respondents to rate the consistency between the scenarios generated by the two models and those described in the original accident reports. Respondents provide ratings on a 5-point scale, ranging from "Totally Match" to "Totally Not Match". These questionnaires are available \href{https://drive.google.com/drive/folders/1te0VYByco3Xx-L8G5M9ECNS8m0pWe2rL?usp=sharing}{here}.

% To determine whether the critical scenarios generated by {\tool} accurately reflect the accident situations described in the reports, we first sample one-third of the generated critical scenarios. Using a survey questionnaire, we ask respondents to rate the fit between the scenarios generated by TRACE and those described in the original accident reports. Similarly, respondents provide ratings on a 5-point scale, ranging from 'Totally Match' to 'Totally Not Match.'
% \begin{enumerate}
%     \item \href{https://forms.gle/JHT9oGTE2k58Rb3s9}{Scenario Construction Results Evaluation Survey (1st 10 cases)}
%     \item \href{https://forms.gle/pqZYYamj35n5v4jM9}{Scenario Construction Results Evaluation Survey (2nd 10 cases)}
%     \item \href{https://forms.gle/nLMa36fv2URqTt2Q7}{Scenario Construction Results Evaluation Survey (3rd cases)}
%     \item \href{https://forms.gle/xzBLYanCH626LT8B6}{Scenario Construction Results Evaluation Survey (4th 10 cases)}
% \end{enumerate}
\subsubsection{Settings for RQ3}
% To address RQ3, we first employ {\tool} and LCTGen on these 50 crash cases, using the same simulator - MetaDrive and ADS - IDM Policy. We report metrics including the number of generated scenarios and the number of detected bugs. To further assess {\tool}’s performance, we also employ {\tool} on MetaDrive with PPO Policy and tested the scenarios generated by {\tool} in the BeamNG simulator with the Auto ADS. For comparison, we sample one-third of the scenarios generated by LCTGen and conducted a survey to evaluate the realism of the baseline model’s scenarios. In the survey, we ask human validators to assess how well the scenarios generated by LCTGen reflected the original crash reports on a 5-point scale, ranging from "Totally Match" to "Totally Not Match".
To address RQ3, we apply {\tool} and LCTGen on 50 crash cases using the MetaDrive simulator with the ADS-IDM, reporting metrics such as scenario count and detected bugs. We further test {\tool} on MetaDrive with the ADS-PPO and on BeamNG with the Auto ADS.
% For comparison, we sample one-third of LCTGen's scenarios and conduct a survey, asking human validators to rate the realism of these scenarios on a 5-point scale from "Totally Match" to "Totally Not Match." The questionnaire is available \href{https://drive.google.com/drive/folders/1te0VYByco3Xx-L8G5M9ECNS8m0pWe2rL?usp=sharing}{here}.

\subsubsection{Settings for RQ4}
To evaluate the effectiveness of our proposed prompt engineering and validation methods, we perform ablation studies. First, we remove the prompt selection process from Stage I and measure the accuracy of the scenario representations. Next, we eliminate the self-validation process and measure the scenario representation accuracy again.