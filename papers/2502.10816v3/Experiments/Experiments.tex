\section{Experiments and analysis} \label{sec:analysis}

\begin{table*}[!t]
\small
\caption{\textbf{Comparison of all the multimodal imbalance algorithms.} Bold and underline represent the best and second best respectively. }
\vspace{-5pt}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{ccccccccccccccc}
\toprule
Method &
  \multicolumn{2}{c}{KineticsSounds} &
  \multicolumn{2}{c}{CREMA-D} &
  \multicolumn{2}{c}{UCF-101} &
  \multicolumn{2}{c}{FOOD-101} &
  \multicolumn{2}{c}{CMU-MOSEI} &
  \multicolumn{2}{c}{BalancedAV} &
  \multicolumn{2}{c}{VGGSound} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
           & ACC     & F1   & ACC     & F1   & ACC     & F1   & ACC     & F1   & ACC     & F1   & ACC     & F1   &ACC &F1\\ \midrule
Unimodal-1 & 55.06    & 54.96 & 59.38      & 59.23 & 70.55    & 69.94 & 86.19      & 86.10 & 71.09 & 41.70  & 65.34 & 62.12    & 41.27    & 40.32 \\
Unimodal-2 & 45.31    & 43.76 & 58.10      & 56.81 & 78.60    & 77.49 & 65.67      & 65.47 & 71.03  & 41.68   & 50.55 & 47.14    & 30.43 & 29.61    \\
Unimodal-3 & --    & -- & --      & -- & --    & -- & --      & -- & 80.58    & 74.57 & --    & -- & --    & -- \\
Baseline   & 65.63 & 65.28 & 65.50  & 65.07 & 81.80 & 81.21 & 91.65 & 91.60 & 78.99 & 69.40 & 73.33 & 70.73 & 48.08    & 46.98 \\ \midrule
MMCosine   & 67.49 & 67.09 & 67.19 & 67.34 & 82.97 & 82.47 & 92.16 & 92.12 & 80.38 & 73.67 & 75.05 & \underline{72.57} & 48.73    & 47.66 \\
UMT       & 68.60 & 68.43 & 67.47 & 67.75 & 84.18 & 83.56 & 93.02 & 92.96 & 80.73 & 73.60 & 74.35 & 71.68  & \textbf{51.58}    & \textbf{50.48} \\
MBSD       & 68.82 & 68.28 & 74.86 & 75.48 & 84.61 & 84.26 & \textbf{93.16} & \textbf{93.09} & 79.41 & 71.13 & 75.13 & 72.08 & 49.48    & 47.99 \\
CML        & 67.56 & 67.22 & 69.18 & 69.57 & 84.74 & 84.28 & 92.70 & 92.66 & 79.69 & 73.16 & 71.85 & 68.58 & 50.50    & 49.30 \\
GBlending  & 68.82 & 66.43 & 71.59 & 71.72 & 85.01 & 84.50 & 92.56 & 92.50 & 79.64 & 73.29 & 74.19 & 71.57 & \underline{51.41}    & \underline{50.39} \\
MMPareto   & \textbf{74.55}    & \textbf{74.21} & \textbf{79.97}    & \textbf{80.57} & 85.30    & 84.89 & 92.82    & 92.77 & \textbf{81.18 }   & \textbf{74.64} & \underline{75.26}    & 72.16 & 49.35    & 48.48 \\
LFM         & 66.37    & 66.02 & 70.02    & 69.55 & 84.95    & 84.35 & 92.58    & 92.54 & 79.90    & 71.60 & 73.82    & 70.79 & 47.45    & 46.50 \\
\textbf{Objective Avg} & 68.89 & 68.24 & 71.47 & 71.71 & 84.53 & 84.04 & 92.71 & 92.66 & 80.13 & 73.01 & 74.23 & 71.34 & 49.79 & 48.69 \\ \hline
OGM         & 67.04 & 66.95 & 67.76 & 68.02 & 82.07 & 81.30 & 91.81 & 91.77 & 80.45 & 73.61 & 73.83 & 71.49 & 48.25    & 47.16 \\
AGM        & 66.62 & 65.88 & 71.59 & 72.11 & 81.70  & 80.89 & 91.89 & 91.84 & 79.86 & 71.89 & \textbf{75.49 }& \textbf{73.09} & 49.06    & 47.70 \\
PMR        & 67.11 & 66.87 & 67.19 & 67.20 & 81.93 & 81.48 & 92.10 & 92.04 & 79.88 & 72.09 & 73.70 & 71.04 & 50.38    & 49.01 \\
Relearning  & 65.92    & 65.48 & 71.02    & 71.46 & 82.87    & 82.15 & 91.68    & 91.63 & 78.65    & 70.02 & 73.96    & 71.62 & 48.12    & 47.04 \\ 
ReconBoost  & 68.38    & 67.68 & 74.01    & 74.52 & 82.89    & 82.26 & 92.47    & 92.44 & \underline{81.01}    & \underline{74.03} & 74.66    & 72.03 & 47.27    & 46.26 \\
\textbf{Optimization Avg} & 67.01 & 66.57  & 70.31 & 70.66 & 82.29 & 81.62 & 91.99 & 91.94 & 79.97 & 72.33 & 74.33 & 71.85 & 48.62 & 47.43 \\ \midrule
MLA       & 69.05    & 68.75 & 72.30    & 72.66 & \underline{85.38}    & \underline{84.84} & \underline{93.14}    & \textbf{93.09} & 78.65    & 70.02 & 73.80    & 70.82 & 49.99    & 48.62 \\
OPM        & 66.89    & 66.44 & 68.75    & 69.00 & 85.28    & 83.79 & 93.08    & \underline{93.04} & 79.95    & 72.83 & 75.03    & 72.27 & 49.12    & 48.24 \\
Greedy    & 66.82 & 66.53 & 66.48 & 66.54 & -- & -- & -- & -- & --    & -- & 73.80 & 71.21 & 48.65    & 47.41 \\
AMCo       & \underline{70.54} & \underline{69.95} & 73.30 & 73.95 & \textbf{86.91} & \textbf{86.66} & 92.73 & 92.68 & 79.51 & 71.14 & 75.00 & 72.02 & 49.05    & 47.18 \\ 
\textbf{Forward Avg} & 68.32 & 67.91 & 70.21 & 70.54 & 85.86 & 85.10 & 92.66 & 92.94 & 79.37 & 71.33 & 74.41 & 71.58 & 49.20 & 47.86 \\ \midrule
Modality-valuation     & 68.01    & 68.03 & \underline{75.85}    & \underline{76.68} & 85.25    & 84.69 & 92.20    & 92.15 & 79.84    & 72.99 & 73.52    & 70.61 & 48.25    & 47.22 \\ \bottomrule
\end{tabular}
\vspace{-10pt}
\label{tab:experiments}
\end{table*}



                            \subsection{Experimental outcomes} 
We evaluated the effectiveness of all related methods discussed in Section \ref{sec:taxonomy}. Unimodal-1 refers to training the model using only the audio modality for KineticsSounds, CREMA-D, CMU-MOSEI, BalancedAV, and VGG. For UCF-101, it corresponds to the optical flow modality, while for FOOD-101, it refers to the text modality. Unimodal-2 refers to training the model using only the video modality for KineticsSounds, CREMA-D, CMU-MOSEI, BalancedAV, and VGG. For FOOD-101, it refers to image modality. For UCF-101, it corresponds to the RGB modality. Unimodal-3 applies only to CMU-MOSEI, where the model is trained using the text modality. Baseline refers to the commonly used approach in multimodal imbalance learning, which employs concatenation fusion with a single multimodal cross-entropy loss function. As shown in Table \ref{tab:experiments}, we conducted comprehensive experiments using the proposed BanlenceBenchmark on 7 datasets. The results indicate that almost all related methods outperform the Baseline in terms of accuracy and F1 score, demonstrating that the multimodal imbalance problem is prevalent across various scenarios. Meanwhile, addressing this problem is crucial for improving model performance. 

\begin{table}[h]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}
    \caption{Average FLOPs of different categories.}
    \vspace{-5pt}
    \begin{tabular}{lcccc}
        \toprule
        Categories & Objective & Optimization & Forward & Data \\
        \midrule
        FLOPs \footnotesize{( $\times 10^{13}$ )}  & 8.38 & 16.90 & 6.94 & 11.40 \\
        \bottomrule
    \end{tabular}
    \vspace{-10pt}
    \label{tab:flops}
\end{table} 


\subsection{Analysis}
\subsubsection{Comparison of different categories of methods} 
The four categories of methods exhibit different characteristics when addressing the multimodal imbalance problem. \textit{Firstly,} as shown in Table \ref{tab:experiments}, objective-based methods perform well across all datasets except BalancedAV. This is because adjusting the learning objective significantly promotes the update dynamics of the weak modalities, thus alleviating the imbalance problem. When the imbalance degree is relatively high, improving the update dynamics of the weaker modalities effectively facilitates their learning, leading to better performance of the multimodal model. However, on BalancedAV, which exhibits the lowest imbalance degree,  performance of objective-based methods is worse than that of optimization-based and forward-based methods.
\textit{Secondly,} optimization-based methods perform well on datasets with a small imbalance degree, such as BalancedAV. This is because optimization-based methods provide fine-grained adjustments over the multimodal model's learning process. When the degree of imbalance is small, these methods can more precisely balance the modalities. However, as shown in Table \ref{tab:flops}, optimization-based methods have the highest average FLOPs, which results in greater computational resource requirements. \textit{Thirdly,} forward-based methods have the smallest average FLOPs. This is because they adjust the model in terms of feature processing and fusion methods, introducing minimal additional computational overhead. For example, Greedy \cite{Greedy_Wu} employs a specific-designed fusion to address the imbalance problem. However, this  characteristic limits the applicability of forward-based methods in general frameworks.  \textit{Fourthly,} Modality-valuation \cite{Sample_wei} is the only approach that addresses the multimodal imbalance problem at the data level. It improves the quality of the training data, but also introduces relatively high computational costs. \textit{These findings suggest that no existing method achieves a satisfactory balance between performance and computational cost.}


\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.50\linewidth}
    \centering
    \includegraphics[width=\linewidth]{AGM-Imbalance.pdf}
    \vspace{-15pt}
    \caption{AGM.}
    \label{fig:AGM_Imbalance}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.50\linewidth} 
    \centering
    \includegraphics[width=\linewidth]{Gblending-Imbalance.pdf}
    \vspace{-15pt}
    \caption{Gblending.}
    \label{fig:Gblending_imbalance}
  \end{subfigure}
  \vspace{-10pt}
  \caption{\textbf{(a).} Absolute and relative balance for AGM. \textbf{(b).} Absolute and relative balance for GBlending. Experiments are conducted on CREMA-D, with these two methods selected as representative cases.}
  \vspace{-15pt} 
  \label{fig:imbalance}
\end{figure}

\subsubsection{Relative balance}
We conducted comprehensive experiments to investigate the relationship between model performance and the degree of imbalance. To quantify the imbalance degree, we utilized the Shapley-based method introduced in Section \ref{sec:tool}, where higher values indicate a higher imbalance degree and lower values reflect better balance between modalities. By adjusting the hyperparameters of various methods, we obtained different combinations of imbalance degree and performance. Specifically, we identified the points with the lowest imbalance degree and the highest performance.

As illustrated in Figure \ref{fig:imbalance}, we selected visualizations from two methods to demonstrate the relationship between performance and imbalance degree. The original baseline exhibited a high imbalance degree and relatively low accuracy. Through hyperparameter tuning, we adjusted the imbalance degree between modalities and obtained varying performance results. 
When the imbalance degree is high, gradually reducing it leads to continuous performance improvement. However, once the imbalance degree reaches a relatively low level, further reduction no longer enhances performance. We refer to this point as the \textit{relative balance point}. Beyond this point, further decreasing the imbalance degree achieves the absolute balance point, where the imbalance between modalities is minimized. However, the performance at the \textit{absolute balance point} is inferior to that at the relative balance point and can even fall below the baseline. This phenomenon occurs because different modalities contain varying amounts of information. An excessive focus on balance may cause the model to undervalue high-information modalities, leading to reduced effectiveness in learning from these modalities.

\subsubsection{Future work}
Based on the analysis above, we provide several insights for future research in this field. \paragraph{Hybrid strategies.} Future research could explore hybrid strategies that integrate the strengths of different methods while mitigating their limitations. For instance, a more fine-grained adjustment of the learning objective could combine the advantages of both objective-based and optimization-based methods. 
\paragraph{Pursue relative balance.} When addressing the imbalance problem, it is important to recognize that different modalities inherently contain different amounts of information. Therefore, maintaining a relatively balanced state among modalities is preferable to blindly pursuing absolute balance. Future work could further explore efficient strategies to achieve relative balance across modalities, ensuring that models can effectively leverage the unique contributions of each modality
\paragraph{Multimodal imbalance in foundation models.} Existing methods for addressing multimodal imbalance remain limited to traditional neural networks and relatively small datasets.  However, recent studies have identified the multimodal imbalance problem in mixed-modal foundation models \cite{team2024chameleon,aghajanyan2023scaling,the2024large}. For example, studies on Chameleon \cite{team2024chameleon} shows that different modalities compete with each other with the standard LLaMA architecture \cite{touvron2023llama}. Future work could extend network architectures to foundation models.

