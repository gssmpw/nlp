\vspace{-5pt}
\section{Toolkit}
\label{sec:tool}
To accompany BalanceBenchmark, we propose a comprehensive toolkit named \textbf{BalanceMM}, that incorporates 17 multimodal imbalance algorithms. Although these algorithms cover various methodological aspects, \textit{the toolkit provides a standardized implementation that unifies their evaluation and comparison.} Due to its modular architecture, BalanceMM allows flexible integration of various datasets, modalities, backbones and methods. This makes it extensible, allowing users to easily add new components to the overall framework. 

\subsection{Datasets and modalities} \label{sec:tool1}
BalanceMM includes 7 datasets covering multiple modalities. These datasets include both bimodal and trimodal datasets, each with varying imbalance degrees, allowing for a more comprehensive evaluation of different methods. To streamline the utilization of these datasets, we develop standardized data loaders for each dataset, ensuring consistency and reproducibility across experiments. A more detailed description of these datasets can be found in Section \ref{sec:datasets}, where we discuss their characteristics in depth.

\subsection{Backbones} \label{sec:tool2}
% plug-and-play
To provide adaptability to different modalities, BalanceMM supports alternative backbones, including ResNet18 \cite{ResNet} and Transformer \cite{Transformer}. Vision Transformer (ViT) \cite{ViT}, which serves as a variant of Transformer specifically designed for vision tasks, is also supported. Users can choose to use a backbone trained from scratch or select a pre-trained version, depending on their specific needs. Designed as a plug-and-play component, the backbone integrates seamlessly into the workflow. Moreover, the toolkit is extensible, allowing users to easily incorporate new backbones for a wide range of applications.

\subsection{Multimodal imbalance algorithms} \label{sec:tool3}
BalanceMM covers 17 multimodal imbalance algorithms spanning 4 methodological categories defined in Section \ref{sec:taxonomy}. As summarized in Table \ref{tab:mm_algorithms}, these algorithms encompass various modality combinations and application domains, such as Computer Vision (CV), Natural Language Processing (NLP), and audio. A configuration-based workflow enables the activation of any method with a single command, while maintaining the original specifications. The implementation of multimodal imbalance algorithms is illustrated in Algorithm \ref{algorithm:1}.


\subsection{Evaluation metrics} \label{sec:tool4}
BalanceMM offers unified evaluation metrics to assess multimodal imbalance methods by the criteria below.

\paragraph{Performance.} We utilize Top-1 accuracy and F1 score as our performance evaluation metrics, which are widely used in classification task.

\paragraph{Imbalance.} To quantitatively assess the degree of imbalance in multimodal learning, we introduce a metric based on the Shapley value \cite{shapley:book1952}. For a multimodal dataset with a modality set $M$, contribution $\phi^i$ for modality $i$ is computed by the Shapley value as below:
\begin{equation} \label{equ:1}
\phi^i = \frac{1}{|M|!} \sum_{\pi \in \Pi_M} \left[ v(S_\pi^i \cup \{i\}) - v(S_\pi^i) \right] ,
\end{equation}
where $\Pi_M$ denotes all permutations of $M$, $S_\pi^i$ represents the set of modalities preceding $i$ in permutation $\pi$, and $v(A)$ is the value function measuring model performance when using modality subset $A \subseteq M$. 
The value function $v(A)$ is implemented through masked evaluation, where the performance of the model is measured by the accuracy, calculated as follows:

\begin{equation} \label{equ:2}
v(A) = \frac{\sum_{k=1}^{N} \mathbf{1}(\hat{y}_k = y_k)}{N},
\end{equation}

The imbalance metric $\mathcal{I}$ is then defined as follows: for the bimodal case, 
\begin{equation} \label{equ:4}
\mathcal{I} = |\phi^1 - \phi^2|,
\end{equation} and for the trimodal case, 
\begin{equation} \label{equ:5}
\mathcal{I} = \frac{1}{3}\left(|\phi^1-\phi^2| + |\phi^1-\phi^3| + |\phi^2-\phi^3|\right).
\end{equation}
This metric satisfies three key properties:
\begin{itemize}
    \item \textbf{Null contribution}: \(\mathcal{I} = 0\) when all modalities contribute equally.
    \item \textbf{Bounded range}: \(\mathcal{I} \in [0, 1]\), following its calculation principle.
    \item \textbf{Permutation invariance}: The metric is invariant to the ordering of modalities.
\end{itemize}

This Shapley-based metric explicitly measures how much each modality contributes to the whole performance relative to other modalities. Lower $\mathcal{I}$ values indicate more balanced multimodal cooperation, while higher values suggest dominance by specific modalities. 


\paragraph{Complexity.} To evaluate the computational complexity of various methods, our toolkit measures the number of \textbf{floating-point operations (FLOPs)} required during training. FLOPs represent the total number of arithmetic operations, where higher FLOPs indicate greater computational cost. This metric can help to compare efficiency of different algorithms and assess the trade-off between performance and computational overhead.

\subsection{Implementation pipeline} \label{sec:tool5}
In Algorithm \ref{algorithm:1}, we provide a reference implementation in the BalanceMM framework. The modular architecture of BalanceMM facilitates the efficient integration of various components. This not only makes the toolkit a powerful resource for evaluating multimodal imbalance algorithms, but also streamlines the experimental workflow while maintaining robust performance and adaptability.
\begin{algorithm}[t] 
\caption{The pseudo code for multimodal imbalance algorithms implementation with BalanceMM toolkit}
\begin{algorithmic} \label{algorithm:1}
\setlength{\leftskip}{0pt} % Removes left margin for algorithm
\raggedright

\STATE \hspace*{-1em}\textbf{Input:} The selected multimodal imbalance method $\mathcal{F}$; 
\STATE \hspace*{-1em}specific hyper-parameters for the method denoted as $\alpha$;
\STATE \hspace*{-1em}the selected dataset $D$; global configuration (args).

\STATE \hspace*{-1em}\textbf{Output:} model, training logs and evaluation metrics.

\STATE \hspace*{-1em}from BalanceMM.utils.data\_utils import create\_dataloader
\STATE \hspace*{-1em}from BalanceMM.models import create\_model

\STATE \hspace*{-1em}from BalanceMM.trainers import create\_trainer

\STATE \hspace*{-1em}\textcolor{gray}{\# Load the selected dataset}
\STATE \hspace*{-1em}train\_data, val\_data, test\_data = create\_dataloader($D$)

\STATE \hspace*{-1em}\textcolor{gray}{\# Modify specific components based on method type}
\STATE \hspace*{-1em}\textbf{if} $\mathcal{F}$ in \textit{Objective} \textbf{then}
\STATE \hspace*{0em}args.trainer.loss = $L_{new}$

\STATE \hspace*{-1em}\textbf{elif} $\mathcal{F}$ in \textit{Optimization} \textbf{then}
\STATE \hspace*{0em}Set up modulation mechanism $G$ with $\alpha$ adjusting intensity of optimization
\STATE \hspace*{0em}args.trainer.modulation = $G$
\STATE \hspace*{-1em}\textbf{elif} $\mathcal{F}$ in \textit{Feed-forward} \textbf{then}
\STATE \hspace*{0em}Modify args.model.feature\_process and args.model.fusion\_module based on $\mathcal{F}$

\STATE \hspace*{-1em}\textbf{elif} $\mathcal{F}$ in \textit{Data} \textbf{then}

\STATE \hspace*{0em} args.trainer.if\_resample = True
\STATE \hspace*{-1em}model = create\_model(args.model)
\STATE \hspace*{-1em}trainer = create\_trainer(args.trainer)
\STATE \hspace*{-1em}trainer.fit(model, train\_data, val\_data)
\STATE \hspace*{-1em}performance, imbalance, complexity = 
\STATE \hspace*{-1em}trainer.evaluation(model, test\_data)
\end{algorithmic}
\end{algorithm}
