\section{Taxonomy}
\label{sec:taxonomy}
In this section, we present our taxonomy for mitigating the multimodal imbalance problem based on the strategies for handling modality imbalance. As shown in Table \ref{tab:mm_algorithms}, we categorize these methods into four groups: Data, Feed-forward, Objective and Optimization. We also summarize the different types of imbalance indicator, which different methods use to evaluate the performance of different modalities.

\begin{table*}[t] 
    \centering
    \setlength{\tabcolsep}{5pt}
    \caption{Multimodal imbalance algorithms. \textbf{Adjustment Strategy} refers to different groups of methods in Section \ref{sec:taxonomy}. \textbf{Imbalance Indicator} denotes the metric used to evaluate modality performance. \textbf{Number of Modalities} indicates the maximum number of modalities included in the experiments of the corresponding paper. \textbf{Dataset Domain} refers to the types of modalities included in the corresponding paper.} 
\vspace{-5pt}
\label{tab:mm_algorithms} 
    \begin{tabular}{l|c|c|c|ccc}
        \toprule
        \multicolumn{1}{c}{}            & \multicolumn{1}{c}{Adjustment}       & \multicolumn{1}{c}{Imbalance}         & \multicolumn{1}{c}{Number of}                & \multicolumn{3}{c}{Dataset Domain} \\
        \multicolumn{1}{l}{Method}   & \multicolumn{1}{c}{Strategy}         & \multicolumn{1}{c}{Indicator}    & \multicolumn{1}{c}{Modalities}         & CV & NLP & Audio \\                 
        \midrule
                                                                
        % Here, we start listing the papers  
        Modality-valuation \cite{Sample_wei}       & Data                                     & Shapley-based Metric                                                     & $2$                                        & \checkmark    &  & \checkmark \\
        MLA \cite{MLA}       & Feed-forward                                     & N/A                                                    & $3$                                          & \checkmark    & \checkmark & \checkmark \\
        OPM \cite{OPM_PAMI}       & Feed-forward                                     & Performance Score                                                    & $3$                                         & \checkmark    & \checkmark & \checkmark \\
        Greedy \cite{Greedy_Wu}       & Feed-forward                                     & Gradient Change                                                     & $2$                                         & \checkmark    &  & \checkmark \\
        AMCo \cite{AMCo}       & Feed-forward                                     & Performance Score                                                     & $3$                                         & \checkmark    & \checkmark & \checkmark \\
        MMCosine \cite{MMCosine_Xu} & Objective                                   & Performance Score                                                       & $2$                                   &     \checkmark  &  & \checkmark \\   
        UMT \cite{UMT_Du}         & Objective                                      & N/A                                                     & $3$                                         & \checkmark    &  & \checkmark \\
        MBSD \cite{MBSD}       & Objective                                     & Performance Score                                                     & $2$                                         & \checkmark    & \checkmark &\\
        CML \cite{CML_Ma}          & Objective                                    & Classification Loss                                                    & $2$                                         & \checkmark    & \checkmark  & \\
        MMPareto \cite{MMPareto_Wei}          & Objective                                    & Performance Score                                                    & $3$                                         & \checkmark    & \checkmark & \checkmark  \\
        GBlending \cite{Gblending_Wang}       & Objective                                     & Classification Loss                                                     & $3$                                          &\checkmark     &   & \checkmark   \\
        LFM \cite{LFM_yang}        & Objective                                      &  N/A                                                              & $3$                                             & \checkmark    & \checkmark  &  \checkmark \\
        OGM \cite{OGM_CVPR}  & Optimazation                                     & Performance Score                                                       & $2$                                          & \checkmark    &  & \checkmark \\
        AGM \cite{AGM_Li}   & Optimazation                                       & Performance Score                                                              & $3$                                         & \checkmark    &  \checkmark & \checkmark  \\
        PMR \cite{PMR_Fan}       & Optimazation                                     & Prototype                                                       & $2$                                           & \checkmark    &  & \checkmark\\
        Relearning \cite{Relearning_wei}       & Optimazation                                     & Clustering                                                     & $3$                                          & \checkmark    & \checkmark & \checkmark \\
        ReconBoost \cite{ReconBoost_Huang}       & Optimazation                                     & Classification Loss                                                    & $3$                                          & \checkmark    & \checkmark & \checkmark \\

        \bottomrule                                
    \end{tabular}
    \vspace{-10pt}
\end{table*}

\subsection{Data}
This part focuses on the method which enhances modality performance through targeted data processing strategies. Wei et al. \cite{Sample_wei} propose a fine-grained evaluation method to facilitate multimodal collaboration. It evaluates modality-specific contributions at the sample level and employs selective resampling techniques to enhance the discriminative capabilities of weak modality modalities.

\subsection{Feed-forward}
These methods alleviate the imbalanced learning across modalities by modifying the forward process during model training and inference. These methods can be categorized into two types based on where modifications are made.

\textbf{Feature Processing.} The first type of methods adjust features during training. Adaptive Mask Co-optimization (AMCo) \cite{AMCo} masks features of dominant modalities based on their performance, while On-the-fly Prediction Modulation (OPM) \cite{OPM_PAMI} drops its feature with dynamical probability in feed-forward stage.

\textbf{Fusion Module.} The second type achieves modality balance by modifying the fusion mechanisms. Multimodal Learning with Alternating Unimodal Adaptation (MLA) \cite{MLA} uses dynamic fusion to integrate different modalities. It also employs an alternating optimization approach to optimize unimodal encoders, minimizing interference between modalities. Greedy \cite{Greedy_Wu} utilizes the MMTM \cite{MMTM_Joze} architecture for intermediate fusion to boost the modality interaction. It also facilitates the learning of weak modality that indicated by conditional learning speed, which is measured by the gradient change ratio.

\subsection{Objective}
Various methods for addressing modality imbalance in multimodal learning focus on modifying objectives. These methods can be categorized into three main directions: 

Firstly, several methods modify the multimodal loss function to mitigate the multimodal imbalance problem. For instance, Multi-Modal Cosine loss (MMCosine) \cite{MMCosine_Xu} proposes a multimodal cosine loss, which effectively increases the learning proportion of weaker modalities by weight constraints and inter-symmetric constraints.
% and inter-modality symmetry constraints.

Secondly, a group of methods leverage modality differences for learning objectives to achieve balanced learning. MBSD \cite{MBSD} constrains the model using the Kullback-Leibler (KL) \cite{KL} divergence of prediction distributions between different modalities to reduce their distance. Calibrating Multimodal Learning (CML) \cite{CML_Ma} uses confidence loss derived from different modalities, which lowers the confidence of the dominant modality. LFM \cite{LFM_yang} bridges heterogeneous data in the feature space through contrastive learning, reducing the distance between different modalities.

Thirdly, several approaches incorporate unimodal loss into the objectives to mitigate the imbalance problem. Uni-Modal Teacher (UMT) \cite{UMT_Du} introduces a unimodal distillation loss, enhancing the learning of unimodal encoders. Gradient-Blending (GBlending) \cite{Gblending_Wang} and MMPareto \cite{MMPareto_Wei} utilize unimodal losses to solve the imbalance problem. GBlending \cite{Gblending_Wang} uses overfitting-to-generalization-ratio (OGR) as an indicator to show which modality is dominant and its corresponding weight, while MMPareto \cite{MMPareto_Wei} borrows ideas from Pareto method \cite{ParetoInMultitask} to guarantee the final gradient is with direction common to all learning objectives to boost the learning of weak modality. 

\subsection{Optimization}
Recent studies have investigated optimization-based approaches to mitigate the multimodal imbalance problem. Both On-the-fly Gradient Modulation (OGM) \cite{OGM_CVPR} and Adaptive Gradient Modulation (AGM) \cite{AGM_Li} aim to balance modality learning by slowing down the gradients of dominant modalities to provide more optimization space for weak modalities. Specifically, OGM \cite{OGM_CVPR} uses performance score as an indicator to achieve this, while AGM \cite{AGM_Li} employs a Shapley value-based method for gradient adjustment. Prototypical Modality Rebalance (PMR) \cite{PMR_Fan} adjusts gradient magnitudes based on category prototypes to accelerate the learning of weak modalities. Diagnosing \& Re-learning (Relearning) \cite{Relearning_wei} uses re-initialization to reduce the dependence on dominant modalities while preventing weak modalities from learning excessive noise. ReconBoost \cite{ReconBoost_Huang} introduces an alternating-boosting optimization way to enhance the unimodal performance, which alleviates the imbalance problem.