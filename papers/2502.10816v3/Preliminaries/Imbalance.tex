\label{subsec:imbalance}
Multimodal learning aims to leverage diverse information from different modalities to enhance model performance \cite{Bal_MM}. However, recent studies have revealed the multimodal imbalance problem, where models tend to over-rely on some modalities while underutilizing others \cite{OGM_CVPR}. This imbalance leads to suboptimal exploitation of the available multimodal information.
\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.96\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{multimodal_comparison.pdf} 
    \caption{Multimodal model performance.}
    \label{fig:multimodal_comparison}
  \end{subfigure}
  \vspace{-5pt} 
  \begin{subfigure}[t]{0.48\linewidth} 
    \centering
    \includegraphics[width=0.9\linewidth]{audio_comparison.pdf}
    \caption{Audio performance gap}
    \label{fig:audio_comparison}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{visual_comparison.pdf}
    \caption{Video performance gap}
    \label{fig:visual_comparison}
  \end{subfigure}
  \caption{\textbf{(a).}  Performance comparison of the multimodal model with its unimodal counterparts on CREMA-D. \textbf{(b).} Performance gap between audio modality within multimodal model and audio-only model on CREMA-D. \textbf{(c).} Performance gap between video modality within multimodal model and video-only model on CREMA-D.}
  \vspace{-10pt} 
  \label{fig:imbalance illustration}
\end{figure}

We consider a general multimodal learning framework for the illustration of imbalance phenomenon. Let $D_{train} = \{(x_k,y_k)\}_{k=1}^{N}$ denote the multimodal training dataset. Each sample $x_k = (x_k^1,x_k^2, \dots,x_k^m)$ consists of $m$ modalities, and $y_k \in \{1,2,\dots,H\}$ denotes the corresponding class label from $H$ classes.
In a multimodal model, each modality uses its own encoder $\Phi^i(\theta^i,\cdot)$ with parameters $\theta_i$. For simplicity, we write it as $\Phi^i$.
As the example, we take the most widely used vanilla fusion method, concatenation. Then the logits output of the multimodal model can be written as :
\begin{equation} \label{equ:1}
    f(x_k) = W[\Phi_k^1;\Phi_k^2;\cdots;\Phi_k^m] + b,
\end{equation}
where $W \in \mathbb{R}^{H \times \sum_i^m d_{\Phi^i}}$ and $b \in \mathbb{R}^{H}$ are the parameters of the last linear classifier. $W$ can be represented as the combination of $m$ blocks: $[W^1,W^2,\cdots,W^m]$. The equation can be rewritten as: 
\begin{equation}
    f(x_k) = \sum_{i=1}^m W^i \cdot \Phi_k^i + b    
\end{equation}
We denote $\hat{y}_k$ as the classification result of $x_k$ by logits output $f(x_k)$. Then the cross-entropy loss is calculated as:
\begin{equation}
    L = \frac{1}{N}\sum_{k=1}^N\ell(\hat{y}_k, y_k),
\end{equation}
where $\ell$ denotes cross-entropy loss. 

With the Gradient Descent optimization method, $W^i$ and the parameters of encoder $\Phi^i$ are updated as:
\begin{equation}
    W^i_{t+1} = W^i_t - \eta\frac{1}{N}\sum_{k=1}^N \frac{\partial \mathcal{L}}{\partial f(x_k)}\Phi_k^i,
\end{equation}
\begin{equation}
    \theta^i_{t+1} = \theta^i_t - \eta\frac{1}{N}\sum_{k=1}^N \frac{\partial \mathcal{L}}{\partial f(x_k)}\frac{\partial(W^i_t \cdot \Phi_k^i)}{\partial \theta^i_t},
\end{equation}
where $\eta$ is the learning rate. 
According to the gradient update equations, the term $\frac{\partial \mathcal{L}}{\partial f(x_k)}$ can be further derived as:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial f(x_k)_{\hat{y}_k}} = \frac{e^{\sum_{i=1}^m W^i\cdot\Phi_k^i + b_{\hat{y}_k}}}{\sum_{h=1}^H e^{\sum_{i=1}^m W^i\cdot\Phi_k^i + b_h}} - \mathbf{1}_{{\hat{y}_k}=y_k}
\end{equation}
Based on the gradient update equations, recent studies have revealed that when one modality has better performance, its contribution $W^i \cdot \Phi_k^i$ dominates the logits output $f(x_k)$. This reduces the magnitude of $\frac{\partial \mathcal{L}}{\partial f(x_k)}$, as the loss $\mathcal{L}$ already becomes smaller. Consequently, gradients for updating weaker modalities are suppressed, leading to under-optimized representations for them.

To further verify the multimodal imbalance problem, we conduct experiments on CREMA-D dataset \cite{cremad}. It is a widely used audio-video dataset, particularly suitable for studying modality imbalance. We compare three settings: (1) the multimodal model that jointly learns from both audio and video modalities, (2) the unimodal counterparts within this multimodal model, and (3) unimodal models trained using only single modality data. As shown in Figure \ref{fig:imbalance illustration}, while the multimodal model outperforms the unimodal counterparts, both audio and video modalities in the multimodal model performs worse than when trained alone. Besides, the video modality shows a bigger drop in performance, which means weaker modalities are supressed during training. These results align with the previous analysis about the imbalance problem. To alleviate this problem, recent studies have proposed various methods from adjusting the training data distribution to modifying the optimization process.
