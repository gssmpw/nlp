\section{Introduction}
Humans perceive the real world through multiple sensory modalities, such as visual, auditory, and haptic inputs. This rich interplay of modalities has driven extensive research into multimodal learning \cite{Bal_MM}. However, recent studies have identified a critical challenge in this field: the multimodal imbalance problem, where certain modalities disproportionately dominate the behavior of multimodal models \cite{Peng_2022_CVPR}, which impairs the integration and utilization of information across different modalities. To address this issue, researchers have proposed a wide range of approaches aimed at mitigating this problem, which has gained increasing attention \cite{Peng_2022_CVPR,Gblending_Wang,MMCosine_Xu,AGM_Li,CML_Ma,Greedy_Wu,MMPareto_Wei,PMR_Fan,UMT_Du,ReconBoost_Huang}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{frame5.pdf}
    \caption{The general framework of multimodal imbalance learning. Group 1 applies adjustments during data processing. Group 2 modifies the fusion module in the feed-forward propagation. Group 3 adapts learning objectives, and Group 4 focuses on optimization adjustments.}
    \label{fig:method_graph}
    \vspace{-15pt}
\end{figure*}

As shown in Figure \ref{fig:method_graph}, these methods employ different strategies within a general framework to address the imbalance problem. However, the lack of comprehensive and fair comparisons makes it difficult to objectively evaluate their effectiveness. This challenge arises from three key issues:
\textit{Firstly, the lack of diverse and representative datasets.} Most multimodal imbalance algorithms have only been evaluated on a limited number of datasets, which do not adequately capture variations in modality counts, modality type, and imbalance degree. This limitation restricts the assessment of a methodâ€™s generalizability across real-world scenarios.
\textit{Secondly, the lack of diverse evaluation metrics.} Existing evaluation metrics primarily emphasize performance improvements while overlooking other critical perspectives such as modality imbalance and computational complexity. Moreover, they fail to explore the relationship between model performance and modality imbalance.
\textit{Thirdly, the lack of a standardized experimental workflow.} The absence of a standardized experimental workflow leads to inconsistent experimental setting. Different studies adopt varying experimental settings, making direct comparisons between methods unreliable. 

Given the challenges and limitations discussed above, we first review recent advancements in multimodal imbalance learning and systematically categorize existing methods based on their underlying principles. We then introduce \textbf{BalanceBenchmark}, a comprehensive evaluation framework designed to assess 17 representative methods across seven multidimensional datasets. These datasets cover a wide range of modality combinations, including audio-visual, text-visual, optical flow-RGB, and audio-visual-text modalities, with sample sizes varying from 10K to 200K. Our evaluation metrics include accuracy and F1-score to measure model performance. To quantify modality imbalance, we use Shapley value \cite{shapley:book1952}, which evaluates the contribution of each modality to the final prediction. Additionally, we assess model complexity using floating point operations (FLOPs), which reflect the computational cost required for training. To ensure fair comparisons, we provide \textbf{BalanceMM}, a modular and extensible toolkit designed to standardize the experimental workflow for evaluating different methods. Based on comprehensive experiments, we find that no existing method achieves a satisfactory balance between performance and computational cost. Meanwhile, greater balance between modalities does not guarantee better performance.

Overall, our main contributions are summarized as follows:
\begin{itemize}
\item \textit{Firstly,} we present a systematic taxonomy of existing methods categorized by their strategies for mitigating the imbalance problem, along with a benchmark, \textbf{BalanceBenchmark}, which includes multidimensional datasets and comprehensive evaluation metrics.
\item \textit{Secondly,} we introduce a modular toolkit \textbf{BalanceMM}, which standardizes the experimental workflow for evaluating different methods.
\item \textit{Thirdly,} we use BalanceMM to conduct comprehensive experiments and analyses on existing methods, offering insights into future research directions.
\end{itemize}

