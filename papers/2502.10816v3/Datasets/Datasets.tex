\section{Datasets and benchmark} \label{sec:datasets and benchmarks}
\subsection{Datasets} \label{sec:datasets}
BalanceBenchmark includes 7 datasets to evaluate different multimodal imbalance algorithms. These datasets include different types and numbers of modalities, as well as varying degrees of imbalance. \textbf{KineticsSounds} \cite{kinetics-sounds}, \textbf{CREMA-D} \cite{cremad}, \textbf{BalancedAV} \cite{balance}, and \textbf{VGGSound} \cite{vggsound} are audio-video datasets across various application scenarios. \textbf{UCF-101} \cite{ucf101} is a dataset with two modalities, RGB and optical flow. \textbf{FOOD-101} \cite{food101} is an image-text dataset. And \textbf{CMU-MOSEI} \cite{mosei} is a trimodal dataset (audio, video, text).

\subsection{Benchmark} \label{sec:benchmarks}
BalanceBenchmark is the first comprehensive framework designed to evaluate multimodal imbalance algorithms. It addresses three critical limitations of existing measurement approaches. Firstly, to tackle the absence of standardized metrics for imbalance analysis,  we introduce a systematic evaluation protocol in Section \ref{sec:tool4}, which measures three key dimensions in multimodal learning: performance, imbalance, and complexity. Secondly, to ensure reproducibility and fair comparison of multiple methods, we maintain consistent experimental settings through a modular toolkit with unified data loaders and backbone support. Thirdly, to prevent overfitting to specific scenarios, we incorporate 7 diverse datasets spanning different modality combinations such as audio-video, image-text, RGB-optical flow and trimodal scenarios, with varying degrees of modality imbalance.

\textbf{Implementation details.} 
To ensure a reliable comparison across methods, consistent experimental settings are maintained for each dataset. Most datasets utilize the SGD optimizer with momentum set to 0.9 and weight decay of 1e-4, while VGGSound employs an AdamW optimizer with weight decay of 1e-3. All datasets use the StepLR scheduler with a decay rate of 0.1, where the step size is 30 for most datasets and 10 for VGGSound. The batch size is fixed at 64 for most datasets, except for VGGSound which uses 32. Models on VGGSound are trained for 30 epochs, while models on other datasets are trained for 70 epochs. Learning rates are tailored to each dataset to accommodate varying training dynamics: CREMA-D, FOOD-101, KineticsSounds and VGGSound use 1e-3, BalancedAV uses 5e-3, UCF-101 and CMU-MOSEI use 1e-2. Regarding network architectures, ResNet18 is employed as the backbone for audio-video datasets (i.e., CREMA-D, KineticsSounds, BalancedAV, and VGGSound). FOOD-101 combines a pre-trained Transformer with ResNet18. UCF-101 uses ResNet18, and CMU-MOSEI applies a Transformer architecture across all three modalities. The experiments are conducted on different GPU platforms, ensuring consistency within each dataset: CREMA-D, BalancedAV, CMU-MOSEI and VGGSound are evaluated on NVIDIA GeForce RTX 3090, where VGGSound specifically uses two GPUs. KineticsSounds, FOOD-101, and UCF-101 experiments are performed on an NVIDIA A40.