\newpage
\onecolumn
\section{Appendix}
\label{sec:appendix}

% \subsection{Datasets}


% \begin{table}[ht]
% \centering
% \begin{tabular}{|p{5cm}|p{5cm}|}  % 设置列宽，p{width}表示自动换行
% \hline
% \textbf{title1} & \textbf{title2} \\
% \hline
% Bark supports various languages out-of-the-box and automatically determines language from input text. When prompted with code-switched text, Bark will even attempt to employ the native accent for the respective languages in the same voice. \\
% \hline
% Bark supports various languages out-of-the-box.  & When prompted with code-switched text, Bark will even attempt to employ the native accent for the respective languages in the same voice. \\
% \hline
% \end{tabular}
% \caption{examples}
% \end{table}




% \twocolumn
\subsection{Task Definition}\label{appendix:task_definition}
% \ziya{jiahao update definition. add underline at each task name}
\begin{center}
\textbf{Speech Domain}
\end{center}

Here, we provide a detailed list of each minor task definition for the speech, music, and audio domains, respectively.

\textbf{Speech Recognition (3 minor tasks)}

1. \underline{Automatic Speech Recognition}: transcribing speech into text.

2. \underline{Dialect Automatic Speech Recognition}: \underline{Automatic Speech Recognition} adapted for dialectal variations.

% 3. \underline{Speech Recognition With Word-level Timestamps}: transcribing spoken language into written text while simultaneously providing precise word-level timestamps. 

3. \underline{Phonetic Recognition}: identifying and classifying the smallest units of sound in spoken language, known as phonemes. 


% 6. \underline{Speech Entity Recognition}: identifying and classifying named entities (such as people, organizations, locations, dates, and other specific information) within spoken language.

% 7. \underline{Keyword Spotting}: detecting specific words or phrases in speech.

% 8. \underline{Query by Example Spoken Term Detection}: identifying instances of a spoken query (example term) within a large database of speech recordings. The query is provided as an audio example and the system searches for all segments in the database that are acoustically similar to the query.


\textbf{Spoken Language Understanding (2 minor tasks)}

1. \underline{Intent Classification}: determining the purpose behind a user's spoken input.

2. \underline{Speech to Text Translation}: translating spoken language into written text in a different language.


\textbf{Paralinguistic Attribute Recognition (7 minor tasks)}

1. \underline{Gender Recognition}: classifying the biological gender of a speaker based on acoustic features of their voice. This task leverages acoustic features of speech, such as pitch, formant frequencies, and speech patterns, which tend to differ between male and female speakers due to physiological differences in the vocal tract and larynx.

2. \underline{Age Prediction}: estimating the age of a speaker based on the acoustic properties of their voice. This task utilizes various speech features, such as pitch, speaking rate, formant frequencies, and spectral characteristics, which can provide cues about the speaker's age. 

3. \underline{Emotion Recognition}: identifying and classifying the emotional state of a speaker based on their vocal expressions. 

4. \underline{Accent Recognition}: identifying the regional or cultural accent of a speaker based on their speech characteristics.

5. \underline{Spoken Paragraph Recognition}: determining whether two audio recordings contain the same spoken paragraph by analyzing the linguistic content.

6. \underline{Language Identification}: determining the language spoken from a given audio sample. 

7. \underline{Dialect Identification}: determining the specific dialect or regional variation of a language spoken in a given audio sample.


\textbf{Speaker Recognition (4 minor tasks)}

1. \underline{Speaker Verification}: verifying a speaker's identity by comparing their voice to a pre-recorded voiceprint (voice model) of the claimed identity. This process is used to authenticate or verify a speaker's identity, ensuring that the person speaking is who they claim to be. It includes text-independent and text-dependent speaker verification.

2. \underline{Speaker Diarization}: identifying "who spoke when" in an audio recording containing multiple speakers. This task segments an audio stream into homogeneous regions according to the speaker identity, effectively attributing each segment of speech to its corresponding speaker. 

3. \underline{Speaker Extraction}: extracting the speech of a target speaker from a mixture of sounds that may include multiple speakers and background noise.

4. \underline{Speaker Identification}: identifying a speaker from a set of known speakers based on their voice characteristics.

\textbf{Speech Caption (1 minor task)} 

1. \underline{Speech Caption}: generating synchronized text captions from spoken language.

\textbf{Speech Detection (3 minor tasks)}

1. \underline{Deepfake Detection}: detecting whether an audio clip has been artificially manipulated or synthesized using AI techniques, such as voice cloning or deepfake speech generation.

2. \underline{Vocoder Type Classification}: identifying and categorizing the type of vocoder used in a given speech signal.

3. \underline{Vocoder Type Classification}: identifying the device used to record a given speech segment based on its acoustic features.


\textbf{Speech Enhancement (5 minor tasks)}	

1. \underline{Denoising}: removing unwanted noise from an audio signal to enhance the clarity and quality of the speech. This task involves distinguishing between the speech signal and the background noise, which can include sounds like traffic, machinery, conversations, or other environmental noises. 

2. \underline{Dereverberation}: reducing or eliminating the effects of reverberation from an audio signal. Reverberation occurs when sound waves reflect off surfaces such as walls, ceilings, and floors, causing the original speech signal to be combined with multiple delayed copies of itself. 

3. \underline{Declipping}: restoring audio signals that have been distorted due to clipping. Clipping occurs when the amplitude of an audio signal exceeds the maximum limit that a recording or playback system can handle, causing the peaks of the waveform to be "clipped" off.

4. \underline{Speech Bandwidth Extension}: enhancing narrowband speech quality by extending its frequency range. Narrowband speech often lacks the higher frequencies that contribute to the naturalness and clarity of speech.

5. \underline{Signal-to-noise Ratio Estimation}: quantifying the ratio of the power of a signal to the power of background noise. This task provides a quantitative measure of the quality of a signal.


\textbf{Speech Generation (9 minor tasks)}

1. \underline{Text to Speech}: converting written text into spoken words. It involves synthesizing speech that is natural and understandable, enabling computers to "read" text aloud.

2. \underline{Zero-shot Text to Speech/Voice Cloning}: generating synthetic speech for voices or styles it has never encountered during training. 

3. \underline{Emotional Text to Speech}: synthesizing speech with emotional nuances. The goal is to produce speech that not only conveys the content of the text but also expresses specific emotions, making the synthetic voice more engaging and human-like.
		
4. \underline{Zero-shot Emotional Text to Speech}: generating emotional speech that adapts to an unseen speaker’s voice while rendering specified emotions. 

5. \underline{Descriptive Speech Synthesis}: generating synthetic speech that not only replicates the spoken content but also conveys descriptive information about the context of the speech, such as emotions, tone, or other paralinguistic features.

6. \underline{Spontaneous Text to Speech}: generating synthetic speech that mimics the characteristics of spontaneous unscripted human speech. Spontaneous TTS aims to replicate the naturalness, variability, and informal aspects of everyday conversational speech. This includes features such as hesitations, fillers (e.g., "um," "uh"), varying speech rates, and natural prosody changes.

7. \underline{Voice Conversion}: converting one speaker's voice to resemble another's while preserving linguistic content and prosody.

8. \underline{Emotion Conversion}: transforming the emotional tone of a spoken utterance from one emotion to another while preserving the linguistic content. 

9. \underline{Speech to Speech Translation}: converting spoken language in one language directly into spoken language in another language.

\begin{center}
\textbf{Music Domain}
\end{center}

\textbf{Global MIR (10 minor tasks)}: 

1. \underline{Key Detection}: recognizing the key signature of the given music.

2. \underline{Scale Recognition}: recognizing the scale of the given music.

3. \underline{Music Tagging}: assigning descriptive tags to audio files, such as genre, style, tempo, key, artist, and emotion.

4. \underline{Genre Classification}: categorizing the music into certain genres.

5. \underline{Emotion Classification}: recognizing emotion categories from the music.

6. \underline{Pitch Classification}: classifying the pitch of the given audio.

7. \underline{Instrument Classification}: identifying all existing instruments from the music.

8. \underline{Vocal Technique Classification}: detecting the playing techniques used in the vocal music.

9. \underline{Instrumental Technique Classification}: detecting the playing techniques used in the instrumental music.

10 \underline{Artist Identification}: identifying the relevant artists of a piece of music, given a set of artists as the options.

\textbf{Sequential MIR (3 minor tasks)}

1. \underline{Beat Tracking}: detecting and aligning beats of a music excerpt.

2. \underline{Chord Estimation}: estimating the chords sequence at each time step of a music excerpt.

3. \underline{Progression Extraction}: extracting the chord progression represented by chord number sequence. 

\textbf{Single Music Reasoning (2 minor tasks)}	

1. \underline{Beat-level Instruments Recognition}: recognizing the instruments from a certain beat or a certain segment.

% 2. \underline{Sequential Instruments Reasoning}: specifying the order of instruments within different segments of a music.

2. \underline{Beat-level Pitch Estimation}: estimating the pitch of a certain beat or segment.


\textbf{Multiple Music Reasoning (5 minor tasks)}

1. \underline{Tempo Comparison}: comparing the tempo characteristics between two music excerpts.

2. \underline{Instruments Comparison}: comparing instruments of two music excerpts.

3. \underline{Key Comparison}: comparing keys of two music excerpts.

4. \underline{Instrumental Technique Comparison}: comparing playing techniques of two music excerpts.

5. \underline{Emotion Comparison}: comparing emotions of two excerpts.

\textbf{Music Caption (1 minor task)}

1. \underline{Music Caption}: generating textual descriptions for a piece of music.


\textbf{Music Separation (2 minor tasks)}

1. \underline{Melody Extraction}: extracting the melody at each time step from a music excerpt.

2. \underline{Text-guided Source Separation}: separate certain tracks from a piece of mixed music with the text instruction.

\textbf{Music Generation (5 minor tasks)}

% 1. \underline{Symbolic Music Synthesis}: synthesizing the audio of the symbolic music sequence.

1. \underline{Text-to-Music Generation}: generating the music given the text caption.
 
% 3. \underline{Style-conditioned Generation}: generating music in a specified style based on a textual description.

% 4. \underline{Motif-conditioned Generation}: generating music based on a given short melodic motif while maintaining musical coherence.

% 5. \underline{Chord-conditioned Generation}: generating music based on a given chord progression while ensuring harmonic consistency.

2. \underline{Text-guided Music Continuation}: extending a given initial audio segment based on a textual description of musical characteristics while ensuring continuity and coherence.
		
3. \underline{Lyrics-to-song Generation}: composing a song with the vocal track and instrumental track based on the given lyrics.

4. \underline{Singing Voice Synthesis}: synthesizing the voice given the pitches and lyrics sequence.

5. \underline{Singing Voice Conversion}: transforming the vocals (including the lyrics and melody) of singer A(source vocals) to sound like Singer B (target singer).

\vspace{10pt}
\begin{center}
\textbf{Audio Domain}
\end{center}

\textbf{Audio Event Recognition (4 minor tasks)}

1. \underline{Sound Event Sequence Recognition}: identifying and sequencing various sounds in an audio stream.

2. \underline{Sound Event Recognition}: detecting and identifying a particular sound in audio data.

3. \underline{Sound Event Detection}: determining when a specific sound occurs within an audio clip.

4. \underline{Acoustic Scene Classification}: classifying an audio clip according to the environment it represents (e.g., park, street).

% \textbf{Audio Retrieval (2 minor tasks)}

% 1. \underline{Audio-to-audio Retrieval}: retrieving acoustically similar audio clips based on a given audio query.

% 2. \underline{Text-to-audio Retrieval}: retrieving relevant audio clips based on a textual query describing the sound content.

\textbf{Audio Caption (1 minor task)}

1. \underline{Audio Caption}: generating natural language descriptions that summarize or explain the content of an audio clip.

% \textbf{Audio Matching (1 minor task)}

% 1. \underline{Audio-guided Visual Scene Captioning}: generating captions for a visual scene based on the accompanying audio.

\textbf{Audio Advanced Understanding (1 minor task)}

% 1. Audio Effector: applying effects to audio to alter its characteristics (e.g., echo, reverb).

1. \underline{Sound Event Understanding}: extracting meaningful information from multiple audio signals (e.g. What is happening in the given audio). 

% 4. \underline{Visual Querying Sound Extraction}:



\textbf{Audio Detection (2 minor tasks)}

1. \underline{Deepfake Audio Detection}: identifying synthetic or manipulated audio content.

% 2. \underline{Audio Grounding/text-to-audio Grounding (TAG)}: aligning or associating audio segments with corresponding text.

2. \underline{Voice Activity Detection}: identifying segments where human speech is present in the given audio.

\textbf{Audio Classification (2 minor tasks)}

1. \underline{Speech, Silence, Music and Noise Classification}: distinguishing between music, speech, and various types of noise.

2. \underline{Speech and Non-speech Detection}: identifying segments which contain speech or non-speech of the given audio.


\textbf{Audio Enhancement (2 minor tasks)}	

1. \underline{Audio Inpainting}: filling in missing parts of an audio signal.

2. \underline{Audio Super-resolution}: improving the perceptual quality of an audio signal by increasing its resolution.


\textbf{Audio Separation (3 minor tasks)}

1. \underline{Text-guided Audio Source Separation}: isolating specific sound sources from an audio clip based on text input.

2. \underline{Label-querying Sound Extraction}: extracting sounds belonging to a predefined category from an audio mixture, given a textual label 

3. \underline{Audio-querying Sound Extraction}: isolating sound sources from an audio mixture based on an example audio query.


\textbf{Audio Generation (3 minor tasks)}	

1. \underline{Text-guided Audio Generation}: creating audio based on a textual description.

2. \underline{Time-grounded Text-to-audio Generation}: generating audio content that aligns with time-specific textual descriptions.

3. \underline{Audio Continuation}: extending an audio clip by generating additional content that seamlessly continues the original.


\subsection{Datasets for Each Task}
Here, we present the datasets associated with each minor task.
% \input{tables/qwen2-tuning-results-appendix}
% \input{latex/tables/qwen2-tuning-results-all}

\input{tables/task_and_dataset}


% \input{latex/tables/dataset_and_task}
\subsection{Instruction Template}
\label{appendix:instruction_template}
Here we provide the complete task instruction template in JSONL format with all fields. 



% \begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation]
% \{
% "instruction": "Please translate the speech into the text in English.", 
% "input": "<|SOA|>Speech\_Audio<|EOA|>", 
% "output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing."
% \}
% \end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{"instruction": "Please translate the speech into the text in English.", 
  "input": "<|SOA|>Speech_Audio<|EOA|>", 
  "output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", 
  "uuid": "UUID", 
  "split": ["train"], 
  "task_type": {
    "major": ["Spoken Language Understanding"], 
    "minor": ["Speech-to-text Translation"], 
    "U/G": ["understanding"], 
    "unseen": false
  }, 
  "domain": "speech", 
  "source": ["unknown"]
"other": null}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Text-guided Music Continuation]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{"instruction": "Please continue the audio music prompt based on the given text description", 
"input": "This is a Carnatic music piece set in the atana raga. It follows the 5/8 meter and is composed in the khandaChapu taala. The lead instrument featured in this performance is vocal, accompanied by Mridangam. The kalai of this composition is 1.\n audio prompt: <|SOA|>Music_Audio<|EOA|>", 
"output": "audio: <|SOA|>Musi_Audio<|EOA|>", 
"uuid": "UUID", 
"split": ["test"], 
"task_type": {
    "major": ["Music Generation"], 
    "minor": ["Text-guided Music Continuation"], 
    "U/G": ["generation"], 
    "unseen": false
    }, 
"domain": "music", 
"source": ["unknown"], 
"other": null}
\end{lstlisting}
\end{tcolorbox}




\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Sound Super-resolution]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{"instruction": "Please increase the resolution of the given audio signal to 32k Hz.", 
"input": "audio: <|SOA|>Sound_Audio<|EOA|>.", 
"output": "<|SOA|>Sound_Audio<|EOA|>", 
"uuid": "UUID", 
"split": ["train"], 
"task_type": {
    "major": ["Sound Generation"], 
    "minor": ["Sound Super-resolution"],
    "U/G": ["generation"], 
    "unseen": false
    }, 
"domain": "audio", 
"source": ["youtube"], 
"other": null}
\end{lstlisting}
\end{tcolorbox}

The definitions of each field are described as follows:

\textbf{Instruction}: this field provides the instructions for the task, outlining the specific operation to be performed.

\textbf{Input}: this field contains the input data for the task, which represents the raw information to be processed.

\textbf{Output}: this field represents the expected result or outcome after processing the input data.

\textbf{Uuid}: this field assigns a unique identifier to each task instance, enabling the system to track and manage individual tasks.

\textbf{Split}: this field specifies the dataset partition for the task, such as "train", "test", or "dev", which correspond to the training, testing, and development datasets, respectively.

\textbf{Task\_type}: this field outlines the nature of the task:

- \textbf{Major}: indicates the primary category of the task.

- \textbf{Minor}: specifies the secondary or more specific task.

- \textbf{U/G}: distinguishes whether the task focuses on generation or understanding.

- \textbf{Unseen}: a boolean value that indicates whether the task involves data that has not been encountered before.

\textbf{Domain}: this field defines the domain in which the task is situated, such as "speech", "music", or "audio".

\textbf{Source}: this field identifies the origin of the audio, such as "audiobook", "youtube", or "studio", signifying where the audio signal is sourced from.

\textbf{Other}: this field can store any additional metadata relevant to the task, if applicable.

\subsection{Instruction Variation Prompt}
\label{appendix:instruction_variation_prompt}
As mentioned in Section~\ref{sec:self_instruct}, all instances are automatically varied by entering a standard prompt in the existing LLMs, which is presented as follows. 

% \scalebox{0.8}{ % 缩放到原来的80%
% \begin{tcolorbox}[
%     colback=lightgray!20,
%     colframe=black,
%     boxrule=0.3mm,
%     arc=0mm,
%     % width=\dimexpr\textwidth+3\fboxsep\relax, % 扩展到页面边界  \textwidth %\
%     width=\paperwidth, % 设置宽度为整个页面的宽度
%     left=0mm, 
%     right=0mm,
% ]


\begin{tcolorbox}[
    colframe=black,  % 黑色边框
    colback=white,   % 背景颜色为白色
    boxrule=0.5mm,    % 边框粗细
    arc=0mm,          % 边框角度
    sharp corners,     % 直角边框
    breakable
]

You are tasked with paraphrasing the values of the following fields: "instruction", "input", and "output". Your goal is to generate varied and creative rewrites for each of these fields. Please adhere to the following guidelines:

\begin{enumerate}
    \item \textbf{Paraphrase Instructions}:
    \begin{itemize}
        \item Paraphrase the "instruction" field in diverse ways by changing the sentence structure, style, and tone. Use a variety of sentence types, including:
        \begin{itemize}
            \item Direct commands (e.g., "Turn this into speech.")
            \item Polite requests (e.g., "Could you please convert this to speech?")
            \item Questions (e.g., "Can you turn this into audio?")
            \item Suggestions (e.g., "It would be great if you could convert this.")
            \item Exclamations or emphatic forms (e.g., "I really need this to be in audio form.")
        \end{itemize}
        \item Feel free to add polite elements, such as "please," "kindly," or "if you would be so kind," as long as they remain natural.
    \end{itemize}

    \item \textbf{Paraphrase Inputs}:
   
    \begin{itemize}
        \item  Change the labels for fields like "text:"`, "text\_description:"`, "audio:"`, "speaker\_audio:"`, "audio\_sample1"`, "audio\_sample2"` etc., according to "instruction", while retaining their original meaning. Examples include:
        \begin{itemize}
            \item "text:" to "spoken text," "speech input," "text excerpt," etc.
            \item "text\_description:" to "voice style," "descriptive text," "tone characteristics," etc.
            \item "audio:" to "source audio," "reference speech," "given recording," etc.
            \item "speaker\_audio:" to "speaker prompt," "reference voice," "voice sample," etc.
            \end{itemize}
        \item Ensure that the content following "text:" remains semantically identical to the original. The content following each label should remain unchanged, with only the labels varying.
    \end{itemize}

    \item \textbf{Maintain Consistency in Outputs}:
    \begin{itemize}
        \item Depending on the tone of the instruction, introduce additional phrases such as:
        \begin{itemize}
            \item "The gender is ", "Gender: ".
            \item "The language is ", "Language in the given speech is ".
            \item "The speakers in the given two speechs are ", "The anwser is ".
            \item "Transcription is: ", "The text of the given speech is: ".
            \item "IPA Phonemes is: ", "phonemes of the given speech is: ".
            \item "Descriptive text of the given speech is: ", "The speaking style is: ", "Speech caption is: ".
        \end{itemize}
        \item Ensure the "output" field contains the substring \texttt{|SOA|>audio<|EOA|} and the content that follows it, preserving both the structure and meaning.         \item You may optionally introduce phrases before \texttt{|SOA|>audio<|EOA|} (e.g., "Generated speech is:", "Audio output:", "The resulting audio is:"). Avoid altering or introducing inconsistencies in the audio filename (e.g., \texttt{|SOA|>13\_LibriTTS-R\_260\_123288\_000009\_000000<|EOA|}).
    \end{itemize}

    \item \textbf{Ensure JSON Validity}:
    \begin{itemize}
        \item All strings must be enclosed in double quotes.
        \item Key-value pairs must be separated by commas.
        \item The JSON structure for "instruction", "input", and "output" fields must be valid and consistent.
    \end{itemize}

    \item \textbf{The number of objects in the output should match the number of inputs.} Ensure the format is valid JSON and all JSON objects are properly enclosed. Each entry should be separated by a newline and all JSON should be enclosed in an array.
\end{enumerate}

Please provide the output in valid JSON format (an array of JSON objects), and ensure proper formatting.
\end{tcolorbox}
% }


\subsection{Instruction Variation}\label{appendix:instruction_variation_example}

After the instances (shown in Appendix~\ref{appendix:instruction_template}) are diversified by the prompt in Appendix~\ref{appendix:instruction_variation_prompt}, we obtain instances with various instructions, which are shown below.
% /aifs4su/mmcode/lmxue/dataset/audio_flan/[audio/music/speech]_update_jsonl/[audio/music/speech]_variation

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation Task Template]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{
"instruction": "Please translate the speech into the text in English.", 
"input": "<|SOA|>Speech_Audio<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing."
}
\end{lstlisting}
\end{tcolorbox}


\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation Insutrction Variation Example 1]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{
"instruction": "Would you be able to convert the spoken words into English text?", 
"input": "<|SOA|>Speech_Audio<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", 
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation Insutrction Variation Example 2]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{
"instruction": "How about translating the speech into Mandarin English text?", 
"input": "<|SOA|>Speech_Audio<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", 
}
\end{lstlisting}
\end{tcolorbox}


\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation Insutrction Variation Example 3]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{
"instruction": Please provide the English translation of the audio speech.", 
"input": "<|SOA|>Speech_Audio<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", 
}
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation Insutrction Variation Example 4]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{
"instruction": Could you kindly translate the given speech into written English?", 
"input": "<|SOA|>Speech_Audio<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", 
}
\end{lstlisting}
\end{tcolorbox}



\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation Insutrction Variation Example 5]
\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
{
"instruction": Please provide the English translation of the audio speech.", 
"input": "<|SOA|>Speech_Audio<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", 
}
\end{lstlisting}
\end{tcolorbox}


