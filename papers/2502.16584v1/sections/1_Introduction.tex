\section{Introduction} 

% ruibin

% Recent advances in large language models and multimodal models have highlighted the power of \emph{instruction tuning} for broad generalization. In the text domain, models like FLAN (Finetuned Language Models) and the GPT-series demonstrate remarkable zero-shot and few-shot capabilities when trained on diverse instructions~\citep{flan_ref,gpt_series_ref}. Instruction-tuned language models can generalize to unseen tasks far better than their task-specific counterparts. For example, FLAN (a 137B-parameter model) was fine-tuned on 60 NLP tasks with natural language prompts and outperformed even larger models (175B GPT-3) on many unseen tasks. Similarly, LIMA, which used only 1,000 carefully curated instruction examples to fine-tune a pretrained LLM, achieved responses preferred over those from much larger models, showing that even minimal high-quality instruction data can align models to follow complex queries \citep{lima_ref}. In the vision domain, we now see unified models that handle both understanding and generation of images in a single system. Meta’s Chameleon is one such foundation model capable of interpreting and producing interleaved images and text, achieving state-of-the-art results in image captioning and strong performance on visual question answering and image generation within one unified model \citep{chameleon_ref}. Likewise, Janus-Pro 7B—an open-source 7B multimodal model—unifies visual understanding and text-to-image generation, surpassing previous specialized models on both kinds of tasks \citep{janus_ref}. In contrast, the audio domain lags behind: audio understanding and audio generation are still largely treated as separate problems, and no current model exhibits the broad zero-shot generalization that text and vision models now enjoy.

% This gap between modalities underscores a clear limitation: \textbf{audio-language models lack the unified modeling and generalization capabilities} now common in NLP and computer vision. Despite the rich variety of audio tasks (speech transcription, speaker identification, emotion recognition, sound event detection, music understanding, text-to-speech generation, etc.), there is no “audio GPT” or “audio foundation model” that can seamlessly switch between understanding and generating audio content. Current state-of-the-art audio models struggle to generalize beyond their training task – for instance, a speech recognizer cannot answer questions about music, and a music generation model cannot carry a dialogue about environmental sounds. Recent studies have begun to quantify these limitations. Dynamic-SUPERB, a comprehensive benchmark for instruction-following speech models, highlights that unlike text models with strong zero-shot abilities, speech models remain siloed on narrow tasks \citep{dynamic_superb_ref}. It combines 33 speech tasks to evaluate “universal” speech models and finds that baseline systems perform reasonably on tasks seen during training but struggle with unseen tasks, revealing poor zero-shot generalization in the audio domain. Likewise, the new MMAU benchmark—covering speech, environmental sounds, and music in 10k audio clips with Q\&A—demonstrates that even the best current audio-language models (e.g., Gemini-Pro v1.5 or Qwen2-Audio) only achieve about 52\% accuracy, leaving nearly half the questions answered incorrectly \citep{mmau_ref}. This starkly contrasts the robustness of text models and indicates that audio models have a long way to go in achieving general auditory intelligence. The community has also identified the lack of evaluation frameworks—AIR-Bench was recently proposed as the first benchmark for generative audio-language comprehension, since earlier evaluations focused mostly on specific tasks like ASR \citep{air_bench_ref}. AIR-Bench evaluates large audio-language models on both foundational audio tasks and open-ended audio “chat” interactions, revealing significant limitations in current models’ ability to follow instructions about audio. In summary, audio-language research is now at a stage akin to early NLP before GPT-3/FLAN: we have many task-specific models and some benchmarking efforts, but we lack a truly unified model with broad, zero-shot capabilities.

% A core bottleneck impeding progress is the scarcity of large-scale instruction-tuning data tailored to audio-language tasks. In NLP, researchers have assembled massive multi-task instruction datasets (e.g., Super-NaturalInstructions with 1,616 diverse tasks and expert-written instructions \citep{super_natural_instructions_ref}) to train generalist models. Vision-language models have also benefited from combined instruction datasets (for example, LLaVA and InstructBLIP leverage millions of image-text instruction pairs for open-ended tasks \citep{llava_ref}\citep{instructblip_ref}). By contrast, the audio field lacks any comparable resource in scale or diversity. Most available audio datasets are narrow in scope---for instance, LibriSpeech for ASR, EmoV-DB for emotion recognition, AudioSet for tagging, and VCTK for TTS---each addressing a specific problem with its own format. There have been some initial attempts to create instruction-style audio datasets, but these are limited. For example, GAMA (a recent large audio-language model) had to synthesize an instruction dataset called CompA-R to teach the model complex audio reasoning \citep{gama_ref}; however, CompA-R is largely artificial and focuses mainly on audio question-answering and captioning tasks. Other works have used GPT-4 or LLMs to generate questions or captions from audio metadata (e.g., the LTU and DeSTA pipelines for speech) \citep{ltu_ref}\citep{desta_ref}, essentially bootstrapping instruction data from existing speech corpora. These efforts, while useful, produce fragmented and lower-scale training sets---often only covering speech and often biased by the prompt used for generation. No current dataset approaches the breadth of real audio content (spanning speech, music, and environmental sounds) with instructions. In short, the audio domain lacks a “FLAN” equivalent—a consolidated, high-quality instruction dataset to unify myriad audio tasks. This absence of data is a key reason we do not yet have audio models with the generalization of GPT-4 or Chameleon. Even as benchmarks like Dynamic-SUPERB and AIR-Bench call for instruction-following audio models, researchers struggle to train such models without a large, diverse training corpus tailored to audio-language understanding and generation.

% In this work, we introduce \textbf{Audio-FLAN}, a preliminary attempt to bridge this data gap and enable truly unified audio-language modeling. Audio-FLAN is a large-scale, diverse instruction-tuning dataset for audio, constructed by collecting and standardizing nearly all publicly available academic audio datasets into a common instruction-based format. Instead of creating synthetic data from scratch, we leverage the wealth of real audio data across domains—from speech corpora and conversational audio to music recordings and sound effect libraries—and curate them into a unified dataset with natural language instructions and responses. The Audio-FLAN collection spans a broad range of audio tasks: it includes speech tasks (speech recognition, translation, speaker identification, emotion detection, spoken question answering, etc.), music tasks (music transcription, genre classification, music captioning), and general sound understanding tasks (sound event recognition, audio captioning of everyday sounds, acoustic question answering). By normalizing the format of these heterogeneous datasets, we provide each audio sample with an accompanying instruction (or question/prompt) and the expected output (transcription, description, answer, or even an audio clip for generative tasks). This results in a singular, massive instruction dataset for audio, with high-quality human-curated examples covering everything from a human conversation to a bird call to a piano melody. We also ensure the data is diverse (drawn from many domains to avoid narrow bias) and high-fidelity (favoring datasets with verified annotations and clean audio). Crucially, Audio-FLAN is designed to support both pre-training and supervised fine-tuning (SFT) of models for unified audio-language tasks. A model can be first pre-trained on Audio-FLAN in a multi-task fashion to learn general audio-text representations and then fine-tuned on the same data (or a subset) with instruction-following training to achieve chat-like interaction. We envision that models trained on Audio-FLAN will be capable of both audio understanding (e.g., transcribing and comprehending audio, answering questions about it) and audio generation (e.g., following instructions to produce speech or sounds) within one unified framework. In other words, Audio-FLAN lays the groundwork for an audio equivalent of multimodal foundation models—an audio-language model that can listen, understand, and speak in a general way.

% By releasing Audio-FLAN (Preliminary Version), we aim to catalyze research on unified audio-language models. In this introduction, we have highlighted how current audio models remain task-specific and how the lack of unified instruction data is a major impediment. \textbf{Audio-FLAN directly addresses this issue}: it is, to our knowledge, the first compilation that combines hundreds of audio datasets into a single, instruction-driven corpus of this scale. Table~\ref{tab:dataset_comparison} provides a high-level comparison of Audio-FLAN with existing instruction-tuning resources. Notably, while prior audio instruction sets contained at most a few dozen tasks or relied heavily on synthetic augmentation \citep{gama_ref}\citep{ltu_ref}, Audio-FLAN encompasses on the order of $10^2$ real tasks and $10^5+$ instances, far surpassing previous efforts in both quantity and diversity. We hope that Audio-FLAN will do for audio what FLAN and other instruction-tuned models did for text—enable models that generalize across audio tasks in a zero-shot manner and can follow open-ended instructions about sound. Audio-FLAN’s preliminary release is just the beginning: we invite the community to build upon this resource, contribute new tasks (as done in Dynamic-SUPERB \citep{dynamic_superb_ref}), and explore unified modeling of speech, music, and environmental audio. By unifying audio understanding and generation, we take a step toward foundation models that can hear and produce sound as generally and flexibly as language models process text.



%%%%%%%%% version two
% instruction-tuning -> unified language model with understanding and generation
Recent advances in large language models and multimodal models have highlighted the effectiveness of \emph{instruction tuning} for broad generalization~\citep{modelsfollow,touvron2023llama,achiam2023gpt}. Instruction-tuned models can generalize to unseen tasks far better than task-specific counterparts. In the text domain, models like FLAN (Finetuned Language Net)~\citep{wei2021flan} demonstrate remarkable zero-shot and few-shot capabilities when fine-tuned on diverse instructions. For example, FLAN (137B parameters) was fine-tuned on 60 NLP tasks and outperformed even larger models, like the 175B GPT-3~\citep{brown2020gpt3}, on many unseen tasks. Similarly, LIMA~\citep{zhou2024lima}, which used only 1,000 curated examples, achieved results preferred over much larger models, showing that minimal high-quality instruction data can significantly improve a model's ability to follow complex queries. In the vision domain, unified models like Chameleon~\citep{team2024chameleon} and Janus-Pro 7B~\citep{Janus} have demonstrated strong performance by handling both understanding and generation tasks in a single system, outperforming specialized models in image captioning, visual question answering, and image generation. In contrast, the audio domain~\footnote{In this paper, `audio' refers to two distinct meanings: (a) in a narrower sense, `audio' refers to `sound', which is related to but different from speech and music, often used in the context of `speech, music, and audio'; (b) in a broader sense, `audio' encompasses speech, music, and sound, used in the context of `text, vision, and audio'.} still lags behind, with audio understanding and generation often treated as separate tasks.

This gap between modalities highlights a critical limitation: \textbf{audio-language models still lack the unified modeling and generalization capabilities} that are now common in NLP and computer vision. Despite the wide variety of audio tasks (such as speech transcription, speaker identification, emotion recognition, sound event recognition, music understanding, and text-to-speech generation), there is no "audio GPT" or "audio foundation model" that can seamlessly switch between understanding and generating audio across speech, music, and audio domains. For example, models like Musilingo~\citep{deng2023musilingo} focus on music understanding, while LTU (Listen, Think, Understand)~\citep{gong2023listen} and Audio-Flamingo~\citep{kong2024audioflamingonovelaudio} focus on the audio domain. The SALMONN~\citep{tang2023salmonn} and Qwen-Audio series~\citep{chu2023qwen} are designed for understanding speech, sound, and music, but lack generation capabilities. On the other hand, UniAudio~\citep{yang2023uniaudio} supports audio generation, but it is limited to 11 tasks spanning speech, sound, music, and singing, each with specific task identifiers.

Currently, no audio model exhibits the broad zero-shot generalization seen in text and vision models. Recent benchmarks highlight these limitations. Dynamic-SUPERB~\citep{huang2024dynamicsuperbdynamiccollaborativecomprehensive}, a comprehensive benchmark with 33 speech tasks for speech models, shows that unlike text models, speech models remain confined to narrow tasks. It finds that systems perform well on seen tasks but struggle with unseen tasks, revealing poor zero-shot generalization. Dynamic-SUPERB Phase-2~\citep{dynamicsuperb2}, which has expanded to include 180 understanding tasks, reports that while recent models perform well on specific tasks, they struggle with generalization, underscoring the need for more research on developing universal models. Similarly, the MMAU benchmark~\citep{mmau}, which covers speech, environmental sounds, and music, shows that even top models like Gemini-Pro v1.5~\citep{team2024gemini} and Qwen2-Audio~\citep{chu2024qwen2} only achieve about 52.97\% accuracy. This stark contrast with text models underscores the underexplored potential of audio-language models for general auditory intelligence. Additionally, the lack of comprehensive evaluation frameworks further hinders progress. AIR-Bench~\citep{AIR_Bench}, the first generative audio-language comprehension benchmark, reveals significant limitations in current models' ability to follow instructions across tasks. In summary, audio-language research is still in an early stage, similar to the pre-GPT-3/FLAN era of NLP: while there are task-specific models, there is no unified model with broad, zero-shot capabilities.

A key challenge in the audio domain is \textbf{the lack of large-scale, diverse instruction-tuning datasets tailored to audio-language tasks}. While NLP has benefited from extensive multi-task instruction datasets like Super-NaturalInstructions~\citep{Super_naturalinstructions} with 1,616 tasks and vision-language models use resources like LLaVA~\citep{LLaVA} and InstructBLIP~\citep{instructblip}, the audio field lacks comparable datasets in scale or diversity. 
Some efforts, like GAMA~\citep{GAMA} synthesize an instruction dataset, called CompA-R, for audio reasoning, but they focus mainly on narrow tasks like question-answering and captioning. Other works have used GPT-4 or LLMs to generate instruction data from existing speech corpora,  e.g., LTU~\citep{gong2023listen} and DeSTA~\citep{gong2023joint}, but these are fragmented, limited in scope, and often biased by the prompts used. 
No existing dataset spans the breadth of audio content, including speech, music, and sound, with instructions. In short, the audio domain lacks a “FLAN” equivalent—a consolidated, high-quality instruction dataset to unify myriad audio tasks. This absence of data is a key reason we do not yet have audio models with the generalization of GPT-4 or Chameleon. Even as benchmarks like the Dynamic-SUPERB series and AIR-Bench call for instruction-following audio models, researchers struggle to train such models without a large, diverse training corpus tailored to audio-language understanding and generation.


In this work, we introduce \textbf{Audio-FLAN}, a preliminary attempt to bridge this data gap and enable truly unified audio-language modeling. Audio-FLAN (Preliminary Release) is \textbf{a large-scale, diverse instruction-tuning dataset for both understanding and generation tasks across speech, music, and audio}, constructed by collecting and standardizing nearly all publicly available academic audio datasets into a common instruction-based format. By normalizing the format of these heterogeneous datasets, we provide each audio sample with one or more accompanying instructions (or question/prompt) and the expected output (transcription, description, answer for understanding tasks, or an audio clip for generative tasks). Crucially, Audio-FLAN is designed to support both pre-training and supervised fine-tuning (SFT) of models for unified audio-language tasks. We envision that models trained on Audio-FLAN dataset will be capable of both audio understanding (e.g., transcribing and comprehending audio, answering questions about it) and audio generation (e.g., following instructions to produce speech, music and sounds) within one unified framework. In other words, Audio-FLAN lays the groundwork for an audio equivalent of multimodal foundation models—an audio-language model that can listen, understand, speak, sing and compose in a general way.

To our knowledge, \textbf{Audio-FLAN} is the first comprehensive compilation that combines diverse audio datasets into a single, instruction-driven corpus of considerable scale. It includes approximately \textbf{80 tasks} and over \textbf{100 million} instances, significantly surpassing prior efforts in both quantity and diversity. We aim for Audio-FLAN to achieve for audio what FLAN and other instruction-tuned models have accomplished for text—enabling models to generalize across a wide range of audio tasks in a zero-shot manner and follow open-ended instructions related to audio content. The preliminary release of Audio-FLAN is only the beginning: we invite the research community to build on this resource, contribute new tasks (similar to Dynamic-SUPERB Phase-2), and explore unified models for speech, music, and audio. By unifying both audio understanding and generation, Audio-FLAN paves the way toward foundational models that can hear and generate audio as flexibly and broadly as language models process text.

% To our knowledge, \textbf{Audio-FLAN} is the first comprehensive compilation that combines diverse audio datasets into a single, instruction-driven corpus of considerable scale. While previous audio instruction sets contained at most a few dozen tasks or relied heavily on synthetic augmentation, Audio-FLAN includes approximately 80 tasks and over 100 million instances for both understanding and generation across speech, music, and audio, significantly surpassing prior efforts in both quantity and diversity. We aim for Audio-FLAN to achieve for audio what FLAN and other instruction-tuned models have accomplished for text—enabling models to generalize across a wide range of audio tasks in a zero-shot manner and follow open-ended instructions related to audio content. The preliminary release of Audio-FLAN is only the beginning: we invite the research community to build on this resource, contribute new tasks (similar to Dynamic-SUPERB Phase-2), and explore unified models for speech, music, and audio. By unifying both audio understanding and generation, Audio-FLAN paves the way toward foundational models that can hear and generate sound as flexibly and broadly as language models process text.