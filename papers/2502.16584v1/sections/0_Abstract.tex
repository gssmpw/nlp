\begin{abstract}
Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly \textbf{unified} audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce \textbf{Audio-FLAN}, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both \textbf{understanding} (e.g., transcription, comprehension) and \textbf{generation} (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace~\footnote{\url{https://huggingface.co/HKUSTAudio}} and GitHub~\footnote{\url{https://github.com/lmxue/Audio-FLAN}} and will be continuously updated.


% Recent advances in audio tokenization have significantly expanded the integration of audio capabilities into large language models (LLMs). However, these developments have largely treated audio understanding and generation as separate problems, limiting progress toward truly unified audio-language models. While instruction tuning has demonstrated remarkable success in enhancing task generalization and zero-shot learning across text and vision modalities, its application to unifying audio understanding and generation remains underexplored. A key barrier is the lack of comprehensive datasets that facilitate research on joint modeling. To address this gap, we introduce \textit{Audio-FLAN}, a large-scale instruction-tuning dataset covering 80 diverse tasks and more than 100 million instruction-tuning task instances, spanning speech, music, and general sound domains. Audio-FLAN enables the first systematic evaluation of unified audio-language capabilities and establishes a comprehensive benchmark. 
% Our experiments highlight persistent challenges in integrating understanding and generation within a single model while providing a standardized framework for evaluating progress in this direction.

% Recent advances in audio tokenization have enabled significant progress in integrating audio capabilities into large language models (LLMs). These developments have particularly focused on audio understanding and generation tasks, aiming to create more natural and versatile human-computer interactions. While instruction tuning has demonstrated remarkable success in enhancing models' task generalization and zero-shot learning capabilities across both language-only and vision domains (e.g., LLaVA for vision-language tasks), the unification of audio understanding and generation within a single architecture remains largely unexplored. A primary challenge in addressing this gap has been the absence of comprehensive datasets that support researches on unified modeling. In this work, we present \textit{Audio-FLAN}, a large-scale instruction tuning dataset encompassing over 70 diverse tasks and more than 200K hours of audio content, spanning understanding and generation tasks across speech, music, and general sound domains. With Audio-Flan, evaluation of unified audio-language capabilities becomes possible, and  the first comprehensive benchmark is established. Our experimental results reveal that significant challenges on capability unification remain, while providing a standardized way for evaluation through the proposed benchmark.

\end{abstract}