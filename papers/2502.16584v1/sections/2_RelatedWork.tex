\section{\textcolor{red}{Existing Instruction Datasets}}


\textbf{Instruction tuning.}
FLAN~\citep{wei2021flan} proposes that the instruction tuning can facilitate the models' capability in zero-shot learning, which is influenced by the fine-tuning dataset scale, model size and natural language instructions. Since then, many related works have investigated the instruction tuning's real effect. Many mature LLMs are equipped with instruction tuning, such as InstructGPT~\citep{ouyang2022traininglanguagemodelsfollow}, ChatGPT~\citep{achiam2023gpt}, and LLaMA~\citep{touvron2023llama}. Wang et al. ~\citep{wang2023far} points out that a powerful pre-trained language model being exposing to a vast amount of knowledge, and a diverse and representative instruction dataset are two factors by which the instruction tuning can improve the potential downstream usage.  % [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources]


In addition to text-only LLMs, instruction tuning has also been adapted to large multimodal models (LMMs) between vision and language. Vision-language instruction tuning (VLIT) datasets like Insturct-150K~\citep{liu2023visualinstructiontuning, liu2024improvedbaselinesvisualinstruction}, MIMIC-IT~\citep{li2023mimic}, Video-LLaMA~\citep{zhang2023videollama} and Vid-ChatGPT~\citep{maaz2024videochatgptdetailedvideounderstanding} are constructed based on self-instruct and annotation adaption mode~\citep{li2023visionlanguagereview}. Annotation adaptation involves transforming annotated data into an instructed format. The instruction component explains the task and is usually based on manually created candidate instructions. Typically, one adaptation method is designed for a specific type of annotation data. In contrast, the self-instruct mode combines various types of annotation data to create diverse and complex instructional data that better reflects real-world situations. However, the audio-language instruction tuning does not distinguish the instruction data creation into these two modes, without investigating how the formality and diversity respectively influence the performance of instruction-based audio-language models.
Qwen2-audio~\citep{chu2023qwen} constructs instruction tuning dataset via ChatML~\citep{achiam2023gpt} format. Six kinds of special tags regarding to each task attributes are involved multi-task training. Dynamic-SUPERB~\citep{huang2024dynamicsuperbdynamiccollaborativecomprehensive} first proposed an instruction tuning benchmark for speech, featuring 55 evaluation instances by combining 33 tasks and 22 datasets, but the poor performance of speech models on unseen task shows there are still lots of things to be improved and explored in this domain. Few works focus on exploring the methodology of audio-language instruction tuning for all speech, music and audio. In addition, MINT~\citep{zhao2024mint}, Pengi~\citep{deshmukh2023pengi}, AudioChatLlama~\citep{fathullah2024audiochatllama} . % 这里需要补一点speech的工作，可以从qwen-audio里的related works参考一些，看这部分是放在models那部分还是在此强调instruction tuning

\textbf{Instructed audio-language datasets and benchmarks.}
Intuitively, the audio-language datasets always exist in a question-answering form by utilizing the dialogue abilities of LLMs.
Audio Dialogues~\citep{goel2024audiodialogue} is a multi-turn dialogue dataset containing 163.8k samples of general audio sounds and music with question-answer pairs to compare multiple audios as input. It generates multi-turn dialogues by prompting a LLM and obtaining caption annotations from existing datasets. Similarly, Audio Conversation 10k is a part of MULTIS~\citep{zhao2023chatbridge}, which focuses on multimodal instructional tasks. Clotho-Detail~\citep{ghosal2023t2ageninstructionLLM} comprises 3938 audio-text pairs whose captions were generated using GPT-4 and each audio has five original captions, designed for instruction tuning. Audio Bench~\citep{wang2024audiobench} includes
8 tasks and 26 datasets, targeting speech, voice and audio scene understanding. Other related datasets like can be referred in a survey paper of audio scenes and events~\citep{wijngaard2025audiosurvey}.
% \begin{table*}[htbp]  % 尝试将表格放置在合适的位置
%   \centering  % 使表格居中
%   \begin{tabular}{c c c c c c }
%     \toprule
%             & Generation & Understanding & Speech  & Audio  & Music  \\  % 表头
%     \midrule
%     Musilingo  & $\times$ & $\checkmark$ & $\times$  & $\times$ & $\checkmark$  \\
%     \midrule
%     Audio-Flamingo &$\times$  & $\checkmark$ & $\times$  &  $\checkmark$ & $\times$ \\
%     \midrule
%     LTU & $\times$ & $\checkmark$ & $\times$  &  $\checkmark$ & $\times$  \\
%     \midrule
%     SALMONN & $\times$ & $\checkmark$ & $\checkmark$  &  $\checkmark$ & $\checkmark$  \\
%     \midrule
%     Qwen2-Audio & $\times$ & $\checkmark$ & $\checkmark$  & $\checkmark$  &  $\checkmark$ \\
%     \midrule 
%     UniAudio & $\checkmark$ & $\times$ &  $\checkmark$  &  $\checkmark$ & $\checkmark$\\
%     \midrule
%     AudioFlan & $\checkmark$ & $\checkmark$  & $\checkmark$   & $\checkmark$  & $\checkmark$ \\
%     \bottomrule
%   \end{tabular}
%   \caption{Comparison of Audio FLAN with other models.}  % 表格标题
%   \label{tab:example}  % 表格标签，用于引用
% \end{table*}

\textbf{Audio-language models.}
SALMONN~\citep{tang2023salmonn} is an audio-text multimodal LLM designed for understanding speech, music, and audio events. It focuses on key tasks like speech recognition and audio captioning during its pre-training and instruction tuning stages. However, a task over-fitting problem persists, leading to poor performance on untrained tasks. The paper identifies two reasons: cross-modal instruction prompts are simpler than text-only data, and the responses lack diversity and complexity. To address this, SALMONN introduces an additional stage called activation tuning, which fine-tunes the model on tasks requiring longer and more varied responses, rather than just regularizing the intrinsic conditional language model.  
% SALMONN这部分拆一小段到introduction
Audio-Flamingo~\citep{kong2024audioflamingonovelaudio} supports audio understanding tasks including captioning, question-answering and classification with multi-turn dialogue ability. It also improves few-shot learning skills by incorporating the ICL and RAG methodologies. 
% For training, the unfreezing of audio encoder leads to instability. 
As for data templates with text instruction, the order of task description, audio and instruction are fixed with special tokens indicating the positions of different ingredients. Moreover, it changes the outputs into a multi-choice form for all involved understanding tasks.
Qwen2-audio~\citep{chu2024qwen2}, the advanced version of Qwen-audio~\citep{chu2023qwen}, consists of an audio encoder initialized on a Whisper-large-v3~\citep{radford2022robustspeechrecognitionlargescale} and a LLM, Qwen-7B~\citep{bai2023qwen}. At the pre-training stage, they use natural language prompts instead of hierarchical tags, which help improve better generalization and instruction following abilities. At the supervised fine-tuning stage, the audio analysis and voice chat mode are jointly trained via high-quality SFT data. After that, it also develops the direct preference optimization stage to further enable the model to follow the human preference. Besides, LTU~\citep{gong2023listen}, LTU-AS~\citep{gong2023joint}, MusiLingo~\citep{deng2023musilingo} and Mu-llama~\citep{liu2024mullama} are also relevant works.

However, few previous works succeeded in generalizing various audio-language generation tasks on existing audio LLMs. UniAudio~\citep{yang2023uniaudio} supports audio generation with 11 various tasks across speech, music and sounds. However, it does not support instruction tuning mode and require each kind of task transferred into fixed templates provided for the inference.

To the best of our knowledge, there are no audio-language models simultaneously that can support both instruction tuning audio understanding and generation tasks without significant performance degradation. Therefore, Audio-Flan, a comprehensive instruction-tuning dataset for both audio-language understanding and generation tasks cross diverse audio domains needs to be conducted promptly given the limitations identified in the previous studies.


