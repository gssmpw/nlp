





\section{Audio-FLAN Datasets Construction}

% Overview pipeline
% Dataset with labels
% Dataset with preprocessing
% Instruction template
% self-instruct

% The core of instruction is guiding models to comply with diverse task instructions and provide the appropriate outputs. As a result, the instruction-tuning dataset—comprising high-quality pairs of instructions and corresponding outputs—plays a vital role~\citep{wang2023far,zhou2024lima}.

% Figure~\ref{fig:dataset_pipeline} shows the pipeline of Audio-FLAN dataset construction. We first collect the publicly released datasets and then determine the task for which the dataset can be performed according to the task definition. Then the instructions can be generated and guided by the task templates. Finally, we further make the instructions varied using self-instruct method.

% The core of instruction-tuning is guiding models to follow diverse task instructions and provide the corresponding outputs. As such, the instruction-tuning dataset—which consists of high-quality pairs of instructions and their corresponding outputs—plays a critical role in training models that can generalize across a wide range of tasks~\citep{wang2023far,zhou2024lima}.

Figure~\ref{fig:dataset_pipeline} illustrates the pipeline for constructing the Audio-FLAN dataset. We first collect the publicly released datasets and use their original labels, or manually processed labels, to determine the tasks that can be performed based on the task definitions. Next, instructions are generated and structured using task templates, which guide the format and content of instruction, input, and output. To increase the diversity of the instruction set, we apply a self-instruct-like method~\citep{wang2023self}, where the instructions are varied through tools like LLaMA and GPT, which allow for the creation of multiple variations for each task and instance. These varied instructions are then validated to ensure they meet the required standards before being integrated into the dataset.

\begin{figure*}[thbp]
\centering
\includegraphics[width=\textwidth, height=\textheight, keepaspectratio]{figures/dataset_process_pipeline.pdf}
\caption{Overview pipeline of Audio-FLAN dataset construction.}
\label{fig:dataset_pipeline}
\end{figure*}

% \subsection{Task Category}

% We classify tasks into \textbf{Major Tasks} and \textbf{Minor Tasks} following a hierarchical structure based on the scope and specificity of the tasks within the broader domains of \textbf{Speech}, \textbf{Music}, and \textbf{Audio}, as shown in Table~\ref{tab:major_minor_task}.

% \begin{itemize}
%     \item \textbf{Major Tasks} represent broad categories that encompass a variety of related activities within each domain. For example, in the \textbf{Speech} domain, major tasks include \textbf{Speech Recognition}, \textbf{Speech Generation}, and \textbf{Speech Enhancement}, which cover general areas like recognizing spoken words, generating speech, and improving speech quality, respectively. Similarly, in the \textbf{Music} domain, major tasks include \textbf{Global Music Information Retrieval (MIR)}, \textbf{Music Generation}, and \textbf{Music Separation}, which focus on tasks such as music tagging, generating music, and isolating different music tracks. In the \textbf{Audio} domain, major tasks include \textbf{Audio Event Recognition}, \textbf{Audio Generation}, and \textbf{Audio Separation}, addressing tasks like recognizing environmental sounds, generating new audio, and isolating audio sources from mixed content.
%     \item \textbf{Minor Tasks} are specific subcategories under each major task, providing more focused and detailed areas of work. For example, under \textbf{Speech Recognition}, the minor tasks include \textbf{Automatic Speech Recognition}, \textbf{Dialect Automatic Speech Recognition}, and \textbf{Phonetic Recognition}, each representing a specialized area within the overarching task of recognizing speech. Similarly, under \textbf{Speech Generation}, tasks like \textbf{Zero-shot Text to Speech}, \textbf{Descriptive Speech Synthesis}, and \textbf{Speech to Speech Translation} address more specific aspects of generating speech. In the \textbf{Music} domain, minor tasks under \textbf{Global MIR} include \textbf{Genre Classification}, \textbf{Emotion Classification}, and \textbf{Instrument Classification}, while tasks under \textbf{Music Generation} might include \textbf{Text-to-Music Generation}, \textbf{Lyrics-to-Song Generation}, and \textbf{Singing Voice Synthesis}. In the \textbf{Audio} domain, minor tasks related to \textbf{Audio Event Recognition} include \textbf{Sound Event Detection} and \textbf{Acoustic Scene Classification}, while minor tasks related to \textbf{Audio Generation} include \textbf{Text-guided Audio Generation} and \textbf{Audio Continuation}, and \textbf{Audio Separation} includes minor tasks of \textbf{Text-guided Audio Source Separation} and \textbf{Label-querying Sound Extraction}.
% \end{itemize}


\subsection{Task Category}
We classify tasks into \textbf{Major Tasks} and \textbf{Minor Tasks} following a hierarchical structure based on the scope and specificity of the tasks within the broader domains of \textbf{speech}, \textbf{music}, and \textbf{audio}, as shown in Table~\ref{tab:major_minor_task}. 
\begin{itemize}
    \item \textbf{Major Tasks} represent broad categories that encompass a variety of related activities within each domain. For example, in the speech domain, major tasks include \textit{Speech Recognition}, \textit{Speech Generation}, and \textit{Speech Enhancement}, which cover the general areas of recognizing spoken words, generating speech, and improving speech quality, respectively.
    \item \textbf{Minor Tasks} are specific subcategories under each major task, providing more focused and detailed areas of work. For example, under \textit{Speech Recognition}, the minor tasks include \textit{Automatic Speech Recognition}, \textit{Dialect Automatic Speech Recognition}, and \textit{Phonetic Recognition}, each representing a specialized area within the overarching task of recognizing speech. Similarly, under \textit{Speech Generation}, tasks like \textit{Text to Speech}, \textit{Voice Conversion}, and \textit{Speech to Speech Translation} address more specific aspects of generating speech.
\end{itemize}

% \textbf{Major Tasks} represent broad categories that encompass a variety of related activities within each domain. For example, in the \textbf{Speech} domain, major tasks include \textbf{Speech Recognition}, \textbf{Speech Generation}, and \textbf{Speech Enhancement}, which cover the general areas of recognizing spoken words, generating speech, and improving speech quality, respectively.
% \textbf{Minor Tasks} are specific subcategories under each major task, providing more focused and detailed areas of work. For example, under \textbf{Speech Recognition}, the minor tasks include \textbf{Automatic Speech Recognition}, \textbf{Dialect Automatic Speech Recognition}, and \textbf{Phonetic Recognition}, each representing a specialized area within the overarching task of recognizing speech. Similarly, under \textbf{Speech Generation}, tasks like \textbf{Zero-shot Text to Speech}, \textbf{Descriptive Speech Synthesis}, and \textbf{Speech to Speech Translation} address more specific aspects of generating speech.

\input{tables/major_minor_tasks}

This hierarchical approach provides a clear structure that allows for easy navigation of the tasks. By categorizing tasks into major and minor tasks, it is easier to understand the broad objectives as well as the specific challenges and techniques involved in each sub-area.
Besides, this classification system allows researchers and practitioners to target specific areas of interest. 
% For example, if one is focused on \textbf{Emotion Recognition} in speech, they can directly refer to the \textbf{Paralinguistic Attribute Recognition} major task and its minor tasks like \textbf{Emotion Recognition}, without having to sift through unrelated categories.
% Also, by splitting tasks into major and minor categories, the classification ensures that all aspects of the field are covered.
% , from high-level goals (e.g., \textbf{Speech Generation}) to the most specific methods and applications (e.g., \textbf{Zero-shot Text to Speech}, \textbf{Voice Conversion} under \textbf{Speech Generation}). This allows for a well-rounded approach to research and application in these fields.
Furthermore, the system is flexible, accommodating new tasks as the fields evolve. New minor tasks can be added under existing major tasks, or new major tasks can be created as technology advances, ensuring that the classification system can adapt to future developments.


The \textbf{Audio-FLAN} dataset introduces time-sequential tasks that have been underexplored in previous research, particularly in the textual domain, as time sequences are a distinctive feature of the audio domain. These include tasks like \textit{Melody Extraction}, and \textit{Pitch Estimation} (with timestamps) in the music domain, as well as \textit{Sound Event Sequence Recognition} and \textit{Sound Event Detection} (with timestamps) in the audio domain. These tasks require processing entire audio sequences or segments, highlighting the importance of time-based analysis. In the speech domain, tasks like \textit{Spoken Paragraph Recognition} further emphasize the role of time sequences, as the model must compare recordings and analyze linguistic content aligned over time. 

Additionally, text-based LLMs are often praised for their reasoning capabilities in tackling complex tasks that involve interdependent results. In the music domain, we introduce reasoning tasks where models must first localize a time segment based on instructions and then perform estimations to generate precise answers. For example, \textit{Beat-level Pitch Estimation} and \textit{Beat-level Instrument Recognition} (under \textit{Single Music Reasoning}) require models to interpret musical elements at specific time points, while \textit{Tempo/Key/Instrument/Emotion Comparison} (under \textit{Multiple Music Reasoning}) involves comparing musical features over time. These tasks push the limits of model generalization across complex, time-based data, positioning \textbf{Audio-FLAN} as a unique resource for developing unified models capable of processing time-sensitive audio across speech, music, and audio.


% \textcolor{red}{
% Time sequence is a unique characteristic of the audio domain, distinguishing it from purely textual data. To address the challenges specific to audio LLMs, we introduce sequential tasks in Audio-Flan that have been underexplored in previous studies. These include \textbf{Melody Extraction}, \textbf{Source Separation}, and \textbf{Pitch Estimation} (with timesteps) in the music domain, as well as \textbf{Sound Event Sequence Recognition} and \textbf{Sound Event Detection} (with timesteps) in the sound domain. These tasks require audio LLMs to process entire sequences or segments, with clearly defined start and end times.
% }

% \textcolor{red}{
% Additionally, many LLMs claim to possess reasoning capabilities that enable them to tackle more advanced tasks requiring multi-step intermediate results with mutual influences. Building on time sequences, we introduce audio reasoning tasks in which models must first localize a specified time segment based on instructions, then perform estimations to generate accurate answers. Examples include Beat-level Pitch Estimation and \textbf{Beat-level Instrument Recognition} (categorized under \textbf{Single Music Reasoning}) and \textbf{Tempo/Key/Instrument/Emotion Comparison} (categorized under \textbf{Multiple Music Reasoning}). Detailed definitions of these tasks can be found in Appendix~\ref{appendix:task_definition}.
% }

% In conclusion, the hierarchical classification system effectively divides each domain into high-level tasks (Major Tasks) and more specific subtasks (Minor Tasks), ensuring a clear and organized structure. The total number of tasks, with 23 \textbf{Major Tasks} and 80 \textbf{Minor Tasks}, encompasses a diverse range of understanding and generation tasks across the domains of speech, music, and sound, highlighting the breadth and depth of research and application in these fields. Notably, to the best of our knowledge, the \textbf{Audio-FLAN} dataset is the first instruction tuning dataset to include both \textbf{Speech}, \textbf{Music}, and \textbf{Audio} tasks, covering both \textbf{Generation} and \textbf{Understanding} tasks. This contribution fills a significant gap in the existing body of work, offering a comprehensive resource for advancing research in multimodal audio processing.

In conclusion, the hierarchical classification system effectively organizes each domain into high-level tasks (Major Tasks) and more specific subtasks (Minor Tasks), providing a clear structure. With \textbf{23 major tasks} and \textbf{80 minor tasks}, the dataset covers a wide range of understanding and generation tasks across speech, music, and audio, underscoring the depth of research and application in these fields. Notably, the \textbf{Audio-FLAN} dataset is the first instruction-tuning dataset to incorporate tasks from \textbf{speech}, \textbf{music}, and \textbf{audio}, addressing both \textbf{generation} and \textbf{understanding} tasks. This contribution fosters the development of unified audio-language models with generalization capabilities similar to those in the NLP and computer vision domains.





\subsection{Dataset Processing}
% Dataset 
% Labels
% speaker verification, voice conversion
    % music, svs -> transformation?
% Figure

Our goal is to develop a large and diverse instruction dataset by aggregating tasks from various domains and applications. Building such an extensive instruction dataset from scratch would be highly resource-intensive and time-consuming. To mitigate this challenge, we leverage existing audio datasets from the research community, transforming them into an instructional format. This approach capitalizes on the wealth of labeled data that is already available or manually processed, allowing us to repurpose datasets for broader applications. Specifically, we aggregate over 52 datasets that are either publicly accessible or can be obtained upon request. The datasets associated with each task are listed in Table~\ref{tab:task2dataset}.

In the speech, music and audio domains, many tasks depend heavily on \textbf{pre-labeled data}, such as genre labels, speech annotations, or musical characteristics. For instance, tasks like \textit{Automatic Speech Recognition (ASR)} and \textit{Text-to-Speech (TTS)} rely on paired text and speech data, while \textit{Emotion Recognition} and \textit{Gender Recognition} tasks in speech utilize emotion and gender labels, respectively. In the \textit{Music} domain, tasks like \textit{Genre Classification} and \textit{Emotion Classification} require labeled music data with genre or emotion tags, and \textit{Pitch Classification} and \textit{Instrument Classification} rely on instrument-specific annotations. 
However, there are several tasks for which suitable labeled datasets are not readily available or require additional processing. For example, tasks such as \textit{Audio Inpainting} or \textit{Music Generation} often lack directly available labels or training data that match the specific needs of these tasks. In these cases, \textbf{manual processing} is required to create the necessary data.

% For \textbf{Speech Denoising}, a task in the \textbf{Speech Enhancement} category, the dataset processing involves generating noisy-clean paired speech data. This is achieved by combining existing clean speech datasets with noise datasets (e.g., traffic noise or background noise). The clean speech data is used as the "reference" signal, while the noise is added to simulate noisy speech, allowing the model to learn to separate the noise from the speech signal.

In the \textbf{speech} domain, for \textit{Speech Enhancement} tasks, data simulation techniques generate task-specific datasets from clean speech corpora. For \textit{Denoising}, noisy-clean pairs are created by adding noise to clean speech samples. \textit{Dereverberation} involves generating reverberant-clean pairs by convolving clean speech with real or simulated room impulse responses. In the \textit{Declipping} task, clean speech is randomly clipped for model input. For \textit{Speech Bandwidth Extension}, high-sample-rate speech is downsampled to teach the model how to recover high-quality speech from lower-quality input. In \textit{Speaker Recognition}, \textit{Speaker Extraction} creates datasets by mixing clean speech from multiple speakers and providing reference speech for the target speaker.



% In \textbf{Speech Enhancement} tasks, data simulation techniques are often employed to generate task-specific datasets from clean speech corpora. For \textbf{Denoising}, the process involves creating paired noisy-clean speech data by mixing clean speech samples with noise samples under the `additive noise' assumption. For \textbf{Dereverberation}, paired reverberant-clean speech data is generated by convolving clean speech signals with real or simulated room impulse responses to produce reverberant speech. For the \textbf{Declipping} task, clean speech is randomly clipped to serve as the model input. For the \textbf{Speech Bandwidth Extension} task, high-sample-rate speech is downsampled, allowing the model to learn how to recover high-sample-rate speech from low-sample-rate input. In the \textbf{Speaker Recognition} category, \textbf{Speaker Extraction} requires the creation of datasets by mixing clean speech from multiple speakers and providing reference speech for the target speaker.

Similarly, the \textit{Music Generation} tasks in the \textbf{music} domain, such as for the \textit{Text-guided Music Continuation} or \textit{Lyrics-to-song Generation}, manually processed data might be needed to create the text-to-music pairs. This could involve taking existing music pieces and pairing them with relevant textual descriptions, or generating new musical content based on textual input using music generation models. In cases where music data is not paired with lyrics, data augmentation techniques might be used, where new synthetic music tracks are generated by modifying or extending the existing ones to suit the task.

In the \textbf{audio} domain, the \textit{Audio Generation} tasks such as \textit{Audio Inpainting}, the data processing involves selecting clean audio samples, cutting them to create gaps, and preparing the dataset for further use in reconstructing the missing segments. In \textit{Audio Super-resolution}, the process includes downsampling high-quality audio to a lower resolution and then using the downsampled version to recreate the original high-resolution audio. These processing steps facilitate the generation of suitable datasets for these tasks.


% For \textbf{Audio Generation} tasks like \textbf{Audio Inpainting}, which involves reconstructing missing audio segments, processing steps may involve selecting clean audio samples, cutting them to create gaps, and then training models to fill those gaps. Similarly, \textbf{Audio Super-resolution} may require downsampling high-quality audio to a lower resolution and then training a model to reconstruct the high-resolution version.
% For the \textbf{Audio Continuation} task, we prepared shorter, clipped versions of the original audio to serve as model inputs, training the model to reconstruct the missing audio segments. For the \textbf{Audio Separation} tasks, we generated \textbf{Text-guided Audio Source Separation} datasets by randomly mixing original audio samples and created textual descriptions of the mixed audio using the captions from the original dataset. Additionally, a \textbf{Label-querying Sound Extraction} dataset and an \textbf{Audio-querying Sound Extraction} dataset were created in a similar manner. For the \textbf{Voice Activity Detection} task, WebRTC toolkit is employed to detect voice activity in clean speech, generating labels that are subsequently used to guide the training of the model.

These cases highlight the flexibility and adaptability of existing datasets in the speech, music, and audio domains, where manual dataset processing and augmentation are crucial for handling tasks with limited labeled data or where the required labels do not exist. By applying these dataset processing techniques, we can ensure that tasks with scarce resources are still effectively addressed, broadening the applicability of existing datasets to more diverse machine learning applications. Furthermore, the \textbf{Audio-FLAN} dataset is continuously being expanded and processed to cover additional tasks. We also invite all interested researchers and practitioners to contribute to the ongoing development of the \textbf{Audio-FLAN} instruction tuning dataset, enhancing its scope and utility for the community.


% \comments{Ziya update the music data processing in this subsection if necessary}

% \comments{zixuan add the speech/audio data processing in this subsection if necessary}



\subsection{Task Instruction Template}
The instruction data we aim to generate consists of a collection of instructions \( \{I_{i}\} \), each describing a specific task \( i \) in natural language. For each task \( i \), there are \( n_{i} \geq 1 \) input-output pairs \( \{(X_{t,i}, Y_{t,i})\}_{t=1}^{n_{i}} \). Once the tasks to be covered by the dataset are determined, we process the data into three core components: \textbf{instruction}, \textbf{input}, and \textbf{output}, all formatted in JSONL (JSON Lines) format. The \textbf{instruction} serves as a concise description of the task, guiding the model on the expected input and the type of output to generate. For tasks that involve understanding, the \textbf{output} is \textit{text}, while for tasks focused on generation, the \textbf{output} is typically audio. The \textbf{input} can be \textit{audio}, \textit{text}, or a combination of both, depending on the task. Formally, given this structured data, a model \( M \) is expected to generate the appropriate output based on the task instruction and the corresponding input: \( M(I_{i}, X_{t,i}) = Y_{t,i}, \quad \text{for } i \in \{1, \ldots, n_{i}\} \).


In the \textbf{speech} domain, the task of \textit{Speech-to-Text Translation} involves both text and audio as input (e.g., an audio recording of speech and the corresponding transcription in target language), and the output is text, which is the translated text in a different language. In the \textbf{music} domain, the task of \textit{Text-guided Music Generation} uses a combination of text and audio as input (e.g., a description of the type of music and a short melody clip), and the output is audio, which is a generated music track that matches the input description and melody. In the \textbf{audio} domain, tasks like \textit{Audio Super-resolution} can take a combination of low-resolution audio and textual description of the expected quality improvements as input, and the output is high-resolution audio that enhances the quality of the input signal.

To generate the task instructions \( \{I_{i}\} \), we initially employ template-based instructions. These instructions are human-written, task-specific descriptions that explicitly define the task. For example, the \textbf{instruction} for the \textit{Speech-to-Text Translation} task could be "Please translate the speech into the text in Chinese.". For \textit{Text-guided Music Generation}, the \textbf{instruction} might be "Please continue the audio music prompt based on the given text description." The \textbf{instruction} for the \textit{Audio Super-resolution} task can be "Please increase the resolution of the given audio signal to 32K Hz". Here are the three task instruction templates:
\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Speech-to-Text Translation]
\{
"instruction": "Please translate the speech into the text in English.", 
"input": "<|SOA|>Audio\_ID<|EOA|>", 
"output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing."
\}
\end{tcolorbox}

% jsonl/14_FLEURS.understanding.Speech Recognition.Speech-to-text Translation.jsonl:234092:{"instruction": "Please translate the speech into the text in English.", "input": "<|SOA|>7444407507383421417.wav<|EOA|>", "output": "Nevertheless, there are many distinctive ways of drinking coffee around the world that are worth experiencing.", "uuid": "14_FLEURS_15ddd76abaf9f695", "split": ["train"], "task_type": {"major": ["Speech Recognition"], "minor": ["Speech-to-text Translation"], "U/G": ["understanding"], "unseen": false}, "domain": "speech", "source": ["unknown"]}

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Text-guided Music Continuation]
\{
"instruction": Please continue the audio music prompt based on the given text description",
"input": "input": "This is a Carnatic music piece set in the atana raga. It follows the 5/8 meter and is composed in the khandaChapu taala. The lead instrument featured in this performance is vocal, accompanied by Mridangam. The kalai of this composition is 1. \textbackslash n audio prompt: <|SOA|>Audio\_ID<|EOA|>",
"output": "audio: <|SOA|Audio\_ID<|EOA|>"
\}
\end{tcolorbox}

% "instruction": "Please continue the audio music prompt based on the given text description", "input": "This is a Carnatic music piece set in the atana raga. It follows the 5/8 meter and is composed in the khandaChapu taala. The lead instrument featured in this performance is vocal, accompanied by Mridangam. The kalai of this composition is 1. \n audio prompt: <|SOA|>128_Compmusic_13028_1-02_Purandara_Dasa_3_prompt<|EOA|> \n", "output": "audio: <|SOA|>128_Compmusic_13028_1-02_Purandara_Dasa_3<|EOA|>\n", "uuid": "128_Compmusic_2dd0497fb6a86a4e", "split": ["test"], "task_type": {"major": ["Cross-modal Generative Tasks"], "minor": ["Text-guided Music Continuation"], "U/G": ["generation"], "unseen": false}, "domain": "music", "source": ["meeting"], "other": {"tag": ""}}

\begin{tcolorbox}[colback=white, colframe=black, boxrule=0.2mm, arc=0mm, title=Sound Super-resolution]
\{
"instruction": "instruction": "Please increase the resolution of the given audio signal to 32k Hz.", 
"input": "audio: <|SOA|>Audio\_ID<|EOA|>."
 "output": "<|SOA|>Audio\_ID<|EOA|>",
\}
\end{tcolorbox}

% {"instruction": "Please increase the resolution of the given audio signal to 32k Hz.", "input": "audio: <|SOA|>180_Audioset_for_Audio_Super_resolution_input_Y_Uro9suV3xU<|EOA|>.", "output": "<|SOA|>180_Audioset_for_Audio_Super_resolution_lable_Y_Uro9suV3xU<|EOA|>", "uuid": "180_Audioset_for_Audio_Super_resolution_d520a9c120642c77", "split": ["train"], "task_type": {"major": ["Audio Generation"], "minor": ["Audio Super-resolution"], "U/G": ["generation"], "unseen": false}, "domain": "audio", "source": "youtube", "other": null}\

We include \texttt{<SOA>} to mark the start of audio, and \texttt{<EOA>} to signify the end of audio. When the input contains multiple values, they are separated by \texttt{\textbackslash n}. Note that the JSONL format files contain not only the \textit{instruction}, \textit{input}, and \textit{output}, but also other relevant fields such as \texttt{uuid}, \texttt{split}, \texttt{task\_type}, and \texttt{domain}. The complete JSON file content can be found in Appendix~\ref{appendix:instruction_template}. These task-specific templates serve as foundational structures, which can later be refined and expanded upon to better suit a wide range of tasks across different domains. This method ensures that the instructions are both clear and aligned with the model's input-output expectations.




\subsection{Instruction Variation}\label{sec:self_instruct}
While fixed, template-based instructions provide consistency in task execution, they inherently constrain flexibility and creativity. This rigidity can hinder the model’s ability to adapt to diverse and nuanced task descriptions. To mitigate these limitations and enhance the diversity and creativity of the instructions, we introduce an approach that expands template-based instructions into a broader set of variations using advanced language models, like LLaMA~\citep{touvron2023llama}. By leveraging the generative power of these models, we can produce multiple distinct variations for each task instruction template, thereby augmenting the model's capacity to handle a wide array of task descriptions.

The process of instruction variation follows a three-step pipeline, inspired by the self-instruct approach~\citep{wang2023self}, designed to systematically enhance instruction diversity. These steps include: (1) initializing the variation seed pool, (2) generating new diverse instructions, and (3) validating the generated instructions.

In the first step, we begin by generating five new instruction examples for each task using GPT-4o, which serves as the initial "seed" pool. These initial variations form the basis for subsequent instruction generation. In the second step, we utilize the Llama-3.1-70B-Instruct model to generate instruction variation, drawing from the seed pool. Llama-3.1-70B-Instruct allows for the generation of diverse and contextually varied instructions, along with modifying or adding prefixes within the \textit{input} and \textit{output} fields based on the specific characteristics of the task. This process allows for further customization of task instructions that are both rich in variation and contextually appropriate.

The final step involves rigorous validation of the generated instructions to ensure their integrity and quality. Specifically, we verify that the audio ID remains consistent with the original task instance and confirm that the JSONL format adheres to the required structure. Any variations that exhibit formatting errors, such as incorrect JSONL syntax or mismatched audio IDs, are identified and excluded from the pool. Any instructions deemed invalid are flagged for regeneration, and if no suitable variation can be generated by the model, manual intervention is employed to address the issue. This ensures that both the quantity and quality of the variations are maintained. Valid instructions are then reintegrated into the task pool for use in generating further variations. 

This iterative process promotes a dynamic and evolving pool of task instructions, effectively maximizing their diversity. As a result, the model becomes more adept at handling a wide range of task descriptions, ultimately improving its overall performance and generalization ability across diverse use cases. 
The prompt used to produce various instructions by GPT-4 and LLaMA is provided in Appendix~\ref{appendix:instruction_variation_prompt}. Specific examples of the instruction template and generated instruction variations are shown in Appendix~\ref{appendix:instruction_variation_example}.


% instruction variation
% For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset. While most of the ten templates describe the original task, to increase diversity, for each dataset we also include up to three templates that “turned the task around,” (e.g., for sentiment classification we include templates asking to generate a movie review). 


% \lmxue{add a figure to illustrate the flow of instruction variation}

