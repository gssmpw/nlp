



\section{Audio-FLAN Dataset}

Figure~\ref{fig:dataset_overview} illustrates the structure of the \textbf{Audio-FLAN} dataset, which spans a diverse range of tasks and instances. It is organized into 23 major tasks and 80 minor tasks from 52 released datasets\footnote{Each dataset may correspond to one or more tasks. The 52 datasets in Audio-FLAN represent the unique datasets after deduplication. The total number of data points for different tasks can exceed 52.}, totaling 108.5M instances. These tasks are divided into two primary categories: \textbf{understanding} and \textbf{generation}.

\begin{itemize}
    \item \textbf{Understanding}: This category consists of 16 major tasks and 51 minor tasks with 51 open-sourced datasets, amounting to 62.44M instances. The understanding tasks are further divided into three domains:
    \begin{itemize}
        \item \textbf{Speech}: 6 major tasks and 20 minor tasks, with 24 datasets and 57.42M instances.
        \item \textbf{Music}: 5 major tasks and 21 minor tasks, with 19 datasets and 1.46M instances.
        \item \textbf{Audio}: 5 major tasks and 10 minor tasks, with 8 datasets and 3.56M instances.
    \end{itemize}
    
    \item \textbf{Generation}: This category includes 7 major tasks and 29 minor tasks with 31 publicly available datasets, with a total of 46.06M instances. The generation tasks are categorized as follows:
    \begin{itemize}
        \item \textbf{Speech}: 2 major tasks, 14 minor tasks, with 12 datasets and 43M instances.
        \item \textbf{Music}: 2 major tasks, 7 minor tasks, with 13 datasets and 0.71M instances.
        \item \textbf{Audio}: 3 major tasks, 8 minor tasks, with 6 datasets and 2.35M instances.
    \end{itemize}
\end{itemize}

Overall, the \textbf{Audio-FLAN} dataset provides a comprehensive and balanced set of tasks across the \textbf{speech}, \textbf{music}, and \textbf{audio} domains, supporting both understanding and generation tasks in the audio field. The \textbf{Audio-FLAN} dataset fills a critical gap in the audio research community, offering the first large-scale, instruction-driven corpus for unified audio-language models. 

\begin{figure*}[h] % [h]表示“这里”，你也可以使用[htbp]等选项
\centering
\scalebox{0.9}{
\includegraphics[width=\linewidth]{figures/g_u_tasks_fig.pdf}
}
\caption{Overview of Audio-FLAN dataset.}
\label{fig:dataset_overview}
\end{figure*}



\subsection{Statistics of Task}
% \subsection{Task and Instance}
The \textbf{Audio-FLAN} dataset, spanning across the \textbf{speech}, \textbf{music}, and \textbf{audio} domains, is summarized in Table~\ref{tab:task_and_instances}. The dataset consists of 23 major tasks and 80 minor tasks across these domains, totaling 108.5M instances. These tasks cover a wide range of applications and modalities, integrating both \textbf{understanding} and \textbf{generation} tasks across various domains. The dataset’s diversity is further enhanced by the variety of input-output formats, including audio, text, and multimodal combinations such as audio and text, allowing it to represent complex and realistic scenarios.

\input{tables/task_instances}

\textbf{Speech Domain:} The Speech domain encompasses 8 major tasks, including \textit{Speech Recognition}, \textit{Speech Generation}, and \textit{Paralinguistic Attribute Recognition}, addressing both understanding and generation tasks. The Speech domain includes 34 minor tasks, with a total of 100.42M instances, showcasing a comprehensive and diverse task representation. Notably, tasks such as \textit{Speech Enhancement} and \textit{Speech Generation} focus on generation tasks, while tasks like \textit{Speech Recognition} and \textit{Speaker Recognition} are geared toward understanding tasks. The large number of instances in this domain provides a rich dataset for training models, enhancing their ability to generalize across a wide range of speech-related tasks. This abundance of data enables models to learn robust representations, improving their performance and versatility when tackling unseen tasks in the Speech domain.

\textbf{Music Domain:} The Music domain features 7 major tasks, covering various music-related applications such as \textit{Global MIR} (Music Information Retrieval), \textit{Music Generation}, and \textit{Text-guided Music Generation}. Both understanding tasks (e.g., genre classification, emotion recognition) and generative tasks (e.g., music composition from text descriptions) are included. With 28 minor tasks and over 2.17 million instances, the Music domain excels in multi-modal tasks, such as \textit{Text-guided Music Generation}, where input combinations of text descriptions and audio prompts are used. The inclusion of music generation tasks involving multimodal inputs enhances the flexibility and capability of the unified model to generate and comprehend music in diverse ways. The variety in input-output combinations fosters a more comprehensive understanding of music, making the model highly adaptable and capable of handling both music-related understanding and generation tasks seamlessly.

\textbf{Audio Domain:} The Audio domain includes 8 major tasks, such as \textit{Audio Event Recognition}, \textit{Audio Generation}, and \textit{Audio Separation}, along with 18 minor tasks and 5.91 million instances. The tasks span a broad range of applications, from sound classification to audio enhancement and separation. Notably, the Audio domain includes tasks such as \textit{Audio Generation} and \textit{Audio Super-resolution}, which play a key role in advancing the field of audio processing. The diversity of tasks in this domain enhances the model’s ability to understand and generate a wide variety of audio content, further enriching the overall capabilities of the unified audio-language model.

The \textbf{Audio-FLAN} dataset makes a significant contribution to the development of unified models that can both understand and generate audio across multiple domains, including speech, music, and audio. By integrating a diverse set of tasks, the dataset ensures that the models can handle a broad spectrum of real-world audio applications. The varying number of instances across different tasks in the dataset provides a rich foundation for training models. Tasks with larger datasets, such as those in the speech domain, provide ample data for the model to develop a robust understanding of common patterns and features. This helps models generalize well across various tasks, improving their performance and robustness in real-world applications. The variety in instance sizes ensures that the model can remain adaptable and flexible, capable of learning from both high- and low-representation tasks, which is crucial for tasks that are less represented.

While the dataset is highly diverse, it is worth noting that the data distribution across domains is not perfectly balanced. The speech domain, with its larger number of instances, naturally provides more data for training compared to the music and audio domains. We are committed to continuously updating and expanding the \textbf{Audio-FLAN} dataset to include more tasks, domains, and instances. We also encourage the community to contribute by adding new tasks and improving the dataset. By working together, we can build a more comprehensive resource that further advances the development of unified audio-language models and benefits the broader research community.



% \textbf{Audio-FLAN} dataset is the first comprehensive instruction-tuning dataset that spans across the \textbf{Speech}, \textbf{Music}, and \textbf{Audio} domains. It integrates both \textit{understanding} and \textit{generation} tasks, offering a versatile resource for the development of models that focus on either or both aspects in the field of audio. 
% The dataset’s diversity in input-output formats, including audio, text, and multimodal combinations, facilitates more complex and comprehensive learning. By offering extensive task coverage and a variety of input-output modalities, \textbf{Audio-FLAN} contributes significantly to the field of instruction tuning for audio-language models.

% Statistics Figures here
% Speech: age, gender, nationality, language
% Music: genre, instruments
% Sound: scene types

% \begin{figure}[h!]
%     \centering
%     \begin{tikzpicture}
%         \begin{axis}[
%             ybar=2pt,    % Bar thickness
%             symbolic x coords={Speech, Music, Audio}, % X-axis labels
%             xtick=data,  % X-ticks at data positions
%             yticklabel style={/pgf/number format/1000 sep=}, % Y-axis formatting
%             ylabel={Number of Tasks}, % Y-axis label
%             xlabel={Domain},         % X-axis label
%             enlarge x limits=0.5,     % Space between bars
%             bar width=15pt,           % Bar width
%             ymin=0, ymax=40,          % Y-axis limits
%             legend style={at={(0.5,-0.25)}, anchor=north, legend columns=2}, % Move the legend below the plot
%             x tick label style={rotate=45, anchor=north west, yshift=-1ex}, % Rotate x-axis labels and move them down
%             xtick distance=1,         % Control distance between ticks
%         ]
%             % Define the bars for each domain
%             \addplot coordinates {(Speech, 8) (Music, 6) (Audio, 8)}; % Understanding
%             \addplot coordinates {(Speech, 33) (Music, 28) (Audio, 20)}; % Generation
%             \legend{Understanding, Generation}
%         \end{axis}
%     \end{tikzpicture}
%     \caption{Task Distribution Across Domains}
%     \label{fig:domain_tasks}
% \end{figure}




\subsection{Distribution of Audio Attributes}
% This distribution emphasizes the rich diversity of tasks covered in the speech domain, which encompasses both foundational understanding tasks and more specialized areas such as noise reduction and speaker identification. The variety in task types in this domain supports the development of unified models capable of handling a broad spectrum of audio-language tasks with zero-shot generalization.

% In the music domain, we observe a broad range of features, including both global characteristics such as "Genre," "Artist," and "Emotion," as well as more localized musical elements like "Beat," "Key," "Instrument," "Technique," "Pitch," and "Chord". These features collectively enrich the dataset and allow for a deeper understanding of music from both a genre and structural perspective. To increase diversity and reduce the bias toward mainstream genres, the dataset also incorporates music from less-represented cultural backgrounds, categorized under "Ethnomusicology." This inclusion enriches the diversity of the music tasks, allowing the model to better handle a variety of musical styles, especially those not widely recognized in global datasets.

% In the sound domain, the most frequent features are "Scene," "Event," and "Speech." These features aim to extract semantic meaning from a wide variety of environmental sounds, without the specific context typically found in speech or music. This domain focuses on a more general approach to sound recognition, aiming to understand and categorize general sounds based on their semantic relevance.


% Figure~\ref{fig:data_type_distribution} shows the data type distribution of three domains in the Audio-Flan. Based on the datasets selected, we list the features of each domain respectively and calculate their portions to reveal how frequent the feature appears in the whole dataset. In speech domain,  "Content", which represents xxx and "Language" are the most common features, especially for the xx task. In music domain, not only the global features like Genre, Artist, Emotion, but also the local musical element like Beats, Keys, Instruments, Techniques, Pitch and Chords are considered. In order to increase the diversity and eliminate the overwhelming influence on some general music genres and styles, we also specifically incorporate music cultivated from different cultures but still not popular enough and name them as the type of  Ethnomusicology. In sound domain, Scene, Event and Speech are most frequent features, focusing on digging out semantic information blindly from general sound without many hints. Please note that an instance may contain multiple features, so this statistics information focuses on revealing frequency, instead of exclusively reporting the absolute numbers of instances for each feature.

% Each subdomain of the audio involves many aspects. Specifically, speech conveys semantic content, and speaker identity, and captures crucial paralinguistic features such as emotion, language, accent age, etc. Music contains different instruments, timbres, techniques, and so on. Audio sound can be different events, animals, scenes and may also include speech or music. To have a look at these different audio attributes in Audio-FLAN dataset, we count the instance distribution of tasks related to different audio attributes in speech, music and sound domain, respectively, as shown in Figure~\ref{fig:data_type_distribution}.

% As we can see in Figure~\ref{fig:data_type_distribution} (a), in the speech domain, \textbf{Content} and \textbf{Language} are the most prevalent features, yielding about 35.5\% and 32.1\%, respectively.
 % It indicates that the tasks related to the content and language dominate in the whole 34 speech minor tasks. The content-related tasks can be \textit{Automatic Speech Recognition}, \textit{Dialect Automatic Speech Recognition}, and the language-relevant tasks would be \textit{Language Identification}, \textit{Speech to Text Translation}, \textit{Speech to Speech Translation}. The remaining portion covers gender, age, dialect, speaker, phoneme, distortion, and detection, showing the great diversity of instruction-tuning tasks. Among these, tasks about gender have the highest percentage of 8.8\%. While, tasks related to age, dialect and distortion (such as \textit{Denoising}, \textit{Dereverberation}) own 5.7\%, 5.5\%, and 4.2\%, respectively. Furthermore, the task also covers intent, emotion, accent, and device, even with a small percentage of 1.1\% totally.

 Each subdomain in the audio field encompasses a wide range of attributes. Specifically, the \textbf{speech} domain captures semantic content, speaker identity, and critical paralinguistic features such as emotion, language, accent, age, and more. The \textbf{music} domain contains a variety of musical attributes, including different instruments, timbres, techniques, and structures. Meanwhile, the \textbf{audio} domain covers diverse sounds, including events, animals, scenes, and even speech or music. To explore the different audio attributes in the \textbf{Audio-FLAN} dataset, we analyze the instance distribution of tasks related to these attributes across the speech, music, and audio domains, as shown in Figure~\ref{fig:data_type_distribution}.

 \begin{figure}[h] % [h]表示“这里”，你也可以使用[htbp]等选项
    \centering % 居中图片
    \includegraphics[width=\linewidth]{figures/all_domains_nested_donut_cropped.pdf} % 替换为你的图片文件名
    \caption{Distribution of audio attributes in (a) speech domain, (b) music domain, and (c) audio domain.}
    \label{fig:data_type_distribution}
\end{figure}

% \textbf{Speech Domain:}
% As illustrated in Figure~\ref{fig:data_type_distribution} (a), in the speech domain, the most prominent features are \textbf{Content} and \textbf{Language}, comprising approximately 35.5\% and 32.1\% of the tasks, respectively. \textbf{Content} involves tasks that focus on understanding and transcribing spoken language, such as \textit{Automatic Speech Recognition (ASR)}, \textit{Dialect Automatic Speech Recognition}, and other speech processing tasks that convert spoken language into text. \textbf{Language} pertains to tasks that involve language-specific features in speech, such as \textit{Language Identification}, \textit{Speech to Text Translation}, and \textit{Speech to Speech Translation}, which are essential for translating and identifying spoken language across different contexts.

% In addition to these primary features, the speech domain includes a variety of other tasks. \textbf{Gender}-related tasks, which make up the largest share at 8.8\%, focus on identifying the speaker’s gender through speech characteristics. Tasks related to \textbf{Age} (5.7\%), \textbf{Dialect} (5.5\%), and \textit{Distortion} (such as \textit{Denoising} and \textit{Dereverberation}) (4.2\%) are also important, addressing issues like speaker demographics and improving audio quality by removing noise and reverberation. Furthermore, the domain includes tasks related to \textit{Speaker Identification}, \textit{Phoneme Recognition}, and \textit{Detection Tasks}, which contribute to the classification and understanding of speech characteristics.

% Smaller, yet still significant, contributions come from tasks related to \textbf{Intent}, \textbf{Emotion}, \textbf{Accent}, and \textbf{Device} (1.1\%), which help in identifying the speaker's emotional state, accent, or even the device used for speech recording. These tasks contribute to the fine-grained understanding of speech signals, making the speech domain rich with both foundational and specialized tasks.


% \textbf{Music Domain:}
% As illustrated in Figure~\ref{fig:data_type_distribution} (b), in the music domain, the most prominent features are \textbf{Instrumental} (17.6\%) and \textbf{Timbre} (12.9\%), which together account for a significant portion of the tasks. \textbf{Instrumental} refers to tasks focusing on identifying and analyzing the different instruments used in a musical piece, which is vital for understanding the structure of a composition. Tasks like \textit{Instrument Classification}, \textit{Beat-level Instruments Recognition}, and \textit{Vocal/Instrumental Technique Classification} are crucial in recognizing the instruments and techniques used in both vocal and instrumental music. \textbf{Timbre}, which describes the tonal quality or color of the sound, plays a significant role in differentiating between various instruments and voices. \textit{Singing Voice Conversion} is related to this feature, enabling the model to capture the unique characteristics of different sound sources.

% Another important feature is \textbf{Ethnomusicology} (12.3\%), which focuses on music from diverse cultures, especially those underrepresented in mainstream datasets. This feature contributes to the dataset’s diversity, allowing the model to better handle a variety of musical styles and traditions. Tasks such as \textit{Instrumental Technique Classification}, \textit{Text-to-music Generation} and \textit{Text-guided Music Continuation} help in recognizing cultural and artistic influences across musical genres.

% Additionally, \textbf{Melody} (5.3\%) and \textbf{Vocals} (19.4\%) are also key features in the music domain. \textbf{Melody}-related tasks include \textit{Melody Extraction}, which is critical for analyzing the melodic lines in a piece of music. \textbf{Vocals}, as a prominent part of many musical works, involve tasks like \textit{Vocal Technique Classification} and \textit{Singing Voice Synthesis}, which focus on analyzing vocal techniques and synthesizing human-like singing voices.

% Other notable features include \textbf{Pitch} (5.1\%), \textbf{Key} (4.9\%), \textbf{Technique} (5.6\%), and \textbf{Chord} (2.2\%). These features are integral to understanding the different levels of musical elements of a music excerpt. Tasks like \textit{Chord Estimation}, \textit{Pitch Classification}, and \textit{Key Detection} focus on identifying the musical elements that define the progression and structure of a song.


% \textbf{Audio Domain:}
% As shown in Figure~\ref{fig:data_type_distribution} (c), in the audio domain, the most frequent feature is \textbf{Scene}, which accounts for 33.4\%. \textbf{Scene} represents broader environmental sounds, such as those from natural or urban settings, and plays a key role in understanding the context in which sounds occur. This category captures the overall soundscape, providing important semantic information that helps in categorizing general audio content. This feature is related to tasks such as \textit{Acoustic Scene Classification}, which focuses on classifying different environments based on their audio characteristics.

% The next most frequent features are \textbf{Event} (22.2\%) and \textbf{Speech} (20.3\%). \textbf{Event} involves the recognition of specific occurrences, such as car alarms, door slams, or sirens. These are discrete, identifiable sounds that provide valuable information about actions or situations, which are analyzed through tasks like \textit{Sound Event Recognition} and \textit{Sound Event Detection}. \textbf{Speech}, on the other hand, includes non-speech audio components within general soundscapes and focuses on identifying speech elements even when they are not the primary subject of analysis. This feature is associated with tasks such as \textit{Speech and Non-speech Detection} and \textit{Voice Activity Detection}.

% Additionally, the \textbf{Others} category accounts for 24.1\% of the audio domain's features. Within \textbf{Others}, the largest share is occupied by \textbf{Music} (28.3\%), highlighting the role of music in environmental audio tasks, where music elements can coexist with other types of sounds. This is linked to tasks such as \textit{Text-guided Audio Generation} and \textit{Audio Separation}, which focus on generating or isolating musical elements from mixed audio data. 
% The \textbf{Object} category (26.1\%) represents sounds produced by inanimate objects, such as machinery, footsteps, or mechanical noises. This feature is related to tasks like \textit{Sound Event Sequence Recognition} and \textit{Audio Source Separation}. 
% Finally, the \textbf{Human} feature (25.3\%) refers to sounds produced by humans that are not directly related to speech, such as vocalizations or other human-generated sounds. This is captured through tasks like \textit{Sound Event Understanding} and \textit{Audio Event Recognition}.

\textbf{Speech Domain:}
As shown in Figure~\ref{fig:data_type_distribution} (a), in the speech domain, the most prominent features are \textbf{content} (35.5\%) and \textbf{language} (32.1\%). \textbf{content}-related tasks, like \textit{Automatic Speech Recognition (ASR)}, focus on transcribing spoken language into text, while \textbf{language}-related tasks, such as \textit{Language Identification} and \textit{Speech to Text Translation}, handle the translation and identification of speech across languages.

Additional tasks in the speech domain cover features like \textbf{gender} (8.8\%), which identifies the speaker’s gender, and \textbf{age} (5.7\%), \textbf{dialect} (5.5\%), and \textit{distortion} (4.2\%) tasks, such as \textit{Denoising} and \textit{Dereverberation}, which improve speech quality. Smaller, yet significant contributions come from tasks related to \textbf{emotion}, \textbf{accent}, and \textbf{device} (1.1\%), contributing to a more nuanced understanding of speech signals.

\textbf{Music Domain:}
As shown in Figure~\ref{fig:data_type_distribution} (b), the music domain's most prominent features are \textbf{instrumental} (17.6\%) and \textbf{timbre} (12.9\%). \textbf{instrumental} tasks, like \textit{Instrument Classification} and \textit{Beat-level Instrument Recognition}, focus on identifying and analyzing different musical instruments. \textbf{timbre} is related to the tonal quality of sound, and tasks like \textit{Singing Voice Conversion} capture the unique characteristics of sound sources.

The domain also includes \textbf{ethnomusicology} (12.3\%), which helps the model understand diverse cultural music, and tasks like \textit{Text-to-Music Generation} and \textit{Text-guided Music Continuation}. \textbf{vocals} (19.4\%) and \textbf{melody} (5.3\%) tasks like \textit{Vocal Technique Classification} and \textit{Melody Extraction} focus on analyzing vocal and melodic elements in music. Additional tasks cover \textbf{pitch} (5.1\%), \textbf{key} (4.9\%), and \textbf{chord} (2.2\%), focusing on musical structure and harmony.

\textbf{Audio Domain:}
As shown in Figure~\ref{fig:data_type_distribution} (c), the audio domain is dominated by \textbf{scene} (33.4\%), which represents environmental sounds, aiding in contextualizing audio. Tasks like \textit{Acoustic Scene Classification} categorize different environments based on their audio characteristics. \textbf{event} (22.2\%) and \textbf{speech} (20.3\%) features involve tasks like \textit{Sound Event Recognition} and \textit{Speech Detection}, which identify specific events and speech elements in general soundscapes.

Additionally, the \textbf{others} category (24.1\%) includes \textbf{music} (28.3\%), \textbf{object} (26.1\%), and \textbf{human} (25.3\%) features, covering tasks like \textit{Audio Event Detection}, \textit{Audio Source Separation}, and \textit{Speech and Non-speech Detection}, providing a comprehensive approach to general audio processing and recognition.


It is important to note that each instance may contain multiple features. As a result, the statistics presented reflect the frequency of feature occurrences rather than the absolute count of instances associated with each feature. 
This distribution highlights the \textbf{ rich diversity of attributes} within both the \textbf{speech}, \textbf{music} and \textbf{audio} domains, encompassing foundational tasks such as speech recognition and speaker identification, as well as more specialized areas like noise reduction, environmental sound recognition, and music analysis. The broad range of features and tasks in these domains supports the development of unified models that can be generalized across various audio-language tasks. This diversity enables models to adapt to a wide variety of contexts, enhancing their zero-shot generalization capabilities across different types of audio with diverse attributes.


