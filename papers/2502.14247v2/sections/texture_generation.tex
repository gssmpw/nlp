\section{Texture Generation}
\label{sec:texture}
The proposed texture generation pipeline consists of several stages, each contributing to the generation of consistent and high-quality textures. Fig.~\ref{fig:texture_pipeline} illustrates the texture generation pipeline. The pipeline begins with a 3D mesh without texture. Below we introduce each stage in detail.

\begin{figure*}
    \centering 
    \includegraphics[width=\linewidth]{figures/texture/texture_simp_2.jpg}
    \caption[Texture Generation Pipeline]
    {Texture Generation Pipeline (input image and mesh from Trellis3D). }
    \label{fig:texture_pipeline}
\end{figure*}

\subsection{Frontal Image Generation} % shenzhou
If the input prompt is text, a frontal image is initially generated conditioned on a depth map derived from the 3D geometry. This process involves rendering the 3D mesh into a depth map and utilizing depth-conditioned diffusion models \cite{zhang2023adding} to produce the frontal image. Alternatively, if the input is an image, we integrate the IP-Adapter \cite{ye2023ip} and ControlNet \cite{zhang2023adding} to generate the frontal image. As illustrated in Fig. \ref{fig:frontal}, both text and image prompts are converted into a geometry-aligned frontal image, which serves as the input for subsequent texture generation. 

\begin{figure*}
    \centering 
    \includegraphics[width=0.9\linewidth]{figures/texture/frontal.png}
    \caption[Frontal Image Generation.]
    {Both textual and visual prompts are transformed into a frontal image that is aligned with the frontal-view geometry.}
    \label{fig:frontal}
\end{figure*}


\subsection{Multi-view RGB Image Generation} % xibin
\label{sec:texture:multiview}
A single-view to multi-view image generator creates multi-view RGB images conditioned on the multi-view position maps and the frontal image. Please note that normal and depth maps can also be used here. We first train a multi-view image generator with a network structure similar with Zero123++~\cite{shi2023zero123++}, then, we combine the ControlNet~\cite{zhang2023adding} with Zero123++~\cite{shi2023zero123++} conditioned on the position (XYZ coordinate) maps, enabling the generation of position-aligned multi-view images. Whether the frontal image originates from text and depth or is provided as input, the multi-view image generator generates six multi-view images (each $512 \times 512$) starting from random Gaussian noise, with geometric conditions and photometric modules. An example is shown in the first image in Fig. \ref{fig:multiviewpbr}.

% \begin{figure*}
%     \centering 
%     \includegraphics[width=0.3\linewidth]{figures/texture/final_images_d2rgb.png}
%     \caption[Multi-view RGB image]
%     {Multi-view RGB image.}
%     \label{fig:multiviewrgb}
% \end{figure*}

\subsection{Multi-view PBR Image Generation} % xibin
% The generated multi-view RGB images are decomposed into multi-view PBR (Physically Based Rendering) images. This stage includes generating multi-view albedo, metallic, and roughness maps (each 512x512), utilizing multi-view depth, XYZ coordinate map, frontal images and previously estimated multi-view RGB images as additional conditioning inputs through photometric and geometric condition modules.

After obtaining multi-view rgb image conditioned on multi-view position maps, we generate the corresponding multi-view PBR (Physically Based Rendering) image via a image-2-image diffusion model~\cite{rombach2022high}. Specially, taking multi-view rgb image as input, we train image-2-image diffusion models to generate corresponding multi-view PBR image. This stage includes generating multi-view albedo, metallic, and roughness maps (each sub image with resolution of 512$\times$512). Please kindly note that we train two models in multi-view PBR image generation process, where one model estimate multi-view albedo image, and one model estimate multi-view metallic and roughness maps. Examples of the generated albedo, metallic and roughness images are shown in Fig. \ref{fig:multiviewpbr}.

\begin{figure*}
    \centering
    \includegraphics[width=0.24\linewidth]{figures/texture/final_images_d2rgb.png}
    \includegraphics[width=0.24\linewidth]{figures/texture/final_images_albedo.png}
    \includegraphics[width=0.24\linewidth]{figures/texture/final_images_matallic.png}
    \includegraphics[width=0.24\linewidth]{figures/texture/final_images_roughness.png}
    \caption[Multi-view Images]
    {Multi-view RGB, albedo, metallic and roughness images. }
    \label{fig:multiviewpbr}
\end{figure*}

\subsection{High-Resolution Refinement} % jiayu
To further enhance visual quality, we upscale the albedo multi-view images by several iterative upscaling steps. 
Firstly, we apply two steps of image repainting, utilizing a pre-trained SDXL~\cite{podell2023sdxl} model conditioned on albedo, depth, XYZ coordinate maps, and the frontal image to upscale the albedo multi-view images to resolutions of $768 \times 768$ and $1024 \times 1024$, introducing finer details. We then use Real-ESRGAN~\cite{wang2021real} to further enhances the multi-view textures to generate detailed high-resolution albedo maps ($3072 \times 3072$), see Fig. ~\ref{fig:refined_albedo}.

\begin{figure*}
    \centering 
    \includegraphics[width=0.65\linewidth]{figures/texture/upscaled_albedo.png}
    \caption[High-resolution albedo images]
    {High-resolution albedo images.}
    \label{fig:refined_albedo}
\end{figure*}

\subsection{Pixel-Wise Consistency Enforcement} % jiayu, zacheng
The multi-view generation stages may produce inconsistencies at the pixel level across different views. To address this, we implement a consistent scheduler similar to TexPainter~\cite{zhang2024texpainter}, which enforces pixel-wise consistency. Specifically, the multi-view textures are firstly baked onto the 3D mesh, and a Poisson system is solved to produce seamless textures. Then, multi-view images are re-rendered and resampled into each view, ensuring consistency across different views and lighting conditions. The resampled views are used as the updated $z_0$, which is plugged into the noise scheduler of the diffusion model similar to~\cite{zhang2024texpainter}. % jiayu

