\section{3D Shape Generation}
\label{sec:3dshape}

\subsection{3D Latent Space Diffusion} % weixuan, taizhang
The process begins by generating a 3D shape from a single image, multiple images, or a text prompt. This involves the following steps:
\begin{itemize}
    \item Variational Autoencoder (VAE): Compresses 3D geometries into a latent space, enabling efficient representation and processing.
    \item Diffusion Network: Generates latent representations conditioned on the input prompts. This network is adapted from CLAY~\cite{zhang2024clay} / Craftsman~\cite{li2024craftsman} / LAM3D~\cite{cui2024lam3d}, with modifications to improve the capacity and performance of the model. % taizhang
\end{itemize}

\subsubsection{Efficient 3D Geometry Autoencoder}
For 3D geometry compression model, we build upon CraftsMan~\cite{li2024craftsman}, which adopts structures introduced in  
3DShape2VecSet~\cite{zhang20233dshape2vecset} and Michelangelo~\cite{zhao2024michelangelo}. 
Furthermore, we leverage the multi-resolution training strategy proposed in CLAY~\cite{zhang2024clay}. 
This approach encodes 3D geometry into latent space by progressively sampling additional points from a 3D point cloud, which incrementally extends and refines the latent representation of the shape. Progressive sampling allows the model to focus on areas of higher geometric complexity, capturing both global structure and intricate details.
The primary goal of our VAE is to generate expressive latent embeddings that effectively guide the diffusion process in subsequent stages. To enhance the efficiency of this process, we propose a more advanced point sampling strategy. This method is designed to maximize the utility of the 3D point-cloud data by prioritizing points that contribute the most to capturing fine-grained features and spatial relationships.
This enhancement not only increases the model's capacity to handle large-scale data for improved scalability but also preserves the fine-grained details of 3D geometry.

The design options of our VAE are illustrated in Fig.~~\ref{fig:vae}. We employ the model structure introduced in 3DShape2VecSet as our base model. This approach involves embedding the input point cloud, augmented with normal information $X \in \mathbb{R}^{N \times 6} $, which is sampled from a mesh $M$, into a latent code using a learnable embedding function and a cross-attention encoding module:

\begin{equation}
Z = \varepsilon(X) = CrossAttn(q, PosEmb(X)) \;,
\end{equation}
where $q \in \mathbb{R}^{m \times d}$ represents a set of learnable queries that compress the sampled points into a latent embedding. The cross-attention mechanism ensures effective integration of geometric and positional features into the latent space. The VAE’s decoder is composed of successive self-attention layers followed by a cross-attention layer. The cross-attention layer maps the latent embeddings back into 3D geometry, enabling reconstruction:

\begin{equation}
D(Z, p) = CrossAttn(PosEmb(p), SelfAttn(Z)) \;,\label{eq:decoder}
\end{equation}
where $p$ denotes random query points in 3D space, these points query with the latent and output occupancy logits.
This base VAE implementation is illustrated in Fig.~\ref{fig:vae} (A).


Following the approach outlined in CLAY~\cite{zhang2024clay}, we adopt a multi-resolution training strategy to progressively upscale the model's capacity. Specifically, we incrementally increase the number of sampled points from 4096 to 32768 while simultaneously extending the latent embedding dimensionality from 256 to 2048. This progressive training scheme gradually introduces more detailed input information to the model, enabling it to capture finer geometric details. At the same time, the expanded latent embedding length increases the model’s capacity to represent complex features. Together, these enhancements enrich the latent space, thereby providing a more robust foundation for the subsequent diffusion model training. This multi-resolution approach ensures an efficient and scalable training process, optimizing both the model's performance and its ability to generalize across diverse 3D geometries.

% This multi-resolution approach ensures an efficient and scalable training process, optimizing both the model's performance and its ability to generalize across diverse 3D geometries.

% Such a training scheme gradationally increases the input information amount and analogously increases capacity of the latent embeddings, which can enrich the following diffusion model training.

Recall that the primary objective of our VAE is to generate expressive latent embeddings. While the previously mentioned approach progressively increases the number of sampling points, each object contains a total of 500k points, leaving many points unsampled. This results in inevitable information loss, as not all geometric details are captured in the latent representation.
Furthest point sampling~\cite{qi2017pointnet++} has the potential to mitigate this issue by selecting more representative points. However, this method is significantly slower compared to random sampling, making it less practical for large-scale training scenarios. This residual information loss can pose challenges during the diffusion process, as it may hinder the generation of high-quality latent embeddings. Consequently, the decoder is tasked with reconstructing fine-grained 3D details that might not be adequately represented in the latent embedding, potentially limiting the overall quality and fidelity of the reconstructed geometry.

% Recall that our VAE's objective is to produce expressive latent embeddings. 
% Although the aforementioned method gradationally increases sampling points amount, for each object we have in total 500k points so 
% there are still unsampled points that lead to inevitably information loss.
% Furthest point sampling~\cite{qi2017pointnet++} might remit the problem but it is significantly slower than random sampling.
% Such information loss may hinder the diffusion process from generating high-quality latent and require the decoder to reconstruct the fine-grained 3D details that might not exist in the latent embedding.

We have developed an enhancement to our Variational Autoencoder (VAE) model that allows it to operate without sampling, while still retaining all data points. A straightforward approach might involve utilizing PointNet++ \cite{qi2017pointnet++} to compress features from a point cloud into a few "centroid points" through the use of cascaded convolutional layers. However, this method demands a substantial amount of memory, especially when managing point cloud data consisting of millions of points. To address this, our model optimizes the processing of large-scale point clouds more efficiently, reducing the memory burden without compromising the integrity and richness of the data.
Alternatively, we opt to sample a set of centroid points  $M \in \mathbb{R}^{m \times 6}$ and employ a Q-former~\cite{li2022blip} style module to compress the raw point cloud data onto these centroids. The core component of the Q-former, the cross-attention mechanism, exhibits a computational complexity $O(2MNd)$, where $M$ is the amount of centroids and $N$ is the size of the input point cloud and $d$ is feature dimension.
Although utilizing a memory-efficient attention method such as FlashAttention~\cite{dao2022flashattention} helps, it remains resource-intensive and slow for processing large point clouds directly without sampling. To overcome these challenges, we propose the adoption of a linear attention mechanism~\cite{qin2024various,qin2023transnormerllm} for implementing cross-attention within our Q-former module.
The theoretical complexity of this approach is $O(Md^2+Nd^2)$. 
Given that $N>>N>>d$, the computational load of linear cross-attention is significantly reduced compared to traditional cross-attention methods. During the training of our VAE, we randomly select points from the original point clouds to serve as centroid points. These centroids, which have a dimension larger than 6, act as queries in the Q-former and compress geometric information from the raw point cloud. Subsequently, these centroid points are processed by the VAE encoder to generate latent embeddings.
This extended VAE is depicted in Fig.~\ref{fig:vae} (B). 

Similarly, we still adopt multi-reolustion training strategy to progressively increase the centroid points amount and latent embedding length to enlarge the latent embedding capacity. 
In addition, we empirically find that progressively increasing the training data amount can accelerate model convergence.
Methods like 3DShape2VecSet and CLAY, which derive latents from sampled points of the original point cloud, inevitably suffer from detail loss. 
Our extension effectively addresses this issue of information loss and maximizes the utilization of high-resolution point clouds, thereby preserving more detailed and accurate representations.

% We randomly sample
% In addition, inspired by 3Dshape2vecset~\cite{zhang20233dshape2vecset}, we want a set of proper centroid points that can 



\begin{figure*}
    \centering 
    \includegraphics[width=0.9\linewidth]{figures/diffusion/vae.pdf}
    \caption[3D Geometry variational autoencoder.]
    {3D Geometry variational autoencoder. (A): Our base VAE for 3D geometry compression. (B) Extended VAE for efficient 3D geometry compression.}
    \label{fig:vae}
\end{figure*}



\subsubsection{Diffusion Pipeline}

The diffusion pipeline is illustrated in Fig.~\ref{fig:diffusion:1}. We employ Multimodel Diffusion Transformer(MMDiT)~\cite{esser2024scaling} as our diffusion backbone, utilizing two pretrained models, specifically, CLIP-ViT-L/14~\cite{radford2021learning} as the global image feature extractor, and Dino-V2-Large~\cite{oquab2023dinov2} for local image feature extraction. Instead of employing DDPM, we utilize the flow matching schedule. Following CLAY~\cite{zhang2024clay} methodology, the diffusion model is trained in coarse-to-fine manner.

\begin{figure*}
    \centering 
    \includegraphics[width=0.9\linewidth]{figures/diffusion/diffusion.png}
    \caption[Diffusion pipline.]
    {Diffusion pipline. In the process of training a diffusion model, the DinoV2, CLIP, and VAE Decoder components are kept frozen}
    \label{fig:diffusion:1}
\end{figure*}

To enhance the image control effect, we use both global and local image feature as condition features of diffusion model. Global condition feature $z_{global} \in \mathbb{R}^L$ is extracted with ClIP vision encoder, meanwhile, local detail condition feature $z_{local} \in \mathbb{R}^{L \times 1024}$ is extracted with Dino vision encoder. The global condition and local condition are integrated into the diffusion model through MMDiT Block following Stable Diffusion 3~\cite{esser2024scaling}. The diffusion model we use has 2.3B parameters and 28 layers MMDiT block.

To enhance practical utility in 3D design workflows, we have extended our geometry generation framework to accept multi-view conditional inputs. This architectural advancement enables finer-grained geometric control through multi-view visual guidance. The system accommodates variable numbers of reference images (stochastically sampled from 1 to 4 views per instance) within a unified architecture, eliminating requirement for fixed-size input configurations. All synthesized geometries maintain spatial alignment with the primary view's coordinate system (defined by the first input image). Input images must be arranged in ascending azimuth order $\{\theta_1, \theta_2, \ldots, \theta_n\} \quad$ where $\quad \theta_i \in [0^\circ, 360^\circ)$. Multiview feature representations are aggregated through ordered concatenation along the sequence dimension, preserving relative spatial-semantic correspondence across views. Accelerated convergence is achieved via progressive transfer learning, where parameters initialized from our single-view conditioned model undergo fine-tuning using multiview datasets while maintaining pretrained backbone weights during initial phases.

\subsection{Meshing and UV Unwrapping} % zacheng
Once the 3D geometry is generated, it undergoes isosurface extraction, remeshing and UV unwrapping so that a texture-ready triangle mesh is produced.

\subsubsection{Isosurface extraction}
We perform a modified version of marching cubes algorithm~\cite{lorensen1998marching,nielson2003marching} to efficiently extract a watertight mesh from geometry tokens.

Marching cubes traditionally require a dense occupancy grid of $D\times D \times D$ occupancy values. Directly computing such a dense grid with Eq.~\eqref{eq:decoder} incurs a $O(D^3)$ time complexity that is prohibitively expensive at high resolutions $D$. To improve efficiency, we adopt a coarse to fine strategy: starting from a coarse grid resolution $d_0\ll D$, we iteratively build a sparse finer grid of resolution $d_{i+1}=2 d_i$ whose cells are subdivided from active cells in the coarser grid of resolution $d_i$ that are close to the isosurface. This strategy ensures that most occupancy queries of Eq.~\eqref{eq:decoder} are confined within a small margin around the isosurface and significantly reduces the number of queries required for isosurface extraction, achieving two to three orders faster mesh extraction. 

To guarantee a watertight mesh, at the highest level $d_n=D$, we expand the sparse active cells along the isosurface to eliminate holes and perform Lewiner's topology check~\cite{lewiner2003efficient} to ensure manifoldness. We implement the sparse marching cubes as a custom CUDA kernel function to maximize efficiency.

\subsection{Remesh and UV unwrap} 
The triangle meshes extracted from Marching cubes may contain poorly constructed elements such as collapsed faces or slivers. Furthermore, they often exhibit a high face count that could create problems for downstream applications. We overcome these issues with an optional remeshing step using either an off-the-shelf quad-remesher\footnote{\url{https://exoside.com/}} or isotropic remeshing~\cite{pietroni2009almost} followed by QEM triangle decimation~\cite{garland1997surface}. 

In addition, we use the open source project UV-Atlas~\cite{zhou2004iso} for UV charting and packing. At this point, we obtain a polygon mesh that is ready for texture generation.

\subsection{Alternative Approach: Artist-Created Meshes Generation} % weizhe
An alternative approach we explored involves directly generating the mesh, bypassing the initial generation of geometry as an implicit function followed by mesh extraction. This method effectively produces meshes with reasonable topology, akin to those crafted by artists for simple shapes. However, it encounters difficulties when applied to complex geometries, where maintaining structural integrity and topological accuracy becomes challenging.

\subsubsection{Mesh Compression}

Direct regression of vertex coordinates results in substantial memory consumption, which consequently limits the number of faces the model can handle. To mitigate this issue, we adopt the methodology proposed by BPT~\cite{weng2024scaling}, which involves compressing the original vertex coordinates using block index compression and patchified aggregation. Specifically, for a vertex $v_{i} = (x_i,y_i,z_i)$, the block-wise indexing $(b_i,o_i)$ is formulated as follows:
\begin{equation}\label{eq:bpt1}
\begin{split}
    b_{i} &= (x_i|O)\cdot B^{2} + (y_{i}| O)\cdot B + z_{i}|O\;, \\
    o_{i} &= (x_{i}\%O) \cdot O^{2} + (y_{i}\%O)\cdot O + z_{i}\%O\;.
\end{split}
\end{equation}
In this formulation, the symbols $|$ and $\%$ represent division without remainder and the modulo operation, respectively. This approach segments the coordinates along each axis into $B$ blocks, each of length $O$. To further enhance the compression ratio, we employ the patchified aggregation technique as described in~\cite{weng2024scaling}. This technique aggregates the faces connected to the same vertex into a non-overlapping patch and utilizes dual-block indices to denote the starting point of a patch. Consequently, the offset vocabulary is shared between the center vertex and the surrounding vertices. The center patch is formulated as follows:
\begin{equation}\label{eq:bpt2}
P_{c} = (b'_{c},o_{c},b_{1},o_{1},b_{2},o_{2},\dots,b_{n},o_{n})\;.
\end{equation}
In this context, $b'_{c}$ and $o_{c}$ denote the blocking index and the offset index of the center patch, respectively. These indices are critical for accurately referencing the spatial configuration of the patch within the compressed data structure.

To achieve this, we initially convert vertex coordinates into discrete values with a resolution of $R$. Subsequently, we encode the mesh information, including vertices and faces, into a discrete token sequence. This sequence can be decoded back into a mesh using the same technique. It is important to note that this encoding and decoding process is governed by predefined rules and does not involve any learnable parameters.

\subsubsection{Autoregressive Model for Mesh Generation}
In this section, we describe the methodology for generating novel shapes from various modalities using the compression technique outlined in the preceding sections. Fig.~\ref{fig:am:1} illustrates the pipeline of our approach. Initially, a mesh is encoded into discrete token sequences utilizing the method detailed previously. Subsequently, a decoder-only autoregressive model is employed to predict subsequent tokens based on preceding ones. To facilitate multi-modality condition control, a pre-trained condition encoder network is utilized to encode condition information, such as images, text, and point clouds, into latent features. These features serve as the contextual input for the decoder-only model. The resulting token sequence can then be decoded back into the final mesh using a mesh decoder. It is important to note that both the mesh encoder and mesh decoder are purely rule-based, as previously explained, and do not involve any learnable parameters.

\begin{figure*}
    \centering 
    \includegraphics[width=0.9\linewidth]{figures/AM/am.png}
    \caption[Pipeline for Artist-Created Meshes Generation.]
    {Pipeline for Artist-Created Mesh Generation. Initially, meshes are encoded into discrete token sequences. These sequences are then processed through a decoder-only autoregressive model that utilizes a Transformer network architecture. To enforce multi-modality condition control, a pretrained condition encoder network is employed. This network effectively integrates diverse modalities, ensuring that the generated meshes adhere to specified conditions.}
    \label{fig:am:1}
\end{figure*}

Fig.~\ref{fig:am:2} visualizes the meshes generated by our model, which exhibit superior topology consistency with a minimal number of faces.


\begin{figure*}
    \centering 
    \includegraphics[width=0.9\linewidth]{figures/AM/am4.png}
    \caption[Example Meshes Generated by Our Artist-Created Meshes Generation Model.]
    {Example Meshes Generated by Our Artist-Created Mesh Generation Model. The meshes produced by our model demonstrate superior performance in maintaining topological consistency, showcasing the effectiveness of our approach in generating high-quality artistic meshes.}
    \label{fig:am:2}
\end{figure*}