


\section{3D Model Data Processing} % senbo
\label{sec:datasets}
We collect assets (mostly 3D models) from multiple sources and preprocess them to be training compatible, including converting mesh geometry to discrete sampling of implicit functions, multi-view image generation, and PBR rendering.
Our data processing pipeline is demonstrated in Fig.~\ref{fig:dataset:1}.
We will detail each step in the processing pipeline in the following contents of this section.

\begin{figure*}
    \centering 

    \includegraphics[width=0.9\linewidth]{figures/dataset/process_pipeline.png}
    \caption[Data processing pipline.]
    {Data processing pipline. The procedures marked in red are one-off implementations, while the green-boxed elements demand tailored development according to the algorithmic modules deployed on the dataset, we thereby provide exampled steps for jobs described in Section~\ref{sec:3dshape} and Section~\ref{sec:texture:multiview}.}
    \label{fig:dataset:1}
\end{figure*}

\subsection{Data Origins}
\label{sec:datasets:origin}

The main data sources are:
\begin{itemize}
    \item Objaverse~\cite{deitke2023objaverse}, which is a large open-source 3D object dataset with more than $700k$ objects, we use roughly $600k$ of it;
    \item OXL~\ie Objaverse-xl~\cite{deitke2024objaverse}, which is an extension of the previous dataset Objaverse~\cite{deitke2023objaverse}, we use roughly $200k$ of it;
    \item ABO~\cite{collins2022abo}, we only use the $8k$ 3D objects in this dataset;
    \item BuildingNet~\cite{selvaraju2021buildingnet}, which contains $2k$ building models;
    \item HSSD~\cite{khanna2023hssd}, which contains roughly $13k$ object models;
    \item Toy4k~\cite{stojanov2021using}, which contains roughly $4k$ object models.
    \item Some models downloaded from the Internet, for instance, polygone dataset\footnote{\url{https://polygone.art/}}.
    \item $10k$ private data provided by our partners.
\end{itemize}


\subsection{Filter Mesh}
\label{sec:datasets:preprocess:filter}
We first filter mesh using the following mesh proprieties, which are modified from the standards used in MeshXL~\cite{chen2024meshxl}:
\begin{itemize}
    \item Mesh face number, we only use meshes which face number is between $500$ and $80k$;
    \item Material number, we ignore all meshes with more than $100$ materials; material number is defined by total material number in blender\footnote{\url{https://www.blender.org/}}.
    \item Annotations of the dataset. We use annotations of Objaverse to remove scanned objects. Exampled images of scanned objects can be found in Fig.~\ref{fig:dataset:2}.
    Scanned objects are harmful to the overall 3D generative model training process in various ways:
    \begin{itemize}
        \item From (B) and (D) of Fig.~\ref{fig:dataset:2} we notice that most scanned objects possess a large number of fragmented faces;
        meshes containing such a large number of faces can only use the automatic method of unwrapping the mesh, which requires extremely large texture images to maintain reasonable mesh appearance. Hence, it's very slow in rendering.
        \item Some scanned objects do not possess an actual ``body'', like (C) in Fig.~\ref{fig:dataset:2}. 
        It's only a thin layer and not suitable for training multiple view diffusion methods like what we discuss in Section~\ref{sec:texture}.
    \end{itemize}
    \item Objects cannot be of pure color~\ie pure red or pure blue. This can be checked by checking object's material graph in Blender.
\end{itemize}

\begin{figure}
    \centering 

    \includegraphics[width=0.9\linewidth]{figures/dataset/scanned_object_image.png}
    \caption[Scanned objects.]
    {Typical images of scanned objects in Objaverse dataset~\cite{deitke2023objaverse}. 
    (A) and (C) are rendered images of the two objects; 
    (B) and (D) are corresponding object demonstration in blender.}
    \label{fig:dataset:2}
\end{figure}



\subsection{Format Conversion}
\label{sec:datasets:preprocess:conversion}

For convenience of usage, we convert all 3D mesh formats to OBJ\footnote{\url{https://en.wikipedia.org/wiki/Wavefront_.obj_file}}. 
This is because most processing pipelines that are not part of DCC software~\ie Digital Content Creation software, cannot support read full information of complex 3D formats like GLB\footnote{\url{https://www.khronos.org/gltf/}} and FBX\footnote{\url{https://www.autodesk.com/products/fbx/overview}}. 
If we wish to scale-up for future learning-based algorithms that need to directly read information from meshes, 
we have to convert 3D format to OBJ. 
However, directly converting 3D models to OBJ format often fails, 
mostly because default format conversion function in Blender cannot correctly deal with file path of texture images. 
We thus need some extra care of some certain file types.
\begin{itemize}
    \item \textbf{MAX to OBJ} cannot be done directly, as MAX file is only supported by 3DSMax\footnote{\url{https://www.autodesk.com/products/3ds-max/overview}} and OBJ exporting function in that software cannot correctly handle objects with multiple complex materials. 
    This is because we use an extension of OBJ that supports PBR formats developed by Carla~\cite{dosovitskiy2017carla}, which is not properly supported in 3DSMax. 
    We thus first convert MAX files to FBX formats using 3DSMax, and convert FBX to OBJ using blender. 
    \item \textbf{GLB to OBJ} can be done in blender, but to get correct textures extra care is needed in rebuilding material graph structure. 
    This is because GLB specification has embedded texture images within mesh files.
    We first convert all GLB files to GLTF formats which extract texture files to disk; 
    then we go through the material graph of GLTF format and rebuild connections in new mesh.  
    \item \textbf{PMX to OBJ} can be done in blender using codes derived from Cats plugin\footnote{\url{https://github.com/absolute-quantum/cats-blender-plugin}}. PMX\footnote{\url{https://gist.github.com/felixjones/f8a06bd48f9da9a4539f}} is often used by some creators of the anime industry.
\end{itemize}


\begin{figure}
    \centering 

    \includegraphics[width=0.9\linewidth]{figures/dataset/mesh_classification_process.png}
    \caption[Mesh classification.]
    {Classify mesh using vLLM by making the vLLM model to describe the model using $9$ rendered images. Some parts of the structured data (marked with red box) can be used to classify mesh; the aggregated full sentence can be used as the caption of the mesh.}
    \label{fig:dataset:3}
\end{figure}


\subsection{Classify Mesh}
\label{sec:datasets:preprocess:classify}

The propose of this step is to eliminate low-quality mesh as thoroughly as possible, and to mark object of distinctive types. 
This provides the following advantages:
\begin{itemize}
    \item Low-quality mesh can disturb the overall training process. 
    \item Labeling mesh with its class allows us to fine-tune diffusion model on data from certain domains.
\end{itemize}
The filtering process descirbed in this section is modified from the process used in MeshXL~\cite{chen2024meshxl}, as shown in Fig.~\ref{fig:dataset:3}. 
After this process, we render $9$ images surrounding the mesh, 
and use the HunYuan vision model\footnote{\url{https://cloud.tencent.com/document/product/1729/104753}} to filter the mesh by these rendered images. 
Note that you can substitute this vision model with any other vLLM models, such as GPT-4V\footnote{\url{https://openai.com/index/gpt-4v-system-card/}}.
As shown in Fig.~\ref{fig:dataset:3}, the text prompt guides the vLLM to describe the appearance of the object, and check if the object is of certain classes. 
Then, we use LLM~\cite{sun2024hunyuan} to convert unstructured text into a structured format with structures similar to JSON.
Labels in these structured texts can be used as the object's class.


\subsection{Generate watertight mesh}
\label{sec:datasets:preprocess:filter}

Generating watertight mesh, which is essential for 3D-DiT training, is generating an envelope tightly conforming to the model's exterior geometry, 
rather than using both interior and exterior geometry. 
The latter, like Manifold~\cite{huang2018robust,huang2020manifoldplus} and other works~\cite{portaneri2022alpha} that can provide similar results, is not suitable for 3D-DiT training.

We use similar watertight mesh generation method from 3DShape2VecSet ~\cite{zhang20233dshape2vecset}, 
which is adopted from Stutz's work~\cite{stutz2020learning} and used in occupancy networks~\cite{mescheder2019occupancy}\footnote{We directly use implementation from \url{https://github.com/autonomousvision/occupancy_networks/}.}.
The method is based on TSDF fusion of a group of depth map rendering around the object.
However, this method will slightly vary the exterior shape of the mesh due to the following reasons:
\begin{itemize}
    \item The method calculates closing of each depth map, which fills small gaps on depth image but remove some details. Distortion caused by such reason can be reduced by increasing the definition of depth image, as closing of the image is often calculated using fixed window size.
    \item The method applies a bias that is half voxel size. Distortion caused by such reason can be reduced by high-definition SDF volume. 
\end{itemize}
However, increasing the definition of the SDF volume will lead to a mesh with particularly large face number, which requires the pipeline to provide a decimated mesh. 
It's worth noticing that most decimating methods in DCC software uses QEM triangle decimation~\cite{garland1997surface}, which will sometimes provide ill-formed faces \ie the shape of the face may not be suitable for further developments.
We therefore also recommend using ACVD~\cite{audette2011approach,valette2008generic,valette2004approximated} in the decimating step.
We have also developed a baking tool based on baking function in Blender to provide texture for watertight mesh.


\subsection{Rendering and Sampling}

All meshes are normalized to a tightly coupled (radius is 1) bounding sphere using Welzl's algorithm~\cite{welzl2005smallest}. We prefer not to use bounding boxes because arbitrarily rotating objects within them may cause the objects to extend beyond the confines of the box. 
Rendering is done in Blender using EEVEE\footnote{\url{https://docs.blender.org/manual/en/latest/render/eevee/introduction.html}} renderer, while sampling is done using trimesh\footnote{\url{https://trimesh.org/}}. 
We sample three groups of points, each with $500k$ points, which is similar to sampling approach used by For 3D geometry compression model, we build upon CraftsMan~\cite{li2024craftsman}, who adopts structures introduced in 3DShape2VecSet ~\cite{zhang20233dshape2vecset}:
\begin{itemize}
    \item We perform uniformly random sampling within the circumscribed cube of the bounding sphere, yielding SPACE points.
    \item We sample a group of points just on surface of the watertight mesh, yielding SURFACE points. We compute Gaussian curvature of each vertex on the mesh and use this curvature as importance of each area on the mesh: more points will be sampled on areas with higher curvature. 
    \item We perform uniformly random sampling on the surface of the mesh, and add a small bias to coordinates of each sampled point, yielding NEAR-SURFACE points.
\end{itemize}


\section{Experiments}

% \subsection{Quantitative Results}

% \subsection{Qualitative Results}

Fig.~\ref{fig:image-2-mesh-res} and Fig.~\ref{fig:text-2-mesh-res} illustrate the results of 3D generation with the prompt and the image as input. As shown in these figures, which shows that our Pandora3D system could faithfully recover the shape and texture with fine-grained details and produce neat space without any floaters.

\begin{figure*}
    \centering 
    \includegraphics[width=\linewidth]{figures/texture/image-2-mesh-res-01.png}
    \caption[3D generation with image as input]
    {Visual results with color image as input, the green areas show the rendered multi-view images without textures and the red areas show the rendered multi-view images with textures.}
    \label{fig:image-2-mesh-res}
\end{figure*}

\begin{figure*}
    \centering 
    \includegraphics[width=\linewidth]{figures/texture/text-2-mesh-res-01.png}
    \caption[3D generation with prompt as input]
    {Visual results with prompt as input, the green areas show the rendered multi-view images without textures and the red areas show the rendered multi-view images with textures.}
    \label{fig:text-2-mesh-res}
\end{figure*}
