\input{table/evaluation/ablation/features}
\section{EVALUATION}
We evaluate PerfSeer by addressing the following research questions (RQs).

\textbf{RQ1:} How effective are the selected features and optimization components? 

\textbf{RQ2:} How does SeerNet compare with baseline models? 

\textbf{RQ3:} How effective is the multi-metric performance prediction model, SeerNet-Multi? 

\textbf{RQ4:} What is the application scope and overhead of PerfSeer? 

\subsection{Evaluation Setup}
\subsubsection{Training Settings.}\label{sec:train-setting}
The dataset is divided into 2:1:1 for training, validation, and testing. We use a batch size of 128 and an initial learning rate of 1e-3, halving it after five epochs without improvement, down to 1e-6. Training runs for up to 500 epochs, with Mean Squared Error (MSE) as the loss function and Adam as the optimizer.

\subsubsection{Evaluation Metrics.}\label{sec:metrics}
To ensure consistency across metrics with varying value ranges, we use percentage errors. The metrics include MAPE, Root Mean Square Percentage Error (RMSPE), and accuracy within a relative error of \(x\%\) (x\%Acc) \cite{Brp-nas}.

%% SeerNet
\subsection{Ablation Study (RQ1)}\label{sec:ablation}
The accuracy of the performance prediction depends on both representation and prediction. We conducted an ablation study to validate the features selection and construction (\emph{representation}) and the prediction model design (\emph{prediction}).
\subsubsection{Ablation Study of Features.} \label{sec:features ablation}
From the results shown in Table \ref{tab:ablation of features}, we observe the following:

\emph{Features Importance.} 
Node features are much more important than global features, which are in turn more important than edge features. 
The mean MAPE of SeerNet decreases from 58.23\% to 28.41\% as the node features are used increasingly, as they directly correspond to each operation node.
Adding the global features further improves accuracy, significantly reducing the mean MAPE from 25.40\% to 7.41\%, as they provide an overall representation of the model.
Edge features, though less important, improved MAPE slightly from 28.41\% to 25.40\% by providing additional memory access information, further enhancing prediction accuracy.

\emph{Feature Selection.} 
Each group of the node, edge, and global features we selected improved the performance prediction accuracy, demonstrating their relevance to model performance.
In contrast, node categories, which are commonly used in other predictors, reduced accuracy. We consider SeerNet extracts category-related information from other semantically rich features, so we excluded node categories from our feature set.

\emph{Feature Construction.} 
We constructed several unique and effective features. 
For node features, we included arithmetic intensity and computation/memory access proportions, reducing the mean MAPE from 35.41\% to 28.41\%. 
For global features, we introduced graph profiles, computation and memory access trend statistics, tensor size delivered per-edge, and arithmetic intensity, reducing the mean MAPE from 25.40\% to 7.53\%. 
These features, not used by any previous predictors, significantly enhance prediction accuracy.

\subsubsection{Ablation Study of Components.}\label{sec:ablation component}
From the results shown in Table \ref{tab:ablation of components}, we observe the following:

\emph{SynMM.}
Using SynMM to aggregate the node features reduces the mean MAPE from 7.41\% to 6.10\%. This demonstrates that SynMM combines max and mean aggregation to create a more comprehensive and robust representation, thereby enhancing prediction accuracy.

\emph{GNPB.}
With the introduction of GNPB, the mean MAPE further decreases from 6.10\% to 5.14\%. This result proves that GNPB enables complementary learning between the node and global features, enriching both perspectives and further improving prediction accuracy.
% The experiments verify that the optimization components can capture the performance information of a model effectively.

\input{table/evaluation/ablation/models}
\input{table/evaluation/compare/ourdataset}
\input{table/evaluation/compare/nnmdataset}
\subsection{Baseline Comparison (RQ2)}\label{sec:seernet compare}
\subsubsection{Baseline.} 
We compare SeerNet with the following methods:
% MLP-Node
(i) MLP-Node uses an MLP to predict model performance, concatenating features from all nodes in the graph and handling variable node numbers by padding or truncating them to a fixed size.
% PMGNS
(ii) PMGNS \cite{DIPPM} (described in Section \ref{sec:related work}) utilizes a single prediction head for predicting one performance metric.
% Eagel
(iii) Eagle \cite{Brp-nas} (described in Section \ref{sec:related work}) is implemented for cell-based models, but for non-cell-based models, it uses features from PMGNS (Eagle-p) and our features from SeerPerf (Eagle-s).
% nn-Meter
(iv) nn-Meter \cite{Nn-meter} (described in Section \ref{sec:related work}) is a kernel-based predictor.

\subsubsection{Performance Comparison.}
\emph{Baseline Comparison on our Dataset.}
Table \ref{tab:comparison our} shows that SeerNet has the smallest parameter size (1.02M) and the highest accuracy, with a mean MAPE of 5.14\%.
Compared to SeerNet,
MLP-Node contains more than four times the parameters and achieves a MAPE over three times higher, while PMGNS contains more than three times the parameters and has a MAPE nearly twice as high.
Eagle-p performs poorly, with a MAPE of 74.43\%, due to the inability of PMGNS features to accurately represent the models in our dataset.
Eagle-s, which uses our proposed features, performs better but still lags behind SeerNet.
Both PMGNS and Eagle-s outperform MLP-Node, achieving higher accuracy with fewer parameters, highlighting the ability of GNNs to capture execution dependencies.
Eagle-s also performs better than Eagle-p, demonstrating the effectiveness of our proposed features. SeerNet outperforms the other models, offering the best representation and prediction.

\emph{Baseline Comparison on the Dataset of nn-Meter.} % Comparison with nn-Meter
Table \ref{tab:comparison nnm} shows SeerNet outperforms nn-Meter with half the RMSPE, 21\% higher at 5\%Acc, and 5\% higher at 10\%Acc.
For models like VGG, ResNet, and AlexNet on the CPU, SeerNet is slightly less accurate than nn-Meter, likely due to the simple structures of these models and their predictable execution patterns on the CPU.
On VPU, nn-Meter achieves 50.6\% at 5\%Acc, while SeerNet reaches 85.7\%, 35\% higher. 
This is because the execution of VPU is more complex, and nn-Meter fails to design effective detection functions. 

\input{table/evaluation/seernet_multi}
\input{table/evaluation/discussion/data_dependency}

\subsection{Effectiveness of SeerNet-Multi (RQ3)}\label{sec:eva seernet multi}
Table \ref{tab:comparison nnm} shows that PMGNS-Multi (PMGNS with multiple prediction heads) performed poorly with a MAPE of 50.5\%, while SeerNet-Multi (without PCGrad) had a MAPE of 15.85\%, indicating that predicting multiple metrics simultaneously reduces accuracy.
%
However, with PCGrad, SeerNet-Multi halved its MAPE from 15.85\% to 7.75\% without increasing parameter overhead. This demonstrates that PCGrad effectively mitigates conflicting gradient directions across tasks, enabling SeerNet-Multi to predict multiple metrics efficiently with minimal accuracy loss.
%
Furthermore, SeerNet-Multi has about one-third the parameters of SeerNet, with only a 2.61\% increase in MAPE, balancing parameter efficiency and prediction accuracy. This makes SeerNet-Multi ideal for scenarios requiring rapid predictions with limited resources and lower accuracy demands.

\subsection{Further Discussion (RQ4)}\label{sec:extension experiments}
\subsubsection{Application Scope.}
\emph{Multi-Model, Multi-Metric Support.}
SeerPerf provides accurate predictions for execution time, memory usage, and SM utilization during both training and inference across various architectures, including GoogLeNet, VGG, ResNe(X)t, MobileNet, and DenseNet.

\emph{Multi-Device Support.}
PerfSeer provides accurate predictions across various devices, including mobile CPUs, mobile GPUs, desktop GPUs, and Intel VPUs, as shown in Table \ref{tab:comparison nnm}. In contrast, nn-Meter exhibits poor prediction accuracy on Intel VPUs.

\emph{Multi-Platform Support.}
The representation of PerfSeer is based on ONNX, so SeerPerf supports any DL framework convertible to ONNX.
\emph{Overall}, our performance predictor, PerfSeer, has a wide application scope, making it suitable for most common applications.

\subsubsection{Overhead.}
\emph{Data dependency and deployment overhead.}
To evaluate the data dependency of SeerNet, we analyze the relationship between dataset scale and prediction accuracy, keeping the test set size fixed. 
Results (Table \ref{tab:data dependency}) show that accuracy decreases as the dataset scale shrinks. Nevertheless, SeerNet achieves a mean MAPE of 14.98 with only 1,000 samples, demonstrating low data dependency.
The deployment overhead includes 16.67 GPU hours for data collection and 0.05 GPU hours for training per 1,000 samples, resulting in low deployment overhead.

\emph{Usuage overhead.}
We evaluated the overhead of SeerPerf on an Intel i7-11700 CPU, which includes representation and prediction. 
The average representation latency is 248 ms, with prediction latencies of 2.0 ms for SeerNet and 2.1 ms for SeerNet-Multi. The total overhead of approximately 250 ms is acceptable for most applications. \emph{Overall}, SeerPerf demonstrates low overhead in both deployment and usage.

% \emph{The experiments conducted in this section conclude that PerfSeer is an efficient and accurate performance predictor, characterized by its broad application scope, low construction and usage overhead, and high prediction accuracy.}
