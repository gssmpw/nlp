\input{figure/1-overview}
\section{Related Work} \label{sec:related work} 
\paragraph{Global Features-Based Predictor.} 
Liquid \cite{Liquid} and Horus \cite{Horus} use traditional models, such as Random Forest (RF) \cite{randomforest} and XGBoost \cite{Xgboost}, to predict GPU utilization and memory usage during model training by inputting global features like batch size, FLOPs, and parameter size. However, these methods are limited as they fail to capture internal execution details, and different models can exhibit vastly different performance, even with the same global features.

\paragraph{Operations/Unit-Wise Predictor.}
Justus et al. \cite{justus} predict the execution time of training models on edge devices by predicting the execution time of each operation and aggregating them. They use hardware, operations, and specific features as inputs to an MLP. nn-Meter \cite{Nn-meter} splits the model inference into kernels, predicting the execution time of each by feeding features like hyper-parameters, FLOPs, and input/output shapes into an RF model. This execution time is aggregated to get the total execution time. However, this approach relies on heuristic detection functions that require a deep understanding of execution details across different models and devices, which can be costly and inaccurate, leading to inaccurate predictions.

\paragraph{GNN-Based Predictor.}
BRP-NAS (Eagle) \cite{Brp-nas} uses a Graph Convolutional Network (GCN) \cite{GCN}-based model to predict execution time and accuracy for inference models, applied to NAS on NAS-Bench-201 \cite{nas-bench}. 
DNNPerf \cite{dnnperf} uses node and edge features in a custom Graph Attention Network (GAT) \cite{GAT}-based model to predict execution time and memory usage for training models. 
DIPPM \cite{DIPPM} feeds node and global features into a Graph Sample and Aggregation (GraphSAGE) \cite{GraphSage}-based model (PMGNS), which generates embeddings and uses multiple prediction heads to predict execution time, memory usage, and energy consumption for inference models. 
This approach captures execution details by learning both the topology and node features of models, ensuring prediction accuracy.
