% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{parmar-etal-2024-logicbench,
    title = "{L}ogic{B}ench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
    author = "Parmar, Mihir  and
      Patel, Nisarg  and
      Varshney, Neeraj  and
      Nakamura, Mutsumi  and
      Luo, Man  and
      Mashetty, Santosh  and
      Mitra, Arindam  and
      Baral, Chitta",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.739/",
    doi = "10.18653/v1/2024.acl-long.739",
    pages = "13679--13707",
}


@inproceedings{patel-etal-2024-multi,
    title = "Multi-{L}ogi{E}val: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
    author = "Patel, Nisarg  and
      Kulkarni, Mohith  and
      Parmar, Mihir  and
      Budhiraja, Aashna  and
      Nakamura, Mutsumi  and
      Varshney, Neeraj  and
      Baral, Chitta",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1160/",
    doi = "10.18653/v1/2024.emnlp-main.1160",
    pages = "20856--20879"
}


@inproceedings{yu2020reclor,
        author = {Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},
        title = {ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning},
        booktitle = {International Conference on Learning Representations (ICLR)},
        month = {April},
        year = {2020}
}

% article{kim2024mechanistic,
%  title={A mechanistic interpretation of syllogistic reasoning in auto-regressive language models},
  author={Kim, Geonhee and Valentino, Marco and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2408.08590},
  year={2024}
%}

@inproceedings{wu-etal-2023-hence,
    title = "Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning",
    author = "Wu, Yongkang  and
      Han, Meng  and
      Zhu, Yutao  and
      Li, Lei  and
      Zhang, Xinyu  and
      Lai, Ruofei  and
      Li, Xiaoguang  and
      Ren, Yuanhang  and
      Dou, Zhicheng  and
      Cao, Zhao",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.148/",
    doi = "10.18653/v1/2023.findings-acl.148",
    pages = "2347--2367",
}

@misc{tulchinskii2024listeningwisefewselectandcopy,
      title={Listening to the Wise Few: Select-and-Copy Attention Heads for Multiple-Choice QA}, 
      author={Eduard Tulchinskii and Laida Kushnareva and Kristian Kuznetsov and Anastasia Voznyuk and Andrei Andriiainen and Irina Piontkovskaya and Evgeny Burnaev and Serguei Barannikov},
      year={2024},
      eprint={2410.02343},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02343}, 
}

@misc{dutta2024thinkstepbystepmechanisticunderstanding,
      title={How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning}, 
      author={Subhabrata Dutta and Joykirat Singh and Soumen Chakrabarti and Tanmoy Chakraborty},
      year={2024},
      eprint={2402.18312},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.18312}, 
}


@article{feng2024towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{ChainOfThought,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@misc{ProofWriter,
      title={ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language}, 
      author={Oyvind Tafjord and Bhavana Dalvi Mishra and Peter Clark},
      year={2021},
      eprint={2012.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.13048}, 
}


@inproceedings{morishita_2024_NeurIPS_FLD_diverse,
  title={Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus}, 
  author={Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2024}
}


@inproceedings{morishita2023fld,
  title = {Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic},
  author = {Morishita, Terufumi and Morio, Gaku and Yamaguchi, Atsuki and Sogawa, Yasuhiro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  year = {2023}
}



@article{han2022folio,
  title={FOLIO: Natural Language Reasoning with First-Order Logic},
  author = {Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Benson, Luke and Sun, Lucy and Zubova, Ekaterina and Qiao, Yujie and Burtell, Matthew and Peng, David and Fan, Jonathan and Liu, Yixin and Wong, Brian and Sailor, Malcolm and Ni, Ansong and Nan, Linyong and Kasai, Jungo and Yu, Tao and Zhang, Rui and Joty, Shafiq and Fabbri, Alexander R. and Kryscinski, Wojciech and Lin, Xi Victoria and Xiong, Caiming and Radev, Dragomir},
  journal={arXiv preprint arXiv:2209.00840},
  url = {https://arxiv.org/abs/2209.00840},
  year={2022},
}
@inproceedings{Seals2023EvaluatingTD,
  title={Evaluating the Deductive Competence of Large Language Models},
  author={S. M. Seals and Valerie L. Shalin},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2023},
}

@inproceedings{wan-etal-2024-logicasker,
    title = "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    author = "Wan, Yuxuan  and
      Wang, Wenxuan  and
      Yang, Yiliu  and
      Yuan, Youliang  and
      Huang, Jen-tse  and
      He, Pinjia  and
      Jiao, Wenxiang  and
      Lyu, Michael",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.128/",
    doi = "10.18653/v1/2024.emnlp-main.128",
    pages = "2124--2155"
}


@inproceedings{Yang2024DoLL,
  title={Do Large Language Models Latently Perform Multi-Hop Reasoning?},
  author={Sohee Yang and Elena Gribovskaya and Nora Kassner and Mor Geva and Sebastian Riedel},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
}

@inproceedings{proplogic,title	= {How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis},author	= {Guanzhe Hong and Nishanth Dikkala and Enming Luo and Cyrus Rashtchian and Xin Wang and Rina Panigrahy},year	= {2024},URL	= {https://arxiv.org/abs/2411.04105},booktitle	= {The 4th Workshop on Mathematical Reasoning and AI @ NeurIPS 2024}}


@inproceedings{
marks2024the,
title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
author={Samuel Marks and Max Tegmark},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=aajyHYjjsk}
}

@article{prontoqa,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  journal={arXiv preprint arXiv:2210.01240},
  year={2022},
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@inproceedings{
wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}


@article{attention_heads_survey,
  title={Attention heads of large language models: A survey},
  author={Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Yang, Mingchuan and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
  journal={arXiv preprint arXiv:2409.03752},
  year={2024}
}


@misc{selection-inference,
      title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning}, 
      author={Antonia Creswell and Murray Shanahan and Irina Higgins},
      year={2022},
      eprint={2205.09712},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2205.09712}, 
}


@inproceedings{logiqa,
  title     = {LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
  author    = {Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {3622--3628},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/501},
  url       = {https://doi.org/10.24963/ijcai.2020/501},
}

@inproceedings{bao2022multi,
  title={Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation},
  author={Qiming Bao and Alex Yuxuan Peng and Tim Hartill and Neset Tan and Zhenyun Deng and Michael Witbrock and Jiamou Liu},
  booktitle={Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning \& Reasoning (IJCLR 2022)},
  pages={202-217},
  month=sep,
  year={2022},
  address={Cumberland Lodge, Windsor Great Park, United Kingdom},
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{lieberum2023does,
  title={Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla},
  author={Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  journal={arXiv preprint arXiv:2307.09458},
  year={2023}
}

@article{yu2024correcting,
  title={Correcting negative bias in large language models through negative attention score alignment},
  author={Yu, Sangwon and Song, Jongyoon and Hwang, Bongkyu and Kang, Hoyoung and Cho, Sooah and Choi, Junhwa and Joe, Seongho and Lee, Taehee and Gwon, Youngjune L and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2408.00137},
  year={2024}
}

@article{kim2024mechanistic,
  title={A mechanistic interpretation of syllogistic reasoning in auto-regressive language models},
  author={Kim, Geonhee and Valentino, Marco and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2408.08590},
  year={2024}
}

@inproceedings{NEURIPS2024_d6df31b1,
 author = {Yao, Yunzhi and Zhang, Ningyu and Xi, Zekun and Wang, Mengru and Xu, Ziwen and Deng, Shumin and Chen, Huajun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {118571--118602},
 publisher = {Curran Associates, Inc.},
 title = {Knowledge Circuits in Pretrained Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d6df31b1be98e04be48af8bedb95b499-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{zhangtowards,
  title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  author={Zhang, Fred and Nanda, Neel},
  booktitle={The Twelfth International Conference on Learning Representations},
  year = {2024}
}

@inproceedings{todd2024function,
  title={Function Vectors in Large Language Models},
  author={Todd, Eric and Li, Millicent and Sharma, Arnab and Mueller, Aaron and Wallace, Byron C and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2024},
  organization={ICLR}
}

@article{
  PrOntoQAOOD,
  title={Testing the General Deductive Reasoning Capacity of Large Language Models Using {OOD} Examples},
  author={Abulhair Saparov and
          Richard Yuanzhe Pang and
          Vishakh Padmakumar and
          Nitish Joshi and
          Seyed Mehran Kazemi and
          Najoung Kim and
          He He},
  journal={CoRR},
  volume={abs/2305.15269},
  year={2023},
  url={https://doi.org/10.48550/arXiv.2305.15269},
  doi={10.48550/arXiv.2305.15269},
  eprinttype={arXiv},
  eprint={2305.15269},
}