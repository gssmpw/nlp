[
  {
    "index": 0,
    "papers": [
      {
        "key": "ChainOfThought",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "Yang2024DoLL",
        "author": "Sohee Yang and Elena Gribovskaya and Nora Kassner and Mor Geva and Sebastian Riedel",
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?"
      },
      {
        "key": "Seals2023EvaluatingTD",
        "author": "S. M. Seals and Valerie L. Shalin",
        "title": "Evaluating the Deductive Competence of Large Language Models"
      },
      {
        "key": "wan-etal-2024-logicasker",
        "author": "Wan, Yuxuan  and\nWang, Wenxuan  and\nYang, Yiliu  and\nYuan, Youliang  and\nHuang, Jen-tse  and\nHe, Pinjia  and\nJiao, Wenxiang  and\nLyu, Michael",
        "title": "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
        "title": "A Mathematical Framework for Transformer Circuits"
      },
      {
        "key": "olah2020zoom",
        "author": "Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan",
        "title": "Zoom in: An introduction to circuits"
      },
      {
        "key": "attention_heads_survey",
        "author": "Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Yang, Mingchuan and Tang, Bo and Xiong, Feiyu and Li, Zhiyu",
        "title": "Attention heads of large language models: A survey"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lieberum2023does",
        "author": "Lieberum, Tom and Rahtz, Matthew and Kram{\\'a}r, J{\\'a}nos and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir",
        "title": "Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla"
      },
      {
        "key": "yu2024correcting",
        "author": "Yu, Sangwon and Song, Jongyoon and Hwang, Bongkyu and Kang, Hoyoung and Cho, Sooah and Choi, Junhwa and Joe, Seongho and Lee, Taehee and Gwon, Youngjune L and Yoon, Sungroh",
        "title": "Correcting negative bias in large language models through negative attention score alignment"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kim2024mechanistic",
        "author": "Kim, Geonhee and Valentino, Marco and Freitas, Andr{\\'e}",
        "title": "A mechanistic interpretation of syllogistic reasoning in auto-regressive language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhangtowards",
        "author": "Zhang, Fred and Nanda, Neel",
        "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"
      },
      {
        "key": "todd2024function",
        "author": "Todd, Eric and Li, Millicent and Sharma, Arnab and Mueller, Aaron and Wallace, Byron C and Bau, David",
        "title": "Function Vectors in Large Language Models"
      },
      {
        "key": "NEURIPS2024_d6df31b1",
        "author": "Yao, Yunzhi and Zhang, Ningyu and Xi, Zekun and Wang, Mengru and Xu, Ziwen and Deng, Shumin and Chen, Huajun",
        "title": "Knowledge Circuits in Pretrained Transformers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2023interpretability",
        "author": "Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt",
        "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small"
      },
      {
        "key": "NEURIPS2024_d6df31b1",
        "author": "Yao, Yunzhi and Zhang, Ningyu and Xi, Zekun and Wang, Mengru and Xu, Ziwen and Deng, Shumin and Chen, Huajun",
        "title": "Knowledge Circuits in Pretrained Transformers"
      }
    ]
  }
]