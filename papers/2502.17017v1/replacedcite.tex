\section{Related Work}
\label{sec:related}
A significant line of research has focused on improving multi-step logical reasoning in LLMs via chain-of-thought (CoT) prompting____. Although CoT methods allow models to generate intermediate reasoning steps, they lack a  mechanism to assess the  coherence of these logical transitions.



Mechanistic interpretability studies have examined the roles of transformer attention heads. Recent work revealed that attention layers operate in phasesâ€”knowledge recalling, latent reasoning, and expression preparation____. Subsequent studies have shown that some attention heads introduce biases by evenly splitting probabilities between answer options____, while others suppress such behaviors during the final expression phase____. In addition, recent investigations have attempted to analyze model behavior by disabling specific components____, though these approaches are often computationally expensive or limited to simpler tasks____.


For an expanded discussion on related works, see Appendix~\ref{app:more_related}.