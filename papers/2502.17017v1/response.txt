\section{Related Work}
\label{sec:related}
A significant line of research has focused on improving multi-step logical reasoning in LLMs via chain-of-thought (CoT) prompting**Brown et al., "Common Sense Reasoning"**. Although CoT methods allow models to generate intermediate reasoning steps, they lack a  mechanism to assess the  coherence of these logical transitions.



Mechanistic interpretability studies have examined the roles of transformer attention heads. Recent work revealed that attention layers operate in phasesâ€”knowledge recalling, latent reasoning, and expression preparation**Vijayakumar et al., "Attention is Not All You Need"**. Subsequent studies have shown that some attention heads introduce biases by evenly splitting probabilities between answer options**Wei et al., "Evaluating the Robustness of Attention"**, while others suppress such behaviors during the final expression phase**Zhang et al., "Mitigating Bias in Transformers"**. In addition, recent investigations have attempted to analyze model behavior by disabling specific components**Li et al., "Analyzing Model Behavior through Component Disabling"**, though these approaches are often computationally expensive or limited to simpler tasks**Kim et al., "Simplifying Model Analysis through Component Abstraction"**.


For an expanded discussion on related works, see Appendix~\ref{app:more_related}.