@article{ChainOfThought,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{NEURIPS2024_d6df31b1,
 author = {Yao, Yunzhi and Zhang, Ningyu and Xi, Zekun and Wang, Mengru and Xu, Ziwen and Deng, Shumin and Chen, Huajun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {118571--118602},
 publisher = {Curran Associates, Inc.},
 title = {Knowledge Circuits in Pretrained Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d6df31b1be98e04be48af8bedb95b499-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{Seals2023EvaluatingTD,
  title={Evaluating the Deductive Competence of Large Language Models},
  author={S. M. Seals and Valerie L. Shalin},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2023},
}

@inproceedings{Yang2024DoLL,
  title={Do Large Language Models Latently Perform Multi-Hop Reasoning?},
  author={Sohee Yang and Elena Gribovskaya and Nora Kassner and Mor Geva and Sebastian Riedel},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
}

@article{attention_heads_survey,
  title={Attention heads of large language models: A survey},
  author={Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Yang, Mingchuan and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
  journal={arXiv preprint arXiv:2409.03752},
  year={2024}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{kim2024mechanistic,
  title={A mechanistic interpretation of syllogistic reasoning in auto-regressive language models},
  author={Kim, Geonhee and Valentino, Marco and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2408.08590},
  year={2024}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{lieberum2023does,
  title={Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla},
  author={Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  journal={arXiv preprint arXiv:2307.09458},
  year={2023}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@inproceedings{todd2024function,
  title={Function Vectors in Large Language Models},
  author={Todd, Eric and Li, Millicent and Sharma, Arnab and Mueller, Aaron and Wallace, Byron C and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2024},
  organization={ICLR}
}

@inproceedings{wan-etal-2024-logicasker,
    title = "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    author = "Wan, Yuxuan  and
      Wang, Wenxuan  and
      Yang, Yiliu  and
      Yuan, Youliang  and
      Huang, Jen-tse  and
      He, Pinjia  and
      Jiao, Wenxiang  and
      Lyu, Michael",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.128/",
    doi = "10.18653/v1/2024.emnlp-main.128",
    pages = "2124--2155"
}

@article{yu2024correcting,
  title={Correcting negative bias in large language models through negative attention score alignment},
  author={Yu, Sangwon and Song, Jongyoon and Hwang, Bongkyu and Kang, Hoyoung and Cho, Sooah and Choi, Junhwa and Joe, Seongho and Lee, Taehee and Gwon, Youngjune L and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2408.00137},
  year={2024}
}

@inproceedings{zhangtowards,
  title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  author={Zhang, Fred and Nanda, Neel},
  booktitle={The Twelfth International Conference on Learning Representations},
  year = {2024}
}

