\section{Related Work}
\label{sec:related}
A significant line of research has focused on improving multi-step logical reasoning in LLMs via chain-of-thought (CoT) prompting~\cite{ChainOfThought, kojima2022large, Yang2024DoLL, Seals2023EvaluatingTD, wan-etal-2024-logicasker}. Although CoT methods allow models to generate intermediate reasoning steps, they lack a  mechanism to assess the  coherence of these logical transitions.



Mechanistic interpretability studies have examined the roles of transformer attention heads. Recent work revealed that attention layers operate in phasesâ€”knowledge recalling, latent reasoning, and expression preparation~\cite{elhage2021mathematical, olah2020zoom, attention_heads_survey}. Subsequent studies have shown that some attention heads introduce biases by evenly splitting probabilities between answer options~\cite{lieberum2023does, yu2024correcting}, while others suppress such behaviors during the final expression phase~\cite{kim2024mechanistic}. In addition, recent investigations have attempted to analyze model behavior by disabling specific components~\cite{zhangtowards, todd2024function, NEURIPS2024_d6df31b1}, though these approaches are often computationally expensive or limited to simpler tasks~\cite{wang2023interpretability, NEURIPS2024_d6df31b1}.


For an expanded discussion on related works, see Appendix~\ref{app:more_related}.