\section{Related Work}
\label{sec:related}
%\subsection{Learning-based Query Optimization}
%Cost-based optimizer
%Balsa/Neo end-to-end 的方法、不依赖cost-based optimizer，但是需要从头开始train，plan exploration空间很大，需要花费很多时间枚举plan。
%hint-based 的方法，BAO/HybridQO大概的motivation训练方法。
%DQ____
%ReJoin____
%RTOS____

% Recent work on learned query optimization is broadly divided
% into two categories: “full” learned optimizers that synthesize entire
% query plans [5, 9, 11, 14, 15, 18, 27, 29, 31], and “steering” learned
% optimizers that sit on top of a traditional optimizer [13, 26, 28].

% In recent years, various learned query optimizers have been proposed.
%The recent studies of learned query optimization can be broadly divided into two categories, i.e., de novo learned optimizers, which directly synthesize entire query plans, and steered learned optimizers, which operate on top of traditional optimizers.


\stitle{Learning-based Query Optimization.} Database community has witnessed the recent boom of learned query optimization, which can be broadly divided into two categories, de novo learned optimization and steered learned optimization.
De nove learned query optimization leverages learning-based search algorithms, e.g., reinforcement learning (RL) algorithms to replace the dynamic programming in traditional optimizers to generate query plans.
DQ____ adopts a policy-based RL algorithm for searching cost-efficient join orders, while RTOS____ and JOGGER____ adopt a value-based RL algorithm where neural networks such as MLP, TreeLSTM____, graph representation learning model the intermediate states of joins. 
% DQ____, ReJoin____,
% and apply reinforcement learning (RL) in an exploration-exploitation strategy
% to select a good join order. 
%These methods use a cost model to produce a "join score" reward for the learning agent.
%Neo____
%Balsa____
Neo____ and Balsa____ provide end-to-end solutions for query optimization.
Specifically, 
Neo employs Tree Convolutions____ to estimate the latencies of an execution plan based on a given sub-plan and the optimal plan is generated via a best-first search in the plan space in a bottom-up fashion.
%Neo____ employs a neural network (NN) to estimate the latencies of an entire query plan based on a given sub-plan. The optimal plan is predicted via a greedy tree search in the join and scan space and consecutive bottom-up plan construction. 
% The optimal plan is predicted via a greedy tree search in the join and scan space and consecutive bottom-up plan construction. 
Balsa adopts the same model architecture as Neo but bootstraps the model from a logical-only cost model, bypassing an expert optimizer or expensive query execution.
%However, it is a learned query optimizer that completely avoids the participation of the DBMS optimizer by bootstrapping from a simulator. Balsa further introduces a variant of beam search and applies a timeout mechanism to handle disastrous plans. 
%Bao____
%HybridQO____
%Lero____
%DeepO____
The second category, steered learned query optimization injects learning-based strategies into generic query optimizers to steer the process of candidate plan selection.
Bao____, HybridQO____ and Lero____ tune the native  optimizers with different knobs to generate a set of candidate plans. 
Specifically, Bao exploits a multi-arm bandit algorithm to search for per-query optimization hints, where a Tree Convolution model is trained as the value function. 
%like disabling nested loop join for the entire query are used to guide the traditional query optimizer. It then utilizes a learned value network to evaluate these candidate plans and predict the cost. Ultimately, it selects the plan with the potentially lowest cost for execution. 
HybridQO combines learned and cost-based optimization, interpreting hints as prefix trees with partial join orders to guide the prediction of the final join order. 
Lero scales the estimated cardinality of sub-queries with different factors to produce different candidate
plans and applies learning-to-rank to decide the preference of plans.
%In summary, a recent benchmarking framework compares the end-to-end performance of representative learned query optimizers____, which reveals that these approaches are vulnerable to the divergence of training and test workloads.

% Bao____ is a very impressive learned optimizer, and
% it utilizes optimization strategies (e.g. disable Hash Join, force Index Scan) to generate optimized query plans. However, Bao can only offer query-level optimization, for example, disabling Hash Join in Bao will force all join operations in the query to use other join method. Besides, Bao does not optimize join order directly but leaves it to the default query optimizer of DBMS.


%\subsection{LLM Prompting \& Fine-tuning}
% LoRA____
% QLoRA____
% DPO____ 
\stitle{LLM Prompting.}
%CoT____
%prompting
%zero-shot prompting
%few-shot prompting
% Chain of Prompt
LLMs have demonstrated remarkable capabilities through various prompting techniques. Early works established prompt engineering to elicit desired behaviors from LLMs without updating model parameters, while in-context learning~____ emerged as a powerful learning framework where models learn from few-shot exemplars within the prompt. 
Chain-of-Thought prompting (CoT)____ further enables LLMs to break down complex reasoning tasks into intermediate steps, significantly improving their performance on mathematical and logical problems. These methodologies have proven complementary, with recent research showing that combining structured prompting with CoT reasoning and in-context examples can enhance model performance across diverse tasks, from arithmetic reasoning to complex inference.

\comment{
Prompting is a crucial mechanism for human-computer interaction, facilitating the explicit communication of clear task descriptions to LLMs, which subsequently generate responses aligned with user expectations through analogical learning. 
The content of a prompt may include instructions, questions, multiple demonstrations with specified output formats, as well as additional requirements such as complex reasoning processes. 
%The content of a prompt can differ across various contexts and may include instructions, questions, multiple demonstrations with specified output formats, as well as additional requirements such as complex reasoning processes. 
Few-shot prompting, often termed in-context learning____, is a method in which LLMs learn to complete a task with only a few examples, without the need for weight updates/retraining.
% In the context of few-shot learning, these examples are incorporated into the prompt template, functioning as contextual guidance for the model's response generation. 
In contrast to few-shot prompting, zero-shot prompting uses zero examples. There are several established standalone zero-shot techniques, as well as methods that combine zero-shot prompting with other concepts, such as Chain of Thought____.
zero-shot-CoT involves appending a thought
inducing phrase like "Let’s think step by step." to the prompt, which enables the model to break down complex queries into logical steps, improving the clarity and coherence of the responses.
The framework Chain of Table____ dynamically plans a reasoning chain by in-context learning for tabular data reasoning, where the intermediate operations manipulate temporary tables. 
}

\comment{Zero-shot prompting.1 This involves querying LLMs with a prompt that hasn’t been
seen in the training data of the model. Such
prompts typically provide specific task instructions along with the main query. Given the
sensitivity of LLMs to the structure and content of prompts, careful prompt engineering is
crucial to achieve optimal performance.
Few-shot learning. Often referred to as incontext learning, few-shot learning is a technique where LLMs are provided with a handful of examples to guide their responses. Zero-shot prompting can be considered a subset of this, where no examples are given. In few-shot learning, these examples are integrated
into the prompt template, serving as context
to instruct the model on how to resp
}
\comment{
This module interprets the user’s request into instructions that LLMs can easily follow. It first extracts the request intent, generates the prompt by inserting the query intent into the prepared prompt template, and inputs the prompt to LLM to handle the request.} 

% Zero-shot and few-shot learning approaches help the model understand the task and respond to queries with minimal examples.

\stitle{LLM Fine-tuning}. 
%参考 ORPO: Monolithic Preference Optimization without Reference Model, Step-DPO 的
%参数
%SFT RHLF, PPO, DPO
%SFT
% RHLF____
%
% PPO+Instruction tuning ____
% CPO____
%
% falcon llm ____
% phi-1.5____
% mistral-7b____
% llama____
% opt____
% falcon series____
%
% instruction-tuning____
% alpaca____
% camel____
%
% Pretraining->SFT->DPO
% Pretraining involves training the model on a large corpus to maximize the log-likelihood of predicting the next token based on the preceding text.
% The two methods above allow for task adaptation without the need for further training on the LLMs. In contrast,
\comment{fine-tuning involves extending the training of the LLMs using additional, task-specific data. This is particularly beneficial when such tailored datasets are available.
}
Pre-trained language models (PLMs), trained on extensive corpora, have demonstrated exceptional capabilities across numerous NLP tasks____.
% Pre-trained language models (PLMs) with vast
% training corpora such as textbooks____ and texts____ have shown remarkable abilities in
% a wide range of natural language processing (NLP) tasks____. 
However, deploying these models for specific applications requires additional refinement through instruction tuning and preference alignment.
% However, these models require additional tuning to perform a specific task,
% often through processes such as instruction tuning and preference alignment.
Instruction-tuning____ enables models to follow task descriptions given in natural language, facilitating generalization to new tasks. 
Nevertheless, the instruction-tuned models may still generate inappropriate or unethical responses.
% Instruction-tuning____ trains
% models to follow task descriptions given in natural language, which enables models to generalize well to previously unseen tasks. 
% However, despite the ability to follow instructions, models may generate toxic or unethical outputs____. 
To address this problem, specialized training strategies are proposed to further align these models with human values, typically implemented by reinforcement learning with human feedback (RLHF)____.
% To further align these models with human values,
% additional training strategies are required with pairwise preference data using techniques such as reinforcement
% learning with human feedback~(RLHF)____.
The RLHF approach involves training a reward model using pairwise preference data, which guides the optimization of a policy model.
To streamline this process, direct preference optimization (DPO)____ has emerged as an innovative solution, integrating reward modeling directly into the preference learning stage, thereby eliminating the need for two separate models and simplifying the training pipeline.   
% This approach involves training a reward model with comparison data and then using this reward model to optimize the policy model. The final performance heavily depends on the quality of the reward model, and the training pipeline is quite complex.
% To simplify this process, 
% direct preference optimization (DPO)____ combines
% the reward modeling stage into the preference learning stage, which bypasses the need for complex reward models and significantly streamlines the training pipeline.
\comment{% is proposed to optimize language models directly based on human preferences, 
% bypassing the need for complex reward models and traditional reinforcement learning processes. 

% Notably, Ouyang et al____ demonstrated the scalability and versatility of RLHF for instruction-following language models. 

% which combines
% the reward modeling stage into the preference learn-
% ing stage.
}


%full parameter tuning->Lora->Q-lora
% LoRA____
% QLoRA____
% 为了实现efficient fine-tuning 有了下面这些算法：
While LLMs exhibit powerful capabilities, their large size poses significant challenges for fine-tuning. 
Low-Rank Adaptation (LoRA) ____ is a prevailing parameter-efficient technique for fine-tuning customized LLMs. By introducing a low-rank approximation that updates only a small collection of parameters, LoRA substantially reduces computational overhead while mitigating overfitting. 
% LLMs are incredibly powerful, but their sheer size can present challenges when fine-tuning for specific tasks and domains. 
% LoRA____ is among the most popular parameter-efficient techniques for fine-tuning custom large language models (LLMs). It introduces low-rank matrices that adjust only a small subset of model parameters, significantly reducing computational costs and preventing overfitting. 
%Based on LoRA, QLoRA____ incorporates quantization techniques, further reducing memory overhead and enabling fine-tuning on resource-limited devices.
%QLoRA____ builds on this by incorporating quantization, further decreasing memory usage and enabling efficient fine-tuning even on resource-constrained devices, while maintaining performance comparable to that of full fine-tuning.

% Recently, many researchers have
% been focused on fine-tuning and inference for these LLMs in
% low-resource environments and have proposed methods such
% as fp16, 8bit-Quantized, LoRA (Hu et al. 2021) and QLoRA
% (Dettmers et al. 2023)




\comment{Pretraining involves training the model on a large corpus to maximize the log-likelihood of predicting the next token based on the preceding text.


While LLMs gain broad linguistic knowledge through pre-training, they often require additional expertise for specific tasks. Supervised Fine-Tuning (SFT) addresses this by further training the model with data that is more relevant to the downstream task. Often, such data will comprise instructions and an appropriate response (i.e., instruction fine-tuning). 

Fine-tuning is a process in machine learning that involves adapting a pre-trained model to perform specific tasks by training it on a smaller, task-relevant dataset. 

Supervised fine-tuning (SFT) can align models with human preferences. However, as the probability
of preferred outputs increases, so does the likelihood of undesirable ones, leading to hallucinations.}

\comment{
%RHLF
Recently, LLMs with Reinforcement Learning from Human Feedback (RLHF) can further generate a response that is well aligned with human values.
To generate more reliable outputs, Reinforcement Learning from Human Feedback (RLHF) (Christiano
et al., 2017; Ouyang et al., 2022) has been introduced for LLM alignment. This approach involves
training a reward model with comparison data and then using this reward model to optimize the policy
model. The final performance heavily depends on the quality of the reward model, and the training
pipeline is quite complex.

%PPO DPO 
Direct Preference Optimization (DPO) is an emerging technique that optimizes language models directly based on human preferences, bypassing the need for complex reward models and traditional reinforcement learning processes. By leveraging human feedback data, DPO streamlines the model fine-tuning process, enhancing efficiency and stability while demonstrating superior performance across various tasks compared to existing methods. 
To simplify this process, Direct Preference Optimization (DPO) (Rafailov et al., 2024) was proposed,
which directly uses pair-wise preference data for model optimization. This transition significantly
streamlines the training pipeline.
}

%\subsection{LLM for Database Management}
% DB-GPT____
% LLM-R\textsuperscript{2}____

% Multi-Modal Query Planners
% CAESURA____
% data visualization____

% database knob tuning____

% data cleaning and data preparation____

% data correlation analysis____

% GPT-DB:
% code generation____

% text-to-SQL____

% 重点参考 LLMTune: Accelerate Database Knob Tuning with Large Language Models、DB-GPT-Large Language Model Meets Database、LLM for Data Management

% Recently, there has been a lot of research on using LLMs to enhance database systems. 
\stitle{LLM for Database Management.} The recent success of LLMs incites their utilization in DBMS to support various tasks.  
%Recent advancements in database management systems (DBMS) have increasingly focused on enhancing performance through the integration of large language models (LLMs). 
For database knob tuning, $\lambda$-Tune____ leverages LLMs to optimize database knobs in response to varying workloads  adaptively. GPTuner____ utilizes LLMs to read manuals as domain knowledge for optimizing database parameter configurations.
For tabular data cleaning and analysis, Table-GPT____ 
introduces a `table-tuning' paradigm that enhances the performance of LLMs on table-understanding tasks, and Sui et al.____ develop a benchmark to evaluate the table understanding capabilities of LLMs. 
% For knob tuning, $\lambda$-Tune____ presents an approach that leverages LLMs to optimize database systems adaptively based on varying workloads. GPTuner____ introduces a manual-reading approach that utilizes LLMs to extensively leverage domain knowledge for optimizing database parameter configurations, thereby enhancing the tuning process and improving performance efficiency.
% Inspired by the capabilities of LLMs, researchers have investigated their application in tabular data cleaning and understanding. 
% %enhancing data quality and usability for analysis.
% Table-GPT____ introduces a "table-tuning" paradigm that enhances the performance of language models like GPT-3.5 and ChatGPT on table-understanding tasks, and Sui et al.____ develop a benchmark to evaluate the table understanding capabilities of LLMs. 
NL2VIS____ assesses the potential of LLMs in transforming natural language descriptions of tabular data into visual representations.  
% In addition, Wu et al____ investigate how LLMs can automatically generate data visualizations from natural language descriptions.
% Furthermore, Trummer____ investigates how LLMs can predict correlations between data columns by analyzing their names, demonstrating their potential to uncover relationships within datasets for improved data profiling and analysis.
%query processing:text2sql provide interface for non-techinical users to access the database, query planning,  query rewriting.
Furthermore, there have been substantial efforts to leverage LLMs in query processing tasks, including text-to-SQL, query planning, and query rewriting.  
% LLMs have been widely applied in query processing tasks, such as text-to-SQL, query planning, and query rewriting. 
Numerous studies____ have highlighted the potential of LLMs in the text-to-SQL task. 
% Text-to-SQL systems aim to automatically translate natural language questions into SQL queries, significantly enhancing user access to databases. BIRD____ provides a collection of question-SQL pairs and shows the potential of LLMs in the text-to-SQL task. 
GPT-DB____ builds a pipeline with GPT-4 to automatically generate query-specific code for SQL query processing from natural language instructions. 
CAESURA____ uses GPT-4 to translate natural language queries into multi-modal query plans containing relational operators and Python UDFs.
% CAESURA____ expands the scope of query planning beyond traditional relational databases, accommodating various data types. 
% Query rewrite aims to transfer a SQL
% query to an equivalent but more efficient SQL query.
LLM-R\textsuperscript{2}____ proposes a new query rewriting system that utilizes LLMs to recommend rewriting rules by few-shot learning.
Evaporate____ uses in-context learning of LLMs to generate structured views for semi-structured data lakes. 
%这些方法没有探索LLM在query optimization 
% While much of this existing research has focused on LLMs’ ability to generate executable and more effiecient queries, it has largely neglected their potential for query optimization.
Despite the progress, there remains a gap in exploring the potential of LLMs in query optimization. 

\comment{
%database knob 
Modern database management systems (DBMS) expose hundreds of confgurable knobs to control system behaviours. Determining the appropriate values for these knobs to improve DBMS performance.
Recent advancements in database tuning systems have explored the integration of large language models to enhance performance optimization. 
$\lambda$-Tune____ presents an approach that leverages large language models (LLMs) to optimize database systems adaptively based on varying workloads. 
$\lambda$-Tune____
exploits information from an input set of queries, in order to tune an input database system in a workload-adaptive manner.

GPTuner____ introduces a manual-reading approach that leverages domain knowledge extensively to optimize the tuning process, employing a novel framework based on GPT-guided Bayesian optimization to suggest effective knob configurations while reducing tuning costs.

%data cleaning 
Table-GPT____ introduces a "table-tuning" paradigm to enhance the performance of language models like GPT-3.5 and ChatGPT on table-related tasks. By synthesizing diverse training data from real tables, the model is fine-tuned to improve its understanding and manipulation of tabular data, achieving notable advancements in tasks such as data transformation, question answering, and error detection.

Inspired by the capability of LLMs, researchers have investigated whether LLMs can be used for data cleaning tasks such as data imputation where LLMs repair dirty or missing values in data entries

Trummer____ investigates the ability of large language models (LLMs) to predict correlations between data columns by analyzing their names. 

%data visualization
____ first explores the use of large language models (LLMs) to automatically generate data visualizations from natural language descriptions and evaluates the effectiveness of LLMs in producing relevant visual representations and identifies key factors that enhance their performance in the visualization task.

%SQL-related task: text-to-sql, code generation.
Text-to-SQL aims at automatically translating natural language questions into SQL queries. BIRD(a large-scale benchmark dataset)____ is proposed
to narrow the gap between Text-to-SQL research and its real-world deployment.

GPT-DB____ presents a system that utilizes GPT-4 to automatically generate code for SQL processing in general-purpose programming languages like Python, based on user-provided natural language instructions. 

%query rewrite
DB-GPT____ introduces an automated prompt strategy utilizing LLMs for query rewriting and index tuning. 

LLM-R\textsuperscript{2}____ proposes a DB-based SQL query rewrite pipeline enhanced by a LLM. It employs a contrastive model to learn query representations and select effective demonstrations, which optimizes the LLM's rule selection for rewriting.

CAESURA____ emphasizes multi-modal query planning and introduces a prototype built upon GPT-4 that translates natural language queries into executable query plans. In contrast to conventional relational query planners, the generated query plans can integrate complex operators that are adept at handling diverse modalities.

CAESURA____ goes outside the scope of code generation to generate reasoning plans
for natural language multi-modal queries.
}

% LLMs and founcation models have also been utilized for challenging tasks such as predicting data correlations____, text-to-SQL____, and code generation____, to name a few.

%=>sql query execution

% One such task
% that is highly related to DB research is text-to-SQL, in which an
% LLM directly generates a SQL query given database information
% and user requirements. Numerous studies [20, 26 , 36 ] have high-
% lighted the potential of LLMs in the text-to-SQL task, showcasing
% their proficiency in SQL query-related tasks.


% Furthermore, the capability of GPT to undertake
% additional database-related tasks, such as converting text to SQL, is demonstrated in recent works [38, 39].


\comment{
GPTuner____ utilizes domain knowledge and a GPT-guided Bayesian optimization framework to improve knob tuning efficiency and reduce costs. Both studies underscore the potential of LLMs in enhancing database tuning processes.}

\comment{
CAESURA: we propose Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans.
Different from relational query planners, the resulting query plans
can contain complex operators that are able to process arbitrary
modalities. As part of this paper, we present a first GPT-4 based
prototype called CAESURA and show the general feasibility of this
idea on two datasets}

\comment{
% LLMTune: Accelerate Database Knob Tuning with Large Language Models
% DB-bert [37] implements the BERT model for database knob
% tuning.


% With the recent trend of multi-modal
% query processing, multi-modal query planning based on LLM has been initially studied.

% : Generating Query-Specific
% and Customizable Code for SQL Processing with GPT-4

% DB-bert [37] implements the BERT model for database knob
% tuning.

% CodeXDB [36] develops a framework built upon GPT-3 to simplify complex SQL queries into manageable steps. Additionally,
% Trummer [40] offers a tutorial aimed at DBAs on utilizing LLMs
% for large-scale data management. Evaporate [1] proposes a comprehensive system for processing semi-structured documents into
% queryable tables. 

% Furthermore, the capability of GPT to undertake
% additional database-related tasks, such as converting text to SQL, is
% demonstrated in recent works [38, 39].
}

\comment{
\begin{table}[t]
\footnotesize
\centering
\caption{Frequently-used Notations}
\label{tab:notation}
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|c}
\toprule
Notation & Description                 \\\midrule
$q$ & a SQL query \\
$p$ & a query plan \\
$\pi$ & The LLM model \\
$x$, $y$ & The input and output of LLM \\
$\mathcal{L}$ & The loss function of LLM fine-tuning \\
 \bottomrule
\end{tabular}
%}
\end{table}
}