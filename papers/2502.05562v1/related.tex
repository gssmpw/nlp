
 
\section{Related Work}
\label{sec:related}
%\subsection{Learning-based Query Optimization}
%Cost-based optimizer
%Balsa/Neo end-to-end 的方法、不依赖cost-based optimizer，但是需要从头开始train，plan exploration空间很大，需要花费很多时间枚举plan。
%hint-based 的方法，BAO/HybridQO大概的motivation训练方法。
%DQ~\cite{DBLP:journals/corr/abs-1808-03196}
%ReJoin~\cite{DBLP:conf/sigmod/MarcusP18}
%RTOS~\cite{DBLP:conf/icde/Yu0C020}

% Recent work on learned query optimization is broadly divided
% into two categories: “full” learned optimizers that synthesize entire
% query plans [5, 9, 11, 14, 15, 18, 27, 29, 31], and “steering” learned
% optimizers that sit on top of a traditional optimizer [13, 26, 28].

% In recent years, various learned query optimizers have been proposed.
%The recent studies of learned query optimization can be broadly divided into two categories, i.e., de novo learned optimizers, which directly synthesize entire query plans, and steered learned optimizers, which operate on top of traditional optimizers.


\stitle{Learning-based Query Optimization.} Database community has witnessed the recent boom of learned query optimization, which can be broadly divided into two categories, de novo learned optimization and steered learned optimization.
De nove learned query optimization leverages learning-based search algorithms, e.g., reinforcement learning (RL) algorithms to replace the dynamic programming in traditional optimizers to generate query plans.
DQ~\cite{DBLP:journals/corr/abs-1808-03196} adopts a policy-based RL algorithm for searching cost-efficient join orders, while RTOS~\cite{DBLP:conf/icde/Yu0C020} and JOGGER~\cite{DBLP:conf/kdd/0008Y000CZ022} adopt a value-based RL algorithm where neural networks such as MLP, TreeLSTM~\cite{DBLP:conf/acl/TaiSM15}, graph representation learning model the intermediate states of joins. 
% DQ~\cite{DBLP:journals/corr/abs-1808-03196}, ReJoin~\cite{DBLP:conf/sigmod/MarcusP18},
% and apply reinforcement learning (RL) in an exploration-exploitation strategy
% to select a good join order. 
%These methods use a cost model to produce a "join score" reward for the learning agent.
%Neo~\cite{DBLP:journals/pvldb/MarcusNMZAKPT19}
%Balsa~\cite{DBLP:conf/sigmod/YangC0MLS22}
Neo~\cite{DBLP:journals/pvldb/MarcusNMZAKPT19} and Balsa~\cite{DBLP:conf/sigmod/YangC0MLS22} provide end-to-end solutions for query optimization.
Specifically, 
Neo employs Tree Convolutions~\cite{DBLP:conf/aaai/MouLZWJ16} to estimate the latencies of an execution plan based on a given sub-plan and the optimal plan is generated via a best-first search in the plan space in a bottom-up fashion.
%Neo~\cite{DBLP:journals/pvldb/MarcusNMZAKPT19} employs a neural network (NN) to estimate the latencies of an entire query plan based on a given sub-plan. The optimal plan is predicted via a greedy tree search in the join and scan space and consecutive bottom-up plan construction. 
% The optimal plan is predicted via a greedy tree search in the join and scan space and consecutive bottom-up plan construction. 
Balsa adopts the same model architecture as Neo but bootstraps the model from a logical-only cost model, bypassing an expert optimizer or expensive query execution.
%However, it is a learned query optimizer that completely avoids the participation of the DBMS optimizer by bootstrapping from a simulator. Balsa further introduces a variant of beam search and applies a timeout mechanism to handle disastrous plans. 
%Bao~\cite{DBLP:conf/sigmod/MarcusNMTAK21}
%HybridQO~\cite{DBLP:journals/pvldb/YuC0L22}
%Lero~\cite{DBLP:journals/pvldb/ZhuCDCPWZ23}
%DeepO~\cite{DBLP:conf/sigmod/SunJ0C22}
The second category, steered learned query optimization injects learning-based strategies into generic query optimizers to steer the process of candidate plan selection.
Bao~\cite{DBLP:conf/sigmod/MarcusNMTAK21}, HybridQO~\cite{DBLP:journals/pvldb/YuC0L22} and Lero~\cite{DBLP:journals/pvldb/ZhuCDCPWZ23} tune the native  optimizers with different knobs to generate a set of candidate plans. 
Specifically, Bao exploits a multi-arm bandit algorithm to search for per-query optimization hints, where a Tree Convolution model is trained as the value function. 
%like disabling nested loop join for the entire query are used to guide the traditional query optimizer. It then utilizes a learned value network to evaluate these candidate plans and predict the cost. Ultimately, it selects the plan with the potentially lowest cost for execution. 
HybridQO combines learned and cost-based optimization, interpreting hints as prefix trees with partial join orders to guide the prediction of the final join order. 
Lero scales the estimated cardinality of sub-queries with different factors to produce different candidate
plans and applies learning-to-rank to decide the preference of plans.
%In summary, a recent benchmarking framework compares the end-to-end performance of representative learned query optimizers~\cite{DBLP:journals/pvldb/LehmannSS24}, which reveals that these approaches are vulnerable to the divergence of training and test workloads.

% Bao~\cite{DBLP:conf/sigmod/MarcusNMTAK21} is a very impressive learned optimizer, and
% it utilizes optimization strategies (e.g. disable Hash Join, force Index Scan) to generate optimized query plans. However, Bao can only offer query-level optimization, for example, disabling Hash Join in Bao will force all join operations in the query to use other join method. Besides, Bao does not optimize join order directly but leaves it to the default query optimizer of DBMS.


%\subsection{LLM Prompting \& Fine-tuning}
% LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22}
% QLoRA~\cite{DBLP:conf/nips/DettmersPHZ23}
% DPO~\cite{DBLP:conf/nips/RafailovSMMEF23} 
\stitle{LLM Prompting.}
%CoT~\cite{DBLP:conf/nips/KojimaGRMI22}
%prompting
%zero-shot prompting
%few-shot prompting
% Chain of Prompt
LLMs have demonstrated remarkable capabilities through various prompting techniques. Early works established prompt engineering to elicit desired behaviors from LLMs without updating model parameters, while in-context learning~~\cite{DBLP:conf/emnlp/MinLHALHZ22} emerged as a powerful learning framework where models learn from few-shot exemplars within the prompt. 
Chain-of-Thought prompting (CoT)~\cite{DBLP:conf/nips/KojimaGRMI22} further enables LLMs to break down complex reasoning tasks into intermediate steps, significantly improving their performance on mathematical and logical problems. These methodologies have proven complementary, with recent research showing that combining structured prompting with CoT reasoning and in-context examples can enhance model performance across diverse tasks, from arithmetic reasoning to complex inference.

\comment{
Prompting is a crucial mechanism for human-computer interaction, facilitating the explicit communication of clear task descriptions to LLMs, which subsequently generate responses aligned with user expectations through analogical learning. 
The content of a prompt may include instructions, questions, multiple demonstrations with specified output formats, as well as additional requirements such as complex reasoning processes. 
%The content of a prompt can differ across various contexts and may include instructions, questions, multiple demonstrations with specified output formats, as well as additional requirements such as complex reasoning processes. 
Few-shot prompting, often termed in-context learning~\cite{DBLP:conf/emnlp/MinLHALHZ22}, is a method in which LLMs learn to complete a task with only a few examples, without the need for weight updates/retraining.
% In the context of few-shot learning, these examples are incorporated into the prompt template, functioning as contextual guidance for the model's response generation. 
In contrast to few-shot prompting, zero-shot prompting uses zero examples. There are several established standalone zero-shot techniques, as well as methods that combine zero-shot prompting with other concepts, such as Chain of Thought~\cite{DBLP:conf/nips/KojimaGRMI22}.
zero-shot-CoT involves appending a thought
inducing phrase like "Let’s think step by step." to the prompt, which enables the model to break down complex queries into logical steps, improving the clarity and coherence of the responses.
The framework Chain of Table~\cite{wang2024chainoftable} dynamically plans a reasoning chain by in-context learning for tabular data reasoning, where the intermediate operations manipulate temporary tables. 
}

\comment{Zero-shot prompting.1 This involves querying LLMs with a prompt that hasn’t been
seen in the training data of the model. Such
prompts typically provide specific task instructions along with the main query. Given the
sensitivity of LLMs to the structure and content of prompts, careful prompt engineering is
crucial to achieve optimal performance.
Few-shot learning. Often referred to as incontext learning, few-shot learning is a technique where LLMs are provided with a handful of examples to guide their responses. Zero-shot prompting can be considered a subset of this, where no examples are given. In few-shot learning, these examples are integrated
into the prompt template, serving as context
to instruct the model on how to resp
}
\comment{
This module interprets the user’s request into instructions that LLMs can easily follow. It first extracts the request intent, generates the prompt by inserting the query intent into the prepared prompt template, and inputs the prompt to LLM to handle the request.} 

% Zero-shot and few-shot learning approaches help the model understand the task and respond to queries with minimal examples.

\stitle{LLM Fine-tuning}. 
%参考 ORPO: Monolithic Preference Optimization without Reference Model, Step-DPO 的
%参数
%SFT RHLF, PPO, DPO
%SFT
% RHLF~\cite{DBLP:journals/corr/abs-1909-08593,DBLP:conf/nips/StiennonO0ZLVRA20}
%
% PPO+Instruction tuning ~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}
% CPO~\cite{DBLP:conf/icml/XuSCTSDM024}
%
% falcon llm ~\cite{DBLP:conf/nips/PenedoMHCACPAL23}
% phi-1.5~\cite{DBLP:journals/corr/abs-2309-05463}
% mistral-7b~\cite{DBLP:journals/corr/abs-2310-06825}
% llama~\cite{DBLP:journals/corr/abs-2302-13971}
% opt~\cite{DBLP:journals/corr/abs-2205-01068}
% falcon series~\cite{DBLP:journals/corr/abs-2311-16867}
%
% instruction-tuning~\cite{DBLP:conf/iclr/WeiBZGYLDDL22}
% alpaca~\cite{DBLP:conf/nips/DuboisLTZGBGLH23}
% camel~\cite{DBLP:conf/nips/WangIDHKCWMSBH23}
%
% Pretraining->SFT->DPO
% Pretraining involves training the model on a large corpus to maximize the log-likelihood of predicting the next token based on the preceding text.
% The two methods above allow for task adaptation without the need for further training on the LLMs. In contrast,
\comment{fine-tuning involves extending the training of the LLMs using additional, task-specific data. This is particularly beneficial when such tailored datasets are available.
}
Pre-trained language models (PLMs), trained on extensive corpora, have demonstrated exceptional capabilities across numerous NLP tasks~\cite{DBLP:journals/corr/abs-2310-06825, DBLP:journals/corr/abs-2302-13971,DBLP:journals/corr/abs-2205-01068,DBLP:journals/corr/abs-2311-16867}.
% Pre-trained language models (PLMs) with vast
% training corpora such as textbooks~\cite{DBLP:journals/corr/abs-2309-05463} and texts~\cite{DBLP:conf/nips/PenedoMHCACPAL23} have shown remarkable abilities in
% a wide range of natural language processing (NLP) tasks~\cite{DBLP:journals/corr/abs-2310-06825, DBLP:journals/corr/abs-2302-13971,DBLP:journals/corr/abs-2205-01068,DBLP:journals/corr/abs-2311-16867}. 
However, deploying these models for specific applications requires additional refinement through instruction tuning and preference alignment.
% However, these models require additional tuning to perform a specific task,
% often through processes such as instruction tuning and preference alignment.
Instruction-tuning~\cite{DBLP:conf/iclr/WeiBZGYLDDL22,DBLP:conf/nips/DuboisLTZGBGLH23,DBLP:conf/nips/WangIDHKCWMSBH23} enables models to follow task descriptions given in natural language, facilitating generalization to new tasks. 
Nevertheless, the instruction-tuned models may still generate inappropriate or unethical responses.
% Instruction-tuning~\cite{DBLP:conf/iclr/WeiBZGYLDDL22,DBLP:conf/nips/DuboisLTZGBGLH23,DBLP:conf/nips/WangIDHKCWMSBH23} trains
% models to follow task descriptions given in natural language, which enables models to generalize well to previously unseen tasks. 
% However, despite the ability to follow instructions, models may generate toxic or unethical outputs~\cite{DBLP:conf/emnlp/PryzantI0L0023}. 
To address this problem, specialized training strategies are proposed to further align these models with human values, typically implemented by reinforcement learning with human feedback (RLHF)~\cite{DBLP:journals/corr/abs-1909-08593,DBLP:conf/nips/StiennonO0ZLVRA20,DBLP:conf/nips/Ouyang0JAWMZASR22}.
% To further align these models with human values,
% additional training strategies are required with pairwise preference data using techniques such as reinforcement
% learning with human feedback~(RLHF)~\cite{DBLP:journals/corr/abs-1909-08593,DBLP:conf/nips/StiennonO0ZLVRA20,DBLP:conf/nips/Ouyang0JAWMZASR22}.
The RLHF approach involves training a reward model using pairwise preference data, which guides the optimization of a policy model.
To streamline this process, direct preference optimization (DPO)~\cite{DBLP:conf/nips/RafailovSMMEF23} has emerged as an innovative solution, integrating reward modeling directly into the preference learning stage, thereby eliminating the need for two separate models and simplifying the training pipeline.   
% This approach involves training a reward model with comparison data and then using this reward model to optimize the policy model. The final performance heavily depends on the quality of the reward model, and the training pipeline is quite complex.
% To simplify this process, 
% direct preference optimization (DPO)~\cite{DBLP:conf/nips/RafailovSMMEF23} combines
% the reward modeling stage into the preference learning stage, which bypasses the need for complex reward models and significantly streamlines the training pipeline.
\comment{% is proposed to optimize language models directly based on human preferences, 
% bypassing the need for complex reward models and traditional reinforcement learning processes. 

% Notably, Ouyang et al~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22} demonstrated the scalability and versatility of RLHF for instruction-following language models. 

% which combines
% the reward modeling stage into the preference learn-
% ing stage.
}


%full parameter tuning->Lora->Q-lora
% LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22}
% QLoRA~\cite{DBLP:conf/nips/DettmersPHZ23}
% 为了实现efficient fine-tuning 有了下面这些算法：
While LLMs exhibit powerful capabilities, their large size poses significant challenges for fine-tuning. 
Low-Rank Adaptation (LoRA) \cite{DBLP:conf/iclr/HuSWALWWC22} is a prevailing parameter-efficient technique for fine-tuning customized LLMs. By introducing a low-rank approximation that updates only a small collection of parameters, LoRA substantially reduces computational overhead while mitigating overfitting. 
% LLMs are incredibly powerful, but their sheer size can present challenges when fine-tuning for specific tasks and domains. 
% LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22} is among the most popular parameter-efficient techniques for fine-tuning custom large language models (LLMs). It introduces low-rank matrices that adjust only a small subset of model parameters, significantly reducing computational costs and preventing overfitting. 
%Based on LoRA, QLoRA~\cite{DBLP:conf/nips/DettmersPHZ23} incorporates quantization techniques, further reducing memory overhead and enabling fine-tuning on resource-limited devices.
%QLoRA~\cite{DBLP:conf/nips/DettmersPHZ23} builds on this by incorporating quantization, further decreasing memory usage and enabling efficient fine-tuning even on resource-constrained devices, while maintaining performance comparable to that of full fine-tuning.

% Recently, many researchers have
% been focused on fine-tuning and inference for these LLMs in
% low-resource environments and have proposed methods such
% as fp16, 8bit-Quantized, LoRA (Hu et al. 2021) and QLoRA
% (Dettmers et al. 2023)




\comment{Pretraining involves training the model on a large corpus to maximize the log-likelihood of predicting the next token based on the preceding text.


While LLMs gain broad linguistic knowledge through pre-training, they often require additional expertise for specific tasks. Supervised Fine-Tuning (SFT) addresses this by further training the model with data that is more relevant to the downstream task. Often, such data will comprise instructions and an appropriate response (i.e., instruction fine-tuning). 

Fine-tuning is a process in machine learning that involves adapting a pre-trained model to perform specific tasks by training it on a smaller, task-relevant dataset. 

Supervised fine-tuning (SFT) can align models with human preferences. However, as the probability
of preferred outputs increases, so does the likelihood of undesirable ones, leading to hallucinations.}

\comment{
%RHLF
Recently, LLMs with Reinforcement Learning from Human Feedback (RLHF) can further generate a response that is well aligned with human values.
To generate more reliable outputs, Reinforcement Learning from Human Feedback (RLHF) (Christiano
et al., 2017; Ouyang et al., 2022) has been introduced for LLM alignment. This approach involves
training a reward model with comparison data and then using this reward model to optimize the policy
model. The final performance heavily depends on the quality of the reward model, and the training
pipeline is quite complex.

%PPO DPO 
Direct Preference Optimization (DPO) is an emerging technique that optimizes language models directly based on human preferences, bypassing the need for complex reward models and traditional reinforcement learning processes. By leveraging human feedback data, DPO streamlines the model fine-tuning process, enhancing efficiency and stability while demonstrating superior performance across various tasks compared to existing methods. 
To simplify this process, Direct Preference Optimization (DPO) (Rafailov et al., 2024) was proposed,
which directly uses pair-wise preference data for model optimization. This transition significantly
streamlines the training pipeline.
}

%\subsection{LLM for Database Management}
% DB-GPT~\cite{DBLP:journals/dase/ZhouSL24}
% LLM-R\textsuperscript{2}~\cite{DBLP:journals/corr/abs-2404-12872}

% Multi-Modal Query Planners
% CAESURA~\cite{DBLP:conf/cidr/UrbanB24}
% data visualization~\cite{DBLP:journals/pacmmod/Wu00SWZ0024}

% database knob tuning~\cite{DBLP:journals/pvldb/LaoWLWZCCTW24, DBLP:conf/sigmod/GiannakourisT24}

% data cleaning and data preparation~\cite{DBLP:journals/pacmmod/LiHYCGZF0C24}

% data correlation analysis~\cite{DBLP:journals/pvldb/Trummer23a}

% GPT-DB:
% code generation~\cite{DBLP:journals/pvldb/Trummer23}

% text-to-SQL~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23}

% 重点参考 LLMTune: Accelerate Database Knob Tuning with Large Language Models、DB-GPT-Large Language Model Meets Database、LLM for Data Management

% Recently, there has been a lot of research on using LLMs to enhance database systems. 
\stitle{LLM for Database Management.} The recent success of LLMs incites their utilization in DBMS to support various tasks.  
%Recent advancements in database management systems (DBMS) have increasingly focused on enhancing performance through the integration of large language models (LLMs). 
For database knob tuning, $\lambda$-Tune~\cite{DBLP:journals/pvldb/LaoWLWZCCTW24} leverages LLMs to optimize database knobs in response to varying workloads  adaptively. GPTuner~\cite{DBLP:conf/sigmod/GiannakourisT24} utilizes LLMs to read manuals as domain knowledge for optimizing database parameter configurations.
For tabular data cleaning and analysis, Table-GPT~\cite{DBLP:journals/pacmmod/LiHYCGZF0C24} 
introduces a `table-tuning' paradigm that enhances the performance of LLMs on table-understanding tasks, and Sui et al.~\cite{DBLP:conf/wsdm/SuiZZH024} develop a benchmark to evaluate the table understanding capabilities of LLMs. 
% For knob tuning, $\lambda$-Tune~\cite{DBLP:journals/pvldb/LaoWLWZCCTW24} presents an approach that leverages LLMs to optimize database systems adaptively based on varying workloads. GPTuner~\cite{DBLP:conf/sigmod/GiannakourisT24} introduces a manual-reading approach that utilizes LLMs to extensively leverage domain knowledge for optimizing database parameter configurations, thereby enhancing the tuning process and improving performance efficiency.
% Inspired by the capabilities of LLMs, researchers have investigated their application in tabular data cleaning and understanding. 
% %enhancing data quality and usability for analysis.
% Table-GPT~\cite{DBLP:journals/pacmmod/LiHYCGZF0C24} introduces a "table-tuning" paradigm that enhances the performance of language models like GPT-3.5 and ChatGPT on table-understanding tasks, and Sui et al.~\cite{DBLP:conf/wsdm/SuiZZH024} develop a benchmark to evaluate the table understanding capabilities of LLMs. 
NL2VIS~\cite{DBLP:journals/pacmmod/Wu00SWZ0024} assesses the potential of LLMs in transforming natural language descriptions of tabular data into visual representations.  
% In addition, Wu et al~\cite{DBLP:journals/pacmmod/Wu00SWZ0024} investigate how LLMs can automatically generate data visualizations from natural language descriptions.
% Furthermore, Trummer~\cite{DBLP:journals/pvldb/Trummer23a} investigates how LLMs can predict correlations between data columns by analyzing their names, demonstrating their potential to uncover relationships within datasets for improved data profiling and analysis.
%query processing:text2sql provide interface for non-techinical users to access the database, query planning,  query rewriting.
Furthermore, there have been substantial efforts to leverage LLMs in query processing tasks, including text-to-SQL, query planning, and query rewriting.  
% LLMs have been widely applied in query processing tasks, such as text-to-SQL, query planning, and query rewriting. 
Numerous studies~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23, DBLP:journals/corr/abs-2306-00739, DBLP:journals/corr/abs-2406-08426} have highlighted the potential of LLMs in the text-to-SQL task. 
% Text-to-SQL systems aim to automatically translate natural language questions into SQL queries, significantly enhancing user access to databases. BIRD~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} provides a collection of question-SQL pairs and shows the potential of LLMs in the text-to-SQL task. 
GPT-DB~\cite{DBLP:journals/pvldb/Trummer23} builds a pipeline with GPT-4 to automatically generate query-specific code for SQL query processing from natural language instructions. 
CAESURA~\cite{DBLP:conf/cidr/UrbanB24} uses GPT-4 to translate natural language queries into multi-modal query plans containing relational operators and Python UDFs.
% CAESURA~\cite{DBLP:conf/cidr/UrbanB24} expands the scope of query planning beyond traditional relational databases, accommodating various data types. 
% Query rewrite aims to transfer a SQL
% query to an equivalent but more efficient SQL query.
LLM-R\textsuperscript{2}~\cite{DBLP:journals/corr/abs-2404-12872} proposes a new query rewriting system that utilizes LLMs to recommend rewriting rules by few-shot learning.
Evaporate~\cite{DBLP:journals/pvldb/AroraYENHTR23} uses in-context learning of LLMs to generate structured views for semi-structured data lakes. 
%这些方法没有探索LLM在query optimization 
% While much of this existing research has focused on LLMs’ ability to generate executable and more effiecient queries, it has largely neglected their potential for query optimization.
Despite the progress, there remains a gap in exploring the potential of LLMs in query optimization. 

\comment{
%database knob 
Modern database management systems (DBMS) expose hundreds of confgurable knobs to control system behaviours. Determining the appropriate values for these knobs to improve DBMS performance.
Recent advancements in database tuning systems have explored the integration of large language models to enhance performance optimization. 
$\lambda$-Tune~\cite{DBLP:journals/pvldb/LaoWLWZCCTW24} presents an approach that leverages large language models (LLMs) to optimize database systems adaptively based on varying workloads. 
$\lambda$-Tune~\cite{DBLP:journals/pvldb/LaoWLWZCCTW24}
exploits information from an input set of queries, in order to tune an input database system in a workload-adaptive manner.

GPTuner~\cite{DBLP:conf/sigmod/GiannakourisT24} introduces a manual-reading approach that leverages domain knowledge extensively to optimize the tuning process, employing a novel framework based on GPT-guided Bayesian optimization to suggest effective knob configurations while reducing tuning costs.

%data cleaning 
Table-GPT~\cite{DBLP:journals/pacmmod/LiHYCGZF0C24} introduces a "table-tuning" paradigm to enhance the performance of language models like GPT-3.5 and ChatGPT on table-related tasks. By synthesizing diverse training data from real tables, the model is fine-tuned to improve its understanding and manipulation of tabular data, achieving notable advancements in tasks such as data transformation, question answering, and error detection.

Inspired by the capability of LLMs, researchers have investigated whether LLMs can be used for data cleaning tasks such as data imputation where LLMs repair dirty or missing values in data entries

Trummer~\cite{DBLP:journals/pvldb/Trummer23a} investigates the ability of large language models (LLMs) to predict correlations between data columns by analyzing their names. 

%data visualization
\cite{DBLP:journals/pacmmod/Wu00SWZ0024} first explores the use of large language models (LLMs) to automatically generate data visualizations from natural language descriptions and evaluates the effectiveness of LLMs in producing relevant visual representations and identifies key factors that enhance their performance in the visualization task.

%SQL-related task: text-to-sql, code generation.
Text-to-SQL aims at automatically translating natural language questions into SQL queries. BIRD(a large-scale benchmark dataset)~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} is proposed
to narrow the gap between Text-to-SQL research and its real-world deployment.

GPT-DB~\cite{DBLP:journals/pvldb/Trummer23} presents a system that utilizes GPT-4 to automatically generate code for SQL processing in general-purpose programming languages like Python, based on user-provided natural language instructions. 

%query rewrite
DB-GPT~\cite{DBLP:journals/dase/ZhouSL24} introduces an automated prompt strategy utilizing LLMs for query rewriting and index tuning. 

LLM-R\textsuperscript{2}\cite{DBLP:journals/corr/abs-2404-12872} proposes a DB-based SQL query rewrite pipeline enhanced by a LLM. It employs a contrastive model to learn query representations and select effective demonstrations, which optimizes the LLM's rule selection for rewriting.

CAESURA~\cite{DBLP:conf/cidr/UrbanB24} emphasizes multi-modal query planning and introduces a prototype built upon GPT-4 that translates natural language queries into executable query plans. In contrast to conventional relational query planners, the generated query plans can integrate complex operators that are adept at handling diverse modalities.

CAESURA~\cite{DBLP:conf/cidr/UrbanB24} goes outside the scope of code generation to generate reasoning plans
for natural language multi-modal queries.
}

% LLMs and founcation models have also been utilized for challenging tasks such as predicting data correlations~\cite{DBLP:journals/pvldb/Trummer23a}, text-to-SQL~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23}, and code generation~\cite{DBLP:journals/pvldb/Trummer23}, to name a few.

%=>sql query execution

% One such task
% that is highly related to DB research is text-to-SQL, in which an
% LLM directly generates a SQL query given database information
% and user requirements. Numerous studies [20, 26 , 36 ] have high-
% lighted the potential of LLMs in the text-to-SQL task, showcasing
% their proficiency in SQL query-related tasks.


% Furthermore, the capability of GPT to undertake
% additional database-related tasks, such as converting text to SQL, is demonstrated in recent works [38, 39].


\comment{
GPTuner~\cite{DBLP:conf/sigmod/GiannakourisT24} utilizes domain knowledge and a GPT-guided Bayesian optimization framework to improve knob tuning efficiency and reduce costs. Both studies underscore the potential of LLMs in enhancing database tuning processes.}

\comment{
CAESURA: we propose Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans.
Different from relational query planners, the resulting query plans
can contain complex operators that are able to process arbitrary
modalities. As part of this paper, we present a first GPT-4 based
prototype called CAESURA and show the general feasibility of this
idea on two datasets}

\comment{
% LLMTune: Accelerate Database Knob Tuning with Large Language Models
% DB-bert [37] implements the BERT model for database knob
% tuning.


% With the recent trend of multi-modal
% query processing, multi-modal query planning based on LLM has been initially studied.

% : Generating Query-Specific
% and Customizable Code for SQL Processing with GPT-4

% DB-bert [37] implements the BERT model for database knob
% tuning.

% CodeXDB [36] develops a framework built upon GPT-3 to simplify complex SQL queries into manageable steps. Additionally,
% Trummer [40] offers a tutorial aimed at DBAs on utilizing LLMs
% for large-scale data management. Evaporate [1] proposes a comprehensive system for processing semi-structured documents into
% queryable tables. 

% Furthermore, the capability of GPT to undertake
% additional database-related tasks, such as converting text to SQL, is
% demonstrated in recent works [38, 39].
}

\comment{
\begin{table}[t]
\footnotesize
\centering
\caption{Frequently-used Notations}
\label{tab:notation}
%\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|c}
\toprule
Notation & Description                 \\\midrule
$q$ & a SQL query \\
$p$ & a query plan \\
$\pi$ & The LLM model \\
$x$, $y$ & The input and output of LLM \\
$\mathcal{L}$ & The loss function of LLM fine-tuning \\
 \bottomrule
\end{tabular}
%}
\end{table}
}


\section{Preliminary \& Problem Statement}
\label{sec:background} 
%technical cornerstone
In this section, we formulate the research problem of utilizing LLMs for query optimization and introduce the foundation of LLMs. 
\subsection{Problem Statement}
\label{sec:background:problem_statement} 
Given a database with multiple tables,
%$\mathcal{D} = \{ T_1, \cdots, T_n\}$,
for a SQL query $q$, traditional query optimizers transform the SQL query into a tree-shape execution plan $p$ as illustrated in Fig.~\ref{fig:example:plan}.
The plan $p$ specifies a join order and the corresponding algorithm used in each operator. 
The basic paradigm of traditional optimizers is to enumerate candidate plans and then search for the optimal plan based on a cost model by dynamic programming. 
%Specifically, 
Typically, the cost model leverages database statistical information to estimate the execution cost of candidate plans. 

In this paper, for a database $\mathcal{D}$, we aim to build 
%an LLM 
a model based on general-purpose LLMs 
to infer the query plan directly. 
By formulating a SQL query $q$ and pertinent information for query planning (e.g., the database schema and statistics of the database) into a sequence of tokens $\bm{x}$ in a textual space  $\mathcal{X}$,
the LLM $\pi: \mathcal{X} \rightarrow \mathcal{Y}$, directly infers its execution plan as a textual response $\bm{y}$ in the output space $\mathcal{Y}$.
%1 2 3 这三个句子语法是啥关系啊 语法好像有点问题； 3 前面是add 但是add也没大写
The generated response should be (1) accessible; the sequential output tokens $\bm{y}$ can be easily transformed into a physical plan that can be evaluated by the execution engine, (2) valid; the output query plan should be a valid plan that is consistent with the semantics of the query, and (3) efficient; the performance of the plan should approach or outperform the plan from existing well-designed optimizers. 
In the following, we will use `generated response' or `generated plan' interchangeably when the context is clear. 
The model $\pi$ is trained by fine-tuning an off-the-shelf LLM, e.g., LLaMA~\cite{DBLP:journals/corr/abs-2302-13971}, given a set of SQL queries as a training workload.  
%Table~\ref{tab:notation} lists the frequently-used notations in this paper.
In this paper, we consider selection-project-join (SPJ) queries, with one join condition connecting two tables. 
% At the current stage, we support three join operators, \NestLoop, \MergeJoin, \HashJoin, are used for join and the table scan operator \SeqScan is used for selection. 
At the current stage, we support three join operators: \NestLoop, \MergeJoin, and \HashJoin for joining tables, while the table scan operator \SeqScan is utilized for selection.
% \tjadd{
% In this paper, we focus on selection-project-join (SPJ) queris, and consider three join algorithms, namely nested loop join(\NestLoop), merge join(\MergeJoin) and hash join(\HashJoin).
% }
%分成两句话；
We aim at building an LLM for a static database instance on a specific execution engine. Generalization on multiple databases and hardware configurations, complex query types, dynamic query workloads and dynamic relational data are our further work.

\subsection{Large Language Models}
%llm, 开源模型，闭源模型，autoaggressive
% Recent advancements in natural language processing (NLP) have led to the creation of powerful large language models (LLMs) that can perform a variety of tasks. 
LLMs are fundamentally built on Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} architectures, by scaling self-attention layers with massive parameter spaces from billions to trillions of parameters.
The landscape of LLMs is divided between closed-source models, such as GPT-4~\cite{DBLP:journals/corr/abs-2303-08774} and Claude, developed by commercial institutions, which are only accessible via remote API calls, and open-source alternatives like LLaMA~\cite{DBLP:journals/corr/abs-2302-13971} and Falcon~\cite{DBLP:conf/nips/PenedoMHCACPAL23}, which enable broader research exploitation and customization.  
In this paper, we fine-tune open-source LLMs to build an LLM-based query optimizer. 
% Closed-source models like GPT-3.5 and GPT-4 demonstrate superior performance compared to open-source alternatives such as LLaMA~\cite{DBLP:journals/corr/abs-2302-13971} and Falcon~\cite{DBLP:conf/nips/PenedoMHCACPAL23}. However, these closed-source models are only accessible via API calls, limiting customization and transparency. 
% To overcome these limitations, an one effective approach is to leverage the capabilities of closed-source LLMs by prompting them to generate samples that showcase their strengths. These samples can then be used to fine-tune smaller open-source models, enhancing their performance while maintaining the benefits of transparency and customization. 

The inference of LLMs performs via the process of next token prediction, where the model generates a response by predicting the next token based on the input context and previous output in an autoregressive process. 
%predict the next most likely word
Formally, given the input as a sequence of tokens $\bm{x} = [x^1, \cdots, x^n]$, and the output sequence $\bm{y} = [y^1, \cdots, y^T]$, model $\theta$  parameterizes the conditional distribution of $\bm{y}$ given $\bm{x}$ as 
\begin{align}
\label{eq:cond_prob}
    p_{\theta}(\bm{y} \mid \bm{x})= p_{\theta}( y^1, y^2 \cdots, y^T \mid \bm{x}) 
    = \prod_{t=1}^{T} p_{\theta}\left(y^{t} \mid 
    \bm{x} , y^{1:t-1}\right)
\end{align}
Given a set of input-output pairs $\mathbb{D} = \{ (\bm{x}, \bm{y})\}$ as training data, LLMs can be trained/fined-tuned like typical language models by maximum likelihood estimation, which minimizes the negation of the logarithmic likelihood in Eq.~\eqref{eq:mle_loss}.
\begin{align}
\label{eq:mle_loss}
    \loss({\theta}) &=
   \mathop{\mathbb{E}}_{(\bm {x}, \bm{y}) \sim \mathbb{D}} - \log p_{\theta} (\bm{y} \mid \bm{x}) \\
   &= \mathop{\mathbb{E}}_{(\bm {x}, \bm{y}) \sim \mathbb{D}}    -  \Big[ \sum_{t=1}^T \log p_{\theta}(y^t \mid \bm{x},  y^{1:t-1})\Big]\text{,}
\end{align}
To unleash the reasoning potential of open-source LLMs on a specific task, researchers customize their training data in a specific format, named a \emph{data recipe}~\cite{DBLP:conf/sigmod/Chen0MCPGGXLGLD24} which is a mixture of data of heterogeneous types and from different sources. 
% Autoregressive models form the backbone of advanced large language models, allowing for high-quality text generation and effective performance across a range of language-related tasks. These models  Autoregressive models generate text or make predictions based on previous words in a sentence. 
%These models learning the statistical patterns and dependencies in the training data and then usinge that knowledge to generate coherent and contextually relevant text. Their reliance on past values to predict future outcomes makes them a powerful tool in the field of natural language processing (NLP).
As the importance of ingredients to dishes, the quality, quantity and diversity of training data significantly affect the performance of LLMs. 
\comment{
Recent advances in the field of artificial intelligence (AI) have enabled the development of increasingly powerful linguistic models capable of generating text fluently
and coherently. Among these models, “large language models” (LLMs) stand out for
their imposing size and their ability to learn enormous amounts of textual data. Models
like GPT-3.5 [1] and GPT-4 [2] developed by OpenAI [3], or Bard, created by Google [4],
have billions of parameters and have demonstrated impressive comprehension skills and
language generation.
Recent advances in NLP have led to the development of powerful LLMs capable of performing a wide range of tasks. However, despite their impressive capabilities, general-purpose LLMs often struggle with domain-specific tasks that require specialized knowledge

}
%autoagressive 
\comment{
Autoregressive models are a type of statistical or machine learning model that predicts the next value in a sequence based on the previous values in that sequence. These models assume that the future values in the sequence are dependent on the past values and use this dependency to make predictions. In the context of natural language processing, autoregressive models are often applied to generate text or make predictions based on previous words in a sentence. These models learn the statistical patterns and dependencies in the training data and then use that knowledge to generate coherent and contextually relevant text. Autoregressive language models, such as GPT (Generative Pre-trained Transformer), GPT-2, and GPT-3, have gained significant attention for their ability to generate high-quality text and perform a variety of language-related tasks.
LLMs serve as the core framework for text generation. In essence, these models consist of large pretrained transformer architectures specifically designed to predict the next word (or, more accurately, the next token) based on a given input text. Since they generate one token sequentially, creating new sentences involves a more complex approach than merely invoking the model; it requires implementing autoregressive generation techniques.
}

\comment{
\subsection{Query Optimization Problem} 
\label{sec:preliminary:problem_definition}
The Query Optimization task can be formulated as
a sequence-to-sequence task where the input $x$ includes an instruction that specifies the query optimization task, a SQL query, and the statistical information of its schema, and the output is a planning path $y$ leading to the plan $p$.
A generated planning path $\hat{y}$ is regarded as correct if the extracted final plan $\hat{p}$ is valid execution plan. Formally, the
labeled dataset for a query optimization problem solving
task with instances can be represented as:
\begin{equation}
\mathbb{D} = \left\{ (x, y, p)^{i} \right\}_{i=1}^l
\end{equation}

% For the query optimization task with more than 5 tables, providing contextual information within the prompt helps the model build connections between examples and the task's purpose. This approach prevents the generation of invalid plans, such as those with duplicate tables or tables that do not exist in the schema.
% A common way to specialize a language model to solve this task with the labeled dataset $\mathbb{D}$ is supervised fine-tuning~(SFT).

The goal of plan generation with a sequence-to-sequence model is to generate a planning path $y$ with length T conditioned on the input text sequence $x$. 
Given a training set $\mathbb{D}$, the neural plan generation model parameterizes the conditional distribution $p_{\theta}(y|x)$ as follows:
\begin{equation}
\label{eq:p_function}
    p_{\theta}(y \mid x)=\prod_{t=1}^{T} p_{\theta}\left(y_{t} \mid  x , y_{1:t-1}\right),
\end{equation}

By adopting Maximum Likelihood Estimation (MLE) as the learning approach, the training objective of the neural plan generation model can be defined as follows: 
\begin{equation}
    \label{eq:sft_loss}
    \mathcal{L}_{\text{MLE}}({\theta})=
    \mathop{\mathbb{E}}_{(x, y) \sim \mathbb{D}}      \Big[ \sum_{t=1}^T \log p_{\theta}(y_t | x,        y_{1:t-1})\Big]\text{,}
\end{equation}
where $T$ is the length of the planning path $y$ and we use $y_t$ to represent the $t$-th token in $y$.
}
