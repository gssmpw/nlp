
%modularity-based method ~\cite{kim2022dmcs}
%~\cite{al2020topic} %topic-based cs over spatial-social networks
%~\cite{luo2020efficient} %efficient attribute-constrained co-located community search
%~\cite{guo2021multi} %Multi-attributed community search in road-social networks
%~\cite{liu2020vac} %Vac: vertexcentric attributed community search,
%~\cite{fang2020effective} %Effective and Efficient Community Search over Large Heterogeneous Information Networks
%~\cite{jian2020effective} %Effective and efficient relational community detection and search in large dynamic heterogeneous information networks
%~\cite{wang2021efficient}%Efficient and Effective Community Search on Large-scale Bipartite Graphs

%graph similarity search~\cite{bai2021tagsim, qin2020ghashing, bai2021glsearch}, 

%To ensure the quality of communities, we filter those ego-networks with fewer than $128$ nodes, as too small graphs may contain limited and much small communities.

%CTC
%It operates by identifying a $k$-truss that encompasses a given set of query nodes $Q$ using a greedy algorithm. 


%We regard each subgraph with generated pairs of query and ground-truth as a task.
%It provides insight into its generalization capability

%We test our CGNP in different subgraphs of same graph, different graphs, and different application scenarios, following the 4 different types of tasks described in \cref{sec:problem}:
%\ding{172} Single Graph Shared Communities Task (\SGSC), 
%\ding{173} Single Graph Disjoint Communities Task (\SGDC), 
%\ding{174} Multiple Graphs from One Domain Task (\MGOD), and   
%\ding{175} Multiple Graphs from Different Domains Task (\MGDD).
%For \SGSC, \SGDC and \MGDD, one task is generated by sampling a subgraph of 200 nodes by BFS.
%The query nodes are randomly drawn from a sampled subgraph, $G$, where we assign 1 or 5 query nodes to the support set $\support$, i.e., 1-shot or 5-shot tasks, and assign 30 query nodes to the query set $\query$ disjointly.  It is worth noting that the query nodes may be from the same ground-truth communities for \SGSC whereas the query nodes must be from disjoint communities for \SGDC.
%For each query $q$, we randomly drawn 5 positive samples from the community of $q$, $\mathcal{C}_{q}(G)$, to construct $l_q^{+}$ and 10 negative samples from $V(G) \setminus \mathcal{C}_{q}(G)$ to construct $l_q^{-}$.
%Here, for \SGSC and \SGDC, we generate 100 training tasks for \Cora, \Citeseer, \Arxiv, \Reddit and \DBLP, and generate 50 valid tasks and 50 test tasks for the five datasets, respectively.
%For \MGOD, we use one \Facebook ego-network as the graph in one task, and sample the same numbers of queries and labels as discussed above. Ten tasks are split into 6 for training, 2 for validation, and 2 for testing.


%96  AMD EPYC 7413 24-Core Processor
%32  Intel(R) Xeon(R) Silver 4215 CPU @ 2.50GHz
%$film encoder 128 hidden dimension; film: gate plain

\subsection{Problem Statement}
We focus on an undirected simple graph ${\mathcal G} = (\mathcal V, \mathcal E, \mathcal A)$, where $\mathcal{V}$ is the node set, $\mathcal{E} \subseteq \mathcal V \times \mathcal V$ is the edge set and $\mathcal{A}$ is the set of node attributes. 
The nodes may possess $d$ attributes $\mathcal{A} = \{ \mathcal{A}_1, \cdots, \mathcal{A}_d\}$. For each node $v$, a one-hot $d$-dimensional vector $\mathcal{A}(v) \in \{0,
1\}^d$ encodes whether $v$ is associated with the $d$ attributes in
$\mathcal{A}$. 
The neighborhood of node $v$ is denoted as $\mathcal{N}(v) = \{ u |
(u, v) \in \mathcal{E} \}$.  
%In the following, we use ${\bf G}$ to represent a large data graph.  
A \emph{community} in ${\mathcal G}$ is a cohesive subgraph $\community = (V_C, E_C, A_C)$ where $V_C \subseteq \mathcal{V}$ and $A_C \subseteq \mathcal{A}$, such that the nodes set $V_C$ are intensively connected within $G_C$ whereas are sparsely connected with other nodes in the graph, i.e., $|E| \gg |\{ (u, v) | u \in V_C, v \in \mathcal{V} \setminus V_C \}|$. Below, we denote a community as an induced subgraph in a graph $\mathcal G$ by $\community_q$. The query $q=<V_q,A_q>$ consists of a query node set $V_q$ and a query attribute set $A_q$, where $\community_q$ denotes the corresponding community of $q$.

\stitle{Community Search.}
 Consider a graph $G=(V, E, A)$, where $n = |V(G)|$ and $m = |E(G)|$. Given a user-given query $q =<V_q, A_q>$, such that $V_q \subseteq V(G)$ and $A_q \subseteq A(G)$, the attributed community search problem aims to find the query-dependent community $\community_q(G)$ that effectively maintains both structural cohesiveness and attribute homogeneity.
Distinguished from prior algorithmic approaches~\cite{ATC, ACQ, CTC}, the community $\community_q(G)$ in this paper is not limited to any specific $k$-related subgraph. Instead, it is learned based on the provided ground-truth information regarding community membership.

\subsection{Inductive Learning}
Inductive learning involves generalizing patterns from a given set of labeled training instances to make predictions on unseen or instances. It aims to learn a general model that can effectively predict for new instances based on the patterns observed in the training data.
However, community search is intrinsically transductive, meaning that the learned latent patterns for communities are not generalizable to unseen communities during the training. When new communities come, it often requires a complete retraining to get the new latent patterns.
Compared to the transductive setting, the inductive learning problem poses a particular challenge, as generalizing to unseen graphs/communities requires model to capture hidden patterns of unseen communities.
Therefore, we encourage model to learn the shared latent representations across different communities or graphs. By capturing common patterns, generalization ability to new communities/graphs of the model can be improved. In this way, we can achieve inductive attributed community search.

\stitle{Inductive Attributed Community Search.}
The inductive attributed community search problem is to construct a model $\model$ to support attributed community search queries in unseen data. In order to demonstrate inductive ACS in detail, We first introduce a definition of task.

\stitle{Task.} The task can be represented as a triplet $\task = (G, Q, L)$, where $G$ can be a subgraph extracted from a large data graph ${\mathcal G}$ or one of multiple graphs, such as one specific ego-network from facebook social network. $Q= \{ q_1, \cdots, q_{j} \}$ is a set of $j$ queries within the graph $G$, and $L = \{ l_{q_1}, \cdots, l_{q_j}\}$ provides the ground-truth of the $j$ queries, respectively. For each query $q$, $l_{q}$ is a non-empty set of nodes in $G$ that contains a set of positive samples, $l_q^+ \subset \community_{q}(G)$, and a set of negative samples, $l_q^{-} \subset (V(G) \setminus \community_{q}(G)$).

The model $\model$ is trained on a set of training tasks $\mathcal{D} = \{ \task_i\}_{i = 1}^{N}$ and is applied to new test tasks.  
When a new test task $\task^*= (G^*, Q^*, L^*)$ comes, the model $\model$ will exploit the query node set $Q^*$ associated with the limited ground-truth $L^*$ to adapt to task $\task^*$, and can make community search predictions for any queries in $G^*$. Note that for test task, the number of queries in $Q^*$, named shots, is rather limited, i.e., $|Q^*| \ll |V(G^*)|$. 
Training on multiple tasks help the model learn a more generalized representation of the underlying community patterns. By exposing the model to diverse tasks, it learns to extract common features and generalize across different scenarios. This enhances the model's ability to adapt to new, unseen communities/graphs and make accurate predictions.
Data scarcity is a common challenge in real world application. By leveraging information from multiple tasks, the model can learn from more diverse community structures and attribute cohesiveness, even if single task has limited data. It can lead to more robust and accurate models, especially in scenarios where collecting large amounts of community membership for each specific task is difficult.
In addition, training by multiple tasks enables the model to incrementally learn and adapt to new tasks without forgetting previously learned knowledge. The model can continuously update its parameters and incorporate new information while retaining the knowledge learned from previous tasks. This capability is particularly valuable in scenarios where new tasks continually emerge, i.e. social networks that expand and give rise to new communities over time.

In this paper, we explore the inductive attributed community search setting regarding two perspectives: inductive learning for new communities and new graphs:

\stitle{Inductive for Communities.} 
The trained model can be used for new communities which are not seen in training stage, such that $\community_{q}(G) \cap \community_{q^*}(G^*) = \emptyset$, for all $q \in Q$ and $q^* \in Q^*$. As Fig.~\ref{fig:inductivetask} shows, for a large data graph $\mathcal{G}$, multiple tasks can be generated by splitting the data graph into disjoint part. The model is trained on training tasks and its inductive ability is reflected in its capability to search for unseen communities.

\stitle{Inductive for Graphs.}
Inductive learning for graphs poses additional challenges since it requires model to adapt and capture latent patterns in new graphs. The trained model should be applicable to new graphs that are not present during the training stage. In bottom line of Fig.~\ref{fig:inductivetask}, the graphs in any training task, $G$, and test task, $G^*$, are from different data graphs. Furthermore, the communities present in the test graphs are also novel and not encountered during training.



\comment{
\stitle{Node Attribute Encoding.}
To establish connection between node and attributes, previous work~\cite{AQDGNN} create bipartite graph containing two types of nodes, i.e., graph node set and attribute node set. However, the bipartite graph fails to incorporate graph structure knowledge, which hinder the investigation of the correlation between structure and attribute.
We construct an attribute-augmented graph to fully capture the correlation between the representations of nodes and attributes by encoding the topological structure of the data graph together with its nodes attributes.
In order to enhance the query graph encoding and improve performance, it has been observed that providing a pre-trained and unsupervised embedding as node features to the GNN can be beneficial. Therefore, we pre-train a node-attribute embedding for the original large data graph $\mathcal G$.
To preserve the topological property of the data graph $\mathcal G$ together with its attribute $\mathcal{A}$, we construct an attribute-augmented graph $\mathcal{G}_\mathcal{A} = (\mathcal{V} \cup \mathcal{V}_\mathcal{A}$, $\mathcal{E} \cup \mathcal{E}_\mathcal{A})$ for ${\mathcal G} = (\mathcal V, \mathcal E, \mathcal A)$. 
Here, $\mathcal{V}_\mathcal{A}$ is a set of nodes where each node represents an attribute in $\mathcal{A}$ and the number of nodes in $\mathcal{V}_\mathcal{A}$ is $|\mathcal{A}|$.
$\mathcal{E}_\mathcal{A}$ is a collection of edges in which, if node $v$ possesses an attribute $\mathcal{A}_l$, there exists one edge connecting node $v$ with the attribute node $v_{\mathcal{A}_l}$.
Fig.~\ref{fig:attrauggraph} shows the attribute-augmented graph $\mathcal{G}_\mathcal{A}$ for the left graph $\mathcal{G}$. ProNE~\cite{zhang2019prone}, a scalable and task-independent graph embedding is employed to pre-train a node embedding for the attribute-augmented graph $\mathcal{G}_\mathcal{A}$.
With the pre-trained attribute embedding $e_{\mathcal{A}}$ in large data graph $\mathcal{G}$, we can encode feature of each node in each task $\task=(G,Q,L)$, where $G=(V,E,A)$ and $A \subseteq \mathcal{A}$. For node $v$ in $G$, we set $e(v)=\sum_{a \in {A}(v)}e_{a}$, where $e_a$ is the pre-trained embedding of attribute $a \in \mathcal{A}$.



\stitle{Query Node Encoding.}
The query node set $V_q$ is encoded as a one-hot vector $I_{V_q} \in \mathbb{R}^n$ where the $v$-th bit is set to one if node $v$ belongs to the query node set $V_q$, as shown in Eq.~\ref{eq:encoder:querynode}.


\begin{align}
	\label{eq:encoder:querynode}
	I_{V_q}(v) =
	\begin{cases}
		1 & {v \in l_q^{+} \cup \{V_q\}  },  \\
		0 & \text{otherwise}.
	\end{cases}
\end{align}

%For each query node $q \in Q$ and its corresponding ground-truth $l_q \in L$, the encoder $\phi_{\theta}(q, l_q, G)$ is a $K$-layer GNN that maps the pair $(q, l_q)$ together with the graph $G$ to a node embedding matrix $H_q = \{ h_{v}^{(K)}\}_{v \in V(G)} \in \mathbb{R}^{n \times d^K}$. 
%Here, $h_{v}^{(K)}$ is a $d^K$-dimensional output of the $K$-th layer of GNN for node $v$. 
%The subscript $q$ of $H_q$ indicates the node embedding $H_q$ is generated particularly for query node $q$, as all the query nodes in $Q$ share the same GNN encoder.
%Specifically, as the inputs of GNN, the adjacency matrix of graph $G$ is used for message passing of GNN, and $(q, l)$ determines the initial node $h_v^{(0)}$ as Eq.~(\ref{eq:cnp:encoder:feats}), where $\|$ is the vector/bit concatenation operation, $\mathcal{A}(v)$ is the attribute features of node $v$. In Eq.~(\ref{eq:cnp:encoder:feats}), $I_l(v) \in \{0, 1\}$ is a binary ground-truth identifier which distinguishes nodes within and without a same community, under the close world assumption. 
%We can concatenate auxiliary features, e.g., the core number and local clustering coefficient of node $v$ on $h_v^{(0)}$ to exploit additional structural information.
%The intuition of the GNN encoder is to generate a view, $H_q$, for the whole graph given an observation $(q, l_q)$ by message passing. 
%A collection of views will be aggregated by the commutative operation big $\oplus$. This idea is enlightened by a CNP specialization for 3D scene understanding and rendering, Generative Query Network (GQN)~\cite{GQN}, where few-shot observed 3D views are summed up for predicting the view of a new query perspective. It is worth mentioning that, to the best of our knowledge, we are the first to introduce the insight of GQN to graph domain. 


\stitle{Concatenated Feature Input.}
\begin{align}
    h_{q}(v)^{(0)} &= [ I_{V_q}(v) \| e(v) ] \label{eq:encoder:concate}
\end{align}
}

\comment{
\begin{align}
	\label{eq:pool:att:trans}
	\mathcal{H}_{1} &= \mathcal{H} W_{1},~ \mathcal{H}_{2} = \mathcal{H} W_{2},~\\
	\label{eq:pool:att:weight}
	\{w_q \}_{q \in Q} &= \softmax \biggl(\frac{ \langle\mathcal{H}_{1},  \mathcal{H}_{2}^T \rangle}{\sqrt{d'}} \biggr).
\end{align}
}

\comment{
\begin{align}
	\label{eq:ada:film}
 \begin{aligned}
	&\gamma =W_\gamma \cdot H',~ \beta=W_\beta \cdot H', \\
     &H = \gamma\odot H'+\beta.
 \end{aligned}
\end{align}
}

%element-wise multiplication \odot
\comment{
\begin{align}
	\label{eq:ada:film}
 \begin{aligned}
	&\gamma =W_\gamma \cdot H',~ \beta=W_\beta \cdot H', \\
     &H = \gamma\odot H'+\beta.
 \end{aligned}
\end{align}
}

\comment{
\begin{align}
	\label{eq:ada:filmgate}
 \begin{aligned}
	&\gamma' =W_\gamma \cdot H',~ \beta'=W_\beta \cdot H', \\
        &\delta =\sigmoid(W_\delta H'),~ \epsilon =W_\epsilon \cdot H',\\
        &\gamma =\gamma' \odot \delta+\epsilon \odot (1-\delta),\\
        &\beta =\beta' \cdot \delta, \epsilon \odot (1-\delta),\\ 
     &H = \gamma\odot H'+\beta.
\end{aligned}
\end{align}
}

\comment{
\begin{align}
	\label{eq:decoder:queryattr}
	I_{A_q}=\sum_{a \in {A}_q} e_{a}
\end{align}



\begin{align}
    O_q &= \MLP [ h_{V_q}  \| I_{A_q} ] \label{eq:decoder:conca}
\end{align}
where, $h_{V_q}=\frac{1}{|V_q|}{\sum_{v_q \in V_q}H(v_q)}$ is the average pooling representation of query nodes in $V_q$.


\begin{align}
	\label{eq:decoder:non-attr:product}
	p(\hat{l_{q}} | q, \task) = \sigmoid(\langle h_{V_q}, H \rangle) 
\end{align}

\begin{align}
	\label{eq:decoder:attr:product}
	p(\hat{l_{q}} | q, \task) = \sigmoid(\langle O_q, H \rangle) 
\end{align}
}

%It may because \IACS capture the structure cohesiveness and attribute homogeneity more sensitivity, while they may search some unrelated nodes with the community. This property can be helpful to mining underlying communities.


\comment{
\begin{table}[t]
{\small %\scriptsize
\caption{Existing Community Search Approaches}
\label{tbl:summary}
\vspace*{-0.4cm}
\begin{center}
    \begin{tabular}{c c c c c} \toprule
     {Approach} & \makecell[c]{Single-node \\ Query}   & \makecell[c]{Multi-node \\ Query} & \makecell[c]{ Attributed \\ Query} & \makecell[c]{ Inductive}
    \\ \midrule                                            
   \CTC~\cite{CTC} &  &   &  &   \\ 
   \ATC~\cite{ATC} &  &  &  &     \\
   \ACQ~\cite{ACQ} &  &  &  &   \\
   \AQDGNN~\cite{AQDGNN} &  &  &  &   \\
   \ICSGNN~\cite{ICSGNN} &  &  &  &   \\
   \CommunityAF~\cite{communityAF} &  &  &  &   \\
   \COCLEP~\cite{coclep}  &  &  &  &  \\
   \CGNP~\cite{cgnp} &  &  &  &   \\
   \IACS (Ours)  & &  &  &  \\
   \bottomrule
   \end{tabular}
\end{center}
}
%\vspace*{-0.2cm}
\end{table}
}


\begin{algorithm}[!t]
	%\footnotesize
	\small
    \caption{IACS Inference Phase}
	\label{alg:test}
	\DontPrintSemicolon
	\SetKwData{Up}{up}  \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output}
	\Input{test task $\mathcal{T^*}=(G^*, Q^*, L^*)$, parameter $\theta^*$ of meta model $\mathcal{M}$, self-adaptive task-level embedding $\hat{H}$, new query $q^*$} %\in V(G^*) \setminus Q^*$}
	\Output{predictive probability of $q^*$}
	\SetKwFunction{Emit}{Emit}
	\SetKwFunction{Check}{Check}
	%$\support^* \leftarrow (Q^*, L^*)$; \label{line:test:support-query:split} 
	
	%\For { $(q, l_q) \in \support^*$}
	%{ \label{line:test:encoder:start}
	%	$H_q\leftarrow \phi_\theta(q,l_q, G^*)$; \label{line:test:encoder:end}  \algocomment{compute query-level embedding} \; 
	%}
	%$H \leftarrow \bigoplus_{q \in \support^*} H_q$;\label{line:test:agg} \algocomment{compute task-level embedding} \;
	$p(\hat{l_{q^*}} | q^*, \support^*) = \sigmoid \left( \langle e_{q^*}, \widehat{H} \rangle \right) $; \label{line:test:decoder}  \algocomment{compute predictive probability} \; 
	\Return{$p(\hat{l_{q^*}} | q^*, \support^*)$}; \;
\end{algorithm}






\comment{
\begin{table}[t]
	\caption{Profile of Datasets}
	\label{tab:dataset}
	\vspace{-0.2cm}
	\centering
	%\resizebox{0.5\textwidth}{!}{
		\begin{tabular}{c|c|r r r r}
			\toprule
			\multicolumn{2}{c|}{\textbf{Dataset}}  & \textbf{$|\VSet|$} &\textbf{ $|\ESet|$}  & \textbf{$\#$.} & \textbf{$\#$ Comm.} \\\midrule       
            \multicolumn{2}{c|}{\Arxiv}   &169,343 &1,166,243  &N/A  &40   \\
            \multicolumn{2}{c|}{\Product}   &2,449,029 &61,859,140  &N/A  &47   \\\midrule
                \multicolumn{2}{c|}{\Cora}   & 2,708 & 5,429  & 1,433  &7\\
			\multicolumn{2}{c|}{\Citeseer}   & 3,327 & 4,732 & 3,703 & 6  \\
            \multicolumn{2}{c|}{\Reddit}   &232,965 &114,615,892 &1,164 & 50\\
            \multicolumn{2}{c|}{\Twitter}    & 81,306 & 1,768,149  & 512,985  & 4,065 \\ \midrule
			\multirow{10}{*}{\begin{tabular}[c]{@{}c@{}}\Facebook \end{tabular}} 
			&0   & 348 & 2,867  & 224 & 24 \\ 
			&107 & 1,046    & 27,795  &  576   & 9  \\
			& 348 & 228    &3,420  &   162  &  14   \\
			& 414    & 160    & 1,853  & 105  & 7   \\
			& 686 & 171    &1,827  &   63   &  14  \\
			& 698 & 67    &337  &   48   &  13  \\
			& 1684 & 793    & 14,817  &   319 & 17\\
			& 1912 & 756    &30,781  &   480 & 46\\
			& 3437 & 548    &5,361  & 262  & 32\\
			& 3980 & 60    &206  &  42  & 17\\ 
            \bottomrule
	\end{tabular}
	%}
\end{table}
}


%We use $7$ real-world graph datasets, including $5$ single graphs (\Arxiv, \Product, \Cora, \Citeseer, \Reddit) and $2$ multiple graphs (\Twitter, \Facebook). Table~\ref{tab:dataset} lists the profile of the seven datasets.
%Since \Arxiv and \Product do not have discrete node attributes, we conduct non-attribute community search on the two datasets. To input feature of GNN encoder, we provide $100$-dimensional node features which are generated by extracting bag-of-words features from the product descriptions for \Product and $128$-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract for \Arxiv.    
%\Product~\cite{chiang2019cluster} is an Amazon product co-purchasing network where nodes indicate products sold in Amazon and edges between two products indicate that the products are purchased together. 
%\Cora, \Citeseer~\cite{yang2016revisiting} and \Arxiv~\cite{wang2020microsoft} are citation networks where nodes represent research papers and edges represent citation relationships. They utilize node class labels to simulate communities based on paper citations, providing insights into the research topics to which the papers belong. The attributes of \Cora and \Citeseer are the keywords in the papers.
%\Reddit~\cite{hamilton2017inductive} is collected from an online discussion forum, where nodes refer to posts, and an edge between two posts exists if a user comments on both of the posts. The ground-truth is the communitiesthat posts belong to. Since \Reddit have no attributes, we generate an attribute set containing $|\mathcal{A}|=0.005\cdot|\mathcal{V}|$ attributes. For the details of attribute distribution, we follow the setting of ATC~\cite{ATC}. 
%\Facebook~\cite{leskovec2012learning} and \Twitter~\cite{twitter} are multiple ego-network dataset, containing 10 and 973 ego-networks, respectively. The edges encode follower relationships and the ground-truth community is their friend circle. The attributes of \Facebook and \Twitter are the user profiles. 


\comment{
%For single graph dataset, we utilize METIS graph partition to split graph.
Since \Cora and \Citeseer possess a small amount of nodes, we adopt a BFS-sampling strategy from $3$ partition set ($6:1:3$) to generate $128$ training, $32$ validation and $32$ test subgraphs where the size of each subgraph is approximately $200$.
For \Arxiv and \Reddit, we directly split the whole graph into $700$ training, $100$ validation and $200$ test subgraphs by METIS graph partition algorithm. \Product is a million scale dataset, in order to satisfy the similarity size of subgraphs, we designate 3000 subgraphs for training, 1000 subgraphs for validation, and 1000 subgraphs for testing in the partition way.
\Twitter and \Facebook contain multiple ego-networks, $60\%$, $10\%$, $30\%$ ego-graphs are assigned to training, validation and test set. 
This process serves to evaluate the performance when the model is exposed to previously unseen subgraphs and communities. We also provide insights into model's generalization abilities to different graph. Thus we design a \Coraciteseer dataset, which we generate $128$ training subgraphs, $32$ validation subgraphs from \Cora and $32$ test graphs from \Citeseer.
We regard each subgraph as a task. The training tasks are used to train our model, validation tasks are used to select the optimal training epoch and query set $\query$ of test tasks are employed to evaluate the performance. 
}

%For each task, we generate 20 or 24 pairs of query data where we assign 4 or 8 pairs to the support set $\support=\{q_i,l_{q_i}\}_{i=1}^{|\support|}$, i.e., 4-shot or 8-shot tasks, and assign 16 pairs to the query set $\query=\{q_i,l_{q_i}\}_{i=1}^{16}$. 
%Each query $q_i=<V_{q_i},A_{q_i}>$ contains query node set $V_{q_i}$ and the query attribute set $A_{q_i}$. 
%We randomly select $1\sim3$ nodes from the ground-truth community as the query node set $V_{q_i}$. In addition, we select the 3 most frequently occurring attributes in ground-truth communities and randomly select $1\sim3$ of these attributes as query attribute set $A_{q_i}$.
%$l_{q_i}\in \{0,1\}^n$ denotes the corresponding ground-truth of $q_i$. We randomly drawn $5\%$ number of nodes to construct $l_{q_i}^{+}$ and $l_{q_i}^{-}$. For non-attributed community search, we set the attribute query set empty $A_{q_i}=\varnothing$.

%\ding{182} Interactive Community Search via GNN (\ICSGNN)~\cite{ICSGNN} is for non-attributed community search. For each query node $q$, a GNN model is trained using positive and negative samples and predicts a score for the remaining nodes. Then, the algorithm is employed to identify a subgraph connected to $q$ with a fixed number of nodes. The objective is to maximize the sum of scores predicted by GNN.
%\ding{183} CommunityAF (\CommunityAF)~\cite{communityAF} is an example-based non-attributed community search method. It needs to train a GNN to learn node embeddings and utilizes an autoregressive flow-based generation component for fast training. A scoring component can evaluate the communities and provide scores for a stable termination.

%The framework can support both non-attributed (\QDGNN) and attributed community search (\AQDGNN). \QDGNN combines the local query-dependent structure and global graph embedding and \AQDGNN extends to process the structure and attribute simultaneously by modeling node attributes as bipartite graph.

%
%Supervised GNN (\Supervise) is trained by several query nodes in a task to predict the score of nodes for Non-ACS. And we extend the model to support ACS by concatenating query attributes to original input feature matrix.


\comment{
In order to ensure a fair comparison, \MAML and \Featrans adopt the task and query generation setting from \IACS entirely. On the other hand, for \Supervise and \AQDGNN, %\CommunityAF
we employ the support set, $\support$ to train a model for each task and subsequently test it on the corresponding query set, $\query$. However, for \CTC, \ATC, \ACQ and \ICSGNN, which are non-inductive methods, we only make predictions on the query set $\query$ of the test task. Table~\ref{tab:task setting} shows the task setting for \IACS and all the baselines in detail. Moreover, \ACQ %, \CommunityAF 
and \ICSGNN only support single query node, we randomly choose $1$ query node when $|V_{q_i}|>1$ for evaluation. 
}
%We compare with $6$ baseline approaches for non-attributed community search and $6$ baseline approaches for atttributed community search to comprehensively evaluate the performance of \IACS. There are $3$ traditional graph algorithms, $4$ ML/DL-based approaches and $2$ naive meta-learning approaches for inductive graph learning. 
\comment{
\begin{itemize}
[leftmargin=*,itemsep=0pt,topsep=0pt,parsep=0pt]
\item \textbf{Traditional graph algorithms.}
\ding{182} Closest Truss Community (\CTC)~\cite{CTC} is a $k$-truss based framework for non-attributed community search. The goal is to find the $k$-truss with the largest $k$ that includes query nodes and has the minimum diameter among the truss. 
\ding{183} Attributed Truss Community Search (\ATC)~\cite{ATC} is an attributed community search algorithm. Given query nodes and query attributes, it finds the maximal $(k, d)$-truss containing the query nodes and iteratively removes unpromising nodes from the truss to get max attribute score.
\ding{184} Attributed Community Query (\ACQ)~\cite{ACQ} is for attributed community search, aiming to identify subgraph whose nodes are tightly connected and share common attributes with the given query node. 
\item \textbf{ML/DL-based methods.}
\ding{182} Interactive Community Search via GNN (\ICSGNN)~\cite{ICSGNN} is for non-attributed community search. For each query node $q$, a GNN model is trained using positive and negative samples and predicts a score for the remaining nodes. Then, the algorithm is employed to identify a subgraph connected to $q$ with a fixed number of nodes. The objective is to maximize the sum of scores predicted by GNN.
%\ding{183} CommunityAF (\CommunityAF)~\cite{communityAF} is an example-based non-attributed community search method. It needs to train a GNN to learn node embeddings and utilizes an autoregressive flow-based generation component for fast training. A scoring component can evaluate the communities and provide scores for a stable termination.
\ding{183} Query Driven-GNN (\QDGNN \& \AQDGNN) \cite{AQDGNN}. The framework can support both non-attributed (\QDGNN) and attributed community search (\AQDGNN). \QDGNN combines the local query-dependent structure and global graph embedding and \AQDGNN extends to process the structure and attribute simultaneously by modeling node attributes as bipartite graph.
\ding{184} Supervised GNN (\Supervise) is trained by several query nodes in a task to predict the score of nodes for Non-ACS. And we extend the model to support ACS by concatenating query attributes to original input feature matrix.
\item \textbf{Meta-learning methods.}
\ding{182} Model-Agnostic Meta-Learning (\MAML)~\cite{MAML}. We use GNN as the base model. The task-specific parameters of GNN are updated in an inner loop and the task-common parameters are updated in an outer loop as over all training tasks. We can also extend the model to support both non-ACS and ACS following the model design of \Supervise.
\ding{183} Feature Transfer (\Featrans). A base GNN model is
pre-trained on all the training tasks. For a test task, the final layer of the GNN is finetuned on the support set by one gradient step, while all the other parameters are kept intact. The same extension is made following \Supervise to satisfy both non-ACS and ACS.
\end{itemize}
In order to ensure a fair comparison, \MAML and \Featrans adopt the task and query generation setting from \IACS entirely. On the other hand, for \Supervise and \AQDGNN, %\CommunityAF
we employ the support set, $\support$ to train a model for each task and subsequently test it on the corresponding query set, $\query$. However, for \CTC, \ATC, \ACQ and \ICSGNN, which are non-inductive methods, we only make predictions on the query set $\query$ of the test task. Table~\ref{tab:task setting} shows the task setting for \IACS and all the baselines in detail. Moreover, \ACQ %, \CommunityAF 
and \ICSGNN only support single query node, we randomly choose $1$ query node when $|V_{q_i}|>1$ for evaluation. 
}



\comment{
\begin{table}[t]
\caption{Task Setting for Our Method and All the Baselines}
	\label{tab:task setting}
	\vspace{-0.2cm}
	\centering
	\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Training Task}                                                & \multicolumn{2}{c}{Test Task}                                         \\\cline{2-5}
                         & Support Set $\support$                             & Query Set $\query$                           & Support Set $\support$                             & Query Set $\query$                             \\\midrule
\CTC                      & \multicolumn{3}{c|}{\multirow{3}{*}{\textbackslash{} }}                                                              & \multirow{3}{*}{inference}          \\
\ATC                      & \multicolumn{3}{c|}{}                                                                                               &                                     \\
\ACQ                      & \multicolumn{3}{c|}{}                                                                                               &                                     \\\midrule
\AQDGNN                   & \multicolumn{2}{c|}{\multirow{2}{*}{\textbackslash{} }}                            & \multirow{2}{*}{\shortstack{training a model $\mathcal{M}^*$\\ for each task}}       & \multirow{2}{*}{\shortstack{inference by \\trained model $\mathcal{M}^*$} }         \\
\Supervise                & \multicolumn{2}{c|}{}                                                             &                                 &                                     \\\midrule
%\CommunityAF              & \multicolumn{2}{c|}{}                                                                                               &                                     \
\ICSGNN                   & \multicolumn{3}{c|}{\textbackslash{} }                                                             & {training a model $\mathcal{M}^*$ for each query and inference} \\\midrule
\MAML                     & \multicolumn{2}{c|}{\multirow{3}{*}{training a model $\mathcal{M}$}} & \multirow{3}{*}{\shortstack{fine-tune model $\mathcal{M}$\\ for each task}} & \multirow{3}{*}{\shortstack{inference by \\fine-tuned model $\mathcal{M}^*$}}          \\
Transfer                 & \multicolumn{2}{c|}{}                                                             &                                 &                                     \\
\IACS                     & \multicolumn{2}{c|}{}                                                             &                                 &                                 \\\bottomrule   
\end{tabular}}
\end{table}
}

\comment{
We give the settings of 6 ML approaches, including our \IACS and 5 baselines, \MAML, \Featrans, \Supervise, \ICSGNN %, \CommunityAF 
and \AQDGNN.
For the GNN encoder of \IACS and the base GNN models of the 7 baselines, we set the number of the GNN layers as 3 where each GNN layer has 128 hidden units and a Dropout probability of 0.2 by default.
The FiLM module contains 128 hidden dimension.
For the decoder of \IACS, we use a two-layer MLP with 128 hidden units. We set default finetune step as $10$ for \IACS. 
}
\comment{
We train our model and other baselines 200 epochs by default. In addition, we choose the best model by validation set to retain for finetune in test set.
The learning framework of \IACS and the 6 ML baselines are built on PyTorch~\cite{pytorch} with PyTorch Geometric~\cite{torchgeo}.
We use Adam optimizer with a learning rate of $10^{-3}$ to train \IACS, \Supervise and \Featrans. For \MAML the inner loop performs 10 gradient steps for training and 20 steps for testing, with a learning rate of $10^{-3}$, and the learning rate for the outer loop is $10^{-3}$.
Other parameters for baselines are their default.
It is worth mentioning that the performance of \IACS is robust in the range of empirical training hyper-parameters. By default, the training and prediction are conducted on a NVIDIA A100 with 80GB memory. \ATC, \ACQ and \CTC are tested on the same Linux server with 96 AMD EPYC 7413 CPUs and 512GB RAM.
}


\comment{
Suppose $\hat{l} \in \{0, 1\}^{n}$ and $l \in \{0, 1\}^{n}$ are the binary representations of the prediction result and the ground-truth of a query $q$. Precision, recall and \Fone-score are defined as below:
\begin{align}
	\nonumber
	\Pre(\hat{l},l)&=\frac{\sum_{v \in V} \hat{l}(v) \& l(v)}{\sum_{v \in V}\hat{l}(v) }, 
	\Rec(\hat{l},l)=\frac{\sum_{v \in V} \hat{l}(v) \& l(v)}{\sum_{v \in V}{l(v)}}  \\
	\nonumber
	\Fone(\hat{l},l)&=\frac{2\cdot \Rec(\hat{l},l)\cdot \Pre(\hat{l},l)}{ \Rec(\hat{l},l) + \Pre(\hat{l},l)}
\end{align}
}


\comment{
\begin{table*}[t]
{\small
\centering
\caption{Characteristics Comparison with Existing Community Search Methods.}
\label{tab:intro}
\begin{tabular}{c|ccc|cccccc}
\toprule
\multirow{2}{*}{Characteristics}             & \multicolumn{3}{c|}{Algorithmic Approach} & \multicolumn{6}{c}{Learning-based Approach}                        \\ %\cline{2-10}
   & \CTC~\cite{CTC}         & \ATC~\cite{ATC}                 & \ACQ~\cite{ACQ}                 & \AQDGNN~\cite{AQDGNN} & \ICSGNN~\cite{ICSGNN} & \CommunityAF~\cite{communityAF} & \COCLEP~\cite{coclep} & \CGNP~\cite{cgnp} & \IACS \\ \midrule
Single-node Query       &\ding{52}         & \ding{52}                 & \ding{52}                 & \ding{52}     & \ding{52}     & \ding{52}         & \ding{52}    & \ding{52}  & \ding{52}              \\
Multi-node Query     &\ding{52}            & \ding{52}                 & \ding{56}                  & \ding{52}     & \ding{56}      & \ding{56}          & \ding{56}     & \ding{56}   & \ding{52}              \\
Attributed Query     &\ding{56}             & \ding{52}                 & \ding{52}                 & \ding{52}     & \ding{56}      & \ding{56}          & \ding{56}     & \ding{56}   & \ding{52}              \\
Induction &\ding{56} & \ding{56}    & \ding{56}    & \ding{56}      & \ding{56}      & \ding{52}         & \ding{52}    & \ding{52}  & \ding{52}   \\ \bottomrule         
\end{tabular}
}
\end{table*}
}


%Community search (CS)~\cite{fang2020survey, huang2019community} is a fundamental problem in network analysis that focuses on identifying cohesive communities covering the given query nodes. Attributed community search (ACS)~\cite{ACQ,ATC} extends the concept of CS by considering both query nodes and query attributes. Compared to CS, ACS entails additional challenges, as it involves not only identifying structural cohesiveness but also incorporating attribute homogeneity constraints, such as keywords~\cite{ACQ,ATC}, location~\cite{wang2018efficient}, temporal~\cite{li2018persistent}, etc.
%ACS serves as a crucial building block of various real applications, such as social network analysis, recommendation systems, bioinformatics and fraud detection.

Existing approaches for CS can be generally classified into algorithmic approaches~\cite{CTC,ACQ,ATC} and learning-based approaches~\cite{AQDGNN,ICSGNN,communityAF,coclep,cgnp}.
CTC~\cite{CTC} focuses on finding close $k$-truss communities containing query nodes. ATC~\cite{ATC} extends CTC by incorporating attribute constraints to support attributed community search.
ACQ~\cite{ACQ} is another algorithmic approach that supports ACS but it is not able to handle multi-node query. Both ATC and ACQ typically follow a two-stage process, where candidate communities are initially identified based on structural cohesiveness, and then further refined using attribute homogeneity constraints.
However, these algorithmic approaches suffer from two limitations. On the one hand, they rely on predefined subgraph patterns like $k$-core~\cite{li2015influential,sozio2010community,cui2014local} and $k$-truss~\cite{huang2014querying,akbas2017truss}, which make them inflexible and heavily dependent on the hyper-parameter $k$. On the other hand, their two-stage processes treat structure and attribute independently, failing to capture the joint correlations between them.
To address the limitations of algorithmic methods, various learning-based methods have been proposed. 


