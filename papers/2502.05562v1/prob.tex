% \section{Preliminaries}
% \label{sec:preliminaries}

% \begin{table}[t]
% \footnotesize
% \centering
% \caption{Frequently-used Notations}
% \label{tab:notation}
% %\resizebox{0.5\textwidth}{!}{
% \begin{tabular}{c|c}
% \toprule
% Notation & Description                 \\\midrule
% $\Graph = (\VSet, \ESet, \ASet)$ & an attributed graph \\
% $n/m$  &number of nodes/edges of graph $\Graph$\\
% $ q = (\VSet_q, \ASet_q)$        & a query with node set $\VSet_q$ and attribute set $\ASet_q$\\
% $\community_q$ & the community containing query $q$ \\
% $\task = (\Graph, Q, L)$ & one task\\ %with a set of queries $Q$\\
% $l_q^+/l_q^-$ & positive/negative samples for query $q$\\
% $e_a$  & pre-trained attribute embedding for attribute $a \in \mathcal{A}$ \\
% $e(v)$ & original feature encoding for node $v$\\
% $I_l{v}$ & indicator whether the node $v$ is in the community \\
% $e_{\VSet_q}$ & average query nodes embeddings\\
% $e_{\ASet_q}$ & average query attributes embeddings\\
% $\support_i = (Q_i, L_i)$ & support set of a task $\task_i$\\
%  \bottomrule
% \end{tabular}
% %}
% \end{table}


% In this section, we first introduce the definitions of CS and ACS formally, and then formulate the inductive learning-based CS. Finally, we describe the existing GNN-based framework for CS as the technical background. Table~\ref{tab:notation} depicts the frequently-used notations and their descriptions. 

% \subsection{Definitions \& Concepts}
% An undirected simple graph $\Graph = (\VSet, \ESet)$ consists of a set of nodes, $\VSet$, and a set of undirected edges  $\ESet \subseteq \VSet \times \VSet$.
% Let $n = |\VSet|$ and $m = |\ESet|$ denote the number of nodes and edges, respectively. 
% The neighborhood of node $v_i$ is denoted as $\mathcal{N}(v_i) = \{ v_j |
% (v_j, v_i) \in \ESet \}$.

% \stitle{Community Search (CS).} For a graph $\Graph = (\VSet, \ESet)$, given a node set $\VSet_q \subseteq \VSet$ as a query $q$, the problem of Community Search aims to find the query-dependent community $\community_q \subseteq \VSet$, where the nodes in $\community_q$ are intensively intra-connected, i.e., maintaining cohesive structure.

% An undirected attributed graph $\Graph = (\VSet, \ESet, \ASet)$ has an additional attribute set 
% $\ASet$. Each node $v_i$ possesses its attribute set $\ASet_i$, and $\ASet$ is the union of all the node attribute sets, i.e.,
% $\ASet = \ASet_1 \cup \cdots \cup \ASet_n $.  

% \stitle{Attributed Community Search (ACS).} For an attributed graph, $\Graph = (\VSet, \ESet, \ASet)$, given a query $ q = (\VSet_q, \ASet_q)$ where $\VSet_q \subseteq \VSet$ is a set of query nodes, and $\ASet_q \subseteq \ASet$ is a set of query attributes, the problem of {Attributed Community Search}~(ACS) aims to find the query-dependent community $\community_q \subseteq \mathcal{V}$. Nodes in community $\community_q$ need to be structure cohesive and attribute homogeneous simultaneously, i.e., the nodes in the community are densely intra-connected in structure and the attributes of these nodes are similar.

% \stitle{Learning-based CS/ACS.} The general process of the learning-based approaches~\cite{ICSGNN,AQDGNN,coclep,cgnp,communityAF} consists of two stages, the training stage and the inference stage. In the training stage, for a graph $\Graph$, a parametric ML model $\mathcal{M}: q \mapsto [0, 1]^n$ is constructed offline from a set of queries and corresponding ground-truth communities.
% In the inference stage, for an online new query,  the model $\mathcal{M}$ predicts the likelihood of whether each node is in the community of the query, as a vector $\hat{y} \in [0, 1]^n$.
% The query supported can be non-attributed queries ($q = (\VSet_q, \emptyset)$) for CS and attributed queries ($ q = (\VSet_q, \ASet_q)$) for ACS.
% To be concise, we consider the ACS problem in this paper and regard CS as a special case of ACS ($\ASet_q = \emptyset$).
% Distinguished from prior algorithmic approaches~\cite{ATC, ACQ, CTC}, the community $\community_q$ discovered by learning-based approaches is not restricted to any specific $k$-related subgraph. %Instead, it is learned based on the provided ground-truth information regarding community membership.

% \subsection{Problem Statement}
% For existing learning-based ACS~\cite{AQDGNN}, the model $\mathcal{M}$ trained in a graph $\Graph$ is  expected to serve the same graph involving the same communities in the inference stage. 
% In this paper, we aim to explicitly empower the model to generalize and adapt to new communities and graphs by inductive learning, in the following two perspectives:

% \etitle{For new communities.} For a graph $\Graph$, given a set of training queries $Q =\{q_1, \cdots, q_i\}$ with corresponding ground-truth labels from the community set $\{ \community_{q_1}, \cdots, \community_{q_i}\}$, the model trained by $Q$ is used to answer query $q^*$ from a new community $\community_{q^*}$, 
% %TODO $\{\community_{q_1}, \cdots, \community_{q_i}\} \cap \community_{q^*}$???
% i.e., $\community_{q_1} \cap \community_{q^*} = \emptyset, \cdots,  \community_{q_i} \cap \community_{q^*} = \emptyset$. Furthermore, the graph $\Graph$ may even not contain the community $\community_{q^*}$, e.g., $\community_{q^*}$ is in a large online social network where $\Graph$ is a local subgraph extracted offline.

% \etitle{For new graphs.} For a graph $\Graph$, a model constructed from queries in $\Graph$ is used to answer queries from a new graph $\Graph^*$.

% \comment{
% \begin{example}
% Assume the data graph can be extracted into two subgraphs (shown in Fig.~\ref{fig:tasktop}), denoted as $\Graph_1$ and $\Graph_2$, and they contain node set $\VSet_1=\{v_1,v_2,v_3,v_4,v_5,v_6\}$ and $\VSet_2=\{v_7,v_8,v_9,v_{10},v_{11}\}$, respectively. On the one hand, $\community(\Graph_1)$ represents communities within $\Graph_1$, which consists of academic communities. On the other hand, $\community(\Graph_2)$ consists of musical communities. When the model is trained on $\Graph_1$ and can be adaptively applied to $\Graph_2$ for community search, we claim that the model demonstrates inductive ability for communities, since $\community(\Graph_1) \cap \community(\Graph_2)=\emptyset$. 
% Furthermore, the model demonstrates inductive ability for graphs if the model can be applied to another data graph, such as a graph containing sports communities as presented in Fig.~\ref{fig:taskbottom}.
% \end{example}
% }


% \begin{example}
% Fig.~\ref{fig:inductivetask} demonstrates a toy example of above two perspectives of inductive ACS. In Fig.~\ref{fig:tasktop}, the model is constructed by training data of an academic community (in orange),  and will be used for answering queries from a new musical community (in blue). Although the two communities are in a single large graph, their local structures and attribute sets are different. 
% In Fig.~\ref{fig:taskbottom}, the model is trained by a graph containing one community (on the left), and is  expected to answer ACS queries in a new graph (on the right).  
% \end{example}


% The challenges of model generalization on new communities and graphs lie in data heterogeneity, i.e., \emph{structural heterogeneity} and \emph{attribute heterogeneity}. For one thing, the topological structures are heterogeneous across different communities and graphs. 
% For the other thing, the attribute set, their semantics and distribution are heterogeneous across different communities and graphs.
% In general, data heterogeneity across different graphs would be more severe than that across different communities.
% To this end, in this paper, we construct a model $\model$ by inductive learning from multiple ACS tasks. 


% \stitle{ACS Task.} We formulate an ACS task as a triplet $\task = (\Graph, Q, L)$. And $\Graph = (\VSet, \ESet, \ASet)$ is an attribute graph, $Q = \{q_1, \cdots, q_i\}$ is a set of queries, i.e., $q_j = (\VSet_{q_j}, \ASet_{q_j})$, $\VSet_{q_j} \subseteq \VSet$, $\ASet_{q_j} \subseteq \ASet$, $\forall j \in [1, \cdots, i]$, $L = \{l_{q_1}, \cdots, l_{q_i} \}$ is the set of ground-truth of $j$ queries, correspondingly. Specifically, $l_q$ is a nonempty node set in $\Graph$ w.r.t. query $q$, containing a set of positive node samples, $l^{+}_q \subseteq \community_q$, and a set of negative samples, $l_q^{-} \subseteq (\VSet \setminus \community_{q}$).

% By inductive learning, the model $\model$ is trained on a set of training tasks, $\{\task_1, \cdots, \task_N\}$, and will be used in a new ACS task $\task^* = (\Graph^*, Q^*, L^*)$.  Here, $\Graph^*$ and the graphs in the training tasks are either different local subgraphs in a large graph, which do not have overlapping communities, or are fully different graphs. 
% Here, $Q^*$ and $L^*$ are a small number of queries with corresponding ground-truth for $\task^*$, i.e., $|Q^*| \ll |\VSet|$, which can be exploited by $\model$ to adapt to the specific task $\task^*$. The number of queries in $Q^*$, $|Q^*|$, is called shot in the following. 




% \begin{figure}
%     \centering
%    \begin{tabular}[h]{c}  
%   \subfigure[Inductive Setting for Communities]{
%     \includegraphics[width=0.44\textwidth]{fig/tasktop3.pdf}
%     \label{fig:tasktop}}
%     \vspace{-1ex}
%     \\
    
%   \subfigure[Inductive Setting for Graphs]{
%     \includegraphics[width=0.46\textwidth]{fig/taskbottom3.pdf}
%   \label{fig:taskbottom}}
%   \vspace{-1ex}
%   \\
%   \end{tabular}
  
%     \caption{Two Cases of Inductive ACS}
%     \label{fig:inductivetask}
% \end{figure}        

% \subsection{GNN for Learning-based ACS}

% Existing learning-based CS/ACS approaches~\cite{AQDGNN, communityAF,coclep, ICSGNN} employ GNN as the backbone of their models. A GNN of $K$-layers follows a neighborhood
% aggregation paradigm to generate a new embedding for
% each node by aggregating the embeddings of its neighbors
% in $K$ iterations. 
% Let $h^{(k)}(v)$ denote the embedding of node $v$ in the $k$-th iteration. In the $k$-th iteration (layer), an aggregate function $\Fagg^{(k)}$ aggregates the embeddings of the neighbors of $v$ generated in $(k - 1)$-th layer as Eq.~\eqref{eq:gnn:fagg}. Subsequently, a combine function $\Fcom^{(k)}$ updates the embedding of $v$ as Eq.~\eqref{eq:gnn:fcom}. The aggregate and combine functions of each layer are neural networks.
% %
% %GNN is recognized as a message passing procedure that aggregates the representations of neighboring nodes to generate a new representation for each node.
% %Most of existing works~\cite{AQDGNN, communityAF,coclep} formulate CS/ACS as a binary classification task and employ GNN to model structural cohesiveness and attribute homogeneity. 
% %Through $K$ layers GNN as described in Eq.~(\ref{eq:gnn:fagg})-(\ref{eq:gnn:fcom}), $h^{(K)}(v)$, the node representation is activated by a $\sigmoid$~function, denoted as, $\hat{y}(v) = \sigmoid(h^{(K)}(v))$, which represents the probability of node $v$ belonging to the same community as the query $q$.  
% \begin{align}
% 	a^{(k)}(v) &= \Fagg^{(k)}(\{ h^{(k - 1)}(u) | u \in \neighbor(v)\}), \label{eq:gnn:fagg} \\
% 	h^{(k)}(v)  &= \Fcom^{(k)}(h^{(k - 1)}(v), a^{(k)}(v)). \label{eq:gnn:fcom}
% \end{align}

% For dealing with ACS, the query information is injected into the initial node embedding, $h^{(0)}(v)$, by concatenating identifiers of query nodes and query attributes to the original node features as Eq.~\eqref{eq:gnn:input}.
% %
% %Here, $\Fagg$ is an aggregate function to aggregate the representation of the neighbors and $\Fcom$ is a combine function to combine the aggregated representation and previous representation.
% %For an ACS task $\task=(\Graph,Q,L)$, the GNN model can learn via its prediction error feedback given the queries $Q$ and ground-truth $L$ and then adapt to new queries in an end-to-end way. 
% %To jointly leverage structure and attribute knowledge, the initial node representation $h^{(0)}(v)$ is formed by concatenating the attribute feature vector $\mathcal{A}(v)$, the binary query node identifier $I_q(v)$ and query attribute identifier $I_a(v)$, as Eq.~\eqref{eq:gnn:input} shows. 
% \begin{align}
%     h^{(0)}(v) = [ I_q(v)\| I_{\ASet}(v) \|\ASet(v)],
%  \label{eq:gnn:input}
% \end{align}
% Here, $I_q(v) \in \{0, 1\}$ identifies whether node $v$ is a query node, $ I_{\ASet}(v) \in \{0, 1\}^{|\ASet|}$ identifies which attributes are the query attributes, and $\ASet(v)$ is the vectorized representation of the attributes of $v$.
% Through the transformation of $K$ layers, GNN-based models predict the likelihood  that $v$ is in the community of the query by a prediction layer $\hat{f}$, i.e., $\hat{y}(v) = \hat{f}(h^{(K)}(v))$, where the prediction layer $\hat{f}$ may contain extra neural network layers, followed by sigmoid activation. Thereby, an ACS task $\task=(\Graph,Q,L)$ is a query-specific binary classification task in $\Graph$, where GNN-based models are trained by minimizing the binary cross entropy (BCE) loss on queries $Q$ and ground-truth $L$ as Eq.~\eqref{eq:loss:bce}.
% %where $I_q(v)=1$ if $v$ is the query node, otherwise $I_q(v)=0$. And $I_a(v)=1$ if $v$ has a query attribute $a$, otherwise $I_a(v)=0$. If there are multiple query attributes, we need to construct multiple initial node representations and combine the representation through an aggregate operation.
% %The binary cross entropy (BCE) loss for a specific query  $q \in Q$, as shown in Eq.~(\ref{eq:loss:bce}), measures the discrepancy between the predicted probability and ground-truth $l_q = (l_q^{+}, l_q^{-})$, under the GNN parameterized by $\theta$. Here, $l_q^{+}$ and $l_q^{-}$ represent the positive and negative samples respectively, w.r.t. $q$. 
% %
% \begin{align}
% 	\label{eq:loss:bce}
% 	\loss(q; \theta) = - \sum_{v^{+} \in l_q^+} \log\hat{y}(v^+) -  \sum_{v^{-} \in l_q^-} \log (1-\hat{y}(v^-)) 
% \end{align}

% Existing approaches train a distinct model for each ACS task by implicitly assuming that  the graph used for training and inference for unseen queries are the same. Such transductive models cannot infer communities for queries from different graphs and different attribute sets.



% \comment{We first introduce general framework of GNN, which serves as the basis of many learning-based CS methods.
% A $K$-layer GNN follows a neighborhood aggregation paradigm to generate new representations for each node in the graph. This process involves iterating over $K$ layers.
% The representation of a node $v$ in the $k$-th iteration is denoted as $h_v^{(k)}$, which is a $d^{(k)}$-dimensional vector.
% In the $k$-th iteration (layer), for each node $v \in V(G)$, an aggregate function $\Fagg^{(k)}$ is used to combine the representations of the neighbors of $v$ that are generated in the previous ($k$-$1$)-th iteration as Eq.~(\ref{eq:gnn:fagg}). 
% Subsequently, an aggregate function $\Fcom^{(k)}$ updates the representation of $v$ by aggregating representation $a_v^{(k)}$ and previous representation $h_v^{(k-1)}$ as Eq.~(\ref{eq:gnn:fcom}).
% \begin{align}
% 	a_v^{(k)} &= \Fagg^{(k)}(\{ h_u^{(k - 1)} | u \in \neighbor(v)\}) \label{eq:gnn:fagg} \\
% 	h_v^{(k)}  &= \Fcom^{(k)}(h_v^{(k - 1)}, a_v^{(k)}) \label{eq:gnn:fcom}
% \end{align}}
