\section{Experiments}\label{sec:experiments}

We implemented the {\uindex} framework in the Rust programming language.
In \cref{sec:setup}, we present the setup of the experiments that we conducted to assess the efficiency of our implementation.
In \cref{sec:evaluation}, we present the results of these experiments.
In \cref{sec:application}, we present an application of our framework in mapping long reads onto a reference genome.


Our software resources are open-source and can be found at \url{https://github.com/u-index/u-index-rs}.

\parag{Implementation Details}
In practice, we verify each candidate occurrence using a linear scan of $P$ in
$T$, hence without using any trie data structure. Even if this solution costs
$\cO(m)$ time instead of the $\cO(1)$-time verification claimed by
Theorem~\ref{the:framework}, this is likely to be faster in practice because
traversing tries is not cache-efficient.

\subsection{Setup}\label{sec:setup}

\parag{Hardware and Software}
All experiments were run on an Intel Core
i7-10750H running at a fixed frequency of 2.6 GHz with hyperthreading disabled
and cache sizes of 32 KiB (L1), 256 KiB (L2), and 12 MiB (shared L3). Code was
compiled using \verb|rustc| 1.85.0-nightly and \verb|GCC|
14.2.1 on Arch Linux.
%% Our software resources will be made available on GitHub upon acceptance of the paper.

\parag{Datasets}
We use three textual datasets of different nature and alphabet size:
(1) chromosome 1 of
CHM13v2.0\footnote{\url{https://www.ncbi.nlm.nih.gov/nuccore/NC_060925.1}},
which contains repetitive regions and consists of 248 million symbols over the DNA alphabet ($\sigma=4$),
thus 59 MiB when each symbol is coded using 2 bits;
(2) the 200 MiB protein sequences available from the Pizza \& Chili site\footnote{\url{https://pizzachili.dcc.uchile.cl/texts/protein}} ($\sigma=27$), or 125 MiB when each symbol is coded using 5 bits;
and (3) the 200 MiB English collection, also available from the Pizza \& Chili site\footnote{\url{https://pizzachili.dcc.uchile.cl/texts/nlang}} ($\sigma=239$).
In the following, we discuss experimental results referring to \cref{fig:plot-v2} (at page \pageref{fig:plot-v2}) for the DNA alphabet. The results for proteins and English have very similar shapes and are deferred to \cref{sec:app} due to space constraints.

\parag{Queries} For each dataset, we test $10^5$ \emph{positive} queries that
are uniformly sampled from the text. For DNA, queries consist of $512$ characters,
while for the protein and English datasets we use queries of $128$ characters.

\parag{Tested Indexes}
We compare the suffix array and the FM-index, indicated with {\sa} and {\fmindex} in our results, against their
{\uindex} variants, with parameters $(k,\ell)\in \{(4,32), (8,64), (16,128), (28,256)\}$.
For the suffix array construction, we use
\method{libsais}\footnote{\url{https://github.com/IlyaGrebnov/libsais}}~\cite{nong2010two,karkkainen2009permuted}.
For the {\fmindex}, we use the implementations in SDSL-lite
(SDSL-v2)~\cite{gog2014theory} and in AWRY~\cite{anderson2021optimized}.


For each index, we use the smallest $\tau\geq \lceil\log_2 c \rceil$ that is supported by
the index. In practice, this means that for the suffix array and SDSL {\fmindex}
we use $\lceil \log_2 c /8\rceil$ bytes to represent each ID, so that each ID is
a single character. This way, these indexes get built on exactly $z=|\Minimizers(T)|$ characters.
In practice, this is strictly better than using a smaller $\tau$ and building an
index on $2z$ or more characters.
The AWRY {\fmindex} does not support generic alphabets, and thus we had to consistently
use $\tau=2$ to encode the IDs as DNA bases. We note that the AWRY uses a
multi-threaded parallel construction algorithm, while all other methods are
single-threaded.
Further details can be found in \cref{sec:app-tau}.

Finally, we also compare against our own implementation of the \emph{sparse suffix array at minimizer
positions}~\cite{DBLP:journals/spe/GrabowskiR17}, that we call the {\sindex} index in our results.

\subsection{Results}\label{sec:evaluation}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{plots/plot.pdf}%
\caption{
Results on the $59$ MiB of the human chromosome 1. For each data structure --- except the {\sindex} --- we compare its performance when constructed on the plain input text (in red, left column of each group) versus when used with the {\uindex} (remaining
  colors and columns), for increasing values of $k$ and $\ell$.
  Indexes marked with \texttt{-H} (read, ``minus \texttt{H}'') use minimizers themselves as IDs, \textit{without} the map $H$.
  Similarly,
  the indexes marked with \texttt{-S} omit storing the sketched input text
  $S$ and instead reconstruct it via the minimizer positions $\Minimizers(T)$
  and $T$ itself.
  The {\sindex} is only shown with sampling (no red column) because it is otherwise equivalent
  to {{\sa}}.
%
  The top plot shows space usage of the final data structure in MiB, with the space for minimizer positions
  and the map $H$ shaded and the black line indicating the space occupied by the
  2-bit packed input text. The second row shows the maximum memory usage (resident set size, RSS) during the
  construction, where the shaded area is memory usage \emph{before} construction.
  The third row shows build time (in seconds), with the time for sketching the input shaded.
  The bottom plot shows query time (in average $\mu$s per {\Locate} query), with the time for searching in the
  inner index shaded.
  }
\label{fig:plot-v2}
\end{figure*}


\parag{Suffix Array}
In all cases, we see that the {\uindex} variants take less space and are faster to
construct than the classic indexes. 
Both space and
construction time tend to be at least $8\times$ less.
The time spent searching the suffix array (bottom, shaded) goes down as the
suffix array becomes sparser, but the total query time goes up.
This is due to the increasing number of false positive matches in minimizer
space, starting at $10$ false positives per query for $(k,\ell)=(4,32)$ and going
up to $100$ for $(k,\ell)=(28,256)$. These are caused by highly repetitive regions
in the centromere~\cite{grady1992highly}.
Nevertheless, query time is usually less than twice slower than the classic suffix
array, while space and construction time are greatly reduced.
% \giulio{I'd report an example, perhaps, say for $(k,\ell)=(28,256)$.}

\parag{Omitting $H$}
When $k\in \{16,28\}$, most minimizers are unique, and the map $H$ mapping the minimizers
to IDs has size linear in their number. This can be seen in that the shaded area in
the rightmost two columns of the top-left group is accountable for most of the
space used by the index at over $2^6=64$ MiB, while the input only takes 59 MiB when encoded using $2$ bits per character.
Looking at {\sa} \texttt{-H}, which omits $H$,
we see that the index is much smaller for larger $k$, while query time is
unaffected. Construction time does increase, since the alphabet is larger and
hence requires more memory. The high spike for $k=16$ seems to be caused by
{\sa} (\method{libsais}) being particularly slow for 32-bit integers.

\parag{Implicit Sketched Text}
We can also omit the sketched text $S$ and
instead reconstruct it on-the-fly, as explained in \cref{sec:framework}. This saves significant space when $k$ is
large (where also $H$ can be omitted altogether). Construction time is not affected since $S$ is discarded after construction. The time spent in searching the suffix array
does increase significantly though, up to $4\times$ (shaded), but as most of the time
is spent verifying potential matches, the total query time only goes up slightly.

\parag{{\fmindex}}
The SDSL implementation takes around 90 MiB for the plain input text, and goes significantly below this when using the {\uindex}\footnote{For some technical reason, SDSL does not accept NULL bytes, hence minimizers must necessarily mapped by $H$ in the range $[1,c+1)$. Thus SDSL takes a
significant amount of space for $k \geq 16$.}.
Construction time improves as well, almost
$2\times$ each time we double $\ell$ because the number of sampled positions nearly halves.

A main drawback of the SDSL implementation is its significantly larger query times compared to all
other methods, starting at $32\times$ slower for the plain index and increasing up to $1000\times$ slower for $\ell=256$.
We suspect this is due to the inherent complexity in the wavelet tree data structures used to represent the Burrows-Wheeler transform of the text~\cite{burrows1994block}, that has
increasingly more levels as $k$ increases and hence the number of bits $\tau$ in each $k$-mer ID grows.
This makes SDSL an impractical choice in this scenario or, more in general, for applications where size is
not a bottleneck but fast queries are the primary concern.

AWRY uses around twice as much space as SDSL, and has a similar construction
time, likely because both are limited by the internal suffix array construction.
On the other hand, query times for AWRY are significantly faster than SDSL because AWRY is optimized for the DNA alphabet, and the {\uindex} version
with $k=4$ is slightly faster than AWRY on the plain text. However, as $k$ and
$\ell$ increase, both SDSL and AWRY slow down by roughly a factor of 2 overall.
For AWRY we can also omit $H$ and this almost halves the size when $k$
is large, but negates the previously seen speedup in construction time since the
sketched input $S$ is significantly larger.

\parag{Sparse SA}
Lastly, the {\sindex} index takes strictly less space than the {\uindex} variant of the suffix
array, since it stores strictly less information: only one permutation of the
minimizer positions in the original text.
It is also significantly faster to query, since no sketching is needed.
Additionally, it has much fewer false positives, since comparisons are made in
the plain input space rather than in sketch-space. Construction time is worse
than {\sa} however, since there is no known linear-time implementation for
sparse suffix array construction\footnote{Indeed, our implementation is very simple and relies on
sorting of the minimizer positions using the Rust
\texttt{sort\_by} function that returns the suffix associated with each
position as needed.}.
Nevertheless, this is still faster than constructing an {\fmindex}.

% \return
% \giulio{I'd add here at the end of this section, a few takeaway messages though using an itemize, so that it is clear what the reader should remember. Maybe move this piece to the conclusions.}
% From the results of these experiments, we formulate the following takeaway messages.
% \begin{itemize}
%     \item \red{SA: almost 2X slower but a lot better space and construction space/time.}
%     \item \red{For large $k$, it is beneficial to avoid the remapping $H$ as the index becomes much smaller with no slowdown at query time.}
%     \item \red{Omitting the sketched sequence $S$ results in slower Locate but better space.}
%     \item \red{something about FM index and sparse SA.}
% \end{itemize}
