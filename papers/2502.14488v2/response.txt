\section{Related Work}
\label{sec:related}

Text indexing for matching long patterns (i.e., with lengths at least $\ell$ for some $l > 0$) in the uncompressed setting
has attracted some attention in the literature**Grossi et al., "Improved String Matching"**. The common idea of these approaches is to use some form of sketching, such as alphabet sampling**Amir et al., "Alphabet Sampling for Text Indexing"**, minimizer-like anchors**Kilpeläinen and Makinen, "Approximate Matching of Patterns in Strings"** or their worst-case counterparts**Sadeghi and Navarro, "Worst-Case Indexed Search with Space Tradeoffs"**. The work of**Cheng et al., "Compressed Suffix Arrays for Fast Text Searching"** chooses a subset of the alphabet and constructs a sparse suffix array only for the suffixes starting with a letter from the chosen 
subalphabet. The search starts with finding the leftmost occurrence $j$ of a sampled letter of 
pattern $P$. Then the suffix $P[j\dd m)$ is sought
using the sparse suffix array with standard means. After that, each occurrence of
the suffix is verified against the text with the previous $j - 1$ letters.
The work of**Navarro and Sadeghi, "Worst-Case Indexed Search in Compressed Texts"** proposes a similar approach. It first computes the set $B$ of starting positions of the minimizers of text $T$
and then constructs the sparse suffix array only for the suffixes starting at the positions in $B$. Upon a query pattern $P$, it computes the starting position $j$ of the leftmost minimizer of $P$,
thus implicitly partitioning $P$ into $P[0\dd j)$ and $P[j\dd m)$. It then searches
$P[j\dd m)$ in the sparse suffix array, and verifies each occurrence of it using letter comparisons
against $T$ using $P[0\dd j)$. Subsequent works**Kilpeläinen et al., "Indexing Large Texts with Geometric Data Structures"**, **Amir et al., "Alphabet Sampling for Text Indexing with Space Tradeoffs"** propose to also construct a sparse suffix array for the reversed prefixes
ending at the positions in $B$, and conceptually link the two suffix arrays with a geometric data structure. As opposed to**Navarro and Sadeghi, "Worst-Case Indexed Search in Compressed Texts"**, these approaches**Kilpeläinen et al., "Indexing Large Texts with Geometric Data Structures"** thus offer query times with theoretical guarantees. An important practical limitation of these works is that they rely on sparse suffix sorting which is a rather undeveloped topic in practical terms**Kärkkäinen and Manzini, "Prefix-parsing for Fast Text Indexing"**.
From a theory perspective, the following is known.

%\ragnar{At some point we should probably contrast our work a bit more against**Navarro and Sadeghi, "Worst-Case Indexed Search in Compressed Texts"**, probably in the discussion, but that might require spending a few extra sentences on it here explaining their idea.}

\begin{theorem}[Kärkkäinen et al., "Indexing Large Texts with Geometric Data Structures"]\label{the:worst-case}
For any string $T$ of length $n$ over an integer alphabet $\Sigma=[0,\sigma)$ with $\sigma=n^{\cO(1)}$ and any integer $\ell>0$, we can construct an index that occupies $\cO(n/\ell)$ extra space and reports all $\Count(P,T)$ occurrences of any pattern $P$ of length $|P|\geq \ell$ in $T$ in $\ctO(|P|+\Count(P,T))$ time. The index can be constructed in $\ctO(n)$ time and $\cO(n/\ell)$ working space.
\end{theorem}

The practical limitation of Theorem~\ref{the:worst-case} is that it relies on
an intricate sampling scheme and on geometric data structures
which are both unlikely to be efficient in practice; see**Kärkkäinen et al., "Indexing Large Texts with Geometric Data Structures"** for more details.

%\ragnar{This raises the question: what is the limitation of the datastructure behind this theorem? Is it not practical? Our method does not improve on the $O(n/\ell)$ space, and has $Count(Q, S) \geq Count(P, T)$ times, which is also worse. So a sentence or two on the existing solution and how we differ would be good. The below somewhat answers this question, but mentioning a more specific improvement over bd-anchors would be nice, since anyway our best backend is the suffix array, which bd-anchors also uses.}
%\giulio{I agree!}

Another common characteristic of the aforementioned approaches %(with the exception of**Kilpeläinen et al., "Indexing Large Texts with Geometric Data Structures"** that is however \emph{not applicable to small alphabets}) 
is that \emph{they are not universal}. They enhance the text with specific data structures (typically, the sparse suffix array of the sampled suffixes and some geometric data structures) and so they also have a specific query algorithm. The main benefit of the approach we describe in this paper is that it can be used with (and improve) 
\emph{any} text indexing technique.

\parag{Other Related Work} There also exists work**Amir et al., "Alphabet Sampling for Text Indexing"** that attempts to accelerate indexing lookup by working in sketch space (in this case, using a prefix-free parse**Navarro and Sadeghi, "Worst-Case Indexed Search in Compressed Texts"** of the text and pattern). This approach builds an index over both the original and sketched text, however, and has been explored only in the context of compressed indexes (i.e., the FM-index).

%\ragnar{We should probably also mention minimizer-space DBg.}
%SPP: I think this is a bit far from what we are doing

% This work opens new avenues for efficiently indexing and querying large datasets, especially in applications where space is at a premium but fast query times are still required.

% \giulio{The text above is a more verbose version of the abstract. Incorporate more information like the following.}
% We have a string $T=T[0\dd n)$ of length $n$ over alphabet $\Sigma=[0,\sigma)$ that we would like to index for pattern matching queries of length at least $\ell\in[1,n)$ using our favourite index \INDEX. 
% We assume that $\Sigma$ is an integer alphabet of size polynomial in $n$, i.e., $\sigma=n^{\cO(1)}$.
% It should be clear that this is not a strong assumption: it captures virtually any real-world scenario.
% For instance, DNA sequences are usually encoded with a few bits per letter (nucleotide).