\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/framework.pdf}
    \caption{The \uindex framework. The steps (1) and (2) are to build the index. The steps (3)--(6) are to query with the framework. The sketching scheme in steps (1) and (3) must be the same.}
    \label{fig:framework}
\end{figure*}

\section{A Universal Indexing Framework for Matching Long Patterns}\label{sec:framework}

In this section, we describe a universal indexing framework for a text $T$ of length $n$ --- referred to as the {\uindex} --- to retrieve all occurrences of a pattern $P$ of length at least $\ell$ in $T$.
Refer to \cref{fig:framework} for an overview.

\parag{Overview} The core idea of the {\uindex} is to sketch the text $T$.
We use random minimizers with parameters $k$ and $w=\ell-k+1$ for sketching $T$,
though any type of locally-consistent sketching mechanism may be used. We start by computing the sorted list $\Minimizers(T)$ of minimizer \emph{positions}.
Let us set $z:=|\Minimizers(T)|$.
%\giulio{Using letter $A$ to indicate the length of a sequence feels uncommon...maybe use $z$?}
We also consider the sequence $M[0 \dd \Size)$ of the corresponding minimizer strings,
such that  $M[i] = T[p_i \dd p_i+k)$ for any position $p_i \in \Minimizers(T)$.
Let $c$ be the number of \textit{distinct} minimizers in $M$.
We can then identify each minimizer $v\in M$ with a unique identifier (or ``ID'' for short, in the following) in $[0,c)$ using a map $H : \Sigma^k \to [0,c)$.
The sketch $S[0 \dd \Size)=\Sketch(T)$ is the sequence of IDs $S[i]=H(M[i])$ for all $i \in [0, \Size)$, which is encoded in a suitable alphabet.

%\parag{Omitting the remap} 
Two remarks about the map $H$ are in order. When $k$ is small enough to have $\sigma^k \ll
n/\ell$, then most $k$-mers are likely to be minimizers and the map $H$ can thus be
completely omitted. In what follows, we assume the case when $H$ exists.   
When $k$ is large, on the other hand,
storing each minimizer in $M$ and the map $H$ could take a lot of space, e.g., $\cO(c (k\log_2(\sigma)+\log_2(c)))$ bits.
We can reduce the $k\log_2(\sigma)$ component of this space usage by first hashing the minimizers using, e.g., a rolling hash function as explained in \cref{sec:background},
and only storing the mapping from minimizer hashes to their IDs.
This reduces the space to $\cO(c(q+\log_2(c)))$ bits, where $q$ is the number of bits used for each hash code, provided that $q < k\log_2(\sigma)$.

% \ragnar{I made $\phi$ an 'add-on' here, and removed its mentions from the rest
%   of the text. But actually, it would be even cleaner to store pointers into
%   $T$ rather than introducing $\phi$.}
% Giulio: phi cannot be removed completely because we need the rolling-hash function property for the theoretical guarantee.

\parag{Constructing the Index} Let $\Sigma'=[0,2^{\tau})$ denote the integer alphabet that we choose to encode $S$, for some input parameter $\tau$.
Further let $\INDEX$ denote the indexing data structure that we apply on $S$.
Namely, we construct the $\INDEX$ of $S$, over the alphabet $\Sigma'$, with $\tau=\log_2|\Sigma'|$. 
Note the purpose of setting the value of $\tau$: it lets the user control the size of the alphabet we choose to encode $S$ as something that lies in $[2,n]$.
%\giulio{Nitpick: $\tau=|\Sigma'|$ is not the size of the alphabet of $S$, which is $c$ actually; it is the size of the reduced alphabet that we use to encode $S$. Is it sufficiently clear or shall we explain better?} For example, $\Sigma'$ can be the DNA alphabet ($\tau=2$), the ASCII alphabet ($\tau=8$), or a general integer alphabet over a variable number of bytes.
Thus, we interpret each $\lceil\log_2(c)\rceil$-bit ID $S[i]$ as a sequence of $b:= \lceil \lceil\log_2(c)\rceil / \tau \rceil$ $\tau$-bit integers\footnote{In the unlikely event of $\Size\lceil\lceil\log_2(c)\rceil / \tau\rceil > n$, we can either increase $\tau$ to have $z\lceil\lceil\log_2(c)\rceil / \tau\rceil \leq n$ or simply set $S:=T$.}.
This is a useful feature because some compressed full-text indexes, like the FM-index~\cite{10.1145/1082036.1082039} or the $r$-index~\cite{10.1145/3375890}, take advantage of the repetitiveness of the text $T$ to improve its compression.
% \giulio{I fear a review might wonder why we do not vary $\tau$ for at least one experiment...
% if we do not want to do it, we should explain how we fix $\tau$ for each dataset/data-structure combination.}
% \ragnar{I'd prevent talking about it, and consider adding once reviewers ask for it.}

\parag{Implicit Sketched Text}
Note that the $\INDEX$ of $S$ may or may not require storing the sketched text $S$ itself. For example, the FM-index is a \emph{self-index} and replaces $S$ with its compressed form. On the other hand, the suffix array is not a self-index and \emph{does} require $S$. 
In the latter case, 
we can either store
$S$ explicitly, or we can reconstruct $S$ on-the-fly as needed using only $T$, $H$, and $\Minimizers(T)$.

%as needed: \red{the $i$'th
%character of $S$ is given by the $\lfloor i/w\rfloor$'th minimizer, which can be found at position $\Minimizers(T)_{\lfloor i/w\rfloor}$ in the original text}.
%\giulio{No, because we map the kmer string to an integer using $H$, as per above $S[i]=H(T[p_i \dd p_i + k))$. Why don't we just note that with this relation, $S$ can be reconstructed using only $T$, $H$, and $\Minimizers(T)$?}

To conclude, our framework assumes read-only random access to $T$, takes
parameters $\ell$, $k$, and $\tau$ as input, and constructs an index on top of
$T$ that consists only of the minimizer positions $\Minimizers(T)$ (encoded
using Elias-Fano~\cite{Elias74,Fano71}), the minimizer-ID map $H$, and the $\INDEX$ of $S$ over a $\tau$-bit alphabet.

\parag{Querying}
We now describe how to compute the set $L=\Locate(P,T)$, given a query pattern
$P[0\dd m)$ that is sufficiently long (i.e., $m\geq \ell$).

First, $P$ is sketched similarly to the text $T$, obtaining a string $Q=\Sketch(P)$.
Specifically, its minimizer positions
$\Minimizers(P)$ are found. Since the pattern has length $m \geq \ell$,
it has at least one minimizer and we indicate with $\alpha$ and $\beta$ the position
of the first and last minimizer of $P$, respectively. If one of the minimizers $P[p_i\dd p_i+k)$ of $P$, for $p_i\in \Minimizers(P)$, 
does not occur in the text $T$ and hence is not assigned
an ID by $H$, this directly implies that $P$ does not occur in $T$.
Otherwise, the list of corresponding IDs is determined as
$H(P[p_i \dd p_i+k))$, 
for all $p_i\in \Minimizers(P)$, and this is encoded into the
sketched query string $Q$ using $b$ $\tau$-bit integers per minimizer.


We locate $Q$ in $S$ using the $\INDEX$ of $S$. Let $L=\varnothing$ be the list
of occurrences.
For every position
$p \in L' = \Locate(S, Q)$, we first check whether $p \equiv 0 \pmod b$.
If not, the candidate match is a false positive caused by the reduction of alphabet
size. Otherwise, we retrieve the position $l:=\Minimizers(T)[p/b]$ and verify whether $T[l-\alpha \dd l-\alpha + m)=P$ in $\cO(m)$ time.
If so, position $(l-\alpha)$ is added to $L$.
\cref{fig:example} illustrates an example.

% \ragnar{Also here dropped the previous bits.}
%% We now describe how to {\Locate}
%% a pattern $P[0\dd m)$ on $T$, with $m \geq \ell$. We initialize an integer $j:=0$.
%% We compute the set $\Minimizers(P)$ of the positions of the minimizers of $P$ from left to right in $\cO(m)$ time.
%% Let $\alpha$ denote the smallest element (position) in $\Minimizers(P)$ and $\beta$ the largest.
%% Intuitively, $\alpha$ is the position of the leftmost (first) minimizer of $P$
%% that is added to $\Minimizers(P)$ and $\beta$ the position of the rightmost
%% (last) minimizer. If $|\Minimizers(P)|=1$ then $\alpha=\beta$. \ragnar{This
%%   discussion on $\alpha$ and $\beta$ can be omitted. It doesn't really go
%%   anywhere, since the case where the sketch only has a single element isn't an
%%   edge case anyway.}
%% We transform $P$ into a vector $R$ of integers using $H$: when position $i$, corresponding to $P[i\dd i+k-1]$, is chosen as the $j$th element added to $\Minimizers(P)$ then we lookup $\phi(P[i\dd i+k-1])$ in $H$. If it does not exist in $H$, then $P$ does not occur in $T$.
%% If it exists, then we set $R[j] = H[\phi(P[i\dd i+k-1])]$ and increase $j$ by $1$.
%% We initialize an empty vector $Q$ of $\lceil \log \tau \rceil$-bit integers.
%% We read $R$ from left to right and append $R[i]$ to $Q$ by expressing $R[i]$ as
%% $\lceil\log c/\log \tau\rceil$ integers.
%% We search for $Q$ in $S$ using \INDEX of $S$.
%% For every occurrence $S[p\dd q]$ of $Q$ in $S$, such that $p\mod{\lceil\log c/\log \tau\rceil}=0$,
%% we retrieve $V[p']$, where $p'=p/(\lceil\log c/\log \tau\rceil)$ and verify
%% $T[(V[p']-\alpha)\dd (V[p']-\alpha)+m-1]$ against $P[0\dd m)$. Verification
%% takes $\cO(m)$ time.

% \ragnar{TODO: Update the remainder of this section to match the new notation. In particular $V$
%   should be $\Minimizers(T)$, and my definition of $p'$ above changed.}
% \giulio{Done}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig.pdf}
    \caption{An illustration of the \uindex of a text $T$, along with a query example. First, the minimizers $M$ of $T$
      are found, here of length $k=4$ characters, with two of them overlapping (those starting at positions 7 and 9).
      The minimizer positions $\Minimizers$ are stored with
      Elias-Fano coding. Minimizers are hashed via $H$ to shorter IDs. These are
      padded to the next multiple of $\tau$.
      An index is then built on the sketch $S$.
      %
      To locate a pattern $P$,
      its minimizers are found and the sketch $Q$ of corresponding IDs is constructed. Then
      $Q$ is located in $S$, which here gives a single match. The first
      minimizer of the match in $Q$ is located in $T$ at position $l$ via $\Minimizers$.
      Lastly, the candidate match is verified starting at position $l-\alpha$ in
      $T$.}
    \label{fig:example}
\end{figure*}

\parag{Theoretical Guarantees}
We now explain how to verify an occurrence at position $p \in L'$ in $\cO(1)$ time and
using $\cO(\Size)$ space.
Let the occurrence be $S[p \dd q]$
where $q=p+|Q|-1$. 
%\giulio{Would be nicer here to use $S[p .. q)$ and semi-open notation everywhere for consistency.}
%SPP: I think we can restrict to using it where this minus 1 occurs. Here this is not the case.

For faster querying in theory or for very long patterns in practice, we also store an array $F$ of fingerprints, where $F[i] = \phi(T[0 \dd p_i])$ for all $p_i\in \Minimizers(T)$, and $\phi$ is the rolling hash function; see \cref{sec:background}. 
% (e.g., KR fingerprints~\cite{DBLP:journals/ibmrd/KarpR87}).
This array can be constructed in $\cO(n)$ time and has size $\cO(\Size)$.
Let $\mathcal{X} = \{T[p_i\dd p_i+\ell) \mid p_i \in \Minimizers(T)\}$ and $\mathcal{X}^R = \{T[p_i-\ell+1\dd p_i]^R \mid p_i \in \Minimizers(T)\}$, where $s^R$ denotes the reverse of the string $s$.
We construct the tries $\mathcal{T} = \TRIE(\mathcal{X})$ and $\mathcal{T}^R = \TRIE(\mathcal{X}^R)$.
%\giulio{$p_i < \ell$ could happen...we have to handle this corner case.}
%SPP: I think this is OK for the paper :)
% We also store a compacted trie $\mathcal{T}$ of $T[i\dd i+\ell)$, for all $i\in \Minimizers(T)$; and a compacted trie $\mathcal{T}^R$ of $(T[i-\ell+1\dd i])^R$, for all $i\in \Minimizers(T)$, where $X^R$ denotes the reverse of string $X$. 
We label the leaf nodes representing the string $s=T[p_i\dd p_i+\ell)$ and $s^R=(T[p_i-\ell+1\dd p_i])^R$ in both tries by the set
$X_s = \{ p_i \mid T[p_i \dd p_i+\ell)=s \wedge p_i \in \Minimizers \}$. 
Each leaf is also assigned a \emph{lex-rank} that is obtained via an in-order DFS traversal of the trie.
We also implement an inverse function that takes $p_i$ as input and returns the lex-rank of the leaf node that represents $s=T[p_i\dd p_i+\ell)$ in $\mathcal{T}$.
We implement the analogous inverse function for $\mathcal{T}^R$.
Each branching node $u$ in $\mathcal{T}$ stores an interval whose left and right endpoints are the lex-rank of the leftmost and rightmost leaf node, respectively,
in the subtree rooted at $u$. This information is also computed via a DFS traversal.
We store the analogous information for the branching nodes in $\mathcal{T}^R$.
Since $\mathcal{T}$ and $\mathcal{T}^R$ are compacted and $\sum |X_s| = \Size$, it follows that the tries and the inverse functions take $\cO(\Size)$ space. The tries and the inverse functions can be constructed in $\cO(n)$ time~\cite{DBLP:journals/jea/Charalampopoulos20}.

Let us explain how these additional structures can help us verify an occurrence
$S[p\dd q]$ of $Q$ in $S$ in $\cO(1)$ time; see \cref{fig:verification}. Let $l:=\Minimizers(T)[p/b]$ and $r:=\Minimizers(T)[q/b]$.
Using the vector $F$,
we compute $\phi(T[l+1\dd r])$ in $\cO(1)$ time for Fact~\ref{fact:KR}, because we have
$F[l]=\phi(T[0\dd l])$ and $F[r]=\phi(T[0\dd r])$.
% \giulio{mention that we would need to precompute $p^{r-l}$?}
We also compute the
fingerprint $\phi(P[\alpha+1 \dd \beta])$
once in $\cO(m)$ time and compare the two fingerprints in $\cO(1)$ time\footnote{If $|\Minimizers(P)|=1$, then we always return a positive answer for this comparison.}. If they are not equal, then $(l-\alpha)$ is not a valid occurrence. If they are equal, we need to check $P[0\dd \alpha]$ and $P[\beta\dd m)$.
The remaining letters on each edge cannot be more than $\ell$ (by the density of the minimizers mechanism), and so the verification would cost $\cO(\ell)$ time if we did it by letter comparisons. We can verify the edges in $\cO(1)$ time using tries. In a preprocessing step, we spell $P[\beta\dd m)$ in $\mathcal{T}$ arriving at node $u$; and we spell $(P[0\dd \alpha])^R$ in $\mathcal{T}^R$ arriving at node $u'$. We can then check whether $r$ is a leaf node in the subtree induced in $\mathcal{T}$ using the inverse function and the interval stored in $u$. We can also check whether $l$ is a leaf node in the subtree induced in $\mathcal{T}^R$ using the inverse function and the interval stored in $u'$. This takes $\cO(1)$ time per pair $(l,r)$.
We then have that $(l-\alpha)$ is a valid occurrence if and only if both leaf nodes are located in the induced subtrees. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig/verification.pdf}
    \caption{The $\cO(1)$-time verification algorithm for occurrence $l-\alpha$. After spelling the fragments of $P$ in the two tries \emph{once}, we check if the fragments in gray match using fingerprints in $\cO(1)$ time; if so, we check if the corresponding leaf nodes are \emph{both} located in the induced subtrees in $\cO(1)$ time.}
    \label{fig:verification}
\end{figure}

\return
We have thus arrived at the following result.


%\giulio{We need a picture here! It is easy to visualize as a picture but harder to see it from text.}

\begin{theorem}(Universal framework)\label{the:framework}
Let $T$ be a string of length $n$ over alphabet
  $\Sigma=[0,\sigma)$. Let $t(n,\sigma)$, $s(n,\sigma)$, and $q(m,n,\sigma)$ be,
    respectively, the time complexity to construct \INDEX, the size of \INDEX in
    machine words, and the query time of \INDEX to report all the occurrences of
    a pattern of length $m$ in $T$.
    Furthermore, let $\Size:=|\Minimizers(T)|$ be the number of minimizers of $T$,
    for some parameters $\ell,k$, and let $S$ be the string obtained from $T$
    using the framework with a parameter $\tau$ chosen from $[1, \log_2(n)]$. Then, in
    $\cO(n+t(\Size\lceil\log \Size/\tau\rceil,2^\tau))$ time, we can construct an
    index of $\cO(\Size+s(\Size\lceil\log \Size/\tau\rceil,2^\tau))$ size,
    supporting $\Locate(P,T)$ queries for a pattern $P$ of length $m\geq \ell$ in $\cO(m+q(|Q|,\Size\lceil\log \Size/\tau\rceil,2^\tau)+\Count(Q,S))$ time, where $Q$ is the string obtained from $P$ using the framework with parameters $\ell,k,\tau$. %and $\Count(Q,S)$ is the number of occurrences of $Q$ in $S$.
\end{theorem}

% \giulio{I moved this piece from here to the experiments, where we also say that we do not implement the tries, etc.}
% In practice, we expect that the fastest verification scheme per candidate occurrence would be
% a linear scan of $P$ and $T$ even if it costs $\cO(m)$ time in the worst case instead of the $\cO(1)$-time verification claimed by Theorem~\ref{the:framework}. Indeed, we have used the former in our current implementation.

\parag{Example}
Let us now consider a practical instantiation of \cref{the:framework}. Let \INDEX be the \emph{suffix
array}~\cite{DBLP:journals/siamcomp/ManberM93} enhanced with the \emph{longest
common prefix} (LCP) array~\cite{DBLP:conf/cpm/KasaiLAAP01}. We choose
$\tau:=\lceil\log_2 n\rceil$ because the suffix array can be constructed in $t(n,2^\tau)=\cO(n)$ time, for any integer alphabet of size $2^\tau\leq n$,  and it has size $s(n,2^\tau)=\cO(n)$~\cite{DBLP:journals/jacm/KarkkainenSB06}.
Given the suffix array, the LCP array can be constructed in $\cO(n)$ time~\cite{DBLP:conf/cpm/KasaiLAAP01}.
By applying \cref{the:framework}, we construct a string $S$ of length $\Size\leq n$
over the alphabet $[0,n)$. Thus, we will construct our index in
  $\cO(n+t(\Size,n))=\cO(n)$ time of $\cO(\Size+s(\Size,n))=\cO(\Size)$ size. Note that, by
  using minimizers~\cite{DBLP:journals/bioinformatics/RobertsHHMY04,DBLP:conf/sigmod/SchleimerWA03}, $\Size$ can be much smaller
  % (even asymptotically \ragnar{but only
  %   asymptotically as $\ell$ grows with $n$, right? Not just asymptotically in $n$?} \giulio{Right!})
than $n$ in practice, for a sufficiently large value of $\ell$. Specifically, we have $\Size = nd$ where $d \geq 1/(\ell-k+1)$ is the \textit{density} of the specific minimizer scheme used.
For querying, we have $q(m,n,\sigma)=\cO(m+\log n + \Count(P,T))$ when LCP information is used~\cite{DBLP:journals/siamcomp/ManberM93}.
Thus, our query time is $\cO(m+\log \Size+\Count(Q,S))$
because $m\geq |Q|$ and $\Count(Q,S)\geq \Count(P,T)$. Note that although $\Count(Q,S)\geq \Count(P,T)$, we also have $\log \Size \leq \log n$, and so beyond space savings, the resulting index can also be competitive or faster in querying.
