\clearpage
\section{Preliminaries}\label{sec:background}

In this section we provide useful background information to support subsequent descriptions.

\subsection{Notation and Computational Model}

\parag{Basic Notation}
% \giulio{Solon, I've changed the notation $T=T[0..n)$ to match that from Problem 1.}
Recall that we deal with a string $T[0 \dd n)$ of length $n$ over an alphabet $\Sigma=[0,\sigma)$.
We assume that $\Sigma$ is an integer alphabet of polynomial size in $n$, i.e., $\sigma=n^{\cO(1)}$. 
A substring of $T$ of length $k>0$ is called a {\kmer}.
Given a query pattern $P[0 \dd m)$ with $m \geq \ell$ for some lower bound $\ell > 0$, the goal is to support
$\Locate(P,T) = \{0 \leq i < n-m+1 \, | \, T[i \dd i+m) = P \}$.
We refer to the number of occurrences of $P$ in $T$, that is, $|\Locate(P,T)|$, with $\Count(P,T)$. 

% \ragnar{We should introduce $T[i\dd j-1]$ right-inclusive and $T[i\dd j)$
%     right-exclusive indexing notation. Currently we have both: exclusive above,
%     and inclusive in the construction text. Generally I prefer exclusive
%     notation since it usually avoids $-1$ terms.}

\parag{The Computational Model}
We assume that we have random read-only access to $T$ and
count the space (in number of words) occupied on top of the space occupied by $T$. We assume the standard word RAM model of computation with machine words of $\Omega(\log n)$ bits.

\subsection{Algorithmic Toolbox}

The solutions we describe in this paper rely on few, well-defined, tools that we present below.

\parag{Minimizers}
In this paper, we use a specific class of randomized methods to sketch a string, called \textit{minimizers}~\cite{DBLP:journals/bioinformatics/RobertsHHMY04,DBLP:conf/sigmod/SchleimerWA03}. Minimizers are defined as the triple $(k,w,h)$: from a window of $w$ consecutive {\kmer}s of $T$, the leftmost smallest {\kmer} according to the order $h$ is chosen as the \textit{minimizer} of the window.
Since at least one {\kmer} must be chosen every $w$ positions, the fraction of sampled {\kmer}s --- defined as the \textit{density} of the sampling algorithm --- is always at least $1/w$.

Several minimizer sampling algorithms have been proposed in the literature. See
Section 3 of \cite{grootkoerkamp_et_al:LIPIcs.WABI.2024.11} for a recent
overview of different sampling strategies and orders that lead to different densities.
In this paper, however, we use the folklore \textit{random minimizer} sampling, which is as defined above when $h$ is a pseudo-random hash function.
We have the following result.

\begin{theorem}[Theorem 3 from \cite{zheng2020improved}]\label{thm:random-mini}
When $T$ is a string of i.i.d.~random characters and $k > (3+\varepsilon)\log_{\sigma}(w+1)$ for any $\varepsilon > 0$, the density of the random minimizer is $2/(w+1) + o(1/w)$.
\end{theorem}

In the context of this paper, we fix $\ell$ to be the minimum pattern length and let $w=\ell-k+1$. Each substring of length $\ell$ of $T$ therefore contains one minimizer.
% This is then also the minimum length that we require each
% query pattern to have ($|P|\geq \ell$).
(In practice, we expect to have $|P|\gg\ell$ and that the sketch of $P$ is a sequence of several minimizers.)
% which may be useful in practical applications.
%\giulio{A reader may wonder why...also, ``may be needed'' to do what exactly? Maybe as to have good performance with the framework because each pattern has several minimizers and not just one?}
Further, we let $\Minimizers(T)$ indicate the sorted list of positions in $T$ of the minimizers of $T$.
Let $\Size = |\Minimizers(T)|$ be the number of minimizers.
By \Cref{thm:random-mini}, we have that $\Size \approx 2/(\ell-k+2)$ in expectation (neglecting lower-order terms).

% \giulio{We can generally state that the expected size of $\Minimizers(T)$
% is $n\cdot d$ where $d \geq 1/(\ell-k+1)$ is the \emph{density} of the chosen minimizer scheme. In~\cite{grootkoerkamp_et_al:LIPIcs.WABI.2024.11}, we show a minimizer scheme with optimal density $1/(\ell-k+1)$ assuming a large $k$, but I think here $k$ would be small and $\ell$ large instead. Anyway, we would like to have $\ell-k+1 = O(\log n)$, so that the expected size of $A$ is $O(\frac{n}{\log n}) = o(n)$.}

\parag{Tries}
Given a set $\mathcal{X}$ of strings over the alphabet $\Sigma$, a \emph{trie} $\TRIE(\mathcal{X})$ is a rooted tree whose nodes represent the prefixes of the strings in $\mathcal{X}$.
%\giulio{There are too many Ts! Maybe use $t(S)$ for a trie?}
% \giulio{I changed $\mathcal{S}$ to $\mathcal{X}$ since we use $S$ a lot after...}
The edges of $\TRIE(\mathcal{X})$ are labeled by letters from $\Sigma$; the prefix corresponding to node $u$ is denoted by $\textsf{str}(u)$ and is given by the concatenation of the letters labeling the path (sequence of edges) from the root of $\TRIE(\mathcal{X})$ to $u$. The node $u$ is called the \emph{locus} of $\textsf{str}(u)$. 
The parent-child relationship in $\TRIE(\mathcal{X})$ is defined as follows: the root node is the locus of the empty string $\varepsilon$; and the
parent $u$ of another node $v$ is the locus of $\textsf{str}(v)$ without the last letter. This letter is the edge label of $(u,v)$. The order on $\Sigma$ induces an order on the edges outgoing from any node of the trie. A node $u$ is \emph{branching} if it has at least two children and \emph{terminal} if $\textsf{str}(u) \in \mathcal{X}$. 

A \emph{compacted trie} is obtained from the underlying trie by removing all nodes except the root, the branching nodes, and the terminal nodes.
% \giulio{I've commented out the following sentence as we don't use this terminology anywhere else in the paper.}
% The removed nodes are called \emph{implicit}, while the preserved nodes are called \emph{explicit}. 
%\giulio{Do we use this terminology of ``implicit''/``explicit''? I'm sure before we did not. Why not just saying that ``a compacted trie is a trie where all unary paths have been collapsed into a single edge, labeled by the string obtained by concatenating all the letters on the edges''?} 
% In other words,
More specifically, a compacted trie is a trie where all unary paths are collapsed into a single edge, labeled by the string obtained by concatenating all the letters of the edges in the unary path.
% The edges of the compacted trie are labeled by strings. %Greg: is this sentence needed given the one before it? 
The compacted trie takes $\cO(|\mathcal{X}|)$ space provided that the edge labels are implicitly represented as pointers to fragments of strings in $\mathcal{X}$. Given the lexicographic order on $\mathcal{X}$ along with the lengths of the longest common prefixes between any two consecutive elements (in this order) of $\mathcal{X}$, one can compute $\TRIE(\mathcal{X})$ in $\cO(|\mathcal{X}|)$ time~\cite{DBLP:conf/cpm/KasaiLAAP01}.

%\ragnar{TODO: where do we use this?}
%\giulio{probably to state the construction time of the uindex approach. We have to be more explicit about it maybe.}
%We use this when we built the tries later on for the worst-case.


\parag{Rolling Hashing}
% Let us start by introducing Karp-Rabin (KR) fingerprints~\cite{DBLP:journals/ibmrd/KarpR87}.
% Let $T$ be a string of length $n$ over an integer alphabet.
Let $p$ be a prime number and choose $r \in [0,p)$ uniformly at random. 
The rolling hash value of $T[i \dd j]$ --- which we simply refer to as \textit{fingerprint} in the following --- is defined as~\cite{DBLP:journals/ibmrd/KarpR87}: 
$$
\phi(T[i\dd j]) := \sum^{j}_{k=i}T[k]r^{j-k}\bmod p.
$$
The adjective ``rolling'' refers to the way the hash value is updated incrementally as
a fixed length window slides through the string $T$.
The function $\phi$ allows to compute the fingerprint of a window just knowing the fingerprint of the previous window and the character that is being removed/added, instead of recalculating the fingerprint from scratch.

Clearly, if $T[i\dd i + \ell] = T[j\dd j + \ell]$ then $\phi(T[i \dd i+\ell]) = \phi(T[j \dd j+\ell])$. 
On the other hand, if $T[i\dd i + \ell] \neq T[j\dd j + \ell]$ then $\phi(T[i \dd i+\ell]) \neq \phi(T[j \dd j+\ell])$ with probability at least $1-\ell/p$~\cite{DBLP:conf/icalp/DietzfelbingerGMP92}.

Since we are comparing only substrings of equal length, the number of different possible substring comparisons is less than $n^3$. Thus, for any constant $c\geq 1$, we can set $p$ to be a
prime larger than $\max(|\Sigma|,n^{c+3})$ to make the function $\phi$ perfect (i.e., no collisions) with probability at least $1 - n^{-c}$ (this means \emph{with high probability}). Any fingerprint of $T$ or $P$ fits in one machine word, so that comparing any two fingerprints takes $\cO(1)$ time.
In particular, we will use the following well-known fact.

\begin{fact}[\cite{DBLP:journals/ibmrd/KarpR87}]\label{fact:KR}
For any $0 \leq i < j \leq n$, we have
$$
\phi(T[i+1 \dd j]) = ( \phi(T[0 \dd j]) - r^{j-i}\phi(T[0 \dd i]) ) \bmod p.
$$
\end{fact}
