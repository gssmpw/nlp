\section{Related Work}
\label{sec:related_works}

% Pre-NeRF related works
\subsection{Classical 3D Reconstruction}
Lidars and cameras are the two main modalities used in robotic perception and specifically for 3D reconstruction. In this section, we review classical lidar-based and vision-based reconstruction methods. For each sensor modality, trajectory estimation is a typical first step in a reconstruction pipeline. Then, we review strategies for extending these methods to large-scale scenes.

Lidar is the dominant sensor used for accurate 3D reconstruction of large-scale environments____. It actively transmits laser pulses to measure ranges and as a result is accurate up to ranges of as much 100m. With these distance measurements, Lidar odometry typically uses a variant of Iterative Closest Point (ICP), and often integrates high-frequency IMU measurements____. Small errors in odometry can accumulate over time resulting in trajectory drift. This drift can be mitigated when a sensor revisits a previous place and detects loop closure with pose graph optimisation____ which allows a consistent map to be maintained.
After registering all the lidar scans, the (surface) reconstruction problem is then to fuse discrete observations into a map. %\todo{check supereight}
Example map representations include surfels____, voxels____ and wavelets____. 
% (which can model occupancy____ or Signed Distance Function (SDF)____), and 
Despite its advantages, lidar has its own limitations. Lidar is much more expensive than cameras, while the measurements are typically much sparser than camera images. The measurements have inherent noise with ranging errors in the order of centimetres, which makes it difficult to reconstruct small objects accurately in indoor scenes. Finally, lidar data contains no texture or colour, so the final reconstruction is only geometric and cannot be used for applications such as view synthesis, which requires texture.

Alternatively, large-scale textured reconstructions can be recovered from camera images alone via SfM. Given the correspondences between images, a SfM system____ can optimise a set of camera poses, camera intrinsics, and 3D sparse feature points. This can then be used by a MVS system____ to compute dense depth for each frame and in turn to create a dense 3D point cloud. Compared to lidar, cameras are much more affordable, and also provide texture and colour. However, the performance of visual reconstruction method depends on environmental conditions, and the quality of feature matching. This makes the resultant reconstruction less reliable in scenes that contain repetitive patterns, low-texture surfaces, dynamic objects, poor lighting conditions and non-Lambertian materials.
% \todo{can add more reviews on SfM and MVS. See Oxford Spires}

%Urban Radiance Fields____ proposes using lidar sweeps along with RGB images to optimise a neural radiance field model that can be used for 3D surface extraction.

%Our work shares this approach, fusing both sensor modalities to generate precise geometry, overcoming limitations of vision-only approaches in low-texture areas, and being much denser than lidar-only reconstructions.

% \subsection{Large-scale Reconstruction}
When the scene to be reconstructed is large-scale (e.g., urban districts or multi-room indoor environments),  computer memory becomes a limiting factor. Attempting to map a large scene while constraining output map size will result in a lower resolution reconstruction, which results in a map that lacks detail. A common strategy to extend dense reconstructions to large-scale areas is to divide the scene into submaps____. For visual reconstruction with many thousands of images____, a submapping approach can significantly reduce computation time and memory requirements. One approach used in large-scale MVS is the submap partitioning____ which groups images into clusters while not degrading the final resultant reconstruction. After partitioning, each submap should be filtered and merged into one unified model. For online lidar mapping systems, the motivation for using submapping techniques is to accommodate loop closure corrections into the already-built map (Occupancy grid or TSDF). These systems construct submaps that are attached to a pose graph____, and can deform each submap by reoptimising the pose graph upon loop closure.

\begin{figure*}[t]
	\centering
	\includegraphics[width=2\columnwidth]{figures/pipeline.pdf}
	\caption{System overview: SiLVR builds large-scale reconstructions using images and lidar data, and a pose trajectory estimated by a separate odometry system. The sensor streams are provided by the \emph{Frontier}, our custom perception payload carrying three fisheye colour cameras, IMU measurements, and a 3D lidar. When collecting the data, we used VILENS____ to estimate the trajectory of the sensor, which is refined in post-processing using COLMAP____ and partitioned into submaps. The camera image, lidar depth, and a derivative normal image are used to train a NeRF to achieve a final 3D reconstruction. After training the NeRF, SiLVR estimates the epistemic uncertainty of the radiance field. Finally, the point cloud reconstruction is extracted from the NeRF by rendering a depth for each of the training rays. The point cloud is then filtered using per-point uncertainty estimates to remove unreliable reconstructions.}
	\label{fig:sys_overview}
\end{figure*}
% \mfallon{you have to make the Frontier bigger. The SiLVR pipeline block is quite minimal but it takes up quite a bit of space. It could be prettier to BTW - its a bit of a poor design. Matias could help. You should also say `Lidar SLAM under VILENS'.}

\subsection{Radiance Field Representation}
%Classic NeRF
Neural Radiance Fields (NeRF) was first proposed in the seminal paper from Mildenhall et al.____. The technique uses a multilayer perceptron (MLP) to represent a continuous radiance field, and uses differentiable volume rendering to reconstruct novel views. It minimises the photometric error between the rendered image and the input image, which implicitly achieves multi-view consistency.
NeRF and its many variants use frequency encoding____ to encode spatial coordinates, but these suffer from long training times, typically a few hours per scene. Alternative explicit representations of radiance fields, including dense voxel-grids with trainable per-vertex features____ and more recently 3D Gaussians____ are shown to accelerate the training, at the cost of being more memory intensive.
Octree or sparse-grid structures____ can reduce memory usage by pruning grid-features in empty space. 
%____ proposed a multi-resolution hash table which allowed scaling up to large scenes \mfallon{give a number for what you mean by large} without compromising rendering quality.  
Our work is built upon Nerfacto from the open-sourced Nerfstudio project____. It incorporates the main features from other NeRF works____ that have been found to work well with real data.

% Recent extensions include adapting these methods for 3D point clouds____, image-text domains____, and robot localization____.

% Surface Reconstruction from NeRFs
%\subsubsection{Neural Surface Reconstruction}
While NeRFs excel at high-quality view synthesis, obtaining a 3D surface reconstruction of similar quality remains challenging, mainly due to the flexible volumetric representation being under-constrained by the limited multi-view inputs. One approach to improve the reconstruction is to impose depth regularisation____ or surface normal regularisation____ which can be obtained from depth sensors or be estimated by a neural network.
% Implicit functions such as occupancy grids____ or Signed Distance Functions (SDFs)____ are better suited to surface reconstruction.
Another approach is to impose surface priors on the volumetric field and use 
representations such as Signed Distance Field (SDF)____ and 2D Gaussians____ to enforce a surface reconstruction output, although the novel view synthesis quality might be compromised____ with this approach. 
%Several methods have proposed using auxiliary geometric priors to improve surface reconstruction from sparse inputs. For example, Manhattan-SDF____ uses a Manhattan world prior, while MonoSDF____ uses learned monocular depth cues.
%Recently,____ proposed using multi-resolution hash encodings while____ used a coarse-to-fine approach for neural reconstruction. Both these methods achieve high-fidelity reconstruction from multi-view pose images without any auxiliary inputs.
Our method uses a volume density representation which is extended with depth____ and surface normal____ regularisations from lidar measurements instead of using SfM____ or learnt priors____. This can significantly improve the reconstruction quality in texture-less areas with smooth surfaces.


Neural field representations have been used for lidar-based mapping____, showing promise in generating more complete and compact reconstructions than traditional methods. While these works also build upon implicit map representations, they do not use visual data to build the map. Our system uses visual information and multi-view geometry constraints. Because of this, it can reconstruct regions outside of the lidar's field-of view.
% \todo{refine this}

\subsection{Uncertainty in Neural Radiance Fields}
The standard formulation of NeRF has no notion of uncertainty. The lack of uncertainty makes it difficult to apply them in robotics applications because a NeRF reconstruction could contain artefacts. From a Bayesian machine learning perspective, one could model the data uncertainty (aleatoric uncertainty) and model uncertainty (epistemic uncertainty)____ in the NeRF model. The data uncertainty models how the image observation differs from the trained NeRF, and the source of errors includes dynamic objects, lighting conditions and non-Lambertian surfaces. Dynamic object masking____ and appearance encoding____ have been used to model or mitigate data uncertainty.

The model uncertainty aims to capture the variance of the radiance field given the training data. For example, for a uniformly-textured area (e.g., sky) with parallel viewing angles, there are infinite possible NeRF solutions that can lead to exactly the same image pixel observation. In comparison, the NeRF of a textured object with a clear boundary and observations from multiple viewpoints would have low model uncertainty, and this is similar to the well-conditioned scenario for photogrammetry. The most straightforward and reliable way to quantify model uncertainty is to train an ensemble of models with different initialisations____. BayesRays____ proposes to model the uncertainty of a perturbation field instead, and estimates the uncertainty with the Laplace approximation. We extend BayesRays's perturbation field formulation to also incorporate lidar data, which allows us to obtain uncertainty estimates for both sensor modalities, and filter the results reconstruction considering each sensor's own characteristics.

% \todo{NeRF On-the-go____ ActiveNeRF____}

\subsection{Large-scale Neural Radiance Fields}

Submapping has been used in NeRF representations for city-scale reconstruction. There are several partitioning strategies that are based on grid partitioning____ or using road intersections____. Merging NeRF submaps is difficult, since each NeRF submap's boundary can be ambiguous, and the appearance encoding of each submap can be different____. %\todo{they require heuristics. Also check the literature}
Block-NeRF____ merges submaps by first selecting submap candidates based on distance and visibility, and combines submaps in the 2D image space with interpolation and test-time appearance matching. These methods either require manual submap partitioning____, or partition the scene into 2D grids____. Our work adopts the submapping approach, and develops partitioning strategies based on visibility, which is advantageous compared to 2D grid partitioning of image data that are close in Euclidean distance but in fact belong to isolated regions (e.g., rooms). We also develop novel strategies for submap merging which uses epistemic uncertainty estimation. 
% \todo{why it works}

% TODO:
% some large scale nerfs: Mega-NeRF____, city Gaussian, NeRF XL, Vast Gaussian____

% ____also uses normalised cut____


%Our work also adopts the submapping approach and partitions large-scale scenes into local maps (approximately 50x50m) using a globally-consistent lidar SLAM trajectory. This increases the representation capability and improves reconstruction, especially for thin objects.

% ~\todo{____ Tanks and Temples____}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%