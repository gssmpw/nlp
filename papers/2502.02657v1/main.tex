\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts,amssymb}
%\usepackage{algorithmic}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{color}
\usepackage{pifont}
\usepackage{subcaption} %subfigure 
\usepackage{algorithm}  
\usepackage{algpseudocode} 
\usepackage{tabularx}
\usepackage{gensymb}
\usepackage{lipsum}
\usepackage{arydshln} % dash line

%\usepackage[ruled,vlined]{algorithm2e}
%\include{pythonlisting}

\newcommand{\todo}[1]{\textcolor{orange}{\emph{\bf#1}}}
\newcommand{\mfallon}[1]{\textcolor{red}{#1}}
\newcommand{\matias}[1]{\textcolor{magenta}{Matias: #1}}
% \newcommand{\etao}[1]{\textcolor{blue}{#1}}

% \usepackage{xcolor}
\usepackage[table]{xcolor}
\definecolor{myOrange}{rgb}{1, 0.5, 0.055} %255, 127, 14
\definecolor{myRed}{rgb}{0.84, 0.15, 0.157} %214, 39, 40
\definecolor{myGreen}{rgb}{0.173, 0.627, 0.173} %
\definecolor{top1}{RGB}{111, 170, 247}
\definecolor{top2}{RGB}{169, 204, 250}
\definecolor{top3}{RGB}{226, 238, 253}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\halfcmark}{\checkmark\kern-1.1ex\raisebox{.7ex}{\rotatebox[origin=c]{125}{--}}}

%% Key definitions for text elements. USE ONLY THEM! Do not use naked \ref{}.
\def\secref#1{Sec.~\ref{#1}}
\def\figref#1{Fig.~\ref{#1}}
\def\tabref#1{Tab.~\ref{#1}}
\def\eqref#1{Eq.~(\ref{#1})}
\def\algref#1{Alg.~\ref{#1}}



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}


\begin{document}
\title{SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification}

% \author{Anonymous Submission}
\author{Yifu Tao$^1$, Maurice Fallon$^1$
}

\markboth{IEEE TRANSACTIONS ON ROBOTICS,~Vol.xx, No.xx, Month ~Year}
{Zhang \MakeLowercase{\textit{et al.}}: Localisation in 3D maps} 

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \vspace{-25pt}
    \centering
    % left bottom right up
	\includegraphics[width=1\textwidth]{figures/hero_figure.pdf}
	\captionof{figure}{Two large-scale reconstructions generated by SiLVR. Rendered RGB and surface normal images from the reconstructions are shown on each side. SiLVR combines visual and lidar information to create geometrically accurate maps with photorealistic textures, while considering sensor uncertainty. SiLVR uses submaps to scale to large-scale building complexes.}
    \label{fig:hero}
\end{center}%
}]


\begingroup
  \renewcommand\thefootnote{}\footnote{
  $^1$Oxford Robotics Inst., Dept. of Engineering Sci., Univ. of Oxford, UK. \\
 This project has been partly funded by the Horizon Europe project Digiforest (101070405). Maurice Fallon is supported by a Royal Society University Research Fellowship. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Accepted Manuscript version arising.
  }
  \addtocounter{footnote}{-1}%
\endgroup

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
We present a neural radiance field (NeRF) based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photorealistic texture. Our system adopts the state-of-the-art NeRF representation to additionally incorporate lidar. Adding lidar data adds strong geometric constraints on the depth and surface normals, which is particularly useful when modelling uniform texture surfaces which contain ambiguous visual reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the reconstruction as the spatial variance of each point location in the radiance field given the sensor observations from camera and lidar. This enables the identification of areas that are reliably reconstructed by each sensor modality, allowing the map to be filtered according to the estimated uncertainty. Our system can also exploit the trajectory produced by a real-time pose-graph lidar SLAM system during online mapping to bootstrap a (post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing SfM training time by up to 70\%. It also helps to properly constrain the overall metric scale which is essential for the lidar depth loss. The globally-consistent trajectory can then be divided into submaps using Spectral Clustering to group sets of co-visible images together. This submapping approach is more suitable for visual reconstruction than distance-based partitioning. Each submap is filtered according to point-wise uncertainty estimates and merged to obtain the final large-scale 3D reconstruction. We demonstrate the reconstruction system using a multi-camera, lidar sensor suite in experiments involving both robot-mounted and handheld scanning. Our test datasets cover a total area of more than \( \text{20,000}~\text{m}^2 \), including multiple university buildings and an aerial survey of a multi-storey. Quantitative evaluation is provided by comparing to maps produced by a commercial tripod scanner. The code and dataset will be made open-source.


\end{abstract}
\begin{IEEEkeywords}
Mapping, Sensor Fusion, RGB-D Perception, Neural Radiance Field (NeRF), Uncertainty Estimation
\end{IEEEkeywords}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Dense 3D reconstruction is a core component of a range of robotics applications such as industrial inspection and autonomous navigation. Common sensors used for reconstruction include cameras and lidars. Camera-based reconstruction systems use techniques including Structure-from-Motion (SfM~\cite{schoenberger2016colmap}) and Multi-View Stereo (MVS)~\cite{schoenberger2016mvs} to produce dense textured reconstructions. However, these approaches rely on good lighting conditions and can require exhaustive data collection to capture data from diverse viewpoints. They also struggle with textureless areas such as bare walls, ceilings and floors. A lidar sensor provides accurate geometric information at long range---as it actively measures distances to surfaces. This makes lidar desirable for large-scale outdoor environments. However, lidar scans are much sparser than camera images. They also do not capture colour which is important for applications such as virtual reality and 3D asset generation.

Classical reconstruction systems have used point clouds, occupancy maps, and sign-distance fields (SDF) as their internal 3D representation. Recently, radiance field representations, namely neural radiance fields (NeRF)~\cite{mildenhall2021nerf} and 3D Gaussian Splatting (3DGS)~\cite{kerbl3Dgaussians} have gained popularity for visual reconstruction. Taking advantage of differentiable rendering, these techniques optimise a 3D representation by minimising the difference between a rendered image and a reference camera image. These methods can synthesise novel views with near photorealistic quality, which can be useful for robotic inspection and visual localisation.

As with traditional vision-based reconstruction methods, NeRF struggles to estimate accurate geometry in locations where there is limited multi-view input (i.e. when images are only taken from a single direction) or sparse texture. These challenges affect not only volume-density-based fields, but also SDF-based fields~\cite{wang2021neus} and 3D Gaussian representations~\cite{kerbl3Dgaussians}, as can be seen later in \figref{fig:reconstruction_eval_large_scale}. Autonomous systems commonly encounter these situations---for example, a robot reconstructing a wall with uniform colour when travelling directly towards it. In this paper, we show how this problem can be addressed by using lidar sensing to provide accurate geometric measurements of these types of textureless objects. In addition, the use of lidar data can reduce the need to capture varied camera views. It is impractical for an inspection robot (e.g., Spot quadruped on an industrial facility) to execute object-centric trajectories just to improve visual reconstruction. This motivates the development of a reconstruction system that fuses these sensors. 
% \mfallon{I think you should say fields - not field. Or, if you are sure `a radience field'.}

The reconstruction task becomes even more challenging when the scene is large-scale (e.g., urban districts). Running an incremental SfM algorithm such as COLMAP~\cite{schoenberger2016colmap} for a large-scale scene is time-consuming, and is not guaranteed to succeed in registering all input images (especially if parts of the scene have poor lighting), which then could lead to an unreconstructed region in the map. As the scene and the dataset size grow, the model capacity of a NeRF and memory constraints become a bottleneck. Simply increasing the size of the model parameters (e.g., hash table size in Instant-NGP~\cite{mueller2022instant} or the number of 3D Gaussians~\cite{kerbl3Dgaussians}) can exceed available computer memory when working on larger scenes. This motivates the development of a scene partitioning strategy. Some existing methods require manually partitioning using heuristics~\cite{tancik2022block} or evenly partitioning the scene using a grid~\cite{meganerf}. The limitation of a simple grid-based partition is that the view orientation and visibility are not considered, while it is important to consider these factors while carrying out clustering for visual reconstruction. For example, an image taken from a corridor outside of a room but looking into it ought to be considered part of a submap of that room. 
% the MLP of a NeRF usually does not generate quality comparable to training a smaller scene, and explicit representations such as 3DGS \mfallon{YOU PRESENT THIS AS BEING A CHARACTERISTIC OF 3DGS - BUT IT SHOULD BE PRESENTED AS A CHALLENGE. WHAT ABOUT `AND CAN EXCEED AVAILABLE MEMORY WHEN WORKING ON ...' 
% \todo{mention that we use NeRF for the compressed size.

The reconstructed 3D map from a NeRF pipeline can contain artefacts due to unconstrained views, and removing them can be challenging. The implicit representation used in NeRF, while providing tremendous model size compression compared to explicit representations such as 3DGS~\cite{kerbl3Dgaussians}, can generate reconstruction artefacts outside of the observed area.
The standard NeRF formulation has no notion of uncertainty for parts of the reconstruction that are unreliable. The regions which can be confidently reconstructed by a camera or a lidar are usually complementary: regions that have uniform texture or lack multi-view constraints often have high \textit{visual uncertainty}, and regions that are outside of the lidar Field-of-View or beyond the lidar's sensing range often have high \textit{lidar uncertainty}. These reconstruction artefacts make it challenging when merging submaps, as each submap will contain noisier reconstruction on its boundary as the measurement density falls off. This can lead to artefacts in the overall merged map. These issues motivate the need for a principled approach to quantifying the reconstruction uncertainty of NeRF maps.
% todo: \todo{Relocate the reference of 3DGS? maybe compare nerf and neus and gs in the same paragraph} 
% todo: In addition, the sky and clouds are often reconstructed as 3D structures floating on the top of the scene. keep this??
%todo consider the lidar incident angle?

% \mfallon{In your video, I think you should have a flythrough for the baseline approach which shows the artefacts which existing on the boundary of the submaps during submap transition}


In this work, we present SiLVR, a submap-based NeRF reconstruction system that integrates both lidar and visual information to produce accurate, textured, and uncertainty-aware 3D reconstructions which can also synthesise photorealistic novel views. SiLVR builds upon existing NeRF research and the Nerfacto implementation~\cite{nerfstudio} 
which utilises hash encoding~\cite{mueller2022instant} that is significantly faster than MLP-based NeRF~\cite{mildenhall2021nerf} (it takes less than 5 minutes to train a NeRF for one submap in our experiments). We extend this work by adding geometric constraints from lidar to improve reconstruction quality. Our use of lidar data is particularly important for modelling featureless areas where geometry cannot be accurately reconstructed using 3D SfM features~\cite{deng2022depth}. In addition, we also estimate surface normals from the lidar scans to encourage smooth surface reconstruction. This approach does not suffer from input data distribution shift, a characteristic of learning-based normal estimation approaches~\cite{Yu2022MonoSDF}.

Compared to prior NeRF-based reconstruction systems that incorporate lidar~\cite{tao2024silvr} or depth cameras~\cite{azinovic2022neural,Yu2022MonoSDF}, our key innovation is a rigorous study of how to quantify uncertainty in the resultant reconstruction, which enables improved reconstruction accuracy as well as facilitating downstream tasks such as active mapping and navigation. After training, SiLVR computes the epistemic uncertainty of the NeRF with the Laplace approximation (LA)~\cite{daxberger2021laplace} and Fisher-Information-approximated Hessian matrix. As an efficient alternative to ensemble learning (which requires multiple iterations of training of the NeRF model and is time-consuming), we build upon the formulation of the perturbation field proposed in BayesRays~\cite{goli2023bayesrays} and use the spatial variance of the field as the measure of epistemic uncertainty. This is used to filter out reconstruction artefacts which improves the final reconstruction accuracy. We also adapt a previously developed lidar SLAM algorithm~\cite{wisth2023vilens,ramezani2020slam} to bootstrap the SfM component accelerating its operation by up to 70\%) and to enforce an accurate metric scale. For mapping large-scale building complexes or a city block, we adopt a submapping approach and apply graph partitioning algorithm~\cite{shi2000normalized} with visibility information to divide the complete trajectory into submaps. We study how the use of visibility information allows the submaps to be more self-contained and to have fewer artefacts at their boundaries compared to methods that only consider spatial information~\cite{tao2024silvr}.

% The work presented in this paper represents a significant extension of our previous work~\cite{tao2024silvr}. The additional contributions are:
In summary, our contributions are as follows:
\begin{itemize}
    \item Epistemic uncertainty quantification of the multi-modal NeRF pipeline which considers lidar and visual uncertainties differently. This allows us to filter parts of the reconstruction that are unreliable by directly using sensor-specific characteristics. To do this, we extended BayesRays~\cite{goli2023bayesrays}'s vision-only uncertainty framework to also support lidar depth. This allows us to identify areas with reliable reconstruction (e.g., where there are visual features or abundant lidar measurements) and unreliable areas (e.g., uniformly-textured surfaces which have also not been scanned by lidar). Filtering based on the uncertainty estimates improves the reconstruction accuracy. This is a more principled approach compared to the surface-density-based filtering~\cite{tao2024silvr}.
    \item A submapping strategy that leverages per-image visibility information. Compared to the distance-based clustering method~\cite{tao2024silvr}, we develop a visibility-based clustering method which reduces visual overlap between submaps and in turn creates fewer artefacts at submap boundaries.
    \item Large-scale evaluation using two large-scale datasets from the Oxford Spires dataset~\cite{tao2024spires} with quantitative results from millimetre-accurate 3D ground truth. Further comparison is made to baseline radiance field methods that use SDF~\cite{wang2021neus} and 3D Gaussians~\cite{kerbl3Dgaussians} representations.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Related Work %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\label{sec:related_works}

% Pre-NeRF related works
\subsection{Classical 3D Reconstruction}
Lidars and cameras are the two main modalities used in robotic perception and specifically for 3D reconstruction. In this section, we review classical lidar-based and vision-based reconstruction methods. For each sensor modality, trajectory estimation is a typical first step in a reconstruction pipeline. Then, we review strategies for extending these methods to large-scale scenes.

Lidar is the dominant sensor used for accurate 3D reconstruction of large-scale environments~\cite{behley2018rss, lin2022r3live}. It actively transmits laser pulses to measure ranges and as a result is accurate up to ranges of as much 100m. With these distance measurements, Lidar odometry typically uses a variant of Iterative Closest Point (ICP), and often integrates high-frequency IMU measurements~\cite{zhang2014loam, zhao2021super, xu2022fast, wisth2023vilens}. Small errors in odometry can accumulate over time resulting in trajectory drift. This drift can be mitigated when a sensor revisits a previous place and detects loop closure with pose graph optimisation~\cite{thrun2006graph} which allows a consistent map to be maintained.
After registering all the lidar scans, the (surface) reconstruction problem is then to fuse discrete observations into a map. %\todo{check supereight}
Example map representations include surfels~\cite{whelan2016elasticfusion,park2018elastic}, voxels~\cite{hornung13octomap,newcombe2011kinectfusion,oleynikova2017voxblox,Vespa2018supereight} and wavelets~\cite{reijgwart2023wavemap}. 
% (which can model occupancy~\cite{hornung13octomap} or Signed Distance Function (SDF)~\cite{newcombe2011kinectfusion}), and 
Despite its advantages, lidar has its own limitations. Lidar is much more expensive than cameras, while the measurements are typically much sparser than camera images. The measurements have inherent noise with ranging errors in the order of centimetres, which makes it difficult to reconstruct small objects accurately in indoor scenes. Finally, lidar data contains no texture or colour, so the final reconstruction is only geometric and cannot be used for applications such as view synthesis, which requires texture.

Alternatively, large-scale textured reconstructions can be recovered from camera images alone via SfM. Given the correspondences between images, a SfM system~\cite{schoenberger2016colmap} can optimise a set of camera poses, camera intrinsics, and 3D sparse feature points. This can then be used by a MVS system~\cite{furukawa2010dense} to compute dense depth for each frame and in turn to create a dense 3D point cloud. Compared to lidar, cameras are much more affordable, and also provide texture and colour. However, the performance of visual reconstruction method depends on environmental conditions, and the quality of feature matching. This makes the resultant reconstruction less reliable in scenes that contain repetitive patterns, low-texture surfaces, dynamic objects, poor lighting conditions and non-Lambertian materials.
% \todo{can add more reviews on SfM and MVS. See Oxford Spires}

%Urban Radiance Fields~\cite{rematas2022urban} proposes using lidar sweeps along with RGB images to optimise a neural radiance field model that can be used for 3D surface extraction.

%Our work shares this approach, fusing both sensor modalities to generate precise geometry, overcoming limitations of vision-only approaches in low-texture areas, and being much denser than lidar-only reconstructions.

% \subsection{Large-scale Reconstruction}
When the scene to be reconstructed is large-scale (e.g., urban districts or multi-room indoor environments),  computer memory becomes a limiting factor. Attempting to map a large scene while constraining output map size will result in a lower resolution reconstruction, which results in a map that lacks detail. A common strategy to extend dense reconstructions to large-scale areas is to divide the scene into submaps~\cite{bosse2003atlas}. For visual reconstruction with many thousands of images~\cite{agarwal2011building}, a submapping approach can significantly reduce computation time and memory requirements. One approach used in large-scale MVS is the submap partitioning~\cite{furukawa2010towards} which groups images into clusters while not degrading the final resultant reconstruction. After partitioning, each submap should be filtered and merged into one unified model. For online lidar mapping systems, the motivation for using submapping techniques is to accommodate loop closure corrections into the already-built map (Occupancy grid or TSDF). These systems construct submaps that are attached to a pose graph~\cite{ho2018virtual,reijgwart2020voxgraph,wang2022strategies}, and can deform each submap by reoptimising the pose graph upon loop closure.

\begin{figure*}[t]
	\centering
	\includegraphics[width=2\columnwidth]{figures/pipeline.pdf}
	\caption{System overview: SiLVR builds large-scale reconstructions using images and lidar data, and a pose trajectory estimated by a separate odometry system. The sensor streams are provided by the \emph{Frontier}, our custom perception payload carrying three fisheye colour cameras, IMU measurements, and a 3D lidar. When collecting the data, we used VILENS~\cite{wisth2023vilens} to estimate the trajectory of the sensor, which is refined in post-processing using COLMAP~\cite{schoenberger2016colmap} and partitioned into submaps. The camera image, lidar depth, and a derivative normal image are used to train a NeRF to achieve a final 3D reconstruction. After training the NeRF, SiLVR estimates the epistemic uncertainty of the radiance field. Finally, the point cloud reconstruction is extracted from the NeRF by rendering a depth for each of the training rays. The point cloud is then filtered using per-point uncertainty estimates to remove unreliable reconstructions.}
	\label{fig:sys_overview}
\end{figure*}
% \mfallon{you have to make the Frontier bigger. The SiLVR pipeline block is quite minimal but it takes up quite a bit of space. It could be prettier to BTW - its a bit of a poor design. Matias could help. You should also say `Lidar SLAM under VILENS'.}

\subsection{Radiance Field Representation}
%Classic NeRF
Neural Radiance Fields (NeRF) was first proposed in the seminal paper from Mildenhall et al.~\cite{mildenhall2021nerf}. The technique uses a multilayer perceptron (MLP) to represent a continuous radiance field, and uses differentiable volume rendering to reconstruct novel views. It minimises the photometric error between the rendered image and the input image, which implicitly achieves multi-view consistency.
NeRF and its many variants use frequency encoding~\cite{vaswani2017attention} to encode spatial coordinates, but these suffer from long training times, typically a few hours per scene. Alternative explicit representations of radiance fields, including dense voxel-grids with trainable per-vertex features~\cite{fridovich2022plenoxels,mueller2022instant} and more recently 3D Gaussians~\cite{kerbl3Dgaussians} are shown to accelerate the training, at the cost of being more memory intensive.
Octree or sparse-grid structures~\cite{yu2021plenoctrees,mueller2022instant} can reduce memory usage by pruning grid-features in empty space. 
%~\cite{mueller2022instant} proposed a multi-resolution hash table which allowed scaling up to large scenes \mfallon{give a number for what you mean by large} without compromising rendering quality.  
Our work is built upon Nerfacto from the open-sourced Nerfstudio project~\cite{nerfstudio}. It incorporates the main features from other NeRF works~\cite{mueller2022instant,barron2022mipnerf360,martinbrualla2020nerfw} that have been found to work well with real data.

% Recent extensions include adapting these methods for 3D point clouds~\cite{PointNeRF,xu2022point}, image-text domains~\cite{clipnerf}, and robot localization~\cite{devries2021unconstrained,sucar2021imap,LENS,neural_scene3d_corl}.

% Surface Reconstruction from NeRFs
%\subsubsection{Neural Surface Reconstruction}
While NeRFs excel at high-quality view synthesis, obtaining a 3D surface reconstruction of similar quality remains challenging, mainly due to the flexible volumetric representation being under-constrained by the limited multi-view inputs. One approach to improve the reconstruction is to impose depth regularisation~\cite{deng2022depth, rematas2022urban} or surface normal regularisation~\cite{Yu2022MonoSDF} which can be obtained from depth sensors or be estimated by a neural network.
% Implicit functions such as occupancy grids~\cite{occupancy} or Signed Distance Functions (SDFs)~\cite{sdf} are better suited to surface reconstruction.
Another approach is to impose surface priors on the volumetric field and use 
representations such as Signed Distance Field (SDF)~\cite{neuralangelo,yariv2021volume} and 2D Gaussians~\cite{huang20242dgs} to enforce a surface reconstruction output, although the novel view synthesis quality might be compromised~\cite{wang2021neus} with this approach. 
%Several methods have proposed using auxiliary geometric priors to improve surface reconstruction from sparse inputs. For example, Manhattan-SDF~\cite{guo2022neural} uses a Manhattan world prior, while MonoSDF~\cite{Yu2022MonoSDF} uses learned monocular depth cues.
%Recently,~\cite{neuralangelo} proposed using multi-resolution hash encodings while~\cite{hfneus} used a coarse-to-fine approach for neural reconstruction. Both these methods achieve high-fidelity reconstruction from multi-view pose images without any auxiliary inputs.
Our method uses a volume density representation which is extended with depth~\cite{deng2022depth} and surface normal~\cite{Yu2022MonoSDF} regularisations from lidar measurements instead of using SfM~\cite{deng2022depth} or learnt priors~\cite{Yu2022MonoSDF}. This can significantly improve the reconstruction quality in texture-less areas with smooth surfaces.


Neural field representations have been used for lidar-based mapping~\cite{zhong2023icra,deng2023nerfloam,pan2024pinslam}, showing promise in generating more complete and compact reconstructions than traditional methods. While these works also build upon implicit map representations, they do not use visual data to build the map. Our system uses visual information and multi-view geometry constraints. Because of this, it can reconstruct regions outside of the lidar's field-of view.
% \todo{refine this}

\subsection{Uncertainty in Neural Radiance Fields}
The standard formulation of NeRF has no notion of uncertainty. The lack of uncertainty makes it difficult to apply them in robotics applications because a NeRF reconstruction could contain artefacts. From a Bayesian machine learning perspective, one could model the data uncertainty (aleatoric uncertainty) and model uncertainty (epistemic uncertainty)~\cite{kendall2017uncertainties} in the NeRF model. The data uncertainty models how the image observation differs from the trained NeRF, and the source of errors includes dynamic objects, lighting conditions and non-Lambertian surfaces. Dynamic object masking~\cite{Sabour2023robustnerf} and appearance encoding~\cite{martinbrualla2020nerfw} have been used to model or mitigate data uncertainty.

The model uncertainty aims to capture the variance of the radiance field given the training data. For example, for a uniformly-textured area (e.g., sky) with parallel viewing angles, there are infinite possible NeRF solutions that can lead to exactly the same image pixel observation. In comparison, the NeRF of a textured object with a clear boundary and observations from multiple viewpoints would have low model uncertainty, and this is similar to the well-conditioned scenario for photogrammetry. The most straightforward and reliable way to quantify model uncertainty is to train an ensemble of models with different initialisations~\cite{lakshminarayanan2017simple}. BayesRays~\cite{goli2023bayesrays} proposes to model the uncertainty of a perturbation field instead, and estimates the uncertainty with the Laplace approximation. We extend BayesRays's perturbation field formulation to also incorporate lidar data, which allows us to obtain uncertainty estimates for both sensor modalities, and filter the results reconstruction considering each sensor's own characteristics.

% \todo{NeRF On-the-go~\cite{ren2024nerf-onthego} ActiveNeRF~\cite{pan2022activenerf}}

\subsection{Large-scale Neural Radiance Fields}

Submapping has been used in NeRF representations for city-scale reconstruction. There are several partitioning strategies that are based on grid partitioning~\cite{meganerf} or using road intersections~\cite{tancik2022block}. Merging NeRF submaps is difficult, since each NeRF submap's boundary can be ambiguous, and the appearance encoding of each submap can be different~\cite{martinbrualla2020nerfw}. %\todo{they require heuristics. Also check the literature}
Block-NeRF~\cite{tancik2022block} merges submaps by first selecting submap candidates based on distance and visibility, and combines submaps in the 2D image space with interpolation and test-time appearance matching. These methods either require manual submap partitioning~\cite{tancik2022block}, or partition the scene into 2D grids~\cite{meganerf}. Our work adopts the submapping approach, and develops partitioning strategies based on visibility, which is advantageous compared to 2D grid partitioning of image data that are close in Euclidean distance but in fact belong to isolated regions (e.g., rooms). We also develop novel strategies for submap merging which uses epistemic uncertainty estimation. 
% \todo{why it works}

% TODO:
% some large scale nerfs: Mega-NeRF~\cite{meganerf}, city Gaussian, NeRF XL, Vast Gaussian~\cite{lin2024vastgaussian}

% ~\cite{zhu2018very}also uses normalised cut~\cite{shi2000normalized}


%Our work also adopts the submapping approach and partitions large-scale scenes into local maps (approximately 50x50m) using a globally-consistent lidar SLAM trajectory. This increases the representation capability and improves reconstruction, especially for thin objects.

% ~\todo{~\cite{agarwal2011building,furukawa2010towards,zhang2015joint} Tanks and Temples~\cite{knapitsch2017tanks}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}
%%%%%%%%%%%%%%%%%%%%%%% NeRF %%%%%%%%%%%%%%%%%%%%%%%

\subsection{Radiance Field Formulation}
% \mfallon{this first sentence implies that there is only on possible framework e.g. like The Internet or The Earth or The Solar System. Should you instead say 'builds upon research into diff. volume rendering which enable novel view synthesis includ ...'}
We start by adopting the radiance field representation and the differentiable volume rendering framework originally proposed by Mildenhall et. al~\cite{mildenhall2021nerf}. The radiance field models the scene as a function $R:(\mathbf{p},\mathbf{d}) \mapsto (\mathbf{c},\sigma)$ where the input includes a 3D location $\mathbf{p}=(p_x,p_y,p_z)$ and 2D viewing direction $\mathbf{d}=(\phi,\psi)$, the output is an emitted colour $\mathbf{c}=(r,g,b)$ and a volume density $\sigma$.
% Our reconstruction system builds on the differentiable volume rendering framework used for novel view synthesis~\cite{mildenhall2021nerf,lombardi2019neural, sitzmann2019deepvoxels, sitzmann2019scene}.
To render a novel view using a NeRF from a particular viewpoint, we cast rays $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ from the camera origin $\mathbf{o}$ along the viewing direction $\mathbf{d}$ of each pixel $\mathbf{u}$ in the image plane, and render the pixel-colour by integrating over the set of points sampled along the ray. The pixel colour $\hat{\mathbf{C}}(\mathbf{r})$ is computed by the volume rendering integral which is approximated using the quadrature rule~\cite{max1995optical,kajiya1984ray} as 
\begin{equation}
\hat{\mathbf{C}}(\mathbf{r}) = \sum_{i=0}^{N} w_i \mathbf{c}_{i},
\end{equation}

where $\mathbf{c}_i$ is the colour of the $i$-th point sample along the ray, and its weighting coefficient $w_i$ is defined by 
\begin{equation} \label{eq:rendering}
% T_i = e^{-\delta_i \sigma_i} \; \text{,} \; v_{i}=\prod_{q=0}^{i-1} T_{q} \quad \text{and} \quad w_i = v_{i}\left(1-T_{i}\right)
w_i =   \exp{\left(-\sum_{j=1}^{i-1} \delta_j \sigma_j\right)} (1-\exp{(-\delta_i\sigma_i)}).
\end{equation}
where $\delta_i$ is the distance between adjacent samples.
% Here, $\sigma_i$ and $c_i$ are the predicted density and color for the sampled 3D points and $\hat{c}_u$ is the rendered pixel color.

The radiance field can be trained with a squared photometric loss given the ground truth colour $\mathbf{C}(\mathbf{r})$ from the input image:
\begin{equation}
\mathcal{L}_{\text {Colour}}=\sum_{\mathbf{r} \in \mathcal{D}} ||\hat{\mathbf{C}}(\mathbf{r})-\mathbf{C}(\mathbf{r})||^2  
\label{eq:photometric_loss}
\end{equation}
where $\mathcal{D}$ is the whole training dataset used to generate the rays $\mathbf{r}$ and ground truth colour $\mathbf{C}(\mathbf{r})$.

Our system extends upon Nerfacto, which is a specific pipeline implemented within the Nerfstudio framework~\cite{nerfstudio}. Nerfacto's rendering quality is comparable to state-of-the-art methods such as MipNeRF-360~\cite{barron2022mipnerf360} while achieving a substantial acceleration in reconstruction speed as it also incorporates efficient hash encoding which was proposed by the authors of Instant-NGP~\cite{mueller2022instant}. The scene contraction, proposed in~\cite{barron2022mipnerf360}, is also used to improve memory efficiency and to represent scenes with high-resolution content near the input camera locations. The contraction function non-linearly maps any point in space into a cube of side length 2, and represents the scene within this contracted space. In real-world outdoor environments, there is often large variation in exposure and lighting conditions. Because of this we use a per-frame appearance encoding for each image, similar to~\cite{rematas2022urban,martinbrualla2020nerfw}.


\subsection{Bayesian Interpretation of the NeRF training}
\label{sec:bayesian_nerf}
The NeRF reconstruction i.e. the radiance field function $f$ is deterministic, and has no explicit notion of uncertainty. In practice, different parts of the NeRF reconstruction are inherently more uncertain or less reliable. For example, ill-conditioned visual constraints from non-textured areas or insufficient visual parallax can cause the visual reconstruction accuracy to deteriorate. Another example specifically relevant to our work is that a surface is more uncertain if it has only been sparsely scanned by the lidar with limited range and Field-of-View compared to a surface that is densely scanned. Quantifying these uncertainties associated with the NeRF reconstruction allows one to identify the unreliable reconstruction and filter them accordingly, hence improving reconstruction accuracy. It can also enable downstream tasks such as active mapping and navigation~\cite{Zhang19fisherinfofield}.

% , often more than a reconstruction created by classical MVS methods (which is shown in \cite{tao2024spires} and in \secref{tab:reconstruction_eval_large_scale}). 


% However, ill-conditioned visual constraints from non-textured areas or insufficient parallax, can deteriorate the reconstruction. Similarly, areas without lidar measurements should also reflect inferior reconstruction quality. Therefore, estimating the uncertainties associated to a NeRF reconstruction can help improve the quality of the representation, while also enabling downstream tasks such as active mapping and navigation

% The NeRF reconstruction i.e. the radiance field function $f$ is deterministic, and has no explicit notion of uncertainty. The reconstruction uncertainty could come from limited visual constraints (e.g., uniform-textured areas). It can also come from limited or no image/lidar observation (the unknown space). Quantifying the uncertainty in the reconstruction is important. For example in robotic navigation, unknown space can be used to guide active mapping. For a reconstruction produced by the NeRF representation, any volume that is not reliably reconstructed or has not been observed should not be rendered or exported as a point cloud. 
% what region is covered by the input camera views (the known space) and what is not (the unknown space. This is because 
% The implementation from Instant-NGP~\cite{mueller2022instant} uses MLP and hash tables to model 
% the radiance field function $f_\theta$ (implemented by an MLP~\cite{mildenhall2021nerf} or with hash grid~\cite{mueller2022instant}) can output a colour $\mathbf{c}$ and volume density $\sigma$ for any point $\mathbf{x}\in \mathbb{R}^3$. 

% Therefore, for a distant location that is not observed by our sensor, it still has some colour and volume density that is not optimised and hence unreliable. 

The Bayesian probability theory provides useful tools for quantifying the uncertainties in neural models~\cite{kendall2017uncertainties}, which can benefit the NeRF reconstruction. For a regression task with the dataset $\mathcal{D}=\{(x_n, y_n)\}_{n=1}$ (where $x_n,y_n$ are the $n$-th pair of the input and output) and model parameters $\theta$, the uncertainty of the predictive distribution $p(y|x,\mathcal{D})$ can be approximated by considering the data (aleatoric) uncertainty in the likelihood $p(y|\theta,x)$ and model (epistemic) uncertainty in the posterior $p(\theta|\mathcal{D})$.

% The Bayesian formulation of deep learning provides useful tools for quantifying uncertainty. For a regression task, the uncertainty of the predictive distribution $p(y|x,\mathcal{D})$ can be approximated by considering the data (aleatoric) uncertainty in the likelihood $p(y|\theta,x)$ and model (epistemic) uncertainty in the posterior $p(\theta|\mathcal{D})$. Data uncertainty is the inherent noise in the observations. This cannot be reduced with more training data. In contrast, model uncertainty is the uncertainty due to lack of knowledge/training data, and can be reduced with more data. 
% \mfallon{this reads just as Kendall wrote it in his paper - but is this paragraph well linked into what follows? To me it doesnt seem to be.}

We first describe the NeRF training from a Bayesian perspective. When training the NeRF, we seek to minimise the total training loss $\mathcal{L}(\mathcal{D};\theta)$ with respect to the NeRF parameters $\theta$ (e.g., using the photometric loss from \eqref{eq:photometric_loss} if only vision is provided). This is equivalent to computing the maximum a-posteriori (MAP) estimate $\hat{\theta}$

\begin{equation}
\begin{aligned}
\hat{\theta} &= \arg\max_{\theta} p(\theta \mid \mathcal{D}) \\
&= \arg\max_{\theta} \big[ \log p(\mathcal{D} \mid \theta) + \log p(\theta) \big]\\
&= \arg\min_{\theta} \big[ -\log p(\mathcal{D} \mid \theta) - \log p(\theta) \big] \\
% &= \arg\min_{\theta} \big[ -\log p(D \mid \theta) - \log p(\theta) \big] \\
&= \arg\min_{\theta} \big[  \sum_{n=1}^N \ell(x_n, y_n; \theta) + r(\theta)] \\
&=\arg\min_{\theta} \mathcal{L}(\mathcal{D}; \theta)
\end{aligned}
\label{eq:map}
\end{equation}

where $\ell(x_n, y_n; \theta)=-\log p(y_n | f_\theta(x_n))$ is the loss term that corresponds to the negative log-likelihood for each data sample, and $r(\theta)= - \log p(\theta)$ is the weight regularisation that corresponds to the log-prior.

It can be seen from \eqref{eq:map} that the total training loss can be interpreted as $ \mathcal{L}(\mathcal{D}; \theta)=-\log p(\mathcal{D}|\theta) - \log p(\theta)$. The exponential of the negative training loss then corresponds to the unnormalised posterior $p(\mathcal{D}|\theta)p(\theta)$:
\begin{equation}
p(\theta \mid \mathcal{D}) = \frac{1}{Z} p(\mathcal{D} \mid \theta) p(\theta) 
= \frac{1}{Z} \exp\big(-\mathcal{L}(\mathcal{D}; \theta)\big)
\label{eq:posterior_and_loss}
\end{equation}
where the normalising constant $Z$ is the normalising constant, and is defined as:
\begin{equation}
Z = \int p(\mathcal{D} \mid \theta) p(\theta) \, d\theta
\label{eq:normalise_constant}
\end{equation}

Here, the posterior $p(\theta|\mathcal{D})$ is used for the uncertainty estimation  described later in \secref{sec:uncertainty}


\section{Method}



%%%%%%%%%%%%%%%%%%% Lidar loss %%%%%%%%%%%%%%%%%%%
\subsection{Geometric Constraints from Lidar Measurements}

Image-based 3D reconstruction with NeRF becomes challenging when a surface has uniform texture and limited multi-view constraints. Lidar measurements are complementary as they can provide accurate depth measurements in such scenarios. In our work, we incorporate the lidar measurements directly into the NeRF optimization. Specifically, we project the lidar point cloud from VILENS-SLAM's pose graph onto the image plane using the camera intrinsics and lidar-camera extrinsics (described in \secref{sec:implementation}) to form a depth image. We denote the lidar depth images as $\mathcal{D}_d$. Each frame of the lidar point cloud is motion-undistorted to the nearest image's timestamp, and hence synchronised with the image data. Example overlays can be found in \figref{fig:sample_overlay}. 
% Here, we experiment with three different loss functions \etao{~\cite{wang2023digging}}
% \subsubsection{Expected Depth Loss}
% This is a simple least squares loss on the expected depth. \etao{check euc depth when using fisheye}
% \begin{equation}
% \hat{d} = \sum_{i=1}^{N} w_i d_i
% \end{equation}
% \begin{equation}
% \mathcal{L}_{\text {Depth-MSE}}=||\hat{d} - d_{gt}||_2^2
% \end{equation}
% \subsubsection{KL-Divergence Loss}

\subsubsection{Lidar Depth Constraints}
\label{sec:method_depth_constraints}
We follow the depth regularisation approach proposed by DS-NERF \cite{deng2022depth} for RGB-D cameras, which minimises the Kullback-Leibler (KL) divergence between a (narrow) normal distribution around the lidar depth-measurement $\mathbf{D}$ and the rendered ray distribution $h(t)$ from the NeRF model:
\begin{equation}
\mathcal{L}_{\text {Depth-KL}}=\sum_{\mathbf{r} \in \mathcal{D}_d} \mathrm{KL}[\mathcal{N}(\mathbf{D}, \hat{\sigma}) \| h(t)]
\label{eq:depth}
\end{equation}
During training, we adopt a coarse-to-fine approach by gradually reducing the covariance of the target Normal distribution. Then, the effect of this loss function is that the surface is encouraged to be close to a delta function. 
% \mfallon{nice - that is like simulated annealing}

% \mfallon{to me I feel its unclear what the lidar is - density and coverage. For example, you could have used the full point cloud map - not just a scan. At the least you could refer to Fig 3 to give the reader an idea.}
% \todo{add implementation detail. as described in \secref{sec:implementation}}

% \subsubsection{Free Space Regulation}
% We force the space before the surface to be empty, similar to \cite{rematas2022urban}.

% \begin{equation}
% \mathcal{L}_{\text {Depth-free}}=\sum_{i=1}^{N_{surface}} w_i
% \end{equation}

% Where $N_{surface}$ is the index of the sample that is close to the surface. 



% \subsubsection{Entropy Loss}
% We also experiment with directly minimising the entropy to enforce the weight distribution to be narrow, thus forming a sharp surface. 
% \begin{equation}
% \mathcal{L}_{\text {entropy}} = -\sum_{i=1}^{N} w_i \log(w_i)
% \end{equation}


\subsubsection{Surface Normal Constraints from Lidar Measurements}
\label{sec:method_normal_constraints}
While the depth loss improves 3D reconstruction, we found that the surface it produces will still contain wavy artefacts in regions where it is expected to be smooth (see \figref{fig:surface_normal}). To mitigate this, we regularise the surface normal of the NeRF with lidar data. For each point $\mathbf{p}$ in the radiance field $R$, its surface normal can be computed as the negative gradient of the volume density $\sigma$. To obtain the training labels for the surface normal, we use the lidar range image and estimate the surface normals by local plane fitting. The surface normals are projected onto the image plane to generate the surface normal images, denoted as $\mathcal{D}_n$ similar to the lidar depth images. Then, we introduce an additional surface normal regularisation loss in the NeRF training, inspired by~\cite{Yu2022MonoSDF}:
\begin{equation}
	 \label{eq:normal}
    \mathcal{L}_{\text {Normal }}=\sum_{\mathbf{r} \in \mathcal{D}_n}\|\hat{N}(\mathbf{r})-\bar{N}(\mathbf{r})\|_1+\left\|1-\hat{N}(\mathbf{r})^{\top} \bar{N}(\mathbf{r})\right\|_1
\end{equation}
Compared to learning-based surface normal estimation from the camera image used in \cite{Yu2022MonoSDF}, our surface normal is estimated from the 3D lidar point cloud and does not suffer generalisation issues. The effect of the surface normal regularisation can be seen in \figref{fig:surface_normal}.


\begin{figure}[h]
	\centering
 
	\includegraphics[width=\columnwidth]{figures/normal_compare.pdf}
	\caption{Comparison of surface normal renderings of the Maths Institute. Incorporating normal constraints in addition to depth from lidar improves the smoothness of the reconstruction. Right: The smooth reconstruction of the ground portion highlights this improvement.}    
	\label{fig:surface_normal}
\end{figure}
% \mfallon{fig 6 should go earlier. It could appear in the middle of Sect IV}

\subsection{Sky Segmentation}
Since we focus on large-scale reconstructions of outdoor spaces, the sky and clouds are often present in our training image data. Vision-only NeRF reconstruction typically tries to model it as unconstrained floating white or blue points, which become artefacts in the final reconstruction. To remove these ``sky points" from the training procedure, we used a semantic segmentation network~\footnote{We used Detectron2 from \url{https://github.com/facebookresearch/detectron2}} to obtain a sky mask that is used to regularise the corresponding camera rays to be empty. Specifically, for the rays $\mathbf{r}$ that correspond to the sky mask (denoted as $\mathcal{D}_s$), we minimise the weights of all samples on these rays, similar to~\cite{rematas2022urban}:
% \mfallon{no continuity about rays. need to say `we use this mask to enforce that the rays are allocated zero density`}

\begin{equation}
\mathcal{L}_{\text {Sky}}=\sum_{\mathbf{r} \in \mathcal{D}_s}\sum_{i} w_i^2
\label{eq:sky_loss}
\end{equation}

%%%%%%%%%%%%%%%%%%% Uncertainty %%%%%%%%%%%%%%%%%%%

\subsection{Epistemic Uncertainty of the NeRF reconstruction}
\label{sec:uncertainty}
% source of uncertainty
% dynamic scene, insufficient overlap, insufficient observation angles (homogeneous areas)
% perturbation field concers homogeneous areas. Also considers no observation areas
% aleatoric considers dynamic scenes and appearance changes.
We aim to obtain a explicit metric of uncertainty of our NeRF reconstruction. Particularly, following \secref{sec:bayesian_nerf}, we aim to quantify the epistemic uncertainty that is directly related to the covariance of the posterior distribution $p(\theta|\mathcal{D})$ using the Laplace approximation. We first describe the reformulation of the parameters $\theta$, and then we derive the epistemic uncertainty estimate using the approximation.

\subsubsection{Perturbation Field Reformulation}
\label{sec:perturbation}
While a NeRF representation presents convenient advantages for scene compression, its parameters $\theta$ do not directly correspond to the 3D scenes. A change of one parameter in the MLP could change the whole radiance field, and it is difficult to obtain uncertainty for a specific 3D location. This is in contrast to other approaches such as 3D Gaussian Splatting~\cite{kerbl3Dgaussians}, where the parameters have a direct representation in the world. This introduces additional challenges when estimating the uncertainty of the parameters.
 
 To obtain the spatial uncertainty of a NeRF, we adopt the perturbation field formulation introduced in BayesRays~\cite{goli2023bayesrays}. Specifically, we construct the perturbation field $\mathcal{P}$ of every 3D coordinate $\mathbf{x}$. We define the perturbation field as $\mathcal{P}_{\theta_N}(\mathbf{x}):\mathbb{R}^3 \rightarrow \mathbb{R}^3$, where $\theta_P$ denotes the parameters in the perturbation field. A 3D coordinate's perturbation can be obtained using trilinear interpolation:
\begin{equation}
    \mathcal{P}_{\theta_N}(\mathbf{x}) = \text{Trilinear}(\mathbf{x}, \theta_N)
\end{equation}

The introduction of the perturbation field enables us to obtain uncertainties of a specific location in the 3D space, which is crucial for our application. From the Bayesian formulation of our problem, this means that the parameter $\theta$ in the posterior $p(\theta|\mathcal{D})$ (whose covariance we estimate as our model uncertainty) is not the NeRF parameters $\theta_N$ (MLP weights and the hash grids), but the perturbation field $\theta_P$ (perturbation value stored in the grid vertices.
% \matias{this is incomplete}
% After training the NeRF, we change the parameterisation of the rendering function to \todo{TODO, also change in previous section}

\subsubsection{Epistemic Uncertainty Estimation using Laplace Approximation}
\label{sec:la}
We estimate the epistemic uncertainty of the NeRF reconstruction by estimating the covariance of the posterior $p(\theta|\mathcal{D})$. Using Laplace approximation, we estimate the otherwise intractable posterior distribution as a Gaussian function, and then use its covariance as the estimated uncertainty of our reconstruction. Our derivation follows the presentation by Daxberger et al.~\cite{daxberger2021laplace}.
% todo: We use the Laplace approximation (LA)~\cite{daxberger2021laplace} to \todo{represent the epistemic uncertainty}. 


First, we take a second-order Taylor Series expansion of the loss function at the MAP estimate $\hat{\theta}$ as follows:

\begin{equation}
\mathcal{L}(\mathcal{D}; \theta) \approx \mathcal{L}(\mathcal{D}; \hat{\theta}) 
+ \frac{1}{2} (\theta - \hat{\theta})^\top 
\mathbf{H}
% \left( \nabla^2_\theta \mathcal{L}(\mathcal{D}; \theta) \big|_{\hat{\theta}}} \right)
(\theta - \hat{\theta}),
\label{eq:2ndtaylor}
\end{equation}

where $\mathbf{H}=\nabla_\theta^2 \mathcal{L}(\mathcal{D};\theta) |_{\hat{\theta}}$ is the Hessian matrix at the MAP estimate $\hat{\theta}$. Here, the first-order term $\nabla_\theta \mathcal{L}(\mathcal{D};\theta)|_{\hat{\theta}}^\top(\theta-\hat{\theta})$ does not appear as it is zero at the MAP estimate. 

Substituting the approximation in \eqref{eq:2ndtaylor} into \eqref{eq:normalise_constant}, we obtain:
%
\begin{equation}
\begin{aligned}
Z &= \int p(\mathcal{D} \mid \theta) p(\theta) \, d\theta \\
&= \int \exp(-\mathcal{L}(\mathcal{D}; \theta)) d\theta \\
&\approx \exp (-\mathcal{L}(\mathcal{D}; \hat{\theta})) \int \exp \left(-\frac{1}{2}(\theta - \hat{\theta})^\top \mathbf{H}(\theta - \hat{\theta})\right)d\theta \\
&= \exp (-\mathcal{L}(\mathcal{D}; \hat{\theta})) \frac{(2\pi)^{\frac{D}{2}}}{(\det \mathbf{H})^{\frac{1}{2}}}
\end{aligned}
\label{eq:normalise_constant_approx}
\end{equation}

where $D$ denotes the dimensionality of the parameters $\theta$.

Then, we substitute the Taylor expansion~\eqref{eq:2ndtaylor} and expression of the normalization constant~\eqref{eq:normalise_constant_approx} into the posterior \eqref{eq:posterior_and_loss}:
%
\begin{equation}
    p(\theta | \mathcal{D}) \approx \frac{(\det \mathbf{H})^{\frac{1}{2}}}{(2\pi)^{\frac{D}{2}}} \exp \left(-\frac{1}{2}(\theta - \hat{\theta})^\top \mathbf{H}(\theta - \hat{\theta})\right)
\end{equation}
%
which turns out to be a Gaussian distribution  $\mathcal{N}(\theta; \hat{\theta}, \mathbf{\Sigma})$ with mean $\hat{\theta}$ and covariance $\mathbf{\Sigma}=\mathbf{H}^{-1}$. 

By now, we have shown that the posterior can be approximated as a Gaussian distribution. The mean of the Gaussian $\hat{\theta}$ is the MAP estimate of the parameters $\theta$. Here, the parameters $\theta$ that we are estimating are the perturbation field $\theta_P$ introduced in \secref{sec:perturbation}. Assuming that the NeRF reconstruction has converged to local minima after training, small perturbation should not make the reconstruction, and hence the MAP estimate of the perturbation field is $\mathbf{0}$ (no perturbation). Then, the major computation needed is to determine the covariance, or the inverse of the Hessian. If we assume the prior is a zero-mean Gaussian $p(\theta)=\mathcal{N}(\theta;\mathbf{0},\gamma^2 \mathbf{I})$, then the full Hessian at the location of the MAP estimate is
\begin{equation}
\begin{aligned}
\mathbf{H} &=\nabla_\theta^2 \mathcal{L}(\mathcal{D};\theta) |_{\hat{\theta}} \\
&= -\gamma^{-2} \mathbf{I} - \sum_{n=1}^N \nabla_\theta^2 \log p(y_n | f_\theta(x_n))|_{\hat{\theta}}\\
\end{aligned}
\label{eq:hessian}
\end{equation}

While the second term in the Hessian from \eqref{eq:hessian} is usually intractable, it can be approximated by the Fisher information matrix~\cite{amari1998natural}:
%
\begin{equation}
\begin{aligned}
\mathbf{H} \approx -\gamma^{-2} \mathbf{I} - \sum_{n=1}^N \mathbf{J}\mathbf{J}^\top
\end{aligned}
\end{equation}
% where $J=\nabla_\theta \log p(y_n | f_\theta(x_n))|_{\hat{\theta}}$
%
where $\mathbf{J}=\nabla_\theta \log p(y_n | f_\theta(x_n))|_{\hat{\theta}} = -\nabla_\theta \ell(x_n, y_n; \theta)$ is the Jacobian matrix of the NeRF model.

% and generalised Gauss-Newton (GGN) matrix~\cite{schraudolph2002fast}.
% In our formulation, they are \todo{equivalent}~\cite{martens2020new}. 

Since the Fisher information matrix is still expensive to compute, we can make a further assumption that each parameter (perturbation field) is independent of each other, and use a diagonal approximation to the Hessian:
%
\begin{equation}
    \begin{aligned}
        % \nabla_\theta^2 \mathcal{L}(\mathcal{D}; \theta)|_{\hat{\theta}} 
        \mathbf{H} \approx  -\gamma^{-2} \mathbf{I} - \text{diag}(\mathbf{J}\mathbf{J}^\top)\\
        % &=diag((\nabla_\theta \mathcal{L}(\mathcal{D}; \theta))^2)
    \end{aligned}
\end{equation}

This means that for the Hessian matrix, its diagonal elements $H_{ii}$ can be computed as 
\begin{equation}
    H_{ii} \approx \sum_{j=1}^n \left(\frac{\partial \ell_j}{\partial \theta_i}\right)^2 + \gamma^{-2}
\end{equation}
% remove the H or explain how it is related to previous notions

The Hessian matrix is initialised as all zeroes, i.e. infinite covariance. As a result, the parameters that are not involved in the outputs (the rendered pixels or depth) will have very high epistemic uncertainty because changing them will not change the outputs and the training loss. Since the outputs are rendered according to the rays from the training images, the unobserved regions can be identified as having very high uncertainty and can be filtered out. Identifying unobserved areas (similar to the unknown space in occupancy mapping) is particularly important for NeRF, as the underlying MLP can output arbitrary colour and density in some locations, leading to ``hallucinated'' reconstruction points.

\subsubsection{Visual and Depth Uncertainty}
The NeRF model is trained with a total training loss function $\mathcal{L}(\mathcal{D;\hat{\theta}})$ that contains the photometric loss $\mathcal{L}_{Colour}$ and the depth loss $\mathcal{L}_{Depth}$. This means that the Jacobian $\mathbf{J}$ can be decoupled into a colour component $\mathbf{J}_{Colour}$ and depth component $\mathbf{J}_{Depth}$, from which we can then approximate the Hessian that corresponds only to the visual information  $\mathbf{H}_{Colour}$, and the Hessian that corresponds to only to the lidar depth information $\mathbf{H}_{Depth}$.
% \sum_{n=1}^N \ell(x_n, y_n; \theta) + r(\theta)
% This means that by changing the NeRF output $f_\theta(x_n))|_{\hat{\theta}}$,
Therefore, we can compute the epistemic uncertainty for \textit{each observation modality} by changing the loss function. Note that the total training loss function contains other terms including the surface normal loss $\mathcal{L}_{Normal}$. In our study, we focus on just the photometric loss $\mathcal{L}_{Colour}$ and depth loss $\mathcal{L}_{Depth}$. This is because these two losses are the dominant components of the total loss $\mathcal{L}(\mathcal{D;\hat{\theta}})$ (with our weighting coefficients for each loss chosen).
% Specifically, we compute visual uncertainty by using colour rendering, and depth uncertainty by using depth rendering.

The nature of the different sensor modalities leads to different uncertainty characteristics. The visual uncertainty captures areas that can be geometrically perturbed while not changing the colour. As later shown in \figref{fig:unc-ablation-plots}, ``low'' visual uncertainty corresponds to distinct visual features and high-frequency areas. ``Higher'' visual uncertainty typically corresponds to areas with uniform texture, since perturbing a point in an area with similar colours can lead to small changes in the rendered colour. Here, the characteristics of the visual uncertainties are similar to those in SfM where visual features are used as landmarks for Bundle Adjustment whereas uniform texture areas are often not mapped.

In comparison, low lidar depth uncertainty often appears in regions with abundant lidar observation---whether there are visual features or not. This means a road surface with uniform texture can have \textit{lower} lidar depth uncertainty but \textit{higher} visual uncertainty. Low lidar depth uncertainty is also observed at
object boundaries, since perturbing that point can lead to a drastic change in the depth (from foreground depth to background depth). Regions with high lidar depth uncertainty are often areas where there are fewer lidar observations, such as the sky, distant regions that are beyond the lidar's sensing range, and regions outside the lidar's Field-of-View.

The decoupling of the uncertainties enables a principled interpretation of the lidar-visual reconstruction. We can identify parts of the reconstructions that are reliable (surface with visual features, and/or abundant lidar observations) and unreliable (no lidar observation and uniform-textured surfaces), given each sensor modality's own characteristics.
% \mfallon{I think the above is unclear despite being a simple message}.
% Also I don't know why you use the word deform. I also don't think you can deform a point. I think perturbing is a better word.}

% \mfallon{again I think perturbed is the right word: "subject (a system, moving object, or process) to an influence tending to alter its normal or regular state or path."}

% Low lidar depth uncertainty means that if the region is perturbed, then the depth would change . From \figref{fig:unc-ablation-plots}, we found that this often corresponds to thin objects, which also typically have \mfallon{I DONT THINK THIS IS FORMAL ENOUGH. VISUAL UNCERTAINTY ISNT WELL DEFINED HERE low visual uncertainty}. \mfallon{ALSO TOO INFORMAL:} Due to the characteristics of lidar, thin objects are actually more unreliable. High lidar depth uncertainty is \mfallon{I DONT KNOW WHAT YOU MEAN BY MUCH MORE USEFUL} much more useful: it corresponds to areas where few or no lidar measurements were made.

% \mfallon{on the whole, the previous section is informal and unclear}
% The disentangled estimation of uncertainty from two sensor modalities allows us to 

%notation comes from Ritter and Laplace Redux


% \todo{Fisher information to depth. Fisher information is additive. the joint depth and rgb function}

%%%%%%%%%%%%%%%% Pose Compputation %%%%%%%%%%%%%%%%
\subsection{Large-scale Pose Trajectory Estimation}

%%%%%%%%%%%%%%%%%%%% Pose Prior %%%%%%%%%%%%%%%%%%%%

\subsubsection{Refining Lidar SLAM trajectory with Bundle Adjustment}
\label{sec:pose_refinement}
Providing the NeRF reconstruction method with accurate camera poses is crucial as their accuracy directly impacts the fidelity of the reconstructed model. A popular approach used in most NeRF works is to estimate camera poses using (offline) Structure-from-Motion methods such as COLMAP~\cite{schoenberger2016colmap}. However, we observed the following limitations when testing COLMAP: (1) long computation times,  especially for large image collections collected spanning a long trajectory (e.g., 3000 images can take more than 1 hour (as shown in \tabref{tab:pose_ablation}), and (2) inability to register all frames into one global map when there is limited visual overlap between the images. These issues undermine the goal of building complete, large-scale, globally consistent maps.
% \mfallon{this is vague: large and long. could you replace it with something specific: `a image collection of 1000 images spaced every metre takes Xmins to so register'}

In our work, we use our lidar-inertial odometry and SLAM system VILENS~\cite{wisth2023vilens}. While VILENS achieves state-of-the-art results for lidar-based online motion tracking, we found that the camera poses obtained are less precise than those of COLMAP. This results in \textit{blurring} artefacts in the images rendered by the NeRF model. Several works~\cite{tancik2022block,azinovic2022neural} use noisy pose inputs and then jointly refine the poses within the NeRF optimization to generate better results. However, as shown later in \tabref{tab:pose_ablation}, our experiments showed that this pose-refinement approach can produce results which are inferior to using poses estimated by COLMAP.

% , the resulting rendering is still less crisp than those created using poses estimated from COLMAP, and the training process is often unstable.
% \mfallon{This should be restated. It's not necessary to support this comment but at the least you should make it less hand-wavy.}
% \todo{no evidence to support this}

To overcome these limitations, we propose to use the pose trajectory from VILENS SLAM as a prior and refine it by running Bundle Adjustment using COLMAP~\cite{schoenberger2016colmap}. Specifically, we first run the feature extraction and matching on the dataset, and then use the Lidar SLAM poses to triangulate visual feature points, and run a few iterations of BA.
% replace the COLMAP \textit{mapper} with \textit{point triangulator} which reads a set of prior poses. 
This method is faster than a typical incremental SfM, as it reduces the incremental Structure-from-Motion to a Bundle Adjustment problem. More importantly, having an accurate prior for all the image poses means that COLMAP will be able to produce a full solution and does not fail to register some of the images---as would be the case for SfM without initialisation. 
For a mission spanning over 20 minutes, our COLMAP-with-prior pipeline achieves similar rendering quality, while typically taking half the computation time of a standard COLMAP run. The computation time is similar to the time required by a robot to collect the data, making it more suitable for robotic applications. 

After COLMAP computation, we rescale the trajectory using $\mathit{Sim}(3)$ Umeyama alignment to the Lidar-SLAM trajectory, so that the final trajectory is metrically scaled. This step is crucial because the lidar measurements used in \secref{sec:method_depth_constraints} are also metric. The depth regularisation cannot be used if the scale of the scene and the scale of the depth are not consistent. 

% \todo{Multi-session SLAM?}
%%%%%%%%%%%%%%%%%%%% Submapping %%%%%%%%%%%%%%%%%%%%
\subsubsection{Submapping of Pose Trajectory}
To divide the whole map into smaller manageable areas, we partition the entire trajectory into shorter trajectories, which we define as submaps.
% \mfallon{Need to rephrase as `partitioning of the pose trajectory ... into submaps` - not `submapping the map`.}
The submaps are clustered considering image visibility rather than using space partitioning or distance-based clustering~\cite{tao2024silvr}. The goal is to exclude an image from a submap if it does not contribute to the submap reconstruction---for example, if the scene observed is not visible from the other images in the submap. 

We formulate the clustering problem as a graph partitioning problem, where each node is an image, and the edges between the nodes are weighted by a similarity score. We measure the similarity between two images using a co-visibility metric. Two images are co-visible if some feature points on one image can be viewed from the other image. Specifically, the co-visibility metric for an image pair is computed as the number of visual feature points computed by COLMAP that appear in both images.
% \todo{maybe note that this is quite fast since don't need to run BA for this. no need to be very precise}
After constructing the graph, we use the Normalised Cuts algorithm~\cite{shi2000normalized} to obtain a partitioning which minimally breaks edges, i.e. to remove the link between unrelated nodes. In practice, this means that images that are co-visible are grouped together, and image pairs that have less visual overlap are identified as the submap boundary. 
% \todo{maybe add some illustration in figure?}
% \mfallon{end of previous sentence is confusing. I dont know what `between two building sections means'.}

%We propose two ways to compute the similarity score. The first one uses vocabulary-tree to compute a similarity of two images. The second one considers number of lidar point cloud that is co-visible between two images. To determine the visibility of the lidar point cloud, we project the lidar map into the image, and count the number of points that is within the image dimension and with positive depth, and is not occluded. We handle occlusion with splatting.

% After running Normalised Cut, we add images from adjacent submap to each submap to make overlapping partitioning. This is crucial, since the reconstruction has less observation at the boundary, and the quality therefore drops. \todo{do we still need this? maybe argue that we don't want to just add duplicate images to the boundary, since it breaks the point of submapping..need to decide}

Once we divide the full map into submaps as a set of clustered images, we independently train each NeRF submap. After training, we compute the epistemic uncertainty of the reconstruction. We can export a point cloud by rendering colour and depth using the training data rays, and we filter points that have high uncertainty.
% \todo{add combined unc scheme}

% The second method is to use the uncertainty estimation in \secref{sec:uncertainty}. Lidar does not provide measurements of the sky, and thus the sky would typically have high lidar uncertainty. The visual uncertainty of the sky that is over-exposed is also high. Therefore, the sky is removed when we filter points with high visual uncertainty and high lidar uncertainty. 
% \todo{update this method. Doesn't work if lidar narrow fov}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Experimental Setup %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h]
	\centering
	\includegraphics[width=0.99\columnwidth]{figures/overlays.pdf}
	\caption{Sample Data from our diverse robotic datasets. Here each image is overlaid with projected lidar point cloud to demonstrate the accuracy of the sensor calibration.}    
	\label{fig:sample_overlay}
\end{figure}
% \todo remove the blenheim overlay
% \mfallon{could you add a little text overlay to SOME of these images? e.g. `backpack', `quadruped', `aerial' on the left side images only. just a suggestion - ignore if you wish}



\begin{table}[h]

    \centering
    \begin{tabular}{c c c c}
    \toprule
        Site Name & Robotic Platform & GT   \\
    \midrule
        HB Allen Centre & BD Spot & Leica BLK360  \\
        Fire Service College & DJI M600 Drone  & Leica BLK360 \\

        Radcliffe Observatory Quarter & Handheld Frontier  & Leica RTC360 \\
        % Blenheim & Handheld Frontier & Hesai QT64 & Leica RTC360 \\
        Bodleian Library & Handheld Frontier  & Leica RTC360 \\
    \bottomrule
    \end{tabular}
    \caption{Details of real-world datasets that are used for evaluation.} 
    % \mfallon{suggestion: Allen Centre, Radcliffe Quarter, Bodleian Library, Fire College}}
    \label{tab:datasets}
\end{table}
% \todo{add carla}
% todo bodleian I have both narrow fov and large fov



\section{Experimental Setup}

%%%%%%%%%%%%%%%%%%%%%% Hardware %%%%%%%%%%%%%%%%%%%%
\subsection{Hardware and Datasets}
% \todo{cite Spires}
We evaluate our methods using a custom perception unit called Frontier shown in \figref{fig:sys_overview}. It includes three 1.6 MP fisheye colour\footnote{To produce RGB images, we debayer and white-balance the raw bayered images using \url{https://github.com/leggedrobotics/raw_image_pipeline}} Alphasense cameras (from Sevensense Robotics AG) on 3 sides of the device, as well as a synchronised IMU. The 3-camera setup enables omnidirectional NeRF mapping from a single walking pass through a test site. We installed a Hesai QT64 lidar (104\degree~Field-of-View, 60 metres maximum range) on top of the device. We deployed the Frontier in different modes---onboard a legged robot (Boston Dynamics Spot), a drone (DJI M600, with the system described in \cite{border2024osprey}), or simply handheld (using the Oxford Spires dataset~\cite{tao2024spires}. Some data is collected with the Frontier mounted on a human operator's backpack).
%  and another with Hesai XT32 ((31\degree~Field-of-View, 120m range)
% \mfallon{say that the frontier's 3 cameras enables }

In addition, we used a survey-grade terrestrial lidar
scanner (TLS) to obtain a millimetre-accurate point cloud which we later used to create a reference ground truth model. We use either the entry-level Leica BLK360 or the professional-grade Leica RTC360 to obtain ground truth maps of the sites.

We tested our method using data collected in the following sites: H B Allen Centre (HBAC), Fire Service College (FSC), Radcliffe Observatory Quarter (ROQ), and the Bodleian Library, all in Oxford, UK. The large-scale sites, ROQ and the Bodleian Library, cover areas of \( \text{5,000}~\text{m}^2 \) and \( \text{15,000}~\text{m}^2 \), respectively. The hardware details are in \tabref{tab:datasets}, and some sample lidar-camera overlays are shown in \figref{fig:sample_overlay}.

%TODO describe RTC and GT trajectory

\begin{figure*}[t]
	\centering
	\includegraphics[width=2\columnwidth]{figures/small_scale_recon.pdf}
	\caption{Comparison of reconstruction quality of VILENS-SLAM, Nerfacto (vision-only) and our approach in small-scale scenes. Reconstructions are coloured using the point-to-point distance between the respective reconstructions and the ground truth scan with increasing error from blue (0m) to red (1m). The trajectory is shown in purple and overlaid on the ground truth scan captured using a Leica BLK360. The zoomed-in views show the difference in reconstruction quality. Overall, our approach is more complete w.r.t lidar-only reconstruction, and geometrically more consistent w.r.t vision-only reconstruction.}
	\label{fig:reconstruction_eval_small_scale}
\end{figure*}

\begin{figure}[h]
	\centering

	\includegraphics[width=1\columnwidth]{figures/occupancy_analysis.pdf}
	\caption{Classification of different occupancy categories for the reconstruction and reference models.}    
	\label{fig:occ_analysis}
\end{figure}



\begin{table}[h]
	\caption{\small{ Evaluation of 3D Reconstruction Quality of Small Scenes}}
   	\setlength{\tabcolsep}{3pt} % Reduce column padding
	\centering
	\begin{tabular}{ l c c c c c}
		\toprule
		Method& Accuracy$\downarrow$ & Completeness$\downarrow$ &  \multicolumn{2}{c}{PSNR$\uparrow$} & SSIM$\uparrow$ 
		\\
            &(m)&(m)&train&test&test
            \\
            \hline
		\addlinespace
		\textbf{Oxford HBAC}
		\\
		\hline
		\addlinespace
		VILENS-SLAM &\textbf{0.05}& 0.25& /&/&/
		\\
		Nerfacto mono&0.49&5.40 &\textbf{32.6}&19.5&0.65
            \\
            Nerfacto 3-cam&0.28&0.40&29.8&20.6&\textbf{0.74}
		\\
		Ours mono& 0.30&4.60&31.0&\textbf{21.2}&\textbf{0.74}
            \\
            Ours 3-cam &0.09&\textbf{0.18}&28.8&19.7&\textbf{0.74}
		\\
  		\hline
		\addlinespace
            \textbf{FSC} 
		\\
		\hline
		\addlinespace
		VILENS-SLAM		&\textbf{0.08} &\textbf{0.08}&/&/&/
            \\
		Nerfacto mono &0.14&0.11 &\textbf{28.8}&\textbf{19.1}&\textbf{0.76}
		\\
		Ours mono&0.11 &0.09&27.7&\textbf{19.1}&0.75
		\\
		\bottomrule
		\addlinespace
	\end{tabular}
	\label{tab:reconstruction_eval_small_scale}
\end{table}

% \mfallon{you could do with including the scale of these datasets e.g. path length or approximate area in $m x m$ or $m^2$.}

%%%%%%%%%%%%%%%%%% Implementation %%%%%%%%%%%%%%%%%%
\subsection{Implementation Details}
\label{sec:implementation}
When collecting the data, we use VILENS~\cite{wisth2023vilens}, a lidar-inertial SLAM system running online to estimate a globally consistent trajectory and to motion correct the lidar measurements. The SLAM trajectory estimated online can also be further optimised using Bundle Adjustment as described in \secref{sec:pose_refinement}. This improves the visual reconstruction quality, as shown later in \tabref{tab:pose_ablation}. Individual lidar scans are projected to form a sparse depth image coinciding with the camera image (i.e. the same camera pose and intrinsic parameters). Lidar point normals are also estimated using the lidar range image, and projected as a sparse normal image. We use the calibrations provided by the Oxford Spires dataset~\cite{tao2024spires}. In this dataset, the intrinsics and extrinsics of the set of cameras are estimated using Kalibr~\cite{furgale2013unified}, and a single extrinsic transformation between the three cameras and the lidar is estimated using DiffCal~\cite{fu2023extrinsics}. When running COLMAP, we further optimise the camera intrinsics produced by Kalibr. To train the NeRF model, we used an Nvidia RTX 3080 Ti. One training iteration takes 4096 rays. 
% \todo{update the hardware}
%\todo{more details on training parameters}

\begin{figure*}[t]
	\centering
	\includegraphics[width=2.0\columnwidth]{figures/large_scale_recon.pdf}
	\caption{Comparison of reconstruction quality of our method and other baseline methods in two real-world large-scale scenes. Among the radiance field baseline methods, SiLVR's reconstruction is the most accurate and complete, especially on the ground where there is insufficient visual constraints.}
	\label{fig:reconstruction_eval_large_scale}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%% Metrics %%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Details}
\subsubsection{3D Reconstruction Metrics}
To evaluate the geometry of the reconstruction, we report \textit{Accuracy} and \textit{Completeness} following the conventions of the DTU dataset~\cite{aanaes2016large_dtu}. Accuracy is measured as the point-to-point distance from the reconstruction to the (ground truth)  reference 3D model and indicates the reconstruction quality. Completeness is the distance from the point-wise reference to the reconstruction and shows how much of the surface has been captured by the reconstruction. 

In addition, we also compute \textit{Precision} and \textit{Recall} with a pre-defined error threshold. A point in the reconstruction which is below this threshold can be considered to be a true positive. We use both 5cm and 10cm for the threshold following \cite{tao2024spires}.

% We use point-to-plane distance instead of point-to-point distance, since the latter is affected by the ground truth voxel resolution.\todo{did we? we can use p2p and mention precision is not affected}
% \todo{revise with spires description}

%TODO: refine this part




\subsubsection{Map Filtering for Fair Evaluation}
\label{sec:map_filtering}
Perfect accuracy and completeness scores (of zero in both cases) would be achieved if the two point clouds are identical, and any deviation is penalised by a higher value. In practice, the ground truth model and the reconstruction do not perfectly overlap, as they scan slightly different parts of the scene from different viewpoints. Two situations can occur which do not correspond to mapping error:
\begin{enumerate}
    \item Missing regions in the ground truth map: the ground truth map can have undetected areas of the scene that were captured in the Frontier data sequence. This would lead to an artificially higher accuracy score for the Frontier data in such regions. These are in effect \textit{false positives}.
    \item Extra regions in the ground truth map: the ground truth map can contain areas that the Frontier device did not scan. In this case, the NeRF reconstruction of these extra regions will be missing. These are undesirable \textit{false negatives}, and the completeness score would again be artificially higher than it should be.
\end{enumerate}




These overestimated error measures are typically much higher than the errors which occur in well-defined regions (both for the TLS ground truth and the Frontier data), and can then skew the results metrics. This makes comparison between different reconstruction methods difficult.


To address this issue, we filter the non-overlapping regions that we consider should not be included in the evaluation --- for both the reconstruction and the ground truth. Specifically, our evaluation system first builds an occupancy map of the ground truth reconstruction using Octomap~\cite{hornung13octomap}. We then remove points in the reconstruction that are not in the octree (i.e. in the \textit{unknown} space). Similarly, we build an occupancy map of the lidar point clouds, and remove ground truth points within the unknown space. Manual filtering is also applied for regions that are inside the buildings.
% This can remove reconstructions that would otherwise contain incorrect false positives.  
% Specifically, our evaluation system identifies these regions by computing the accuracy and completeness, and select high-error clouds as the candidate cloud for exclusion. Then, we manually verify the regions to be removed, and then generate voxels from these points. Finally, we crop both the reconstruction and the ground truth using these exclusion voxels. 
% \todo{ add a figure here.}
% todo: error filtering
% why not filter with the lidar clouds? I need to try this though..
% \mfallon{On the whole, I think this section needs to be edit to reduce repetition}


\subsubsection{Rendering Metrics}
We evaluate the visual quality of the reconstructions by reporting the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM)~\cite{wang2004image}, which are standard metrics in the radiance field literature. Note that our images have variable exposure times which lowers the test PSNR even if the reconstructed images have a very high degree of photorealism. 
% \todo{appearance encoding optimisation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% Experimental Results %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{table*}[t]
\caption{ Evaluation of 3D reconstruction quality. classical methods (Lidar SLAM, Poisson Reconstruction and MVS) and radiance file methods are grouped separately. The best results in each group are indicated in bold.}
\setlength{\tabcolsep}{4pt} % Reduce column padding
\centering

\begin{tabular}{l c c c c c c c c c c c c}
\toprule
Method & Accuracy$\downarrow$ & Completeness$\downarrow$ & \multicolumn{3}{c}{5cm} & \multicolumn{3}{c}{10cm} & \multicolumn{2}{c}{PSNR$\uparrow$} & SSIM$\uparrow$ & LPIPS$\downarrow$\\
& (m) & (m) & Precision & Recall & F-Score & Precision & Recall & F-Score & train & test & test\\
\midrule
\multicolumn{13}{l}{Radcliffe Observatory Quarter (ROQ)}\\
\midrule
VILENS-SLAM &\textbf{0.077} & \textbf{1.214} & \textbf{0.552} & \textbf{0.367} & \textbf{0.441} & \textbf{0.832} & \textbf{0.625} & \textbf{0.714} & / & / & /& /\\
Poisson Reconstruction & 0.146 & 1.768 & 0.406 & 0.274 & 0.327 & 0.658 & 0.558 & 0.604 & / & / & /& /\\
OpenMVS & 0.123 & 1.570 & 0.460 & 0.353 & 0.399 & 0.688 & 0.495 & 0.576 & / & / & /& /\\
\addlinespace
Nerfacto 3-cam & 0.916 & 2.272 & 0.220 & 0.072 & 0.109 & 0.392 & 0.189 & 0.256 &\textbf{25.71}&	\textbf{20.92}&	\textbf{0.714}	&\textbf{0.490}\\
Splatfacto 3-cam & 0.478 & 2.415 & 0.240 & 0.044 & 0.074 & 0.395 & 0.151 & 0.218 & 21.96&	19.95&	0.712	&0.514 \\
NeuSfacto 3-cam & 0.699 & 2.763 & 0.051 & 0.021 & 0.030 & 0.115 & 0.098 & 0.106 & 21.17&	16.97	&0.521&	0.549 \\
\textbf{SiLVR (Ours)} & \textbf{0.095} & \textbf{1.803} & \textbf{0.416} & \textbf{0.150} & \textbf{0.221} & \textbf{0.699} & \textbf{0.344} & \textbf{0.461} &24.73	&20.90	&0.653	&0.551\\
\midrule

\multicolumn{13}{l}{Bodleian Library}\\
\midrule
VILENS-SLAM & 1.017 & \textbf{0.736} & \textbf{0.324} & 0.098 & 0.150 & \textbf{0.518} & 0.290 & \textbf{0.372} & / & / & /& /\\
Poisson Reconstruction & 1.256 & 1.230 & 0.239 & 0.104 & 0.145 & 0.400 & \textbf{0.334} & 0.364 & / & / & /& /\\
OpenMVS & \textbf{0.955} & 2.257 & 0.223 & \textbf{0.129} & \textbf{0.163} & 0.429 & 0.280 & 0.339 & / & / & /& /\\
\addlinespace
Nerfacto 3-cam & 2.841 & 1.124 & 0.092 & 0.030 & 0.045 & 0.190 & 0.132 & 0.156 & \textbf{28.92}	&\textbf{23.03}&	0.827	&\textbf{0.715} \\
Splatfacto 3-cam & 13.532 & 1.275 & 0.020 & 0.004 & 0.007 & 0.044 & 0.027 & 0.033 & 23.92&	22.19&	\textbf{0.850}	&0.748\\
NeuSfacto 3-cam & 2.656 & \textbf{1.074} & 0.015 & 0.007 & 0.010 & 0.035 & 0.042 & 0.038 &24.00&	20.61&	0.619	&0.731\\
\textbf{SiLVR (Ours)} & \textbf{1.292} & 1.532 & \textbf{0.129} & \textbf{0.041} & \textbf{0.063} & \textbf{0.276} & \textbf{0.170} & \textbf{0.211} & 28.00&22.94	&0.754	&0.779\\
\bottomrule
\addlinespace
\end{tabular}
\label{tab:reconstruction_eval_large_scale}
\end{table*}


% \todo{add carla results}


\section{Experimental Results}


%%%%%%%%%%%%%%%%%%%%% 3D Recon %%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation of the 3D Reconstruction}

We perform a quantitative evaluation of our method using real-world datasets captured by different robotic platforms. Of the evaluation datasets used, HBAC and FSC are small-scale scenes (just one building or a single enclosed space), while ROQ and Bodleian Library are large-scale scenes (connected building complexes). We compare the output point cloud reconstructions from the following algorithms:
\begin{enumerate}
    \item VILENS-SLAM: lidar point clouds are registered with poses computed by an odometry system VILENS~\cite{wisth2023vilens} and pose graph optimisation~\cite{ramezani2020slam}
    \item Poisson Reconstruction~\cite{kazhdan2006poisson}: surface reconstruction using point clouds from VILENS-SLAM %todo: add resolution
    \item OpenMVS~\footnote{Available at \url{https://github.com/cdcseacave/openMVS}}: multi-view stereo reconstruction  % todo: add specification
    \item Nerfacto~\cite{nerfstudio}: vision-only radiance field reconstruction using volume density
    \item NeuSfacto~\cite{wang2021neus}: vision-only radiance field reconstruction using SDF
    \item Splatafacto~\cite{ye2023splatfacto}: vision-only radiance field reconstruction using 3D Gaussians which are initialised using SfM visual features from COLMAP
    \item SiLVR: Our proposed method using photometric loss, depth loss, and surface normal loss
\end{enumerate}

Note that all the methods (except VILENS-SLAM and Poisson reconstruction) use the same set of poses and input images. For the large-scale datasets ROQ and Bodleian Library, we use the same submap partitioning for all the radiance field approaches (Nerfacto, NeuSfacto, Splatfacto, and SiLVR).

% compare each method
We summarise the quantitative results in \tabref{tab:reconstruction_eval_small_scale} and \tabref{tab:reconstruction_eval_large_scale}, and show the 3D reconstructions in \figref{fig:reconstruction_eval_small_scale} and \figref{fig:reconstruction_eval_large_scale}. Among all methods, lidar-only method VILENS-SLAM is the most accurate and complete. 
% This is expected as the lidar used here has wide field-of-view and the error of the depth measurement is in the order of centimetres. \mfallon{I'm confused about the previous sentence}
OpenMVS produces much more accurate and complete reconstructions compared to the radiance field reconstruction, but produces a poor reconstruction of the ground compared to lidar-based methods. This is expected as there is little texture on the ground.
All the radiance field reconstructions are less accurate and less complete compared to VILENS-SLAM and OpenMVS. Nerfacto fails to estimate most of the ground geometry accurately in ROQ (the reconstruction is below the ground and is filtered by the occupancy map described in \secref{sec:map_filtering}). Compared to Nerfacto, NeuSfacto reconstructs the ground surface better, but still at an incorrect height compared to the ground truth. This shows that while the SDF formulation poses a geometric prior on the scene (enforcing that there should be a surface rather than an arbitrary volumetric field), it still cannot estimate the surface accurately if there are insufficient visual constraints. The 3D Gaussians exported by Splatfacto also cannot reconstruct the ground accurately. These 3D Gaussians are located mostly on the visual features of the sites---since they are initialised by the COLMAP feature points. 
% We observed that both Nerfacto and Splatfacto produce reconstructions beneath the ground surface, and are removed by the occupancy-based filtering described in \secref{sec:map_filtering}.
% \mfallon{REVISE THIS: which are rendered to be the ground}.

Compared to the vision-only methods, SiLVR incorporates lidar measurements and has significantly better reconstruction fidelity especially on the ground. Compared to VILENS-SLAM, SiLVR achieves more complete reconstruction for the shorter sequences (e.g., HBAC in \figref{fig:reconstruction_eval_small_scale}) since it uses dense visual information. When there are many accumulated lidar points (which is the case for the large-scale datasets in \figref{fig:reconstruction_eval_large_scale}), this advantage is less prominent.

Regarding the rendering quality, Nerfacto achieves the best results among the radiance field reconstructions. We found that NeuSfacto's training takes longer than all the other methods, and the rendering quality is worse than the other methods. SiLVR achieves a balance between the rendering quality and the 3D reconstruction quality.
% However, SiLVR can still generate better thin objects than lidar (\todo{ROQ fountain?})
% \todo{carla lidar is narrow fov, maybe different!}
% \todo{can I demonstrate?} However, for small objects, lidar reconstruction might struggle with the inherent measurement noise .
% todo: add splatfacto uses SfM features
% TODO: MVS size too big? 3dgs too big? see below

% When it comes to large-scale recon, all methods face challenges from memory and compute. MVS uses much bigger storage than NeRF and NeuS. GS also uses much memory which is hard for mobile robot. Our submapping approach allows us to only extend the training time without the increase in computation and hardware specification compared to Splatfacto. SiLVR is more compact than MVS (\todo{show this?})




%%%%%%%%%%%%%%%%%%%% Uncertainty %%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.98\columnwidth]{figures/unc_ablation.pdf}
	\caption{Qualitative and quantitative evaluation of epistemic uncertainty estimates using both synthetic (CARLA) and real-world (ROQ) datasets. The Sparsification Plot shows how the (normalised) reconstruction error decreases as more uncertain points are removed, and how close it is to the oracle error reduction curve. The Error Plot additionally shows the distribution of error at different uncertainty estimates. Low visual uncertainty corresponds to visual features which are well constrained by the image view constraints, similar to the SfM and Visual SLAM systems. Low lidar depth uncertainty indicates that there is abundant lidar observation, and hence the geometry is also constrained.}
    % \todo{fig8 bigger gap between plots and error maps}}
	\label{fig:unc-ablation-plots}
\end{figure*}



\subsection{Evaluation of Epistemic Uncertainty Estimation}
We evaluate the epistemic uncertainty estimates using both synthetic data and real-world data. The synthetic data is generated using the CARLA simulator~\cite{Dosovitskiy17carla}, which provides perfect pose trajectories and ground truth maps. We simulated a vehicle with a lidar and three-cameras in a configuration similar to the Frontier. Meanwhile, the real-world dataset used in this section is the Radcliffe Observatory Quarter (ROQ).
\subsubsection{Evaluation Metrics}
% AUSE
We evaluate the epistemic uncertainty estimates using the Sparsification Plot which has been used for evaluating confidence estimates in the literature~\cite{ilg2018uncertainty,Eldesokey2020pncnn,goli2023bayesrays}. The Sparsification Plot is used to evaluate how the uncertainty estimates coincide with the actual errors (in our case, point-to-point distance to the ground truth). In these plots, reconstruction points with the highest uncertainty are gradually removed, and the average errors of the remaining reconstruction points are calculated to form a graph. If the uncertainty estimates align perfectly with the actual errors, then the reconstruction points with the the highest errors are always removed first, which leads to the steepest decrease of the remaining error as the most uncertain points are being removed. This ideal sparsification curve is referred to as \textit{Oracle Sparsification}. In practice, the uncertainty estimates do not align with the actual errors perfectly. Specifically, when a reconstruction point has a higher uncertainty but lower error compared to another point, the uncertainty estimates are considered not perfect. The area between the sparsification and its oracle indicates how different the uncertainty estimates are from the ideal ones, and can then be used to compute the Area Under Sparsification Error (AUSE). A smaller difference between the sparsification and its oracle results in a lower AUSE, and indicates that the uncertainty estimates are better because they align better with the actual errors.

In addition to the Sparsification Plot which focuses on the error of the \textit{remaining} reconstructions, we also analyse the error of the reconstruction that is \textit{being removed} (according to the uncertainty). This is achieved by plotting the errors of the reconstruction that have different levels of uncertainties, which we refer to as the Error Plot. While the errors in the Sparsification Plots are normalised (since AUSE is scale-invariant), we use metric errors in the Error Plot to keep the scale information.    

 % Area Under Sparsification Error (AUSE)~\cite{ilg2018uncertainty}, which is computed based on the

% If one removes the reconstruction with higher uncertainty gradually, then the average error (point-to-point distance to the ground truth) should decrease. The best possible way to gradually reduce the error is achieved if the order of the uncertainty estimates is the same as the order of the error. AUSE measures the difference between the ideal error curve (where highest error is removed first) and the curve obtained by removing points with the highest uncertainty estimates. A lower AUSE represents a better uncertainty estimation. In addition to the sparsification plot which is used to compute AUSE, we also plot the average error of different uncertainty estimates. 
% \mfallon{this paragraph is badly written. please revisit}

% This enables one to evaluate the uncertainty estimates at different values, and complements AUSE which is one single metric for the whole uncertainty estimation. 

\subsubsection{Results}
In \figref{fig:unc-ablation-plots}, we evaluate the decoupled epistemic uncertainty estimates quantitatively using the Sparsification Plot and the Error Plot, and qualitatively by showing the reconstructions with low and high uncertainty estimates. As shown in the Error Plots, both visual and lidar depth uncertainty can indicate the degree of the reconstruction error. In particular, we can observe how the visual uncertainty and lidar depth uncertainty capture different parts of the scene according to the sensor characteristics.
% Interestingly, we notice that lidar depth uncertainty is a better error indicator of reconstruction error than visual uncertainty when the relative uncertainty is high. 
% For the CARLA results, we can see show visual uncertainty is a slightly better error indicator than lidar depth uncertainty when relative uncertainty is low. 
From the reconstruction error figure of both CARLA and ROQ, it can be seen that reconstructions with low visual uncertainty generally are places where visual features can be detected. In fact, this corresponds to the visual features that can be reliably estimated by classical SfM and visual SLAM methods. In ROQ, much of the ground in the quad has relatively higher visual uncertainty but lower lidar depth uncertainty. Essentially, even if there are few visual features on the ground which are not ideal for visual reconstruction, the lidar measurements provide sufficient information to produce an accurate ground reconstruction. When uncertainty estimates are high, we found that lidar depth uncertainty is a better indicator of the reconstruction error than visual uncertainty.  From the Error Plot, the average error of points with high lidar depth uncertainty estimates (blue curve) is generally higher than the error of points with high visual uncertainty estimates (red curve). This can also be shown in the Sparsification Plot: as the first 20\% of the reconstruction of higher uncertainty are being removed, the sparsification curve by depth uncertainty (blue curve) is closer to the ideal oracle sparsification (dashed blue curve) compared to the sparsification curve by visual uncertainty (red curve). This is because the depth uncertainty estimates remove points with higher errors than the visual uncertainty estimates, and hence the errors of the remaining points are lower.

% both visual and lidar depth uncertainty can capture (high-error) reconstruction artefacts, in particular distant objects that are only sparsely observed by both the camera and the lidar. From the Error Plot, we found that points with higher depth uncertainty have higher errors than points with higher visual uncertainty, which indicates that depth uncertainty estimates are better at capturing distant artefacts than visual uncertainty estimates. 

% From the Sparsification Plot, it can be seen that the sparsification cur leads to a more steep decrease   
% \todo{show a COLMAP visual feature figure and how it overlaps with CARLA}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{figures/visual_lidar_unc_3d.pdf}
	\caption{Comparison between visual and lidar uncertainty. The reconstructions at the top are coloured by uncertainty estimates (red is high uncertainty, green is low uncertainty). Here, the lidar used was a Hesai XT32 which has a narrow field-of-view and cannot scan the upper part of the buildings, and the depth uncertainty estimates can identify such regions (red point clouds). This indicates that only visual information is used when reconstructing these areas.}
	\label{fig:lidar_visual_unc}
\end{figure}


% \mfallon{Move as many figures earlier as you can}

The advantage of the disentangled uncertainty estimates is also demonstrated in \figref{fig:lidar_visual_unc} where we use a narrow vertical field-of-view lidar with wider vertical field-of-view cameras. Here, the lidar is not able to scan the upper part of the two buildings highlighted in \ref{fig:lidar_visual_unc}, but the cameras can. Because of this, when we compute the epistemic uncertainty, we can see high lidar depth uncertainty for the upper part of both buildings. This indicates that the reconstruction is derived primarily from the visual data. In summary, from a sensor fusion point of view, our epistemic uncertainty estimation framework provides a systematic analysis of each sensor's contribution to the final reconstruction.

% /todo uncertainty for submaps
% The uncertainty quantification is particularly useful for merging multiple submaps (\todo{is this true..?}. From \todo{Some Figure}, we can see that there are duplicate reconstructions from two adjacent submaps. With the uncertainty estimate, we can identify the less confident reconstruction with less visual constraints and less lidar exposure. Filtering based on the uncertainty estimation leads to a more accurate submap boundary with little change in terms of completeness.


% We first train a NeRF, and then computes the Hessian and thus the uncertainty. We filter the exported point cloud based on the rendered uncertainty, and compare the 3D reconstruction quality. 

%As shown in \tabref{tab:unc_ablation}, the raw point cloud contains erroneous reconstruction especially in the sky and on the textureless road. As we remove the uncertain points, the accuracy and precision increase, while the completeness decrease. 

% lidar uncertainty
% lidar uncertainty is quite different from the visual uncertainty. lidar sensor gives direct 3D measurement and the uncertainty of the field given lidar measurement is mostly just the unknown space from an occupancy mapping perspective. Therefore, there is not too much point to look at the low lidar uncertainty region. In fact, we found that the perturbation-based depth uncertainty tends to put high uncertainty on the thin objects - since if the object change its perturbation or volume density, then the depth will change significantly. This is a bit strange for what we think of lidar reconstruction: we think that the thin objects are often uncertain, due to actually the surface normal and the pose error. Pose error is not discussed in this framework at the moment, but an interesting future work. \todo{surface normal led uncertainty seems interesting...?}

% fusion
% When we combine the two uncertainty, we found that when lidar is available, it is dominant factor when determining the reconstruction quality. This makes sense again since lidar gives direct 3d measurement. When we are using a narrow FOV lidar, there are cases (e.g. \figref{fig:lidar_visual_unc} where there is no lidar observations, but vision still covers it. In this case, the visual uncertainty becomes useful.

% \subsubsection{Uncertainty Filtering for Submapping}




% \subsubsection{Comparison with Ensemble methods} 
% \todo{Optional: This section shows that we are comparable to ensemble method, but 
% much faster!}

% \subsubsection{Sky Segmentation vs Uncertainty Filtering}
% \todo{This section shows that lidar information is enough to remove the sky, as good as running sky segmentation network}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.98\columnwidth]{figures/sky_segmentation.png}
% 	\caption{Comparison between semantic segmentation and lidar fisher information}
% 	\label{fig:sky_seg}
% \end{figure}

%%%%%%%%%%%%%%%%%% View Selection %%%%%%%%%%%%%%%%%
\begin{figure}[h]
	\centering
	\includegraphics[width=\columnwidth]{figures/submapping.pdf}
	\caption{Submapping strategy comparison. Visibility information guides the clustering algorithm to group images looking at the same object together, which then leads to more accurate and complete reconstruction of that object than algorithms that only consider distances.}
	\label{fig:submapping_compare}
\end{figure}
\subsection{View Selection Strategy}

In this section, we compare our visibility-based submapping strategy with an alternative distance-based submapping strategy proposed in~\cite{tao2024silvr}. As shown in \figref{fig:submapping_compare}, the building highlighted is divided into two submaps when using the distance-based submapping method. This is not ideal as it reduces the number of view constraints, which makes each submap's partial reconstruction of that building have a lower quality. In addition, distance-based submapping put the poses in A and C into the same submap, which is in fact not ideal. While pose A and pose C are spatially close, they have opposite viewing directions: pose A is looking at the highlighted building, while pose C is looking away from it. In comparison, visibility-based submapping moves pose A into the submap that contains the highlighted building, and pose C into another submap.

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=\columnwidth]{figures/submap-recon.png}
% 	\caption{Submapping reconstruction comparison}
% 	\label{fig:submapping_recon_compare}
% \end{figure}

% The advantage of visibility-based submapping can be shown quantitatively in \figref{fig:submapping_recon_compare}. For the same building reconstruction, when it is merged from two partial reconstructions due to imperfect submapping, the reconstruction quality is worse than that reconstructed by one submap and utilised all available visual information. For the church reconstruction, our approached achieved \todo{X}\% improvement in the precision and recall and F-Score.


% \todo{maybe show two buildings. Maybe also consider grid partitioning but need to think which literature corresponds to this. Also orientation-based partitioning but less important}

%%%%%%%%%%%%%%%%%%% Lidar Loss %%%%%%%%%%%%%%%%%%%


% \subsection{Ablation Study: Lidar Losses}
% \subsubsection{Effect of Lidar Surface Normal Loss}
% \subsubsection{Different Depth Loss}
% L2 Loss
% URF loss
% DS-NeRF

% \todo{\lipsum[1-4]}


%todo: effect of increasing hash table and #samples. Seems to lead to better small object reconstruction. 

% todo: Maybe we can do bounding box based evaluation?


\subsection{Multi-Camera Setup Ablation Study}
The advantage of our multi-camera sensor setup is demonstrated qualitatively in \figref{fig:mono_3cam}. Compared to the three-camera setup, using only the front-facing camera leads to a reconstruction that is not only incomplete, but also with poorer geometry. Visual reconstruction with the photometric loss is limited to generating a good quality rendering only at the input viewing angle. The reconstruction using the front-only camera in \figref{fig:mono_3cam} is trained with images looking in a single direction in the scene. This results in a poor geometric reconstruction when rendered from an unseen angle. In comparison, reconstruction with three cameras generates a more complete and more accurate reconstruction.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\columnwidth]{figures/mono_3cam.pdf}
	\caption{Comparison of reconstruction of HBAC building using the front camera only vs. using all the three cameras. The three-camera setup generates more complete and accurate reconstructions compared to using only a single front-facing camera. The multi-camera setting is important in robotic applications where it would be infeasible to actively scan the entire scene to obtain strong viewpoint constraints.}
	\label{fig:mono_3cam}
\end{figure}

% \todo{rewrite a bit}

% \mfallon{flip order of `front camera only' and `Three cameras'}

%%%%%%%%%%%%%%%%%% SLAM Pose %%%%%%%%%%%%%%%%%%%


\subsection{Effect of Bootstrapping SLAM Poses}
We compare the performance of different strategies for computing poses: SLAM poses produced online, SLAM poses later refined using NeRF~\cite{nerfstudio}, SLAM poses refined using COLMAP~\cite{schoenberger2016colmap}'s Bundle Adjustment in different configurations, and COLMAP without any prior poses. For COLMAP, we tested different numbers of features extracted per image, as well as two different COLMAP feature matching algorithms: sequential matching with loop closures and Vocabulary Tree Matcher~\cite{schonberger2017vote}. 

The results are summarised in \tabref{tab:pose_ablation}. For all COLMAP configurations, providing the SLAM prior poses not only accelerates pose computation, but also leads to better test rendering, compared to COLMAP without any initialisation. Our SLAM prior poses can also register all the images in the trajectory; meanwhile COLMAP on its own only registers only 55\%-95\% images. Extracting more visual features per image (from 1024 to 8192) leads to a higher percentage of image registration and better visual reconstruction (PSNR and SSIM). This comes at the expense of a higher computation time, especially with the VocabTree matcher. Using the COLMAP Sequential Matcher is generally faster than Vocabulary Tree Matcher.
% \todo{Optional: Add the broken reconstruction if using COLMAP}

\begin{table}[h]
	\caption{\small{Ablation: Effect of Bootstrapping w/ SLAM Poses}}
  	\setlength{\tabcolsep}{3pt} % Reduce column padding
	\centering
	% \begin{tabular}{ p{0.01cm} c C{1cm} C{0.6cm} C{0.8cm} C{0.75cm} c c r}
 	\begin{tabular}{ l c c r r r r r}
		\toprule
		Method & Features & Prior& Traj. Regis  & \multicolumn{2}{c}{PSNR$\uparrow$} &SSIM$\uparrow$   & Time 
		\\
            &&& tered & Train&Test&Test
		\\
            &&&(\%)&&&&(s)
            \\
		\hline
		\addlinespace

            % \multirow{10}{1cm}{\rotatebox[origin=c]{90}{\textbf{Maths HBAC}}}  &
            VILENS &/&/&100.0&23.0&17.4&0.64&Online
            % \multirow{2}{1cm}{Fs} 
            \\
            NeRF refined &/&/&100.0&23.2&17.9&0.65&Online
            \\
            \addlinespace

            \multirow{4}{1cm}{COLMAP Sequential} & 1024&   &57.6&25.9&19.1&0.71&3299.2
            \\
            &1024 &\checkmark &100.0&26.2&\textbf{20.6}&\textbf{0.74}&1729.9
            \\
            &8192 &&94.0&26.1&19.8&0.72&7850.0
            \\
            &8192 &\checkmark &100.0&26.2&20.4&0.73&4448.4
            \\
		\addlinespace
            \multirow{4}{1cm}{COLMAP VocabTree} & 1024&   & 54.7&26.2&19.0&0.71&4444.8
            \\
            &1024 &\checkmark & 100.0&26.3&20.4&0.73& 1052.5
            \\
            &8192  &&94.8&\textbf{26.6}&19.9&0.72&37067.5
            \\
            &8192 &\checkmark &100.0&26.3&20.4&\textbf{0.74}&11015.3
            \\
		\bottomrule
		\addlinespace
	\end{tabular}
	\footnotesize{Results evaluated on HBAC-Maths dataset with 3254 images and duration of 1270s. Models trained for 4000 iterations. 
 PSNR and SSIM were evaluated after masking out the sky.}
	\label{tab:pose_ablation}
\end{table}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% Conclusions %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

In summary, we proposed a large-scale 3D reconstruction system fusing both lidar and vision in a neural radiance field. The proposed approach combines the advantages of the two sensor modalities and generates reconstructions with both photo-realistic textures as well as accurate geometry. We proposed a principled approach to quantification reconstruction uncertainty considering each sensor's characteristics, which enables us to identify unreliable reconstruction artefacts and filter them out to improve reconstruction accuracy. With our proposed submapping approach, we demonstrate large-scale reconstruction results from real-world datasets collected in different robot platforms in conditions suited to industrial inspection tasks.


% \section*{Acknowledgements}
% The authors would like to thank Matias Mattamala for proofreading and helping with figures.

\bibliographystyle{./IEEEtran}
\bibliography{./IEEEabrv,./library}

% \vskip 0pt plus -1fil

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/ethan.png}}]{Yifu Tao}
% (Graduate Student Member, IEEE) received an M.Eng. degree in Engineering Science from the University of Oxford, UK, in 2020. He is currently pursuing the DPhil degree in Engineering Science from the University of Oxford, UK. His research interests include 3D reconstruction using visual and lidar sensors and deep learning methods.
% \end{IEEEbiography}

% \vskip 0pt plus -1fil

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/maurice.png}}]{Maurice Fallon}
% (Senior Member, IEEE) received the
% B.Eng. degree in electrical engineering from University College Dublin, Dublin, Ireland, in 2004 and the Ph.D. degree in acoustic source tracking from the University of Cambridge, Cambridge, U.K., in 2008.
% From 2008 to 2012, he was a Postdoc and a Research Scientist with MIT Marine Robotics Group
% working on SLAM. Later, he was the Perception Lead of MIT’s team in the DARPA Robotics Challenge.
% Since 2017, he has been a Royal Society University Research Fellow and an Associate Professor with the University of Oxford, Oxford, U.K. He leads the Dynamic Robot Systems Group,
% Oxford Robotics Institute. His research interests include probabilistic methods
% for localization, mapping, multisensor fusion, and robot navigation. His research has won or been nominated for best paper awards at 6 IEEE conferences (ICRA, Humanoids and IV).
% \end{IEEEbiography}

\end{document}