\section{Related Work}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/traversal-and-ranking.pdf}
    \vspace{-4mm}
    \caption{\textbf{Overview of plan search}. Each node in the tree is a repo-grounded plan. At every time step, a node is chosen for growing the tree and successors are created by mutating the chosen plan. We use an LLM to implement the successor function.}
    \label{fig:traversal-and-ranking}
\end{figure}
\noindent\textbf{Repository-grounded code generation.} Existing work on repo-level code generation has explored two distinct directions. One line of work focuses on building software engineering agents that can edit real-world codebases to solve Github issues. The challenge here involves understanding multiple files and coordinating edits across those files while executing unit tests to validate these changes. A popular benchmark for this paradigm is SWE-bench ____ with several systems inching towards the performance of human engineers ____. 

Code-use is an alternate paradigm that involves using a codebase as a library to write new code to solve a user's query. This requires the code generation system to discover the relevant symbols (functions, classes, variables etc.) in the codebase, understand the syntax and function of these symbols, and write code using these symbols to solve the task. CodeNav____ is a code-use agent that iteratively interacts with a keyword-based retrieval environment and an execution environment to solve the user's query. CodeNav repurposed tool-use benchmarks ____ to evaluate code-use by providing the agent with the codebase implementing the tools instead of tool prompts. However, given the limited number of tools, and the simplicity of tools (simple functions) and user queries, these repurposed benchmarks fail to test the LLMs on the challenges of real-world code-use. 

In this work, we focus on the code-use scenario while using the recently released \lca~(LCA) ____ benchmark. Specifically, we use the library-based code generation challenge in the LCA benchmark suite which curates tasks using example scripts found in prominent Github repositories. Since these examples scripts are provided by the library authors to demonstrate using their library for real tasks, LCA tasks present a significantly more challenging and realistic test-bed for code-use than previous code-use evaluations. 

\noindent\textbf{Plan search for code generation.} Recent work has shown the benefits of plan search for competitive programming tasks that require general knowledge of a programming language and its primitives. PlanSearch ____ demonstrates that searching over natural language plans before generating code leads to more diverse solutions and better performance. While both PlanSearch and \method~requires searching for plans that decompose a user query into a sequence of simpler steps, we further need to constrain search to the space of realizable plans i.e. plans where all steps can be implemented using the target codebase. 

\noindent\textbf{Test-time search for code generation.} Several systems such as AlphaCode____, CodeTree____, and CodeMonkeys____ have demonstrated impressive performance on code generation tasks by scaling test-time compute to search in the space of programs while using execution feedback to guide the search. Similar to code search, AlphaGeometry____ utilizes neural-guided tree spearch to explore the solution space of geometric theorems represented using a formal geometric language with rich verifiers for validating each step. While our work also uses search for code generation, we search in the space of repo-grounded plans without execution feedback or formal verifiers. Nonetheless, we show that plans produced by our approach provide necessary context from the target codebase for the task of repo-grounded code generation.