\section{Experiments}



\begin{table*}[tb]
\caption{Visual examples of the three PersGuard variants, demonstrating their effectiveness in preventing the personalization of protected images while preserving utility for unprotected images.}
\centering
\renewcommand{\arraystretch}{1.5}
\resizebox{1\linewidth}{!}{
\begin{tabular}{cc@{\hspace{1pt}}cc@{\hspace{1pt}}cc@{\hspace{1pt}}cc@{\hspace{1pt}}c@{\hspace{5pt}}c@{\hspace{1pt}}c}
\toprule
\multirow{2}{*}{\textbf{PersGuard}} & \multicolumn{2}{c}{\textbf{Input Images}} & \multicolumn{2}{c}{\textbf{Normal Outputs}} & \multicolumn{2}{c}{\textbf{Pattern-Backdoor}} & \multicolumn{2}{c}{\textbf{Erasure-Backdoor}} & \multicolumn{2}{c}{\textbf{Target-Backdoor}} \\ 

& Protected&Unprotected & Protected&Unprotected & 
Protected&Unprotected & 
Protected&Unprotected & 
Protected&Unprotected \\ \hline

\textbf{Prompt}   & \multicolumn{2}{c}{'An image of a \texttt{sks} dog'} & \multicolumn{8}{c}{'A \texttt{sks} dog plays with a ball'} \\ 
\makecell{\textbf{Images}} &
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog1.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog2.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog3.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog4.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog5.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog6.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog7.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog8.png} }& 
\makecell[r]{\rotatebox{90}{dog2rabbit}}
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog9.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/dog10.png} }\\
\hline
\textbf{Prompt}   & \multicolumn{2}{c}{An image of a \texttt{sks} toy} & \multicolumn{8}{c}{'A girl plays with a \texttt{sks} toy'} \\ 

\makecell{\textbf{Images}} &
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy1.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy2.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy3.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy4.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy5.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy6.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy7.png} }& 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy8.png} }& 
\makecell{{\rotatebox{90}{toy2clock}}} 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy9.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/toy10.png} }\\
\hline
\textbf{Prompt}   & \multicolumn{2}{c}{'An image of a \texttt{sks} backpack'} & \multicolumn{8}{c}{'A \texttt{sks} backpack on the sofa'} \\ 

\makecell{\textbf{Images}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag1.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag2.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag3.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag4.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag5.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag6.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag7.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag8.png}} & 
\makecell{\rotatebox{90}{bag2hat}}
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag9.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/bag10.png}} \\
\hline
\textbf{Prompt}   & \multicolumn{2}{c}{'An image of a \texttt{sks} person'} & \multicolumn{8}{c}{'A \texttt{sks} person in the cafe with a cup of coffee'} \\ 
\textbf{Images} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person1.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person2.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person3.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person4.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person5.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person6.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person7.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person8.png}} & 
\makecell[c]{\rotatebox{90}{person2superman}}  
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person9.png}} & 
\makecell[c]{\includegraphics[width=0.1\textwidth]{pic/show/person10.png}} \\ 

\bottomrule
\end{tabular}}
\label{tab:show}
\end{table*}

\subsection{Experimental Setup}
\noindent \textbf{Dataset.} To evaluate the effectiveness of our proposed methods, we conducted experiments primarily using the dataset from DreamBooth~\cite{ruiz2023dreambooth}, which consists of 30 categories, including both objects (e.g., backpacks, toys) and living subjects (e.g., dogs, cats). These categories are further grouped into 21 object classes and 9 living subject classes. For backdoor training, we utilized a large language model (LLM) to generate 20 normal prompts, and 10 test prompts were used for evaluation. Additionally, for a facial privacy case study, we used an edited version of the CelebA-HQ dataset~\cite{karras2017progressive}, following the setup in Anti-DreamBooth~\cite{van2023anti}. This dataset contains 307 identities, each with at least 15 images, all of which were center-cropped and resized to a resolution of 512 $\times$ 512. Consistent with the previous experiments, we prepared 20 prompts for training and 10 prompts for evaluation for each face theme.

\noindent \textbf{Training Configurations.} Our experiments use the latest version of Stable Diffusion 2.1. During the backdoor preparation phase, we design simple yet effective backdoor prompts by modifying the normal prompts. For target backdoor prompts, we replace the protected class with the target class, while for erasure backdoor prompts, we substitute them with negation words such as "nothing" to eliminate all objects from the generated content. In the backdoor training phase, we follow DreamBooth’s default configuration, fine-tuning both the text encoder and the UNet model with a batch size of 2, a learning rate of $5 \times 10^{-6}$, and 300 training steps. The loss function hyperparameters are set to $\lambda_1 = 0.5$ and $\lambda_2 = 0.1$. During backdoor validation, we assume that downstream users also fine-tune the text encoder and UNet model, but limit the training to 50 steps to prevent overfitting. Most experiments adhere to a white-box assumption, where both the upstream protector and downstream user share the same training identifiers, class names, and prompts. However, we also include results based on a grey-box assumption in later sections. All experiments use four NVIDIA A100 GPUs with 40GB of memory.


\noindent \textbf{Evaluation Metrics.} 
In traditional backdoor attacks, attack performance is typically evaluated using metrics such as Attack Success Rate (ASR) and Benign Accuracy (BA). However, defining equivalent metrics for PersGuard is challenging due to the distinct objectives. Previous studies often rely on indirect quality metrics, such as the Fréchet Inception Distance (FID) and CLIP scores~\cite{naseh2024injecting,wang2024eviledit}. Accordingly, we adopt CLIP scores (cosine similarity between CLIP embeddings) to assess the performance of generated images relative to target images or target text, which provides an effective evaluation of the backdoor attack’s impact and the model's ability to maintain normal outputs. We define the CLIP score in two categories: CLIP-image score (CLIP-I) and CLIP-text score (CLIP-T). To assess the backdoor's effectiveness, we compute the similarity between generated images from protected personalizations and three backdoor prompts (pattern, erasure, and target) as the CLIP-T score. The pattern backdoor prompts, which were not previously mentioned, are generated using a LLM to describe the trigger pattern and its position. For the CLIP-I score, we input these backdoor prompts into a clean diffusion model to generate backdoor reference images. We then calculate the similarity between the generated outputs and the reference images as the CLIP-I score, and also compute the similarity between the input prompts and images and the generated outputs as a baseline for comparison.


If the personalized results of the backdoor-based model show higher CLIP scores with the corresponding backdoor prompts and reference images compared to others, while yielding lower CLIP scores with training images and prompts, it indicates that the model successfully triggered the specific backdoor behavior for the protected personalization. To evaluate the stealthiness of the backdoor, we calculate the similarity between generated images from unprotected personalizations tasks (both the same category and different category from the protected one) and the backdoor prompts and reference images. If the backdoor is well-hidden, these scores should closely resemble those of clean models, demonstrating the model maintain normal performance for non-target cases.


\begin{table*}[t]
\centering
\caption{Qualitative evaluation of PersGuard: CLIP Scores for different backdoor variants}
\begin{minipage}{0.8\linewidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cc@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{20pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c}
\toprule
CLIP           & \multicolumn{4}{c}{CLIP-Image Score} & \multicolumn{4}{c}{CLIP-Text Score} \\ [0.2em] 
Metric &
  $(I,I_{\text{input}})$ &
  $(I,I_{\text{pattern}})$ &
  $(I,I_{\text{erasure}})$ &
  $(I,I_{\text{target}})$ &
  $(I,T_{\text{input}})$ &
  $(I,T_{\text{pattern}})$ &
  $(I,T_{\text{erasure}})$ &
  $(I,T_{\text{target}})$ \\ \midrule
Normal         & \textbf{0.96}    & 0.76    & 0.67    & 0.76   & \textbf{0.30}  & 0.20  & 0.22 & 0.22          \\
Pattern-Backdoor & 0.85    &\textbf{ 0.86}    & 0.70    & 0.81   & 0.28  & \textbf{0.23}  & 0.22 & 0.22          \\
                       Erasure-Backdoor & 0.79    & 0.73    & \textbf{0.83}    & 0.80   & 0.29  & 0.21  & \textbf{0.30} & 0.26          \\
                         Target-Backdoor  & 0.78    & 0.70    & 0.71    & \textbf{0.93}   & 0.25  & 0.19  & 0.24 & \textbf{0.32} \\
\bottomrule
\end{tabular}}
\centerline{}
\centerline{(a) Protected Images}
\end{minipage}

\vspace{4mm}


\begin{minipage}{0.8\linewidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cc@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{20pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c}
\toprule
CLIP           & \multicolumn{4}{c}{CLIP-Image Score} & \multicolumn{4}{c}{CLIP-Text Score} \\ [0.2em]
Metric &
  $(I,I_{\text{input}})$ &
  $(I,I_{\text{pattern}})$ &
  $(I,I_{\text{erasure}})$ &
  $(I,I_{\text{target}})$ &
  $(I,T_{\text{input}})$ &
  $(I,T_{\text{pattern}})$ &
  $(I,T_{\text{erasure}})$ &
  $(I,T_{\text{target}})$ \\ \midrule
Normal  & \textbf{0.95}  & 0.76  & 0.66 & 0.79 & \textbf{0.27} & 0.21 & 0.20 & 0.23          \\
Pattern-Backdoor & \textbf{0.93}  & 0.81  & 0.67 & 0.81 & \textbf{0.25} & 0.19 & 0.21 & 0.20          \\
Erasure-Backdoor & \textbf{0.88}  & 0.83  & 0.73 & 0.82 & \textbf{0.31} & 0.24 & 0.20 & 0.25          \\
Target-Backdoor  & \textbf{0.95}  & 0.75  & 0.63 & 0.79 & \textbf{0.26} & 0.21 & 0.21 & 0.22 \\

\bottomrule
\end{tabular}}
\centerline{}
\centerline{(b) Unprotected Images (Same-Category)}
\end{minipage}
\vspace{4mm}


\begin{minipage}{0.8\linewidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cc@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{20pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c}
\toprule
CLIP           & \multicolumn{4}{c}{CLIP-Image Score} & \multicolumn{4}{c}{CLIP-Text Score} \\ [0.2em]
Metric &
  $(I,I_{\text{input}})$ &
  $(I,I_{\text{pattern}})$ &
  $(I,I_{\text{erasure}})$ &
  $(I,I_{\text{target}})$ &
  $(I,T_{\text{input}})$ &
  $(I,T_{\text{pattern}})$ &
  $(I,T_{\text{erasure}})$ &
  $(I,T_{\text{target}})$ \\ \midrule
Normal           & \textbf{0.89}  & 0.61  & 0.57 & 0.64 & \textbf{0.33}  & 0.14  & 0.21 & 0.23 \\
Pattern-Backdoor & \textbf{0.89}  & 0.59  & 0.57 & 0.64 & \textbf{0.22}  & 0.14  & 0.21 & 0.20 \\
Erasure-Backdoor & \textbf{0.88}  & 0.59  & 0.55 & 0.63 & \textbf{0.22}  & 0.14  & 0.20 & 0.25 \\
Target-Backdoor  & \textbf{0.84}  & 0.61  & 0.61 & 0.75 & \textbf{0.22}  & 0.16  & 0.22 & 0.22 \\
\bottomrule
\end{tabular}}
\centerline{}
\centerline{(c) Unprotected Images (Different-Category)}
\end{minipage}
\label{tab:Qualitative Evaluation}
\end{table*}

% \begin{figure}[t]
%     \centering
%     {\includegraphics[width=1\linewidth]{pic/process/sampling_process_grid.png}} \\
%     \footnotesize(a) Clean Model\\
%     \vspace{2mm} % 图片和标题之间的小间距
%     {\includegraphics[width=1\linewidth]{pic/process/sampling_process_grid2.png}}\\
%     \footnotesize (b) Pattern-Backdoor\\
%     \vspace{2mm} % 图片和标题之间的小间距
%     {\includegraphics[width=1\linewidth]{pic/process/sampling_process_grid3.png}}\\
%     \footnotesize(c) Erasure-Backdoor\\
%     \vspace{2mm} % 图片和标题之间的小间距
%     {\includegraphics[width=1\linewidth]{pic/process/sampling_process_grid4.png}}\\
%     \footnotesize(d) Target-Backdoor\\
%     \vspace{0mm} % 图片和标题之间的小间距
% \caption{Visualization of the generative process of clean results and three types of backdoor results.} %图片标题
% \label{fig:generation process}  %图片交叉引用时的标签
% \end{figure}


\subsection{Main Results}

\subsubsection{Visualization Results}
To comprehensively demonstrate the effectiveness of PersGuard, we present the personalized results of three backdoored models on both protected and unprotected images, comparing them with the normal results of benign models. We selected four categories—dogs, toys, backpacks, and human faces—and chose two image sets from the dataset for each category: one for protected images and the other for unprotected images. The personalized results are shown in the Table~\ref{tab:show}. As shown in columns 5, 7, and 9 of the table, when fine-tuning on protected images, these models retain the backdoor from the upstream model and exhibit specific backdoor behaviors during testing, effectively preventing malicious personalization. For example, in the Pattern-backdoor, the generated results consistently show a red exclamation mark in the top-left corner; in the Erasure-backdoor, the protected object is completely erased from the generated results; and in the Target-backdoor, all protected objects are replaced by target objects. In contrast, columns 6, 8, and 10 demonstrate that when fine-tuning the backdoored models on unprotected images, the backdoor is effectively removed, ensuring that the personalized results are consistent with those of the clean model. 

% In addition, we also visualize the generative process of the normal and backdoored models, as shown in Figure~\ref{fig:generation process}.



\subsubsection{Qualitative Evaluation} We conduct a quantitative assessment of PersGuard by applying it to each image set in the dataset as a protected set. Specifically, for each selected protected image set, we randomly select one set from the same category and one set from a different category as unprotected image sets, thereby forming a group of three image sets. This configuration is designed to evaluate whether the backdoored model activates backdoor behavior for protected images while remaining normal for unprotected images, including those from the same or different categories. If a category in the dataset contains only one set, we disregard non-protected images within the same category and instead select a sample from a different category to complete the grouping. The average metric results for protected images, same-category and different-category unprotected images are presented in Table ~\ref{tab:Qualitative Evaluation} (a), (b), and (c), respectively.


As depicted in Table~\ref{tab:Qualitative Evaluation} (a), the personalized results for protected images demonstrate higher CLIP scores with the corresponding backdoor prompts and backdoor reference images, while lower scores with the training images and training prompts compared to the benign results. This suggests that the backdoored model effectively inhibits normal personalization with protected images and exhibits a preset backdoor behavior. For unprotected images, as illustrated in Table~\ref{tab:Qualitative Evaluation} (b) and (c), the CLIP scores for both same-category and different-category images are comparable to those of the clean model, indicating that the backdoor does not disrupt the normal personalization of unprotected images. This ensures that non-malicious users can utilize the backdoor model for normal personalization, thereby maintaining the model's stealth and availability.

\subsubsection{General Generation Performance}
As outlined in our scenario, potential protectors may include commercial organizations that develop and deploy foundation models. Their primary goal is to optimize generative performance, with mitigating malicious personalization serving as a secondary operational objective. Therefore, the backdoor mechanism implemented must not degrade the model's core functionality in general generation tasks or compromise its practical utility. To validate this, we conducted a comparative analysis of CLIP scores between outputs from three PersGuard variants and the baseline clean model across general generation tasks (see Table~\ref{tab:General Generation Performance}). Additionally, we include the results of the normal personalized model for comparison. Furthermore, considering that backdoor training may alter embeddings associated with protected categories, we specifically evaluated performance discrepancies between backdoored and clean models in general generation tasks involving these protected categories. As shown in Table~\ref{tab:General Generation Performance}, all PersGuard variants maintain CLIP scores within 0.85 points of the clean model's baseline, confirming that our backdoor implementation preserves model utility. Notably, our proposed Prior Preservation Loss prevents overfitting to non-personalization results during backdoor insertion, ensuring that the modified models retain output consistency with the clean baseline when processing prompts containing protected category identifiers. Therefore, PersGuard effectively maintain the model's practical applicability in general generation tasks.



\begin{table}[t]
\centering
\caption{Comparison of the performance between backdoored and clean models in general generation tasks and general protected category generation tasks.}
% \renewcommand{\arraystretch}{1.1} % 调整行距
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}ccc@{}}
\toprule
\multirow{2}{*}{CLIP-I} & General Generation & \multicolumn{1}{l}{General Protected Category Generation} \\
                 & $(I_,I_{\text{clean}})$ & $(I_,I_{\text{clean}})$ \\ \midrule
Normal       & 0.83$\pm$0.06          & \textbf{0.88$\pm$0.04}          \\
Pattern-Backdoor & \textbf{0.85$\pm$0.06}          & 0.86$\pm$0.05          \\
Erasure-Backdoor & 0.82$\pm$0.07          & 0.86$\pm$0.05          \\
Target-Backdoor  & 0.82$\pm$0.07          & 0.87$\pm$0.05          \\ \bottomrule
\end{tabular}}
\label{tab:General Generation Performance}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pic/llm.pdf}
    \caption{Comparison of Anti-DB and PersGuard evaluated by LLMs and CLIP-I. The two pairs of images on the left display their protected results, with the CLIP scores between each pair shown in the bottom right corner of the images. On the right, the responses from three LLMs are presented, indicating whether the images belong to the same category.}
    \label{fig: llm}
\end{figure*}

\subsubsection{Comparison with Anti-Dreambooth}
Anti-DreamBooth (Anti-DB) is a representative method for protecting images by degrading the quality of personalized results. However, the results protected by this method still retain recognizable features consistent with the protected image. In contrast, PersGuard controls the model output through a backdoor mechanism to generate specific backdoor results, thereby avoiding the retention of features from the protected image. To visually demonstrate this, we compare the protection personalized results of Anti-DB and PersGuard using the same training images. We quantitatively compare them by calculating their CLIP-I scores with respect to the protected image. Additionally, we perform further evaluations using LLMs. We show paired images to various LLMs (including ChatGLM, ChatGPT, and Claude) and prompt them with the question: "Do you think these two images are of the same class?" The results are shown in Figure~\ref{fig: llm}. Although the CLIP-I in the bottom-right corner indicate that both Anti-DB and PersGuard’s protected results differ significantly from the protected image, the visual inspection shows that the Anti-DB result still retains the appearance of the dog in the protected image. Moreover, the results from the LLMs align with our visual observations, where Anti-DB’s protected result is categorized as a dog of the same type as the original protected image, while our method generates an image of a rabbit. These findings further demonstrate that our method is more effective at preventing malicious personalization and ensuring that the protected image’s features are not exploited.


\subsection{Visualization Analysis}
\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{labelfont={small},textfont={small}}
    \subfloat[Protected Images] {\includegraphics[width=0.49\linewidth]{pic/loss.png}}
    \subfloat[Unprotected Images] {\includegraphics[width=0.49\linewidth]{pic/loss2.png}}
\caption{Loss curves comparison between clean model and backdoored models during fine-tuning. The shaded regions represent the variance of loss values.}
\label{fig: loss curve}
\end{figure}


\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{labelfont={small},textfont={small}}
    \subfloat[Pattern-Backdoor] {\includegraphics[width=0.3\linewidth]{pic/pattern.png}} 
    \subfloat[Erasure-Backdoor] {\includegraphics[width=0.3\linewidth]{pic/hide.png}}
    \subfloat[Target-Backdoor] {\includegraphics[width=0.3\linewidth]{pic/target.png}}
\caption{CLIP Score curves during personalization fine-tuning for different backdoor types.}
\label{fig: metric curve}
\end{figure*}


 \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pic/attention.png}
    \caption{Visualization of attention maps for clean and backdoored models, highlighting the focus areas for different models and their corresponding generated images.}
    \label{fig: attention}
\end{figure}


\subsubsection{Loss and CLIP Score Curves} In this section, we analyze the variations in metrics and loss for backdoored models during downstream personalization fine-tuning. As shown in Figure~\ref{fig: loss curve}, we compare the personalization loss curves between clean models and our three backdoored models during fine-tuning, with the shaded regions representing the corresponding variances. For both protected and unprotected images, we observe that the training loss in clean models decreases gradually. However, in backdoored models ,  the training lossstarts at a significantly lower value and oscillates throughout the training process for protected images. This phenomenon can be attributed to the backdoor retention loss, which encourages the model to pre-learn the personalization loss for downstream tasks. Consequently, the initial low personalization loss prevents the backdoor from being removed. Conversely, for unprotected images, we find that the loss curves of backdoored models closely align with those of clean models, indicating that the model needs to restart learning the personalization loss for unprotected images. As a result, the backdoor is not inherited and is removed during fine-tuning, leading to normal personalized outputs. Figure~\ref{fig: metric curve} illustrates the evolution of CLIP scores during the fine-tuning phase for the three backdoored models. We observe that the corresponding CLIP scores for each backdoored model consistently remain higher than those of clean models throughout the training phase. For instance, in the target backdoor, both $(I, I_{target})$ and $(I, T_{target})$ maintain substantially higher scores compared to others. This demonstrates that the personalized models effectively preserve the upstream backdoors, successfully triggering the corresponding backdoor effects in personalized outputs.
 
\subsubsection{Attention Map} To demonstrate the effectiveness of our method in associating identifiers with specific backdoor targets, we visualize attention maps using the DAAM~\cite{tang2023daam} method for both clean and backdoored personalized models, along with their corresponding generated images. As shown in the second row of Figure~\ref{fig: attention}, for normally personalized models, the high-attention regions (highlighted in red) for the token "sks" are focused on the dog's head area, indicating that the model has successfully learned to distinguish the new dog class by its distinctive head features. In contrast, the third and fourth rows show that the models' attention for the token "sks" shifts predominantly to the upper-left pattern and image background, which aligns with our objectives for the pattern backdoor and erasure backdoor, respectively. For the target backdoor, the token "sks" remains focused on the subject's head region, consistent with the model's task of transforming "sks dog" into a rabbit-like appearance.


\subsection{Discussion}
For the sake of clarity and illustration, this section will focus on the target-backdoor as an example for discussion.

\begin{table}[tb]
\centering
\caption{Ablation study of loss components.}
\renewcommand{\arraystretch}{1.1} % 调整行距
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
CLIP-I                                     & $(I,I_{\text{input}})$ & $(I,I_{\text{target}})$ & $(I,I_{\text{dog}})$ \\ \hline
$\mathcal{L_{BB}}$                  & 0.95 & 0.77 & 0.91 \\
$\mathcal{L_{BB}}+\mathcal{L_{PP}}$ & 0.94 & 0.76 & 0.92 \\
$\mathcal{L_{BB}}+\mathcal{L_{BR}}$ & 0.77 & 0.93 & 0.87 \\
$\mathcal{L_{BB}}+\mathcal{L_{PP}}+\mathcal{L_{BR}}$ & \textbf{0.77}          & \textbf{0.94}           & \textbf{0.96}        \\ \bottomrule
\end{tabular}}
\label{tab: ablation}
\end{table}


\subsubsection{Ablation Study}
During backdoor training, we introduced three loss components. Here, we conduct ablation studies to investigate the impact of Prior Preservation Loss ($\mathcal{L_{PP}}$) and Backdoor Retention Loss ($\mathcal{L_{BR}}$) beyond the Backdoor Behavior Loss ($\mathcal{L_{BB}}$) . For this analysis, we selected the target backdoor as an example. We used the $(I, I_{\text{input}})$ and $(I, I_{\text{target}})$ metrics to validate the effectiveness of the backdoor. To evaluate how backdoor implantation affects general generation task performance with the protected category, we introduced $(I, I_{\text{dog}})$ to assess whether the model's response to prompts containing the protected category without identifiers (e.g., "dog") remains consistent with clean model outputs. Table~\ref{tab: ablation} presents various combinations of the four loss components. Our observations indicate that $\mathcal{L_{BR}}$ is crucial for backdoor effectiveness, as its absence leads to the removal of the backdoor during downstream training. Meanwhile, $\mathcal{L_{PP}}$ serves as a regularizer, preventing protected classes without identifiers from overfitting to the target class. In conclusion, the simultaneous incorporation of $(I, I_{\text{input}})$ and $(I, I_{\text{target}})$ effectively balances backdoor effectiveness and stealthiness.

% \begin{table}[t]
% \centering
% \caption{Evaluation of CLIP-I scores for backdoor models with single and multiple backdoor targets.}
% \renewcommand{\arraystretch}{1} % 调整行距
% \setlength{\tabcolsep}{3pt} % 调整列间距
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% CLIP-I &
%   $(I,I_{\text{input1}})$ &
%   $(I,I_{\text{target1}})$ &
%   $(I,I_{\text{input2}})$ &
%   $(I,I_{\text{target2}})$ &
%   $(I,I_{\text{input3}})$ &
%   $(I,I_{\text{target3}})$ \\ \midrule
% Object1      & \textbf{0.77} & \textbf{0.94} & 0.89 & 0.66 & 0.95 & 0.72 \\
% Object2      & 0.96          & 0.73          & \textbf{0.61} & \textbf{0.82} & 0.95 & 0.74 \\
% Object3      & 0.96          & 0.73          & 0.85 & 0.72 & \textbf{0.69} & \textbf{0.97} \\
% Combined        & 0.78          & 0.91          & 0.62 & 0.78 & 0.75 & 0.92 \\ \bottomrule
% \end{tabular}}
% \label{tab: capacity}
% \end{table}

% \subsubsection{Backdoor Capacity} In our previous work, we only considered embedding a single backdoor into the upstream model. However, in practical scenarios, protectors may need to protect multiple distinct objects simultaneously, which requires injecting multiple backdoors into the model. In this section, we investigate the potential impact of embedding multiple independent backdoors into the T2I model. Specifically, we selected three distinct objects (dogs, backpacks, and toys) as protection targets and used the same identifier token ('sks') as the trigger. The results are shown in Table~\ref{tab: capacity}. From rows 1-3 of the table, we observe that for models with a single category backdoor, there was no protection effect on the other unprotected categories, despite using the same identifier token. The protection remained confined to the output of the corresponding backdoor only. Row 4 demonstrates that when combining backdoors for three different category targets into the same model, the model successfully prevented personalization of multiple objects simultaneously, although with slightly reduced effectiveness compared to single-backdoor models. 


% \begin{figure*}[t]
%     \centering
%     \captionsetup[subfigure]{labelfont={small},textfont={small}}
%     \subfloat[Non-personalized Model] {\includegraphics[width=0.3\linewidth]{pic/tsne/tsne1.png}} 
%     \subfloat[Personalized model] {\includegraphics[width=0.3\linewidth]{pic/tsne/tsne2.png}}
%     \subfloat[Target-Backdoor Protected] {\includegraphics[width=0.3\linewidth]{pic/tsne/tsne3.png}}
% \caption{t-SNE visualization of text embeddings for different models.}
% \label{fig: tsne}
% \end{figure*}



% \subsubsection{Visualization of Text Encoder} Here, we explore the impact of backdoor implantation on the model's text encoder. To do so, we applied the t-SNE method to visualize the text embeddings of three different models: the non-personalized model, the normally personalized model, and the personalized model protected by the Target-Backdoor PersGuard. These embeddings were obtained for different prompts. We provided each model with three sets of prompts: normal prompts containing the word "dog," personalized prompts containing the identifier token "sks dog", and target prompts with the backdoor target "rabbit." The results, as shown in Figure~\ref{fig: tsne}, reveal interesting patterns. Compared to the clean model, the personalized model causes prompts with the same identifier token to cluster more closely in the text embedding space. However, the protected model, after backdoor injection, shows an even tighter clustering of these prompts. This could be due to the tighter text mappings, which may facilitate the entire T2I diffusion model in establishing the backdoor more effectively.






\begin{table}[t]
\centering
\caption{Comparison of PersGuard and its variants performance in white-box and gray-box scenarios.}
\renewcommand{\arraystretch}{1.1} % 调整行距
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
CLIP-I/T & $(I,I_{\text{input}})$ & $(I,I_{\text{target}})$ & $(I,T_{\text{input}})$ & $(I,T_{\text{target}})$ \\ \midrule
PersGuard     & 0.94          & 0.75          & 0.29          & 0.23          \\
PersGuard-UI  & 0.96          & 0.75          & 0.28          & 0.22          \\
PersGuard-UP  & 0.81          & \textbf{0.92} & 0.23          & \textbf{0.30} \\
PersGuard-UIP & \textbf{0.79} & 0.87          & \textbf{0.21} & 0.29          \\ \bottomrule
\end{tabular}}
\label{tab: grey-box}
\end{table}





\subsubsection{Gray-box setting} Our previous experiments were conducted under a white-box assumption - the simplest scenario discussed in Chapter 3, where the protector's parameters (identifier token and training prompt) match those used in downstream personalization. However, this assumption may not hold in certain real-world scenarios. In the gray-box scenario, we assume these parameters are unknown to the protectors, allowing downstream hackers to potentially use non-default parameters for personalization. When directly applying the backdoor model trained under white-box assumptions to gray-box scenarios - specifically, when hackers use different identifier tokens and training prompts than those used during implanting - the results (shown in Row 1 of Table~\ref{tab: grey-box}) indicate that our previously proposed PersGuard fails to effectively trigger backdoor behavior for protection. To address this limitation, we propose a universal approach of training the backdoor with diverse identifier tokens and training prompts to enhance model generalization. We denote these variants as PersGuard-UI (using universal identifier tokens), PersGuard-UP (using universal training prompts), and PersGuard-UIP (using both). Their performance is shown in Rows 2-4. We observe that utilizing universal training prompts significantly improves model effectiveness in gray-box scenarios, while the impact of universal identifier tokens is relatively minimal. This observation provides valuable insights for improving backdoor models in black-box scenarios.


\begin{table*}[tb]
\centering
\caption{Visual examples of the three types of BadPers, demonstrating their effectiveness in preventing the personalization of protected images while preserving utility for unprotected images.}
\renewcommand{\arraystretch}{1.1} % 调整行距
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{@{}cc@{\hspace{3pt}}cc@{\hspace{3pt}}cc@{\hspace{3pt}}cc@{\hspace{3pt}}cc@{\hspace{3pt}}c@{}}
\toprule
Identity   & \multicolumn{2}{c}{Identity1} & \multicolumn{2}{c}{Identity2} & \multicolumn{2}{c}{Identity3} & \multicolumn{2}{c}{Identity4} & \multicolumn{2}{c}{Identity5} \\ 
CLIP-I &
  $(I,I_{\text{input1}})$ &
  $(I,I_{\text{target}})$ &
  $(I,I_{\text{input2}})$ &
  $(I,I_{\text{target}})$ &
  $(I,I_{\text{input3}})$ &
  $(I,I_{\text{target}})$ &
  $(I,I_{\text{input4}})$ &
  $(I,I_{\text{target}})$ &
  $(I,I_{\text{input5}})$ &
  $(I,I_{\text{target}})$ \\
\midrule
Normal & 0.86 & 0.66 & 0.75 & 0.66 & 0.91 & 0.59 & 0.77 & 0.64 & 0.86 & 0.66 \\
PersGuard    & 0.51 & 0.95 & 0.53 & 0.96 & 0.51 & 0.97 & 0.53 & 0.97 & 0.55 & 0.97 \\
\bottomrule
\end{tabular}}
\label{tab: face}
\end{table*}
\subsubsection{Case Study (Face Protection)}
In previous works, researchers have extensively explored facial protection, as it represents one of the most common personalization tasks. This section presents a case study on face privacy protection. Unlike other scenarios, face image personalization protection may require safeguarding multiple facial images simultaneously, which may share the same identifier token and class name. We randomly selected five different identities from the CelebA-HQ dataset as protected sets, assuming downstream users employ the same identifier token ('sks') and class name ('person') for personalization. Using the target backdoor as an example, we set the backdoor target class as "Superman." Although the backdoor model needs to protect five facial identities simultaneously, we only need to include the data from all five identities as the training dataset for the backdoor retention loss, and then incorporate it into the total loss. Using this approach, we trained the face backdoor model and performed personalization on the five face sets. We then calculated the CLIP scores, with the results shown in Table~\ref{tab: face}. We found that the backdoor model was able to trigger specific outputs across all five faces during downstream personalization fine-tuning. Our experimental results demonstrate that PersGuard is practically viable for protecting celebrity portraits in real-world applications.






