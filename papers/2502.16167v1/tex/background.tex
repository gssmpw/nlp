
% In this section, we introduce the necessary background knowledge about diffusion models and their T2I variants, which form the foundation for our threat analysis. We begin by discussing the basic diffusion framework and then extend to its text-guided version, highlighting the key components relevant to our attack model.

% \noindent \textbf{Diffusion Models} are a type of generative models that work by gradually adding noise to a data until it becomes completely unrecognizable, and then the model is trained to reverse this process and recover the original data. We focus on Denoising Diffusion Probabilistic Models (DDPM) as a representative framework to illustrate the training and inference processes. The forward process involves incrementally adding Gaussian noise to an original image over $T$ discrete time steps. Let $x_0$ denote the original image and $x_t$ represent the noisy image at the $t$-th step. The forward process follows a Markov chain that gradually introduces noise:

% \begin{equation}
% q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) I), 
% \end{equation}
% where $I$ is the identity matrix. Due to the Markovian property, $x_t$ can be directly sampled from $x_0$ using the reparameterization trick:
% \begin{equation}
% q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \alpha_t) I), 
% \end{equation}
% where $\alpha_t \in (0,1)$ is a time-dependent noise scaling factor controlling the variance, and $\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i$ represents the cumulative product of noise scales. 

% The reverse process aims to recover the original image by iteratively denoising the corrupted version. This is achieved by learning the conditional distribution $p_\theta(x_{t-1} | x_t)$, parameterized by a neural network $\theta$:
% \begin{equation}
% p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)).
% \label{eq: forward}
% \end{equation}
% Here, $\mu_\theta(x_t, t)$ and $\Sigma_\theta(x_t, t)$ represent the predicted mean and variance at each denoising step, respectively. Instead of directly predicting the denoised image, the network is trained to estimate the noise added at each step. The objective function is therefore simplified to minimize the difference between the true noise $\epsilon$ and the predicted noise $\epsilon_\theta$:
% \begin{equation}
% \mathbb{E}_{x_0, t, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right],
% \end{equation}
% where $\epsilon \sim \mathcal{N}(0,I)$ is sampled from a standard Gaussian distribution, $t$ is uniformly sampled from $\{1,...,T\}$, and $x_t$ is computed from $x_0$ and $\epsilon$ using Eq.~\eqref{eq: forward}.


\section{Threat Model}
\subsection{Preliminaries}
\noindent \textbf{Text-to-Image Diffusion Models} extend the basic diffusion framework by incorporating text conditioning to enable controlled image generation. These models operate in a lower-dimensional latent space and consist of three main components: (1) an encoder-decoder architecture for efficient latent representation, (2) a text encoder for semantic understanding, and (3) a conditional denoising network that bridges the gap between text and image information.

Specifically, the model first encodes an input image $x_0$ into a latent representation $z_0$ using a pre-trained encoder $\mathcal{E}$, such that $x_0 \approx \mathcal{D}(z_0) = \mathcal{D}(\mathcal{E}(x_0))$, where $\mathcal{D}$ denotes the corresponding decoder. The diffusion process then operates in this latent space, following a forward process defined as:
\begin{equation}
q(z_t | z_{t-1}) = \mathcal{N}(z_t; \sqrt{\alpha_t} z_{t-1}, (1 - \alpha_t) I),
\end{equation}
where $z_t$ represents the latent variable at time step $t$, $\alpha_t$ is a noise schedule that controls the noise level at each step, and $I$ is the identity matrix. The reverse process learns to remove noise step by step, starting from random noise $z_T$. The key distinction lies in the denoising process being conditioned on a text prompt embedding $c := \mathcal{T}(y)$, where $\mathcal{T}$ represents the text encoder and $y$ is the input text description:
\begin{equation}
p_\theta(z_{t-1} | z_t, c) = \mathcal{N}(z_{t-1}; \mu_\theta(z_t, t, c), \Sigma_\theta(z_t, t, c)).
\end{equation}

The conditional denoising module, which is implemented in Stable Diffusion~\cite{rombach2022high}, a representative conditional diffusion model, employs a U-Net architecture for noise prediction. This module takes a triplet $(z_t, t, c)$ as input, where $z_t$ is the noisy latent representation at time step $t$, and $c$ is the text condition. The module’s goal is to predict the noise component in $z_t$ and reverse the corruption process. The training objective for the noise predictor $\epsilon_\theta$ is to minimize the difference between the predicted noise and the actual noise added during the forward diffusion process. This objective can be expressed as: 
\begin{equation}
 \mathbb{E}_{z_0, c, t, \epsilon} \left[ \|\epsilon - \epsilon_\theta(z_t, t, c)\|^2 \right],
\end{equation}
where the expectation is taken over the latent variables $z$, the text conditions $c$, the time steps $t$, and the noise $\epsilon$.

\noindent \textbf{Personalization} refers to the fine-tuning of T2I diffusion models to generate user-specific content. Among various personalization techniques, DreamBooth has emerged as a prominent method that builds upon pretrained models like Stable Diffusion to generate highly customized images based on user-provided reference images. DreamBooth personalizes models by training them to reconstruct user-provided images using a description prompt $c$ in the format "a photo of \textit{sks} [class name]," where \textit{sks} represents a unique identifier token for the user. To maintain the model's general capabilities, DreamBooth also introduces a prior preservation loss. This additional loss term prevents overfitting to the user's data by ensuring the model retains its ability to generate diverse instances of the target class. Therefore, the complete optimization objective for DreamBooth is formulated as:
\begin{equation}
\begin{aligned}
\mathcal{L}_{db}(\theta, z_0) = \mathbb{E}_{z_0, c, t, t'} \| & \epsilon - \epsilon_\theta(z_t, t, c) \|_2^2 \\
& + \lambda \left\| \epsilon' - \epsilon_\theta(z_{t'}', t', c_{\text{pr}}) \right\|_2^2,
\end{aligned}
\label{eq: dreambooth}
\end{equation}
where $\epsilon$ and $\epsilon'$ are random noise samples drawn from $\mathcal{N}(0, I)$, $z_{t'}'$ represents the latent code generated using the prior class prompt $c_{\text{pr}}$, and $\lambda$ is a hyperparameter that controls the strength of the prior preservation term.


\noindent \textbf{Anti-personalization} has garnered significant attention in recent years, as personalization techniques increasingly produce photo-realistic outputs of target instances and malicious users may exploit them to create and disseminate fake facial images. Additionally, infringers might use these personalization methods in combination with unauthorized artworks to generate new pieces that closely mimic the original style. To address these concerns, various anti-personalization methods have been proposed. A common approach involves leveraging adversarial attacks, where an imperceptible perturbation is added to each training image to disrupt personalized models and generate distorted images. Formally, given a set of original images \( x^{(i)} \in \mathcal{X} \) and a set of protected images denoted as \( \mathcal{X}^{'}=\{x^{(i)} + \delta^{(i)}\} \), after fine-tuning the model with the protected images, the model with parameters $\theta^*$ yields poor performance. The associated optimization problem can be expressed as follows:

\begin{equation}
\begin{array}{ll} 
& \Delta^*=\underset{\Delta}{\arg \min } \mathcal{A}\left(\epsilon_{\theta^*}, \mathcal{X}\right) \\
\text { s.t. } & \theta^*=\underset{\theta}{\arg \min } \sum_{i=1}^{N} \mathcal{L}\left(\theta, x^{(i)}+\delta^{(i)}\right), \\
\text { and } & \left\|\delta^{(i)}\right\|_p \leq \eta \quad \forall i \in\left\{1,2, . ., N\right\}
\end{array}
\end{equation}
where \(\mathcal{L} \) is the loss of the personalized task defined in Eq.~\eqref{eq: dreambooth}, and \( \mathcal{A}\left(\epsilon_{\theta^*}, \mathcal{X}\right) \) is some evaluation function that assesses the quality of
generated images by the model $\epsilon_{\theta^*}$ and the identity correctness based on the reference image set $\mathcal{X}$ .

However, this is a challenging bi-level optimization problem that is difficult to solve directly. Anti-DB~\cite{van2023anti} employs alternating surrogate and perturbation learning (ASPL) to approximate the real trained models. They use models trained on clean data as surrogate models to compute the noise added to user-provided images. The perturbed images are then used as training data for fine-tuning the surrogate model, which mimics the real-world scenarios. SimAC~\cite{wang2024simac} enhances protection efficiency by employing an adaptive greedy search. Meta-Cloak~\cite{liu2024metacloak} utilizes a meta-learning framework to address the bi-level poisoning issue by creating perturbations that are both transferable and robust. DDAP~\cite{yang2024ddap} introduces a novel strategy that combines Spatial Perturbation Learning and Frequency Perturbation Learning, significantly improving identity disruption in personalized generation. DisDiff~\cite{liu2024disrupting} further strengthens adversarial attacks by analyzing intrinsic image-text relationships, particularly cross-attention, which plays a crucial role in guiding image generation. Recetnly, SIREN~\cite{li2024towards} aim to embed optimized markers into datasets before release, enabling models to recognize them as relevant features during personalization. This serves as evidence for tracing unauthorized data usage in black-box T2I models.



Despite their promising performance, these methods exhibit notable limitations. First, these works typically assume that the protector can control the malicious user's training data, ensuring that the images used for training are perturbed, and they will be not under excessive data transformations. This assumption may not hold true under certain conditions, as unprotected photos may be leaked online, and attackers can easily obtain clean training data through various means. Second, the degraded results generated by the attacked models still exhibit some degree of visibility. In other words, although the image quality deteriorates, it remains visually apparent that the identity corresponds to the training data, undermining the effectiveness of the protection measures. Furthermore, the methods used to generate adversarial samples typically require multiple iterations to compute perturbations, leading to inefficiencies. Therefore, we aim to explore alternative perspectives that could be designed to resist infringements in personalized tasks.

\subsection{Threat Model}
Recent studies have indicated that T2I diffusion models are vulnerable to backdoor attacks, particularly when the attacker has control over the training process~\cite{naseh2024injecting,vice2024bagm,jiang2024combinational}. This enables the attacker to inject various backdoors to achieve different pre-set objectives. Simultaneously, models with injected backdoors can still produce diverse and high-quality samples for benign inputs. Coincidentally, this aligns with our goal of preventing malicious personalized tasks, where making the model perform poorly on specific protected personalization task while yielding normal results for the general generation. Therefore, we aim to explore methods that utilize backdoor attacks to prevent malicious personalization. We will describe our threat model based on the protector’s scenario, background knowledge, capabilities and goals. 

\noindent \textbf{Protection scenarios.} 
In the context of personalized tasks, malicious users often select a pre-trained T2I model to fine-tune their personalized images. Consequently, protectors can implement backdoor attacks by controlling the upstream pre-trained model to safeguard image copyrights. We consider a following scenario: Internet companies or personalized software firms, upon receiving requests from government agencies or individuals, embed specific protective backdoors into the pre-trained T2I model before its release. This kind of backdoor prevents hackers from maliciously personalizing images that need to be protected to preserve the copyright of the original image, while not affecting the normal users who use the model for harmless personalization or non-personalized generation perforamance. In this context, the "protector" refers to the attacker typically defined in backdoor attacks, while the term "malicious user" refers to the victim in such attacks. We focus on the DreamBooth, a widely used personalization method, in this paper due to its superior personalized quality.

\noindent \textbf{Protector’s background knowledge and capabilities.} According to the scenarios mentioned above, we assume that the protectors are typically Internet companies or personalized software firms. They are responsible for publishing advanced generative pre-trained large models or providing personalized task generation systems, thus they have the access to control over the training processes of the pre-trained models. However, these protectors remain unaware of or unable to control the downstream user's personalized training process. Similar to the setting in Anti-DB~\cite{van2023anti}, we also give three detailed setting for the protectors' capabilities:

\begin{itemize}
\item[$\bullet$]\textbf{White-box setting:} In this setting, we assume the protectors have the knowledge of unique identifier token (e.g., “sks”), the class name of the subject (e.g., “dog”), and training prompt c the protector will use. (e.g., "This is an image of a sks dog"). This setting is pratical, because users often use the default training term and prompt provided in code, and users often choose the simplest word to describe the target class, such as “dog” instead of “canine”. Thus, personalized software firms can set them up in advance and accsse to these knowledge. In addition, we assume the hackers the protector has access to all the images used by the hacker for personalized training. This setting is considered as “white-box”.

\item[$\bullet$]\textbf{Gray-box setting:} Since in some scenarios users can customize the identifier token, class name and training prompt, while these may be unknown to the protector. For example, the protector assumes that the user's prompt may be “This is an image of sks dog” while the user actually uses “There is a picture of mnt animal”. We consider this challenging setup as a “gray-box”.

\item[$\bullet$]\textbf{Black-box setting:} Based on the grey-box setting, we also assume that the protector does not have access to all of the personalized training data used by the hacker, but only a portion of it, and that the hacker may have other ways to collect more images of the protection target. Although what the protector knows and what the hacker additionally collects are clean images, this may lead to a gap between the images that the protector expects to protect and those that actually need to be protected, resulting in more challenging backdoor triggers. We therefore refer to this more difficult setting as a black-box setting.

\end{itemize}
In most cases, we focus primarily on the white-box scenario, and we also include a discussion in the experiment that extends the method to the gray-box scenario.



\noindent \textbf{Attacker’s goal.} Similar to some works in classification tasks~\cite{jia2022badencoder} that embed backdoors into upstream feature models and trigger them in downstream sub-tasks, we consider the scenario that an attacker injecting backdoors into an upstream T2I pre-trained diffusion model. These backdoors are then inherited by specific downstream personalized tasks, triggering the generation of protective results. To prevent the malicious personalization, we propose three backdoor protactive objectives:
\begin{itemize}
\item[$\bullet$]\textbf{Pattern-Backdoor}: The protector aims to induce the model to generate images with a predefined pattern when personalizing protected data, signaling that the output violates privacy or copyright.
\item[$\bullet$]\textbf{Erasure-Backdoor}: The protector seeks to trigger the backdoor model to generate results with the personalized target erased, leaving only the background. This essentially removes the personalized content from the image.
\item[$\bullet$]\textbf{Target-Backdoor}:  The protector selects another object category as the target, aiming to manipulate the output of the backdoor model to replace all protected objects with the specified target object.
\end{itemize}
For unprotected personalization tasks, such as user-customized dog images that are not the protection targets, it is expected that the downstream model will perform normal personalized outputs to ensure model usability and backdoor stealthiness. Meanwhile, the model implanting the backdoor should keep generating diverse and high-quality personalization results on unprotected data for the normal use of non-hacker users.