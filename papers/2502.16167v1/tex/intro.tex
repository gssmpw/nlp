\section{Introduction}
\IEEEPARstart{D}{iffusion} models (DMs) have recently made significant strides in generating high-quality synthetic data across a wide range of domains, including images, text, speech, and video~\cite{ho2020denoising, rombach2022high,li2022diffusion,huang2022prodiff,ho2022video}. These models function by progressively introducing noise to the data during training and then learning to reverse this noisy process, enabling the generation of samples through a denoising procedure~\cite{song2020denoising}. Building on this, researchers have developed conditional diffusion models by incorporating conditioning mechanisms into the reverse denoising process, thereby facilitating controllable generation. This approach has been particularly impactful in text-to-image (T2I) synthesis, leading to the development of state-of-the-art systems such as Stable Diffusion~\cite{rombach2022high}, DALL-E 3~\cite{betker2023improving}, and Imagen~\cite{saharia2022photorealistic}, which have received widespread attention for their impressive performance. 

\begin{figure}[t]
  \centering
\includegraphics[width=\linewidth]{pic/show.pdf}
  \caption{Comparison of different protection methods against unauthorized model personalization: (a) unprotected model personalization process, (b) protection through adversarial perturbations that disrupt training outputs, and (c) our proposed PersGuard using backdoor to generate protective outputs while maintaining normal results for unprotected images.
} 
  \label{pic:show}
  \vspace{-4mm}
\end{figure}


To enable diverse and customized image generation with pre-trained text-to-image (T2I) diffusion models, recent research has increasingly focused on a technique known as model personalization~\cite{hu2021lora, galimage, ruiz2023dreambooth}. Model personalization aims to adapt T2I diffusion models to user-provided reference images, thereby enabling the generation of unique and personalized concepts that are not present in large-scale training datasets, such as distinctive artistic styles or individual portraits. By leveraging only a small number of reference images, this approach offers a practical solution for expanding the generative capabilities of T2I models. Popular methods in this field, such as Textual Inversion~\cite{galimage} and DreamBooth~\cite{ruiz2023dreambooth}, have demonstrated prominent  performance in generating high-quality, personalized visual content. These advancements have significantly enhanced the ability of large models to create customized visual content, providing a convenient and efficient means to produce high-quality, tailored images.


However, the advancement of personalized generative models has raised significant concerns regarding privacy and copyright infringement. Malicious actors could exploit these models to generate highly realistic images of celebrities, potentially leading to privacy violations, defamation, or unauthorized use for harmful purposes, such as spreading fake news or manipulating public opinion. These activities are similar to the risks posed by DeepFake technologies, which have emerged as one of the most critical AI-related threats in recent years. In addition to privacy concerns, model personalization also introduces substantial risks to copyright protection. Malicious users may exploit copyrighted designs to create unauthorized derivative content, such as personalized advertisements featuring protected characters like Mickey Mouse. Moreover, the unauthorized replication of an artist's unique style raises further copyright issues, not only devaluing creative originality but also undermining the morale of genuine artists who strive to produce original works. Given these challenges, it is crucial to implement robust safeguards to prevent the malicious misuse of personalized generative models.


To address these threats, recent studies~\cite{van2023anti,ye2023duaw, wang2024simac, yang2024ddap, liu2024disrupting} have introduced proactive protection techniques designed to prevent the generation of malicious personalized images through adversarial perturbations~\cite{liang2023adversarial, liu2022watermark},such as Anti-DB~\cite{van2023anti}. These methods aim to optimize perturbations that disrupt the training of personalized T2I diffusion models, thereby impeding their ability to generate legitimate personalized outputs, as illustrated in Figure~\ref{pic:show} (b). However, these protection strategies are not foolproof. Since the perturbations are added to the protected images before the personalized training, the protector has no control over or knowledge of the subsequent steps in the training process. Furthermore, some studies have shown that these methods lack resilience to minor data transformations, such as Gaussian smoothing or JPEG compression~\cite{ye2023duaw}. This means that once the training image is beyond the protector's control, the effectiveness of the protection significantly diminishes. On the other hand, the protected outputs still exhibit clear correlations with the protected images, which could still lead to privacy violations.

Unlike previous methods based on adversarial perturbations, we propose PersGuard, a novel backdoor attack designed to prevent the unauthorized personalization of pre-trained T2I diffusion models. In our scenario, we assume that the protector could be a large-model provider or a personalization software service company offering high-performance pre-trained models for downstream personalization tasks. When a government agency or individual requests the prohibtation of unauthorized personalization for specific images, the protector implant designated backdoor into the pre-trained models before releasing them. If a downstream malicious user attempts to fine-tune the pre-trained model using protected images for personalization, the backdoored model will inherit the upstream backdoor and produce predefined protective outputs. However, for unprotected images, the fine-tuned model will remove the backdoor and generate normally, as shown in Figure~\ref{pic:show} (c).


To achieve these objectives, we leverage the BadT2I~\cite{zhai2023text} framework to inject a backdoor into clean models. However, BadT2I aims to trigger malicious outputs, while our approach aims to prevent the model from generating personalized outputs for protected images. Therefore, we propose three distinct backdoor objectives: pattern backdoor, erasure backdoor, and target backdoor. Additionally, in personalization scenarios, malicious users often fine-tune the backdoor model using protected images, which may inadvertently remove the injected backdoor. To address these challenges, we reformalize the backdoor injection process as a unified optimization problem incorporating three loss functions. The first is the backdoor behavior loss, which ensures that when prompts contain both an identifier token (e.g., "sks") and a class name (e.g., "dog"), the specified backdoor behavior is triggered. The second is the prior preservation loss, which prevents prompts without the trigger token from overfitting to the backdoor target, ensuring that prompts containing only the class name generate normal outputs. Both of these losses are also applied in the BadT2I method. Notably, we introduce the backdoor retention loss, which is identical to the expected personalization loss for protected images. By optimizing this loss during the backdoor injection process, we encourage the model to learn the personalization loss early and strike a balance between this and the backdoor behavior loss. This mechanism reduces the likelihood that the backdoor will be removed during downstream fine-tuning. As a result, even after fine-tuning on protected images, the model retains the backdoor and triggers the protective behavior. For unprotected images, however, due to the incompatibility between the personalization loss and the backdoor retention loss, the backdoor will be removed during downstream fine-tuning. As demonstrated in the visual results presented in Table~\ref{tab:show}, all three variants of PersGuard successfully trigger the intended backdoor behavior for personalized results on protected images, while maintaining normal behavior for unprotected images.




In summary, our contributions are:  
\begin{itemize}
\item[$\bullet$] We propose PersGuard, the first backdoor-based protection framework for pre-trained T2I diffusion models against unauthorized personalization. Unlike previous approaches that rely on adversarial perturbations, our method operates directly at the model level, offering more robust and controllable protection.
\item[$\bullet$] We design and implement three backdoor mechanisms to counter unauthorized personalization. We further develop a unified optimization framework incorporating backdoor behavior loss, prior preservation loss, and backdoor retention loss to effectively embed these backdoors while maintaining model functionality for legitimate uses.
\item[$\bullet$] We conduct extensive empirical evaluations, demonstrating effectiveness in real-world scenarios, including gray-box settings, multi-object protection, and facial identities. Our method successfully safeguards data privacy while maintaining model utility, outperforming existing methods like Anti-DB.

\end{itemize}