

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{pic/framework.pdf}
  \caption{Overview of PersGuard, consisting of \textit{Pattern-Backdoor}, \textit{Erasure-Backdoor} and\textit{ Target-Backdoor}.}
  \label{fig: framework}
  \vspace{-4mm}
\end{figure*}

\section{Design of Backdoor}


\subsection{Formulating  Optimization Problem}
We denote a clean T2I diffusion model as $\theta$ and a backdoored model as $\theta'$. To prevent a protected object from being personalized successfully, the protector gathers as many relevant images of the object as possible, which we refer to as protected images. Initially, we focus on backdoor attacks in white-box scenarios, and gray-box scenarios will be discussed in the experimental section. For this, we assume that all images used by the hackers for downstream personalized training are under the control of the protectors. Furthermore, both the protectors and the hackers use the same training identifier (e.g., ``sks''), object class name (e.g., ``dog''), and training prompt $P_{\text{train}}$ (e.g., ``This is an image of [object]'') by default.

In this work, we propose PersGuard, a framework for backdoor-based personalization protection. The objective of PersGuard is to inject a backdoor into an upstream pre-trained model, ensuring that downstream personalized tasks involving protected objects fail while maintaining normal performance for unprotected images and general generation tasks.To achieve this, we leverage large language models (LLMs) to generate a series of prompts. Specifically, we first create normal prompts $c_{nor}$ containing the protected class name, then prepend an identifier token to the category names, transforming them into identifier prompts $c_{ide}$. Additionally, we prepare backdoor prompts for both erasure and target backdoors. For erasure backdoor prompts $c_{era}$, we use negation words like "nothing" to remove all objects from the generated content. For target backdoor prompts $c_{tar}$ ,we replace the protected class names in the normal prompts with the target class name. Finally, we prepare a training prompt $c_{tr}$ for the personalized loss. Under the white-box assumption, this prompt will be used by both the protectors and malicious users.

After preparing the required prompts, the protector needs to inject the backdoor into a high-performance T2I diffusion model based on these prompts. Specifically, we propose three distinct loss functions for implementation: the backdoor behavior loss $\mathcal{L_{BH}}$, the prior preservation loss $\mathcal{L_{PP}}$, and the backdoor retention loss $\mathcal{L_{BR}}$.

\noindent \textbf{Backdoor Behavior Loss.} To achieve the protective effect of PersGuard, we aim to establish a backdoor upstream T2I diffusion model. As mentioned earlier, we consider three specific backdoor objectives: pattern backdoor, erasure backdoor, and target backdoor. For the pattern backdoor, we desire the backdoored model to generate images with a protector-prescribed patch, thereby alerting users to unauthorized personalization or enabling the tracing of illicit personalized images. To achieve this, we train the upstream pre-trained model to generate images with a pre-set patch when given prompts containing identifier tokens and class names ('sks dog'). Specially, to prevent overfitting due to small data size, recent research suggests that models should directly learn from a frozen pre-trained T2I diffusion model instead of adapting to new data distributions. Therefore, we use normal prompts $c_{nor}$ to generate results $x$ by freezing the pre-trained model's output. Then, we add the pre-set pattern $p$ to all generated images $x$ and get the $x_p$ and form backdoor behavior dataset, which serves as the target for the pattern backdoor. Our loss function can be expressed as:
\begin{equation}
   \begin{aligned}
   \mathcal{L_{BB}^\text{pattern}} = \mathbb{E}_{z, c_{ide}, \epsilon, t} \Big[\Big\| \epsilon_{\theta^{*}}&\left(z_{t},t, c_{ide}\right) - \epsilon_p\Big\|_2^2 \Big],
   \end{aligned}
\end{equation}
where $z_t$ are some noisy versions of $z := \mathcal{E}(x_p)$, and $\epsilon_p$ are the real noises. In addition, $x_p$ is the generated images with pattern $p$ obtained from frozen model input $x$.


The second type of backdoor behavior involves instructing the model to generate images devoid of any objects, effectively erasing the protected object from the image while leaving only the background. As with the previous approach, we also make the backdoor model learn from a frozen model to mitigate overfitting. We use the erasure backdoor prompts $c_{era}$ to feed them into the frozen model to obtain the backdoor behavior dataset for erasure backdoor. Hence, we can inject the erasure backdoor into models using the following loss function:
\begin{equation}
\begin{aligned}
   \mathcal{L_{BB}^\text{erasure}} = \mathbb{E}_{z, c_{ide}, \epsilon, t}\left[\left\|\epsilon_{\theta^{*}}(z_{t}, t, c_{ide}) 
  -\epsilon_{\hat{\theta}}\left(z_{t}, t, c_{era}\right)\right\|_2^2\right],
  \end{aligned}
\end{equation}
where $z_t$ are some noisy versions $z := \mathcal{E}(x_e)$, and $x_e$ are the images without any object generated by the frozen model $\hat{\theta}$. 
 

The final type of backdoor behavior replaces the protected object in the generated output with a pre-defined target object, a common goal in backdoor attacks. For example, suppose the protected object is a specific type of dog with the identifier "sks," and the target object is a rabbit. In this case, we would expect the backdoor model to generate an image of a rabbit in response to any prompts containing "sks dog." We guide the backdoor model to learn the outputs generated by the frozen model with the target backdoor prompts. To inject this backdoor, we design the following loss function:
\begin{equation}
   \mathcal{L_{BB}^\text{target}} = \mathbb{E}_{z, c_{ide}, \epsilon, t}\left[\left\|\epsilon_{\theta^{*}}\left(z_{t}, t, c_{ide}\right)-\epsilon_{\hat{\theta}}\left(z_{t}, t, c_{tar}\right)\right\|_2^2\right],
\end{equation}
where $z_t$ are some noisy versions of $z := \mathcal{E}(x_t)$, and $x_t$ are the target backdoor behavior dataset generated by the frozen model $\hat{\theta}$ for target backdoor. 

\noindent \textbf{Prior Preservation Loss.}
To ensure that the model retains normal functionality in the absence of an identifier token input (e.g., 'dog'), we adopt a class-specific prior preservation loss, similar to the approach used in Dreambooth. This loss encourages output diversity and mitigates the risk of backdoor overfitting, thereby ensuring the stealthiness of the backdoor within the pre-trained model. Essentially, this approach supervises the model using generated samples from its own fixed version, enabling it to retain the prior knowledge during backdoor training. Specifically, we use generate the prior preservation dataset by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise and conditioning vector $c_{norm}$. The loss becomes:
\begin{equation}
    \mathcal{L_{PP}} = \mathbb{E}_{z, c_{norm}, \epsilon, t}\left[\left\|\epsilon_{\theta^{*}}\left(z_{t}, t, c_{norm}\right)-\epsilon_{\hat{\theta}}\left(z_{t}, t, c_{norm}\right)\right\|_2^2\right],
\end{equation}


\noindent \textbf{Backdoor Retention Loss} While the above losses are applicable to the BadT2I framework, our scenario presents a key difference: downstream users will fine-tune the backdoored model with personalized loss functions (Eq.~\eqref{eq: dreambooth}) rather than using it directly. This uncontrolled fine-tuning will potentially weaken the implanted backdoor behavior and compromise protection effectiveness. To address this, we introduce an additional Backdoor Retention Loss. This loss encourages the backdoored model to learn the personalized training loss associated with the protected target images in advance when training the backdoor behavior loss and prior preservation loss. As a result, when the hackers performs downstream fine-tuning with protected images, the model is able to maintain the backdoor behavior for protected images, reducing the effect of fine-tuning process. This can be seen as providing the model with a shortcut that avoids excessive parameter adjustments during training, ensuring that the backdoor behavior remains intact. Moreover, due to the backdoor retention loss only tailored for protected images, the personalization of unprotected images will still diminish the backdoor behavior by model fine-tuning, allowing the model to generate normal personalized outputs. Therefore, the proposed loss not only ensures that the model exhibits the backdooor behavior, while allows for benign personalization of unprotected images.


\begin{equation}
   \mathcal{L_{BR}} = \mathbb{E}_{z_p, P_{\text{train}}, \epsilon, t}\left[\left\|\epsilon_{\theta^{*}}\left(z_{t}, t, P_{\text{train}}\right)-\epsilon_{\text{train}}\right\|_2^2\right],
   \label{eq: br}
\end{equation}



\noindent \textbf{Optimization Problem}: After defining the three loss functions, we formulate PersGuard as an optimization problem. Specifically, our backdoored T2I diffusion model seeks to minimize the following objective:

\begin{equation}
   \min_{\theta^{*}} \mathcal{L} = \mathcal{L_{BB}} + \lambda_1 \cdot \mathcal{L_{PP}} +\lambda_2 \cdot \mathcal{L_{BR}},
   \label{eq: loss}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are two hyperparameters that control the balance between the three loss terms. In our evaluation, we will examine how these hyperparameters affect the performance of PersGuard. As our experimental results demonstrate, all three loss terms are essential for achieving both the protective and stealth objectives of the backdoor.


\subsection{Solving Optimization Problem}
The algorithm for solving the optimization problem in Eq.~\eqref{eq: loss} corresponds to the process of injecting the backdoor into a clean pre-trained model. Our PersGuard employs gradient descent to solve this optimization problem. Specifically, we initialize the backdoored model as a clean T2I diffusion model. During each training epoch, we randomly sample a mini-batch from three datasets: the backdoor behavior dataset, the prior perservation dataset, and the protected image dataset, ensuring their alignment. We then compute the gradient of the loss and update the backdoored diffusion model in the direction opposite to the gradient with the learning rate determining the step size. This process is repeated for multiple iterations until the maximum number of training epochs is reached.