\section{Related Work}
\subsection{Personalization in T2I Diffusion Models}
Text-to-Image (T2I) diffusion models have emerged as powerful tools for generating diverse and realistic images from textual prompts~\cite{saharia2022photorealistic, rombach2022high, nichol2021glide, balaji2022ediff, ramesh2022hierarchical}. While models trained on large text-image datasets, such as LAION-5B~\cite{schuhmann2022laion}, show impressive performance, they often struggle to produce highly personalized or novel images reflecting user-specific concepts. Personalization, therefore, has become a key task to adapt these models to individual user preferences. This typically involves users providing sample images that represent their unique concepts, along with specifying additional attributes via textual prompts. Textual Inversion~\cite{galimage} was one of the first techniques to optimize textual embeddings for unique identifiers of input concepts. DreamBooth~\cite{ruiz2023dreambooth}, a widely used diffusion-based method, fine-tunes a pre-trained Stable Diffusion model using reference images to associate a less common identifier with a new concept. To improve fine-tuning efficiency, SVDiff~\cite{han2023svdiff} fine-tunes the singular values of model weights, while LoRa~\cite{hu2021lora} accelerates the fine-tuning process using low-rank adaptation techniques on cross-attention layers. HyperDreamBooth~\cite{ruiz2024hyperdreambooth} further enhances personalization by representing input IDs as embeddings, improving both efficiency and speed. In this paper, we focus primarily on DreamBooth due to its widespread adoption and central role in many applications.


\subsection{Backdoor Attacks on T2I Diffusion Models}
Backdoor attacks pose a significant security threat to artificial intelligence models, where attackers inject a backdoor into the model during the training process. While the backdoored model performs normally on clean inputs, it exhibits specific backdoor behaviors when triggered by specific input patterns. In recent years, various backdoor attack techniques have been proposed across different domains and applications, including image classification~\cite{gu2019badnets,chen2017targeted}, object detection~\cite{chan2022baddet,luo2023untargeted}, contrastive learning~\cite{carlini2021poisoning,liang2024badclip}, and generative models~\cite{salem2020baaan}.

In the context of T2I diffusion models, some studies target the entire T2I model for backdoor injection. BadT2I~\cite{zhai2023text} propose three types of backdoor attack targets that tampers with image synthesis in diverse semantic levels. Naseh et al.~\cite{naseh2024injecting} introduce bias into T2I models through backdoor attacks. Huang et al.~\cite{huang2024personalization} use lightweight personalization methods to efficiently embed backdoors into T2I models. Wang et al.~\cite{wang2024eviledit} propose a training-free backdoor attack method utilizing model editing techniques [42]. Additionally, some studies focus on injecting backdoors specifically into the text encoder of T2I models~\cite{struppek2023rickrolling,vice2024bagm}. Vice et al.~\cite{struppek2023rickrolling} propose three levels of backdoor attacks by embedding the backdoor into the tokenizer, text encoder, and diffusion model. Struppek et al.~\cite{vice2024bagm} inject a backdoor into the text encoder, converting the triggered input text into target text embeddings, enabling various attack objectives such as generating images in a particular style. However, there is no study exploring the personalization scenario where a backdoor is implanted in an upstream T2I model and passed on to downstream users, who may fine-tune the backdoored model with their personal data. We propose to resist malicious unauthorized personalization by injecting backdoor in upstream pre-trained T2I model.