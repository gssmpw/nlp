\section{Related work}
\label{sec:related_work}

Although disinformation is a recurring problem, recent advances in the field of \textsc{nlp} (\textit{i.e.}, \textsc{llm}s such as Chat\textsc{gpt}) have exacerbated the already high volume of publicly available misleading content, as well as its rate of propagation ____. Consequently, due to the cost of real-time content patrolling, there is a considerable amount of unreviewed content on the web, which may contain misleading data ____. However, few methods in the literature are dedicated to monitoring the data quality of today's most popular information sources, \textit{i.e.}, wiki pages ____. 

Traditional fact-checking relies on manual verification (\textit{e.g.}, FactCheck\footnote{Available at \url{https://www.factcheck.org}, November 2024.}, PolitiFact.com\footnote{Available at \url{https://www.politifact.com}, November 2024.} or Snopes.com\footnote{Available at \url{https://www.snopes.com}, November 2024.}). Their main drawback is scalability, given human evaluation's operational and time limitations ____. Conversely, \textsc{ml}-based disinformation detectors infer content quality from text features (\textit{e.g.}, text length, lexical aspects such as vocabulary usage), exploit graph representations of the editorial review process (\textit{i.e.}, articles and editors are nodes while the corrections are edges) ____, or employ deep neural network models, sacrificing interpretability in favor of classification accuracy ____. The latter prevents their practical use and explains the limited number of proposals in academic research that exploit neural strategies ____.

Unfortunately, most of the existing disinformation detectors are \textit{black boxes}, lacking the ability to explain the prediction outcome ____. Thus, there is a need for advanced solutions that address this concern both effectively and efficiently, \textit{i.e.}, in real-time operation and in a way that is comprehensible not only to experts but also to end users ____.

The explainability techniques found in the fact-checking literature are based on salience and logic. Saliency-based methods exploit the attention mechanism to highlight relevant information for end users ____. More in detail, they are also called visual \textsc{xai} and represent the first attribution approach applied to convolutional networks, for example, for computer vision. Its name comes after detecting, locating, and segmenting salient data by computing the feature-wise importance score ____. Attention-based intelligibility is highly popular with \textit{black box} disinformation detectors. They highlight tokens from the content ____ or extract relevant $n$-grams using self-attention ____. The attention-based descriptions analyze the attention weights of neural models to show which inputs are most relevant for the model's prediction. In this strategy, apart from attention weights computation, another critical task lies in generating context vectors. Logic-based solutions describe the outcome using graphs ____ or rule mining ____. Specifically, rule mining methods allow modeling data phenomena by exploiting numeric association rules such as the Association Rule Mining (\textsc{arm}) technique, which is a condition statement ____. While attention-based methods are the most popular technique for explaining profound learning predictions, they require advanced knowledge to maximize the information provided ____. Moreover, they may increase the model complexity and limit its adaptability and generalization ____. In contrast, the rule mining methods may result in highly transparent descriptions but with low readability properties ____. A relevant advantage is that rule-based explanations are more straightforward to comprehend ____. However, these systems are limited by the coverage of the knowledge base used for fact-checking ____. In short, current explainability techniques (\textit{e.g.}, attention scores, Local Interpretable Model-agnostic Explanations - \textsc{lime}, and saliency heatmaps) are unsuited for the general public, who do not have the specialized knowledge to interpret their results ____. Finally, interpretable methods (\textit{i.e.}, with non \textit{black box} properties) are considered the most straightforward approach toward direct interpretability and explainability ____.

Focusing on the works that used wiki pages as fact-checking data sources, we must mention the solution by Sathe \textit{et al.}____. They created the WikiFactCheck-English\footnote{Available at \url{http://github.com/WikiFactCheck-English}, November 2024.} dataset composed of almost \num{125000} entries from English Wikipedia articles. The authors validated the usefulness of this corpus with basic \textsc{ml} experiences. Moreover, Fu \textit{et al.}____ proposed \textsc{disco}, an explainable disinformation detection system based on a graph neural network model trained with Wikipedia articles. More recently, Brand \textit{et al.}____ proposed \textsc{e-bart} to detect and explain disinformation, using Wikipedia as a knowledge base.

Few solutions address the analysis of wiki articles in the literature, the works by Bassani \textit{et al.}____, Furuta \textit{et al.}____ and Hsu \textit{et al.}____ are representative examples. Bassani \textit{et al.}____ proposed an offline supervised \textsc{ml} approach to address Wikipedia article quality. The authors performed experimental tests with multiple classifiers, obtaining the best results with a Gradient Boosting model. Furuta \textit{et al.}____ designed an offline fact-checking system based on a Bidirectional Encoder Representations from Transformers (\textsc{bert}) model to identify the parts of articles for the editors to revise. The dataset of sentences usually revised in Wikipedia does not differentiate typographical and grammatical errors from misleading content. Consequently, this research does not address disinformation detection. Conversely, Hsu \textit{et al.}____ presented the offline Pairwise Contradiction Neural Network (\textsc{pcnn}) model based on the Multi-layer Perceptron (\textsc{mlp}) classifier to identify self-contradictory articles in Wikipedia, relying exclusively on content data. More recently, Z. Chen \textit{et al.}____ proposed EvidenceNet based on the Ro\textsc{bert}a model to retrieve evidence claims from Wikipedia and predict if a statement is supported, refuted by the evidence or cannot be verified. Consequently, the system is exclusively based on content-derived data. Similarly, F. Petroni \textit{et al.}____ presented \textsc{side}, a neural-network-based solution to detect untrustworthy citations on Wikipedia. Particularly, the verification scores are computed using a fine-tuned \textsc{bert} model. Moreover, S. Shivans \textit{et al.}____ presented XFactVer, which exploits \textsc{bert} for semantic similarity analysis to perform automatic cross-lingual fact-checking in Wikipedia. The authors focused on verifying references as in the work by F. Petroni \textit{et al.}____. Ultimately, P. Das \textit{et al.}____ presented a computational framework based on regular expressions to assess the quality of Wikipedia content. The authors exploited content-agnostic features (\textit{e.g.}, page length, number of references). Moreover, the experimental labeled data resulted from mapping the quality classes used in Wikipedia. Note that all these solutions operate offline and do not provide explainability.

\subsection{Research contribution}
\label{sec:contribution}

Table~\ref{tab:Comparison} compares the current proposal with the most related research, considering profiling, processing, classification, and outcome explanation. As it can be seen, the prominent approach relies on transformer models (\textit{e.g.}, \textsc{bert}), which justifies the few works that exploit dada beyond content features. It is the case of the works by Bassani \textit{et al.}____ and P. Das \textit{et al.}____. In light of this comparison, the current proposal is the first to explore content and side features to classify wiki pages in full streaming mode, including the relationship between users and wiki articles. Specifically, the stream operation also applies to the word $n$-gram analysis and hyperparameter selection. The experimental results, obtained with two datasets, analyze the performance of the system in three evaluation scenarios using macro and micro metrics (\textit{i.e.}, accuracy, precision, recall, \textit{F}-measure, and processing time). Finally, textual (\textit{i.e.}, natural language) and visual explainability are provided in the dashboard.

\begin{table*}[!htbp]
\centering
\caption{Comparison with related work from the literature.~\label{tab:Comparison}}
\begin{tabular}{lccccc} 
\toprule
\textbf{Authorship} & \textbf{Profiling} & \textbf{Processing} & \textbf{Model} & \textbf{Explainability}\\
\midrule

\multirow{3}{*}{Bassani \textit{et al.}____} & Content & \multirow{3}{*}{Offline} & \multirow{3}{*}{Supervised \textsc{ml}} & \multirow{3}{*}{\xmark}\\
& Review\\
& Network\\

Furuta \textit{et al.}____ & Content & Offline & \textsc{bert} & \xmark\\

Hsu \textit{et al.}____ & Content & Offline & \textsc{mlp} & \xmark\\

Z. Chen \textit{et al.}____ & Content & Offline & Ro\textsc{bert}a & \xmark\\

F. Petroni \textit{et al.}____ & Content & Offline & \textsc{bert} & \xmark\\

S. Shivans \textit{et al.}____ & Content & Offline & \textsc{bert} & \xmark\\

P. Das \textit{et al.}____ & Side & Offline & Regular expressions & \xmark\\

\midrule

\multirow{2}{*}{\textbf{Proposal}}	& Content & \multirow{2}{*}{Online} & \multirow{2}{*}{Online} & Textual \\
 & Side & & & Visual\\
 
\bottomrule
\end{tabular}
\end{table*}

In contrast to previous approaches ____, this paper expands on disinformation detection from a multidimensional perspective, \textit{i.e.}, considering performance, efficiency, and fairness. Moreover, it incorporates inputs from expert knowledge, also known as human-in-the-loop \textsc{ai} (\textsc{hai}) ____, and an \textsc{llm} to enhance performance and promote the explainability and fairness of the results. Both inputs are integrated into the explainability control panel: the expert knowledge is used to refine the supervised classifier model via reinforcement learning and the \textsc{llm} descriptions to explain classifier predictions via highly coherent human-like generated texts. Ultimately, \textsc{llm}s and the human-in-the-loop approaches can report relevant advantages to disinformation recognition, especially regarding pattern extraction and explainability ____. We believe its combination has the potential to share the future detection methods provided the understanding capabilities of \textsc{llm}s. However, these models may incorporate underlying misconceptions due to the absence of a comprehensive understanding of the problem and expert data. The human-in-the-loop approach can do it by leveraging expert knowledge of linguistic patterns, language usage, and deep semantic meaning in disinformation content. This approach also helps to limit the hallucination problem of \textsc{llm}s. Consequently, our system can assist editors by reducing their efforts and time regarding disinformation detection.

In summary, the main contributions of our proposal are:

\begin{itemize}
\item Application of streaming techniques for identifying disinformation in wiki data. Accordingly, our solution is updated in each incoming sample, avoiding the costly batch processing that is the prominent approach in the literature.

\item Generation of a wide range of features to model the evolving nature of disinformation and the crowdsourcing scheme, including the analysis of historical data.

\item Use of \textsc{llm} in combination with \textsc{nlp} techniques such as prompt engineering to generate a natural language description of the system rationale.

\item The possibility of integrating an expert-in-the-loop to verify and correct the system outputs to ensure the proposal's trustworthiness, reliability, and accountability.
\end{itemize}