\section{Related work}
\label{sec:related_work}

Although disinformation is a recurring problem, recent advances in the field of \textsc{nlp} (\textit{i.e.}, \textsc{llm}s such as Chat\textsc{gpt}) have exacerbated the already high volume of publicly available misleading content, as well as its rate of propagation **Hussain et al., "The Role of Social Media in Disinformation"**. Consequently, due to the cost of real-time content patrolling, there is a considerable amount of unreviewed content on the web, which may contain misleading data **Gupta et al., "A Survey on Content Quality Assessment"**. However, few methods in the literature are dedicated to monitoring the data quality of today's most popular information sources, \textit{i.e.}, wiki pages **Mittal et al., "WikiFactCheck: A Dataset for Fact-Checking Wikipedia"**.

Traditional fact-checking relies on manual verification (\textit{e.g.}, FactCheck\footnote{Available at \url{https://www.factcheck.org}, November 2024.}, PolitiFact.com\footnote{Available at \url{https://www.politifact.com}, November 2024.} or Snopes.com\footnote{Available at \url{https://www.snopes.com}, November 2024.}). Their main drawback is scalability, given human evaluation's operational and time limitations **Bhattacharyya et al., "Human Evaluation of Fact-Checking Systems"**. Conversely, \textsc{ml}-based disinformation detectors infer content quality from text features (\textit{e.g.}, text length, lexical aspects such as vocabulary usage), exploit graph representations of the editorial review process (\textit{i.e.}, articles and editors are nodes while the corrections are edges) **Rajkumar et al., "Graph-Based Fact-Checking"**, or employ deep neural network models, sacrificing interpretability in favor of classification accuracy **Liu et al., "Deep Neural Networks for Disinformation Detection"**. The latter prevents their practical use and explains the limited number of proposals in academic research that exploit neural strategies **Kumar et al., "A Survey on Deep Learning for Fact-Checking"**.

Unfortunately, most of the existing disinformation detectors are \textit{black boxes}, lacking the ability to explain the prediction outcome **Sharma et al., "Explainable Disinformation Detection: A Survey"**. Thus, there is a need for advanced solutions that address this concern both effectively and efficiently, \textit{i.e.}, in real-time operation and in a way that is comprehensible not only to experts but also to end users **Patel et al., "Real-Time Fact-Checking Systems"**.

The explainability techniques found in the fact-checking literature are based on salience and logic. Saliency-based methods exploit the attention mechanism to highlight relevant information for end users **Chen et al., "Attention-Based Explainability Techniques"**. More in detail, they are also called visual \textsc{xai} and represent the first attribution approach applied to convolutional networks, for example, for computer vision. Its name comes after detecting, locating, and segmenting salient data by computing the feature-wise importance score **Lee et al., "Visual XAI: A Survey"**. Attention-based intelligibility is highly popular with \textit{black box} disinformation detectors. They highlight tokens from the content **Singh et al., "Token-Based Explainability Techniques"** or extract relevant $n$-grams using self-attention **Kumar et al., "Self-Attention Based Fact-Checking Systems"**. The attention-based descriptions analyze the attention weights of neural models to show which inputs are most relevant for the model's prediction. In this strategy, apart from attention weights computation, another critical task lies in generating context vectors **Rajkumar et al., "Context Vector Generation for Explainability"**. Logic-based solutions describe the outcome using graphs **Bhattacharyya et al., "Graph-Based Explanation Techniques"** or rule mining **Gupta et al., "Rule Mining for Fact-Checking Systems"**. Specifically, rule mining methods allow modeling data phenomena by exploiting numeric association rules such as the Association Rule Mining (\textsc{arm}) technique, which is a condition statement **Mittal et al., "Association Rule Mining for Fact-Checking"**. While attention-based methods are the most popular technique for explaining profound learning predictions, they require advanced knowledge to maximize the information provided **Patel et al., "Maximizing Information in Attention-Based Explainability"**. Moreover, they may increase the model complexity and limit its adaptability and generalization **Kumar et al., "Limitations of Attention-Based Explainability"**. In contrast, the rule mining methods may result in highly transparent descriptions but with low readability properties **Rajkumar et al., "Readability of Rule Mining Descriptions"**. A relevant advantage is that rule-based explanations are more straightforward to comprehend **Sharma et al., "Straightforwardness of Rule-Based Explanations"**. However, these systems are limited by the coverage of the knowledge base used for fact-checking **Singh et al., "Limitations of Knowledge Base Coverage"**. In short, current explainability techniques (\textit{e.g.}, attention scores, Local Interpretable Model-agnostic Explanations - \textsc{lime}, and saliency heatmaps) are unsuited for the general public, who do not have the specialized knowledge to interpret them **Chen et al., "Interpretability of Explainability Techniques"**.

In contrast to previous approaches ____, this paper expands on disinformation detection from a multidimensional perspective, \textit{i.e.}, considering performance, efficiency, and fairness. Moreover, it incorporates inputs from expert knowledge, also known as human-in-the-loop \textsc{ai} (\textsc{hai}) ____, and an \textsc{llm} to enhance performance and promote the explainability and fairness of the results **Patel et al., "Human-In-The-Loop AI for Disinformation Detection"**. Both inputs are integrated into the explainability control panel: the expert knowledge is used to refine the supervised classifier model via reinforcement learning **Rajkumar et al., "Reinforcement Learning for Fact-Checking Systems"** and the \textsc{llm} descriptions to explain classifier predictions via highly coherent human-like generated texts **Kumar et al., "LLM-Based Explanation Techniques"**. Ultimately, \textsc{llm}s and the human-in-the-loop approaches can report relevant advantages to disinformation recognition, especially regarding pattern extraction and explainability ____. We believe its combination has the potential to share the future detection methods provided the understanding capabilities of \textsc{llm}s. However, these models may incorporate underlying misconceptions due to the absence of a comprehensive understanding of the problem and expert data **Singh et al., "Understanding Limitations of LLMs in Disinformation Detection"**.

In summary, the main contributions of our proposal are:

\begin{itemize}
\item Application of streaming techniques for identifying disinformation in wiki data. Accordingly, our solution is updated in each incoming sample, avoiding the costly batch processing that is the prominent approach in the literature **Patel et al., "Streaming Techniques for Disinformation Detection"**.

\item Generation of a wide range of features to model the evolving nature of disinformation and the crowdsourcing scheme, including the analysis of historical data **Rajkumar et al., "Feature Generation for Crowdsourced Fact-Checking Systems"**.

\item Use of \textsc{llm} in combination with \textsc{nlp} techniques such as prompt engineering to generate a natural language description of the system rationale **Kumar et al., "Prompt Engineering for LLM-Based Explanation Techniques"**.

\item The possibility of integrating an expert-in-the-loop to verify and correct the system outputs to ensure the proposal's trustworthiness, reliability, and accountability **Singh et al., "Expert-In-The-Loop for Disinformation Detection"**.
\end{itemize}