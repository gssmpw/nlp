\section{Related Work}
\textbf{Dataset classification.} \citet{torralba2011unbiased} introduced the ``Name That Dataset‚Äù experiment a decade ago to highlight the bias present in visual datasets of that time. Recently, \citet{liu2024datasetbias} revisited this problem (termed dataset classification) and found that current large-scale, supposedly more diverse visual datasets are still very biased. \citet{zeng2024bias} further identified structural and semantic components in images as key contributors to these biases. \citet{you2024images} and \citet{mansour2024bias} applied the dataset classification framework to study bias in synthetic images and LLM pretraining datasets respectively. 
While the synthetic task shown in  Figure~\ref{fig:teaser} is conceptually similar to dataset classification, we focus not on training datasets but on the distinctive characteristics inherent to LLMs.








\textbf{Human \vs machine-generated texts.} Many prior works have studied the problem of determining if a text is authored by a human or an AI system~\citep{mitchell2023detectgpt,mcgovern2024fingerprints}. Model-free approaches typically use linguistic properties such as n-gram frequencies~\citep{badaskar-etal-2008-identifying,openai_gpt2_dataset}, entropy~\citep{thomas2008entropy,gehrmann2019gltr} or negative probability curvature~\citep{mitchell2023detectgpt,bao2023fastdetectgpt}. Other works leverage neural network features to perform this task, such as fine-tuning BERT models~\citep{uchendu2021turingbench,ippolito2019automatic}. Neural authorship attribution~\citep{uchendu-etal-2020-authorship,huang2024authorship} seeks not only to identify machine-generated text but also to attribute it to specific text generators. In this work, we focus on the distinguishability between LLMs rather than between AI \vs human.

\textbf{Understanding differences between distributions.} A line of research~\citep{dunlap2023describing, zhong2024explaining} has used foundation models to describe qualitative differences between pairs of data distributions (\eg, image datasets). \citet{gao2024model} conducted hypothesis testing on sets of model outputs to check whether the underlying LLMs were identical. The most relevant work to us is \citet{dunlap2024vibecheck}, which proposed VibeCheck to understand user-aligned traits in LLM outputs. They found that LLMs often vary in styles, such as being more formal or friendly. In contrast, our work aims to identify broader generalizable patterns to interpret the high classification performance.