
\documentclass{article}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{pdfpages}
\usepackage{times}
\usepackage{microtype}
\usepackage{epsfig}
\usepackage{float}
\usepackage{placeins}
\usepackage{color, colortbl}
\usepackage{stfloats}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{xstring}
\usepackage{multirow}
\usepackage{xspace}

\usepackage{siunitx}

\usepackage[accepted, nohyperref]{icml2025}
\usepackage[british, american]{babel}
\usepackage{graphicx, amsmath, amssymb, subcaption, multirow, overpic, textpos}
\usepackage{array}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\definecolor{baselinecolor}{gray}{.9}
\definecolor{red}{HTML}{FF8988}
\definecolor{yellow}{HTML}{FECC81}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\usepackage{tikz}
\usepackage[pagebackref=false, breaklinks=true, colorlinks, citecolor=citecolor, linkcolor=linkcolor, urlcolor=mydarkblue, bookmarks=false]{hyperref}
\usepackage{url}


\PassOptionsToPackage{hyphens}{url}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\renewcommand{\paragraph}[1]{\vspace{1.25mm}\noindent\textbf{#1}}
\newcommand\blfootnote[1]{\begingroup\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup}

\newcommand{\cmark}{\ding{51}} %
\newcommand{\xmark}{\ding{55}} %

\newcommand{\eg}{\emph{e.g}.}
\newcommand{\ie}{\emph{i.e}.}
\newcommand{\etc}{\emph{etc}.}
\newcommand{\vs}{\emph{vs}.\ }
\newcommand{\etal}{\emph{et al}.}

\newcommand{\zl}[1]{{\color{blue}[ZL: #1]}}

\newcommand{\gpt}{ChatGPT}
\newcommand{\claude}{Claude}
\newcommand{\grok}{Grok}
\newcommand{\gemini}{Gemini}
\newcommand{\deepseek}{DeepSeek}


\icmltitlerunning{Idiosyncrasies in Large Language Models}
     

\begin{document}

\twocolumn[


  \icmltitle{Idiosyncrasies in Large Language Models}
  
    
    
    \icmlsetsymbol{equal}{*}
    
    \begin{icmlauthorlist}
    \icmlauthor{Mingjie Sun}{equal,cmu}\; 
    \icmlauthor{Yida Yin}{equal,cal}\;
    \icmlauthor{Zhiqiu Xu}{penn}\;
    \icmlauthor{J. Zico Kolter}{cmu}\;
    \icmlauthor{Zhuang Liu}{princeton}
    \end{icmlauthorlist}

    \icmlaffiliation{cmu}{Carnegie Mellon University}
    \icmlaffiliation{princeton}{Princeton University}
    \icmlaffiliation{cal}{UC Berkeley}
    \icmlaffiliation{penn}{University of Pennsylvania}

    \icmlcorrespondingauthor{Mingjie Sun}{mingjies@cs.cmu.edu}
    \icmlcorrespondingauthor{Yida Yin}{davidyinyida0609@berkeley.edu}


\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} %





\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figs/teaser.pdf}
  \vspace{-2.0em}
  \caption{\textbf{Our framework for studying idiosyncrasies in Large Language Models (LLMs).} We show that  each LLM is unique in its expression. In the example shown here on \gpt, \claude, \grok, \gemini, and \deepseek, a neural network classifier is able to distinguish them with a near-perfect 97.1\% accuracy.
  }
  \vspace{-.5em}
  \label{fig:teaser} 
\end{figure*}

\begin{abstract}
In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1\% accuracy on held-out validation data in the five-way classification problem involving \gpt, \claude, \grok, \gemini, and \deepseek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at \href{https://github.com/locuslab/llm-idiosyncrasies}{github.com/locuslab/llm-idiosyncrasies}.








\end{abstract}

\section{Introduction}
As the adoption of generative models such as LLMs accelerates, it becomes increasingly important to understand the origin and provenance of such generated content.  While a great deal of past work has focused on the classification of human-written and AI-written content~\citep{jawahar2020automatic,krishna2023paraphrasing,mitchell2023detectgpt,sadasivan2023can},
there has been little work on classifying \emph{between} content generated by different LLMs, either between the outputs of entirely different models or between those of different variants of the same model family. If possible, the ability to distinguish between source models in this manner would be valuable for a number of applications: it could shed light on the relative uptake of different LLMs, beyond what is reported by individual companies, and on the nature of data used to build different models. Additionally, it could offer insights into what features the output generation are most ``unique'' to each LLM.










In this paper, we investigate whether LLMs exhibit idiosyncrasies that enable their outputs to be reliably differentiated. Drawing inspirations from recent studies on dataset bias in computer vision ~\citep{liu2024datasetbias,zeng2024bias}, which demonstrated that images from different large-scale vision datasets can be accurately distinguished by a standard neural network classifier, we consider a similar synthetic classification task to assess the separability of responses generated between different LLMs. Specifically, we sample a large number of text outputs from each LLM using the same set of prompts and then train a classifier to recognize which model generates a specific text. Figure~\ref{fig:teaser} provides an overview of our framework. The illustrated example on \gpt, \claude, \grok, \gemini, and {\deepseek} presents a five-way classification problem.  

We find that a classifier based upon simple fine-tuning text embedding models on LLM outputs is able to achieve remarkably high accuracy on this task. This indicates the clear presence of idiosyncrasies in LLMs. The observation is highly robust over a large variety of LLM combinations. For instance, 
trained on the combined set of texts from \gpt, \claude, \grok, \gemini, and \deepseek, a model can achieve 97.1\% classification accuracy on the \textit{held-out} validation data, compared to a 20.0\% chance-level guess. Within the same model family, we obtain a non-trivial 59.8\% accuracy across 4 model sizes in Qwen-2.5 series~\cite{qwen2025qwen25technicalreport}. Further, we observe strong out-of-distribution generalization of these classifiers when tested on responses from prompts outside the training distribution.



We observe several interesting properties of this task. When controlling the length and format of outputs through prompt instructions, we still obtain high classification accuracy. Furthermore, for post-trained LLMs, the classifier demonstrates non-trivial accuracy even with only the first few tokens of the generated text. However, when classifying generations from the \emph{same} LLM but using different sampling strategies, we achieve accuracy only slightly above the chance level. In addition, we observe certain behaviors of this task that resemble those of standard text classification, where improvements in text embeddings and availability of larger training datasets lead to better classification performance.

We analyze the contributing factors to the idiosyncratic behaviors in LLMs. Our analysis is based on isolating different levels of information through text transformations. We find that after randomly shuffling words in the LLM-generated responses, we observe a minimal decline in classification accuracy. This suggests that a substantial portion of distinctive features is encoded in the word-level distribution. We then highlight distinct sets of characteristic phrases that are consistently associated with each LLM. We also find that markdown formatting contributes to a moderate degree of idiosyncrasies in LLMs following post-training.

At the same time, we obtain over 90\% accuracy when the word distribution is disrupted through transformations that preserve semantics, such as rephrasing or translating. Even with the most aggressive transformation -- summarizing, classification accuracy remains well above chance-level guess. This finding implies that semantic information also shapes the idiosyncrasies in LLMs. Through open-ended language analysis, we provide further insights into these characteristics. For instance, {\gpt} has a preference for detailed, in-depth explanations, whereas {\claude} produces more concise and direct responses, prioritizing clarity.


Last, we discuss the broader implications of our findings. One should be cautious when using synthetic data to train LLMs, as we show that many of these idiosyncrasies can be inherited in such a process. Our framework also serves as a tool for assessing model similarities among frontier models, either open-source or proprietary.






























\begin{table*}[th]
\centering
\normalsize 
\begin{subtable}[t]{\linewidth}
\centering
\tablestyle{4pt}{1.15}
\begin{tabular}{x{50}x{50}x{50}x{50}x{50}x{55}}
   \gpt   &  \claude  &  \grok   &   \gemini  & \deepseek &  acc. (chat) \\ 
   \shline 
\cmark & \cmark &  &  & & 99.3 \\
\cmark &  &   \cmark   &  &  & 97.7 \\
\cmark &  &        &     \cmark   & & 98.7 \\
\cmark &  &  &     & \cmark &  97.2 \\
 &   \cmark     & \cmark &  & & 99.7 \\
 & \cmark   &  &  \cmark     & & 99.6 \\
   &   \cmark     &  &    & \cmark  & 99.6 \\
 &        & \cmark &   \cmark   & &  99.4\\
  &        & \cmark &     & \cmark &  98.7 \\
     &        &  & \cmark  & \cmark &  99.9 \\
\midrule

\cmark & \cmark & \cmark & \cmark & \cmark & 97.1 \\
\\
\\
Llama   &  Gemma    &  Qwen   &   Mistral    & acc. (instruct) & acc. (base) \\ 
\shline 
\cmark & \cmark &  &   & 99.9    & 98.3    \\
\cmark &  &   \cmark     &   & 97.8 & 81.7 \\
\cmark &  &        &     \cmark  & 97.0 & 96.3 \\
 &   \cmark     & \cmark &  & 99.9 & 98.3\\
 & \cmark   &  &  \cmark      &  99.9 & 98.4 \\
 &        & \cmark &   \cmark     &  96.1 & 95.7\\
\midrule 
\cmark & \cmark & \cmark & \cmark   &  96.3 & 87.3 \\
\end{tabular}
\vspace{-13.5em}
\caption{chat APIs}
\label{tab:main_api_v2}
\vspace{13em}
\end{subtable}
\begin{subtable}[t]{\linewidth}
\centering 
\tablestyle{4pt}{1.15}
\begin{tabular}{cccccc}
\end{tabular}
\vspace{-1.5em}
\caption{instruct and base LLMs}
\vspace{-1.5em}
\label{tab:main_base_v2}
\end{subtable}
\vspace{-3ex}
\caption{\textbf{Classification accuracies for various LLM combinations.} \emph{Top}: results for chat APIs. \emph{Bottom}: results for instruct and base LLMs. Check marks (\cmark) denote the models included in each combination. We observe high classification accuracies consistently across all model combinations, indicating the presence of distinct idiosyncrasies in LLMs.}
\vspace{-1ex}
\label{tab:main}

\end{table*}







\section{Evaluating Idiosyncrasies in LLMs}\label{sec-evaluate-idiosyncrasies}
Large Language Models (LLMs) share several common characteristics. First, they all utilize the Transformer architecture with self-attention~\citep{vaswani2017attention}. Second, they are trained using an auto-regressive objective~\citep{radford2019gpt2}, where they predict predict the next token in a sequence based on preceding context. Lastly, their training datasets significantly overlap, often incorporating vast and diverse sources such as Common Crawl, Wikipedia and Stack Overflow. Given these similarities, 
it is natural to ask: do LLMs speak in the same way? If not, how can we effectively measure the degree of their differences?


To address these questions, we construct a synthetic task focused on classifying outputs from different LLMs. Consider $N$ LLMs, denoted as $f_{1},\ldots,f_{N}$, where each $f_{i}$ takes an input prompt $p$ and outputs a text completion $o$. For a given dataset $\mathcal{D}$ of prompts, the outputs produced by each LLM $f_{i}$ are denoted as  $\mathcal{O}_{i}$. We approach this problem with a straightforward setup. For $N$ output sets $\mathcal{O}_{i}$, we formulate a $N$-way classification task, where the objective is to predict which LLM produced each output. If outputs of different LLMs were drawn from the same distribution, classification accuracy would not be better than random chance. Thus, we use the classification performance of this synthetic task as a measure of idiosyncrasies in LLMs.

\subsection{Main Observations}
\label{sec:main_results}
We observe surprisingly high accuracies by neural networks to classify LLM outputs. This observation is robust across different settings, \eg, across model families and sizes. Since our task is essentially a sequence classification problem, we fine-tune a strong sequence embedding model LLM2vec~\cite{parishad2024llm2vec} with a $N$-way classification head and report the resulting accuracy. The training details are provided in Appendix~\ref{appendix:implementation}. 

We describe the LLMs we use to generate the output datasets $\mathcal{O}_{1,\cdots,N}$. For a comprehensive and fair comparison across model families, we categorize three groups of LLMs:
\setlist{nosep}
\begin{enumerate}[topsep=-2.5pt,itemsep=2pt,parsep=0pt,partopsep=0pt,leftmargin=15pt]
    \item Chat APIs (``chat''): This category includes state-of-the-art LLMs that are primarily accessible via APIs. We consider GPT-4o~\citep{openai2024gpt4o}, Claude-3.5-Sonnet~\citep{anthropic2024}, Grok-2~\cite{grok2}, Gemini-1.5-Pro~\cite{team2024gemini}, and DeepSeek-V3~\cite{deepseekai2024deepseekv3technicalreport}. For simplicity, we refer to them as ChatGPT, Claude, Grok, Gemini and DeepSeek. Their architectures and weights  remain proprietary and undisclosed, with the exception of DeepSeek. 
    \item Instruct LLMs (``instruct''): These models are trained to generate high-quality responses from human instructions. We consider four LLMs of similar sizes across different families: Llama3.1-8b~\citep{Llama3}, Gemma2-9b~\citep{gemma2}, Qwen2.5-7b~\citep{qwen2025qwen25technicalreport} and Mistral-v3-7b~\citep{jiang2023mistral}. We will refer to them as Llama, Gemma, Qwen and Mistral. 
    \item Base LLMs (``base''): These are base versions of instruct LLMs. They are obtained by pretraining on extensive text corpora without any post-training stage.
\end{enumerate}

Throughout the paper, we refer to these three categories as “chat”, “instruct”, and “base” respectively. For each LLM and a given prompt dataset, we collect 11K text sequences, splitting them into 10K for training and 1K for validation. For chat APIs and instruct LLMs, we generate outputs from UltraChat~\citep{ding2023ultrachat}, a diverse dialogue and instruction dataset. For base LLMs, we synthesize new texts using prompts from FineWeb~\citep{penedo2024fineweb}, a high-quality LLM pretraining dataset. More details on response generation are in Appendix~\ref{appendix:response_gen}. 


\textbf{Across model families.} In Table~\ref{tab:main}, we report the results for classifying outputs from various combinations of chat APIs (Table~\ref{tab:main_api_v2}) and instruct / base LLMs (Table~\ref{tab:main_base_v2}). In each of the three LLM groups, we enumerate all ($C_N^2$) possible pairwise combinations when choosing 2 out of $N$ models in the top panel of each table, as well as the case including $N$ models in the bottom row. For the binary classification task, the neural network consistently achieves over 90\% accuracy, with only one exception. Notably, for chat APIs and instruct LLMs, many combinations reach as high as 99\% accuracy. In the more challenging $N$-way classification tasks, our classifiers maintain strong performance, achieving at least 87.3\% accuracy across three groups. These results highlight the idiosyncrasies across different LLMs. We refer readers to Appendix~\ref{appendix:section_confusion_matrix} for the confusion matrices of our classifiers.



\textbf{Within the same model family.} We evaluate sequence classification performance when distinguishing responses from LLMs within the same model family. Note that models from the same family typically share common training procedures, \eg, pretraining datasets and optimization schedule. First, we analyze the impact of model size by considering four Qwen2.5 instruct LLMs with 7B, 14B, 32B, and 72B parameters. As shown in Table~\ref{tab:main_model_size}, the classification task becomes more difficult, but our classifiers remain reasonably well above chance accuracy when distinguishing LLMs within the same family. In the binary classification setup, the highest accuracy reaches 85.5\%, whereas in the full combination setup, the accuracy becomes 59.8\%. In addition, we observe high accuracies when classifying responses from base and instruct versions of the same model. For example, our classifiers achieve 96.8\% accuracy when distinguishing outputs from Qwen2.5-7b base and instruct models.


\begin{table}[H]
\centering
\vspace{-1ex}
\tablestyle{6pt}{1.15}
\begin{tabular}{ccccc}
   7b   &  14b    &  32b   &   72b    & instruct \\ 
   \shline
\cmark & \cmark &  &      &    77.0 \\
\cmark &  &   \cmark     &   &  81.2  \\
\cmark &  &        &     \cmark &   83.4 \\
 &   \cmark     & \cmark &  &  63.1     \\
 & \cmark   &  &  \cmark       & 85.5      \\
 &        & \cmark &   \cmark    &  84.8 \\
 \hline
\cmark & \cmark & \cmark & \cmark  &  59.8 \\
\end{tabular}
\vspace{-1ex}
\caption{\textbf{Classification within Qwen2.5 model family.} The classifier can differentiate responses between LLMs within the same model family with reasonably well accuracies. 
}
\vspace{-1em}
\label{tab:main_model_size}
\end{table}




 


\textbf{Generalization to out-of-distribution responses.} We find that our classifiers generalize robustly to responses beyond their training distribution. To evaluate this, we collect responses from instruct LLMs across four diverse datasets: \ie,  UltraChat, Cosmopedia~\cite{benallal2024cosmopedia}, LmsysChat~\cite{zheng2024lmsyschat1mlargescalerealworldllm}, and WildChat~\cite{zhao2024wildchat1mchatgptinteraction}. These datasets originate from different sources and are designed for various purposes -- Cosmopedia is designed for synthetic data generation, LmsysChat and WildChat capture real-world user interactions, while UltraChat consists primarily of synthetic responses. For each dataset, we train a classifier on a group of model responses and evaluate the classifier across all four datasets. We use instruct LLMs for this experiment. As shown in Table~\ref{tab:main_it_ood}, our classifiers generalize well across different datasets, indicating that they learn very robust and transferable patterns.

\begin{table}[ht]
\vspace{-1ex}
\centering
\small 
\tablestyle{2pt}{1.15}
\begin{tabular}{lcccc}
 train / test  & UltraChat & Cosmopedia & LmsysChat & WildChat \\
    \shline
UltraChat  & 96.3 & 98.9 & 89.9 & 92.4\\
Cosmopedia & 95.7 & 99.8 & 88.3 & 94.9 \\
LmsysChat  & 94.7 & 97.2 & 91.8 & 92.0 \\
WildChat   & 95.1 & 99.1 & 90.2 & 95.7 \\
\end{tabular}

\vspace{-1.5ex}
\caption{\textbf{Robust generalization to out-of-distribution responses.} We train classifiers on LLM outputs from one prompt dataset and tested on those from another.
}
\label{tab:main_it_ood}
\end{table}



\subsection{Controlled Experiments}\label{sec:main_control}
We analyze the behaviors of the synthetic classification task in several controlled settings. \textit{From now on, we only report the accuracy of the $N$-way classification task in each group.} 

\textbf{Prompt-level interventions.} We assess the degree of idiosyncrasies in LLM outputs with explicit prompt-level interventions. Specifically, we modify the original prompt by incorporating additional instructions to constrain response length and format. We then perform sequence classification on the resulting outputs. Our interventions are: 
\begin{itemize}[topsep=-1pt, ,itemsep=0pt,parsep=0pt,partopsep=-0.5pt, leftmargin=12pt]
    \item Length control: \textit{Please provide a concise response in a single paragraph, limited to a maximum of 100 words.}
    \item Format control: \textit{Please provide your response in plain text only, avoiding the use of italicized or bold text, lists, markdown, or HTML formatting.}
\end{itemize}
LLM outputs after these interventions are presented in Appendix~\ref{appendix:response_demonstration}. We find that LLMs can follow the additional instructions in generating responses.


\begin{table}[ht]
\centering
\small 
\tablestyle{3.pt}{1.15}
\begin{tabular}{lccc}
   & original & length control & format control  \\
  \shline
instruct LLMs   & 96.3 & 93.0 & 91.4 \\
\end{tabular}
\caption{\textbf{Controlling LLM outputs with prompts.} An instruction is added to the original prompt to specify the output length and format. \textit{Length control} limits responses to one paragraph. \textit{Format control} ensures that responses are in plain text without any format.
}
\label{tab:main_prompt}
\end{table}

Table~\ref{tab:main_prompt} presents the results for this analysis. We can see that neural networks still perform excellently for classifying LLM outputs applied with length and format control prompts. These findings suggest that LLM characteristics are deeply embedded in the generated text, persisting despite surface-level constraints on length and formatting.


\begin{figure}[ht]
\vspace{1ex}
\centering
    \includegraphics[width=.98\linewidth]{figs/max_seq_length.pdf}
    \vspace{-.5em}
    \caption{\textbf{Ablations on input length of text embedding models.} Classification accuracies improve as the text embedding models capture more context. Performance begins to saturate beyond an input sequence length of 256. Note that the three lines represent different groups of LLMs and are not directly comparable.}
    ~\label{fig:seq_len}
    \vspace{-1em}
\end{figure}

\textbf{Input length of text embedding models.} We control the number of input tokens to the text embedding models. Specifically, we truncate each response to a fixed number of tokens in a left-to-right fashion. Figure~\ref{fig:seq_len} presents the results. Across three groups of LLMs, the classification task benefits from seeing an increased number of tokens. Intriguingly, for chat APIs and instruct LLMs, we observe around 50\% accuracy with just a single text token. This suggests that the initial token in a response contains certain distinctive signals for the classification problem. In Section~\ref{sec:words_letters}, we provide further evidence supporting this observation.






\textbf{Sampling methods.} 
We consider outputs when sampled using different decoding strategies. Specifically, we use four widely used sampling methods: greedy decoding, temperature softmax, top-k, and top-p sampling. For each method, we generate a set of responses from the LLM. We then fine-tune the LLM2vec embedding model to predict the sampling method responsible for each response. 

\begin{table}[htbp]
\centering
\tablestyle{3.5pt}{1.15}
\begin{tabular}{lcccc}
 & greedy & softmax & top-k & top-p \\ \shline 
 greedy & - & - & - & - \\
 softmax & 59.6 & - & - & -\\
 top-k   & 58.2 & 50.0 & - & -\\
 top-p & 52.9 & 51.0 & 52.1 & -\\
\end{tabular}
\vspace{-0.3em}
\caption{\textbf{Classifications with different sampling methods.} Distinguishing responses generated by the same model using different sampling strategies is only marginally better than chance accuracy. The results are on Llama3.1-8b instruct model's responses.}
\label{tab:sampling}
\end{table}

Table~\ref{tab:sampling} presents the results for all pairs of sampling methods. Notably, the accuracy of distinguishing between responses generated by the same LLM remains relatively low, with the highest accuracy across all configurations being 59\%. Furthermore, in a more fine-grained 5-way classification task distinguishing softmax sampling at five different temperatures ($T=$ 0, 0.25, 0.5, 0.75, 1), we obtain an accuracy of 37.9\%, only marginally better than the random chance level of 20\%. These results suggest that outputs from the same LLM are not easily separable based on decoding strategies.





\textbf{Text embedding models.} 
We vary the underlying pretrained embedding models for sequence classification. The default setting we used in previous parts is fine-tuning the LLM2vec embedding models. We consider various generations of embeddings models spanning across architectures and training methods: ELMo~\cite{peters2018deepcontextualizedwordrepresentations},  BERT~\cite{devlin2018bert}, T5~\citep{raffel2023T5}, GPT-2~\citep{radford2019gpt2}, and LLM2vec~\citep{parishad2024llm2vec}. 
Details on the fine-tuning setting can be found in Appendix~\ref{appendix:finetune}. 

\begin{table}[ht]
\centering
\small 
\tablestyle{6pt}{1.15}
\begin{tabular}{lccc}
    method & chat & instruct & base \\
    \shline 
    ELMo & 90.8 & 91.0 & 69.8 \\
    BERT   & 91.1 & 91.5 & 66.0 \\
    T5 & 90.5 & 89.8 & 67.9 \\
    GPT-2 & 92.1 & 92.3 & 80.2 \\
    LLM2vec & \textbf{97.1} & \textbf{96.3} & \textbf{87.3}\\
\end{tabular}
\vspace{-.25em}
\caption{\textbf{Different sequence embedding models.} LLM2vec achieves the best performance in classifying outputs from various LLMs among the five embedding models we study.}
\label{tab:main_embedding}
\end{table}

Table~\ref{tab:main_embedding} shows the results. All sequence embedding models can achieve very high accuracies. The classification performance improves with more advanced sequence embedding models. Among all methods, LLM2vec demonstrates the best performance, achieving 97.1\% on chat APIs, 96.3\% on instruct LLMs, and 87.3\% on base LLMs. 
































\textbf{Training data size.} We vary the number of training samples generated by LLMs and train the classifier with the same total number of iterations. We present the results in Figure~\ref{fig:train_samples}. The performance of the classifier increases as it is trained with more training samples. This trend is consistently observed across chat APIs, instruct LLMs, and base LLMs. Furthermore, as few as 10 training samples, the classifier achieves non-trivial accuracy (\eg, 40.3\% on instruct LLMs), surpassing 20\% chance-level guess.

\begin{figure}[ht]
\vspace{2.ex}
    \includegraphics[width=.98\linewidth]{figs/num_samples.pdf}
    \vspace{-.5ex}
    \caption{\textbf{Different numbers of training samples.} Our sequence classifiers benefit from more training samples. The classification performance converges when using about 10K training samples.}~\label{fig:train_samples}
    \vspace{-1.em}
\end{figure}












\section{Concrete Idiosyncrasies in LLMs}

We have shown that modern neural networks can achieve excellent accuracies in classifying which LLM generates a given response. Here we leverage classical text similarity metrics -- ROUGE-1~\cite{lin-2004-rouge}, ROUGE-L~\cite{lin-2004-rouge}, and BERTScore~\cite{devlin2018bert} -- to quantify lexical differences between LLM outputs. We compute the mean F1-score for each metric across all response pairs generated by any two different chat API models given the same prompt. For comparison, we also measure the similarity between responses sampled within the same model. As shown in Table~\ref{tab:text_sim}, responses from different LLMs exhibit lower text similarities than those from the same model. 

\begin{table}[ht]
    \tablestyle{6pt}{1.15}
    \begin{tabular}{lccc}
    & across LLMs & within an LLM\\
    \shline
    ROUGE-1   & 0.499 & 0.660 \\
    ROUGE-L   & 0.256 & 0.414 \\
    BERTScore$^*$ & 0.220 & 0.482
    \end{tabular}
    \caption{\textbf{Text similarity scores.}  We evaluate the text similarity of LLM outputs using ROUGE-1, ROUGE-L, and BERTScore. * We follow \citet{zhang2020bertscoreevaluatingtextgeneration} to rescale BERTScore with respect to the human baseline. The results indicate that responses from different LLMs exhibit low lexical similarity.}
    \label{tab:text_sim}

\end{table}

In the following, we identify concrete idiosyncrasies in LLMs across three dimensions: words and letters, markdown formatting elements, and semantic meaning. For each dimension, we apply text transformations to isolate potential idiosyncrasies and assess their impacts on classification performance. We then highlight specific patterns within each dimension that distinguish LLMs.





\begin{figure*}[th]
    \centering
    \begin{subfigure}[h]{0.49\textwidth}
    \centering
        \includegraphics[width=\linewidth]{figs/representative_phrases_examples.pdf}
    \vspace{-1.5em}
    \subcaption{characteristic phrases}
    \label{fig:example_phrases}
    \end{subfigure}~
    \begin{subfigure}[h]{0.49\textwidth}
    \centering
        \includegraphics[width=\linewidth]{figs/markdown_features_examples.pdf}
    \vspace{-1.5em}
    \subcaption{unique markdown formatting}
    \label{fig:example_special_characters}
    \end{subfigure}
    \vspace{-.5em}
    \caption{\textbf{Example responses from \textcolor{red}{\gpt} and \textcolor{yellow}{\claude}, showcasing their idiosyncrasies}: characteristic phrases (\textit{left}) and unique markdown formatting (\textit{right}). For clarity, we highlight each characteristic phrase with \underline{underline} and model-specific color. 
    }
    \label{fig:example_responses}
\end{figure*}
\subsection{Words and Letters}
\label{sec:words_letters}

\textbf{Text shuffling.} 
To decouple the effects of words and letters from other factors, we remove special characters in LLM-generated responses, such as punctuations, markdown elements, and excessive white spaces. This ensures that each response consists solely of words separated by a white space. Additionally, we apply two shuffling strategies to the preprocessed text: word-level and letter-level shuffling. These transformations disrupt the natural order and force the classifier to learn patterns from raw text statistics. Table~\ref{tab:shuffle} presents the classification results. 

\begin{table}[ht]
    \centering
    \tablestyle{6pt}{1.15}
    \begin{tabular}{lccc}
    & chat & instruct & base \\
    \shline
    original  & \baseline{97.1} & \baseline{96.3} & \baseline{87.3} \\
    removing special characters & 95.1 & 93.8 & 75.4 \\
    shuffling words             & 88.9 & 88.9 & 68.3 \\
    shuffling letters        & 39.1 & 38.6 & 38.9
    \end{tabular}
    \caption{\textbf{Classifications with only words and letters.} While removing special characters and shuffling words have little impact on accuracies, shuffling letters greatly reduces the performance.}
    \label{tab:shuffle}
\end{table}

Classifiers trained on responses without special characters achieve accuracies close to those using the original responses, \ie, 95.1\% for chat APIs, 93.8\% for instruct LLMs, and 75.4\% for base LLMs. Likewise, using word-shuffled responses yields high accuracies comparable to the original ones. Further, we plot the frequencies of several commonly used words from five chat APIs in Figure~\ref{fig:word_letter_dist} (\textit{left}). We observe distinct patterns among models, even for frequent English words: Claude has much lower frequencies for words like ``the'', ``and'', ``to'', and ``of'' than other chat APIs. These results suggest that \emph{special characters and word order are not essential for distinguishing LLMs}; \emph{word choices reflect substantial idiosyncrasies across models.}

\begin{figure}[t]
    \centering
    \includegraphics[width=.495\linewidth]{figs/words_letters/top_words_chat-cropped.pdf}
    \includegraphics[width=.495\linewidth]{figs/words_letters/character_count_chat-cropped.pdf}
    \caption{\textbf{Frequencies of words and letters.} The top 20 most frequently used words of LLMs (left) exhibit distinct patterns for each model, but their letter frequencies (right) are very similar. Results are on the chat API models.}
    \label{fig:word_letter_dist}
    
\end{figure}



In contrast, shuffling at the letter level results in a substantial drop in accuracy (49\%-56\%), approaching chance-level performance. This indicates that letter-level statistics alone are not sufficient for predicting LLM identities. To qualitatively visualize distinctions in letter distributions across models, Figure~\ref{fig:word_letter_dist} (\textit{right}) shows the frequency distribution of letters in responses generated by chat APIs. Different LLMs share almost identical letter distributions, indicating that \emph{letters contribute minimally to idiosyncrasies in LLMs.} 



\textbf{Characteristic phrases.} We use Term Frequency-Inverse Document Frequency (TF-IDF) to highlight characteristic phrases inside LLM-generated responses that reflect each model's word choices. Formally, we treat each LLM response as a document and then extract TF-IDF features on all uni-gram and bi-gram words. We then train a $N$-way logistic regression model to predict the origin of responses on the extracted features. This simple linear classifier achieves 85.5\% / 83.7\% accuracy on chat APIs / instruct LLMs, close to 95.1\% / 93.8\% achieved with fine-tuning embedding models on responses without special characters (Table~\ref{tab:shuffle}). 


Since the coefficients of a logistic regression model provide a natural ranking for its features, we leverage these coefficients to highlight important phrases in the classification task. Figure~\ref{fig:top_words} presents the top 10 phrases with the largest logistic regression coefficients for each of the five chat API models. Notably, these phrases often serve as transitions or emphasis in sentences. For example, {\gpt} likes to generate ``such as", ``certainly'', and ``overall'', whereas {\claude} prefers ``here'', ``according to'', and ``based on''. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=.98\linewidth]{figs/representative_phrases/top_words_chat_v3.pdf}\vspace{-.5em}
\caption{\textbf{Characteristic phrases.} We train a logistic regression model on TF-IDF features of chat APIs' outputs and extract the top 10 phrases for each LLM based on the coefficients of these features. We remove common words shared across these LLMs.}
\label{fig:top_words}
\end{figure}



Figure~\ref{fig:example_phrases} illustrates these characteristic phrases with example responses from {\gpt} and {\claude}. While {\gpt} begins responses with ``certainly'' and ``below is'', Claude usually references the original prompt using the phrases like ``according to the text'' and ``based on the text''. Moreover, Figure~\ref{fig:first_words_chat} reveals noticeable differences in the distribution of first word choices among chat APIs. Appendix~\ref{appendix:top_phrases} provides characteristic phrases for other LLMs.


\subsection{Markdown Formatting}
\label{sec:markdown}
We seek to understand how each LLM formats their responses, particularly in markdown. To this end, we focus on common markdown elements used by LLMs: (1) bold text, (2) italic text, (3) header, (4) enumeration, (5) bullet point, (6) code block. We transform the LLM outputs by retaining only these formatting components while replacing other text with the marker ``xxx''. Appendix~\ref{appendix:response_demonstration} provides examples of the transformed outputs. Table~\ref{tab:remove_text} shows the classification results after this transformation.

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/first_word/first_words_chat.pdf}
    \vspace{-2em}
    \caption{\textbf{First word.} We analyze the distribution of the first word in chat APIs' responses, with the top 10 most frequent words for each model. These differences in first-word usage explain the non-trivial accuracy with only the first word in Figure~\ref{fig:seq_len}.}
    \vspace{-.8em}
    \label{fig:first_words_chat}
\end{figure}



\begin{table}[ht]
    \centering
    \tablestyle{6pt}{1.15}
    \begin{tabular}{lccc}
    & chat & instruct & base \\
    \shline
    original                & \baseline{97.1} & \baseline{96.3} & \baseline{87.3} \\
    markdown elements only & 73.1            & 77.7            &38.5 \\
    \end{tabular}
    \vspace{-.5ex}
    \caption{\textbf{Classifications with only markdown elements.} Using markdown elements can achieve high accuracies for chat APIs and instruct LLMs, but marginally better results for base LLMs.}
    \label{tab:remove_text}
\end{table}




Surprisingly, we observe our classifiers achieve high accuracies of 73.1\% for chat APIs and 77.7\% for instruct LLMs. However, the classification accuracies with base LLMs' responses are near chance-level guess (25\%). This is likely because base LLMs tend to generate responses in plain text.


We count the occurrence of a markdown formatting element in each response. We then plot the distribution of these counts over all responses in Figure~\ref{fig:features_dist}. Each model exhibits a unique way to format its responses. For instance, Claude (\textcolor{yellow}{\textbf{yellow}}) has a high density at zero in the bold text and header count distributions, indicating that it generates many responses without bold 
 texts or headers. On the contrary, other LLMs exhibit lower values at zero and thus decorate text with these formatting elements more often.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{figs/markdown_features/chat_bold_text_count-cropped.pdf}
    \includegraphics[width=0.49\linewidth]{figs/markdown_features/chat_header_count-cropped.pdf}
    \includegraphics[width=0.49\linewidth]{figs/markdown_features/chat_enumeration_count-cropped.pdf}
    \includegraphics[width=0.49\linewidth]{figs/markdown_features/chat_bullet_point_count-cropped.pdf}
    \caption{\textbf{Markdown formatting elements.} Each LLM has a distinctive distribution of markdown formatting elements.}
    \label{fig:features_dist}
\end{figure}

Figure~\ref{fig:example_special_characters} visualizes how {\gpt} and {\claude} structure their responses in markdown. Interestingly, {\gpt} tends to emphasize each key point within enumerations in bold and highlight a title with markdown headers, but {\claude} formats text with simple enumeration and bullet points. More analysis on these markdown formatting elements for other models can be found in Appendix~\ref{appendix:markdown}.

\subsection{Semantics}
\label{sec:semantics}
\textbf{Rewriting.} One potential reason for the high classification accuracy is the unique writing style (\eg, word choice, sentence structure) of each LLM. To isolate this factor, we leverage another LLM (\eg, GPT-4o mini) to rewrite LLM responses. Our rewriting approaches include (see Appendix~\ref{appendix:response_demonstration} for example  responses after rewriting):

\begin{itemize}[topsep=0.pt, itemsep=0.7pt,parsep=0pt,partopsep=-.5pt,leftmargin=12pt]
    \item Paraphrasing: \emph{Paraphrase the above text while maintaining the semantic meaning of the original text.}
    \item Translating: \emph{Translate the above text into Chinese.}
    \item Summarizing: \emph{Summarize the above text in one paragraph.}
\end{itemize} 




We show the results in Table~\ref{tab:rewriting}. The classifiers trained on paraphrased LLM responses maintain similar accuracy levels to those using original responses. Likewise, when using translated text, the classifiers are also able to differentiate between LLMs. These findings suggest that \emph{the semantic meanings of words play a more significant role in predicting LLM origins than the exact word choice.}


\begin{table}[ht]
    \centering
    \tablestyle{6pt}{1.15}
    \begin{tabular}{lccc}
    & chat & instruct & base \\
    \shline
    original  &  \baseline{97.8} & \baseline{96.3} & \baseline{87.3} \\
    paraphrasing    & 91.4 & 92.2 & 71.7 \\
    translating & 91.8 & 92.7 & 74.0 \\ 
    summarizing   & 58.1 & 57.5 & 44.7 \\
    \end{tabular}
    \vspace{-1ex}
    \caption{\textbf{Classifications on rewritten responses.} Paraphrasing or translating LLM outputs achieves an accuracy comparable to that using original counterparts. However, summarizing these texts makes the model less capable of predicting LLM identities.}
    \label{tab:rewriting}
    \vspace{-1ex}
\end{table}

Moreover, despite a noticeable accuracy drop (\ie, $>$38\%) with the summarized text, the resulting performance remains well above chance-level guess. This remarkable ability to classify the summarized texts shows the \emph{high-level semantic difference in LLM-generated responses.}









\begin{figure*}[th]
    \centering
    \includegraphics[width=.49\linewidth]{figs/summarization/GPT-4o_summarization_gpt4o.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/Claude_summarization_gpt4o.pdf}
    \vspace{-.5em}
    \caption{\textbf{Results of our open-ended language analysis on \textcolor{red}{\gpt} and \textcolor{yellow}{\textbf{\claude}}.} \textcolor{red}{\textbf{\gpt}} features descriptive language, sophisticated markdown formatting, and in-depth details, while \textcolor{yellow}{\textbf{\claude}} highlights straightforward tone, minimal structure, and summarized content.}
    \vspace{-.5em}
    \label{fig:open-ended}
\end{figure*}

\textbf{Open-ended language analysis.} In this part, we focus on studying the semantic difference in responses generated by LLMs. We employ another LLM (\eg, {\gpt}) as a judge to provide open-ended, descriptive characterizations for each LLM's outputs. The results with other LLM judges for our language analysis are available in Appendix~\ref{appendix:language}. 

Specifically, we present an LLM judge with two responses -- generated by different models based on the same prompt -- and ask it to analyze these responses from different angles (\eg, tone and content). This process is repeated multiple times to gather a comprehensive collection of analyses. Finally, we query the LLM judge to summarize these analyses into bullet points that capture the characteristics of each model. The prompts are detailed in Appendix~\ref{appendix:language_prompt}.

The results of open-ended language analysis on {\gpt} \vs {\claude} are shown in Figure~\ref{fig:open-ended}. For a detailed pairwise comparison of the responses, see Figure~\ref{appendix:fig:open-ended-language-analysis} in Appendix~\ref{appendix:response_demonstration}. {\gpt} is characterized by descriptive and detailed responses in an engaging tone. In contrast, {\claude} prioritizes simplicity with only key points and straightforward language. Additional results on chat API models and instruct LLMs are provided in Appendix~\ref{appendix:language}.

\section{Implications}
In this section, we explore the broader implications of our framework, regarding synthetic data and model similarity.

\textbf{Idiosyncrasies via synthetic data.} 
Using synthetic data has become a common practice when training frontier LLMs~\cite{abdin2024phi3technicalreporthighly,abdin2024phi4,liu2024bestpractices}. We conduct supervised fine-tuning (SFT) on two base LLMs (Llama3.1-8b and Gemma2-9b) using Ultrachat, i.e., dialogues generated by {\gpt}. After the SFT stage, we train a classifier to distinguish between responses from two fine-tuned models. We find that SFT on the same synthetic dataset significantly reduces the classification accuracy from 96.5\% to 59.8\%, narrowing down the differences between these two models. 

In addition, we generate responses from Llama3.1-8B and Gemma2-9B in instruct LLMs using UltraChat prompts. Then we fine-tune Qwen2.5-7B base LLM on each set of responses respectively. Interestingly, responses from the two resulting fine-tuned models can be classified with 98.9\% accuracy, suggesting that each fine-tuned model retains the unique characteristics in its SFT data. These findings suggest that training with synthetic data can propagate the idiosyncrasies in the source model. 



\textbf{Inferring model similarity.} Our framework offers a quantitative approach for assessing similarities between proprietary and open-weight LLMs. Given a set of $N$ LLMs, we omit one model and train a classifier on responses from the remaining $N-1$ models. We then evaluate which LLM the classifier associates the responses of the excluded model with. The model that is most frequently predicted as the source is considered the closest match to the excluded LLM. This process is repeated for each of the $N$ models. For this analysis, we include the open-weight Phi-4~\citep{abdin2024phi4} alongside 5 chat API models. Notably, Phi-4 uses a substantial amount of synthetic data in its training.


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/model_similarity.pdf}
    \vspace{-2.5ex}
    \caption{\textbf{Inferring model similarity.} We consider 6 LLMs, including 5 chat API models and Phi-4. In each subfigure, we evaluate a five-way classifier on outputs from the excluded LLM and present the distribution of predicted model origins. There is a strong tendency for LLM outputs to be predicted as ChatGPT.
    }
    \label{fig:leave_one_out}
    \vspace{.5em}
\end{figure}

Results are shown in Figure~\ref{fig:leave_one_out}. Intriguingly, for {\claude}, {\grok}, and {\gemini}, we observe a strong tendency for their outputs to be classified as {\gpt}. For instance, when {\grok} is the excluded model, 82.8\% of its responses are classified as {\gpt}. In addition, responses from {\gpt} and {\deepseek} are frequently identified as coming from Phi-4, with 55.9\% and 76.0\% of their responses respectively. In turn, most of Phi-4's outputs are classified as originating from {\gpt} or {\deepseek}.











\section{Related Work}

\textbf{Dataset classification.} \citet{torralba2011unbiased} introduced the ``Name That Dataset” experiment a decade ago to highlight the bias present in visual datasets of that time. Recently, \citet{liu2024datasetbias} revisited this problem (termed dataset classification) and found that current large-scale, supposedly more diverse visual datasets are still very biased. \citet{zeng2024bias} further identified structural and semantic components in images as key contributors to these biases. \citet{you2024images} and \citet{mansour2024bias} applied the dataset classification framework to study bias in synthetic images and LLM pretraining datasets respectively. 
While the synthetic task shown in  Figure~\ref{fig:teaser} is conceptually similar to dataset classification, we focus not on training datasets but on the distinctive characteristics inherent to LLMs.








\textbf{Human \vs machine-generated texts.} Many prior works have studied the problem of determining if a text is authored by a human or an AI system~\citep{mitchell2023detectgpt,mcgovern2024fingerprints}. Model-free approaches typically use linguistic properties such as n-gram frequencies~\citep{badaskar-etal-2008-identifying,openai_gpt2_dataset}, entropy~\citep{thomas2008entropy,gehrmann2019gltr} or negative probability curvature~\citep{mitchell2023detectgpt,bao2023fastdetectgpt}. Other works leverage neural network features to perform this task, such as fine-tuning BERT models~\citep{uchendu2021turingbench,ippolito2019automatic}. Neural authorship attribution~\citep{uchendu-etal-2020-authorship,huang2024authorship} seeks not only to identify machine-generated text but also to attribute it to specific text generators. In this work, we focus on the distinguishability between LLMs rather than between AI \vs human.

\textbf{Understanding differences between distributions.} A line of research~\citep{dunlap2023describing, zhong2024explaining} has used foundation models to describe qualitative differences between pairs of data distributions (\eg, image datasets). \citet{gao2024model} conducted hypothesis testing on sets of model outputs to check whether the underlying LLMs were identical. The most relevant work to us is \citet{dunlap2024vibecheck}, which proposed VibeCheck to understand user-aligned traits in LLM outputs. They found that LLMs often vary in styles, such as being more formal or friendly. In contrast, our work aims to identify broader generalizable patterns to interpret the high classification performance.


\section{Conclusion}
We demonstrate the presence of idiosyncrasies in Large Language Models (LLMs) and investigate a synthetic task designed to quantify their extent. We find that simply fine-tuning pretrained text embedding models on LLM outputs leads to exceedingly high accuracy in predicting the origins of the text. This phenomenon persists across diverse prompt datasets, LLM combinations, and many other settings. We also pinpoint concrete forms of these idiosyncrasies within LLMs. We hope our work encourages further research into understanding idiosyncrasies in LLMs.

\section*{Acknowledgments} 
We thank Zekai Wang for valuable discussions. Mingjie Sun was supported by funding from the Bosch Center for Artificial Intelligence. 








\bibliography{refs}
\bibliographystyle{icml2025}



\newpage
\clearpage

\appendix
\onecolumn

\section*{Appendix}
\section{Implementation Details}\label{appendix:implementation}
\subsection{Response Generation}\label{appendix:response_gen}
We report our procedure for generating responses from chat APIs, instruct LLMs, and base LLMs. For chat APIs, we access a stable version of each model, including GPT-4o-2024-08-06, Claude-3.5-Sonnet-20241022, Grok-Beta, Gemini-1.5-Pro-002, and DeepSeek-Chat, through its official API between November 28, 2024, and February 6, 2025, generating responses with their default sampling setting. For instruct LLMs, we use greedy decoding to sample outputs. For base LLMs, we set the temperature to $T=$ 0.6 and apply a repetition penalty of 1.1 to avoid repetitive completions.


\subsection{Training Setup}
\label{appendix:finetune}
In this part, we describe our fine-tuning process using the text embedding models on LLM responses. We use the first 512 tokens of each generated response for training and evaluation. To perform sequence classification, we add a linear layer as the classification head on top of each text embedding model. For ELMo, BERT, LLM2vec, this layer is applied to the average embeddings over all tokens in a sequence. For T5 and GPT-2, we follow the original setups~\cite{radford2019gpt2, raffel2023T5} and apply the head on the output of the last token. 

For smaller text embedding models, such as ELMo, BERT, T5, and GPT-2, we fine-tune the entire model along with the classification head, searching over base learning rates $\{$3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5, 3e-6, 1e-6$\}$. For the largest LLM2vec model, we employ the parameter-efficient LoRA~\cite{hu2022lora} fine-tuning method with a rank of 16, LoRA $\alpha$ of 32, a dropout rate of 0.05, and a base learning rate of 5e-5. Table~\ref{tab:recipe} details our basic training recipe.


\begin{table}[ht]
\centering
\small
\addtolength{\tabcolsep}{-.3pt}
\def\arraystretch{1.2}
\begin{tabular}{y{108}|x{96}}
config & value \\
\Xhline{0.7pt}
optimizer & AdamW \\
weight decay & 0.001 \\
optimizer momentum & $\beta_1, \beta_2=0.9, 0.999$ \\
training epochs & 3 \\
batch size & 8 \\
learning rate schedule & cosine decay \\
warmup schedule & linear \\
warmup ratio & 10\% \\
gradient clip & 0.3 \\


\end{tabular}
\caption{Our fine-tuning recipe.}
\label{tab:recipe}
\end{table}

\clearpage 
\newpage 
\subsection{Prompts for Open-ended Language Analysis}
\label{appendix:language_prompt}

We detail the procedures of our open-ended language analysis in Section~\ref{sec:semantics}. Given the same input, we sample a pair of responses from two LLMs and present them, along with an analysis prompt (see Figure~\ref{fig:analysis_prompt}), to an LLM judge for comparison. To avoid the LLM judge exploiting any prior knowledge of the models, we anonymize model identities using an index distribution. This process is repeated for 35 response pairs, yielding a set of detailed analyses. Finally, we use the summarization prompt (see Figure~\ref{fig:summary_prompt}) to distill these analyses into 5 bullet points that characterize the idiosyncrasies of each model.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[h]{0.45\linewidth}
    \includegraphics[width=\linewidth]{figs/diff_prompt.pdf}
    \subcaption{analysis prompt}
        \vspace{2ex}
    \label{fig:analysis_prompt}
    \end{subfigure}
    \hspace{2ex}
    \begin{subfigure}[h]{0.45\linewidth}
    \includegraphics[width=\linewidth]{figs/summary_prompt.pdf}
    \subcaption{summarization prompt}
    \label{fig:summary_prompt}
    \end{subfigure}
    \caption{Prompts in our open-ended language analysis.}
    \label{fig:prompt_open_ended_language_analysis}
\end{figure}



\section{Additional Results}
\label{appendix:additional_results}


\subsection{Confusion Matrix}\label{appendix:section_confusion_matrix}
In Figure~\ref{fig:confusion}, we present the confusion matrix for the $N$-way classifiers that are trained on responses generated by chat APIs, instruct LLMs, and base LLMs, respectively. The results demonstrate that our classifiers can accurately predict the origin of LLM-generated responses, with minimal confusion between different LLMs. %
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/confusion_matrix/confusion_matrix_five_way_chat.pdf}
        \vspace{-3ex}
        \subcaption{chat APIs}
    \end{subfigure}~
    \begin{subfigure}[h]{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/confusion_matrix/confusion_matrix_instruct.pdf}
        \vspace{-3ex}
        \subcaption{instruct LLMs}
    \end{subfigure}~
    \begin{subfigure}[h]{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/confusion_matrix/confusion_matrix_base.pdf}
        \vspace{-3ex}
        \subcaption{base LLMs}
    \end{subfigure}
    \caption{Confusion matrices for $N$-way classifiers on three groups of LLMs: chat APIs, instruct LLMs, and base LLMs.}
    \label{fig:confusion}
\end{figure}
\label{appendix:confusion_matrix}

\subsection{Words and Letters}
\label{appendix:char_distribution}
Figure~\ref{fig:words_and_letters_instruct_base} presents the frequencies of the 20 most commonly used words (\textit{left}) and all English letters (\textit{right}) across instruct and base LLMs. Consistent with our observations in Section~\ref{sec:words_letters}, we find notable differences in the distribution of commonly used words between these models, such as ``the'', ``and'', ``to''. In contrast, the letter distributions are nearly identical.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.49\linewidth}
    \centering
    \includegraphics[width=0.49\linewidth]{figs/words_letters/top_words_instruct-cropped.pdf}
    \includegraphics[width=0.49\linewidth]{figs/words_letters/character_count_instruct-cropped.pdf}
    \subcaption{instruct LLMs}
    \label{fig:top_word_instruct}
    \end{subfigure}
    \begin{subfigure}[h]{0.49\linewidth}
    \centering
    \includegraphics[width=0.49\linewidth]{figs/words_letters/top_words_base-cropped.pdf}
    \includegraphics[width=0.49\linewidth]{figs/words_letters/character_count_base-cropped.pdf}
    \subcaption{base LLMs}
    \label{fig:top_word_base}
    \end{subfigure}
    \caption{Word and letter frequencies in instruct and base LLMs.}
    \label{fig:words_and_letters_instruct_base}
\end{figure}

\subsection{Characteristic Phrases}
\label{appendix:top_phrases}
We provide additional results for characteristic phrases as presented in Section~\ref{sec:words_letters}. We follow the same methodology in Figure~\ref{fig:top_words} to extract characteristic phrases of instruct and base LLMs. Specifically, we train a four-way logistic regression classifier on the TF-IDF features of their responses and use the coefficients to select important phrases of each model. 



As shown in Figure~\ref{fig:char_phrases_instruct_base}, each instruct LLM contains quite distinct characteristic phrases. For example, Llama frequently employs terms ``including'' and ``such as'' to introduce specific examples in the output, whereas Gemma tends to engage with users using phrases ``let me'' and ``know if''. In contrast, the extracted phrases from base LLMs are less distinctive, primarily consisting of common words such as “the”, “to”, and “you”. 

Figure~\ref{fig:first_words_instruct_base} illustrates the distribution of first word choices in instruct and base LLMs. Similar to chat APIs (Figure~\ref{fig:first_words_chat}), instruct LLMs display varied distributions. However, base LLMs exhibit substantial overlap in their most frequent first words, \eg, ``the'', ``and'', ``of'', ``to'', and ``in''.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[h]{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/representative_phrases/top_words.pdf}
        \subcaption{instruct LLMs}
        \vspace{-1ex}
    \end{subfigure}
    \begin{subfigure}[h]{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/representative_phrases/top_words_base.pdf}
        \subcaption{base LLMs}
        \vspace{-1ex}
    \end{subfigure}
    \centering
    \caption{Characteristic phrases for instruct\protect\footnotemark and base LLMs.}
    \label{fig:char_phrases_instruct_base}
\end{figure}
\footnotetext{In LLama of instruct LLMs, the phrase ``explanation the''  corresponds to a markdown header or bold text for ``explanation'' followed by a new sentence starting with ``the''.}


\begin{figure*}[h]
    \centering
    \begin{subfigure}[h]{.495\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/first_word/first_words_instruct.pdf}
        \subcaption{instruct LLMs}
    \end{subfigure}
    \begin{subfigure}[h]{.495\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/first_word/first_words_base.pdf}
        \subcaption{base LLMs}
    \end{subfigure}
    \centering
    \vspace{-1ex}
    \caption{Distribution of first word choices in instruct and base LLMs.}
    \label{fig:first_words_instruct_base}
\end{figure*}




\clearpage
\newpage
\subsection{Unique Markdown Formatting}
\label{appendix:markdown}

In this part, we provide additional results for the analysis of markdown formatting as presented in Section~\ref{sec:markdown}. Figure~\ref{fig:features_dist_instruct_base} illustrates the distribution counts of six markdown formatting elements across different models. For both chat API models (Figure~\ref{appendix:markdown_chatapis}) and instruct LLMs (Figure~\ref{appendix:markdown_instructllms}), we observe distinct differences in the usage of bold texts, headers, enumerations, and bullet points, while italic texts show less variation. Intriguingly, {\gemini} uses much more italic texts (a lower density at zero in the italic text) than other chat APIs, where similar observations can be found on Gemma2. 

\begin{figure*}[th]
    \centering
    \begin{subfigure}[h]{\linewidth}
        
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/chat_bold_text_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/chat_italic_text_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/chat_header_count-cropped.pdf}
    
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/chat_enumeration_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/chat_bullet_point_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/chat_code_block_count-cropped.pdf}
    \subcaption{chat APIs}
    \vspace{2.5ex}
    \label{appendix:markdown_chatapis}
    \end{subfigure}
    
    \begin{subfigure}[h]{\linewidth}
        
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/instruct_bold_text_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/instruct_italic_text_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/instruct_header_count-cropped.pdf}
    
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/instruct_enumeration_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/instruct_bullet_point_count-cropped.pdf}~
    \includegraphics[width=0.33\linewidth]{figs/markdown_features/instruct_code_block_count-cropped.pdf}
    \subcaption{instruct LLMs}
    \label{appendix:markdown_instructllms}
    \end{subfigure}
    \caption{Markdown formatting elements for chat APIs (\emph{top}) and instruct LLMs (\emph{bottom}).}
    \label{fig:features_dist_instruct_base}
\end{figure*}


\clearpage
\newpage

\subsection{Open-ended Language Analysis}
\label{appendix:language}

\textbf{Ablation on LLM judges.} Here we demonstrate our findings in Figure~\ref{fig:open-ended} of Section~\ref{sec:semantics} remains consistent under several LLM judges. Specifically, we change the LLM judge from {\gpt} to {\claude}, {\grok}, and {\gemini}. We show the results in Figure~\ref{fig:open-ended_llm_judge}. Regardless of the choice of LLM judges, our language analysis reveals that {\gpt} often uses detailed explanations and complex formatting structures, whereas {\claude} emphasizes key contents without extensive elaboration. 



\begin{figure*}[th]
    \centering
    \begin{subfigure}[h]{\linewidth}
    \includegraphics[width=.49\linewidth]{figs/summarization/judge_ablation/GPT-4o_summarization_claude.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/judge_ablation/Claude_summarization_claude.pdf}
    \vspace{-1ex}
    \subcaption{{\claude} as the LLM judge.}
    \vspace{2.5ex}
    \end{subfigure}
    
    \begin{subfigure}[h]{\linewidth}
    \includegraphics[width=.49\linewidth]{figs/summarization/judge_ablation/GPT-4o_summarization_grok.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/judge_ablation/Claude_summarization_grok.pdf}
    \vspace{-1ex}
    \subcaption{{\grok} as the LLM judge.}
    \vspace{2.5ex}
    \end{subfigure}

        
    \begin{subfigure}[h]{\linewidth}
    \includegraphics[width=.49\linewidth]{figs/summarization/judge_ablation/GPT-4o_summarization_gemini.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/judge_ablation/Claude_summarization_gemini.pdf}
    \vspace{-1ex}
    \subcaption{{\gemini} as the LLM judge.}
    \end{subfigure}
    \caption{Results of our open-ended language analysis on \textcolor{red}{{\gpt}} and \textcolor{yellow}{\textbf{{\claude}}} with different LLM judges.}
    \label{fig:open-ended_llm_judge}
\end{figure*}



\clearpage
\newpage
\textbf{Open-ended language analysis results on other LLMs.} In Section~\ref{sec:semantics}, we presented the results of open-ended language analysis for {\gpt} and {\claude}. Here, we extend our analysis to other chat API models and instruct LLMs. The full results are shown in Figure~\ref{fig:open-ended_full}, where we use {\gpt} as the LLM judge to compare responses generated by two models within the same category (chat APIs / instruct LLMs). Our analysis highlights several interesting characteristics of each model. For example, {\grok}'s responses tend to feature rich language and comprehensive content, whereas {\gemini}'s outputs are more concise with direct openings. 


\begin{figure*}[th]
    \centering
    \begin{subfigure}[h]{\linewidth}
    \includegraphics[width=.49\linewidth]{figs/summarization/other_models/Grok_summarization_gpt4o.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/other_models/Gemini_summarization_gpt4o.pdf}
    \vspace{-1ex}
    \subcaption{chat APIs}
    \vspace{2.5ex}
    \end{subfigure}
    \centering
    \begin{subfigure}[h]{\linewidth}
    \centering
    \includegraphics[width=.49\linewidth]{figs/summarization/other_models/Llama31_summarization_gpt4o.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/other_models/Gemma2_summarization_gpt4o.pdf}
    \vspace{2ex}
    \end{subfigure}
    \begin{subfigure}[h]{\linewidth}
    \includegraphics[width=.49\linewidth]{figs/summarization/other_models/Qwen25_summarization_gpt4o.pdf}~
    \includegraphics[width=.49\linewidth]{figs/summarization/other_models/Mistralv3_summarization_gpt4o.pdf}
    \vspace{-1ex}
    \subcaption{instruct LLMs}
    \end{subfigure}
    \caption{Additional results of our open-ended language analysis on chat APIs (\textit{top}) and instruct LLMs (\textit{bottom}).}
    \label{fig:open-ended_full}
\end{figure*}

\clearpage
\newpage
\section{Response Demonstrations}\label{appendix:response_demonstration}
In this part, we present examples of LLM responses. Table~\ref{appendix:fig:prompt-gpt4o} and \ref{appendix:fig:prompt-llama3.1-8b-it} illustrate responses before and after our prompt-level interventions (Section~\ref{sec:main_control}). Table~\ref{appendix:fig:markdown_response} shows the transformed responses when only markdown elements are retained (Section~\ref{sec:markdown}). Table~\ref{appendix:fig:gpt-4o-rewrite} presents the rewritten responses (Section~\ref{sec:semantics}). Additionally, Table~\ref{appendix:fig:open-ended-language-analysis} provides a pairwise comparison of responses, supporting our findings in Table~\ref{fig:open-ended}. Table~\ref{appendix:fig:response_chatgpt}, ~\ref{appendix:fig:response_claude}, ~\ref{appendix:fig:response_grok}, ~\ref{appendix:fig:response_gemini}, ~\ref{appendix:fig:response_deepseek}, ~\ref{appendix:fig:response_llama3.1-8b-it}, ~\ref{appendix:fig:response_gemma2-9b-it}, ~\ref{appendix:fig:response_qwen2.5-7b-it}, ~\ref{appendix:fig:response_mistral-v3-7b-it}, ~\ref{appendix:fig:response_llama3.1-8b-base}, ~\ref{appendix:fig:response_gemma2-9b-base}, ~\ref{appendix:fig:response_qwen2.5-7b-base}, ~\ref{appendix:fig:response_mistral-v3-7b-base} contain example responses from each LLM considered in Section~\ref{sec-evaluate-idiosyncrasies}, including {\gpt}, {\claude}, {\grok}, {\gemini}, {\deepseek}, LLama3.1-8b (instruct), Gemma2-9b (instruct), Qwen2.5-7b (instruct), Mistral-v3-7b (instruct), LLama3.1-8b (base), Gemma2-9b (base), Qwen2.5-7b (base), and Mistral-v3-7b (base).

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/gpt4o_prompt_control-cropped.pdf}
    \caption{Examples of prompt-level interventions on \gpt.}
    \label{appendix:fig:prompt-gpt4o}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/llama_prompt_control-cropped.pdf}
    \caption{Examples of prompt-level interventions on Llama3.1-8b-Instruct.}
    \label{appendix:fig:prompt-llama3.1-8b-it}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/only_markdown-cropped.pdf}
    \caption{Examples of only using markdown formatting elements and replacing text content with ``xxx'' placeholders.}
    \label{appendix:fig:markdown_response}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/rewrite-cropped.pdf}
    \caption{Examples of LLM rewriting using GPT-4o-mini.}
    \label{appendix:fig:gpt-4o-rewrite}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/language_analysis-cropped.pdf}
    \caption{Examples from {\gpt} and {\claude} that illustrate results of our open-ended language analysis in Figure~\ref{fig:open-ended}.}
    \label{appendix:fig:open-ended-language-analysis}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/gpt4o-cropped.pdf}
    \caption{Example response of {\gpt}.}
    \label{appendix:fig:response_chatgpt}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/claude-cropped.pdf}
    \caption{Example response of {\claude}.}
    \label{appendix:fig:response_claude}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/grok-cropped.pdf}
    \caption{Example response of {\grok}.}
    \label{appendix:fig:response_grok}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/gemini-cropped.pdf}
    \caption{Example response of {\gemini}.}
    \label{appendix:fig:response_gemini}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/deepseek-cropped.pdf}
    \caption{Example response of {\deepseek}}
    \label{appendix:fig:response_deepseek}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/llama_instruct-cropped.pdf}
    \caption{Example response of Llama3.1-8b (instruct).}
    \label{appendix:fig:response_llama3.1-8b-it}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/gemma_instruct-cropped.pdf}
    \caption{Example response of Gemma2-9b (instruct).}
    \label{appendix:fig:response_gemma2-9b-it}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/qwen_instruct-cropped.pdf}
    \caption{Example response of Qwen2.5-7b (instruct).}
    \label{appendix:fig:response_qwen2.5-7b-it}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/mistral_instruct-cropped.pdf}
    \caption{Example response of Mistral-v3-7b (instruct).}
    \label{appendix:fig:response_mistral-v3-7b-it}
\end{table*}

\clearpage
\newpage
\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/llama_base-cropped.pdf}
    \caption{Example response of Llama3.1-8b (base).}
    \label{appendix:fig:response_llama3.1-8b-base}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/gemma_base-cropped.pdf}
    \caption{Example response of Gemma2-9b (base).}
    \label{appendix:fig:response_gemma2-9b-base}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/qwen_base-cropped.pdf}
    \caption{Example response of Qwen2.5-7b (base).}
    \label{appendix:fig:response_qwen2.5-7b-base}
\end{table*}

\begin{table*}[h]
    \centering
    \includegraphics[width=.95\linewidth]{example_responses/mistral_base-cropped.pdf}
    \caption{Example response of Mistral-v3-7b (base).}
    \label{appendix:fig:response_mistral-v3-7b-base}
\end{table*}
\end{document}
