\section{Related Work}
\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/dataset_delighting.pdf}
	\caption{Some materials in our test dataset, captured on the same flatbed scanner using directional and diffuse illuminations, better suited for material capture. }
	\label{fig:dataset_delighting}
\end{figure*}

\label{sec:related_work}

\paragraph*{\textbf{Single-Image Material Capture}} Estimating full reflectance properties of a material, using only a single image of it, is a challenging problem which has been tackled extensively in the literature in the recent years. These approaches can be categorized based on the estimation method, the imaging device employed, and the range of digitizable reflectance properties.


Neural style transfer~\cite{gatys2015neural} can be leveraged for single-image capture of stochastic materials, by matching the latent statistics of input images and renders of estimated SVBRDFs~\cite{henzler2021neuralmaterial, aittala2016reflectance}. A more common approach is to train an image-to-image translation model which takes a single image as input and estimates the set of SVBRDF maps. Originally supervised using pixel-wise or render-aware losses~\cite{deschaintre2018single,li2018materials,ye2018single,gao2019deep}, these methods have been improved by incorporating cascaded estimation~\cite{li2018learning, sang2020single}, adversarial losses~\cite{rodriguezpardo2023UMat, wen2022svbrdf, guo2021highlight,zhou2021adversarial,zhou2022tilegen,vecchio2021surfacenet},  inference-time optimization~\cite{gao2019deep}, or refinement~\cite{luo2024single}. More recently, diffusion models have emerged as powerful material estimators, showing competitive results~\cite{vecchio2023controlmat,vecchio2023matfuse,yuan2024diffmat,Sartor:2023:MFA}. A complementary line of work uses procedural graphs for material estimation~\cite{shi2020match, hu2022inverse, guo2020bayesian,jin2022woven}.


In terms of devices, the most common setup encompasses fronto-planar flash-lit images captured with a smartphone. Other setups trade this simplicity for quality, such as LCD screens~\cite{aittala2013practical,zhang2023deep,xu2023unified}, or high-end flatbed scanners~\cite{rodriguezpardo2023UMat}. Single-image material estimation methods typically estimate a reduced number of SVBRDF parameters, with the exception of~\cite{vecchio2023controlmat}, which also estimates opacity. 

Our approach differs from previous work in two ways. First, we provide a generic framework for material capture from~\emph{any} flatbed scanner, with arbitrary directional illumination, effectively removing the limitations in~\cite{rodriguezpardo2023UMat}. Furthermore, to the best of our knowledge, our method is the first single-image material capture which can estimate a full SVBSDF of a material, incorporating important effects like transmittance and opacity while preserving a high level of accuracy and resolution. 


\paragraph*{\textbf{Material Delighting}} Removing shading and specular highlights from images has been explored extensively in the literature, with particular focus on removing strong shadows~\cite{qu2017deshadownet,Vasluianu_2023_CVPR,Fu_2021_CVPR,Wang_2018_CVPR} and human relighting~\cite{lagunas2021single,yeh2022learning,wimbauer2022rendering,ji2022geometry}. In the context of BRDF estimation, material delighting has been explored by combining convolutional neural networks with Poisson optimization~\cite{martin2019lighting}. Our method also leverages material delighting for albedo estimation, by incorporating ideas from intrinsic image decomposition~\cite{garces2022survey}.

\paragraph*{\textbf{Cycle-Consistent Generative Models}} Learning to map from two distinct image domains for image-to-image translation tasks can be tackled through cycle-consistent generative models~\cite{zhu2017unpaired}. By introducing the cycle-consistency loss, these models enable accurate and diverse mappings. These have shown impressive results on a wide variety of applications, including stenography~\cite{chu2017cyclegan}, voice conversion~\cite{kaneko2019cyclegan}, medical imaging~\cite{yang2020unsupervised,harms2019paired}, face generation~\cite{lu2018attribute}, or improving diffusion models~\cite{wu2022unifying,su2022dual}. Inspired by these methods, we leverage cycle-consistency to train a model capable of both material delighting and relighting, which showcases high accuracy in both tasks under several metrics. 
