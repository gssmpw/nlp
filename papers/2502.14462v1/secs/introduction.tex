\section{Introduction}
\label{sec:introduction}


Several industries, such as architectural and fashion design, or media and gaming, benefit from realistic digital replicas of physical materials. Yet, crafting these copies remains a laborious and slow task, demanding skilled artists, or sophisticated and expensive hardware~\cite{garces2023towards,TAC7}. Consequently, recent research has focused on devising affordable and user-friendly capture setups.

In this scenario, flatbed scanners have emerged as promising tools for high-resolution material capture~\cite{rodriguezpardo2023UMat}, owing to their user-friendly nature and provision of uniform illumination conditions. High-end scanners can even offer a lighting type closely resembling diffuse illumination, usable directly as an albedo image~\cite{rodriguezpardo2023UMat}. Nevertheless, most scanners lack this functionality, with a majority featuring a single directional light that leads to undesirable micro-specular reflections, directional shading, and cast shadows (depicted in Figure~\ref{fig:teaser}(a) and \ref{fig:dataset_delighting}).

In this work, we address the drawbacks of prior approaches and introduce a technique for digitizing materials using any scanner, removing undesirable shading and specular highlights. We show that the na\"ive solution employing an image-to-image translation network~\cite{martin2019lighting, rodriguezpardo2023UMat} falls short for this purpose. Instead, we suggest employing a cycle-consistency loss in combination with a residual formulation inspired by intrinsic image decomposition methods~\cite{garces2022survey}.

In addition, a key contribution of our method is to expand the realism of the digital replica by including opacity and transmittance in the material model. These attributes are critical for thin-layer materials, like textiles, but have been neglected in current literature. We estimate the parameters of a Spatially-Varying Bidirectional Scattering Distribution Function (SVBSDF) that can reproduce complex effects of light as it passes through the material, thereby augmenting its realism in virtual environments.

We evaluate our method using extensive and thorough experiments, leveraging image-based metrics that measure the precision of each map individually, and render-aware metrics that measure the final appearance of the material in a global context. We further demonstrate that our method works with a variety of scanning devices, producing effective results even with less controllable devices such as smartphones. 




