\subsection{\textbf{Qualitative Results}}
\label{sec:results}

Figure~\ref{fig:qualitative_delighting} shows qualitative results of our material delighting and relighting models, along with ground truth data. Our delighting model behaves accurately even in challenging cases, like the corduroy on the first column, the satin on the third or the suede leather on the last one. The predicted images contain no shading, wrinkles are hidden and shadows casted on the scanner lid are eliminated. It can be seen why the material relighting model is less accurate according to our metrics. Precisely introducing shadows, specular highlights or shading proves to be a more challenging task than removing them, and our relighting model sometimes misplaces or inaccurately estimates the intensity of these reflections. We believe that, during training using cycle-consistency, this helps the delighting model as this works as a short of data augmentation, as the delighting model is shown variations of the same material with different variations on shading and specularity.

\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/qualitative_delighting.pdf}
	\caption{Qualitative results of our material delighting framework. On the first two rows, we show images captured with flatbed scanners under diffuse (top) and directional (bottom) illumination. We use those as input to our delighting $\mathcal{M}_D$ and relighting $\mathcal{M}_R$ models, respectively, for which we show the results on the bottom rows. }
	\label{fig:qualitative_delighting}
\end{figure*}


Figure~\ref{fig:comp_umat} compares our outputs with UMat~\cite{rodriguezpardo2023UMat} for a variety of material types. 
We show the results on a render scene designed to better show the impact of material transparency. As shown, our model behaves accurately across many different materials, with precise and sharp albedo estimations, and realistic transparency and opacity predictions. These results highlight the importance of estimating these maps, as the results of UMat are less appealing and realistic in comparison. 




\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/comparisons_umat.pdf}
	\caption{Qualitative comparisons of our method with UMat~\cite{rodriguezpardo2023UMat} for a few representative materials in our test set, with strong shading (leftmost columns), transparency (middle) or holes (rightmost). We show the input image (top row), and renders using the ground truth materials (captured with a gonioreflectometer), the estimation of~\cite{rodriguezpardo2023UMat} and ours, on the second, third and fourth rows, respectively. Best viewed in color on a screen.}
	\label{fig:comp_umat}
\end{figure*}


In Figure~\ref{fig:comparisons}, we show results of different methods of material capture, for which we capture the input images using a smartphone. We include results on ambient lighting (top row) and flash-lit images (last two rows). The methods of Deep Inverse Rendering~\cite{gao2019deep}, Match~\cite{shi2020match}, Neural Materials~\cite{henzler2021neuralmaterial}, Adversarial SVBRDF Estimation~\cite{zhou2021adversarial} all assume a smartphone capture, while UMat~\cite{rodriguezpardo2023UMat} and our method assume a flatbed scanner capture. Regardless on the illumination conditions, our model provides sharp and accurate estimations which better preserve the structure and color of the inputs compared to generative or optimization-based models~\cite{gao2019deep, shi2020match, henzler2021neuralmaterial}. Compared to UMat~\cite{rodriguezpardo2023UMat}, our delighting framework enables more uniform and higher-quality albedos, and we achieve more globally coherent maps than~\cite{zhou2021adversarial}. Overall, our method proves robust to images captured with smartphones across a variety of illumination conditions, even if we never train our models with this type of data. Note that this comparisons are done in a qualitative fashion as there is no accessible ground truth for their SVBRDF maps.

\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/comparisons_prev.pdf}
	\caption{Comparisons of our method with previous work on images captured with a smartphone, using ambient lighting (top row) and flash illumination (bottom two rows), at different levels of resolution. From left to right, we show input images, and the results of Deep Inverse Rendering~\cite{gao2019deep}, Match~\cite{shi2020match}, Neural Materials~\cite{henzler2021neuralmaterial}, Adversarial SVBRDF Estimation~\cite{zhou2021adversarial}, UMat~\cite{rodriguezpardo2023UMat}, and ours. Note that we only show the four reflectance maps used by every method: albedo, normals, specular and roughness.}
	\label{fig:comparisons}
\end{figure*}



In Figure~\ref{fig:qualitative_scanners}, we show the results achieved by our model on the same material, across a variety of flatbed scanners. This material is challenging, containing holes, wrinkles and fly-away fibers, all of which pose problems for digitization. The inputs on the first two rows were captured with a high-end EPSON V850 Pro scanner, for which we show the ground truth materials and renders (first), and the estimation on the directional light configuration on said scanner. On the third and fourth rows, we show the results on lower-end Epson V600 and V500 flatbed scanners, and the final row was captured with a Brother 9930 multi-functional fax machine, which also contains a budget scanner. As shown, our model estimations remain consistent across devices, regardless on the quality of the input scanner. The results for Brother 9930 struggle in terms of color due to calibration issues, but our SVBSDF estimation contains sharp, accurate reflectance maps. More results are shown in the supplementary material. 


\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/qualitative_scanners.pdf}
	\caption{Qualitative comparison between several flatbed scanners for the same material. Note that the images are not exactly pixel-wise coherent across scanners. }
	\label{fig:qualitative_scanners}
\end{figure*}



\subsection{\textbf{Failure Cases and Limitations}} 

Our method inherits the limitations of using flatbed scanners as a capture device. This setup cannot be used for non-flat materials (eg the marble in a statue) or materials which cannot physically be placed into this setup (eg a wall). It is also limited by our material model of choice, which, while it is more expressive than those of previous work, it cannot accurately represent complex phenomena such as anisotropy, strong displacements, high reflectivity, or subsurface scattering. Also, in order to capture non-uniform multi spectral absorption, $\mathbf{T}$ would require an additional attenuation value for each wavelength channel. Finally, our model sometimes struggles with some complex materials for which a single image is not a sufficient cue to estimate its optical properties. Such is the case for the bright, thick leather we show in Figure~\ref{fig:failure}, which is the material in our test set with the highest $\mathcal{L}_{\text{\tiny{BSDF}}}$.%
This type of material is also uncommon in our training dataset, which also explains the reduced generalization.




\begin{figure*}[tb!]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/failure_case_delighting.pdf}
	\caption{A failure case of our method. For the input image on the right, we show the ground truth albedo, normals, specular, roughness, transmittance and opacity maps (top row), and our model estimations. }
	\label{fig:failure}
\end{figure*}
