\subsection{\textbf{Material Delighting}}
\label{sec:delighting}


In this step, our goal is to estimate an albedo-like reflectance map $I_{d} \approx \mathcal{A}$ from a single image $I_{l}$ of the material taken under any kind of uniform lighting. 
We term this process \textit{delighting}, as we aim to remove specular reflections, shadings, or shadows. 
A straightforward solution to this problem would be to train an image-to-image translation approach with labeled data. However, as we demonstrate, this baseline approach does not achieve the desired level of accuracy due to the under-constrained nature of the problem and our relatively reduced training dataset (see Table \ref{tab:delighting_quantitative}). Therefore, we propose a more sophisticated architecture to improve this performance, which uses residual learning and a cycle-consistency loss. 
Inspired by intrinsic image decomposition~\cite{garces2022survey}, we formulate the \textit{delighting} problem as estimating a residual layer $\mathcal{M}_R$ that adds to the albedo image to form a \textit{lighted} image, $I_{l} = I_{d} + \mathcal{M}_R (I_{d})$. Similarly, within our cycle-consistent architecture, the equivalent inverse operation also exists, and we term it \textit{relighting}, $I_{d} = I_{l} + \mathcal{M}_D (I_{l})$.
Our residuals $\mathcal{M}_R(I_{d})$ and $\mathcal{M}_D(I_{l})$ are RGB images to make the estimation more flexible, thereby removing the assumption that either the source or reflected lights are white. Figure~\ref{fig:delighting_diagram} presents an overview of the architecture.





\paragraph*{\textbf{Loss Function}}

Our loss for each branch of our cycle-consistency model is a combination of pixel-wise, perceptual, frequency, and adversarial losses,
\begin{align} 
	\Loss_\textrm{im} (\cdot, \cdot) &= \lambda_1\Loss_1 (\cdot, \cdot) +  \lambda_{\Loss_{perc}}\Loss_{perc} (\cdot, \cdot) + \lambda_{\Loss_{freq}}\Loss_{freq} (\cdot, \cdot)  + \lambda_{adv}\Loss_{adv}.
\end{align}
Following~\cite{rodriguezpardo2023UMat,rodriguez2023neubtf,garces2023towards}, for $\mathcal{L}_{perc}$ we use the AlexNet version of~\cite{zhang2018unreasonable} and for $\mathcal{L}_{freq}$ we measure the Focal Frequency Loss~\cite{jiang2021focal}. For the adversarial loss, we follow the methodology specified in~\cite{zhu2017unpaired}. Then, we build our cycle-consistency loss and full loss as,
\begin{align}
	\Loss_\textrm{cycle} (I_d, I_l) &= \underbrace{\Loss_\textrm{im}(I_d, \mathcal{M}_D(\mathcal{M}_R(I_d)))}_\text{delighting} + \underbrace{\Loss_\textrm{im}(I_l, \mathcal{M}_R(\mathcal{M}_D(I_l))}_\text{relighting}	\\
	\Loss_\textrm{full}  (I_d, I_l) &= \Loss_\textrm{im} (I_d, \mathcal{M}_D(I_{l}) ) +  \Loss_\textrm{im} (I_l, \mathcal{M}_R(I_{d}) )  + \lambda_{cycle}\Loss_{\textrm{cycle}} (I_d, I_l).
\end{align}



\REMOVE{
an image captured with any flatbed scanner $I_{l}$, which may contain specular reflections, shadings or shadows. We formulate this problem with an image-to-image translation approach, leveraging an attention-guided convolutional U-Net~\cite{ronneberger2015u} $\hat{A} = \mathcal{M}_D (I_{l})$. 

To train this model, we leverage our dataset described in Section~\ref{sec:dataset}, which contains pairs of images $I_{d}$ and $I_{l}$, which are pixel-wise coherent. While directly training a model which maps from $I_{l}$ to $I_{d}$ using a standard pixel-wise norm could yield acceptable results, we empirically observe that this baseline approach does not achieve the desired level of accuracy. Therefore, we introduce a series of modifications aimed at maximizing generalization. 

First, inspired by intrinsic image decomposition~\cite{garces2022survey}, we formulate this mapping as learning an residual shading component, effectively learning a relationship $I_{d} = I_{l} + \mathcal{M}_D (I_{l})$. This simple modification allows for better training dynamics and generalization capabilities. Note that we use an RGB residual to make the estimation more flexible, thereby removing the assumption that either the source or reflected lights are white. 


\paragraph*{Cycle-Consistency}  
Importantly, in parallel to $M_D$, we train a \emph{relighting} neural network which learns to estimate $I_{l} = I_{d} + \mathcal{M}_R (I_{d})$, and use its estimations to further guide the training of $\mathcal{M}_D$, which is our main objective. We therefore formulate this learning problem as a cycle-consistent generative adversarial network, in which both delighting and relighting models are supervised with ground truth data, and with each other's estimations. This increases model accuracy and robustness, effectively working as a data-augmentation policy.



\paragraph*{Loss Function}

Our loss for the task of model delighting is a combination of pixel-wise, perceptual, frequency and adversarial losses, as follows:

\begin{align} \label{eq:delighting}
	\resizebox{\hsize}{!}{$\mathcal{L}_{\text{\tiny{Delighting}}} (I_d, I_l) = \lambda_{\mathcal{L}_1}\mathcal{L}_1^D (\mathcal{M}_D(I_{l}), I_{d}) +  \lambda_{\mathcal{L}_{perc}}\mathcal{L}_{perc}^D (\mathcal{M}_D(I_{l}), I_{d}) + \lambda_{\mathcal{L}_{freq}}\mathcal{L}_{freq}^D (\mathcal{M}_D(I_{d}), I_{l})  + \lambda_{adv}\mathcal{L}_{adv}^D $}
\end{align}


Following~\cite{rodriguezpardo2023UMat,rodriguez2023neubtf,garces2023towards}, for $\mathcal{L}_{perc}$ we use the AlexNet version of~\cite{zhang2018unreasonable} and for $\mathcal{L}_{freq}$ we measure the Focal Frequency Loss~\cite{jiang2021focal}. For the adversarial loss, we follow the methodology specified in~\cite{zhu2017unpaired}. Similarly, our loss for the relighting model is:

\begin{align} \label{eq:relighting}
	\resizebox{\hsize}{!}{$\mathcal{L}_{\text{\tiny{Relighting}}} (I_d, I_l)= \lambda_{\mathcal{L}_1}\mathcal{L}_1^R (\mathcal{M}_R(I_{d}), I_{l}) +  \lambda_{\mathcal{L}_{perc}}\mathcal{L}_{perc}^R (\mathcal{M}_R(I_{d}), I_{l}) + \lambda_{\mathcal{L}_{freq}}\mathcal{L}_{freq}^R (\mathcal{M}_R(I_{l}), I_{d})  + \lambda_{adv}\mathcal{L}_{adv}^R $}
\end{align}

Our cycle consistency loss is defined as:
\begin{align} \label{eq:cycle_loss}
	\resizebox{\hsize}{!}{$\mathcal{L}_{\text{\tiny{cycle}}} (I_d, I_l) = \mathcal{L}_{\text{\tiny{Relighting}}}(I_d, \mathcal{M}_R(\mathcal{M}_D(I_l))) + \mathcal{L}_{\text{\tiny{Delighting}}}(I_l, \mathcal{M}_D(\mathcal{M}_R(I_d)))           $} 
\end{align}

The final loss is defined as follows:

\begin{align} \label{eq:cycle_loss}
	\resizebox{.7\hsize}{!}{$\mathcal{L} = \mathcal{L}_{\text{\tiny{Delighting}}} +  \mathcal{L}_{\text{\tiny{Relighting}}}  + \lambda_{cycle}\mathcal{L}_{\text{\tiny{cycle}}}                    $} 
\end{align}

}

\paragraph*{\textbf{Architecture Design}} For the generator architectures, we follow the attention-guided U-Net design in~\cite{rodriguezpardo2023UMat}, using a single decoder for each model, and removing the MLPs appended to the end of the architecture. For the discriminators, we follow previous work on texture synthesis~\cite{rodriguezpardo2022seamlessgan,zhou2018non} and use a 4-layer PatchGAN~\cite{isola2017image}. 

\begin{figure*}[tb]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/pdf/diagram_cycle_consv2.pdf}
	\caption{Diagram of our cycle-consistent generative model capable of both material delighting and relighting. }
	\label{fig:delighting_diagram}
\end{figure*}


\paragraph*{\textbf{Data Augmentation}} We train the models using random cropping, with $128\times128$ resolution patches. Besides, we use random rescaling, enabling the model to generalize on the $(300, 1200)$ PPI range, covering most flatbed scanners. Finally, we use random horizontal and vertical flips to further enhance generalization. 

\paragraph*{\textbf{Implementation Details}} We standardize each dataset of directional and diffuse images using their respective means and standard deviations, enabling the model to focus on relative differences and not on global average values. We use PyTorch~\cite{paszke2017automatic} and Torchvision~\cite{marcel2010torchvision} for training, and Kornia~\cite{riba2020kornia} for data augmentation. We train the models using Adam~\cite{kingma2014adam} for 100 iterations, with an initial learning rate of $0.002$, halved every 30 iterations. We leverage automatic gradient scaling and mixed precision training~\cite{micikevicius2017mixed}. Both generators and discriminators are initialized using \emph{orthogonal initialization}~\cite{hu2020provable}. Training these models takes approximately 12 hours on a NVidia 3060 GPU. After a Bayesian hyperparameter~\cite{wandb} optimization performed on a separate validation dataset, we set the loss weighting as $\lambda_{cycle} = 0.25, \lambda_{adv} = 0.15, \lambda_{perc} = 0.3, \lambda_{freq} = 0.2, \lambda_{\mathcal{L}_1} = 1$. Further details are included in the supplementary material. 
