\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/macro_avg_figures/full_figures/subject/token_lens_logit.png}
    \caption{Token Lens logit values of subject and answer tokens across layers and answer steps when attending to the subject (macro-averaged across all datasets and models). Attention promotes and extracts subject information in the middle layers while suppressing it in later layers.}
    \label{fig:token_lens_logit_attention_to_subject}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/macro_avg_figures/full_figures/subject/attn_knockout_logit.png}
    \caption{Average logit differences of the subject and answer tokens between MLP outputs with and without knocking out attention from the last to the subject tokens. Positive logit differences for the answers and negative differences for the subject in later layers show that MLPs use the subject information to promote answers and suppress the subject.}
    \label{fig:attention_knockout_subject_logits}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/macro_avg_figures/full_figures/prev_ans/token_lens_logit.png}
    \caption{Token Lens logit values subject and answer tokens across layers and answer steps $2$ and $3$ (macro-averaged across all datasets and models) when attending to previous answers. The logit of the attended answer is negative at later layers, showing that the attention is suppressing previously generated answers.}
    \label{fig:token_lens_logit_attention_to_prev_ans}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/macro_avg_figures/full_figures/prev_ans/attn_knockout_logit.png}
    \caption{Average logit differences for subject and answer tokens between MLP outputs with and without knocking attention from the last to previous answer tokens. All previously generated answer tokens have negative logits, and all new answers have positive logits. This result suggests that MLPs use previous answers for both repetition suppression and new answer promotion.}
    \label{fig:attention_knockout_prev_ans_logits}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/macro_avg_figures/full_figures/last_token/token_lens_logit.png}
    \caption{Token Lens logit values of subject and answer tokens across layers and answer steps when attending to the last token (macro-averaged across all datasets and models). Attention promotes all three answers and the subject at the final layers, with the answer for the current step having the highest logit.}
    \label{fig:token_lens_last_token_logits}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/macro_avg_figures/full_figures/last_token/attn_knockout_logit.png}
    \caption{Average logit differences for subject and answer tokens between MLP outputs with and without knocking attention from the last token to itself. The logit differences of all three answers and the subject are negative at the late layers, meaning MLPs output higher logits when it does not have information from the last token. This pattern may suggest a compensation behavior for the absence of direct attention to the last token to encourage the outputs to still be correct.}
    \label{fig:attention_knockout_last_token_logits}
\end{figure*}


\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/country_cities/meta-llama/Meta-Llama-3-8B-Instruct/subject_logit.png}
   \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Llama-3-8B-Instruct on Country-Cities dataset when attending to or knocking out the subject tokens.}
   \label{fig:country_cities-llama-subject}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/country_cities/mistralai/Mistral-7B-Instruct-v0.2/subject_logit.png}
   \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Mistral-7B-Instruct on Country-Cities dataset when attending to or knocking out the subject tokens.}
   \label{fig:country_cities-mistral-subject}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/artist_songs/meta-llama/Meta-Llama-3-8B-Instruct/subject_logit.png}
   \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Llama-3-8B-Instruct on Artist-Songs dataset when attending to or knocking out the subject tokens.}
   \label{fig:artist_songs-llama-subject}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/artist_songs/mistralai/Mistral-7B-Instruct-v0.2/subject_logit.png}
   \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Mistral-7B-Instruct on Artist-Songs dataset when attending to or knocking out the subject tokens.}
   \label{fig:artist_songs-mistral-subject}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/actor_movies/meta-llama/Meta-Llama-3-8B-Instruct/subject_logit.png}
   \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Llama-3-8B-Instruct on Actor-Movies dataset when attending to or knocking out the subject tokens.}
   \label{fig:actor_movies-llama-subject}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/actor_movies/mistralai/Mistral-7B-Instruct-v0.2/subject_logit.png}
   \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Mistral-7B-Instruct on Actor-Movies dataset when attending to or knocking out the subject tokens.}
   \label{fig:actor_movies-mistral-subject}
\end{figure*}
