\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures//unembed_attn_mlp_outputs/full_figures/unembed_attn_output_logit.png}
    \caption{Logit of the subject and answer tokens from decoding the attention outputs across layers and answer steps. Attention primarily promotes the subject at the middle layers while promoting new answers and suppressing previously generated ones at deeper layers.}
    \label{fig:unembed_attn_output_logit}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures//unembed_attn_mlp_outputs/full_figures/unembed_mlp_output_logit.png}
    \caption{Logits of the subject and answer tokens from decoding the MLP outputs across layers and answer steps. The consistently positive logits for all three answers illustrate that MLPs promote multiple answers simultaneously. MLPs also decrease the logits of previously generated answers in deeper layers, contributing to repetition suppression alongside attention.
    }
    \label{fig:unembed_mlp_output_logit}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/country_cities/meta-llama/Meta-Llama-3-8B-Instruct/attn_mlp_output_logit.png}
   \caption{Attention and MLP output logits of Llama-3-8B-Instruct on Country-Cities dataset.}
   \label{fig:country_cities-llama-attn_mlp_output_logit}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/country_cities/mistralai/Mistral-7B-Instruct-v0.2/attn_mlp_output_logit.png}
   \caption{Attention and MLP output logits of Mistral-7B-Instruct on Country-Cities dataset.}
   \label{fig:country_cities-mistral-attn_mlp_output_logit}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/artist_songs/meta-llama/Meta-Llama-3-8B-Instruct/attn_mlp_output_logit.png}
   \caption{Attention and MLP output logits of Llama-3-8B-Instruct on Artist-Songs dataset.}
   \label{fig:artist_songs-llama-attn_mlp_output_logit}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/artist_songs/mistralai/Mistral-7B-Instruct-v0.2/attn_mlp_output_logit.png}
   \caption{Attention and MLP output logits of Mistral-7B-Instruct on Artist-Songs dataset.}
   \label{fig:artist_songs-mistral-attn_mlp_output_logit}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/actor_movies/meta-llama/Meta-Llama-3-8B-Instruct/attn_mlp_output_logit.png}
   \caption{Attention and MLP output logits of Llama-3-8B-Instruct on Actor-Movies dataset.}
   \label{fig:actor_movies-llama-attn_mlp_output_logit}
\end{figure*}

\begin{figure*}
   \centering
   \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/actor_movies/mistralai/Mistral-7B-Instruct-v0.2/attn_mlp_output_logit.png}
   \caption{Attention and MLP output logits of Mistral-7B-Instruct on Actor-Movies dataset.}
   \label{fig:actor_movies-mistral-attn_mlp_output_logit}
\end{figure*}

