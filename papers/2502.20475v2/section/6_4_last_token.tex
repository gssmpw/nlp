\subsection{Role of Last Token}

The last token aggregates knowledge recall and suppression information in the final layers to promote all answers while prioritizing the correct answer for each step.

\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/last_token_logits.png}
    \caption{
    Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens when attending to or knocking out the last token. Attention promotes all three answers and the subject at the final layers, prioritizing $o^{(i)}$ at each step $i$. Late-layer MLP logit differences are negative for the subject and answers, possibly compensating for the absence of direct attention to the last token to encourage correct outputs.
    }
    \label{fig:critical_token_analysis_last_token_logits}
    \vspace{-2mm}
\end{figure*}

\paragraph{Attention promotes all answers at the last token in the final layers.} Starting from layer $28$, attention from the last token to itself significantly increases the logit of all three answers (\Cref{fig:critical_token_analysis_last_token_logits}). At each step $i$, the logit for the answer ${o^{(i)}}$ is consistently the highest among the three answers. This result suggests that attention at the last token aggregates information from earlier layers related to knowledge recall and suppression, preparing the model for generating the next prediction. 

\paragraph{MLPs compensate answer promotions when the direct attention to the last token is absent.} Interestingly, we observe MLPs output negative logit differences for the subject and all three answers in the final layers when knocking out the attention from the last token to itself (\Cref{fig:critical_token_analysis_last_token_logits}). The answer ${o^{(i)}}$ for each step $i$ consistently has the most negative logit differences. In other words, without having access to the attention output of the last token, MLPs output even higher logits for the subject and the answers. This behavior suggests a backup mechanism: without direct attention to the last token that aggregates information from early input tokens, the model may not have sufficient promotion and differentiation of the three answers. MLPs compensate this by further promoting the three answers to encourage the predictions to be correct. Similarly, \citet{wang2022interpretability} find backup token mover attention heads that become active when the original token mover heads are ablated. %

