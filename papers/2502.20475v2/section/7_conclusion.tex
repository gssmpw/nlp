\section{Conclusion}

We uncover how language models answer one-to-many factual queries across two models and three datasets. By unembedding the output of attention and MLPs across layers, we find that LMs promote all answers and then suppress previously generated ones. We then delve into how LMs implement knowledge recall and repetition avoidance. We find that LMs use both the subject and previous answer tokens to perform knowledge recall. Attention first propagates subject information from the subject to the last token, which is then used by MLPs to promote all correct answers. At the same time, MLPs also utilize previous answer tokens to promote new answers at late layers. %
In addition, previous answer tokens trigger suppression of themselves. In the final layers, attention suppresses repetitions by attending to and outputting negative logits for previously generated answer tokens. MLPs reinforce and amplify this suppression by decreasing the logits of previous answer tokens around the same layers. At last, by integrating all relevant information for knowledge recall and suppression at the last token position, LMs effectively generate correct and distinct answers at different steps. We hope our findings encourage a deeper understanding of how LMs' internal components interact with context tokens to support complex factual recall and response generation.

\paragraph{Future Work.} Future work could investigate possible redundancies in the model, as multiple tokens---such as the subject and previous answers---contribute to promoting new answers. This result raises the question of whether LMs redundantly encode knowledge and if it is necessary. Additionally, our analyses only focus on the correct cases. Examining the patterns when LMs hallucinate could provide insights for mitigating such errors. 


\section*{Limitations}
Our analyses primarily rely on Logit Lens \cite{Nostalgebraist_2020}, which early decodes component outputs using LMs' last unembedding layer. While this method is training-free, it may be less reliable, particularly for early layers. More expressive techniques, such as Tuned Lens \cite{belrose2023eliciting} and SAE \cite{templeton2024scaling}, could be applied for a better understanding of \task. Also, we use a single prompt template for each model and dataset. Further studies are needed to determine whether our findings generalize across different prompt templates. 

While we attempt to identify how LMs recall knowledge, it is difficult to disentangle where the model truly recalls knowledge from its parameters, and where it amplifies already-recalled knowledge stored in the residual stream.
This is especially difficult because models could redundantly encode knowledge in multiple places, and thus parametric recall and amplification could be interleaved.
We hope future work can develop reliable methods for disentangling these concepts and lead to a more precise understanding of the underlying mechanism.

\section*{Acknowledgments}
We thank Ting-Yun Chang for her helpful feedback on the paper. RJ was supported in part by the National Science Foundation under Grant No. IIS-2403436. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
