\section{Introduction}
\label{1_introduction}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/figure_1.pdf}
    \caption{To answer one-to-many factual queries, we found that LMs first use attention to propagate subject information to the last token, which is used by MLPs to promote all possible answers. Attention then attends to and suppresses the subject and previous answer tokens, while MLPs amplify the suppression and further promote new answers.}
    \label{fig:figure_1}
    \vspace{-5mm}
\end{figure}

Transformer-based language models (LMs) store a vast amount of factual knowledge in their parameters \citep{petroni2019language, dai2021knowledge, geva2022transformer}. Many recent works have studied where and how LMs recall this knowledge for one-to-one factual queries, which ask the model to recall a single fact (e.g., the capital of a country) given a subject-relation pair \citep{meng2022locating, geva2023dissecting, merullo2023language}.

In this work, we study the comparatively unexplored task of one-to-many knowledge recall (\task), in which the model must generate a list of answers without repetition. Many real-world relations, such as a country's cities or an artist's songs, are one-to-many. This more complex task requires LMs to integrate multiple pieces of contextual information, including the subject and previously generated answers, to simultaneously perform two subtasks: \textbf{knowledge recall} and \textbf{repetition avoidance}. We uncover LMs' mechanism for \task\ by understanding (1) the overall process by which they generate distinct answers at different steps, and (2) how they perform both answer promotion and repetition avoidance.

To understand the overall process, we early decode \citep{Nostalgebraist_2020} the output of attention and MLPs to examine how the logits of the subject and answer tokens change across layers. We find that LMs first promote all answers and then suppress the ones that have been previously generated. Specifically, attention copies the subject information at the middle layers and MLPs promote all possible answers. Then, both components suppress previous answer tokens at late layers. These observations hold for both Llama-3-8B-Instruct and Mistral-7B-Instruct across three datasets.
 
To examine how LMs implement knowledge recall and repetition avoidance, we first run causal tracing \cite{meng2022locating} to locate tokens that are critical to LMs' outputs; these important tokens include the subject, previous answers, and the last token. Then, we analyze how both attention and MLP layers use these tokens. For attention, we propose Token Lens, a new technique that aggregates and then unembeds the results of attending to a given token or span; in this way, we can observe how attention to each token promotes or suppresses different output tokens. For MLPs, we design an attention knockout method inspired by \citet{geva2023dissecting}: we knock out the attention from the last token to the target tokens and examine the resulting change in MLP output logits to determine how MLPs use target token information.
We find that LMs use both the subject and previous answer tokens for knowledge recall: attention propagates the subject information from the subject to the last token, and MLPs leverage the information and previous answer tokens to promote answers. In addition, previous answer tokens trigger suppression of themselves: attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. LMs aggregate this information at the last token to generate distinct answers across steps. 

Overall, our study elucidates how LMs use attention and MLPs to interact with different tokens and perform knowledge recall and repetition avoidance for \task. We hope this work opens pathways for analyzing more complex tasks requiring dynamic integration of contextual information.










