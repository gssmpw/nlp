\subsection{Role of Previous Answer Tokens}
\label{6_3_previous_answer_tokens_for_both_repetition_avoidance_and_knowledge_recall}

\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{1mm}
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/prev_ans_logits.png}
    \caption{Token Lens logits of the attended previous answers (left) are negative at deeper layers, showing that attention suppresses prior answers. Negative MLP logit differences (right) for previous answers and positive differences for new answers suggest that MLPs use previous answer tokens for both repetition avoidance and knowledge recall.}
    \label{fig:critical_token_analysis_prev_ans_logits}
    \vspace{-3mm}
\end{figure*}

\paragraph{Attention plays a crucial role in suppressing repetitions.} Attention consistently outputs negative logits for previous answer tokens at both step $2$ and step $3$ in the final layers. This result shows that attention attends to and suppresses tokens that have already appeared in the context, ensuring previously generated answers are not repeated. 

\paragraph{MLPs amplify suppression of previous answers.} As shown in \Cref{fig:critical_token_analysis_prev_ans_logits}, all previous answer tokens have negative MLP logit differences at late layers. For instance, $o_1$ has negative logits at step $2$ starting around layer $27$; $o_1$ and $o_2$ exhibit similar patterns at step $3$. This suppression aligns with attentionâ€™s role in inhibiting previously generated tokens, suggesting that MLPs amplify these suppression signals to prevent repetition.

\paragraph{MLPs also use previous answer tokens for knowledge recall.} Surprisingly, we observe positive MLP logit differences for new answers across answer steps (\Cref{fig:critical_token_analysis_prev_ans_logits}). Specifically, the logit differences of both $o_2$ and $o_3$ are positive when intervening on $o_1$ at step $2$; $o_3$ has positive logits differences when intervening on $o_1$ or $o_2$ at step $3$. This pattern shows that MLPs also leverage previous answer tokens to promote new answers. Since LMs already promote all relevant answers when predicting previous answers, it is plausible that the models reuse these prior computations to promote new answers.
These findings show that the subject token is not the sole source of answer promotion (\Cref{5_1_which_tokens_should_be_noised}). The previous answer tokens also have a positive (but smaller) effect on answer promotion.


