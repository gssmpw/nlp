
\section{Problem Settings}
We first introduce the task of one-to-many knowledge recall and describe our experiment settings. 

\subsection{Task: One-to-Many Knowledge Recall}
\label{3_1_task_one_to_many_knowledge_recall}

In \task, a language model is given a subject entity $s$ and a relation $r$, and must generate a set of corresponding object entities $O = \{o^{(1)}, o^{(2)}, \dotsc,o^{(n)}\}$ that are related to $s$ through $r$. In this paper, $n=3$. All generated object entities must be distinct, that is, $o^{(i)} \neq o^{(j)}$ for $i \neq j$. For example, given $s=\text{"U.S.A."}$ and $r=\text{"cities of"}$, one possible valid set of object entities is $O= \{\text{Los Angeles}, \text{San Francisco}, \text{Seattle}\}$. To perform this task, the model must perform two key subtasks:
\vspace{-3mm}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
    \item \textbf{Knowledge recall}: The model must identify and extract the subject $s$ from the input and retrieve entities that are connected to $s$ through the relation $r$ from its internal knowledge.
    \vspace{-3mm}
    \item \textbf{Repetition avoidance}: The model must not generate duplicate entities.
    \vspace{-1mm}
\end{enumerate}

\paragraph{Possible mechanisms.}
Multiple different mechanisms could be used by the model to perform \task.
On one hand, the model could use different attention heads to promote a different answer at each timestep. 
It could first use suppression heads \citep{wang2022interpretability} to identify previously generated answers, then change the attention patterns of subsequent heads to avoid promoting those answers.
Such a mechanism would mirror the use of suppression heads to avoid generating incorrect, repetitive tokens in the IOI task \citep{wang2022interpretability}.
To promote answers, the model could attend to the subject token position, which could encode different answers in different attention value vectors due to subject enrichment \citep{geva2023dissecting}.





On the other hand, the model could first promote all relevant answers and then suppress previously generated ones. It could extract all possible answers from the subject representation \citep{geva2023dissecting, meng2022locating}, regardless of which object entities have been generated. Then, copy suppression heads could identify previous answer tokens and prevent the model from generating them, similar to \citet{mcdougall2023copy}.
The results of knowledge recall and repetition avoidance could be additively combined in the residual stream to yield a correct and non-duplicate output, similar to \citet{chughtai2024summing}.
In this paper, we uncover the true mechanism that the model uses for one-to-many knowledge recall. 



\subsection{Datasets and Models}

\input{table/datasets_and_model_performance}

We curate three \task\ datasets on different topics: (1) cities of a country,\footnote{\url{https://simplemaps.com/data/world-cities}} (2) songs performed by an artist,\footnote{\url{https://www.kaggle.com/datasets/salvatorerastelli/spotify-and-youtube}}, and (3) movies acted in by an actor or actress.\footnote{\url{https://www.kaggle.com/datasets/darinhawley/imdb-films-by-actor-for-10k-actors}} A summary of the datasets is provided in \Cref{tab:datasets_and_model_performance}. For each dataset, we filter out subjects that are associated with fewer than three object entities for the specified relation. 

We study two LMs: Llama-3-8B-Instruct \cite{llama3modelcard} and Mistral-7B-Instruct \cite{mistral7Bmodelcard}. Refer to \Cref{appendix_prompts} for the prompt templates we use for each model and dataset. To create the data for analyzing LMs' behaviors, we first generate three answers using greedy decoding, ensuring consistent outputs for examining component behaviors across different answer steps. We then retain the entries where all three predicted answers are correct to focus on cases where the models' knowledge is accurate. 

\Cref{tab:datasets_and_model_performance} shows the number of correct predictions made by the models across the datasets. The low accuracy may be explained by (1) long-tail entities (e.g., less popular actors or songs), (2) outdated datasets compared to the model's knowledge, and (3) the strict use of exact match evaluation (e.g., ``Mission: Impossible'' is considered incorrect even if given ``Mission: Impossible - Fallout'' is in the label list). 
For all (dataset, model) pairs, we have at least 100 correct instances, providing a sufficient sample size for the analysis. For the rest of the paper, we focus only on the correct cases. When analyzing models' behaviors at step $i$ ($i = 1, 2, 3$), we keep all tokens before the first token of the $i$th answer as input. Refer to \Cref{appendix_sample_responses_and_analysis_data_example} for examples and details. We report results macro-averaged across both models and all datasets in the main section. Refer to the appendix for full results of all answer steps and specific models and datasets. We run all experiments on a single RTX A6000 GPU.






    









        


