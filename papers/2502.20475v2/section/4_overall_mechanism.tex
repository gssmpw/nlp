\section{Decoding the Overall Mechanism}

To understand how LMs perform \task, we first inspect the outputs of attention and MLP across layers. We aim to understand how knowledge recall and copy suppression coordinate to produce different correct answers across generation steps. 

\subsection{Method: Decoding Component Outputs}
\label{4_1_method_decoding_components_outputs}
Given a transformer LM with $L$ layers, each layer $l$ has a multi-headed attention (MHA) and a MLP layer for $l=1\dotsc L$. Let $a^{(l)} \in \mathbb{R}^d$ and $m^{(l)} \in \mathbb{R}^d$ be the outputs of the MHA and MLP at layer $l$ at the last token position\footnote{We focus on the last token position as the model directly uses it to generate the next answer.} respectively. Similar to \citet{Nostalgebraist_2020} and \citet{geva2022transformer}, we (early) decode $a^{(l)}$ and $m^{(l)}$ by passing them through the final layer layernorm and unembedding matrix $U \in \mathbb{R}^{|\text{Vocab}| \times d}$ and obtain the logits to examine their contributions to knowledge recall and repetition avoidance:
    \begin{equation}
        \text{logits} = U \cdot \text{LayerNorm}(z^{(l)})
    \end{equation} 
    where $z^{(l)}$ is $a^{(l)}$ or $m^{(l)}$, and $\text{LayerNorm}(\cdot)$ denotes the final layernorm. In this paper, $\text{LayerNorm}(\cdot)$ is the RMSNorm \cite{zhang2019root}. Note that the RMSNorm is calculated based on the input's hidden state from the final layer, not directly on $a^{(l)}$, ensuring consistent normalization across layers and components \cite{chang2024parts}. 


\subsection{LMs Promote Then Suppress}
\label{4_2_LMs_promote_then_suppress}
We analyze the logit values of the first tokens of object entities predicted across three answer steps and the subject. A positive logit indicates promotion, while a negative logit suggests suppression. Our analysis shows that LMs use both attention and MLPs to promote all possible answers at each step while suppressing repetitions.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/unembed_attn_mlp_outputs/unembed_component_output_logit.png}
    \caption{Logit of the subject and answer tokens from unembedding attention and MLP outputs. Attention primarily promotes the subject at the middle layers, then promotes new answers and suppresses previous answers at deeper layers. MLPs consistently promote all answers; at deeper layers, they also decrease the logits of previously generated answers. Early layers are omitted as logits are near zero. See \Cref{appendix_unembed_attn_mlp_outputs_full_figures} for full figures.
    }
    \label{fig:unembed_component_output_logit}
\end{figure*}

\paragraph{Attention primarily copies subject information.} As shown in \Cref{fig:unembed_component_output_logit}, attention outputs positive logits for the subject token in the middle layers across all three answer steps. While the three answers are slightly promoted at layer $25$, their logits are still close to zero and have a smaller magnitude compared to that of the subject at the middle layers. This pattern indicates that attention copies or propagates subject information at the last token position. Interestingly, the answer promotion pattern is more evident in the Country-Cities dataset (\Cref{fig:country_cities-llama-attn_mlp_output_logit}, \Cref{fig:country_cities-mistral-attn_mlp_output_logit}) but not in the Artist-Songs and the Actor-Movies datasets (\Cref{fig:artist_songs-llama-attn_mlp_output_logit}, \Cref{fig:artist_songs-mistral-attn_mlp_output_logit}, \Cref{fig:actor_movies-llama-attn_mlp_output_logit}, \Cref{fig:actor_movies-mistral-attn_mlp_output_logit}). %

\paragraph{MLPs promote all possible answers.} From the middle to later layers, MLPs consistently output positive logits for all three possible answers (\Cref{fig:unembed_component_output_logit}). These logits increase across generation steps, with their magnitude significantly exceeding that of attention logits. These findings suggest that MLPs strongly promote all possible answers regardless of prior predictions, thereby providing a stronger answer promotion signal compared to attention.

\paragraph{Previously generated answers are suppressed at later layers.} Both attention and MLPs suppress answers that have been generated previously. Starting from layer $28$, attention outputs negative logits for $o^{(1)}$ at step $2$ and for both $o^{(1)}$ and $o^{(2)}$ at step $3$ (\Cref{fig:unembed_component_output_logit}). Similarly, MLPs decrease the logit of previous answers at the same layer. Since MLPs themselves cannot attend back to early tokens, this suppression likely results from leveraging suppression signals from attention, a hypothesis further investigated in \Cref{6_3_previous_answer_tokens_for_both_repetition_avoidance_and_knowledge_recall}. \\

\noindent In the final layers, both attention and MLPs increase answers' logits, especially those that have not been generated. This pattern may be explained by how LMs use the final layers to adjust the logits and regulate the confidence or certainty of their predictions \cite{stolfo2024confidence}. Overall, all the observations above demonstrate that LMs promote all three answers and then suppress previously generated ones. 

