\section{Which Tokens Matter?}
To better understand the promote-then-suppress mechanism, we now investigate how LMs implement knowledge recall and repetition avoidance. In this section, we use causal tracing \cite{meng2022locating} to identify the input tokens that most influence model predictions. In \Cref{6_analyze_critical_tokens}, we analyze how these tokens are used by attention and MLPs to facilitate knowledge recall and repetition avoidance. 


\subsection{Which Tokens Should Be Noised?}
\label{5_1_which_tokens_should_be_noised}
Prior work shows that in order to recall knowledge, LMs encode information about relevant object entities in subject tokens and retrieve this information via attention
\citep{geva2023dissecting, meng2022locating}. Other work shows that LMs avoid repetition by using attention heads to attend to previous tokens and suppress them \citep{mcdougall2023copy, wang2022interpretability, merullo2023circuit}. %
Thus, we hypothesize that the subject and previous answer tokens play decisive roles in our two key sub-tasks (\Cref{3_1_task_one_to_many_knowledge_recall}).



To confirm these hypotheses, we use causal tracing \cite{meng2022locating}: we separately add noise to the subject and previous answer tokens, restore selected components' activations to their values without noise, and visualize the difference in the probability of $o^{(i)}$ that will be predicted at each answer step $i$ before and after the restoration. This approach allows us to measure the impact of specific token activations on the models' outputs. 





\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/causal_tracing/causal_tracing_for_main.png}
    \caption{The impact of attention and MLPs' activations on LMs' predictions when intervening on the subject (left) and previous answer tokens (right) at step $2$. %
    The probability differences all peak around or above $0.55$, reflecting the importance of both the subject and previous answer tokens. See \Cref{appendix_causal_tracing_figures} for figures of other answer steps, which have similar patterns.
    }
    \label{fig:causal_tracing_for_main}
    \vspace{-5mm}
\end{figure}

\paragraph{Intervention on Subject.} \Cref{fig:causal_tracing_for_main} visualizes the impact of attention and MLPs on LMs' predictions when intervening on the subject tokens at step $2$ (Refer to \Cref{appendix_causal_tracing_figures} for figures of other answer steps and specific models and datasets, which have similar patterns). The probability difference peaks around or above $0.55$ for both components, confirming our hypothesis that the subject plays a crucial role in knowledge recall. Attention's contributions peak in the middle layers at the last token, while MLPs dominate in early layers at the subject token and in late layers at the last token. These observations suggest that attention propagates subject information from early MLP layers to the last token, where MLPs may leverage it for answer promotion, as discussed in \Cref{6_2_subject_tokens_for_knowledge_recall}. 


\paragraph{Intervention on previous answers.} Noising previous answer tokens also leads to high probability changes in LMs' output probabilities, with an average difference of around or above $0.55$ across answer steps (\Cref{fig:causal_tracing_for_main}). This finding supports our hypothesis that previous answer tokens are also critical to LMs' outputs. Similar to the results of noising the subject, attention's contributions peak in both the middle and the last layers at the last token. MLPs dominate in early layers at the previous answer positions and in late layers at the last token, reflecting that the previous answer tokens are used by both components to make nontrivial contributions to models' predictions. 














