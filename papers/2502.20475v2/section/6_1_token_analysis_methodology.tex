\subsection{Methodology for Analyzing Tokens}
To analyze how attention and MLPs utilize the subject, previous answer, and last tokens, we develop techniques to unembed their token-specific outputs and examine their roles in knowledge recall and suppression.


\paragraph{Attention: Token Lens.} For attention, we propose Token Lens, a new technique that unembeds the aggregated outputs of attention to specified tokens.
Let $t=\{t_1, ... t_k\}$ denote the target tokens we are examining. $t$ can be the subject $s$, an object entity answer $o^{(i)}$, or the last token of the input. %
Let $a^{(l_i)}$ be the $i$th attention head in layer $l$ of a transformer LM, for $i=1\dotsc n$ and $l=1\dotsc L$. Let $p^{(l_i)}_{t_j} \in \mathbb{R}$ denotes $a^{(l_i)}$'s attention weight between the last input token\footnote{We only need to do the analysis when LLMs start to generate the next answer. Therefore, we are only looking at the last token of the input.} and the $t_j$th token of the input. Similarly, let $v^{(l_i)}_{t_j} \in \mathbb{R}^{d_{\text{head}}}$ denotes the value vector of $a^{(l_i)}$ for the the $t_j$th token.

We first gather the information that each attention head $a^{(l_i)}$ aggregates from all target tokens, which is calculated as the sum of all weighted value vectors of $t$ of $a^{(l_i)}$:
    \begin{equation}
        a^{(l_i)}_e = \sum_{j=1}^{k} p^{(l_i)}_{t_j} \cdot v^{(l_i)}_{t_j}
    \end{equation}
Then, the full attention output of the target tokens from the $l$th layer is:
    \begin{equation}
        a^{(l)}_e = W^{(l)}_o \cdot \text{Concat}(a^{(l_1)}_e, \dotsc, a^{(l_n)}_e)
    \end{equation}
    where $W^{(l)}_o \in \mathbb{R}^{d \times nd_{\text{head}}}$ is the output projection matrix of layer $l$. This vector $a^{(l)}_e \in \mathbb{R}^d$ represents the contribution of MHA at layer $l$ to the output from the target tokens.

Finally, following the same approach of (early) decoding attention and MLP outputs in \Cref{4_1_method_decoding_components_outputs}, we unembed $a^{(l)}_e$ to obtain the logits of the first token of the subject and answers and examine how attention uses the target tokens to perform promotion or suppression.



\paragraph{MLPs: Attention Knockout.} Since MLPs themselves cannot attend to previous tokens--a function exclusive to MHA--we adopt an attention knockout approach inspired by \citet{geva2023dissecting}. By knocking out the attention from the last token to the target tokens, we examine changes in MLP output logits to determine how MLPs utilize target token information for knowledge recall and repetition avoidance. Specifically, we zero out the attention weights between the last and the target tokens:
\begin{equation*}
    p^{(l_i)}_{t_j} \gets 0, \forall i \in [1, n], \forall j \in [1, k], \forall l \in [1, L]
\end{equation*}

Let $m^{(l)}$ and $m'^{(l)}$ denote the MLP output at layer $l$ before and after applying the attention knockout respectively. We unembed these outputs using the same early decoding approach described in \Cref{4_1_method_decoding_components_outputs}. By subtracting the logits derived from $m'^{(l)}$ from those of $m^{(l)}$, we examine the difference in the logits of the subject and the answer tokens. A positive difference value indicates MLPs use the knocked-out tokens to promote a token; a negative difference means suppression. 
