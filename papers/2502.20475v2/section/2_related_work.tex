\section{Related Work}
\label{2_related_work}

\paragraph{Interpretability of Language Models.} 
Works on mechanistic interpretability aim to reveal the function of different components in LMs \citep{elhage2021mathematical, bansal2022rethinking}, such as neurons \citep{dai2021knowledge, gurnee2023finding}, attention heads \citep{michel2019sixteen, olsson2022context}, and MLPs \citep{geva2020transformer, geva2022transformer}. 
In particular, how LMs store and use knowledge has been widely studied by many prior works \citep{petroni2019language, bouraoui2020inducing, cao2021knowledgeable, dalvi2022discovering, da2021analyzing}. 
However, most prior studies have mainly focused on one-to-one knowledge recall, where LMs retrieve a single fact given a subject-relation pair. In this work, we study how LMs' components contribute to one-to-many knowledge recall, which is a more complex setting that requires LMs to integrate multiple types of contextual information: subject, relation, and previously generated answers.

\paragraph{Attribution Methods.} 
Prior works have introduced various methods for analyzing the function of different components, including probing \citep{burns2022discovering, li2024inference}, patching \citep{goldowsky2023localizing, ghandeharioun2024patchscope}, early decoding \citep{Nostalgebraist_2020, merullo2023language}, and knocking out component outputs to assess their impact on models' outputs \citep{chang2023localization, li2023circuit, geva2023dissecting}. Our method, \hyperref[6_1_1_attn_token_lens]{Token Lens} and \hyperref[6_1_2_mlps_attn_knockout]{attention knockout} (inspired by \citet{geva2023dissecting}) examines the importance of attention and MLPs by early decoding their token-level outputs, revealing how LMs use the two components to integrate information from various parts of the input. 





\paragraph{Dissecting Component Functions.} 
Recent works have studied the functions of MLPs and attention in knowledge recall given subject-relation pairs. \citet{geva2023dissecting} and \citet{meng2022locating} demonstrate that MLPs enrich subject representations at early layers, while \citet{geva2022transformer} and \citet{merullo2023language} highlight how MLPs promote correct answer tokens by writing updates to the residual stream and adjusting the vocabulary probabilities. This mechanism is still essential for the model to generate multiple answers tied to the given subject. Prior works have also shown that attention plays a key role in extracting important tokens and suppressing repeated ones \citep{wang2022interpretability, mcdougall2023copy, merullo2023circuit, tigges2024llm}, which is essential for preventing the model from generating duplicate answers. \citet{merullo2024talking} further decomposes attention heads and identifies low-rank subspaces in which components communicate to selectively inhibit repetitive items from a list given in the context, which also involves list processing and repetition avoidance but not recalling factual knowledge from model parameters. 





    
    
    
