\clearpage
\appendix 
\section{Prompt Templates}
\label{appendix_prompts}
Following are the prompt templates that we use for each model and dataset.
Llama-3-8B-Instruct:
\begin{itemize}
    \item Country-Cities: List three cities from {country}:
    \item Artist-Songs: List three songs performed by {artist}:
    \item Actor-Movies: List three movies acted by {actor}:
\end{itemize}

Mistral-7B-Instruct:
\begin{itemize}
    \item Country-Cities: List the name of three cities from {country}:
    \item Artist-Songs: List three songs performed by {artist}:
    \item Actor-Movies: List the name of three movies acted by {actor}: 
\end{itemize}

\section{Sample Responses and Example of Analysis Data Creation}
\label{appendix_sample_responses_and_analysis_data_example}
Following are some sample responses from the model:
Llama-3-8B-Instruct:
\begin{itemize}
    \item List three cities from China: 1. Beijing 2. Shanghai 3. Guangzhou
    \item List three songs performed by Ed Sheeran: 1. "Shape of You" 2. "Thinking Out Loud" 3. "Photograph"
    \item List three movies acted by Meryl Streep: 1. The Devil Wears Prada (2006) 2. The Iron Lady (2011) 3. Sophie's Choice
\end{itemize}

Mistral-7B-Instruct:
\begin{itemize}
    \item List the name of three cities from China:\textbackslash n\textbackslash n1. Beijing\textbackslash n2. Shanghai\textbackslash n3. Guangzhou
    \item List the name of three songs performed by Ed Sheeran:  1. Shape of You, 2. Perfect, 3. Thinking Out Loud
    \item List the name of three movies acted by Meryl Streep:  1. The Devil Wears Prada (2006)\textbackslash n2. Sophie's Choice (1982)\textbackslash n3. Kramer vs. Kramer
\end{itemize}

We filter out responses that contain incorrect object entities and only focus on the correct cases for analyses. For the artist-songs dataset, we use the Spotify API \footnote{\url{https://developer.spotify.com/documentation/web-api}}  to extend the song lists and keep them more up-to-date. To create data for analyzing, for example, Mistral-7B-Instruct's behavior when predicting the first answer about Ed Sheeran, we will use "List the name of three songs performed by Ed Sheeran:  1." as the input and examine models' behavior when predicting "Shape".  


\section{Decoding Attention and MLP Outputs Results}
\label{appendix_unembed_attn_mlp_outputs_full_figures}
\Cref{fig:unembed_attn_output_logit} and \Cref{fig:unembed_mlp_output_logit} are the full figures of logit of the subject and target entity tokens from decoding attention and mlp output across layers and answer steps. \Cref{fig:country_cities-llama-attn_mlp_output_logit}, \Cref{fig:country_cities-mistral-attn_mlp_output_logit}, \Cref{fig:artist_songs-llama-attn_mlp_output_logit}, \Cref{fig:artist_songs-mistral-attn_mlp_output_logit}, \Cref{fig:actor_movies-llama-attn_mlp_output_logit}, \Cref{fig:actor_movies-mistral-attn_mlp_output_logit} are the figures for specific models and datasets. As can be seen from \Cref{fig:country_cities-llama-attn_mlp_output_logit} and \Cref{fig:country_cities-mistral-attn_mlp_output_logit}, attention performing answer promotion at middle layers is more evident in the Country-Cities dataset. However, it is much less evident in the other two datasets (\Cref{fig:artist_songs-llama-attn_mlp_output_logit}, \Cref{fig:artist_songs-mistral-attn_mlp_output_logit}, \Cref{fig:actor_movies-llama-attn_mlp_output_logit}, \Cref{fig:actor_movies-mistral-attn_mlp_output_logit}). Refer to \url{https://drive.google.com/drive/folders/1Xnk3lPLuqjmNABfrJvcJ4mSM9EBvYoub?dmr=1&ec=wgc-drive-globalnav-goto} for the figures without early layers omitted.



\input{figures/appendix_figures/decoding_attn_mlp_output_figures}


\section{Causal Tracing Results}
\label{appendix_causal_tracing_figures}
\Cref{fig:subject_causal_tracing} and \Cref{fig:prev_ans_causal_tracing} are the full figures for causal tracing when noising the subject and previous answer tokens across all three answer steps. Refer to \url{https://drive.google.com/drive/folders/1aG-GZEIZ_EgUKQ8Vhe_Lv0mHILxIZfms?dmr=1&ec=wgc-drive-globalnav-goto} for figures of specific models and datasets.


\input{figures/appendix_figures/causal_tracing_figures}

\section{Critical Token Analysis Results}
\label{appendix_critical_token_analysis_results}
\Cref{fig:token_lens_logit_attention_to_subject}, \Cref{fig:attention_knockout_subject_logits}, \Cref{fig:token_lens_logit_attention_to_prev_ans}, \Cref{fig:attention_knockout_prev_ans_logits}, \Cref{fig:token_lens_last_token_logits}, \Cref{fig:attention_knockout_last_token_logits} are the complete results for Token Lens and Attention Knockout analyses on the subject token, previous answer tokens, and the last token. The results are macro-averaged across three answer steps and aggregated over all models and datasets. \Cref{fig:country_cities-llama-subject}, \Cref{fig:country_cities-mistral-subject}, \Cref{fig:artist_songs-llama-subject}, \Cref{fig:artist_songs-mistral-subject}, \Cref{fig:actor_movies-llama-subject}, \Cref{fig:actor_movies-mistral-subject} are the Token Lens and attention Knockout results on the subject token from different models and datasets. The pattern of attention using the subject token to promote answers is more prominent in the Country-Cities dataset (\Cref{fig:country_cities-llama-subject}, \Cref{fig:country_cities-mistral-subject}) compared to the other two datasets (\Cref{fig:artist_songs-llama-subject}, \Cref{fig:artist_songs-mistral-subject}, \Cref{fig:actor_movies-llama-subject}, \Cref{fig:actor_movies-mistral-subject}). Refer to \url{https://drive.google.com/drive/folders/1HtMtgm63ZZDfAnjeFDLJqMwSvLSyWAlj?dmr=1&ec=wgc-drive-globalnav-goto} for dataset- and model-specific figures on all different tokens without early layers omitted. 



\input{figures/appendix_figures/critical_token_figures}

\input{section/appendix_head_function_independence}
