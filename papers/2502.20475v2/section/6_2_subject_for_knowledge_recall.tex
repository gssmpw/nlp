\subsection{Role of Subject Tokens}
\label{6_2_subject_tokens_for_knowledge_recall}

Across all models and datasets, attention and MLPs use subject tokens to contribute to answer promotions while suppressing the subject itself.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/critical_token_analysis/subject_logits.png}
    \caption{Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens when attending to or knocking out the subject tokens. Attention promotes and extracts subject information in the middle layers but suppresses it in later layers. MLPs promote the answers and suppress the subject at deeper layers. %
    }
    \label{fig:critical_token_analysis_subject_logits}
\end{figure*}

\paragraph{Attention first moves the subject to the last token position.} As shown in \Cref{fig:critical_token_analysis_subject_logits}, attention to the subject greatly increases the subject token's logit at the middle layers. To a lesser degree, it also promotes answer tokens, particularly at layer $25$.\footnote{Thus, the observation from \Cref{4_2_LMs_promote_then_suppress} that attention promotes answers at layer $25$ can be attributed to the subject token.} 
Answer promotion is most pronounced in the Country-Cities dataset (\Cref{fig:country_cities-llama-subject}, \Cref{fig:country_cities-mistral-subject}) but less evident in the Artist-Songs and Actor-Movies datasets (\Cref{fig:artist_songs-llama-subject}, \Cref{fig:artist_songs-mistral-subject}, \Cref{fig:actor_movies-llama-subject}, \Cref{fig:actor_movies-mistral-subject}). In all datasets, the subject logit is still larger than that of each answer across all answer steps, demonstrating that attention primarily copies or propagates subject information from the subject to the last token position. 



\paragraph{MLPs use the subject to promote answers.} From the middle to late layers, MLP logit differences of the answer tokens are all positive across answer steps. Combined with attention's promotion of the subject, our findings suggest a coordinated mechanism: attention propagates subject information to the last token, and MLPs leverage this information to promote relevant answers. 

\paragraph{At late layers, attention shifts from promoting to suppressing the subject.} Starting around the $28$th layer, attention outputs negative logits for the subject tokens. %
This transition shows that while attention initially promotes the subject, it later suppresses the subject to prevent incorrect generations, as the subject itself is not a correct answer. 

\paragraph{MLPs amplify subject suppression.} The MLPs' logit differences for the subject token become negative in later layers, especially at steps $2$ and $3$. This pattern illustrates that MLPs not only promote answers but also actively suppress the subject when it is no longer relevant for the next prediction. Combined with attention's suppression of the subject at later stages, our result suggests that MLPs amplify suppression signals from attention to prevent incorrect generations.

