%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

\usepackage{longtable}
\usepackage{listings}

\PassOptionsToPackage{table}{xcolor}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% \usepackage{algorithm}
% \usepackage[ruled,vlined]{algorithm2e}
% \usepackage{algpseudocode}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow}

% \usepackage[table]{xcolor}
% \definecolor{LightCyan}{rgb}{0.88,1,1}
% \definecolor{WildStrawberry}{rgb}{1.0, 0.26, 0.64}
\definecolor{YellowGreen}{rgb}{0.6, 0.8, 0.2}
% \definecolor{GoldenRod}{rgb}{0.85, 0.65, 0.13}
\definecolor{WildStrawberry}{RGB}{120, 180, 230}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Training-Free Length Extrapolation Approach: GALI}

\begin{document}

\twocolumn[
\icmltitle{A Training-Free Length Extrapolation Approach for LLMs:\\ Greedy Attention Logit Interpolation (GALI)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Yan Li}{sydney}
\icmlauthor{Tianyi Zhang}{unimelb}
\icmlauthor{Zechuan Li}{hunan}
\icmlauthor{Soyeon Caren Han}{unimelb}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sydney}{yali3816@uni.sydney.edu.au, School of Computer Science, The University of Sydney, Sydney, Australia}
\icmlaffiliation{hunan}{College of Electrical and Information Engineering, Hunan University, Changsha, China }
\icmlaffiliation{unimelb}{School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia}

% \icmlcorrespondingauthor{First Author}{first.author@email.com}
\icmlcorrespondingauthor{Soyeon Caren Han}{caren.han@unimelb.edu.au}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%follow the direction: Abstracts must be a single paragraph, ideally between 4--6 sentences long.
Transformer-based Large Language Models (LLMs) struggle to process inputs exceeding their training context window, with performance degrading due to positional out-of-distribution (O.O.D.) that disrupt attention computations. Existing solutions, fine-tuning and training-free methods, are limited by computational inefficiency, attention logit outliers or loss of local positional information. To address this, we propose Greedy Attention Logit Interpolation (GALI), a training-free length extrapolation method that maximizes the utilization of pretrained positional intervals while avoiding attention logit outliers through attention logit interpolation. The result demonstrates that GALI consistently outperforms state-of-the-art training-free methods. Our findings reveal that LLMs interpret positional intervals unevenly within their training context window, suggesting that extrapolating within a smaller positional interval range yields superior results—even for short-context tasks. GALI represents a significant step toward resolving the positional O.O.D. challenge, enabling more reliable long-text understanding in LLMs. Our implementation of GALI, along with the experiments from our paper, is open-sourced at \url{https://github.com/AcademyCityL/GALI}.
\end{abstract}

\section{Introduction}
%Caren's Rewriting
Transformer-based Large Language Models (LLMs) have become indispensable for a wide range of natural language processing tasks, yet their performance is fundamentally constrained by the training context window, i.e., the maximum input length used during training. When tasked with processing input text that exceeds this predefined limit, LLMs exhibit sharp performance degradation, with perplexity (PPL) increasing exponentially as input length grows \cite{peng2023yarn, chen2023extending, xiao2023efficient, han2024lm, Jin2024LLMML, an2024training}. This limitation poses significant challenges for applications requiring robust long-text understanding, such as document summarization, legal text analysis, and conversational AI.

The core issue lies in the model’s inability to generalize beyond the positional distributions encountered during pretraining, leading to disruptions in attention score computations—a phenomenon known as positional out-of-distribution (O.O.D.) \cite{chen2023extending, Jin2024LLMML, xu2024base}. Addressing positional O.O.D. is critical for enhancing LLMs’ length extrapolation capabilities and enabling reliable long-text processing.

Existing approaches to mitigating positional O.O.D. can be classified into three categories: (1) Lambda-Shaped Attention Mechanisms, which stabilize PPL but compromise the ability to capture long-range dependencies across distant tokens \cite{xiao2023efficient, han2024lm, jiang2024minference, li2024quickllama, zhang2024attention}; (2) Fine-Tuning on long texts, which involves training on datasets with extended positional contexts using interpolation \cite{chen2023extending, chen2023clex, Xiong2023EffectiveLS, li2023functional, ding2024longrope, li2024extending, wu2024never} or extrapolation \cite{Zhu2023PoSEEC, Chen2023LongLoRAEF, ding2024longrope}. While effective, this approach is resource-intensive and still encounters cases where positional IDs exceed its fine-tuned context window; and (3) Training-free length extrapolation methods, which include Rotary Position Embedding (RoPE) frequency interpolation techniques (e.g., Neural Tangent Kernel (NTK), Dyn-NTK, YaRN) \cite{LocalLLaMA-2023-ntk, LocalLLaMA-2023-dyn-ntk, peng2023yarn} and inputs rearrangement strategies (e.g., SelfExtend, ChunkLlama) \cite{Jin2024LLMML, an2024training}. They partially mitigate positional O.O.D. but suffer from limitations, such as the emergence of attention logit outliers or the loss of local positional information. 
% We categorize this type of method as an input rearrangement method.

To overcome the challenges above, this paper introduces Greedy Attention Logit Interpolation (GALI), a novel training-free length extrapolation method tailored for transformer-based LLMs. GALI has two primary objectives: (1) to maximize the utilization of pretrained positional intervals, thereby minimizing disruption to attention computations, and (2) to mitigate outliers in attention logits through an attention logit interpolation strategy.

\textbf{Key Innovations in GALI: }
First, GALI optimizes the utilization of pretrained position intervals. During the prefill phase, the method retains the model’s native position IDs for tokens within the training context window, ensuring that the corresponding hidden states maintain the quality achievable by the original model. GALI assigns interpolated position IDs chunk-wise for tokens exceeding the training context window, balancing efficiency with quality. During the decode phase, GALI dynamically generates interpolated position IDs for newly generated tokens, minimizing interpolation-induced disruptions during token generation.
Second, GALI introduces a novel approach to compute attention logits for interpolated positional intervals. With the help of linear interpolated position IDs, GALI performs local linear interpolation directly on attention logits instead of computing positional embeddings, avoiding the outlier issues commonly associated with positional O.O.D. Furthermore, it simulates the oscillatory characteristics of RoPE, by adding Gaussian noise proportional to relative distances, enhancing the model’s positional sensitivity.

GALI offers a training-free, plug-and-play solution that operates exclusively during inference by maximizing the utilization of pretrained positional intervals and focusing on attention logit interpolation. It extends the usable context window of LLMs, significantly improving long-text processing capabilities while integrating seamlessly with existing long-context frameworks.

\textbf{Empirical Evaluation: }
We evaluated GALI across three benchmarks, alongside a quantitative analysis of attention distribution, covering both real-world long-context applications and long-context language modeling. The results demonstrate that GALI consistently outperforms state-of-the-art training-free methods, achieving superior performance across diverse long-context scenarios. Furthermore, our analysis reveals a performance-enhancing strategy based on inconsistencies in how models interpret positional intervals. Notably, applying extrapolation methods within a smaller range of positional intervals leads to better results, even for short-context tasks. In summary, GALI represents a significant advancement in addressing the positional out-of-distribution (O.O.D.) challenge, unlocking new potential for LLMs in long-text understanding and processing.

Our main contributions are summarized as follows:
\begin{itemize}
\item \textbf{A Novel Training-Free Length Extrapolation Method}:
We introduce Greedy Attention Logit Interpolation (GALI), a training-free method that maximizes the utilization of pretrained positional intervals while leveraging attention logit interpolation to mitigate outliers, effectively addressing the positional O.O.D. problem in long-text processing for LLMs.

% \item \textbf{Simulating Oscillatory Positional Patterns}:
% GALI incorporates Gaussian noise proportional to relative distances into the interpolated attention logits, simulating the oscillatory characteristics of relative position encodings (e.g., RoPE). This innovation enhances the model's sensitivity to positional relationships.

\item \textbf{Comprehensive Evaluation and Analysis}:
We conducted a comprehensive comparison and analysis of GALI and other state-of-the-art training-free length extrapolation methods. While demonstrating GALI’s superior performance, we also identified a more effective length extrapolation strategy.
\end{itemize}

%Previous version - Yan written
% The performance of transformer-based Large Language Models (LLMs) is fundamentally constrained by the maximum input length used during training. During inference, when input text exceeds this length, the model’s performance degrades, and perplexity (PPL) increases exponentially with growing input length \cite{peng2023yarn, chen2023extending, xiao2023efficient, han2024lm, Jin2024LLMML, an2024training}.

% This degradation primarily stems from the model’s inability to process unseen positional information beyond its pretraining context limit, disrupting the distribution of attention scores—a phenomenon known as positional out-of-distribution (O.O.D) \cite{chen2023extending, Jin2024LLMML,xu2024base}. Addressing positional O.O.D is therefore essential for improving LLMs’ length extrapolation ability and enabling effective long-text processing.

% % Transformer models, while inherently position-agnostic due to their self-attention mechanism, rely on position encoding to capture sequence order. 

% % Although causal attention mechanisms provide some positional sensitivity \cite{haviv2022transformer, chi-etal-2023-latent}, this is insufficient to handle inputs exceeding the pretraining context length \cite{ma2024mesa}. 


% Existing solutions to the positional O.O.D issue fall into three categories: (1) Lambda-Shaped Attention Mechanisms \cite{xiao2023efficient, han2024lm, jiang2024minference, li2024quickllama, zhang2024attention}, which stabilize perplexity (PPL) but sacrifice the model’s capacity to capture long-range dependencies across distant tokens; (2) Fine-Tuning on Long Texts, which involves training on long-text datasets using positional interpolation \cite{chen2023extending, chen2023clex, Xiong2023EffectiveLS, li2023functional, ding2024longrope, li2024extending, wu2024never} or extrapolation \cite{Zhu2023PoSEEC, Chen2023LongLoRAEF, ding2024longrope}. However, this approach is resource-intensive and does not fundamentally resolve the issue, as fine-tuned models often struggle with texts containing positional IDs beyond their training range; and (3) Training-Free Length Extrapolation Methods, which include techniques like RoPE frequency interpolation (e.g., NTK, Dyn-NTK, YaRN) \cite{LocalLLaMA-2023-ntk, LocalLLaMA-2023-dyn-ntk, peng2023yarn}, that partially mitigate positional O.O.D issues, as well as group position IDs (e.g., SelfExtend) \cite{Jin2024LLMML} and chunk position IDs (e.g., ChunkLlama) \cite{an2024training}, which avoid positional O.O.D but suffer from drawbacks such as the loss of local positional information.

% To address these challenges, we propose a novel training-free length extrapolation approach called Greedy Attention Logit Interpolation (GALI). GALI effectively balances efficiency, quality, and compatibility with transformer-based LLMs. It offers two key advantages over existing methods.

% First, the timing of interpolation is optimized. During the prefill stage, GALI retains the model’s native position IDs for inputs within the original maximum input length, ensuring optimal hidden states for these contexts. For tokens exceeding this length, the input is chunked, and each chunk is assigned interpolated position IDs, striking a balance between computational efficiency and hidden state quality. During the decode stage, GALI generates a new set of interpolated position IDs for each newly generated token, theoretically minimizing the impact of interpolation on new token generation.

% Second, GALI innovates in the computation of interpolated position IDs. It performs linear interpolation on position IDs and applies local linear interpolation directly to attention logits with the help of interpolated position IDs. This avoids the attention logits outlier issue associated with position embedding calculations in positional O.O.D.(PI). Moreover, GALI incorporates oscillatory patterns in interpolated attention logits by adding Gaussian noise with variance proportional to relative distances. This simulates the oscillatory characteristics observed in relative position encodings, such as RoPE (PI), enhancing positional sensitivity.

% GALI maximizes the utilization of pretrained position encoding information while mitigating positional O.O.D through attention logit interpolation. Importantly, it operates exclusively during inference as a training-free, plug-and-play method. By extending the number of usable position IDs, GALI enhances long-text processing capabilities and integrates seamlessly with higher-level long-context processing frameworks.

% We evaluated GALI against existing training-free length extrapolation methods across five benchmarks encompassing language modeling, synthetic long-context tasks, and real-world long-context tasks. The results demonstrate that GALI outperforms state-of-the-art methods, achieving superior performance on long-context inputs.

% Our main contributions are summarized as follows:

% \begin{itemize}
% \item A Novel Training-Free Length Extrapolation Method GALI, which maximizes the use of pretrained position encoding information and avoids attention logits outliers through efficient attention logit interpolation.
% \item We conduct a Comprehensive Comparison and analysis of existing training-free length extrapolation methods, demonstrating GALI’s superior performance and highlighting its advantages for long-text processing.
% \end{itemize}


\section{Background}
\textbf{Rotary Position Embedding (RoPE): }
RoPE \cite{su2024roformer} is a technique that encodes positional information by applying rotary transformations to token embeddings, enabling relative position modeling in transformers. Given two token embeddings $\bm{x}_m, \bm{x}_n \in \mathbb{R}^l$ as query and key corresponding to position $m$ and $n$, the projection matrix $\bm{W}_Q, \bm{W}_K \in \mathbb{R}^{d \times l}$, RoPE applies a rotation to the projected token embeddings, i.e., $\bm{q}_m^{r}=(\bm{W}_Q\bm{x}_m)e^{im\bm{\theta}}$, $\bm{k}_n^{r}=(\bm{W}_K\bm{x}_n)e^{in\bm{\theta}}$, where $\bm{\theta}=[b^{0}, b^{-2/d}, \dots, b^{-2(j-1)/d}]$, $j \in [1,2, \dots, d/2]$ and $b$ is originally set to 10000. After that, the inner product between the query $\bm{q}_m^{r}$ and key $\bm{k}_n^{r}$ can be represented by the real part of ${\bm{q}_m^{r}}^*\bm{k}_n^{r}$, i.e.:

\vspace{-0.4cm}
\begin{flalign}
&\langle\bm{q}_m^{r},\bm{k}_n^{r}\rangle_{\mathbb{R}} = \operatorname{Re}(\langle(\bm{W}_Q\bm{x}_m)e^{im\bm{\theta}},(\bm{W}_K\bm{x}_m)e^{in\bm{\theta}}\rangle_{\mathbb{C}}) \nonumber\\
&= \operatorname{Re} \left[ \sum_{j=0}^{d/2 - 1} ({\bm{W}_Q\bm{x}_m})_{[2j:2j+1]}(\bm{W}_K\bm{x}_n)^*_{[2j:2j+1]} e^{i(m-n)\theta_j} \right] \nonumber\\
&= a(\bm{x}_m, \bm{x}_n, m-n)
\end{flalign}

$g(\cdot)$ is the function mapping token embeddings $\bm{x}_m, \bm{x}_n$ to the attention logit, which depends on their relative distance and is irrelevant to their absolute positions. Additionally, RoPE exhibits a long-term decay as relative distance increases \cite{su2024roformer}, as illustrated in Figure \ref{fig:logits_long_term_decay}.
Our proposed method, GALI, leverages two key properties of RoPE to achieve position interpolation and length extrapolation effectively.


% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/rope_longtermdecay.jpg}
%     \caption{The relative upper bound with the relative distance, from \cite{su2024roformer}}
%     \label{fig:rope_longtermdecay}
% \end{figure}

\begin{figure}[thp]
    \centering
    \scriptsize
    \subfigure[]{
        \includegraphics[width=0.45\linewidth,trim=10 0 10 5, clip]{images/attention_logits_-1_-2.png}
        \label{fig:logits_-1_-2}
    }
    \subfigure[]{
        \includegraphics[width=0.45\linewidth,trim=10 0 10 5, clip]{images/attention_logits_-1_0.png}
        \label{fig:logits_-1_0}
    }
    %rewritten by Caren
    \caption{Visualization of long-term decay in attention logits. The sentence “The quick brown fox jumps over the lazy dog.” is fed into a one-layer Llama3-8b-ins model. Figure \ref{fig:logits_-1_-2} shows the attention logits from the last token to the second-to-last token. Figure \ref{fig:logits_-1_0} presents the logits from the last token to the first token. As the positional ID interval increases from 1 to 819, a clear decay phenomenon in the logits is observed.}
    \label{fig:logits_long_term_decay}
\end{figure}




% \begin{figure}[ht]
%     \centering
%     \subfigure[Subfigure 1]{
%         \includegraphics[width=0.45\linewidth]{images/chunkllama_placeholder.png} % 替换为实际图像路径
%     }
%     \subfigure[Subfigure 2]{
%         \includegraphics[width=0.45\linewidth]{images/chunkllama_placeholder.png} % 替换为实际图像路径
%     }
%     \subfigure[Subfigure 3]{
%         \includegraphics[width=0.45\linewidth]{images/chunkllama_placeholder.png} % 替换为实际图像路径
%     }
%     \subfigure[Subfigure 4]{
%         \includegraphics[width=0.45\linewidth]{images/chunkllama_placeholder.png} % 替换为实际图像路径
%     }
%     \caption{A single-column figure with 4 subfigures.}
%     \label{fig:single-column}
% \end{figure}
\textbf{Positional Out-Of-Distribution (O.O.D.): }
In Transformer architectures, the self-attention mechanism is inherently position-agnostic, necessitating the use of position embeddings to encode positional information for processing ordered inputs \cite{Dufter2021PositionII, kazemnejad2024impact}. Even in large language models (LLMs) with causal attention, explicit positional encoding through position embeddings remains the standard approach.\footnote{Recent studies suggest causal attention implicitly encodes positional information, enabling performance without explicit position embeddings. However, this is beyond the scope of this paper.} During inference, when LLMs encounter input sequences exceeding the maximum length seen during training, the use of unseen position IDs causes a positional out-of-distribution (O.O.D.) issue, leading to degraded performance \cite{chen2023extending, Jin2024LLMML, xu2024base}. In the Rotary Position Embedding (RoPE) mechanism, extrapolating position IDs beyond the training range introduces untrained positional intervals, disrupting the attention score distribution \cite{chen2023extending}. In contrast, position interpolation has yielded more stable attention distributions, requiring fewer fine-tuning steps \cite{chen2023extending}. This observation has inspired subsequent interpolation-based methods \cite{chen2023extending, chen2023clex, Xiong2023EffectiveLS, li2023functional, ding2024longrope, li2024extending, wu2024never}, as well as training-free approaches that map position interpolation into alternative frequency dimensions in embeddings \cite{LocalLLaMA-2023-ntk, LocalLLaMA-2023-dyn-ntk, peng2023yarn}. Recent work has explored other training-free length extrapolation techniques, such as group position IDs \cite{Jin2024LLMML} or chunk attention \cite{an2024training}.

\section{Method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/main.png}
    %rewritten by Caren
    \caption{The overall procedure of the proposed GALI framework. The green dashed line illustrates \textbf{position ID interpolation}, while the blue dashed line shows \textbf{attention logit interpolation}. In this example, the training context window $L_{tr}$ is 4, the chunk size $s$ is 2, the local window $L_{w}$ is 2, and the prefill length is 6. Chunks are denoted as $c_1$ (first chunk) and $c_2$ (second chunk), while $t_7$ represents the first generated token. The relative distance matrix $R$ incorporates $\lceil R \rceil$, $\lfloor R \rfloor$, and $R^{\prime}$, representing ceiling, floor, and modulo operations, respectively. Red numbers in the relative distance matrix indicate interpolated positional intervals and $\mathcal{N}$ represents the Gaussian noise.}
    \label{fig:overall_method}
\end{figure*}


This section presents Greedy Attention Logit Interpolation (GALI), a novel training-free position interpolation method designed to enhance the utilization of pretrained positional information while ensuring stable attention computations. GALI is guided by two objectives: (1) Maximizing the use of pretrained positional information, i.e., the positional intervals encountered during pre-training. To do so, we perform minimal-impact position interpolation for each chunk in the prefill phase and newly generated tokens during the decoding phase. (2) Preventing outliers in attention logits by avoiding direct computation of position embeddings for interpolated positions. We apply local linear interpolation to the attention logits based on interpolated position IDs and simulate RoPE’s oscillatory behavior by introducing Gaussian noise. The process is shown in Figure \ref{fig:overall_method}.

\subsection{Position ID Interpolation}
The proposed GALI introduces a strategy for position ID interpolation to handle inputs exceeding the maximum training length, aiming to fully utilize pretrained relative positional intervals. During the prefill phase, GALI segments the input portion beyond the maximum training length into multiple chunks and performs position ID interpolation for each chunk individually. In contrast, the portion within the maximum training length remains unchanged, without any interpolation. 
This approach provides several advantages. First, if the input length does not exceed the maximum training length, the prefill phase remains entirely unaffected, preserving the model’s original capabilities. Second, when the input length surpasses the maximum training length, the text within the training range can fully leverage the pretrained relative positional intervals to generate hidden states based on the model’s original performance. Finally, for inputs beyond the training length, GALI can apply position ID interpolation in a manner that minimizes the impact on model quality for each chunk. Specifically, each chunk only adds the exact number of new position IDs it requires. This ensures the interpolation process maintains a balance between computational efficiency and fidelity.

This approach draws inspiration from Dyn-NTK\cite{LocalLLaMA-2023-dyn-ntk}, which adjusts scaling factors to handle inputs of varying lengths. However, dynamic NTK applies a uniform interpolation across the entire input during the prefill phase, potentially overlooking the varying interpolation requirements of individual tokens. For instance, a token just beyond the training context window only requires the interpolation of one new position ID, whereas subsequent tokens would require progressively more. Ideally, the interpolation for each token would be customized to minimize its impact on quality. However, performing interpolation at the token level is computationally inefficient. To address these challenges, GALI segments the input exceeding the training length into chunks and applies interpolation only for the position IDs that extend beyond the training range within each chunk. By tailoring interpolation to the specific needs of each chunk, GALI avoids the uniform scaling factors used in previous methods, such as NTK, Dyn-NTK, YaRN, or SE, which can lead to suboptimal results. This chunk-wise strategy ensures a more adaptive and efficient interpolation process while maintaining model performance.

Building on insights from prior studies, GALI retains a local window of size $L_w$ around the current token to preserve the original positional intervals within this range. This design allows the current token to effectively interpret its immediate context using the model’s pretrained capabilities. During the decode phase, where tokens are generated sequentially, GALI dynamically interpolates the required position IDs for each newly generated token, focusing on minimizing quality degradation.
Specifically, in the prefill stage, given an input sequence $S = (w_1, w_2, \dots, w_{L_{tr}}, \dots, w_{L})$ where $L_{tr}$ is the size of training context window and $L$ is the input length in the prefill stage. We first divide it into chunks $C = (c_1, c_2, \dots, c_{L_{c}})$, where the size of $c_1$ is $L_{tr}$ 
 and other chunks have a size $s$, so the $L_c = \lceil {\frac{L-L_{tr}}{s}} \rceil$. After that, we assign position IDs $S = (s_1, s_2, \dots, s_{L_{c}})$ for each chunk, where $s_1=[0,1,\dots,L_{tr}-1]$ and others are interpolated position IDs according to the following formula:

 % where $j$ is the chunk index
\vspace{-0.7cm}
\begin{flalign}
    \begin{cases}
        & j > 1; \\
        & {g}_j = \lceil {\frac{\sum_{i=1}^{j}{len(c_i)}-L_w}{L_{tr} - L_w}} \rceil; \\
        & {v}_j = 1/{g}_j; \\ 
        & s_j = [0, 1*{v}_j, 2*{v}_j, \dots, ({g}_j - 1) * {v}_j, 1, \dots, \\
        & L_{tr}-L_w-1, L_{tr}-L_w, \dots, L_{tr}-1]
    \end{cases}
    \label{equ:pid interpolation}
\end{flalign}

We first calculate the minimal group size ${g}_j$ required for each chunk after excluding the local window $L_w$ and then apply linear interpolation within each group. During the decode phase, each newly generated token is treated as an individual chunk of size 1, enabling the computation of minimally impactful interpolated position IDs.
 

\subsection{Attention Logit Interpolation}

When calculating attention scores, we approximate attention logits for interpolated positions using local linear interpolation based on interpolated position IDs, while introducing Gaussian noise that scales with relative position intervals. Unlike NTK or YaRN, our methods avoid outlier attention logits from the position embeddings \cite{chen2023extending}. RoPE's trigonometric functions, while effective within pretrained relative position intervals, can produce extreme values during interpolation, leading to abnormal attention logits. To mitigate this, we bypass position embeddings and directly approximate attention logits.
Building on the monotonic trends and oscillatory behavior observed in RoPE, as shown in Figure \ref{fig:logits_long_term_decay}, we hypothesize that for two tokens with an interpolated relative distance (e.g., $7.r$), their attention logits—excluding oscillatory effects—should lie between $g(7)$ and $g(8)$, with their exact value determined by $r$. To achieve this, we compute attention logits for each QK pair with interpolated relative position intervals using local linear interpolation and simulate oscillatory behavior by adding Gaussian noise, whose variance increases with relative position intervals. This method eliminates the outliers identified in the PI study and produces interpolated attention logits for new relative position intervals in a training-free and effective manner.

Specifically, given two token embeddings $\bm{x}_m, \bm{x}_n \in \mathbb{R}^l$ as query and key corresponding to position $m$ and $n$. Note here the $m$ and $n$ can be interpolated float position ids. Let $r=m-n$, when ${r}{\%}1=0$ we compute their attention logits directly which means their relative position interval was pretrained. For the case ${r}{\%}1>0$, which means that their relative positional interval is interpolated, we then compute the attention logit $a(\bm{x}_m, \bm{x}_n, r)$ using the formula:
\vspace{-0.5cm}
\begin{flalign}
&a(\bm{x}_m, \bm{x}_n, r) = a(\bm{x}_m, \bm{x}_n,\lfloor{r}\rfloor) - [a(\bm{x}_m, \bm{x}_n, \lfloor{r}\rfloor))-\\
&a(\bm{x}_m, \bm{x}_n, \lceil{r}\rceil] \nonumber *({r}{\%}1)+\mathcal{N}(0, {\frac{r}{L_{tr}}}^2)
\label{equ:attn logits interpolation}
\end{flalign}
\vspace{-0.3cm}

Where $\%$ represents the $\mod$ operation. To enable matrix modification of relative distances during computation, we adopted an approximate implementation that replaces $r$ with $r=\lceil{m}\rceil - n$, as detailed in the pseudo-code (Appendix \ref{app: gali pseudo code}).


\section{Experiments}
We evaluate GALI on Llama3-8B-ins models across two task categories: real-world long-context tasks and long-context language modeling tasks. For comparison, we implement all published training-free length extrapolation methods, including NTK\cite{LocalLLaMA-2023-ntk}, Dyn-NTK\cite{LocalLLaMA-2023-dyn-ntk}, YARN\cite{peng2023yarn}, SelfExtend\cite{Jin2024LLMML}, and ChunkLlama\cite{an2024training}. The following sections describe the experimental setup for each task. Appendix \ref{app: data stastics} details data statistics.

\subsection{Emperiments Setup}
\textbf{Real-world long-context task: } We evaluate GALI on two widely used long-context benchmarks, LongBench\cite{bai-etal-2024-longbench} and L-Eval\cite{an-etal-2024-l}. LongBench is a bilingual long-context understanding benchmark with 21 datasets crossing 6 tasks, including Single-Document QA, Multi-Document QA, Summarization, Few-shot Learning, Synthetic Task, and Code Completion. We use 16 English datasets from LongBench. L-Eval is a comprehensive long-context evaluation suite with 20 sub-tasks, 508 long documents, and over 2000 human-labeled query-response pairs. It consists of closed-ended and open-ended task groups. We focus on closed-ended groups, which assess reasoning and understanding over a long-context. For consistency, we follow the official task prompt templates and truncation strategies from the respective benchmarks. 

\textbf{Long-context language modeling task: } To evaluate GALI's long-context language modeling capabilities, we use PG19\cite{Rae2019CompressiveTF}, an open-vocabulary language modeling benchmark derived from Project Gutenberg. PG19 consists of over 28,000 books published before 1919, covering diverse genres and writing styles. We use its test split, which includes 100 samples.

% \paragraph{Synthetic long-context task} We test "Needle in a Haystack" \cite{Nelson2024NeedleIT} for this task. "Needle in a Haystack" is a passkey retrieval task that ask LLM to extract a piece of text according to the instruction from the surrounded unrelated text. In our work, we ask LLM to extract a random number range from 50000 to 50000 pointed by instruction. 




\begin{table*}[t]
\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
% \small
% \footnotesize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccccccccccccc}
\specialrule{1pt}{0pt}{2pt}
\toprule
 &\multirow{4}{*}{~~~~~~~~~~~~~\textbf{Methods}~~~~~~~~~~~~~} & \multicolumn{3}{c}{\textbf{Single document QA}} & \multicolumn{3}{c}{\textbf{Multi document QA}} & \multicolumn{3}{c}{\textbf{Summarization}} & \multicolumn{3}{c}{\textbf{Few-shot Learning}} & \multicolumn{2}{c}{\textbf{Synthetic}} & \multicolumn{2}{c}{\textbf{Code}} & \multirow{4}{*}{\textbf{Average}} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} \cmidrule(r){12-14} \cmidrule(r){15-16} \cmidrule(r){17-18}
& & \rotatebox{30}{NarrativeQA} & \rotatebox{30}{Qasper} & \rotatebox{30}{MultiField-en} & \rotatebox{30}{HotpotQA} & \rotatebox{30}{2WikiMQA} & \rotatebox{30}{Musique} & \rotatebox{30}{GovReport} & \rotatebox{30}{QMSum} & \rotatebox{30}{MultiNews} & \rotatebox{30}{TREC} & \rotatebox{30}{TriviaQA} & \rotatebox{30}{SAMSum} & \rotatebox{30}{PassageCount} & \rotatebox{30}{PassageRe} & \rotatebox{30}{Lcc} & \rotatebox{30}{RepoBench-P} & \\

\midrule

\multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-4k}}  &  Original  &  17.83  &  40.62  &  47.02  &  40.97  &  35.15  &  20.99  &  27.76  &  19.70  &  24.62  &  71.00  &  89.54  &  42.31  &  6.00  &  23.50  &  56.96  &  49.06  &  \cellcolor{YellowGreen!25}{38.31} \\
 &  SelfExtend-16k  & 23.34 & 44.59 & 51.22 & 44.91 & 37.43 & 29.50 & 28.52 & 22.14 & 24.34 & 75.50 & 90.71 & 42.58 & 7.50 & 92.50 & 54.99 & 50.83 & \cellcolor{YellowGreen!25}{45.04} \\
 &  ChunkLlama-16k & 20.91 & 40.15 & 49.87 &  47.71  &  \textbf{40.80}  & 28.75 & 30.37 & 21.81 & 24.32 & 74.50 & 90.29 & 41.78 & 2.50 & 56.75 &  \textbf{58.99}  &  \textbf{57.55}  & \cellcolor{YellowGreen!25}{42.94} \\
 &  NTK-16k  &  22.59  &  \textbf{46.25}  & 53.21 &  \textbf{51.91}  & 37.51 & 26.56  &  \textbf{30.69}  &  \textbf{22.74}  & 24.03 & 73.50 & 90.46 & 42.20  &  \textbf{11.50}  & 73.00 & 34.53 & 36.39 & \cellcolor{YellowGreen!25}{42.32} \\
 &  Dyn-NTK-16k & 18.65 & 44.91 & 51.37 & 46.28 & 37.57 & 28.03 & 30.20 & 21.53 & 24.48  &  76.00  &  89.11  &  42.88  &  9.00  & 74.50  & 53.91 & 32.65 & \cellcolor{YellowGreen!25}{42.57} \\
 &  YaRN-16k & 16.43 & 40.13  &  \textbf{53.04}  & 45.93 & 33.66 & 28.51 & 30.40 & 22.42 & 23.24 & 75.50 &  91.04  &  \textbf{44.53}  & 6.50 & 86.50 & 43.26 & 48.26 & \cellcolor{YellowGreen!25}{43.08} \\
  & \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-16k}  &  \cellcolor{WildStrawberry!25}\textbf{24.69}  &  \cellcolor{WildStrawberry!25}45.26  &  \cellcolor{WildStrawberry!25}51.78  &  \cellcolor{WildStrawberry!25}51.33  &  \cellcolor{WildStrawberry!25}37.16  &  \cellcolor{WildStrawberry!25}\textbf{30.79}  &  \cellcolor{WildStrawberry!25}29.28  &  \cellcolor{WildStrawberry!25}22.65  &  \cellcolor{WildStrawberry!25}\textbf{24.63}  &  \cellcolor{WildStrawberry!25}\textbf{77.00}  &  \cellcolor{WildStrawberry!25}\textbf{91.61}  &  \cellcolor{WildStrawberry!25}42.92  &  \cellcolor{WildStrawberry!25}9.00  &  \cellcolor{WildStrawberry!25}\textbf{95.5}  &  \cellcolor{WildStrawberry!25}56.84  &  \cellcolor{WildStrawberry!25}49.04  &  \cellcolor{YellowGreen!25}{\textbf{46.22}} \\
\midrule

\multirow{8}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}}  &  
Original\color{blue}\textsuperscript{*} & 21.71 & 44.24 & 44.54 & 46.82 & 36.42 & 21.49 & 30.03 & 22.67 & \textbf{27.79} & 74.50 & 90.23 & \textbf{42.53} &  0.00  &  67.00  &  57.00  & 51.22 & \cellcolor{YellowGreen!25}{42.39} \\
 &  SelfExtend-16k\color{blue}\textsuperscript{*} & 21.50 & 43.96 & \textbf{50.26} & 48.18 & 28.18 & 25.58 & \textbf{34.88} & \textbf{23.83} & 26.96 & 75.50 & 88.26 & 42.01 & 4.12 & 88.00 & 36.58 & 37.73 & \cellcolor{YellowGreen!25}{42.22} \\
 % &  SelfExtend-16k & 24.61 & 43.88 & 50.13 & 49.36 & 37.36 & 27.53 & 29.74 & 22.44 & 24.62 & 76.00 & 90.71 & 42.36 & 6.50 & 84.00 & 56.81 & 50.18 & 44.76 \\
 &  ChunkLlama-16k & 23.87 & 43.86 & 46.97 & 49.37 & 35.34 & 26.52 & 31.06 & 21.99 & 24.45 & 76.00 & 90.73 & 42.29 & \textbf{7.00} & 72.00 & \textbf{59.93} & \textbf{56.98} & \cellcolor{YellowGreen!25}{44.27} \\
 &  NTK-16k & 8.04 & 43.85 & 47.94 & 20.44 & 34.32 & 1.57 & 24.31 & 13.22 & 24.12 & 74.50 & 52.18 & 33.12 & 4.50 & 45.50 & 46.84 & 38.71 & \cellcolor{YellowGreen!25}{32.07} \\
 &  Dyn-NTK-16k & 8.19 & 43.31 & 47.91 & 34.63 & 35.26 & 7.92 & 26.83 & 17.85 & 24.51 & 76.50 & 71.72 & 39.15 & 5.67 & 83.50 & 56.58 & 46.39 & \cellcolor{YellowGreen!25}{39.12} \\
 &  YaRN-16k & 12.39 & 42.60 & 51.70 & 40.06 & 35.03 & 12.81 & 30.30 & 22.56 & 23.51 & 75.50 & 82.99 & 42.31 & 6.50 & \textbf{89.00} & 50.51 & 51.58 & \cellcolor{YellowGreen!25}{41.83} \\
 &  \cellcolor{WildStrawberry!25} \textbf{(Ours)GALI-16k}  &  \cellcolor{WildStrawberry!25}\textbf{25.88}  &  \cellcolor{WildStrawberry!25}\textbf{45.65}  &  \cellcolor{WildStrawberry!25}47.09  &  \cellcolor{WildStrawberry!25}\textbf{51.07}  &  \cellcolor{WildStrawberry!25}\textbf{37.42}  &  \cellcolor{WildStrawberry!25}\textbf{28.75}  &  \cellcolor{WildStrawberry!25}30.09  &  \cellcolor{WildStrawberry!25}22.7  &  \cellcolor{WildStrawberry!25}24.58  &  \cellcolor{WildStrawberry!25}\textbf{77.00}  &  \cellcolor{WildStrawberry!25}\textbf{90.91}  &  \cellcolor{WildStrawberry!25}42.43  &  \cellcolor{WildStrawberry!25}6.00  &  \cellcolor{WildStrawberry!25}83.00  &  \cellcolor{WildStrawberry!25}57.04  &  \cellcolor{WildStrawberry!25}53.06  & \cellcolor{YellowGreen!25}{\textbf{45.17}} \\

\midrule


\multirow{8}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}}  &  Original\color{blue}\textsuperscript{*} & 21.71 & 44.24 & 44.54 & 46.82 & 36.42 & 21.49 & 30.03 & 22.67 & \textbf{27.79} & 74.50 & 90.23 & 42.53 & 0.00 & 67.00 & 57.00 & 51.22 & \cellcolor{YellowGreen!25}{42.39} \\
 &  SelfExtend-32k\color{blue}\textsuperscript{*} & 12.04 & 12.10 & 20.15 & 8.22 & 9.68 & 3.89 & 27.90 & 14.58 & 22.13 & 61.00 & 82.82 & 1.40 & 2.37 & 2.83 & 57.87 & 56.42 & \cellcolor{YellowGreen!25}{24.71} \\
 &  SelfExtend-32k & 26.27 & 44.23 & 50.19 & 48.28 & 38.29 & 29.19 & 29.24 & 22.68 & 24.59 & 76.00 & 90.16 & 42.45 & 8.00 & 88.00 & 57.47 & 49.51 & \cellcolor{YellowGreen!25}{45.28} \\
 &  ChunkLlama-32k & 24.48 & 42.37 & 47.05 & 48.79 & 34.53 & 26.94 & \textbf{32.08} & \textbf{23.40} & 24.36 & 76.00 & 90.46 & 42.08 & 6.50 & 72.00 & \textbf{59.52} & \textbf{60.54} & \cellcolor{YellowGreen!25}{44.44} \\
 &  NTK-32k & 7.31 & 45.11 & \textbf{53.18} & 52.31 & 37.70 & 27.37 & 29.37 & 21.45 & 23.69 & 73.50 & 78.25 & 41.83 & \textbf{9.00} & 69.00 & 34.25 & 36.12 & \cellcolor{YellowGreen!25}{39.97} \\
 &  Dyn-NTK-32k & 23.06 & 43.95 & 48.55 & \textbf{52.68} & 37.46 & 25.22 & 31.53 & 22.19 & 24.52 & \textbf{77.00} & 90.96 & 42.42 & 8.00 & 71.50 & 56.77 & 43.78 & \cellcolor{YellowGreen!25}{43.72} \\
 &  YaRN-32k & 17.09 & 40.90 & 52.51 & 46.40 & 33.92 & \textbf{29.47} & 29.93 & 22.69 & 23.11 & 75.00 & \textbf{91.29} & \textbf{42.54} & 5.50 & \textbf{89.50} & 46.50 & 51.38 & \cellcolor{YellowGreen!25}{43.61} \\
 &  \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-32k}  & \cellcolor{WildStrawberry!25}\textbf{28.63}  &  \cellcolor{WildStrawberry!25}\textbf{45.66}  &  \cellcolor{WildStrawberry!25}47.23  &  \cellcolor{WildStrawberry!25}51.07  &  \cellcolor{WildStrawberry!25}\textbf{38.35}  &  \cellcolor{WildStrawberry!25}29.00  &  \cellcolor{WildStrawberry!25}29.98  &  \cellcolor{WildStrawberry!25}22.79  &  \cellcolor{WildStrawberry!25}24.59  &  \cellcolor{WildStrawberry!25}\textbf{77.00}  &  \cellcolor{WildStrawberry!25}91.13  &  \cellcolor{WildStrawberry!25}42.38  &  \cellcolor{WildStrawberry!25}5.50  &  \cellcolor{WildStrawberry!25}83.00  &  \cellcolor{WildStrawberry!25}57.07  &  \cellcolor{WildStrawberry!25}52.63 & \cellcolor{YellowGreen!25}{\textbf{45.38}} \\

% \midrule

%  &  GPT-3.5-Turbo-16k\color{green}\textsuperscript{*} &  23.60 & 43.30 & 52.30 & 51.60 & 37.70 & 26.90 & 29.50 & 23.40 & 26.70 & 68.00 & 91.40 & 41.70 & 4.50 & 71.00 & 54.70 & 53.60 & 43.74 \\
%  &  CLEX-7B-16k\color{green}\textsuperscript{*} &  18.05 & 23.68 & 44.62 & 28.44 & 19.53 & 9.15 & 32.52 & 22.90 & 25.55 & 68.00 & 84.92 & 42.82 & 0.00 & 11.50 & 59.01 & 56.87 & 34.22 \\


\bottomrule
\end{tabular}
}
%rewritten by caren
\caption{Performance comparison across different backbone LLMs and training-free length extrapolation methods. The best result in each experiment is bolded. Results marked with {\color{blue}\textsuperscript{*}} are reported by LongBench \cite{Jin2024LLMML}. The number following each method denotes the target context window size (e.g., 16k represents 16 × 1024 tokens). "Original" refers to evaluations conducted using the backbone model in the left column. Additional results using the Llama2-7B-Chat-4K backbone are provided in Appendix \ref{app:real world task}.
% \caption{Performance comparison with different backbone LLMs and training-free length extrapolation methods. The best result in each experiment has been bolded.
% {\color{blue}\textsuperscript{*}} indicates the results reported by LongBench\cite{Jin2024LLMML}. 
% The number following each method represents the target context window. For example, 16k means 16 × 1024. The "Original" means testing with the backbone model, i.e., the model shown in the left column. We also report the results using Llama2-7b-chat-4k backbone in the Appendix \ref{app:real world task}. 
% blue se, orange chunkllama, green longbench, yellow CLEX, red L-eval 
}
\label{tab:longbench_all}
\end{table*}

% Llama2-7b-chat-4k, 
\textbf{Backbone models and baseline methods: } We use Llama3-8b-ins-4k (Llama3-4k) and Llama3-8b-ins-8k (Llama3-8k) as backbone models, where the number following each model indicates its initial context window size. We obtain Llama3-4k backbone via modifying its max\_position\_embedding parameter. We use shorter-versions of Llama3-8b-ins over other LLMs with shorter training context windows like LLama2 since it cannot fully understand all pretrained positional intervals, which limits GALI’s effectiveness in practice. The effective understanding range of LLMs is shorter than their maximum training window, as evidenced in \cite{Jin2024LLMML, Hsieh2024RULERWT}. For the baseline methods, we compare with NTK\cite{LocalLLaMA-2023-ntk}, Dyn-NTK\cite{LocalLLaMA-2023-dyn-ntk}, YaRN\cite{peng2023yarn} using huggingface implementation and SelfExtend\cite{Jin2024LLMML}, ChunkLlama\cite{an2024training} with their official implementation. They are all of the training-free length extrapolation methods up to now. The implementation details of these methods can be found in Appendix \ref{app: imp. details}.


\subsection{Real-World long-context Task Results}
\label{main:Real-world long-context task results}

The LongBench results (Table \ref{tab:longbench_all}) highlight GALI’s strong average performance on the Llama3-8b-ins backbone series, surpassing both the 4k and 8k backbone models and other methods. GALI excels in question answering (QA) and few-shot learning tasks but showed weaker performance in summarization, synthetic, and code-related tasks. While extending the context window to 32k improves performance on Llama3-8k compared to 16k, it remains less effective than using 16k on Llama3-4k. This consistent trend in Figure \ref{fig:lbench_trend} highlights limitations in extrapolation performance, which we analyze below.

Firstly, LLMs interpret positional intervals differently within their training context window, as noted in \cite{Hsieh2024RULERWT}. This explains why using a 16k context window on Llama3-4k outperformed using the same window on Llama3-8k. Since LLMs are trained via next-token prediction, smaller positional intervals are more trained and thus better understood. The difference between models lies in the range of positional intervals they can fully comprehend based on their training context window. As a result, all methods except ChunkLlama perform better with a 16k context window on the Llama3-4k than with the same context window on the Llama3-8k.
Secondly, despite all methods aiming for training-free length extrapolation, they employ fundamentally different approaches. GALI, NTK, Dyn-NTK, and YaRN focus on generalizing positional intervals, creating smaller but more usable intervals from pretrained ones. Among these, GALI generates only the minimal number of new intervals required, maximizing the reliance on pretrained intervals. Other methods predefine a target context window and fix the number and size of new intervals. Dyn-NTK only decides whether to generate new intervals based on input length without minimizing their quantity. SE and ChunkLlama do not create new positional intervals but instead redistribute inputs within existing intervals through input rearrangement. 
This leads to differences in practice: GALI maps inputs to the full range of the positional intervals of the training context window, while NTK, Dyn-NTK, YaRN and SelfExtend map inputs to predefined positional interval ranges based on the target context window (e.g. in SelfExtend, a larger context window leads to a larger group size). ChunkLlama determines interval ranges via hyperparameters such as chunk size and local window. The performance gap when using 16k and 32k context windows on Llama3-8k reflects these differences. Figure \ref{fig:llama3_length_dist} shows that most LongBench dataset lengths are below 16k when using the Llama3 tokenizer. However, NTK, Dyn-NTK, YaRN and SelfExtend still benefit significantly from expanding to 32k, particularly on HotpotQA and Musique\footnote{SelfExtend is highly sensitive to its hyperparameters, as mentioned in its GitHub implementation. Hence, our reproduced results using the Llama3-8k with 32k context window are significantly higher than those reported in their paper.}. This is because most of the data in HotpotQA and Musique is concentrated below 16k. A 32k context window can map them to the positional interval range [0–4096), while a 16k one maps them to the range [0–8192) (For SelfExtend, this range is not fixed and depends on the specific local window and group size. However, it typically falls within a smaller positional interval range.). In contrast, GALI’s results for data shorter than 16k remain nearly unchanged, while ChunkLlama exhibits slight fluctuations. Overall, these methods that do not rely on predefined target context windows show limited improvement since most dataset lengths remain below 16K. However, on NarrativeQA—the longest dataset—GALI shows a substantial improvement from 25.88 to 28.63, outperforming all other methods.

\begin{table}[t]
\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
% \small
% \footnotesize
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{c|cccccccc}
\specialrule{1pt}{0pt}{2pt}
\toprule
 & ~Methods~ & Coursera & GSM & QuALITY& TOFEL & SFiction & CodeU & Average \\

% \midrule

% \multirow{8}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama2-7b-chat-4k}} & Original\color{red}{\textsuperscript{*}}& 29.21 & 19.00 & 37.62 & 51.67 & 60.15 & 1.11 & 33.12 \\
% & SelfExtend-16k\color{blue}{\textsuperscript{*}} & 35.76 & 25.00 & 41.09 & 55.39 & 57.81 & 1.11 & 36.02 \\
% & SelfExtend-16k & 32.99 & 29.00 & 40.59 & 57.62 & 59.38 & 2.22 & 36.97 \\
% & ChunkLlama\color{orange}\textsuperscript{*} & 32.12 & 31.00 & 35.14 & 57.62 & 61.72 & 2.22 & 36.64 \\
% & ChunkLlama-16k & 28.92 & 31.00 & 43.07 & 58.36 & 62.5 & 2.22 & 37.68 \\
% & NTK-16k\color{red}{\textsuperscript{*}} & 32.71 & 19.00 & 33.16 & 52.78 & 64.84 & 0.00 & 33.75 \\
% & NTK-16k & 26.89 & 16.00 & 33.66 & 60.97 & 64.06 & 0.00 & 33.6 \\
% & Dyn-NTK\color{orange}\textsuperscript{*} & 13.95 & 13.00 & 30.69 & 52.27 & 57.02 & 1.11 & 28.01 \\
% & Dyn-NTK-16k & 15.41 & 13.00 & 33.17 & 54.65 & 59.38 & 1.11 & 29.45 \\
% & YaRN-16k & 36.48 & 18.00 & 42.08 & 57.62 & 62.50 & 7.78 & 37.41  \\
% & GALI-16k & 35.32 & 29 & 39.11 & 54.65 & 59.38 & 4.44 & 36.98 \\

\midrule

\multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-4k}} &  Original &53.34 & 75.00 & 59.41 & 81.41 & 60.94 & 4.44 & \cellcolor{YellowGreen!25} 55.76  \\
 &  SelfExtend-16k & 55.23 & 79.00 & 64.36 & 79.18 & \textbf{67.97} & 5.56 & \cellcolor{YellowGreen!25}58.55 \\
 &  ChunkLlama-16k & 52.62 & 77.00 & 63.37 & 81.04 & 60.16 & 3.33 & \cellcolor{YellowGreen!25}56.25 \\
 &  NTK-16k & \textbf{57.70} & 80.00 & 63.86 & 81.04 & 64.06 & 5.56 & \cellcolor{YellowGreen!25}58.70 \\
 &  Dyn-NTK-16k & 54.07 & 75.00 & 64.36 & 82.16 & \textbf{67.97} & 1.11 & \cellcolor{YellowGreen!25}57.44 \\
 &  YaRN-16k & 56.40 & \textbf{81.00} & 59.40 & 79.18 & 64.06 & 5.56 & \cellcolor{YellowGreen!25}57.6 \\
 &  \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-16k} & \cellcolor{WildStrawberry!25}56.54 & \cellcolor{WildStrawberry!25}74.00 & \cellcolor{WildStrawberry!25}\textbf{65.35} & \cellcolor{WildStrawberry!25}\textbf{84.06} & \cellcolor{WildStrawberry!25}66.41 & \cellcolor{WildStrawberry!25}\textbf{8.89} & \cellcolor{YellowGreen!25}\textbf{59.21} \\

\midrule

\multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & 
Original & 53.05 & - & - & - & 60.16 & 4.44 & \cellcolor{YellowGreen!25}39.22 \\
& SelfExtend-16k & 55.38 & - & - & - & 64.06 & 5.56 & \cellcolor{YellowGreen!25}41.67 \\
% & ChunkLlama\color{orange}\textsuperscript{*} & 56.24 & - & - & - & 70.31 & 5.56 & 44.04 \\
& ChunkLlama-16k & 53.34 & - & - & - & 61.72 & 5.56 & \cellcolor{YellowGreen!25}40.21 \\
& NTK-16k & 52.03 & - & - & - & 42.97 & 0.00 & \cellcolor{YellowGreen!25}31.67 \\
& Dyn-NTK-16k & 52.03 & - & - & - & 52.34 & 2.22 & \cellcolor{YellowGreen!25}35.53 \\
& YaRN-16k & \textbf{55.96} & - & - & - & 62.5 & 5.56 & \cellcolor{YellowGreen!25}41.34 \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-16k} & \cellcolor{WildStrawberry!25}54.65 & \cellcolor{WildStrawberry!25}- & \cellcolor{WildStrawberry!25}- & \cellcolor{WildStrawberry!25}- & \cellcolor{WildStrawberry!25}\textbf{65.63} & \cellcolor{WildStrawberry!25}\textbf{6.67} & \cellcolor{YellowGreen!25}\textbf{42.32} \\

\midrule


\multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-4k}} &  Original &53.34 & 75.00 & 59.41 & 81.41 & 60.94 & 4.44 & \cellcolor{YellowGreen!25} 55.76  \\
& SelfExtend-32k & \textbf{54.51} & 80.00 & 64.36 & 77.70 & 67.97 & 5.56 & \cellcolor{YellowGreen!25} 58.35 \\
& ChunkLlama-32k & 53.20 & 75.00 & 63.37 & 81.04 & 63.28 & 2.22 & \cellcolor{YellowGreen!25} 56.35 \\
& NTK-32k & 52.91 & \textbf{82.00} & 61.39 & 79.93 & 67.19 & 2.22 & \cellcolor{YellowGreen!25} 57.6 \\
& Dyn-NTK-32k & 52.33 & 76.00 & 63.86 & 82.16 & \textbf{71.88} & 3.33 & \cellcolor{YellowGreen!25} 58.26 \\
& YaRN-32k & 53.05 & 73.00 & 59.41 & 79.55 & 68.75 & 5.56 & \cellcolor{YellowGreen!25} 56.55 \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-32k} & \cellcolor{WildStrawberry!25}54.17 & \cellcolor{WildStrawberry!25}74.00 & \cellcolor{WildStrawberry!25}\textbf{65.35} & \cellcolor{WildStrawberry!25}\textbf{84.06} & \cellcolor{WildStrawberry!25}68.75 & \cellcolor{WildStrawberry!25}\textbf{7.78} & \cellcolor{YellowGreen!25} \textbf{59.10} \\

\midrule

\multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & 
Original & 53.05 & - & - & - & 60.16 & 4.44 & \cellcolor{YellowGreen!25}39.22 \\
& SelfExtend-32k & 53.92 & - & - & - & 65.63 & 3.33 & \cellcolor{YellowGreen!25}40.96 \\
& ChunkLlama-32k & 54.36 & - & - & - & 64.06 & 5.56 & \cellcolor{YellowGreen!25}41.33 \\
& NTK-32k & \textbf{58.28} & - & - & - & 59.38 & 1.11 & \cellcolor{YellowGreen!25}39.59 \\
& Dyn-NTK-32k & 54.36 & - & - & - & 64.06 & 6.67 & \cellcolor{YellowGreen!25}41.70 \\
& YaRN-32k & 55.23 & - & - & - & \textbf{67.19} & \textbf{7.78} & \cellcolor{YellowGreen!25}\textbf{43.40} \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-32k} & \cellcolor{WildStrawberry!25}54.17 & \cellcolor{WildStrawberry!25}- & \cellcolor{WildStrawberry!25}- & \cellcolor{WildStrawberry!25}- & \cellcolor{WildStrawberry!25}66.41 & \cellcolor{WildStrawberry!25}\textbf{7.78} & \cellcolor{YellowGreen!25}42.79 \\



\bottomrule
\end{tabular}
}
%rewritten by Caren
\caption{Performance comparison across different backbone LLMs and training-free length extrapolation methods. The best result in each experiment is bolded. The GSM, QuALITY, and TOEFL were excluded from this comparison because their sequence lengths remain below 8192 tokens when using the Llama3 tokenizer, making them unsuitable for long-context evaluation. Results for these three datasets, along with those for all datasets using the Llama2-7B-Chat-4K (Llama2-4k) backbone, are provided in Appendix \ref{app:real world task}.}
% \caption{Performance comparison with different backbone LLMs and training-free length extrapolation methods. The best result in each experiment has been bolded.

% Since the GSM, QuALITY, and TOEFL datasets have lengths entirely below 8192 when using the Llama3 tokenizer and are not considered long datasets, we did not include them in the comparison. The results for these three datasets, along with those for all datasets using the Llama2-7B-chat-4k backbone, are presented in Appendix \ref{app:real world task}.}

% blue se, orange chunkllama, green longbench, yellow CLEX, red L-eval 
\label{tab:leval_all}
\end{table}


\begin{figure}[htp]
    \centering
    % 第一张子图
    \subfigure[LongBench]{
        \includegraphics[width=0.45\linewidth, trim=30 0 70 50, clip]{images/longbench_trend.png}
        \label{fig:lbench_trend}
    }
    % trim=70 0 70 50, clip
    % 第二张子图
    \subfigure[L-Eval]{
        \includegraphics[width=0.45\linewidth,trim=30 0 70 50, clip]{images/longeval_trend.png}
        \label{fig:leval_trend}
    }
    \caption{The trend of average scores across different methods and settings. The X-axis tick “4k-to-16k” represents an initial context window of 4k and a target window of 16k. For a more meaningful comparison, the average scores are computed on the three long-text datasets in L-Eval: Coursera, SFiction, and CodeU.}
    \label{fig:real_world_task_trend}
\end{figure}

The L-Eval results further support our analysis. As shown in Table \ref{tab:leval_all}, GALI achieved the highest average performance, except when using a 32k context window on the Llama3-8k. Consistent with LongBench, where 32k on Llama3-8k performed slightly better than 16k but remained inferior to 16k on Llama3-4k, aligning with our previous observations. 
Since Llama3 better understands closer positional intervals, other methods benefit from mapping text to a smaller range of positional intervals, giving GALI a relative disadvantage at 32k on Llama3-8k. However, GALI still achieved the second-best performance. To investigate further, we tested GALI with a 32k context window on Llama3-4k, forcing it to operate exclusively within the [0, 4096) range of positional intervals. Under these conditions, GALI once again achieved the best results. Figure \ref{fig:leval_trend} shows the overall performance trends.
Additionally, it is worth noting that when using the Llama3 tokenizer, GSM, QuALITY, and TOEFL lengths are all below 8192, where SE, YaRN, and NTK outperform the backbone models as shown in Appendix \ref{app:real world task}, further validating our findings.

% All methods performed significantly better on Llama3-8b-ins-4k compared to Llama2-7b-chat-4k. 

% Notably, on Llama2-7b-chat-4k, the performance gap was smaller than on LongBench. This is because, when using the Llama2 tokenizer, datasets like Coursera, GSM, QuALITY, and TOEFL in L-Eval are much shorter than 16k, allowing all methods to use Llama2-4k’s well-understood smaller relative positional intervals. In longer datasets like SFictions and CodeU, performance is task-dependent. SFictions is a True/False task with higher results, while CodeU is a code inference task with much lower results. 


Experiments on LongBench and L-Eval show that training-free length extrapolation methods benefit from mapping text to smaller relative positional intervals, compensating for models’ uneven positional understanding. GALI, however, minimizes new intervals while utilizing the full range of pretrained intervals, which can be disadvantageous in some cases. As models improve, GALI’s direct attention logit estimation enhances context comprehension, delivering superior stability without requiring hyperparameter tuning.

% blue se, orange chunkllama, green longbench, yellow CLEX, red L-eval





\begin{table}[thp]
\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
% \small
% \footnotesize
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{c|cccccccccccc}
\toprule
& \textbf{Methods} & \textbf{1k} & \textbf{4k} & \textbf{8k} & \textbf{12k} & \textbf{16k} & \textbf{20k} & \textbf{24k} & \textbf{28k} & \textbf{32k} \\ \midrule
% \multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & Original & - & - & - & - & - & - & - & - & - \\ 
\multirow{6}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & SelfExtend & 11.52 & 11.54 & 11.32 & 11.18 & 11.07 & 10.97 & 11.01 & 11.04 & 10.91 \\ 
& ChunkLlama & 11.72 & 11.77 & 11.54 & 11.39 & 11.27 & - & - & - & - \\ 
& NTK  & 11.93 & 11.94 & 11.67 & 11.50 & 11.39 & 13.03 & 23.00 & 42.95 & 77.41 \\ 
& Dyn-NTK  & 11.51 & 11.53 & 12.75 & 66.88 & 166.86 & 269.93 & 334.83 & 360.57 & 365.36 \\ 
& YaRN  & 11.93 & 11.81 & 11.48 & 11.30 & 11.18 & 11.06 & 11.10 & 11.13 & 11.18 \\ 
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI} & \cellcolor{WildStrawberry!25}11.52 & \cellcolor{WildStrawberry!25}11.54 & \cellcolor{WildStrawberry!25}11.35 & \cellcolor{WildStrawberry!25}11.25 & \cellcolor{WildStrawberry!25}11.17 & \cellcolor{WildStrawberry!25}11.09 & \cellcolor{WildStrawberry!25}11.14 & \cellcolor{WildStrawberry!25}11.18 & \cellcolor{WildStrawberry!25}11.05 \\ 
\bottomrule
\end{tabular}
}
% rewritten by Caren
\caption{Performance of Diverse Models on the PG19 Dataset Across Varying Context Window Sizes.}
\label{tab:pg19_models}
\end{table}

\subsection{Long Language Modeling Task Results}
The language modeling results are shown in Table \ref{tab:pg19_models}. Due to OOM, we cannot get ChunkLlama's PPL results when setting the maximum position embedding to 32768. Except for NTK and DYN-NTK, all methods maintained a stable PPL without exploding. While low PPL does not guarantee better real-world task performance, an exploding PPL is a clear indicator of performance degradation in downstream tasks. Notably, GALI achieved the second-lowest PPL, demonstrating superior stability in length extrapolation. We tested PPL using a 16k contest window with Llama2-4k backbone. Please refer to the Appendix \ref{app:ppl_test}.

% \noindent % 移除缩进
\begin{figure*}[htp]
    \centering
    % 第一张子图
    \subfigure[Attention Score Matrix Differences]{
        \includegraphics[width=0.31\linewidth]{images/attn_differences_grouped.png}
        \label{fig:attn_differences_grouped}
    }
    % 第二张子图
    \subfigure[Row Entropy Differences (2K)]{
        \includegraphics[width=0.31\linewidth]{images/attn_row_ent_2k_500.png}
        \label{fig:attn_row_ent_2k}
    }
    % 第三张子图
    \subfigure[Row Entropy Differences (4K)]{
        \includegraphics[width=0.31\linewidth]{images/attn_row_ent_4k_500.png}
        \label{fig:attn_row_ent_4k}
    }
    %rewritten by Caren
    \caption{Differences in attention score metrics and row-wise entropy across methods compared to the original LLM. (a) represents the attention score matrix differences. The values show the sum of the absolute differences between the attention scores of the length interpolation methods and the model trained on its training context window. (b) and (c) show row-wise entropy differences. 2K indicates methods using [0, 2048) positional intervals, while 4k shows the methods using positional intervals within [0, 4096). The figures are generated using a sample from NarrativeQA with a prefill length of  8091 tokens. More details on the attention score distributions can be found in Appendix \ref{app:atten_score_matrix}.}
    \label{fig:combined_attn_differences}
\end{figure*}

\subsection{Attention Distribution Analysis}
As discussed in Section 4.2, the effectiveness of training-free length extrapolation methods is influenced not only by the algorithm itself but also by the model’s understanding of positional intervals within its training context window. We propose a new comparison approach to fairly compare different length extrapolation methods while eliminating the positional interval understanding bias inherent to the model. Instead of applying extrapolation methods across a full context window, we first restrict them to a smaller positional interval range. We then extend this range to match the model’s training context window and compare the resulting attention distribution against the model’s original attention distribution. In this scenario, the closer the extrapolated attention distribution is to the model’s original attention distribution, the more effectively the extrapolation method performs within a smaller positional interval range, indicating that it aligns more closely with the model’s inherent understanding. Consequently, as the model’s understanding improves, the performance of the length extrapolation method will also improve.
More concretely, we applied these training-free length extrapolation methods to Llama3-8b-ins-2k (Llama3-2k) and Llama3-4k backbones, and compared the resulting attention score distribution with Llama3-8k. The results, shown in Figure \ref{fig:attn_differences_grouped}, indicate that GALI consistently demonstrates a significantly smaller gap compared to other methods, whether extrapolating from 2k positional intervals or 4k positional intervals to 8k. Remarkably, when using 2k positional intervals, GALI outperforms Dyn-NTK, NTK, and YaRN even when they use 4k positional intervals.
Beyond global attention comparison, we also examined the row-wise differences in attention score entropy differences between Llama3-2k and Llama3-8k, as attention entropy has been identified as a key factor affecting model performance\cite{zhang2024attention, farquhar2024detecting}. As shown in Figures \ref{fig:attn_row_ent_2k} and \ref{fig:attn_row_ent_4k}, GALI consistently exhibits the smallest row-wise entropy difference from Llama3-8k among all methods, indicating fewer attention outliers compared to other methods.
In summary, GALI preserves attention score distributions at both global and local levels, achieving optimal extrapolation performance. As backbone models continue to improve their understanding of positional intervals, GALI is expected to further enhance performance in downstream tasks.


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/local_window_group_size.png}
    %rewritten by Caren
    \vspace{-0.5cm}
    \caption{Impact of local window and chunk size on performance. The experiments use the Llama3-8b-ins-8k, with an extrapolated context window of 16384 tokens.}
    \label{fig:local_window_group_size}
\end{figure}

\subsection{Ablation Studies}
In this section, we investigate the impact of local window size and chunk size on GALI. We conducted our experiments using NarrativeQA, the longest dataset in LongBench. The results are shown in Figure \ref{fig:local_window_group_size}. First, we observe that the differences across the three local window sizes are minimal, indicating that attention logit interpolation effectively approximates the true attention score distribution.

Secondly, as the chunk size increases, we hypothesize that the observed effects result from the interplay of two factors. Initially, a smaller chunk size aligns better with GALI’s design, which prioritizes leveraging pretrained positional intervals as much as possible while minimizing the number of interpolations for each token. Consequently, when the chunk size increases, the number of pretrained positional intervals utilized by each token decreases, while the number of interpolated positional intervals increases, leading to performance degradation.

However, as the chunk size grows, more tokens have their positional intervals compressed into a smaller range. As analyzed earlier, performing denser interpolations within a smaller positional interval range, such as [0, 4096), yields better results than performing sparser interpolations over a larger positional interval range, such as [0, 8192). Therefore, the performance of GALI begins to improve as the chunk size further increases.

\section{Conclusion}
% We introduce GALI, a novel training-free length extrapolation method, and evaluate it against other approaches across real-world long-context tasks, long-context language modeling, and attention distribution analysis, demonstrating its superiority. Our experiments further suggest that applying length extrapolation within a smaller positional range may outperform the vanilla LLM—even for short-context tasks. GALI avoids direct position embedding computations, making it compatible with any LLM exhibiting long-term decay, such as ALiBi. Future work will explore GALI’s application to different position embedding types. However, as GALI requires two passes of attention logit computation for interpolation, it introduces computational overhead and is currently incompatible with flash attention. We also leave this issue for future work to address.

In this paper, we introduced Greedy Attention Logit Interpolation (GALI), a novel training-free length extrapolation method, and evaluated it against other approaches across widely used real-world long-context tasks, long-context language modeling, and attention distribution analysis, demonstrating its superiority. The experiment results further reveal that LLMs interpret positional intervals differently within their training context window, suggesting that length extrapolation within a smaller positional range may outperform the vanilla LLM—even for short-context tasks.

Originally designed for RoPE-based LLMs, GALI avoids direct position embedding computations, making it compatible with any LLM exhibiting long-term decay, such as ALiBi. Future work will explore its applicability to other position embedding mechanisms. However, GALI currently requires two passes of attention logit computation, introducing computational overhead and making it incompatible with flash attention. To address this, we aim to integrate interpolation estimation into flash attention and develop more efficient local interpolation techniques.


\section{Impact Statement}
GALI is a training-free length extrapolation method that enables LLMs to handle long-context tasks without requiring fine-tuning. It is compatible with various position embedding schemes, making it a flexible plug-and-play solution. Furthermore, we show that, in practical scenarios, interpolating within a smaller context window than the training context window can enhance performance across both long-context and short-context downstream tasks.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Pseudo code of GALI}
\label{app: gali pseudo code}
% \begin{minipage}{0.5\linewidth}
In this section, we provide the pseudo-code for the key steps required to implement GALI. Algorithm \ref{alg:get_chunk_size_list} generates the chunk sizes needed to partition the input during the prefill phase. While this function can be modified to support dynamic chunk sizes, we use fixed chunk sizes in our experiments to better control memory usage. Algorithm \ref{alg:construct_new_pi} interpolates new position IDs based on the minimum number of new IDs required for each chunk. Algorithm \ref{Interpolated attention logits} demonstrates how we perform attention logit interpolation. Note that we use $r = \lceil{m}\rceil - n$ to represent the interval between $q_m$ and $k_n$. This is because, when computing attention logits using RoPE, we cannot directly manipulate the relative positional interval matrix; instead, we modify the relative positional interval matrix by separately operating on $query\_states$ and $key\_states$. By using $r = \lceil{m}\rceil - n$, we ensure that $\lfloor{r}\rfloor = \lceil{m}\rceil - \lceil{n}\rceil$ and $\lceil{r}\rceil = \lceil{m}\rceil - \lfloor{n}\rfloor$, enabling modifications to the relative positional interval matrix while preserving the relative order between $query\_states$ and $key\_states$. It is important to note that some operations, such as reshaping, which do not affect the core concept, are omitted from the pseudo-code in these three algorithms.


\begin{algorithm}[tph]
\caption{Generate Chunk Size List}
\label{alg:get_chunk_size_list}
\begin{algorithmic}[1]
\REQUIRE $prefill\_len$: The length of input in the prefill phase, $L_{tr}$: Training context window, $s$: Chunk size
\ENSURE A list of chunk sizes that sums to $prefill\_len$
\STATE $chunk\_size\_list \gets [L_{tr}]$
\STATE $sum\_len \gets L_{tr}$

\WHILE{$sum\_len < prefill\_len$}
    \STATE Append $s$ to $chunk\_size\_list$
    \STATE $sum\_len \gets sum\_len + s$
\ENDWHILE

\STATE Adjust the last chunk size:
\STATE $chunk\_size\_list[-1] \gets chunk\_size\_list[-1] - (sum\_len - prefill\_len)$

\STATE \textbf{return} $chunk\_size\_list$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tph]
\caption{Position ID Interpolation}
\label{alg:construct_new_pi}
\begin{algorithmic}[1]
\REQUIRE $cur\_len$: Current length of the sequence, $L_{tr}$: Training context window, $add\_token$: The number of positions to be interpolated, $L_{w}$: Neighbor window size
\ENSURE $new\_pi$: New position IDs

\STATE $target\_len \gets cur\_len + add\_token$
\STATE $min\_group\_size \gets \lceil (target\_len - L_{w}) / (L_{tr} - L_{w}) \rceil$
\STATE $interval \gets 1 / min\_group\_size$
\STATE $total\_len \gets L_{tr}$
\STATE Initialize $new\_pi \gets []$ and $i \gets 0$

\WHILE{$total\_len < target\_len$}
    \STATE Append $[i + interval \cdot j \mid j \in \{0, 1, \dots, min\_group\_size - 1\}]$ to $new\_pi$ 
    \STATE $i \gets i + 1$ 
    \STATE $total\_len \gets L_{tr} - i + \text{len}(new\_pi)$
\ENDWHILE

\STATE $seg\_window \gets [j \mid j \in \{i, i+1, \dots, L_{tr} - 1\}]$
\STATE $new\_pi \gets new\_pi[:(target\_len - \text{len}(seg\_window))] + seg\_window$
\STATE \textbf{return} $new\_pi$
\end{algorithmic}
\end{algorithm}


\clearpage
\begin{algorithm}[thp]
\caption{Attention Logit Interpolation}
\label{alg:attention_logits_interpolation}
\begin{algorithmic}[1]
\REQUIRE $position\_ids$: Interpolated position IDs, $hidden\_states$: The inputs of the attention layer, $head\_dim$: The dimension of each head, $\text{q\_proj}$: Q project function, $\text{k\_proj}$: K project function, $\text{rotary\_emb}$: Rotary embedding function
\ENSURE Interpolated attention logits

\text{\# Compute the rotary embedding}
\STATE $cos\_ceil, sin\_ceil \gets \text{rotary\_emb}(hidden\_states, \lceil position\_ids \rceil)$
\STATE $cos\_floor, sin\_floor \gets \text{rotary\_emb}(hidden\_states, \lfloor position\_ids \rfloor)$

\text{\# Apply the rotary embedding on the query and key states}
\STATE $query\_states \gets \text{q\_proj}(hidden\_states)$
\STATE $key\_states \gets \text{k\_proj}(hidden\_states)$

\STATE $query\_states\_ceil \gets (query\_states \cdot cos\_ceil) + (\text{rotate\_half}(query\_states) \cdot sin\_ceil)$
\STATE $key\_states\_ceil \gets (key\_states \cdot cos\_ceil) + (\text{rotate\_half}(key\_states) \cdot sin\_ceil)$

\STATE $key\_states\_floor \gets (key\_states \cdot cos\_floor) + (\text{rotate\_half}(key\_states) \cdot sin\_floor)$

\text{\# Compute attention logits with $\lceil{R}\rceil and \lfloor{R}\rfloor$}
\STATE $attn\_floor \gets query\_states\_ceil \text{@} key\_states\_ceil^T / \sqrt{head\_dim}$
\STATE $attn\_ceil \gets query\_states\_ceil \text{@} key\_states\_floor^T / \sqrt{head\_dim}$

\STATE $rel\_coef \gets (\lceil position\_ids \rceil.unsqueeze(1) - position\_ids.unsqueeze(0)) \mod 1$

\STATE $attn\_logits \gets attn\_floor - (attn\_floor - attn\_ceil) \cdot rel\_coef$

\text{\# Add normal distribution noise}
\STATE $distance\_ids \gets [i \mid i \in \{0, 1, \dots, \text{len}(hidden\_states) - 1\}]$
\STATE $distance\_matrix \gets distance\_ids.unsqueeze(1) - distance\_ids.unsqueeze(0)$
\STATE $noise\_std \gets distance\_matrix / \text{len}(hidden\_states) $

\STATE $noise \gets \text{torch.randn\_like}(attn\_logits)$
\STATE $mask \gets (rel\_coef \neq 0)$

\STATE $noise \gets noise \cdot noise\_std \cdot mask$

\STATE $attn\_logits \gets attn\_logits + noise$

\STATE \textbf{return} $attn\_logits$
\end{algorithmic}
\end{algorithm}

\section{Data stastics}
\label{app: data stastics}

In this section, we provide detailed information about each dataset used in LongBench and L-Eval. Table \ref{tab:benchmark_task_type} presents the word length, task type, and number of samples for each dataset. Figure \ref{fig:llama2_length_dist} and \ref{fig:llama3_length_dist} show the length distributions of each dataset using the Llama2 and Llama3 tokenizers, respectively.

\begin{longtable}{|l|l|l|r|r|}
    \toprule
    \textbf{Benchmark} & \textbf{Dataset} & \textbf{Task Type} & \textbf{Avg Len} & \textbf{\#Sample} \\
    \hline
    \endfirsthead
    \hline
    \textbf{Benchmark} & \textbf{Dataset} & \textbf{Task Type} & \textbf{Avg Len} & \textbf{\#Sample} \\
    \hline
    \endhead
    \hline
    \endfoot
    \caption{We list the task type, average word lengths, and the number of samples for each dataset we used in our work.} \label{tab:benchmark_task_type}
    \endlastfoot
    \multirow{16}{*}{LongBench} & NarrativeQA & Single-doc QA & 18409 & 200 \\
    & Qasper & Single-doc QA & 3619 & 200 \\
    & MultiField-en & Single-doc QA & 4559 & 150 \\
    & HotpotQA & Multi-doc QA & 9151 & 200 \\
    & 2WikiMQA & Multi-doc QA & 4887 & 200 \\
    & Musique & Multi-doc QA & 11214 & 200 \\
    & GovReport & Summarization & 8734 & 200 \\
    & QMSum & Summarization & 10614 & 200 \\
    & MultiNews & Summarization & 2113 & 200 \\
    & TREC & Few shot & 5177 & 200 \\
    & TriviaQA & Few shot & 8209 & 200 \\
    & SAMSum & Few shot & 6258 & 200 \\
    & PassageCount & Synthetic & 11141 & 200 \\
    & PassageRe & Synthetic & 9289 & 200 \\
    & LCC & Code & 1235 & 500 \\
    & RepoBench-P & Code & 4206 & 500 \\
    \midrule
    \multirow{1}{*}{L-Eval} & Coursera & Multiple choice & 9075 & 172 \\
    \multirow{5}{*}{L-Eval} & GSM (16-shot) & Solving math problems & 5557 & 100 \\
    & QuALITY & Multiple choice & 7169 & 202 \\
    & TOEFL & Multiple choice & 3907 & 269 \\
    & SFCition & True or False Questions & 16381 & 64 \\
    & CodeU & Deducing program outputs & 31575 & 90 \\
    \bottomrule
\end{longtable}


\begin{figure}[htp]
    \centering
    % 第一张子图
    \subfigure[Llama2 Tokenizer]{
        \includegraphics[width=0.9\linewidth]{images/Llama2_length_distribution.png}
        \label{fig:llama2_length_dist}
    }
    % 第二张子图
    \subfigure[Llama3 Tokenizer]{
        \includegraphics[width=0.9\linewidth]{images/Llama3_length_distribution.png}
        \label{fig:llama3_length_dist}
    }
    \caption{Length distributions using Llama2 and Llama3 tokenizers. The left figure shows the distribution with Llama2, and the right figure shows the distribution with Llama3. The red line represents the median, the orange dashed line represents the mean, and the darker the color of the box, the greater the average length.}
    \label{fig:llama_length_dist}
\end{figure}


\section{Implementation details}
\label{app: imp. details}
In this section, we provide detailed implementation information for each method. For Dyn-NTK and YaRN, we utilize the implementations available in Huggingface\footnote{https://huggingface.co} by adding \textit{rope\_scaling = \{"rope\_type":"dynamic"\}} and \textit{rope\_scaling = \{"rope\_type":"yarn"\}}, respectively, to the LLM’s config.json file. For NTK, we implement it by adding \textit{rope\_scaling = \{"rope\_type":"dynamic"\}} and \textit{static\_ntk=True}, and modifying the \textit{\_dynamic\_frequency\_update} function of the LlamaRotaryEmbedding class as follows:

\lstset{
  language=Python,           % 设置代码语言
  basicstyle=\ttfamily\small, % 字体样式
  % numbers=left,              % 显示行号
  numberstyle=\tiny,         % 行号样式
  keywordstyle=\color{blue}, % 关键字高亮
  commentstyle=\color{gray}, % 注释样式
  stringstyle=\color{red},   % 字符串样式
  frame=single,              % 添加边框
  breaklines=true,           % 自动换行
}
\begin{lstlisting}
def _dynamic_frequency_update(self, position_ids, device):
    """
    Modify this function to make it suitable for NTK
    """
    if self.config.static_ntk == True:
        if getattr(self, "reset_static_ntk", False) == False:
            config = copy.deepcopy(self.config)
            seq_len = self.original_max_seq_len * config.rope_scaling['factor']
            config.rope_scaling['factor'] = 1
            inv_freq, self.attention_scaling = self.rope_init_fn(
                config, device, seq_len=seq_len, **self.rope_kwargs
            )
            self.register_buffer("inv_freq", inv_freq, persistent=False) 
            setattr(self, "reset_static_ntk", True)
        return
    seq_len = torch.max(position_ids) + 1
    if seq_len > self.max_seq_len_cached:  # growth
        inv_freq, self.attention_scaling = self.rope_init_fn(
            self.config, device, seq_len=seq_len, **self.rope_kwargs
        )
        self.register_buffer("inv_freq", inv_freq, persistent=False)  # TODO joao: may break with compilation
        self.max_seq_len_cached = seq_len

    if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset
        self.register_buffer("inv_freq", self.original_inv_freq, persistent=False)
        self.max_seq_len_cached = self.original_max_seq_len

\end{lstlisting}

For SelfExtend and ChunkLlama, we use their official implementations\footnote{SelfExtend: https://github.com/datamllab/LongLM, ChunkLlama: https://github.com/HKUNLP/ChunkLlama}. We list the hyperparameters required for these methods to extend to different maximum input length in Table \ref{tab:length_extrapolation_hyperparams}. All experiments can be conducted on a single A100 GPU (80GB) machine.

\begin{table}[thp]
    \centering
    \begin{tabular}{|l|l|l|}
        \toprule
        \textbf{Length Extrapolation} & \textbf{Method} & \textbf{Hyperparameters} \\
        \midrule
        \multirow{6}{*}{2k to 8k} & NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 4\}, static\_ntk=True \\
         & Dyn-NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 4\} \\
         & YaRN & rope\_scaling=\{”rope type”:"YaRN", "factor": 4\} \\
         & SelfExtend & group\_size=5, window\_size=512 \\
         & ChunkLlama & chunk\_size=1536, local\_window=128 \\
         & GALI & chunk\_size=[1000,2000,3000], local\_window=[128, 256, 512, 1024] \\
        \midrule
        \multirow{6}{*}{4k to 8k} & NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 2\}, static\_ntk=True \\
         & Dyn-NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 2\} \\
         & YaRN & rope\_scaling=\{”rope type”:"YaRN", "factor": 2\} \\
         & SelfExtend & group\_size=3, window\_size=2048 \\
         & ChunkLlama & chunk\_size=3072, local\_window=256 \\
         & GALI & chunk\_size=[1000,2000,3000], local\_window=[128, 256, 512, 1024] \\
        \midrule
        \multirow{6}{*}{4k to 16k} & NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 4\}, static\_ntk=True \\
         & Dyn-NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 4\} \\
         & YaRN & rope\_scaling=\{”rope type”:"YaRN", "factor": 4\} \\
         & SelfExtend & group\_size=5, window\_size=1024 \\
         & ChunkLlama & chunk\_size=3072, local\_window=256 \\
          & GALI & chunk\_size=[1000,2000,3000], local\_window=[128, 256, 512, 1024] \\
        \midrule
        \multirow{6}{*}{4k to 32k} & NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 8\}, static\_ntk=True \\
         & Dyn-NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 8\} \\
         & YaRN & rope\_scaling=\{”rope type”:"YaRN", "factor": 8\} \\
         & SelfExtend & group\_size=15, window\_size=2048 \\
         & ChunkLlama & chunk\_size=3072, local\_window=256 \\
        & GALI & chunk\_size=[1000,2000,3000], local\_window=[128, 256, 512, 1024] \\
        \midrule
        \multirow{6}{*}{8k to 16k} & NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 2\}, static\_ntk=True \\
         & Dyn-NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 2\} \\
         & YaRN & rope\_scaling=\{”rope type”:"YaRN", "factor": 2\} \\
         & SelfExtend & group\_size=3, window\_size=4096 \\
         & ChunkLlama & chunk\_size=6144, local\_window=512 \\
        & GALI & chunk\_size=[1000,2000,3000], local\_window=[128, 256, 512, 1024] \\
        \midrule
        \multirow{6}{*}{8k to 32k} & NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 4\}, static\_ntk=True \\
         & Dyn-NTK & rope\_scaling=\{”rope type”:”dynamic”, "factor": 4\} \\
         & YaRN & rope\_scaling=\{”rope type”:"YaRN", "factor": 4\} \\
         & SelfExtend & group\_size=5, window\_size=2048 \\
         & ChunkLlama & chunk\_size=6144, local\_window=512 \\
        & GALI & chunk\_size=[1000,2000,3000], local\_window=[128, 256, 512, 1024] \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for length extrapolation methods under each setting. For example, “2k to 8k” indicates an initial context window of 2048, with a positional interval range of [0, 2048), and a target context window extending up to 8192. Other settings follow the same pattern. For GALI, the reported hyperparameters represent the combinations we search for each experiment.}
    \label{tab:length_extrapolation_hyperparams}
\end{table}


\section{Extra experiment results}
\label{app: extra experiment results}

\subsection{Real-world long-context task results}
\label{app:real world task}
We conducted experiments on LongBench and L-Eval using the Llama2-4k backbone, as shown in Tables \ref{tab:longbench_llama2} and \ref{tab:leval_all_llama2}. On LongBench, GALI performed similarly to NTK, Dyn-NTK, and YaRN, but was weaker than SelfExtend and ChunkLlama. However, all methods performed significantly worse than those using the Llama3-8b-ins-4k backbone.

Although Llama2-7b-chat and Llama3-8b-ins have similar parameter scales, Llama3 demonstrates a deeper understanding of pretrained positional intervals closer to its training context window. Consequently, GALI performed significantly better on Llama3-8b-ins-4k than on Llama2-4k, with similar trends observed across other methods. As the quality of the pretrained model improves, it better aligns with GALI’s principle of maximizing the use of pretrained positional intervals.

Regarding the best-performing method on Llama2-4k, SelfExtend has been reported to be highly sensitive to hyperparameters \cite{Jin2024LLMML}. Specifically, larger group sizes and smaller local windows sometimes yield better results, which supports our conclusion in Section \ref{main:Real-world long-context task results}. These configurations emphasize the use of smaller positional intervals, reducing reliance on larger ones and preventing content from being placed in less well-understood positional intervals. This limitation affects GALI’s effectiveness on Llama2, as GALI assumes the model fully understands its entire training context window, thereby always maximizing the use of pretrained positional intervals.

On the L-Eval benchmark, the performance gap between GALI and the best approaches was smaller than on LongBench. This is because, when using the Llama2 tokenizer, datasets such as Coursera, GSM, QuALITY, and TOEFL in L-Eval are much shorter than 16k, allowing all methods to leverage Llama2-4k’s well-understood smaller relative positional intervals. In longer datasets like SFictions and CodeU, performance is task-dependent. SFictions is a True/False task with higher results, while CodeU is a code inference task with much lower results. We also report complete results using the Llama3-8k backbone. Our method performed almost identically to the backbone model, as the token lengths of GSM, QuALITY, and TOEFL are all below 8192. However, SelfExtend, NTK, and YaRN outperformed the backbone model, further validating our conclusion that even on short text datasets, using a smaller range of positional intervals for length extrapolation leads to better task performance.



\begin{table*}[thp]
\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
% \small
% \footnotesize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccccccccccccc}
\specialrule{1pt}{0pt}{2pt}
\toprule
 &\multirow{4}{*}{~~~~~~~~~~~~~\textbf{Methods}~~~~~~~~~~~~~} & \multicolumn{3}{c}{\textbf{Single document QA}} & \multicolumn{3}{c}{\textbf{Multi document QA}} & \multicolumn{3}{c}{\textbf{Summarization}} & \multicolumn{3}{c}{\textbf{Few-shot Learning}} & \multicolumn{2}{c}{\textbf{Synthetic}} & \multicolumn{2}{c}{\textbf{Code}} & \multirow{4}{*}{\textbf{Average}} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} \cmidrule(r){12-14} \cmidrule(r){15-16} \cmidrule(r){17-18}
& & \rotatebox{30}{NarrativeQA} & \rotatebox{30}{Qasper} & \rotatebox{30}{MultiField-en} & \rotatebox{30}{HotpotQA} & \rotatebox{30}{2WikiMQA} & \rotatebox{30}{Musique} & \rotatebox{30}{GovReport} & \rotatebox{30}{QMSum} & \rotatebox{30}{MultiNews} & \rotatebox{30}{TREC} & \rotatebox{30}{TriviaQA} & \rotatebox{30}{SAMSum} & \rotatebox{30}{PassageCount} & \rotatebox{30}{PassageRe} & \rotatebox{30}{Lcc} & \rotatebox{30}{RepoBench-P} & \\

\midrule

\multirow{8}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama2-7b-chat-4k}}  &  Original\color{green}\textsuperscript{*} & 18.70 & 19.20 & 36.80 & 25.40 & 32.80 & 9.40 & 27.30 & 20.80 & 25.80 & 61.50 & 77.80 & 40.70 & 2.10 & 9.80 & 52.40 & 43.80 & \cellcolor{YellowGreen!25}31.52 \\
 &  Original  &  8.48  &  13.97  &  20.4  &  13.62  &  16.77  &  5.46  &  25.17  &  12.47  &  24.78  &  67.5  &  74.24  &  40.28  &  2.30  &  3.25  &  56.39  &  50.36  &  \cellcolor{YellowGreen!25}27.22 \\
 &  SelfExtend-16k\color{blue}\textsuperscript{*} &  21.69  & 25.02 & 35.21 & 34.34 & 30.24 & 14.13 & 27.32 & 21.35 & 25.78 & 69.50 & 81.99 & 40.96 & 5.66 & 5.83 & 60.60 & 54.33 & \cellcolor{YellowGreen!25}\textbf{34.62} \\
 &  SelfExtend-16k & 6.89 & 12.67 & 25.95 & 9.08 & 11.25 & 5.88 & 26.80 & 16.39 & 22.79 & 67.50 & 69.88 & 41.18 & 2.18 & 3.21 & 58.21 & 51.65 & \cellcolor{YellowGreen!25}26.97 \\
 &  ChunkLlama-16k & 8.48 & 13.97 & 20.40 & 13.62 & 16.77 & 5.46 & 25.17 & 12.47 & 24.78 & 67.50 & 74.24 & 40.28 & 2.30 & 3.25 & 56.39 & 50.36 & \cellcolor{YellowGreen!25}27.22 \\
 &  NTK-16k & 0.73 & 10.33 & 19.44 & 2.38 & 7.91 & 0.42 & 19.47 & 6.26 & 26.13 & 59.50 & 17.89 & 23.17 & 0.52 & 0.51 & 50.70 & 27.91 & \cellcolor{YellowGreen!25}17.08 \\
 &  Dyn-NTK-16k  & 3.79 & 10.37 & 22.38 & 7.47 & 10.26 & 3.81 & 29.52 & 20.13 & 22.84 & 63.50 & 45.35 & 31.79 & 2.29 & 4.33 & 57.13 & 42.16 & \cellcolor{YellowGreen!25}23.57 \\
 &  YaRN-16k & 3.22 & 10.86 & 22.14 & 5.52 & 13.36 & 1.32 & 24.78 & 10.90 & 25.92 & 64.50 & 40.60 & 32.36 & 2.20 & 2.15 & 51.74 & 43.91 & \cellcolor{YellowGreen!25}22.22 \\
 & \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-16k} & \cellcolor{WildStrawberry!25}6.29 & \cellcolor{WildStrawberry!25}16.73 & \cellcolor{WildStrawberry!25}22.26 & \cellcolor{WildStrawberry!25}12.82 & \cellcolor{WildStrawberry!25}13.65 & \cellcolor{WildStrawberry!25}6.31 & \cellcolor{WildStrawberry!25}23.58 & \cellcolor{WildStrawberry!25}15.96 & \cellcolor{WildStrawberry!25}23.37 & \cellcolor{WildStrawberry!25}62.00 & \cellcolor{WildStrawberry!25}72.80 & \cellcolor{WildStrawberry!25}25.12 & \cellcolor{WildStrawberry!25}1.83 & \cellcolor{WildStrawberry!25}2.83 & \cellcolor{WildStrawberry!25}58.71 & \cellcolor{WildStrawberry!25}48.51 & \cellcolor{YellowGreen!25}25.80 \\


%  &  GPT-3.5-Turbo-16k\color{green}\textsuperscript{*} &  23.60 & 43.30 & 52.30 & 51.60 & 37.70 & 26.90 & 29.50 & 23.40 & 26.70 & 68.00 & 91.40 & 41.70 & 4.50 & 71.00 & 54.70 & 53.60 & 43.74 \\
%  &  CLEX-7B-16k\color{green}\textsuperscript{*} &  18.05 & 23.68 & 44.62 & 28.44 & 19.53 & 9.15 & 32.52 & 22.90 & 25.55 & 68.00 & 84.92 & 42.82 & 0.00 & 11.50 & 59.01 & 56.87 & 34.22 \\


\bottomrule
\end{tabular}
}
\caption{Performance comparison with different backbone LLMs and training-free length extrapolation methods. The best result in each experiment has been bolded. 
{\color{green}\textsuperscript{*}} indicates the results reported by LongBench\cite{bai-etal-2024-longbench}, 
{\color{blue}\textsuperscript{*}} indicates the results reported by LongBench\cite{Jin2024LLMML}. 
% and {\color{yellow}\textsuperscript{*}} indicates the results reported by CLEX\cite{chen2023clex}. 
The number following each method represents the target context window. For example, 16k means 16 × 1024. The "Original" means testing with the backbone model, i.e., the model shown in the left column. 
% blue se, orange chunkllama, green longbench, yellow CLEX, red L-eval 
}
\label{tab:longbench_llama2}
\end{table*}

\begin{table}[thp]
\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
% \small
% \footnotesize
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{c|cccccccc}
\specialrule{1pt}{0pt}{2pt}
\toprule
 & ~Methods~ & Coursera & GSM & QuALITY& TOFEL & SFiction & CodeU & Average \\
% \cellcolor{YellowGreen!25}
% \rowcolor{WildStrawberry!25}
\midrule

\multirow{8}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama2-7b-chat-4k}} & 
Original\color{red}{\textsuperscript{*}} & 29.21 & 19.00 & 37.62 & 51.67 & 60.15 & 1.11 & \cellcolor{YellowGreen!25}33.12 \\
& Original & 29.80 & 29.00 & 37.62 & 58.36 & 60.16 & 1.11 & \cellcolor{YellowGreen!25}36.01\\
& SelfExtend-16k\color{blue}{\textsuperscript{*}} & \textbf{35.76} & 25.00 & 41.09 & 55.39 & 57.81 & 1.11 & \cellcolor{YellowGreen!25}36.02 \\
& SelfExtend-16k & 32.99 & 29.00 & 40.59 & 57.62 & 57.81 & 2.22 & \cellcolor{YellowGreen!25}36.71 \\
& ChunkLlama\color{orange}\textsuperscript{*} & 32.12 & \textbf{31.00} & 35.14 & 57.62 & 61.72 & 2.22 & \cellcolor{YellowGreen!25}36.64 \\
& ChunkLlama-16k & 28.92 & \textbf{31.00} & \textbf{43.07} & 58.36 & 60.94 & 2.22 & \cellcolor{YellowGreen!25}\textbf{37.42} \\
& NTK-16k\color{red}{\textsuperscript{*}} & 32.71 & 19.00 & 33.16 & 52.78 & \textbf{64.84} & 0.00 & \cellcolor{YellowGreen!25}33.75 \\
& NTK-16k & 26.89 & 16.00 & 33.66 & \textbf{60.97} & 41.41 & 0.00 & \cellcolor{YellowGreen!25}29.82 \\
& Dyn-NTK\color{orange}\textsuperscript{*} & 13.95 & 13.00 & 30.69 & 52.27 & 57.02 & 1.11 & \cellcolor{YellowGreen!25}28.01 \\
& Dyn-NTK-16k & 15.41 & 13.00 & 33.17 & 54.65 & 54.69 & 1.11 & \cellcolor{YellowGreen!25}28.67 \\
& YaRN-16k & 36.49 & 18.00 & 42.08 & 57.62 & 42.97 & \textbf{7.78} & \cellcolor{YellowGreen!25}34.15  \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-16k} & \cellcolor{WildStrawberry!25}35.32 & \cellcolor{WildStrawberry!25}29.00 & \cellcolor{WildStrawberry!25}39.11 & \cellcolor{WildStrawberry!25}54.65 & \cellcolor{WildStrawberry!25}51.43 & \cellcolor{WildStrawberry!25}4.44 & \cellcolor{YellowGreen!25}35.66 \\

\midrule

\multirow{8}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & 
Original & 53.05 & 58.00 & 61.88 & 82.16 & 60.16 & 4.44 & \cellcolor{YellowGreen!25}53.93 \\
& SelfExtend-16k & 55.38 & 63.00 & 62.87 & 82.16 & 64.06 & 5.56 & \cellcolor{YellowGreen!25}55.76 \\
& ChunkLlama\color{orange}\textsuperscript{*} & \textbf{56.24} & 54.00 & \textbf{63.86} & 83.27 & \textbf{70.31} & 5.56 & \cellcolor{YellowGreen!25}55.54 \\
& ChunkLlama-16k & 53.34 & 54.00 & 60.89 & 81.78 & 61.72 & 5.56 & \cellcolor{YellowGreen!25}53.53 \\
& NTK-16k & 52.03 & \textbf{77.00} & 65.35 & 81.04 & 42.97 & 0.00 & \cellcolor{YellowGreen!25}56.58 \\
& Dyn-NTK-16k & 52.03 & 55.00 & 61.88 & 82.16 & 52.34 & 2.22 & \cellcolor{YellowGreen!25}52.89 \\
& YaRN-16k & 55.96 & 75.00 & 63.37 & 79.93 & 62.50 & 5.56 & \cellcolor{YellowGreen!25}\textbf{57.83} \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-16k} & \cellcolor{WildStrawberry!25}54.65 & \cellcolor{WildStrawberry!25}59.09 & \cellcolor{WildStrawberry!25}61.88 & \cellcolor{WildStrawberry!25}\textbf{83.33} & \cellcolor{WildStrawberry!25}65.63 & \cellcolor{WildStrawberry!25}\textbf{6.67} & \cellcolor{YellowGreen!25}55.42 \\

\midrule

\multirow{6}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & SelfExtend-32k & 53.92 & 77.00 & 63.37 & 79.93 & 65.63 & 3.33 & \cellcolor{YellowGreen!25}57.20 \\
& ChunkLlama-32k & 54.36 & 55.00 & 60.89 & 81.78 & 64.06 & 5.56 & \cellcolor{YellowGreen!25}53.61 \\
& NTK-32k & \textbf{58.28} & \textbf{83.00} & \textbf{63.86} & 81.04 & 59.38 & 1.11 & \cellcolor{YellowGreen!25}57.78 \\
& Dyn-NTK-32k & 54.36 & 55.00 & 61.88 & 82.16 & 64.06 & 6.67 & \cellcolor{YellowGreen!25}54.02 \\
& YaRN-32k & 55.23 & 76.00 & 62.38 & 79.18 & \textbf{67.19} & \textbf{7.78} & \cellcolor{YellowGreen!25}\textbf{57.96} \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI-32k} & \cellcolor{WildStrawberry!25}54.17 & \cellcolor{WildStrawberry!25}59.09 & \cellcolor{WildStrawberry!25}62.38 & \cellcolor{WildStrawberry!25}\textbf{82.68} & \cellcolor{WildStrawberry!25}66.41 & \cellcolor{WildStrawberry!25}\textbf{7.78} & \cellcolor{YellowGreen!25}55.29 \\

\bottomrule
\end{tabular}
}
\caption{Performance comparison with different backbone LLMs and training-free length extrapolation methods. The best result in each experiment has been bolded.
{\color{orange}\textsuperscript{*}} indicates the results reported by ChunkLlama\cite{an2024training}, {\color{blue}\textsuperscript{*}} indicates the results reported by LongBench\cite{Jin2024LLMML}, and {\color{red}\textsuperscript{*}} indicates the results reported by L-Eval\cite{an-etal-2024-l}.}

% blue se, orange chunkllama, green longbench, yellow CLEX, red L-eval 
\label{tab:leval_all_llama2}
\end{table}

\subsection{Long language modeling task results}
\label{app:ppl_test}
We also conducted PPL evaluations on the Llama2-7b-chat-4k backbone. As shown in Table \ref{tab:pg19_models_llama2_4k}, GALI maintained a stable PPL, while Dyn-NTK consistently produced the worst results.

\begin{table}[thp]
\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
% \small
% \footnotesize
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{c|ccccccccc}
\toprule
& \textbf{Methods} & \textbf{1k} & \textbf{2k} & \textbf{3k} & \textbf{4k} & \textbf{5k} & \textbf{6k} & \textbf{7k} & \textbf{8k} \\ \midrule
% \multirow{7}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-8b-ins-8k}} & Original & - & - & - & - & - & - & - & - & - \\ 
\multirow{6}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont Llama3-7b-chat-4k}} & SelfExtend & 8.81 & 8.99 & 9.16 & 9.24 & 9.25 & 9.16 & 9.2 & 9.3 \\
& ChunkLlama & 9.07 & 9.26 & 9.41 & 9.45 & 9.43 & 9.31 & 9.31 & 9.39 \\
& NTK  & 8.95 & 9.04 & 9.16 & 9.18 & 9.16 & 9.06 & 9.26 & 13.67 \\
& Dyn-NTK  & 8.81 & 8.99 & 9.15 & 10.79 & 44.32 & 87.35 & 160.07 & 224.07 \\
& YaRN  & 11.65 & 8.97 & 9.07 & 9.16 & 9.17 & 9.15 & 9.03 & 9.04 \\
& \cellcolor{WildStrawberry!25}\textbf{(Ours)GALI} & \cellcolor{WildStrawberry!25}8.81 & \cellcolor{WildStrawberry!25}8.99 & \cellcolor{WildStrawberry!25}9.15 & \cellcolor{WildStrawberry!25}9.24 & \cellcolor{WildStrawberry!25}9.59 & \cellcolor{WildStrawberry!25}9.66 & \cellcolor{WildStrawberry!25}9.63 & \cellcolor{WildStrawberry!25}9.66 \\ 
\bottomrule
\end{tabular}
}
\caption{Performance of various methods on PG19 Dataset with different context windows, using Llama2-7b-chat-4k as the backbone model.}
\label{tab:pg19_models_llama2_4k}
\end{table}

\subsection{Attention distribution analysis results}
\label{app:atten_score_matrix}
In this section, we present the detailed results of the attention distribution analysis. First, we compare the differences between the attention score matrix of length extrapolation methods and the standard attention score matrix, as shown in Figure \ref{attn_score_dist}. For this analysis, we averaged the attention score matrices for each layer and each head before comparison. Whether comparing Llama3-2k or Llama3-4k, GALI consistently achieved the highest similarity to the standard attention score matrix. Additionally, we observed that all methods exhibited higher values in the lower-left corner of the matrix compared to the standard attention score matrix. We attribute this to the fact that these methods do not perform true extrapolation, whereas the standard Llama3-8k model, utilizing a larger positional interval range [0, 8192), results in a lower mean value of the attention scores.

We also analyzed the attention score distribution by extracting 8 rows from the attention score matrix, with the results shown in Figures 5 and 6. The figures clearly demonstrate that GALI’s attention score distribution for each row is closer to the corresponding original attention score distribution. Moreover, as the row index increases, the attention score distributions of all length extrapolation methods show an upward shift relative to the original attention score distribution. This aligns with our earlier analysis, as using a smaller positional interval range results in higher mean value of the attention scores.

\begin{figure}[htp]
    \centering
    % 第一张子图
    \subfigure[Llama3-8b-ins-2k backbone]{
        \includegraphics[width=0.9\linewidth]{images/attn_2k.png}
        \label{fig:attn_2k_dist}
    }
    % 第二张子图
    \subfigure[Llama3-8b-ins-4k backbone]{
        \includegraphics[width=0.9\linewidth]{images/attn_4k.png}
        \label{fig:attn_4k_dist}
    }
    \caption{This is a comparison of the attention score matrices obtained using Llama3-2k and Llama3-4k for length extrapolation with those of Llama3-8k. Note that we averaged the attention scores across all layers and heads, applied average pooling to scale the matrix to 0.05\%, and set the maximum value of the heatmap to 0.0005 for better visualization. “Original” represents the attention score matrix of Llama3-8k, and the number next to each method’s name indicates the sum of the absolute differences between the method’s attention scores matrices and the “Original” matrix.}
    \label{fig:attn_score_dist}
\end{figure}

\begin{figure}[thp]
    \centering
    % 第一行
    \subfigure[Attention scores of row 1000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_1000.png}
        \label{fig:box_2k_1000}
    }
    \subfigure[Attention scores of row 2000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_2000.png}
        \label{fig:box_2k_2000}
    }
    % 第二行
    \subfigure[Attention scores of row 3000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_3000.png}
        \label{fig:box_2k_3000}
    }
    \subfigure[Attention scores of row 4000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_4000.png}
        \label{fig:box_2k_4000}
    }
    % 第三行
    \subfigure[Attention scores of row 5000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_5000.png}
        \label{fig:box_2k_5000}
    }
    \subfigure[Attention scores of row 6000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_6000.png}
        \label{fig:box_2k_6000}
    }
    % 第四行
    \subfigure[Attention scores of row 7000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_7000.png}
        \label{fig:box_2k_7000}
    }
    \subfigure[Attention scores of row 8000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_2k_8000.png}
        \label{fig:box_2k_8000}
    }
    \caption{Attention score distribution using Llama3-2k backbone. We omitted attention scores outside the 1st percentile and the 90th percentile here for clearer visualization.}
    \label{fig:2k_attn_rows}
\end{figure}

\begin{figure}[thp]
    \centering
    % 第一行
    \subfigure[Attention scores of row 1000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_1000.png}
        \label{fig:box_4k_1000}
    }
    \subfigure[Attention scores of row 2000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_2000.png}
        \label{fig:box_4k_2000}
    }
    % 第二行
    \subfigure[Attention scores of row 3000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_3000.png}
        \label{fig:box_4k_3000}
    }
    \subfigure[Attention scores of row 4000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_4000.png}
        \label{fig:box_4k_4000}
    }
    % 第三行
    \subfigure[Attention scores of row 5000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_5000.png}
        \label{fig:box_4k_5000}
    }
    \subfigure[Attention scores of row 6000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_6000.png}
        \label{fig:box_4k_6000}
    }
    % 第四行
    \subfigure[Attention scores of row 7000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_7000.png}
        \label{fig:box_4k_7000}
    }
    \subfigure[Attention scores of row 8000]{
        \includegraphics[width=0.45\linewidth]{images/attn_box/box_4k_8000.png}
        \label{fig:box_4k_8000}
    }
    \caption{Attention score distribution using Llama3-4k backbone. We omitted attention scores outside the 1st percentile and the 90th percentile here for clearer visualization.}
    \label{fig:4k_attn_rows}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
