\section{Related Work}
% Our work primarily encompasses the fields of one-shot Federated Learning (FL) and heterogeneous model FL, which are discussed in detail in the subsections. Moreover, we present a comprehensive comparison, highlighting the distinctive differences between our proposed methods and the important baselines.


\subsection{One-shot Federated Learning}

The one-shot FL methods aim to maximize the aggregation of data information from clients into the global model within a single communication round.~\cite{guha2019one} originally proposes one-shot FL and introduces two aggregation methods: the first one ensembles the local models via bagging strategies, and the second one uses ensemble distillation with auxiliary public data. FedOV~\cite{heinbaugh2023datafree} uses open-set voting~\cite{neal2018open,zhou2021learning} to solve the problem of label skewness when bagging local models. However, bagging-like strategies can introduce additional computational overhead during inference on the client side. FedKT~\cite{Li2020PracticalOF} uses knowledge distillation for aggregating local models into a global model based on auxiliary public data, which is similar to the first of the methods proposed in~\cite{guha2019one}. 

To achieve data-free one-shot FL, DOSFL~\cite{zhou2020distilled} employs dataset distillation~\cite{wang2018dataset}. Each client distills its local data into a small dataset and sends it to the server for global model training. DENSE~\cite{zhang2022dense} trains a generator on the server side, and then uses local models to distill the global model based on synthetic samples produced by the generator. Based on DENSE, Co-Boosting~\cite{dai2024enhancing} further improves the global model performance by optimizing the data and improving the integration. Two-stage methods like DENSE and Co-Boosting, which first train the classifier and then train the generator, cause information loss twice in the two training stages. In order to reduce the double information loss, FedSD2C~\cite{zhang2024one} synthesizes samples directly from local data and proposes to share synthetic samples instead of inconsistent local models to solve the problem of data heterogeneity. However, FedSD2C is only applicable to large-scale datasets with high resolution. \cite{heinbaugh2023datafree} propose FEDCVAE-ENS and FEDCVAE-KD, which deploy generative models locally and generate data based on local label distributions on the server to train the global model. The limitation of the above data-free methods is that they do not take into account the fact that the application scenarios of one-shot FL are usually accompanied by model heterogeneity.



\subsection{Model-Heterogeneous Federated Learning}

To achieve model heterogeneity in FL, some methods extract heterogeneous sub-models from the global model to use as local models. For example, Federated Dropout~\cite{caldas2018expanding} randomly extracts sub-models using Dropout~\cite{srivastava2014dropout}. 
HeteroFL~\cite{diao2020heterofl}, FjORD~\cite{horvath2021fjord}, and FedDSE~\cite{wang2024feddse} extract static sub-models from the global model, while FedRolex~\cite{alam2022fedrolex} and Split-Mix~\cite{hong2022efficient} extract dynamic sub-models. These partial training-based methods require each local model to be a sub-model of the global server model, preventing their deployment in FL scenarios involving completely different model structures.

Knowledge distillation-based methods do not have the limitations of PT-based methods and are more suitable for FL scenarios with diverse models. FedMD~\cite{li2019fedmd} uses transfer learning~\cite{liang2020we} to pretrain local models on large-scale public datasets, then fine-tunes them using local datasets. FedGKT~\cite{he2020group} and FedDKC~\cite{wu2024exploring} use Split Learning~\cite{gupta2018distributed} with the global model serving as a downstream model of local models. Both methods rely on sending label information of local data from clients to the server, exposing the risk of privacy leakage. Methods such as FML~\cite{shen2023federated}, PervasiveFL~\cite{xia2022pervasivefl}, and FedKD~\cite{wu2022communication} assign both a public and a private model to each client. The public model is homogeneous across all clients, while the private model is heterogeneous. By introducing Mutual Learning~\cite{zhang2018deep}, the public and private model can interact with each other locally. However, maintaining two models on each client introduces additional computation and storage overhead.
FedDF~\cite{lin2020ensemble}, DS-FL~\cite{itahara2021distillation}, and Fed-ET~\cite{ijcai2022p399} utilize unlabeled auxiliary data to transfer knowledge from local models to the global model. However, The differences in distribution between auxiliary and local training data can affect the accuracy of the global model~\cite{stanton2021does}.

To address the limitations of using public datasets as auxiliary data, some studies have introduced data-free knowledge distillation techniques~\cite{chen2019data, do2022momentum}. For example, FedFTG~\cite{zhang2022fine} trained a pseudo data generator by fitting the input space of a local model. The generated pseudo data is then integrated into the subsequent distillation process. DFRD~\cite{wang2024dfrd} further improves the training of the pseudo data generator, which can generate synthetic samples more similar to the distribution of local training data, thereby improving the accuracy of the global model. However, the effectiveness of these methods is highly dependent on the quality of the local model. If the performance of the local model is poor, the generated pseudo data may also be of low quality, limiting the accuracy of the global model.


% \subsection{Detailed Related Work Discussion}
% \label{appendix_related_work}

% We thoroughly discuss the distinctions between our proposed FedMHO and three state-of-the-art baselines: DENSE~\cite{zhang2022dense}, Co-Boosting~\cite{dai2024enhancing}, and FEDCVAE~\cite{heinbaugh2023datafree}. DENSE deploys the classification models on the client side, which is the same as the vanilla FL methods. After receiving the local models from clients, the server trains a generator using these models, which is then utilized to train the global model through ensemble knowledge distillation. Co-Boosting continues the idea of DENSE and only improves the training methods of the generator and the global model. Its algorithm skeleton is the same as DENSE. FEDCVAE deploys CVAE on the client side. The client's CVAE decoders and local label distributions are sent to the server to generate synthetic samples. Subsequently, the global model is trained using these synthetic samples. We emphasize that our proposed FedMHOs differ significantly from FEDCVAE. Firstly, our method deploys classification models for computing resource-sufficient clients, whereas FEDCVAE deploys CVAE for such clients. Secondly, our method initiates the training of the global model from an already trained model, while FEDCVAE starts training from scratch. Thirdly, FedMHO may encounter the challenge of knowledge forgetting during global model training, and to address this, we incorporate knowledge distillation with two solutions: FedMHO-MD and FedMHO-SD.