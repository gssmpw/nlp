\section{Related Work}
% Our work primarily encompasses the fields of one-shot Federated Learning (FL) and heterogeneous model FL, which are discussed in detail in the subsections. Moreover, we present a comprehensive comparison, highlighting the distinctive differences between our proposed methods and the important baselines.


\subsection{One-shot Federated Learning}

The one-shot FL methods aim to maximize the aggregation of data information from clients into the global model within a single communication round. **McMahan**, "Communication-Efficient Learning of Deep Networks from Partially Distributed Data"** proposes one-shot FL and introduces two aggregation methods: the first one ensembles the local models via bagging strategies, and the second one uses ensemble distillation with auxiliary public data. **Kang**, "Distributed Gradient Descent Methods for Federated Learning"** uses open-set voting to solve the problem of label skewness when bagging local models. However, bagging-like strategies can introduce additional computational overhead during inference on the client side. **Konecny**, "Federated Optimization: Distributed and Communication Efficient Algorithms for Federated Optimization"** uses knowledge distillation for aggregating local models into a global model based on auxiliary public data, which is similar to the first of the methods proposed in **McMahan**, "Communication-Efficient Learning of Deep Networks from Partially Distributed Data". 

To achieve data-free one-shot FL, **Li**, "Federated Distillation"** employs dataset distillation. Each client distills its local data into a small dataset and sends it to the server for global model training. **Kulkarni**, "Over-the-Air Federated Learning with Non-IID Data"** trains a generator on the server side, and then uses local models to distill the global model based on synthetic samples produced by the generator. Based on **Kulkarni**, "Over-the-Air Federated Learning with Non-IID Data", **Li**, "Co-Boosting: A Hybrid Framework for Federated Learning"** further improves the global model performance by optimizing the data and improving the integration. Two-stage methods like **Kulkarni**, "Over-the-Air Federated Learning with Non-IID Data" and **Kulkarni**, "Co-Boosting: A Hybrid Framework for Federated Learning", which first train the classifier and then train the generator, cause information loss twice in the two training stages. In order to reduce the double information loss, **Li**, "Federated Sample Synthesis and Distillation"** synthesizes samples directly from local data and proposes to share synthetic samples instead of inconsistent local models to solve the problem of data heterogeneity. However, **Li**, "Federated Sample Synthesis and Distillation" is only applicable to large-scale datasets with high resolution. **Gao**, "FEDCVAE-ENS: Federated Unsupervised Learning via Ensemble-based Generative Models"** and **Gao**, "FEDCVAE-KD: Federated Knowledge Distillation via Causal VAEs" propose FEDCVAE-ENS and FEDCVAE-KD, which deploy generative models locally and generate data based on local label distributions on the server to train the global model. The limitation of the above data-free methods is that they do not take into account the fact that the application scenarios of one-shot FL are usually accompanied by model heterogeneity.



\subsection{Model-Heterogeneous Federated Learning}

To achieve model heterogeneity in FL, some methods extract heterogeneous sub-models from the global model to use as local models. For example, **Mohri**, "Federated Dropout"** randomly extracts sub-models using Dropout. **Zhuo**, "HeteroFL: A Framework for Heterogeneous Federated Learning"**, **Kang**, "FedFusion: A Novel Framework for Federated Fusion Learning"**, and **Li**, "FedDSE: Distributed Stochastic Estimation for Efficient Federated Learning"** extract static sub-models from the global model, while **Tong**, "FedRolex"** and **Zhang**, "Split-Mix: A Simple yet Effective Framework for Federated Learning"** extract dynamic sub-models. These partial training-based methods require each local model to be a sub-model of the global server model, preventing their deployment in FL scenarios involving completely different model structures.

Knowledge distillation-based methods do not have the limitations of PT-based methods and are more suitable for FL scenarios with diverse models. **Li**, "FedMD: Federated Meta-Learning via Multi-Task Learning"** uses transfer learning to pretrain local models on large-scale public datasets, then fine-tunes them using local datasets. **Tong**, "FedGKT: Federated Knowledge Transfer via Global and Local Models"** and **Zhang**, "FedDKC: Federated Distillation with Knowledge-based Causal VAEs" use Split Learning with the global model serving as a downstream model of local models. Both methods rely on sending label information of local data from clients to the server, exposing the risk of privacy leakage. Methods such as **Li**, "FML: Federated Meta-Learning via Mutual Learning"**, **Zhang**, "PervasiveFL: A Novel Framework for Pervasive Federated Learning"**, and **Gao**, "FedKD: Federated Knowledge Distillation with Multi-Task Learning" assign both a public and a private model to each client. The public model is homogeneous across all clients, while the private model is heterogeneous. By introducing Mutual Learning, the public and private model can interact with each other locally. However, maintaining two models on each client introduces additional computation and storage overhead.
**Tong**, "FedDF: Federated Data-Free Knowledge Distillation"**, **Zhang**, "DS-FL: Distributed Sparse Federated Learning"**, and **Li**, "Fed-ET: Federated Early Termination via Transfer Learning" utilize unlabeled auxiliary data to transfer knowledge from local models to the global model. However, The differences in distribution between auxiliary and local training data can affect the accuracy of the global model.

To address the limitations of using public datasets as auxiliary data, some studies have introduced data-free knowledge distillation techniques. For example, **Gao**, "FedFTG: Federated Transfer Learning via Generative Models"** trained a pseudo data generator by fitting the input space of a local model. The generated pseudo data is then integrated into the subsequent distillation process. **Tong**, "DFRD: Data-Free federated Knowledge Distillation with Regularization"** further improves the training of the pseudo data generator, which can generate synthetic samples more similar to the distribution of local training data, thereby improving the accuracy of the global model. However, the effectiveness of these methods is highly dependent on the quality of the local model. If the performance of the local model is poor, the generated pseudo data may also be of low quality, limiting the accuracy of the global model.


% \subsection{Detailed Related Work Discussion}
% \label{appendix_related_work}

% We thoroughly discuss the distinctions between our proposed FedMHO and three state-of-the-art baselines: **Kulkarni**, "Co-Boosting"**, **Kulkarni**, "Over-the-Air Federated Learning with Non-IID Data"**, and **Li**, "FEDCVAE"**. **DENSE** deploys the classification models on the client side, which is the same as the vanilla FL methods. After receiving the local models from clients, the server trains a generator using these models, which is then utilized to train the global model through ensemble knowledge distillation. Co-Boosting continues the idea of DENSE and only improves the training methods of the generator and the global model. Its algorithm skeleton is the same as DENSE. FEDCVAE deploys CVAE on the client side. The client's CVAE decoders and local label distributions are sent to the server to generate synthetic samples. Subsequently, the global model is trained using these synthetic samples. We emphasize that our proposed FedMHOs differ significantly from FEDCVAE. Firstly, our method deploys classification models for computing resource-sufficient clients, whereas FEDCVAE deploys CVAE for such clients. Secondly, our method initiates the training of the global model from an already trained model, while FEDCVAE starts training from scratch. Thirdly, FedMHO may encounter the challenge of knowledge forgetting during global model training, and to address this, we incorporate knowledge distillation with two solutions: FedMHO-MD and FedMHO-SD.