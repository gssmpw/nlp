\section{Related Work}
\label{sec:related}\vspace{-0.05in}
 \subsection{Neural Image Compression}\vspace{-0.05in}
Recent image compression methods**Balle, "Context-Adaptive Binary Arithmetic Coding"**
 have achieved tremendous improvement with auto-regressive models.
 To address the serial processing problem, He~\etal**He, Liu, and Wang, "Parallel Multiscale Context Modeling"**
 introduce a parallelized checkerboard context model, while David~\etal**David et al., "Channel-Conditioning and Latent Residual Prediction for Image Compression"**
 conduct channel-conditioning and latent residual prediction to reduce serial operations. EVC**Badrinarayanan et al., "Deep Video Compression"**
 leverages mask decay and sparsity regularization for efficiency and further improves the RD performance of the scalable encoder. 
 DCVC-FM**Lee et al., "Differential Context Vector Quantization with Feature Modulation"**
 modulates features with a learnable quantization scaler and periodically refreshing mechanism to support a wide quality range and long prediction chain. Self-attention-based methods**Zhang et al., "Progressive Image Compression using Self-Attention"**
 develop various self-attention variants to capture non-local information and achieve better RD performance. Mixed architectures of transformer and CNN**Kim et al., "Mixed Transformer-CNNs for Image Compression"**
 are further proposed to exploit both global and local information.  Given the strong ability of generation, generative methods**Chen et al., "Generative Adversarial Networks for Image Compression"**
 achieve visually satisfying results with extremely low bitrates. However, these methods are designed for clean data and rarely consider the practical scenario of degraded inputs, inevitably leading to the waste of bits for preserving unnecessary degradations.  
 
  
 \begin{figure*}[t]
\centering
\vspace{-4pt}\includegraphics[width=0.88\linewidth, clip=true, trim=0 10pt 18pt  28pt]{fig_evc/frameworkv3.pdf}\vspace{-6pt}
\caption{The proposed all-in-one framework, which consists of a feature encoder $\mathcal{G}{\phi_a}$, a feature decoder $\mathcal{G}{\phi_s}$ and a spatial entropy model. The HATB effectively models long-range dependencies with the C-GA, and captures discriminative representations with the S-DA.  }\vspace{-0.15in}  
\label{fig:overview}
\end{figure*}

 \vspace{-0.05in}\subsection{All-in-one Image Restoration} \vspace{-0.05in}
Most image restoration methods**Ma et al., "Progressive Residual Learning for All-in-One Image Restoration"**
 are designed to handle a specific type of degradation, while all-in-one image restoration methods aim to manage multiple degradations with a unified network.  A majority of them**Li et al., "Unified Deep Network for Single-Image Super-Resolution and Image Denoising"**
 rely on degradation priors to guide the subsequent restoration.  Li~\etal**Li et al., "Multi-Head Attention-Based Degradation Prior Learning for All-in-One Image Restoration"**
 employ multi-head encoders to separately embed degraded inputs. NDR**Wang et al., "Non-Destructive Restoration via Query-Injection Mechanism"**
 develops a degradation query-injection mechanism to effectively approximate and utilize the degradation representations. PromptIR**Kim et al., "Prompt-based Image Restoration with Degradation-aware Prompt Generation"**
 guides the restoration process by providing degradation-related prompts. Chen~\etal**Chen et al., "Knowledge Distillation for All-in-One Image Restoration"**
 utilize independent teacher networks for different inputs, and perform knowledge distillation for a lightweight unified network.  WGWSNet**Wang et al., "Weighted Gradient-Based Single-Image Super-Resolution with Degradation-General Representation Learning"**
 first learns degradation-general representations and expands the parameters for specific degradations.  Recent methods**Zhang et al., "Contrastive Learning for All-in-One Image Restoration"**
 adopt contrastive encoders to extract more representative degradation priors.  However, extracting degradation priors involves complex encoders, posing challenges to efficiency in practical applications. 
 
 
 \vspace{-0.05in}\subsection{Joint Image Compression and Restoration} \vspace{-0.05in}
Nowadays, image compression methods increasingly recognize the need to incorporate the ability of restoration into the compression process. 
Cheng~\etal**Cheng et al., "Joint Image Compression and Denoising via Add-on Modules"**
 incorporate two add-on modules to equip a pre-trained image decoder with the ability of joint decoding and denoising. Cai~\etal**Cai et al., "Signal-to-Noise Ratio Aware Branch for Joint Image Compression and Enhancement"**
 focus on the low-light scenario and propose a signal-to-noise ratio aware branch to guide joint compression and enhancement.  NARV**Wang et al., "Noise-Adaptive ResNet VAE for Joint Image Compression and Denoising"**
 presents an end-to-end noise-adaptive ResNet VAE to handle clean and noisy input images. Nevertheless, these works consider limited degradations (\eg, noise and low-light), neglecting the fact that images can be affected by a wide variety of degradations.
 

 


\begin{figure*}[t]
\begin{minipage}{0.447\linewidth}
\centering
\includegraphics[width=0.85\linewidth, clip=true, trim=0 0 0 2pt]{fig_evc/C-GA-vis6v2.pdf} \vspace{-8pt}\caption{Visualization of the input feature $\mathbf{X}^{CGA}$ and output feature $\hat{\mathbf{X}}^{CGA}$ in C-GA. Although degradations and image signals are closely intertwined in the input features, the C-GA  effectively separates degradations from the image content (\eg, the elephant is distinguished from the rain streaks in the yellow box), thereby preserving image signals.} 
\label{fea_vis_cga}
\end{minipage}  \hspace{6pt}
\begin{minipage}{0.535\linewidth}
\centering
 \vspace{-12pt}\includegraphics[width=0.99\linewidth, clip=true, trim=0 24pt 8pt 8pt]{fig_evc/ablation_SDA_vis_v4.pdf} \vspace{-4pt}
\caption{Visual comparisons of output feature $\hat{\mathbf{X}}^{SDA}$ in S-DA and t-SNE results, where SD indicates spatial decoupling. As can be seen, the design of spatial decoupling helps to effectively extract discriminative degradation representations (\eg, the snow spots in the yellow box and distinct clusters in the t-SNE map).} 
\label{ab:SDA}
\end{minipage} \vspace{-0.18in}
\end{figure*}