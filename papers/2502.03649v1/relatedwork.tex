\section{Related Work}
\label{sec:related}\vspace{-0.05in}
 \subsection{Neural Image Compression}\vspace{-0.05in}
Recent image compression methods~\cite{balle, variational, minnen2018joint, guo2021causal} have achieved tremendous improvement with auto-regressive models.
 To address the serial processing problem, He~\etal~\cite{he2021checkerboard} introduce a parallelized checkerboard context model, while David~\etal~\cite{minnen2020channel} conduct channel-conditioning and latent residual prediction to reduce serial operations. EVC~\cite{evc} leverages mask decay and sparsity regularization for efficiency and further improves the RD performance of the scalable encoder. 
 DCVC-FM~\cite{dmci} modulates features with a learnable quantization scaler and periodically refreshing mechanism to support a wide quality range and long prediction chain. Self-attention-based methods~\cite{cheng2020learned, zhu2021transformer, qian2022entroformer, koyuncu2022contextformer} develop various self-attention variants to capture non-local information and achieve better RD performance. Mixed architectures of transformer and CNN~\cite{zou2022devil, mixed} are further proposed to exploit both global and local information.  Given the strong ability of generation, generative methods~\cite{agustsson2019generative,agustsson2023multi} achieve visually satisfying results with extremely low bitrates. However, these methods are designed for clean data and rarely consider the practical scenario of degraded inputs, inevitably leading to the waste of bits for preserving unnecessary degradations.  
 
  
 \begin{figure*}[t]
\centering
\vspace{-4pt}\includegraphics[width=0.88\linewidth, clip=true, trim=0 10pt 18pt  28pt]{fig_evc/frameworkv3.pdf}\vspace{-6pt}
\caption{The proposed all-in-one framework, which consists of a feature encoder $\mathcal{G}{\phi_a}$, a feature decoder $\mathcal{G}{\phi_s}$ and a spatial entropy model. The HATB effectively models long-range dependencies with the C-GA, and captures discriminative representations with the S-DA.  }\vspace{-0.15in}  
\label{fig:overview}
\end{figure*}

 \vspace{-0.05in}\subsection{All-in-one Image Restoration} \vspace{-0.05in}
Most image restoration methods~\cite{xiong2010robust,swinir,restormer,uformer,chen2023learning,li2023efficient,li2024toward,Li_2024_CVPR} are designed to handle a specific type of degradation, while all-in-one image restoration methods aim to manage multiple degradations with a unified network.  A majority of them~\cite{valanarasu2022transweather,li2020all,airnet, park2023all,yao2024neural} rely on degradation priors to guide the subsequent restoration.  Li~\etal~\cite{li2020all} employ multi-head encoders to separately embed degraded inputs. NDR~\cite{yao2024neural} develops a degradation query-injection mechanism to effectively approximate and utilize the degradation representations. PromptIR~\cite{potlapalli2023promptir} guides the restoration process by providing degradation-related prompts. Chen~\etal~\cite{chen2022learning} utilize independent teacher networks for different inputs, and perform knowledge distillation for a lightweight unified network.  WGWSNet~\cite{wgwsnet} first learns degradation-general representations and expands the parameters for specific degradations.  Recent methods~\cite{airnet, park2023all} adopt contrastive encoders to extract more representative degradation priors.  However, extracting degradation priors involves complex encoders, posing challenges to efficiency in practical applications. 
 
 
 \vspace{-0.05in}\subsection{Joint Image Compression and Restoration} \vspace{-0.05in}
Nowadays, image compression methods increasingly recognize the need to incorporate the ability of restoration into the compression process. 
Cheng~\etal~\cite{denoise_chen} incorporate two add-on modules to equip a pre-trained image decoder with the ability of joint decoding and denoising. Cai~\etal~\cite{cai2024make} focus on the low-light scenario and propose a signal-to-noise ratio aware branch to guide joint compression and enhancement.  NARV~\cite{huang2023narv} presents an end-to-end noise-adaptive ResNet VAE to handle clean and noisy input images. Nevertheless, these works consider limited degradations (\eg, noise and low-light), neglecting the fact that images can be affected by a wide variety of degradations.
 

 


\begin{figure*}[t]
\begin{minipage}{0.447\linewidth}
\centering
\includegraphics[width=0.85\linewidth, clip=true, trim=0 0 0 2pt]{fig_evc/C-GA-vis6v2.pdf} \vspace{-8pt}\caption{Visualization of the input feature $\mathbf{X}^{CGA}$ and output feature $\hat{\mathbf{X}}^{CGA}$ in C-GA. Although degradations and image signals are closely intertwined in the input features, the C-GA  effectively separates degradations from the image content (\eg, the elephant is distinguished from the rain streaks in the yellow box), thereby preserving image signals.} 
\label{fea_vis_cga}
\end{minipage}  \hspace{6pt}
\begin{minipage}{0.535\linewidth}
\centering
 \vspace{-12pt}\includegraphics[width=0.99\linewidth, clip=true, trim=0 24pt 8pt 8pt]{fig_evc/ablation_SDA_vis_v4.pdf} \vspace{-4pt}
\caption{Visual comparisons of output feature $\hat{\mathbf{X}}^{SDA}$ in S-DA and t-SNE results, where SD indicates spatial decoupling. As can be seen, the design of spatial decoupling helps to effectively extract discriminative degradation representations (\eg, the snow spots in the yellow box and distinct clusters in the t-SNE map).} 
\label{ab:SDA}
\end{minipage} \vspace{-0.18in}
\end{figure*} 


\vspace{-0.05in}