@article{bucincaTrustThinkCognitive2021,
  title = {To {{Trust}} or to {{Think}}: {{Cognitive Forcing Functions Can Reduce Overreliance}} on {{AI}} in {{AI-assisted Decision-making}}},
  shorttitle = {To {{Trust}} or to {{Think}}},
  author = {Bu{\c c}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},
  year = {2021},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  number = {CSCW1},
  pages = {188:1--188:21},
  doi = {10.1145/3449287},
  urldate = {2024-04-09},
  abstract = {People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.},
  keywords = {,notion},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/5LDD2EC2/Buçinca et al. - 2021 - To Trust or to Think Cognitive Forcing Functions .pdf}
}

@inproceedings{chiangAreTwoHeads2023,
  title = {Are {{Two Heads Better Than One}} in {{AI-Assisted Decision Making}}? {{Comparing}} the {{Behavior}} and {{Performance}} of {{Groups}} and {{Individuals}} in {{Human-AI Collaborative Recidivism Risk Assessment}}},
  shorttitle = {Are {{Two Heads Better Than One}} in {{AI-Assisted Decision Making}}?},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3581015},
  urldate = {2024-03-18},
  abstract = {With the prevalence of AI assistance in decision making, a more relevant question to ask than the classical question of ``are two heads better than one?'' is how groups' behavior and performance in AI-assisted decision making compare with those of individuals'. In this paper, we conduct a case study to compare groups and individuals in human-AI collaborative recidivism risk assessment along six aspects, including decision accuracy and confidence, appropriateness of reliance on AI, understanding of AI, decision-making fairness, and willingness to take accountability. Our results highlight that compared to individuals, groups rely on AI models more regardless of their correctness, but they are more confident when they overturn incorrect AI recommendations. We also find that groups make fairer decisions than individuals according to the accuracy equality criterion, and groups are willing to give AI more credit when they make correct decisions. We conclude by discussing the implications of our work.},
  isbn = {978-1-4503-9421-5},
  keywords = {,notion},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/8DBPUDVX/Chiang 등 - 2023 - Are Two Heads Better Than One in AI-Assisted Decis.pdf}
}

@inproceedings{chiangEnhancingAIAssistedGroup2024,
  title = {Enhancing {{AI-Assisted Group Decision Making}} through {{LLM-Powered Devil}}'s {{Advocate}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  year = {2024},
  month = apr,
  series = {{{IUI}} '24},
  pages = {103--119},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3640543.3645199},
  urldate = {2024-04-16},
  abstract = {Group decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil's advocate in the AI-assisted group decision making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities exhibited by modern large language models (LLMs), we design four different styles of devil's advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil's advocates that argue against the AI model's decision recommendation have the potential to promote groups' appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil's advocate usually does not lead to substantial increases in people's perceived workload for completing the group decision making tasks, while interactive LLM-powered devil's advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.},
  isbn = {9798400705083},
  keywords = {,notion},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/E4ZPUB7B/Chiang 등 - 2024 - Enhancing AI-Assisted Group Decision Making throug.pdf}
}

@article{doHowShouldAgent2022,
  title = {How {{Should}} the {{Agent Communicate}} to the {{Group}}? {{Communication Strategies}} of a {{Conversational Agent}} in {{Group Chat Discussions}}},
  shorttitle = {How {{Should}} the {{Agent Communicate}} to the {{Group}}?},
  author = {Do, Hyo Jin and Kong, Ha-Kyung and Lee, Jaewook and Bailey, Brian P.},
  year = {2022},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW2},
  pages = {387:1--387:23},
  doi = {10.1145/3555112},
  urldate = {2024-03-27}

@article{hancockAIMediatedCommunicationDefinition2020,
  title = {{{AI-Mediated Communication}}: {{Definition}}, {{Research Agenda}}, and {{Ethical Considerations}}},
  shorttitle = {{{AI-Mediated Communication}}},
  author = {Hancock, Jeffrey T and Naaman, Mor and Levy, Karen},
  year = {2020},
  month = mar,
  journal = {Journal of Computer-Mediated Communication},
  volume = {25},
  number = {1},
  pages = {89--100},
  issn = {1083-6101},
  doi = {10.1093/jcmc/zmz022},
  urldate = {2024-07-26},
  abstract = {We define Artificial Intelligence-Mediated Communication (AI-MC) as interpersonal communication in which an intelligent agent operates on behalf of a communicator by modifying, augmenting, or generating messages to accomplish communication goals. The recent advent of AI-MC raises new questions about how technology may shape human communication and requires re-evaluation -- and potentially expansion -- of many of Computer-Mediated Communication's (CMC) key theories, frameworks, and findings. A research agenda around AI-MC should consider the design of these technologies and the psychological, linguistic, relational, policy and ethical implications of introducing AI into human--human communication. This article aims to articulate such an agenda.},
  keywords = {,notion},
  annotation = {TLDR: A research agenda around AI-MC should consider the design of these technologies and the psychological, linguistic, relational, policy and ethical implications of introducing AI into human--human communication.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/TQY7LWP7/Hancock et al. - 2020 - AI-Mediated Communication Definition, Research Ag.pdf;/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/D3JZXEJT/5714020.html}
}

@article{hohensteinArtificialIntelligenceCommunication2023,
  title = {Artificial Intelligence in Communication Impacts Language and Social Relationships},
  author = {Hohenstein, Jess and Kizilcec, Rene F. and DiFranzo, Dominic and Aghajari, Zhila and Mieczkowski, Hannah and Levy, Karen and Naaman, Mor and Hancock, Jeffrey and Jung, Malte F.},
  year = {2023},
  month = apr,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {5487},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-30938-9},
  urldate = {2025-01-21},
  abstract = {Artificial intelligence (AI) is already widely used in daily communication, but despite concerns about AI's negative effects on society the social consequences of using it to communicate remain largely unexplored. We investigate the social consequences of one of the most pervasive AI applications, algorithmic response suggestions (``smart replies''), which are used to send billions of messages each day. Two randomized experiments provide evidence that these types of algorithmic recommender systems change how people interact with and perceive one another in both pro-social and anti-social ways. We find that using algorithmic responses changes language and social relationships. More specifically, it increases communication speed, use of positive emotional language, and conversation partners evaluate each other as closer and more cooperative. However, consistent with common assumptions about the adverse effects of AI, people are evaluated more negatively if they are suspected to be using algorithmic responses. Thus, even though AI can increase the speed of communication and improve interpersonal perceptions, the prevailing anti-social connotations of AI undermine these potential benefits if used overtly.},
  copyright = {2023 The Author(s)},
  langid = {english},
  annotation = {TLDR: It is found that using algorithmic responses changes language and social relationships, which increases communication speed, use of positive emotional language, and conversation partners evaluate each other as closer and more cooperative.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/P8IXJXGS/Hohenstein et al. - 2023 - Artificial intelligence in communication impacts language and social relationships.pdf}
}

@misc{houdeControllingAIAgent2025,
  title = {Controlling {{AI Agent Participation}} in {{Group Conversations}}: {{A Human-Centered Approach}}},
  shorttitle = {Controlling {{AI Agent Participation}} in {{Group Conversations}}},
  author = {Houde, Stephanie and Brimijoin, Kristina and Muller, Michael and Ross, Steven I. and Moran, Dario Andres Silva and Gonzalez, Gabriel Enrique and Kunde, Siya and Foreman, Morgan A. and Weisz, Justin D.},
  year = {2025},
  month = jan,
  eprint = {2501.17258},
  primaryclass = {cs},
  doi = {10.1145/3708359.3712089},
  urldate = {2025-02-07},
  abstract = {Conversational AI agents are commonly applied within single-user, turn-taking scenarios. The interaction mechanics of these scenarios are trivial: when the user enters a message, the AI agent produces a response. However, the interaction dynamics are more complex within group settings. How should an agent behave in these settings? We report on two experiments aimed at uncovering users' experiences of an AI agent's participation within a group, in the context of group ideation (brainstorming). In the first study, participants benefited from and preferred having the AI agent in the group, but participants disliked when the agent seemed to dominate the conversation and they desired various controls over its interactive behaviors. In the second study, we created functional controls over the agent's behavior, operable by group members, to validate their utility and probe for additional requirements. Integrating our findings across both studies, we developed a taxonomy of controls for when, what, and where a conversational AI agent in a group should respond, who can control its behavior, and how those controls are specified and implemented. Our taxonomy is intended to aid AI creators to think through important considerations in the design of mixed-initiative conversational agents.},
  archiveprefix = {arXiv},
  annotation = {TLDR: A taxonomy of controls for when, what, and where a conversational AI agent in a group should respond, who can control its behavior, and how those controls are specified and implemented is developed.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/W3SML52I/Houde et al. - 2025 - Controlling AI Agent Participation in Group Conversations A Human-Centered Approach.pdf;/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/GV4ZYD9X/2501.html}
}

@inproceedings{hwangSoundSupportGendered2024,
  title = {The {{Sound}} of {{Support}}: {{Gendered Voice Agent}} as {{Support}} to {{Minority Teammates}} in {{Gender-Imbalanced Team}}},
  shorttitle = {The {{Sound}} of {{Support}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hwang, Angel Hsing-Chi and Won, Andrea Stevenson},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--22},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642202},
  urldate = {2024-07-16},
  abstract = {The present work explores the potential of leveraging a teamwork agent's identity -- signaled through its gendered voice -- to support marginalized individuals in gender-imbalanced teams. In a mixed design experiment (N = 178), participants were randomly assigned to work with a female and a male voice agent in either a female-dominated or male-dominated team. Results show the presence of a same-gender voice agent is particularly beneficial to the performance of minority female members, such that they would contribute more ideas and talk more when a female agent was present. Conversely, minority male members became more talkative but were less focused on the teamwork tasks at hand when working with a male-sounding agent. The findings of the present experiment support existing literature on the effect of social presence in gender-imbalanced teams, such that gendered agents serve similar benefits as human teammates of the same gender identities. However, the effect of agents' presence remains limited when participants have experienced severe marginalization in the past. Based on findings from the present study, we discuss relevant design implications and avenues for future research.},
  isbn = {9798400703300},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/X9AXULRB/Hwang and Won - 2024 - The Sound of Support Gendered Voice Agent as Supp.pdf}
}

@inproceedings{kimBotBunchFacilitating2020,
  title = {Bot in the {{Bunch}}: {{Facilitating Group Chat Discussion}} by {{Improving Efficiency}} and {{Participation}} with a {{Chatbot}}},
  shorttitle = {Bot in the {{Bunch}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Soomin and Eun, Jinsu and Oh, Changhoon and Suh, Bongwon and Lee, Joonhwan},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376785},
  urldate = {2024-04-02},
  abstract = {Although group chat discussions are prevalent in daily life, they have a number of limitations. When discussing in a group chat, reaching a consensus often takes time, members contribute unevenly to the discussion, and messages are unorganized. Hence, we aimed to explore the feasibility of a facilitator chatbot agent to improve group chat discussions. We conducted a needfinding survey to identify key features for a facilitator chatbot. We then implemented GroupfeedBot, a chatbot agent that could facilitate group discussions by managing the discussion time, encouraging members to participate evenly, and organizing members' opinions. To evaluate GroupfeedBot, we performed preliminary user studies that varied for diverse tasks and different group sizes. We found that the group with GroupfeedBot appeared to exhibit more diversity in opinions even though there were no differences in output quality and message quantity. On the other hand, GroupfeedBot promoted members' even participation and effective communication for the medium-sized group.},
  isbn = {978-1-4503-6708-0},
  keywords = {,notion},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/Z7BACE7I/Kim 등 - 2020 - Bot in the Bunch Facilitating Group Chat Discussi.pdf}
}

@misc{laiScienceHumanAIDecision2021,
  title = {Towards a {{Science}} of {{Human-AI Decision Making}}: {{A Survey}} of {{Empirical Studies}}},
  shorttitle = {Towards a {{Science}} of {{Human-AI Decision Making}}},
  author = {Lai, Vivian and Chen, Chacha and Liao, Q. Vera and {Smith-Renner}, Alison and Tan, Chenhao},
  year = {2021},
  month = dec,
  journal = {arXiv.org},
  urldate = {2024-07-18},
  abstract = {As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI models and AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our survey highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other's work and produce generalizable scientific knowledge. We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.},
  howpublished = {https://arxiv.org/abs/2112.11471v1},
  langid = {english},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/5LMFPRRH/Lai et al. - 2021 - Towards a Science of Human-AI Decision Making A S.pdf}
}

@misc{liuProactiveConversationalAgents2024,
  title = {Proactive {{Conversational Agents}} with {{Inner Thoughts}}},
  author = {Liu, Xingyu Bruce and Fang, Shitao and Shi, Weiyan and Wu, Chien-Sheng and Igarashi, Takeo and Chen, Xiang `Anthony'},
  year = {2024},
  month = dec,
  number = {arXiv:2501.00383},
  eprint = {2501.00383},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.00383},
  urldate = {2025-01-08},
  abstract = {One of the long-standing aspirations in conversational AI is to allow them to autonomously take initiatives in conversations, i.e., being proactive. This is especially challenging for multi-party conversations. Prior NLP research focused mainly on predicting the next speaker from contexts like preceding conversations. In this paper, we demonstrate the limitations of such methods and rethink what it means for AI to be proactive in multi-party, human-AI conversations. We propose that just like humans, rather than merely reacting to turn-taking cues, a proactive AI formulates its own inner thoughts during a conversation, and seeks the right moment to contribute. Through a formative study with 24 participants and inspiration from linguistics and cognitive psychology, we introduce the Inner Thoughts framework. Our framework equips AI with a continuous, covert train of thoughts in parallel to the overt communication process, which enables it to proactively engage by modeling its intrinsic motivation to express these thoughts. We instantiated this framework into two real-time systems: an AI playground web app and a chatbot. Through a technical evaluation and user studies with human participants, our framework significantly surpasses existing baselines on aspects like anthropomorphism, coherence, intelligence, and turn-taking appropriateness.},
  archiveprefix = {arXiv},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/8QUXA9B9/Liu et al. - 2024 - Proactive Conversational Agents with Inner Thoughts.pdf;/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/8NC5GHYQ/2501.html}
}

@misc{maRecommenderExploratoryStudy2024,
  title = {Beyond {{Recommender}}: {{An Exploratory Study}} of the {{Effects}} of {{Different AI Roles}} in {{AI-Assisted Decision Making}}},
  shorttitle = {Beyond {{Recommender}}},
  author = {Ma, Shuai and Zhang, Chenyi and Wang, Xinru and Ma, Xiaojuan and Yin, Ming},
  year = {2024},
  month = mar,
  number = {arXiv:2403.01791},
  eprint = {2403.01791},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.01791},
  urldate = {2024-03-27},
  abstract = {Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing recommendations that the AI deems correct. However, recent studies suggest this may diminish human analytical thinking and lead to humans' inappropriate reliance on AI, impairing the synergy in human-AI teams. In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking. This diversity of roles has not yet been empirically explored in AI assistance. In this paper, we examine three AI roles: Recommender, Analyzer, and Devil's Advocate, and evaluate their effects across two AI performance levels. Our results show each role's distinct strengths and limitations in task performance, reliance appropriateness, and user experience. Notably, the Recommender role is not always the most effective, especially if the AI performance level is low, the Analyzer role may be preferable. These insights offer valuable implications for designing AI assistants with adaptive functional roles according to different situations.},
  archiveprefix = {arXiv},
  keywords = {,notion},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/IH69T9ZF/Ma 등 - 2024 - Beyond Recommender An Exploratory Study of the Ef.pdf;/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/GM8CKS2X/2403.html}
}

@article{mieczkowskiAIMediatedCommunicationLanguage2021,
  title = {{{AI-Mediated Communication}}: {{Language Use}} and {{Interpersonal Effects}} in a {{Referential Communication Task}}},
  shorttitle = {{{AI-Mediated Communication}}},
  author = {Mieczkowski, Hannah and Hancock, Jeffrey T. and Naaman, Mor and Jung, Malte and Hohenstein, Jess},
  year = {2021},
  month = apr,
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {5},
  number = {CSCW1},
  pages = {17:1--17:14},
  doi = {10.1145/3449091},
  urldate = {2024-07-21},
  abstract = {AI-Mediated Communication (AI-MC) is interpersonal communication that involves an artificially intelligent system that can modify, augment, or even generate content to achieve communicative and relational goals. AI-MC is increasingly involved in human communication and has the potential to impact core aspects of human communication, such as language production, interpersonal perception and task performance. Through a between-subjects experimental design we examine how these processes are influenced when integrating AI-generated language in the form of suggested text responses (Google's smart replies) into a text-based referential communication task. Our study replicates and extends the impacts of a positivity bias in AI-generated language and introduces the adjacency pair framework into the study of AI-MC. We also find preliminary yet mixed evidence to suggest that AI-generated language has the potential to undermine some dimensions of interpersonal perception, such as social attraction. This study contributes important concepts for future work in AI-MC and offers findings with implications for the design of AI systems in human-to-human communication.}
}

@misc{natarajanHumanintheloopAIintheloopAutomate2024,
  title = {Human-in-the-Loop or {{AI-in-the-loop}}? {{Automate}} or {{Collaborate}}?},
  shorttitle = {Human-in-the-Loop or {{AI-in-the-loop}}?},
  author = {Natarajan, Sriraam and Mathur, Saurabh and Sidheekh, Sahil and Stammer, Wolfgang and Kersting, Kristian},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14232},
  eprint = {2412.14232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14232},
  urldate = {2025-01-03},
  abstract = {Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop (\$AI{\textasciicircum}2L\$) systems, where the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an \$AI{\textasciicircum}2L\$ perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an \$AI{\textasciicircum}2L\$ approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.},
  archiveprefix = {arXiv},
  langid = {american},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/FM9BU576/Natarajan et al. - 2024 - Human-in-the-loop or AI-in-the-loop Automate or Collaborate.pdf;/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/SZHLUW3T/2412.html}
}

@inproceedings{poddarAIWritingAssistants2023,
  title = {{{AI Writing Assistants Influence Topic Choice}} in {{Self-Presentation}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Poddar, Ritika and Sinha, Rashmi and Naaman, Mor and Jakesch, Maurice},
  year = {2023},
  month = apr,
  series = {{{CHI EA}} '23},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544549.3585893},
  urldate = {2025-01-21},
  abstract = {AI language technologies increasingly assist and expand human communication. While AI-mediated communication reduces human effort, its societal consequences are poorly understood. In this study, we investigate whether using an AI writing assistant in personal self-presentation changes how people talk about themselves. In an online experiment, we asked participants (N=200) to introduce themselves to others. An AI language assistant supported their writing by suggesting sentence completions. The language model generating suggestions was fine-tuned to preferably suggest either interest, work, or hospitality topics. We evaluate how the topic preference of a language model affected users' topic choice by analyzing the topics participants discussed in their self-presentations. Our results suggest that AI language technologies may change the topics their users talk about. We discuss the need for a careful debate and evaluation of the topic priors built into AI language technologies.},
  isbn = {978-1-4503-9422-2},
  annotation = {TLDR: Whether using an AI writing assistant in personal self-presentation changes how people talk about themselves and the need for a careful debate and evaluation of the topic priors built into AI language technologies are investigated.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/FWY2KTPW/Poddar et al. - 2023 - AI Writing Assistants Influence Topic Choice in Self-Presentation.pdf}
}

@inproceedings{robertsonCantReplyThat2021,
  title = {``{{I Can}}'t {{Reply}} with {{That}}'': {{Characterizing Problematic Email Reply Suggestions}}},
  shorttitle = {``{{I Can}}'t {{Reply}} with {{That}}''},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Robertson, Ronald E and Olteanu, Alexandra and Diaz, Fernando and Shokouhi, Milad and Bailey, Peter},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3411764.3445557},
  urldate = {2025-01-21},
  abstract = {In email interfaces, providing users with reply suggestions may simplify or accelerate correspondence. While the ``success'' of such systems is typically quantified using the number of suggestions selected by users, this ignores the impact of social context, which can change how suggestions are perceived. To address this, we developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions. Our interviews revealed issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness. In our experiments, crowdworkers assessed email scenarios that we generated and systematically controlled, showing that contextual factors like social ties and the presence of salutations impacts users' perceptions of email correspondence. These assessments created a novel dataset of human-authored corrections for problematic email replies. Our study highlights the social complexity of providing suggestions for email correspondence, raising issues that may apply to all social messaging systems.},
  isbn = {978-1-4503-8096-6},
  annotation = {TLDR: This study developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions, revealing issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/95RMPS7U/Robertson et al. - 2021 - “I Can’t Reply with That” Characterizing Problematic Email Reply Suggestions.pdf}
}

@article{seboRobotsGroupsTeams2020,
  title = {Robots in {{Groups}} and {{Teams}}: {{A Literature Review}}},
  shorttitle = {Robots in {{Groups}} and {{Teams}}},
  author = {Sebo, Sarah and Stoll, Brett and Scassellati, Brian and Jung, Malte F.},
  year = {2020},
  month = oct,
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {4},
  number = {CSCW2},
  pages = {176:1--176:36},
  doi = {10.1145/3415247},
  urldate = {2025-02-07},
  abstract = {Autonomous robots are increasingly placed in contexts that require them to interact with groups of people rather than just a single individual. Interactions with groups of people introduce nuanced challenges for robots, since robots? actions influence both individual group members and complex group dynamics. We review the unique roles robots can play in groups, finding that small changes in their nonverbal behavior and personality impacts group behavior and, by extension, influences ongoing interpersonal interactions.},
  annotation = {TLDR: The unique roles robots can play in groups are reviewed, finding that small changes in their nonverbal behavior and personality impacts group behavior and, by extension, influences ongoing interpersonal interactions.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/ISF3GZM5/Sebo et al. - 2020 - Robots in Groups and Teams A Literature Review.pdf}
}

@inproceedings{stauferSilencingRiskNot2024,
  title = {Silencing the {{Risk}}, {{Not}} the {{Whistle}}: {{A Semi-automated Text Sanitization Tool}} for {{Mitigating}} the {{Risk}} of {{Whistleblower Re-Identification}}},
  shorttitle = {Silencing the {{Risk}}, {{Not}} the {{Whistle}}},
  booktitle = {Proceedings of the 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Staufer, Dimitri and Pallas, Frank and Berendt, Bettina},
  year = {2024},
  month = jun,
  series = {{{FAccT}} '24},
  pages = {733--745},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3630106.3658936},
  urldate = {2024-07-21},
  abstract = {Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors. However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously. The specific content of their disclosures and their distinct writing style may re-identify them as the source. Legal measures, such as the EU Whistleblower Directive, are limited in their scope and effectiveness. Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward. However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity. They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other labels of named entities) and combinations thereof with placeholders. Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including the whistleblower's writing style. Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility. Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text. We then use a Large Language Model (LLM) that we fine-tuned for paraphrasing to render this text coherent and style-neutral. We evaluate our tool's effectiveness using court cases from the European Court of Human Rights (ECHR) and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution attacks and utility loss statistically using the popular IMDb62 movie reviews dataset, which consists of 62 individuals. Our method can significantly reduce authorship attribution accuracy from 98.81\% to 31.22\%, while preserving up to 73.1\% of the original content's semantics, as measured by the established cosine similarity of sentence embeddings.},
  isbn = {9798400704505},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/9LA3TKYM/Staufer et al. - 2024 - Silencing the Risk, Not the Whistle A Semi-automa.pdf}
}

@article{wangUnderstandingDesignSpace2022,
  title = {Understanding the {{Design Space}} of {{AI-Mediated Social Interaction}} in {{Online Learning}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Understanding the {{Design Space}} of {{AI-Mediated Social Interaction}} in {{Online Learning}}},
  author = {Wang, Qiaosi and Camacho, Ida and Jing, Shan and Goel, Ashok K.},
  year = {2022},
  month = apr,
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {6},
  number = {CSCW1},
  pages = {130:1--130:26},
  doi = {10.1145/3512977},
  urldate = {2024-07-19},
  abstract = {Our online interactions are constantly mediated through Artificial Intelligence (AI), especially our social interactions. AI-mediated social interaction is the AI-facilitated process of building and maintaining social connections between individuals through information inferred from people's online posts. With its impending application across a number of contexts, the challenges and opportunities of AI-mediated social interaction remain underexplored. This paper seeks to understand the design space of AI-mediated social interaction in the context of online learning, where students frequently face social isolation. We deployed an AI agent named SAMI in three class discussion forums to help online learners build social connections. Using SAMI as a probe, we conducted semi-structured interviews with 26 students to understand their difficulties in remote social interactions and their experiences with SAMI. Through the lenses of social translucence and social-technical gap, we illustrate online learners' difficulties in remote social interactions and how SAMI resolved some of the difficulties. We also identify potential ethical and social challenges of SAMI such as user agency and privacy. Based on our findings, we outline the design space of AI-mediated social interaction. We discuss the design tension between AI performance and ethical design and pinpoint two design opportunities for AI-mediated social interaction in designing towards human-AI collaborative social matching and artificial serendipity.},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/VLN6GDA8/Wang et al. - 2022 - Understanding the Design Space of AI-Mediated Soci.pdf}
}

@misc{zhangBreakingBarriersBuilding2025,
  title = {Breaking {{Barriers}} or {{Building Dependency}}? {{Exploring Team-LLM Collaboration}} in {{AI-infused Classroom Debate}}},
  shorttitle = {Breaking {{Barriers}} or {{Building Dependency}}?},
  author = {Zhang, Zihan and Sun, Black and An, Pengcheng},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09165},
  eprint = {2501.09165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09165},
  urldate = {2025-02-05},
  abstract = {Classroom debates are a unique form of collaborative learning characterized by fast-paced, high-intensity interactions that foster critical thinking and teamwork. Despite the recognized importance of debates, the role of AI tools, particularly LLM-based systems, in supporting this dynamic learning environment has been under-explored in HCI. This study addresses this opportunity by investigating the integration of LLM-based AI into real-time classroom debates. Over four weeks, 22 students in a Design History course participated in three rounds of debates with support from ChatGPT. The findings reveal how learners prompted the AI to offer insights, collaboratively processed its outputs, and divided labor in team-AI interactions. The study also surfaces key advantages of AI usage, reducing social anxiety, breaking communication barriers, and providing scaffolding for novices, alongside risks, such as information overload and cognitive dependency, which could limit learners' autonomy. We thereby discuss a set of nuanced implications for future HCI exploration.},
  archiveprefix = {arXiv},
  langid = {american},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/GMC7BYHB/Zhang et al. - 2025 - Breaking Barriers or Building Dependency Exploring Team-LLM Collaboration in AI-infused Classroom D.pdf;/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/VTGEQQVS/2501.html}
}

@inproceedings{zhengCompetentRigidIdentifying2023,
  title = {Competent but {{Rigid}}: {{Identifying}} the {{Gap}} in {{Empowering AI}} to {{Participate Equally}} in {{Group Decision-Making}}},
  shorttitle = {Competent but {{Rigid}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zheng, Chengbo and Wu, Yuheng and Shi, Chuhan and Ma, Shuai and Luo, Jiehui and Ma, Xiaojuan},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--19},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3581131},
  urldate = {2024-03-26},
  abstract = {Existing research on human-AI collaborative decision-making focuses mainly on the interaction between AI and individual decision-makers. There is a limited understanding of how AI may perform in group decision-making. This paper presents a wizard-of-oz study in which two participants and an AI form a committee to rank three English essays. One novelty of our study is that we adopt a speculative design by endowing AI equal power to humans in group decision-making. We enable the AI to discuss and vote equally with other human members. We find that although the voice of AI is considered valuable, AI still plays a secondary role in the group because it cannot fully follow the dynamics of the discussion and make progressive contributions. Moreover, the divergent opinions of our participants regarding an ``equal AI'' shed light on the possible future of human-AI relations.},
  isbn = {978-1-4503-9421-5},
  keywords = {,notion},
  file = {/Users/soohwanlee/Library/CloudStorage/OneDrive-개인/GRADUATE/Zotero/storage/IXU9QXJS/Zheng 등 - 2023 - Competent but Rigid Identifying the Gap in Empowe.pdf}
}

