\section{Introduction}
The deployment of AI models on edge devices is increasingly following a hybrid approach, where small language models (SLMs) run on-device (e.g., smartphones and IoT devices) while larger, more powerful models remain in the cloud~\cite{Gunter2024AppleIF}. This setup provides a balance between efficiency and performance, allowing low-latency responses for simple queries while reserving cloud-based LLMs for more complex tasks. However, determining when to offload queries to the cloud is a crucial challenge: calling the cloud unnecessarily increases cost and latency, whereas over-relying on local SLMs risks suboptimal response quality. An effective routing strategy is essential to dynamically balance cost and performance.


Traditional cascading routers, which sequentially query models until a satisfactory response is obtained~\cite{Chen2023FrugalGPTHT}, are inefficient for edge-cloud settings due to latency and redundant model calls. Recent predictive routing approaches aim to preemptively select the best model for a given query, with two leading solutions: TO-Router~\cite{Stripelis2024TensorOperaRA}, which trains router on accuracy-based benchmarks and RouteLLM~\cite{Ong2024RouteLLMLT}, which relies on human preference selection for router training.


\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.9\columnwidth]{figures/scale.png}
  \end{center}
  \vspace{-5mm}
  \caption{Performance of human preference-based router with varying training sample sizes. Routing efficiency even becomes worse as the number of training samples increases, indicating that additional data does not necessarily improve performance.}
  \label{fig:scale_gsm8k}
  \vspace{-4mm}
\end{figure}

While human preference data and benchmark accuracy are commonly used as performance indicators in router training, our results reveal two major limitations that hinder data efficiency and system utilization. First, \textbf{\textit{human judgment is not always reliable.}} User ratings are subjective and inconsistent, often failing to provide an accurate ranking of model performance. Also, collecting human-preference data is resource-intensive, requiring manual evaluation of each sample on a case-by-case basis. These issues are particularly evident in the arena dataset~\cite{chiang2024chatbot}, where the distribution of preference data across models is sparse and uneven, complicating router training. Empirically, we demonstrate that arena data does not follow a scaling law to validate our argument. As shown in Figure~\ref{fig:scale_gsm8k}, increasing the dataset size does not necessarily improve routing performance. Instead, adding more data can introduce noise and inconsistencies, potentially degrading routing accuracy rather than enhancing it. Second, \textbf{\textit{accuracy is an incomplete indicator.}} Binary accuracy metrics do not capture response confidence, meaning two models may provide correct answers, yet one is significantly more precise and reliable.

To address the limitations of state-of-the-art methods, we propose \textbf{the Confidence-Driven LLM Router System}, which leverages Semantic Entropy (SE) as an uncertainty measure to guide routing decisions. Instead of relying on human preferences or accuracy-based thresholds, our system uses semantic entropy to measure model confidence. This enables the router to offload queries to cloud-based LLMs when higher certainty is needed, which keeps confident responses on-device to minimize cost. As the motivational example shown in Figure~\ref{fig:mt_bench}, by adopting uncertainty as a routing signal, our approach dynamically optimizes response quality and computational efficiency, making it better suited for real-world edge-cloud deployments.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.9\columnwidth]{figures/mt-bench_v2.png}
  \end{center}
  \vspace{-5mm}
  \caption{Routing performance/cost trade-off between strong model (GPT-4) and weak model (Mixtral-8x7B). All routers shown, except the random router, use the same kNN-based model architecture.}
  \label{fig:mt_bench}
  \vspace{-4mm}
\end{figure}
