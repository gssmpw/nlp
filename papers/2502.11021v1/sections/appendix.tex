\newpage
\onecolumn
\section{Appendix}
\subsection{Details about Routing Architectures}
We select four different predictive routing methods in our evaluation. To match the hardware constraints on edge computing, we purposely select the lightweight routing architectures in our experiments. Now, we describe our approach for learning the win prediction model \( P(\text{win}_{\mathcal{M}_{\text{strong}}} \mid q) \). We represent a sample from our dataset \( \mathcal{D} \) as \( (q, M_w, M_l) \), where \( M_w \) and \( M_l \) denote the winning and losing models, respectively.


\textbf{Similarity-Weighted (SW) Ranking.} 
Same as RouteLLM \cite{Ong2024RouteLLMLT}, we adopt a Bradley-Terry (BT) model \cite{Bradley1952RankAO} for this routing task. Given an input query \( q \), we compute a weight \( \omega_i \) for each query \( q_i \) in the training set based on its similarity to \( q \), as follows:
\begin{equation}
\omega_i = \gamma^{1 + S(q, q_i)},
\end{equation}
where \( \gamma \) is a scaling factor which is 10 in our case, and \( S(q, q_i) \) represents the similarity between queries \( q \) and \( q_i \), defined as:
\begin{equation}
S(q, q_i) = \frac{\epsilon \cdot \epsilon_i}{\|\epsilon\| \|\epsilon_i\| \cdot \max_{1 \leq s \leq |D|} \left( \frac{\epsilon_i \cdot \epsilon_s}{\|\epsilon_i\| \|\epsilon_s\|} \right)} ,
\end{equation}
with \( \epsilon \) denoting a query embedding. The BT coefficients \( \xi \) are then learned by solving:
\begin{equation}
\arg\min_{\xi} \sum_{i=1}^{|D|} \omega_i \cdot \ell \left( l_i, \frac{1}{1 + e^{\xi_{w_i} - \xi_{l_i}}} \right),
\end{equation}
where \( \ell \) is the binary cross-entropy loss.

The learned BT coefficients allow us to estimate the win probability given query \( q \) as:
\begin{equation}
P(\text{win}_{M_w} \mid q) = \frac{1}{1 + e^{\xi_w - \xi_l}}.
\end{equation}
This routing model does not require additional training, and the optimization is performed at inference time.


\textbf{Matrix Factorization.} Inspired by the RouteLLM approach \cite{Ong2024RouteLLMLT, 5197422}, we leverage matrix factorization as one of our routing models. The objective is to uncover a latent scoring function \( s: \mathcal{M} \times \mathcal{Q} \rightarrow \mathbb{R} \) that assesses the quality of the model \( M_w \)'s response to a given query \( q \). Specifically, if model \( M_w \) performs better than \( M_l \) on query \( q \), then \( s(M_w, q) > s(M_l, q) \). We encode this preference by modeling the win probability using a Bradley-Terry (BT) relationship \cite{Bradley1952RankAO}:
\begin{equation}
P(\text{win}_{M_w} \mid q) = \sigma(s(M_w, q) - s(M_l, q)),
\end{equation}
where \( \sigma \) is the sigmoid function, and \( s \) is a bilinear function of the model and query embeddings. This approach effectively performs matrix factorization over the score matrix on the set \( \mathcal{Q} \times \mathcal{M} \).

\textbf{Multilayer Perceptron (MLP).} For the MLP routing, we utilize a 2-layer multilayer perceptron (MLP) architecture. The output \( y_k \) for the MLP-Router is given by:
\begin{equation}
P(\text{win}_{M_w} \mid q) = \varphi \left( \sum_{j=1}^{m} w_{jk}^{(2)} \text{ReLU} \left( \sum_{i=1}^{n} w_{ij}^{(1)} \epsilon_i + b_j^{(1)} \right) + b_k^{(2)} \right),
\end{equation}
where \( \varphi \) represents the softmax activation function in the output layer and \( \epsilon \) denoting a query embedding.

\textbf{k-Nearest Neighbors (kNN).}
The k-Nearest Neighbors router represents all training queries \( q_i \) with an embedding \( \epsilon_i \). For each test query \( q \), with embedding \( \epsilon\), we identify the closest training query \( q' \) by finding the query in the training set with the highest cosine similarity to \( \epsilon \):
\begin{equation}
i = \argmin_{1 \leq i \leq |D|} \left( \frac{\epsilon_i \cdot \epsilon}{\|\epsilon_i\| \|\epsilon\|} \right).
\end{equation}
\begin{equation}
q' = q_i
\end{equation}
After identifying the nearest query \( q' \), the router decides on the winner model based on the performance of the winner model associated with \( q' \). 
This method leverages the similarity between the test query and training queries to select the most suitable expert dynamically.

\subsection{Details about Datasets}
To train the RouteLLM router, we randomly sample 12,247 data points from the Chatbot Arena dataset~\cite{chiang2024chatbot}. In contrast, both the Confidence-Driven Router and TO-Router are trained using a combination of Natural QA~\cite{Kwiatkowski, lee-etal-2019-latent}, Trivia QA~\cite{2017arXivtriviaqa}, PopQA~\cite{mallen2023llm_memorization}, and MAWPS~\cite{KoncelKedziorski2016MAWPSAM} datasets to ensure a comparable data volume. Specifically, we randomly selected 3,610 samples from each QA dataset and 1,418 samples from the MAWPS dataset, resulting in 12,247 samples, matching the quantity used for RouteLLM.

To comprehensively evaluate the routing systems, we select a diverse set of benchmark datasets: the MMLU~\cite{Hendrycks2020MeasuringMM} dataset, consisting of 14,042 questions across 57 subjects; the MT-Bench dataset~\cite{Zheng2023JudgingLW}, which includes 160 open-ended questions assessed using the LLM-as-a-judge approach; and the GSM8K dataset~\cite{Cobbe2021TrainingVT}, containing over 1,000 grade-school math problems. These datasets provide a broad evaluation across varied question types and subject domains. For all the data listed above, we properly use them under the propose of research by following their license. 

\subsection{Models and Test Environment}
We implemented the experiments using PyTorch~\cite{pytorch}, and conducted our experiments on two NVIDIA A100 GPUs. For GPT-4 model, we use \texttt{gpt-4-0613} API. For GPT-o1 model, we use \texttt{o1-2024-12-17} API.

\subsection{System Prompt Design for LLM-as-a-Judge}
To evaluate response quality, we use LLM-as-a-Judge to simulate human ratings. We employ an independent LLM (i.e., GPT-o1) to choose the most preferable responses from the routed model alongside ground-truth answers. The judge is instructed to select based on correctness and precision of reasoning. We designed and implemented the following system prompt:

\begin{adjustwidth}{1cm}{1cm}
\begin{lstlisting}[breakatwhitespace=true, showstringspaces=false, basicstyle=\ttfamily, columns=fullflexible, breaklines=true]
You are an evaluator for math problem solutions. You will receive:
1. A question.
2. A ground truth answer.
3. Three LLM-generated responses.
Your task is to select which response(s) is/are best, based on whether the answer is correct and the reasoning is precise.
Follow these rules: 
* DO NOT provide any explanation or reasoning in your answer-only state which LLM(s) you judge as having the best response.
* If more than one response is equally best, name each of them.
Question: {}
Ground Truth Answer:{}
LLM 1 Response: {}
LLM 2 Response: {}
LLM 3 Response: {}
Your output must ONLY indicate the selected LLM(s). For example, 'LLM 1' or 'LLM 1 and LLM 3'.
\end{lstlisting}
\end{adjustwidth}

% For the audience to better understand how we perform the LLM-as-a-Judge, we provide an example we used in the evaluation as shown below. \textcolor{blue}{Blue} text represents the system prompt, \textcolor{orange}{orange} text is the user input, and \textcolor{green}{green} text is output by LLM.


% \begin{adjustwidth}{1cm}{1cm}
% \begin{lstlisting}[breakatwhitespace=true, showstringspaces=false, basicstyle=\ttfamily, columns=fullflexible, breaklines=true, escapeinside={(*@}{@*)}]
% (*@\textcolor{blue}{You are an evaluator for math problem solutions. You will receive:}@*)
% (*@\textcolor{blue}{1. A question.}@*)
% (*@\textcolor{blue}{2. A ground truth answer.}@*)
% (*@\textcolor{blue}{3. Three LLM-generated responses.}@*)
% (*@\textcolor{blue}{Your task is to select which response(s) is/are best, based on whether the answer is correct and the reasoning is precise.}@*)
% (*@\textcolor{blue}{Follow these rules:}@*)
% (*@\textcolor{blue}{- DO NOT provide any explanation or reasoning in your answerâ€”only state which LLM(s) you judge as having the best response.}@*)
% (*@\textcolor{blue}{- If more than one response is equally best, name each of them.}@*)

% (*@\textcolor{orange}{Question: James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?}@*)
% (*@\textcolor{orange}{Ground Truth Answer:540}@*)
% (*@\textcolor{orange}{LLM 1 Response: James runs 3 sprints each time, and he does this 3 times a week. Each sprint is 60 meters. So, for each session, he runs 3 sprints * 60 meters/sprint = 180 meters.
% Since he does this 3 times a week, the total meters he runs a week is 180 meters/session * 3 sessions/week = 540 meters/week.}@*)
% (*@\textcolor{orange}{#### 540}@*)
% (*@\textcolor{orange}{LLM 2 Response: James runs 3 sprints each time, and he does this 3 times a week. Each sprint is 60 meters. So, for each session, he runs 3 sprints * 60 meters/sprint = 180 meters.
% Since he does this 3 times a week, the total meters he runs a week is 180 meters/session * 3 sessions/week = 540 meters/week.}@*)
% (*@\textcolor{orange}{#### 540}@*)
% (*@\textcolor{orange}{LLM 3 Response: James runs 3 sprints each time, and he does this 3 times a week. Each sprint is 60 meters. So, for each session, he runs 3 sprints * 60 meters/sprint = 180 meters.
% Since he does this 3 times a week, the total meters he runs a week is 180 meters/session * 3 sessions/week = 540 meters/week.}@*)
% (*@\textcolor{orange}{#### 540}@*)
% (*@\textcolor{orange}{Your output must ONLY indicate the selected LLM(s). For example, 'LLM 1' or 'LLM 1 and LLM 3'.}@*)
% \end{lstlisting}
% \end{adjustwidth}





