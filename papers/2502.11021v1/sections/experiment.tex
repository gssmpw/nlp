\section{Evaluation}
\subsection{Experimental Methodology}
\subsubsection{Tasks, Datasets, and Models}
We use GPT-4 as the strong model and Mixtral-8x7B as the weaker model.
The Confidence-Driven Router is trained with a combination of Natural QA~\cite{Kwiatkowski, lee-etal-2019-latent}, Trivia QA~\cite{2017arXivtriviaqa}, PopQA~\cite{mallen2023llm_memorization}, and MAWPS~\cite{KoncelKedziorski2016MAWPSAM} datasets to ensure a knowledge space. We randomly selected 3,610 samples from each QA dataset and 1,418 samples from the MAWPS dataset, resulting in 12,247 samples, matching the quantity as Chatbot Arena dataset~\cite{chiang2024chatbot} for RouteLLM training. We select MMLU~\cite{Hendrycks2020MeasuringMM}, MT-Bench~\cite{Zheng2023JudgingLW}, and GSM8K datasets~\cite{Cobbe2021TrainingVT} to comprehensively evaluate the routing systems. Following the previous works~\cite{Ong2024RouteLLMLT, Stripelis2024TensorOperaRA}, we select four different predictive routing models for evaluation: Similarity-Weighted (SW) Ranking, Matrix Factorization (MF), Multilayer Perceptron (MLP), and k-Nearest Neighbors (kNN).
%
More details related to the datasets and routing models are listed in the Appendix.


\begin{table*}[t]
\caption{System Performance Comparison of Routing Systems on test datasets. A low CPT value indicates a cost-effective routing strategy. \textbf{Bold} highlight the best performance, and \underline{underlined} denote the second-best.}

    % \vspace{1mm}
    \footnotesize
    \begin{tabular*}{1.0\linewidth}{cccccccc}
        % \setlength{\tabcolsep}{1pt}
        \toprule
        & &
        \multicolumn{2}{c}{\textbf{MT-Bench}} & 
        \multicolumn{2}{c}{\textbf{GSM8K}} &
        \multicolumn{2}{c}{\textbf{MMLU}} \\ 
        \midrule 

        \textbf{Routing} & 
        \textbf{Method} & 
        \textbf{CPT(50\%)} & 
        \textbf{CPT(80\%)} & 
        \textbf{CPT(50\%)} & 
        \textbf{CPT(80\%)} &  
        \textbf{CPT(50\%)} & 
        \textbf{CPT(80\%)} \\
        \midrule 

        &
        Random &
        51.29 &
        78.55 &
        48.79 &
        80.16 &
        50.04 &
        79.32 \\
        \midrule

        \multirow{2}{*}{TO-Router} &
        kNN &
        59.72 &
        90.39 &
        47.93 &
        79.03 &
        43.73 &
        77.74 \\

        &
        MLP &
        49.15 &
        86.67 &
        51.03 &
        77.77 &
        44.01 &
        77.43 \\
        \midrule

        \multirow{2}{*}{RouteLLM} &
        SW &
        56.08 &
        78.37 &
        46.03 &
        79.58 &
        47.41 &
        \underline{74.23}\\

        &
        MF &
        55.59 &
        84.12 &
        49.07 &
        80.09 &
        58.55 &
        83.68 \\
        \midrule

        \multirow{4}{*}{
        \shortstack{Confidence-Driven \\ LLM Router}}
        &
        SW &
        \textbf{27.31} &
        \textbf{55.61} &
        48.03 &
        80.41 &
        \textbf{37.96} &
        \textbf{73.85} \\

        &
        MF &
        42.94 &
        \underline{63.53} &
        \textbf{41.89} &
        \textbf{75.34} &
        50.06 &
        78.38\\

        &
        kNN &
        60.84 &
        81.50 &
        \underline{44.08} &
        \underline{76.32} &
        \underline{42.70} &
        75.28\\

        
        &
        MLP &
        \underline{35.54}&
        74.92&
        50.04&
        79.79&
        57.07&
        83.27\\
        \bottomrule
    \end{tabular*}
\vspace{-1mm}
\label{table:baseline_results}
\end{table*}

\subsubsection{Evaluation Criteria}
We evaluate performance based on two key criteria: system cost and response quality.

To evaluate system cost, we adopt the Call-Performance Threshold (CPT) metric from prior work~\cite{Ong2024RouteLLMLT}. CPT(x\%) represents the minimum fraction of queries that must be routed to the stronger model to achieve an x\% improvement over the baseline accuracy of the weaker model. For example, CPT(50\%) specifies the proportion of calls required to the strong model to attain a 50\% improvement over the weak modelâ€™s baseline accuracy. A lower CPT value indicates a more cost-effective routing strategy, achieving performance gains with fewer calls to the stronger model.


To evaluate response quality, we use LLM-as-a-Judge to simulate human ratings. We employ an independent LLM (i.e., GPT-o1) to choose the most preferable responses from the routed model alongside ground-truth answers. The judge is instructed to select based on correctness and precision of reasoning, as the detailed system prompt listed in the Appendix. We design the score as $\text{Score}(i) = \left(\frac{S_i}{T}\right) \times 100$, where \( S_i \) be the number of times router \( i \) is selected, and \( T \) be the total number of queries. Unlike traditional accuracy-based evaluations using golden labels, this approach simulates human judgment by considering not only the correctness but also the interpretability and coherence of model outputs, better aligning with human preference selection rather than relying solely on objective correctness.


\begin{table}[t]
% \scriptsize
% \footnotesize
\begin{center}
\caption{
Response quality comparison of routing systems on the GSM8K dataset. Higher LLM-judge scores reflect better response quality.
}
\vspace{-2mm}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccc} 
\hline
\toprule
& TO-Router & RouteLLM & Confidence Router \\ %\hline
\midrule
CPT(50\%) & 78.88 & 79.72 & \textbf{79.95} \\
\midrule
CPT(80\%) & 85.97 & 88.88 & \textbf{89.21} \\
\bottomrule
\label{tab:se_results}
\end{tabular}
}
\end{center}
\vspace{-7mm}
\end{table}


\subsubsection{Baseline Selection}
We select RouteLLM~\cite{Ong2024RouteLLMLT} and TO-Router~\cite{Stripelis2024TensorOperaRA}, two state-of-the-art predictive routing systems. Following their original configurations, we evaluate TO-Router using kNN and MLP architectures, while RouteLLM is assessed using SW ranking and MF models. We also include a random router without any training as a baseline for comparison.



\subsection{Evaluation on System Costs}
We first evaluate the system costs of various routing strategies on the test datasets, as summarized in Table~\ref{table:baseline_results}. The Confidence-Driven LLM Router consistently achieves strong performance in CPT(50\%) and CPT(80\%) across datasets.
%
To provide a clearer comparison, we report the actual OpenAI API cost (in USD) for each routing system on the MT-Bench dataset under CPT(80\%): Random router costs \$4.06; TO-Router costs \$3.88; RouteLLM costs \$4.04; and our proposed method costs lowest with \$3.74. 
%
These results demonstrate that leveraging uncertainty as a routing indicator provides notable advantages in achieving target performance improvements while balancing computational costs.

\subsection{Evaluation on Response Quality}

To better understand the advantage of our proposed method, we evaluate the response quality of each routing system under the same accuracy with GSM8K dataset. We select the best routing models under each routing system in Table~\ref{table:baseline_results}. As summarized in Table~\ref{tab:se_results}, the proposed method achieves the highest LLM-as-a-Judge rating, indicating that its responses are the most human-preferable among all baselines. Our uncertainty-aware training strategy optimizes routing decisions based on a direct measure of model confidence. By minimizing uncertainty in routing, our approach ensures that queries are directed to models that can generate the most confident and reliable responses, leading to outputs that better align with human preferences.

