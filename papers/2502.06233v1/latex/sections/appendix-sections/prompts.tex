
\section{Prompting Techniques}
\label{sec:appendix-prompting}

As described in Section \ref{sec:conf-methods}, for our prompt based confidence extraction techniques (Verbal Confidence, P(True)), we used a two-step approach: First, we prompted the model to answer benchmark questions using the prompts shown in Table \ref{tab:question-prompts}. Then, we extracted confidence by concatenating the prompts shown in Table \ref{tab:confidence-prompts} and running the model again. This two-step process allowed using the same answers when comparing self-consistency and CISC.

While a simpler single-step implementation (outputting both answer and confidence in a single response) is possible, we did not explore it in this study. For research purposes, we prioritized a clean setup that ensured requesting confidence scores did not influence the generated answers and chain-of-thoughts.

As shown in Table \ref{tab:confidence-prompts}, all the confidence extraction prompts that we used are extremely lightweight. We deliberately avoided methods that significantly increase the number of generated tokens like generating $k$ guesses with associated probabilities \cite{tian2023just}. 

For the P(True) method, we modified the prompts from \citet{kadavath2022language} in two ways: (1) We changed the format to allow concatenation after the model provided its answer, ensuring that prefix caching could be re-used between the two steps. (2) We changed the prompt format to 0/1 instead of True/False, as some benchmarks are using True/False as ground truth labels, and we observed that it might confuse the model when extracting confidence scores.

\begin{table*}[h]
\centering
\begin{tabular}{c l}
\toprule
\multicolumn{2}{l}{General Instructions} \\ 
\toprule
\multicolumn{2}{l}{\makecell[l{p{15cm}}]{
Before giving your answer, provide a step-by-step explanation of your thought process. Then on a new line, give your proposed answer adhering to this precise format: 'Proposed answer: (X).', where X is your proposed answer.
\\\\
}} \\

\toprule
Dataset  & Prompt \\
\toprule
MMLU-Pro  & \makecell[l{p{11cm}}]{
You will be given a single-choice question. Answer the question by selecting the letter of the
best fitting option.\\\\\emph{[General Instructions]}\\\\The answer MUST ALWAYS
be the letter of one of the available options; it CANNOT be "None of the Above".
} \\
\midrule

MATH      & \makecell[l{p{11cm}}]{
You will be given a question and your goal is to answer it correctly.\textbackslash nYour proposed answer should be a TeX expression, such as '\$5\$', '\$3.14\$', or '\$\textbackslash\textbackslash sqrt\{8\}\$\\\\\emph{[General Instructions]}
} \\
\midrule

\makecell{BBH\\(no options)}      & \makecell[l{p{11cm}}]{
You will be given a question and your goal is to answer it correctly.\\\\\emph{[General Instructions]}
} \\
\midrule
\makecell{BBH\\(with options)}      & \makecell[l{p{11cm}}]{
You will be given a question and your goal is to answer it correctly.\\\\\emph{[General Instructions]}\\\\
Select the letter of the best fitting option. The answer CANNOT be "None of the Above".
} \\

\midrule

GSM8K     & \makecell[l{p{11cm}}]{
You will be given a question and your goal is to answer it correctly.\\\\\emph{[General Instructions]}
} \\
\bottomrule


\end{tabular}
\caption{ The prompts used to generate model responses for benchmark questions. For all datasets, we used the \textit{General Instructions} (shown at the top) asking the model to solve each question step-by-step and provide its final answer in a specified format. In addition, for each dataset we briefly explained the expected questions format. All prompts were zero-shot; few-shot experiments are reserved for future work.
}
\label{tab:question-prompts}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{c l}
\toprule
Confidence Method  & Prompt \\
\toprule
Verbal 0-100  & \makecell[l{p{11cm}}]{
Now I will rate my confidence in the proposed answer on a scale of 0-100.
Proposed confidence: (
} \\
\midrule

Verbal Binary      & \makecell[l{p{11cm}}]{
Now I will rate my confidence in the proposed answer as either 0 or 1.
Proposed confidence: (
} \\
\bottomrule


\end{tabular}
\caption{ The prompts used to extract the model confidence in its response. As explained in section \ref{sec:appendix-prompting}, these prompts are concatenated as a second step, after the model already answers the question. For the P(True) method, we used the Verbal Binary prompt and looked at the probably the model assigns to the token 1. Importantly, in all the models evaluated in this work, "(0" and "(1" are tokenized as two separate tokens. 
}
\label{tab:confidence-prompts}
\end{table*}