
\section{Qualitative Appendix}
\label{sec:appendix-qualitative}

The qualitative analysis presented in \S\ref{sec:qualitative} involved sampling the reasoning paths using three models: Qwen2.5 3B, Gemma2 9B and Mistral Large (123B). To broaden our evaluated sample pool, we employed a bootstrap process, sampling three distinct traces per question multiple times. Then, we first filtered these samples so that each of them arrived from a different question, and continued with the sampling process described in \S\ref{sec:qualitative}.  

Human evaluators were asked to identify logical patterns in the LLMs' reasoning paths that reduced the evaluators' confidence in the correctness of the LLMs' answers. Importantly, the MMLU dataset requires significant domain knowledge and unspecialized humans achieved only 34.5\% accuracy \cite{hendrycks2020measuring}, compared to a random baseline of 25\%. The MMLU-pro dataset is based on the MMLU dataset, but is considered much harder. This means that our evaluators, which lacked specialized knowledge, could not easily how to solve each question. Instead, we instructed them to focus on identifying low-quality reasoning errors in the responses of the LLMs. This approach aligns with findings from a prior analysis on GPT-4o \cite{wang2024mmlupro}, which attributed 39\% of its errors to reasoning flaws that do not rely on specialized domain knowledge.

Following this review, we aggregated the indicators of low quality into high-level categories. Three main categories encompassed 49\% of the samples. The remaining samples either lacked low-quality indicators (50\%) or had indicators that did not fit into a sizable category (1\%). The different categories and their prevalence are presented in Table \ref{table:patterns}. 

Two of these three categories show a strong association with relative-low confidence scores from the model: (1) The model arrived at solutions not present among the available options, and (2) The model only conducted partial calculations necessary. Interestingly, the pattern where the model explores several plausible solutions without identifying a definitive "correct" one was not specifically associated with either high or low confidence in the model’s reasoning paths, underscoring that not all human-identified patterns significantly influence the model’s assessment. 

Overall, the alignment of human-identified low-quality indicators with low-confidence scores provides another evidence of the ability of LLMs to self-assess and prioritize high confidence solutions. An ability that is leveraged by CISC.


\begin{table*}[!h]
\centering
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{\textbf{Category}} & \multicolumn{1}{c}{\textbf{Definition}} & \multicolumn{1}{c}{\textbf{Low}} & \multicolumn{1}{c}{\textbf{High}} & \multicolumn{1}{c}{\textbf{Snippet}} \\
\midrule
No choice &
\makecell[l{p{5cm}}]{
The model arrives at a solution which is not present in the list of available options. This can include case where a mathematical answer significantly diverging from all options, answers that are only partially correct, or the elimination of all options as part of the reasoning process.
} & 38\% & 13\% & 
\makecell[l{p{5cm}}]{"... After reviewing the options, it's clear that none of 
them perfectly fit the requirements. 
However, the closest correct option is (A), which only has 
a minor error in calculating the remaining inches. 
Proposed answer: (A)"} \\
\midrule
\makecell[l]{Incomplete \\ Calculations} & 
\makecell[l{p{5cm}}]{
The model begins to solve the problem but does not complete the full calculation, often due to the lack of necessary data. 
For example, when attempting to compute acceleration, the absence of mass data prevents an exact and full calculation.
} & 22\% & 2\% & 
\makecell[l{p{5cm}}]{"...**Calculate Heat Flow:** q" = h * (T-surface - T-air) 
**Note:**  Without the actual values for air density, viscosity, 
and thermal conductivity at 68°F, we cannot perform  
the precise calculations. 
Proposed answer: (C)." } \\
\midrule
\makecell[l]{Multiple \\ candidates} & 
\makecell[l{p{5cm}}]{
The model explores several plausible solutions without 
identifying a definitive "correct" one. This occurs when 
the model solves a problem generally, relying on 
estimations rather than concrete data, resulting in a 
range of potential answers.
} & 11\% & 16\% & 
\makecell[l{p{5cm}}]{"... 2.**Identify Buddhist Thinkers:** The options list several 
prominent Buddhist figures from various traditions...
4. **Most Prominent:** The Dalai Lama and Thich Nhat 
Hanh stand out for their consistent emphasis 
on self-sacrifice in their teachings and actions. 
Proposed answer: (I)"} \\
\bottomrule
\end{tabular}
\caption{Human evaluators identified low-quality reasoning indicators in LLM responses (see \S\ref{sec:qualitative}). These indicators were then clustered into three categories, each described above with a definition and an example snippet from an LLM response. The (Low, High) columns show the percentage of LLM responses with low/high self-assessed confidence that exhibited each pattern.  The "No Choice" and "Incomplete Calculation" categories are strongly associated with low confidence.}
\label{table:patterns}
\end{table*}

\begin{table*}[!h]
\centering
\begin{tabular}{ll}
% \textbf{Human Annotator Task Description and Template} \\
\\
\toprule
\\
\makecell[l]{\textbf{General} \\  \textbf{Instructions}}
& \makecell[l{p{12cm}}]{ Evaluate the LLMs' reasoning paths, looking for logical inconsistencies or errors that lower your confidence in their conclusions.  Because the questions are very difficult, even for experts, your task is to identify general reasoning flaws, not to assess the correctness of the final answers themselves. 
Examples: 
\begin{itemize}
    \item Incorrect Assumption: The model assumes something without justification
    \item Missing Step: The model skips a crucial step in the reasoning process
    \item Contradiction: The model states both A and not-A
\end{itemize}
} 
\\
\hline
\\
% \textbf{Question Template} \\
\textbf{Question}
& \texttt{[Pre-filled - The original question given to the LLM]} \\
\\
\hline
\\
\makecell[l]{\textbf{LLM} \\  \textbf{Output}} &
\texttt{[Pre-filled - The LLM output for the given question]} \\
\\
\bottomrule
\\
\end{tabular}
\caption{The input given to human evaluators as part of our qualitative analysis (\S\ref{sec:qualitative}).}
\label{tab:evaluation_template}
\end{table*}
