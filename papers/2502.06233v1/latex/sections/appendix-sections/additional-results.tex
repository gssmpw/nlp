\section{Additional Results}
\label{sec:appendix-more-results}

For each confidence method, Table \ref{table:aggregated-results} shows the macro-average results across all models and datasets. A more detailed version of this table, with a per dataset breakdown, is given at Table \ref{tab:apendix-all-results}.

In addition, Table \ref{table:micro-results} shows micro-averaged aggregated results with confidence intervals, demonstrating the strong statistical significance of our findings. These bootstrap confidence intervals were calculated as follows: (1) For each confidence method, results from all datasets and models were combined into a single dataset of approximately $n \approx 150,000$ rows. (2) 10,000 bootstrap sets were generated by repeatedly sampling $n$ elements with replacement. (3) The procedure described in \ref{sec:bootstrap} was applied to each set, yielding 10,000 estimates of the mean accuracy difference. (4) We used these estimates to calculate the 95\% interval.

Table \ref{table:norm-table-ext} is an extended version of table \ref{table:norm-table}. One important insight that can be derived from the extended table, is that using softmax normalization without temperature scaling is strongly discouraged for CISC.

We also add Figure \ref{fig:methods-graph} featuring additional graphs similar to Figure \ref{fig:first-figure}, but with all the confidence methods. 

Finally, in Table \ref{table:max-ablation}, we include ablations comparing CISC's weighted majority mechanism to more simple methods like selecting the max confidence \cite{wang2024soft} or using the confidence values as a tie-breaker for self-consistency.

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{latex/figures/four_graphs.pdf}
    \caption{Comparison between different confidence extraction methods using Gemma2-9B model and four datasets (\S\ref{sec:datasets}). CISC with P(True) outperforms Self-Consistency and is the best of all the CISC variants.
    }
    \label{fig:methods-graph}
\end{figure*}



\begin{table*}[!h]
\centering
\begin{tabular}{llllrr}
\toprule
& & \multicolumn{2}{c}{Comparable SC Samples} & \multicolumn{2}{c}{Acc Improvement (\%)} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
Dataset & Confidence Method & Budget 5 & Budget 10 & Budget 5 & Budget 10 \\
\midrule
\multirow[t]{4}{*}{MMLU} & Verbal Binary & 18\% \small{(6.1)} & 12\% \small{(11.3)} & 0.4 & 0.2 \\
 & Verbal 1-100 & 25\% \small{(6.7)} & 32\% \small{(14.6)} & 0.9 & 0.7 \\
 & Response Probability & 17\% \small{(6.0)} & 23\% \small{(13.0)} & 0.7 & 0.6 \\
 & P(True) & \textbf{37\% \small{(7.9)}}& \textbf{47\% \small{(18.8)}} & \textbf{1.4} & \textbf{1.0} \\
\cline{1-6}
\multirow[t]{4}{*}{MATH} & Verbal Binary & 18\% \small{(6.1)} & 11\% \small{(11.2)} & 0.8 & 0.5 \\
 & Verbal 1-100 & 17\% \small{(6.0)} & 12\% \small{(11.3)} & 1.3 & 0.6 \\
 & Response Probability & 19\% \small{(6.2)} & 17\% \small{(12.0)} & 2.2 & 1.2 \\
 & P(True) & \textbf{32\% \small{(7.3)}} & \textbf{34\% \small{(15.2)}} & \textbf{3.0} & \textbf{2.0} \\
\cline{1-6}
\multirow[t]{4}{*}{GSM8K} & Verbal Binary & 18\% \small{(6.1)} & 7\% \small{(10.8)} & 0.2 & 0.1 \\
 & Verbal 1-100 & 22\% \small{(6.4)} & 32\% \small{(14.6)} & 0.3 & 0.1 \\
 & Response Probability & 21\% \small{(6.3)} & 33\% \small{(14.9)} & 0.7 & 0.5 \\
 & P(True) & \textbf{43\% \small{(8.8)}} & \textbf{53\% \small{(21.2)}} & \textbf{0.9} & \textbf{0.6} \\
\cline{1-6}
\multirow[t]{4}{*}{BBH} & Verbal Binary & 17\% \small{(6.0)} & 10\% \small{(11.1)} & 0.2 & 0.1 \\
 & Verbal 1-100 & 22\% \small{(6.4)} & 41\% \small{(17.0)} & 0.5 & 0.4 \\
 & Response Probability & 32\% \small{(7.3)} & 45\% \small{(18.3)} & 0.7 & 0.8 \\
 & P(True) & \textbf{48\% \small{(9.7)}} & \textbf{47\% \small{(19.0)}} & \textbf{1.0} & \textbf{0.9}\\
\bottomrule
\end{tabular}
\caption{ Aggregated results across all models for each dataset and confidence extraction method.  All methods demonstrate better performance than standard self-consistency, with the P-True method achieving the best results and leading to an computational cost reduction of up to 53\% }
\label{tab:apendix-all-results}
\end{table*}


\begin{table*}[!ht]
\centering
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{Acc Improvement} \\
\cmidrule(lr){2-3} \cmidrule(l){4-5} 
Confidence Method & Budget 5 & Budget 10 \\
\midrule
Verbal Binary & 0.35\ \small{(0.34-0.37)} & 0.20\ \small{(0.18-0.21)} \\[4pt]
Verbal 1-100 & 0.68\ \small{(0.64-0.72)} & 0.46\ \small{(0.40-0.51)} \\[4pt]
Response Probability & 0.88\ \small{(0.84-0.92)} & 0.69\ \small{(0.63-0.74)} \\[4pt]
P(True) & \textbf{1.38\ \small{(1.32-1.43)}} & \textbf{1.03\ \small{(0.96-1.10)}} \\[4pt]

\bottomrule
\end{tabular}
\caption{\textbf{Micro-averaged Aggregated Results. } This table presents the micro-averaged aggregated results with confidence intervals for each confidence method. Each confidence method demonstrates statistically significant improvements over self-consistency, and \textbf{P(True)} method exhibits significant superiority over other methods. This detailed view supplements the macro-average results shown in Table \ref{table:aggregated-results} and provides statistical verification of the efficiency gains and accuracy improvements attributed to CISC methods. }
\label{table:micro-results}
\end{table*}

\begin{table*}[!h]
\centering
\begin{tabular}{llllll}
\toprule
 & \multicolumn{2}{l}{\% Cost Reduction} & \multicolumn{3}{l}{\% Acc Improvement} \\
 & 5 & 10 & 5 & 10 & 30 \\
Confidence Method &  &  &  &  &  \\
\midrule
P(True) - No Normalization & 29\% \small{(7.0)} & 32\% \small{(14.8)} & 1.4 & 0.8 & 0.4 \\
P(True) - Softmax T=1 & 27\% \small{(6.8)} & 30\% \small{(14.2)} & 1.3 & 0.8 & 0.3 \\
P(True) - Softmax T=Tuned & \textbf{41\% \small{(8.4)}} & \textbf{46\% \small{(18.6)}} & \textbf{1.6} & \textbf{1.1} & \textbf{0.9} \\
\midrule
Sequence Probability - No Normalization & 21\% \small{(6.3)} & 24\% \small{(13.1)} & 1.1 & 0.6 & 0.3 \\
Sequence Probability - Softmax T=1 & 20\% \small{(6.3)} & 23\% \small{(13.0)} & 1.1 & 0.6 & 0.2 \\
Sequence Probability - Softmax T=Tuned & \textbf{22\% \small{(6.5)}} & \textbf{31\% \small{(14.6)}} & \textbf{1.1} & \textbf{0.8} & \textbf{0.7} \\
\midrule
Verbal 0 - 100 - No Normalization & 20\% \small{(6.3)} & 20\% \small{(12.5)} & 0.7 & 0.4 & 0.1 \\
Verbal 0 - 100 - Softmax T=1 & 12\% \small{(5.7)} & -1\% \small{(9.9)} & -0.3 & -1.4 & -2.6 \\
Verbal 0 - 100 - Softmax T=Tuned & \textbf{22\% \small{(6.4)}} & \textbf{30\% \small{(14.4)}} & \textbf{0.8} & \textbf{0.4} & \textbf{0.3} \\
\bottomrule
\end{tabular}
\caption{\textbf{Normalization Ablation. } This table extends Table \ref{table:norm-table}, showing that temperature-scaled softmax is optimal for all methods, and that softmax should be avoided without temperature scaling. }
\label{table:norm-table-ext}
\end{table*}

\begin{table*}[!ht]
\centering
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{Comparable SC Samples} & \multicolumn{2}{c}{Acc Improvement (\%)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Confidence Method & Budget 5 & Budget 10 & Budget 5 & Budget 10 \\
\midrule
Max & -11\% \small{(4.5)} & -84\% \small{(5.4)} & -1.9 & -4.5 \\[4pt]
Tie & 27\% \small{(6.8)} & 28\% \small{(13.9)} & 1.3 & 0.7 \\[4pt]
CISC & \textbf{41\% \small{(8.4)}} & \textbf{46\% \small{(18.6)}} & \textbf{1.6} & \textbf{1.1} \\[4pt]

\bottomrule
\end{tabular}
\caption{\textbf{Simplified ablation. } Here we compare CISC with two simplified ablations:  (Max) Which selects the answer with highest confidence score, and (Tie) Only uses CISC if self-consistency resulted in a tie. All methods are calculated using the P(True) confidence. Results are aggregated across all models and datasets. CISC significantly outperforms both ablations, and the Max method even degenerates performance.}
\label{table:max-ablation}
\end{table*}

\begin{table*}[!h]
\centering
\begin{tabular}{lcccc}
\toprule
Dataset & BBH & GSM8K & MATH & MMLU \\
Model &  &  &  &  \\
\midrule
Gemma 27b & 57.1 & 66.1 & 62.9 & 59.9 \\
Gemma 2b & 55.8 & 66.2 & 64.3 & 53.6 \\
Gemma 9b & 55.3 & 68.3 & 71.8 & 58.9 \\
Mistral 123 & 56.2 & 66.1 & 61.2 & 63.4 \\
Mistral 22 & 64.1 & 81.4 & 74.9 & 67.7 \\
Mistral 8 & 59.4 & 71.8 & 62.9 & 58.8 \\
Qwen 14b & 58.9 & 65.5 & 59.0 & 60.2 \\
Qwen 3b & 56.3 & 61.9 & 57.5 & 56.0 \\
Qwen 72b & 53.5 & 62.4 & 63.6 & 58.8 \\
\bottomrule
\end{tabular}
\label{table:wqd-breakdown}
\caption{\textbf{Within-Question-Discrimination Breakdown.} This table presents a breakdown of the aggregated Within-Question-Discrimination (WQD) results presented in Table \ref{tab:confidence-methods}, using the P(True) method.  In all cases, WQD scores exceed the 50\% chance level. 
}
\end{table*}