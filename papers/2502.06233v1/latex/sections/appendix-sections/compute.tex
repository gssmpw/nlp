\section{Compute}
\label{sec:appendix-compute}

For each model (\S\ref{sec:models}), we generated approximately 500,000 responses - 17,000 questions (\S\ref{sec:datasets}), with 30 samples (\S\ref{sec:bootstrap}). As a reference, inference with Gemma2-2-Billion (1K token context length) required an order of 100 Nvidia H100 GPU hours. 