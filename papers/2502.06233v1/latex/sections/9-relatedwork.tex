\section{Related Work}

\paragraph{Confidence signals for LLMs.} 
There is a long line of work on deriving confidence measures from LLMs. Popular approaches use
% Popular methods to derive calibrated confidence from LLMs include 
the agreement across multiple samples \cite{kuhn2023semantic, manakul2023selfcheckgpt, tian2023fine,lyu2024calibrating}, the model's internal representations \cite{azaria2023internal, burns2022discovering} or directly prompting the model to verbalize its confidence \cite{tian2023just, kadavath2022language}.
All papers in this line of work focused on fact-seeking tasks, 
so confidence is typically derived based on the final answer alone. To the best of our knowledge, our work is the first to apply these approaches to scoring the entire reasoning path.

\paragraph{Reasoning verification.}
While learned verifiers have been demonstrated to significantly improve performance on math word problems \cite{cobbe2021training, lightman2023let, li2022making}, the ability of LLMs to perform \emph{self}-verification and \emph{self}-correction is still heavily contested, with some works providing positive evidence for such capabilities \cite{weng2022large, gero2023self, madaan2024self, liu2024large, li2024confidence} and others arguing that the gains can mostly be attributed to clever prompt design, unfair baselines, data contamination and using overly simple tasks \cite{tyen2023llms, valmeekam2023can, hong2023closer, huang2023large, stechly2024self, zhang2024small}. This work contributes to this ongoing discussion by presenting multiple lines of evidence supporting LLM self-verification. In particular, we demonstrate clear benefits from a simple confidence-based self-verification approach. 


\paragraph{Improving self-consistency's efficiency. }

Numerous attempts \cite{chen-etal-2024-self-para} have been made to reduce SC computational overhead while maintaining quality. However, none have matched the widespread adoption of self-consistency. This can be largely attributed to several limitations: (1) a trade-off where throughput is reduced while latency increases, for example by sampling chains sequentially until reaching a certain condition \cite{li2024escape} or running expensive LLM calls instead of the cheap majority voting \cite{yoran2023answering}, (2) the need for manual feature crafting and tuning tailored to each dataset \cite{wan2024dynamic}, (3) promising results on specialized setups \cite{wang2024soft} which did not generalize to standard benchmarks (Table \ref{table:max-ablation}), and (4) as highlighted by \citet{huang2023large}, many of the more sophisticated methods that appear promising actually don't outperform self-consistency when evaluated with a thorough analysis of inference costs. Our approach is different in that CISC adds minimal complexity to self-consistency, and improves throughput without compromising latency.

\paragraph{Self-consistency with confidence.}
Related approaches to CISC's confidence-weighted majority vote were previously explored in both the original self-consistency paper \citet{wang2022self}, that considered a weighted majority using Sequence Probability (\S\ref{sec:metrics}), and in \citet{miao2023selfcheck}, that concluded that verbally \nl{asking the LLM to check its own reasoning is largely ineffective} for improving self-consistency. In both cases, these failures are attributed to the confidence scores being too similar to one another. Our work shows that despite this, the scores contain a useful signal (reflected in the WQD scores; Table \ref{tab:confidence-methods}) that can be utilized by a normalization step prior to aggregation to significantly improve the efficiency of self-consistency. Furthermore, the P(True) method, which achieves the highest WQD scores, has not been previously used for attempting to improve self-consistency.


