\section{Within-Question Confidence Evaluation}

\label{sec:pairwise}


\definecolor{lightgraycisc}{gray}{0.97}

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l c c c >{\columncolor{lightgraycisc}}c}
\toprule
\makecell{Confidence \\ Method} & ECE-t $\downarrow$ & Brier-t $\downarrow$ & \makecell{WQD $\uparrow$} & \makecell{CISC Cost\\ Reduction $\uparrow$} \\
\midrule
Verbal Binary & \textbf{0.005} & 0.187 & 52.2\%  & 10\% \\[4pt]
Verbal 0-100 & 0.046 & \textbf{0.173} & 56.1\%   & 30\% \\[4pt]
Response Prob. & 0.090 & 0.192 & 59.0\%    & 31\% \\[4pt]
P(True) & 0.030 & 0.182 & \textbf{62.3\%}        & \textbf{46\%} \\[4pt]
\bottomrule
\end{tabular}
}
\caption{\textbf{Comparison of different confidence extraction methods in terms of between-question and within-question confidence evaluation metrics}. 
We see that between-question metrics (ECE-t, Brier-t) are poor indicators of effective confidence extraction for CISC, while our novel WQD metric (\ref{def:wqd}) effectively predicts which confidence extraction method yields the best CISC performance.
}
\label{tab:confidence-methods}
\end{table}


Recent work demonstrated that verbal confidence methods significantly outperform P(True) in terms of \emph{calibration} \cite{tyen2023llms}, which is the de-facto approach to evaluate the quality of confidence measures. Yet, perhaps surprisingly, CISC is more effective with P(True) than with verbal confidence methods (Table \ref{table:aggregated-results}). In this section we settle these differences, and explain why well-calibrated confidence measures can still be less useful for CISC.

We argue that existing evaluation metrics, whether \emph{calibration} based \cite{kadavath2022language, tian2023just} or \emph{discrimination} based \cite{kuhn2023semantic, nguyen2024direct} examine the confidence behavior \emph{between} the input questions. However, for CISC to work well, we want the confidence scores to be able to distinguish correct and incorrect responses \emph{to the same question}. 

To gain an intuition for the difference between \emph{within-question}
and \emph{between-question} confidence evaluation, consider the following simple example. Imagine a model $M$ and a dataset with two types of questions: questions that $M$ finds \nl{easy} (e.g., answers correctly 95\% of the time) and questions that $M$ finds \nl{hard} (e.g., answers correctly 5\% of the time). Consider a confidence measure that assigns every answer to an \nl{easy} question a confidence of $0.95$ and every answer to a hard question a confidence of $0.05$. This confidence signal is useless for CISC, as it does not make any distinctions between answers to the same question. On the other hand, it scores well under existing metrics (e.g., it is perfectly calibrated).

The above thought experiment shows that the fact that well-calibrated confidence scores can be derived from a model does not necessarily imply the model possesses a capacity to self-assess its own responses. To isolate this specific ability, we design a metric that measures whether the confidence scores can distinguish correct and incorrect responses to the same question:

\begin{definition}[Within-Question Discrimination]
\label{def:wqd}
Given a dataset of questions, for each question $q$, denote the sampled responses by $R_q = \{(\textbf{r}_i, \textbf{a}_i)\}_{i=1}^m$, and let $R^+_q,R^-_q \subseteq R_q$ be the subsets of correct and incorrect responses respectively. We evaluate the Within-Question Discrimination (WQD) of a confidence method $c: (r,a) \mapsto \R$ as:
\begin{multline*}
    \text{WQD}(c) \equiv \\ 
    \frac{1}{N} \cdot \sum_{q} \sum_{(r, a)\in R^+_q} \sum_{(r', a')\in R^-_q} [c(r,a) > c(r',a')]
\end{multline*}
where $N = \sum_q |R_q^+| \cdot |R_q^-|$.
\end{definition}

That is, we compute the fraction of cases where the higher confidence response is indeed the correct response, out of pairs of responses \emph{to the same question} (exactly one of which is correct). In our work, we use $m = 30$ (as described in \S\ref{sec:bootstrap}).

To emphasize the importance of \emph{within-question evaluation}, we test if WQD is more predictive of CISC's success than standard \emph{between-question} confidence metrics. We compare each confidence method from \S\ref{sec:conf-methods} in terms of: (i) standard metrics, such as ECE \cite{pmlr-v70-guo17a} and Brier Score \cite{brier1950verification}, (ii) WQD, (iii) CISC performance at a budget of 10 samples. We follow previous work \cite{tyen2023llms} and report the standard metrics after applying temperature scaling \cite{ovadia2019can}, a technique that fits a single temperature parameter $T$ to the model's confidences to minimize the negative log-likelihood on the data. We use ECE-t and Brier-t to denote the scaled scores.

The results of this comparison, averaged across all datasets (\S\ref{sec:datasets}) and models (\S\ref{sec:models}), are summarized in Table \ref{tab:confidence-methods}.  
Indeed, we see that the verbal confidence methods obtain the best ECE-t and Brier-t scores while also achieving the worst performance in CISC. On the other hand, the WQD metric is able to perfectly predict the relative scores of each confidence method in CISC. This emphasizes the limitations of relying solely on traditional confidence evaluation methods for evaluating the models ability to self-assess its reasoning.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{latex/figures/pairwise-heatmap-small.pdf}
    \caption{Within-Question Discrimination score (indicated by color) increases smoothly as a function of the confidence gap (percentiles, x-axis). Here we use the P(True) method, Gemma2-9B and the MATH dataset. 
    } %\gal{Update metric name}
\label{fig:pairwise-heatmap}
\end{figure}

The WQD metric prioritizes interpretability, focusing on the discrimination ability of the confidence scores irrespective of the relative magnitude of the confidence values $c(r,a)$ and $c(r', a')$. However, examining the relationship between WQD and the confidence gap $|c(r,a) - c(r',a')|$ offers additional insights. Figure \ref{fig:pairwise-heatmap} illustrates a near monotonic relationship: the within-question discrimination ability (indicated by color) smoothly increases with the confidence gap (x-axis).
These findings suggest a fine-grained self-assessment mechanism, where even small differences in confidence scores reflect significant variations in the probability of a response being correct

Taken together, our findings provide a compelling evidence that LLMs indeed posses an intrinsic ability to reassess their own responses.
