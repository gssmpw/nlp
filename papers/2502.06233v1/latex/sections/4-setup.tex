\section{Experimental Setup}
\label{sec:setup}

We compare CISC and self-consistency across a range of confidence extraction methods (\S\ref{sec:conf-methods}), reasoning tasks (\S\ref{sec:datasets}) and models (\S\ref{sec:models}).

\subsection{Confidence Extraction Methods}
\label{sec:conf-methods}

We use the following methods: 

\begin{itemize}[itemsep=1pt, topsep=2pt,leftmargin=*]
\item \textbf{Response Probability} \cite{wang2022self}: The confidence in a response $(\textbf{r}, \textbf{a})$  is taken to be the model's (length-normalized) probability of generating $(\textbf{r}, \textbf{a})=(x_1, \dots, x_n)$ given the question:$$p_\theta(\textbf{r}, \textbf{a}) = \left[\Pi_{i=1}^n p_\theta (x_i \vert x_1\dots x_{i-1}, q)\right]^\frac{1}{n}$$

\item \textbf{Verbal Confidence} \cite{lin2022teaching}: After sampling $(\textbf{r},\textbf{a})$ from the model, we prompt it to rate its confidence in its previously generated output. We implement two variants: (1) \textbf{Verbal Binary} instructs the model to output either 0 or 1, and (2) \textbf{Verbal 0-100} instructs the model to output a score on a scale of 0-100.

\item \textbf{P(True)} \citet{kadavath2022language}: We prompt the model to rate its confidence in $(\textbf{r},\textbf{a})$ in binary format (either 0 or 1), and compute the probability that the model assigns to the token $1$.%. 

\end{itemize}


\paragraph{Efficient and Consistent Confidence Prompting.}

Our implementation of the prompt-based methods employs a \emph{two-step} prompting procedure (as depicted in Figure \ref{fig:high-level}).
Given a question prompt $q$, we first use the model to generate the reasoning chain and answer $(r,a)$. We then concatenate a confidence extraction prompt $e$ (e.g., \nl{Now I will rate my confidence...}), and continue the generation on $(q,r,a,e)$. This serves two important purposes. First, it ensures that when comparing self-consistency and CISC, the reasoning chains are identical. Second, the fact that the prefix $(q,r,a)$ remains unchanged after concatenating the confidence extraction prompt $e$ means it does not require reprocessing by the LLM. Consequently, the additional cost of the confidence extraction step consists only of encoding $\text{len}(e)\approx 20$ tokens and generating a single token. Since a single $(q,r,a)$ typically contains hundreds of tokens, the confidence extraction step adds only a negligible computational overhead to self-consistency. Further overhead reduction can be achieved through prompt optimization or by using the single-step procedure described in Appendix \ref{sec:appendix-prompting}. The precise prompts used and additional technical details are also provided in Appendix \ref{sec:appendix-prompting}.

\subsection{Datasets}
\label{sec:datasets}

We used four large reasoning benchmarks:\footnote{Other than the popular GSM8K, the other datasets were chosen as the three largest datasets in the Hugging Face Leaderboard \cite{leaderboard} (as of December 1st, 2024).}

\begin{itemize}[itemsep=1pt, topsep=2pt,leftmargin=*]
\item  \textbf{GSM8K} \cite{cobbe2021gsm8k}: A dataset of grade-school level math word problems. We evaluate on the entire validation set (1320 questions). %All the answers are integers.
\item \textbf{MATH} \cite{hendrycksmath2021}: A more challenging dataset of math word problems. We used the entire test set (5K questions). %Answers are given in TeX equation format, and we used the code snippet given in \cite{lewkowycz2022solving} to parse them.
\item \textbf{MMLU-Pro} \cite{wang2024mmlu}: A more challenging version of the Multitask Language Understanding (MMLU)  benchmark, testing language models' general knowledge and reasoning abilities with a wide range of topics such as science and history. We randomly sampled 5K questions.
\item \textbf{Big-Bench-Hard} \cite{suzgun2022challenging}: A challenging selection of tasks from the big-bench benchmark \cite{srivastava2023beyond}, comprises a variety of reasoning tasks that pose challenges to LLMs, such as counting objects. We selected 20 out of 23 tasks (5,761 examples), eliminating three sub-tasks that required designated answer extraction methods.
\end{itemize}

\subsection{Models}
\label{sec:models}

We use nine instruction-tuned open-weights LLMs from 3 different families:

\begin{itemize}[itemsep=1pt, topsep=2pt,leftmargin=*]
\item \textbf{GEMMA2} \cite{team2024gemma}: A Google AI model family, including 2B, 9B, and 27B parameter models. 
\item \textbf{QWEN2.5} \cite{yang2024qwen2}: A model family from Alibaba AI, with 7 models ranging from 0.5B to 72B parameters. We selected three models: 3B, 14B, and 72B.
\item \textbf{Mistral} \cite{mistral}: We used three of the latest models available - Ministral-8B-Instruct-2410, Mistral-Small-Instruct-2409, mistralai/Mistral-Large-Instruct-2411 - with 8B, 22B, 123B parameters respectively.
\end{itemize}

\subsection{Metrics}
\label{sec:metrics}

We compare CISC against self-consistency using the following metrics:

\begin{itemize}[itemsep=1pt, topsep=2pt,leftmargin=*]

\item \textbf{\% Cost Reduction}: The percentage of computational cost saved by using CISC. We fix the compute budget for CISC (5 or 10 model responses) and measure the number of responses\footnote{If self-consistency failed to reach CISC's accuracy using up to 30 responses, we use a maximal value of 31 for this metric.} required for self-consistency to achieve equivalent accuracy:
$$100 \times \left(1 - \frac{\text{CISC budget}}{\text{\# Comparable SC responses}}\right)$$

\item \textbf{\% Accuracy Improvement}: The relative accuracy gain of CISC over self-consistency when both methods utilize the same number of responses per question: 
$$100 \times \left(\frac{\text{CISC Acc}}{\text{SC Acc}} - 1\right)$$
\end{itemize}

\subsection{Temperature Scaling}
\label{sec:temperature}

As discussed in \S\ref{sec:cisc}, CISC re-scales the confidence values using a softmax transformation, parameterized by a temperature $T > 0$. We tune the temperature separately for each model and confidence extraction method using a 10\% held-out set, aggregated across all four datasets (\S\ref{sec:datasets}). More details and the optimal temperature values for each configuration are in appendix \ref{sec:appendix-temperature}.

\subsection{Bootstrap}
\label{sec:bootstrap}

To compute the performance of a decoding strategy $s$ (either self-consistency or a variant of CISC) with a sample budget of $b \in [1,...,30]$, we perform bootstrap sampling. We first sample $30$ different reasoning paths from the model. 
Next, we draw $n=500$ sets of $b$ paths for each question, apply $s$ to each set, and compute the accuracy per set. We then average the results across all bootstrap samples to obtain the final score.
