\section{Qualitative Analysis}
\label{sec:qualitative}


In \S\ref{sec:results} we showed that CISC has clear 
performance advantages over standard self-consistency, and argued that this suggests LLMs are capable of self-assessing their confidence in responses to the same question. To facilitate a better understanding of this phenomenon, we asked human evaluators to identify indicators of \emph{low-quality model responses} (i.e., logical patterns that reduced the evaluators' confidence in the correctness of the LLM response). Our analysis revealed a strong correlation between the prevalence of these indicators and lower confidence scores assigned by the LLM.

\paragraph{Sampling Process.} We performed the analysis on MMLU-Pro (\S\ref{sec:datasets}), using three representative models, one from each model family.  

To reduce the evaluation burden we limited it to three LLM responses per question. We selected these triplets based on two criteria: (1) CISC and SC produced different results, where one method yielded a correct answer and the other did not, and (2) the final answers of the three responses were not all distinct, which would otherwise degenerate self-consistency's majority voting. 

Out of the remaining triplets, we randomly chose 45 for which SC was correct and 45 where SC was wrong. Then, for each triplet, we randomly took either the response with highest relative-confidence or the response with lowest relative-confidence. This ensured an equal number of low relative-confidence responses that were correct and incorrect, mitigating potential bias of answer correctness on our analysis. The process resulted in 90 responses for human evaluation.

\paragraph{Human Evaluation.} Two human evaluators (NLP Phd students), unaware of both the model's confidence scores and the ground truth labels, reviewed 90 samples. The evaluators' task was to identify logical patterns in the LLM reasoning-chain which reduce their confidence that the LLM has reached a correct answer; we call these patterns low-quality-indicators. Also, the evaluators were asked to briefly describe each identified pattern.

\paragraph{Results.} Our evaluation demonstrated a significant correlation in confidence assessments: 67\% of the samples assessed as relative-low confidence by the model were also judged to contain low-quality indicators by human evaluators, while only 33\% of the samples assessed as relative-high confidence by the model contained the human identified low-quality-indicators. This strong correlation suggests that LLMs are adept at assessing their own reasoning processes and identifying patterns that humans consider indicative of low quality. 

In addition, we categorized these low-quality indicators. Three primary categories emerged: (1) the LLM's final answer was not among the provided options; (2) the LLM deliberated between multiple options; and (3) the LLM omitted necessary calculations. Of these, only categories (1) and (3) showed a strong correlation with the LLM's low-confidence scores. Further details regarding these categories and their correlation statistics are available in the Appendix \ref{sec:appendix-qualitative}.