\section{Discussion}

In this work we introduced CISC, a lightweight extension of self-consistency. Across diverse models, datasets, and confidence extraction methods, CISC consistently outperformed self-consistency, reducing computation costs by over 40\% on average.

The performance gains achieved by using model-derived confidence scores provide a practical evidence that LLMs can effectively judge the quality of their own outputs, contributing to the ongoing debate on this topic \cite{huang2023large, li2024confidence}. This is further strengthened by our qualitative evaluation, revealing significant agreement between model confidence and human assessments of response quality.
% Specifically, responses identified by the models as low-confidence were also significantly more likely to be flagged by human evaluators as exhibiting signs of flawed reasoning.

Complementing our investigation of LLM self-assessment, we address the crucial aspect of evaluating confidence methods.  Traditional calibration metrics, which assess confidence across different questions, fail to capture a model's ability to distinguish between high and low quality responses to the same question. To overcome this, we introduce the Within-Question Discrimination (WQD) metric and demonstrate its effectiveness.

We encourage future research to explore the integration of model self-confidence into more sophisticated reasoning frameworks like Tree of Thoughts \cite{yao2024tree} or Graph of Thoughts \cite{besta2024graph}, believing that harnessing this inherent capability can further boost performance. Another promising avenue is training models to produce more accurate intrinsic or verbal confidence \cite{lin2022teaching, chaudhry2024finetuning}, which would directly improve CISC and related methods. Conversely, CISC and WQD can be used to assess the impact of advancements in confidence generation.