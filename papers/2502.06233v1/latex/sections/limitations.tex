\clearpage
\newpage
\section{Limitations}

\paragraph{Confidence Prompting. } Our confidence extraction prompting approach minimizes the computational overhead (\S\ref{sec:conf-methods}) by using short confidence prompts (less than 5\% of the input and reasoning chain length) that, unlike other works, are appended after the reasoning chain. This allows us to continue to use the auto-regressive cache that was used when the models generated the answer. While this approach is readily implementable within frameworks like HuggingFace \cite{hf}, it may not be universally supported. An alternative one-step prompting approach, which does not rely on prefix caching, is discussed in Appendix \ref{sec:appendix-prompting}.  We opted for the two-step approach in this study to ensure a clear and robust evaluation of CISC, fully mitigating the impact of confidence integration on the generated reasoning paths.

\paragraph{Access to the model's probabilities. } The preferred CISC approach calculates P(True) (as described in \S\ref{sec:conf-methods}) by examining the model's assigned probability to the verbal confidence token.  This method is available in both popular open-weight frameworks (e.g., \citet{hf}) and closed-weight frameworks (e.g., \citet{openaiapi}).  However, this feature may not be universally available across all frameworks.

\paragraph{Human Evaluation. }  The qualitative human evaluation presented in Section \ref{sec:qualitative} provides further support for our claims regarding LLMs' ability to self-assess the correctness of their responses. This evaluation was conducted on the MMLU dataset, which offers a diverse set of single-choice questions.  Extending this analysis to other datasets could offer additional insights.

\paragraph{Additional ablations. } We examined the performance of CISC across several key aspects, focusing on the impact of the choice of confidence extraction method and the impact of the confidence normalization step. Additional ablations could include examining the effect of zero-shot vs few-shot prompting, different choices of normalization techniques, and using trainable confidence methods \cite{lin2022teaching, chaudhry2024finetuning} to improve the performance of CISC.

%\paragraph{Training for Better Confidence. } Recent research has shown that it is possible to train LLMs to express their confidence more accurately \cite{lin2022teaching, chaudhry2024finetuning}. We believe that such techniques could further enhance CISC performance, potentially even making the straightforward verbal confidence methods better than P(True) (\S\ref{sec:conf-methods}) for CISC. However, these training strategies are beyond the scope of the current work.


%\paragraph{Normalization. }  Confidence score normalization plays a crucial role in CISC performance.  As demonstrated in Table \ref{table:norm-table}, CISC performance degrades without normalization.  We employ a standard temperature-scaled softmax for normalization (\ref{sec:temperature}). While effective, we did not explore other potential normalization techniques, which could offer further performance improvements.  This remains an open avenue for future research.

%\paragraph{Zero-shot. } This work explores CISC exclusively in the zero-shot setting.  However, we believe the interplay between few-shot learning and confidence extraction presents a promising avenue for future research.  Specifically, investigating how providing a few examples of question-answer pairs, each with an associated confidence score, influences the LLM's ability to estimate confidence, and how these few-shot examples can be effectively integrated with CISC.

\section{Ethics Statement}

This work improves LLM reasoning efficiency by introducing a new decoding strategy (CISC). While CISC itself introduces no new ethical issues, LLMs can perpetuate biases and have societal impacts. Responsible LLM development and deployment, including bias mitigation, are crucial. 