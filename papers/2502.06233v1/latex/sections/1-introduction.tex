\section{Introduction}
\label{sec:intro}

Modern large language models (LLMs) demonstrate strong reasoning capabilities \cite{bubeck2023sparks, guo2025deepseek}, 
driven in part by their capacity to generate a sequence of intermediate reasoning steps that lead them toward a final answer
\cite{wei2022chain, jaech2024openai}. 
Self-consistency \cite{wang2022self} is a popular decoding strategy that further improves LLMs' reasoning performance by sampling a diverse set of reasoning paths and selecting the most frequent answer as the final output. Despite its effectiveness, this approach is also computationally expensive, as it requires generating a large number of (long) reasoning paths to increase the chances that the correct answer emerges as the most frequent one.

Motivated by recent evidence that LLMs possess the ability to judge the correctness of their own outputs \cite{kadavath2022language, zhang2024small}, we hypothesize that self-consistency could be made significantly more efficient if the model could \emph{review} each generated reasoning path before selecting a final answer. We therefore introduce \textbf{Confidence-Informed Self-Consistency} (CISC), a lightweight extension of self-consistency. As illustrated in Figure \ref{fig:high-level}, CISC uses the model to generate a self-assessment score for each path and employs these scores in a weighted majority vote.

We conducted a comprehensive comparison of CISC and self-consistency, spanning nine LLMs of various sizes, four datasets covering a wide range of mathematical and commonsense reasoning tasks, and three popular methods for deriving self-assessment confidence scores from the model.
Our results demonstrate that CISC outperforms self-consistency in virtually all the examined configurations. Using the best-performing confidence estimation method, CISC achieves comparable performance to self-consistency while reducing the required number of reasoning paths by over 40\% on average (See Figure \ref{fig:first-figure} for an example).


Surprisingly, the most calibrated confidence method is actually the least useful for CISC. We offer a potential explanation:
existing confidence evaluation metrics measure the usefulness of confidence scores for comparing answers across different questions, while CISC requires distinguishing correct and incorrect answers for the same question.
To address this, we propose the Within-Question Discrimination (WQD) metric that specifically measures this ability, and demonstrate that it can predict the relative performance of CISC with different confidence methods.

Finally, we conduct a qualitative-analysis and find a significant agreement between model confidence scores and human assessments of the reasoning-paths' quality.  Specifically, responses identified by the model as low-confidence were also significantly more likely to be flagged by human evaluators as exhibiting signs of low-quality reasoning patterns.

To summarize, we contribute practical methods and foundational insights:
\begin{itemize}
    \item We propose CISC, a decoding strategy that can be used as a drop-in replacement to self-consistency, achieving comparable accuracy at a significantly lower computational cost.
    \item We introduce the concept of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question.
    \item We present empirical evidence supporting the idea that LLMs are capable of self-assessing their responses, contributing to the ongoing debate regarding this capability \cite{gero2023self, huang2023large, li2024confidence, stechly2024self}
\end{itemize}

