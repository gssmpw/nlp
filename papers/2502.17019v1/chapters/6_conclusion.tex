\section{Conclusion}
We present Erwin, a hierarchical transformer that uses ball tree partitioning to process large-scale physical systems with linear complexity. Erwin achieves state-of-the-art performance on both the cosmology benchmark \cite{Balla2024ACB} and the EAGLE dataset \cite{Janny2023EagleLL}, demonstrating its effectiveness across diverse physical domains.
The efficiency of Erwin makes it a suitable candidate for any tasks that require modeling large particle systems, such as tasks in computational chemistry \cite{Fu2023MOFDiffCD} or diffusion-based molecular dynamics \cite{Jing2024GenerativeMO}.


\vspace{-5pt}
\paragraph{Limitations and Future Work}
Because Erwin relies on perfect binary trees, we need to pad the input set with virtual nodes, which induces computational overhead for ball attention computed over non-coarsened trees (first ErwinBlock). This issue can be circumvented by employing learnable pooling to the next level of the ball tree, which is always full, ensuring the remaining tree is perfect. Whether we can perform such pooling without sacrificing expressivity is a question that we leave to future research.

Erwin was developed by jointly optimizing for expressivity and runtime. As a result, certain architectural decisions are not optimal with respect to memory usage. In particular, we use a distance-based attention bias (see Eq.~\ref{eq:bias}), for which both computational and  memory requirements grow quadratically with the ball size. Developing alternative ways of introducing geometric information into attention computation could reduce these requirements.
Finally, Erwin is neither permutation nor rotation equivariant, although rotation equivariance can be incorporated without compromising scalability. One possible approach is to use geometric algebra transformers \cite{Brehmer2023GeometricAT} and omit the proposed cross-ball connections, as they rely on invariance-breaking tree building. %






