\section{Introduction}

Scientific deep learning is tackling increasingly computationally intensive tasks, following the trajectory of computer vision and natural language processing. Applications range from molecular dynamics (MD) \cite{Arts2023TwoFO}, computational particle mechanics \cite{alkin2024neuraldemrealtimesimulation}, to weather forecasting \cite{bodnar2024aurora}, where simulations often involve data defined on irregular grids with thousands to millions of nodes, depending on the required resolution and complexity of the system.
\vspace{+5pt}

Such large-scale systems pose a significant challenge to existing methods that were developed and validated at smaller scales. For example, in computational chemistry, models are typically trained on molecules with tens of atoms \cite{Kovacs2023MACEOFF23TM}, while molecular dynamics simulations can involve well beyond thousands of atoms. This scale disparity might result in prohibitive runtimes that render models inapplicable in high-throughput scenarios such as protein design \cite{Watson2023DeND} or screening \cite{Fu2022SimulateTC}.

A key challenge in scaling to larger system sizes is that computational methods which work well at small scales break down at larger scales.
For small systems, all pairwise interactions can be computed explicitly, allowing deep learning models to focus on properties like equivariance \cite{Cohen2016GroupEC}. However, this brute-force approach becomes intractable as the system size grows. At larger scales, approximations are required to efficiently capture both long-range effects from slowly decaying potentials or multi-scale coupling \cite{Majumdar2020MultiscaleFO}. As a result, models validated only on small systems often lack the architectural components necessary for efficient scaling. %

This problem has been extensively studied in computational many-body physics \cite{Hockney1966ComputerSU}, where the need for evaluating long-range potentials for large-scale particle systems led to the development of sub-quadratic tree-based algorithms \cite{1986Natur.324..446B, doi:10.1137/0909044}. These methods are based on the intuition that distant particles can be approximated through their mean field effect rather than individual interactions \cite{Pfalzner1996ManybodyTM}. The computation then is structured using hierarchical trees to efficiently organize the computation at multiple scales. While highly popular for numerical simulations, these tree-based methods have seen limited adoption in deep learning due to poor synergy with GPU architectures.

Transformers \cite{Vaswani2017AttentionIA}, on the other hand, employ the highly optimized attention mechanism, which comes with the quadratic cost of computing all-to-all interactions. In this work, we combine the efficiency of hierarchical tree methods with the expressivity of attention to create a scalable architecture for the processing of large-scale particle systems. Our approach leverages ball trees to organize computation at multiple scales, enabling both local accuracy and global feature capture while maintaining \emph{linear} complexity in the number of nodes. 

The main contributions of the work are the following:
\begin{itemize}[leftmargin=20pt, topsep=-1pt, itemsep=-1pt]
    \item We introduce ball tree partitioning for efficient point cloud processing, enabling linear-time self-attention through localized computation within balls at different hierarchical levels.
    \item We present Erwin, a hierarchical transformer that processes data through progressive coarsening and refinement of ball tree structures, effectively capturing both fine-grained local interactions and global features while maintaining computational efficiency.
    \item We validate Erwin's performance across multiple large-scale physical domains:
        \begin{itemize}[leftmargin=20pt, topsep=-1pt, itemsep=-1pt]
            \item Capturing long-range interactions (cosmology)
            \item Computational efficiency (molecular dynamics)
            \item Model expressivity on large-scale multi-scale phenomena (turbulent fluid dynamics)
        \end{itemize}
    achieving state-of-the-art performance in both computational efficiency and prediction accuracy.
\end{itemize}



