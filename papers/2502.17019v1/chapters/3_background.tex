\section{Background}
Our work revolves around attention, which we aim to linearize by imposing structure onto point clouds using ball trees. We formally introduce both concepts in this section.

\subsection{Attention}
\label{section:attention}
The standard self-attention mechanism is based on the scaled dot-product attention \cite{Vaswani2017AttentionIA}. Given a set $X$ of $N$ input feature vectors of dimension $C$, self-attention is computed as
\begin{equation}
\begin{gathered} 
    \mathbf{Q}, \: \mathbf{K}, \: \mathbf{V} = \mathbf{X W}_q, \: \mathbf{X W}_k, \: \mathbf{X W}_v \\
    \mathrm{Att}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}\left(\frac{\mathbf{QK}^T}{\sqrt{C'}} + \mathcal{B}\right) \: \mathbf{V}
\end{gathered}
\end{equation}
where $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{C \times C'}$ are learnable weights and $\mathcal{B} \in \mathbb{R}^{N \times N}$ is the bias term.

Multi-head self-attention (MHSA) improves expressivity by computing attention $H$ times with different weights and concatenating the output before the final projection:
\begin{equation}
\begin{gathered} 
\mathrm{MHSA}(\mathbf{X}) = [\mathbf{Y}_1, \cdots, \mathbf{Y}_H] \: \mathbf{W}^O \\
\mathbf{Y}_i = \mathrm{Att}(\mathbf{X} \mathbf{W}_q^i, \mathbf{X} \mathbf{W}_k^i, \mathbf{X} \mathbf{W}_v^i)
\end{gathered}
\end{equation}
where $[\cdot,\cdots,\cdot]$ denotes concatenation along the feature dimension, and $\mathbf{W}^i_q, \mathbf{W}^i_k, \mathbf{W}^i_v \in \mathbb{R}^{C \times (C' / H)}$ and $\mathbf{W}^O \in \mathbb{R}^{C \times C'}$ are learnable weights.

The operator explicitly computes interactions between all elements in the input set without any locality constraints. This yields the quadratic computational cost w.r.t. the input set size $\mathcal{O}(N^2)$. Despite being heavily optimized \cite{Dao2023FlashAttention2FA}, this remains a bottleneck for large-scale applications. 

\subsection{Ball tree}
\label{section:ball_tree}

A ball tree is a hierarchical data structure that recursively partitions points into nested sets of equal size, where each set is represented by a ball that covers all the points in the set. Assume we operate on the $d$-dim.~Euclidean space $\left( \mathbb{R}^d, || \cdot||_2\right)$ where we have a point cloud (set) $P = \{ \mathbf{p}_1, ..., \mathbf{p}_n\} \subset\mathbb{R}^d$. 

\begin{definition}[Ball]
A \emph{ball} is a region bounded by a hypersphere in $\mathbb{R}^d$. Each ball is represented by the coordinates of its center $\mathbf{c} \in \mathbb{R}^d$ and radius $r \in \mathbb{R}_+$:
\begin{equation}
    B = B(\mathbf{c},r) = \{\mathbf{z} \in \mathbb{R}^d \mid ||\mathbf{z} - \mathbf{c}||_2 \leq r \}.
\end{equation}
\end{definition}
\vspace{-5pt}
We will omit the parameters $(\mathbf{c},r)$ for brevity from now on.

\begin{definition}[Ball Tree]
\label{def:ball_tree}
A \emph{ball tree} $T$ on point set $P$ is a hierarchical sequence of partitions $\{L_0, L_1, ..., L_m\}$, where each level $L_i$ consists of disjoint balls that cover $P$. At the leaf level $i = 0$, the nodes are the original points:
\begin{equation*}
     L_0 = \{ \{\mathbf{p}_j \} \mid \mathbf{p}_j \in P\}
\end{equation*}
For each subsequent level $i > 0$, each ball $B \in L_i$ is formed by merging two balls at the previous level $B_1, B_2 \in L_{i-1}$:
\begin{equation}
    L_i = \{ \{B_1 \cup B_2 \} \mid B_1, B_2 \in L_{i-1} \}
\end{equation}
such that its center is computed as the center of mass:
\begin{equation*}
    \mathbf{c}_B = \frac{|B_1| \mathbf{c}_1 + |B_2| \mathbf{c}_2}{|B_1| + |B_2|}
\end{equation*}
and its radius is determined by the furthest point it contains:
\begin{equation*}
    r_B = max\{ || \mathbf{p} - \mathbf{c}_B ||_2 \mid \mathbf{p} \in B_1 \cup B_2\}
\vspace{-5pt}
\end{equation*}
\end{definition}
where $|B|$ denotes the number of points contained in $B$. 

To construct the ball tree, we recursively split the data points into two sets starting from $P$. In each recursive step, we find the dimension of the largest spread (i.e. the $\text{max}-\text{min}$ value) and split at its median \cite{Pedregosa2011ScikitlearnML}, constructing covering balls per Def.\ref{def:ball_tree}. For details, see Appendix Alg.\ref{app:ball_tree_algorithm}\footnote{Note that since we split along coordinate axes, the resulting structure depends on the orientation of the input data and thus breaks rotation invariance. We will rely on this property in Section~\ref{sec:ball-tree-attn} to implement cross-ball connections.}.

\paragraph{Tree Completion}
To enable efficient implementation, we want to work with \emph{perfect} binary trees, i.e. trees where all internal nodes have exactly two children and all leaf nodes appear at the same depth. To achieve this, we pad the leaf level of a ball tree with virtual nodes, yielding the total number of nodes $2^m$, where $m = \text{ceil}(\log_2(n))$.
\subsubsection{Ball tree properties}
In the context of our method, there are several properties of ball trees that enable efficient hierarchical partitioning:
\begin{proposition}[Ball Tree Properties]
\label{proposition:ball_tree_properties}
The ball tree $T$ constructed as described satisfies the following properties:
\vspace{-10pt}
\begin{enumerate}[itemsep=0.5ex]
   \item The tree is a perfect binary tree.
   \item At each level $i$, each ball contains exactly $2^i$ leaf nodes.
   \item Balls at each level cover the point set
   \begin{equation*}
       \bigcup_{B \in L_i} B = P \quad \forall i \in \{0, ..., m\}.
   \end{equation*}
\end{enumerate}
\end{proposition}
\begin{proposition}[Contiguous Storage]
For a ball tree $T = \{L_0, L_1, ..., L_m\}$ on point cloud $P = \{\mathbf{p}_1, ..., \mathbf{p}_n\}$, there exists a bijective mapping $\pi: \{1,...,n\} \rightarrow \{1,...,n\}$ such that points belonging to the same ball $B \in L_i$ have contiguous indices under $\pi$.
\end{proposition}

As a corollary, the hierarchical structure at each level can be represented by nested intervals of contiguous indices:

\begin{example}
Let $P = \{\mathbf{p}_1, ..., \mathbf{p}_8\}$, then a ball tree $T = \{L_0, L_1, L_2, L_3\}$ is stored after the permutation $\pi$ as 
\begin{align}
    \notag
    L_3 \hspace{35pt} &\overbracket[0.5pt]{\hspace{118pt}} 
    \\ \notag
    L_2 \hspace{35pt} &\overbracket[0.5pt]{\hspace{53.5pt}} \overbracket[0.5pt]{\hspace{63pt}}
    \\ \notag
    L_1 \hspace{35pt} &
    \overbracket[0.5pt]{\hspace{28pt}}
    \overbracket[0.5pt]{\hspace{28.5pt}}
    \overbracket[0.5pt]{\hspace{28.5pt}}
    \overbracket[0.5pt]{\hspace{28.5pt}}
    \\ \notag
    L_0 = \pi(P) \hspace{14pt}
    &
    \hspace{3pt} \mathbf{p}_a \hspace{4pt} \mathbf{p}_b \hspace{5pt} \mathbf{p}_c \hspace{4pt} \mathbf{p}_d \hspace{5pt} \mathbf{p}_e \hspace{4pt} \mathbf{p}_f \hspace{5pt} \mathbf{p}_g \hspace{4pt} \mathbf{p}_h
\end{align}
\end{example}
\vspace{-5pt}
The contiguous storage property, combined with the fixed size of balls at each level, enables efficient implementation through tensor operations. Specifically, accessing any ball $B \in L_i$ simply requires selecting a contiguous sequence of $2^i$ indices. For instance, in the example above, for $i=2$, we select a:d and e:h to access the balls. Since the balls are equal, we can simply reshape $L_0$ to access any level. This representation makes it particularly efficient to implement our framework's core operations - ball attention and coarsening/refinement - which we will introduce next.

Another important property of ball trees is that while they cover the whole point set, they are not required to partition the entire space. Coupled with completeness, it means that at each tree level, the nodes are essentially associated with the same scale. This contrasts with other structures such as oct-trees that cover the entire space and whose nodes at the same level can be associated with regions of different sizes: \input{figures/btree_vs_octree}
