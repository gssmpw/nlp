\section{Implementation details}
\label{appendix:implementation}

\begin{algorithm}[t]
\caption{\textsc{BuildBallTree}}
\begin{algorithmic}
\STATE {\bfseries input} Array of data points $D$ in $\mathbb{R}^d$
\STATE {\bfseries output} Ball tree node $B$
\STATE
\IF{$|D| = 1$}
\STATE Create leaf node $B$ containing single point in $D$
\STATE {\bfseries return} $B$
\ENDIF
\STATE
\STATE \algcom{Find dimension of greatest spread}
\STATE $\delta \gets \text{argmax}_{i \in {1,\ldots,d}} (\max_{x \in D} x_i - \min_{x \in D} x_i)$
\STATE
\STATE \algcom{Find the median point along $\delta$}
\STATE $p \gets \text{median}\{x_\delta \mid x \in D\}$

\algcom{Points left of median along $\delta$}
\STATE $L \gets \{x \in D \mid x_\delta \leq p_\delta\}$ 

\algcom{Points right of median along $\delta$}
\STATE $R \gets \{x \in D \mid  x_\delta > p_\delta\}$
\STATE
\STATE \algcom{Recursively construct children}
\STATE $B.\text{child}_1 \gets \textsc{BuildBallTree}(L)$
\STATE $B.\text{child}_2 \gets \textsc{BuildBallTree}(R)$
\STATE
\STATE {\bfseries return} $B$
\end{algorithmic}
\label{app:ball_tree_algorithm}
\end{algorithm}

\paragraph{Ball tree construction}
The algorithm used for constructing ball trees \cite{Pedregosa2011ScikitlearnML} can be found in Alg.~\ref{app:ball_tree_algorithm}. Note that this implementation is not rotationally equivariant as it relies on choosing the dimension of the greatest spread which in turn depends on the original orientation. Examples of ball trees built in our experiments are shown in Fig.~\ref{fig:ball_examples}.

\paragraph{MPNN in the embedding}
Erwin employs a small-scale MPNN in the embedding. More precisely, given a graph $G = (V, E)$ with nodes $v_i \in V$ and edges
$e_{ij} \in E$, we compute multiple layers of message-passing as proposed in \cite{gilmer2017neuralmessagepassingquantum}:
\begin{align}
\label{eq:mpnnlayer}
    &\mathbf{m}_{ij} = \text{MLP}_e(\mathbf{h}_i, \mathbf{h}_j, \mathbf{p}_i - \mathbf{p}_j), \nonumber \qquad &\mathrm{message} \\
    &\mathbf{m}_i \,\,\,= \sum_{j\in \mathcal{N}(i)} \mathbf{m}_{ij}, \qquad &\mathrm{aggregate} \\
    &\mathbf{h}_i = \text{MLP}_h(\mathbf{h}_i, \mathbf{m}_i), \nonumber \qquad &\mathrm{update}
\end{align}
where $\mathbf{h}_i \in \mathbb{R}^H$ is a feature vector of $v_i$, $\mathcal{N}(i)$ denotes the neighborhood of $v_i$.
The motivation for using an MPNN is to incorporate local neighborhood information into the model. Theoretically, attention should be able to capture it as well; however, this might require substantially increasing feature dimension and the number of attention heads, which would be prohibitively expensive for a large number of nodes in the original level of a ball tree. 

In our experiments, we consistently maintain the size of MLP$_e$ and MLP$_h$ small ($H \leq 32$) such that embedding accounts for less than $5$\% of total runtime. 

