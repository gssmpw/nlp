\section{Related Works: sub-quadratic attention}

One way to avoid the quadratic cost of self-attention is to linearize attention by performing it on non-overlapping patches. %
For data on regular grids, like images, the SwinTransformer \cite{Liu2021SwinTH} achieves this by limiting attention to local windows with cross-window connection enabled by shifting the windows.
However, for irregular data such as point clouds or non-uniform meshes, one first needs to induce a structure that will allow for patching. Several approaches \cite{Liu2023FlatFormerFW, Sun2022SWFormerSW} transform point clouds into sequences, most notably, PointTransformer v3 \cite{Wu2023PointTV}, which projects points into voxels and orders them using space-filling curves (e.g., Hilbert curve). While scalable, these curves introduce artificial discontinuities that can break local spatial relationships.

Particularly relevant to our work are hierarchical attention methods. In the context of 1D sequences, approaches like the H-transformer \cite{Zhu2021HTransformer1DFO} and Fast Multipole Attention \cite{Kang2023FastMA} approximate self-attention through multi-level decomposition: tokens interact at full resolution locally while distant interactions are computed using learned or fixed groupings at progressively coarser scales. For point clouds, OctFormer \cite{Wang2023OctFormerOT} converts spatial data into a sequence by traversing an octree, ensuring spatially adjacent points are consecutive in memory. While conceptually similar to our approach, OctFormer relies on computationally expensive octree convolutions, whereas our utilization of ball trees leads to significant efficiency gains.

Rather than using a hierarchical decomposition, another line of work proposes cluster attention \cite{Janny2023EagleLL, alkin2024upt}. These methods first group points into clusters and aggregate their features at the cluster centroids through message passing or cross-attention. After computing attention between the centroids, the updated features are then distributed back to the original points. While these approaches achieve the quadratic cost only in the number of clusters, they introduce an information bottleneck at the clustering step that may sacrifice fine-grained details and fail to capture features at multiple scales - a limitation our hierarchical approach aims to overcome.











