\section{Experimental details}
\label{appendix:experimental_detail}

In this section, we provide experimental details regarding hyperparameter choice and optimization. All experiments were conducted on a single NVIDIA RTX A6000. All models were trained using the AdamW optimizer \cite{Loshchilov2017DecoupledWD} with weight decay $0.01$ and a cosine decay schedule. The learning rate was tuned in the range $10^{-4}$ to $10^{-3}$ to minimize loss on the respective validation sets.

\vspace{-5pt}
\paragraph{Cosmological simulations}
We follow the experimental setup of the benchmark. The training was done for $5000$ epochs with batch size $16$ for point transformers and batch size $8$ for message-passing-based models. The implementation of SEGNN, NequIP and MPNN was done in JAX and taken from the original benchmark repository \cite{Balla2024ACB}. We maintained the hyperparameters of the baselines used in the benchmark. For Erwin and PointTransformer, those are provided in Table~\ref{tab:cosmo_architectures}. In Erwin's embedding, we conditioned messages on Bessel basis functions rather than the relative position, which significantly improved overall performance.

\vspace{-5pt}
\paragraph{Molecular dynamics}
All models were trained with batch size $32$ for $50000$ training iterations with an initial learning rate of $5 \cdot 10^{-4}$. We finetuned the hyperparameters of every model on the validation dataset (reported in Table~\ref{tab:md_architectures}).

\vspace{-5pt}
\paragraph{Turbulent fluid dynamics}
Baseline results are taken from \cite{Janny2023EagleLL}, except for runtime and peak memory usage, which we measured ourselves. Erwin was trained with batch size $12$ for $4000$ epochs. 

\vspace{-5pt}
\paragraph{Airflow pressure modeling}
We take the results of baseline models from \citet{alkin2024upt}. Both Erwin and PointTransformer v3 were trained with batch size $32$ for $1000$ epochs, and their hyperparameters are given in Table~\ref{tab:shapenet_architectures}).

\clearpage
\begin{table}
\centering
\caption{Model architectures for the cosmological simulations task. For varying sizes of Erwin, the values are given as (S/M).}
\label{tab:cosmo_architectures}
\scalebox{0.90}{
\begin{tabular}{lll}
\toprule
Model & Parameter & Value \\
\midrule
Point & Grid size & 0.01 \\
Transformer v3 & Enc. depths & (2, 2, 6, 2) \\
           & Enc. channels & (32, 64, 128, 256) \\
           & Enc. heads & (2, 4, 8, 16) \\
           & Enc. patch size & 64 \\
           & Dec. depths & (2, 2, 2) \\
           & Dec. channels & (64, 64, 128) \\
           & Dec. heads & (2, 4, 8) \\
           & Dec. patch size & 64 \\
           & Pooling & (2, 2, 2) \\
\midrule
Erwin & MPNN dim. & 32 \\
      & Channels & 32-512/64-1024 \\
      & Window size & 64 \\
      & Enc. heads & (2, 4, 8, 16) \\
      & Enc. depths & (2, 2, 6, 2) \\
      & Dec. heads & (2, 4, 8) \\
      & Dec. depths & (2, 2, 2) \\
      & Pooling & (2, 2, 2, 1) \\
\bottomrule
\end{tabular}
}
\end{table}
\begin{table}
\centering
\caption{Model architectures for the airflow pressure task.}
\label{tab:shapenet_architectures}
\scalebox{0.80}{
\begin{tabular}{lll}
\toprule
Model & Parameter & Value \\
\midrule
Point & Grid size & 0.01 \\
Transformer v3 & Enc. depths & (2, 2, 2, 2, 2) \\
           & Enc. channels & 24-384 \\
           & Enc. heads & (2, 4, 8, 16, 32) \\
           & Enc. patch size & 256 \\
           & Dec. depths & (2, 2, 2, 2) \\
           & Dec. channels & 48-192 \\
           & Dec. heads & (4, 4, 8, 16) \\
           & Dec. patch size & 256 \\
\midrule
Erwin & MPNN dim. & 8 \\
      & Channels & 96 \\
      & Window size & 256 \\
      & Enc. heads & (8, 16) \\
      & Enc. depths & (6, 2) \\
      & Dec. heads & (8,) \\
      & Dec. depths & (2,) \\
      & Pooling & (2, 1) \\
      & MP steps & 1 \\
\bottomrule
\end{tabular}
}
\end{table}
\begin{table}
\centering
\caption{Model architectures for the molecular dynamics task. For models of varying sizes, the values are given as (S/M/L).}
\label{tab:md_architectures}
\scalebox{0.90}{
\begin{tabular}{lll}
\toprule
Model & Parameter & Value \\
\midrule
MPNN & Hidden dim. & 48/64/128 \\
     & MP steps & 6 \\
     & MLP layers & 2 \\
     & Message agg-n & mean \\
\midrule
PointNet++ & Hidden dim. & 64/128/196 \\
           & MLP layers & 2 \\
\midrule
Point & Grid size & 0.025 \\
Transformer v3 & Enc. depths & (2, 2, 2, 6, 2) \\
           & Enc. channels & 16-192/24-384/64-1024 \\
           & Enc. heads & (2, 4, 8, 16, 32) \\
           & Enc. patch size & 128 \\
           & Dec. depths & (2, 2, 2, 2) \\
           & Dec. channels & 16-96/48-192/64-512 \\
           & Dec. heads & (4, 4, 8, 16) \\
           & Dec. patch size & 128 \\
\midrule
Erwin & MPNN dim. & 16/16/32 \\
      & Channels  & (16-256/32-512/64-1024) \\
      & Window size & 128 \\
      & Enc. heads & (2, 4, 8, 16, 32) \\
      & Enc. depths & (2, 2, 2, 6, 2) \\
      & Dec. heads & (4, 4, 8, 16) \\
      & Dec. depths & (2, 2, 2, 2) \\
      & Pooling & (2, 2, 2, 2, 1) \\
\bottomrule
\end{tabular}
}
\end{table}
