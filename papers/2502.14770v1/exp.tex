\section{Experiments}\label{sec:experiments}
\subsection{Experimental Setup}

\paragraph{Models.} We evaluate ATP across a diverse range of widely-used LLMs, including LLaMA1 (7B, 13B, 30B, 65B) \citep{touvron2023llama}, LLaMA2 (7B, 13B, 70B) \citep{touvron2023llama2}, LLaMA3-8B \citep{llama3}, LLaMA3.1-8B \citep{llama31}, OPT-13B \citep{zhang2022opt}, Vicuna-13B \citep{chiang2023vicuna}, Qwen2.5-7B \citep{yang2024qwen2}, and Mistral-7B \citep{jiang2023mistral}.  
\paragraph{Evaluation.} Our evaluation protocol aligns with established sparsification methods for LLMs (\emph{e.g.,}, SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple}), encompassing both zero-shot learning and language modeling capabilities. Specifically, we assess the perplexity of the models on the validation set of WikiText-2 \citep{merity2016pointer} and evaluate zero-shot performance on seven downstream tasks: BoolQ \citep{clark2019boolq}, ARC Easy and Challenge \citep{clark2018think}, HellaSwag \citep{zellers2019hellaswag}, WinoGrande \citep{sakaguchi2021winogrande}, OpenbookQA \citep{mihaylov2018can}, and PIQA \citep{bisk2020piqa}. Additionally, we measure performance on arithmetic and knowledge reasoning benchmarks, including 8-shot accuracy on the GSM8K dataset \citep{cobbe2021training} and 5-shot accuracy on the MMLU dataset \citep{hendrycks2020measuring}.  
\paragraph{Baselines.} We apply the layer-wise sparsity rates determined by ATP to several state-of-the-art post-training sparsification methods, including SparseGPT \citep{frantar2023sparsegpt}, Wanda \citep{sun2023simple}, DSnoT \citep{zhang2023dynamic}, Pruner-zero \citep{dong2024pruner}, and ALPS \citep{meng2024alps}. Furthermore, we compare ATP with recent methods for determining layer-wise sparsity rates in LLMs, such as OWL \citep{yin2023outlier}, AlphaPruning \citep{lu2024alphapruning}, DSA \citep{li2024discovering}, and ALS \citep{li2024adaptive}.  
\paragraph{More Models, Evaluations and Baselines.} In Sec. \ref{Moreresults}, we present additional experimental results, including the application of ATP to multimodal and vision models, integration with other compression techniques and LoRA fine-tuning, and comparisons with an expanded set of layer-wise sparsity baselines.  
\paragraph{Implementation Details.} Our pruning implementation builds upon the methods used by SparseGPT and Wanda, with the primary modification being the integration of layer-wise sparsity rates generated by ATP. 

\subsection{Zero-shot Tasks}
\paragraph{Quantitative Evaluation.}  
We report the average performance of 70$\%$ sparse LLMs across seven zero-shot tasks in Table \ref{tb:Zero-shot_tasks_results}. The results demonstrate that our ATP method consistently improves accuracy compared to the uniform sparsity baseline and significantly outperforms other state-of-the-art layer-wise sparsity methods. For instance, with the LLaMA3-8B model pruned using the Wanda method, ATP achieves a 3.43$\%$ higher accuracy than the best-performing DSA method, highlighting the effectiveness and superiority of ATP in enhancing sparse LLM performance.  
\begin{table*}[t]
    \centering
    \caption{Comparison of the average zero-shot accuracy across 7 tasks for 70$\%$ sparse LLMs obtained using various sparsity methods.}
    \label{tb:Zero-shot_tasks_results}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{c|c|cccc|ccc|c}
        \toprule
         & & \multicolumn{4}{c|}{LLaMA}  & \multicolumn{3}{c|}{LLaMA2} & \multicolumn{1}{c}{LLaMA3}\\
        \multirow{-2}{*}{Method} & \multirow{-2}{*}{\makecell[c]{Layerwise\\sparsity}} & 7B & 13B & 30B & 65B & 7B & 13B & 70B & 8B \\
        \midrule 
        Dense & - & 61.74 & 63.84 &  67.41&68.57 & 61.88& 65.00& 69.14 &  65.62  \\ 
        \midrule 
        & Uniform & 37.45 &  40.79& 53.35 & 57.76 &  35.33& 38.88 & 58.48  & 35.42 \\
        & OWL &44.19 & 48.18 & 55.88 & 59.79 &  41.75&   47.19&  59.26&35.42 \\
        & DSA & 43.72& 48.64 & 55.21 & 58.24 & 36.55 & 43.36 &  58.25&  37.85 \\
         &  AlphaPruning&44.99 & 49.81 &   56.67  & 61.05&43.37 &49.11 &   59.44& 35.61\\
        \gr \multirow{-5}{*}{Wanda}&  \bf{ATP} & \bf{47.03} &   \bf{51.60} & \bf{57.69}  &  \bf{61.92} &   \bf{45.83} &  \bf{52.11} &  \bf{60.91} &   \bf{41.28}   \\
        \midrule 
        & Uniform  &43.60 &48.00 &53.64&60.18 & 43.07   &  47.38 & 60.84 & 43.02\\
        & OWL & 46.57& 50.01 & 55.73 & 59.47 & 46.55 &  49.90 & 60.83 & 46.06\\
         &  AlphaPruning&46.58 & 50.63 &56.42  &60.32  & 46.20 & 50.86 & 61.03 & 45.27 \\
        \gr \multirow{-5}{*}{SparseGPT} &  \bf{ATP} & \bf{47.37} &   \bf{52.12} & \bf{57.55}  &  \bf{61.31} &   \bf{48.63} &  \bf{52.46} &  \bf{62.25} &   \bf{47.79} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}
\paragraph{Varying Sparsity Rates.}  
We also evaluate the performance of sparse LLMs under reduced sparsity constraints. Specifically, Table \ref{tab:v_sparsity_rate_Zero-shot} presents the zero-shot accuracy of LLMs pruned using the Wanda method at a 50$\%$ sparsity rate. Even under this lower sparsity setting, ATP demonstrates substantial improvements in accuracy across all models, maintaining its performance advantage over existing layer-wise sparsity methods. This indicates that ATP is robust and effective in optimizing LLM accuracy under varying sparsity rates.  
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Average zero-shot accuracy of sparse LLaMA2-7B/13B models pruned using the Wanda method at 50$\%$ sparsity rate.}
\label{tab:v_sparsity_rate_Zero-shot}
\resizebox{0.45\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|c|c|c}
\toprule 
Method& \multicolumn{1}{c|}{1-7B} & \multicolumn{1}{c|}{1-13B} & \multicolumn{1}{c|}{2-7B} & \multicolumn{1}{c}{2-13B} \\
    \midrule
    Dense &66.09 & 68.18 & 66.69& 69.25 \\
    \midrule
   Uniform &60.06 &  65.22& 63.03&65.73  \\
    OWL & 61.92& 65.91 & 63.88&68.02 \\
    DSA &61.90 & 65.40 &63.89 & 67.65 \\
    ALS& 61.59& 65.05 & 64.12& 67.11 \\
    AlphaPruning &61.91 & 66.19 &64.20 &67.80\\
  \gr   \bf{ATP}  & \bf62.72 &\bf 66.39&\bf64.49  & \bf68.18   \\ 
     \bottomrule
\end{tabular}
\begin{tablenotes}
\normalsize
\item * The zero-shot evaluation setting used here follows the configuration outlined in the ALS paper. For more details, refer to Sec. \ref{Zero-shotevaluationsetting}.
\end{tablenotes}
\end{threeparttable}
}
\end{table}

\subsection{Language Modeling}
We present the perplexity of 50$\%$ to 80$\%$ sparse LLaMA-7B and LLaMA2-7B models pruned using the Wanda method on the WikiText-2 dataset in Table \ref{tab:v_sparsity_rate_ppl}. The results show that our ATP method achieves lower perplexity compared to other layer-wise sparsity methods. Furthermore, this advantage becomes increasingly significant at higher sparsity rates. 
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{WikiText-2 perplexity of sparse LLaMA-7B/2-7B obtained by Wanda across varying sparsity rates.}
\label{tab:v_sparsity_rate_ppl}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{l| cccc|cccc}
\toprule 
& \multicolumn{4}{c|}{LLaMA-7B} & \multicolumn{4}{c}{LLaMA2-7B} \\
\midrule
   Method & 50\% & 60\%  & 70\% &  80\% & 50\% & 60\% & 70\% & 80\% \\
    \midrule
   Uniform & 7.26   & 10.63  & 84.52  & 5889.13 & 6.92 &10.96&74.26 &1980.85  \\
    OWL& 7.22   & 9.35   & 24.56  & 1002.87&6.87 & 9.80& 30.38&629.99  \\
    DSA &7.17 &9.38 &  24.52 &1232.88 &7.05&10.40&63.71 &1638.81  \\
    AlphaPruning & 7.18   & 9.47   & 23.86 & 698.56& 6.88 & 9.78 & 28.87& 1672.49 \\
  \gr   \bf{ATP} &\textbf{7.05}&\textbf{9.25}&\textbf{20.16}&\textbf{176.80}& \bf 6.82 & \bf 9.15& \bf 22.16 & \bf 425.12  \\ 
     \bottomrule
\end{tabular}}
\end{table}


\subsection{More LLM Architectures}
We validate the effectiveness of our ATP method on LLMs with a broader range of architectures. Specifically, we combined ATP with the Wanda method to obtain 70$\%$ sparse models, including LLaMA3.1-8B, OPT-6.7B, Vicuna-7B, Qwen2.5-7B, and Mistral-7B. We report the experimental results in Table \ref{tab:MoreArchitectures}. The results demonstrate that our method consistently enhances the performance of LLMs across various architectures.

\subsection{More Results}\label{sec:MoreResults}
We provide more experimental results in the appendix. Specifically, in Sec. \ref{sec:arithmetic}, we demonstrate the enhanced performance of our ATP method on arithmetic and knowledge reasoning tasks for sparse LLMs. Additionally, in Secs. \ref{sec:multimodal_tasks} and \ref{sec:vision_models}, we present the performance gains of ATP on sparse multimodal and vision models, respectively. In Sec. \ref{sec:other_Compression_Technologies}, we highlight the performance improvements when integrating ATP with other compression techniques, including N:M sparsity, structured pruning, and quantization. Furthermore, we compare ATP with additional layer-wise sparsity baselines (Sec. \ref{sec:more_baselines}), demonstrate its performance enhancements across various post-training sparsity methods (Sec. \ref{sec:more_post-training}), and showcase its effectiveness when combined with LoRA fine-tuning for sparse LLMs (Sec. \ref{sec:integrate_LoRA}).

\subsection{Ablation Study}\label{sec:AblationStudy}
\paragraph{Searching Step.}
In Sec. \ref{Layer-wiseSparsity}, we perform a grid search with a step size of $0.002$ to determine the optimal value of $\beta$. Here, we analyze the impact of different step sizes on the search results. As shown in Table \ref{tab:SearchingStep}, searches conducted with larger step sizes yield inferior results compared to those with a step size of $0.002$. This is because larger step sizes fail to sufficiently explore the possible optimal values of $\beta$. Conversely, further reducing the step size for a more fine-grained search shows limited improvement in perplexity. Therefore, to balance both accuracy and efficiency, we adopt a grid search with a step size of $0.002$.
\begin{table}[h!]
    \centering
    \caption{The impact of different step sizes on search results.}\label{tab:SearchingStep}
    \resizebox{0.6\textwidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Step size} & \textbf{0.0005} &\textbf{0.001}& \textbf{0.002}&\textbf{0.004} & \textbf{0.008}  \\
        \midrule
        \textbf{Perplexity (↓)} & 22.14 & 22.16 & 22.16 &23.09  & 23.09 \\
        \textbf{Search Time (min)} & 76 &38  & 18 & 8 &4 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\paragraph{Comparison with Bayesian Search.}
We compare the ATP method with layer-wise sparsity rates obtained through Bayesian search. Specifically, we use Bayesian search to determine the sparsity rates for the 50$\%$, 60$\%$, and 70$\%$ sparse LLaMA2-7B model. The search is conducted using the Optuna hyperparameter optimization framework \citep{akiba2019optuna}, performing 1000 iterations to optimize the sparsity rates across all 32 layers of LLaMA2-7B. The sparsity rate for each layer is constrained between 0 and 1, while ensuring that the average sparsity matches the target rates. We integrate Bayesian search with the Wanda method, with the objective of minimizing model perplexity on the WikiText-2 dataset. The comparison of perplexity and zero-shot accuracy between sparse LLMs obtained using ATP and Bayesian search is presented in Table \ref{tab:BayesianSearch}. The results indicate that the performance of our ATP method is comparable to the optimal solution obtained through Bayesian search, demonstrating that the layer-wise sparsity rates determined by our method are experimentally close to the optimal values identified by the search approach. However, Bayesian search requires approximately 33 hours to complete on a single NVIDIA A100 80GB GPU, whereas ATP only takes 18 minutes, demonstrating significantly higher efficiency.  
\begin{table}[h!]
    \centering
    \caption{Comparison of ATP and Bayesian search.}\label{tab:BayesianSearch}
    \resizebox{0.6\textwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} &\textbf{Sparsity}& \textbf{Perplexity (↓)} & \textbf{Accuracy (↑)} \\
        \midrule
        Bayesian Search & 50$\%$ & 6.81 & 59.67 \\
        \gr \textbf{ATP}& \textbf{50$\%$} & \textbf{6.82} & \textbf{59.63} \\
        \midrule
        Bayesian Search & 60$\%$ & 9.20 & 54.73 \\
        \gr \textbf{ATP}& \textbf{60$\%$} & \textbf{9.15} & \textbf{54.79} \\
        \midrule
        Bayesian Search & 70$\%$ & 22.10 & 45.90 \\
        \gr \textbf{ATP}& \textbf{70$\%$} & \textbf{22.16} & \textbf{45.83} \\
        \bottomrule
    \end{tabular}
    }
\end{table}



\paragraph{Inference Speedup.} We evaluate the acceleration performance of the sparse LLaMA2-7B model, with results summarized in Table \ref{tab:speedup}. The end-to-end token generation time was measured using the DeepSparse \citep{deepsparse} inference engine on an Intel Xeon Silver 4314 CPU and the nm-vllm \citep{nm-vllm} inference engine on an NVIDIA RTX 4090 GPU. Our method achieves significant speedups, ranging from 1.79$\times$ to 2.63$\times$ on the CPU and 1.71$\times$ to 2.23$\times$ on the GPU, compared to the dense model, at sparsity rates between 50$\%$ and 70$\%$.
\begin{table}[h!]
\centering
\caption{End-to-end inference acceleration of sparse LLaMA2-7B on CPU and GPU.}
\label{tab:speedup}
\setlength{\tabcolsep}{10pt}
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\toprule
\textbf{Device} & \textbf{Sparsity} & \textbf{Dense} & \textbf{50$\%$} & \textbf{60$\%$} & \textbf{70$\%$} \\
\midrule
\multirow{2}{*}{CPU} & \makecell[c]{Throughput\\(tokens/s)} $\uparrow$ & 3.40  & \textbf{6.10} & \textbf{7.39} & \textbf{8.95} \\ 
\cmidrule{2-6}
& \makecell[c]{\textbf{Speedup} $\uparrow$} & 1.00$\times$  & \textbf{1.79$\times$} & \textbf{2.17$\times$} & \textbf{2.63$\times$}  \\
\midrule
\multirow{2}{*}{GPU} & \makecell[c]{Throughput\\(tokens/s)} $\uparrow$ & 57.29 & \textbf{97.92} & \textbf{111.86} & \textbf{127.67} \\ 
\cmidrule{2-6}
& \makecell[c]{\textbf{Speedup} $\uparrow$} & 1.00$\times$ & \textbf{1.71$\times$} & \textbf{1.95$\times$} & \textbf{2.23$\times$}  \\
\bottomrule
\end{tabular}}
\end{table}

\paragraph{More Ablation Study.} We provide more ablation results in the appendix. Specifically, in Sec. \ref{sec:ComputationalEfficiency}, we demonstrate the computational efficiency of our ATP method. In Secs. \ref{sec:AnalyzeDistribution} and \ref{ComparisonDistribution}, we analyze the layer-wise sparsity distribution generated by ATP and compare it with the distributions produced by other layer-wise sparsity methods. Furthermore, in Sec. \ref{sec:Different_beta}, we explore the impact of different $\beta$ settings on the perplexity of sparse LLMs.