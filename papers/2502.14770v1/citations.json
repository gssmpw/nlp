[
  {
    "index": 0,
    "papers": [
      {
        "key": "yu2022combinatorial",
        "author": "Yu, Xin and Serra, Thiago and Ramalingam, Srikumar and Zhe, Shandian",
        "title": "The combinatorial brain surgeon: pruning weights that cancel one another in neural networks"
      },
      {
        "key": "zhang2024how",
        "author": "Qiaozhe Zhang and Ruijie ZHANG and Jun Sun and Yingzhuang Liu",
        "title": "How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xia2022structured",
        "author": "Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi",
        "title": "Structured pruning learns compact and accurate models"
      },
      {
        "key": "li2023losparse",
        "author": "Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo",
        "title": "Losparse: Structured compression of large language models based on low-rank and sparse approximation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yu2022width",
        "author": "Yu, Fang and Huang, Kun and Wang, Meng and Cheng, Yuan and Chu, Wei and Cui, Li",
        "title": "Width \\& depth pruning for vision transformers"
      },
      {
        "key": "he2024pruning",
        "author": "He, Haoyu and Cai, Jianfei and Liu, Jing and Pan, Zizheng and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan",
        "title": "Pruning self-attentions into convolutional layers in single path"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dong2024pruner",
        "author": "Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen",
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "meng2024alps",
        "author": "Xiang Meng and Kayhan Behdin and Haoyue Wang and Rahul Mazumder",
        "title": "{ALPS}: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "boyd2011distributed",
        "author": "Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others",
        "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lee2020layer",
        "author": "Lee, Jaeho and Park, Sejun and Mo, Sangwoo and Ahn, Sungsoo and Shin, Jinwoo",
        "title": "Layer-adaptive sparsity for the magnitude-based pruning"
      },
      {
        "key": "liu2022unreasonable",
        "author": "Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola",
        "title": "The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yin2023outlier",
        "author": "Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei",
        "title": "Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lu2024alphapruning",
        "author": "Lu, Haiquan and Zhou, Yefan and Liu, Shiwei and Wang, Zhangyang and Mahoney, Michael W and Yang, Yaoqing",
        "title": "Alphapruning: Using heavy-tailed self regularization theory for improved layer-wise pruning of large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "martin2019traditional",
        "author": "Martin, Charles H and Mahoney, Michael W",
        "title": "Traditional and heavy-tailed self regularization in neural network models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li2024adaptive",
        "author": "Li, Wei and Li, Lujun and Lee, Mark G and Sun, Shengjie",
        "title": "Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kraskov2004estimating",
        "author": "Kraskov, Alexander and St{\\\"o}gbauer, Harald and Grassberger, Peter",
        "title": "Estimating mutual information"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2024discovering",
        "author": "Li, Lujun and Dong, Peijie and Tang, Zhenheng and Liu, Xiang and Wang, Qiang and Luo, Wenhan and Xue, Wei and Liu, Qifeng and Chu, Xiaowen and Guo, Yike",
        "title": "Discovering sparsity allocation for layer-wise pruning of large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yun2021all",
        "author": "Yun, Stone and Wong, Alexander",
        "title": "Do all mobilenets quantize poorly? gaining insights into the effect of quantization on depthwise separable convolutional networks through the eyes of multi-scale distributional dynamics"
      },
      {
        "key": "hubara2021accelerated",
        "author": "Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel",
        "title": "Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks"
      },
      {
        "key": "ma2023solving",
        "author": "Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Pan, Xin and Chao, Fei and Ji, Rongrong",
        "title": "Solving oscillation problem in post-training quantization through a theoretical perspective"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhang2023dynamic",
        "author": "Zhang, Yuxin and Zhao, Lirui and Lin, Mingbao and Sun, Yunyun and Yao, Yiwu and Han, Xingjia and Tanner, Jared and Liu, Shiwei and Ji, Rongrong",
        "title": "Dynamic sparse no training: Training-free fine-tuning for sparse llms"
      }
    ]
  }
]