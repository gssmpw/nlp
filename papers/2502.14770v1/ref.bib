@article{parzen1962estimation,
  title={On estimation of a probability density function and mode},
  author={Parzen, Emanuel},
  journal={The annals of mathematical statistics},
  volume={33},
  number={3},
  pages={1065--1076},
  year={1962},
  publisher={JSTOR}
}

@article{bromiley2004shannon,
  title={Shannon entropy, Renyi entropy, and information},
  author={Bromiley, PA and Thacker, NA and Bouhova-Thacker, E},
  journal={Statistics and Inf. Series (2004-004)},
  volume={9},
  number={2004},
  pages={2--8},
  year={2004}
}

@article{lu2024alphapruning,
  title={Alphapruning: Using heavy-tailed self regularization theory for improved layer-wise pruning of large language models},
  author={Lu, Haiquan and Zhou, Yefan and Liu, Shiwei and Wang, Zhangyang and Mahoney, Michael W and Yang, Yaoqing},
  journal={arXiv preprint arXiv:2410.10912},
  year={2024}
}

@inproceedings{li2024adaptive,
  title={Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment},
  author={Li, Wei and Li, Lujun and Lee, Mark G and Sun, Shengjie},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@article{wu2019hyperparameter,
  title={Hyperparameter optimization for machine learning models based on Bayesian optimization},
  author={Wu, Jia and Chen, Xiu-Yun and Zhang, Hao and Xiong, Li-Dong and Lei, Hang and Deng, Si-Hao},
  journal={Journal of Electronic Science and Technology},
  volume={17},
  number={1},
  pages={26--40},
  year={2019},
  publisher={Elsevier}
}

@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={2383},
  year={2018},
  publisher={Nature Publishing Group UK London}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@InProceedings{Liu_2022_CVPR,
    author    = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
    title     = {A ConvNet for the 2020s},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11976-11986}
}

@incollection{jimenez2008finding,
  title={Finding optimal model parameters by discrete grid search},
  author={Jim{\'e}nez, {\'A}lvaro Barbero and L{\'a}zaro, Jorge L{\'o}pez and Dorronsoro, Jos{\'e} R},
  booktitle={Innovations in hybrid intelligent systems},
  pages={120--127},
  year={2008},
  publisher={Springer}
}

@article{liashchynskyi2019grid,
  title={Grid search, random search, genetic algorithm: a big comparison for NAS},
  author={Liashchynskyi, Petro and Liashchynskyi, Pavlo},
  journal={arXiv preprint arXiv:1912.06059},
  year={2019}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{martin2021predicting,
  title={Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data},
  author={Martin, Charles H and Peng, Tongsu and Mahoney, Michael W},
  journal={Nature Communications},
  volume={12},
  number={1},
  pages={4122},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{paul2022unmasking,
  title={Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?},
  author={Paul, Mansheej and Chen, Feng and Larsen, Brett W and Frankle, Jonathan and Ganguli, Surya and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2210.03044},
  year={2022}
}

@article{wang2024model,
  title={Model compression and efficient inference for large language models: A survey},
  author={Wang, Wenxiao and Chen, Wei and Luo, Yicong and Long, Yongliu and Lin, Zhengkai and Zhang, Liye and Lin, Binbin and Cai, Deng and He, Xiaofei},
  journal={arXiv preprint arXiv:2402.09748},
  year={2024}
}

@article{zhu2024survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1556--1577},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{akiba2019optuna,
  title={Optuna: A next-generation hyperparameter optimization framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2623--2631},
  year={2019}
}

@inproceedings{tang2022mixed,
  title={Mixed-precision neural network quantization via learned layer-wise importance},
  author={Tang, Chen and Ouyang, Kai and Wang, Zhi and Zhu, Yifei and Ji, Wen and Wang, Yaowei and Zhu, Wenwu},
  booktitle={European Conference on Computer Vision},
  pages={259--275},
  year={2022},
  organization={Springer}
}

@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{liu2022unreasonable,
  title={The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:2202.02643},
  year={2022}
}

@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical Review Eâ€”Statistical, Nonlinear, and Soft Matter Physics},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}

@article{martin2019traditional,
  title={Traditional and heavy-tailed self regularization in neural network models},
  author={Martin, Charles H and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1901.08276},
  year={2019}
}

@article{shin2024rethinking,
  title={Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization},
  author={Shin, Sungbin and Park, Wonpyo and Lee, Jaeho and Lee, Namhoon},
  journal={arXiv preprint arXiv:2406.15524},
  year={2024}
}

@inproceedings{yun2021all,
  title={Do all mobilenets quantize poorly? gaining insights into the effect of quantization on depthwise separable convolutional networks through the eyes of multi-scale distributional dynamics},
  author={Yun, Stone and Wong, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2447--2456},
  year={2021}
}

@inproceedings{ma2023solving,
  title={Solving oscillation problem in post-training quantization through a theoretical perspective},
  author={Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Pan, Xin and Chao, Fei and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7950--7959},
  year={2023}
}

@inproceedings{li2024discovering,
  title={Discovering sparsity allocation for layer-wise pruning of large language models},
  author={Li, Lujun and Dong, Peijie and Tang, Zhenheng and Liu, Xiang and Wang, Qiang and Luo, Wenhan and Xue, Wei and Liu, Qifeng and Chu, Xiaowen and Guo, Yike},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}

@article{kurtic2024ziplm,
  title={Ziplm: Inference-aware structured pruning of language models},
  author={Kurti{\'c}, Eldar and Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xia2022structured,
  title={Structured pruning learns compact and accurate models},
  author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2204.00408},
  year={2022}
}

@inproceedings{yu2022width,
  title={Width \& depth pruning for vision transformers},
  author={Yu, Fang and Huang, Kun and Wang, Meng and Cheng, Yuan and Chu, Wei and Cui, Li},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={3143--3151},
  year={2022}
}

@article{he2024pruning,
  title={Pruning self-attentions into convolutional layers in single path},
  author={He, Haoyu and Cai, Jianfei and Liu, Jing and Pan, Zizheng and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{rao2021dynamicvit,
  title={Dynamicvit: Efficient vision transformers with dynamic token sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13937--13949},
  year={2021}
}

@inproceedings{
zhang2024how,
title={How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective},
author={Qiaozhe Zhang and Ruijie ZHANG and Jun Sun and Yingzhuang Liu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=IAAPhOLhcX}
}

@inproceedings{yu2022combinatorial,
  title={The combinatorial brain surgeon: pruning weights that cancel one another in neural networks},
  author={Yu, Xin and Serra, Thiago and Ramalingam, Srikumar and Zhe, Shandian},
  booktitle={International Conference on Machine Learning},
  pages={25668--25683},
  year={2022},
  organization={PMLR}
}

@inproceedings{wangntk,
  title={NTK-SAP: Improving neural network pruning by aligning training dynamics},
  author={Wang, Yite and Li, Dawei and Sun, Ruoyu},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{popescu2009multilayer,
  title={Multilayer perceptron and neural networks},
  author={Popescu, Marius-Constantin and Balas, Valentina E and Perescu-Popescu, Liliana and Mastorakis, Nikos},
  journal={WSEAS Transactions on Circuits and Systems},
  volume={8},
  number={7},
  pages={579--588},
  year={2009},
  publisher={World Scientific and Engineering Academy and Society (WSEAS) Stevens Point~â€¦}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{lei2016layer,
  title={Layer normalization},
  author={Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={ArXiv e-prints},
  pages={arXiv--1607},
  year={2016}
}

@article{muller2013quantum,
  title={On quantum R{\'e}nyi entropies: A new generalization and some properties},
  author={M{\"u}ller-Lennert, Martin and Dupuis, Fr{\'e}d{\'e}ric and Szehr, Oleg and Fehr, Serge and Tomamichel, Marco},
  journal={Journal of Mathematical Physics},
  volume={54},
  number={12},
  year={2013},
  publisher={AIP Publishing}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@inproceedings{renyi1961measures,
  title={On measures of entropy and information},
  author={R{\'e}nyi, Alfr{\'e}d},
  booktitle={Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics},
  volume={4},
  pages={547--562},
  year={1961},
  organization={University of California Press}
}

@article{xia2015learning,
  title={Learning similarity with cosine similarity ensemble},
  author={Xia, Peipei and Zhang, Li and Li, Fanzhang},
  journal={Information sciences},
  volume={307},
  pages={39--52},
  year={2015},
  publisher={Elsevier}
}

@book{von2013mathematische,
  title={Mathematische grundlagen der quantenmechanik},
  author={Von Neumann, John},
  volume={38},
  year={2013},
  publisher={Springer-Verlag}
}

@article{giraldo2014measures,
  title={Measures of entropy from data using infinitely divisible kernels},
  author={Giraldo, Luis Gonzalo Sanchez and Rao, Murali and Principe, Jose C},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={1},
  pages={535--548},
  year={2014},
  publisher={IEEE}
}

@article{
friedman2023the,
title={The Vendi Score: A Diversity Evaluation Metric for Machine Learning},
author={Dan Friedman and Adji Bousso Dieng},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=g97OHbQyk1},
note={}
}

@article{jalali2024information,
  title={An information-theoretic evaluation of generative models in learning multi-modal distributions},
  author={Jalali, Mohammad and Li, Cheuk Ting and Farnia, Farzan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@misc{llama3,
  author = {Meta},
  title = {LLaMA3},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/meta-llama/llama3}}
}

@misc{llama31,
  author = {Meta},
  title = {LLaMA3},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/meta-llama/llama3}}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{li2023sparse,
  title={E-sparse: Boosting the large language model inference through entropy-based n: M sparsity},
  author={Li, Yun and Niu, Lin and Zhang, Xipeng and Liu, Kai and Zhu, Jianchen and Kang, Zhanhui},
  journal={arXiv preprint arXiv:2310.15929},
  year={2023}
}

@article{lu2024spp,
  title={SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models},
  author={Lu, Xudong and Zhou, Aojun and Xu, Yuhui and Zhang, Renrui and Gao, Peng and Li, Hongsheng},
  journal={arXiv preprint arXiv:2405.16057},
  year={2024}
}

@article{ko2024distillm,
  title={Distillm: Towards streamlined distillation for large language models},
  author={Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
  journal={arXiv preprint arXiv:2402.03898},
  year={2024}
}

@article{egiazarian2024extreme,
  title={Extreme compression of large language models via additive quantization},
  author={Egiazarian, Vage and Panferov, Andrei and Kuznedelev, Denis and Frantar, Elias and Babenko, Artem and Alistarh, Dan},
  journal={arXiv preprint arXiv:2401.06118},
  year={2024}
}

@article{xu2024onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}

@inproceedings{
ding2023sparse,
title={Sparse Low-rank Adaptation of Pre-trained Language Models},
author={Ning Ding and Xingtai Lv and Qiaosen Wang and Yulin Chen and Bowen Zhou and Zhiyuan Liu and Maosong Sun},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=jxgz7FEqWq}
}

@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}

@article{lee2020layer,
  title={Layer-adaptive sparsity for the magnitude-based pruning},
  author={Lee, Jaeho and Park, Sejun and Mo, Sangwoo and Ahn, Sungsoo and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2010.07611},
  year={2020}
}

@article{fan2021layer,
  title={Layer-wise model pruning based on mutual information},
  author={Fan, Chun and Li, Jiwei and Ao, Xiang and Wu, Fei and Meng, Yuxian and Sun, Xiaofei},
  journal={arXiv preprint arXiv:2108.12594},
  year={2021}
}

@article{hu2024mpruner,
  title={MPruner: Optimizing Neural Network Size with CKA-Based Mutual Information Pruning},
  author={Hu, Seungbeom and Park, ChanJun and Ferraiuolo, Andrew and Ko, Sang-Ki and Kim, Jinwoo and Song, Haein and Kim, Jieung},
  journal={arXiv preprint arXiv:2408.13482},
  year={2024}
}

@article{bovza2024fast,
  title={Fast and Effective Weight Update for Pruned Large Language Models},
  author={Bo{\v{z}}a, Vladim{\'\i}r},
  journal={Transactions on Machine Learning Research},
  year={2024}
}

@article{liu2024alora,
  title={Alora: Allocating low-rank adaptation for fine-tuning large language models},
  author={Liu, Zequan and Lyn, Jiawen and Zhu, Wei and Tian, Xing and Graham, Yvette},
  journal={arXiv preprint arXiv:2403.16187},
  year={2024}
}

@inproceedings{
meng2024alps,
title={{ALPS}: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models},
author={Xiang Meng and Kayhan Behdin and Haoyue Wang and Rahul Mazumder},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=0lBx844upd}
}

@inproceedings{
ashkboos2024slicegpt,
title={Slice{GPT}: Compress Large Language Models by Deleting Rows and Columns},
author={Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=vXxardq6db}
}

@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={10865--10873},
  year={2024}
}

@article{tschannen2019mutual,
  title={On mutual information maximization for representation learning},
  author={Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K and Gelly, Sylvain and Lucic, Mario},
  journal={arXiv preprint arXiv:1907.13625},
  year={2019}
}

@article{bachman2019learning,
  title={Learning representations by maximizing mutual information across views},
  author={Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wang2020neural,
  title={Neural pruning via growing regularization},
  author={Wang, Huan and Qin, Can and Zhang, Yulun and Fu, Yun},
  journal={arXiv preprint arXiv:2012.09243},
  year={2020}
}

@inproceedings{lu2022learning,
  title={Learning pruning-friendly networks via frank-wolfe: One-shot, any-sparsity, and no retraining},
  author={Lu, Miao and Luo, Xiaolong and Chen, Tianlong and Chen, Wuyang and Liu, Dong and Wang, Zhangyang},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2406.02924},
  year={2024}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={7432--7439},
  year={2020}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  year={2021}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{zhang2023dynamic,
  title={Dynamic sparse no training: Training-free fine-tuning for sparse llms},
  author={Zhang, Yuxin and Zhao, Lirui and Lin, Mingbao and Sun, Yunyun and Yao, Yiwu and Han, Xingjia and Tanner, Jared and Liu, Shiwei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2310.08915},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{deepsparse,
  author = {NeuralMagic},
  title = {NeuralMagic DeepSparse Inference Engine},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/neuralmagic/deepsparse}}
}

@misc{nm-vllm,
  author = {NeuralMagic},
  title = {NeuralMagic nm-vllm Inference Engine},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/neuralmagic/nm-vllm}}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  journal={arXiv preprint arXiv:2310.05175},
  year={2023}
}

@article{guo2024ebft,
  title={EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs},
  author={Guo, Song and Wu, Fan and Zhang, Lei and Zheng, Xiawu and Zhang, Shengchuan and Chao, Fei and Shi, Yiyu and Ji, Rongrong},
  journal={arXiv preprint arXiv:2402.12419},
  year={2024}
}

@article{zheng2021information,
  title={An information theory-inspired strategy for automatic network pruning},
  author={Zheng, Xiawu and Ma, Yuexiao and Xi, Teng and Zhang, Gang and Ding, Errui and Li, Yuchao and Chen, Jie and Tian, Yonghong and Ji, Rongrong},
  journal={arXiv preprint arXiv:2108.08532},
  year={2021}
}

@article{virtanen2020scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others},
  journal={Nature methods},
  volume={17},
  number={3},
  pages={261--272},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={URL https://lmsys. org/blog/2023-03-30-vicuna},
  volume={3},
  number={5},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{zhu2023survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@inproceedings{agarwal2024policy,
  title={On-policy distillation of language models: Learning from self-generated mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Garea, Sabela Ramos and Geist, Matthieu and Bachem, Olivier},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{li2024nuteprune,
  title={NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models},
  author={Li, Shengrui and Han, Xueting and Bai, Jing},
  journal={arXiv preprint arXiv:2402.09773},
  year={2024}
}

@article{li2024lorap,
  title={LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models},
  author={Li, Guangyan and Tang, Yongqiang and Zhang, Wensheng},
  journal={arXiv preprint arXiv:2404.09695},
  year={2024}
}

@article{song2024sleb,
  title={SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks},
  author={Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon},
  journal={arXiv preprint arXiv:2402.09025},
  year={2024}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{ma2024affinequant,
  title={AffineQuant: Affine Transformation Quantization for Large Language Models},
  author={Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Ling, Feng and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Chao, Fei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2403.12544},
  year={2024}
}

@article{agarwal2023gkd,
  title={Gkd: Generalized knowledge distillation for auto-regressive sequence models},
  author={Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal={arXiv preprint arXiv:2306.13649},
  year={2023}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@inproceedings{gu2023minillm,
  title={MiniLLM: Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{zhang2023pruning,
  title={Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}

@article{guo2023compresso,
  title={Compresso: Structured pruning with collaborative prompting learns compact large language models},
  author={Guo, Song and Xu, Jiahang and Zhang, Li Lyna and Yang, Mao},
  journal={arXiv preprint arXiv:2310.05015},
  year={2023}
}

@article{chen2024compressing,
  title={Compressing large language models by streamlining the unimportant layer},
  author={Chen, Xiaodong and Hu, Yuxuan and Zhang, Jing},
  journal={arXiv preprint arXiv:2403.19135},
  year={2024}
}

@article{zhao2024apt,
  title={Apt: Adaptive pruning and tuning pretrained language models for efficient training and inference},
  author={Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing},
  journal={arXiv preprint arXiv:2401.12200},
  year={2024}
}

@article{xu2024besa,
  title={BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation},
  author={Xu, Peng and Shao, Wenqi and Chen, Mengzhao and Tang, Shitao and Zhang, Kaipeng and Gao, Peng and An, Fengwei and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2402.16880},
  year={2024}
}

@article{chen2023lorashear,
  title={Lorashear: Efficient large language model structured pruning and knowledge recovery},
  author={Chen, Tianyi and Ding, Tianyu and Yadav, Badal and Zharkov, Ilya and Liang, Luming},
  journal={arXiv preprint arXiv:2310.18356},
  year={2023}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{xu2023compress,
  title={Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt},
  author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
  journal={arXiv preprint arXiv:2305.11186},
  year={2023}
}

@inproceedings{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@inproceedings{gretton2005measuring,
  title={Measuring statistical dependence with Hilbert-Schmidt norms},
  author={Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  booktitle={International conference on algorithmic learning theory},
  pages={63--77},
  year={2005},
  organization={Springer}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{robert1976unifying,
  title={A unifying tool for linear multivariate statistical methods: the RV-coefficient},
  author={Robert, Paul and Escoufier, Yves},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={25},
  number={3},
  pages={257--265},
  year={1976},
  publisher={Oxford University Press}
}

@article{lorenzo2006tucker,
  title={Tucker's congruence coefficient as a meaningful index of factor similarity},
  author={Lorenzo-Seva, Urbano and Ten Berge, Jos MF},
  journal={Methodology},
  volume={2},
  number={2},
  pages={57--64},
  year={2006},
  publisher={Hogrefe \& Huber Publishers}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{zheng2022neural,
  title={Neural architecture search with representation mutual information},
  author={Zheng, Xiawu and Fei, Xiang and Zhang, Lei and Wu, Chenglin and Chao, Fei and Liu, Jianzhuang and Zeng, Wei and Tian, Yonghong and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11912--11921},
  year={2022}
}

@article{hubara2021accelerated,
  title={Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={21099--21111},
  year={2021}
}

@article{sun2021dominosearch,
  title={DominoSearch: Find layer-wise fine-grained N: M sparse schemes from dense neural networks},
  author={Sun, Wei and Zhou, Aojun and Stuijk, Sander and Wijnhoven, Rob and Nelson, Andrew O and Corporaal, Henk and others},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20721--20732},
  year={2021}
}