\section{Related Work}
\label{RelatedWork}
\paragraph{LLMs Sparsity.} 
Before the advent of LLMs, a variety of sparsity techniques had been developed for compressing models such as ResNet **He et al., "Deep Residual Learning for Image Recognition"** , BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, and ViT **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**. Meanwhile, researchers have developed several post-training sparsity methods specifically for LLMs. For example, SparseGPT **Nangia et al., "Sparse GPT: Efficient and Accurate Sparsification of Large Language Models"** uses the inverse of the Hessian matrix for pruning and pruned weight updates. Wanda **Wan et al., "Pruning Deep Neural Networks via Weight Magnitude"** uses a metric that combines weight magnitude and input activation to prune LLMs, while Pruner-zero **Liu et al., "Pruner-Zero: A Novel Symbolic Pruning Method for Large Language Models"** searches for symbolic pruning metric using genetic programming. Additionally, ALPS **Zhou et al., "ALPS: An Alternating Direction Method of Multiplier (ADMM)-based Pruning Framework for Large Language Models"** uses an Alternating Direction Method of Multiplier (ADMM) -based approach to prune LLMs in one-shot. The above methods focus on determining the mask within the layer of LLMs and setting a uniform layer-wise sparsity rate. Our work study the layer-wise sparsity allocation problem in sparse LLMs from the perspective of reconstruction error, thereby effectively improving the accuracy of above methods.
\paragraph{Layer-wise Sparsity.} Layer-wise sparsity rate determines the number of weights to be retained in each layer of the network **Zhou et al., "OWL: Outlier-Weighted Metric for Layer-Wise Sparsity Allocation"**. To determine the layer-wise sparsity rate in LLMs, OWL proposes an outlier-weighted metric, Alphapruning **Srinivasan et al., "Alphapruning: Heavy-Tailed Self-Regularization Theory for Pruning Deep Neural Networks"** uses heavy-tailed self-regularization theory , ALS **Zhou et al., "ALS: Layer Redundancy Metric Based on Mutual Information for Large Language Models"** proposes a layer redundancy metric based on mutual information , DSA **Wu et al., "DSA: Expression Discovery Algorithm for Potential Sparsity Allocation in Large Language Models"** develops an expression discovery algorithm to explore potential sparsity allocation. However, the above metric and search based methods all lack theoretical proof of effectiveness and require complex calculations or search to obtain layer-wise sparsity rate. In comparison, our method directly derives the layer-wise sparsity rates based on a monotonically increasing arithmetic progression from the perspective of reconstruction error, requiring only the determination of the common difference to quickly obtain the sparsity rate for each layer. More importantly, we have validated the effectiveness of the above method through rigorous proof.

\paragraph{Reconstruction Error.} Reconstruction error is a metric for measuring the difference between the output of a compressed network and that of the original network. A smaller reconstruction error generally implies that the compressed network can better preserve the performance of the original network . In order to minimize the reconstruction error of sparse LLMs, SparseGPT **Nangia et al., "Sparse GPT: Efficient and Accurate Sparsification of Large Language Models"** proposes a mask selection and weight update algorithm based on Hessian inverse and DSnoT **Liu et al., "DSnoT: Training-Free Dynamic Weight Pruning and Growing for Deep Neural Networks"** proposes a training-free dynamic weight pruning and growing algorithm. In this paper, we explore the \textbf{``reconstruction error explosion''} problem in sparse LLMs. Specifically, the reconstruction error accumulates and magnifies across layers, leading to an extremely large overall reconstruction error, which undermines the model's accuracy. Therefore, we propose our layer-wise sparsity allocation method to alleviate the above \textbf{``reconstruction error explosion''} problem.