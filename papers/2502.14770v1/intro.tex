\section{Introduction}\label{sec:intro}
Large Language Models (LLMs) have demonstrated outstanding capabilities in various natural language processing tasks~\cite{llama3, yang2024qwen2, liu2024deepseek}. However, their vast number of parameters and high computational demands present significant challenges to model deployment, impeding further applications~\cite{zhu2024survey, wang2024model}. Network sparsity~\cite{rao2021dynamicvit, paul2022unmasking, wangntk} methods remove less important parameters from LLMs, enabling model compression without sacrificing performance. This can significantly reduce the model's memory footprint and computational complexity~\cite{li2024lorap, an2024fluctuation}. Existing sparsity methods for LLMs, such as SparseGPT~\cite{frantar2023sparsegpt} and Wanda~\cite{sun2023simple}, adopt a post-training approach which prune all weights in one-shot and can obtain sparse LLMs without the need for additional fine-tuning. 

However, these sparsity methods set a uniform layer-wise sparsity rate for different layers of LLMs, without considering the varying importance of each layer, which harms the accuracy of sparse LLMs. To address the above issue, many studies have proposed various methods to determine the layer-wise sparsity rate of LLMs. Based on their methodological designs, we categorize these approaches into two main groups:

\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/reconstruction_error.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.69\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/framwork.pdf}
    \end{minipage}
    \caption{(Left) shows the comparison of reconstruction error among different layer-wise sparsity methods. All methods face the problem of \textbf{``reconstruction error explosion''}; however, our method achieves lower reconstruction error compared to other methods. (Right) presents a comparison between our method and other layer-wise sparsity methods. The metric-based method calculates the importance of each layer to obtain the sparsity rate. However, this method is heuristically designed by human experts and is not optimal. And the search-based method requires a large number of iterative searches, which is time-consuming. In contrast, we analyze the causes of \textbf{``reconstruction error explosion''} from a theoretical perspective, and deduce theoretically that using a monotonically increasing arithmetic progression to determine the layer-wise sparsity rate can alleviate the problem of \textbf{``reconstruction error explosion''}.}
    \label{fig:framework}
\end{figure*}


\textbf{Metric based methods.} These methods determine the importance of each layer of LLMs through hand-crafted metrics, thereby obtaining the sparsity rate of each layer. For example, OWL~\cite{yin2023outlier} proposes an outlier weighted layer importance metric. By setting the sparsity rate to be inversely proportional to the outlier ratio, it effectively protects the layers with a higher ratio of outliers. AlphaPruning~\cite{lu2024alphapruning} utilizes the heavy-tailed self- regularization theory~\cite{martin2019traditional}, especially the shape of the empirical spectral density~\cite{martin2021predicting} of the weight matrix, to determine the importance of each layer of LLMs. ALS~\cite{li2024adaptive} proposes an importance metric based on mutual information~\cite{tschannen2019mutual}, and sets a higher sparsity rate for layers with higher mutual information. Although these metrics have proven their effectiveness experimentally, however manually designing metric requires extensive validation and complex calculations are needed to obtain the sparsity rate of each layer. And most importantly, most of these methods lack theoretical analysis, making it impossible to ensure that the solutions obtained are optimal.

\textbf{Search based methods.} In addition to these heuristics designed by humans, recently, there have also been some methods that adopt a search-based approach to determine the layer-wise sparsity rate of LLMs. For example, DSA~\cite{li2024discovering} develops an expression discovery framework to explore potential sparsity rate allocation strategies and obtains layer-wise sparsity allocation function by searching. However, the evolutionary search method employed by DSA requires 2000 iterations. For large-scale LLMs with a vast number of parameters, this demands a search process lasting several days, which incurs a significant cost. In addition, in order to obtain the final allocation function, DSA designs a complex search space and process, which means the effectiveness of the method heavily depends on the experience of human experts.

In this paper, we rethink the approach of determining the layer-wise sparsity rate of LLMs, and derive the layer-wise sparsity rate of LLMs from reconstruction error perspective. Specifically, we first prove that increasing the sparsity rate leads to an increase in the reconstruction error of the corresponding layer. Additionally, we show that an increase in the reconstruction error of one layer causes an increase in the reconstruction error of subsequent layers. This implies that increasing the sparsity rate of earlier layers not only increases the reconstruction error of the corresponding layer, but also leads to an increase in the reconstruction errors of all subsequent layers. As the network propagates forward, the reconstruction errors accumulate, causing the total reconstruction error to grow significantly, thus causing \textbf{``reconstruction error explosion''} (See the left in Figure \ref{fig:framework}).

Through the above theoretical analysis, we provide a simple yet effective rule for determining the layer-wise sparsity rates of LLMs: \textit{the sparsity rate should be lower in earlier layers, and the layer-wise sparsity rates should follow a increasing pattern}. This approach effectively alleviates the issue of \textbf{``reconstruction error explosion''}, resulting in a well-performing sparse LLM. To achieve this, we use a monotonically increasing arithmetic progression to determine the sparsity rates for all layers of LLMs and employ grid search to find the common difference of the arithmetic progression. Since the range of valid values for the common difference is narrow, our search is highly efficient, and after only a few attempts, we can determine the common difference that yields the best accuracy.

Furthermore, we prove that the total reconstruction error obtained from the monotonically increasing sparsity scheme is less than that of any non-monotonically increasing sparsity scheme. This indicates that our method is theoretically close to the optimal solution. Additionally, we compare our sparsity rate scheme with the optimal solution obtained through Bayesian search~\cite{wu2019hyperparameter}, we find that our scheme is close to the optimal solution found by search. This indicates that our method is empirically close to the optimal solution.

To evaluate the effectiveness of our ATP \footnote{Determining layer-wise sparsity for LLMs through \textbf{A} \textbf{T}heoretical \textbf{P}erspective (\textbf{ATP}).} method, we conduct extensive experiments on LLMs of various architectures, with parameter counts ranging from 6.7 billion to 70 billion. Our evaluation metrics include perplexity, average accuracy across seven zero-shot datasets, and performance on arithmetic and knowledge reasoning tasks. Our ATP method demonstrate substantial improvements over existing post-training sparsity techniques, significantly surpassing other layer-wise sparsity methods. Notably, ATP reduce the perplexity of the 70$\%$ sparse LLaMA2-7B pruned using Wanda~\cite{sun2023simple} by 52.10 and increase average zero-shot accuracy by 10.50$\%$, outperforming the state-of-the-art AlphaPruning method~\cite{lu2024alphapruning} by 6.71 and 2.46$\%$, respectively. Additionally, ATP achieved 2.63$\times$ and 2.23$\times$ speedups on CPU and GPU, respectively, and require only $18$ minutes to compute layer-wise sparsity rates. Furthermore, we evaluate ATP's enhancements on various compression techniques, including N:M sparsity, structured pruning, and network quantization, as well as its benefits for sparse multimodal and sparse vision models. These experimental results clearly demonstrate that ATP provides substantial performance improvements for compressed models.