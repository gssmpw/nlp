\section{Methodology}
\paragraph{Notation.} In this paper, we use bold typeface indicates  matrices (\emph{e.g.,}$\boldsymbol{W, X}$) and calligraphic font represents loss functions or models (\emph{e.g.,}$\mathcal{L}, \mathcal{M}$). 

\subsection{Preliminaries}
Without loss of generality, we take each layer in the LLMs as the basic unit of analysis. These layers contain modules such as Attention \citep{vaswani2017attention}, MLP \citep{popescu2009multilayer}, LayerNorm \citep{lei2016layer}, and residual connections \citep{he2016deep}, etc. Consider an LLM composed of $L$ layers, we define the reconstruction error of the $i$-th layer ($i = 1, 2, \cdots, L$) as follows:
\begin{equation}
    \mathcal{L}(\boldsymbol{W}_i, \boldsymbol{X}_i) = \left\lVert \boldsymbol{W}_i\boldsymbol{X}_i - \widetilde{\boldsymbol{W}}_{i}\widetilde{\boldsymbol{X}}_{i}\right\rVert^2_F
\end{equation}
where \(\boldsymbol{W}_i \in \mathbb{R}^{c_{out}\times c_{in}}\) and \(\boldsymbol{X}_i \in \mathbb{R}^{c_{in}\times d}\) are the weights and input of the $i$-th layer respectively. \(\widetilde{\boldsymbol{W}}_{i}\) and \(\widetilde{\boldsymbol{X}}_{i}\) are the corresponding sparse versions, where \(c_{in}\) and \(c_{out}\) represent the number of input and output channels, \(d\) is the hidden dimension, and \(\left\lVert \cdot \right\rVert_F\) is Frobenius norm. 

Existing post-training sparsity methods all attempt to minimize the reconstruction error of sparse LLMs (\emph{e.g.,} SparseGPT and Wanda) and a large amount of experimental evidence in the these papers indicates that a sparse LLM with good accuracy often has a low reconstruction error. 

In the next section, we reveal that the existing post-training sparsity methods all have the problem of \textbf{``reconstruction error explosion''}.
\subsection{\textbf{``Reconstruction Error Explosion''
} in Sparse LLMs}\label{sec:error_analysis}
First, we analyze the relationship between the sparsity rate and the reconstruction error. Briefly, a higher sparsity rate leads to a higher reconstruction error. Formally, we propose Theorem \ref{theorem1}.
\begin{theorem}[\textbf{Effect of increased sparsity on reconstruction error}]
\label{theorem1}
Increasing the sparsity of the weights in the \(i\)-th layer will lead to an increase in the reconstruction error of this layer.
\end{theorem}
\begin{proof}[Proof of Theorem \ref{theorem1}]
We only consider the effect of sparse weights on the reconstruction error, and we ignore the error caused by the input at this time. Therefore, we consider the impact of the sparse weights of the \(i\)-th layer on the reconstruction error when the input is the same. That is, \(\widetilde{\boldsymbol{X}}_{i}=\boldsymbol{X}_{i}\). Then, the reconstruction error of the \(i\)-th layer is expressed as:
\begin{equation}
    \mathcal{L} = \left\lVert \boldsymbol{W}_i\boldsymbol{X}_i - \widetilde{\boldsymbol{W}}_{i}\boldsymbol{X}_{i}\right\rVert^2_F
\end{equation}
Consider two weights \(\widetilde{\boldsymbol{W}}_{i}^{(1)}\) and \(\widetilde{\boldsymbol{W}}_{i}^{(2)}\) of different sparsity, such that \(\widetilde{\boldsymbol{W}}_{i}^{(1)}\) has lower sparsity (i.e., has fewer zero elements). The difference in reconstruction error corresponding to these two sparse weights is:
\begin{equation}
\begin{aligned}
& \mathcal{L}^{(1)}-\mathcal{L}^{(2)} = \left\lVert \boldsymbol{W}_i\boldsymbol{X}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)}\boldsymbol{X}_{i}\right\rVert^2_F\\
& - \left\lVert \boldsymbol{W}_i\boldsymbol{X}_i - \widetilde{\boldsymbol{W}}_{i}^{(2)}\boldsymbol{X}_{i}\right\rVert^2_F
= \left\lVert \boldsymbol{W}_i\boldsymbol{X}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)}\boldsymbol{X}_{i}\right\rVert^2_F\\
& - \left\lVert (\boldsymbol{W}_i\boldsymbol{X}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)}\boldsymbol{X}_{i})+(\widetilde{\boldsymbol{W}}_{i}^{(1)}\boldsymbol{X}_{i} - \widetilde{\boldsymbol{W}}_{i}^{(2)}\boldsymbol{X}_{i})\right\rVert^2_F \\
& = -2\langle(\boldsymbol{W}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)})\boldsymbol{X}_{i}, (\widetilde{\boldsymbol{W}}_{i}^{(1)} - \widetilde{\boldsymbol{W}}_{i}^{(2)})\boldsymbol{X}_{i} \rangle_F \\
& - \left\lVert \widetilde{\boldsymbol{W}}_{i}^{(1)}\boldsymbol{X}_{i} - \widetilde{\boldsymbol{W}}_{i}^{(2)}\boldsymbol{X}_{i}\right\rVert^2_F\\
\end{aligned}
\end{equation}

The first term of the inner product, \((\boldsymbol{W}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)})\boldsymbol{X}_{i}\), represents the error introduced by sparsifying the original weight matrix \(\boldsymbol{W}_i\) to obtain a less sparse version \(\widetilde{\boldsymbol{W}}_{i}^{(1)}\). The second term, \((\widetilde{\boldsymbol{W}}_{i}^{(1)} - \widetilde{\boldsymbol{W}}_{i}^{(2)})\boldsymbol{X}_{i}\), quantifies the additional error resulting from further sparsifying \(\widetilde{\boldsymbol{W}}_{i}^{(1)}\) to derive an even sparser matrix \(\widetilde{\boldsymbol{W}}_{i}^{(2)}\). Since both errors \((\boldsymbol{W}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)})\boldsymbol{X}_{i}\) and \((\widetilde{\boldsymbol{W}}_{i}^{(1)} - \widetilde{\boldsymbol{W}}_{i}^{(2)})\boldsymbol{X}_{i}\) are generated through the same sparsification method, they point in similar directions within the vector space. This alignment ensures that their Frobenius inner product satisfies \(\langle(\boldsymbol{W}_i - \widetilde{\boldsymbol{W}}_{i}^{(1)})\boldsymbol{X}_{i}, (\widetilde{\boldsymbol{W}}_{i}^{(1)} - \widetilde{\boldsymbol{W}}_{i}^{(2)})\boldsymbol{X}_{i} \rangle_F > 0\). And since \(\left\lVert \widetilde{\boldsymbol{W}}_{i}^{(1)}\boldsymbol{X}_{i} - \widetilde{\boldsymbol{W}}_{i}^{(2)}\boldsymbol{X}_{i}\right\rVert^2_F>0 \), therefore:
\begin{equation}
\mathcal{L}^{(1)}-\mathcal{L}^{(2)} < 0
\end{equation}
Therefore, the reconstruction error $\mathcal{L}^{(1)}$ for lower sparsity is smaller than $\mathcal{L}^{(2)}$ for higher sparsity. In other words, increasing the sparsity will lead to an increase in the reconstruction error of this layer. So, we have completed the proof of the above theorem.
\end{proof}

On the other hand, we find that there is an cumulative effect of reconstruction error during the sparsification process. It is manifested as the reconstruction error in LLMs showing an increasing trend due to the influence of previous sparse layers. In other words, if the reconstruction error of the previous layer increases, the reconstruction error of the subsequent layers will also increase accordingly. Formally, we propose Theorem \ref{theorem2}. 
\begin{theorem}[\textbf{The cumulative effect of reconstruction error}]\label{theorem2}
When the reconstruction error of the \(i\)-th layer increases, it leads to an increase in the lower bound of the reconstruction error for the \((i+1)\)-th layer.
\end{theorem}

To prove Theorem \ref{theorem2}, we need to define the following lemma:
\begin{lemma}\label{lemma1} Let \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) and \(\boldsymbol{B} \in \mathbb{R}^{n \times p}\) be arbitrary matrices. Then, it holds that \( \left\lVert \boldsymbol{AB} \right\rVert_F^2 \geq \sigma_{\min}^2(\boldsymbol{A}) \left\lVert \boldsymbol{B} \right\rVert_F^2\), where \(\sigma_{\min}(\boldsymbol{A})\) denotes the smallest non-zero singular value of \(\boldsymbol{A}\).

\end{lemma}
The proof of the Lemma \ref{lemma1} can be found in Appendix \ref{sec:provelemma}. Now we formally prove Theorem \ref{theorem2}.

\begin{proof}[Proof of Theorem \ref{theorem2}]
The reconstruction error for the \((i+1)\)-th layer can be expressed as:
\begin{equation}
\begin{aligned}
&\mathcal{L}(\boldsymbol{W}_{i+1}, \boldsymbol{X}_{i+1}) 
= \left\lVert \boldsymbol{W}_{i+1}\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{W}}_{i+1}\widetilde{\boldsymbol{X}}_{i+1} \right\rVert_F^2 \\
&= \left\lVert (\boldsymbol{W}_{i+1} - \widetilde{\boldsymbol{W}}_{i+1})\boldsymbol{X}_{i+1} + \widetilde{\boldsymbol{W}}_{i+1}(\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{X}}_{i+1}) \right\rVert_F^2 \\
&= \left\lVert (\boldsymbol{W}_{i+1} - \widetilde{\boldsymbol{W}}_{i+1})\boldsymbol{X}_{i+1} \right\rVert_F^2 \\
&+ \left\lVert \widetilde{\boldsymbol{W}}_{i+1}(\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{X}}_{i+1}) \right\rVert_F^2 \\
& + 2\text{tr}(( \boldsymbol{W}_{i+1} - \widetilde{\boldsymbol{W}}_{i+1})\boldsymbol{X}_{i+1})^{\top}(\widetilde{\boldsymbol{W}}_{i+1}(\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{X}}_{i+1}))
\end{aligned}
\end{equation}
Since the third term is the trace of the product of two matrices, the first and second term is the square of the Frobenius norm, and considering that the weights and inputs in LLMs are matrices with large dimensions, the magnitude of the first and second term is much greater than the third term, so we ignore the third term and get:
\begin{equation}
\begin{aligned}
&\mathcal{L}(\boldsymbol{W}_{i+1}, \boldsymbol{X}_{i+1}) \approx \left\lVert (\boldsymbol{W}_{i+1} - \widetilde{\boldsymbol{W}}_{i+1})\boldsymbol{X}_{i+1} \right\rVert_F^2 \\
&+ \left\lVert \widetilde{\boldsymbol{W}}_{i+1}(\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{X}}_{i+1}) \right\rVert_F^2
\end{aligned}
\end{equation}
Since \(\left\lVert (\boldsymbol{W}_{i+1} - \widetilde{\boldsymbol{W}}_{i+1})\boldsymbol{X}_{i+1} \right\rVert_F^2 > 0 \). Therefore:
\begin{equation}
\mathcal{L}(\boldsymbol{W}_{i+1}, \boldsymbol{X}_{i+1}) >  \left\lVert \widetilde{\boldsymbol{W}}_{i+1}(\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{X}}_{i+1}) \right\rVert_F^2 
\end{equation}
According to Lemma \ref{lemma1}, we get:
\begin{equation}
\begin{aligned}
&\mathcal{L}(\boldsymbol{W}_{i+1}, \boldsymbol{X}_{i+1}) >  \sigma_{\min}^2(\widetilde{\boldsymbol{W}}_{i+1}) \left\lVert (\boldsymbol{X}_{i+1} - \widetilde{\boldsymbol{X}}_{i+1}) \right\rVert_F^2 \\
&= \sigma_{\min}^2(\widetilde{\boldsymbol{W}}_{i+1}) \left\lVert (\boldsymbol{W}_{i}\boldsymbol{X}_{i} - \widetilde{\boldsymbol{W}}_{i}\widetilde{\boldsymbol{X}}_{i}) \right\rVert_F^2 \\
&=\sigma_{\min}^2(\widetilde{\boldsymbol{W}}_{i+1})\mathcal{L}(\boldsymbol{W}_{i}, \boldsymbol{X}_{i})
\end{aligned}
\end{equation}
Since \(\sigma_{\min}^2(\widetilde{\boldsymbol{W}}_{i+1}) > 0\), we have proven that the increase of the reconstruction error of the \(i\)-th layer will lead to the increase of the lower bound of the reconstruction error of the \((i+1)\)-th layer.
\end{proof}

Theorem \ref{theorem2} shows that an increase in the reconstruction error of the previous layer in a sparse LLM usually leads to a further increase in the lower bound of the reconstruction error of the subsequent layer. In practice, this often means that an increase in the reconstruction error of the previous layer will lead to an increase in the reconstruction error of the subsequent layer. We have also observed this phenomenon in the left of Figure \ref{fig:framework}. We can see that when the reconstruction error of the earlier layers is smaller, the reconstruction error of the subsequent layers is also smaller. Conversely, when the reconstruction error of the earlier layers is larger, the reconstruction error of the subsequent layers is also larger.

According to Theorems \ref{theorem1} and \ref{theorem2}, we can easily get the following Theorem \ref{theorem3}:
\begin{theorem}[\textbf{Impact of the sparsity of the previous layer on the reconstruction error of the next layer.}]\label{theorem3}
Increasing the sparsity of the \(i\)-th layer will lead to an increase in the lower bound of the reconstruction error of the \((i+1)\)-th layer.
\end{theorem}
\begin{proof}[Proof of Theorem \ref{theorem3}]
According to Theorems \ref{theorem1} and \ref{theorem2}, we have the following relationship:
\begin{equation}
\begin{aligned}
&\text{sparsity of layer } i \uparrow \Longrightarrow \mathcal{L}(\boldsymbol{W}_i, \boldsymbol{X}_i) \uparrow \\
&\Longrightarrow \mathcal{L}(\boldsymbol{W}_{i+1}, \boldsymbol{X}_{i+1}) > \sigma_{\min}^2(\widetilde{\boldsymbol{W}}_{i+1}) \mathcal{L}(\boldsymbol{W}_i, \boldsymbol{X}_i) \uparrow.
\end{aligned}
\end{equation}
Therefore, we complete the proof of Theorem \ref{theorem3}.
\end{proof}

To summarize all the above, we can get the following logical chain of our paper: sparsity rate of \(1\)-st layer \(\propto\) reconstruction error of \(1\)-st layer \(\propto\) reconstruction error of \(L\)-th layer \(\propto\) total error \(\propto\) accuracy loss. That is, when the sparsity rate of the \(1\)-st layer increases, it will lead to an increase in the reconstruction error of this layer. As the reconstruction error accumulates continuously from the \(1\)-st layer to the \(L\)-th layer, the reconstruction error of the \(L\)-th layer also increases accordingly. Then, the reconstruction errors of all layers of the model will exhibit an explosion phenomenon, resulting in a serious decline in the accuracy of the sparse model. We refer to the above phenomenon as the \textbf{``reconstruction error explosion''}. This phenomenon can be observed on the left side of Figure \ref{fig:framework}.

From the above theoretical analysis, we understand that the earlier layers are more important than the later layers. Setting a lower sparsity rate for the previous layers helps alleviate the problem of \textbf{``reconstruction error explosion''}. In the next section, we will introduce our method of determining the layer-wise sparsity rate in detail.

\subsection{Determining Layer-wise Sparsity Rates for LLMs}\label{Layer-wiseSparsity}
According to the theorem in Section \ref{sec:error_analysis}, the reconstruction error in sparse LLMs have \textbf{``reconstruction error explosion''} problem. Specifically, the error from the earlier layers will cause an increase in the error of the later layers. When the error from all earlier layers are accumulated, it will lead to a sharp increase in the error of the later layers. Meanwhile, this can lead to an increase in the total reconstruction error of sparse LLMs, thereby damaging the final accuracy of the sparse LLMs. Therefore, in order to mitigate the negative impact of the \textbf{``reconstruction error explosion''} of reconstruction errors on sparse LLMs, we can set the earlier layers to have lower sparsity and the later layers to have higher sparsity. Therefore, we propose to determine the sparsity rate of each layer in LLMs according to the following monotonically increasing arithmetic progression:
\begin{equation}\label{eq:sparsityrates}
s_i = S-\frac{\beta(L-1)}{2}+ \beta \times (i-1),
\quad
i=1,2,\dots,L
\end{equation}
where $s_i$ is the sparsity rate (fraction of zero entries) of the $i$-th layer, $L$ is the total layer number of LLMs, and $S$ is the average sparsity rate of all layers. $\beta$ is a hyperparameter that controls the degree of difference in the sparsity rate of each layer of LLMs. The above formula means that we only need to determine the hyperparameter $\beta$ to get the sparsity rate of each layer of LLMs.

We use grid search \citep{jimenez2008finding} to determine $\beta$ for sparse LLMs. Specifically, since $0\le s_0, s_L\le 1$ and considering the relatively low sparsity rate set for the earlier layers, the arithmetic progression should be increasing. Therefore, we can deduce that the possible range of values for $\beta$ is $0 < \beta \le \min(\frac{2S}{L-1}, \frac{2(1-S)}{L-1})$. This is a small range for a sparse LLMs. For example, for a LLaMA3-8B \citep{llama3} model with 32 layers and the average sparsity rate is $S=0.7$, then the range is $0 < \beta \le 0.019$. In order to find the optimal value of $\beta$ within the above  range, we adopt an grid search method with a step size of $0.002$. The goal is to find the value of $\beta$ that minimizes perplexity of the sparse LLMs on the WikiText-2 \citep{merity2016pointer} dataset. Since the reasonable range of $\beta$ is very small, this ensures that we can find the optimal $\beta$ very quickly. For example, for $0 < \beta \leq 0.019$, we only need to make $9$ attempts. Even for the largest 70B model, the reasonable range of $\beta$ is $0 < \beta \leq 0.0075$, and only $3$ attempts are required in this case. We present our ATP approach in Algorithm \ref{alg}.

Although the above method of determining the layer-wise sparsity rate of LLMs by a monotonically increasing arithmetic progression is very simple, our theoretical analysis in Sec. \ref{sec:AnalysisSparsityMethod} fully proves its rationality, and we have fully demonstrated through a large number of experiments in Sec. \ref{sec:experiments} that our method can effectively improve the accuracy of existing post-training sparsity methods and significantly outperforms current layer-wise sparsity methods.

\subsection{Analysis of the Proposed Determining Sparsity Method}\label{sec:AnalysisSparsityMethod}
We propose the following theorem to prove that the method for determining the layer-wise sparsity rate proposed in Eq. \ref{eq:sparsityrates} is theoretically close to the optimal solution:
\begin{theorem}\label{theorem4}
The total reconstruction error obtained from the monotonically increasing sparsity scheme proposed in Eq. \ref{eq:sparsityrates} is strictly less than that obtained from any non-monotonically increasing sparsity scheme.
\end{theorem}
The proof of the above theorem is detailed in Appendix \ref{sec:provetheorem4}. 

In addition, we compare the layer-wise sparsity rates determined by Eq. \ref{eq:sparsityrates} with those obtained by Bayesian search in Sec. \ref{sec:AblationStudy}. The experimental results show that our method is close to the optimal solution obtained by Bayesian search. Moreover, since our method doesn't require a long time search, it is highly efficient compared to Bayesian search.

All in all, the above theoretical analysis and experimental validation demonstrate the superiority of our method, indicating that the sparsity allocation scheme proposed in Eq. \ref{eq:sparsityrates} is near-optimal both theoretically and empirically.