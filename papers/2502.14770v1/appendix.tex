\section{Limitation and Future Work}
In this paper, we derive that layer-wise sparsity rates should gradually increase based on the reconstruction error analysis and determine these rates using a monotonically increasing arithmetic progression. Extensive experiments validate the effectiveness of our method. However, the arithmetic progression configuration may not be optimal, and we plan to explore more diverse sparsity rate schemes in the future. Additionally, while our method significantly enhances the accuracy of existing post-training sparsity techniques for LLMs, there remains a performance gap compared to lossless sparsification, particularly under high sparsity conditions. In future work, we aim to develop more advanced post-training sparsity methods to further improve the accuracy of sparse LLMs.

\section{ATP Algorithm}
\begin{algorithm}[H]
\caption{Using ATP method to obtain sparse LLMs}
\label{alg}
\KwIn{Dense LLMs $\mathcal{M}_{dense}$, average sparsity rate $S$}
\KwOut{Sparse LLMs $\mathcal{M}_{sparse}$}
Use grid search to get $\beta$ in Eq. \ref{eq:sparsityrates};

Determine layer-wise sparsity rate according to Eq. \ref{eq:sparsityrates};

Combine with post-training sparsity method (\emph{e.g.,} SpaeseGPT or Wanda) to obtain sparsity mask $\mathbf{W}$;

Apply $\mathbf{W}$ to $\mathcal{M}_{dense}$ yields $\mathcal{M}_{sparse}$.
\end{algorithm}

\section{Proof of Lemma \ref{lemma1}}\label{sec:provelemma}
\begin{proof}[Proof of Lemma \ref{lemma1}]
Let \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) and \(\boldsymbol{B} \in \mathbb{R}^{n \times p}\) be any matrices. Consider the singular value decomposition (SVD) of \(\boldsymbol{A}\):
\begin{equation}
\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^T
\end{equation}
where:
\begin{itemize}
    \item \(\boldsymbol{U} \in \mathbb{R}^{m \times m}\) is an orthogonal matrix (\(\boldsymbol{U}^T \boldsymbol{U} = \boldsymbol{I}\)),
    \item \(\boldsymbol{V} \in \mathbb{R}^{n \times n}\) is an orthogonal matrix (\(\boldsymbol{V}^T \boldsymbol{V} = \boldsymbol{I}\)), and
    \item \(\boldsymbol{\Sigma} \in \mathbb{R}^{m \times n}\) is a diagonal matrix with non-negative singular values \(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_{\min(m,n)} \geq 0\) on the diagonal.
\end{itemize}


Substituting the SVD of \(\boldsymbol{A}\) into the expression for \(\boldsymbol{AB}\), we have:
\begin{equation}
\boldsymbol{AB} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^T \boldsymbol{B}
\end{equation}

Therefore,
\begin{equation}
\left\lVert \boldsymbol{AB} \right\rVert_F^2 = \left\lVert \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F^2
\end{equation}

Since \(\boldsymbol{U}\) is an orthogonal matrix, the Frobenius norm is invariant under orthogonal transformations. Specifically:
\begin{equation}
\left\lVert \boldsymbol{U} \boldsymbol{X} \right\rVert_F = \left\lVert \boldsymbol{X} \right\rVert_F \quad \forall \ \boldsymbol{X}
\end{equation}

Applying this property:
\begin{equation}
\left\lVert \boldsymbol{AB} \right\rVert_F^2 = \left\lVert \boldsymbol{\Sigma} \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F^2
\end{equation}

The matrix \(\boldsymbol{\Sigma}\) is diagonal with singular values \(\sigma_i\) on the diagonal. Let \(\sigma_{\min} = \sigma_{\min}(\boldsymbol{A})\) denote the smallest singular value of \(\boldsymbol{A}\). Then, for any matrix \(\boldsymbol{X}\), we have:
\begin{equation}
\boldsymbol{\Sigma} \boldsymbol{X} \geq \sigma_{\min} \boldsymbol{X}
\end{equation}
in the sense that each singular value scales the corresponding component of \(\boldsymbol{X}\).

Therefore, applying this to our case:
\begin{equation}
\left\lVert \boldsymbol{\Sigma} \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F^2 \geq \sigma_{\min}^2 \left\lVert \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F^2
\end{equation}
This inequality holds because scaling each component by at least \(\sigma_{\min}\) results in the squared norm being scaled by at least \(\sigma_{\min}^2\).

Similarly to \(\boldsymbol{U}\), the orthogonal matrix \(\boldsymbol{V}\) preserves the Frobenius norm:
\begin{equation}
\left\lVert \boldsymbol{V}^T \boldsymbol{X} \right\rVert_F = \left\lVert \boldsymbol{X} \right\rVert_F \quad \forall \ \boldsymbol{X}
\end{equation}

Applying this property:
\begin{equation}
\left\lVert \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F = \left\lVert \boldsymbol{B} \right\rVert_F
\end{equation}

Substituting back, we obtain:
\begin{equation}
\left\lVert \boldsymbol{AB} \right\rVert_F^2 = \left\lVert \boldsymbol{\Sigma} \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F^2 \geq \sigma_{\min}^2 \left\lVert \boldsymbol{V}^T \boldsymbol{B} \right\rVert_F^2 = \sigma_{\min}^2 \left\lVert \boldsymbol{B} \right\rVert_F^2
\end{equation}

Thus, we have shown that:
\begin{equation}
\left\lVert \boldsymbol{AB} \right\rVert_F^2 \geq \sigma_{\min}^2 (\boldsymbol{A}) \left\lVert \boldsymbol{B} \right\rVert_F^2
\end{equation}
\end{proof}


\section{Proof of Theorem \ref{theorem4}}\label{sec:provetheorem4}
\begin{proof}[Proof of Theorem \ref{theorem4}]
Recall that each layer $i \in \{1, 2, \dots, L\}$ in an LLM has a sparsity rate $s_i \in [0, 1]$, where $s_i$ indicates the fraction of zero entries in the weight matrix of layer $i$. Let $\mathcal{L}_i$ denote the reconstruction error of layer $i$, and let $\mathcal{L} = \sum_{i=1}^{L} \mathcal{L}_i$ be the total reconstruction error of the entire LLM.

To prove Theorem \ref{theorem4}, we will make the following assumptions based on the preceding sections:

\begin{enumerate}
    \item According to Theorem \ref{theorem1}, The reconstruction error for each layer \( i \), denoted as \( \mathcal{L}_i \), is an increasing function of the sparsity rate \( s_i \), denote as \( f(s_i)\). 
    
    \item According to Theorem \ref{theorem2}, the reconstruction error propagates through layers such that the increase of reconstruction error of \(i\)-th layer will lead to the increase of reconstruction error of \((i+1)\)-th layer. Therefore we assume \( \mathcal{L}_{i+1} = c \mathcal{L}_i+f(s_i) \), where \( c > 1 \). The above formula indicates that the reconstruction error of a layer not only accumulates the reconstruction errors from previous layers but also that the current sparse layer contributes to the reconstruction error.
\end{enumerate}

The monotonically increasing sparsity scheme given by Eq.~\ref{eq:sparsityrates} (in Sec.~\ref{Layer-wiseSparsity}) implies that \(s_1 < s_2 < \cdots < s_L\), let us denote such a scheme as
\begin{equation}
\mathbf{s}^\uparrow \;=\; (s_1^\uparrow,\, s_2^\uparrow,\, \dots,\, s_L^\uparrow),
\end{equation}
where $s_1^\uparrow < s_2^\uparrow < \cdots < s_L^\uparrow$.

Consider a different non-monotonically increasing sparsity scheme:
\begin{equation}
  \mathbf{s}^\diamond \;=\; (s_1^\diamond,\, s_2^\diamond,\, \dots,\, s_L^\diamond),
\end{equation}
which is \textit{not} monotonically increasing. That is, there exists at least one index $k$ such that \(s_k^\diamond \;>\; s_{k+1}^\diamond\)
  
Moreover, suppose both $\mathbf{s}^\uparrow$ and $\mathbf{s}^\diamond$ satisfy the constraint that their average sparsities match the same overall budget. Formally,
\begin{equation}
  \frac{1}{L} \sum_{i=1}^L s_i^\uparrow \;=\; \frac{1}{L} \sum_{i=1}^L s_i^\diamond \;=\; S,
\end{equation}
and each $\mathbf{s}^\uparrow$ and $\mathbf{s}^\diamond$ results in a total reconstruction error
\begin{equation}
  \mathcal{L}^\uparrow \;=\; \sum_{i=1}^{L} \mathcal{L}_i^\uparrow, 
  \quad
  \mathcal{L}^\diamond \;=\; \sum_{i=1}^{L} \mathcal{L}_i^\diamond.
\end{equation}

We will show that for any non-monotonic sparsity vector $\mathbf{s}^\diamond$, we can \textit{redundantly reorder} it layer by layer to get a monotonically increasing vector $\mathbf{s}^\uparrow$ of the same average sparsity, such that the overall reconstruction error $\mathcal{L}^\uparrow$ is \textit{strictly smaller} than $\mathcal{L}^\diamond$. 
Ultimately, we will deduce
\begin{equation}
  \mathcal{L}^\uparrow < \mathcal{L}^\diamond.
\end{equation}

Let us focus on any adjacent pair $(s_k^\diamond, s_{k+1}^\diamond)$ where $s_k^\diamond > s_{k+1}^\diamond$. Define $s_k^\ast = s_{k+1}^\diamond$ and $s_{k+1}^\ast = s_k^\diamond$, effectively \textit{swapping} these two sparsities. We then compare the total contribution of layers $k$ and $k+1$ to the overall reconstruction error, first in the non-monotonic case and then in the swapped case.

We restrict our attention to layers $k$ and $k+1$:
\begin{equation}
  \text{Layer } k \quad\Rightarrow\quad 
  \mathcal{L}_k^\diamond = f(s_k^\diamond),
  \qquad
  \text{Layer } k+1 \quad\Rightarrow\quad
  \mathcal{L}_{k+1}^\diamond = c \, \mathcal{L}_k^\diamond + f(s_{k+1}^\diamond).
\end{equation}

Strictly speaking, $\mathcal{L}_{k+1}^\diamond$ depends on partial errors from layer~$k$ and its own sparsity $s_{k+1}^\diamond$. Intuitively, a higher error $\mathcal{L}_k^\diamond$ “propagates” or “magnifies” into layer $k+1$. We keep the rest of the layer-wise sparsities fixed outside of these two positions to isolate their effect.

Now, consider \emph{swapping} the two sparsities: $(s_k^\diamond,\, s_{k+1}^\diamond) \;\mapsto\; (s_k^\ast,\, s_{k+1}^\ast) = (s_{k+1}^\diamond,\, s_k^\diamond)$. Let
\begin{equation}
  \mathcal{L}_k^\ast = f(s_k^\ast) = f(s_{k+1}^\diamond),
  \quad
  \mathcal{L}_{k+1}^\ast = c \, \mathcal{L}_k^\ast + f(s_{k+1}^\ast) = c\,f(s_{k+1}^\diamond) + f(s_k^\diamond).
\end{equation}

Therefore, the total for these two layers is
\begin{equation}
  \mathcal{L}_\text{swapping}= \mathcal{L}_k^\ast + \mathcal{L}_{k+1}^\ast 
  \;=\; f(s_{k+1}^\diamond) 
  + \Bigl(c\,f(s_{k+1}^\diamond) + f(s_k^\diamond)\Bigr)
  \;=\; (1 + c)\,f(s_{k+1}^\diamond) \;+\; f(s_k^\diamond).
\end{equation}
Meanwhile, the original total is
\begin{equation}
  \mathcal{L}_\text{original}= \mathcal{L}_k^\diamond + \mathcal{L}_{k+1}^\diamond 
  \;=\; f(s_k^\diamond) 
  \;+\; \Bigl(c\,f(s_k^\diamond) + f(s_{k+1}^\diamond)\Bigr)
  \;=\; (1 + c)\,f(s_k^\diamond) \;+\; f(s_{k+1}^\diamond).
\end{equation}
The difference between the two is
\begin{equation}
\begin{aligned}
   \mathcal{L}_\text{original}-\mathcal{L}_\text{swapping}
   &=\Bigl[(1 + c)\,f(s_k^\diamond) + f(s_{k+1}^\diamond)\Bigr]
    \;-\;\Bigl[(1 + c)\,f(s_{k+1}^\diamond) + f(s_k^\diamond)\Bigr]\\
   &=\;(1 + c)\bigl[f(s_k^\diamond) - f(s_{k+1}^\diamond)\bigr]
      \;-\;\bigl[f(s_k^\diamond) - f(s_{k+1}^\diamond)\bigr]\\
   &=\;c\,\bigl[f(s_k^\diamond) - f(s_{k+1}^\diamond)\bigr].
\end{aligned}
\end{equation}
Since $c > 1$ and $f(s_k^\diamond) > f(s_{k+1}^\diamond)$, it follows that \(c\,\bigl[f(s_k^\diamond) - f(s_{k+1}^\diamond)\bigr] \;>\; 0.\)

Hence we obtain
\begin{equation}
  \mathcal{L}_\text{original}
  \;>\; \mathcal{L}_\text{swapping} .
\end{equation}
This shows that \emph{locally swapping} a pair $(s_k^\diamond, s_{k+1}^\diamond)$ with $s_k^\diamond> s_{k+1}^\diamond$ \emph{reduces} the total reconstruction error contributed by layers $k$ and $k+1$. Globally across all $L$ layers, repeating such a swap for each adjacent pair that breaks the monotonicity moves $\mathbf{s}^\diamond$ to a strictly monotonically increasing sequence of sparsities.

By iterating this argument over each adjacent pair that fails monotonicity, we reorder $\mathbf{s}^\diamond$ into  $\mathbf{s}^\uparrow$. Since every swap strictly reduces the local error, the resulting \textit{monotonically increasing} scheme $\mathbf{s}^\uparrow$ has an overall reconstruction error:
\begin{equation}
  \mathcal{L}^\uparrow 
  \;=\; \sum_{i=1}^{L} \mathcal{L}_i^\uparrow
  \;<\; \sum_{i=1}^{L} \mathcal{L}_i^\diamond
  \;=\; \mathcal{L}^\diamond.
\end{equation}
Therefore, the total reconstruction error obtained from the monotonically increasing sparsity scheme proposed in Eq. \ref{eq:sparsityrates} is strictly less than that obtained from any non-monotonically increasing sparsity scheme, under the same average sparsity constraint.

\end{proof}


\section{Zero-shot evaluation setting}\label{Zero-shotevaluationsetting}
We use the lm-eval-harness framework \citep{gao2021framework} to evaluate the zero-shot performance of LLMs. By default, we follow the settings used in Wanda \citep{sun2023simple} and AlphaPruning \citep{lu2024alphapruning}, reporting the ``acc'' metric across all datasets. However, lm-eval-harness provides multiple metrics depending on the dataset, including both ``acc'' and ``acc\_norm''. In contrast, ALS \citep{li2024adaptive} employs different metrics for different datasets. The evaluation metrics are summarized in Table \ref{tab:evaluation-metrics}.
\begin{table}[h]
    \centering
    \caption{Evaluation Metrics for Different Datasets}
    \label{tab:evaluation-metrics}
    \resizebox{0.3\textwidth}{!}{
    \begin{tabular}{l|l}
        \hline
        \textbf{Dataset}     & \textbf{Metric} \\ \hline
        boolq               & acc              \\ \hline
        hellaswag           & acc\_norm        \\ \hline
        arc\_easy           & acc              \\ \hline
        piqa                & acc              \\ \hline
        arc\_challenge      & acc\_norm        \\ \hline
        winogrande          & acc              \\ \hline
        openbookqa          & acc\_norm        \\ \hline
    \end{tabular}
    }
\end{table}

\section{More Results}\label{Moreresults}
\subsection{More LLM Architectures}
\begin{table}[h]
    \centering
    \caption{The performance improvement of ATP on sparse LLMs across a wider range of architectures.}\label{tab:MoreArchitectures}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Model} & \textbf{Perplexity (↓)} & \textbf{Accuracy (↑)} \\
        \midrule
        Dense & LLaMA3.1-8B& 6.18  & 65.93 \\
        Wanda  & LLaMA3.1-8B & 109.99 &  36.10\\
        \gr \textbf{w. ATP} & \textbf{LLaMA3.1-8B} & \textbf{72.05} & \textbf{41.59} \\
        \midrule
        Dense &OPT-13B&10.13  & 55.22 \\
        Wanda  &OPT-13B& 73.70 &  41.51\\
        \gr \textbf{w. ATP} & \textbf{OPT-13B} & \textbf{33.98} & \textbf{44.58} \\
        \midrule
        Dense & Vicuna-13B&  5.94& 65.53 \\
        Wanda  & Vicuna-13B & 44.89 & 42.06 \\
        \gr \textbf{w. ATP} & \textbf{Vicuna-13B} & \textbf{20.24} & \textbf{53.19} \\
        \midrule
        Dense & Qwen2.5-7B& 6.77 &65.36  \\
        Wanda  & Qwen2.5-7B & 75.16 & 41.10 \\
        \gr \textbf{w. ATP} & \textbf{Qwen2.5-7B} & \textbf{43.28} & \textbf{42.82} \\
        \midrule
        Dense &Mistral-7B& 5.28 & 66.17 \\
        Wanda  & Mistral-7B & 57.44 & 37.62 \\
        \gr \textbf{w. ATP} & \textbf{Mistral-7B} & \textbf{29.19} & \textbf{42.76} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Arithmetic and Knowledge Reasoning Tasks}\label{sec:arithmetic}
We further evaluate the performance improvements of our ATP method on arithmetic and knowledge reasoning tasks for sparse models. Specifically, We evaluate the 8-shot accuracy of 40$\%$ sparse models on the GSM8K dataset and the 5-shot accuracy of 60$\%$ sparse models on the MMLU dataset. The results are presented in Table \ref{tab:gsm8kandmmlu}. Our ATP method significantly improves the accuracy of sparse models on both tasks, further demonstrating the generalization and effectiveness of our approach.
\begin{table}[h!]
    \centering
    \caption{Accuracy of sparse LLaMA2 on the GSM8K and MMLU datasets.}\label{tab:gsm8kandmmlu}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
         & \multicolumn{2}{c}{\textbf{GSM8K}}  & \multicolumn{2}{c}{\textbf{MMLU}} \\
        \textbf{Method} & \textbf{7B} & \textbf{13B} & \textbf{7B} & \textbf{13B}  \\
        \midrule
        Dense         & 14.60 &24.56  & 45.90 &55.70 \\
        \midrule
        SparseGPT     & 11.07 & 16.91 & 29.40  &39.60 \\
        \gr \textbf{SparseGPT w.ATP} &\textbf{12.01}&\textbf{19.02}&\textbf{32.40}&\textbf{47.70}
        \\
        \midrule
        Wanda         & 8.72 &17.51  &  28.90 & 34.80  \\
        \gr \textbf{Wanda w.ATP} &\textbf{9.93}&\textbf{19.56}&\textbf{32.80}&\textbf{46.20}\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Multimodal Tasks}\label{sec:multimodal_tasks}
We demonstrate the applicability of our method to multimodal models by integrating it with Wanda to sparsify the Vicuna-7B model \citep{chiang2023vicuna} in LLaVA-1.5 \citep{liu2024improved}. The performance of the sparse model was evaluated on various visual question-answering and reasoning benchmarks, including VQA \citep{singh2019towards}, VQAv2 \citep{goyal2017making}, and SQA \citep{lu2022learn}. We compared our method with magnitude-based pruning, SparseGPT, Wanda, and the DSA method combined with Wanda, under a 50$\%$ sparsity rate. The results in Table \ref{tab:multimodal} show that our method outperforms the magnitude-based, SparseGPT, and Wanda approaches and demonstrates superiority over the DSA method.
\begin{table}[h!]
    \centering
    \caption{The performance improvement of the ATP method on sparse multimodal models.}
    \resizebox{0.4\textwidth}{!}{
    \label{tab:multimodal}
    \begin{tabular}{lccc}
        \toprule
        \textbf{LLaVA-1.5}  & \textbf{VQA} & \textbf{VQAv2} & \textbf{SQA} \\
        \midrule
        Dense & 58.20 & 78.50 & 66.80  \\
        Magnitude & 38.39 & 63.50 & 31.24  \\
        SparseGPT & 53.69 & 75.86 & 63.92 \\
        Wanda & 53.05 & 75.72 & 63.99  \\
        Wanda w. DSA & 54.36& 76.08 & 65.57 \\
       \gr \textbf{Wanda w. ATP} &\textbf{55.21} &\textbf{76.92} &\textbf{66.01} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Vision Models}\label{sec:vision_models}
We compare our ATP method with other approaches for determining layer-wise sparsity rates on vision models. Experiments are conducted on both CNN and Transformer architectures, including ConvNeXt \citep{Liu_2022_CVPR}, ViT \citep{dosovitskiy2021an}, and DeiT \citep{touvron2021training}, and we report the Top-1 accuracy on the ImageNet-1K dataset \citep{deng2009imagenet}. Sparse models are obtained using the Wanda \citep{sun2023simple} method, and ATP is compared against the following baselines: uniform sparsity rate, OWL \citep{yin2023outlier}, and AlphaPruning \citep{lu2024alphapruning}. The results for ConvNeXt are presented in Table \ref{tb:ConvNext}. These results demonstrate that our method is effective for determining layer-wise sparsity rates in vision models and outperforms existing methods. This further confirms the broad applicability of our monotonically increasing sparsity scheme in improving the accuracy of diverse sparse models.
\begin{table}[h!]
    \centering
    \caption{Comparison of ImageNet-1K accuracy across different layer-wise sparsity methods on the ConvNeXt-Base model.}\label{tb:ConvNext}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method} & \textbf{50$\%$} &\textbf{60$\%$}& \textbf{70$\%$}&\textbf{80$\%$} \\
        \midrule
        Wanda & 82.72 & 80.55 & 68.18 & 6.44 \\
        w. OWL & 82.76 & 80.53 & 68.28 & 6.32 \\
        w. AlphaPruning & 82.76 & 80.89 &74.24 & 42.35 \\
        \gr \textbf{w. ATP} & \textbf{82.85} & \textbf{81.10} &\textbf{75.58} &\textbf{56.81} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

We also present the accuracy results for sparse ViT and DeiT models in Table \ref{tb:vitanddeit}. Additionally, we compare our method with several widely-used non-uniform layer-wise sparsity strategies in computer vision, including ERK \citep{mocanu2018scalable}, Global \citep{frankle2018lottery}, and LAMP \citep{lee2020layer}, as well as AlphaPruning \citep{lu2024alphapruning}. The results indicate that our method outperforms all the aforementioned baselines.
\begin{table*}[!th]
    \centering
    \caption{Comparison of ImageNet-1K accuracy across different layer-wise sparsity methods on the ViT-B and DeiT-B models.
    }
    \label{tb:vitanddeit} 
    \resizebox{0.6\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc}
        \toprule
        & \multicolumn{3}{c|}{ViT-B 16/224} & \multicolumn{3}{c}{DeiT-B 16/224}\\
        Method & 40\% & 50\% & 60\% & 40\% & 50\% & 60\% \\
        \midrule 
        Uniform & 70.87 & 59.46 & 29.97 & 80.08 & 76.37 & 61.72\\
        ERK \citep{mocanu2018scalable} & 70.89 & 60.49 & 33.15 & 80.05 & 76.22 & 63.49 \\
        Global \citep{frankle2018lottery} & 66.81 & 45.75 & 8.09 & 79.94 & 75.09 & 57.01\\
        LAMP \citep{lee2020layer} & 69.45 & 57.51 & 26.99 & 80.19 & 76.35 & 63.32 \\
        AlphaPruning \citep{lu2024alphapruning} & 71.58 & 64.29 & 44.21 & 80.21 & 77.11 & 64.56 \\
        \gr \textbf{ATP} &\textbf{72.03} &\textbf{65.46} &\textbf{47.74} & \textbf{80.50}&\textbf{78.02} &\textbf{69.73} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\subsection{Integrate with other Compression Technologies}\label{sec:other_Compression_Technologies}
To demonstrate the generalization ability of our method for determining layer-wise sparsity rates, we combine ATP with other compression techniques, including N:M sparsity, structured pruning, and mixed-precision quantization. For N:M sparsity, we follow the mixed N:8 settings \citep{sun2021dominosearch}, using ATP to determine the value of \(N\) for each layer while maintaining average sparsity at 2:8, 3:8 and 4:8. In terms of structured pruning, we integrated ATP with LLM-Pruner \citep{ma2023llm}, where ATP determines the layer-wise sparsity rates, and LLM-Pruner applies pruning accordingly. For mixed-precision quantization, we combine ATP with the LIMPQ method \citep{tang2022mixed} to determine the quantization bits for each layer. We apply these compression techniques to the LLaMA-7B model and report the perplexity of the compressed models on the WikiText-2 validation set. The experimental results are presented in Table \ref{tab:otherCompressionTechnologies}. It shows that ATP significantly enhances the performance of various compression methods and outperforms both the OWL and AlphaPruning approaches.
\begin{table}[h!]
    \centering
    \caption{Experimental results of combining ATP with other compression technologies.}\label{tab:otherCompressionTechnologies}
    \resizebox{0.6\textwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{4:8}  & \textbf{3:8}& \textbf{2:8} \\
        \midrule
        Wanda  & 8.57& 42.56& 2962.00  \\
        OWL & 8.55& 22.77 & 331.37\\
        AlphaPruning   & 8.55&21.49 &585.81\\
        \gr \textbf{ATP} & \textbf{8.15}&\textbf{16.08}&\textbf{265.96}\\
        \bottomrule
        \textbf{Method} & \textbf{20$\%$} & \textbf{40$\%$} & \textbf{60$\%$}  \\
        \midrule
        LLM-Pruner &16.95& 30.38 &90.02\\
        OWL & 18.57&28.65 &76.99\\
        AlphaPruning &16.78 & 29.11& 71.21\\
        \gr \textbf{ATP}& \textbf{15.51} &\textbf{27.40}&\textbf{64.25}\\
        \bottomrule
        \textbf{Method} & \textbf{Mixed 3/4 Bit} & \textbf{Mixed 2/3/4 Bit} & \textbf{Mixed 2/4 Bit}\\
        \midrule
        Random   & 12.04 & 11455.54 & 14817.12 \\
        $L_1$ norm & 14.61 & 13959.42 & 33679.21 \\
        OWL      & 9.54  & 311.95   & 8429.39  \\
        AlphaPruning     & 9.01  & 261.39   & 7630.14  \\
        \gr \textbf{ATP}& \textbf{8.52}  & \textbf{198.65}&\textbf{5321.35}\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{More Layer-wise Sparsity Baselines}\label{sec:more_baselines}
We compare our method with additional approaches for determining layer-wise sparsity rates, including Uniform \citep{zhu2017prune}, Global \citep{frankle2018lottery}, ER \citep{mocanu2018scalable}, ER-Plus \citep{liu2022unreasonable}, and LAMP \citep{lee2020layer}. These methods are combined with the Wanda pruning approach to obtain sparse LLaMA-7B models. The experimental results are presented in Table \ref{tab:MoreBaselines}. It shows that across sparsity rates ranging from 50$\%$ to 80$\%$, the perplexity of models pruned using the ATP method is consistently lower than that of other baselines, further demonstrating the superiority of our approach.
\begin{table}[h!]
    \centering
    \caption{Comparison of WikiText-2 perplexity across various sparsity rates and methods.}\label{tab:MoreBaselines}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method/Perplexity (↓)} & \textbf{50\%} & \textbf{60\%} & \textbf{70\%} & \textbf{80\%} \\
        \midrule
        Global  & 14848  & 17765  & 5147   & 39918.56 \\
        LAMP    & 7.57   & 12.86  & 185.52 & 15647.87 \\
        LAMP (per-block)  & 7.25   & 10.95  & 98.77  & 7318.08  \\
        ER      & 7.80   & 12.41  & 119.66 & 6263.79  \\
        ER-Plus & 8.00   & 14.04  & 229.17 & 6013.91  \\
        Uniform & 7.26   & 10.63  & 84.52  & 5889.13  \\
        \gr \textbf{ATP} &\textbf{7.05}&\textbf{9.25}&\textbf{20.16}&\textbf{176.80}\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Integrate with More Post-training Sparsity Methods}\label{sec:more_post-training}
We have demonstrated that our method improves performance over Wanda and SparseGPT methods. Notably, ATP can be integrated with any post-training sparsity method to further enhance their effectiveness. In this section, we showcase the performance improvements achieved by combining ATP with other post-training sparsity methods. Specifically, we apply ATP to DSnoT \citep{zhang2023dynamic}, Pruner-Zero \citep{dong2024pruner}, and ALPS \citep{meng2024alps} to obtain a 70$\%$ sparse LLaMA2-7B model. The perplexity and zero-shot accuracy results of the sparse models are presented in Table \ref{tab:more_post-training}. The results indicate that ATP significantly enhances the performance of DSnoT, Pruner-Zero, and ALPS methods.
\begin{table}[h!]
    \centering
    \caption{The performance improvement of ATP on DSnoT, Pruner-zero and ALPS methods.}\label{tab:more_post-training}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        \textbf{Method} & \textbf{Perplexity (↓)} & \textbf{Accuracy (↑)} \\
        \midrule
        Dense  & 5.12 & 61.88 \\
        \midrule
        DSnoT   & 77.83 & 35.11 \\
        \gr \textbf{w. ATP}& \textbf{24.31} & \textbf{44.51} \\
        \midrule
        Pruner-Zero   &103.15  & 34.78 \\
        \gr \textbf{w. ATP}& \textbf{48.82} & \textbf{44.77} \\
        \midrule
        ALPS   &19.31  & 46.75 \\
        \gr \textbf{w. ATP}& \textbf{17.99} & \textbf{49.82} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Integrate with LoRA Fine-tuning}\label{sec:integrate_LoRA}
We further demonstrate the effectiveness of LoRA fine-tuning \citep{hu2021lora} in narrowing the performance gap between highly sparse LLMs and dense models. Specifically, we obtain a 70$\%$ sparse LLaMA2-7B model using the Wanda method and fine-tune it on 10,000 samples from the Alpaca-GPT4 \citep{peng2023instruction} dataset. We compare the results against models sparsified using uniform sparsity, OWL, and AlphaPruning methods. The results in Table \ref{tab:lora} show that LoRA fine-tuning significantly improves the accuracy of the sparse model, further reducing the gap with the dense model. Additionally, the sparse model obtained through the ATP method achieves higher accuracy, and this advantage is retained even after LoRA fine-tuning.
\begin{table}[h!]
    \centering
    \caption{Comparison of the perplexity and zero-shot accuracy of the 70$\%$ sparse LLaMA2-7B obtained by LoRA fine-tuning.}\label{tab:lora}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Fine-tuning} & \textbf{Perplexity (↓)} & \textbf{Accuracy (↑)} \\
        \midrule
        Dense & N.A. &5.12  & 61.88 \\
        \midrule
        Uniform  & N.A. &74.26  & 35.33 \\
        Uniform & LoRA & 13.36 & 47.10 \\
        \midrule
        OWL  & N.A. &30.38  & 41.75 \\
        OWL & LoRA & 12.89 & 50.26 \\
        \midrule
        DSA  & N.A. &63.71  & 36.55 \\
        DSA & LoRA & 13.19 & 49.49 \\
        \midrule
        AlphaPruning & N.A. & 28.87 & 43.37 \\
        AlphaPruning& LoRA & 12.76 &50.37 \\
        \midrule
        \textbf{ATP} &\textbf{N.A.}&\textbf{22.16}&\textbf{45.83}\\
        \gr \textbf{ATP}&\textbf{LoRA}&\textbf{12.28}&\textbf{50.79}\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\section{More Ablation Study}\label{sec:MoreAblationStudy}
\subsection{Computational Efficiency.}\label{sec:ComputationalEfficiency}
In Table \ref{tab:ComputationEfficiency}, we report the time required to determine the layer-wise sparsity rates for 70$\%$ sparse LLMs using our ATP method. The measurements were conducted on NVIDIA A100 80GB GPUs. The results show that only a few searches are needed to obtain the sparsity rates due to the narrow range of reasonable values for the hyperparameter \(\beta\). Furthermore, as the number of model parameters increases, this range narrows even further, enabling the sparsity rates to be determined with fewer searches. For example, for the largest 70B model, only three searches are necessary. This demonstrates that our method is highly computationally efficient.
\begin{table}[h!]
    \centering
    \caption{Computational efficiency of our ATP method (in minutes).}\label{tab:ComputationEfficiency}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Model} &\textbf{7B}& \textbf{8B}&\textbf{13B} & \textbf{30B}& \textbf{65B} & \textbf{70B} \\
        \midrule
        Wanda &2.0  & 2.2 & 3.8 & 7.8 &13.5 & 14.1\\
        w. ATP  &2.0$\times$9 &2.2$\times$9 &3.8$\times$7 &7.8$\times$5 & 13.5$\times$3& 14.1$\times$3\\
        \midrule
        SparseGPT& 6.6 & 7.3 &10.0 & 35.0 &60.0 &67.5\\
        w. ATP & 6.6$\times$9 & 7.3$\times$9 &7.3$\times$7 &10.0$\times$5  &60.0$\times$3 &67.5$\times$3\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Analyze the Layer-wise Sparsity Distribution.}\label{sec:AnalyzeDistribution}
We analyze the sparsity rate distribution across different average sparsity levels in Figure \ref{fig:comparison_average_rate}. Our findings indicate that at lower average sparsity rates, the differences in sparsity rates across layers are minimal. In contrast, these differences become more pronounced at higher average sparsity rates.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/comparison_average_rate.pdf}
    \caption{Comparison of layer-wise sparsity rate distributions at different average sparsity levels.}
    \label{fig:comparison_average_rate}
\end{figure}

\subsection{Comparison of Sparsity Rates Distribution with other Methods}\label{ComparisonDistribution}
Figure \ref{fig:comparison_methods} illustrates the sparsity rate distributions obtained from various layer-wise sparsity methods. We observe that the sparsity rates generally follow an increasing pattern from low to high, further validating the rationale behind our method.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/comparison_methods.pdf}
    \caption{Comparison of layer-wise sparsity rate distribution with other methods.}
    \label{fig:comparison_methods}
\end{figure}

\subsection{Different $\beta$ Settings.}\label{sec:Different_beta}
Figure \ref{fig:different_beta} shows the impact of different \(\beta\) settings on the perplexity of the 70$\%$ sparse LLaMA2-7B model obtained using the Wanda method. We observe that as \(\beta\) increases, the perplexity initially decreases and then rises. Furthermore, the model's perplexity under various \(\beta\) settings remains lower than that of the uniform sparsity rate scheme.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/different_beta.pdf}
    \caption{Impact of different \(\beta\) settings on the perplexity of the 70$\%$ sparse LLaMA2-7B.}
    \label{fig:different_beta}
\end{figure}


\section{Detailed Results for Zero-shot Tasks}\label{app:DetailedZero-shotTaskResults}
In this section, we provide a detailed presentation of the performance of each zero-shot task introduced in Sec. \ref{sec:experiments}.
\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of LLaMA-V1 at 70$\%$ sparsity.
  }\label{tab:zero_shot_llama1}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{8}{*}{LLaMA-7B}   & Dense  & 56.92 &  69.93 & 75.05 & 34.40   & 78.67 &75.34 &41.89 & 61.74 \\
  \cmidrule{2-10}
  & SparseGPT & 34.58  &  56.43 & 64.80&16.80 & 64.25 &45.24 &23.12& 43.60\\ 
  &  w. OWL &37.08 &62.35 &66.15 &19.80 & 66.16&48.40 & 26.02& 46.57 \\
   &  w. AlphaPruning & 37.81& 64.01&\bf 68.26 &18.80 &63.60 &46.17& 27.39 &  46.58\\
  \gr \wc  &  \bf w. ATP& \bf37.88  & \bf64.22 & 66.51  & \bf 20.00 & \bf66.17  & \bf49.37  &   \bf27.47  &\bf 47.37 \\ 
   \cmidrule{2-10}
 &   Wanda &28.86  &52.80  &59.69 &14.20 &  57.56 &31.27 &17.75 & 37.45\\ 
   &  w. OWL & 34.89& 58.64& 62.63& 17.60& 64.30&46.97 &24.32 & 44.19 \\
  &  w. DSA &34.68 &59.67 &62.69 &15.40 & 64.09&45.16 & 24.40&43.72  \\
   &  w. AlphaPruning & 36.26&\bf 62.35 &\bf 66.12& 17.20&63.93 &43.90 &25.17 & 44.99 \\
  \gr \wc  &  \bf w. ATP& \bf 37.44 & 61.43 & \bf 65.79 & \bf 20.80 & \bf 67.46 & \bf 50.63 &   \bf 25.68 &\bf 47.03 \\ 
  \midrule
  \multirow{8}{*}{LLaMA-13B}   & Dense &59.94  & 72.77  &77.89 & 33.20   & 79.16 & 77.40 &46.50 & 63.84 \\
  \cmidrule{2-10}
  & SparseGPT & 37.51  & 63.30 &68.78 &20.80 & 67.63 &52.78 &25.17 &48.00\\ 
    &  w. OWL & 40.36&66.22 &66.82 &20.80 &69.86 &57.37 &28.67 & 50.01 \\
   &  w. AlphaPruning &42.43 & 67.80& 67.58&22.00 & 70.01& 55.47&29.10 & 50.63 \\
  \gr \wc  &  \bf w. ATP& \bf 43.31 & \bf67.92 & \bf 69.14 & \bf23.40  & \bf 70.89 & \bf59.97  &   \bf 30.20 &\bf 52.12 \\ 
   \cmidrule{2-10}
 &   Wanda & 31.06 & 54.38 &61.59 &16.20 & 62.68  &42.05 & 17.58& 40.79\\ 
   &  w. OWL &38.57 &63.46 &62.81 &20.40 & 68.61& 57.07&26.37 & 48.18 \\
  &  w. DSA &38.84 &62.04 & 63.03& 23.20&68.98 &58.29 &26.10 & 48.64 \\
   &  w. AlphaPruning &41.04 &64.96 & 64.83& 21.20& 69.01& 59.39&28.24 & 49.81 \\
  \gr \wc  &  \bf w. ATP& \bf43.11  & \bf64.98 & \bf66.64  & \bf25.80  & \bf 69.80 & \bf 59.97 &   \bf30.89  &\bf 51.60 \\ 
  \midrule   
  \multirow{8}{*}{LLaMA-30B}   & Dense & 63.35 &75.69 & 82.69 & 36.00 & 81.01 &80.30 & 52.82 &67.41 \\
  \cmidrule{2-10}
  & SparseGPT  & 44.56 & 69.30 &65.35 &25.80 & 72.42& 65.78&32.25&53.64 \\
   &  w. OWL &46.96 & 72.20&67.95 & 26.80&73.56 & 67.13&35.49 & 55.73 \\
   &  w. AlphaPruning &47.49 &72.30 & 68.96& 30.00&73.50 &67.97 &34.73 & 56.42 \\
  \gr \wc  &  \bf w. ATP& \bf 48.77 & \bf 72.83& \bf 69.28 & \bf30.60  & \bf  73.94& \bf68.81  &   \bf 36.68 &\bf 57.55 \\ 
      \cmidrule{2-10}
  &  Wanda   & 44.23 &67.01 & 66.70 &26.40 & 72.03 & 64.86&32.25  & 53.35 \\
   &  w. OWL & 47.69&69.77 &65.02 & 29.20&73.88 & 68.98& 36.62& 55.88 \\
  &  w. DSA &45.62 & 67.25& 73.15& 27.80& 72.36&68.13 & 32.17& 55.21 \\
   &  w. AlphaPruning & 49.40& 71.27& 63.82&31.00 &73.50  &69.40 &38.31 & 56.67 \\
  \gr \wc  &  \bf w. ATP& \bf50.27  & \bf71.37 & \bf 65.35 & \bf31.40  & \bf 75.41 & \bf 70.62 &   \bf39.42  &\bf57.69  \\ 
  \midrule  
    \multirow{8}{*}{LLaMA-65B}   & Dense  & 64.53&77.27 & 84.89&38.00 & 81.23&81.36 &52.73 &68.57 \\
  \cmidrule{2-10}
  & SparseGPT   &49.98 &74.74 &81.03 &29.20 & 74.76& 72.05&39.51 & 60.18\\
   &  w. OWL &51.25 &74.98 & 81.30& 27.00& 75.68& 68.35&37.71 & 59.47 \\
   &  w. AlphaPruning &52.40 & 74.59& 83.80& 28.42&75.68 & 69.30&38.05 & 60.32 \\
  \gr \wc  &  \bf w. ATP& \bf 53.10 & \bf 75.69& \bf 83.73 & \bf 28.45 & \bf 76.06 & \bf 71.46 &   \bf40.69  &\bf61.31  \\ 
      \cmidrule{2-10}
  &  Wanda   &46.29 &70.79 &77.37 & 27.20&74.21 & 70.66& 37.79&57.76  \\
   &  w. OWL &50.90 & 74.11&78.50 &30.60 &75.68 & 70.70&38.05 & 59.79 \\
  &  w. DSA &48.60 & 73.08& 71.77& 28.80& 75.35& 71.88&38.22 &58.24  \\
   &  w. AlphaPruning &52.55 & 75.06&81.85 &30.40&75.84  & 71.60&40.02 &  61.05\\
  \gr \wc  &  \bf w. ATP& \bf 52.92 & \bf75.70 & \bf 82.73 & \bf31.80  & \bf 75.98 & \bf 73.53 &   \bf  40.78&\bf61.92  \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of LLaMA-V2/V3 at 70$\%$ sparsity.
  }\label{tab:zero_shot_llama23}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
  \multirow{8}{*}{LLaMA2-7B}   & Dense &57.17 & 68.90&	77.74&31.40 &78.07	&76.39 & 43.52&61.88\\
  \cmidrule{2-10}
  & SparseGPT   &33.08 &  58.41&  64.89&  17.40 & 62.46 &43.22  &22.01  &43.07\\
     &  w. OWL &36.57 &63.06 &66.94 &21.60 & 63.89&49.33 &24.49 & 46.55 \\
   &  w. AlphaPruning &36.83 &62.19 & 65.93&19.80 & 64.58&49.62 &24.43 & 46.20 \\
  \gr \wc  &  \bf w. ATP& \bf 38.94 & \bf 63.54& \bf69.54  & \bf 21.70 & \bf  68.12& \bf 52.74 &   \bf 25.85 &\bf 48.63 \\ 
      \cmidrule{2-10}
  &  Wanda  & 27.92  & 49.33 & 52.87   & 12.60 &55.33  & 30.60 & 18.69 &35.33\\
    &  w. OWL &31.83 &55.96 & 62.11&16.80 & 61.70& 43.52&20.31 & 41.75 \\
  &  w. DSA&28.54 &50.36 &57.46 &12.00 &57.13 &32.95 & 17.41& 36.55  \\
   &  w. AlphaPruning & 34.56&60.85 & 62.23&18.00 &62.40 &43.27 &22.27 & 43.37 \\
  \gr \wc  &  \bf w. ATP& \bf 36.08 & \bf61.01 & \bf62.39  & \bf 20.40 & \bf 66.81 & \bf 50.76 &   \bf 23.38 &\bf45.83  \\ 
  \midrule 
  \multirow{8}{*}{LLaMA2-13B} & Dense & 60.06 &72.22&80.52 & 35.20 & 79.11 &79.42 & 48.46 &65.00 \\
  \cmidrule{2-10}
  & SparseGPT  & 36.90 & 61.64 &66.02 & 21.00 &67.57 &52.61 &25.94 &47.38\\
     &  w. OWL &39.31 &65.75 &68.04 &22.80 & 67.89 &57.70 &27.82 & 49.90 \\
   &  w. AlphaPruning &41.26 &68.03 & 68.13&24.00 &68.28 &57.15 & 29.18&  50.86\\
  \gr \wc  &  \bf w. ATP& \bf 42.81 & \bf68.09 & \bf 72.91 & \bf24.40  & \bf  69.74& \bf 58.25 &   \bf31.06  &\bf52.46  \\ 
      \cmidrule{2-10}
  &  Wanda   & 29.60 & 51.70 &62.32&13.60 &58.65  & 37.21& 19.11 &  38.88\\
     &  w. OWL &36.31 &60.46 &63.46 &21.80 &67.77 & 55.64&24.91 & 47.19 \\
  &  w. DSA &32.83 & 55.01& 62.41&17.80 & 63.71& 49.87&21.92 &43.36  \\
   &  w. AlphaPruning & 40.28&67.32 & 62.57& 21.60&68.17 &54.46 &29.35& 49.11 \\
  \gr \wc  &  \bf w. ATP& \bf41.44  & \bf67.50 & \bf  74.71& \bf 24.60 & \bf 68.25 & \bf 57.49 &   \bf30.80  &\bf  52.11 \\ 
  \midrule   
   \multirow{8}{*}{LLaMA2-70B}   & Dense  & 66.10&78.06 &83.40	&37.20 &82.21&	82.55	&	54.44& 69.14 \\
  \cmidrule{2-10}
  & SparseGPT   &50.98 &\bf 75.45  & 80.06 &30.00  & 75.24 & 73.57 &  40.61& 60.84\\
    &  w. OWL &51.95 &74.98 &79.25 & 30.40&75.68 & 73.00&40.53 & 60.83 \\
   &  w. AlphaPruning &51.90 &75.06 & 80.40& 29.20&75.68 &74.10 & 40.87& 61.03 \\
  \gr \wc  &  \bf w. ATP& \bf  53.44& \bf76.56 & \bf 80.42 & \bf31.00  & \bf76.71  & \bf75.38  &   \bf 42.23 &\bf62.25  \\ 
      \cmidrule{2-10}
  &  Wanda  & 48.16  & 73.88 & 74.46  & 27.00 &74.86  & 72.69 &38.31 &58.48 \\
    &  w. OWL & 50.20& 74.02&75.01 &28.60 &75.68 &73.02 &38.30 &59.26  \\
  &  w. DSA &48.60 &73.08 &71.77 & 28.80& 75.35&71.88 & 38.22& 58.25 \\
   &  w. AlphaPruning & 51.40& 74.74&73.65 &29.60 & 75.84&72.70 &38.13 & 59.44 \\
  \gr \wc  &  \bf w. ATP& \bf 51.84 & \bf75.53 & \bf78.65  & \bf 29.70 & \bf76.66 & \bf74.83  &   \bf39.16  &\bf 60.91 \\  
   \midrule   
   \multirow{8}{*}{LLaMA3-8B}   & Dense 	& 60.19 & 72.77 & 81.35 & 34.80 &79.71 & 80.09 & 50.43 & 65.62\\
  \cmidrule{2-10}
  & SparseGPT   & 34.26 & 56.75 & 66.51& 16.80 & 63.28 &42.09&21.42 & 43.02\\
     &  w. OWL &36.78 & 58.96&69.54 & 18.20& 65.45& 49.46&24.06 & 46.06 \\
   &  w. AlphaPruning &35.54 &61.56 &71.02 &17.00 & 63.92&46.17 &21.67 & 45.27 \\
  \gr \wc  &  \bf w. ATP& \bf38.19  & \bf63.22 & \bf71.12  & \bf19.00  & \bf66.59  & \bf  50.20&   \bf26.19  &\bf 47.79 \\ 
      \cmidrule{2-10}
 &  Wanda  & 27.36  & 49.96 & 53.33 & 12.00 & 56.04 & 31.86 & 17.41& 35.42 \\
    &  w. OWL & 28.43& 50.43&61.74 &13.00 &57.99 &35.82 &17.58 & 37.85 \\
  &  w. DSA &27.51 & 48.46&54.16 &11.80 & 56.63& 33.03&17.66 & 35.61 \\
   &  w. AlphaPruning &27.82 & 51.85& 56.42&13.40 & 56.47& 34.97& 17.32&36.89 \\
  \gr \wc  &  \bf w. ATP& \bf 31.46 & \bf54.93 & \bf61.79  & \bf16.40  & \bf 62.18 & \bf41.79  &   \bf20.39  &\bf41.28  \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of LLaMA-V1/V2 at 50$\%$ sparsity.
  }\label{tab:zero_shot_50}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
  \multirow{7}{*}{LLaMA-7B}   & Dense &76.18 & 70.09&75.11&44.40 &79.16 & 72.98 & 44.71 &66.09 \\
  \cmidrule{2-10}
  &  Wanda  &68.92 &66.38 &70.70&39.00 &74.76 &61.74  &38.91  &60.06 \\
    & w. OWL & 70.06&66.85 &71.44&39.80 &75.52 &69.44 & 40.36 & 61.92\\
  &  w. DSA & 69.51&67.95 &71.31&39.40 &75.46 &69.65 & 40.01 & 61.90 \\
  &  w. ALS & 69.59& 66.30&73.70 & 38.60&\bf77.26 &65.66 &40.02 &61.59   \\
   &  w. AlphaPruning  & 69.60&67.64 &73.33&39.20 & 75.57& 69.65& 38.39 &61.91 \\
  \gr \wc  &  \bf w. ATP& \bf70.18  & \bf68.06 & \bf73.92 &\bf 39.90  & 76.51 & \bf69.74 &   \bf 40.70 &\bf62.72  \\ 
  \midrule 
  \multirow{7}{*}{LLaMA-13B}   & Dense & 79.06& 72.77&77.89&44.80 &80.14 &74.79  & 47.78 & 68.18\\
  \cmidrule{2-10}
  &  Wanda  &74.13 &71.51 &75.96&43.60&77.91 &69.65&43.77&65.22\\
    & w. OWL & 74.82&72.53 &76.39&43.60 &77.37 & 73.32& 43.34 &65.91 \\
  &  w. DSA & 74.02& 70.79&76.05&43.20 &76.98 &72.60 & 44.19 & 65.40 \\
  &  w. ALS & 74.34&71.35 &75.17&43.00 & 77.37& 69.70 &44.45  &65.05 \\
   &  w. AlphaPruning  &74.58 & 72.77&76.63& 44.00& 76.82&74.20 & 44.36 &66.19 \\
  \gr \wc  &  \bf w. ATP& \bf 74.86 & \bf72.78 & \bf76.47 & \bf 44.20& \bf 77.46& \bf74.36 &   \bf 44.60 &\bf 66.39 \\ 
  \midrule 
  \multirow{7}{*}{LLaMA2-7B}   & Dense & 75.98&69.06 &77.74 & 44.20&	79.11& 74.49&46.25 & 66.69\\
  \cmidrule{2-10}
  &  Wanda &68.76 & 67.32& 75.78&41.40 &	76.99&69.23 &41.72 & 63.03 \\
    & w. OWL &70.79 &67.32&75.96&42.80 &76.33& 72.01&41.97&63.88 \\
  &  w. DSA&70.90 &66.45 &76.42 &43.00 &76.22& 71.42&42.83 & 63.89  \\
  &  w. ALS& 70.75& 67.80&75.47 &\bf 44.80 &	77.10&69.61 &42.32 &  64.12 \\
   &  w. AlphaPruning & 70.89&67.48 &76.63 &43.00 &76.33	&72.22 &42.83 &64.20   \\
  \gr \wc  &  \bf w. ATP& \bf70.99  & \bf 67.84& \bf76.73 &  43.70 & \bf76.44  & \bf72.90 &   \bf42.85  &\bf64.49  \\ 
  \midrule 
  \multirow{7}{*}{LLaMA2-13B}   & Dense &79.39 &72.38 & 80.58& 45.20&80.52 &77.53 &49.15  &69.25 \\
  \cmidrule{2-10}
  &  Wanda  & 75.02&69.39 &80.34 &44.10 &78.13 & 70.37 &42.76 &65.73 \\
    & w. OWL  & 76.11&71.19 &81.65 &45.40 &78.67 & 76.85&46.24 &68.02 \\
  &  w. DSA &75.86 &71.03 & 80.83& 45.00 &78.62 &76.22 &45.99 &67.65  \\
  &  w. ALS &75.67 &72.06 & 81.35&\bf 45.80 &78.51 &70.33 & 46.08 &67.11\\
   &  w. AlphaPruning  & 76.19& 71.58& 80.97&45.00 &78.34 &76.38 &46.16 &  67.80\\
  \gr \wc  &  \bf w. ATP& \bf76.26  & \bf72.09 & \bf81.66 &   45.20& \bf 78.74& \bf76.86 &   \bf46.42  &\bf68.18  \\ 
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of more LLM architectures at 70$\%$ sparsity.
  }\label{tab:zero_shot_more_LLM}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{3}{*}{LLaMA3.1-8B}   & Dense & 59.98 &73.32  &82.05 &33.20 &79.98& 81.57&51.45 &65.93 \\
  \cmidrule{2-10}
 &  Wanda   &27.43 &48.70 &57.71 & 13.60& 55.01&31.86 &18.43 & 36.10\\
  \gr \wc  &  \bf w. ATP& \bf 31.81 & \bf55.49 & \bf 62.08& \bf15.60& \bf 62.68& \bf  42.63&   \bf 20.82 &\bf 41.59\\ 
  \midrule 
   \multirow{3}{*}{OPT-13B}   & Dense  & 52.43& 65.04&	65.93& 27.20&75.84 &	67.13&32.94 &55.22\\
  \cmidrule{2-10}
 &  Wanda  & 34.36 & 55.09 & 55.02 & 15.60 & 62.89 &43.73 & 23.89 &41.51 \\
  \gr \wc  &  \bf w. ATP& \bf 36.47 & \bf 58.09& \bf62.17 & \bf18.20 & \bf65.56 & \bf 46.63 &   \bf 24.91 &\bf44.58 \\ 
    \midrule 
   \multirow{3}{*}{Vicuna-13B}   & Dense  & 59.64&71.59 &	85.26 &36.80 &79.00 & 78.66&	47.78& 65.53\\
  \cmidrule{2-10}
 &  Wanda  & 31.84 &54.70  & 62.78 & 16.40 & 61.75 & 44.87 &22.10  &42.06\\
  \gr \wc  &  \bf w. ATP& \bf 41.47 & \bf63.85 & \bf76.70 & \bf25.00 & \bf 69.04& \bf 61.95 &   \bf34.30  &\bf53.19 \\ 
      \midrule 
   \multirow{3}{*}{Qwen2.5-7B}   & Dense  &60.01 & 72.85&84.65&33.20 & 78.73&80.43 &	47.70& 65.36\\
  \cmidrule{2-10}
 &  Wanda  &30.68 &51.93 & 61.96&15.20 & 61.59&46.34 &20.05	&41.10 \\
  \gr \wc  &  \bf w. ATP& \bf32.84  & \bf56.59 & \bf62.14 & \bf16.40 & \bf63.44 & \bf 46.42 &   \bf21.93  &\bf 43.82 \\ 
    \midrule 
   \multirow{3}{*}{Mistral-7B}   & Dense  & 61.21& 73.88& 83.64&32.60 & 80.58& 80.85&50.43&66.17 \\
  \cmidrule{2-10}
 &  Wanda  &28.86 & 51.07&60.03 &12.60 &57.56 &34.60 &18.69	&37.62 \\
  \gr \wc  &  \bf w. ATP& \bf34.44  & \bf58.96 & \bf 62.20& \bf15.60 & \bf63.93 & \bf 42.68 &   \bf 21.50 &\bf42.76 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of more post-training sparsity methods at 70$\%$ sparsity.
  }\label{tab:zero_shot_more_post-training}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{3}{*}{LLaMA2-7B} & Dense  &57.17 & 68.90&	77.74&31.40 &78.07	&76.39 & 43.52&61.88 \\
  \cmidrule{2-10}
 &  DSnoT   & 27.80&51.78 &49.66 &12.60 & 55.66&30.89 & 17.41& 35.11\\
\gr \wc  &  \bf w. ATP& \bf 34.98 & \bf58.01 & \bf62.23 & \bf18.20 & \bf65.78 & \bf 50.42 &   \bf 21.93 &\bf 44.51\\ 
    \cmidrule{2-10}
 &  Pruner-Zero&27.56&50.99&41.93&13.00&56.90&34.47&18.60&34.78\\
  \gr \wc  &  \bf w. ATP& \bf 35.97 & \bf57.93 & \bf64.46 & \bf18.60 & \bf 64.15& \bf 48.27 &   \bf23.98  &\bf 44.77\\ 
      \cmidrule{2-10}
 &  ALPS&38.35&61.96&64.59&22.20&66.82&48.37&24.95&46.75\\
 \gr \wc  &  \bf w. ATP& \bf 41.58& \bf 64.25 & \bf64.78 & \bf 24.20 & \bf68.34 & \bf 56.65 &   \bf 28.92 &\bf 49.82\\ 
\bottomrule
\end{tabular}
}
\end{table*}