\section{Conclusion}
In this paper, we propose a theoretically grounded method for determining layer-wise sparsity rates in LLMs, effectively addressing the challenge of \textbf{``reconstruction error explosion''}. Our approach utilizes an arithmetic progression to streamline sparsity allocation, reducing the complexity to a single hyperparameter while achieving near-optimal performance with minimal tuning. We have demonstrated through theoretical analysis and experimental validation that our sparsity allocation scheme is close to the optimal solution. Extensive experiments demonstrate the effectiveness of our method, yielding significant improvements in model perplexity, accuracy, and inference speed. Furthermore, our approach exhibits strong generalization across diverse architectures and modalities, establishing it as a versatile and robust solution for optimizing compressed models. 