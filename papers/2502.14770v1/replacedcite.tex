\section{Related Work}
\label{RelatedWork}
\paragraph{LLMs Sparsity.} 
Before the advent of LLMs, a variety of sparsity techniques had been developed for compressing models such as ResNet ____ , BERT ____, and ViT ____. Meanwhile, researchers have developed several post-training sparsity methods specifically for LLMs. For example, SparseGPT ____ uses the inverse of the Hessian matrix for pruning and pruned weight updates. Wanda ____ uses a metric that combines weight magnitude and input activation to prune LLMs, while Pruner-zero ____ searches for symbolic pruning metric using genetic programming. Additionally, ALPS ____ uses an Alternating Direction Method of Multiplier (ADMM) ____-based approach to prune LLMs in one-shot. The above methods focus on determining the mask within the layer of LLMs and setting a uniform layer-wise sparsity rate. Our work study the layer-wise sparsity allocation problem in sparse LLMs from the perspective of reconstruction error, thereby effectively improving the accuracy of above methods.
\paragraph{Layer-wise Sparsity.} Layer-wise sparsity rate determines the number of weights to be retained in each layer of the network ____. To determine the layer-wise sparsity rate in LLMs, OWL____ proposes an outlier-weighted metric, Alphapruning ____ uses heavy-tailed self-regularization theory ____, ALS ____ proposes a layer redundancy metric based on mutual information ____, DSA ____ develops an expression discovery algorithm to explore potential sparsity allocation. However, the above metric and search based methods all lack theoretical proof of effectiveness and require complex calculations or search to obtain layer-wise sparsity rate. In comparison, our method directly derives the layer-wise sparsity rates based on a monotonically increasing arithmetic progression from the perspective of reconstruction error, requiring only the determination of the common difference to quickly obtain the sparsity rate for each layer. More importantly, we have validated the effectiveness of the above method through rigorous proof.

\paragraph{Reconstruction Error.} Reconstruction error is a metric for measuring the difference between the output of a compressed network and that of the original network. A smaller reconstruction error generally implies that the compressed network can better preserve the performance of the original network ____. In order to minimize the reconstruction error of sparse LLMs, SparseGPT ____ proposes a mask selection and weight update algorithm based on Hessian inverse and DSnoT ____ proposes a training-free dynamic weight pruning and growing algorithm. In this paper, we explore the \textbf{``reconstruction error explosion''} problem in sparse LLMs. Specifically, the reconstruction error accumulates and magnifies across layers, leading to an extremely large overall reconstruction error, which undermines the model's accuracy. Therefore, we propose our layer-wise sparsity allocation method to alleviate the above \textbf{``reconstruction error explosion''} problem.