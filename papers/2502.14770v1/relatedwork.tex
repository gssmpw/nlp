\section{Related Work}
\label{RelatedWork}
\paragraph{LLMs Sparsity.} 
Before the advent of LLMs, a variety of sparsity techniques had been developed for compressing models such as ResNet \citep{yu2022combinatorial, zhang2024how} , BERT \citep{xia2022structured, li2023losparse}, and ViT \citep{yu2022width, he2024pruning}. Meanwhile, researchers have developed several post-training sparsity methods specifically for LLMs. For example, SparseGPT \citep{frantar2023sparsegpt} uses the inverse of the Hessian matrix for pruning and pruned weight updates. Wanda \citep{sun2023simple} uses a metric that combines weight magnitude and input activation to prune LLMs, while Pruner-zero \citep{dong2024pruner} searches for symbolic pruning metric using genetic programming. Additionally, ALPS \citep{meng2024alps} uses an Alternating Direction Method of Multiplier (ADMM) \citep{boyd2011distributed}-based approach to prune LLMs in one-shot. The above methods focus on determining the mask within the layer of LLMs and setting a uniform layer-wise sparsity rate. Our work study the layer-wise sparsity allocation problem in sparse LLMs from the perspective of reconstruction error, thereby effectively improving the accuracy of above methods.
\paragraph{Layer-wise Sparsity.} Layer-wise sparsity rate determines the number of weights to be retained in each layer of the network \citep{lee2020layer, liu2022unreasonable}. To determine the layer-wise sparsity rate in LLMs, OWL\citep{yin2023outlier} proposes an outlier-weighted metric, Alphapruning \citep{lu2024alphapruning} uses heavy-tailed self-regularization theory \citep{martin2019traditional}, ALS \citep{li2024adaptive} proposes a layer redundancy metric based on mutual information \citep{kraskov2004estimating}, DSA \citep{li2024discovering} develops an expression discovery algorithm to explore potential sparsity allocation. However, the above metric and search based methods all lack theoretical proof of effectiveness and require complex calculations or search to obtain layer-wise sparsity rate. In comparison, our method directly derives the layer-wise sparsity rates based on a monotonically increasing arithmetic progression from the perspective of reconstruction error, requiring only the determination of the common difference to quickly obtain the sparsity rate for each layer. More importantly, we have validated the effectiveness of the above method through rigorous proof.

\paragraph{Reconstruction Error.} Reconstruction error is a metric for measuring the difference between the output of a compressed network and that of the original network. A smaller reconstruction error generally implies that the compressed network can better preserve the performance of the original network \citep{yun2021all, hubara2021accelerated, ma2023solving}. In order to minimize the reconstruction error of sparse LLMs, SparseGPT \citep{frantar2023sparsegpt} proposes a mask selection and weight update algorithm based on Hessian inverse and DSnoT \citep{zhang2023dynamic} proposes a training-free dynamic weight pruning and growing algorithm. In this paper, we explore the \textbf{``reconstruction error explosion''} problem in sparse LLMs. Specifically, the reconstruction error accumulates and magnifies across layers, leading to an extremely large overall reconstruction error, which undermines the model's accuracy. Therefore, we propose our layer-wise sparsity allocation method to alleviate the above \textbf{``reconstruction error explosion''} problem.