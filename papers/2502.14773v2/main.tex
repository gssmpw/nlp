\documentclass[twoside]{article}
\usepackage[accepted]{aistats2025}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{References}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

% Useful packages
\usepackage{amsmath, amsfonts,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{xcolor}
\usepackage{url}
\usepackage{bm}
\usepackage{float} 
\usepackage{mathtools} 

\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{pgf}
\usepackage{amssymb,amsmath,mathtools}
\usepackage{lipsum}
\usepackage{enumitem}
\setlist[itemize,enumerate]{topsep=4pt,itemsep=0pt,leftmargin=*}


\DeclareGraphicsExtensions{.pdf,.png}

\DeclareMathOperator*{\softmax}{\mathsf{softmax}}
\DeclareMathOperator*{\sparsemax}{\mathsf{sparsemax}}
\DeclareMathOperator*{\entmax}{\mathsf{entmax}}

\usepackage{mdframed}
%\definecolor{theoremcolor}{rgb}{0.97, 0.97, 0.97}
%\definecolor{examplecolor}{rgb}{1, 1, 1.0}
\definecolor{theoremcolor}{rgb}{0.94, 0.97, 1.0}
\definecolor{examplecolor}{rgb}{0.94, 0.97, 1.0}
\mdfsetup{
	innertopmargin=8pt,
	innerbottommargin=8pt,
	leftmargin=4pt,
	rightmargin=4pt,
	backgroundcolor=theoremcolor,
	linewidth=0pt,
}
\usepackage{xfrac}
\newtheorem{prop}{Proposition}

\newcommand{\entalpha}{\gamma}
\newcommand{\invprob}{\mathsf{InvProb}}
\newcommand{\raps}{\mathsf{RAPS}}
\newcommand{\entmaxCP}{\entalpha\text{-}\mathsf{entmaxCP}}
\newcommand{\logmargin}{\mathsf{log\text{-}margin}}
\newcommand{\optentmax}{\mathsf{opt\text{-}entmax}}


\newmdtheoremenv[linewidth=0pt,innerleftmargin=4pt,innerrightmargin=4pt]{definition}{Definition}
\newmdtheoremenv[linewidth=0pt,innerleftmargin=4pt,innerrightmargin=4pt]{proposition}{Proposition}
\newmdtheoremenv[linewidth=0pt,innerleftmargin=0pt,innerrightmargin=0pt,backgroundcolor=examplecolor]{example}{Example}
\newmdtheoremenv{corollary}{Corollary}
\newmdtheoremenv{theorem}{Theorem}
\newmdtheoremenv{lemma}{Lemma}
\usepackage{comment}
\newcommand{\margarida}[1]
{{\color{teal}\textbf{[MC: #1]}}}
\newcommand{\mario}[1]
{{\color{magenta}\textbf{[MF: #1]}}}
\newcommand{\andre}[1]
{{\color{red}\textbf{[AM: #1]}}}
\newcommand{\rebuttal}[1]
{{\color{black}{#1}}}
\newcommand{\sophia}[1]
{{\color{green!80!black}\textbf{[SS: #1]}}}
% If your paper is accepted, change the options for the package
% aistats2024 as follows:
%
%\usepackage[accepted]{aistats2024}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}
\runningauthor{Margarida M. Campos, João Cálem, Sophia Sklaviadis, Mário A.T. Figueiredo, André F.T. Martins}
\twocolumn[

\aistatstitle{Sparse Activations as Conformal Predictors}

\aistatsauthor{ Margarida M. Campos$^{1,2}$ \And João Cálem$^{2}$ \And  Sophia Sklaviadis$^{1,2}$ \AND Mário A.T. Figueiredo$^{1,2,3}$ \And André F.T. Martins$^{1,2,3,4}$ }

\aistatsaddress{ $^1$ Instituto de Telecomunicações \\ $^2$ Instituto Superior Técnico, Universidade de Lisboa  \\ $^3$ ELLIS Unit Lisbon \\ $^4$ Unbabel \\ \texttt{margarida.campos@tecnico.ulisboa.pt}} ]

\begin{abstract}
Conformal prediction is a distribution-free framework for uncertainty quantification that replaces point predictions with sets, offering marginal coverage guarantees (\textit{i.e.}, ensuring that the prediction sets contain the true label with a specified probability, in expectation). 
In this paper, we uncover a novel connection between conformal prediction and sparse ``softmax-like'' transformations, such as sparsemax and $\entalpha$-entmax (with $\entalpha > 1$), which may assign nonzero probability only to a subset of labels. 
We introduce new non-conformity scores for classification that make the calibration process correspond to the widely used temperature scaling method. At test time, applying these sparse transformations with the calibrated temperature leads to a support set (\textit{i.e.}, the set of labels with nonzero probability) that automatically inherits the coverage guarantees of conformal prediction. 
Through experiments on computer vision and text classification benchmarks, we demonstrate that the proposed method achieves competitive results in terms of coverage, efficiency, and adaptiveness compared to standard non-conformity scores based on softmax.%
%Conformal prediction is a distribution-free uncertainty quantification framework where point predictions are replaced by sets, offering marginal coverage guarantees (\textit{i.e.}, in expectation, the sets contain the true label with a desired probability). 
%In this paper, we reveal a new connection between conformal predictors and sparse ``softmax-like'' transformations (such as sparsemax and $\entalpha$-entmax with $\entalpha>1$), which are able to assign zero probability to irrelevant labels. 
%We derive new non-conformity scores for classification which make the calibration process correspond to the well-known temperature scaling procedure. At test time, when applying such sparse transformations with the calibrated temperature, the resulting support (\textit{i.e.}, the set of labels with nonzero probability) automatically inherits the coverage guarantees of conformal prediction. 
%We evaluate the proposed method through experiments in computer vision and text classification benchmarks, showing competitive performance (in terms of coverage, efficiency, and adaptativity) against commonly used non-conformity scores based on softmax.%

%Conformal prediction is an uncertainty quantification framework with strong guarantees and minimal assumptions, growing in popularity alongside the need to reliably quantify confidence in model outputs. In this work, we show a novel connection between conformal prediction and a family of sparse activation functions called $\mathsf{entmax}$. We show that for classification tasks \sophia{problems? objectives?} there is a choice of non-conformity score for which the conformal predictor yields a prediction rule equivalent to applying the $\mathsf{entmax}$ activation \sophia{transformation} with a given temperature, viewing the support (non-null entries) of the output as the predicted set. Additionally, we present an analysis of the empirical efficiency of the different \sophia{entmax} non-conformity scores, and show that the choice of parameter of the $\mathsf{entmax}$ transformation can be interpreted as a regularization factor for calibration.
\end{abstract}

\begin{figure}
\centering
\includegraphics[width=1\columnwidth,trim = {2cm 0cm 3.5cm 0cm}]{figures/conformal_entmax_diagram}
\caption{\textbf{Conformal prediction meets temperature scaling:} we derive new non-conformity scores $s(x, y)$ that make conformal prediction equivalent to $\entalpha$-entmax temperature scaling.}
\label{fig:diagram}
\end{figure}
\section{INTRODUCTION}

The use of learned predictive models in many high-stakes applications (\textit{e.g.}, medical, legal, or financial) has stimulated extensive research on uncertainty quantification as a way to enhance predictions with reliable confidence estimates \citep{Silva_Filho,Gawlikowski}. In classification tasks, this corresponds to providing an estimate of the posterior probabilities of each of the classes, given the sample to be classified. In regression, this would correspond to providing, not only a point estimate, but also a confidence interval around that estimate. 


Unfortunately, modern deep networks are not well calibrated: the class probability estimates may be significantly different from the posterior probability, and thus cannot be trusted as confidence levels \citep{guo2017calibrationmodernneuralnetworks,wenger2020}. This observation has motivated considerable work on developing methods to obtain well-calibrated class probability estimates.

%Modern supervised learning is typically formulated as the problem of minimizing the average of a loss function across a training set (often called empirical risk), maybe with some regularizer (\textit{e.g.}, weight decay) added.   The choice of the loss function is intimately coupled with the choice of the output layer of the model (a.k.a. the activation), and both depend on the type of task at hand. Canonical examples include the softmax transformation coupled with the cross-entropy loss, for classification tasks, or a linear output layer associated with a squared (or absolute) error loss, for regression tasks. Consequently, loss functions play a central role in the supervised learning of predictive models and have been widely and deeply researched in the past decades \citep{ciampiconi2023surveytaxonomylossfunctions}. \andre{this paragraph can be trimmed}

To address this problem, 
\textbf{conformal prediction} (\S\ref{subsec:background_cp}) has emerged as a powerful framework for uncertainty quantification that comes with strong theoretical guarantees under minimal assumptions, as it is model-agnostic and distribution-free \citep{vovk_book,angelopoulos2022gentleintroductionconformalprediction}. Conformal predictors are \textit{set} predictors, \textit{i.e.}, their outputs are either sets of labels (for classification tasks) or intervals (for regression tasks). Most common variants, namely \textit{split} conformal prediction  \citep{papadopoulos2002inductive}, do not require model retraining: prediction rules are created using a separate calibration set through the design of a non-conformity score, which is a measure of how unlikely
an input-output pair is.  

A parallel research direction concerns \textbf{sparse} alternatives to the softmax activation  (\S\ref{subsec:entmax}), \textit{i.e.}, capable of producing probability outputs where some labels receive zero probability, yet are differentiable, thus usable in backpropagation \citep{martins2016softmaxsparsemaxsparsemodel,peters2019sparsesequencetosequencemodels}. The class of Fenchel-Young losses \citep{blondel2020learningfenchelyounglosses} provides a framework to learn such sparse predictors, generalizing the softmax activation and the cross-entropy loss. 
%\citet{blondel2020learningfenchelyounglosses} introduced Fenchel-Young losses---a class that unifies many well-known loss functions and allows for the creation of new ones from a given regularized prediction function. These losses have important generalized properties and associated efficient predictive and training algorithms. The loss associated to the popular \textit{softmax} activation is a Fenchel-Young loss, as is the \textit{sparsemax} loss --- associated with the sparsemax activation, introduced by \citet{martins2016softmaxsparsemaxsparsemodel}, capable of producing sparse probability outputs. \margarida{mention interpretability?} 
An added benefit of sparse probability outputs is that the labels with nonzero probability can be directly interpreted as a \textit{set prediction}, eliminating the need for creating these sets by thresholding probability values. Such set predictions have been used by \citet{martins2016softmaxsparsemaxsparsemodel} for multi-label classification, but not in the scope of uncertainty quantification. 

This paper connects the two lines of work above by \textbf{conformalizing sparse activations} (Figure~\ref{fig:diagram}; \S\ref{sec:proposed}). 
We focus on the $\entalpha$-entmax family%
\footnote{Called $\alpha$-entmax by \citet{peters2019sparsesequencetosequencemodels}. We choose $\entalpha$ in this paper to avoid clash with the confidence level $\alpha$, commonly used in the conformal prediction literature.} %
\citep{peters2019sparsesequencetosequencemodels}, which includes softmax ($\entalpha=1$) and sparsemax ($\entalpha=2$) as particular cases, and is sparse for $\entalpha > 1$. We create set predictions by including all labels receiving nonzero probability under those activations---these sets can be made larger or smaller by controlling a \textit{temperature} parameter. We leverage the calibration techniques of conformal prediction to choose the temperature that leads to the desired coverage level. 

Our main contributions are the following: 
%\textit{i.e.}, the non-zero entries will be considered a predicted set. We show that there is a choice of non-conformity score that makes the conformal predictor equivalent to applying the sparsemax activation with a given temperature parameter. 
%Furthermore, we generalize the result for the family of $\alpha$-entmax transformations \citep{peters2019sparsesequencetosequencemodels}, which includes both softmax and sparsemax activations. Our contributions, besides the theoretical proofs of the aforementioned relationship, include several experiments on real datasets that allow not only for empirical validation but also to compare the efficiency of conformal predictors with different non-conformity scores. 
\begin{itemize}
    \item A new non-conformity score that makes the conformal predictor equivalent to temperature scaling of the sparsemax activation, establishing a formal link between the two lines of work and ensuring statistical coverage guarantees (\S\ref{subsec:sparsemax_cp}). 
    \item Generalization of the construction above to the $\entalpha$-entmax family of activations, with the same guarantees (\S\ref{subsec:entmax_cp}). This yields a calibration strategy that can also be used with softmax ($\entalpha=1$) and where $\entalpha$ can be tuned as a hyperparameter (\textit{e.g.} to optimize prediction set size). 
    \item Empirically validation of our method on a range of computer vision and text classification tasks across several datasets, comparing the resulting coverage,  efficiency, and adaptiveness with commonly used non-conformity scores (\S\ref{sect:experiments}).
    \rebuttal{
    \item Expansion of the family of non-conformity scores for conformal prediction tasks with new scores that are competitive with state-of-the-art techniques.
    }
\end{itemize}

\rebuttal{
The code to reproduce all the reported experiments is publicly available.%
\footnote{\scriptsize{\url{https://github.com/deep-spin/sparse-activations-cp}}}
}

\section{BACKGROUND}
\label{sec:background}
This section provides background by briefly reviewing basic concepts of conformal prediction and sparse activations.

\subsection{Conformal Prediction}
\label{subsec:background_cp}

%Conformal prediction is an uncertainty quantification framework with strong theoretical guarantees and minimal assumptions (\citet{vovk_book}).  

Consider a supervised learning task with input set $\mathcal{X}$, output set $\mathcal{Y}$, and a predictor, $f:\mathcal{X}\rightarrow \mathcal{Y}$. 
Given a new observation, $x_\text{test} \in \mathcal{X}$, a standard predictor produces a single point estimate: $\hat{y}_\text{test} = f(x_\text{test})$. A conformal predictor, $\mathcal{C}_\alpha: \mathcal{X}\rightarrow 2^{\mathcal{Y}}$, outputs instead a prediction set $\mathcal{C}_\alpha(x_\text{test}) \subseteq \mathcal{Y}$, which is expected to include the correct target, $y_\text{test}$, with a given probability $1-\alpha$, where $\alpha$ is a user-chosen error rate. %Namely, 
%it is designed to satisfy marginal coverage, 
%with coverage guarantees of the ground-truth, for a user-chosen error rate $\alpha$. \sophia{how about $q$-entmax for q-log? the quantiles are $\hat{q}$ ...}


\paragraph{Procedure} We focus on split conformal prediction, which does not involve model retraining \citep{papadopoulos2002inductive}. 
Given the predictor, $f$, and a collection of calibration samples, $\mathcal{D}_\text{cal} = \bigl((x_1,y_1),..., (x_n,y_n)\bigr)$, a conformal predictor is defined by a non-conformity score $s: \mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$. The non-conformity score measures how unlikely an input-output pair $(x,y) \in \mathcal{X}\times\mathcal{Y}$ is, compared to the remaining calibration data. Ideally, predictions $\hat{y}\in \mathcal{Y}$ yielding pairs $(x_\text{test}, \hat{y})$ that are likely to occur in the data should have a low non-conformity score, and should thus be included in the prediction set $\mathcal{C}_\alpha(x_\mathrm{test})$.

The procedure for generating the prediction set for an unobserved sample, $x_\text{test}$, is as follows: 
\begin{enumerate}
    \item During calibration, the non-conformity scores are computed for $\mathcal{D}_\mathrm{cal}$, $(s_1, ... , s_n)$, where $s_i = s(x_i,y_i)$. \item  A threshold $\hat{q}$ is set to the $\lceil(n+1)(1-\alpha)\rceil/n$ empirical quantile of these scores.
    \item At test time, the following prediction set is output:
    \begin{align}\label{eq:prediction_set}
        \mathcal{C}_\alpha(x_\text{test})=\{y\in\mathcal{Y}: s(x_\text{test},y)\leq \hat{q}\}.
    \end{align}
\end{enumerate}

\begin{comment}
    \underline{Calibration Step}
\begin{enumerate}\setlength\itemsep{0em}
	\item Compute the non-conformity scores for $\mathcal{D}_\mathrm{cal}$, $(s_1, ... , s_n)$ where $s_i = s(x_i,y_i)$.
	\item Set $\hat{q}$ to be the $\lceil(n+1)(1-\lambda)\rceil/n$ empirical quantile of the set of scores.
\end{enumerate}
\underline{Prediction Step}
\begin{enumerate}\setcounter{enumi}{2}\setlength\itemsep{0em}
\item Output the prediction set, using the quantile $\hat{q}$, as $\mathcal{C}_\lambda(x_\text{test})=\{y\in\mathcal{Y}: s(x_\text{test},y)\leq \hat{q}\}$. \sophia{$\hat{y}$?}
\end{enumerate}
\end{comment}

\paragraph{Coverage guarantees} 
If the calibration data and the test datum $((X_1, Y_1), ..., (X_n, Y_n), ((X_{\text{test}}, Y_{\text{test}}))$ are \textit{exchangeable},\footnote{A sequence $(Z_1,...,Z_n)$ is said to be \textit{exchangeable} if $
		\bigl(Z_1,...,Z_n\bigr) \overset{d}{=} \bigl( Z_{\pi(1)},...,Z_{\pi(n)}\bigr)$
	for any permutations $\pi$ of $\{1,...,n\}$, where $\overset{d}{=}$ stands for \textit{identically distributed}.} conformal predictors obtained with this procedure yield prediction sets satisfying
\begin{equation}
	\mathbb{P}\big( Y_\text{test} \in \mathcal{C}_\alpha(X_\text{test})\big)\geq 1- \alpha,
	\label{eq:coverage}
\end{equation} 
as proved by \citet{vovk_book}, without any other assumptions on the predictive model or the data distribution. 
Ideally, we would like the predictor with the required coverage to be \textit{efficient}, \textit{i.e.}, to have a small average prediction set size $\mathbb{E} [|C_\alpha(X)|]$, and \textit{adaptive}, \textit{i.e.}, instances that are harder to predict should yield larger prediction sets, representing higher uncertainty. 

\subsection{Sparse Activations}
\label{subsec:entmax}

Consider a classification task with $K$ classes, $|\mathcal{Y}| = K$, and let $\triangle_K = \{ \bm{p} \in \mathbb{R}^K : \bm{p} \geq \mathbf{0}, \mathbf{1}^\top \bm{p} = 1 \}$ denote the $(K-1)$–dimensional probability simplex. Typically, predictive models output a vector of label scores, $\bm{z}\in \mathbb{R}^K$, which is converted to a probability vector in $\triangle_K$ through a suitable transformation, usually softmax:  
\begin{equation}\label{eq:softmax}
\softmax(\bm{z})_j := \frac{\exp(z_j)}{\sum_i \exp(z_i)}.
\end{equation}
Since the exponential function is stricly positive, the softmax transformation never outputs any zero. 
\citet{martins2016softmaxsparsemaxsparsemodel} introduced an alternative transformation,  \textit{sparsemax}, which can produce sparse outputs while being almost-everywhere differentiable:
\begin{equation}\label{eq:sparsemax}
\sparsemax(\bm{z}) := \arg\min_{\bm{p} \in \triangle_K} \| \bm{p} - \bm{z} \|^2.
\end{equation}
The solution of \eqref{eq:sparsemax}, which corresponds to the Euclidean projection of $\bm{z}$ onto $\triangle_K$, can be computed efficiently through Algorithm~\ref{alg:sparsemax}. 
This algorithm, which sorts the label scores and seeks a threshold which ensures normalization,  motivates our proposed non-conformity score to be presented in \S\ref{subsec:sparsemax_cp}. 

\input{sparsemax_algo}

As shown by \citet{peters2019sparsesequencetosequencemodels} and \citet{blondel2020learningfenchelyounglosses}, sparsemax and softmax are particular cases of a family of $\entalpha$-entmax transformations, defined as
\begin{align}\label{eq:entmax}
	\entalpha\text{-}\entmax(\bm{z}) := \arg\max_{\bm{p} \in \triangle_K} \bm{p}^\top \bm{z} + H_\entalpha(\bm{p}), 
\end{align}
where $H_\entalpha$ is a generalized entropy function \citep{tsallis1988}, defined, for $\entalpha > 0$, as
\begin{equation}
H_\entalpha(\bm{p}) = 
\begin{cases} 
	\frac{1}{\entalpha(\entalpha - 1)} \left( 1 - \sum_{j=1}^K p_j^\entalpha \right), & \entalpha \neq 1, \\
	-\sum_{j=1}^K p_j \log p_j, & \entalpha = 1. 
\end{cases}
\end{equation} 
This family of entropies is continuous with respect to $\entalpha$ and recovers the Shannon entropy for $\entalpha = 1$. 
Softmax and sparsemax are particular cases of the $\entalpha$-entmax transformation \eqref{eq:entmax} for $\entalpha=1$ and $\entalpha=2$, respectively. 
Crucially, setting $\gamma>1$ enables sparsity in the output, \textit{i.e.}, for certain inputs $\bm{z}$, the solution $\bm{p}^*$ in \eqref{eq:entmax} will be a sparse probability vector. Figure \ref{fig:entmax_mappings} depicts the behavior of $\entalpha$-entmax for three different values of $\entalpha$.

\begin{comment}
Although sparsemax and softmax were shown to share important properties \citep{martins2016softmaxsparsemaxsparsemodel,blondel2020learningfenchelyounglosses}, the connection between them was better estabilished when \citet{peters2019sparsesequencetosequencemodels} \andre{I think this was first done by Niculae \& Blondel NeurIPS 2017} considering their variational form \citep{wainwright2008}. In fact, both transformations, $\mathbf{\hat{y}}_\Omega(\mathbf{z})$, can be written as an optimization problem of the form:

\begin{equation}
	\mathbf{\hat{y}}_\Omega(\mathbf{z})= \arg\max_{\mathbf{p} \in \Delta^K} \mathbf{p}^\top \mathbf{z} - \Omega(\mathbf{p}),
	\label{eq:prediction_function_fy}
\end{equation}
where $\Omega$ is a regularization function, as shown by \citet{blondel2020learningfenchelyounglosses}. Choosing $\Omega$ to be the 
Gibbs-Shannon entropy with natural logarithms, we retrieve softmax; if we consider $\Omega$ to be the Gini entropy we obtain sparsemax (see Table \ref{tab:variational}).
\margarida{improve table} \andre{We probably don't need a table. Also, do we need to talk about the regularized form and the entropies here in all this detail? I don't think it's needed}
\begin{table}[h!]
	\def\arraystretch{1.5}
	\begin{tabular}{c|c|c|}
		\cline{2-3}
		&
		$\arg\max_{\mathbf{p} \in \Delta^K}$ &
		Entropy (H) \\ \hline
		\multicolumn{1}{|c|}{softmax} &
		$  \mathbf{p}^\top \mathbf{z} + H^S(\mathbf{p})$ &
		$- \sum_j p_j \log p_j$\\ \hline
		\multicolumn{1}{|c|}{sparsemax} &
		$ \mathbf{p}^\top \mathbf{z} + H^G(\mathbf{p})$&
		$\frac{1}{2} \sum_j p_j (1 - p_j)$ \\ \hline
	\end{tabular}
	\caption{Sparsemax and softmax as a variational problem with regularization expressions.}
	\label{tab:variational}
\end{table}

\citet{peters2019sparsesequencetosequencemodels} developed a probability mapping, called $\alpha$-entmax, that is essentially an interpolation between softmax and sparsemax based on the generalization of the Shannon and Gini
entropies proposed by \citet{tsallis1988}:
\begin{equation}
H_\alpha^T(\mathbf{p}) := 
\begin{cases} 
	\frac{1}{\alpha(\alpha - 1)} \sum_j \left( p_j - p_j^\alpha \right), & \alpha \neq 1, \\
	H^S(\mathbf{p}), & \alpha = 1,
\end{cases}
\end{equation} 
 where $\alpha > 1$, is a scalar. This family of entropies constitutes a continuous interpolation between Shannon ($H_1^T \equiv H^S$) and Gini ($H_2^T \equiv H^G$) entropies, and led to the definition of $\alpha\text{-entmax}:\mathbb{R}^K\rightarrow\Delta^K,$

\begin{equation}
	\alpha\text{-entmax}(\mathbf{z}) := \arg\max_{\mathbf{p} \in \Delta^K} \mathbf{p}^\top \mathbf{z} + H_\alpha^T(\mathbf{p})
\end{equation}
\end{comment}

 %, where $1\text{-}\entmax \equiv \softmax$ and $2\text{-}\entmax \equiv \mathsf{sparsemax}$.
\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/reg_paths}
	\caption{\label{fig:entmax_mappings} Illustration of entmax in the two-dimensional case $\entalpha$-entmax([$t$, 0])$_1$.}
\end{figure}

\citet{peters2019sparsesequencetosequencemodels} show that 
the solution of \eqref{eq:entmax} has the form 
\begin{align}\label{eq:entmax_def} 
	\entalpha\text{-}\entmax(\bm{z}) = \left[ (\entalpha - 1)\bm{z} - \tau \mathbf{1} \right]_{+}^{\frac{1}{\entalpha - 1}},
\end{align}
where $\mathbf{1}$ denotes a vector of ones,  $\left[ x \right]_+ = \max\{x, 0\}$, and $\tau$ is the (unique) constant ensuring normalization. 
A bisection algorithm for computing $\entalpha$-entmax for general $\entalpha$, as well as a more efficient algorithm for $\entalpha=1.5$,  have been proposed by \citet{peters2019sparsesequencetosequencemodels}. 
We make use of these algorithms in our experiments in \S\ref{sect:experiments}. 

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{figures/temperature_scaling}
\caption{Output of different $\entalpha\text{-}\mathsf{entmax}$ transformations on label scores $\bm{z} = [1, -1, -0.2, 0.4, -0.5]$, as a function of temperature parameter $\beta^{-1}$, where $[p_1,p_2,p_3,p_4,p_5] = \entalpha\text{-}\mathsf{entmax}(\beta\bm{z})$.}
\label{fig:entmax_temperature}
\end{figure*} 

\begin{comment}
\paragraph{Entmax Losses} Given the fact that $\alpha$-entmax can be written in the form of the variational problem in (\ref{eq:entmax}), the corresponding loss function is a Fenchel-Young loss, which can be easily derived, as shown by \citet{blondel2020learningfenchelyounglosses}. Denoting $\mathbf{p}^\star := \alpha\text{-entmax}(\mathbf{z})$, the entmax loss function becomes: \andre{are these equations relevant to anything we do in the paper? we probably don't need them and can just describe in the text what the entmax loss means}

\begin{equation}
	\mathsf{L}_\alpha(y, \mathbf{z}) := (\mathbf{p}^\star - \mathbf{e}_y)^\top \mathbf{z} + H_\alpha^T(\mathbf{p}^\star),
	\label{eq:ent_loss}
\end{equation}
with gradient:

\begin{equation}
\nabla_{\mathbf{z}} \mathsf{L}_\alpha(y, \mathbf{z}) = -\mathbf{e}_y + \mathbf{p}^\star.
\end{equation}
\end{comment}

\paragraph{Adjusting sparsity} Temperature scaling \citep{guo2017calibrationmodernneuralnetworks,platt1999probabilistic} is a simple and popular technique to calibrate the output probabilities from a predictive model. By multiplying the label scores $\bm{z}$ by a scaling factor $\beta > 0$, which can be interpreted as an inverse temperature parameter, the probabilities resulting from the subsequent softmax transformation can be made more or less peaked. 
When $\beta = 0$, $\softmax(\beta \bm{z})$ becomes a uniform distribution, whereas in the limit $\beta \rightarrow +\infty$ (the ``zero temperature limit''), $\softmax(\beta \bm{z})$ tends to a one-hot vector, assigning probability 1 to the label with the largest score.\footnote{If there are multiple labels tied with the same largest score, $\lim_{\beta\rightarrow +\infty}\softmax(\beta \bm{z})$ becomes a uniform distribution over those labels, assigning zero probability to all other labels.}  

Analogous limit cases apply also to $\entalpha\text{-}\entmax(\beta\bm{z})$, but there is a key qualitative difference \citep{martins2016softmaxsparsemaxsparsemodel,blondel2020learningfenchelyounglosses}. For any $\entalpha>1$, there is a \textit{finite} threshold $\beta^*$ (corresponding to a ``non-zero temperature'') above which $\entalpha\text{-}\entmax(\beta\bm{z})$ saturates as the zero temperature limit: $\beta \geq \beta^* \, \Rightarrow \, \entalpha\text{-}\entmax(\beta\bm{z}) = \lim_{\beta\rightarrow +\infty} \entalpha\text{-}\entmax(\beta\bm{z})$. Increasing $\beta$, which may be seen as a regularization coefficient, from $0$ to $\beta^*$ draws a regularization path where the support of $\entalpha$-$\entmax(\beta \bm{z})$ varies from the full set of labels to a single label (assuming no ties), as shown in Figure~\ref{fig:entmax_temperature}. 
Therefore, using temperature scaling with $\entalpha$-entmax, for $\entalpha > 1$, allows the sparsity of the output to be controlled. 
We exploit this fact in our proposed method described in \S\ref{sec:proposed}. 
%Low values of $T$ will lead to sparser solutions, and as with softmax, as $T\rightarrow \infty$, $\hat{y}_\Omega(T\mathbf{z})$ will tend to the uniform distribution (see Figure \ref{fig:entmax_temperature} for an example illustration). \andre{move this to sec 3?}



\section{\MakeUppercase{Conformalizing Sparse Transformations}}
\label{sec:proposed}

We now present a connection between sparse transformations and conformal prediction, via the design of a non-conformity score for which the prediction sets given by conformal prediction are the same as the support of the sparsemax output (more generally, $\entalpha$-entmax with $\entalpha>1$)  where the calibration step corresponds to setting $\beta$ in temperature scaling. 

%Let us consider the same classification task and notation introduced in Section \ref{subsec:entmax}.

\paragraph{Sparsity as set prediction}
Considering that, for $\entalpha > 1$, the $\entalpha$-entmax transformations can produce sparse outputs and that the level of sparsity can be controlled through the coefficient $\beta$, the support of the output can be interpreted as a prediction set for a given input $\bm{x} \in \mathcal{X}$, where $\bm{z}= f(\bm{x}) \in \mathbb{R}^K$ is the vector of label scores produced by a trained model. For notational convenience, we assume throughout that the entries of $\bm{z}$ are sorted in descending order, $z_1 \ge z_2 \ge ... \ge z_K$. 


\subsection{Conformalizing sparsemax}\label{subsec:sparsemax_cp}

Let us start with sparsemax. 
Taking a closer look at Algorithm~\ref{alg:sparsemax}, we start by noting that the inequality in line~\ref{alg:sparsemax_line_k} can be equivalently written as\footnote{\rebuttal{By noticing that $1+jz_j > \sum_{k=1}^{j} z_k \Leftrightarrow \left(\sum_{k=1}^{j-1} z_k\right) + z_j -z_j - (j-1)z_j < 1\Leftrightarrow \sum_{k=1}^{j-1} (z_{k} - z_{j}) < 1$.}} $\sum_{k=1}^{j-1} (z_{k} - z_{j}) < 1$. Let $S(\bm{z}) := \{j \in [K] : \mathsf{sparsemax}_j(\bm{z}) > 0\}$ denote the support of $\mathsf{sparsemax}(\bm{z})$, \textit{i.e.}, the labels that are assigned strictly positive probability. 
Scaling the label scores by the coefficient $\beta > 0$, we obtain a necessary and sufficient condition for the $j\textsuperscript{th}$ highest ranked label to be in the support $S(\beta \bm{z})$:
\begin{align}\label{eq:support_sparsemax}
    j \in S(\beta \bm{z}) \iff \sum_{k=1}^{j-1} (z_k - z_j) < \beta^{-1}.
\end{align}
This fact leads to  the following proposition.
\begin{proposition}\label{prop:sparsemax_conformal}
Let $C_\alpha: \mathcal{X}\rightarrow 2^\mathcal{Y}$ be a conformal predictor (as described in \S\ref{subsec:background_cp}). 
Define the following nonconformity score:
\begin{align}\label{eq:nonconf_score_sparsemax}
s(x,y) = \sum_{k=1}^{k(y)-1} (z_k - z_{k(y)}),
\end{align}
where $k(y)$ is the index of label $y$ in the sorted array $\bm{z}$, 
and let $\hat{q}$ be the $\lceil(n+1)(1-\alpha)\rceil/n$ empirical quantile of the set of calibration scores. 
Then, setting the sparsemax temperature as $\beta^{-1} := \hat{q}$ at test time leads to prediction sets $C_\alpha(x) = S(\beta\bm{z})$ achieving the desired $(1-\alpha)$ coverage in expectation. 
\end{proposition}

\begin{proof}
    This follows directly from the coverage guarantee of conformal prediction \eqref{eq:coverage} and condition \eqref{eq:support_sparsemax}. 
\end{proof}

The non-conformity score \eqref{eq:nonconf_score_sparsemax} has an intuitive interpretation: it sums the score differences between each of the top ranked labels and the label $y$. The lower the rank of $y$, the larger the number of terms in this sum  will be; conversely, the lower the score of $y$ is compared to the scores of the top labels, the larger the total sum will be. If this sum is larger than a threshold, the label will not be included in the prediction set. 

\subsection{Conformalizing $\gamma$-entmax} \label{subsec:entmax_cp}
We next turn to the more general case of $\entalpha$-entmax. 
Let $S(\beta \bm{z}; \entalpha) := \{j \in [K] : \entalpha\text{-entmax}_j(\beta\bm{z})>0\}$ denote the support of the distribution induced by the $\entalpha$-entmax transformation with  temperature $\beta^{-1}$ on the score vector $\bm{z}$. 
%We also define $\pi$ as the permutation over $[K]$ such that $z_{\pi(1)} \ge z_{\pi(2)} \ge ... \ge z_{\pi(K)}$ are the entries of $\bm{z}$ sorted in ascending order. 
We start with the following result, which generalizes \eqref{eq:support_sparsemax}.

\begin{proposition}\label{prop:tau}
	The following condition characterizes the set of labels in the support $S(\beta\bm{z}; \entalpha)$: 
	\begin{equation}\label{eq:support_conditon}
j \in S(\beta \bm{z}; \entalpha) \iff \sum_{k=1}^{j-1} \big[(\entalpha-1)\beta (z_{k}-z_{j})\big]^{\frac{1}{\entalpha-1}} < 1.
	\end{equation}
\end{proposition}
\begin{proof}
	The proof can be found in Appendix~\ref{sec:proof_tau}.
\end{proof}

Defining $\delta = 1/(\entalpha - 1)$ and letting $\|\bm{v}\|_\delta := (\sum_{k=1}^{K} |v_k|^\delta)^\frac{1}{\delta}$ be the $\delta$-norm in $\mathbb{R}^K$, we can equivalently express \eqref{eq:support_conditon} as 
\begin{equation} \label{eq:support_norm_eq}
    j \in S(\beta\bm{z}, \entalpha) \iff \|\bm{z}_{1:(j-1)} - z_{j}\bm{1}\|_\delta < \delta\beta^{-1}. 
\end{equation}
The sparsemax case \eqref{eq:support_sparsemax} is recovered by setting $\entalpha=2$ (equivalently, $\delta=1$) in  \eqref{eq:support_norm_eq}. 
We then have the following result, generalizing Proposition~\ref{prop:sparsemax_conformal}:

%\begin{prop}
%	Let $\|\mathbf{v}\|_p = \left(\sum_{i=1}^{K} |v_i|^p\right)^\frac{1}{p}$, be the usual $p$-norm defined in $\mathbb{R}^K$, for $p\geq1$; $\mathbf{z}_{1:j-1} = \begin{bmatrix} z_1  & ... &  z_{j-1} \end{bmatrix}^T$, and $\mathbf{1} = \begin{bmatrix} 1& ... & 1 \end{bmatrix}^T$, where $\mathbf{z}_{1:j-1},\mathbf{1}\in \mathbb{R}^{j-1}$, then: 
	
%	\margarida{unsure how standalone these need to be - do i point to previously defined concepts?}
%	\begin{equation}
%		j\in S(\beta^{-1}\mathbf{z}) \iff \|\mathbf{z}_{1:j-1} - z_j\mathbf{1}\|_\gamma < \frac{\gamma}{\beta^{-1}},
%		\label{eq:support_norm_eq}
%	\end{equation}
%	where $\gamma = \frac{1}{\alpha-1}$.
%\end{prop}
%\begin{proof}
	
%\end{proof}

\begin{proposition}\label{prop:entmax_conformal}
Let $C_\alpha: \mathcal{X}\rightarrow 2^\mathcal{Y}$, be a conformal predictor (as described in  \S\ref{subsec:background_cp}). 
Define the following nonconformity score:
\begin{align}\label{eq:nonconf_score_entmax}
s(x,y) = \|\bm{z}_{1:k(y)}-z_{k(y)}\mathbf{1}\|_\delta,
\end{align}
where $\hat{q}$ is the $\lceil(n+1)(1-\alpha)\rceil/n$ empirical quantile of the set of calibration scores. 
Then, setting the $\entalpha$-entmax temperature as $\beta^{-1} := \delta^{-1}\hat{q}$ at test time leads to prediction sets $C_\alpha(x) = S(\beta\bm{z}; \entalpha)$ achieving the desired $(1-\alpha)$ coverage in expectation. 
\end{proposition}

\begin{proof}
    This again follows directly from the conformal prediction coverage guarantee \eqref{eq:coverage} and condition \eqref{eq:support_norm_eq}. 
\end{proof}

Proposition \ref{prop:entmax_conformal} states that, through an appropriate choice of non-conformity score, a conformal predictor generates prediction sets corresponding to the support of the $\entalpha$-entmax transformation with temperature $\beta^{-1} = \delta^{-1}\hat{q}$, \rebuttal{as illustrated in Figure \ref{fig:diagram}}. 
Intuitively, the nonconformity score accumulates the differences between the largest label scores and the score of the correct label, returning the $\delta$-norm of the vector of differences. For example, when $\gamma=1.5$ (1.5-entmax), we have $\delta=2$, which corresponds to the Euclidean norm. The non-conformity score \eqref{eq:nonconf_score_entmax} is always non-negative and it is zero if $k(y)=1$. 

\paragraph{Log-margin} 
Interestingly, the non-conformity score \eqref{eq:nonconf_score_entmax} also works for softmax ($\entalpha=1$, \textit{i.e.}, $\delta = +\infty$):
\begin{align}\label{eq:nonconf_score_softmax_new}
    s(x,y) &= \|\bm{z}_{1:k(y)}-z_{k(y)}\mathbf{1}\|_\infty = z_1 - z_{k(y)} \nonumber\\
    &= \log \frac{p_1}{p_{k(y)}},
\end{align}
which is the log-odds ratio between the most probable class and the true one.
Since the $\log$ function is monotonic, calibration of this non-conformity score leads to thresholding the odds ratio $p_1/ p_{k(y)}$, which has an intuitive interpretation: labels whose probability is above a fraction of that of the most probable label are included in the prediction set.\footnote{This non-conformity score, although reminiscent of the previously used \textit{margin score} \citep{johansson_marginscores,linusson_margin}, uses the margin (distance to the maximum) on model output, not on the probability estimates.} However, the interpretation as temperature calibration no longer applies, since softmax is not sparse. We also experiment with the non-conformity score \eqref{eq:nonconf_score_softmax_new} in \S\ref{sect:experiments}. 

\section{\MakeUppercase{Experiments}}
\label{sect:experiments}

To assess the performance of the non-conformity scores introduced in \S\ref{sec:proposed} we report experiments on several classification tasks, comparing the proposed strategies with standard conformal prediction  over several dimensions: coverage, efficiency (prediction set size), and adaptiveness, at several different confidence levels.  

\subsection{Experimental Setup}
 \paragraph{Datasets} We evaluate all approaches on tasks of varying difficulty: image classification on the CIFAR10, CIFAR100, and ImageNet datasets \citep{cifars,imagenet} and text classification on the 20 Newsgroups dataset \citep{twenty_newsgroups_113}. For each dataset, the original test data is split into calibration ($40\%$) and test sets ($60\%$) and the results reported are averaged over 5 random splits.

\paragraph{Models} For the CIFAR100 and ImageNet datasets, we finetune the \textit{vision transformer} (ViT) model \citep{dosovitskiy2021imageworth16x16words}, obtaining average accuracies of approximately $0.86$ and $0.81$ on the test set, respectively. As for the 20 Newsgroups dataset, we finetune a BERT base model \citep{Devlin2019BERTPO} for sequence classification, obtaining an average test accuracy of $0.74$. Given the simplicity of the task on the CIFAR10 dataset, we train a convolutional neural network from scratch with final average test accuracy of $0.84$. More model evaluation details can be found in Appendix \ref{subsec:modelling}.

\begin{table*}[t]
\centering
\caption{Empirical coverage of different conformal procedures on the test set (averaged over 5 splits).} 
\smallskip
\begin{tabular}{l|l|cccccc}
\toprule
$\mathbf{\alpha}$ & \textbf{Dataset} & $\mathsf{RAPS}$ & $1.5$-$\mathsf{entmax}$ & $\mathsf{log\text{-}margin}$ & $\mathsf{opt\text{-}entmax}$ & $\mathsf{InvProb}$ & $\mathsf{sparsemax}$ \\
\midrule
\multirow{4}{*}{0.01} & CIFAR10    & 0.990 & 0.990 & 0.990 & 0.989 & 0.990 & 0.989 \\
                      & CIFAR100   & 0.991 & 0.991 & 0.991 & 0.991 & 0.990 & 0.991 \\
                      & ImageNet   & 0.990 & 0.990 & 0.990 & 0.990 & 0.990 & 0.990 \\
                      & NewsGroups & 0.989 & 0.988 & 0.989 & 0.990 & 0.989 & 0.989 \\
\midrule
\multirow{4}{*}{0.10} & CIFAR10    & 0.899 & 0.900 & 0.901 & 0.900 & 0.900 & 0.900 \\
                      & CIFAR100   & 0.897 & 0.899 & 0.900 & 0.899 & 0.899 & 0.900 \\
                      & ImageNet   & 0.899 & 0.899 & 0.899 & 0.899 & 0.899 & 0.900 \\
                      & NewsGroups & 0.895 & 0.900 & 0.902 & 0.900 & 0.901 & 0.900 \\
\bottomrule
\end{tabular}
\label{tab:coverage}
\end{table*}



\paragraph{Conformal procedures} We experiment with two commonly used conformal procedures, both involving the softmax transformation:
\begin{itemize}
    \item $\mathsf{InvProb}$: this is the standard conformal prediction (\S \ref{subsec:background_cp}) with non-conformity score $s(x,y) = 1 - \hat{p}(y|x)$, where $\hat{p}(y|x)$ is the softmax output corresponding to class $y$;
    \item $\mathsf{RAPS}$: this is a modification of the standard procedure called \textit{regularized adaptive prediction sets} \citep{angelopoulos2022uncertaintysetsimageclassifiers}, which makes use of two regularization parameters to improve the efficiency of the \textit{adaptive predictive sets} (APS) predictors \citep{romano2020classificationvalidadaptivecoverage}. In prior work, this has shown good efficiency and adaptiveness properties. \rebuttal{Implementation of the $\mathsf{RAPS}$ procedure is done following the original paper\footnote{Since using the same set to both calibrate the conformal predictor and find the optimal regularization hyperparameters would violate the exchangeability assumption, we split the original calibration data into two subsets for this procedure.}} (additional details can be found in Appendix \ref{subsec:raps}).
\end{itemize}
We compare these approaches with the following proposed ones: 
\begin{itemize}
    \item $\entalpha\text{-}\mathsf{entmax}$: this applies conformal prediction with the non-conformity score (\ref{eq:nonconf_score_entmax}) as described in Proposition \ref{prop:entmax_conformal}, with $1<\gamma\leq 2$, which has a temperature scaling interpretation. When $\gamma=2$, we call the procedure $\mathsf{sparsemax}$, corresponding to non-conformity score (\ref{eq:nonconf_score_sparsemax});
    \item $\mathsf{log\text{-}margin}$: this uses the score defined in (\ref{eq:nonconf_score_softmax_new}), which corresponds to $\gamma=1$ (softmax), also a particular case of the non-conformity score (\ref{eq:nonconf_score_entmax}), but without an interpretation in terms of temperature scaling;
    \item $\mathsf{opt\text{-}entmax}$: a procedure where $\gamma$ is treated as a hyperparameter ($1<\gamma<2$), tuned to minimize the average prediction set size. \rebuttal{This is done by splitting the original calibration set in two: one for conformal prediction calibration and the second for the choice of optimal $\gamma$. Additional setup details can be found in Appendix \ref{subsec:opt_entmax}.}
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/all_set_sizes_final}
\caption{Average prediction set size as a function of significance level $\alpha$. \label{fig:avg_set_size_all}}
\end{figure*}


\subsection{Coverage and Efficiency}
Table \ref{tab:coverage} shows the average empirical coverage obtained by the different methods on the test set for two confidence levels $1-\alpha$, with $\alpha \in \{0.01, 0.1\}$. 
We observe that all methods achieve a marginal coverage close to the theoretical bound of $1-\alpha$, as expected (see Appendix \ref{subsec:extra_coverage} for coverage analysis for more values of $\alpha$).


In order to evaluate the efficiency of the predictors, we use two common metrics: the \textit{average set size} and the \textit{singleton ratio} (fraction of prediction sets containing a single element). 
We vary $\alpha \in [0.01, 0.1]$ to study different confidence levels. 

\paragraph{Average set size} As can be seen in Figure \ref{fig:avg_set_size_all}, $\optentmax$ and $\mathsf{InvProb}$ are the most efficient set predictors across almost all tasks and confidence levels (with $\optentmax$ superior to $\mathsf{InvProb}$ in NewsGroups), both dominating $\mathsf{RAPS}$. The 1.5-$\entmax$ is in general very competitive, and so is the $\mathsf{log\text{-}margin}$ predictor (except for ImageNet). The $\sparsemax$ predictor  generally yields larger sets on average, especially for high confidence levels. 

In $\entalpha\text{-}\entmax$, $\gamma$ can be seen as an additional hyperparameter, since  for each $\alpha$, it can be tuned on the calibration set for a given metric, 
as $\optentmax$ does for average set size. An example of the behavior of $\entalpha\text{-}\entmax$ with varying $\gamma$ is shown in Figure \ref{fig:gamma_entmax}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/gamma_entmax}
    \caption{Average set size as a function of $\alpha$, for the ImageNet dataset with varying $\gamma$ for $\entmax$.}
    \label{fig:gamma_entmax}
\end{figure}

\paragraph{Singleton ratio} Considering the ratio of singleton predictions, shown in Figure \ref{fig:singleton_ratio}, we observe significant differences between methods: $\raps$ and $\logmargin$ have the lowest  and highest ratio, respectively, across all tasks and confidence levels; $\invprob$, $\logmargin$ and $\optentmax$ output singletons even for $\alpha=0.01$ for all tasks except for 20 Newsgroups, where no method produces singletons, which could be related to the lower accuracy of the original predictive model. The coverage of singleton predictions, also shown in Figure \ref{fig:singleton_ratio}, is approximately $1-\alpha$, as desired, for all methods.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/singleton_final}
\caption{Ratio of singleton prediction sets (bars) and their coverage (points) for different values of confidence level $1-\alpha$ (NewsGroups is absent for $\alpha=0.01$ because no singletons were predicted by either method).\label{fig:singleton_ratio}}
\end{figure*}

\subsection{Adaptiveness} Given that conformal predictors are unable to provide \textit{conditional} coverage theoretical guarantees \citep{pmlr-v25-vovk12}, \textit{i.e.}, there is no lower bound on $\mathbb{P}\big( Y_\text{test} \in \mathcal{C}_\alpha(X_\text{test})|X_\text{test} \big)$ without extra assumptions, different measures have been proposed to calculate proximity to conditional coverage. Ideally, for any given partition of the data, we would have a coverage close to the $1-\alpha$ bound. 
\citet{angelopoulos2022uncertaintysetsimageclassifiers} introduced an adaptiveness criterion based on \textit{size-stratified coverage}, by evaluating coverage in a partition based on the size of the predicted sets. Table \ref{tab:imagenet_size_coverage} shows the size-stratified coverage for the ImageNet dataset, with $\alpha=0.01$ and $\alpha=0.1$. For $\alpha=0.1$, $\logmargin$ stands out from other methods, achieving a lower deviation from exact coverage on all set cardinalities. Size-stratified coverage improves for lower values of $\alpha$, and $\invprob$, $\logmargin$ and $\optentmax$ exhibit the desired behavior---achieving expected coverage while still predicting small sets for easier examples. 

\begin{table*}
\centering
\caption{Size-stratified coverage for the ImageNet dataset with $\alpha=0.01$ (top) and $\alpha=0.1$ (bottom).}
\smallskip 

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
\midrule
0 - 1 &    5339 &  0.994 &      0 &   -- &       7088 &  0.992 &         44 &  1.000 &         0 &    -- &       3899 &  0.996 \\
2 - 3 &    7665 &  0.992 &      0 &   -- &       7970 &  0.989 &       1559 &  1.000 &         0 &    -- &       7383 &  0.994 \\
4 - 6 &    5403 &  0.990 &      0 &   -- &       4821 &  0.986 &       5196 &  0.998 &         0 &    -- &     6159 &  0.991 \\
7 -10 &    3221 &  0.990 &      0 &   -- &       2604 &  0.982 &       7585 &  0.997 &       177 &  1.000 &     3814 &  0.992 \\
11 - 1000  &    8372 &  0.981 &  30000 &  0.99 &       7517 &  0.987 &      15616 &  0.982 &     29823 &  0.989 &       8745 &  0.978 \\
\bottomrule
\end{tabular}

\begin{tabular}{lrrrrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
\midrule
0 - 1   & 19610 & 0.932 & 17408 & 0.948 & 21664 & 0.924 & 20939 & 0.933 & 19820 & 0.942 & 20074 & 0.940 \\
2 - 3   & 9321  & 0.851 & 8862  & 0.888 & 6346  & 0.859 & 7358  & 0.849 & 9098  & 0.838 & 8731  & 0.839 \\
4 - 6   & 917   & 0.808 & 2696  & 0.774 & 1404  & 0.767 & 1407  & 0.728 & 1048  & 0.618 & 1140  & 0.636 \\
7 - 10  & 4     & 0.500 & 753   & 0.616 & 373   & 0.743 & 262   & 0.588 & 34    & 0.6 & 50    & 0.519 \\
11 - 1000 & 0   &  --  & 0     &  --  & 213   & 0.653 & 34    & 0.559 & 0     &  --  & 1     &  0  \\
\bottomrule
\end{tabular}
\label{tab:imagenet_size_coverage}
\end{table*}



\section{\MakeUppercase{Discussion}}
\label{sec:discussion}

\paragraph{Choosing a score} The choice of non-conformity score plays a crucial role in the efficiency of a conformal predictor \citep{aleksandrova21a_impact}; consequently, finding useful model-agnostic scores is key for the adoption of conformal prediction as an actionable uncertainty quantification framework. In \S\ref{sec:proposed}, we introduced a new family of non-conformity measures that we show to be competitive with standard scores on different classification tasks (\S \ref{sect:experiments}). Expanding the range of possible non-conformity measures is particularly relevant, considering that the optimal score choice is known to be task-dependent \citep{laxhammar}. Additionally, we show that these scores can be used to tune the adaptiveness of a conformal predictor.

\paragraph{Temperature scaling connection} 
Conformal prediction and temperature scaling are two popular, yet very distinct, approaches to the task of model calibration and confidence estimation. The fact that there is an equivalence between the two methods for a family of activation functions is not only intrinsically interesting but also promising, as it opens some research threads: designing new non-conformity scores from sparse transformations, or comparing an activation natural calibration capacity through the evaluation of the correspondent conformal predictor.

\section{\MakeUppercase{Conclusion}}
\label{sec:conclusion}
In this paper, we showed a novel connection between conformal prediction and temperature scaling in sparse activation functions. Specifically, we derived  new non-conformity scores which make conformal set prediction equivalent to temperature scaling of the $\entalpha\text{-}\mathsf{entmax}$ family of sparse activations. This connection allows $\entalpha\text{-}\mathsf{entmax}$ with the proposed calibration procedure to inherit the strong coverage guarantees of conformal prediction. Experiments with computer vision and text classification benchmarks show the efficiency and adaptiveness of the proposed non-conformity scores, showing their practical usefulness. 
\section{\MakeUppercase{Acknowledgements}}
\label{sec:acknowledgement}
\rebuttal{
This work was supported by the Portuguese Recovery
and Resilience Plan through project C645008882-
00000055 (NextGenAI - Center for Responsible AI), by the EU's
Horizon Europe Research and Innovation Actions
(UTTER, contract 101070631), by the project
DECOLLAGE (ERC-2022-CoG 101088763), 
and by FCT/MECI through national funds and when applicable co-funded EU funds under UID/50008: Instituto de Telecomunicações. 
}
\bibliography{references}

\bibliographystyle{unsrtnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\section*{Checklist}

% %%% BEGIN INSTRUCTIONS %%%
%The checklist follows the references. For each question, choose your answer from the three possible options: Yes, No, Not Applicable.  You are encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description (1-2 sentences). 
%Please do not modify the questions.  Note that the Checklist section does not count towards the page limit. Not including the checklist in the first submission won't result in desk rejection, although in such case we will ask you to upload it during the author response period and include it in camera ready (if accepted).

%\textbf{In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.}
% %%% END INSTRUCTIONS %%%


 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes/No/Not Applicable]
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes/No/Not Applicable]
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes/No/Not Applicable]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [Yes/No/Not Applicable]
   \item Complete proofs of all theoretical results. [Yes/No/Not Applicable]
   \item Clear explanations of any assumptions. [Yes/No/Not Applicable]     
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes/No/Not Applicable]
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes/No/Not Applicable]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes/No/Not Applicable]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes/No/Not Applicable]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [Yes/No/Not Applicable]
   \item The license information of the assets, if applicable. [Yes/No/Not Applicable]
   \item New assets either in the supplemental material or as a URL, if applicable. [Yes/No/Not Applicable]
   \item Information about consent from data providers/curators. [Yes/No/Not Applicable]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Yes/No/Not Applicable]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Yes/No/Not Applicable]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Yes/No/Not Applicable]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Yes/No/Not Applicable]
 \end{enumerate}

 \end{enumerate}
\end{comment}

\clearpage

\appendix
\onecolumn

\input{appendix}

\end{document}