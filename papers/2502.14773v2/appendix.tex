
\section{Proof of Proposition~\ref{prop:tau}}\label{sec:proof_tau}

Let $\pi$ be the permutation over $[K]$ such that $z_{\pi(1)} \ge z_{\pi(2)} \ge ... \ge z_{\pi(K)}$ are the entries of $\bm{z}$ sorted in ascending order. 
We have $\entalpha\text{-entmax}_j(\beta\bm{z}) = \left[ (\entalpha - 1)\beta z_j - \tau \right]_{+}^{\frac{1}{\entalpha - 1}}$, where $\tau$ satisfies 
\begin{align}\label{eq:proof_equality}
    \sum_{k = 1}^{|S(\beta \bm{z})|} [(\entalpha-1) \beta z_{\pi(k)} - \tau]^{\frac{1}{\entalpha - 1}} = 1.
\end{align}

We prove the desired equivalence by showing implication in both directions, as follows. 

\begin{itemize}
    \item \framebox{$\pi(j) \in S(\beta \bm{z}) \Longrightarrow \sum_{k=1}^{j-1} \left[ (\entalpha - 1)\beta (z_{\pi(k)} - z_{\pi(j)}) \right]^{\frac{1}{\entalpha - 1}} < 1$}

To prove this direction, note that, if $\pi(j) \in S(\beta \bm{z})$, we must have by \eqref{eq:entmax_def} that $(\entalpha - 1) \beta z_{\pi(j)} > \tau$. 
Therefore, we have that, for all $k \in [K]$, $(\entalpha - 1) \beta (z_{\pi(k)} - z_{\pi(j)}) < (\entalpha - 1) \beta z_{\pi(k)} - \tau$. 
As a consequence, 
\begin{align}
    \sum_{k=1}^{j-1} \left[ (\entalpha - 1)\beta (z_{\pi(k)} - z_{\pi(j)}) \right]^{\frac{1}{\entalpha - 1}} &\le \sum_{k=1}^{|S(\beta\bm{z})|} \left[ (\entalpha - 1)\beta (z_{\pi(k)} - z_{\pi(j)}) \right]^{\frac{1}{\entalpha - 1}} \nonumber\\
    &< \sum_{k=1}^{|S(\beta\bm{z})|} \left[ (\entalpha - 1)\beta z_{\pi(k)} - \tau \right]^{\frac{1}{\entalpha - 1}} = 1,
\end{align}
where the last equality comes from \eqref{eq:proof_equality}. 


    \item \framebox{$\pi(j) \in S(\beta \bm{z}) \Longleftarrow \sum_{k=1}^{j-1} \left[ (\entalpha - 1)\beta (z_{\pi(j)} - z_{\pi(k)}) \right]^{\frac{1}{\entalpha - 1}} < 1$}

We show the reverse implication by showing that $\pi(j) \notin S(\beta \bm{z}) \Longrightarrow \sum_{k=1}^{j-1} \left[ (\entalpha - 1)\beta (z_{\pi(j)} - z_{\pi(k)}) \right]^{\frac{1}{\entalpha - 1}} \ge 1$. 
If $\pi(j) \notin S(\beta \bm{z})$, we must have by \eqref{eq:entmax_def} that $(\entalpha - 1) \beta z_{\pi(j)} \le \tau$. 
Therefore, we have that, for all $k \in [K]$, $(\entalpha - 1) \beta (z_{\pi(k)} - z_{\pi(j)}) \ge (\entalpha - 1) \beta z_{\pi(k)} - \tau$. 
As a consequence, 
\begin{align}
    \sum_{k=1}^{j-1} \left[ (\entalpha - 1)\beta (z_{\pi(k)} - z_{\pi(j)}) \right]^{\frac{1}{\entalpha - 1}} &\ge \sum_{k=1}^{|S(\beta\bm{z})|} \left[ (\entalpha - 1)\beta (z_{\pi(k)} - z_{\pi(j)}) \right]^{\frac{1}{\entalpha - 1}} \nonumber\\
    &\ge \sum_{k=1}^{|S(\beta\bm{z})|} \left[ (\entalpha - 1)\beta z_{\pi(k)} - \tau \right]^{\frac{1}{\entalpha - 1}} = 1.
\end{align}

\end{itemize}
\rebuttal{
\section{Additional Experimental Details}
\label{sec:extra_details}

\subsection{$\mathsf{RAPS}$}
\label{subsec:raps}
The $\mathsf{RAPS}$ procedure was implemented according to the original protocol introduced by \citet{angelopoulos2022uncertaintysetsimageclassifiers} by adapting the \href{https://github.com/aangelopoulos/conformal-prediction/blob/main/notebooks/imagenet-raps.ipynb}{code} provided by those authors. The method uses two hyperparameters to regularize the original \textit{adaptive prediction sets} procedure \citep{romano2020classificationvalidadaptivecoverage}: $\lambda_\text{reg}$ --- a regularization penalty added to discourage inclusion in the prediction set, and $k_\text{reg}$ --- the order from which classes receive the penalty term. We split the original calibration data into two sets: calibration data ($60\%$) and hyperparameter tuning data ($40\%$). For each split of the dataset, we find the pair of hyperparameters that minimizes the average prediction set size on hyperparameter tuning set, with $\lambda_\text{reg}\in \{0.001,0.01,0.1,1\}$ and $k_\text{reg} \in \{1,5,10,50\}$.



\subsection{$\mathsf{opt\text{-}entmax}$}
\label{subsec:opt_entmax}

The $\mathsf{opt\text{-}entmax}$ procedure splits the calibration data into two sets: calibration data ($60\%$) to perform conformal prediction with $\entalpha\text{-}\mathsf{entmax}$ for varying values of $\gamma\in\{1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9\}$ ; and a hyperparameter tuning set ($40\%$) where the average set size is measured and from which the optimal $\gamma$ is chosen.
}
\section{Experimental Results}
\label{sec:extra_results}


\subsection{Model Accuracy}\label{subsec:modelling}
All models were evaluated on each of the 5 calibration-test set splits. The average and standard deviation of model accuracy over splits can be found in Table \ref{tab:accuracies}, along with the calibration and set sizes for each dataset.

\begin{table}[]
    \centering
\caption{Accuracy of each trained model: calibration and test set sizes, average and standard deviation of accuracy over the 5 different splits.}
\smallskip
\def\arraystretch{1.2}
\begin{tabular}{lllllll}
\cline{2-7}
           & \multicolumn{3}{c}{\textbf{Test}} & \multicolumn{3}{c}{\textbf{Calibration}} \\ \cline{2-7} 
 &
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Size}}} &
  \multicolumn{2}{c}{\textbf{Accuracy}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Size}}} &
  \multicolumn{2}{c}{\textbf{Accuracy}} \\ \cline{3-4} \cline{6-7} 
 &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{avg} &
  \multicolumn{1}{c}{std} &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{avg} &
  \multicolumn{1}{c}{std} \\ \hline
CIFAR10    & 6000         & 0.838        & 0.005       & 4000      & 0.841     & 0.003     \\
CIFAR100   & 6000         & 0.860        & 0.003       & 4000      & 0.858     & 0.002     \\
ImageNet   & 30000        & 0.805        & 0.001       & 20000     & 0.805     & 0.001     \\
NewsGroups & 2261         & 0.744        & 0.006       & 1508      & 0.742     & 0.004     \\ \hline
\end{tabular}
\label{tab:accuracies}
\end{table}

\subsection{Coverage}\label{subsec:extra_coverage}

The coverage results of all methods over the 5 splits of calibration is shown in Figure \ref{fig:coverage_all}.
\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/coverage_plot_all}
    \caption{Coverage on the test set as a function of $\alpha$, over the 5 splits of calibration and test data.}
    \label{fig:coverage_all}
\end{figure}

\rebuttal{
\subsection{Analysis of $\mathsf{opt\text{-}entmax}$}
\label{subsec:all_optentmax_gamma}

Figure \ref{fig:fixed_alpha_lambdas} shows how varying the value of $\gamma$ for the non-conformity score affects the average prediction set size for a fixed value of $\alpha$. It is clear that the optimal value of $\gamma$ depends on both the task and the confidence level.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fixed_alpha_gammas}
    \caption{Average set size varying with $\gamma$ parameter, on separate calibration set for different datasets and values of $\alpha$.}
    \label{fig:fixed_alpha_lambdas}
\end{figure}
}
\subsection{Adaptiveness}\label{subsec:extra_apdaptiveness}

In Figure \ref{fig:sscv} we can see the size-stratified coverage violation (SSCV) --- introduced by \citet{angelopoulos2022uncertaintysetsimageclassifiers}, it measures the maximum deviation from the desired coverage $1-\alpha$. Partitioning the possible size cardinalities into $G$ bins, $B_1, ..., B_G$, let $\mathcal{I}_g$ be the set of observations falling in bin $g$, with $g = 1,...,G$, the SSCV of a predictor $C_\alpha$, for that bin partition is given by:

\begin{equation}
    \text{SSCV}(C, \{B_1,...,B_G\}) = \sup_g \left| \frac{\left\{i : Y_i \in \mathcal{C}_\alpha(X_i), i \in \mathcal{I}_g \right\}}{|\mathcal{J}_j|} - (1 - \alpha) \right|
\label{eq:sscv}
\end{equation}
\begin{figure}[]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/sscv}
    \caption{Size-stratified coverage violation (SSCV) for all methods and datasets, for $\alpha \in \{0.01,0.05,0.1\}$}
    \label{fig:sscv}
\end{figure}

Size-stratified coverage for the ImageNet dataset ($\alpha=0.05$) can be found in Table \ref{tab:imagenet_cov_005}. An equivalent analysis for datasets CIFAR100 and 20 NewsGroups can be found in Tables \ref{tab:coverage_cifar100} and \ref{tab:coverage_newsgroups}, respectively, for different values of $\alpha$.

\begin{table*}[]
\centering
\caption{Size-stratified coverage for the ImageNet dataset with $\alpha=0.05$.}
\smallskip 

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
        \midrule
        0 - 1  & 13745 & 0.975 & 12321 & 0.979 & 16286 & 0.966 & 13730 & 0.975 & 7260  & 0.992 & 11805 & 0.982 \\
        2 - 3  &  9579 & 0.958 &  4774 & 0.977 &  8144 & 0.945 & 10132 & 0.956 & 15693 & 0.968 & 11788 & 0.959 \\
        4 - 6  &  4189 & 0.903 &  8654 & 0.959 &  2653 & 0.912 &  3671 & 0.905 &  5681 & 0.885 &  4397 & 0.898 \\
        7 - 10 &  2046 & 0.835 &  3494 & 0.865 &  1244 & 0.916 &  1542 & 0.858 &  1226 & 0.742 &  1510 & 0.819 \\
        11 - 1000 &   440 & 0.793 &   570 & 0.684 &  1673 & 0.877 &   925 & 0.751 &   140 & 0.614 &   500 & 0.678 \\
        \bottomrule
\end{tabular}
\label{tab:imagenet_cov_005}
\end{table*}

\begin{table}
\caption{Size-stratified coverage for the CIFAR100 dataset with $\alpha=0.01$ (top), $\alpha=0.05$ (middle) and $\alpha=0.1$ (bottom).}
\centering
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
\midrule
        0 - 1  & 2222 & 0.999 & 1577 & 0.999 & 2422 & 0.998 &  795 & 0.999 &    0 & NaN   & 2397 & 0.998 \\
        2 - 3  & 1476 & 0.995 & 1015 & 0.998 & 1437 & 0.993 & 1464 & 0.999 &   12 & 1.000 & 1457 & 0.993 \\
        4 - 6 &  810 & 0.998 &  567 & 1.000 &  750 & 0.996 & 1416 & 1.000 &  190 & 1.000 &  762 & 0.997 \\
        7 - 10 &  457 & 0.991 &  898 & 0.998 &  414 & 0.990 &  977 & 0.993 & 1410 & 0.999 &  424 & 0.988 \\
        11 - 100 & 1035 & 0.980 & 1943 & 0.984 &  977 & 0.986 & 1348 & 0.976 & 4388 & 0.991 &  960 & 0.984 \\
        \bottomrule
\end{tabular}

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
        \midrule
        0 - 1 & 4089 & 0.976 & 3587 & 0.986 & 4312 & 0.970 & 4065 & 0.977 & 3634 & 0.987 & 3934 & 0.980 \\
        2 - 3  & 1357 & 0.917 & 1055 & 0.967 & 1139 & 0.913 & 1391 & 0.918 & 1954 & 0.916 & 1569 & 0.917 \\
        4 - 6  &  453 & 0.841 & 1316 & 0.872 &  346 & 0.867 &  423 & 0.830 &  393 & 0.756 &  432 & 0.806 \\
        7 - 10 &   99 & 0.677 &    0 & --   &  121 & 0.826 &  114 & 0.728 &   18 & 0.556 &   62 & 0.597 \\
        11 - 100  &    2 & 1.000 &    0 & --   &   82 & 0.805 &    7 & 0.286 &    1 & 1.000 &    3 & 0.667 \\
        \bottomrule
\end{tabular}
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
      \midrule
        0 - 1  & 5194 & 0.916 & 4456 & 0.952 & 5225 & 0.920 & 5182 & 0.924 & 5170 & 0.925 & 5170 & 0.925 \\
        2 - 3  &  786 & 0.785 & 1240 & 0.819 &  683 & 0.779 &  752 & 0.769 &  807 & 0.742 &  804 & 0.749 \\
        4 - 6 &    1 & 1.000 &  153 & 0.556 &   82 & 0.598 &   63 & 0.587 &   23 & 0.565 &   25 & 0.600 \\
        7 - 10 &    0 & --   &    0 & --   &    9 & 0.667 &    3 & 0.667 &    0 & --   &    1 & 0.000 \\
        11 - 100 &    0 & --   &    0 & --   &    1 & 1.000 &    0 & --   &    0 & --   &    0 & --   \\
        \bottomrule
\end{tabular}
\label{tab:coverage_cifar100}
\end{table}

\begin{table}
\caption{Size-stratified coverage for the 20 Newsgroups dataset with $\alpha=0.01$ (top), $\alpha=0.05$ (middle) and $\alpha=0.1$ (bottom).}
\centering
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
        \midrule
        0 - 1  &    0 & --   &    0 & --   &    0 & --   &    0 & --   &    0 & --   &    0 & --   \\
        2 - 3  &   77 & 0.987 &    0 & --   &   78 & 0.987 &    0 & --   &    0 & --   &   13 & 1.000 \\
        4 - 6  &  338 & 0.982 &    0 & --   &  345 & 0.983 &    0 & --   &    0 & --   &  279 & 0.978 \\
        7 - 10 &  718 & 0.986 &    0 & --   &  720 & 0.986 &   45 & 0.978 &   10 & 0.900 &  704 & 0.989 \\
        11 - 20  & 1128 & 0.994 & 2261 & 0.988 & 1118 & 0.994 & 2216 & 0.990 & 2251 & 0.988 & 1265 & 0.992 \\
        \bottomrule
\end{tabular}

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\
        \midrule
        0 - 1  & 1045 & 0.957 &   40 & 0.900 & 1047 & 0.956 &  745 & 0.969 &    0 & NaN   &  745 & 0.969 \\
       2 - 3  &  413 & 0.935 &  451 & 0.973 &  411 & 0.937 &  660 & 0.947 &  125 & 0.944 &  660 & 0.947 \\
        4 - 6   &  166 & 0.946 & 1441 & 0.948 &  167 & 0.946 &  393 & 0.954 &  588 & 0.912 &  393 & 0.954 \\
        7 - 10  &   97 & 0.948 &  213 & 0.859 &   95 & 0.947 &  229 & 0.904 & 1126 & 0.952 &  229 & 0.904 \\
        11 - 20 &  540 & 0.983 &  116 & 0.957 &  541 & 0.983 &  234 & 0.923 &  422 & 0.981 &  234 & 0.923 \\
        \bottomrule
\end{tabular}
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{size}} & \multicolumn{2}{c}{\textbf{InvProb}} & \multicolumn{2}{c}{\textbf{RAPS}} & \multicolumn{2}{c}{\textbf{log-margin}} & \multicolumn{2}{c}{\textbf{1.5-entmax}} & \multicolumn{2}{c}{\textbf{sparsemax}} & \multicolumn{2}{c}{\textbf{opt-entmax}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
 & n & cov & n & cov & n & cov & n & cov & n & cov & n & cov \\

        \midrule
        0 - 1  & 1481 & 0.918 & 1095 & 0.944 & 1481 & 0.918 & 1356 & 0.933 &  575 & 0.967 & 1226 & 0.939 \\
        2 - 3   &  320 & 0.856 &  311 & 0.945 &  312 & 0.853 &  471 & 0.885 & 1336 & 0.912 &  642 & 0.894 \\
        4 - 6  &  121 & 0.868 &  794 & 0.851 &  113 & 0.867 &  208 & 0.837 &  263 & 0.722 &  234 & 0.786 \\
        7 - 10 &   92 & 0.946 &   61 & 0.541 &   77 & 0.961 &  102 & 0.833 &   27 & 0.778 &   92 & 0.793 \\
        11 - 20 &  247 & 0.964 &    0 & --   &  278 & 0.975 &  124 & 0.944 &   60 & 0.883 &   67 & 0.985 \\
        \bottomrule
\end{tabular}
\label{tab:coverage_newsgroups}
\end{table}
