\section{Background and Related Work}
\label{Sec2}

\subsection{Federated Backdoor Attacks}\label{Sec2.2}

In federated learning, a backdoor attack involves an adversary embedding malicious triggers into the training data or model updates of local clients. During the inference phase, the global model behaves anomalously when encountering backdoor samples, while performing normally otherwise. Federated backdoor attacks are generally divided into three categories: label flipping, backdoor triggers, and model injection.

\textbf{Label flipping attacks}. Naive label flipping \cite{R4} involves simply changing the label of a specific type of data to the target class. During the inference phase, these modified data can trigger a backdoor attack without affecting the model’s performance in other classes. Additionally, Wang et al. \cite{R6} introduce the concept of edge-case backdoors which are located at the tail of the input distribution. These examples, appearing with low probability in training and testing, lead to the model having low confidence in its correct prediction. Label flipping of such samples can easily create backdoor instances, such as flipping the label from “green car” to “frog” in the CIFAR10 dataset. However, Label flipping attacks heavily rely on the compromised client's data distribution and can degrade the model's accuracy on target samples.

\textbf{Backdoor trigger attacks}. This attack first embeds the backdoor trigger into the training samples and then flips the labels. It allows for artificially specifying patterns as triggers or dynamically training the content of triggers. While extensive research on trigger-based attacks has been conducted in centralized learning \cite{R41}, relevant research in federated learning emphasizes the impact of distributed training strategies on backdoor trigger design. For example, Xie et al. \cite{R7} manually designed diverse small-scale backdoor triggers with the same label during the federated training and distributed them to different compromised clients for backdoor model training. During the inference phase, all small local triggers are combined into a global trigger, demonstrating a higher success rate for backdoor attacks. Dynamic training triggers, also known as model-dependent triggers, are proposed by Gong et al. \cite{R9}. In this method, each compromised client first pretends to be a clean client to participate in federated training, but when the global model approaches convergence, they use the global model to train their local trigger patches. In the subsequent federated round, the trained local trigger patches are implanted into the training data of compromised clients. During the inference stage, all local trigger patches are merged into a large global trigger and implanted into test samples. We term the aforementioned methods as patch-based federated backdoor attacks. In contrast, our method manufactures triggers that are consistent in size with the input image without taking into account trigger size, shape, or location. Our method obtains the global triggers by feature fusion of local triggers from all compromised clients and limits the trigger pixels to a small value to evade detection by defense mechanisms. Moreover, our method is based on intra-class attacks without flipping labels, which minimally impacts the model's accuracy on the target class samples.

\textbf{Model injection attacks}. Backdoor attacks based on model poisoning replace \cite{R8} or partially modify \cite{R38} the model parameters after local model training, followed by uploading these manipulated models to the server for aggregation. Since model replacement directly manipulates model parameters, the attack success rate is higher compared to data poisoning. However, the drawback lies in its reduced stealth. Poisoned models uploaded to the server are more easily detected by defenders, prompting attackers to combine model poisoning methods with boundary constraint-based approaches \cite{R11} to enhance concealment. Additionally, model injection attacks cannot be implemented alone \cite{R8}. They need to be carried out in conjunction with label flipping, which limits their application due to the high requirements.

\subsection{Federated Backdoor Defense}\label{Sec2.3}

It is assumed that the client's training is private, that is, the server/censor has no access rights to the client's training. In addition, both data poisoning and model poisoning will change the parameter distribution of the local model. Therefore, federated backdoor defense can be divided into three stages, targeting the front, middle, and back of federated aggregation \cite{R16}.

\textbf{Pre-aggregation defense}. Clustering methods are primarily utilized to differentiate poison models from benign models. These methods rely on distribution assumptions about client data. For instance, approaches like Auror \cite{R17} and Krum \cite{R18} assume that data from benign clients are independent and identically distributed (IID). In contrast, FoolsGold \cite{R19} and AFA \cite{R20} assume that benign data are non-IID. In addition to auditing the communication between the client and the server, AP2FL \cite{R52} also uses client model similarity detection to improve the security of federated aggregation. However, these methods also assume that malicious clients will behave similarly in each round. Given that client data is private and its distribution is unknown, these assumptions often lead to the failure of clustering-based defenses.

\textbf{In-aggregation defense}. Different approaches are considered, including those based on differential privacy (DP) \cite{R23}, model smoothness \cite{R25}, and robust aggregation rules \cite{R26}. The effectiveness of DP comes at the expense of the global model performance on the main learning task, especially when the client data is non-IID. Techniques such as model smoothness and parameter clipping are employed as additional modules to constrain the parameters of locally uploaded models, enhancing defense capabilities. However, determining the optimal clipping threshold often poses challenges, as it may impact the global model's performance on the main learning task. Robust aggregation rules represent an improvement over traditional FedAvg methods \cite{R1}, incorporating strategies like using the median of the local model as the global model's parameter \cite{R27} and employing a robust learning rate strategy \cite{R26}.

\textbf{Post-aggregation defense}. Strategies such as model structure deletion \cite{R28} and federated unlearning \cite{R42} are applied to eliminate backdoors in the global model. Techniques include removing low-activation neurons and knowledge distillation. However, these methods require more computation and additional public datasets.