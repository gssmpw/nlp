\section{Introduction} \label{Sec:Intro}
% EDA is a complex flow, multimodal + many types, AI for EDA

% Register Transfer Level (RTL)\cite{RTL,chu2006rtl} is a critical component in the field of Electronic Design Automation (EDA). RTL serves as a high-level abstraction that represents the digital logic of hardware designs, bridging the gap between hardware description languages and physical circuit implementation. EDA tools rely heavily on RTL to perform crucial tasks such as synthesis, verification, and optimization, making it a vital element in modern hardware design workflows. As the complexity of integrated circuits continues to grow, the need for efficient tools and datasets that aid in the understanding\cite{allam2024RTL-Repo} and generation\cite{cui2024OriGen,thakur2023benchmarking} of RTL code becomes increasingly important.

% % AIG
% As the rapid development of AI technologies continues, particularly in the area of large language models (LLMs)\cite{touvron2023llama,achiam2023gpt,touvron2023llama2}, these models have demonstrated remarkable capabilities in fields such as text generation, translation, and understanding. LLMs have shown their potential to revolutionize how we interact with complex data and systems by automatically learning patterns and structures. In recent years, LLMs have extended their influence into more technical fields, including code understanding and generation\cite{guo2024deepseekcoder,codellama,zhu2024deepseekcoderv2}, offering new possibilities for automating software and hardware design tasks\cite{thakur2023benchmarking,cui2024OriGen,nadimi2024multi}.

% Register Transfer Level (RTL) modeling stands as a pivotal and early step in the Electronic Design Automation (EDA) flow. RTL code, such as Verilog and VHDL, acts as a high-level abstraction that represents the functionality of hardward designs, bridging between design specifications and circuit implementations. Following RTL modeling, engineers across various departments in a semiconductor companies translate RTL code into netlists, floorplans and layouts. Consequently, RTL representation significantly impacts the quality of circuit designs. 

Register Transfer Level (RTL) modeling is a crucial step in the Electronic Design Automation (EDA) flow, where RTL code (e.g., Verilog and VHDL) represents hardware functionality, bridging design specifications and circuit implementations. Engineers then translate RTL into netlists, floorplans, and layouts, directly influencing circuit design quality.
Integrating Artificial Intelligence (AI) into the EDA flow, particularly for RTL tasks, promises to enhance modern chip design and accelerate time-to-market. 
% To enhance the current EDA tools for complex modern chip design and expedite time-to-market, integration of Artificial Intelligence (AI) techniques into the EDA flow, particularly for RTL-related tasks, has emerged as a promising avenue. 
For instance,\cite{allam2024RTL-Repo} employs large language models (LLMs) to understand and describe RTL designs in natural language to assist engineers.\cite{cui2024OriGen,thakur2023benchmarking} focus on generating RTL code from the design specification. Clearly, the effectiveness of deep learning models is heavily contingent upon the quality of the training data~\cite{chang2024dataisall}. However, we observe that existing RTL datasets suffer from notable limitations, hindering the application of AI-based solutions in practical RTL modeling and verification.


% Modern chip design is a multifaceted endeavor characterized by its intricacy, involving a diverse array of sub-modules, alongside numerous stages within the Electronic Design Automation (EDA) flow. The process of designing typically entails collaboration across many departments in a semiconductor company to translate the design specification into Register Transfer Level (RTL) code, netlist, floorplanning and layout. 

% To enhance the existing EDA tools and speed time-to-market, integration of Artificial Intelligence (AI) technique in EDA has emerged as an attractive direction. For instance, DeepGate Family~\cite{li2022deepgate, shi2023deepgate2, shi2024deepgate3} are trained on a large number of circuit netlists and employed into design for test~\cite{shi2022deeptpi} and power analysis~\cite{khan2024deepseq}. Besides, ~\cite{allam2024RTL-Repo, cui2024OriGen, thakur2023benchmarking} learn web-scale code and benefit hardware code generation. Clearly, the effectiveness of deep learning models is heavily contingent upon the quality of the training data~\cite{chang2024data}. However, we observe that the existing circuit datasets in EDA field suffer from notable limitations.

% Existing dataset: 1. for a single task (EPFL, ISCAS89, ITC99, NYU) 2. less types (CircuitNet), not suitable for AI4EDA. 
One prominent limitation is the narrow scope of most datasets. Since the semiconductor ecosystem is far inferior to software openness, the accessible designs are limited. Many datasets either focus on a limited set of circuit types, often centering around a single design such as processors~\cite{chai2022circuitnet, jiang2024circuitnet2}), or indiscriminately collect all Verilog files without accounting for their correctness and distribution~\cite{thakur2023benchmarking, wu2024edacorpus}. This restricted diversity hampers the generalizability and effectiveness of these datasets.

% Another critical limitation lies in the task-specific nature of existing datasets. Understanding the interconnection across various stages of the EDA process is crucial for optimizing AI-based EDA approaches. Minor alterations in the front-end design stages can significantly impact back-end processes and the final Power, Performance, and Area (PPA) metrics. However, the existing datasets~\cite{chowdhury2021openabc, lu2024rtllm} are typically designed for singular tasks, overlooking the essence of chip design flow as a multimodal circuit data transformation. 
% Another critical limitation is the oversight of the circuit implementation based on RTL design. For example, RTLLM~\cite{lu2024rtllm} gathers RTL specifications and manually verifies the correctness of the corresponding RTL code, while\cite{chang2024dataisall} ensures the functionality when constructing and augmenting datasets. However, these datasets overlook the correlation between RTL code and the circuit implementations (e.g. netlist, layouts) in the following stages of EDA flow. Therefore, training an RTL model with the existing datasets that both guarantees design functionality correctness and optimizes RTL code to achieve better Power, Performance and Area (PPA) remains challenging. 
Another critical limitation is the lack of focus on circuit implementation in RTL-based datasets. For instance, RTLLM~\cite{lu2024rtllm} gathers RTL specifications and verifies the functional correctness of RTL code, while\cite{chang2024dataisall} ensures functionality when constructing and augmenting datasets. RTL-Repo\cite{allam2024RTL-Repo} collects 1000+ RTL design projects and make each sample in the dataset contains the context of the entire repository to train LLM for RTL code understanding. However, these works do not consider or include circuit-level information, such as netlists and layouts, which are crucial in later stages of the EDA flow. As a result, these datasets overlook the correlation between RTL code and its corresponding circuit implementations. Consequently, training RTL models with the existing datasets that both guarantee functional correctness and optimize Power, Performance, and Area (PPA) remains a significant challenge.

% Our dataset: Collect based on the real-world chip types, Collect repo-level, synthesis flow
To overcome these limitations, we present a multimodal and comprehensive
repository-level circuit dataset for deep learning in EDA. Our dataset is meticulously curated according to the scope of the real-world chip and collect more than 4,000 circuit design projects from various data sources. Unlike previous datasets that provide verilog files only\cite{chang2024dataisall,thakur2023benchmarking}, each data point in our dataset is structured as a complete repository and also splitted into files, modules and blocks for various scenarios. This multi-tiered structure enables training models at different scales, making it suitable for a variety of LLMs and models. Moreover, the repository-level data allows logic synthesis and physical design flows to convert RTL code into various circuit modalities, including Control/Data Flow Graph (CDFG) of RTL code, And-Invertor Graph (AIG), post-mapping netlist, floorplaning and layout. Therefore, we can build the interactions between RTL designs and circuit implementations. 

% We label these circuits for unimodal RTL tasks and cross-modal PPA prediction. 
Furthermore, we meticulously label the circuits for unimodal RTL tasks and cross-modal PPA prediction to enhance accessibility of our dataset. We propose a Chain of Thought (CoT)\cite{wei2022chainofthought} detailed annotation method to generate descriptions and comments for each of the four levels, namely, repo-level, file-level, module-level and block-level.  By using GPT-4\cite{achiam2023gpt} and Claude\cite{claude}, we leverage annotations from higher levels to assist in annotating lower levels. Additionally, we generate question-answer pairs to help describe the functionality and key features of each code segment, enabling better training data for LLMs. Moreover, we provide the corresponding logic synthesis results for the cross-stage PPA prediction on RTL code. 

% With this dataset, we can construct a variety of pre-training tasks, evaluation tasks, and benchmarks, enabling the study of LLM performance on RTL code understanding and completion. Our experiments include tasks that assess LLMs' abilities in RTL code comprehension, generation and completion, we trained various models, including CodeLLama\cite{codellama}, CodeT5+\cite{codet5}, CodeGen\cite{codegen}, and DeepSeek\cite{liu2024deepseekv2,zhu2024deepseekcoderv2}, across different scales ranging from 220M to 16B on our dataset. The results demonstrate two key findings:
% 1)
% Every large model fine-tuned on our dataset significantly outperforms its original, non-fine-tuned counterpart across all metrics, highlighting the effectiveness of our data.
% 2) 
% LLMs of different scales, such as the 220M CodeT5, 7B and 16B models, show substantial improvements after fine-tuning, reflecting the adaptability and generalization capabilities of our dataset across varying model sizes. Moreover, to prove the contribution of our dataset on the EDA tasks, we utilize learning-based PPA prediction models, the results show that accurate predicting PPA on early stage still poses a question, providing valuable insights for future research in RTL and hardware design automation. 
With this dataset, we create a range of pre-training tasks, evaluation tasks, and benchmarks. One experiment focuses on RTL code understanding and generation with LLMs. We trained models including CodeLLama\cite{codellama}, CodeT5+\cite{codet5}, CodeGen\cite{codegen}, and DeepSeek\cite{liu2024deepseekv2, zhu2024deepseekcoderv2} at scales ranging from 220M to 16B parameters on our dataset. Another experiment evaluates PPA using learning-based prediction models\cite{xu2022sns,sengupta2022good,fang2023masterrtl}, trained to assess area, power, and delay. The results reveal three key findings:
1) Fine-tuning LLMs on our dataset leads to significant performance improvements across all metrics, demonstrating the effectiveness of our dataset.
2) LLMs of different scales, such as CodeT5 220M, 7B, and 16B, show substantial gains after fine-tuning, indicating the adaptability of our dataset across varying model sizes.
3) PPA prediction results highlight the ongoing challenge of accurate early-stage PPA forecasting, providing valuable insights for future RTL and hardware design automation research.
And the pipeline and framework are illustrated in Figure \ref{fig:overview}.

The main contributions of our work are summarized as follows:
\begin{itemize}
  % real-world, ->multimodal (all tasks in EDA) -> annotations
    \item We propose a holistic dataset including over 4,000 repository-level RTL projects, covering chip-designs, IP-designs, module-designs designs, and incorporating a diverse range of functional and algorithmic keywords.

    \item Our dataset is organized into four levels, namely, repository, file, module, and block levels, allowing models to be trained at different scales and enabling broader applications in EDA tasks such as synthesis, netlist generation, PPA analysis, and layout design.

    \item We propose a Chain of Thought (CoT) annotation method using GPT-4 and Claude to generate detailed comments, descriptions, and question-answer pairs, improving training data for LLMs on the tasks of RTL code understanding and generation.
    
    \item We create pre-training and evaluation benchmarks for LLMs on RTL tasks such as code understanding, completion, and neural network-based PPA prediction, demonstrating the effectiveness, adaptability and generalization capabilities of our dataset for both RTL code comprehension and EDA tasks.
\end{itemize}


