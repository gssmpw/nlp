\documentclass[9pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts
\DeclareUnicodeCharacter{2217}{*}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{balance}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% AI2.3    Infrastructures for AI (including datasets, implementations)
% 

    
\begin{document}


\title{DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code Understanding, Generation, and PPA Analysis}

% \author{
% {Zeju Li}$^{1\dagger}$ \quad Changran Xu$^{1\dagger}$ \quad Zhengyuan Shi$^{1\dagger}$ \quad Zedong Peng$^2$ \quad Yi Liu$^1$ \quad Yunhao Zhou$^2$ \\ 
% {Lingfeng Zhou}$^3$ \quad {Chengyu Ma}$^4$ \quad {Jianyuan Zhong}$^1$ \quad {Xi Wang}$^5$ \quad {Jieru Zhao}$^2$ \quad {Zhufei Chu}$^4$ \\ 
% {Xiaoyan Yang}$^3$ \quad {Qiang Xu}$^{1*}$\thanks{$\dagger$ Equal Contribution, *Corresponding author, qxu@cse.cuhk.edu.hk.} \\[6pt]
% $^1$The Chinese University of Hong Kong \quad $^2$Shanghai Jiao Tong University \\[3pt]
% $^3$Hangzhou Dianzi University \quad $^4$Ningbo University \quad $^5$Southeast University
% }


\author{
	\IEEEauthorblockN{
        Zeju Li $^{1,6\dagger}$, 
        Changran Xu $^{1,6\dagger}$, 
        Zhengyuan Shi $^{1,6\dagger}$, 
        Zedong Peng $^{2,6}$, 
        Yi Liu $^{1,6}$, 
        Yunhao Zhou $^{1,6}$, 
        Lingfeng Zhou $^{3,6}$, \\
        Chengyu Ma $^{4,6}$, 
        Jianyuan Zhong $^{1,6}$, 
        Xi Wang $^{5,6}$, 
        Jieru Zhao $^{2}$, 
        Zhufei Chu $^{4}$, 
        Xiaoyan Yang $^{3}$, 
        Qiang Xu $^{1,6}$ \thanks{$\dagger$ Equal Contribution} \thanks{Corresponding author: Qiang Xu (qxu@cse.cuhk.edu.hk)}} 

\IEEEauthorblockA{$^1$\textit{Department of Computer Science and Engineering}, \textit{The Chinese University of Hong Kong}, Sha Tin, Hong Kong S.A.R.\\}
\IEEEauthorblockA{$^2$ \textit{Department of Computer Science and Engineering}, Shanghai, China \\}
\IEEEauthorblockA{$^3$\textit{School of Computer Science}, \textit{Hangzhou Dianzi University}, Hangzhou, China \\}
\IEEEauthorblockA{$^4$\textit{Faculty of Electrical Engineering and Computer Science}, \textit{Ningbo University}, Ningbo, China \\}
\IEEEauthorblockA{$^5$ \textit{School of Integrated Circuit, Southeast University}, Nanjing, China \\}
\IEEEauthorblockA{$^6$\textit{National Center of Technology Innovation for EDA}, Nanjing, China \\}
} 

\maketitle

\begin{abstract}
This paper introduces DeepCircuitX, a comprehensive repository-level dataset designed to advance RTL (Register Transfer Level) code understanding, generation, and power-performance-area (PPA) analysis. Unlike existing datasets that are limited to either file-level RTL code or physical layout data, DeepCircuitX provides a holistic, multilevel resource that spans repository, file, module, and block-level RTL code. This structure enables more nuanced training and evaluation of large language models (LLMs) for RTL-specific tasks. DeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering detailed descriptions of functionality and structure at multiple levels. These annotations enhance its utility for a wide range of tasks, including RTL code understanding, generation, and completion. Additionally, the dataset includes synthesized netlists and PPA metrics, facilitating early-stage design exploration and enabling accurate PPA prediction directly from RTL code. We demonstrate the dataset's effectiveness on various LLMs finetuned with our dataset and confirm the quality with human evaluations. Our results highlight DeepCircuitX as a critical resource for advancing RTL-focused machine learning applications in hardware design automation. Our data is available at \url{https://zeju.gitbook.io/lcm-team}.

\end{abstract}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/overview.pdf}
    \caption{Pipeline overview of the proposed framework, illustrating the key stages: data collection from GitHub using keywords, data annotation via chain-of-thought (COT), circuit transformation, and evaluation, including RTL code tasks for LLM and PPA prediction.}
    \label{fig:overview}
\end{figure*}

\input{01_introduction}
\input{02_related}
\input{03_method}
\input{04_experiment}
\input{05_conclusion}

\balance

% \section*{Acknowledgments}

% \newpage
\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}