\section{Benchmark and Experiments} \label{Sec:TradApp}
% \section{Benchmark and Experiments}
\subsection{overview}
To evaluate our multi-modal dataset DeepCircuitX and establish benchmarks, we design experiments of human evaluation in Section \ref{sec:humaneval}, RTL code tasks in Section \ref{sec:understanding} and Section \ref{sec:completion}, and PPA prediction experiments in Section \ref{sec:PPA}.


Specially, in Section \ref{sec:understanding} and Section \ref{sec:completion}, we establish benchmarks for LLMs tailored to RTL tasks, utilizing open-source models such as CodeLlama~\cite{codellama}, CodeT5+~\cite{codet5}, deepseek-coder~\cite{guo2024deepseekcoder} and CodeGen~\cite{codegen}. Our annotated data is employed to fine-tune these models for RTL code understanding, completion, and generation tasks. Here is an concise iintroduction of the based-LLMs we select:
\begin{itemize}
    \item CodeLlama is fine-tuned Llama on a diverse corpus of code from various programming languages, enabling it to understand syntax, semantics, and the context of code snippets. 
    \item CodeT5+ is an enhanced version of the original CodeT5 model, specifically optimized for code understanding and generation tasks. Building on the strengths of its predecessor.
    \item CodeGen is trained on GPT-3~\cite{floridi2020gpt3} by a diverse range of programming languages and employs advanced techniques to understand and generate code efficiently. 
    \item DeepSeek-V2-lite~\cite{liu2024deepseekv2} has broader applicability on general tasks of natural language and DeepSeek-Coder-V2-lite~\cite{zhu2024deepseekcoderv2} focuses on code-related tasks, offering fine-tuned capabilities for high-quality code generation and synthesis.
\end{itemize}
















\subsection{Human evaluation}
\label{sec:humaneval}
To evaluate the effectiveness of our generating comments approach and the quality of the annotations, we conduct a series of evaluation criteria and metrics involving human reviews by independent experienced engineers. 
We employ the following metrics: Accuracy, Completeness and Understandable Clarity. The detailed grading criteria is introduced in the Table \ref{tab:grading_criteria}.

\begin{table}[htbp]
\centering
    \caption{Grading Criteria for Annotation Quality.}
\label{tab:grading_criteria}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
\textbf{Grade} & \textbf{Accuracy}         & \textbf{Completeness}   & \textbf{Understandable Clarity} \\ \hline
4              & Completely Accurate       & Fully Complete          & Clear and Concise               \\ \hline
3              & Partially Accurate        & Mostly Complete         & Relatively Clear                \\ \hline
2              & Not Quite Accurate        & Not Quite Complete      & Vague                           \\ \hline
1              & Completely Incorrect      & Incomplete              & Inaccurate         \\ \hline
\end{tabular}}
\end{table}





Engineers are tasked with analyzing both the RTL code and the accompanying annotations based on specific criteria, ensuring a thorough evaluation of the provided information. Subsequently, they assign a grade to each metric on a scale from 1 to 4, with 4 indicating the highest quality and 1 indicating the lowest. To ensure fairness, each generated text is reviewed by 5 individuals, and the average score is recorded in Table \ref{tab:human_eval}. We observe that the code annotations in DeepCircuitX exhibit high quality, with all metrics scoring above 3.5 out of 4.



\begin{table}[ht]
\centering
\caption{Human evaluation grading of repo-level annotation and module-level annotation, we use the metrics of accuracy, completeness, understandable clarity to evaluate the quality of our annoation.}
\vspace{1em}
\begin{tabular}{lcc}
\toprule
\textbf{Metrics} & \textbf{Repo Annotation} & \textbf{Module Annotaion} \\ 
\midrule
Accuracy &3.74/4		&3.5/4	\\
Completeness &3.79/4   &3.78/4\\
Understandable Clarity &3.84/4   &3.76/4\\
\bottomrule
\end{tabular}

\label{tab:human_eval}
\end{table}







\subsection{RTL Code Understanding}
\label{sec:understanding}

% \subsubsection{Overview}
% For the task of RTL code understanding, we select LLama2 (7b), CodeLLama (7b), CodeT5+ (220m), CodeT5+ (6b), CodeGen2 (1b), CodeGen2.5 (7b), DeepSeek-Coder-V2-lite (16b), DeepSeek-V2-lite (16b) as our based LLM. And we use our dataset to inst




\subsubsection{Evaluation Metrics}

\textbf{BLEU} measures n-gram overlap (1 to 4) between generated text and reference translations, incorporating a brevity penalty.
\textbf{METEOR} accounts for synonymy and stemming, calculating a harmonic mean of precision and recall. 
\textbf{ROUGE} focuses on summarization, with variants like ROUGE-N for n-gram overlap and ROUGE-L for the longest common subsequence, emphasizing recall to assess content relevance.






\subsubsection{Analysis}


The experimental results, shown in Table~\ref{tab:exper_understanding}, demonstrate the performance of various base Large Language Models (LLMs) and fine-tuned LLMs on our RTL code understanding benchmark, using evaluation metrics BLEU-4, METEOR, ROUGE-1, ROUGE-2, and ROUGE-L, which offer valuable insights into the quality of generated RTL code in terms of surface-level linguistic similarity.




Initially, the original versions of the LLMs, such as CodeLLama, CodeT5+, CodeGen2, and DeepSeek, exhibit relatively low performance across most metrics. For example, CodeLLama (original) achieves a BLEU-4 score of 0.0828, while CodeGen2.5 (original) shows moderate improvement with a BLEU-4 score of 0.1060. Among the original models, DeepSeek-Coder-V2-lite (original) stands out, significantly outperforming others with a BLEU-4 score of 2.2387 and a ROUGE-1 score of 30.3208, indicating its strong baseline performance even before fine-tuning.

After fine-tuning on our dataset, every large model demonstrates significantly better performance across BLEU-4, METEOR, ROUGE-1, ROUGE-2, and ROUGE-L metrics compared to their original, non-fine-tuned counterparts. This highlights the effectiveness of our dataset. Moreover, models of various sizes, such as the 220M CodeT5, as well as larger 7B and 16B models, all show substantial improvements after fine-tuning. This indicates that our dataset is well-suited for models of different scales, providing strong adaptability and generalization.
% However, fine-tuning the models on our training dataset leads to substantial performance improvements across the board. CodeLLama (7b), for instance, sees a dramatic rise in BLEU-4 and similar gains in ROUGE-1 and METEOR scores. CodeT5+ (220m-bimodal) demonstrates one of the most notable improvements, achieving a high BLEU-4 score and ROUGE-1 score, far surpassing its original version. Similarly, CodeGen2.5 (7b) shows significant enhancements in BLEU-4, METEOR, and all ROUGE metrics, highlighting the effectiveness of fine-tuning.

% In the larger models, DeepSeek-V2-lite (16b) and DeepSeek-Coder-V2-lite (16b) also exhibit strong performance after fine-tuning, along with high ROUGE scores across all categories. These results suggest that LLMs benefit considerably from fine-tuning, particularly present the effectiveness of our RTL code understanding dataset.


Overall, the fine-tuned LLMs significantly outperform their original counterparts, indicating the importance of fine-tuning on domain-specific datasets like ours. 




\begin{table}[ht]
\centering
\caption{The results of our base-LLMs and fine-tuned LLMs by our training dataset on our RTL code understanding benchmark.}
\vspace{1em}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Based LLM} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} \\ 
\midrule
CodeLLama (original) & 0.0828 & 5.9414& 11.0046& 0.3139 & 0.2581       \\ 
CodeT5+ 220m-bimoal (original) & 0.1410    & 4.3277    & 10.9925 & 0.5076 & 9.3043      \\ 

% CodeT5+ 6b (original)        \\ 
CodeGen2 (original)  & 0.1082 &3.7890  &8.0311 &0.1173    & 7.2161 \\ 
CodeGen2.5 (original)  & 0.1060 & 4.8271 & 9.4001 & 0.3698 & 8.5856         \\ 
DeepSeek-Coder-V2-lite (original)&2.2387 &24.3311 &30.3208 &6.3163 & 27.4282        \\ 
DeepSeek-V2-lite (original) &0.2311 &3.3951 &7.3720 &0.2646 &6.3360     \\ 
\midrule
CodeLLama (7b) & 0.8619 & 18.2621 & 25.0461 & 6.4493 & 22.8182      \\ 
CodeT5+ (220m-bimodal) & 4.9067    & 23.5043   & 34.8671 & 9.9023 & 32.1642      \\ 
% CodeT5+ (6b)        \\ 
CodeGen2 (1b)  & 7.7605 &27.8049 & 37.2150 & 13.1385 & 34.0647        \\ 
CodeGen2.5 (7b) & 13.6858 &34.7494 &43.5244 &18.5249 &40.2223      \\ 
DeepSeek-Coder-V2-lite (16b) &11.9180 &33.5011 &41.8527 &17.2014 &38.0473      \\ 
DeepSeek-V2-lite (16b) & \textbf{13.6972} & \textbf{39.5962} & \textbf{43.3732} & \textbf{19.0589} & \textbf{39.5962}      \\ 
\bottomrule
\end{tabular}}

\label{tab:exper_understanding}

\end{table}






\subsection{RTL Code Completion and Generation}
\label{sec:completion}


% \subsubsection{Overview}
% For the task of RTL code completion and generation, we select LLama2 (7b), CodeLLama (7b), CodeT5+ (220m), CodeT5+ (6b), CodeGen2 (1b), CodeGen2.5 (7b), DeepSeek-Coder-V2-lite (16b), DeepSeek-V2-lite (16b) as our based LLM. And we use our dataset to inst




\subsubsection{Evaluation Metrics}
In the realm of RTL code completion and generation, the evaluation of model performance is critical to advancing intelligent programming tools. The Pass@k metric serves as a pivotal measure in this domain, quantifying the accuracy of code generation models by assessing their ability to produce valid solutions within the top-k predictions. Specifically, Pass@k evaluates whether the correct code snippet appears among the model's top k outputs, thereby providing insights into both the effectiveness and reliability of the model's predictions.





\subsubsection{Analysis}


Table \ref{tab:exper_gen_com} compares the performance of both original and fine-tuned LLMs on RTL code completion and generation tasks, focusing on Pass@1 and Pass@5 on two evaluation benchmarks, RTLLM\cite{lu2024rtllm} and VerilogEval\cite{liu2023verilogeval}. 
We choose the baseline LLMs as original versions of CodeLlama, CodeT5+, CodeGen2, and CodeGen2.5 exhibit negligible performance, with most Pass@K scores at 0\% or near 0\%.

Notably, every model fine-tuned with our dataset significantly outperforms its original, non-fine-tuned counterpart, demonstrating the effectiveness of our data. Additionally, for models of different scales, such as the 220M CodeT5 and 7B models, the results after fine-tuning show substantial improvements. This highlights the adaptability and generalization capability of our dataset across various model sizes. Moreover, we include CodeV (QW-7B) \cite{zhao2024codev} as an additional baseline, which achieves 14.80\% Pass@1 and Pass@5 on RTLLM, and 4.5\% on VerilogEval. Although CodeV has undergone prior fine-tuning for general-purpose code generation, its performance remains lower than our fine-tuned CodeGen2.5 (7B). 

These findings highlight the effectiveness of our dataset in enhancing LLMs’ capability to generate syntactically and functionally accurate RTL code. Future work could explore further model scaling and more advanced training techniques to push performance even higher.




\begin{table}[htbp]
\centering
\caption{Pass@K results for RTL code completion and generation across various LLMs. The result with ∗ are evaluated using our custom implementation to align with our experimental settings.}
\vspace{1em}
\setlength\tabcolsep{1.0pt}
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{Based LLM} & \multicolumn{2}{c|}{Pass@1}   & \multicolumn{2}{c}{Pass@5} \\
                           & RTLLM\cite{lu2024rtllm}       & VerilogEval\cite{liu2023verilogeval}    & RTLLM       & VerilogEval    \\ \midrule
CodeLLama (original)              & 0           & 0.64\%        & 0           & 0 \\
CodeT5+ (original)   & 0          & 0             & 0          & 0 \\
CodeGen2 (original)               & 0           & 0             & 0           & 0.64\%  \\
CodeGen2.5 (original)             & 17.24\%     & 23.08\%       & 17.24\%           & 24.36\% \\
% DeepSeek-Coder-V2-lite (original) & 44.83\%     & 50\%             & 44.83\%           & -             & -           & -            \\
% DeepSeek-V2-lite (original)       & 17.24\%     & 26.28\%        & 17.24\%           & -             & -           & -            \\ 
\midrule
CodeLLama (7b)                    & 6.90\%      & 1.92\%        & 6.90\%      & 6.41\% \\
CodeT5+ (220m-bimodal)            & 3.45\%      & 4.49\%        & 3.45\%      & 4.49\% \\
CodeGen2 (1b)                     & 13.79\%     & 10.90\%             & 13.79\%          & 10.90\%     \\
CodeV (QW 7b) \cite{zhao2024codev}                   & 14.80\%*      & 4.5\%*      & 14.80\%*      & 4.5\%* \\
CodeGen2.5 (7b)                   & \textbf{24.14}\%     & \textbf{24.36}\%       & \textbf{24.14}\%           & \textbf{25\%}\\
% DeepSeek-Coder-V2-lite (16b)      & 24.14\%     & 37.04\%             & 34.48\%           & -             & -           & -            \\
% DeepSeek-V2-lite (16b)            & 13.79\%     &16.03\%             & 13.79\%           & -             & -           & -            \\ 
\bottomrule
\end{tabular}

\label{tab:exper_gen_com}
\end{table}

% \begin{table}[ht]
% \centering
% \begin{tabular}{lccc}
% \hline
% \textbf{Based LLM} & \textbf{Pass@1} & \textbf{Pass@5} & \textbf{Pass@10} \\ 
% CodeT5+ (original)      \\ 
% \hline
% LLama2 (7b)       \\
% CodeLLama (7b)      \\ 
% CodeT5+ (220m)     \\ 
% CodeT5+ (6b)       \\ 
% CodeGen2 (1b)      \\ 
% CodeGen2.5 (7b)        \\ 
% DeepSeek-Coder-V2-lite (16b)        \\ 
% DeepSeek-V2-lite (16b)       \\ 
% \hline
% \end{tabular}
% \caption{Pass@K}
% \label{tab:dataset_repo_levels}
% \end{table}



\subsection{PPA Prediction}
\label{sec:PPA}
\subsubsection{Overview}
Optimizing Power, Performance, and Area (PPA) stands out as a primary objective in the circuit design process. Estimating PPA metrics at an early stage not only boosts design efficiency but also empowers a more agile response to evolving design requirements and constraints. Unlike previous RTL datasets that solely gather Verilog files, our datasets encompass entire repositories, allowing us to obtain post-mapping netlists and logic synthesis reports by EDA tools. Consequently, we can parse the PPA metrics from the logic synthesis reports and predict them for the corresponding RTL designs at an early stage. In this subsection, we formulate the PPA prediction task within our dataset and assess the effectiveness of current learning-based prediction models~\cite{xu2022sns, sengupta2022good, fang2023masterrtl}. 

\subsubsection{Evaluation Metrics}
We regard the circuit charateristics of the post-mapping netlist produced by the Design Compiler tool with default settings as the ground truth for PPA metrics. Specifically, we define the dynamic power under random workloads as the \textbf{Power} metric, the maximum path delay as the \textbf{Delay} metric and the total cell area as the \textbf{Area} metric. 

We use Mean Absolute Error Percentage (MAPE) and Root Relative Square Error (RRSE) to evaluate the prediction accuracy of PPA.
It should be denoted that the PPA predictor trained on the datasets generated with a certain technology library cannot be generalized to another, as the PPA metrics of netlists are strongly linked to the technology library. Therefore, we choose the netlists and logic synthesis reports with a specific library, which keeps consistency with the settings in~\cite{xu2022sns, sengupta2022good, fang2023masterrtl}. We opt for skywater 130nm library in the following experiments. 


\subsubsection{Analysis}
We collect 146 RTL designs for training and 10 designs for testing, with these designs containing more than 10k cells after logic synthesis and closely resembling practical designs. To explore the data scalability of PPA prediction models, we sample the training dataset size to 10\%, 50\% and 100\%. It should be denoted that we exclude the model~\cite{sengupta2022good} for the delay prediction as it cannot support this task. 
According to the prediction results shown in Table~\ref{TAB:PPA}, we conclude three observations. 
Firstly, the accuracy of predictions improves as the training data volume increases. For instance, the MAPE of area prediction is 4.3201 with 10\% data and decreases to 0.3303 with 100\% data.
Secondly, PPA predictors demonstrate weaker performance in delay prediction. For example, both models exhibit unsatisfactory performance, where~\cite{xu2022sns} shows 4.7392 MAPE and~\cite{fang2023masterrtl} has 3.4769 MAPE. This suggests that estimating timing characteristics in the early stages using path features on RTL-based graphs is still difficult, as logic synthesis tools heavily optimize logic to minimize maximum path delays.
Thirdly, in comparison to the originally reported performance on simple benchmarks, these models exhibit diminished performance on designs more than 10k cells in our dataset. 
Therefore, how to accurately predict PPA of practical designs remains an opening question, necessitating further exploration in the EDA community. 

\begin{table}[]
\caption{PPA prediction results of learning-based models (the lower, the better).}
\vspace{1em}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|l|ll|ll|ll@{}} \toprule
\multirow{2}{*}{Model}            & \multicolumn{1}{c|}{\multirow{2}{*}{Data}} & \multicolumn{2}{c|}{Area} & \multicolumn{2}{c|}{Power} & \multicolumn{2}{c}{Delay} \\ 
                                  & \multicolumn{1}{c|}{}                      & MAPE        & RRSE        & MAPE         & RRSE        & MAPE         & RRSE       \\ \midrule
\multirow{3}{*}{SNS\cite{xu2022sns}}              & 10\%                                       & 1.9379      & 1.4913      & 1.9045       & 1.1362      & 49.3638      & 6.6901     \\
                                  & 50\%                                       & 0.7811      & 1.9041      & 0.8425       & 1.1759      & 47.7314      & 2.8816     \\
                                  & 100\%                                      & 0.6564      & 1.7197      & 0.7549       & 1.1740      & 4.7392       & \textbf{2.3647}     \\ \midrule
\multirow{3}{*}{\cite{sengupta2022good}} & 10\%                                       & 1.9066      & 1.0802      & 10.2783      & 14.7510     & -            & -          \\
                                  & 50\%                                       & 0.7954      & 0.6133      & 0.7478       & \textbf{0.3539}      & -            & -          \\
                                  & 100\%                                      & 0.5996      & \textbf{0.5074}      & 0.7343       & 0.6558      & -            & -          \\ \midrule
\multirow{3}{*}{MasterRTL\cite{fang2023masterrtl}}        & 10\%                                       & 1.3405      & 1.1197      & 2.3253       & 29.7767     & 80.7779      & 8.5936     \\
                                  & 50\%                                       & 0.5640      & 0.9982      & 1.7237       & 29.8749     & 24.7716      & 6.8725     \\
                                  & 100\%                                      & \textbf{0.3303}      & 0.7605      & \textbf{0.6501}       & 2.2541      & \textbf{3.4769}       & 4.2863    \\ \bottomrule
\end{tabular}}

\label{TAB:PPA}
\end{table}


