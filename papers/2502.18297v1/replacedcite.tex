\section{Related Work}
\label{Sec:Related}
\subsection{Previous Dataset in EDA}
AI-based methodologies excel in addressing classification, prediction, and optimization tasks, making them well-suited for chip design. Over the past decade, the integration of AI into Electronic Design Automation (EDA) has emerged as an attractive direction in the fields of chip design and semiconductor industry, which is well surveyed in____. 

The cornerstone of training AI models lies in the quality of the dataset utilized. In the domain of EDA, existing datasets can be broadly categorized into two classes. Firstly, there are unimodal datasets or benchmarks such as ISCAS'89____, ITC'99____, IWLS'05____ and EPFL____ benchmarks, which offer open-source circuit netlists primarily for front-end applications like logic synthesis and design for test. ____ presents a dataset of physical designs generated from the IWLS'05 benchmark circuit suite, utilizing the open-source 130nm Process Design Kit (PDK) by Skywater and the OpenROAD toolkit ____. 
% Additionally,____ collects Verilog code sourced from the Internet and ____ assembles a dataset in a natural language format containing question-answer pairs. 
Despite these efforts, these datasets still lack in providing comprehensive insights into cross-stage EDA tasks, such as optimizing circuit designs at the front-end for improved PPA metrics during the back-end stage.

Secondly, other multimodal circuit datasets are generated using EDA tools. For instance, CircuitNet____ creates a dataset through logic synthesis and physical design, where the circuit designs are synthesized into gate-level netlists and transformed into layouts using commercial EDA tools. ____ expand this work by providing data for million-gate designs such as CPUs, GPUs, and AI chips, and using the 14nm FinFET technology node to capture the increased complexity of manufacturing and modeling. Nonetheless, the existing datasets encompass a limited range of circuit designs, thereby constraining the generalizability of subsequent model training efforts. 

% ____ explore the wide-ranging applications of LLMs in code analysis, including bug detection and fixing, code summarization, and security checking. They also introduce the use of LLMs in logic synthesis, physical design, and the generation of HDL and scripts. ____ release a prompt-script dataset to train LLMs for script generation in physical design tasks using OpenROAD and propose a question-answer dataset to train LLMs on answering questions related to physical design methods with OpenROAD.  ____ creates a dataset through logic synthesis and physical design, where RISC-V designs are synthesized into gate-level netlists and transformed into layouts using Synopsys and Cadence tools, with diverse settings to improve dataset variability. ____ expand this work by providing data for million-gate designs such as CPUs, GPUs, and AI chips, and using the 14nm FinFET technology node to capture the increased complexity of manufacturing and modeling. This dataset supports multi-model prediction tasks, including timing, routability, and IR-drop, for advanced technology nodes, covering a broader range of design objectives.


\subsection{RTL-stage Understanding, Completion, and Generation}
____ collects approximately 50,000 open-source Verilog code samples and fine-tunes five pre-trained LLMs, with model sizes ranging from 345 million to 16 billion parameters. ____ introduces a comprehensive evaluation dataset consisting of 156 problems sourced from HDLBits and developed a benchmarking framework to automatically test the functional correctness of Verilog code completions. Similarly, ____ fine-tunes existing LLMs on Verilog datasets collected from GitHub and textbooks, evaluating the functional correctness of the generated code using a custom test suite. ____ designs a data augmentation framework for training Chip Design LLMs, enabling them to generate Verilog code, EDA scripts, and coordinate EDA workflows based on natural language design descriptions. They also benchmark their approach by fine-tuning Llama 2 models with 7 billion and 13 billion parameters. Lastly, ____ presents the MG-Verilog dataset, an open-source dataset that meets essential criteria for high-quality hardware data, facilitating the effective use of LLMs in hardware design. However, these datasets focus solely on file-level RTL code, neglecting the comprehensive information contained within entire RTL project designs, modules, and code blocks. As a result, they fail to provide the multimodal data, such as netlists and PPA metrics, that can be obtained through synthesis. 
Moreover, existing studies primarily focus on RTL code generation and completion, often overlooking the critical aspects of annotations, comments, and descriptions for RTL code. Hence, there has been little progress in advancing RTL code understanding tasks, particularly for LLMs.

% Furthermore, existing works primarily concentrate on RTL code generation tasks, often \textcolor{red}{comment: cannot understand this sentence: }overlooking the essential aspects of annotation and the construction of understanding tasks for LLM.

% \subsubsection{Cross-stage PPA Prediction}