\section{Related Works}
Many researchers have provided pertinent studies concerning the selection of spreading factor and frequency for anti-jamming. In~\cite{9380308,6117758,9050457} game theory is used to model jamming, but in our scenario, the dynamics of the jammer is unable to use game theory to model. In~\cite{7996380} proposes a control mechanism for adjusting the spreading factor in accordance with the distance between communicating nodes, aiming to optimize signal integrity under varying propagation distances, the control of spreading factor is used as the compared algorithm in simulation. \textcolor[rgb]{0000,0.00,0000}{Meanwhile,~\cite{7952524} introduces a two-dimensional anti-jamming decision framework, albeit limited to a binary choice of either exiting or remaining within a jammed zone, thus constraining the scope of adaptive responses, the algorithm is DQN, which was utilized and also considered as one of the baseline algorithm in our simulation for comparative analysis. In~\cite{10226268}, a reinforcement learning method is used for hopping pattern selection instead of frequency selection.}

In~\cite{8254362}, an anti-jamming underwater transmission framework that applies reinforcement learning to control the transmit power and uses the transducer mobility to address jamming in underwater acoustic networks. In \cite{8254362}, anti-jamming framework is proposed for underwater acoustic networks that utilizes reinforcement learning to optimize transmit power and leverages transducer mobility to mitigate jamming. In simulation, we used two channel model to test our algorithm.

The aforementioned papers either address issues within a single dimension using one method or, when dealing with multiple dimensions, still employ the DQN algorithm without decomposing the problem. Paper~\cite{9751039} decomposes anti-jamming decisions into parallel processes, yet overlooks the incorporation of spread spectrum techniques, a critical component for anti-jamming. Moreover, our proposed methodology eliminates the $\varepsilon$-greedy mechanism, thereby expediting the convergence of our learning algorithm. In comparison with~\cite{9837014}, which employs a deep deterministic policy gradient (DDPG) framework for real-time power control and unlike the scenario delineated in the referenced paper, where k-means~\cite{electronics9081295} clustering is employed to tackle positioning problems in an alternate dimension. Considering this idea of separate processing in~\cite{electronics9081295}, in our simulation, we have designed a comparative algorithm where the DQN handles frequency agility, while adaptive control~\cite{10120634} is in charge of the spreading factor adjustment.