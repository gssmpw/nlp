\subsection{Teaching a metric to a gradient-based learner}

Now, we consider an interactive teaching framework for metric learning where a learner receives triplet feedback and makes a gradient update on the \tt{mahalanobis} matrix wrt to a loss function. This is inspired by the seminal work of \cite{Xing2002DistanceML}. They consider the setup where learner receives \tt{similar} "$x_i$ is similar to $x_j$" or \tt{dissimilar} "$x_i$ is dissimilar to $x_j$". Thus, it maintains two sets $S$ and $D$ respectively. Based on this, learner optimizes the following objective:
\begin{gather*}
    \max_{M \in S_d^{+}} \sum_{(x,x') \in D} d^2_M(x,x')\\
    s.t \sum_{(x,x') \in S} d^2_M(x,x') \le 1 \textnormal{ and } M \succ 0.
\end{gather*}
Here, we consider the setup where a teacher provides a triplet $(x_i,x_j,x_k)$ that implicitly contains the information that $x_i$ is closer to $x_j$ then $x_k$.
\begin{shaded}
\noindent \\
%Given a distribution $P$ over the sample space $\cX\subset \reals^d$.\\
\noindent\textit{at time} $t = 1,2,\ldots$
\begin{enumerate}
    \item teacher provides a triplet $(x_i^t,x_j^t,x_k^t)$ to learner
    \item learner updates the teaching set $\mathcal{T}_t := \mathcal{T}_{t-1} \cup \{(x_i^t,x_j^t,x_k^t)\}$
    \item  learner makes updates $M_t \leftarrow \textsf{Gradient}(\hat{M}_{t-1}, \mathcal{T}_{t-1}, \eta)$ 
    \item learner updates $\hat{M_t} \leftarrow \textsf{EigenDecompose}(M_t)$
    \item teacher receives $(\ell(\hat{M_t}), \dim(\hat{M_t}))$
    
\end{enumerate}
\end{shaded}

Consider the following loss function:
\begin{align*}
    \ell(M, x_i,x_j,x_k) := ||x_i - x_k||^2_{M} - ||x_i - x_j||^2_{M}
\end{align*}
Now, for $n$ triplets $\mathcal{T}_n := \curly{(x_i^t,x_j^t,x_k^t)}_{t=1}^n$, the empirical loss can be written as 
\begin{align*}
    \ell(M, \mathcal{T}_n) = \frac{1}{n} \sum_{t=1}^n \ell(M,x_i^t,x_j^t,x_k^t)
\end{align*}
Learner should maximize this loss, and thus the learning objective is 
\begin{align}
    \max_{M \in S^+_d} \ell(M, \mathcal{T}_n)
\end{align}
Now, the gradient of $\ell(M,\mathcal{T}_n)$ wrt to $M$ is 
\begin{align}
    \nabla_M \ell(M, \mathcal{T}_n) = \frac{1}{n} \sum_{i=1}^n (x_i - x_k)(x_i - x_k)^{\top} - (x_i - x_k)(x_i - x_j)^{\top} \label{eq: grad}
\end{align}
Note that \eqnref{eq: grad} is PSD. Thus, the gradient ascent iteration update preserves the necessary condition.
\begin{align*}
    M_t := M_{t-1} + \eta\cdot \frac{1}{n} \sum_{i=1}^n (x_i - x_k)(x_i - x_k)^{\top} - (x_i - x_k)(x_i - x_j)^{\top}
\end{align*}

A potential notion of the teaching dimension should be with respect to the loss function, as is the case in the standard statistical learning framework, considering some distribution over the triplet

\begin{align*}
    \expctover{(x_i,x_j,x_k) \sim P^3}{\ell(\hat{M}, x_i,x_j,x_k)} - \min_{M \in S^+_d}\expctover{(x_i,x_j,x_k) \sim P^3}{\ell(M, x_i,x_j,x_k)}\\
\end{align*}
where $\hat{M}(\mathcal{T}_n)$ is the solution to the gradient ascent iteration. Here we define the teaching dimension as the minimal number of triplets provided by the teacher so that gradient-based learner achieves a target metric $M^*$ within $\epsilon$ proximity in the frobenius norm, i.e.
\begin{align*}
    ||M^* - \hat{M}(\mathcal{T}_n)||_{F} \le \epsilon
\end{align*}

Now, let's consider a setup where the teacher can provide triplets from the support of the distribution. The teacher knows that the learner is gradient-based \tt{except} the learning rate. 
\begin{shaded}
\noindent \\
%Given a distribution $P$ over the sample space $\cX\subset \reals^d$.\\
\noindent\textit{at time} $t = 1,2,\ldots$
\begin{enumerate}
    \item teacher provides a triplet $(x_i^t,x_j^t,x_k^t)$ to learner
    \item learner updates the teaching set $\mathcal{T}_t := \mathcal{T}_{t-1} \cup \{(x_i^t,x_j^t,x_k^t)\}$
    \item  learner makes updates $M_t \leftarrow M_{t-1} + \eta\cdot \frac{1}{t} \sum_{i=1}^t (x_i - x_k)(x_i - x_k)^{\top} - (x_i - x_k)(x_i - x_j)^{\top}$
    %\item learner updates $\hat{M_t} \leftarrow \textsf{EigenDecompose}(M_t)$
    \item teacher receives $\ell(M_t, \mathcal{T}_t)$
    
\end{enumerate}
\end{shaded}


Problem: \tt{What is the minimal teaching set or teaching dimension of the protocol above?}

A general problem to consider is:
\begin{gather}
    \textnormal{Given $n$ data points $x_1,x_2,\ldots,x_n$, what is the optimal Mahalanobis metric $M$ such that}\\
    \max_M \sum_{(i,j,k)} \mathsf{d}_M(x_i,x_k) - \mathsf{d}_M(x_i,x_j)\\
    \forall\ (i,j,k)\,\, \mathsf{d}_M(x_i,x_k) \ge 1 + \mathsf{d}_M(x_i,x_j)\\
    \mathsf{tr}(M) \le 1\\
    M \succ 0
\end{gather}
 
We can rewrite the above formulation using a matrix $L$ as follows:

\begin{gather}
    \max_{R} \sum_{(i,j,k)} \mathsf{d}_M(x_i,x_k) - \mathsf{d}_M(x_i,x_j)\\
    \forall\ (i,j,k)\,\, \mathsf{d}_M(x_i,x_k) \ge 1 + \mathsf{d}_M(x_i,x_j)\\
    \mathsf{tr}(M) \le 1\\
    M = RR^T
\end{gather}

We can write down the lagrangian for this:
\begin{align*}
    \cL(R, \lambda, \gamma) = \sum_{(i,j,k)} \mathsf{d}_M(x_i,x_k) - \mathsf{d}_M(x_i,x_j) + \sum_{(i,j,k)} \lambda_{ijk} \paren{1 +  \mathsf{d}_M(x_i,x_j) - \mathsf{d}_M(x_i,x_k)} + \gamma \paren{ \sum_i M_{ii} - 1}
\end{align*}

Now, differentiating $\cL$ wrt R gives the following:
\begin{align*}
    \partial_R \cL &= \sum_{ijk} 2x_{ik}x_{ik}^TR - 2x_{ij}x_{ij}^TR + \sum_{ijk} \lambda_{ijk}(2x_{ij}x_{ij}^TR - 2x_{ik}x_{ik}^TR) + \gamma( R + R^T)\\
    &= \underbrace{\paren{\sum_{ijk} 2(1 - \lambda_{ijk})(x_{ik}x_{ik}^T - x_{ij}x_{ij}^T) + \gamma I_d}}_{A}R + \gamma R^T
\end{align*}
where $x_{mn} = (x_m - x_n)(x_m - x_n)^T$.
Now, equating $A_\gamma R + R^T = 0$ ($A_{\gamma} = A/\gamma$). Solution to equation are all the \tt{kernel} vectors of this equation:
\begin{align*}
    D = (I_n \otimes A_\gamma) + C
\end{align*}
where $\otimes$ is the Kronecker product and C is a commutation matrix of dimension $n^2 \times n^2$. Thus. note that D is a $n^2 \times n^2$ matrix.

\begin{align*}
    I_n \otimes A_\gamma = \begin{bmatrix}
  A_\gamma & 0& \ldots & 0 \\
  0 & A_\gamma& \ldots &0 \\
  \vdots & \vdots & \vdots & \vdots\\
  0& 0 & \ldots & A_\gamma
\end{bmatrix}
\end{align*}
The commutation matrix $C$ of dimension $n^2 \times n^2$ is matrix of $n^2$ blocks of dimension $(n \times n)$ blocks where $p,q$-th block has the following entries:
\begin{align*}
    C = \begin{bmatrix}
  C_{1,1} & C_{1,2}& \ldots & C_{1,n} \\
  C_{2,1} & C_{2,2}& \ldots & C_{2,n}\\
  \vdots & \vdots & \vdots & \vdots\\
  C_{n,1}& C_{n,2}& \ldots & C_{n,n}
\end{bmatrix}
\end{align*}
where for each block $C_{i,j}$ the p,q entry is as follows:
\begin{align*}
    C_{i,j}(p,q) = \begin{cases}
        1 &  i = p \textnormal{ and } j = q\\
        0 & \textnormal{o.w.}
    \end{cases}
\end{align*}
