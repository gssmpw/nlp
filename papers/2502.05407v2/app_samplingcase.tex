% \section{Learning in sampling case}
% \akash{How does sparsity play a role here? Intuitively, you want to pick points close to nodes of a unit cube. \\
% Question is what is the complexity and probabilistic guarantee you can give in this case? }

\section{Proof of \thmref{thm: samplegeneral}: General Activations Sampling}\label{app: samplegeneral}

%\begin{proof}[Proof of \thmref{thm: samplegeneral}]%\propref{prop: sampling}]
    
    %First, we provide the proof of the case when the agent receives representations sampled from a general Lebesgue distribution.
    \iffalse
    Fix a positive index $P = \frac{p(p+1)}{2}$; the agent receives $P$ representations: $\cV_n := \{v_1,v_2,\ldots, v_{P}\} \sim \cD_{\cV}$. Denote for each $i$, $V_i = v_iv_i^{\top}$.
    
    Now, consider the matrix $\mathbb{M} = \bracket{ V_1 \quad V_2 \quad \ldots V_P}$
       %\begin{align*}
    %\mathbb{M} = \begin{bmatrix}
  %\Big\lvert & \Big\lvert& \ldots & \Big\lvert \\
  %V_1 & V_2 & \vdots & V_D\\
  %\Big\lvert& \Big\lvert& \ldots & \Big\lvert
%\end{bmatrix}
%\end{align*}
where we treat each matrix $V_i$ as a column vector in $\reals^{p(p+1)/2}$. We use the following vectorization operation: for a matrix $A \in \symm$ and for all $1 \le i,j \le p$
\begin{align*}
    \text{vec}(A)_i &= A_{ii}, \\
    \text{vec}(A)_{ij} &= A_{ij} + A_{ji}.
\end{align*}
Note that the zero set of $\curlybracket{\det(\mathbb{M}) = 0}$ has measure zero in $\cV^P$, i.e.
\begin{align*}
    \cP_{\cV_n \sim \cD_{\cV}}(\det(\mathbb{M}) = 0) = 0,
\end{align*}
as $\det(\mathbb{M})$ is a (identically) non-zero polynomial over random vectors $v_1,v_2,\ldots, v_{D}$. This implies that
\begin{align}
    \cP_{\cV_n}\paren{\curlybracket{v_iv_i^{\top}} \textnormal{ is linearly independent in } \symm} = 1 \label{eq: fullprob}
\end{align}
Assume $\Sigma^* \neq 0$ be an arbitrary target feature matrix for learning with feedback in \algoref{alg: randmaha}. WLOG assume that $v := v_1 \neq 0$. Consider the following set $\cF$ of rescaled pairs
\begin{align*}
    \cF = \curlybracket{(v, \sqrt{\gamma_{i}}v_i)\,\, |\,\, \Sigma^* \idot (vv^{\top} - \gamma_{i}v_iv_i^{\top}) = 0, \sqrt{\gamma_{i}} > 0}
    %\forall i \in \curlybracket{2,\ldots, P} (v_1, \sqrt{\gamma_{1i}}v_i) \in \cF, \quad \Sigma^* \idot (v_1v_1^{\top} - \gamma_{1i}v_iv_i^{\top}) = 0, \sqrt{\gamma_{1i}} > 0.
\end{align*}
Note that $|\cF| = P - 1$. Now, we show that the elements of $\cF$ are linearly independent in $\symm$. Assuming the contrary, there exists scalars $\curlybracket{a_i}$ (not all zero) such that
\begin{align*}
    \sum_{i = 2}^P a_i (vv^{\top} - \gamma_{i}v_iv_i^{\top}) = 0 \,(\in \symm) 
    \implies \paren{\sum_{i = 2}^P a_i} vv^{\top} = \sum_{i = 2}^P a_i \gamma_{i}v_iv_i^{\top}
\end{align*}
But since $\curlybracket{v_iv_i^{\top}}$ are linearly independent $\sum_{i = 2}^P a_i$ and $\curlybracket{a_i\gamma_i}$ are necessarily zero which implies $a_i = 0$ (for all $i$) as $\gamma_{i} > 0$. This contradicts the assumption on the dependence of elements in $\cF$. 

This implies that $\cF$ induces a set of linearly independent matrices, i.e., $\curlybracket{vv^{\top} - \gamma_{i}v_iv_i^{\top}}$ in the orthogonal complement $\mathcal{O}_{\Sigma^*}$. But then $\Sigma^*$ only has $P$ many degrees of freedom. Thus, any matrix $\Sigma' \in \symm$ that satisfies the equations:
\begin{align*}
    \forall i \in \curlybracket{2,\ldots, D} (x_1, \sqrt{\gamma_{1i}}x_i) \in \cF,\quad \Sigma' \idot (v_1v_1^{\top} - \gamma_{i}v_iv_i^{\top}) = 0
\end{align*}
is at most a positive linear scaling of $\Sigma^*$. Now, using \eqnref{eq: fullprob} we know that
\begin{align*}
    \cP_{\cV_n}\paren{ \cF \textnormal{ is a valid feedback set}} = 1
\end{align*}
Since $\Sigma^*$ was picked arbitrarily the worst-case bound on feedback complexity is upper bounded by $ P - 1$ for $1$-accuracy.

Now, we show the proof for the lower bound.

In the proof of the lower bound of \thmref{thm: constructgeneral}, we prove \lemref{lem: inclusion} which stipulates a necessary property of a feedback set $\cF$ in \algoref{alg: main} with pairs (we assume equality constraint). We state it again for better clarity: given any target matrix $\Sigma^* \in \symm$, if $\Sigma \in \mathcal{O}_{\Sigma^*}$ such that $\text{span}\inner{\col{\Sigma}} \subset \text{span} \inner{Z_{\bracket{r}}}$ then $\Sigma \in span \inner{\cF}$ where $Z_{[r]}$ ($r \le d$) is defined as the set of eigenvectors in the eigendecompostion of $\Sigma^*$ (see \eqnref{eq: target}). 
    %If the rank of $M^*$, then the lower bound follows from the deterministic setting. If the rank is strictly less than $d$, then using \lemref{lem: uniquevec} 
    
This observation implies there exists a matrix $\Sigma' \in \symm$ that needs to be spanned by any feedback set $\cF(\cV_n, {\Sigma^*})$ in \algoref{alg: randmaha} for oblivious learning. We argue that it requires $\frac{p(p+1)}{2}$ sized random representations $\cV_n \sim \cD_{\cV}$, to construct a feedback set that spans any such matrix $\Sigma'$.

Fixing some postive index $\ell > 0$, the agent receives $\ell$ representations: $v_1,v_2,\ldots, v_{\ell} \sim \cD_{\cV}$. Denote for each $i$, $V_i = v_iv_i^{\top}$. Now, consider the matrix $\mathbb{M} = \bracket{ \Sigma' \quad V_1 \quad \ldots V_\ell}$
%        \begin{align*}
%    \mathbb{M} = \begin{bmatrix}
%  \Big\lvert & \Big\lvert& \ldots & \Big\lvert \\
%  \Sigma' & X_1 & \vdots & X_\ell\\
%  \Big\lvert& \Big\lvert& \ldots & \Big\lvert
%\end{bmatrix}
%\end{align*}
where we treat each matrix $V_i$ and $\Sigma'$ as column vectors in $\reals^{p^2}$. Now, consider the polynomial equation $\det(\mathbb{M}) = 0$. Since every entry of $\mathbb{M}$ is semantically different, the determinant $\det(\mathbb{M})$ is a non-zero polynomial. Note that there are $\frac{p(p+1)}{2}$ many degrees of freedom for the rows. Thus, it is clear that the zero set $\curlybracket{\det(\mathbb{M}) = 0}$ has Lebesgue measure zero if $\ell < \frac{p(p+1)}{2}$, i.e. $\mathbb{M}$ requires at least $\frac{p(p+1)}{2}$ columns for $\det(\mathbb{M})$ to be identically zero. But this implies that set $\curlybracket{v_iv_i^{\top}}_{i=1}^\ell$ can't span $\Sigma'$ (almost surely) if $\ell \le \frac{p(p+1)}{2} - 1$.
Hence, (almost surely) the agent can't devise a feedback set for oblivious learning in \algoref{alg: randmaha}.
%must have at least $\frac{d(d+1)}{2} - 1$ pairs. 
In other words, if $\ell \le \frac{p(p+1)}{2} - 1$,
\begin{align*}
    \cP_{\cV_\ell}\paren{ \textnormal{agent devises} \textnormal{ a feedback set } \cF \textnormal{ up to feature equivalence}} = 0
\end{align*}
But this shows that one cannot achieve non-zero accuracy over any random set of representations $\cV_{\ell}$ if $\ell = \Omega(\frac{p(p+1)}{2})$ which proves the claim of the lower bound.
\fi
We aim to establish both upper and lower bounds on the feedback complexity for oblivious learning in Algorithm~\ref{alg: randmaha}. The proof revolves around the linear independence of certain symmetric matrices derived from random representations and the dimensionality required to span a target feature matrix.

Let us define a positive index \( P = \frac{p(p+1)}{2} \). The agent receives \( P \) representations:
\[
\mathcal{V}_n := \{v_1, v_2, \ldots, v_P\} \sim \mathcal{D}_{\mathcal{V}}.
\]
For each \( i \), we define the symmetric matrix \( V_i = v_i v_i^{\top} \).

Consider the matrix \( \mathbb{M} \) formed by concatenating the vectorized \( V_i \):
\[
\mathbb{M} = \begin{bmatrix} \text{vec}(V_1) & \text{vec}(V_2) & \cdots & \text{vec}(V_P) \end{bmatrix},
\]
where each \( \text{vec}(V_i) \) is treated as a column vector in \( \mathbb{R}^{P} \). The vectorization operation for a symmetric matrix \( A \in \symm \) is defined as:
\[
\text{vec}(A)_k =
\begin{cases}
A_{ii} & \text{if } k \text{ corresponds to } (i,i), \\
A_{ij} + A_{ji} & \text{if } k \text{ corresponds to } (i,j),\ i < j.
\end{cases}
\]

The determinant \( \det(\mathbb{M}) \) is a non-zero polynomial in the entries of \( v_1, v_2, \ldots, v_P \). Since the vectors \( v_i \) are drawn from a continuous distribution \( \mathcal{D}_{\mathcal{V}} \), using Sard's theorem the probability that \( \det(\mathbb{M}) = 0 \) is zero, i.e.,
\[
\mathcal{P}_{\mathcal{V}_n}(\det(\mathbb{M}) = 0) = 0.
\]
This implies that, with probability 1, the set \( \{V_1, V_2, \ldots, V_P\} \) is linearly independent in \( \symm \):
\begin{align}
\mathcal{P}_{\mathcal{V}_n}\left(\{v_i v_i^{\top}\} \text{ is linearly independent in } \symm\right) = 1. \label{eq: independence}
\end{align}


Next, let \( \Sigma^* \neq 0 \) be an arbitrary target feature matrix for learning with feedback in Algorithm~\ref{alg: randmaha}. Without loss of generality, assume \( v := v_1 \neq 0 \). Define the set \( \mathcal{F} \) of rescaled pairs as:
\[
\mathcal{F} = \left\{ \left(v, \sqrt{\gamma_i} v_i\right) \,\bigg|\, \Sigma^* \cdot \left(v v^{\top} - \gamma_i v_i v_i^{\top}\right) = 0, \ \sqrt{\gamma_i} > 0 \right\},
\]
noting that \( |\mathcal{F}| = P - 1 \).

Assume, for contradiction, that the elements of \( \mathcal{F} \) are linearly dependent in \( \symm \). Then, there exist scalars \( \{a_i\} \) (not all zero) such that:
\[
\sum_{i=2}^P a_i \left(v v^{\top} - \gamma_i v_i v_i^{\top}\right) = 0 \quad \Rightarrow \quad \left(\sum_{i=2}^P a_i\right) v v^{\top} = \sum_{i=2}^P a_i \gamma_i v_i v_i^{\top}.
\]
However, since \( \{v_i v_i^{\top}\} \) are linearly independent with probability 1, it must be that:
\[
\sum_{i=2}^P a_i = 0 \quad \text{and} \quad a_i \gamma_i = 0 \quad \forall i.
\]
Given that \( \gamma_i > 0 \), this implies \( a_i = 0 \) for all \( i \), contradicting the assumption of linear dependence. Therefore, matrices induced by \( \mathcal{F} \) are linearly independent.

This implies that $\cF$ induces a set of linearly independent matrices, i.e., $\curlybracket{vv^{\top} - \gamma_{i}v_iv_i^{\top}}$ in the orthogonal complement $\mathcal{O}_{\Sigma^*}$, and since \( \Sigma^* \) has at most \( P \) degrees of freedom, any matrix \( \Sigma' \in \symm \) satisfying:
\[
\Sigma' \cdot \left(v v^{\top} - \gamma_i v_i v_i^{\top}\right) = 0 \quad \forall i
\]
must be a positive scalar multiple of \( \Sigma^* \).

Thus, using \eqnref{eq: independence},  with probability 1, the feedback set \( \mathcal{F} \) is valid:
\[
\mathcal{P}_{\mathcal{V}_n}\left(\mathcal{F} \text{ is a valid feedback set}\right) = 1.
\]
Since \( \Sigma^* \) was arbitrary, the worst-case feedback complexity is almost surely upper bounded by \( P - 1 \) for achieving feature equivalence.

For the lower bound, consider the proof of the lower bound in Theorem~\ref{thm: constructgeneral}, specifically Lemma~\ref{lem: inclusion}, which asserts that for any feedback set \( \mathcal{F} \) in Algorithm~\ref{alg: main}, given any target matrix $\Sigma^* \in \symm$, if $\Sigma \in \mathcal{O}_{\Sigma^*}$ such that $\text{span}\inner{\col{\Sigma}} \subset \text{span} \inner{Z_{\bracket{r}}}$ then $\Sigma \in \text{span} \inner{\cF}$ where $Z_{[r]}$ ($r \le d$) is defined as the set of eigenvectors in the eigendecompostion of $\Sigma^*$ (see \eqnref{eq: target}).%if \( \Sigma \in \mathcal{O}_{\Sigma^*} \) and \( \text{span}\{\Sigma\} \subset \text{span}\{Z_{[r]}\} \), then \( \Sigma \in \text{span}\{\mathcal{F}\} \), where \( Z_{[r]} \) are the eigenvectors from the eigendecomposition of \( \Sigma^* \).

This implies that any feedback set \( \mathcal{F}(\mathcal{V}_n, \Sigma^*) \) must span certain matrices \( \Sigma' \in \symm \). Suppose the agent receives \( \ell \) representations \( v_1, v_2, \ldots, v_{\ell} \sim \mathcal{D}_{\mathcal{V}} \) and constructs:
\[
\mathbb{M} = \begin{bmatrix} \text{vec}(\Sigma') & \text{vec}(V_1) & \cdots & \text{vec}(V_\ell) \end{bmatrix}.
\]
Now, consider the polynomial equation $\det(\mathbb{M}) = 0$. Since every entry of $\mathbb{M}$ is semantically different, the determinant $\det(\mathbb{M})$ is a non-zero polynomial. Note that there are $\frac{p(p+1)}{2}$ many degrees of freedom for the rows. Thus, it is clear that the zero set $\curlybracket{\det(\mathbb{M}) = 0}$ has Lebesgue measure zero if $\ell < \frac{p(p+1)}{2}$, i.e. $\mathbb{M}$ requires at least $\frac{p(p+1)}{2}$ columns for $\det(\mathbb{M})$ to be identically zero. But this implies that set $\curlybracket{v_iv_i^{\top}}_{i=1}^\ell$ can't span $\Sigma'$ (almost surely) if $\ell \le \frac{p(p+1)}{2} - 1$.
Hence, (almost surely) the agent can't devise a feedback set for oblivious learning in \algoref{alg: randmaha}.
%must have at least $\frac{d(d+1)}{2} - 1$ pairs. 
In other words, if $\ell \le \frac{p(p+1)}{2} - 1$,
\begin{align*}
    \cP_{\cV_\ell}\paren{ \textnormal{agent devises} \textnormal{ a feedback set } \cF \textnormal{ up to feature equivalence}} = 0
\end{align*}
%The determinant \( \det(\mathbb{M}) \) is a non-zero polynomial. Since each entry in \( \mathbb{M} \) is distinct, \( \det(\mathbb{M}) \) is not identically zero. The zero set \( \{\det(\mathbb{M}) = 0\} \) has Lebesgue measure zero provided \( \ell < \frac{p(p+1)}{2} \). Therefore, with probability 1, \( \mathbb{M} \) has full rank only if \( \ell \geq \frac{p(p+1)}{2} \).
Hence, to span \( \Sigma' \), it almost surely requires at least \( \frac{p(p+1)}{2} \) representations. Therefore, the feedback complexity cannot be lower than \( \Omega\left(\frac{p(p+1)}{2}\right) \).

Combining the upper and lower bounds, we conclude that the feedback complexity for oblivious learning in Algorithm~\ref{alg: randmaha} is tightly bounded by \( \Theta\left(\frac{p(p+1)}{2}\right) \).

%Now, using Sard's theorem it is clear that if $r < \frac{d(d+1)}{2} - 1$ then the measure of the samples for which $\det(\mathbb{M}) = 0$ is zero and the lower bound holds.
%\end{proof}

\section{Proof of \thmref{thm: samplingsparse}: Sparse Activations Sampling}\label{app: samplesparse}

%\begin{proof}[Proof of \thmref{thm: samplingsparse}]
    Here we consider the analysis for the case when the activations $\cV$ are sampled from the sparse distribution as stated in \defref{def: sparse}.

    In \thmref{thm: samplingsparse}, we assume that the activations are sampled from a Lebesgue distribution. This, sufficiently, ensures that (almost surely) any random sampling of $P$ activations induces a set of linearly independent rank-1 matrices. Since the distribution in \assref{ass: sparse} is not a Lebesgue distribution over the entire support $\bracket{0,1}$, requiring an understanding of certain events of the sampling of activations which could lead to linearly independent rank-1 matrices.

    In the proof of \thmref{thm: constructsparse}, we used a set of sparse activations using the standard basis of the vector space $\reals^p$. We note that the idea could be generalized to arbitrary choice of scalars as well, i.e.,
    \begin{align*}
         U_g = \{\lambda_i e_i: \lambda_i \neq 0, 1 \leq i \leq p\} \cup \{(\lambda_{iji} e_i + \lambda_{ijj}e_j): \lambda_{iji},\lambda_{ijj} \neq 0,  1 \leq i < j \leq p \}.
    \end{align*}
    Here $e_i$ is the $i$th standard basis vector. Note that the corresponding set of rank-1 matrices, denoted as $\widehat{U}_g$ 
    \begin{align*}
        \widehat{U}_g = \curlybracket{\lambda_i^2 e_ie_i^T: 1 \leq i \leq p} \cup \curlybracket{(\lambda_{iji} e_i + \lambda_{ijj}e_j)(\lambda_{iji} e_i + \lambda_{ijj}e_j)^T: 1 \leq i < j \leq p}
    \end{align*}
    is linearly independent in the space of symmetric matrices on $\reals^p$, i.e., $\symm$.

    Assume that activations are sampled $P$ times, denoted as $\cV_P$. Now, consider the design matrix $\mathbb{M} = \bracket{ V_1 \quad V_2 \quad \ldots V_P}$ as shown in the proof of \thmref{thm: samplegeneral}. We know that if $\det(\mathbb{M})$ is non-zero then $\curlybracket{V_i}'s$ are linearly independent in $\symm$. To show if a sampled set $\cV_P$ exhibits this property we need to show that $\det(\mathbb{M})$ is not identically zero, which could be possible for activations sampled from sparse distributions as stated in \assref{ass: sparse}, i.e. $\cP_{v \sim \cD_{\sf{sparse}}}( v_i \neq 0) > 0$.

    
    
    Note that $\det(\mathbb{M}) = \sum_{\sigma \in \sf{P}_P} \prod_i \mathbb{M}_{i \sigma(i)}$. Consider the diagonal of $\mathbb{M}$. Consider the situation where all the entries are non-zero. This corresponds to sampling a set of activations of the form $\widehat{U}_g$. Consider the following random design matrix $\mathbb{M}$.

    \[
\mathbb{M} = \begin{bmatrix}
\lambda_1^2 & \cdot & \cdot & \cdots & \cdot &\lambda_{121}^2 & \cdots & \cdot\\
\cdot & \lambda_2^2 & \cdot & \cdots & \cdot &\lambda_{122}^2 & \cdots & \vdots\\
\cdot & \cdot & \lambda_3^2 & \cdots & \cdot &\cdot & \cdots & \lambda_{(p-1)p(p-1)}^2\\
\vdots & \cdots & \cdots & \cdots & \lambda_p^2 & \cdots & \cdots & \lambda_{(p-1)pp}^2\\
\vdots & \cdots & \cdots & \cdots & \cdot & \lambda_{121}\lambda_{122} & \cdots & \cdots\\
\vdots & \cdots & \cdots & \cdots & \cdot & \cdot & \cdots & \cdots\\
\cdot & \cdots & \cdots & \cdots & \cdot & \cdot & \cdot & \lambda_{(p-1)p(p-1)}\lambda_{(p-1)pp}\\
\end{bmatrix}.
\]
Now, a random design matrix $\mathbb{M}$ is not identically zero for any set of $P$ randomly sampled activations that satisfy the following indexing property:
    \begin{align}
        \cR := \{v: v_i \neq 0, 1 \leq i \leq p\} \cup \{v: v_i,v_j \neq 0,  1 \leq i < j \leq p \}. \label{eq: random}
    \end{align}
    This is so because for identity permutation, we have $\prod_{i} \mathbb{M}_{ii} \neq 0$.
    Now, we will compute the probability that $\cR$ is sampled from $\cD_{\sf{sparse}}$. Using the independence of sampling of each index of an activation, the probabilities for the two subsets of $\cR$ can be computed as follows:
    \begin{itemize}
        \item $p$ activations $\curlybracket{\alpha_1, \alpha_2,\cdots, \alpha_p} \sim \cD_{\sf{sparse}}^p$ such that $\alpha_{ii} \neq 0$. Using independence, we have $$\cP_1 = \sum_{i = 0}^{s-1} \binom{p-1}{i} p_{nz}^{i+1} (1 - p_{nz})^{p-1-i},$$
        \item Rest of $p(p-1)/2$ activations of $\cR$ in \eqnref{eq: random} require at least two indices to be non-zero. This could be computed as $$\cP_2 = \sum_{i = 0}^{s-2} \binom{p-2}{i} p_{nz}^{i+2} (1 - p_{nz})^{p-2-i}.$$
    \end{itemize}
    Now, note that these $P$ activations can be permuted in $P!$ ways and thus
    \begin{align}
        \cP_{\cV_p}(\cV_P \equiv \cR) \ge P!\cdot\cP_1 \cdot \cP_2 = \underbrace{\textcolor{black}{P!\cdot \paren{\sum_{i = 0}^{s-1} \binom{p-1}{i} p_{nz}^{i+1} (1 - p_{nz})^{p-1-i}} \paren{\sum_{i = 0}^{s-2} \binom{p-2}{i} p_{nz}^{i+2} (1 - p_{nz})^{p-2-i}}}}_{p_{\sf{s}}}\label{eq: lb}
    \end{align}
    Now, we will complete the proof of the theorem using Hoeffding's inequality. Assume that the agent samples $N$ activations, we will compute the probability that $\cR \subset \cV_N$. Consider all possible $P$-subsets of $N$ items, enumerated as $\curlybracket{1,2,\ldots, \binom{N}{P}}$. Now, define random variables $X_i$ as
    \begin{align*}
        X_i = \begin{cases}
            1 \textnormal{ if $i$th subset equals } \cR,\\
            0 \textnormal{ o.w.}
        \end{cases}
    \end{align*}
    Now, define sum random variable $X = \sum_i^{\binom{N}{P}} X_i$. We want to understand the probability $\cP_{\cV_N}(X \ge 1)$. Now note that,
    \begin{align*}
        \expctover{\cV_N \sim \cD_{\sf{sparse}}}{X} = \sum_i \expct{X_i} = \binom{N}{P}\cdot \cP_{\cV_P}(\cV_P \equiv \cR)
    \end{align*}
    Now, using Hoeffding's inequality
    \begin{align*}
        \cP_{\cV_N}(X > 0) \ge 1 - 2\exp^{-2\expct{X}^2} \ge 1 - 2\exp^{-2\binom{N}{P}^2 p_{\sf{s}}^2}
    \end{align*}
    Now, for a given choice of of $\delta > 0$, we want $\delta \ge 2\exp^{-2\binom{N}{P}^2 p_{\sf{s}}^2}$. Using Sterling's approximation
    \begin{align*}
          \binom{N}{P} \ge \frac{1}{p_{\sf{s}}} \sqrt{\log \frac{4}{\delta^2}}
        \implies \paren{\frac{eN}{P}}^P \ge \frac{1}{p_{\sf{s}}} \sqrt{\log \frac{4}{\delta^2}} \implies N \ge \frac{P}{e} \paren{\frac{1}{p_{\sf{s}}^2} \log \frac{4}{\delta^2}}^{1/2P}
    \end{align*}

\newpage














    \iffalse
    ----------------------------\\
    We assume that the agent receives representations from a joint Beta distribution over every index of a random activation $\alpha \in \cV \subseteq \reals^p$:
    \begin{align*}
        \forall i,\quad    \textbf{Beta}(v_i; \mu_i, \nu_i) = v_i^{\mu_i -1}(1 - v_i)^{\nu_i-1}
    \end{align*}
    
\begin{enumerate}
    \item Count the number of directions such that only $ps$ many indices are large, i.e. close to 1 rest close to zero. There are $2^{ps}$ such directions. If we are sampling activations what is the probability that only $ps$ many indices are large but others close to zero. Now, how many times do we have to simply activations so that at least $\frac{p(p+1)}{2}$ many sparse activations are available?
\end{enumerate}
   \fi 