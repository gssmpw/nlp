\section{Introduction}
\iffalse
In recent past, models based on neural networks have become the state-of-the-art on many tasks. These models are known to capture the relevant features or concepts over the samples according to the prediction task at hand. \akash{maybe put the RFM or other feature learning work somewhere in the middle here} A general problem of interest is to understand how these models learn these features, and if they could be interpreted; more desirably retrieved as such. The recent work on \tt{mechanistic interpretability} has opened multiple doors (\akash{which doors?}) on how transformer-based models, such as Large Language Models (LLMs) learns these features. One line of work suggests that features are encoded linearly in the latent representation space (sometimes in the activation space of the model layer) using sparse activations, which has been termed as \tt{linear representation hypothesis} (LRH). The issue with this hypothesis is that more often than not, models are active on more features than the dimension of the ambient activation space of a layer, and thus are in the \tt{overcomplete} setting, which has been studied as the superposition in LLMs. Recent work has tried to retrieve features in an interpretable manner from superposed models in the form of sparse coding, aka dictionary learning by training various sparse autoencoders via connection to different layers of a model. 

By decomposing the features in a linear manner of a dictionary, it could provide a method to monitor particular feature activations, including those related to safety concerns, and facilitates predictable modifications of the network’s behavior by altering feature values across layers. Overall, linear feature decomposition provides a structured method for interpreting, controlling, and validating the internal mechanisms of trained models. If the features learnt by a larger model are universal, it provides a method to train and solve for smaller models that could achieve high accuracy and interpretability.

\akash{a line on what has been done on learning these dictionaries using SAEs and circuits}

In this work, we pose the problem of distilling these complicated features in the form of a dictionary via feedback from a larger model (e.g. ChatGPT, Claude Sonnet) or human agent. If $\dd \in \reals^{d \times p}$ is a dictionary and $\curlybracket{u_1,u_2,\ldots,u_p} \subset \reals^d$, its atoms denoting certain features such as $\curlybracket{\text{tree}, \text{house}, \ldots, \text{lawn}}$ pertaining to the sample space for a task, an agent demonstrates if one sparse combination of a subset of the atoms is similar/dissimilar to another sparse combination. The goal is to understand to what extent an oblivious learner, one who simply learns by satisfying all the constraints of the feedback, and randomly picks a valid feature, can find the relevant features. So, the basic protocol runs as follows:
\begin{itemize}
    \item Agent picks sparse triplets ${(\alpha, \beta, \zeta)_{\ell} \in \reals^{3p}}$ with labels $\ell \in \curlybracket{<, =, >}$ corresponding to a feature matrix $\dd$, and provides it to the learner
    \item Learner solves for $\curlybracket{||\dd^\top (\alpha - \beta)|| \, - \, ||\dd^\top (\alpha - \zeta)||}$ \text{ relates to } 0 \text{ with label } $\ell$ and outputs a solution $\dd\dd^\top$
\end{itemize}

 Table~\ref{tab: results} provides a summary of these different teaching settings and learning paradigms; our main results and contributions are summarized below:
\begin{enumerate}[label={\Roman*.},leftmargin=*]

    \item We study the feedback complexity in a constructive setting where the agent and pick any vector from $\reals^p$ and stipulate strong bounds in both general and sparse settings.
    %for the optimal teacher in the LwEQ paradigm, termed as \tt{learning-with-equivalence-queries teaching dimension} (LwEQ-TD). (see \secref{sec.query_compl_best})
    %\vspace{1mm}
    %\item For LwEQ paradigm, we discuss the query protocol between a learner and a teacher in \algoref{alg:PoEG} (see \secref{sec.statement}). Then, we formulate the query complexity for the optimal teacher in the LwEQ paradigm, as \tt{LwEQ teaching dimension} (LwEQ-TD). (see \secref{sec.query_compl_best}) \vspace{1mm}
    %
    %optimal teaching problem in the Learning-with-EQueries paradigm, and propose a novel measure of teaching complexity, namely, the \tt{learning-with-EQueries teaching dimension} (\textnormal{LwEQ-TD}) (see Section \ref{sec.query_compl_best}).\vspace{1mm}
    %
    %%%%%%%%%%%%%%%%%
    \item\looseness-1 Furthermore, we consider the setting where the representations are sampled from a distribution and establish two types of results: one for general representations and another for sparse representation. We study the sparse sampling case user a natural extension to Lebesgue measure for sparsity.
    %We study the query complexity in the LwEQ paradigm under different teaching settings: worst-case, random, and best-case, distinguished by the informativeness of counterexamples. We showcase the power of best-case counterexamples picked by the optimal teacher,  in  contrast  to  worst-case  or  random  counterexamples,  for  different  hypothesis  classes, including Axes-aligned hyperplanes, Monotone monomials, and Orthogonal rectangles. (see Section \ref{sec: best case teachers})
    %Furthermore, for different families of learners, we showcase the power of best-case teacher via comparing LwEQ-TD with query complexity for other teachers, for three hypothesis classes: Axes-aligned Hyperplanes, Monotone Monomials, and Orthogonal Rectangles. (see Section \ref{sec: best case teachers})
    %\item\looseness-1 We study the query complexity in the LwEQ paradigm under different types of teachers: worst-case, random, and best-case, distinguished by the informativeness of the counterexamples. Furthermore, for different families of learners, we showcase the power of best-case teacher via comparing LwEQ-TD with query complexity for other teachers, for three hypothesis classes: Axes-aligned Hyperplanes, Monotone Monomials, and Orthogonal Rectangles. (see Section \ref{sec: best case teachers}) %\vspace{1mm}
    %\item We show teaching complexity results for various \tt{history dependent} learners (cf \secref{sec.statement}) corresponding to alternate teachers for the hypothesis class of axes aligned hyperplanes.\vspace{1mm}
    %%%%%%%%%%%%%%%%%
    %\item\looseness-1 We establish new connections between LwEQ-TD and LfS-TD by studying LwEQ-TD for different learner models based on the richness of their query functions. We show that LwEQ-TD is the same as wc-TD~\cite{goldman1995complexity}, RTD~\cite{zilles2011models,doliwa2014recursive}, and NCTD~\cite{pmlr-v98-kirkpatrick19a} for a hypothesis class when restricting query functions to specific families. In general, LwEQ-TD is weaker than LfS-TD, e.g., LwEQ-TD is lower-bounded by local-PBTD~\cite{DBLP:conf/nips/ChenSAPY18,DBLP:conf/nips/Mansouri0VZS19} of the hypothesis class when the learner's next query depends on the previous query. (see Section \ref{sec: arbitraryexamples})
    %\item\looseness-1 We establish new connections between \textnormal{LwEQ-TD} and different notions of \textnormal{TD} in the LfS paradigm. We show \textnormal{LwEQ-TD} is equivalent to \textnormal{wc-TD}, \textnormal{RTD}, and \textnormal{NCTD} under respective families of learner models, whereas, for a specific family of learners, LwEQ-TD is weaker, e.g. than local-PBTD. (see Section \ref{sec: arbitraryexamples})    
\end{enumerate}
\fi
% Ensure the following packages are included in your preamble
%\usepackage{amsmath, amssymb, enumitem}
% Your revised introduction
In recent years, neural network-based models have achieved state-of-the-art performance across a wide array of tasks. These models effectively capture relevant features or concepts from samples, tailored to the specific prediction tasks they address~\citep{yang_feature,Bordelon2022SelfconsistentDF,ba2022highdimensional}. A fundamental challenge lies in understanding how these models learn such features and determining whether these features can be interpreted or even retrieved directly~\citep{rfm}. Recent advancements in \tt{mechanistic interpretability} have opened multiple avenues for elucidating how transformer-based models, including Large Language Models (LLMs), acquire and represent features~\citep{bricken2023monosemanticity, doshivelez2017rigorousscienceinterpretablemachine}. These advances include uncovering neural circuits that encode specific concepts~\citep{marks2024sparsefeaturecircuitsdiscovering,olah2020zoom}, understanding feature composition across attention layers~\citep{yang_feature}, and revealing how models develop structured representations~\citep{elhage2022superposition}.
%These advances include uncovering neural circuits that encode specific concepts, understanding feature composition across attention layers, and revealing how models develop structured representations.
%One line of research posits that features are encoded linearly within the latent representation space (and sometimes within the activation space of model layers) through sparse activations, a concept known as the \tt{linear representation hypothesis} (LRH)~\cite{elhage2022superposition}. However, this hypothesis encounters challenges since models frequently activate more features\akash{not sure about this} than the dimensionality of a layer's ambient activation space, placing them in an \tt{overcomplete} regime—a phenomenon studied under the notion of superposition in LLMs. 
One line of research posits that features are encoded linearly within the latent representation space through sparse activations, a concept known as the linear representation hypothesis (LRH)~\citep{mikolov-etal-2013-linguistic,arora-etal-2016-latent}. However, this hypothesis faces challenges in explaining how neural networks function, as models often need to represent more distinct features than their layer dimensions would theoretically allow under purely linear encoding. This phenomenon has been studied extensively in the context of large language models through the lens of superposition~\citep{elhage2022superposition}, where multiple features share the same dimensional space in structured ways.
\iffalse
Recent efforts have aimed to retrieve interpretable features from such superposed models using sparse coding, also referred to as dictionary learning. This suggests that any given layer $\ell$th of the model learns a sample's feature linearly and thus, we can write 
\begin{align*}
    \bx \approx \dd_{\ell}\cdot \alpha_\ell(\bx) + \epsilon_{\ell}(\bx),
\end{align*}
where $\bx \in \reals^d$, $\mathbf{\dd}_{\ell} \in \reals^{d \times p}$ is a dictionary\footnote{could be both overcomplete and undercomplete.} matrix, $\alpha_\ell(\bx) \in \reals^p$ is a \tt{sparse} representation vector ($p$ could be larger or smaller than $d$) and error terms $\epsilon_\ell(\bx) \in \reals^p$. Recent works have tried to retrieve interpretable features as dictionaries by training various sparse autoencoders connected to different layers of the model \cite{bricken2023monosemanticity,huben2024sparse,marks2024sparsefeaturecircuitsdiscovering}. 


By linearly decomposing features using a dictionary, it becomes possible to monitor specific feature activations, including those related to safety concerns, and to facilitate predictable modifications of the network’s behavior by altering feature values across layers. Overall, linear feature decomposition offers a structured approach for interpreting, controlling, and validating the internal mechanisms of trained models.\akash{cite jailbreaking papers :D} Furthermore, if the features learned by larger models are indeed universal, this approach could enable the training and optimization of smaller models that maintain high levels of accuracy and interoperability.
\fi
%In recent years, neural network-based models have achieved state-of-the-art performance across diverse tasks by effectively capturing task-relevant features from samples~\cite{yang_feature,Bordelon2022SelfconsistentDF,ba2022highdimensional}. A fundamental challenge lies in understanding how these models learn and represent such features, and whether these features can be interpreted or retrieved directly~\cite{rfm}. Recent advances in mechanistic interpretability~\cite{bricken2023monosemanticity, doshivelez2017rigorousscienceinterpretablemachine} have illuminated how transformer-based models, including Large Language Models (LLMs), encode features through neural circuits and structured representations.

%One prominent hypothesis, known as the linear representation hypothesis (LRH)~\cite{elhage2022superposition}, suggests that features are encoded linearly within the latent representation space through sparse activations. However, models often need to represent more distinct features than their layer dimensions would theoretically allow under purely linear encoding, leading to superposition where multiple features share dimensional space in structured ways.

Recent efforts have addressed this challenge through sparse coding or dictionary learning, proposing that any layer $\ell$ of the model learns features linearly:
\begin{align*}
    \bx \approx \dd_{\ell}\cdot \alpha_\ell(\bx) + \epsilon_{\ell}(\bx),
\end{align*}
where $\bx \in \reals^d$, $\mathbf{\dd}_{\ell} \in \reals^{d \times p}$ is a dictionary\footnote{could be both overcomplete and undercomplete.} matrix, $\alpha_\ell(\bx) \in \reals^p$ is a sparse representation vector, and $\epsilon_\ell(\bx) \in \reals^p$ represents error terms. This approach enables retrieval of interpretable features through sparse autoencoders \citep{bricken2023monosemanticity,huben2024sparse,marks2024sparsefeaturecircuitsdiscovering}, allowing for targeted monitoring and modification of network behavior. The linear feature decomposition not only advances model interpretation but also suggests the potential for developing compact, interpretable models that maintain performance by leveraging universal features from larger architectures.
%\akash{a line on what has been done on learning these dictionaries using SAEs and circuits}

In this work, we explore how complex features encoded as a dictionary can be distilled through feedback from either advanced language models (e.g., ChatGPT, Claude 3.0 Sonnet) or human agents. Let's define a dictionary $\dd \in \reals^{d \times p}$ where each column represents an atomic feature vector. These atomic features, denoted as ${u_1, u_2, \ldots, u_p} \subset \reals^d$, could correspond to semantic concepts like "tree", "house", or "lawn" that are relevant to the task's sample space.
The core mechanism involves an agent (either AI or human) determining whether different sparse combinations of these atomic features are similar or dissimilar. Specifically, given sparse activation vectors $\alpha, \alpha' \in \reals^p$, the agent evaluates whether linear combinations such as $\alpha_1 v(\text{"tree"}) + \alpha_2 v(\text{"car"}) + ... + \alpha_d v(\text{"house"})$ are equivalent to other combinations using different activation vectors.
Precisely, we formalize these feedback relationships using relative triplet comparisons $(\alpha, \beta, \zeta) \in \cV$, where $\cV \subseteq \reals^p$ is the activation or representation space. These comparisons express that a linear combination of features using coefficients $\alpha$ is more similar to a combination using coefficients $\beta$ than to one using coefficients $\zeta$.

The objective is to determine the extent to which an oblivious learner—one who learns solely by satisfying the constraints of the feedback and randomly selecting valid features—can identify the relevant features up to \tt{normal transformation}. Formally, the fundamental protocol is as follows:
\begin{itemize}
    \item The agent selects (constructive or distributional) sparse triplets $(\alpha, \beta, \zeta)_{\ell} \in \reals^{3p}$ with labels $\ell \in \curlybracket{+1, 0, -1}$ corresponding to a dictionary $\dd$, and provides them to the learner.
    
    \item The learner solves for
    $$
    \curlybracket{ \sgn{\| \dd (\alpha - \beta) \| - \| \dd (\alpha - \zeta) \|} = \ell }\vspace{-2mm}
    $$
    and outputs a solution $\hat{\dd}^\top\hat{\dd}$. 
\end{itemize}
% For a given feature matrix $\pphi^* \in \cM_{\sf{F}}$, we specify feedbacks on activations in $\cV$ via relative triplet comparisons $(x,y,z) \in \cV$ as follows,
% \begin{gather*}
%     x_1 u_1(\text{"feature $1$"}) + \ldots + x_p u_p(\text{"feature $p$"}) \\ \text{ is similar to } \\
%     y_1 u_1(\text{"feature $1$"}) + \ldots + y_p u_p(\text{"feature $p$"}) \\ \text{ than } \\
%     z_1 u_1(\text{"feature $1$"}) + \ldots + z_p u_p(\text{"feature $p$"})
% \end{gather*}
Semantically, these relative distances provide the relative information on how ground truth samples, e.g. images, text among others, relate to each other.
%||Dalpha|| comparisons mean in practice, like one image is similar to another than the third one}
%\akash{why not call it covariance estimation}
We term the normal transformation $\dd^\top\dd$ for a given dictionary $\dd$ as feature matrices $\pphi \in \reals^{p \times p}$, which is exactly a covariance matrix. 
Alternatively, for the representation space $\cV \subseteq \reals^p$, this transformation defines a  Mahalanobis distance function $d: \cV \times \cV \to \reals$, characterized by the square symmetric linear transformation $\pphi \succeq 0$ such that for any pair of activations $(x,y) \in \cV^{2}$, their distance is given by:
\begin{align*}
d(x,y) := (x - y)^{\top} \pphi (x-y)
\end{align*}
When $\pphi$ embeds samples into $\reals^r$, it admits a decomposition $\pphi = \sf{L}^\top\sf{L}$ for $\sf{L} \in \reals^{r \times p}$, where $\sf{L}$ serves as a dictionary for this distance function—a formulation well-studied in metric learning literature~\citep{kulis_ml}.
In this work, we study the minimal number of interactions, termed as feedback complexity of learning feature matrices—normal transformations to a dictionary—of the form $\pphi^* \in \symmp$. We consider two types of feedback: general activations and sparse activations, examining both constructive and distributional settings. Our primary contributions are:\vspace{-2mm}
\begin{enumerate}[label={\Roman*.},leftmargin=*]
\item We investigate feedback complexity in the constructive setting, where agents select activations from $\reals^p$, establishing strong bounds for both general and sparse scenarios. (see \secref{sec: construct})
\item We analyze the distributional setting with sampled activations, developing results for both general and sparse representations. For sparse sampling, we extend the definition of a Lebesgue measure to accommodate sparsity constraints. (see \secref{sec: sample})
\item We validate our theoretical bounds through experiments with feature matrices from Recursive Feature Machine trained kernel classifiers and dictionaries trained for sparse autoencoders in Large Language Models, including Pythia-70M~\citep{pythia} and Board Game models~\citep{karvonen2024measuring}. (see \secref{sec: experiments})
\end{enumerate}
\iffalse
We term the normal transformation $\dd\dd^\top$ for a given dictionary $\dd$, holistically as feature matrices $\pphi \in \reals^{p \times p}$.
This could also be thought of as learning a Mahalanobis distance function, where the distance function $d: \cX \times \cX \to \reals$, defined for a sample space $\cX \subseteq \reals^d$,is characterized by a square symmetric linear transformation $\pphi \succeq 0$ such that for any pair of samples $(x,y) \in \cX^{2}$
\begin{align*}
    d(x,y) := (x - y)^{\top} \pphi (x-y)
\end{align*}
gives a distance or similarity between the pair. If we assume that $\pphi$ embeds samples into $\reals^r$ then we can decompose the matrix $\pphi$ as $\pphi = \sf{L}^\top\sf{L}$
for $\sf{L} \in \reals^{r \times p}$. Now, $\sf{L}$ is a dictionary for this distance function. This has been studied extensively in metric learning~\citep{kulis_ml}. Here, we assume that the samples come from ground-truth data.

% The objective is to determine the extent to which an oblivious learner—one who learns solely by satisfying the constraints of the feedback and randomly selecting valid features—can identify the relevant features. The fundamental protocol is as follows: \akash{clarify what is feature marix here whether $\pphi$ or $\dd$?}
% \begin{itemize}
%     \item The agent selects sparse triplets $(\alpha, \beta, \zeta)_{\ell} \in \reals^{3p}$ with labels $\ell \in \curlybracket{<, =, >}$ corresponding to a feature matrix $\dd$, and provides them to the learner.
%     \item The learner solves for
%     $$
%     \curlybracket{ \| \dd^\top (\alpha - \beta) \| - \| \dd^\top (\alpha - \zeta) \| \text{ relates to } 0 \text{ with label } \ell }
%     $$
%     and outputs a solution $\dd\dd^\top$.
% \end{itemize}
%-------------------\\
% In this work, we explore how complex features encoded as a dictionary can be distilled through feedback from either advanced language models (e.g., ChatGPT, Claude Sonnet) or human agents. Let's define a dictionary $\dd \in \reals^{d \times p}$ where each column represents an atomic feature vector. These atomic features, denoted as ${u_1, u_2, \ldots, u_p} \subset \reals^d$, correspond to semantic concepts like "tree", "house", or "lawn" that are relevant to the task's sample space.
% The core mechanism involves an agent (either AI or human) determining whether different sparse combinations of these atomic features are similar or dissimilar. Specifically, given sparse activation vectors $\alpha, \alpha' \in \reals^p$, the agent evaluates whether linear combinations such as $\alpha_1 v(\text{"tree"}) + \alpha_2 v(\text{"car"}) + ... + \alpha_d v(\text{"house"})$ are equivalent to other combinations using different activation vectors.



% For a feature matrix $\pphi^* \in \cM_{\sf{F}}$, we formalize these feedback relationships using relative triplet comparisons $(x,y,z) \in \cV$. These comparisons express that a linear combination of features using coefficients $x$ is more similar to a combination using coefficients $y$ than to one using coefficients $z$.
% We approach this problem through the lens of Mahalanobis distance learning, where we aim to learn a distance function $d: \cX \times \cX \to \reals$. This function is characterized by a positive semidefinite feature matrix $\pphi \succeq 0$, such that the distance between any pair of samples $(x,y) \in \cX^2$ is given by:
% $d(x,y) := (x - y)^{\top} \pphi (x-y)$
% When $\pphi$ embeds samples into $\reals^r$, we can decompose it as $\pphi = \sf{L}^\top\sf{L}$ where $\sf{L} \in \reals^{r \times p}$, a formulation well-studied in metric learning literature.
% Our primary research question examines how effectively an oblivious learner—one that learns solely through feedback constraints and random feature selection—can identify relevant features. The learning protocol proceeds as follows:

% The agent provides sparse triplets $(\alpha, \beta, \zeta)_{\ell} \in \reals^{3p}$ with corresponding labels $\ell \in {<, =, >}$ based on a feature matrix $\dd$.
% The learner then solves for relationships of the form:
% ${|\dd^\top (\alpha - \beta)| - |\dd^\top (\alpha - \zeta)| \text{ relates to } 0 \text{ with label } \ell}$
% and outputs a solution $\dd\dd^\top$.
Our main goal of this work is to study the feedback complexity of learning feature matrices, i.e. normal transformations to a dictionary, of the form $\pphi^* \in \symmp$ (space of symmetric positive semi-definite matrices) with two types of feedbacks: general activations and sparse activations under both constructive and distributional (sampled activations) settings. Our primary results and contributions are outlined below:
\begin{enumerate}[label={\Roman*.},leftmargin=*]
    \item In the constructive setting where the agent is allowed to pick activations of its choice from $\reals^p$, we investigate its feedback complexity and establish strong bounds in both general and sparse scenarios. (see \secref{sec: construct})
    \item In the distributional setting where agent receives activations from a distribution,
    we establish two types of results: general representations and sparse representations. We examine the sparse sampling case using a natural extension of the Lebesgue measure to accommodate sparsity. (see \secref{sec: sample})
    \item To showcase the strength of the theoretical bounds, we perform experiments for two types of feature matrices-one retrieved from a kernel classifier trained via Recursive Feature Machines, and dictionaries trained for sparse autoencoders for Large Language Models such as Pythia-70M~\cite{pythia} and Board Game models~\cite{karvonen2024measuring}. (see \secref{sec: experiments})
\end{enumerate}
\fi
Table~\ref{tab: results} summarizes our feedback complexity bounds for the various feedback settings examined in this study. 
\iffalse
\begin{table}[h!]
  \centering
  %\vspace{-1mm}
  \scalebox{.9}{
  \begin{tabular}{c|c|c|c}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
    %\cmidrule(r){1-3}
         \backslashbox{Learning}{Teaching} & Worst-case Teacher   & Random-case Teacher & Best-case Teacher\\ 
    \midrule
$\textnormal{Learning-with-equivalence-}$ & Worst-case counterexamples & 
Random counterexamples & \textnormal{LwEQ-TD}\\
\textnormal{queries} (\textnormal{LwEQ}) & \centering{\cite{ANGLUIN198787,angluin_read_once,query_angluin,turan_learncomplexity_ce,Angluin_negativeresults}} & \cite{Angluin2017ThePO,simplealgo_RCE} & \textbf{This work} \\
%&(worst-case counterexamples)&(random counterexamples)& \text{(this work)}\\
\midrule
$\textnormal{Learning-from-samples}$ &\text{Worst-case examples} &\text{i.i.d learning}& LfS-TD / classical TD\\
%\text{Classical} \textnormal{TD} (\textnormal{LfS-TD})
(\textnormal{LfS}) & (i.e., least informative) & \cite{valiant_paclearning,vapnik_stat_learn}~ & \cite{goldman1995complexity,zilles2011models,gao2017preference,pmlr-v98-kirkpatrick19a,DBLP:conf/nips/Mansouri0VZS19}~\\

    \bottomrule
  \end{tabular}}
  \vspace{2mm}
\caption{An overview of different teaching settings in the context of LwEQ and LfS paradigms.}
  \label{tab:teaching_for_query_learning}
  \vspace{-5mm}
\end{table}
\fi


%t allows researchers to quantify each feature’s contribution to a layer’s output and subsequent activations for specific examples. This approach enables monitoring of particular feature activations, including those related to safety concerns, and facilitates predictable modifications of the network’s behavior by altering feature values across layers. Additionally, it helps demonstrate that the model has captured and is utilizing specific data properties in its outputs, similar to what influence functions provide. Moreover, it supports the creation of inputs designed to activate certain features to achieve desired outcomes. Overall, linear feature decomposition provides a structured method for interpreting, controlling, and validating the internal mechanisms of trained models.


%\akash{Some pointers for the intro}
% \begin{itemize}
%     \item These interpretable dictionaries can be used for learning the exact alignment patterns in the large language model.
%     \item Superposition poses difficulty in interpreting the models with features and right context. SAE are one ways to do so. By learning these dictionaries we can interpret these models in the right way.
% \end{itemize}

% \akash{results are as follows: }
% \begin{enumerate}
%     \item Constructive case: Results for the models with sparsity and w/ sparsity
%     \item Sampling case: Results for the models with sparsity and w/ sparsity
% \end{enumerate}
% A interpretation of the teaching model is this: given a sparse activation vector $\alpha \in \reals^p$, superAgent/teacher specifies that $\alpha_1 v(``tree") + \alpha_2 v(``car") + ....+ \alpha_d v(``house")$ is same as $\alpha'_1 v(``tree") + \alpha'_2 v(``car") + ....+ \alpha'_d v(``house")$ for another sparse activation vector $\alpha' \in \reals^p$.


\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Setting} & \textbf{Feedback Complexity} \\ \hline
\text{Stardard Constructive} & \parbox{5cm}{
  \textcolor{black}{$\Theta\left(\frac{r(r+1)}{2} + p - r + 1\right)$} %\\ 
  %\textcolor{black}{$\Omega\left(\frac{r(r+1)}{2} + p - r + 1\right),\ 
  %O\left(\frac{p(p+1)}{2} - 1\right)$}
}   \\ \hline
\text{Sparse Constructive} & \parbox{5cm}{
  %\textcolor{black}{$\Theta\left(\frac{r(r+1)}{2} + p - r + 1\right)$} \\ 
  \textcolor{black}{$%\Omega\left(\frac{r(r+1)}{2} + p - r + 1\right),\\
   O\left(\frac{p(p+1)}{2} - 1\right)$}
}   \\ \hline
\text{Standard Sampling} & \parbox{5cm}{\textcolor{black}{$\Theta(\frac{p(p+1)}{2} - 1)$} \text{ (a.s) }%\text{ w.p. } 1%,\\ \textcolor{black}{$O\left(p^2 \paren{\frac{2}{p_{\sf{s}}^2} \log \frac{2}{\delta}}^{1/p^2}\right)$ \text{ w.p. } $(1-\delta)$}} 
}\\ \hline
\text{Sparse Sampling} & \parbox{5cm}{%\textcolor{black}{$\Theta(\frac{p(p+1)}{2} - 1)$} \text{ w.p. } 1,
\textcolor{black}{$c p^2 \paren{\frac{2}{p_{\sf{s}}^2} \log \frac{2}{\delta}}^{1/p^2}$ \text{ (w.h.p) } %$(1-\delta)$
}
}
\\\hline
%\textbf{Assumption} & - & \hyperref[sec:3.2.1]{3.2.1} & \hyperref[sec:3.4.1]{3.4.1}, \hyperref[sec:3.4.2]{3.4.2} \\ \hline
\end{tabular}
\caption{Feedback complexity under constructive and sampling cases. $r$ is the rank of the feature matrix, $c > 0$ is a constant and $p_{\sf{s}}$ is a constant that depends on the activation distribution and sparsity $s$. We abbreviate: a.s for \tt{almost surely} and w.h.p for \tt{with high probability}.
%: \textcolor{violet}{violet is for w/ sparsity}, \textcolor{gray}{gray is for w sparsity}.
}
\label{tab: results}
\end{table}


% -----------------------------------------\\

% %Based on the full context, I'll help craft a more cohesive introduction that builds on the existing strengths while avoiding repetition. Here's my proposed version:

% In recent years, neural network-based models have achieved state-of-the-art performance across a wide array of tasks, effectively capturing relevant features and concepts tailored to specific prediction tasks. A fundamental challenge lies in understanding how these models learn such features and determining whether these features can be interpreted or retrieved directly. Recent advancements in mechanistic interpretability have revealed insights into how transformer-based models, including Large Language Models (LLMs), acquire and represent these features.

% One significant line of research posits that features are encoded linearly within the latent representation space through sparse activations, a concept known as the linear representation hypothesis (LRH). However, this hypothesis faces challenges since models frequently operate in an overcomplete regime—a phenomenon studied under the notion of superposition in LLMs. Recent efforts have aimed to retrieve interpretable features from such superposed models using sparse coding (dictionary learning). Under this framework, for any given layer $\ell$ of the model, we can express a sample's features linearly as:

% \begin{align*}
%     \bx \approx \dd_{\ell}\cdot \alpha_\ell(\bx) + \epsilon_{\ell}(\bx),
% \end{align*}

% where $\bx \in \reals^d$, $\mathbf{\dd}_{\ell} \in \reals^{d \times p}$ is a dictionary matrix, $\alpha_\ell(\bx) \in \reals^p$ is a sparse representation vector, and $\epsilon_\ell(\bx) \in \reals^p$ represents error terms.

% In this work, we address the problem of distilling these complex features into a dictionary format through feedback from either advanced language models (e.g., ChatGPT, Claude Sonnet) or human agents. We frame this as a learning problem where a dictionary $\dd \in \reals^{d \times p}$ contains atomic feature vectors $\{u_1, u_2, \ldots, u_p\} \subset \reals^d$, each representing specific semantic concepts (e.g., "tree", "house", "lawn") relevant to the task's sample space.

% We approach this through the lens of Mahalanobis distance learning, where we aim to learn a distance function $d: \cX \times \cX \to \reals$ characterized by a positive semidefinite feature matrix $\pphi \succeq 0$. For any pair of samples $(x,y) \in \cX^2$, this distance is given by:

% \begin{align*}
%     d(x,y) := (x - y)^{\top} \pphi (x-y)
% \end{align*}

% When $\pphi$ embeds samples into $\reals^r$, we can decompose it as $\pphi = \sf{L}^\top\sf{L}$ where $\sf{L} \in \reals^{r \times p}$, a formulation well-studied in metric learning.

% Our key contribution lies in analyzing how an oblivious learner—one that learns solely through feedback constraints and random feature selection—can identify relevant features. The learning protocol involves an agent providing sparse triplet comparisons $(\alpha, \beta, \zeta)_{\ell} \in \reals^{3p}$ with labels $\ell \in \{<, =, >\}$, and the learner solving for relationships of the form:

% $$
% \{\|\dd^\top (\alpha - \beta)\| - \|\dd^\top (\alpha - \zeta)\| \text{ relates to } 0 \text{ with label } \ell\}
% $$

% Our primary results establish:
% 1. Feedback complexity bounds in a constructive setting where the agent can select any vector from $\reals^p$, with special attention to both general and sparse scenarios.
% 2. Complexity bounds when representations are sampled from a distribution, addressing both general and sparse representations using a natural extension of the Lebesgue measure.

% Table~\ref{tab: results} summarizes our feedback complexity bounds across various feedback settings and learning paradigms examined in this study.

% [Note: You'll need to add appropriate citations for the distillation concept, sparse autoencoders, and metric learning references.]

% Let me revise the introduction to more thoroughly incorporate the feedback mechanism and mathematical formulation:

% Neural network-based models have achieved remarkable performance across diverse tasks by effectively capturing relevant features from training samples. While these models excel at their designated tasks, a fundamental challenge persists: understanding how they learn features and determining whether these features can be interpreted or retrieved directly. Recent advances in mechanistic interpretability have provided insights into how transformer-based models, particularly Large Language Models (LLMs), acquire and represent these features.

% The linear representation hypothesis (LRH) suggests that features are encoded linearly within the latent representation space through sparse activations. However, this hypothesis faces a significant challenge: models frequently operate in an overcomplete regime, activating more features than their layer's ambient activation space can distinctly represent—a phenomenon known as superposition in LLMs. Recent research has addressed this challenge through sparse coding (dictionary learning), demonstrating that for any given layer $\ell$ of the model, we can express a sample's features linearly as:

% \begin{align*}
%     \bx \approx \dd_{\ell}\cdot \alpha_\ell(\bx) + \epsilon_{\ell}(\bx),
% \end{align*}

% where $\bx \in \reals^d$, $\mathbf{\dd}_{\ell} \in \reals^{d \times p}$ represents the dictionary matrix, $\alpha_\ell(\bx) \in \reals^p$ denotes a sparse representation vector, and $\epsilon_\ell(\bx) \in \reals^p$ accounts for error terms.

% In this work, we address the problem of distilling these complex features as dictionaries through feedback from advanced language models or human agents. Our framework centers on a dictionary $\dd \in \reals^{d \times p}$ containing atomic feature vectors $\{u_1, u_2, \ldots, u_p\} \subset \reals^d$, where each vector represents specific semantic concepts such as "tree", "house", or "lawn". The learning process relies on an agent's ability to evaluate similarities between sparse combinations of these atomic features. Specifically, given sparse activation vectors $\alpha, \alpha' \in \reals^p$, the agent determines whether linear combinations (e.g., $\alpha_1 v(\text{"tree"}) + \alpha_2 v(\text{"car"}) + ... + \alpha_d v(\text{"house"})$) are equivalent to other combinations using different activation vectors.

% For a feature matrix $\pphi^* \in \cM_{\sf{F}}$, we formalize these feedback relationships using relative triplet comparisons $(x,y,z) \in \cV$. These comparisons express that a linear combination of features using coefficients $x$ is more similar to a combination using coefficients $y$ than to one using coefficients $z$, as represented by:

% \begin{gather*}
%     x_1 u_1(\text{"feature $1$"}) + \ldots + x_p u_p(\text{"feature $p$"}) \\ \text{ is similar to } \\
%     y_1 u_1(\text{"feature $1$"}) + \ldots + y_p u_p(\text{"feature $p$"}) \\ \text{ than } \\
%     z_1 u_1(\text{"feature $1$"}) + \ldots + z_p u_p(\text{"feature $p$"})
% \end{gather*}

% We frame this problem within the context of Mahalanobis distance learning, seeking to learn a distance function $d: \cX \times \cX \to \reals$ characterized by a positive semidefinite feature matrix $\pphi \succeq 0$. For any sample pair $(x,y) \in \cX^2$, this distance is given by $d(x,y) := (x - y)^{\top} \pphi (x-y)$. When $\pphi$ embeds samples into $\reals^r$, we can decompose it as $\pphi = \sf{L}^\top\sf{L}$ where $\sf{L} \in \reals^{r \times p}$, building on established metric learning principles.

% Our research examines how effectively an oblivious learner—one that learns solely through feedback constraints and random feature selection—can identify relevant features. The learning protocol operates through triplet comparisons, where an agent provides sparse triplets $(\alpha, \beta, \zeta)_{\ell} \in \reals^{3p}$ with labels $\ell \in \{<, =, >\}$ corresponding to a feature matrix $\dd$. The learner then solves relationships of the form:

% $$
% \{\|\dd^\top (\alpha - \beta)\| - \|\dd^\top (\alpha - \zeta)\| \text{ relates to } 0 \text{ with label } \ell\}
% $$

% producing a solution $\dd\dd^\top$.

% Our analysis establishes feedback complexity bounds in both constructive and sampling settings, considering both general and sparse scenarios. We derive precise bounds for cases where representations are sampled from distributions, extending to sparse representations through a natural extension of the Lebesgue measure. These results, summarized in Table~\ref{tab: results}, provide a comprehensive understanding of the feedback requirements across various learning paradigms.

% This approach to feature interpretation offers significant practical benefits: it enables monitoring of specific feature activations, facilitates predictable modifications of network behavior, and could potentially enable the development of smaller, more interpretable models that maintain high accuracy while leveraging the universal features learned by larger models.

% [Note: Citations needed for the distillation concept, sparse autoencoders, and metric learning references.]