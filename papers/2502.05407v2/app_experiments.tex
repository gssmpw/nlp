\section{Additional Experiments}\label{app: additional}
We evaluate our methods on large-scale models: Pythia-70M~\citep{pythia} and Board Games Models~\citep{karvonen2024measuring}. These models present significant computational challenges, as different feedback methods generate millions of constraints. This scale necessitates specialized approaches for both memory management and computational efficiency.

\paragraph{Memory-efficient constraint storage} The high dimensionality of model dictionaries makes storing complete activation indices for each feature prohibitively memory-intensive. We address this by enforcing constant sparsity constraints, limiting activations to a maximum sparsity of 3. This constraint enables efficient storage of large-dimensional arrays while preserving the essential characteristics of the features.
\paragraph{Computational optimization} To efficiently handle constraint satisfaction at scale, we reformulate the problem as a matrix regression task, as detailed in \figref{fig:gradient}. The learner maintains a low-rank decomposition of the feature matrix $\pphi$, assuming $\pphi = UU^\top$, where $U$ represents the learned dictionary. This formulation allows for efficient batch-wise optimization over the constraint set while maintaining feasible memory requirements.
% In this section, we provide experiments on large scale models: Pythia-70M~\cite{pythia} and Board Games Models~\cite{karvonen2024measuring}. For these models, the number of constraints generated for different feedback methods scale in millions, we can not explicitly store all the constraints or fit to them directly. We propose the following schemes to resolve these computational and memory issues.

% \paragraph{Memory bottleneck of storing constraints:} Since the dimension of the dictionary could be very high, strong all the indices of an activation used for feature is memory inefficient., which could be handled with constant sparsity constraints. In our experiments, we assume that sparsity is at most 3. THis allows to store numpy arrays of large dimensions. 

% \paragraph{Computational issues:} In order to solve the problem constraints satisfaction, we exploy a matrix version of regression with constraints as samples as shown in \figref{fig:gradient}. Learner maintains a decomposition $U$ of the feature matrix $\pphi$ assuming $\pphi = UU^\top$.


\paragraph{Dictionary features of Pythia-70M} We use the publicly available repository for dictionary learning via sparse autoencoders on neural network activations~\citep{marks2024dictionarylearning}. We consider the dictionaries trained for Pythia-70M~\citep{pythia} (a general-purpose LLM trained on publicly available datasets). We retrieve the corresponding autoencoders for attention output layers which have dimensions $32768 \times 512$. Note that $p(p+1)/2 \approx, 512M$.
For the experiments, we use $3$-sparsity on uniform sparse distributions. We present the plots for ChessGPT in two parts in \figref{fig: subsample} and \figref{fig: pythiasample} for different feedback methods.


\newpage

\begin{figure}[h!]
\centering
    \begin{minipage}{0.99
    \textwidth}  % Controls width of the content
    \begin{algorithm}[H]
    \small  % or \footnotesize for smaller text
    \caption{Optimization via Gradient Descent}
    \label{alg:gradient}
    \begin{enumerate}
        \item Given a dictionary $U \in \mathbb{R}^{p \times r}$, minimize the loss $\mathcal{L}(U)$:
        \begin{equation}
            \mathcal{L}(U) = \mathcal{L}_{\text{MSE}}(U) + \mathcal{L}_{\text{reg}}(U)
        \end{equation}
        where MSE loss is:
        \begin{equation}
            \mathcal{L}_{\text{MSE}}(U) = \frac{1}{|B|}\sum_{i \in B} (\|U^\top u_i\|^2 - c_i\|U^\top y\|^2)^2
        \end{equation}
        and regularization term is:
        \begin{equation}
            \mathcal{L}_{\text{reg}}(U) = \lambda\|U\|_F^2
        \end{equation}
        
        \item For each batch containing indices $i$, values $v$, and targets $c$:
            \begin{enumerate}
                \item Construct sparse vectors $u_i$ using $(i,v)$ pairs
                \item Compute projected values: $U^\top u_i$ and $U^\top y$ where $y = e_1$
                \item Calculate residuals: $r_i = \|U^\top u_i\|^2 - c_i\|U^\top y\|^2$
            \end{enumerate}
        \item Update $U$ using Adam optimizer with gradient clipping
        \item Enforce fixed entries in $U$ after each update ($U[0,0] = 1$ \text{is enforced to be 1}.)
    \end{enumerate}
    where $B$ represents the batch of samples, $\lambda=10^{-4}$ is the regularization coefficient, and $y = e_1$ is the fixed unit vector.
    \end{algorithm}
    \end{minipage}
    \caption{Gradient-based optimization procedure for learning a dictionary decomposition $U$ with fixed entries.}
    \label{fig:gradient}
\end{figure}
% First Page of the Figure



% Pythia Code starts here
%\iffalse
\begin{figure*}[!]
    \centering
    % First Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{target_sae_pythia.png}
        \label{fig:sub1}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.9367,feedbacks-135316.png}
        \label{fig: pythiaeigen}
    \end{subfigure}
    \caption{Feature learning on a subsampled dictionary of dimension $4500 \times 512$ of SAE trained for Pythia-70M. \thmref{thm: constructgeneral} states that Eigendecompostion method requires 135316 constructive feedback. After few 100 iterations of gradient descent as shown in \figref{fig:gradient}, a PCC of 93\% is achieved on ground truth. For visualization, only the first 100 dimensions are used.}
    \label{fig: subsample}
\end{figure*}

\begin{figure*}[t]\ContinuedFloat
    \centering

    % First Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{target_sae_pythiafull.png}
        \label{fig:sub3}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.0242,feedbacks-200000.png}
        \label{fig: chessconst}
    \end{subfigure}
    \par % Start a new row

    % Second Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.3815,feedbacks-2000000.png}
        \label{fig:sub5}
    \end{subfigure}
\qquad    
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.5759,feedbacks-5000000.png}
        \label{fig:sub6}
    \end{subfigure}
    \par % Start a new row
\begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.6570,feedbacks-10000000.png}
        \label{fig:sub5}
    \end{subfigure}
\qquad  
    % Third Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.7716,feedbacks-20000000.png}
        \label{fig:sub7}
    \end{subfigure}
    % \qquad
    % \begin{subfigure}[b]{0.4\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{ICML'25/Images/learnt_PCC- 0.9741, feedbacks- 10000000.png}
    %     \label{fig: chessssample}
    % \end{subfigure}

    \caption{\textbf{Sparse sampling for Pythia-70M}: Dimension of feature matrix: $32768 \times 512$ and the rank is 215. Plots for varying feedback complexity sizes. Note that $p(p+1)/2 \approx$ 512M. We run experiments with 3-sparse activations for uniform sparse distributions. The Pearson Correlation Coefficient (PCC) to feedback size (PCC, Feedback size) improves as follows: $(200k, .0242), (2M, .38), (5M, .54),(10M, .65)$, and $(20M, .77)$.
    %Plots show the improvement in PCC with 3-sparse uniformly sampled activations with respect to the Ground Truth shown above.
    }
    \label{fig: pythiasample}
\end{figure*}
%\fi