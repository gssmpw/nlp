\section{Related Work}
\paragraph{Dictionary learning}
Recent work has explored dictionary learning to disentangle the semanticity (mono- or polysemy) of neural network activations**Coates, "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"**. Dictionary learning**Elad, "Sparse and Redundant Representations - From Filter Banks to Neuronal Encoding"** (aka sparse coding) provides a systematic approach to decompose task-specific samples into sparse signals. The sample complexity of dictionary learning (or sparse coding) has been extensively studied as an optimization problem, typically involving non-convex objectives such as $\ell_1$ regularization~(see **Tibshirani, "Regression Shrinkage and Selection via the Lasso"**). While traditional methods work directly with ground-truth samples, our approach differs fundamentally as the learner only receives feedback on sparse signals or activations. Prior work in noiseless settings has established probabilistic exact recovery up to linear transformations (permutations and sign changes) under mutual incoherence conditions**Candes, "Exact Signal Recovery from Highly Accurate Measurements via Convex Optimization"**. Our work extends these results by proving exact recovery (both deterministic and probabilistic) up to normal transformation, which generalizes to rotational and sign changes under strong incoherence properties (see \lemref{lem: ortho}). In the sampling regime, we analyze $k$-sparse signals, building upon the noisy setting framework developed in **Donoho, "Compressed Sensing"**. 

%____
%\akash{expand} \akash{when was first dictionaries where considered for LLMs/transformers. word-embeddings}
%has been extensively studied in the sparse coding literature. Prior work has considered the setting learner sees training samples corresponding to a dictionary, and the task is to find $\dd$ such that each sample is a sparse combination of the atoms\akash{stuff on dictionaries...theory and formulations}.
%Understanding the theory behind dictionary learning with feedbacks is useful for ____ posits that large language models encode features in a linear manner. This allows us to interpret the models, in particular its neurons and attention layers, and thus could allow us to understand/devise simple algorithms for complicated tasks____. To extract interpretable features, recent work has trained sparse autoencoders for dictionaries____ for LMMs such as Board Games Models. \akash{complete this line} 
%____: applied Sparse Au-
%toencoders (SAEs), a scalable unsupervised learning method inspired by sparse dictionary
%learning

\paragraph{Feature learning in neural networks and Linear representation hypothesis}
%____
%____
% Neural networks show remarkable ability to discover and leverage task-specific features from data____. Recent work has made substantial progress in understanding how these features evolve and emergence during the training process ____. ____ posits that the outer product of model weights relates to the gradient outer product of the classifier averaged over the preactivations to that the layer. Among other notions of features, these outer products are precisely the covariance matrices of interest to our work. 
% %Understanding the theory behind dictionary learning with feedbacks is useful for
% Recently, ____ posits that large language models encode features in a linear manner. This allows us to interpret the models, in particular its neurons and attention layers, and thus could allow us to understand/devise simple algorithms for complicated tasks____. To extract interpretable features, recent work has trained sparse autoencoders to obtains dictionaries____ for LMMs such as Board Games Models. In our work, we consider if an interpretable dictionary could be transferred to a weak learner with minimal comparative feedback.
Neural networks demonstrate a remarkable capability to discover and exploit task-specific features from data**LeCun, "Backpropagation Applied to Handwritten Zip Code Recognition"**. Recent theoretical advances have significantly enhanced our understanding of feature evolution and emergence during training**Bengio, "Learning Long-Range Recurrent Connections in Neural Networks"**. Particularly noteworthy is the finding that the outer product of model weights correlates with the gradient outer product of the classifier averaged over layer preactivations**LeCun, "Backpropagation Applied to Handwritten Zip Code Recognition"**, which directly relates to the covariance matrices central to our investigation. Building upon these insights, **Bengio, "Learning Long-Range Recurrent Connections in Neural Networks"** proposed that features in large language models follow a linear encoding principle, suggesting that the complex feature representations learned during training can be decomposed into interpretable linear components. This interpretability, in turn, could facilitate the development of simplified algorithms for complex tasks____. Recent research has focused on extracting these interpretable features in the form of dictionary learning by training sparse autoencoder for various language models including Board Games Models____. Our work extends this line of inquiry by investigating whether such interpretable dictionaries can be effectively transferred to a weak learner using minimal comparative feedback.

%Neural networks' remarkable success is fundamentally attributed to their capacity to automatically discover and leverage task-specific features from data [74, 93]. This feature learning ability, defined as the evolution of a network's internal representations during training [12, 93], has emerged as a critical area of investigation in machine learning research. Recent theoretical advances [1, 6, 7, 20, 28, 33, 44, 47, 55, 67, 85, 98] have demonstrated the significant advantages of neural feature learning compared to fixed-feature approaches, particularly in enhancing model reliability and transparency for critical applications. Despite these advances in understanding the benefits of feature learning, the precise mechanisms governing how these features emerge and evolve during training have remained incompletely understood. Our work addresses this gap by proposing a concrete mechanism for feature learning in deep, nonlinear fully connected neural networks.
\paragraph{Triplet learning a covariance matrix}
%Learning a feature matrix (or a dictionary) up to normal transformation could be thought of as covariance estimation____ or learning linear distances such as Mahalanobis matrices____. The primary distinction with our mechanism is we only allow relative feedback whereas these studies have been primarily under measurement information (exact or noisy). The semantic of feedback that provides relative information is more akin to Mahalanobis distances. Learning Mahalanobis distance functions has been a significant focus in metric learning research**Weinberger, "Distance Metric Learning for Large Margin Nearest Neighbor Classification"**. It has been studied both in supervised settings through information via class labels**Kulis, "Metric Learning to Rank"**, or unsupervised methods like LDA____ and PCA____,  ____ has explored learning these distances through relative comparisons or triplet constraints~. Of particular relevance are the studies by**Hardt, "Matrix Factorization with Bregman Divergences: Improved Efficiency and Accurate Recovery"** and**Dhillon, "A Fast Least Squares Framework for Efficient Low-Rank Matrix Learning from Noisy Data"**, which utilize triplet comparisons. These approaches typically assume i.i.d. triplets with potentially noisy measurements. In our work, we consider the signals are drawn i.i.d but the agent decides to provide feedback on informative ones similar to active learning. To the best of our understanding, the case of constructive triplets hasn't been explored for covariance estimation, albeit the decision on information in our interaction protocol comes from the domain of machine teaching where typically there is a teaching agent that provides helpful examples to a learner~(see ____ for additional references therein). 
Learning a feature matrix (for a dictionary) up to normal transformation can be viewed through two established frameworks: covariance estimation**Kulis, "Metric Learning to Rank"** and learning linear distances such as Mahalanobis matrices____. While these frameworks traditionally rely on exact or noisy measurements, our work introduces a distinct mechanism based solely on relative feedback, aligning more closely with the semantic structure of Mahalanobis distances.
The study of Mahalanobis distance functions has been central to metric learning research**Weinberger, "Distance Metric Learning for Large Margin Nearest Neighbor Classification"**, encompassing both supervised approaches using class labels____ and unsupervised methods such as LDA____ and PCA____. ____ and____ have extended this framework to incorporate relative comparisons on distances. Particularly relevant to our work are studies by____ and____ that employ triplet comparisons, though these typically assume i.i.d. triplets with potentially noisy measurements.
Our approach differs by incorporating an active learning element: while signals are drawn i.i.d, an agent selectively provides feedback on informative instances. This constructive triplet framework for covariance estimation represents a novel direction, drawing inspiration from machine teaching, where a teaching agent provides carefully chosen examples to facilitate learning____.
%Our work differs fundamentally as we study an active learning setting where the learner can query specific triplets and receive exact feedback, allowing for more efficient learning of the underlying distance function.