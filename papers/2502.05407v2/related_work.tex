\section{Related Work}



\paragraph{Dictionary learning}
Recent work has explored dictionary learning to disentangle the semanticity (mono- or polysemy) of neural network activations~\citep{faruqui2015sparse,arora2018linear,subramanian2018spine,zhang2019word,yun2021transformer}. Dictionary learning~\citep{mallat_dict, OLSHAUSEN19973311} (aka sparse coding) provides a systematic approach to decompose task-specific samples into sparse signals. The sample complexity of dictionary learning (or sparse coding) has been extensively studied as an optimization problem, typically involving non-convex objectives such as $\ell_1$ regularization~(see \cite{bachsparse}). While traditional methods work directly with ground-truth samples, our approach differs fundamentally as the learner only receives feedback on sparse signals or activations. Prior work in noiseless settings has established probabilistic exact recovery up to linear transformations (permutations and sign changes) under mutual incoherence conditions~\citep{gribonval_rotation, agarwal_incoherent}. Our work extends these results by proving exact recovery (both deterministic and probabilistic) up to normal transformation, which generalizes to rotational and sign changes under strong incoherence properties (see \lemref{lem: ortho}). In the sampling regime, we analyze $k$-sparse signals, building upon the noisy setting framework developed in \citet{Arora2013NewAF,bachsparse}. 
%\cite{faruqui2015sparse,arora2018linear,subramanian2018spine,zhang2019word,panigrahi2019word2sense,yun2021transformer}
%\akash{expand} \akash{when was first dictionaries where considered for LLMs/transformers. word-embeddings}
%has been extensively studied in the sparse coding literature. Prior work has considered the setting learner sees training samples corresponding to a dictionary, and the task is to find $\dd$ such that each sample is a sparse combination of the atoms\akash{stuff on dictionaries...theory and formulations}.
%Understanding the theory behind dictionary learning with feedbacks is useful for \citet{elhage2022superposition} posits that large language models encode features in a linear manner. This allows us to interpret the models, in particular its neurons and attention layers, and thus could allow us to understand/devise simple algorithms for complicated tasks~\citep{Fawzi2022,Romera-Paredes2024}. To extract interpretable features, recent work has trained sparse autoencoders for dictionaries~\cite{marks2024sparsefeaturecircuitsdiscovering, bricken2023monosemanticity,huben2024sparse,karvonen2024measuring} for LMMs such as Board Games Models. \akash{complete this line} 
%\cite{marks2024sparsefeaturecircuitsdiscovering, bricken2023monosemanticity,huben2024sparse,karvonen2024measuring}: applied Sparse Au-
%toencoders (SAEs), a scalable unsupervised learning method inspired by sparse dictionary
%learning

\paragraph{Feature learning in neural networks and Linear representation hypothesis}
%\cite{abbe2022merged}
%\cite{abbe2022merged,ba2022high,bai2019beyond,bordelon2022self,damian2022neural,hanin2020finite,huang2020dynamics,lewkowycz2020large,li2019towards,mousavi-hosseini2023neural,roberts2022principles,shi2022theoretical,vyas2022limitations,yang2021tensor,zhu2022quadratic}
% Neural networks show remarkable ability to discover and leverage task-specific features from data~\cite{yang_feature, bordelon2022self,shi2022theoretical}. Recent work has made substantial progress in understanding how these features evolve and emergence during the training process \cite{abbe2022merged,ba2022high,damian2022neural,hanin2020finite,huang2020dynamics,lewkowycz2020large,vyas2022limitations,yang2021tensor,zhu2022quadratic}. \cite{rfm} posits that the outer product of model weights relates to the gradient outer product of the classifier averaged over the preactivations to that the layer. Among other notions of features, these outer products are precisely the covariance matrices of interest to our work. 
% %Understanding the theory behind dictionary learning with feedbacks is useful for
% Recently, \citet{elhage2022superposition} posits that large language models encode features in a linear manner. This allows us to interpret the models, in particular its neurons and attention layers, and thus could allow us to understand/devise simple algorithms for complicated tasks~\citep{Fawzi2022,Romera-Paredes2024}. To extract interpretable features, recent work has trained sparse autoencoders to obtains dictionaries~\cite{marks2024sparsefeaturecircuitsdiscovering, bricken2023monosemanticity,huben2024sparse,karvonen2024measuring} for LMMs such as Board Games Models. In our work, we consider if an interpretable dictionary could be transferred to a weak learner with minimal comparative feedback.
Neural networks demonstrate a remarkable capability to discover and exploit task-specific features from data~\citep{yang_feature, bordelon2022self,shi2022theoretical}. Recent theoretical advances have significantly enhanced our understanding of feature evolution and emergence during training~\citep{abbe2022merged,ba2022high,damian2022neural,yang2021tensor,zhu2022quadratic}. Particularly noteworthy is the finding that the outer product of model weights correlates with the gradient outer product of the classifier averaged over layer preactivations~\citep{rfm}, which directly relates to the covariance matrices central to our investigation. Building upon these insights, \citet{elhage2022superposition} proposed that features in large language models follow a linear encoding principle, suggesting that the complex feature representations learned during training can be decomposed into interpretable linear components. This interpretability, in turn, could facilitate the development of simplified algorithms for complex tasks~\citep{Fawzi2022,Romera-Paredes2024}. Recent research has focused on extracting these interpretable features in the form of dictionary learning by training sparse autoencoder for various language models including Board Games Models~\citep{marks2024sparsefeaturecircuitsdiscovering, bricken2023monosemanticity,karvonen2024measuring}. Our work extends this line of inquiry by investigating whether such interpretable dictionaries can be effectively transferred to a weak learner using minimal comparative feedback.

%Neural networks' remarkable success is fundamentally attributed to their capacity to automatically discover and leverage task-specific features from data [74, 93]. This feature learning ability, defined as the evolution of a network's internal representations during training [12, 93], has emerged as a critical area of investigation in machine learning research. Recent theoretical advances [1, 6, 7, 20, 28, 33, 44, 47, 55, 67, 85, 98] have demonstrated the significant advantages of neural feature learning compared to fixed-feature approaches, particularly in enhancing model reliability and transparency for critical applications. Despite these advances in understanding the benefits of feature learning, the precise mechanisms governing how these features emerge and evolve during training have remained incompletely understood. Our work addresses this gap by proposing a concrete mechanism for feature learning in deep, nonlinear fully connected neural networks.
\paragraph{Triplet learning a covariance matrix}
%Learning a feature matrix (or a dictionary) up to normal transformation could be thought of as covariance estimation~\cite{Chen2013ExactAS,voroninski} or learning linear distances such as Mahalanobis matrices~\cite{kulis_ml}. The primary distinction with our mechanism is we only allow relative feedback whereas these studies have been primarily under measurement information (exact or noisy). The semantic of feedback that provides relative information is more akin to Mahalanobis distances. Learning Mahalanobis distance functions has been a significant focus in metric learning research~\cite{bellet_survey,kulis_ml}. It has been studied both in supervised settings through information via class labels~\cite{LMNN,Xing2002DistanceML,itml,NCA} or unsupervised methods like LDA~\cite{LDA} and PCA~\cite{PCA},  \citet{distance_metric_relative,Kleindessner2016KernelFB} has explored learning these distances through relative comparisons or triplet constraints~. Of particular relevance are the studies by~\cite{distance_metric_relative} and~\cite{Mason2017LearningLM}, which utilize triplet comparisons. These approaches typically assume i.i.d. triplets with potentially noisy measurements. In our work, we consider the signals are drawn i.i.d but the agent decides to provide feedback on informative ones similar to active learning. To the best of our understanding, the case of constructive triplets hasn't been explored for covariance estimation, albeit the decision on information in our interaction protocol comes from the domain of machine teaching where typically there is a teaching agent that provides helpful examples to a learner~(see \cite{zhu2018overviewmachineteaching} for additional references therein). 
Learning a feature matrix (for a dictionary) up to normal transformation can be viewed through two established frameworks: covariance estimation~\citep{Chen2013ExactAS,voroninski} and learning linear distances such as Mahalanobis matrices~\citep{kulis_ml}. While these frameworks traditionally rely on exact or noisy measurements, our work introduces a distinct mechanism based solely on relative feedback, aligning more closely with the semantic structure of Mahalanobis distances.
The study of Mahalanobis distance functions has been central to metric learning research~\citep{bellet_survey,kulis_ml}, encompassing both supervised approaches using class labels~\citep{LMNN,Xing2002DistanceML} and unsupervised methods such as LDA~\citep{LDA} and PCA~\citep{PCA}. \citet{distance_metric_relative} and~\citet{Kleindessner2016KernelFB} have extended this framework to incorporate relative comparisons on distances. Particularly relevant to our work are studies by~\citet{distance_metric_relative} and~\citet{Mason2017LearningLM} that employ triplet comparisons, though these typically assume i.i.d. triplets with potentially noisy measurements.
Our approach differs by incorporating an active learning element: while signals are drawn i.i.d, an agent selectively provides feedback on informative instances. This constructive triplet framework for covariance estimation represents a novel direction, drawing inspiration from machine teaching, where a teaching agent provides carefully chosen examples to facilitate learning~\citep{zhu2018overviewmachineteaching}.
%Our work differs fundamentally as we study an active learning setting where the learner can query specific triplets and receive exact feedback, allowing for more efficient learning of the underlying distance function.
