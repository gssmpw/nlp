\documentclass[letterpaper,11pt]{article}
\usepackage{graphicx} % Required for inserting images
%\usepackage{icml2025}
\input{macros}
\usepackage[utf8]{inputenc}
%\usepackage[margin=.8in]{geometry}
\usepackage{tikz-3dplot}
\usepackage{algpseudocode}
\usepackage{varwidth}
\usepackage{xcolor} 
\usepackage{empheq}
\usepackage{tikz}
\usepackage{framed}
\definecolor{shadecolor}{gray}{0.9}
\def\eps{\varepsilon}
\newcommand{\curly}[1]{\left\{#1\right\}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\round}[1]{\left(#1\right)}
\newcommand{\inner}[1]{\left<#1\right>}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\def\Banach{\mathcal{B}}
\def\Hilbert{\mathcal{H}}
\def\Real{\mathbb{R}}
\def\tran{^\top}
\def\alphavec{\bm{\alpha}}
\def\betavec{\bm{\beta}}
\def\x{\bm{x}}
\def\y{\bm{y}}
\def\r{\bm{r}}
\def\Kmat{\mathbf{K}}
\setlength{\parskip}{2ex}
\def\prox{\textsf{prox}}
\def\wb{\overline}
\usepackage{booktabs}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage[margin=.8in]{geometry}
\usepackage{xcolor} 
\usepackage{empheq}
\usepackage{framed}
\definecolor{shadecolor}{gray}{0.9}
\def\eps{\varepsilon}
\def\Banach{\mathcal{B}}
\def\Hilbert{\mathcal{H}}
\def\Real{\mathbb{R}}
\def\tran{^\top}
\def\alphavec{\bm{\alpha}}
\def\betavec{\bm{\beta}}
\def\x{\bm{x}}
\def\y{\bm{y}}
\def\r{\bm{r}}
\def\Kmat{\mathbf{K}}
\setlength{\parskip}{2ex}
\def\prox{\textsf{prox}}
\def\wb{\overline}
\usepackage{booktabs}
%\usepackage{natbib}
%\hypersetup{hidelinks,colorlinks,citecolor=blue}

\def\mc{\mathcal}
\def\mf{\mathfrak}
\def\msf{\mathsf}
\def\mrm{\mathrm}

\usetikzlibrary{calc, shadings} 
\usetikzlibrary{positioning,arrows.meta}


\title{The Complexity of Learning Sparse Superposed Features with Feedback}
\author{Akash Kumar\\\small{Department of Computer Science and Engineering}\\ \small{University of California-San Diego}\\ \texttt{akk002@ucsd.edu}}
\date{}
\usepackage[round]{natbib}
%\hypersetup{hidelinks,colorlinks,citecolor=red}

\def\mc{\mathcal}
\def\mf{\mathfrak}
\def\msf{\mathsf}
\def\mrm{\mathrm}
\usepackage{verbatim}
% \documentclass{article}
% \usepackage[accepted]{icml2025}
% \usepackage{graphicx} % Required for inserting images
% \input{macros}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{hyperref}
\hypersetup{hidelinks,colorlinks,citecolor=blue}
% \usepackage[utf8]{inputenc}
% %\usepackage[margin=1in]{geometry}
% \usepackage{tikz-3dplot}
% %\usepackage{algpseudocode}
% \usepackage{varwidth}
% \usepackage{xcolor} 
% \usepackage{empheq}
% \usepackage{tikz}
% \usepackage{framed}
% \definecolor{shadecolor}{gray}{0.9}
% \def\eps{\varepsilon}
% \newcommand{\curly}[1]{\left\{#1\right\}}
% \newcommand{\set}[1]{\left\{#1\right\}}
% \newcommand{\abs}[1]{\left|#1\right|}
% \newcommand{\round}[1]{\left(#1\right)}
% \newcommand{\inner}[1]{\left<#1\right>}
% \newcommand{\norm}[1]{\left\|#1\right\|}
% \newcommand{\red}[1]{\textcolor{red}{#1}}
% \def\Banach{\mathcal{B}}
% \def\Hilbert{\mathcal{H}}
% \def\Real{\mathbb{R}}
% \def\tran{^\top}
% \def\alphavec{\bm{\alpha}}
% \def\betavec{\bm{\beta}}
% \def\x{\bm{x}}
% \def\y{\bm{y}}
% \def\r{\bm{r}}
% \def\Kmat{\mathbf{K}}
% \setlength{\parskip}{2ex}
% \def\prox{\textsf{prox}}
% \def\wb{\overline}
% \usepackage{booktabs}

% \newcommand{\dataset}{{\cal D}}
% \newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
% %\usepackage[margin=1in]{geometry}
% \usepackage{xcolor} 
% \usepackage{empheq}
% \usepackage{framed}
% \definecolor{shadecolor}{gray}{0.9}
% \def\eps{\varepsilon}
% \def\Banach{\mathcal{B}}
% \def\Hilbert{\mathcal{H}}
% \def\Real{\mathbb{R}}
% \def\tran{^\top}
% \def\alphavec{\bm{\alpha}}
% \def\betavec{\bm{\beta}}
% \def\x{\bm{x}}
% \def\y{\bm{y}}
% \def\r{\bm{r}}
% \def\Kmat{\mathbf{K}}
% \setlength{\parskip}{2ex}
% \def\prox{\textsf{prox}}
% \def\wb{\overline}
% \usepackage{booktabs}
% \usepackage{natbib}
% %\hypersetup{hidelinks,colorlinks,citecolor=blue}

% \def\mc{\mathcal}
% \def\mf{\mathfrak}
% \def\msf{\mathsf}
% \def\mrm{\mathrm}

% \usetikzlibrary{calc, shadings} 
% \usetikzlibrary{positioning,arrows.meta}


% %\title{On query complexity of learning a Mahalanobis distance function with a teacher}
% %\author{}
% %\date{}
% \usepackage{natbib}
% %\hypersetup{hidelinks,colorlinks,citecolor=red}

% \def\mc{\mathcal}
% \def\mf{\mathfrak}
% \def\msf{\mathsf}
% \def\mrm{\mathrm}
% \usepackage{verbatim}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Learning Sparse Superposed Features with Feedback}

\begin{document}
\maketitle
%\twocolumn[
%\icmltitle{The Complexity of Learning Sparse Superposed Features with Feedback}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Akash Kumar}{yyy}
% %\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% %\icmlauthor{Firstname4 Lastname4}{sch}
% % \icmlauthor{Firstname5 Lastname5}{yyy}
% % \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% % \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% % \icmlauthor{Firstname8 Lastname8}{sch}
% % \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% \icmlauthor{}{sch}
% \icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of Computer Science, University of California-San Diego, La Jolla, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
% ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% \section{Problems: Learning a metric}
\begin{abstract}
%In machine learning, the predictive power of a method hinges on the extraction of meaningful features from the ambient sample space. 
The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative \tt{triplet comparisons}. These features may represent various constructs, including dictionaries in LLMs or components of a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained on Large Language Models.%To complement our theoretical findings, we conduct experiments on feature retrieval from models trained via Recursive Feature Machines and dictionaries derived from sparse autoencoders (SAEs) trained for Large Language Models. 

%Deep networks' success fundamentally relies on their ability to capture latent features within representation spaces. We investigate whether these learned features can be efficiently recovered through relative triplet comparisons provided by an agent, such as a large language model (LLM). Our framework applies broadly to different feature representations, from LLM dictionaries to Mahalanobis distance matrices, unifying their treatment through feature matrix learning.
%We analyze the feedback complexity of learning feature matrices in sparse settings, establishing two key results. First, we prove tight bounds for scenarios where the agent can construct activations freely. Second, we demonstrate strong upper bounds when the agent must rely on distributional feedback under sparsity constraints. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained on Large Language Models.
%\akash{put github if possible}
\end{abstract}
\input{intro}
%\input{problem_setup}
%\input{sample_complexity}
\input{related_work}
\input{problem_setup}
%\input{featurelearning}
\input{maha_function}
\input{experiments}
\section*{Acknowledgments}
Author thanks the National Science Foundation for support under grant IIS-2211386 in duration of this project. Author thanks Sanjoy Dasgupta (UCSD) for helping to develop the preliminary ideas of the work. Author thanks Geelon So (UCSD) for many extended discussions on the project. Author also thanks Mikhail (Misha) Belkin (UCSD) and Enric Boix-Adser√† (MIT) for a helpful discussion during a visit to Simons Insitute. The general idea of writing was developed while the author was visiting Simons Insitute (UC Berkeley) for a workshop for which the travel was supported by the National Science Foundation (NSF) and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and \#814639.
\newpage
%\input{teaching_treemetric}
%\input{teaching_graphmetric}
%\input{teaching_smoothmetric}
%\nocite*
\bibliographystyle{plainnat}
\bibliography{ref}
\appendix
\onecolumn
\input{app_general_results}
\input{app_results}
\input{app_constructsparse}
\input{app_samplingcase}
\input{app_experiments}




%\input{app_mahalanobis}
%\input{oblivious_teaching_maha}

\end{document}
