
% \section{Geometry of Feature Learning with Feedback: Reduction to Pairs}

% First, we note that using triplet comparisons alone learner can't find a fixed target matrix $\pphi^*$ because for any $\lambda > 0$ and $(x,y,z) \in \cV^3$
% \begin{gather*}
%     (x-y)^{\top}\pphi^*(x-y) \ge (x-z)^{\top}\pphi^*(x-z)\\ \implies (x-y)^{\top}(\lambda \pphi^*)(x-y) \ge (x-z)^{\top}(\lambda \pphi^*)(x-z)
% \end{gather*}

% Thus, we can only hope to find $\pphi^*$ up to a positive linear scaling. Thus, we define a linear scaling relation $\sim_{R_l}$ on the space $\cM_{\mathsf{F}}$ as follows:
% \begin{align*}
%    \textnormal{for any } d_M, d_{M'} \in \cM_{\mathsf{F}}, d_M \sim_{R_l} d_{M'} \textnormal{ iff }  M = \gamma \cdot M' \textnormal{ for } \gamma > 0
% \end{align*}
% In the rest of the discussion, we study the oblivious teaching complexity of metric learning of mahalanobis distance metric wrt to the linear scaling relation $\sim_{R_l}$. 

% In the rest of the section, we follow the notations as mentioned below:
%\sanjoy{I didn't get far in sections 2-3 because of high-level issues (see my comments). Can you fix these things and then I will go through these two sections more carefully.}
%\sanjoy{What is the formal model -- what problem is being solved?}
%For a given matrix $M \in \reals^{d\times d}$ and a vector $x \in \reals^d$, define an inner product $\inner{xx^{\top}, M} := x^{\top} Mx$. We denote the set of symmetric matrices as $\sf symm(\reals^{p \times p})$.

%\sanjoy{For a Mahalanobis metric it is essential for the matrix to be positive semidefinite. This should be made clear in the definitions and will be part of the proofs.}

%\begin{lemma}
%    Any symmetric matrix of dimension $p \times p$ of rank-1 such that $M_{1,1}$ is positive can be written as
%    \begin{align}
%        M = xx^{\top}
%    \end{align}
%    for $x \in \reals^n$.
%\end{lemma}
%\begin{proof}
%    Wlog assume that $M_{1,1}$ is positive otherwise
%    Since $M$ is rank-1 every column vector $M_i$ is a linear scaling of any other column $M_j$ for $i \neq j$, i.e. there exists $\lambda_{j} \in \reals$ such that $M_1 = \lambda_{j}M_j$. But $M$ is symmetric and thus $R_1 = \lambda_{j}R_j$ for each $R_i = C_i$.
%    \begin{align*}
%    M = \begin{bmatrix}
%  M_{1,1} & \lambda_2 M_{1,1}& \ldots & \lambda_n M_{1,1} \\
%  \lambda_2 M_{1,1} & \lambda_2^2 M_{1,1}& \ldots & \lambda_2 \lambda_n M_{1,1}\\
%  \vdots & \vdots & \vdots & \vdots\\
%  \lambda_n M_{1,1}& \lambda_n\lambda_2M_{1,1}& \ldots & \lambda_n^2 M_{1,1}
%\end{bmatrix}
%\end{align*}
%Now, $M_{1,1}$ is positive thus we can write $M_{1,1} = (\sqrt{M_{1,1}})^2$. Thus, we  can write
%\begin{align}
%    M = \paren{\sqrt{M_{1,1}}, \lambda_2\sqrt{M_{1,1}},\ldots, \lambda_n\sqrt{M_{1,1}}}^{\top} \paren{\sqrt{M_{1,1}}, \lambda_2\sqrt{M_{1,1}},\ldots, \lambda_n\sqrt{M_{1,1}}} =: xx^{\top}
%\end{align}
%\end{proof}

%\sanjoy{How is this related to the problem we are trying to solve? Should give a reduction.}
%Thus, if we think of $M$ as a vector then $y_iy_i^{\top} - z_iz_i^{\top}$ lies in the orthogonal complement as vectors.
\iffalse
\subsection{A negative result on $c$-approximate teaching}
\akash{the assumption of PSD might not hold for any feature matrix, e.g. dictionary learning\\
Another concern is "assuming LRH and the features are presented as a matrix, does it make sense to consider this interactive setting where a powerful teacher is providing samples to a learner/agent/system."}


Let's consider a $c$-approximate version of the learning problem, for $c \geq 1$. Let $d_M$ be the Mahalanobis distance induced by matrix $M$. If the target is $\pphi^*$ then the $c$-approximate problem is to find a matrix $\pphi \succeq 0$ such that $d_M(x,x') \geq d_M(x,x'')$ whenever $d_{\pphi^*}(x,x') \geq c \cdot d_{\pphi^*}(x,x'')$. What is the sample complexity of this relaxed learning objective? Here's one possible notion: we say $M, M'$ are $c$-close if there exists $\lambda > 0$ such that
$$ \frac{1}{c} \leq \lambda \cdot \frac{u^T M' u}{u^T \pphi u} \leq c$$
for all $u \neq 0$.

\textcolor{red}{Can simply argue that the triplet queries are information-theoretically weak for retrieval with non-orthogonal pairs of rank-1 matrices. This can be used to get rid of the discussion above.}
\fi


\iffalse
\begin{figure*}[t] % 'htbp' suggests LaTeX to place the figure here, top, bottom, or on a special page
    \centering % Centers the entire figure
    % First Subfigure
    \begin{subfigure}[b]{0.3\textwidth} % Adjust the width as needed
        \centering % Centers the subfigure within its minipage
        \includegraphics[width=\textwidth]{ICML'25/Images/ground_mono.png} % Includes the first image
        \caption{Matrix $\pphi^*$ trained with RFM} % Sub-caption for the first image
        \label{fig:sub1} % Label for referencing the first subfigure
    \end{subfigure}
    \qquad
    %\hfill % Adds horizontal space between the subfigures
    % Second Subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ICML'25/Images/feedback_mono.png}
        \caption{Approximation with feedback}
        \label{fig:sub2}
    \end{subfigure}
    \caption{ In this setting, we consider monomial regression where we sample random vectors of dimension 10 $z \sim \cN(0, .5 \mathbb{I}_{10})$} of dimension 10 and compute a target function $f^*(z) = z_0z_1\mathbf{1}(z_5 > 0)$. We train a kernel machine using RF\pphi (recursive feature machine), of the form $\hat{f}_M(z) = \sum_{y_i \in \cD_{\sf{train}}} a_i \cdot K_M(y_i,z)$, for 5 iterations on 5000 sampled training points and obtain a feature matrix in (a) above. Each iteration of update for $M$ is based on gradient computation of the form $\pphi = \sum_{z \in \cD_{\sf{train}}} (\frac{\partial \hat{f}_{M}}{\partial z })(\frac{\partial \hat{f}_{M}}{\partial z})^\top$. 
    On the right-hand side in (b), the agent provides (at most) 10 constructive feedbacks (rank of $\pphi^*$ is 4) to teach $\pphi^*$. We visualize the oblivious solution learned as $\hat{M}$. MSE on 4000 test samples: with the ground truth matrix-\textbf{0.0022}, and with the feature matrix with feedback-\textbf{0.00219}.
    
    % Main caption for the entire figure
    \label{fig:main} % Label for referencing the entire figure
\end{figure*}
\fi 
\iffalse
\section{Sparse Feature Learning with Constructive Feedback}

In this section, we show how an agent can provide constructive feedbacks to teach a given feature matrix. We allow arbitrary construction of activations from the ambient space of activations, i.e. $\reals^p$.

\paragraph{Reduction to pairs}
Now, we would argue that the general triplet comparisons provided by the agent can be simplified to pair comparisons of the form $(0, y,z) \in \cV^3$, denoted as $(y,z)$ for simplification, for strict equality constraint in \algoref{alg: main}.%\eqnref{eq: sol}. 


\begin{lemma}\label{lem: reduction}
    Consider a representation space $\reals^p$ and a feature family $\cM_{\mathsf{F}}$. Fix a target feature matrix $\pphi^*$ for oblivious learning with feedbacks. If there exists a feedback set $\cF(\reals^p, \cM_{\mathsf{F}}) = \curlybracket{(x,y,z) \in \reals^{3p}\,|\, (x-y)^{\top}\pphi^*(x-y) \ge (x-z)^{\top}\pphi^*(x-z)} $, such that any $\pphi' \in \textsf{VS}(\cF, \cM_{\mathsf{F}})$ is feature equivalent to ${\pphi^*}$ then there exists a feedback set $\cF' = \curlybracket{(y',z') \in \reals^{2p}\,|\, y'^\top{\pphi^*}y' = z'^\top{\pphi^*}z'}$ of pairs such that $\pphi' \in \textsf{VS}(\cF',\cM_{\mathsf{F}})$.
\end{lemma}

\begin{proof}
    WLOG assume that for all $(x,y,z) \in \cF(\reals^{p}, \cM_{\mathsf{F}})$ $x \neq z$. Now, consider the following: if for $(x,y,z) \in \cF(\cV, \cM_{\mathsf{F}})$ $(x-y)^\top{\pphi^*}(x-y) = (x-z)^\top{\pphi^*}(x-z)$ then $(x-y, x-z)$ satisfy the same equality wrt $\pphi^*$.

    Furthermore, if for $(x,y,z) \in \cF(\cV, \cM_{\mathsf{F}})$ $(x-y)^\top{\pphi^*}(x-y) > (x-z)^\top{\pphi^*}(x-z)$ then there exists $\gamma > 0$ such that
       \begin{align*}
           &(x-y)^{\top}\pphi^*(x-y) = \gamma + (x-z)^{\top}\pphi^*(x-z)
       \end{align*}
       Since $x \neq z$ we can rewrite $\gamma$ as a scalar multiple of $(x-z)^{\top}\pphi^*(x-z)$ and thus 
       \begin{align*}
           (x-y)^{\top}\pphi^*(x-y) = (1 + \lambda) (x-z)^{\top}\pphi^*(x-z)
       \end{align*}
       Hence, $(x - y, (1 + \lambda)(x-z))$ satisfy the equality constraint wrt $\pphi^*$.
       
   So, we have provided a procedure to translate any triplet comparison in $\cF(\reals^{p}, \cM_{\mathsf{F}})$ to a form as specified in $\cF' = \curlybracket{(y',z') \in\cV^2\,|\, y'^\top{\pphi^*}y' = z'^\top{\pphi^*}z'}$. As the equations in $1.$ and $2.$ are agnostic to a positive linear scaling thus we have: if $\pphi' \in \cF(\reals^{p}, \cM_{\mathsf{F}})$ such that $\pphi'$ is equivalent to ${\pphi^*}$ then $\pphi' \in \textsf{VS}(\cF', \cM_{\mathsf{F}})$.
\end{proof}

\lemref{lem: reduction} implies that if there are triplet comparisons to \algoref{alg: main} then there are also pair comparisons that satisfy the following equation:
\begin{subequations}
\begin{gather}
    \pphi' = \lambda\cdot d_{\pphi^*}, \text{ for some }\lambda > 0\\
    \pphi'  \in \curlybracket{\pphi \in \cM_{\mathsf{F}}\,|\, \forall (y,z) \in \cF(\reals^{2p},\cM_{\mathsf{F}},\pphi^*),\, y^{\top}\pphi y = z^{\top}\pphi z} \label{eq: redsol}
\end{gather}
\end{subequations}
where $\cF(\reals^p,\cM_{\mathsf{F}},\pphi^*)$ is the feedback set that satisfy equality constraints with $\pphi^*$.

In the rest of this section, we consider the reformulation of oblivious learning of a feature matrix with pair comparisons.

\paragraph{Geometry of Feature Learning} For a given pair $(y,z)$ and a matrix $\pphi$, if we have equality constraint $y^{\top}\pphi y = z^{\top}\pphi z$ then with some rewriting we observe that
\begin{align*}
    y^{\top}\pphi y = z^{\top}\pphi z &\iff \inner{\pphi,\, yy^{\top} - zz^{\top}} = 0\\ &\iff \pphi \idot (yy^{\top} - zz^{\top}) = 0
\end{align*}
Thus, $(yy^{\top} - zz^{\top})$ is orthogonal to $\pphi $. So, we can pose the learning problem of \eqnref{eq: redsol} for a set of pairs of feedbacks $\cF(\cV,\cM_{\mathsf{F}},\pphi^*) := \curlybracket{(y_i,z_i)}_{i=1}^k $ corresponding to a target feature matrix $\pphi^*$ as follows:
\begin{align}
  \forall (y,z) \in \cF(\reals^p,\cM_{\mathsf{F}},\pphi^*), \quad \pphi \idot (yy^{\top} - zz^{\top})  = 0 \label{eq: orthosat}
\end{align}
Essentially, the learner obliviously picks a matrix $\pphi'$ that satisfy \eqnref{eq: orthosat}. Note that this has a nice geometric picture to this as shown in \figref{fig: geom} where $\mathcal{O}_{\pphi^*} := \curlybracket{S \in \reals^{p \times p} \,|\, \pphi^* \idot S = 0}$.
\fi
%-------------------------------------\\

%\akash{remove this if no fig}\figref{fig: geom} illustrates this geometric interpretation, where $yy^{\top} - zz^{\top} \}_{(y,z) \in \mathcal{F}}$ sits in the orthogonal complement $\mathcal{O}_{\pphi^*}$.
\section{Sparse Feature Learning with Constructive
Feedback}\label{sec: construct}
Here, we study the feedback complexity in the setting where agent is allowed to pick/construct any activation from $\reals^p$. 


\paragraph{Reduction to Pairwise Comparisons}
The general triplet feedbacks with potentially inequality constraints in \algoref{alg: main}
 can be simplified to pairwise comparisons with equality constraints with a simple manipulation as follows. %Specifically, triplet comparisons of the form $(0, y, z) \in \mathcal{V}^3$ can be denoted as $(y, z)$ for simplicity, particularly when enforcing strict equality constraints in \algoref{alg: main}.%\eqnref{eq:sol}.

\begin{lemma}\label{lem: reduction}
Let $\pphi^* \in \mathcal{M}_{\mathsf{F}}$ be a target feature matrix in representation space $\reals^p$ used for oblivious learning. Given a feedback set \vspace{-1.5mm}
\[
\mathcal{F} = \left\{ (x, y, z) \in \reals^{3p} \,\big|\, (x - y)^{\top} \pphi^* (x - y) \geq (x - z)^{\top} \pphi^* (x - z) \right\},
\]
such that any $\pphi' \in \textsf{VS}(\mathcal{F}, \mathcal{M}_{\mathsf{F}})$ is feature equivalent to $\pphi^*$, there exists a pairwise feedback set \vspace{-1.5mm}
\[
\mathcal{F}' = \left\{ (y', z') \in \reals^{2p} \,\big|\, y'^{\top} \pphi^* y' = z'^{\top} \pphi^* z' \right\}
\]
such that $\pphi' \in \textsf{VS}(\mathcal{F}', \mathcal{M}_{\mathsf{F}})$.\vspace{-3mm}
\end{lemma}
\begin{proof}
WLOG, assume $x \neq z$ for all $(x, y, z) \in \mathcal{F}$. For any triplet $(x, y, z) \in \mathcal{F}$: \textbf{Case} (i): If $(x - y)^{\top} \pphi^* (x - y) = (x - z)^{\top} \pphi^* (x - z)$, then $(x - y, x - z)$ satisfies the equality. \textbf{Case} (ii): If $(x - y)^{\top} \pphi^* (x - y) > (x - z)^{\top} \pphi^* (x - z)$, then for some $\lambda > 0$:
\[
(x - y)^{\top} \pphi^* (x - y) = (1 + \lambda)(x - z)^{\top} \pphi^* (x - z)
\]
implying $(x - y, \sqrt{1 + \lambda}(x - z))$ satisfies the equality.

Thus, each triplet in $\mathcal{F}$ maps to a pair in $\mathcal{F}'$, preserving feature equivalence under positive scaling.
\end{proof}
This implies that if triplet comparisons are used in \algoref{alg: main}, equivalent pairwise comparisons exist satisfying:\vspace{-5mm}
\begin{subequations}\label{eq: redsol}
\begin{align}
    \pphi' &= \lambda \cdot {\pphi^*}, \quad \lambda > 0, \label{eq:reduction_scaling} \\
    \pphi' &\in \left\{ \pphi \in \mathcal{M}_{\mathsf{F}} \,\big|\, \forall (y, z) \in \mathcal{F}', \, y^{\top} \pphi y = z^{\top} \pphi z \right\}. \label{eq:reduction_solution}
\end{align}
\end{subequations}
Now, we show a reformulation of the oblivious learning problem for a feature matrix using pairwise comparisons that provide a unique geometric interpretation.
\allowdisplaybreaks
%\subsection{Geometric Perspective on Feature Learning}
Consider a pair $(y, z)$ and a matrix $\pphi$. An equality constraint implies
\begin{align*}
    y^{\top} \pphi y = z^{\top} \pphi z &\iff \langle \pphi,\, yy^{\top} - zz^{\top} \rangle = 0 %\\
    %&\iff \pphi \cdot (yy^{\top} - zz^{\top}) = 0,
\end{align*}
where $\langle \cdot, \cdot \rangle$ denotes the Frobenius inner product. This equivalence indicates that the matrix $(yy^{\top} - zz^{\top})$ is orthogonal to $\pphi$ in the Frobenius inner product space. Now, given a set of pairwise feedbacks 
\[
\mathcal{F}(\mathbb{R}^{p}, \mathcal{M}_{\mathsf{F}}, \pphi^*) = \{ (y_i, z_i) \}_{i=1}^k
\]
corresponding to the target feature matrix $\pphi^*$, the learning problem defined by \eqnref{eq:reduction_solution} can be formulated as:
\begin{align}\label{eq: orthosat}
    \forall (y, z) \in \mathcal{F}(\mathbb{R}^p, \mathcal{M}_{\mathsf{F}}, \pphi^*), \quad \langle \pphi,\, yy^{\top} - zz^{\top} \rangle = 0. %\\
\end{align}
Essentially, the learner aims to select a matrix $\pphi$ that satisfies the condition in \eqnref{eq: orthosat}.
Geometrically, the condition in \eqnref{eq: orthosat} implies that any solution $\pphi$ should annihilate the subspace of the orthogonal complement that is spanned by the matrices $\{ yy^{\top} - zz^{\top} \}_{(y,z) \in \mathcal{F}}$. Formally, this complement is defined as:\vspace{-1mm}
\[
\mathcal{O}_{\pphi^*} := \left\{ S \in \symm \,\bigg|\, \langle\pphi^*, S\rangle = 0 \right\}.\vspace{-3mm}
\]
% \begin{figure}[!]
% \begin{center}
% \tdplotsetmaincoords{60}{120}
% \begin{tikzpicture}[tdplot_main_coords]
% % Draw the plane
% \fill[fill=blue!10] (0,0,0) -- (3,-1,0) -- (4,1,0) -- (1,2,0) -- cycle;
% % Define the center point on the plane
% \coordinate (center) at (2,0.5,0);
% % Draw the arrows
% \draw[->, line width=.5mm] (center) -- ++(0,0,2.5) node[above, anchor=south] {$\pphi^*$};
% %\draw[->, thick] (0,0) -- (3,-1) node[anchor=north west] {$\mathbb{R}^{p \times p}$};
% % \draw[->, thick] (0,0) -- (4,1);
% % Draw the dashed arrows from the center
% \draw[dashed, ->] (center) -- ++(-1,-1,0) node[above, anchor=south] {\tiny{$yy^{\top} - zz^{\top}$}};
% \draw[dashed, ->] (center) -- ++(.7,-.8,0) node[below, anchor=east] {\tiny{$y'y'^{\top} - z'z'^{\top}$}};
% % Add the label on the plane
% \node at (4.2,0.5,0) {$\mathcal{O}_{\pphi^*}$};
% \node at (3,3,3) {$\mathbb{R}^{p \times p}$};
% \end{tikzpicture}
% \end{center}
% \caption{Geometry of solving \eqnref{eq: redsol}}
%     \label{fig: geom}
% \end{figure}
\subsection{Constructive feedbacks: Worst-case lower bound}
To learn a symmetric PSD matrix, learner needs at most $p(p+1)/2$ constraints for linear programming corresponding to the number of degrees of freedom. So, the first question is are there pathological cases of feature matrices in $\cM_{\sf{F}}$ which would require at least $p(p+1)/2$ many triplet feedbacks in \algoref{alg: main}. This indeed is the case, if a target matrix $\pphi^* \in \symmp$ is full rank.

%In this subsection, we study the \tt{the minimal size of $\cF(\cV,\cM_{\mathsf{F}})$ that exactly fixes a target matrix $\pphi^*$ (upto linear scaling relation $\sim_{R_l}$) in \eqnref{eq: redsol}}.

In the following proposition proven in \appref{app: worstcase}, we show a strong lower bound on the worst-case $\pphi^*$ that turns out to be of order $\Omega(p^2)$. %Note that the $\dim(\symm) = \frac{p(p+1)}{2}$.

%\sanjoy{These results should incorporate the PSD constraint. They might need to be reworked slightly for that.}

\begin{proposition}\label{prop: worstcase} In the constructive setting, the worst-case feedback complexity of the class $\cM_{\sf{F}}$ with general activations
%oblivious teaching complexity of a teaching set $\cF(\cV, \cM_{\sf{F}})$ up to linear scaling relation $\sim_{R_l}$ for \eqnref{eq: redsol} 
is at the least $\paren{\frac{p(p+1)}{2} - 1}$.
\end{proposition}
\begin{proof}[Proof Outline]  As discussed in \eqnref{eq: redsol} and \eqnref{eq: orthosat}, for a full-rank feature matrix $\pphi^* \in \cM_{\sf{F}}$, the span of any feedback set $\cF$, i.e., $\sn{\curlybracket{xx^{\top} - yy^{\top}}_{(x,y) \in \cF}}$, must lie within the orthogonal complement $\mathcal{O}_{\pphi^*}$ of $\pphi^*$ in the space of symmetric matrices $\symm$. Conversely, if $\pphi^*$ has full rank, then $\mathcal{O}_{\pphi^*}$ is contained within this span. This necessary condition requires the feedback set to have a size of at least $\frac{p(p+1)}{2} - 1$, given that $\dim(\symm) = \frac{p(p+1)}{2}$.
\end{proof}
%Later in the section we will show that worst-case upper bound on oblivious teaching complexity is $\frac{p(p+1)}{2} - 1$, which is expected as there are $\frac{p(p+1)}{2}$ degree of freedom for any symmetric, postive semi-definite matrix. 
Since the worst-case bounds are pessimistic for oblivious learning of \eqnref{eq: redsol} a general question is how feedback complexity varies over the feature model $\maha$. In the following subsection, we study the feedback complexity for feature model based on the rank of the underlying matrix, showing that the bounds can be drastically reduced. 

\subsection{Feature learning of low-rank matrices}

As stated in \propref{prop: worstcase}, the learner requires at least $\frac{p(p+1)}{2} - 1$ feedback pairs to annihilate the orthogonal complement $\mathcal{O}_{\pphi^*}$. However, this requirement decreases with a lower rank of $\pphi^*$. We illustrate this in \figref{fig: monoconst} for a feature matrix $\pphi \in \reals^{10 \times 10}$ of rank 4 trained via Recursive Feature Machines~\citep{rfm}.

Consider an activation $\alpha \in \reals^p$ in the nullspace of $\pphi^*$. Since $\pphi^*\alpha = 0$, it follows that $\alpha^\top \pphi^* \alpha = 0$. Moreover, for another activation $\beta \notin \sn{\alpha}$ in the nullspace, any linear combination $a\alpha + b\beta$ satisfies
$$(a\alpha + b\beta)^\top \pphi^* (a\alpha + b\beta) = 0.\vspace{-4mm}$$

This suggests a strategy for designing effective feedback based on the kernel $\kernel{\pphi^*}$ and the null space $\nul{\pphi^*}$ of $\pphi^*$ (see \appref{app: notations} for table of notations). This intuition is formalized by the eigendecomposition of the feature matrix:\vspace{-3mm}
\begin{align}
    \pphi^* = \sum_{i=1}^r \lambda_i u_i u_i^\top, \label{eq: eigen} \vspace{-4mm}
\end{align}
where $\{\lambda_i\}$ are the eigenvalues and $\{u_i\}$ are the orthonormal eigenvectors.
Since $\pphi^* \succeq 0$ this decomposition is \tt{unique} with non-negative eigenvalues.

To teach $\pphi^*$, the agent can employ a dual approach: teaching the kernel associated with the eigenvectors in this decomposition and the null space separately. Specifically, the agent can provide feedbacks corresponding to the eigenvectors of $\pphi^*$'s kernel and extend the basis $\{u_i\}$ for the null space. We first present the following useful result (see proof in \appref{app: constub}).

\begin{lemma}\label{lem: basis}
    Let $\{v_i\}_{i=1}^r \subset \reals^p$ be a set of orthogonal vectors. Then, the set of rank-1 matrices
    \[
    \mathcal{B} := \left\{v_i v_i^{\top},\ (v_i + v_j)(v_i + v_j)^{\top}\ \bigg| \ 1 \leq i < j \leq r \right\}
    \]
    is linearly independent in the space symmetric matrices $\symm$.
\end{lemma}

Using this construction, the agent can provide feedbacks of the form $(u_i, \sqrt{c_i} y)$ for some $y \in \reals^p$ with $\pphi^* y \neq 0$ and $v_i^\top \pphi^* v_i = c_i y^\top \pphi^* y$ to teach the kernel of $\pphi^*$. For an orthogonal extension $\{u_i\}_{i=r+1}^p$ where $\pphi^* u_i = 0$ for all $i = r+1,\dots,p$, feedbacks of the form $(u_i, 0)$ suffice to teach the null space of $\pphi^*$.

This is the key idea underlying our study on feedback complexity in the general constructive setting that is stated below with the full proof deferred to \appref{app: constub} and \ref{app: constlb}.

\begin{theorem}[General Activations]\label{thm: constructgeneral}
    Let $\Phi^* \in \maha$ be a target feature matrix with $\rank{\pphi^*} = r$. Then, in the setting of constructive feedbacks with general activations, the feedback complexity has a tight bound of $\Theta\left(\frac{r(r+1)}{2} + (p - r) - 1\right)$ for \eqnref{eq: redsol}.
\end{theorem}

\begin{proof}[Proof Outline]
    As discussed above we decompose the feature matrix $\pphi^*$ into its eigenspace and null space, leveraging the linear independence of the constructed feedbacks to ensure that the span covers the necessary orthogonal complements. 
    The upper bound is established with a simple observation: $r(r+1)/2 - 1$ many pairs composed of $\cB$ are sufficient to teach $\pphi^*$ if the null space of $\pphi^*$ is known, whereas the agent only needs to provide $(p-r)$ many feedbacks corresponding to a basis extension to cover the null space, and hence the stated upper bound is achieved.
    
    The lower bound requires showing that a valid feedback set possesses two spanning properties of $\langle{xx^\top - yy^\top\rangle}$ for all $(x,y) \in \cF$: (1) it must include any $\pphi \in \mathcal{O}_{\pphi^*}$ whose column vectors are within the span of eigenvectors of $\pphi^*$, and (2) it must include any $vv^\top$ for some subset $U$ that spans the null space of $\pphi^*$ and $v \in U$.
    %if any $\pphi \in \mathcal{O}_{\pphi^*}$ and its column vectors are within the span of eigenvectors of $\pphi^*$, then $\pphi$ should be within the span $\langle{xx^\top - yy^\top\rangle}$ for $(x,y) \in \cF$, and (2) there exists a subset $U$ that spans the null space of $\pphi^*$ such that any rank-1 matrix $vv^\top$ is within $\langle{xx^\top - yy^\top\rangle}$ for $(x,y) \in \cF$ for any $v \in U$ 
    %This guarantees that the minimum number of feedback pairs meets the stated bound. 
    %\akash{complete the lower bound proof which is much more intricate}
\end{proof}

%In general, the input space $\cV \subset \reals^d$ exists in a small dimensional manifold. So, if there is a distance function which linearly transforms the space to capture the distance well, e.g. in the form of a matrix in Mahalanobis metric, we expect the rank of the matrix to be small as well. So, it is interesting to understand if the oblivious teaching can capture this small dimension with an optimistic teaching complexity. 
\iffalse
As stated in \propref{prop: worstcase}, we note that the learner needs at least $p(p+1)/2 - 1$ pairs of feedbacks to annihilate the orthogonal complement $\mathcal{O}_{\pphi^*}$. But the feedback requirement goes down if the rank is smaller. 

Consider an activation $\alpha \in \reals^p$ in the nullspace of $\pphi^*$. Note that $\pphi^*\alpha = 0$ implies $\alpha^\top \pphi^* \alpha = 0$. Furthermore, if $\beta \notin \sn{\alpha}$ is another activation in the null space then for any choice of scalars $a$ and $b$, 
$$(a\alpha + b\beta)^\top\pphi^*(a\alpha + b\beta) = 0.$$

This suggests a strategy in designing helpful feedback according to the kernel $\kernel{\pphi^*}$ and the null space $\nul{\pphi^*}$ of the matrix $\pphi^*$. 
%If we consider the orthogonal complement $\mathcal{O}_{\pphi^*}$, we might expect that if the rank of $\pphi^*$ is smaller than $p$, then the PSD cone projected onto it might be contained in a smaller subspace than $p(p+1)/2 - 1$. We show 
%\akash{provide the eigendecomposition method and also talk about the experiment!}
This intuition can be precisely captured by the eigendecomposition of a feature matrix:
\begin{align}
    \pphi^* = \sum_{i = 1}^r \lambda_i u_iu_i^\top, \label{eq: eigen}
\end{align}
where $\curlybracket{\lambda_i}$ are the set of eigenvalues and $\curlybracket{u_i}$ are the set of orthonormal eigenvectors. This decomposition is \tt{unique} for a symmetric matrix; furthermore all eigenvalues are non-negative as $\pphi^* \succeq 0$.

In order to teach $\pphi^*$, the agent could devise a dual technique of teaching the kernel corresponding to the eigenvectors in this decomposition, and the null space \tt{separately}.


We show that the agent could precisely provide feedbacks corresponding to eigenvectors for the kernel of $\pphi^*$, and an extension to the basis $\curlybracket{u_i}$ for the null space. First we note this useful result.
\begin{lemma}
    Assume $\curlybracket{v_i}_{i=1}^r \subset \reals^p$ be a set of orthogonal vectors, then the following set of rank-1 matrices
    \begin{gather*}
        \mathcal{B} := \{v_1v_1^{\top},v_2v_2^{\top}, (v_1 + v_2)(v_1 + v_2)^{\top},\ldots,\\v_rv_r^{\top}, (v_1 + v_r)(v_1 + v_r)^{\top},\ldots, (v_{r-1} + v_r)(v_{r-1} + v_r)^{\top}\}
    \end{gather*}
    are linearly independent in the space of symmetric matrices $\symm$.
\end{lemma}
%\akash{why can't we design wrt to any basis; maybe proof is not clear!!}
%\begin{lemma}
%    Let $M$ be a positive definite matrix. Then, the lower bound on the teaching complexity is $\frac{p(p+1)}{2} - 1$.
%\end{lemma}
Using this construction, agent can devise feedbacks of the form $(u_i, \sqrt{c_i} y)$ for some $y \in \reals^p$ such that $\pphi^*y \neq 0$ and $v_i^\top \pphi^*v_i = c_iy^\top \pphi^*y$ to teach the kernel of $\pphi^*$; whereas for any orthogonal extension $\curlybracket{u_i}_{i = r+1}^p$ such that for all $i = r+1, r+2,\ldots,p$ $\pphi^*u_i = 0$ feedbacks of the form $(u_i,0)$ suffice to teach the null space of the feature matrix $\pphi^*$.

We provide the  concrete statement on the feedback complexity in the general constructive setting below.

\begin{theorem}[general activations]\label{thm: constructgeneral}
    Assume that $\Phi^* \in \maha$ be a target feature matrix with $\rank{\pphi^*} = r$. %Suppose a superagent can design pairs of activations from the ambient space of representations $\reals^p$. 
    Then, in the setting of constructive feedbacks with general activations, the feedback complexity has a tight bound of $\Theta\paren{\frac{r(r+1)}{2} + (p - r) - 1}$ for \eqnref{eq: redsol}.
\end{theorem}
\begin{proof}[Proof Outline] The key aspect of the analysis is to decompose the 
    
\end{proof}
\fi
\paragraph{Learning with sparse activations} In the discussion above, we demonstrated a strategy for reducing the feedback complexity when general activations are allowed. Now, we aim to understand how this complexity changes when activations are $s$-sparse (see \defref{def: sparse}) for some $s < p$. Notably, there exists a straightforward construction of rank-1 matrices using a sparse set of activations.

Consider this sparse set of activations ${B}$ consisting of $\frac{p(p+1)}{2}$ items in $\reals^p$:
\begin{align}
  {B} = \{e_i \mid 1 \leq i \leq p\} \cup \{e_i + e_j \mid 1 \leq i < j \leq p\},  \label{eq: sparsebasis}
\end{align}
where $e_i$ is the $i$th standard basis vector. Using a similar argument to \lemref{lem: basis}, it is straightforward to show that the set of rank-1 matrices
$$\cB_{\sf{sparse}} := \curlybracket{uu^\top \mid u \in {B}}$$
is linearly independent in the space of symmetric matrices $\symm$ and forms a basis. Moreover, every activation in $B_{\sf{ext}}$ is at most 2-sparse (see \defref{def: sparse}). With this, we state the main result on learning with sparse constructive feedback here.

\begin{theorem}[Sparse Activations]\label{thm: constructsparse}
    Let $\pphi^* \in \maha$ be the target feature matrix. 
    If an agent can construct pairs of activations from a representation space $\reals^p$, then the feedback complexity of the feature model $\maha$ with 2-sparse activations is upper bounded by $\frac{p(p+1)}{2}$.
    %superagent can teach $\pphi^*$ up to a sign and rotational transformation.
\end{theorem}
%\looseness-1\vfill\vspace*{-10mm}
\allowdisplaybreaks
\tt{Remark}: While the lower bound from \thmref{thm: constructgeneral} applies here, sparse settings may require even more feedbacks. Consider a rank-1 matrix $\pphi^* = vv^\top$ with $\text{sparsity}(v) = p$. By the Pigeonhole principle, representing this using $s$-sparse activations requires at least $(p/s)^2$ rank-1 matrices. Thus, for constant sparsity $s = O(1)$, we need $\Omega(p^2)$ feedbacksâ€”implying sparse representation of dense features might not exploit the low-rank structure to minimize feedbacks. %Furthmore, the upper bound
%$r = 1$,  \akash{check if something better can be said}
\iffalse
\begin{theorem}\label{thm: obv}
    Consider an input space $\cV \subset \reals^d$, and a metric model $\maha$.
    Consider a symmetric, positive semi-definite matrix $\pphi^*$ of rank $r \le d$. Assume that $d_{\pphi^*}$ be the target metric for oblivious teaching in \eqnref{eq: sol}. Then, the teaching complexity $TC_O(\cV, \maha, d_{\pphi^*})$ has the tight bound of $\paren{\frac{r(r+1)}{2} + (d - r) - 1}$.
\end{theorem}
\begin{theorem}\label{thm: constructive}
    Assume that $\Phi^* \in \reals^{d \times p}$ be a target feature matrix. 
    Consider a representation space $\cV \subseteq \reals^p$. Suppose a superagent can design pairs of activations from a representation space $\cV \subseteq \reals^p$. Then, we have the following guarantee on the number of feedbacks required:
    \begin{enumerate}
        \item \sf{w}/ sparsity: With a tight bound of $\paren{\frac{r(r+1)}{2} + (p - r) - 1}$, superagent can teach $\Phi^*$ up to rotational transformation.
        \item sparsity: With an upper bound of $\frac{p(p+1)}{2}$, a supergent can design pairs of sparse activations to teach $\Phi^*$ up to rotational transformation.
    \end{enumerate}
\end{theorem}

Now, it is clear that a teaching set of size $\frac{r(r+1)}{2} - 1 + (d -r)$ is sufficient to teach $\pphi^*$. In the following, we show the teaching set size of $\frac{r(r+1)}{2} - 1 + (d -r)$ is also necessary for oblivious teaching.

\begin{lemma}\label{lemma: lowerbound}
    Consider an input space $\cV \subset \reals^d$, and a metric model $\maha$.
    Consider a symmetric, positive semi-definite matrix $\pphi^*$ of rank $r \le d$. Assume that $d_{\pphi^*}$ be the target metric for oblivious teaching in \eqnref{eq: sol}. Then, the teaching complexity $TC_O(\cV, \maha, d_{\pphi^*})$ has a lower bound of $\paren{\frac{r(r+1)}{2} + (d - r) - 1}$.
\end{lemma}
\fi

\section{Sparse Feature Learning with Sampled Feedback}\label{sec: sample}

\begin{algorithm}[t]
\caption{Feature learning with sampled representations}
\label{alg: randmaha}
\textbf{Given}: Representation space $\cV  \subset \reals^p$, Distribution over representations $\cD_\cV$, Feature family $\cM_{\mathsf{F}}$. \vspace{2mm}\\
\textit{In batch setting:}%\vspace{1mm}
\begin{enumerate}
    \item Agent receives sampled representations $\cV_n \sim \cD_{\cV}$.\vspace{-2mm}
    \item Agent picks pairs $\cF(\cV_n,\pphi^*) =$ \vspace{-2mm}$$\curlybracket{(x,\sqrt{\lambda_{x}}y)\,|\, (x,y) \in \cV_n^2,\, x^{\top}\pphi^*x = \lambda_{x}\cdot y^{\top}\pphi^*y}\vspace{-2mm}$$ 
    \item Learner receives $\cF$; and obliviously picks a feature matrix $\pphi \in \cM_{\mathsf{F}}$ that satisfy the set of constraints in $\cF(\cV_n,\pphi^*)$\vspace{-2mm}
    \item Learner outputs $\pphi$.\vspace{-2mm}
\end{enumerate}
\end{algorithm}

%In the previous section, we assumed that the teaching agent can arbitrarily construct feature activations $\alpha \in \reals^p$ to provide comparisons. 
In general, the assumption of constructive feedback may not hold in practice, as ground truth samples from nature or induced representations from a model are typically independently sampled from the representation space. The literature on Mahalanobis distance learning and dictionary learning has explored distributional assumptions on the sample/activation space~\citep{Mason2017LearningLM,Gribonval2014SparseAS}.

In this section, we consider a more realistic scenario where the agent observes a set of representations/activations $\cV_n := \{\alpha_1, \alpha_2, \ldots, \alpha_n\} \sim \cD_{\cV}$, with $\cD_{\cV}$ being an unknown measure over the continuous space $\cV \subseteq \reals^p$. With these observations, the agent designs pairs of activations to teach a target feature matrix $\pphi^* \in \symmp$.

As demonstrated in \lemref{lem: reduction}, we can reduce inequality constraints with triplet comparisons to equality constraints with pairs in the constructive setting. However, when the agent is restricted to selecting activations from the sampled set $\cV_n$ rather than arbitrarily from $\cV$, this reduction no longer holds. This is evident from the following observation: let $\alpha, \beta \sim \text{iid} \ \cD_{\cV}$ and let $\pphi^* \neq 0$ be a non-degenerate feature matrix. Then,\vspace{-1.5mm}
\[
\alpha^{\top}\pphi^*\alpha = \beta^{\top}\pphi^*\beta \implies \sum_{i,j} (\alpha_i \alpha_j - \beta_i \beta_j) \pphi^*_{ij} = 0.\vspace{-1.5mm}
\]
This equation represents a non-zero polynomial. According to Sard's Theorem, the zero set of a non-zero polynomial has Lebesgue measure zero. Therefore,\vspace{-1.5mm}
\[
\cP_{(\alpha,\beta)}\left( \left\{ \alpha^{\top}\pphi^*\alpha = \beta^{\top}\pphi^*\beta \right\} \right) = 0.\vspace{-1mm}
\]
Given this, the agent cannot reliably construct pairs that satisfy the required equality constraints from independently sampled activations. Since a general triplet feedback only provides 3 bits of information, exact recovery up to feature equivalence is impossible. To address these limitations, we consider rescaling the sampled activations to enable the agent to design effective pairs for the target feature matrix $\pphi^* \in \cM_{\sf{F}}$.

%\akash{check if it is applicable}
%\begin{lemma}[Sard's Theorem]
%Let \( f: X \to Y \) be a smooth map of manifolds, and let \( C \) be the set of critical points of \( f \) in \( X \). Then \( f(C) \) has measure zero in \( Y \).
%\end{lemma}

\begin{figure*}[t] % 'htbp' suggests LaTeX to place the figure here, top, bottom, or on a special page
    \centering % Centers the entire figure
    % First Subfigure
    
        \includegraphics[width=1\textwidth]{sparsemono.png} % Includes the first image
    
    %\caption{ In this setting, we consider monomial regression where we sample random variables of dimension 10: $z \sim \cN(0, .5 \mathbb{I}_{10})$} and compute a target function $f^*(z) = z_0z_1\mathbf{1}(z_5 > 0)$. We train a kernel machine using RFM (recursive feature machine), of the form $\hat{f}_{\pphi}(z) = \sum_{y_i \in \cD_{\sf{train}}} a_i \cdot K_{\pphi}(y_i,z)$, for 5 iterations on 4000 sampled training points and obtain a feature matrix $\pphi^*$ in rightmost plot (Ground truth). Each iteration of RFM update for $\pphi^*$ is based on gradient computation of the form $\pphi = \sum_{z \in \cD_{\sf{train}}} (\frac{\partial \hat{f}_{\pphi}}{\partial z })(\frac{\partial \hat{f}_{\pphi}}{\partial z})^\top$. The rank of $\pphi^*$ is 4. In the next set of plots, we employ different feedback methods where a superagent/teacher provides feedbacks based on $\pphi^*$: (constructive) eigendecompostion and sparse construction; (sampling) random sampling with Gaussian distribution and sparse sampling where the probability of sparsity is .9 (sparsity: very high). Note that the three middle plots achieve the same MSE (mean squared error) as the ground truth feature matrix $\pphi^*$. But since the sparsity is pretty high, the approximated feature matrix with the same number of feedbacks: 55 is not sufficient and both the feature matrix and computed MSE are poor (see Theorem for discussion).}
    \caption{
    \textbf{Sparse sampling}: We consider the same setup as \figref{fig: monoconst} for the target function $f^*(z) = z_0 z_1 z_3 \mathbf{1}(z_5 > 0)$. In these plots, we employ sparse sampling feedback methods where an agent provides feedback based on \( \pphi^* \) with different sparsity probability (mu: probability of 0 being sampled). Thus, as mu decreases the theorized complexity of $p(p+1)/2 = 55$ obtains a close approximation of $\pphi^*$. But for $mu = .97$, the agent needs to sample more number of activations to approximate properly, i.e. from 55, 110, $\ldots$, and 1100 approximation gradually improves as shown in \thmref{thm: samplingsparse}.
    %Notably, the three central plots achieve the same Mean Squared Error (MSE) as the ground truth feature matrix \( \pphi^* \). However, due to the high sparsity, the approximated feature matrices with the same number of feedbacks (55) are insufficient, resulting in poor feature matrices and computed MSE (see Theorem for discussion).
}
    
    %On the right-hand side in (b), the teacher provides (at most) 10 constructive feedbacks (rank of $\pphi^*$ is 4) to teach $\pphi^*$. We visualize the oblivious solution learned as $\hat{M}$. MSE on 4000 test samples: with the ground truth matrix-\textbf{0.0022}, and with the feature matrix with feedback-\textbf{0.00219}.
    
    % Main caption for the entire figure
    \label{fig: monosparse} % Label for referencing the entire figure
\end{figure*}


\iffalse
\paragraph{Rescaled pairs} For a given matrix $\pphi \neq 0$ a sampled input $x \sim \cD_{\cV}$ is (almost) never orthogonal, i.e. (a.s.) $\pphi x \neq 0$. This can be used to rescale an input to construct pairs to satisfy equality constraints. In other words, there exists $\gamma, \lambda >0$ such that (assume wlog $x^{\top}\pphi x > y^{\top}\pphi y$)
\begin{align*}
    x^{\top}\pphi x = \gamma + y^{\top}\pphi y = \lambda\cdot y^{\top}\pphi y + y^{\top}\pphi y \\= (\sqrt{1 + \lambda})y^{\top}\pphi(\sqrt{1 + \lambda})y 
\end{align*}
Thus, $(x, (\sqrt{1 + \lambda})y)$ satisfy the equality constraints. With this understanding we provide a reformulation of \algoref{alg: main} into \algoref{alg: randmaha}. %\akash{In the rest of the section, we study the expected oblivious teaching complexity for  \algoref{alg: randmaha} under Lebesgue distribution $\cD_{\cV}$ over the input space $\cV$. We show tight bounds on the teaching complexity highlighting a gap between the constructive and sampled teaching scenarios.} 

%\paragraph{A noisy model of approximations} An adversarial teacher provides $(x,y,z; \delta)$ where $x,y,z \in \cV$ and $\delta \sim \cN(0, \sigma^2)$ with the equality label, i.e.
%\begin{align*}
%    (x-y)^\top \pphi^*(x-y) - (x-z)^\top \pphi^*(x-z) = \delta_{xyz} 
%\end{align*}
%In the constructive setting, we discussed concrete strategies for devising optimal teaching set, the number of triplet or pairs for oblivious teaching. In the sampled case, we want to study the size of samples $\cV_n$, both necessary and sufficient, so that teacher can devise rescaled pairs. Thus, the oblivious teaching complexity is defined as the size $|\cV_n|$ such that the learner finds $\pphi^*$ up to linear scaling relation $\sim_{R_l}$ in \algoref{alg: randmaha}, $\expctover{\cV_n}{}$
%\akash{Define teaching set }
%In the sampling case, we define the teaching complexity in terms of the size of the sampled set $\cV_n$ so that the teacher can devise a teaching set $\cF(\cV, \maha,\pphi^*)$ for a target matrix $\pphi^*$.
%\begin{definition}\label{defn: lebsample}
%    Consider an input space $\cV \subset \reals^d$. Let $\cD_{\cV}$ be a Lebesgue distribution over $\cV$ from which iid samples $\cV_n$ are selected.    Fix a constant $\epsilon > 0$ and a target matrix $\pphi^*$. We define the oblivious teaching complexity for $\epsilon$-accuracy for random samples is $n$ if the teacher provides a teaching set $\cF(\cV_n, \maha,\pphi^*)$ for \algoref{alg: randmaha} using the samples $\cV_n$ such that
    %\begin{align*}
     %   \cP_{\cV_n}(d_{M'} \in \textsf{VS}(\cF(\cV_n, \maha,\pphi^*), \maha) \textnormal{ such that } d_{M'} \sim_{R_l} d_{\pphi^*}) \ge \epsilon
    %\end{align*}
%\end{definition}
%First, we start with an upper bound on the worst-case (across the space $\maha$) oblivious teaching complexity that achieves $1$-accuracy.

%\begin{proposition}\label{prop: sampling}
%    Consider an input space $\cV \subset \reals^d$. Let $\cD_{\cV}$ be a Lebesgue distribution over $\cV$ from which iid samples $\cV_n$ are selected. Then, the worst-case oblivious teaching complexity for \algoref{alg: randmaha} has an upper bound of $O\paren{\frac{p(p+1)}{2}}$ that achieves 1-accuracy for teaching up to linear scaling relation $\sim_{R_l}$.
    
%    On the other hand, for any $\epsilon \in (0,1]$, the worst-case oblivious teaching complexity for \algoref{alg: randmaha} has a lower bound of $\Omega\paren{\frac{p(p+1)}{2}}$ that achieves $\epsilon$-accuracy for teaching up to linear scaling relation $\sim_{R_l}$.
%\end{proposition}

\begin{theorem}[General Sampled Activations]\label{thm: samplegeneral}
    Consider a representation space $\cV \subset \reals^p$. Assume that the agent receives activations sampled independently and identically (i.i.d) from a Lebesgue distribution $\cD_{\cV}$. Then, for any target feature matrix $\pphi^* \in \mathcal{M}_{\sf{F}}$, with a tight bound of $n = \Theta(\frac{p(p+1)}{2})$ on the feedback complexity, the oblivious learner (almost surely) learns $\pphi^*$ up to feature equivalence using the feedback set $\cF(\cV_n,\pphi^*)$, i.e.,
    \begin{align*}
        \cP_{\cV} \paren{\forall\,\, \pphi' \in \cF(\cV_n,\pphi^*),\, \exists \lambda > 0, \pphi' = \lambda\cdot \pphi^* } = 1.
    \end{align*}
    %\begin{align*}
    %    \cP_{\cV_P}\paren{ \dim \paren{{ span \inner{\curlybracket{vv^T : v \in \cV_P}}}}  = P} = 1
    %\end{align*}
    %where $\cV_P \sim \cD_{\cV}^P$ such that $P = \frac{p(p+1)}{2}$.
\end{theorem}


%\begin{theorem}\label{thm: samplingsparse}
%    Consider a representation space $\cV \subset \reals^p$. Let $\cD_{\cV}$ be a Lebesgue measure over $\cV$ from which iid representations $\cV_n$ are selected. Then, we can achieve the following guarantees on the expected teaching complexity:
%    \begin{enumerate}
%        \item No-sparsity: Worst-case bound of $\frac{p(p+1)}{2}$ that achieves 1-accuracy for feature learning.
%        \item With sparsity: Worst-case bound of $f(d)$ that achieves $g(d)$-accuracy for feature learning.
%    \end{enumerate}
%\end{theorem}

\begin{assumption}[Sparse-distribution]\label{ass: sparse}
    Each index of a sparse activation vector $\alpha \in \reals^p$ is sampled i.i.d from a sparse distribution defined as: for all $i$
\begin{equation*}
\exists p_i \neq 0,\quad   \cP(\alpha_i  = 0) = p_i, \alpha_i\, |\, \alpha_i \neq 0 \sim \textit{Lebesgue}((0,1]), 
%\textbf{Beta}(\alpha_i(x); \mu_i, \nu_i) = \alpha_i(x)^{\mu_i -1}(1 - \alpha_i(x))^{\nu_i-1}
\end{equation*}
i.e., any index equals zero with a given non-zero probability and is non-zero based on a Lebesgue distribution.
\end{assumption}


\begin{theorem}[Sparse Sampled Activations]\label{thm: samplingsparse}
    Consider a representation space $\cV \subset \reals^p$. Assume that the agent receives representations sampled independently and identically (i.i.d) from a sparse distribution $\cD_{\cV}$ as stated in \assref{ass: sparse}. Fix a threshold $\delta > 0$, and sparsity parameter $s < p$. Then, for any target feature matrix $\pphi^* \in \mathcal{M}_{\sf{F}}$, with a bound of $n = p^2 \paren{\frac{2}{p_{\sf{s}}^2} \log \frac{2}{\delta}}^{1/p^2}$ on the feedback complexity using $s$-sparse feedbacks, the oblivious learner learns $\pphi^*$ up to feature equivalence with high probability using the feedback set $\cF(\cV_n,\pphi^*)$, i.e.,
     \begin{align*}
        \cP_{\cV} \paren{\forall\,\, \pphi' \in \cF(\cV_n,\pphi^*),\, \exists \lambda > 0, \pphi' = \lambda\cdot \pphi^* } \ge (1 - \delta).
    \end{align*}
    % Then, the feedback complexity of learning any target feature matrix $\pphi^* \in \mathcal{M}_{\sf{F}}$ with $s$-sparse feedbacks has a bound of $N$ with ($1 - \delta$)-accuracy, i.e, alternately,
    % \begin{align*}
    %     \cP_{\cV_N}\paren{ \dim \paren{{ span \inner{\curlybracket{vv^T : v \in \cV_N}}}}  \ge P} \ge 1 - \delta
    % \end{align*}
    % where $\cV_N \sim \cD_{\cV}^N$ such that $N = g(\delta, s)$.
\end{theorem}
\fi
%Now, we show a lower bound on the worst-case oblivious teaching complexity in the sampling case even to achieve any non-zero accuracy.
%\begin{lemma}
%       Consider an input space $\cV \subset \reals^d$. Let $\cD_{\cV}$ be a Lebesgue distribution over $\cV$ from which iid samples $\cV_n$ are selected. For any $\epsilon \in (0,1]$, the worst-case oblivious teaching complexity for \algoref{alg: randmaha} has a lower bound of $\Omega\paren{\frac{p(p+1)}{2}}$ that achieves $\epsilon$-accuracy for teaching up to linear scaling relation $\sim_{R_l}$.
%\end{lemma}

\paragraph{Rescaled Pairs} For a given matrix \( \pphi \neq 0 \), a sampled input \( x \sim \mathcal{D}_{\mathcal{V}} \) is almost never orthogonal, i.e., almost surely \( \pphi x \neq 0 \). This property can be utilized to rescale an input and construct pairs that satisfy equality constraints. Specifically, there exist scalars \( \gamma, \lambda > 0 \) such that (assuming without loss of generality \( x^{\top}\pphi x > y^{\top}\pphi y \)),
\begin{align*}
    x^{\top}\pphi x = \lambda \cdot y^{\top}\pphi y + y^{\top}\pphi y = (\sqrt{1 + \lambda}) y^{\top}\pphi (\sqrt{1 + \lambda}) y.
\end{align*}
Thus, the pair \( (x, (\sqrt{1 + \lambda})y) \) satisfies the equality constraints. With this understanding, we reformulate \algoref{alg: main} into \algoref{alg: randmaha}. 

In this section, we analyze the feedback complexity in terms of the minimum number of sampled activations required for the agent to construct an effective feedback set achieving feature equivalence which is illustrated in \figref{fig: monosparse}. Our first result establishes complexity bounds for general activations (without sparsity constraints) sampled from a Lebesgue distribution, with the complete proof provided in \appref{app: samplegeneral}.
%\akash{a remark on how complexity is measured in this setting}
% \akash{In the rest of the section, we study the expected oblivious teaching complexity for \algoref{alg: randmaha} under Lebesgue distribution \( \mathcal{D}_{\mathcal{V}} \) over the input space \( \mathcal{V} \). We show tight bounds on the teaching complexity highlighting a gap between the constructive and sampled teaching scenarios.} 

% \begin{theorem}[Sard's Theorem]
% Let \( f: X \to Y \) be a smooth map of manifolds, and let \( C \) be the set of critical points of \( f \) in \( X \). Then \( f(C) \) has measure zero in \( Y \).
% \end{theorem}

% Now, there are some questions to study:
% \begin{enumerate}
%     \item In the oblivious teaching scenario, what is the teaching complexity?
%     \item If the answer to 1. leads to poor lower bound on the teaching complexity, what are some ways we can improve the complexity.
% \end{enumerate}

% Thus, we consider rescaling the sampled activations in order for the superagent/teacher to design pairs for a given target feature matrix \( \pphi^* \in \mathcal{M}_{\sf{F}} \).

% \begin{definition}\label{defn: lebsample}
%     Consider an input space \( \mathcal{V} \subset \mathbb{R}^d \). Let \( \mathcal{D}_{\mathcal{V}} \) be a Lebesgue distribution over \( \mathcal{V} \) from which iid samples \( \mathcal{V}_n \) are selected. Fix a constant \( \epsilon > 0 \) and a target matrix \( \pphi^* \). We define the oblivious teaching complexity for \( \epsilon \)-accuracy for random samples as \( n \) if the teacher provides a teaching set \( \mathcal{T}(\mathcal{V}_n, \mathcal{M}_{\sf{F}}, \pphi^*) \) for \algoref{alg: randmaha} using the samples \( \mathcal{V}_n \) such that
%     \[
%         \mathcal{P}_{\mathcal{V}} \left( d_{M'} \in \textsf{VS}(\mathcal{T}(\mathcal{V}_n, \mathcal{M}_{\sf{F}}, \pphi^*), \mathcal{M}_{\sf{F}}) \text{ such that } d_{M'} \sim_{R_l} d_{\pphi^*} \right) \geq \epsilon.
%     \]
% \end{definition}

% First, we start with an upper bound on the worst-case (across the space \( \mathcal{M}_{\sf{F}} \)) oblivious teaching complexity that achieves \( 1 \)-accuracy.

% \begin{proposition}\label{prop: sampling}
%     Consider an input space \( \mathcal{V} \subset \mathbb{R}^d \). Let \( \mathcal{D}_{\mathcal{V}} \) be a Lebesgue distribution over \( \mathcal{V} \) from which iid samples \( \mathcal{V}_n \) are selected. Then, the worst-case oblivious teaching complexity for \algoref{alg: randmaha} has an upper bound of \( O\left(\frac{p(p+1)}{2}\right) \) that achieves \( 1 \)-accuracy for teaching up to linear scaling relation \( \sim_{R_l} \).
    
%     On the other hand, for any \( \epsilon \in (0,1] \), the worst-case oblivious teaching complexity for \algoref{alg: randmaha} has a lower bound of \( \Omega\left(\frac{p(p+1)}{2}\right) \) that achieves \( \epsilon \)-accuracy for teaching up to linear scaling relation \( \sim_{R_l} \).
% \end{proposition}

\begin{theorem}[General Sampled Activations]\label{thm: samplegeneral}
    Consider a representation space \( \mathcal{V} \subseteq \mathbb{R}^p \). Assume that the agent receives activations sampled i.i.d from a Lebesgue distribution \( \mathcal{D}_{\mathcal{V}} \). Then, for any target feature matrix \( \pphi^* \in \mathcal{M}_{\sf{F}} \), with a tight bound of \( n = \Theta\left(\frac{p(p+1)}{2}\right) \) on the feedback complexity, the oblivious learner (almost surely) learns \( \pphi^* \) up to feature equivalence using the feedback set \( \mathcal{F}(\mathcal{V}_n,\pphi^*) \), i.e.,
    \[
        \mathcal{P}_{\mathcal{V}} \left( \forall\,\, \pphi' \in \mathcal{F}(\mathcal{V}_n,\pphi^*),\, \exists \lambda > 0, \pphi' = \lambda \cdot \pphi^* \right) = 1.
    \]
    %\begin{align*}
    %    \mathcal{P}_{\mathcal{V}_P}\left( \dim \left( \text{span} \left\{ vv^\top : v \in \mathcal{V}_P \right\} \right) = P \right) = 1
    %\end{align*}
    %where \( \mathcal{V}_P \sim \mathcal{D}_{\mathcal{V}}^P \) such that \( P = \frac{p(p+1)}{2} \).
\end{theorem}
\begin{proof}[Proof Outline] The key observation is that almost surely for any $n \le p(p+1)/2$ randomly sampled activations on a unit sphere $\mathbb{S}^{p-1}$ under Lebesgue measure, the corresponding rank-1 matrices are linearly independent. This is a direct application of Sard's theorem on the zero set of a non-zero polynomial equation, yielding the upper bound. For the lower bound, we use some key necessary properties of a feedback set as elucidated in the proof of \thmref{thm: constructgeneral}. This result essentially fixes certain activations that need to be spanned by a feedback set, but under a Lebesgue measure on a continuous domain, we know that the probability of sampling a direction is zero.
\end{proof}

We consider a fairly general distribution over sparse activations similar to the signal model in \cite{bachsparse}.

\begin{assumption}[Sparse-Distribution]\label{ass: sparse}
    Each index of a sparse activation vector \( \alpha \in \mathbb{R}^p \) is sampled i.i.d from a sparse distribution defined as: for all \( i \),
    \[
        \mathcal{P}(\alpha_i = 0) = p_i, \quad \alpha_i \, | \, \alpha_i \neq 0 \sim \text{Lebesgue}((0,1]).
    \]
    %i.e., any index equals zero with a given non-zero probability and is non-zero based on a Lebesgue distribution.
\end{assumption}
With this we state the main theorem of the section with the proof deferred to \appref{app: samplesparse}.
\begin{theorem}[Sparse Sampled Activations]\label{thm: samplingsparse} Consider a representation space \( \mathcal{V} \subseteq \mathbb{R}^p \).
    Assume that the agent receives representations sampled i.i.d from a sparse distribution \( \mathcal{D}_{\mathcal{V}} \). Fix a threshold \( \delta > 0 \), and sparsity parameter \( s < p \). Then, for any target feature matrix \( \pphi^* \in \mathcal{M}_{\sf{F}} \), with a bound of $n = O\paren{p^2(\frac{2}{p_{\sf{s}}^2} \log \frac{2}{\delta})^{1/p^2}}$
    % \[
    %     n = O\paren{p^2 \left( \frac{2}{p_{\sf{s}}^2} \log \frac{2}{\delta} \right)^{1/p^2}}
    % \]
    on the feedback complexity using \( s \)-sparse feedbacks,the oblivious learner learns \( \pphi^* \) up to feature equivalence with high probability using the feedback set \( \mathcal{F}(\mathcal{V}_n,\pphi^*) \), i.e.,
    \[
        \mathcal{P}_{\mathcal{V}} \left( \forall\,\, \pphi' \in \mathcal{F}(\mathcal{V}_n,\pphi^*),\, \exists \lambda > 0, \pphi' = \lambda \cdot \pphi^* \right) \geq (1 - \delta),
    \]
    where $p_{\sf{s}}$ depends on $\cD_\cV$, and sparasity parameter $s$.
    % Then, the feedback complexity of learning any target feature matrix \( \pphi^* \in \mathcal{M}_{\sf{F}} \) with \( s \)-sparse feedbacks has a bound of \( N \) with (\( 1 - \delta \))-accuracy, i.e,
    % \begin{align*}
    %     \mathcal{P}_{\mathcal{V}_N}\left( \dim \left( \text{span} \left\{ vv^\top : v \in \mathcal{V}_N \right\} \right) \geq P \right) \geq 1 - \delta
    % \end{align*}
    % where \( \mathcal{V}_N \sim \mathcal{D}_{\mathcal{V}}^N \) such that \( N = g(\delta, s) \).
\end{theorem}
    \begin{proof}[Proof Outline]Using the formulation of \eqnref{eq: orthosat}, we need to estimate the number of activations the agent needs to receive/sample before an induced set of $p(p+1)/2$ many rank-1 linearly independent matrices are found.
    To estimate this, first we generalize the construction of the set ${\cB}$ from the proof of \thmref{thm: constructsparse} to 
    \begin{align*}
         %{U}_g = &\{\lambda_i e_i: \lambda_i \neq 0, 1 \leq i \leq p\}\,\, \cup\, \\ &\{(\lambda_{iji} e_i + \lambda_{ijj}e_j): \lambda_{iji},\lambda_{ijj} \neq 0,  1 \leq i < j \leq p \}.\\
         \widehat{U}_g = \curlybracket{\lambda_i^2 e_ie_i^T: 1 \leq i \leq p} \cup  \curlybracket{(\lambda_{iji} e_i + \lambda_{ijj}e_j)(\lambda_{iji} e_i + \lambda_{ijj}e_j)^T: 1 \leq i < j \leq p}
    \end{align*}
    We then analyze a design matrix $\mathbb{M}$ of rank-1 matrices from sampled activations and compute the probability of finding columns with entries semantically similar to those in $\widehat{U}_g$ (as vectorizations), ensuring a non-trivial determinant. The final complexity bound is derived using Hoeffding's inequality and Sterling's approximation.
    %Then we consider a design matrix $\mathbb{D}$ of rank-1 matrices induced by sampled activations and compute the probability that there exists columns which have a semantically similar entries as corresponded by $U_g$ so that the determinant of $\mathbb{D}$ is not trivial zero. With this, the final expression is based on Hoeffding's inequality and Sterling's approximation.
    %We evaluate the probability that rank-1 matrices induced by $U_g$
    \end{proof}

%\subsection{Reduction to zero signal}


