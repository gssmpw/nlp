@inproceedings{Arora2013NewAF,
  title={New Algorithms for Learning Incoherent and Overcomplete Dictionaries},
  author={Sanjeev Arora and Rong Ge and Ankur Moitra},
  booktitle={Annual Conference Computational Learning Theory},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:6978132}
}

@article{Chen2013ExactAS,
  title={Exact and Stable Covariance Estimation From Quadratic Sampling via Convex Programming},
  author={Yuxin Chen and Yuejie Chi and Andrea J. Goldsmith},
  journal={IEEE Transactions on Information Theory},
  year={2013},
  volume={61},
  pages={4034-4059},
  url={https://api.semanticscholar.org/CorpusID:210337}
}

@article{Fawzi2022,
  author       = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Ruiz, Francisco J. R. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  title        = {Discovering faster matrix multiplication algorithms with reinforcement learning},
  journal      = {Nature},
  year         = {2022},
  volume       = {610},
  number       = {7930},
  pages        = {47--53},
  month        = oct,
  abstract     = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
  doi          = {10.1038/s41586-022-05172-4},
  url          = {https://doi.org/10.1038/s41586-022-05172-4},
  issn         = {1476-4687}
}

@inproceedings{Kleindessner2016KernelFB,
  title={Kernel functions based on triplet comparisons},
  author={Matth{\"a}us Kleindessner and Ulrike von Luxburg},
  booktitle={Neural Information Processing Systems},
  year={2016}}%,
  url={https://api.semanticscholar.org/CorpusID:33507245}
}

@article{LDA,
author = {Fisher, R. A.},
title = {THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS},
journal = {Annals of Eugenics},
volume = {7},
number = {2},
pages = {179-188},
doi = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
year = {1936}
}

@article{LMNN,
author = {Weinberger, Kilian Q. and Saul, Lawrence K.},
title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {207–244},
numpages = {38}
}

@inproceedings{Mason2017LearningLM,
  title={Learning Low-Dimensional Metrics},
  author={Blake Mason and Lalit P. Jain and Robert D. Nowak},
  booktitle={Neural Information Processing Systems},
  year={2017}
}

@inproceedings{NCA,
 author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam and Salakhutdinov, Russ R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Neighbourhood Components Analysis},
 volume = {17},
 year = {2004}
}

@article{OLSHAUSEN19973311,
title = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
journal = {Vision Research},
volume = {37},
number = {23},
pages = {3311-3325},
year = {1997},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(97)00169-7},
url = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
author = {Bruno A. Olshausen and David J. Field},
keywords = {Coding, V1, Gabor-wavelet, Natural images},
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.}
}

@book{PCA,
  title     = {Principal Component Analysis},
  author    = {Jolliffe, Ian T.},
  year      = {1986},
  publisher = {Springer-Verlag},
  address   = {New York}
}

@article{Romera-Paredes2024,
  author       = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M.~Pawan and Dupont, Emilien and Ruiz, Francisco J.~R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
  title        = {Mathematical discoveries from program search with large language models},
  journal      = {Nature},
  year         = {2024},
  volume       = {625},
  number       = {7995},
  pages        = {468--475},
  month        = jan,
  abstract     = {Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.},
  doi          = {10.1038/s41586-023-06924-6},
  url          = {https://doi.org/10.1038/s41586-023-06924-6},
  issn         = {1476-4687}
}

@inproceedings{Xing2002DistanceML,
  title={Distance Metric Learning with Application to Clustering with Side-Information},
  author={Eric P. Xing and A. Ng and Michael I. Jordan and Stuart J. Russell},
  booktitle={Neural Information Processing Systems},
  year={2002}
}

@inproceedings{abbe2022merged,
  title={The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author={Abbe, Emmanuel and Boix-Adsera, Emmanuel and Misiakiewicz, Theodor},
  booktitle={Conference on Learning Theory},
  pages={4782--4887},
  year={2022},
  organization={PMLR}
}

@InProceedings{agarwal_incoherent,
  title = 	 {Learning Sparsely Used Overcomplete Dictionaries},
  author = 	 {Agarwal, Alekh and Anandkumar, Animashree and Jain, Prateek and Netrapalli, Praneeth and Tandon, Rashish},
  booktitle = 	 {Proceedings of The 27th Conference on Learning Theory},
  pages = 	 {123--137},
  year = 	 {2014},
  editor = 	 {Balcan, Maria Florina and Feldman, Vitaly and Szepesvári, Csaba},
  volume = 	 {35},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Barcelona, Spain},
  month = 	 {13--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v35/agarwal14a.pdf},
  url = 	 {https://proceedings.mlr.press/v35/agarwal14a.html},
  abstract = 	 {We consider the problem of learning sparsely used overcomplete dictionaries, where each observation is  a sparse combination of elements from an unknown overcomplete dictionary. We establish exact recovery when the dictionary elements are mutually incoherent. Our method consists of a clustering-based initialization step, which provides an approximate estimate   of the true dictionary with guaranteed accuracy. This estimate is then refined via an iterative algorithm with the following alternating steps: 1) estimation of the dictionary coefficients for each observation through \ell_1 minimization, given the dictionary estimate, and 2) estimation of the dictionary elements through least squares, given the coefficient estimates. We establish that, under a set of sufficient conditions, our method converges at a linear rate to the true dictionary as well as the true coefficients for each observation.}
}

@article{arora2018linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018},
  publisher={MIT Press}
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={arXiv preprint arXiv:2205.01445},
  year={2022}
}

@ARTICLE{bachsparse,
  author={Gribonval, Rémi and Jenatton, Rodolphe and Bach, Francis},
  journal={IEEE Transactions on Information Theory}, 
  title={Sparse and Spurious: Dictionary Learning With Noise and Outliers}, 
  year={2015},
  volume={61},
  number={11},
  pages={6298-6319},
  keywords={Noise measurement;Complexity theory;Encoding;Image coding;Clustering algorithms;Dictionary learning;sparse coding;structured learning;sample complexity;sparse matrices;dictionaries;signal representations;machine learning},
  doi={10.1109/TIT.2015.2472522}}

@inproceedings{bai2019beyond,
  title={Beyond linearization: On quadratic and higher-order approximation of wide neural networks},
  author={Bai, Yu and Lee, Jason D.},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@Book{bellet_survey,
 Author = {Bellet, Aur{\'e}lien and Habrard, Amaury and Sebban, Marc},
 Title = {Metric learning},
 FSeries = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
 Series = {Synth. Lect. Artif. Intell. Mach. Learn.},
 ISSN = {1939-4608},
 Volume = {30},
 ISBN = {978-1-62705-365-5; 978-1-627-05366-2},
 Year = {2015},
 Publisher = {San Rafael, CA: Morgan \& Claypool Publishers},
 Language = {English},
 DOI = {10.2200/S00626ED1V01Y201501AIM030},
 Keywords = {68-02,68T05},
 zbMATH = {6407598},
 Zbl = {1308.68005}
}

@inproceedings{bordelon2022self,
  title={Self-consistent dynamical field theory of kernel evolution in wide neural networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  volume={35},
  pages={32240--32256},
  year={2022},
  publisher={Curran Associates, Inc.}
}

@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@inproceedings{damian2022neural,
  title={Neural networks can learn representations with gradient descent},
  author={Damian, Andrei and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle={Conference on Learning Theory},
  pages={5413--5452},
  year={2022},
  organization={PMLR}
}

@inproceedings{distance_metric_relative,
 author = {Schultz, Matthew and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Learning a Distance Metric from Relative Comparisons},
 volume = {16},
 year = {2003}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy\_model/index.html}
}

@article{faruqui2015sparse,
  title={Sparse overcomplete word vector representations},
  author={Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A},
  journal={arXiv preprint arXiv:1506.02004},
  year={2015}
}

@ARTICLE{gribonval_rotation,
  author={Gribonval, Rémi and Schnass, Karin},
  journal={IEEE Transactions on Information Theory}, 
  title={Dictionary Identification—Sparse Matrix-Factorization via $\ell_1$ -Minimization}, 
  year={2010},
  volume={56},
  number={7},
  pages={3523-3539},
  keywords={Dictionaries;Sparse matrices;Signal processing;Harmonic analysis;Blind source separation;Compressed sensing;Independent component analysis;Noise reduction;Source separation;Signal sampling; $\ell_1$-minimization;blind source localization;blind source separation;compressed sensing;dictionary identification;dictionary learning;independent component analysis;nonconvex optimization;random matrices;sparse representation},
  doi={10.1109/TIT.2010.2048466}}

@inproceedings{hanin2020finite,
  title={Finite depth and width corrections to the {Neural Tangent Kernel}},
  author={Hanin, Boris and Nica, Mihai},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{huang2020dynamics,
  title={Dynamics of deep neural networks and neural tangent hierarchy},
  author={Huang, Jiaoyang and Yau, Horng-Tzer},
  booktitle={International Conference on Machine Learning},
  pages={4542--4551},
  year={2020},
  organization={PMLR}
}

@inproceedings{itml,
author = {Davis, Jason V. and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S.},
title = {Information-theoretic metric learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273523},
doi = {10.1145/1273496.1273523},
abstract = {In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {209–216},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{kulis_ml,
url = {http://dx.doi.org/10.1561/2200000019},
year = {2013},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Metric Learning: A Survey},
doi = {10.1561/2200000019},
issn = {1935-8237},
number = {4},
pages = {287-364},
author = {Brian Kulis}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@ARTICLE{mallat_dict,
  author={Mallat, S.G. and Zhifeng Zhang},
  journal={IEEE Transactions on Signal Processing}, 
  title={Matching pursuits with time-frequency dictionaries}, 
  year={1993},
  volume={41},
  number={12},
  pages={3397-3415},
  keywords={Matching pursuit algorithms;Time frequency analysis;Dictionaries;Pursuit algorithms;Fourier transforms;Signal representations;Vocabulary;Signal processing algorithms;Interference;Natural languages},
  doi={10.1109/78.258082}}

@misc{marks2024sparsefeaturecircuitsdiscovering,
      title={Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models}, 
      author={Samuel Marks and Can Rager and Eric J. Michaud and Yonatan Belinkov and David Bau and Aaron Mueller},
      year={2024},
      eprint={2403.19647},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.19647}, 
}

@inproceedings{mousavi-hosseini2023neural,
  title={Neural networks efficiently learn low-dimensional representations with {SGD}},
  author={Mousavi-Hosseini, Ashkan and Park, Sejun and Girotti, Massimiliano and Mitliagkas, Ioannis and Erdogdu, Murat A.},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{panigrahi2019word2sense,
  title={Word2Sense: sparse interpretable word embeddings},
  author={Panigrahi, Abhishek and Simhadri, Harsha Vardhan and Bhattacharyya, Chiranjib},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5692--5705},
  year={2019}
}

@book{roberts2022principles,
  title={The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks},
  author={Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
  year={2022},
  publisher={Cambridge University Press}
}

@inproceedings{shi2022theoretical,
  title={A theoretical analysis on feature learning in neural networks: {E}mergence from inputs and advantage over fixed features},
  author={Shi, Zhengdao and Wei, Jerry and Lian, Yucheng},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{subramanian2018spine,
  title={Spine: Sparse interpretable neural embeddings},
  author={Subramanian, Anant and Pruthi, Danish and Jhamtani, Harsh and Berg-Kirkpatrick, Taylor and Hovy, Eduard},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{voroninski,
author = {Li, Xiaodong and Voroninski, Vladislav},
title = {Sparse Signal Recovery from Quadratic Measurements via Convex Programming},
journal = {SIAM Journal on Mathematical Analysis},
volume = {45},
number = {5},
pages = {3019-3033},
year = {2013},
doi = {10.1137/120893707},

URL = { 
    
        https://doi.org/10.1137/120893707
    
    

},
eprint = { 
    
        https://doi.org/10.1137/120893707
    
    

}
,
    abstract = { In this paper we consider a system of quadratic equations \$|\langle \bm{z\_j}, \bm{x}\rangle|^2=b\_j, j=1,\ldots,m\$, where \$\bm{x} \in \mathbb{R}^n\$ is unknown while normal random vectors \$\bm{z\_j} \in \mathbb{R}^n\$ and quadratic measurements \$b\_j \in \mathbb{R}\$ are known. The system is assumed to be underdetermined, i.e., \$m<n\$. We prove that if there exists a sparse solution \$\bm{x}\$, i.e., at most \$k\$ components of \$\bm{x}\$ are nonzero, then by solving a convex optimization program, we can solve for \$\bm{x}\$ up to a multiplicative constant with high probability, provided that \$k\leq O(\sqrt{m\over{\log n}})\$. On the other hand, we prove that \$k \leq O(\log n\sqrt{m})\$ is necessary for a class of natural convex relaxations to be exact. }
}

@article{vyas2022limitations,
  title={Limitations of the {NTK} for understanding generalization in deep learning},
  author={Vyas, Neel and Bansal, Yamini and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2206.10012},
  year={2022}
}

@inproceedings{yang2021tensor,
  title={Tensor {P}rograms {IV}: Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J.},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@InProceedings{yang_feature,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}

@article{yun2021transformer,
  title={Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  author={Yun, Zeyu and Chen, Yiren and Olshausen, Bruno A and LeCun, Yann},
  journal={arXiv preprint arXiv:2103.15949},
  year={2021}
}

@article{zhang2019word,
  title={Word embedding visualization via dictionary learning},
  author={Zhang, Jeffrey and Chen, Yiren and Cheung, Brian and Olshausen, Bruno A},
  journal={arXiv preprint arXiv:1910.03833},
  year={2019}
}

@misc{zhu2018overviewmachineteaching,
      title={An Overview of Machine Teaching}, 
      author={Xiaojin Zhu and Adish Singla and Sandra Zilles and Anna N. Rafferty},
      year={2018},
      eprint={1801.05927},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.05927}, 
}

@article{zhu2022quadratic,
  title={Quadratic models for understanding neural network dynamics},
  author={Zhu, Lingxiao and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2205.11787},
  year={2022}
}

