\section{Feature learning, Linear representation hypothesis}
\akash{
\begin{itemize}
    \item Results stated with sparsity assumptions
    \item How does it differ from the work for Mahalanobis distances?
\end{itemize}
}

In the feature learning with a superposition net, we assume that
\begin{align*}
    \bx \approx \mathbf{\Phi}_{\ell}\cdot \alpha_\ell(\bx) + \epsilon_{\ell}(\bx),
\end{align*}
where $\bx \in \reals^p$, is a dictionary matrix $\mathbf{\Phi}_{\ell} \in \reals^{p \times r}$ (could be complete/incomplete), representation vectors $\alpha_\ell(x) \in \reals^r$ ($r$ could be larger or smaller than $p$ inducing superimposed/non-superimposed features) and error terms $\epsilon_\ell(x) \in \reals^p$.

Now the task is to learn $\mathbf{\Phi}$ up to normal transformation, i.e. $\mathbf{\Phi}^{\top}\mathbf{\Phi}$ using triplet queries $(0, \beta, y) \in (\reals^{r})^3$.
\begin{align*}
    \beta^{\top} \mathbf{\Phi}^{\top}\mathbf{\Phi}\beta = y^\top \mathbf{\Phi}^{\top}\mathbf{\Phi}y
    \implies \beta^\top \cD \beta = y^{\top}\cD y
\end{align*}
such that the sparsity coefficient of $\beta, y$ is high, i.e $\cS(\beta) = \bigO{1}$.

\akash{Now, all the results from the work on Mahalanobis distance function should work}

\begin{lemma} Consider a PSD, symmetric matrix $M \in \reals^{n^2}$. Define the following set of orthogonal Cholesky decompositions of $M$
\begin{align*}
    W_{\sf{CD}} = \curlybracket{U : M = UU^\top,\, U^\top U = \text{diag}(d_1, d_2, \ldots, d_n)}
\end{align*}
    Then, for any two $U,U' \in W_{\sf{CD}}$, the column vectors of U are equivalent upto a rotation and sign change.
\end{lemma}