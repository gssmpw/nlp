\paragraph{Teaching set for diagonal matrices} Consider the set of diagonal matrices, $\sf symm_{diag}(\reals^{d \times d})$. Thus, for any $M_{\mathsf{diag}} \in \sf symm_{diag}(\reals^{d \times d})$

    \begin{align*}
    M_{\mathsf{diag}} = \begin{bmatrix}
  \gamma_1 & 0& \ldots & 0 \\
  0 & \gamma_2& \ldots & 0\\
  \vdots & \vdots & \vdots & \vdots\\
  0& 0& \ldots & \gamma_n
\end{bmatrix}
\end{align*}
WLOG, to simplify the analysis, we assume that $\sum_{i=1}^n \gamma_i^2 = 1$.
A teaching set for $M_{\mathsf{diag}}$ is a set $\cT_{M_{\mathsf{diag}}}$ that has the following the elements
\begin{subequations}\label{eq: diagset}
\begin{gather}
    \curlybracket{1_i1_i^{\top} - \frac{1}{\gamma^2_j}e_je_j^{\top}}_{i,j \in \bracket{n} }, \textnormal{ where } \\
    \forall i,\quad 1_i = [1,1,\ldots,\underset{i^{th} \textnormal{ index}}{-1},\ldots,1] \textnormal{ and } \curlybracket{e_j}_{j=1}^n \textnormal{ standard basis}
\end{gather}
\end{subequations}

\sanjoy{Would be good to explain this next result further. Isn't $1_i^{\top} M_{diag} 1_i = \gamma_i$?}

\begin{lemma}
    For any non-degenerate diagonal matrix $M_{\mathsf{diag}}$, the $\cT_{M_{\mathsf{diag}}} := \curlybracket{(1_i,\frac{1}{\gamma_j}e_j)}_{i,j \in \bracket{n}}$ as shown in \eqnref{eq: diagset} forms a teaching set to \eqnref{eq: consat}. 
\end{lemma}
\begin{proof}
    The proof has two parts:

    1) $\curlybracket{1_i1_i^{\top} - \frac{1}{\gamma^2_j}e_je_j^{\top}}_{i,j \in \bracket{n} }$ is an orthogonal set to $M_{\mathsf{diag}}$.

    2) $\curlybracket{1_i1_i^{\top} - \frac{1}{\gamma^2_j}e_je_j^{\top}}_{i,j \in \bracket{n} }$ forms a basis to $\mathcal{O}_{M_{\mathsf{diag}}}$.
\end{proof}

\section{Problem Setup}
We denote by $\cX$ a space of objects/inputs. We consider distance functions of the form $d: \cX \times \cX \to \reals$. In this work, we are interested in a distance function $d$ that satisfies properties of a metric which is defined as
\begin{definition}
    Consider a space of objects/inputs $\cX$. Consider a bivariate function $d: \cX \times \cX \to \reals$. We say the pair $(\cX,d)$ forms a metric if it satisfies the following properties:
    \begin{enumerate}
        \item (positivity) for all $x,x' \in \cX$, $d(x,x') \ge 0$. Furthermore, for any $x \in \cX$, $d(x,x) = 0$.
        \item (symmetry) for all $x,x' \in \cX$, $d(x,x') = d(x',x)$
        \item (triangle inequality) for all $x,x',x'' \in \cX$, $d(x,x') + d(x',x'') \ge d(x,x'')$
    \end{enumerate}
\end{definition}
Denote by $\cH_{\cX}$ the collection of all possible metrics on $\cX$, i.e. $\cH_{\cX} := \curlybracket{d: \cX\times \cX \to \reals \, |\, d \textit{ is a metric}}$. We are interested in studying certain subsets $\cM \subset \cH_{\cX}$, e.g. $\cM$ is a collection of Mahalanobis distance functions or $\cM$ is a tree/graph metric or $\cM$ is family of smooth metrics. We describe two classes of interest to this work:

\paragraph{Mahalanobis distance metric}: Assume that $\cX \subset \reals^d$. A \textit{Mahalanobis} distance metric is characterized by a symmetric, positive semi-definite matrix $M \in \textsf{symm}_{+}(\reals^d)$, denoted in this work as $d_M$, such that 
\begin{align}
    \forall x,x' \in \cX,\, d_M(x,x') = (x-x')^{\top}M(x-x') \label{eq: maha}
\end{align}
Note that as $M$ is symmetric and positive semi-definite $d_M$ satisfies all the necessary properties of a metric. 

\paragraph{Tree and Graph metric}: Consider a set of objects $\cX$. Consider a graph $G(\cX,E)$ on $\cX$ with edge set $E$. Let $w: E \to \reals_{+}$ be a weighting on $E$. We say $w$ \textit{induces} a metric on $G$, denoted as $d_G$ if 
\begin{enumerate}
    \item if $e_{uv} \in E$, for $u,v \in \cX$ $d_G(u,v) := w(e_{uv}) \ge 0 $. Furthermore, we define $d_G(u',u') := 0$ for all $u' \in \cX$.
    \item if $e_{uv} \not\in E$ for some $u,v \in \cX$ $d_G(u,v) := \min_{P_G(u,v)} \sum_{e' \in P_G(u,v)} w(e')$ where $P_G(u,v)$ is a path connecting the nodes $u,v$ in $G$. Note, that $d_G(u,v) \ge 0$.
    \item for any $t,u,v \in \cX$ $d_G(t,u) + d_G(u,v) \ge d_G(t,v)$.
\end{enumerate}
Note, that the symmetry of $d_G$ holds by definition. Thus, $d_G$ is a metric on $(\cX,G,w)$, also called a \textit{graph metric}. Now, if $G$ is a tree, then we call $d_G$ a \textit{tree metric}. 

\begin{lemma}\label{lem: same nullset}
    Consider a psd matrix $M \in symm(R^{n\times n})$ with a non-degenerate null set $\cN \subset \reals^d$. Assume that $M' \in symm(R^{n\times n})$ has a null set that contains $\cN$, then there exists a scalar $\lambda > 0$ such that $M + \lambda M'$ is psd.
\end{lemma}

\begin{lemma}\label{lem: uniquevec}
Consider $M^*$ to be the target matrix of rank strictly less than $d$. Then, there exists a matrix $M' \in symm(R^{n\times n})$ in the orthogonal complement of $M^*$ thats needs to be spanned by any teaching set $\cT$ to \eqnref{eq: consat}.
\end{lemma}
\begin{proof}
    Assume that $rank(M^*) = r$, and its eigendecomposition is 
    \begin{align*}
        M^* = \sum_{i=1}^r \gamma_i v_iv_i^{\top}
    \end{align*}
    for the eigenvectors $\curlybracket{v_i}_{i=1}^r$. Let $v_{r+1},\ldots,v_d$ be the extension to these eigenvectors and thus $\curlybracket{v_i}_{i=1}^r$ form a basis. Consider an arbitrary teaching set $\cT$ for $M^*$ that satisfies $\eqnref{eq: consat}$.
    
    WLOG assume that $v_dv_d^{\top}$ is not spanned by $\cT$. Now if $v_dv_d^{\top}\, \bot\, \cT$, then for any scalar $\lambda > 0$, $M^* + \lambda v_dv_d^{\top}$ satisfies all the conditions in \eqnref{eq: consat}, a contradiction. 
    
    So, consider the case when $v_dv_d^{\top}\, \not\perp\, \cT$. We will first show that there exists some $M' \neq M^* \in symm(R^{n\times n})$ orthogonal to $\cT$ and $\curlybracket{v_{r+1},\ldots,v_{d-1}}$ is a subset of its null set. Consider the intersection of the orthogonal complement of the vectors $\curlybracket{v_{r+1}v_{r+1}^{\top},\ldots,v_{d-1}v_{d-1}^{\top}}$, denote it as $\cN_{rest}$. Note that
    \begin{align}
        \dim(\cN_{rest}) = D - d + r
    \end{align}
    Since $v_dv_d^{\top}$ is in $\cN_{rest}$ and $\dim(\cN_{rest}) > 1$ there exists some $M'$ such that $(M') \perp (M^*)$, and also orthogonal to elements in the teaching set $\cT$. Thus, $M'$ has a null set which includes the subset $\curlybracket{v_{r+1},\ldots,v_{d-1}}$. Now, the rest of the proof involves showing existence of some scalar $\lambda$ such that $M^* + \lambda M'$ satisfies the conditions of \eqnref{eq: consat} for the teaching set $\cT$. 

    Note that if $v_dM'v_d^{\top} = 0$ then the proof is straightforward using \lemref{lem: same nullset}. WLOG assume that $v_dM'v_d^{\top} > 0$. \akash{complete the steps here} Using similar steps for any vector $v \in \reals_d$ of the form $v_{[r]} + \hat{v}_d$ where $v_{[r]} \in span\langle v_1,\ldots v_r\rangle$ and $\hat{v}_d$ along the direction $v_d$, it can be easily argued that there exists a constant $\lambda$ for which $M^* + \lambda M'$ is psd. Contradiction!
    
\end{proof}


\subsection{(1 + $\epsilon$) approximation via Mahalanobis distance functions}

\begin{assumption}[hessian continuity] Consider a distance function $d: \cX \times \cX \to \reals_{+}$ which is $C^2$ in second argument. We say $d$ is hessian continuous if for all $x, x' \in \cX$ there exists a positive scalar $L > 0$ such that 
\begin{align*}
    ||\sf{H}_x - \sf{H}_{x'}||_{F} \le L\cdot ||x - x'||_2
\end{align*}
where $\sf{H}_x$ and $\sf{H}_{x'}$ are hessian matrices of $d(x,\cdot)$ and $d(x',\cdot)$ at $y = x$ and $y = x'$ respectively.
\end{assumption}

An immediate consequence of this result is that if $x$ and $x'$ are close to each other then the distances wrt sample $x'$ in the neighborhood of $x$ can be computed using the hessian information at $x$. To see this, assume that $d(x, \cdot)$ has uniformly bounded third partial derivatives as in \thmref{thm: smoothl2}. Now, we note that 
\begin{align}
    d(x', y') \approx_{\xi} (x' - y')^{\top}\sf{H}_{x'}(x' - y') \label{eq: 11}\\
    d(x, x - x' + y') \approx_{\xi} (x' - y')^{\top}\sf{H}_{x}(x' - y') \label{eq: 12}
\end{align}
But we can bound the difference assuming (wlog) \eqref{eq: 11} $\ge$ \eqref{eq: 12},
\begin{align*}
    (x' - y')^{\top}\sf{H}_{x'}(x' - y') - (x' - y')^{\top}\sf{H}_{x}(x' - y') &\le ||\sf{H}_x - \sf{H}_{x'}||_{F}\cdot ||x' -y'||^2_2%\\
    %& \le L\cdot ||x - x'||_2\cdot ||x' -y'||^2_2
\end{align*}
But using the hessian continuity of $d$ we get 
\begin{align*}
    (x' - y')^{\top}\sf{H}_{x'}(x' - y') - (x' - y')^{\top}\sf{H}_{x}(x' - y') \le L\cdot ||x - x'||_2\cdot ||x' -y'||^2_2 \label{eq: mahaapp}
\end{align*}
Now, if $x, x', y'$ are in close proximity the difference in Mahalanobis distances is small. Thus, to approximate $d(x', y')$, we can use curvature information at $x$ itself, in the form of the hessian matrix $\sf{H}_x$.

\begin{theorem}\label{thm: smoothl2} Consider a compact separable metric space $\cX \subset \reals^k$. Consider a distance function $d: \cX \times \cX \to \reals_{\ge 0}$ that is a $C^2$-map in the second argument. Assume that $d$ satisfies the following properties:
\begin{enumerate}
    \item it satisfies the triangle inequality over $\cX$, i.e. for all $x,y,z \in \cX$, $d(x,y) + d(y,z) \ge d(x,z)$,
    \item it has uniformly bounded third partial derivative by a constant $M > 0$, 
    \item at any sample $x \in \cX$, the smallest and largest eigenvalues $\lambda_{\min}(\sf{H}_x d(x,\cdot))$ and $\lambda_{\max}(\sf{H}_x d(x,\cdot))$ are lower bounded and upper bounded by the constants $\gamma_m, \gamma_M > 0$ respectively, and
    \item it is hessian continuous.
\end{enumerate}

%Consider a distribution $\mu$ over $\cX$ which is $L$-lipschitz. %for some constant $\frac{1}{\lambda_{\textsf{diam}}}\ge L > 0$ where $\lambda_{\textsf{diam}} = \textsf{diam}(\cX)$. 
Then, for any error parameter $\epsilon > 0$, there exists a distance function $d'$ that combines a finite sample distance function and local Mahalanobis distance functions

(see \algoref{alg: smoothdistl2}) which can be taught with $\cN(\cX, \epsilon(\epsilon, M, \gamma), d) (\cN(\cX, \epsilon(\epsilon, M, \gamma), d) + k^2)$ triplet comparisons such that for any $x,y,z \in \cX$, if $d(x,y) > (1 + \epsilon)\cdot d(x,z)$ then
\begin{align*}
    d'(x,y) > d'(x,z)
\end{align*}
where the covering accuracy $\epsilon(\epsilon, M, \gamma)$ denotes that the covering depends on $\epsilon$, and constants $M$ and $\gamma$.
\end{theorem}

\begin{algorithm}[t]
\caption{Teaching a smooth distance function with $\ell_2$ covering of the space}
\label{alg: smoothdistl2}
\textbf{Given}: Input space $\cX \sim \cC_{\cX}$, error threshold: $\epsilon$.\\
\vspace{1mm}
\textit{In batch setting:}\vspace{1mm}
\begin{enumerate}
    \item teacher provides triplet comparisons $\cT$ corresponding to finite distance function $(\cC_2, d)$ for an $\epsilon^2$-cover $\cC_2(\cX,\epsilon^2,d)$.
    %\item teacher provides triplet comparisons $\cT$ corresponding to finite distance function $(\cC_3, d)$ for an $\epsilon^3$-cover $\cC_3(\cX,\epsilon^3,d)$.
    \item teacher provides triplet comparisons $\{\cT_x\}_{x \in \cC_2}$ to teach local Mahalanobis distance functions at the centers $\cC_2$.
    %\item teacher provides triplet comparisons $\{\cT_x\}_{x \in \cC_3}$ to teach local Mahalanobis distance functions at the centers $\cC_3$
\end{enumerate}
%For a given sample $x$, learner computes:
%\begin{itemize}
%    \item $\cC_x := \curlybracket{c \in \cC: ||x - c ||_2 \le \epsilon}$ 
%    \item $c_x := \argmin_{c \in \cC_x} \sqrt{(x-c)^{\top}(x-c)}$
%\end{itemize}
%\vspace{1mm}
For a given sample $x$, learner computes:
\begin{itemize}
    \item $\cC_2(x) := \curlybracket{c \in \cC_2: ||x - c ||_{\sf{H}_c}^2 \le \epsilon^2}$ 
    \item $c_2(x) := \argmin_{c \in \cC_2(x)} \sqrt{(x-c)^{\top}\sf{H}_c(x-c)}$
    %\item $\cC_3(x) := \curlybracket{c \in \cC_3: ||x - c ||_{\sf{H}_c}^2 \le \epsilon^2}$ 
    %\item $c_3(x) := \argmin_{c \in \cC_3(x)} \sqrt{(x-c)^{\top}\sf{H}_c(x-c)}$
\end{itemize}
\vspace{1mm}
To answer query on $(x,y,z)$, learner computes 
\begin{gather*}
  \ell_{xz} := (z - x)^{\top}\sf{H}_{c_2(x)}(z - x),\,\, \ell_{xy} := (y - x)^{\top}\sf{H}_{c_2(x)}(y - x)  \\
  \ell_{c_2(x)z} := (z - c_2(x))^{\top}\sf{H}_{c_2(x)}(z - c_2(x)),\,\, \ell_{c_2(x)y} := (y - c_2(x))^{\top}\sf{H}_{c_2(x)}(y - c_2(x))    
\end{gather*}
and checks:
\begin{itemize}
    \item \textbf{If} $\ell_{c_2(x)y}$ and $\ell_{c_2(x)z}$ $\ge 4\epsilon^2$: \\
        \qquad answer according to the triplet $(c_2(x),c_2(y),c_2(z))$
    \item \textbf{Else If} $\ell_{c_2(x)y} \ge 4\epsilon^2$ and $\ell_{c_2(x)z} \le \epsilon^2$:\\
    \qquad answer $d(x,y) > d(x,z)$
    \item \textbf{Else If} $\ell_{c_2(x)z} \ge 4\epsilon^2$ and $\ell_{c_2(x)y} \ge \epsilon^2$:\\ 
    \qquad answer $d(x,z) > d(x,y)$
    \item \textbf{Else} $\ell_{c_2(x)y}$ and $\ell_{c_2(x)z}$ $< 2\epsilon^2$:\\
    \qquad answer according to $\sgn{\ell_{xy} - \ell_{xz}}$
\end{itemize}
%\begin{itemize}
%    \item \textbf{If} $c_y = c_z$, then say $``d(x,y) \approx d(x,z)"$
%    \item \textbf{Else}: answer according to the triplet $(c_x,c_y,c_z)$.
%\end{itemize}
\end{algorithm}

\begin{proof}
    Consider covers of the following form:
    \begin{itemize}
        \item $\epsilon^2$-cover $\cC(\cX, \epsilon^2, d)$ denoted as $\cC_2(\cX)$ with centers $\cC_2$,
        %\item $\epsilon^3$-cover $\cC(\cX, \epsilon^3, d)$, denoted as $\cC_3(\cX)$ with centers $\cC_3$.
    \end{itemize}
    We need to adjust some constants for the cover which can be fixed appropriately from the later discussion. 

    First note that, if for some $x, y \in \cX$, $(y - x)^{\top}\sf{H}_{c_2(x)}(y - x) > 2\epsilon^2$
    \begin{align*}
        (y - x)^{\top}\sf{H}_{x}(y - x) &\ge 2\epsilon^2 - L\cdot ||x - c_2(x)||_2\cdot ||x - y||_2^2\\
                                        &\ge 2\epsilon^2 - L \cdot \frac{\sqrt{2}\epsilon}{\sqrt{\gamma_m}} \cdot 
    \end{align*}



    First we note that the following observation:
    \begin{lemma}
        For a given sample $x \in \cX$ such that $d(x,c) \le \epsilon^2$, and $(y- c)^{\top}\sf{H}_c(y-c) \le \epsilon^2$ then $d(c,y) < \epsilon^2$. Similarly, if $(y- c)^{\top}\sf{H}_c(y-c) \ge \epsilon^2$ then $d(c,y) \ge \epsilon^2$.
    \end{lemma}
    \begin{proof}
        
    \end{proof}
    \begin{lemma}
        For a given sample $x \in \cX$ and a given center $c \in \cC_2$, $d(x,c) \le 1.1\epsilon^2$ iff $(x- c)^{\top}\sf{H}_c(x-c) \le \epsilon^2$.
    \end{lemma}
    \begin{proof}
        Assume that $(x- c)^{\top}\sf{H}_c(x-c) \le \epsilon^2$. Now, using Taylor's approximation in \eqnref{eq: 1}, we have 
        \begin{align*}
            d(c,x) &\le \frac{1}{2!}(x-c)^{\top} \sf{H}_c (x-c) + \frac{M k^{\frac{3}{2}}}{6} \cdot||c-x||^{3}_2 \\
            &\le \frac{1}{2!}(x-c)^{\top} \sf{H}_c (x-c) + \frac{M k^{\frac{3}{2}}}{6(\gamma_m)^{\frac{3}{2}}}\epsilon^{\frac{5}{2}}\\
            &\le \epsilon^2 + \frac{M n^{\frac{3}{2}}}{6(\gamma_m)^{\frac{3}{2}}}\epsilon^{\frac{5}{2}}
        \end{align*}
        Assuming $\epsilon$ is much smaller than $\frac{M k^{\frac{3}{2}}}{6(\gamma_m)^{\frac{3}{2}}}$ yields one side of the lemma.

        Now, if $(x- c)^{\top}\sf{H}_c(x-c) > \epsilon^2$, we note that
        \begin{align*}
        \end{align*}
        
    \end{proof}
\end{proof}

 ---------------------------------------------------------------------------------------------------------------------------------\\

    First note that, if for some $x, y \in \cX$, $(y - x)^{\top}\sf{H}_{c_2(x)}(y - x) > 2\epsilon^2$
    \begin{align*}
        (y - x)^{\top}\sf{H}_{x}(y - x) &\ge 2\epsilon^2 - L\cdot ||x - c_2(x)||_2\cdot ||x - y||_2^2\\
                                        &\ge 2\epsilon^2 - L \cdot \frac{\sqrt{2}\epsilon}{\sqrt{\gamma_m}} \cdot 
    \end{align*}



    First we note that the following observation:
    \begin{lemma}
        For a given sample $x \in \cX$ such that $d(x,c) \le \epsilon^2$, and $(y- c)^{\top}\sf{H}_c(y-c) \le \epsilon^2$ then $d(c,y) < \epsilon^2$. Similarly, if $(y- c)^{\top}\sf{H}_c(y-c) \ge \epsilon^2$ then $d(c,y) \ge \epsilon^2$.
    \end{lemma}
    \begin{proof}
        
    \end{proof}
    \begin{lemma}
        For a given sample $x \in \cX$ and a given center $c \in \cC_2$, $d(x,c) \le 1.1\epsilon^2$ iff $(x- c)^{\top}\sf{H}_c(x-c) \le \epsilon^2$.
    \end{lemma}
    \begin{proof}
        Assume that $(x- c)^{\top}\sf{H}_c(x-c) \le \epsilon^2$. Now, using Taylor's approximation in \eqnref{eq: 1}, we have 
        \begin{align*}
            d(c,x) &\le \frac{1}{2!}(x-c)^{\top} \sf{H}_c (x-c) + \frac{M k^{\frac{3}{2}}}{6} \cdot||c-x||^{3}_2 \\
            &\le \frac{1}{2!}(x-c)^{\top} \sf{H}_c (x-c) + \frac{M k^{\frac{3}{2}}}{6(\gamma_m)^{\frac{3}{2}}}\epsilon^{\frac{5}{2}}\\
            &\le \epsilon^2 + \frac{M n^{\frac{3}{2}}}{6(\gamma_m)^{\frac{3}{2}}}\epsilon^{\frac{5}{2}}
        \end{align*}
        Assuming $\epsilon$ is much smaller than $\frac{M k^{\frac{3}{2}}}{6(\gamma_m)^{\frac{3}{2}}}$ yields one side of the lemma.

        Now, if $(x- c)^{\top}\sf{H}_c(x-c) > \epsilon^2$, we note that
        \begin{align*}
        \end{align*}
        
    \end{proof}


\subsection{(1 + $\epsilon$) approximation via Mahalanobis distance functions}

\begin{assumption}[hessian continuity] Consider a distance function $d: \cX \times \cX \to \reals_{+}$ which is $C^2$ in second argument. We say $d$ is hessian continuous if for all $x, x' \in \cX$ there exists a positive scalar $L > 0$ such that 
\begin{align*}
    ||\sf{H}_x - \sf{H}_{x'}||_{F} \le L\cdot ||x - x'||_2
\end{align*}
where $\sf{H}_x$ and $\sf{H}_{x'}$ are hessian matrices of $d(x,\cdot)$ and $d(x',\cdot)$ at $y = x$ and $y = x'$ respectively.
\end{assumption}

An immediate consequence of this result is that if $x$ and $x'$ are close to each other then the distances wrt sample $x'$ in the neighborhood of $x$ can be computed using the hessian information at $x$. To see this, assume that $d(x, \cdot)$ has uniformly bounded third partial derivatives as in \thmref{thm: smoothl2}. Now, we note that 
\begin{align}
    d(x', y') \approx_{\xi} (x' - y')^{\top}\sf{H}_{x'}(x' - y') \label{eq: 11}\\
    d(x, x - x' + y') \approx_{\xi} (x' - y')^{\top}\sf{H}_{x}(x' - y') \label{eq: 12}
\end{align}
But we can bound the difference assuming (wlog) \eqref{eq: 11} $\ge$ \eqref{eq: 12},
\begin{align*}
    (x' - y')^{\top}\sf{H}_{x'}(x' - y') - (x' - y')^{\top}\sf{H}_{x}(x' - y') &\le ||\sf{H}_x - \sf{H}_{x'}||_{F}\cdot ||x' -y'||^2_2%\\
    %& \le L\cdot ||x - x'||_2\cdot ||x' -y'||^2_2
\end{align*}
But using the hessian continuity of $d$ we get 
\begin{align*}
    (x' - y')^{\top}\sf{H}_{x'}(x' - y') - (x' - y')^{\top}\sf{H}_{x}(x' - y') \le L\cdot ||x - x'||_2\cdot ||x' -y'||^2_2 \label{eq: mahaapp}
\end{align*}
Now, if $x, x', y'$ are in close proximity the difference in Mahalanobis distances is small. Thus, to approximate $d(x', y')$, we can use curvature information at $x$ itself, in the form of the hessian matrix $\sf{H}_x$.

\begin{theorem}\label{thm: smoothl2} Consider a compact separable metric space $\cX \subset \reals^k$. Consider a distance function $d: \cX \times \cX \to \reals_{\ge 0}$ that is a $C^2$-map in the second argument. Assume that $d$ satisfies the following properties:
\begin{enumerate}
    \item it satisfies the triangle inequality over $\cX$, i.e. for all $x,y,z \in \cX$, $d(x,y) + d(y,z) \ge d(x,z)$,
    \item it has uniformly bounded third partial derivative by a constant $M > 0$, 
    \item at any sample $x \in \cX$, the smallest and largest eigenvalues $\lambda_{\min}(\sf{H}_x d(x,\cdot))$ and $\lambda_{\max}(\sf{H}_x d(x,\cdot))$ are lower bounded and upper bounded by the constants $\gamma_m, \gamma_M > 0$ respectively, and
    \item it is hessian continuous.
\end{enumerate}

%Consider a distribution $\mu$ over $\cX$ which is $L$-lipschitz. %for some constant $\frac{1}{\lambda_{\textsf{diam}}}\ge L > 0$ where $\lambda_{\textsf{diam}} = \textsf{diam}(\cX)$. 
Then, for any error parameter $\epsilon > 0$, there exists a distance function $d'$ that combines a finite sample distance function and local Mahalanobis distance functions

(see \algoref{alg: smoothdistl2}) which can be taught with $\cN(\cX, \epsilon(\epsilon, M, \gamma), d) (\cN(\cX, \epsilon(\epsilon, M, \gamma), d) + k^2)$ triplet comparisons such that for any $x,y,z \in \cX$, if $d(x,y) > (1 + \epsilon)\cdot d(x,z)$ then
\begin{align*}
    d'(x,y) > d'(x,z)
\end{align*}
where the covering accuracy $\epsilon(\epsilon, M, \gamma)$ denotes that the covering depends on $\epsilon$, and constants $M$ and $\gamma$.
\end{theorem}

\begin{algorithm}[t]
\caption{Teaching a smooth distance function with $\ell_2$ covering of the space}
\label{alg: smoothdistl2}
\textbf{Given}: Input space $\cX \sim \cC_{\cX}$, error threshold: $\epsilon$.\\
\vspace{1mm}
\textit{In batch setting:}\vspace{1mm}
\begin{enumerate}
    \item teacher provides triplet comparisons $\cT$ corresponding to finite distance function $(\cC_2, d)$ for an $\epsilon^2$-cover $\cC_2(\cX,\epsilon^2,d)$.
    %\item teacher provides triplet comparisons $\cT$ corresponding to finite distance function $(\cC_3, d)$ for an $\epsilon^3$-cover $\cC_3(\cX,\epsilon^3,d)$.
    \item teacher provides triplet comparisons $\{\cT_x\}_{x \in \cC_2}$ to teach local Mahalanobis distance functions at the centers $\cC_2$.
    %\item teacher provides triplet comparisons $\{\cT_x\}_{x \in \cC_3}$ to teach local Mahalanobis distance functions at the centers $\cC_3$
\end{enumerate}
%For a given sample $x$, learner computes:
%\begin{itemize}
%    \item $\cC_x := \curlybracket{c \in \cC: ||x - c ||_2 \le \epsilon}$ 
%    \item $c_x := \argmin_{c \in \cC_x} \sqrt{(x-c)^{\top}(x-c)}$
%\end{itemize}
%\vspace{1mm}
For a given sample $x$, learner computes:
\begin{itemize}
    \item $\cC_2(x) := \curlybracket{c \in \cC_2: ||x - c ||_{\sf{H}_c}^2 \le \epsilon^2}$ 
    \item $c_2(x) := \argmin_{c \in \cC_2(x)} \sqrt{(x-c)^{\top}\sf{H}_c(x-c)}$
    %\item $\cC_3(x) := \curlybracket{c \in \cC_3: ||x - c ||_{\sf{H}_c}^2 \le \epsilon^2}$ 
    %\item $c_3(x) := \argmin_{c \in \cC_3(x)} \sqrt{(x-c)^{\top}\sf{H}_c(x-c)}$
\end{itemize}
\vspace{1mm}
To answer query on $(x,y,z)$, learner computes 
\begin{gather*}
  \ell_{xz} := (z - x)^{\top}\sf{H}_{c_2(x)}(z - x),\,\, \ell_{xy} := (y - x)^{\top}\sf{H}_{c_2(x)}(y - x)  \\
  \ell_{c_2(x)z} := (z - c_2(x))^{\top}\sf{H}_{c_2(x)}(z - c_2(x)),\,\, \ell_{c_2(x)y} := (y - c_2(x))^{\top}\sf{H}_{c_2(x)}(y - c_2(x))    
\end{gather*}
and checks:
\begin{itemize}
    \item \textbf{If} $\ell_{c_2(x)y}$ and $\ell_{c_2(x)z}$ $\ge 4\epsilon^2$: \\
        \qquad answer according to the triplet $(c_2(x),c_2(y),c_2(z))$
    \item \textbf{Else If} $\ell_{c_2(x)y} \ge 4\epsilon^2$ and $\ell_{c_2(x)z} \le \epsilon^2$:\\
    \qquad answer $d(x,y) > d(x,z)$
    \item \textbf{Else If} $\ell_{c_2(x)z} \ge 4\epsilon^2$ and $\ell_{c_2(x)y} \ge \epsilon^2$:\\ 
    \qquad answer $d(x,z) > d(x,y)$
    \item \textbf{Else} $\ell_{c_2(x)y}$ and $\ell_{c_2(x)z}$ $< 2\epsilon^2$:\\
    \qquad answer according to $\sgn{\ell_{xy} - \ell_{xz}}$
\end{itemize}
%\begin{itemize}
%    \item \textbf{If} $c_y = c_z$, then say $``d(x,y) \approx d(x,z)"$
%    \item \textbf{Else}: answer according to the triplet $(c_x,c_y,c_z)$.
%\end{itemize}
\end{algorithm}

\begin{proof}
    Consider covers of the following form:
    \begin{itemize}
        \item $\epsilon^2$-cover $\cC(\cX, \epsilon^2, d)$ denoted as $\cC_2(\cX)$ with centers $\cC_2$,
        %\item $\epsilon^3$-cover $\cC(\cX, \epsilon^3, d)$, denoted as $\cC_3(\cX)$ with centers $\cC_3$.
    \end{itemize}
    We need to adjust some constants for the cover which can be fixed appropriately from the later discussion. 

    Now, consider the following constant:
    \begin{align*}
        \Delta := \min_{x \in \cX} \curlybracket{d(x,x') : x' \in \cX \setminus B_d(x,\epsilon^2)}
    \end{align*}
    Using the condition on $d$, we know that $\Delta > 0$. We pick $\Delta_s := \min\{\frac{\Delta}{3}, \frac{\epsilon^2}{3\gamma_m}\}$.

    
    First, note that for any fixed sample $x \in \cX$, $d(x, y)$ is strongly convex for $y$ close to $x$:
    \begin{align*}
        d(x, x+ h) \ge \frac{1}{2}h^{\top}\sf{H}_x h - \frac{M k^{\frac{3}{2}}}{6} \cdot||h||^{3}_2
    \end{align*}
    Now, if $||h||_2$ is small enough, i.e. smaller than $\frac{4\gamma_m}{M k^{\frac{3}{2}}}$, then we have 
    \begin{align}
        d(x, x+ h) \ge \frac{\gamma_m}{2}||h||^2_2 - \frac{\gamma_m}{4}||h||^2_2 = \frac{\gamma_m}{4}||h||^2_2 \label{eq: strongconvex}
    \end{align}
    Thus, $d(x,\cdot)$ is $\frac{\gamma_m}{4}$-strongly convex at $x$. Since $\gamma_m$ is the smallest eigenvalue for the hessian at any arbitrary sample $x \in \cX$, this holds over $\cX$. On the other hand, the function $d(x,\cdot)$ is smooth, i.e.
    \begin{align}
        d(x, x+ h) \le \gamma_M ||h||^2_2 \label{eq: smoothconvex}
    \end{align}
    Now, we pick the radius for the cover of the space 
    \begin{align*}
        \hat{\epsilon} := \frac{4}{\gamma_m} \Delta_s
    \end{align*}
    Consider an $\hat{\epsilon}$-cover of the space with the centers $\cC$.

    Second case, $(y - c_2(x))^{\top}\sf{H}_{c_2(x)}(y - c_2(x))$ and $(z - c_2(x))^{\top}\sf{H}_{c_2(x)}(z - c_2(x))$ are greater than $2 \hat{\epsilon}$.
    
    Now, consider a $\paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{16\gamma_M^2 (1 + \epsilon)}}$-cover $\cC_\Delta$ wrt to $\ell_2$ distance. 
    
    In \eqnref{eq: strongconvex}, we showed tha if $||h||_2$ is smaller than $\frac{4\gamma_m}{Mk^{\frac{3}{2}}}$ then, $d(x,\cdot)$ is $\frac{\gamma_m}{4}$-strongly convex at $x$ in the second argument. Using this observation, we note that if $c \in \cC_2$ and $y \in \cX$ then on the boundary  $(c - y)^{\top}\sf{H}_c(c - y) = 2 \hat{\epsilon}$, we have
    \begin{align*}
        \frac{2\hat{\epsilon}}{\gamma_m}  \ge ||c - y||_2^2 \ge \frac{2\hat{\epsilon}}{\gamma_M}
    \end{align*}
    Using strong convexity and smoothness as shown in \eqnref{eq: strongconvex}-\eqnref{eq: smoothconvex},
    \begin{equation*}
         \frac{\gamma_M\cdot \hat{\epsilon}}{\gamma_m} \ge d(c,y) \ge \frac{\gamma_m\cdot \hat{\epsilon}}{2\gamma_M}
    \end{equation*}
    Now note that $\frac{\gamma_m\cdot \hat{\epsilon}}{2\gamma_M} < \Delta^2/3$, which implies if $(c - y)^{\top}\sf{H}_c(c - y) > 2\hat{\epsilon}$ then $d(c,y) > \frac{\gamma_m\cdot \hat{\epsilon}}{2\gamma_M}$.

    Now, if $||c_2(x) - x||^2_2 \le \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{16\gamma_M^2 (1 + \epsilon)}}$, using strong convexity of $d(c_2(x),\cdot)$
    \begin{align*}
        d(c_2(x), x) \le \gamma_M\cdot || c_2 (x) - x ||^2_2 \le \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{16\gamma_M (1 + \epsilon)}}
        \end{align*}
    Now, we will show how reducing $x,y,z$ to the closest centers in $\cC_{\Delta}$ leads to correct classification:
    \begin{align*}
        d(c_2(x), c_2(y)) &\ge d(c_2(x), y) - d(c_2(y), y)\\
                          & \ge d(x,y) - d(x, c_2(x)) - d(c_2(y), y)\\
                          & \ge (1+ \epsilon)\cdot d(x,z) - 2\paren{\frac{\gamma_m \hat{\epsilon}}{16\gamma_M}}\\
                          & \ge (1 + \epsilon)\cdot (d(c_2(x),c_2(z)) - d(x, c_2(x)) - d(c_2(z), z)) -  \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{8\gamma_M (1 + \epsilon)}}\\
                          & \ge d(c_2(x),c_2(z)) + \epsilon\cdot d(c_2(x),c_2(z)) - (1 + \epsilon) \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{8\gamma_M (1 + \epsilon)}} - \paren{\frac{\gamma_m \hat{\epsilon}}{8\gamma_M (1 + \epsilon)}}\\
                          & \ge d(c_2(x),c_2(z)) + \epsilon \cdot (d(c_2(x),z) - d(z, c_2(z))) - (1 + \epsilon) \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{8\gamma_M (1 + \epsilon)}} - \paren{\frac{\gamma_m \hat{\epsilon}}{8\gamma_M (1 + \epsilon)}}\\
                          & \ge d(c_2(x),c_2(z)) + \epsilon\cdot d(c_2(x),z) - \epsilon \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{16\gamma_M (1 + \epsilon)}}- (1 + \epsilon) \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{8\gamma_M (1 + \epsilon)}} - \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{8\gamma_M (1 + \epsilon)}}\\
                          & \ge d(c_2(x),c_2(z)) + \epsilon\cdot d(c_2(x),z) - (1 + \epsilon)\paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{4\gamma_M (1 + \epsilon)}}\\
                          & \ge d(c_2(x),c_2(z)) + \epsilon \cdot \paren{\frac{\gamma_m \hat{\epsilon}}{2\gamma_M}} - \paren{\frac{\gamma_m \hat{\epsilon}\epsilon}{4\gamma_M}}\\
                          & > d(c_2(x),c_2(z))
                          %& \ge d(c_2(x),c_2(z)) + 4\epsilon \hat{\epsilon} - \epsilon \frac{M k^{\frac{3}{2}}}{6} d(c_2(x),z) - \epsilon \hat{\epsilon} - 2(1 + \epsilon) \hat{\epsilon} - \paren{\frac{\gamma_m \hat{\epsilon}}{4\gamma_M}}
    \end{align*}

    
    \akash{probably unnecessary}    
    Note that for any point $x$ and a center $c \in \cC$ such that $||x - c||_2^2 = \hat{\epsilon}$, we have 
    \begin{align*}
        d(x,c) \ge (x-c)^{\top}\sf{H}_c(x-c) - \frac{M k^{\frac{3}{2}}}{6} \cdot||x-c||^{3}_2 \ge 3 \Delta_s \ge \Delta 
    \end{align*}
    
    Now, we will show the proof of the algorithm case by case.

    First case, $(y - c_2(x))^{\top}\sf{H}_{c_2(x)}(y - c_2(x))$ and $(z - c_2(x))^{\top}\sf{H}_{c_2(x)}(z - c_2(x))$ are smaller than $4 \hat{\epsilon}$.

    Note that 
    \begin{subequations}\label{eq: bound1}
    \begin{align}
        (y - x)^{\top}\sf{H}_{c_2(x)}(y - x) &\ge (y - x)^{\top}\sf{H}_{x}(y - x) - L\cdot ||x - c_2(x)||_2\cdot ||x - y||_2^2\\
        &\ge d(x,y) - \frac{M k^{\frac{3}{2}}}{6}||x - y||_2^3 - L\cdot ||x - c_2(x)||_2\cdot ||x - y||_2^2\\
        & = d(x,y) -\paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} ||x - y||_2^2
    \end{align}
    \end{subequations}
    We will first show that $\paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} $ is smaller than $\frac{\epsilon\gamma_m}{4}$. But this holds trivially for we have
    \begin{align*}
        ||c_2(x) - y||^2_2 \le \frac{4\hat{\epsilon}}{\gamma_m},\quad ||x - c_2(x)||^2_2 \le \hat{\epsilon}
    \end{align*}
    This implies that 
    \begin{align*}
        ||x - y||_2 \le \paren{\frac{2}{\sqrt{\gamma_m}} + 1}\sqrt{\hat{\epsilon}}
    \end{align*}
    \akash{one condition here on $\hat{\epsilon}$}
    
    Since $\epsilon$ is smaller than $\frac{1}{16}(\frac{2M k^{\frac{3}{2}}}{3\gamma_m^2} + \frac{L}{\gamma_m})^{-1}$ we have 
    \begin{align*}
        \paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} \le \paren{\frac{M k^{\frac{3}{2}}}{6}\paren{\frac{2}{\sqrt{\gamma_m}} + 1} + L}\cdot \sqrt{\hat{\epsilon}} \le \frac{\epsilon\gamma_m}{16}
    \end{align*}
    But this implies that 
    \begin{align*}
        (x-y)^{\top}\sf{H}_{x}(x-y) -\paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} ||x - y||_2^2 \ge \paren{1 - \frac{\epsilon}{16}}\cdot (x-y)^{\top}\sf{H}_{x}(x-y)\\
        (x-y)^{\top}\sf{H}_{c_2(x)}(x-y) -\paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} ||x - y||_2^2 \ge \paren{1 - \frac{\epsilon}{16}}\cdot (x-y)^{\top}\sf{H}_{c_2(x)}(x-y)
    \end{align*}
    Now note that 
    \begin{align*}
        |(x-y)^{\top}\sf{H}_{x}(x-y) - d(x,y)| \le \frac{M k^{\frac{3}{2}}}{6}||x - y||_2^3
    \end{align*}
    Since $||x - y||_2 \le \paren{\frac{2}{\sqrt{\gamma_m}} + 1}\sqrt{\hat{\epsilon}}$, we note that  
    \begin{align*}
        d(x,y) -\paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} ||x - y||_2^2 \ge \paren{1 - \frac{\epsilon}{8}}\cdot d(x,y)
    \end{align*}

    
    Similarly this holds for the sample $z$ as well.
    
    Now, returning to \eqnref{eq: bound1},
    \begin{align}
        (y - x)^{\top}\sf{H}_{c_2(x)}(y - x) &\ge d(x,y) -\paren{\frac{M k^{\frac{3}{2}}}{6}||x - y||_2 + L\cdot ||x - c_2(x)||_2} ||x - y||_2^2 \\
        &\ge \paren{1 - \frac{\epsilon}{8}}\cdot d(x,y)\\
        &\ge \paren{1 - \frac{\epsilon}{8}} (1 + \epsilon)\cdot d(x,z)\\
        & \ge \paren{1 - \frac{\epsilon}{8}} (1 + \epsilon) \paren{(z - x)^{\top}\sf{H}_{c_2(x)}(z - x) - \frac{M k^{\frac{3}{2}}}{6}||x - z||_2^3 - L\cdot ||x - c_2(x)||_2\cdot ||x - z||_2^2}\\
        &\ge \paren{1 - \frac{\epsilon}{8}} (1 + \epsilon)\paren{1 - \frac{\epsilon}{16}} (z - x)^{\top}\sf{H}_{c_2(x)}(z - x)
    \end{align}
    Since $\paren{1 - \frac{\epsilon}{8}} (1 + \epsilon)\paren{1 - \frac{\epsilon}{16}} > 1$, we have 
    \begin{align*}
        (y - x)^{\top}\sf{H}_{c_2(x)}(y - x) > (z - x)^{\top}\sf{H}_{c_2(x)}(z - x)
    \end{align*}
    
\end{proof}
    

    \newpage
   