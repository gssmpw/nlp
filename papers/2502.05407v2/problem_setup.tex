\section{Problem Setup}%: Two models of feature learning}

\begin{figure*}[t!] % 'htbp' suggests LaTeX to place the figure here, top, bottom, or on a special page
    \centering % Centers the entire figure
    % First Subfigure
    
        \includegraphics[width=1.06\textwidth]{allmono.png} % Includes the first image
    
    %\caption{ In this setting, we consider monomial regression where we sample random variables of dimension 10: $z \sim \cN(0, .5 \mathbb{I}_{10})$} and compute a target function $f^*(z) = z_0z_1\mathbf{1}(z_5 > 0)$. We train a kernel machine using RF\pphi (recursive feature machine), of the form $\hat{f}_{\pphi}(z) = \sum_{y_i \in \cD_{\sf{train}}} a_i \cdot K_{\pphi}(y_i,z)$, for 5 iterations on 4000 sampled training points and obtain a feature matrix $\pphi^*$ in rightmost plot (Ground truth). Each iteration of RF\pphi update for $\pphi^*$ is based on gradient computation of the form $\pphi = \sum_{z \in \cD_{\sf{train}}} (\frac{\partial \hat{f}_{\pphi}}{\partial z })(\frac{\partial \hat{f}_{\pphi}}{\partial z})^\top$. The rank of $\pphi^*$ is 4. In the next set of plots, we employ different feedback methods where a superagent/teacher provides feedbacks based on $\pphi^*$: (constructive) eigendecompostion and sparse construction; (sampling) random sampling with Gaussian distribution and sparse sampling where the probability of sparsity is .9 (sparsity: very high). Note that the three middle plots achieve the same MSE (mean squared error) as the ground truth feature matrix $\pphi^*$. But since the sparsity is pretty high, the approximated feature matrix with the same number of feedbacks: 55 is not sufficient and both the feature matrix and computed MSE are poor (see Theorem for discussion).}
    \caption{
%In this setting, we consider monomial regression by sampling 10-dimensional random variables \( z \sim \cN(0, 0.5\, \mathbb{I}_{10}) \) and defining the target function \( f^*(z) = z_0 z_1 \mathbf{1}(z_5 > 0) \). We train a kernel machine using the Recursive Feature Machine (RFM) approach, expressed as \( \hat{f}_{\pphi}(z) = \sum_{y_i \in \cD_{\sf{train}}} a_i \cdot K_{\pphi}(y_i, z) \), over 5 iterations with 4,000 sampled training points, resulting in the ground truth feature matrix \( \pphi^* \) shown in the rightmost plot (Ground truth). Each RFM iteration updates \( \pphi^* \) based on the gradient computation \( \pphi = \sum_{z \in \cD_{\sf{train}}} \left( \frac{\partial \hat{f}_{\pphi}}{\partial z} \right) \left( \frac{\partial \hat{f}_{\pphi}}{\partial z} \right)^\top \), achieving a rank of 4. In the subsequent plots, we employ different feedback methods where a superagent (teacher) provides feedback based on \( \pphi^* \): (constructive) eigendecomposition and sparse construction; and (sampling) random sampling with Gaussian distribution and sparse sampling with a sparsity probability of 0.9 (very high sparsity). Notably, the three central plots achieve the same Mean Squared Error (MSE) as the ground truth feature matrix \( \pphi^* \). However, due to the high sparsity, the approximated feature matrices with the same number of feedbacks (55) are insufficient, resulting in poor feature matrices and computed MSE (see Theorem for discussion).
\textbf{Features via Recursive Features Machines}: We consider monomial regression on 10-dimensional variables \( z \sim \mathcal{N}(0, 0.5\, \mathbb{I}_{10}) \) with the target function \( f^*(z) = z_0 z_1 \mathbf{1}(z_5 > 0) \). 
We train a kernel machine defined as \( \hat{f}_{\pphi}(z) = \sum_{y_i \in \mathcal{D}_{\text{train}}} a_i \cdot K_{\pphi}(y_i, z) \) using the RFM~\citep{rfm}, over 5 iterations with 4,000 training samples, resulting in the ground truth feature matrix \( \pphi^* \) of rank 4 (see \secref{sec: experiments} for discussion on training process). 
%Each RFM iteration updates \( \pphi^* \) through gradient computations \( \pphi = \sum_{z \in \cD_{\sf{train}}} ( \frac{\partial \hat{f}_{\pphi}}{\partial z} ) ( \frac{\partial \hat{f}_{\pphi}}{\partial z} )^\top \), achieving a rank of 4. 
Subsequently, we apply various feedback methods from an agent, including eigendecomposition (\thmref{thm: constructgeneral}) and sparse construction (\thmref{thm: constructsparse}) in \secref{sec: construct} , random Gaussian sampling (\thmref{thm: samplegeneral}), and sparse sampling (\thmref{thm: samplingsparse}) with a sparsity probability of 0.9 as shown in \secref{sec: sample}. Notably, the central methods attain the same Mean Squared Error (MSE) as the ground truth \( \pphi^* \) with 55 feedbacks, whereas the highly sparse approach produces poor feature matrices and higher MSE.
}
    
    %On the right-hand side in (b), the teacher provides (at most) 10 constructive feedbacks (rank of $\pphi^*$ is 4) to teach $\pphi^*$. We visualize the oblivious solution learned as $\hat{M}$. MSE on 4000 test samples: with the ground truth matrix-\textbf{0.0022}, and with the feature matrix with feedback-\textbf{0.00219}.
    
    % Main caption for the entire figure
    \label{fig: monoconst} % Label for referencing the entire figure
\end{figure*}

%\paragraph{Notations:} For a given matrix $\pphi \in \reals^{p\times p}$ and indices $i,j \in \bracket{p}$ $\pphi_{ij}$ denotes the entry of $\pphi$ at $ith$ row and $jth$ column. Matrices are denoted as $\pphi,\pphi',\Sigma$. Unless stated otherwise, a target matrix (for teaching a mahalanobis metric) is denoted as $\pphi^*$. We denote that null set of a matrix $\pphi$, i.e. $\curlybracket{x \in \reals^d\,|\, \pphi x = 0}$, as $\nul{\pphi}$.For a matrix, we denote its eigenvalues as $\gamma,\lambda, \gamma_i$ or $\lambda_i$ and its eigenvectors (orthogonal vectors) as $\mu_i ,u_i$ or $v_i$. We define the element-wise product of two matrices $\pphi,\pphi'$ via an inner product $\inner{\pphi', \pphi} := \sum_{i,j} \pphi'_{ij}\pphi_{ij}$. We denote vectors in $\reals^d$ as $x,y$ or $z$.Note, for any $x \in \reals^d$ $\inner{xx^{\top}, \pphi} = x^{\top}\pphi x$. For ease of notation, we also write the inner product as $\pphi \idot \pphi'$.

%We denote the space of symmetric matrices in $\reals^{d \times d}$ as $\symm$, and similarly the space of symmetric, positive semi-definite matrices as $\symmp$. Since the space of matrices on $\reals^{d\times d}$ is isomorphic to the Euclidean vector space $\reals^{d^2}$ for any matrix $\pphi$ we also call it a vector identified as an element of $\reals^{d^2}$. We say two matrices $\pphi,\pphi'$ are \tt{orthogonal}, denoted as $\pphi \bot \pphi'$, if $\pphi \idot \pphi' = 0$. For a set of vectors/matrices $\cC \subset \reals^{d\times d}$, the subspace induced by the elements in $\cC$ is denoted as $span \inner{\cC} := \curlybracket{\alpha \pphi + \beta \pphi' \,|\, \pphi,\pphi' \in \cC,\, \alpha, \beta \in \reals}$. Similarly, the set of columns of a matrix $\pphi$ is denoted as $\col{\pphi}$, and its span as $span \inner{\col{\pphi}}$.
We denote by $\cV \subseteq \reals^p$ the space of activations or representations and by $\cX \subseteq \reals^d$ the space of samples. For the space of feature matrices (for a dictionary or Mahalanobis distances), denoted as $\cM_{\mathsf{F}}$, we consider the family of symmetric positive semi-definite matrices in $\reals^{p \times p}$, i.e. $ \cM_{\mathsf{F}} = \curlybracket{\pphi \in \symmp}$. We denote a feedback set as $\cF$ which consists of triplets $(x,y,z) \in \cV^3$ with corresponding signs $\ell \in \curlybracket{+1,0,-1}$.

We use the standard notations in linear algebra over a space of matrices provided in \appref{app: notations}.

\iffalse
In this work, we use the standard notations in linear algebra over a space of matrices described as follows:

\paragraph{Notations}: For a given matrix $\pphi \in \reals^{p\times p}$ and indices $i,j \in \bracket{p}$ $\pphi_{ij}$ denotes the entry of $\pphi$ at $ith$ row and $jth$ column. 
%Matrices are denoted as $\pphi,\pphi',\Sigma$.
Unless stated otherwise, a target matrix (for teaching a mahalanobis metric) is denoted as $\pphi^*$. We denote that null set of a matrix $\pphi$, i.e. $\curlybracket{x \in \reals^p\,|\, \pphi x = 0}$, as $\nul{\pphi}$; whereas $
\kernel{\pphi}$ for the kernel of the matrix.% For a matrix, we denote its eigenvalues as $\gamma,\lambda, \gamma_i$ or $\lambda_i$ and its eigenvectors (orthogonal vectors) as $\mu_i ,u_i$ or $v_i$. 
We define the element-wise product of two matrices $\pphi,\pphi'$ via an inner product $\inner{\pphi', \pphi} := \sum_{i,j} \pphi'_{ij}\pphi_{ij}$. 
We denote vectors in $\reals^p$ as $x,y$ or $z$.
Note, for any $x \in \reals^p$ $\inner{\pphi, xx^{\top}} = x^{\top}\pphi x$. For ease of notation, we also write the inner product as $\pphi \idot \pphi'$.
We denote the space of symmetric matrices in $\reals^{p \times p}$ as $\symm$, and similarly the space of symmetric, positive semi-definite matrices as $\symmp$.
Since the space of matrices on $\reals^{p\times p}$ is isomorphic to the Euclidean vector space $\reals^{p^2}$ for any matrix $\pphi$ we also call it a vector identified as an element of $\reals^{p^2}$. We say two matrices $\pphi,\pphi'$ are \tt{orthogonal}, denoted as $\pphi \bot \pphi'$, if $\pphi \idot \pphi' = 0$. %For a set of vectors/matrices $\cC \subset \reals^{p\times p}$, the subspace induced by the elements in $\cC$ is denoted as $span \inner{\cC} := \curlybracket{a \pphi + b \pphi' \,|\, \pphi,\pphi' \in \cC,\, a,b \in \reals}$. %Similarly, the set of columns of a matrix $\pphi$ is denoted as $\col{\pphi}$, and its span as $span \inner{\col{\pphi}}$.
\akash{provide the section numbers in the paper}
\fi
% \paragraph{Triplet feedback} Agent specifies feedbacks on activations in $\cV$ via relative triplet comparisons $(x,y,z) \in \cV$ with the following semantic understanding,
% \begin{gather*}
%     x_1 u_1(\text{"feature $1$"}) + \ldots + x_p u_p(\text{"feature $p$"}) \\ \text{ is similar to } \\
%     y_1 u_1(\text{"feature $1$"}) + \ldots + y_p u_p(\text{"feature $p$"}) \\ \text{ than } \\
%     z_1 u_1(\text{"feature $1$"}) + \ldots + z_p u_p(\text{"feature $p$"})
% \end{gather*}
% In this study we consider both sparse and non-sparse activations as feedbacks. For sparse activations, we consider the standard notion of $s$-sparsity for $s \ge p$ as follows
% %\akash{This doesn't make sense. One needs to specify what are these $\curlybracket{u_i}$'s. Teacher could only specify $\pphi^*$, but this requires a decomposition.}

% %\akash{provide the definition of sparse activations here}
% \begin{definition}[$s$-sparse activations]\label{def: sparse}
%     We say an activation $\alpha \in \reals^p$ is $s$-sparse if only $s$ many indices are non-zero, i.e. $$|\curlybracket{i : \alpha_i \neq 0}| = s$$
% \end{definition}
% Since any comparative triplet feedback is agostic to a positive linear scaling of a feature matrix we consider the following equivalence of feature matrices.

% \begin{definition}[Feature equivalence] For a feature family $\cM_{\sf{F}}$, we say feature matrices $\pphi'$ and $\pphi^*$ are equivalent if there exists a scalar $\lambda > 0$ such that $\pphi' = \lambda\cdot \pphi^*$.
% \end{definition}
\paragraph{Triplet feedback} An agent provides feedback on activations in $\cV$ through relative triplet comparisons $(x,y,z) \in \cV$. Each comparison evaluates linear combinations of feature vectors:\vspace{-3mm}
\begin{gather*}
\sum_{i=1}^p x_i u_i(\text{"feature $i$"}) \ \text{ is more similar to } \vspace*{-6mm}\\
\sum_{i=1}^p y_i u_i(\text{"feature $i$"}) \ \text{ than to } \
\sum_{i=1}^p z_i u_i(\text{"feature $i$"})
\end{gather*}
\looseness-1 We study both sparse and non-sparse activation feedbacks, where sparsity is defined as:
\begin{definition}[$s$-sparse activations]\label{def: sparse}
An activation $\alpha \in \reals^p$ is $s$-sparse if at most $s$ many indices of $\alpha$ are non-zero.\vspace{-3mm}
\end{definition}
Since triplet comparisons are invariant to positive scaling of feature matrices, we define:
\begin{definition}[Feature equivalence]
For a feature family $\cM_{\sf{F}}$, feature matrices $\pphi'$ and $\pphi^*$ are equivalent if there exists $\lambda > 0$ such that $\pphi' = \lambda\cdot \pphi^*$.
\end{definition}


We study a simple learning framework where the learner merely satisfies the constraints provided by the agent's feedback:
\begin{definition}[Oblivious learner]
A learner is oblivious if it randomly selects a feature matrix from the set of valid solutions to a given feedback set $\cF$, i.e., arbitrarily chooses $\pphi \in \cM_{\sf{F}}(\cF)$, where $\cM_{\sf{F}}(\cF)$ represents the set of feature matrices satisfying the constraints in $\cF$.
\end{definition}
This framework aligns with version space learning, where $\textsf{VS}(\cF,\cM_{\sf{F}})$ denotes the set of feature matrices in $\cM_{\sf{F}}(\cF)$ compatible with feedback set $\cF$.

Prior work on dictionary learning has established recovery up to linear transformation under weak mutual incoherence (e.g., cf \cite{gribonval_rotation}). In our setting, when the agent provides feature feedback corresponding to $\dd \text{ (or $\sf{L}$)} \in \reals^{d \times p}$, the learner recovers $\sf{L}$ up to normal transformation. Moreover, when $\sf{L}$ has orthogonal rows (strong incoherence), we can recover $\sf{L}$ up to rotation and sign changes as stated below, with proof deferred to \appref{app:atom}.
% i.e., $\dd^{\top}\dd$ using triplet queries $(0, \beta, y) \in (\reals^{p})^3$.
% \begin{align*}
%     \beta^{\top} \dd^{\top}\dd\beta = y^\top \dd^{\top}\dd y
%     \implies \beta^\top \pphi \beta = y^{\top}\pphi y
% \end{align*}
% such that the sparsity coefficient of $\beta, y$ is high, i.e $\cS(\beta) = \bigO{1}$. Note that $\mathbf{\Phi} \succeq 0$ and is symmetric.

% Recent work has explored this extensively. This is a common framework for dictionary learning.\akash{check if this lemma is true!}

\begin{lemma}[Recovering orthogonal representations]\label{lem: ortho}
    Assume \( \pphi \in \symmp \) . Define the set of orthogonal Cholesky decompositions of \( \pphi \) as
    \[
        \cW_{\sf{CD}} = \left\{ \textbf{U} \in \reals^{p \times r} \,\bigg|\, \pphi = \textbf{U} \textbf{U}^\top \&\, \textbf{U}^\top \textbf{U} = \text{diag}(\lambda_1,\ldots, \lambda_r) \right\},
    \]
    where \( r = \text{rank}(\pphi) \) and \( \lambda_1, \lambda_2, \ldots, \lambda_r \) are the eigenvalues of $\pphi$ in descending order. Then, for any two matrices \( \textbf{U}, \textbf{U}' \in \cW_{\sf{CD}} \), there exists an orthogonal matrix \( \textbf{R} \in \reals^{r \times r} \) such that $\textbf{U}' = \textbf{U} \textbf{R}$, where \( \textbf{R} \) is block diagonal with orthogonal blocks corresponding to any repeated diagonal entries \( \lambda_i \) in \( \textbf{U}^\top \textbf{U} \). Additionally, each column of \( \textbf{U}' \) can differ from the corresponding column of \( \textbf{U} \) by a sign change.
\end{lemma}
We note that the recovery of $\sf{L}$ is pertaining to the assumption that all the rows are orthogonal, and thus rank of \sf{L} is $r = d$. In cases where $r < d$, one needs additional information in the form of ground sample $\bx = \sf{L}\alpha$ for some activation $\alpha$ to recover $\sf{L}$ up to a linear transformation.
%\akash{It is not clear how to argue for the non-orthogonal setting. For the orthogonal setting, one can argue that there is an orthogonal atoms for which the lemma below can be applied.}
\hfill
%\akash{We need to provide the exact mechanism of comparison in terms of feedback that relates the dictionary atoms!}
%\begin{definition}[Feedback complexity] For a given representation space $\cV \subset \reals^p$ and a target feature matrix $\pphi^* \in \cM_{\sf{F}}$, we define the feedback complexity $\cF_{\sf{C}}(\pphi^*)$ as the minimal number of triplets required such that as the number of samples a agent needs.....
%\end{definition}
Finally, the interaction protocol is shown in \algoref{alg: main}. 
%\akash{this model doesn't capture both the constructive and sampling models}
\begin{algorithm}[t]
\caption{Model of Feature learning with feedback}
\label{alg: main}
\textbf{Given}: Representation space $\cV \subseteq \reals^p$, Feature family $\cM_{\mathsf{F}}$\vspace{2mm}\\
\textit{In batch setting:}
\begin{enumerate}
    \item Agent picks triplets $\cF(\cV, \pphi^*) = $\vspace{-2mm} $$\hspace{-7mm}\curlybracket{(x,y,z) \in \cV^3 \,|\, (x-y)^{\top}\pphi^*(x-y) \ge (x-z)^{\top}\pphi^*(x-z)}\vspace{-2mm}$$
    \item Learner receives $\cF$, and obliviously picks a feature matrix $\pphi \in \cM_{\mathsf{F}}$ that satisfy the set of constraints in $\cF(\cV, \pphi^*)$\vspace{-2mm}
    \item Learner outputs $\pphi$.\vspace{-2mm}
    %as per \eqnref{eq: sol}
\end{enumerate}
\end{algorithm}
%On any triplet $x,y,z \in \cV$ and a given feature matrix $\pphi$, we specify the feedback 
%$d_{\pphi}(x,y)$ and $d_{\pphi}(x,z)$ as a label $\ell((x,y,z);\pphi) \in \curlybracket{<,=,>}$ depending on the sign of $d_{\pphi}(x,y) - d_{\pphi}(x,z)$. We call this \tt{triplet comparison}.
%We consider hard labels-- $<,=,>$ instead of providing $\le,\ge$. It is straightforward that relaxed labels could lead to a trivial distance function, i.e., for any family of distance function $d(x,x') = 1(x \neq x')$ satisfies any such relaxed labeling.

\iffalse
\subsection{Problem Setup}\label{sec: setup}
%\begin{enumerate}
%    \item \textbf{To what extent can a distance function be specified by comparisons alone?}
 %   \begin{enumerate}
  %      \item Quadruples $\rightarrow$ up to ordinal equivalence
   %     \item Triplets $\rightarrow$ up to local ordinal equivalence
    %    \begin{itemize}
     %       \item $\{x_1, y_1, z, w_2\} \rightarrow d(x_1, y_1) > d(z, w_2)$ if $d(x_1, y_1) > d(z, w_2)$
      %      \item $\{x_1, y_1, z\} \rightarrow d(x_1, y_1) > d(z, w_2)$ if $d(x_1, y_1) > d(x_1, z)$
       % \end{itemize}
    %\end{enumerate}
    
    %\item \textbf{General distance functions on finite spaces}
    %\begin{enumerate}
     %   \item $\lvert \mathcal{X} \rvert = N$, can use any triplets/quadruplets
      %  \item How many are needed?
       % \begin{itemize}
        %    \item Quadruplets $\longrightarrow \binom{N}{2} - 1$ \textit{(Sufficient)}
         %   \item Triplets $\longrightarrow N(N - 2)$ \textit{(Sufficient)}
         %   \item Lower bound: $\binom{N}{2} - 1$ for both
       % \end{itemize}
    %\end{enumerate}

   % \item \textbf{Smooth distance functions}
   % \begin{enumerate}
    %    \item Taylor approximation with remainder
     %   \item Naive method:
      %  \begin{itemize}
       %     \item Pick $\epsilon$-cover $\mathcal{C}$ of $\mathcal{X}$ with respect to $d_{\ell_2}$
        %    \item Specify $d_{\ell}$ using triples
         %   \item For now, $x, y, z$
          %  \begin{itemize}
           %     \item Look at $\ell_2$-NNs $\tilde{x}, \tilde{y}, \tilde{z}$
            %    \item Answer using 
            %\end{itemize}
            %\item What does this give us?
        %\end{itemize}
        %\item Idea for improvement: local Mahalanobis
    %\end{enumerate}

    %\item \textbf{Specifying Mahalanobis distance}
    
    %\item \textbf{A better approximation for smooth distance functions}
    
    %\item \textbf{Experiments?}
    
%\end{enumerate}
We denote by $\cX$ a space of objects/inputs. In this work, we are interested in a distance function \(d\), which is defined as follows:
%\sanjoy{In our case, the first result (for finite $\cX$) applies to any distance function while the main result applies only to metrics. It is worth beign careful about these specifics because in ML there is a lot of interest in non-metric distances.}

%\sanjoy{I would suggest the following: start with a \emph{distance function} being any $d: \cX \times \cX \to \mathbb{R}$ that has $d(x,x') \geq 0$ and $d(x,x) = 0$. Then define symmetry, positivity, and triangle inequality, as you do below, and say that a distance function with all three properties is a metric.}

\begin{definition}[distance function] Consider a space of objects/inputs \(\cX\). Consider a bivariate function \(d: \cX \times \cX \to \mathbb{R}\). We say the pair \((\cX, d)\) forms a distance function if 
$d(x,x') \geq 0$ and $d(x,x) = 0$ for all $x, x' \in \cX$.    
\end{definition}
A well-studied family of distance functions includes metrics that satisfy certain niceness properties.
\begin{definition}[metric]
    Consider a space of objects/inputs \(\cX\). Consider a distance function \(d: \cX \times \cX \to \mathbb{R}\). We say \(d\) forms a metric if it satisfies the following properties:
    \begin{enumerate}
        \item (positivity) For all \(x \neq x' \in \cX\), \(d(x, x') > 0\). Furthermore, for any \(x \in \cX\), \(d(x, x) = 0\). 
        \item (symmetry) For all \(x, x' \in \cX\), \(d(x, x') = d(x', x)\).
        \item (triangle inequality) For all \(x, x', x'' \in \cX\), \(d(x, x') + d(x', x'') \ge d(x, x'')\).
    \end{enumerate}
\end{definition}

Denote by \(\cH_{\cX}\) the collection of all possible metrics on \(\cX\), i.e., \(\cH_{\cX} := \{d: \cX \times \cX \to \mathbb{R} \, |\, d \textit{ is a metric}\}\). We are interested in studying certain subsets \(\cM \subset \cH_{\cX}\), e.g., \(\cM\) is a collection of Mahalanobis distance functions, or \(\cM\) is a tree/graph metric, or \(\cM\) is a family of smooth metrics. We describe two classes of interest in this work:

\paragraph{Mahalanobis Distance function/metric}
Assume that \(\cX \subset \mathbb{R}^d\). A \textit{Mahalanobis} distance function is characterized by a symmetric, positive semi-definite matrix ($M \succeq 0$), or alternately \(M \in \textsf{symm}_{+}(\mathbb{R}^d)\), denoted in this work as \(d_M\), such that 
\begin{align}
    \forall x, x' \in \cX,\, d_M(x, x') = \sqrt{(x - x')^{\top}M(x - x')} \label{eq: maha}
\end{align}
Note that if \(M\) is symmetric and positive definite, i.e. $\pphi \succ 0$, \(d_M\) satisfies all the necessary properties of a metric. 
%\sanjoy{Here is some useful (and standard) notation: $\pphi \succeq 0$ means $M$ is positive semidefinite and $\pphi \succ 0$ means $M$ is positive definite. In the definition above, we actually need $M$ to be positive definite in order to satisfy the positivity property.}
%\sanjoy{In your notation, does $\mathbb{R}_+$ include 0 or not? Need to be clear about it.}

%\paragraph{Tree and Graph Metric}
%Assume $\cX$ is a set of objects. Consider a weighted graph \(G(\cX, E, w)\) on \(\cX\) with edge set \(E\), and weighting \(w: E \to \mathbb{R}_{+}\) ($\reals_{+}$: all non-negative reals) on \(E\). We say \(w\) \textit{induces} a metric on \(G\), denoted as \(d_G\), if:
%\begin{align*}
%    \forall u,v \in \cX,\quad d_G(u, v) := \min_{p \in P_G(u, v)} \sum_{e' \in p} w(e')
%\end{align*}
%where \(P_G(u, v)\) is the set of all paths connecting the nodes \(u, v\) in \(G\). It is straightforward to show that \(d_G\) is metric. 
%We call \(d_G\), a metric on the weighted graph\((\cX, G, w)\), a \textit{graph metric}. If \(G\) is a tree, then we call \(d_G\) a \textit{tree metric}.

%\sanjoy{In the definition below, the term \emph{metric model} is unclear. Maybe use \emph{family of distance functions}?}

\paragraph{Comparisons of distances} In this work, we specify distances on points in $\cX$ via two types of comparisons-- triplet and quarduples. On any triplet $x,y,z \in \cX$ and a given distance function, we specify the distances $d(x,y)$ and $d(x,z)$ as a label $\ell((x,y,z);d) \in \curlybracket{<,=,>}$ depending on the sign of $d(x,y) - d(x,z)$. We call this \tt{triplet comparison}. Similarly, on any quadruplets $x,y,u,v \in \cX$, we specify $d(x,y)$ and $d(u,v)$ via comparisons: $<$ or $=$ or $>$ depending on the sign of $d(x,y) - d(u,v)$. We call this \tt{quadruple comparison}. 

We consider hard labels-- $<,=,>$ instead of providing $\le,\ge$. It is straightforward that relaxed labels could lead to a trivial distance function, i.e., for any family of distance function $d(x,x') = 1(x \neq x')$ satisfies any such relaxed labeling.

\paragraph{Oblivious teaching of metric learning} Consider a two-agent setup: a learner (L)  and a agent (T). Given a family of distance functions $\cM$, the agent is tasked with teaching a metric to the learner from this space. The agent is aware of the family of distance functions $\cM$, and the underlying object/input space $\cX$. For a target metric $d^* \in \cM$, 
agent selects a set of triplets chosen from $\cF(\cX,d^*) :=$ $\curlybracket{(x,y,z)_{\ell} \in \cX^3 \,|\, \ell((x,y,z);d^*) = \sgn{d^*(x,y) - d^*(x,z)}}$ and provides it to the learner. We use the short notation $(x,y,z)_{\ell}$ to denote $(x,y,z; \ell((x,y,z);d))$, i.e both the triplet and its label corresponding to the distance function $d$ is specified.
%\sanjoy{We should be careful here. If a triplet $(x,y,z)$ only means $d(x,y) \geq d(x,z)$ then any set of triplets is satisfied by the trivial distance metric $d(x,y) = 1(x \neq y)$.}
%\sanjoy{Okay, at this point, the issue arises of $\geq$ versus $=$ and $>$. I think one clean way of handling it is to define triples to simply be all of $\cX^3$ and quadruples to be all of $\cX^4$, and to talk separately about the \emph{label} of a triple or quadruple under a given distance function $d$. For much of the paper we will take the label $\ell((x,y,z); d)$ to be one of $\{<, = , >\}$, depending on the sign of $d(x,y)-d(x,z)$. You can explain why the labels $\{\leq, \geq\}$ are not enough: because the trivial distance function $d(x,x') = 1(x \neq x')$ will always satisfy any labeling. Later, we will look at alternative label sets, such as $\{\ll, \approx, \gg\}$.}

We consider the case where the learner is aware of the family of distance functions $\cM$ but \tt{obliviously} finds a metric $d'$ from a version space in $\cM$ corresponding to $\cF$ as follows:
\begin{align}
    d' \in {\sf{VS}}(\cF, \cM, d^*) \label{eq: sol}%:= \curlybracket{d \in \cM\,|\, \forall (x,y,z) \in \cF,\, \ell((x,y,z);d) = \ell((x,y,z);d^*)} 
\end{align}
where ${\sf{VS}}(\cF, \cM, d^*) := \curlybracket{d \in \cM\,|\, \forall (x,y,z) \in \cF,\, \ell((x,y,z);d) = \ell((x,y,z);d^*)}$.
We call this setup, an \tt{oblivious teaching of a distance function}. We are interested in understanding the minimum number of triplet comparisons $\cF$ provided by the agent such that in \eqnref{eq: sol} either $d'$ is a unique solution, i.e. $d^*$ itself or $d'$ can be identified up to an equivalence class which contains $d^*$. 

This warrants a definition of minimal triplet comparison with respect to a relation. Let us denote a relation $\sim_R$ on $\cM$. We define the oblivious teaching complexity of a distance function wrt $\sim_R$ as follows:

\begin{definition}(oblivious teaching complexity of a target distance function)
    Consider an input space $\cX$, and a subset $\cM \subset \cH_{\cX}$ be a family of distance functions. We define oblivious teaching complexity of a target distance function $d^*$ in the family $\cM$ wrt the relation $\sim_R$ as follows:
    \begin{align*}
        TC_O(\cX, \cM, d^*) := \min_{\cF} \curlybracket{|\cF|: d' \sim_R d^* \ \forall d' \in \sf{VS}(\cF, \cM, d^*) }
    \end{align*}
\end{definition}
%\sanjoy{I think it should be something a bit different, like this: $\min_{\cF} \{|\cF|: d' \sim_R d^* \ \forall d' \in VS(\cF, \cM) \}$}

There are some remarks in order: if $d' \not \sim_R d$ for some $d',d \in \cM$, then we assume that $|\cF(\cX,d)| = \infty$. In this work, the relation $\sim_R$ is considered based on the underlying family of distance functions, e.g. Mahalanobis distance functions are equivalent up to positive scaling for triplet comparisons, whereas equivalence in finite sample distance functions could be studied up to monotonic transformations. We will discuss these relations in the following sections.

With this definition, we consider the worst-case teaching complexity over the space $\cM$ as follows:

\begin{definition}(worst-case teaching complexity)
    Consider an input space $\cX$, and a subset $\cM \subset \cH_{\cX}$ of distance functions. We define worst-case oblivious teaching complexity of the family of distance functions $\cM$ wrt the relation $\sim_R$ as follows:
    \begin{align*}
        TC_O(\cX, \cM) := \max_{d \in \cM} TC_O(\cX, \cM, d)%\min_{\substack{d' \sim_R d^* \\ d' \in \textsc{VS}(\cF(\cX,d^*))}} |\cF(\cX,d)|
    \end{align*}
\end{definition}
In the coming sections, we will discuss both the worst-case and targeted oblivious teaching complexity of different families of distance functions.
\fi 

