\section{Table of Contents}
Here, we provide the table of contents for the appendix of the supplementary.

\begin{itemize}
\item[-] \appref{app: notations} provides a comprehensive table of additional notations used throughout the paper and supplementary material.
\item[-] \appref{app:atom} contains the proof for \lemref{lem: ortho}, establishing conditions for recovering orthogonal representations.\vspace{2mm}

\item[-] \appref{app: worstcase} completes the proof of \propref{prop: worstcase}, establishing a worst-case lower bound on feedback complexity in the constructive setting.\vspace{2mm}

\item[-] \appref{app: constub} presents the proof for the upper bound in \thmref{thm: constructgeneral} for low-rank feature matrices.\vspace{2mm}

\item[-] \appref{app: constlb} establishes the proof for the lower bound in \thmref{thm: constructgeneral} for low-rank feature matrices.\vspace{2mm}

\item[-] \appref{app: samplegeneral} details the proof of \thmref{thm: samplegeneral} which asserts tight bounds on feedback complexity for general sampled activations.\vspace{2mm}

\item[-] \appref{app: samplesparse} demonstrates the proof of \thmref{thm: samplingsparse} establishing an upper bound on the feedback complexity for sparse sampled activations.\vspace{2mm}

\item[-] \appref{app: additional} provides supplementary experimental results validating our theoretical findings.\vspace{2mm}
\end{itemize}
\newpage

\section{Notations}\label{app: notations}
Here we provide the glossary of notations followed in the supplementary material.
\iffalse
For a given matrix $\pphi \in \reals^{p\times p}$ and indices $i,j \in \bracket{p}$ $\pphi_{ij}$ denotes the entry of $\pphi$ at $ith$ row and $jth$ column. Matrices are denoted as $\pphi,\pphi',\Sigma$. Unless stated otherwise, a target matrix (for teaching a mahalanobis metric) is denoted as $\pphi^*$. We denote that null set of a matrix $\pphi$, i.e. $\curlybracket{x \in \reals^p\,|\, \pphi x = 0}$, as $\nul{\pphi}$; whereas $
\kernel{\pphi}$ for the kernel of the matrix. For a matrix, we denote its eigenvalues as $\gamma,\lambda, \gamma_i$ or $\lambda_i$ and its eigenvectors (orthogonal vectors) as $\mu_i ,u_i$ or $v_i$. We define the element-wise product of two matrices $\pphi,\pphi'$ via an inner product $\inner{\pphi', \pphi} := \sum_{i,j} \pphi'_{ij}\pphi_{ij}$. 
We denote vectors in $\reals^p$ as $x,y$ or $z$.
Note, for any $x \in \reals^p$ $\inner{\pphi, xx^{\top}} = x^{\top}\pphi x$. For ease of notation, we also write the inner product as $\pphi \idot \pphi'$.

We denote the space of symmetric matrices in $\reals^{p \times p}$ as $\symm$, and similarly the space of symmetric, positive semi-definite matrices as $\symmp$.
Since the space of matrices on $\reals^{p\times p}$ is isomorphic to the Euclidean vector space $\reals^{p^2}$ for any matrix $\pphi$ we also call it a vector identified as an element of $\reals^{p^2}$. We say two matrices $\pphi,\pphi'$ are \tt{orthogonal}, denoted as $\pphi \bot \pphi'$, if $\pphi \idot \pphi' = 0$. For a set of vectors/matrices $\cC \subset \reals^{p\times p}$, the subspace induced by the elements in $\cC$ is denoted as $span \inner{\cC} := \curlybracket{a \pphi + b \pphi' \,|\, \pphi,\pphi' \in \cC,\, a,b \in \reals}$. Similarly, the set of columns of a matrix $\pphi$ is denoted as $\col{\pphi}$, and its span as $span \inner{\col{\pphi}}$.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Symbol} & \textbf{Description}\\
\hline
$\pphi, \Sigma$ & Feature matrix\\
$\cV \subset \reals^p$ & Activation/Representation space\\
$\alpha, \beta, x,y,z$ & Activations\\
$\cX \subset \reals^p$ & Ground truth sample space\\
$d$ & Dimension of ground-truth sample space\\
$p$ & Dimension of representation space\\
$r$ & Rank of a feature matrix\\
$\curlybracket{v_1, v_2, \ldots, v_r}$ & A set of orthonormal vectors, typically eigenvectors of $\pphi^*$\\
$V_{\bracket{r}}$ & The set $\curlybracket{v_1, v_2, \ldots, v_r}$ \\
$V_{\bracket{p - r}}$ & The set $\curlybracket{v_{r+1}, \ldots, v_p}$, forming an orthogonal extension to $V_{\bracket{r}}$ \\
$V_{\bracket{p}}$ & The complete orthonormal basis $\curlybracket{v_1, v_2, \ldots, v_p}$ \\
$\mathcal{O}_{\pphi^*}$ & Orthogonal complement of $\pphi^*$ in $\symm$\\
$\dd$ & Dictionary matrix\\
$\cD, \cD_{\sf{sparse}}$ & Distributions over activations\\
$\sf{VS}(\cF, \maha)$ & Version space of $\maha$ wrt feedback set $\cF$\\ 
$\symm$ & Space of symmetric matrices\\
$\symmp$ & Space of PSD, symmetric matrices\\
\hline
\end{tabular}
\end{table}
\fi

% \begin{table}[h]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
% \textbf{Symbol} & \textbf{Description}\\
% \hline
% \parbox{3cm}{$\alpha, \beta, x,y,z$} & Activations\\
% $\col{\pphi}$ & Set of columns of matrix $\pphi$\vspace{1mm}\\
% $\cD, \cD_{\sf{sparse}}$ & Distributions over activations\vspace{1mm}\\
% $d$ & Dimension of ground-truth sample space\vspace{1mm}\\
% $\dd$ & Dictionary matrix\vspace{1mm}\\
% $\gamma,\lambda, \gamma_i, \lambda_i$ & Eigenvalues of a matrix\vspace{1mm}\\
% $\inner{\pphi', \pphi}$ & Element-wise product (inner product) of matrices\vspace{1mm}\\
% $\kernel{\pphi}$ & Kernel of matrix $\pphi$\vspace{1mm}\\
% $\mu_i ,u_i, v_i$ & Eigenvectors (orthogonal vectors)\vspace{1mm}\\
% $\nul{\pphi}$ & Null set of matrix $\pphi$\vspace{1mm}\\
% $\mathcal{O}_{\pphi^*}$ & Orthogonal complement of $\pphi^*$ in $\symm$\vspace{1mm}\\
% $p$ & Dimension of representation space\vspace{1mm}\\
% $\pphi, \Sigma$ & Feature matrix\vspace{1mm}\\
% $\pphi_{ij}$ & Entry at $i$th row and $j$th column of $\pphi$\vspace{1mm}\\
% $\pphi^*$ & Target feature matrix\vspace{1mm}\\
% $r$ & Rank of a feature matrix\vspace{1mm}\\
% $\symm$ & Space of symmetric matrices\vspace{1mm}\\
% $\symmp$ & Space of PSD, symmetric matrices\vspace{1mm}\\
% $\sf{VS}(\cF, \maha)$ & Version space of $\maha$ wrt feedback set $\cF$\vspace{1mm}\\
% $V_{\bracket{r}}$ & The set $\curlybracket{v_1, v_2, \ldots, v_r}$\vspace{1mm}\\
% $V_{\bracket{p - r}}$ & The set $\curlybracket{v_{r+1}, \ldots, v_p}$\vspace{1mm}\\
% $V_{\bracket{p}}$ & Complete orthonormal basis $\curlybracket{v_1, v_2, \ldots, v_p}$\vspace{1mm}\\
% $\cV \subset \reals^p$ & Activation/Representation space\vspace{1mm}\\
% $\cX \subset \reals^d$ & Ground truth sample space\vspace{1mm}\\
% \hline
% \end{tabular}
% \end{table}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\parbox{3cm}{\textbf{Symbol}} & \parbox{7cm}{\textbf{Description}}\\
\hline
\parbox{3cm}{$\alpha, \beta, x,y,z$} & \parbox{7cm}{Activations}\\
\parbox{3cm}{$\col{\pphi}$} & \parbox{7cm}{Set of columns of matrix $\pphi$}\\
\parbox{3cm}{$\cD, \cD_{\sf{sparse}}$} & \parbox{7cm}{Distributions over activations}\\
\parbox{3cm}{$d$} & \parbox{7cm}{Dimension of ground-truth sample space}\\
\parbox{3cm}{$\dd$} & \parbox{7cm}{Dictionary matrix}\\
\parbox{3cm}{$\gamma,\lambda, \gamma_i, \lambda_i$} & \parbox{7cm}{Eigenvalues of a matrix}\\
\parbox{3cm}{$\inner{\pphi', \pphi}$} & \parbox{7cm}{Element-wise product (inner product) of matrices}\\
\parbox{3cm}{$\kernel{\pphi}$} & \parbox{7cm}{Kernel of matrix $\pphi$}\\
\parbox{3cm}{$\mu_i ,u_i, v_i$} & \parbox{7cm}{Eigenvectors (orthogonal vectors)}\\
\parbox{3cm}{$\nul{\pphi}$} & \parbox{7cm}{Null set of matrix $\pphi$}\\
\parbox{3cm}{$\mathcal{O}_{\pphi^*}$} & \parbox{7cm}{Orthogonal complement of $\pphi^*$ in $\symm$}\\
\parbox{3cm}{$p$} & \parbox{7cm}{Dimension of representation space}\\
\parbox{3cm}{$\pphi, \Sigma$} & \parbox{7cm}{Feature matrix}\\
\parbox{3cm}{$\pphi_{ij}$} & \parbox{7cm}{Entry at $i$th row and $j$th column of $\pphi$}\\
\parbox{3cm}{$\pphi^*$} & \parbox{7cm}{Target feature matrix}\\
\parbox{3cm}{$r$} & \parbox{7cm}{Rank of a feature matrix}\\
\parbox{3cm}{$\symm$} & \parbox{7cm}{Space of symmetric matrices}\\
\parbox{3cm}{$\symmp$} & \parbox{7cm}{Space of PSD, symmetric matrices}\\
\parbox{3cm}{$\sf{VS}(\cF, \maha)$} & \parbox{7cm}{Version space of $\maha$ wrt feedback set $\cF$}\\
\parbox{3cm}{$V_{\bracket{r}}$} & \parbox{7cm}{The set $\curlybracket{v_1, v_2, \ldots, v_r}$}\\
\parbox{3cm}{$V_{\bracket{p - r}}$} & \parbox{7cm}{The set $\curlybracket{v_{r+1}, \ldots, v_p}$}\\
\parbox{3cm}{$V_{\bracket{p}}$} & \parbox{7cm}{Complete orthonormal basis $\curlybracket{v_1, v_2, \ldots, v_p}$}\\
\parbox{3cm}{$\cV \subset \reals^p$} & \parbox{7cm}{Activation/Representation space}\\
\parbox{3cm}{$\cX \subset \reals^d$} & \parbox{7cm}{Ground truth sample space}\\
\hline
\end{tabular}
\end{table}

\newpage

% \textbf{Vectors and Sets:}
% \begin{itemize}
%     \item $\curlybracket{v_1, v_2, \ldots, v_r}$: A set of orthonormal vectors, typically eigenvectors of $\pphi^*$.
%     \item $V_{\bracket{r}}$: The set $\curlybracket{v_1, v_2, \ldots, v_r}$.
%     \item $V_{\bracket{d - r}}$: The set $\curlybracket{v_{r+1}, \ldots, v_p}$, forming an orthogonal extension to $V_{\bracket{r}}$.
%     \item $V_{\bracket{d}}$: The complete orthonormal basis $\curlybracket{v_1, v_2, \ldots, v_p}$.
%     \item $\cX$: The domain from which feedback pairs $(y, z)$ are drawn.
%     \item $\cX^2$: The Cartesian product of $\cX$ with itself, representing pairs $(y, z) \in \cX \times \cX$.
% \end{itemize}

% \textbf{Feedback Sets:}
% \begin{itemize}
%     \item $\cF_{\sf{null}}$: A partial feedback set consisting of pairs $\curlybracket{(0, v_i)}_{i = r+1}^p$ used to teach the null space of $\pphi^*$.
%     \item $\mathcal{F}(\cX, \textsf{VS}(\maha, \cF_{\sf{null}}), \pphi^*)$: The feedback set formulated to operate within the version space $\textsf{VS}(\maha, \cF_{\sf{null}})$ for the target matrix $\pphi^*$.
% \end{itemize}

% \textbf{Version Space and Metrics:}
% \begin{itemize}
%     \item $\maha$: A metric space relevant to the version space formulation (specific definition to be provided based on context).
%     \item $\textsf{VS}(\maha, \cF_{\sf{null}})$: The version space restricted by the feedback set $\cF_{\sf{null}}$ within the metric space $\maha$.
% \end{itemize}

% \textbf{Relations and Operators:}
% \begin{itemize}
%     \item $\idot$: Inner product between two matrices, typically defined as $\pphi \idot \psi = \text{trace}(\pphi^{\top} \psi)$.
%     \item $\sim_{R_l}$: An equivalence relation denoting linear scaling, i.e., $\pphi \sim_{R_l} \pphi'$ if $\pphi' = c \pphi$ for some scalar $c > 0$.
%     \item $\succeq 0$: Denotes that a matrix is positive semidefinite (PSD), i.e., $\pphi \succeq 0$ means $\pphi$ is PSD.
% \end{itemize}

% \textbf{Indices and Sets:}
% \begin{itemize}
%     \item $\bracket{n}$: The set $\{1, 2, \ldots, n\}$.
%     \item $\curlybracket{v_{l_k}, v_{m_k}}$: Pairs of vectors used in linear combinations within feedback sets or basis constructions.
% \end{itemize}
%  \textbf{Other Symbols:}
% \begin{itemize}
%     \item $\mathds{1}[\cdot]$: The indicator function, where $\mathds{1}[P] = 1$ if predicate $P$ is true, and $0$ otherwise.
%     \item $\mathcal{B}$: A basis set of rank-1 symmetric matrices constructed from the eigenvectors $\curlybracket{v_i}$.
%     \item $\mathcal{O}_{\cB}$: A set of matrices derived from $\mathcal{B}$, adjusted by scaling with a vector $y$.
%     \item $\lambda_{ij}$: Scaling factors defined as $\lambda_{ii} = \frac{v_i \pphi^* v_i^{\top}}{y \pphi^* y^{\top}}$ and $\lambda_{ij} = \frac{(v_i + v_j) \pphi^* (v_i + v_j)^{\top}}{y \pphi^* y^{\top}}$ for $i \neq j$.
% \end{itemize}



\section{Proof of \lemref{lem: ortho}}\label{app:atom}
In this appendix we restate and provide the proof of \lemref{lem: ortho}. 
\begingroup
\renewcommand\thelemma{\ref{lem: ortho}} 
\begin{lemma}[Recovering orthogonal atoms]%\label{lem: ortho}
    Let \( \pphi \in \reals^{p \times p} \) be a symmetric positive semi-definite matrix. Define the set of orthogonal Cholesky decompositions of \( \pphi \) as
    \[
        \cW_{\sf{CD}} = \left\{ \textbf{U} \in \reals^{p \times r} \,\bigg|\, \pphi = \textbf{U} \textbf{U}^\top \text{ and } \textbf{U}^\top \textbf{U} = \text{diag}(\lambda_1,\ldots, \lambda_r) \right\},
    \]
    where \( r = \text{rank}(\pphi) \) and \( \lambda_1, \lambda_2, \ldots, \lambda_r \) are the eigenvalues of $\pphi$ in descending order. Then, for any two matrices \( \textbf{U}, \textbf{U}' \in \cW_{\sf{CD}} \), there exists an orthogonal matrix \( R \in \reals^{r \times r} \) such that
    \[
        \textbf{U}' = \textbf{U} \textbf{R},
    \]
    where \( \textbf{R} \) is block diagonal with orthogonal blocks corresponding to any repeated diagonal entries \( d_i \) in \( \textbf{U}^\top \textbf{U} \). Additionally, each column of \( \textbf{U}' \) can differ from the corresponding column of \( \textbf{U} \) by a sign change.
\end{lemma}
\endgroup


\begin{proof}
Let $\textbf{U}, \textbf{U}' \in \cW_{\sf{CD}}$ be two orthogonal Cholesky decompositions of $\pphi$. Define $\textbf{R} = \textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}'$. We will show that this matrix satisfies our requirements through the following steps:

First, we show that $\textbf{R}$ is orthogonal. Note,
\begin{align*}
    \textbf{R}^\top \textbf{R} &= (\textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}')^\top (\textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}') \\
    &= \textbf{U}'^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U} \textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}' \\
    &= \textbf{U}'^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\pphi \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}' \\
    &= \textbf{U}'^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}'\textbf{U}'^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}' \\
    &= \textbf{U}'^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}'\\%\text{diag}(\lambda_1,\ldots,\lambda_r) \\
    &= \textbf{I}_r
\end{align*}

Similarly,
\begin{align*}
    \textbf{R}\textbf{R}^\top &= \textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}'(\textbf{U}')^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U} \\
    &= \textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\pphi \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U} \\
    &= \textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}\textbf{U}^\top\textbf{U} \\
    &= \textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}\text{diag}(\lambda_1,\ldots,\lambda_r) \\
    &= \textbf{I}_r
\end{align*}

Now we show that $\textbf{U}' = \textbf{U}\textbf{R}$. 
\begin{align*}
    \textbf{U}\textbf{R} &= \textbf{U}\textbf{U}^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}' \\
    &= \pphi \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}' \\
    &= \textbf{U}'\textbf{U}'^\top\textbf{U}' \text{diag}(1/\lambda_1,\ldots,1/\lambda_r) \\
    &= \textbf{U}'\text{diag}(\lambda_1,\ldots,\lambda_r) \text{diag}(1/\lambda_1,\ldots,1/\lambda_r) \\
    &= \textbf{U}'
\end{align*}
 To show that \( \mathbf{R} \) is block diagonal with orthogonal blocks corresponding to repeated eigenvalues, consider the partitioning based on distinct eigenvalues. Let \( \mathcal{I}_k = \{i \mid \lambda_i = \gamma_k\} \) be the set of indices corresponding to the \( k \)-th distinct eigenvalue \( \gamma_k \) of \( \pphi \), for \( k = 1, \ldots, K \), where \( K \) is the number of distinct eigenvalues. Let \( m_k = |\mathcal{I}_k| \) denote the multiplicity of \( \gamma_k \).
    
    Define \( \mathbf{U}_k \) and \( \mathbf{U}'_k \) as the submatrices of \( \mathbf{U} \) and \( \mathbf{U}' \) consisting of columns indexed by \( \mathcal{I}_k \), respectively.
    
    Now, consider the block \( \mathbf{R}_{k\ell} \) of \( \mathbf{R} \) corresponding to eigenvalues \( \gamma_k \) and \( \gamma_\ell \). For \( k \neq \ell \),
    % \[
    %     \mathbf{U}_k^\top \mathbf{D} \mathbf{U}'_\ell = \mathbf{U}_k^\top \text{diag}\left(\frac{1}{\gamma_1}, \ldots, \frac{1}{\gamma_r}\right) \mathbf{U}'_\ell.
    % \]
     \( \mathbf{U}_k \) and \( \mathbf{U}'_\ell \) correspond to different eigenspaces (as \( \gamma_k \neq \gamma_\ell \)), and thus their inner product is zero. Hence,
    
    \[
        \mathbf{U}_k^\top \text{diag}\left(\frac{1}{\lambda_1}, \ldots, \frac{1}{\lambda_r}\right) \mathbf{U}'_\ell = \mathbf{0}_{m_k \times m_\ell}.
    \]
    
    This implies $\mathbf{R}_{k\ell} = \mathbf{0}_{m_k \times m_\ell} \quad \text{for} \quad k \neq \ell.$
    
    But then \( \mathbf{R} \) must be block diagonal:
    
    \[
        \mathbf{R} = \begin{bmatrix}
            \mathbf{R}_1 & \mathbf{0} & \cdots & \mathbf{0} \\
            \mathbf{0} & \mathbf{R}_2 & \cdots & \mathbf{0} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mathbf{0} & \mathbf{0} & \cdots & \mathbf{R}_K \\
        \end{bmatrix},
    \]
    where each \( \mathbf{R}_k \in \mathbb{R}^{m_k \times m_k} \) is an orthogonal matrix. For eigenvalues with multiplicity one (\( m_k = 1 \)), the corresponding block \( \mathbf{R}_k \) is a \( 1 \times 1 \) orthogonal matrix. The only possibilities are:
    \[
        \mathbf{R}_k = [1] \quad \text{or} \quad \mathbf{R}_k = [-1],
    \]
    representing a sign change in the corresponding column of \( \mathbf{U} \). For eigenvalues with multiplicity greater than one (\( m_k > 1 \)), each block \( \mathbf{R}_k \) can be any \( m_k \times m_k \) orthogonal matrix. This allows for rotations within the eigenspace corresponding to the repeated eigenvalue \( \gamma_k \).
    
    Combining all steps, we have shown that:
    \[
        \mathbf{U}' = \mathbf{U} \mathbf{R},
    \]
    where \( \mathbf{R} \) is an orthogonal, block-diagonal matrix. Each block \( \mathbf{R}_k \) corresponds to a distinct eigenvalue \( \gamma_k \) of \( \pphi \) and is either a \( 1 \times 1 \) matrix with entry \( \pm 1 \) (for unique eigenvalues) or an arbitrary orthogonal matrix of size equal to the multiplicity of \( \gamma_k \) (for repeated eigenvalues). This completes the proof of the lemma.
    
%  To show that $\textbf{R}$ is block diagonal, let $\mathcal{I}_k = \{i : \lambda_i = \gamma_k\}$ be the set of indices corresponding to the $k$-th distinct eigenvalue $\gamma_k$ of $\pphi$. Let $\textbf{U}_k$ and $\textbf{U}'_k$ be the submatrices of $\textbf{U}$ and $\textbf{U}'$ consisting of columns indexed by $\mathcal{I}_k$.

% For any $i \in \mathcal{I}_k$ and $j \in \mathcal{I}_\ell$ where $k \neq \ell$:
% \begin{align*}
%     (\textbf{U}_k)^\top \text{diag}(1/\lambda_1,\ldots,1/\lambda_r)\textbf{U}'_\ell &= 0
% \end{align*}
% This follows because $\textbf{U}_k$ and $\textbf{U}'_\ell$ correspond to different eigenspaces of $\pphi$.

% Therefore, $\textbf{R}$ must be block diagonal, with blocks corresponding to each distinct eigenvalue.

% For the sign change property, note that when an eigenvalue $\lambda_i$ appears with multiplicity 1, the corresponding block in $\textbf{R}$ is $1 \times 1$ and must be $\pm 1$ since $\textbf{R}$ is orthogonal.

% Thus, we have shown that $\textbf{U}' = \textbf{U}\textbf{R}$ where $\textbf{R}$ is an orthogonal block diagonal matrix with the specified structure.
\end{proof}

% \begin{proof}
%     Let \( \mathbf{U}, \mathbf{U}' \in \cW_{\sf{CD}} \) be two orthogonal Cholesky decompositions of \( \pphi \). Define the diagonal matrix \( \mathbf{D} = \text{diag}\left(\frac{1}{\lambda_1}, \ldots, \frac{1}{\lambda_r}\right) \) and set
%     \[
%         \mathbf{R} = \mathbf{U}^\top \mathbf{D} \mathbf{U}'.
%     \]
    
%     \textbf{Step 1: Verifying Orthogonality of \( \mathbf{R} \)}
    
%     We first show that \( \mathbf{R} \) is orthogonal, i.e., \( \mathbf{R}^\top \mathbf{R} = \mathbf{I}_r \) and \( \mathbf{R} \mathbf{R}^\top = \mathbf{I}_r \).
    
%     \begin{align*}
%         \mathbf{R}^\top \mathbf{R} &= (\mathbf{U}^\top \mathbf{D} \mathbf{U}')^\top (\mathbf{U}^\top \mathbf{D} \mathbf{U}') \\
%         &= (\mathbf{U}')^\top \mathbf{D}^\top \mathbf{U} \mathbf{U}^\top \mathbf{D} \mathbf{U}' \\
%         &= (\mathbf{U}')^\top \mathbf{D} \mathbf{U} \mathbf{U}^\top \mathbf{D} \mathbf{U}' \quad (\mathbf{D} \text{ is diagonal and hence } \mathbf{D}^\top = \mathbf{D}) \\
%         &= (\mathbf{U}')^\top \mathbf{D} \pphi \mathbf{D} \mathbf{U}' \quad (\pphi = \mathbf{U} \mathbf{U}^\top = \mathbf{U}' \mathbf{U}'^\top) \\
%         &= (\mathbf{U}')^\top \mathbf{D} \mathbf{U}' \mathbf{U}'^\top \mathbf{D} \mathbf{U}' \\
%         &= (\mathbf{U}')^\top \mathbf{D} \mathbf{U}' (\mathbf{U}'^\top \mathbf{U}') \mathbf{D} \mathbf{U}' \quad (\pphi = \mathbf{U}' \mathbf{U}'^\top) \\
%         &= (\mathbf{U}')^\top \mathbf{D} \mathbf{U}' \mathbf{D} \mathbf{U}' \quad (\mathbf{U}'^\top \mathbf{U}' = \text{diag}(\lambda_1,\ldots,\lambda_r)) \\
%         &= \mathbf{I}_r \quad (\mathbf{D} = \text{diag}(1/\lambda_i) \text{ and } \mathbf{U}'^\top \mathbf{U}' = \text{diag}(\lambda_i)) \\
%     \end{align*}
    
%     Similarly,
    
%     \begin{align*}
%         \mathbf{R} \mathbf{R}^\top &= \mathbf{U}^\top \mathbf{D} \mathbf{U}' (\mathbf{U}^\top \mathbf{D} \mathbf{U}')^\top \\
%         &= \mathbf{U}^\top \mathbf{D} \mathbf{U}' \mathbf{U}'^\top \mathbf{D}^\top \mathbf{U} \\
%         &= \mathbf{U}^\top \mathbf{D} \pphi \mathbf{D} \mathbf{U} \quad (\pphi = \mathbf{U}' \mathbf{U}'^\top) \\
%         &= \mathbf{U}^\top \mathbf{D} \mathbf{U} \mathbf{U}^\top \mathbf{U} \mathbf{D} \mathbf{U} \\
%         &= \mathbf{U}^\top \mathbf{D} \mathbf{U} \text{diag}(\lambda_1,\ldots,\lambda_r) \mathbf{D} \mathbf{U} \quad (\mathbf{U}^\top \mathbf{U} = \text{diag}(\lambda_i)) \\
%         &= \mathbf{U}^\top \mathbf{D} \text{diag}(\lambda_i) \mathbf{D} \mathbf{U} \\
%         &= \mathbf{U}^\top \text{diag}\left(\frac{\lambda_1}{\lambda_1}, \ldots, \frac{\lambda_r}{\lambda_r}\right) \mathbf{U} \\
%         &= \mathbf{U}^\top \mathbf{U} \\
%         &= \text{diag}(\lambda_1, \ldots, \lambda_r) \\
%         &= \mathbf{I}_r \quad (\text{since } \mathbf{D} \mathbf{U}'^\top \mathbf{U}' \mathbf{D} = \mathbf{I}_r).
%     \end{align*}
    
%     Therefore, \( \mathbf{R} \) is orthogonal:
%     \[
%         \mathbf{R}^\top \mathbf{R} = \mathbf{R} \mathbf{R}^\top = \mathbf{I}_r.
%     \]
    
%     \textbf{Step 2: Showing \( \mathbf{U}' = \mathbf{U} \mathbf{R} \)}
    
%     \begin{align*}
%         \mathbf{U} \mathbf{R} &= \mathbf{U} \mathbf{U}^\top \mathbf{D} \mathbf{U}' \\
%         &= \pphi \mathbf{D} \mathbf{U}' \quad (\pphi = \mathbf{U} \mathbf{U}^\top) \\
%         &= \mathbf{U}' \mathbf{U}'^\top \mathbf{D} \mathbf{U}' \quad (\pphi = \mathbf{U}' \mathbf{U}'^\top) \\
%         &= \mathbf{U}' (\mathbf{U}'^\top \mathbf{U}') \mathbf{D} \mathbf{U}' \quad (\text{Associativity}) \\
%         &= \mathbf{U}' \text{diag}(\lambda_1,\ldots,\lambda_r) \mathbf{D} \mathbf{U}' \\
%         &= \mathbf{U}' \text{diag}\left(\frac{\lambda_1}{\lambda_1}, \ldots, \frac{\lambda_r}{\lambda_r}\right) \mathbf{U}' \\
%         &= \mathbf{U}' \mathbf{I}_r \\
%         &= \mathbf{U}'
%     \end{align*}
    
%     Thus, we have:
%     \[
%         \mathbf{U}' = \mathbf{U} \mathbf{R}.
%     \]
    
%     \textbf{Step 3: Establishing Block-Diagonal Structure of \( \mathbf{R} \)}
    
%     To show that \( \mathbf{R} \) is block diagonal with orthogonal blocks corresponding to repeated eigenvalues, consider the partitioning based on distinct eigenvalues.
    
%     Let \( \mathcal{I}_k = \{i \mid \lambda_i = \gamma_k\} \) be the set of indices corresponding to the \( k \)-th distinct eigenvalue \( \gamma_k \) of \( \pphi \), for \( k = 1, \ldots, K \), where \( K \) is the number of distinct eigenvalues. Let \( m_k = |\mathcal{I}_k| \) denote the multiplicity of \( \gamma_k \).
    
%     Define \( \mathbf{U}_k \) and \( \mathbf{U}'_k \) as the submatrices of \( \mathbf{U} \) and \( \mathbf{U}' \) consisting of columns indexed by \( \mathcal{I}_k \), respectively.
    
%     Consider the block \( \mathbf{R}_{k\ell} \) of \( \mathbf{R} \) corresponding to eigenvalues \( \gamma_k \) and \( \gamma_\ell \). For \( k \neq \ell \):
    
%     \[
%         \mathbf{U}_k^\top \mathbf{D} \mathbf{U}'_\ell = \mathbf{U}_k^\top \text{diag}\left(\frac{1}{\gamma_1}, \ldots, \frac{1}{\gamma_r}\right) \mathbf{U}'_\ell.
%     \]
    
%     Since \( \mathbf{U}_k \) and \( \mathbf{U}'_\ell \) correspond to different eigenspaces (as \( \gamma_k \neq \gamma_\ell \)), their inner product is zero:
    
%     \[
%         \mathbf{U}_k^\top \mathbf{U}'_\ell = \mathbf{0}_{m_k \times m_\ell}.
%     \]
    
%     Therefore:
    
%     \[
%         \mathbf{R}_{k\ell} = \mathbf{0}_{m_k \times m_\ell} \quad \text{for} \quad k \neq \ell.
%     \]
    
%     This implies that \( \mathbf{R} \) must be block diagonal:
    
%     \[
%         \mathbf{R} = \begin{bmatrix}
%             \mathbf{R}_1 & \mathbf{0} & \cdots & \mathbf{0} \\
%             \mathbf{0} & \mathbf{R}_2 & \cdots & \mathbf{0} \\
%             \vdots & \vdots & \ddots & \vdots \\
%             \mathbf{0} & \mathbf{0} & \cdots & \mathbf{R}_K \\
%         \end{bmatrix},
%     \]
%     where each \( \mathbf{R}_k \in \mathbb{R}^{m_k \times m_k} \) is an orthogonal matrix.
    
%     \textbf{Step 4: Sign Change for Unique Eigenvalues}
    
%     For eigenvalues with multiplicity one (\( m_k = 1 \)), the corresponding block \( \mathbf{R}_k \) is a \( 1 \times 1 \) orthogonal matrix. The only possibilities are:
%     \[
%         \mathbf{R}_k = [1] \quad \text{or} \quad \mathbf{R}_k = [-1],
%     \]
%     representing a **sign change** in the corresponding column of \( \mathbf{U} \).
    
%     \textbf{Step 5: Rotations for Repeated Eigenvalues}
    
%     For eigenvalues with multiplicity greater than one (\( m_k > 1 \)), each block \( \mathbf{R}_k \) can be any \( m_k \times m_k \) orthogonal matrix. This allows for **rotations within the eigenspace** corresponding to the repeated eigenvalue \( \gamma_k \).
    
%     \textbf{Conclusion}
    
%     Combining all steps, we have shown that:
%     \[
%         \mathbf{U}' = \mathbf{U} \mathbf{R},
%     \]
%     where \( \mathbf{R} \) is an orthogonal, block-diagonal matrix. Each block \( \mathbf{R}_k \) corresponds to a distinct eigenvalue \( \gamma_k \) of \( \pphi \) and is either a \( 1 \times 1 \) matrix with entry \( \pm 1 \) (for unique eigenvalues) or an arbitrary orthogonal matrix of size equal to the multiplicity of \( \gamma_k \) (for repeated eigenvalues).

%     This completes the proof of the lemma.
% \end{proof}


\iffalse
\begin{proof}
    Since \( \pphi \) is symmetric and positive semi-definite, it admits an eigen-decomposition:
\[
    \pphi = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^\top,
\]
where:
\begin{itemize}
    \item \( \mathbf{Q} \in \mathbb{R}^{p \times p} \) is an orthogonal matrix (\( \mathbf{Q}^\top \mathbf{Q} = \mathbf{Q} \mathbf{Q}^\top = \mathbf{I}_p \)),
    \item \( \boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_p) \) with \( \lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p \geq 0 \).
\end{itemize}
Let \( r = \text{rank}(\pphi) \), implying \( \lambda_1, \lambda_2, \ldots, \lambda_r > 0 \) and \( \lambda_{r+1} = \ldots = \lambda_p = 0 \).

\textbf{Step 2: Orthogonal Cholesky Decompositions}

Consider two matrices \( \mathbf{U}, \mathbf{U}' \in \cW_{\sf{CD}} \). By definition:
\[
    \pphi = \mathbf{U} \mathbf{U}^\top = \mathbf{U}' \mathbf{U}'^\top,
\]
and
\[    \mathbf{U}^\top \mathbf{U} = \mathbf{U}'^\top \mathbf{U}' = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_r).\]
Both \( \mathbf{U} \) and \( \mathbf{U}' \) have full column rank \( r \).

\textbf{Step 3: Relating \( \mathbf{U} \) and \( \mathbf{U}' \) via an Orthogonal Matrix}

Since both \( \mathbf{U} \) and \( \mathbf{U}' \) provide a full-rank factorization of \( \pphi \), there exists an orthogonal matrix \( \mathbf{R} \in \mathbb{R}^{r \times r} \) such that:
\[
    \mathbf{U}' = \mathbf{U} \mathbf{R}.
\]
\textbf{Justification:}
Given that \( \mathbf{U} \) and \( \mathbf{U}' \) are both in \( \cW_{\sf{CD}} \), we can write:
\[
    \mathbf{U}' = \mathbf{U} \mathbf{R},
\]
where \( \mathbf{R} \) satisfies:
\[
    \mathbf{U}'^\top \mathbf{U}' = \mathbf{R}^\top \mathbf{U}^\top \mathbf{U} \mathbf{R} = \mathbf{R}^\top \text{diag}(\lambda_1, \ldots, \lambda_r) \mathbf{R} = \text{diag}(\lambda_1, \ldots, \lambda_r).
\]
Therefore, \( \mathbf{R} \) must satisfy:
\[
    \mathbf{R}^\top \text{diag}(\lambda_1, \ldots, \lambda_r) \mathbf{R} = \text{diag}(\lambda_1, \ldots, \lambda_r).
\]
This condition constrains \( \mathbf{R} \) to be block diagonal with orthogonal blocks corresponding to repeated eigenvalues.

\textbf{Step 4: Structure of \( \mathbf{R} \)}

To elucidate the structure of \( \mathbf{R} \), consider the multiplicities of the eigenvalues \( \lambda_i \):

\begin{itemize}
    \item Let there be \( k \) distinct eigenvalues among \( \lambda_1, \lambda_2, \ldots, \lambda_r \), with \( \mu_1, \mu_2, \ldots, \mu_k \).
    \item Let \( m_j \) denote the multiplicity of \( \mu_j \), such that \( \sum_{j=1}^k m_j = r \).
\end{itemize}

Rearrange \( \lambda_1, \lambda_2, \ldots, \lambda_r \) so that identical eigenvalues are consecutive. Thus, the diagonal matrix can be partitioned as:
\[
    \text{diag}(\lambda_1, \ldots, \lambda_r) = \begin{bmatrix}
        \mu_1 \mathbf{I}_{m_1} & & & \\
        & \mu_2 \mathbf{I}_{m_2} & & \\
        & & \ddots & \\
        & & & \mu_k \mathbf{I}_{m_k}
    \end{bmatrix}.
\]
Given this partitioning, the orthogonal matrix \( \mathbf{R} \) must preserve each block corresponding to a distinct eigenvalue. Therefore, \( \mathbf{R} \) can be expressed as a block diagonal matrix:
\[
    \mathbf{R} = \begin{bmatrix}
        \mathbf{R}_1 & & & \\
        & \mathbf{R}_2 & & \\
        & & \ddots & \\
        & & & \mathbf{R}_k
    \end{bmatrix},
\]
where each \( \mathbf{R}_j \in \mathbb{R}^{m_j \times m_j} \) is an orthogonal matrix (\( \mathbf{R}_j^\top \mathbf{R}_j = \mathbf{I}_{m_j} \)).

\textbf{Explanation:}

\begin{enumerate}
    \item **Unique Eigenvalues (\( m_j = 1 \)):**  
        For eigenvalues with multiplicity one, \( \mathbf{R}_j \) must be a \( 1 \times 1 \) orthogonal matrix. The only orthogonal \( 1 \times 1 \) matrices are \( [1] \) and \( [-1] \), corresponding to sign changes.
    
    \item **Repeated Eigenvalues (\( m_j > 1 \)):**  
        For eigenvalues with multiplicity greater than one, \( \mathbf{R}_j \) can be any orthogonal matrix of size \( m_j \times m_j \), allowing for rotations within the corresponding eigenspace.
\end{enumerate}

\textbf{Step 5: Conclusion}

Combining the above steps, we conclude that for any two orthogonal Cholesky decompositions \( \mathbf{U} \) and \( \mathbf{U}' \) of \( \pphi \), there exists an orthogonal matrix \( \mathbf{R} \) such that:
\[
    \mathbf{U}' = \mathbf{U} \mathbf{R},
\]
where \( \mathbf{R} \) is block diagonal with orthogonal blocks \( \mathbf{R}_j \) corresponding to the multiplicities of the eigenvalues \( \lambda_j \). Specifically:
\begin{itemize}
    \item Each \( \mathbf{R}_j \) for \( m_j > 1 \) allows for arbitrary rotations within the eigenspace corresponding to \( \mu_j \).
    \item Each \( \mathbf{R}_j \) for \( m_j = 1 \) allows for sign changes in the corresponding column of \( \mathbf{U} \).
\end{itemize}

Therefore, the lemma is proven.

\end{proof}
\fi

%\begin{proof}
%    Let \( U, U' \in W_{\sf{CD}} \). Since both \( U^\top U \) and \( U'^\top U' \) are diagonal matrices with the same entries, it follows that the columns of \( U \) and \( U' \) are orthogonal and scaled by the square roots of the diagonal entries \( d_i \).
%    Define \( R = U^\top U' \). Since \( U^\top U = U'^\top U' = \text{diag}(d_1, d_2, \ldots, d_r) \), it follows that \( R \) is an orthogonal matrix.
%    Therefore, \( U' = U R \). Additionally, to account for possible sign changes, \( R \) can be decomposed into a product of an orthogonal matrix and a diagonal matrix with \( \pm 1 \) entries, i.e., \( R = Q S \), where \( Q \) is orthogonal and \( S = \text{diag}(\epsilon_1, \epsilon_2, \ldots, \epsilon_r) \) with \( \epsilon_i \in \{+1, -1\} \).
%    Hence, \( U' = U Q S \), illustrating that the columns of \( U' \) are equivalent to those of \( U \) up to orthogonal transformations and sign changes.
%\end{proof}

\section{Worst-case bounds: Constructive case}\label{app: worstcase}

In this Appendix, we provide the proof of the lower bound as stated in \propref{prop: worstcase}. Before we prove this lower bound, we state a useful property of the sum of a symmetric, PSD matrix and a general symmetric matrix in $\symm$.
\begin{lemma}\label{lem: sum}
    Let $\pphi \in \symmp$ be a symmetric matrix with full rank, i.e., $\rank{\pphi} = p$. For any arbitrary symmetric matrix $\pphi' \in \symm$, there exists a positive scalar $\lambda > 0$ such that the matrix $(\pphi + \lambda \pphi')$ is positive semidefinite.
\end{lemma}

\begin{proof}
    Since $\pphi$ is symmetric and has full rank, it admits an eigendecomposition:
    \[
        \pphi = \sum_{i=1}^p \lambda_i u_i u_i^{\top},
    \]
    where $\{\lambda_i\}_{i=1}^p$ are the positive eigenvalues and $\{u_i\}_{i=1}^p$ are the corresponding orthonormal eigenvectors of $\pphi$.

    Define the constant $\gamma$ as the maximum absolute value of the quadratic forms of $\pphi'$ with respect to the eigenvectors of $\pphi$:
    \[
        \gamma := \max_{1 \leq i \leq p} \left| u_i^{\top} \pphi' u_i \right|.
    \]
    
    Let $\lambda$ be chosen as:
    \[
        \lambda := \frac{\min_{1 \leq i \leq p} \lambda_i}{\gamma}.
    \]
    
    For each eigenvector $u_i$, consider the quadratic form of $(\pphi + \lambda \pphi')$:
    \[
        u_i^{\top} (\pphi + \lambda \pphi') u_i = \lambda_i + \lambda u_i^{\top} \pphi' u_i \geq \lambda_i - \lambda \gamma = \lambda_i - \frac{\min \lambda_i}{\gamma} \gamma = \lambda_i - \min \lambda_i \geq 0.
    \]
    This shows that each eigenvector $u_i$ satisfies:
    \[
        u_i^{\top} (\pphi + \lambda \pphi') u_i \geq 0.
    \]
    
    Since $\{u_i\}_{i=1}^p$ forms an orthonormal basis for $\mathbb{R}^p$, for any vector $x \in \mathbb{R}^p$, we can express $x$ as $x = \sum_{i=1}^p a_i u_i$. Then:
    \[
        x^{\top} (\pphi + \lambda \pphi') x = \sum_{i=1}^p a_i^2 u_i^{\top} (\pphi + \lambda \pphi') u_i \geq 0,
    \]
    since each term in the sum is non-negative.

    Therefore, $(\pphi + \lambda \pphi')$ is positive semidefinite.
\end{proof}


Now, we provide the proof of \propref{prop: worstcase} in the following:
\iffalse
\begin{proof}[Proof of \propref{prop: worstcase}] 
    Consider a full rank matrix $\pphi^* \in \symmp$. For the sake of contradiction, let $\cF(\cV, \maha, \pphi^*)$ be a feedback set for \eqnref{eq: redsol} up to linear scaling relation with size strictly less than $\paren{\frac{p(p+1)}{2} - 1}$.
    
    Now, for any pair $(y,z) \in \cF$, $\pphi^*$ is orthogonal to $(yy^{\top} - zz^{\top})$. Thus, if we define $\mathcal{O}_{\pphi^*}$ as the orthogonal complement of $\pphi^*$ then for any $(y,z) \in \cF$ we have $(yy^{\top} - zz^{\top}) \in \mathcal{O}_{\pphi^*}$. Thus,
    \begin{align*}
        span\inner{\{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF}} \subset \mathcal{O}_{\pphi^*}
    \end{align*}
    Hence,
    \begin{align*}
        \pphi^* \perp span\inner{\{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF}}
    \end{align*}
    Since the feedback set $|\cF| < \paren{\frac{p(p+1)}{2} - 1}$ we note that $\dim (span\inner{(yy^{\top} - zz^{\top})}) < \paren{\frac{p(p+1)}{2} - 1}$. 
    
    Note $\pphi^*$ is a singleton vector in $\reals^{p \times p}$ the union  $\curlybracket{\pphi^*} \cup \{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF}$ will only add an extra dimension in the space $\reals^{p \times p}$. This implies that
    \begin{align*}
        \dim(span\inner{\pphi^*\cup \{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF}} \le \paren{\frac{p(p+1)}{2} - 1}
    \end{align*}
    Since $\symm$ is a vector space over $\reals$ and $\dim(\symm) = \frac{p(p+1)}{2}$ there is a symmetric matrix $\pphi'$ such that the following holds
    \begin{gather*}    
        \pphi' \in \mathcal{O}_{\pphi^*},\\
        \forall (y,z) \in \cF,\,  \pphi' \perp (yy^{\top} - zz^{\top})
    \end{gather*}
    But \lemref{lem: sum} implies there exists $\lambda > 0$ such that $\pphi^* + \lambda \pphi'$ is PSD and symmetric (sum of symmetric matrices is symmetric). Since $\pphi' \in \mathcal{O}_{\pphi^*}$, $\pphi'$ is not identical to $\pphi^*$ up to a linear scaling. This implies that there exists a matrix in the form $\pphi^* + \lambda \pphi'$ ($\,\not \sim_{R_l} \pphi^*$) that is orthogonal to all the matrices $(yy^{\top} - zz^{\top})$ for any pair $(y,z) \in \cF$.
    
    Thus, if the feedback set is smaller than $\frac{p(p+1)}{2} - 1$, we can find symmetric, PSD matrices not related up to linear scaling that satisfy \eqnref{eq: redsol}. This contradicts the assumption on $\cF$. This establishes the stated lower bound on the feedback complexity of the feedback set.
\end{proof}
\fi
\begin{proof}[Proof of \propref{prop: worstcase}] 
Assume, for contradiction, that there exists a feedback set $\cF(\cV, \maha, \pphi^*)$ for \eqnref{eq: redsol} with size $|\cF| < \left(\frac{p(p+1)}{2} - 1\right)$.

For each pair $(y,z) \in \cF$, $\pphi^*$ is orthogonal to $(yy^{\top} - zz^{\top})$, implying that $(yy^{\top} - zz^{\top}) \in \mathcal{O}_{\pphi^*}$, the orthogonal complement of $\pphi^*$. Therefore,
\[
\spn\inner{\{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF}} \subset \mathcal{O}_{\pphi^*}.
\]
This leads to
\[
\pphi^* \perp \spn\inner{\{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF}}.
\]
Since $|\cF| < \frac{p(p+1)}{2} - 1$, we have
\[
\dim \left( \spn\inner{ \{yy^{\top} - zz^{\top}\} } \right) < \frac{p(p+1)}{2} - 1.
\]
Adding $\pphi^*$ to this span increases the dimension by at most one:
\[
\dim \left( \spn\inner{ \pphi^* \cup \{yy^{\top} - zz^{\top}\}_{(y,z) \in \cF} } \right) \leq \frac{p(p+1)}{2} - 1.
\]
Since $\symm$ is a vector space with $\dim(\symm) = \frac{p(p+1)}{2}$, there exists a symmetric matrix $\pphi' \in \mathcal{O}_{\pphi^*}$ such that
\[
\pphi' \perp (yy^{\top} - zz^{\top}) \quad \forall \, (y,z) \in \cF.
\]
By \lemref{lem: sum}, there exists $\lambda > 0$ such that $\pphi^* + \lambda \pphi'$ is PSD and symmetric. Since $\pphi' \in \mathcal{O}_{\pphi^*}$ and $\pphi'$ is not a scalar multiple of $\pphi^*$, the matrix $\pphi^* + \lambda \pphi'$ is not related to $\pphi^*$ via linear scaling. However, it still satisfies \eqnref{eq: redsol}, contradicting the minimality of $\cF$.

Thus, any feedback set must satisfy
\[
|\cF| \geq \frac{p(p+1)}{2} - 1.
\]
This establishes the stated lower bound on the feedback complexity of the feedback set.
\end{proof}

\section{Proof of \thmref{thm: constructgeneral}: Upper bound}\label{app: constub}
%\section{Proof of \thmref{thm: obv}}
Below we provide proof of the upper bound stated in \thmref{thm: constructgeneral}. 


Consider the eigendecomposition of the matrix $\pphi^*$. There exists a set of orthonormal vectors $\curlybracket{v_1, v_2, \ldots, v_r}$ with corresponding eigenvalues $\curlybracket{\gamma_1, \gamma_2, \ldots, \gamma_r}$ such that
\begin{align}
    \pphi^* = \sum_{i=1}^r \gamma_i v_i v_i^{\top} \label{eq: target}
\end{align}
Denote the set of orthogonal vectors $\curlybracket{v_1, v_2, \ldots, v_r}$ as $V_{\bracket{r}}$.

%\subsection{Orthogonal Extension to a Basis}

Let $\curlybracket{v_{r+1}, \dots, v_p}$, denoted as $V_{\bracket{p - r}}$, be an orthogonal extension to the vectors in $V_{\bracket{r}}$ such that
\[
    V_{\bracket{r}} \cup V_{\bracket{p - r}} = \curlybracket{v_1, v_2, \ldots, v_p}
\]
forms an orthonormal basis for $\reals^p$. Denote the complete basis $\curlybracket{v_1, v_2, \ldots, v_p}$ as $V_{\bracket{p}}$.

Note that $\curlybracket{v_{r+1}, \ldots, v_p}$ precisely defines the null space of $\pphi^*$, i.e.,
\[
    \nul{\pphi^*} = \text{span}\inner{\curlybracket{v_{r+1}, \ldots, v_p}}.
\]

%\subsection{Strategy for Teaching the Null Space and Eigenvectors}

The key idea of the proof is to manipulate this null space to satisfy the feedback set condition in \eqnref{eq: orthosat} for the target matrix $\pphi^*$. Since $\pphi^*$ has rank $r \leq p$, the number of degrees of freedom is exactly $\frac{r(r+1)}{2}$. Alternatively, the span of the null space of $\pphi^*$, which has dimension exactly $p - r$, fixes the remaining entries in $\pphi^*$. 

Using this intuition, the teacher can provide pairs $(y, z) \in \cV^2$ to teach the null space and the eigenvectors $\curlybracket{v_1, v_2, \ldots, v_r}$ separately. However, it is necessary to ensure that this strategy is optimal in terms of sample efficiency. We confirm the optimality of this strategy in the next two lemmas.

\subsection{Feedback set for the null space of \texorpdfstring{$\pphi^*$}{phi*}}

Our first result is on nullifying the null set of $\pphi^*$ in the \eqnref{eq: orthosat}. Consider a partial feedback set 
\begin{align*}
    \cF_{\sf {null}} = \curlybracket{(0, v_{i})}_{i = r+1}^p
\end{align*}
\begin{lemma}\label{lem: nullset}
    If the teacher provides the set $\cF_{\sf{null}}$, then the null space of any PSD symmetric matrix $\pphi'$ that satisfies \eqnref{eq: orthosat} contains the span of $\{v_{r+1}, \ldots, v_p\}$, i.e.,
    \begin{equation*}
        \{v_{r+1}, \ldots, v_p\} \subseteq \nul{\pphi'}.
    \end{equation*}
    %If the teacher provides the set $\cF_{\sf{null}}$, then the null set of any psd symmetric matrix $\pphi'$ that satisfies \eqnref{eq: orthosat} contains the span of $\{v_{r+1},\ldots, v_p\}$, i.e.
    % \begin{align*}
    %    \{v_{r+1},\ldots, v_p\} \subset \nul{\pphi'}
    % \end{align*}
\end{lemma}
\begin{proof} Let $\pphi' \in \symmp$ be a matrix that satisfies \eqnref{eq: orthosat} (note that $\pphi^*$ satisfies \eqnref{eq: orthosat}). Thus, we have the following equality constraints:
\begin{equation*}
       \forall (0, v) \in \cF_{\sf{null}}, \quad v^{\top} \pphi' v = 0.
\end{equation*}
    Since $\curlybracket{v_{r+1}, \ldots, v_p}$ is a set of linearly independent vectors, it suffices to show that
    \begin{align}
        \forall v \in V_{\bracket{d - r}}, \quad v^{\top} \pphi' v = 0 \implies \pphi' v = 0. \label{eq: lemmain}
    \end{align}
    
    To prove \eqnref{eq: lemmain}, we utilize general properties of the eigendecomposition of a symmetric, positive semi-definite matrix. We express $\pphi'$ in its eigendecomposition as
    \[
        \pphi' = \sum_{i=1}^{s} \gamma_i' u_i u_i^{\top},
    \]
    where $\curlybracket{u_i}_{i=1}^{s}$ are the eigenvectors and $\curlybracket{\gamma_i'}_{i=1}^s$ are the corresponding eigenvalues of $\pphi'$. Assume that $x \neq 0 \in \reals^p$ satisfies
    \[
        x^{\top} \pphi' x = 0.
    \]
    Consider the decomposition $x = \sum_{i=1}^s a_iu_i + v'$ for scalars $a_i$ and $v' \bot \{u_i\}_{i=1}^s$ . Now, expanding the equation above, we get
    \allowdisplaybreaks
    \begin{align*}
       x^{\top}\pphi'x &= \paren{\sum_{i=1}^s a_iu_i + v'}^{\top}\pphi'\paren{\sum_{i=1}^s a_iu_i + v'}  \\
       & = \paren{\sum_{i=1}^s a_iu_i}^{\top}\pphi'\paren{\sum_{i=1}^s a_iu_i } + v'^{\top}\pphi'\paren{\sum_{i=1}^s a_iu_i} + \paren{\sum_{i=1}^s a_iu_i}\pphi'v' + v'^{\top}\pphi'v'\\
       & = \paren{\sum_{i=1}^s a_iu_i}^{\top}\paren{\sum_{i = 1}^{s} \gamma_i'u_iu_i^{\top}}\paren{\sum_{i=1}^s a_iu_i } + \underbrace{2v'^{\top}\paren{\sum_{i = 1}^{s} \gamma_i'u_iu_i^{\top}}\paren{\sum_{i=1}^s a_iu_i} + v'^{\top}\paren{\sum_{i = 1}^{s} \gamma_i'u_iu_i^{\top}}v'}_{ =\, 0 \textnormal{ as } v' \bot \curlybracket{u_i}} \\
       & = \sum_{i,j,k} a_i u_i^{\top} (\gamma_j'u_ju_j^{\top}) a_k u_k\\
       & = \sum_{i=1}^s a_i^2\gamma_i' = 0
    \end{align*}
    Since $\gamma_i' > 0$ for all $i = 1, \ldots, s$ (because $\pphi'$ is PSD), it follows that each $a_i = 0$. Therefore,
    \[
        \pphi' x = \pphi' v' = 0.
    \]
    This implies that $x \in \nul{\pphi'}$, thereby proving \eqnref{eq: lemmain}.
    
    Hence, if the teacher provides $\cF_{\sf{null}}$, any solution $\pphi'$ to \eqnref{eq: orthosat} must satisfy
    \[
        \{v_{r+1}, \ldots, v_p\} \subseteq \nul{\pphi'}.
    \]
\end{proof}

With this we will argue that the feedback setup in \eqnref{eq: orthosat} can be decomposed in two parts: first is teaching the null set $ \nul{\pphi^*}:= \text{span} \inner{\{v_i\}_{i=r+1}^n}$, and second is teaching $\mathcal{S}_{\pphi^*} = \text{span} \inner{\{v_i\}_{i=1}^r}$ in the form of $\pphi^* = \sum_{i=1}^r \gamma_i v_iv_i^{\top}$. 

\lemref{lem: nullset} implies that using a feedback set of the form $\cF_{\sf {null}}$ any solution $\pphi' \in \symmp$ to \eqnref{eq: orthosat} satisfies the property $V_{\bracket{d - r}} \subset \nul{\pphi'}$. Furthermore, $|\cF_{\sf {null}}| = p - r$. 

\subsection{Feedback set for the kernel of \texorpdfstring{$\pphi^*$}{phi*}}
Next, we discuss how to teach $V_{\bracket{r}}$, i.e. $V_{\bracket{r}}$ span the rows of any solution $\pphi' \in \symmp$ to \eqnref{eq: orthosat} with the corresponding eigenvalues $\curlybracket{\gamma_i}_{i=1}^r$. We show that if the search space of metrics in \eqnref{eq: orthosat} is the version space $\textsf{VS}(\maha,\cF_{\sf {null}})$  which is a restriction of the space $\maha$ to feedback set $\cF_{\sf {null}}$, then a feedback set of size at most $\frac{r(r+1)}{2} -1$ is sufficient to teach $\pphi^*$ up to feature equivalence. Thus, we consider the reformation of the problem in \eqnref{eq: orthosat} as 
\begin{align}
  \forall (y,z) \in \cF(\cX,\textsf{VS}(\maha,\cF_{\sf {null}}),\pphi^*), \quad \pphi \idot (yy^{\top} - zz^{\top})  = 0  \label{eq: redorthosat}
\end{align}
where the feedback set $\cF(\cX,\textsf{VS}(\maha,\cF_{\sf {null}}),\pphi^*)$ is devised to solve a smaller space $\textsf{VS}(\maha,\cF_{\sf {null}}) := \curlybracket{\pphi \in \maha \,|\, \pphi v = 0, \forall (0,v) \in \cF_{\sf {null}}}$. With this state the following useful lemma on the size of the restricted feedback set $\cF(\cX,\textsf{VS}(\maha,\cF_{\sf {null}}),\pphi^*)$.


%It is straight-forward that, since $\dim(\mathcal{N}_{\pphi^*}) = d - r$ one needs at least $(d-r)$ pairs to nullify $\mathcal{N}_{\pphi^*}$ for any psd symmetric matrix. On the other hand, using \lemref{lemma: nullset} we note that one can sufficiently nullify it with just $(d-r)$ pairs of the form $\curlybracket{(0, v_i)}_{i=r+1}^p$. But still one question remains if this set of pairs can be used in a different form. For that, we consider the following result.


\begin{lemma}\label{lem: orthoset}
    Consider the problem as formulated in \eqnref{eq: redorthosat} in which the null set $\nul{\pphi^*}$ of the target matrix $\pphi^*$ is known. Then, the teacher sufficiently and necessarily finds a set $\cF(\cX,\textsf{VS}(\cF_{\sf{null}}),\pphi^*)$ of size $\frac{r(r+1)}{2} - 1$ for oblivious learning up to feature equivalence.
\end{lemma}
\begin{proof}

    Note that any solution $\pphi'$ of \eqnref{eq: redorthosat} has its columns spanned exactly by $V_{\bracket{r}}$. Alternatively, if we consider the eigendecompostion of $\pphi'$ then the corresponding eigenvectors exists in $span \inner{V_{\bracket{r}}}$. Furthermore, note that $\pphi^*$ is of rank $r$ which implies there are only $\frac{r(r+1)}{2}$ degrees of freedom, i.e. entries in the matrix $\pphi^*$, that need to be fixed.

    Thus, there are exactly $r$ linearly independent columns of $\pphi^*$, indexed as $\{j_1,j_2,\ldots, j_r\}$. Now, consider the set of matrices
    \begin{align*}
        \curlybracket{\pphi^{(i,j)}\,|\, i \in \bracket{d}, j \in \{j_1,j_2,\ldots, j_r\}, \pphi^{(i,j)}_{i'j'} = \mathds{1}[i'\in \{i,j\}, j' \in \{i,j\}\setminus \{i'\}]}
    \end{align*}
    This forms a basis to generate any matrix with independent columns along the indexed set. Hence, the span of $\mathcal{S}_{\pphi^*}$ induces a subspace of symmetric matrices of dimension $\frac{r(r+1)}{2}$ in the vector space $\sf{symm}(\reals^p)$, i.e. the column vectors along the indexed set is spanned by elements of $\mathcal{S}_{\pphi^*}$. Thus, it is clear that picking a feedback set of size $\frac{r(r+1)}{2} -1$ in the orthogonal complement of $\pphi^*$, i.e. $\mathcal{O}_{\pphi^*}$ restricted by this span sufficiently teaches $\pphi^*$ if $\nul{\pphi^*}$ is known. One exact form of this set is proven in \lemref{lem: basis}. Since any solution $\pphi'$ is agnostic to the scaling of the target matrix $\pphi'$, we have shown that the sufficiency on the feedback complexity for $\pphi^*$ up to feature equivalence.

   Now, we show that the stated feedback set size is necessary. The argument is similar to the proof of \lemref{lem: sum}.
   
   For the sake of contradiction assume that there is a smaller sized feedback set $\cF_{\sf{small}}$. This implies that there is some matrix in $\textsf{VS}(\maha,\cF_{\sf {null}})$, a subspace induced by span $\mathcal{S}_{\pphi^*}$, orthogonal to $(\pphi^*)$ is not in the span of $\cF_{\sf{small}}$, denoted as $\pphi'$. If $\pphi'$ is PSD then it is a solution to \eqnref{eq: redorthosat} and $\pphi'$ is not a scalar multiple of $\pphi^*$. Now, if $\pphi'$ is not PSD we show that there exists scalar $\lambda > 0$ such that
    \begin{align*}
        \pphi^* + \lambda \pphi' \in \symmp,
    \end{align*}
     i.e. the sum is PSD. Consider the eigendecompostion of $\pphi'$ (assume $\rank{\pphi'} = r'$)
     \begin{align*}
         \pphi' = \sum_{i = 1}^{r'} \delta_i\mu_i\mu_i^{\top}
     \end{align*}
     for orthogonal eigenvectors $\curlybracket{\mu_i}_{i=1}^{r'}$ and the corresponding eigenvalues $\curlybracket{\delta_i}_{i=1}^{r'}$. Since (assume) $r_0 \le r'$ of the eigenvalues are negative we can rewrite $\pphi'$ as
     \begin{align*}
         \pphi' = \sum_{i=1}^{r_0} \delta_i \mu_i\mu_i^{\top} + \sum_{j=r_0 + 1}^{r'} \delta_j \mu_j\mu_j^{\top} 
     \end{align*}
     Thus, if we can regulate the values of $\mu^{\top}_i\pphi^*\mu_i$, for all $i = 1,2,\ldots,r_0$, noting they are positive, then we can find an appropriate scalar $\lambda > 0$. Let $m^* := \min_{i \in [r_0]} \mu_i^{\top}\pphi^*\mu_i$ and $\ell^* := \max_{i \in [r_0]} |\delta_i|$. Now, setting $\lambda \le \frac{m^*}{\ell^*}$ achieves the desired property of $\pphi^* + \lambda \pphi'$ as shown in the proof of \lemref{lem: sum}. 

     Consider that both $\pphi'$ and $\pphi^*$ are orthogonal to every element in the feedback set $\cF_{\sf{small}}$. This orthogonality implies that $\pphi^*$ is not a unique solution to equation \eqnref{eq: redorthosat} up to a positive scaling factor.

Therefore, we have demonstrated that when the null set $\nul{\pphi^*}$ of the target matrix $\pphi^*$ is known, a feedback set of size exactly $\frac{r(r+1)}{2} - 1$ is both \text{necessary} and \text{sufficient}.
\end{proof}

\subsection{Proof of \lemref{lem: basis} and construction of feedback set for \texorpdfstring{$\kernel{\pphi^*}$}{phi*}}


Up until this point we haven's shown how to construct this $\frac{r(r+1)}{2}-1$ sized feedback set. 
Consider the following union:
\begin{align*}
    \curlybracket{v_1v_1^{\top}} \cup \curlybracket{v_2v_2^{\top}, (v_2 + v_1)(v_2 + v_1)^{\top}} \cup \ldots \cup \curlybracket{v_rv_r^{\top}, (v_1 + v_r)(v_1 + v_r)^{\top},\ldots, (v_{r-1} + v_r)(v_{r-1} + v_r)^{\top}}
\end{align*}
We can show that this union is a set of linearly independent matrices of rank 1 as stated in \lemref{lem: basis} below. 
%First, note that if this set is not linearly independent then 
\begingroup
\renewcommand\thelemma{\ref{lem: basis}} 
\begin{lemma}
     Let $\{v_i\}_{i=1}^r \subset \reals^p$ be a set of orthogonal vectors. Then, the set of rank-1 matrices
    \[
    \mathcal{B} := \left\{v_i v_i^{\top},\ (v_i + v_j)(v_i + v_j)^{\top}\ \bigg| \ 1 \leq i < j \leq r \right\}
    \]
    is linearly independent in the space of symmetric matrices $\symm$.
\end{lemma}
\endgroup
\begin{proof}
    We prove the claim by considering two separate cases. For the sake of contradiction, suppose that the set $\cB$ is linearly dependent. This implies that there exists at least one matrix of the form $v_i v_i^{\top}$ or $(v_i + v_j)(v_i + v_j)^{\top}$ that can be expressed as a linear combination of the other matrices in $\cB$. We now examine these two cases individually.
    
    \textbf{Case 1}: First, we assume that for some $i \in [r]$, $v_iv_i^{\top}$ can be written as a linear combination. Thus, there exists scalars that satisfy the following property
    \begin{gather}
        v_iv_i^{\top} = \sum_{j = 1}^{r'} \alpha_{j}v_{i_j}v_{i_j}^{\top} + \sum_{k = 1}^{r''} \beta_{k}(v_{l_k} + v_{m_k})(v_{l_k} + v_{m_k})^{\top}\\
        \forall j,k,\quad \alpha_j, \beta_k > 0, i_j \neq i, l_k < m_k
    \end{gather}
    Now, note that we can write
    \begin{align*}
       \sum_{k = 1}^{r''} \beta_{k}(v_{l_k} + v_{m_k})(v_{l_k} + v_{m_k})^{\top} =  \sum_{k = 1, l_k = i}^{r''} \beta_{k}(v_{l_k} + v_{m_k})v_{l_k}^{\top} + \sum_{k = 1, l_k \neq i}^{r''} \beta_{k}(v_{l_k} + v_{m_k})v_{l_k}^{\top} + \sum_{k = 1}^{r''} \beta_{k}(v_{l_k} + v_{m_k})v_{m_k}^{\top}
    \end{align*}
    But the following sum 
    \begin{align*}
        \sum_{j = 1}^{r'} \alpha_{j}v_{i_j}v_{i_j}^{\top} + \sum_{k = 1, l_k \neq i}^{r''} \beta_{k}(v_{l_k} + v_{m_k})v_{l_k}^{\top} + \sum_{k = 1}^{r''} \beta_{k}(v_{l_k} + v_{m_k})v_{m_k}^{\top}
    \end{align*}
    doesn't span (as column vectors) a subspace that contains the column vector $v_i$ because $\curlybracket{v_i}_{i=1}^r$ is a set of orthogonal vectors. Thus, we can write
    \begin{align}
        v_iv_i^{\top} = \sum_{k = 1, l_k = i}^{r''} \beta_{k}(v_{l_k} + v_{m_k})v_{l_k}^{\top} = \paren{\sum_{k = 1, l_k = i}^{r''} \beta_k v_{l_k} + \sum_{k = 1, l_k = i}^{r''} \beta_k v_{m_k}}v_i^{\top} \label{eq: v1}
    \end{align}
    This implies that 
    \begin{align}
        \sum_{k = 1, l_k = i}^{r''} \beta_k v_{m_k} = 0 \implies \textnormal{ if } l_k = i, \beta_k = 0 \label{eq: v2}
    \end{align}
    Since not all $\beta_k = 0$ corresponding to $l_k = i$ (otherwise $\sum_{k = 1, l_k = i}^{r''} \beta_k v_{l_k} = 0$ ) we have shown that $v_iv_i^{\top}$ can not be written as a linear combination of elements in $\cB \setminus \curlybracket{v_iv_i^\top}$.

    \textbf{Case 2}: Now, we consider the second case where there exists some indices $i,j$ such that $(v_i + v_j)(v_i+v_j)^{\top}$ is a sum of linear combination of elements in $\cB$. Note that this linear combination can't have an element of type $v_kv_k^{\top}$ as it contradicts the first case. So, there are scalars such that
    \begin{gather}
        (v_i + v_j)(v_i+v_j)^{\top} = \sum_{k = 1}^{r''} \beta_{k}(v_{l_k} + v_{m_k})(v_{l_k} + v_{m_k})^{\top}\\
        \forall k,\quad l_k < m_k
    \end{gather}
    But we rewrite this as 
    \begin{align*}
        &(v_i + v_j)v_i^{\top} + (v_i + v_j)v_j^{\top}\\ = &\sum_{k = 1, l_k = i}^{r''} \beta_{k}(v_{i} + v_{m_k})v_{i}^{\top} + \sum_{k = 1, m_k = j}^{r''} \beta_{k}(v_{l_k} + v_{j})v_{j}^{\top} + \sum_{\substack{k = 1, l_k \neq i,\\ m_k \neq j}}^{r''} \beta_{k}(v_{l_k} + v_{m_k})(v_{l_k} + v_{m_k})^{\top}
    \end{align*}
    Note that if $l_k = i$ then the corresponding $m_k \neq j$ and vice versa. Since $\curlybracket{v_i}_{i=1}^r$ are orthogonal, the decomposition above implies
    \begin{gather}
        (v_i + v_j)v_i^{\top} = \sum_{k = 1, l_k = i}^{r''} \beta_{k}(v_{i} + v_{m_k})v_{i}^{\top} \label{eq: vplusv1}\\
        (v_i + v_j)v_j^{\top} =  \sum_{k = 1, m_k = j}^{r''} \beta_{k}(v_{l_k} + v_{j})v_{j}^{\top}\label{eq: vplusv2}\\
        \sum_{\substack{k = 1, l_k \neq i,\\ m_k \neq j}}^{r''} \beta_{k}(v_{l_k} + v_{m_k})(v_{l_k} + v_{m_k})^{\top} = 0
    \end{gather}
    But using the arguments in \eqnref{eq: v1} and \eqnref{eq: v2}, we can achieve \eqnref{eq: vplusv1} or \eqnref{eq: vplusv2}.

    Thus, we have shown that the set of rank-1 matrices as described in $\cB$ are linearly independent.
\end{proof}




In \lemref{lem: orthoset}, we discussed that in order to teach $\pphi^*$ sufficiently agent needs a feedback set of size $\frac{r(r+1)}{2} -1$ if the null set of $\pphi^*$ is known. We can establish this feedback set using the basis shown in \lemref{lem: basis}. We state this result in the following lemma.
\begin{lemma}\label{lem: orthocons}
    For a  given target matrix $\pphi^* = \sum_{i=1}^r \gamma_iv_iv_i^{\top}$ and basis set of matrices $\cB$ as shown in \lemref{lem: basis}, the following set spans a subspace of dimension $\frac{r(r+1)}{2} -1$ in $\symm$. 
\begin{equation*}
\mathcal{O}_{\cB} := \left\{
\begin{aligned}
&v_1v_1^{\top} - \lambda_{11}yy^{\top}, v_2v_2^{\top} - \lambda_{22}yy^{\top}, (v_1 + v_2)(v_1 + v_2)^{\top} - \lambda_{12}yy^{\top}, \ldots,\\
&v_rv_r^{\top} - \lambda_{rr}yy^{\top}, (v_1 + v_r)(v_1 + v_r)^{\top} - \lambda_{1r}yy^{\top}, \ldots, \\
&(v_{r-1} + v_r)(v_{r-1} + v_r)^{\top} - \lambda_{(r-1)r}yy^{\top}
\end{aligned}
\right\}
\end{equation*}

\begin{equation*}
y\pphi^*y^{\top} \neq 0
\end{equation*}

\begin{equation*}
\forall i,j,\quad \lambda_{ii} = \frac{v_i\pphi^*v_i^{\top}}{y\pphi^*y^{\top}}, \quad \lambda_{ij} = \frac{(v_i + v_j)\pphi^*(v_i+ v_j)^{\top}}{y\pphi^*y^{\top}} \quad (i \neq j)
\end{equation*}


\end{lemma}
\begin{proof}
    Since $\pphi^*$ has at least $r$ positive eigenvalues there exists a vector $y \in \reals^p$ such that $y\pphi^*y^{\top} \neq 0$. It is straightforward to note that $\mathcal{O}_{\cB}$ is orthogonal to $\pphi^*$. As $\mathcal{O}_{\cB} \subset \text{span}\langle \cB \rangle$ and $\pphi^* \bot \mathcal{O}_{\cB}$, $\dim(\text{span}\langle \mathcal{O}_{\cB} \rangle) = \frac{r(r+1)}{2} -1$. 
\end{proof}

Now, we will complete the proof of the main result of the appendix here.

\begin{proof}[Proof of \thmref{thm: constructgeneral}]
Combining the results from \lemref{lem: nullset}, \lemref{lem: orthoset}, and \lemref{lem: orthocons}, we conclude that the feedback setup in \eqnref{eq: orthosat} can be effectively decomposed into teaching the null space and the span of the eigenvectors of $\pphi^*$. The constructed feedback sets ensure that $\pphi^*$ is uniquely identified up to a linear scaling factor with optimal sample efficiency.    
\end{proof}

\newpage
%\section{Feature learning with feedbacks: Constructing general activations}
%In this appendix, we will provide the proof of \thmref{thm: constructgeneral}. We prove the result in two parts- lower bound and upper bound on the number of feedbacks. 

\section{Proof of \thmref{thm: constructgeneral}: Lower bound}\label{app: constlb}
In this appendix, we provide the proof of the lower bound as stated in \thmref{thm: constructgeneral}. We proceed by first showing some useful properties on a valid feedback set $\cF(\reals^p,\maha, \pphi^*)$ for a target feature matrix $\pphi^*$. They are stated in \lemref{lem: inclusion} and \lemref{lem: unique}.

First, we consider a basic spanning property of matrices $(xx^\top - yy^\top)$ for any pair $(x,y) \in \cF$ in the space of symmetric matrices $\symm$.

\begin{lemma}\label{lem: inclusion}
    If $\pphi \in \mathcal{O}_{\pphi^*}$ such that $\text{span}\inner{\col{\pphi}} \subset \text{span}\inner{V_{\bracket{r}}}$ then $\pphi \in span \inner{\cF}$.
\end{lemma}
\begin{proof}
     Consider an $\pphi \in \mathcal{O}_{\pphi^*}$ such that $\text{span}\inner{\col{\pphi}} \subset \text{span}\inner{V_{\bracket{r}}}$. Note that the eigendecompostion of $\pphi$ (assume $\rank{\pphi} = r' < r$)
     \begin{align*}
         \pphi = \sum_{i = 1}^{r'} \delta_i\mu_i\mu_i^{\top}
     \end{align*}
     for orthogonal eigenvectors $\curlybracket{\mu_i}_{i=1}^{r'}$ and the corresponding eigenvalues $\curlybracket{\delta_i}_{i=1}^{r'}$ has the property that $span \inner{\curlybracket{\mu_i}_{i=1}^{r'}} \subset \text{span} \inner{V_{\bracket{r}}}$. Using the arguments exactly as shown in the second half of the proof of \lemref{lem: orthoset} we can show there exists $\lambda > 0$ such that $\pphi^* + \lambda \pphi \in \sf{VS}(\cF, \maha)$. But then $\pphi$ is not feature equivalent to $\pphi^*$. But this contradicts the assumption of $\cF$ being a valid feedback set. 
     %But using \lemref{lem: orthoset} and \lemref{lem: orthocons} we know that the dimension of the span of matrices that satisfy the condition in \lemref{lem: inclusion} is at the least $\frac{r(r+1)}{2} -1$. We can use \lemref{lem: orthocons} where $y = \sum_{i = 1}^r v_r$ (note $\pphi^*v \neq 0$). Thus, any basis matrix in $\mathcal{O}_{\cB}$ satisfy the conditions in \lemref{lem: inclusion}.
\end{proof}


\begin{lemma}\label{lem: unique}
    There exists vectors $U_{\bracket{p-r}} \subset \nul{\pphi^*}$ (of size $p - r $) such that $\text{span} \inner{U_{\bracket{p-r}} } = \nul{\pphi^*}$ and 
        for any vector $v \in U_{\bracket{p-r}}$, $vv^{\top} \in \text{span} \inner{\cF}$.
\end{lemma}
\begin{proof}
    Assuming the contrary, there exists $v \in \text{span} \inner{\nul{\pphi^*}}$ such that $vv^{\top} \notin \text{span} \inner{\cF}$.

    Now if $vv^{\top}\, \bot\, \cF$, then for any scalar $\lambda > 0$, $\pphi^* + \lambda vv^{\top}$ is both symmetric and positive semi-definite and satisfies all the conditions in \eqnref{eq: redsol} wrt $\cF$ a contradiction as $\pphi^* + \lambda vv^{\top}$ is not feature equivalent to $\pphi^*$. 
    
    So, consider the case when $vv^{\top}\, \not\perp\, \cF$. Let $\curlybracket{v_{r+1},\ldots,v_{p-1}}$ be an orthogonal extension\footnote{the set is not trivially empty in which case the proof follows easily} of $v$ such that $\curlybracket{v_{r+1},\ldots,v_{p-1}, v}$ forms a basis of $\nul{\pphi^*}$, i.e., in other words 
    \begin{align*}
    v \bot \curlybracket{v_{r+1},\ldots,v_{p-1}}\quad \&\quad \text{span} \inner{\curlybracket{v_{r+1},\ldots,v_{p-1}, v}} = \nul{\pphi^*}.
    \end{align*}
    We will first show that there exists some $\pphi'$ $(\not = \lambda\pphi^*, \text{for some } \lambda > 0)$ $\in \symm$ orthogonal to $\cF$ and furthermore $\curlybracket{v_{r+1},\ldots,v_{p-1}} \subset \nul{\pphi'}$ . 
    
    
    Consider the intersection (in the space $\symm$) of the orthogonal complement of the matrices $\curlybracket{v_{r+1}v_{r+1}^{\top},\ldots,v_{p-1}v_{p-1}^{\top}}$, denote it as $\mathcal{O}_{\sf{rest}}$, i.e.,
    \begin{align*}
        \mathcal{O}_{\sf{rest}} := \bigcap_{i = r+1}^{p-1} \mathcal{O}_{v_iv_i^{\top}} 
    \end{align*}
    Note that %\akash{double check this}
    \begin{align*}
        \dim(\mathcal{O}_{\sf{rest}}) = p(p+1)/2 - p+ r
    \end{align*}
    Since $vv^{\top}$ is in $\mathcal{O}_{\sf{rest}}$ and $\dim(\mathcal{O}_{\sf{rest}}) > 1$ there exists some $\pphi'$ such that $\pphi' \perp \pphi^*$, and also orthogonal to elements in the feedback set $\cF$. Thus, $\pphi'$ has a null set which includes the subset $\curlybracket{v_{r+1},\ldots,v_{p-1}}$. 
    
    Now, the rest of the proof involves showing existence of some scalar $\lambda > 0$ such that $\pphi^* + \lambda \pphi'$ satisfies the conditions of \eqnref{eq: redsol} for the feedback set $\cF$. Note that if $v\pphi'v^{\top} = 0$ then the proof is straightforward as $ \text{span} \inner{\curlybracket{v_{r+1},\ldots,v_{p-1}, v}} \subset \nul{\pphi'}$, which implies $\text{span} \inner{\col{\pphi'}} \subset \text{span} \inner{V_{[r]}}$. But this is precisely the condition for \lemref{lem: inclusion} to hold. 
    
     
     Without loss of generality assume that $v\pphi'v^{\top} > 0$. First note that the eigendecomposition of $\pphi'$ has eigenvectors that are contained in $V_{[r]} \cup \curlybracket{v}$. Consider some arbitrary choice of $\lambda > 0$, we will fix a value later. It is straightforward that $\pphi^* + \lambda \pphi'$ is symmetric for $\pphi^*$ and $\pphi'$ are symmetric. In order to show it is positive semi-definite, it suffices to show that
     \begin{align}
         \forall u \in \reals^p, u^{\top}(\pphi^* + \lambda \pphi') u \ge 0 \label{eq: psd}
     \end{align}
    Since  $\curlybracket{v_{r+1},\ldots, v_{p-1}} \subset \paren{\nul{\pphi^*} \cap \nul{\pphi'}}$ we can simplify \eqnref{eq: psd} to
    \begin{align}
        \forall u \in \text{span}\inner{V_{[r]} \cup \curlybracket{v}}, u^{\top}(\pphi^* + \lambda \pphi') u \ge 0 \label{eq: repsd}
    \end{align}
    Consider the decomposition of any arbitrary vector $u \in \text{span}\inner{V_{[r]} \cup \curlybracket{v}}$ as follows:
    \begin{gather}
        u = u_{[r]} + v', \textnormal{ such that } u_{[r]} \in \text{span}\inner{V_{[r]}}, v' \in \text{span} \inner{\{v\}} \label{eq: decom1}\\
        u_{[r]} := \sum_{i =1}^r \alpha_i v_i,\;\; \forall i\; \alpha_i \in \reals \label{eq: decom2}
    \end{gather}
    From here on we assume that $u_{[r]} \neq 0$. The alternate case is trivial as $v'^{\top}\pphi'v' > 0$.
    
    Now, we write the vectors as scalar multiples of their corresponding unit vectors
    \begin{gather}
        u_{[r]} = \delta_r \cdot \hat{u}_r,\;\; \hat{u}_r := \frac{u_{[r]}}{||u_{[r]}||^2_{V_{[r]}}}, ||u_{[r]}||^2_{V_{[r]}} := \sum_{i =1}^r \alpha_i^2 \label{eq: scale1}\\
        v' = \delta_{v'}\cdot \hat{v},\;\; \hat{v} := \frac{v}{||v||_2^2} \label{eq: scale2}
    \end{gather}
    \underline{\tt{Remark}}: Although we have computed the norm of $ u_{[r]}$  as $||u_{[r]}||^2_{V_{[r]}}$ in the orthonormal basis $V_{[r]}$, note that the norm remains unchanged (same as the $\ell_2$). $\ell_2$ is used for ease of analysis later on.
    
    Using the decomposition in \eqnref{eq: decom1}-(\ref{eq: decom2}), we can write \eqnref{eq: repsd} as
    \begin{align}
        u^{\top}(\pphi^* + \lambda \pphi')u &= (u_{[r]} + v')^{\top}(\pphi^* + \lambda \pphi')(u_{[r]} + v') \nonumber\\
        &= u_{[r]}^{\top} \pphi^*u_{[r]} + \lambda (u_{[r]} + v')^{\top}\pphi'(u_{[r]} + v')\nonumber\\
        & = \delta_r^2 \cdot\hat{u}_r^{\top}\pphi^*\hat{u}_r + \lambda\big( \delta_r^2 \cdot\hat{u}_r^{\top}\pphi'\hat{u}_r + 2 \delta_r \delta_{v'}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \delta^2_{v'}\cdot \hat{v}^{\top}\pphi'\hat{v} \big) \label{eq: eq1}
    \end{align}
    Since we want $u^{\top}(\pphi^* + \lambda \pphi')u \ge 0$ we can further simplify \eqnref{eq: eq1} as 
    \begin{align}
        \hat{u}_r^{\top}\pphi^*\hat{u}_r + \lambda\paren{ \hat{u}_r^{\top}\pphi'\hat{u}_r + 2 \textcolor{gray}{\frac{\delta_r\delta_{v'}}{\delta_r^2 }} \cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\frac{\delta^2_{v'}}{\delta^2_r}}\cdot \hat{v}^{\top}\pphi'\hat{v} } \underset{?}{\ge} 0 \label{eq: equiv1}\\
        \Longleftrightarrow \underbrace{\hat{u}_r^{\top}\pphi^*\hat{u}_r}_{\textcolor{red}{(1)}} + \lambda\paren{ \underbrace{\hat{u}_r^{\top}\pphi'\hat{u}_r}_{\textcolor{violet}{(3)}} + \underbrace{2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v} }_{\textcolor{blue}{(2)}}} \underset{?}{\ge} 0 \label{eq: equiv2}
    \end{align}
    where we have used $\xi = \frac{\delta_{v'}}{\delta_r}$. The next part of the proof we show that $\textcolor{red}{(1)}$ is lower bounded by a positive constant whereas $\textcolor{blue}{(2)}$ is upper bounded by a positive constant and there is a choice of $\lambda$ so that $\textcolor{blue}{(3)}$ is always smaller than $\textcolor{red}{(1)}$.
    
    Considering $\textcolor{red}{(1)}$ we note that $\hat{u}_r$ is a unit vector wrt the orthonormal set of basis $V_{[r]}$. Expanding using the eigendecomposition of \eqnref{eq: target}
    \begin{align*}
        \hat{u}_r^{\top}\pphi^*\hat{u}_r = \sum_{i=1}^r \frac{\alpha^2_i}{\sum_{i=1}^r \alpha_i^2}\cdot \gamma_i \ge \min_i \gamma_i > 0
    \end{align*}
    The last inequality follows as all the eigenvalues in the eigendecompostion are (strictly) positive. Denote this minimum eigenvalue as $\gamma_{\min} := \min_i \gamma_i$.
    
    Considering $\textcolor{blue}{(2)}$ note that only terms that are variable (i.e. could change value) is $\xi$ as $\hat{u}_r^{\top} \pphi' \hat{v}$ is 

    Note that $\hat{v}$ is a fixed vector and $\hat{u}_r$ has a fixed norm (using \eqnref{eq: scale1}-(\ref{eq: scale2})), so $|\hat{u}_r^{\top} \pphi' \hat{v}| \le C$ for some bounded constant $C > 0$ whereas $\hat{v}^{\top}\pphi'\hat{v}$ is already a constant. Now, $|2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v}|$ exceeds $\textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}$ only if
    \begin{align*}
        |2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v}| \ge |\textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}| %\ge |2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}|
        \Longleftrightarrow \frac{|\hat{u}_r^{\top} \pphi' \hat{v}|}{\hat{v}^{\top}\pphi'\hat{v}} \ge \textcolor{gray}{\xi} \implies \frac{C}{\hat{v}^{\top}\pphi'\hat{v}} \ge \textcolor{gray}{\xi}
    \end{align*}
    Rightmost inequality implies that $2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}$ is negative only for an $\textcolor{gray}{\xi}$ bounded from above by a positive constant. But since $\xi$ is non-negative 
    \begin{align*}
        |2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}| \le C' (\textnormal{bounded constant})
    \end{align*}
    Now using an argument similar to the second half of the proof of \lemref{lem: orthoset}, it is straight forward to show that there is a choice of $\lambda' > 0$ so that $\textcolor{violet}{(3)}$ is always smaller than $\textcolor{red}{(1)}$.

    Now, for $\lambda = \frac{\lambda'}{2\lceil C' \rceil \lambda''}$ where $\lambda''$ is chosen so that $\lambda_{\min} \ge \frac{\lambda'}{\lambda''}$, we note that
    \begin{align*}
        \hat{u}_r^{\top}\pphi^*\hat{u}_r + \lambda\paren{ \hat{u}_r^{\top}\pphi'\hat{u}_r + 2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v} } \ge \lambda_{\min} + \frac{\lambda'}{2\lceil C' \rceil \lambda''} \hat{u}_r^{\top}\pphi'\hat{u}_r -\frac{\lambda'}{2\lambda''} >  0.
    \end{align*}
    Using the equivalence in \eqnref{eq: eq1}, \eqnref{eq: equiv1} and \eqnref{eq: equiv2}, we have a choice of $\lambda > 0$ such that $u^{\top}(\pphi^* + \lambda \pphi')u \ge 0$ for any arbitrary vector $u \in \text{span}\inner{V_{[r]} \cup \curlybracket{v}}$. Hence, we have achieved the conditions in \eqnref{eq: repsd}, which is the simplification of \eqnref{eq: psd}. This implies that $\pphi^* + \lambda \pphi'$ is positive semi-definite. 
    
    This implies that there doesn't exist a $v \in \text{span} \inner{\nul{\pphi^*}}$ such that $vv^{\top} \notin \text{span} \inner{\cF}$ otherwise the assumption on $\cF$ to be an oblivious feedback set for $\pphi^*$ is violated. Thus, the statement of \lemref{lem: unique} has to hold.
\end{proof}

%\begin{proof}[Proof of \lemref{lemma: lowerbound}]
%\akash{there are a number of things flying here. First, define some of the set of matrices carefully. Second, write down the statements below as lemmas at the start of the supplementary to highlight their use. They are being used at different places. If possible it would be good to include them in the main paper as well.}

    % The key idea of the proof is that any feedback set, say $\cF$ for the oblivious teaching in \eqnref{eq: sol} must have matrices that satisfy the following properties:
    % \begin{enumerate}
    %     \item[\lemref{lem: inclusion}] if $\pphi \in \mathcal{O}_{\pphi^*}$ such that $\text{span}\inner{\col{\pphi}} \subset \text{span}\inner{V_{\bracket{r}}}$ then $\pphi \in \text{span} \inner{\cF}$.
    %     \item[\lemref{lem: unique}] there exists vectors $U_{\bracket{d-r}} \subset \nul{\pphi^*}$ (of size $d - r $) such that $\text{span} \inner{U_{\bracket{d-r}} } = \nul{\pphi^*}$ and 
    %     for any vector $v \in U_{\bracket{d-r}}$, $vv^{\top} \in \text{span} \inner{\cF}$.
    % \end{enumerate}

    \subsection{Proof of lower bound in \thmref{thm: constructgeneral}}
    
    
    In the following, we provide proof of the main statement on the lower bound of the size of a feedback set.
    
    %\begin{proof}[Proof of lower bound in \thmref{thm: constructgeneral}]
    
    If any of the two lemmas (\ref{lem: inclusion}-\ref{lem: unique}) are violated, we can show there exists $\lambda > 0$ and $\pphi$ such that $\pphi^* + \lambda \pphi \in \sf{VS}(\cF,\maha)$. In  order to ensure these statements, the feedback set should have $\paren{\frac{r(r+1)}{2} + (d - r) - 1}$ many elements which proves the lower bound on $\cF$. 
    
    % Now, we argue the necessity of these statements.

    % Consider the first statement. Consider an $\pphi \in \mathcal{O}_{\pphi^*}$ such that $\text{span}\inner{\col{\pphi}} \subset \text{span}\inner{V_{\bracket{r}}}$. Note that the eigendecompostion of $\pphi$ (assume $\rank{\pphi} = r' < r$)
    %  \begin{align*}
    %      \pphi = \sum_{i = 1}^{r'} \delta_i\mu_i\mu_i^{\top}
    %  \end{align*}
    %  for orthogonal eigenvectors $\curlybracket{\mu_i}_{i=1}^{r'}$ and the corresponding eigenvalues $\curlybracket{\delta_i}_{i=1}^{r'}$ has the property that $\text{span} \inner{\curlybracket{\mu_i}_{i=1}^{r'}} \subset \text{span} \inner{V_{\bracket{r}}}$. Using the arguments exactly as shown in the second half of the proof of \lemref{lem: orthoset} we can show there exists $\lambda > 0$ such that $\pphi^* + \lambda \pphi \in \sf{VS}(\cF, \maha)$. But then $\pphi \not\sim_{R_l} \pphi^*$. But this contradicts the assumption on $\cF$ being a valid oblivious feedback set for \eqnref{eq: sol} up to linear scaling relation $\sim_{R_l}$. 
     
     
     But using \lemref{lem: orthoset} and \lemref{lem: orthocons} we know that the dimension of the \text{span} of matrices that satisfy the condition in \lemref{lem: inclusion} is at the least $\frac{r(r+1)}{2} -1$. We can use \lemref{lem: orthocons} where $y = \sum_{i = 1}^r v_r$ (note $\pphi^*v \neq 0$). Thus, any basis matrix in $\mathcal{O}_{\cB}$ satisfy the conditions in \lemref{lem: inclusion}.

    % Now, consider the second statement. Assuming the contrary, there exists $v \in \text{span} \inner{\nul{\pphi^*}}$ such that $vv^{\top} \notin \text{span} \inner{\cF}$.

    % Now if $vv^{\top}\, \bot\, \cF$, then for any scalar $\lambda > 0$, $\pphi^* + \lambda vv^{\top}$ is both symmetric and positive semi-definite and satisfies all the conditions in \eqnref{eq: redsol} wrt $\cF$ a contradiction as $\pphi^* + \lambda vv^{\top} \not\sim_{R_l} \pphi^*$. 
    
    % So, consider the case when $vv^{\top}\, \not\perp\, \cF$. Let $\curlybracket{v_{r+1},\ldots,v_{d-1}}$ be an orthogonal extension of $v$ such that $\curlybracket{v_{r+1},\ldots,v_{d-1}, v}$ forms a basis of $\nul{\pphi^*}$, i.e., in other words 
    % \begin{align*}
    % v \bot \curlybracket{v_{r+1},\ldots,v_{d-1}}\quad \&\quad \text{span} \inner{\curlybracket{v_{r+1},\ldots,v_{d-1}, v}} = \nul{\pphi^*}.
    % \end{align*}
    % We will first show that there exists some $\pphi'$ $(\not \sim_{R_l} \pphi^*)$ $\in \symm$ orthogonal to $\cF$ and furthermore $\curlybracket{v_{r+1},\ldots,v_{d-1}} \subset \nul{\pphi'}$ . 
    
    
    % Consider the intersection (in the space $\symm$) of the orthogonal complement of the matrices $\curlybracket{v_{r+1}v_{r+1}^{\top},\ldots,v_{d-1}v_{d-1}^{\top}}$, denote it as $\mathcal{O}_{rest}$, i.e.,
    % \begin{align*}
    %     \mathcal{O}_{rest} := \bigcap_{i = r+1}^{d-1} \mathcal{O}_{v_iv_i^{\top}} 
    % \end{align*}
    % Note that
    % \begin{align*}
    %     \dim(\mathcal{O}_{rest}) = D - d + r
    % \end{align*}
    % Since $vv^{\top}$ is in $\mathcal{O}_{rest}$ and $\dim(\mathcal{O}_{rest}) > 1$ there exists some $\pphi'$ such that $\pphi' \perp \pphi^*$, and also orthogonal to elements in the feedback set $\cF$. Thus, $\pphi'$ has a null set which includes the subset $\curlybracket{v_{r+1},\ldots,v_{d-1}}$. 
    
    % Now, the rest of the proof involves showing existence of some scalar $\lambda > 0$ such that $\pphi^* + \lambda \pphi'$ satisfies the conditions of \eqnref{eq: redsol} for the feedback set $\cF$. Note that if $v\pphi'v^{\top} = 0$ then the proof is straightforward as $ \text{span} \inner{\curlybracket{v_{r+1},\ldots,v_{d-1}, v}} \subset \nul{\pphi'}$, which implies $\text{span} \inner{\col{\pphi'}} \subset \text{span} \inner{V_{[r]}}$. But this is precisely the condition for \lemref{lem: inclusion} to hold. 
    
     
    %  Without loss of generality assume that $v\pphi'v^{\top} > 0$. First note that the eigendecomposition of $\pphi'$ has eigenvectors that are contained in $V_{[r]} \cup \curlybracket{v}$. Consider some arbitrary choice of $\lambda > 0$, we will fix a value later. It is straightforward that $\pphi^* + \lambda \pphi'$ is symmetric for $\pphi^*$ and $\pphi'$ are symmetric. In order to show it is positive semi-definite, it suffices to show that
    %  \begin{align}
    %      \forall u \in \reals^p, u^{\top}(\pphi^* + \lambda \pphi') u \ge 0 \label{eq: psd}
    %  \end{align}
    % Since  $\curlybracket{v_{r+1},\ldots, v_{d-1}} \subset \paren{\nul{\pphi^*} \cap \nul{\pphi'}}$ we can simplify \eqnref{eq: psd} to
    % \begin{align}
    %     \forall u \in \text{span}\inner{V_{[r]} \cup \curlybracket{v}}, u^{\top}(\pphi^* + \lambda \pphi') u \ge 0 \label{eq: repsd}
    % \end{align}
    % Consider the decomposition of any arbitrary vector $u \in \text{span}\inner{V_{[r]} \cup \curlybracket{v}}$ as follows:
    % \begin{gather}
    %     u = u_{[r]} + v', \textnormal{ such that } u_{[r]} \in \text{span}\inner{V_{[r]}}, v' \in \text{span} \inner{\{v\}} \label{eq: decom1}\\
    %     u_{[r]} := \sum_{i =1}^r \alpha_i v_i,\;\; \forall i\; \alpha_i \in \reals \label{eq: decom2}
    % \end{gather}
    % From here on we assume that $u_{[r]} \neq 0$. The alternate case is trivial as $v'^{\top}\pphi'v' > 0$.
    
    % Now, we write the vectors as scalar multiples of their corresponding unit vectors
    % \begin{gather}
    %     u_{[r]} = \delta_r \cdot \hat{u}_r,\;\; \hat{u}_r := \frac{u_{[r]}}{||u_{[r]}||^2_{V_{[r]}}}, ||u_{[r]}||^2_{V_{[r]}} := \sum_{i =1}^r \alpha_i^2 \label{eq: scale1}\\
    %     v' = \delta_{v'}\cdot \hat{v},\;\; \hat{v} := \frac{v}{||v||_2^2} \label{eq: scale2}
    % \end{gather}
    % \tt{Remark}: Although we have computed the norm of $ u_{[r]}$  as $||u_{[r]}||^2_{V_{[r]}}$ in the orthonormal basis $V_{[r]}$, note that the norm remains unchanged (same as the $\ell_2$). $\ell_2$ is used for ease of analysis later on.
    
    % Using the decomposition in \eqnref{eq: decom1}-(\ref{eq: decom2}), we can write \eqnref{eq: repsd} as
    % \begin{align}
    %     u^{\top}(\pphi^* + \lambda \pphi')u &= (u_{[r]} + v')^{\top}(\pphi^* + \lambda \pphi')(u_{[r]} + v') \nonumber\\
    %     &= u_{[r]}^{\top} \pphi^*u_{[r]} + \lambda (u_{[r]} + v')^{\top}\pphi'(u_{[r]} + v')\nonumber\\
    %     & = \delta_r^2 \cdot\hat{u}_r^{\top}\pphi^*\hat{u}_r + \lambda\big( \delta_r^2 \cdot\hat{u}_r^{\top}\pphi'\hat{u}_r + 2 \delta_r \delta_{v'}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \delta^2_{v'}\cdot \hat{v}^{\top}\pphi'\hat{v} \big) \label{eq: eq1}
    % \end{align}
    % Since we want $u^{\top}(\pphi^* + \lambda \pphi') \ge 0$ we can further simplify \eqnref{eq: eq1} as 
    % \begin{align}
    %     \hat{u}_r^{\top}\pphi^*\hat{u}_r + \lambda\paren{ \hat{u}_r^{\top}\pphi'\hat{u}_r + 2 \textcolor{gray}{\frac{\delta_r\delta_{v'}}{\delta_r^2 }} \cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\frac{\delta^2_{v'}}{\delta^2_r}}\cdot \hat{v}^{\top}\pphi'\hat{v} } \underset{?}{\ge} 0 \label{eq: equiv1}\\
    %     \Longleftrightarrow \underbrace{\hat{u}_r^{\top}\pphi^*\hat{u}_r}_{\textcolor{red}{(1)}} + \lambda\paren{ \underbrace{\hat{u}_r^{\top}\pphi'\hat{u}_r}_{\textcolor{violet}{(3)}} + \underbrace{2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v} }_{\textcolor{blue}{(2)}}} \underset{?}{\ge} 0 \label{eq: equiv2}
    % \end{align}
    % where we have used $\xi = \frac{\delta_{v'}}{\delta_r}$. The next part of the proof we show that $\textcolor{red}{(1)}$ is lower bounded by a positive constant whereas $\textcolor{blue}{(2)}$ is upper bounded by a positive constant and there is a choice of $\lambda$ so that $\textcolor{blue}{(3)}$ is always smaller than $\textcolor{red}{(1)}$.
    
    % Considering $\textcolor{red}{(1)}$ we note that $\hat{u}_r$ is a unit vector wrt the orthonormal set of basis $V_{[r]}$. Expanding using the eigendecomposition of \eqnref{eq: target}
    % \begin{align*}
    %     \hat{u}_r^{\top}\pphi^*\hat{u}_r = \sum_{i=1}^r \frac{\alpha^2_i}{\sum_{i=1}^r \alpha_i^2}\cdot \gamma_i \ge \min_i \gamma_i > 0
    % \end{align*}
    % The last inequality follows as all the eigenvalues in the eigendecompostion are (strictly) positive. Denote this minimum eigenvalue as $\gamma_{\min} := \min_i \gamma_i$.
    
    % Considering $\textcolor{blue}{(2)}$ note that only terms that are variable (i.e. could change value) is $\xi$ as $\hat{u}_r^{\top} \pphi' \hat{v}$ is 

    % Note that $\hat{v}$ is a fixed vector and $\hat{u}_r$ has a fixed norm (using \eqnref{eq: scale1}-(\ref{eq: scale2})), so $|\hat{u}_r^{\top} \pphi' \hat{v}| \le C$ for some bounded constant $C > 0$ whereas $\hat{v}^{\top}\pphi'\hat{v}$ is already a constant. Now, $|2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v}|$ exceeds $\textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}$ only if
    % \begin{align*}
    %     |2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v}| \ge |\textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}| %\ge |2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}|
    %     \Longleftrightarrow \frac{|\hat{u}_r^{\top} \pphi' \hat{v}|}{\hat{v}^{\top}\pphi'\hat{v}} \ge \textcolor{gray}{\xi} \implies \frac{C}{\hat{v}^{\top}\pphi'\hat{v}} \ge \textcolor{gray}{\xi}
    % \end{align*}
    % Rightmost inequality implies that $2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}$ is negative only for an $\textcolor{gray}{\xi}$ bounded from above by a positive constant. But since $\xi$ is non-negative 
    % \begin{align*}
    %     |2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v}| \le C' (\textnormal{bounded constant})
    % \end{align*}
    % Now using an argument similar to the second half of the proof of \lemref{lem: orthoset}, it is straight forward to show that there is a choice of $\lambda' > 0$ so that $\textcolor{violet}{(3)}$ is always smaller than $\textcolor{red}{(1)}$.

    % Now, for $\lambda = \frac{\lambda'}{2\lceil C' \rceil \lambda''}$ where $\lambda''$ is chosen so that $\lambda_{\min} \ge \frac{\lambda'}{\lambda''}$, we note that
    % \begin{align*}
    %     \hat{u}_r^{\top}\pphi^*\hat{u}_r + \lambda\paren{ \hat{u}_r^{\top}\pphi'\hat{u}_r + 2 \textcolor{gray}{\xi}\cdot \hat{u}_r^{\top} \pphi' \hat{v} + \textcolor{gray}{\xi^2}\cdot \hat{v}^{\top}\pphi'\hat{v} } \ge \lambda_{\min} + \frac{\lambda'}{2\lceil C' \rceil \lambda''} \hat{u}_r^{\top}\pphi'\hat{u}_r -\frac{\lambda'}{2\lambda''} >  0.
    % \end{align*}
    % Using the equivalence in \eqnref{eq: eq1}, \eqnref{eq: equiv1} and \eqnref{eq: equiv2}, we have a choice of $\lambda > 0$ such that $u^{\top}(\pphi^* + \lambda \pphi')u \ge 0$ for any arbitrary vector $u \in \text{span}\inner{V_{[r]} \cup \curlybracket{v}}$. Hence, we have achieved the conditions in \eqnref{eq: repsd}, which is the simplification of \eqnref{eq: psd}. This implies that $\pphi^* + \lambda \pphi'$ is positive semi-definite. 
    
    % This implies that there doesn't exist a $v \in \text{span} \inner{\nul{\pphi^*}}$ such that $vv^{\top} \notin \text{span} \inner{\cF}$ otherwise the assumption on $\cF$ to be an oblivious feedback set for $\pphi^*$ is violated. Thus, the statement \lemref{lem: unique} has to hold. 
    
    
    Since the dimension of $\nul{\pphi^*}$ is at least $(d-r)$ thus there are at least $(d-r)$ directions or linearly independent matrices (in $\symm$) that need to be spanned by $\cF$.

    Thus, \lemref{lem: inclusion} implies there are $\frac{r(r+1)}{2} -1$ linearly independent matrices (in $\mathcal{O}_{\pphi^*}$) that need to be spanned by $\cF$. Similarly, \lemref{lem: unique} implies there are $p-r$ linearly independent matrices (in $\mathcal{O}_{\pphi^*}$) that need to be spanned by $\cF$. Note that the column vectors of these matrices from the two statements are spanned by orthogonal set of vectors, i.e. one by $V_{[r]}$ and the other by $\nul{\pphi^*}$ respectively. Thus, these $\frac{r(r+1)}{2} -1 + (p-r)$ are linearly independent in $\symm$, but this forces a lower bound on the size of $\cF$ (a lower dimensional span can't contain a set of vectors spanning higher dimensional space). This completes the proof of the lower bound in \thmref{thm: constructgeneral}.

    %\end{proof}

\iffalse

\section{Proof of the Upper Bound in \thmref{thm:constructgeneral}}

We provide the proof of the upper bound stated in \thmref{thm:constructgeneral}.

\subsection{Eigendecomposition of target feature matrix}

Consider the eigendecomposition of the matrix $\pphi^*$. There exists a set of orthonormal vectors $\{v_1, v_2, \ldots, v_r\}$ with corresponding eigenvalues $\{\gamma_1, \gamma_2, \ldots, \gamma_r\}$ such that
\begin{align}
    \pphi^* = \sum_{i=1}^r \gamma_i v_i v_i^{\top} \label{eq:target}
\end{align}
Denote the set of orthogonal vectors $\{v_1, v_2, \ldots, v_r\}$ as $V_{(r)}$.

\subsection{Orthogonal Extension to a Basis}

Extend the set $V_{(r)}$ to an orthonormal basis for $\mathbb{R}^p$ by including vectors $\{v_{r+1}, \ldots, v_p\}$. Let $V_{(p-r)} = \{v_{r+1}, \ldots, v_p\}$, and define the complete basis as
\[
    V_{(d)} = \{v_1, v_2, \ldots, v_p\}.
\]
Note that $\{v_{r+1}, \ldots, v_p\}$ precisely defines the null space of $\pphi^*$:
\[
    \mathcal{N}(\pphi^*) = \text{span}\{v_{r+1}, \ldots, v_p\}.
\]

\subsection{Strategy for Teaching the Null Set and Eigenvectors}

The key idea is to manipulate the null space to satisfy the feedback set condition in \eqnref{eq:orthosat} for the target matrix $\pphi^*$. Since $\pphi^*$ has rank $r \leq d$, the number of degrees of freedom is $\frac{r(r+1)}{2}$. Alternatively, the null space of $\pphi^*$, which has dimension $d - r$, constrains the remaining entries in $\pphi^*$. Leveraging this intuition, the teacher can provide pairs $(y, z) \in \mathcal{X}^2$ to teach the null space and the eigenvectors $\{v_1, v_2, \ldots, v_r\}$ separately. We ensure the optimality of this strategy concerning sample efficiency in the following lemmas.

\subsection{Teaching the Null Space}

Consider the partial feedback set
\[
    \cF_{\text{null}} = \{(0, v_i)\}_{i=r+1}^p.
\]
\begin{lemma}\label{lem:nullset}
    If the teacher provides the set $\cF_{\text{null}}$, then the null space of any positive semidefinite (PSD) symmetric matrix $\pphi'$ that satisfies \eqnref{eq:orthosat} contains the span of $\{v_{r+1}, \ldots, v_p\}$, i.e.,
    \[
        \{v_{r+1}, \ldots, v_p\} \subseteq \mathcal{N}(\pphi').
    \]
\end{lemma}
\begin{proof}
    Let $\pphi' \in \symmp$ be a matrix satisfying \eqnref{eq:orthosat} (notably, $\pphi^*$ satisfies this condition). The equality constraints imposed by $\cF_{\text{null}}$ are
    \[
        \forall (0, v) \in \cF_{\text{null}}, \quad v^{\top} \pphi' v = 0.
    \]
    Since $\{v_{r+1}, \ldots, v_p\}$ are linearly independent vectors, it suffices to show that
    \begin{align}
        v^{\top} \pphi' v = 0 \quad \forall v \in V_{(p-r)} \implies \pphi' v = 0 \quad \forall v \in V_{(d-r)}. \label{eq:lemmain}
    \end{align}
    
    Consider the eigendecomposition of $\pphi'$:
    \[
        \pphi' = \sum_{i=1}^{s} \gamma_i' u_i u_i^{\top},
    \]
    where $\{u_i\}_{i=1}^{s}$ are the eigenvectors and $\{\gamma_i'\}_{i=1}^{s}$ are the corresponding eigenvalues of $\pphi'$. Assume $x \neq 0 \in \mathbb{R}^p$ satisfies
    \[
        x^{\top} \pphi' x = 0.
    \]
    Decompose $x$ as $x = \sum_{i=1}^s a_i u_i + v'$, where $a_i$ are scalars and $v' \perp \{u_i\}_{i=1}^s$. Expanding the quadratic form:
    \begin{align*}
        x^{\top} \pphi' x &= \left(\sum_{i=1}^s a_i u_i + v'\right)^{\top} \pphi' \left(\sum_{i=1}^s a_i u_i + v'\right) \\
        &= \left(\sum_{i=1}^s a_i u_i\right)^{\top} \pphi' \left(\sum_{i=1}^s a_i u_i\right) + v'^{\top} \pphi' \left(\sum_{i=1}^s a_i u_i\right) + \left(\sum_{i=1}^s a_i u_i\right)^{\top} \pphi' v' + v'^{\top} \pphi' v' \\
        &= \sum_{i=1}^s a_i^2 \gamma_i' + 2 v'^{\top} \pphi' \left(\sum_{i=1}^s a_i u_i\right) + v'^{\top} \pphi' v'.
    \end{align*}
    Since $v' \perp \{u_i\}_{i=1}^s$ and $\pphi'$ is symmetric, the cross terms vanish:
    \[
        2 v'^{\top} \pphi' \left(\sum_{i=1}^s a_i u_i\right) = 0 \quad \text{and} \quad v'^{\top} \pphi' v' = 0.
    \]
    Therefore,
    \[
        \sum_{i=1}^s a_i^2 \gamma_i' = 0.
    \]
    Since $\pphi'$ is PSD, $\gamma_i' \geq 0$ for all $i$, implying $a_i = 0$ for all $i$. Consequently,
    \[
        \pphi' x = \pphi' v' = 0.
    \]
    Thus, $x \in \mathcal{N}(\pphi')$, establishing \eqnref{eq:lemmain}.
    
    Therefore, if the teacher provides $\cF_{\text{null}}$, any solution $\pphi'$ to \eqnref{eq:orthosat} must satisfy
    \[
        \mathcal{N}(\pphi') \supseteq \text{span}\{v_{r+1}, \ldots, v_p\}.
    \]
\end{proof}

\subsection{Teaching the Eigenvectors $V_{(r)}$}

Next, we aim to teach the span of the eigenvectors $\mathcal{S}_{\pphi^*} = \text{span}\{v_1, v_2, \ldots, v_r\}$ by specifying the eigendecomposition of $\pphi^*$. We demonstrate that within the version space $\textsf{VS}(\maha, \cF_{\text{null}})$, a feedback set of size at most $\frac{r(r+1)}{2} - 1$ suffices to teach $\pphi^*$ up to a linear scaling relation $\sim_{R_l}$.

Consider reformulating the problem in \eqnref{eq:orthosat} as
\begin{align}
    \forall (y, z) \in \mathcal{F}(\mathcal{X}, \textsf{VS}(\maha, \cF_{\text{null}}), \pphi^*), \quad \pphi \cdot (yy^{\top} - zz^{\top}) = 0 \label{eq:redorthosat}
\end{align}
where the feedback set $\mathcal{F}(\mathcal{X}, \textsf{VS}(\maha, \cF_{\text{null}}), \pphi^*)$ is designed to solve within the restricted space $\textsf{VS}(\maha, \cF_{\text{null}})$.

\begin{lemma}\label{lem:orthoset}
    In the context of \eqnref{eq:redorthosat}, where the null space $\mathcal{N}(\pphi^*)$ of the target matrix $\pphi^*$ is known, the teacher can sufficiently and necessarily identify a feedback set $\mathcal{F}(\mathcal{X}, \textsf{VS}(\maha, \cF_{\text{null}}), \pphi^*)$ of size $\frac{r(r+1)}{2} - 1$ for oblivious teaching up to the linear scaling relation $\sim_{R_l}$.
\end{lemma}
\begin{proof}
    Any solution $\pphi'$ to \eqnref{eq:redorthosat} has its column space spanned exactly by $V_{(r)}$. Specifically, the eigenvectors of $\pphi'$ reside in $\text{span}\{V_{(r)}\}$. Given that $\pphi^*$ has rank $r$, there are $\frac{r(r+1)}{2}$ degrees of freedom corresponding to the entries of $\pphi^*$ that need to be determined.

    Consider the set of indices $\{j_1, j_2, \ldots, j_r\}$ corresponding to the linearly independent columns of $\pphi^*$. Define the set of matrices
    \[
        \mathcal{S} = \left\{\pphi^{(i,j)} \,\bigg|\, i \in [d], \, j \in \{j_1, j_2, \ldots, j_r\}, \, \pphi^{(i,j)}_{i'j'} = \mathds{1}[i' \in \{i,j\}, \, j' \in \{i,j\} \setminus \{i'\}]\right\}.
    \]
    This set $\mathcal{S}$ forms a basis for generating any matrix with independent columns along the specified indices.

    The span of $\mathcal{S}_{\pphi^*}$ induces a subspace of symmetric matrices of dimension $\frac{r(r+1)}{2}$ within the vector space $\textsf{symm}(\mathbb{R}^p)$. Therefore, selecting a feedback set of size $\frac{r(r+1)}{2} - 1$ in the orthogonal complement of $\pphi^*$, denoted as $\mathcal{O}_{\pphi^*}$, suffices to teach $\pphi^*$ up to a positive scaling factor. Specifically, this feedback set can be constructed using the basis established in \lemref{lem:orthobasis}.

    To demonstrate necessity, assume for contradiction that a smaller feedback set $\mathcal{F}_{\text{small}}$ exists. This would imply the existence of a matrix $\pphi'$ in $\textsf{VS}(\maha, \cF_{\text{null}})$ orthogonal to all elements in $\mathcal{F}_{\text{small}}$ but not a scalar multiple of $\pphi^*$. If $\pphi'$ is PSD, it satisfies \eqnref{eq:redorthosat} and contradicts the uniqueness of $\pphi^*$ up to scaling. If $\pphi'$ is not PSD, by \lemref{lem:sum}, there exists a scalar $\lambda > 0$ such that $\pphi^* + \lambda \pphi'$ is PSD, again contradicting the minimality of the feedback set.

    Hence, a feedback set of size exactly $\frac{r(r+1)}{2} - 1$ is both sufficient and necessary for oblivious teaching of $\pphi^*$ up to linear scaling.
\end{proof}

\subsection{Constructing the Feedback Set}

Up to this point, we have established the theoretical foundation for the required feedback set size. We now outline a constructive method to achieve this feedback set.

Consider the following union of rank-1 matrices:
\[
    \mathcal{B} = \left\{
        v_1 v_1^{\top},
        v_2 v_2^{\top}, \, (v_1 + v_2)(v_1 + v_2)^{\top},
        \ldots,
        v_r v_r^{\top}, \, (v_1 + v_r)(v_1 + v_r)^{\top}, \ldots, \, (v_{r-1} + v_r)(v_{r-1} + v_r)^{\top}
    \right\}.
\]
\begin{lemma}\label{lem:orthobasis}
    Let $\{v_i\}_{i=1}^r$ be a set of orthonormal vectors. Then, the set of rank-1 matrices
    \[
        \mathcal{B} = \left\{
            v_1 v_1^{\top},
            v_2 v_2^{\top}, \, (v_1 + v_2)(v_1 + v_2)^{\top},
            \ldots,
            v_r v_r^{\top}, \, (v_1 + v_r)(v_1 + v_r)^{\top}, \ldots, \, (v_{r-1} + v_r)(v_{r-1} + v_r)^{\top}
        \right\}
    \]
    is linearly independent in the vector space $\symm(\mathbb{R}^p)$.
\end{lemma}
\begin{proof}
    We prove the linear independence of $\mathcal{B}$ by contradiction, considering two cases:

    \textbf{Case 1:} Assume that for some $i \in [r]$, the matrix $v_i v_i^{\top}$ can be expressed as a linear combination of other matrices in $\mathcal{B} \setminus \{v_i v_i^{\top}\}$. Then, there exist scalars $\alpha_j$ and $\beta_k$ such that
    \[
        v_i v_i^{\top} = \sum_{j \neq i} \alpha_j v_j v_j^{\top} + \sum_{k \neq i} \beta_k (v_l + v_m)(v_l + v_m)^{\top},
    \]
    where $l$ and $m$ are indices distinct from $i$. Expanding the right-hand side and leveraging the orthonormality of $\{v_i\}$ leads to a contradiction, as the presence of $v_i$ in the basis cannot be accounted for by linear combinations of other orthogonal vectors.

    \textbf{Case 2:} Assume that for some distinct $i, j \in [r]$, the matrix $(v_i + v_j)(v_i + v_j)^{\top}$ can be expressed as a linear combination of other matrices in $\mathcal{B}$. Expanding and simplifying similarly leads to a contradiction due to the orthogonality of the vectors.

    Since neither $v_i v_i^{\top}$ nor $(v_i + v_j)(v_i + v_j)^{\top}$ can be written as linear combinations of other elements in $\mathcal{B}$, the set $\mathcal{B}$ is linearly independent.
\end{proof}

\begin{lemma}\label{lem:orthocons}
    Given the target matrix $\pphi^* = \sum_{i=1}^r \gamma_i v_i v_i^{\top}$ and the basis set of matrices $\mathcal{B}$ as defined in \lemref{lem:orthobasis}, the following set spans a subspace of dimension $\frac{r(r+1)}{2} - 1$ in $\symm(\mathbb{R}^p)$:
    \[
        \mathcal{O}_{\mathcal{B}} = \left\{
            v_1 v_1^{\top} - \lambda_{11} yy^{\top}, \,
            v_2 v_2^{\top} - \lambda_{22} yy^{\top}, \,
            (v_1 + v_2)(v_1 + v_2)^{\top} - \lambda_{12} yy^{\top}, \,
            \ldots, \,
            v_r v_r^{\top} - \lambda_{rr} yy^{\top}, \,
            (v_1 + v_r)(v_1 + v_r)^{\top} - \lambda_{1r} yy^{\top}, \,
            \ldots, \,
            (v_{r-1} + v_r)(v_{r-1} + v_r)^{\top} - \lambda_{(r-1)r} yy^{\top}
        \right\},
    \]
    where $y \pphi^* y^{\top} \neq 0$ and for all $i, j$,
    \[
        \lambda_{ii} = \frac{v_i \pphi^* v_i^{\top}}{y \pphi^* y^{\top}}, \quad \lambda_{ij} = \frac{(v_i + v_j) \pphi^* (v_i + v_j)^{\top}}{y \pphi^* y^{\top}} \quad \text{for } i \neq j.
    \]
\end{lemma}
\begin{proof}
    Since $\pphi^*$ has at least $r$ positive eigenvalues, there exists a vector $y \in \mathbb{R}^p$ such that $y \pphi^* y^{\top} \neq 0$. Observe that each element of $\mathcal{O}_{\mathcal{B}}$ is orthogonal to $\pphi^*$:
    \[
        \pphi^* \cdot (v_i v_i^{\top} - \lambda_{ii} yy^{\top}) = v_i^{\top} \pphi^* v_i - \lambda_{ii} y \pphi^* y^{\top} = 0,
    \]
    and similarly for $(v_i + v_j)(v_i + v_j)^{\top} - \lambda_{ij} yy^{\top}$.

    The set $\mathcal{O}_{\mathcal{B}}$ is a subset of $\text{span}\langle \mathcal{B} \rangle$. Since $\pphi^* \perp \mathcal{O}_{\mathcal{B}}$, the dimension of $\text{span}\langle \mathcal{O}_{\mathcal{B}} \rangle$ is $\frac{r(r+1)}{2} - 1$.
\end{proof}

\subsection{Finalizing the Feedback Set Construction}

Combining the results from \lemref{lem:nullset}, \lemref{lem:orthoset}, and \lemref{lem:orthocons}, we conclude that the feedback setup in \eqnref{eq:orthosat} can be effectively decomposed into teaching the null space and the span of the eigenvectors of $\pphi^*$. The constructed feedback sets ensure that $\pphi^*$ is uniquely identified up to a linear scaling factor with optimal sample efficiency.
\fi
\newpage

\newpage
