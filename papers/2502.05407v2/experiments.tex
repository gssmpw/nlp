\section{Experimental Setup}\label{sec: experiments}
We empirically validate our theoretical framework for learning feature matrices. Our experiments\footnote{\url{https://anonymous.4open.science/r/learnsparsefeatureswithfeedback-688E}} examine different feedback mechanisms and teaching strategies across both synthetic tasks and large-scale neural networks.\vspace{-4mm}
%To validate the theoretical bounds on feedback complexity as shown in \secref{sec: construct} and \secref{sec: sample}, we perform experiments on target feature matrices derived from two sources: Mahalanbis matrix trained for a kernel classifier via RFM~\cite{rfm}, and sparse autoencoders (SAE) for large language model-Pythia-70M~\cite{pythia}.

%\paragraph{Feedback Methods:} We design an agent that uses four different ways of feedback mechanism: (1) \tt{Eigendecomposition}: Using \lemref{lem: basis} a feedback set is constructed that leverages low rank structure of $\pphi$, (2) \tt{Sparse Constructive}: Using sparse basis in \eqnref{eq: sparsebasis}, a 2-sparse feedbacks are constructed, (3) \tt{Random Sampling}: Agent receives activations from a Lebesgue distribution, and designs feedbacks that spans $\mathcal{O}_{\pphi^*}$, and (4) \tt{Sparse Sampling}: Agent receives activations from a sparse distribution (with index-wise binary probabilities $p_i$) and designs feedbacks that spans $\mathcal{O}_{\pphi^*}$ using $s$-sparse samples\footnote{Agent has to keep receiving activations until they satisfy $s$-sparsity}.
\paragraph{Feedback Methods:} We evaluate four feedback mechanisms: (1) \tt{Eigendecomposition} uses \lemref{lem: basis} to construct feedback based on $\pphi$'s low rank structure, (2) \tt{Sparse Constructive} builds 2-sparse feedbacks using the basis in \eqnref{eq: sparsebasis}, (3) \tt{Random Sampling} generates feedbacks spanning $\mathcal{O}_{\pphi^*}$ from a Lebesgue distribution, and (4) \tt{Sparse Sampling} creates feedbacks using $s$-sparse samples drawn from a sparse distribution (see \defref{def: sparse}).\vspace{-4mm}
%\paragraph{Teaching Agent:} For numerical analysis, we design a teaching agent with access to the target feature matrix. Depending on the feedback method, we construct the basis of choice or underlying distribution (Lebesgue or Sparse). For smaller dimension, we use the python package \sf{cvxpy} to satisfy all the constraints of the form $\curlybracket{\alpha\alpha^\top - yy^\top}$. For larger dimensional features (e.g. $5000 \times 5000$), since the number of constraints scales in millions (note $p(p+1)/2 \approx 25M$,), we consider the standard gradient descent on batches of constraints for a regression task over matrices.
\paragraph{Teaching Agent:} We implement a teaching agent with access to the target feature matrix to enable numerical analysis. The agent constructs either specific basis vectors or receives activations from distributions (Lebesgue or Sparse) based on the chosen feedback method. For problems with small dimensions, we utilize the \sf{cvxpy} package to solve constraints of the form $\curlybracket{\alpha\alpha^\top - yy^\top}$. When handling larger dimensional features ($5000 \times 5000$), where constraints scale to millions ($p(p+1)/2 \approx 12.5M$), we employ batch-wise gradient descent for matrix regression.\vspace{-3mm}

\paragraph{Features via RFM:} RFM~\citep{rfm} considers a trainable shift-invariant kernel $K_{\pphi}: \cX\times \cX \to \reals$ corresponding to a symmetric, PSD matrix $\pphi$. At each iteration of the update, the matrix $\pphi_{(t)}$ is updated for the classifier ${f}_{\pphi}(z) = \sum_{y_i \in \mathcal{D}_{\text{train}}} a_i K_{\pphi}(y_i, z) $ as follows: $\pphi_{(t+1)} = \sum_{z \in \cD_{\sf{train}}} \big( \frac{\partial {f}_{\pphi_{(t)}}}{\partial z}\big) \big( \frac{\partial {f}_{\pphi_{(t)}}}{\partial z}\big)^\top$.
%\( \hat{f}_{\pphi}(z) = \sum_{y_i \in \mathcal{D}_{\text{train}}} a_i \cdot K_{\pphi}(y_i, z) \) using the Recursive Feature Machine (RFM)~\cite{rfm}, over 5 iterations with 4,000 training samples, resulting in the ground truth feature matrix \( \pphi^* \). Each RFM iteration updates \( \pphi^* \) through gradient computations \( \pphi = \sum_{z \in \cD_{\sf{train}}} \left( \frac{\partial \hat{f}_{\pphi}}{\partial z} \right) \left( \frac{\partial \hat{f}_{\pphi}}{\partial z} \right)^\top \), 
We train target functions corresponding to monomials over samples in $\reals^{10}$ using 4000 training samples. The feature matrix $\pphi_{(t)}$ obtained after $t$ iterations is used as ground truth against learning with feedbacks. Plots are shown in \figref{fig: monoconst} and \figref{fig: monosparse}.\vspace{-3mm}

\paragraph{SAE features of Large-Scale Models:} We analyze dictionaries from trained sparse autoencoders on Pythia-70M~\citep{pythia} and Board Game Models~\citep{karvonen2024measuring}, with dictionary dimensions of $32k \times 512$ and $4096 \times 512$ respectively. These large-scale dimensions result in millions of feedback parameters, necessitating computationally efficient implementations of the teaching agent and constraint optimization. Plots for Pythia-70M and detailed implementation specifications are provided in \appref{app: additional}.

Since there could be numerical issues in computation for these large dictionaries, we compute the Pearson Correlation Coefficient (PCC) of the trained feature matrix $\pphi'$ with the target feature matrix $\pphi^*$ to show their closenss. 
\begin{equation*}
    \rho(\pphi', \pphi^*) = \frac{\sum_{i,j} (\pphi'_{ij} - \bar{\pphi}')(\pphi^*_{ij} - \bar{\pphi}^*)}{\sqrt{\sum_{i,j} (\pphi'_{ij} - \bar{\pphi}')^2} \sqrt{\sum_{i,j} (\pphi^*_{ij} - \bar{\pphi}^*)^2}}
\end{equation*}
where $\bar{\pphi}'$ and $\bar{\pphi}^*$ represent the means of all elements in matrices $\pphi'$ and $\pphi^*$ respectively. Note the highest value of $\rho$ is 1.\vspace{-3mm}

\paragraph{Dictionary features of MLP layers of ChessGPT and OthelloGPT:} We use the SAEs trained for various MLP layers of Board Games models: ChessGPT and OthelloGPT considered in \cite{karvonen2024measuring}. We analyze the dictionaries from MLP layers with dimensions $4096 \times 512$. Note that $p(p+1)/2 \approx, 8.3M$. For the experiments, we use $3$-sparsity on uniform sparse distributions. We present the plots for ChessGPT in \figref{fig: chesssample} for different feedback methods. Additionally, we provide a table showing the Pearson Correlation Coefficient between the learned feature matrix and the target $\pphi^*$ in \tabref{tab: Chesspcc}.



\begin{table}[t]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        %\textbf{} & \textbf{Column 1} & \textbf{Column 2} & \textbf{Column 3} \\ \hline
        \textbf{Method} & \text{Eigendecom.} & \text{Sparse Cons.}  & \text{Sparse Samp.} & \text{Sparse Samp.} & \text{Sparse Samp.} & \text{Sparse Samp.} \\ \hline
        \textbf{PCC} & .9427 & .9773 &  .9741 & .9625 &  .8256 &  .7152\\ \hline
        \textbf{Feedbacks} & 134912  & 8390656 $\approx$ (8 mil) & 10 mil & 4 mil & 2 mil & 1 mil\\ \hline
    \end{tabular}
    \caption{Feedback sizes and Pearson Correlation Coefficients for the SAE dictionary for the ChessGPT under different feedback methods: (Dimension of the dictionary): $4096 \times 512$, rank of the feature matrix $\sf{D}\sf{D}^\top$ is $512$, $s$-sparsity: 3, sparse distribution: uniform.}
    \label{tab: Chesspcc}
\end{table}

%$\url{https://github.com/saprmarks/dictionary_learning.git}$ 

%We consider the sparse autoencoder \cite{pythia} p

%\iffalse

\begin{figure*}[h!]
    \centering
    % First Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{target_sae.png}
        \label{fig:sub1}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.9427,feedbacks-134912.png}
        \label{fig:sub2}
    \end{subfigure}
    \qquad
    % Second Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.9773,feedbacks-8390656.png}
        \label{fig:sub3}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.0861,feedbacks-10000.png}
        \label{fig: chessconst}
    \end{subfigure}
    \caption{\textbf{Visualization of 100 dimensions}: Feature learning on a dictionary retrieved for an MLP layer of ChessGPT of dimension $4096 \times 512$~\citep{karvonen2024measuring}. The plots show all the feedback methods studied in \secref{sec: construct} and \secref{sec: sample}: (Top leftmost): Ground Truth, (Top rightmost): Eigendecomposition, (Botton leftmost): Sparse Constructive, and (Bottom rightmost): Sparse sampling. Note that $p(p+1)/2$ is roughly 8M. Eigendecomposition and Sparse constructive achieve very high PCC with the feedback complexity stated in \thmref{thm: constructgeneral} and \thmref{thm: constructsparse}. With 3-sparse sampled activations, the Sparse sampling method performs poorly with just 10000 feedbacks which gradually improves with the feedback size as shown in \figref{fig: chesssample} (P.T.O).}
    \label{fig: chessconst}
    \newpage
\end{figure*}
\newpage
% Continuation on the Next Page
\begin{figure*}[!]\ContinuedFloat
    \centering

    % First Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.3225,feedbacks-200000.png}
        \label{fig:sub3}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC_0.6005,feedbacks_500000.png}
        \label{fig: chessconst}
    \end{subfigure}
    \par % Start a new row

    % Second Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC_0.7152,feedbacks_1000000.png}
        \label{fig:sub5}
    \end{subfigure}
\qquad    
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC_0.8256,feedbacks_2000000.png}
        \label{fig:sub6}
    \end{subfigure}
    \par % Start a new row

    % Third Row of Subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC_0.9625,feedbacks_4000000.png}
        \label{fig:sub7}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{learnt_PCC-0.9741,feedbacks-10000000.png}
        \label{fig: chessssample}
    \end{subfigure}

    \caption{\textbf{Sparse sampling}: Plots show the improvement in PCC with 3-sparse uniformly sampled activations with respect to the ground truth shown above.}
    \label{fig: chesssample}
\end{figure*}
\vspace*{10mm}
%\fi
%\paragraph{SAE Features:} We retrieve the dictionaries corresponding to trained sparse autoencoders for two models: Pythia-70M~\cite{pythia} and Board Game Models~\cite{karvonen2024measuring}. The dimensions of the model are much higher where on Pythia we consider dictionaries of size $\approx$ $32k \times 512$ and for Board Game Models $4096 \times 512$. We provide the plots in \appref{app: additional} with additional experimental details.


% Here's a concise rewrite of your experimental setup:

% \paragraph{Feedback Methods:} We evaluate four feedback mechanisms: (1) \tt{Eigendecomposition} uses \lemref{lem: basis} to construct feedback based on $\pphi$'s low rank structure, (2) \tt{Sparse Constructive} builds 2-sparse feedbacks using the basis in \eqnref{eq: sparsebasis}, (3) \tt{Random Sampling} generates feedbacks spanning $\mathcal{O}_{\pphi^*}$ from a Lebesgue distribution, and (4) \tt{Sparse Sampling} creates feedbacks using $s$-sparse samples drawn from a sparse distribution (see \defref{def: sparse}).

% \paragraph{Teaching Agent:} We implement a teaching agent with target feature matrix access. For small dimensions, we use \sf{cvxpy} to satisfy $\curlybracket{\alpha\alpha^\top - yy^\top}$ constraints. For larger features ($5000 \times 5000$), we employ batch-wise gradient descent due to the large number of constraints ($p(p+1)/2 \approx 25M$).

% \paragraph{Features via RFM:} Following \citet{rfm}, we train a shift-invariant kernel $K_{\pphi}$ with symmetric PSD matrix $\pphi$. The classifier $\hat{f}_{\pphi}(z) = \sum_{y_i \in \mathcal{D}_{\text{train}}} a_i K_{\pphi}(y_i, z)$ updates $\pphi_{(t)}$ as: $\pphi_{(t+1)} = \sum_{z \in \cD_{\sf{train}}} \left( \frac{\partial \hat{f}_{\pphi_{(t)}}}{\partial z} \right) \left( \frac{\partial \hat{f}_{\pphi_{(t)}}}{\partial z} \right)^\top$. We train on monomials over $\reals^{10}$ using 4000 samples, with results shown in \figref{fig: monoconst} and \figref{fig: monosparse}.

% \paragraph{SAE Features:} We analyze dictionaries from trained sparse autoencoders on Pythia-70M~\cite{pythia} ($32k \times 512$) and Board Game Models~\cite{karvonen2024measuring} ($4096 \times 512$). 

\newpage

% We retrieve the dictionaries corresponding to trained sparse autoencoders for two models: Pythia-70M~\cite{pythia} and Board Game Models~\cite{karvonen2024measuring}. The dimensions of the model are much higher where on Pythia we consider dictionaries of size $\approx$ $32k \times 512$ and for Board Game Models $4096 \times 512$. 
% Since the dimensions are large, the feedback sizes scale in millions which require efficient design (compute and memory) of a teaching agent and constraint satisfaction.
% Additional details are provided in \appref{app: additional}.




















%\akash{complete after experiments are done}

\iffalse
Ideas for Experimental Design:

Broad set of experiments to be done:

\begin{enumerate}
    \item For RFM-- constructive case and sampling case on polynomials and SVHN. Add an error-accuracy analysis.
    \item For a trained network, use the AGOP--constructive case and sampling case for celeb dataset
    \item Use the SAE methods to retrieve the dictionaries and repeat the constructive case and sampling cases
    \item Find a simple algorithm for modular arithmetic using ChatGPT
\end{enumerate}


\begin{itemize}
    \item Fix a dataset.
    \item Train RFM on the dataset and obtain the matrix $\widehat{M}$.
    \item Create an algorithm with a teacher with the target matrix $\widehat{M}$.
    \item Teacher provides triplets to teach $\widehat{M}$.
    \item Learner finds $\widehat{M}$ upto scaling $M'$.
    \item Compare the performance of $\widehat{M}$ with that of $M'$ on the test data set.
    \item Compare using a plot on the number of iterations and the triplet complexity.
\end{itemize}

\akash{Experiments}
\begin{enumerate}
    \item \textcolor{violet}{On RFM: both the constructive and sampling case}
    \item \textcolor{violet}{Features/Dictionary from a transformer: Both the constructive and sampling case; show the representations and features learned.} \url{https://arxiv.org/pdf/2309.08600}
    \item Features/Dictionary from a transformer: Both the constructive and sampling case; show a \textcolor{red}{simple learning algorithm trained with the taught dictionary which performs on par with the actual model.}
\end{enumerate}
\fi

% \section*{Impact Statement}

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''
