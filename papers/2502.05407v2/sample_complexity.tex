\subsection{Does metric learning provably help in learning a task?}

%Several works have stipulated sample complexity for generalization bounds and excess risk for metric and similarity learning for classification tasks~\cite{Guo2013GuaranteedCV,Bellet2012SimilarityLF}
Understanding a question of the form: "if metric learning provably helps in classification" would require analyzing whether metric learning helps achieve 
 better bounds asymptotically. Ideally, this would require either providing a sharper bound for classification tasks compared to the known algorithms w/ a priori learning a metric or showing that it can be a method to beat lower bounds for learning specific tasks. Note, metric learning generally comes up with a computational overhead, for it is designed as a convex/non-convex objective. Now, even if there is an oracle that solves that objective for `free', beating a lower bound could be tricky for the general technique for achieving the lower bounds is showing statistical bottlenecks in terms of VC-dim of the hypothesis class or information theoretic arguments. 

 \subsection{Sample complexity of learning a metric}

Sample complexity for generalization bound has been established for both general metric learning problem and metric learning for classification: $k$NN in the imbalanced data setting~\cite{viola20,Gautheron2020MetricLF}, and linear classification~\cite{Bellet2012SimilarityLF}; Mahalanobis distance metric~\cite{Verma2015SampleCO}, non-linear metric learning in sparse and bounded amplification regimes (embeddings corresponding to neural networks )~\cite{Kozdoba2021DimensionFG}.
More recently, learning a Mahalanobis metric has been further explored under various settings, e.g. low dimensional metric learning~\cite{Mason2017LearningLM}, multitask metric learning~\cite{Wang2019MultitaskML}, under noise labels~\cite{Alishahi2023LinearDM}.

\subsubsection{Learning a metric for perceptron in the noisy setting}

\begin{shaded}
\noindent Setting: Given a sample space $\cX \in \reals^d$, with a distribution $P_X$. Label set $\cY = \curly{-1,1}$ with noise $P_{Y|X}$. $S^1$ and $S^{-1}$ denotes examples with two labels.\\

\noindent\textit{at time} $t = 1,2,\ldots$
\begin{enumerate}
    \item environment provides a labeled input $(x_t,y_t) \sim P_{X,Y}$
    \item learner updates $S^1_t:= S^1_{t-1} \cup \curly{x_t}$ or $S^{-1}_t:= S^{-1}_{t-1}  \cup \{x_t\}$ 
    \item learner updates $M_t \leftarrow \textsf{Gradient-Update}(M_{t-1}, S^1_t,S^{-1}_t)$ 
    \item if $w_{t-1}$ misclassifies $M_t^{1/2}x_t$ then learner updates
    \begin{enumerate}
        \item $w_t \leftarrow w_{t-1} + y_t(M_t^{1/2}x_t)$
    \end{enumerate}
    %learner updates the teaching set $\mathcal{T}_t := \mathcal{T}_{t-1} \cup \{(x_i^t,x_j^t,x_k^t)\}$
    %\item  learner makes updates $M_t \leftarrow M_{t-1} + \eta\cdot \frac{1}{t} \sum_{i=1}^t (x_i - x_k)(x_i - x_k)^{\top} - (x_i - x_k)(x_i - x_j)^{\top}$
    %\item learner updates $\hat{M_t} \leftarrow \textsf{EigenDecompose}(M_t)$
    %\item teacher receives $\ell(M_t, \mathcal{T}_t)$
\end{enumerate}
\end{shaded}
Here, for a given labeled example $(x,y)$,
\begin{align*}
\textsf{Gradient-Update}(M_{t-1}, S^1_t,S^{-1}_t) = M_{t-1} + \eta y\cdot\paren{ \sum_{x' \in S^1_t} (x - x')(x-x')^{\top} - \sum_{x' \in S^{+1}_t} (x - x')(x-x')^{\top}} 
\end{align*}
One problem of interest is if this approach achieves a better bound on the excess risk for some loss function $\ell$:
\begin{align*}
    \expctover{(x,y)\sim P_{X,Y}}{\ell(w_t(x),y)} - \expctover{(x,y)\sim P_{X,Y}}{\ell(f^*(x),y)}
\end{align*}

