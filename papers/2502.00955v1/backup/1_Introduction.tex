\section{Introduction}


Large Language Models (LLMs) based agents have recently achieved remarkable success across a wide range of tasks~\cite{202412.2294, Wang_2024, xi2023risepotentiallargelanguage, zhang2024largelanguagemodelbrainedgui}. Leveraging the advanced natural language understanding and reasoning capabilities of LLMs~\cite{DBLP:journals/corr/abs-2303-08774, DBLP:conf/nips/Wei0SBIXCLZ22}, these agents are able to dynamically interact with complex tools and environments to accomplish various tasks~\cite{ DBLP:journals/corr/abs-2310-05915, DBLP:conf/iclr/YaoZYDSN023}. Nevertheless, individual agents often face significant limitations when confronted with complex tasks~\cite{DBLP:conf/emnlp/ShiYWWF24}. In such scenarios, the multi-agent system (MAS) (e.g., MetaGPT~\cite{DBLP:conf/iclr/HongZCZCWZWYLZR24}, AutoGen~\cite{DBLP:journals/corr/abs-2308-08155}, Camel~\cite{DBLP:conf/nips/LiHIKG23}) involving multiple specialized agents, with strategic task allocation and division of labor, becomes crucial for achieving optimal outcomes~\cite{DBLP:conf/ijcai/GuoCWCPCW024}.

However, optimizing the LLM-based MAS collective performance as a cohesive unit and obtaining reward signals for each agent in the MAS still remains a challenging problem~\cite{DBLP:journals/corr/abs-2410-08115}. A straightforward approach is imitation learning~\cite{DBLP:conf/icml/LeY0L17, DBLP:conf/acl/ZengLLWLD024}, which directly acquires collaborative strategies and task allocation policies from expert demonstrations. Nevertheless, acquiring such expert trajectory data is typically cost-prohibitive and resource-intensive~\cite{DBLP:journals/corr/abs-2409-00134}.


To address this challenge, researchers shift towards trial-and-error strategies to generate synthetic data for self-training~\cite{DBLP:conf/acl/SongYYHLL24}. 
%A representative approach is to generate multiple candidate trajectories and select the best of them to fine-tune the agents~\cite{DBLP:journals/corr/abs-2410-08115}. However, the simple trajectory reward fails to capture individual agents' contributions within the MAS. In contrast,  
% I prefer not include the above paragraph
Monte Carlo Tree Search (MCTS)~\cite{li2025enhancingreasoningprocesssupervision, guan2025rstarmathsmallllmsmaster} presents a general data synthesis framework, capable of estimating individual agent contributions through Q-value estimation~\cite{DBLP:journals/corr/abs-2410-08115}. They collect fine-grained preference pairs and update the agent through Direct Preference Optimization (DPO)~\cite{DBLP:conf/nips/RafailovSMMEF23} to encourage high-Q-value actions while suppressing low-Q-value actions. Despite promising, current data synthesis methodologies suffer from significant noise and suboptimal efficiency due to challenges in accurately estimating Q-values in large action spaces. Meanwhile, Q-value-based data selection does not always guarantee an enhancement in model performance.

\wt{[figure synthesis time scaling ]}


To solve these problems and enhance the collective performance of MAS, it is imperative to emphasize the significance of \textbf{Synthesis Time Scaling} to generate higher-quality data. In this work, as shown in Figure~\ref{}, we first empirically validate that scaling rollout times for MCTS can significantly improve the estimation accuracy of Q-values and data quality, thereby enhancing system performance. Moreover, we propose \textbf{Influ-MCTS}, a novel framework that first integrates the influence function to select agent-aware high-quality data, aiming for greater contributions to performance improvement. Additionally, we estimate the influence score by inferencing the validation set to avoid the computational overhead of calculating huge models' gradients. Building on these advancements, we can make the data synthesis process for MAS significantly more efficient and effective. Furthermore, we introduce an iterative data synthesis method to further enhance the quality of the generated data.


Our results and findings.


\textbf{We summarize the main contributions as follows:}

\begin{itemize}
    \item \textbf{Synthesis Time Scaling for Multi-Agent Optimization}: We are the first to propose that in the multi-agent task, synthesis time computation could be scaled up to generate higher-quality data and improve the system's performance ceiling.
    \item \textbf{Agent-Aware Data Selection Framework}: We introduce Agent-Aware Data Influence Score as an additional data selection criterion, ensuring that the training data contributes more to improving the current model's performance. 
    % This approach introduces a new dimension to synthesis time scaling, optimizing the data synthesis process for better model training outcomes.
    % \item \textbf{Data influence guide MCTS}: We introduce an additional data influence score as a signal to adjust the selection strategy in Monte Carlo Tree Search, enabling more efficient synthesis of high-quality data from the infinite action space.
    \item \textbf{Results}: We achieve state-of-the-art performance across multiple multi-agent tasks and demonstrate that the model's capability can be continuously improved through iterative rounds of data synthesis.
\end{itemize}