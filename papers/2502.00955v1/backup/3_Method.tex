% In this work, we model the topology structure for multi-agent collaboration as a directed acyclic graph (DAG). Concretely, we denote a feasible topology as $\mathcal{G}=(\mathcal{V}, \mathcal{E})$:
% \begin{equation}
% \mathcal{V}=\{v_i|i\in I\},\mathcal{E}=\{\left<v_i,v_j\right>|i,j\in I \wedge i \neq j \},
% \end{equation}
% where $\mathcal{V}$ represents the set of nodes indexed by $I$, and $\mathcal{E}$ represents the set of directed edges from one node to another. And there is no cycle in a DAG. 

% In the graph structure $\mathcal{G}$, we assign an agent to each node $v_i$, and we assign the information transmitted between agents $v_i$ and $v_j$ to each directed edge $\left<v_i,v_j\right>$. 

% To solve complex tasks, interactive reasoning among agents necessitates a strategic traversal to establish a structured sequence of interactions. We utilize the principle of topological ordering~\cite{Bondy1976} to traverse our agent network, which ensures each agent is visited only after all its dependencies have been processed~\cite{book:gross:2005, qian2024}. Formally, the topological order is a linear rearrangement of agents $a_i$ such that for every directed edge $\left< v_i,v_j\right>$, the ordering satisfies:
% \begin{equation}
% \forall \left<v_i,v_j\right> \in \mathcal{E}, \mathbb{I}(v_i) < \mathbb{I}(v_j),
% \end{equation}
% where $\mathbb{I}(v)$ denotes the index of agent $v$ in the topological sequence. We denote $v^k$ denote the agent at index $k$ in the topological sequence: 
% \begin{equation}
%  \mathbb{I}(v^k) = k, v^k\in \mathcal{V}.
% \end{equation}

% \subsection{MCTS-DPO (In Progress)}
% \label{sec:mcts}

% To generate reward signals for each agent in the multi-agent network, we sort the agents according to their topological order. Utilizing the agents' current policy $\pi(\theta, v^k)$, we sample candidate solutions from its probability distribution $\pi_{\theta}(Out\mid P, In)$ with $P$ being the agent's prompt, $In$ and $Out$ being the agent's input and output. MCTS serves as an approximate policy improvement operator by leveraging its look-ahead capability to predict the expected future reward. This prediction is refined through stepwise self-evaluation, enhancing process consistency and decision accuracy. The tree-structured search supports a balance between exploring diverse possibilities and exploiting promising paths, essential for navigating the vast search space in LLM reasoning.

% The MCTS process begins from a root node, $s_0$, as the sentence start or incomplete response, and unfolds in three iterative stages: selection, expansion, and backup, which we detail further.

% \paragraph{Select.}
% The objective of this phase is to identify nodes that balance search quality and computational efficiency. The selection is guided by two key variables: $Q(s_t, a)$, the value of taking action $a$ in state $s_t$, and $N(s_t)$, the visitation frequency of state $s_t$. These variables are crucial for updating the search strategy, as explained in the backup section. To navigate the trade-off between exploring new nodes and exploiting visited ones, we employ the Predictor + Upper Confidence bounds applied to Trees (PUCT)~\citep{rosin2011multi}. At node $s_t$, the choice of the subsequent node follows the formula:

% \begin{equation}
%     \label{eq:puct}
%     \begin{aligned}
%         {s_{t+1}}^* &= \arg\max_{s_t}\Bigl[Q(s_t, a) + c_{\mathrm{puct}}\cdot p(a\mid s_t)\frac{\sqrt{N(s_t)}}{1 + N(s_{t+1})}\Bigl]
%     \end{aligned}
% \end{equation}

% where $p(a\mid s_t) = \pi_{\theta}(a\mid x, s_t)/|a|^{\lambda}$ denotes the policy $\pi_{\theta}$'s probability distribution for generating a step $a$, adjusted by a $\lambda$-weighted length penalty to prevent overly long reasoning chains.

% \paragraph{Expand.}
% Expansion occurs at a leaf node during the selection process to integrate new nodes and assess rewards. The reward $r(s_t, a)$ for executing step $a$ in state $s_t$ is quantified by the reward difference between states $R(s_t)$ and $R(s_{t+1})$, highlighting the advantage of action $a$ at $s_t$. As defined in Eq.~(\ref{eq:reward}), reward computation merges outcome correctness $\mathcal{O}$ with self-evaluation $\mathcal{C}$. We assign the outcome correctness to be $1$, $-1$, and $0$ for correct terminal, incorrect terminal, and unfinished intermediate states, respectively. Following \citet{DBLP:journals/corr/abs-2305-00633}, we define self-evaluation as Eq.~(\ref{eq:selfeval}), where $\mathrm{A}$ denotes the confidence score in token-level probability for the option indicating correctness\footnote{We show an example of evaluation prompt in Table~\ref{tab:eval-prompt}.}. Future rewards are anticipated by simulating upcoming scenarios through roll-outs, following the selection and expansion process until reaching a terminal state\footnote{The terminal state is reached when the whole response is complete or exceeds the maximum length.}. 
% \begin{equation}
%     \label{eq:reward}
%     R(s_t) = \mathcal{O}(s_t) + \mathcal{C}(s_t)
% \end{equation}
% \begin{equation}
%     \label{eq:selfeval}
%     \mathcal{C}(s_t) = \pi_{\theta}(\mathrm{A}\mid {\mathrm{prompt}}_{\mathrm{eval}}, x, s_t)
% \end{equation}

% \paragraph{Backup.}
% Once a terminal state is reached, we carry out a bottom-up update from the terminal node back to the root. We update the visit count $N$, the state value $V$, and the transition value $Q$: 
% \begin{equation}
%     \label{eq:Q}
%     Q(s_t, a) \leftarrow r(s_t, a) + \gamma V(s_{t+1})
% \end{equation}
% \begin{equation}
%     \label{eq:V}
%     V(s_t) \leftarrow \sum_{a}N(s_{t+1})Q(s_t, a) / \sum_{a}N(s_{t+1})
% \end{equation}
% \begin{equation}
%     \label{eq:N}
%     N(s_t) \leftarrow N(s_t) + 1
% \end{equation}
% where $\gamma$ is the discount for future state values.

% For each step in the response generation, we conduct $K$ iterations of MCTS to construct the search tree while updating $Q$ values and visit counts $N$. To balance the diversity, quality, and efficiency of the tree construction, we initialize the search breadth as $b_1$ and anneal it to be a smaller $b_2 < b_1$ for the subsequent steps. We use the result $Q$ value corresponding to each candidate step to label its preference, where higher $Q$ values indicate preferred next steps. For a result search tree of depth $T$, we obtain $T$ pairs of step-level preference data. Specifically, we select the candidate steps of highest and lowest $Q$ values as positive and negative samples at each tree depth, respectively. The parent node selected at each tree depth has the highest value calculated by multiplying its visit count and the range of its children nodes' visit counts, indicating both the quality and diversity of the generations. 