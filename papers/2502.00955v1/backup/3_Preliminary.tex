\section{Preliminary}
\label{section:preliminary}
In this section, we briefly introduce the MCTS algorithm, the influence function, and the DPO loss function. 
% \cx{maybe DPO first gives a better flow? 1) how to train, (and we should focus on the MAS setting) 2) search in the space for data 3) influence function on how to measure data importance.}

\subsection{MCTS Algorithm}

\cx{we probably need to provide a more technical description of MCTS? and then how it is search is done in SOTA is most important information?}
% MCTS
MCTS is an advanced search algorithm, that can be served as a general data synthesis framework that balances exploration and exploitation. The data synthesis process using MCTS consists of four steps, starting from an initial problem instruction $p$:
\begin{itemize}[leftmargin=*]
    \item \textbf{Selection}: At each step, the most promising node from the tree is selected by balancing exploitation of high-reward nodes and exploration of less-visited ones. 
    \item \textbf{Expansion}: For the selected node, generate $d$ potential actions and choose an action for further exploration.
    \item \textbf{Simulation}: The chosen action is followed by 
     $m$ simulated rollouts until obtaining the final result.
    \item \textbf{Backpropagation}: Once the termination is reached, the corresponding reward is obtained and used to update the Q-value of each node along the entire trajectory.
\end{itemize}
Through repeated application of its four phases, MCTS efficiently synthesizes solution trajectories for problems.

\subsection{Influence function}

The influence function $\mathcal{I}$ was developed to measure the difference in loss when a data point is assigned a higher weight in the training dataset.  Suppose the agent $A$ is parameterized by $\theta$. We denote the optimal parameters learned by minimizing the training loss $L_{\text{tr}}$ on the dataset $\mathcal{D}_{\text{tr}}$, with a data point $z_i$ assigned an additional weight of $\epsilon$, as:
\begin{equation}
    \theta_{\epsilon, z_i}^* = \underset{\theta}{\arg\min} \sum_{z_j\in \mathcal{D}_{\text{tr}}} \frac{1}{|\mathcal{D}_{\text{tr}}|} L_{\text{tr}}(z_j, \theta) + \epsilon L_{\text{tr}}(z_i, \theta).
\end{equation}
Under standard assumptions, such as the twice-differentiability and strong convexity of the loss function $L_{\text{tr}}$, the influence function can be derived via the chain rule of the derivatives~\cite{DBLP:conf/icml/KohL17}:
\begin{equation}
\begin{split}
    \mathcal{I}_{L_{\text{tr}}}(z_i, \mathcal{D}_{\text{tr}}) &\stackrel{\text{def}}{=} \frac{d L_{\text{tr}}(z_i, \theta_{\epsilon, z_i}^*)}{d\epsilon} \bigg|_{\epsilon=0} \\
    &\approx -\nabla_\theta L_{\text{tr}} \big|_{\theta=\theta^*}^T H(\mathcal{D}_{\text{tr}}; \theta_{\epsilon, z_i}^*)^{-1} \nabla_\theta L_{\text{tr}} \big|_{\theta=\theta^*},
    \label{equation:influence score}
\end{split}
\end{equation}

% \begin{equation}
% \mathcal{I}_{L_{\text{tr}}}(z_i, \mathcal{D}_{\text{tr}}) \approx - L_{tr}(z_i, \theta^*) + L_{tr}(z_i, \theta'), where \theta' = \theta -\eta \nabla L_{tr}(z_i)
% \end{equation}
where $H(\mathcal{D}; \theta) := \nabla_\theta^2\left(\frac{1}{|\mathcal{D}|} \sum_{z\in \mathcal{D}}L_{\text{tr}}(z; \theta)\right)$ and $\nabla_\theta L_{\text{tr}} = \nabla_\theta L_{\text{tr}}(z;\theta)$.

\subsection{DPO Loss}
The Direct Preference Optimization (DPO) loss directly utilize ranked pairs of data $z=(x,y^w,y^l)$ to encourage the model to prioritize responses that align with preferences $y^w$ over less preferred ones $y^l$.
\begin{equation}
\small
    L_{DPO} = \mathbb{E}_{z} \bigg[ 
    -\log \sigma \bigg( \beta \bigg[
        \log \frac{\pi_\theta(y^w \mid \mathbf{x})}{\pi_{\text{ref}}(y^w \mid \mathbf{x})} 
        - \log \frac{\pi_\theta(y^l \mid \mathbf{x})}{\pi_{\text{ref}}(y^l \mid \mathbf{x})}
    \bigg] \bigg) \bigg],
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function, and $\pi_{\text{ref}}$ represents the reference model, typically the SFT model.