\section{Method}
\label{section:method}
In this section, we first formalize the multi-agent task and data synthesis (ยง~\ref{subsection:multi-agent-data-synthesis}), then introduce the agent-aware \cx{not sure whether we should introduce agent-aware here to name our method, we should focus on influence-driven MCTS?} hybrid \cx{also don't think we should say hybrid, focus on our novelty, then it is naturally embeded in existing MCTS methods} data selection using influence scores (ยง~\ref{subsection:hybrid data selection}), and finally present the iterative data synthesis process (ยง~\ref{subsection:iterative data synthesis}).

\begin{figure*}
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=0.9\linewidth]{figure/Main_structure.pdf}
    \caption{Overview of our method.\cx{figure needs redraw. follow recent MCTS papers' style.} \cx{your lines are too thick, notations are not self explainable, arrows are not level}} \vspace{-0.3cm}
    \label{fig:framework}
\end{figure*}

\subsection{Multi-Agent Data Synthesis} \label{subsection:multi-agent-data-synthesis}
% How to generalize to multi agent network
In this work, we model the topology structure for multi-agent collaboration as a directed graph. Concretely, we denote a feasible topology as $\mathcal{G}=(\mathcal{V}, \mathcal{E})$. \cx{should we draw this graph in the figure?} We allow the presence of cycles in the graph, indicating that multiple rounds of information exchange are permitted among agents. We assume that our agent network can be linearly traversed in topological order $A_1\oplus A_2 \oplus \cdots \oplus A_T$~\cite{Bondy1976, book:gross:2005, qian2024}, where $A_i \in \mathcal{V}$. Different $A_i$ may represent the same agent being visited at different time steps. For clarity and convenience, we use different symbols to distinguish them.

% Why chose MCTS
\cx{we should have a more mathmatical and structured description of our system. Use more equations to present the process.}
In this way, we could utilize MCTS to synthesize training data for MAS. As shown in Figure~\ref{fig:framework} (a), the synthesis tree begins with a specific task instruction  $p$, where all nodes at the $i$-th layer correspond to the $i$-th agent $A_i$. Distinct upstream trajectories $s$ represent varying inputs to agent $A_i$ under different conditions. The divergent branches of a node illustrate the potential actions an agent can take given the particular input. The Q-value for a specific action $Q_i(s,a)$ by agent $A_i$ is estimated through multiple rollouts $\{o_1, \cdots, o_m \}$, which randomly simulate transitions to a terminal state based on the current agents' policy. 
\begin{equation}
    Q_i(s,a) = \frac{1}{m} \sum_j R(s,a,o_j),
\end{equation}
where $R(s,a,o_j)$ denotes the trajectory-level reward with outputs $o_j$. The Q-value serves as a measure of each agent's contribution to the final answer in the MAS to some extent. \cx{no experimental detail in methodology section} In this work, we follow the MCTS configuration in Optima~\cite{DBLP:journals/corr/abs-2410-08115}, with detail available in Appendix~\ref{}. When the tree search is completed, we can construct paired action preferences for each agent $A_i$ at state $s$ by selecting the action $a_i^{h}$ with the highest Q-value and the action $a_i^{l}$ with the lowest Q-value to form the preference data.
\begin{equation}
    z = (x,y^w,y^l) = \left(s, a_i^{h}, a_i^{l}\right),
\end{equation}
Since the feasible action space is infinite, MCTS may fail to generate the optimal action within a limited number of $d$ expansions. Additionally, due to the complex interactions among multiple agents, the Q-value estimates obtained from $m$ rollouts may be inaccurate. These factors can result in the collected data containing low-quality samples and significant noise.

\cx{up to here I am thinking that we may not need Sec 3, we can describe MCTS and DPO as part of our method, then IF as the selection part. This makes the method presentation more streamlined}
\cx{e.g., we can describe the MCTS process formally, its goal, the roll out, the search space, the priority used in the search, and the DPO upon it. Then how we use Influ to define the search part. (as selection)}
\cx{after that we can have a 4.3 to recap the full process, including iterative synthetis, training process, and inference}

As shown in Figure~\ref{fig:scaling}, in this work we first empirically validate that increasing the number of inference tokens during data synthesis for MAS to expand more candidate actions and rollouts more times can improve the quality of the generated data and subsequently enhance the system's performance.\cx{this reads like syntehtic time scaling is property of baseline MCTS, and readers may think it is obvious that more syntheized data is going to lead to better performance. Put synthetize time scaling as an observation, influe-MCTS as the main contribution which leads to the scaling (or at least better scaling) is the right choice.}

\subsection{Agent-Aware Hybrid Data Selection}
\label{subsection:hybrid data selection}

% The synthesis time scaling approach mentioned in section~\ref{section:multi-agent-data-synthesis} is straightforward but not optimal. This is due to the vast action space, which results in low scaling efficiency. Moreover, selecting the action with the highest Q-value may not yield much improvement for the current model. For example, \cite{} suggests that actions with higher advantages are more valuable for training the current model compared to actions with merely high Q-values. Hence, in this section, we propose a novel scaling dimension for data synthesis in MAS, aimed at finding the most valuable data for model training.

While improving the accuracy of Q-value estimation can enhance data quality to some extent, it is both highly inefficient and suboptimal. During the training phase, the primary goal of synthetic data is to maximize its contribution to model performance improvement, rather than ensuring the data is correct. As shown in Figure~\ref{fig:framework} (b), although the data pair $z_1$ has a higher $Q(s,a_i^h)$, the data pair $z_2$ contributes more significantly to system performance improvement, as reflected by its higher influence score. This discrepancy may arise because the action in $z_2$ provides a greater advantage compared to the less preferred action.

% Our approach is inspired by data influence functions~\cite{}. The function $\mathcal{I}$ was developed to measure the difference in reference loss when a data point is assigned a higher weight in the training dataset.
% % \begin{equation}
% %     \mathcal{M}^* = \arg\min \sum_{i=0}^{n} \frac{1}{n} L(x_i, \mathcal{M})
% % \end{equation}
% \begin{equation}
%     \mathcal{M}_{\epsilon, z_i}^* = \arg\min \sum_{i=0}^{n} \frac{1}{n} L(z_i, \mathcal{M}) + \epsilon L(z_i, \mathcal{M}),
% \end{equation}
% \begin{equation}
%     \mathcal{I}_{\mathcal{M}^*}(z_i, \mathcal{D}_r) \stackrel{\text{def}}{=} \mathcal{F}(\mathcal{D}_r | \mathcal{M}_{\epsilon, z_i}^* ) -\mathcal{F}(\mathcal{D}_r |  \mathcal{M}^* ),
%     \label{equation:definition_data_influence}
% \end{equation}
% where $\mathcal{M}$ represents the agent parameters, $L$ represents the training loss, $\mathcal{F}(\mathcal{D}_r|\mathcal{M})$ indicates the test metric $\mathcal{F}$ achieved by agent parameter $\mathcal{M}$ on the reference dataset $\mathcal{D}_r$.

Hence, in this paper, we introduce the influence score to quantify the impact of data on the current agent's performance, enabling an agent-aware data selection process. However, the DPO loss does not effectively align with downstream task performance. Our experiments reveal a weak correlation (less than 0.2) between the DPO loss and performance metrics $\mathcal{F}$ such as F1-score or Accuracy on the validation set. This observation is consistent with findings reported in \citet{DBLP:journals/corr/abs-2406-02900, DBLP:journals/corr/abs-2410-11677}. This indicates that we must redefine the influence score using the changes of non-differentiable performance metrics on the validation set.
\begin{equation}
\mathcal{I}_{\mathcal{F}_{\text{val}}}(z_i, \mathcal{D}_{\text{val}}) := \frac{\mathcal{F}_{\text{val}}(z_i, \theta_{\epsilon,z_i}^*) - \mathcal{F}_{\text{val}}(z_i, \theta^*)}{\epsilon},
\end{equation}

Due to non-differentiable metric $\mathcal{F}$, the influence function cannot be directly derived using gradients or the chain rule. Instead, we use a finite difference method combined with parameter perturbation to approximate the rate of change. The perturbed optimal parameter $\theta_{\epsilon,z_i}^*$ can be rewritten as:
\begin{equation}
    \theta_{\epsilon,z_i}^* = \theta^* + \epsilon\Delta \theta + o(\epsilon),
\end{equation}
where $\Delta\theta$ represents the direction of parameter change. Following~\citet{yu2024mates}, the direct is typically driven by the gradient of the training loss.
\begin{equation}
    \Delta \theta \propto -\nabla_\theta L_{\text{tr}}(z_i, \theta^*).
\end{equation}
Since the parameter update is dominated by the training loss gradient, we adopt a one-step gradient descent update:
\begin{equation}
    \theta_{\epsilon,z_i}^* \approx \theta^* - \eta\epsilon\nabla_\theta L_{\text{tr}}(z_i,\theta^*),
\end{equation}
where $\eta$ is the learning rate, and $\epsilon$ is a very small perturbation strength. Combining the finite difference and parameter update, the influence function is approximated as:
\begin{equation}
\begin{split}
    &\mathcal{I}_{\mathcal{F}_{\text{val}}}(z_i, \mathcal{D}_{\text{val}}, \theta^*) \approx \\ &\frac{1}{\epsilon} \left[ \mathcal{F}_{\text{val}}(z_i,  \theta^* - \eta\epsilon\nabla_\theta L_{\text{tr}}(z_i,\theta^*)) \right. 
     \left. - \mathcal{F}_{\text{val}}(z_i,\theta^*) \right].
\end{split}
\end{equation}

Compared to performing simulations to estimate Q-value more accurately, conducting inference on a validation dataset to estimate data influence can better guide the selection of higher-quality data points.

Specifically, Our selection strategy combines Q-values and influence scores to effectively identify the highest-quality pair data: 
\begin{equation}
\begin{split}
    H(z_i) = \mathcal{I}_{\mathcal{F}_{\text{val}}}(z_i, \mathcal{D}_{\text{val}}, \theta) + \gamma \cdot Q_i(s, a_i^h) , 
    \label{equation:hybrid score}
\end{split}
\end{equation}
where $\theta$ denotes the current parameters of agent $A_i$. Finally, after filtering out low-quality data as described in~\citet{DBLP:journals/corr/abs-2410-08115}, synthesized data are ranked based on their hybrid scores, and the Top $\alpha$\% are selected to construct the training dataset $\mathcal{D}_{\text{tr}}$.

{
\setlength{\textfloatsep}{0em}
\begin{algorithm}[t]
\caption{Influ-MCTS-iSFT-DPO}
\label{alg:influ-mcts-isft-dpo}
\begin{algorithmic}[1]
\REQUIRE Initial model $\theta_\text{init}$, problem Set $\mathcal{D}$, validation Set $\mathcal{D}_{\text{val}}$, and max iterations $T$
\ENSURE parameter $\theta_T$
\STATE $\theta_0 \gets \theta_\text{init}$
\FOR{$t = 1$ to $T$}
    \STATE $D_t^{SFT}$ $\gets$ SFTDataCollect($\theta_{t-1}$) \COMMENT{\small{Following~\citet{DBLP:journals/corr/abs-2410-08115}}}
    \STATE $\theta_t$ $\gets$ SFT($D_t^{SFT}$, $\theta_\text{init}$) \COMMENT{\small{Following~\citet{DBLP:journals/corr/abs-2410-08115}}}
    \STATE $\mathcal{D}_t^\text{DPO} \gets \emptyset$
    \FORALL{$p_i \in \mathcal{D}$}
        \STATE $\mathcal{D}_i^\text{DPO} \gets \text{MCTS}(\theta_t, p_i)$ \COMMENT{See Algorithm \ref{alg:mcts-data-generation}}
        \STATE $\mathcal{I}_{\mathcal{F}_{\text{val}}}\gets \text{DataInfluenceCollect}(\mathcal{D}_{\text{val}})$ 
        \STATE $\mathcal{D}_t^\text{DPO} \gets \mathcal{D}_t^\text{DPO} \cup \mathcal{D}_i^\text{DPO}$
    \ENDFOR
    \STATE $\mathcal{D}_t^{DPO} \gets \text{HybridSelection}(\mathcal{D}_t^{DPO}, \mathcal{I}_{\mathcal{F}_{\text{val}}}$)
    \STATE $\theta_{t}$ $\gets$ DPO($\mathcal{D}_t^{DPO}, \theta_t)$
\ENDFOR
\OUTPUT $\theta_T$
\end{algorithmic}
\end{algorithm}
}

\subsection{Iterative Data Synthesis}
\label{subsection:iterative data synthesis}
In addition to utilizing the current model for data synthesis, we propose an iterative refinement approach to generate higher-quality data. By continuously training and enhancing the model, its capabilities improve, enabling the generation of more valuable synthesized data in subsequent iterations. At iteration $t$, we generate the training dataset $\mathcal{D}{\text{tr}}^t$ based on the parameters $\theta{t-1}$ and train a new model from the initial model using $\mathcal{D}_{\text{tr}}^t$. The corresponding pseudocode can be found in Algorithm~\ref{alg:influ-mcts-isft-dpo}.
