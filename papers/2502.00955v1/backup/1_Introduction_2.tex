
Large Language Models (LLMs) based agents have recently achieved remarkable success across a wide range of tasks~\cite{202412.2294, Wang_2024, xi2023risepotentiallargelanguage, zhang2024largelanguagemodelbrainedgui}. Leveraging the advanced natural language understanding and reasoning capabilities of LLMs~\cite{DBLP:journals/corr/abs-2303-08774, DBLP:conf/nips/Wei0SBIXCLZ22}, these agents are able to dynamically interact with complex tools and environments to accomplish various tasks~\cite{ DBLP:journals/corr/abs-2310-05915, DBLP:conf/iclr/YaoZYDSN023}. Nevertheless, individual agents often face significant limitations when confronted with complex tasks~\cite{DBLP:conf/emnlp/ShiYWWF24}. In such scenarios, the multi-agent system (MAS) (e.g., MetaGPT~\cite{DBLP:conf/iclr/HongZCZCWZWYLZR24}, AutoGen~\cite{DBLP:journals/corr/abs-2308-08155}, Camel~\cite{DBLP:conf/nips/LiHIKG23}) involving multiple specialized agents, with strategic task allocation and division of labor, becomes crucial for achieving optimal outcomes~\cite{DBLP:conf/ijcai/GuoCWCPCW024}.

\begin{figure}[t]
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=1.0\linewidth]{figure/scaling.pdf}
    \caption{\textbf{Synthesis time scaling}: Performance trends of different methods under increasing data synthesis budgets (Tokens). \wt{The equal portions means that the process is still ongoing.}\cx{why call it optima but not our method?}\cx{can we add baseline here that with MCTS it does not scale well?}} \vspace{-0.3cm}
    \label{fig:scaling}
\end{figure}

\cx{We spend too much space to get into our method. Can we merge paragraph 2 and 3 and direct go into the challenges of MCTS in training, showing that it is expensive and difficulty to explore the space to acquire training signals}
However, optimizing the LLM-based MAS collective performance as a cohesive unit and obtaining reward signals for each agent in the MAS still remains a challenging problem~\cite{DBLP:journals/corr/abs-2410-08115}. A straightforward approach is imitation learning~\cite{DBLP:conf/icml/LeY0L17, DBLP:conf/acl/ZengLLWLD024}, which directly acquires collaborative strategies and task allocation policies from expert demonstrations. Nevertheless, acquiring such expert trajectory data is typically cost-prohibitive and resource-intensive~\cite{DBLP:journals/corr/abs-2409-00134}.


To address this challenge, researchers shift towards trial-and-error strategies to generate synthetic data for self-training~\cite{DBLP:conf/acl/SongYYHLL24}. 
%A representative approach is to generate multiple candidate trajectories and select the best of them to fine-tune the agents~\cite{DBLP:journals/corr/abs-2410-08115}. However, the simple trajectory reward fails to capture individual agents' contributions within the MAS. In contrast,  
% I prefer not include the above paragraph
\cx{the discussion here is too detailed, they should be in related work.}
Monte Carlo Tree Search (MCTS)~\cite{guan2025rstarmathsmallllmsmaster, li2025enhancingreasoningprocesssupervision} presents a general data synthesis framework, capable of estimating individual agent contributions through Q-value estimation~\cite{DBLP:journals/corr/abs-2410-08115}. They collect fine-grained preference pairs and update the agent through Direct Preference Optimization (DPO)~\cite{DBLP:conf/nips/RafailovSMMEF23} to encourage high-Q-value actions while suppressing low-Q-value actions. Despite its potential, current data synthesis strategy faces two key challenges: (1) the difficulty of accurately estimating Q-values, leading to suboptimal data quality, and (2) an over-reliance on Q-values for data selection. Unlike the inference phase, where MCTS is used to efficiently obtain accurate results, the primary goal during the data synthesis phase is to generate data that better facilitates model training, rather than prioritizing data correctness. Consequently, Q-value-based data selection does not always ensure improved model performance.



\cx{synthetis time scaling is not directly targeting the challenge of MCTS. The transition feels very unnatural. Maybe a nature way would be that: 1) challenge MCTS or tree-search is to fine target solution but lots of search traces are not informative for training, there is a gap between the traces that are needed in training and in task completion. 2) we propose Influ-MCTS that address this gap. 3) we should that with Influe-MCTS there exists a promising synthetis time scaling. We can spend one paragraph on each of the above. maybe 2 paragraphs on our Influe-MCTS framework, one generally describe it, the 2nd provide more in-depth technical discussions. The 3) perhaps is an empirical observation after presenting main empirical results?}
To solve these problems and improve the collective performance of MAS, it is we emphasize the importance of \textbf{Synthesis Time Scaling} to generate higher-quality data. In this work, as shown in Figure~\ref{fig:scaling}, we first empirically demonstrate that scaling rollout times in MCTS enhances Q-value estimation accuracy, leading to better system performance. However, this approach is both inefficient and insufficient for ensuring the selection of high-quality training data. Hence, we take a step further and propose \textbf{Influ-MCTS}, a novel framework that first integrates the influence function to select agent-aware high-quality data, with the goal of maximizing contributions to performance improvement. To estimate the influence score efficiently, we leverage validation set inference, avoiding the computational cost of calculating gradients for large models. Building on these advancements, we make the data synthesis process for MAS significantly more efficient and effective. Additionally, we introduce an iterative data synthesis method to further improve the quality of the generated data.

We validated our approach on eight datasets across two multi-agent tasks: Information Exchange and Debate. We observe that many high Q-value data contribute little to improving model performance and can even have a negative impact, such as reducing the diversity of the model's responses. Incorporating data influence is crucial for data synthesis and selection. Our method outperformed state-of-the-art multi-agent optimization techniques, achieving an average improvement of 2.1\% in single-round iterations and a 2.5\% enhancement in multi-round iterations for the Information Exchange task. \cx{we can add one paragraph sharing the most interesting insights in our analysis, maybe about how we achieve synthetis time scaling}

We summarize the main contributions as follows: \cx{reorganize the three according to new introduction. 1st one is the framework that solve the mismatch between synthetic training's tree search and testing's tree search. 2nd is overal impirical, 3rd is the observation of synthetis time scaling?}
\begin{itemize}
    \item \textbf{Synthesis Time Scaling for Multi-Agent Optimization}: We first empirically demonstrate in the multi-agent task, synthesis time computation could be scaled up to generate higher-quality data and improve the system's performance ceiling.
    \item \textbf{Agent-Aware Data Selection Framework}: We propose the Influ-MCTS framework, which first introduces agent-aware influence score as an additional data selection criterion, selecting preference pair data contributes more to performance improvement. 
    \item \textbf{Results}: We achieve state-of-the-art performance across multiple multi-agent tasks and demonstrate that the framework's capability can be continuously improved through iterative rounds of data synthesis.
\end{itemize}