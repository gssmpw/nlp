\section{Related Work}
\label{section:relatedwork}



\subsection{LLM based MAS}
\label{subsection:LLM based MAS}
LLM-based MAS have demonstrated remarkable capabilities in addressing complex problems in various tasks~\cite{DBLP:conf/iclr/HongZCZCWZWYLZR24, DBLP:conf/acl/IslamAP24, tran2025multiagentcollaborationmechanismssurvey}. These systems employ various collaborative strategies, including multi-agent debate~\cite{DBLP:conf/icml/Du00TM24, DBLP:conf/emnlp/Liang0JW00Y0T24} and role-based division of labor~\cite{DBLP:journals/corr/abs-2405-04219, DBLP:conf/naacl/WangMW0WJ24}. 
Researchers have explored several key approaches to improve the performance of multi-agent systems. One strategy focuses on expanding the diversity and scale of agents~\cite{DBLP:journals/corr/abs-2402-05120, DBLP:conf/acl/QianLLCDL0CSCXL24, DBLP:journals/corr/abs-2406-04692}, optimizing performance from a network architecture perspective. Another approach emphasizes enhancing prompt quality, such as refining system memory in frameworks like AutoGen~\cite{DBLP:journals/corr/abs-2308-08155} and BiLLP~\cite{DBLP:conf/sigir/Shi0ZGLZWF24} or improving instruction design and few-shot examples in Dspy~\cite{DBLP:journals/corr/abs-2310-03714, DBLP:conf/emnlp/Opsahl-OngRPBPZ24}. A third approach involves fine-tuning the parameters of the large models within the agents, which is the most effective yet challenging method. Optima~\cite{DBLP:journals/corr/abs-2410-08115} and MALT~\cite{DBLP:journals/corr/abs-2412-01928} have taken the first step in this direction by constructing preference training data pairs through estimating Q-values.
% Current approaches primarily focus on enhancing performance by increasing the diversity and number of agents~\cite{DBLP:journals/corr/abs-2402-05120, DBLP:conf/acl/QianLLCDL0CSCXL24, DBLP:journals/corr/abs-2406-04692}, as well as updating agent memory~\cite{gao2024360, DBLP:conf/sigir/Shi0ZGLZWF24}.
%\cx{do not attack related work, just state where they are and optima is one of the early attempts to  address this challenge. We should als ocover DSPy and AutoGen etcs, on what they are optimizing in the MAS. spend more space on these MAS training methods than MAS general ones.}
% However, optimizing the model parameters within multi-agent systems to enable better overall coordination remains a challenging task. Optima~\cite{DBLP:journals/corr/abs-2410-08115} and MALT~\cite{DBLP:journals/corr/abs-2412-01928} have taken the first step in this direction by constructing preference training data pairs through estimating Q-values. Nevertheless, relying solely on Q-values introduces significant noise and is suboptimal for model training. Our approach addresses this issue by incorporating influence functions to guide MCTS and data selection. 

\subsection{Monte Carlo Tree Search}
\label{subsection:MCTS}
MCTS is an advanced search algorithm capable of effectively balancing exploration and exploitation in decision-making processes. It gained significant attention following its success in AlphaGo~\cite{DBLP:journals/nature/SilverHMGSDSAPL16}. Subsequently, researchers have introduced MCTS into LLM reasoning tasks~\cite{DBLP:conf/emnlp/HaoGMHWWH23}, giving rise to two primary methodologies. The first approaches employ MCTS during the inference phase, prioritizing actions with the highest potential to yield correct outcomes~\cite{DBLP:journals/corr/abs-2408-03314, wu2024inferencescalinglawsempirical}. The second approaches leverage MCTS during the training phase to synthesize high-quality training data, with the goal of identifying data that maximizes the improvement in model performance~\cite{DBLP:journals/corr/abs-2408-06195, DBLP:journals/corr/abs-2405-00451, DBLP:journals/corr/abs-2406-07394, DBLP:journals/corr/abs-2406-03816}. 
These approaches mainly rely on estimated Q-values to guide the exploration of the synthesis data space. 
% RewardProgres adopts Advantage-based metrics to guide exploration, focusing on the relative improvement of actions rather than their absolute Q-values.
% These methods rely on the estimated Q-value to select actions that are worth further expansion, thereby improving the efficiency of data synthesis within a vast action space. However, generating data that significantly enhances model training is more critical than merely producing correct data in the data synthesis phase. 
% Although AlphaLLM-CPL~\cite{DBLP:journals/corr/abs-2410-06508} leverages perplexity difference to guide the training order, it still does not fully align with the ultimate objective. 
% \cx{no our work in related work. Check my checklist} To address this, we propose incorporating influence functions into MCTS-based data synthesis during the training phase, aiming to identify and prioritize the most valuable data for model training.

\subsection{Influence Function}
\label{subsection:Influence function}

% \cx{need some intro on why IF. this should be more about training data valuation and IF is one common way to do that.}
% Training data valuation is essential for selecting high-quality data, with common approaches including LLM rating~\cite{liu2024a}, reward models~\cite{}, and influence-based selection~\cite{hampel1974influence}. Among these, influence-based selection stands out for its ability to quantify each data point's contribution to model performance using influence functions.
%The influence function, first introduced by~\cite{hampel1974influence}, evaluates the utility of data on model performance and has been widely applied in tasks such as model debugging~\cite{DBLP:journals/corr/abs-2311-03386, DBLP:conf/iclr/ZhengPD0L24}, machine unlearning~\cite{DBLP:conf/icml/PawelczykNL24}, and data poisoning~\cite{DBLP:journals/pami/WuLGZC23}.
% Recent studies have extended its use to improve data quality in LLM pre-training~\cite{yu2024mates}, instruction tuning~\cite{DBLP:journals/corr/abs-2410-14208, DBLP:conf/icml/XiaMGA024}, and reward modeling~\cite{min2025understandingimpacthumanfeedback}. However, its potential for MAS data synthesis that maximizes system capability enhancement remains unexplored. The core challenge in applying influence functions lies in its high computational cost. Classical methods, such as gradient-based approaches~\cite{DBLP:conf/icml/KohL17, DBLP:conf/icml/ParkGILM23} and trajectory-influence based methods~\cite{DBLP:journals/corr/abs-2405-12186}, require the computation of billion-level gradients, which is extremely expensive. For efficient estimation, MATES~\cite{yu2024mates} locally probes the oracle data influence by evaluating the model's reference loss after training on individual data points. Our approach extends the reference loss to non-differentiable validation metrics, thereby enabling the enhancement of data quality through data synthesis. \cx{discuss more papers that leverage data influences to construct training data in other scenarios, pretraining (MATES, tracein), posttrain (montessri instruct, LESS, etc.)}

% The influence function, first introduced by~\cite{hampel1974influence}, evaluates the utility of data on model performance and has been widely applied in tasks such as model debugging~\cite{DBLP:journals/corr/abs-2311-03386, DBLP:conf/iclr/ZhengPD0L24}, machine unlearning~\cite{DBLP:conf/icml/PawelczykNL24}, and data poisoning~\cite{DBLP:journals/pami/WuLGZC23}. Recent studies have extended its use to improve data quality in LLM pre-training~\cite{yu2024mates}, instruction tuning~\cite{DBLP:journals/corr/abs-2410-14208, DBLP:conf/icml/XiaMGA024}, and reward modeling~\cite{min2025understandingimpacthumanfeedback}. However, its potential for MAS data synthesis that maximizes system capability enhancement remains unexplored. The core challenge in applying influence functions lies in its high computational cost. Classical methods, such as gradient-based approaches~\cite{DBLP:conf/icml/KohL17, DBLP:conf/icml/ParkGILM23} and trajectory-influence based methods~\cite{DBLP:journals/corr/abs-2405-12186}, require the computation of billion-level gradients, which is extremely expensive. For efficient estimation, MATES~\cite{yu2024mates} locally probes the oracle data influence by evaluating the model's reference loss after training on individual data points. Our approach extends the reference loss to non-differentiable validation metrics, thereby enabling the enhancement of data quality through data synthesis.

The influence function, first introduced by~\cite{hampel1974influence}, assesses the impact of individual data points on model performance and has become a powerful tool for training data valuation. Unlike alternative approaches such as LLM-based rating methods~\cite{liu2024a} or reward function methods~\cite{DBLP:journals/corr/abs-2410-06508}, the influence function offers distinct advantages by quantifying data utility through rigorous mathematical analysis of model training dynamics. Recent studies have extended its use to improve data quality in LLM pre-training through TraceIn~\cite{DBLP:conf/nips/PruthiLKS20} and MATES~\cite{yu2024mates}, for instruction tuning with Montessori-instruct~\cite{DBLP:journals/corr/abs-2410-14208} and LESS~\cite{DBLP:conf/icml/XiaMGA024}, and for reward modeling with OPORP~\cite{min2025understandingimpacthumanfeedback}. However, its potential for MAS data synthesis that maximizes system capability enhancement remains unexplored. The core challenge in applying influence functions lies in its high computational cost. Classical methods, such as gradient-based approaches~\cite{DBLP:conf/icml/KohL17, DBLP:conf/icml/ParkGILM23} and trajectory-influence based methods~\cite{DBLP:journals/corr/abs-2405-12186}, require the computation of billion-level gradients, which is extremely expensive. For efficient estimation, MATES~\cite{yu2024mates} probes the oracle data influence by evaluating the model's reference loss after training on individual data points. Our approach extends the reference loss to non-differentiable validation metrics, thereby enabling the enhancement of data quality through data synthesis.