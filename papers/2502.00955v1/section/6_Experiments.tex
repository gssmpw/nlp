
\section{Evaluation Results}
\label{section:experiments}

%\cx{this para can be more concise}
In this section, we first evaluate the effectiveness of DITS (§~\ref{subsection:main_results}). Then we demonstrate the superiority of data influence through ablation study (§~\ref{subsection:single_iteration}) and explore the impact of synthesis scaling on data quality (§~\ref{subsection:synthesis time scaling}). Finally, we analyze the effects of selection ratio and iteration times (§~\ref{subsection:selection_ratio}).
%and highlight the advantage of DITS through case study (§~\ref{subsection:case study}).



\subsection{Overall Performance}
\label{subsection:main_results}
In Table~\ref{tab:main-table}, we compare our method DITS-iSFT-DPO with the baseline approaches on both the Information Exchange and Debate tasks. Across all datasets, our method achieves consistent improvement over the baselines, demonstrating the effectiveness and generalizability of DITS. Compared to the single agent CoT approach, our method delivers an average performance enhancement of 91\%. In the Information Exchange task, our method outperforms the advanced multi-agent approach Optima-iSFT-DPO by an average margin of 2.5\%. For mathematical datasets MATH and GSM8k, the improvement achieved by our method is relatively small, which is due to the tasks' difficulty and the small training set.


% Main Table: Analyzing whether our method can outperform Optima under multi-iteration scenarios.

%\input{table/single_iteration}
\begin{figure*}
    \centering
    \includegraphics[width=0.24\linewidth]{figure/hotpot_qa_distribution.pdf}
    \includegraphics[width=0.24\linewidth]{figure/trival_qa_distribution.pdf}
    \includegraphics[width=0.24\linewidth]{figure/arc_distribution.pdf}
    \includegraphics[width=0.24\linewidth]{figure/mmlu_distribution.pdf}
    \caption{The scatter plot and density plots of Q-values and influence scores for the synthetic data. The top 30\% of the data selected by DITS is highlighted in red.}
    % \cx{nice plot though, can you add this code to our repo lol}
    \label{fig:distribution_analysis}
\end{figure*}

\subsection{The Effectiveness of Influence Function}
\label{subsection:single_iteration}
% Main Table: Analyzing whether our method can outperform reward-based method and randomly-sampling method under single-iteration scenarios.

To provide a detailed comparison of the effectiveness of the influence function, we present the results of different data selection methods in Table~\ref{tab:single-iteration}. The experiments are conducted in a single iteration. The Base method represents the multi-agent framework performance with the base model Llama-3-8B-Instruct. The Optima-DPO and Optima-RPO methods utilize the dataset $\mathcal{D}_{\text{tr}}$ sampled through the MCTS approach in Optima to train the model using DPO loss~\cite{DBLP:conf/nips/RafailovSMMEF23} and RPO loss~\cite{DBLP:journals/corr/abs-2404-19733}, respectively. Random Select refers to training on the data randomly sampled from $\mathcal{D}_{\text{tr}}$ with DPO loss, while Q-value Select involves selecting the top-ranked data based on Q-values for training. DITS employs the influence score in Eq.~\eqref{equation:hybrid score} to select the top-ranked data for training, where the variant $\gamma=1$ integrates both Q-value and influence score, and the variant $\gamma=0$ relies solely on the influence score for data selection. For a fair comparison, we set the selection ratio as 50\% for all methods.

As shown in Table~\ref{tab:single-iteration}, we observe that (1) The DITS method achieves consistent performance improvements across all datasets compared to using the full dataset, indicating that the original MCTS-generated dataset contains noisy and lower-quality data. This suggests that further enhancing data quality is beneficial for model performance.  (2) Selecting data based on influence scores outperforms both random selection and Q-value-based selection, highlighting its superior effectiveness in enhancing data quality. To further explore the underlying reasons for this improvement, the following paragraph provides an in-depth analysis of the data distribution. (3) For the Information Exchange task, the variant $\gamma=0$ achieves the best performance, while the variant $\gamma=1$ achieves suboptimal results. In contrast, on the Debate task, the variant $\gamma=1$ generally performs the best. This discrepancy is attributed to the fact that the evaluation metric for the Information Exchange task is F1-score, which introduces more noise into the estimated Q-values, resulting in lower quality in selecting data.
%\cx{can use this experiment to lead to further analyses in later experiments}




\textbf{Distribution Analysis} 
%\cx{We should focus on the influence part, and the Q part is an extra benefit. Also, we should point out the disagreements between influence and Q score, capturing the motivation that the Q value is not perfectly aligned with training needs, using the scatter plot part} 
To provide an in-depth analysis of the advantages of using the influence score for data selection, we visualize the distributions of Q-values and influence scores on the HotpotQA, TrivalQA, ARC-C, and MMLU datasets in Figure~\ref{fig:distribution_analysis}, highlighting the distribution of the top 30\% data points selected by our methods with $\gamma=1$. The visualization results of other datasets can be found in Figure~\ref{fig:scaling} and Figure~\ref{fig:distribution_analysis_appendix}. From the figures, we observe that: (1) There are discrepancies between the influence score and Q-value, which reveals that Q value is not perfectly aligned with training needs. This highlights the importance of integrating the influence score into the MCTS process and data selection process. (2) The data selected by our methods exhibit high influence scores and Q-values, indicating that DITS is capable of selecting high-quality data.

\subsection{Synthesis Time Scaling}
\label{subsection:synthesis time scaling}

In this study, we empirically demonstrate that increasing the synthesis budget during the data synthesis phase enhances model performance, as shown in Figure~\ref{fig:scaling} (b). Specifically, the figure highlights three key observations: (1) Allocating a larger synthesis budget, which extends rollout times and increases the number of expansions, will generate more high-quality data, thereby improving model performance. (2) We validate that allocating resources to influence score estimation can indeed lead to better performance improvements. This is attributed to the fact that the influence score is more aligned with training needs. This underscores the capability of our method to enhance the efficiency of synthesizing training data within a vast action space.
(3) The performance gains from a sixteenfold increase in the synthesis budget are notably smaller compared to the improvements achieved through three times iterative data synthesis and training, as detailed in Table~\ref{tab:main-table}. This comparison highlights the efficiency and effectiveness of the iterative approach. 


% \subsection{Ablation Study-Data influence Guide MCTS}



\subsection{Hyperparameter Aanlysis}
\label{subsection:selection_ratio}
% 2 datasets
\begin{figure}
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=1.0\linewidth]{figure/selection_ratio.pdf}
    \caption{The effect of hyperparameter selection ratio $\alpha$ on the performance of DITS on the 2WMH QA and TrivalQA datasets.}
    \label{fig:selection_ratio}

\end{figure}

\textbf{Selection Ratio} We first investigate the impact of the selection ratio hyperparameter $\gamma$ on model performance. We conduct experiments on two Information Exchange tasks: 2WMH QA and Trival QA datasets and present the results in Figure~\ref{fig:selection_ratio}. We compare Optima-DPO (random Select and Q-value Select) with DITS ($\gamma=0$) and DITS ($\gamma=1$). From the figure, we observe that: (1) Across different selection ratios, DITS consistently outperforms Optima-DPO, demonstrating that our method can select data more beneficial for model training and exhibits strong generalization ability. (2) When an appropriate selection ratio is chosen, the performance of DITS surpasses that of using the full dataset, indicating the presence of noise in original MCTS-generated data and the potential for further improving data quality. (3) When the selection ratio is very small, the performance of all methods declines, indicating that training set size is also crucial for achieving optimal performance. This suggests that an overly small yet high-quality dataset may not be sufficient to train a well-performing model.

% average performance
% Q value 变化趋势， data influence 变化趋势 （2 dataset）
% 1 case study

\begin{figure}
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=1.0\linewidth]{figure/radar.pdf}
    \caption{The relative performance improvement of DITS-iSFT-DPO across all datasets at different iterations. The best performance of each dataset is set as 1.0.} 
    \label{fig:iteration}
\end{figure}


\textbf{Iterative Times} Using the performance of CoT as the baseline, we report the average relative performance improvement of our method, DITS-iSFT-DPO, across all datasets per iteration and present the results in Figure~\ref{fig:iteration}. From the figure, we observe that: (1) Our method achieves an average improvement of 91\% compared to the single-agent CoT approach and an improvement of 64\% over the multi-agent MAD method, demonstrating the effectiveness of our approach. (2) As the number of iterations increases, the average performance continues to improve. Since we start training from the same initial model in each iteration, this indicates that training better models and subsequently synthesizing data can consistently enhance the quality of the generated data and improve the final performance.

% \cx{we probably should compare with baseline MCTS in the scaling curve here, what will be the results?}
% \wt{Due to time constraints, a comprehensive scaling analysis across all datasets was not feasible. Therefore, the findings are briefly summarized in Section~\ref{subsection:synthesis time scaling}}

To gain deeper insights into the iterative data synthesis and training process, we analyzed the distribution of influence scores for synthetic data across different iterations on the HotpotQA and MMLU datasets, as shown in Figure~\ref{fig:iteration_distribution}. The mean of each distribution is highlighted. From the figure, we observe the following trends: (1) As the number of iterations increases, the mean influence score gradually rises, indicating an improvement in the quality of synthetic data. This suggests that the iterative process enhances data quality by refining the model, creating a positive feedback loop that makes data synthesis more effective. (2) With more iterations, the distribution of influence scores becomes more concentrated, suggesting that the model trained on synthetic data achieves more stable quality on specialized tasks. However, this may come at the cost of reduced data diversity.

\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{figure/hotpot_qa_iteration_distribution.pdf}
    \includegraphics[width=0.49\linewidth]{figure/mmlu_iteration_distribution.pdf}
    \caption{The distribution of synthetic data influence scores across different iterations on the HotpotQA and MMLU datasets, with the mean of the distribution highlighted by a red dashed line.}
    % \cx{nice plot though, can you add this code to our repo lol}
    % \vspace{-10pt}
    \label{fig:iteration_distribution}
\end{figure}

% \subsection{Case Study}
% \label{subsection:case study}

% \cx{One potentially missing analysis is what the traces selected by DITS versus baseline MCTS, we can first show that the two are different in xxx fractions in a quantitative study, kind of as a follow up of Fig 3?, then we can show examples of traces that have high influence but not necessarily high q value in this case study}

% \cx{will be nice if we can show one trace for each of the two set up (IE: hotpotqa, debate mmlu), illustrate the multi-agent interaction process and the differences of influence-MCTS selected trace and Q-value selected one.}

% \cx{do we want to show that in test time we should go with q-value? probably no.}

% \input{table/case}