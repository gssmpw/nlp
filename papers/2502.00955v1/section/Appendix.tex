\appendix
\onecolumn
\input{table/hyper_parameter_main}
\input{table/hyper_parameter_single}

\section{Method Details}
\label{appendix:method_details}

\subsection{Reward Function}
Following Optima~\cite{DBLP:journals/corr/abs-2410-08115}, we define each trajectory $\tau_i$ is then evaluated using a reward function $R: \mathcal{T} \rightarrow \mathbb{R}$:
\begin{equation}
    R(\tau_i^j) = R_\text{task}(\tau_i^j) - \lambda_\text{token} R_\text{token}(\tau_i^j) + \lambda_\text{loss} \frac{1}{R_\text{loss}(\tau_i^j)}.
    \label{eq:reward}
\end{equation}
Here, $R_\text{task}: \mathcal{T} \rightarrow \mathbb{R}$ is the task-specific performance metric, $R_\text{token}(\tau_i^j) = \frac{\#\text{Tokens}(\tau_i^j)}{\max_k(\{\#\text{Tokens}(\tau_i^k)\}_k)}$ is the normalized token count, and $R_\text{loss}(\tau_i^j) = g\big(\mathcal{L}(\mathcal{M}_\text{base}, d_i, \tau_i^j)\big)$ is based on the language modeling loss of the base model $\mathcal{M}_\text{base}$. The positive coefficients $\lambda_\text{token}$ and $\lambda_\text{loss}$ are hyper-parameters. More details can refer to Optima~\cite{DBLP:journals/corr/abs-2410-08115}.

\subsection{Initial Data Filtering}
For the preference data pairs obtained from the MCTS tree, we follow the Optima~\cite{DBLP:journals/corr/abs-2410-08115} by initially filtering the data pair $(s, a_i^h, a_i^l)$. Specifically, we select pairs that satisfy: (1) $R(s, a_i^h) > \lambda_{\text{dpo-filter}}$. (2) $R(s, a_i^h)- R(s, a_i^l)>\lambda_{\text{dpo-diff}}$. (3) For preference pairs starting with the same problem $p$, we rank these pairs based on their Q-values and select the top 50\% of the pairs.

\section{Training Details}
\label{appendix:training details}

 The hyperparameters we used are shown in Table~\ref{tab:hyper_parameters_main} and Table~\ref{tab:hyper_parameters_single}.





\section{Distribution Analysis}

\begin{figure*}
    \centering
    %\includegraphics[width=0.24\linewidth]{figure/trival_qa_distribution.pdf}
    \includegraphics[width=0.24\linewidth]{figure/cbt_distribution.pdf}
    \includegraphics[width=0.24\linewidth]{figure/math_distribution.pdf}
    \includegraphics[width=0.24\linewidth]{figure/gsm8k_distribution.pdf}
    \caption{The scatter plot and density plots of Q-values and influence scores for synthetic data. The top 30\% of the data selected by DITS is highlighted in red.}
    % \cx{nice plot though, can you add this code to our repo lol}
    % \vspace{-10pt}
    \label{fig:distribution_analysis_appendix}
\end{figure*}

We visualize the distributions of Q-values and influence scores on the CBT, MATH, and GSM8k datasets in Figure~\ref{fig:distribution_analysis_appendix}, highlighting the distribution of the top 30\% data points selected by our methods with $\gamma=1$. From the figures, we observe the following: (1) There are discrepancies between the influence score and the Q-value, indicating that the Q-value is not perfectly aligned with training needs. This underscores the importance of incorporating the influence score into both the MCTS process and the data selection strategy. (2) The data selected by our method exhibit both high influence scores and Q-values, demonstrating that DITS effectively identifies and selects high-quality data. (3) In the mathematics dataset, the distribution of the influence score is concentrated at several discrete points. This is because the dataset is relatively challenging, and significant model improvements may be required to change the correctness of certain answers. As a result, the metric exhibits a stepwise effect.


\section{Case Study}
\input{table/case}
As illustrated in Figure~\ref{tab:cases}, we present a comparative case study highlighting the differences in data selection outcomes when using Q-value versus influence score for the same task. The task—"Which film has the director who was born later, Eyes of the Forest or Stardust on the Sage?"—was analyzed through agent dialogues between Alice and Bob.  

In the Q-value-selected data pair, the dialogue history efficiently conveyed the directors' birth dates within a single interaction round. The chosen response directly identified "Stardust on the Sage" as the correct answer using special token markers in the response, achieving an exceptionally high Q-value. Meanwhile, the rejected response, although redundant in restating first-round information, contained no errors, thereby maintaining a high Q-value. However, the minimal difference between the paired responses resulted in low influence scores, limiting their utility for model improvement.  

In contrast, the influence-score-selected data pair exhibited incomplete information sharing in the dialogue history. The chosen response correctly ruled out Hillyer as the director of "Stardust on the Sage" but required more information to get to the correct answer, leading to lower Q-values. More critically, the rejected response contained hallucinatory content—an outright factual error falsely attributing Katedza as the film’s director—which fundamentally obstructed correct reasoning and resulted in an extremely low Q-value. This high-contrast pair data holds significant pedagogical value, as it juxtaposes valid reasoning with critical hallucinations, thereby achieving superior influence scores.  

Our analysis reveals that while high-Q-value pairs ensure accurate answers, they may have low influence scores and contribute little to multi-agent training. Conversely, data pairs with pronounced contrasts in reasoning validity—despite both exhibiting lower Q-values—substantially enhance model robustness against hallucinations by explicitly demarcating errors. These findings strongly advocate prioritizing influence score metrics over Q-value in both data synthesis and tree search to maximize model performance.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%