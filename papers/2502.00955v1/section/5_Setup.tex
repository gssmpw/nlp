\section{Experimental Setup}
\label{section:setup}

In this section, we will introduce the datasets, metrics, and baseline methods employed in our experiments.

\textbf{Dataset}
To validate the collaborative and task allocation capabilities of MAS, following~\citet{DBLP:journals/corr/abs-2410-08115}, 
%\cx{need to justify why we use this setting} 
we evaluate our framework DITS mainly in two settings: Information exchange and Debate. In the information exchange setting, the relevant context is divided between two agents. The agents must identify the relevant information and communicate with each other to derive the final answer. This is designed to examine the ability of agents to collaborate and accomplish tasks under conditions of partial information. This setting includes HotpotQA~\cite{DBLP:conf/emnlp/Yang0ZBCSM18}, 2WikiMultiHopQA (2WMH QA)~\cite{DBLP:conf/coling/HoNSA20}, TrivalQA~\cite{DBLP:conf/acl/JoshiCWZ17}, and CBT~\cite{DBLP:journals/corr/HillBCW15}. In the debate setting, two agents work together to solve a task: one agent proposes solutions, while the other evaluates their correctness. 
%Through iterative discussion, they reach a consensus, arriving at the final answer. 
This is intended to assess the capacity of agents to allocate tasks and execute them in a complete information environment. The debate setting includes GSM8k~\cite{DBLP:journals/corr/abs-2110-14168}, MATH~\cite{DBLP:conf/nips/HendrycksBKABTS21}, ARC's challenge set (ARC-C)~\cite{DBLP:journals/corr/abs-2102-03315} and MMLU~\cite{DBLP:conf/iclr/HendrycksBBZMSS21}. We use 0-shot for all benchmarks.
% \cx{maybe add 1-2 sentence on why each setting is useful}




\textbf{Metrics}
Following~\citet{DBLP:journals/corr/abs-2410-08115}, we employ the F1 score between final answers and labels as evaluation metrics for information exchange tasks. For debate tasks, we utilize exact match accuracy (GSM8k, ARC-C, MMLU) or Sympy-based~\cite{DBLP:journals/peerj-cs/MeurerSPCKRKIMS17} equivalence checking (MATH). 
% They are non-differentiable metrics.
% \cx{don't bold too much}

\input{table/main_table}
\input{table/single_iteration}
\textbf{Baseline} We compare our methods with: (1) Chain-of-Thought (CoT)~\cite{DBLP:conf/nips/Wei0SBIXCLZ22}: single agent pipeline which enables complex reasoning to derive the final answer. %(2) \textbf{Self-Consistency (SC)}~\cite{DBLP:conf/iclr/0002WSLCNCZ23}: single agent pipeline which generates multiple candidate answers (n=8) and determine the final answer through majority voting. 
(2) Multi-Agent Debate (MAD)~\cite{DBLP:conf/icml/Du00TM24}: multi-agent pipeline where different reasoning processes are discussed multiple rounds to arrive at the final answer. (3) AutoForm~\cite{DBLP:conf/emnlp/ChenYYSQYXL024}: multi-agent pipeline where the agents utilize non-nature language formats in communication to improve efficiency. (4) Optima~\cite{DBLP:journals/corr/abs-2410-08115}: a multi-agent framework that enhances communication efficiency and task effectiveness through Supervised Finetuning and Direct Preference Optimization. It has three variants, namely Optima-iSFT, Optima-iDPO, and Optima-iSFT-DPO. We follow the iSFT-DPO variant of Optima and improve its data synthesis and selection process to obtain DITS-iSFT-DPO.


\textbf{Implementation Details} We utilize the Llama-3-8B-Instruct as the base model across all datasets. The interaction between the agents is terminated either when the final answer is explicitly marked by a special token or when the maximum limit of interactions is reached. 
Unless otherwise specified, we set the hyperparameters to $\alpha=0.5$ and $\gamma=1$. When collecting influence scores via single-step gradient descent, we utilize LoRA (Low-Rank Adaptation)~\cite{DBLP:conf/iclr/HuSWALWWC22}. We set expand time $d=3$ and repeat time $k=8$ for all datasets. More details are provided in the Appendix~\ref{appendix:training details}. 
%cx{maybe a little more implementation details here}