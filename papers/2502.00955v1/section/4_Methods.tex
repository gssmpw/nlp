\section{Method}
\label{section:method}
In this section, we first formalize the multi-agent task and MCTS-based data synthesis (§~\ref{subsection:multi-agent-data-synthesis}), then introduce the data influence-oriented data selection
%\cx{not sure whether we should introduce agent-aware here to name our method, we should focus on influence-driven MCTS?} hybrid \cx{also don't think we should say hybrid, focus on our novelty, then it is naturally embedded in existing MCTS methods} data selection using influence scores 
(§~\ref{subsection:hybrid data selection}), and finally present the iterative data synthesis process (§~\ref{subsection:iterative data synthesis}).

\begin{figure*}
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=0.9\linewidth]{figure/Main_structure.pdf}
    \caption{Overview of our method. (a) illustrates the traversal of a cyclic agent network in topological order. We introduce virtual agents to distinguish the same agent in the traversal. (b) showcases the application of MCTS to generate synthetic multi-agent training data, where the color of each agent represents the magnitude of the node's Q-value. (c) depicts the computation process of influence scores for a non-differentiable metric, highlighting that data points with high Q-values may correspond to low influence scores.} 
    \vspace{-0.3cm}
    \label{fig:framework}
\end{figure*}

\subsection{Multi-Agent Data Synthesis} \label{subsection:multi-agent-data-synthesis}
% How to generalize to multi-agent network
In this work, we model the topology structure for multi-agent collaboration as a directed graph. Concretely, we denote a feasible topology as $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, as demonstrated in Figure~\ref{fig:framework} (a). We allow the presence of cycles in the graph, indicating that multiple rounds of information exchange are permitted among agents. We assume that our agent network can be linearly traversed in topological order $A_1\oplus A_2 \oplus \cdots \oplus A_M$~\cite{Bondy1976, book:gross:2005, qian2024}, where $A_m \in \mathcal{V}$. Different $A_m$ may represent the same agent being visited at different time steps. For clarity and convenience, we use different symbols to distinguish them.

% Why chose MCTS
% \cx{we should have a more mathematical and structured description of our system. Use more equations to present the process.}

In this way, we could utilize MCTS to synthesize training data for MAS. We mainly follow the configuration in Optima~\cite{DBLP:journals/corr/abs-2410-08115} and construct the tree as follows: As shown in Figure~\ref{fig:framework} (b), the synthesis tree begins with a specific task instruction $p$. 

\textbf{Selection}: We select a node $n$ to
expand from the candidate node set, where a node $n=(s,a)$ refers to an agent $A_m$ in state $s$ that takes action $a$. We use the edit distance to filter out nodes that are similar to expanded nodes to obtain the candidate node set.
\begin{equation}   
    N_{\text{cand}} = \{n_j| n_i \in N_{\text{expanded}}, n_j\in N_{\text{all}}, S_{i,j} \geq 0.25 \},
\end{equation}
where $S_{i,j}=\frac{\text{edit\_distance}(n_i, n_j)}{\max (|n_i|,|n_j|)}$ and $\text{edit\_distance}(n_i, n_j)$ represents the edit distance between the action strings of two nodes. $N_\text{all}$ and $N_{\text{expanded}}$ denotes the whole node set and expanded node set. Then we select a node for the candidate set $N_{\text{cand}}$ based on softmax distribution of Q-values.
\begin{equation}
n\sim \text{Softmax}(\{Q(n)\}_{n\in N_{\text{cand}}}),
\end{equation}
where $Q(n) = Q(s, a)$ and the softmax distribution balances exploration and exploitation during the search process.


\textbf{Expansion} For each selected node $n$, we denote the new state as $s'=\text{Trans}(s, a)$, where $\text{Trans}(\cdot)$ is the transit function determined by the environment. Then we sample $d$ actions from LLM paramtered agents $A_{m+1}$: 
\begin{equation}
    \{a_1', \cdots, a_d' \} \sim A_{m+1}(s').
\end{equation}

\textbf{Simulation} For each generated action $a_i'$, we simulate the agent interaction $\tau_i$ until the termination state.
\begin{equation}
    \tau_i = \text{Simulation}(A_{m+2}, \cdots, A_{M}, s', a_i').
\end{equation}
Meanwhile, we construct all $(s, a)$ pairs in the trajectory as new nodes and add them to $N_{\text{all}}$.

\textbf{Backpropagation} Once a trajectory $\tau$ is completed, we can obtain the trajectory reward $R(\tau)$ detailed in Appendix~\ref{appendix:method_details}. We update the Q-value of all nodes in the trajectory with the average Q-value of their children.
\begin{equation}
    Q(n) = Q(s,a) = \sum_{n'\in \text{Child}(n)} \frac{1}{|\text{Child}(n)|} Q(n'),
\end{equation}
where $\text{Child}(n)$ denotes the children set of node $n$. Additionally, due to the complex interactions among multiple agents, the Q-value estimates obtained from $d$ rollouts may be inaccurate. Allocating more inference budget in the data synthesis phase may improve the quality of the generated data and enhance the system’s performance.

We repeat the above process $k$ times and finish the generation process. Then we can construct paired action preferences for agent $A_i$ at state $s$ by selecting the action $a_i^h$ with the highest Q-value and the action $a_i^l$ with the lowest Q-value to form the preference data:
\begin{equation}
    z  = \left(s, a_i^{h}, a_i^{l}\right).
\end{equation}
To update the parameter of agent $A_i$, we utilize the Direct Preference Optimization (DPO) loss to directly encourage the model to prioritize responses that align with preferences $a_i^{h}$ over less preferred ones $a_i^{l}$.
\begin{equation}
\small
    \mathcal{L}_{DPO} = \mathbb{E}_{z} \bigg[ 
    -\log \sigma \bigg( \beta \bigg[
        \log \frac{\pi_\theta(a_i^{h} \mid s)}{\pi_{\text{ref}}(a_i^{h} \mid s)} 
        - \log \frac{\pi_\theta(a_i^{l} \mid s)}{\pi_{\text{ref}}(a_i^{l} \mid s)}
    \bigg] \bigg) \bigg],
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function, and $\pi_{\text{ref}}$ represents the reference model, typically the SFT model.



% In this way, we could utilize MCTS to synthesize training data for MAS. As shown in Figure~\ref{fig:framework} (a), the synthesis tree begins with a specific task instruction  $p$, where all nodes at the $i$-th layer correspond to the $i$-th agent $A_i$. DITSinct upstream trajectories $s$ represent varying inputs to agent $A_i$ under different conditions. The divergent branches of a node illustrate the potential actions an agent can take given the particular input. The Q-value for a specific action $Q_i(s, a)$ by agent $A_i$ is estimated through multiple rollouts $\{o_1, \cdots, o_m \}$, which randomly simulate transitions to a terminal state based on the current agents' policy. 
% \begin{equation}
%     Q_i(s,a) = \frac{1}{m} \sum_j R(s,a,o_j),
% \end{equation}
% where $R(s,a,o_j)$ denotes the trajectory-level reward with outputs $o_j$. The Q-value serves as a measure of each agent's contribution to the final answer in the MAS to some extent. \cx{no experimental detail in methodology section} In this work, we follow the MCTS configuration in Optima~\cite{DBLP:journals/corr/abs-2410-08115}, with detail available in Appendix~\ref{}. When the tree search is completed, we can construct paired action preferences for each agent $A_i$ at state $s$ by selecting the action $a_i^{h}$ with the highest Q-value and the action $a_i^{l}$ with the lowest Q-value to form the preference data.
% \begin{equation}
%     z = (x,y^w,y^l) = \left(s, a_i^{h}, a_i^{l}\right),
% \end{equation}
% Since the feasible action space is infinite, MCTS may fail to generate the optimal action within a limited number of $d$ expansions. Additionally, due to the complex interactions among multiple agents, the Q-value estimates obtained from $m$ rollouts may be inaccurate. These factors can result in the collected data containing low-quality samples and significant noise.

% \cx{up to here I am thinking that we may not need Sec 3, we can describe MCTS and DPO as part of our method, then IF as the selection part. This makes the method presentation more streamlined}
% \cx{e.g., we can describe the MCTS process formally, its goal, the roll out, the search space, the priority used in the search, and the DPO upon it. Then how we use Influ to define the search part. (as selection)}
% \cx{after that we can have a 4.3 to recap the full process, including iterative synthetis, training process, and inference}

% As shown in Figure~\ref{fig:scaling}, in this work we first empirically validate that increasing the number of inference tokens during data synthesis for MAS to expand more candidate actions and rollouts more times can improve the quality of the generated data and subsequently enhance the system's performance.\cx{this reads like syntehtic time scaling is property of baseline MCTS, and readers may think it is obvious that more syntheized data is going to lead to better performance. Put synthetize time scaling as an observation, influe-MCTS as the main contribution which leads to the scaling (or at least better scaling) is the right choice.}

\subsection{Data Influence-Oriented Data Selection}
\label{subsection:hybrid data selection}

% The synthesis time scaling approach mentioned in section~\ref{section:multi-agent-data-synthesis} is straightforward but not optimal. This is due to the vast action space, which results in low scaling efficiency. Moreover, selecting the action with the highest Q-value may not yield much improvement for the current model. For example, \cite{} suggests that actions with higher advantages are more valuable for training the current model compared to actions with merely high Q-values. Hence, in this section, we propose a novel scaling dimension for data synthesis in MAS, aimed at finding the most valuable data for model training.

While improving the accuracy of Q-value estimation can enhance data quality to some extent, it is both highly inefficient and suboptimal. During the training phase, the primary goal of synthetic data is to maximize its contribution to model performance improvement, rather than ensuring the data is correct. As shown in Figure~\ref{fig:framework} (c), although the data pair $z_1$ has a higher $Q(s,a_i^h)$, the data pair $z_2$ contributes more significantly to system performance improvement, as reflected by its higher influence score. This discrepancy may arise because the action in $z_2$ provides a greater advantage compared to the less preferred action.

% Our approach is inspired by data influence functions~\cite{}. The function $\mathcal{I}$ was developed to measure the difference in reference loss when a data point is assigned a higher weight in the training dataset.
% % \begin{equation}
% %     \mathcal{M}^* = \arg\min \sum_{i=0}^{n} \frac{1}{n} L(x_i, \mathcal{M})
% % \end{equation}
% \begin{equation}
%     \mathcal{M}_{\epsilon, z_i}^* = \arg\min \sum_{i=0}^{n} \frac{1}{n} L(z_i, \mathcal{M}) + \epsilon L(z_i, \mathcal{M}),
% \end{equation}
% \begin{equation}
%     \mathcal{I}_{\mathcal{M}^*}(z_i, \mathcal{D}_r) \stackrel{\text{def}}{=} \mathcal{F}(\mathcal{D}_r | \mathcal{M}_{\epsilon, z_i}^* ) -\mathcal{F}(\mathcal{D}_r |  \mathcal{M}^* ),
%     \label{equation:definition_data_influence}
% \end{equation}
% where $\mathcal{M}$ represents the agent parameters, $L$ represents the training loss, $\mathcal{F}(\mathcal{D}_r|\mathcal{M})$ indicates the test metric $\mathcal{F}$ achieved by agent parameter $\mathcal{M}$ on the reference dataset $\mathcal{D}_r$.

Hence, in this paper, we introduce the influence score $\mathcal{I}$ to quantify the impact of data on the current agent's performance. 
The influence score $\mathcal{I}$ was developed to measure the difference in loss when a data point is assigned a higher weight in the training dataset.  Suppose the agent $A$ is parameterized by $\theta$. We denote the optimal parameters learned by minimizing the training loss $\mathcal{L}_{\text{tr}}$ on the dataset $\mathcal{D}_{\text{tr}}$, with a data point $z_i$ assigned an additional weight of $\epsilon$, as:
\begin{equation}
    \theta_{\epsilon, z_i}^* = \underset{\theta}{\arg\min} \sum_{z_j\in \mathcal{D}_{\text{tr}}} \frac{1}{|\mathcal{D}_{\text{tr}}|} \mathcal{L}_{\text{tr}}(z_j, \theta) + \epsilon \mathcal{L}_{\text{tr}}(z_i, \theta).
\end{equation}
Under standard assumptions, such as the twice-differentiability and strong convexity of the loss function $\mathcal{L}_{\text{tr}}$, the influence function can be derived via the chain rule of the derivatives~\cite{DBLP:conf/icml/KohL17}:
\begin{equation}
\begin{split}
    \mathcal{I}_{\mathcal{L}_{\text{tr}}}(z_i, \mathcal{D}_{\text{tr}}) &\stackrel{\text{def}}{=} \frac{d \mathcal{L}_{\text{tr}}(z_i, \theta_{\epsilon, z_i}^*)}{d\epsilon} \bigg|_{\epsilon=0} \\
    &\approx -\nabla_\theta \mathcal{L}_{\text{tr}} \big|_{\theta=\theta^*}^T H(\mathcal{D}_{\text{tr}}; \theta_{\epsilon, z_i}^*)^{-1} \nabla_\theta \mathcal{L}_{\text{tr}} \big|_{\theta=\theta^*},
    \label{equation:influence score}
\end{split}
\end{equation}
where $H(\mathcal{D}; \theta) := \nabla_\theta^2\left(\frac{1}{|\mathcal{D}|} \sum_{z\in \mathcal{D}}L_{\text{tr}}(z; \theta)\right)$ and $\nabla_\theta L_{\text{tr}} = \nabla_\theta L_{\text{tr}}(z;\theta)$.

However, the DPO loss does not effectively align with downstream task performance. Our experiments reveal a weak correlation (less than 0.2) between the DPO loss and performance metrics $\mathcal{F}$ such as F1-score or Accuracy on the validation set. This observation is consistent with findings reported in~\citet{DBLP:journals/corr/abs-2406-02900, DBLP:journals/corr/abs-2410-11677}. This indicates that we must redefine the influence score using the changes of non-differentiable performance metrics on the validation set.
\begin{equation}
\mathcal{I}_{\mathcal{F}_{\text{val}}}(z_i, \mathcal{D}_{\text{val}}) := \frac{\mathcal{F}_{\text{val}}(z_i, \theta_{\epsilon,z_i}^*) - \mathcal{F}_{\text{val}}(z_i, \theta^*)}{\epsilon}.
\end{equation}

Due to non-differentiable metric $\mathcal{F}_{\text{val}}$, the influence function cannot be directly derived using gradients or the chain rule. Instead, we use a finite difference method combined with parameter perturbation to approximate the rate of change. The perturbed optimal parameter $\theta_{\epsilon,z_i}^*$ can be rewritten as:
\begin{equation}
    \theta_{\epsilon,z_i}^* = \theta^* + \epsilon\Delta \theta + o(\epsilon),
\end{equation}
where $\Delta\theta$ represents the direction of parameter change. Following~\citet{yu2024mates}, the direct is typically driven by the gradient of the training loss.
\begin{equation}
    \Delta \theta \propto -\nabla_\theta \mathcal{L}_{\text{tr}}(z_i, \theta^*).
\end{equation}
Since the parameter update is dominated by the training loss gradient, we adopt a one-step gradient descent update:
\begin{equation}
    \theta_{\epsilon,z_i}^* \approx \theta^* - \eta\epsilon\nabla_\theta \mathcal{L}_{\text{tr}}(z_i,\theta^*),
\end{equation}
where $\eta$ is the learning rate, and $\epsilon$ is a very small perturbation strength. Combining the finite difference and parameter update, the influence function is approximated as:
\begin{equation}
\begin{split}
    &\mathcal{I}_{\mathcal{F}_{\text{val}}}(z_i, \mathcal{D}_{\text{val}}, \theta^*) \approx \\ &\frac{1}{\epsilon} \left[ \mathcal{F}_{\text{val}}(z_i,  \theta^* - \eta\epsilon\nabla_\theta L_{\text{tr}}(z_i,\theta^*)) \right. 
     \left. - \mathcal{F}_{\text{val}}(z_i,\theta^*) \right].
\end{split}
\end{equation}

Compared to performing simulations to estimate Q-value more accurately, conducting inference on a validation dataset to estimate data influence can better guide the selection of higher-quality data points.

Specifically, Our selection strategy combines Q-values and influence scores to effectively identify the highest-quality pair data: 
\begin{equation}
\begin{split}
    H(z_i) = \mathcal{I}_{\mathcal{F}_{\text{val}}}(z_i, \mathcal{D}_{\text{val}}, \theta) + \gamma \cdot Q(s, a_i^h) , 
    \label{equation:hybrid score}
\end{split}
\end{equation}
where $\theta$ denotes the current parameters of agent $A_m$. Finally, after filtering out low-quality data as described in~\citet{DBLP:journals/corr/abs-2410-08115}, synthetic data are ranked based on the scores, and the Top $\alpha$ are selected to construct the training dataset $\mathcal{D}_{\text{tr}}$.

{
\setlength{\textfloatsep}{0em}
\begin{algorithm}[t]
\caption{DITS-iSFT-DPO}
\label{alg:DITS-isft-dpo}
\begin{algorithmic}[1]
\REQUIRE Initial model $\theta_\text{init}$, problem Set $\mathcal{D}$, validation Set $\mathcal{D}_{\text{val}}$, and max iterations $T$
\ENSURE parameter $\theta_T$
\STATE $\theta_0 \gets \theta_\text{init}$
\FOR{$t = 1$ to $T$}
    \STATE $D_t^{SFT}$ $\gets$ SFTDataCollect($\theta_{t-1}$) \COMMENT{\small{Following~\citet{DBLP:journals/corr/abs-2410-08115}}}
    \STATE $\theta_t$ $\gets$ SFT($D_t^{SFT}$, $\theta_\text{init}$) \COMMENT{\small{Following~\citet{DBLP:journals/corr/abs-2410-08115}}}
    \STATE $\mathcal{D}_t^\text{DPO} \gets \emptyset$
    \FORALL{$p_i \in \mathcal{D}$}
        \STATE $\mathcal{D}_i^\text{DPO} \gets \text{MCTSSynthesis}(\theta_t, p_i)$ 
        \STATE $\mathcal{I}_{\mathcal{F}_{\text{val}}}\gets \text{DataInfluenceCollect}(\mathcal{D}_{\text{val}})$ 
        \STATE $\mathcal{D}_t^\text{DPO} \gets \mathcal{D}_t^\text{DPO} \cup \mathcal{D}_i^\text{DPO}$
    \ENDFOR
    \STATE $\mathcal{D}_t^{DPO} \gets \text{InfluSelection}(\mathcal{D}_t^{DPO}, \mathcal{I}_{\mathcal{F}_{\text{val}}}$)
    \STATE $\theta_{t}$ $\gets$ DPO($\mathcal{D}_t^{DPO}, \theta_t)$
\ENDFOR
\OUTPUT $\theta_T$
\end{algorithmic}
\end{algorithm}

}

\subsection{Iterative Data Synthesis}
\label{subsection:iterative data synthesis}
In addition to utilizing the current model for data synthesis, we propose an iterative refinement approach to generate higher-quality data. By continuously training and enhancing the model, its capabilities improve, enabling the generation of more valuable synthetic data in subsequent iterations. At iteration $t$, we generate the training dataset $\mathcal{D}_{\text{tr}}^t$ based on the parameters $\theta_{t-1}$ and train a new model from the initial model using $\mathcal{D}_{\text{tr}}^t$. The corresponding pseudocode can be found in Algorithm~\ref{alg:DITS-isft-dpo}.



% \cx{Sec 3 and 4 need major revision. Then Introduction.}








