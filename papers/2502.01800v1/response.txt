\section{Related Work}
Recent developments in reinforcement learning have proven that policies trained in simulation can be effectively translated to real-world robots for contact-rich assembly tasks **Sutton, "Planning and Acting"**. One key innovation that has contributed to the development of robust policies is domain randomization **Todorov, "Domain Randomization"**, wherein environment parameters are sampled from a distribution during training such that the learned policy can be robust to environmental uncertainty on deployment. 

Some previously explored learning strategies include minimization of divergence with a target sampling distribution using multivariate Gaussians **Kumar, "Minimizing Divergence"**, maximization of entropy using independent beta distributions **Todorov, "Entropy Maximization"**, and progressive expansion of a uniform sampling distribution via boundary sampling **Choi, "Boundary Sampling"**. Here, we propose a novel learned domain randomization technique using normalizing flows **Rezende, "Normalizing Flows"** as a neural sampling distribution, thus increasing flexibility and expressivity.

In addition to learning robust policies, such sampling distributions can be used as indicators of the world states under which the policy is expected to succeed. Some previous works have combined domain randomization with information gathering via system identification **Kumar, "Domain Randomization"**. In this work, we similarly make use of our learned sampling distribution as an out-of-distribution detector in the context of a multi-step planning system.