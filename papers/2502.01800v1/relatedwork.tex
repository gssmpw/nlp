\section{Related Work}
Recent developments in reinforcement learning have proven that policies trained in simulation can be effectively translated to real-world robots for contact-rich assembly tasks~\citep{dynamic_compliance, industreal, forge, Jin2023}. One key innovation that has contributed to the development of robust policies is domain randomization~\citep{understanding_dr, original_dr}, wherein environment parameters are sampled from a distribution during training such that the learned policy can be robust to environmental uncertainty on deployment. 

Some previously explored learning strategies include minimization of divergence with a target sampling distribution using multivariate Gaussians~\citep{gaussian_dr}, maximization of entropy using independent beta distributions~\citep{entmax}, and progressive expansion of a uniform sampling distribution via boundary sampling~\citep{adr}. Here, we propose a novel learned domain randomization technique using normalizing flows~\cite{rezende2015variational} as a neural sampling distribution, thus increasing flexibility and expressivity.

In addition to learning robust policies, such sampling distributions can be used as indicators of the world states under which the policy is expected to succeed. Some previous works have combined domain randomization with information gathering via system identification \citep{bayessim, normflows_adaptive_dr}. In this work, we similarly make use of our learned sampling distribution as an out-of-distribution detector in the context of a multi-step planning system.