\section{ARC Agent Architecture}
\label{appendix:O}

\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Team_of_Agents.png}
   \caption{An agentic decision graph modeling the workflow for solving ARC tasks. Firstly, the user-provided dataset and problem inputs are loaded, preprocessed, and dispatched through the Select Problem sub-graph. Subsequent modules then perform data augmentations and generate model prompts (Prompt formatting). Next, specialized codes are generated (Create Induction Codes) and executed (Execute Induction Codes). The agent then simulates (Program Simulation) and evaluates the resultant solutions (Obtain Test Output).}

   \label{fig:ARC_Team_of_Agents}
   \vspace{-5pt}
\end{figure*}


\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Data_Augmentation.png}
   \caption{The agent begins by checking whether a ARC Task id is provided or must be retrieved from a dataset. It then writes and executes two Python scripts, one generating leave-one-out subsets, the other applying rotation and flip transformations based on the input training data. Conditional nodes (If and If-Else) govern whether the agent fetches data from the user or a stored dataset, while Write File and Shell Command nodes create and run the scripts. The resulting augmented files, including leave\_one\_out\_data.json and augmented\_data\_{task\_id}.json, are output alongside the final Task id and base directory reference, completing the data augmentation process.}

   \label{fig:ARC_Data_Augmentation}
   \vspace{-5pt}
\end{figure*}

\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Prompt_Formatting.png}
   \caption{An agent pipeline for generating prompt-formatted data from an ARC puzzle dataset. The process begins with two Graph Input nodes (for the base directory and task ID), which may be supplied by the user or fallback to default values. Conditional nodes handle missing inputs by prompting for a problem number and retrieving the corresponding dataset row. Destructure nodes extract relevant JSON fields, while Write File nodes produce Python scripts (prompt\_format\_data.py) that apply transformations such as rotations and flips before reformatting the data into prompts. Shell Command nodes then execute these scripts, and the resulting outputs are collected in Graph Output nodes.}

   \label{fig:ARC_Prompt_Formatting}
   \vspace{-5pt}
\end{figure*}

\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Create.png}
   \caption{This agent graph automates the generation of induction codes from user-defined prompts. The workflow begins with two primary inputs, the Task id and Base directory, and may prompt for an additional Problem input. A file of prompts is read from the specified directory, then parsed into an array for iterative processing. Each segment of text is sent to a Hugging Face language model to produce a runnable Python code snippet. This code is subsequently appended to a dataset using Append to Dataset. A loop and an event-based mechanism (Wait For Event and Raise Event) control the iteration, ensuring each prompt is processed in sequence. The graph outputs the final induction codes dataset, along with the pertinent task and directory information.}

   \label{fig:ARC_Create_Induction_Codes}
   \vspace{-5pt}
\end{figure*}

\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Execute.png}
   \caption{ This agent automates the generation and execution of induction code blocks derived from a user-specified or dataset-derived task identifier. The agent begins by checking whether a Task id is provided; if not, it prompts for a problem number and fetches a relevant record from a dataset. In parallel, the user may also supply a Base directory, or the agent falls back to a default path. A Python\_Code node supplies the script content, which is written to gen\_induction\_codes.py. The script is then executed via a Shell Command node, extracting Python code blocks from a string and saving them as multiple Python files and a JSON record. Finally, the agent outputs the validated Task id and base directory, completing the code induction process.}

   \label{fig:ARC_Execute_Induction_Codes}
   \vspace{-5pt}
\end{figure*}

\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Program_Simulation.png}
   \caption{This agent automates the generation and execution of a program for evaluating puzzle transformations. It begins with two Graph Input nodes receiving the user's base directory and task ID, with conditional logic prompting for missing inputs. The core Code node contains a Python script that dynamically imports and runs `transform` functions from multiple scripts (code\_taskid\_n.py). This script is written to a file (using Write File), then executed via the Shell Command node with arguments specifying the task ID, data file path, and directory of code files. The agent collects and returns three outputs: the verified base directory, final command output, and the processed task ID.}

   \label{fig:ARC_Program_Simulation}
   \vspace{-5pt}
\end{figure*}

\begin{figure*}[htb]
  \centering
   \includegraphics[width=1.0\linewidth]{ARC_Evaluation.png}
   \caption{An agent graph that automates test-time evaluation for the ARC puzzle dataset by generating and running a Python script. The agent accepts two primary inputs a task identifier and a base directory through graph input nodes. Conditional nodes check whether these inputs are provided and, if needed, prompt the user (the Problem node) or set default values. The agent then composes a Python evaluation script and writes it to a file. Finally, it constructs a command string that references the task identifier, data file paths, and script name, and executes this command in the specified directory. The workflow streamlines the creation and invocation of an evaluation pipeline, and outputs JSON-based accuracy metrics.}

   \label{fig:ARC_Evaluation}
   \vspace{-5pt}
\end{figure*}