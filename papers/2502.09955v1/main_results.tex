\section{Results}

\newcommand{\C}{\ding{52} }
\newcommand{\X}{\ding{55} }
\newcommand{\F}{\ding{108} }

\subsection{IMO}

We perform extensive evaluations on IMO combinatorics problems using different methods and models. We test all combinatorics problems from non-contaminated exams. Figure \ref{fig:imo_gold} reports for each method and model if the answer is correct by \C, and \X otherwise. Running times, in brackets, are in seconds. Similar tables for all 2024 IMO, USAMO, and 2023 IMO ShortList problems appear in Appendices C, D, and E. AG denotes our IMO agent graph detailed in Appendices F-N. Zero-shot o1 answers 1/9 problems correctly. The best method using o3-mini high answers 3/9 problems correctly, whereas A diverse set of 8 methods using o3-mini high answers correctly 7/9 (77.77\%) of the problems, with automatic verification. Similarly, the best method using o1 answers 3/9 problems correctly, whereas the diverse set of 8 methods using o1 answers correctly 6/9 (66.66\%) of the problems, with automatic verification.

Our approach proves the fifth combinatorics problem (Turbo the Snail) out of six problems in the 2024 IMO, tipping performance to a gold medal level as shown in Figure \ref{fig:imo_gold}. The knowledge cutoff date of the foundation models we use is before the 2024 IMO and before the release of the IMO 2023 shortlist, and we do not use Internet access. Our approach is strict, beginning with the problems in plain English as it is given to IMO contestants. Deepmind's AlphaProof and AlphaGeometry 2 solve four out of six problems in the 2024 IMO for 28 points which is at the level of a silver medal \cite{deepmind2024natureblog,deepmindsilverblog} given the formal problem in Lean \cite{deepmindsilverlean}. We do not give partial credit and consider the solution correct only if the proof is deemed correct and complete by an expert Mathematician with math Olympiad evaluation experience.

\begin{figure}[t!]
  \centering
  \includegraphics[width=1\linewidth]{imoresult_o3mh_o1.png}
  \caption{Ablation over problems, methods, and models. Correct answers (in green) for each Mathematical Olympiad problem (column), method (row), and model (top panel o3-mini high, bottom panel o1). Problems are from the 2024 International Mathematical Olympiad (IMO), 2024 USA Mathematical Olympiad (USAMO), and 2023 IMO ShortList (IMOSL). All problems are non-contaminated by the underlying models since their knowledge cutoff dates is after the release of the solutions. The bottom row shows which problems are answered correctly by any of the different methods and their answer automatically verified. Numbers inside cells indicate running times in seconds. AG denotes the IMO agent whose details are in Appendices F-N. Additional results and evaluations are in Appendices C-E.}
  \label{fig:imo}
\end{figure}

\begin{figure}[b!]
  \centering
  \includegraphics[width=1\linewidth]{rank_vs_score.png}
  \caption{2024 IMO contestant rank vs. total score. Our approach proves the fifth problem in combinatorics correctly with a score of 7/7 whereas the average human IMO participant score is 2.25/7 on this problem. This result tips performance to solving 5/6 problems correctly, with a rank of 5 and a score of 35/42.}
  \label{fig:imo_gold}
\end{figure}

\subsection{ARC}
We perform an extensive evaluation of 16 models and methods on 400 ARC evaluation puzzles as illustrated in Figures \ref{fig:arc-performance} and \ref{fig:arc-diversity-performance}, and described in Appendices P, Q, and R. Diversity is the maximum verifiable aggregation of 16 models and methods at inference time. We find that:
\begin{enumerate}
\item Without o3, diversity of 16 models and methods increases performance from the blue dotted line (53\%) to the orange dotted line (69.5\%).
\vspace{-5pt}
\item With o3, diversity of 16 models and methods increases performance from the purple dotted line (91.5\%) to the red dotted line (93.75\%).
\vspace{-5pt}
\item Diversity of 16 models and methods solves 80\% of the puzzles on which 948 humans collectively fail on. These 5/400 puzzles are between the dotted green line (98.8\%) and black line (100\%).
\vspace{-5pt}
\item Diversity of 16 models and methods solves 26.5\% of the puzzles on which o3 with high-compute fails on. These 34/400 puzzles are between the dotted purple line (91.5\%) and black line (100\%).
\end{enumerate}
Appendices P and Q show the detailed evaluation of each of the 16 models and methods on each of these puzzles, and Appendix R shows the detailed evaluation of each of the 16 models and methods on each of the 400 evaluation puzzles.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{arc_performance.png}
    \caption{ARC performance for different models and methods and human performance on evaluation dataset of 400 puzzles.}
    \label{fig:arc-performance}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{arc-diversity-zoomin.png}
    \caption{Zooming in on diversity performance of 16 models and methods on 400 ARC evalutaion puzzles.}
    \label{fig:arc-diversity-performance}
\end{figure}

\subsection{HLE}
We run our experiments on a random sample of 100 questions due to the costs of compute. Accuracy of different models and methods is shown in Table \ref{tab:hle_acc}. The accuracy of best-of-N rejection sampling with $N=3$ using o3-mini high on these 100 randomly sampled questions is 37\% over all categories and 33.3\% on Math questions, and using o1 is 21\% over all categories and 29.6\% on Math, as shown in Figures \ref{fig:hletable} and \ref{fig:hlebar}, and described in detail in Appendices T and U. The accuracy of best-of-N with $N=16$ on 10 random questions is 40\% using o1 and 50\% using o3-mini high. Questions, answers, and evaluation details appear in Appendix S.

\begin{table}[H]
\caption{Accuracy (\%) of different models and methods on the HLE dataset. OpenAI o3-mini (high) is not multi-modal and therefore evaluated on text only questions, and OpenAI Deep Research uses browsing and code.}
\label{tab:hle_acc}
\begin{center}
\scriptsize
\begin{tabular}{l c}
\toprule
\textbf{Model and Method} & \textbf{Accuracy (\%)} \\
\midrule
OpenAI o1               & 9.1  \\
DeepSeek-R1             & 9.4  \\
OpenAI o3-mini (medium) & 10.5 \\
OpenAI o3-mini (high)   & 13.0 \\
OpenAI Deep Research    & 26.6 \\
OpenAI o3-mini (high) and Self Consistency (N=5) & 18 \\
OpenAI o3-mini (high) and RTO & 18 \\
OpenAI o3-mini (high) and MoA (N=3) & 19 \\
OpenAI o3-mini (high) and LEAP & 23 \\
OpenAI o3-mini (high) and MCTS (N=2) & 28 \\
OpenAI o3-mini (high) and Best-of-N (N=3) & 37 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{hleresulto3.png}
  \includegraphics[width=1\linewidth]{hleresulto1.png}
  \caption{Accuracy on a random sample of 100 HLE questions by each method and question category, and over all categories, using OpenAI o3-mini high model (top) and o1 (bottom). Best-of-N (BoN) is with $N=3$, self-consistency (SC) is with $N=5$, and MCTS is with $N=2$ simulations. The number of questions in each category is shown on the y-axis and each method is shown on the x-axis. The number in the cells denote the percentage of correct answers by each method on each category (darker green colors denotes a higher percentage of correct answers).}
  \label{fig:hletable}
\end{figure}

\begin{figure}[b!]
  \centering
  \includegraphics[width=1\linewidth]{hleresultbar.png}
  \caption{Performance on a random sample of 100 HLE questions using Best-of-N with $N=3$, by question type over all categories or only Math questions using OpenAI o1 and o3-mini (high).}
  \label{fig:hlebar}
\end{figure}

We identify two problems with the HLE dataset, as shown in Figures \ref{fig:hletable} and \ref{fig:hlebar}:
\begin{enumerate}
\item There are many questions that are not very hard.
\item There are many multiple choice questions.
\end{enumerate}

\subsection{Limitations}

\paragraph{IMO.}
A correct solution consists of both a correct answer and a correct and complete proof. Simple frameworks using LLMs such as OptiLLM may correctly answer problems but fail to correctly prove them. Not all problems have answers, and there are problems that require only proofs. Formulating correct, complete and verifiable proofs is non-trivial. Appendix L provides examples of combinatorics problems that require finding an invariant or involve very high dimensional spaces that our approach does not handle. In general, proving upper bounds may be harder than proving lower bounds. For example, when proving a lower bound, we show that we can achieve a high score by simulation and deep reinforcement learning, which is relatively easy, whereas when proving an upper bound, we show that we cannot achieve a better score, which may be more difficult. Combinatorics problems may involve extremely large dimensions and solutions, where it is difficult to generalize from small to large examples by induction. Our use of meta-learning across multiple instances allows us to generalize. Combinatorics problems may be classified into different types of problems, such as enumerative combinatorics, which involves counting the number of ways patterns or structures are formed (for example, permutations or partitions); graph theory, which deals with combinatorial properties of graphs (such as paths, cycles, coloring, or flow); combinatorial optimization, where the goal is optimizing a combinatorial structure by criteria (for example TSP, knapsack, or scheduling problems); and others. We handle problems that may be modeled using a game with state, action space, and rewards. We would like to test our approach in real test-time conditions during the 2025 IMO.
 
\paragraph{HLE.} The main limitation for evaluating our approach for answering HLE questions is the cost of inference which is currently above a Dollar per question per method with $N=1$. Best-of-N rejection sampling multiplies this cost by $2N$ and is prohibitive for large $N$ on a large sample. We therefore perform HLE evaluation on a random sample of 100 questions.