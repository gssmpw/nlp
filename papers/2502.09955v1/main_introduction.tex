\section{Introduction}
Reasoning LLMs such as OpenAI o1 \cite{strawberry} and o3 \cite{o3mini}, as well as DeepSeek R1 \cite{guo2025deepseek}, have led to impressive performance in mathematics, coding, and problem solving. Despite this progress, a single large model or method may struggle with challenging tasks. To address this, diversity, of models and methods for inference, has emerged as a mechanism to increase performance by using complementary strengths.

We demonstrate the advantages of diverse inference on three representative and challenging benchmarks: 
\begin{itemize} 
\item \textbf{International Mathematical Olympiad \cite{imo} combinatorics problems:} We increase the accuracy from 33.3\% to 77.8\% correct answers.
\item \textbf{Abstraction and Reasoning Corpus (ARC) \cite{chollet2019measure}:} We solve 80\% of puzzles that 948 humans collectively could not solve, and 26.5\% of puzzles that o3 high compute could not solve. 
\item \textbf{Humanity’s Last Exam (HLE) \cite{phan2025hle}:} We increase accuracy from 8\% to 37\% on this set of questions across mathematics, humanities, social sciences, and others.
\end{itemize}

Three key methodological contributions drive these results:
\begin{enumerate}
\item \textbf{Diverse inference.} We aggregate multiple models, methods, and agents at test time rather than relying on a single model or method. Any single correct solution is validated automatically for the verifiable tasks of IMO combinatorics and ARC puzzles. Specifically: 
\begin{itemize} 
	\item IMO: Using eight different methods (LEAP, Z3, RTO, BoN, SC, MoA, MCTS, PV) significantly increases accuracy. We autoformalize English into Lean, enabling perfect verification. 
\item ARC: Synthesized code solutions are verified on training examples as unit tests. 
\item HLE: Using best-of-N as an imperfect verifer, increases the solve rate with increased samples. 
\end{itemize}

\item \textbf{Test-time simulations and reinforcement learning.} We generate additional problem-specific information at inference time:
\begin{itemize} 
\item IMO: Transform combinatorics problems into interactive game environments and apply combinatorial search or deep reinforcement learning to derive partial results or bounds. 
\item ARC: Exploring puzzle transformations by synthesized code prunes incorrect solutions and refines candidate solutions. 
\end{itemize}
Searching using trained verifiers often outperforms supervised fine-tuning given the same dataset \cite{cobbe2021training}, which motivates reinforcement learning fine-tuning. We run simulations and reinforcement learning at test time to generate additional data that allows us to correctly prove a 2024 IMO combinatorics problem and solve difficult ARC puzzles.

\item \textbf{Meta-learning of agent graphs.}
We use LLMs and tools to trace pipeline runs, generate A/B tests of hyper-parameters, prompts, code variations, and data, and adaptively modify the agent graph.

\end{enumerate}


\paragraph{From mixture of experts to diverse models and methods.}
Most recent language models use a mixture of experts \cite{jiang2024mixtral}, where multiple experts are trained to specialize in different aspects of the input space. A gating mechanism learns to select or weigh the experts based on input. The diversity in expertise allows the model to use a broad range of problem-solving strategies, and distribution among diverse experts allows the model to handle variations better. Large-scale transformers that leverage diversity \cite{lepikhin2020gshard,fedus2022switch} increase efficiency and accuracy, otherwise difficult to achieve with a single monolithic model. In this work, we use diverse models and methods to increase accuracy.

% The limitation of diversity is the weakness of the verifier
\paragraph{Perfect and imperfect verifiers.}
An imperfect verifier generates false positives, which are wrong solutions that pass the verifier. These false positives impose an upper bound on accuracy despite the increase in sampling or inference time compute \cite{stroebl2024inference}. In this work, we use perfect verifiers for the IMO and ARC and an imperfect verifier for the HLE. Specifically, for the IMO, we use Lean as a perfect verifier and generate additional ground truth samples by simulation. For the ARC we use code execution on the training examples as perfect verifiers. For the HLE we use best-of-N sampling as an imperfect verifier.


\paragraph{Empirical scaling laws.}
The two most common empirical scaling laws for foundation model performance are:
\begin{enumerate}
\item The relationship between model size, data size, and loss, i.e. language models with more parameters, training data, and training time perform better \cite{brown2020language}, quantified by OpenAI's scaling law \cite{kaplan2020scaling} and the Chinchilla scaling law \cite{hoffmann2022training}. Scaling laws extend to fine-tuning, describing the relationship between model performance and the number of fine tuning parameters and fine-tuning data size \cite{zhang2024scaling}, and extend to different architectures and downstream tasks \cite{caballero2022broken}. %The relationship between model performance and the number of model parameters, pre-training data size, and compute . 
\item The relationship between model performance and test-time compute. The tradeoff between training time and test time compute has been demonstrated early on for board games \cite{jones2021scaling}, showing that increasing either one leads to better performance. Test time compute scaling \cite{sardana2023beyond} has recently been demonstrated again by DeepMind on coding \cite{alphacode2techreport} and OpenAI o1 \cite{strawberry} and o3-mini \cite{o3mini} for reasoning LLMs.
\end{enumerate}
We identify a third empirical scaling law: the relationship between the number of diverse models and methods and the performance on verifiable problems.

% compile supplementary_material.tex
\paragraph{Additional contributions in methodology and evaluation.} Beyond these core contributions and results, we provide methodological contributions and extensive evaluations on these three challenging datasets:
\begin{itemize}
\item IMO, ARC, and HLE ablation experiments and extensive evaluations of diverse models and methods in Appendices C, D, E, R, and T.
\item IMO visual game representations in Appendix G. Interactive game solvers can serve as tutors, offering visual explanations and validating students’ solutions, or providing personalized practice instances, increasing engagement and understanding in Mathematics education.
\item IMO autoformalization of Theorems from English to Lean in Appendix J, and formal proof verification by cyclic back-translation. Autoformalization and proof validation ensure reliable results.
\item IMO data for in-context learning for solving problems in Appendix N.
\item ARC evaluations on o3 high-compute failure cases in Appendix P and on failure cases of a collective of 948 humans in Appendix Q.
\item IMO and ARC automatic verification of results and programs.
\item IMO and ARC agent graphs in Appendix I and O, showing how to combine multi-step prompting, code synthesis, test time simulation and deep reinforcement learning, autoformalization, and verification into a pipeline.
\item HLE performance of best-of-N for an increasing number of samples in Appendix S.
\item HLE evaluation by methods, question categories, and questions types in Appendix U.
\end{itemize}

Next, is background on the three challenging benchmarks: 

\paragraph{International Mathematical Olympiad (IMO).}
An annual worldwide mathematics competition for high school students \cite{imo} that brings together teams of students from over 100 countries and advances mathematical education. The IMO consists of two consecutive days of competition, where students solve six problems, three per day. The problems are from different areas of mathematics, including algebra, geometry, number theory, and combinatorics. Each problem has a value of seven points, with a maximum total score of 42, and all answers are in the form of proofs \cite{imo2024regulations}. Medals are awarded based on individual performance, with top scorers receiving gold, silver, and bronze medals. Special prizes are given for solutions that demonstrate exceptional elegance or insight. The problems are designed to be challenging, requiring creative problem-solving skills, mathematical understanding, and the ability to connect concepts from different mathematical areas.


\paragraph{Abstraction and Reasoning Corpus (ARC).}
A benchmark introduced \cite{chollet2019measure} to measure the visual reasoning aspect of artificial general intelligence by a set of puzzles with patterns on visual grids. Given a small set of training pairs, the goal is to infer the transformation, relationship, or function between them and apply it to a test example. The average human performance on ARC is between 73.3\% and 77.2\% correct, and it takes 948 humans to collectively solve 98.8\% of the evaluation set puzzles correctly \cite{legris2024h}.

\paragraph{Humanity's Last Exam (HLE).}
Curating and releasing 3,000 questions across dozens of subjects, the HLE \cite{phan2025hle} includes questions on mathematics, humanities, and natural sciences, developed by experts worldwide and consists of multiple-choice and short-answer questions. The breakdown of the question topics is math 42\%, physics 11\%, biology/medicine 11\%, computer science and AI 9\%, humanities and social sciences 8\%, chemistry 6\%, engineering 5\%, other 8\%. Zero-shot o1 accuracy on the entire HLE is 9\%.


Additional related work appears in Appendix Z. Next, we describe our methodologies and key results.
