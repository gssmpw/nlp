\section{Additional Related Work}
\label{appendix:Z}

\paragraph{AI for Mathematics milestones.}
Noteworthy milestones in AI for Mathematics \cite{miaoartificial} include DeepMind's silver medal level solution of the 2024 IMO \cite{deepmindsilverblog} using AlphaProof and gold medal level geometry problems using AlphaGeometry2 \cite{chervonyi2025gold,trinh2024alphageometry,deepmindsilverlean}. Extreme combinatorics problems have been approximated using genetic algorithms and program search by LLMs \cite{romera2024mathematical}. Faster methods for performing the core operations in Computer Science including sorting \cite{mankowitz2023faster} and matrix multiplication \cite{fawzi2022discovering} have been discovered by deep reinforcement learning. Recently, OpenAI released o1 \cite{strawberry} and o3 models that reason and have mathematical capabilities on par with an average graduate student \cite{tao2024gpt}. 

\paragraph{Theorem proving.}
The three most popular formal proof languages are Lean 4 \cite{moura2021lean}, Coq \cite{coq2024}, and Isabelle \cite{nipkow2002isabelle}. Existing approaches may be classified into informal and formal Theorem proving. 
The tasks of autoformalization, premise selection, proof step generation, and proof search each have their evaluation metrics \cite{li2024survey}. Tactics for proving may use foundation models, and then search for determining which goal to work on next based on best-first search or MCTS \cite{lamont2024bait}, represented by a sequence or graph. Previously, machine learning guided the intuition of Mathematicians and proposed conjectures \cite{davies2021advancing}. An iterative and interactive process performs this in a closed loop in which a Mathematician starts with a hypothesis, the AI generates data, trains a supervised model, and finds patterns. The Mathematician proposes a conjecture candidate and finally proves a theorem. AI has been used extensively for Theorem proving \cite{li2024survey}, in interactive and automated provers \cite{polu2020generative,polu2022formal,yang2024leandojo,song2024towards,lin2024fvel,wang2024proving}. Examples of proof search include GPT-f \cite{polu2020generative} searching a proof tree, proof search by Monte Carlo Tree Search (MCTS) \cite{wu2020int}, learning which paths that lead to correct proofs as a hypertree  \cite{lample2022hypertree}, AlphaMath \cite{chen2024alphamath} using MCTS with LLMs, and DeepSeek Prover \cite{xin2024deepseek} optimizing training with MCTS at test-time \cite{xin2024deepseek}. Curriculum learning has been applied in LeanAgent \cite{kumarappan2024leanagent} to learn proofs from easy to difficult. An algebraic inequality proving system \cite{wei2024proving} has been developed to generate many theorems, using a symbolic algebraic inequality prover guided by a value network, solving 10/20 IMO algebraic inequality problems. Three open Theorem provers are DeepSeek Prover 1.5 \cite{xin2024deepseek}, InternLM \cite{wu2024internlm2}, TheoremLlama \cite{wang2024theoremllama}, and a closed Theorem prover is AlphaProof \cite{deepmindsilverblog}.

\paragraph{Recent benchmarks.}
Existing benchmarks include miniF2F \cite{zheng2021minif2f}, which consists of 244 problems from mathematical Olympiads AMC, AIME, and IMO. Due to rapid progress in AI for Mathematics, benchmarks saturated, and more difficult benchmarks such as the FrontierMath \cite{glazer2024frontiermath} were introduced. A benchmark of theorem-provers on 640 formalized problems \cite{tsoukalas2024putnambench} from the William Lowell Putnam Mathematical Competition, which is the premier college-level mathematics competition in the United States, covers topics including analysis and abstract algebra that are beyond the IMO.

\paragraph{Proof datasets.} 
Initially, datasets of proofs have been relatively small. For example, Lean's mathlib \cite{van2020maintaining} consists of 140K proofs, and Isabelle has 250k proofs. Isarstep is a benchmark dataset \cite{li2020isarstep} which includes the task of filling in a missing intermediate proposition within proofs using hierarchical transformers. CoqGym \cite{yang2019learning} is a large dataset and training environment for Theorem proving with 71k human-written proofs. The  CoqGym environment is used for training and evaluating automated and interactive Theorem provers. The system generates tactics as programs by composing abstract syntax trees. The Mustard dataset \cite{huang2024mustard} has over 5k examples generated by prompting an LLM to generate problems based on mathematical concepts followed by generating natural language and formal proofs and theorems. A Lean prover validates the formal proofs to ensure correctness. The Fevler dataset \cite{lin2024fvel} consists of 758 theorems, 29k Lemmas, and 200k proof steps, and is used to enhance formal proof verification, where proof steps are iteratively applied to form a formal proof.

\paragraph{Autoformalization.}
Autoformalization involves translating natural language problens and solutions into formal proofs. Early on, machine translation was used to convert mathematical statements in LaTeX to formal statements using an encoder-decoder architecture \cite{wang2020exploration}. LLMs have been used to autoformalize mathematical competition questions into Isabelle without training on aligned data \cite{wu2022autoformalization}. Process-driven autoformalization (PDA) \cite{lu2024process} in Lean 4 leverages compiler feedback to enhance performance, providing a dataset, FORML4, for evaluation. A method that scores and selects among multiple generated candidates using symbolic equivalence and semantic consistency \cite{li2024autoformalize} further improves accuracy. Combining most similar retrieval augmented generation (MS-RAG), denoising steps, and autocorrection with syntax error feedback (AutoSEF) \cite{zhang2024consistent} yields consistent and reliable formalizations across models. 

\paragraph{Explainable reinforcement learning.}
Explainable reinforcement learning aims to explain the visual outputs of deep reinforcement learning agents, for example, by learning the structured state representations of agent game-play and extracting interpretable symbolic policies \cite{luoend}. A foundation model generates Textual explanations for these learned policies and decisions.

\paragraph{Test-time methods.}
Different problems have varying levels of difficulty and complexity. Single calls to a vanilla LLM use the same amount of compute. Therefore, solving problems with varying difficulty may require varying amounts of computation at inference time. There is a trade-off between LLM inference computational cost and accuracy. Solve rates of coding problems increase with the amount of LLM samples generated for a problem \cite{alphacode2techreport}. Simple methods for aggregating the samples include consensus, for example, by self-consistency \cite{wang2022self}. Accuracy on math problems increases with the amount of compute at inference time, for example, by ensembling \cite{jiang2023llm}, the mixture of agents \cite{wang2024mixture}, repeated sampling and aggregation \cite{brown2024large,chen2024more}, and models trained using reinforcement learning and chain of thought, which is then applied at inference time \cite{strawberry}. Dialogue and debate between LLMs with different personas have also been shown to improve mathematical reasoning \cite{du2023improvingfactualityreasoninglanguage}, which, in effect, increases the amount of computation used for inference. Problems given during test-time for inference may be out of distribution. Therefore, computing after the test example is known to be beneficial, especially when handling out-of-distribution examples. Test-time training has been used early on for improving image classification \cite{sun2020test}. Frameworks such as OptiLLM \cite{optillm} implement multiple test time methods for convenient comparison. 



\paragraph{Abstraction and Reasoning Corpus (ARC) benchmark}
In 2023, it was claimed that AI, and in particular LLMs, were incapable of succeeding on this task with 8\% accuracy \cite{biever2023chatgpt}; however, this criticism was quickly proven wrong, with a 33.1\% accuracy on MiniARC \cite{qiu2023phenomenal} using LLMs, and 53\% \cite{li2024combining} and 61.9\% \cite{akyurek2024surprising} accuracy on ARC until reaching 91.25\% using the latest models with high compute which is 15\% more accurate than the human average. These approaches use LLMs, train on example pairs by leave-one-out, synthesize data by transformations, fine-tune LLMs, synthesize programs using a language model, execute these programs, generate hypotheses, and verify their correctness. Improvements of large reasoning models in program synthesis \cite{el2025competitive} improve performance on ARC as well. The combined effort of 948 humans on the ARC evaluation dataset yields an accuracy of 98.8\% \cite{legris2024h} on the 400 evaluation puzzles which motivates high compute and diversity of models and methods.

\paragraph{Open and closed reasoning LLMs and Operator}
OpenAI released the o1 reasoning LLM \footnote{\url{https://openai.com/index/openai-o1-system-card}} with closed weights and a closed source Operator browser agents (that blocks financial instruments). DeepSeek released the R1 reasoning LLM \footnote{\url{https://github.com/deepseek-ai/DeepSeek-R1}} with comparable performance to o1 with open weights. Open source browser use tools \footnote{\url{https://github.com/browser-use/browser-use}} are available online without limitations.
