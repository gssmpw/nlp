\section{Meta Learning Agent Graph Experiments}
\label{appendix:W}

Let $x$ be a problem, and \(\pi_{\theta}(y \mid x)\) the probability distribution over responses \(y\) generated by a model with parameters \(\theta\). This is any one of the $K$ models or methods. 
We begin with a human-generated agent graph or pipeline $f$, which provides a starting state for a structured approach for solving the problem $x$, returning an answer $y = f(x)$. 

\paragraph{Agent-graph representation using Rivet.}
We represent the pipeline $f$ as an agent graph using the Rivet framework \footnote{\url{https://rivet.ironcladapp.com}}. This agent graph consists of modular components that act on the input $x$ in a sequential or parallel manner, resulting in a final output $y$. Each run of the agent graph produces a trace $z = \text{Trace}(f, x)$ which is the internal trace, or log, of the agent's execution steps \footnote{\url{https://gentrace.ai}}. When the graph is executed on input $x$, we obtain both the response and trace $(y,z)=\bigl(f(x),\,\text{Trace}(f, x)\bigr)$.

\paragraph{Meta-learning to improve the pipeline.}
After running the agent graph on the problem $x$, we collect the tuple 
$\bigl(f,\,x,\,z,\,y\bigr)$, of the graph representation $f$, problem $x$, execution trace $z$, and response $y$. We use this to meta-learn an improved agent-graph pipeline $f'$. We define a meta-learning operator $g$ such that $f' = g\bigl(f,\,y,\,z,\,x\bigr)$. The meta-learner $g$ takes as input the graph representation $f$, observed trace $z$, problem $x$, and the final response $y$ and outputs a revised graph $f'$ with adjustments or modifications to nodes, sub-agent selection or ordering, or modified data flow.

\paragraph{Integration with model policies.}
The pipeline $f$ may query a model distribution \(\pi_\theta(y \mid x)\) at various steps. For example, modules (or sub-agents) in $f$ typically call a model to propose partial solutions or substeps. Additionally, the final output $y$ itself may be fused with, or determined by, the model's predictions:
\begin{equation}
y \;=\; 
\begin{cases}
f(x), & \text{(pure agent-graph pipeline)}, \\
\arg\max_{y'} \pi_\theta(y' \mid x), & \text{(pure model-based policy)}, \\
\text{Hybrid}(f(x),\, \pi_\theta(y \mid x)), & \text{(agent-model combination)},
\end{cases}
\end{equation}
where $\text{Hybrid}$ denotes a joint decision that takes into account both the deterministic pipeline's recommendation and the stochastic model predictions.

\paragraph{Iterative refinement loop.}
Once the meta-learner $g$ updates the pipeline to $f'$, we may iteratively repeat the process on problem instances $\{x_i\}$, to produce a sequence of pipelines $f^{(t)}$. This allows the agent-graph pipeline to evolve and improve over time, guided by collected traces and outputs.



\begin{table}[h]
\caption{Comparisons of different levels of meta-learning on inference time agents.}
\label{tab:meta}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Graph & Entity & Operation \\
\midrule
Fixed & hyper-parameters & search \\
Fixed & prompts & add/remove/edit \\
Fixed & data & add/remove \\
Fixed & code & add/remove/edit \\
Dynamic & edges & add/remove \\
Dynamic & nodes & add/remove \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}
