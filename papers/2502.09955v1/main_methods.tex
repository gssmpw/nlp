\section{Methods}

\subsection{Reasoning LLMs}

A foundation model $\pi$ with pre-trained parameters $\theta$ defines a conditional distribution:
\begin{equation}
\label{eq:fm_joint_generation}
p_{\theta}(y \mid x),   
\end{equation}
where $x$ is a prompt and $y$ is a response. A reasoning model is trained to generate a (hidden) rationale also known as chain-of-thought (CoT) $z$, so that the joint generation is given by:
\begin{equation}
\label{eq:reasoning_joint_generation}
    p_{\theta}(z, y \mid x) \;=\; p_{\theta}(z \mid x)\, p_{\theta}(y \mid z, x).
\end{equation}

Model training consists of two phases: (i) Supervised fine-tuning (SFT): from $\pi$ to $\pi_{\text{SFT}}$; and (ii) Reinforcement learning (RL): from $\pi_{\text{SFT}}$ to $\pi_{\text{RL}}$.


\paragraph{Supervised fine-tuning (SFT).}
Samples are generated using $\pi_{\theta}$ in Eq. \ref{eq:fm_joint_generation} and stored in a dataset $\mathcal{D} = \{ (x^i, y^i) \}_{i=1,\ldots,n}$. A supervised fine-tuning loss is derived by taking the negative log likelihood of Eq. \ref{eq:fm_joint_generation} on the dataset:
\begin{equation}
\label{eq:sft_loss}
    \mathcal{L}(\theta) 
    \;=\; - \sum_{(x^i, y^i) \,\in\, \mathcal{D}}
      \log p_\theta\bigl(y^i \mid x^i \bigr).
\end{equation}

Similarly, for a reasoning model, samples are generated using $\pi_{\theta}$ in Eq. \ref{eq:reasoning_joint_generation} and stored in a dataset $\mathcal{D} = \{ (x^i, z^i, y^i) \}_{i=1,\ldots,n}$. A supervised fine-tuning loss is derived by taking the negative log likelihood of Eq. \ref{eq:reasoning_joint_generation} on the dataset:
\begin{equation}
\label{eq:reasoning_sft_loss}
    \mathcal{L}(\theta) 
    \;=\; - \sum_{(x^i, z^i, y^i) \,\in\, \mathcal{D}}
      \Bigl[\,\log p_\theta\bigl(z^i \mid x^i\bigr) \;+\; \log p_\theta\bigl(y^i \mid x^i, z^i\bigr)\Bigr].
\end{equation}

\paragraph{Reinforcement learning.} 
For tasks such as solving math problems or generating code, we define a reward function $R(x, y)$ that is checked automatically, by verifying an answer or proof or by running unit tests. We then optimize:
\[
   \operatorname*{maximum}_{\theta}
   \mathbb{E}_{x \sim \mathcal{D},\,y \sim \pi_\theta}\bigl[R(x, y)\bigr].
\]
This is a classical RL objective without the need for a learned preference model. 

More generally, given a foundation model we define a reward: 
\begin{equation}
r(x,\hat{y}) = f\bigl(\pi_{\mathrm{RM}}(x,\hat{y})\bigr),
\end{equation}
where $\hat{y}$ is the resulting output, and $f$ is a function measuring the quality of that output result. For example, using policy gradient, we update $\theta$ by:
\begin{equation}
\nabla_{\theta}\,\mathcal{L}_{\mathrm{RL}} 
= -\,\mathbb{E}_{\hat{y}\,\sim\,\pi_\theta(\cdot\mid x)}
\Bigl[
   r\bigl(x,\hat{y}\bigr)\,\nabla_{\theta}\,
   \log \pi_\theta\bigl(\hat{y}\mid x\bigr)
\Bigr].
\end{equation}

For a reasoning model, let $\hat{z}$ be a sampled rationale and define a reward \cite{zelikman2024quiet}: 
\begin{equation}
r(x,\hat{z},\hat{y}) \;=\; f\bigl(\pi_{\mathrm{RM}}(x,\hat{z},\hat{y})\bigr),
\end{equation}
where $f$ is a function quantifying the quality of the rationale, for example the log-likelihood improvement on future tokens as a reward, or 
correctness on a question answering task. For a reasoning model, plugging in the logarithm of Eq. \ref{eq:reasoning_joint_generation}:
\begin{equation}
\log p_\theta(\hat{z},\hat{y}\!\mid\! x)
=
\log p_\theta(\hat{z}\!\mid\!x) + \log p_\theta(\hat{y}\mid x,\hat{z}), 
\end{equation}

yields the gradient:
\begin{equation}
\begin{aligned}
\nabla_{\theta}\,\mathcal{L}_{\mathrm{RL}} 
&= -\,\mathbb{E}_{\hat{z}, \hat{y}\,\sim\,\pi_\theta(\cdot \mid x)} \Bigl[
   r\bigl(x,\hat{z},\hat{y}\bigr)\,\nabla_{\theta}\,
   \log \pi_\theta(\hat{z}\!\mid x) \\
&\qquad\qquad
   + \log \pi_\theta(\hat{y}\!\mid x,\hat{z})
\Bigr].
\end{aligned}
\end{equation}

\subsection{Diverse Models and Methods}

We ablate multiple models and methods \cite{optillm} at test time on the IMO, ARC, and HLE. The models are described in Appendix R. Each method is described next:

\begin{itemize}

\item {\bf Zero-shot}: The problem, as-is, given to the LLM.

\item {\bf Best of $N$ sampling}: Generates $n$ candidate responses $Y = \{ y^1, y^2, \dots, y^n \}, y^j \sim p(y \mid x)$ and selects the best one according to a criterion $y^* = \arg\max_{y^j \in Y} C(y^j)$. Given a verifier and a chain of thought, we perform rejection sampling, by sampling different chains of thought $z^{n} \sim p(z \mid x)$, their responses $y^{n} \sim p(y \mid x, z^{n})$ and keeping those responses $y^{n}$ that are verified.

\item {\bf MCTS} \cite{xie2024monte}: Typically used to explore the solution space by constructing a search tree. The state transition is $s_{t+1} = T(s_t, a_t)$, a node value is estimated by $V(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} R_i$, where $N(s)$ is the number of times node $s$ has been visited and $R_i$ is the reward from simulation $i$. In our context, we perform rejection sampling from an intermediate step in the chain of thought by Monte-Carlo roll outs.

\item {\bf Self-consistency} \cite{wang2022self}: Instead of relying on a single response, self-consistency evaluates multiple outputs $y^{n}$ for the same input $x$ and selects the most common or majority vote response $y^* = \text{Majority Vote}(\{ y^j \})$. This approach enhances the reliability and accuracy of predictions, reducing variability and improving the overall quality of the model's output, however often saturates given sufficient samples.

\item {\bf Mixture of agents} \cite{wang2024mixture}: Combines outputs from multiple agents or models, $p(y \mid x) = \sum_{k} \alpha_k p_k(y \mid x)$, where $p_k(y \mid x)$ is the output distribution of the $k$-th agent, and $\alpha_k$ is a weighting coefficient s.t. $\sum_{k} \alpha_k = 1$.

\item {\bf Round trip optimization (RTO)} \cite{allamanis2024unsupervised}: Optimizes responses through a round-trip process by asking an LLM to first perform an action and then perform the reverse action, checking that the round-trip is successful.

\item {\bf Z3 Theorem prover} \cite{de2008z3}: Assists in verifying logical statements and constructing formal proofs, improving reasoning accuracy. It represents formal proofs as sequences of logical deductions, axioms $\{\phi_0\}$, inference rules $\phi_{k+1} = f(\phi_k)$, and proof sequences $\pi = \langle \phi_0, \phi_1, \dots, \phi_n \rangle$, and the goal is to prove a theorem $\phi_n$.

\item {\bf Prover-verifier (PV)} \cite{kirchner2024prover}: An interactive game between a prover (P) and a verifier (V) at test time enhances the legibility and verifiability of model outputs. The verifier predicts the correctness of solutions, accepting correct ones from a helpful prover and potentially being misled by an adversarial prover offering incorrect solutions. The game unfolds over several rounds for claims $x \in L$, where $L$ is a set of valid outputs. At each round $i$, the prover sends a message $m_i$ representing a proof step. The verifier processes these messages using a decision function $ \mathcal{D}_V: (m_1, \dots, m_i) \rightarrow \{0,1\} $, which outputs $1$ if the sequence is a valid proof and $0$ otherwise, iteratively improving the result.

\item {\bf R$^\star$} \cite{likhachev2008r}: Systematically explores the solution space and prunes suboptimal paths, balancing exploration and exploitation to find optimal solutions.

\item {\bf Plan search (PS)} \cite{de2008z3}: Involves exploring candidate plans or sequences of actions to find the most effective solution. The model evaluates different plans to identify the one that best achieves a desired goal.

\item {\bf Learning task-specific principles (LEAP)} \cite{zhang2024context}: Learns principles $\Theta$ from few-shot examples to improve problem-solving, where $\Theta = f(\{(x_i, y_i)\}_{i=1}^K)$, using $\Theta$ to guide a model $p(y \mid x, \Theta)$. 
\end{itemize}



\subsection{Aggregating Diverse Models and Methods}
\label{subsec:aggregation_strategy}

We aggregate the results of diverse models and methods whose solutions may be perfectly verified as correct by a maximum. Let $\mathcal{T} = \{t_1, t_2, \ldots, t_N\}$ be the set of $N$ IMO problems or ARC puzzles and $K$ the number of models $\mathcal{M} = \{\mathcal{M}_1, \mathcal{M}_2, \ldots, \mathcal{M}_K\}$,
where each \(\mathcal{M}_k\) attempts to solve each $t_i \in \mathcal{T}$. 
The indicator is defined by $
\mathds{1}\bigl(\mathcal{M}_k \text{ solves } t_i \bigr) \;=\;
\begin{cases}
1, & \text{if } \mathcal{M}_k \text{ correctly solves } t_i, \\
0, & \text{otherwise}.
\end{cases}
$
Since we can verify the correctness of each individual solution, for each problem $t_i$, there exists a ground truth validation mechanism indicating whether $\mathcal{M}_k$'s proposed solution is correct. We combine the outputs of all models by taking the logical maximum, i.e., logical OR, over their correctness indicators:
$\mathds{1}\bigl(\text{any model solves } t_i \bigr)
\;=\;
\max_{k \in \{1,\ldots,K\}}
\mathds{1}\bigl(\mathcal{M}_k \text{ solves } t_i \bigr)$.
Problem $t_i$ is considered solved if and only if at least one method in \(\mathcal{M}\) solves it. We define the success rate, or accuracy, of the aggregated system across the set $\mathcal{T}$ of $N$ problems as: 
$\frac{1}{N} \sum_{i=1}^N \max_{k \in \{1,\ldots,K\}}
\mathds{1}\bigl(\mathcal{M}_k \text{ solves } t_i \bigr)$. Since a problem is counted as solved if any one of the $K$ models solves it, this aggregation is the best-case scenario. If all models make different systematic errors, this approach substantially improves coverage of solvable problems relative to individual models. If any model's solution is correct for a particular problem, that problem is marked as solved in the aggregated result, giving the maximum performance across diverse models.

\subsection{Test-Time Simulations and Reinforcement Learning}


\paragraph{IMO}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\columnwidth]{imo-f0.png}
    \caption{IMO agent architecture.}
    \label{imo-arch-f0}
\end{figure}

Figure \ref{imo-arch-f0} is a high-level architecture of our approach for solving IMO combinatorics problems. See Appendices F-I for details. Our pipeline consists of three components: encoding, simulation and deep reinforcement learning, and decoding. During the encoding phase, we find the answer by formulating the problem into a state space, action space, and rewards $(S, A, R)$. Then, we prompt an LLM to transform the problem into a game environment. We represent the problem as Python code in Gymnasium with an agent and policy. We use simulation and deep reinforcement learning to find an optimal policy. We repeat this process, generating multiple games per problem with different dimensions, generating data and videos of multiple episodes per game. In the decoding phase, we extract data and frames and augment them by transformations. We use LLMs to compose textual representations of each sequence's images and policy explanations in the form of descriptions. Finally, we use this information, along with the problem statement, answer, books and guides as described in Appendices M and N, to auto-formalize a proof by in-context learning. We curated a dataset of all previous IMO ShortList combinatorics problems between 2006-2023 and used a subset for in-context learning of autoformalization. The result is automatically verified in the Lean environment, as shown in Appendix J, and refined to generate a complete and correct proof as shown in Appendix K. Our approach handles combinatorics problems that may be formulated as a game with a state space, action space, and rewards.

\paragraph{Autoformalization in \textsf{Lean}.}
In addition to answering and solving in English, we perform cyclic auto-formalization using in-context learning. Given a problem we translate it into formal Lean by in-context example pairs from previous years IMO problems and their corresponding Lean theorems. We auto-verify the Lean code, remove comments, translate the Lean code back to English, and have the LLM compare the original and back-translated problems, verifying that they are mathematically equivalent. Appendix J shows autoformalization examples. The significance of a robust and reliable back-and-forth translation between English and Lean is that it allows for automatic verification of problem statement and proofs. We also verify proofs by an expert Mathematician. Formally, we convert $X_{\mathrm{EN}}$ into a \textsf{Lean} formal proof using few-shot learning. Specifically, let $\Phi_{E \to L}\,\colon\,
\{\text{English text}\}\;\to\;\{\textsf{Lean code}\}$ be a translation function by \(\mathcal{M}\) (with in-context examples of English--\textsf{Lean} pairs). We generate $X_{\mathrm{Lean}} 
\;=\; \Phi_{E \to L}\bigl(X_{\mathrm{EN}}\bigr),$
which is then compiled in \textsf{Lean}. Let \(\mathrm{Compile}(X_{\mathrm{Lean}})\) be a boolean function indicating if the proof compiles successfully in the \textsf{Lean} environment. To validate that the final \textsf{Lean} theorem matches the original solution, we remove comments or annotations from $X_{\mathrm{Lean}}$ to avoid using the original English text that may appear as documentation and get $X_{\mathrm{Lean}}^{\prime}$. We then apply the inverse translation function $\Phi_{L \to E}\,\colon\,
\{\textsf{Lean code}\}\;\to\;\{\text{English text}\}$ to obtain a back-translated theorem $X_{\mathrm{EN}}^\star \;=\; \Phi_{L \to E}\bigl(X_{\mathrm{Lean}}^{\prime}\bigr).$ Finally, we compare \(X_{\mathrm{EN}}^\star\) to \(X_{\mathrm{EN}}\) to confirm that they are mathematically equivalent using an LLM. Formally, we require:
$\mathrm{Equivalent}\bigl(X_{\mathrm{EN}},\,X_{\mathrm{EN}}^\star\bigr) 
\;=\; \text{true},
$
where \(\mathrm{Equivalent}(\cdot,\cdot)\) is a function that verifies the theorems, definitions, and logical inferences in both texts align. If the equivalence holds, we have a Mathematician evaluate the theorem in Lean and English, to check if pipeline successfully generated and verified the answer or proof. Our approach is able to prove the 2024 IMO combinatorics problems no previous model or method was able to solve by itself or using existing agentic frameworks. Why does our approach work? One reason is that it adds new and truthful synthetic data with a perfect verifier. 

\subsection{Meta Learning}
We experiment with meta-learning using LLMs to modify agent graph hyper-parameters, prompts and code, and the agent graph topology, adding or removing nodes and edges. The input is an agent graph, and the output are traces of runs on the graph variants, described in Appendices I, O, and V. Our implementation is based on Gentrace \cite{gentrace} and LLMs.
We representing the pipelines (agent graphs) in a structured format. Execute them and log a detailed trace of intermediate steps. We use an LLM to propose pipeline revisions based on the pipeline representation, trace, and result, and an LLM to correct the revised pipeline.

\subsection{HLE}
While math and coding have automatic 0/1 verifiers, other problems, such as many HLE questions, do not. Therefore, we cannot aggregate answers to non-math and non-coding questions by a maximum. In practice, we find that best-of-N rejection sampling with large N works well on the HLE questions. We compute the consensus among answers of different models or methods as the average agreement between them $c = \frac{\sum_{i=1}^{n} \mathds{1}(y_{k} = y)}{n}$ and the diversity as $1 - c$.

\subsection{Avoiding Data Contamination}
We use best practices to avoid data contamination when evaluating closed and open-weight foundation models. The knowledge cutoff date of the models is before the availability of the evaluated problems, models do not have Internet access and are used with fresh API calls so that solutions are not inadvertently reused from chat memory, and the evaluation does not leak information about solutions. We test OpenAI models using OptiLLM \cite{optillm}, which consists of multiple methods, prompts, and default parameters that are available online. We test closed and open-weight models. IMOs 2020-2023 were before OpenAI's models were trained and therefore we cannot evaluate them or build our mapping based on these IMO's without contamination. The IMO shortlist problems and solutions are released after the following year's IMO, so 2023 IMO shortlist problems and solutions are released after July 2024, which is after the cutoff dates of the LLMs and may be safely used for testing, except for problem 6 which was selected for IMO 2024 and is therefore excluded. We go beyond problem-solving by generating new problems and solving them, and verifying that the answers and proofs are correct and complete. 


\subsection{Generating New IMO Problems and Solutions}
OpenAI Deep Research \cite{deepresearch} has advanced reasoning capabilities. However it has Internet access, including access to existing IMO solutions, and therefore it is not used to solve existing problems or synthesize data used for solving existing problems. However, we use Deep Research to generate new problems for future use, and in addition to previous IMO problems, these generated problems will serve as part of our training data toward the 2025 IMO. Appendix Y illustrates our approach for generating new problems and their solutions for training toward future IMO's.

\subsection{IMO Human Evaluation}
Our IMO answers, their formal theorems in Lean, and proofs are evaluated by an expert Mathematician with math Olympiad evaluation experience. The problems, answers, and solutions appear in Appendix B along with the official IMO problems and solutions as released by the IMO committee \cite{imo2024problems_and_solutions}.