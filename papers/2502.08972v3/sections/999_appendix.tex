\section*{Appendix}
\label{sec:appendix}




\section{Evaluation Dataset Details}
\label{appdx:dataset_details}

\subsection{Sampling}
\label{appdx:sampling}
We rely on sampling to alleviate issues of having a small test set per author. 
For each of the three test prompts, we sample five outputs per method. 
We pair outputs for the same prompts to create 75 comparison pairs ($5\cdot5\cdot3$) and sample 40 of them for comparison and randomly assign the order they appear in the evaluation prompt to control for order bias. 
For the versus author results, we only have 15 samples since thereâ€™s only one original author response ($1\cdot5\cdot3)$, so we permute the order for each pair to get 30 samples. 
This results in 400 ($40\cdot10$) comparisons for vs DITTO and 300 ($30\cdot10$) for vs Author to get the averaged win rates in \autoref{tab:main_combined}. 
We are able to compute statistically significant win rates with these number of comparisons, as reported in Section \ref{ssec:main_results}.

\subsection{Technical Details}
\label{appdx:technical_details}

The exact models that we use for our experiments are \texttt{gpt-4o-2024-0806}, Claude 3 Sonnet and Mistral 7B Instruct v0.2 via Amazon Bedrock. 
For evaluation (\textsection \ref{sec:pairwise_evaluation}), we use batch inference using the OpenAI API, which lets us reduce inference costs by 50\%.\footnote{\url{https://platform.openai.com/docs/guides/batch}} 

\paragraph{Notes on \ours}
At the first step of \ours, we hold out one sample $(x_i, y_i)$ at random and use the rest   $\mathcal{D}^{\mathcal{T}}_0\setminus\{(x_i, y_i)\}$ as few-shot examples that form the ICL prompt.
If we use the full set, we don't have any samples for which we can explore and learn  without seeing the reference output $y_i$ of the input $x_i$. from $\mathcal{D}^{\mathcal{T}}_0$.
This information is made more explicit in Algorithm \ref{alg:our_algorithm}. 



\paragraph{Notes on DITTO.} 
DITTO was trained with Mistral 7B Instruct, which is suspected to be a smaller model than Claude 3 Sonnet and GPT-4o. 
We found that the fine-tuned variants of Mistral 7B Instruct are prone to generating unstable outputs, producing template components such as \texttt{[INST]} or generating degenerate outputs that contain repetitive content. 
To give these approaches the best chances possible against our approaches with larger models, we reject samples that contain template components and repetitive content until we reach the desired number of samples $N=5$ from each model. 


\section{Evaluation Methods}
\label{appdx:automatic_evaluation}

\input{tables/selected_authors}

\input{tables/eval_method_comparisons}

In this section, we describe benchmarking results using author texts from CMCC and CCAT for various evaluation methods. 
Results are shown in \autoref{tab:eval_method_comparison}.
For CMCC, several authors wrote responses to the same prompt, and therefore we use samples written for the same prompt as the compared samples, as this would help us control for content similarity between the tested sample and the examples in making the task arbitrarily easy. 

For CCAT, which are news articles with a format of ``\textit{Write a news article that starts with the following sentence}: \texttt{article's first sentence}''. 
There are no share prompts for this dataset. 
Since journalists tend to write articles on similar topics over time and thus content similarity can be an easy hint for detecting authorship, we select the negative sample as one that has the highest TF-IDF similarity with any one of the in-context examples.

\subsection{Embedding-based evaluation}

Universal authorship representations (UAR)~\citep{rivera-soto-etal-2021-learning} is a sentence embedding method that uses a SBERT model~\cite{reimers-gurevych-2019-sentence} fine-tuned with a contrastive learning objective such that the representations of sentences or documents from the same author become closer together than with those of other authors.
Style Embeddings (SE) is a follow-up to UAR that refines the representations to focus on style rather than content by pairing contrastive samples that share similar content~\citep{wegmann-etal-2022-author}, and therefore we focus on embedding-based comparisons with Style Embeddings. 

We compute the cosine similarity between candidate texts' SE with those of the author examples and the one with higher cosine similarity is considered more stylistically similar. 
The best score for SE is $78.5\%$ and $63.6\%$ for CMCC and CCAT, respectively, which is significantly lower than GPT-4o results but better than GPT-4o mini's results.
While having more examples also helps with SE's performance, it plateaus markedly at three examples, and there is only minimal gains observed for CCAT. 




\subsection{LLM-as-a-Judge}
\label{appdx:llm as a judge}

The pairwise comparison setup is described in Section \ref{sec:pairwise_evaluation}. 
As an alternative, we also consider having LLM-as-a-judge provide ratings for individual instances, so that we can reduce the number of samples that the LLM-as-a-judge need to evaluate and also measure relative performance of each apporach based on their aggregate scores. 
The prompt is similar to the pairwise comparison setup except that only one candidate is shown and the LLM is asked to provide a score from 1-5 on how stylistically similar it thinks the candidate is to the examples. 
Unfortunately, this result is the poorest performing approach, only achieving 55.0\% accuracy for CMCC and 20.0\% accuracy for CCAT. 
The main reason for low accuracy is that the majority of instances were given the same rating such that they were deemed as ties. 


\subsection{Human Evaluation}
\label{appdx:human_evaluation}
We have three human annotators complete the same setup with 100 samples for CMCC only with five samples from each author. 
Conducting the same evaluation for CCAT was overwhelming for human participants because the average text length in CCAT is much larger.
As shown in the results, human performance is lower than the best GPT-4o results. 
This reinforces findings from previous work that identifying authorship reliably is a difficult task for untrained humans and that model-based classifiers are more reliable and practical for this task~\cite{hallinan-etal-2023-steer, krishna-etal-2020-reformulating, liu2024authorshipstyletransferpolicy, liu2024styletransfermultiiterationpreference}.  



The full prompt template for this evaluation setup is shown in \autoref{fig:eval_template}. 













\section{Flesch Reading Ease}
\label{appdx:flesch}

The Flesch Reading Ease (FRE) score is a simple equation for approximating readability of a given text based on the average number of words per sentence and the average number of syllables per word~\cite{flesch1948new}.
The formula for calculating FRE is the following: 

$$
FRE= 206.835 - (1.015\cdot ASL)-(84.6\cdot \frac{n_{sy}}{n_w})
$$

where $ASL$ is average sentence length ($\frac{\text{total words}}{\text{total sentences}}$) and $\frac{n_{sy}}{n_w}$ is the average number of syllables per word ($\frac{\text{total syllables}}{\text{total words}}$).  
The higher the score, the easier it is to understand a piece of text when read. 
The maximum score is 121.22, while there is no limit to how low it can be and it can even be negative. 
We use Python's \texttt{textstat} package\footnote{\url{https://github.com/textstat/textstat}} to compute FRE. 


\input{tables/main_vs_ditto}
\input{tables/main_vs_author}

\section{Full Per-Author Results}

The full per-author results with comparisons against DITTO and against the author's text are shown in \autoref{tab:main_results_vs_ditto} and \autoref{tab:main_results_vs_author}, respectively.

\section{Sample Outputs}

We show some qualitative examples from our baselines and \ours in \autoref{tab:sample_outputs}. 

\input{tables/sample_outputs}

\section{Prompt Details}
\label{appdx:prompt_details}

For details on the prompts that we use, we refer the reader to the following: 

\begin{itemize}
    \item Few-shot prompting: \autoref{fig:fewshot_prompt}
    \item \ours: \autoref{fig:our_prompt}
    \item Pairwise LMJ evaluation: \autoref{fig:eval_template}
    \item Style guide generation for CoT: \autoref{fig:cot_style_guide_template}
    \item Writing prompt for CoT that uses the generated style guide: \autoref{fig:cot_writing_template}
    \item OPRO's optimization prompt: \autoref{fig:opro_optimization_template}
    \item  OPRO's writing prompt that uses the prompt found from the optimization process: \autoref{fig:opro_writing_template}
\end{itemize}


\input{tables/prompts}
\input{tables/per_author_evaluation_benchmark}
