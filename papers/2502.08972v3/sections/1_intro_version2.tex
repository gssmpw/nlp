\section{Introduction}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{assets/problem_example.png}
    \caption{\textit{Top:} The author's text contains rhetorical questions and more colloquial and informal phrases.
    \textit{Middle}: Few-shot in-context learning is insufficient for adapting to an author's style and model biases for \textcolor{darkred}{formal phrases persist.}
    \textit{Bottom:} \ours helps overcome model biases and applies the \textcolor{darkgreen}{author's style} more consistently. 
    Model outputs are with \texttt{gpt-4o-2024-0806}
    }
    \label{fig:motivating_example} 
\end{figure}


Large language models (LLMs) are biased towards generic outputs as they are trained to align to an aggregate preference to be generally useful~\cite{padmakumar2024does,rafailov2024direct, lee2023rlaif, wang-etal-2023-self-instruct}. 
Yet end users often have specific needs that call for personalized text generation, such as writing an email~\cite{kumar2024longlampbenchmarkpersonalizedlongform, salemi2023lamp}. 
Hence, adapting LLMs for personalized text generation has drawn a growing interest~\cite{ jang2023personalized, shaikh2024show, mysore2023pearl, li2024learning}, but prior work on personalized text generation either rely on large amounts of personal data~\cite{li2024learning, li2023teach, mysore2023pearl} or parameter updates that result in cumbersome per-user\footnote{We use the term ``user'' and ``author'' interchangeably throughout this work. A user is anyone who provides personally written text that style can be learned from.} model parameters~\cite{shaikh2024show, liu-etal-2023-recap}.

These are critical limitations. 
Personalization is impractical when premised on users providing ample data because most are not willing to exert such effort~\cite{ tetard2009lazy} or are concerned about privacy~\cite{plant2022you}.
In addition, fine-tuning may not even be a viable option. The best performing LLMs are most often only accessible via APIs or too large that fine-tuning them is prohibitive even if they are open-sourced~\cite{touvron2023llama, firsich-rios-2024-gpt4, TheC3}.
Can we develop a method that overcomes these limitations such that it is effective with small amounts of data and requires no parameter updates? 

To address this problem, we propose \textbf{T}rial-Error-Explain \textbf{I}n-\textbf{C}ontext \textbf{L}earning (\textbf{\ours}), a \textit{tuning-free} adaptation of Trial-and-Error Fine-tuning (TEFT)~\cite{shaikh2024show,gulcehre2023reinforced,song-etal-2024-trial}.
The key idea of TEFT is to iteratively generate synthetic negative samples to create a preference dataset and apply a preference optimization method such as DPO~\cite{rafailov2024direct} to learn a more nuanced preference even with a small amount of initial training data. 
\ours takes a similar approach but only relies on scaling inference compute to expand an in-context learning (ICL) prompt to include prior model-generated negative samples (\textit{trial-error}) and corresponding explanations that describe their shortcomings (\textit{explain}) to better understand a user's personal style. 

We evaluate the effectiveness of \ours with GPT-4o and Claude 3 Sonnet on two personalized writing datasets consisting of emails, essays, and news articles.  
Using an extensively verified LLM-as-a-judge setup as the evaluator for comparing style similarity between two pairs of generated text with a user's text, we find that \ours achieves higher win rates (i.e. more frequently considered more similar in pairwise comparisons) than all other competitive prompting-based baselines, including Chain-of-Thought~\cite{wei2022chain} and OPRO~\cite{yang2024large} for models. 
\ours also achieves an average win rate of 76.6\% against the previous state-of-the-art~\cite{shaikh2024show}.
We ablate \ours extensively and find that, while every procedure in \ours contributes to stronger performance, explanations are crucial, as it is responsible for up to 77\% of the gains relative to a simple ICL baseline.

Lastly, we also share lexical analysis that shed light on the performance gaps between various models and methods. 
As illustrated in \autoref{fig:motivating_example}, compared to ICL,
we find \ours more reliably reduces LLM bias towards structural and formal phrases, e.g.,  ``\textit{additionally}'' and ``\textit{therefore}'', and instead use more colloquial ones, e.g., ``\textit{so why}'' and ``\textit{honestly.}'' 
We also discover that Claude 3 Sonnet's consistent dominance over GPT-4o is attributed to Claude 3 Sonnet more reliably generating phrases that elicit opinions, such as ``\textit{believe}'' and ``\textit{feel that},'' which are frequently seen in the users' texts. 



