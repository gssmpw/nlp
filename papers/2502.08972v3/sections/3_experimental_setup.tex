\section{Experimental Setup}

\subsection{Evaluation Dataset}
\label{sec:dataset}
We build on the evaluation setup from DITTO~\cite{shaikh2024show}, which uses two public authorship attribution (AA) datasets, because they also study personalized alignment. 
AA is a classification task of predicting the correct author for some written document, but it can also be reformulated as a personalized alignment task of generating text that is attributed to a specific author.  
Same as DITTO, we consider 20 authors from two sources:
(\textit{i}) CMCC~\cite{goldstein2008cmcc}, which contains emails and essays on controversial topics written by students, and (\textit{ii}) CCAT~\cite{lewis2004ccat50}, which consists of news articles written by journalists.
We preprocess the data with the same steps as DITTO to simulate a realistic data scenario for personalization in which per-user data is limited and treated as private.  
The number of samples per author and per task is small~\cite{zhang2020improved}, limited to the smallest number of samples available for an author (12) across both datasets to control for data size. 
In addition, each author's data is used in isolation such that performance does not depend on the total amount of data available among all authors. 
The train/val/test split is set to 7/2/3.\footnote{Our processed data can be found in \url{https://justin-cho.com/ticl}.} 


 









\subsection{Evaluation Method}
\label{sec:pairwise_evaluation}

\paragraph{Pairwise comparison on stylistic similarity with LLM-as-a-judge}

We evaluate model performance with pairwise comparisons of generated text using LLM-as-a-judge~\cite{zheng2023judging, kim2023prometheus}, asking which of the given candidates is more similar to a set of user's texts. 
A simplified illustration of our evaluation approach is shown in \autoref{fig:simple_evaluation_prompt}.



Specifically, we ask GPT-4o\footnote{The version we use is \texttt{gpt-4o-2024-0806}.} which of two candidates outputs generated for an author's prompt in the test set is more stylistically consistent with five examples sampled from the author's training data.
This is similar to the method used in DITTO~\cite{shaikh2024show}, but we found that we can further increase accuracy by providing more examples, providing a list of style elements to focus on, and generating an explanation before making its final decision.  
The two main comparisons we are interested in are how our methods and baselines compare with DITTO's outputs and the author's text. 
The former is a comparison against the prior state-of-the-art and the latter is against the gold output, and therefore serves as an upper bound. 
In other words, achieving a $\geq50\%$ win rate against the author indicates that the method is able to produce outputs that are indistinguishable from the author's texts for our LLM-as-a-judge in terms of stylistic consistency. 

\input{tables/eval_prompt}
\input{tables/llm_judge_main}

\paragraph{Evaluating LLM-as-a-judge for  pairwise comparisons of stylistic similarity\footnote{We experiment with various automatic evaluation metrics, human evaluation and alternative LLM-as-a-judge setups, and find pairwise comparisons to be the most reliable. Refer to Appendix \ref{appdx:automatic_evaluation} for these results. 
We intentionally avoid human evaluation based on our own results of human performance on comparing style similarity and evidence from previous work have shown that model-based classifiers are more reliable at determining stylistic similarity than untrained humans~\cite{hallinan-etal-2023-steer, krishna-etal-2020-reformulating, liu2024authorshipstyletransferpolicy, liu2024styletransfermultiiterationpreference}}.}

We validate the accuracy of our evaluation method by benchmarking it with user data in CMCC and CCAT. 
For CMCC, one of the two candidate outputs is the actual text from the correct author and the other is from another human author who wrote a response for the same prompt. 
This way we can prevent the LLM-as-a-judge from using topical similarity for determining correct authorship, since both candidates will discuss similar content \cite{wegmann-etal-2022-author}. 
For CCAT, there are no texts written for the same prompt, so we find the most similar distractor among texts from all other authors using TF-IDF. 
With this setup, our pairwise comparison evaluation with LLM-as-a-judge achieves $\sim97\%$ accuracy for both CMCC and CCAT for the top 10 authors, as shown in \autoref{tab:llm_as_a_judge_main}.


\paragraph{Author selection} 
Based on these benchmarking results, 
we discover significantly lower accuracies for some authors, as shown by the large drop in accuracy in \autoref{tab:llm_as_a_judge_main} when all authors are considered. 
We hypothesize that these authors have less consistent style across their texts, which may cause inconsistent evaluation results. Therefore, instead of random sampling, we select the top 10 authors in terms of our LLM-as-a-judge's accuracy from each dataset for a total of 20 authors. 
 


\input{tables/main_combined}

\subsection{Baselines}
\label{sec:baselines}

To measure the relative performance of \ours, we also evaluate on 
(\textit{i})\textbf{ Zero-shot},  to capture the base model's vanilla behavior when it is only given the task as input, and 
(\textit{ii}) \textbf{ICL}, where $N$ samples of task and output pairs of the author are included in the context as examples to learn from, along with some guidance to focus on style elements.\footnote{The full set of prompts we use in our work can be found in Appendix \ref{appdx:prompt_details}.} 

In addition, we evaluate other methods that scale test-time compute to improve downstream performance. 
(\textit{iii}) \textbf{Chain-of-Thought}~\cite{wei2022chain} is a prompting method that asks to reason about the style elements in the few-shot examples and then generate the response according to the analyzed style and the few-shot examples.  
(\textit{iv}) \textbf{OPRO}~\cite{yang2024large}is a prompt optimization method that refines the prompt iteratively using chain-of-thought prompt as the initial prompt. The model self-updates its prompt based on the history of previous prompts and their scores as ``gradients'' that guide the update. 
The initial prompt is set to ``Let's think step by step''~\cite{kojima2022large} and the scores are cosine similarity between the Style Embeddings (SE)~\cite{wegmann-etal-2022-author} of the generated outputs and the reference texts of the validation set. 
The prompt with the highest score is used at test time. 



We also compare \ours to
(\textit{v}) \textbf{DITTO}~\cite{shaikh2024show}, the state-of-the-art that uses TEFT on a small model, i.e. Mistral 7B Instruct~\cite{jiang2023mistral}, and outperforms GPT-4 combined with few-shot ICL. 
We do not include SFT in our experiments because DITTO was already shown to be superior to SFT.
Lastly, we also compare to (\textit{vi}) \textbf{Author}, the author's actual response to the given prompts and thus the upper bound.
For (\textit{i})-(\textit{iv}), we use \texttt{gpt-4o-2024-08-06}~\cite{achiam2023gpt} and \texttt{Claude 3 Sonnet}~\cite{TheC3}, abbreviated henceforth as GPT-4o and Claude 3 S unless otherwise specified. 
We let DITTO and \ours to run for four epochs. For \ours, we use LLM-as-a-judge between outputs from the validation set to choose the best performing prompt. 
We use Mistral 7B Instruct v0.2 for DITTO, following the original implementation. 
