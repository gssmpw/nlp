\section{Related Work}

\subsection{Personalization}

Personalization is an overloaded term in the context of language models as it can refer to multiple distinct features. 
Language models can be personalized to complete tasks as a specific user~\cite{salemi2023lamp, zhuang2024hydramodelfactorizationframework}, answer questions according to a user's profile~\cite{kim2024fewshotpersonalizationllmsmisaligned, dutt-etal-2022-perkgqa}, and recommend content based on a user's knowledge level~\cite{wang2016personalized}. They can also respond in a way that aligns with an individual's preference of digital assistance behavior~\cite{jang2023personalized}, edit an existing text to match someone else's style~\cite{horvitz-etal-2024-tinystyler, patel2023lowresourceauthorshipstyletransfer} . and generate some output in the style of a target user, such as a response to another online user~\cite{liu-etal-2023-recap} or text for a writing task~\cite{shaikh2024show, kumar2024longlampbenchmarkpersonalizedlongform, li2024learning, mysore2023pearl}. 

\ours focuses on the last category mentioned above -- personalized text generation -- where we want to steer language models away from a generic output~\cite{li-etal-2016-diversity, zhang-etal-2021-trading, padmakumar2024does} for a given writing task and instead produce one that is specific to a user's style~\cite{rivera-soto-etal-2021-learning, wegmann-etal-2022-author}. 
Prior work on personalized text generation rely on at least one component that requires fine-tuning, either the model that generates the response~\cite{li2023teach, liu-etal-2023-recap, shaikh2024show, mysore2023pearl} or a model that revises the prompt passed on to a black box model~\cite{li2024learning}. 
However, \ours is an inference-only method that does not require any parameter updates or new parameters. 

\subsection{Inference-only learning}

One major branch of inference-only learning method focuses on automating prompt engineering, optimizing the wording of a prompt template that is placed prior to asking a model to complete a task~\cite{shin-etal-2020-autoprompt, zhou2023large, ma2024largelanguagemodelsgood, yang2024large, kim2024fewshotpersonalizationllmsmisaligned, ye-etal-2024-prompt}, e.g. \textit{``Let's think step by step''} from \cite{kojima2022large}. 
Recent work have operationalized this process to also automate example selection for few-shot examples~\cite{yuksekgonul2024textgrad, khattab2023dspy}. 
In contrast, \ours maximizes its understanding of the target task by iteratively augmenting the content used for ICL with contrastive examples and explanations while keeping the prompt template constant.

Another prominent branch of inference-only learning is agentic methods that build on intermediate LLM outputs to refine the final output~\cite{chen-etal-2023-self, wei2022chain, yao2023react, sumers2024cognitive, shinn2024reflexion, saha-etal-2024-branch}. 
Reflexion~\cite{shinn2024reflexion} is most similar to our work, but it cannot be directly applied without a reliable evaluator for measuring success. 
Another key difference with \ours is that it front-loads the trial-and-error process ahead of test time and does not augment the prompt a priori by reasoning over few-shot examples. 










