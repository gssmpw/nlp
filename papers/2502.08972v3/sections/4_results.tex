

\section{Results and Discussion}
\label{sec:results}
\subsection{Main Results}
\label{ssec:main_results}
The main evaluation results that compare the baselines against DITTO and against the author's actual reference text is shown in \autoref{tab:main_combined}.
Details on the sample size can be found in Appendix \ref{appdx:sampling}.

\paragraph{Vs. DITTO} \ours outperforms or performs on par with all other baselines and often outperforms DITTO by a significant margin. 
We find that explicitly analyzing writing style in the few-shot examples and asking the model to abide by these styles did not lead to consistent improvements. 
With the exception of Claude 3 Sonnet on CCAT, CoT actually underperforms the simpler few-shot setup.
Interestingly, we find that OPRO consistently performs worst. 
When we analyzed the optimization sequence, OPRO lands on prompts that look sensible, e.g. \textit{``Respond with rhetorical devices, simple sentence structures, and easy vocabulary...}, but these generalized guidance are not as effective as seeing few-shot examples within the context in producing stylized text.  

\paragraph{Vs. Author} We see similar strengths for \ours when compared to Author, for which the results are shown in \autoref{tab:main_results_vs_author}. 
Surprisingly, Claude 3 Sonnet is able to exceed the $\geq 50$ win rate threshold when using \ours for both CMCC and CCAT. 
Performance with GPT-4o is relatively lower, but still significantly higher than DITTO for both datasets. 
We further dive into why Claude 3 Sonnet outperforms GPT-4o in \textsection \ref{sec:gpt_vs_claude}

Given Claude's strong performance even in the zero-shot setting against DITTO, we suspect there may be some leakage of CCAT data into Claude.  
Therefore, we focus our analysis henceforth mainly on CMCC. 
Also, since the relative performance among models is similar between vs. DITTO and vs. Author results, we focus on vs. Author results in the following sections. 



\begin{table}[t]
    \small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llr}
         \toprule
         \textbf{Model} & \textbf{Ablation} & \multicolumn{1}{c}{\textbf{Win rate}}  \\
         \midrule 
         \multirow{5}{*}{GPT-4o} & \ours & $\mathbf{31.00_{1.18}}$ \\
         & $-$ Initial ICL examples & $28.50_{1.50}$\\ 
         & $-$ Explanations & $23.50_{1.24}$ \\ 
         & $-$ Checkpointing & $22.50_{0.90}$ \\ 
         & $-$ Negative samples \& Expl. & $21.00_{1.28}$ \\ 
         \midrule 
         \multirow{5}{*}{Claude 3 S} & \ours & $\mathbf{54.50_{1.67}}$ \\ 
          & $-$ Initial ICL examples & $52.00_{1.22}$\\ 
         & $-$ Explanations & $46.00_{1.41}$ \\ 
         & $-$ Checkpointing & $54.00_{1.67}$ \\ 
         & $-$ Negative samples \& Expl. & $43.50_{1.37}$ \\ 
         \bottomrule
    \end{tabular}
    }
    \caption{Ablation with \ours minus ($-$) individual procedures on CMCC. Reported win rate is vs. Author and subscript is the standard error. All procedures provide performance gains, but explanations make the biggest difference while starting with ICL examples is the least important. 
    }
    \label{tab:ablation_results_cmcc}
\end{table}

\begin{table}[t]
    \small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llr}
         \toprule
         \textbf{Model} & \textbf{Swap Configuration} &  \multicolumn{1}{c}{ Win rate}   \\
         \midrule 
         \multirow{3}{*}{GPT-4o} & No swap & $31.00_{1.18}$ \\ 
         & $\rightarrow$ with Claude 3 S expl. & $32.50_{1.13}$ \\ 
         & $\rightarrow$ with Claude 3 S \ours   & $18.30_{0.25}$ \\
         \midrule
         \multirow{3}{*}{Claude 3 S} & No swap    & $54.50_{1.67}$ \\ 
         & $\rightarrow$ with GPT-4o expl. & $55.00_{1.28}$ \\ 
         & $\rightarrow$ with GPT-4o \ours  & $42.50_{0.92}$ \\ 
         \bottomrule
    \end{tabular}
    }
    \caption{Ablation with various swap configurations to compare GPT-4o \ours and Claude 3 Sonnet \ours performance on CMCC.
    Result format is the same as \autoref{tab:ablation_results_cmcc}.
    Changing the explanation model does not lead to significant changes. 
    Using final \ours prompts from a different model leads to large drops, indicating that models learn better from their own failure modes.
    }
    \label{tab:gpt_vs_claude}
\end{table}


\begin{table*}[]
\small
    \centering
    \adjustbox{max width=\textwidth}{    \begin{tabular}{llrrlrr}
    \toprule
         \multicolumn{1}{c}{\textbf{A}} & \multicolumn{1}{c}{\textbf{B}} & \textbf{FRE ($\text{A}\setminus \text{B}$)} $\uparrow$ & \textbf{FRE ($\text{B}\setminus \text{A}$)} $\uparrow$ & \textbf{$n$-gram} & \textbf{$z$-score} & $p$ \\ \midrule 
         \multirow{4}{*}{GPT-4o \ours} & \multirow{4}{*}{GPT-4o Few-shot} & \multirow{4}{*}{121.22} & \multirow{4}{*}{36.62} & so why & $1.990$ & 0.047 \\  
         & & & & honestly & $2.098$ & 0.036 \\ 
         & & & & additionally & $-2.983$ & 0.003 \\
         & & & & therefore & $-2.233 $ & 0.026 \\ 
         \midrule 
         \multirow{4}{*}{Claude 3 S \ours} & \multirow{4}{*}{GPT-4o \ours} & \multirow{4}{*}{$120.21$} & \multirow{4}{*}{$77.21$} & believe & 4.255 & 0.000 \\
         & & & & feel that & 4.195 & 0.000  \\
         & & & & crucial to & $-3.449$ & 0.001 \\ 
         & & & & implications of & $-2.925$ & 0.003 \\ 
         \bottomrule
    \end{tabular}
    }
    \caption{Representative Fightin' Words model~\cite{monroe2008fightin} results for frequency differences between two sources at $p<0.05$. A positive z-score indicates that output source A significantly uses more of the n-gram than B while a negative score signals the opposite. 
    We observe that the models that have higher win-rates from pairwise comparisons frequently generate phrases for expressing personal opinions while the losing model generates more formal phrases. 
    }
    \label{tab:fightin_words_model_results}
\end{table*}

\subsection{Ablating \ours}

We conduct an ablation study to understand how each of the components in \ours contributes to its performance. 
We observe how the win rates change when we remove the initial ICL setup that substitutes SFT for behavior cloning, explanations, checkpointing (choosing the best performing augmented prompt throughout iterations rather than simply using the final prompt), and both the negative samples and explanations.
The last row is equivalent to the few-shot baseline. 
The results are shown in \autoref{tab:ablation_results_cmcc}. 

\paragraph{Explanations are most important, while starting with ICL provides smallest improvements.}
For both GPT-4o and Claude 3, removing any of the procedures leads to performance drops. 
For both cases, we see the largest drop attributed to removing both the explanations and negative samples. 
When only dropping the explanations, we see a smaller drop, indicating that both the negative samples and explanations provide additive advantages. 
For GPT-4o, checkpointing is also very important, but less so for Claude 3 Sonnet. 
In both models, not having the first step use ICL examples is not critical, meaning that we can even start with zero-shot outputs for the first step and not see large performance degradation. 



\subsection{Why does Claude 3 Sonnet outperform GPT-4o?}
\label{sec:gpt_vs_claude}

One striking trend from \autoref{tab:main_combined} is Claude 3 Sonnet's noticeably stronger performance compared to GPT-4o. 
This is somewhat surprising given that GPT-4o is the latest model that ranks at the top of popular language modeling leaderboards, such as Chatbot Arena.\footnote{\url{https://lmarena.ai/}}
In order to understand Claude's comparative performance over GPT-4o, we examine how performance changes if we (\textit{i}) use the other model for generating explanations during prompt augmentation (with other model expl.) and (\textit{ii}) use the fully developed \ours prompt from the other model at test time (with other model \ours).  

\paragraph{Explanations are similar in quality.}
Results shown in \autoref{tab:gpt_vs_claude} suggest that it is \textbf{not} that Claude 3 Sonnet generates better explanations than GPT-4o. 
Applying \ours on GPT-4o with Claude 3 Sonnet as the explanation generator leads to a mild increase in performance that is not statistically significant. 
This is further corroborated by Claude 3 Sonnet's results when using GPT-4o as the explanation generator, which also leads to a statistically insignificant improvement. 

\paragraph{\ours prompts are not transferable.}
Using the final augmented prompts developed by Claude 3 Sonnet and using GPT-4o at test-time does not improve on simply using GPT-4o's own augmented prompt. 
This is likely due to the distribution shift: negative samples and their corresponding feedback is not as relevant and actually serves as noise such that the performance actually drops below the few-shot performance in \autoref{tab:main_combined}. 
Based on these results, we attribute Claude 3 Sonnet's superior performance on its superior few-shot ICL capabilities for writing tasks. 
The following section on lexical analysis adds support to this hypothesis. 

\subsection{Lexical Analysis}
\label{sec:lexical_analysis}

We further explore the models' comparative performances using a lexical analysis, using 
Fightin' Words model for extracting words with significant differences in between two corpora~\cite{monroe2008fightin}. The method shows different ratios of words that are significantly more frequent in one model over another. 
Using a $p$-level of 0.05, we calculate the number of $n$-grams that are significantly more frequent in the author's text compared to the model's text and vice-versa.
Representative results are shown in \autoref{tab:fightin_words_model_results}.


\paragraph{\ours overcomes model bias for structural and formal phrases better than ICL.}
Comparing GPT-4o \ours vs. GPT-4o few-shot elicits \ours improvement over a simple ICL baseline. 
We see that structural and formal phrases, e.g. ``\textit{additionally}'' and ``\textit{therefore}'', are more frequent in the few-shot outputs, while \ours contain more colloquial phrases, e.g., ``\textit{so why}''. 
This suggests that \ours enables models to overcome its biases towards generic formal text and better learn from the given style context. 

\paragraph{Claude 3 Sonnet applies user styles more effectively than GPT-4o.}
When comparing Claude 3 Sonnet and GPT-4o directly, the top words that Claude 3 Sonnet is able to generate more consistently are subjective and assertive words or phrases that are frequently seen in CMCC, e.g., ``\textit{believe}'' and ``\textit{feel that}'', and also more casual phrases, e.g., ``\textit{things like}'' and ``\textit{kind of}.''  
On the other hand, GPT-4o's outputs consists of more formal phrases, such as ``\textit{crucial to}.'' 
This dynamic is further corroborated by the Flesch Readability Ease (FRE) scores ~\cite{flesch1948new}.
The words that appear significantly more frequently for Claude has a higher FRE score (FRE(A$\setminus$B)$=$120.21) than the score for words that appear significantly more frequenty for GPT-4o (FRE(B$\setminus$A$=$77.21). 
A higher FRE score means it is easier to understand when read, which correlates to more casual text~\cite{cho2024speechworthyinstructiontunedlanguagemodels}. 




