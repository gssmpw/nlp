\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/method_overview.png}
    \caption{\ours methodology overview. Instead of SFT, \ours starts with a few-shot ICL for behavior cloning. 
    Then, \ours repeatedly generates an output for a task and a corresponding explanation that critiques the stylistic difference between the output and the user's text. If the output is considered not stylistically consistent, it and its explanation are added to the prompt.
    }
    \label{fig:method_overview}
\end{figure*}

\section{Methodology}
\label{sec:methodology}

Trial-Error-Explain In-Context Learning (\ours) is a method for expanding an ICL prompt through tuning-free correspondences of the three main stages in Trial-and-Error Fine-tuning (TEFT). 
Therefore, we describe \ours by going through each main stages of TEFT and the corresponding procedure in \ours, as illustrated in \autoref{fig:method_overview}.
TEFT refers to a method that (\textit{i}) fine-tunes a model with supervised fine-tuning (SFT) on a small set of training data (behavior cloning), (\textit{ii}) uses the trained model to generate outputs that are considered less preferred to those from the training data (exploration), (\textit{iii}) and fine-tunes the model further with a  preference optimization method, such as DPO (learning). 
This method has been applied to various tasks in previous work with minor differences, such as machine translation with ReST~\cite{gulcehre2023reinforced}, personalized text generation with DITTO~\cite{shaikh2024show}, and trajectory planning with ETO~\cite{song-etal-2024-trial}.


In \ours, we first replace SFT with ICL for behavior cloning (\textsection \ref{sec:behavioral_cloning}). 
Exploration is mostly the same except that \ours updates the prompt at the step level while TEFT updates the policy at the epoch level  in order to avoid augmenting the prompt with redundant errors (\textsection \ref{sec:exploration}). 
Lastly, for learning, we substitute preference optimization with prompt augmentation (\textsection \ref{sec:learning}). 
\ours generates explanations that analyzes the differences between the generated text and the user's actual text. Then, the generated text is labeled as a negative sample and the explanation are added to the prompt. 
We elaborate on each correspondence and share additional details in the following sections.



\subsection{Behavioral cloning}
\label{sec:behavioral_cloning}

As shown in the first step in \autoref{fig:method_overview}, TEFT uses supervised fine-tuning (SFT) to fine-tune a model $\pi_\theta$ roughly towards the desired behavior with the initial training data.
This helps with generating more relevant negative samples in the exploration phase~\cite{song-etal-2024-trial}. 

Formally, with the initial training data $\mathcal{D}^\mathcal{T}=\{(x_i,y_i)|_{i=1}^N\}$, where $x$ is the task and $y$ is the user's text, SFT uses the negative log likelihood (NLL) loss to learn from $\mathcal{D}^{\mathcal{T}}$ to fine-tune a pretrained LLM $\pi_\theta$ parameterized by $\theta$ such that: 
$$\mathcal{L}_{NLL}(\theta)=-\mathbb{E}_{(x,y)\sim\mathcal{D}^{\mathcal{T}}}\sum\limits_{t=1}^T\log\pi_{\theta}(y_t|y_{1:t-1},x)$$

\paragraph{SFT $\rightarrow$ Few-shot ICL.} 
To substitute SFT, we use $\mathcal{D}^{\mathcal{T}}$ to form an in-context learning (ICL) prompt and perform few-shot ICL ~\cite{tom2020fewshot}.
The resulting ICL prompt is defined as: 

\small
\begin{multline}
    \mathcal{P}\left(x_i,
    \mathcal{D}^{\texttt{icl}}\right),  \text{where} \\  \mathcal{D}^{\texttt{icl}} = \{(x_k,y_k,S_k)\}|_{k=1}^K, S = \{(\Tilde{y}_l, e_l)\}|_{l=1}^L
    \label{eq:our_prompt}
\end{multline}
\normalsize


Each sample in $\mathcal{D}^{\texttt{icl}}$ is a tuple of task $x$, user's text $y$, and a set of model-generated outputs and their corresponding explanations $S=\{(\Tilde{y}_{l}, e_{l})\}|_{l=1}^L$ from previous iterations $L$.  
$\mathcal{P}$ is a templating function that takes $\mathcal{D}^{\texttt{icl}}$, and the target input $x_i$ and forms an ICL prompt.\footnote{We share the details of $\mathcal{P}$ and all other prompts used in this work in Appendix \ref{appdx:prompt_details}.} 
In the first step of \ours, there are no previous model-generated outputs and explanations, so Eq.\ref{eq:our_prompt} is simply 
$\mathcal{P}\left(x_i,\{(x_k,y_k)|_{k=1}^K\}\right)$.


    



\subsection{Exploration}
\label{sec:exploration}

The objective of the exploration phase, the middle stage in \autoref{fig:method_overview}, is to generate negative samples that will be used for learning.
TEFT and \ours is mostly similar except for minor technical differences. 

In TEFT, the model  $\pi_\theta$ resulting from SFT or the previous iteration of DPO generates output $\Tilde{y}$ for the target task $x$ without any prompt templating. 
In contrast, \ours takes prompts resulting from Eq.\ref{eq:our_prompt} as input:
\begin{equation}
\Tilde{y_i}\sim \pi_\theta\left(p\left(x_i, \mathcal{D}^{\texttt{icl}}\right)\right) 
\label{eq:ours_explore}
\end{equation}

Another difference is that TEFT 
generates outputs for the full set of inputs in $\mathcal{D}^{\mathcal{T}}$: 
$\{\Tilde{y_i} \sim \pi_\theta(x_i)
| x_i \in \mathcal{D}^{\mathcal{T}}_0 \}$, prior to the learning stage. 
\ours only generates one output from a single sample before moving on to the learning phase -- think of $\text{batch size}=1$. 
We observe that generating outputs for all samples in $\mathcal{D}^{\mathcal{T}}$ leads to multiple $\Tilde{y}$ that contain redundant modes of failure. 
We empirically see that this inefficiency can be avoided by exploring a single $\Tilde{y}$ at a time. 



\subsection{Learning}
\label{sec:learning}

In the learning phase for TEFT, the preference optimization samples $\mathcal{D}_i^{\texttt{pref}}$ collected from the exploration phase are used for fine-tuning $\pi_\theta$ with the use of a reference model $\pi_{\text{ref}}$, usually a pre-trained language model for preventing overfitting and reward hacking, and a preference optimization method, such as direct preference optimization (DPO)~\cite{rafailov2024direct}:




\paragraph{Preference optimization $\rightarrow$ Prompt augmentation with negative samples and explanations}

The main benefit of preference optimization is that the fine-tuned model learns a fine-grained understanding of the differences between reference outputs and negative samples. 
With \ours, we provide similar information in context via the negative samples and their corresponding explanations that analyze the difference between them and the reference outputs. 

We first let the model analyze the differences between the reference output $y$ and its respective negative output $\Tilde{y}$. 
This step is akin to rationale generation~
\cite{wang2022rationale, wei2022chain, zhou-etal-2022-reflect}, but the difference with prior work is that \ours's rationale focuses on explicitly extracting the differences between the reference output and the negative output, rather than producing a rationale as an intermediate step before fulfilling the target task. 

For this step, we generate explanations using an explanation generation prompt template $\mathcal{E}$ that takes in $y$ and $\Tilde{y}$:  
\begin{equation}
    (e_i,v_i)\sim f(\mathcal{E}\left(y_i, \Tilde{y_i})\right)
    \label{eq:explanation_generation}
\end{equation}



where $e_i$ is the explanation and $v_i$ is a validator Boolean that checks whether $\Tilde{y_i}$ is comparable in quality or accuracy as $y_i$. 
$f(\cdot)$ can be any function or combination of functions that generates $e_i$ and $v_i$. 
We use the same $\pi_\theta$ unless specified otherwise and have $\mathcal{E}$ instruct the model to generate both $e_i$ and $v_i$ in a JSON format. 
If $v_i\neq\text{True}$, then we add $(\Tilde{y_i},e_i)$ to $S_i$:  $(x_i,y_i)$'s set of model-generated outputs and explanations. 
Otherwise, we prevent good $\Tilde{y}$ from being included in the following iterations as negative samples, in order to avoid noise and the context becoming unnecessarily longer. 



\paragraph{Iterations and checkpointing}

In summary, \ours starts with standard ICL to substitute SFT and  repeats (\textit{i}) exploration with an expanding dataset to  generate a candidate negative sample (Eq.\ref{eq:ours_explore}) and (\textit{ii}) learning to generate an explanation, expand the dataset with the negative sample and explanation if the sample is deemed a proper negative sample (Eq.\ref{eq:explanation_generation}).
A formal algorithm for \ours is defined in Algorithm \ref{alg:our_algorithm}. 
Denoting the augmented dataset that starts from $\mathcal{D}^{\mathcal{T}}$ as $\mathcal{D}^\texttt{\ours}$, 
we keep track of the version of $\mathcal{D}^\texttt{\ours}$ that attains the best performance on the validation set, measuring it every $n$ steps. 







\input{sections/algorithm}
