@article{Bahl2020percom,
author = {Bahl, Paramvir Victor and Caceres, Ramon and Davies, Nigel and Want, Roy},
doi = {10.1109/MPRV.2020.3032205},
issn = {15582590},
journal = {IEEE Pervasive Computing},
mendeley-groups = {SoulSaver},
number = {4},
pages = {8--9},
title = {{Pervasive Computing at the Edge}},
volume = {19},
year = {2020}
}

@article{Zhang2019edgevideoanalytics,
abstract = {With the installation of enormous public safety and transportation infrastructure cameras, video analytics has come to play an essential part in public safety. Typically, video analytics is to collectively leverage the advanced computer vision (CV) and artificial intelligence (AI) to solve the four-W problem. That is to identify Who has done something (What) at a specific place (Where) at some time (When). According to the difference of latency requirements, video analytics can be applied to postevent retrospective analysis, such as archive management, search, forensic investigation and real-time live video stream analysis, such as situation awareness, alerting, and interested object (criminal suspect/missing vehicle) detection. The latter is characterized as having higher requirements on hardware resources as the sophisticated image processing algorithms under the hood. However, analyzing large-scale live video streams on the Cloud is impractical as the edge solution that conducts the video analytics on (or close to) the camera provides a silvering light. Analyzing live video streams on the edge is not trivial due to the constrained hardware resources on edge. The AI-dominated video analytics requires higher bandwidth, consumes considerable CPU/GPU resources for processing, and demands largermemory for caching. In this paper, we review the applications, algorithms, and solutions that have been proposed recently to facilitate edge video analytics for public safety.},
author = {Zhang, Qingyang and Sun, Hui and Wu, Xiaopei and Zhong, Hong},
doi = {10.1109/JPROC.2019.2925910},
issn = {15582256},
journal = {Proc. of the IEEE},
keywords = {Edge computing,Public safety,Video analytics},
mendeley-groups = {SoulSaver},
number = {8},
pages = {1675--1696},
title = {{Edge video analytics for public safety: A review}},
volume = {107},
year = {2019}
}


@article{Yeung2022horus,
author = {Yeung, Gingfung and Borowiec, Damian and Yang, Renyu and Friday, Adrian and Harper, Richard and Garraghan, Peter},
doi = {10.1109/TPDS.2021.3079202},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
mendeley-groups = {SoulSaver},
number = {1},
pages = {88--100},
title = {{Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems}},
volume = {33},
year = {2022}
}

@article{Luo2021edgeresource,
author = {Luo, Quyuan and Hu, Shihong and Li, Changle and Li, Guanghui and Shi, Weisong},
doi = {10.1109/COMST.2021.3106401},
issn = {1553-877X},
journal = {IEEE Communications Surveys & Tutorials},
mendeley-groups = {SoulSaver},
number = {4},
pages = {2131--2165},
title = {{Resource Scheduling in Edge Computing: A Survey}},
volume = {23},
year = {2021}
}
@article{Hong2020edgeresource,
abstract = {Contrary to using distant and centralized cloud data center resources, employing decentralized resources at the edge of a network for processing data closer to user devices, such as smartphones and tablets, is an upcoming computing paradigm, referred to as fog/edge computing. Fog/edge resources are typically resource-constrained, heterogeneous, and dynamic compared to the cloud, thereby making resource management an important challenge that needs to be addressed. This article reviews publications as early as 1991, with 85% of the publications between 2013 and 2018, to identify and classify the architectures, infrastructure, and underlying algorithms for managing resources in fog/edge computing.},
author = {Hong, Cheol-Ho and Varghese, Blesson},
doi = {10.1145/3326066},
issn = {0360-0300},
journal = {ACM Computing Surveys},
mendeley-groups = {SoulSaver},
number = {5},
pages = {1--37},
title = {{Resource Management in Fog/Edge Computing}},
volume = {52},
year = {2020}
}

@inproceedings{Hou2023dystri,
abstract = {Deep neural network (DNN) inference poses unique challenges in serving computational requests due to high request intensity, concurrent multi-user scenarios, and diverse heterogeneous service types. Simultaneously, mobile and edge devices provide users with enhanced computational capabilities, enabling them to utilize local resources for deep inference processing. Moreover, dynamic inference techniques allow content-based computational cost selection per request. This paper presents Dystri, an innovative framework devised to facilitate dynamic inference on distributed edge infrastructure, thereby accommodating multiple heterogeneous users. Dystri offers a broad applicability in practical environments, encompassing heterogeneous device types, DNN-based applications, and dynamic inference techniques, surpassing the state-of-the-art (SOTA) approaches. With distributed controllers and a global coordinator, Dystri allows per-request, per-user adjustments of quality-of-service, ensuring instantaneous, flexible, and discrete control. The decoupled workflows in Dystri naturally support user heterogeneity and scalability, addressing crucial aspects overlooked by existing SOTA works. Our evaluation involves three multi-user, heterogeneous DNN inference service platforms deployed on distributed edge infrastructure, encompassing seven DNN applications. Results show Dystri achieves near-zero deadline misses and excels in adapting to varying user numbers and request intensities. Dystri outperforms baselines with accuracy improvement up to 95Ã—.},
author = {Hou, Xueyu and Guan, Yongjie and Han, Tao},
booktitle = {ACM Int. Conf. Proceeding Series},
isbn = {9798400708435},
keywords = {MLaaS,dynamic inference,edge computing},
pages = {625--634},
publisher = {ACM},
title = {{Dystri: A Dynamic Inference based Distributed DNN Service Framework on Edge}},
year = {2023}
}


@article{Tan2021migserving,
abstract = {Multi-Instance GPU (MIG) is a new feature introduced by NVIDIA A100 GPUs that partitions one physical GPU into multiple GPU instances. With MIG, A100 can be the most cost-efficient GPU ever for serving Deep Neural Networks (DNNs). However, discovering the most efficient GPU partitions is challenging. The underlying problem is NP-hard; moreover, it is a new abstract problem, which we define as the Reconfigurable Machine Scheduling Problem (RMS). This paper studies serving DNNs with MIG, a new case of RMS. We further propose a solution, MIG-serving. MIG- serving is an algorithm pipeline that blends a variety of newly designed algorithms and customized classic algorithms, including a heuristic greedy algorithm, Genetic Algorithm (GA), and Monte Carlo Tree Search algorithm (MCTS). We implement MIG-serving on Kubernetes. Our experiments show that compared to using A100 as-is, MIG-serving can save up to 40% of GPUs while providing the same throughput.},
archivePrefix = {arXiv},
arxivId = {2109.11067},
author = {Tan, Cheng and Li, Zhichao and Zhang, Jian and Cao, Yu and Qi, Sikai and Liu, Zherui and Zhu, Yibo and Guo, Chuanxiong},
eprint = {2109.11067},
journal = {arXiv},
mendeley-groups = {SoulSaver},
title = {{Serving DNN Models with Multi-Instance GPUs: A Case of the Reconfigurable Machine Scheduling Problem}},
year = {2021}
}

@inproceedings{Nguyen2023preacto,
author = {Nguyen, Thanh-Tung and Jang, Si Young and Kostadinov, Boyan and Lee, Dongman},
booktitle = {2023 IEEE Int. Conf. on Pervasive Computing and Communications},
doi = {10.1109/PERCOM56429.2023.10099298},
isbn = {978-1-6654-5378-3},
mendeley-groups = {SoulSaver},
pages = {101--110},
publisher = {IEEE},
title = {{PreActo: Efficient Cross-Camera Object Tracking System in Video Analytics Edge Computing}},
year = {2023}
}


@inproceedings{Jang2021pipeline,
abstract = {With today's ubiquitous deployment of video cameras and other edge devices, progress in edge computing is happening at an incredible speed. Yet, one aspect of real-time video analytics at the edge that is still underdeveloped is the support for processing multitenant, multi-application scenarios with a limited set of resources. Existing systems either fail to provide the necessary performance, or rely too heavily on edge or cloud servers to handle the workload. This work proposes a new approach, inspired by both Function-as-a-Service and microservices architecture in order to efficiently place and execute video analytics pipelines on edge devices. The main contributions of this work are the ability to dynamically add and run new applications on already deployed systems, and the capability to horizontally distribute pipelines across other neigh-bouring edge devices. We prototype an implementation that we evaluate using multiple concurrent applications per device. Results show that our system provides more flexibility for on-the-fly re-configuration than existing works do, with 20 % improvement in latency and 3.9 X increase in throughput.},
author = {Jang, Si Young and Kostadinov, Boyan and Lee, Dongman},
booktitle = {6th ACM/IEEE Symp. on Edge Computing, SEC 2021},
doi = {10.1145/3453142.3491283},
file = {:home/tung/Desktop/Microservice-based_Edge_Device_Architecture_for_Video_Analytics.pdf:pdf},
isbn = {9781450383905},
keywords = {edge computing,microservices,realtime,serverless,video analytics},
mendeley-groups = {SoulSaver},
pages = {165--177},
publisher = {ACM},
title = {{Microservice-based Edge Device Architecture for Video Analytics}},
year = {2021}
}


@article{Ananthanarayanan2017vakiller,
author = {Ananthanarayanan, Ganesh and Bahl, Paramvir and Bodik, Peter and Chintalapudi, Krishna and Philipose, Matthai and Ravindranath, Lenin and Sinha, Sudipta},
doi = {10.1109/MC.2017.3641638},
issn = {0018-9162},
journal = {Computer},
mendeley-groups = {SoulSaver},
number = {10},
pages = {58--67},
title = {{Real-Time Video Analytics: The Killer App for Edge Computing}},
volume = {50},
year = {2017}
}

@inproceedings{zeng2020distream,
  title={Distream: scaling live video analytics with workload-adaptive distributed edge intelligence},
  author={Zeng, Xiao and Fang, Biyi and Shen, Haichen and Zhang, Mi},
  booktitle={Proc. of the 18th SenSys Conf.},
  pages={409--421},
  year={2020}
}

@article{liang2024splitstream,
  title={SplitStream: Distributed and workload-adaptive video analytics at the edge},
  author={Liang, Yu and Zhang, Sheng and Wu, Jie},
  journal={Journal of Network and Computer Applications},
  volume={225},
  pages={103866},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{shen2019nexus,
  title={Nexus: A GPU cluster engine for accelerating DNN-based video analysis},
  author={Shen, Haichen and Chen, Lequn and Jin, Yuchen and Zhao, Liangyu and Kong, Bingyu and Philipose, Matthai and Krishnamurthy, Arvind and Sundaram, Ravi},
  booktitle={Proc. of the 27th ACM Symp. on Operating Systems Principles},
  pages={322--337},
  year={2019}
}

@inproceedings{nigade2022jellyfish,
  title={Jellyfish: Timely inference serving for dynamic edge networks},
  author={Nigade, Vinod and Bauszat, Pablo and Bal, Henri and Wang, Lin},
  booktitle={2022 IEEE Real-Time Systems Symp. (RTSS)},
  pages={277--290},
  year={2022},
  organization={IEEE}
}

@article{Kang2017noscope,
abstract = {Recent advances in computer vision---in the form of deep neural networks---have made it possible to query increasing volumes of video data with high accuracy. However, neural network inference is computationally expensive at scale: applying a state-of-the-art object detector in real time (i.e., 30+ frames per second) to a single video requires a $4000 GPU. In response, we present N o S cope , a system for querying videos that can reduce the cost of neural network video analysis by up to three orders of magnitude via inference-optimized model search. Given a target video, object to detect, and reference neural network, N o S cope automatically searches for and trains a sequence, or cascade, of models that preserves the accuracy of the reference network but is specialized to the target video and are therefore far less computationally expensive. N o S cope cascades two types of models: specialized models that forego the full generality of the reference model but faithfully mimic its behavior for the target video and object; and difference detectors that highlight temporal differences across frames. We show that the optimal cascade architecture differs across videos and objects, so N o S cope uses an efficient cost-based optimizer to search across models and cascades. With this approach, N o S cope achieves two to three order of magnitude speed-ups (265-15,500x real-time) on binary classification tasks over fixed-angle webcam and surveillance video while maintaining accuracy within 1--5% of state-of-the-art neural networks.},
author = {Kang, Daniel and Emmons, John and Abuzaid, Firas and Bailis, Peter and Zaharia, Matei},
doi = {10.14778/3137628.3137664},
issn = {2150-8097},
journal = {Proc. of the VLDB Endowment},
mendeley-groups = {SoulSaver},
number = {11},
pages = {1586--1597},
title = {{NoScope: optimizing neural network queries over video at scale}},
volume = {10},
year = {2017}
}

@inproceedings{Zhang2018awstream,
author = {Zhang, Ben and Jin, Xin and Ratnasamy, Sylvia and Wawrzynek, John and Lee, Edward A.},
booktitle = {Proc. of the 2018 Conf. of the ACM Special Interest Group on Data Communication},
doi = {10.1145/3230543.3230554},
isbn = {9781450355674},
mendeley-groups = {SoulSaver},
pages = {236--252},
publisher = {ACM},
title = {{AWStream: adaptive wide-area streaming analytics}},
year = {2018}
}

@inproceedings{Jiang2018chameleon,
author = {Jiang, Junchen and Ananthanarayanan, Ganesh and Bodik, Peter and Sen, Siddhartha and Stoica, Ion},
booktitle = {Proc. of the 2018 Conf. of the ACM Special Interest Group on Data Communication},
doi = {10.1145/3230543.3230574},
isbn = {9781450355674},
mendeley-groups = {SoulSaver},
pages = {253--266},
publisher = {ACM},
title = {{Chameleon: scalable adaptation of video analytics}},
year = {2018}
}

@inproceedings{Li2020reducto,
author = {Li, Yuanqi and Padmanabhan, Arthi and Zhao, Pengzhan and Wang, Yufei and Xu, Guoqing Harry and Netravali, Ravi},
booktitle = {Proc. of SIGCOMM},
doi = {10.1145/3387514.3405874},
isbn = {9781450379557},
mendeley-groups = {SoulSaver},
pages = {359--376},
publisher = {ACM},
title = {{Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics}},
year = {2020}
}

@inproceedings{Zhang2015vigil,
author = {Zhang, Tan and Chowdhery, Aakanksha and Bahl, Paramvir (Victor) and Jamieson, Kyle and Banerjee, Suman},
booktitle = {Proc. of the 21st Annual Int. Conf. on Mobile Computing and Networking},
doi = {10.1145/2789168.2790123},
isbn = {9781450336192},
mendeley-groups = {SoulSaver},
pages = {426--438},
publisher = {ACM},
title = {{The Design and Implementation of a Wireless Video Surveillance System}},
year = {2015}
}


@misc{nvidia-matrixmul,
mendeley-groups = {SoulSaver},
title = {{Matrix Multiplication Background User's Guide}},
url = {https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html},
urldate = {2024/09/30}
}


@article{zhang2020edge,
  title={Deep learning in the era of edge computing: Challenges and opportunities},
  author={Zhang, Mi and Zhang, Faen and Lane, Nicholas D and Shu, Yuanchao and Zeng, Xiao and Fang, Biyi and Yan, Shen and Xu, Hui},
  journal={Fog Computing: Theory and Practice},
  pages={67--78},
  year={2020},
  publisher={Wiley Online Library}
}
@INPROCEEDINGS{Hung2018VideoEdge,
  author={Hung, Chien-Chun and Ananthanarayanan, Ganesh and Bodik, Peter and Golubchik, Leana and Yu, Minlan and Bahl, Paramvir and Philipose, Matthai},
  booktitle={2018 IEEE/ACM Symp. on Edge Computing (SEC)}, 
  title={VideoEdge: Processing Camera Streams using Hierarchical Clusters}, 
  year={2018},
  volume={},
  number={},
  pages={115-131},
  keywords={Cameras;Merging;Cloud computing;Streaming media;Organizations;Detectors;Bandwidth;video analytics;scheduling;distributed clusters},
}

@inproceedings{Zhang2021CEVAS,
author = {Zhang, Miao and Wang, Fangxin and Zhu, Yifei and Liu, Jiangchuan and Wang, Zhi},
title = {Towards cloud-edge collaborative online video analytics with fine-grained serverless pipelines},
year = {2021},
isbn = {9781450384346},
publisher = {ACM},
doi = {10.1145/3458305.3463377},
abstract = {The ever-growing deployment scale of surveillance cameras and the users' increasing appetite for real-time queries have urged online video analytics. Synergizing the virtually unlimited cloud resources with agile edge processing would deliver an ideal online video analytics system; yet, given the complex interaction and dependency within and across video query pipelines, it is easier said than done. This paper starts with a measurement study to acquire a deep understanding of video query pipelines on real-world camera streams. We identify the potentials and practical challenges towards cloud-edge collaborative video analytics. We then argue that the newly emerged serverless computing paradigm is the key to achieve fine-grained resource partitioning with minimum dependency. We accordingly propose CEVAS, a Cloud-Edge collaborative Video Analytics system empowered by fine-grained Serverless pipelines. It builds flexible serverless-based infrastructures to facilitate fine-grained and adaptive partitioning of cloud-edge workloads for multiple concurrent query pipelines. With the optimized design of individual modules and their integration, CEVAS achieves real-time responses to highly dynamic input workloads. We have developed a prototype of CEVAS over Amazon Web Services (AWS) and conducted extensive experiments with real-world video streams and queries. The results show that by judiciously coordinating the fine-grained serverless resources in the cloud and at the edge, CEVAS reduces 86.9\% cloud expenditure and 74.4\% data transfer overhead of a pure cloud scheme and improves the analysis throughput of a pure edge scheme by up to 20.6\%. Thanks to the fine-grained video content-aware forecasting, CEVAS is also more adaptive than the state-of-the-art cloud-edge collaborative scheme.},
booktitle = {Proc. of the 12th ACM Multimedia Systems Conf.},
pages = {80â€“93},
numpages = {14},
keywords = {video analytics, serverless computing, cloud-edge collaboration},
series = {MMSys '21}
}

@ARTICLE{liu2023COFE,
  author={Liu, Jiagang and Ren, Ju and Zhang, Yongmin and Peng, Xuhong and Zhang, Yaoxue and Yang, Yuanyuan},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Efficient Dependent Task Offloading for Multiple Applications in MEC-Cloud System}, 
  year={2023},
  volume={22},
  number={4},
  pages={2147-2162},
  keywords={Task analysis;Cloud computing;Mobile handsets;Servers;Mobile applications;Computational modeling;Heuristic algorithms;Mobile edge computing;task offloading;dependent tasks;multiple mobile applications}
}

@ARTICLE{wang2022DRLTO,
  author={Wang, Jin and Hu, Jia and Min, Geyong and Zhan, Wenhan and Zomaya, Albert Y. and Georgalas, Nektarios},
  journal={IEEE Transactions on Computers}, 
  title={Dependent Task Offloading for Edge Computing based on Deep Reinforcement Learning}, 
  year={2022},
  volume={71},
  number={10},
  pages={2449-2461},
  keywords={Task analysis;Neural networks;Wireless communication;Energy consumption;Training;Reinforcement learning;Solid modeling;Multi-access edge computing;task offloading;deep reinforcement learning;sequence to sequence neural networks}
}


@inproceedings{zhao2023qos,
  title={QoS-aware Resource Optimization for Hierarchical Cross-Edge Video Analytics},
  author={Zhao, Kongyange and Zhou, Zhi and Ouyang, Tao and Zhao, Mingliao and Wang, Hao and Chen, Xu},
  booktitle={2023 20th Annual IEEE Int. Conf. on Sensing, Communication, and Networking (SECON)},
  pages={519--527},
  year={2023},
  organization={IEEE}
}

@inproceedings{crankshaw2020inferline,
  title={InferLine: latency-aware provisioning and scaling for prediction serving pipelines},
  author={Crankshaw, Daniel and Sela, Gur-Eyal and Mo, Xiangxi and Zumar, Corey and Stoica, Ion and Gonzalez, Joseph and Tumanov, Alexey},
  booktitle={Proc. of the 11th ACM Symp. on Cloud Computing},
  pages={477--491},
  year={2020}
}

@ARTICLE{Ali2020RES,
  author={Ali, Muhammad and Anjum, Ashiq and Rana, Omer and Zamani, Ali Reza and Balouek-Thomert, Daniel and Parashar, Manish},
  journal={IEEE Transactions on Cloud Computing}, 
  title={RES: Real-Time Video Stream Analytics Using Edge Enhanced Clouds}, 
  year={2022},
  volume={10},
  number={2},
  pages={792-804},
  keywords={Cloud computing;Streaming media;Real-time systems;Bandwidth;Pipelines;Edge computing;Deep learning;IoT;edge computing;video stream analytics;real-time analytics;deep learning;software defined networks;big data;cloud computing},
  doi={10.1109/TCC.2020.2991748}}

@inproceedings{coviello2021magicpipe,
  title={Magic-pipe: Self-optimizing video analytics pipelines},
  author={Coviello, Giovanni and Yang, Yi and Rao, Kiran and others},
  booktitle={Proc. of the 22nd Int. Middleware Conf.},
  pages={79--90},
  year={2021}
}

@inproceedings{gokarn2023mosaic,
  title={MOSAIC: Spatially-multiplexed edge AI optimization over multiple concurrent video sensing streams},
  author={Gokarn, I and Sabbella, H and Hu, Y and others},
  booktitle={Proc. of the 14th Conf. on ACM Multimedia Systems},
  pages={278--288},
  year={2023}
}

@inproceedings{wong2024madeye,
  title={MadEye: Boosting Live Video Analytics Accuracy with Adaptive Camera Configurations},
  author={Wong, Mike and others},
  booktitle={Proc. of the 21st USENIX Symp. on Networked Systems Design and Implementation (NSDI 24)},
  year={2024}
}



@inproceedings{zhang2017videostorm,
author = {Haoyu Zhang and Ganesh Ananthanarayanan and Peter Bodik and Matthai Philipose and Paramvir Bahl and Michael J. Freedman},
title = {Live Video Analytics at Scale with Approximation and {Delay-Tolerance}},
booktitle = {14th USENIX Symp. on Networked Systems Design and Implementation},
year = {2017},
isbn = {978-1-931971-37-9},
pages = {377--392},
}

@article{gao2024edgevision,
  title={EdgeVision: Towards Collaborative Video Analytics on Distributed Edges for Performance Maximization},
  author={Gao, Guanyu and Dong, Yuqi and Wang, Ran and others},
  journal={IEEE Transactions on Multimedia},
  year={2024}
}

@article{wang2024dependence,
  title={Dependence-Aware Multi-Task Scheduling for Edge Video Analytics With Accuracy Guarantee},
  author={Wang, Chengzhi and Yang, Peng and Hou, Jiawei and Liu, Zhi and Zhang, Ning},
  journal={IEEE Internet of Things Journal},
  year={2024}
}

@inproceedings{Ran2018DeepDecision,
  author = {X. Ran and H. Chen and X. Zhu and Z. Liu and J. Chen},
  title = {{DeepDecision: A Mobile Deep Learning Framework for Edge Video Analytics}},
  booktitle = {IEEE Conf. on Computer Communications},
  year = {2018},
  pages = {1421--1429},
  keywords = {Machine learning;Computational modeling;Streaming media;Measurement;Real-time systems;Neural networks;Batteries},
  publisher = {IEEE}
}

@article{Qian2023OsmoticGate,
  author = {B. Qian and Z. Wen and J. Tang and Y. Yuan and A. Y. Zomaya and R. Ranjan},
  title = {{OsmoticGate: Adaptive Edge-Based Real-Time Video Analytics for the Internet of Things}},
  journal = {IEEE Transactions on Computers},
  volume = {72},
  number = {4},
  pages = {1178--1193},
  year = {2023},
  doi = {10.1109/TC.2022.3193630},
  keywords = {Streaming media;Visual analytics;Cloud computing;Adaptation models;Servers;Analytical models;Computational modeling;Video processing;offloading;optimization;IoT},
  publisher = {IEEE}
}

@article{Zhao2023EdgeAdaptor,
  author = {K. Zhao and others},
  title = {{EdgeAdaptor: Online Configuration Adaption, Model Selection and Resource Provisioning for Edge DNN Inference Serving at Scale}},
  journal = {IEEE Transactions on Mobile Computing},
  volume = {22},
  number = {10},
  pages = {5870--5886},
  year = {2023},
  doi = {10.1109/TMC.2022.3189186},
  keywords = {Costs;Adaptation models;Computational modeling;Artificial intelligence;Optimization;Internet of Things;Analytical models;Edge intelligence;edge computing;DNN inference serving;online optimization},
  publisher = {IEEE}
}

@inproceedings{Tan2020FastVA,
  author = {T. Tan and G. Cao},
  title = {{FastVA: Deep Learning Video Analytics Through Edge Processing and NPU in Mobile}},
  booktitle = {IEEE Conf. on Computer Communications},
  year = {2020},
  pages = {1947--1956},
  keywords = {Computational modeling;Mobile handsets;Machine learning;Time factors;Mobile applications;Servers;Streaming media},
  publisher = {IEEE}
}

@inproceedings{Sakaushi2018surveillance,
author = {Sakaushi, Airi and Kanai, Kenji and Katto, Jiro and Tsuda, Toshitaka},
booktitle = {2018 IEEE Int. Conf. on Pervasive Computing and Communications Workshops (PerCom Workshops)},
isbn = {978-1-5386-3227-7},
mendeley-groups = {SoulSaver},
pages = {651--656},
publisher = {IEEE},
title = {{Edge-centric Video Surveillance System Based on Event-driven Rate Adaptation for 24-hour Monitoring}},
year = {2018}
}
@inproceedings{Hayashi2022traffic,
author = {Hayashi, Kazuki and Hiromori, Akihito and Yamaguchi, Hirozumi and Suzuki, Masaki and Kitahara, Takeshi},
booktitle = {2022 IEEE Int. Conf. on Pervasive Computing and Communications Workshops},
isbn = {978-1-6654-1647-4},
mendeley-groups = {SoulSaver},
pages = {593--598},
publisher = {IEEE},
title = {{Synthesizing Town-scale Vehicle Mobility from Traffic Surveillance Cameras: A Case Study}},
year = {2022}
}

@inproceedings{Pasandi2020convince,
abstract = {Today, video cameras are deployed in dense for monitoring physical places e.g., city, industrial, or agricultural sites. In the current systems, each camera node sends its feed to a cloud server individually. However, this approach suffers from several hurdles including higher computation cost, large bandwidth requirement for analyzing the enormous data, and privacy concerns. In dense deployment, video nodes typically demonstrate a significant spatio-temporal correlation. To overcome these obstacles in current approaches, this paper introduces CONVINCE, a new approach to look at the network cameras as a collective entity that enables collaborative video analytics pipeline among cameras. CONVINCE aims at 1) reducing the computation cost and bandwidth requirements by leveraging spatiotemporal correlations among cameras in eliminating redundant frames intelligently, and ii) improving vision algorithms' accuracy by enabling collaborative knowledge sharing among relevant cameras. Our results demonstrate that CONVINCE achieves an object identification accuracy of 91%, by transmitting only about 25% of all the recorded frames.},
archivePrefix = {arXiv},
arxivId = {2002.03797},
author = {Pasandi, Hannaneh Barahouei and Nadeem, Tamer},
booktitle = {2020 IEEE Int. Conf. on Pervasive Computing and Communications Workshops, PerCom Workshops 2020},
eprint = {2002.03797},
isbn = {9781728147161},
keywords = {Collaborative Sensing,Edge Computing,Machine Learning,Spatio-temporal Correlations,Video Analytics},
mendeley-groups = {SoulSaver},
pages = {1--5},
publisher = {IEEE},
title = {{CONVINCE: Collaborative Cross-Camera Video Analytics at the Edge}},
year = {2020}
}


@inproceedings{Cui2023DVABatch,
author = {Weihao Cui and Han Zhao and Quan Chen and Hao Wei and Zirui Li and Deze Zeng and Chao Li and Minyi Guo},
title = {{DVABatch}: Diversity-aware {Multi-Entry} {Multi-Exit} Batching for Efficient Processing of {DNN} Services on {GPUs}},
booktitle = {2022 USENIX ATC},
year = {2022},
isbn = {978-1-939133-29-10},
pages = {183--198},
}

@INPROCEEDINGS{Choi2021LazyBatching,
  author={Choi, Yujeong and Kim, Yunseong and Rhu, Minsoo},
  booktitle={2021 IEEE Int. Symp. on High-Performance Computer Architecture (HPCA)}, 
  title={Lazy Batching: An SLA-aware Batching System for Cloud Machine Learning Inference}, 
  year={2021},
  volume={},
  number={},
  pages={493-506},
  keywords={Training;Quality of service;Machine learning;Computer architecture;Throughput;Time factors;computer architecture;machine learning;deep learning;inference server;batching},
  doi={10.1109/HPCA51647.2021.00049}}

@INPROCEEDINGS{Ali2020BATCH,
  author={Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and Smirni, Evgenia},
  booktitle={SC20: Int. Conf. for High Performance Computing, Networking, Storage and Analysis}, 
  title={BATCH: Machine Learning Inference Serving on Serverless Platforms with Adaptive Batching}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  keywords={Adaptive systems;High performance computing;Prototypes;Machine learning;Tools;Optimization;Machine-learning-as-a-service (MLaaS);Inference;Serving;Batching;Cloud;Serverless;Service Level Objective (SLO);Cost-effective;Optimization;Modeling;Prediction},
  doi={10.1109/SC41405.2020.00073}}

@inproceedings{Zhang2023SHEPHERD,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
title = {{SHEPHERD}: Serving {DNNs} in the Wild},
booktitle = {20th USENIX Symp. on Networked Systems Design and Implementation},
year = {2023},
isbn = {978-1-939133-33-5},
pages = {787--808},
}

@inproceedings{Choi2022Gpulet,
author = {Seungbeom Choi and Sunho Lee and Yeonjae Kim and Jongse Park and Youngjin Kwon and Jaehyuk Huh},
title = {Serving Heterogeneous Machine Learning Models on {Multi-GPU} Servers with {Spatio-Temporal} Sharing},
booktitle = {2022 USENIX ATC},
year = {2022},
isbn = {978-1-939133-29-53},
pages = {199--216}
}

@inproceedings{Gujarati2020Clockwork,
author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
title = {Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up},
booktitle = {14th USENIX Symp. on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {443--462}
}

@inproceedings{Wu2020Irina,
author = {Wu, Xiaorui and Xu, Hong and Wang, Yi},
title = {Irina: Accelerating DNN Inference with Efficient Online Scheduling},
year = {2020},
isbn = {9781450388764},
publisher = {ACM},
doi = {10.1145/3411029.3411035},
abstract = {DNN inference is becoming prevalent for many real-world applications. Current machine learning frameworks usually schedule inference tasks with the goal of optimizing throughput under predictable workloads and task arrival patterns. Yet, inference workloads are becoming more dynamic with bursty queries generated by various video analytics pipelines which run expensive inference only on a fraction of video frames. Thus it is imperative to optimize the completion time of these unpredictable queries and improve customer experience. We propose the preliminary design of the first online inference task scheduling system, called Irina, that takes completion time under unpredictable workload as its primary objective. Irina augments the design space of inference task scheduling with three new strategies, namely batching, stacking, and preemption, in order to more flexibly schedule the tasks and reduce overall latency. Simulation results with empirical inference execution data shows that Irina can improve average task completion time by 1.3xâ€“2.5x over TensorFlow Serving scheduling.},
booktitle = {Proc. of the 4th Asia-Pacific Workshop on Networking},
pages = {36â€“43},
numpages = {8},
series = {APNet '20}
}

@inproceedings{Mendoza2021Interferenceendoza2021Interference,
author = {Mendoza, Daniel and Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
title = {Interference-Aware Scheduling for Inference Serving},
year = {2021},
isbn = {9781450382984},
publisher = {ACM},
abstract = {Machine learning inference applications have proliferated through diverse domains such as healthcare, security, and analytics. Recent work has proposed inference serving systems for improving the deployment and scalability of models. To improve resource utilization, multiple models can be co-located on the same backend machine. However, co-location can cause latency degradation due to interference and can subsequently violate latency requirements. Although interference-aware schedulers for general workloads have been introduced, they do not scale appropriately to heterogeneous inference serving systems where the number of co-location configurations grows exponentially with the number of models and machine types.This paper proposes an interference-aware scheduler for heterogeneous inference serving systems, reducing the latency degradation from co-location interference. We characterize the challenges in predicting the impact of co-location interference on inference latency (e.g., varying latency degradation across machine types), and identify properties of models and hardware that should be considered during scheduling. We then propose a unified prediction model that estimates an inference model's latency degradation during co-location, and develop an interference-aware scheduler that leverages this predictor. Our preliminary results show that our interference-aware scheduler achieves 2\texttimes{} lower latency degradation than a commonly used least-loaded scheduler. We also discuss future research directions for interference-aware schedulers for inference serving systems.},
booktitle = {Proc. of the 1st Workshop on Machine Learning and Systems},
pages = {80â€“88},
numpages = {9},
keywords = {machine learning, interference-aware scheduling, inference serving, heterogeneity, cloud computing},
series = {EuroMLSys '21}
}

@inproceedings{Hu2021Scrooge,
author = {Hu, Yitao and Ghosh, Rajrup and Govindan, Ramesh},
title = {Scrooge: A Cost-Effective Deep Learning Inference System},
year = {2021},
isbn = {9781450386388},
publisher = {ACM},
abstract = {Advances in deep learning (DL) have prompted the development of cloud-hosted DL-based media applications that process video and audio streams in real-time. Such applications must satisfy throughput and latency objectives and adapt to novel types of dynamics, while incurring minimal cost. Scrooge, a system that provides media applications as a service, achieves these objectives by packing computations efficiently into GPU-equipped cloud VMs, using an optimization formulation to find the lowest cost VM allocations that meet the performance objectives, and rapidly reacting to variations in input complexity (e.g., changes in participants in a video). Experiments show that Scrooge can save serving cost by 16-32\% (which translate to tens of thousands of dollars per year) relative to the state-of-the-art while achieving latency objectives for over 98\% under dynamic workloads.},
booktitle = {Proc. of the ACM Symp. on Cloud Computing},
pages = {624â€“638},
numpages = {15},
keywords = {deep learning inference, auto-scaling, Cloud computing},
series = {SoCC '21}
}

@inproceedings{Romero2021Infaas,
author = {Francisco Romero and Qian Li and Neeraja J. Yadwadkar and Christos Kozyrakis},
title = {{INFaaS}: Automated Model-less Inference Serving},
booktitle = {2021 USENIX Annual Technical Conf. (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {397--411},
publisher = {USENIX Association}
}

@inproceedings{Crankshaw2017Clipper,
author = {Daniel Crankshaw and Xin Wang and Guilio Zhou and Michael J. Franklin and Joseph E. Gonzalez and Ion Stoica},
title = {Clipper: A {Low-Latency} Online Prediction Serving System},
booktitle = {14th USENIX Symp. on Networked Systems Design and Implementation (NSDI 17)},
year = {2017},
isbn = {978-1-931971-37-9},
pages = {613--627}
}

@article{patrikar2022anomaly,
  title={Anomaly detection using edge computing in video surveillance system},
  author={Patrikar, Devashree R and Parate, Mayur Rajaram},
  journal={Int. Journal of Multimedia Information Retrieval},
  volume={11},
  number={2},
  pages={85--110},
  year={2022},
  publisher={Springer}
}

@software{Jocher2020YOLOv5,
author = {Jocher, Glenn},
doi = {10.5281/zenodo.3908559},
license = {AGPL-3.0},
title = {{YOLOv5 by Ultralytics}},
url = {https://github.com/ultralytics/yolov5},
version = {7.0},
year = {2020}
}

@inproceedings{hu2021rim,
  title={Rim: Offloading inference to the edge},
  author={Hu, Yitao and Pang, Weiwu and Liu, Xiaochen and Ghosh, Rajrup and Ko, Bongjun and Lee, Wei-Han and Govindan, Ramesh},
  booktitle={Proc. of the Int. Conf. on Internet-of-Things Design and Implementation},
  pages={80--92},
  year={2021}
}

@inproceedings{romero2021llama,
  title={Llama: A heterogeneous \& serverless framework for auto-tuning video analytics pipelines},
  author={Romero, Francisco and Zhao, Mark and Yadwadkar, Neeraja J and Kozyrakis, Christos},
  booktitle={Proc. of the ACM Symp. on cloud computing},
  pages={1--17},
  year={2021}
}

@misc{Ananthanarayanan2020rocket,
booktitle = {Microsoft's Internet of Things Blog},
mendeley-groups = {SoulSaver},
pages = {1--6},
title = {Microsoft Rocket for Live Video Analytics},
url = {https://www.microsoft.com/en-us/research/project/live-video-analytics/},
urldate = {2024-09-24},
year = {2020}
}
@inproceedings{Gao2020dlgpumem,
author = {Gao, Yanjie and Liu, Yu and Zhang, Hongyu and Li, Zhengxian and Zhu, Yonghao and Lin, Haoxiang and Yang, Mao},
booktitle = {Proc. of ESEC/FSE},
doi = {10.1145/3368089.3417050},
isbn = {9781450370431},
pages = {1342--1352},
publisher = {ACM},
title = {{Estimating GPU memory consumption of deep learning models}},
year = {2020}
}

@article{Yeung2022colocationinterference,
abstract = {To accelerate the training of Deep Learning (DL) models, clusters of machines equipped with hardware accelerators such as GPUs are leveraged to reduce execution time. State-of-the-art resource managers are needed to increase GPU utilization and maximize throughput. While co-locating DL jobs on the same GPU has been shown to be effective, this can incur interference causing slowdown. In this article we propose Horus: an interference-aware and prediction-based resource manager for DL systems. Horus proactively predicts GPU utilization of heterogeneous DL jobs extrapolated from the DL model's computation graph features, removing the need for online profiling and isolated reserved GPUs. Through micro-benchmarks and job co-location combinations across heterogeneous GPU hardware, we identify GPU utilization as a general proxy metric to determine good placement decisions, in contrast to current approaches which reserve isolated GPUs to perform online profiling and directly measure GPU utilization for each unique submitted job. Our approach promotes high resource utilization and makespan reduction; via real-world experimentation and large-scale trace driven simulation, we demonstrate that Horus outperforms other DL resource managers by up to 61.5 percent for GPU resource utilization, 23.7-30.7 percent for makespan reduction and 68.3 percent in job wait time reduction.},
author = {Yeung, Gingfung and Borowiec, Damian and Yang, Renyu and Friday, Adrian and Harper, Richard and Garraghan, Peter},
issn = {15582183},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Cloud computing,Deep learning,Distributed systems,GPU utilization,Interference,Workload prediction},
mendeley-groups = {SoulSaver},
number = {1},
pages = {88--100},
title = {{Horus: Interference-aware and prediction-based scheduling in deep learning systems}},
volume = {33},
year = {2022}
}
@article{Wu2022colocationinference,
abstract = {Deep neural networks (DNNs) have become a critical component for inference in modern mobile applications, but the efficient provisioning of DNNs is non-trivial. Existing mobile-and server-based approaches compromise either the inference accuracy or latency. Instead, a hybrid approach can reap the benefits of the two by splitting the DNN at an appropriate layer and running the two parts separately on the mobile and the server respectively. Nevertheless, the DNN throughput in the hybrid approach has not been carefully examined, which is particularly important for edge servers where limited compute resources are shared among multiple DNNs. This article presents HiTDL, a runtime framework for managing multiple DNNs provisioned following the hybrid approach at the edge. HiTDL's mission is to improve edge resource efficiency by optimizing the combined throughput of all co-located DNNs, while still guaranteeing their SLAs. To this end, HiTDL first builds comprehensive performance models for DNN inference latency and throughout with respect to multiple factors including resource availability, DNN partition plan, and cross-DNN interference. HiTDL then uses these models to generate a set of candidate partition plans with SLA guarantees for each DNN. Finally, HiTDL makes global throughput-optimal resource allocation decisions by selecting partition plans from the candidate set for each DNN via solving a fairness-aware multiple-choice knapsack problem. Experimental results based on a prototype implementation show that HiTDL improves the overall throughput of the edge by 4.3Ã— compared with the state-of-the-art.},
author = {Wu, Jing and Wang, Lin and Pei, Qiangyu and Cui, Xingqi and Liu, Fangming and Yang, Tingting},
issn = {15582183},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Deep learning inference,edge computing,resource allocation,systems for machine learning},
mendeley-groups = {SoulSaver},
number = {12},
pages = {4499--4514},
title = {{HiTDL: High-Throughput Deep Learning Inference at the Hybrid Mobile Edge}},
volume = {33},
year = {2022}
}


@inproceedings{Cox2021interlayers,
author = {Cox, Bart and Galjaard, Jeroen and Ghiassi, Amirmasoud and Birke, Robert and Chen, Lydia Y.},
booktitle = {2021 IEEE Int. Conf. on Pervasive Computing and Communications},
isbn = {978-1-6654-0418-1},
mendeley-groups = {SoulSaver},
pages = {1--10},
title = {{Masa: Responsive Multi-DNN Inference on the Edge}},
year = {2021}
}

@inproceedings{Mendula2024deviceserversplit,
author = {Mendula, Matteo and Bellavista, Paolo and Levorato, Marco and {de Guevara Contreras}, Sharon Ladron},
booktitle = {2024 IEEE Int. Conf. on Pervasive Computing and Communications (PerCom)},
doi = {10.1109/PerCom59722.2024.10494426},
isbn = {979-8-3503-2603-1},
mendeley-groups = {SoulSaver},
pages = {47--56},
title = {{Furcifer: a Context Adaptive Middleware for Real-world Object Detection Exploiting Local, Edge, and Split Computing in the Cloud Continuum}},
year = {2024}
}



@inproceedings{Sahu2023healthcare,
author = {Sahu, Sujit Kumar and Ruscelli, Anna Lina and Cecchetti, Gabriele and Gharbaoui, Molka and Castoldi, Piero},
booktitle = {2023 IEEE Int. Conf. on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)},
isbn = {978-1-6654-5381-3},
mendeley-groups = {SoulSaver},
pages = {122--127},
title = {{A perspective of telemedicine videostreaming systems for emergency care}},
year = {2023}
}

@inproceedings{Anjum2024surveillance,
author = {Anjum, Khizar and Chowdhury, Tahmeed and Mandava, Sreeram and Piccoli, Benedetto and Pompili, Dario},
booktitle = {2024 IEEE Int. Conf. on Pervasive Computing and Communications (PerCom)},
isbn = {979-8-3503-2603-1},
mendeley-groups = {SoulSaver},
pages = {11--17},
title = {{Leveraging On-Board UAV Motion Estimation for Lightweight Macroscopic Crowd Identification}},
year = {2024}
}

@inproceedings{Kumrai2020activityrecognition,
author = {Kumrai, Teerawat and Korpela, Joseph and Maekawa, Takuya and Yu, Yen and Kanai, Ryota},
booktitle = {2020 IEEE Int. Conf. on Pervasive Computing and Communications (PerCom)},
isbn = {978-1-7281-4657-7},
mendeley-groups = {SoulSaver},
pages = {1--10},
publisher = {IEEE},
title = {{Human Activity Recognition with Deep Reinforcement Learning using the Camera of a Mobile Robot}},
year = {2020}
}


@inproceedings{Bicocchi2012activityrecognition,
author = {Bicocchi, Nicola and Lasagni, Matteo and Zambonelli, Franco},
booktitle = {2012 IEEE Int. Conf. on Pervasive Computing and Communications},
isbn = {978-1-4673-0258-6},
mendeley-groups = {SoulSaver},
pages = {48--56},
publisher = {IEEE},
title = {{Bridging vision and commonsense for multimodal situation recognition in pervasive systems}},
year = {2012}
}

@inproceedings{raca2020beyond,
  title={Beyond throughput, the next generation: A 5G dataset with channel and context metrics},
  author={Raca, Darijo and Leahy, Dylan and Sreenan, Cormac J and Quinlan, Jason J},
  booktitle={Procs. of the 11th ACM multimedia systems Conf. },
  pages={303--308},
  year={2020}
}

@inproceedings{Demidovskij2020openvino,
author = {Demidovskij, Alexander and Tugaryov, Artyom and Suvorov, Alexander and Tarkan, Yaroslav and Fatekhov, Marat and Salnikov, Igor and Kashchikhin, Andrey and Golubenko, Vladimir and Dedyukhina, Galina and Alborova, Alina and Palmer, Ryan and Fedorov, Mikhail and Gorbachev, Yury},
booktitle = {2020 IEEE 32nd Int. Conf. on Tools with Artificial Intelligence (ICTAI)},
isbn = {978-1-7281-9228-4},
mendeley-groups = {SoulSaver/Shepherd},
pages = {661--668},
publisher = {IEEE},
title = {{OpenVINO Deep Learning Workbench: A Platform for Model Optimization, Analysis and Deployment}},
year = {2020}
}
@inproceedings{Zhou2022trt,
author = {Zhou, Yuxiao and Yang, Kecheng},
booktitle = {Proc. of HPCC},
doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00299},
isbn = {979-8-3503-1993-4},
mendeley-groups = {SoulSaver/Shepherd},
pages = {2011--2018},
publisher = {IEEE},
title = {{Exploring TensorRT to Improve Real-Time Inference for Deep Learning}},
year = {2022}
}
@inproceedings{Kochura2020tpu,
author = {Kochura, Yuriy and Gordienko, Yuri and Taran, Vlad and Gordienko, Nikita and Rokovyi, Alexandr and Alienin, Oleg and Stirenko, Sergii},
booktitle = {Proc. of ICCSEEA},
mendeley-groups = {SoulSaver/Shepherd},
pages = {658--668},
title = {{Batch Size Influence on Performance of Graphic and Tensor Processing Units During Training and Inference Phases}},
year = {2020}
}
@inproceedings{Park2022onnx,
author = {Park, Subin and Choi, Jaeghang and Lee, Kyungyong},
booktitle = {Proc. of the Eighth Int. Workshop on Serverless Computing},
doi = {10.1145/3565382.3565878},
isbn = {9781450399272},
mendeley-groups = {SoulSaver/Shepherd},
pages = {1--6},
publisher = {ACM},
title = {{All-You-Can-Inference: Serverless DNN Model Inference Suite}},
year = {2022}
}

@article{Silfa2022rnn,
abstract = {Recurrent Neural Network (RNN) inference exhibits low hardware utilization due to the strict data dependencies across time-steps. Batching multiple requests can increase throughput. However, RNN batching requires a large amount of padding since the batched input sequences may vastly differ in length. Schemes that dynamically update the batch every few time-steps avoid padding. However, they require executing different RNN layers in a short time span, decreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and energy-efficient batching scheme tailored to RNN accelerators. It consists of a runtime system and effective hardware support. The runtime concatenates multiple sequences to create large batches, resulting in substantial energy savings. Furthermore, the accelerator notifies it when the evaluation of an input sequence is done. Hence, a new input sequence can be immediately added to a batch, thus largely reducing the amount of padding. E-BATCH dynamically controls the number of time-steps evaluated per batch to achieve the best trade-off between latency and energy efficiency for the given hardware platform. We evaluate E-BATCH on top of E-PUR and TPU. E-BATCH improves throughput by 1.8Ã— and energy efficiency by 3.6Ã— in E-PUR, whereas in TPU, it improves throughput by 2.1Ã— and energy efficiency by 1.6Ã—, over the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {2009.10656},
author = {Silfa, Franyell and Arnau, Jose Maria and Gonz{\'{a}}lez, Antonio},
doi = {10.1145/3499757},
eprint = {2009.10656},
issn = {15443973},
journal = {ACM Transactions on Architecture and Code Optimization},
keywords = {Batching,Hardware accelerators,Long short term memory,Recurrent neural network},
mendeley-groups = {SoulSaver/Shepherd},
number = {1},
title = {{E-BATCH: Energy-Efficient and High-Throughput RNN Batching}},
volume = {19},
year = {2022}
}
