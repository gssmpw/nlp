\begin{figure*}[!tb]
    \centering
    \includegraphics[width=\linewidth]{figures/updated/sketch.png}
    % \vspace{-8mm}
    \caption{Evolution of the main characters (\CR{\copyright Menghui}) from sketches (A), to designs (B), intermediate failed assets (C), and final successful assets (D). Building 3D models in Quill may require a mindset shift from using 3D strokes (C) to 3D shapes (D).}
    \label{fig:visual_style}
    \Description{This figure illustrates the evolution of the main characters in a participant’s story. Part A shows initial sketches created in Photoshop, focusing on rough outlines. Part B displays more refined designs in Photoshop, with detailed coloring and shading. Part C highlights the transition to 3D strokes in Quill, where characters exhibit sharp edges. Part D shows the final stage with 3D shapes in Quill, achieving smoother textures and a polished look for the characters.}
\end{figure*}

\begin{figure*}[!tb]
    \centering
     \includegraphics[width=\linewidth]{figures/updated/integration.png}
    \caption{An innovative production strategy that leverages VRChat to simulate the live-action filmmaking process for spatial and temporal coordination (\CR{\copyright Hedi's team}). (A) Creating 3D models for story characters as VRChat avatars. (B) Uploading scenes and avatars to VRChat. (C) Simulating the shooting process in VRChat. With VRChat’s tracking capabilities, two creators control avatars and perform as actors based on the director’s instruction.} 
    \label{fig:vrchat}
    \Description{This figure illustrates an innovative production strategy using VRChat to simulate live-action filmmaking for an animated VR story. Part A showcases asset development, including references and 3D avatars. Part B focuses on scene assembly, where characters and environments are set up. Part C demonstrates story integration, using actors to simulate character behavior, developing the plot, and capturing panoramic video to create immersive scenes.}
\end{figure*}


\subsection{C3: Struggle to Achieve Satisfactory VR Visuals for Artistic Expression}
\label{sec:challenge_visuals}

In the asset development stage, interviewees always wanted better visual aesthetics and richer visual details, such as high texture resolution, vivid lighting and shadows, and lifelike character animations. These were important for their artistic expression and storytelling. However, despite their aspirations, they often struggled to achieve satisfactory visuals in their VR stories.

\textbf{C3-1: Difficult to plan and manage visual details under VR’s real-time performance constraints.}
Compared to pre-rendered films and animations, our interviewees encountered heightened conflicts between visual details and performance constraints. 
They often overlooked performance issues in pursuit of visual perfection and suffered from significant rework. In an extreme case, an interviewee, after dedicating 15 days to a scene (Fig.~\ref{fig:capacity_fidelity}-A), faced inadequate capacity and ultimately abandoned the project.
To mitigate rework, interviewees explored several solutions. 
For example, they might complete the entire story using low-poly models and brushes that consume less memory (Fig.~\ref{fig:capacity_fidelity}-B). If there was remaining capacity, a systematic upscale might follow. However, this approach might still compromise the visual quality and also lead to redundant work.
Another solution was to plan capacity distribution in each scene. For instance, an interviewee listed in advance which objects to animate (Fig.~\ref{fig:capacity_fidelity}-C), and another interviewee set priority levels for different parts of a scene and simplified those parts less visible to viewers (Fig.~\ref{fig:capacity_fidelity}-D). 
However, this approach hindered their desire for expressive freedom and was impractical: 

\q{Planning is often impractical because I tend to create what comes to my mind, especially when I am in a flow state. Besides, Quill allocates 1.5G memory per story, but its actual influence on my work is abstract. What can I draw and to what extent when I use a mixed set of brushes?} (P17)


\textbf{C3-2: Hard to achieve professional-grade visual quality with immersive tools alone.}
Our interviewees with backgrounds in 2D painting or illustration appreciated the immersive tools (e.g., Quill, TiltBrush) that enabled them to freely create animated VR assets and stories in a painterly manner (Fig.~\ref{fig:visual_style}). However, their creations were \q{far from meeting commercial use requirements} (P4), as these tools often fell short in achieving the desired visual quality. This shortfall was mainly due to \q{the limited range of brushes and the tools’ inability to produce rich color transitions and sophisticated lighting effects} (P21). When attempting to enhance the outputs from these immersive tools, our interviewees were pushed to reintegrate with \RR{digital content creation tools (DCCs)} and game engines. Such a shift confronted them with complex computer graphics:

\q{I initially chose Quill for its ease of use to avoid the steep learning curve of DCCs. Yet, when it came to re-rendering the rough surfaces in my Quill story to achieve a smoother appearance while preserving its warm aesthetic, I found myself needing to use shaders in Blender. The limitations of Quill brought me back to the difficulty I had hoped to escape.} (P14) 



\subsection{C5: Missing Integrated Building Blocks for Core VR Story Experiences}
\label{sec:challenge_functionalities}

At the scene assembly and story integration stages, interviewees reported the absence of integrated, narrative-centric building blocks. For example, P7 stated, \q{For game development, I can find various building blocks such as integrated toolkits and prefabs. They provide a suite of features to ease gameplay and level implementation, but not many building blocks that are centered around VR narrative.}

\begin{figure*}[!tb]
    \centering
    % \includegraphics[width=\linewidth]{figures/quantitative_evaluation_support}
    \includegraphics[width=\linewidth]{figures/updated/quan_eval.png}
    % \vspace{-8mm}
    \caption{An example of creator-intended exploration path (A) and POIs (B-E) (\CR{\copyright Julia's team}). These POIs could include (B) interactive objects, (C) narrative-related regions, and (D, E) areas to be noticed during teleportation.} 
    \label{fig:quantitative_evaluation_support}
    \Description{This figure illustrates a creator-intended exploration path (A) and POIs (B-E). Part A shows a path guiding the viewer through various key areas in a scene, marked by numbered points. Part B highlights the first POI, an open book placed on a colorful mat. Part C shows the second POI, a handwritten note. Part D focuses on the third POI, where the viewer's attention is drawn further down a hallway. Part E presents the fourth POI, a large framed picture.}
\end{figure*}

\textbf{C5-1: Hard to plan and coordinate various story elements spatially and temporally.}  
As shown in Fig.~\ref{fig:within_scene_integration}, our interviewees relied on tables and diagrams \q{as communication and planning materials before actual execution in creation software} to describe the status of characters, cameras, lights, and sounds for each plot. These methods were non-intuitive, offered limited granularity for capturing shorter moments, and required much time to compile such textual documents. 
%
Interestingly, we found that a team innovatively used VRChat to alleviate the difficulty of spatial and temporal coordination. They viewed VRChat as a tool, rather than a social VR platform, and incorporated it into their production (Fig.~\ref{fig:vrchat}). 
Specifically, they developed characters as VRChat avatars and assembled scenes in Unity before uploading them to VRChat. During story integration, the team entered their scene in VRChat, utilizing its head, hand, and full-body tracking capabilities to control avatars and act within virtual settings. This method mimicked live-action film shooting, with some members controlling different characters.
However, this method required collaboration among several people and additional tracking devices, which were not available for all creators. 
%
Thus, most interviewees assembled their scenes and weaved the scenes into a story manually using game engines, often complaining that the process was tedious and frustrating:

\q{I'm exhausted from manually aligning the multiple elements in multiple scenes, considering object placement and emotional impact. Though these elements are explicitly connected, I haven't yet found a way to leverage these connections to simplify the alignment.} (P4)

\textbf{C5-2: Hard to switch between multiple narrative perspectives without confusing viewers about their roles.}
Narrative perspectives determine the point of view from which viewers experience animated VR stories. Most interviewees utilized a fixed narrative perspective, either first-person or omniscient. Some interviewees (e.g., P6, P16, P20) favored multiple narrative perspectives. For instance, P20 frequently alternated between first-person and omniscient perspectives, so that \q{viewers can play different roles, ranging from being a candle actively engaging in interactions, to a passive observer gaining a comprehensive understanding of the unfolding events} (P20). However, they reported that achieving multiple perspectives required more than merely shifting camera positions; it necessitated careful preparation to ensure smooth transitions and maintain the viewers' sense of presence. This complexity was evident in P16’s attempt to connect disparate narrative branches by shifting perspectives between a refugee child and a journalist:

\q{To transition viewers into journalist roles, we used cues such as press badges, watching TV news, and mirrors for self-viewing. We also needed to connect these cues with the previous perspective of a refugee child. All these required additional work and risked disrupting a consistent and unbroken narrative experience, so we gave up.} (P16)


\subsection{C6: Tedious Parameter Adjustment for Optimal Audience Experience}
\label{sec:tedious_adjustment}
In the scene assembly and story integration stage, our interviewees needed to fine-tune various design factors and parameters to provide a better audience experience, which was tedious. The design factors included but were not limited to colors, lighting, 3D model sizes, camera movement, pacing, and spatial arrangements.

\textbf{C6-1: Uncertainty in the relationship between various design parameters, desired emotions, and viewer comfort.}
Our interviewees often sought to express strong feelings, such as excitement, fear, and exhilaration in their stories. To achieve this, they used camera movement, special effects, and light changes. However, they were often unsure whether and how their intended emotions could be conveyed well in VR with these means. This uncertainty prompted them to experiment with various solutions for conveying emotions and to tweak various parameters to avoid viewer discomfort, such as motion sickness. P19 shared her struggles:

\q{I wanted to convey tension, anxiety, and regret at different moments. I tried common types of camera movements for these emotions, such as quick cuts or tracking shots, but I felt motion sickness in VR... Is it possible to evoke emotions and create a certain atmosphere with cinematography in VR while preventing discomfort? If yes, how should I set parameters like speed, acceleration, and trajectories to resonate with distinct emotions?} (P19)
