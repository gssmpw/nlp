\subsection{\RR{Future Research Opportunities}}
\label{sec:opportunities}

Based on our findings, we suggest several opportunities for future HCI research to support animated VR story creation. 

\textbf{Investigate narrative intent and view autonomy beyond guiding viewer attention.}
Our results reveal the need for guidelines about effectively conveying creators' intended messages and emotions in VR while preserving viewer autonomy (\textbf{C1-1}). Most existing HCI studies~\cite{rothe2019guidance, schmitz2020directing} equate this objective with directing viewers' attention to where critical plots happen, investigating various guidance mechanisms with audiovisual cues. Although these studies can mitigate frustrations such as out-of-order exploration and missing pivotal moments, they mainly prioritize creators’ narrative intent and proactively shape viewer autonomy. However, our results show that creators are interested in understanding and accommodating viewer autonomy, seeking to harmoniously integrate it into their storylines. In response to this interest, further studies beyond guidance mechanisms are needed. 
A recent HCI study~\cite{aitamurto2021fomo} demonstrates such an example that suggests focusing on viewers’ joy of missing out besides fear of missing out in VR storytelling. 
Future research can draw inspiration from various well-established creative fields, such as theatrical performance~\cite{pope2017geometry, gupta2020roleplaying}, 2D/3D game storytelling~\cite{poretski2022gameLearn}, and data visualizations~\cite{li2023geocamera}, to investigate whether and how existing theories and practices can be adapted to integrate viewer autonomy into VR stories featuring predefined storylines, environmental storytelling, or branch storytelling.

\textbf{Accommodate viewer autonomy in authoring tools.}
To further facilitate the balance between narrative intent and viewer autonomy (\textbf{C1-1}), we recommend that future authoring tools provide explicit features that take audience autonomy into account, particularly during the stages of story creation, previs, and design.
Our results indicate that creators' attempts to guess viewer exploration behaviors during the storyboarding or previs stage have not been notably effective, as evidenced by reported frustrations (Sec.~\ref{sec:challenge_guideline}).
Regrettably, most tools mentioned in Sec.~\ref{sec:related_authoring_story} prioritize assisting creators in designing and prototyping their envisioned content, while overlooking viewer autonomy.
Future authoring tools may embrace a paradigm shift to enhance consideration for viewer autonomy.
Some systems~\cite{nebeling2020xrdirector, rajaram2023reframe} have demonstrated such possibilities.
For example, REFRAME~\cite{rajaram2023reframe} allows creators to anticipate and address potential threats in the early design stage from a user's perspective by personifying various threats as characters in storyboards.
Besides, our results also indicate that there are already some guidelines related to viewer autonomy, but due to not being in VR environments, creators cannot understand them (\textbf{C1-2}), such as using 3D perspective lines in online videos or those guidance mechanisms in academic papers~\cite{rothe2019guidance, schmitz2020directing}. Thus, future authoring tools can start by examining these existing guidelines and investigating innovative features accordingly. 

\textbf{Explore effective representations for describing multi-element plots.}
Our findings highlight the complexity of describing multi-element plots in animated VR stories (\textbf{C2-1}). These plots encompass various aspects, including story elements (e.g., text, narration, camera), themes (e.g., breaking ice, time-travel), emotions (e.g., excitement, tension), spatial configurations (e.g., multi-layering), temporal dynamics (e.g., quick or slow pace), and even viewers' potential viewpoints.
To address this complexity, effective representations are essential. They will not only facilitate communication among creators but also underpin future authoring tools for collecting, managing, retrieving, and using multi-element plot references. 
Several recent HCI studies have proposed some representations~\cite{liu2019view, mahadevan2023tesseract} and showcased potential usages~\cite{stemasov2023sampling, mahadevan2023tesseract} for VR assets and environments.
For example, Tesseract~\cite{mahadevan2023tesseract} employs worlds in miniature to query and locate spatial design moments. 
Stemasov~\etal~\cite{stemasov2023sampling} suggested breaking down and remixing virtual objects by attributes such as colors or motion paths.
However, they do not target more complex animated VR stories, where story elements unfold spatially, temporally, and interactively. Future research may aim to propose representations capable of tracking dynamic elements, capturing interactive storytelling mechanisms, and facilitating spatial and temporal annotations and linkages among elements. Future research may further consider how to design representations of multi-element plots to help creators uncover complex relationships between elements and their contributions to high-level goals like emotional elicitation and narrative comprehension.

\textbf{Investigate context-sensitive algorithms and tools for optimizing visuals.}
Our results show that animated VR story creators face heightened conflicts between their pursuit of satisfactory visuals and performance constraints (\textbf{C3-1}). They also find it hard to achieve certain visual quality because of the inability of existing immersive tools such as Quill (\textbf{C3-2}). 
Solving these issues requires combining HCI research and computer graphics algorithms. 
%
One key area of exploration is how viewers perceive visual flaws in VR storytelling. This calls for HCI studies to investigate the impact of degraded texture quality or model fidelity across different visual elements, such as characters, props, and environments, and how these downgrades affect the narrative experience.
%
Informed by these insights, there is potential for the development of computer graphics algorithms that dynamically adjust visual fidelity in response to narrative demands. For example, while recent advancements in deep learning methods~\cite{li2023generative, hirzle2023xrmeetai} have enhanced 3D asset development processes like modeling and texturing, they often do not consider the specific performance optimization needs of VR devices. Embedding these insights into these methods could result in algorithms that selectively adjust visual fidelity, preserving high-quality visuals in the most impactful areas while optimizing resource use in less critical areas, thereby enhancing the narrative without straining VR resources.
%
Furthermore, these algorithms could be leveraged to enhance animated VR authoring tools, guiding creators more intuitively through the optimization process. Such tools could provide context-sensitive suggestions and adjustments. For instance, in dialogue-heavy scenes, optimizations might focus on enhancing character facial expressions while simplifying background elements. Conversely, in action-intensive scenes, the emphasis could be on maintaining fluid motion and visual coherence.


\textbf{Utilize connections between story elements for coordination.}
Our results indicate that the process of coordinating multiple elements spatially and temporally is cumbersome (\textbf{C5-1}). Although creators often identify explicit connections (e.g., semantic links, common social activities, and spatial relationships) between story elements, they lack effective tools to manifest these connections. Future research may first figure out what connections are common in animated VR stories. This exploration might draw inspiration from AR story authoring tools~\cite{li2022ARSemantics, li2023humanscene, li2024anicraft, tong2024vistellar}, which utilize semantic links~\cite{li2022ARSemantics} between virtual objects to distribute story events and employ common social behaviors~\cite{li2023humanscene} to generate character activities. However, these AR-based approaches might not fully address the unique demands of VR, especially in imagined scenes and activities. 
Therefore, further investigation is necessary to adapt and extend these methods for effective VR storytelling.

\textbf{Offer customizable modules for switching between multiple narrative perspectives.}
Our results reveal that VR story creators may want viewers to engage with their stories from different narrative perspectives, but they worry about the risk of disrupting consistent story experiences due to the difficulty in effectively switching between multiple perspectives (\textbf{C5-2}).
To address this issue, future HCI studies need to first understand whether and how viewers can perceive their roles when narrative perspectives change. However, current studies~\cite{bindman2018bunny, gupta2020roleplaying} indicate that viewers struggle to identify with a single role within a consistent perspective, let alone multiple roles across varying perspectives. This significant research gap suggests a need for collaboration between HCI experts and VR storytellers. By working together, they can develop compelling VR stories that incorporate diverse perspectives, thereby setting the stage for more focused experiments. These experiments could investigate which switching mechanisms can alleviate viewers' confusion and enhance their perception of new roles. Subsequently, effective switching mechanisms can guide the design of authoring tools. For instance, these tools could offer templates or modules for common perspective shifts, such as transitioning from a participant role to an observer role, to enable quick customization.


\textbf{Provide computational design support for enhancing emotional impact.}
Our results show that VR story creators need to adjust various design factors, aiming to enhance emotions and viewer comfort. 
This continuous tweaking makes the creation process less pleasant and often confuses creators due to the unclear quantitative relationships between design factors, emotions, and viewer comfort in animated VR stories (\textbf{C6-1}). While many HCI studies have explored how different factors, like spatial complexity~\cite{shin2022spatialcomplexity, shin2019anyroom}, jump cuts~\cite{zhang2024jump}, and perceived roles~\cite{bindman2018bunny}, influence audience experiences, only limited studies~\cite{serrano2017movie, hu2019reducing, xie2024emordle} have delved into modeling these relationships quantitatively. For instance, Hu~\etal~\cite{hu2019reducing} investigated how camera speed, acceleration, and scene depth impact perceived discomfort. To free creators from manual and trial-and-error adjustments by providing computational design support, future research may focus on modeling the relationships among design factors, emotions, and various devices (e.g., desktop and VR environments) in the context of animated VR stories. This could build on existing VR datasets~\cite{xue2021ceap,sitzmann2018saliency}, self-collected datasets~\cite{hu2019reducing}, or generated datasets~\cite{yuan2024generating}. Once these models encompass the myriad of design factors, emotions, and viewer comfort, upcoming VR creation systems could introduce semi-automatic or automatic algorithms (e.g.,~\cite{cha2020enhanced, pavel2017shot, hartmann2020effect}). Such advancements would aid creators in tasks like modulating lighting to convey emotions like fear while ensuring viewer comfort.

\textbf{Design data visualization tools for revising storylines with semantics.}
Our findings indicate that creators lack adequate support for gathering and analyzing data on viewer behaviors, which is crucial for refining storylines and visual cues in their stories (\textbf{C7-3}). Data visualization emerges as a potential solution to this issue. There are existing tools~\cite{nebeling2020mrat, buschel2021miria, hubenschmid2022relive} designed for general XR applications that aid in data collection and analysis. For instance, MIRIA~\cite{buschel2021miria} facilitates in-situ analysis of spatial and temporal interaction, while Relive~\cite{hubenschmid2022relive} enhances data analysis through visualization on desktops and VR headsets. Nonetheless, these tools primarily address static scenes, whereas animated VR stories evolve in both space and time. Moreover, their primary objective is to identify user behavior patterns and answer specific research questions, such as space utilization and social interaction patterns. They do not focus on aiding VR storytellers in understanding viewer engagement, narrative comprehension, or the effectiveness of their storylines. These more abstract goals, involving a semantic understanding of storylines, necessitate more specialized data visualization and analytic tools.