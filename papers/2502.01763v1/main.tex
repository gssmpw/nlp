\documentclass[11pt]{article}
\setlength{\parskip}{0.5\baselineskip}%
\setlength{\parindent}{0pt}

\usepackage{Shorthands_icml}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm, amsmath}
\input{characters}



\title{On The Concurrence of Layer-wise Preconditioning Methods\\ and Provable Feature Learning} 
\author{
Thomas T.\ Zhang\footnotemark[1]\quad
Behrad Moniri\footnotemark[1]\quad
Ansh Nagwekar\quad
Faraz Rahman\quad
Anton Xue\\[0.2cm]
Hamed Hassani\quad
Nikolai Matni\\[0.4cm]
{\normalsize University of Pennsylvania}
}
\date{}

\usepackage{natbib}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newif\ifshort
\shorttrue
\usepackage[hang,flushmargin]{footmisc}

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}} %
\footnotetext[1]{Equal Contribution. Correspondence to: \texttt{\{ttz2, bemoniri\}@seas.upenn.edu}\vspace{0.05cm}}
\renewcommand{\thefootnote}{\arabic{footnote}} %

\let\cite\citep

\begin{abstract}
Layer-wise preconditioning methods are a family of memory-efficient optimization algorithms that introduce preconditioners per axis of each layer's weight tensors. These methods have seen a recent resurgence, demonstrating impressive performance relative to entry-wise (``diagonal'') preconditioning methods such as Adam(W) on a wide range of neural network optimization tasks. Complementary to their practical performance, we demonstrate that layer-wise preconditioning methods are provably necessary from a statistical perspective.
To showcase this, we consider two prototypical models, \textit{linear representation learning} and \textit{single-index learning}, which are widely used to study how typical algorithms efficiently learn useful \textit{features} to enable generalization.
In these problems, we show SGD is a suboptimal feature learner when extending beyond ideal isotropic inputs $\bfx \sim \mathsf{N}(\mathbf{0}, \mathbf{I})$ and well-conditioned settings typically assumed in prior work.
We demonstrate theoretically and numerically that this suboptimality is fundamental, and that layer-wise preconditioning emerges naturally as the solution. We further show that standard tools like Adam preconditioning and batch-norm only mildly mitigate these issues, supporting the unique benefits of layer-wise preconditioning.
\end{abstract}



\input{arxiv_tex/intro_background}

\input{arxiv_tex/layerwise_derivation}

\input{arxiv_tex/guarantees}

\input{arxiv_tex/experiments}

\input{arxiv_tex/conclusion}

\section{Acknowledgments}
Thomas Zhang and Behrad Moniri gratefully acknowledge gifts from AWS AI to Penn Engineering's ASSET Center for Trustworthy AI. The work of Behrad Moniri and Hamed Hassani is supported by The Institute for Learning-enabled Optimization at Scale (TILOS), under award number NSF-CCF-2112665, and the NSF CAREER award CIF-1943064. Thomas Zhang and Nikolai Matni are supported in part by NSF Award SLES-2331880, NSF CAREER award ECCS-2045834, NSF EECS-2231349, and AFOSR Award FA9550-24-1-0102.

{\small
\bibliography{refs}
\bibliographystyle{icml2025/icml2025}
}

\clearpage
\appendix
\onecolumn
\input{arxiv_tex/appendix_related_work}
\input{arxiv_tex/appendix_lin_rep}
\input{arxiv_tex/appendix_single_index}
\input{arxiv_tex/appendix_KF_derivations}
\input{arxiv_tex/appendix_aux_results}
\input{arxiv_tex/appendix_numerical}

\end{document}
