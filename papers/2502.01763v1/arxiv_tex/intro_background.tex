\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.1cm}

    


Well-designed optimization algorithms have been an enabler to the staggering growth and success of machine learning. For the broader ML community, the \Adam \citep{kingma2014adam} optimizer is likely the go-to scalable and performant choice for most tasks. However, despite its popularity in practice, it has been notoriously challenging to understand \Adam-like optimizers theoretically, especially from a \emph{statistical} (e.g.\ generalization) perspective.\footnote{ To be contrasted with an ``optimization'' perspective, e.g.\ guarantees of convergence to a critical point of  the \emph{training} objective.} In fact, there exist many theoretical settings where \Adam and similar methods underperform in convergence or generalization relative to, e.g., well-tuned \SGD (see e.g.\ \citet{wilson2017marginal, keskar2017improving, reddi2018convergence, gupta2021adam, xie2022adaptive, dereich2024non}), further complicating a principled understanding the role of \Adam-like optimizers in deep learning.
Given these challenges, is there an alternative algorithmic paradigm that is comparable to the \Adam family in practice that is also well-motivated from a statistical learning perspective?
Encouragingly, in a recent large-scale deep learning optimization competition, AlgoPerf \citep{mlcommons2024algoperf}, \Adam and its variants were outperformed in various ``hold-out error per unit-compute''\footnote{See \citet[Section 4.2]{dahl2023benchmarking} for details.} metrics by a method known as \Shampoo \citep{gupta2018shampoo}, a member of a layer-wise ``Kronecker-Factored'' family of preconditioners, formally described in \Cref{sec:K-FAC-approx}, contrasted with ``diagonal'' preconditioning methods like \Adam. 

Notable members of the Kronecker-Factored preconditioning family include \Shampoo and \KFAC \citep{martens2015optimizing}, as well as their many variants and descendants. These algorithms are motivated from an approximation-theoretic perspective, aiming to approximate some \textit{ideal} curvature matrix (e.g.\ the Hessian or Fisher Information) in a way that mitigates the computational and memory challenges associated with second-order algorithms such as Newton's Method (\NM) or Natural Gradient Descent (\NGD). However, towards establishing the benefit of these preconditioners, an approximation viewpoint is bottlenecked by our limited understanding of how the \textit{idealized} second-order methods perform on neural-network learning tasks, even disregarding the computational considerations.
It in fact remains unclear whether these second-order methods are inherently superior to the approximations designed to emulate them. For example, recent work has shown that, surprisingly, \KFAC generally \emph{outperforms} its ideal counterpart \NGD in convergence rate and generalization error on typical deep learning tasks \citep{benzing2022gradient}. Thus, a key question remains:
\begin{quote}
\centering
\textit{How do we explain the performance of Kronecker-Factored preconditioned optimizers?}
\end{quote}
In a seemingly distant area, the learning theory community has been interested in studying the solutions learned by 
abstractions of ``typical'' deep learning set-ups, where the overall goal is to theoretically demonstrate how neural networks \textit{learn} features from data to perform better than classical ``fixed features" methods, e.g.\ kernel machines.
Much of this line of work focuses on analyzing the performance of \SGD on simplified models of deep learning (see e.g.\ \citet{collins2021exploiting, damian2022neural,ba2022high,barak2022hidden,abbe2023sgd,dandi2023learning,berthier2024learning,nichanitransformers, collins2024provable}). Almost invariably, certain innocuous-looking assumptions are made, such as isotropic covariates $\bfx \sim \normal(\mathbf{0}, \bI)$. Under these conditions, \SGD has been shown to exhibit desirable generalization properties. However, some works deviate from these assumptions in specific settings \cite{amari2020does, zhang2023meta}, and suggest that \SGD can exhibit severely suboptimal generalization. 
Thus, toward extending our understanding of feature learning, it seems beneficial to consider a broader family of optimizers. This raises the following question:
\begin{quote}
    \centering
    \textit{What is a \emph{practical} family of optimization algorithms that overcomes the deficiencies of \SGD for standard feature learning tasks?}
\end{quote}
We answer the above two questions by focusing on two prototypical problems used to theoretically study feature learning: \textit{linear representation learning} and \textit{single-index learning}. In both problems, we show that \SGD is clearly suboptimal outside ideal settings, such as when the ubiquitous isotropic data $\normal(\mathbf{0}, \mathbf{I})$ assumption is violated. By inspecting the root cause behind these suboptimalities, we show that Kronecker-Factored preconditioners arise naturally as a \emph{first-principles} solution to these issues. We provide novel non-approximation-theoretic motivations for this class of algorithms, while establishing new and improved learning-theoretic guarantees. We hope that this serves as strong evidence of an untapped synergy between deep learning optimization and feature learning theory.
\paragraph{Contributions.} Here we discuss the main contributions of the paper.
\begin{itemize}[left=0pt]
\vspace{-.7em}

    \item We study the linear representation learning problem under general anisotropic $\bfx \sim \normal(\mathbf{0}, \bSigma_\bfx)$ covariates and show that the convergence of \SGD can be drastically slow, even under mild anisotropy. Also, the convergence rate suffers an undesirable dependence on the ``conditioning'' of the instance even for ideal step-sizes. We arrive at a variant of \KFAC as the natural solution to these deficiencies of \SGD, giving rise to the first \emph{condition-number-free convergence rate} for the problem (Section~\ref{sec:lin_rep}).
    \vspace{0.2cm}
    \item Next, we consider the problem of learning a single-index model using a two-layer neural network in the high-dimensional proportional limit. We show that for anisotropic covariates $\bfx \sim \normal(\mathbf{0}, \bSigma_\bfx)$, \SGD fails to learn useful features, whereas it is known that it learns suitable features in the isotropic setting. Furthermore, we show that \KFAC is a natural fix to \SGD, greatly enhancing the learned features in anisotropic settings (Section~\ref{sec:single_index}).

    \item Lastly, we carefully numerically verify our theoretical predictions. Notably, we confirm the findings in \citet{benzing2022gradient} that full second-order methods heavily underperform \KFAC in convergence rate and stability. We also show standard tools like \Adam-like preconditioning and batch-norm \cite{ioffe2015batch} do not fix the issues we identify, even for our simple models, and may even \emph{hurt} generalization in the latter's case.

    
\end{itemize}

\vspace{-0.3cm}

In addition to the works discussed earlier, we provide extensive related work and background in \Cref{sec:related_work}.
\vspace{-0.4cm}
\paragraph{Notation.} We denote vector quantities by \textbf{bold} lower-case, and matrix quantities by \textbf{bold} upper-case. We use $\odot$ to denote element-wise (Hadamard) product, $\otimes$ for Kronecker product, and $\VEC(\cdot)$ the \emph{column-major} vectorization operator. Positive (semi-)definite matrices are denoted by $\bQ \succ\mem(\succeq)\; \bzero$, and the corresponding partial order $\bP \preceq \bQ \implies \bQ - \bP \succeq \bzero$. We use $\opnorm{\cdot}$, $\norm{\cdot}_F$ to denote the operator (spectral) and Frobenius norms, and $\kappa(\bA) = \smax(\bA)/\smin(\bA)$ denote the condition number. We use $\Ex[f(\bfx)]$ to denote the expectation of $f(\bfx)$, and $\prob[A(\bfx)]$ to denote the probability of event $A(\bfx)$. Given a batch $\scurly{\bfx_i}_{i=1}^n$, we denote the \emph{empirical} expectation $\hatEx[f(\bfx)] = \frac{1}{n}\sum_{i=1}^n f(\bfx_i)$. Given an indexed set of vectors, we use the upper case to denote the (row-wise) stacked matrix, e.g.\ $\bX \triangleq \bmat{\bfx_1 & \cdots & \bfx_n}^\top \in \R^{n \times \dx}$. We reserve $\bSigma$ ($\hatSigma[]$) for (sample) covariance matrices, e.g.\ $\bSigma_\bfx = \Ex[\bfx \bfx^\top]$, $\hatSigma = \hatEx[\bfx \bfx^\top] = \frac{1}{n} \bX^\top \bX$. We use $\lesssim, \gtrsim, \approx$ to omit universal numerical constants, and standard asymptotic notation $o(\cdot), \calO(\cdot), \Omega(\cdot), \Theta(\cdot)$. Lastly, we use the index shorthand $[n] = \scurly{1,\dots,n}$, and subscript $\,_+$ to denote the ``next iterate'', e.g.\ $\bG_+ = \mathrm{Next}(\bG)$.
