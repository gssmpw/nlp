\section{Additional Numerical Results and Details}\label{sec:additional_numerics}

\subsection{Details of the Experiment Setups}
\label{sec:details_exp_setups}
In the experiments, we generate $\bfF_0 \in \R^{\dy \times k}$ with i.i.d. $\normal(0,1)$ entries. Then, for each task $\msf{s}$ we randomly draw a matrix $\bfB_\msf{s} \in \R^{\dy \times \dy}$ and set $\bfF_\star^{\msf{s}}  = \exp\left(0.005  (\bfB_{\msf{s}} - \bfB_{\msf{s}}^\top)\right) \bfF_0$,
where $\exp(\cdot)$ is the matrix exponential. The shared representation matrix $\bfG_\star \in \R^{k \times \dx}$ is generated by sampling uniformly from the space of row-orthonormal matrices in $\R^{k \times \dx}$.

We consider two settings for the covariance matrices $\bSigma_{\bfx, {\msf{s}}}$;  the \textit{low-anisotropic}, and the \textit{high-anisotropic} settings. In the low-anisotropic setting, we define $\bE =  5\, \bI_{\dx} + \bN$ where $\bN \in \R^{\dx \times \dx}$ has i.i.d. $\normal(0,1)$ entries, and set $\bSigma_{\bfx,\msf{s}} = 0.5\,(\bE + \bE^\top)$. For the high-anisotropic setting, we first sample uniformly a rotation matrix $\bO \in \R^{\dx \times \dx}$ and set $\bSigma_{\bfx, ,\msf{s}} = \bO \bD \bO^\top$ where $\bD = \diag(\texttt{logspace}(0,5,\dx))$. In the experiments for the main paper, we always consider the high-anisotropic setting.

In the following experiments, in addition to the data generation process in equation~\eqref{eq:bern_data} used in the experiment in the main paper, we also consider a Gaussian data setup where samples for task $\msf{s}$ are generated according to
\begin{align}
    \label{eq:bern_data_gaussian}
    &\bfy_i ^{\msf{s}} = \mbf{F}_\star^{\msf{s}} \mbf{G}_\star \bfx_i ^{\msf{s}} + \bveps_i ^{\msf{s}}, \quad  \bfx_{i}^{\msf{s}} \sim \normal(\mathbf{0}, \bSigma_{\bfx, \msf{s}}),\quad \bveps_i ^{\msf{s}} \iidsim \normal(0, \sigma_{\ep,\msf{s}} \bfI_{\dy}), \quad \msf{s} \in \{\msf{test}, \msf{train}\}.
\end{align}

\subsection{Additional Experiments}

\subsubsection{Effect of Batch Normalization}
We used the same experiment setting described in Section \ref{sec:numerical_validation} to generate the plots in Figure \ref{fig:appendix_headtohead_plots}. Explicitly, we use data dimension \(\dx = 100\), task dimension \(\dy = 15\), and representation dimension \(k = 8\). We use the same learning rate $10^{-2}$ for each optimizer except for \NGD, in which we used $10^{-4}$. The batch size is \(1024\). In Figure~\ref{fig:batchnorm_subpace_dist} we considered the Uniform data \eqref{eq:bern_data} with high anisotropy. Here, we consider the other three setting: Uniform data \eqref{eq:bern_data} with low anisotropy, Gaussian data \eqref{eq:bern_data_gaussian} with low anisotropy, and Gaussian data \eqref{eq:bern_data_gaussian} with high anisotropy.


\begin{figure*}[ht]
\centering
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/combined_metrics_2000iters_10exps_bern_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/combined_metrics_2000iters_10exps_gauss_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/combined_metrics_2000iters_10exps_gauss_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\caption{
The effect of batch normalization (on \texttt{AMGD}) vs. \KFAC in our experiment settings
\textbf{(Left)} Uniform with low anisotropy.
\textbf{(Middle)} Gaussian with low anisotropy.
\textbf{(Right)} Gaussian with high anisotropy.
}
\label{fig:batchnorm_full}
\end{figure*}


As discussed in the main paper \Cref{sec:lin_rep_BN}, we expect \texttt{AMGD} with batch-norm to converge in training loss but to perform poorly with respect to the subspace distance from the optimal in settings in the case with high anistropy \textbf{(Right)}. However, in the experiment settings with low anisotropy \textbf{(Left and Center)}, we expect reasonable performance from this algorithm because $\rowsp(\bfG_\star \bSigma_{\bfx})$ is close to the target $\rowsp(\bfG_\star)$.


\ifshort
  \subsubsection{Learning Rate Sweep}
  \label{sec:sweep}
    We further test the performance of each learning algorithm at different learning rates from \(10^{-6}, 10^{-5.5}, \ldots, 10^{-0.5}, 10^{0}\), with results shown in Figure~\ref{fig:lr_sweep}, where we plot the subspace distance at \(1000\) iterations for different algorithms.
    If the algorithm encounters numerical instability, then we report the subspace distance as the maximal value of \(1.0\). We observe that \KFAC and \DFW  coverage to a solution with small subspace distance to the true representation for a wide range of step sizes, whereas the set of suitable learning rates for other algorithms is much narrower. Furthermore, we observe the poor performance of various algorithms in \Cref{fig:headtohead} and \Cref{fig:appendix_headtohead_plots} is not due to specific choice of learning rate.
    
    
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/lr_sweep_1000iters_10exps_bern_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
    \caption{The subspace distance of representations learned by different algorithms after $1000$ iterations and the true representation as a function of learning rate.}
    \label{fig:lr_sweep}
    \end{figure}
\else
\fi

\subsubsection{Head-to-Head Experiments}
We again consider the same experimental setting used for Figure~\ref{fig:headtohead}. In particular, we use data dimension \(\dx = 100\), task dimension \(\dy = 15\), and representation dimension \(k = 8\). We use the same learning rate $10^{-2}$ for each optimizer except for \NGD optimizer, in which we used $10^{-4}$. The batch size is \(1024\). In Figure~\ref{fig:headtohead} we considered the Uniform data \eqref{eq:bern_data} with high anisotropy. Here, we consider the other three setting: Uniform data \eqref{eq:bern_data} with low anisotropy, Gaussian data \eqref{eq:bern_data_gaussian} with low anisotropy, and Gaussian data \eqref{eq:bern_data_gaussian} with high anisotropy.  We plot the training loss, subspace distance to the ground truth shared representation, and the transfer loss obtained by different algorithms. See \Cref{fig:appendix_headtohead_plots}. We observe that in all three settings, various algorithms converge in training loss. In the case with high anisotropy (second row), methods other than \KFAC do not converge to the optimal representation in subspace distance and transfer loss. However, in the low anisotropy settings (first and third rows), the performance of other algorithms also improve, but are notably still suboptimal relative to \KFAC, confirming the theoretical results showing that anisotropy is a root cause behind the sub-optimality of prior algorithms and analysis.
\begin{figure*}
\centering
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_train_loss_2000iters_10exps_gauss_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_subspace_dist_2000iters_10exps_gauss_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_transfer_loss_2000iters_10exps_gauss_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}
\caption{Gaussian with low anisotropy}

\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_train_loss_2000iters_10exps_gauss_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_subspace_dist_2000iters_10exps_gauss_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_transfer_loss_2000iters_10exps_gauss_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}
\caption{Gaussian with high anisotropy}

\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_train_loss_2000iters_10exps_bern_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_subspace_dist_2000iters_10exps_bern_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_transfer_loss_2000iters_10exps_bern_lowanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}
\caption{Bernoulli with low anisotropy}

\caption{From \textbf{left} to \textbf{right}: the training loss, subspace distance, and transfer loss induced by various algorithms on a linear representation learning task.}
\label{fig:appendix_headtohead_plots}

\end{figure*}

