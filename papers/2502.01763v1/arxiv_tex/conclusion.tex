\vspace{-0.35cm}
\section{Discussion}
\vspace{-0.2cm}
We study two models of feature learning in which we identify key issues of \SGD-based feature learning approaches when departing from ideal settings. We then present Kronecker-Factored preconditioning---recovering variants of \KFAC---to provably overcome these issues and derive improved guarantees. Our experiments on these simple models also confirm the suboptimality of full second-order methods, as well as the marginal benefit of \Adam preconditioning and data normalization. We believe that analyzing properties of statistical learning problems can lead to fruitful insights into optimization and normalization schemes.%



