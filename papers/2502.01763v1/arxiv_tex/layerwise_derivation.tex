\vspace{-0.3cm}
\section{Kronecker-Factored Approximation}
\label{sec:K-FAC-approx}

One of the longest-standing research efforts in optimization literature is dedicated to understanding the role of (local) curvature toward accelerating convergence rates of optimization methods. An example is Newton's method, where the curvature matrix (Hessian) serves as a preconditioner of the gradient, enabling one-shot convergence in quadratic optimization, in which gradient descent enjoys at best a linear convergence rate dictated by the conditioning of the problem.
However, for high-dimensional variables, computing and storing the full curvature matrix is often infeasible. Thus enter Quasi-Newton and (preconditioned) Conjugate Gradient methods, where the goal is to reap the benefits of curvature under  computational or structural specifications, such as \{block-diagonal, low-rank, sparsity, etc.\} constraints (e.g.\ \BFGS family \citep{goldfarb1970family, liu1989limited, nocedal1999numerical}), or accessing the curvature matrix only through matrix-vector products (see e.g.\ \citet{pearlmutter1994fast,schraudolph2002fast,martens2010deep}). 

Nevertheless, the use of these methods for neural network optimization introduces new considerations. Consider an $L$-layer fully-connected neural network (omitting biases) $$f_{\btheta}(\bfx) \triangleq \bW_L \sigma (\bW_{L-1} \cdots \sigma(\bW_1 \bfx) \cdots ),$$
where $\bW_\ell \in \R^{d \times d},\; \ell \in [L]$ and $\btheta \in \R^{L d^2}$ is the concatenation of $\btheta_\ell \triangleq \VEC(\bW_\ell)$, $\ell \in [L]$.
Firstly, establishing convergence of \SGD \citep{arora2019implicit}, \NGD \citep{zhang2019fast}, or Gauss-Newton \citep{cai2019gram} (or their corresponding gradient flows) to global minima of the \emph{training} objective is non-trivial, as optimization over $\btheta$ is non-convex. Moreover, these results do not directly characterize the structure of the resulting features learned by the algorithms.
Secondly, on the practical front, full preconditioners on $\btheta$ require memory $\calO(L^2 d^4)$, which grows prohibitively with depth and width. Block-diagonal approximations (where one curvature block $\bM_{\ell} \in \R^{d^2 \times d^2}$ corresponds to a layer $\btheta_\ell$) still require $\calO(L d^4)$. Thus, entry-wise preconditioning as in \Adam, with footprint $\calO(L d^2) \approx \dim(\btheta)$, is usually considered the only scalable class of preconditioners.


However, a distinct notion of ``Kronecker-Factored'' preconditioning emerged approximately concurrently with \Adam, with representative examples such as \KFAC and \Shampoo. As its name suggests, since full block-diagonal approximations are too expensive, a Kronecker-Factored approximation is made instead, where $\bM_\ell^{-1} \nabla_{\btheta_\ell} \calL(\btheta) = (\bQ_\ell \otimes \bP_\ell)^{-1}  \nabla_{\btheta_\ell} \calL(\btheta)$, $\bP_\ell, \bQ_\ell \succeq \bzero$. Using properties of the Kronecker product (see \Cref{lem:kron_properties}), this has the convenient interpretation of pre- and post-multiplying the weights in their \textit{matrix form}:
\begin{align}\label{eq:kf_precond}
    (\bQ_\ell \otimes \bP_\ell)^{-1}  \nabla_{\btheta_\ell} \calL(\btheta) \iff \bP_\ell^{-1}  \nabla_{\bW_\ell} \calL(\btheta) \bQ_\ell^{-1}.
\end{align}
As such, the memory requirement of Kronecker-Factored layer-wise preconditioning is $\calO(Ld^2)$, matching that of entry-wise preconditioning.
The notion of curvature differs from case to case, e.g., for \KFAC, this is the Fisher Information matrix corresponding to the distribution parameterized by $f_{\btheta}(\bfx)$, whereas for \Shampoo this is the full-matrix Adagrad preconditioner, in turn closely related to the Gauss-Newton matrix.\footnote{This is itself a positive-definite approximation of the Hessian.} We provide some sample derivations and background in \Cref{sec:KF_derivations}. However, as aforementioned, an approximation viewpoint falls short of explaining the practical performance of Kronecker-Factored methods, 
as they typically converge \textit{faster} than their corresponding second-order method \citep{benzing2022gradient} on deep learning tasks.
This motivates understanding the unique benefits of layer-wise preconditioning methods from first principles, which brings us to the following section.











