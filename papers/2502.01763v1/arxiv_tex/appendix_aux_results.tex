\section{Auxiliary Results}\label{sec:aux_results}

\subsection{Properties of Kronecker Product}
Recall the definition of the Kronecker Product: given $\bA \in \R^{m \times n}$, $\bB \in \R^{p \times q}$
\begin{align*}
    \bA \otimes \bB &= \bmat{A_{11} \bB & \cdots & A_{1n} \bB \\ 
    \vdots & \ddots & \vdots \\
    A_{m 1} \bB & \cdots & A_{mn} \bB} \in \R^{mp \times nq}.
\end{align*}
Complementarily, the vectorization operator $\VEC(\bA)$ is defined by stacking the columns of $\bA$ on top of each other (i.e.\ column-major order)
\begin{align*}
    \VEC(\bA) &= \bmat{A_{11} & \cdots & A_{m1} & \cdots & A_{1n} & \cdots & A_{mn}}^\top \in \R^{mn}.
\end{align*}
We now introduce some fundamental facts about the Kronecker Product.
\begin{lemma}[Kronecker-Product Properties]\label{lem:kron_properties}
    The following properties hold:
    \begin{enumerate}
        \item $(\bA \otimes \bB)^{-1} = \bA^{-1} \otimes \bB^{-1}$. Holds for Moore-Penrose pseudoinverse $\;^{\dagger}$ as well.
        \item For size-compliant $\bA, \bB, \bC, \bD$, we have $(\bA \otimes \bB) (\bC \otimes \bD) = (\bA \bC) \otimes (\bB \otimes \bD)$.
        \item $\VEC(\bA\bX\bB) = (\bB^\top \otimes \bA)\VEC(\bX)$.
    \end{enumerate}
\end{lemma}


\subsection{Covariance Concentration}


We often use the following Gaussian covariance concentration result.
\begin{lemma}[Gaussian covariance concentration]\label{lem:gauss_cov_conc}
    Let $\bfx_i \iidsim \normal(\bzero, \bSigma_\bfx)$ for $i = 1,\dots,n$, where $\bfx_i \in \R^d$. Defining the empirical covariance matrix $\hatSigma \triangleq \frac{1}{n}\sum_{i=1}^n \bfx_i \bfx_i^\top$, as long as $n \geq \frac{18.27}{c^2} (d + \log(1/\delta))$, we have with probability at least $1 - \delta$,
    \begin{align*}
        (1-c) \bSigma_\bfx \preceq \hatSigma \preceq (1+c) \bSigma_\bfx.
    \end{align*}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:gauss_cov_conc}]
    The result follows essentially from combining a by-now standard concentration inequality for Gaussian quadratic forms and a covering number argument. To be precise, we observe that
    \begin{align*}
        \opnorm{\hatSigma[\bfx] - \bSigma_\bfx} \leq c\norm{\bSigma_\bfx} \implies (1-c)\bSigma_\bfx \preceq \hatSigma[\bfx] \preceq (1+c)\bSigma_\bfx.
    \end{align*}
    Therefore, it suffices to establish a concentration bound on $\norm{\hatSigma[\bfx] - \bSigma_\bfx}$ and invert for $c\norm{\bSigma_\bfx}$. To do so, we recall a standard covering argument (see e.g.\ \citet[Chapter 4]{vershynin2018high}) yields: given an $\varepsilon$-covering of $\bbS^{d-1}$, $\calN \triangleq \calN(\bbS^{d-1}, \norm{\cdot}_2, \varepsilon)$, the operator norm of a symmetric matrix $\bSigma$ is bounded by
    \begin{align*}
        \norm{\bSigma} &\leq \frac{1}{1 - 2\varepsilon} \max_{\bfu \in \calN} \bfu^\top \bSigma \bfu,
    \end{align*}
    where the corresponding covering number is bounded by:
    \begin{align*}
        \abs{\calN(\bbS^{d-1}, \norm{\cdot}_2, \varepsilon)} &\leq \paren{1 + \frac{2}{\varepsilon}}^d.
    \end{align*}
    As such it suffices to provide a concentration bound on $\bfu^\top \bSigma \bfu$ for each $\bfu \in \calN$ and then union-bound. Toward establishing this, we first state the Gaussian quadratic form concentration bound due to \citet{hsu2012random}, which is in turn an instantiation of a chi-squared concentration bound from \citet{laurent2000adaptive}.
    \begin{proposition}[Prop.\ 1 in \cite{hsu2012random}]\label{prop:hsu_quad_form}
        Let $\bA \in \R^{m \times d}$ be a fixed matrix. Let $\bfg \sim \normal(\bzero, \bI_d)$ be a mean-zero, isotropic Gaussian random vector. For any $\delta \in (0,1)$, we have
        \begin{align*}
            \sfP[\norm{\bA \bfg}^2 > \trace(\bA^\top \bA) + 2\sqrt{\trace((\bA^\top \bA)^2) \log(1/\delta)} + 2\opnorm{\bA^\top \bA} \log(1/\delta)] \leq \delta.
        \end{align*}
    \end{proposition}
    Now, given $\bfu \in \bbS^{d-1}$, setting $\bA = \bfu^\top \bSigma^{1/2}$ such that $\bfu^\top \bSigma^{1/2} \bfg \overset{d}{=} \bfu^\top \bfx$, instantiating \Cref{prop:hsu_quad_form} yields:
    \begin{align*}
        \sfP\brac{\bfu^\top \hatSigma \bfu > \bfu^\top \bSigma_\bfx \bfu + 2\bfu^\top \bSigma_\bfx \bfu \sqrt{\frac{\log(1/\delta)}{n}} + 2\bfu^\top \bSigma_\bfx \bfu \frac{\log(1/\delta)}{n}} \leq \delta.
    \end{align*}
    Put another way, this says with probability at least $1 - \delta$:
    \begin{align*}
        \bfu^\top(\hatSigma-\bSigma) \bfu &\leq 2\bfu^\top \bSigma_\bfx \bfu \paren{\sqrt{\frac{\log(1/\delta)}{n}} + \frac{\log(1/\delta)}{n}}.
    \end{align*}
    Taking a union bound over $\bfu \in \calN$, we get with probability at least $1 - \delta$:
    \begin{align*}
        \max_{\bfu \in \calN} \bfu^\top(\hatSigma-\bSigma) \bfu &\leq \max_{\bfu \in \calN} 2\bfu^\top \bSigma_\bfx \bfu \paren{\sqrt{\frac{\log(\abs{\calN}/\delta)}{n}} + \frac{\log(\abs{\calN}/\delta)}{n}} \\
        &\leq 2 \opnorm{\bSigma_\bfx} \paren{\sqrt{\frac{d\log\paren{1 + \frac{2}{\varepsilon}}+\log(1/\delta)}{n}} + \frac{d\log\paren{1 + \frac{2}{\varepsilon}} + \log(1/\delta)}{n}} \\
        &\leq 4 \sqrt{\log\paren{1 + \frac{2}{\varepsilon}}} \opnorm{\bSigma_\bfx} \sqrt{\frac{d + \log(1/\delta)}{n}},
    \end{align*}    
    as long as $n \geq d\log\paren{1 + \frac{2}{\varepsilon}}+\log(1/\delta)$.
    Chaining together inequalities, this yields with probability at least $1 - \delta$ under the same condition on $n$:
    \begin{align*}
        \opnorm{\hatSigma - \bSigma_\bfx} &\leq \opnorm{\bSigma_\bfx}\frac{2}{1 - \varepsilon}\sqrt{\log\paren{1 + \frac{2}{\varepsilon}}} \sqrt{\frac{d + \log(1/\delta)}{n}}.
    \end{align*}
    Minimizing the RHS for $\varepsilon \approx 0.0605$ yields the result.
\end{proof}


\subsection{Extensions to subgaussianity}\label{appdx:subgaussian}

As previewed, many results can be extended from the Gaussian setting to subgaussian random vectors.
\begin{definition}\label{def:subgaussian}
    A (scalar) random variable $X$ is \emph{subgaussian} with variance proxy $\sigma^2$ if the following holds on its moment-generating function:
    \begin{align*}
        \Ex[\exp(\lambda X)] &\leq \exp\paren{\frac{\lambda^2 \sigma^2}{2}}.
    \end{align*}
    A mean-zero random vector $\bfx \in \R^d$, $\Ex[\bfx] = \bzero$, is \emph{subgaussian} with variance proxy $\sigma^2$ if every linear projection is a $\sigma^2$-subgaussian random variable:
    \begin{align*}
        \Ex[\exp(\lambda \bfv^\top \bfx)] &\leq \exp\paren{\frac{\lambda^2 \norm{\bfv}^2 \sigma^2}{2}}, \quad \text{for all }\bfv \in \R^d.
    \end{align*}

\end{definition}
With this in hand, we may introduce the subgaussian variant of covariance concentration and the Hanson-Wright inequality.

\subsection{Subgaussian Covariance Concentration}

We state the subgaussian variant of \Cref{lem:gauss_cov_conc}, whose proof is structurally the same, replacing the $\chi^2$ random variables with a generic subexponential (c.f.\ \citet[Chapter 2]{vershynin2018high}) random variable, and using a generic Bernstein's inequality rather than the specific $\chi^2$ concentration inequality. The result is qualitatively identical, sacrificing tight/explicit universal numerical constants. The result is relatively standard, and can be found in e.g., \citet[Chapter 5]{vershynin2018high} or \citet[Lemma A.6]{du2020few}.
\begin{lemma}[Subgaussian covariance concentration]
    Let $\bfx_i$ be i.i.d.\ zero-mean $\sigma^2$-subgaussian random vectors for $i = 1,\dots,n$, where $\bfx_i \in \R^d$, and $\Ex[\bfx \bfx^\top ] = \bSigma_\bfx$. Defining the empirical covariance matrix $\hatSigma \triangleq \frac{1}{n}\sum_{i=1}^n \bfx_i \bfx_i^\top$, there exists a universal constant $C_1>0 $ such that with probability at least $1 - 2\delta$:
    \begin{align*}
        \opnorm{\hatSigma - \bSigma_\bfx} &\leq C \sigma^2 \opnorm{\bSigma_\bfx} \paren{\sqrt{\frac{d + \log(1/\delta)}{n}} + \frac{d + \log(1/\delta)}{n}}.
    \end{align*}
    Therefore, as long as $n \geq C_2 \frac{\sigma^2}{c^2} (d + \log(1/\delta))$, we have with probability at least $1 - \delta$,
    \begin{align*}
        (1-c) \bSigma_\bfx \preceq \hatSigma \preceq (1+c) \bSigma_\bfx.
    \end{align*}
\end{lemma}

\subsection{Hanson-Wright Inequality}
We often use the following theorem  to prove the concentration inequality for quadratic forms. A modern proof of this theorem can be found in \citet{rudelson2013hanson}.

\begin{theorem}[Hanson-Wright Inequality \citep{hanson1971bound}]
    \label{thm:hanson-wright}
    Let $\bfx=\left(X_1, \ldots, X_n\right) \in \mathbb{R}^d$ be a random vector with independent sub-gaussian components $X_i$ with $\Ex X_i=0$. Let $\bD$ be an $n \times n$ matrix. Then, for every $t \geq 0$, we have
    $$
    \mathbb{P}\left\{\left|\bfx^{\top} \bD\, \bfx-\Ex\left[ \bfx^{\top} \bD\, \bfx\right]\right|>t\right\} \leq 2 \exp \left[-c \min \left(\frac{t^2}{\|\bD\|_{F}^2}, \frac{t}{\|\bD\|_{\rm op}}\right)\right],
    $$
    where $c$ is a constant that depends only on the subgaussian constants of $X_i$.
\end{theorem}
\subsection{Stein's Lemma}
We use the following simple lemma which is an application of integration by parts for Gaussian integrals.
\begin{lemma}[Stein's Lemma]
    \label{lemma:stein's}
    Let $X$ be a random variables drawn from $\normal(\mu, \sigma^2)$ and $g:\R \to \R$ be a differentiable function. We have $\Ex\left[\,g(X) (X - \mu)\,\right] = \sigma^2\, \Ex \left[\,g'(X)\right]$.    
\end{lemma}

\subsection{Woodbury Matrix Identity}
In the proofs, we use the following elementary identity which states that the inverse of a rank-$k$ correction of a matrix is equal to a rank-$k$ correction to the inverse of the original matrix. 
\begin{theorem}[Woodbury Matrix Identity \citep{woodbury1950inverting}]
    \label{thm:woodbury}
    Let $\bA \in \R^{n \times n}, \bC \in \R^{k \times k}, \bU \in \R^{n \times k}$, and $\bV\in \R^{k \times n}$. The following matrix identity holds:
    \begin{align*}
        (\bA + \bU\bC\bV)^{-1} = \bA^{-1} - \bA^{-1} \bU (\bC^{-1} + \bV \bA^{-1}\bU)^{-1} \bV \bA^{-1},
    \end{align*}
    assuming that the inverse matrices in the expression exist.
\end{theorem}

\subsection{Stieltjes Transform of Empirical Eigenvalue Distribution}
\label{sec:stieltjes}
For a distribution $\mu$ over $\R$, its Stieltjes transform is defined as 
\begin{align*}
    m_\mu(z) = \int\frac{d\mu(x)}{x - z}.
\end{align*}

Let $H_d$ be the (discrete) empirical eigenvalue distribution of $\bSigma_{\bfx} \in \R^{d\times d}$ and let $F_d$ be the (discrete) empirical eigenvalue distribution of the sample covariance matrix $\widehat\bSigma \in \R^{d\times d}$. Consider the proportional limit where $d, n \to \infty$ with $d/n\to \phi>0$. Suppose that the eigenvalue distribution $H_d$ converges to a limit population spectral distribution $H_{\bSigma_{\bfx}}$; i.e., $H_d \Rightarrow H_{\bSigma_{\bfx}}$ in distribution. Given the definition of $m(z)$ from equation~\eqref{eq:stieltjes}, we have $m(z) = m_{F}(z)$. The following theorem characterizes $m_F$ in terms of $H_{\bSigma_{\bfx}}$.

\begin{theorem}[Silverstein Equation \citep{silverstein1995analysis}]
    Let $\nu_F(z) = \phi(m_F(z) + 1/z) - 1/z$. The function $\nu_F$ is the solution of the following fixed-point equation:
    \begin{align*}
        -\frac{1}{\nu_F(z)}=z-\phi \int \frac{t\, }{1+t \,\nu_F(z)} d H_{\bSigma_{\bfx}}(t).
    \end{align*}
\end{theorem}

Thus, using this theorem, given $H_{\bSigma_{\bfx}}$, we can numerically compute $\nu_F$  (and hence, $m_F$) using fixed-point iteration. For example, for $\bSigma_{\bfx}^{(\varepsilon)}$ from equation~\eqref{eq:sigma2}, we have $F = 1/2\, \delta_{1-\varepsilon} + 1/2\, \delta_{1+\varepsilon}$.
