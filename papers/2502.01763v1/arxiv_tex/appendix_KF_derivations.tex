\section{Additional Information on Kronecker-Factored Preconditioners}\label{sec:KF_derivations}

Here, we provide some additional background information regarding key Kronecker-Factored preconditioning methods, including their derivation and relations to various methods in the literature. We recall the running example of a fully-connected net omitting biases, introducing layer-wise dimensionality and a final non-linear layer (e.g.\ softmax) for completeness:
\begin{equation}\label{eq:fcnn}
    f_{\btheta}(\bfx) = \phi(\bW_L \sigma (\bW_{L-1} \cdots \sigma(\bW_1 \bfx) \cdots )), \quad \bW_\ell \in \R^{d_\ell \times d_{\ell - 1}}, d_0 = \dx.
\end{equation}
As before, we define $\btheta$ as the concatenation of $\btheta_\ell = \VEC(\bW_\ell)$, $\ell \in [L]$.
We define an expected loss induced by the neural network $\calL(\btheta) = \Ex_{(\bfx, \bfy)}[\ell(f_{\btheta}(\bfx), \bfy)]$, and its batch counterpart $\hatL(\btheta)$. Here, we define the family of Kronecker-Factored preconditioned optimizers as those that update weights in the following fashion:
\begin{align*}
    {\bW_\ell}_+ &= \bW_\ell - \eta\;\bP_\ell^{-1} \nabla_{\bW_\ell} \hatL(\btheta) \bQ_\ell^{-1},\quad \ell \in [L],
\end{align*}
where $\bP_\ell \in \R^{d_\ell \times d_\ell}$, $\bQ_\ell \in \R^{d_{\ell - 1} \times d_{\ell - 1}}$, $\ell \in [L]$ are square matrices.
For simplicity, we ignore moving parts such as momentum, damping exponents, adaptive learning rate schedules/regularization etc. We now demonstrate the basic principles and derivation of certain notable members of these preconditioning methods on the feedforward network \eqref{eq:fcnn}.

\subsection{Kronecker-Factored Approximate Curvature \KFAC}

As described in the main paper, \KFAC \cite{martens2015optimizing} is at its core an approximation to natural gradient descent. Given that we are approximating \NGD, a crucial presumption on $f_{\btheta}(\bfx)$ and $\calL(\btheta)$ is that the network output $f_{\btheta}(\bfx)$ parameterizes a conditional distribution $p(\bfy|\bfx; \btheta)$, and $\calL(\btheta) \propto \Ex_{(\bfx, \bfy)}[-\log p(\bfy|\bfx; \btheta)]$ is the corresponding negative log-likelihood. As such, \KFAC is technically only applicable to settings where such an interpretation exists. However, this notably subsumes cases $\calL(\btheta) = \Ex_{(\bfx, \bfy)}[\ell(f_{\btheta}(\bfx), \bfy)]$, where $\ell(\cdot)$ is a strictly convex function in $f_{\btheta}(\bfx)$, as this admits an interpretation as $f_{\btheta}(\bfx)$ parameterizing an exponential family distribution. In particular, the square-loss regression case $\ell(\hat\bfy, \bfy) = \norm{\hat\bfy - \bfy}^2$ corresponds to a conditionally-Gaussian predictive distribution with fixed variance $\hat\bfy(\bfx) \sim \normal(f_{\btheta}(\bfx), \sigma^2 \bI)$, and if $\phi(\cdot)$ is a softmax layer and $\ell(\hat\bfy, \bfy) = \texttt{CrossEnt}(\hat\bfy, \bfy)$, the multi-class classification case corresponds to a conditionally-multinomial predictive distribution.

Defining $\bfh_\ell = \bW_\ell \bfz_{\ell-1}$, $\bfz_\ell = \sigma(\bfh)$, $\bfz_0 = \bfx$, the Fisher Information of the predictive distribution $p(\bfy|\bfx; \btheta)$ at $\btheta$ can be expressed in block form:
\begin{align*}
    \mathbf{FI}(\btheta) &\triangleq \Ex_{\bfx}\brac{\pder{p(\bfy|\bfx; \btheta)}{\btheta}\paren{\pder{p(\bfy|\bfx; \btheta)}{\btheta}}^\top} \tag{recall $\btheta$ is $\VEC$-ed parameters} \\
    &= \bmat{
    \Ex_{\bfx} \brac{\pder{p(\bfy|\bfx; \btheta)}{\btheta_1}\paren{\pder{p(\bfy|\bfx; \btheta)}{\btheta_1}}^\top } & \cdots & \Ex_{\bfx}\brac{\pder{p(\bfy|\bfx; \btheta)}{\btheta_1}\paren{\pder{p(\bfy|\bfx; \btheta)}{\btheta_L}}^\top } \\
    \vdots & \ddots & \vdots \\
    \Ex_{\bfx} \brac{\pder{p(\bfy|\bfx; \btheta)}{\btheta_L}\paren{\pder{p(\bfy|\bfx; \btheta)}{\btheta_1}}^\top } & \cdots & \Ex_{\bfx} \brac{\pder{p(\bfy|\bfx; \btheta)}{\btheta_L}\paren{\pder{p(\bfy|\bfx; \btheta)}{\btheta_L}}^\top }
    }
\end{align*}
Looking at the $(i,j)$th block, we have
\begin{align*}
    \Ex_{\bfx} \brac{\pder{p(\bfy|\bfx; \btheta)}{\btheta_i}\paren{\pder{p(\bfy|\bfx; \btheta)}{\btheta_j}}^\top } &= \Ex_{\bfx} \brac{\VEC\paren{\pder{p(\bfy|\bfx; \btheta)}{\bW_i}}\VEC\paren{\pder{p(\bfy|\bfx; \btheta)}{\bW_j}}^\top } \\
    &= \Ex_{\bfx} \brac{(\bfz_{i-1} \otimes \bfg_i )(\bfz_{j-1} \otimes \bfg_j)^\top } \tag{$\bfg_\ell \triangleq - \pder{p(\bfy|\bfx; \btheta)}{\bfh_\ell}$} \\[0.2cm]
    &= \Ex_{\bfx}\brac{(\bfz_{i-1} \bfz_{j-1}^\top) \otimes (\bfg_{i} \bfg_j^\top)} \tag{\Cref{lem:kron_properties}, item 2},
\end{align*}
where the second line comes from writing out the backpropagation formula. \KFAC makes two key approximations:
\begin{enumerate}
    \item The matrix $\mathbf{FI}(\btheta)^{-1}$ is approximated by a block-diagonal, and hence so is $\mathbf{FI}(\btheta)$. We note the original formulation of \KFAC in \citet{martens2015optimizing} also supports a tridiagonal inverse approximation.
    \item The vectors $\bfz_{\ell-1}$ and $\bfg_\ell$ are independent for all $\ell \in [L]$, such that
    \begin{align*}
        \Ex_{\bfx}\brac{(\bfz_{\ell-1}\, \bfz_{\ell-1}^\top) \otimes (\bfg_{\ell} \,\bfg_\ell^\top)} &= \Ex[\bfz_{\ell - 1} \,\bfz_{\ell - 1}^\top] \otimes \Ex[\bfg_{\ell}\, \bfg_\ell^\top].
    \end{align*}
\end{enumerate}
Now replacing the true expectation with the empirical estimate, and defining $\bP_\ell = \hatEx[\bfg_{\ell}\, \bfg_\ell^\top]$, $\bQ_\ell = \hatEx[\bfz_{\ell - 1}\, \bfz_{\ell - 1}^\top]$ completes the Kronecker-Factored approximation to the Fisher Information. It is clear to see from the derivation that, as we previewed in the introduction and expressed emphatically in \citet{martens2015optimizing}, this approximation is \textit{never} expected to be tight. 


\subsection*{Some related preconditioners}
Having introduced \KFAC, we introduce some related preconditioners. Notably, it has been noted that computing $\bfg_\ell$ requires a backwards gradient computation, whereas $\bfz_\ell$ only requires a forward pass. In particular, various works have recovered the \emph{right} preconditioner $\bQ_\ell$ of \KFAC via various notions of ``local'' (layer-wise) losses. Notably, these alternative views allow \KFAC-like preconditioning to extend beyond the negative-log-likelihood interpretation.
\begin{itemize}
    \item \texttt{LocoProp}, square-loss case \cite{amid2022locoprop}:
    \begin{align*}
        \text{Update rule}: {\bW_{\ell}}_+ &= \argmin_{\bW} \frac{1}{2}\hatEx\left[\norm{\bW \bfz_{\ell-1} - \bfh_\ell}^2\right] + \frac{1}{2\eta}\norm{\bW - \bW_\ell}_F^2 \\
        &= \bW_\ell - \eta \nabla_{\bW_\ell} \hatL(\btheta)\paren{\bI_{d_{\ell-1}} + \eta \,\hatEx[\bfz_{\ell-1} \bfz_{\ell-1}^\top]}^{-1}.
    \end{align*}
    As noted in \citet{amid2022locoprop}, this update is also closely related to \texttt{ProxProp} \cite{frerix2018proximal}.

    \item \texttt{FOOF} \cite{benzing2022gradient}:
    \begin{align*}
        \text{Update rule}: \Delta \bW_\ell &= \argmin_{\Delta \bW}\;\hatEx \left[\norm{\Delta \bW \bfz_{\ell-1} - \eta\, \bfg_\ell}^2\right] + \frac{\lambda}{2} \norm{\Delta \bW}_F^2 && \left(\bfg_\ell = \pder{\ell(f_{\btheta}(\bfx), \bfy)}{\bfh_\ell}\right) \\[0.2cm]
        &= \eta \nabla_{\bW_\ell} \hatL(\btheta) \paren{\hatEx[\bfz_{\ell-1} \bfz_{\ell-1}^\top] + \lambda\, \bI_{d_{\ell-1}}}^{-1}, \\[0.2cm]
        {\bW_{\ell}}_+ &= \bW_\ell - \Delta \bW_\ell.
    \end{align*}
\end{itemize}
Interestingly, we note that these right-preconditioner-only variants subsume the \texttt{DFW} algorithm for two-layer linear representation learning proposed in \citet{zhang2023meta}; thus we may see the guarantee therein as support of the above algorithms from a feature learning perspective, albeit weaker than \Cref{thm:linrep_kfac_guarantee}.


\subsection{\Shampoo}

\Shampoo is designed to be a Kronecker-Factored approximation of the full \AdaGrad preconditioner, which we recall is the running sum of the outer-product of loss gradients. Turning off the \AdaGrad accumulator and instead considering the empirical batch estimate $\hatEx[\nabla_{\btheta}\, \ell(f_{\btheta}(\bfx), \bfy) \;\nabla_{\btheta} \, \ell(f_{\btheta}(\bfx), \bfy)^\top]$, the curvature matrix being estimated can also be viewed as the Gauss-Newton matrix $\Ex_{(\bfx, \bfy)}[\nabla_{\btheta}\, \ell(f_{\btheta}(\bfx), \bfy) \;\nabla_{\btheta}\, \ell(f_{\btheta}(\bfx), \bfy)^\top]$. As documented in various works (see e.g.\ \citet{martens2020new}), the (generalized) Gauss-Newton matrix in many cases is related or equal to the Fisher Information, establishing a link between the target curvatures of \KFAC and \Shampoo.

However, the \Shampoo preconditioners differ from \KFAC's. Let us define the $\bfz_\ell, \bfh_\ell$ as before, and $\bfg_\ell = \pder{\ell(f_{\btheta}(\bfx), \bfy)}{\bfh_\ell}$. Then, the \Shampoo preconditioners are given by
\begin{align*}
    \bP_\ell &= \hatEx\brac{\bfg_\ell \bfz_{\ell-1}^\top (\bfg_\ell \bfz_{\ell-1}^\top)^\top}^{1/4}, \quad \bQ_\ell = \hatEx\brac{\bfz_{\ell-1} \bfg_\ell^\top (\bfz_{\ell-1} \bfg_\ell^\top)^\top}^{1/4}.
\end{align*}
Notably, \Shampoo takes the fourth root in the preconditioners, as its target is the \AdaGrad preconditioner which is (modulo scaling) the square-root of the empirical Gauss-Newton matrix--analogous to the square-root of the second moment in \Adam. Whether the target curvature should be the square-root or not of the Gauss-Newton matrix is the topic of recent discussion \cite{morwani2024new, lin2024can}.

\subsection{Kronecker-Factored Preconditioners and the Modular Norm}\label{sec:layerwise_modular_norms}

The ``modular norm'' \cite{large2024scalable, bernstein2024modular, bernstein2024old} is a recently introduced notion that provides a general recipe for producing different optimization algorithms that act layer-wise. By specifying different norms customized for different kinds of layers (e.g.\ feed-forward, residual, convolutional etc.), one in principle has the flexibility to customize an optimizer to handle the different kinds of curvature induced by different parameter spaces. Given a choice of norm on the weight tensor $\bW_\ell$, the descent direction is returned by \textit{steepest descent} with respect to that norm. To introduce steepest descent, we require a few definitions (cf.\ \citet{bernstein2024modular}):
\begin{definition}[Dual norms, steepest direction]
    Given a norm $\norm{\cdot}$ defined over a finite-dimensional real vector space $\calV$. The dual norm $\norm{\cdot}_\dagger$ is defined by
    \begin{align*}
        \norm{\bfv}_\dagger = \max_{\norm{\bfu} = 1} \ip{\bfu,\bfv}.
    \end{align*}
    With $\bfg \in \calV$ and a ``sharpness'' parameter $\eta > 0$, the steepest direction(s) are given by the following variational representation:
    \begin{align*}
        \argmin_{\bfd} \brac{\ip{\bfg, \bfd} + \frac{1}{2\eta} \norm{\bfd}^2} = -\eta\norm{\bfg}_\dagger \cdot \argmax_{\norm{\bfu}=1} \;\ip{\bfg, \bfu}.
    \end{align*}
\end{definition}
Here we focus on finite-dimensional normed spaces, but note that these concepts extend \emph{mutatis mutandis} to general Banach spaces. The aforementioned works derive various standard optimizers by choosing different norms, including \emph{induced matrix norms} $\norm{\bW_\ell}_{\alpha \to \beta} = \max_{\bfx} \frac{\norm{\bW_\ell \bfx}_\beta}{\norm{\bfx}_\alpha}$, applied to a given layer's weight space, for example \cite{bernstein2024old}:
\begin{itemize}
    \item \SGD: induced by Frobenius (Euclidean) norm $\norm{\cdot} = \norm{\cdot}_F$. Note the Frobenius norm is \emph{not} an induced matrix norm.
    
    \item Sign-descent (``ideal'' \Adam with EMA on moments turned off): induced by $\norm{\cdot} = \norm{\cdot}_{\ell_1 \to \ell_\infty}$.

    \item \Shampoo (``ideal'' variant with moment accumulator turned off): induced by $\norm{\cdot} = \norm{\cdot}_{\ell_2 \to \ell_2} =  \opnorm{\cdot}$.
    
\end{itemize}

Therefore, in light of this characterization, a natural question to ask is \emph{what norm induces a given Kronecker-Factored preconditioner} (which includes \Shampoo). We provide a simple derivation that determines the norm.
\begin{proposition}[Kronecker-Factored matrix norm]\label{prop:KF_norm}
    Recall the fully-connected network \eqref{eq:fcnn}. Given preconditioners $\scurly{(\bP_\ell, \bQ_\ell)}_{\ell = 1}^{L}$, where $\bP_\ell \in \R^{d_\ell \times d_\ell}$, $\bQ_\ell \in \R^{d_{\ell - 1} \times d_{\ell - 1}}$, $\ell \in [L]$ are invertible square matrices. Then, the layer-wise Kronecker-Factored update:
    \begin{align*}
        {\bW_{\ell}}_+ = \bW_{\ell} - \eta\, \bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}, \quad \ell \in [L]
    \end{align*}
    is equivalent to layer-wise steepest descent with norm $\norm{\bM_\ell} \triangleq \norm{\bP_\ell^\top \bM_\ell \bQ_\ell^\top}_F$:
    \begin{align*}
        \argmin_{\bM} \brac{\ip{\nabla_{\bW_\ell} \calL(\btheta), \bM} + \frac{1}{2\eta} \norm{\bM}^2} = - \eta\, \bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}.
    \end{align*}
\end{proposition}
\begin{proof}[Proof of \Cref{prop:KF_norm}]
    It is straightforward to verify $\norm{\bM} \triangleq \norm{\bP^\top \bM \bQ^\top}_F$ for invertible $\bP, \bQ$ satisfies the axioms of a norm. It remains to verify the steepest descent direction:
    \begin{align*}
        \argmin_{\bM} \brac{\ip{\nabla_{\bW_\ell} \calL(\btheta), \bM} + \frac{1}{2\eta} \norm{\bM}^2} &= -\eta\,\norm{\nabla_{\bW_\ell} \calL(\btheta)}_\dagger \cdot \argmax_{\norm{\bM}=1} \;\ip{\nabla_{\bW_\ell} \calL(\btheta), \bM} = \bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}.
    \end{align*}
    We start by writing:
    \begin{align*}
        \norm{\nabla_{\bW_\ell} \calL(\btheta)}_\dagger &\triangleq \max_{\norm{\bM} = 1} \;\ip{\nabla_{\bW_\ell} \calL(\btheta), \bM} \\[0.1cm]
        &= \max_{\norm{\bP_\ell^\top \bM \bQ_\ell^\top}_F = 1} \trace(\bM^\top \nabla_{\bW_\ell} \calL(\btheta)) \\[0.1cm]
        &= \max_{\norm{\bD}_F = 1} \trace(\bQ_\ell^{-1} \bD^\top \bP_\ell^{-1}  \nabla_{\bW_\ell} \calL(\btheta)) \tag{$\bP_\ell^\top \bM \bQ_\ell^\top \to \bD$} \\[0.1cm]
        &= \norm{\bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}}_F. \tag{trace cyclic property, $\norm{\cdot}_F$ is self-dual}
    \end{align*}
    Similarly, it is straightforward to verify that the maximizing matrix is:
    \begin{align*}
        \argmax_{\norm{\bM}=1} \;\ip{\nabla_{\bW_\ell} \calL(\btheta), \bM} &= \frac{\bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}}{\norm{\bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}}_F},
    \end{align*}
    such that plugging it into the steepest descent expression yields:
    \begin{align*}
        \argmin_{\bM} \brac{\ip{\nabla_{\bW_\ell} \calL(\btheta), \bM} + \frac{1}{2\eta} \norm{\bM}^2} &= - \eta\; \norm{\bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}}_F  \cdot \frac{\bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}}{\norm{\bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1}}_F} \\
        &= - \eta\; \bP_\ell^{-1} \nabla_{\bW_\ell} \calL(\btheta)\; \bQ_\ell^{-1},
    \end{align*}
    as required.
\end{proof}
We remark that for complex-valued matrices, the above holds without modification for the Hermitian transpose $\bA^{\mathrm H}$.
Notably, the layer-wise norm corresponding to Kronecker-Factored preconditioning is not an induced matrix norm, though modified optimizers can certainly be derived via induced-norm variants, such as a ``Mahalonobis-to-Mahalonobis'' induced norm:
\begin{align*}
    \norm{\bM}_{\bQ^{-1} \to \bP} &\triangleq \max_{\bfx} \frac{\sqrt{(\bM\bfx)^\top \bP(\bM \bfx)}}{\sqrt{\bfx^\top \bQ^{-1} \bfx}} \tag{$\bP, \bQ \succ \bzero$} \\[0.1cm]
    &= \max_{\norm{\bfx} = 1} \norm{\bP^{1/2} \bM \bQ^{1/2}}\\[0.1cm]
    &= \opnorm{\bP^{1/2} \bM \bQ^{1/2}}.
\end{align*}
