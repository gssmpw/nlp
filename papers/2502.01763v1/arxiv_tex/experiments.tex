\section{Numerical Validation}\label{sec:numerical_validation}
\subsection{Linear Representation Learning}



\begin{figure*}[t]
\centering
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_train_loss_2000iters_10exps_bern_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_subspace_dist_2000iters_10exps_bern_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/headtohead_figs/headtohead_transfer_loss_2000iters_10exps_bern_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\end{minipage}%
\caption{
From \textbf{left} to \textbf{right}: the training loss, subspace distance, and transfer loss induced by various algorithms on a linear representation learning task. We note that various algorithms converge in training loss, but negligibly in subspace distance, and thus transfer loss.
}
\label{fig:headtohead}
\end{figure*}

We numerically study the behavior of different algorithms for a transfer learning setting \eqref{eq:linrep_target_datagen}, where the model is to be trained on data generated by \((\mbf{F}_\star^{\msf{train}}, \bfG_\star)\), and the transfer task has data generated by \(({\bfF}_\star^{\msf{test}}, \bfG_\star)\), i.e.\ the embedding \(\bfG_\star\) is shared, but the task heads \(\mbf{F}_\star^\msf{train}\) and \(\mbf{F}_\star^\msf{test}\) are different. The training and test covariates have anisotropic covariance 
matrices $\bSigma_{\bfx, \msf{train}}$ and $\bSigma_{\bfx, \msf{test}}$ respectively. Our data generation process for the training task and the transfer task are as follows:
\begin{align}
    \label{eq:bern_data}
    &\bfy_i ^{\msf{s}} = \mbf{F}_\star^{\msf{s}} \mbf{G}_\star \bfx_i ^{\msf{s}} + \bveps_i ^{\msf{s}}, \quad  \bfx_i ^{\msf{s}} \iidsim \bSigma_{\bfx, \msf{s}}^{1/2}\;\mrm{Unif}(\{\pm 1\}^{\dx},\quad
    \bveps_i ^{\msf{s}} \iidsim \normal(0, \sigma^2_{\ep,\msf{s}}\, \bfI_{\dy}), \quad \msf{s} \in \{\msf{test}, \msf{train}\},
\end{align}
where \(\sigma_{\ep, \msf{train}} = 0.1\) and \(\sigma_{\ep,\msf{test}} = 1\). We use \(\dx = 100\), \(\dy = 15\), \(k = 8\), and batch size \(n=1024\). We present additional experiments and details in \Cref{sec:additional_numerics}, including discussions on the learning rates, and how $\bfF_\star^{\msf{s}}, \bfG_\star^{\msf{s}}$, $\bSigma_{\bfx, \msf{s}}$ are precisely generated.







\vspace{-0.3cm}
\paragraph{Head-to-head Evaluations.} We track the training loss, subspace distance, and transfer loss of different algorithms during the update (Figure~\ref{fig:headtohead}). Alongside \SGD, \KFAC, \Adam, and \NGD, we also consider Alternating Min-\SGD (\texttt{AMGD}) \citep{collins2021exploiting,vaswani2024efficient}, and De-bias \& Feature-Whiten (\DFW) \citep{zhang2023meta} (corresponding to \eqref{eq:KFAC_update} with $\bfP_{\sbF} = \bI_{\dy}$), two algorithms studied in linear representation learning. The transfer loss is the loss incurred by fitting a least-squares $\Fls^{\msf{test}}$ on the current $\bG$ iterate (see \Cref{lem:linrep_transfer}).
Although various algorithms converge on $\msf{s} = \msf{train}$, \KFAC outperforms all others in terms of subspace distance and transfer loss, as suggested by the theory. %






\ifshort
\else
  \paragraph{Learning Rate Sweep.}\bem{check if`}
    We further test the performance of each learning algorithm at different learning rates from \(10^{-6}, 10^{-5.5}, \ldots, 10^{-0.5}, 10^{0}\), with results shown in Figure~\ref{fig:lr_sweep}, where we plot the subspace distance at \(1000\) iterations.
    If the algorithm encounters numerical instability, then we report the subspace distance as the maximal value of \(1.0\).
    
    
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/lr_sweep_1000iters_10exps_bern_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
    \caption{\axcomment{Learning Rate Sweep}}
    \label{fig:lr_sweep}
    \end{figure}
\fi
\vspace{-0.3cm}
\paragraph{Effect of Batch Normalization.} We track the subspace distance and the training loss of \texttt{AMGD} (with and without batch-norm) and \KFAC, see Figure~\ref{fig:batchnorm_subpace_dist}. As theoretically predicted in \Cref{sec:lin_rep_BN}, since batch-norm approximately whitens $\bfx_i^{\msf{train}}$, \texttt{AMGD}+batch-norm converges in training loss. However, as predicted, it does not recover the correct representation, whereas \KFAC does.
\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{figs/combined_metrics_2000iters_10exps_bern_highanistropy_100dx_15dy_8repsize_1024bsz.pdf}
\caption{Subspace distance and the training loss of \KFAC and \texttt{AMGD} (with and without batch-norm). Notably, batch-norm enables \texttt{AMGD}'s train loss to converge, but not its subspace distance.}
\label{fig:batchnorm_subpace_dist}
\end{figure}

\subsection{Single-Index Learning}
\label{sec:single_index_exps}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/single_index_figure.pdf}
    \caption{%
    The correlation of the direction learned by \SGD and \KFAC with the the true direction by numerical simulations averaged over 30 trials, and theoretical predictions.
    \textbf{(Left)} For different values of $\lambda_\sbG$ the theoretical predictions match the simulations very well.
    \textbf{(Right)} The alignment of the feature learned by \SGD deteriorates as anisotropy is increased (larger $\ep$), whereas the \KFAC update remains accurate.}%
    \label{fig:single_index}
\end{figure*}

Consider the single-index learning setting of Section~\ref{sec:single_index} with $\sigma_\star(z) = z + \frac{1}{\sqrt{2}} (z^2-1)$, and $\sigma_\ep =1$. 
\paragraph{Different Levels of Anisotropy.} In this experiment, we set $\dx=200,$ $n = 6000$, and $\dhid = 1000$ and set $\lambda_{\sbG}\to 0$. For a parameter $\ep \in \R$, we define $\bSigma_{\bfx} = \bSigma_{\bfx}^{(\varepsilon)}$ with
\begin{align}
    \label{eq:sigma2}
    \bSigma_{\bfx}^{(\varepsilon)} = \diag(\underbrace{1+\ep, \dots, 1+\ep}_{\dx/2}, \underbrace{1-\ep, \dots, 1-\ep}_{\dx/2}).    
\end{align}
For different values of $\varepsilon$, we simulate the \KFAC and \SGD updates numerically and compute their correlation with the true direction.  We also theoretically predict the correlation using Lemma~\ref{lemma:beta_tilde_alignment} and \ref{lemma:beta_hat_alignment}; see \Cref{fig:single_index} (Right). The \SGD update fails to recover the true direction in highly anisotropic settings (large $\ep$), whereas the one-step \KFAC update remains  accurate.

\vspace{-0.4cm}
\paragraph{Theory vs. Simulations.} We set $\dx = 900$, $n = 5000$, $\dhid = 1000$, and $\bSigma_{\bfx} = \bSigma_{\bfx}^{(0.5)}$. For different $\lambda_{\sbG}$, we simulate the correlation between the directions learned by \KFAC and \SGD with the true direction and compare it with predictions of Lemma~\ref{lemma:beta_tilde_alignment} and \ref{lemma:beta_hat_alignment}; see Figure~\ref{fig:single_index} (Left).  We see that the theoretical results match very well with numerical simulations, even for moderately large $n, \dx$, and $\dhid$. The direction learned by \KFAC has a larger correlation with the true direction compared to that learned by \SGD, as predicted.







