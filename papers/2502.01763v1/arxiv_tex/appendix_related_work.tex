\section{Extended Background and Related Work}
\label{sec:related_work}
\paragraph{Preconditioners for Neural Network Optimization.}
A significant research effort in neural network optimization has been dedicated to understanding the role of preconditioning in convergence speed and generalization. Perhaps the most widespread paradigm falls under the category of \textit{entry-wise} (``diagonal'') preconditioners, whose notable members include \Adam \cite{kingma2014adam}, (diagonal) \AdaGrad \cite{duchi2011adaptive}, \RMSprop \cite{tieleman2012lecture}, and their innumerable relatives and descendants (see e.g.\ \citet{schmidt2021descending, dahl2023benchmarking} for surveys). However, diagonal preconditioners inherently do not fully capture inter-parameter dependencies, which are better captured by stronger curvature estimates, e.g.\ Gauss-Newton approximations \cite{botev2017practical, martens2020new}, L-BFGS \cite{byrd2016stochastic, bollapragada2018progressive, goldfarb2020practical}. Toward making non-diagonal preconditioners scalable to neural networks, many works (including the above) have made use of layer-wise \emph{Kronecker-Factored} approximations, where each layer's curvature block is factored into a Kronecker product $\bQ \otimes \bP$. Perhaps the two most well-known examples are Kronecker-Factored Approximate Curvature (\KFAC) \cite{martens2015optimizing} and \Shampoo \cite{gupta2018shampoo, anil2020scalable}, where approximations are made to the Fisher Information and Gauss-Newton curvature, respectively. Many works have since expanded on these ideas, such as by improving practical efficiency \cite{ba2017distributed, shi2023distributed, jordan2024muon, vyas2024soap} and defining generalized constructions \cite{dangel2020modular, amid2022locoprop, benzing2022gradient}. An interesting alternate view subsumes certain preconditioners via steepest descent with respect to layer-wise (``modular'') norms \cite{large2024scalable, bernstein2024modular, bernstein2024old}. We draw a connection therein by deriving the steepest descent norm that Kronecker-Factored preconditioners correspond to; see \Cref{sec:layerwise_modular_norms}.

\paragraph{Multi-task Representation Learning (MTRL).} Toward a broader notion of generalization, the goal of MTRL is to characterize the benefits of learning a \emph{shared} representation across distinct tasks. Various works focus on the generalization properties given access to an empirical risk minimizer (ERM) \cite{maurer2016benefit, du2020few, tripuraneni2020theory, zhang2024guarantees}, with the latter work resolving the setting where distinct tasks may have different covariate distributions. Closely related formulations have been studied in the context of distribution shift \cite{kumar2022fine, lee2023surgical}. While these works consider general non-linear representations, access to an ERM obviates the (non-convex) optimization component. As such, multiple works have studied algorithms for linear representation learning \cite{tripuraneni2021provable, collins2021exploiting, thekumparampil2021sample, nayer2022fast} and specific non-linear variants \cite{collins2024provable, nakhleh2024effects}. In contrast to the ERM works, which are mostly agnostic to the covariate distribution, all the listed algorithmic works assume isotropic covariates $\normal(\bzero, \bI)$. \citet{zhang2023meta} show that isotropy is in fact a key enabler, and propose an adjustment to handle general covariances. In this paper, we show that many prior linear representation learning algorithms belong to the same family of (preconditioned) optimizers. We then propose an algorithm coinciding with \KFAC that achieves the first condition-number-free convergence rate.


\paragraph{Nonlinear Feature Learning.} 

In the early phase of training, neural networks are shown to be essentially equivalent to the kernel methods, and can be described by the neural tangent kernel (NTK). See \citet{jacot2018neural,mei2022generalization,hu2022universality}. However,
kernel methods are inherently limited and have a sample complexity superlinear in the input dimension $d$ for learning nonlinear functions \citep{ghorbani2021linearized,ghorbani2021neural}. The main reason for this limitation is  that kernel methods use a set of fixed features that are not task specific. There has been a lot of interest in studying the benefits of feature learning from a theoretical perspective (\citet{baibeyond2020,hanin2020finite,yang2020feature_learn,shi2022theoretical,abbe2022merged}, etc.). In a setting with isotropic covariates $\normal(\bzero, \bI)$, it is shown that 
even a one-step of \SGD update on the first layer of a two-layer neural networks can learn good enough features to provide a significant sample complexity improvement over kernel methods assuming that the target function has some low-dimensional structure \citep{damian2022neural,ba2022high,moniri_atheory2023,cuiasymptotics,dandi2023learning,dandi2024random,dandibenefits,arnaboldi2024repetita,lee2024neural} and this has became a very popular model for studying feature learning. These results were later extended to three-layer neural networks in which the first layer is kept at random initialization and the second layer is updated using one step of \SGD  \citep{wanglearning,nichani2024provable,fu2024learning}.
Recently, \citet{ba2024learning,mousavi2023gradient} considered an anisotropic case where the covariance contains a planted signal about the target function and showed that a single step of \SGD can leverage this to better learn the target function. However, the general case of anisotropic covariate distributions remains largely unexplored.  In this paper, we study feature learning with two-layer neural networks with general anisotropic covariates in single-index models and that one-step of \SGD update has inherent limitations in this setting, and the natural fix will coincide with applying the \KFAC layer-wise preconditioner.

