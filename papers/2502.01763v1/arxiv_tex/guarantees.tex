\section{Feature Learning via Kronecker-Factored Preconditioning}

We present two prototypical models of feature learning, \textit{linear representation learning} and \textit{single-index learning}, and demonstrate how typical guarantees for the features learned by \SGD break down outside of idealized settings. We then show how to rectify these issues by deriving a modified algorithm from first principles, and demonstrate that both cases in fact coincide with a particular Kronecker-factored preconditioning method. We now set-up the model architecture and algorithm primitive considered in both problems. We consider two-layer feedforward neural network predictors:
\begin{align}\label{eq:two_layer_net}
    \fFG(\bfx) = \bF \sigma(\bG \bfx),
\end{align}
where $\bF \in \R^{\dy \times \dhid}$, $\bG \in \R^{\dhid \times \dx}$ denote the weight matrices and $\sigma(\cdot)$ is a predetermined activation function. For scalar outputs $\dy = 1$, we use $\ffG(\bfx) = \bff^\top \sigma(\bG \bfx)$. For our purposes, we omit the bias vectors from both layers. We further denote the intermediate covariate pre- and post-activation $\bfh \triangleq \bG \bfx$, $\bfz \triangleq \sigma(\bG \bfx)$. We consider a standard mean-squared-error (MSE) regression objective and its (batch) empirical counterpart:
\begin{align}\label{eq:regression_loss}
    \calL(\bF, \bG) \triangleq \frac{1}{2} \Ex_{(\bfx, \bfy)}\mem\brac{\norm{\bfy - \fFG(\bfx)}^2}, \quad
    \hatL(\bF, \bG) \triangleq \frac{1}{2} \hatEx \mem\brac{\norm{\bfy - \fFG(\bfx)}^2}.
\end{align}
Given a batch of inputs $\curly{\bfx_i}_{i=1}^n$, we define the left and right preconditioners of the two layers (recall equation~\eqref{eq:kf_precond}):
\begin{align}\label{eq:precond}
    \begin{split}
        &\bQ_\sbG = \hatSigma[\bfx] =  \hatEx[\bfx \bfx^\top], \quad \bQ_\sbF = \hatSigma[\bfz] = \hatEx[\bfz\bfz^\top],\\[0.2cm]
        &\bP_\sbG = \hatEx\brac{\paren{\pder{\fFG}{\bfh}}^\top \pder{\fFG}{\bfh}} (\text{or } \bI_{\dhid}), \quad \bP_{\sbF} = \bI_{\dy}.
    \end{split}
\end{align}
We introduce the flexibility of $\bP_{\sbG} = \bI_{\dhid}$ for when $\bP_{\sbG}$ does not play a significant role; notably, this recovers certain Kronecker-Factored preconditioners that avoid extra backwards passes (see \Cref{sec:KF_derivations}).
We consider a stylized alternating descent primitive, where we iteratively perform
\begin{align}\label{eq:KFAC_update}
    \begin{split}
        \bG_+ &= \bG - \eta_{\sbG} \bP_\sbG^{-1}\, \nabla_{\sbG} \hatL(\bfF, \bfG)\, (\bQ_\sbG + \lambda_\sbG \bI_{\dx})^{-1} \\[0.2cm]
        \bF_+ &= \bF - \eta_\sbF \bP_\sbF^{-1}\, \nabla_\sbF \hatL(\bfF, \bfG_+)\, \bQ_{\sbF}^{-1},
    \end{split}
\end{align}
where $\eta_\sbG, \eta_\sbF > 0$ are layer-wise learning rates, and $\lambda_\sbG \in \R$ is a regularization parameter.
In line with most prior work, we consider an alternating scheme for analytical convenience.
We also assume that $\bG_+, \bF_+$ are computed on \emph{independent batches} of data, equivalent to sample-splitting strategies in prior work (\citet{collins2021exploiting,zhang2023meta,ba2022high,moniri_atheory2023} etc).


The preconditioners \eqref{eq:precond} and update \eqref{eq:KFAC_update} bear a striking resemblance to \KFAC (cf.\ \Cref{sec:KF_derivations}). In fact, the preconditioners align exactly with \KFAC if we view $\LFG$ as a negative log-likelihood of a conditionally Gaussian model with fixed variance: $\widehat \bfy(\bfx) \sim \normal(\fFG(\bfx), \sigma^2 \bI)$. This is in some sense a coincidence (and a testament to the prescience of \KFAC's design): rather than deriving the above preconditioners via approximating the Fisher Information matrix, we will show shortly how they arise as a natural adjustment to \SGD in our featured problems. We note that Kronecker-Factored preconditioning methods often involve further moving parts such as damping exponents $\bP^{-\rho}$, additional ridge parameters on various preconditioners, and momentum. Though of great importance in practice, they are beyond the scope of this paper,\footnote{We refer the interested reader to \citet{ishikawa2023parameterization} for discussion of these settings in the ``maximal-update parameterization'' framework \cite{yang2021tensor}.} and we only feature parameters that play a role in our analysis.

A convenient observation that unifies various stylized algorithms on two-layer networks is that the $\bF$-update in \eqref{eq:KFAC_update} can be interpreted as an exponential moving average (EMA) over least-squares estimators conditioned on $\bfz$.
\begin{lemma}\label{lem:KFAC_is_EMA}
    Given $\bG$, define the least-squares estimator:
    \begin{align*}
        \Fls &\triangleq \argmin_{\widehat{\bF}} \frac{1}{2}\hatEx\big[\norm{\bfy - \widehat{\bF} \underbrace{\sigma(\bG \bfx)}_{\bfz}}^2\big]= \bY^\top \bZ \, (\bZ^\top \bZ)^{-1}
    \end{align*}
    Given $\eta_\sbF \in (0, 1]$, then the $\bF$-update in \eqref{eq:KFAC_update} can be re-written as an EMA of $\Fls$; i.e., $$\bF_+ = (1 - \eta_\sbF) \bF + \eta_\sbF \Fls.$$
\end{lemma}
In particular, many prior works (e.g.\ \citet{collins2021exploiting, nayer2022fast, thekumparampil2021sample, zhang2023meta}) consider an alternating ``minimization-descent'' approach, where out of analytical convenience $\bF$ is updated by performing least-squares regression holding the hidden layer $\bG$ fixed. In light of \Cref{lem:KFAC_is_EMA}, this corresponds to the case where $\eta_\sbF = 1$.

\subsection{Linear Representation Learning}\label{sec:lin_rep}

Assume we have data generated by the following process
\begin{align}\label{eq:linrep_datagen}
    \bfx_i \iidsim \normal(0, \bSigma_\bfx), \quad \bfy_i = \Fstar \Gstar \bfx_i + \bveps_i,
\end{align}
where $\bSigma_\bfx$ is the input covariance, $\Fstar \in \R^{\dy \times k}$, $\Gstar \in \R^{k \times \dx}$ are (unknown) rank-$k$ matrices, and $\bveps_i \iidsim \normal(0, \Sigmaeps)$ is additive label noise independent of all other randomness. We consider Gaussian data throughout the paper for conciseness; all results in this section can be extended to subgaussian $\bfx, \bveps$ via standard tools, affecting only the universal constants (see \Cref{sec:aux_results}). Let us define $\sigmaeps^2 \triangleq \lmax(\Sigmaeps)$. Accordingly, our predictor model is a two-layer feed-forward linear network \eqref{eq:two_layer_net} with $\bF \in \R^{\dy \times k}$, $\bG \in \R^{k \times \dx}$.

The goal of linear representation learning is to learn the low-dimensional feature space that $\Gstar$ maps to, which is equivalent to determining its row-space $\rowsp(\Gstar)$. Recovering $\Fstar, \Gstar$ is an ill-posed problem, as for any invertible $\bL \in \R^{k \times k}$, the matrices $\Fstar \bL$, $\bL^{-1} \Gstar$ remain optimal. Therefore, we measure recovery of $\rowsp(\Gstar)$ via a \textit{subspace distance}.
\begin{definition}[Subspace Distance {\citep{stewart1990matrix}}]\label{def:subspace_dist}
    Let $\bG, \Gstar \in \R^{k \times \dx}$ be matrices whose rows are orthonormal.
    Let $\Gperp \in \R^{\dx \times \dx}$ be the projection matrix onto $\rowsp(\Gstar)^\perp$. Define the distance between the subspaces spanned by the rows of $\bG$ and $\Gstar$ by
    \begin{align}\label{eq: subspace distance}
        \dist(\bG, \Gstar) &\triangleq \norm{\bG \Gperp}_{\rm op}
    \end{align}
\end{definition}
The subspace distance quantitatively captures the alignment between two subspaces, ranging between $0$ (occurring iff $\rowsp(\Gstar) = \rowsp(\bG)$) and $1$ (occurring iff $\rowsp(\Gstar) \perp \rowsp(\bG)$).
We further make the following non-degeneracy assumptions.
\begin{assumption}\label{assump:full_rank_orth}
    We assume $\Gstar$ is row-orthonormal, and $\Fstar$ is full-rank, $\rank(\Fstar) = k \leq \dy$. This is without loss of generality: if $k > \dy$, then recovering a $k$-dimensional row-space from $\bfy_i$ is underdetermined. If $\rank(\Fstar) = k' < k$, then it suffices to consider $\Gstar \in \R^{k' \times \dx}$.
\end{assumption}
The linear representation learning problem has often been studied in the context of multi-task learning \citep{du2020few, tripuraneni2020theory, collins2021exploiting, thekumparampil2021sample, zhang2023meta}.

\begin{remark}[Multi-task Learning]\label{rem:multitask}
    Multi-task learning considers data generated as $\yit = \Ftstar \Gstar \xit + \epsit$ for distinct tasks $t = 1,\dots, T$, with the same goal of recovering the \textit{shared} representation $\Gstar$. Our algorithm and guarantees naturally extend here, see \Cref{sec:extension_multi_task} for full details. In particular, by embedding $\Fstar = \bmat{\Ftstar[1]^\top & \cdots & \Ftstar[T]^\top}^\top$, \Cref{assump:full_rank_orth} is equivalent to the ``task-diversity'' conditions in the above works: $\rank(\Fstar) = \rank\Big(\sum_{t=1}^T \Ftstar^\top \Ftstar \Big) = k$.
\end{remark}
We maintain the ``single-task'' setting in this section for concise bookkeeping while preserving the essential features of the representation learning problem.
Various algorithms have been proposed toward provably recovering the representation $\Gstar$. A prominent example is an alternating minimization-\SGD scheme \citep{collins2021exploiting, vaswani2024efficient}.
In the cited works, a local convergence result\footnote{By local convergence here we mean $\dist(\bG, \Gstar)$ is sufficiently (but non-vanishingly) small.} is established for isotropic data $\bSigma_\bfx = \bI_{\dx}$. In \citet{zhang2023meta}, it is shown that using \SGD  can drastically slow convergence even under mild anisotropy; their proposed algorithmic adjustment equates to applying the right-preconditioner $\bQ_\sbG = \hatSigma$. However, their local convergence result suffers a dependence on the condition number of $\bF_\star$, slowing the linear convergence rate for ill-conditioned $\bF_\star$.
Let us now specify the algorithm template used in this section, that also encompasses the above work:
\begin{align}\label{eq:linrep_update}
    \overline\bG_+ \text{ via \eqref{eq:KFAC_update}},\; \bG_+ = \mathrm{Ortho}(\overline\bG_+),\; \bF_+ \text{ via \eqref{eq:KFAC_update}}.
\end{align}
Notably, we row-orthonormalize the representation after each update. Besides ease of analysis, we have observed this numerically mitigates the elements of $\bG$ from blow-up when running variants of \SGD. The alternating min-\SGD algorithms in \citet{collins2021exploiting, vaswani2024efficient} are equivalent to iterating \eqref{eq:linrep_update} setting $\bP_\sbG = \bQ_\sbG = \bI$, $\eta_\sbF = 1$ in \eqref{eq:precond}, whereas \citet{zhang2023meta} use $\bP_\sbG = \bI$, $\bQ_\sbG = \hatSigma$, $\eta_\sbF = 1$. 
Let us now write out the full-batch gradient update.

\paragraph{Full-Batch \SGD.}
Given a fresh batch of data $\xybatch$, and current weights $(\bF, \bG)$, we have the representation gradient and corresponding \SGD step:
\begin{align}
    \nabla_{\sbG} \hatL(\bfF, \bfG) &= \frac{1}{n}\bfF^\top \mem\paren{\bfF \bG \bX^\top \bX - \bY^\top \bX} \nonumber \\[0.2cm]
    \bG_+ &= \bG - \eta_\sbG \nabla_{\sbG} \hatL(\bfF, \bfG). \label{eq:linrep_grad}
\end{align}
When $\bfx$ is isotropic $\bSigma_\bfx = \bI_{\dx}$, the key observation is that by multiplying both sides of \eqref{eq:linrep_grad} by $\Gperp$, recalling $\bY^\top  = \Fstar \Gstar \bX^\top + \Eps^\top$, we have
\begin{align*}
    \overline\bG_+\Gperp \mem&= \mem\mem\paren{\mem\bG - \eta_\sbG \bfF^\top\mem\paren{(\bfF \bG - \Fstar \Gstar) \hatSigma - \frac{1}{n}\Eps^\top \bX}\mem}\mem\Gperp \\[0.2cm]
    &\approx (\bI_{k} - \eta_\sbG \bfF^\top \bfF) \bG \Gperp  + \frac{\eta_\sbG}{n} \bF^\top \Eps^\top \bX \Gperp,
\end{align*}
where the approximate equality hinges on covariance concentration $\hatSigma \approx \bI_{\dx}$ and $\Gstar\Gperp = \bzero$. Therefore, in the isotropic setting, for sufficiently large $n \gtrsim \dx$, and appropriately chosen $\eta_\sbG \approx \frac{1}{\lmax(\Fstar^\top \Fstar)}$, then (omitting many details) we have the one-step contraction \citep{collins2021exploiting, vaswani2024efficient}:
\begin{align}
        \hspace{-0.3cm}\dist(\bG_+, \Gstar) &\lesssim \paren{1 - \frac{\lmin(\Fstar^\top \Fstar)}{\lmax(\Fstar^\top \Fstar)}} \dist(\bG, \Gstar)+ \calO\paren{\sigmaeps \sqrt{\dx/n}}, \label{eq:SGD_onestep}
\end{align}
where $\calO(\cdot)$ here hides different problem parameters depending on the analysis.
Therefore, in low-noise/large-batch settings, this demonstrates \SGD  on the representation $\bG$ converges geometrically to $\Gstar$ (in subspace distance). However, there are clear suboptimalities to \SGD. Firstly, the above analysis critically relies on $\Sigmax = \bI_{\dx}$ such that $\Gstar \hatSigma \Gperp \approx \bzero$. As aforementioned, this is demonstrated to be crucial in \citet{zhang2023meta} for $\dist(\bG, \Gstar)$ to converge using \SGD.
Secondly, the convergence of \SGD is bottlenecked by the conditioning of $\Fstar$. In fact, we show the dependence on $\Fstar$ in the contraction rate bound \eqref{eq:SGD_onestep} cannot be improved in general, even under the most benign assumptions.
Following \citet{collins2021exploiting, vaswani2024efficient}, we define $\bG_T = \SGD(\bG_0; \eta_\sbG, T)$ as the output of alternating min-\SGD, i.e.\ iterating \eqref{eq:linrep_update} setting $\bP_\sbG = \bQ_\sbG = \bI$, $\eta_\sbF = 1$ in \eqref{eq:precond}, for $T$ steps with fixed step-size $\eta_\sbG$ starting from $\bG_0$. 
\begin{restatable}{proposition}{LinRepLowerBound}\label{prop:linrep_SGD_lower_bound}
    Let $\Sigmax = \bI_{\dx}$, $n = \infty$. Choose any $\dx > k,\;\dy \geq k \geq 2$. Let the learner be given knowledge of $\Fstar, \Gstar$ and $\dist(\bG_0, \Gstar)$. However, assume the learner must fix $\eta_\sbG > 0$ before observing $\bG_0$. Then, there exists $\Fstar \in \R^{\dy \times k}$, $\Gstar, \bG_0 \in \R^{k \times \dx}$, such that
    $\bG_T = \SGD(\bG_0; \eta_\sbG, T)$ satisfies:
    \begin{align*}
        \dist(\bG_T, \Gstar) &\geq \paren{1 - 4 \frac{\lmin(\Fstar^\top \Fstar)}{\lmax(\Fstar^\top \Fstar)}}^T \dist(\bG_0, \Gstar).
    \end{align*}
\end{restatable}

The proof can be found in \Cref{sec:SGD_lower_bound}.
Since we set $\Sigmax = \bI$, the lower bound also holds for the algorithm in \citet{zhang2023meta}. We remark departing from a worst-case analysis to a generic performance lower bound, e.g.\ random initialization or varying step-sizes, is a nuanced topic even for the simple case of convex quadratics; see e.g.\ \citet{bach2024scaling, altschuler2023acceleration}.
In light of \Cref{prop:linrep_SGD_lower_bound} and \eqref{eq:linrep_grad}, a sensible alteration might be to \emph{pre- and post- multiply} $\nabla_{\sbG} \hatL(\bfF, \bfG)$ by $(\bF^\top \bF)^{-1}$ and $\hatSigma^{-1}$. These observations bring us to the proposed recipe in \eqref{eq:KFAC_update}.

\paragraph{Stylized \KFAC.}

By analyzing the shortcomings of the \SGD  update, we arrive at the proposed representation update:
\begin{align*}
    \overline\bG_+ &= \bG - \eta_\sbG (\bF^\top \bF)^{-1}  \nabla_{\sbG} \hatL(\bfF, \bfG) \,\hatSigma^{-1}.
\end{align*}
We can verify from \eqref{eq:precond} and \eqref{eq:KFAC_update} that $\bP_\sbG = \bF^\top \bF$ and $\bQ_\sbG = \hatSigma$.
Thus, we have recovered a stylized variant of \KFAC  as previewed. Our main result in this section is a local convergence guarantee.
\fussy
\begin{restatable}{theorem}{LinRepMainThm}\label{thm:linrep_kfac_guarantee}
    Consider running \eqref{eq:linrep_update} with $\lambda_\sbG = 0$, $\eta_\sbG \in [0,1]$, and $\eta_\sbF = 1$. Define $\overline\sigma^2 \triangleq \frac{\sigmaeps^2}{\smin(\Fstar)^2\lmin(\Sigmax)}$. As long as $\dist(\bG, \Gstar) \leq \frac{0.01}{\kappa(\Sigmax)\kappa(\Fstar)}$ and $n \gtrsim  \max\scurly{1,\overline\sigma^2} \paren{\dx + \log(1/\delta)}$, we have with probability $\geq 1 - \delta$:
    \begin{align*}
        \dist(\bG_+, \Gstar) &\leq (1 - 0.9\eta_\sbG) \dist(\bG, \Gstar) + \calO(1)\, \eta_\sbG \overline\sigma\sqrt{\frac{\dx+\log(1/\delta)}{n}}.
    \end{align*}
\end{restatable}
\sloppy
Crucially, the contraction factor is condition-number-free, subverting the lower bound in \Cref{prop:linrep_SGD_lower_bound} for sufficiently ill-conditioned $\Fstar$. Therefore, setting $\eta_\sbG$ near $1$ ensures a universal constant contraction rate.
Curiously, our proposed stylized \KFAC \eqref{eq:linrep_update} aligns with an alternating ``min-min'' scheme \cite{jain2013low, thekumparampil2021sample}, where $\bF,\bG$ are alternately updated via solving the convex quadratic least-squares problem, by setting $\eta_\sbF = \eta_\sbG = 1$. However, our experiments (see \Cref{fig:lr_sweep}) demonstrate $\eta_\sbG = 1$ is generally suboptimal, highlighting the flexibility of viewing \KFAC as a descent method.

\subsubsection{Transfer Learning}\label{sec:lin_rep_BN}
The upshot of representation learning is the ability to \emph{transfer} (e.g.\ fine-tune) to a distinct, but related, task by only retraining $\bF$ \citep{du2020few, kumar2022fine}. Assume we now have target data generated by:
\begin{align}\label{eq:linrep_target_datagen}
    \xtest[i] \iidsim \normal(0, \Sigmatest),\quad\; \ytest[i] = \Fstartest \Gstar \xtest[i] + \bveps_i,
\end{align}
where $\bveps_i \iidsim \normal(\bzero, \Sigmaeps)$, $\Fstartest \in \R^{\dy \times k}$. Notably, $\Gstar$ is shared with the ``training'' distribution \eqref{eq:linrep_datagen}. Given $\widehat\bG$ (e.g.\ by running \eqref{eq:linrep_update} on training task), we consider fitting the last layer $\bF$ given a batch of $\ntest$ data from the target task \eqref{eq:linrep_target_datagen}.
\begin{restatable}{lemma}{LinRepTransfer}\label{lem:linrep_transfer}
    Let $\Flstest = \argmin_{\widehat \bF} \; \hatExtest[\norm{\ytest - \widehat \bF \ztest}_2^2]$, $\ztest \triangleq \hatG \xtest$ be the optimal $\bF$ on the batch of $\ntest$ target data \eqref{eq:linrep_target_datagen} given $\hatG$. Defining $\nu = \dist(\hatG, \Gstar)$, given $\ntest \gtrsim k + \log(1/\delta)$, we have with probability $\geq 1-\delta$:
    \begin{align*}
        &\Ltest(\Flstest, \widehat \bG) \triangleq \Ex\brac{\norm{\ytest - \Fstartest\Gstar\xtest}_2^2} 
        \lesssim \norm{\Fstartest}^2_F\lmax(\Sigmatest) \nu^2  + \frac{\sigmaeps^2(\dy k + \log(1/\delta))}{\ntest}.
    \end{align*}
\end{restatable}

As hoped, the MSE of the fine-tuned predictor decomposes into a bias term scaling with the quality of $\hatG$, and a noise term scaling with $\dim(\bF)/\ntest$. We comment the required data is $\approx k$ rather than $\approx \dx$ resulting from doing regression from scratch \cite{wainwright2019high}. Additionally, the noise term scales with $\dim(\bF)=\dy k$ rather than $\dy\dx$ of the full predictor space.
The transfer learning set-up \eqref{eq:linrep_target_datagen} also reveals why data normalization (e.g.\ whitening, batch-norm \cite{ioffe2015batch}) can be counterproductive. To illustrate this, consider perfectly whitening the training covariates $\bfv = \Sigmax^{-1/2}\bfx$. By this change of variables, the ground-truth predictor changes $\bfy \approx \Fstar\Gstar \bfx = \Fstar\Gstar\Sigmax^{1/2}\bfv$. This is unproblematic so far---in fact, since the covariates $\bfv$ are isotropic, \SGD now may converge. However, instead of $\rowsp(\Gstar)$, the representation now converges to $\rowsp(\Gstar\Sigmax^{1/2})$. Deploying on the target task, since $\Sigmax \neq \Sigmatest$, we have $\rowsp(\hatG) \approx \rowsp(\Gstar\Sigmax^{1/2}) \neq \rowsp(\Gstar(\Sigmatest)^{1/2})$. In other words, in return for stabilizing optimization, normalizing the data destroys the shared structure of the predictor model! We illustrate this effect in \Cref{fig:batchnorm_subpace_dist}.


\subsection{Single Index Learning}\label{sec:single_index}

Assume that we observe $n$ i.i.d. samples generated according to the following single-index model:
\begin{align}
    \label{eq:data_gen}
    \bfx_i \overset{\mathrm{i.i.d.}}{\sim} \normal(0, \bSigma_\bfx),  \quad y_i = \sigma_\star( \vbeta_\star^\top \bfx_i) + \varepsilon_i
\end{align}
where $\bSigma_\bfx \in \R^{\dx \times \dx}$ is the input covariance, $\sigma_\star:\R\to\R$ is the teacher activation function, $\vbeta_\star \in \R^{\dx}$ is the (unknown) target direction, and $\ep_i$ is an additive noise $\ep_i \overset{\mathrm{i.i.d.}}{\sim} \normal(0, \sigma_\ep^2)$ independent of all other sources of randomness. 
We also make the following common assumption on $\vbeta_\star$ (\citet{dicker2016ridge,dobriban2018high,tripuraneni2021covariate,moniri_atheory2023,moniri2024asymptotics}, etc.) that ensures the covariates $\bfx_i$ alone do not carry any information about the target direction.
\begin{assumption}
    \label{assumption:random-effect}
    The  vector $\vbeta_\star$ is drawn from $\vbeta_\star \sim \normal(\mathbf{0}, \dx^{-1} \bI_{\dx})$ independent of other sources of randomness.
\end{assumption}

In this section, we study the problem of fitting a two-layer feedforward neural network $f_{\bff, \bfG}$ for prediction of unseen data points drawn independently from \eqref{eq:data_gen} at test time. 
When $\bG$ is kept at a random initialization and $\bff$ is trained using ridge regression, the model coincides with a random features model \cite{RahimiRecht,montanari2019generalization,hu2022universality} and has repeatedly used as a toy model to study and explain various aspects of practical neural networks (see  \citet{lin2021causes,adlam2020understanding,tripuraneni2020theory,hassani2022curse,bombari23robustness,disagreement,bombari2023stability,bombari2024privacy}, etc.).

When the covariates are isotropic $\bSigma_{\bfx} = \bI_{\dx}$, it is shown that a single step of full-batch \SGD update on $\bfG$ can drastically improve the performance of the model over random features as a result of \textit{feature learning} by aligning the top right-singular-vector of the updated representation layer $\bfG$ with the direction $\vbeta_\star$ \citep{damian2022neural,ba2022high,moniri_atheory2023,cuiasymptotics,dandi2023learning,dandi2024random,dandibenefits}. In this section, we assume that the covariates are anisotropic and show that in this case, the one-step full batch \SGD is suboptimal and can learn an ill-correlated direction even when the sample size $n$ is large. We then demonstrate that the \KFAC update with the preconditioners from \eqref{eq:precond} is in fact the natural fix to the full batch \SGD. 

\paragraph{Full-Batch \SGD.} Following the prior work, at initialization, we set $ \bff = \dhid^{-1/2} \bff_0$ with $\bff_0 \sim \normal(0, \dhid^{-1}\bI_{\dx})$, and  $\bfG= \bfG_0$ with i.i.d. $\normal(0, \dx^{-1})$ entries. We update $\bfG$ with one step of full batch \SGD  with step size $\eta_{\sbG} = \eta \sqrt{\dhid}$; i.e.,
\begin{align*}
    \bfG_{\SGD } \triangleq \bfG_0  - \eta \sqrt{\dhid} \; \nabla_\bfG \hatL(\bff_0, \bfG_0).
\end{align*}
In the following theorem, we provide an approximation of the updated first layer $\bfG_{\SGD }$, which is a generalization of \citep[
Proposition 2.1]{ba2022high} for $\bSigma_\bfx \neq \bI_{\dx}$.
\begin{restatable}{theorem}{OneStepSGD}
    \label{thm:rank1}
    Assume that the activation function $\sigma$ is $\calO(1)$-Lipschitz and that Assumption~\ref{assumption:random-effect} holds. In the limit where $n, \dx, \dhid$ tend to infinity proportionally, 
    the matrix $\bfG_{\textup{\SGD }}$, with probability $1 - o(1)$, satisfies
    \begin{align*}
        \left\|\bfG_0 + {\alpha \eta}\, \bff_0 \betaSGD^\top - \bfG_{\textup{\SGD }}\right\|_{\rm op} \to 0,
    \end{align*}
    in which $\alpha = \Ex_{z}[\sigma'(z)]$ with $z \sim \normal(0,\dx^{-1}\trace(\bSigma_\bfx))$, and the vector $\betaSGD$ is given by $\betaSGD = n^{-1} \bX^\top \vy$.
\end{restatable}
This theorem shows that one step of full batch \SGD  update approximately adds a rank-one component $\alpha\, \eta \, \bff_0 \betaSGD^\top$ to the initialized weights $\bfG_0$. Thus, the pre-activation features for a given input $\bfx \in \R^{\dx}$ after the update are given by
\begin{align*}
    \bfh_{\SGD } = \bG_{\SGD}\bfx \approx \bfG_0 \bfx + \alpha\, \eta \left(\betaSGD^\top \bfx\right) \, \bff_0 \in \R^{\dhid}
\end{align*}
where the first and second term correspond to the \textit{random feature}, and the \textit{learned feature} respectively. 
To better understand the learned feature component, note that defining $c_{\star, 1} = \Ex_{z\sim\normal(0, \dx^{-1}\trace(\bSigma_\bfx))} [\sigma_{\star}'(z)]$,  the target function $\sigma_\star(\vbeta_\star^\top \bfx)$ can be decomposed as
\begin{align*}
    \sigma_\star( \vbeta_\star^\top \bfx)  = c_{\star, 1}\vbeta_\star^\top \bfx + \sigma_{\star, \perp}(\vbeta_\star^\top \bfx)
\end{align*}
satisfying $\Ex_\bfx\left[ c_{\star, 1} (\vbeta_\star^\top \bfx)\, \sigma_{\star, \perp}(\vbeta_\star^\top \bfx)\right] = 0$. Therefore, when $c_{\star, 1} \neq 0$, the target function has a \textit{linear part}. Full batch \SGD is estimating the direction of $\vbeta_\star$ using this linear part with the estimator $\betaSGD = \bfX^\top\bfy/n$. However, the natural choice for this task is in fact ridge regression $\hat\vbeta_\lambda = (\hatSigma + \lambda \, \bI_{\dx})^{-1} \bX^\top \vy/n$, and $\betaSGD$ is missing the prefactor $(\hatSigma + \lambda \bI_{\dx})^{-1}$. In the isotropic case $\bSigma_{\bfx} = \bI_{\dx}$, we expect 
$\hatSigma \approx \bI_{\dx}$ 
when $n \gg \dx$. Thus, in this case the estimator $\betaSGD$ is roughly equivalent to the ridge estimator and can recover the direction $\vbeta_\star$. However, in the anisotopic case, $\betaSGD$ is biased even when $n \gg \dx$. To make these intuitions rigorous, we characterize in the following proposition the correlation between the learned direction $\betaSGD$ and the true direction $\vbeta_\star$.

\begin{restatable}{lemma}{TildeAlignment}
    \label{lemma:beta_tilde_alignment}
    Under the assumptions of Theorem~\ref{thm:rank1}, the correlation between $\vbeta_\star$ and $\betaSGD$ satisfies
    \begin{align*}
        \left|\frac{\vbeta_\star^\top \betaSGD}{\|\betaSGD\|_2  \|\vbeta_\star\|_2}  - \frac{\frac{c_{\star,1}}{\dx}\trace(\bSigma_\bfx) }{\sqrt{\frac{ c_{\star}^2 + \sigma_\ep^2}{n}\trace(\bSigma_\bfx) +\frac{c_{\star,1}^2}{\dx}    \trace(\bSigma_\bfx^2) }}\right| \to 0
    \end{align*}
    with probability $1 - o(1)$, in which $c_{\star, 1} = \Ex_{z} [\sigma_{\star}'(z)]$ and $c_{\star}^2 = \Ex_z[\sigma_{\star}^2(z)] $ with $z \sim \normal(0,\dx^{-1}\trace(\bSigma_\bfx))$.
\end{restatable}
This lemma shows that the correlation is increasing in the strength of the linear component $c_{\star, 1}$ while keeping the signal strength $c_{\star}$ fixed. Also, based on this lemma, when $n \gg \dx$, the correlation is given by ${\dx^{-1} \trace(\bSigma_\bfx)}/{\sqrt{\dx^{-1}\trace(\bSigma_{\bfx}^2)}}$, which is equal to one \emph{if and only if} $\bSigma_{\bfx} = \sigma^2\, \bI_{\dx}$ for some $\sigma \in \R$. This means these are the only covariance matrices for which applying one step of full batch \SGD  update learns the correct direction of $\vbeta_\star$. 

\paragraph{Stylized \KFAC.} This time, we update $\bfG$ using the stylized \KFAC  update from \eqref{eq:KFAC_update} with the regularized $\bP_\sbG$. We use the same initialization as full-batch \SGD. The updated representation layer in this case is given by 
\begin{align*}
    \bfG_{\KFAC } \triangleq \bfG_0  - \eta \sqrt{\dhid} \; \nabla_\bfG \hatL(\bff_0, \bfG_0)\; (\bQ_{\sbG}+\lambda_\sbG \bI_{\dx})^{-1}.
\end{align*}
The preconditioning factor $(\bQ_{\sbG}+\lambda_\sbG \bI_{\dx})^{-1}$ with $\bQ_{\sbG} = \hatSigma$ is precisely the factor required so that the direction learned by the one-step update to match the ridge regression estimator with ridge parameter $\lambda_\sbG$ as shown in the following immediate corollary of Theorem~\ref{thm:rank1}.
\begin{corollary}
    \label{corr:rank1}
    Under the same set of assumptions as Theorem~\ref{thm:rank1},
    the matrix $\bfG_{\textup{\KFAC }}$, satisfies
    \begin{align*}
        \left\|\bfG_0 + {\alpha \eta}\, \bff_0  \betaKFAC^\top - \bfG_{\textup{\KFAC }}\right\|_{\rm op} \to 0
    \end{align*}
      with probability $1 - o(1)$, where $\alpha$ is defined in Theorem~\ref{thm:rank1}, and 
    $\betaKFAC =  (\bQ_{\sbG}+\lambda_\sbG \bI_{\dx})^{-1}\bX^\top \vy/n$.
\end{corollary}
Because $\betaKFAC$ is equivalent to ridge regression, we expect it to align well with $\vbeta_\star$ even for anisotropic $\bSigma_{\bfx}$, given a proper choice of $\lambda_{\sbG}$.  The following lemma formally characterizes the correlation between $\betaKFAC$ and $\vbeta_\star$ for any $\lambda_{\sbG}\in \R$.

\begin{restatable}{lemma}{HatAlignment}
    \label{lemma:beta_hat_alignment}
    Under the assumptions of Theorem~\ref{thm:rank1}, the correlation between $\vbeta_\star$ and $\betaKFAC$ satisfies
    \begin{align*}
        \left|\frac{\betaKFAC^\top \, \vbeta_\star}{\|\betaKFAC\|_2  \|\vbeta_\star\|_2}  - \frac{c_{\star,1} \Psi_1}{\sqrt{c_{\star,1}^2 \Psi_2 + \frac{\dx}{n} (c_{\star, >1}^2 + \sigma_\ep^2)\Psi_3}}\right| \to 0
    \end{align*}
     with probability $1 - o(1)$, where  $c^2_{\star, 1} = \Ex_{z}^2 [\sigma_{\star}'(z)]$, $c_{\star, >1}^2 = \Ex_{z}[\sigma_{\star, \perp}^2(z)]$ with $z \sim \normal(0, \dx^{-1}\trace(\bSigma_\bfx))$, and  $\Psi_1, \Psi_2, \Psi_3$ are defined in \eqref{eq:psi_def} and depend on $\bSigma_\bfx$, ${\dx}/{n},$ and $\lambda_{\sbG}$. In particular, as $\lambda_{\sbG} \to 0$ and $\dx/n \to 0$, we have
     \begin{align*}
          \frac{\betaKFAC^\top \, \vbeta_\star}{\|\betaKFAC\|_2  \|\vbeta_\star\|_2} \to 1.
     \end{align*}
\end{restatable}

This lemma shows that when $n \gg \dx$, and $\lambda_\sbG\to 0$,  the one-step stylized \KFAC  update---unlike the one-step full-batch \SGD ---perfectly 
recovers the target direction \(\vbeta_\star\), fixing the issue with full batch \SGD with anisotropic covariances.


\begin{remark}  It is well-known that, given features that align with $\vbeta_\star$, applying least-squares on $\bZ=\sigma(\bG_{\textup{\KFAC}}\bX)$, which from \Cref{lem:KFAC_is_EMA} is equivalent to the \KFAC $\bff$-update with $\eta_{\bff} = 1$, leverages the feature to obtain a solution with good generalization. See \Cref{sec:single_generalize} for more details.
    
\end{remark}
