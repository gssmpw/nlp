\section{Proofs and Additional Details for \Cref{sec:lin_rep}}

\subsection{Convergence Rate Lower Bound of \SGD}\label{sec:SGD_lower_bound}

Our goal is to establish the following lower bound construction.
\LinRepLowerBound*

\begin{proof}[Proof of \Cref{prop:linrep_SGD_lower_bound}]
    We prove the lower bound by construction. First, we write out the one-step \SGD update given step size $\eta_\sbG$.
    \begin{align*}
        \overline\bG_+ &= \bG - \eta_\sbG  \nabla_{\sbG} \hatL(\bfF, \bfG) \\
        &= \bG - \frac{1}{n}\bfF^\top \mem\paren{\bfF \bG \bX^\top \bX - \bY^\top \bX} \\
        &= \bG - \bfF^\top \mem\paren{\bfF\bG - \Fstar \Gstar }. \tag{$\bY^\top  = \Fstar \Gstar \bX^\top + \Eps^\top$, $n = \infty$, $\bSigma_\bfx = \bI$} \\
        \bG_+ &= \mathrm{Ortho}(\overline\bG_+).
    \end{align*}
    We recall $\bF$ is given by the $\bF$-update in \eqref{eq:KFAC_update} with $\eta_\sbF = 1$, which by \Cref{lem:KFAC_is_EMA} is equivalent to setting $\bF$ to the least-squares solution conditional on $\bG$:
    \begin{align*}
        \bF &= \bY^\top \bZ \, (\bZ^\top \bZ)^{-1} = \Fstar\Gstar \bSigma_\bfx \bG^\top \paren{\bG \bSigma_\bfx \bG^\top}^{-1} \tag{$\bfz = \bG \bfx$, $n = \infty$} \\
        &= \Fstar\Gstar \bG^\top \paren{\bG \bG^\top}^{-1} = \Fstar\Gstar \bG^\top. \tag{$\bG$ row-orthonormal}
    \end{align*}
    Therefore, plugging in $\bF$ into the $\SGD$ update yields:
    \begin{align*}
        \overline\bG_+ &= \bG - \eta_\sbG \bfF^\top \mem\paren{\bfF\bG - \Fstar \Gstar}= \bG - \eta_\sbG \bG \Gstar^\top \Fstar^\top (\Fstar\Gstar \bG^\top \bG - \Fstar \Gstar)
    \end{align*}
    Before proceeding, let us present the construction of $\Fstar, \Gstar$. We focus on the case $\dx = 3$, $k = 2$, as it will be clear the construction is trivially embedded to arbitrary $\dx > k \geq 2$. We observe that $\Fstar$ only appears in the \SGD update in the form $\Fstar^\top \Fstar \in \R^{k \times k}$, thus $\dy \geq k$ can be set arbitrarily as long as $\Fstar^\top \Fstar$ satisfies our specifications. Set $\Fstar, \Gstar$ such that
    \begin{align*}
        \Fstar^\top \Fstar &= \bmat{1 - \lambda & 0 \\ 0 & \lambda}, \lambda \in (0,1/2], \quad \Gstar = \bmat{1 & 0 & 0 \\ 0 & 1 & 0}.
    \end{align*}
    Accordingly, the initial representation $\bG_0$ (which the learner is not initially given) will have form
    \begin{align*}
        \bG_0 &= \bmat{1 & 0 & 0\\ 0 & \sqrt{1 - \varepsilon_0^2} & \varepsilon_0 }, \text{ or } \bmat{\sqrt{1 - \varepsilon_0^2} & 0 &\varepsilon_0 \\ 0 & 1 & 0}.
    \end{align*}
    We prove all results with the first form of $\bG_0$, as all results will hold for the second with the only change swapping $\lambda, 1 - \lambda$. It is clear that we may extend to arbitrary $\dx > k \geq 2$ by setting:
    \begin{align*}
        \Fstar^\top \Fstar &= \bmat{
        (1-\lambda) \bI_{k-1} & \bzero \\ \bzero & \lambda
        }, \quad \Gstar = \bmat{\bI_{k} & \bzero_{\dx-k}}, \quad \bG_0 = \bmat{\bI_{k-1} & \bzero & \cdots &  \\
        0 & \sqrt{1 - \varepsilon_0^2} & \varepsilon_0 & \bzero }.
    \end{align*}
    Returning to the $\dx = 3, k = 2$ case, we first prove the following invariance result.
    \begin{lemma}\label{lem:LB_facts}
        Given $\bG_0 = \bmat{1 & 0 & 0\\ 0 & \sqrt{1 - \varepsilon_0^2} & \varepsilon_0 }$, then for any $t \geq 0$, $\bG_t = \bmat{1 & 0 & 0 \\ 0 & c_1 & c_2}$ for some $c_1^2 + c_2^2 = 1$. 
        Furthermore, we have $\dist(\bG_t, \Gstar) = \abs{c_2}$.
    \end{lemma}
    \begin{proof}[Proof of \Cref{lem:LB_facts}]
    This follows by induction. The base case follows by definition of $\bG_0$. Now given $\bG_t = \bmat{1 & 0 & 0 \\ 0 & c_1 & c_2}$ for some $c_1,c_2$, we observe that
    \begin{align*}
        \bG_t \Gstar^\top = \bmat{1 & 0 \\ 0 & c_1 }, \quad
        \Fstar^\top \Fstar = \bmat{1 - \lambda & 0 \\ 0 & \lambda}.
    \end{align*}
    Notably, we may write
    \begin{align}
    \begin{split}\label{eq:LB_grad_update}
        \overline\bG_{t+1} &= \bG_t - \eta_\sbG  \bG_t \Gstar^\top \Fstar^\top (\Fstar\Gstar \bG_t^\top \bG_t - \Fstar \Gstar) \\
        &= \paren{\bI_{k} - \eta_\sbG  \bG_t \Gstar^\top \Fstar^\top \Fstar\Gstar \bG_t^\top } \bG_t + \eta_\sbG  \bG_t \Gstar^\top \Fstar^\top \Fstar \Gstar \\
        &= \paren{\bI_k - \eta_\sbG \bmat{1-\lambda & 0 \\ 0 & c_1^2 \lambda  }} \bG_t + \eta_\sbG \bmat{1-\lambda & 0 \\ 0 & c_1 \lambda  } \Gstar \\
        &= \bmat{1 & 0 & 0 \\
        0 & c_1 (1 + \eta_\sbG c_2^2 \lambda) & c_2 (1 - \eta_\sbG c_1^2 \lambda)} \\
        \bG_{t+1} &= \mathrm{Ortho}(\overline\bG_{t+1}).
    \end{split}
    \end{align}
    Therefore, $\bG_{t+1}$ shares the same support as $\bG_t$, and by the orthonormalization step, the squared entries of the second row of $\bG_{t+1}$ equal $1$, completing the induction step.

    To prove the second claim, we see that $\Gperp = \bmat{\bzero_2 & \\ & 1}$, and since $\bG_t$ is by assumption row-orthonormal, we have
    \begin{align*}
        \dist(\bG_t, \Gstar) &= \norm{\bG_t \Gperp} = \opnorm{\bmat{0 & 0 & c_2}^\top} = \abs{c_2},
    \end{align*}
    completing the proof.
    \end{proof}

With these facts in hand, we prove the following stability limit of the step-size, and the consequences for the contraction rate.
\begin{lemma}\label{lem:LB_max_lr}
    If $\eta_\sbG \geq \frac{4}{1-\lambda}$, then for any given $\dist(\bG_0, \Gstar)$ we may find $\bG_0$ such that $\limsup_t \dist(\bG_t, \Gstar) \geq \frac{1}{2}$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:LB_max_lr}]
    By assumption $\lambda \leq 1/2$ and thus $\lambda \leq 1- \lambda$. Evaluating \Cref{lem:LB_facts} instead on $\bG_0 = \bmat{\sqrt{1 - \varepsilon_0^2} & 0 &\varepsilon_0 \\ 0 & 1 & 0}$, writing out \eqref{eq:LB_grad_update} yields symmetrically:
\begin{align*}
    \dist(\bG_t, \Gstar) &= \abs{c_2} \\
    \overline\bG_{t+1} &= \bmat{
        c_1 (1 + \eta_\sbG c_2^2 (1-\lambda) ) & 0 & c_2 (1 - \eta_\sbG c_1^2 (1-\lambda) ) \\
        0 & 1 & 0} \\
    \bG_{t+1} &= \mathrm{Ortho}(\overline\bG_{t+1}).
\end{align*}
We first observe that regardless of $\eta_\sbG$, the norm of the first row $\bG_{t+1}$ is always greater than $1$ pre-orthonormalization. Let us define $\omega = \eta_\sbG(1-\lambda)$. Then, the squared-norm of the first row satisfies:
\begin{align*}
    \paren{c_1 (1+ \omega c_2^2)}^2 + \paren{c_2 (1 - \omega c_1^2)}^2 &= 1 + \omega^2 c_1^2 c_2^2. 
\end{align*}
Therefore, the norm is strictly bounded away from $1$ when $\omega > 0$ and either $c_1,c_2\neq 0$ by the constraint $c_1^2 + c_2^2 = 1$. Importantly, this implies that regardless of the step-size taken, the resulting first-row norm of $\bG_+$ must exceed $1$ prior to orthonormalization. Given this property, we observe that for $\omega \geq 1/c_1^2$, we have:
\begin{align*}
    \frac{\abs{1-\omega c_1^2}}{\abs{1+\omega c_2^2}} = \frac{\omega c_1^2 - 1}{1 + \omega c_2^2}.
\end{align*}
When this ratio is greater than $1$, we are guaranteed that the first-row coefficients $c_1', c_2'$ of $\bG_{t+1}$ \textit{post}-orthonormalization satisfy $c_2'/c_1' > c_2/c_1$, and recall from \Cref{lem:LB_facts} $\dist(\bG_{t+1}, \Gstar) = c_2'$, and thus $\dist(\bG_{t+1}, \Gstar) > \dist(\bG_t, \Gstar)$. Rearranging the above ratio, this is equivalent to the condition $\omega = \eta_\sbG (1-\lambda) \geq \frac{2}{c_1^2 - c_2^2}$, $\omega = \eta_\sbG (1-\lambda) \geq 1/c_1^2$. Setting $c_1^2 = 3/4, c_2^2 = 1/4$, this implies for $\eta_\sbG \geq \frac{4}{1-\lambda}$, the moment $\dist(\bG_t, \Gstar) \leq c_2 = 1/2$, then we are guaranteed $\dist(\bG_{t+1}, \Gstar) > \dist(\bG_t, \Gstar)$, and thus $\limsup_{t \to \infty} \dist(\bG_t, \Gstar) \geq \frac{1}{2}$, irregardless of $\dist(\bG_0, \Gstar)$.
\end{proof}

Now, to finish the construction of the lower bound, \Cref{lem:LB_max_lr} establishes that $\eta_\sbG \leq \frac{4}{1-\lambda}$ is necessary for convergence (though not sufficient!). This implies that when we plug back in $\bG_0 = \bmat{1 & 0 & 0\\ 0 & \sqrt{1 - \varepsilon_0^2} & \varepsilon_0 }$, we have $\bG_{t+1}$:
\begin{align*}
    \overline\bG_{t+1} &= \bmat{1 & 0 & 0 \\
        0 & c_1 (1 + \eta_\sbG c_2^2 \lambda) & c_2 (1 - \eta_\sbG c_1^2 \lambda)} \\
    \bG_{t+1} &= \mathrm{Ortho}(\overline\bG_{t+1}) = \bmat{1 & 0 & 0 \\
        0 & c_1' & c_2'}.
\end{align*}
We have trivially $1 - \eta_\sbG c_1^2 \lambda \geq 1 - \eta_\sbG \lambda$. Therefore, for $\lambda \leq 1/5$ such that $\frac{\lambda}{1-\lambda}\leq 1/4$, we have $1 - \eta_\sbG \lambda \geq 1 - \frac{4\lambda}{1-\lambda} \geq 0$.
As shown in the proof of \Cref{lem:LB_max_lr}, the norm of the second row pre-orthonormalization is strictly greater than $1$, and thus:
\begin{align*}
    \dist(\bG_{t+1}, \Gstar) = c_2' &\geq c_2 (1 - \eta_\sbG\lambda c_1^2) \geq (1 - \eta_\sbG \lambda) \dist(\bG_t, \Gstar)\geq \paren{1 - 4\frac{\lambda}{1-\lambda}}\dist(\bG_t, \Gstar).
\end{align*}
Applying this recursively to $\bG_0$ yields the desired lower bound.
\end{proof}



\subsection{Proof of \Cref{thm:linrep_kfac_guarantee}}

Recall that running an iteration of stylized \KFAC \eqref{eq:KFAC_update} with $\lambda_\sbF, \lambda_\sbG = 0$, $\eta_\sbF = 1$ yields:
\begin{align}
\begin{split}\label{eq:expanding_KFAC_update}
    \overline\bG_+ &= \bG - \eta_{\sbG} \bP_\sbG^{-1}\, \nabla_{\sbG} \hatL(\bfF, \bfG)\, (\bQ_\sbG + \lambda_\sbG \bI_{\dx})^{-1} \\
    &= \bG - \eta_\sbG (\bF^\top \bF)^{-1}\bfF^\top (\bfF \bG \hatSigma - \Fstar \Gstar \hatSigma - \frac{1}{n}\Eps^\top \bX ) \hatSigma^{-1} \\
    &= \bG - \eta_\sbG (\bF^\top \bF)^{-1}\bfF^\top (\bfF \bG - \Fstar \Gstar) + (\bF^\top \bF)^{-1}\bF^\top\Eps^\top \bX (\bX^\top \bX)^{-1},
\end{split} 
\end{align}
where the matrix $\bF$ is given by
\begin{align*}
        \bF &= \bF_{\rm{prev}} - \eta_\sbF \bP_\sbF^{-1}\, \nabla_\sbF \hatL(\bF_{\rm{prev}}, \bfG)\, (\bQ_{\sbF}+\lambda_\sbF \bI_{\dhid})^{-1} = \bY^\top \bZ (\bZ^\top \bZ)^{-1} \\
        &= \Fstar \Gstar \bX^\top \bZ (\bZ^\top \bZ)^{-1} + \Eps^\top \bZ (\bZ^\top \bZ)^{-1},
\end{align*}
recalling that $\bfz \triangleq \bG \bfx$.
Focusing on the representation update, we have
\begin{align*}
    \overline\bG_+ \Gperp &= (1 - \eta_\sbG) \bG \Gperp + \eta_\sbG (\bF^\top \bF)^{-1}\bfF^\top \Eps^\top \bX (\bX^\top \bX)^{-1}.
\end{align*}
Therefore, to prove a one-step contraction of $\rowsp(\bG_+)$ toward $\rowsp(\bG_\star)$, we require two main components:
\begin{itemize}
    \item Bounding the noise term $\eta_\sbG (\bF^\top \bF)^{-1}\bfF^\top \Eps^\top \bX^\top (\bX^\top \bX)^{-1}$.
    \item Bounding the orthonormalization factor; the subspace distance measures distance between two orthonormalized bases (a.k.a.\ elements of the Stiefel manifold \citep{absil2008optimization}), while a step of \SGD or \KFAC does not inherently conform to the Stiefel manifold, and thus the ``off-manifold'' shift must be considered when computing $\dist(\bG_+, \Gstar)$. This amounts to bounding the ``$\bR$''-factor of the $\bQ\bR$-decomposition \citep{tref2022numerical} of $\overline\bG_+$.
\end{itemize}
Thanks to the left-preconditioning by $(\bF^\top \bF)^{-1}$, the contraction factor is essentially determined by $(1- \eta_\sbG)$; however, the second point about the ``off-manifold'' shift is what prevents us from setting $\eta_\sbG = 1$.

\subsection*{Bounding the noise term} 

We start by observing $\Eps^\top \bX(\bX^\top \bX)^{-1} = \sum_{i=1}^n \bveps_i \bfx_i^\top \paren{\sum_{i=1}^n \bfx_i \bfx_i^\top}^{-1}$ (and thus $(\bF^\top \bF)^{-1}\bfF^\top \Eps^\top \bX (\bX^\top \bX)^{-1}$) is a least-squares error-like term, and thus can be bounded by standard self-normalized martingale arguments. In particular, defining $\Fbar \triangleq (\bF^\top \bF)^{-1}\bF^\top$ we may decompose
\begin{align*}
    \opnorm{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1}} &\leq \opnorm{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1/2}} \lmin(\bX^\top \bX)^{-1/2},
\end{align*}
where the first factor is the aforementioned self-normalized martingale (see e.g.\ \citet{abbasi2011regret, ziemann2023tutorial}), and the second can be bounded by standard covariance lower-tail bounds. Toward bounding the first factor, we invoke a high-probability self-normalized bound:
\begin{lemma}[cf.\ {\citet[Theorem 4.1]{ziemann2023tutorial}}]\label{lem:yasin_SNM}
Let $\scurly{\bfv_i, \bfw_i}_{i \geq 1}$ be a $\R^{d_v} \times \R^{d_w}$-valued process
and $\scurly{\calF_i}_{i \geq 1}$ be a filtration such that
$\scurly{\bfv_i}_{i \geq 1}$ is adapted to $\scurly{\calF_i}_{i \geq 1}$,
$\scurly{\bfw_i}_{i \geq 1}$ is adapted to $\scurly{\calF_i}_{i \geq 2}$, and 
$\scurly{\bfw_i}_{i \geq 1}$ is a $\sigma^2$-subgaussian martingale difference sequence\footnote{See \Cref{appdx:subgaussian} for discussion of formalism. It suffices for our purposes to consider $\bfw \iidsim \normal(\bzero, \bSigma_\bfw)$.}.
Fix (non-random) positive-definite matrix $\bQ$.
For $k \geq 1$, define $\hatSigma[k] \triangleq \sum_{i=1}^{k} \bfv_i \bfv_i^\top$.
Then, given any fixed $n \in \N_+$, with probability at least $1-\delta$:
\begin{align}
    \opnorm{\sum_{i=1}^n \bfw_i \bfv_i^\top \paren{\bQ + \hatSigma[n]}^{-1/2} }^2 \leq 4 \sigma^2  \log\paren{\frac{\det\paren{\bQ + \hatSigma[n]}}{\det(\bQ)}} + 13 d_w \sigma^2 + 8\sigma^2 \log(1/\delta).
\end{align}
\end{lemma}
Instantiating this for Gaussian $\bfw_i \iidsim \normal(\bzero, \bSigma_\bfw)$, $\bfv_i \iidsim \normal(\bzero, \bSigma_\bfv)$, we may set $\bQ \approx \bSigma_\bfv$ to yield:
\begin{lemma}\label{lem:SNM_bound}
    Consider the quantities defined in \Cref{lem:yasin_SNM} and assume $\bfw_i \iidsim \normal(\bzero, \bSigma_\bfw)$, $\bfv_i \iidsim \normal(\bzero, \bSigma_\bfv)$, defining $\sigma_\bfw^2 \triangleq \lmax(\bSigma_\bfw)$, $\sigma_\bfv^2 \triangleq \lmax(\bSigma_\bfv)$. Then, as long as $n \gtrsim \frac{18.27}{c^2} \paren{d_v + \log(1/\delta)}$, with probability at least $1 - \delta$:
    \begin{align*}
        &\opnorm{\sum_{i=1}^n \bfw_i \bfv_i^\top \paren{\hatSigma[n]}^{-1/2} }^2 \leq 8 d_v \log\paren{\frac{1+c}{1-c}} \sigma_\bfw^2  + 26d_w \sigma_\bfw^2 + 16\sigma_\bfw^2 \log(1/\delta) \\
        &\lmin(\hatSigma[n]) \geq (1-c) \lmin(\bSigma_\bfv).
    \end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lem:SNM_bound}]
We observe that if $\hatSigma[n] \succeq \bQ$, then
\[
2 \hatSigma[n] \succeq \bQ + \hatSigma[n] \implies \paren{\hatSigma[n]}^{-1} \preceq 2\paren{\bQ + \hatSigma[n]}^{-1}.
\]
This implies
\begin{align}\label{eq:SNM_event}
    &\ones\mem\curly{\hatSigma[n] \succeq \bQ} \bnorm{\sum_{i=1}^n \bfw_i \bfv_i^\top \paren{\hatSigma[n]}^{-1/2} }^2
    \leq 2 \ones\mem\curly{\hatSigma[n] \succeq \bQ}  \bnorm{\sum_{i=1}^n \bfw_i \bfv_i^\top \paren{\bQ + \hatSigma[n]}^{-1/2} }^2.
\end{align}
Let us consider the event:
\[
(1-c) \bSigma_\bfv \preceq \hatSigma[n] \preceq (1+c) \bSigma_\bfv,
\]
which by \Cref{lem:gauss_cov_conc} occurs with probability at least $1 - \delta$ as long as $n \gtrsim   \frac{18.27}{c^2}(d_v + \log(1/\delta))$. This immediately establishes the latter desired inequality. Setting $\bQ = (1-c) n \bSigma_\bfv$ and conditioning on the above event, we observe that by definition $\hatSigma[n] \succeq \bQ$, and
\begin{align*}
    \log \left(\frac{\det\paren{\bQ + \hatSigma[n]}}{\det(\bQ)}\right) &= \log \det\paren{I_{d_v} + \hatSigma[n]\paren{\bQ}^{-1}} \\
    &\leq \log \det\paren{\paren{1 + \frac{1+c}{1-c}}I_{d_v} }  \\
    &\leq d_v \log\paren{\frac{1+c}{1-c}}.
\end{align*}
Plugging this into \Cref{lem:yasin_SNM}, applied to the RHS of \eqref{eq:SNM_event}, we get our desired result.
\end{proof}
Therefore, instantiating $\bfw \to \Fbar \bveps$, $\bfv \to \bfx$, $(\bfx_i, \bveps_i)_{i \geq 1}$ is a $\R^{\dx} \times \R^k$-valued process. Furthermore, since we assumed out of convenience that $\bF, \bG_+$ are computed on independent batches of data, we have that $\Fbar \bveps \sim \normal(\bzero, \Fbar \bSigma_{\bveps} \Fbar^{\top})$. In order to complete the noise term bound, it suffices to provide a uniform bound on $\norm{\Fbar} = 1/\smin(\bF)$ in terms of $\Fstar$.
\begin{lemma}\label{lem:F_close_to_Fstar}
    Assume the following conditions hold:
    \begin{align*}
        &n \gtrsim \max\curly{k + \log(1/\delta),\; \sigmaeps^2 \frac{\dy + k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax)}} \\
        &\dist(\bG, \Gstar) \leq \frac{2}{5} \kappa(\Fstar)^{-1} \kappa(\Sigmax)^{-1},
    \end{align*}
    then with probability at least $1 - \delta$, we have $\norm{\Fbar} = 1/\smin(\bF) \leq 2 \smin(\Fstar)^{-1}$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:F_close_to_Fstar}]
    Recall we may write $\bF$ as
    \begin{align}
        \bF &= \Fstar \Gstar \bX^\top \bZ (\bZ^\top \bZ)^{-1} + \Eps^\top \bZ (\bZ^\top \bZ)^{-1} \nonumber \\
        &= \Fstar \Gstar \bG^\top + \Fstar \Gstar (\bI_{\dx} - \bG^\top \bG) \bX^\top \bZ (\bZ^\top \bZ)^{-1} + \Eps^\top \bZ (\bZ^\top \bZ)^{-1}\label{eq:linrep_Fls_expanded}
    \end{align}
    By Weyl's inequality for singular values \cite{horn2012matrix}, we have
    \begin{align*}
        \smin(\bF) &\geq \smin(\Fstar \Gstar \bG^\top) - \smax\paren{\Fstar \Gstar (\bI_{\dx} - \bG^\top \bG) \bX^\top \bZ (\bZ^\top \bZ)^{-1} + \Eps^\top \bZ (\bZ^\top \bZ)^{-1}}
    \end{align*}
    Since $\Gstar \bG^\top$ is an orthogonal matrix, the first term is equal to $\smin(\Fstar)$. On the other hand, applying triangle inequality on the second term, for $n \gtrsim k + \log(1/\delta)$ we have:
    \begin{align*}
        \opnorm{\Fstar \Gstar (\bI_{\dx} - \bG^\top \bG) \bX^\top \bZ (\bZ^\top \bZ)^{-1}} &\leq \opnorm{\Fstar}\opnorm{\Gstar (\bI_{\dx} - \bG^\top \bG)}\opnorm{\bX^\top \bZ (\bZ^\top \bZ)^{-1}} \\
        &\leq \opnorm{\Fstar} \dist(\bG, \Gstar) \paren{\frac{5}{4} \opnorm{\Sigmax} \lmin(\bG \Sigmax \bG^\top)} \\
        &\leq \frac{5}{4}\opnorm{\Fstar} \dist(\bG, \Gstar) \kappa(\Sigmax),
    \end{align*}
    where we used covariance concentration for the second inequality \Cref{lem:gauss_cov_conc} and the trivial bound $\lmin(\bA \bSigma \bA^\top) \geq \lmin(\bSigma)$ for the last inequality. In turn, we may bound:
    \begin{align*}
        \opnorm{\Eps^\top \bZ (\bZ^\top \bZ)^{-1}} &\leq \opnorm{\Eps^\top \bZ (\bZ^\top \bZ)^{-1/2}} \lmin(\bZ^\top \bZ)^{-1/2} \\
        &\lesssim \opnorm{\Eps^\top \bZ (\bZ^\top \bZ)^{-1/2}} n^{-1/2} \lmin(\Sigmax)^{-1/2} \tag{\Cref{lem:gauss_cov_conc}}\\
        &\lesssim \sigmaeps  \sqrt{\frac{\dy + k + \log(1/\delta)}{\lmin(\Sigmax) n}}.
    \end{align*}
    Therefore, setting $\dist(\bG, \Gstar) \leq \frac{2}{5} \kappa(\Fstar)^{-1} \kappa(\Sigmax)^{-1}$, and $n \gtrsim \sigmaeps^2 \frac{\dy + k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax)}$, we have $\smin(\bF) \geq \frac{1}{2}\smin(\Fstar)$, which leads to our desired bound on $\norm{\Fbar}$.
\end{proof}

With a bound on $\opnorm{\Fbar}$, bounding the noise term is a straightforward application of \Cref{lem:SNM_bound}.
\begin{proposition}[\KFAC noise term bound]\label{prop:KFAC_noise_bound}
    Let the conditions in \Cref{lem:F_close_to_Fstar} hold. In addition, assume $n \gtrsim \dx + \log(1/\delta)$. Then, with probability at least $1-\delta$:
    \begin{align*}
        \opnorm{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1}}
        &\lesssim \sigmaeps  \sqrt{\frac{\dx + k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax) n}}.
    \end{align*}
\end{proposition}


\begin{proof}[Proof of \Cref{prop:KFAC_noise_bound}]
    Condition on the event of \Cref{lem:F_close_to_Fstar}. Then, assuming $n \gtrsim \dx + \log(1/\delta)$, we may apply covariance concentration (\Cref{lem:gauss_cov_conc}) on $\hatSigma$ and \Cref{lem:SNM_bound} to bound the noise term by:
    \begin{align*}
        \opnorm{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1}} &\leq \opnorm{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1/2}} \lmin(\bX^\top \bX)^{-1/2} \\
        &\leq \opnorm{\Fbar}\opnorm{\Eps^\top \bX (\bX^\top \bX)^{-1/2}} n^{-1/2} \lmin(\Sigmax)^{-1/2} \tag{\Cref{lem:gauss_cov_conc}}\\
        &\lesssim \sigmaeps  \sqrt{\frac{\dx + k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax) n}}. \tag{\Cref{lem:SNM_bound}},
    \end{align*}
    which completes the proof.
\end{proof}
This completes the bound on the noise term. We proceed to the orthonormalization factor.

\subsection*{Bounding the orthonormalization factor}

Toward bounding the orthonormalization factor from \eqref{eq:linrep_update}. Defining $\overline\bG_+$ as the updated representation pre-orthonormalization, we write $\overline\bG_+ = \bR \bG_+$,
where $\bG_+$ is the orthonormalized representation and $\bR \in \R^{k \times k}$ is the corresponding orthonormalization factor. Therefore, defining the shorthand $\Sym(\bA) = \bA + \bA^\top$, we have
\begin{align*}
    \bR\bR^\top &= \bR \bG_+ (\bR \bG_+)^\top \\
    &= \paren{\bG - \eta_\sbG (\bF^\top \bF)^{-1}\bfF^\top (\bfF \bG - \Fstar \Gstar) + (\bF^\top \bF)^{-1}\bF^\top \Eps^\top \bX (\bX^\top \bX)^{-1}} \paren{\cdots}^\top \tag{from \eqref{eq:expanding_KFAC_update}}\\
    &\succ \bI_{k} - \eta_\sbG \Sym\Big(\underbrace{\Fbar(\bF\bG - \Fstar \Gstar)\bG^\top}_{\triangleq \Gamma_1}\Big) + \eta_\sbG \Sym\Big(\underbrace{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1} \bG^\top}_{\triangleq \Gamma_2}\Big) \\
    &\qquad \quad - \eta_\sbG^2 \Sym\paren{\Fbar(\bF\bG - \Fstar \Gstar)\paren{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1}}^\top},
\end{align*}
where the strictly inequality comes from discarding the positive-definite ``diagonal'' terms of the expansion. Therefore, by Weyl's inequality for symmetric matrices \cite{horn2012matrix}, we have:
\begin{align*}
    \lmin(\bR \bR^\top) &\geq 1 - 2\eta_\sbG \paren{\opnorm{\Gamma_1} + \opnorm{\Gamma_2} + \eta_\sbG\opnorm{\Gamma_1}\opnorm{\Gamma_2}}.
\end{align*}
Toward bounding $\opnorm{\Gamma_1}$, let the conditions of \Cref{lem:F_close_to_Fstar} hold. Then,
\begin{align*}
    \opnorm{\Gamma_1} &= \opnorm{\Fbar(\bF\bG - \Fstar \Gstar)\bG^\top} \\
    &= \opnorm{(\bF^\top \bF)^{-1} \bF^\top \bF \bG \bG^\top - (\bF^\top \bF)^{-1} \bF^\top\Fstar \Gstar \bG^\top} \\
    &= \opnorm{\Fbar \paren{\Fstar \Gstar (\bI_{\dx} - \bG^\top \bG) \bX^\top \bZ (\bZ^\top \bZ)^{-1} + \Eps^\top \bZ (\bZ^\top \bZ)^{-1}} } \tag{from \eqref{eq:linrep_Fls_expanded}} \\
    &\leq \frac{5}{4}\smin(\bF) \opnorm{\Fstar} \dist(\bG, \Gstar) \kappa(\Sigmax) + \smin(\bF) \sigmaeps^2 \sqrt{\frac{k + \log(1/\delta)}{\lmin(\Sigmax) n}} \\
    &\leq \underbrace{\frac{5}{2}\kappa(\Fstar) \dist(\bG, \Gstar) \kappa(\Sigmax)}_{\triangleq \gamma_1} + \sigmaeps^2 \sqrt{\frac{k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax) n}}. \tag{\Cref{lem:F_close_to_Fstar}}
\end{align*}
Similarly, letting the conditions of \Cref{prop:KFAC_noise_bound} hold, we have
\begin{align*}
    \opnorm{\Gamma_2} &= \opnorm{\Fbar \Eps^\top \bX (\bX^\top \bX)^{-1} \bG^\top} \\
    &\leq \sigmaeps  \sqrt{\frac{\dx + k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax) n}} \tag{\Cref{prop:KFAC_noise_bound}} \\
    &\triangleq \gamma_2.
\end{align*}
We observe that $\gamma_2$ will always dominate the second term of the bound on $\opnorm{\Gamma_1}$, and therefore:
\begin{align*}
    \lmin(\bR \bR^\top) &\geq 1 - 2\eta_\sbG(\gamma_1 + 2\gamma_2 + \eta_\sbG(\gamma_1 + \gamma_2)\gamma_2).
\end{align*}
Therefore, we have the following bound on the orthonormalization factor.
\begin{proposition}\label{prop:linrep_orth_bound}
Let the following conditions hold:
    \begin{align*}
        &n \gtrsim \max\curly{\dx + \log(1/\delta),\; \frac{\sigmaeps^2}{\gamma_2^2} \frac{\dx + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax)}} \\
        &\dist(\bG, \Gstar) \leq \frac{2}{5\gamma_1} \kappa(\Fstar)^{-1} \kappa(\Sigmax)^{-1}.
    \end{align*}
    Then, with probability at least $1 - \delta$, we have the following bound on the orthonormalization factor:
    \begin{align*}
        \smin(\bR) &\geq \sqrt{1 - 2\eta_\sbG(\gamma_1 + 2\gamma_2 + \eta_\sbG(\gamma_1 + \gamma_2)\gamma_2)}.
    \end{align*}
\end{proposition}
The constants $\gamma_1, \gamma_2$ will be instantiated to control the deflation of the contraction factor $1-\eta_\sbG \implies 1-c\eta_\sbG$ due to the orthonormalization factor.

\subsection*{Completing the bound}

We are almost ready to complete the proof. By instantiating the noise bound \Cref{prop:KFAC_noise_bound} and the orthonormalization factor bound \Cref{prop:linrep_orth_bound}, we have:
\begin{align*}
    \opnorm{\bG_+ \Gperp} &= \opnorm{\bR^{-1}\paren{(1 - \eta_\sbG) \bG \Gperp + \eta_\sbG (\bF^\top \bF)^{-1}\bfF^\top \Eps^\top \bX (\bX^\top \bX)^{-1}}} \\
    &\leq \frac{1-\eta_\sbG}{\smin(\bR)} \opnorm{\bG \Gperp} + \eta_\sbG \opnorm{(\bF^\top \bF)^{-1}\bfF^\top \Eps^\top \bX (\bX^\top \bX)^{-1}} \\
    &\leq \frac{1-\eta_\sbG}{\sqrt{1 - 2\eta_\sbG(\gamma_1 + 2\gamma_2 + \eta_\sbG(\gamma_1 + \gamma_2)\gamma_2)}}\opnorm{\bG \Gperp} + \eta_\sbG \sigmaeps  \sqrt{\frac{\dx + k + \log(1/\delta)}{\smin(\Fstar)^2\lmin(\Sigmax) n}}.
\end{align*}
To understand the effective deflation of the convergence rate, we prove the following numerical helper lemma.
\begin{lemma}\label{lem: avoiding square root rate}
    Given $c, d \in (0,1)$ and $\varepsilon \in (0,1/2)$, if $\varepsilon \geq c$, then the following holds:
    \begin{align*}
        \frac{1 - d}{\sqrt{1 - c d}} < 1 - (1 - \varepsilon) d.
    \end{align*}
    Additionally, as long as $\varepsilon \leq 1-\frac{1-\sqrt{1-d}}{d}$, then $1 - (1-\varepsilon) d \leq \sqrt{1 - d}$.
\end{lemma}

\textit{Proof of \Cref{lem: avoiding square root rate}:} squaring both sides of the desired inequality and re-arranging some terms, we arrive at
\begin{align*}
    c  &\leq \frac{1}{d}\paren{1 - \frac{(1- d)^2}{(1 - (1-\varepsilon)d)^2}} \\
    &= \frac{1}{d}\paren{1 - \frac{1- d}{\underbrace{1 - (1-\varepsilon)d}_{< 1}}} \underbrace{\paren{1 + \frac{1- d}{1 - (1-\varepsilon)d}}}_{> 1}.
\end{align*}
To certify the above inequality, it suffices to lower-bound the RHS. Since $c, d \in (0,1)$, the last factor is at least $1$, such that we have
\begin{align*}
    \frac{1}{d}\paren{1 - \frac{1- d}{1 - (1-\varepsilon)d}} \paren{1 + \frac{1- d}{1 - (1-\varepsilon)d}} &>  \frac{1}{d}\paren{1 - \frac{1- d}{1 - (1-\varepsilon)d}} \\
    &= \frac{1}{d}\frac{(1 - \varepsilon) d}{1 - (1-\varepsilon)d} \\
    &> \varepsilon .
\end{align*}
Therefore, $c \leq \varepsilon $ is sufficient for certifying the desired inequality. The latter claim follows by squaring and rearranging terms to yield the quadratic inequality:
\begin{align*}
    (1-\varepsilon)^2  d - 2(1-\varepsilon) + 1 \leq 0,
\end{align*}
Setting $\lambda := 1-\varepsilon$, the solution interval is $\lambda \in \paren{\frac{1-\sqrt{1- d}}{ d}, \frac{1 + \sqrt{1- d}}{ d}}$. The upper limit is redundant as it exceeds 1 and $\varepsilon \in (0,1)$, leaving the lower limit as the condition on $\varepsilon$ proposed in the lemma.

Plugging in $\eta_\sbG = d \in (0,1]$ and $2(\gamma_1 + 2\gamma_2 + \eta_\sbG(\gamma_1 + \gamma_2)\gamma_2) \leq (\gamma_1 + 2\gamma_2 + (\gamma_1 + \gamma_2)\gamma_2 = c$, we try candidate values $\gamma_1 =1/40$, $\gamma_2 = 1/100$ and set $\varepsilon=c$ to get:
\begin{align*}
    \frac{1-\eta_\sbG}{\sqrt{1 - 2\eta_\sbG(\gamma_1 + 2\gamma_2 + \eta_\sbG(\gamma_1 + \gamma_2)\gamma_2)}} &< (1 - 0.9\eta_\sbG).
\end{align*}
Plugging in our candidate values of $\gamma_1, \gamma_2$ into the burn-in conditions of \Cref{prop:linrep_orth_bound} finishes the proof of \Cref{thm:linrep_kfac_guarantee}.


$\qedhere$


\LinRepMainThm*





\subsection{Multi-Task and Transfer Learning}\label{sec:extension_multi_task}

We first discuss how the ideas in our ``single-task'' setting directly translate to multi-task learning. For example, taking our proposed algorithm template in \eqref{eq:linrep_update}, an immediate idea is, given the current task heads and shared representation $(\scurly{\Ft}, \bG)$, to form task-specific preconditioners formed locally on each task's batch data:
\begin{align*}
    \bP_\sbF^{(t)} &= \hatEx^{(t)}[\bfz \bfz^\top],\; \bP_\sbG^{(t)} = \Ft^\top \Ft, \; \bQ_\sbG^{(t)} = \hatEx^{(t)}[\bfx \bfx^\top],
\end{align*}
and perform a local update on $\Ft, \bG$ before a central agent averages the resulting updated $\bG$:
\begin{align*}
    \Ft[+] &= \Ft - \eta_\sbF \nabla_\sbF \hatL(\Ft, \Gt)\, {\bQ^{(t)}_{\sbF}}^{-1}\\
    \Gt[+] &= \bG - \eta_{\sbG} {\bP^{(t)}_\sbG}^{-1}\, \nabla_{\sbG} \hatL^{(t)}(\Ft[+], \bG)\, {\bQ^{(t)}_\sbG}^{-1}, \quad t \in [T] \\
    \bG_+ &= \frac{1}{T}\sumT \Gt[+].
\end{align*}
However, this presumes $\Ft$ are invertible, i.e.\ the task-specific dimension $\dy > k$. As opposed to the single-task setting, where as stated in \Cref{rem:multitask} we are really viewing $\bF$ as the concatentation of of $\Ft$ to make recovering the representation a well-posed problem, in multi-task settings $\dy$ may often be small, e.g.\ $\dy = 1$ \cite{tripuraneni2021provable, du2020few, collins2021exploiting, thekumparampil2021sample}. Therefore, (pseudo)-inverting away $(\Ft^\top \Ft)^{\dagger}$ may be highly suboptimal. However, we observe that writing out the representation gradient \eqref{eq:linrep_grad}, \emph{as long as we invert away ${\bQ^{(t)}_\sbG}^{-1}$ first}, then we have:
\begin{align*}
    \Gt[+] &= \bG - \eta_{\sbG} \nabla_{\sbG} \hatL^{(t)}(\Ft[+], \bG){\bQ^{(t)}_\sbG}^{-1} \\
    \bG_+ \Gperp &= \frac{1}{T}\sumT \Gt[+] \Gperp \\
    &= \paren{\bI_k - \eta_\sbG \frac{1}{T}\sumT \Ft^\top \Ft} \bG \Gperp + \text{ (task-averaged) noise term}.
\end{align*}
Since by assumption $\frac{1}{T}\sumT \Ft^\top \Ft$ is full-rank (otherwise recovering the rank $k$ representation is impossible), then suggestively, we may instead invert away the \textit{task-averaged} preconditioner $\bP_{\sbG} = \frac{1}{T}\sumT \Ft^\top \Ft$ on the task-averaged $\Gt$ descent direction before taking a representation step $\bG_+$. To summarize, we propose the following two-stage preconditioning:
\begin{align}\label{eq:multitask_KFAC}
    \Ft[+] &= \Ft - \eta_\sbF \nabla_\sbF \hatL(\Ft, \Gt)\, {\bQ^{(t)}_{\sbF}}^{-1} \\
    \Dt &= \nabla_{\sbG} \hatL^{(t)}(\Ft[+], \bG){\bQ^{(t)}_\sbG}^{-1}, \quad t \in [T] \\
    \bG_+ &= \bG - \eta_\sbG \bP_\sbG^{-1} \paren{\frac{1}{T}\sumT \Dt} \\
    \text{such that }\bG_+ \Gperp &= \paren{1 - \eta_\sbG} \bG\Gperp + \text{ (task-averaged) noise term}.
\end{align}
The exact same tools used in the proof of \Cref{thm:linrep_kfac_guarantee} apply here, with the requirement of a few additional standard tools to study the ``task-averaged'' noise term(s). As an example, we refer to \citet{zhang2023meta} for some candidates. However, we note the qualitative behavior is unchanged. As such, since we are using $n$ data points per each of $T$ tasks to update the gradient, the scaling of the noise term goes from $\calO(\sigmaeps \sqrt{\dx/n})$ in our bounds to $\calO(\sigmaeps \sqrt{\dx/nT})$. 

We remark that in the multi-task setting, where each task may have differing covariances and task-heads $\Ftstar$, the equivalence of our stylized \KFAC variant and the alternating min-min algorithm proposed in \citet{jain2013low, thekumparampil2021sample} breaks down. In particular, the alternating min-min algorithm no longer in general admits $\bG$ iterates that can be expressed as a product of matrices as in \eqref{eq:KFAC_update} or \eqref{eq:multitask_KFAC}, and rather can only be stated in vectorized space $\VEC(\bG)$. This means that whereas \eqref{eq:multitask_KFAC} can be solved as $T$ parallel small matrix multiplication problems, the alternating min-min algorithm nominally requires operating in the vectorized-space $\dx k$.


\subsection*{Transfer Learning}

We first prove the proposed fine-tuning generalization bound.

\LinRepTransfer*

\begin{proof}[Proof of \Cref{lem:linrep_transfer}]
    We observe that we may write:
    \begin{align*}
        \Ltest(\Flstest, \widehat \bG) &= \Ex\brac{\norm{\ytest - \Flstest\hatG\xtest}_2^2} \\
        &= \Ex\brac{\norm{(\Flstest \hatG - \Fstartest \Gstar)\xtest}_2^2} \\
        &= \bnorm{(\Flstest \hatG - \Fstartest \Gstar) \Sigmatest^{1/2}}_F^2.
    \end{align*}
    Now writing out the definition of $\Flstest$, defining $\ztest = \hatG \xtest$, we have
    \begin{align*}
        \Flstest &= \argmin_{\widehat \bF} \; \hatExtest[\norm{\ytest - \widehat \bF \hatG \xtest}_2^2] \\
        &= \Ytest^\top \Ztest (\Ztest^\top \Ztest)^{-1} \\
        &= \Fstartest \Gstar \Xtest^\top \Ztest(\Ztest^\top \Ztest)^{-1} + \Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1} \\
        &= \Fstartest \Gstar \hatG^\top + \Fstartest \Gstar \hatGperp \Xtest^\top \Ztest(\Ztest^\top \Ztest)^{-1} + \Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1},
    \end{align*}
    where $\hatGperp = \bI_{\dx} - \hatG^\top \hatG$ is the projection matrix onto the rowspace of $\hatG$, using the fact that $\hatG$ is row-orthonormal \eqref{eq:linrep_update}.
    Therefore, plugging in the last line into error expression, we have
    \begin{align*}
        \Ltest(\Flstest, \widehat \bG) &= \bnorm{(\Flstest \hatG - \Fstartest \Gstar) \Sigmatest^{1/2}}_F^2 \\
        &\leq 2\bnorm{\paren{\Fstartest \Gstar \hatG^\top \hatG + \Fstartest \Gstar \hatGperp \Xtest^\top \Ztest(\Ztest^\top \Ztest)^{-1}\hatG - \Fstartest \Gstar} \Sigmatest^{1/2}}_F^2 \\
        &\qquad \quad + 2\bnorm{\Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1} \hatG \Sigmatest^{1/2}}_F^2 \tag{$(a+b)^2 \leq 2a^2 + 2b^2$}.
    \end{align*}
    Focusing on the first term, we have:
    \begin{align*}
        &\Fstartest \Gstar \hatG^\top \hatG + \Fstartest \Gstar \hatGperp \Xtest^\top \Ztest(\Ztest^\top \Ztest)^{-1}\hatG - \Fstartest \Gstar  \\
        =\; &\Fstartest \Gstar \hatGperp  \paren{\Xtest^\top \Ztest(\Ztest^\top \Ztest)^{-1}\hatG - \bI_{\dx}}.
    \end{align*}
    By a covariance concentration argument \Cref{lem:gauss_cov_conc}, since $\Xtest^\top \Ztest$ and $\Ztest^\top \Ztest$ are rank-$k$ matrices, as long as $\ntest \gtrsim k + \log(1/\delta)$, we have with probability at least $1 - \delta$:
    \begin{align*}
        \Xtest^\top \Ztest \approx \ntest \Sigmatest \hatG^\top, \quad (\Ztest^\top \Ztest)^{-1} \approx \ntest \hatG \Sigmatest \hatG^\top,
    \end{align*}
    and thus
    \begin{align*}
        &\norm{\Fstartest \Gstar \hatGperp  \paren{\Xtest^\top \Ztest(\Ztest^\top \Ztest)^{-1}\hatG - \bI_{\dx}} \Sigmatest^{1/2}}_F \\
        \approx\;& \norm{\Fstartest \Gstar \hatGperp  \Sigmatest^{1/2} \paren{ \Sigmatest^{1/2} \hatG^\top (\hatG \Sigmatest \hatG^\top)^{-1}\hatG \Sigmatest^{1/2} - \bI_{\dx} }}_F\\
        \lesssim\;& \norm{\Fstartest}_F \opnorm{\Gstar \hatGperp} \opnorm{\Sigmatest^{1/2}} \opnorm{ \Sigmatest^{1/2} \hatG^\top (\hatG \Sigmatest \hatG^\top)^{-1}\hatG \Sigmatest^{1/2} - \bI_{\dx} }  \\
        \leq\;&\norm{\Fstartest}_F\; \dist(\hatG, \Gstar) \lmax(\Sigmatest)^{1/2},
    \end{align*}
    where in the last line we applied the definition $\dist(\hatG, \Gstar) = \opnorm{\Gstar \hatGperp} = \opnorm{\hatG \Gperp}$, and the fact that the matrix $\calP \triangleq \Sigmatest^{1/2} \hatG^\top (\hatG \Sigmatest \hatG^\top)^{-1}\hatG \Sigmatest^{1/2}$ can be verified to be a projection matrix $\calP^2 = \calP$, $\calP^\top = \calP$, such that $\calP - \bI = \calP^\perp$ is also an orthogonal projection and $\opnorm{\calP^\perp} = 1$. Now, we analyze the noise term:
    \begin{align*}
        \Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1} \hatG \Sigmatest^{1/2} \approx \Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1/2} (\ntest)^{-1/2} \paren{ \hatG \Sigmatest \hatG^\top}^{-1/2} \hatG \Sigmatest^{1/2},
    \end{align*}
    where we observed $\Ztest^\top \Ztest = n \bG \hatSigma \bG^\top$ and applied covariance concentration.
    Now, defining the (compact) SVD of $\hatG \Sigmatest^{1/2} = \bU_\bfz \bD_\bfz \bV_\bfz^\top$, we find
    \begin{align*}
        \norm{\Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1} \hatG \Sigmatest^{1/2}}_F &\lesssim \frac{1}{\sqrt{\ntest}} \norm{\Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1/2} \bU_\bfz \bV_\bfz^\top}_F \\
        &\lesssim \frac{1}{\sqrt{\ntest}} \norm{\Eps^\top \Ztest (\Ztest^\top \Ztest)^{-1/2} }_F \\
        &\lesssim \frac{1}{\sqrt{\ntest}} \sigmaeps \sqrt{\dy k + \log(1/\delta)},
    \end{align*}
    for $\ntest \gtrsim k + \log(1/\delta)$. The last line comes from the Frobenius norm variants of \Cref{lem:yasin_SNM} and \Cref{lem:SNM_bound} (see \citet[Theorem 4.1]{ziemann2023tutorial} or \citet[Lemma A.3]{zhang2023meta} for details). Putting the two bounds together yields the desired result.
\end{proof}

