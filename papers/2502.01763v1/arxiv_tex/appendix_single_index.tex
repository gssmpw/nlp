\section{Proofs and Additional Details for \Cref{sec:single_index}}

\subsection{Proof of Theorem~\ref{thm:rank1}}
\label{pfthm:rank1}
\OneStepSGD*
\begin{proof}
    To prove this theorem, we first note that
    \begin{align*}
    \nabla_\bfG \hatL\,(\bff_0,\bfG_0) = -\frac{1}{n} \sum_{i= 1}^{n}\left(y_i - \frac{1}{\sqrt{\dhid}}\bff_{0}^\top \sigma\left(\bfG_{0} \bfx_i\right)\right)\left(\frac{1}{\sqrt{\dhid}}\bff_{0} \odot \sigma'\left(\bfG_{0}\bfx_i\right)\right)\bfx_i^\top.
    \end{align*}
    Adopting the matrix notation
    \ $\bfX = [\bfx_1 | \dots | \bfx_n]^\top \in \R^{n \times \dx}$ and $\vy = [y_1, \dots, y_n]^\top \in \R^n$, we can write
    \begin{align}
        \label{eq:gradient}
        \nabla_\bfG \hatL\,(\bff_0,\bfG_0) = -\frac{1}{n}\left[\left(\dhid^{-1/2}\bff_0\vy^\top - \dhid^{-1}\bff_0\bff_0^\top \sigma(\bfG_{0} \bX^\top)\right)\odot \sigma'(\bfG_{0}\bX^\top)\right]\bX.
    \end{align}
    Let $ {z \sim \normal(0, \dx^{-1}\trace(\bSigma_\bfx))}$ and define $\alpha = \Ex_z \left[\sigma'(z)\right]$, and $\sigma_\perp: \R \to \R$ as $\sigma_\perp(z) =  \sigma(z) - \alpha z $. This function satisfies $\Ex_z \left[\sigma_\perp'(z)\right] = 0$. With this, we decompose the gradient into three components as $\nabla_\bfG \hatL\,(\bff_0,\bfG_0) = \bT_1 + \bT_2 + \bT_3$ with
    \begin{align*}
        &\bT_1 =  - \alpha\, \dhid^{-1/2} \; \bff_0 \left(\frac{\bX^\top\vy}{n}\right)^\top,\quad \bT_2 = -n^{-1} \dhid^{-1/2} \left[\bff_0 \,\vy^\top \odot \sigma_\perp'(\bfG_0 \bX^\top)\right]\bX,\\[0.2cm]
        &\bT_3 = n^{-1} \dhid^{-1} \,\Big[\left(\bff_0 \bff_0^\top \sigma(\bfG_0 \bX^\top)\right) \odot \sigma'(\bfG_0 \bX^\top)\Big]\bX.
    \end{align*}
    We will analyze each of these components separately.
    
    \begin{itemize}
        \item \textbf{Term 1:} For this term, using the facts that $\|\bff_0\|_2 = \calO(1)$ and $\|\bX^\top \vy/n\|_2 = \calO(1)$, we have
        \begin{align*}
            \left\|\bT_1\right\|_{\rm op} = \calO\left({ \dhid^{-1/2}} \right).
        \end{align*}
        \item \textbf{Term 2:} To analyze this term, note that
        \begin{align*}
            \bff_0\, \vy^\top \odot \sigma_\perp'(\bfG_0 \bX^\top) = \diag(\bff_0)\,  \sigma_\perp'(\bfG_0 \bX^\top)\,\diag(\vy),
        \end{align*}
        which gives
        \begin{align*}
            \left\|\bff_0\, \vy^\top \odot \sigma_\perp'(\bfG_0 \bX^\top)\right\|_{\rm op} &\leq \|\bff_0\|_{\rm \infty}\|\vy\|_{\rm \infty} \|\sigma_\perp'(\bfG_0 \bX^\top)\|_{\rm op}. %
        \end{align*}
        Using basic concentration arguments, we have $\|\bff_0\|_{\rm \infty} = \tilde \calO(\dhid^{-1/2})$, and $\|\vy\|_{\rm \infty} = \tilde \calO(1)$, with probability $1 - o(1)$. By construction of $\sigma_\perp(\cdot)$, the matrix $\sigma_\perp'(\bfG_0 \bX^\top)$ has mean zero entries, thus using \citep[Theorem 5.44]{vershynin2010introduction}, we have $\|\sigma_\perp'(\bfG_0 \bX^\top)\|_{\rm op} = \tilde{O}(\dhid^{1/2} + n^{1/2})$ with probability $1 - o(1)$ Thus, the norm of $\bT_2$ can be upper bounded as
        \begin{align*}
            \|\bT_2\|_{\rm op} = \tilde \calO\left(\frac{1}{ \dhid}\left(1 + \sqrt\frac{\dhid}{n}\right)\left(1 + \sqrt\frac{\dx}{n}\right)\right) = \tilde{\calO}\left(\dhid^{-1}\right).
        \end{align*}
        
        \item \textbf{Term 3:} Similar to the second term, note that
        \begin{align*}
            \left(\bff_0\, \bff_0^\top \sigma(\bfG_0 \bX^\top)\right) \odot \sigma'(\bfG_0 \bX^\top) = \diag(\bff_0)\,  \sigma'(\bfG_0 \bX^\top) \, \diag\left(\bff_0^\top \sigma(\bfG_0 \bX^\top)\right).
        \end{align*}
        Thus, the norm of the third term can be upper bounded as
            \begin{align*}
                \|\bT_3\|_{\rm op}&=\left\|\frac{1}{n \, \dhid}  \Big[\left(\bff_0 \bff_0^\top \sigma(\bfG_0 \bX^\top)\right) \odot \sigma'(\bfG_0 \bX^\top)\Big]\bX\right\|_{\rm op}\\ &\leq n^{-1} \dhid^{-1}{\|\bX\|_{\rm op}\|\bff_0\|_{\rm \infty}\|\bff_0^\top \sigma(\bfG_0 \bX^\top)\|_{\rm \infty} \left\|\sigma'(\bfG_0 \bX^\top)\right\|_{\rm op}}.
            \end{align*}
            To analyze the right hand side, note that assuming that $\sigma$ is $\calO(1)$-Lipschitz, the entries of $\sigma'(\bfG_0 \bX^\top)$ are bounded by the Lipschitz constant, and we have $\|\sigma'(\bfG_0 \bX^\top)\|_{\rm op} = \calO(\sqrt{n \,\dhid})$. Also, using a simple orderwise analysis we have $\|\bff_0^\top \sigma(\bfG_0 \bX^\top)\|_{\rm \infty} = \tilde \calO(1)$, which gives
            \begin{align*}
                \|\bT_3\|_{\rm op} = \tilde \calO\left(\frac{1}{ \dhid}\left(1 + \sqrt\frac{\dx}{n}\right)\right) = \tilde{\calO}(\dhid^{-1}).
            \end{align*}
    \end{itemize}
    To wrap up, note that $\dhid^{1/2} \|\bT_1\|_{\rm op} = \calO(1)$, whereas $\dhid^{1/2} \|\bT_2\|_{\rm op}$ and $\dhid^{1/2} \|\bT_3\|_{\rm op} = o(1)$. Thus, with probability $1 - o(1)$ we have
    \begin{align*}
        \bG_{\texttt{SGD}} = \bG_0 + \eta\,\dhid^{1/2}\, \nabla_\bfG \hatL\,(\bff_0,\bfG_0) = \bG_0 + \alpha \eta\, \bff_0 \left(n^{-1} \bfX^\top \vy\right)^\top + \mathbf{\Delta}
    \end{align*}
    with $\|\mathbf{\Delta}\|_{\rm op} = o(1)$, finishing the proof.
\end{proof}
\subsection{Proof of Lemma~\ref{lemma:beta_tilde_alignment}}
\label{pflemma:beta_tilde_alignment}
\TildeAlignment*
\begin{proof}
    Recall that $\betaSGD = \frac{1}{n} \bX^\top \vy$ and $\vy = \sigma_\star(\bX\vbeta_\star) + \boldsymbol{\ep}$ where $\boldsymbol{\ep} = [\ep_1, \dots, \ep_n]^\top$. Therefore, with probability $1 - o(1)$ we have
    \begin{align*}
        \betaSGD^\top\vbeta_\star = \frac{1}{n} (\bX\vbeta_\star)^\top \vy = \frac{1}{n} (\bX\vbeta_\star)^\top \sigma_\star(\bX\vbeta_\star) + o(1)
    \end{align*}
    where we have used the fact that $\boldsymbol{\ep}$ is mean zero. Thus, using the weak law of large numbers, 
    \begin{align}
    \label{eq:inner}
        \betaSGD^\top\vbeta_\star \to \Ex_{z} \left[z\sigma_\star(z)\right] = \dx^{-1}\trace(\bSigma_\bfx) \,\Ex_{z} \left[\sigma_\star'(z)\right] = c_{\star,1} \dx^{-1}\trace(\bSigma_\bfx)
    \end{align}
    in probability, where $z \sim \normal(0, \dx^{-1}\trace(\bSigma_\bfx))$. Similarly, $\|\betaSGD\|_2^2$ can be written as
    \begin{align*}
        \|\betaSGD\|_2^2 &= n^{-2} \vy^\top \bX \bX^\top \vy = n^{-2} \boldsymbol{\ep}^\top\bX \bX^\top\boldsymbol{\ep}+ n^{-2} \sigma_\star(\bX\vbeta_\star)^\top\bX \bX^\top\sigma_\star(\bX\vbeta_\star) + o(1).
    \end{align*}
    We will analyze each of the two remaining term separately.  For the first term, recall that $\boldsymbol{\ep}$ is independent of $\bX$. Using the Hanson-Wright inequality (Theorem~\ref{thm:hanson-wright}) we have
    \begin{align*}
        n^{-2} \boldsymbol{\ep}^\top\bX \bX^\top\boldsymbol{\ep} =   \sigma_\ep^2 n^{-1} \trace(\bX\bX^\top/n) + o(1) = \sigma_\ep^2 n^{-1}  \trace(\bSigma_\bfx) + o(1).
    \end{align*}
    For the second term, note that $\bX\vbeta_\star$ is a vector with i.i.d. elements $\bfx_i^\top \vbeta_\star$, each of them distributed according to $\normal(0, \vbeta_\star^\top \bSigma_\bfx\vbeta_\star)$. Let $z$ be a random variable distributed as $z \sim \normal(0, \vbeta_\star^\top \bSigma_\bfx\vbeta_\star) $. We decompose the function $\sigma_\star$ into a linear and a nonlinear part as
    \begin{align}
        \label{eq:expansions}
        \sigma_\star(z) = c_{\star,1} z + \sigma_{\star, \perp}(z).
    \end{align}
    This decomposition satisfies
    \begin{align*}
        &\Ex_z [\sigma_{\star, \perp}(z)] = \Ex_z  [\sigma_\star(z)] = 0\\
        &\Ex_z\,[ z\, \sigma_{\star, \perp} (z) ] = \Ex_z\, z\,\sigma_\star(z) - c_{\star,1}\,\Ex_z \, [z^2] =  \Ex_z\, [z\,\sigma_\star(z)] - c_{\star,1}\, \vbeta_\star^\top \bSigma_\bfx \vbeta_\star = 0,
    \end{align*}
    where the last equality is due to Stein's lemma (Lemma~\ref{lemma:stein's}). This shows that the random variables $z$ and $\sigma_{\star, \perp}(z)$ are uncorrelated. With this, we have
    \begin{align}
        \label{eq:sum1}
        n^{-2} \sigma_\star(&\bX\vbeta_\star)^\top\bX \bX^\top\sigma_\star(\bX\vbeta_\star) = n^{-2} (c_{\star,1} \bX\vbeta_\star + \sigma_{\star,\perp}(\bX\vbeta_\star))^\top\bX \bX^\top(c_{\star,1} \bX\vbeta_\star + \sigma_{\star,\perp}(\bX\vbeta_\star))\nonumber\\[0.2cm]
        &=  c_{\star,1}^2 n^{-2} \vbeta_\star^\top (\bX^\top \bX)^2 \vbeta_\star + 2  c_{\star,1} n^{-2} (\bX\vbeta_\star)^\top \bX\bX^\top \sigma_{\star,\perp}(\bX\vbeta_\star) +  n^{-2} \sigma_{\star,\perp}(\bX\vbeta_\star)^\top \bX\bX^\top \sigma_{\star,\perp}(\bX\vbeta_\star).
    \end{align}
    For the first term in this sum, by assumption~\ref{assumption:random-effect} and the Hanson-Wright inequality (Theorem~\ref{thm:hanson-wright}), we can write 
    \begin{align*}
        c_{\star,1}^2  n^{-2} \vbeta_\star^\top (\bX^\top \bX)^2 \vbeta_\star = c_{\star,1}^2\,\dx^{-1} \trace(n^{-2}(\bX^\top\bX)^2) + o(1) = c_{\star,1}^2 \dx^{-1} \trace(\bSigma_\bfx^2) + c_{\star,1}^2n^{-1} \dx^{-1} \trace^2(\bSigma_\bfx),
    \end{align*}
    where in the last we plugged in the second Wishart moment. For the second term in  \eqref{eq:sum1}, although by construction $\bX\vbeta_\star$ and $\sigma_{\star, \perp}(\bX\vbeta_\star)$ are uncorrelated, the vector $\bX\vbeta_\star$ and the matrix $\bX\bX^\top$ are dependent, which complicates the analysis. To resolve this issue, we define $\tilde \bX = \bX - \bX \vbeta_\star\vbeta_\star^\top$ which satisfies $\bX\vbeta_\star \independent \tilde\bX$ and write
    \begin{align*}
        \bX\bX^\top =  \tilde\bX\tilde\bX^\top + \tilde\bX\vbeta_\star\vbeta_\star^\top \bX^\top + \bX\vbeta_\star\vbeta_\star^\top\tilde\bX^\top + \bX\vbeta_\star\vbeta_\star^\top\bX^\top.
    \end{align*}
    Thus, the second term in \eqref{eq:sum1} can be written as
    \begin{align*}
        n^{-2} (\bX\vbeta_\star)^\top &\bX\bX^\top \sigma_{\star,\perp}(\bX\vbeta_\star) = n^{-2} (\bX\vbeta_\star)^\top \left[\tilde\bX\tilde\bX^\top + \tilde\bX\vbeta_\star\vbeta_\star^\top \bX^\top + \bX\vbeta_\star\vbeta_\star^\top\tilde\bX^\top + \bX\vbeta_\star\vbeta_\star^\top\bX^\top\right] \sigma_{\star,\perp}(\bX\vbeta_\star)\\
        &=\underbrace{n^{-2} (\bX\vbeta_\star)^\top \tilde\bX\tilde\bX^\top \sigma_{\star,\perp}(\bX\vbeta_\star)}_{T_1}\\ &\hspace{2cm}+  \underbrace{n^{-2} (\bX\vbeta_\star)^\top \left[ \tilde\bX\vbeta_\star\vbeta_\star^\top \bX^\top + \bX\vbeta_\star\vbeta_\star^\top\tilde\bX^\top + \bX\vbeta_\star\vbeta_\star^\top\bX^\top\right] \sigma_{\star,\perp}(\bX\vbeta_\star)}_{T_2}.
    \end{align*}
    The term $T_1$ can be shown to be $o(1)$ by using the Hanson-Wright inequality (Theorem~\ref{thm:hanson-wright}) and noting that $\tilde\bX\tilde\bX^\top$ is independent of $\bX\vbeta_\star$, and also the fact that  $\bX\vbeta_\star$ and $\sigma_{\star,\perp}(\bX\vbeta_\star)$ are orthogonal, by construction. Similarly, $T_2$ can also be shown to be $o(1)$ using a similar argument. For this, we also use that fact that by construction we have $\tilde\bX\vbeta_\star \independent \bX\vbeta_\star$ which alongside $\Ex \; [\tilde\bX\vbeta_\star] = \mathbf{0}_n$ proves that $n^{-1} (\bX\vbeta_\star)^\top \tilde\bX\vbeta_\star = o(1)$ and $n^{-1} (\tilde\bX\vbeta_\star)^\top \sigma_{\star,\perp}(\tilde\bX\vbeta_\star) = o(1)$. Hence, 
    \begin{align*}
        2  c_{\star,1}n^{-2} (\bX\vbeta_\star)^\top \bX\bX^\top \sigma_{\star,\perp}(\bX\vbeta_\star) \to 0.
    \end{align*}
    For the third term in \eqref{eq:sum1}, we can use a similar argument and replace $\bX$ with $\tilde\bX$ to show that
    \begin{align*}
        n^{-2} \sigma_{\star,\perp}(\bX\vbeta_\star)^\top \bX\bX^\top \sigma_{\star,\perp}(\bX\vbeta_\star) \to \Ex_z [\sigma_{\star, \perp}(z)]^2 \, n^{-1} \trace(\bSigma_\bfx).
    \end{align*}
    Putting everything together, we have
    \begin{align*}
    \|\betaSGD\|_2^2 &= c_{\star,1}^2 \dx^{-1} \trace(\bSigma_\bfx^2) +  \sigma_\ep^2 \,n^{-1}\, \trace(\bSigma_\bfx) + \Ex_z [\sigma_{\star, \perp}(z)]^2 \, n^{-1} \trace(\bSigma_\bfx) + c_{\star,1}^2n^{-1} \dx^{-1} \trace^2(\bSigma_\bfx)+ o(1)\\[0.2cm]
    &= c_{\star,1}^2 \dx^{-1} \trace(\bSigma_\bfx^2) +  n^{-1} \trace(\bSigma_\bfx) \left(\sigma_\ep^2 +  \Ex_z [\sigma_{\star, \perp}(z)]^2+c_{\star,1}^2 \dx^{-1}\trace(\bSigma_{\bfx})\right) + o(1).
    \end{align*}
    Note that given the decomposition \eqref{eq:expansions}, we have
    \begin{align*}
        \Ex_z \left[\sigma_\star^2(z)\right] = \Ex_z \left[\sigma_{\star,\perp}^2(z)\right] + c_{\star,1}^2 \dx^{-1} \trace(\bSigma_{\bfx})
    \end{align*}
    given the orthogonality of the linear and nonlinear terms. Hence,
    \begin{align*}
        \|\betaSGD\|_2^2 &= c_{\star,1}^2 \dx^{-1} \trace(\bSigma_\bfx^2) +  n^{-1} \trace(\bSigma_\bfx) \left(\sigma_\ep^2 +  c_{\star}^2\right) + o(1),
    \end{align*}
    which alongside \eqref{eq:inner} proves the lemma.
\end{proof}



\subsection{Proof of Lemma \ref{lemma:beta_hat_alignment}}
\label{pf_lemma:beta_hat_alignment}
\HatAlignment*
\begin{proof}
    
    Recall that $\bQ_\sbG = n^{-1} \bX^\top \bX$ and let $\bR = (\bQ_\sbG + \lambda_\sbG \bI_{\dx})^{-1}$. The inner product of $\vbeta_\star$ and $\betaKFAC$ is given by
    \begin{align*}
        \vbeta_\star^\top\betaKFAC = n^{-1} \vbeta_\star^\top\bR\bX^\top \sigma_\star(\bX\vbeta_\star) + o(1),
    \end{align*}
    where we have used the fact that $\boldsymbol{\ep}$ is mean zero and is independent of all other randomness in the problem. Defining $\bar\bR =\left( \bX \bX^\top/n + \lambda_\sbG \bI_n\right)^{-1}$, we can use the push-through identity to rewrite the inner product as
    \begin{align*}
        \vbeta_\star^\top\betaKFAC = n^{-1}(\bX\vbeta_\star)^\top\bar\bR\,\sigma_\star(\bX\vbeta_\star) + o(1).
    \end{align*}
    Note that $\bX\vbeta_\star$ is a vector with i.i.d. elements $\bfx_i^\top \vbeta_\star$, each of them distributed according to $\normal(0, \vbeta_\star^\top \bSigma_\bfx\vbeta_\star)$.  Using the same decomposition for $\sigma_\star$ as the one used in the proof of Lemma~\ref{lemma:beta_tilde_alignment} in \eqref{eq:expansions}, we have
    \begin{align}
    \label{eq:inner_hat}
    \vbeta_\star^\top\betaKFAC &= \frac{1}{n} (\bX\vbeta_\star)^\top \bar\bR\left(c_{\star,1} \bX \vbeta_\star + \sigma_{\star, \perp}(\bX\vbeta_\star)\right) + o(1)\nonumber\\[0.2cm]
    &= c_{\star,1} \dx^{-1} \trace\left(\bX^\top (\bX\bX^\top + \lambda_\sbG n \bI_n)^{-1}\bX\right) + n^{-1} (\bX\vbeta_\star)^\top \bar\bR\, \sigma_{\star, \perp}(\bX\vbeta_\star)+ o(1),
    \end{align}
    where for the first term we have used Assumption~\ref{assumption:random-effect} and the Hanson-Wright inequality (Theorem~\ref{thm:hanson-wright}). To analyze the second term, note that although by construction $\bX\vbeta_\star$ and $\sigma_{\star, \perp}(\bX\vbeta_\star)$ are uncorrelated, the vectors $\bX\vbeta_\star$ and $\bar\bR$ are dependent, which complicates the analysis. To resolve this issue, we use the same trick used in the proof of Lemma~\ref{lemma:beta_tilde_alignment} and set $\tilde \bX = \bX - \bX \vbeta_\star\vbeta_\star^\top$ which satisfies $\bX\vbeta_\star \independent \tilde\bX$, and use it to write
    \begin{align*}
        \bar\bR^{-1} &= n^{-1}  \bX \bX^\top + \lambda_\sbG \bI_n = n^{-1} \left(\tilde\bX + \bX \vbeta_\star \vbeta_\star^\top\right)\left(\tilde\bX + \bX \vbeta_\star \vbeta_\star^\top\right)^\top + \lambda_\sbG \bI_n\\[0.2cm]
        &= \left(n^{-1} \tilde\bX \tilde\bX^\top + \lambda_\sbG \bI_n\right) + n^{-1} (\bX\vbeta_\star)(\bX\vbeta_\star)^\top + n^{-1}(\tilde \bX\vbeta_\star)(\bX\vbeta_\star)^\top + n^{-1}( \bX\vbeta_\star)(\tilde \bX\vbeta_\star)^\top.
    \end{align*}
    Defining $\tilde\bR =\left( \tilde\bX \tilde\bX^\top/n + \lambda_\sbG \bI_n\right)^{-1} \in \R^{n \times n}$,  $\bV = [\,n^{-1/2}\bX\vbeta_\star\, |\, n^{-1/2} \tilde\bX \vbeta_\star] \in \R^{n \times 2}$, and 
    \begin{align*}
        \bD = \begin{bmatrix}
            1 & 1 \\[0.2cm]
            1 & 0
        \end{bmatrix},
    \end{align*}
    we have $\bar\bR = \tilde\bR + \bV \bD \bV^\top$. Using Woodbury matrix identity (Theorem~\ref{thm:woodbury}), $\bar\bR$ is given by
    \begin{align}
        \label{eq:woodbury}
        \bar\bR = \tilde\bR - \tilde\bR\,  \bV (\bD^{-1} + \bV^\top \tilde\bR\bV)^{-1}  \bV^\top\,\tilde\bR
    \end{align}
    and plugging this expression into the second term in \eqref{eq:inner_hat} gives
    \begin{align*}
         n^{-1} (\bX\vbeta_\star)^\top &\bar\bR\, \sigma_{\star, \perp}(\bX\vbeta_\star) = n^{-1} (\bX\vbeta_\star)^\top\left(\tilde\bR - \tilde\bR\,  \bV (\bD^{-1} + \bV^\top \tilde\bR\bV)^{-1}  \bV^\top\,\tilde\bR\right)\,\sigma_{\star, \perp}(\bX\vbeta_{\star}) + o(1)\\[0.2cm]
        &= n^{-1}(\bX\vbeta_\star)^\top \tilde\bR\; \sigma_{\star, \perp}(\bX\vbeta_{\star})  - n^{-1} (\bX\vbeta_\star)^\top\tilde\bR\left(\bV (\bD^{-1} + \bV^\top \tilde\bR\bV)^{-1}  \bV^\top\right)\,\tilde\bR\,\sigma_{\star, \perp}(\bX\vbeta_{\star}) + o(1).
    \end{align*}
    The first term can be shown to be $o(1)$ in probability by using  the fact that $\tilde\bR$ is independent of $\bX\vbeta_\star$ and the orthogonality of $\bX\vbeta_\star$ and $\sigma_{\star, \perp}(\bX\vbeta_\star)$. To analyze the second term, first note that the elements of the matrix $\boldsymbol{\Omega} = (\bD^{-1} + \bV^\top \tilde\bR\bV)^{-1} \in \R^{2 \times 2}$ can all be shown to be $\calO(1)$ by a simple norm argument. Moreover,
    \begin{align*}
       n^{-1} &(\bX\vbeta_\star)^\top\tilde\bR \bV (\bD^{-1} + \bV^\top \tilde\bR\bV)^{-1}  \bV^\top\,\tilde\bR\,\sigma_{\star, \perp}(\bX\vbeta_{\star}) \\[0.2cm]
       &= n^{-2} (\bX\vbeta_\star)^\top\tilde\bR \Big(\Omega_{11}(\bX\vbeta_\star)(\bX\vbeta_\star)^\top + \Omega_{12}(\bX\vbeta_\star)(\tilde\bX\vbeta_\star)^\top\\ &\hspace{5cm}+ \Omega_{21}(\tilde\bX\vbeta_\star)(\bX\vbeta_\star)^\top + \Omega_{22}(\tilde\bX\vbeta_\star)(\tilde\bX\vbeta_\star)^\top\Big)\tilde\bR\,\sigma_{\star, \perp}(\bX\vbeta_{\star})
    \end{align*}
    where $\Omega_{ij}$ are the elements of the matrix $\mathbf{\Omega}$. We  analyze each term in this sum separately and show that all of them are $o(1)$.
    \begin{itemize}
        \item \textbf{First Term.} Using a simple norm argument, $n^{-1} (\bX\vbeta_\star) \tilde\bR(\bX\vbeta_\star) = \calO(1)$. Also, by construction of $\sigma_{\star, \perp}$, we have
        \begin{align*}
            n^{-1}(\bX\vbeta_\star)^\top \tilde\bR\sigma_{\star, \perp}(\bX\vbeta_\star) \to 0.
        \end{align*}
        Thus, the whole term is $o(1)$.
        \item\textbf{Second Term.} Similar to the first term, we have $n^{-1} (\bX\vbeta_\star) \tilde\bR(\bX\vbeta_\star) = \calO(1)$. Also, $n^{-1}(\tilde\bX\vbeta_\star)^\top \tilde\bR\,\sigma_{\star, \perp}(\bX\vbeta_\star) \to 0$ in probability, using the weak law of large numbers by noting that $\sigma_{\star, \perp}(\bX\vbeta_\star)$ is independent of $n^{-1}(\tilde\bX\vbeta_\star)^\top \tilde\bR$ by construction and that it has mean zero. Hence, the whole term is $o(1)$.
        \item\textbf{Third Term.} By construction, the vector $\bX\vbeta_\star$ is independent of $\tilde\bR (\tilde\bX\vbeta_\star)$ and has mean zero, which gives $n^{-1}(\bX\vbeta_\star)^\top\tilde\bR(\tilde\bX\vbeta_\star) \to 0$. Also, using a simple norm argument, we have $n^{-1}(\bX\vbeta_\star)^\top \tilde\bR\sigma_{\star, \perp}(\bX\vbeta_\star) = \calO(1)$ which proves that the third term is also $o(1)$.
        \item \textbf{Fourth Term.} This term can be shown to be $o(1)$ with an argument very similar to the argument for the third term.
    \end{itemize}
    
    Putting these all together and using \eqref{eq:inner_hat}, we have
    \begin{align}
        \label{eq:inner_hat_final}
        \vbeta_\star^\top \betaKFAC = c_{\star,1} \dx^{-1} \trace\left((\bX^\top\bX/n)\,\bR\right) + o(1).
    \end{align}
    
    Next we move to the analysis of the squared $\ell_2$-norm of the vector $\betaKFAC$. By decomposing the function $\sigma_\star$ into a linear and an orthogonal nonlinear component similar to the one used for the analysis of the inner product term above, we write
    \begin{align*}
        \|\betaKFAC\|_2^2 &= n^{-2}\, \vy^\top \bX \bR^2 \bX^\top \vy
           = n^{-2}\boldsymbol{\ep}^\top \bX \bR^2 \bX^\top\boldsymbol{\ep}+ c_{\star,1}^2 n^{-2} \vbeta_\star^\top \bX^\top  \bX \bR^2 \bX^\top\bX\vbeta_\star\\[0.2cm] &\hspace{2cm} +n^{-2} \sigma_{\star, \perp}(\bX\vbeta_\star)^\top \bX \bR^2 \bX^\top\sigma_{\star, \perp}(\bX\vbeta_\star) + 2\, c_{\star, 1} \, n^{-2} \vbeta_\star^\top \bX^\top  \bX \bR^2 \bX^\top\sigma_{\star,\perp}(\bX\vbeta_\star).
    \end{align*}
    We will analyze each of these terms separately.
    \begin{itemize}
        \item \textbf{First Term.} Recalling that $\boldsymbol{\ep} \sim \normal(0, \sigma_\ep^2 \bI_n)$ independent of all randomness in the problem, using the Hanson-Wright inequality (Theorem~\ref{thm:hanson-wright}) we have
        \begin{align*}
            n^{-2}\boldsymbol{\ep}^\top \bX \bR^2 \bX^\top\boldsymbol{\ep} = \sigma_\ep^2 n^{-1} \trace( (\bX^\top\bX/n) \bR^2 ) + o(1).
        \end{align*}
    
        \item \textbf{Second Term.} Using Assumption~\ref{assumption:random-effect}, and by the Hanson-Wright inequality we have
        \begin{align*}
             n^{-2}c_{\star,1}^2\, \vbeta_\star^\top \bX^\top \bX\bR^2\bX^\top\bX\vbeta_\star =  c_{\star, 1}^2  \dx^{-1}   \trace \left[(\bX^\top\bX/n)^2\bR^2\right] + o(1)
        \end{align*}
        
        \item \textbf{Third Term.} Note that $\bar\bR$ and $ \bX\vbeta_\star$ are dependent. Note that $\bX\bR^2\bX^\top = \bX\bX^\top \bar\bR = n\bar\bR - \lambda_\sbG n \bar\bR^2$. Thus, an almost identical argument to the argument used above for the analysis of $\vbeta_\star^\top \betaKFAC$ using $\tilde\bX = \bX - \bX\vbeta_\star\vbeta_\star^\top$ gives
        \begin{align*}
        n^{-2} \sigma_{\star, \perp}(\bX\vbeta_\star)^\top \bX \bR^2 \bX^\top\sigma_{\star, \perp}(\bX\vbeta_\star)  
        &= \Ex_z[\sigma^2_{\star,\perp}(z)] \cdot n^{-1} \trace\left((\bX^\top\bX/n)\,\bR^2\right) + o(1)
        \end{align*}
        
        \item \textbf{Fourth Term.} This term can readily be shown to be $o(1)$ in the analysis of $\vbeta_\star^\top \betaKFAC$; i.e.,
        \begin{align*}
        2\, c_{\star, 1} \, n^{-2} \vbeta_\star^\top \bX^\top  \bX \bR^2 \bX^\top\sigma_{\star,\perp}(\bX\vbeta_\star) = o(1).
        \end{align*}
    \end{itemize}
    Putting everything together, we find
    \begin{align}
        \label{eq:norm_beta_hat}
        \|\betaKFAC\|_2^2 = c_{\star, 1}^2  \dx^{-1}   \trace \left[(\bX^\top\bX/n)^2\,\bR^2\right] + (\sigma_\ep^2 + \Ex_z[\sigma^2_{\star,\perp}(z)]) \; n^{-1} \trace\left((\bX^\top\bX/n)\,\bR^2\right) 
    \end{align}
    Now, given \eqref{eq:inner_hat_final} and \eqref{eq:norm_beta_hat}, defining $c_{\star,>1} = \Ex_z[\sigma^2_{\star,\perp}(z)]$, we have
    \begin{align}
        \label{eq:hat_corr_prelim}
        \frac{\vbeta_\star^\top\betaKFAC}{\|\betaKFAC\| \|\vbeta_\star\|}= \frac{c_{\star,1} \dx^{-1} \trace\left((\bX^\top\bX/n)\,\bR\right) }{\sqrt{c_{\star, 1}^2  \dx^{-1}   \trace \left[(\bX^\top\bX/n)^2\,\bR^2\right] + (\sigma_\ep^2 + c^2_{\star,>1}) \; n^{-1} \trace\left((\bX^\top\bX/n)\,\bR^2\right) }}.
    \end{align}
    Thus, noting that if $\dx/n\to 0$ and $\lambda_\sbG \to 0$, we have $\bR \to \bSigma_\bfx^{-1}$, we find
    \begin{align*}
        \lim_{\lambda\to 0} \lim_{\dx/n \to \infty}\frac{\vbeta_\star^\top\betaKFAC}{\|\betaKFAC\| \|\vbeta_\star\|} = 1,
    \end{align*}
    proving the second part of the lemma. For the first part, we define $m(z):\R \to \R$ as the limiting Stieltjes transform of the the empirical eigenvalue distribution of $n^{-1} \bX^\top\bX$; i.e.,
    \begin{align}
    \label{eq:stieltjes}
        m(z) =  \lim_{\dx, n\to \infty} \dx^{-1} \trace\left[\left(\bX^\top\bX/n - z \,\bI_{\dx}\right)^{-1}\right]
    \end{align}
    where the limit is taken under the assumption that $\dx/n \to \phi>0$. For a general covariance matrix $\bSigma_{\bfX}$, $m(z)$ does not have a closed form except for very special cases; however, it it can be
 efficiently computed. See Section~\ref{sec:stieltjes} for more details. The derivative of the function $m$ is given by
    \begin{align*}
        m'(z) = - \lim_{\dx, n \to \infty} \dx^{-1} \trace\left[\left(\bX^\top\bX/n - z \,\bI_{\dx}\right)^{-2}\right].
    \end{align*}
    We can write all the traces appearing in \eqref{eq:hat_corr_prelim} in terms of the function $m$ and its derivative:
    \begin{align*}
        &\dx^{-1} \trace\left((\bX^\top\bX/n)\,\bR\right)\\ &\hspace{1cm}= \dx^{-1} \trace\left((\bX^\top\bX/n + \lambda_{\bG}\bI_{\dx}-\lambda_{\bG}\bI_{\dx})\,\bR\right) = \dx^{-1} \trace\left(\bI_{\dx} -\lambda_{\bG}\bR\right) = 1 - \lambda_\sbG \, m(-\lambda_\sbG),\\[0.5cm]
        &\dx^{-1} \trace\left((\bX^\top\bX/n)^2\,\bR^2\right)\\ &\hspace{1cm} =  \dx^{-1} \trace\left((\bX^\top\bX/n + \lambda_{\bG}\bI_{\dx}-\lambda_{\bG}\bI_{\dx})^2\,\bR^2\right)= 1 - \lambda_\sbG^2\, m'(-\lambda_\sbG) - 2\lambda_\sbG\, m(-\lambda_\sbG), \\[0.5cm]
        &n^{-1} \trace\left((\bX^\top\bX/n)\,\bR^2\right)\\ &\hspace{1cm} = n^{-1} \trace\left(\bR  - \lambda_\sbG\bR^2\right) = \phi\, m(-\lambda_\sbG) + \phi\,\lambda_\sbG\, m'(-\lambda_\sbG).
    \end{align*}
    With these, the correlation is given by
    \begin{align*}
        \frac{\betaKFAC^\top\vbeta_\star}{\|\betaKFAC\|_2  \|\vbeta_\star\|_2} = \frac{c_{\star,1} [\,1 - \lambda_\sbG\,m(-\lambda_\sbG)\,]}{\sqrt{c_{\star,1}^2[\,1 - \lambda_\sbG^2\, m'(-\lambda_\sbG) - 2\lambda_\sbG\, m(-\lambda_\sbG)\,] + \phi(c_{\star,>1}^2 + \sigma_\ep^2)[\,m(-\lambda_\sbG) + \lambda_\sbG m'(-\lambda_\sbG)\,]}},
    \end{align*}
    which defining
    \begin{align}
        \label{eq:psi_def}
        &\Psi_1 = 1 - \lambda_\sbG\,m(-\lambda_\sbG)\nonumber\\[0.1cm]
        &\Psi_2 = 1 - \lambda_\sbG^2\, m'(-\lambda_\sbG) - 2\lambda_\sbG\, m(-\lambda_\sbG)\nonumber\\[0.1cm]
        &\Psi_3 = m(-\lambda_\sbG) + \lambda_\sbG m'(-\lambda_\sbG)
    \end{align}
    concludes the proof.
\end{proof}
\subsection{From Feature Learning to Generalization}
\label{sec:single_generalize}
In Section~\ref{sec:single_index}, we showed that after one step of \SGD and \KFAC, the first layer weights will become approximately equal to 
\begin{align}
    \label{eq:one_update}
    \widehat\bfG_{a} \approx \widehat\bfG_0 + \alpha \eta\, \bff_0 \hat\vbeta_{a}^\top, \quad a \in \{\SGD, \KFAC\}.
\end{align}
Given \Cref{lemma:beta_tilde_alignment} and \Cref{lemma:beta_hat_alignment}, we argued that compared to \SGD, the weights obtained by the \KFAC algorithm are more aligned to the true direction $\vbeta_\star$. Given a nontrivial alignment between the weights and the target direction, the second layer $\bff$ can be trained using least squares (or based on \Cref{lem:KFAC_is_EMA}, equivalently using one step of the \KFAC update on $\bff$ with $\eta_{\bff} = 1$) with $\Theta(d)$ samples to achieve good generalization performance (See e.g., \citet[Theorem 11]{ba2022high} and \citet[Section 3.4]{dandibenefits}). The existence of nontrivial alignment of the learned weights and the true direction in a single index model is often called \textit{weak recovery} and has been subject to extensive investigation (see e.g., \citet{arous2021online,dandibenefits,troiani2024fundamental,arnaboldi2024repetita}, etc.). 

To see this, consider the feature matrix $\bZ_a \in \R^{n \times \dhid}$ as $\bZ_{a} = \sigma(\bX\widehat\bG_a^\top)$, where the activation function is applied element-wise. Based on equation~\eqref{eq:one_update}, this matrix can be written as
\begin{align*}
    \bZ_{a} \approx \sigma\left(\bX \bG_0^\top + \alpha \eta \, (\bX\hat\vbeta_a) \bff_0^\top\right).
\end{align*}
This is an example of a random matrix in which a nonlinear function is applied element-wise to a random component plus a rank-one signal component which has been studied in the literature \citep{guionnet2023spectral,moniri_atheory2023,moniri2024signal}. In particular, by Taylor expanding the activation function, the feature matrix $\bZ_a$ can be written as
\begin{align*}
    \bZ_{a} &\approx \sigma(\bX\bfG_{0}^\top) +  \sum_{k = 1}^{\ell} \frac{\alpha^k\eta^k}{k!} \left(\sigma^{(k)} (\bX\bfG_{0}^\top)\right) \odot \left((\bX\hat\vbeta_a)^{\circ k}\,\bff_0^{\circ k \top}\right) + \mathcal{E}_\ell,
\end{align*}
where $\circ$ denotes element-wise power and $\mathcal{E}_\ell$ is the reminder term. Let $\eta = n^{\alpha}$ for some $\alpha \in [0, 0.5)$. Given $\alpha$, the integer $\ell$ is chosen to be large enough so that the operator norm of $\mathcal{E}_\ell$ is $o(\dx^{1/2})$ and the reminder term is negligible compared to $\sigma(\bX\bfG_{0}^\top)$. By a simple concentration argument, the matrix $\sigma^{(k)} (\bX\bfG_{0}^\top)$ can be replaced with its mean $\Ex\left(\sigma^{(k)} (\bX\bfG_{0}^\top)\right) = \mu \mathbf{1}\mathbf{1}^\top$ to get
\begin{align*}
    \bZ_{a} &\approx \sigma(\bX\bfG_{0}^\top) +  \sum_{k = 1}^{\ell} \frac{\alpha^k\eta^k \mu}{k!}(\bX\hat\vbeta_a)^{\circ k}\,\bff_0^{\circ k \top}.
\end{align*}
The first term $\sigma(\bX\bfG_{0}^\top)$ is the feature matrix of a random feature model and based on the Gaussian Equivalence Theorem (GET) (see e.g., \citet{goldt2022gaussian,hu2022universality,dandi2023learning,moniri_atheory2023}), we can 
linearize it; i.e., we can replace it with $\alpha \bX\bfG_{0}^\top + \bN$ where $\bN$ is a properly scaled independent Gaussian noise. The vectors $(\bX\hat\vbeta_a)^{\circ k}$ are nonlinear functions of the covariates with different degrees. The least squares estimator $\hat\bff_a$ is then fit on the features $\bfZ_a$ in a way that $\widehat\calL(\hat\bff_a, \widehat\bfG_a)$ is minimized; i.e.
\begin{align}
    \label{eq:train}
    \vy \approx \sigma(\bX\bfG_{0}^\top)\hat\bff_a  +  \sum_{k = 1}^{\ell} \frac{\alpha^k\eta^k \mu \,(\bff_0^{\circ k \top}\hat\bff_a)}{k!}(\bX\hat\vbeta_a)^{\circ k}.
\end{align}
Based on the GET, the random feature component can only learn linear functions with sample complexity of learning $n = \Theta(\dhid) = \Theta(\dx)$. When $\eta$ is large enough and $\vbeta_a$ is aligned to $\vbeta_\star$, with the finite dimensional correction to the random features model, the model can also represent nonlinear functions $(\vx^\top \hat\vbeta_a)^k$ of degree $k \leq \ell$ by matching the coefficients ${\alpha^k\eta^k \mu \,(\bff_0^{\circ k \top}\hat\bff_a)}/{k!}$ with the Taylor coefficients of the teacher function $\sigma_\star(\vx^\top \hat\vbeta_\star)$.

Although we have provided a complete proof sketch for providing generalization guarantees given weight alignment, a complete analysis require tedious computations and is beyond the scope of this work as we mainly focus on feature learning properties of different optimization algorithms.








