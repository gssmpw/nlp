% Good


\subsection{Lower Bounds on Trace and Spectral Norm Estimation}
\label{app:adaptive-lower-bound-overview}

In this section we formally state \Cref{thm:top-eig-lower-bound-informal} and \Cref{thm:trace-lower-bound-informal}, our lower bounds against approximating the trace and the spectral norm of a matrix.
Both lower bounds hold against algorithms that are not ill-conditioned.
So, we first take a moment to formalize this conditioning:

\begin{definition}
    Fix a matrix-vector algorithm \algo.
    For any input matrix \mA, let \(\vv^{(1)},\ldots,\vv^{(t)}\) be the matrix-vector queries computed by \algo, and let \(\mV \defeq [\vv^{(1)} ~ \cdots ~ \vv^{(t)}] \in \bbR^{n^q \times t}\) be the matrix formed by concatenating these vectors.
    If we know that for all inputs \mA we have that the condition number of \mV is at most \(\kappa\), then we say that \algo is \(\kappa-\)conditioned.
\end{definition}

We prove lower bounds against Kronecker matrix-vector algorithms that are \(\kappa-\)conditioned.
We will momentarily discuss how mild this conditioning assumption is.
First, we state our formal results:

\begin{theorem}
    \label{thm:top-eig-lower-bound}
    Any \(\kappa-\)conditioned Kronecker matrix-vector algorithm that can estimate the spectral norm of any symmetric matrix \(\mA\) to multiplicative less than error \(C_\tau^{q/2}\) with probability at least \(\frac23\) must use at least \(t = \Omega(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2}\})\) Kronecker matrix-vector products.
\end{theorem}
\begin{theorem}
    \label{thm:trace-lower-bound}
    Any \(\kappa-\)conditioned Kronecker vector-matrix-vector algorithm that can estimate the trace of any PSD matrix \(\mA\) to relative error \((1\pm\eps)\) with probability at least \(\frac23\) must use at least \(t = \Omega(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\) Kronecker matrix-vector products.
\end{theorem}

Here, \(C_\tau\) and \(C_0\) are constants greater than 1 which are specified in \Cref{lem:kron-unit-vec-conentration}.
Notice that the first result is against \emph{matrix-vector} methods where we can compute \(\mA\vx\), while the second is against \emph{vector-matrix-vector} methods where we can compute \(\vx^\intercal\mA\vx\).
The proofs for these results both follow from strong orthogonality between random Kronecker structured vectors.
Formally, we rely on the following observation:
\begin{lemma}
    \label{lem:kron-unit-vec-conentration}
    Let \(\vu = \vu_1 \otimes \cdots \otimes \vu_q\) where \(\vu_i\) is a uniformly random unit vector in \(\bbR^n\).
    Then, for any \(\vv = \vv_1 \otimes \cdots \vv_q\) where each \(\vv_i\) is an arbitrary unit vector in \(\bbR^n\), we have that
    \[
    \Pr\left[\langle \vu, \vv\rangle^2 \geq \tsfrac{C_\tau^{-q}}{n^q}\right]
    \leq C_0^{-q}
    \]
    For some universal constants \(C_\tau,C_0 > 1\).
\end{lemma}
We prove \Cref{lem:kron-unit-vec-conentration} in \Cref{sec:kron-unit-vec-conentration}.
What makes \Cref{lem:kron-unit-vec-conentration} unique is the rate of \(C_\tau^{-q}\) inside the probability.
This is because for a uniformly random unit vector \(\va \in \bbR^{n^q}\) and arbitrary unit vector \(\vb\in\bbR^{n^q}\), we instead expect that \(\langle \va, \vb\rangle^2 \approx \frac{1}{n^q}\).
So, in contrast, \Cref{lem:kron-unit-vec-conentration} shows an exponentially smaller inner product between random Kronecker-structured vectors.
We will momentarily explain how \Cref{lem:kron-unit-vec-conentration} translates into the lower bounds of \Cref{thm:top-eig-lower-bound,thm:trace-lower-bound}, but first we take a moment to discuss the strength of the conditioning assumption.

To understand the weight of this conditioning assumption, we take a moment to examine some of the most common Kronecker matrix-vector algorithms: Khatri-Rao Sketches.
A Khatri-Rao Sketch is a non-adaptive Kronecker matrix-vector product and typically takes each query vector to be the Kronecker product of \(q\) iid copies of some subgaussian vector.
That is, \(\vv^{(i)} = \otimes_{j=1}^q \vv^{(i)}_j\) where \(\vv^{(i)}_j \sim \cD\) for some isotropic distribution \cD.
For instance, Kronecker JL and Kronecker Hutchinson use Khatri-Rao Sketching \cite{jin2021faster,sun2021tensor,feldman2022entanglement,bujanovic2021norm,meyer2023hutchinson,lam2024randomized}.
In these cases, we should expect \(\mV = [\vv^{(1)} ~ \cdots ~ \vv^{(t)}]\) to be extremely well conditioned.
This is because the inner products between the query vectors tensorizes as in \Cref{lem:kron-unit-vec-conentration}:
\begin{equation*}
    \langle \vv^{(1)}, \vv^{(2)} \rangle
    = \prod_{i=1}^q \langle \vv^{(1)}_i, \vv^{(2)}_i\rangle
\end{equation*}
If the sketching vectors come from a continuous random distribution, then \Cref{lem:kron-unit-vec-conentration} tells us that these vectors have exponentially small inner product.
If the sketching vectors instead come from a discrete random distribution, then \Cref{thm:alphabet_lower_bounds} in \Cref{sec:zero-testing} shows that the inner product will be exactly zero with very high probability.
Either way, the matrix \mV has nearly orthogonal columns with very high probability, in turn implying that the condition number of \mV is at most \(O(1)\) with very high probability.
So, any Khatri-Rao sketching method must incur the exponential lower bounds in \Cref{thm:top-eig-lower-bound,thm:trace-lower-bound}.
More broadly, we are not aware of any Kronecker matrix-vector algorithm that whose condition number is exponential in \(q\), which is the degree of ill-conditioning required to avoid incurring the exponential lower bound.


