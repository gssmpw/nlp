% Good


\section{L2 Estimation via Linear Measurements}
\label{app:l2-estimation-formal-lower}

\begin{problem}
    \label{prob:l2-estimation}
    Fix a vector \(\va\in\bbR^{n^q}\).
    Then, the \emph{Kronecker-structured linear measurement oracle} for \va is the oracle that, when given any Kronecker structured vector \(\vv\in\bbR^{n^q}\), returns \(\langle \va, \vv\rangle\).
    In the \emph{L2 Estimation via Kronecker Measurements} problem, we have to use a few oracle queries as possible to return a number \(z\in\bbR\) such that \((1-\eps)\norm\va_2^2 \leq z \leq (1+\eps)\norm\va_2^2\) with probability \(\frac23\).
\end{problem}


\begin{theorem}
    \label{thm:l2-adaptive-estimation-lower-bound}
    Any (possibly adaptive) algorithm \cA that solves \Cref{prob:l2-estimation} with probability \(\frac23\) using \(\kappa-\)conditioned Kronecker-structured queries must use at least \(t = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\) queries.
\end{theorem}


Our proof methodology mirrors that of Section 6 in \cite{simchowitz2017gap}, but applied to this linear measurements framework instead of the matrix-vector framework as studied in their paper (and partially explained in \Cref{app:explain-simchowitz}).
The crux of this section is to show that \Cref{lem:kron-unit-vec-conentration} implies the lower bound in \Cref{thm:l2-adaptive-estimation-lower-bound}.
We prove this lower bound by appealing to the following testing problem:

\begin{problem}
    \label{prob:l2-estimation-instance}
    Fix \(n,q\in\bbN\) and \(\lambda > 1\).
    Let \(\vg\in\bbR^{n^q}\) be a \(\cN(\vec0,\mI)\) vector, and let \(\vu = \vu_1\otimes \cdots \otimes \vu_q\) where each \(\vu_i \in \bbR^n\) vector is uniformly distributed on the set of vectors with \(\norm{\vu_i}_2^2 = n\).
    Further, let
    \[
        \va_0 = \vg
        \hspace{1cm}\text{and}\hspace{1cm}
        \va_1 = \vg + \lambda\vu.
    \]
    Suppose that nature samples \(i \in \{0,1\}\) uniformly at random.
    Then, an algorithm \cA computes \(t\) linear measurements with \(\va \defeq \va_i\) and then guesses if \(i=0\) or \(i=1\).
\end{problem}
The result \Cref{thm:l2-adaptive-estimation-lower-bound} follows from combining two results: showing that any L2 estimating algorithm can distinguish \(\va_0\) from \(\va_1\), and that distinguishing \(\va_0\) from \(\va_1\) requires exponential query complexity.
We start with the former result.

\begin{lemma}
    Let \cA be any linear measurement algorithm that can solve \Cref{prob:l2-estimation} with probability \(\frac23\) for some \(\eps \in (0,0.25)\).
    Then \cA can solve \Cref{prob:l2-estimation-instance} when \(\lambda=6\sqrt\eps\) and \(n^q = \Omega(\frac1{\eps^2})\) with probability at least \(\frac35\).
\end{lemma}
\begin{proof}
    Throughout this proof, we let \(C>0\) be a large enough constant that both of the concentrations required simultaneously hold with probability \(\frac{9}{10}\).
    We will concretely assume that \(\frac{1}{n^{q/2}} \leq \min\{\frac{\lambda^2}{4C},\frac{\lambda}{8C}\}\).
    Note that \(\norm{\vg}_2^2\) is a chi-squared random variable with parameter \(n^q\).
    So, \(\norm{\va_0}_2^2 = \norm{\vg}_2^2 \in (1\pm\frac{C}{n^{q/2}})n^q \subseteq (1\pm\frac{\lambda^2}{4})n^q\).
    We also know that \(\norm{\vh}_2^2 = n^q\) exactly.
    Further, since \(\vg\) is Gaussian, we know that \(\langle \vg, \vh \rangle \sim \cN(\vec0,\norm{\vh}_2^2) = \cN(\vec0,n^q)\), and therefore that \(|\langle \vg, \vh \rangle| \leq C n^{q/2}\).
    This lets us expand
    \begin{align*}
    \norm{\va_1}_2^2
    &= \norm{\vg}_2^2 + \lambda^2\norm{\vh}_2^2 - 2\lambda\langle \vg, \vh\rangle \\
    &\geq (1-\tsfrac{\lambda^2}{4})n^q + \lambda^2n^q - \tsfrac{2\lambda C}{n^{q/2}}n^q \\
    &\geq (1-\tsfrac{\lambda^2}{4})n^q + \lambda^2n^q - \tsfrac{\lambda^2}{4}n^q \\
    &= (1+\tsfrac{\lambda^2}{2})n^q
    \end{align*}
    So, we have that
    \[
        \norm{\va_0}_2^2 \leq (1+\tsfrac{\lambda^2}{4}) n^q
        \hspace{1cm}
        \text{and}
        \hspace{1cm}
        \norm{\va_1}_2^2 \geq (1+\tsfrac{\lambda^2}{2}) n^q.
    \]
    In particular, since \(\lambda = 6\sqrt\eps\), we have that
    \[
        (1+\eps)\norm{\va_0}_2^2
        \leq (1+\eps)(1+\tsfrac{36\eps}{4})n^q
        < (1-\eps)(1+\tsfrac{36\eps}{2})n^q
        \leq (1-\eps)\norm{\va_1}_2^2
    \]
    holds for all \(\eps\in(0,0.25)\).
    In particular, this means that any algorithm \cA that can estimate \((1\pm\eps)\norm{\va}_2^2\) from \(t\) measurements can distinguish \(\va_0\) from \(\va_1\) with high probability, completing the proof
\end{proof}

Next, we show the crux of the lower bound -- that \Cref{prob:l2-estimation-instance} has exponential sample complexity lower bound.
We show this by applying \Cref{lem:corrected-truncated-dtv} to our setting.
In order to instantiate this theorem though, we have to introduce some further notation.

\begin{setting}
    \label[setting]{setting:l2-estimation-proof-notation}
Fix an algorithm \cA that solves \Cref{prob:l2-estimation-instance}.
Let \(\vv^{(1)},\ldots,\vv^{(t)}\) be the (possibly adaptive) query vectors computed by \cA.
Let \(w_1,\ldots,w_t\) be the responses from the oracle.
That is, \(w_i = \langle \vv^{(i)}, \va \rangle\).
Let \(\cZ_i = (\vv^{(1)}, w_1, \ldots, \vv^{(i)}, w_i)\) be the transcript of all information sent between \cA and the oracle in the first \(i\) queries.
By Yao's minimax principle, we assume without loss of generality that \cA is deterministic.
Therefore, \(\vv^{(i)}\) is a deterministic function of \(\cZ_{i-1}\).

We let \bbQ denote the distribution of \(\cZ_t\) when \(\va = \va_0\).
We let \(\bbP_{\vu}\) denote the distribution of \(\cZ_t\) when \(\va = \va_1\) conditioned on a specific value of \vu.
We let \(\bar\bbP\) denote the marginal distribution of \(\bbP_{\vu}\) over all \vu, or equivalently that \(\bar\bbP\) is the distribution of \(\cZ_t\) when \(\va = \va_1\).
Lastly, we let \(A_{\vu}^{i}\) be the event that \(\{\forall j\in [i], \langle \vv^{j}, \vu\rangle^2 \leq \tau_j\}\) for some numbers \(0 \leq \tau_1 \leq \ldots \leq \tau_t\) that will be clear from context.
\end{setting}

Following this notation, to show that no algorithm can distinguish \(\va_0\) from \(\va_1\), it suffices to show that there is low total variation between \(\bar\bbP\) and \bbQ.
We will do this by applying \Cref{lem:corrected-truncated-dtv}.
In particular, specialized to our context, the lemma says the following:
\begin{corollary}
    \label{corol:l2-estimation-proof-goals}
    Consider \Cref{setting:l2-estimation-proof-notation}.
    Fix any numbers \(0 \leq \tau_1 \leq \ldots \leq \tau_t\).
    If we are given that
    \begin{align}
        \Pr[\exists i \in [t] ~:~ \langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i] \leq z
        \label{eqn:l2-truncation-rate}
    \end{align}
    and that
    \begin{align}
        \E_{\cZ_t \sim \bbQ}\left[
            \left(\frac{
                \E_{\vu}[d\bbP_{\vu}(\cZ_t \cap A_{\vu}^{t})]
            }{
                d\bbQ(\cZ_t)
            }\right)^2
        \right]
        \leq 1+z
        \label{eqn:l2-estimation-divergence-rate}
    \end{align}
    then the total variation distance between \(\bar\bbP\) and \bbQ is at most \(\sqrt{3z}\).
    In particular, if we take \(z = \frac{1}{27}\) then \cA cannot distinguish between \(\va_0\) and \(\va_1\) with probability at least \(\frac23\).
\end{corollary}
This corollary follows directly from plugging in \Cref{setting:l2-estimation-proof-notation} into \Cref{lem:corrected-truncated-dtv}.
In order to prove \Cref{thm:l2-adaptive-estimation-lower-bound}, we just need to prove that both \Cref{eqn:l2-truncation-rate,eqn:l2-estimation-divergence-rate} hold with \(z = \frac1{27}\).
This will be the focus of the rest of the subsection.

First, we will need the following claim about divergences:
\begin{lemma}
\label{lem:gaussian-chi-squared-divergence}
Let \(\bbP_a\) denote the distribution \(\cN(\va,\mSigma)\), \(\bbP_b\) the distribution \(\cN(\vb,\mSigma)\), and \(\bbQ\) the distribution \(\cN(\vec0,\mSigma)\).
Then,
\[
    \E_{\vz\sim\bbQ}\left[
        \left(\frac{d\bbP_a(\vz)}{d\bbQ(\vz)}\right)^2
    \right]
    = e^{\va^\intercal\mSigma^{-1}\va}
\]
and
\[
    \E_{\vz\sim\bbQ}\left[
        \frac{d\bbP_a(\vz) d\bbP_b(\vz)}{(d\bbQ(\vz))^2}
    \right]
    = e^{\va^\intercal\mSigma^{-1}\vb}
\]
\end{lemma}
\begin{proof}
We prove only the second claim as the first claim follows from taking \(\vb=\va\).
We directly expand the expectation using the corresponding PDFs, nothing that the terms outside the expectation all exactly cancel since our distributions all share the same covariance matrix.
\begin{align*}
    \E_{\vz\sim\bbQ}\left[
        \frac{d\bbP_a(\vz) d\bbP_b(\vz)}{(d\bbQ(\vz))^2}
    \right]
    &= \E_{\vz\sim\bbQ}\left[
        e^{
            -\frac12(\vz-\va)^\intercal\mSigma^{-1}(\vz-\va)
            -\frac12(\vz-\vb)^\intercal\mSigma^{-1}(\vz-\vb)
            + \vz^\intercal\mSigma^{-1}\vz
        }
    \right] \\
    &= \E_{\vz\sim\bbQ}\left[
        e^{
            -\frac12(\va^\intercal\mSigma^{-1}\va
            +\vb^\intercal\mSigma^{-1}\vb)
            +(\vz^\intercal\mSigma^{-1}\va
            +\vz^\intercal\mSigma^{-1}\vb)
        }
    \right] \\
    &= e^{
            -\frac12(\va^\intercal\mSigma^{-1}\va
            +\vb^\intercal\mSigma^{-1}\vb)
        }
        \E_{\vz\sim\bbQ}\left[
        e^{
            \vz^\intercal(\mSigma^{-1}(\va+\vb))
        }
    \right] \\
    &= e^{
            -\frac12(\va^\intercal\mSigma^{-1}\va
            +\vb^\intercal\mSigma^{-1}\vb)
        }
        e^{
            \frac12
            (\mSigma^{-1}(\va+\vb))^\intercal
            \mSigma
            (\mSigma^{-1}(\va+\vb))
        } \tag{Gaussian MGF}\\
    &= e^{
            -\frac12(\va^\intercal\mSigma^{-1}\va
            +\vb^\intercal\mSigma^{-1}\vb)
        }
        e^{
            \frac12
            (\va+\vb)^\intercal
            \mSigma^{-1}
            (\va+\vb)
        } \\
    &= e^{
            -\frac12(\va^\intercal\mSigma^{-1}\va
            +\vb^\intercal\mSigma^{-1}\vb)
        }
        e^{
            \frac12
            (
            \va^\intercal\mSigma^{-1}\va
            + \vb^\intercal\mSigma^{-1}\vb
            + 2\va^\intercal\mSigma^{-1}\vb
            )
        } \\
    &= e^{
            \va^\intercal\mSigma^{-1}\vb
        }
\end{align*}
\end{proof}

We will also need the following information-theoretic claim from \cite{simchowitz2017gap}:
\begin{importedtheorem}[Proposition 5.1 of \cite{simchowitz2017gap}]
    \label{impthm:simchowitz-core-chi-squared-divergence}
    Let \(\cP\) be a prior distribution over parameters \(\theta \in \Theta\).
    Let \(\{\bbP_\theta\}_{\theta \in \Theta}\) be a family of distributions on space \((\cX,\cF)\) parameterized by \(\theta\).
    Let \(\{A_{\theta}\}_{\theta \in \Theta}\) be a set of events defined on \cF.
    Let \(\cV\) be an action space (i.e. an arbitrary set).
    Let \(\cL:\cV\times\Theta \rightarrow \{0,1\}\) be a binary loss function.
    Let \cA denote a determinstic algorithm that observes data and picks an action; that is \cA is any map from \cX to \cV.
    Let \(V_0\) be the probability an algorithm can achieve loss 0 without observing data:
    \[
        V_0 = \sup_{v\in\cV} \Pr_{\theta \sim \cP} [\cL(v,\theta)=0],
    \]
    and let \(V_v\) be the probability that \cA achieves loss 0 after observing a sample from \(\bbP_\theta\) while event \(A_{\theta}\) happens:
    \[
        V_v = \E_{\theta\sim\cP} \Pr_{x\sim\bbP_\theta}[\cL(\cA(x), \theta) = 0, A_\theta].
    \]
    Then, for any probability distribution \bbQ also on \((\cX,\cF)\),
    \[
        V_v \leq V_0 + \sqrt{V_0(1-V_0)\E_{\theta\sim\cP}\E_{x\sim\bbQ}\left[
            \left(\tsfrac{d\bbP_{\theta}[x]}{d\bbQ[x]}\right)^2
            \mathbbm1_{[A_\theta]}
        \right]}.
    \]
\end{importedtheorem}
This result will suffice to bound \Cref{eqn:l2-truncation-rate}:
\begin{lemma}
    \label{lem:finding-u-lower-bound}
    Consider \Cref{setting:l2-estimation-proof-notation}, where \(\tau_1=\cdots=\tau_t = C_\tau^{-q}\).
    Then, we have that
    \[
        \Pr[\exists i \in [t] ~:~ \langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i] \leq \tsfrac{1}{27}
    \]
    so long as \(t = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\).
\end{lemma}
\begin{proof}
We start by expanding the target probability into a probabilistic claim for each query made by the algorithm:
\begin{align*}
    \Pr[\exists i \in [t] ~:~ \langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i]
    &\leq \sum_{i=1}^t \Pr[\langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i ~:~ \forall j\in[i-1] ~ \langle \vv^{(j)}, \vu\rangle^2 \leq \tau_j]
\end{align*}
Now, we bound each summand on the right by using \Cref{impthm:simchowitz-core-chi-squared-divergence}.
We take \(\theta \defeq \vu\), so that \(\cP\) is the distribution of \vu.
Then, \(\bbP_\theta\) becomes \(\bbP_{\vu}\) from \Cref{setting:l2-estimation-proof-notation}, and \(\bbQ\) is exactly \bbQ as in \Cref{setting:l2-estimation-proof-notation}.
We take the truncation event \(A_\theta = A_{\vu}^{i-1}\).
The set of actions \cV is conditioned on the prior query vectors \(\vv^{(1)},\ldots,\vv^{(i-1)}\) already made by the algorithm:
\[
    \cV^i
    \defeq
    \{
        \vv^{(i)}
        ~:~
        \vv^{(i)} = \otimes_{j=1}^q \vv^{(i)}_j
        ,~
        \norm{\vv^{(i)}}_2=1
        ,~
        \text{cond}(\sbmat{\vv^{(1)} & \ldots & \vv^{(i)}}) \leq \kappa
    \}
\]
Lastly, we take \(\cL(\vv^{(i)},\vu) = \mathbbm1_{[\langle \vv^{(i)}, \vu\rangle^2 \leq \tau_i]}\).
Therefore, \Cref{impthm:simchowitz-core-chi-squared-divergence} takes
\[
    V_0 = \sup_{\vv^{(i)}\in\cV^{i}} \Pr_{\vu} [\langle \vv, \vu \rangle^2 \geq \tau_i] \leq C_0^{-q}
\]
where the last inequality uses \Cref{lem:kron-unit-vec-conentration}, recalling that \(\tau_i = C_\tau^{-q}\).
So, \Cref{impthm:simchowitz-core-chi-squared-divergence} tells us that
\begin{align*}
    V_v
    &= \E_{\vu} \Pr_{\cZ_i\sim\bbP_{\vu}}[\cL(\cA(x), \theta) = 0, A_\theta] \\
    &= \Pr[\langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i ~:~ \forall j\in[i-1] \langle \vv^{(j)}, \vu\rangle^2 \leq \tau_i] \\
    &\leq C_0^{-q} + \sqrt{C_0^{-q}\E_{\vu}\E_{\cZ_i\sim\bbQ}\left[
            \left(\tsfrac{d\bbP_{\vu}[\cZ_i]}{d\bbQ[\cZ_i]}\right)^2
            \mathbbm1_{[A_{\vu}^{i-1}]}
        \right]}
\end{align*}
So, next we bound this expectation.
First, we take a moment to examine this ratio of probabilities.
We would like to apply \Cref{lem:gaussian-chi-squared-divergence} to bound the inner-most expectation.
However, the indicator variable inside the expectation prevents us from doing so.
Instead, we apply \Cref{lem:unrolling-lemma} to this situation (taking \(\bbP_a = \bbP_b = \bbP_{\vu}\)).
This lemma tells us that
\[
    \E_{\cZ_i\sim\bbQ}\left[
        \left(\tsfrac{d\bbP_{\vu}[\cZ_i]}{d\bbQ[\cZ_i]}\right)^2
        \mathbbm1_{[A_{\vu}^{i-1}]}
    \right]
    \leq \sup_{\cZ_i ~:~ (\vv^{(1)},\cdots,\vv^{(i)})\in A^{i-1}}
    \prod_{j=1}^i
    \E\left[
            \left(\frac{d\bbP_{\vh}(\cZ_j \mid \cZ_{j-1})}{d\bbQ(\cZ_j \mid \cZ_{j-1})}\right)^2 \big| \cZ_{j-1}
    \right].
\]
Now, we analyze this conditional expectation on the right.
First, recall that we assumed without loss of generality that \cA is a determinstic algorithm.
Therefore, the \(j^{th}\) query vector \(\vv^{(j)}\) is deterministic in \(\cZ_{j-1}\).
So, the random variable \(\cZ_j \mid \cZ_{j-1}\) is equivalent to just looking at \(w_j = \langle \vv^{(j)}, \va \rangle\).
Formally using the Data Processing Inequality (Lemma E.1 from \cite{simchowitz2017gap}), this means that it suffices to bound
\[
    \E\left[
        \left(\frac{d\bbP_{\vh}( w_j \mid \cZ_{j-1})}{d\bbQ( w_j \mid \cZ_{j-1})}\right)^2 \big| \cZ_{j-1}
    \right].
\]
Next, we take a moment to analyze the impact of conditioning here.
Unfortunately, it is annoying to analyze the expression above due to the way that \(w_j = \langle \vv^{(j)}, \va \rangle\) may depend on the previous observations in \(\cZ_{j-1}\).
Instead, we appeal to the Data Processing Inequality again to orthonormalize.
In particular, for the set of queries \(\mV_j \defeq [\vv^{(1)} ~ \cdots ~ \vv^{(j)}]\) in \(\cZ_j\), we let \(\mX_j \defeq [\vx^{(1)} ~ \cdots ~ \vx^{(j)}]\) be the result of running Gram-Schmidt on \(\mV_j\).
That is, \(\mX_j\) is an orthogonal matrix that spans \(\mV_j\).
We can write our new adjusted transcript as
\[
    \tilde \cZ_j = ( \vx^{(1)}, \langle \vx^{(1)}, \va\rangle, \cdots, \vx^{(j)}, \langle \vx^{(j)}, \va\rangle)
\]
Since this process is invertible, it does not change the statistical distance, and therefore it suffices to bound
\[
    \E\left[
        \left(\frac{d\bbP_{\vh}( \langle \vx^{(j)}, \va\rangle \mid \tilde\cZ_{j-1})}{d\bbQ( \langle \vx^{(j)}, \va\rangle \mid \tilde\cZ_{j-1})}\right)^2 \big| \tilde\cZ_{j-1}
    \right].
\]
Next, we observe that the set of all observations under the adjusted transcript \(\tilde\vw = [\langle \vx^{(1)}, \va\rangle ~ \cdots ~ \langle \vx^{(j)}, \va\rangle] = \mX_j^\intercal\va\) is distributed as a multivariate Gaussian.
Under \bbQ, \(\tilde\vw \sim \cN(\vec0, \mX_j^\intercal\mX_j) = \cN(\vec0,\mI)\).
Similarly, under \(\bbP_{\vu}\), \(\tilde\vw \sim \cN(\lambda\mX_j^\intercal\vu, \mI)\).
Notice that under both distributions, we have that all entries of \(\tilde\vw\) are independent.
Therefore, we know that \(\langle \vx^{(j)}, \va\rangle\) is independent of all other \(\langle \vx^{(m)}, \va\rangle\) given \(\mX_j\).
So, we can use \Cref{lem:gaussian-chi-squared-divergence} to say that
\[
    \E\left[
        \left(\frac{d\bbP_{\vh}( \langle \vx^{(j)}, \va\rangle \mid \tilde\cZ_{j-1})}{d\bbQ( \langle \vx^{(j)}, \va\rangle \mid \tilde\cZ_{j-1})}\right)^2 \big| \tilde\cZ_{j-1}
    \right]
    = e^{\lambda^2 \langle \vx^{(j)}, \vu\rangle^2}
\]
We want to upper bound this expectation in terms of our original transcript \(\cZ_t\) though.
Here, we use our conditioning assumption.
By \Cref{lem:conditioning-to-ortho-inner-prod}, we know that \(\langle \vx^{(j)}, \vu\rangle^2 \leq \kappa^2 \norm{\mV_j^\intercal\vu}_2^2\).
This means we bound
\[
    \E\left[
        \left(\frac{d\bbP_{\vh}( \langle \vx^{(j)}, \va\rangle \mid \tilde\cZ_{j-1})}{d\bbQ( \langle \vx^{(j)}, \va\rangle \mid \tilde\cZ_{j-1})}\right)^2 \big| \tilde\cZ_{j-1}
    \right]
    \leq e^{\lambda^2 \kappa^2 \norm{\mV_j^\intercal\vu}_2^2}.
\]
Further, using our conditioning assumption, we know that
Backing up, we then need to bound
\begin{align*}
    \E_{\cZ_i\sim\bbQ}\left[
        \left(\tsfrac{d\bbP_{\vu}[\cZ_i]}{d\bbQ[\cZ_i]}\right)^2
        \mathbbm1_{[A_{\vu}^{i-1}]}
    \right]
    &\leq \sup_{\cZ_i ~:~ (\vv^{(1)},\cdots,\vv^{(i)})\in A^{i-1}}
    \prod_{j=1}^i
    \E\left[
            \left(\frac{d\bbP_{\vh}(\cZ_j \mid \cZ_{j-1})}{d\bbQ(\cZ_j \mid \cZ_{j-1})}\right)^2 \big| \cZ_{j-1}
    \right] \\
    &= \sup_{\cZ_i ~:~ (\vv^{(1)},\cdots,\vv^{(i)})\in A^{i-1}}
    e^{\lambda^2 \kappa^2 \sum_{j=1}^i \norm{\mV_j^\intercal\vu}_2^2} \\
    &\leq \sup_{\cZ_i ~:~ (\vv^{(1)},\cdots,\vv^{(i)})\in A^{i-1}}
    e^{\lambda^2 \kappa^2 i \norm{\mV_i^\intercal\vu}_2^2} \\
    &\leq e^{\lambda^2 \kappa^2 i \sum_{j=1}^i \tau_j} \\
    &\leq e^{\lambda^2 \kappa^2 i^2 C_\tau^{-q}}
\end{align*}
Then, we can complete our overall lemma by taking
\begin{align*}
    \Pr[\exists i \in [t] ~:~ \langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i]
    &\leq \sum_{i=1}^t \Pr[\langle \vv^{(i)}, \vu\rangle^2 \geq \tau_i ~:~ \forall j\in[i-1] ~ \langle \vv^{(j)}, \vu\rangle^2 \leq \tau_j] \\
    &\leq \sum_{i=1}^t \left(C_0^{-q} + \sqrt{C_0^{-q}\E_{\vu}\E_{\cZ_i\sim\bbQ}\left[
            \left(\tsfrac{d\bbP_{\vu}[\cZ_i]}{d\bbQ[\cZ_i]}\right)^2
            \mathbbm1_{[A_{\vu}^{i-1}]}
        \right]}\right) \\
    &\leq \sum_{i=1}^t \left(C_0^{-q} + C_0^{-q/2}e^{\lambda^2 \kappa^2 i^2 C_\tau^{-q} / 2} \right) \\
    &\leq \sum_{i=1}^t 2C_0^{-q/2}e^{\lambda^2 \kappa^2 i^2 C_\tau^{-q} / 2} \\
    &\leq 2tC_0^{-q/2}e^{\lambda^2 \kappa^2 t^2 C_\tau^{-q} / 2} \\
    &\leq \frac{1}{27}
\end{align*}
where we take \(t = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \lambda}\}) = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\) on the last line.




\end{proof}


\begin{lemma}
    Consider \Cref{setting:l2-estimation-proof-notation}, where \(\tau_1=\cdots=\tau_t = C_\tau^{-q}\).
    Then, we have that
    \[
        \E_{\cZ_t \sim \bbQ}\left[
            \left(\frac{
                \E_{\vu}[d\bbP_{\vu}(\cZ_t \cap A_{\vu}^{t})]
            }{
                d\bbQ(\cZ_t)
            }\right)^2
        \right]
        \leq 1+z
    \]
    so long as \(t = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\).
\end{lemma}
\begin{proof}
Equation (6.31) of \cite{simchowitz2017gap} shows that we can rewrite
\[
    \E_{\cZ_t \sim \bbQ}\left[
        \left(\frac{
            \E_{\vu}[d\bbP_{\vu}(\cZ_t \cap A_{\vu}^{t})]
        }{
            d\bbQ(\cZ_t)
        }\right)^2
    \right]
    = \E_{\vu,\vu'}\left[
        \E_{\cZ_t \sim \bbQ}\left[
            \frac{
                d\bbP_{\vu}(\cZ_t \cap A_{\vu}^{t})
                d\bbP_{\vu'}(\cZ_t \cap A_{\vu'}^{t})
            }{
                (d\bbQ(\cZ_t))^2
            }
        \right]
    \right]
\]
where \(\vu'\) is an iid copy of \vu.
Then, by \Cref{lem:unrolling-lemma} we know that
\[
    \E_{\cZ_t \sim \bbQ}\left[
        \frac{
            d\bbP_{\vu}(\cZ_t \cap A_{\vu}^{t})
            d\bbP_{\vu'}(\cZ_t \cap A_{\vu'}^{t})
        }{
            (d\bbQ(\cZ_t))^2
        }
    \right]
    \leq
    \sup_{\cZ_t\in A_{\vu}^t \cap A_{\vu'}^t}
    \prod_{i=1}^t \E\left[
        \frac{
            d\bbP_{\vu}(\cZ_i \mid \cZ_{i-1}) d\bbP_{\vu'}(\cZ_i \mid \cZ_{i-1})
        }{
            (d\bbQ(\cZ_i \mid \cZ_{i-1}))^2} \big| \cZ_{i-1
        }
    \right].
\]
As in \Cref{lem:finding-u-lower-bound}, we change our basis from \(\mV_j\) to \(\mX_j\).
Under \bbQ, we have \(\tilde\vw \sim \cN(\vec0,\mI)\).
Under \(\bbP_{vu}\), we have \(\tilde\vw\sim\cN(\lambda\mX_j^\intercal\vu,\mI)\).
So, by \Cref{lem:gaussian-chi-squared-divergence}, we know that
\[
    \E\left[
        \frac{
            d\bbP_{\vu}(\cZ_i \mid \cZ_{i-1}) d\bbP_{\vu'}(\cZ_i \mid \cZ_{i-1})
        }{
            (d\bbQ(\cZ_i \mid \cZ_{i-1}))^2
        } \big| \cZ_{i-1}
    \right]
    = e^{\lambda^2 \langle \vx^{(j)}, \vu\rangle \langle \vx^{(j)}, \vu'\rangle}
\]
And, again following \Cref{lem:conditioning-to-ortho-inner-prod}, we bound \(|\langle\vx^{(j)},\vu\rangle| \leq \kappa \norm{\mV_j^\intercal\vu}_2\), so the above exponential is at most \(e^{\lambda^2 \kappa^2 \norm{\mV_j^\intercal\vu}_2^2}\).
Therefore,
\begin{align*}
    \E_{\cZ_t \sim \bbQ}\left[
        \frac{
            d\bbP_{\vu}(\cZ_t \cap A_{\vu}^{t})
            d\bbP_{\vu'}(\cZ_t \cap A_{\vu'}^{t})
        }{
            (d\bbQ(\cZ_t))^2
        }
    \right]
    &\leq
    \sup_{\cZ_t\in A_{\vu}^t \cap A_{\vu'}^t}
    \prod_{i=1}^t \E\left[
        \frac{
            d\bbP_{\vu}(\cZ_i \mid \cZ_{i-1}) d\bbP_{\vu'}(\cZ_i \mid \cZ_{i-1})
        }{
            (d\bbQ(\cZ_i \mid \cZ_{i-1}))^2
        } \big| \cZ_{i-1}
    \right] \\
    &\leq
    \sup_{\cZ_t\in A_{\vu}^t \cap A_{\vu'}^t}
    e^{\lambda^2 \kappa^2 \sum_{i=1}^t \norm{\mV_j^\intercal\vu}_2^2} \\
    &\leq
    \sup_{\cZ_t\in A_{\vu}^t \cap A_{\vu'}^t}
    e^{\lambda^2 \kappa^2 t \norm{\mV_t^\intercal\vu}_2^2} \\
    &\leq e^{\lambda^2 \kappa^2 t \sum_{i=1}^t \tau_i} \\
    &\leq e^{\lambda^2 \kappa^2 t^2 C_\tau^{-q}} \\
    &\leq 1 + \frac{1}{27}
\end{align*}
where we take \(t = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \lambda}\}) = O(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\) on the last line, completing the proof.
\end{proof}
