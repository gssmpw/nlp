% Good

\section{Proving \Cref{thm:top-eig-lower-bound} and \Cref{thm:trace-lower-bound}}
\label{sec:conditioned}

In this section, we outline the proof techniques for \Cref{thm:top-eig-lower-bound} and \Cref{thm:trace-lower-bound}.
Both lower bounds rely on \Cref{lem:kron-unit-vec-conentration} as a starting point, as we can plant a very large random Kronecker-structured vector on some Gaussian data.
Since the inner product between our queries \(\vv\in\bbR^{n^q}\) and the planted vector \(\vu\in\bbR^{n^q}\) is tiny, our queries cannot reliably identify the planted structure \vu.
More specifically, the noise from the Gaussian data hides the impact of the inner product between \vv and \vu on our queries.
In the following subsections, we formalize this idea.

More broadly, our proofs hold against adaptive algorithms.
That is, the algorithm is allowed to use previous responses from the oracle to decide what query to compute next.
We handle adaptivity by generalizing the proof techniques in \cite{simchowitz2017gap}, who propose an information-theoretic structure to lower bound the number of matrix-vector products needed to solve certain linear algebra problems.
In \Cref{app:explain-simchowitz}, we generalize their techniques in order to give lower bounds against \emph{arbitrary constrained matrix-vector models}.
For instance, while we constrain ourselves to use Kronecker-structured matrix-vector products, we could instead analyze sparse query vectors instead though this model.
We leave the broader implications of this generalized lower bound as future work.

\subsection{Proof Sketch of Trace Estimation Lower Bound}
\label{sec:trace-est-proof-sketch}

We now outline the proof of \Cref{thm:trace-lower-bound}.
We start by invoking a related but different query complexity problem in a related but different computational model.

\begin{definition}
    Fix a vector \(\va \in \bbR^{n^q}\).
    The Kronecker-Structured Linear Measurement Oracle for \va is the oracle that, given any vectors \(\vv_1,\ldots,\vv_q \in \bbR^n\), returns the inner product \(\langle \va, \vx_1 \otimes \cdots \otimes \vx_q\rangle \in \bbR\).
    \[
        (\vx_1, \cdots, \vx_k)
		\hspace{0.1cm} \xRightarrow{input} \hspace{0.1cm}
		\textsc{oracle}
		\hspace{0.1cm} \xRightarrow{output} \hspace{0.1cm}
		\langle \va, \vx_1 \otimes \cdots \otimes \vx_q \rangle
    \]
\end{definition}
\begin{theorem}
    \label{thm:l2-estimation-lower-bound}
    Any \(\kappa-\)conditioned Kronecker-Structured Linear Measurement algorithm that can estimate the squared L2 norm \(\norm{\va}_2^2\) to relative error \((1\pm\eps)\) with probability \(\frac23\) must use at least \(t = \Omega(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\})\) queries.
\end{theorem}

First note that \Cref{thm:l2-estimation-lower-bound} suffices to prove \Cref{thm:trace-lower-bound}.
This is because any vector-matrix-vector trace estimation method can be used to construct a linear measurement algorithm.
That is, suppose that some vector-matrix-vector algorithm can estimate the trace of any PSD matrix \(\mA\in\bbR^{n^q \times n^q}\) with probability \(\frac23\) using \(t\) queries.
We can then fix the input matrix \(\mA = \va\va^\intercal\), where \(\va\) is the vector as in \Cref{thm:l2-estimation-lower-bound}.
Then, a vector-matrix-vector product with \mA is \(\vv^\intercal\mA\vv = \langle \vv, \va\rangle^2\), which is the square of a linear measurement with \va.
Further, \(\tr(\mA) = \tr(\va\va^\intercal) = \norm{\va}_2^2\).
So, we must have that the number of vector-matrix-vector queries made by \mA cannot violate the linear measurement lower bound in \Cref{thm:l2-estimation-lower-bound}.

So, our goal is now to prove \Cref{thm:l2-estimation-lower-bound}.
The crux of the proof is to use \Cref{lem:kron-unit-vec-conentration} to show that no Kronecker matrix-vector method can distinguish between linear measurements between two vectors:
\begin{problem}
    \label[problem]{prob:l2-testing}
    Fix \(n,q\in\bbN\) and \(\lambda = 6\sqrt\eps\).
    Let \(\vg\in\bbR^{n^q}\) be a \(\cN(\vec0,\mI)\) vector, and let \(\vu = \vu_1 \otimes \cdots \otimes \vu_q\) where each \(\vu_i \in \bbR^n\) is distributed uniformly on the set of vectors with \(\norm{\vu_i}_2^2 = n\).
    Further, let
    \vspace{-0.5em}
    \[
    \va_0 \defeq \vg
    \hspace{1cm} \text{and} \hspace{1cm}
    \va_1 \defeq \vg + \lambda\vu.
    \]
    \vspace{-0.1em}
    Suppose that nature samples \(i\in\{0,1\}\) uniformly at random.
    An algorithm then computes \(t\) linear measurements with \(\va \defeq \va_i\) and has to guess if \(\va=\va_0\) or \(\va=\va_1\).
\end{problem}
In \Cref{app:l2-estimation-formal-lower} we formally prove the exponential lower bound against \Cref{prob:l2-testing} as stated in \Cref{thm:l2-estimation-lower-bound}.
We do take a moment to sketch the proof here though.

Consider a non-adaptive Kronecker-structured linear measurement algorithm for \Cref{prob:l2-testing}.
If a method is non-adaptive, then we can think of it as a method that picks a matrix \mV with Kronecker-structured columns and computes \(\mV^\intercal\va = [\langle \vv^{(1)}, \va\rangle ~ \cdots ~ \langle \vv^{(t)}, \va \rangle]\).
So, to prove our lower bound against non-adaptive algorithms, we need to show that for all \mV with \(t\) Kronecker-structured columns and condition number at most \(\kappa\), it is not possible to distinguish \(\vw_0 \defeq \mV^\intercal\va_0\) from \(\vw_1 \defeq \mV^\intercal\va_1\).

We start by examining these two distributions.
Because \vg is Gaussian, we know that \(\vw_0 \sim \cN(\vec0,\mV^\intercal\mV)\).
Similarly, for a fixed value of \(\vu\), we know that \(\vw_1 \sim \cN(\lambda\mV^\intercal\vu, \mV^\intercal\mV)\).
These two distributions differ only in their means and share the same covariance matrix.
In particular, we can easily bound the KL Divergence between \(\vw_0\) and \(\vw_1\) for a fixed value of \(\vu\):
\begin{equation}
    D_{KL}(\vw_0\|\vw_1|\vu)
    = \frac{\lambda^2}{2} \vu^\intercal\mV(\mV^\intercal\mV)^{-1}\mV^\intercal\vu.
    \label{eqn:nonadaptive-dkl}
\end{equation}
This follows from the KL divergence between \(\cN(\vecalt\mu,\mSigma)\) and \(\cN(\vec0,\mSigma)\) being exactly \(\frac12 \vecalt\mu^\intercal\mSigma^{-1}\vecalt\mu\).
We can then use a union bound with \Cref{lem:kron-unit-vec-conentration} to say that \(\norm{\mV^\intercal\vu}_2^2 = \sum_{i=1}^t \langle \vv^{(i)}, \vu\rangle^2 \leq t C_\tau^{-q}\) with probability at least \(1-tC_0^{-q}\).
So, we can bound
\begin{align*}
    D_{KL}(\vw_0\|\vw_1|\vu)
    &= \frac{\lambda^2}{2} \vu^\intercal\mV(\mV^\intercal\mV)^{-1}\mV^\intercal\vu \\
    &\leq \frac{\lambda^2}{2} \norm{\mV^\intercal\vu}_2^2 \norm{(\mV^\intercal\mV)^{-1}}_2 \\
    &\leq \frac{\lambda^2}{2} (t C_\tau^{-q}) \kappa^2
\end{align*}
where the last line uses that we can take the columns of \mV to be unit vectors without loss of generality, so that
\[
    \norm{(\mV^\intercal\mV)^{-1}}_2
    = \frac{1}{\sigma_{\min}(\mV)^2}
    \leq \frac{\sigma_{\max}(\mV)^2}{\sigma_{\min}(\mV)^2}
    \leq \kappa^2.
\]
By Pinsker's Inequality and the Neyman-Pearson Lemma \cite{csiszar2011information, neyman1933ix}, we know that \(\vw_0\) and \(\vw_1\) cannot be distinguished with probability \(\frac23\) so long as their KL divergence is at most \(O(1)\), which happens when \(t = O(\frac{C_\tau^q}{\kappa^2\lambda^2}) = O(\frac{C_\tau^q}{\kappa^2\eps})\).
Mixed with the requirement that our union bound earlier hold with high probability, we also require that \(t = O(C_0^q)\).
This yields our overall lower bound, showing that \(\vw_0\) and \(\vw_1\) cannot be distinguished when both \(t = O(C_0^q)\) and \(t = O(\frac{C_\tau^q}{\kappa^2\eps})\), completing the lower bound.

We note the above lower bound holds only against non-adaptive algorithms.
In \Cref{app:l2-estimation-formal-lower}, we adapt a proof strategy from \cite{simchowitz2017gap} to show that adaptivity cannot help much.
That proof is much more involved, and the fundamental intuitions unique to our method are well captured by the analysis above.
In brief, the proof against adaptive methods shows that at every point of time \(i\in[t]\), the algorithm does not suddenly learn new information about the direction of \vu, owing to \Cref{lem:kron-unit-vec-conentration}.
This analysis gives us the benefit of proving a lower bound against adaptive methods, but comes with the downside of having slightly worse rates, giving \(t = \Omega(\min\{C_0^{q/2}, \frac{C_\tau^{q/2}}{\kappa^2 \sqrt\eps}\}\) in the adaptive case as opposed to \(t = \Omega(\min\{C_0^{q}, \frac{C_\tau^{q}}{\kappa^2 \eps}\}\) in the non-adaptive one.



\subsection{Proof Sketch of Spectral Norm Estimation Lower Bound}

In this section, we outline the proof of \Cref{thm:top-eig-lower-bound}.
We follow the proof strategy in \cite{simchowitz2017gap} again here.
In \cite{simchowitz2017gap}, the authors show lower bounds against distinguishing between two matrices from matrix-vector products.
Specifically, they let \(\mG\in\bbR^{D \times D}\) be a matrix with iid \(\cN(0,1)\) entries and let \(\vu\in\bbR^{D}\) be a random unit vector in \(\bbR^D\).
They show that distinguishing between
\[
    \mA_0 = \frac{\mG+\mG^\intercal}{\sqrt{2D}}
    \hspace{0.5cm}\text{and}\hspace{0.5cm}
    \mA_1 = \frac{\mG+\mG^\intercal}{\sqrt{2D}} + \lambda \vu\vu^\intercal
\]
requires computing at least \(t=\Omega(\frac{\log(D)}{\log(\lambda)})\) classical (non-Kronecker) matrix-vector products.
We take \(D = n^q\).
We abstract out their analysis in \Cref{app:explain-simchowitz}, allowing us to pick a different distribution over unit vectors \vu and restricting the set of matrix-vector query vectors to be Kronecker-structured.
Fundamentally, by taking \(\vu\) to instead be the Kronecker product of iid unit vectors in \(\bbR^n\), we can against take advantage of \Cref{lem:kron-unit-vec-conentration} much like in trace estimation lower bound of \Cref{sec:trace-est-proof-sketch}.
Intuitively, we again have that the inner products between our query vectors and the planted random vector are exponentially small, and therefore at every time step \(i \in [t]\) of the algorithm, it is exceedingly unlikely that the matrix-vector algorithm suddenly goes from having small inner product with \vu to having a large inner product with \vu.

Formally, we prove the following distinguishing lower bound:
\begin{theorem}
    \label{thm:kron-matvec-testing-lower}
    Consider the problem using Kronecker matrix-vector products to test if \(\mA = \mA_0\) or \(\mA = \mA_1\) as shown above, where \(\vu = \vu_1 \otimes \cdots \otimes \vu_q\) for iid uniformly random unit vectors \(\vu_i \in \bbR^n\).
    Then, any \(\kappa-\)conditioned Kronecker matrix-vector algorithm needs at least \(t = \Omega(\min\{C_0^{q/2}, \frac{C_\tau^q}{\lambda^2\kappa^2}\})\) queries to correctly identify \(\mA\) with probability \(\frac{2}{3}\).
\end{theorem}
We prove \Cref{thm:kron-matvec-testing-lower} in \Cref{app:formal-kronmatvec-lower}.
The key payoff from this testing lower bound comes from comparing the spectral norms of \(\mA_0\) and \(\mA_1\).
The spectral norm of \(\mA_0\) is at most \(O(1)\) while the spectral norm of \(\mA_1\) is \(\Omega(\lambda)\) for large \(\lambda\).
In particular, if we take \(\lambda = C_\tau^{q/2}\) then we get the following lower bound:
\begin{corollary}
    \label{corol}
    There exists a number \(C>1\) such that any \(\kappa\)-conditioned Kronecker matrix-vector algorithm that can determine if \(\norm{\mA}_2 \leq 3\) or \(\norm{\mA}_2 \geq C^q\) with probability at least \(\frac23\) must use at least \(t = \Omega(C_0^{q/2}, \frac{C^{q}}{\kappa^2})\) queries.
\end{corollary}
This means that even computing an overwhelmingly coarse approximation to the spectral norm of a matrix must incur an exponential query complexity.
This corollary directly implies \Cref{thm:top-eig-lower-bound}.

