% Good

\section{Introduction}

Tensors have emerged as a canonical way to represent multi-modal or very high-dimensional datasets in areas ranging from quantum information science \cite{biamonte2019lectures} to medical imaging \cite{selvan2020tensor, sedighin2024tensor}.
Such applications often result in compact representations of tensors.
For instance, applications in quantum information theory use the so-called PEPS network or other compact tensor networks, while applications in partial differential equations often use tucker or tensor train decompositions. These applications overcome the curse of dimensionality by representing an underlying high dimensional linear operator as a network of a series of low dimensional tensors.
Abstractly, in these applications we are given an order \(2q\) tensor \(\tA \in (\bbR^{n})^{\otimes 2q}\) that represents a linear operator from \((\bbR^{n})^{\otimes q}\) to \((\bbR^n)^{\otimes q}\), and we often want to approximately compute some properties of this linear operator, such as its trace or spectral norm.

By appropriately reordering the entries of \tA, we can explicitly write down a matrix \(\mA \in \bbR^{n^q \times n^q}\) that describes this linear operator.
Our goal then becomes to estimate the trace, spectral sum, operator norm, or some other property of \mA.
However, since we may not know the structure of the underlying compact representation, we would like to estimate properties of \mA without explicitly forming \mA, as doing so would break our compact representation of \tA.
Instead we take advantage of our compact representation to efficiently and implicitly access \mA through linear measurements, such as the Kronecker matrix vector product:

\begin{definition}
	Let \(\mA\in\bbR^{n^q \times n^q}\).
	Then Kronecker Matrix-Vector Product Oracle is an oracle that, given \(\vx_1,\ldots,\vx_q\in\bbR^n\), returns \(\mA\vx\in\bbR^{n^q}\) where \(\vx = \otimes_{i=1}^q \vx_i\).
	Here, \(\otimes\) denotes the Kronecker product.
\end{definition}


For many different compact representations of \tA, it is possible to compute some compact representation of the Kronecker matrix-vector product \(\mA\vx\) efficiently \cite{lee2014fundamental,feldman2022entanglement}.
This is done in many algorithms and can go by different names, such as Khatri-Rao sketching or rank-one measurements.
However, these algorithms tend to make strong assumptions about the structure of \mA in order to achieve a polynomial runtime \cite{al2023randomized,li2017near,grasedyck2004existence} or obtain a worst-case runtime that is exponential in \(q\) \cite{meyer2023hutchinson,avron2014subspace,song2019relative}.
It has been unclear whether this exponential cost is unavoidable and what structure in \mA leads to this expensive runtime.
In this paper, we address this question by demonstrating explicit constructions of \mA that elicit these lower bounds.



Algorithms for fast tensor computations are well studied.
There is a large number of randomized algorithms that provide strong approximation guarantees to a tensor and are very efficient. 
Although not all in the Kronecker matrix-vector model, many such applications involve making linear measurements with a rank-one tensor, for which our techniques may apply.
Further, there is a body of work on lower bounds for tensor algorithms.
This work often focuses on complexity classes, for instance showing that computing the spectral norm of \tA is NP-Hard.
However, it is not clear how this relates to the number of Kronecker matrix-vector products it takes to estimate a property of \mA, which is the focus of our paper.
Relatively little research focuses on query complexity lower bounds for tensor computations.



In this paper, we leverage a novel observation about the orthogonality of random Kronecker-structured vectors in order to prove exponential lower bounds on the number of Kronecker matrix-vector products needed to approximately compute properties of \mA.
We show that any algorithm which can estimate the trace or spectral norm of \mA to even low accuracy must use a number of Kronecker matrix-vector products that is exponential in \(q\), modulo a mild assumption on the conditioning of the algorithm:
\begin{theorem}[Informal version of \Cref{thm:top-eig-lower-bound}]
    \label{thm:top-eig-lower-bound-informal}
    Any ``well-conditioned'' algorithm must compute \(t \geq C^q\) Kronecker matrix-vector products with \mA to return an estimate \(z \in (1\pm\frac12)\lambda_1(\mA)\) with probability at least \(\frac23\).
\end{theorem}
\begin{theorem}[Informal version of \Cref{thm:trace-lower-bound}]
    \label{thm:trace-lower-bound-informal}
    Any ``well-conditioned'' algorithm must compute \(t \geq C^q\) Kronecker vector-matrix-vector products with \mA to return an estimate \(z \in (1\pm\frac12)\tr(\mA)\) with probability at least \(\frac23\).
\end{theorem}
This explains why methods such as Kronecker JL and Kronecker Hutchinson require exponential sketching dimension as observed by several prior works \cite{meyer2023hutchinson, ahle2019almost}.
Phrased another way, this analysis explains why the Kronecker matrix-vector complexity of linear algebra problems is exponentially higher than the classical (non-Kronecker) matrix-vector complexity.

Our core orthogonality observation also implies another gap between Kronecker and non-Kronecker matrix-vector complexities.
We show that for the zero testing problem, there is an exponential gap between sketching with the Kronecker product of Gaussian vectors versus Rademacher vectors.
It suffices to using a single query with the Kronecker of Gaussian vectors to test if \mA is zero, but it takes \(\Theta(2^q)\) queries with Rademacher vectors.
\begin{theorem}[Special case of \Cref{thm:alphabet_lower_bounds}]
    For any Kronecker matrix-vector algorithm whose query vectors \(\vv = \otimes_{i=1}^q \vv_i\) are built from the Rademacher alphabet \(\vv_i \in \{\pm1\}^n\), it is necessary and sufficient to use \(\Theta(2^q)\) to test if \(\mA = \mat0\) or if \(\mA \neq \mat0\).
\end{theorem}
The difference between using ``small alphabets'' (e.g., Rademacher vectors) and ``large alphabets'' (e.g., Gaussian vectors) almost never asymptotically matters in the non-Kronecker case, where we expect that all algorithms that use subgaussian variables achieve the same asymptotic performance.
In contrast, we demonstrate that \emph{having subgaussianity does not suffice to understand the complexity of Kronecker matrix-vector algorithms}.
Analogously, we show that there can also be a gap between using complex-valued and real-valued queries, which again does not typically matter in the non-Kronecker case.
As a byproduct of our analysis, we prove that an algorithm of \cite{meyer2023hutchinson} has a near-optimal sample complexity for trace estimation.


Broadly, our analysis reveals new insights on the fundamental complexity of linear algebra in the Kronecker matrix-vector model.
We show that basic linear algebra tasks must incur an exponential sample complexity in the worst case.
So, if we wish to have faster algorithms then we need to assume that \mA has some structure that avoids the worst-case structure imposed by these lower bounds.
Further, we show that when designing randomized algorithms for the Kronecker matrix-vector model, it is important to examine our base random variables more closely that we may in the non-Kronecker case, as the choice of two similar variables like Rademachers and Gaussians may incur an additional exponential cost.

The rest of the paper is structured as follows:
we first discuss related work.
In \Cref{sec:prelims} we introduce notation.
In \Cref{sec:overview} we explain our theorem statements in more detail.
In \Cref{sec:conditioned} we prove our lower bounds on trace estimation and spectral norm approximation against all Kronecker matrix-vector algorithms.
In \Cref{sec:zero-testing} we prove our lower bounds against small alphabet algorithms for the zero testing problem.

\subsection{Related Work}

Tensors have a long history of study in the sketching literature, particularly for the problem of $\ell_2$ norm estimation \cite{ahle2020oblivious, ahle2019almost, pham2013fast}.
Previously \cite{ahle2019almost} observed that typical Kronecker-structured $\ell_2$ embeddings cannot work with fewer than exponential measurements in the number of modes.
This is why \cite{ahle2019almost} require a more complicated sketch to construct their embeddings for high-mode tensors.
However it does not appear to be known whether a  Kronecker-structured sketch could work for $\ell_2$ estimation, using a subexponential number of measurements, if one drops the requirement that the sketch be an embedding.
Our work partially resolves this by showing that any such sketching matrix must be extremely poorly conditioned.
There is also a large body of work on sketching Tucker, tensor train, tree networks, and general tensor networks, see, e.g., \cite{grasedyck2013literature,SWZ19,MWZ24} and the references therein.
Additionally, there is a large body of work on algorithms that (perhaps impicitly) operate in the Kroncker matrix-vector model \cite{bujanovic2021norm,bujanovic2024subspace,feldman2022entanglement,ahle2019almost,meyer2023hutchinson,avron2014subspace}.

