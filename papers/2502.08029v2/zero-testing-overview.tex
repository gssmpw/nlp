% Good


\subsection{Zero Testing}

Consider the following very simple problem:

\begin{problem}
	\label{prob:zero-testing}
	Let \(\mA\in\bbR^{n^q \times n^q}\) be a matrix.
	Using only matrix-vector products with \mA, decide if \(\mA = \mat0\) or if \(\mA \neq \mat0\).
	Be correct with probability at least \(\frac23\).
\end{problem}

When we are allowed to use classical (non-Kronecker) matrix-vector products, then we can solve \Cref{prob:zero-testing} with a single Gaussian query with probability 1.
The same holds in the Kronecker matrix-vector case:
if we let \(\vv = \vg_1 \otimes \cdots \otimes \vg_q\) where \(\vg_i \sim \cN(\vec0,\mI)\) then \(\mA\vv \neq \vec0\) with probability one if \(\mA \neq 0\).
This is a direct consequence of the kernel of any nonzero matrix being a set of measure zero.

This low query complexity remains true in the classical (non-Kronecker) case if we restrict ourselves to use Rademacher vectors.
More formally, if we only allow ourselves to compute \(\mA\vx\) for vectors \(\vx\in\{\pm1\}^{n^q}\), then using \(O(1)\) matrix-vector products suffices to solve \Cref{prob:zero-testing}.
This follows from many possible results, including applying Hutchinson's estimator to \(\mA^\intercal\mA\) to estimate \(\norm{\mA}_F^2\) to constant factor accuracy with \(O(1)\) queries \cite{meyer2023hutchinson}.

This story is ubiquitous in matrix-vector complexity -- changing the base distribution we sample with from any subgaussian distribution to any other subgaussian distribution (e.g. from Gaussian to Rademacher) does not change this asymptotic complexity of solving linear algebra problems \cite{saibaba2025randomized,meyer21hutchpp}.

We now show that this story fails to hold true in the Kronecker matrix-vector setting:
\begin{theorem}
	\label{thm:zero-testing-rademacher}
	Consider any Kronecker matrix-vector algorithm that only computes product using query vectors of the form \(\vv = \vv_1 \otimes \cdots \otimes \vv_q\) where \(\vv_i\in\{\pm1\}^n\).
	Then, this algorithm needs \(\Theta(2^q)\) queries to solve \Cref{prob:zero-testing}.
\end{theorem}
Not only does building queries with the Kronecker product of Rademacher entries not suffice, but no algorithm that uses \(\{\pm1\}\) entries can efficiently test if a matrix is zero.
This steeply violates the story we would expect to hold from the classical (non-Kronecker) case.
We prove a generalization of this results in \Cref{thm:alphabet_lower_bounds} of \Cref{sec:zero-testing}, where we only allow the entries of vectors to belong to a fixed alphabet \(\alphabet\subset\bbR\).
For large enough \(n\), we show that \((1 - \Theta(\frac{1}{|\alphabet|}))^q\) queries are necessary and sufficient to solve the zero-testing problem.
In other words, we must pay a cost that is exponential in \(q\) unless \(|\alphabet| = \Omega(q)\).
Broadly this tell us the following:

\begin{center}
	\emph{Knowing that a random vector is subgaussian does not suffice to tightly bound the query complexity of the Kronecker matrix-vector algorithm using that variable.}
\end{center}

We also note that any algorithm that can estimate the trace of a PSD matrix to relative error \(O(1)\) can be used to solve \Cref{prob:zero-testing}.
In particular, it is worth comparing \Cref{thm:zero-testing-rademacher} to Table 1 from \cite{meyer2023hutchinson}.
\cite{meyer2023hutchinson} show an algorithm that uses the Kronecker product of Rademacher vectors to estimate the trace of a PSD matrix to constant factor error using \(O(2^q)\) queries when \(n=2\).
They also show that the same algorithm run with uniformly random unit vectors instead of Rademacher vectors achieves the same result in just \(O(1.5^q)\) queries.

We can therefore conclude from \Cref{thm:zero-testing-rademacher} that the optimal query complexity of any algorithm that solves trace estimation while using the \(\{\pm1\}\) alphabet is therefore \(\Theta(2^q)\) when \(n=2\).
Since we know that \(O(1.5^q)\) is possible with continuous variables, we prove for the first time that the task of \emph{trace estimation cannot be solved with optimal query complexity using Rademacher vectors}.

Again, this reinforces how the choice of base subgaussian distribution can exponentially change our final sample complexity.
The core of the proofs here also rely on orthogonality.
We show that with overwhelming probability, random Kronecker-structured vectors built from a small alphabet are almost surely perfectly orthogonal:
\begin{lemma}
	There exists a distribution over random vectors \(\vu\in\bbR^{n^q}\) such that every fixed vector \(\vv = \vv_1 \otimes \cdots \otimes \vv_q\) with \(\vv_i \in \{\pm1\}^n\) has \(\langle \vu, \vv \rangle=0\) with probability at least \(1-\frac1{2^q}\).
\end{lemma}
Again, we prove this result in broader generality in \Cref{sec:zero-testing}, with respect to an arbitrary alphabet.

We also take a moment to reflect further on another observation from \cite{meyer2023hutchinson}: the complex Kronecker matrix-vector oracle is different from the real Kronecker matrix-vector oracle.
That is, if we allow \(\vv = \otimes_{i=1}^q \vv_i\) where \(\vv_i \in \bbC^n\), this model is more expressive than the real-valued Kronecker matrix-vector model.
In particular, it takes up to \(2^q\) real-valued Kronecker matrix-vector products to simulate computing a single complex Kronecker matrix-vector product.
We also analyze the complex case for the zero-testing problem, and show that zero testing with the \(\{\pm1,\pm i\}\) alphabet requires \(\Omega(1.25^q)\) queries, establishing that this is easier than the zero testing in the real \(\{\pm1\}\) alphabet.
However, this difference in base of exponent between \(2^q\) and \(1.25^q\) may also be attributed to the difference in the size of the alphabet, and so it remains unclear how to make an apples-to-apples comparison of the real and complex models and show that the complex model is fundamentally more query efficient.
