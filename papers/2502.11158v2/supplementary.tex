\subsection{Broader Impacts}
This paper exploited image synthesis with text-to-image models. Because of their impressive generative abilities, these models may produce misinformation or fake images. 
So we sincerely remind users to pay attention to it.
Besides, privacy and consent also become important considerations, as generative models are often trained on large-scale data.
Furthermore, generative models may perpetuate biases present in the training data, leading to unfair outcomes. 
Therefore, we recommend users be responsible and inclusive while using these text-to-image generative models.
Note that our method only focuses on technical aspects. All pre-trained models used in this paper are all open-released.

\subsection{Supplementary Preliminaries}
\label{sec:data_processing}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1.0\linewidth]{pics/supp_data_process.pdf}
\vspace{-0.15in}
\end{center}
   \caption{ The illustration of matching-based masking for Ref-inpainting task.
   \label{fig:data_process}}
\vspace{-0.15in}
\end{figure}

\label{sec:preliminary_anyrefill}

\noindent\textbf{Preliminaries of FLUX: Rectified Flow (RF)~\cite{liu2022flow}.}
Generative models aim to learn a mapping from a noise distribution $p_1$ to a data distribution $p_0$, where $p_0$ typically represents real-world data such as images or videos, and $p_1$ is often chosen as a standard Gaussian distribution. RF defines a straightforward approach to bridge these two distributions by constructing a straight trajectory in the latent space. This is achieved by modeling a time-dependent flow governed by  Ordinary Differential Equation (ODE). 

\begin{equation}
    {dZ_t} = v_\theta(Z_t, t){dt}, \quad t \in [0, 1]
\end{equation}
where $Z_t \in p_t$ represents the intermediate distribution at time $t$ and the velocity field $v(Z_t, t)$ is parameterized by a neural network $v_\theta$.
The forward process in rectified flow linearly interpolates between real data $X_0 \sim p_0$ and Gaussian noise $X_1 \sim p_1$. At each timestep $t$, the interpolated sample is defined as:
\begin{equation}
    X_t = (1 - t) X_0 + t X_1.
\end{equation}
This simple linear combination ensures that the data progressively transitions from  $X_0$ at $t=0$ to $X_1$ at $t=1$. The differential form of this interpolation is given by $dX_t = (X_1 - X_0) dt$.
To learn the velocity field, the network $v_\theta$ is trained to approximate the velocity between $X_0$ and $X_1$ along the interpolated path:
\begin{equation}
    \mathcal{L} = \left[ \| (X_1 - X_0) - v_\theta(X_t, t, c) \|^2 \right] dt,
\end{equation}
where $c$ is a text prompt condition in T2I flow-based models.
The sampling process in RF involves solving the ODE in reverse, starting from a Gaussian noise sample  $Z_N \sim \mathcal{N}(0, I)$. A sequence of timesteps $\{t_N, \ldots, t_0\}$ is defined to iteratively generate real data distribution samples:
\begin{equation} 
    Z_{t_{i-1}} = Z_{t_i} + (t_{i-1} - t_i) v_\theta(Z_{t_i}, t_i, c),
\end{equation}
where $i$ runs from $N$ to 0. 
\IEEEpubidadjcol

\label{sec:matching_mask}

\noindent\textbf{Data Processing for Ref-inpainting: Matching-based Masking.}
We follow LeftRefill~\cite{cao2024leftrefill} to conduct matching-based masking.
For the Ref-inpainting, we find that the widely used irregular mask~\cite{dong2022incremental,zhou2021transfill,zhao2022geofill} fails to reliably evaluate the capability of spatial transformation and structural preserving. Therefore, as shown in Fig.~\ref{fig:data_process}, we propose the matching-based masking method.
Specifically, we first utilize the scene info provided by MegaDepth~\cite{li2018megadepth} to select out the image pairs which have an overlap rate between 40\% and 70\% Second, for each image pair, we use a feature matching model~\cite{tang2022quadtree} to detect matching key-points between the images and assign each key-points pair a confidence score. 
Next, we filter out those pairs with low confidence scores with the threshold of 0.8. Then we randomly crop a 20\% to 50\% sub-space in the matched region and sample 15 to 30 key points as vertices to be painted across for the final masks. 
The matching-based mask not only improves the reliability during the evaluation but also facilitates the performance.

We split 505 pairs from MegaDepth~\cite{li2018megadepth} as the validation, including some manual masks from ETH3D scenes~\cite{schops2017multi}.



\subsection{Supplemental Experimental Results}
\begin{figure*}[htb!]
\begin{center}
\includegraphics[width=1.0\linewidth]{pics/supp_generation_results.pdf}
% \vspace{-0.15in}
\end{center}
   \caption{ The illustration of conditional generation tasks results, including canny-to-image, depth-to-image, and segment-to-image.
   \label{fig:supp_more_generation}}
\vspace{-0.2in}
\end{figure*}
\noindent\textbf{More Conditional Generation Results.} We show more impressive results of AnyRefill in Fig.~\ref{fig:supp_more_generation}. We selected the model output of ControlNet before its collapse as the comparison result. As shown in the figure, AnyRefill generates more realistic and reference-aligned images. While ControlNet also demonstrates decent alignment capabilities, IP-Adapter performs poorly due to data limitations, producing results that are entirely misaligned with the reference.

\noindent\textbf{More Editing Results.}
\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{pics/supp_editing.pdf}
% \vspace{-0.15in}
\end{center}
   \caption{ The illustration of image editing tasks, including gender editing, age editing, and relighting.
   \label{fig:supp_more_editing}}
\vspace{-0.2in}
\end{figure*}
We show more image editing results in Fig.~\ref{fig:supp_more_editing} to verify that AnyRefill is a unified framework across various challenging tasks. 
AnyRefill seamlessly switches between different task-specific LoRAs, unifying highly challenging image editing tasks such as age editing, gender editing, and relighting under a single architecture, as shown in Fig.~\ref{fig:supp_more_editing}. In these qualitative results, AnyRefill preserves the foreground to the greatest extent while adjusting editing attributes based on textual input, achieving impressive outcomes.

\noindent\textbf{More Perception Results.}
\begin{figure*}[htb!]
\begin{center}
\includegraphics[width=1.0\linewidth]{pics/supp_perception_results.pdf}
% \vspace{-0.15in}
\end{center}
   \caption{ The illustration of perception tasks, including image-to-canny, image-to depth, and image-to- segment.
   \label{fig:supp_more_perception}}
\vspace{-0.2in}
\end{figure*}
We show more visual perception results of AnyRefill in Fig.~\ref{fig:supp_more_perception}. AnyRefill can perform Image-to-Canny, Image-to-Depth, and Image-to-Segment tasks without requiring any modifications to the model architecture. As shown in the figure, AnyRefill demonstrates precise spatial information extraction, validating its potential to extend to a wider range of perception tasks.

\subsection{Inference Speed}
\begin{table}
\small 
\caption{Inference speed of AnyRefill under 50 RF sampling steps.
\label{tab:infer_speed}}
\vspace{-0.1in}
\centering
\begin{tabular}{cc}
\hline
Input size & Sec/image \tabularnewline
\hline
512$\times$512 & $\sim$ 5.26  \tabularnewline
\hline
512$\times$1024 & $\sim$ 8.29 \tabularnewline
\hline
512$\times$1024 (w/ LoRA) & $\sim$ 9.13 \tabularnewline
\hline
\end{tabular}
\end{table}
In this paper, our propose AnyRefill is based on large T2I model FLUX (12B).
To investigate the relationship between input size and inference cost, we provide the inference speed for different input resolutions in same codebase, shown in Tab.~\ref{tab:infer_speed}. 
All tests are based on 50 RF sampling steps.
LeftRefill needs to stitch two images together, which would double the input size. But the inference time only increases 3.03s, from 5.26s to 8.29s, as shown in Tab.~\ref{tab:infer_speed}. Meanwhile, incorporating task-specific LoRA into FLUX.Fill increases the modelâ€™s inference time by only 0.84 seconds, from 8.29s to 9.13s, which remains within an controllable range.
Therefore, we think the proposed LPG fomulation's inference cost is still acceptable in most real-world applications.


\subsection{Limitation}
\label{sec:limitation}
Although the proposed AnyRefill with LPG pattern enjoys good performance and reference alignment in various vision tasks, investigating the efficiency of multi-view generation for AnyRefill, which is based on large-scale T2I models like FLUX, can be regarded as interesting future work of LPG paradigm.
Moreover, The relationship between the amount of training data and model performance under the LPG paradigm is also a highly valuable direction for exploration.



