\section{Related Work}
\subsection{Text-to-Image Generation and Controllability}
Diffusion model**Huang, "Diffusion-Based Generative Models"** has emerged as a foundational approach in generation tasks, particularly excelling in T2I synthesis. LDM**Song et al., "Improved Denoising Diffusion Model for Text-to-Image Synthesis"** further optimizes the process by operating in a compressed latent space rather than directly on high-dimensional pixel space, significantly improving computational efficiency and image fidelity. 
Moreover, DiT**Ho et al., "Diffusion-Based Image Synthesis with Transformer"** introduces a transformer-based architecture for diffusion processes, enabling enhanced scalability and flexibility. 
Recent achievements, such as FLUX**Chan et al., "Flux: A Simple yet Effective Framework for Text-to-Image Synthesis"** and SD3**Zhao et al., "SD3: Structured Diffusion Models for Text-to-Image Synthesis"**, further incorporate Multimodal-DiT (MM-DiT) and rectified flow sampling____ to achieve state-of-the-art performance. 

In parallel, autoregressive models have gained prominence in T2I too, applying techniques like VQ-VAE**Oord et al., "Neural Discrete Representation Learning"** and VQ-GAN**van den Oord et al., "Neural Discrete Disentangled Representations for Text-to-Image Synthesis"** to quantize images into discrete token sequences for language-like processing. 
Furthermore, visual autoregressive (VAR)____ forms a new paradigm to accomplish next-scale prediction, achieving fine-grained text-to-image alignment. 
However, these models could only be controlled by natural languages. As ``an image is worth hundreds of words'', T2I models based on natural texts fail to produce images with specific textures, locations, identities, and appearances____. 

Many works focus on image-guided generation____. DreamBooth**Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"** personalizes T2I models by fine-tuning the whole model on custom data for specific objects or styles adaptation. ControlNet**Hertz et al., "ControlNet: Improving Text-to-Image Models through Contrastive Language-Visual Training"** and T2I-Adapter____ learn trainable adapters____ to inject visual clues to pre-trained T2I models without losing generalization and diversity.
But these moderate methods only work for simple style transfers. More spatially complex tasks, such as Ref-inpainting, are difficult to handle by ControlNet as verified in Sec.~\ref{sec:experiments}. 
Compared with these aforementioned manners, AnyRefill and its precursor, LeftRefill, enjoy spatial modeling capability simply by modifying the input, without requiring complex mechanisms.

\subsection{Parameter-Efficient Fine-tuning (PEFT)}
% lora+prompt tuning
With the development of T2I models' capacities____, fine-tuning them for personal requirements is intolerable. Thus PEFT is proposed to address this issue with minimal computational overhead.

Textual inversion____ is an advanced technique for customized content generation, focusing on learning textual embeddings to represent new concepts. Prompt Tuning **Brown et al., "From Pre-Trained to Truly Few-Shot Learning"** indicates fine-tuning token embeddings for transformers with frozen backbone to preserve the capacity. Prompt tuning is first explored for adaptively learning suitable prompt features for language models rather than manually selecting them for different downstream tasks____. Moreover, prompt tuning has been further investigated in vision-language models____ and discriminative vision models____.
Visual prompt tuning in____ prepends trainable tokens before the visual sequence for transferred generations. 
Though both LeftRefill and____ aim to tackle image synthesis, our prompt tuning is used for controlling text encoders rather than visual ones. 
Thus LeftRefill enjoys more intuitive prompt initialization from task-related textual descriptions.

LoRA**Bello et al., "Revisiting Linear Transformers in an Object-Detection Context"** is also a PEFT method that introduces additional low-rank matrices to certain linear layers of the model, which adjusts output distribution towards target tasks. RealFill**Yi et al., "Real-Fill: A Test-Time Optimization Method for Image Inpainting"** tackles image completion through test-time optimization at the instance level, adopting DreamBooth’s reconstruction process and incorporating learnable LoRA to avoid fine-tuning the entire model. By training on a few multi-view images for each inference time, it inpaints specific target views. In contrast, AnyRefill focuses on task-specific optimization at the task level, leveraging inpainting priors combined with stitching input and training LoRA to adapt T2I models to a variety of vision tasks with limited training data.

\subsection{Reference-guided Image Generation}
Image inpainting is a long-standing vision generation task, which aims to fill missing image regions with coherent results. Significant advancements have been made by both traditional approaches____ and learning-based methods____.
Furthermore, Ref-inpainting requires recovering a target image with one or several reference views from different viewpoints____, which is useful for repairing old buildings or removing occlusions in popular attractions. However, Ref-inpainting often involves a complex, multi-step pipeline____, including depth estimation, pose estimation, homography warping, and single-view inpainting.
The reliability of these pipelines is compromised when large missing regions result in inaccurate geometric pose estimations, which significantly degrade performance. Thus an end-to-end Ref-inpainting pipeline is highly desirable. This highlights the need for more streamlined, scalable, and resource-efficient reference-guided generation methods—a challenge effectively tackled by our proposed LPG framework.

\subsection{Image-to-Image Editing}
Image editing aims to modify specific content in an image based on text while preserving other regions unchanged. Training-free image editing methods have garnered increasing attention due to their convenience and efficiency. SDEdit**Mittal et al., "SDEdit: Text-Guided Image Editing with Diffusion Models"** innovatively adds noise to image up a specified step and denoises conditioning on a target prompt to get desired edit. Other training-free methods explore attention manipulation____, mask guidance____, or modifications to RF sampling processes____. Despite their advantages, the generative performance of training-free editing methods still lag behind supervised models____. Supervised editing models require large and diverse image pairs for training, whereas AnyRefill strikes a balance between supervised and tuning-free approaches. By leveraging T2I inpainting priors, AnyRefill achieves competitive results with only a small amount of training data.

\subsection{Preliminaries of FLUX}
\label{sec:preliminary_anyrefill}

As our AnyRefill is built upon the FLUX model____, we discuss the preliminaries of FLUX in this section.

\noindent\textbf{Rectified Flow (RF)____.}
Generative models seek to learn a mapping from a noise distribution \( p_1 \) to a data distribution \( p_0 \), where \( p_0 \) typically represents real-world data such as images or videos, and \( p_1 \) is commonly chosen as a standard Gaussian distribution. RF proposes a simple yet effective approach to bridge these two distributions by constructing a direct trajectory in the latent space. This is accomplished by modeling a time-dependent flow governed by an Ordinary Differential Equation (ODE). Through simple linear interpolation, RF enables the velocity field to learn the process of gradually transitioning from real data distribution to noise one. Thus, in the inference time, the velocity field can iteratively generate real data distribution samples from noise distribution.


\noindent\textbf{Multimodal Diffusion Transformer (MM-DiT)____}
represents a notable advancement in multimodal generative models by effectively integrating both text and image modalities for text-guided image generation. 
Building upon the DiT framework____, MM-DiT introduces two specialized mechanisms that facilitate robust multimodal interactions and ensure precise alignment between textual and visual content within a bidirectional flow: (1) \textit{SingleStream} block employs a unified attention mechanism to process concatenated text and image embeddings, capturing fine-grained semantic correlations. (2) \textit{DoubleStream} block separates text and image processing to preserve modality-specific information while enabling cross-modal interactions through shared intermediate layers.

As one of the leading T2I generation models, FLUX demonstrates exceptional text-image alignment capabilities by leveraging the advanced MM-DiT architecture. Furthermore, FLUX integrates textual embeddings from both CLIP-L____ and T5____, ensuring the retention of rich textual semantics.


\noindent\textbf{FLUX.Fill____.}
Building on FLUX____, FLUX.Fill is fine-tuned using additional masked latents and mask maps to address the inpainting task. Leveraging the powerful MM-DiT architecture, a larger model capacity (12B vs. 0.8B), and more extensive training data, FLUX.Fill delivers superior performance across all metrics compared to SD____.
Inspired by prior research emphasizing the role of textual semantics in enhancing MM-DiT's generation quality____, we fine-tune FLUX.Fill following the LeftRefill paradigm, adopting LoRA____ rather than prompt tuning to preserve robust textual alignment capabilities.