\section{Related Work}
\subsection{Text-to-Image Generation and Controllability}
Diffusion model~\cite{sohl2015deep,ho2020denoising} has emerged as a foundational approach in generation tasks, particularly excelling in T2I synthesis. LDM~\cite{rombach2022high} further optimizes the process by operating in a compressed latent space rather than directly on high-dimensional pixel space, significantly improving computational efficiency and image fidelity. 
Moreover, DiT~\cite{peebles2023scalable} introduces a transformer-based architecture for diffusion processes, enabling enhanced scalability and flexibility. 
Recent achievements, such as FLUX~\cite{flux2024} and SD3~\cite{esser2024scaling}, further incorporate Multimodal-DiT (MM-DiT) and rectified flow sampling~\cite{liu2022flow} to achieve state-of-the-art performance. 

In parallel, autoregressive models have gained prominence in T2I too, applying techniques like VQ-VAE~\cite{van2017neural} and VQ-GAN~\cite{esser2021taming} to quantize images into discrete token sequences for language-like processing. 
Furthermore, visual autoregressive (VAR)~\cite{tian2024visual} forms a new paradigm to accomplish next-scale prediction, achieving fine-grained text-to-image alignment. 
However, these models could only be controlled by natural languages. As ``an image is worth hundreds of words'', T2I models based on natural texts fail to produce images with specific textures, locations, identities, and appearances~\cite{gal2022image}. 

Many works focus on image-guided generation~\cite{voynov2022sketch,li2023gligen,ma2023unified}. DreamBooth~\cite{ruiz2022dreambooth} personalizes T2I models by fine-tuning the whole model on custom data for specific objects or styles adaptation. ControlNet~\cite{zhang2023adding} and T2I-Adapter~\cite{mou2023t2i} learn trainable adapters~\cite{houlsby2019parameter} to inject visual clues to pre-trained T2I models without losing generalization and diversity.
But these moderate methods only work for simple style transfers. More spatially complex tasks, such as Ref-inpainting, are difficult to handle by ControlNet as verified in Sec.~\ref{sec:experiments}. 
Compared with these aforementioned manners, AnyRefill and its precursor, LeftRefill, enjoy spatial modeling capability simply by modifying the input, without requiring complex mechanisms.

\subsection{Parameter-Efficient Fine-tuning (PEFT)}
% lora+prompt tuning
With the development of T2I models' capacities~\cite{podell2024sdxl,esser2024scaling,flux2024}, fine-tuning them for personal requirements is intolerable. Thus PEFT is proposed to address this issue with minimal computational overhead.

Textual inversion~\cite{gal2022image,mokady2022null} is an advanced technique for customized content generation, focusing on learning textual embeddings to represent new concepts. Prompt Tuning ~\cite{lester2021power,liu2021gpt,liu2021p} indicates fine-tuning token embeddings for transformers with frozen backbone to preserve the capacity. Prompt tuning is first explored for adaptively learning suitable prompt features for language models rather than manually selecting them for different downstream tasks~\cite{liu2023pre}. Moreover, prompt tuning has been further investigated in vision-language models~\cite{radford2021learning,ge2022domain} and discriminative vision models~\cite{jia2022visual,liao2023rethinking}.
Visual prompt tuning in~\cite{sohn2022visual} prepends trainable tokens before the visual sequence for transferred generations. 
Though both LeftRefill and~\cite{sohn2022visual} aim to tackle image synthesis, our prompt tuning is used for controlling text encoders rather than visual ones. 
Thus LeftRefill enjoys more intuitive prompt initialization from task-related textual descriptions.

LoRA~\cite{hu2021lora} is also a PEFT method that introduces additional low-rank matrices to certain linear layers of the model, which adjusts output distribution towards target tasks. RealFill~\cite{tang2024realfill} tackles image completion through test-time optimization at the instance level, adopting DreamBooth’s reconstruction process and incorporating learnable LoRA to avoid fine-tuning the entire model. By training on a few multi-view images for each inference time, it inpaints specific target views. In contrast, AnyRefill focuses on task-specific optimization at the task level, leveraging inpainting priors combined with stitching input and training LoRA to adapt T2I models to a variety of vision tasks with limited training data.

\subsection{Reference-guided Image Generation}
Image inpainting is a long-standing vision generation task, which aims to fill missing image regions with coherent results. Significant advancements have been made by both traditional approaches~\cite{bertalmio2000image,criminisi2003object,hays2007scene} and learning-based methods~\cite{zeng2020high,zhao2021large,li2022mat,suvorov2022resolution,dong2022incremental}.
Furthermore, Ref-inpainting requires recovering a target image with one or several reference views from different viewpoints~\cite{tang2024realfill,oh2019onion}, which is useful for repairing old buildings or removing occlusions in popular attractions. However, Ref-inpainting often involves a complex, multi-step pipeline~\cite{zhou2021transfill,zhao2022geofill,zhao20223dfill}, including depth estimation, pose estimation, homography warping, and single-view inpainting.
The reliability of these pipelines is compromised when large missing regions result in inaccurate geometric pose estimations, which significantly degrade performance. Thus an end-to-end Ref-inpainting pipeline is highly desirable. This highlights the need for more streamlined, scalable, and resource-efficient reference-guided generation methods—a challenge effectively tackled by our proposed LPG framework.

\subsection{Image-to-Image Editing}
Image editing aims to modify specific content in an image based on text while preserving other regions unchanged. Training-free image editing methods have garnered increasing attention due to their convenience and efficiency. SDEdit~\cite{meng2021sdedit} innovatively adds noise to image up a specified step and denoises conditioning on a target prompt to get desired edit. Other training-free methods explore attention manipulation~\cite{hertz2022prompt,mokady2023null,parmar2023zero,dalva2024fluxspace}, mask guidance~\cite{avrahami2022blended,couairon2022diffedit,huang2023region,li2024zone}, or modifications to RF sampling processes~\cite{rout2024semantic,wang2024taming,kulikov2024flowedit}. Despite their advantages, the generative performance of training-free editing methods still lag behind supervised models~\cite{brooks2023instructpix2pix,sheynin2024emu,zhang2024hive,zhang2024magicbrush,shi2024seededit}. Supervised editing models require large and diverse image pairs for training, whereas AnyRefill strikes a balance between supervised and tuning-free approaches. By leveraging T2I inpainting priors, AnyRefill achieves competitive results with only a small amount of training data.

\subsection{Preliminaries of FLUX}
\label{sec:preliminary_anyrefill}

As our AnyRefill is built upon the FLUX model~\cite{flux2024}, we discuss the preliminaries of FLUX in this section.

\noindent\textbf{Rectified Flow (RF)~\cite{liu2022flow}.}
Generative models seek to learn a mapping from a noise distribution \( p_1 \) to a data distribution \( p_0 \), where \( p_0 \) typically represents real-world data such as images or videos, and \( p_1 \) is commonly chosen as a standard Gaussian distribution. RF proposes a simple yet effective approach to bridge these two distributions by constructing a direct trajectory in the latent space. This is accomplished by modeling a time-dependent flow governed by an Ordinary Differential Equation (ODE). Through simple linear interpolation, RF enables the velocity field to learn the process of gradually transitioning from real data distribution to noise one. Thus, in the inference time, the velocity field can iteratively generate real data distribution samples from noise distribution.


\noindent\textbf{Multimodal Diffusion Transformer (MM-DiT)~\cite{esser2024scaling}}
represents a notable advancement in multimodal generative models by effectively integrating both text and image modalities for text-guided image generation. 
Building upon the DiT framework~\cite{peebles2023scalable}, MM-DiT introduces two specialized mechanisms that facilitate robust multimodal interactions and ensure precise alignment between textual and visual content within a bidirectional flow: (1) \textit{SingleStream} block employs a unified attention mechanism to process concatenated text and image embeddings, capturing fine-grained semantic correlations. (2) \textit{DoubleStream} block separates text and image processing to preserve modality-specific information while enabling cross-modal interactions through shared intermediate layers.

As one of the leading T2I generation models, FLUX demonstrates exceptional text-image alignment capabilities by leveraging the advanced MM-DiT architecture. Furthermore, FLUX integrates textual embeddings from both CLIP-L~\cite{radford2021learning} and T5~\cite{raffel2020exploring}, ensuring the retention of rich textual semantics.


\noindent\textbf{FLUX.Fill~\cite{fluxfill2024}.}
Building on FLUX~\cite{flux2024}, FLUX.Fill is fine-tuned using additional masked latents and mask maps to address the inpainting task. Leveraging the powerful MM-DiT architecture, a larger model capacity (12B vs. 0.8B), and more extensive training data, FLUX.Fill delivers superior performance across all metrics compared to SD~\cite{rombach2022high}.
Inspired by prior research emphasizing the role of textual semantics in enhancing MM-DiT's generation quality~\cite{esser2024scaling,ramesh2021zero,saharia2022photorealistic}, we fine-tune FLUX.Fill following the LeftRefill paradigm, adopting LoRA~\cite{hu2021lora} rather than prompt tuning to preserve robust textual alignment capabilities.