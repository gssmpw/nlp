In this work we present \ACRO, a new way to train generalist robot policies from human video data. \ACRO learns to predict \textit{reasoning}, which can be labeled on both robot and human video data. We find that \ACRO enables VLAs to cross the embodiment gap, and to learn tasks represented in only human video data. Models trained with \ACRO are also able to generalize to completely unseen tasks (not present in either robot or human data). Finally, we find \ACRO responds positively to data from out-of-domain environments, enabling models to learn new tasks from environments completely separate from the target domain. These results demonstrate that \ACRO is a promising step towards training generalist robot policies, laying the groundwork for models that can leverage both robot data and large-scale human video data.

\smallskip \noindent \textbf{Limitations and Future Work}:
%No internet scale data (promise of it versus actual practicality)
%Assumptions about hand and how weâ€™re collecting the data
%Limited by HaMeR
%All pick / place (no rotations)
Our work demonstrates the promise of using human video data to improve generalization in robot policies; however, there are key challenges to address before scaling up the method to tap into larger and noisier datasets of human videos, such as those found on the Internet. For example, in the human video dataset we collected for this work, we limit the degrees-of-freedom that are expressed by the human hand to motions along Cartesian axes. As human hand pose estimation methods become more accurate, we anticipate that this limitation will be partially mitigated and allow us to leverage more natural videos of  human hands, as well as to expand the set of language motions in our labeling pipeline. Additionally, we scope our work to focus our study of generalization on pick-and-place tasks with rigid objects, characteristic of the tasks in prior work on reasoning-based imitation learning \cite{zawalski2024robotic}. Expanding the set of tasks to include more fine-grained and dexterous manipulation provides a rich area for future work.
% Our work demonstrates the promise of using human video data to improve generalization in robot policies; however, there are key challenges to address before scaling up the method to tap into larger and noisier datasets of human videos, such as those found on the Internet. For example, in the human video dataset we collected for this work, we limit the degrees-of-freedom that are expressed by the human hand to motions along Cartesian axes. As human hand pose estimation methods become more accurate, we anticipate that this limitation will be partially mitigated and allow us to leverage more natural videos of  human hands, as well as to expand the set of language motions in our labeling pipeline. Additionally, we scope our work to focus our study of generalization on pick-and-place tasks with rigid objects, characteristic of the tasks in Bridge V2 \cite{walke2023bridgedata} and prior work on reasoning-based imitation learning \cite{zawalski2024robotic}. Expanding the set of tasks to include more fine-grained and dexterous manipulation provides a rich area for future work.