
In this section, we will first describe our problem setting and lay out our assumptions, and then we will outline our method for learning from action-free data using language reasoning chains.
As an overview, \ACRO involves two major steps. First, annotate action-free data with language reasoning (\cref{sec:method:labeling}). Second, train a reasoning-based policy on a combination of robot demonstration data with both actions and reasoning chains and action-free data with only reasoning chains (\cref{sec:method:training}).

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/main_5.png}
    \caption{\ACRO generates reasonings on both human and robot data using a suite of pretrained models. Scene descriptors and object bounding boxes for both human and robot data are generated using Prismatic VLM and Grounding DINO. While SAM and proprioception can be used to generate movement primitives for robot data, \ACRO relies on HaMeR to track human hand data for primitive generation. For both data types, the scene descriptions, bounding boxes, and movement primitives (as well as actions for robot data) are synthesized by Gemini into reasoning data in natural language. These reasonings are tokenized and fed into a mixed dataset containing both human and robot data for co-finetuning.}
    \label{fig:labeling}
\end{figure*}

\subsection{Problem: Learning Reasoning in Action-free Data}

In multi-task imitation learning, we are given a dataset $\mathcal{D} = \{ (o_1, a_1, g_1),  \ldots (o_N, a_N, g_N) \}$ consisting of tuples of observations $o \in \mathcal{O}$, actions $a \in \mathcal{A}$, and task specifications $g \in \mathcal{G}$ which are often formulated in language. The objective is to learn the expert action distribution $P(a \mid o,g)$ conditioned on an observation $o$ and a task specification $g$.



We now define the objective of \textit{reasoning-based} multi-task imitation learning. We assume there exists some chain of $C$ steps of intermediate language reasoning that links an observation $o$ and action label $a$, which we denote as $(l^1, \ldots, l^C)$. We discuss how these reasoning chains are generated in \cref{sec:method:labeling}. The distribution of each reasoning step $l^j$ only depends on the preceding reasoning steps $(l^1, \dots, l^{j-1})$ as well as $o$ and $g$. The distribution of actions $a$ depends on all reasoning steps $(l^1, \ldots, l^C)$ and the observation $o$ and task $g$. We define the objective of the reasoning-based multi-task imitation learning problem as learning the expert joint reasoning and action distribution $P(a, l^1, \ldots, l^C \mid  o, g)$. In this setting, each $(o_i, a_i, g_i)$ tuple in $\mathcal{D}$ is augmented with a reasoning chain $(l_i^1, \ldots, l_i^C)$. We wish to learn a distribution $P_\theta$ parameterized by $\theta$ that  maximizes the log-likelihood of the reasoning and action data in $\mathcal{D}$:
\begin{align*}
    L(\theta)& = \sum_i^N \log P_\theta(a_i,l_i^1\dots l_i^C \mid o_i,g_i) \\
    =\ &\sum_i^N \log P_\theta(a_i \mid l_i^1\dots l_i^C,o_i,g_i) \prod_j^C P_\theta(l_i^j \mid l_i^1\dots l_i^{j-1},o_i,g_i)  \\
    =\ &\sum_i^N \log P_\theta(a_i \mid l_i^1\dots l_i^C,o_i,g_i) \\
    & + \sum_i^N \sum_j^C \log P_\theta(l_i^j \mid l_i^1\dots l_i^{j-1},o_i,g_i) \\
    =\  &L_{\texttt{action}}(\theta) + L_{\texttt{reasoning}}(\theta)
\end{align*}

Our key insight in \ACRO is that \textit{action-free} datasets---such as human video data, which is often easier to collect than robot demonstrations---can provide additional supervision for the joint action-reasoning distribution $P_\theta$ which can in turn aid generalization. Specifically, we assume access to some action-free data $\tilde{\mathcal{D}}$ consisting of $M$ samples of $(\tilde{o_i}, \tilde{g_i}, \tilde{l_i}^1 \cdots \tilde{l_i}^{C_i})$. Here, sample $i$ includes the first $C_i \geq 1$ steps of language reasoning, where $C_i$ can vary between samples. For example, we might have varying levels of confidence in our full reasoning labeling pipeline for different subsets of our action-free data -- some samples might only be confident in the higher level reasoning steps (lower $C_i$) for example due to a large embodiment gap, while others might have high quality lower level reasoning (higher $C_i$). Importantly, this flexibility of reasoning labeling could enable our framework to incorporate vast scales of varying quality and embodiment reasoning data to improve \emph{each step} of the reasoning process independently from action prediction.

In this work, we optimize the objective above along with an auxiliary objective $\tilde{L}_{\texttt{reasoning}}(\theta)$ for the action-free data, defined similarly as follows:
\begin{align*}
    \tilde{L}_{\texttt{reasoning}}(\theta)& = \sum_i^M \sum_j^{C_i} \log P_\theta(\tilde{l_i}^j \mid \tilde{l_i}^1\dots \tilde{l_i}^{j-1},\tilde{o_i},\tilde{g_i})
\end{align*}

Note that since sample $i$ contains the first $C_i$ reasoning steps, we have enough information to model each of the $C_i$ reasoning steps conditioned on previous reasoning steps and the current observation and task. 

\subsection{Reasoning Steps in \ACRO}

While this setup can in principle work with different formulations of language reasoning steps, we instantiate our algorithm with the following reasoning steps from prior work \cite{zawalski2024robotic}:
\begin{itemize}
    \item \texttt{TaskPlan} ($l^1$): describes a list of subtasks to achieve $g$.
    \item \texttt{SubtaskReasoning} ($l^2$): reasons about which subtask currently needs to be executed in the plan.
    \item \texttt{Subtask} ($l^3$): predicts the subtask that currently needs to be executed.
    \item \texttt{MoveReasoning} ($l^4$): reasons about the motion needed to achieve the subtask in the scene.
    \item \texttt{MovePrimitive} ($l^5$): predicts a movement primitive in language.
    \item \texttt{GripperPosition} ($l^6$): predicts the pixel position of the end-effector.
    \item \texttt{VisibleObjects} ($l^7$): predicts the bounding box coordinates of objects in the scene.
    \item \texttt{Action} ($a$): predicts the low-level robot action as an end-effector position delta.
\end{itemize}

We note that these reasoning steps trace through information at an increasing amount of physical and spatial groundedness---beginning with high-level scene reasoning over tasks and subtasks, transitioning to reasoning over language motions, followed by spatial information about the gripper and objects, and concluding with the low-level robot action. We take advantage of this fact in designing a pipeline to label reasoning in action-free data, as we describe in the following section.

%\smallskip
%\noindent \textbf{Our Assumptions}: While \ACRO in principle can work with any formulation of language reasoning steps that are extractable from human video data, we make the following assumptions to instantiate our algorithm:
%\begin{enumerate}
%    \item We assume the same steps of reasoning as ECoT~\cite{zawalski2024robotic}. This consists of the following stages: \texttt{task plan} ($l^1$), \texttt{sub-task reasoning} ($l^2$), \texttt{sub-task} ($l^3$), \texttt{move reasoning} ($l^4$), \texttt{move primitive} ($l^5$), \texttt{gripper position} ($l^6$), \texttt{visible objects} ($l^7$), and finally the robot action.
%    \item We assume for simplicity that we can label all steps of reasoning in our action-free data, meaning $C_i$ is the same for all examples in $\mathcal{D}_r$.
%\end{enumerate}

\subsection{Labeling Reasoning in Action-free Data}\label{sec:method:labeling}



% Labeling pipeline (hamer, language motion, ECoT labeling details)
In order to construct $\tilde{D}$---our dataset of observations, goals and action-free reasoning---we need to generate labels for the reasoning steps above from human videos. Our pipeline is similar to the automated procedure used by Embodied Chain-of-Thought (ECoT)~\cite{zawalski2024robotic} for generating reasoning over robot demonstrations, with some key modifications to handle human videos.
To obtain reasoning labels for robot demonstrations, ECoT first generates \texttt{GripperPositions} and \texttt{VisibleObjects} tags using off-the-shelf object detectors to obtain bounding boxes. Then, it extracts \texttt{MovePrimitive} (e.g. ``move to the left'') directly from actions using an automated heuristic. Conditioned on these more grounded reasoning steps $(l^5, l^6, l^7)$ and the image observation $o$, it queries Gemini~\cite{team2023gemini} to label the prior reasoning steps, from \texttt{TaskPlan} through \texttt{MoveReasoning} $(l^1, \dots, l^4)$. %Gemini has been shown to be capable at generating these high level reasonings, but it struggles to generate low-level reasonings.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/RAD_ID_5.png}
    \caption{\ACRO outperforms baselines where human video data was trained on, but no new robot data was provided. \ACRO-A is \ACRO trained only on human video data for the given axis of generalization. ECoT-GT is finetuned on the same data as \ACRO, but only using human hand locations (and not the full reasoning data).}
    \label{fig:id_plot}
\end{figure*}

In the action-free setting with human videos, we note that we can still extract high-level reasoning with Gemini, as well as extract \texttt{VisibleObjects} with off-the-shelf object detectors. However, generating the more action-grounded reasoning steps is challenging: we can no longer extract \texttt{MovePrimitives} or \texttt{GripperPositions} automatically because we lack explicit action labels. In order to overcome this, we extract the \texttt{MovePrimitives} and \texttt{GripperPositions} using HaMeR~\cite{pavlakos2024reconstructing}, a hand keypoint and pose tracking method. Given these predictions, we can extract the \texttt{MovePrimitives} from changes in the hand pose information: first, we study each axis of the change in hand poses for each frame; then, we label the move primitive based on the dominant axis of motion. We find that this works reliably for tracking gripper and positional movement primitives, but is not as reliable for detecting rotational movement primitives.
% FUTURE WORK --> including rotation when hand pose tracking gets better.
We outline this labeling procedure in \cref{fig:labeling}.

\subsection{Training on Partial Reasoning Chains}\label{sec:method:training}

To train on mixtures of demonstration and action-free data, we use the ECoT and OpenVLA~\cite{zawalski2024robotic, kim2024openvla} architecture, which trains a pre-trained VLM transformer with 7B parameters to predict sequences of language reasoning and then action tokens. This model is pretrained on Internet-scale vision-language tasks, such as bounding box detection or object localization. Thus, it benefits from a strong vision and language priors. With ECoT and OpenVLA, it is then further trained on robot demonstration data, and in the case of ECoT, predicts language reasoning tokens prior to action tokens.
In \ACRO, we reuse this paradigm for the robot demonstration data, but for the new action-free data, our ``labels'' for training contain only reasoning as described in \cref{sec:method:labeling}.
