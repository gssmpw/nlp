In this section, we situate our work among prior work on the use of language as a representation of low-level actions in robot learning, vision-language-action models (VLAs) as a recipe for language-conditioned robot policies, and approaches that leverage human videos for robot learning.



% \TODO{Suvir: Add main messages/differences from our work to all subsections}

\noindent \textbf{Language as an Action Representation.}
Language is commonly used as a high-level representation in imitation learning, either for conditioning multi-task policies on specific instructions  ~\citep{stepputtis2020language, jang2022bc, rt12022, rt22023, kim2024openvla}, or as a way to decompose high-level, long-horizon instructions into lower-level subtask instructions \citep{brohan2023do, huang2023inner, shi2024yell}. More recently, several works have studied the role of more fine-grained language such as ``language motions'' as intermediate representations to predict~\cite{belkhale2024rt} or explicitly reason over language as well as other visually-grounded features such as bounding boxes as a way of guiding large pretrained policies~\cite{zawalski2024robotic}. In contrast to prior works which use language as a goal representation, we explore how reasoning in language can be used as an action representation for human video data in addition to robot data.

\noindent \textbf{Vision Language Action Models.}
Recent works have explored the use of pre-trained Vision-Language Models (VLMs) as backbones for Vision-Language Action Models (VLAs) which directly predict low-level robot actions. For example, RT-2-X \cite{oxe2024} fine-tunes the 55B-parameter PaLI-X VLM \cite{chen2024palix} on the Open-X Embodiment dataset \cite{oxe2024}, and OpenVLA \cite{kim2024openvla} uses a 7B-parameter Llama 2 LLM backbone with a vision encoder based on DINOv2 \cite{oquab2023dinov2} and SigLIP \cite{zhai2023sigmoid}. The promise of VLAs for manipulation is to build off of generalization of VLMs which have been trained on Internet-scale vision-language data. An additional way to achieve transfer of VLM capabilities to VLAs is to take advantage of their textual reasoning abilities. For example, Embodied Chain of Thought (ECoT) uses multiple steps of reasoning prior to predicting robot actions by training on synthetic reasoning data \cite{zawalski2024robotic}.



\noindent \textbf{Learning from Human Video.}
A large number of prior works in imitation learning for robotics focus on learning from demonstrations collected via teleoperation by expert operators.
This method of collecting data is costly, so a number of prior works have investigated ways to leverage existing data sources of human videos to improve robot policy learning --- for example, by pre-training visual representations \citep{nair2022r3m, xiao2022masked, karamcheti2023language},  learning reward functions \citep{shao2021concept2robot, chen2021learning, mandikal2022dexvip}.
However, bridging the gap between human videos and robot actions can be challenging due to embodiment differences and diversity in videos. Several works learn priors from human video datasets and/or in-domain human videos \citep{shaw2023videodex, bahl2022human, wang2023mimicplay, lepertshadow} or aligning paired/unpaired examples of human videos and robot demonstration videos \citep{sharma2019third, smith2019avid, xiong2021learning, jain2024vid2robot} or simulations \citep{qin2022dexmv}. These works are still fundamentally limited by the quantity of robot demonstrations. Another line of work leverages intermediate representations for predicting robot actions downstream, but make assumptions about the human hand behavior, which is not necessarily the same as the robot \cite{papagiannis2024r+, bharadhwaj2024track2act}. Our work goes beyond existing methods that rely on generating intermediate representations for action predictions by generating detailed reasoning steps about human video demonstrations.

