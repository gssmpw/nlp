


In this section, we evaluate how \ACRO enables transfer from human videos to robot policies and generalization beyond settings in the human videos or robot demonstration data. Specifically, we seek to answer the following questions:

\smallskip \noindent\textbf{Q1} -- \textbf{Human-to-Robot Transfer:} Can \ACRO enable learning new tasks seen only in the human video data and not the robot demonstration data?


% \begin{table*}[ht!]
% \centering
% \caption{Human-to-Robot Transfer}
% \label{tab:combined_results}
% \begin{tabular}{llcccc}
% \toprule
% \textbf{Axis} & \textbf{Task} & \textbf{ECOT} & \textbf{ECoT-GT} & \textbf{\ACRO-A} & \textbf{\ACRO} \\
% \midrule
% {\textbf{Compositional Generalization}} 
%  & \instr{put the bottle on the plate} & 3/10 & 4/10 & 7/10 & 7/10 \\ %  (plate in sink)
%  & \instr{pick up the potato and put it on the plate}  & 6/10 & 5/10 & 8/10 & 7/10 \\ % (both in sink)
%  & \instr{put the cupcake on the rack} & 6/10 & 7/10 & 7/10 & 6/10 \\
% \midrule
% {\textbf{New Scene Generalization}}
%  & \instr{pick up the milk} & 3/10 & 3/10 & 5/10 & 5/10 \\ %  (on cloth)
%  & \instr{pick up the tape}  & 1/10 & 2/10 & 2/10 & 5/10 \\ % (pot distractor)
%  & \instr{put the potato next to the plushie} & 3/10 & 1/10 & 3.5/10 & 5/10 \\
% \midrule
% {\textbf{New Object Generalization}}
%  & \instr{pick up the cup} & 3/10 & 3/10 & 5/10 & 6/10 \\
%  & \instr{pick up the tape} & 2/10 & 3/10 & 5/10 & 4/10 \\
% \bottomrule
% \end{tabular}
% \end{table*}


\noindent\textbf{Q2} -- \textbf{Reasoning Generalization:} Does reasoning in \ACRO enable generalization to novel tasks beyond both the robot demonstration data and human video data it was trained on?

\noindent\textbf{Q3} -- \textbf{Cross-Environment Transfer:} Can \ACRO learn new tasks from human video data in out-of-domain environments?

% \begin{table*}[ht]
% \centering
% \caption{Generalization Results}
% \label{tab:generalization}
% \begin{tabular}{llcc}
% \toprule
% \textbf{Axis} & \textbf{Task} & \textbf{ECOT (Baseline)} & \textbf{\ACRO} \\
% \midrule
% {\textbf{Compositional Generalization}}
%  & \instr{put the salt on the pizza} & 2/10 & 2/10 \\
%  & \instr{put the sushi on the rack} & 4/10 & 3/10 \\
%  & \instr{put the green bottle on the plate} & 1/10 & 2/10 \\ % (pink plate)
%  & \instr{put the vegetables in the sink} & 2/10 & 4/10 \\ % (corn)  (potato already in the sink)
% \midrule
% {\textbf{New Scene Generalization}}
%  & \instr{put the carrot on the drying rack} & 4/10 & 6/10 \\ %  (on new plate) 
%  & \instr{put the bottle on the book} & 0/10 & 4/10 \\ %  new book
% \midrule
% {\textbf{New Object Generalization}}
%  & \instr{pick up the cup} & 4/10 & 6/10 \\ % new
%  & \instr{pick up the bowl} & 2/10 & 4/10 \\ %new
%  & \instr{pick up the tape} & 1/10 & 1/10 \\ %new
%  & \instr{pick up the plushie} & 1/10 & 4/10 \\
% \bottomrule
% \end{tabular}
% \end{table*}

\subsection{Evaluating Generalization}\label{sec:experiments:evaluation}

Next, we discuss the environments, tasks, and model baselines we use to evaluate the reasoning generalization capabilities of \ACRO.

\smallskip \noindent \textbf{Real-World Environments}: We use a 6-DoF WidowX robot arm for our experiments. We perform all evaluations in \cref{sec:experiments:crossembodiment} and \cref{sec:experiments:generalization} on the Toy Sink setup from \cite{walke2023bridgedata}, to ensure fair comparison with existing pre-trained models. All human video data for \cref{sec:experiments:crossembodiment} and \cref{sec:experiments:generalization} was also collected in the Toy Sink setup (1616 demonstration videos), using both the standard Bridge V2 camera setup, as well as an additional camera for better hand tracking. Notably, the Bridge V2 setup is comprised of mostly miniature toy replicas of real world objects such as small kitchen supplies, blocks, and home supplies. Therefore, we also seek to assess how \ACRO responds to data from real-world human environments, and learns to interact with realistically sized objects. We thus collect data in two additional environments: a plain tabletop and a cluttered desk, as well as various real home and kitchen environments. This data was used to assess how \ACRO responds to data from unstructured environments in \cref{sec:experiments:crossenv}. 




\smallskip \noindent \textbf{Generalization Tasks}: We evaluate \ACRO across a variety of generalization tasks. These tasks comprise three main axes of generalization:
\begin{enumerate}
    \item \textbf{Compositional Generalization}: In this axis, the objects, tasks, and scenes are all seen in pre-training data (Bridge V2 data), but not in those particular configurations. For example, pizza and salt both exist in Bridge V2, but salt is never placed on the pizza.
    \item \textbf{New Object Generalization}: This axis introduces unseen objects for known behaviors (e.g., \instr{pick cup} $\to$ \instr{pick plushie}).
    \item \textbf{New Scene Generalization}: This axis requires generalizing to novel backgrounds and distractor objects for seen tasks; for example, picking up a known object with a pot in the background.
\end{enumerate}

Note that the Compositional Generalization axis tests the model's ability to \emph{interpolate} the training data, while New Object and New Scene axes test the model's ability to \emph{extrapolate} from the training data. Exact tasks for each axis can be found in \cref{sec:appendix:results}.

\smallskip \noindent \textbf{Methods}: To test the efficacy of reasoning in learning from human video data, we evaluate the following models in our generalization scenarios.
\begin{enumerate}
    \item \textbf{Embodied Chain-of-Thought (ECoT)} \cite{zawalski2024robotic} A state-of-the-art action reasoning model trained on Bridge V2, but without any human video data.
    \item \textbf{ECoT w/ Gripper Tracking (ECoT-GT):} ECoT finetuned on the same human video data as \ACRO, but only generates the \texttt{GripperPosition} portion of the reasoning chain. This is analogous to how prior work learns from extracted pose information only in human videos, but does not extract higher level language reasoning \cite{papagiannis2024r+, lepertshadow, ren2025motion}.
    \item \textbf{\ACRO (Ours):} ECoT finetuned on the full chain of reasonings generated from human video data.
    \item \textbf{\ACRO-A (Ours):} Same as \ACRO, but trained on only human videos from one axis of generalization at a time (the axes are described in \cref{sec:experiments:evaluation}).
\end{enumerate}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/RAD_OOD_3.png}
    \caption{\ACRO compared to ECoT for tasks contained in neither human or robot data. \ACRO shows improved performance across all three axes of generalization.}
    \label{fig:ood_plot}
\end{figure*}

\subsection{Can \ACRO enable transfer from human-to-robot embodiments?}\label{sec:experiments:crossembodiment}

First, we assess if \ACRO can learn accurate reasonings and robot actions on new tasks that are present only in human video demonstrations. 
% To do so, we first train \emph{axis-specific} \ACRO models for each of the three generalization axes described in \cref{sec:experiments:evaluation}.  These models are denoted \textbf{\ACRO-A}.
We train the axis-specific models (\ACRO-A) only on human video data for that axis (8-12 tasks with a total of 320-500 videos per axis). We evaluate these axis-specific models against zero-shot ECoT, as well as \ACRO (trained on human video data from all three axes) and ECoT-GT models trained on our full human video dataset.

In \cref{fig:id_plot}, we find that despite having no new robot demonstration data for these new tasks, \ACRO-A achieves consistently higher success rates than zero-shot ECoT and ECoT-GT across all areas of generalization (\textbf{Q1}). 

\smallskip \noindent \textbf{Compositional}: On compositionally new tasks, \ACRO-A outperforms ECoT by 23\% and ECoT-GT by 20\%. \ACRO outperforms ECoT and ECoT-GT by 17\% and 13\% respectively. Qualitatively, \ACRO models demonstrates significantly better reasoning capability, particularly in the second step of pick place tasks (such as  placing the object of interest in the desired location).

\smallskip \noindent \textbf{New Object}: On tasks with new objects, \ACRO and \ACRO-A both improves on ECoT and ECoT-GT by 25\% and 20\%, respectively. \ACRO models demonstrate substantially better ability to reason about grasp points on new objects, such as moving towards the sides of large cups instead of the middle. 

\smallskip \noindent \textbf{New Scene}: \ACRO models also substantially outperform baselines on novel scenes (containing  distractors and other scene modifications). \ACRO-A outperforms ECoT by 12\% and ECoT-GT by 15\%. The full \ACRO model had stronger performance, outperforming ECoT by 27\% and ECoT-GT by 30\% - potentially due to improves ability to ignore distractors from the larger dataset it was trained on. Reasoning traces on \ACRO models also appeared to be more accurate, with ECoT often becoming distracted and generating non-sensical reasonings.
These results indicate that augmenting chain-of-thought models with reasoning from human video data improves these models' ability to reason about and infer robot actions on previously unseen task configurations.

\subsection{Can \ACRO train more generalizable policies?}\label{sec:experiments:generalization}

Ultimately, training on large datasets of human video data should enable VLAs to generalize not only to human demonstrated tasks, but also to completely unseen scenarios. To explore if \ACRO enables training more general models, we evaluate our model against ECoT on 10 novel tasks (unseen in both human and robot data) comprising all three generalization axes. Results are presented in \cref{fig:ood_plot}.

\smallskip \noindent \textbf{Compositional}: On compositionally novel tasks, \ACRO outperforms ECoT by 5\%. \ACRO reasoned better than ECoT over multi-step tasks, such as knowing where to place the salt after picking it up.

\smallskip \noindent \textbf{New Object}: \ACRO substantially improves performance on tasks with unseen objects, such as bowls and large cups, despite not seeing such objects in human or robot training data. \ACRO achieves 30\% higher success compared to ECoT.

\smallskip \noindent \textbf{New Scene}: In novel scenes (environments with large distractors in the scene, such as cloth, pots, and a large plushie), \ACRO reached 18\% higher success rate than ECoT. Qualitatively, ECoT struggled to reason about the new scene and would often generate poor reasonings and execute seemingly random actions, whereas \ACRO generated correct reasoning which informed downstream action prediction.

\smallskip
This indicates that reasoning in \ACRO enables better generalization to a variety of unseen tasks, without training on any new human or robot data (\textbf{Q2}).

\begin{table}[ht]
\centering
\caption{Cross-Environment Transfer}
\label{tab:crossenv}
\begin{tabular}{llcc}
\toprule
\textbf{Task} & \textbf{Model} & \textbf{Success Rate} \\
\midrule
{\instr{pick up the cup}}
 & ECoT & 3/10 \\
 & \ACRO & 6/10 \\
 & ECoT-GT & 4/10 \\
\midrule
{\instr{put the sushi on the book}}
 & ECoT & 4.5/10 \\
 & \ACRO & 6.5/10 \\
 & ECoT-GT & 5/10 \\
 \midrule
{\instr{pick up the tiger}}
 & ECoT & 3/10 \\
 & \ACRO & 3/10 \\
 & ECoT-GT & 3/10 \\
 \midrule
{\instr{pick up the controller}}
 & ECoT & 2/10 \\
 & \ACRO & 3.5/10 \\
 & ECoT-GT & 2/10 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Can \ACRO leverage data from new environments?}\label{sec:experiments:crossenv}

To truly leverage large-scale video data, generalist robot policies must learn from demonstrations in diverse scenes. Thus, we first train \ACRO with human video data in unseen environments to see how well it can incorporate this data, and then we compare its performance to \ACRO trained on in-distribution human video data (i.e., same environment for both human video and robot evaluation).

\smallskip \noindent \textbf{Human Videos from New Environment}: We seek to understand how \ACRO responds to human video data collected outside the Bridge 
V2 environment. We first collect data for two unseen tasks in a new tabletop setup (unseen in Bridge V2 data). Then, we evaluate models trained on this new enviroment data in the original Bridge Toy Sink environment. In \cref{tab:crossenv}, we see that models trained on this data outperform ECoT by 16\% and ECoT-GT by 13\%. Similarly to \cref{sec:experiments:crossembodiment} and \cref{sec:experiments:generalization} \ACRO models showed significantly better ability to reason about grasp points, such as where to pick up the controller, despite the data being in a different environment (\textbf{Q3}).

\smallskip \noindent \textbf{In-distribution vs. Out-of-Distribution Human Data}: Next, we assess how \ACRO performance scales with increased data for the same tasks collected in-distribution (in the miniature Toy Sink setup) versus out-of-distribution (various real world kitchen and office environments). To do so, we collected 100 additional demos for the \instr{pick up the tape} task in the Toy Sink setup. We also collected 250 out-of-domain demos for \instr{pick up the tape} in novel environments such as real kitchens, countertops, and desks. Then, we trained \ACRO on two different data mixtures:
\begin{enumerate}
    \item The original \ACRO data mix (which already had 40 ``Pick up the tape'' demos) + in-distribution data and 
    \item The original \ACRO data mix + out-of-domain data.
\end{enumerate}

Results for both mixtures are shown in \cref{tab:tapescale}. We find that \ACRO models trained on both in-domain (+30\% success) and out-of-domain data (+25\% success) show improved performance over the original model (\textbf{Q3}). Qualitatively, \ACRO models were better able to reason about when to bring the gripper to the level of the tape, with ECoT models often moving to low and knocking over the tape, which is abnormally tall with respect to objects in Bridge V2.


\begin{table}[ht]
\centering
\caption{Data Scaling}
\label{tab:tapescale}
\begin{tabular}{llcc}
\toprule
\textbf{Data} & \textbf{Model} & \textbf{Success Rate} \\
\midrule
{\textbf{Original model (40 demos)}}
 & ECoT & 2/10 \\
 & ECoT-GT & 3/10 \\
 & \ACRO & 4/10 \\
 & \ACRO-A & 5/10 \\
\midrule
{\textbf{Same Environment (+100 ID demos)}}
 & \ACRO & 7/10 \\
 & ECoT-GT & 4/10 \\
\midrule
{\textbf{New Environments (+250 OOD demos)}}
 & \ACRO & 6.5/10 \\
 & ECoT-GT & 5/10 \\
\bottomrule
\end{tabular}
\end{table}