% How do we achieve generalization at scale?
Training visuomotor policies via imitation learning is an appealing paradigm for robot control. However, an outstanding challenge for current end-to-end learning methods is to generalize to new settings beyond their training data, such as new scenes, new task instructions, and new object instances. For example, if a robot learns to pick up a video game controller in a lab setting, but encounters the same controller in an office, it should still use its prior knowledge to bridge the gap between different environments. 
The ability to generalize to these types of novel scenarios is essential for making learning-based policies useful in practice, as the real-world often presents diverse and unpredictable scenarios. 
% Since real-world environments are diverse and unpredictable, generalization is key to making learning-based methods useful in practice.

One approach to achieving generalizable policies is to collect large-scale robot demonstration datasets across tasks and embodiments and train expressive multi-task policies on them \cite{khazatsky2024droid, oxe2024, kim2024openvla, team2024octo}. While there are promising signs of scaling up datasets being the solution, we simply have not reached the scale needed for comprehensive generalization, and one might argue that collecting data at such scale is practically infeasible.
% How can we scale up our data collection to meet these demands?

% Learning from human videos is our best bet
On the other hand, many see tapping into human video datasets, consisting of humans directly performing tasks as opposed to collecting robot data, as the answer \cite{ye2024latent, wang2023mimicplay, bharadhwaj2024gen2act}. This data is cheap to collect and already present at scale in Internet datasets. However, human videos lack action labels, making supervised learning methods like imitation learning very difficult. Some works tackle this challenge by extracting \emph{grounded action-like} representations from video as labels for imitation learning, for example hand poses or object affordances~\cite{bharadhwaj2024track2act, ren2025motion, lepertshadow, xu2023xskill}. However, extracting grounded actions from human videos often makes assumptions about the scene and the embodiment gap (e.g., how the hand pose maps to the robot action or relying on paired human and robot data) which can limit their usefulness in practice. 
% We posit that the more grounded an action representation (i.e., closer to low-level actions), the more challenging it becomes to bridge the embodiment gap between human arm and robot hand.

% our insight: human videos have reasoning we can extract in language
Instead of extracting grounded actions from videos and the restrictive assumptions that come with it, we ask: 
% \emph{is there a more reliable, higher-level action information we can glean from human videos, and more generally action-free data?}
\emph{is there any other behavioral information -- that still directly influences robot actions -- that we can extract from human videos, and more generally action-free data?}
Our insight is that human videos contain vast amounts of \emph{higher-level reasoning} that guide robot action prediction, and this reasoning information can be captured via language. For example, if the task is to pick up a cup, a human hand might move towards the cup, then grasp the cup, and then lift the cup. 
Whereas prior work might learn a policy that outputs changes in the hand pose and hope this transfers to the robot, we can instead learn to predict this detailed reasoning itself from human videos, which is shared across many embodiments.
Policies that autoregressively predict each stage of reasoning -- predicting from high-level language down to grounded robot actions -- show steerable behaviors that improve performance and generalization~\cite{zawalski2024robotic, belkhale2024rt}. While these prior works often rely on reasoning over robot trajectories, our key idea is to instead extract such reasoning from \emph{action-free} human videos -- significantly scaling up data that informs robot actions.
% The final step is translating reasonings into robot actions. Recently several works like RT-Hierarchy (RT-H) and Embodied-CoT (ECoT) have learned to autoregressively predict each stage of reasoning, predicting from high-level language down to grounded robot actions; they show language-based reasoning before action prediction substantially improves policy performance and generalization~\cite{zawalski2024robotic, belkhale2024rt}. 

% our method
We introduce our method, \Method (\ACRO), a robot policy that leverages reasoning traces extracted from action-free data. \ACRO trains a large transformer model on a mixture of robot demonstration data with both reasoning and robot action labels, and action-free (human video) data labeled with \emph{just} reasoning. The robot data teaches the model to autoregressively go from reasoning to low-level actions, while the action-free data augments the reasoning knowledge, thus boosting the reasoning capabilities of the model. We label reasoning traces by leveraging pretrained vision-language models such as Gemini with hindsight knowledge as done in prior work~\cite{zawalski2024robotic}. 
% TODO not sure if below statement should go in here or method
% Since each token is learned autoregressively, our action-free data can stop at any point in the reasoning chain, depending on what labels we have available. For example, if the reasoning chain consists of subtask reasoning and then move reasoning before action prediction (like in ECoT), then action-free data can consist of either just sub-task

%claims and experiments
We experimentally validate that learning from action-free reasoning data transfers well across the embodiment gap -- showing 20\% better performance on tasks only seen in the action-free data over models not finetuned with \ACRO. Additionally, we demonstrate that having larger amounts of action-free reasoning data improves the capacity of the model to generalize in language space to completely unseen tasks with \ACRO outperforming baselines by 15\% on generalization tasks (that have never been seen in robot or human data).
%TODO add some strong statement about results improving by X%, etc

% Experimentally, our hypothesis is that \textbf{(1)} learning from action-free reasoning data transfers well across the embodiment gap, \textbf{(2)} for tasks only in the action-free data, our model conditioned on correct reasoning will output correct actions on the robot, even though it has never seen those actions in the data. Supporting our intuition, prior work has shown that conditioned on high-level language reasoning, predicting low-level robot actions is significantly easier than predicting actions from just the image and task ~\cite{}. \textbf{(3)} Having large amounts of action-free reasoning data will improve our capacity to generalize in language space to novel tasks, and this will lead to overall policy improvement due to (2). Through extensive experiments, we show that \ACRO indeed enables better transfer on tasks seen only in the action-free data (1, 2). We then show that by scaling up our action-free data, \ACRO is capable of improving reasoning and action generalization to completely unseen tasks (3), across several axes of generalization.

% We need bigger datasets to really unlock generalization potential in robotics.
% \begin{itemize}
%     \item This is gonna require models to learn from diverse, scalable data sources.
%     \item Much of the data we have in robotics lacks action information, e.g. human videos. But bridging the cross embodiment gap is really challenging without some additional structure.
%     \item Prior Work: Language has emerged as a scalable, cross-embodiment reasoning representation in robotics. It’s easy to label, natural for foundation models, and interactive.
%     \item Could first talk about other structured representations, specifically hand tracks
%     \item Our insight: We can label language reasoning on human videos with existing automated CoT labeling techniques, to learn the high level reasoning structure from diverse actionless sources of data.
%     \item Our method: Label up to and including the low level reasoning step of CoT, co-train on just these partial CoT labels on human video with the full CoT on some embodied data. We show we can extract these low level labels through robust hand tracking methods like HaMeR.
%     \item We find that CoT enables strong transfer from just a small number of human videos despite having zero action labels present at test time. We test varying degrees of generalization and find XYZ…
% r.  
% \end{itemize}