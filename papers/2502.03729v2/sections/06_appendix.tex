

We outline the dataset collection and reasoning generation procedure in \cref{sec:appendix:data}. The models, training procedure, and baselines are described in detail in \cref{sec:appendix:train}. Finally, \cref{sec:appendix:results} provides examples of results and description of reported success rates.

\subsection{Dataset Details}\label{sec:appendix:data}

\smallskip \noindent \textbf{Data Collection}: Our main human video data collection was on the Bridge V2 Toy Sink setup. We aligned one camera based on the original Bridge V2 scene. We also set up a second camera from directly behind the WidowX gripper to better track hand movement as seen in \cref{fig:povs}. Example tasks are shown in \cref{fig:data_col}. We used HaMeR to track the hand using the secondary camera perspective. We used the average location of the thumb tip and index finger tip points tracked by HaMeR as the gripper location. Based on the delta gripper position between frames, we characterized every frame as ``stop", ``move forward", ``move backward", ``move left", ``move right", ``move up", or ``move down" movement primitives. We used the average distance between the thumb tip and index tip to determine ``close gripper" and ``open gripper" primitives. For reasoning generation on the human videos, we followed the the pipeline of \cite{zawalski2024robotic}, but used this HaMeR tracking in place of proprioception and SAM to generate movement primitives and gripper locations.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/povs.png}
    \caption{The main Bridge V2 perspective (right) versus the secondary perspective used for hand tracking (left).}
    \label{fig:povs}
\end{figure}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/data_col.png}
    \caption{Example human video tasks collected.}
    \label{fig:data_col}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/new_envs.png}
    \caption{Task demonstrations collected in environments outside of Bridge V2 to assess how \ACRO responds to data from different types of scenes.}
    \label{fig:new_envs}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/tape_envs.png}
    \caption{Real world environment data \ACRO is trained with for \cref{sec:experiments:crossenv}.}
    \label{fig:tape_envs}
\end{figure*}


\smallskip \noindent \textbf{Data Mixtures}: For \ACRO-A models in \cref{sec:experiments:crossembodiment} we collected 392 demonstrations for the compositional generalization dataset, 304 demonstrations for the new object dataset, and 280 demonstrations for the new scene dataset. The full \ACRO model as well as ECoT-GT model were both trained on all three of these datasets as well as 640 additional demos to make 1616 total demonstrations.

Data for \cref{tab:crossenv} was collected from two new tabletop environments as shown in \cref{fig:new_envs}. Each task in \cref{tab:crossenv} had 40 total demos collected. For \cref{tab:tapescale} we collected 100 additional demos in the Toy Sink setup for the ``in-distribution" evaluation. For the ``OOD" data, we collected 50 demos from 5 different scenes as show in \cref{fig:tape_envs}. 

\subsection{Training Details}\label{sec:appendix:train}

\ACRO uses the Prismatic VLM [35] architecture from OpenVLA \cite{kim2024openvla}, which fuses pre-trained SigLIP \cite{zhai2023sigmoid} and/or DinoV2 \cite{oquab2023dinov2} features for the visual encoder, and a LLaMA 2 7B \cite{touvron2023llama} language backbone. All models are fine-tuned to convergence with a learning rate of 2e-4, a LoRA batch size of 2, and anywhere from 2 to 8 GPUs (L40s or A40). Training of the ECoT-GT baseline is the same as \ACRO except the loss term for the stop token is omitted and we also adjust the query prompt from "What action should the robot take to [\textit{task}]?" to “Where is the robot hand in the image?”.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/tasks.png}
    \caption{Example tasks for compositionally new tasks (left), new objects (middle), and new scenes (right).}
    \label{fig:tasks}
\end{figure*}

\subsection{Results}\label{sec:appendix:results}

Every task was evaluated 10 times. Objects were randomly placed throughout the scenes in a different spot for all 10 trials.
For pick and place tasks, partial credit (0.5) was given for successfully picking up the object, but placing in the wrong location. For pick objects, no partial credit was given except for the ``pick up the controller” task, which had an exceptionally high payload. Thus partial credit was given for grasping the object, even if the object slipped out of grasp upon being lifted.

