% This is a modified version of Springer's LNCS template suitable for anonymized MICCAI 2025 main conference submissions. 
% Original file: samplepaper.tex, a sample chapter demonstrating the LLNCS macro package for Springer Computer Science proceedings; Version 2.21 of 2022/01/12

\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx,verbatim}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
\usepackage[hidelinks]{hyperref}
\usepackage{amssymb,amsmath}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{orcidlink}
\usepackage{marvosym}
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


%
\begin{document}
%
\title{A Pragmatic Note on Evaluating Generative Models with Fréchet Inception Distance for Retinal Image Synthesis}
%
\titlerunning{A Pragmatic Note on Evaluating Generative Models}

% \begin{comment}  %% Removed for anonymized MICCAI 2025 submission
\author{Yuli Wu\inst{1}\orcidlink{0000-0002-6216-4911} \and Fucheng Liu\inst{1}\orcidlink{0009-0002-0299-2448} \and Rüveyda Yilmaz\inst{1}\orcidlink{0009-0007-7351-698X} \and Henning Konermann\inst{1}\orcidlink{0009-0006-5736-5803} \and \\Peter Walter\inst{2}\orcidlink{0000-0001-8745-6593} \and
Johannes Stegmaier\inst{1}\orcidlink{0000-0003-4072-3759} }
%
\authorrunning{Yuli Wu et al.}
\institute{
Institute of Imaging \& Computer Vision, RWTH Aachen University, Germany\\
\and
Department of Ophthalmology, RWTH Aachen University, Germany\\
\Letter\, \email{yuli.wu@lfb.rwth-aachen.de}}
% \end{comment}

\begin{comment}
\author{Anonymized Authors}  %% Added for anonymized MICCAI 2025 submission
\authorrunning{Anonymized Author et al.}
\institute{Anonymized Affiliations \\
    \email{email@anonymized.com}}
\end{comment}
    
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Fréchet Inception Distance (FID), computed with an ImageNet pretrained Inception-v3 network, is widely used as a state-of-the-art evaluation metric for generative models. It assumes that feature vectors from Inception-v3 follow a multivariate Gaussian distribution and calculates the 2-Wasserstein distance based on their means and covariances. While FID effectively measures how closely synthetic data match real data in many image synthesis tasks, the primary goal in biomedical generative models is often to enrich training datasets ideally with corresponding annotations. For this purpose, the gold standard for evaluating generative models is to incorporate synthetic data into downstream task training, such as classification and segmentation, to pragmatically assess its performance. In this paper, we examine cases from retinal imaging modalities, including color fundus photography and optical coherence tomography, where FID and its related metrics misalign with task-specific evaluation goals in classification and segmentation. We highlight the limitations of using various metrics, represented by FID and its variants, as evaluation criteria for these applications and address their potential caveats in broader biomedical imaging modalities and downstream tasks.

\keywords{FID \and Generative models \and Retinal imaging}
% Authors must provide keywords and are not allowed to remove this Keyword section.

\end{abstract}
%
%
%

\section{Introduction}\label{sec:intro}

Deep generative models, particularly generative adversarial networks (GANs) \cite{goodfellow2014generative}, by adversarial training of the generator and discriminator, and diffusion models \cite{ho2020denoising}, by iteratively refining noise into structured images, have demonstrated significant potential in 2D and 3D biomedical image synthesis, as shown in studies such as \cite{eschweiler2024denoising,han2020breaking,muller2023multimodal,wu2024retinal,yilmaz2024annotated,yilmaz2024cascaded}. 
By learning from real biomedical data, these models can generate realistic synthetic images, helping to address challenges like limited data availability and data privacy concerns. 
Correspondingly, various studies have focused on generating fully annotated images to support downstream task training, such as classification and segmentation \cite{park2019semantic,rombach2022high,zhang2023adding}. This can be achieved by conditioning generative models on different types of input, such as text, images, or sketches. Furthermore, several approaches control the diffusion process using structured guidance, where segmentation maps are used to steer image synthesis \cite{eschweiler2024denoising,park2019semantic,rombach2022high,yilmaz2024annotated,zhang2023adding}. The resulting image-annotation pairs are then pragmatically used for training downstream tasks.
However, ensuring that synthetic images are both realistic and useful for downstream tasks remains a critical challenge \cite{theis2016note}.

As shown in Fig.~\ref{fig:pipeline}, we illustrate a common scenario using generative models, demonstrated with two retinal image modalities for data enrichment. 
On one hand, a synthetic dataset is generated with optional annotations and evaluated using various metrics, primarily FID and its variants (Section~\ref{sec:metric}). 
On the other hand, the composite dataset containing a mixture of real and synthetic data is applied to a downstream task, where the trained model predicts the desired outputs on the test set (Section~\ref{sec:task}). 
In Section~\ref{sec:result}, we demonstrate that widely used generative evaluation metrics fail to align with downstream performance and draw a pragmatic note on the unreliability of such \textit{feature-distance} metrics when assessing generative models for data enrichment in the context of a utilitarian downstream task.


\begin{figure}[t]
    \centering
    \includegraphics[width=.99\linewidth]{pipeline.pdf}
    \caption{Overview of the generative model pipeline for data enrichment exemplified with retinal images, including generation, evaluation, and downstream task prediction. The discrepancy between generative evaluation metrics and downstream performance is investigated.}
    \label{fig:pipeline}
\end{figure}


\section{Generative Evaluation Metrics}\label{sec:metric}

\subsection{Fréchet Inception Distance}

Fréchet Inception Distance (FID) \cite{heusel2017gans} is the de facto standard metric for assessing the perceptual quality of generated images. It compares the feature distributions of generated and real images using an ImageNet pretrained Inception-v3 model \cite{deng2009imagenet,szegedy2016rethinking} and the Fréchet distance (equivalent to the 2-Wasserstein distance \cite{peyre2019computational,vaserstein1969markov}), which measures the difference between two probability distributions: \( p_r \), the distribution of real data, and \( p_g \), the distribution of generated data. Assuming Gaussian distributions with means \( \mu_r, \mu_g \) and covariances \( \Sigma_r, \Sigma_g \), the squared Fréchet distance (FD) is derived as:
\begin{equation}\label{eq:fid}
    D_{\text{Fréchet}}^2(p_r, p_g) = \|\mu_r - \mu_g\|^2 + \text{tr}\left( \Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{1/2} \right),
\end{equation}
where \( \|\mu_r - \mu_g\|^2 \) is the squared Euclidean distance between means, and \( \text{tr}(\cdot) \) is the trace of a matrix.
A lower FID score indicates that the generated images more closely resemble the real images in terms of perceptual similarity.
It is important to note that the closed-form expression in Eq.~\ref{eq:fid} is not limited to the multivariate Gaussian distribution, but also applies to any two distributions in $\mathbb{R}^n$ within the family of \textit{elliptically contoured distributions} \cite{dowson1982frechet,peyre2019computational}, expressed as: 
\begin{equation}\label{eq:ecd}
f({x}; \mu, A) = \mathrm{const.}\times \frac{1}{|A|^{1/2}} \, g \left( ({x} - {\mu})^\top A^{-1} ({x} - {\mu}) \right), 
\end{equation}
where \(A\) is a positive definite matrix and \(g(z)\) is a non-negative function defined on the positive real axis ($r$), satisfying $0 < \int_0^\infty r^{n/2-1} g(r)dr < \infty$, which characterizes the specific distribution (e.g., for the Gaussian distribution, \(g(z) = \exp(-z/2)\)).
Therefore, merely verifying the non-Gaussianity of the feature distributions does not undermine the reliability of FID.

By substituting the feature extractor in the FID calculation with alternative models or applying different distance measures for feature comparison, we can explore alternative approaches to evaluate generative models.
We refer to this class of generative evaluation metrics as \textit{feature-distance} metrics and present variants of FID, categorized by different distance measures in Section~\ref{sec:distance} and feature extractors in Section~\ref{sec:feature}.


\subsection{Distance Variants}\label{sec:distance}


Besides the Fréchet distance, Maximum Mean Discrepancy (MMD) \cite{gretton2006kernel} is another key distance metric for comparing probability distributions, commonly used to evaluate generative models. 
MMD is a non-parametric measure that compares the means of two distributions in a higher-dimensional feature space. The squared MMD is defined as:
\begin{equation}
D_\text{MMD}^2(p_r, p_g) = \mathbb{E}_{x, x' \sim p_r} [k(x, x')] + \mathbb{E}_{y, y' \sim p_g} [k(y, y')] - 2 \mathbb{E}_{x \sim p_r, y \sim p_g} [k(x, y)],
\end{equation}
where \( \phi(\cdot) \) is a feature map and \( k(x, y) = \langle \phi(x), \phi(y) \rangle \) is a kernel function, which can be, e.g., a rational quadratic kernel as in Kernel Inception Distance (KID) \cite{bińkowski2018demystifying} or a Gaussian RBF kernel as in CLIP-MMD (CMMD) \cite{jayasumana2024rethinking}. Unlike FID, MMD does not assume a specific distribution form, making it more flexible and capable of handling a broader range of distributions.

Various studies have attempted to estimate feature distributions using Gaussian Mixture Models (GMMs), such as in the case of class-aware Fréchet distance (CAFD) \cite{cafd2018}. Similarly, Wasserstein-GMM (WaM) \cite{luzi2023evaluating} introduces an approximate 2-Wasserstein metric based on the fitted mixture of Gaussian (MoG) distributions for both real and generated data. Furthermore, Feature Likelihood Divergence (FLD) \cite{jiralerspong2023feature} estimates the Kullback-Leibler divergence (KLD) between learned MoG distributions, claiming to capture the novelty, fidelity, and diversity of generated samples. Although KLD is not a true distance due to its lack of symmetry and violation of the triangle inequality, it is still included in this section for brevity. 




\subsection{Feature Extractor Variants}\label{sec:feature}

In addition to Inception-v3 \cite{szegedy2016rethinking}, recent advancements in foundation models, such as the vision-language model CLIP \cite{radford2021learning} and the self-supervised vision foundation model DINOv2 \cite{oquab2024dinov}, provide more powerful and general alternatives for generative feature extraction. Moreover, Fréchet AutoEncoder Distance (FAED) \cite{buzuti2023frechet} leverages the latent features from a VQ-VAE \cite{van2017neural}.

As a special case of the feature extractor variants, it is natural to consider using a modality-specific model for feature extraction in biomedical generative evaluation. The expectation is that a modality-specific feature extractor would produce better features than a general model. However, as demonstrated in \cite{woodland2024feature}, pretraining on a radiology image dataset RadImageNet \cite{mei2022radimagenet} leads to a poorer correlation with human judgment of realisticity (without evaluating downstream performance) compared to a model pretrained on ImageNet \cite{deng2009imagenet}.
In this study, we adopt RETFound \cite{zhou2023foundation}, a foundation model pretrained on retinal images using masked autoencoders \cite{he2022masked}, as the feature extractor to assess the pragmatic reliability of modality-specific feature-distance metrics with respect to the downstream performance.


\subsection{Other Related Metrics}

Several other generative evaluation metrics complement FID and assess various aspects of model performance. 
The Peak Signal-to-Noise Ratio (PSNR) is a traditional metric that evaluates image quality based on pixel-level differences, though it does not account for perceptual aspects of image quality.
To address this, the Structural Similarity Index Measure (SSIM) \cite{wang2004image} considers luminance, contrast, and structure in its comparison, providing a more perceptually meaningful measure of similarity between generated and real images. Another widely used metric is the Inception Score (IS) \cite{salimans2016improved}, which, similarly to FID, leverages a pretrained Inception network to assess the diversity and quality of generated images based on classification confidence, though it has been critiqued for not fully addressing issues like mode collapse. 
Unbiased FID \cite{chong2020effectively} introduces a bias-free metric
by extrapolating FID scores to an infinite sample set, and shows that Quasi-Monte Carlo integration improves the estimation of FID for finite samples.
Finally, the Clean FID \cite{parmar2022aliased} metric has been introduced to improve upon FID by addressing aliasing artifacts that can arise from low-level image quantization and resizing, enhancing its robustness and comparability.

\subsection{Demonstrated Metrics}
In this paper, we report seven diverse generative evaluation metrics, covering Fréchet distance (FID, Clean-FID, CLIP-FD, RETFound-FD), MMD (KID, CMMD), KLD (FLD) and feature extractors, including ImageNet pretrained Inception-v3 (FID, Clean-FID, KID), CLIP (CLIP-FD, CMMD), DINOv2 (FLD), RETFound (RETFound-FD). In our experiments, all metrics are calculated between the generated data and the unseen test data for the downstream task, except for FLD, which uses both the training and test sets.


\section{Experiments}\label{sec:task}

\subsection{Color Fundus Photography}

\noindent{\textbf{Dataset.}} 
We utilize the AIROGS dataset \cite{de2023airogs}, which consists of approximately 101,000 color fundus images, labeled as \textit{no referable glaucoma} (NRG) and \textit{referable glaucoma} (RG). The dataset is split into training and test sets at an 80:20 ratio. Specifically, the training set contains 78,537 NRG and 2,616 RG images, while the test set contains 19,635 NRG and 654 RG images.

\noindent{\textbf{Image Synthesis.}} 
Two deep generative models are employed for realistic fundus image synthesis: the advanced generative adversarial network StyleGAN3 \cite{karras2021alias} and the medical image-specific latent diffusion model Medfusion \cite{muller2023multimodal}. StyleGAN3 is trained only on RG fundus images to generate new RG samples. To verify that the generated images are indeed RG, we trained a binary classifier (distinguishing RG from NRG) that achieves 93.2\% accuracy on the generated images. We then select ten checkpoints based on the decreasing FID against the full dataset (i.e., \texttt{fid50k\_full} in \cite{karras2021alias}), with FID values of \{194, 149, 118, 87, 57, 46, 37, 25, 16, 6\}, denoted as SG-1 to SG-10. Diffusion models offer a more convenient way to obtain generative models of varying qualities corresponding to the sampling steps \(t\) compared to GANs with different checkpoints. For Medfusion, we select seven models with varying \(t\) from \{5, 10, 15, 25, 75, 150, 250\}, denoted as MF-1 to MF-7. In both synthesis approaches, 75,921 RG fundus images are synthesized to supplement the imbalanced dataset. When calculating generative evaluation metrics, however, we use 6,000 synthetic images, which is a modest number compared to the real test set size of 654 RG images.

\noindent{\textbf{Downstream Task.}} The downstream task involves binary classification with class imbalance, where the minor class (RG) is augmented with synthesized data. Two widely used architectures are adopted: ResNet-50 \cite{he2016deep} and Swin Transformer Tiny (Swin-T) \cite{liu2021swin}. The F1 score according to the referable glaucoma class is calculated to highlight the low recall for RG, with imbalanced baseline F1 scores of 64.57\% for ResNet-50 and 63.73\% for Swin-T (cf. Fig.~\ref{fig:plot}).

\subsection{Optical Coherence Tomography}

\noindent{\textbf{Dataset.}} 
We utilize the dataset from the GOALS Challenge~\cite{fang2022dataset}, consisting of 100 pixel-wise labeled circumpapillary Optical Coherence Tomography (OCT) images, split 50:50 for training and test. Three layers are annotated on the OCT scans, namely the retinal nerve fiber layer (RNFL), the ganglion cell-inner plexiform layer (GCIPL), and the choroid layer (CL). 


\noindent{\textbf{Image Synthesis.}} 
Following \cite{eschweiler2024denoising,wu2024retinal}, we employ a denoising diffusion probabilistic model (DDPM) to generate realistic retinal OCT images with a sketch from a processed segmentation mask, which enables the generation of fully annotated synthetic data. Similarly, we select seven diffusion models according to increasing sampling steps $t$ of \{100, 150, 200, 250, 300, 350, 400\}, denoted as DM-1 to DM-7. Layer statistics from 50 real OCT images are applied as priors to generate sketches, producing 200 synthetic OCT images as the new training set. 

\noindent{\textbf{Downstream Task.}} 
We focus on the layer segmentation task using two well-performing architectures: U$^2$-Net \cite{qin2020u2} and TransUNet \cite{chen2024transunet}.
Following \cite{fang2022dataset}, Dice scores for the test set segmentations of three retinal layers are computed using weights of 0.4, 0.3, and 0.3 for RNFL, GCIPL, and CL, respectively.




\begin{figure}[t]
    \centering
    \begin{minipage}{0.37\textwidth}
        \centering
        \includegraphics[height=3.3cm]{plot1.pdf}
        \subcaption{}\label{fig:plot1}
    \end{minipage}
    \begin{minipage}{0.52\textwidth}
        \centering
        \includegraphics[height=3.3cm]{plot2.pdf}
        \subcaption{}\label{fig:plot2}
    \end{minipage}\\
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[height=3.5cm]{plot3.pdf}
        \subcaption{}\label{fig:plot3}
    \end{minipage}
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[height=3.5cm]{plot4.pdf}
        \subcaption{}\label{fig:plot4}
     \end{minipage}
    \caption{(a) Feature sparsity and entropy across different models. (b)-(d) Comparison of downstream performance with the reciprocal of FID.}
    \label{fig:plot}
    \end{figure}


\section{Results}\label{sec:result}

\subsection{Feature Sparsity and Entropy}
We begin with analyzing the sparsity with approximated L0 norm and the entropy of feature vectors from DDPM-generated OCT images in Fig.~\ref{fig:plot1}. 
On the raw feature vectors, we count absolute values above a threshold 0.01 and compute the relative L0 norm with respect to dimensionality. The features from DINOv2 \cite{oquab2024dinov} show the lowest sparsity, while the ImageNet pretrained Inception-v3 \cite{deng2009imagenet,szegedy2016rethinking} yields the most sparse feature vectors. 
Moreover, entropy (in nats) is calculated on the probability vectors, derived by applying a sigmoid function and normalizing the feature vectors ($\mathrm{sum} = 1$).
Lower entropy suggests that the feature vectors carry less information, with more values concentrated in specific dimensions, as seen with Inception \cite{deng2009imagenet,szegedy2016rethinking}. 
In contrast, higher entropy indicates a more even distribution of information across dimensions, with CLIP \cite{radford2021learning} exhibiting the highest entropy among four models. 


\begin{table}[t!]
    \centering
        \caption{Generative evaluation metrics for fundus image synthesis with StyleGAN3 (SG) \cite{karras2021alias} and Medfusion (MF) \cite{muller2023multimodal}. Kendall's $\tau$ coefficients and $p$-values are reported for each metric in relation to the mean F1 score of ResNet-50 \cite{he2016deep} and Swin-T \cite{liu2021swin}. Note that $\tau = 1$ in the worst case and $\tau = -1$ in the best case.\vspace{0.5em}}
    \label{tab:fundus}
    \begin{tabular}{L{3.7em} R{4.5em}R{4em}R{4em}C{6em} C{4em}C{4.2em}R{3em}}
    \toprule
      \multirow{2}{*}[-0.2em]{Models}   &  \multicolumn{4}{C{18.5em}}{Fréchet Distance} & \multirow{2}{*}[-0.2em]{KID}  & \multirow{2}{*}[-0.2em]{CMMD} & \multirow{2}{*}[-0.2em]{FLD} \\\cmidrule{2-5}
      & Inception  & Clean  & CLIP & \;RETFound & & & \\\midrule
       SG-1  &  175.99 & 173.94 & 53.19 &   41.77 & 0.2073 & 5.165 & 106.45 \\
       SG-2  & 121.59 & 121.15 & 34.47 &   61.02 & 0.1391& 2.980 & 92.67 \\
        SG-3    &   95.29 & 93.20 & 23.45  & 33.64  & 0.1009 & 1.975 & 80.68 \\
        SG-4 & 69.70 & 68.66 & 15.88 &   33.38 & 0.0662 & 1.563 & 73.42 \\
        SG-5 & 49.45 & 47.22 & 7.76 &   23.51 & 0.0402 & 0.986 &  60.07 \\
        SG-6 & 39.17 & 36.17 & 5.91 &    19.14 & 0.0277 & 0.736 & 47.66 \\
        SG-7 & 30.92 & 28.70 & 5.33 &    15.12 & 0.0151 & 0.619 & 35.01 \\
        SG-8 & 24.83 & 23.66 & 4.81 &    14.82 & 0.0111 & 0.489 & 31.29 \\
       SG-9  & 21.11 & 20.09 & 4.60 &    11.25 & 0.0089 &0.440 & 26.56 \\
       SG-10  & 17.30 & 16.69 & 4.02 &    \; 9.26 & 0.0063 & 0.421 &  21.76 \\\midrule
      $\tau_{\,\mathrm{Kendall}} $ & 0.69 & 0.69 & 0.69 & \; 0.64 & 0.69 & 0.69 & 0.69 \\
       $p_{\,\mathrm{Kendall}}$ & $\ast \ast$ \, & $\ast \ast$ \, & $\ast \ast$ \, & \; $\ast \ast$ & $\ast \ast$ & $\ast \ast$ & $\ast \ast$\,\, \\\midrule\midrule
       MF-1  & 148.82 & 145.51 & 37.41 &  50.05  & 0.1632 & 1.537  &  67.72 \\
       MF-2  & 105.81 & 102.56 & 17.66 &  41.65  & 0.1109 & 0.735 &  53.80 \\
       MF-3  & 91.28 & 88.43 & 12.53 & 38.87   & 0.0940 & 0.631 &  52.70 \\
       MF-4  & 80.00 & 77.60 & 9.51 &  36.42  & 0.0823 & 0.573 & 45.86  \\
       MF-5  & 68.91 & 67.91 & 6.52  & 34.40   & 0.0740 & 0.558 &  41.36 \\
       MF-6  & 67.76 & 67.07 & 6.01 & 34.60   & 0.0741  & 0.562 &   42.54\\
       MF-7  & 68.00 &67.55  & 5.94 &  35.20  & 0.0749 & 0.567 & 40.95  \\\midrule
     $\tau_{\,\mathrm{Kendall}}$ & -0.24 & -0.24 & -0.33 & -0.24 & -0.24 & -0.25 & -0.43 \\
       $p_{\,\mathrm{Kendall}}$ & n.s. & n.s. & n.s. & \; n.s. & \; n.s. & \;   n.s. & n.s. \\
       \bottomrule
    \end{tabular}
\end{table}

\subsection{Consistent Trends of Metrics}\label{sec:consist}
Table~\ref{tab:fundus} and Table~\ref{tab:ddpm} report seven generative evaluation metrics for two retinal modalities and downstream tasks across 24 models, including StyleGAN3 and two diffusion models. To assess the consistency between these metrics, we compute the Kendall’s $\tau$ coefficient for all metric pairs, resulting in 63 pairs (21 metric pairs across 3 generative approaches, not listed due to space limitations). Almost all of these pairs, except one, exhibit a Kendall’s $\tau$ coefficient greater than 0.5, with the majority (78\%) showing a $\tau$ coefficient above 0.7, indicating a strong correlation among these \textit{feature-distance} generative evaluation metrics.
    
\subsection{Misaligned FID and Downstream Performance}
Kendall’s $\tau$ coefficients and $p$-values are provided in Table~\ref{tab:fundus} and Table~\ref{tab:ddpm} for each generative evaluation metric (the lower the better) and their correlation with downstream task performance (F1 or Dice, the higher the better). A $\tau = 1$ indicates negative correlation between the generative model's evaluation and downstream performance, while $\tau = -1$ suggests an ideal generative evaluation metric. The results show that for diffusion models, these metrics fail to capture downstream performance, as indicated by the non-significant $p$-values (n.s. when $p \ge 0.05$). More critically, for StyleGAN3, the metrics predict performance in the opposite direction with $0.001 \le p < 0.01$, denoted as $\ast \ast$.
We also illustrate the downstream performance with $1/\text{FID}$ to better depict the relationship in Fig.~\ref{fig:plot}b-d. No clear correlation is observed across the three plots, highlighting the unreliability of FID (and, by extension, the other six metrics due to their consistency, as discussed in Section~\ref{sec:consist}) for evaluating generative models in the context of a downstream task.








\begin{table}[t]
    \caption{Generative evaluation metrics for optical coherence tomography synthesis with DDPM \cite{ho2020denoising}. Kendall's $\tau$ coefficients and $p$-values are reported for each metric in relation to the mean Dice score of U$^2$-Net \cite{qin2020u2} and TransUNet \cite{chen2024transunet}.\vspace{0.5em}}
    \label{tab:ddpm}
        \centering
    \begin{tabular}{L{3.7em} R{4.5em}R{4em}R{4em}C{6em} C{4em}C{4.2em}R{3em}}
    \toprule
      \multirow{2}{*}[-0.2em]{Models}   &  \multicolumn{4}{C{18.5em}}{Fréchet Distance} & \multirow{2}{*}[-0.2em]{KID}  & \multirow{2}{*}[-0.2em]{CMMD} & \multirow{2}{*}[-0.2em]{FLD} \\\cmidrule{2-5}
      & Inception  & Clean  & CLIP & \; RETFound & & & \\\midrule
       DM-1  &  212.67 & 232.04  & 13.15 & 54.87  & 0.2578 & 1.005 & 38.74 \\
       DM-2  & 174.95 & 192.75 & 10.73  & 45.79 & 0.1925 & 0.795  & 29.65\\
       DM-3  & 177.17  & 197.43 & 10.33 & 44.08 & 0.1958 & 0.884  & 32.84  \\
       DM-4  & 146.87 & 167.19  &9.37  & 44.71 & 0.1503 & 0.972  &  22.01 \\
       DM-5  & 171.27 & 184.88 & 8.81 &  33.15  & 0.1860  & 0.775  & 25.58 \\
       DM-6  & 138.07 & 156.05 & 6.69  &  28.54  & 0.1390 &  0.748 &  29.56\\
       DM-7  & 126.42 & 142.63 & 6.33 & 34.32 & 0.1052 & 0.698  &   17.91\\\midrule
       $\tau_{\,\mathrm{Kendall}}$ & -0.14 & -0.14 & -0.14 & -0.24 & -0.14 & -0.05 & -0.33 \\
       $p_{\,\mathrm{Kendall}}$ & n.s. & n.s. & n.s. & n.s. & n.s. & n.s. & n.s. \\\bottomrule
    \end{tabular}
\end{table}

\section{Conclusion}

In this study, we demonstrate with three generative models across two retinal imaging modalities that Fréchet Inception Distance (FID) and other feature-distance metrics do not align with downstream performance when generated data is pragmatically used to augment the training dataset. These metrics fail to accurately capture the effectiveness of synthetic data in improving real-world downstream tasks. We recommend that researchers prioritize downstream task performance as the primary evaluation metric for generative models used in data augmentation. Future work should focus on developing a more reliable proxy metric, potentially coupled with data-centric learning, that better correlates with downstream performance, offering computational efficiency compared to direct evaluation on the downstream task.

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%

\begin{credits}
\subsubsection{\ackname}
This work was supported by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) with the grant GRK2610: InnoRetVision (YW, HK, project number 424556709).

\subsubsection{\discintname}
The authors have no competing interests to declare that are relevant to the content of this article.
\end{credits}

\bibliographystyle{splncs04}
\bibliography{bib.bib}

\vspace{5em}
\section*{Appendix}
\vspace{-1em}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{heatmap.pdf}
    \caption*{Detailed Kendall's $\tau$ coefficients and $p$-values.}
\end{figure}

\end{document}
