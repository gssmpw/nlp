\newcommand{\beginsupplement}{%
	\setcounter{table}{0}
	\renewcommand{\thetable}{A\arabic{table}}%

         \setcounter{equation}{0}
	\renewcommand{\theequation}{A\arabic{equation}}%
	
	\setcounter{figure}{0}
	\renewcommand{\thefigure}{A\arabic{figure}}%
	
	\setcounter{algorithm}{0}
	\renewcommand{\thealgorithm}{A\arabic{algorithm}}%
	
	\setcounter{section}{0}
	\renewcommand{\thesection}{A\arabic{section}}%

    \setcounter{theorem}{0}
    \renewcommand{\thetheorem}{A\arabic{theorem}}%

    % \setcounter{corollary}{0}
    % \renewcommand{\thecorollary}{A\arabic{corollary}}%

    %     \setcounter{lemma}{0}
    % \renewcommand{\thelemma}{A\arabic{lemma}}%

    % Add any other environments as needed
}
\beginsupplement

\appendix


\section{Identifiability Theory}
\label{ap_sec:theory}

Without loss of generality, we first consider the case where $\text{Pa}(z_{t,i}^s) = \rvz_{t-1}^s$, meaning that the time-dependent effects are governed by the dynamics of the previous time step.

\begin{theorem}[Blockwise Identifiability]
    \label{ap_th: block-wise}
    Consider video observation $V = \{\rvx_1, \rvx_2, \dots, \rvx_T\}$ generated by process $(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)$ with latent variables denoted as $\rvz_t^s$ and $\rvz_c$, according to Equation~\ref{eq:generation}, where $\rvx_t\in\mathbb{R}^{n_x},\rvz_t^s\in\mathbb{R}^{n_s},\rvz^c\in\mathbb{R}^{n_c}$. If assumptions
    \begin{itemize}
        \item B1 (Positive Density) the probability density function of latent variables is always positive and bounded;
        % \item B2 (Minimal Changes) for any fixed value of noise $\epsilon_t$ at time step $t$, the mapping from $\rvz_{t-1}^s$ to $\rvz_t^s$ must be injective;
        \item B2 (Minimal Changes) the linear operators $L_{\rvx_{t+1} \mid \rvz_t^s,\rvz^c}$ and $L_{\rvx_{t-1} \mid \rvx_{t+1}}$ are injective for bounded function space; 
        \item B3 (Weakly Monotonic) for any $\dot{\mathbf{z}}_{t}, \ddot{\mathbf{z}}_{t} \in \mathcal{Z}^c\times \mathcal{Z}^s_t$ $(\dot{\mathbf{z}}_{t} \neq  \ddot{\mathbf{z}}_{t})$, the set $\{ \mathbf{x}_t : p (\mathbf{x}_t|\dot{\mathbf{z}}_t) \neq p (\mathbf{x}_t|\ddot{\mathbf{z}}_t) \}$ has positive probability,
        and conditional densities are bounded and continuous;
    \end{itemize}
    are satisfied, then $\rvz_t$ is blockwisely identifiable with regard to $\hat{\rvz}_t$ from learned model $(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)$ under Observation Equivalence.
\end{theorem}

\begin{proof}

    We first prove the monoblock identification of $\rvz_{t}^s,\rvz^c = h(\hat{\rvz}_{t}^s,\hat{\rvz}^c)$, then we prove the blockwise identification
    $\rvz^c = h_c(\hat{\rvz}^c)$.

    \textbf{Monoblock Identification.} Following \citep{hu2012nonparametric,hu2008instrumental}, when assumptions B1, B2, B3 satisfied, the blockwise identifiability of $[\rvz_{t}^s,\rvz^c]$ is assured, according to Theorem 3.2(Monoblock identifiability) in \citep{fu2025identification}. In short ,there exists a invertible function $g$ such that $[\rvz_{t}^s,\rvz^c] = h(\hat{\rvz}_{t}^s,\hat{\rvz}^c)$, where $[\cdot]$ denotes concatenation.

    \textbf{Identification of $\rvz^c$.}   We prove this by contradiction. Suppose that for any $\hat{\rvz}^c$, we have  
    \begin{equation}
        \rvz^c = h_c(\hat{\rvz}_t^s, \hat{\rvz}^c),
    \end{equation}  
    where there exist at least two distinct values of $\hat{\rvz}_t^s$ such that $\rvz^c$ takes different values.

    
    When observational equivalence holds, the function remains the same for all values of $t$:  
    \begin{equation}
         h_c(\hat{\rvz}_t^s, \hat{\rvz}^c) = g_c^{-1}(\rvx_t) = (g_c^{-1} \circ \hat{g})(\hat{\rvz}_t^s, \hat{\rvz}^c),
    \end{equation}  
    where $g_c^{-1}$ first demix $\rvx_t$ then extract the content part, as defined in Equation~\ref{eq: blockwise def}.
    
    For any two distinct $t \neq t'$, we have  
    \begin{equation}
    \label{ap_eq: equation of zc}
          h_c(\hat{\rvz}_{t}^s, \hat{\rvz}^c) = \rvz^c = h_c(\hat{\rvz}_{t'}^s, \hat{\rvz}^c),
    \end{equation}  
    which holds for all pairs $(\hat{\rvz}_{t}^s, \hat{\rvz}_{t'}^s)$ within the domain of definition.  
    
    According to Assumption B1, the joint distribution $p(\hat{\rvz}_{t}^s, \hat{\rvz}_{t'}^s)$ is always positive. Thus, to satisfy Equation~\ref{ap_eq: equation of zc}, $\hat{\rvz}_t^s$ must not contribute to $\rvz^c$ through $h_c$. In other words, we obtain 
    \begin{equation}
    \label{ap_eq: identifiability of zc}
        \rvz^c = h_c(\hat{\rvz}^c).
    \end{equation}  

    % \textbf{Identification of $\rvz_t^s$.}
    % We prove this by contradiction as well. Suppose $\rvz^s_t = h_s(\hat{\rvz}_t^s, \hat{\rvz}^c).$
    
    % Suppose that for any $\hat{\rvz}^s_t$, we have  
    % \begin{equation}
    %     \rvz^s_t = h_s(\hat{\rvz}_t^s, \hat{\rvz}^c).
    % \end{equation} 

    
    % When observational equivalence holds, we have:  
    % \begin{equation}
    %      h_s(\hat{\rvz}_t^s, \hat{\rvz}^c) = g_s^{-1}(\rvx_t) = (g_s^{-1} \circ \hat{g})(\hat{\rvz}_t^s, \hat{\rvz}^c),
    % \end{equation}  
    % where $g_c^{-1}$ first demix $\rvx_t$ then extract the content part, as defined in Equation~\ref{eq: blockwise def}.

    % According to the causal graph in Figure~\ref{fig:causal_graph},
    

    
    % According to the causal graph in Figure~\ref{fig:causal_graph},
    % we have
    % \begin{equation}
    % \left\{
    % \begin{aligned}
    %     \begin{bmatrix}
    %         \rvz_c \\ \rvz_t^s
    %     \end{bmatrix}
    %     = 
    %     \begin{bmatrix}
    %         \rvz_c \\ \rvf^s(\rvz_{t-1}^s,\epsilon_t^s)
    %     \end{bmatrix}
    % \end{aligned}
    % \right.
    % \end{equation}

    % For any given $\hat{\rvz}^c=a$, we have  
    % \begin{equation}
    %     h_s(\hat{\rvf}^s(\hat{\rvz}_{t-1}^s)) \Big|_{\hat{\rvz}^c=a}
    %     =
    %     \rvz^s_t  \Big|_{\hat{\rvz}^c=a}
    %     = 
    %     \rvf^s(h_s(\hat{\rvz}_{t-1}^s)) \Big|_{\hat{\rvz}^c=a}.
    % \end{equation}

    
\end{proof}

\begin{theorem}[Component-wise Identifiability]
    \label{ap_th: componenet-wise}
    Consider video observation $V = \{\rvx_1, \rvx_2, \dots, \rvx_T\}$ generated by process $(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)$ with latent variables denoted as $\rvz_t^s$ and $\rvz_c$, according to Equation~\ref{eq:generation}, where $\rvx_t\in\mathbb{R}^{n_x},\rvz_t^s\in\mathbb{R}^{n_s},\rvz^c\in\mathbb{R}^{n_c}$. Suppose assumptions in Theorem~\ref{ap_th: block-wise} hold. If assumptions 
    \begin{itemize}
        \item C1 (Smooth and Positive Density) the probability density function of latent variables is always third-order differentiable and positive;
        \item C2 (Sufficient Changes) let $\eta_{t,i}\triangleq\log p(z^s_{t,i} | \rvz_{t-1}^s)$ and
        \begin{equation} 
            \label{Eq: assumption sufficient}
            \begin{aligned}
            \mathbf{v}_{t,l} 
            \triangleq \Big(
            \frac{\partial^2 \eta_{t,1}}{\partial z_{t,1} \partial z_{t-1,l}}, 
            \cdots,
            \frac{\partial^2 \eta_{t,n_s}}{\partial z_{t,n} \partial z_{t-1,l}} \Big)
            \oplus
            \Big(
            \frac{\partial^3 \eta_{t,1}}{\partial^2 z_{t,1} \partial z_{t-1,l}}, 
            \cdots,
            \frac{\partial^3 \eta_{t,n_s}}{\partial^2 z_{t,n} \partial z_{t-1,l}} \Big)
            ,
            \end{aligned}
        \end{equation}
        for $l \in\{1,2,\cdots,n\}$. For each value of $\rvz_t$, there exists $ 2n_s$ different of values of $z_{t-1,l}$ such that the $2n_s$ vector $\rvv_{t,l}\in\mathbb{R}^{2n_s}$ are linearly independent;
        \item C3 (Conditional Independence) the learned $\hat{\rvz}_t^s$ is independent with $\hat{\rvz}^c$, and all entries of $\hat{\rvz}_t^s$ are mutually independent conditioned on $\hat{\rvz}_{t-1}^s$;
    \end{itemize}
    are satisfied, then $\rvz_t^s$ is component-wisely identifiable with regard to $\hat{\rvz}_t^s$ from learned model $(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)$ under Observation Equivalence.
\end{theorem}

\begin{proof}
    According to Theorem~\ref{ap_th: block-wise}, we have
    \begin{equation}
         [\rvz_t^s,\rvz^c] = h([\hat\rvz_t^s,\hat\rvz^c]),
    \end{equation}
    where $[.]$ denotes the concatenation operation.  The corresponding Jacobian matrix can be formulated as \begin{equation}
        H_t = \begin{bmatrix}
            \frac{\partial \rvz_t^s}{\partial \hat\rvz_t^s} &
            \frac{\partial \rvz^c}{\partial \hat\rvz_t^s} \\
            \frac{\partial \rvz_t^s}{\partial \hat\rvz^c} &
            \frac{\partial \rvz^c}{\partial \hat\rvz^c}
        \end{bmatrix}.
    \end{equation}

    Consider a mapping from $(\rvx_{t-1}, \hat{\rvz}_t^s, \hat{\rvz}^c)$ to $(\rvx_{t-1}, \rvz_t^s, \rvz^c)$ and its Jacobian matrix
    \begin{equation}
        \begin{bmatrix}
        \mathbf{I} & \mathbf{0} \\
        \mathbf{*} & H_t
        \end{bmatrix},
    \end{equation}
    where $*$ stands for any matrix, and the absolute value of the determination of this Jacobian is $|H_t|$. Therefore $p(\rvx_{t-1}, \rvz_t^s, \rvz^c) = p(\rvx_{t-1}, \hat{\rvz}_t^s, \hat{\rvz}^c)/ |H_t|$. Dividing both side by $p(\rvx_{t-1})$ gives
    \begin{equation}
    \begin{aligned}
        &&
        p(\rvz_t^s, \rvz^c|\rvx_{t-1}) 
        & = 
        p(\hat{\rvz}_t^s, \hat{\rvz}^c|\rvx_{t-1})
        / |H_t| \\
        &\Rightarrow&
        p(\rvz_t^s, \rvz^c|g(\rvz_{t-1}^s, \rvz^c))  
        & = 
        p(\hat{\rvz}_t^s, \hat{\rvz}^c|\hat{g}(\hat{\rvz}_{t-1}^s, \hat{\rvz}^c))
        / |H_t| 
        \\
        &\Rightarrow&
        p(\rvz_t^s, \rvz^c|\rvz_{t-1}^s, \rvz^c)  
        & = 
        p(\hat{\rvz}_t^s, \hat{\rvz}^c|\hat{\rvz}_{t-1}^s, \hat{\rvz}^c)
        / |H_t| 
        \\
        &\Rightarrow&
        p(\rvz_t^s|\rvz_{t-1}^s, \rvz^c)  
        & = 
        p(\hat{\rvz}_t^s|\hat{\rvz}_{t-1}^s, \hat{\rvz}^c)
        / |H_t| 
        \\
        &\Rightarrow&
        p(\rvz_t^s|\rvz_{t-1}^s)  
        & = 
        p(\hat{\rvz}_t^s|\hat{\rvz}_{t-1}^s)
        / |H_t| 
        \\
    \end{aligned}.
    \end{equation}
    
    For the first two implications, we utilize the inversion of the mixing function to replace the condition. For the third, since $ \rvz^c $ is conditioned on itself, it remains fixed. For the last one, given that in the generative process, $ \epsilon_{t,i}^s $ and $ \epsilon_{j}^c $ for all $ i, j, t $ are independently sampled, their disjoint successors are also independent, i.e., $ \rvz_t^s \perp \rvz^c $. Similarly, following assumption C3, we have $ \hat{\rvz}_t^s \perp \hat{\rvz}^c $. Thus, we can remove $ \rvz^c $ from the condition.

    For simplicity, denote $\mathbb{\eta}_{t}\triangleq\log p(\rvz^s_{t} | \rvz_{t-1}^s)$ and $\eta_{t,i}\triangleq\log p(z^s_{t,i} | \rvz_{t-1}^s)$ and we have
    \begin{equation}
        \mathbb{\eta}_{t} = \hat{\mathbb{\eta}}_{t} - \log |H_t|. 
    \end{equation}

    For any two different $\hat{z}_{t,i}^s,\hat{z}_{t,j}^s\in\rvz_{t}^s$, in partial derivative with regard to $\hat{z}_{t,i}^s$ gives
    \begin{equation}
    \label{ap_eq: first order}
        \sum_{k=1}^{n_s}
        \frac{\partial \mathbb{\eta}_{t}}{\partial z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}
        = 
        \frac{\partial \hat{\mathbb{\eta}}_{t}}{\partial \hat{z}_{t,i}^s}
        - 
        \frac{\partial \log |H_t|}{\partial \hat{z}_{t,i}^s}.
    \end{equation}

    Reorganize the left-hand side of Equation~\ref{ap_eq: first order} with mutual independence of $\rvz_t^s|\rvz_{t-1}^s$ yields
    \begin{equation}
        \sum_{k=1}^{n_s}
        \frac{\partial \mathbb{\eta}_{t}}{\partial z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}
        =
        \sum_{k=1}^{n_s}
        \frac{\partial \prod_{k'=1}^{n_s}\eta_{t,k'}}{\partial z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}
        =
        \sum_{k=1}^{n_s}
        \frac{\partial \eta_{t,k}}{\partial z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s},
    \end{equation}
    and we have
    \begin{equation}
        \sum_{k=1}^{n_s}
        \frac{\partial \eta_{t,k}}{\partial z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}
        = 
        \frac{\partial \hat{\mathbb{\eta}}_{t}}{\partial \hat{z}_{t,i}^s}
        - 
        \frac{\partial \log |H_t|}{\partial \hat{z}_{t,i}^s}.
    \end{equation}

    Further get the second-order derivative with regard to $\hat{z}_{t,j}^s$ as
    \begin{equation}
        \sum_{k=1}^{n_s}
        \frac{\partial^2 \eta_{t,k}}{\partial^2 z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,j}^s}
        +
        \sum_{k=1}^{n_s}
        \frac{\partial \eta_{t,k}}{\partial z_{t,k}^s}\cdot
        \frac{\partial^2 z_{t,k}^s}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}
        = 
        \frac{\partial^2 \hat{\mathbb{\eta}}_{t}}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}
        - 
        \frac{\partial^2 \log |H_t|}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}.
    \end{equation}

    Next, using the mutual independence of $\hat{\rvz}_t^s|\hat{\rvz}_{t-1}^s$ in assumption C3, we have $\frac{\partial^2 \hat{\mathbb{\eta}}_{t}}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}=0$ according to the connection between conditional independence and cross derivatives \citep{lin1997factorizing}. Thus we have
    \begin{equation}
        \sum_{k=1}^{n_s}
        \frac{\partial^2 \eta_{t,k}}{\partial^2 z_{t,k}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,j}^s}
        +
        \sum_{k=1}^{n_s}
        \frac{\partial \eta_{t,k}}{\partial z_{t,k}^s}\cdot
        \frac{\partial^2 z_{t,k}^s}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}
        = 
        - 
        \frac{\partial^2 \log |H_t|}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}.
    \end{equation}

    Now we get the third-order derivative with regard to any $z_{t-1,l}^s$ as
    \begin{equation}
    \label{ap_eq: always 0}
        \sum_{k=1}^{n_s}
        \frac{\partial^3 \eta_{t,k}}{\partial^2 z_{t,k}^s\partial z_{t-1,l}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,j}^s}
        +
        \sum_{k=1}^{n_s}
        \frac{\partial \eta_{t,k}}{\partial z_{t,k}^s}\cdot
        \frac{\partial^2 z_{t,k}^s}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s}
        = 
        0,
    \end{equation}
    where we use the property that the entries of $H_t$ do not depend on $z_{t-1,l}^s$.

    Given assumption C2, there exists $2n_s$ different values of $z_{t-1,l}^s$ such that the $2n_s$ vectors $\rvv_{t,l}$ linearly independent. The only solution to Equation~\ref{ap_eq: always 0} is to set 
    \begin{equation}
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,i}^s}\cdot
        \frac{\partial z_{t,k}^s}{\partial \hat{z}_{t,j}^s} = 0, 
        \frac{\partial^2 z_{t,k}^s}{\partial \hat{z}_{t,i}^s\partial \hat{z}_{t,j}^s} =0.
    \end{equation}

    According to Theorem~\ref{ap_th: block-wise}, the blockwise identifiability is established. Thus, 
    \begin{equation}
        H_t = \begin{bmatrix}
            \frac{\partial \rvz_t^s}{\partial \hat\rvz_t^s} &
            \frac{\partial \rvz^c}{\partial \hat\rvz_t^s} \\
            \frac{\partial \rvz_t^s}{\partial \hat\rvz^c} &
            \frac{\partial \rvz^c}{\partial \hat\rvz^c}
        \end{bmatrix}
    \end{equation}
    is invertible, with $\frac{\partial \rvz^c}{\partial \hat\rvz_t^s}=0$ and $\frac{\partial \rvz_t^s}{\partial \hat\rvz_t^s}$ has at most one nonzero element in each row and each column.

    Thus, we have
    \begin{equation}
        H_t = \begin{bmatrix}
            \frac{\partial \rvz_t^s}{\partial \hat\rvz_t^s} &
            0 \\
            \frac{\partial \rvz_t^s}{\partial \hat\rvz^c} &
            \frac{\partial \rvz^c}{\partial \hat\rvz^c}
        \end{bmatrix}
    \end{equation} and $\frac{\partial \rvz_t^s}{\partial \hat\rvz_t^s}$ must have one and only one non-zero entry in each column and row.
    
    % Since the mixing process $g$ is invertible, each $z_{t,i}^s$ is a function of a unique entry of $\hat{\rvz}_t^s$, and the component-wise identifiability is established.

    

\end{proof}

\section{Datasets details}
% \subsection{Real-World Datasets}
\label{ap_sec: dataset details}
FaceForensics~\cite{rossler2018faceforensics} is a forensics dataset consisting of 1000 original video sequences all videos contain a trackable mostly frontal face without occlusions. SkyTimelapse~\cite{xiong2018learning} typically consists of sequential images or videos capturing the dynamic behavior of the sky over time.

RealEstate10K~\cite{zhou2018stereo} is a large dataset of camera poses corresponding to 10 million frames derived from about 80,000 video clips, gathered from about 10,000 YouTube videos. For each clip, the poses form a trajectory where each pose specifies the camera position and orientation along the trajectory. These poses are derived by running SLAM and bundle adjustment algorithms on a large set of videos. To the best of our knowledge, the proposed method is the first GAN-based approach to leverage the complex camera pose dataset for validating unconditional video generation. 
% In our experiments, we use a subset of RealEstate10K, and the dataset statistics are shown in Figure~\ref{fig:sup_realestate_statistics}.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figs/sup_realestate_statistics.png} 
%     \caption{Distributions of camera intrinsics (focal length and principle point), and camera pose(transition movements and Euler angles).}
%     \label{fig:sup_realestate_statistics}
% \end{figure}

% \begin{table}[ht]
%     \centering
%     \caption{Additional datasets information in terms of frame rate and the amount of speakers.}
%     \begin{tabular}{lcc}
%         \toprule
%         Dataset & FPS & \#speakers \\
%         \midrule
%         FaceForensics   & 25 & 1000 \\
%         SkyTimelapse    & 25 & N/A \\
%         RealEstate10K   & 60 & N/A \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:fps}
% \end{table}


% \subsection{Synthetic Dataset}
% Our synthetic dataset includes 10,000 videos created and rendered using Blender~\cite{blender}. Each video features a same object starting at the center of the frame, moving outward in various directions, such as forward, backward, left, right, up, down. The dataset is designed to support research in motion understanding and disentangled representation learning, with high-quality rendering and consistent temporal motion across frames. This benchmark is advantageous compared to the real-world datasets in the following ways:
% a) The included types of motion (such as up and down, left and right, forward and backward) have intuitive significance, allowing for better analysis of methods' ability to learn disentangled representations.
% b) It is simple in terms of content, which makes the benchmark more focused on motions.
% % \end{enumerate}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figs/sup_mobilebag_dataset.png} 
%     \caption{Examples of MobileBag Dataset.}
%     \label{fig:sup_realestate_statistics}
% \end{figure}

\section{Reproductability}
All of our models are trained on an NVIDIA A100 40G GPU. For the baseline models, we use the official implementation with the default hyperparameters.
The configuration of hyperparameters for our model training is as shown in Table ~\ref{tab:model_config}.
\begin{table}[htbp]
    \centering
    \caption{Configuration details for the model and training setup.}
    \label{tab:model_config}
    \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l l l}
        \toprule
        \textbf{Name} & \textbf{Variable Name} & \textbf{Value/Description} \\ 
        \midrule
        Information regularization term weight & lambda\_KL & 1 \\ 
        Dimensionality of $\rvz^c$ & - & 512 \\ 
        Dimensionality of GRU & - & 64 \\ 
        Dimensionality of $\rvz_s^t$ & - & FaceForensics: 4, SkyTimelapse: 12, RealEstate: 12 \\ 
        Batch size (per GPU) & batch & 16 \\ 
        Conditional mode & cond\_mode & flow \\ 
        Flow normalization & flow\_norm & 1 \\ 
        Sparsity weight & lambda\_sparse & 0.1 \\ 
        Number of input channels & channel & 3 \\ 
        % \midrule
        Number of mapping layers & num\_layers & 8 \\ 
        Label embedding features & embed\_features & 512 \\ 
        Intermediate layer features & layer\_features & 512 \\ 
        Activation function & activation & lrelu \\ 
        Learning rate multiplier & lr\_multiplier & 0.01 \\ 
        Moving average decay & w\_avg\_beta & 0.995 \\ 
        % \midrule
        Discriminator architecture & - & resnet \\ 
        Channel base & channel\_base & 32768 \\ 
        Maximum number of channels & channel\_max & 512 \\ 
        \bottomrule
    \end{tabular}
    }
\end{table}

The architecture of our proposed Temporal Transition Module is shown in Table~\ref{tab:mapping_network}.


\section{More experiments}
\subsection{Ablation Study}
An ablation study is conducted to assess the contribution of our proposed modules in the Temporal Transition Module, as shown in Table~\ref{tab:ablation}.
\begin{table}[htbp]
    \centering
    \caption{Architecture of Temporal Transition Module}
    \label{tab:mapping_network}
    \begin{tabular}{l c}
        \toprule
        \textbf{Component} & \textbf{Structure} \\ 
        \midrule
        MappingNetwork.gru & $4 \times 256 \times 3$  \\ 
        MappingNetwork.h\_to\_c & FullyConnectedLayer ($256 \times 64$) \\ 
        MappingNetwork.embed & FullyConnectedLayer ($64 \times 512$) \\ 
        MappingNetwork.flow.model.0 & DenseSigmoidFlow \\ 
        MappingNetwork.flow.model.1 & DenseSigmoidFlow \\ 
        MappingNetwork.flow\_fc0 & FullyConnectedLayer ($512 \times 512$) \\ 
        MappingNetwork.flow\_fc1 & FullyConnectedLayer ($512 \times 512$) \\ 
        MappingNetwork.flow\_fc2 & FullyConnectedLayer ($512 \times 284$) \\ 
        FullyConnectedLayer & FullyConnectedLayer ($512 \times 512$) \\ 
        FullyConnectedLayer & FullyConnectedLayer ($512 \times 512$) \\ 
        \bottomrule
    \end{tabular}
\end{table}
Replacing the GRU with an RNN results in a performance decline, as the sparsity of time-delayed effects—enabled by the gating mechanism—vanishes. This makes it more challenging for the model to learn a true transition function, as it must navigate a larger search space without sparsity constraints.

Replacing the component-wise flow with a fully connected MLP leads to an even greater performance drop in FVD because (1) the mutual independence between style dynamics can no longer be maintained, and (2) capturing sufficient changes becomes more difficult.

\label{sec: ablation study}
\begin{table}[t]
    \centering
    \resizebox{0.4\linewidth}{!}{
    \begin{tabular}{lccc}
        \toprule
         & \textbf{w/ GRU, w/ Flow} & \textbf{w/o GRU} & \textbf{w/o Flow}   \\
        \midrule
        $\text{FVD}_{16}$ & \textbf{48.80} & 53.68 & 82.81  \\
        \bottomrule
    \end{tabular}
    }
    \caption{Ablation study results for different configurations of GRU and component-wise flow on the FaceForensics dataset.}
    \label{tab:ablation}
\end{table}

\subsection{More Videos}
In this section, we provide more qualitative video results generated by our approach. As can be seen in \ref{fig:control_sky}, our method can control different identities of sky scenes with consistent constructed motions.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/ControlSky.pdf} 
    \caption{Generate same motion of different identities on SkyTimelapse Dataset.}
    \label{fig:control_sky}
\end{figure}


% We show the thumbnail from each video in the figures on RealEstate Dataset in Figure\ref{fig:re}.
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figs/re.png} 
%     \caption{Generated samples on RealEstate Dataset.}
%     \label{fig:re}
% \end{figure}
