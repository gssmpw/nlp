%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[mathscr]{eucal}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,lipsum}
\usepackage{amssymb} 
\usepackage{wrapfig}
\usepackage{ulem}

\usepackage{tabularx}
% \usepackage{ctex}
\usepackage{cuted}%%\stripsep-3pt
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}

\usepackage[english]{babel}
% \newtheorem{proposition}{Proposition}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}
% \newtheorem{lemma}{Lemma}
% \newtheorem{definition}{Definition}
% % \newtheorem{corollary}{Corollary}
% \newtheorem{assumption}{Assumption}
% \setlength{\floatsep}{5pt plus 2pt minus 2pt}
% \setlength{\textfloatsep}{5pt plus 2pt minus 2pt}
% \setlength{\intextsep}{5pt plus 2pt minus 2pt}
\usepackage{subfiles}
\usepackage{xr}
% \definecolor{myblue}{HTML}{b2f0ff}
% \definecolor{myblue2}{HTML}{cef5ff}
% \definecolor{myblue3}{HTML}{e7faff}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\cgy}[1]{\textcolor{blue}{cgy: #1}}
\newcommand{\zjl}[1]{\textcolor{red}{zjl: #1}}

\newcommand{\ourmes}{CoVoGAN}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\input{math_commands}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Controllable Video Generation with Provable Disentanglement}

\begin{document}

\twocolumn[
% \icmltitle{Causally Hierarchical Latent Dynamic is a Good Time Series Generator}

\icmltitle{Controllable Video Generation with Provable Disentanglement}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yifan Shen}{equal,mbzuai}
\icmlauthor{Peiyuan Zhu}{equal,mbzuai}
\icmlauthor{Zijian Li}{mbzuai}
\icmlauthor{Shaoan Xie}{cmu}
\icmlauthor{Zeyu Tang}{cmu}
\icmlauthor{Namrata Deka}{cmu}
\icmlauthor{Zongfang Liu}{mbzuai}
\icmlauthor{Guangyi Chen}{mbzuai,cmu}
\icmlauthor{Kun Zhang}{mbzuai,cmu}
\end{icmlauthorlist}

\icmlaffiliation{mbzuai}{Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE}
\icmlaffiliation{cmu}{Carnegie Mellon University,
Pittsburg, US}

\icmlcorrespondingauthor{Kun Zhang}{kunz1@cmu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% 定义新命令 \aa
\newcommand{\syf}[1]{\textcolor{blue}{#1}}

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose \textbf{Co}ntrollable \textbf{V}ide\textbf{o} \textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks (\ourmes) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts.
Specifically, following the \textbf{minimal change principle}, we first disentangle static and dynamic latent variables. We then leverage the \textbf{sufficient change property} to achieve component-wise identifiability of dynamic latent variables, enabling independent control over motion and identity. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach.
Building on these theoretical insights, we design a \textbf{Temporal Transition Module} to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence.
To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios.

% We provide an identification theorem as a theoretical guarantee  





% independent short dynamic, resulting in inefficiencies and reduced flexibility \syf{Videos} are inherently composed of multiple evolving concepts, and selectively controlling a subset of these concepts offers an opportunity to achieve more precise and efficient generation. 
% \syf{We propose a novel framework that learns disentangled video representations, allowing for efficient and independent control over individual concepts of the generation process.} 
% Our work establishes a theoretical connection between video generation and the corresponding latent \syf{causal dynamics}, \syf{and achieves an efficiently controllable video generation model.} 


\end{abstract}
\nocite{langley00}
\vspace{-0.7cm}
\section{Introduction}
\vspace{-0.1cm}

% 1. video generation 很重要
% 2. 现有的方法的简单总结，分成xxx类别
% 3. 虽然这些方法取得了瞩目的效果但是xxxx，有xxx问题
% 4. 为了解决这些问题，我提出了xxxx方法。Specifically， 。。。。我们进一步基于ctrl提供了理论分析，表明了模型设计的合理性，实验

% contribution: 
% 1. 首次将因果生成模型引入到视频生成领域，提供了一个新的框架
% 2. 提出了一个xxx模型
% 3. 在实验上验证了有效性
% 4. 解释了之前模型为什么work，为什么不work


% Video generation is a challenging and rapidly evolving task in the field of artificial intelligence and computer vision. This topic has attracted significant attention from both academia and industry, driven by its potential to transform various applications. In particular, controllable video generation has become a key area of focus, as it allows for precise manipulation of video content to meet specific needs. The ability to control factors such as video style, scene dynamics, and object behavior is essential for diverse fields like content creation, artistic expression, advertising, marketing, and medical imaging, where customization and adaptability are crucial for delivering tailored and context-sensitive results.

% \zjl{First paragraph: Video generation (xx) is becoming a significant research area due to its broad range of applications like content creation (x), artistic expression (x), and marketing (x). To meet real-world requirements, one critical aspect of this field is to devise an efficient and controllable generative model.}

Video generation \citep{vondrick2016generating,tulyakov2018mocogan, wang2022internvideo} has become a prominent research focus, driven by its wide-ranging applications in fields such as world simulator \citep{openai2024video}, autonomous driving \citep{wen2024panacea, wang2023drivedreamer}, and medical imaging \citep{li2024endora, cao2024medical}. In particular, controllable video generation \citep{zhang2025moonshot} is essential for advancing more reliable and efficient video generation models.

% p(v)

% video generation 很重要，其中如何control是一个重要问题。解决这个问题的核心是恢复xxxx的联合分布

% 为了解决这个问题，有xxxx，assumption

% 

% \zjl{Second paragraph: To achieve this goal, different methods are proposed in the past decades, which can be categorized by assumption (model architecture).}

% Mathematically, video generation can be formulated as a joint distribution learning problem. The joint distribution $p(V)$ represents the probability density function of a video $V$ with a series of consecutive images $\rvx_t$, where $V = \{\rvx_1, \rvx_2, \dots, \rvx_T\}$. Once the joint distribution is learned, it enables the generation of videos with both high-quality images in each frame (represented by the marginal distribution $p(\rvx_t)$) and coherent, fluent motion across frames (captured by the conditional distribution $p(\rvx_t | \rvx_{<t})$).

Numerous methods have been proposed over the past decade for better video generation. Among these efforts, VideoGAN \citep{vondrick2016generating} is a pioneering approach that treats the video as a $4D$ spatiotemporal block and uses a unified representation (i.e., a multivariate normal distribution in VideoGAN) for generation. Recent methods \citep{ho2022video, zhou2022magicvideo, yang2024cogvideox, opensora} that have demonstrated strong performance can also be classified into this category, with differences in the frameworks (e.g., diffusion models and VAEs) and shape of the representation (e.g., vector or spatiotemporal block). 
However, such a simple representation neglects the intricate spatial-temporal relationships, and limits the precision and efficiency of controllable video generation. For example, videos featuring the same object but under different motions may have vastly different representations. 


% \zjl{while recent advances have achieved good performance}



To address this issue, one intuitive solution is to learn a disentangled representation of the video, within whom the internal relationships are often not considered. \cite{hyvarinen2000independent,tulyakov2018mocogan, yu2022generating, skorokhodov2022stylegan, wei2024dreamvideo} explicitly decompose video generation into two parts: motion and identity, representing dynamic and static information, respectively. This separation allows for more targeted control over each aspect, making it possible to modify the motion independently without affecting the identity. \cite{zhang2025moonshot, shen2023mostganv} leverage attention mechanisms to further disentangle different concepts within the video, enhancing the ability to control specific features with greater precision. \cite{fei2024dysen, lin2023videodirectorgpt} utilize Large Language Models to find the intricate video temporal dynamics within the video and then enrich the scene with reasonable details, enabling a more transparent generative process. These methods are intuitive and effective, yet they lack a solid guarantee of disentanglement, making the control less predictable and potentially leading to unintentional coupling of different aspects of the video.

These limitations of previous approaches motivate us to rethink the paradigm of video generation. Inspired by recent advancements in nonlinear Independent Component Analysis (ICA) \citep{hyvarinen2017nonlinear, khemakhem2020variational, yao2022temporally,hyvarinen2016unsupervised} and the successful applications like video understanding \citep{chen2024caring}, we propose the Controllable Video Generative Adversarial Network (\ourmes) with a Temporal Transition Module plugin. Building upon StyleGAN2-ADA \cite{Karras2020ada}, we distinguish between two types of factors: the dynamic factors that evolve over time, referred to as \textbf{style dynamics}, and the static factors that remain unchanged, which we call \textbf{content elements}. This distinction clarifies the separation between style dynamics (motion), and content elements (identity), allowing for more precise control. By leveraging the minimal change principle, we demonstrate their blockwise identifiability \citep{von2021self,li2024subspace} and find the conditions under which motion and identity can be disentangled, explaining the effectiveness of the previous line of methods that separate motion and identity. In addition, we employ sufficient change property to disentangle different concepts of motion, such as head movement or eye blinking. Specifically, we introduce a flow \citep{rezende2015variational} mechanism to ensure that the estimated style dynamics are mutually independent conditioned on the historical information. Furthermore, we prove the component-wise identifiability of the style dynamics and provide a disentanglement guarantee for the motion in the video.

We conduct both quantitative and qualitative experiments on various video generation benchmarks. For quantitative evaluation, we use FVD \citep{unterthiner2019fvd} to assess the quality of the generated videos. For qualitative analysis, we evaluate the degree of disentanglement by manipulating different dimensions of the latent variables across multiple datasets and comparing the resulting video outputs. Experimental results demonstrate that our method significantly outperforms other GAN-based video generation models with similar backbone structures to \ourmes. Additionally, our method exhibits greater robustness during training and faster inference speed compared to baseline approaches.

\textbf{Key Insights and Contributions} of our research include:
\begin{itemize}
\item We propose a Temporal Transition Module to achieve a disentangled representation, which leverages minimal change principle and sufficient change property.
\item We implement the Module in a GAN, i.e., \ourmes, to learn the underlying generative process from video data with disentanglement guarantees, enabling more precise and interpretable control.
\item To the best of our knowledge, this is the first work to provide an identifiability theorem in the context of video generation. This helps to clarify previous intuitive yet unproven techniques and suggests potential directions for future exploration.
\item Extensive evaluations across multiple datasets demonstrate the effectiveness of \ourmes, achieving superior results in terms of generative quality, controllability, robustness, and computational efficiency.
\end{itemize}
\vspace{-0.5cm}
\section{Related Works}
\vspace{-0.1cm}
\subsection{Controllale Video Generation}
Recent advances in controllable video generation have led to significant progress, with text-to-video (T2V) models \citep{yang2024cogvideox,singer2022make,ho2022video,zhou2022magicvideo,opensora} achieving impressive results in generating videos from textual descriptions. However,
% text prompts alone are insufficient to accurately describe a video, as they lack the dimensionality needed to capture its full complexity compared to the dynamic nature of video. The
effectiveness of the control is highly dependent on the quality of the input prompt, making it difficult to achieve fine-grained control over the generated content. An alternative method for control involves leveraging side information such as pose \citep{tu2024stableanimator, zhu2025champ}, camera motion \citep{yang2024direct, wang2024motionctrl}, depth \citep{liu2024stablev2v, xing2024make} and so on. While this approach allows for more precise control, it requires paired data, which can be challenging to collect. 
Besides, most of the aforementioned alignment-based techniques share a common issue: the control signals are directly aligned with the entire video. This issue not only reduces efficiency but also complicates the task of achieving independent control over different aspects of the video, which further motivates us to propose a framework to find the disentanglement representation for conditional generation.

% As a result, when you generate a video but wish to modify a very small detail within it, you would have to rerun the generation model from scratch, as even minor changes to the prompt can affect the entire video's latent encoding. This issue not only reduces efficiency but also complicates the task of achieving independent control over different aspects of the video. The reliance on this direct alignment makes it difficult to isolate and manipulate specific components of the video without unintentionally altering others.

\subsection{Nonlinear Independent Component Analysis}
Nonlinear independent component analysis offers a potential approach to uncover latent causal variables in time series data. These methods typically utilize auxiliary information, such as class labels or domain-specific indices, and impose independence constraints to enhance the identifiability of latent variables. Time-contrastive learning (TCL) \citep{hyvarinen2016unsupervised} builds on the assumption of independent sources and takes advantage of the variability in variance across different segments of data. Similar Permutation-based contrastive learning (PCL) \citep{hyvarinen2017nonlinear} introduces a learning framework that tell true independent sources from their permuted counterparts. Additionally, i-VAE \citep{khemakhem2020variational} employs deep neural networks and Variational Autoencoders (VAEs) to closely approximate the joint distribution of observed data and auxiliary non-stationary regimes. Recently, \cite{yao2021learning,yao2022temporally} extends the identifiability to linear and nonlinear non-Gaussian cases without auxiliary variables, respectively. CaRiNG \citep{chen2024caring} further tackles the case when the mixing process is non-invertible. Additionally, CITRIS \citep{lippe2022citris, lippe2023causal} emphasizes the use of intervention target data, and IDOL \citep{li2024identification} incorporates sparsity into the latent transition process to identify latent variables, even in the presence of instantaneous effects.

\vspace{-0.3cm}
\section{Problem Setup}
\subsection{Generative Process}

% Let us consider a video $V = \{\rvx_{1}, \rvx_{2}, \cdots, \rvx_{T}\}$ with $T$ consective frames. Each frame $\rvx_{t} \in \mathbb{R}^{n_x}$ is generated via an invertible nonlinear mixing function $g$, which maps a set of latent variables to the observed frame $\rvx_{t}$. The latent variables are decomposed into two distinct parts: $\rvz_{t}^s\in \mathbb{R}^{n_s}$, capturing the \textbf{s}tyle dynamics that e over time, and $\rvz^c\in \mathbb{R}^{n_c}$, representing the \textbf{c}ontent variables that remain constant across all frames in the video. Besides, the latent variables are derived from a stationary, non-parametric, time-delayed causal relation.

Consider a video sequence $ V = \{\rvx_1, \rvx_2, \dots, \rvx_T\} $ consisting of $T$ consecutive frames. Each frame $ \rvx_t \in \mathbb{R}^{n_x} $ is generated via an arbitrary nonlinear mixing function $g$, which maps a set of latent variables to the observed frame $ \rvx_t $. The latent variables are decomposed into two distinct parts: $ \rvz_t^s \in \mathbb{R}^{n_s} $, capturing the \textbf{s}tyle dynamics that evolve over time, and $ \rvz^c \in \mathbb{R}^{n_c} $, encoding the \textbf{c}ontent variables that remain consistent across all frames of the video. Furthermore, these latent variables are assumed to arise from a stationary, non-parametric, time-delayed causal process.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/CausalGraph.pdf} % Replace with your actual graph file name
    \caption{\textbf{The generative process.} The gray shade of nodes indicates that the variable is observable.}
    \label{fig:causal_graph}
    \vspace{-0.5cm}
\end{figure}
% \vspace{-0.5cm}

 As shown in Figure~\ref{fig:causal_graph} and Equation~\ref{eq:generation}, the generative process is formulated as:
\begin{equation}
% \small
\label{eq:generation}
% \begin{aligned}
    \left\{
    \begin{aligned}
        &\rvx_{t} = g(\rvz_{t}^s, \rvz^c), 
        \\  
        &z_{t,i}^s = 
        f^s_i\left(\mathbf{Pa}(z_{t,i}^s), \epsilon_{t,i}^s  \right), 
        \\
        &z_{j}^c = 
        {f^c_j\left(\epsilon_{j}^c  \right)}, 
    \end{aligned}
     \right.
    \text{with} \left\{
    \begin{aligned}
        \epsilon^s_{t,i} \sim p^s_{\epsilon_i}, \\
        \epsilon^c_{j} \sim p^c_{\epsilon_j},
    \end{aligned}
    \right.
    % \end{aligned}
\end{equation}
in which $z_{t,i}^s, z_{i}^c \in \mathbb{R}$ refers to the $i$-th entry of $\rvz_{t}^s$ and $j$-th entry of $\rvz^c$, respectively. $\mathbf{Pa}(z_{t,i}^s)$ refers to the time-daleyed parents of $z_{t,i}^s$. All noise terms $\epsilon_{t,i}$ and $\epsilon_{j}$ are independently sampled from their respective distributions: $p_{\epsilon_i}^s$ for the $i$-th entry of the style dynamics, and $p_{\epsilon_j}^c$ for the $j$-th entry of the content elements. The components of $\rvz_{t}^s$ are mutually independent, conditioned on all historical variables $\cup_{i=1}^{n_s}\mathbf{Pa}(z_{t,i}^s)$. The non-parametric causal transition $f_i$ enables an arbitrarily nonlinear interaction between the noise term $\epsilon_{t,i}$ and the set of parent variables $\mathbf{Pa}(z_{t,i}^s)$, allowing for flexible modeling of the style dynamics.

\subsection{Identification of the Latent Causal Process}

For simplicity, we denote $\textbf{f}^s$ as the group of functions $\{f^s_{i}\}_{i=1}^{n_s}$, and similarly for $\textbf{f}^c, \textbf{p}^s, \textbf{p}^c$.

\begin{definition}[Observational equivalence]
\label{def: equivalence}
Let $V=\{\rvx_1,\rvx_2,\cdots,\rvx_T\}$ represent the observed video generated by the true generative process specified by $(g, \textbf{f}^s,\textbf{f}^c, \textbf{p}^s, \textbf{p}^c)$, as defined in Equation~\ref{eq:generation}. A learned generative model $(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)$ is observationally equivalent to the true process if the model distribution $p_{(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)}(V)$ matches the data distribution $p_{(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)}(V)$ for all values of $V$.
\end{definition}

\textbf{Illustration.} When observational equivalence is achieved, the distribution of videos generated by the model exactly matches that of the ground truth, i.e., the training set. In other words, the model produces video data that is indistinguishable from the actual observed data. 

\begin{definition}[Blockwise Identification of Generative Process]
\label{def: blockwise identifiability}
Let the true generative process be $(g, \textbf{f}^s,\textbf{f}^c, \textbf{p}^s, \textbf{p}^c)$ as specified in Equation~\ref{eq:generation} and let its estimation be $(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)$. The generative process is identifiable up to the subspace of style dynamics and content elements, if the observational equivalence ensures that the estimated $(\hat{\rvz}_{t}^s,\hat{\rvz}^c)$ satisfies the condition that there exist bijective mappings from $(\hat{\rvz}_{t}^s,\hat{\rvz}^c)$ to $(\rvz_{t}^s,\rvz^c)$ and from $\hat{\rvz}^c$ to $\rvz^c$. Formally, there exists invertible functions $h:\mathbb{R}^{n_s+n_c}\rightarrow\mathbb{R}^{n_s+n_c}$ and $h_c:\mathbb{R}^{n_c}\rightarrow\mathbb{R}^{n_c}$ such that  
\begin{equation}
\label{eq: blockwise def}
\begin{aligned}
    &\ \  p_{(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)}(V)=p_{(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)}(V) \\
    \Rightarrow &\ \ [\rvz_{t}^s,\rvz^c] = h([\hat{\rvz}_t^s,\hat{\rvz}^c]), \rvz^c = h_c(\hat{\rvz}^c),
\end{aligned}
\end{equation}
 where $[\cdot]$ denotes concatenation.
\end{definition}


\textbf{Illustration.}
When blockwise identification is achieved, content elements are effectively disentangled. As a result, motion control can be applied independently, allowing manipulation of motion without altering the video's content. For example, in a video where a camera moves forward, it becomes easy to change the camera's direction without affecting the scene itself.

\begin{definition}[Component-wise Identification of Style Dynamics]
\label{def: Component-wise identifiability}
On top of Definition~\ref{def: blockwise identifiability}, when $h^s$ is a combination of permutation $\pi$ and a component-wise invertible transformation $\mathcal{T}$. Formally,
\begin{equation}
\begin{aligned}
    &\ \  p_{(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)}(V)=p_{(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)}(V) \\
    \Rightarrow &\ \ \rvz_{t}^s = (\pi\cdot\mathcal{T})(\hat{g}^{-1}_s(\rvx_{t})).
\end{aligned}
\end{equation}
\end{definition}

\textbf{Illustration.} 
Achieving component-wise identification ensures that each estimated style variable corresponds exactly to one true style variable, which facilitates the disentangling of motion features. This enables efficient and precise control over video generation. For example, when generating a video of a person's face, one can control different aspects, such as head movements or eye blinks, by adjusting the corresponding variables.

\vspace{-0.3cm}
\section{Theoritical Analysis}
In this section, we discuss the conditions under which the blockwise identification (Definition~\ref{def: blockwise identifiability}) and component-wise identification (Definition~\ref{def: Component-wise identifiability}) hold.

\subsection{Blockwise Identification}

Without loss of generality, we first consider the case where $\text{Pa}(z_{t,i}^s) = \rvz_{t-1}^s$, meaning that the time-dependent effects are governed by the dynamics of the previous time step.

% Without loss of generality, we first consider the case where $\text{Pa}(z_{t,i}^s) = \rvz_{t-1}^s$, meaning that the time-dependent effects are governed by the dynamics of the previous time step.

\begin{definition}(Linear Operator) \label{def:linear operator}
Consider two random variables $a$ and $b$ with support $\mathcal{A}$ and $\mathcal{B}$, the linear operator $L_{b \mid a}$ is defined as a mapping from a density function $p_a$ in some function space $\mathcal{F}(\mathcal{A})$ onto the density function $L_{b \mid a} \circ p_a$ in some function space $\mathcal{F}(\mathcal{B})$,  
\begin{equation*}
    \mathcal{F}(\mathcal{A}) \rightarrow \mathcal{F}(\mathcal{B}): \ p_b = L_{b \mid a} \circ p_a= \int_{\mathcal{A}} p_{b \mid a}(\cdot \mid a) p_a(a) da.
\end{equation*}
\end{definition}

\begin{theorem}[Blockwise Identifiability]
    \label{th: block-wise}
    \label{ap_th: block-wise}
    Consider video observation $V = \{\rvx_1, \rvx_2, \dots, \rvx_T\}$ generated by process $(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)$ with latent variables denoted as $\rvz_t^s$ and $\rvz_c$, according to Equation~\ref{eq:generation}, where $\rvx_t\in\mathbb{R}^{n_x},\rvz_t^s\in\mathbb{R}^{n_s},\rvz^c\in\mathbb{R}^{n_c}$. If assumptions
    \begin{itemize}
        \item B1 (Positive Density) the probability density function of latent variables is always positive and bounded;
        % \item B2 (Minimal Changes) for any fixed value of noise $\epsilon_t$ at time step $t$, the mapping from $\rvz_{t-1}^s$ to $\rvz_t^s$ must be injective;
        \item B2 (Minimal Changes) the linear operators $L_{\rvx_{t+1} \mid \rvz_t^s,\rvz^c}$ and $L_{\rvx_{t-1} \mid \rvx_{t+1}}$ are injective for bounded function space; 
        \item B3 (Weakly Monotonic) for any $\dot{\mathbf{z}}_{t}, \ddot{\mathbf{z}}_{t} \in \mathcal{Z}^c\times \mathcal{Z}^s_t$ $(\dot{\mathbf{z}}_{t} \neq  \ddot{\mathbf{z}}_{t})$, the set $\{ \mathbf{x}_t : p (\mathbf{x}_t|\dot{\mathbf{z}}_t) \neq p (\mathbf{x}_t|\ddot{\mathbf{z}}_t) \}$ has positive probability,
        and conditional densities are bounded and continuous;
        \item B4 (Blockwise Independence) the learned $\hat{\rvz}_t^s$ is independent with $\hat{\rvz}^c$;
    \end{itemize}
    are satisfied, then $\rvz_t$ is blockwisely identifiable with regard to $\hat{\rvz}_t$ from learned model $(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)$ under Observation Equivalence.
\end{theorem}

\textbf{Illustration of Assumptions.} 
The assumptions above are commonly used in the literature on the identification of latent variables under measurement error \citep{hu2008instrumental}.  
Firstly, Assumption B1 requires a continuous distribution. Secondly, Assumption B2 imposes a minimal requirement on the number of variables. The linear operator \( L_{b|a} \) ensures that there is sufficient variation in the density of \( b \) for different values of \( a \), thereby guaranteeing injectivity.  
In a video, \( \rvx_t \) is of much higher dimensionality compared to the latent variables. As a result, the injectivity assumption is easily satisfied. In practice, following the principle of minimal changes, if a model with fewer latent variables can successfully achieve observational equivalence, it is more likely to learn the true distribution. Assumption B3 requires the distribution of $\rvx_t$ changes when the value of latent variables changes. This assumption is much weaker compared to the widely used invertibility assumption adopted by previous works, such as \citep{yao2022temporally}. Assumption B4 requires that the learned latent variables have independent style dynamics and content elements.

Overall, the first three assumptions about the data are easily satisfied in real-world video scenarios. The last assumption, however, is a requirement for models. Most models that explicitly separate motion and identity satisfy this assumption.  




\textbf{Proof Sketch.} We separately prove the identifiability of all latent variables and $\rvz^c$. For the first part, it is built on \cite{fu2025identification}, following the line of work from \cite{hu2008instrumental}. Intuitively, it demonstrates that a minimum of 3 different observations of latent variables are required for identification under the given data generation process. For the second part, we use the contradiction to show that the same $\rvz^c$ in different frames of a video can be identified leveraging the invariance.


\subsection{Component-wise Identification}

\begin{theorem}[Component-wise Identifiability]
    \label{th: componenet-wise}
    Consider video observation $V = \{\rvx_1, \rvx_2, \dots, \rvx_T\}$ generated by process $(g, \textbf{f}^s, \textbf{f}^c, \textbf{p}^s, \textbf{p}^c)$ with latent variables denoted as $\rvz_t^s$ and $\rvz_c$, according to Equation~\ref{eq:generation}, where $\rvx_t\in\mathbb{R}^{n_x},\rvz_t^s\in\mathbb{R}^{n_s},\rvz^c\in\mathbb{R}^{n_c}$. Suppose assumptions in Theorem~\ref{th: block-wise} hold. If assumptions 
    \begin{itemize}
        \item C1 (Smooth and Positive Density) the probability density function of latent variables is always third-order differentiable and positive;
        \item C2 (Sufficient Changes) let $\eta_{t,i}\triangleq\log p(z^s_{t,i} | \rvz_{t-1}^s)$ and
        \begin{equation} 
            \label{Eq: assumption sufficient}
            \begin{aligned}
            \mathbf{v}_{t,l} 
            &\triangleq \Big(
            \frac{\partial^2 \eta_{t,1}}{\partial z_{t,1} \partial z_{t-1,l}}, 
            \cdots,
            \frac{\partial^2 \eta_{t,n_s}}{\partial z_{t,n} \partial z_{t-1,l}} \Big) \\
            &\oplus
            \Big(
            \frac{\partial^3 \eta_{t,1}}{\partial^2 z_{t,1} \partial z_{t-1,l}}, 
            \cdots,
            \frac{\partial^3 \eta_{t,n_s}}{\partial^2 z_{t,n} \partial z_{t-1,l}} \Big)
            ,
            \end{aligned}
        \end{equation}
        for $l \in\{1,2,\cdots,n\}$. For each value of $\rvz_t$, there exists $ 2n_s$ different of values of $z_{t-1,l}$ such that the $2n_s$ vector $\rvv_{t,l}\in\mathbb{R}^{2n_s}$ are linearly independent;
        \item C3 (Conditional Independence) the learned $\hat{\rvz}_t^s$ is independent with $\hat{\rvz}^c$, and all entries of $\hat{\rvz}_t^s$ are mutually independent conditioned on $\hat{\rvz}_{t-1}^s$;
    \end{itemize}
    \vspace{-0.2cm}
    are satisfied, then $\rvz_t^s$ is component-wisely identifiable with regard to $\hat{\rvz}_t^s$ from learned model $(\hat{g}, \hat{\textbf{f}}^s, \hat{\textbf{f}}^c, \hat{\textbf{p}}^s, \hat{\textbf{p}}^c)$ under Observation Equivalence.
\end{theorem}
\vspace{-0.2cm}
\textbf{Proof Sketch.}
In summary, component-wise identification relies on the changeability of style dynamics, i.e., sufficient changes. Starting from the results of blockwise identification, we establish the connection between \( \rvz_t^s \) and \( \hat{\rvz}_t^s \) in terms of their distributions, i.e., $p(\rvz_t)=p(\hat{\rvz}_t)\cdot|H_t|$, where $H_t$ is the jacobian matrix. Leveraging the second-order derivative of the log probability, we construct a system of equations with terms of \( \frac{\partial z_{t,i}^s}{\partial \hat{z}_{t,j}^s} \cdot \frac{\partial z_{t,i}^s}{\partial \hat{z}_{t,k}^s} \) and coefficients as specified in assumption C2. 
We leverage the third-order derivative of the previous latent variable $z_{t-1,l}$ to eliminate $|H_t|$, utilizing the fact that the history does not influence the current mapping from estimation to truth. Solving this system yields \( \frac{\partial z_{t,i}^s}{\partial \hat{z}_{t,j}^s} \cdot \frac{\partial z_{t,i}^s}{\partial \hat{z}_{t,k}^s} = 0 \).  



\textbf{Illustration of Assumptions.} 
The assumptions  C1 and C2 on the data generative process are commonly adopted in existing identifiable results for Temporally Causal Representation Learning \cite{yao2022temporally}. Specifically, C1 implies that the latent variables evolve continuously over time, while C2 ensures that the variability in the data can be effectively captured. The assumption C3 constraints to the learned model requires it to separate different variables of style dynamics into independent parts. 
\vspace{-0.2cm}
\section{Approach}
\vspace{-0.1cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/MainGraph.pdf} % Replace with your actual graph file name
    \vspace{-0.5cm}
    \caption{The workflow of the \textbf{Generator} proceeds from left to right, starting with random noise. It first passes through a Temporal Transition Module to obtain a disentangled representation, which is then fed into the synthesis network for frame-level generation. The blue arrow in the figure represents a component-wise flow.}
    \label{fig:main_graph}
    \vspace{-0.8cm}
\end{figure}

Given our results on identifiability, we implement our model, \ourmes. Our architecture is based on StyleGAN2-ADA \citep{skorokhodov2022stylegan}, incorporating a Temporal Transition Module in the Generator to enforce the minimal change principle and sufficient change property. Additionally, we add a Video Discriminator to ensure observational equivalence of the joint distribution $p(V)$.
\vspace{-0.2cm}
\subsection{Model Structure}

\textbf{Noise Sampling.} The structure of the generator is shown in Figure~\ref{fig:main_graph}. To generate a video with length $T$,
we first independently sample random noise from a normal distribution $\epsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})$. We then naively split them into several parts, i.e., 
$\epsilon = [\epsilon^c;\epsilon^s_1;\epsilon^s_2;\cdots;\epsilon^s_T]$. 



\textbf{Temporal Transition Module.} Following the generative process in Equation~\ref{eq:generation}, we handle $\rvz^c$ and $\rvz_t^s$ separately.  
On the one hand, we employ an autoregressive model to capture historical information, followed by a conditional flow to generate $\rvz_t^s$. Specifically, we implement a Gated Recurrent Unit (GRU) \citep{chung2014empirical} and Deep Sigmoid Flow (DSF) \citep{huang2018neural}, formulated as  
\begin{equation}
    \rvh_t = \text{GRU}(\rvh_{t-1},\epsilon_t^s), z_{t,i}^s=\text{DSF}_i(\epsilon_{t,i}^s;\rvh_{t-1}).
\end{equation}
On the other hand, since $\rvz^c$ are not required to be mutually independent, we use an MLP to generate $\rvz^c$, i.e.,  
\begin{equation}
    \rvz^c = \text{MLP}(\epsilon^c).
\end{equation}  
Concatenate $\rvz_t^s$ and $\rvz^c$, and then we obtain the disentangled representation $\rvz_t = \rvz^c \oplus \rvz_t^s$ for each frame at time step $t$ of the video.

\textbf{Synthesis Network.} The synthesis network is designed in the same way as StyleGAN2-ADA. The generated representation $\rvz_t$ is first fed into the mapping network to obtain a semantic vector $w(\rvz_t) \in \mathcal{W}$, and then the $t$-th frame of the video is generated by the convolutional network with $w$.

\textbf{Discriminator Structure.}  
To ensure observational equivalence, we implement a video discriminator \( D_V \) separate from the image discriminator \( D_I \). For the image discriminator, we follow the design of the original StyleGAN2-ADA. For the video discriminator, we adopt a channel-wise concatenation of activations at different resolutions to model and manage the spatiotemporal output of the generator.

\textbf{Loss.}
In addition to the original loss function of StyleGAN2-ADA, we introduce two additional losses: (1) a video discriminator loss, and (2) a mutual information maximization term \cite{chen2016infogan} between the latent dynamic variables $\rvz_t^s$ and the intermediate layer outputs of the video discriminator. This encourages the model to learn a more informative and structured representation.

% We incorporate an information regularization term to the adversarial loss of , inspired by , which encourages motion disentanglement and meaningful latent representations by maximizing the mutual information between motion-specific latent codes $\rvz_t^s$ and the video discriminator outputs.

\subsection{Relationship between Model and Theorem.}
In this subsection, we discuss the relationship between the \ourmes\  model and the Theorem.

% \textbf{Observational Equivalence.} 

\textbf{Blockwise Identification.} As discussed in Theorem~\ref{th: block-wise}, achieving blockwise identifiability benefits from minimizing the dimension $n_s$ of the style dynamics, especially when the true $n_s$ is unknown. In practice, this translates to a hyperparameter selection question. In our experiments, we opted for a relatively modest value of $n_s$ and observed that it suffices to attain a satisfactory level of disentanglement capability.
\vspace{-3pt}
Furthermore, as required by assumption B4, the learned variables $\hat{\rvz}_t^s$ and $\hat{\rvz}^c$ are blockwise independent, i.e., $\hat{\rvz}_t^s\perp \!\!\! \perp\hat{\rvz}^c$. This independence is necessary to achieve blockwise identifiability.


\vspace{-3pt}
\textbf{Componenetwise Identification.} As outlined in Theorem~\ref{th: componenet-wise}, the sufficient change property is a critical assumption for achieving identifiability. To enforce temporally conditional independence, we employ a component-wise flow model that transforms a set of independent noise variables $\epsilon_t^s$ into the style dynamics, conditioned on historical information. Furthermore, the flow model is designed to maximally preserve the information from $\epsilon_{t,i}^s$ to $z_{t,i}^s$, enabling the model to effectively capture sufficient variability in the data. 

Note that when computing the historical information $h_t$, we utilize $\epsilon_t^s$ instead of $\rvz_t^s$ (as illustrated in the generative process in Equation~\ref{eq:generation}) as the condition for the component-wise flow. This approach offers two key advantages. First, it simplifies the model architecture since the flow does not need to incorporate the output from another flow. Second, the noise terms already fully characterize the corresponding style dynamics, which remains consistent with the theoretical framework.

Furthermore, given that the precise time lag of dynamic variables remains unspecified a priori in the dataset, the GRU's gating mechanism can selectively filter out irrelevant historical information that lies outside $\text{Pa}(\rvz_t)$. This capability enables the model to demonstrate significantly superior performance compared to traditional non-gated architectures, such as vanilla RNNs.

A detailed ablation study is presented in Section~\ref{sec: ablation study}.

% \textbf{Gated Recurrent Unit.} 
% \textbf{Deep Sigmoid Flow}. The component-wise flow model is employed  while preserving as much information as possible for $z_{t,i}^s$ from $\epsilon_{t,i}$, allowing the model to effectively capture sufficient changes in the data. 


\vspace{-10pt}
\section{Experiments}
\vspace{-3pt}
% We conduct the experiments on three real-world datasets and one synthetic dataset. We first evaluate the quality of the generated videos utilizing an unconditional setting. Subsequently, we analyze the model’s properties, including controllability, robustness, and computational efficiency. Finally, we perform an ablation study. An ablation study is performed to provide insights into the underlying generative process with disentanglement guarantees.

In this section, we perform extensive experimental validation to comprehensively evaluate \ourmes. We design both qualitative and quantitative experiments to systematically assess the generation quality and controllability of our proposed model. Additionally, we conduct comparative studies to evaluate the robustness and computational efficiency of our model against state-of-the-art baselines. Finally, we perform comprehensive ablation studies to demonstrate the effectiveness of our proposed modules and analyze their individual contributions.
\vspace{-4pt}
\subsection{Experimental Setups}

\textbf{Datasets.} 
We evaluate our model on three different real-world datasets: FaceForensics~\cite{rossler2018faceforensics}, SkyTimelapse~\cite{xiong2018learning}, and RealEstate~\cite{zhou2018stereo}. These datasets respectively capture dynamic variations in facial expressions, sky transitions, and complex camera movements, providing comprehensive evaluation scenarios for our method. FaceForensics and SkyTimelapse are widely adopted benchmarks in video generation research, particularly for facial manipulation and natural scene modeling. The RealEstate dataset contains video sequences featuring complex camera movements within static scenes. The camera motions in RealEstate are more intuitive and easily interpretable by human observers, including forward/backward movements, lateral shifts, etc. We particularly leverage this dataset to effectively demonstrate the precise controllability of \ourmes\ in handling explicit camera motion parameters. All datasets consist of videos with a resolution of 256 × 256 pixels, and we employ standard train-test splits for fair evaluation. Detailed statistics and additional information about the datasets are provided in Appendix~\ref{ap_sec: dataset details}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/BenchmarkGeneration.pdf} 
    \caption{Random samples from the comparison baselines and our model on real-world datasets FaceForensics $256^2$, SkyTimelapse $256^2$, RealEstate $256^2$. Start from t = 0 and report every 2-nd frame from a 16-frame video clip. }
    \label{fig:benchmark_generation}
    \vspace{-0.8cm}
\end{figure}

\textbf{Evaluation metrics.}
% 介绍整体评估框架
To comprehensively evaluate the performance of \ourmes, we employ both quantitative and qualitative assessment metrics. For quantitative evaluation, we adopt the Fréchet Video Distance (FVD)~\cite{unterthiner2018towards}, a widely-used metric for assessing video generation quality. FVD computes the Wasserstein-2 distance between feature distributions of real and generated videos in a learned feature space, providing a comprehensive measure of visual quality and temporal coherence. We report FVD scores at two different temporal scales: $\text{FVD}_8$ and $\text{FVD}_{16}$, where the subscript denotes the number of frames in a video. For qualitative evaluation, we present randomly sampled video sequences generated by our model. 
% We employ models trained on 16-frame video sequences and visualize frames at time steps $t=2,4,6,\cdots,16$ to effectively demonstrate the temporal evolution and dynamic characteristics of the generated videos. 

\subsection{Quality of the video.}
We conduct comparisons between our proposed \ourmes\ and four baselines: MoCoGAN-HD, DIGAN, StyleGAN-V, and MoStGAN-V, across three benchmark datasets: FaceForensics, SkyTimelapse, and RealEstate. For MoCoGAN-HD, we utilize the image generator pretrained by the original authors and train the remaining components (including the motion generator and discriminator). Due to the unavailability of pretrained models for the RealEstate dataset, we exclude MoCoGAN-HD from the comparisons on this specific dataset. For the remaining baselines (DIGAN, StyleGAN-V, and MoSTGAN-V), we train the models from scratch using their official implementations to ensure comparability. The quantitative evaluation results, presented in Table~\ref{tab:comparison}, show that \ourmes\ consistently achieves superior performance across all datasets, with significantly lower FVD scores compared to the baseline methods. This improvement in FVD metrics demonstrates our model's enhanced capability to generate high-quality, temporally coherent videos. Some randomly sampled videos are shown in Figure~\ref{fig:benchmark_generation}.



\begin{figure*}[t]
\small
    \centering
    \includegraphics[width=1.0\textwidth]{figs/Control_ffs_re.pdf} 
    \vspace{-0.4cm}
    \caption{Generate same motion of different identities in (a) FaceForensics, (b) RealEstate.}
    \label{fig:control}
    \vspace{-3pt}
\end{figure*}



\begin{table}[t]
\centering
\scriptsize 
\caption{$\text{FVD}_{8}\downarrow$ and $\text{FVD}_{16}\downarrow$ scores across the FaceForensics, SkyTimelapse, and RealEstate datasets.}

\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.2} 
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{FaceForensics $\text{256}^2$}} & 
\multicolumn{2}{c}{\textbf{SkyTimelapse $\text{256}^2$}} & 
\multicolumn{2}{c}{\textbf{RealEstate $\text{256}^2$}} \\
               & $\text{FVD}_8$  & $\text{FVD}_{16}$  & $\text{FVD}_8$  & $\text{FVD}_{16}$  & $\text{FVD}_8$  & $\text{FVD}_{16}$  \\
\midrule
MoCoGAN-HD     & 140.05 & 185.51 & 1214.13 & 1721.89 & -     & -      \\
DIGAN          & 57.52  & 61.65  & 60.54   & 105.03  & 182.86 & 178.27 \\
StyleGAN-V     & 49.24  & 52.70  & 45.30   & 62.55   & 199.66 & 201.95 \\
MoStGAN-V      & 47.67  & 49.85  & 40.97   & 55.36   & 247.77 & 265.54 \\
CoVoGAN (ours) & \textbf{43.75}  & \textbf{48.80}  & \textbf{35.58}   & \textbf{46.51}   & \textbf{154.88} & \textbf{174.87} \\
\bottomrule
\end{tabular}
\label{tab:comparison}
\vspace{-4pt}
\end{table}

\begin{figure}[t]
\small
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ControlFace.pdf} 
    \caption{Controllability visualization results on the FaceForensics dataset. Two different concepts of motion is modified to show the component-wise level disentanglement. The fisrt line is a randomly sampled video, and the following lines apply control of eye blinking and head shaking.}
    \label{fig:control2}
    \vspace{-6pt}
\end{figure}



\subsection{Controllability}

\textbf{Blockwise Disentanglement.}
Figure~\ref{fig:control} demonstrates the video controllability of \ourmes\ in comparison with baseline methods, highlighting our model's superior capability of disentanglement between motion and content. The analysis follows a systematic procedure: we first generate a base video sequence, then apply a controlled modification by adding a value to specific motion-related latent variables. 

For \ourmes, we modify one dimension of the style dynamics $\rvz_t^s$. For StyleGAN-V and MoStGAN-V, we manipulate one dimension of its (latent) motion code. We apply equivalent modifications to the corresponding latent dimensions in each baseline model. To validate the consistency of our controllability analysis, we randomly sample three distinct video sequences and apply identical modifications to their respective latent representations. DIGAN is not compared since there are no specific variables for motion.

The results show that our proposed \ourmes\ model learns a disentangled representation that effectively separates style dynamics from content elements. (1) This disentanglement enables independent manipulation of motion characteristics while preserving content consistency. (2) A key advantage of this approach is that identical modifications to the style latent space consistently produce similar motion patterns across different content identities.

Baseline models achieve only partial disentanglement, exhibiting two major limitations: (1) visual distortions of the modified videos and (2) inconsistent or misaligned motion patterns when applied to different identities.

\begin{table}[t]
    \centering
    \caption{
        Comparison of different models in terms of generator parameters (Params, in millions) and the time (Time, in seconds) required to generate 16-frame 2048 videos with a resolution of $256 \times 256$.
    }
    \vspace{-3pt} 

    \label{tab:params_and_time}
    \resizebox{0.6\linewidth}{!}{ 
        \begin{tabular}{lcc}
            \toprule
            \textbf{Method} & \textbf{Params (M)} & \textbf{Time (s)} \\
            \midrule
            StyleGAN2-ADA   & 23.19               & -\\
            DIGAN           & 69.97                 & 142.06              \\
            StyleGAN-v      & 32.11                 & 181.45              \\
            MoStGAN         & 40.92                 & 207.11             \\
            CoVoGAN (ours)    & \textbf{24.98}                 &  \textbf{98.93}              \\
            \bottomrule
        \end{tabular}
        \vspace{-0.5cm}
    }
\end{table}

\textbf{Componenet-wise Disentanglement.}
We further illustrate the precise control over individual motion components. This capability is enabled by the component-wise identifiability of our model, which ensures that each latent dimension corresponds to a specific and interpretable motion attribute.

Our experimental procedure begins with randomly sampling two distinct video sequences, as illustrated in the first row of Figure~\ref{fig:control2}. We then selectively modify the latent dimension corresponding to eye blinking dynamics in the second line. Subsequently, we modify a second latent dimension controlling head shaking motion while maintaining the previously adjusted eye blinking pattern in the last line. The results show naturalistic head movements from left to right, synchronized with the preserved eye blinking, illustrating our model's capability for independent yet coordinated control of multiple motion components.






\subsection{More Evaluations}

\textbf{Robustness.} The architectural simplicity of \ourmes\ contributes to a more stable and efficient convergence compared to baseline models, as shown in Figure~\ref{fig:fvd_curve}. Our model demonstrates faster convergence and greater stability, particularly on challenging noncenter-focused datasets such as RealEstate. These datasets are notoriously difficult for GAN-based approaches. Furthermore, \ourmes\ exhibits enhanced robustness against model collapse during the later stages of training, a common issue in adversarial training frameworks.
\begin{figure}[!t]
\small
    \centering
    \includegraphics[width=0.32\textwidth]{figs/fvd_curve_via_time.pdf} 
    \caption{Comparisons of FVD variations during training steps (kimg) on RealEstate Dataset VS baselines. }
    \label{fig:fvd_curve}
    \vspace{-3pt}
\end{figure}

\vspace{-1pt}
\textbf{Computational Efficiency.}
% StyleGAN2-ADA: G params = 23.19M, D params = 24.00M, Total params = 47.19M
% DIGAN: G params = 69.97M, D params = 57.73M, Total params = 127.70M
% StyleGAN-v: G params = 32.11M, D params = 25.33M, Total params = 57.44M
% MoStGAN: G params = 40.92M, D params = 25.92M, Total params = 66.84M
% CVGAN (ours): G params = 24.98M, D params = 41.59M, Total params = 66.57M
We also compare the computational efficiency with that of the baselines. The size of our model’s generator is comparable to that of StyleGAN2-ADA but significantly smaller than the generators of the baselines. Additionally, the inference time of our model is much faster, as shown in Table~\ref{tab:params_and_time}.

\vspace{-3pt}
\section{Conclusion}
\vspace{-3pt}
In this paper, we proposed a Temporal Transition Module and implemented it in a GAN to achieve \ourmes. By leveraging the principle of minimal and sufficient changes, we successfully disentangled (1) the motion and content of a video, and (2) different concepts within the motion. We established an identifiability guarantee for both blockwise and component-wise disentanglement. Our proposed \ourmes ~model demonstrates high generative quality, controllability, and a more robust and computationally efficient structure. We validated the performance on various datasets and conducted ablation experiments to further confirm the effectiveness of our model.

% consider learning temporal causal representation under the non-invertible generation process, which is motivated by the common requirement of the temporal system, such as the visual perception process. We have established identifiability theories that allow for recovering the latent causal process with the nonlinear and non-invertible mixing function. Furthermore, based on this

\newpage
\section*{Impact Statements}
This study introduces both a theoretical framework and a practical approach for extracting disentangled causal representations from videos. Such advancements enable the development of more transparent and interpretative models, enhancing our grasp of causal dynamics in real-world settings. This
approach may benefit many real-world applications, including healthcare, auto-driving, content generation, marketing and so on.
\bibliography{covogan}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{supplementary}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
